- en: 13  Data Analysis Examples
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://wesmckinney.com/book/data-analysis-examples](https://wesmckinney.com/book/data-analysis-examples)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*This Open Access web version of *Python for Data Analysis 3rd Edition* is
    now available as a companion to the [print and digital editions](https://amzn.to/3DyLaJc).
    If you encounter any errata, [please report them here](https://oreilly.com/catalog/0636920519829/errata).
    Please note that some aspects of this site as produced by Quarto will differ from
    the formatting of the print and eBook versions from O’Reilly.'
  prefs: []
  type: TYPE_NORMAL
- en: If you find the online edition of the book useful, please consider [ordering
    a paper copy](https://amzn.to/3DyLaJc) or a [DRM-free eBook](https://www.ebooks.com/en-us/book/210644288/python-for-data-analysis/wes-mckinney/?affId=WES398681F)
    to support the author. The content from this website may not be copied or reproduced.
    The code examples are MIT licensed and can be found on GitHub or Gitee.*  *Now
    that we've reached the final chapter of this book, we're going to take a look
    at a number of real-world datasets. For each dataset, we'll use the techniques
    presented in this book to extract meaning from the raw data. The demonstrated
    techniques can be applied to all manner of other datasets. This chapter contains
    a collection of miscellaneous example datasets that you can use for practice with
    the tools in this book.
  prefs: []
  type: TYPE_NORMAL
- en: The example datasets are found in the book's accompanying [GitHub repository](http://github.com/wesm/pydata-book).
    If you are unable to access GitHub, you can also get them from the [repository
    mirror on Gitee](https://gitee.com/wesmckinn/pydata-book).
  prefs: []
  type: TYPE_NORMAL
- en: 13.1 Bitly Data from 1.USA.gov
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In 2011, the URL shortening service [Bitly](https://bitly.com) partnered with
    the US government website [USA.gov](https://www.usa.gov) to provide a feed of
    anonymous data gathered from users who shorten links ending with *.gov* or *.mil*.
    In 2011, a live feed as well as hourly snapshots were available as downloadable
    text files. This service is shut down at the time of this writing (2022), but
    we preserved one of the data files for the book's examples.
  prefs: []
  type: TYPE_NORMAL
- en: 'In the case of the hourly snapshots, each line in each file contains a common
    form of web data known as JSON, which stands for JavaScript Object Notation. For
    example, if we read just the first line of a file, we may see something like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: 'Python has both built-in and third-party libraries for converting a JSON string
    into a Python dictionary. Here we’ll use the `json` module and its `loads` function
    invoked on each line in the sample file we downloaded:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting object `records` is now a list of Python dictionaries:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: Counting Time Zones in Pure Python
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose we were interested in finding the time zones that occur most often
    in the dataset (the `tz` field). There are many ways we could do this. First,
    let’s extract a list of time zones again using a list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Oops! Turns out that not all of the records have a time zone field. We can
    handle this by adding the check `if "tz" in rec` at the end of the list comprehension:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'Just looking at the first 10 time zones, we see that some of them are unknown
    (empty string). You can filter these out also, but I’ll leave them in for now.
    Next, to produce counts by time zone, I’ll show two approaches: a harder way (using
    just the Python standard library) and a simpler way (using pandas). One way to
    do the counting is to use a dictionary to store counts while we iterate through
    the time zones:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'Using more advanced tools in the Python standard library, you can write the
    same thing more briefly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'I put this logic in a function just to make it more reusable. To use it on
    the time zones, just pass the `time_zones` list:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: 'If we wanted the top 10 time zones and their counts, we can make a list of
    tuples by `(count, timezone)` and sort it:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'We have then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'If you search the Python standard library, you may find the `collections.Counter`
    class, which makes this task even simpler:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: Counting Time Zones with pandas
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'You can create a DataFrame from the original set of records by passing the
    list of records to `pandas.DataFrame`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: 'We can look at some basic information about this new DataFrame, such as column
    names, inferred column types, or number of missing values, using `frame.info()`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'The output shown for the `frame` is the *summary view*, shown for large DataFrame
    objects. We can then use the `value_counts` method for the Series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: 'We can visualize this data using matplotlib. We can make the plots a bit nicer
    by filling in a substitute value for unknown or missing time zone data in the
    records. We replace the missing values with the `fillna` method and use Boolean
    array indexing for the empty strings:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'At this point, we can use the [seaborn package](http://seaborn.pydata.org)
    to make a horizontal bar plot (see [Top time zones in the 1.usa.gov sample data](#usa_gov_counts)
    for the resulting visualization):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/7dd52fd5fa321b6a9d79111326825f9b.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.1: Top time zones in the 1.usa.gov sample data'
  prefs: []
  type: TYPE_NORMAL
- en: 'The `a` field contains information about the browser, device, or application
    used to perform the URL shortening:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'Parsing all of the interesting information in these “agent” strings may seem
    like a daunting task. One possible strategy is to split off the first token in
    the string (corresponding roughly to the browser capability) and make another
    summary of the user behavior:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, suppose you wanted to decompose the top time zones into Windows and non-Windows
    users. As a simplification, let’s say that a user is on Windows if the string
    `"Windows"` is in the agent string. Since some of the agents are missing, we’ll
    exclude these from the data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We want to then compute a value for whether or not each row is Windows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, you can group the data by its time zone column and this new list of operating
    systems:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'The group counts, analogous to the `value_counts` function, can be computed
    with `size`. This result is then reshaped into a table with `unstack`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Finally, let’s select the top overall time zones. To do so, I construct an
    indirect index array from the row counts in `agg_counts`. After computing the
    row counts with `agg_counts.sum("columns")`, I can call `argsort()` to obtain
    an index array that can be used to sort in ascending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'I use `take` to select the rows in that order, then slice off the last 10 rows
    (largest values):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas has a convenience method called `nlargest` that does the same thing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, this can be plotted in a grouped bar plot comparing the number of Windows
    and non-Windows users, using seaborn''s `barplot` function (see [Top time zones
    by Windows and non-Windows users](#usa_gov_tz_os)). I first call `count_subset.stack()`
    and reset the index to rearrange the data for better compatibility with seaborn:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f69b4d795dc2eedaf3e3e02e6e0a9d87.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.2: Top time zones by Windows and non-Windows users'
  prefs: []
  type: TYPE_NORMAL
- en: 'It is a bit difficult to see the relative percentage of Windows users in the
    smaller groups, so let''s normalize the group percentages to sum to 1:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'Then plot this in [Percentage Windows and non-Windows users in top occurring
    time zones](#usa_gov_tz_os_normed):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/a218b1a30914e6cbfa11d7eb83a4f1cb.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.3: Percentage Windows and non-Windows users in top occurring time
    zones'
  prefs: []
  type: TYPE_NORMAL
- en: 'We could have computed the normalized sum more efficiently by using the `transform`
    method with `groupby`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 13.2 MovieLens 1M Dataset
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: '[GroupLens Research](https://grouplens.org/datasets/movielens) provides a number
    of collections of movie ratings data collected from users of MovieLens in the
    late 1990s and early 2000s. The data provides movie ratings, movie metadata (genres
    and year), and demographic data about the users (age, zip code, gender identification,
    and occupation). Such data is often of interest in the development of recommendation
    systems based on machine learning algorithms. While we do not explore machine
    learning techniques in detail in this book, I will show you how to slice and dice
    datasets like these into the exact form you need.'
  prefs: []
  type: TYPE_NORMAL
- en: 'The MovieLens 1M dataset contains one million ratings collected from six thousand
    users on four thousand movies. It’s spread across three tables: ratings, user
    information, and movie information. We can load each table into a pandas DataFrame
    object using `pandas.read_table`. Run the following code in a Jupyter cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'You can verify that everything succeeded by looking at each DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Note that ages and occupations are coded as integers indicating groups described
    in the dataset’s *README* file. Analyzing the data spread across three tables
    is not a simple task; for example, suppose you wanted to compute mean ratings
    for a particular movie by gender identity and age. As you will see, this is more
    convenient to do with all of the data merged together into a single table. Using
    pandas’s `merge` function, we first merge `ratings` with `users` and then merge
    that result with the `movies` data. pandas infers which columns to use as the
    merge (or *join*) keys based on overlapping names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'To get mean movie ratings for each film grouped by gender, we can use the `pivot_table`
    method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: 'This produced another DataFrame containing mean ratings with movie titles as
    row labels (the "index") and gender as column labels. I first filter down to movies
    that received at least 250 ratings (an arbitrary number); to do this, I group
    the data by title, and use `size()` to get a Series of group sizes for each title:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'The index of titles receiving at least 250 ratings can then be used to select
    rows from `mean_ratings` using `.loc`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: 'To see the top films among female viewers, we can sort by the `F` column in
    descending order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]'
  prefs: []
  type: TYPE_PRE
- en: Measuring Rating Disagreement
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Suppose you wanted to find the movies that are most divisive between male and
    female viewers. One way is to add a column to `mean_ratings` containing the difference
    in means, then sort by that:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'Sorting by `"diff"` yields the movies with the greatest rating difference so
    that we can see which ones were preferred by women:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: 'Reversing the order of the rows and again slicing off the top 10 rows, we get
    the movies preferred by men that women didn’t rate as highly:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose instead you wanted the movies that elicited the most disagreement among
    viewers, independent of gender identification. Disagreement can be measured by
    the variance or standard deviation of the ratings. To get this, we first compute
    the rating standard deviation by title and then filter down to the active titles:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, we sort in descending order and select the first 10 rows, which are roughly
    the 10 most divisively rated movies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'You may have noticed that movie genres are given as a pipe-separated (`|`)
    string, since a single movie can belong to multiple genres. To help us group the
    ratings data by genre, we can use the `explode` method on DataFrame. Let''s take
    a look at how this works. First, we can split the genres string into a list of
    genres using the `str.split` method on the Series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, calling `movies.explode("genre")` generates a new DataFrame with one row
    for each "inner" element in each list of movie genres. For example, if a movie
    is classified as both a comedy and a romance, then there will be two rows in the
    result, one with just `"Comedy"` and the other with just `"Romance"`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we can merge all three tables together and group by genre:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: 13.3 US Baby Names 1880–2010
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: The United States Social Security Administration (SSA) has made available data
    on the frequency of baby names from 1880 through the present. Hadley Wickham,
    an author of several popular R packages, has this dataset in illustrating data
    manipulation in R.
  prefs: []
  type: TYPE_NORMAL
- en: 'We need to do some data wrangling to load this dataset, but once we do that
    we will have a DataFrame that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: 'There are many things you might want to do with the dataset:'
  prefs: []
  type: TYPE_NORMAL
- en: Visualize the proportion of babies given a particular name (your own, or another
    name) over time
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the relative rank of a name
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: Determine the most popular names in each year or the names whose popularity
    has advanced or declined the most
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyze trends in names: vowels, consonants, length, overall diversity, changes
    in spelling, first and last letters'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: 'Analyze external sources of trends: biblical names, celebrities, demographics'
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
- en: With the tools in this book, many of these kinds of analyses are within reach,
    so I will walk you through some of them.
  prefs: []
  type: TYPE_NORMAL
- en: As of this writing, the US Social Security Administration makes available data
    files, one per year, containing the total number of births for each sex/name combination.
    You can download the [raw archive](http://www.ssa.gov/oact/babynames/limits.html)
    of these files.
  prefs: []
  type: TYPE_NORMAL
- en: 'If this page has been moved by the time you’re reading this, it can most likely
    be located again with an internet search. After downloading the “National data”
    file *names.zip* and unzipping it, you will have a directory containing a series
    of files like *yob1880.txt*. I use the Unix `head` command to look at the first
    10 lines of one of the files (on Windows, you can use the `more` command or open
    it in a text editor):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: 'As this is already in comma-separated form, it can be loaded into a DataFrame
    with `pandas.read_csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'These files only contain names with at least five occurrences in each year,
    so for simplicity’s sake we can use the sum of the births column by sex as the
    total number of births in that year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: 'Since the dataset is split into files by year, one of the first things to do
    is to assemble all of the data into a single DataFrame and further add a `year`
    field. You can do this using `pandas.concat`. Run the following in a Jupyter cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'There are a couple things to note here. First, remember that `concat` combines
    the DataFrame objects by row by default. Second, you have to pass `ignore_index=True`
    because we’re not interested in preserving the original row numbers returned from
    `pandas.read_csv`. So we now have a single DataFrame containing all of the names
    data across all years:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'With this data in hand, we can already start aggregating the data at the year
    and sex level using `groupby` or `pivot_table` (see [Total births by sex and year](#baby_names_total_births)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/6716e4c3fc8eb1a44199a90351da2a0d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.4: Total births by sex and year'
  prefs: []
  type: TYPE_NORMAL
- en: 'Next, let’s insert a column `prop` with the fraction of babies given each name
    relative to the total number of births. A `prop` value of `0.02` would indicate
    that 2 out of every 100 babies were given a particular name. Thus, we group the
    data by year and sex, then add the new column to each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting complete dataset now has the following columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: 'When performing a group operation like this, it''s often valuable to do a sanity
    check, like verifying that the `prop` column sums to 1 within all the groups:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: 'Now that this is done, I’m going to extract a subset of the data to facilitate
    further analysis: the top 1,000 names for each sex/year combination. This is yet
    another group operation:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'We can drop the group index since we don''t need it for our analysis:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting dataset is now quite a bit smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: We’ll use this top one thousand dataset in the following investigations into
    the data.
  prefs: []
  type: TYPE_NORMAL
- en: Analyzing Naming Trends
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'With the full dataset and the top one thousand dataset in hand, we can start
    analyzing various naming trends of interest. First, we can split the top one thousand
    names into the boy and girl portions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'Simple time series, like the number of Johns or Marys for each year, can be
    plotted but require some manipulation to be more useful. Let’s form a pivot table
    of the total number of births by year and name:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, this can be plotted for a handful of names with DataFrame’s `plot` method
    ([A few boy and girl names over time](#baby_names_some_names) shows the result):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e1a3230ba5c578a21a784b09be9c0e91.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.5: A few boy and girl names over time'
  prefs: []
  type: TYPE_NORMAL
- en: On looking at this, you might conclude that these names have grown out of favor
    with the American population. But the story is actually more complicated than
    that, as will be explored in the next section.
  prefs: []
  type: TYPE_NORMAL
- en: Measuring the increase in naming diversity
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'One explanation for the decrease in plots is that fewer parents are choosing
    common names for their children. This hypothesis can be explored and confirmed
    in the data. One measure is the proportion of births represented by the top 1,000
    most popular names, which I aggregate and plot by year and sex ([Proportion of
    births represented in top one thousand names by sex](#baby_names_tot_prop) shows
    the resulting plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/24e093fb2856ccb1ffa83d884da1713d.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.6: Proportion of births represented in top one thousand names by
    sex'
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that, indeed, there appears to be increasing name diversity (decreasing
    total proportion in the top one thousand). Another interesting metric is the number
    of distinct names, taken in order of popularity from highest to lowest, in the
    top 50% of births. This number is trickier to compute. Let’s consider just the
    boy names from 2010:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: 'After sorting `prop` in descending order, we want to know how many of the most
    popular names it takes to reach 50%. You could write a `for` loop to do this,
    but a vectorized NumPy way is more computationally efficient. Taking the cumulative
    sum, `cumsum`, of `prop` and then calling the method `searchsorted` returns the
    position in the cumulative sum at which `0.5` would need to be inserted to keep
    it in sorted order:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: 'Since arrays are zero-indexed, adding 1 to this result gives you a result of
    117\. By contrast, in 1900 this number was much smaller:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'You can now apply this operation to each year/sex combination, `groupby` those
    fields, and `apply` a function returning the count for each group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: 'This resulting DataFrame `diversity` now has two time series, one for each
    sex, indexed by year. This can be inspected and plotted as before (see [Plot of
    diversity metric by year](#baby_names_diversity_fig)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/1e1bea5330bbf1ad2202b26ba95095b8.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.7: Plot of diversity metric by year'
  prefs: []
  type: TYPE_NORMAL
- en: As you can see, girl names have always been more diverse than boy names, and
    they have only become more so over time. Further analysis of what exactly is driving
    the diversity, like the increase of alternative spellings, is left to the reader.
  prefs: []
  type: TYPE_NORMAL
- en: The “last letter” revolution
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'In 2007, baby name researcher Laura Wattenberg pointed out that the distribution
    of boy names by final letter has changed significantly over the last 100 years.
    To see this, we first aggregate all of the births in the full dataset by year,
    sex, and final letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we select three representative years spanning the history and print the
    first few rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, normalize the table by total births to compute a new table containing
    the proportion of total births for each sex ending in each letter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'With the letter proportions now in hand, we can make bar plots for each sex,
    broken down by year (see [Proportion of boy and girl names ending in each letter](#baby_names_last_letter)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/5f557600f8cbeb90fd07375592d49c18.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.8: Proportion of boy and girl names ending in each letter'
  prefs: []
  type: TYPE_NORMAL
- en: 'As you can see, boy names ending in *n* have experienced significant growth
    since the 1960s. Going back to the full table created before, I again normalize
    by year and sex and select a subset of letters for the boy names, finally transposing
    to make each column a time series:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: 'With this DataFrame of time series in hand, I can make a plot of the trends
    over time again with its `plot` method (see [Proportion of boys born with names
    ending in d/n/y over time](#baby_names_letter_over_time)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e7ac4057ad5de3681057e0690540ab39.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.9: Proportion of boys born with names ending in d/n/y over time'
  prefs: []
  type: TYPE_NORMAL
- en: Boy names that became girl names (and vice versa)
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: 'Another fun trend is looking at names that were more popular with one gender
    earlier in the sample but have become preferred as a name for the other gender
    over time. One example is the name Lesley or Leslie. Going back to the `top1000`
    DataFrame, I compute a list of names occurring in the dataset starting with "Lesl":'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, we can filter down to just those names and sum births grouped by
    name to see the relative frequencies:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Next, let’s aggregate by sex and year, and normalize within year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, it’s now possible to make a plot of the breakdown by sex over time
    (see [Proportion of male/female Lesley-like names over time](#baby_names_lesley)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/61d4a67e0c93651a04b70d68220275c0.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.10: Proportion of male/female Lesley-like names over time'
  prefs: []
  type: TYPE_NORMAL
- en: 13.4 USDA Food Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The US Department of Agriculture (USDA) makes available a database of food
    nutrient information. Programmer Ashley Williams created a version of this database
    in JSON format. The records look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: Each food has a number of identifying attributes along with two lists of nutrients
    and portion sizes. Data in this form is not particularly amenable to analysis,
    so we need to do some work to wrangle the data into a better form.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can load this file into Python with any JSON library of your choosing.
    I’ll use the built-in Python `json` module:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 'Each entry in `db` is a dictionary containing all the data for a single food.
    The `"nutrients"` field is a list of dictionaries, one for each nutrient:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE78]'
  prefs: []
  type: TYPE_PRE
- en: 'When converting a list of dictionaries to a DataFrame, we can specify a list
    of fields to extract. We’ll take the food names, group, ID, and manufacturer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE79]'
  prefs: []
  type: TYPE_PRE
- en: From the output of `info.info()`, we can see that there is missing data in the
    `manufacturer` column.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see the distribution of food groups with `value_counts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE80]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, to do some analysis on all of the nutrient data, it’s easiest to assemble
    the nutrients for each food into a single large table. To do so, we need to take
    several steps. First, I’ll convert each list of food nutrients to a DataFrame,
    add a column for the food `id`, and append the DataFrame to a list. Then, these
    can be concatenated with `concat`. Run the following code in a Jupyter cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE81]'
  prefs: []
  type: TYPE_PRE
- en: 'If all goes well, `nutrients` should look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE82]'
  prefs: []
  type: TYPE_PRE
- en: 'I noticed that there are duplicates in this DataFrame, so it makes things easier
    to drop them:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE83]'
  prefs: []
  type: TYPE_PRE
- en: 'Since `"group"` and `"description"` are in both DataFrame objects, we can rename
    for clarity:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE84]'
  prefs: []
  type: TYPE_PRE
- en: 'With all of this done, we’re ready to merge `info` with `nutrients`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE85]'
  prefs: []
  type: TYPE_PRE
- en: 'We could now make a plot of median values by food group and nutrient type (see
    [Median zinc values by food group](#fig_wrangle_zinc)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE86]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/c65abbdbb1b538d16ea4f45c46581a89.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.11: Median zinc values by food group'
  prefs: []
  type: TYPE_NORMAL
- en: 'Using the `idxmax` or `argmax` Series methods, you can find which food is most
    dense in each nutrient. Run the following in a Jupyter cell:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE87]'
  prefs: []
  type: TYPE_PRE
- en: 'The resulting DataFrame is a bit too large to display in the book; here is
    only the `"Amino Acids"` nutrient group:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE88]'
  prefs: []
  type: TYPE_PRE
- en: 13.5 2012 Federal Election Commission Database
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: 'The US Federal Election Commission (FEC) publishes data on contributions to
    political campaigns. This includes contributor names, occupation and employer,
    address, and contribution amount. The contribution data from the 2012 US presidential
    election was available as a single 150-megabyte CSV file *P00000001-ALL.csv* (see
    the book''s data repository), which can be loaded with `pandas.read_csv`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE89]'
  prefs: []
  type: TYPE_PRE
- en: '*Note* *Several people asked me to update the dataset from the 2012 election
    to the 2016 or 2020 elections. Unfortunately, the more recent datasets provided
    by the FEC have become larger and more complex, and I decided that working with
    them here would be a distraction from the analysis techniques that I wanted to
    illustrate.*  *A sample record in the DataFrame looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE90]'
  prefs: []
  type: TYPE_PRE
- en: You may think of some ways to start slicing and dicing this data to extract
    informative statistics about donors and patterns in the campaign contributions.
    I’ll show you a number of different analyses that apply the techniques in this
    book.
  prefs: []
  type: TYPE_NORMAL
- en: 'You can see that there are no political party affiliations in the data, so
    this would be useful to add. You can get a list of all the unique political candidates
    using `unique`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE91]'
  prefs: []
  type: TYPE_PRE
- en: One way to indicate party affiliation is using a dictionary:[¹](#fn1)
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE92]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, using this mapping and the `map` method on Series objects, you can compute
    an array of political parties from the candidate names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE93]'
  prefs: []
  type: TYPE_PRE
- en: 'A couple of data preparation points. First, this data includes both contributions
    and refunds (negative contribution amount):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE94]'
  prefs: []
  type: TYPE_PRE
- en: 'To simplify the analysis, I’ll restrict the dataset to positive contributions:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE95]'
  prefs: []
  type: TYPE_PRE
- en: 'Since Barack Obama and Mitt Romney were the main two candidates, I’ll also
    prepare a subset that just has contributions to their campaigns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE96]'
  prefs: []
  type: TYPE_PRE
- en: Donation Statistics by Occupation and Employer
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Donations by occupation is another oft-studied statistic. For example, attorneys
    tend to donate more money to Democrats, while business executives tend to donate
    more to Republicans. You have no reason to believe me; you can see for yourself
    in the data. First, the total number of donations by occupation can be computed
    with `value_counts`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE97]'
  prefs: []
  type: TYPE_PRE
- en: 'You will notice by looking at the occupations that many refer to the same basic
    job type, or there are several variants of the same thing. The following code
    snippet illustrates a technique for cleaning up a few of them by mapping from
    one occupation to another; note the “trick” of using `dict.get` to allow occupations
    with no mapping to “pass through”:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE98]'
  prefs: []
  type: TYPE_PRE
- en: 'I’ll also do the same thing for employers:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE99]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, you can use `pivot_table` to aggregate the data by party and occupation,
    then filter down to the subset that donated at least $2 million overall:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE100]'
  prefs: []
  type: TYPE_PRE
- en: 'It can be easier to look at this data graphically as a bar plot (`"barh"` means
    horizontal bar plot; see [Total donations by party for top occupations](#groupby_fec_occ_party)):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE101]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/f4e8c3c83f755658e00aaadb7414d336.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.12: Total donations by party for top occupations'
  prefs: []
  type: TYPE_NORMAL
- en: 'You might be interested in the top donor occupations or top companies that
    donated to Obama and Romney. To do this, you can group by candidate name and use
    a variant of the `top` method from earlier in the chapter:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE102]'
  prefs: []
  type: TYPE_PRE
- en: 'Then aggregate by occupation and employer:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE103]'
  prefs: []
  type: TYPE_PRE
- en: Bucketing Donation Amounts
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'A useful way to analyze this data is to use the `cut` function to discretize
    the contributor amounts into buckets by contribution size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE104]'
  prefs: []
  type: TYPE_PRE
- en: 'We can then group the data for Obama and Romney by name and bin label to get
    a histogram by donation size:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE105]'
  prefs: []
  type: TYPE_PRE
- en: 'This data shows that Obama received a significantly larger number of small
    donations than Romney. You can also sum the contribution amounts and normalize
    within buckets to visualize the percentage of total donations of each size by
    candidate ([Percentage of total donations received by candidates for each donation
    size](#fig_groupby_fec_bucket) shows the resulting plot):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE106]'
  prefs: []
  type: TYPE_PRE
- en: '![](../Images/e84a484574c968c98c4258588ab07435.png)'
  prefs: []
  type: TYPE_IMG
- en: 'Figure 13.13: Percentage of total donations received by candidates for each
    donation size'
  prefs: []
  type: TYPE_NORMAL
- en: I excluded the two largest bins, as these are not donations by individuals.
  prefs: []
  type: TYPE_NORMAL
- en: This analysis can be refined and improved in many ways. For example, you could
    aggregate donations by donor name and zip code to adjust for donors who gave many
    small amounts versus one or more large donations. I encourage you to explore the
    dataset yourself.
  prefs: []
  type: TYPE_NORMAL
- en: Donation Statistics by State
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'We can start by aggregating the data by candidate and state:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE107]'
  prefs: []
  type: TYPE_PRE
- en: 'If you divide each row by the total contribution amount, you get the relative
    percentage of total donations by state for each candidate:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE108]*  *## 13.6 Conclusion'
  prefs: []
  type: TYPE_NORMAL
- en: We've reached the end of this book. I have included some additional content
    you may find useful in the appendixes.
  prefs: []
  type: TYPE_NORMAL
- en: In the 10 years since the first edition of this book was published, Python has
    become a popular and widespread language for data analysis. The programming skills
    you have developed here will stay relevant for a long time into the future. I
    hope the programming tools and libraries we've explored will serve you well.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: This makes the simplifying assumption that Gary Johnson is a Republican even
    though he later became the Libertarian party candidate.[↩︎](#fnref1)**
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
