["```py\nfrom sympy import *\n\nx,y = symbols('x y')\n\nz = x**2 / sqrt(2*y**3 - 1)\n\nprint(latex(z))\n\n# prints\n# \\frac{x^{2}}{\\sqrt{2 y^{3} - 1}}\n```", "```py\nimport webbrowser\nfrom sympy import *\n\nx,y = symbols('x y')\n\nz = x**2 / sqrt(2*y**3 - 1)\n\nwebbrowser.open(\"https://latex.codecogs.com/png.image?\\dpi{200}\" + latex(z))\n```", "```py\n# Factorials multiply consecutive descending integers down to 1\n# EXAMPLE: 5! = 5 * 4 * 3 * 2 * 1\ndef factorial(n: int):\n    f = 1\n    for i in range(n):\n        f *= (i + 1)\n    return f\n\n# Generates the coefficient needed for the binomial distribution\ndef binomial_coefficient(n: int, k: int):\n    return factorial(n) / (factorial(k) * factorial(n - k))\n\n# Binomial distribution calculates the probability of k events out of n trials\n# given the p probability of k occurring\ndef binomial_distribution(k: int, n: int, p: float):\n    return binomial_coefficient(n, k) * (p ** k) * (1.0 - p) ** (n - k)\n\n# 10 trials where each has 90% success probability\nn = 10\np = 0.9\n\nfor k in range(n + 1):\n    probability = binomial_distribution(k, n, p)\n    print(\"{0} - {1}\".format(k, probability))\n```", "```py\n# Factorials multiply consecutive descending integers down to 1\n# EXAMPLE: 5! = 5 * 4 * 3 * 2 * 1\ndef factorial(n: int):\n    f = 1\n    for i in range(n):\n        f *= (i + 1)\n    return f\n\ndef approximate_integral(a, b, n, f):\n    delta_x = (b - a) / n\n    total_sum = 0\n\n    for i in range(1, n + 1):\n        midpoint = 0.5 * (2 * a + delta_x * (2 * i - 1))\n        total_sum += f(midpoint)\n\n    return total_sum * delta_x\n\ndef beta_distribution(x: float, alpha: float, beta: float) -> float:\n    if x < 0.0 or x > 1.0:\n        raise ValueError(\"x must be between 0.0 and 1.0\")\n\n    numerator = x ** (alpha - 1.0) * (1.0 - x) ** (beta - 1.0)\n    denominator = (1.0 * factorial(alpha - 1) * factorial(beta - 1)) / \\\n\t    (1.0 * factorial(alpha + beta - 1))\n\n    return numerator / denominator\n\ngreater_than_90 = approximate_integral(a=.90, b=1.0, n=1000,\n    f=lambda x: beta_distribution(x, 8, 2))\nless_than_90 = 1.0 - greater_than_90\n\nprint(\"GREATER THAN 90%: {}, LESS THAN 90%: {}\".format(greater_than_90,\n    less_than_90))\n```", "```py\nimport math\n\ndef normal_pdf(x: float, mean: float, std_dev: float) -> float:\n    return (1.0 / (2.0 * math.pi * std_dev ** 2) ** 0.5) *\n      math.exp(-1.0 * ((x - mean) ** 2 / (2.0 * std_dev ** 2)))\n\ndef approximate_integral(a, b, n, f):\n    delta_x = (b - a) / n\n    total_sum = 0\n\n    for i in range(1, n + 1):\n        midpoint = 0.5 * (2 * a + delta_x * (2 * i - 1))\n        total_sum += f(midpoint)\n\n    return total_sum * delta_x\n\np_between_61_and_62 = approximate_integral(a=61, b=62, n=7,\n  f= lambda x: normal_pdf(x,64.43,2.99))\n\nprint(p_between_61_and_62) # 0.0825344984983386\n```", "```py\nimport math\n\ndef normal_cdf(x: float, mean: float, std_dev: float) -> float:\n    return (1 + math.erf((x - mean) / math.sqrt(2) / std_dev)) / 2\n\nmean = 64.43\nstd_dev = 2.99\n\nx = normal_cdf(66, mean, std_dev) - normal_cdf(62, mean, std_dev)\n\nprint(x)  # prints 0.49204501470628936\n```", "```py\nimport random\nfrom scipy.special import erfinv\n\ndef inv_normal_cdf(p: float, mean: float, std_dev: float):\n    return mean + (std_dev * (2.0 ** 0.5) * erfinv((2.0 * p) - 1.0))\n\nmean = 64.43\nstd_dev = 2.99\n\nfor i in range(0,1000):\n    random_p = random.uniform(0.0, 1.0)\n    print(inv_normal_cdf(random_p, mean, std_dev))\n```", "```py\nfrom math import exp\n\n# Probability of leak in one year\np_leak = .05\n\n# number of years\nt = 5\n\n# Probability of leak within five years\n# 0.22119921692859512\np_leak_5_years = 1.0 - exp(-p_leak * t)\n\nprint(\"PROBABILITY OF LEAK WITHIN 5 YEARS: {}\".format(p_leak_5_years))\n```", "```py\nfrom numpy.random import normal\nimport pandas as pd\n\n# Import points from CSV\npoints = [p for p in pd.read_csv(\"https://bit.ly/2KF29Bd\").itertuples()]\n\n# Building the model\nm = 0.0\nb = 0.0\n\n# The number of iterations to perform\niterations = 150000\n\n# Number of points\nn = float(len(points))\n\n# Initialize with a really large loss\n# that we know will get replaced\nbest_loss = 10000000000000.0\n\nfor i in range(iterations):\n\n    # Randomly adjust \"m\" and \"b\"\n    m_adjust = normal(0,1)\n    b_adjust = normal(0,1)\n\n    m += m_adjust\n    b += b_adjust\n\n    # Calculate loss, which is total sum squared error\n    new_loss = 0.0\n    for p in points:\n        new_loss += (p.y - (m * p.x + b)) ** 2\n\n    # If loss has improved, keep new values. Otherwise revert.\n    if new_loss < best_loss:\n        print(\"y = {0}x + {1}\".format(m, b))\n        best_loss = new_loss\n    else:\n        m -= m_adjust\n        b -= b_adjust\n\nprint(\"y = {0}x + {1}\".format(m, b))\n```", "```py\nimport math\nimport random\n\nimport numpy as np\nimport pandas as pd\n\n# Desmos graph: https://www.desmos.com/calculator/6cb10atg3l\n\npoints = [p for p in pd.read_csv(\"https://tinyurl.com/y2cocoo7\").itertuples()]\n\nbest_likelihood = -10_000_000\nb0 = .01\nb1 = .01\n\n# calculate maximum likelihood\n\ndef predict_probability(x):\n    p = 1.0 / (1.0001 + math.exp(-(b0 + b1 * x)))\n    return p\n\nfor i in range(1_000_000):\n\n    # Select b0 or b1 randomly, and adjust it randomly\n    random_b = random.choice(range(2))\n\n    random_adjust = np.random.normal()\n\n    if random_b == 0:\n        b0 += random_adjust\n    elif random_b == 1:\n        b1 += random_adjust\n\n    # Calculate total likelihood\n    true_estimates = sum(math.log(predict_probability(p.x)) \\\n       \tfor p in points if p.y == 1.0)\n    false_estimates = sum(math.log(1.0 - predict_probability(p.x)) \\\n        for p in points if p.y == 0.0)\n\n    total_likelihood = true_estimates + false_estimates\n\n    # If likelihood improves, keep the random adjustment. Otherwise revert.\n    if best_likelihood < total_likelihood:\n        best_likelihood = total_likelihood\n    elif random_b == 0:\n        b0 -= random_adjust\n    elif random_b == 1:\n        b1 -= random_adjust\n\nprint(\"1.0 / (1 + exp(-({0} + {1}*x))\".format(b0, b1))\nprint(\"BEST LIKELIHOOD: {0}\".format(math.exp(best_likelihood)))\n```", "```py\n# GRAPH\" https://www.desmos.com/calculator/iildqi2vt7\n\nfrom pulp import *\n\n# declare your variables\nx = LpVariable(\"x\", 0)   # 0<=x\ny = LpVariable(\"y\", 0) # 0<=y\n\n# defines the problem\nprob = LpProblem(\"factory_problem\", LpMaximize)\n\n# defines the constraints\nprob += x + 3*y <= 20\nprob += 6*x +2*y <= 45\n\n# defines the objective function to maximize\nprob += 200*x + 300*y\n\n# solve the problem\nstatus = prob.solve()\nprint(LpStatus[status])\n\n# print the results x = 5.9375, y = 4.6875\nprint(value(x))\nprint(value(y))\n```", "```py\n# declare your variables\nx = LpVariable(\"x\", 0, cat=LpInteger) # 0<=x\ny = LpVariable(\"y\", 0, cat=LpInteger) # 0<=y\n```", "```py\nimport numpy as np\nimport pandas as pd\n# load data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\ndf = pd.read_csv('https://bit.ly/3ilJc2C', compression='zip', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\n# Note we should do some linear scaling here\nX = (df.values[:, :-1] / 255.0)\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Get a count of each group to ensure samples are equitably balanced\nprint(df.groupby([\"class\"]).agg({\"class\" : [np.size]}))\n\n# Separate training and testing data\n# Note that I use the 'stratify' parameter to ensure\n# each class is proportionally represented in both sets\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y,\n    test_size=.33, random_state=10, stratify=Y)\n\nnn = MLPClassifier(solver='sgd',\n                   hidden_layer_sizes=(100, ),\n                   activation='logistic',\n                   max_iter=480,\n                   learning_rate_init=.1)\n\nnn.fit(X_train, Y_train)\n\nprint(\"Training set score: %f\" % nn.score(X_train, Y_train))\nprint(\"Test set score: %f\" % nn.score(X_test, Y_test))\n\n# Display heat map\nimport matplotlib.pyplot as plt\nfig, axes = plt.subplots(4, 4)\n\n# use global min / max to ensure all weights are shown on the same scale\nvmin, vmax = nn.coefs_[0].min(), nn.coefs_[0].max()\nfor coef, ax in zip(nn.coefs_[0].T, axes.ravel()):\n    ax.matshow(coef.reshape(28, 28), cmap=plt.cm.gray, vmin=.5 * vmin, vmax=.5 * vmax)\n    ax.set_xticks(())\n    ax.set_yticks(())\n\nplt.show()\n```"]