["```py\nfrom sympy import *\n\n# plot relu\nx = symbols('x')\nrelu = Max(0, x)\nplot(relu)\n```", "```py\nfrom sympy import *\n\n# plot logistic\nx = symbols('x')\nlogistic = 1 / (1 + exp(-x))\nplot(logistic)\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nall_data = pd.read_csv(\"https://tinyurl.com/y2qmhfsr\")\n\n# Extract the input columns, scale down by 255\nall_inputs = (all_data.iloc[:, 0:3].values / 255.0)\nall_outputs = all_data.iloc[:, -1].values\n\n# Split train and test data sets\nX_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs,\n    test_size=1/3)\nn = X_train.shape[0] # number of training records\n\n# Build neural network with weights and biases\n# with random initialization\nw_hidden = np.random.rand(3, 3)\nw_output = np.random.rand(1, 3)\n\nb_hidden = np.random.rand(3, 1)\nb_output = np.random.rand(1, 1)\n\n# Activation functions\nrelu = lambda x: np.maximum(x, 0)\nlogistic = lambda x: 1 / (1 + np.exp(-x))\n\n# Runs inputs through the neural network to get predicted outputs\ndef forward_prop(X):\n    Z1 = w_hidden @ X + b_hidden\n    A1 = relu(Z1)\n    Z2 = w_output @ A1 + b_output\n    A2 = logistic(Z2)\n    return Z1, A1, Z2, A2\n\n# Calculate accuracy\ntest_predictions = forward_prop(X_test.transpose())[3] # grab only output layer, A2\ntest_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\naccuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\nprint(\"ACCURACY: \", accuracy)\n```", "```py\n# Build neural network with weights and biases\n# with random initialization\nw_hidden = np.random.rand(3, 3)\nw_output = np.random.rand(1, 3)\n\nb_hidden = np.random.rand(3, 1)\nb_output = np.random.rand(1, 1)\n```", "```py\n# Activation functions\nrelu = lambda x: np.maximum(x, 0)\nlogistic = lambda x: 1 / (1 + np.exp(-x))\n\n# Runs inputs through the neural network to get predicted outputs\ndef forward_prop(X):\n    Z1 = w_hidden @ X + b_hidden\n    A1 = relu(Z1)\n    Z2 = w_output @ A1 + b_output\n    A2 = logistic(Z2)\n    return Z1, A1, Z2, A2\n```", "```py\n# Calculate accuracy\ntest_predictions = forward_prop(X_test.transpose())[3]  # grab only A2\ntest_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\naccuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\nprint(\"ACCURACY: \", accuracy)\n```", "```py\nfrom sympy import *\n\nA2, y = symbols('A2 Y')\nC = (A2 - Y)**2\ndC_dA2 = diff(C, A2)\nprint(dC_dA2) # 2*A2 - 2*Y\n```", "```py\nfrom sympy import *\n\nZ2 = symbols('Z2')\n\nlogistic = lambda x: 1 / (1 + exp(-x))\n\nA2 = logistic(Z2)\ndA2_dZ2 = diff(A2, Z2)\nprint(dA2_dZ2) # exp(-Z2)/(1 + exp(-Z2))**2\n```", "```py\nfrom sympy import *\n\nA1, W2, B2 = symbols('A1, W2, B2')\n\nZ2 = A1*W2 + B2\ndZ2_dW2 = diff(Z2, W2)\nprint(dZ2_dW2) # A1\n```", "```py\nfrom sympy import *\n\nW1, W2, B1, B2, A1, A2, Z1, Z2, X, Y = \\\n    symbols('W1 W2 B1 B2 A1 A2 Z1 Z2 X Y')\n\n# Calculate derivative of cost function with respect to A2\nC = (A2 - Y)**2\ndC_dA2 = diff(C, A2)\nprint(\"dC_dA2 = \", dC_dA2) # 2*A2 - 2*Y\n\n# Calculate derivative of A2 with respect to Z2\nlogistic = lambda x: 1 / (1 + exp(-x))\n_A2 = logistic(Z2)\ndA2_dZ2 = diff(_A2, Z2)\nprint(\"dA2_dZ2 = \", dA2_dZ2) # exp(-Z2)/(1 + exp(-Z2))**2\n\n# Calculate derivative of Z2 with respect to A1\n_Z2 = A1*W2 + B2\ndZ2_dA1 = diff(_Z2, A1)\nprint(\"dZ2_dA1 = \", dZ2_dA1) # W2\n\n# Calculate derivative of Z2 with respect to W2\ndZ2_dW2 = diff(_Z2, W2)\nprint(\"dZ2_dW2 = \", dZ2_dW2) # A1\n\n# Calculate derivative of Z2 with respect to B2\ndZ2_dB2 = diff(_Z2, B2)\nprint(\"dZ2_dB2 = \", dZ2_dB2) # 1\n\n# Calculate derivative of A1 with respect to Z1\nrelu = lambda x: Max(x, 0)\n_A1 = relu(Z1)\n\nd_relu = lambda x: x > 0 # Slope is 1 if positive, 0 if negative\ndA1_dZ1 = d_relu(Z1)\nprint(\"dA1_dZ1 = \", dA1_dZ1) # Z1 > 0\n\n# Calculate derivative of Z1 with respect to W1\n_Z1 = X*W1 + B1\ndZ1_dW1 = diff(_Z1, W1)\nprint(\"dZ1_dW1 = \", dZ1_dW1) # X\n\n# Calculate derivative of Z1 with respect to B1\ndZ1_dB1 = diff(_Z1, B1)\nprint(\"dZ1_dB1 = \", dZ1_dB1) # 1\n```", "```py\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\n\nall_data = pd.read_csv(\"https://tinyurl.com/y2qmhfsr\")\n\n# Learning rate controls how slowly we approach a solution\n# Make it too small, it will take too long to run.\n# Make it too big, it will likely overshoot and miss the solution.\nL = 0.05\n\n# Extract the input columns, scale down by 255\nall_inputs = (all_data.iloc[:, 0:3].values / 255.0)\nall_outputs = all_data.iloc[:, -1].values\n\n# Split train and test data sets\nX_train, X_test, Y_train, Y_test = train_test_split(all_inputs, all_outputs,\n    test_size=1 / 3)\nn = X_train.shape[0]\n\n# Build neural network with weights and biases\n# with random initialization\nw_hidden = np.random.rand(3, 3)\nw_output = np.random.rand(1, 3)\n\nb_hidden = np.random.rand(3, 1)\nb_output = np.random.rand(1, 1)\n\n# Activation functions\nrelu = lambda x: np.maximum(x, 0)\nlogistic = lambda x: 1 / (1 + np.exp(-x))\n\n# Runs inputs through the neural network to get predicted outputs\ndef forward_prop(X):\n    Z1 = w_hidden @ X + b_hidden\n    A1 = relu(Z1)\n    Z2 = w_output @ A1 + b_output\n    A2 = logistic(Z2)\n    return Z1, A1, Z2, A2\n\n# Derivatives of Activation functions\nd_relu = lambda x: x > 0\nd_logistic = lambda x: np.exp(-x) / (1 + np.exp(-x)) ** 2\n\n# returns slopes for weights and biases\n# using chain rule\ndef backward_prop(Z1, A1, Z2, A2, X, Y):\n    dC_dA2 = 2 * A2 - 2 * Y\n    dA2_dZ2 = d_logistic(Z2)\n    dZ2_dA1 = w_output\n    dZ2_dW2 = A1\n    dZ2_dB2 = 1\n    dA1_dZ1 = d_relu(Z1)\n    dZ1_dW1 = X\n    dZ1_dB1 = 1\n\n    dC_dW2 = dC_dA2 @ dA2_dZ2 @ dZ2_dW2.T\n\n    dC_dB2 = dC_dA2 @ dA2_dZ2 * dZ2_dB2\n\n    dC_dA1 = dC_dA2 @ dA2_dZ2 @ dZ2_dA1\n\n    dC_dW1 = dC_dA1 @ dA1_dZ1 @ dZ1_dW1.T\n\n    dC_dB1 = dC_dA1 @ dA1_dZ1 * dZ1_dB1\n\n    return dC_dW1, dC_dB1, dC_dW2, dC_dB2\n\n# Execute gradient descent\nfor i in range(100_000):\n    # randomly select one of the training data\n    idx = np.random.choice(n, 1, replace=False)\n    X_sample = X_train[idx].transpose()\n    Y_sample = Y_train[idx]\n\n    # run randomly selected training data through neural network\n    Z1, A1, Z2, A2 = forward_prop(X_sample)\n\n    # distribute error through backpropagation\n    # and return slopes for weights and biases\n    dW1, dB1, dW2, dB2 = backward_prop(Z1, A1, Z2, A2, X_sample, Y_sample)\n\n    # update weights and biases\n    w_hidden -= L * dW1\n    b_hidden -= L * dB1\n    w_output -= L * dW2\n    b_output -= L * dB2\n\n# Calculate accuracy\ntest_predictions = forward_prop(X_test.transpose())[3]  # grab only A2\ntest_comparisons = np.equal((test_predictions >= .5).flatten().astype(int), Y_test)\naccuracy = sum(test_comparisons.astype(int) / X_test.shape[0])\nprint(\"ACCURACY: \", accuracy)\n```", "```py\n# Interact and test with new colors\ndef predict_probability(r, g, b):\n    X = np.array([[r, g, b]]).transpose() / 255\n    Z1, A1, Z2, A2 = forward_prop(X)\n    return A2\n\ndef predict_font_shade(r, g, b):\n    output_values = predict_probability(r, g, b)\n    if output_values > .5:\n        return \"DARK\"\n    else:\n        return \"LIGHT\"\n\nwhile True:\n    col_input = input(\"Predict light or dark font. Input values R,G,B: \")\n    (r, g, b) = col_input.split(\",\")\n    print(predict_font_shade(int(r), int(g), int(b)))\n```", "```py\nimport pandas as pd\n# load data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\ndf = pd.read_csv('https://bit.ly/3GsNzGt', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\n# Note we should do some linear scaling here\nX = (df.values[:, :-1] / 255.0)\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Separate training and testing data\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)\n\nnn = MLPClassifier(solver='sgd',\n                   hidden_layer_sizes=(3, ),\n                   activation='relu',\n                   max_iter=100_000,\n                   learning_rate_init=.05)\n\nnn.fit(X_train, Y_train)\n\n# Print weights and biases\nprint(nn.coefs_ )\nprint(nn.intercepts_)\n\nprint(\"Training set score: %f\" % nn.score(X_train, Y_train))\nprint(\"Test set score: %f\" % nn.score(X_test, Y_test))\n```"]