- en: 6  Data Loading, Storage, and File Formats
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
- en: 原文：[https://wesmckinney.com/book/accessing-data](https://wesmckinney.com/book/accessing-data)
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*This Open Access web version of *Python for Data Analysis 3rd Edition* is
    now available as a companion to the [print and digital editions](https://amzn.to/3DyLaJc).
    If you encounter any errata, [please report them here](https://oreilly.com/catalog/0636920519829/errata).
    Please note that some aspects of this site as produced by Quarto will differ from
    the formatting of the print and eBook versions from O’Reilly.'
  prefs: []
  type: TYPE_NORMAL
- en: If you find the online edition of the book useful, please consider [ordering
    a paper copy](https://amzn.to/3DyLaJc) or a [DRM-free eBook](https://www.ebooks.com/en-us/book/210644288/python-for-data-analysis/wes-mckinney/?affId=WES398681F)
    to support the author. The content from this website may not be copied or reproduced.
    The code examples are MIT licensed and can be found on GitHub or Gitee.*  *Reading
    data and making it accessible (often called *data loading*) is a necessary first
    step for using most of the tools in this book. The term *parsing* is also sometimes
    used to describe loading text data and interpreting it as tables and different
    data types. I’m going to focus on data input and output using pandas, though there
    are numerous tools in other libraries to help with reading and writing data in
    various formats.
  prefs: []
  type: TYPE_NORMAL
- en: 'Input and output typically fall into a few main categories: reading text files
    and other more efficient on-disk formats, loading data from databases, and interacting
    with network sources like web APIs.'
  prefs: []
  type: TYPE_NORMAL
- en: 6.1 Reading and Writing Data in Text Format
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: pandas features a number of functions for reading tabular data as a DataFrame
    object. [Table 6.1](#tbl-table_parsing_functions) summarizes some of them; `pandas.read_csv`
    is one of the most frequently used in this book. We will look at binary data formats
    later in [Binary Data Formats](#io_binary).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.1: Text and binary data loading functions in pandas'
  prefs: []
  type: TYPE_NORMAL
- en: '| Function | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `read_csv` | Load delimited data from a file, URL, or file-like object; use
    comma as default delimiter |'
  prefs: []
  type: TYPE_TB
- en: '| `read_fwf` | Read data in fixed-width column format (i.e., no delimiters)
    |'
  prefs: []
  type: TYPE_TB
- en: '| `read_clipboard` | Variation of `read_csv` that reads data from the clipboard;
    useful for converting tables from web pages |'
  prefs: []
  type: TYPE_TB
- en: '| `read_excel` | Read tabular data from an Excel XLS or XLSX file |'
  prefs: []
  type: TYPE_TB
- en: '| `read_hdf` | Read HDF5 files written by pandas |'
  prefs: []
  type: TYPE_TB
- en: '| `read_html` | Read all tables found in the given HTML document |'
  prefs: []
  type: TYPE_TB
- en: '| `read_json` | Read data from a JSON (JavaScript Object Notation) string representation,
    file, URL, or file-like object |'
  prefs: []
  type: TYPE_TB
- en: '| `read_feather` | Read the Feather binary file format |'
  prefs: []
  type: TYPE_TB
- en: '| `read_orc` | Read the Apache ORC binary file format |'
  prefs: []
  type: TYPE_TB
- en: '| `read_parquet` | Read the Apache Parquet binary file format |'
  prefs: []
  type: TYPE_TB
- en: '| `read_pickle` | Read an object stored by pandas using the Python pickle format
    |'
  prefs: []
  type: TYPE_TB
- en: '| `read_sas` | Read a SAS dataset stored in one of the SAS system''s custom
    storage formats |'
  prefs: []
  type: TYPE_TB
- en: '| `read_spss` | Read a data file created by SPSS |'
  prefs: []
  type: TYPE_TB
- en: '| `read_sql` | Read the results of a SQL query (using SQLAlchemy) |'
  prefs: []
  type: TYPE_TB
- en: '| `read_sql_table` | Read a whole SQL table (using SQLAlchemy); equivalent
    to using a query that selects everything in that table using `read_sql` |'
  prefs: []
  type: TYPE_TB
- en: '| `read_stata` | Read a dataset from Stata file format |'
  prefs: []
  type: TYPE_TB
- en: '| `read_xml` | Read a table of data from an XML file |'
  prefs: []
  type: TYPE_TB
- en: 'I’ll give an overview of the mechanics of these functions, which are meant
    to convert text data into a DataFrame. The optional arguments for these functions
    may fall into a few categories:'
  prefs: []
  type: TYPE_NORMAL
- en: Indexing
  prefs: []
  type: TYPE_NORMAL
- en: Can treat one or more columns as the returned DataFrame, and whether to get
    column names from the file, arguments you provide, or not at all.
  prefs: []
  type: TYPE_NORMAL
- en: Type inference and data conversion
  prefs: []
  type: TYPE_NORMAL
- en: Includes the user-defined value conversions and custom list of missing value
    markers.
  prefs: []
  type: TYPE_NORMAL
- en: Date and time parsing
  prefs: []
  type: TYPE_NORMAL
- en: Includes a combining capability, including combining date and time information
    spread over multiple columns into a single column in the result.
  prefs: []
  type: TYPE_NORMAL
- en: Iterating
  prefs: []
  type: TYPE_NORMAL
- en: Support for iterating over chunks of very large files.
  prefs: []
  type: TYPE_NORMAL
- en: Unclean data issues
  prefs: []
  type: TYPE_NORMAL
- en: Includes skipping rows or a footer, comments, or other minor things like numeric
    data with thousands separated by commas.
  prefs: []
  type: TYPE_NORMAL
- en: Because of how messy data in the real world can be, some of the data loading
    functions (especially `pandas.read_csv`) have accumulated a long list of optional
    arguments over time. It's normal to feel overwhelmed by the number of different
    parameters (`pandas.read_csv` has around 50). The online pandas documentation
    has many examples about how each of these works, so if you're struggling to read
    a particular file, there might be a similar enough example to help you find the
    right parameters.
  prefs: []
  type: TYPE_NORMAL
- en: Some of these functions perform *type inference*, because the column data types
    are not part of the data format. That means you don’t necessarily have to specify
    which columns are numeric, integer, Boolean, or string. Other data formats, like
    HDF5, ORC, and Parquet, have the data type information embedded in the format.
  prefs: []
  type: TYPE_NORMAL
- en: Handling dates and other custom types can require extra effort.
  prefs: []
  type: TYPE_NORMAL
- en: 'Let’s start with a small comma-separated values (CSV) text file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE0]'
  prefs: []
  type: TYPE_PRE
- en: '*Note* *Here I used the Unix `cat` shell command to print the raw contents
    of the file to the screen. If you’re on Windows, you can use `type` instead of
    `cat` to achieve the same effect within a Windows terminal (or command line).*  *Since
    this is comma-delimited, we can then use `pandas.read_csv` to read it into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE1]'
  prefs: []
  type: TYPE_PRE
- en: 'A file will not always have a header row. Consider this file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE2]'
  prefs: []
  type: TYPE_PRE
- en: 'To read this file, you have a couple of options. You can allow pandas to assign
    default column names, or you can specify names yourself:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE3]'
  prefs: []
  type: TYPE_PRE
- en: 'Suppose you wanted the `message` column to be the index of the returned DataFrame.
    You can either indicate you want the column at index 4 or named `"message"` using
    the `index_col` argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE4]'
  prefs: []
  type: TYPE_PRE
- en: 'If you want to form a hierarchical index (discussed in [Ch 8.1: Hierarchical
    Indexing](/book/data-wrangling#pandas_hierarchical)) from multiple columns, pass
    a list of column numbers or names:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE5]'
  prefs: []
  type: TYPE_PRE
- en: 'In some cases, a table might not have a fixed delimiter, using whitespace or
    some other pattern to separate fields. Consider a text file that looks like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE6]'
  prefs: []
  type: TYPE_PRE
- en: 'While you could do some munging by hand, the fields here are separated by a
    variable amount of whitespace. In these cases, you can pass a regular expression
    as a delimiter for `pandas.read_csv`. This can be expressed by the regular expression
    `\s+`, so we have then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE7]'
  prefs: []
  type: TYPE_PRE
- en: Because there was one fewer column name than the number of data rows, `pandas.read_csv`
    infers that the first column should be the DataFrame’s index in this special case.
  prefs: []
  type: TYPE_NORMAL
- en: 'The file parsing functions have many additional arguments to help you handle
    the wide variety of exception file formats that occur (see a partial listing in
    [Table 6.2](#tbl-table_read_csv_function)). For example, you can skip the first,
    third, and fourth rows of a file with `skiprows`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE8]'
  prefs: []
  type: TYPE_PRE
- en: 'Handling missing values is an important and frequently nuanced part of the
    file reading process. Missing data is usually either not present (empty string)
    or marked by some *sentinel* (placeholder) value. By default, pandas uses a set
    of commonly occurring sentinels, such as `NA` and `NULL`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE9]'
  prefs: []
  type: TYPE_PRE
- en: 'Recall that pandas outputs missing values as `NaN`, so we have two null or
    missing values in `result`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE10]'
  prefs: []
  type: TYPE_PRE
- en: 'The `na_values` option accepts a sequence of strings to add to the default
    list of strings recognized as missing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE11]'
  prefs: []
  type: TYPE_PRE
- en: '`pandas.read_csv` has a list of many default NA value representations, but
    these defaults can be disabled with the `keep_default_na` option:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE12]'
  prefs: []
  type: TYPE_PRE
- en: 'Different NA sentinels can be specified for each column in a dictionary:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE13]'
  prefs: []
  type: TYPE_PRE
- en: '[Table 6.2](#tbl-table_read_csv_function) lists some frequently used options
    in `pandas.read_csv`.'
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.2: Some `pandas.read_csv` function arguments'
  prefs: []
  type: TYPE_NORMAL
- en: '| Argument | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `path` | String indicating filesystem location, URL, or file-like object.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `sep` or `delimiter` | Character sequence or regular expression to use to
    split fields in each row. |'
  prefs: []
  type: TYPE_TB
- en: '| `header` | Row number to use as column names; defaults to 0 (first row),
    but should be `None` if there is no header row. |'
  prefs: []
  type: TYPE_TB
- en: '| `index_col` | Column numbers or names to use as the row index in the result;
    can be a single name/number or a list of them for a hierarchical index. |'
  prefs: []
  type: TYPE_TB
- en: '| `names` | List of column names for result. |'
  prefs: []
  type: TYPE_TB
- en: '| `skiprows` | Number of rows at beginning of file to ignore or list of row
    numbers (starting from 0) to skip. |'
  prefs: []
  type: TYPE_TB
- en: '| `na_values` | Sequence of values to replace with NA. They are added to the
    default list unless `keep_default_na=False` is passed. |'
  prefs: []
  type: TYPE_TB
- en: '| `keep_default_na` | Whether to use the default NA value list or not (`True`
    by default). |'
  prefs: []
  type: TYPE_TB
- en: '| `comment` | Character(s) to split comments off the end of lines. |'
  prefs: []
  type: TYPE_TB
- en: '| `parse_dates` | Attempt to parse data to `datetime`; `False` by default.
    If `True`, will attempt to parse all columns. Otherwise, can specify a list of
    column numbers or names to parse. If element of list is tuple or list, will combine
    multiple columns together and parse to date (e.g., if date/time split across two
    columns). |'
  prefs: []
  type: TYPE_TB
- en: '| `keep_date_col` | If joining columns to parse date, keep the joined columns;
    `False` by default. |'
  prefs: []
  type: TYPE_TB
- en: '| `converters` | Dictionary containing column number or name mapping to functions
    (e.g., `{"foo": f}` would apply the function `f` to all values in the `"foo"`
    column). |'
  prefs: []
  type: TYPE_TB
- en: '| `dayfirst` | When parsing potentially ambiguous dates, treat as international
    format (e.g., 7/6/2012 -> June 7, 2012); `False` by default. |'
  prefs: []
  type: TYPE_TB
- en: '| `date_parser` | Function to use to parse dates. |'
  prefs: []
  type: TYPE_TB
- en: '| `nrows` | Number of rows to read from beginning of file (not counting the
    header). |'
  prefs: []
  type: TYPE_TB
- en: '| `iterator` | Return a `TextFileReader` object for reading the file piecemeal.
    This object can also be used with the `with` statement. |'
  prefs: []
  type: TYPE_TB
- en: '| `chunksize` | For iteration, size of file chunks. |'
  prefs: []
  type: TYPE_TB
- en: '| `skip_footer` | Number of lines to ignore at end of file. |'
  prefs: []
  type: TYPE_TB
- en: '| `verbose` | Print various parsing information, like the time spent in each
    stage of the file conversion and memory use information. |'
  prefs: []
  type: TYPE_TB
- en: '| `encoding` | Text encoding (e.g., `"utf-8` for UTF-8 encoded text). Defaults
    to `"utf-8"` if `None`. |'
  prefs: []
  type: TYPE_TB
- en: '| `squeeze` | If the parsed data contains only one column, return a Series.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `thousands` | Separator for thousands (e.g., `","` or `"."`); default is
    `None`. |'
  prefs: []
  type: TYPE_TB
- en: '| `decimal` | Decimal separator in numbers (e.g., `"."` or `","`); default
    is `"."`. |'
  prefs: []
  type: TYPE_TB
- en: '| `engine` | CSV parsing and conversion engine to use; can be one of `"c"`,
    `"python"`, or `"pyarrow"`. The default is `"c"`, though the newer `"pyarrow"`
    engine can parse some files much faster. The `"python"` engine is slower but supports
    some features that the other engines do not. |'
  prefs: []
  type: TYPE_TB
- en: Reading Text Files in Pieces
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: When processing very large files or figuring out the right set of arguments
    to correctly process a large file, you may want to read only a small piece of
    a file or iterate through smaller chunks of the file.
  prefs: []
  type: TYPE_NORMAL
- en: 'Before we look at a large file, we make the pandas display settings more compact:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE14]'
  prefs: []
  type: TYPE_PRE
- en: 'Now we have:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE15]'
  prefs: []
  type: TYPE_PRE
- en: The elipsis marks `...` indicate that rows in the middle of the DataFrame have
    been omitted.
  prefs: []
  type: TYPE_NORMAL
- en: 'If you want to read only a small number of rows (avoiding reading the entire
    file), specify that with `nrows`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE16]'
  prefs: []
  type: TYPE_PRE
- en: 'To read a file in pieces, specify a `chunksize` as a number of rows:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE17]'
  prefs: []
  type: TYPE_PRE
- en: 'The `TextFileReader` object returned by `pandas.read_csv` allows you to iterate
    over the parts of the file according to the `chunksize`. For example, we can iterate
    over `ex6.csv`, aggregating the value counts in the `"key"` column, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE18]'
  prefs: []
  type: TYPE_PRE
- en: 'We have then:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE19]'
  prefs: []
  type: TYPE_PRE
- en: '`TextFileReader` is also equipped with a `get_chunk` method that enables you
    to read pieces of an arbitrary size.'
  prefs: []
  type: TYPE_NORMAL
- en: Writing Data to Text Format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'Data can also be exported to a delimited format. Let’s consider one of the
    CSV files read before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE20]'
  prefs: []
  type: TYPE_PRE
- en: 'Using DataFrame’s `to_csv` method, we can write the data out to a comma-separated
    file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE21]'
  prefs: []
  type: TYPE_PRE
- en: 'Other delimiters can be used, of course (writing to `sys.stdout` so it prints
    the text result to the console rather than a file):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE22]'
  prefs: []
  type: TYPE_PRE
- en: 'Missing values appear as empty strings in the output. You might want to denote
    them by some other sentinel value:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE23]'
  prefs: []
  type: TYPE_PRE
- en: 'With no other options specified, both the row and column labels are written.
    Both of these can be disabled:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE24]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also write only a subset of the columns, and in an order of your choosing:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE25]'
  prefs: []
  type: TYPE_PRE
- en: Working with Other Delimited Formats
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'It''s possible to load most forms of tabular data from disk using functions
    like `pandas.read_csv`. In some cases, however, some manual processing may be
    necessary. It’s not uncommon to receive a file with one or more malformed lines
    that trip up `pandas.read_csv`. To illustrate the basic tools, consider a small
    CSV file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE26]'
  prefs: []
  type: TYPE_PRE
- en: 'For any file with a single-character delimiter, you can use Python’s built-in
    `csv` module. To use it, pass any open file or file-like object to `csv.reader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE27]'
  prefs: []
  type: TYPE_PRE
- en: 'Iterating through the reader like a file yields lists of values with any quote
    characters removed:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE28]'
  prefs: []
  type: TYPE_PRE
- en: 'From there, it’s up to you to do the wrangling necessary to put the data in
    the form that you need. Let''s take this step by step. First, we read the file
    into a list of lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE29]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we split the lines into the header line and the data lines:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE30]'
  prefs: []
  type: TYPE_PRE
- en: 'Then we can create a dictionary of data columns using a dictionary comprehension
    and the expression `zip(*values)` (beware that this will use a lot of memory on
    large files), which transposes rows to columns:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE31]'
  prefs: []
  type: TYPE_PRE
- en: 'CSV files come in many different flavors. To define a new format with a different
    delimiter, string quoting convention, or line terminator, we could define a simple
    subclass of `csv.Dialect`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE32]'
  prefs: []
  type: TYPE_PRE
- en: '[PRE33]'
  prefs: []
  type: TYPE_PRE
- en: 'We could also give individual CSV dialect parameters as keywords to `csv.reader`
    without having to define a subclass:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE34]'
  prefs: []
  type: TYPE_PRE
- en: The possible options (attributes of `csv.Dialect`) and what they do can be found
    in [Table 6.3](#tbl-table_csv_dialect).
  prefs: []
  type: TYPE_NORMAL
- en: 'Table 6.3: CSV `dialect` options'
  prefs: []
  type: TYPE_NORMAL
- en: '| Argument | Description |'
  prefs: []
  type: TYPE_TB
- en: '| --- | --- |'
  prefs: []
  type: TYPE_TB
- en: '| `delimiter` | One-character string to separate fields; defaults to `","`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `lineterminator` | Line terminator for writing; defaults to `"\r\n"`. Reader
    ignores this and recognizes cross-platform line terminators. |'
  prefs: []
  type: TYPE_TB
- en: '| `quotechar` | Quote character for fields with special characters (like a
    delimiter); default is `''"''`. |'
  prefs: []
  type: TYPE_TB
- en: '| `quoting` | Quoting convention. Options include `csv.QUOTE_ALL` (quote all
    fields), `csv.QUOTE_MINIMAL` (only fields with special characters like the delimiter),
    `csv.QUOTE_NONNUMERIC`, and `csv.QUOTE_NONE` (no quoting). See Python’s documentation
    for full details. Defaults to `QUOTE_MINIMAL`. |'
  prefs: []
  type: TYPE_TB
- en: '| `skipinitialspace` | Ignore whitespace after each delimiter; default is `False`.
    |'
  prefs: []
  type: TYPE_TB
- en: '| `doublequote` | How to handle quoting character inside a field; if `True`,
    it is doubled (see online documentation for full detail and behavior). |'
  prefs: []
  type: TYPE_TB
- en: '| `escapechar` | String to escape the delimiter if `quoting` is set to `csv.QUOTE_NONE`;
    disabled by default. |'
  prefs: []
  type: TYPE_TB
- en: '*Note* *For files with more complicated or fixed multicharacter delimiters,
    you will not be able to use the `csv` module. In those cases, you’ll have to do
    the line splitting and other cleanup using the string’s `split` method or the
    regular expression method `re.split`. Thankfully, `pandas.read_csv` is capable
    of doing almost anything you need if you pass the necessary options, so you only
    rarely will have to parse files by hand.*  *To *write* delimited files manually,
    you can use `csv.writer`. It accepts an open, writable file object and the same
    dialect and format options as `csv.reader`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE35]*  *### JSON Data'
  prefs: []
  type: TYPE_NORMAL
- en: 'JSON (short for JavaScript Object Notation) has become one of the standard
    formats for sending data by HTTP request between web browsers and other applications.
    It is a much more free-form data format than a tabular text form like CSV. Here
    is an example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE36]'
  prefs: []
  type: TYPE_PRE
- en: 'JSON is very nearly valid Python code with the exception of its null value
    `null` and some other nuances (such as disallowing trailing commas at the end
    of lists). The basic types are objects (dictionaries), arrays (lists), strings,
    numbers, Booleans, and nulls. All of the keys in an object must be strings. There
    are several Python libraries for reading and writing JSON data. I’ll use `json`
    here, as it is built into the Python standard library. To convert a JSON string
    to Python form, use `json.loads`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE37]'
  prefs: []
  type: TYPE_PRE
- en: '`json.dumps`, on the other hand, converts a Python object back to JSON:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE38]'
  prefs: []
  type: TYPE_PRE
- en: 'How you convert a JSON object or list of objects to a DataFrame or some other
    data structure for analysis will be up to you. Conveniently, you can pass a list
    of dictionaries (which were previously JSON objects) to the DataFrame constructor
    and select a subset of the data fields:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE39]'
  prefs: []
  type: TYPE_PRE
- en: 'The `pandas.read_json` can automatically convert JSON datasets in specific
    arrangements into a Series or DataFrame. For example:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE40]'
  prefs: []
  type: TYPE_PRE
- en: 'The default options for `pandas.read_json` assume that each object in the JSON
    array is a row in the table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE41]'
  prefs: []
  type: TYPE_PRE
- en: 'For an extended example of reading and manipulating JSON data (including nested
    records), see the USDA food database example in [Ch 13: Data Analysis Examples](#data-analysis-examples).'
  prefs: []
  type: TYPE_NORMAL
- en: 'If you need to export data from pandas to JSON, one way is to use the `to_json`
    methods on Series and DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE42]'
  prefs: []
  type: TYPE_PRE
- en: 'XML and HTML: Web Scraping'
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: Python has many libraries for reading and writing data in the ubiquitous HTML
    and XML formats. Examples include lxml, Beautiful Soup, and html5lib. While lxml
    is comparatively much faster in general, the other libraries can better handle
    malformed HTML or XML files.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas has a built-in function, `pandas.read_html`, which uses all of these
    libraries to automatically parse tables out of HTML files as DataFrame objects.
    To show how this works, I downloaded an HTML file (used in the pandas documentation)
    from the US FDIC showing bank failures.[¹](#fn1) First, you must install some
    additional libraries used by `read_html`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE43]'
  prefs: []
  type: TYPE_PRE
- en: If you are not using conda, `pip install lxml` should also work.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas.read_html` function has a number of options, but by default it
    searches for and attempts to parse all tabular data contained within `<table>`
    tags. The result is a list of DataFrame objects:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE44]'
  prefs: []
  type: TYPE_PRE
- en: Because `failures` has many columns, pandas inserts a line break character `\`.
  prefs: []
  type: TYPE_NORMAL
- en: 'As you will learn in later chapters, from here we could proceed to do some
    data cleaning and analysis, like computing the number of bank failures by year:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE45]'
  prefs: []
  type: TYPE_PRE
- en: Parsing XML with lxml.objectify
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
- en: XML is another common structured data format supporting hierarchical, nested
    data with metadata. The book you are currently reading was actually created from
    a series of large XML documents.
  prefs: []
  type: TYPE_NORMAL
- en: Earlier, I showed the `pandas.read_html` function, which uses either lxml or
    Beautiful Soup under the hood to parse data from HTML. XML and HTML are structurally
    similar, but XML is more general. Here, I will show an example of how to use lxml
    to parse data from a more general XML format.
  prefs: []
  type: TYPE_NORMAL
- en: 'For many years, the New York Metropolitan Transportation Authority (MTA) published
    a number of data series about its bus and train services in XML format. Here we’ll
    look at the performance data, which is contained in a set of XML files. Each train
    or bus service has a different file (like *Performance_MNR.xml* for the Metro-North
    Railroad) containing monthly data as a series of XML records that look like this:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE46]'
  prefs: []
  type: TYPE_PRE
- en: 'Using `lxml.objectify`, we parse the file and get a reference to the root node
    of the XML file with `getroot`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE47]'
  prefs: []
  type: TYPE_PRE
- en: '`root.INDICATOR` returns a generator yielding each `<INDICATOR>` XML element.
    For each record, we can populate a dictionary of tag names (like `YTD_ACTUAL`)
    to data values (excluding a few tags) by running the following code:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE48]'
  prefs: []
  type: TYPE_PRE
- en: 'Lastly, convert this list of dictionaries into a DataFrame:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE49]'
  prefs: []
  type: TYPE_PRE
- en: 'pandas''s `pandas.read_xml` function turns this process into a one-line expression:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE50]'
  prefs: []
  type: TYPE_PRE
- en: For more complex XML documents, refer to the docstring for `pandas.read_xml`
    which describes how to do selections and filters to extract a particular table
    of interest.**  **## 6.2 Binary Data Formats
  prefs: []
  type: TYPE_NORMAL
- en: 'One simple way to store (or *serialize*) data in binary format is using Python’s
    built-in `pickle` module. pandas objects all have a `to_pickle` method that writes
    the data to disk in pickle format:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE51]'
  prefs: []
  type: TYPE_PRE
- en: 'Pickle files are in general readable only in Python. You can read any "pickled"
    object stored in a file by using the built-in `pickle` directly, or even more
    conveniently using `pandas.read_pickle`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE52]'
  prefs: []
  type: TYPE_PRE
- en: '*Caution* *`pickle` is recommended only as a short-term storage format. The
    problem is that it is hard to guarantee that the format will be stable over time;
    an object pickled today may not unpickle with a later version of a library. pandas
    has tried to maintain backward compatibility when possible, but at some point
    in the future it may be necessary to “break” the pickle format.*  *pandas has
    built-in support for several other open source binary data formats, such as HDF5,
    ORC, and Apache Parquet. For example, if you install the `pyarrow` package (`conda
    install pyarrow`), then you can read Parquet files with `pandas.read_parquet`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE53]'
  prefs: []
  type: TYPE_PRE
- en: I will give some HDF5 examples in [Using HDF5 Format](#io_hdf5). I encourage
    you to explore different file formats to see how fast they are and how well they
    work for your analysis.
  prefs: []
  type: TYPE_NORMAL
- en: Reading Microsoft Excel Files
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: 'pandas also supports reading tabular data stored in Excel 2003 (and higher)
    files using either the `pandas.ExcelFile` class or `pandas.read_excel` function.
    Internally, these tools use the add-on packages `xlrd` and `openpyxl` to read
    old-style XLS and newer XLSX files, respectively. These must be installed separately
    from pandas using pip or conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE54]'
  prefs: []
  type: TYPE_PRE
- en: 'To use `pandas.ExcelFile`, create an instance by passing a path to an `xls`
    or `xlsx` file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE55]'
  prefs: []
  type: TYPE_PRE
- en: 'This object can show you the list of available sheet names in the file:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE56]'
  prefs: []
  type: TYPE_PRE
- en: 'Data stored in a sheet can then be read into DataFrame with `parse`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE57]'
  prefs: []
  type: TYPE_PRE
- en: 'This Excel table has an index column, so we can indicate that with the `index_col`
    argument:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE58]'
  prefs: []
  type: TYPE_PRE
- en: 'If you are reading multiple sheets in a file, then it is faster to create the
    `pandas.ExcelFile`, but you can also simply pass the filename to `pandas.read_excel`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE59]'
  prefs: []
  type: TYPE_PRE
- en: 'To write pandas data to Excel format, you must first create an `ExcelWriter`,
    then write data to it using the pandas object''s `to_excel` method:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE60]'
  prefs: []
  type: TYPE_PRE
- en: 'You can also pass a file path to `to_excel` and avoid the `ExcelWriter`:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE61]'
  prefs: []
  type: TYPE_PRE
- en: Using HDF5 Format
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
- en: HDF5 is a respected file format intended for storing large quantities of scientific
    array data. It is available as a C library, and it has interfaces available in
    many other languages, including Java, Julia, MATLAB, and Python. The “HDF” in
    HDF5 stands for *hierarchical data format*. Each HDF5 file can store multiple
    datasets and supporting metadata. Compared with simpler formats, HDF5 supports
    on-the-fly compression with a variety of compression modes, enabling data with
    repeated patterns to be stored more efficiently. HDF5 can be a good choice for
    working with datasets that don't fit into memory, as you can efficiently read
    and write small sections of much larger arrays.
  prefs: []
  type: TYPE_NORMAL
- en: 'To get started with HDF5 and pandas, you must first install PyTables by installing
    the `tables` package with conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE62]'
  prefs: []
  type: TYPE_PRE
- en: '*Note* *Note that the PyTables package is called "tables" in PyPI, so if you
    install with pip you will have to run `pip install tables`.*  *While it''s possible
    to directly access HDF5 files using either the PyTables or h5py libraries, pandas
    provides a high-level interface that simplifies storing Series and DataFrame objects.
    The `HDFStore` class works like a dictionary and handles the low-level details:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE63]'
  prefs: []
  type: TYPE_PRE
- en: 'Objects contained in the HDF5 file can then be retrieved with the same dictionary-like
    API:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE64]'
  prefs: []
  type: TYPE_PRE
- en: '`HDFStore` supports two storage schemas, `"fixed"` and `"table"` (the default
    is `"fixed"`). The latter is generally slower, but it supports query operations
    using a special syntax:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE65]'
  prefs: []
  type: TYPE_PRE
- en: The `put` is an explicit version of the `store["obj2"] = frame` method but allows
    us to set other options like the storage format.
  prefs: []
  type: TYPE_NORMAL
- en: 'The `pandas.read_hdf` function gives you a shortcut to these tools:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE66]'
  prefs: []
  type: TYPE_PRE
- en: 'If you''d like, you can delete the HDF5 file you created, like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE67]'
  prefs: []
  type: TYPE_PRE
- en: '*Note* *If you are processing data that is stored on remote servers, like Amazon
    S3 or HDFS, using a different binary format designed for distributed storage like
    [Apache Parquet](http://parquet.apache.org) may be more suitable.*  *If you work
    with large quantities of data locally, I would encourage you to explore PyTables
    and h5py to see how they can suit your needs. Since many data analysis problems
    are I/O-bound (rather than CPU-bound), using a tool like HDF5 can massively accelerate
    your applications.'
  prefs: []
  type: TYPE_NORMAL
- en: '*Caution* *HDF5 is *not* a database. It is best suited for write-once, read-many
    datasets. While data can be added to a file at any time, if multiple writers do
    so simultaneously, the file can become corrupted.****  ***## 6.3 Interacting with
    Web APIs'
  prefs: []
  type: TYPE_NORMAL
- en: 'Many websites have public APIs providing data feeds via JSON or some other
    format. There are a number of ways to access these APIs from Python; one method
    that I recommend is the [`requests` package](http://docs.python-requests.org),
    which can be installed with pip or conda:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE68]'
  prefs: []
  type: TYPE_PRE
- en: 'To find the last 30 GitHub issues for pandas on GitHub, we can make a `GET`
    HTTP request using the add-on `requests` library:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE69]'
  prefs: []
  type: TYPE_PRE
- en: It's a good practice to always call `raise_for_status` after using `requests.get`
    to check for HTTP errors.
  prefs: []
  type: TYPE_NORMAL
- en: 'The response object’s `json` method will return a Python object containing
    the parsed JSON data as a dictionary or list (depending on what JSON is returned):'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE70]'
  prefs: []
  type: TYPE_PRE
- en: Since the results retrieved are based on real-time data, what you see when you
    run this code will almost definitely be different.
  prefs: []
  type: TYPE_NORMAL
- en: 'Each element in `data` is a dictionary containing all of the data found on
    a GitHub issue page (except for the comments). We can pass `data` directly to
    `pandas.DataFrame` and extract fields of interest:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE71]'
  prefs: []
  type: TYPE_PRE
- en: With a bit of elbow grease, you can create some higher-level interfaces to common
    web APIs that return DataFrame objects for more convenient analysis.
  prefs: []
  type: TYPE_NORMAL
- en: 6.4 Interacting with Databases
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: In a business setting, a lot of data may not be stored in text or Excel files.
    SQL-based relational databases (such as SQL Server, PostgreSQL, and MySQL) are
    in wide use, and many alternative databases have become quite popular. The choice
    of database is usually dependent on the performance, data integrity, and scalability
    needs of an application.
  prefs: []
  type: TYPE_NORMAL
- en: 'pandas has some functions to simplify loading the results of a SQL query into
    a DataFrame. As an example, I’ll create a SQLite3 database using Python’s built-in
    `sqlite3` driver:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE72]'
  prefs: []
  type: TYPE_PRE
- en: 'Then, insert a few rows of data:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE73]'
  prefs: []
  type: TYPE_PRE
- en: 'Most Python SQL drivers return a list of tuples when selecting data from a
    table:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE74]'
  prefs: []
  type: TYPE_PRE
- en: 'You can pass the list of tuples to the DataFrame constructor, but you also
    need the column names, contained in the cursor’s `description` attribute. Note
    that for SQLite3, the cursor `description` only provides column names (the other
    fields, which are part of Python''s Database API specification, are `None`), but
    for some other database drivers, more column information is provided:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE75]'
  prefs: []
  type: TYPE_PRE
- en: 'This is quite a bit of munging that you’d rather not repeat each time you query
    the database. The [SQLAlchemy project](http://www.sqlalchemy.org/) is a popular
    Python SQL toolkit that abstracts away many of the common differences between
    SQL databases. pandas has a `read_sql` function that enables you to read data
    easily from a general SQLAlchemy connection. You can install SQLAlchemy with conda
    like so:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE76]'
  prefs: []
  type: TYPE_PRE
- en: 'Now, we''ll connect to the same SQLite database with SQLAlchemy and read data
    from the table created before:'
  prefs: []
  type: TYPE_NORMAL
- en: '[PRE77]'
  prefs: []
  type: TYPE_PRE
- en: 6.5 Conclusion
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
- en: Getting access to data is frequently the first step in the data analysis process.
    We have looked at a number of useful tools in this chapter that should help you
    get started. In the upcoming chapters we will dig deeper into data wrangling,
    data visualization, time series analysis, and other topics.
  prefs: []
  type: TYPE_NORMAL
- en: '* * *'
  prefs: []
  type: TYPE_NORMAL
- en: For the full list, see [https://www.fdic.gov/bank/individual/failed/banklist.html](https://www.fdic.gov/bank/individual/failed/banklist.html).[↩︎](#fnref1)******
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
