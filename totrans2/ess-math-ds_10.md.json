["```py\n    from math import exp\n\n    p = 1000\n    r = .05\n    t = 3\n    n = 12\n\n    a = p * (1 + (r/n))**(n * t)\n\n    print(a) # prints 1161.4722313334678\n    ```", "```py\n    from math import exp\n\n    p = 1000 # principal, starting amount\n    r = .05 # interest rate, by year\n    t = 3.0 # time, number of years\n\n    a = p * exp(r*t)\n\n    print(a) # prints 1161.834242728283\n    ```", "```py\n    from sympy import *\n\n    # Declare 'x' to SymPy\n    x = symbols('x')\n\n    # Now just use Python syntax to declare function\n    f = 3*x**2 + 1\n\n    # Calculate the derivative of the function\n    dx_f = diff(f)\n    print(dx_f) # prints 6*x\n    print(dx_f.subs(x,3)) # 18\n    ```", "```py\n    from sympy import *\n\n    # Declare 'x' to SymPy\n    x = symbols('x')\n\n    # Now just use Python syntax to declare function\n    f = 3*x**2 + 1\n\n    # Calculate the integral of the function with respect to x\n    # for the area between x = 0 and 2\n    area = integrate(f, (x, 0, 2))\n\n    print(area) # prints 10\n    ```", "```py\n    from scipy.stats import binom\n\n    n = 137\n    p = .40\n\n    p_50_or_more_noshows = 0.0\n\n    for x in range(50,138):\n        p_50_or_more_noshows += binom.pmf(x, n, p)\n\n    print(p_50_or_more_noshows) # 0.822095588147425\n    ```", "```py\n    from scipy.stats import beta\n\n    heads = 8\n    tails = 2\n\n    p = 1.0 - beta.cdf(.5, heads, tails)\n\n    print(p) # 0.98046875\n    ```", "```py\n    from math import sqrt\n\n    sample = [1.78, 1.75, 1.72, 1.74, 1.77]\n\n    def mean(values):\n        return sum(values) /len(values)\n\n    def variance_sample(values):\n        mean = sum(values) / len(values)\n        var = sum((v - mean) ** 2 for v in values) / len(values)\n        return var\n\n    def std_dev_sample(values):\n        return sqrt(variance_sample(values))\n\n    mean = mean(sample)\n    std_dev = std_dev_sample(sample)\n\n    print(\"MEAN: \", mean) # 1.752\n    print(\"STD DEV: \", std_dev) # 0.02135415650406264\n    ```", "```py\n    from scipy.stats import norm\n\n    mean = 42\n    std_dev = 8\n\n    x = norm.cdf(30, mean, std_dev) - norm.cdf(20, mean, std_dev)\n\n    print(x) # 0.06382743803380352\n    ```", "```py\n    from math import sqrt\n    from scipy.stats import norm\n\n    def critical_z_value(p, mean=0.0, std=1.0):\n        norm_dist = norm(loc=mean, scale=std)\n        left_area = (1.0 - p) / 2.0\n        right_area = 1.0 - ((1.0 - p) / 2.0)\n        return norm_dist.ppf(left_area), norm_dist.ppf(right_area)\n\n    def ci_large_sample(p, sample_mean, sample_std, n):\n        # Sample size must be greater than 30\n\n        lower, upper = critical_z_value(p)\n        lower_ci = lower * (sample_std / sqrt(n))\n        upper_ci = upper * (sample_std / sqrt(n))\n\n        return sample_mean + lower_ci, sample_mean + upper_ci\n\n    print(ci_large_sample(p=.99, sample_mean=1.715588,\n        sample_std=0.029252, n=34))\n    # (1.7026658973748656, 1.7285101026251342)\n    ```", "```py\n    from scipy.stats import norm\n\n    mean = 10345\n    std_dev = 552\n\n    p1 = 1.0 - norm.cdf(11641, mean, std_dev)\n\n    # Take advantage of symmetry\n    p2 = p1\n\n    # P-value of both tails\n    # I could have also just multiplied by 2\n    p_value = p1 + p2\n\n    print(\"Two-tailed P-value\", p_value)\n    if p_value <= .05:\n        print(\"Passes two-tailed test\")\n    else:\n        print(\"Fails two-tailed test\")\n\n    # Two-tailed P-value 0.01888333596496139\n    # Passes two-tailed test\n    ```", "```py\n    from numpy import array\n\n    v = array([1,2])\n\n    i_hat = array([2, 0])\n    j_hat = array([0, 1.5])\n\n    # fix this line\n    basis = array([i_hat, j_hat])\n\n    # transform vector v into w\n    w = basis.dot(v)\n\n    print(w) # [2\\. 3.]\n    ```", "```py\n    from numpy import array\n\n    v = array([1,2])\n\n    i_hat = array([-2, 1])\n    j_hat = array([1, -2])\n\n    # fix this line\n    basis = array([i_hat, j_hat])\n\n    # transform vector v into w\n    w = basis.dot(v)\n\n    print(w) # [ 0, -3]\n    ```", "```py\n    import numpy as np\n    from numpy.linalg import det\n\n    i_hat = np.array([1, 0])\n    j_hat = np.array([2, 2])\n\n    basis = np.array([i_hat,j_hat]).transpose()\n\n    determinant = det(basis)\n\n    print(determinant) # 2.0\n    ```", "```py\n    from numpy import array\n    from numpy.linalg import inv\n\n    A = array([\n        [3, 1, 0],\n        [2, 4, 1],\n        [3, 1, 8]\n    ])\n\n    B = array([\n        54,\n        12,\n        6\n    ])\n\n    X = inv(A).dot(B)\n\n    print(X) # [19.8 -5.4 -6\\. ]\n    ```", "```py\n    from numpy.linalg import det\n    from numpy import array\n\n    i_hat = array([2, 6])\n    j_hat = array([1, 3])\n\n    basis = array([i_hat, j_hat]).transpose()\n    print(basis)\n\n    determinant = det(basis)\n\n    print(determinant) # -3.330669073875464e-16\n    ```", "```py\n    from sympy import *\n\n    basis = Matrix([\n        [2,1],\n        [6,3]\n    ])\n\n    determinant = det(basis)\n\n    print(determinant) # 0\n    ```", "```py\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.linear_model import LinearRegression\n\n    # Import points\n    df = pd.read_csv('https://bit.ly/3C8JzrM', delimiter=\",\")\n\n    # Extract input variables (all rows, all columns but last column)\n    X = df.values[:, :-1]\n\n    # Extract output column (all rows, last column)\n    Y = df.values[:, -1]\n\n    # Fit a line to the points\n    fit = LinearRegression().fit(X, Y)\n\n    # m = 1.75919315, b = 4.69359655\n    m = fit.coef_.flatten()\n    b = fit.intercept_.flatten()\n    print(\"m = {0}\".format(m))\n    print(\"b = {0}\".format(b))\n\n    # show in chart\n    plt.plot(X, Y, 'o') # scatterplot\n    plt.plot(X, m*X+b) # line\n    plt.show()\n    ```", "```py\n    import pandas as pd\n\n    # Read data into Pandas dataframe\n    df = pd.read_csv('https://bit.ly/3C8JzrM', delimiter=\",\")\n\n    # Print correlations between variables\n    correlations = df.corr(method='pearson')\n    print(correlations)\n\n    # OUTPUT:\n    #          x        y\n    # x  1.00000  0.92421\n    # y  0.92421  1.00000\n\n    # Test for statistical significance\n    from scipy.stats import t\n    from math import sqrt\n\n    # sample size\n    n = df.shape[0]\n    print(n)\n    lower_cv = t(n - 1).ppf(.025)\n    upper_cv = t(n - 1).ppf(.975)\n\n    # retrieve correlation coefficient\n    r = correlations[\"y\"][\"x\"]\n\n    # Perform the test\n    test_value = r / sqrt((1 - r ** 2) / (n - 2))\n\n    print(\"TEST VALUE: {}\".format(test_value))\n    print(\"CRITICAL RANGE: {}, {}\".format(lower_cv, upper_cv))\n\n    if test_value < lower_cv or test_value > upper_cv:\n        print(\"CORRELATION PROVEN, REJECT H0\")\n    else:\n        print(\"CORRELATION NOT PROVEN, FAILED TO REJECT H0 \")\n\n    # Calculate p-value\n    if test_value > 0:\n        p_value = 1.0 - t(n - 1).cdf(test_value)\n    else:\n        p_value = t(n - 1).cdf(test_value)\n\n    # Two-tailed, so multiply by 2\n    p_value = p_value * 2\n    print(\"P-VALUE: {}\".format(p_value))\n\n    \"\"\"\n    TEST VALUE: 23.835515323677328\n    CRITICAL RANGE: -1.9844674544266925, 1.984467454426692\n    CORRELATION PROVEN, REJECT H0\n    P-VALUE: 0.0 (extremely small)\n    \"\"\"\n    ```", "```py\n    import pandas as pd\n    from scipy.stats import t\n    from math import sqrt\n\n    # Load the data\n    points = list(pd.read_csv('https://bit.ly/3C8JzrM', delimiter=\",\") \\\n        .itertuples())\n\n    n = len(points)\n\n    # Linear Regression Line\n    m = 1.75919315\n    b = 4.69359655\n\n    # Calculate Prediction Interval for x = 50\n    x_0 = 50\n    x_mean = sum(p.x for p in points) / len(points)\n\n    t_value = t(n - 2).ppf(.975)\n\n    standard_error = sqrt(sum((p.y - (m * p.x + b)) ** 2 for p in points) / \\\n        (n - 2))\n\n    margin_of_error = t_value * standard_error * \\\n                      sqrt(1 + (1 / n) + (n * (x_0 - x_mean) ** 2) / \\\n                           (n * sum(p.x ** 2 for p in points) - \\\n    \tsum(p.x for p in points) ** 2))\n\n    predicted_y = m*x_0 + b\n\n    # Calculate prediction interval\n    print(predicted_y - margin_of_error, predicted_y + margin_of_error)\n    # 50.792086501055955 134.51442159894404\n    ```", "```py\n    import pandas as pd\n    from sklearn.linear_model import LinearRegression\n    from sklearn.model_selection import KFold, cross_val_score\n\n    df = pd.read_csv('https://bit.ly/3C8JzrM', delimiter=\",\")\n\n    # Extract input variables (all rows, all columns but last column)\n    X = df.values[:, :-1]\n\n    # Extract output column (all rows, last column)\\\n    Y = df.values[:, -1]\n\n    # Perform a simple linear regression\n    kfold = KFold(n_splits=3, random_state=7, shuffle=True)\n    model = LinearRegression()\n    results = cross_val_score(model, X, Y, cv=kfold)\n    print(results)\n    print(\"MSE: mean=%.3f (stdev-%.3f)\" % (results.mean(), results.std()))\n    \"\"\"\n    [0.86119665 0.78237719 0.85733887]\n    MSE: mean=0.834 (stdev-0.036)\n    \"\"\"\n    ```", "```py\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import confusion_matrix\n    from sklearn.model_selection import KFold, cross_val_score\n\n    # Load the data\n    df = pd.read_csv(\"https://bit.ly/3imidqa\", delimiter=\",\")\n\n    X = df.values[:, :-1]\n    Y = df.values[:, -1]\n\n    kfold = KFold(n_splits=3, shuffle=True)\n    model = LogisticRegression(penalty='none')\n    results = cross_val_score(model, X, Y, cv=kfold)\n\n    print(\"Accuracy Mean: %.3f (stdev=%.3f)\" % (results.mean(),\n    results.std()))\n    ```", "```py\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    from sklearn.metrics import confusion_matrix\n    from sklearn.model_selection import train_test_split\n\n    # Load the data\n    df = pd.read_csv(\"https://bit.ly/3imidqa\", delimiter=\",\")\n\n    # Extract input variables (all rows, all columns but last column)\n    X = df.values[:, :-1]\n\n    # Extract output column (all rows, last column)\\\n    Y = df.values[:, -1]\n\n    model = LogisticRegression(solver='liblinear')\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n\n    \"\"\"\n    The confusion matrix evaluates accuracy within each category.\n    [[truepositives falsenegatives]\n     [falsepositives truenegatives]]\n\n    The diagonal represents correct predictions,\n    so we want those to be higher\n    \"\"\"\n    matrix = confusion_matrix(y_true=Y_test, y_pred=prediction)\n    print(matrix)\n    ```", "```py\n    import pandas as pd\n    from sklearn.linear_model import LogisticRegression\n    import numpy as np\n    from sklearn.model_selection import train_test_split\n\n    # Load the data\n    df = pd.read_csv(\"https://bit.ly/3imidqa\", delimiter=\",\")\n\n    # Extract input variables (all rows, all columns but last column)\n    X = df.values[:, :-1]\n\n    # Extract output column (all rows, last column)\n    Y = df.values[:, -1]\n\n    model = LogisticRegression(solver='liblinear')\n\n    X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.33)\n    model.fit(X_train, Y_train)\n    prediction = model.predict(X_test)\n\n    # Test a prediction\n    while True:\n        n = input(\"Input a color {red},{green},{blue}: \")\n        (r, g, b) = n.split(\",\")\n        x = model.predict(np.array([[int(r), int(g), int(b)]]))\n        if model.predict(np.array([[int(r), int(g), int(b)]]))[0] == 0.0:\n            print(\"LIGHT\")\n        else:\n            print(\"DARK\")\n    ```", "```py\nimport pandas as pd\n# load data\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.neural_network import MLPClassifier\n\ndf = pd.read_csv('https://tinyurl.com/y6r7qjrp', delimiter=\",\")\n\n# Extract input variables (all rows, all columns but last column)\nX = df.values[:, :-1]\n\n# Extract output column (all rows, last column)\nY = df.values[:, -1]\n\n# Separate training and testing data\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=1/3)\n\nnn = MLPClassifier(solver='sgd',\n                   hidden_layer_sizes=(3, ),\n                   activation='relu',\n                   max_iter=100_000,\n                   learning_rate_init=.05)\n\nnn.fit(X_train, Y_train)\n\nprint(\"Training set score: %f\" % nn.score(X_train, Y_train))\nprint(\"Test set score: %f\" % nn.score(X_test, Y_test))\n\nprint(\"Confusion matrix:\")\nmatrix = confusion_matrix(y_true=Y_test, y_pred=nn.predict(X_test))\nprint(matrix)\n```"]