- en: '11 Faster PySpark: Understanding Spark’s query planning'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 更快的PySpark：理解Spark的查询规划
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How Spark uses CPU, RAM, and hard drive resources
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark如何使用CPU、RAM和硬盘资源
- en: Using memory resources better to speed up (or avoid slowing down) computations
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好地使用内存资源以加快（或避免减慢）计算
- en: Using the Spark UI to review useful information about your Spark installation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark UI来查看有关你的Spark安装的有用信息
- en: How Spark splits a job into stages and how to profile and monitor those stages
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark如何将作业拆分为阶段以及如何分析和监控这些阶段
- en: Classifying transformations into narrow and wide operations and how to reason
    about them
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将转换分类为窄操作和宽操作，以及如何对它们进行推理
- en: Using caching judiciously and avoiding unfortunate performance drop with improper
    caching
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适度使用缓存并避免因不当缓存而导致的性能下降
- en: 'Imagine the following scenario: you write a readable, well-thought-out PySpark
    program. When submitting your program to your Spark cluster, it runs. You wait.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象以下场景：你编写了一个易于阅读、经过深思熟虑的PySpark程序。当你将你的程序提交到你的Spark集群时，它开始运行。你等待。
- en: How can we peek under the hood and see the progression of our program? Troubleshoot
    which step is taking a lot of time? This chapter is about understanding how we
    can access information about our Spark instance, such as its configuration and
    layout (CPU, memory, etc.). We also follow the execution of a program from raw
    Python code to optimized Spark instructions. This knowledge will remove a lot
    of magic from your program; you’ll be in a position to know what’s happening at
    every stage of your PySpark job. If your program takes too long, this chapter
    will show you where (and how) to look for the relevant information.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何窥视内部并查看程序的进度？调试哪个步骤花费了很长时间？本章是关于理解我们如何访问有关我们的Spark实例的信息，例如其配置和布局（CPU、内存等）。我们还从原始Python代码到优化的Spark指令跟踪程序的执行。这些知识将消除你程序中的许多神秘；你将处于一个位置，可以知道PySpark作业的每个阶段发生了什么。如果你的程序运行时间过长，本章将向你展示在哪里（以及如何）查找相关信息。
- en: '11.1 Open sesame: Navigating the Spark UI to understand the environment'
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 打开 sesame：导航Spark UI以了解环境
- en: This section covers how Spark uses allocated computing and memory resources
    and how we can configure how many resources are assigned to Spark. With this,
    you will be able to configure your jobs to use more or fewer resources, depending
    on how complex they are.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了Spark如何使用分配的计算和内存资源，以及我们如何配置分配给Spark的资源数量。有了这个，你将能够根据任务的复杂程度配置你的作业使用更多或更少的资源。
- en: Stepping up from local Spark
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从本地Spark升级
- en: Thus far, I have kept the data set sizes manageable to avoid needing to leverage
    a distributed (paid) instance of Spark. When learning a new technology (or if
    you are reading this book and working through the code examples), having a cloud
    cluster up—even if it is small—means that you are paying for something that will
    sit idle most of the time. When learning, experimenting, or developing small proofs
    of concept, don’t hesitate to use Spark locally, testing on a distributed environment
    as you go. You’ll avoid stressing out about cloud costs while ensuring your code
    scales healthily.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我已保持数据集大小可控，以避免需要利用分布式（付费）的Spark实例。在学习新技术（或者如果你正在阅读这本书并运行代码示例），即使是一个小型的云集群——这也意味着你正在为大部分时间处于闲置状态的东西付费。在学习、实验或开发小型概念验证时，不要犹豫，使用本地Spark，在分布式环境中进行测试。这样你就可以避免对云成本感到焦虑，同时确保你的代码健康扩展。
- en: On the other hand, some of the material in the chapter relies on having Spark
    running on multiple machines. Because of this, if you are running Spark locally,
    your results will be different.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，本章的一些材料依赖于Spark在多台机器上运行。因此，如果你在本地运行Spark，你的结果将会有所不同。
- en: 'For this chapter, we go back to the word occurrences count example from chapters
    2 and 3\. To avoid some frantic page flipping, the code is reproduced in listing
    11.1\. Our program follows a pretty simple set of steps:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们回到第2章和第3章中的单词出现次数计数示例。为了避免一些疯狂的翻页，代码在列表11.1中重现。我们的程序遵循一套相当简单的步骤：
- en: We create a `SparkSession` object to access the data frame functionality of
    PySpark as well as to connect to our Spark instance.
  id: totrans-16
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个`SparkSession`对象来访问PySpark的数据帧功能以及连接到我们的Spark实例。
- en: We create a data frame containing all the text files (line by line) within the
    chosen directory, and we count the occurrence of each word.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们创建一个包含所选目录内所有文本文件（逐行）的数据帧，并计算每个单词的出现次数。
- en: We show the top 10 most frequent words.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们展示了最常出现的10个单词。
- en: Listing 11.1 Our end-to-end program counting the occurrence of words
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.1 我们的端到端程序统计单词出现次数
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Like with any modern PySpark program, ours starts with creating a SparkSession
    and connecting to our Spark instance.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 就像任何现代 PySpark 程序一样，我们的程序从创建 SparkSession 并连接到我们的 Spark 实例开始。
- en: ❷ results maps to a data frame from a data source plus a series of transformations.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 结果映射到从数据源加一系列转换得到的数据帧。
- en: ❸ By show()-ing the results, we trigger the chain of transformations and display
    the top 10 most frequent words.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过显示结果，我们触发了转换链，并显示了出现频率最高的前 10 个单词。
- en: 'The work starts happening at the `show()` method: since it is an action, it
    triggers the chain of transformations from the `results` variable. I do not print
    the results of our program, because (a) we know it works and (b) we focus on what
    Spark is actually doing under the hood.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 工作从 `show()` 方法开始：由于它是一个动作，它触发了从 `results` 变量开始的转换链。我没有打印我们程序的结果，因为（a）我们知道它工作得很好，并且（b）我们专注于
    Spark 实际在底层做了什么。
- en: When starting Spark, either locally or on a cluster, the program allocates compute
    and memory resources for us to use. Those resources are displayed through a web
    portal called the *Spark UI*. For the UI to be available, we need to create and
    instantiate a `SparkSession`, which we did at the beginning of our PySpark program.
    When working locally, go to `localhost:4040` to check the Spark UI landing page.
    If the 4040 port is currently in use, Spark will spit out a `WARN` `Utils:` `Service`
    `'SparkUI'` `could` `not` `bind` `on` `port` `4040.` `Attempting` `port` `ABCD`
    message. Replace the 4040 after the colon with the port number listed. If you
    are working on a managed Spark cluster, cloud, or on premise, refer to your provider
    documentation for accessing the SparkUI.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 当启动 Spark，无论是本地还是集群上，程序都会为我们分配计算和内存资源。这些资源通过一个名为 *Spark UI* 的网页门户显示。为了使 UI 可用，我们需要创建并实例化一个
    `SparkSession`，我们在 PySpark 程序的开始就做了这件事。当本地工作时，请访问 `localhost:4040` 以检查 Spark UI
    首页。如果 4040 端口当前正在使用中，Spark 将输出一个 `WARN` `Utils:` `Service` `'SparkUI'` `could`
    `not` `bind` `on` `port` `4040.` `Attempting` `port` `ABCD` 消息。将冒号后面的 4040 替换为列出的端口号。如果你在一个管理的
    Spark 集群、云或本地环境中工作，请参阅你的提供商文档以访问 SparkUI。
- en: 'The Spark UI landing pages (also known as the Job tab on the top menu) contain
    a lot of information, which we can divide into a few sections:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Spark UI 的首页（也称为顶部菜单中的作业标签）包含大量信息，我们可以将其分为几个部分：
- en: The top menu provides access to the main sections of the Spark UI, which we
    explore in this chapter.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顶部的菜单提供了访问 Spark UI 主要部分的入口，我们将在本章中对其进行探讨。
- en: The timeline provides a visual overview of the activities impacting your `SparkSession`;
    in our case, we see the cluster allocating resources (an executor driver, since
    we work locally) and performing our program.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 时间线提供了影响你的 `SparkSession` 的活动的视觉概述；在我们的例子中，我们看到集群正在分配资源（一个执行器驱动程序，因为我们本地工作）并执行我们的程序。
- en: The jobs, which in our case are triggered by the `show()` action (depicted in
    the Spark UI as `showString`), are listed at the bottom of the page. In the case
    where a job is processing, it would be listed as *in progress*.
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的例子中，由 `show()` 动作（在 Spark UI 中表示为 `showString`）触发的作业列在页面底部。如果作业正在处理，它将显示为
    *进行中*。
- en: '![](../Images/11-01.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-01.png)'
- en: Figure 11.1 The landing page (Jobs) for the Spark UI. We see an empty timeline
    because the only event that has happened is the launch of the PySpark shell.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 Spark UI 的首页（作业）。我们看到时间线为空，因为唯一发生的事件是 PySpark shell 的启动。
- en: The next sections cover the different tabs of the Spark UI at a high level.
    We cover the information Spark provides about its own configuration and its memory
    and resource usage before plunging into understanding how our program was executed.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将概述 Spark UI 的不同标签页。在深入了解我们的程序是如何执行之前，我们先了解 Spark 提供的有关其自身配置及其内存和资源使用的信息。
- en: '11.1.1 Reviewing the configuration: The environment tab'
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 检查配置：环境标签页
- en: This section covers the Environment tab of the Spark UI. This tab contains the
    configuration of the environment that our Spark instance sits on, so the information
    is useful for troubleshooting library problems, providing configuration information
    if you run into weird behavior (or a bug!), or understanding the specific behavior
    of a Spark instance.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 Spark UI 的环境标签页。此标签页包含我们的 Spark 实例所在环境的配置，因此这些信息对于解决库问题、在遇到奇怪的行为（或错误！）时提供配置信息或了解
    Spark 实例的具体行为非常有用。
- en: '![](../Images/11-02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-02.png)'
- en: Figure 11.2 The Environment tab contains information about the hardware, OS,
    and libraries/software versions Spark is sitting on top of.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 环境标签页包含有关 Spark 所在的硬件、操作系统和库/软件版本的信息。
- en: The Environment tab contains all the information about how the machines on your
    cluster are set up. It covers information about the JVM and Scala versions installed
    (remember, Spark is a Scala program), as well as the options Spark is using for
    this session. If you want a complete description of each field (including those
    not listed), the Spark configuration page ([http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html))
    contains a fairly readable description.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 环境标签页包含有关集群上机器配置的所有信息。它涵盖了关于已安装的 JVM 和 Scala 版本的信息（记住，Spark 是一个 Scala 程序），以及
    Spark 为本次会话使用的选项。如果您想了解每个字段（包括未列出的字段）的完整描述，Spark 配置页面 ([http://spark.apache.org/docs/latest/configuration.html](http://spark.apache.org/docs/latest/configuration.html))
    包含了一个相当易读的描述。
- en: Note There are other sections (Hadoop Properties and Classpath Entries) that
    are useful if you need to pinpoint specific Hadoop or library issues or when submitting
    a bug, but for our purposes, we can skip them with no hard feelings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：还有其他部分（Hadoop 属性和类路径条目）在您需要定位特定的 Hadoop 或库问题时很有用，或者在提交错误报告时，但我们为了我们的目的可以跳过它们，而且不会有任何不快。
- en: A lot of the entries are self-explanatory, but they are, nonetheless, important
    when troubleshooting a PySpark job that doesn’t work as planned. Other than a
    few identifiers, such as the application ID and name, Spark will list any configuration
    option and optional libraries we provide our job with in the UI. For instance,
    in chapter 9, the BigQuery Spark connector would be listed under `spark.jars`
    and `spark.repl.local.jars`.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 许多条目都是不言自明的，但在调试一个按计划无法正常工作的 PySpark 作业时，它们仍然很重要。除了几个标识符，如应用程序 ID 和名称外，Spark
    还会在 UI 中列出我们为作业提供的任何配置选项和可选库。例如，在第 9 章中，BigQuery Spark 连接器将列在 `spark.jars` 和 `spark.repl.local.jars`
    下。
- en: The only Python-specific option worth mentioning here is the `spark.submit.pyFiles`.
    Since we are running PySpark from the REPL, no files, per se, were submitted to
    Spark. When using `spark-submit` with a Python file or module, your file(s) name
    will be listed there.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 值得在这里提及的唯一 Python 特定选项是 `spark.submit.pyFiles`。由于我们从 REPL 运行 PySpark，实际上并没有向
    Spark 提交任何文件。当使用 `spark-submit` 与 Python 文件或模块一起使用时，您的文件（名）将列在那里。
- en: Note Python libraries installed on the cluster are not listed in the Spark UI.
    When working locally, installing a new library for our Spark instance is as simple
    as installing it on our local Python. When working on a managed cluster, Spark
    provides some strategies to avoid doing it manually, which is heavily dependent
    on the provider. (See [http://mng.bz/nYrK](http://mng.bz/nYrK) for more information.)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：集群上安装的 Python 库在 Spark UI 中没有列出。当本地工作时，为我们的 Spark 实例安装一个新的库就像在我们的本地 Python
    中安装它一样简单。当在托管集群上工作时，Spark 提供了一些策略来避免手动操作，这很大程度上取决于提供商。（更多信息请见 [http://mng.bz/nYrK](http://mng.bz/nYrK)）
- en: 'This section is more about knowing *what* you can find rather than going on
    a very long (and boring) description of every configuration flag. By remembering
    the information available in the Environment tab at a high level, you can rapidly
    zero in if you are facing a problem you believe is OS-, JVM-, or (Java/Scala)
    library-related. If we reuse the example from chapter 9, if you have a `BigQuery`
    `provider` `not` `found` error, your first reflex should be to check the Environment
    tab to see if the jar is listed as a dependency. It’ll also provide great information
    when filing a bug report: you can now easily provide detailed information.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节更多地是关于了解“什么”而不是对每个配置标志进行非常长（且无聊）的描述。通过记住环境标签页中可用的信息，您可以在面对您认为与 OS-、JVM-或（Java/Scala）库相关的问题时迅速定位。如果我们重用第
    9 章的例子，如果您遇到 `BigQuery` `provider` `not` `found` 错误，您的第一个反应应该是检查环境标签页，看看 jar 是否被列为依赖项。在提交错误报告时，它也会提供大量信息：您现在可以轻松提供详细信息。
- en: 'The next section covers the resources we usually care about the most when running
    a PySpark program: memory, CPU, and hard drive. More specifically, we review how
    Spark allocates resources to the executor and how to change the defaults.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节涵盖了我们在运行 PySpark 程序时最关心的资源：内存、CPU 和硬盘。更具体地说，我们回顾了 Spark 如何分配资源给执行器以及如何更改默认值。
- en: '11.1.2 Greater than the sum of its parts: The Executors tab and resource management'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 集体大于部分之和：执行器标签页和资源管理
- en: In this section, I review the Executors tab, which contains information about
    the computing and memory resources available to our Spark instance. Referring
    to this tab during the course of a Spark job allows you to monitor the health
    of your Spark installation, as well as resource usage across all nodes.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我回顾了 Executors 选项卡，其中包含有关我们 Spark 实例可用的计算和内存资源的信息。在 Spark 作业过程中参考此选项卡允许你监控
    Spark 安装的健康状况以及所有节点上的资源使用情况。
- en: After clicking on Executors, we are presented with a summary and detailed view
    of all the nodes in our cluster. Since I am working locally, I see only one node
    that is playing the role of driver.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 点击 Executors 后，我们会看到我们集群中所有节点的摘要和详细视图。由于我是在本地工作，我只看到一个节点，它扮演着驱动器的角色。
- en: When considering the processing power of a cluster, we think about CPU and RAM.
    In figure 11.3, my cluster is made up of 12 CPU cores (`local[*]`—see section
    11.1.1—gives access to all the cores of my local CPU) and 434.4 MiB (mebibytes,
    or 2^20 [1,048,576] bytes, not to be confused with megabytes, which are in base
    10 [10^6 or 1,000,000]). By default, Spark will allocate 1 GiB (gebibyte) of memory
    to the driver process. (See the sidebar at the end of this section for the formula
    on how to get from 1GiB to 434.4Mib.)
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑集群的处理能力时，我们会想到 CPU 和 RAM。在图 11.3 中，我的集群由 12 个 CPU 核心组成（`local[*]`—见 11.1.1
    节—可以访问本地 CPU 的所有核心）和 434.4 MiB（梅比字节，或 2^20 [1,048,576] 字节，不要与基于 10 的兆字节混淆，后者为
    10^6 或 1,000,000）。默认情况下，Spark 将为驱动器进程分配 1 GiB（吉字节）的内存。（参见本节末尾的侧边栏，了解如何从 1GiB 转换到
    434.4MiB 的公式。）
- en: '![](../Images/11-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-03.png)'
- en: Figure 11.3 My local Spark UI Executors tab. I have 434.4 MiB of storage memory
    and 12 CPU cores available.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.3 我的本地 Spark UI Executors 选项卡。我有 434.4 MiB 的存储内存和 12 个 CPU 核心可用。
- en: 'Spark uses RAM for three main purposes, as illustrated in figure 11.4:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 主要使用 RAM 的三个目的，如图 11.4 所示：
- en: A portion of the RAM is reserved for Spark internal processing, such as user
    data structures, internal metadata, and safeguarding against potential out-of-memory
    errors when dealing with large records.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部分RAM被保留用于 Spark 内部处理，例如用户数据结构、内部元数据和在大记录处理时防止潜在的内存不足错误。
- en: The second portion of the RAM is used for operations (*operational memory*).
    This is the RAM used during data transformation.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAM 的第二部分用于操作（*操作内存*）。这是在数据转换期间使用的 RAM。
- en: The last portion of the RAM is used for the storage (*storage memory*) of data.
    RAM access is a lot faster than reading and writing data from and to disk, so
    Spark will try to put as much data in memory as possible. If operational memory
    needs grow beyond what’s available, Spark will *spill* some of the data from RAM
    to disk.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RAM 的最后一部分用于数据的存储（*存储内存*）。与从磁盘读取和写入数据相比，RAM 访问要快得多，因此 Spark 会尽可能多地将数据放在内存中。如果操作内存需求超过了可用资源，Spark
    将会将一些数据从 RAM 溢出到磁盘。
- en: '![](../Images/11-04.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-04.png)'
- en: Figure 11.4 A simplified layout, or the resources Spark uses by default. Spark
    uses RAM as much as it can, resorting to disk when RAM is not enough (via spilling).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.4 简化的布局，或 Spark 默认使用的资源。Spark 尽可能地使用 RAM，当 RAM 不足时（通过溢出）会求助于磁盘。
- en: Spark provides a few configuration flags to change the memory and number of
    CPU cores available. We have access to two identical sets of parameters to define
    the resources our drivers and executors will have access to.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 提供了一些配置标志来更改可用的内存和 CPU 核心数。我们有两组相同的参数来定义驱动器和执行器将能够访问的资源。
- en: When creating the `SparkSession`, you can set the `master()` method to connect
    to a specific cluster manager[¹](#pgfId-1012776) (in cluster mode) when working
    locally and specify the resources/number of cores to allocate from your computer.
    In listing 11.2, I decide to go from 12 cores to only 8 by passing `master("local[8]")`
    in my `SparkSession` builder object. When working locally, our (single) machine
    will host the driver and perform the work on the data. In the case of a Spark
    cluster, you’ll have a driver node coordinating the work, a cluster manager, and
    a series of worker nodes hosting executors performing the work (see chapter 1
    for a refresher).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 `SparkSession` 时，你可以设置 `master()` 方法以连接到特定的集群管理器[¹](#pgfId-1012776)（在集群模式下）并在本地工作时指定从你的计算机分配的资源/核心数。在列表
    11.2 中，我决定将核心数从 12 个减少到 8 个，通过在 `SparkSession` 构造器对象中传递 `master("local[8]")` 实现。在本地工作时，我们的（单个）机器将托管驱动器并在数据上执行工作。在
    Spark 集群的情况下，你将有一个协调工作的驱动器节点、一个集群管理器和一系列托管执行器的工人节点（参见第 1 章以刷新知识）。
- en: Note What about GPUs? As of Spark 3.0, GPU usage has been greatly simplified,
    but GPUs are still not common stock in Spark instances. GPU, like CPU, would be
    in the processing sector of the diagram. For more information, check out the RAPIDS+Spark
    section on the Nvidia website ([https://nvidia.github.io/spark-rapids/Getting-Started/](https://nvidia.github.io/spark-rapids/Getting-Started/)).
    Most, if not all, cloud providers provide the option to equip your Spark/Databricks
    cluster with GPU nodes.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 关于 GPU 呢？截至 Spark 3.0，GPU 使用已经大大简化，但 GPU 在 Spark 实例中仍然不是常见的配置。GPU，就像 CPU
    一样，会在处理部分的图中。有关更多信息，请查看 Nvidia 网站上的 RAPIDS+Spark 部分 ([https://nvidia.github.io/spark-rapids/Getting-Started/](https://nvidia.github.io/spark-rapids/Getting-Started/))。大多数，如果不是所有，云提供商都提供将
    GPU 节点配置到你的 Spark/Databricks 集群的选项。
- en: 'Memory allocation is done through configuration flags; the most important when
    working locally is `spark.driver.memory`. This flag takes size as an attribute
    and is set via the `config()` method of the `SparkSession` builder object. The
    different abbreviations are listed in table 11.1: Spark will not accept decimal
    numbers, so you need to pass integer values.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 内存分配是通过配置标志完成的；在本地工作时最重要的标志是 `spark.driver.memory`。此标志将大小作为属性，并通过 `SparkSession`
    构造对象的 `config()` 方法设置。不同的缩写列在表 11.1 中：Spark 不接受小数，因此你需要传递整数值。
- en: Table 11.1 The different value types Spark will accept for size. You can change
    the `1` to another integer value.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表 11.1 Spark 将接受的用于大小的不同值类型。你可以将 `1` 改为另一个整数值。
- en: '| Abbreviation | Definition |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 缩写 | 定义 |'
- en: '| `1b` | 1 byte |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `1b` | 1 字节 |'
- en: '| `1k` or `1kb` | 1 kibibyte = 1,024 bytes |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `1k` 或 `1kb` | 1 kibibyte = 1,024 字节 |'
- en: '| `1m` or `1mb` | 1 mebibyte = 1,024 kibibytes |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| `1m` 或 `1mb` | 1 mebibyte = 1,024 kibibytes |'
- en: '| `1g` or `1gb` | 1 gibibyte = 1,024 mebibytes |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `1g` 或 `1gb` | 1 gibibyte = 1,024 mebibytes |'
- en: '| `1t` or `1tb` | 1 tebibyte = 1,024 gibibytes |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `1t` 或 `1tb` | 1 tebibyte = 1,024 gibibytes |'
- en: '| `1p` or `1pb` | 1 pebibyte = 1,024 tebibytes |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `1p` 或 `1pb` | 1 pebibyte = 1,024 tebibytes |'
- en: Warning Spark uses power-of-two numbers (there are 1,024 bytes in a kibibyte,
    whereas there are only 1,000 bytes in a kilobytes), whereas RAM memory is usually
    shown in power-of-ten units.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 Spark 使用 2 的幂次数（kibibyte 中有 1,024 字节，而 kilobytes 中只有 1,000 字节），而 RAM 内存通常以
    10 的幂次数单位显示。
- en: In listing 11.2, I combine those two options into a new `SparkSession` creation.
    If you already have Spark running, make sure you shut it down (exit the shell
    where you launched PySpark) and start again to make sure Spark picks up the new
    configuration. When working on your local machine, unless you have a strong reason
    to do otherwise, limit memory allocation to 50% of your total RAM to account for
    the other programs/tasks running at the same time. For Spark in cluster mode,
    the documentation recommends not going over 75% of the available RAM.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 11.2 中，我将这两个选项组合成一个新的 `SparkSession` 创建。如果你已经启动了 Spark，请确保将其关闭（退出启动 PySpark
    的 shell）并重新启动，以确保 Spark 识别新的配置。在本地机器上工作，除非你有充分的理由这样做，否则将内存分配限制为总 RAM 的 50%，以考虑同时运行的其它程序/任务。对于集群模式的
    Spark，文档建议不要超过可用 RAM 的 75%。
- en: Listing 11.2 Relaunching PySpark to change the number of cores/RAM available
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 重新启动 PySpark 以更改可用的核心/内存数量
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ local[8] means that we use only eight cores for the master.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ local[8] 表示我们只为 master 使用八个核心。
- en: ❷ The driver will use 16 g instead of the default of 1 g.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 驱动程序将使用 16 g 而不是默认的 1 g。
- en: If you are launching PySpark using the `pyspark` command (e.g., when SSH-ing
    into a master node on a managed cloud Spark instance) or using `spark-submit`,
    you will need to pass the configuration as either command-line arguments or in
    the configuration file (see appendix B for more details). In our case, the Spark
    UI shows the configuration using command-line arguments (`--conf` syntax) in the
    `java.sun.command` field (see section 11.1.1). I show the result of our new `SparkSession`
    in figure 11.5.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用 `pyspark` 命令（例如，在 SSH 连接到托管云 Spark 实例的主节点时）或使用 `spark-submit`，你需要将配置作为命令行参数或配置文件中的参数传递（有关更多详细信息，请参阅附录
    B）。在我们的例子中，Spark UI 在 `java.sun.command` 字段中显示配置（请参阅 11.1.1 节），我展示了我们新的 `SparkSession`
    的结果。
- en: '![](../Images/11-05.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-05.png)'
- en: Figure 11.5 In the Environment tab, the `sun.java.command` has the same configuration
    flags we passed in the `--conf` launcher syntax.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.5 在环境标签页中，`sun.java.command` 具有与我们在 `--conf` 启动语法中传递的相同的配置标志。
- en: Math time! How to go from 1 GiB to 434.4 MiB
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 数学时间！如何从 1 GiB 转换到 434.4 MiB
- en: As mentioned earlier in the chapter, Spark allocates 1 GiB of memory by default
    to the driver program. What’s the deal with 434.4 MiB? How do we go from 1 GiB
    of allocated memory to 434.4 MiB of usable memory?
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，Spark 默认为驱动程序程序分配 1 GiB 的内存。434.4 MiB 是怎么回事？我们是如何从 1 GiB 的分配内存转换为 434.4
    MiB 的可用内存的？
- en: 'In figure 11.4, I explained that Spark segments the memory on a node into three
    sections: the reserved, operational, and storage memory. The 434.4 MiB represents
    both the operational and storage memory. A few configuration flags are responsible
    for the exact memory split:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 11.4 中，我解释了 Spark 将节点上的内存分为三个部分：预留、操作和存储内存。434.4 MiB 代表操作和存储内存。一些配置标志负责精确的内存分割：
- en: '`spark.{driver|executor}.memory` determines the total memory envelope available
    to the Spark driver or an executor, which I’ll call `M` (by default `1g`). You
    can have different memory requirements for your driver versus your executor, but
    I usually see both values being the same.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.{driver|executor}.memory` 确定了 Spark 驱动程序或执行程序可用的总内存封套，我将称之为 `M`（默认为
    `1g`）。您可以为驱动程序和执行程序设置不同的内存需求，但通常我看到这两个值是相同的。'
- en: '`spark.memory.fraction`, which I’ll call `F`, sets the fraction of memory available
    to Spark (operational plus storage; by default `0.6`).'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.memory.fraction`，我将称之为 `F`，设置 Spark 可用内存的比例（操作加存储；默认为 `0.6`）。'
- en: '`spark.memory.storageFraction`, which I’ll call `S`, is the fraction of the
    memory available to Spark (`M` `×` `F`) and will be used predominantly for storage
    (by default `0.5`).'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`spark.memory.storageFraction`，我将称之为 `S`，是 Spark 可用内存的比例（`M × F`），主要用于存储（默认为
    `0.5`）。'
- en: 'In the case of 1 GiB of RAM being provided, Spark will start by putting 300
    MiB aside. The rest will be split between reserved and allocated (operational
    plus storage) using the `spark.memory.fraction` value: `(1` `GiB` `-` `300` `MiB)`
    `*` `0.6` `=` `434.4MiB`. This is the value shown in the Spark UI. Internally,
    Spark will manage the operational and storage memory using the `spark.memory.storageFraction`
    ratio. In our case, since the ratio is at `0.5`, the memory will be split evenly
    between operational and storage.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在提供 1 GiB RAM 的情况下，Spark 将首先预留 300 MiB。其余部分将根据 `spark.memory.fraction` 值在预留和分配（操作加存储）之间分割：`(1
    GiB - 300 MiB) * 0.6 = 434.4 MiB`。这是 Spark UI 中显示的值。内部，Spark 将使用 `spark.memory.storageFraction`
    比率管理操作和存储内存。在我们的案例中，由于比率是 `0.5`，内存将在操作和存储之间平均分割。
- en: '![](../Images/11-05-unnumb.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-05-unnumb.png)'
- en: 'In practice, the storage can outgrow its allotted place: the `spark.memory.storageFraction`
    defines the zone where Spark will protect the data from being spilled to disk
    (e.g., during memory-intensive computations), but if we have more data than what
    fits into the storage memory, Spark will borrow from the operational memory section.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，存储可能会超出其分配的空间：`spark.memory.storageFraction` 定义了 Spark 将保护数据不被溢出到磁盘的区域（例如，在内存密集型计算期间），但如果我们的数据比存储内存大，Spark
    将从操作内存部分借用。
- en: For the vast majority of your programs, it is not recommended to play with those
    values. While it may seem counterintuitive to use too much memory for storing
    data, remember that reading data from RAM makes Spark much faster than when it
    needs to rely on the hard drive.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于您的大多数程序，不建议玩弄这些值。虽然使用过多内存来存储数据可能看起来有些反直觉，但请记住，从 RAM 中读取数据比 Spark 需要依赖硬盘时要快得多。
- en: In this section, we explored the configuration flags and the relevant parts
    of the Spark UI to review and set the CPU and memory resources. In the next section,
    I run some small jobs, explore the runtime information Spark provides via the
    Spark UI, and explain how we can use this to make better configuration and coding
    decisions.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了配置标志和 Spark UI 的相关部分，以审查和设置 CPU 和内存资源。在下一节中，我将运行一些小任务，探索 Spark UI
    提供的运行时信息，并解释我们如何利用这些信息做出更好的配置和编码决策。
- en: '11.1.3 Look at what you’ve done: Diagnosing a completed job via the Spark UI'
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 查看你所做的工作：通过 Spark UI 诊断已完成的工作
- en: This section covers the most important metrics when reviewing job performance.
    I introduce the concepts of jobs and stages, how Spark reports performance metrics
    for each stage, and how we can interpret the information the Spark UI provided
    to optimize our jobs.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了审查作业性能时最重要的指标。我介绍了作业和阶段的概念，Spark 如何为每个阶段报告性能指标，以及我们如何解释 Spark UI 提供的信息以优化我们的作业。
- en: Note Some cloud-managed Spark jobs (such as Google Dataproc) do not provide
    access to the Spark UI; instead, you have a Spark History Server. The look is
    the same, but it won’t be available *until the job you’re running is done*. In
    the case of a PySpark shell, it means you’ll have to exit the session before you
    can see the results.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：一些云管理的Spark作业（如Google Dataproc）不提供对Spark UI的访问；相反，你有一个Spark历史服务器。外观相同，但它在**运行中的作业完成之前**不可用。在PySpark
    shell的情况下，这意味着你必须在看到结果之前退出会话。
- en: 'Spark organizes the code we submit into jobs. A job is simply a series of transformations
    (`select()`, `groupBy()`, `where()`, etc.) crowned by a final action (`count()`,
    `write()`, `show()`). In chapter 1, I explained that Spark will not start working
    until we submit an action: each action will trigger one job. As an example, the
    code leading to the `results` data frame in listing 11.3 does not contain any
    action. The Spark UI does not display any job (or any sign of work, for that matter)
    after submitting this code in the REPL.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 将我们提交的代码组织成作业。作业简单来说是一系列转换（`select()`、`groupBy()`、`where()` 等）加上一个最终的操作（`count()`、`write()`、`show()`）。在第1章中，我解释了Spark只有在提交操作后才会开始工作：每个操作都会触发一个作业。例如，导致列表11.3中的`results`数据框的代码不包含任何操作。在REPL中提交此代码后，Spark
    UI不会显示任何作业（或任何工作迹象）。
- en: Listing 11.3 The chain of transformation applied to our text files
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3 对我们的文本文件应用的一系列转换
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Only when we submit an action, like the `show()` method at the end of listing
    11.3, do we see work being performed (a very fast progress bar, followed by results
    in the REPL window). On the Spark UI, we see one job (because there is one action),
    detailed in figure 11.6.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 只有当我们提交操作时，例如列表11.3末尾的`show()`方法，我们才能看到工作正在执行（一个非常快的进度条，随后是REPL窗口中的结果）。在Spark
    UI上，我们看到一个作业（因为有一个操作），如图11.6所示。
- en: '![](../Images/11-06.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-06.png)'
- en: Figure 11.6 The Completed Jobs table on the Jobs tab of the Spark UI with our
    one completed job. Our word count, with a single action, is listed as one job.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.6 Spark UI的“作业”标签页上的“完成作业”表，其中包含我们的一个完成作业。我们的单词计数，通过一个操作，被列为一个作业。
- en: 'Each job is internally broken down into stages, which are units of work being
    performed on the data. What goes into a stage depends on how the query optimizer
    decides to split the work. Section 11.1.4 goes into greater detail about how the
    stages are constructed and how we can influence them. Our simple program has three
    steps:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 每个作业在内部被分解成阶段，这些阶段是正在数据上执行的工作单元。一个阶段包含什么取决于查询优化器如何决定分割工作。第11.1.4节更详细地介绍了阶段的构建方式以及我们如何影响它们。我们的简单程序有三个步骤：
- en: Stage 0 reads the data from all (six) text files present in the directory and
    performs all the transformations (split, explode, lowercase, extract the regular
    expression, filter). It then groups by and counts the word frequency for each
    partition independently.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第0阶段从目录中所有（六个）文本文件中读取数据，并执行所有转换（拆分、展开、小写、提取正则表达式、过滤）。然后它按组别进行分组并独立计算每个分区的单词频率。
- en: Spark then *exchanges* (or shuffles) the data across each node to prepare for
    the next stage. Because the data is very small once grouped by (and we only need
    10 records to show), all the data gets back to one node in a single partition.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Spark 然后将数据在每个节点之间进行交换（或洗牌）以准备下一阶段。由于数据分组后非常小（我们只需要10条记录来显示），所有数据都通过单个分区返回到一个节点。
- en: Finally, during stage 1, we compute the total word count for the 10 selected
    records and display the records in table form.
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，在第1阶段，我们计算10个选定记录的总单词数，并以表格形式显示记录。
- en: This two-staged approach to group by/count works because counting the number
    of records is commutative and associative. Chapter 8 covers commutativity and
    associativity and why they matter for Spark.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这种按两个阶段进行分组/计数的做法之所以有效，是因为记录数是交换律和结合律的。第8章涵盖了交换律和结合律以及为什么它们对Spark很重要。
- en: 'In the Completed Stages table (we’ve now moved to the *Stages* tab of the Spark
    UI), Spark provides four main metrics related to the memory consumption:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在“完成阶段”表（我们现在已经移动到Spark UI的“阶段”标签页）中，Spark提供了与内存消耗相关的四个主要指标：
- en: '*Input* is the amount of data read from source. Our program reads 4.1 MiB of
    data. This seems like an inevitable cost: we need to read the data in order to
    perform work. If you have control over the format and organization of your input
    data, you can achieve a significant performance boost.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输入*是从源读取的数据量。我们的程序读取了4.1 MiB的数据。这似乎是一个不可避免的成本：我们需要读取数据才能执行工作。如果你控制输入数据的格式和组织，你可以实现显著的性能提升。'
- en: '*Output* is the counterpoint to input: it represents the data our program outputs
    as the result of the action. Since we print to terminal, we have no value at the
    end of stage 1.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*输出* 是输入的对立面：它代表我们的程序作为动作的结果输出的数据。由于我们打印到终端，所以在阶段 1 的末尾没有值。'
- en: '*Shuffle read* and *shuffle write* are part of the `shuffling` (or exchange;
    see figure 11.7) operation. Shuffling rearranges the memory on a Spark worker
    to prepare for the next stage. In our case, we needed to write 965.6 KiB at the
    end of stage 0 to prepare for stage 1\. In stage 1, we only read 4.8 KiB since
    we asked for just 10 records. Since Spark lazily optimized the whole job, it knows
    right from the start that we need only the count for 10 words; at exchange time
    (between stage 0 and 1), the driver only kept the relevant 5 words for each file,
    dropping the required data to answer our action by 99.5% (from 965.6 to 4.8 KiB).
    When working with massive files and `show()`-ing the content (by default 20 records),
    this results in significant speed increases!'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Shuffle read* 和 *shuffle write* 是 `shuffling`（或交换；参见图 11.7）操作的一部分。Shuffling
    重新排列 Spark 工作节点上的内存，为下一阶段做准备。在我们的例子中，我们需要在阶段 0 的末尾写入 965.6 KiB 以准备阶段 1。在阶段 1 中，我们只读取了
    4.8 KiB，因为我们只请求了 10 条记录。由于 Spark 对整个作业进行了懒惰优化，它从一开始就知道我们只需要 10 个单词的计数；在交换时间（阶段
    0 和 1 之间），驱动程序只为每个文件保留了相关的 5 个单词，通过减少 99.5% 的所需数据（从 965.6 到 4.8 KiB）来回答我们的动作。当处理大量文件并使用
    `show()`（默认 20 条记录）显示内容时，这会导致显著的速度提升！'
- en: '![](../Images/11-07.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.7](../Images/11-07.png)'
- en: Figure 11.7 The two stages of job 0, with the summary statistics for each stage.
    The size of the data ingested is listed in *input*, the data outputted as *output*,
    and we see the intermediate data movement as *shuffle*. We can use these values
    to infer how large of a data set are we processing.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 作业 0 的两个阶段，以及每个阶段的摘要统计信息。数据摄入的大小列在 *input* 中，数据输出为 *output*，我们看到中间数据移动为
    *shuffle*。我们可以使用这些值来推断我们正在处理的数据集有多大。
- en: This is all very relevant information. We gained insight about the memory consumption
    for our job (transformations plus action). We saw how we can measure the time
    each task takes, the time spent in garbage collection, and the amount of data
    ingested and how that data gets sent around between each stages. The next section
    will pay attention to the actual operations spent on the data, which are encoded
    in a plan. We’re getting close to Spark’s secret sauce!
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这都是非常相关的信息。我们获得了关于作业内存消耗的见解（转换加动作）。我们看到了如何测量每个任务的时间、垃圾收集所花费的时间以及摄入的数据量以及这些数据如何在每个阶段之间传输。下一节将关注实际编码在计划中的数据处理操作。我们正接近
    Spark 的秘密配方！
- en: Exercise 11.1
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.1
- en: If we were to add 10 more files to our word count program, without making any
    change to the code, would that change (a) the number of jobs or (b) the number
    of stages? Why?
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在单词计数程序中添加 10 个更多文件，而不对代码进行任何修改，这会改变（a）作业的数量或（b）阶段的数量吗？为什么？
- en: '11.1.4 Mapping the operations via Spark query plans: The SQL tab'
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.4 通过 Spark 查询计划映射操作：SQL 选项卡
- en: This section covers the different plans that Spark goes through, from code to
    actual work on the data. We take our word count example, which we’ve used since
    the beginning of the chapter, break it down into stages, and then break those
    stages into steps. This is a key tool for understanding what’s happening during
    your job at a lower level than what you wrote as code and is the first thing you
    should do when you feel that your code is slow or not performing as expected.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 Spark 从代码到实际数据处理所经历的不同计划。我们以本章开头使用的单词计数示例为例，将其分解为阶段，然后将这些阶段分解为步骤。这是理解您作业在代码以下层面发生情况的关键工具，也是您在觉得代码运行缓慢或未按预期执行时应该做的第一件事。
- en: Head to the SQL tab of the Spark UI and click on the description of the job.
    You should see a long chain of boxes representing the different stages.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 前往 Spark UI 的 SQL 选项卡，并点击作业的描述。您应该看到一个表示不同阶段的漫长框链。
- en: '![](../Images/11-08.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图 11.8](../Images/11-08.png)'
- en: Figure 11.8 The chain of transformations on our data frame, encoded and optimized
    by Spark. Our code instructions on the `results` data frame are represented in
    stages that are illustrated and described when you hover over each box.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.8 Spark 对我们的数据框进行编码和优化的转换链。我们的代码指令在 `results` 数据框中用阶段表示，当您将鼠标悬停在每个框上时，会显示并描述这些阶段。
- en: If you hover over one of the `Scan` `Text`, `Project`, or `Generate` boxes (like
    in figure 11.9), a black box with information about what happened during that
    step appears. In the case of the first box, called `Scan` `text`, we see that
    Spark performs a `FileScan` `text` operation over all the files we passed as an
    argument to `spark.read.text()`.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将鼠标悬停在“`Scan` `Text`”、“`Project`”或“`Generate`”其中一个框上（如图 11.9 所示），会出现一个包含该步骤发生情况的黑色框。在第一个框，称为“`Scan`
    `text`”的情况下，我们看到 Spark 对我们传递给 `spark.read.text()` 的所有文件执行了 `FileScan` `text` 操作。
- en: '![](../Images/11-09.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图 11-09](../Images/11-09.png)'
- en: Figure 11.9 When hovering over one of the `Scan` `Text`, `Project`, or `Generate`
    boxes, a black overlay appears containing a textual representation of the transformation
    undertaken (called a *plan*) during that step. Most of the time we only see the
    beginning and end of the plan because of space constraints.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 当鼠标悬停在“`Scan` `Text`”、“`Project`”或“`Generate`”其中一个框上时，会出现一个黑色覆盖层，其中包含该步骤进行的转换的文本表示（称为*计划*）。由于空间限制，我们大多数时候只能看到计划的开始和结束部分。
- en: We know that Spark ingests our (Python, although the process works for every
    host language) code and translates it into Spark instructions (see chapter 1).
    Those instructions are encoded into a *query plan* and then sent to the executors
    for processing. Because of this, the performance of PySpark, when using the data
    frame API, is very similar to the Spark Scala API.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道 Spark 将我们的（Python，尽管该过程适用于每种主机语言）代码摄入并转换为 Spark 指令（参见第 1 章）。这些指令被编码到*查询计划*中，然后发送到执行器进行处理。正因为如此，当使用数据框
    API 时，PySpark 的性能与 Spark Scala API 非常相似。
- en: Warning If you recall chapter 8, you’ll remember that the translation analogy
    does not work when working with an RDD. In that case, PySpark will serialize the
    data and apply Python code, similar to when we apply a Python UDF.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果您还记得第 8 章，您会记得在处理 RDD 时，翻译类比不起作用。在这种情况下，PySpark 将序列化数据并应用 Python 代码，类似于我们应用
    Python UDF 时的情况。
- en: How do we access this query plan? Glad you asked! Spark does not present a single
    query plan, but four distinct types of plans created in a sequential fashion.
    We see them in logical order in figure 11.10.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何访问这个查询计划？很高兴您问了！Spark 并不提供一个单一的查询计划，而是按顺序创建了四种不同类型的计划。我们在图 11.10 中按逻辑顺序看到它们。
- en: '![](../Images/11-10.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图 11-10](../Images/11-10.png)'
- en: 'Figure 11.10 Spark optimizes jobs using a multitiered approach: unresolved
    logical plan, logical plan, optimized logical plan, and physical plan. The (selected)
    physical plan is the one applied to the data.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 Spark 使用多层方法优化作业：未解析的逻辑计划、逻辑计划、优化逻辑计划和物理计划。选定的（物理）计划是应用于数据的一个。
- en: 'To see the four (full) plans in action without hovering over multiple boxes,
    we have two main options:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要在不悬停在多个框上时看到四个（完整）计划的实际操作，我们有两个主要选项：
- en: In the Spark UI, at the very bottom of the SQL tab for our job, we can click
    on `Details`, where the plans will be displayed textually.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Spark UI 中，在我们的作业的 SQL 选项卡底部，我们可以点击“`Details`”，在那里将以文本形式显示计划。
- en: We can also print them in the REPL, via the data frame’s `explain()` method.
    In that case, we would not have the final action for our plan since an action
    usually returns a pythonic value (number, string, or `None`), none of which has
    an `explain` value.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们也可以通过数据框的 `explain()` 方法在 REPL 中打印它们。在这种情况下，我们不会有计划的最终操作，因为操作通常返回一个 Pythonic
    值（数字、字符串或 `None`），它们都没有 `explain` 值。
- en: This concludes the high-level overview of the SQL tab in the Spark UI. The next
    section will break down both the plans provided by the Spark UI and the `explain()`
    method, and how we interpret them.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了 Spark UI 中 SQL 选项卡的高级概述。下一节将分解 Spark UI 提供的计划和 `explain()` 方法，以及我们如何解释它们。
- en: '11.1.5 The core of Spark: The parsed, analyzed, optimized, and physical plans'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.5 Spark 的核心：解析、分析、优化和物理计划
- en: This section covers the four plans Spark goes into while performing a job. Understanding
    the key concepts and vocabulary from those plans provides a ton of information
    about how the job is structured and gives us an idea about the data journey across
    the cluster as the executors process the data.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 Spark 在执行作业时进入的四个计划。理解这些计划中的关键概念和词汇提供了大量关于作业结构的信息，并让我们对执行器处理数据时跨集群的数据旅程有一个想法。
- en: Note The Spark documentation is not very consistent on the nomenclature of plans.
    Because I am a fan of using single adjectives, I rely on the vocabulary from the
    Spark UI, dropping the “logical” for the first three plans. I’ll use *parsed*,
    *analyzed*, *optimized*, and *physical* for the four stages in figure 11.10.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Spark 文档在计划的命名法上并不一致。因为我喜欢使用单个形容词，所以我依赖于 Spark UI 的词汇表，对于前三个计划省略了“logical”。我将使用
    *parsed*、*analyzed*、*optimized* 和 *physical* 来表示图 11.10 中的四个阶段。
- en: Before jumping into the gist of each plan, it’s important to understand why
    Spark undergoes a whole planning process for a job. Managing and processing large
    data sources across multiple machines comes with its own set of challenges. On
    top of optimizing each node for high-speed processing via adequate usage of RAM,
    CPU, and HDD resources (see figure 11.4), Spark also needs to work through the
    complexity of managing the data across nodes (see section 11.2.1 for more details).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入到每个计划的细节之前，了解为什么 Spark 需要对一个作业进行整个规划过程是很重要的。在多台机器上管理和处理大量数据源带来了一系列挑战。除了通过合理使用
    RAM、CPU 和 HDD 资源来优化每个节点以实现高速处理（参见图 11.4）之外，Spark 还需要解决跨节点管理数据的复杂性（更多细节请参见 11.2.1
    节）。
- en: 'A plan is displayed in the Spark UI via the `explain()` data frame method as
    a tree of steps that are performed in the data. We read a plan, regardless of
    its type, from the most nested line to the least nested one: for most jobs, this
    will mean reading from the bottom to the top. In listing 11.4, the parsed plan
    looks very much like a translation of our Python code into Spark operations. We
    recognize most operations (`explode`, `regexp_extract`, `filter`) in the plan.
    Grouping data is called `Aggregate` in the plans, and selecting data is called
    `Project` (i.e., “I am projecting,” not “My project is overdue”).'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark UI 中，通过 `explain()` 数据框方法显示的计划是一个执行在数据中的步骤树。我们从一个最嵌套的行开始，无论其类型如何，从最内层到最外层读取计划：对于大多数作业，这意味着从底部到顶部读取。在列表
    11.4 中，解析计划看起来非常像是将我们的 Python 代码翻译成 Spark 操作。我们识别出计划中的大多数操作（`explode`、`regexp_extract`、`filter`）。在计划中，数据分组被称为
    `Aggregate`，选择数据被称为 `Project`（即，“我在投射”，而不是“我的项目已经逾期”）。
- en: Tip By default, `explain()` will only print the physical plan. If you want to
    see everything, use `explain(extended=True)`. The documentation for the `explain()`
    method explains other options for formatting and statistics.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：默认情况下，`explain()` 只会打印物理计划。如果您想看到所有内容，请使用 `explain(extended=True)`。`explain()`
    方法的文档解释了其他格式化和统计选项。
- en: Listing 11.4 The parsed logical plan for our job
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.4 我们作业的解析逻辑计划
- en: '[PRE3]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ results.show(5, False)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ results.show(5, False)
- en: ❷ .count()
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ .count()
- en: ❸ .groupby(F.col("word"))
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ .groupby(F.col("word"))
- en: ❹ .where(F.col("word") != "")
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ .where(F.col("word") != "")
- en: ❺ .select(F.regexp_extract(F.col("word"), "[a-z']+", 0).alias("word"))
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ .select(F.regexp_extract(F.col("word"), "[a-z']+", 0).alias("word"))
- en: ❻ .select(F.lower(F.col("word")).alias("word"))
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ .select(F.lower(F.col("word")).alias("word"))
- en: ❼ select(F.explode(F.col("line")).alias("word")).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ select(F.explode(F.col("line")).alias("word")).
- en: ❽ .select(F.split(F.col("value"), " ").alias("line"))
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ .select(F.split(F.col("value"), " ").alias("line"))
- en: ❾ results = spark.read.text("./data/gutenberg_books/1342-0.txt")
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ results = spark.read.text("./data/gutenberg_books/1342-0.txt")
- en: Note Spark needs unique column names when working with data frames, which is
    why we see the `#X` (where `X` is a number) in the plans. For instance, `lower(word#5)`
    `AS` `word#7` still refers to the `word` column, but Spark assigns an increasing
    number after the pound sign.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当 Spark 与数据框一起工作时，需要唯一的列名，这就是为什么我们在计划中看到 `#X`（其中 `X` 是一个数字）的原因。例如，`lower(word#5)`
    `AS` `word#7` 仍然指的是 `word` 列，但 Spark 在井号后面分配了一个递增的数字。
- en: 'Moving from the parsed logical plan to the analyzed logical plan does not change
    much operation wise. On the other hand, Spark now knows the schema of our resulting
    data frame: `word:` `string,` `count:` `string`.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 从解析的逻辑计划到分析的逻辑计划在操作上变化不大。另一方面，Spark 现在知道了我们的结果数据框的架构：`word:` `string,` `count:`
    `string`。
- en: Listing 11.5 The analyzed plan for our word count job
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.5 我们单词计数作业的分析计划
- en: '[PRE4]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '❶ The resulting data frame has two columns: word (a string column) and count
    (also a string, because we are show()-ing the result to the terminal).'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 结果数据框有两个列：单词（一个字符串列）和计数（也是一个字符串，因为我们正在将结果显示到终端）。
- en: The analyzed plan then gets optimized via multiple heuristics and rules based
    on how Spark performs operations. In the next listing, we recognize the same operations
    as the two previous plans (parsed and analyzed), but we don’t have that one-to-one
    mapping anymore. Let’s look at the differences in greater detail.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 分析计划随后通过基于 Spark 执行操作的方式的多个启发式规则和规则进行优化。在下一个列表中，我们识别出与之前两个计划（解析和分析）相同的操作，但我们不再有一对一的映射。让我们更详细地看看差异。
- en: Listing 11.6 The optimized plan for our word count job
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.6 我们单词计数作业的优化计划
- en: '[PRE5]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'First, the `explode()` operation does not have a projection (see `Project`
    `[word#5]` in the analyzed plan). Nothing is too surprising here: this column
    is only used in the chain of computation and does not need to be selected/projected
    explicitly. Spark also does not keep the project step for the casting of the `count`
    as string; the casting happens during the aggregation.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，`explode()` 操作没有投影（参见分析计划中的 `Project` `[word#5]`）。这里没有什么令人惊讶的：这个列仅在计算链中使用，不需要显式地选择/投影。Spark
    也不会保留将 `count` 转换为字符串的投影步骤；转换发生在聚合过程中。
- en: Second, the `regexp_extract()` and `lower()` operations are lumped into a single
    step. Because both are narrow operations that operate on each record independently
    (see listing 11.7), Spark can perform the two transformations in a single pass
    over the data.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，`regexp_extract()` 和 `lower()` 操作被合并为单个步骤。因为这两个操作都是窄操作，它们独立地对每条记录进行操作（参见列表
    11.7），Spark 可以在单次数据遍历中执行这两个转换。
- en: 'Finally, Spark duplicates the `(regexp_extract(lower(word#5),` `[a-z'']+,`
    `0)` `=` `)` step: it performs it during the `Filter` step and then again during
    the `Project` step. Because of this, the `Filter` and `Project` steps of the analyzed
    plan are inverted. This might look counterintuitive at first: since the data is
    in memory, Spark believes that performing the filter (even if it means just throwing
    some CPU cycles away) ahead of time yields better performance.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Spark 重复了 `(regexp_extract(lower(word#5), `[a-z']+,` `0)` `=` `)` 步骤：它在 `Filter`
    步骤中执行，然后在 `Project` 步骤中再次执行。因此，分析计划中的 `Filter` 和 `Project` 步骤被颠倒了。一开始这可能会看起来有些反直觉：由于数据在内存中，Spark
    认为提前执行过滤操作（即使这意味着只是浪费一些 CPU 周期）可以获得更好的性能。
- en: 'Finally, the optimized plan gets converted into actual steps that the executor
    will perform: this is called the *physical plan* (in the sense that Spark will
    actually perform this work on the data, not that you’ll see your cluster doing
    jumping jacks). The physical plan is very different from the others.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，优化后的计划被转换为执行器将执行的实际步骤：这被称为 *物理计划*（在 Spark 实际在数据上执行这项工作的意义上，而不是你看到你的集群在做跳跃动作）。物理计划与其他计划非常不同。
- en: Listing 11.7 The physical plan (actual processing steps) for our word count
    job
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.7 我们单词计数作业的物理计划（实际处理步骤）
- en: '[PRE6]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Spark trades the logical relations for an actual reading of the file (`FileScan`
    `text`). Spark does not actually care much about the actual data for the three
    previous logical plans; it is only worried about getting the column names and
    types, and Spark will coordinate the reading of the data. If we have multiple
    files (like in our case), Spark will split the files between the executors so
    that each one reads what’s needed. Neat!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 用实际的文件读取（`FileScan` `text`）交换了逻辑关系。Spark 并不真正关心前三个逻辑计划的实际数据；它只关心获取列名和类型，Spark
    将协调数据的读取。如果我们有多个文件（就像我们的情况一样），Spark 将在执行器之间分割文件，以便每个执行器读取所需的内容。真 neat！
- en: 'We also have some numbers prefixed by an asterisk—`*(1)` to `*(3)`—which corresponds
    to the `WholeStageCodegen` of the Spark UI SQL schema seen in figure 11.8\. A
    `WholeStageCodegen` is a stage where each operation happens on the same pass over
    the data. For our example, we have three:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还有一些以星号开头的数据——`*(1)` 到 `*(3)`——这对应于 Spark UI SQL 模式中看到的 `WholeStageCodegen`。`WholeStageCodegen`
    是一个每个操作都在同一数据遍历中发生的阶段。在我们的例子中，我们有三个：
- en: Splitting the value
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割值
- en: Filtering out the empty words, extracting the words, and pre-aggregating the
    word counts (like we saw in section 11.1.3)
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤掉空单词，提取单词，并预先聚合单词计数（就像我们在 11.1.3 节中看到的那样）
- en: Aggregating the data into a final data frame
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据聚合到最终的数据框中
- en: The translation from Python instructions to a Spark physical plan is not always
    so clear, but even for complex PySpark programs, I follow the same blueprint of
    looking at the parsed plan and following the transformations all the way to the
    physical plan. This information, combined with the Spark instance overview, proves
    invaluable when you need to diagnose what’s happening under the hood. If you are
    a detective, the Spark UI and its multiple tabs are just like having both the
    weapon from the crime and a confession letter.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 将 Python 指令转换为 Spark 物理计划并不总是那么清晰，但对于复杂的 PySpark 程序，我遵循相同的蓝图，查看解析的计划，并跟踪转换直到物理计划。当你需要诊断底层发生的事情时，这些信息与
    Spark 实例概述相结合，证明是无价的。如果你是侦探，Spark UI 及其多个标签就像同时拥有犯罪武器和忏悔信。
- en: This section covered an overview of the Spark UI. This portal provides invaluable
    information about the configuration of your Spark instance, the resources available,
    and the different jobs in progress or completed. In the next section, I cover
    a few important concepts useful to understanding Spark’s processing performance,
    as well as a few pitfalls and false friends that hinder your data jobs.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 Spark UI 的概述。这个门户提供了关于你的 Spark 实例配置、可用资源和正在进行的或已完成的不同作业的无价信息。在下一节中，我将介绍一些有助于理解
    Spark 处理性能的重要概念，以及一些阻碍你的数据作业的陷阱和错误朋友。
- en: Exercise 11.2
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.2
- en: If you use `results.explain(extended=True)` in the REPL and look at the analyzed
    plan, the schema will read `word:` `string,` `count:` `bigint`, and there is no
    `GlobalLimit/LocalLimit` `6`. Why are the two plans (`explain()` versus Spark
    UI) different?
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 `results.explain(extended=True)` 在 REPL 中，并查看分析的计划，模式将读取 `word:` `string,`
    `count:` `bigint`，并且没有 `GlobalLimit/LocalLimit` `6`。为什么两个计划（`explain()` 与 Spark
    UI）不同？
- en: '11.2 Thinking about performance: Operations and memory'
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 考虑性能：操作和内存
- en: In this section, I cover some basic concepts about distributed data processing
    using PySpark. More specifically, I give a foundation on how to think about your
    program to simplify the logic and speed up the processing. Regardless of if you
    use the data frame API or rely on lower-level RDD operations (see chapter 8),
    you’ll gain useful vocabulary to describe the logic of your program and hints
    for troubleshooting a seemingly slow program.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了使用 PySpark 进行分布式数据处理的一些基本概念。更具体地说，我提供了一个基础，教你如何思考你的程序以简化逻辑并加快处理速度。无论你使用数据帧
    API 还是依赖于更底层的 RDD 操作（见第 8 章），你都将获得描述程序逻辑的有用词汇以及调试看似缓慢程序的建议。
- en: For this section, I use some of the data sets we’ve already encountered in previous
    chapters. I cover two important basic concepts when designing, coding, and profiling
    data pipelines.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我使用了我们在前几章中已经遇到的一些数据集。我介绍了在设计、编码和性能分析数据管道时两个重要的基本概念。
- en: First, I introduce the concept of *narrow* versus *wide* operation. Each transformation
    performed on a data frame (or an RDD) can be classified as either of these. Balancing
    narrow and wide operations is a tricky yet important aspect of making your data
    pipeline run faster.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我介绍了 *窄操作* 与 *宽操作* 的概念。对数据帧（或 RDD）执行的每个转换都可以归类为这两种之一。平衡窄操作和宽操作是使你的数据管道运行更快的一个棘手但重要的方面。
- en: Second, I discuss *caching* as a performance strategy and when it’s the right
    thing to do. Caching a data frame changes how Spark thinks about and optimizes
    the code for data transformation; by understanding the benefits and trade-offs,
    you will know when it’s appropriate to use it.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，我讨论了 *缓存* 作为性能策略，以及何时是正确使用它的时机。缓存数据帧会改变 Spark 对数据转换的思考方式和优化代码的方式；通过理解其优势和权衡，你将知道何时使用它是合适的。
- en: 11.2.1 Narrow vs. wide operations
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.1 窄操作与宽操作
- en: In this section, I introduce the concept of narrow and wide transformation.
    I show how they display in the Spark UI and how thinking about the order of your
    transformations can matter for your program performance.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我介绍了窄转换和宽转换的概念。我展示了它们如何在 Spark UI 中显示，以及思考转换顺序对你的程序性能可能很重要。
- en: In chapter 1, I explained that Spark will lazily think the transformation into
    a plan until an action is triggered. Once the action is submitted, the query optimizer
    will review the steps of the plan and reorganize them in a way that’s most efficient.
    We saw this in action with the optimized plan of listing 11.6, where we not only
    lumped the `regexp_extract()` and `lower()` operations into a single step, but
    duplicated that step (once for filtering, and once for the actual transformation).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我解释了Spark会惰性地将转换思考成计划，直到触发一个动作。一旦动作被提交，查询优化器将审查计划的步骤，并以最有效的方式重新组织它们。我们在列表11.6的优化计划中看到了这一点，其中我们不仅将`regexp_extract()`和`lower()`操作合并为单一步骤，而且还重复了该步骤（一次用于过滤，一次用于实际转换）。
- en: 'Spark knows it can do that because `regexp_extract()` and `lower()` are both
    narrow transformations. Simply put, a narrow transformation is a transformation
    on records that is indifferent to the actual data location. In other words, a
    transformation is considered narrow if it applies to each record independently.
    Our two previous examples are obviously narrow: extracting a regular expression
    or changing the case of a column can be done on a record-by-record basis; the
    order of the records and their physical location on the cluster do not matter.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: Spark知道它可以这样做，因为`regexp_extract()`和`lower()`都是窄转换。简单来说，窄转换是对记录的转换，它对实际数据位置不敏感。换句话说，如果一个转换独立应用于每条记录，则认为它是窄转换。我们之前的两个例子显然是窄转换：提取正则表达式或更改列的大小写可以在逐条记录的基础上完成；记录的顺序和它们在集群上的物理位置并不重要。
- en: '![](../Images/11-11.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-11.png)'
- en: Figure 11.11 A narrow transformation will apply without requiring the records
    to move across nodes. Spark can parallelize the operations across each nodes.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.11 窄转换将应用，而无需记录在节点之间移动。Spark可以在每个节点上并行化操作。
- en: 'Narrow transformations are very convenient when working in a distributed setting:
    they do not require any exchanging (or shuffling) of the records. Because of this,
    Spark will often club many sequential narrow transformations together into a single
    step. Since the data is sitting in RAM (or on the hard drive), reading the data
    only once (by performing multiple operations on each record) usually yields better
    performance than reading the same data multiple times and performing one operation
    each time. PySpark (versions 2.0 and above) can also leverage specialized CPU
    (and since Spark 3.0, GPU) instructions that speed up data transformation.[²](#pgfId-1016520)'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式环境中工作时，窄转换非常方便：它们不需要任何记录交换（或洗牌）。正因为如此，Spark通常会将许多连续的窄转换组合成单一步骤。由于数据位于RAM（或硬盘上），通过在每条记录上执行多个操作（仅读取一次数据）通常比多次读取相同数据并每次执行一个操作具有更好的性能。PySpark（2.0版本及以上）还可以利用专门的CPU（自Spark
    3.0以来，GPU）指令来加速数据转换。[²](#pgfId-1016520)
- en: One important caveat of narrow transformations is, well, that they cannot do
    everything. For instance, grouping a data frame based on the value of some records,
    getting the maximum value of a column, and joining two data frames based on a
    predicate require the data to be logically organized for the operation to succeed.
    The three previous examples are called wide transformations. Unlike their narrow
    counterpart, wide transformations need the data to be laid in a certain way between
    the multiple nodes. For this, Spark uses an exchange step to move the data for
    the operation to complete.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 窄转换的一个重要注意事项是，它们不能做任何事情。例如，根据某些记录的值对数据框进行分组、获取列的最大值以及根据谓词将两个数据框连接起来，都需要数据在逻辑上组织好才能成功执行操作。这三个先前的例子被称为宽转换。与它们的窄转换对应物不同，宽转换需要在多个节点之间以某种方式排列数据。为此，Spark使用交换步骤来移动数据以完成操作。
- en: In the word count example, the group by/count transformation was split into
    two stages, separated by an exchange. In the pre-exchange stage, Spark grouped
    the data on a node-by-node basis, so we ended the stage with each partition being
    grouped (see figure 11.12). Spark then exchanged the data across nodes—in our
    case, because the partition group by reduced the data size considerably, everything
    went to a single CPU core—and then finished the grouping. Spark was even clever
    enough to realize that we needed only five records, so it read only what was needed
    during the `shuffle` `read` operation (see section 11.1.3). Smart!
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在单词计数示例中，`group by/count` 转换被分成了两个阶段，由一个交换操作隔开。在交换前的阶段，Spark按节点逐节点地分组数据，因此我们在这个阶段结束时，每个分区都被分组了（见图11.12）。然后Spark在节点间交换数据——在我们的例子中，由于分区分组大大减少了数据量，所以所有数据都流向了单个CPU核心——然后完成了分组。Spark甚至足够聪明，意识到我们只需要五条记录，因此在`shuffle`
    `read`操作期间只读取所需的记录（见第11.1.3节）。真聪明！
- en: '![](../Images/11-12.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图11-12](../Images/11-12.png)'
- en: Figure 11.12 A wide transformation can happen in two stages because Spark needs
    to exchange data across nodes. Spark calls those necessary exchange operations
    *shuffles*.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.12 由于Spark需要在节点间交换数据，宽转换可以在两个阶段发生。Spark将这些必要的交换操作称为*洗牌*。
- en: Since we need to exchange/send data to the network, wide operations incur a
    performance cost not present in narrow operations. Part of making a data transformation
    program swift is understanding the balance between narrow and wide operations
    and how we can leverage the nature of both in our program.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们需要将数据交换/发送到网络，宽操作会带来窄操作中不存在的性能成本。使数据转换程序快速的部分是理解窄操作和宽操作之间的平衡，以及我们如何在程序中利用两者的特性。
- en: 'Spark’s query optimizer is getting smarter and smarter in reorganizing operations
    to maximize each narrow stage. In listing 11.8, I add three transformations to
    the word count example:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的查询优化器在重新组织操作以最大化每个窄阶段方面变得越来越聪明。在列表11.8中，我在单词计数示例中添加了三个转换：
- en: I keep only the words with over eight letters.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我只保留超过八个字母的单词。
- en: I group by the word length.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我按单词长度分组。
- en: I count the sum of the frequencies.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我计算频率的总和。
- en: Listing 11.8 A more complex word count illustrating the narrow versus wide operations
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.8 一个更复杂的单词计数示例，说明了窄操作与宽操作
- en: '[PRE7]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This is a good example of a poorly written program: I don’t need to group by
    words to then group again by word frequency. If we look at the physical plan,
    two things jump out.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个编写不佳程序的糟糕例子：我不需要先按单词分组，然后再按单词频率分组。如果我们查看物理计划，有两件事会跳出来。
- en: First, PySpark is smart enough to club the `.where(F.length(Fcol("word"))` `>`
    `8)` with the two previously identified narrow transformations. Second, PySpark
    is not smart enough to understand that the first `groupby()` is unnecessary. We
    have some potential room for improvement here. In listing 11.9, I modify the last
    few instructions, so my program accomplishes the same thing with less instruction.
    By removing the (useless) intermediate `groupby()`, I bring back the number of
    steps to three (check the codegen IDs) and therefore reduce the amount of work
    PySpark will have to perform.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，PySpark足够聪明，可以将`.where(F.length(Fcol("word")) > 8)`与之前确定的两个窄转换结合起来。其次，PySpark还不够聪明，无法理解第一个`groupby()`是不必要的。我们在这里有一些改进的潜力。在列表11.9中，我修改了最后几条指令，因此我的程序以更少的指令完成了相同的工作。通过移除（无用的）中间`groupby()`，我将步骤数量恢复到三个（检查codegen
    IDs），因此减少了PySpark需要执行的工作量。
- en: Listing 11.9 Reorganizing our extended word count program to avoid double counting
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.9 重新组织我们的扩展单词计数程序以避免重复计数
- en: '[PRE8]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Without going into bona fide benchmarking, a quick invocation of `timeit` (available
    on the iPython shell/Jupyter notebook) shows that the simplified program yields
    the same result approximately 54% faster. This is not surprising—via the physical
    plan (also available in the Spark UI), we know that the simplified version does
    less work and the code is more focused:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 不进行真正的基准测试，快速调用`timeit`（可在iPython shell/Jupyter笔记本上使用）显示，简化后的程序大约快了54%。这并不奇怪——通过物理计划（也在Spark
    UI中可用），我们知道简化版本做了更少的工作，代码更集中：
- en: '[PRE9]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: While this example might seem a little far-fetched, data pipelines tend to have
    this “append-only” code pattern, where you add more requirements, more work, and
    more code at the end of your chain of transformation. Using the information made
    available via the plans (both logical and physical), you can analyze the actual
    physical steps your code is going through and better understand the performance
    applications. This is not always easy to figure out when reading a complex data
    pipeline; multiple points of view help.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个例子可能有些牵强，但数据管道往往具有这种“只增不减”的代码模式，你在转换链的末尾添加更多要求、更多工作和更多代码。通过使用通过计划（逻辑和物理）提供的信息，你可以分析代码实际执行的物理步骤，更好地理解性能应用。在阅读复杂的数据管道时，这并不总是容易弄清楚；多个观点有助于理解。
- en: 'This section covered the concepts of narrow and wide transformations. We saw
    how to differentiate between the two and the implications of using them in our
    programs. We finally learned, in a practical way, how Spark reorganizes and clubs
    together narrow operations when optimizing query plans. In the next section, I
    introduce the single most misunderstood feature of PySpark: caching.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了窄转换和宽转换的概念。我们看到了如何区分这两者以及在使用它们时的含义。最后，我们以实际的方式学习了Spark在优化查询计划时如何重新组织和组合窄操作。在下一节中，我将介绍PySpark最被误解的功能：缓存。
- en: Exercise 11.3
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 11.3
- en: For the following operations, identify if they are narrow or wide and why.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下操作，确定它们是窄操作还是宽操作，以及原因。
- en: a) `df.select(...)`
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: a) `df.select(...)`
- en: b) `df.where(...)`
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: b) `df.where(...)`
- en: c) `df.join(...)`
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: c) `df.join(...)`
- en: d) `df.groupby(...)`
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: d) `df.groupby(...)`
- en: e) `df.select(F.max(...).over(...))`
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: e) `df.select(F.max(...).over(...))`
- en: '11.2.2 Caching a data frame: Powerful, but often deadly (for perf)'
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2.2 缓存数据帧：强大但往往致命（对性能而言）
- en: This section covers the caching of a data frame. I introduce what it is, how
    it works in Spark, and, most importantly, why you should be very careful about
    using it. Learning how, and especially when, to cache data is key to making your
    programs faster, but also to not making them slower than what they need to be.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了数据帧的缓存。我介绍了它是什么，如何在Spark中工作，最重要的是，为什么你应该非常小心地使用它。学习如何以及何时缓存数据对于使你的程序更快至关重要，但同时也不要使它们比所需的更慢。
- en: 'I showed in figure 11.4 that Spark splits memory into three zones: reserved,
    operational, and storage. By default, each PySpark job (transformations plus actions)
    is independent of one another. For instance, if we `show()` the `results` data
    frame of our word count example five times, Spark will read the data from source
    and transform the data frame five times. While this seems like a highly inefficient
    way of working, bear in mind that data pipelines most often “flow” the data from
    one transformation to the next (hence the pipeline analogy); keeping intermediate
    states is useless and wasteful.'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 我在图11.4中展示了Spark将内存分为三个区域：预留、操作和存储。默认情况下，每个PySpark作业（转换加操作）都是相互独立的。例如，如果我们对单词计数示例的`results`数据帧进行五次`show()`操作，Spark将五次从源读取数据并转换数据帧。虽然这看起来像是一种非常低效的工作方式，但请记住，数据管道通常“流动”数据从一个转换到下一个转换（因此有管道类比）；保持中间状态是无用且浪费的。
- en: Caching changes this. A cached data frame will be serialized to the storage
    memory, which means that retrieving it will be speedy. The trade-off is that you
    take up RAM space on your cluster. In the case of very large data frames, that
    means that some data might spill to disk (leading to a slower retrieval) and that
    your cluster might run slower if you are using memory-heavy processing.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存改变了这一点。缓存的数据帧将被序列化到存储内存中，这意味着检索它将会很快。权衡的是，你会在你的集群上占用RAM空间。在非常大的数据帧的情况下，这意味着一些数据可能会溢出到磁盘（导致检索速度变慢），而且如果你使用的是内存密集型处理，你的集群可能会运行得更慢。
- en: '![](../Images/11-13.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-13.png)'
- en: Figure 11.13 Caching a data frame in our word count program. In this case, the
    second action does not compute the chain of transformations from the `spark.read`
    operation and leverages the cached `df_int` data frame instead.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 在我们的单词计数程序中缓存数据帧。在这种情况下，第二次操作没有计算从`spark.read`操作开始的转换链，而是利用了缓存的`df_int`数据帧。
- en: To cache a data frame, you call its `cache()` method. Because it is a transformation,
    `cache()` will not do anything immediately and waits for an action to be called.
    Once you submit an action, Spark will compute the whole data frame and cache it
    to memory, using disk space if needed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要缓存一个数据帧，你调用它的`cache()`方法。因为它是一个转换，`cache()`不会立即执行，而是等待调用一个动作。一旦你提交一个动作，Spark将计算整个数据帧并将其缓存到内存中，如果需要的话，也会使用磁盘空间。
- en: In the Executors tab, you can also check how much memory storage is being used.
    An uncached data frame will take a little space (because each executor keeps the
    instructions to recompute a data frame on the fly), but nowhere as much as caching
    the data frame.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行器选项卡中，你还可以检查正在使用的内存存储量。未缓存的数据帧会占用一点空间（因为每个执行器都会保留即时重新计算数据帧的指令），但远不如缓存数据帧占用空间多。
- en: 'Persisting: Caching, but with more control'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 持久化：缓存，但具有更多控制
- en: By default, a data frame will be cached using the `MEMORY_AND_DISK` policy,
    which means that the storage RAM will be used as a priority, falling back to disk
    if we run out of memory. An RDD will use the `MEMORY_ONLY` policy, which means
    that it won’t use the disk at all for storage. If we don’t have enough storage
    RAM, Spark will recompute the RDD from scratch (negating the effects of caching).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，数据帧将使用`MEMORY_AND_DISK`策略进行缓存，这意味着存储RAM将优先使用，如果内存不足，则会回退到磁盘。RDD将使用`MEMORY_ONLY`策略，这意味着它根本不会使用磁盘进行存储。如果我们没有足够的存储RAM，Spark将从头开始重新计算RDD（抵消缓存的效果）。
- en: If you want more control over how your data is cached, you can use the `persist()`
    method, passing the level (as a string) as a parameter. Beyond `MEMORY_ONLY` and
    `MEMORY_AND_DISK`, you can also opt for `DISK_ONLY`, which foregoes RAM to go
    straight to disk. You can also add a `_2` suffix (e.g., `MEMORY_ONLY_2`), which
    will use the same heuristic but duplicate each partition over two nodes.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要更多控制数据缓存的方式，你可以使用`persist()`方法，将级别（作为字符串）作为参数传递。除了`MEMORY_ONLY`和`MEMORY_AND_DISK`之外，你也可以选择`DISK_ONLY`，这意味着它将直接跳过RAM而直接写入磁盘。你还可以添加一个`_2`后缀（例如，`MEMORY_ONLY_2`），这将使用相同的启发式方法，但将每个分区复制到两个节点上。
- en: If you can afford the RAM, I suggest using it as much as possible. RAM access
    is orders of magnitude faster than disk. The actual decision will depend on your
    Spark instance configuration.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能负担得起RAM，我建议尽可能多地使用它。RAM的访问速度比磁盘快得多。实际的决定将取决于你的Spark实例配置。
- en: '![](../Images/11-14.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-14.png)'
- en: Figure 11.14 The `results` data frame, successfully cached. Because of the size,
    everything fits into the RAM.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 `results`数据帧，成功缓存。由于大小，所有内容都适合在RAM中。
- en: 'Caching looks like a very useful functionality: it provides an insurance policy,
    so you don’t have to recompute a data frame from scratch if you want to backtrack.
    In practice, this scenario happens very rarely:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 缓存看起来是一个非常有用的功能：它提供了一种保险政策，这样你就不必从头开始重新计算数据帧。在实践中，这种情况很少发生：
- en: Caching takes computing and memory resources that are not available for general
    processing.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存需要计算和内存资源，这些资源对于一般处理是不可用的。
- en: Computing a data frame can sometimes be faster than retrieving it from cache.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算一个数据帧有时可能比从缓存中检索它更快。
- en: 'In a noninteractive program, you seldom need to reuse a data frame more than
    a few times: caching brings no value if you don’t reuse the exact data frame more
    than once.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在非交互式程序中，你很少需要多次重用数据帧：如果你不多次重用确切的数据帧，缓存就没有价值。
- en: In other words, mindless caching will most often hinder your program’s performance.
    Now that I’ve done my public service announcement about how aggressive caching
    will harm you, what are some cases where you want to cache? I have witnessed two
    common use cases where caching is useful.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，无脑缓存通常会阻碍你的程序性能。既然我已经做了关于如何激进的缓存会对你有害的公共服务公告，那么有哪些情况下你想缓存呢？我见证过两种常见的使用场景，其中缓存是有用的。
- en: First, caching is useful when you are experimenting with a data frame that (a)
    fits into memory and (b) needs to refer to the *entire* cached data frame more
    than a few times. In the case of interactive development (where you are using
    the REPL to iterate quickly over the same data frame), caching will provide a
    noticeable increase in speed because you won’t have to read from source every
    time.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，当你正在实验一个数据帧，该数据帧（a）适合内存，并且（b）需要多次引用整个缓存数据帧时，缓存是有用的。在交互式开发的情况下（你使用REPL快速迭代相同的数据帧），缓存将提供明显的速度提升，因为你不必每次都从源读取。
- en: Second, caching is extremely useful when you are training an ML model on Spark.
    ML model fitting will use the training data set multiple times, and recomputing
    it from scratch is unwieldy. In chapter 13, you’ll notice that I casually cache
    my data frames before training.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，当你在Spark上训练ML模型时，缓存非常有用。ML模型拟合将多次使用训练数据集，从头开始重新计算则非常不便。在第13章中，你会注意到我在训练之前随意缓存了我的数据框。
- en: 'As a rule of thumb, before caching, ask yourself this: Do I need this whole
    piece of data more than a handful of times? In most noninteractive data-processing
    programs, the answer will be no, and caching will do more harm than good. In the
    case where the answer is yes, experiment with the different levels of caching
    (RAM versus disk versus both) to see which one fits your program/Spark instance
    the most.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一条经验法则，在缓存之前，问问自己：我是否需要这整个数据集超过几次？在大多数非交互式数据处理程序中，答案将是否定的，缓存可能会弊大于利。如果答案是肯定的，尝试不同的缓存级别（RAM、磁盘或两者结合）以查看哪一个最适合你的程序/Spark实例。
- en: This section has covered the dark art of caching and why less is more. In this
    chapter, I introduced the Spark UI and uncovered the information Spark provides
    at runtime (and after the fact) to help you make better decisions about the performance
    of your programs. Having this handy while you are constructing a data pipeline
    will provide invaluable feedback on the behavior of your code and help you make
    better performance decisions.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了缓存的黑暗艺术以及为什么少即是多。在本章中，我介绍了Spark UI，并揭示了Spark在运行时（以及事后）提供的信息，以帮助您更好地决定程序的性能。在构建数据管道时拥有这些信息将提供宝贵的反馈，有助于您做出更好的性能决策。
- en: Before ending the chapter, I want to stress that obsessing about performance
    right from the start won’t do you any good. Spark provides many optimizations
    out of the box—which you can see when analyzing the logical and physical plans—and
    will provide good performance. When writing a PySpark program, make it *work*
    first, then make it *clean*, and then make it *fast*, using the Spark UI to help
    you along the way. Data pipelines, whether they are for ETL or ML, gain a lot
    from being easy to reason about. Shedding a few minutes of a program is not worth
    it if it takes you a day to decipher what the program does!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束本章之前，我想强调的是，从一开始就过分关注性能并不会对你有任何好处。Spark提供了许多开箱即用的优化——你可以在分析逻辑和物理计划时看到这些优化——并将提供良好的性能。在编写PySpark程序时，首先让它*工作*，然后让它*整洁*，最后让它*快速*，使用Spark
    UI来帮助你一路前行。数据管道，无论是用于ETL还是ML，如果易于推理，将获得很多好处。如果需要花费一天时间来解析程序的功能，那么节省几分钟的时间是不值得的！
- en: Summary
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Spark uses RAM (or memory) to store data (storage memory), as well as for processing
    (operational memory) data. Providing enough memory is paramount in the fast processing
    of Spark jobs and can be configured in the `SparkSession` initialization.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark使用RAM（或内存）来存储数据（存储内存），以及用于处理数据（操作内存）。在快速处理Spark作业时，提供足够的内存至关重要，并且可以在`SparkSession`初始化中进行配置。
- en: The Spark UI provides useful information about cluster configuration. This includes
    memory, CPU, libraries, and OS information.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark UI提供了有关集群配置的有用信息。这包括内存、CPU、库和操作系统信息。
- en: A Spark job consists of a series of transformations and one action. Job progress
    is available in the Job tab of the Spark UI when processing.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark作业由一系列转换和一个动作组成。当处理时，作业进度可以在Spark UI的作业标签页中查看。
- en: A job is split into stages, which are the logical units of work on a cluster.
    Stages are split by exchange operations, which are when the data moves around
    worker nodes. We can look at the stages and steps via the SQL tab in the Spark
    UI and via the `explain()` method over the resulting data frame.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作业被分割成阶段，这些是集群上工作的逻辑单元。阶段通过交换操作分割，这是数据在工作节点之间移动的时候。我们可以通过Spark UI的SQL标签页以及通过`explain()`方法查看结果数据框来查看阶段和步骤。
- en: A stage consists of narrow operations that are optimized as a unit. Wide operations
    may require a shuffle/exchange if the necessary data is not local to a node.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个阶段由一系列优化为单位的窄操作组成。如果必要的数据不是节点本地的，宽操作可能需要洗牌/交换。
- en: Caching moves data from the source to the storage memory (with an option to
    spill to disk if there isn’t enough memory available). Caching interferes with
    Spark’s ability to optimize and is usually not needed in a pipeline-like program.
    It is appropriate when reusing a data frame multiple times, such as during ML
    training.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缓存将数据从源移动到存储内存（如果可用内存不足，可以选择溢出到磁盘）。缓存会干扰Spark的优化能力，在类似管道的程序中通常不需要。当多次重用数据帧时，例如在机器学习训练期间，它是合适的。
- en: '* * *'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ If you are using ephemeral clusters (i.e., spinning up a cluster for a specific
    job and destroying it afterward), you usually don’t need to worry about cluster
    managers, as they are set up for you.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果你正在使用临时集群（即，为特定工作启动集群并在之后销毁它），通常你不需要担心集群管理器，因为它们已经为你设置好了。
- en: ² One example is via using SIMD (*single instruction, multiple data*) instructions
    and loop unrolling. If you are interested in knowing more, look at the release
    notes of the Spark Tungsten project.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ² 一个例子是通过使用SIMD（单指令多数据）指令和循环展开。如果你想了解更多信息，请查看Spark Tungsten项目的发布说明。
