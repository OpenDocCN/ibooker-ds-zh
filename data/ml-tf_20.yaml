- en: 17 LSTMs and automatic speech recognition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17个LSTMs和自动语音识别
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Preparing a dataset for automatic speech recognition using the LibriSpeech corpus
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用LibriSpeech语料库为自动语音识别准备数据集
- en: Training a long short-term memory (LSTM) RNN for converting speech to text
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练长短期记忆（LSTM）RNN将语音转换为文本
- en: Evaluating the LSTM performance during and after training
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练期间和之后评估LSTM性能
- en: Speaking and talking to your electronic devices is commonplace nowadays. Years
    ago, on an early version of my smartphone, I clicked the microphone button and
    used its dictation function to try to speak an email into existence. The email
    that my boss received at work had a whole bunch of typos and phonetic errors,
    though, and he wondered whether I was mixing a little too much after-work activity
    with my official duties!
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用电子设备说话和交谈已经司空见惯了。几年前，在我智能手机的早期版本上，我点击了麦克风按钮，并使用它的语音输入功能尝试将一封电子邮件说成现实。然而，我老板收到的电子邮件中却有一大堆拼写错误和音标错误，他想知道我是不是把太多的下班活动与我的正式职责混合在一起了！
- en: The world has evolved, and so has the accuracy of neural networks in performing
    *automatic speech recognition* (ASR), which is the process of transforming spoken
    audio into written text. Whether you are using your phone’s intelligent digital
    assistant to ask it to schedule a meeting for you, dictating that trusty email,
    or asking your smart device at home to order something, play background music,
    or even start your car, the tasks are powered by ASR functionality.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 世界在变化，神经网络在执行自动语音识别（ASR）方面的准确性也在提高，ASR是将语音音频转换为书面文本的过程。无论你是使用手机的智能数字助手为你安排会议，口述那封可靠的电子邮件，还是要求家里的智能设备订购东西，播放背景音乐，甚至启动你的汽车，这些任务都是由ASR功能驱动的。
- en: How did ASR become part of everyday life? Previous ASR systems relied on brittle
    statistical models that depended on language-specific grammars, whereas today’s
    ASR systems are built on top of robust recurrent neural networks (RNNs) and in
    particular on long short-term memory (LSTM) networks, which are a specific type
    of RNN. Using LSTMs, you can teach a computer to hear audio in segments and to
    convert those segments to language characters over time. Like convolutional neural
    networks (CNNs) and other biologically inspired neural networks discussed in this
    book, LSTMs for speech recognition learn the way humans learn. Each small audio
    segment corresponds to a character of language, and you use as many cells as there
    are characters in the speech. You are trying teach the network to understand language
    the way a human would. As the network learns through each invocation, the network
    weights are updated and fed forward and backward to the LSTM cells, which learn
    each sound and the letters to which it corresponds. The combination of letters
    mapped to sound becomes language.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 语音识别（ASR）是如何成为日常生活一部分的？之前的ASR系统依赖于脆弱的统计模型，这些模型依赖于特定语言的语法，而今天的ASR系统建立在鲁棒的循环神经网络（RNNs）之上，特别是长短期记忆（LSTM）网络，这是一种特定的RNN。使用LSTMs，你可以教会计算机分段听音频，并随着时间的推移将这些段转换为语言字符。像书中讨论的卷积神经网络（CNNs）和其他生物启发的神经网络一样，用于语音识别的LSTMs学习人类的学习方式。每个小的音频段对应于语言中的一个字符，你使用的细胞数量与语音中的字符数量相同。你试图教会网络以人类的方式理解语言。随着网络通过每次调用进行学习，网络权重被更新并向前和向后传递到LSTM细胞中，这些细胞学习每个声音及其对应的字母。映射到声音的字母组合成为语言。
- en: These approaches were made famous by the Deep Speech architecture from Baidu,
    used to outperform the state-of-the-art speech recognition system in 2015 and
    later implemented in open source by the Mozilla Foundation, using your and my
    favorite toolkit, TensorFlow. The original paper is at [https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567).
    I’ll show you how to collect and prepare the training data for the deep speech
    automated speech-to-text model, how to train it with TensorFlow, and how to use
    it to evaluate real-world sound data.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法因百度Deep Speech架构而闻名，该架构在2015年超越了最先进的语音识别系统，后来由Mozilla基金会开源实现，使用你和我最喜欢的工具包TensorFlow。原始论文在[https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567)。我将向你展示如何收集和准备用于深度语音自动语音到文本模型的训练数据，如何使用TensorFlow对其进行训练，以及如何使用它来评估现实世界的声音数据。
- en: Hey, TensorFlow!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 嘿，TensorFlow！
- en: Ever wonder how your phone works when you say, “Hey, Digital Assistant”? At
    first, the assistant may not always get the words right, but Silicon Valley phone
    and computer makers say it will get better over time. Well, they are right. That’s
    why a digital assistant asks for feedback about whether it interpreted words right.
    Frameworks like TensorFlow allow you to train your own model and also provide
    pretrained LSTM models that refine per-character and per-word ASR outputs with
    user feedback from millions of people around the world. In fact, recognition does
    get better over time.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 当你对着手机说“嘿，数字助手”时，你是否好奇手机是如何工作的？起初，助手可能并不总是能正确理解你的话，但硅谷的智能手机和电脑制造商表示，随着时间的推移，它将变得更好。没错，这就是为什么数字助手会询问是否正确理解了你的话。TensorFlow
    等框架允许你训练自己的模型，并提供经过数百万来自世界各地用户反馈的预训练 LSTM 模型，这些模型可以细化每个字符和每个单词的 ASR 输出。实际上，识别确实会随着时间的推移而变得更好。
- en: One common source of training for ASR networks is audiobooks. Audiobooks are
    useful in that they typically have both a parallel corpus of sound and transcripts
    mapping to the spoken words. The LibriSpeech corpus is part of the Open Speech
    and Language Resources (OpenSLR) project and can be used to train the deep-speech
    model. As you know by now, however, you’ll have to take care of some data cleaning
    to get the information ready for training.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ASR 网络训练的一个常见来源是有声读物。有声读物很有用，因为它们通常既有声音的并行语料库，也有与说话词对应的转录。LibriSpeech 语料库是 Open
    Speech and Language Resources (OpenSLR) 项目的一部分，可以用来训练深度语音模型。然而，正如你所知，你需要进行一些数据清理，以便为训练准备信息。
- en: 17.1 Preparing the LibriSpeech corpus
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 准备 LibriSpeech 语料库
- en: Audiobooks are useful inventions that allow us to listen to our favorite books
    while driving and doing other activities. They typically are large sets of sounds—possibly
    hundreds of hours’ worth—broken into smaller snippets and almost always include
    corresponding transcriptions in case you want to read the text you are hearing.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有声读物是有用的发明，它允许我们在开车和其他活动时听我们喜欢的书籍。它们通常是大量声音——可能是数百小时——分成更小的片段，并且几乎总是包含相应的转录，以防你想阅读你正在听到的文本。
- en: One set of open-source audiobooks is available from the Open Speech and Language
    Resources (OpenSLR) webpage and the LibriSpeech corpus. LibriSpeech is a set of
    short clips from audiobooks and transcripts to go with those clips. LibriSpeech
    includes more than 1,000 hours of recorded 16KHz English-speech audio, including
    metadata; original MP3 files; and a separated and an aligned training set of 100,
    360, and 500 hours of speech. The dataset includes transcriptions, along with
    a dev dataset for per-epoch validation and a test set for post-training testing.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 一套开源的有声读物可以从 Open Speech and Language Resources (OpenSLR) 网页和 LibriSpeech 语料库中获取。LibriSpeech
    是一组来自有声读物的短剪辑和相应的转录。LibriSpeech 包括超过 1,000 小时的 16KHz 英语语音录音，包括元数据；原始 MP3 文件；以及
    100、360 和 500 小时的语音训练集的分离和校对版本。该数据集包括转录，还有一个用于每个训练周期验证的开发数据集和一个用于训练后测试的测试集。
- en: Unfortunately, the dataset isn’t usable in the deep-speech model because the
    model expects the Windows Audio Video (.wav) interleaved file audio format instead
    of the Free Lossless Audio Codec (.flac) file format that LibriSpeech comes in.
    As usual, your first step for machine learning involves—you guessed it—data preparation
    and cleaning.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 很不幸，由于深度语音模型期望的是 Windows 音频视频 (.wav) 交错文件音频格式，而不是 LibriSpeech 所使用的 Free Lossless
    Audio Codec (.flac) 文件格式，因此数据集在深度语音模型中不可用。通常，你的第一步是——没错——数据准备和清理。
- en: 17.1.1 Downloading, cleaning, and preparing LibriSpeech OpenSLR data
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.1 下载、清理和准备 LibriSpeech OpenSLR 数据
- en: 'First, you need to download one of the training corpora: the 100, 360, or 500
    hours of training. Depending on how much memory you have, you can choose any of
    the three, but my recommendation is to grab the 100 hours because that’s plenty
    to train a decent deep-speech model. There is only one dev (validation) and test
    set, so you don’t need to choose different hours for those files.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你需要下载一组训练语料库：100、360 或 500 小时的训练数据。根据你的内存大小，你可以选择其中任何一个，但我的建议是选择 100 小时，因为这对于训练一个不错的深度语音模型来说已经足够了。只有一个开发（验证）集和一个测试集，所以你不需要为这些文件选择不同的时间长度。
- en: 'The overall process of preparing the downloaded LibriSpeech data is fairly
    straightforward:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 准备下载的 LibriSpeech 数据的整体过程相当直接：
- en: Download the tarballs for train-100-clean, dev-clean, and test-from [http://www
    .openslr.org/12](http://www.openslr.org/12).
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 train-100-clean、dev-clean 和 test 的 tarball，请从 [http://www.openslr.org/12](http://www.openslr.org/12)。
- en: Unpack the tarballs into LibriSpeech/train-clean-100, LibriSpeech/dev-clean,
    and LibriSpeech/test-clean folders.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将tar文件解压缩到LibriSpeech/train-clean-100、LibriSpeech/dev-clean和LibriSpeech/test-clean文件夹中。
- en: Convert the .flac audio files to .wav audio files for training, validation,
    and test.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将.flac音频文件转换为训练、验证和测试的.wav音频文件。
- en: Take the aggregated transcript files, each of which contains one line for each
    audio file in the chapter. Each line contains the extracted words corresponding
    to the referenced short sound clip. Reformat these aggregate transcripts as one
    .txt transcript file per .wav audio file.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将章节中的每个音频文件对应的一行聚合转录文件取来，每个文件包含一行音频文件。每行包含与引用的短声音剪辑对应的提取出的单词。将这些聚合转录重新格式化为每个.wav音频文件一个.txt转录文件。
- en: Collect the subfolders of .wav and .txt audio/transcript tuples into one flat
    folder structure, and clean up the aggregated transcript and .flac files by removing
    them.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将.wav和.txt音频/转录元组的子文件夹收集到一个扁平的文件夹结构中，并通过删除聚合转录和.flac文件来清理。
- en: The process is shown from left to right in figure 17.1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程在图17.1中从左到右展示。
- en: '![CH17_F01_Mattmann2](../Images/CH17_F01_Mattmann2.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![CH17_F01_Mattmann2](../Images/CH17_F01_Mattmann2.png)'
- en: Figure 17.1 The data cleaning and preparation process that transforms the LibriSpeech
    OpenSLR data for the deep-speech model
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.1 将LibriSpeech OpenSLR数据转换为用于深度语音模型的清洗和准备过程
- en: The good news is that you can build this data cleaning and preparation pipeline
    with some simple Python utility code. You’ll start in listing 17.1 with the file
    downloads, using the `urllib` and `tarfile` libraries, which allow you to download
    remote URLs and untar the archive files.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，您可以使用一些简单的Python实用代码构建这个数据清洗和准备流程。您将从列表17.1开始，使用`urllib`和`tarfile`库进行文件下载，这些库允许您下载远程URL并解压缩存档文件。
- en: warning Downloading this data can take quite a while, because the training data
    alone is ~7 GB. Be prepared to wait for hours, depending on your bandwidth.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：下载这些数据可能需要相当长的时间，因为仅训练数据就大约有7 GB。请准备好等待数小时，具体取决于您的带宽。
- en: Listing 17.1 Downloading and untarring the train, dev, and test LibriSpeech
    data
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.1 下载并解压缩训练、开发和测试LibriSpeech数据
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the urllib downloading library and tarfile library for extracting
    the archives
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入urllib下载库和tarfile库以提取存档
- en: ❷ Creates a function to download a tarfile from a URL and extract it locally
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个函数从URL下载tar文件并在本地提取
- en: ❸ OpenSLR 100 hours train, dev (validation), and test sets
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ OpenSLR 100小时训练、开发（验证）和测试集
- en: ❹ Downloads the data and extracts the stream to its local folders
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 下载数据并将流提取到其本地文件夹
- en: For the next step in the data preparation pipeline, you’ll need to convert the
    .flac audio files to .wav files. Luckily, an easy-to-use Python library called
    `pydub` can perform this task, as well as other conversions and manipulations
    of multimedia files. Though `pydub` is robust, you are using only a subset of
    its features here. Play around with it to discover more.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据准备流程的下一步，您需要将.flac音频文件转换为.wav文件。幸运的是，有一个易于使用的Python库叫做`pydub`可以执行此任务，以及其他多媒体文件的转换和操作。尽管`pydub`功能强大，但您在这里只使用了其功能的一个子集。尝试使用它来发现更多功能。
- en: 17.1.2 Converting the audio
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.2 转换音频
- en: When the tar files are extracted, they appear locally in your LibriSpeech/<data>
    directory where <data> is one of train-clean-100, dev-clean, and test-clean. Below
    those folders are more subfolders, which correspond to different chapter numbers
    and even more subfolders corresponding to sections of chapters. So your code will
    need to traverse these subfolders and for each .flac file use pydub to create
    a .wav file. You’ll handle the process as shown in listing 17.2\. Go get a cup
    of coffee if you are running this code on your laptop; the conversion can take
    up to an hour and a half.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当tar文件被提取后，它们将出现在您的LibriSpeech/<data>目录中，其中<data>是train-clean-100、dev-clean和test-clean之一。在这些文件夹下面是更多子文件夹，它们对应不同的章节编号，还有更多子文件夹对应章节的部分。因此，您的代码需要遍历这些子文件夹，并为每个.flac文件使用pydub创建.wav文件。您将按照列表17.2中所示处理该过程。如果您在笔记本电脑上运行此代码，不妨去倒杯咖啡；转换可能需要长达一个半小时。
- en: Listing 17.2 Converting .flac files to .wav files and traversing datasets
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.2 将.flac文件转换为.wav文件并遍历数据集
- en: '[PRE1]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Given a file path such as LibriSpeech/train-clean-100/307/127535/307-127535-000.flac,
    get its directory name (base_file_path) and filename.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 给定一个文件路径，例如LibriSpeech/train-clean-100/307/127535/307-127535-000.flac，获取其目录名（base_file_path）和文件名。
- en: ❷ Strips the extension and gets the file base name, such as 307-127535-000
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 移除扩展名并获取文件基本名称，例如307-127535-000
- en: ❸ Reads the FLAC file using Pydub
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 Pydub 读取 FLAC 文件
- en: ❹ Derives the .wav filename, which is basename + .wav
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 推导出 .wav 文件名，即 basename + .wav
- en: ❺ Uses Pydub to save the new .wav file
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用 Pydub 保存新的 .wav 文件
- en: ❻ Uses the glob library to get a list of all .flac files in train, dev (validation),
    and test
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用 glob 库获取 train、dev（验证）和 test 中所有 .flac 文件的列表
- en: ❼ Processes the train, dev and test .flac files to .wav
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将 train、dev 和 test 的 .flac 文件转换为 .wav
- en: 'With the audio files in the right format, you have to handle the other parts
    of Libri-Speech: the transcripts from step 4 mentioned previously with one text
    file transcript per audio clip. As they come with the dataset, the transcripts
    are aggregated into subchapter collections, one transcript per subchapter, with
    each line in the transcript subchapter file corresponding to one audio file in
    that subdirectory. For deep speech, you need one text file transcript per audio
    file. Next, you’ll generate per-audio transcripts, one per file.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当音频文件格式正确时，你必须处理 Libri-Speech 的其他部分：之前提到的步骤 4 中的转录文本，每个音频剪辑一个文本文件转录。由于它们与数据集一起提供，转录文本被聚合到子章节集合中，每个子章节一个转录文本，转录子章节文件中的每一行对应于该子目录中的一个音频文件。对于深度语音，你需要每个音频文件一个文本文件转录。接下来，你将生成每个音频文件的转录文本，每个文件一个。
- en: 17.1.3 Generating per-audio transcripts
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.3 生成每个音频文件的转录文本
- en: To create per-audio transcripts, you read each subchapter directory transcript
    aggregation file. Break each line in that file into a separate text transcript
    file, one per audio file, in the subchapter folder. The simple Python in listing
    17.3 handles this task for you. Note that running this code is fairly quick compared
    with the .flac file conversion.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建每个音频文件的转录文本，你需要读取每个子章节目录的转录聚合文件。将该文件中的每一行分解成单独的文本转录文件，每个音频文件一个，在子章节文件夹中。列表
    17.3 中的简单 Python 代码为你处理了这个任务。请注意，与 .flac 文件转换相比，运行此代码相当快。
- en: Listing 17.3 Breaking the subchapter aggregations into single .wav file transcripts
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.3 将子章节聚合分解成单个 .wav 文件转录文本
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Gets the subchapter prefix filename
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取子章节前缀文件名
- en: ❷ For each line in the subchapter aggregation transcript, splits on whitespace,
    and uses the first token as the audio filename for that line’s transcript
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对于子章节聚合转录文本中的每一行，根据空格分割，并使用第一个标记作为该行转录的音频文件名
- en: ❸ The rest of the tokens besides the first are the transcript words for that
    audio file.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 除了第一个标记之外的其他标记是该音频文件的转录单词。
- en: ❹ Gets the transcript files for all sub chapters in train, dev, and test
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 获取 train、dev 和 test 中所有子章节的转录文件
- en: ❺ Processes train, dev, and test into individual transcripts
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将 train、dev 和 test 处理成单独的转录文本
- en: 17.1.4 Aggregating audio and transcripts
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.4 聚合音频和转录文本
- en: 'The final step in the data processing—step 5, if you are keeping score—is collecting
    all the audio .wav files and their associated transcripts into an aggregated top-level
    directory and cleaning up by removing the .flac files and the aggregated transcripts.
    A simple move file function and a delete function call in Python can take care
    of this task for you, as shown in listing 17.4\. You can use the following simple
    paths to represent the dataset top-level paths for aggregation:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 数据处理的最后一步（如果你在计分的话是步骤 5）是收集所有音频 .wav 文件及其相关的转录文本到一个聚合的顶级目录，并通过删除 .flac 文件和聚合的转录文本进行清理。Python
    中的简单移动文件函数和删除函数调用可以为你处理这个任务，如列表 17.4 所示。你可以使用以下简单的路径来表示数据集顶级路径的聚合：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The code in listing 17.4 ought to run fairly quickly because it’s cleanup. It’s
    important to note that you have to run the cleanup steps because you do not want
    to double-count transcripts or audio files.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.4 中的代码应该运行得相当快，因为它只是清理工作。重要的是要注意，你必须运行清理步骤，因为你不希望转录文本或音频文件被重复计数。
- en: Listing 17.4 Aggregating audio and transcripts, and cleaning up
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.4 聚合音频和转录文本，并清理
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ If the target path doesn’t exist, create it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果目标路径不存在，则创建它。
- en: ❷ Walks the top-level train, dev, and test directories, and for each audio and
    transcript file in them (assuming that you delete the .flac files first), moves
    them to the target path
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历顶级 train、dev 和 test 目录，并将每个音频和转录文件（假设首先删除了 .flac 文件）移动到目标路径
- en: ❸ Crawls through a directory and removes all files with the provided extension
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历目录并删除所有具有提供扩展名的文件
- en: ❹ Removes the .flac files
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 删除 .flac 文件
- en: ❺ Moves the audio .wav and associated transcripts to top-level directories
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将音频 .wav 文件和相关的转录文本移动到顶级目录
- en: ❻ Removes the aggregated transcripts
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 删除聚合的转录文本
- en: Armed with the prepared LibriSpeech dataset aggregations in train-clean-100-all,
    dev-clean-all, and test-clean-all, you’re ready to start training with deep speech
    and TensorFlow.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 凭借准备好的LibriSpeech数据集聚合（train-clean-100-all、dev-clean-all和test-clean-all），你就可以开始使用深度语音和TensorFlow进行训练了。
- en: 17.2 Using the deep-speech model
  id: totrans-70
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 使用深度语音模型
- en: Deep Speech is a neural network built in 2014 and 2015 by Baidu in the course
    of trying to make search better. Baidu is a company in China that offers internet
    services and products; it collects and crawls the web and uses artificial intelligence
    to enhance users’ experience of finding what they are looking for. So it’s safe
    to say that Baidu has a ton of data, including speech data. The need to allow
    for dictation to aid search on mobile devices was increasing in 2015 and is expected
    as a default capability today.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 深度语音是一个在2014年和2015年由百度在尝试改进搜索过程中构建的神经网络。百度是一家提供互联网服务和产品的中国公司；它收集和爬取网络，并使用人工智能来增强用户寻找他们所需内容的体验。因此，可以说百度拥有大量的数据，包括语音数据。在2015年，允许在移动设备上进行语音输入以辅助搜索的需求不断增加，并且现在被视为默认功能。
- en: 'Baidu’s paper “Deep Speech: Scaling Up End-to-End Speech Recognition” (available
    at [https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567)) describes
    a multilayered neural network that combines convolutional layers as feature extractors
    from input audio spectrograms that have been segmented into character level utterances.
    Each input audio file is segmented into an utterance corresponding to a letter
    in the alphabet. After feature extraction, the output features are provided to
    a bidirectional LSTM network that feeds forward its output weights to a backward
    layer, which in turn feeds its outputs as it learns. The outputs of the bidirectional
    backward layer are fed into an additional feature extractor and used to predict
    a particular character in the alphabet to which the utterance corresponds. The
    overall architecture is shown in figure 17.2.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 百度的论文“深度语音：端到端语音识别的扩展”（可在[https://arxiv.org/abs/1412.5567](https://arxiv.org/abs/1412.5567)找到）描述了一个多层神经网络，该网络将卷积层作为从分割到字符级别语音的输入音频频谱图的特征提取器。每个输入音频文件被分割成与字母表中一个字母相对应的语音。在特征提取之后，输出特征被提供给一个双向LSTM网络，该网络将其输出权重向前传递到一个反向层，该反向层反过来将其输出作为其学习的一部分。双向反向层的输出被提供给一个额外的特征提取器，并用于预测与语音相对应的字母表中的特定字符。整体架构如图17.2所示。
- en: '![CH17_F02_Mattmann2](../Images/CH17_F02_Mattmann2.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![CH17_F02_Mattmann2](../Images/CH17_F02_Mattmann2.png)'
- en: Figure 17.2 The deep-speech model. Audio is fed in and segmented into samples
    corresponding to character-level utterances. Those utterances are guessed by a
    bidirectional LSTM network (MFCC = Mel frequency cepstral coefficient).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.2 深度语音模型。音频输入并被分割成与字符级别语音对应的样本。这些语音由双向LSTM网络（MFCC = 梅尔频率倒谱系数）猜测。
- en: One of the most famous implementations of the deep-speech architecture was undertaken
    by the Mozilla Foundation and implemented with TensorFlow; it is available at
    [https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech).
    Rather than reimplement all the bells and whistles of this implementation, which
    would require much more space and cover more than a few chapters of this book,
    I am going to highlight the important bits to walk you through setting up and
    running a simplified version of the code. Note that training a deep-speech model
    from scratch will take quite a few days, even with a GPU.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 深度语音架构最著名的实现之一是由Mozilla基金会承担的，并使用TensorFlow实现；它可在[https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech)找到。鉴于重实现这个实现的所有功能将需要更多的空间，并覆盖本书的几个章节，我将突出显示重要的部分，以指导你设置和运行简化版本的代码。请注意，从头开始训练深度语音模型需要相当多的时间，即使有GPU也是如此。
- en: 17.2.1 Preparing the input audio data for deep speech
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.1 准备深度语音的输入音频数据
- en: In chapter 7, I showed you how to use the BregmanToolkit to convert audio files
    from the frequency domain to the time domain. That library isn’t the only one
    that can perform that process, however. A similar process generates the Mel frequency
    cepstral coefficient (MFCC) for an audio file. This process runs a fast Fourier
    transformation (FFT) on the audio file and outputs the file as samples corresponding
    to the requested number of cepstral (or frequency bins) on which the input sound
    registers. The amplitude of those bins, as with the BregmanToolkit, is unique
    for each audio file and can be used to generate features for use in machine learning.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我向您展示了如何使用Bregman Toolkit将音频文件从频域转换为时域。然而，并非只有这个库可以执行这个过程。一个类似的过程为音频文件生成梅尔频率声谱系数（MFCC）。这个过程对音频文件运行快速傅里叶变换（FFT），并将文件输出为与请求的声谱系数（或频率分箱）数量相对应的样本。与Bregman
    Toolkit一样，这些分箱的幅度对每个音频文件都是唯一的，并且可以用来生成用于机器学习的特征。
- en: The BregmanToolkit generates an approximation of these bins, but now I’m going
    to show you a different method that uses TensorFlow native code, along with code
    that uses SciPy. TensorFlow ships with some handy functionality for dealing with
    audio files called `audio_ops`. The library includes code to read .wav files,
    to decode them into spectrograms (similar to chromagrams; chapter 7), and then
    run the MFCC transformation on them into the time domain. Listing 17.5 has some
    simple code that generates MFCC features, using TensorFlow’s audio_ops from a
    random .wav file in the LibriSpeech corpus. Running the code generate a feature-vector
    sized `(1,` `2545,` `26)` or 2,545 samples of 26 cepstral amplitudes for each
    sample.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: Bregman Toolkit生成了这些分箱的近似值，但现在我将向您展示一种不同的方法，该方法使用TensorFlow原生代码，以及使用SciPy的代码。TensorFlow附带了一些处理音频文件的有用功能，称为`audio_ops`。该库包括读取.wav文件的代码，将它们解码成频谱图（类似于音程图；第7章），然后对它们进行MFCC变换，进入时间域。列表17.5包含一些简单的代码，使用TensorFlow的audio_ops从LibriSpeech语料库中的随机.wav文件生成MFCC特征。运行代码生成一个特征向量大小为`(1,
    2545, 26)`或每个样本2,545个26个声谱振幅的样本。
- en: Listing 17.5 Generating MFCC features from wav files with TensorFlow
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.5 使用TensorFlow从wav文件生成MFCC特征
- en: '[PRE5]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The number of cepstral bins to use (26, per the Deep Speech paper)
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要使用的声谱系数分箱数量（根据Deep Speech论文，为26）
- en: ❷ Reads the file and interprets the .wav sound
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 读取文件并解释.wav声音
- en: ❸ Generates the corresponding spectrogram and generates MFCC features
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成相应的频谱图并生成MFCC特征
- en: ❹ Prints the output shape
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印输出形状
- en: You can look at a few of the samples from the audio file and convince yourself
    that there is something for the machine-learning algorithm to learn by reusing
    some plotting code from chapter 7 and plotting the first samples corresponding
    to the 26 bins. The code in listing 17.6 performs this plotting, and its output
    is shown in figure 17.3.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过重新使用第7章中的某些绘图代码并绘制对应于26个分箱的第一个样本来查看音频文件的一些样本，并确信机器学习算法有东西可以学习。列表17.6中的代码执行此绘图，其输出如图17.3所示。
- en: Listing 17.6 Plotting five samples from the audio file’s MFCC features
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.6 绘制音频文件MFCC特征的五个样本的图表
- en: '[PRE6]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ 26 cepstral bins
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 26 个声谱系数（cepstral）分箱
- en: ❷ 5 samples of the 2,545 total
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 2,545个样本中的5个
- en: ❸ Takes the absolute value so that there are no negative bin sizes
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 取绝对值，以确保没有负的分箱大小
- en: ❹ Shows the plot
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 显示图表
- en: '![CH17_F03_Mattmann2](../Images/CH17_F03_Mattmann2.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH17_F03_Mattmann2](../Images/CH17_F03_Mattmann2.png)'
- en: Figure 17.3 MFCC features for five samples of an audio file from the LibriSpeech
    corpus
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3 LibriSpeech语料库中音频文件的五个样本的MFCC特征
- en: There are variances among samples. The various samples correspond to different
    times in the sound file, and ultimately, you will want your ASR machine-learning
    model to predict letters for these utterances. In the Deep Speech paper, the authors
    define a context, which is a set of backward-looking samples and forward-looking
    samples along with the present sample. Having some overlap among samples allows
    for better differentiation, because word- and character-level tones tend to overlap
    in language. Think about saying the word best. The b and the e overlap in sound.
    This same concept applies to training a good machine-learning model because you
    want it to reflect the real world as much as possible.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 样本之间存在差异。不同的样本对应于声音文件中的不同时间点，最终，您希望您的 ASR 机器学习模型能够预测这些语音的字母。在 Deep Speech 论文中，作者定义了一个上下文，这是一个包含向后查看样本和向前查看样本以及当前样本的集合。样本之间的一些重叠允许更好的区分，因为单词和字符级别的音调在语言中往往重叠。想想说“best”这个词。b
    和 e 在声音上重叠。这个相同的概念也适用于训练一个好的机器学习模型，因为您希望它尽可能地反映现实世界。
- en: You can set up a time window to look at several time steps—call this time window
    numcontext samples—in the past and future. Taken along with the present sample,
    your new feature vector becomes N × (2*number cepstrals *numcontext + numcepstrals).
    Through some testing locally, 9 is a reasonable value for numcontext, and because
    you are using 26 cepstrals, you have a new feature vector of size (N × 2*26*9
    + 26) or (N × 494), where N is the number of samples. You can use the code from
    listing 17.5 to generate this new feature vector for each sound file, and then
    use the code in listing 17.7 to complete the job. Another technique the authors
    of the Deep Speech paper used was to take only half the samples from the audio
    file to further reduce the sample density, especially with a forward- and backward-looking
    window. Listing 17.7 takes care of that job for you too.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以设置一个时间窗口来查看过去和未来的几个时间步——称这个时间窗口为 numcontext 样本。结合当前样本，您的新特征向量变为 N × (2*number
    cepstrals *numcontext + numcepstrals)。通过本地测试，9 是 numcontext 的一个合理值，并且因为您使用了 26
    个倒谱系数，您有一个新特征向量的大小为 (N × 2*26*9 + 26) 或 (N × 494)，其中 N 是样本数量。您可以使用列表 17.5 中的代码为每个声音文件生成这个新特征向量，然后使用列表
    17.7 中的代码完成工作。Deep Speech 论文的作者还使用了一种技术，即只从音频文件中提取一半的样本以进一步减少样本密度，尤其是在向前和向后查看的窗口中。列表
    17.7 也为您处理了这个工作。
- en: Listing 17.7 Generating a contextual window of past, present, and future MFCC
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.7 生成过去、现在和未来的 MFCC 上下文窗口
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Takes every second sample and subsets the data by half
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 取每个第二个样本，并按一半的数据子集
- en: ❷ Generates (N × 494 samples) placeholder
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成 (N × 494 样本) 占位符
- en: ❸ Starting min point for past content; has to be at least 9 ts
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 过去内容的起始最小点；至少要有 9 个时间切片
- en: ❹ Ending point max for future content; size time slices 9 ts
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 未来内容的结束最大点；时间切片大小为 9 ts
- en: ❺ Picks up to numcontext time slices in the past, and completes with empty MFCC
    features
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在过去提取最多 numcontext 时间切片，并使用空的 MFCC 特征完成
- en: ❻ Picks up to numcontext time slices in the future, and completes with empty
    MFCC features
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在未来的 numcontext 时间切片中提取，并使用空的 MFCC 特征完成
- en: ❼ Pads if needed for the past or future, or takes past and future
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 如果需要，对过去或未来进行填充，或取过去和未来
- en: ❽ Takes the mean over the standard deviation and normalizes the input values
    for learning
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 对标准差取平均值，并对学习输入值进行归一化
- en: 'Now you’ve got your audio in good shape for training and can run the data preparation
    over the LibriSpeech audio files. Take heed: this process may take many hours
    for data preparation, depending on how much training data you use. There are more
    than 25,000 files, and my recommendation is to start small, with maybe 100 for
    training. This sample would yield a feature vector of (100 × *N* × 494), where
    N is the number of samples (two per audio file).'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您的音频已经准备好用于训练，并且可以在 LibriSpeech 音频文件上运行数据准备。请注意：这个数据准备过程可能需要许多小时，具体取决于您使用多少训练数据。文件超过
    25,000 个，我的建议是从小规模开始，比如用 100 个进行训练。这个样本将产生一个特征向量 (100 × *N* × 494)，其中 N 是样本数量（每个音频文件两个样本）。
- en: The other data preparation you’ll need to take care of involves those transcripts.
    The character-level data needs to be converted to numbers, which is a straightforward
    process that I’ll show you in section 17.2.2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要处理的其他数据准备涉及那些转录文本。字符级数据需要转换为数字，这是一个简单的过程，我将在第 17.2.2 节中向您展示。
- en: 17.2.2 Preparing the text transcripts as character-level numerical data
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.2 准备文本转录本为字符级数值数据
- en: 'With the work you did to prepare the LibriSpeech corpus, for every audio data
    file like LibriSpeech/train-clean-100-all/3486-166424-0004.wav, you’ve got a corresponding
    LibriSpeech/train-clean-100-all/3486-166424-0004.txt file with contents like the
    following:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 通过你为LibriSpeech语料库所做的准备工作，对于每个音频数据文件，例如LibriSpeech/train-clean-100-all/3486-166424-0004.wav，你都有一个相应的LibriSpeech/train-clean-100-all/3486-166424-0004.txt文件，内容如下：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: To take your input feature vector sized (1 × N × 494) and map it to the character-level
    output, you have to process the text output into numbers. One simple way is to
    use Python’s `ord()` function, because all characters in Python are numbers represented
    onscreen as characters by means of a charset. A *charset* is a table that maps
    a particular integer value to some character in the table. Popular charsets include
    ASCII, UTF-8, and UTF-16 for 8-bit and 16-bit Unicode. Python’s `ord()` function
    returns the integer representation of a character, which will work well for your
    network.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要将你的输入特征向量大小（1 × N × 494）映射到字符级输出，你必须将文本输出处理成数字。一种简单的方法是使用Python的`ord()`函数，因为Python中的所有字符都是数字，通过字符集在屏幕上表示为字符。*字符集*是一个将特定整数值映射到表中某个字符的表。流行的字符集包括用于8位和16位Unicode的ASCII、UTF-8和UTF-16。Python的`ord()`函数返回字符的整数表示，这对于你的网络来说将非常有效。
- en: The first steps are opening the transcript file, making sure that it is ASCII-formatted,
    and removing any funky characters. In general, deep speech has been ported to
    support other character sets (language encodings) but I’ll focus on ASCII and
    English in this chapter. (You can find other datasets at [https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech).)
    The easiest method is to use Python’s `codecs` module to force the file to be
    read as UTF-8 and then force-convert it to ASCII. Listing 17.8 has the snippet
    that performs this task. The code also lowercases the text.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是打开转录文件，确保它是ASCII格式，并移除任何奇特的字符。一般来说，深度语音已经被移植以支持其他字符集（语言编码），但我在本章中会专注于ASCII和英语。（你可以在[https://github.com/mozilla/DeepSpeech](https://github.com/mozilla/DeepSpeech)找到其他数据集。）最简单的方法是使用Python的`codecs`模块强制文件以UTF-8格式读取，然后强制将其转换为ASCII。列表17.8提供了执行此任务的代码片段。该代码还将文本转换为小写。
- en: Listing 17.8 Opening the transcript, forcing it to ASCII, and normalizing the
    text
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.8 打开转录，强制转换为ASCII，并规范化文本
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Opens the file as UTF-8
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以UTF-8格式打开文件
- en: ❷ The only supported characters are letters and apostrophes.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只支持字母和撇号。
- en: ❸ Converts any Unicode characters to ASCII equivalents
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将任何Unicode字符转换为ASCII等效字符
- en: ❹ Removes apostrophes to keep contractions together
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 移除撇号以保持缩写词在一起
- en: ❺ Returns lowercase alphabetic characters
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回小写字母字符
- en: When the text is normalized and clean, you can convert it to a numerical array
    by using the `ord()` function. The `text_to_char_array()` function converts the
    cleaned text to a numerical array. To do so, the function scans the string text,
    converts the string to an array of letters—such as `[`'`I`' '`<space>`' '`a`'
    '`m`'`]` for *I am*—and then replaces the letters with their ordinal representation—`[9`
    `0` `1` `13]`—for the transcript *I am*. Listing 17.9 provides the function that
    performs the transcript-to-array conversion.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 当文本被规范化和清理后，你可以使用`ord()`函数将其转换为数值数组。`text_to_char_array()`函数将清理后的文本转换为数值数组。为此，该函数扫描字符串文本，将字符串转换为字母数组——例如，*I
    am*的`[`'`I`' '`<space>`' '`a`' '`m`'`]`——然后将字母替换为它们的序数表示——*I am*转录的`[9` `0` `1`
    `13]`——列表17.9提供了执行转录到数组转换的函数。
- en: Listing 17.9 Generating a numerical array from the clean transcript
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.9 从干净的转录生成数值数组
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Reserves 0 for the space character
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为空格字符保留0
- en: ❷ Creates list of sentence’s words with spaces replaced by ''
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建句子单词列表，将空格替换为''
- en: ❸ Converts each word to an array of its letters
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将每个单词转换为字母数组
- en: ❹ Converts the letters to the ordinal representation
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将字母转换为序数表示
- en: With the audio input prepared and numerical transcripts, you have what you need
    to train the LSTM deep-speech model. Section 17.2.3 takes a quick look at its
    implementation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在音频输入准备和数值转录之后，你就有训练LSTM深度语音模型所需的一切。第17.2.3节简要介绍了其实现。
- en: 17.2.3 The deep-speech model in TensorFlow
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.3 TensorFlow中的深度语音模型
- en: The TensorFlow implementation of deep speech is complex, so you have little
    need to jump into its depths. I prefer to use a slimmed-down version made popular
    by the Silicon Valley Data Science tutorial at [https://www.svds.com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)
    and the associated GitHub code, which has moved to [https://github.com/ mrubash1/RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial/).
    The tutorial defines a simpler version of the deep-speech architecture, which
    I’ll cover in listing 17.10 and later listings.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 实现的深度语音复杂，因此你几乎没有必要深入研究其细节。我更喜欢使用硅谷数据科学教程中流行的简化版本，该教程可在 [https://www.svds.com/tensorflow-rnn-tutorial](https://www.svds.com/tensorflow-rnn-tutorial/)
    和相关的 GitHub 代码 [https://github.com/mrubash1/RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial/)
    找到。该教程定义了一个更简单的深度语音架构版本，我将在列表 17.10 和后续列表中介绍。
- en: The model takes as input training samples of shape (*M*, *N*, 494), where *M*
    is the size of the training batch, *N* is the number of samples in a file divided
    in half, and 494 includes 26 cepstrals and a context of 9 steps in the past and
    future. The first steps of the model involve setting up the network and its initial
    three hidden layers to learn features from the input audio. The initial hyperparameters
    for the work are taken from the Deep Speech paper, including setting dropout to
    `0.5` in layers 1-3, `0` in the LSTM bidirectional layer, and `0.5` in the output
    layer.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型以形状为 (*M*, *N*, 494) 的训练样本作为输入，其中 *M* 是训练批的大小，*N* 是文件中样本的数量的一半，494 包括 26
    个倒谱和过去和未来 9 步的上下文。模型的最初步骤包括设置网络及其初始三个隐藏层，以从输入音频中学习特征。该工作的初始超参数来自深度语音论文，包括在层 1-3
    中设置 dropout 为 `0.5`，在 LSTM 双向层中为 `0`，在输出层中为 `0.5`。
- en: 'The amount of time you’d would spend learning these hyperparameters would not
    an excellent use of your time, so use them as-is from the paper. The `relu_clip`
    is a modified ReLU activation function used by the deep-speech authors that sets
    any input to the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 学习这些超参数所花费的时间不会是时间的高效利用，因此直接使用论文中的超参数。`relu_clip` 是深度语音作者使用的修改过的 ReLU 激活函数，将任何输入设置为以下值：
- en: Any value less than 0 to 0
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何小于 0 到 0 的值
- en: Any value X greater than 0 and less than that of the clip value (20) to X itself
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何大于 0 且小于裁剪值（20）的值 X 将变为 X 本身
- en: Any value greater than the clip value (20) to the clip value (20)
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何大于裁剪值（20）的值将裁剪为裁剪值（20）
- en: Activations are scaled and shifted using this activation function, also from
    the paper.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此激活函数对激活进行缩放和偏移，该激活函数也来自论文。
- en: The network uses 1,024 hidden neurons for its initial three hidden layers and
    for the bidirectional LSTM cell layers, and 1,024 neurons for the last hidden
    layer before making character-level predictions for 29 characters a-z, including
    space, apostrophe, and blank. Listing 17.10 starts the model definition.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 网络使用 1,024 个隐藏神经元作为其初始三个隐藏层和双向 LSTM 单元层，以及 1,024 个神经元用于对 29 个字符（包括空格、撇号和空格）进行字符级预测。列表
    17.10 开始了模型定义。
- en: Listing 17.10 Hyperparameters and setup for deep speech
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.10 深度语音的超参数和设置
- en: '[PRE11]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Dropout to use in each layer
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每层要使用的 dropout
- en: ❷ Uses ReLU clipping as defined in the paper
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用论文中定义的 ReLU 裁剪
- en: ❸ Number of hidden dimensions in each layer
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每层的隐藏维度数量
- en: ❹ Output probabilities for the 29 characters for each cell
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 每个单元的 29 个字符的输出概率
- en: '❺ Input shape: [batch_size, n_steps, n_input + 2*26 cepstrals*9 window backwards/forwards]'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 输入形状：[batch_size, n_steps, n_input + 2*26 cepstrals*9 窗口前后]
- en: ❻ Reshapes for first-layer input (n_steps*batch_size, n_input + 2*26 cepstrals*9
    window)
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对第一层输入进行重塑（n_steps*batch_size, n_input + 2*26 cepstrals*9 窗口）
- en: The next three layers of the model in listing 17.11 pass the input batches of
    data to learn audio features that will be used as input to the bidirectional LSTM
    cells (size 1,024 each). The model also stores TensorFlow summary variables that
    you can inspect with TensorBoard, as I showed you in earlier chapters, in case
    you need to inspect the variable values for debugging.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.11 中的模型接下来的三个层将输入数据批次传递给学习音频特征，这些特征将被用作双向 LSTM 单元（每个大小为 1,024）的输入。该模型还存储
    TensorFlow 摘要变量，您可以使用 TensorBoard 检查这些变量，正如我在前面的章节中向您展示的那样，以防您需要检查变量值进行调试。
- en: Listing 17.11 The audio feature layers of deep speech
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.11 深度语音的音频特征层
- en: '[PRE12]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Implements the first layer
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 实现第一层
- en: ❷ Uses clipped ReLU activation
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用裁剪 ReLU 激活
- en: ❸ Implements the second layer
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 实现第二层
- en: ❹ Uses clipped ReLU activation
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用裁剪 ReLU 激活
- en: ❺ Implements the third layer
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 实现第三层
- en: ❻ Uses clipped ReLU activation
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用剪裁ReLU激活
- en: Deep speech includes the bidirectional LSTM layers in listing 17.12 to learn
    the audio features and their mapping to single-character-level outputs. The initial
    weights (`lstm_ fw_cell`) are passed to each forward cell for learning; then the
    backward cell weights (`lstm_bw_cell`) are used to propagate the learning for
    character prediction in reverse.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 深度语音包括列表17.12中的双向LSTM层来学习音频特征及其映射到单字符级输出。初始权重（`lstm_fw_cell`）传递给每个前向单元进行学习；然后使用反向单元权重（`lstm_bw_cell`）来反向传播字符预测的学习。
- en: Listing 17.12 The bidirectional LSTM layers
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.12 双向LSTM层
- en: '[PRE13]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Forward direction cell
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 前向方向单元
- en: ❷ Backward direction cell
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 反向方向单元
- en: ❸ Reshapes to [n_steps, batch_size, 2*n_cell_dim]
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重塑为[n_steps, batch_size, 2*n_cell_dim]
- en: ❹ Reshapes to [n_steps*batch_size, 2*n_cell_dim]
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 重塑为[n_steps*batch_size, 2*n_cell_dim]
- en: The final layers featurize the LSTM output by using one more hidden layer before
    mapping it to a fully connected layer that corresponds to a softmax distribution
    over the 29 character classes (listing 17.13).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 最终层通过在映射到对应于29个字符类softmax分布的完全连接层之前使用一个额外的隐藏层来特征化LSTM输出（列表17.13）。
- en: Listing 17.13 Final layers of deep speech
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.13 深度语音的最终层
- en: '[PRE14]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Fifth layer with clipped ReLU activation and dropout
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第五层带有剪裁ReLU激活和dropout
- en: ❷ Creates output logits of 29 character classes distribution]
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建29个字符类分布的输出logits
- en: ❸ Reshapes to time major n_steps, batch_size, n_hidden_6 distribution
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 重塑为时间优先的n_steps, batch_size, n_hidden_6分布
- en: The output of deep speech is a probability distribution over the 29 character
    classes for each sample for each audio file in a batch. If the batch size is 50,
    and each file has 75 time steps or samples, the output would be at each step *N*,
    50 output characters corresponding to the sound utterance at that step for each
    of the 50 files in the batch.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 深度语音的输出是每个样本在每个音频文件批次上的29个字符类的概率分布。如果批次大小为50，并且每个文件有75个时间步或样本，则输出将在每个步骤*N*，对应于每个批次中50个文件在该步骤的声音发音的50个输出字符。
- en: The last thing to talk about before running deep speech is how the predicted
    character utterances are evaluated. In section 17.2.4, I’ll discuss how connectionist
    temporal classification (CTC) is used to discern language that overlaps among
    consecutive time steps.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行深度语音之前要讨论的最后一件事是如何评估预测的字符发音。在17.2.4节中，我将讨论如何使用连接主义时序分类（CTC）来区分连续时间步之间的重叠语言。
- en: 17.2.4 Connectionist temporal classification in TensorFlow
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.4 TensorFlow中的连接主义时序分类
- en: The ideal situation in the deep-speech RNN would be for each utterance and time
    step in the audio file input to map directly to a character in the predicted output
    from the network. But the reality is that as you divide the input into time steps,
    an utterance and ultimately character-level output may overlap multiple time steps.
    In the audio file I am saying human things (figure 17.4), it’s entirely possible
    that the predicted output at the first four time steps (t1-t3) will correspond
    to the letter *I* because the utterance occurs at each step, but a portion of
    that utterance also bleeds into time step t4, which is predicted as letter *a*
    because that’s where the *a* sound starts to occur.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度语音RNN的理想情况下，每个音频文件输入中的每个发音和每个时间步都应该直接映射到网络预测输出的一个字符。但现实是，当你将输入划分为时间步时，一个发音和最终的字符级输出可能会跨越多个时间步。在我所说的音频文件中的人类事物（图17.4），完全有可能预测输出在第一个四个时间步（t1-t3）将对应于字母*I*，因为发音在每个步骤发生，但该发音的一部分也渗透到时间步t4，预测为字母*a*，因为那里*a*的声音开始出现。
- en: '![CH17_F04_Mattmann2](../Images/CH17_F04_Mattmann2.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![CH17_F04_Mattmann2](../Images/CH17_F04_Mattmann2.png)'
- en: Figure 17.4 CTC and deep speech aim to generate plausible character-level outputs
    for overlapping possibilities at a time step (t).
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4 CTC和深度语音旨在在每个时间步（t）生成可能的字符级输出
- en: In this scenario, how do you decide whether time step t4 represents the letter
    *I* or *a*, given the overlap?
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，考虑到重叠，你如何决定时间步t4代表字母*I*还是*a*？
- en: You can use CTC. The technique is a loss function similar to cross-entropy loss
    with logits (chapter 6). The loss function calculates all the possible character-level
    combinations for output, given the size (number of time steps) of the input and
    the relationship over steps of time. It defines a function to relate all the output-probability
    classes at each time step over time. The predictions themselves at each time step
    are considered not independently, but together.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用 CTC 技术。该技术是一种类似于具有 logits 的交叉熵损失的损失函数（第 6 章）。损失函数根据输入的大小（时间步的数量）和时间步之间的关系计算输出所有可能的字符级组合。它定义了一个函数来关联每个时间步的输出概率类别。每个时间步的预测本身不是独立考虑的，而是作为一个整体。
- en: TensorFlow comes with a CTC loss function called `ctc_ops` as part of the `tensorflow.python.ops`
    package. You provide to it the logits corresponding to the character-level time-step
    predictions and a placeholder (`int32`) for the predictions. You will fill the
    predictions during each training step with the sparse transcript conversion to
    numerical data, as I showed you in section 17.2.2, along with the desired output
    length and in each epoch. Then the CTC loss is computed and can be used to scan
    across the per-time step character predictions and converge toward full transcript
    predictions which minimize the loss or in turn have the highest likelihood over
    the prediction space.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 随带一个名为 `ctc_ops` 的 CTC 损失函数，作为 `tensorflow.python.ops` 包的一部分。你向它提供与字符级时间步预测对应的
    logits 以及一个占位符（`int32`）用于预测。你将在每个训练步骤中用稀疏转录转换成数值数据填充预测，正如我在第 17.2.2 节中展示的那样，以及期望的输出长度和每个
    epoch。然后计算 CTC 损失，并可用于扫描每个时间步的字符预测，并收敛到完整的转录预测，以最小化损失或反过来在预测空间中具有最高的似然。
- en: In the preceding example, CTC loss would look across all possible predictions
    at the character level and look for minimal loss, not only for time step t4, but
    also for t3-t5, which in turn suggests including a space before *a* and after
    *I*, properly delineating the words in the transcription. Because the desired
    sequence length is provided and is equal to the length of the converted sparse
    transcript, the CTC algorithm can figure out how to weight spaces and other nonletter
    characters to achieve the optimal transcription output. Listing 17.14 sets up
    the CTC loss function and prepares the model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，CTC 损失将查看所有可能的字符级预测，寻找最小损失，不仅对于时间步 t4，也对于 t3-t5，这反过来又建议在 *a* 前面和 *I*
    后面添加一个空格，正确划分转录中的单词。因为提供了所需的序列长度，并且等于转换后的稀疏转录的长度，CTC 算法可以找出如何权衡空格和其他非字母字符以实现最佳的转录输出。列表
    17.14 设置了 CTC 损失函数并准备训练模型。
- en: Listing 17.14 Setting up CTC loss and preparing to train the model
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 17.14 设置 CTC 损失并准备训练模型
- en: '[PRE15]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Inputs converted audio features
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入转换后的音频特征
- en: ❷ 1D array of size [batch_size]
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 大小为 [batch_size] 的一维数组
- en: ❸ Uses sparse_placeholder; will generate a SparseTensor, required by ctc_loss
    op
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 sparse_placeholder；将生成一个 SparseTensor，这是 ctc_loss 操作所需的
- en: ❹ Sets up the BiRNN model
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置 BiRNN 模型
- en: ❺ Sets up the CTC loss function with input audio features (logits), the sparse
    transcript targets, and desired transcript length (seq length)
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用输入音频特征（logits）、稀疏转录目标和期望的转录长度（序列长度）设置 CTC 损失函数
- en: ❻ Creates the optimizer with hyperparameters from the Deep Speech paper and
    train operation
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 使用来自 Deep Speech 论文的超参数创建优化器并进行训练操作
- en: With the CTC loss function defined and ready, the model is ready for training,
    which I’ll help you set up in section 17.3.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义并准备好 CTC 损失函数后，模型就准备好进行训练了，我将在第 17.3 节帮助你设置。
- en: 17.3 Training and evaluating deep speech
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 深度语音的训练与评估
- en: Running the deep-speech model using TensorFlow is similar to running all the
    other models you’ve created thus far. You’ll set train size, which controls the
    number of audio files to use for training. I recommended using hundreds of them
    at a time on a laptop if possible, but not thousands, and with the LibriSpeech
    corpus, you have more than 25,000 files to use for training.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 TensorFlow 运行 deep-speech 模型与运行迄今为止创建的所有其他模型类似。你需要设置训练大小，这控制了用于训练的音频文件数量。如果可能的话，我建议在笔记本电脑上一次使用数百个，但不要超过千个，并且使用
    LibriSpeech 语料库，你有超过 25,000 个文件可用于训练。
- en: A batch size of 50, given 150 training files, creates 3 iterations per epoch.
    Fifty epochs can be trained in a few hours on a CPU and in minutes on a GPU. You
    can tweak these hyperparameters to suit your computing resources. Listing 17.15
    uses the TensorFlow dataset API (chapter 15) to create the dataset as a TensorFlow
    operator lazily loaded in each epoch.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 给定50个批量大小的批次，在150个训练文件的情况下，每个epoch创建3次迭代。在CPU上训练50个epoch可能需要几个小时，在GPU上可能只需要几分钟。你可以调整这些超参数以适应你的计算资源。列表17.15使用TensorFlow数据集API（第15章）创建数据集，作为每个epoch懒加载的TensorFlow操作。
- en: Listing 17.15 Setting up the training parameters and dataset for deep speech
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.15 设置深度语音的训练参数和数据集
- en: '[PRE16]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Hyperparameters, 50 epochs, with 3 batches of 50 for 150 training files per
    epoch
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 超参数设置，50个epoch，每个epoch有3个批次，每个批次50个，共150个训练文件
- en: ❷ Builds a TensorFlow dataset from training files, and sets up random shuffling
    and prefetching
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从训练文件构建TensorFlow数据集，并设置随机洗牌和预取
- en: Listing 17.16 trains the model and outputs CTC loss per batch and average train
    loss per epoch by dividing the loss by number of training files. The audio input
    is prepared, using the techniques described earlier to create MFCC features with
    contextual windows of nine steps in the past and nine in the future, with padding.
    The transcript data is turned into integers based on ordinal values for 26 alpha
    characters, and also space, blank, and apostrophe, totaling 29 different character
    values.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.16 训练模型，并按批次输出CTC损失以及通过除以训练文件数得到的平均训练损失。音频输入通过使用前面描述的技术来创建MFCC特征，具有过去和未来九步的上下文窗口，并进行了填充。转录数据根据26个字母字符的序数值转换为整数，还包括空格、空白和撇号，总共29个不同的字符值。
- en: Listing 17.16 Training deep speech
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表17.16 训练深度语音
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Creates a new TF session
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个新的TF会话
- en: ❷ Creates a new Dataset iterator operation
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个新的数据集迭代操作
- en: ❸ Gets the batch of training audio filenames and then gets the corresponding
    transcript names
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取训练音频文件名批次，然后获取相应的转录名称
- en: ❹ Prepares the audio data by creating MFCC with context window and pads sequences
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过创建具有上下文窗口的MFCC并填充序列来准备音频数据
- en: ❺ Creates numerical per-character level transcripts
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建每个字符级别的数值转录
- en: ❻ Prepares the input to each training step and runs the training operation
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 准备每个训练步骤的输入并运行训练操作
- en: ❼ Prints per-batch loss
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 打印每个批次的损失
- en: ❽ Prints per-epoch average loss per training sample
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 打印每个训练样本的平均损失
- en: 'One of the most comprehensive toolkits and clean code bases for running deep
    speech is the RNN-Tutorial GitHub repo at [https://github.com/mrubash1/RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial/).
    It allows for the following:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 运行深度语音最全面工具集和干净的代码库之一是位于[https://github.com/mrubash1/RNN-Tutorial](https://github.com/mrubash1/RNN-Tutorial/)的RNN-Tutorial
    GitHub仓库。它允许以下操作：
- en: Easy tweaking of model parameters
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数的简单调整
- en: Use of hyperparameters
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数的使用
- en: Training, test, and development sets
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练、测试和开发集
- en: Per-epoch dev-set validations
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个epoch的dev集验证
- en: Final test set validation
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终测试集验证
- en: 'It also prints model decodings per epoch and at the end of training so that
    you get output such as the following:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 它还打印每个epoch和训练结束时的模型解码，以便你得到以下输出：
- en: '[PRE18]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Another nifty feature automatically uses TensorFlow’s Summary operations API
    to log events so that you can use TensorBoard to visualize the model as it’s running.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个巧妙的功能是自动使用TensorFlow的Summary操作API来记录事件，这样你就可以使用TensorBoard在模型运行时可视化模型。
- en: Using a GPU across several days, I managed to train quite a few variations of
    the deep-speech model; I watched validation loss, accuracy, and train loss, and
    plotted them with TensorBoard. Figure 17.5 shows the first 500 epochs of deep-speech
    training.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在几天内使用GPU，我成功训练了相当多深语音模型的变体；我观察了验证损失、准确性和训练损失，并用TensorBoard进行了绘图。图17.5显示了深度语音训练的前500个epoch。
- en: '![CH17_F05_Mattmann2](../Images/CH17_F05_Mattmann2.png)'
  id: totrans-214
  prefs: []
  type: TYPE_IMG
  zh: '![CH17_F05_Mattmann2](../Images/CH17_F05_Mattmann2.png)'
- en: Figure 17.5 TensorBoard output for the first 500 epochs of the deep-speech model
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5深度语音模型前500个epoch的TensorBoard输出
- en: The model converges quickly in the first 90 epochs, and train loss and validation
    loss head in the right direction. The validation label error rate, computed every
    two epochs by default in the RNN-Tutorial code repo, also heads in the right direction,
    demonstrating the robustness of the model.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 模型在前90个epoch内快速收敛，训练损失和验证损失朝正确的方向前进。默认情况下，RNN-Tutorial代码库每两个epoch计算一次验证标签错误率，也朝正确的方向前进，证明了模型的鲁棒性。
- en: Congratulations—you have learned how to create your own ASR system! The big
    web companies aren’t the only ones who can do this. RNNs, data, and some TensorFlow
    are the tools you need to get the job done.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜您——您已经学会了如何创建自己的 ASR 系统！大型网络公司并不是唯一能够做到这一点的人。RNNs、数据和一些 TensorFlow 是您完成任务所需的工具。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Intelligent digital assistants, from home devices to your phones to your TV,
    recognize speech and convert it to text by using RNNs and special instances of
    them called LSTM models. This process is called ASR.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 智能数字助手，从家用设备到您的手机再到您的电视，通过使用 RNNs 和被称为 LSTM 模型的特殊实例来识别语音并将其转换为文本。这个过程被称为 ASR。
- en: You can obtain open data from audiobooks such as the OpenSLR and LibriSpeech
    data of 100, 500, and 1000 hours of recordings, along with aligned textual transcripts
    of those books that you can use to train an LSTM model for ASR.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以从有声读物等开放数据源中获取数据，例如 OpenSLR 和 LibriSpeech 数据，这些数据包括 100、500 和 1000 小时的录音，以及这些书籍的文本转录本，您可以使用这些数据来训练用于
    ASR 的 LSTM 模型。
- en: One of the most famous models for ASR is called Deep Speech. You can re-create
    this model with TensorFlow and the LibriSpeech data by building the deep-speech
    LSTM model.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ASR 中最著名的模型之一称为 Deep Speech。您可以使用 TensorFlow 和 LibriSpeech 数据通过构建 deep-speech
    LSTM 模型来重新创建此模型。
- en: TensorFlow and associated toolkits provide methods to featurize the audio with
    MFCCs, converting from the frequency domain to the time domain.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 和相关工具包提供了使用 MFCCs 特征化音频的方法，将频率域转换为时间域。
- en: Text transcripts can be featurized with Python’s `ord()` function and utilities
    to convert text transcripts to numbers.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文本转录本可以使用 Python 的 `ord()` 函数和转换文本转录本为数字的实用工具进行特征化。
- en: CTC is a loss function and algorithm that allows speech input over nonuniform
    time steps to be aligned with uniform character-level transcripts to obtain the
    best transcriptions in RNNs.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CTC 是一种损失函数和算法，它允许语音输入在非均匀时间步长上与统一字符级别的文本转录本对齐，以在 RNNs 中获得最佳的转录结果。
