- en: '4 Preparing the data, part 2: Transforming the data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 准备数据，第二部分：转换数据
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Dealing with more incorrect values
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理更多错误值
- en: Mapping complex, multiword values to single tokens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将复杂的多词值映射到单个标记
- en: Fixing type mismatches
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修复类型不匹配
- en: Dealing with rows that still contain bad values after cleanup
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理清理后仍包含错误值的行
- en: Creating new columns derived from existing columns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从现有列派生新的列
- en: Preparing categorical and text columns to train a deep learning model
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准备用于训练深度学习模型的分类和文本列
- en: Reviewing the end-to-end solution introduced in chapter 2
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回顾第 2 章中引入的端到端解决方案
- en: In chapter 3, we corrected a set of errors and anomalies in the input dataset.
    There’s still more cleanup and preparation to be done in the dataset, and that’s
    what we’ll do in this chapter. We’ll deal with remaining issues (including multiword
    tokens and type mismatches) and go over your choices about how to deal with the
    bad values that are still present after all the cleanup. Then we’ll go over creating
    derived columns and how to prepare non-numeric data to train a deep learning model.
    Finally, we’ll take a closer look at the end-to-end solution introduced in chapter
    2 to see how the data preparation steps we have completed so far fit into our
    overall journey to a deployed, trained deep learning model for predicting streetcar
    delays.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们纠正了输入数据集中的一组错误和异常。数据集中仍有更多清理和准备工作要做，这就是本章我们将要做的。我们将处理剩余问题（包括多词标记和类型不匹配），并回顾你在所有清理后如何处理仍然存在的错误值的选择。然后，我们将回顾创建派生列以及如何准备非数值数据以训练深度学习模型。最后，我们将更仔细地查看第
    2 章中引入的端到端解决方案，以了解我们已完成的数据准备步骤如何融入我们部署、训练用于预测电车延误的深度学习模型的总体旅程。
- en: 'You will see a consistent theme throughout this chapter: making updates to
    the dataset so that it more closely matches the real-world situation of streetcar
    delays. By eliminating errors and ambiguities to make the dataset better match
    the real world, we increase our chances of getting an accurate deep learning model.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本章中看到一致的主题：对数据集进行更新，使其更接近电车延误的现实世界情况。通过消除错误和歧义，使数据集更好地匹配现实世界，我们增加了获得准确深度学习模型的机会。
- en: 4.1 Code for preparing and transforming the data
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 准备和转换数据的代码
- en: When you have cloned the GitHub repo ([http://mng.bz/v95x](http://mng.bz/v95x))
    associated with this book, the code related to exploring and cleansing the data
    is in the notebooks subdirectory. The next listing shows the files described in
    this chapter.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当你克隆了与本书相关的 GitHub 仓库 ([http://mng.bz/v95x](http://mng.bz/v95x)) 后，探索和清洗数据的代码位于
    notebooks 子目录中。下表展示了本章中描述的文件。
- en: Listing 4.1 Code in the repo related to preparing the data
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.1 仓库中与数据准备相关的代码
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Directory for pickled input and output dataframes
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于存储和输出数据框的 pickled 输入目录
- en: '❷ Notebook containing code to define boundaries around the streetcar network
    (section [See 4.6 Going the distance: Locations.](../Text/04.htm#pgfId-1000233))'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含定义电车网络边界代码的笔记本（见 [4.6 走得更远：位置](../Text/04.htm#pgfId-1000233)）
- en: '❸ Notebook for generating latitude and longitude values associated with delay
    locations (section [See 4.6 Going the distance: Locations.](../Text/04.htm#pgfId-1000233))'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成与延迟位置相关的经纬度值的笔记本（见 [4.6 走得更远：位置](../Text/04.htm#pgfId-1000233)）
- en: ❹ Data preparation notebook
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 数据准备笔记本
- en: '❺ Config file for the data preparation notebook: whether to load the input
    data from scratch, save the transformed output dataframe, and remove bad values
    as well as the filenames for the pickled input and output dataframes'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 数据准备笔记本的配置文件：是否从头开始加载数据，保存转换后的输出数据框，以及删除错误值以及 pickled 输入和输出数据框的文件名
- en: '4.2 Dealing with incorrect values: Routes'
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 处理错误值：路线
- en: 'In chapter 3, we cleaned up the Direction column. As you will recall, the valid
    values for this column corresponded to the compass points plus an extra token
    to indicate two directions. The valid values for the Direction column are universal
    (north, east, south, west) and not specific to the streetcar delay problem. What
    about columns that have values that are unique to the streetcar use case: Route
    and Vehicle? How do we clean up these columns, and what lessons can we learn from
    this cleanup that are applicable to other datasets?'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们清理了方向列。正如你所回忆的，该列的有效值对应于罗盘方向，以及一个额外的标记来表示两个方向。方向列的有效值是通用的（北、东、南、西），并不特定于电车延误问题。那么，对于具有唯一电车使用案例值的列（路线和车辆）怎么办？我们如何清理这些列，以及我们能从这次清理中学到什么，这些经验可以应用到其他数据集中？
- en: If you look at the beginning of the data preparation notebook , you will notice
    a cell with the valid streetcar routes (figure 4.1).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看数据准备笔记本的开始部分，你会注意到一个包含有效电车路线的单元格（图4.1）。
- en: '![CH04_F01_Ryan](../Images/CH04_F01_Ryan.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F01_Ryan](../Images/CH04_F01_Ryan.png)'
- en: Figure 4.1 Valid streetcar routes
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 有效电车路线
- en: The following listing shows the code in the data preparation notebook that cleans
    up the values in the Route column.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了数据准备笔记本中清理路线列值的代码。
- en: Listing 4.2 Code to clean up values in the Route column
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 清理路线列值的代码
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Define a list containing all the valid route values.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个包含所有有效路线值的列表。
- en: ❷ Print out a count of all the unique values in the Route column.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印出路线列中所有唯一值的计数。
- en: ❸ Function to replace route values that aren’t in the list of valid values with
    a placeholder value
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将不在有效值列表中的路线值替换为占位符值的函数
- en: ❹ Apply the check_route function to the Route column.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将check_route函数应用于路线列。
- en: ❺ Print out the revised count of all unique values in the Route Column.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印出路线列中所有唯一值的修订计数。
- en: When the data in the input dataset was entered, the values in the Route column
    were not limited to valid routes, so the column contains many values that aren’t
    valid streetcar routes. If we don’t fix this situation, we will be training our
    deep learning model with data that doesn’t reflect the real-world situation. The
    refactoring of the dataset to make each record a route/direction/time-slot combination
    (chapter 5) will not be possible unless we clean up the values in the Route column.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 当输入数据集中的数据被输入时，路线列中的值并未限制在有效路线上，因此该列包含许多不是有效电车路线的值。如果我们不解决这个问题，我们将用不反映现实世界情况的数据来训练我们的深度学习模型。除非我们清理路线列中的值，否则我们无法将数据集重构为每个记录都是一个路线/方向/时间段组合（第5章）。
- en: 'It is worthwhile to review the cleanup process for the Route and Vehicle columns
    because the same dilemma can occur in many real-world datasets: you have a column
    that has a strictly defined list of valid values, but bad values still occur in
    the dataset because of the way that the data is entered or because of lax error
    checking in the data entry process.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 值得回顾路线和车辆列的清理过程，因为同样的困境可能出现在许多现实世界的数据集中：你有一个具有严格定义的有效值列表的列，但由于数据输入方式或数据输入过程中的错误检查不严格，数据集中仍然存在不良值。
- en: How bad is the problem with the Route column? When we list the values in the
    Route column, we see that there are 14 valid streetcar routes, but the Route column
    contains more than 100 distinct values.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 路线列的问题有多严重？当我们列出路线列中的值时，我们看到有14条有效的电车路线，但路线列包含超过100个不同的值。
- en: We define a simple function, `check_route` , that checks the values in the Route
    column and replaces any values that aren’t in the list of valid route values with
    a `bad value` token. We apply this function to the entire Route column, using
    lambda so that the function is applied to every value in the column. See [http://mng.bz/V8gO](http://mng.bz/V8gO)
    for more details on the power of applying functions to Pandas dataframes by using
    lambda.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个简单的函数，`check_route`，该函数检查路线列中的值，并将任何不在有效路线值列表中的值替换为`bad value`标记。我们将此函数应用于整个路线列，使用lambda函数，以便该函数应用于列中的每个值。有关使用lambda将函数应用于Pandas数据框的更多详细信息，请参阅[http://mng.bz/V8gO](http://mng.bz/V8gO)。
- en: 'We check the number of unique values in the Route column again after applying
    the `check_route` function to confirm that the Route column no longer contains
    any unexpected values. As we expect, the Route column now has 15 distinct values:
    the 14 valid route values, plus `bad route` to indicate that the original dataset
    had a value that is not a valid route.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在应用了`check_route`函数之后，我们再次检查“路线”列中唯一值的数量，以确认“路线”列不再包含任何意外的值。正如我们所预期的，“路线”列现在有15个不同的值：14个有效的路线值，加上`bad
    route`来表示原始数据集中有一个不是有效路线的值。
- en: 4.3 Why only one substitute for all bad values?
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 为什么只对一个所有错误值进行替换？
- en: 'You may ask whether we have any options for bad route values other than substituting
    a single value for all the bad values. What if we are losing some kind of signal
    in the invalid route values when we substitute a single placeholder for all of
    them? Perhaps it would make sense to replace bad values with placeholders that
    reflect why the value is bad, such as the following:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，我们是否有除了用一个单一值替换所有错误值之外的其他选项。如果我们用一个占位符替换所有无效路线值时，我们是否可能丢失某种信号？也许用反映值为何错误的占位符来替换错误值是有意义的，例如以下内容：
- en: '`Bus_route` —For values in the Route column that are not valid streetcar routes
    but are valid bus routes.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`公交路线` — 对于“路线”列中不是有效电车路线但却是有效公交路线的值。'
- en: '`Obsolete_route` —For values in the Route column that are former streetcar
    routes.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`废弃路线` — 对于“路线”列中曾经是电车路线的值。'
- en: '`Non_ttc_route` —For values in the Route column that are valid route designations
    for bus routes from outside Toronto that are not run by the Toronto Transit Commission
    (TTC). The municipalities that surround Toronto (including Mississauga to the
    west, Vaughn to the northwest, Markham to the northeast, and Durham to the east)
    have their own distinct transit operators, and it is theoretically possible that
    a bus route of one of these non-TTC operators could be delayed by a streetcar
    delay. There are no instances of non-TTC routes in the Route column in the current
    dataset, but that doesn’t mean that such a non-TTC route could not appear in the
    dataset in the future. As we will see in chapter 9, we can expect that a model
    will get retrained on new data repeatedly after it is put in production, so the
    data preparation code should be resilient to allow potential future changes in
    the input dataset. Allowing for non-TTC route values in the Route column is an
    example of anticipating potential changes in the dataset.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`非TTC路线` — 对于“路线”列中是来自多伦多以外地区公交路线的有效路线标识，但不由多伦多交通委员会（TTC）运营。围绕多伦多的市政当局（包括西边的密西沙加，西北边的沃恩，东北边的马克汉姆和东边的达勒姆）有自己的独立交通运营商，从理论上讲，这些非TTC运营商中的一家公交路线可能会因电车延误而延误。当前数据集中没有非TTC路线的实例，但这并不意味着这种非TTC路线不能在未来数据集中出现。正如我们在第9章中将会看到的，一旦模型投入生产，我们就可以预期模型将在新数据上反复重新训练，因此数据准备代码应该具有弹性，以允许潜在的未来输入数据集的变化。允许“路线”列中的非TTC路线值是一个预测数据集潜在变化的例子。'
- en: '`Incorrect_route` —For values in the Route column that have never been valid
    routes for any transit operation in the greater Toronto area, including routes.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`错误路线` — 对于“路线”列中从未是大多伦多地区任何交通运营有效路线的值，包括路线。'
- en: As it happens, for the streetcar delay problem, these distinctions are not relevant.
    We are interested only in delays for streetcar routes. But if the problem were
    framed differently, to include predicting delays for transit operations beyond
    streetcars, it could make sense to use finer-grained substitutions for bad values
    in the Route column, such as those in the preceding list. It is certainly worth
    asking whether all nonvalid values for a column are equivalent in terms of the
    goal of the project. In the case of the streetcar delay problem, the answer is
    yes, but the same answer doesn’t necessarily apply to all structured data problems.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于电车延误问题，这些区别并不相关。我们只对电车路线的延误感兴趣。但如果问题被不同地表述，包括预测电车以外的交通运营的延误，那么在“路线”列中对错误值使用更细粒度的替换，如前面列表中的那些，是有意义的。当然，值得问一下，对于一个列的所有非有效值在项目目标方面是否都是等效的。在电车延误问题的案例中，答案是肯定的，但同样的答案并不一定适用于所有结构化数据问题。
- en: '4.4 Dealing with incorrect values: Vehicles'
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 处理不正确的值：车辆
- en: In the same way that there is a fixed list of valid streetcar routes, there
    is also a fixed list of valid vehicles. You can see how this list of valid vehicles
    is compiled and view the sources foFigure 4.r this information in the data preparation
    notebook (figure 4.2).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 与存在固定有效电车路线列表一样，也存在一个固定有效的车辆列表。你可以看到如何编译这个有效车辆列表，并在数据准备笔记本中查看该信息的来源（图4.2）。
- en: '![CH04_F02_Ryan](../Images/CH04_F02_Ryan.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F02_Ryan](../Images/CH04_F02_Ryan.png)'
- en: Figure 4.2 Valid streetcar IDs
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 有效的电车ID
- en: Buses can also be victims of streetcar delays. The list of valid bus IDs is
    more complex (figure 4.3).
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 公交车也可能成为电车延误的受害者。有效公交车ID的列表更复杂（图4.3）。
- en: '![CH04_F03_Ryan](../Images/CH04_F03_Ryan.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F03_Ryan](../Images/CH04_F03_Ryan.png)'
- en: Figure 4.3 Valid bus IDs
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 有效的公交车ID
- en: As we did for the Route column, we define a function in the next listing that
    replaces invalid Vehicle values with a single token to indicate a bad value, reducing
    the number of values in the Vehicle column by more than half.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们对路线列所做的那样，我们在下一个列表中定义了一个函数，该函数将无效的车辆值替换为一个单一标记以表示坏值，从而将车辆列中的值数量减少了超过一半。
- en: Listing 4.3 Code to clean up values in the Vehicle column
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 清理车辆列值的代码
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Count of unique values in the Vehicle column precleanup
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清理前车辆列中唯一值的数量
- en: ❷ Count of unique values in the Vehicle column postcleanup
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 清理后车辆列中唯一值的数量
- en: As it turns out, we will not end up using the Vehicle column in the model training
    described in chapters 5 and 6\. First, in the deployment scenarios in chapter
    8, the user will be somebody who wants to take a trip on a streetcar and needs
    to know whether it will be delayed. In this use case, the user will not know which
    specific vehicle they will be riding in. Because the user will not be able to
    provide the vehicle ID at the time they want to get a prediction, we cannot train
    the model with the data in the Vehicle column. But we could use the Vehicle column
    in a future variation of the model that is aimed at a different set of users (such
    as administrators of the transit authority that runs the streetcars) who would
    know the vehicle ID for a given trip, so it is worthwhile to clean up the values
    in the Vehicle column for potential future use.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们最终不会在第五章和第六章中描述的模型训练中使用车辆列。首先，在第八章中描述的部署场景中，用户将是想要乘坐电车并需要知道是否会延误的人。在这种情况下，用户将不知道他们将乘坐哪一辆具体的车辆。因为用户在想要获取预测时无法提供车辆ID，所以我们不能使用车辆列中的数据进行模型训练。但我们可以使用车辆列在未来模型的一个变体中，该变体针对的是不同的用户群体（例如，运营电车的交通管理局的行政人员），他们知道特定行程的车辆ID，因此清理车辆列中的值以备将来使用是值得的。
- en: '4.5 Dealing with inconsistent values: Location'
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 处理不一致值：位置
- en: The Route and Vehicle columns are typical categorical columns because they have
    a fixed set of valid values that is easily defined. The Location column presents
    a different set of problems because it doesn’t have a neatly defined set of valid
    values. It is worth spending some time going through the issues related to the
    Location column and how to fix them, because this column demonstrates the kind
    of messiness you will come across in real-world datasets. The values in this column
    are specific to the streetcar dataset, but the approaches that we will take to
    clean up these values (getting a consistent case, getting a consistent order for
    values, and replacing inconsistent tokens that refer to the same real-world entity
    with a single token) apply to many datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 路线和车辆列是典型的分类列，因为它们有一个固定且易于定义的有效值集合。位置列则呈现出一组不同的问题，因为它没有一个整洁定义的有效值集合。花些时间研究位置列相关的问题以及如何解决这些问题是值得的，因为这个列展示了你将在现实世界数据集中遇到的混乱类型。这个列中的值是针对电车数据集的，但我们用来清理这些值的方法（获取一致的大小写，获取值的有序排列，以及用一个单一标记替换指代同一现实世界实体的不一致标记）适用于许多数据集。
- en: 'Following are some of the idiosyncrasies of the values in the Location column:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是位置列值的一些特性：
- en: The values in the Location column can be street junctions (“Queen and Connaught”)
    or landmarks (“CNE Loop,” “Main Station,” “Leslie Yard”).
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置列中的值可以是街道交叉口（“皇后街和康纳格街”）或地标（“CNE环”，“主站”，“莱斯利场”）。
- en: There are thousands of valid street junction values. Because routes go beyond
    their namesake streets, a route can have valid junction values that don’t include
    the route name. “Broadview and Dundas” is a valid location value for an incident
    on the King route, for example.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有数千个有效的街道交汇值。因为路线超出了其名称街道的范围，所以一个路线可以具有不包含路线名称的有效交汇值。例如，“Broadview and Dundas”是King路线事件的有效位置值。
- en: The landmark values can be commonly known (such as “St. Clair West station”),
    or they can be specific to the inner workings of the streetcar network (such as
    “Leslie Yard”).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 地标值可以是普遍知名的（例如“St. Clair West station”），也可以是针对电车网络内部运作的特定值（例如“Leslie Yard”）。
- en: The order of the street names isn’t consistent (“Queen and Broadview,” “Broadview
    and Queen”).
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 街道名称的顺序不一致（“Queen and Broadview”、“Broadview and Queen”）。
- en: Many locations have multiple tokens to represent them (“Roncy Yard,” “Roncesvalles
    Yard,” and “Ronc. Carhouse” represent the same location).
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多位置有多个标记来表示它们（“Roncy Yard”、“Roncesvalles Yard”和“Ronc. Carhouse”代表同一位置）。
- en: 'The total count of values in the Location column is much larger than any of
    the other columns we have looked at so far:'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置列中的值总数远大于我们迄今为止查看的任何其他列：
- en: '[PRE3]'
  id: totrans-67
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here are the steps we are going to take to clean up the Location column:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们将要采取的清理位置列的步骤：
- en: Convert all values to lowercase.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有值转换为小写。
- en: Use consistent tokens. When multiple distinct values are used to indicate the
    same location (“Roncy Yard,” “Roncesvalles Yard,” and “Ronc. Carhouse”), replace
    these distinct strings with a single string (“Roncesvalles yard”) so that only
    one string is used to indicate any given location.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一致的标记。当使用多个不同的值来表示同一位置时（“Roncy Yard”、“Roncesvalles Yard”和“Ronc. Carhouse”），将这些不同的字符串替换为一个字符串（“Roncesvalles
    yard”），以确保任何给定位置只使用一个字符串来表示。
- en: Make the order of streets in junction values consistent (replace “Queen and
    Broadview” with “Broadview and Queen”). We do this by ensuring that street names
    in junctions are always ordered with the street name that comes first alphabetically
    coming first in the junction string.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使交汇处的街道顺序一致（将“Queen and Broadview”替换为“Broadview and Queen”）。我们通过确保交汇处的街道名称总是按照字母顺序排列，即字母顺序排在首位的街道名称在交汇字符串中排在首位来实现这一点。
- en: 'We will track our progress by calculating the total percentage drop in the
    number of distinct values in the Location column after taking each step. After
    we make all the values in the Location column lowercase, the number of unique
    values drops by 15%:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过计算在每一步之后位置列中不同值的总数下降的百分比来跟踪我们的进度。在我们将位置列中的所有值转换为小写后，唯一值的数量下降了15%：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Next, we make a set of substitutions to remove duplicate values such as “stn”
    and “station.” You may ask how we determine what values are duplicates. How do
    we know, for example, that “carhouse,” “garage,” and “barn” are all identical
    to “yard” and that we should substitute “yard” for those three values? This question
    is a good one because it leads to an important point about machine learning projects.
    To determine which terms in the Location column are equivalent, we need to have
    domain knowledge about Toronto (especially its geography) and about the streetcar
    network in particular. Any machine learning project is going to require a combination
    of technical expertise in machine learning and domain knowledge about the subject
    area to which machine learning is being applied. To really tackle the credit card
    fraud detection problem outlined in chapter 1, for example, we would need to have
    access to somebody who has deep understanding of the details of credit card fraud.
    For the major example in this book, I chose the streetcar delay problem because
    I know Toronto well and happen to have knowledge of the streetcar network that
    allows me to determine, for example, that “carhouse,” “garage,” and “barn” mean
    the same thing as “yard.” The need for domain knowledge in a machine learning
    project should never be underestimated, and as you consider projects to tackle,
    you should ensure that you have somebody on the team who has adequate knowledge
    of the domain of the project.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们进行一系列替换以删除重复值，例如“stn”和“station”。你可能会问我们如何确定哪些值是重复的。我们如何知道，例如，“carhouse”、“garage”和“barn”都与“yard”相同，我们应该用“yard”替换这三个值？这个问题问得好，因为它引出了一个关于机器学习项目的重要观点。为了确定位置列中哪些术语是等效的，我们需要对多伦多（特别是其地理）以及特定的电车网络有领域知识。任何机器学习项目都将需要机器学习的技术专长以及应用领域的领域知识。例如，要真正解决第1章中概述的信用卡欺诈检测问题，我们就需要能够接触到对信用卡欺诈细节有深入了解的人。对于本书的主要例子，我选择电车延误问题，因为我对多伦多很了解，并且恰好拥有关于电车网络的知识，这使我能够确定，“carhouse”、“garage”和“barn”与“yard”意思相同。在机器学习项目中，领域知识的需求绝不能被低估，当你考虑要解决的问题时，你应该确保团队中有足够了解项目领域的人。
- en: 'What is the effect of the changes we have applied to the Location column? So
    far, these changes bring the total reduction of unique values to 30%:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对位置列应用的变化有什么影响？到目前为止，这些变化使唯一值的总减少率达到30%：
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Finally, for all the location values that are street junctions, make the order
    of the street names consistent, such as by eliminating the difference between
    “broadview and queen” and “queen and broadview.” With this transformation, the
    total reduction in unique values is 36%:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，对于所有街道交叉口的位置值，使街道名称的顺序一致，例如通过消除“broadview and queen”和“queen and broadview”之间的差异。这种转换后，唯一值的总减少率为36%：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: By cleaning up the location values (making all values lowercase, removing duplicate
    values, and ensuring a consistent order for junction pairs), we have reduced the
    number of distinct location values from more than 15,000 to 10,074—a drop of 36%.
    In addition to making the training data reflect the real world more accurately,
    reducing the number of unique values in this column can save us real money, as
    we will see in section 4.6.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 通过清理位置值（将所有值转换为小写，删除重复值，并确保连接对的一致顺序），我们将独特的位置值数量从超过15,000个减少到10,074个——减少了36%。除了使训练数据更准确地反映现实世界外，减少此列中唯一值的数量还可以为我们节省实际费用，正如我们在第4.6节中将要看到的。
- en: '4.6 Going the distance: Locations'
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 走得更远：位置
- en: 'Is the cleanup described in section 4.5 all we can do to clean up the Location
    values? One more possible transformation can give us high-fidelity values in the
    Location column: replace freeform text Location values with longitude and latitude.
    The geocode preparation notebook contains an approach to this transformation that
    uses Google’s geocoding API ([http://mng.bz/X06Y](http://mng.bz/X06Y)).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 第4.5节中描述的清理是我们对位置值可以做的所有清理吗？还有一种可能的转换可以给我们位置列中的高保真值：用经纬度替换自由文本位置值。地理编码准备笔记本包含了一种使用谷歌地理编码API（[http://mng.bz/X06Y](http://mng.bz/X06Y)）进行这种转换的方法。
- en: Why would we want to replace the freeform text locations with longitude and
    latitude? Advantages of using longitude and latitude instead of freeform locations
    include
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为什么要用经纬度来替换自由文本位置？使用经纬度而不是自由文本位置的优势包括
- en: Locations are as exact as the geocode API can make them.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置精确到地理编码API可以做到的程度。
- en: Output values are numeric and can be used to train a deep learning model directly.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出值是数值的，可以直接用于训练深度学习模型。
- en: Disadvantages include
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 不利之处包括
- en: Thirty-five percent of the locations are not street junctions/addresses, but
    locations that are specific to the streetcar network, such as “birchmount yard.”
    Some of these locations will not resolve to longitude and latitude values.
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 35%的位置不是街道交叉口/地址，而是特定于电车网络的地点，例如“birchmount yard”。其中一些地点无法解析为经纬度值。
- en: 'The topology of the streetcar network and the pattern of effect of delays are
    not directly related to a real-world map. Consider two potential locations for
    delays: “king and church” = latitude 43.648949 / longitude -79.377754 and “queen
    and church” = latitude 43.652908 / longitude -79.379458\. From the point of view
    of latitude and longitude, these two locations are close together, but in terms
    of the streetcar network, they are distant because they are on separate lines.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电车网络拓扑和延迟影响模式与实际地图没有直接关系。考虑两个潜在的延迟位置：“king and church” = 纬度43.648949 / 经度-79.377754和“queen
    and church” = 纬度43.652908 / 经度-79.379458。从纬度和经度的角度来看，这两个位置很近，但在电车网络中它们相距甚远，因为它们位于不同的线路上。
- en: This section (and the code in the geocode preparation notebook) assume that
    you are using Google’s geocoding API. This approach has several advantages, including
    copious documentation and a wide user base, but it’s not free. To get the latitude
    and longitude values for a dataset with 60,000 locations in a single batch run
    (that is, not spreading the run over multiple days to keep it within the free
    clip level for geocoding API calls), you can expect to spend around $50\. Several
    alternatives to the Google geocoding API could save you money. Locationiq ([https://locationiq.com](https://locationiq.com)),
    for example, provides a free tier that allows you to process a larger dataset
    of locations in fewer iterations than you would need to stay within the free limit
    for Google’s geocoding API.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 本节（以及地理编码准备笔记本中的代码）假设您正在使用谷歌的地理编码API。这种方法有几个优点，包括丰富的文档和广泛的用户基础，但它不是免费的。为了在单次批量运行中（即，不是将运行分散到多天以保持在地理编码API调用免费剪辑级别内）获取包含60,000个位置的数据集的经纬度值，您可能需要花费大约50美元。Google地理编码API的几个替代方案可以节省您费用。例如，Locationiq
    ([https://locationiq.com](https://locationiq.com)) 提供了一个免费层，允许您在少于您需要保持Google地理编码API免费限制的迭代次数中处理更大的位置数据集。
- en: Because calls to the Google geocoding API are not free, and because an account
    is limited to the number of calls to this API that it can make in a 24-hour period,
    it’s important to prepare the location data so that we make the minimum number
    of geocoding API calls to get the latitude and longitude values.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 由于调用谷歌地理编码API不是免费的，并且由于账户限制在24小时内可以对该API进行的调用次数，因此准备位置数据非常重要，以便我们尽可能少地进行地理编码API调用以获取经纬度值。
- en: 'To make the minimal number of geocoding API calls, we start by defining a new
    dataframe, `df_unique` , with a single column that contains exactly the list of
    unique Location values:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化地理编码API调用次数，我们首先定义一个新的DataFrame，`df_unique`，其中包含一个单列，该列恰好包含唯一位置值的列表：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Figure 4.4 shows a snippet of rows from this new dataframe.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4展示了这个新数据框的行片段。
- en: '![CH04_F04_Ryan](../Images/CH04_F04_Ryan.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F04_Ryan](../Images/CH04_F04_Ryan.png)'
- en: Figure 4.4 Dataframe that contains only distinct location values
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 仅包含唯一位置值的DataFrame
- en: We define a function that calls the Google geocode API, takes a location as
    an argument, and returns a JSON structure. If the structure returned is not empty,
    parse the structure, and return a list with the latitude and longitude values.
    If the structure is empty, return a placeholder value, as shown in the following
    listing.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个函数，该函数调用谷歌地理编码API，以位置作为参数，并返回一个JSON结构。如果返回的结构不为空，则解析该结构，并返回一个包含经纬度值的列表。如果结构为空，则返回一个占位符值，如下面的列表所示。
- en: Listing 4.4 Code to get the latitude and longitude values for a street junction
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.4 获取街道交叉口经纬度值的代码
- en: '[PRE8]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Check to see whether the result is empty.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 检查结果是否为空。
- en: 'If we call this function with a location that the geocoding can interpret,
    we get back a list with the corresponding latitude and longitude values:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们用地理编码可以解释的位置调用此函数，我们将返回一个包含相应纬度和经度值的列表：
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'If we call this function with a location that the geocoding cannot interpret,
    we get the placeholder value back:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们调用此函数时使用地理位置编码无法解释的位置，我们将返回占位符值：
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We call the `get_geocode_results` function to create a new column in this dataframe
    that contains the longitude and latitude values. Dumping both values into one
    column requires doing some additional processing to get what we want: individual
    columns for longitude and latitude. But making the call this way reduces the number
    of geocoding API calls we need to make, saving money and helping us keep within
    the daily limit for geocoding API calls:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们调用`get_geocode_results`函数在此数据框中创建一个新列，该列包含经纬度值。将这两个值都放入一个列中需要做一些额外的处理以获得我们想要的结果：单独的经纬度列。但以这种方式调用可以减少我们需要进行的地理编码API调用次数，节省金钱并帮助我们保持在每日地理编码API调用限制内：
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Next, we create individual columns for longitude and latitude:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建单独的经纬度列：
- en: '[PRE12]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, we join the `df_unique dataframe` with the original dataframe to get
    the longitude and latitude columns added to the original dataframe:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将`df_unique dataframe`与原始数据框合并，以获取添加到原始数据框中的经纬度列：
- en: '[PRE13]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As you can see, it takes several steps (including the initial setup of the Google
    geocoding API) to get latitude and longitude values added to the dataset. Understanding
    how to manipulate latitude and longitude values in the context of a Python program
    is a useful skill for many common business problems that have a spatial dimension,
    and latitude and longitude values make it possible to create visualizations (such
    as the one in figure 4.5) to identify hotspots for delays. As it turns out, the
    deep learning model described in chapters 5 and 6 does not use location data—either
    the freeform text or latitude and longitude. The freeform text locations cannot
    be used in the refactored dataset, and the process for converting freeform text
    locations into latitude and longitude is complicated and difficult to integrate
    into a pipeline. Chapter 9 includes a section on augmenting the model to incorporate
    location data to identify the subsection of a streetcar route on which a delay
    occurs.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，需要几个步骤（包括Google地理位置编码API的初始设置）才能将经纬度值添加到数据集中。在Python程序中理解如何操作经纬度值对于许多具有空间维度的常见商业问题来说是一项有用的技能，经纬度值使得创建可视化（如图4.5所示）以识别延误热点成为可能。实际上，第5章和第6章中描述的深度学习模型没有使用位置数据——无论是自由文本还是经纬度。自由文本位置不能用于重构后的数据集，将自由文本位置转换为经纬度的过程复杂且难以集成到管道中。第9章包括一个关于增强模型以包含位置数据以识别电车路线上的延误子段的章节。
- en: '![CH04_F05_Ryan](../Images/CH04_F05_Ryan.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F05_Ryan](../Images/CH04_F05_Ryan.png)'
- en: Figure 4.5 Heat map showing hotspots for streetcar delays
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 显示电车延误热点图的散点图
- en: 4.7 Fixing type mismatches
  id: totrans-112
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 修复类型不匹配
- en: 'To get the types that get assigned to the dataframe, you can use the `dtypes`
    attribute of the dataframe. The value of this attribute for the dataframe that
    is initially ingested for the streetcar delay dataset looks like this:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取分配给数据框的类型，您可以使用数据框的`dtypes`属性。对于最初用于电车延误数据集的数据框，此属性的值如下所示：
- en: '[PRE14]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Python does a good job of predicting the types of data as that data is ingested,
    but it is not perfect. Fortunately, it’s easy to ensure that you don’t get type
    surprises. This code ensures that continuous columns have a predictable type:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Python在数据摄入时预测数据类型做得很好，但并不完美。幸运的是，确保您不会遇到类型惊喜很容易。此代码确保连续列具有可预测的类型：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Similarly, values that look numeric, such as vehicle IDs, can be misinterpreted
    by Python, which assigns `float64` type to the Vehicle column (figure 4.6).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，看起来像数字的值，如车辆ID，可能会被Python错误地解释，Python将`float64`类型分配给车辆列（图4.6）。
- en: '![CH04_F06_Ryan](../Images/CH04_F06_Ryan.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F06_Ryan](../Images/CH04_F06_Ryan.png)'
- en: Figure 4.6 Python misinterprets the type of the Vehicle column.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 Python错误地解释了车辆列的类型。
- en: 'To correct these types, we use the `astype` function to cast the columns to
    string type and then clip the end of the Vehicle column to remove the vestigial
    decimal point and zero:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了纠正这些类型，我们使用`astype`函数将列转换为字符串类型，然后剪切车辆列的末尾以移除残留的小数点和零：
- en: '[PRE16]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 4.8 Dealing with rows that still contain bad data
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 处理仍然包含不良数据的行
- en: After all the cleanup, how many bad values are left in the dataset?
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有清理工作完成后，数据集中还剩下多少不良值？
- en: '[PRE17]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Compare this result with the total number of rows in the dataset:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 将此结果与数据集中的总行数进行比较：
- en: '[PRE18]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What happens to the size of the dataset if we remove all the rows that contain
    one or more of the remaining bad values?
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们移除包含一个或多个剩余不良值的所有行，数据集的大小会发生什么变化？
- en: '[PRE19]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Removing the bad values removes about 20% of the data. The question is this:
    What is the result of removing the bad values on the model performance? Chapter
    7 describes an experiment that compares the results of training the model with
    and without the bad values. Figure 4.7 shows the result of this experiment.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 移除不良值大约移除了20%的数据。问题是：移除不良值对模型性能的影响是什么？第7章描述了一个实验，比较了在有无不良值的情况下训练模型的结果。图4.7显示了该实验的结果。
- en: '![CH04_F07_Ryan](../Images/CH04_F07_Ryan.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F07_Ryan](../Images/CH04_F07_Ryan.png)'
- en: Figure 4.7 Comparison of model performance with and without bad values in the
    training dataset
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 训练数据集中有无不良值时模型性能的比较
- en: Although validation accuracy of the trained model with bad values not removed
    is around the same as validation accuracy for the model trained with bad values
    removed, recall and false negative count are much worse when we don’t remove bad
    values. We can conclude that removing bad values is good for the performance of
    the trained model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管未移除不良值的训练模型的验证准确率与移除不良值的训练模型的验证准确率大致相同，但当我们不移除不良值时，召回率和假阴性计数要差得多。我们可以得出结论，移除不良值对训练模型的性能有益。
- en: 4.9 Creating derived columns
  id: totrans-133
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.9 创建派生列
- en: In some situations, you will want to create new columns derived from the columns
    in the original dataset. A column with date values (such as Report Date in the
    streetcar dataset) includes information (such as year, month, and day) that could
    be pulled into separate derived columns that could help the performance of the
    model.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，您可能希望创建由原始数据集中的列派生的新列。具有日期值（如街车数据集中的报告日期）的列包含信息（如年、月和日），这些信息可以被拉入单独的派生列中，这可能有助于提高模型的性能。
- en: Figure 4.8 shows the dataframe before derived columns based on Report Date are
    added.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8显示了在添加基于报告日期的派生列之前的数据框。
- en: '![CH04_F08_Ryan](../Images/CH04_F08_Ryan.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F08_Ryan](../Images/CH04_F08_Ryan.png)'
- en: Figure 4.8 Dataframe before derived columns based on Report Date are added
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 在添加基于报告日期的派生列之前的数据框
- en: 'Here is the code to create explicit columns for year, month, and day of month
    from the existing Report Date column:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是创建年份、月份和月份日从现有报告日期列中显式列的代码：
- en: '[PRE20]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Figure 4.9 shows what the dataframe looks like after the derived columns are
    added.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了在添加派生列后数据框的样式。
- en: '![CH04_F09_Ryan](../Images/CH04_F09_Ryan.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F09_Ryan](../Images/CH04_F09_Ryan.png)'
- en: Figure 4.9 Dataframe with derived columns based on Report Date
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 基于报告日期的派生列DataFrame
- en: In chapter 5, we will generate derived columns as part of the process of refactoring
    the dataset. By pulling year, month, and day of month out of the Report Date column
    and into their own columns, we simplify the process of deployment by making it
    straightforward to get date/time trip information from the user.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们将生成派生列作为重构数据集过程的一部分。通过将年份、月份和月份日从报告日期列中提取出来并放入它们自己的列中，我们简化了部署过程，使得从用户那里获取日期/时间信息变得直接。
- en: 4.10 Preparing non-numeric data to train a deep learning model
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.10 准备非数值数据以训练深度学习模型
- en: Machine learning algorithms can be trained only on numeric data, so any non-numeric
    data needs to be transformed into numeric data. Figure 4.10 shows the dataframe
    with the categorical values in their original states.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习算法只能在数值数据上训练，因此任何非数值数据都需要转换为数值数据。图4.10显示了具有原始分类值的DataFrame。
- en: '![CH04_F10_Ryan](../Images/CH04_F10_Ryan.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F10_Ryan](../Images/CH04_F10_Ryan.png)'
- en: Figure 4.10 Dataframe before categorical and text column values are replaced
    by numeric IDs
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 在将分类和文本列值替换为数值ID之前的数据框
- en: 'You can take either of two general approaches to replace categorical values
    with numerical values: *label encoding* , in which each unique categorical value
    in the column is replaced by a numeric identifier, or *one-hot encoding* , in
    which a new column gets generated for each unique categorical value. Rows get
    a `1` in the new column representing their original categorical value and a `0`
    in the other new columns.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以采取两种一般方法中的任何一种来用数值值替换分类值：*标签编码*，其中列中的每个唯一分类值被替换为一个数值标识符，或者*独热编码*，其中为每个唯一分类值生成一个新列。行在新列中表示其原始分类值时为`1`，在其他新列中为`0`。
- en: Label encoding can cause problems for some machine learning algorithms that
    assign significance to the relative values of the numeric identifiers when they
    have no significance. If numeric identifiers replace the values for provinces
    of Canada, for example, starting with `0` for Newfoundland and Labrador and ending
    with `9` for British Columbia, it is not significant that the identifier for Alberta
    (`8`) is less than the identifier for British Columbia.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 标签编码可能会对一些机器学习算法造成问题，这些算法在数值标识符没有实际意义时，会赋予它们相对值的重要性。例如，如果数值标识符用于替换加拿大各省的值，从新foundland
    and Labrador的`0`开始，到British Columbia的`9`结束，那么Alberta的标识符（`8`）小于British Columbia的标识符并不具有意义。
- en: One-hot encoding has its problems as well. If a column has more than a few values,
    one-hot encoding can generate an explosion of new columns that can gobble up memory
    and make manipulation of the dataset difficult. For the streetcar delay dataset,
    we are sticking with label encoding to control the number of columns in the dataset.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot encoding also has its problems. If a column has more than a few values,
    one-hot encoding can generate an explosion of new columns that can gobble up memory
    and make manipulation of the dataset difficult. For the streetcar delay dataset,
    we are sticking with label encoding to control the number of columns in the dataset.
- en: The code snippet in the next listing, from the `encode_categorical` class defined
    in `custom_classes` , uses the `LabelEncoder` function from the sci-kit learn
    library to replace the values in categorical columns with numeric identifiers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表中的代码片段，来自在`custom_classes`中定义的`encode_categorical`类，使用了sci-kit learn库中的`LabelEncoder`函数，将分类列中的值替换为数值标识符。
- en: Listing 4.5 Code to replace categorical column values with numeric identifiers
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 替换分类列值以使用数值标识符的代码
- en: '[PRE21]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Create an instance of LabelEncoder.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个LabelEncoder的实例。
- en: ❷ Use the instance of LabelEncoder to replace values in a categorical column
    with numeric identifiers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用LabelEncoder的实例将分类列中的值替换为数值标识符。
- en: For a complete description of this class, see the description of pipelines in
    chapter 8.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个类的完整描述，请参阅第8章中关于管道的描述。
- en: Figure 4.11 shows what the basic dataframe looks like after the categorical
    columns Day, Direction, Route, hour, month, Location, daym, and year are encoded.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 展示了在将分类列Day、Direction、Route、hour、month、Location、daym和year进行编码后，基本数据框的外观。
- en: '![CH04_F11_Ryan](../Images/CH04_F11_Ryan.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F11_Ryan](../Images/CH04_F11_Ryan.png)'
- en: Figure 4.11 Dataframe with categorical column values replaced by numeric IDs
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 替换分类列值后的数据框
- en: 'One column still contains non-numerical data: Incident. This column contains
    multiword phrases that describe the kind of delay that occurred. If you recall
    the data exploration that we did in chapter 3, we determined that Incident can
    be treated as a categorical column. To illustrate how text columns are prepared,
    let’s treat it as a text column for now, and apply the Python Tokenizer API to
    do the following:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 仍然有一列包含非数值数据：Incident。这列包含描述发生延误类型的短语。如果您还记得我们在第3章中进行的探索，我们确定Incident可以被处理为一个分类列。为了说明如何准备文本列，让我们现在将其视为一个文本列，并应用Python
    Tokenizer API来完成以下操作：
- en: Change all values to lowercase.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有值转换为小写。
- en: Remove punctuation.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 删除标点符号。
- en: Replace all words with numeric IDs.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有单词替换为数值标识符。
- en: The following listing contains the code that accomplishes this transformation.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表包含了完成这种转换的代码。
- en: Listing 4.6 Code to prepare text columns to be part of a model training dataset
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 准备文本列以成为模型训练数据集一部分的代码
- en: '[PRE22]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Tokenizer lowercases and removes punctuation by default.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Tokenizer默认将文本转换为小写并删除标点符号。
- en: Figure 4.12 shows the result of applying this transformation to the Incident
    column.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 展示了将这种转换应用于Incident列的结果。
- en: '![CH04_F12_Ryan](../Images/CH04_F12_Ryan.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F12_Ryan](../Images/CH04_F12_Ryan.png)'
- en: Figure 4.12 Dataframe after values in categorical and text columns are replaced
    by numeric IDs
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 替换分类和文本列中的值后的数据框
- en: 'Figure 4.13 compares the before and after values in the Incident column. Each
    entry in the Incident column is now a list (or array, if you prefer a less Pythonic
    term) of numeric IDs. Note that the After view has a list entry for each word
    in the original column, and IDs are assigned consistently (the same word gets
    assigned a consistent ID regardless of where it appears in the column). Also note
    the following:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13比较了Incident列中的前后值。Incident列中的每个条目现在都是一个列表（或数组，如果您更喜欢不那么Python化的术语）的数值标识符。请注意，在“After”视图中，每个原始列中的单词都有一个列表条目，并且ID分配是一致的（相同的单词无论在列中出现的位置如何，都会分配一个一致的ID）。同时请注意以下几点：
- en: In the end, we treated Incident as a categorical column, which means that multitoken
    values like “Emergency Services” were encoded to a single numeric value. In this
    section, we treated Incident as a text column to illustrate how this portion of
    the code works, so each token in “Emergency Services” is encoded separately.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们将事件视为一个分类列，这意味着多令牌值如“Emergency Services”被编码为单个数值。在本节中，我们将事件视为一个文本列，以说明这部分代码的工作原理，因此“Emergency
    Services”中的每个令牌都被单独编码。
- en: We have illustrated only a single text column, but the code works with datasets
    that have multiple text columns.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们只展示了一个文本列，但代码可以处理具有多个文本列的数据集。
- en: '![CH04_F13_Ryan](../Images/CH04_F13_Ryan.png)'
  id: totrans-174
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH04_F13_Ryan](../Images/CH04_F13_Ryan.png)'
- en: Figure 4.13 Incident column before and after text values are encoded
  id: totrans-175
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图4.13在文本值编码前后的事件列
- en: 4.11 Overview of the end-to-end solution
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.11端到端解决方案概述
- en: 'Before we examine deep learning and the stack that we’ll use to tackle the
    streetcar delay problem, let’s look at the entire solution for the problem by
    revisiting the end-to-end picture introduced in chapter 2\. Figure 4.14 shows
    all the major elements that make up the solution, from the input dataset to the
    trained model deployed and accessed via a simple website or Facebook Messenger.
    The components are grouped in three sections: cleaning up the dataset, building
    and training the model, and deploying the model.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们检查深度学习和我们将用于解决电车延误问题的堆栈之前，让我们通过回顾第2章中引入的端到端图来重新审视整个问题的解决方案。图4.14显示了构成解决方案的所有主要元素，从输入数据集到通过简单网站或Facebook
    Messenger部署和访问的训练模型。组件分为三个部分：清洗数据集、构建和训练模型以及部署模型。
- en: '![CH04_F14_Ryan](../Images/CH04_F14_Ryan.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F14_Ryan](../Images/CH04_F14_Ryan.png)'
- en: Figure 4.14 Summary of the complete streetcar delay project
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14完整电车延误项目的总结
- en: Figure 4.15 zooms in on the components that we used to go from the input dataset
    to a cleaned-up dataset in chapters 2, 3 and 4, including Python and the Pandas
    library for dealing with tabular data.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15放大了我们用于在第2、3和4章中从输入数据集到清洗后的数据集的组件，包括用于处理表格数据的Python和Pandas库。
- en: '![CH04_F15_Ryan](../Images/CH04_F15_Ryan.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F15_Ryan](../Images/CH04_F15_Ryan.png)'
- en: Figure 4.15 From input dataset to cleaned-up dataset
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15从输入数据集到清洗后的数据集
- en: Figure 4.16 zooms in on the components that we will use in chapters 5 and 6
    to build and train the deep learning model, including the deep learning library
    Keras and the pipeline facilities in the scikit-learn library.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16放大了我们将在第5章和第6章中用于构建和训练深度学习模型的组件，包括深度学习库Keras和scikit-learn库中的管道工具。
- en: '![CH04_F16_Ryan](../Images/CH04_F16_Ryan.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F16_Ryan](../Images/CH04_F16_Ryan.png)'
- en: Figure 4.16 From cleaned-up dataset to trained deep learning model and pipeline
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16从清洗后的数据集到训练好的深度学习模型和管道
- en: Figure 4.17 zooms in on the components that we will use in chapter 8 to deploy
    the trained deep learning model. For the web deployment, these components include
    the Flask web deployment library (to render HTML web pages on which the user can
    specify the details of their streetcar trip and see the trained deep learning
    model’s prediction of whether the trip will be delayed). For the Facebook Messenger
    deployment, these components include the Rasa chatbot framework, ngrok for connecting
    Rasa and Facebook, and Facebook application configurations.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17放大了我们将在第8章中用于部署训练好的深度学习模型的组件。对于Web部署，这些组件包括Flask Web部署库（用于渲染HTML网页，用户可以在其中指定他们的电车之旅的详细信息，并查看训练好的深度学习模型对行程是否会延误的预测）。对于Facebook
    Messenger部署，这些组件包括Rasa聊天机器人框架、ngrok用于连接Rasa和Facebook，以及Facebook应用程序配置。
- en: '![CH04_F17_Ryan](../Images/CH04_F17_Ryan.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F17_Ryan](../Images/CH04_F17_Ryan.png)'
- en: Figure 4.17 From trained model and pipeline to deployed deep learning model
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17从训练模型和管道到部署的深度学习模型
- en: 'Figures 4.18 and 4.19 show the goal: a trained deep learning model that has
    been deployed and can be used to get predictions on whether a given streetcar
    trip is going to be delayed, either on a web page (figure 4.18) or in Facebook
    Messenger (figure 4.19).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18和图4.19展示了目标：一个已部署并可用于预测给定电车之旅是否会延误的深度学习模型，无论是在网页上（图4.18）还是在Facebook Messenger上（图4.19）。
- en: '![CH04_F18_Ryan](../Images/CH04_F18_Ryan.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F18_Ryan](../Images/CH04_F18_Ryan.png)'
- en: Figure 4.18 Streetcar delay predictions available on a web page
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18网页上可用的电车延误预测
- en: In this section, we’ve gone through a brief overview of the end-to-end streetcar
    delay prediction project. Over the next four chapters, we will go over all the
    steps needed to get from the cleaned-up dataset in this chapter to a simple web
    page that uses the trained model to predict whether a given streetcar trip is
    going to be delayed.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们已经简要概述了端到端电车延误预测项目。在接下来的四章中，我们将详细介绍从本章的清洗数据集到使用训练模型预测特定电车行程是否会延误的简单网页所需的全部步骤。
- en: '![CH04_F19_Ryan](../Images/CH04_F19_Ryan.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![CH04_F19_Ryan](../Images/CH04_F19_Ryan.png)'
- en: Figure 4.19 Streetcar delay predictions available in Facebook Messenger
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 在Facebook Messenger中可用的电车延误预测
- en: Summary
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Cleaning up the data isn’t the only preparation we need to perform on the dataset.
    We also need to ensure that non-numeric values (such as strings in categorical
    columns) get converted to numeric values.
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗数据并不是我们在数据集上需要进行的唯一准备工作。我们还需要确保非数值值（例如分类列中的字符串）被转换为数值。
- en: If the dataset contains invalid values (such as route values that aren’t actual
    streetcar routes or direction values that cannot be mapped to one of the compass
    points), these values can be replaced by a valid placeholder (such as the most
    common value in a categorical column), or the records that contain them can be
    removed from the dataset.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据集包含无效值（例如不是实际电车路线的路线值或无法映射到任何一个方向上的方向值），这些值可以被替换为一个有效的占位符（例如分类列中最常见的值），或者包含这些值的记录可以从数据集中删除。
- en: When a CSV or XLS file is ingested into a Pandas dataframe, the type of the
    columns is not always assigned correctly. If Python assigns an incorrect type
    to a column, you can convert the column to the required type.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当CSV或XLS文件被导入Pandas数据框时，列的类型并不总是被正确分配。如果Python分配了错误的类型给一个列，你可以将该列转换为所需的类型。
- en: String values in categorical columns need to be mapped to numeric values because
    you cannot train a deep learning model with non-numeric data. You can do this
    mapping by using the `LabelEncoder` function from the scikit-learn library.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类列中的字符串值需要映射到数值，因为无法使用非数值数据训练深度学习模型。你可以通过使用scikit-learn库中的`LabelEncoder`函数来完成此映射。
