- en: Chapter 13\. Eigendecomposition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第13章 特征分解
- en: 'Eigendecomposition is a pearl of linear algebra. What is a pearl? Let me quote
    directly from the book *20,000 Leagues Under the Sea*:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分解是线性代数中的一颗明珠。那么，什么是明珠呢？让我直接引用《海底两万里》中的一段话：
- en: For poets, a pearl is a tear from the sea; for Orientals, it’s a drop of solidified
    dew; for the ladies it’s a jewel they can wear on their fingers, necks, and ears
    that’s oblong in shape, glassy in luster, and formed from mother-of-pearl; for
    chemists, it’s a mixture of calcium phosphate and calcium carbonate with a little
    gelatin protein; and finally, for naturalists, it’s a simple festering secretion
    from the organ that produces mother-of-pearl in certain bivalves.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 对于诗人来说，珍珠是海洋之泪；对东方人来说，它是凝固了的露珠滴；对女士们来说，它是可以戴在手指、颈部和耳朵上的珠宝，呈椭圆形，具有玻璃般的光泽，由珍珠母形成；对化学家来说，它是一种含有少量明胶蛋白的磷酸钙和碳酸钙的混合物；最后，对于自然学家来说，它是某些双壳类动物产生母珍珠的简单腐败分泌物。
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jules Verne
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 朱尔·凡尔纳
- en: 'The point is that the same object can be seen in different ways depending on
    its use. So it is with eigendecomposition: eigendecomposition has a geometric
    interpretation (axes of rotational invariance), a statistical interpetation (directions
    of maximal covariance), a dynamical-systems interpretation (stable system states),
    a graph-theoretic interpretation (the impact of a node on its network), a financial-market
    interpretation (identifying stocks that covary), and many more.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于，相同的对象可以根据其用途以不同方式看待。特征分解也是如此：特征分解有几何解释（旋转不变性的轴）、统计解释（最大协方差的方向）、动力系统解释（稳定系统状态）、图论解释（节点在网络中的影响）、金融市场解释（识别共变的股票）等等。
- en: Eigendecomposition (and the SVD, which, as you’ll learn in the next chapter,
    is closely related to eigendecomposition) is among the most important contributions
    of linear algebra to data science. The purpose of this chapter is to provide you
    an intuitive understanding of eigenvalues and eigenvectors—the results of eigendecomposition
    of a matrix. Along the way, you’ll learn about diagonalization and more special
    properties of symmetric matrices. After extending eigendecomposition to the SVD
    in [Chapter 14](ch14.xhtml#Chapter_14), you’ll see a few applications of eigendecomposition
    in [Chapter 15](ch15.xhtml#Chapter_15).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 特征分解（以及奇异值分解，正如你将在下一章学到的那样，与特征分解密切相关）是线性代数对数据科学的最重要贡献之一。本章的目的是为你提供特征值和特征向量的直观理解——矩阵特征分解的结果。在这过程中，你将学到对称矩阵的对角化和更多特殊性质。在[第14章](ch14.xhtml#Chapter_14)中扩展到奇异值分解后，你将看到特征分解在[第15章](ch15.xhtml#Chapter_15)中的几个应用。
- en: Interpretations of Eigenvalues and Eigenvectors
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[特征值和特征向量的解释](https://example.org/eigenvalues_and_eigenvectors_interpretations)'
- en: There are several ways of interpreting eigenvalues/vectors that I will describe
    in the next sections. Of course, the math is the same regardless, but having mutliple
    perspectives can facilitate intuition, which in turn will help you understand
    how and why eigendecomposition is important in data science.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种解释特征值/向量的方式，我将在接下来的部分中描述。当然，无论如何，数学都是相同的，但多视角可以促进直觉，进而帮助你理解为什么特征分解在数据科学中如此重要。
- en: Geometry
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 几何学
- en: I’ve actually already introduced you to the geometric concept of eigenvectors
    in [Chapter 5](ch05.xhtml#Chapter_5). In [Figure 5-5](ch05.xhtml#fig_5_5), we
    discovered that there was a special combination of a matrix and a vector such
    that the matrix *stretched*—but did not *rotate*—that vector. That vector is an
    eigenvector of the matrix, and the amount of stretching is the eigenvalue.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我实际上已经在[第5章](ch05.xhtml#Chapter_5)中向你介绍了特征向量的几何概念。在[图5-5](ch05.xhtml#fig_5_5)中，我们发现有一种特殊的矩阵和向量的组合，使得矩阵*拉伸*了这个向量，但没有*旋转*。那个向量是矩阵的特征向量，而拉伸的程度是特征值。
- en: '[Figure 13-1](#fig_13_1) shows vectors before and after multiplication by a
    <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    matrix. The two vectors in the left plot ( <math alttext="bold v 1 and bold v
    2"><mrow><msub><mi>𝐯</mi> <mn>1</mn></msub> <mtext>and</mtext> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></math> ) are eigenvectors whereas the two vectors in
    the right plot are not. The eigenvectors point in the same direction before and
    after postmultiplying the matrix. The eigenvalues encode the amount of stretching;
    try to guess the eigenvalues based on visual inspection of the plot. The answers
    are in the footnote.^([1](ch13.xhtml#idm45733293579328))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 13-1](#fig_13_1) 展示了在 <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo>
    <mn>2</mn></mrow></math> 矩阵后乘以前后两个向量的情况。左图中的两个向量（ <math alttext="粗体 v 1 和粗体 v
    2"><mrow><msub><mi>𝐯</mi> <mn>1</mn></msub> <mtext>和</mtext> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></math> ）是特征向量，而右图中的两个向量则不是。特征向量在矩阵后乘之前后指向相同方向。特征值编码了拉伸的量；试着通过视觉检查图表猜测特征值。答案在脚注中。^([1](ch13.xhtml#idm45733293579328))'
- en: 'That’s the geometric picture: an eigenvector means that matrix-vector multiplication
    acts like scalar-vector multiplication. Let’s see if we can write that down in
    an equation (we can, and it’s printed in [Equation 13-1](#eigen-eq)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是几何图像：特征向量意味着矩阵-向量乘法的效果类似于标量-向量乘法。让我们看看能否用方程表示出来（我们可以，并且它在 [方程式 13-1](#eigen-eq)
    中打印出来）。
- en: Equation 13-1\. Eigenvalue equation
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 13-1\. 特征值方程
- en: <math alttext="bold upper A bold v equals lamda bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></math>
- en: 'Be careful with interpreting that equation: it does not say that the matrix
    *equals* the scalar; it says that the *effect* of the matrix on the vector is
    the same as the *effect* of the scalar on that same vector.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在解释这个方程时要小心：它并不是说矩阵*等于*标量；它说的是矩阵对向量的*作用*与标量对同一向量的*作用*是相同的。
- en: '![geoeig](assets/plad_1301.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![geoeig](assets/plad_1301.png)'
- en: Figure 13-1\. Geometry of eigenvectors
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-1\. 特征向量的几何形状
- en: This is called the *eigenvalue equation*, and it’s another key formula in linear
    algebra that is worth memorizing. You’ll see it throughout this chapter, you’ll
    see a slight variation of it in the following chapter, and you’ll see it many
    times when learning about multivariate statistics, signal processing, optimization,
    graph theory, and a myriad of other applications where patterns are identified
    across multiple simultaneously recorded features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为*特征值方程*，它是线性代数中另一个值得记忆的关键公式。在本章中你会看到它，接下来的章节中会看到稍微变化的形式，以及在学习多变量统计、信号处理、优化、图论以及许多其他应用中会看到它，这些应用中跨多个同时记录的特征识别出模式。
- en: Statistics (Principal Components Analysis)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 统计学（主成分分析）
- en: One of the reasons why people apply statistics is to identify and quantify relationships
    between variables. For example, the rise of global temperatures correlates with
    the decline in the number of pirates,^([2](ch13.xhtml#idm45733293558128)) but
    how strong is that relationship? Of course, when you have only two variables,
    a simple correlation (like what you learned in [Chapter 4](ch04.xhtml#Chapter_4))
    is sufficient. But in a multivariate dataset that includes dozens or hundreds
    of variables, bivariate correlations cannot reveal global patterns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 人们应用统计的原因之一是识别和量化变量之间的关系。例如，全球温度上升与海盗数量下降相关，^([2](ch13.xhtml#idm45733293558128))
    但这种关系有多强？当然，当你只有两个变量时，像你在 [第四章](ch04.xhtml#Chapter_4) 中学到的简单相关就足够了。但在包含数十个或数百个变量的多变量数据集中，双变量相关性无法揭示全局模式。
- en: Let’s make this more concrete with an example. Cryptocurrencies are digital
    stores of value that are encoded in a blockchain, which is a system for keeping
    track of transactions. You’ve probably heard of Bitcoin and Ethereum; there are
    tens of thousands of other cryptocoins that have various purposes. We can ask
    whether the entirety of the cryptospace operates as a single system (meaning that
    the value of all coins go up and down together), or whether there are independent
    subcategories within that space (meaning that some coins or groups of coins change
    in value independently of the value of other coins).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个例子来具体化这个概念。加密货币是数字价值存储，编码在区块链中，这是一种跟踪交易的系统。你可能听说过比特币和以太坊；还有成千上万其他具有各种目的的加密币。我们可以询问整个加密空间是否作为一个单一系统运作（意味着所有币值同时上下波动），或者在该空间内是否存在独立的子类别（意味着一些币或一些币组独立于其他币的价值变化）。
- en: We can test this hypothesis by performing a principal components analysis on
    a dataset that contains the prices of various cryptocoins over time. If the entire
    cryptomarket operates as a single entity, then the *scree plot* (a graph of the
    eigenvalues of the dataset’s covariance matrix) would reveal that one component
    accounts for most of the variance of the system, and all other components account
    for very little variance (graph A in [Figure 13-2](#fig_13_2)). In contrast, if
    the cryptomarket had, say, three major subcategories with independent price movements,
    then we would expect to see three large eigenvalues (graph B in [Figure 13-2](#fig_13_2)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过对包含各种加密货币价格的数据集执行主成分分析来测试这个假设。如果整个加密市场像一个单一实体运作，那么*屏风图*（数据集协方差矩阵特征值的图表）将显示一个成分占系统方差的大部分，而所有其他成分则占极少的方差（图A在[图13-2](#fig_13_2)中）。相反，如果加密市场有三个主要子类别具有独立的价格变动，则我们预计会看到三个较大的特征值（图B在[图13-2](#fig_13_2)中）。
- en: '![geoeig](assets/plad_1302.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![geoeig](assets/plad_1302.png)'
- en: Figure 13-2\. Simulated scree plots of multivariate datasets (data is simulated
    to illustrate outcome possibilities)
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-2\. 模拟多变量数据集的屏风图（数据模拟以说明结果可能性）
- en: Noise Reduction
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 噪声减少
- en: Most datasets contain noise. *Noise* refers to variance in a dataset that is
    either unexplained (e.g., random variation) or unwanted (e.g., electrical line
    noise artifacts in radio signals). There are many ways to attenuate or eliminate
    noise, and the optimal noise-reduction strategy depends on the nature and origin
    of the noise and on the characteristics of the signal.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据集包含噪声。*噪声*是指数据集中未解释的（例如随机变化）或不需要的（例如无线电信号中的电气线噪声伪影）方差。有许多方式可以减弱或消除噪声，最佳的噪声减少策略取决于噪声的性质和来源，以及信号的特征。
- en: One method of reducing random noise is to identify the eigenvalues and eigenvectors
    of a system, and “project out” directions in the data space associated with small
    eigenvalues. The assumption is that random noise makes a relatively small contribution
    to the total variance. “Projecting out” a data dimension means to reconstruct
    a dataset after setting some eigenvalues that are below some threshold to zero.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 减少随机噪音的一种方法是识别系统的特征值和特征向量，并在数据空间中与小特征值相关联的方向上“投影出”这些特征。假设随机噪声对总方差的贡献相对较小。在设置一些低于某个阈值的特征值为零后，"投影出"数据维度意味着重建数据集。
- en: You’ll see an example of using eigendecomposition to reduce noise in [Chapter 15](ch15.xhtml#Chapter_15).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在[第15章](ch15.xhtml#Chapter_15)中看到使用特征分解来减少噪音的示例。
- en: Dimension Reduction (Data Compression)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 维度降低（数据压缩）
- en: Information communications technologies like phones, internet, and TV create
    and transmit a huge amount of data, such as pictures and videos. Transmitting
    data can be time-consuming and expensive, and it is beneficial to *compress* the
    data before transmitting it. Compression means to reduce the size of the data
    (in terms of bytes) while having minimal impact on the quality of the data. For
    example, a TIFF format image file might be 10 MB, while the JPG converted version
    might be .1 MB while still retaining reasonably good quality.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 像电话、互联网和电视这样的信息通信技术创造和传输大量数据，如图片和视频。传输数据可能耗时且昂贵，因此在传输之前进行*压缩*是有益的。压缩意味着减少数据的大小（以字节为单位），同时对数据质量影响最小。例如，TIFF格式的图像文件可能为10
    MB，而转换为JPG格式后的版本可能只有0.1 MB，同时保持相对良好的质量。
- en: One way to dimension-reduce a dataset is to take its eigendecomposition, drop
    the eigenvalues and eigenvectors associated with small directions in the data
    space, and then transmit only the relatively larger eigenvector/value pairs. It
    is actually more common to use the SVD for data compression (and you’ll see an
    example in [Chapter 15](ch15.xhtml#Chapter_15)), although the principle is the
    same.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 降低数据集维度的一种方法是进行其特征分解，舍弃与数据空间中小方向相关的特征值和特征向量，然后仅传输相对较大的特征向量/值对。实际上，使用SVD进行数据压缩更为常见（您将在[第15章](ch15.xhtml#Chapter_15)中看到一个例子），尽管原理是相同的。
- en: 'Modern data compression algorithms are actually faster and more efficient than
    the method previously described, but the idea is the same: decompose the dataset
    into a set of basis vectors that capture the most important features of the data,
    and then reconstruct a high-quality version of the original data.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现代数据压缩算法实际上比先前描述的方法更快更高效，但思想是相同的：将数据集分解为捕获数据最重要特征的一组基向量，然后重建原始数据的高质量版本。
- en: Finding Eigenvalues
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 查找特征值
- en: To eigendecompose a square matrix, you first find the eigenvalues, and then
    use each eigenvalue to find its corresponding eigenvector. The eigenvalues are
    like keys that you insert into the matrix to unlock the mystical eigenvector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 要对一个方阵进行特征分解，首先找到特征值，然后使用每个特征值找到其对应的特征向量。特征值就像是你插入矩阵中以解锁神秘特征向量的钥匙。
- en: 'Finding the eigenvalues of a matrix is super easy in Python:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中查找矩阵的特征值非常容易：
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The two eigenvalues (rounded to the nearest hundredth) are −0.37 and 5.37.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 两个特征值（四舍五入到最接近的百分之一）分别是−0.37和5.37。
- en: But the important question isn’t *which function returns the eigenvalues*; instead,
    the important question is *how are the eigenvalues of a matrix identified?*
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 但重要的问题不是*哪个函数返回特征值*；相反，重要的问题是*如何识别矩阵的特征值*？
- en: To find the eigenvalues of a matrix, we start with the eigenvalue equation shown
    in [Equation 13-1](#eigen-eq) and do some simple arithemetic, as shown in [Equation
    13-2](#eigen-reorg).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到矩阵的特征值，我们从[Equation 13-1](#eigen-eq)中显示的特征值方程式开始，并进行一些简单的算术操作，如[Equation
    13-2](#eigen-reorg)所示。
- en: Equation 13-2\. Eigenvalue equation, reorganized
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式13-2。特征值方程式，重新组织
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold upper A bold v minus lamda bold v
    2nd Column equals bold 0 3rd Row 1st Column left-parenthesis bold upper A minus
    lamda bold upper I right-parenthesis bold v 2nd Column equals bold 0 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi> <mo>-</mo> <mi>λ</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi>
    <mi>𝐈</mi> <mo>)</mo> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold upper A bold v minus lamda bold v
    2nd Column equals bold 0 3rd Row 1st Column left-parenthesis bold upper A minus
    lamda bold upper I right-parenthesis bold v 2nd Column equals bold 0 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi> <mo>-</mo> <mi>λ</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi>
    <mi>𝐈</mi> <mo>)</mo> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
- en: The first equation is an exact repeat of the eigenvalue equation. In the second
    equation, we simply subtracted the right-hand side to set the equation to the
    zeros vector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个方程式完全重复了特征值方程式。在第二个方程式中，我们简单地减去右侧以使方程式等于零向量。
- en: 'The transition from the second to the third equation requires some explanation.
    The left-hand side of the second equation has two vector terms, both of which
    involve <math alttext="bold v"><mi>𝐯</mi></math> . So we factor out the vector.
    But that leaves us with the subtraction of a matrix and a scalar ( <math alttext="bold
    upper A minus lamda"><mrow><mi>𝐀</mi> <mo>-</mo> <mi>λ</mi></mrow></math> ), which
    is not a defined operation in linear algebra.^([3](ch13.xhtml#idm45733293437712))
    So instead, we *shift* the matrix by <math alttext="lamda"><mi>λ</mi></math> .
    That brings us to the third equation. (Side note: the expression <math alttext="lamda
    bold upper I"><mrow><mi>λ</mi> <mi>𝐈</mi></mrow></math> is sometimes called a
    *scalar matrix*.)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从第二个方程式到第三个方程式的过渡需要一些解释。第二个方程式的左侧有两个向量项，都涉及<math alttext="bold v"><mi>𝐯</mi></math>。因此，我们将向量因子化。但这样做会导致我们减去一个矩阵和一个标量（<math
    alttext="bold upper A minus lamda"><mrow><mi>𝐀</mi> <mo>-</mo> <mi>λ</mi></mrow></math>），这在线性代数中不是一个定义良好的操作。^([3](ch13.xhtml#idm45733293437712))
    因此，我们改为将矩阵*平移*<math alttext="lamda"><mi>λ</mi></math>。这就得到了第三个方程式。（旁注：表达式<math
    alttext="lamda bold upper I"><mrow><mi>λ</mi> <mi>𝐈</mi></mrow></math>有时称为*标量矩阵*。）
- en: What does that third equation mean? It means that *the eigenvector is in the
    null space of the matrix shifted by its eigenvalue*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个方程式是什么意思？它意味着*特征向量位于其特征值平移的矩阵的零空间中*。
- en: 'If it helps you understand the concept of the eigenvector as the null-space
    vector of the shifted matrix, you can think of adding two additional equations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这有助于你理解特征向量作为矩阵平移后的零空间向量的概念，你可以考虑添加两个额外的方程式：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A overTilde 2nd Column
    equals bold upper A minus lamda bold upper I 2nd Row 1st Column bold upper A overTilde
    bold v 2nd Column equals bold 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A overTilde 2nd Column
    equals bold upper A minus lamda bold upper I 2nd Row 1st Column bold upper A overTilde
    bold v 2nd Column equals bold 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mover accent="true"><mi>𝐀</mi> <mo>˜</mo></mover>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
- en: Why is that statement so insightful? Remember that we ignore trivial solutions
    in linear algebra, so we do not consider <math alttext="bold v equals bold 0"><mrow><mi>𝐯</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> to be an eigenvector. And
    that means that the matrix shifted by its eigenvalue is singular, because only
    singular matrices have a nontrivial null space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这个陈述如此具有洞察力？请记住，在线性代数中我们忽略平凡解，因此我们不认为<math alttext="bold v equals bold 0"><mrow><mi>𝐯</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math>是一个特征向量。这意味着矩阵经过其特征值平移后是奇异的，因为只有奇异矩阵才有一个非平凡的零空间。
- en: 'And what else do we know about singular matrices? We know that their determinant
    is zero. Hence:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 还有关于奇异矩阵我们知道什么？我们知道它们的行列式为零。因此：
- en: <math alttext="StartAbsoluteValue bold upper A minus lamda bold upper I EndAbsoluteValue
    equals 0" display="block"><mrow><mo>|</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi>
    <mo>|</mo> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue bold upper A minus lamda bold upper I EndAbsoluteValue
    equals 0" display="block"><mrow><mo>|</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi>
    <mo>|</mo> <mo>=</mo> <mn>0</mn></mrow></math>
- en: 'Believe it or not, that’s the key to finding eigenvalues: shift the matrix
    by the unknown eigenvalue <math alttext="lamda"><mi>λ</mi></math> , set its determinant
    to zero, and solve for <math alttext="lamda"><mi>λ</mi></math> . Let’s see how
    this looks for a <math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>
    matrix:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 信不信由你，这就是找到特征值的关键：将矩阵平移至未知特征值<math alttext="lamda"><mi>λ</mi></math>，将其行列式设为零，并解出<math
    alttext="lamda"><mi>λ</mi></math>。让我们看看对于一个<math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>×</mo> <mn>2</mn></mrow></math>矩阵，这是什么样子：
- en: <math alttext="StartLayout 1st Row 1st Column StartAbsoluteValue Start 2 By
    2 Matrix 1st Row 1st Column a 2nd Column b 2nd Row 1st Column c 2nd Column d EndMatrix
    minus lamda Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st
    Column 0 2nd Column 1 EndMatrix EndAbsoluteValue equals 2nd Column 0 2nd Row 1st
    Column Start 2 By 2 Determinant 1st Row 1st Column a minus lamda 2nd Column b
    2nd Row 1st Column c 2nd Column d minus lamda EndDeterminant equals 2nd Column
    0 3rd Row 1st Column left-parenthesis a minus lamda right-parenthesis left-parenthesis
    d minus lamda right-parenthesis minus b c equals 2nd Column 0 4th Row 1st Column
    lamda squared minus left-parenthesis a plus d right-parenthesis lamda plus left-parenthesis
    a d minus b c right-parenthesis equals 2nd Column 0 EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"
    separators=""><mfenced close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd> <mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced> <mo>-</mo>
    <mi>λ</mi> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mo>-</mo> <mi>λ</mi></mrow></mtd> <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd>
    <mtd><mrow><mi>d</mi> <mo>-</mo> <mi>λ</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>a</mi> <mo>-</mo> <mi>λ</mi>
    <mo>)</mo> <mo>(</mo> <mi>d</mi> <mo>-</mo> <mi>λ</mi> <mo>)</mo> <mo>-</mo> <mi>b</mi>
    <mi>c</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>λ</mi> <mn>2</mn></msup> <mo>-</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo></mrow> <mi>λ</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>-</mo> <mi>b</mi> <mi>c</mi>
    <mo>)</mo></mrow> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column StartAbsoluteValue Start 2 By
    2 Matrix 1st Row 1st Column a 2nd Column b 2nd Row 1st Column c 2nd Column d EndMatrix
    minus lamda Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st
    Column 0 2nd Column 1 EndMatrix EndAbsoluteValue equals 2nd Column 0 2nd Row 1st
    Column Start 2 By 2 Determinant 1st Row 1st Column a minus lamda 2nd Column b
    2nd Row 1st Column c 2nd Column d minus lamda EndDeterminant equals 2nd Column
    0 3rd Row 1st Column left-parenthesis a minus lamda right-parenthesis left-parenthesis
    d minus lamda right-parenthesis minus b c equals 2nd Column 0 4th Row 1st Column
    lamda squared minus left-parenthesis a plus d right-parenthesis lamda plus left-parenthesis
    a d minus b c right-parenthesis equals 2nd Column 0 EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"
    separators=""><mfenced close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd> <mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced> <mo>-</mo>
    <mi>λ</mi> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mo>-</mo> <mi>λ</mi></mrow></mtd> <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd>
    <mtd><mrow><mi>d</mi> <mo>-</mo> <mi>λ</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>a</mi> <mo>-</mo> <mi>λ</mi>
    <mo>)</mo> <mo>(</mo> <mi>d</mi> <mo>-</mo> <mi>λ</mi> <mo>)</mo> <mo>-</mo> <mi>b</mi>
    <mi>c</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>λ</mi> <mn>2</mn></msup> <mo>-</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo></mrow> <mi>λ</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>-</mo> <mi>b</mi> <mi>c</mi>
    <mo>)</mo></mrow> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: 'You could apply the quadratic formula to solve for the two <math alttext="lamda"><mi>λ</mi></math>
    values. But the answer itself isn’t important; the important thing is to see the
    logical progression of mathematical concepts established earlier in this book:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以应用二次方程式来求解两个<math alttext="lamda"><mi>λ</mi></math>值。但答案本身并不重要；重要的是看到本书前面建立的数学概念的逻辑进展：
- en: The matrix-vector multiplication acts like scalar-vector multiplication (the
    eigenvalue equation).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵-向量乘法的作用类似标量-向量乘法（特征值方程）。
- en: We set the eigenvalue equation to the zeros vector, and factor out common terms.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将特征值方程设为零向量，并提取公共项。
- en: This reveals that the eigenvector is in the null space of the matrix shifted
    by the eigenvalue. We do not consider the zeros vector to be an eigenvector, which
    means the shifted matrix is singular.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这揭示了特征向量位于被特征值移位的矩阵的零空间中。我们不认为零向量是特征向量，这意味着移位矩阵是奇异的。
- en: Therefore, we set the determinant of the shifted matrix to zero and solve for
    the unknown eigenvalue.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因此，我们将移位矩阵的行列式设为零，并求解未知的特征值。
- en: The determinant of an eigenvalue-shifted matrix set to zero is called the *characteristic
    polynomial* of the matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值移位矩阵的行列式设为零被称为矩阵的*特征多项式*。
- en: Notice that in the previous example, we started with a <math alttext="2 times
    2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math> matrix and got a <math
    alttext="lamda squared"><msup><mi>λ</mi> <mn>2</mn></msup></math> term, which
    means this is a second-order polynomial equation. You might remember from your
    high-school algebra class that an *n*th order polynomial has *n* solutions, some
    of which might be complex-valued (this is called the fundamental theorem of algebra).
    So, there will be two values of <math alttext="lamda"><mi>λ</mi></math> that can
    satisfy the equation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在前面的例子中，我们从一个<math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo> <mn>2</mn></mrow></math>矩阵开始，得到了一个<math
    alttext="lamda squared"><msup><mi>λ</mi> <mn>2</mn></msup></math>项，这意味着这是一个二次多项式方程。你可能还记得从高中代数课上，一个*n*次多项式有*n*个解，其中一些可能是复数（这被称为代数基本定理）。因此，会有两个<math
    alttext="lamda"><mi>λ</mi></math>值满足方程。
- en: 'The matching 2s are no coincidence: the characteristic polynomial of an <math
    alttext="upper M times upper M"><mrow><mi>M</mi> <mo>×</mo> <mi>M</mi></mrow></math>
    matrix will have a <math alttext="lamda Superscript upper M"><msup><mi>λ</mi>
    <mi>M</mi></msup></math> term. That is the reason why an <math alttext="upper
    M times upper M"><mrow><mi>M</mi> <mo>×</mo> <mi>M</mi></mrow></math> matrix will
    have *M* eigenvalues.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 匹配的2并非巧合：一个<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>×</mo>
    <mi>M</mi></mrow></math>矩阵的特征多项式将有一个<math alttext="lamda Superscript upper M"><msup><mi>λ</mi>
    <mi>M</mi></msup></math>项。这是为什么<math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>×</mo> <mi>M</mi></mrow></math>矩阵会有*M*个特征值的原因。
- en: Tedious Practice Problems
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 繁琐的练习问题
- en: 'At this point in a traditional linear algebra textbook, you would be tasked
    with finding the eigenvalues of dozens of <math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>×</mo> <mn>2</mn></mrow></math> and <math alttext="3 times 3"><mrow><mn>3</mn>
    <mo>×</mo> <mn>3</mn></mrow></math> matrices by hand. I have mixed feelings about
    these kinds of exercises: on the one hand, solving problems by hand really helps
    internalize the mechanics of finding eigenvalues; but on the other hand, I want
    to focus this book on concepts, code, and applications, without getting bogged
    down by tedious arithmetic. If you feel inspired to solve eigenvalue problems
    by hand, then go for it! You can find myriad such problems in traditional textbooks
    or online. But I took the bold (and perhaps controversial) decision to avoid hand-solved
    problems in this book, and instead to have exercises that focus on coding and
    comprehension.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统线性代数教材中的这一点上，你会被要求手动找出数十个<math alttext="2 times 2"><mrow><mn>2</mn> <mo>×</mo>
    <mn>2</mn></mrow></math>和<math alttext="3 times 3"><mrow><mn>3</mn> <mo>×</mo>
    <mn>3</mn></mrow></math>矩阵的特征值。我对这类练习有着复杂的感受：一方面，通过手工解决问题确实有助于内化找特征值的机制；但另一方面，我希望本书侧重于概念、代码和应用，而不要被繁琐的算术问题所困扰。如果你有兴趣手工解决特征值问题，那就去做吧！你可以在传统教材或在线上找到大量这类问题。但我做出了大胆（也许是有争议的）决定，避免在本书中手动解决问题，而是进行侧重于编码和理解的练习。
- en: Finding Eigenvectors
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 寻找特征向量
- en: 'As with eigenvalues, finding eigenvectors is super-duper easy in Python:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 和特征值一样，在 Python 中找到特征向量是非常容易的：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The eigenvectors are in the columns of the matrix `evecs` and are in the same
    order as the eigenvalues (that is, the eigenvector in the first column of matrix
    `evecs` is paired with the first eigenvalue in vector `evals`). I like to use
    the variable names `evals` and `evecs`, because they are short and meaningful.
    You might also see people use variable names `L` and `V` or `D` and `V`. The `L`
    is for <math alttext="normal upper Lamda"><mi>Λ</mi></math> (the capital of <math
    alttext="lamda"><mi>λ</mi></math> ) and the `V` is for <math alttext="bold upper
    V"><mi>𝐕</mi></math> , the matrix in which each column *i* is eigenvector <math
    alttext="bold v Subscript i"><msub><mi>𝐯</mi> <mi>i</mi></msub></math> . The `D`
    is for *diagonal*, because eigenvalues are often stored in a diagonal matrix,
    for reasons I will explain later in this chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量位于矩阵`evecs`的列中，并且与特征值的顺序相同（即，矩阵`evecs`的第一列中的特征向量与向量`evals`中的第一个特征值配对）。我喜欢使用变量名`evals`和`evecs`，因为它们简短而有意义。你可能还会看到人们使用变量名`L`和`V`或`D`和`V`。`L`代表<math
    alttext="normal upper Lamda"><mi>Λ</mi></math>（<math alttext="lamda"><mi>λ</mi></math>的大写形式），`V`代表<math
    alttext="bold upper V"><mi>𝐕</mi></math>，即矩阵，其中每列*i*是特征向量<math alttext="bold v
    Subscript i"><msub><mi>𝐯</mi><mi>i</mi></msub></math> 。`D`代表*对角线*，因为特征值通常存储在对角线矩阵中，我稍后将在本章解释原因。
- en: Eigenvectors in the Columns, Not the Rows!
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征向量在列中，而不是行中！
- en: The most important thing to keep in mind about eigenvectors when coding is that
    they are stored in the *columns of the matrix*, not the rows! Such dimensional-indexing
    errors are easy to make with square matrices (because you might not get Python
    errors), but accidentally using the rows instead of the columns of the eigenvectors
    matrix can have disastrous consequences in applications. When in doubt, remember
    the discussion from [Chapter 2](ch02.xhtml#Chapter_2) that common convention in
    linear algebra is to assume that vectors are in column orientation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 编码时关于特征向量最重要的事情是它们存储在*矩阵的列中*，而不是行中！在使用方阵时很容易犯这种维度索引错误（因为可能不会收到Python错误），但是在应用中意外地使用特征向量矩阵的行而不是列可能会产生灾难性后果。如有疑问，请记住[第二章](ch02.xhtml#Chapter_2)中的讨论，线性代数中的常见约定是假定向量是列向量。
- en: OK, but again, the previous code shows how to get a NumPy function to return
    the eigenvectors of a matrix. You could have learned that from the `np.linalg.eig`
    docstring. The important question is *Where do eigenvectors come from, and how
    do we find them?*
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但再次强调，前面的代码展示了如何使NumPy函数返回矩阵的特征向量。你可以从`np.linalg.eig`的文档字符串中学到这一点。重要的问题是*特征向量来自哪里，我们如何找到它们？*
- en: 'Actually, I’ve already written how to find eigenvectors: find the vector <math
    alttext="bold v"><mi>𝐯</mi></math> that is in the null space of the matrix shifted
    by <math alttext="lamda"><mi>λ</mi></math> . In other words:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我已经写过如何找到特征向量：找到矩阵通过<math alttext="lamda"><mi>λ</mi></math>移位后的零空间向量<math
    alttext="bold v"><mi>𝐯</mi></math> 。换句话说：
- en: <math alttext="bold v Subscript i Baseline element-of upper N left-parenthesis
    bold upper A minus lamda Subscript i Baseline bold upper I right-parenthesis"
    display="block"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>∈</mo> <mi>N</mi>
    <mrow><mo>(</mo> <mi>𝐀</mi> <mo>-</mo> <msub><mi>λ</mi> <mi>i</mi></msub> <mi>𝐈</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline element-of upper N left-parenthesis
    bold upper A minus lamda Subscript i Baseline bold upper I right-parenthesis"
    display="block"><mrow><msub><mi>𝐯</mi> <mi>i</mi></msub> <mo>∈</mo> <mi>N</mi>
    <mrow><mo>(</mo> <mi>𝐀</mi> <mo>-</mo> <msub><mi>λ</mi> <mi>i</mi></msub> <mi>𝐈</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'Let’s see a numerical example. Follwing is a matrix and its eigenvalues:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一个数值例子。以下是一个矩阵及其特征值：
- en: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>⇒</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <mo>=</mo> <mn>3</mn> <mo>,</mo>
    <msub><mi>λ</mi> <mn>2</mn></msub> <mo>=</mo> <mrow><mo>-</mo> <mn>1</mn></mrow></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>⇒</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <mo>=</mo> <mn>3</mn> <mo>,</mo>
    <msub><mi>λ</mi> <mn>2</mn></msub> <mo>=</mo> <mrow><mo>-</mo> <mn>1</mn></mrow></mrow></math>
- en: 'Let’s focus on the first eigenvalue. To reveal its eigenvector, we shift the
    matrix by 3 and find a vector in its null space:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们专注于第一个特征值。为了揭示其特征向量，我们将矩阵移位3，并找到其零空间中的向量：
- en: <math alttext="Start 2 By 2 Matrix 1st Row 1st Column 1 minus 3 2nd Column 2
    2nd Row 1st Column 2 2nd Column 1 minus 3 EndMatrix equals Start 2 By 2 Matrix
    1st Row 1st Column negative 2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative
    2 EndMatrix right double arrow Start 2 By 2 Matrix 1st Row 1st Column negative
    2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative 2 EndMatrix StartBinomialOrMatrix
    1 Choose 1 EndBinomialOrMatrix equals StartBinomialOrMatrix 0 Choose 0 EndBinomialOrMatrix"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>-</mo> <mn>3</mn></mrow></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>-</mo> <mn>3</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>⇒</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 2 By 2 Matrix 1st Row 1st Column 1 minus 3 2nd Column 2
    2nd Row 1st Column 2 2nd Column 1 minus 3 EndMatrix equals Start 2 By 2 Matrix
    1st Row 1st Column negative 2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative
    2 EndMatrix right double arrow Start 2 By 2 Matrix 1st Row 1st Column negative
    2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative 2 EndMatrix StartBinomialOrMatrix
    1 Choose 1 EndBinomialOrMatrix equals StartBinomialOrMatrix 0 Choose 0 EndBinomialOrMatrix"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>-</mo> <mn>3</mn></mrow></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>-</mo> <mn>3</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>⇒</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: This means that [1 1] is an eigenvector of the matrix associated with an eigenvalue
    of 3.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着[1 1]是与特征值为3的矩阵相关联的特征向量。
- en: I found that null space vector just by looking at the matrix. How are the null
    space vectors (that is, the eigenvectors of the matrix) actually identified in
    practice?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 我只是通过查看矩阵找到了零空间向量。在实践中如何识别零空间向量（即矩阵的特征向量）？
- en: Null space vectors can be found by using Gauss-Jordan to solve a system of equations,
    where the coefficients matrix is the <math alttext="lamda"><mi>λ</mi></math> -shifted
    matrix and the constants vector is the zeros vector. That’s a good way to conceptualize
    the solution. In implementation, more stable numerical methods are applied for
    finding eigenvalues and eigenvectors, including QR decomposition and a procedure
    called the power method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用高斯-约旦方法解方程组可以找到零空间向量，其中系数矩阵是<math alttext="lamda"><mi>λ</mi></math>-移位矩阵，常数向量是零向量。这是一种概念化解决方案的好方法。在实施中，会应用更稳定的数值方法来找到特征值和特征向量，包括QR分解和一种称为幂法的过程。
- en: Sign and Scale Indeterminacy of Eigenvectors
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征向量的符号和缩放不确定性
- en: Let me return to the numerical example in the previous section. I wrote that
    [1 1] was an eigenvector of the matrix because that vector is a basis for the
    null space of the matrix shifted by its eigenvalue of 3.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我回到前一节中的数值示例。我写道[1 1]是矩阵的特征向量，因为该向量是矩阵移位后的零空间的基础。
- en: 'Look back at the shifted matrix and ask yourself, is [1 1] the only possible
    basis vector for the null space? Not even close! You could also use [4 4] or [−5.4
    −5.4] or…I think you see where this is going: *any* scaled version of vector [1
    1] is a basis for that null space. In other words, if <math alttext="bold v"><mi>𝐯</mi></math>
    is an eigenvector of a matrix, then so is <math alttext="alpha bold v"><mrow><mi>α</mi>
    <mi>𝐯</mi></mrow></math> for any real-valued <math alttext="alpha"><mi>α</mi></math>
    except zero.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下移位后的矩阵，并问自己，[1 1]是零空间的唯一可能的基向量吗？一点也不！你还可以使用[4 4]或[-5.4 -5.4]或……我想你明白这是什么意思：向量[1
    1]的*任何*缩放版本都是该零空间的基础。换句话说，如果<math alttext="bold v"><mi>𝐯</mi></math>是矩阵的特征向量，那么对于任何实数值<math
    alttext="alpha"><mi>α</mi></math>（除了零），<math alttext="alpha bold v"><mrow><mi>α</mi>
    <mi>𝐯</mi></mrow></math>也是。
- en: Indeed, eigenvectors are important because of their *direction*, not because
    of their *magnitude*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，特征向量之所以重要是因为它们的*方向*，而不是它们的*大小*。
- en: 'The infinity of possible null space basis vectors leads to two questions:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无限多的零空间基向量引发了两个问题：
- en: '*Is there one “best” basis vector?* There isn’t a “best” basis vector per se,
    but it is convenient to have eigenvectors that are unit normalized (a Euclidean
    norm of 1). This is particularly useful for symmetric matrices for reasons that
    will be explained later in this chapter.^([4](ch13.xhtml#idm45733293126624))'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有一个“最佳”的基向量吗？* 实际上没有一个“最佳”的基向量，但对于对称矩阵来说，拥有单位规范化（欧几里得范数为1）的特征向量是方便的，这将在本章后面的内容中解释。^([4](ch13.xhtml#idm45733293126624))'
- en: '*What is the “correct” sign of an eigenvector?* There is none. In fact, you
    can get different eigenvector signs from the same matrix when using different
    versions of NumPy—as well as different software such as MATLAB, Julia, or Mathematica.
    Eigenvector sign indeterminacy is just a feature of life in our universe. In applications
    such as PCA, there are principled ways for assigning a sign, but that’s just common
    convention to facilitate interpretation.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征向量的“正确”符号是什么？* 没有确定的符号。事实上，当使用不同版本的NumPy以及不同的软件如MATLAB、Julia或Mathematica时，可以从同一个矩阵中获得不同的特征向量符号。特征向量符号的不确定性只是我们宇宙生活的一个特征。在诸如PCA之类的应用中，有一些原则性的方法可以分配符号，但这只是促进解释的常见约定。'
- en: Diagonalizing a Square Matrix
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对角化一个方阵
- en: 'The eigenvalue equation that you are now familiar with lists one eigenvalue
    and one eigenvector. This means that an <math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>×</mo> <mi>M</mi></mrow></math> matrix has *M* eigenvalue equations:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您熟悉的特征值方程列出了一个特征值和一个特征向量。这意味着一个<math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>×</mo> <mi>M</mi></mrow></math>矩阵有*M*个特征值方程：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 1 2nd Column
    equals lamda 1 bold v 1 2nd Row 1st Column  ellipsis 3rd Row 1st Column bold upper
    A bold v Subscript upper M 2nd Column equals lamda Subscript upper M Baseline
    bold v Subscript upper M EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <msub><mi>𝐯</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <msub><mi>𝐯</mi> <mi>M</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mi>M</mi></msub> <msub><mi>𝐯</mi>
    <mi>M</mi></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 1 2nd Column
    equals lamda 1 bold v 1 2nd Row 1st Column  ellipsis 3rd Row 1st Column bold upper
    A bold v Subscript upper M 2nd Column equals lamda Subscript upper M Baseline
    bold v Subscript upper M EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <msub><mi>𝐯</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>⋮</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <msub><mi>𝐯</mi> <mi>M</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mi>M</mi></msub> <msub><mi>𝐯</mi>
    <mi>M</mi></msub></mrow></mtd></mtr></mtable></math>
- en: 'There’s nothing really wrong with that series of equations, but it is kind
    of ugly, and ugliness violates one of the principles of linear algebra: make equations
    compact and elegant. Therefore, we transform this series of equations into one
    matrix equation.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 那系列方程组其实没什么错，但有点丑陋，而丑陋违反了线性代数的一个原则：使方程组简洁而优雅。因此，我们将这系列方程转换成一个矩阵方程。
- en: The key insight for writing out the matrix eigenvalue equation is that each
    column of the eigenvectors matrix is scaled by exactly one eigenvalue. We can
    implement this through postmultiplication by a diagonal matrix (as you learned
    in [Chapter 6](ch06.xhtml#Chapter_6)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 写出矩阵特征值方程的关键洞见在于，特征向量矩阵的每一列都被恰好一个特征值所缩放。我们可以通过后乘对角矩阵来实现这一点（正如您在[第6章](ch06.xhtml#Chapter_6)中学到的）。
- en: 'So instead of storing the eigenvalues in a vector, we store the eigenvalues
    in the diagonal of a matrix. The following equation shows the form of diagonalization
    for a <math alttext="3 times 3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math>
    matrix (using @ in place of numerical values in the matrix). In the eigenvectors
    matrix, the first subscript number corresponds to the eigenvector, and the second
    subscript number corresponds to the eigenvector element. For example, <math alttext="v
    12"><msub><mi>v</mi> <mn>12</mn></msub></math> is the second element of the first
    eigenvector:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们不是将特征值存储在向量中，而是将特征值存储在矩阵的对角线上。以下方程显示了对于一个 <math alttext="3 times 3"><mrow><mn>3</mn>
    <mo>×</mo> <mn>3</mn></mrow></math> 矩阵的对角化形式（在矩阵中使用 @ 替换数值）。在特征向量矩阵中，第一个下标数字对应于特征向量，第二个下标数字对应于特征向量元素。例如，<math
    alttext="v 12"><msub><mi>v</mi> <mn>12</mn></msub></math> 是第一个特征向量的第二个元素：
- en: <math alttext="StartLayout 1st Row 1st Column Start 3 By 3 Matrix 1st Row 1st
    Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 2nd Row
    1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 3rd
    Row 1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at
    EndMatrix Start 3 By 3 Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column
    v 31 2nd Row 1st Column v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column
    v 13 2nd Column v 23 3rd Column v 33 EndMatrix 2nd Column equals Start 3 By 3
    Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column v 31 2nd Row 1st Column
    v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column v 13 2nd Column v 23 3rd
    Column v 33 EndMatrix Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column
    0 3rd Column 0 2nd Row 1st Column 0 2nd Column lamda 2 3rd Column 0 3rd Row 1st
    Column 0 2nd Column 0 3rd Column lamda 3 EndMatrix 2nd Row 1st Column Blank 2nd
    Column equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 v 11 2nd Column lamda
    2 v 21 3rd Column lamda 3 v 31 2nd Row 1st Column lamda 1 v 12 2nd Column lamda
    2 v 22 3rd Column lamda 3 v 32 3rd Row 1st Column lamda 1 v 13 2nd Column lamda
    2 v 23 3rd Column lamda 3 v 33 EndMatrix EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>λ</mi>
    <mn>1</mn></msub></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>λ</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><msub><mi>λ</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>31</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>12</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>22</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>32</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>13</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>23</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>33</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column Start 3 By 3 Matrix 1st Row 1st
    Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 2nd Row
    1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 3rd
    Row 1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at
    EndMatrix Start 3 By 3 Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column
    v 31 2nd Row 1st Column v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column
    v 13 2nd Column v 23 3rd Column v 33 EndMatrix 2nd Column equals Start 3 By 3
    Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column v 31 2nd Row 1st Column
    v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column v 13 2nd Column v 23 3rd
    Column v 33 EndMatrix Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column
    0 3rd Column 0 2nd Row 1st Column 0 2nd Column lamda 2 3rd Column 0 3rd Row 1st
    Column 0 2nd Column 0 3rd Column lamda 3 EndMatrix 2nd Row 1st Column Blank 2nd
    Column equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 v 11 2nd Column lamda
    2 v 21 3rd Column lamda 3 v 31 2nd Row 1st Column lamda 1 v 12 2nd Column lamda
    2 v 22 3rd Column lamda 3 v 32 3rd Row 1st Column lamda 1 v 13 2nd Column lamda
    2 v 23 3rd Column lamda 3 v 33 EndMatrix EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>λ</mi>
    <mn>1</mn></msub></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>λ</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><msub><mi>λ</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>31</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>12</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>22</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>32</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>13</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>23</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>λ</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>33</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: Please take a moment to confirm that each eigenvalue scales all elements of
    its corresponding eigenvector, and not any other eigenvectors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 请花点时间确认每个特征值如何缩放其对应的特征向量的所有元素，而不是其他任何特征向量。
- en: 'More generally, the matrix eigenvalue equation—a.k.a. the diagonalization of
    a square matrix—is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 更一般地说，矩阵特征值方程（即对方阵的对角化）是：
- en: <math alttext="bold upper A bold upper V equals bold upper V bold upper Lamda"
    display="block"><mrow><mi mathvariant="bold">A</mi> <mi mathvariant="bold">V</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi></mrow></math>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold upper V equals bold upper V bold upper Lamda"
    display="block"><mrow><mi mathvariant="bold">A</mi> <mi mathvariant="bold">V</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi></mrow></math>
- en: 'NumPy’s `eig` function returns eigenvectors in a matrix and eigenvalues in
    a vector. This means that diagonalizing a matrix in NumPy requires a bit of extra
    code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy 的 `eig` 函数返回一个矩阵中的特征向量和一个向量中的特征值。这意味着在 NumPy 中对矩阵进行对角化需要额外的代码：
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By the way, it’s often interesting and insightful in mathematics to rearrange
    equations by solving for different variables. Consider the following list of equivalent
    declarations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一句，在数学中通过解不同变量来重新排列方程通常很有趣和深刻。考虑以下等价声明列表：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper V 2nd
    Column equals bold upper V bold upper Lamda 2nd Row 1st Column bold upper A 2nd
    Column equals bold upper V bold upper Lamda bold upper V Superscript negative
    1 Baseline 3rd Row 1st Column bold upper Lamda 2nd Column equals bold upper V
    Superscript negative 1 Baseline bold upper A bold upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">A</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi> <msup><mi mathvariant="bold">V</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper V 2nd
    Column equals bold upper V bold upper Lamda 2nd Row 1st Column bold upper A 2nd
    Column equals bold upper V bold upper Lamda bold upper V Superscript negative
    1 Baseline 3rd Row 1st Column bold upper Lamda 2nd Column equals bold upper V
    Superscript negative 1 Baseline bold upper A bold upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">A</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Λ</mi> <msup><mi mathvariant="bold">V</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Λ</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd></mtr></mtable></math>
- en: 'The second equation shows that matrix <math alttext="bold upper A"><mi>𝐀</mi></math>
    becomes diagonal inside the space of <math alttext="bold upper V"><mi>𝐕</mi></math>
    (that is, <math alttext="bold upper V"><mi>𝐕</mi></math> moves us into the “diagonal
    space,” and then <math alttext="bold upper V Superscript negative 1"><msup><mi>𝐕</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> gets us back out of the diagonal
    space). This can be interpreted in the context of basis vectors: the matrix <math
    alttext="bold upper A"><mi>𝐀</mi></math> is dense in the standard basis, but then
    we apply a set of transformations ( <math alttext="bold upper V"><mi>𝐕</mi></math>
    ) to rotate the matrix into a new set of basis vectors (the eigenvectors) in which
    the information is sparse and represented by a diagonal matrix. (At the end of
    the equation, we need to get back into the standard basis space, hence the <math
    alttext="bold upper V Superscript negative 1"><msup><mi>𝐕</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    .)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个方程表明矩阵 <math alttext="bold upper A"><mi>𝐀</mi></math> 在 <math alttext="bold
    upper V"><mi>𝐕</mi></math> 的空间内变为对角化（也就是说，<math alttext="bold upper V"><mi>𝐕</mi></math>
    将我们移入“对角空间”，然后 <math alttext="bold upper V Superscript negative 1"><msup><mi>𝐕</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> 将我们带回标准空间）。这可以在基向量的背景下解释：矩阵 <math
    alttext="bold upper A"><mi>𝐀</mi></math> 在标准基中是密集的，但然后我们应用一组变换（ <math alttext="bold
    upper V"><mi>𝐕</mi></math> ）将矩阵旋转到新的基向量集合（特征向量），在这些基向量中信息是稀疏的，并由对角矩阵表示。（在方程末尾，我们需要回到标准基空间，因此需要
    <math alttext="bold upper V Superscript negative 1"><msup><mi>𝐕</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    。）
- en: The Special Awesomeness of Symmetric Matrices
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 对称矩阵的特殊优越性
- en: You already know from earlier chapters that symmetric matrices have special
    properties that make them great to work with. Now you are ready to learn two more
    special properties that relate to eigendecomposition.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经从前几章了解到，对称矩阵具有使它们易于处理的特殊属性。现在，你准备学习与特征分解相关的另外两个特殊属性。
- en: Orthogonal Eigenvectors
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正交特征向量
- en: 'Symmetric matrices have orthogonal eigenvectors. That means that all eigenvectors
    of a symmetric matrix are pair-wise orthogonal. Let me start with an example,
    then I’ll discuss the implications of eigenvector orthogonality, and finally I’ll
    show the proof:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对称矩阵具有正交特征向量。这意味着对称矩阵的所有特征向量都是成对正交的。让我从一个例子开始，然后我将讨论特征向量正交的含义，最后我将展示证明：
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The three dot products are all zero (within computer rounding errors on the
    order of 10^(−16). (Notice that I’ve created symmetric matrices as a random matrix
    times its transpose.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 三个点积都为零（在计算机舍入误差为 10^(−16) 的数量级内）。 （注意，我已经创建了对称矩阵作为一个随机矩阵乘以其转置。）
- en: The orthogonal eigenvector property means that the dot product between any pair
    of eigenvectors is zero, while the dot product of an eigenvector with itself is
    nonzero (because we do not consider the zeros vector to be an eigenvector). This
    can be written as <math alttext="bold upper V Superscript upper T Baseline bold
    upper V equals bold upper D"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐕</mi>
    <mo>=</mo> <mi>𝐃</mi></mrow></math> , where <math alttext="bold upper D"><mi>𝐃</mi></math>
    is a diagonal matrix with the diagonals containing the norms of the eigenvectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 正交特征向量属性意味着任意一对特征向量之间的点积为零，而特征向量与自身的点积不为零（因为我们不认为零向量是特征向量）。这可以写成 <math alttext="bold
    upper V Superscript upper T Baseline bold upper V equals bold upper D"><mrow><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup> <mi>𝐕</mi> <mo>=</mo> <mi>𝐃</mi></mrow></math>，其中 <math
    alttext="bold upper D"><mi>𝐃</mi></math> 是一个对角矩阵，对角线包含特征向量的范数。
- en: 'But we can do even better than just a diagonal matrix: recall that eigenvectors
    are important because of their *direction*, not because of their *magnitude*.
    So an eigenvector can have any magnitude we want (except, obviously, for a magnitude
    of zero).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们可以比对角矩阵做得更好：记住特征向量之所以重要是因为它们的*方向*，而不是它们的*大小*。因此，一个特征向量可以有任何我们想要的大小（显然不包括大小为零的情况）。
- en: 'Let’s scale all eigenvectors so they have unit length. Question for you: if
    all eigenvectors are orthogonal and have unit length, what happens when we multiply
    the eigenvectors matrix by its transpose?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将所有特征向量缩放到单位长度。对你的问题：如果所有特征向量都是正交的，并且具有单位长度，那么当我们将特征向量矩阵乘以它的转置时会发生什么？
- en: 'Of course you know the answer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当然你知道答案：
- en: <math alttext="bold upper V Superscript upper T Baseline bold upper V equals
    bold upper I" display="block"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐕</mi>
    <mo>=</mo> <mi>𝐈</mi></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper V Superscript upper T Baseline bold upper V equals
    bold upper I" display="block"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐕</mi>
    <mo>=</mo> <mi>𝐈</mi></mrow></math>
- en: In other words, the eigenvectors matrix of a symmetric matrix is an orthogonal
    matrix! This has multiple implications for data science, including that the eigenvectors
    are super easy to invert (because you simply transpose them). There are other
    implications of orthogonal eigenvectors for applications such as principal components
    analysis, which I will discuss in [Chapter 15](ch15.xhtml#Chapter_15).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，对称矩阵的特征向量矩阵是一个正交矩阵！这对数据科学有多重影响，包括特征向量非常容易求逆（因为你只需转置它们）。正交特征向量还有其他应用，比如主成分分析，我将在[第15章](ch15.xhtml#Chapter_15)中讨论。
- en: I wrote in [Chapter 1](ch01.xhtml#Chapter_1) that there are relatively few proofs
    in this book. But orthogonal eigenvectors of symmetric matrices is such an important
    concept that you really need to see this claim proven.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我在[第1章](ch01.xhtml#Chapter_1)中写道，本书中的证明相对较少。但对称矩阵的正交特征向量是如此重要的概念，你真的需要看到这个主张的证明。
- en: 'The goal of this proof is to show that the dot product between any pair of
    eigenvectors is zero. We start from two assumptions: (1) matrix <math alttext="bold
    upper A"><mi>𝐀</mi></math> is symmetric, and (2) <math alttext="lamda 1"><msub><mi>λ</mi>
    <mn>1</mn></msub></math> and <math alttext="lamda 2"><msub><mi>λ</mi> <mn>2</mn></msub></math>
    are distinct eigenvalues of <math alttext="bold upper A"><mi>𝐀</mi></math> (*distinct*
    meaning they cannot equal each other), with <math alttext="bold v 1"><msub><mi>𝐯</mi>
    <mn>1</mn></msub></math> and <math alttext="bold v 2"><msub><mi>𝐯</mi> <mn>2</mn></msub></math>
    as their corresponding eigenvectors. Try to follow each equality step from left
    to right of [Equation 13-3](#eigen-ortho).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这个证明的目标是展示任意一对特征向量之间的点积为零。我们从两个假设开始：（1）矩阵 <math alttext="bold upper A"><mi>𝐀</mi></math>
    是对称的，（2）<math alttext="lamda 1"><msub><mi>λ</mi> <mn>1</mn></msub></math> 和 <math
    alttext="lamda 2"><msub><mi>λ</mi> <mn>2</mn></msub></math> 是矩阵 <math alttext="bold
    upper A"><mi>𝐀</mi></math> 的不同特征值（*不同*意味着它们不相等），其对应的特征向量为 <math alttext="bold
    v 1"><msub><mi>𝐯</mi> <mn>1</mn></msub></math> 和 <math alttext="bold v 2"><msub><mi>𝐯</mi>
    <mn>2</mn></msub></math>。尝试按照[方程式13-3](#eigen-ortho)从左到右的每个等式步骤进行跟踪。
- en: Equation 13-3\. Proof of eigenvector orthogonality for symmetric matrices
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式13-3\. 对称矩阵的特征向量正交性证明
- en: <math alttext="lamda 1 bold v 1 Superscript upper T Baseline bold v 2 equals
    left-parenthesis bold upper A bold v 1 right-parenthesis Superscript upper T Baseline
    bold v 2 equals bold v 1 Superscript upper T Baseline bold upper A Superscript
    upper T Baseline bold v 2 equals bold v 1 Superscript upper T Baseline lamda 2
    bold v 2 equals lamda 2 bold v 1 Superscript upper T Baseline bold v 2" display="block"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msup><mrow><mo>(</mo><mi>𝐀</mi><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup> <msub><mi>𝐯</mi> <mn>2</mn></msub>
    <mo>=</mo> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>=</mo> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>λ</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda 1 bold v 1 Superscript upper T Baseline bold v 2 equals
    left-parenthesis bold upper A bold v 1 right-parenthesis Superscript upper T Baseline
    bold v 2 equals bold v 1 Superscript upper T Baseline bold upper A Superscript
    upper T Baseline bold v 2 equals bold v 1 Superscript upper T Baseline lamda 2
    bold v 2 equals lamda 2 bold v 1 Superscript upper T Baseline bold v 2" display="block"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msup><mrow><mo>(</mo><mi>𝐀</mi><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup> <msub><mi>𝐯</mi> <mn>2</mn></msub>
    <mo>=</mo> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>=</mo> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>λ</mi> <mn>2</mn></msub> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math>
- en: The terms in the middle are just transformations; pay attention to the first
    and last terms. They are rewritten in [Equation 13-4](#eigen-ortho-2), and then
    subtracted to set to zero.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 中间的项只是变换；注意第一项和最后一项。它们在[方程式13-4](#eigen-ortho-2)中被重写，然后相减以设为零。
- en: Equation 13-4\. Continuing the eigenvector orthogonality proof…
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式13-4\. 继续特征向量正交性证明…
- en: <math alttext="StartLayout 1st Row 1st Column lamda 1 bold v 1 Superscript upper
    T Baseline bold v 2 2nd Column equals lamda 2 bold v 1 Superscript upper T Baseline
    bold v 2 2nd Row 1st Column lamda 1 bold v 1 Superscript upper T Baseline bold
    v 2 minus lamda 2 bold v 1 Superscript upper T Baseline bold v 2 2nd Column equals
    0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi>
    <mn>2</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column lamda 1 bold v 1 Superscript upper
    T Baseline bold v 2 2nd Column equals lamda 2 bold v 1 Superscript upper T Baseline
    bold v 2 2nd Row 1st Column lamda 1 bold v 1 Superscript upper T Baseline bold
    v 2 minus lamda 2 bold v 1 Superscript upper T Baseline bold v 2 2nd Column equals
    0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi>
    <mn>2</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi>λ</mi>
    <mn>1</mn></msub> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: Both terms contain the dot product <math alttext="bold v 1 Superscript upper
    T Baseline bold v 2"><mrow><msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup>
    <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math> , which can be factored out.
    This brings us to the final part of the proof, which is shown in [Equation 13-5](#eigen-ortho-3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 两个术语都包含点积 <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math>
    ，这可以被分解出来。这将我们带到证明的最后部分，显示在[方程式 13-5](#eigen-ortho-3)中。
- en: Equation 13-5\. Eigenvector orthogonality proof, part 3
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 13-5。特征向量正交性证明，第 3 部分
- en: <math alttext="left-parenthesis lamda 1 minus lamda 2 right-parenthesis bold
    v 1 Superscript upper T Baseline bold v 2 equals 0" display="block"><mrow><mrow><mo>(</mo>
    <msub><mi>λ</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis lamda 1 minus lamda 2 right-parenthesis bold
    v 1 Superscript upper T Baseline bold v 2 equals 0" display="block"><mrow><mrow><mo>(</mo>
    <msub><mi>λ</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mi>𝐯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math>
- en: This final equation says that two quantities multiply to produce 0, which means
    that one or both of those quantities must be zero. <math alttext="left-parenthesis
    lamda 1 minus lamda 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>λ</mi> <mn>1</mn></msub>
    <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> cannot
    equal zero because we began from the assumption that they are distinct. Therefore,
    <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math>
    must equal zero, which means that the two eigenvectors are orthogonal. Go back
    through the equations to convince yourself that this proof fails for nonsymmetric
    matrices, when <math alttext="bold upper A Superscript upper T Baseline not-equals
    bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mo>≠</mo> <mi>𝐀</mi></mrow></math>
    . Thus, the eigenvectors of a nonsymmetric matrix are not constrained to be orthogonal
    (they will be linearly independent for all distinct eigenvalues, but I will omit
    that discussion and proof).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 最终方程表明两个量相乘得到0，这意味着这两个量中的一个或者两个必须为零。 <math alttext="left-parenthesis lamda 1
    minus lamda 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>λ</mi> <mn>1</mn></msub>
    <mo>-</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> 不能等于零，因为我们假设它们是不同的。因此，
    <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>𝐯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></math>
    必须等于零，这意味着两个特征向量是正交的。回顾方程式以确信这个证明对于非对称矩阵是不成立的，当 <math alttext="bold upper A Superscript
    upper T Baseline not-equals bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mo>≠</mo> <mi>𝐀</mi></mrow></math> 。因此，非对称矩阵的特征向量不受正交约束（它们对于所有不同的特征值是线性独立的，但我将忽略那部分讨论和证明）。
- en: Real-Valued Eigenvalues
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实数特征值
- en: A second special property of symmetric matrices is that they have real-valued
    eigenvalues (and therefore real-valued eigenvectors).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 对称矩阵的第二个特殊性质是它们具有实数特征值（因此具有实数特征向量）。
- en: 'Let me start by showing that matrices—even with all real-valued entries—can
    have complex-valued eigenvalues:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 让我首先展示矩阵——即使所有条目都是实数——也可以具有复数特征值：
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: (Be careful with interpreting that NumPy array; it is not a <math alttext="3
    times 2"><mrow><mn>3</mn> <mo>×</mo> <mn>2</mn></mrow></math> *matrix*; it is
    a <math alttext="3 times 1"><mrow><mn>3</mn> <mo>×</mo> <mn>1</mn></mrow></math>
    *column vector* that contains complex numbers. Note the `j` and the absence of
    a comma between numbers.)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: （在解释那个 NumPy 数组时要小心；它不是一个 <math alttext="3 times 2"><mrow><mn>3</mn> <mo>×</mo>
    <mn>2</mn></mrow></math> *矩阵*；它是一个 <math alttext="3 times 1"><mrow><mn>3</mn>
    <mo>×</mo> <mn>1</mn></mrow></math> *列向量*，包含复数。注意 `j` 和数字之间没有逗号。）
- en: The <math alttext="3 times 3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math>
    matrix <math alttext="bold upper A"><mi>𝐀</mi></math> has two complex eigenvalues
    and one real-valued eigenvalue. The eigenvectors coupled to the complex-valued
    eigenvalues will themselves be complex-valued. There is nothing special about
    that particular matrix; I literally generated it from random integers between
    −3 and +3\. Interestingly, complex-valued solutions come in conjugate pairs. That
    means that if there is a <math alttext="lamda Subscript j"><msub><mi>λ</mi> <mi>j</mi></msub></math>
    = a + *i*b, then there is a <math alttext="lamda Subscript k"><msub><mi>λ</mi>
    <mi>k</mi></msub></math> = a − *i*b. Their corresponding eigenvectors are also
    complex conjugate pairs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 矩阵 <math alttext="3 times 3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math>
    <math alttext="bold upper A"><mi>𝐀</mi></math> 具有两个复数特征值和一个实数特征值。与复数特征值相关联的特征向量本身将是复数。那个特定矩阵没有什么特别之处；我从介于
    -3 和 +3 之间的随机整数生成它。有趣的是，复数解呈共轭对。这意味着如果有一个 <math alttext="lamda Subscript j"><msub><mi>λ</mi>
    <mi>j</mi></msub></math> = a + *i*b，那么还会有一个 <math alttext="lamda Subscript k"><msub><mi>λ</mi>
    <mi>k</mi></msub></math> = a − *i*b。它们相应的特征向量也是复共轭对。
- en: I don’t want to go into detail about complex-valued solutions, except to show
    you that complex solutions to eigendecomposition are straightforward.^([5](ch13.xhtml#idm45733292508688))
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我不想详细讨论复值解决方案，除了向您展示复值解决方案对特征分解是直观的。
- en: 'Symmetric matrices are guaranteed to have real-valued eigenvalues, and therefore
    also real-valued eigenvectors. Let me start by modifying the previous example
    to make the matrix symmetric:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 对称矩阵保证具有实值特征值，因此也有实值特征向量。让我从修改前面的例子开始，使矩阵对称化：
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is just one specific example; maybe we got lucky here? I recommend taking
    a moment to explore this yourself in the online code; you can create random symmetric
    matrices (by creaing a random matrix and eigendecomposing <math alttext="bold
    upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> ) of any size to confirm that the eigenvalues are real-valued.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是一个具体的例子；也许我们在这里运气好了？我建议你花点时间在在线代码中自行探索一下；你可以创建随机对称矩阵（通过创建一个随机矩阵并特征分解 <math
    alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math> ）来确认特征值是实数。
- en: Guaranteed real-valued eigenvalues from symmetric matrices is fortunate because
    complex numbers are often confusing to work with. Lots of matrices used in data
    science are symmetric, and so if you see complex eigenvalues in your data science
    applications, it’s possible that there is a problem with the code or with the
    data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 对称矩阵保证具有实值特征值是幸运的，因为复数常常让人感到困惑。数据科学中使用的许多矩阵都是对称的，因此如果在数据科学应用中看到复数特征值，可能存在代码或数据问题。
- en: Leveraging Symmetry
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 利用对称性
- en: If you know that you are working with a symmetric matrix, you can use `np.linalg.eigh`
    instead of `np.linalg.eig` (or SciPy’s `eigh` instead of `eig`). The `h` is for
    “Hermitian,” which is the complex version of a symmetric matrix. `eigh` can be
    faster and more numerically stable than `eig`, but works only on symmetric matrices.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道你在处理对称矩阵，可以使用`np.linalg.eigh`代替`np.linalg.eig`（或者SciPy的`eigh`代替`eig`）。`h`代表“Hermitian”，这是对称矩阵的复数版本。`eigh`可能比`eig`更快且数值上更稳定，但仅适用于对称矩阵。
- en: Eigendecomposition of Singular Matrices
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 奇异矩阵的特征分解
- en: I included this section here because I find that students often get the idea
    that singular matrices cannot be eigendecomposed, or that the eigenvectors of
    a singular matrix must be unusual somehow.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这里包含了这一节，因为我发现学生们经常会认为奇异矩阵不能进行特征分解，或者奇异矩阵的特征向量必须在某种程度上不寻常。
- en: 'That idea is completely wrong. Eigendecomposition of singular matrices is perfectly
    fine. Here is a quick example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 那个观点完全错误。奇异矩阵的特征分解完全没问题。这里是一个快速的例子：
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The rank, eigenvalues, and eigenvectors of this matrix are printed here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这个矩阵的秩、特征值和特征向量在这里被打印出来：
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This rank-2 matrix has one zero-valued eigenvalue with a nonzeros eigenvector.
    You can use the online code to explore the eigendecomposition of other reduced-rank
    random matrices.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这个二阶矩阵有一个零值特征值，对应一个非零特征向量。你可以使用在线代码来探索其他降秩随机矩阵的特征分解。
- en: There is one special property of the eigendecomposition of singular matrices,
    which is that at least one eigenvalue is guaranteed to be zero. That doesn’t mean
    that the number of nonzero eigenvalues equals the rank of the matrix—that’s true
    for singular values (the scalar values from the SVD) but not for eigenvalues.
    But if the matrix is singular, then at least one eigenvalue equals zero.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异矩阵的特征分解有一个特殊的性质，即至少有一个特征值保证为零。这并不意味着非零特征值的数量等于矩阵的秩——这对奇异值（来自奇异值分解的标量值）成立，但对特征值不成立。但如果矩阵是奇异的，那么至少有一个特征值等于零。
- en: 'The converse is also true: every full-rank matrix has zero zero-valued eigenvalues.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 反之亦然：每个满秩矩阵都有零个零值特征值。
- en: 'One explanation for why this happens is that a singular matrix already has
    a nontrivial null space, which means <math alttext="lamda equals 0"><mrow><mi>λ</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> provides a nontrivial solution to the equation
    <math alttext="left-parenthesis bold upper A minus lamda bold upper I right-parenthesis
    bold v equals bold 0"><mrow><mo>(</mo> <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi>
    <mo>)</mo> <mi>𝐯</mi> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> .
    You can see this in the previous example matrix: the eigenvector associated with
    <math alttext="lamda equals 0"><mrow><mi>λ</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    is the normalized vector [1 −2 1], which is the linear weighted combination of
    the columns (or rows) that produces the zeros vector.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么会发生这种情况的一个解释是，奇异矩阵已经有一个非平凡的零空间，这意味着<math alttext="lamda equals 0"><mrow><mi>λ</mi>
    <mo>=</mo> <mn>0</mn></mrow></math>为方程<math alttext="left-parenthesis bold upper
    A minus lamda bold upper I right-parenthesis bold v equals bold 0"><mrow><mo>(</mo>
    <mi>𝐀</mi> <mo>-</mo> <mi>λ</mi> <mi>𝐈</mi> <mo>)</mo> <mi>𝐯</mi> <mo>=</mo> <mn
    mathvariant="bold">0</mn></mrow></math>提供了一个非平凡解。你可以在前面的例子矩阵中看到这一点：与<math alttext="lamda
    equals 0"><mrow><mi>λ</mi> <mo>=</mo> <mn>0</mn></mrow></math>相关联的特征向量是标准化向量[1
    −2 1]，它是产生零向量的列（或行）的线性加权组合。
- en: The main take-homes of this section are (1) eigendecomposition is valid for
    reduced-rank matrices, and (2) the presence of at least one zero-valued eigenvalue
    indicates a reduced-rank matrix.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的主要要点是：（1）特征分解对降秩矩阵有效，（2）至少有一个零特征值表明矩阵是降秩的。
- en: Quadratic Form, Definiteness, and Eigenvalues
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 矩阵的二次型、正定性和特征值
- en: 'Let’s face it: *quadratic form* and *definiteness* are intimidating terms.
    But don’t worry—they are both straightforward concepts that provide a gateway
    to advanced linear algebra and applications such as principal components analysis
    and Monte Carlo simulations. And better still: integrating Python code into your
    learning will give you a huge advantage over learning about these concepts compared
    to traditional linear algebra textbooks.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们面对现实：*二次型*和*正定性*是令人生畏的术语。但别担心——它们都是直观的概念，为高级线性代数和应用提供了一个通向主成分分析和蒙特卡洛模拟等领域的大门。更重要的是：将Python代码整合到学习中，相比传统线性代数教科书，将为你带来巨大的优势。
- en: The Quadratic Form of a Matrix
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 矩阵的二次型
- en: 'Consider the following expression:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下表达式：
- en: <math alttext="bold w Superscript upper T Baseline bold upper A bold w equals
    alpha" display="block"><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐀</mi>
    <mi>𝐰</mi> <mo>=</mo> <mi>α</mi></mrow></math>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold w Superscript upper T Baseline bold upper A bold w equals
    alpha" display="block"><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup> <mi>𝐀</mi>
    <mi>𝐰</mi> <mo>=</mo> <mi>α</mi></mrow></math>
- en: In other words, we pre- and postmultiply a square matrix by the same vector
    <math alttext="bold w"><mi>𝐰</mi></math> and get a scalar. (Notice that this multiplication
    is valid only for square matrices.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们通过相同的向量<math alttext="bold w"><mi>𝐰</mi></math>对一个方阵进行前后乘法运算，得到一个标量。（请注意，这种乘法仅适用于方阵。）
- en: This is called the *quadratic form* on matrix <math alttext="bold upper A"><mi>𝐀</mi></math>
    .
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这被称为在矩阵<math alttext="bold upper A"><mi>𝐀</mi></math>上的*二次型*。
- en: 'Which matrix and which vector do we use? The idea of the quadratic form is
    to use one specific matrix and the set of all possible vectors (of appropriate
    size). The important question concerns the signs of <math alttext="alpha"><mi>α</mi></math>
    for all possible vectors. Let’s see an example:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用哪个矩阵和哪个向量？二次型的概念是使用一个特定的矩阵和所有可能的向量（大小合适）。重要的问题涉及所有可能向量的<math alttext="alpha"><mi>α</mi></math>的符号。让我们看一个例子：
- en: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column 2 2nd Column 4 2nd Row 1st Column 0 2nd
    Column 3 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix equals
    2 x squared plus left-parenthesis 0 plus 4 right-parenthesis x y plus 3 y squared"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr> <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mn>2</mn> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>+</mo> <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo>
    <mn>3</mn> <msup><mi>y</mi> <mn>2</mn></msup></mrow></math>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column 2 2nd Column 4 2nd Row 1st Column 0 2nd
    Column 3 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix equals
    2 x squared plus left-parenthesis 0 plus 4 right-parenthesis x y plus 3 y squared"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr> <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mn>2</mn> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>+</mo> <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo>
    <mn>3</mn> <msup><mi>y</mi> <mn>2</mn></msup></mrow></math>
- en: For this particular matrix, there is no possible combination of *x* and *y*
    that can give a negative answer, because the squared terms (2*x*² and 3*y*²) will
    always overpower the cross-term (4*xy*) even when *x* or *y* is negative. Furthermore,
    <math alttext="alpha"><mi>α</mi></math> can be nonpositive only when *x* = *y*
    = 0.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个特定的矩阵，不存在*x*和*y*的任何组合可以给出负答案，因为平方项（2*x*²和3*y*²）总会压倒交叉项（4*xy*），即使*x*或*y*为负。此外，<math
    alttext="alpha"><mi>α</mi></math>只有在*x* = *y* = 0时才可能为非正。
- en: 'That is not a trivial result of the quadratic form. For example, the following
    matrix can have a positive or negative <math alttext="alpha"><mi>α</mi></math>
    depending on the values of *x* and *y*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是二次型的一个平凡结果。例如，下面的矩阵可以根据*x*和*y*的值得到一个正或负的<math alttext="alpha"><mi>α</mi></math>：
- en: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column negative 9 2nd Column 4 2nd Row 1st Column
    3 2nd Column 9 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix
    equals minus 9 x squared plus left-parenthesis 3 plus 4 right-parenthesis x y
    plus 9 y squared" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>9</mn></mrow></mtd> <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>9</mn></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo> <mn>9</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo> <mn>9</mn> <msup><mi>y</mi>
    <mn>2</mn></msup></mrow></math>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column negative 9 2nd Column 4 2nd Row 1st Column
    3 2nd Column 9 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix
    equals minus 9 x squared plus left-parenthesis 3 plus 4 right-parenthesis x y
    plus 9 y squared" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>9</mn></mrow></mtd> <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>9</mn></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo> <mn>9</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo> <mn>9</mn> <msup><mi>y</mi>
    <mn>2</mn></msup></mrow></math>
- en: You can confirm that setting [*x* *y*] to [−1 1] gives a negative quadratic
    form result, while [−1 −1] gives a positive result.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以确认将[*x* *y*]设置为[−1 1]会得到一个负二次型的结果，而[−1 −1]则会得到一个正结果。
- en: 'How can you possibly know whether the quadratic form will produce a positive
    (or negative, or zero-valued) scalar for *all* possible vectors? The key comes
    from considering that a full-rank eigenvectors matrix spans all of <math alttext="double-struck
    upper R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math> , and therefore
    that every vector in <math alttext="double-struck upper R Superscript upper M"><msup><mi>ℝ</mi>
    <mi>M</mi></msup></math> can be expressed as some linear weighted combination
    of the eigenvectors.^([6](ch13.xhtml#idm45733292090752)) Then we start from the
    eigenvalue equation and left-multiply by an eigenvector to return to the quadratic
    form:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你怎么可能知道二次形式对*所有*可能的向量会产生正（或负，或零值）的标量？关键在于考虑到一个满秩特征向量矩阵涵盖了<math alttext="double-struck
    upper R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math>的所有部分，因此<math
    alttext="double-struck upper R Superscript upper M"><msup><mi>ℝ</mi> <mi>M</mi></msup></math>中的每个向量都可以表示为特征向量的某些线性加权组合。^([6](ch13.xhtml#idm45733292090752))
    然后，我们从特征值方程开始，并通过一个特征向量左乘来返回到二次形式：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold v Superscript upper T Baseline bold
    upper A bold v 2nd Column equals lamda bold v Superscript upper T Baseline bold
    v 3rd Row 1st Column bold v Superscript upper T Baseline bold upper A bold v 2nd
    Column equals lamda parallel-to bold v parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup> <mi>𝐀</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>λ</mi><mo>∥</mo><mi>𝐯</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold v Superscript upper T Baseline bold
    upper A bold v 2nd Column equals lamda bold v Superscript upper T Baseline bold
    v 3rd Row 1st Column bold v Superscript upper T Baseline bold upper A bold v 2nd
    Column equals lamda parallel-to bold v parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup> <mi>𝐀</mi>
    <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>λ</mi> <msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>λ</mi><mo>∥</mo><mi>𝐯</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: The final equation is key. Note that <math alttext="parallel-to bold v Superscript
    upper T Baseline bold v parallel-to"><mrow><mrow><mo>∥</mo></mrow> <msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mrow><mi>𝐯</mi> <mo>∥</mo></mrow></mrow></math> is strictly
    positive (vector magnitudes cannot be negative, and we ignore the zeros vector),
    which means that the sign of the right-hand side of the equation is determined
    entirely by the eigenvalue <math alttext="lamda"><mi>λ</mi></math> .
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 最终的方程是关键。注意<math alttext="parallel-to bold v Superscript upper T Baseline bold
    v parallel-to"><mrow><mrow><mo>∥</mo></mrow> <msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mrow><mi>𝐯</mi> <mo>∥</mo></mrow></mrow></math>严格为正（向量大小不可能为负，并忽略零向量），这意味着方程右侧的符号完全由特征值<math
    alttext="lamda"><mi>λ</mi></math>决定。
- en: 'That equation uses only one eigenvalue and its eigenvector, but we need to
    know about any possible vector. The insight is to consider that if the equation
    is valid for each eigenvector-eigenvalue pair, then it is valid for any combination
    of eigenvector-eigenvalue pairs. For example:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 那个方程仅使用一个特征值及其特征向量，但我们需要了解任何可能的向量。关键在于考虑到如果方程对每个特征向量-特征值对都有效，则它对任何特征向量-特征值对的任何组合也有效。例如：
- en: <math alttext="StartLayout 1st Row 1st Column bold v 1 Superscript upper T Baseline
    bold upper A bold v 1 2nd Column equals lamda 1 parallel-to bold v 1 parallel-to
    2nd Row 1st Column bold v 2 Superscript upper T Baseline bold upper A bold v 2
    2nd Column equals lamda 2 parallel-to bold v 2 parallel-to 3rd Row 1st Column
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis Superscript upper T
    Baseline bold upper A left-parenthesis bold v 1 plus bold v 2 right-parenthesis
    2nd Column equals left-parenthesis lamda 1 plus lamda 2 right-parenthesis parallel-to
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis parallel-to 4th Row
    1st Column bold u Superscript upper T Baseline bold upper A bold u 2nd Column
    equals zeta parallel-to bold u parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msubsup><mi>𝐯</mi> <mn>1</mn>
    <mtext>T</mtext></msubsup> <mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <msup><mrow><mo>∥</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msubsup><mi>𝐯</mi> <mn>2</mn> <mtext>T</mtext></msubsup>
    <mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>λ</mi> <mn>2</mn></msub> <msup><mrow><mo>∥</mo><msub><mi>𝐯</mi> <mn>2</mn></msub>
    <mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mrow><mo>(</mo> <msub><mi>𝐯</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msub><mi>λ</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msup><mrow><mo>∥</mo><mrow><mo>(</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐮</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐮</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>ζ</mi><mo>∥</mo><mi>𝐮</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold v 1 Superscript upper T Baseline
    bold upper A bold v 1 2nd Column equals lamda 1 parallel-to bold v 1 parallel-to
    2nd Row 1st Column bold v 2 Superscript upper T Baseline bold upper A bold v 2
    2nd Column equals lamda 2 parallel-to bold v 2 parallel-to 3rd Row 1st Column
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis Superscript upper T
    Baseline bold upper A left-parenthesis bold v 1 plus bold v 2 right-parenthesis
    2nd Column equals left-parenthesis lamda 1 plus lamda 2 right-parenthesis parallel-to
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis parallel-to 4th Row
    1st Column bold u Superscript upper T Baseline bold upper A bold u 2nd Column
    equals zeta parallel-to bold u parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msubsup><mi>𝐯</mi> <mn>1</mn>
    <mtext>T</mtext></msubsup> <mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>λ</mi> <mn>1</mn></msub> <msup><mrow><mo>∥</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msubsup><mi>𝐯</mi> <mn>2</mn> <mtext>T</mtext></msubsup>
    <mi>𝐀</mi> <msub><mi>𝐯</mi> <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>λ</mi> <mn>2</mn></msub> <msup><mrow><mo>∥</mo><msub><mi>𝐯</mi> <mn>2</mn></msub>
    <mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mrow><mo>(</mo> <msub><mi>𝐯</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msub><mi>λ</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>λ</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msup><mrow><mo>∥</mo><mrow><mo>(</mo><msub><mi>𝐯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>𝐯</mi> <mn>2</mn></msub> <mo>)</mo></mrow><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>𝐮</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐮</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>ζ</mi><mo>∥</mo><mi>𝐮</mi><mo>∥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: In other words, we can set any vector <math alttext="bold u"><mi>𝐮</mi></math>
    to be some linear combination of eigenvectors, and some scalar <math alttext="zeta"><mi>ζ</mi></math>
    to be that same linear combination of eigenvalues. Anyway, it doesn’t change the
    principle that the sign of the right-hand side—and therefore also the sign of
    the quadratic form—is determined by the sign of the eigenvalues.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，我们可以将任何向量<math alttext="bold u"><mi>𝐮</mi></math>设置为特征向量的某些线性组合，并且一些标量<math
    alttext="zeta"><mi>ζ</mi></math>也可以是相同的特征值线性组合。无论如何，这不改变右侧的符号——因此也不改变二次形式的符号，这一原则由特征值的符号确定。
- en: 'Now let’s think about these equations under different assumptions about the
    signs of the <math alttext="lamda"><mi>λ</mi></math> s:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们根据对<math alttext="lamda"><mi>λ</mi></math>的不同假设来思考这些方程：
- en: All eigenvalues are positive
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 所有特征值均为正
- en: The right-hand side of the equation is always positive, meaning that <math alttext="bold
    v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi> <mi>𝐯</mi></mrow></math> is always positive for any vector <math alttext="bold
    v"><mi>𝐯</mi></math> .
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 方程的右侧始终为正，这意味着对于任何向量<math alttext="bold v"><mi>𝐯</mi></math>，<math alttext="bold
    v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi> <mi>𝐯</mi></mrow></math>始终为正。
- en: Eigenvalues are positive or zero
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值为正或零
- en: <math alttext="bold v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>𝐯</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></math> is nonnegative and
    will equal zero when <math alttext="lamda equals 0"><mrow><mi>λ</mi> <mo>=</mo>
    <mn>0</mn></mrow></math> (which happens when the matrix is singular).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当矩阵是奇异矩阵时，<math alttext="bold v Superscript upper T Baseline bold upper A bold
    v"><mrow><msup><mi>𝐯</mi> <mtext>T</mtext></msup> <mi>𝐀</mi> <mi>𝐯</mi></mrow></math>是非负的，并且等于零，这时<math
    alttext="lamda equals 0"><mrow><mi>λ</mi> <mo>=</mo> <mn>0</mn></mrow></math>。
- en: Eigenvalues are negative or zero
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值为负或零
- en: The quadratic form result will be zero or negative.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 二次形式的结果将是零或负。
- en: Eigenvalues are negative
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 特征值为负
- en: The quadratic form result will be negative for all vectors.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有向量，二次形式的结果将为负。
- en: Definiteness
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 明确性
- en: '*Definiteness* is a characteristic of a square matrix and is defined by the
    signs of the eigenvalues of the matrix, which is the same thing as the signs of
    the quadratic form results. Definiteness has implications for the invertibility
    of a matrix as well as advanced data analysis methods such as generalized eigendecomposition
    (used in multivariate linear classifiers and signal processing).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*明确性* 是方阵的一个特征，并且由矩阵的特征值的符号定义，这与二次型结果的符号相同。明确性对矩阵的可逆性以及高级数据分析方法（如广义特征分解，用于多变量线性分类器和信号处理）有重要意义。'
- en: There are five categories of definiteness, as shown in [Table 13-1](#def-cat);
    the + and − signs indicate the signs of the eigenvalues.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 有五种明确性类别，如[表格 13-1](#def-cat)所示；+ 和 − 符号表示特征值的符号。
- en: Table 13-1\. Definiteness categories
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 13-1\. 明确性类别
- en: '| Category | Quadratic form | Eigenvalues | Invertible |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 类别 | 二次型 | 特征值 | 可逆 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Positive definite | Positive | + | Yes |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 正定 | 正 | + | 是 |'
- en: '| Positive semidefinite | Nonnegative | + and 0 | No |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 正半定 | 非负 | + 和 0 | 不 |'
- en: '| Indefinite | Positive and negative | + and − | Depends |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 不定 | 正负 | + 和 − | 取决于 |'
- en: '| Negative semidefinite | Nonpositive | − and 0 | No |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 负半定 | 非正 | − 和 0 | 不 |'
- en: '| Negative definite | Negative | − | Yes |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 负定 | 负 | − | 是 |'
- en: “Depends” in the table means that the matrix can be invertible or singular depending
    on the numbers in the matrix, not on the definiteness category.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 表格中的“取决于”意味着矩阵的可逆性或奇异性取决于矩阵中的数字，而不是确定性类别。
- en: <math alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math> Is Positive (Semi)definite
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <math alttext="粗体大写A上标大写T基线粗体大写A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> 是正（半）定的
- en: Any matrix that can be expressed as the product of a matrix and its transpose
    (that is, <math alttext="bold upper S equals bold upper A Superscript upper T
    Baseline bold upper A"><mrow><mi>𝐒</mi> <mo>=</mo> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> ) is guaranteed to be positive definite or positive semidefinite.
    The combination of these two categories is often written as “positive (semi)definite.”
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 任何可以表示为矩阵及其转置乘积（即，<math alttext="粗体大写S等于粗体大写A上标大写T基线粗体大写A"><mrow><mi>𝐒</mi>
    <mo>=</mo> <msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math> ）的矩阵都保证是正定或正半定的。这两个类别的结合通常被写作“正（半）定”。
- en: All data covariance matrices are positive (semi)definite, because they are defined
    as the data matrix times its transpose. This means that all covariance matrices
    have nonnegative eigenvalues. The eigenvalues will be all positive when the data
    matrix is full-rank (full column-rank if the data is stored as observations by
    features), and there will be at least one zero-valued eigenvalue if the data matrix
    is reduced-rank.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 所有数据协方差矩阵都是正（半）定的，因为它们被定义为数据矩阵乘以其转置。这意味着所有协方差矩阵具有非负特征值。当数据矩阵是满秩时（如果数据以观测值和特征存储，则为满列秩），特征值将全部为正；如果数据矩阵是降秩的，则至少会有一个零值特征值。
- en: The proof that <math alttext="bold upper S"><mi>𝐒</mi></math> is positive (semi)definite
    comes from writing out its quadratic form and applying some algebraic manipulations.
    (Observe that the transition from the first to the second equation simply involves
    moving parentheses around; such “proof by parentheses” is common in linear algebra.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="粗体大写S"><mi>𝐒</mi></math> 是正（半）定的证明来源于列出其二次型并应用一些代数操作。（注意，从第一个到第二个方程的过渡只涉及移动括号；这种“括号证明”在线性代数中很常见。）
- en: <math alttext="StartLayout 1st Row 1st Column bold w Superscript upper T Baseline
    bold upper S bold w 2nd Column equals bold w Superscript upper T Baseline left-parenthesis
    bold upper A Superscript upper T Baseline bold upper A right-parenthesis bold
    w 2nd Row 1st Column Blank 2nd Column equals left-parenthesis bold w Superscript
    upper T Baseline bold upper A Superscript upper T Baseline right-parenthesis left-parenthesis
    bold upper A bold w right-parenthesis 3rd Row 1st Column Blank 2nd Column equals
    left-parenthesis bold upper A bold w right-parenthesis Superscript upper T Baseline
    left-parenthesis bold upper A bold w right-parenthesis 4th Row 1st Column Blank
    2nd Column equals parallel-to bold upper A bold w parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup>
    <mi>𝐒</mi> <mi>𝐰</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐰</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi> <mo>)</mo></mrow> <mi>𝐰</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mrow><mo>(</mo> <msup><mi>𝐰</mi> <mtext>T</mtext></msup> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mi>𝐀</mi> <mi>𝐰</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi>𝐀</mi><mi>𝐰</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐀</mi> <mi>𝐰</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>∥</mo><mi>𝐀</mi><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold w Superscript upper T Baseline
    bold upper S bold w 2nd Column equals bold w Superscript upper T Baseline left-parenthesis
    bold upper A Superscript upper T Baseline bold upper A right-parenthesis bold
    w 2nd Row 1st Column Blank 2nd Column equals left-parenthesis bold w Superscript
    upper T Baseline bold upper A Superscript upper T Baseline right-parenthesis left-parenthesis
    bold upper A bold w right-parenthesis 3rd Row 1st Column Blank 2nd Column equals
    left-parenthesis bold upper A bold w right-parenthesis Superscript upper T Baseline
    left-parenthesis bold upper A bold w right-parenthesis 4th Row 1st Column Blank
    2nd Column equals parallel-to bold upper A bold w parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>𝐰</mi> <mtext>T</mtext></msup>
    <mi>𝐒</mi> <mi>𝐰</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>𝐰</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi> <mo>)</mo></mrow> <mi>𝐰</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mrow><mo>(</mo> <msup><mi>𝐰</mi> <mtext>T</mtext></msup> <msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mi>𝐀</mi> <mi>𝐰</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi>𝐀</mi><mi>𝐰</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>𝐀</mi> <mi>𝐰</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>∥</mo><mi>𝐀</mi><mi>𝐰</mi><mo>∥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: The point is that the quadratic form of <math alttext="bold upper A Superscript
    upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> equals the squared magnitude of a matrix times a vector.
    Magnitudes cannot be negative, and can be zero only when the vector is zero. And
    if <math alttext="bold upper A bold w equals bold 0"><mrow><mi>𝐀</mi> <mi>𝐰</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> for a nontrival <math alttext="bold
    w"><mi>𝐰</mi></math> , then <math alttext="bold upper A"><mi>𝐀</mi></math> is
    singular.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于<math alttext="粗体大写A上标大写T基线粗体大写A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> 的二次型等于矩阵乘以向量的平方幅度。幅度不能为负，只有当向量为零时才能为零。如果<math alttext="粗体大写A粗体小写w等于粗体
    0"><mrow><mi>𝐀</mi> <mi>𝐰</mi> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math>对于一个非平凡的<math
    alttext="粗体小写w"><mi>𝐰</mi></math> ，则<math alttext="粗体大写A"><mi>𝐀</mi></math> 是奇异的。
- en: Keep in mind that although all <math alttext="bold upper A Superscript upper
    T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>
    matrices are symmetric, not all symmetric matrices can be expressed as <math alttext="bold
    upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup>
    <mi>𝐀</mi></mrow></math> . In other words, matrix symmetry on its own does not
    guarantee positive (semi)definiteness, because not all symmetric matrices can
    be expressed as the product of a matrix and its transpose.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，尽管所有的<math alttext="bold upper A Superscript upper T Baseline bold upper
    A"><mrow><msup><mi>𝐀</mi> <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math>矩阵都是对称的，但并非所有对称矩阵都可以表示为<math
    alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>𝐀</mi>
    <mtext>T</mtext></msup> <mi>𝐀</mi></mrow></math> 。换句话说，仅靠矩阵的对称性并不能保证正定性或半正定性，因为并非所有对称矩阵都可以表示为矩阵与其转置的乘积。
- en: Generalized Eigendecomposition
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 广义特征分解
- en: 'Consider that the following equation is the same as the fundamental eigenvalue
    equation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下方程与基本特征值方程相同：
- en: <math alttext="bold upper A bold v equals lamda bold upper I bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐈</mi> <mi>𝐯</mi></mrow></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold upper I bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐈</mi> <mi>𝐯</mi></mrow></math>
- en: 'This is obvious because <math alttext="bold upper I bold v equals bold v"><mrow><mi>𝐈</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>𝐯</mi></mrow></math> . Generalized eigendecomposition
    involves replacing the identity matrix with another matrix (not the identity or
    zeros matrix):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这很明显，因为<math alttext="bold upper I bold v equals bold v"><mrow><mi>𝐈</mi> <mi>𝐯</mi>
    <mo>=</mo> <mi>𝐯</mi></mrow></math> 。广义特征分解涉及用另一个矩阵（不是单位矩阵或零矩阵）替换单位矩阵：
- en: <math alttext="bold upper A bold v equals lamda bold upper B bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐁</mi> <mi>𝐯</mi></mrow></math>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold upper B bold v" display="block"><mrow><mi>𝐀</mi>
    <mi>𝐯</mi> <mo>=</mo> <mi>λ</mi> <mi>𝐁</mi> <mi>𝐯</mi></mrow></math>
- en: Generalized eigendecomposition is also called *simultaneous diagonalization
    of two matrices*. The resulting ( <math alttext="lamda comma bold v"><mrow><mi>λ</mi>
    <mo>,</mo> <mi>𝐯</mi></mrow></math> ) pair is not an eigenvalue/vector of <math
    alttext="bold upper A"><mi>𝐀</mi></math> alone nor of <math alttext="bold upper
    B"><mi>𝐁</mi></math> alone. Instead, the two matrices share eigenvalue/vector
    pairs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 广义特征分解也称为*两个矩阵的同时对角化*。产生的（ <math alttext="lamda comma bold v"><mrow><mi>λ</mi>
    <mo>,</mo> <mi>𝐯</mi></mrow></math> ）对不是仅属于矩阵<math alttext="bold upper A"><mi>𝐀</mi></math>或矩阵<math
    alttext="bold upper B"><mi>𝐁</mi></math>的特征值/特征向量。相反，这两个矩阵共享特征值/特征向量对。
- en: 'Conceptually, you can think of generalized eigendecomposition as the “regular”
    eigendecomposition of a product matrix:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上来说，你可以将广义特征分解看作是一个乘积矩阵的“常规”特征分解：
- en: <math alttext="StartLayout 1st Row 1st Column bold upper C 2nd Column equals
    bold upper A bold upper B Superscript negative 1 Baseline 2nd Row 1st Column bold
    upper C bold v 2nd Column equals lamda bold v EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>𝐂</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐀</mi> <msup><mi>𝐁</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐂</mi> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper C 2nd Column equals
    bold upper A bold upper B Superscript negative 1 Baseline 2nd Row 1st Column bold
    upper C bold v 2nd Column equals lamda bold v EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>𝐂</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>𝐀</mi> <msup><mi>𝐁</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>𝐂</mi> <mi>𝐯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>λ</mi> <mi>𝐯</mi></mrow></mtd></mtr></mtable></math>
- en: This is just conceptual; in practice, generalized eigendecomposition does not
    require <math alttext="bold upper B"><mi>𝐁</mi></math> to be invertible.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是概念性的；在实践中，广义特征分解不要求矩阵<math alttext="bold upper B"><mi>𝐁</mi></math>是可逆的。
- en: It is not the case that any two matrices can be simultaneously diagonalized.
    But this diagonalization is possible if <math alttext="bold upper B"><mi>𝐁</mi></math>
    is positive (semi)definite.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有两个矩阵都可以同时对角化。但是如果<math alttext="bold upper B"><mi>𝐁</mi></math>是正定（半）定的，则这种对角化是可能的。
- en: 'NumPy does not compute generalized eigendecomposition, but SciPy does. If you
    know that the two matrices are symmetric, you can use the function `eigh`, which
    is more numerically stable:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy不计算广义特征分解，但SciPy计算。如果你知道这两个矩阵是对称的，你可以使用函数`eigh`，这更加数值稳定：
- en: '[PRE8]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Be mindful of the order of inputs: the second input is the one that is conceptually
    inverted.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在输入顺序时要小心：概念上，第二个输入是被倒置的。
- en: In data science, generalized eigendecomposition is used in classification analysis.
    In particular, Fisher’s linear discriminant analysis is based on the generalized
    eigendecomposition of two data covariance matrices. You’ll see an example in [Chapter 15](ch15.xhtml#Chapter_15).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学中，广义特征分解被用于分类分析。特别是，费舍尔线性判别分析基于两个数据协方差矩阵的广义特征分解。你将在[第 15 章](ch15.xhtml#Chapter_15)中看到一个例子。
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'That was quite a chapter! Here is a reminder of the key points:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 这确实是一章！这里是关键点的提醒：
- en: Eigendecomposition identifies *M* scalar/vector pairs of an <math alttext="upper
    M times upper M"><mrow><mi>M</mi> <mo>×</mo> <mi>M</mi></mrow></math> matrix.
    Those pairs of eigenvalue/eigenvector reflect special directions in the matrix
    and have myriad applications in data science (principal components analysis being
    a common one), as well as in geometry, physics, computational biology, and myriad
    other technical disciplines.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征分解识别*M*个标量/向量对于一个<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>×</mo>
    <mi>M</mi></mrow></math>矩阵。这些特征值/特征向量对反映了矩阵中的特殊方向，并在数据科学（主成分分析是一个常见应用）、几何学、物理学、计算生物学等技术学科中有着广泛的应用。
- en: Eigenvalues are found by assuming that the matrix shifted by an unknown scalar
    <math alttext="lamda"><mi>λ</mi></math> is singular, setting its determinant to
    zero (called the *characteristic polynomial*), and solving for the <math alttext="lamda"><mi>λ</mi></math>
    s.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征值通过假设由未知标量<math alttext="lamda"><mi>λ</mi></math> 转移的矩阵是奇异的，将其行列式设为零（称为*特征多项式*），并解出<math
    alttext="lamda"><mi>λ</mi></math> s来找到。
- en: Eigenvectors are found by finding the basis vector for the null space of the
    <math alttext="lamda"><mi>λ</mi></math> -shifted matrix.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征向量通过寻找<math alttext="lamda"><mi>λ</mi></math> -转移矩阵的零空间的基向量来找到。
- en: '*Diagonalizing a matrix* means to represent the matrix as <math alttext="bold
    upper V Superscript negative 1 Baseline bold upper Lamda bold upper V"><mrow><msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">Λ</mi><mi
    mathvariant="bold">V</mi></mrow></math>, where <math alttext="bold upper V"><mi>𝐕</mi></math>
    is a matrix with eigenvectors in the columns and <math alttext="bold upper Lamda"><mi
    mathvariant="bold">Λ</mi></math> is a diagonal matrix with eigenvalues in the
    diagonal elements.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对角化矩阵*意味着将矩阵表示为<math alttext="bold upper V Superscript negative 1 Baseline
    bold upper Lamda bold upper V"><mrow><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mi mathvariant="bold">Λ</mi><mi mathvariant="bold">V</mi></mrow></math>，其中<math
    alttext="bold upper V"><mi>𝐕</mi></math>是列中包含特征向量的矩阵，<math alttext="bold upper
    Lamda"><mi mathvariant="bold">Λ</mi></math>是对角线上包含特征值的对角矩阵。'
- en: Symmetric matrices have several special properties in eigendecomposition; the
    most relevant for data science is that all eigenvectors are pair-wise orthogonal.
    This means that the matrix of eigenvectors is an orthogonal matrix (when the eigenvectors
    are unit normalized), which in turn means that the inverse of the eigenvectors
    matrix is its transpose.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称矩阵在特征分解中具有几个特殊性质；在数据科学中最相关的是所有特征向量是两两正交的。这意味着特征向量矩阵是正交矩阵（当特征向量单位归一化时），进而意味着特征向量矩阵的逆是其转置。
- en: The *definiteness* of a matrix refers to the signs of its eigenvalues. In data
    science, the most relevant categories are positive (semi)definite, which means
    that all eigenvalues are either nonnegative or positive.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵的*定性*指的是其特征值的符号。在数据科学中，最相关的类别是正（半）定的，这意味着所有特征值要么非负要么正。
- en: A matrix times its transpose is always positive (semi)definite, which means
    all covariance matrices have nonnegative eigenvalues.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个矩阵乘以其转置始终是正（半）定的，这意味着所有协方差矩阵都具有非负特征值。
- en: The study of eigendecomposition is rich and detailed, and many fascinating subtleties,
    special cases, and applications have been discovered. I hope that the overview
    in this chapter provides a solid grounding for your needs as a data scientist,
    and may have inspired you to learn more about the fantastic beauty of eigendecomposition.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征分解的研究内容丰富而详细，已经发现了许多迷人的细节、特殊情况和应用。希望本章概述能为您作为数据科学家的需求提供坚实的基础，并可能激发您进一步了解特征分解的奇妙之美。
- en: Code Exercises
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码练习
- en: Exercise 13-1\.
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 13-1\.
- en: Interestingly, the eigenvectors of <math alttext="bold upper A Superscript negative
    1"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> are the same
    as the eigenvectors of <math alttext="bold upper A"><mi>𝐀</mi></math> while the
    eigenvalues are <math alttext="lamda Superscript negative 1"><msup><mi>λ</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> . Prove that this is the case
    by writing out the eigendecomposition of <math alttext="bold upper A"><mi>𝐀</mi></math>
    and <math alttext="bold upper A Superscript negative 1"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Then illustrate it using a random full-rank <math alttext="5 times 5"><mrow><mn>5</mn>
    <mo>×</mo> <mn>5</mn></mrow></math> symmetric matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，<math alttext="bold upper A Superscript negative 1"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    的特征向量与<math alttext="bold upper A"><mi>𝐀</mi></math> 的特征向量相同，而特征值是<math alttext="lamda
    Superscript negative 1"><msup><mi>λ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>。通过写出<math
    alttext="bold upper A"><mi>𝐀</mi></math> 和<math alttext="bold upper A Superscript
    negative 1"><msup><mi>𝐀</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> 的特征分解来证明这一点。然后使用一个随机的全秩<math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>×</mo> <mn>5</mn></mrow></math> 对称矩阵进行说明。
- en: Exercise 13-2\.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 13-2\.
- en: 'Re-create the left-side panel of [Figure 13-1](#fig_13_1), but using the *rows*
    of <math alttext="bold upper V"><mi>𝐕</mi></math> instead of *columns*. Of course
    you know that this is a coding error, but the results are insightful: it fails
    the geometry test that the matrix times its eigenvector only stretches.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 重新创建[Figure 13-1](#fig_13_1)的左侧面板，但使用<math alttext="bold upper V"><mi>𝐕</mi></math>的*行*而不是*列*。当然你知道这是一个编码错误，但结果很有见地：它未能通过矩阵乘以其特征向量仅进行拉伸的几何测试。
- en: Exercise 13-3\.
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-3。
- en: The goal of this exercise is to demonstrate that eigenvalues are inextricably
    coupled to their eigenvectors. Diagonalize a symmetric random-integers matrix^([7](ch13.xhtml#idm45733291700640))
    created using the additive method (see [Exercise 5-9](ch05.xhtml#exercise_5_9)),
    but randomly reorder the eigenvalues (let’s call this matrix <math alttext="bold
    upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Λ</mi> <mo>˜</mo></mover></math>)
    without reordering the eigenvectors.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 这项练习的目标是展示特征值与它们的特征向量密切相关。使用加法方法创建对称随机整数矩阵^([7](ch13.xhtml#idm45733291700640))（参见[Exercise
    5-9](ch05.xhtml#exercise_5_9)），但随机重新排列特征值（我们将这个矩阵称为<math alttext="bold upper Lamda
    overTilde"><mover accent="true"><mi mathvariant="bold">Λ</mi> <mo>˜</mo></mover></math>），而不重新排列特征向量。
- en: First, demonstrate that you can reconstruct the original matrix as <math alttext="bold
    upper V bold upper Lamda bold upper V Superscript negative 1"><mrow><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Λ</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    . You can compute reconstruction accuracy as the Frobenius distance between the
    original and reconstructed matrix. Next, attempt to reconstruct the matrix using
    <math alttext="bold upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Λ</mi>
    <mo>˜</mo></mover></math>. How close is the reconstructed matrix to the original?
    What happens if you only swap the two largest eigenvalues instead of randomly
    reordering them? How about the two smallest eigenvalues?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，证明你能够将原始矩阵重构为<math alttext="bold upper V bold upper Lamda bold upper V Superscript
    negative 1"><mrow><mi mathvariant="bold">V</mi><mi mathvariant="bold">Λ</mi><msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>。你可以通过计算原始矩阵和重构矩阵之间的Frobenius距离来评估重构精度。接下来，尝试使用<math
    alttext="bold upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Λ</mi>
    <mo>˜</mo></mover></math>重构矩阵。重构矩阵与原始矩阵有多接近？如果仅交换两个最大的特征值而不是随机重新排序呢？那么最小的两个特征值呢？
- en: Finally, create a bar plot showing the Frobenius distances to the original matrix
    for the different swapping options ([Figure 13-3](#fig_13_3)). (Of course, because
    of the random matrices—and thus, random eigenvalues—your plot won’t look exactly
    like mine.)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，创建一个条形图，显示不同交换选项对于原始矩阵的Frobenius距离（[Figure 13-3](#fig_13_3)）。当然，由于随机矩阵——因此，随机特征值——你的图表看起来不会完全与我的相同。
- en: '![exercise 13-3](assets/plad_1303.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 13-3](assets/plad_1303.png)'
- en: Figure 13-3\. Results of Exercise 13-3
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-3。Exercise 13-3的结果
- en: Exercise 13-4\.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-4。
- en: One interesting property of random matrices is that their complex-valued eigenvalues
    are distributed in a circle with a radius proportional to the size of the matrix.
    To demonstrate this, compute 123 random <math alttext="42 times 42"><mrow><mn>42</mn>
    <mo>×</mo> <mn>42</mn></mrow></math> matrices, extract their eigenvalues, divide
    by the square root of the matrix size (42), and plot the eigenvalues on the complex
    plane, as in [Figure 13-4](#fig_13_4).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 随机矩阵的一个有趣性质是它们的复值特征值在一个半径与矩阵大小成比例的圆内分布。为了证明这一点，计算123个随机<math alttext="42 times
    42"><mrow><mn>42</mn> <mo>×</mo> <mn>42</mn></mrow></math>矩阵，提取它们的特征值，除以矩阵大小的平方根（42），并在复平面上绘制特征值，如[Figure 13-4](#fig_13_4)中所示。
- en: '![exercise 13-4](assets/plad_1304.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 13-4](assets/plad_1304.png)'
- en: Figure 13-4\. Results of Exercise 13-4
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图13-4。Exercise 13-4的结果
- en: Exercise 13-5\.
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-5。
- en: This exercise will help you better understand that an eigenvector is the basis
    for the null space of the eigenvalue-shifted matrix—and it will also reveal the
    risks of numerical precision errors. Eigendecompose a random <math alttext="3
    times 3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math> symmetric matrix.
    Then for each eigenvalue, use `scipy.linalg.null_space()` to find a basis vector
    for the null space of each shifted matrix. Are those vectors the same as the eigenvectors?
    Note that you might need to take into consideration the norms and the sign indeterminacies
    of eigenvectors.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习将帮助你更好地理解特征向量是特征值偏移矩阵的零空间的基础，也将揭示数值精度误差的风险。特征分解一个随机 <math alttext="3 times
    3"><mrow><mn>3</mn> <mo>×</mo> <mn>3</mn></mrow></math> 对称矩阵。然后对于每个特征值，使用 `scipy.linalg.null_space()`
    找到每个偏移矩阵的零空间的基向量。这些向量是否与特征向量相同？请注意，你可能需要考虑特征向量的范数和符号不确定性。
- en: When you run the code multiple times for different random matrices, you are
    likely to get Python errors. The error comes from an empty null space for the
    <math alttext="lamda"><mi>λ</mi></math> -shifted matrix, which, upon investigation,
    comes from the shifted matrix being full rank. (Don’t take my word for it; confirm
    this yourself!) That is not supposed to happen, which highlights—yet again—that
    (1) finite-precision math on computers does not always conform to chalkboard math
    and (2) you should use the targeted and more numerically stable functions instead
    of trying to make direct translations of formulas into code.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 当你多次运行代码使用不同的随机矩阵时，你可能会遇到 Python 错误。错误来自于偏移矩阵为空零空间，这个问题在于偏移矩阵是满秩的。 （不要只听我的话，自己确认一下！）这是不应该发生的，这再次突显了（1）计算机上的有限精度数学并不总是符合黑板数学和（2）你应该使用有针对性且更稳定的函数，而不是尝试直接将公式翻译成代码。
- en: Exercise 13-6\.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-6\.
- en: I’m going to teach you a third method to create random symmetric matrices.^([8](ch13.xhtml#idm45733291670912))
    Start by creating a <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo> <mn>4</mn></mrow></math>
    diagonal matrix with positive numbers on the diagonals (they can be, for example,
    the numbers 1, 2, 3, 4). Then create a <math alttext="4 times 4"><mrow><mn>4</mn>
    <mo>×</mo> <mn>4</mn></mrow></math> <math alttext="bold upper Q"><mi>𝐐</mi></math>
    matrix from the QR decomposition of a random-numbers matrix. Use these matrices
    as the eigenvalues and eigenvectors, and multiply them appropriately to assemble
    a matrix. Confirm that the assembled matrix is symmetric, and that its eigenvalues
    equal the eigenvalues you specified.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我将教你第三种方法来创建随机对称矩阵。^([8](ch13.xhtml#idm45733291670912)) 首先创建一个对角线上有正数（例如，可以是数字
    1、2、3、4）的 <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo> <mn>4</mn></mrow></math>
    对角矩阵。然后从一个随机数矩阵的QR分解中创建一个 <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo>
    <mn>4</mn></mrow></math> 的 <math alttext="bold upper Q"><mi>𝐐</mi></math> 矩阵。将这些矩阵用作特征值和特征向量，并适当地相乘以组装一个矩阵。确认组装的矩阵是对称的，并且其特征值等于你指定的特征值。
- en: Exercise 13-7\.
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-7\.
- en: Let’s revisit [Exercise 12-4](ch12.xhtml#exercise_12_4). Redo that exercise
    but use the average of the eigenvalues instead of the squared Frobenius norm of
    the design matrix (this is known as *shrinkage regularization*). How does the
    resulting figure compare with that from [Chapter 12](ch12.xhtml#Chapter_12)?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视[练习 12-4](ch12.xhtml#exercise_12_4)。重新进行该练习，但使用特征值的平均数而不是设计矩阵的平方Frobenius范数（这被称为*收缩正则化*）。结果图与[第
    12 章](ch12.xhtml#Chapter_12)的图相比如何？
- en: Exercise 13-8\.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-8\.
- en: 'This and the following exercise are closely linked. We will create surrogate
    data with a specified correlation matrix (this exercise), and then remove the
    correlation (next exercise). The formula to create data with a specified correlation
    structure is:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习与下一个练习密切相关。我们将创建具有指定相关矩阵的模拟数据（这个练习），然后移除相关性（下一个练习）。创建具有指定相关结构数据的公式是：
- en: <math alttext="bold upper Y equals bold upper V bold upper Lamda Superscript
    1 slash 2 Baseline bold upper X" display="block"><mrow><mi mathvariant="bold">Y</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Λ</mi> <mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup>
    <mi mathvariant="bold">X</mi></mrow></math>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Y equals bold upper V bold upper Lamda Superscript
    1 slash 2 Baseline bold upper X" display="block"><mrow><mi mathvariant="bold">Y</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Λ</mi> <mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup>
    <mi mathvariant="bold">X</mi></mrow></math>
- en: where <math alttext="bold upper V"><mi>𝐕</mi></math> and <math alttext="bold
    upper Lamda"><mi mathvariant="bold">Λ</mi></math> are the eigenvectors and eigenvalues
    of a correlation matrix, and <math alttext="bold upper X"><mi>𝐗</mi></math> is
    an <math alttext="upper N times upper T"><mrow><mi>N</mi> <mo>×</mo> <mi>T</mi></mrow></math>
    matrix of uncorrelated random numbers (*N* channels and *T* time points).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 <math alttext="bold upper V"><mi>𝐕</mi></math> 和 <math alttext="bold upper
    Lamda"><mi mathvariant="bold">Λ</mi></math> 是相关矩阵的特征向量和特征值，<math alttext="bold
    upper X"><mi>𝐗</mi></math> 是一个未相关随机数（*N* 通道和 *T* 时间点）的 <math alttext="upper N
    times upper T"><mrow><mi>N</mi> <mo>×</mo> <mi>T</mi></mrow></math> 矩阵。
- en: 'Apply that formula to create a 3 × 10,000 data matrix <math alttext="bold upper
    Y"><mi>𝐘</mi></math> with the following correlation structure:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 应用该公式创建一个 3 × 10,000 的数据矩阵 <math alttext="bold upper Y"><mi>𝐘</mi></math>，具有以下相关结构：
- en: <math display="block"><mrow><mi>𝐑</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mrow><mn>.9</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mn>1</mn></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.9</mn></mrow></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>𝐑</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mrow><mn>.9</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mn>1</mn></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.9</mn></mrow></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: Then compute the empirical correlation matrix of the data matrix <math alttext="bold
    upper X"><mi>𝐗</mi></math> . It won’t exactly equal <math alttext="bold upper
    R"><mi>𝐑</mi></math> because we are randomly sampling a finite dataset. But it
    should be fairly close (e.g., within .01).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 然后计算数据矩阵 <math alttext="bold upper X"><mi>𝐗</mi></math> 的经验相关矩阵。它不会完全等于 <math
    alttext="bold upper R"><mi>𝐑</mi></math>，因为我们是从有限数据集中随机抽样的。但应该非常接近（例如，误差在 0.01
    范围内）。
- en: Exercise 13-9\.
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 13-9\.
- en: 'Now let’s remove those imposed correlations by *whitening*. Whitening is a
    term in signal and image processing to remove correlations. A multivariate time
    series can be whitened by implementing the following formula:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们通过 *白化* 来消除这些强加的相关性。白化是信号和图像处理中的术语，用于消除相关性。可以通过实施以下公式对多变量时间序列进行白化：
- en: <math alttext="bold upper Y overTilde equals bold upper Y Superscript upper
    T Baseline bold upper V bold upper Lamda Superscript negative 1 slash 2" display="block"><mrow><mover
    accent="true"><mi mathvariant="bold">Y</mi> <mo>˜</mo></mover> <mo>=</mo> <msup><mi
    mathvariant="bold">Y</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi>
    <msup><mi mathvariant="bold">Λ</mi> <mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></math>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Y overTilde equals bold upper Y Superscript upper
    T Baseline bold upper V bold upper Lamda Superscript negative 1 slash 2" display="block"><mrow><mover
    accent="true"><mi mathvariant="bold">Y</mi> <mo>˜</mo></mover> <mo>=</mo> <msup><mi
    mathvariant="bold">Y</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi>
    <msup><mi mathvariant="bold">Λ</mi> <mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></math>
- en: Apply that formula to the data matrix from the previous exercise, and confirm
    that the correlation matrix is the identity matrix (again, within some tolerance
    for random sampling).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 将该公式应用于上一个练习的数据矩阵，并确认相关矩阵是单位矩阵（再次强调，对于随机抽样，误差在某个容差范围内）。
- en: Exercise 13-10\.
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 13-10\.
- en: In generalized eigendecomposition, the eigenvectors are not orthogonal, even
    when both matrices are symmetric. Confirm in Python that <math alttext="bold upper
    V Superscript negative 1 Baseline not-equals bold upper V Superscript upper T"><mrow><msup><mi>𝐕</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>≠</mo> <msup><mi>𝐕</mi> <mtext>T</mtext></msup></mrow></math>
    . This happens because although both <math alttext="bold upper A"><mi>𝐀</mi></math>
    and <math alttext="bold upper B"><mi>𝐁</mi></math> are symmetric, <math alttext="bold
    upper C equals bold upper A bold upper B"><mrow><mi>𝐂</mi> <mo>=</mo> <mi>𝐀</mi>
    <mi>𝐁</mi></mrow></math> is not symmetric.^([9](ch13.xhtml#idm45733291596240))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在广义特征分解中，即使两个矩阵都是对称的，特征向量也不是正交的。在 Python 中确认 <math alttext="bold upper V Superscript
    negative 1 Baseline not-equals bold upper V Superscript upper T"><mrow><msup><mi>𝐕</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>≠</mo> <msup><mi>𝐕</mi> <mtext>T</mtext></msup></mrow></math>
    。这是因为虽然 <math alttext="bold upper A"><mi>𝐀</mi></math> 和 <math alttext="bold upper
    B"><mi>𝐁</mi></math> 都是对称的，但 <math alttext="bold upper C equals bold upper A bold
    upper B"><mrow><mi>𝐂</mi> <mo>=</mo> <mi>𝐀</mi> <mi>𝐁</mi></mrow></math> 不是对称的。^([9](ch13.xhtml#idm45733291596240))
- en: However, the eigenvectors are orthogonal with respect to <math alttext="bold
    upper B"><mi>𝐁</mi></math> , which means that <math alttext="bold upper V Superscript
    upper T Baseline bold upper B bold upper V equals bold upper I"><mrow><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup> <mi>𝐁</mi> <mi>𝐕</mi> <mo>=</mo> <mi>𝐈</mi></mrow></math>
    . Confirm these properties by performing a generalized eigendecomposition on two
    symmetric matrices, and producing [Figure 13-5](#fig_13_5).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，特征向量在 <math alttext="bold upper B"><mi>𝐁</mi></math> 方面是正交的，这意味着 <math alttext="bold
    upper V Superscript upper T Baseline bold upper B bold upper V equals bold upper
    I"><mrow><msup><mi>𝐕</mi> <mtext>T</mtext></msup> <mi>𝐁</mi> <mi>𝐕</mi> <mo>=</mo>
    <mi>𝐈</mi></mrow></math> 。通过对两个对称矩阵执行广义特征分解并生成 [图 13-5](#fig_13_5) 来确认这些属性。
- en: '![exercise 13-10](assets/plad_1305.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![练习 13-10](assets/plad_1305.png)'
- en: Figure 13-5\. Results of Exercise 13-10
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 13-5\. 练习 13-10 的结果
- en: Exercise 13-11\.
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 13-11\.
- en: 'Let’s explore the scaling of eigenvectors. Start by creating a <math alttext="4
    times 4"><mrow><mn>4</mn> <mo>×</mo> <mn>4</mn></mrow></math> matrix of random
    integers drawn between −14 and +14\. Diagonalize the matrix and empirically confirm
    that <math alttext="bold upper A equals bold upper V bold upper Lamda bold upper
    V Superscript negative 1"><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Λ</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    . Confirm that the Euclidean norm of each eigenvector equals 1\. Note that the
    square of a complex number is computed as that number times its complex conjugate
    (hint: use `np.conj()`).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索特征向量的缩放。首先创建一个随机整数矩阵，其大小为 <math alttext="4 times 4"><mrow><mn>4</mn> <mo>×</mo>
    <mn>4</mn></mrow></math>，取值范围在 -14 到 +14 之间。对该矩阵进行对角化，并通过实验证实 <math alttext="bold
    upper A equals bold upper V bold upper Lamda bold upper V Superscript negative
    1"><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Λ</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    。确认每个特征向量的欧几里德范数等于 1。注意，复数的平方通过其复共轭进行计算（提示：使用 `np.conj()`）。
- en: Next, multiply the eigenvectors matrix by any nonzero scalar. I used <math alttext="pi"><mi>π</mi></math>
    for no particularly good reason other than it was fun to type. Does this scalar
    affect the accuracy of the reconstructed matrix and/or the norms of the eigenvectors?
    Why or why not?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，将特征向量矩阵乘以任意非零标量。我选择了<math alttext="pi"><mi>π</mi></math>，没有特别好的理由，只是打字时很有趣。这个标量会影响重构矩阵的准确性和/或特征向量的范数吗？为什么？
- en: Finally, repeat this but use a symmetric matrix, and replace <math alttext="bold
    upper V Superscript negative 1"><msup><mi>𝐕</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    with <math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi> <mtext>T</mtext></msup></math>
    . Does this change the conclusion?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，重复这一过程，但使用对称矩阵，并用<math alttext="bold upper V Superscript upper T"><msup><mi>𝐕</mi>
    <mtext>T</mtext></msup></math>替换<math alttext="bold upper V Superscript negative
    1"><msup><mi>𝐕</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>。这是否改变了结论？
- en: ^([1](ch13.xhtml#idm45733293579328-marker)) Approximately −.6 and 1.6.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.xhtml#idm45733293579328-marker)) 大约是−0.6和1.6。
- en: ^([2](ch13.xhtml#idm45733293558128-marker)) “Open Letter to Kansas School Board,”
    Church of the Flying Spaghetti Monster, [*spaghettimon⁠ster​.org/about/open-letter*](https://www.spaghettimonster.org/about/open-letter).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.xhtml#idm45733293558128-marker)) “开放信件给堪萨斯州教育委员会”，飞行意大利面怪物教堂，[*spaghettimon⁠ster​.org/about/open-letter*](https://www.spaghettimonster.org/about/open-letter)。
- en: ^([3](ch13.xhtml#idm45733293437712-marker)) As I wrote in [Chapter 5](ch05.xhtml#Chapter_5),
    Python will return a result, but that is the scalar broadcast subtracted, which
    is not a linear algebra operation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.xhtml#idm45733293437712-marker)) 正如我在[第5章](ch05.xhtml#Chapter_5)中写的，Python会返回一个结果，但那是标量广播减去的结果，这不是一个线性代数操作。
- en: '^([4](ch13.xhtml#idm45733293126624-marker)) To quell suspense: it makes the
    eigenvectors matrix an orthogonal matrix.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch13.xhtml#idm45733293126624-marker)) 为了消除悬念：它使特征向量矩阵成为正交矩阵。
- en: ^([5](ch13.xhtml#idm45733292508688-marker)) By “straightforward” I mean mathematically
    expected; interpreting complex solutions in eigendecomposition is far from straightforward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch13.xhtml#idm45733292508688-marker)) “直观”指的是数学上的预期；解释特征分解中的复杂解决方案远非直观。
- en: ^([6](ch13.xhtml#idm45733292090752-marker)) In the interest of brevity, I am
    omitting some subtlety here about rare cases where the eigenvectors matrix does
    not span the entire *M*-dimensional subspace.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch13.xhtml#idm45733292090752-marker)) 为了简洁起见，我在这里忽略了有关特征向量矩阵不涵盖整个*M*维子空间的一些细微差别。
- en: ^([7](ch13.xhtml#idm45733291700640-marker)) I often use symmetric matrices in
    exercises because they have real-valued eigenvalues, but that doesn’t change the
    principle or the math; it merely facilitates visual inspection of the solutions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch13.xhtml#idm45733291700640-marker)) 在练习中我经常使用对称矩阵，因为它们具有实数特征值，但这并不改变原则或数学，只是便于解决方案的视觉检查。
- en: ^([8](ch13.xhtml#idm45733291670912-marker)) The first two were the multiplicative
    and additive methods.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.xhtml#idm45733291670912-marker)) 第一个是乘法方法，第二个是加法方法。
- en: ^([9](ch13.xhtml#idm45733291596240-marker)) The reason why the product of two
    symmetric matrices is not symmetric is the same as the reason why **R** from **QR**
    decomposition has zeros on the lower-diagonal.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch13.xhtml#idm45733291596240-marker)) 两个对称矩阵的乘积不对称的原因与**QR**分解中**R**在对角线下有零的原因相同。
