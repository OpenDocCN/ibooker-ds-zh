- en: Chapter 13\. Eigendecomposition
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬13ç«  ç‰¹å¾åˆ†è§£
- en: 'Eigendecomposition is a pearl of linear algebra. What is a pearl? Let me quote
    directly from the book *20,000 Leagues Under the Sea*:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾åˆ†è§£æ˜¯çº¿æ€§ä»£æ•°ä¸­çš„ä¸€é¢—æ˜ç ã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯æ˜ç å‘¢ï¼Ÿè®©æˆ‘ç›´æ¥å¼•ç”¨ã€Šæµ·åº•ä¸¤ä¸‡é‡Œã€‹ä¸­çš„ä¸€æ®µè¯ï¼š
- en: For poets, a pearl is a tear from the sea; for Orientals, itâ€™s a drop of solidified
    dew; for the ladies itâ€™s a jewel they can wear on their fingers, necks, and ears
    thatâ€™s oblong in shape, glassy in luster, and formed from mother-of-pearl; for
    chemists, itâ€™s a mixture of calcium phosphate and calcium carbonate with a little
    gelatin protein; and finally, for naturalists, itâ€™s a simple festering secretion
    from the organ that produces mother-of-pearl in certain bivalves.
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: å¯¹äºè¯—äººæ¥è¯´ï¼Œçç æ˜¯æµ·æ´‹ä¹‹æ³ªï¼›å¯¹ä¸œæ–¹äººæ¥è¯´ï¼Œå®ƒæ˜¯å‡å›ºäº†çš„éœ²ç æ»´ï¼›å¯¹å¥³å£«ä»¬æ¥è¯´ï¼Œå®ƒæ˜¯å¯ä»¥æˆ´åœ¨æ‰‹æŒ‡ã€é¢ˆéƒ¨å’Œè€³æœµä¸Šçš„ç å®ï¼Œå‘ˆæ¤­åœ†å½¢ï¼Œå…·æœ‰ç»ç’ƒèˆ¬çš„å…‰æ³½ï¼Œç”±çç æ¯å½¢æˆï¼›å¯¹åŒ–å­¦å®¶æ¥è¯´ï¼Œå®ƒæ˜¯ä¸€ç§å«æœ‰å°‘é‡æ˜èƒ¶è›‹ç™½çš„ç£·é…¸é’™å’Œç¢³é…¸é’™çš„æ··åˆç‰©ï¼›æœ€åï¼Œå¯¹äºè‡ªç„¶å­¦å®¶æ¥è¯´ï¼Œå®ƒæ˜¯æŸäº›åŒå£³ç±»åŠ¨ç‰©äº§ç”Ÿæ¯çç çš„ç®€å•è…è´¥åˆ†æ³Œç‰©ã€‚
- en: ''
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Jules Verne
  id: totrans-4
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: æœ±å°”Â·å‡¡å°”çº³
- en: 'The point is that the same object can be seen in different ways depending on
    its use. So it is with eigendecomposition: eigendecomposition has a geometric
    interpretation (axes of rotational invariance), a statistical interpetation (directions
    of maximal covariance), a dynamical-systems interpretation (stable system states),
    a graph-theoretic interpretation (the impact of a node on its network), a financial-market
    interpretation (identifying stocks that covary), and many more.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äºï¼Œç›¸åŒçš„å¯¹è±¡å¯ä»¥æ ¹æ®å…¶ç”¨é€”ä»¥ä¸åŒæ–¹å¼çœ‹å¾…ã€‚ç‰¹å¾åˆ†è§£ä¹Ÿæ˜¯å¦‚æ­¤ï¼šç‰¹å¾åˆ†è§£æœ‰å‡ ä½•è§£é‡Šï¼ˆæ—‹è½¬ä¸å˜æ€§çš„è½´ï¼‰ã€ç»Ÿè®¡è§£é‡Šï¼ˆæœ€å¤§åæ–¹å·®çš„æ–¹å‘ï¼‰ã€åŠ¨åŠ›ç³»ç»Ÿè§£é‡Šï¼ˆç¨³å®šç³»ç»ŸçŠ¶æ€ï¼‰ã€å›¾è®ºè§£é‡Šï¼ˆèŠ‚ç‚¹åœ¨ç½‘ç»œä¸­çš„å½±å“ï¼‰ã€é‡‘èå¸‚åœºè§£é‡Šï¼ˆè¯†åˆ«å…±å˜çš„è‚¡ç¥¨ï¼‰ç­‰ç­‰ã€‚
- en: Eigendecomposition (and the SVD, which, as youâ€™ll learn in the next chapter,
    is closely related to eigendecomposition) is among the most important contributions
    of linear algebra to data science. The purpose of this chapter is to provide you
    an intuitive understanding of eigenvalues and eigenvectorsâ€”the results of eigendecomposition
    of a matrix. Along the way, youâ€™ll learn about diagonalization and more special
    properties of symmetric matrices. After extending eigendecomposition to the SVD
    in [ChapterÂ 14](ch14.xhtml#Chapter_14), youâ€™ll see a few applications of eigendecomposition
    in [ChapterÂ 15](ch15.xhtml#Chapter_15).
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾åˆ†è§£ï¼ˆä»¥åŠå¥‡å¼‚å€¼åˆ†è§£ï¼Œæ­£å¦‚ä½ å°†åœ¨ä¸‹ä¸€ç« å­¦åˆ°çš„é‚£æ ·ï¼Œä¸ç‰¹å¾åˆ†è§£å¯†åˆ‡ç›¸å…³ï¼‰æ˜¯çº¿æ€§ä»£æ•°å¯¹æ•°æ®ç§‘å­¦çš„æœ€é‡è¦è´¡çŒ®ä¹‹ä¸€ã€‚æœ¬ç« çš„ç›®çš„æ˜¯ä¸ºä½ æä¾›ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„ç›´è§‚ç†è§£â€”â€”çŸ©é˜µç‰¹å¾åˆ†è§£çš„ç»“æœã€‚åœ¨è¿™è¿‡ç¨‹ä¸­ï¼Œä½ å°†å­¦åˆ°å¯¹ç§°çŸ©é˜µçš„å¯¹è§’åŒ–å’Œæ›´å¤šç‰¹æ®Šæ€§è´¨ã€‚åœ¨[ç¬¬14ç« ](ch14.xhtml#Chapter_14)ä¸­æ‰©å±•åˆ°å¥‡å¼‚å€¼åˆ†è§£åï¼Œä½ å°†çœ‹åˆ°ç‰¹å¾åˆ†è§£åœ¨[ç¬¬15ç« ](ch15.xhtml#Chapter_15)ä¸­çš„å‡ ä¸ªåº”ç”¨ã€‚
- en: Interpretations of Eigenvalues and Eigenvectors
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '[ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡çš„è§£é‡Š](https://example.org/eigenvalues_and_eigenvectors_interpretations)'
- en: There are several ways of interpreting eigenvalues/vectors that I will describe
    in the next sections. Of course, the math is the same regardless, but having mutliple
    perspectives can facilitate intuition, which in turn will help you understand
    how and why eigendecomposition is important in data science.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰å‡ ç§è§£é‡Šç‰¹å¾å€¼/å‘é‡çš„æ–¹å¼ï¼Œæˆ‘å°†åœ¨æ¥ä¸‹æ¥çš„éƒ¨åˆ†ä¸­æè¿°ã€‚å½“ç„¶ï¼Œæ— è®ºå¦‚ä½•ï¼Œæ•°å­¦éƒ½æ˜¯ç›¸åŒçš„ï¼Œä½†å¤šè§†è§’å¯ä»¥ä¿ƒè¿›ç›´è§‰ï¼Œè¿›è€Œå¸®åŠ©ä½ ç†è§£ä¸ºä»€ä¹ˆç‰¹å¾åˆ†è§£åœ¨æ•°æ®ç§‘å­¦ä¸­å¦‚æ­¤é‡è¦ã€‚
- en: Geometry
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å‡ ä½•å­¦
- en: Iâ€™ve actually already introduced you to the geometric concept of eigenvectors
    in [ChapterÂ 5](ch05.xhtml#Chapter_5). In [FigureÂ 5-5](ch05.xhtml#fig_5_5), we
    discovered that there was a special combination of a matrix and a vector such
    that the matrix *stretched*â€”but did not *rotate*â€”that vector. That vector is an
    eigenvector of the matrix, and the amount of stretching is the eigenvalue.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å®é™…ä¸Šå·²ç»åœ¨[ç¬¬5ç« ](ch05.xhtml#Chapter_5)ä¸­å‘ä½ ä»‹ç»äº†ç‰¹å¾å‘é‡çš„å‡ ä½•æ¦‚å¿µã€‚åœ¨[å›¾5-5](ch05.xhtml#fig_5_5)ä¸­ï¼Œæˆ‘ä»¬å‘ç°æœ‰ä¸€ç§ç‰¹æ®Šçš„çŸ©é˜µå’Œå‘é‡çš„ç»„åˆï¼Œä½¿å¾—çŸ©é˜µ*æ‹‰ä¼¸*äº†è¿™ä¸ªå‘é‡ï¼Œä½†æ²¡æœ‰*æ—‹è½¬*ã€‚é‚£ä¸ªå‘é‡æ˜¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œè€Œæ‹‰ä¼¸çš„ç¨‹åº¦æ˜¯ç‰¹å¾å€¼ã€‚
- en: '[FigureÂ 13-1](#fig_13_1) shows vectors before and after multiplication by a
    <math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>
    matrix. The two vectors in the left plot ( <math alttext="bold v 1 and bold v
    2"><mrow><msub><mi>ğ¯</mi> <mn>1</mn></msub> <mtext>and</mtext> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></math> ) are eigenvectors whereas the two vectors in
    the right plot are not. The eigenvectors point in the same direction before and
    after postmultiplying the matrix. The eigenvalues encode the amount of stretching;
    try to guess the eigenvalues based on visual inspection of the plot. The answers
    are in the footnote.^([1](ch13.xhtml#idm45733293579328))'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾ 13-1](#fig_13_1) å±•ç¤ºäº†åœ¨ <math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo>
    <mn>2</mn></mrow></math> çŸ©é˜µåä¹˜ä»¥å‰åä¸¤ä¸ªå‘é‡çš„æƒ…å†µã€‚å·¦å›¾ä¸­çš„ä¸¤ä¸ªå‘é‡ï¼ˆ <math alttext="ç²—ä½“ v 1 å’Œç²—ä½“ v
    2"><mrow><msub><mi>ğ¯</mi> <mn>1</mn></msub> <mtext>å’Œ</mtext> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></math> ï¼‰æ˜¯ç‰¹å¾å‘é‡ï¼Œè€Œå³å›¾ä¸­çš„ä¸¤ä¸ªå‘é‡åˆ™ä¸æ˜¯ã€‚ç‰¹å¾å‘é‡åœ¨çŸ©é˜µåä¹˜ä¹‹å‰åæŒ‡å‘ç›¸åŒæ–¹å‘ã€‚ç‰¹å¾å€¼ç¼–ç äº†æ‹‰ä¼¸çš„é‡ï¼›è¯•ç€é€šè¿‡è§†è§‰æ£€æŸ¥å›¾è¡¨çŒœæµ‹ç‰¹å¾å€¼ã€‚ç­”æ¡ˆåœ¨è„šæ³¨ä¸­ã€‚^([1](ch13.xhtml#idm45733293579328))'
- en: 'Thatâ€™s the geometric picture: an eigenvector means that matrix-vector multiplication
    acts like scalar-vector multiplication. Letâ€™s see if we can write that down in
    an equation (we can, and itâ€™s printed in [Equation 13-1](#eigen-eq)).'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å°±æ˜¯å‡ ä½•å›¾åƒï¼šç‰¹å¾å‘é‡æ„å‘³ç€çŸ©é˜µ-å‘é‡ä¹˜æ³•çš„æ•ˆæœç±»ä¼¼äºæ ‡é‡-å‘é‡ä¹˜æ³•ã€‚è®©æˆ‘ä»¬çœ‹çœ‹èƒ½å¦ç”¨æ–¹ç¨‹è¡¨ç¤ºå‡ºæ¥ï¼ˆæˆ‘ä»¬å¯ä»¥ï¼Œå¹¶ä¸”å®ƒåœ¨ [æ–¹ç¨‹å¼ 13-1](#eigen-eq)
    ä¸­æ‰“å°å‡ºæ¥ï¼‰ã€‚
- en: Equation 13-1\. Eigenvalue equation
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 13-1\. ç‰¹å¾å€¼æ–¹ç¨‹
- en: <math alttext="bold upper A bold v equals lamda bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></math>
- en: 'Be careful with interpreting that equation: it does not say that the matrix
    *equals* the scalar; it says that the *effect* of the matrix on the vector is
    the same as the *effect* of the scalar on that same vector.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è§£é‡Šè¿™ä¸ªæ–¹ç¨‹æ—¶è¦å°å¿ƒï¼šå®ƒå¹¶ä¸æ˜¯è¯´çŸ©é˜µ*ç­‰äº*æ ‡é‡ï¼›å®ƒè¯´çš„æ˜¯çŸ©é˜µå¯¹å‘é‡çš„*ä½œç”¨*ä¸æ ‡é‡å¯¹åŒä¸€å‘é‡çš„*ä½œç”¨*æ˜¯ç›¸åŒçš„ã€‚
- en: '![geoeig](assets/plad_1301.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![geoeig](assets/plad_1301.png)'
- en: Figure 13-1\. Geometry of eigenvectors
  id: totrans-17
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 13-1\. ç‰¹å¾å‘é‡çš„å‡ ä½•å½¢çŠ¶
- en: This is called the *eigenvalue equation*, and itâ€™s another key formula in linear
    algebra that is worth memorizing. Youâ€™ll see it throughout this chapter, youâ€™ll
    see a slight variation of it in the following chapter, and youâ€™ll see it many
    times when learning about multivariate statistics, signal processing, optimization,
    graph theory, and a myriad of other applications where patterns are identified
    across multiple simultaneously recorded features.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸º*ç‰¹å¾å€¼æ–¹ç¨‹*ï¼Œå®ƒæ˜¯çº¿æ€§ä»£æ•°ä¸­å¦ä¸€ä¸ªå€¼å¾—è®°å¿†çš„å…³é”®å…¬å¼ã€‚åœ¨æœ¬ç« ä¸­ä½ ä¼šçœ‹åˆ°å®ƒï¼Œæ¥ä¸‹æ¥çš„ç« èŠ‚ä¸­ä¼šçœ‹åˆ°ç¨å¾®å˜åŒ–çš„å½¢å¼ï¼Œä»¥åŠåœ¨å­¦ä¹ å¤šå˜é‡ç»Ÿè®¡ã€ä¿¡å·å¤„ç†ã€ä¼˜åŒ–ã€å›¾è®ºä»¥åŠè®¸å¤šå…¶ä»–åº”ç”¨ä¸­ä¼šçœ‹åˆ°å®ƒï¼Œè¿™äº›åº”ç”¨ä¸­è·¨å¤šä¸ªåŒæ—¶è®°å½•çš„ç‰¹å¾è¯†åˆ«å‡ºæ¨¡å¼ã€‚
- en: Statistics (Principal Components Analysis)
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»Ÿè®¡å­¦ï¼ˆä¸»æˆåˆ†åˆ†æï¼‰
- en: One of the reasons why people apply statistics is to identify and quantify relationships
    between variables. For example, the rise of global temperatures correlates with
    the decline in the number of pirates,^([2](ch13.xhtml#idm45733293558128)) but
    how strong is that relationship? Of course, when you have only two variables,
    a simple correlation (like what you learned in [ChapterÂ 4](ch04.xhtml#Chapter_4))
    is sufficient. But in a multivariate dataset that includes dozens or hundreds
    of variables, bivariate correlations cannot reveal global patterns.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: äººä»¬åº”ç”¨ç»Ÿè®¡çš„åŸå› ä¹‹ä¸€æ˜¯è¯†åˆ«å’Œé‡åŒ–å˜é‡ä¹‹é—´çš„å…³ç³»ã€‚ä¾‹å¦‚ï¼Œå…¨çƒæ¸©åº¦ä¸Šå‡ä¸æµ·ç›—æ•°é‡ä¸‹é™ç›¸å…³ï¼Œ^([2](ch13.xhtml#idm45733293558128))
    ä½†è¿™ç§å…³ç³»æœ‰å¤šå¼ºï¼Ÿå½“ç„¶ï¼Œå½“ä½ åªæœ‰ä¸¤ä¸ªå˜é‡æ—¶ï¼Œåƒä½ åœ¨ [ç¬¬å››ç« ](ch04.xhtml#Chapter_4) ä¸­å­¦åˆ°çš„ç®€å•ç›¸å…³å°±è¶³å¤Ÿäº†ã€‚ä½†åœ¨åŒ…å«æ•°åä¸ªæˆ–æ•°ç™¾ä¸ªå˜é‡çš„å¤šå˜é‡æ•°æ®é›†ä¸­ï¼ŒåŒå˜é‡ç›¸å…³æ€§æ— æ³•æ­ç¤ºå…¨å±€æ¨¡å¼ã€‚
- en: Letâ€™s make this more concrete with an example. Cryptocurrencies are digital
    stores of value that are encoded in a blockchain, which is a system for keeping
    track of transactions. Youâ€™ve probably heard of Bitcoin and Ethereum; there are
    tens of thousands of other cryptocoins that have various purposes. We can ask
    whether the entirety of the cryptospace operates as a single system (meaning that
    the value of all coins go up and down together), or whether there are independent
    subcategories within that space (meaning that some coins or groups of coins change
    in value independently of the value of other coins).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªä¾‹å­æ¥å…·ä½“åŒ–è¿™ä¸ªæ¦‚å¿µã€‚åŠ å¯†è´§å¸æ˜¯æ•°å­—ä»·å€¼å­˜å‚¨ï¼Œç¼–ç åœ¨åŒºå—é“¾ä¸­ï¼Œè¿™æ˜¯ä¸€ç§è·Ÿè¸ªäº¤æ˜“çš„ç³»ç»Ÿã€‚ä½ å¯èƒ½å¬è¯´è¿‡æ¯”ç‰¹å¸å’Œä»¥å¤ªåŠï¼›è¿˜æœ‰æˆåƒä¸Šä¸‡å…¶ä»–å…·æœ‰å„ç§ç›®çš„çš„åŠ å¯†å¸ã€‚æˆ‘ä»¬å¯ä»¥è¯¢é—®æ•´ä¸ªåŠ å¯†ç©ºé—´æ˜¯å¦ä½œä¸ºä¸€ä¸ªå•ä¸€ç³»ç»Ÿè¿ä½œï¼ˆæ„å‘³ç€æ‰€æœ‰å¸å€¼åŒæ—¶ä¸Šä¸‹æ³¢åŠ¨ï¼‰ï¼Œæˆ–è€…åœ¨è¯¥ç©ºé—´å†…æ˜¯å¦å­˜åœ¨ç‹¬ç«‹çš„å­ç±»åˆ«ï¼ˆæ„å‘³ç€ä¸€äº›å¸æˆ–ä¸€äº›å¸ç»„ç‹¬ç«‹äºå…¶ä»–å¸çš„ä»·å€¼å˜åŒ–ï¼‰ã€‚
- en: We can test this hypothesis by performing a principal components analysis on
    a dataset that contains the prices of various cryptocoins over time. If the entire
    cryptomarket operates as a single entity, then the *scree plot* (a graph of the
    eigenvalues of the datasetâ€™s covariance matrix) would reveal that one component
    accounts for most of the variance of the system, and all other components account
    for very little variance (graph A in [FigureÂ 13-2](#fig_13_2)). In contrast, if
    the cryptomarket had, say, three major subcategories with independent price movements,
    then we would expect to see three large eigenvalues (graph B in [FigureÂ 13-2](#fig_13_2)).
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥é€šè¿‡å¯¹åŒ…å«å„ç§åŠ å¯†è´§å¸ä»·æ ¼çš„æ•°æ®é›†æ‰§è¡Œä¸»æˆåˆ†åˆ†ææ¥æµ‹è¯•è¿™ä¸ªå‡è®¾ã€‚å¦‚æœæ•´ä¸ªåŠ å¯†å¸‚åœºåƒä¸€ä¸ªå•ä¸€å®ä½“è¿ä½œï¼Œé‚£ä¹ˆ*å±é£å›¾*ï¼ˆæ•°æ®é›†åæ–¹å·®çŸ©é˜µç‰¹å¾å€¼çš„å›¾è¡¨ï¼‰å°†æ˜¾ç¤ºä¸€ä¸ªæˆåˆ†å ç³»ç»Ÿæ–¹å·®çš„å¤§éƒ¨åˆ†ï¼Œè€Œæ‰€æœ‰å…¶ä»–æˆåˆ†åˆ™å æå°‘çš„æ–¹å·®ï¼ˆå›¾Aåœ¨[å›¾13-2](#fig_13_2)ä¸­ï¼‰ã€‚ç›¸åï¼Œå¦‚æœåŠ å¯†å¸‚åœºæœ‰ä¸‰ä¸ªä¸»è¦å­ç±»åˆ«å…·æœ‰ç‹¬ç«‹çš„ä»·æ ¼å˜åŠ¨ï¼Œåˆ™æˆ‘ä»¬é¢„è®¡ä¼šçœ‹åˆ°ä¸‰ä¸ªè¾ƒå¤§çš„ç‰¹å¾å€¼ï¼ˆå›¾Båœ¨[å›¾13-2](#fig_13_2)ä¸­ï¼‰ã€‚
- en: '![geoeig](assets/plad_1302.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![geoeig](assets/plad_1302.png)'
- en: Figure 13-2\. Simulated scree plots of multivariate datasets (data is simulated
    to illustrate outcome possibilities)
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾13-2\. æ¨¡æ‹Ÿå¤šå˜é‡æ•°æ®é›†çš„å±é£å›¾ï¼ˆæ•°æ®æ¨¡æ‹Ÿä»¥è¯´æ˜ç»“æœå¯èƒ½æ€§ï¼‰
- en: Noise Reduction
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å™ªå£°å‡å°‘
- en: Most datasets contain noise. *Noise* refers to variance in a dataset that is
    either unexplained (e.g., random variation) or unwanted (e.g., electrical line
    noise artifacts in radio signals). There are many ways to attenuate or eliminate
    noise, and the optimal noise-reduction strategy depends on the nature and origin
    of the noise and on the characteristics of the signal.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: å¤§å¤šæ•°æ•°æ®é›†åŒ…å«å™ªå£°ã€‚*å™ªå£°*æ˜¯æŒ‡æ•°æ®é›†ä¸­æœªè§£é‡Šçš„ï¼ˆä¾‹å¦‚éšæœºå˜åŒ–ï¼‰æˆ–ä¸éœ€è¦çš„ï¼ˆä¾‹å¦‚æ— çº¿ç”µä¿¡å·ä¸­çš„ç”µæ°”çº¿å™ªå£°ä¼ªå½±ï¼‰æ–¹å·®ã€‚æœ‰è®¸å¤šæ–¹å¼å¯ä»¥å‡å¼±æˆ–æ¶ˆé™¤å™ªå£°ï¼Œæœ€ä½³çš„å™ªå£°å‡å°‘ç­–ç•¥å–å†³äºå™ªå£°çš„æ€§è´¨å’Œæ¥æºï¼Œä»¥åŠä¿¡å·çš„ç‰¹å¾ã€‚
- en: One method of reducing random noise is to identify the eigenvalues and eigenvectors
    of a system, and â€œproject outâ€ directions in the data space associated with small
    eigenvalues. The assumption is that random noise makes a relatively small contribution
    to the total variance. â€œProjecting outâ€ a data dimension means to reconstruct
    a dataset after setting some eigenvalues that are below some threshold to zero.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘éšæœºå™ªéŸ³çš„ä¸€ç§æ–¹æ³•æ˜¯è¯†åˆ«ç³»ç»Ÿçš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¹¶åœ¨æ•°æ®ç©ºé—´ä¸­ä¸å°ç‰¹å¾å€¼ç›¸å…³è”çš„æ–¹å‘ä¸Šâ€œæŠ•å½±å‡ºâ€è¿™äº›ç‰¹å¾ã€‚å‡è®¾éšæœºå™ªå£°å¯¹æ€»æ–¹å·®çš„è´¡çŒ®ç›¸å¯¹è¾ƒå°ã€‚åœ¨è®¾ç½®ä¸€äº›ä½äºæŸä¸ªé˜ˆå€¼çš„ç‰¹å¾å€¼ä¸ºé›¶åï¼Œ"æŠ•å½±å‡º"æ•°æ®ç»´åº¦æ„å‘³ç€é‡å»ºæ•°æ®é›†ã€‚
- en: Youâ€™ll see an example of using eigendecomposition to reduce noise in [ChapterÂ 15](ch15.xhtml#Chapter_15).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: æ‚¨å°†åœ¨[ç¬¬15ç« ](ch15.xhtml#Chapter_15)ä¸­çœ‹åˆ°ä½¿ç”¨ç‰¹å¾åˆ†è§£æ¥å‡å°‘å™ªéŸ³çš„ç¤ºä¾‹ã€‚
- en: Dimension Reduction (Data Compression)
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»´åº¦é™ä½ï¼ˆæ•°æ®å‹ç¼©ï¼‰
- en: Information communications technologies like phones, internet, and TV create
    and transmit a huge amount of data, such as pictures and videos. Transmitting
    data can be time-consuming and expensive, and it is beneficial to *compress* the
    data before transmitting it. Compression means to reduce the size of the data
    (in terms of bytes) while having minimal impact on the quality of the data. For
    example, a TIFF format image file might be 10 MB, while the JPG converted version
    might be .1 MB while still retaining reasonably good quality.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: åƒç”µè¯ã€äº’è”ç½‘å’Œç”µè§†è¿™æ ·çš„ä¿¡æ¯é€šä¿¡æŠ€æœ¯åˆ›é€ å’Œä¼ è¾“å¤§é‡æ•°æ®ï¼Œå¦‚å›¾ç‰‡å’Œè§†é¢‘ã€‚ä¼ è¾“æ•°æ®å¯èƒ½è€—æ—¶ä¸”æ˜‚è´µï¼Œå› æ­¤åœ¨ä¼ è¾“ä¹‹å‰è¿›è¡Œ*å‹ç¼©*æ˜¯æœ‰ç›Šçš„ã€‚å‹ç¼©æ„å‘³ç€å‡å°‘æ•°æ®çš„å¤§å°ï¼ˆä»¥å­—èŠ‚ä¸ºå•ä½ï¼‰ï¼ŒåŒæ—¶å¯¹æ•°æ®è´¨é‡å½±å“æœ€å°ã€‚ä¾‹å¦‚ï¼ŒTIFFæ ¼å¼çš„å›¾åƒæ–‡ä»¶å¯èƒ½ä¸º10
    MBï¼Œè€Œè½¬æ¢ä¸ºJPGæ ¼å¼åçš„ç‰ˆæœ¬å¯èƒ½åªæœ‰0.1 MBï¼ŒåŒæ—¶ä¿æŒç›¸å¯¹è‰¯å¥½çš„è´¨é‡ã€‚
- en: One way to dimension-reduce a dataset is to take its eigendecomposition, drop
    the eigenvalues and eigenvectors associated with small directions in the data
    space, and then transmit only the relatively larger eigenvector/value pairs. It
    is actually more common to use the SVD for data compression (and youâ€™ll see an
    example in [ChapterÂ 15](ch15.xhtml#Chapter_15)), although the principle is the
    same.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: é™ä½æ•°æ®é›†ç»´åº¦çš„ä¸€ç§æ–¹æ³•æ˜¯è¿›è¡Œå…¶ç‰¹å¾åˆ†è§£ï¼Œèˆå¼ƒä¸æ•°æ®ç©ºé—´ä¸­å°æ–¹å‘ç›¸å…³çš„ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œç„¶åä»…ä¼ è¾“ç›¸å¯¹è¾ƒå¤§çš„ç‰¹å¾å‘é‡/å€¼å¯¹ã€‚å®é™…ä¸Šï¼Œä½¿ç”¨SVDè¿›è¡Œæ•°æ®å‹ç¼©æ›´ä¸ºå¸¸è§ï¼ˆæ‚¨å°†åœ¨[ç¬¬15ç« ](ch15.xhtml#Chapter_15)ä¸­çœ‹åˆ°ä¸€ä¸ªä¾‹å­ï¼‰ï¼Œå°½ç®¡åŸç†æ˜¯ç›¸åŒçš„ã€‚
- en: 'Modern data compression algorithms are actually faster and more efficient than
    the method previously described, but the idea is the same: decompose the dataset
    into a set of basis vectors that capture the most important features of the data,
    and then reconstruct a high-quality version of the original data.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç°ä»£æ•°æ®å‹ç¼©ç®—æ³•å®é™…ä¸Šæ¯”å…ˆå‰æè¿°çš„æ–¹æ³•æ›´å¿«æ›´é«˜æ•ˆï¼Œä½†æ€æƒ³æ˜¯ç›¸åŒçš„ï¼šå°†æ•°æ®é›†åˆ†è§£ä¸ºæ•è·æ•°æ®æœ€é‡è¦ç‰¹å¾çš„ä¸€ç»„åŸºå‘é‡ï¼Œç„¶åé‡å»ºåŸå§‹æ•°æ®çš„é«˜è´¨é‡ç‰ˆæœ¬ã€‚
- en: Finding Eigenvalues
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æŸ¥æ‰¾ç‰¹å¾å€¼
- en: To eigendecompose a square matrix, you first find the eigenvalues, and then
    use each eigenvalue to find its corresponding eigenvector. The eigenvalues are
    like keys that you insert into the matrix to unlock the mystical eigenvector.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¦å¯¹ä¸€ä¸ªæ–¹é˜µè¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œé¦–å…ˆæ‰¾åˆ°ç‰¹å¾å€¼ï¼Œç„¶åä½¿ç”¨æ¯ä¸ªç‰¹å¾å€¼æ‰¾åˆ°å…¶å¯¹åº”çš„ç‰¹å¾å‘é‡ã€‚ç‰¹å¾å€¼å°±åƒæ˜¯ä½ æ’å…¥çŸ©é˜µä¸­ä»¥è§£é”ç¥ç§˜ç‰¹å¾å‘é‡çš„é’¥åŒ™ã€‚
- en: 'Finding the eigenvalues of a matrix is super easy in Python:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨Pythonä¸­æŸ¥æ‰¾çŸ©é˜µçš„ç‰¹å¾å€¼éå¸¸å®¹æ˜“ï¼š
- en: '[PRE0]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The two eigenvalues (rounded to the nearest hundredth) are âˆ’0.37 and 5.37.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªç‰¹å¾å€¼ï¼ˆå››èˆäº”å…¥åˆ°æœ€æ¥è¿‘çš„ç™¾åˆ†ä¹‹ä¸€ï¼‰åˆ†åˆ«æ˜¯âˆ’0.37å’Œ5.37ã€‚
- en: But the important question isnâ€™t *which function returns the eigenvalues*; instead,
    the important question is *how are the eigenvalues of a matrix identified?*
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†é‡è¦çš„é—®é¢˜ä¸æ˜¯*å“ªä¸ªå‡½æ•°è¿”å›ç‰¹å¾å€¼*ï¼›ç›¸åï¼Œé‡è¦çš„é—®é¢˜æ˜¯*å¦‚ä½•è¯†åˆ«çŸ©é˜µçš„ç‰¹å¾å€¼*ï¼Ÿ
- en: To find the eigenvalues of a matrix, we start with the eigenvalue equation shown
    in [Equation 13-1](#eigen-eq) and do some simple arithemetic, as shown in [Equation
    13-2](#eigen-reorg).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¦æ‰¾åˆ°çŸ©é˜µçš„ç‰¹å¾å€¼ï¼Œæˆ‘ä»¬ä»[Equation 13-1](#eigen-eq)ä¸­æ˜¾ç¤ºçš„ç‰¹å¾å€¼æ–¹ç¨‹å¼å¼€å§‹ï¼Œå¹¶è¿›è¡Œä¸€äº›ç®€å•çš„ç®—æœ¯æ“ä½œï¼Œå¦‚[Equation
    13-2](#eigen-reorg)æ‰€ç¤ºã€‚
- en: Equation 13-2\. Eigenvalue equation, reorganized
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼13-2ã€‚ç‰¹å¾å€¼æ–¹ç¨‹å¼ï¼Œé‡æ–°ç»„ç»‡
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold upper A bold v minus lamda bold v
    2nd Column equals bold 0 3rd Row 1st Column left-parenthesis bold upper A minus
    lamda bold upper I right-parenthesis bold v 2nd Column equals bold 0 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <mi>ğ¯</mi> <mo>-</mo> <mi>Î»</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi>
    <mi>ğˆ</mi> <mo>)</mo> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold upper A bold v minus lamda bold v
    2nd Column equals bold 0 3rd Row 1st Column left-parenthesis bold upper A minus
    lamda bold upper I right-parenthesis bold v 2nd Column equals bold 0 EndLayout"
    display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <mi>ğ¯</mi> <mo>-</mo> <mi>Î»</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi>
    <mi>ğˆ</mi> <mo>)</mo> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
- en: The first equation is an exact repeat of the eigenvalue equation. In the second
    equation, we simply subtracted the right-hand side to set the equation to the
    zeros vector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸€ä¸ªæ–¹ç¨‹å¼å®Œå…¨é‡å¤äº†ç‰¹å¾å€¼æ–¹ç¨‹å¼ã€‚åœ¨ç¬¬äºŒä¸ªæ–¹ç¨‹å¼ä¸­ï¼Œæˆ‘ä»¬ç®€å•åœ°å‡å»å³ä¾§ä»¥ä½¿æ–¹ç¨‹å¼ç­‰äºé›¶å‘é‡ã€‚
- en: 'The transition from the second to the third equation requires some explanation.
    The left-hand side of the second equation has two vector terms, both of which
    involve <math alttext="bold v"><mi>ğ¯</mi></math> . So we factor out the vector.
    But that leaves us with the subtraction of a matrix and a scalar ( <math alttext="bold
    upper A minus lamda"><mrow><mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi></mrow></math> ), which
    is not a defined operation in linear algebra.^([3](ch13.xhtml#idm45733293437712))
    So instead, we *shift* the matrix by <math alttext="lamda"><mi>Î»</mi></math> .
    That brings us to the third equation. (Side note: the expression <math alttext="lamda
    bold upper I"><mrow><mi>Î»</mi> <mi>ğˆ</mi></mrow></math> is sometimes called a
    *scalar matrix*.)'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ä»ç¬¬äºŒä¸ªæ–¹ç¨‹å¼åˆ°ç¬¬ä¸‰ä¸ªæ–¹ç¨‹å¼çš„è¿‡æ¸¡éœ€è¦ä¸€äº›è§£é‡Šã€‚ç¬¬äºŒä¸ªæ–¹ç¨‹å¼çš„å·¦ä¾§æœ‰ä¸¤ä¸ªå‘é‡é¡¹ï¼Œéƒ½æ¶‰åŠ<math alttext="bold v"><mi>ğ¯</mi></math>ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†å‘é‡å› å­åŒ–ã€‚ä½†è¿™æ ·åšä¼šå¯¼è‡´æˆ‘ä»¬å‡å»ä¸€ä¸ªçŸ©é˜µå’Œä¸€ä¸ªæ ‡é‡ï¼ˆ<math
    alttext="bold upper A minus lamda"><mrow><mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi></mrow></math>ï¼‰ï¼Œè¿™åœ¨çº¿æ€§ä»£æ•°ä¸­ä¸æ˜¯ä¸€ä¸ªå®šä¹‰è‰¯å¥½çš„æ“ä½œã€‚^([3](ch13.xhtml#idm45733293437712))
    å› æ­¤ï¼Œæˆ‘ä»¬æ”¹ä¸ºå°†çŸ©é˜µ*å¹³ç§»*<math alttext="lamda"><mi>Î»</mi></math>ã€‚è¿™å°±å¾—åˆ°äº†ç¬¬ä¸‰ä¸ªæ–¹ç¨‹å¼ã€‚ï¼ˆæ—æ³¨ï¼šè¡¨è¾¾å¼<math
    alttext="lamda bold upper I"><mrow><mi>Î»</mi> <mi>ğˆ</mi></mrow></math>æœ‰æ—¶ç§°ä¸º*æ ‡é‡çŸ©é˜µ*ã€‚ï¼‰
- en: What does that third equation mean? It means that *the eigenvector is in the
    null space of the matrix shifted by its eigenvalue*.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ä¸‰ä¸ªæ–¹ç¨‹å¼æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿå®ƒæ„å‘³ç€*ç‰¹å¾å‘é‡ä½äºå…¶ç‰¹å¾å€¼å¹³ç§»çš„çŸ©é˜µçš„é›¶ç©ºé—´ä¸­*ã€‚
- en: 'If it helps you understand the concept of the eigenvector as the null-space
    vector of the shifted matrix, you can think of adding two additional equations:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœè¿™æœ‰åŠ©äºä½ ç†è§£ç‰¹å¾å‘é‡ä½œä¸ºçŸ©é˜µå¹³ç§»åçš„é›¶ç©ºé—´å‘é‡çš„æ¦‚å¿µï¼Œä½ å¯ä»¥è€ƒè™‘æ·»åŠ ä¸¤ä¸ªé¢å¤–çš„æ–¹ç¨‹å¼ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A overTilde 2nd Column
    equals bold upper A minus lamda bold upper I 2nd Row 1st Column bold upper A overTilde
    bold v 2nd Column equals bold 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mover accent="true"><mi>ğ€</mi> <mo>Ëœ</mo></mover></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mover accent="true"><mi>ğ€</mi> <mo>Ëœ</mo></mover>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A overTilde 2nd Column
    equals bold upper A minus lamda bold upper I 2nd Row 1st Column bold upper A overTilde
    bold v 2nd Column equals bold 0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mover accent="true"><mi>ğ€</mi> <mo>Ëœ</mo></mover></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mover accent="true"><mi>ğ€</mi> <mo>Ëœ</mo></mover>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mn mathvariant="bold">0</mn></mrow></mtd></mtr></mtable></math>
- en: Why is that statement so insightful? Remember that we ignore trivial solutions
    in linear algebra, so we do not consider <math alttext="bold v equals bold 0"><mrow><mi>ğ¯</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> to be an eigenvector. And
    that means that the matrix shifted by its eigenvalue is singular, because only
    singular matrices have a nontrivial null space.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆè¿™ä¸ªé™ˆè¿°å¦‚æ­¤å…·æœ‰æ´å¯ŸåŠ›ï¼Ÿè¯·è®°ä½ï¼Œåœ¨çº¿æ€§ä»£æ•°ä¸­æˆ‘ä»¬å¿½ç•¥å¹³å‡¡è§£ï¼Œå› æ­¤æˆ‘ä»¬ä¸è®¤ä¸º<math alttext="bold v equals bold 0"><mrow><mi>ğ¯</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math>æ˜¯ä¸€ä¸ªç‰¹å¾å‘é‡ã€‚è¿™æ„å‘³ç€çŸ©é˜µç»è¿‡å…¶ç‰¹å¾å€¼å¹³ç§»åæ˜¯å¥‡å¼‚çš„ï¼Œå› ä¸ºåªæœ‰å¥‡å¼‚çŸ©é˜µæ‰æœ‰ä¸€ä¸ªéå¹³å‡¡çš„é›¶ç©ºé—´ã€‚
- en: 'And what else do we know about singular matrices? We know that their determinant
    is zero. Hence:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: è¿˜æœ‰å…³äºå¥‡å¼‚çŸ©é˜µæˆ‘ä»¬çŸ¥é“ä»€ä¹ˆï¼Ÿæˆ‘ä»¬çŸ¥é“å®ƒä»¬çš„è¡Œåˆ—å¼ä¸ºé›¶ã€‚å› æ­¤ï¼š
- en: <math alttext="StartAbsoluteValue bold upper A minus lamda bold upper I EndAbsoluteValue
    equals 0" display="block"><mrow><mo>|</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi>
    <mo>|</mo> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartAbsoluteValue bold upper A minus lamda bold upper I EndAbsoluteValue
    equals 0" display="block"><mrow><mo>|</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi>
    <mo>|</mo> <mo>=</mo> <mn>0</mn></mrow></math>
- en: 'Believe it or not, thatâ€™s the key to finding eigenvalues: shift the matrix
    by the unknown eigenvalue <math alttext="lamda"><mi>Î»</mi></math> , set its determinant
    to zero, and solve for <math alttext="lamda"><mi>Î»</mi></math> . Letâ€™s see how
    this looks for a <math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>
    matrix:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ä¿¡ä¸ä¿¡ç”±ä½ ï¼Œè¿™å°±æ˜¯æ‰¾åˆ°ç‰¹å¾å€¼çš„å…³é”®ï¼šå°†çŸ©é˜µå¹³ç§»è‡³æœªçŸ¥ç‰¹å¾å€¼<math alttext="lamda"><mi>Î»</mi></math>ï¼Œå°†å…¶è¡Œåˆ—å¼è®¾ä¸ºé›¶ï¼Œå¹¶è§£å‡º<math
    alttext="lamda"><mi>Î»</mi></math>ã€‚è®©æˆ‘ä»¬çœ‹çœ‹å¯¹äºä¸€ä¸ª<math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>Ã—</mo> <mn>2</mn></mrow></math>çŸ©é˜µï¼Œè¿™æ˜¯ä»€ä¹ˆæ ·å­ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column StartAbsoluteValue Start 2 By
    2 Matrix 1st Row 1st Column a 2nd Column b 2nd Row 1st Column c 2nd Column d EndMatrix
    minus lamda Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st
    Column 0 2nd Column 1 EndMatrix EndAbsoluteValue equals 2nd Column 0 2nd Row 1st
    Column Start 2 By 2 Determinant 1st Row 1st Column a minus lamda 2nd Column b
    2nd Row 1st Column c 2nd Column d minus lamda EndDeterminant equals 2nd Column
    0 3rd Row 1st Column left-parenthesis a minus lamda right-parenthesis left-parenthesis
    d minus lamda right-parenthesis minus b c equals 2nd Column 0 4th Row 1st Column
    lamda squared minus left-parenthesis a plus d right-parenthesis lamda plus left-parenthesis
    a d minus b c right-parenthesis equals 2nd Column 0 EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"
    separators=""><mfenced close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd> <mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced> <mo>-</mo>
    <mi>Î»</mi> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mo>-</mo> <mi>Î»</mi></mrow></mtd> <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd>
    <mtd><mrow><mi>d</mi> <mo>-</mo> <mi>Î»</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>a</mi> <mo>-</mo> <mi>Î»</mi>
    <mo>)</mo> <mo>(</mo> <mi>d</mi> <mo>-</mo> <mi>Î»</mi> <mo>)</mo> <mo>-</mo> <mi>b</mi>
    <mi>c</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>Î»</mi> <mn>2</mn></msup> <mo>-</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo></mrow> <mi>Î»</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>-</mo> <mi>b</mi> <mi>c</mi>
    <mo>)</mo></mrow> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column StartAbsoluteValue Start 2 By
    2 Matrix 1st Row 1st Column a 2nd Column b 2nd Row 1st Column c 2nd Column d EndMatrix
    minus lamda Start 2 By 2 Matrix 1st Row 1st Column 1 2nd Column 0 2nd Row 1st
    Column 0 2nd Column 1 EndMatrix EndAbsoluteValue equals 2nd Column 0 2nd Row 1st
    Column Start 2 By 2 Determinant 1st Row 1st Column a minus lamda 2nd Column b
    2nd Row 1st Column c 2nd Column d minus lamda EndDeterminant equals 2nd Column
    0 3rd Row 1st Column left-parenthesis a minus lamda right-parenthesis left-parenthesis
    d minus lamda right-parenthesis minus b c equals 2nd Column 0 4th Row 1st Column
    lamda squared minus left-parenthesis a plus d right-parenthesis lamda plus left-parenthesis
    a d minus b c right-parenthesis equals 2nd Column 0 EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"
    separators=""><mfenced close="]" open="["><mtable><mtr><mtd><mi>a</mi></mtd> <mtd><mi>b</mi></mtd></mtr>
    <mtr><mtd><mi>c</mi></mtd> <mtd><mi>d</mi></mtd></mtr></mtable></mfenced> <mo>-</mo>
    <mi>Î»</mi> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd> <mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mfenced close="|" open="|"><mtable><mtr><mtd><mrow><mi>a</mi>
    <mo>-</mo> <mi>Î»</mi></mrow></mtd> <mtd><mi>b</mi></mtd></mtr> <mtr><mtd><mi>c</mi></mtd>
    <mtd><mrow><mi>d</mi> <mo>-</mo> <mi>Î»</mi></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mo>(</mo> <mi>a</mi> <mo>-</mo> <mi>Î»</mi>
    <mo>)</mo> <mo>(</mo> <mi>d</mi> <mo>-</mo> <mi>Î»</mi> <mo>)</mo> <mo>-</mo> <mi>b</mi>
    <mi>c</mi> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>Î»</mi> <mn>2</mn></msup> <mo>-</mo>
    <mrow><mo>(</mo> <mi>a</mi> <mo>+</mo> <mi>d</mi> <mo>)</mo></mrow> <mi>Î»</mi>
    <mo>+</mo> <mrow><mo>(</mo> <mi>a</mi> <mi>d</mi> <mo>-</mo> <mi>b</mi> <mi>c</mi>
    <mo>)</mo></mrow> <mo>=</mo></mrow></mtd> <mtd columnalign="left"><mrow><mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: 'You could apply the quadratic formula to solve for the two <math alttext="lamda"><mi>Î»</mi></math>
    values. But the answer itself isnâ€™t important; the important thing is to see the
    logical progression of mathematical concepts established earlier in this book:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åº”ç”¨äºŒæ¬¡æ–¹ç¨‹å¼æ¥æ±‚è§£ä¸¤ä¸ª<math alttext="lamda"><mi>Î»</mi></math>å€¼ã€‚ä½†ç­”æ¡ˆæœ¬èº«å¹¶ä¸é‡è¦ï¼›é‡è¦çš„æ˜¯çœ‹åˆ°æœ¬ä¹¦å‰é¢å»ºç«‹çš„æ•°å­¦æ¦‚å¿µçš„é€»è¾‘è¿›å±•ï¼š
- en: The matrix-vector multiplication acts like scalar-vector multiplication (the
    eigenvalue equation).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŸ©é˜µ-å‘é‡ä¹˜æ³•çš„ä½œç”¨ç±»ä¼¼æ ‡é‡-å‘é‡ä¹˜æ³•ï¼ˆç‰¹å¾å€¼æ–¹ç¨‹ï¼‰ã€‚
- en: We set the eigenvalue equation to the zeros vector, and factor out common terms.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å°†ç‰¹å¾å€¼æ–¹ç¨‹è®¾ä¸ºé›¶å‘é‡ï¼Œå¹¶æå–å…¬å…±é¡¹ã€‚
- en: This reveals that the eigenvector is in the null space of the matrix shifted
    by the eigenvalue. We do not consider the zeros vector to be an eigenvector, which
    means the shifted matrix is singular.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è¿™æ­ç¤ºäº†ç‰¹å¾å‘é‡ä½äºè¢«ç‰¹å¾å€¼ç§»ä½çš„çŸ©é˜µçš„é›¶ç©ºé—´ä¸­ã€‚æˆ‘ä»¬ä¸è®¤ä¸ºé›¶å‘é‡æ˜¯ç‰¹å¾å‘é‡ï¼Œè¿™æ„å‘³ç€ç§»ä½çŸ©é˜µæ˜¯å¥‡å¼‚çš„ã€‚
- en: Therefore, we set the determinant of the shifted matrix to zero and solve for
    the unknown eigenvalue.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬å°†ç§»ä½çŸ©é˜µçš„è¡Œåˆ—å¼è®¾ä¸ºé›¶ï¼Œå¹¶æ±‚è§£æœªçŸ¥çš„ç‰¹å¾å€¼ã€‚
- en: The determinant of an eigenvalue-shifted matrix set to zero is called the *characteristic
    polynomial* of the matrix.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å€¼ç§»ä½çŸ©é˜µçš„è¡Œåˆ—å¼è®¾ä¸ºé›¶è¢«ç§°ä¸ºçŸ©é˜µçš„*ç‰¹å¾å¤šé¡¹å¼*ã€‚
- en: Notice that in the previous example, we started with a <math alttext="2 times
    2"><mrow><mn>2</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math> matrix and got a <math
    alttext="lamda squared"><msup><mi>Î»</mi> <mn>2</mn></msup></math> term, which
    means this is a second-order polynomial equation. You might remember from your
    high-school algebra class that an *n*th order polynomial has *n* solutions, some
    of which might be complex-valued (this is called the fundamental theorem of algebra).
    So, there will be two values of <math alttext="lamda"><mi>Î»</mi></math> that can
    satisfy the equation.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œåœ¨å‰é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬ä»ä¸€ä¸ª<math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math>çŸ©é˜µå¼€å§‹ï¼Œå¾—åˆ°äº†ä¸€ä¸ª<math
    alttext="lamda squared"><msup><mi>Î»</mi> <mn>2</mn></msup></math>é¡¹ï¼Œè¿™æ„å‘³ç€è¿™æ˜¯ä¸€ä¸ªäºŒæ¬¡å¤šé¡¹å¼æ–¹ç¨‹ã€‚ä½ å¯èƒ½è¿˜è®°å¾—ä»é«˜ä¸­ä»£æ•°è¯¾ä¸Šï¼Œä¸€ä¸ª*n*æ¬¡å¤šé¡¹å¼æœ‰*n*ä¸ªè§£ï¼Œå…¶ä¸­ä¸€äº›å¯èƒ½æ˜¯å¤æ•°ï¼ˆè¿™è¢«ç§°ä¸ºä»£æ•°åŸºæœ¬å®šç†ï¼‰ã€‚å› æ­¤ï¼Œä¼šæœ‰ä¸¤ä¸ª<math
    alttext="lamda"><mi>Î»</mi></math>å€¼æ»¡è¶³æ–¹ç¨‹ã€‚
- en: 'The matching 2s are no coincidence: the characteristic polynomial of an <math
    alttext="upper M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo> <mi>M</mi></mrow></math>
    matrix will have a <math alttext="lamda Superscript upper M"><msup><mi>Î»</mi>
    <mi>M</mi></msup></math> term. That is the reason why an <math alttext="upper
    M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo> <mi>M</mi></mrow></math> matrix will
    have *M* eigenvalues.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: åŒ¹é…çš„2å¹¶éå·§åˆï¼šä¸€ä¸ª<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo>
    <mi>M</mi></mrow></math>çŸ©é˜µçš„ç‰¹å¾å¤šé¡¹å¼å°†æœ‰ä¸€ä¸ª<math alttext="lamda Superscript upper M"><msup><mi>Î»</mi>
    <mi>M</mi></msup></math>é¡¹ã€‚è¿™æ˜¯ä¸ºä»€ä¹ˆ<math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>Ã—</mo> <mi>M</mi></mrow></math>çŸ©é˜µä¼šæœ‰*M*ä¸ªç‰¹å¾å€¼çš„åŸå› ã€‚
- en: Tedious Practice Problems
  id: totrans-60
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¹ççš„ç»ƒä¹ é—®é¢˜
- en: 'At this point in a traditional linear algebra textbook, you would be tasked
    with finding the eigenvalues of dozens of <math alttext="2 times 2"><mrow><mn>2</mn>
    <mo>Ã—</mo> <mn>2</mn></mrow></math> and <math alttext="3 times 3"><mrow><mn>3</mn>
    <mo>Ã—</mo> <mn>3</mn></mrow></math> matrices by hand. I have mixed feelings about
    these kinds of exercises: on the one hand, solving problems by hand really helps
    internalize the mechanics of finding eigenvalues; but on the other hand, I want
    to focus this book on concepts, code, and applications, without getting bogged
    down by tedious arithmetic. If you feel inspired to solve eigenvalue problems
    by hand, then go for it! You can find myriad such problems in traditional textbooks
    or online. But I took the bold (and perhaps controversial) decision to avoid hand-solved
    problems in this book, and instead to have exercises that focus on coding and
    comprehension.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ä¼ ç»Ÿçº¿æ€§ä»£æ•°æ•™æä¸­çš„è¿™ä¸€ç‚¹ä¸Šï¼Œä½ ä¼šè¢«è¦æ±‚æ‰‹åŠ¨æ‰¾å‡ºæ•°åä¸ª<math alttext="2 times 2"><mrow><mn>2</mn> <mo>Ã—</mo>
    <mn>2</mn></mrow></math>å’Œ<math alttext="3 times 3"><mrow><mn>3</mn> <mo>Ã—</mo>
    <mn>3</mn></mrow></math>çŸ©é˜µçš„ç‰¹å¾å€¼ã€‚æˆ‘å¯¹è¿™ç±»ç»ƒä¹ æœ‰ç€å¤æ‚çš„æ„Ÿå—ï¼šä¸€æ–¹é¢ï¼Œé€šè¿‡æ‰‹å·¥è§£å†³é—®é¢˜ç¡®å®æœ‰åŠ©äºå†…åŒ–æ‰¾ç‰¹å¾å€¼çš„æœºåˆ¶ï¼›ä½†å¦ä¸€æ–¹é¢ï¼Œæˆ‘å¸Œæœ›æœ¬ä¹¦ä¾§é‡äºæ¦‚å¿µã€ä»£ç å’Œåº”ç”¨ï¼Œè€Œä¸è¦è¢«ç¹ççš„ç®—æœ¯é—®é¢˜æ‰€å›°æ‰°ã€‚å¦‚æœä½ æœ‰å…´è¶£æ‰‹å·¥è§£å†³ç‰¹å¾å€¼é—®é¢˜ï¼Œé‚£å°±å»åšå§ï¼ä½ å¯ä»¥åœ¨ä¼ ç»Ÿæ•™ææˆ–åœ¨çº¿ä¸Šæ‰¾åˆ°å¤§é‡è¿™ç±»é—®é¢˜ã€‚ä½†æˆ‘åšå‡ºäº†å¤§èƒ†ï¼ˆä¹Ÿè®¸æ˜¯æœ‰äº‰è®®çš„ï¼‰å†³å®šï¼Œé¿å…åœ¨æœ¬ä¹¦ä¸­æ‰‹åŠ¨è§£å†³é—®é¢˜ï¼Œè€Œæ˜¯è¿›è¡Œä¾§é‡äºç¼–ç å’Œç†è§£çš„ç»ƒä¹ ã€‚
- en: Finding Eigenvectors
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯»æ‰¾ç‰¹å¾å‘é‡
- en: 'As with eigenvalues, finding eigenvectors is super-duper easy in Python:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œç‰¹å¾å€¼ä¸€æ ·ï¼Œåœ¨ Python ä¸­æ‰¾åˆ°ç‰¹å¾å‘é‡æ˜¯éå¸¸å®¹æ˜“çš„ï¼š
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The eigenvectors are in the columns of the matrix `evecs` and are in the same
    order as the eigenvalues (that is, the eigenvector in the first column of matrix
    `evecs` is paired with the first eigenvalue in vector `evals`). I like to use
    the variable names `evals` and `evecs`, because they are short and meaningful.
    You might also see people use variable names `L` and `V` or `D` and `V`. The `L`
    is for <math alttext="normal upper Lamda"><mi>Î›</mi></math> (the capital of <math
    alttext="lamda"><mi>Î»</mi></math> ) and the `V` is for <math alttext="bold upper
    V"><mi>ğ•</mi></math> , the matrix in which each column *i* is eigenvector <math
    alttext="bold v Subscript i"><msub><mi>ğ¯</mi> <mi>i</mi></msub></math> . The `D`
    is for *diagonal*, because eigenvalues are often stored in a diagonal matrix,
    for reasons I will explain later in this chapter.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å‘é‡ä½äºçŸ©é˜µ`evecs`çš„åˆ—ä¸­ï¼Œå¹¶ä¸”ä¸ç‰¹å¾å€¼çš„é¡ºåºç›¸åŒï¼ˆå³ï¼ŒçŸ©é˜µ`evecs`çš„ç¬¬ä¸€åˆ—ä¸­çš„ç‰¹å¾å‘é‡ä¸å‘é‡`evals`ä¸­çš„ç¬¬ä¸€ä¸ªç‰¹å¾å€¼é…å¯¹ï¼‰ã€‚æˆ‘å–œæ¬¢ä½¿ç”¨å˜é‡å`evals`å’Œ`evecs`ï¼Œå› ä¸ºå®ƒä»¬ç®€çŸ­è€Œæœ‰æ„ä¹‰ã€‚ä½ å¯èƒ½è¿˜ä¼šçœ‹åˆ°äººä»¬ä½¿ç”¨å˜é‡å`L`å’Œ`V`æˆ–`D`å’Œ`V`ã€‚`L`ä»£è¡¨<math
    alttext="normal upper Lamda"><mi>Î›</mi></math>ï¼ˆ<math alttext="lamda"><mi>Î»</mi></math>çš„å¤§å†™å½¢å¼ï¼‰ï¼Œ`V`ä»£è¡¨<math
    alttext="bold upper V"><mi>ğ•</mi></math>ï¼Œå³çŸ©é˜µï¼Œå…¶ä¸­æ¯åˆ—*i*æ˜¯ç‰¹å¾å‘é‡<math alttext="bold v
    Subscript i"><msub><mi>ğ¯</mi><mi>i</mi></msub></math> ã€‚`D`ä»£è¡¨*å¯¹è§’çº¿*ï¼Œå› ä¸ºç‰¹å¾å€¼é€šå¸¸å­˜å‚¨åœ¨å¯¹è§’çº¿çŸ©é˜µä¸­ï¼Œæˆ‘ç¨åå°†åœ¨æœ¬ç« è§£é‡ŠåŸå› ã€‚
- en: Eigenvectors in the Columns, Not the Rows!
  id: totrans-66
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç‰¹å¾å‘é‡åœ¨åˆ—ä¸­ï¼Œè€Œä¸æ˜¯è¡Œä¸­ï¼
- en: The most important thing to keep in mind about eigenvectors when coding is that
    they are stored in the *columns of the matrix*, not the rows! Such dimensional-indexing
    errors are easy to make with square matrices (because you might not get Python
    errors), but accidentally using the rows instead of the columns of the eigenvectors
    matrix can have disastrous consequences in applications. When in doubt, remember
    the discussion from [ChapterÂ 2](ch02.xhtml#Chapter_2) that common convention in
    linear algebra is to assume that vectors are in column orientation.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–ç æ—¶å…³äºç‰¹å¾å‘é‡æœ€é‡è¦çš„äº‹æƒ…æ˜¯å®ƒä»¬å­˜å‚¨åœ¨*çŸ©é˜µçš„åˆ—ä¸­*ï¼Œè€Œä¸æ˜¯è¡Œä¸­ï¼åœ¨ä½¿ç”¨æ–¹é˜µæ—¶å¾ˆå®¹æ˜“çŠ¯è¿™ç§ç»´åº¦ç´¢å¼•é”™è¯¯ï¼ˆå› ä¸ºå¯èƒ½ä¸ä¼šæ”¶åˆ°Pythoné”™è¯¯ï¼‰ï¼Œä½†æ˜¯åœ¨åº”ç”¨ä¸­æ„å¤–åœ°ä½¿ç”¨ç‰¹å¾å‘é‡çŸ©é˜µçš„è¡Œè€Œä¸æ˜¯åˆ—å¯èƒ½ä¼šäº§ç”Ÿç¾éš¾æ€§åæœã€‚å¦‚æœ‰ç–‘é—®ï¼Œè¯·è®°ä½[ç¬¬äºŒç« ](ch02.xhtml#Chapter_2)ä¸­çš„è®¨è®ºï¼Œçº¿æ€§ä»£æ•°ä¸­çš„å¸¸è§çº¦å®šæ˜¯å‡å®šå‘é‡æ˜¯åˆ—å‘é‡ã€‚
- en: OK, but again, the previous code shows how to get a NumPy function to return
    the eigenvectors of a matrix. You could have learned that from the `np.linalg.eig`
    docstring. The important question is *Where do eigenvectors come from, and how
    do we find them?*
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: å¥½çš„ï¼Œä½†å†æ¬¡å¼ºè°ƒï¼Œå‰é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•ä½¿NumPyå‡½æ•°è¿”å›çŸ©é˜µçš„ç‰¹å¾å‘é‡ã€‚ä½ å¯ä»¥ä»`np.linalg.eig`çš„æ–‡æ¡£å­—ç¬¦ä¸²ä¸­å­¦åˆ°è¿™ä¸€ç‚¹ã€‚é‡è¦çš„é—®é¢˜æ˜¯*ç‰¹å¾å‘é‡æ¥è‡ªå“ªé‡Œï¼Œæˆ‘ä»¬å¦‚ä½•æ‰¾åˆ°å®ƒä»¬ï¼Ÿ*
- en: 'Actually, Iâ€™ve already written how to find eigenvectors: find the vector <math
    alttext="bold v"><mi>ğ¯</mi></math> that is in the null space of the matrix shifted
    by <math alttext="lamda"><mi>Î»</mi></math> . In other words:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: å®é™…ä¸Šï¼Œæˆ‘å·²ç»å†™è¿‡å¦‚ä½•æ‰¾åˆ°ç‰¹å¾å‘é‡ï¼šæ‰¾åˆ°çŸ©é˜µé€šè¿‡<math alttext="lamda"><mi>Î»</mi></math>ç§»ä½åçš„é›¶ç©ºé—´å‘é‡<math
    alttext="bold v"><mi>ğ¯</mi></math> ã€‚æ¢å¥è¯è¯´ï¼š
- en: <math alttext="bold v Subscript i Baseline element-of upper N left-parenthesis
    bold upper A minus lamda Subscript i Baseline bold upper I right-parenthesis"
    display="block"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>âˆˆ</mo> <mi>N</mi>
    <mrow><mo>(</mo> <mi>ğ€</mi> <mo>-</mo> <msub><mi>Î»</mi> <mi>i</mi></msub> <mi>ğˆ</mi>
    <mo>)</mo></mrow></mrow></math>
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold v Subscript i Baseline element-of upper N left-parenthesis
    bold upper A minus lamda Subscript i Baseline bold upper I right-parenthesis"
    display="block"><mrow><msub><mi>ğ¯</mi> <mi>i</mi></msub> <mo>âˆˆ</mo> <mi>N</mi>
    <mrow><mo>(</mo> <mi>ğ€</mi> <mo>-</mo> <msub><mi>Î»</mi> <mi>i</mi></msub> <mi>ğˆ</mi>
    <mo>)</mo></mrow></mrow></math>
- en: 'Letâ€™s see a numerical example. Follwing is a matrix and its eigenvalues:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªæ•°å€¼ä¾‹å­ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªçŸ©é˜µåŠå…¶ç‰¹å¾å€¼ï¼š
- en: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>â‡’</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <mo>=</mo> <mn>3</mn> <mo>,</mo>
    <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>=</mo> <mrow><mo>-</mo> <mn>1</mn></mrow></mrow></math>
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>â‡’</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <mo>=</mo> <mn>3</mn> <mo>,</mo>
    <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>=</mo> <mrow><mo>-</mo> <mn>1</mn></mrow></mrow></math>
- en: 'Letâ€™s focus on the first eigenvalue. To reveal its eigenvector, we shift the
    matrix by 3 and find a vector in its null space:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä¸“æ³¨äºç¬¬ä¸€ä¸ªç‰¹å¾å€¼ã€‚ä¸ºäº†æ­ç¤ºå…¶ç‰¹å¾å‘é‡ï¼Œæˆ‘ä»¬å°†çŸ©é˜µç§»ä½3ï¼Œå¹¶æ‰¾åˆ°å…¶é›¶ç©ºé—´ä¸­çš„å‘é‡ï¼š
- en: <math alttext="Start 2 By 2 Matrix 1st Row 1st Column 1 minus 3 2nd Column 2
    2nd Row 1st Column 2 2nd Column 1 minus 3 EndMatrix equals Start 2 By 2 Matrix
    1st Row 1st Column negative 2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative
    2 EndMatrix right double arrow Start 2 By 2 Matrix 1st Row 1st Column negative
    2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative 2 EndMatrix StartBinomialOrMatrix
    1 Choose 1 EndBinomialOrMatrix equals StartBinomialOrMatrix 0 Choose 0 EndBinomialOrMatrix"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>-</mo> <mn>3</mn></mrow></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>-</mo> <mn>3</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>â‡’</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 2 By 2 Matrix 1st Row 1st Column 1 minus 3 2nd Column 2
    2nd Row 1st Column 2 2nd Column 1 minus 3 EndMatrix equals Start 2 By 2 Matrix
    1st Row 1st Column negative 2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative
    2 EndMatrix right double arrow Start 2 By 2 Matrix 1st Row 1st Column negative
    2 2nd Column 2 2nd Row 1st Column 2 2nd Column negative 2 EndMatrix StartBinomialOrMatrix
    1 Choose 1 EndBinomialOrMatrix equals StartBinomialOrMatrix 0 Choose 0 EndBinomialOrMatrix"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mrow><mn>1</mn>
    <mo>-</mo> <mn>3</mn></mrow></mtd> <mtd><mn>2</mn></mtd></mtr> <mtr><mtd><mn>2</mn></mtd>
    <mtd><mrow><mn>1</mn> <mo>-</mo> <mn>3</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mo>â‡’</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd>
    <mtd><mrow><mphantom><mo>-</mo></mphantom> <mn>2</mn></mrow></mtd></mtr> <mtr><mtd><mrow><mphantom><mo>-</mo></mphantom>
    <mn>2</mn></mrow></mtd> <mtd><mrow><mo>-</mo> <mn>2</mn></mrow></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd></mtr> <mtr><mtd><mn>1</mn></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>0</mn></mtd></mtr>
    <mtr><mtd><mn>0</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: This means that [1 1] is an eigenvector of the matrix associated with an eigenvalue
    of 3.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ„å‘³ç€[1 1]æ˜¯ä¸ç‰¹å¾å€¼ä¸º3çš„çŸ©é˜µç›¸å…³è”çš„ç‰¹å¾å‘é‡ã€‚
- en: I found that null space vector just by looking at the matrix. How are the null
    space vectors (that is, the eigenvectors of the matrix) actually identified in
    practice?
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åªæ˜¯é€šè¿‡æŸ¥çœ‹çŸ©é˜µæ‰¾åˆ°äº†é›¶ç©ºé—´å‘é‡ã€‚åœ¨å®è·µä¸­å¦‚ä½•è¯†åˆ«é›¶ç©ºé—´å‘é‡ï¼ˆå³çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼‰ï¼Ÿ
- en: Null space vectors can be found by using Gauss-Jordan to solve a system of equations,
    where the coefficients matrix is the <math alttext="lamda"><mi>Î»</mi></math> -shifted
    matrix and the constants vector is the zeros vector. Thatâ€™s a good way to conceptualize
    the solution. In implementation, more stable numerical methods are applied for
    finding eigenvalues and eigenvectors, including QR decomposition and a procedure
    called the power method.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡ä½¿ç”¨é«˜æ–¯-çº¦æ—¦æ–¹æ³•è§£æ–¹ç¨‹ç»„å¯ä»¥æ‰¾åˆ°é›¶ç©ºé—´å‘é‡ï¼Œå…¶ä¸­ç³»æ•°çŸ©é˜µæ˜¯<math alttext="lamda"><mi>Î»</mi></math>-ç§»ä½çŸ©é˜µï¼Œå¸¸æ•°å‘é‡æ˜¯é›¶å‘é‡ã€‚è¿™æ˜¯ä¸€ç§æ¦‚å¿µåŒ–è§£å†³æ–¹æ¡ˆçš„å¥½æ–¹æ³•ã€‚åœ¨å®æ–½ä¸­ï¼Œä¼šåº”ç”¨æ›´ç¨³å®šçš„æ•°å€¼æ–¹æ³•æ¥æ‰¾åˆ°ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼ŒåŒ…æ‹¬QRåˆ†è§£å’Œä¸€ç§ç§°ä¸ºå¹‚æ³•çš„è¿‡ç¨‹ã€‚
- en: Sign and Scale Indeterminacy of Eigenvectors
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç‰¹å¾å‘é‡çš„ç¬¦å·å’Œç¼©æ”¾ä¸ç¡®å®šæ€§
- en: Let me return to the numerical example in the previous section. I wrote that
    [1 1] was an eigenvector of the matrix because that vector is a basis for the
    null space of the matrix shifted by its eigenvalue of 3.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘å›åˆ°å‰ä¸€èŠ‚ä¸­çš„æ•°å€¼ç¤ºä¾‹ã€‚æˆ‘å†™é“[1 1]æ˜¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œå› ä¸ºè¯¥å‘é‡æ˜¯çŸ©é˜µç§»ä½åçš„é›¶ç©ºé—´çš„åŸºç¡€ã€‚
- en: 'Look back at the shifted matrix and ask yourself, is [1 1] the only possible
    basis vector for the null space? Not even close! You could also use [4 4] or [âˆ’5.4
    âˆ’5.4] orâ€¦I think you see where this is going: *any* scaled version of vector [1
    1] is a basis for that null space. In other words, if <math alttext="bold v"><mi>ğ¯</mi></math>
    is an eigenvector of a matrix, then so is <math alttext="alpha bold v"><mrow><mi>Î±</mi>
    <mi>ğ¯</mi></mrow></math> for any real-valued <math alttext="alpha"><mi>Î±</mi></math>
    except zero.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: å›é¡¾ä¸€ä¸‹ç§»ä½åçš„çŸ©é˜µï¼Œå¹¶é—®è‡ªå·±ï¼Œ[1 1]æ˜¯é›¶ç©ºé—´çš„å”¯ä¸€å¯èƒ½çš„åŸºå‘é‡å—ï¼Ÿä¸€ç‚¹ä¹Ÿä¸ï¼ä½ è¿˜å¯ä»¥ä½¿ç”¨[4 4]æˆ–[-5.4 -5.4]æˆ–â€¦â€¦æˆ‘æƒ³ä½ æ˜ç™½è¿™æ˜¯ä»€ä¹ˆæ„æ€ï¼šå‘é‡[1
    1]çš„*ä»»ä½•*ç¼©æ”¾ç‰ˆæœ¬éƒ½æ˜¯è¯¥é›¶ç©ºé—´çš„åŸºç¡€ã€‚æ¢å¥è¯è¯´ï¼Œå¦‚æœ<math alttext="bold v"><mi>ğ¯</mi></math>æ˜¯çŸ©é˜µçš„ç‰¹å¾å‘é‡ï¼Œé‚£ä¹ˆå¯¹äºä»»ä½•å®æ•°å€¼<math
    alttext="alpha"><mi>Î±</mi></math>ï¼ˆé™¤äº†é›¶ï¼‰ï¼Œ<math alttext="alpha bold v"><mrow><mi>Î±</mi>
    <mi>ğ¯</mi></mrow></math>ä¹Ÿæ˜¯ã€‚
- en: Indeed, eigenvectors are important because of their *direction*, not because
    of their *magnitude*.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: çš„ç¡®ï¼Œç‰¹å¾å‘é‡ä¹‹æ‰€ä»¥é‡è¦æ˜¯å› ä¸ºå®ƒä»¬çš„*æ–¹å‘*ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„*å¤§å°*ã€‚
- en: 'The infinity of possible null space basis vectors leads to two questions:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: æ— é™å¤šçš„é›¶ç©ºé—´åŸºå‘é‡å¼•å‘äº†ä¸¤ä¸ªé—®é¢˜ï¼š
- en: '*Is there one â€œbestâ€ basis vector?* There isnâ€™t a â€œbestâ€ basis vector per se,
    but it is convenient to have eigenvectors that are unit normalized (a Euclidean
    norm of 1). This is particularly useful for symmetric matrices for reasons that
    will be explained later in this chapter.^([4](ch13.xhtml#idm45733293126624))'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*æœ‰ä¸€ä¸ªâ€œæœ€ä½³â€çš„åŸºå‘é‡å—ï¼Ÿ* å®é™…ä¸Šæ²¡æœ‰ä¸€ä¸ªâ€œæœ€ä½³â€çš„åŸºå‘é‡ï¼Œä½†å¯¹äºå¯¹ç§°çŸ©é˜µæ¥è¯´ï¼Œæ‹¥æœ‰å•ä½è§„èŒƒåŒ–ï¼ˆæ¬§å‡ é‡Œå¾—èŒƒæ•°ä¸º1ï¼‰çš„ç‰¹å¾å‘é‡æ˜¯æ–¹ä¾¿çš„ï¼Œè¿™å°†åœ¨æœ¬ç« åé¢çš„å†…å®¹ä¸­è§£é‡Šã€‚^([4](ch13.xhtml#idm45733293126624))'
- en: '*What is the â€œcorrectâ€ sign of an eigenvector?* There is none. In fact, you
    can get different eigenvector signs from the same matrix when using different
    versions of NumPyâ€”as well as different software such as MATLAB, Julia, or Mathematica.
    Eigenvector sign indeterminacy is just a feature of life in our universe. In applications
    such as PCA, there are principled ways for assigning a sign, but thatâ€™s just common
    convention to facilitate interpretation.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ç‰¹å¾å‘é‡çš„â€œæ­£ç¡®â€ç¬¦å·æ˜¯ä»€ä¹ˆï¼Ÿ* æ²¡æœ‰ç¡®å®šçš„ç¬¦å·ã€‚äº‹å®ä¸Šï¼Œå½“ä½¿ç”¨ä¸åŒç‰ˆæœ¬çš„NumPyä»¥åŠä¸åŒçš„è½¯ä»¶å¦‚MATLABã€Juliaæˆ–Mathematicaæ—¶ï¼Œå¯ä»¥ä»åŒä¸€ä¸ªçŸ©é˜µä¸­è·å¾—ä¸åŒçš„ç‰¹å¾å‘é‡ç¬¦å·ã€‚ç‰¹å¾å‘é‡ç¬¦å·çš„ä¸ç¡®å®šæ€§åªæ˜¯æˆ‘ä»¬å®‡å®™ç”Ÿæ´»çš„ä¸€ä¸ªç‰¹å¾ã€‚åœ¨è¯¸å¦‚PCAä¹‹ç±»çš„åº”ç”¨ä¸­ï¼Œæœ‰ä¸€äº›åŸåˆ™æ€§çš„æ–¹æ³•å¯ä»¥åˆ†é…ç¬¦å·ï¼Œä½†è¿™åªæ˜¯ä¿ƒè¿›è§£é‡Šçš„å¸¸è§çº¦å®šã€‚'
- en: Diagonalizing a Square Matrix
  id: totrans-85
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹è§’åŒ–ä¸€ä¸ªæ–¹é˜µ
- en: 'The eigenvalue equation that you are now familiar with lists one eigenvalue
    and one eigenvector. This means that an <math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>Ã—</mo> <mi>M</mi></mrow></math> matrix has *M* eigenvalue equations:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æ‚¨ç†Ÿæ‚‰çš„ç‰¹å¾å€¼æ–¹ç¨‹åˆ—å‡ºäº†ä¸€ä¸ªç‰¹å¾å€¼å’Œä¸€ä¸ªç‰¹å¾å‘é‡ã€‚è¿™æ„å‘³ç€ä¸€ä¸ª<math alttext="upper M times upper M"><mrow><mi>M</mi>
    <mo>Ã—</mo> <mi>M</mi></mrow></math>çŸ©é˜µæœ‰*M*ä¸ªç‰¹å¾å€¼æ–¹ç¨‹ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 1 2nd Column
    equals lamda 1 bold v 1 2nd Row 1st Column  ellipsis 3rd Row 1st Column bold upper
    A bold v Subscript upper M 2nd Column equals lamda Subscript upper M Baseline
    bold v Subscript upper M EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <msub><mi>ğ¯</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>â‹®</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <msub><mi>ğ¯</mi> <mi>M</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mi>M</mi></msub> <msub><mi>ğ¯</mi>
    <mi>M</mi></msub></mrow></mtd></mtr></mtable></math>
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 1 2nd Column
    equals lamda 1 bold v 1 2nd Row 1st Column  ellipsis 3rd Row 1st Column bold upper
    A bold v Subscript upper M 2nd Column equals lamda Subscript upper M Baseline
    bold v Subscript upper M EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <msub><mi>ğ¯</mi>
    <mn>1</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mo>â‹®</mo></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <msub><mi>ğ¯</mi> <mi>M</mi></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mi>M</mi></msub> <msub><mi>ğ¯</mi>
    <mi>M</mi></msub></mrow></mtd></mtr></mtable></math>
- en: 'Thereâ€™s nothing really wrong with that series of equations, but it is kind
    of ugly, and ugliness violates one of the principles of linear algebra: make equations
    compact and elegant. Therefore, we transform this series of equations into one
    matrix equation.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ç³»åˆ—æ–¹ç¨‹ç»„å…¶å®æ²¡ä»€ä¹ˆé”™ï¼Œä½†æœ‰ç‚¹ä¸‘é™‹ï¼Œè€Œä¸‘é™‹è¿åäº†çº¿æ€§ä»£æ•°çš„ä¸€ä¸ªåŸåˆ™ï¼šä½¿æ–¹ç¨‹ç»„ç®€æ´è€Œä¼˜é›…ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å°†è¿™ç³»åˆ—æ–¹ç¨‹è½¬æ¢æˆä¸€ä¸ªçŸ©é˜µæ–¹ç¨‹ã€‚
- en: The key insight for writing out the matrix eigenvalue equation is that each
    column of the eigenvectors matrix is scaled by exactly one eigenvalue. We can
    implement this through postmultiplication by a diagonal matrix (as you learned
    in [ChapterÂ 6](ch06.xhtml#Chapter_6)).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å†™å‡ºçŸ©é˜µç‰¹å¾å€¼æ–¹ç¨‹çš„å…³é”®æ´è§åœ¨äºï¼Œç‰¹å¾å‘é‡çŸ©é˜µçš„æ¯ä¸€åˆ—éƒ½è¢«æ°å¥½ä¸€ä¸ªç‰¹å¾å€¼æ‰€ç¼©æ”¾ã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡åä¹˜å¯¹è§’çŸ©é˜µæ¥å®ç°è¿™ä¸€ç‚¹ï¼ˆæ­£å¦‚æ‚¨åœ¨[ç¬¬6ç« ](ch06.xhtml#Chapter_6)ä¸­å­¦åˆ°çš„ï¼‰ã€‚
- en: 'So instead of storing the eigenvalues in a vector, we store the eigenvalues
    in the diagonal of a matrix. The following equation shows the form of diagonalization
    for a <math alttext="3 times 3"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>3</mn></mrow></math>
    matrix (using @ in place of numerical values in the matrix). In the eigenvectors
    matrix, the first subscript number corresponds to the eigenvector, and the second
    subscript number corresponds to the eigenvector element. For example, <math alttext="v
    12"><msub><mi>v</mi> <mn>12</mn></msub></math> is the second element of the first
    eigenvector:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å› æ­¤ï¼Œæˆ‘ä»¬ä¸æ˜¯å°†ç‰¹å¾å€¼å­˜å‚¨åœ¨å‘é‡ä¸­ï¼Œè€Œæ˜¯å°†ç‰¹å¾å€¼å­˜å‚¨åœ¨çŸ©é˜µçš„å¯¹è§’çº¿ä¸Šã€‚ä»¥ä¸‹æ–¹ç¨‹æ˜¾ç¤ºäº†å¯¹äºä¸€ä¸ª <math alttext="3 times 3"><mrow><mn>3</mn>
    <mo>Ã—</mo> <mn>3</mn></mrow></math> çŸ©é˜µçš„å¯¹è§’åŒ–å½¢å¼ï¼ˆåœ¨çŸ©é˜µä¸­ä½¿ç”¨ @ æ›¿æ¢æ•°å€¼ï¼‰ã€‚åœ¨ç‰¹å¾å‘é‡çŸ©é˜µä¸­ï¼Œç¬¬ä¸€ä¸ªä¸‹æ ‡æ•°å­—å¯¹åº”äºç‰¹å¾å‘é‡ï¼Œç¬¬äºŒä¸ªä¸‹æ ‡æ•°å­—å¯¹åº”äºç‰¹å¾å‘é‡å…ƒç´ ã€‚ä¾‹å¦‚ï¼Œ<math
    alttext="v 12"><msub><mi>v</mi> <mn>12</mn></msub></math> æ˜¯ç¬¬ä¸€ä¸ªç‰¹å¾å‘é‡çš„ç¬¬äºŒä¸ªå…ƒç´ ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column Start 3 By 3 Matrix 1st Row 1st
    Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 2nd Row
    1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 3rd
    Row 1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at
    EndMatrix Start 3 By 3 Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column
    v 31 2nd Row 1st Column v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column
    v 13 2nd Column v 23 3rd Column v 33 EndMatrix 2nd Column equals Start 3 By 3
    Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column v 31 2nd Row 1st Column
    v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column v 13 2nd Column v 23 3rd
    Column v 33 EndMatrix Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column
    0 3rd Column 0 2nd Row 1st Column 0 2nd Column lamda 2 3rd Column 0 3rd Row 1st
    Column 0 2nd Column 0 3rd Column lamda 3 EndMatrix 2nd Row 1st Column Blank 2nd
    Column equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 v 11 2nd Column lamda
    2 v 21 3rd Column lamda 3 v 31 2nd Row 1st Column lamda 1 v 12 2nd Column lamda
    2 v 22 3rd Column lamda 3 v 32 3rd Row 1st Column lamda 1 v 13 2nd Column lamda
    2 v 23 3rd Column lamda 3 v 33 EndMatrix EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>Î»</mi>
    <mn>1</mn></msub></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>Î»</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><msub><mi>Î»</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>31</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>12</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>22</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>32</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>13</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>23</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>33</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column Start 3 By 3 Matrix 1st Row 1st
    Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 2nd Row
    1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at 3rd
    Row 1st Column commercial-at 2nd Column commercial-at 3rd Column commercial-at
    EndMatrix Start 3 By 3 Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column
    v 31 2nd Row 1st Column v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column
    v 13 2nd Column v 23 3rd Column v 33 EndMatrix 2nd Column equals Start 3 By 3
    Matrix 1st Row 1st Column v 11 2nd Column v 21 3rd Column v 31 2nd Row 1st Column
    v 12 2nd Column v 22 3rd Column v 32 3rd Row 1st Column v 13 2nd Column v 23 3rd
    Column v 33 EndMatrix Start 3 By 3 Matrix 1st Row 1st Column lamda 1 2nd Column
    0 3rd Column 0 2nd Row 1st Column 0 2nd Column lamda 2 3rd Column 0 3rd Row 1st
    Column 0 2nd Column 0 3rd Column lamda 3 EndMatrix 2nd Row 1st Column Blank 2nd
    Column equals Start 3 By 3 Matrix 1st Row 1st Column lamda 1 v 11 2nd Column lamda
    2 v 21 3rd Column lamda 3 v 31 2nd Row 1st Column lamda 1 v 12 2nd Column lamda
    2 v 22 3rd Column lamda 3 v 32 3rd Row 1st Column lamda 1 v 13 2nd Column lamda
    2 v 23 3rd Column lamda 3 v 33 EndMatrix EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd
    columnalign="right"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd>
    <mtd><mo>@</mo></mtd></mtr> <mtr><mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd> <mtd><mo>@</mo></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>v</mi> <mn>11</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>21</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>31</mn></msub></mtd></mtr>
    <mtr><mtd><msub><mi>v</mi> <mn>12</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>22</mn></msub></mtd>
    <mtd><msub><mi>v</mi> <mn>32</mn></msub></mtd></mtr> <mtr><mtd><msub><mi>v</mi>
    <mn>13</mn></msub></mtd> <mtd><msub><mi>v</mi> <mn>23</mn></msub></mtd> <mtd><msub><mi>v</mi>
    <mn>33</mn></msub></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><msub><mi>Î»</mi>
    <mn>1</mn></msub></mtd> <mtd><mn>0</mn></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><msub><mi>Î»</mi> <mn>2</mn></msub></mtd> <mtd><mn>0</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd>
    <mtd><mn>0</mn></mtd> <mtd><msub><mi>Î»</mi> <mn>3</mn></msub></mtd></mtr></mtable></mfenced></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>11</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>21</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>31</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>12</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>22</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>32</mn></msub></mrow></mtd></mtr> <mtr><mtd><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msub><mi>v</mi> <mn>13</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>2</mn></msub> <msub><mi>v</mi> <mn>23</mn></msub></mrow></mtd> <mtd><mrow><msub><mi>Î»</mi>
    <mn>3</mn></msub> <msub><mi>v</mi> <mn>33</mn></msub></mrow></mtd></mtr></mtable></mfenced></mrow></mtd></mtr></mtable></math>
- en: Please take a moment to confirm that each eigenvalue scales all elements of
    its corresponding eigenvector, and not any other eigenvectors.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·èŠ±ç‚¹æ—¶é—´ç¡®è®¤æ¯ä¸ªç‰¹å¾å€¼å¦‚ä½•ç¼©æ”¾å…¶å¯¹åº”çš„ç‰¹å¾å‘é‡çš„æ‰€æœ‰å…ƒç´ ï¼Œè€Œä¸æ˜¯å…¶ä»–ä»»ä½•ç‰¹å¾å‘é‡ã€‚
- en: 'More generally, the matrix eigenvalue equationâ€”a.k.a. the diagonalization of
    a square matrixâ€”is:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´ä¸€èˆ¬åœ°è¯´ï¼ŒçŸ©é˜µç‰¹å¾å€¼æ–¹ç¨‹ï¼ˆå³å¯¹æ–¹é˜µçš„å¯¹è§’åŒ–ï¼‰æ˜¯ï¼š
- en: <math alttext="bold upper A bold upper V equals bold upper V bold upper Lamda"
    display="block"><mrow><mi mathvariant="bold">A</mi> <mi mathvariant="bold">V</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi></mrow></math>
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold upper V equals bold upper V bold upper Lamda"
    display="block"><mrow><mi mathvariant="bold">A</mi> <mi mathvariant="bold">V</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi></mrow></math>
- en: 'NumPyâ€™s `eig` function returns eigenvectors in a matrix and eigenvalues in
    a vector. This means that diagonalizing a matrix in NumPy requires a bit of extra
    code:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: NumPy çš„ `eig` å‡½æ•°è¿”å›ä¸€ä¸ªçŸ©é˜µä¸­çš„ç‰¹å¾å‘é‡å’Œä¸€ä¸ªå‘é‡ä¸­çš„ç‰¹å¾å€¼ã€‚è¿™æ„å‘³ç€åœ¨ NumPy ä¸­å¯¹çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–éœ€è¦é¢å¤–çš„ä»£ç ï¼š
- en: '[PRE2]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By the way, itâ€™s often interesting and insightful in mathematics to rearrange
    equations by solving for different variables. Consider the following list of equivalent
    declarations:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: é¡ºä¾¿è¯´ä¸€å¥ï¼Œåœ¨æ•°å­¦ä¸­é€šè¿‡è§£ä¸åŒå˜é‡æ¥é‡æ–°æ’åˆ—æ–¹ç¨‹é€šå¸¸å¾ˆæœ‰è¶£å’Œæ·±åˆ»ã€‚è€ƒè™‘ä»¥ä¸‹ç­‰ä»·å£°æ˜åˆ—è¡¨ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper V 2nd
    Column equals bold upper V bold upper Lamda 2nd Row 1st Column bold upper A 2nd
    Column equals bold upper V bold upper Lamda bold upper V Superscript negative
    1 Baseline 3rd Row 1st Column bold upper Lamda 2nd Column equals bold upper V
    Superscript negative 1 Baseline bold upper A bold upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">A</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi> <msup><mi mathvariant="bold">V</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold upper V 2nd
    Column equals bold upper V bold upper Lamda 2nd Row 1st Column bold upper A 2nd
    Column equals bold upper V bold upper Lamda bold upper V Superscript negative
    1 Baseline 3rd Row 1st Column bold upper Lamda 2nd Column equals bold upper V
    Superscript negative 1 Baseline bold upper A bold upper V EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mi mathvariant="bold">A</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi mathvariant="bold">V</mi> <mi mathvariant="bold">Î›</mi> <msup><mi mathvariant="bold">V</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mi
    mathvariant="bold">Î›</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">A</mi>
    <mi mathvariant="bold">V</mi></mrow></mtd></mtr></mtable></math>
- en: 'The second equation shows that matrix <math alttext="bold upper A"><mi>ğ€</mi></math>
    becomes diagonal inside the space of <math alttext="bold upper V"><mi>ğ•</mi></math>
    (that is, <math alttext="bold upper V"><mi>ğ•</mi></math> moves us into the â€œdiagonal
    space,â€ and then <math alttext="bold upper V Superscript negative 1"><msup><mi>ğ•</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> gets us back out of the diagonal
    space). This can be interpreted in the context of basis vectors: the matrix <math
    alttext="bold upper A"><mi>ğ€</mi></math> is dense in the standard basis, but then
    we apply a set of transformations ( <math alttext="bold upper V"><mi>ğ•</mi></math>
    ) to rotate the matrix into a new set of basis vectors (the eigenvectors) in which
    the information is sparse and represented by a diagonal matrix. (At the end of
    the equation, we need to get back into the standard basis space, hence the <math
    alttext="bold upper V Superscript negative 1"><msup><mi>ğ•</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    .)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒä¸ªæ–¹ç¨‹è¡¨æ˜çŸ©é˜µ <math alttext="bold upper A"><mi>ğ€</mi></math> åœ¨ <math alttext="bold
    upper V"><mi>ğ•</mi></math> çš„ç©ºé—´å†…å˜ä¸ºå¯¹è§’åŒ–ï¼ˆä¹Ÿå°±æ˜¯è¯´ï¼Œ<math alttext="bold upper V"><mi>ğ•</mi></math>
    å°†æˆ‘ä»¬ç§»å…¥â€œå¯¹è§’ç©ºé—´â€ï¼Œç„¶å <math alttext="bold upper V Superscript negative 1"><msup><mi>ğ•</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> å°†æˆ‘ä»¬å¸¦å›æ ‡å‡†ç©ºé—´ï¼‰ã€‚è¿™å¯ä»¥åœ¨åŸºå‘é‡çš„èƒŒæ™¯ä¸‹è§£é‡Šï¼šçŸ©é˜µ <math
    alttext="bold upper A"><mi>ğ€</mi></math> åœ¨æ ‡å‡†åŸºä¸­æ˜¯å¯†é›†çš„ï¼Œä½†ç„¶åæˆ‘ä»¬åº”ç”¨ä¸€ç»„å˜æ¢ï¼ˆ <math alttext="bold
    upper V"><mi>ğ•</mi></math> ï¼‰å°†çŸ©é˜µæ—‹è½¬åˆ°æ–°çš„åŸºå‘é‡é›†åˆï¼ˆç‰¹å¾å‘é‡ï¼‰ï¼Œåœ¨è¿™äº›åŸºå‘é‡ä¸­ä¿¡æ¯æ˜¯ç¨€ç–çš„ï¼Œå¹¶ç”±å¯¹è§’çŸ©é˜µè¡¨ç¤ºã€‚ï¼ˆåœ¨æ–¹ç¨‹æœ«å°¾ï¼Œæˆ‘ä»¬éœ€è¦å›åˆ°æ ‡å‡†åŸºç©ºé—´ï¼Œå› æ­¤éœ€è¦
    <math alttext="bold upper V Superscript negative 1"><msup><mi>ğ•</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    ã€‚ï¼‰
- en: The Special Awesomeness of Symmetric Matrices
  id: totrans-100
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µçš„ç‰¹æ®Šä¼˜è¶Šæ€§
- en: You already know from earlier chapters that symmetric matrices have special
    properties that make them great to work with. Now you are ready to learn two more
    special properties that relate to eigendecomposition.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å·²ç»ä»å‰å‡ ç« äº†è§£åˆ°ï¼Œå¯¹ç§°çŸ©é˜µå…·æœ‰ä½¿å®ƒä»¬æ˜“äºå¤„ç†çš„ç‰¹æ®Šå±æ€§ã€‚ç°åœ¨ï¼Œä½ å‡†å¤‡å­¦ä¹ ä¸ç‰¹å¾åˆ†è§£ç›¸å…³çš„å¦å¤–ä¸¤ä¸ªç‰¹æ®Šå±æ€§ã€‚
- en: Orthogonal Eigenvectors
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ­£äº¤ç‰¹å¾å‘é‡
- en: 'Symmetric matrices have orthogonal eigenvectors. That means that all eigenvectors
    of a symmetric matrix are pair-wise orthogonal. Let me start with an example,
    then Iâ€™ll discuss the implications of eigenvector orthogonality, and finally Iâ€™ll
    show the proof:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µå…·æœ‰æ­£äº¤ç‰¹å¾å‘é‡ã€‚è¿™æ„å‘³ç€å¯¹ç§°çŸ©é˜µçš„æ‰€æœ‰ç‰¹å¾å‘é‡éƒ½æ˜¯æˆå¯¹æ­£äº¤çš„ã€‚è®©æˆ‘ä»ä¸€ä¸ªä¾‹å­å¼€å§‹ï¼Œç„¶åæˆ‘å°†è®¨è®ºç‰¹å¾å‘é‡æ­£äº¤çš„å«ä¹‰ï¼Œæœ€åæˆ‘å°†å±•ç¤ºè¯æ˜ï¼š
- en: '[PRE3]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The three dot products are all zero (within computer rounding errors on the
    order of 10^(âˆ’16). (Notice that Iâ€™ve created symmetric matrices as a random matrix
    times its transpose.)
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‰ä¸ªç‚¹ç§¯éƒ½ä¸ºé›¶ï¼ˆåœ¨è®¡ç®—æœºèˆå…¥è¯¯å·®ä¸º 10^(âˆ’16) çš„æ•°é‡çº§å†…ï¼‰ã€‚ ï¼ˆæ³¨æ„ï¼Œæˆ‘å·²ç»åˆ›å»ºäº†å¯¹ç§°çŸ©é˜µä½œä¸ºä¸€ä¸ªéšæœºçŸ©é˜µä¹˜ä»¥å…¶è½¬ç½®ã€‚ï¼‰
- en: The orthogonal eigenvector property means that the dot product between any pair
    of eigenvectors is zero, while the dot product of an eigenvector with itself is
    nonzero (because we do not consider the zeros vector to be an eigenvector). This
    can be written as <math alttext="bold upper V Superscript upper T Baseline bold
    upper V equals bold upper D"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <mi>ğ•</mi>
    <mo>=</mo> <mi>ğƒ</mi></mrow></math> , where <math alttext="bold upper D"><mi>ğƒ</mi></math>
    is a diagonal matrix with the diagonals containing the norms of the eigenvectors.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: æ­£äº¤ç‰¹å¾å‘é‡å±æ€§æ„å‘³ç€ä»»æ„ä¸€å¯¹ç‰¹å¾å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ä¸ºé›¶ï¼Œè€Œç‰¹å¾å‘é‡ä¸è‡ªèº«çš„ç‚¹ç§¯ä¸ä¸ºé›¶ï¼ˆå› ä¸ºæˆ‘ä»¬ä¸è®¤ä¸ºé›¶å‘é‡æ˜¯ç‰¹å¾å‘é‡ï¼‰ã€‚è¿™å¯ä»¥å†™æˆ <math alttext="bold
    upper V Superscript upper T Baseline bold upper V equals bold upper D"><mrow><msup><mi>ğ•</mi>
    <mtext>T</mtext></msup> <mi>ğ•</mi> <mo>=</mo> <mi>ğƒ</mi></mrow></math>ï¼Œå…¶ä¸­ <math
    alttext="bold upper D"><mi>ğƒ</mi></math> æ˜¯ä¸€ä¸ªå¯¹è§’çŸ©é˜µï¼Œå¯¹è§’çº¿åŒ…å«ç‰¹å¾å‘é‡çš„èŒƒæ•°ã€‚
- en: 'But we can do even better than just a diagonal matrix: recall that eigenvectors
    are important because of their *direction*, not because of their *magnitude*.
    So an eigenvector can have any magnitude we want (except, obviously, for a magnitude
    of zero).'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ä½†æˆ‘ä»¬å¯ä»¥æ¯”å¯¹è§’çŸ©é˜µåšå¾—æ›´å¥½ï¼šè®°ä½ç‰¹å¾å‘é‡ä¹‹æ‰€ä»¥é‡è¦æ˜¯å› ä¸ºå®ƒä»¬çš„*æ–¹å‘*ï¼Œè€Œä¸æ˜¯å®ƒä»¬çš„*å¤§å°*ã€‚å› æ­¤ï¼Œä¸€ä¸ªç‰¹å¾å‘é‡å¯ä»¥æœ‰ä»»ä½•æˆ‘ä»¬æƒ³è¦çš„å¤§å°ï¼ˆæ˜¾ç„¶ä¸åŒ…æ‹¬å¤§å°ä¸ºé›¶çš„æƒ…å†µï¼‰ã€‚
- en: 'Letâ€™s scale all eigenvectors so they have unit length. Question for you: if
    all eigenvectors are orthogonal and have unit length, what happens when we multiply
    the eigenvectors matrix by its transpose?'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å°†æ‰€æœ‰ç‰¹å¾å‘é‡ç¼©æ”¾åˆ°å•ä½é•¿åº¦ã€‚å¯¹ä½ çš„é—®é¢˜ï¼šå¦‚æœæ‰€æœ‰ç‰¹å¾å‘é‡éƒ½æ˜¯æ­£äº¤çš„ï¼Œå¹¶ä¸”å…·æœ‰å•ä½é•¿åº¦ï¼Œé‚£ä¹ˆå½“æˆ‘ä»¬å°†ç‰¹å¾å‘é‡çŸ©é˜µä¹˜ä»¥å®ƒçš„è½¬ç½®æ—¶ä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ
- en: 'Of course you know the answer:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ç„¶ä½ çŸ¥é“ç­”æ¡ˆï¼š
- en: <math alttext="bold upper V Superscript upper T Baseline bold upper V equals
    bold upper I" display="block"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <mi>ğ•</mi>
    <mo>=</mo> <mi>ğˆ</mi></mrow></math>
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper V Superscript upper T Baseline bold upper V equals
    bold upper I" display="block"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <mi>ğ•</mi>
    <mo>=</mo> <mi>ğˆ</mi></mrow></math>
- en: In other words, the eigenvectors matrix of a symmetric matrix is an orthogonal
    matrix! This has multiple implications for data science, including that the eigenvectors
    are super easy to invert (because you simply transpose them). There are other
    implications of orthogonal eigenvectors for applications such as principal components
    analysis, which I will discuss in [ChapterÂ 15](ch15.xhtml#Chapter_15).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œå¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡çŸ©é˜µæ˜¯ä¸€ä¸ªæ­£äº¤çŸ©é˜µï¼è¿™å¯¹æ•°æ®ç§‘å­¦æœ‰å¤šé‡å½±å“ï¼ŒåŒ…æ‹¬ç‰¹å¾å‘é‡éå¸¸å®¹æ˜“æ±‚é€†ï¼ˆå› ä¸ºä½ åªéœ€è½¬ç½®å®ƒä»¬ï¼‰ã€‚æ­£äº¤ç‰¹å¾å‘é‡è¿˜æœ‰å…¶ä»–åº”ç”¨ï¼Œæ¯”å¦‚ä¸»æˆåˆ†åˆ†æï¼Œæˆ‘å°†åœ¨[ç¬¬15ç« ](ch15.xhtml#Chapter_15)ä¸­è®¨è®ºã€‚
- en: I wrote in [ChapterÂ 1](ch01.xhtml#Chapter_1) that there are relatively few proofs
    in this book. But orthogonal eigenvectors of symmetric matrices is such an important
    concept that you really need to see this claim proven.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨[ç¬¬1ç« ](ch01.xhtml#Chapter_1)ä¸­å†™é“ï¼Œæœ¬ä¹¦ä¸­çš„è¯æ˜ç›¸å¯¹è¾ƒå°‘ã€‚ä½†å¯¹ç§°çŸ©é˜µçš„æ­£äº¤ç‰¹å¾å‘é‡æ˜¯å¦‚æ­¤é‡è¦çš„æ¦‚å¿µï¼Œä½ çœŸçš„éœ€è¦çœ‹åˆ°è¿™ä¸ªä¸»å¼ çš„è¯æ˜ã€‚
- en: 'The goal of this proof is to show that the dot product between any pair of
    eigenvectors is zero. We start from two assumptions: (1) matrix <math alttext="bold
    upper A"><mi>ğ€</mi></math> is symmetric, and (2) <math alttext="lamda 1"><msub><mi>Î»</mi>
    <mn>1</mn></msub></math> and <math alttext="lamda 2"><msub><mi>Î»</mi> <mn>2</mn></msub></math>
    are distinct eigenvalues of <math alttext="bold upper A"><mi>ğ€</mi></math> (*distinct*
    meaning they cannot equal each other), with <math alttext="bold v 1"><msub><mi>ğ¯</mi>
    <mn>1</mn></msub></math> and <math alttext="bold v 2"><msub><mi>ğ¯</mi> <mn>2</mn></msub></math>
    as their corresponding eigenvectors. Try to follow each equality step from left
    to right of [Equation 13-3](#eigen-ortho).'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªè¯æ˜çš„ç›®æ ‡æ˜¯å±•ç¤ºä»»æ„ä¸€å¯¹ç‰¹å¾å‘é‡ä¹‹é—´çš„ç‚¹ç§¯ä¸ºé›¶ã€‚æˆ‘ä»¬ä»ä¸¤ä¸ªå‡è®¾å¼€å§‹ï¼šï¼ˆ1ï¼‰çŸ©é˜µ <math alttext="bold upper A"><mi>ğ€</mi></math>
    æ˜¯å¯¹ç§°çš„ï¼Œï¼ˆ2ï¼‰<math alttext="lamda 1"><msub><mi>Î»</mi> <mn>1</mn></msub></math> å’Œ <math
    alttext="lamda 2"><msub><mi>Î»</mi> <mn>2</mn></msub></math> æ˜¯çŸ©é˜µ <math alttext="bold
    upper A"><mi>ğ€</mi></math> çš„ä¸åŒç‰¹å¾å€¼ï¼ˆ*ä¸åŒ*æ„å‘³ç€å®ƒä»¬ä¸ç›¸ç­‰ï¼‰ï¼Œå…¶å¯¹åº”çš„ç‰¹å¾å‘é‡ä¸º <math alttext="bold
    v 1"><msub><mi>ğ¯</mi> <mn>1</mn></msub></math> å’Œ <math alttext="bold v 2"><msub><mi>ğ¯</mi>
    <mn>2</mn></msub></math>ã€‚å°è¯•æŒ‰ç…§[æ–¹ç¨‹å¼13-3](#eigen-ortho)ä»å·¦åˆ°å³çš„æ¯ä¸ªç­‰å¼æ­¥éª¤è¿›è¡Œè·Ÿè¸ªã€‚
- en: Equation 13-3\. Proof of eigenvector orthogonality for symmetric matrices
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼13-3\. å¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡æ­£äº¤æ€§è¯æ˜
- en: <math alttext="lamda 1 bold v 1 Superscript upper T Baseline bold v 2 equals
    left-parenthesis bold upper A bold v 1 right-parenthesis Superscript upper T Baseline
    bold v 2 equals bold v 1 Superscript upper T Baseline bold upper A Superscript
    upper T Baseline bold v 2 equals bold v 1 Superscript upper T Baseline lamda 2
    bold v 2 equals lamda 2 bold v 1 Superscript upper T Baseline bold v 2" display="block"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msup><mrow><mo>(</mo><mi>ğ€</mi><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup> <msub><mi>ğ¯</mi> <mn>2</mn></msub>
    <mo>=</mo> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msup><mi>ğ€</mi>
    <mtext>T</mtext></msup> <msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>=</mo> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>Î»</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math>
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="lamda 1 bold v 1 Superscript upper T Baseline bold v 2 equals
    left-parenthesis bold upper A bold v 1 right-parenthesis Superscript upper T Baseline
    bold v 2 equals bold v 1 Superscript upper T Baseline bold upper A Superscript
    upper T Baseline bold v 2 equals bold v 1 Superscript upper T Baseline lamda 2
    bold v 2 equals lamda 2 bold v 1 Superscript upper T Baseline bold v 2" display="block"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msup><mrow><mo>(</mo><mi>ğ€</mi><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>)</mo></mrow> <mtext>T</mtext></msup> <msub><mi>ğ¯</mi> <mn>2</mn></msub>
    <mo>=</mo> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msup><mi>ğ€</mi>
    <mtext>T</mtext></msup> <msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>=</mo> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>Î»</mi> <mn>2</mn></msub> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math>
- en: The terms in the middle are just transformations; pay attention to the first
    and last terms. They are rewritten in [Equation 13-4](#eigen-ortho-2), and then
    subtracted to set to zero.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸­é—´çš„é¡¹åªæ˜¯å˜æ¢ï¼›æ³¨æ„ç¬¬ä¸€é¡¹å’Œæœ€åä¸€é¡¹ã€‚å®ƒä»¬åœ¨[æ–¹ç¨‹å¼13-4](#eigen-ortho-2)ä¸­è¢«é‡å†™ï¼Œç„¶åç›¸å‡ä»¥è®¾ä¸ºé›¶ã€‚
- en: Equation 13-4\. Continuing the eigenvector orthogonality proofâ€¦
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼13-4\. ç»§ç»­ç‰¹å¾å‘é‡æ­£äº¤æ€§è¯æ˜â€¦
- en: <math alttext="StartLayout 1st Row 1st Column lamda 1 bold v 1 Superscript upper
    T Baseline bold v 2 2nd Column equals lamda 2 bold v 1 Superscript upper T Baseline
    bold v 2 2nd Row 1st Column lamda 1 bold v 1 Superscript upper T Baseline bold
    v 2 minus lamda 2 bold v 1 Superscript upper T Baseline bold v 2 2nd Column equals
    0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi>
    <mn>2</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column lamda 1 bold v 1 Superscript upper
    T Baseline bold v 2 2nd Column equals lamda 2 bold v 1 Superscript upper T Baseline
    bold v 2 2nd Row 1st Column lamda 1 bold v 1 Superscript upper T Baseline bold
    v 2 minus lamda 2 bold v 1 Superscript upper T Baseline bold v 2 2nd Column equals
    0 EndLayout" display="block"><mtable displaystyle="true"><mtr><mtd columnalign="right"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi>
    <mn>2</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msub><mi>Î»</mi>
    <mn>1</mn></msub> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mn>0</mn></mrow></mtd></mtr></mtable></math>
- en: Both terms contain the dot product <math alttext="bold v 1 Superscript upper
    T Baseline bold v 2"><mrow><msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup>
    <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math> , which can be factored out.
    This brings us to the final part of the proof, which is shown in [Equation 13-5](#eigen-ortho-3).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸¤ä¸ªæœ¯è¯­éƒ½åŒ…å«ç‚¹ç§¯ <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math>
    ï¼Œè¿™å¯ä»¥è¢«åˆ†è§£å‡ºæ¥ã€‚è¿™å°†æˆ‘ä»¬å¸¦åˆ°è¯æ˜çš„æœ€åéƒ¨åˆ†ï¼Œæ˜¾ç¤ºåœ¨[æ–¹ç¨‹å¼ 13-5](#eigen-ortho-3)ä¸­ã€‚
- en: Equation 13-5\. Eigenvector orthogonality proof, part 3
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹å¼ 13-5ã€‚ç‰¹å¾å‘é‡æ­£äº¤æ€§è¯æ˜ï¼Œç¬¬ 3 éƒ¨åˆ†
- en: <math alttext="left-parenthesis lamda 1 minus lamda 2 right-parenthesis bold
    v 1 Superscript upper T Baseline bold v 2 equals 0" display="block"><mrow><mrow><mo>(</mo>
    <msub><mi>Î»</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math>
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="left-parenthesis lamda 1 minus lamda 2 right-parenthesis bold
    v 1 Superscript upper T Baseline bold v 2 equals 0" display="block"><mrow><mrow><mo>(</mo>
    <msub><mi>Î»</mi> <mn>1</mn></msub> <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub>
    <mo>)</mo></mrow> <msubsup><mi>ğ¯</mi> <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi>
    <mn>2</mn></msub> <mo>=</mo> <mn>0</mn></mrow></math>
- en: This final equation says that two quantities multiply to produce 0, which means
    that one or both of those quantities must be zero. <math alttext="left-parenthesis
    lamda 1 minus lamda 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>Î»</mi> <mn>1</mn></msub>
    <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> cannot
    equal zero because we began from the assumption that they are distinct. Therefore,
    <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math>
    must equal zero, which means that the two eigenvectors are orthogonal. Go back
    through the equations to convince yourself that this proof fails for nonsymmetric
    matrices, when <math alttext="bold upper A Superscript upper T Baseline not-equals
    bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup> <mo>â‰ </mo> <mi>ğ€</mi></mrow></math>
    . Thus, the eigenvectors of a nonsymmetric matrix are not constrained to be orthogonal
    (they will be linearly independent for all distinct eigenvalues, but I will omit
    that discussion and proof).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆæ–¹ç¨‹è¡¨æ˜ä¸¤ä¸ªé‡ç›¸ä¹˜å¾—åˆ°0ï¼Œè¿™æ„å‘³ç€è¿™ä¸¤ä¸ªé‡ä¸­çš„ä¸€ä¸ªæˆ–è€…ä¸¤ä¸ªå¿…é¡»ä¸ºé›¶ã€‚ <math alttext="left-parenthesis lamda 1
    minus lamda 2 right-parenthesis"><mrow><mo>(</mo> <msub><mi>Î»</mi> <mn>1</mn></msub>
    <mo>-</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>)</mo></mrow></math> ä¸èƒ½ç­‰äºé›¶ï¼Œå› ä¸ºæˆ‘ä»¬å‡è®¾å®ƒä»¬æ˜¯ä¸åŒçš„ã€‚å› æ­¤ï¼Œ
    <math alttext="bold v 1 Superscript upper T Baseline bold v 2"><mrow><msubsup><mi>ğ¯</mi>
    <mn>1</mn> <mtext>T</mtext></msubsup> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></math>
    å¿…é¡»ç­‰äºé›¶ï¼Œè¿™æ„å‘³ç€ä¸¤ä¸ªç‰¹å¾å‘é‡æ˜¯æ­£äº¤çš„ã€‚å›é¡¾æ–¹ç¨‹å¼ä»¥ç¡®ä¿¡è¿™ä¸ªè¯æ˜å¯¹äºéå¯¹ç§°çŸ©é˜µæ˜¯ä¸æˆç«‹çš„ï¼Œå½“ <math alttext="bold upper A Superscript
    upper T Baseline not-equals bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mo>â‰ </mo> <mi>ğ€</mi></mrow></math> ã€‚å› æ­¤ï¼Œéå¯¹ç§°çŸ©é˜µçš„ç‰¹å¾å‘é‡ä¸å—æ­£äº¤çº¦æŸï¼ˆå®ƒä»¬å¯¹äºæ‰€æœ‰ä¸åŒçš„ç‰¹å¾å€¼æ˜¯çº¿æ€§ç‹¬ç«‹çš„ï¼Œä½†æˆ‘å°†å¿½ç•¥é‚£éƒ¨åˆ†è®¨è®ºå’Œè¯æ˜ï¼‰ã€‚
- en: Real-Valued Eigenvalues
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: å®æ•°ç‰¹å¾å€¼
- en: A second special property of symmetric matrices is that they have real-valued
    eigenvalues (and therefore real-valued eigenvectors).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µçš„ç¬¬äºŒä¸ªç‰¹æ®Šæ€§è´¨æ˜¯å®ƒä»¬å…·æœ‰å®æ•°ç‰¹å¾å€¼ï¼ˆå› æ­¤å…·æœ‰å®æ•°ç‰¹å¾å‘é‡ï¼‰ã€‚
- en: 'Let me start by showing that matricesâ€”even with all real-valued entriesâ€”can
    have complex-valued eigenvalues:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘é¦–å…ˆå±•ç¤ºçŸ©é˜µâ€”â€”å³ä½¿æ‰€æœ‰æ¡ç›®éƒ½æ˜¯å®æ•°â€”â€”ä¹Ÿå¯ä»¥å…·æœ‰å¤æ•°ç‰¹å¾å€¼ï¼š
- en: '[PRE4]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: (Be careful with interpreting that NumPy array; it is not a <math alttext="3
    times 2"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>2</mn></mrow></math> *matrix*; it is
    a <math alttext="3 times 1"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>1</mn></mrow></math>
    *column vector* that contains complex numbers. Note the `j` and the absence of
    a comma between numbers.)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ï¼ˆåœ¨è§£é‡Šé‚£ä¸ª NumPy æ•°ç»„æ—¶è¦å°å¿ƒï¼›å®ƒä¸æ˜¯ä¸€ä¸ª <math alttext="3 times 2"><mrow><mn>3</mn> <mo>Ã—</mo>
    <mn>2</mn></mrow></math> *çŸ©é˜µ*ï¼›å®ƒæ˜¯ä¸€ä¸ª <math alttext="3 times 1"><mrow><mn>3</mn>
    <mo>Ã—</mo> <mn>1</mn></mrow></math> *åˆ—å‘é‡*ï¼ŒåŒ…å«å¤æ•°ã€‚æ³¨æ„ `j` å’Œæ•°å­—ä¹‹é—´æ²¡æœ‰é€—å·ã€‚ï¼‰
- en: The <math alttext="3 times 3"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>3</mn></mrow></math>
    matrix <math alttext="bold upper A"><mi>ğ€</mi></math> has two complex eigenvalues
    and one real-valued eigenvalue. The eigenvectors coupled to the complex-valued
    eigenvalues will themselves be complex-valued. There is nothing special about
    that particular matrix; I literally generated it from random integers between
    âˆ’3 and +3\. Interestingly, complex-valued solutions come in conjugate pairs. That
    means that if there is a <math alttext="lamda Subscript j"><msub><mi>Î»</mi> <mi>j</mi></msub></math>
    = a + *i*b, then there is a <math alttext="lamda Subscript k"><msub><mi>Î»</mi>
    <mi>k</mi></msub></math> = a âˆ’ *i*b. Their corresponding eigenvectors are also
    complex conjugate pairs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: çŸ©é˜µ <math alttext="3 times 3"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>3</mn></mrow></math>
    <math alttext="bold upper A"><mi>ğ€</mi></math> å…·æœ‰ä¸¤ä¸ªå¤æ•°ç‰¹å¾å€¼å’Œä¸€ä¸ªå®æ•°ç‰¹å¾å€¼ã€‚ä¸å¤æ•°ç‰¹å¾å€¼ç›¸å…³è”çš„ç‰¹å¾å‘é‡æœ¬èº«å°†æ˜¯å¤æ•°ã€‚é‚£ä¸ªç‰¹å®šçŸ©é˜µæ²¡æœ‰ä»€ä¹ˆç‰¹åˆ«ä¹‹å¤„ï¼›æˆ‘ä»ä»‹äº
    -3 å’Œ +3 ä¹‹é—´çš„éšæœºæ•´æ•°ç”Ÿæˆå®ƒã€‚æœ‰è¶£çš„æ˜¯ï¼Œå¤æ•°è§£å‘ˆå…±è½­å¯¹ã€‚è¿™æ„å‘³ç€å¦‚æœæœ‰ä¸€ä¸ª <math alttext="lamda Subscript j"><msub><mi>Î»</mi>
    <mi>j</mi></msub></math> = a + *i*bï¼Œé‚£ä¹ˆè¿˜ä¼šæœ‰ä¸€ä¸ª <math alttext="lamda Subscript k"><msub><mi>Î»</mi>
    <mi>k</mi></msub></math> = a âˆ’ *i*bã€‚å®ƒä»¬ç›¸åº”çš„ç‰¹å¾å‘é‡ä¹Ÿæ˜¯å¤å…±è½­å¯¹ã€‚
- en: I donâ€™t want to go into detail about complex-valued solutions, except to show
    you that complex solutions to eigendecomposition are straightforward.^([5](ch13.xhtml#idm45733292508688))
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸æƒ³è¯¦ç»†è®¨è®ºå¤å€¼è§£å†³æ–¹æ¡ˆï¼Œé™¤äº†å‘æ‚¨å±•ç¤ºå¤å€¼è§£å†³æ–¹æ¡ˆå¯¹ç‰¹å¾åˆ†è§£æ˜¯ç›´è§‚çš„ã€‚
- en: 'Symmetric matrices are guaranteed to have real-valued eigenvalues, and therefore
    also real-valued eigenvectors. Let me start by modifying the previous example
    to make the matrix symmetric:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µä¿è¯å…·æœ‰å®å€¼ç‰¹å¾å€¼ï¼Œå› æ­¤ä¹Ÿæœ‰å®å€¼ç‰¹å¾å‘é‡ã€‚è®©æˆ‘ä»ä¿®æ”¹å‰é¢çš„ä¾‹å­å¼€å§‹ï¼Œä½¿çŸ©é˜µå¯¹ç§°åŒ–ï¼š
- en: '[PRE5]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This is just one specific example; maybe we got lucky here? I recommend taking
    a moment to explore this yourself in the online code; you can create random symmetric
    matrices (by creaing a random matrix and eigendecomposing <math alttext="bold
    upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> ) of any size to confirm that the eigenvalues are real-valued.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯ä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼›ä¹Ÿè®¸æˆ‘ä»¬åœ¨è¿™é‡Œè¿æ°”å¥½äº†ï¼Ÿæˆ‘å»ºè®®ä½ èŠ±ç‚¹æ—¶é—´åœ¨åœ¨çº¿ä»£ç ä¸­è‡ªè¡Œæ¢ç´¢ä¸€ä¸‹ï¼›ä½ å¯ä»¥åˆ›å»ºéšæœºå¯¹ç§°çŸ©é˜µï¼ˆé€šè¿‡åˆ›å»ºä¸€ä¸ªéšæœºçŸ©é˜µå¹¶ç‰¹å¾åˆ†è§£ <math
    alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math> ï¼‰æ¥ç¡®è®¤ç‰¹å¾å€¼æ˜¯å®æ•°ã€‚
- en: Guaranteed real-valued eigenvalues from symmetric matrices is fortunate because
    complex numbers are often confusing to work with. Lots of matrices used in data
    science are symmetric, and so if you see complex eigenvalues in your data science
    applications, itâ€™s possible that there is a problem with the code or with the
    data.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µä¿è¯å…·æœ‰å®å€¼ç‰¹å¾å€¼æ˜¯å¹¸è¿çš„ï¼Œå› ä¸ºå¤æ•°å¸¸å¸¸è®©äººæ„Ÿåˆ°å›°æƒ‘ã€‚æ•°æ®ç§‘å­¦ä¸­ä½¿ç”¨çš„è®¸å¤šçŸ©é˜µéƒ½æ˜¯å¯¹ç§°çš„ï¼Œå› æ­¤å¦‚æœåœ¨æ•°æ®ç§‘å­¦åº”ç”¨ä¸­çœ‹åˆ°å¤æ•°ç‰¹å¾å€¼ï¼Œå¯èƒ½å­˜åœ¨ä»£ç æˆ–æ•°æ®é—®é¢˜ã€‚
- en: Leveraging Symmetry
  id: totrans-134
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: åˆ©ç”¨å¯¹ç§°æ€§
- en: If you know that you are working with a symmetric matrix, you can use `np.linalg.eigh`
    instead of `np.linalg.eig` (or SciPyâ€™s `eigh` instead of `eig`). The `h` is for
    â€œHermitian,â€ which is the complex version of a symmetric matrix. `eigh` can be
    faster and more numerically stable than `eig`, but works only on symmetric matrices.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ çŸ¥é“ä½ åœ¨å¤„ç†å¯¹ç§°çŸ©é˜µï¼Œå¯ä»¥ä½¿ç”¨`np.linalg.eigh`ä»£æ›¿`np.linalg.eig`ï¼ˆæˆ–è€…SciPyçš„`eigh`ä»£æ›¿`eig`ï¼‰ã€‚`h`ä»£è¡¨â€œHermitianâ€ï¼Œè¿™æ˜¯å¯¹ç§°çŸ©é˜µçš„å¤æ•°ç‰ˆæœ¬ã€‚`eigh`å¯èƒ½æ¯”`eig`æ›´å¿«ä¸”æ•°å€¼ä¸Šæ›´ç¨³å®šï¼Œä½†ä»…é€‚ç”¨äºå¯¹ç§°çŸ©é˜µã€‚
- en: Eigendecomposition of Singular Matrices
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¥‡å¼‚çŸ©é˜µçš„ç‰¹å¾åˆ†è§£
- en: I included this section here because I find that students often get the idea
    that singular matrices cannot be eigendecomposed, or that the eigenvectors of
    a singular matrix must be unusual somehow.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘åœ¨è¿™é‡ŒåŒ…å«äº†è¿™ä¸€èŠ‚ï¼Œå› ä¸ºæˆ‘å‘ç°å­¦ç”Ÿä»¬ç»å¸¸ä¼šè®¤ä¸ºå¥‡å¼‚çŸ©é˜µä¸èƒ½è¿›è¡Œç‰¹å¾åˆ†è§£ï¼Œæˆ–è€…å¥‡å¼‚çŸ©é˜µçš„ç‰¹å¾å‘é‡å¿…é¡»åœ¨æŸç§ç¨‹åº¦ä¸Šä¸å¯»å¸¸ã€‚
- en: 'That idea is completely wrong. Eigendecomposition of singular matrices is perfectly
    fine. Here is a quick example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ªè§‚ç‚¹å®Œå…¨é”™è¯¯ã€‚å¥‡å¼‚çŸ©é˜µçš„ç‰¹å¾åˆ†è§£å®Œå…¨æ²¡é—®é¢˜ã€‚è¿™é‡Œæ˜¯ä¸€ä¸ªå¿«é€Ÿçš„ä¾‹å­ï¼š
- en: '[PRE6]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The rank, eigenvalues, and eigenvectors of this matrix are printed here:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªçŸ©é˜µçš„ç§©ã€ç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡åœ¨è¿™é‡Œè¢«æ‰“å°å‡ºæ¥ï¼š
- en: '[PRE7]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This rank-2 matrix has one zero-valued eigenvalue with a nonzeros eigenvector.
    You can use the online code to explore the eigendecomposition of other reduced-rank
    random matrices.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªäºŒé˜¶çŸ©é˜µæœ‰ä¸€ä¸ªé›¶å€¼ç‰¹å¾å€¼ï¼Œå¯¹åº”ä¸€ä¸ªéé›¶ç‰¹å¾å‘é‡ã€‚ä½ å¯ä»¥ä½¿ç”¨åœ¨çº¿ä»£ç æ¥æ¢ç´¢å…¶ä»–é™ç§©éšæœºçŸ©é˜µçš„ç‰¹å¾åˆ†è§£ã€‚
- en: There is one special property of the eigendecomposition of singular matrices,
    which is that at least one eigenvalue is guaranteed to be zero. That doesnâ€™t mean
    that the number of nonzero eigenvalues equals the rank of the matrixâ€”thatâ€™s true
    for singular values (the scalar values from the SVD) but not for eigenvalues.
    But if the matrix is singular, then at least one eigenvalue equals zero.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: å¥‡å¼‚çŸ©é˜µçš„ç‰¹å¾åˆ†è§£æœ‰ä¸€ä¸ªç‰¹æ®Šçš„æ€§è´¨ï¼Œå³è‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾å€¼ä¿è¯ä¸ºé›¶ã€‚è¿™å¹¶ä¸æ„å‘³ç€éé›¶ç‰¹å¾å€¼çš„æ•°é‡ç­‰äºçŸ©é˜µçš„ç§©â€”â€”è¿™å¯¹å¥‡å¼‚å€¼ï¼ˆæ¥è‡ªå¥‡å¼‚å€¼åˆ†è§£çš„æ ‡é‡å€¼ï¼‰æˆç«‹ï¼Œä½†å¯¹ç‰¹å¾å€¼ä¸æˆç«‹ã€‚ä½†å¦‚æœçŸ©é˜µæ˜¯å¥‡å¼‚çš„ï¼Œé‚£ä¹ˆè‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾å€¼ç­‰äºé›¶ã€‚
- en: 'The converse is also true: every full-rank matrix has zero zero-valued eigenvalues.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: åä¹‹äº¦ç„¶ï¼šæ¯ä¸ªæ»¡ç§©çŸ©é˜µéƒ½æœ‰é›¶ä¸ªé›¶å€¼ç‰¹å¾å€¼ã€‚
- en: 'One explanation for why this happens is that a singular matrix already has
    a nontrivial null space, which means <math alttext="lamda equals 0"><mrow><mi>Î»</mi>
    <mo>=</mo> <mn>0</mn></mrow></math> provides a nontrivial solution to the equation
    <math alttext="left-parenthesis bold upper A minus lamda bold upper I right-parenthesis
    bold v equals bold 0"><mrow><mo>(</mo> <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi>
    <mo>)</mo> <mi>ğ¯</mi> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> .
    You can see this in the previous example matrix: the eigenvector associated with
    <math alttext="lamda equals 0"><mrow><mi>Î»</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    is the normalized vector [1 âˆ’2 1], which is the linear weighted combination of
    the columns (or rows) that produces the zeros vector.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆä¼šå‘ç”Ÿè¿™ç§æƒ…å†µçš„ä¸€ä¸ªè§£é‡Šæ˜¯ï¼Œå¥‡å¼‚çŸ©é˜µå·²ç»æœ‰ä¸€ä¸ªéå¹³å‡¡çš„é›¶ç©ºé—´ï¼Œè¿™æ„å‘³ç€<math alttext="lamda equals 0"><mrow><mi>Î»</mi>
    <mo>=</mo> <mn>0</mn></mrow></math>ä¸ºæ–¹ç¨‹<math alttext="left-parenthesis bold upper
    A minus lamda bold upper I right-parenthesis bold v equals bold 0"><mrow><mo>(</mo>
    <mi>ğ€</mi> <mo>-</mo> <mi>Î»</mi> <mi>ğˆ</mi> <mo>)</mo> <mi>ğ¯</mi> <mo>=</mo> <mn
    mathvariant="bold">0</mn></mrow></math>æä¾›äº†ä¸€ä¸ªéå¹³å‡¡è§£ã€‚ä½ å¯ä»¥åœ¨å‰é¢çš„ä¾‹å­çŸ©é˜µä¸­çœ‹åˆ°è¿™ä¸€ç‚¹ï¼šä¸<math alttext="lamda
    equals 0"><mrow><mi>Î»</mi> <mo>=</mo> <mn>0</mn></mrow></math>ç›¸å…³è”çš„ç‰¹å¾å‘é‡æ˜¯æ ‡å‡†åŒ–å‘é‡[1
    âˆ’2 1]ï¼Œå®ƒæ˜¯äº§ç”Ÿé›¶å‘é‡çš„åˆ—ï¼ˆæˆ–è¡Œï¼‰çš„çº¿æ€§åŠ æƒç»„åˆã€‚
- en: The main take-homes of this section are (1) eigendecomposition is valid for
    reduced-rank matrices, and (2) the presence of at least one zero-valued eigenvalue
    indicates a reduced-rank matrix.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚çš„ä¸»è¦è¦ç‚¹æ˜¯ï¼šï¼ˆ1ï¼‰ç‰¹å¾åˆ†è§£å¯¹é™ç§©çŸ©é˜µæœ‰æ•ˆï¼Œï¼ˆ2ï¼‰è‡³å°‘æœ‰ä¸€ä¸ªé›¶ç‰¹å¾å€¼è¡¨æ˜çŸ©é˜µæ˜¯é™ç§©çš„ã€‚
- en: Quadratic Form, Definiteness, and Eigenvalues
  id: totrans-147
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: çŸ©é˜µçš„äºŒæ¬¡å‹ã€æ­£å®šæ€§å’Œç‰¹å¾å€¼
- en: 'Letâ€™s face it: *quadratic form* and *definiteness* are intimidating terms.
    But donâ€™t worryâ€”they are both straightforward concepts that provide a gateway
    to advanced linear algebra and applications such as principal components analysis
    and Monte Carlo simulations. And better still: integrating Python code into your
    learning will give you a huge advantage over learning about these concepts compared
    to traditional linear algebra textbooks.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é¢å¯¹ç°å®ï¼š*äºŒæ¬¡å‹*å’Œ*æ­£å®šæ€§*æ˜¯ä»¤äººç”Ÿç•çš„æœ¯è¯­ã€‚ä½†åˆ«æ‹…å¿ƒâ€”â€”å®ƒä»¬éƒ½æ˜¯ç›´è§‚çš„æ¦‚å¿µï¼Œä¸ºé«˜çº§çº¿æ€§ä»£æ•°å’Œåº”ç”¨æä¾›äº†ä¸€ä¸ªé€šå‘ä¸»æˆåˆ†åˆ†æå’Œè’™ç‰¹å¡æ´›æ¨¡æ‹Ÿç­‰é¢†åŸŸçš„å¤§é—¨ã€‚æ›´é‡è¦çš„æ˜¯ï¼šå°†Pythonä»£ç æ•´åˆåˆ°å­¦ä¹ ä¸­ï¼Œç›¸æ¯”ä¼ ç»Ÿçº¿æ€§ä»£æ•°æ•™ç§‘ä¹¦ï¼Œå°†ä¸ºä½ å¸¦æ¥å·¨å¤§çš„ä¼˜åŠ¿ã€‚
- en: The Quadratic Form of a Matrix
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: çŸ©é˜µçš„äºŒæ¬¡å‹
- en: 'Consider the following expression:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹è¡¨è¾¾å¼ï¼š
- en: <math alttext="bold w Superscript upper T Baseline bold upper A bold w equals
    alpha" display="block"><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ€</mi>
    <mi>ğ°</mi> <mo>=</mo> <mi>Î±</mi></mrow></math>
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold w Superscript upper T Baseline bold upper A bold w equals
    alpha" display="block"><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup> <mi>ğ€</mi>
    <mi>ğ°</mi> <mo>=</mo> <mi>Î±</mi></mrow></math>
- en: In other words, we pre- and postmultiply a square matrix by the same vector
    <math alttext="bold w"><mi>ğ°</mi></math> and get a scalar. (Notice that this multiplication
    is valid only for square matrices.)
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬é€šè¿‡ç›¸åŒçš„å‘é‡<math alttext="bold w"><mi>ğ°</mi></math>å¯¹ä¸€ä¸ªæ–¹é˜µè¿›è¡Œå‰åä¹˜æ³•è¿ç®—ï¼Œå¾—åˆ°ä¸€ä¸ªæ ‡é‡ã€‚ï¼ˆè¯·æ³¨æ„ï¼Œè¿™ç§ä¹˜æ³•ä»…é€‚ç”¨äºæ–¹é˜µã€‚ï¼‰
- en: This is called the *quadratic form* on matrix <math alttext="bold upper A"><mi>ğ€</mi></math>
    .
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™è¢«ç§°ä¸ºåœ¨çŸ©é˜µ<math alttext="bold upper A"><mi>ğ€</mi></math>ä¸Šçš„*äºŒæ¬¡å‹*ã€‚
- en: 'Which matrix and which vector do we use? The idea of the quadratic form is
    to use one specific matrix and the set of all possible vectors (of appropriate
    size). The important question concerns the signs of <math alttext="alpha"><mi>Î±</mi></math>
    for all possible vectors. Letâ€™s see an example:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä½¿ç”¨å“ªä¸ªçŸ©é˜µå’Œå“ªä¸ªå‘é‡ï¼ŸäºŒæ¬¡å‹çš„æ¦‚å¿µæ˜¯ä½¿ç”¨ä¸€ä¸ªç‰¹å®šçš„çŸ©é˜µå’Œæ‰€æœ‰å¯èƒ½çš„å‘é‡ï¼ˆå¤§å°åˆé€‚ï¼‰ã€‚é‡è¦çš„é—®é¢˜æ¶‰åŠæ‰€æœ‰å¯èƒ½å‘é‡çš„<math alttext="alpha"><mi>Î±</mi></math>çš„ç¬¦å·ã€‚è®©æˆ‘ä»¬çœ‹ä¸€ä¸ªä¾‹å­ï¼š
- en: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column 2 2nd Column 4 2nd Row 1st Column 0 2nd
    Column 3 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix equals
    2 x squared plus left-parenthesis 0 plus 4 right-parenthesis x y plus 3 y squared"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr> <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mn>2</mn> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>+</mo> <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo>
    <mn>3</mn> <msup><mi>y</mi> <mn>2</mn></msup></mrow></math>
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column 2 2nd Column 4 2nd Row 1st Column 0 2nd
    Column 3 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix equals
    2 x squared plus left-parenthesis 0 plus 4 right-parenthesis x y plus 3 y squared"
    display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mn>2</mn></mtd>
    <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>0</mn></mtd> <mtd><mn>3</mn></mtd></mtr></mtable></mfenced>
    <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr> <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced>
    <mo>=</mo> <mn>2</mn> <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo>
    <mn>0</mn> <mo>+</mo> <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo>
    <mn>3</mn> <msup><mi>y</mi> <mn>2</mn></msup></mrow></math>
- en: For this particular matrix, there is no possible combination of *x* and *y*
    that can give a negative answer, because the squared terms (2*x*Â² and 3*y*Â²) will
    always overpower the cross-term (4*xy*) even when *x* or *y* is negative. Furthermore,
    <math alttext="alpha"><mi>Î±</mi></math> can be nonpositive only when *x* = *y*
    = 0.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºè¿™ä¸ªç‰¹å®šçš„çŸ©é˜µï¼Œä¸å­˜åœ¨*x*å’Œ*y*çš„ä»»ä½•ç»„åˆå¯ä»¥ç»™å‡ºè´Ÿç­”æ¡ˆï¼Œå› ä¸ºå¹³æ–¹é¡¹ï¼ˆ2*x*Â²å’Œ3*y*Â²ï¼‰æ€»ä¼šå‹å€’äº¤å‰é¡¹ï¼ˆ4*xy*ï¼‰ï¼Œå³ä½¿*x*æˆ–*y*ä¸ºè´Ÿã€‚æ­¤å¤–ï¼Œ<math
    alttext="alpha"><mi>Î±</mi></math>åªæœ‰åœ¨*x* = *y* = 0æ—¶æ‰å¯èƒ½ä¸ºéæ­£ã€‚
- en: 'That is not a trivial result of the quadratic form. For example, the following
    matrix can have a positive or negative <math alttext="alpha"><mi>Î±</mi></math>
    depending on the values of *x* and *y*:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸æ˜¯äºŒæ¬¡å‹çš„ä¸€ä¸ªå¹³å‡¡ç»“æœã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„çŸ©é˜µå¯ä»¥æ ¹æ®*x*å’Œ*y*çš„å€¼å¾—åˆ°ä¸€ä¸ªæ­£æˆ–è´Ÿçš„<math alttext="alpha"><mi>Î±</mi></math>ï¼š
- en: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column negative 9 2nd Column 4 2nd Row 1st Column
    3 2nd Column 9 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix
    equals minus 9 x squared plus left-parenthesis 3 plus 4 right-parenthesis x y
    plus 9 y squared" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>9</mn></mrow></mtd> <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>9</mn></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo> <mn>9</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo> <mn>9</mn> <msup><mi>y</mi>
    <mn>2</mn></msup></mrow></math>
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="Start 1 By 2 Matrix 1st Row 1st Column x 2nd Column y EndMatrix
    Start 2 By 2 Matrix 1st Row 1st Column negative 9 2nd Column 4 2nd Row 1st Column
    3 2nd Column 9 EndMatrix StartBinomialOrMatrix x Choose y EndBinomialOrMatrix
    equals minus 9 x squared plus left-parenthesis 3 plus 4 right-parenthesis x y
    plus 9 y squared" display="block"><mrow><mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd>
    <mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mrow><mo>-</mo>
    <mn>9</mn></mrow></mtd> <mtd><mn>4</mn></mtd></mtr> <mtr><mtd><mn>3</mn></mtd>
    <mtd><mn>9</mn></mtd></mtr></mtable></mfenced> <mfenced close="]" open="["><mtable><mtr><mtd><mi>x</mi></mtd></mtr>
    <mtr><mtd><mi>y</mi></mtd></mtr></mtable></mfenced> <mo>=</mo> <mo>-</mo> <mn>9</mn>
    <msup><mi>x</mi> <mn>2</mn></msup> <mo>+</mo> <mrow><mo>(</mo> <mn>3</mn> <mo>+</mo>
    <mn>4</mn> <mo>)</mo></mrow> <mi>x</mi> <mi>y</mi> <mo>+</mo> <mn>9</mn> <msup><mi>y</mi>
    <mn>2</mn></msup></mrow></math>
- en: You can confirm that setting [*x* *y*] to [âˆ’1 1] gives a negative quadratic
    form result, while [âˆ’1 âˆ’1] gives a positive result.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥ç¡®è®¤å°†[*x* *y*]è®¾ç½®ä¸º[âˆ’1 1]ä¼šå¾—åˆ°ä¸€ä¸ªè´ŸäºŒæ¬¡å‹çš„ç»“æœï¼Œè€Œ[âˆ’1 âˆ’1]åˆ™ä¼šå¾—åˆ°ä¸€ä¸ªæ­£ç»“æœã€‚
- en: 'How can you possibly know whether the quadratic form will produce a positive
    (or negative, or zero-valued) scalar for *all* possible vectors? The key comes
    from considering that a full-rank eigenvectors matrix spans all of <math alttext="double-struck
    upper R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math> , and therefore
    that every vector in <math alttext="double-struck upper R Superscript upper M"><msup><mi>â„</mi>
    <mi>M</mi></msup></math> can be expressed as some linear weighted combination
    of the eigenvectors.^([6](ch13.xhtml#idm45733292090752)) Then we start from the
    eigenvalue equation and left-multiply by an eigenvector to return to the quadratic
    form:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ æ€ä¹ˆå¯èƒ½çŸ¥é“äºŒæ¬¡å½¢å¼å¯¹*æ‰€æœ‰*å¯èƒ½çš„å‘é‡ä¼šäº§ç”Ÿæ­£ï¼ˆæˆ–è´Ÿï¼Œæˆ–é›¶å€¼ï¼‰çš„æ ‡é‡ï¼Ÿå…³é”®åœ¨äºè€ƒè™‘åˆ°ä¸€ä¸ªæ»¡ç§©ç‰¹å¾å‘é‡çŸ©é˜µæ¶µç›–äº†<math alttext="double-struck
    upper R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math>çš„æ‰€æœ‰éƒ¨åˆ†ï¼Œå› æ­¤<math
    alttext="double-struck upper R Superscript upper M"><msup><mi>â„</mi> <mi>M</mi></msup></math>ä¸­çš„æ¯ä¸ªå‘é‡éƒ½å¯ä»¥è¡¨ç¤ºä¸ºç‰¹å¾å‘é‡çš„æŸäº›çº¿æ€§åŠ æƒç»„åˆã€‚^([6](ch13.xhtml#idm45733292090752))
    ç„¶åï¼Œæˆ‘ä»¬ä»ç‰¹å¾å€¼æ–¹ç¨‹å¼€å§‹ï¼Œå¹¶é€šè¿‡ä¸€ä¸ªç‰¹å¾å‘é‡å·¦ä¹˜æ¥è¿”å›åˆ°äºŒæ¬¡å½¢å¼ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold v Superscript upper T Baseline bold
    upper A bold v 2nd Column equals lamda bold v Superscript upper T Baseline bold
    v 3rd Row 1st Column bold v Superscript upper T Baseline bold upper A bold v 2nd
    Column equals lamda parallel-to bold v parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <mi>ğ¯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup> <mi>ğ€</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ¯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>Î»</mi><mo>âˆ¥</mo><mi>ğ¯</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper A bold v 2nd Column
    equals lamda bold v 2nd Row 1st Column bold v Superscript upper T Baseline bold
    upper A bold v 2nd Column equals lamda bold v Superscript upper T Baseline bold
    v 3rd Row 1st Column bold v Superscript upper T Baseline bold upper A bold v 2nd
    Column equals lamda parallel-to bold v parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><mi>ğ€</mi> <mi>ğ¯</mi></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup> <mi>ğ€</mi>
    <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <mi>Î»</mi> <msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ¯</mi></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>Î»</mi><mo>âˆ¥</mo><mi>ğ¯</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: The final equation is key. Note that <math alttext="parallel-to bold v Superscript
    upper T Baseline bold v parallel-to"><mrow><mrow><mo>âˆ¥</mo></mrow> <msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mrow><mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></mrow></math> is strictly
    positive (vector magnitudes cannot be negative, and we ignore the zeros vector),
    which means that the sign of the right-hand side of the equation is determined
    entirely by the eigenvalue <math alttext="lamda"><mi>Î»</mi></math> .
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€ç»ˆçš„æ–¹ç¨‹æ˜¯å…³é”®ã€‚æ³¨æ„<math alttext="parallel-to bold v Superscript upper T Baseline bold
    v parallel-to"><mrow><mrow><mo>âˆ¥</mo></mrow> <msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <mrow><mi>ğ¯</mi> <mo>âˆ¥</mo></mrow></mrow></math>ä¸¥æ ¼ä¸ºæ­£ï¼ˆå‘é‡å¤§å°ä¸å¯èƒ½ä¸ºè´Ÿï¼Œå¹¶å¿½ç•¥é›¶å‘é‡ï¼‰ï¼Œè¿™æ„å‘³ç€æ–¹ç¨‹å³ä¾§çš„ç¬¦å·å®Œå…¨ç”±ç‰¹å¾å€¼<math
    alttext="lamda"><mi>Î»</mi></math>å†³å®šã€‚
- en: 'That equation uses only one eigenvalue and its eigenvector, but we need to
    know about any possible vector. The insight is to consider that if the equation
    is valid for each eigenvector-eigenvalue pair, then it is valid for any combination
    of eigenvector-eigenvalue pairs. For example:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: é‚£ä¸ªæ–¹ç¨‹ä»…ä½¿ç”¨ä¸€ä¸ªç‰¹å¾å€¼åŠå…¶ç‰¹å¾å‘é‡ï¼Œä½†æˆ‘ä»¬éœ€è¦äº†è§£ä»»ä½•å¯èƒ½çš„å‘é‡ã€‚å…³é”®åœ¨äºè€ƒè™‘åˆ°å¦‚æœæ–¹ç¨‹å¯¹æ¯ä¸ªç‰¹å¾å‘é‡-ç‰¹å¾å€¼å¯¹éƒ½æœ‰æ•ˆï¼Œåˆ™å®ƒå¯¹ä»»ä½•ç‰¹å¾å‘é‡-ç‰¹å¾å€¼å¯¹çš„ä»»ä½•ç»„åˆä¹Ÿæœ‰æ•ˆã€‚ä¾‹å¦‚ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold v 1 Superscript upper T Baseline
    bold upper A bold v 1 2nd Column equals lamda 1 parallel-to bold v 1 parallel-to
    2nd Row 1st Column bold v 2 Superscript upper T Baseline bold upper A bold v 2
    2nd Column equals lamda 2 parallel-to bold v 2 parallel-to 3rd Row 1st Column
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis Superscript upper T
    Baseline bold upper A left-parenthesis bold v 1 plus bold v 2 right-parenthesis
    2nd Column equals left-parenthesis lamda 1 plus lamda 2 right-parenthesis parallel-to
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis parallel-to 4th Row
    1st Column bold u Superscript upper T Baseline bold upper A bold u 2nd Column
    equals zeta parallel-to bold u parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msubsup><mi>ğ¯</mi> <mn>1</mn>
    <mtext>T</mtext></msubsup> <mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <msup><mrow><mo>âˆ¥</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msubsup><mi>ğ¯</mi> <mn>2</mn> <mtext>T</mtext></msubsup>
    <mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>Î»</mi> <mn>2</mn></msub> <msup><mrow><mo>âˆ¥</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub>
    <mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mrow><mo>(</mo> <msub><mi>ğ¯</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msub><mi>Î»</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msup><mrow><mo>âˆ¥</mo><mrow><mo>(</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ®</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ®</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>Î¶</mi><mo>âˆ¥</mo><mi>ğ®</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold v 1 Superscript upper T Baseline
    bold upper A bold v 1 2nd Column equals lamda 1 parallel-to bold v 1 parallel-to
    2nd Row 1st Column bold v 2 Superscript upper T Baseline bold upper A bold v 2
    2nd Column equals lamda 2 parallel-to bold v 2 parallel-to 3rd Row 1st Column
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis Superscript upper T
    Baseline bold upper A left-parenthesis bold v 1 plus bold v 2 right-parenthesis
    2nd Column equals left-parenthesis lamda 1 plus lamda 2 right-parenthesis parallel-to
    left-parenthesis bold v 1 plus bold v 2 right-parenthesis parallel-to 4th Row
    1st Column bold u Superscript upper T Baseline bold upper A bold u 2nd Column
    equals zeta parallel-to bold u parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msubsup><mi>ğ¯</mi> <mn>1</mn>
    <mtext>T</mtext></msubsup> <mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>1</mn></msub></mrow></mtd>
    <mtd columnalign="left"><mrow><mo>=</mo> <msub><mi>Î»</mi> <mn>1</mn></msub> <msup><mrow><mo>âˆ¥</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd
    columnalign="right"><mrow><msubsup><mi>ğ¯</mi> <mn>2</mn> <mtext>T</mtext></msubsup>
    <mi>ğ€</mi> <msub><mi>ğ¯</mi> <mn>2</mn></msub></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msub><mi>Î»</mi> <mn>2</mn></msub> <msup><mrow><mo>âˆ¥</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub>
    <mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mrow><mo>(</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mrow><mo>(</mo> <msub><mi>ğ¯</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow></mrow></mtd> <mtd
    columnalign="left"><mrow><mo>=</mo> <mrow><mo>(</mo> <msub><mi>Î»</mi> <mn>1</mn></msub>
    <mo>+</mo> <msub><mi>Î»</mi> <mn>2</mn></msub> <mo>)</mo></mrow> <msup><mrow><mo>âˆ¥</mo><mrow><mo>(</mo><msub><mi>ğ¯</mi>
    <mn>1</mn></msub> <mo>+</mo><msub><mi>ğ¯</mi> <mn>2</mn></msub> <mo>)</mo></mrow><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr> <mtr><mtd columnalign="right"><mrow><msup><mi>ğ®</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ®</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <msup><mrow><mi>Î¶</mi><mo>âˆ¥</mo><mi>ğ®</mi><mo>âˆ¥</mo></mrow> <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: In other words, we can set any vector <math alttext="bold u"><mi>ğ®</mi></math>
    to be some linear combination of eigenvectors, and some scalar <math alttext="zeta"><mi>Î¶</mi></math>
    to be that same linear combination of eigenvalues. Anyway, it doesnâ€™t change the
    principle that the sign of the right-hand sideâ€”and therefore also the sign of
    the quadratic formâ€”is determined by the sign of the eigenvalues.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢å¥è¯è¯´ï¼Œæˆ‘ä»¬å¯ä»¥å°†ä»»ä½•å‘é‡<math alttext="bold u"><mi>ğ®</mi></math>è®¾ç½®ä¸ºç‰¹å¾å‘é‡çš„æŸäº›çº¿æ€§ç»„åˆï¼Œå¹¶ä¸”ä¸€äº›æ ‡é‡<math
    alttext="zeta"><mi>Î¶</mi></math>ä¹Ÿå¯ä»¥æ˜¯ç›¸åŒçš„ç‰¹å¾å€¼çº¿æ€§ç»„åˆã€‚æ— è®ºå¦‚ä½•ï¼Œè¿™ä¸æ”¹å˜å³ä¾§çš„ç¬¦å·â€”â€”å› æ­¤ä¹Ÿä¸æ”¹å˜äºŒæ¬¡å½¢å¼çš„ç¬¦å·ï¼Œè¿™ä¸€åŸåˆ™ç”±ç‰¹å¾å€¼çš„ç¬¦å·ç¡®å®šã€‚
- en: 'Now letâ€™s think about these equations under different assumptions about the
    signs of the <math alttext="lamda"><mi>Î»</mi></math> s:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬æ ¹æ®å¯¹<math alttext="lamda"><mi>Î»</mi></math>çš„ä¸åŒå‡è®¾æ¥æ€è€ƒè¿™äº›æ–¹ç¨‹ï¼š
- en: All eigenvalues are positive
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰ç‰¹å¾å€¼å‡ä¸ºæ­£
- en: The right-hand side of the equation is always positive, meaning that <math alttext="bold
    v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi> <mi>ğ¯</mi></mrow></math> is always positive for any vector <math alttext="bold
    v"><mi>ğ¯</mi></math> .
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹çš„å³ä¾§å§‹ç»ˆä¸ºæ­£ï¼Œè¿™æ„å‘³ç€å¯¹äºä»»ä½•å‘é‡<math alttext="bold v"><mi>ğ¯</mi></math>ï¼Œ<math alttext="bold
    v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi> <mi>ğ¯</mi></mrow></math>å§‹ç»ˆä¸ºæ­£ã€‚
- en: Eigenvalues are positive or zero
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å€¼ä¸ºæ­£æˆ–é›¶
- en: <math alttext="bold v Superscript upper T Baseline bold upper A bold v"><mrow><msup><mi>ğ¯</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ¯</mi></mrow></math> is nonnegative and
    will equal zero when <math alttext="lamda equals 0"><mrow><mi>Î»</mi> <mo>=</mo>
    <mn>0</mn></mrow></math> (which happens when the matrix is singular).
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: å½“çŸ©é˜µæ˜¯å¥‡å¼‚çŸ©é˜µæ—¶ï¼Œ<math alttext="bold v Superscript upper T Baseline bold upper A bold
    v"><mrow><msup><mi>ğ¯</mi> <mtext>T</mtext></msup> <mi>ğ€</mi> <mi>ğ¯</mi></mrow></math>æ˜¯éè´Ÿçš„ï¼Œå¹¶ä¸”ç­‰äºé›¶ï¼Œè¿™æ—¶<math
    alttext="lamda equals 0"><mrow><mi>Î»</mi> <mo>=</mo> <mn>0</mn></mrow></math>ã€‚
- en: Eigenvalues are negative or zero
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å€¼ä¸ºè´Ÿæˆ–é›¶
- en: The quadratic form result will be zero or negative.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: äºŒæ¬¡å½¢å¼çš„ç»“æœå°†æ˜¯é›¶æˆ–è´Ÿã€‚
- en: Eigenvalues are negative
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç‰¹å¾å€¼ä¸ºè´Ÿ
- en: The quadratic form result will be negative for all vectors.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºæ‰€æœ‰å‘é‡ï¼ŒäºŒæ¬¡å½¢å¼çš„ç»“æœå°†ä¸ºè´Ÿã€‚
- en: Definiteness
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ˜ç¡®æ€§
- en: '*Definiteness* is a characteristic of a square matrix and is defined by the
    signs of the eigenvalues of the matrix, which is the same thing as the signs of
    the quadratic form results. Definiteness has implications for the invertibility
    of a matrix as well as advanced data analysis methods such as generalized eigendecomposition
    (used in multivariate linear classifiers and signal processing).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '*æ˜ç¡®æ€§* æ˜¯æ–¹é˜µçš„ä¸€ä¸ªç‰¹å¾ï¼Œå¹¶ä¸”ç”±çŸ©é˜µçš„ç‰¹å¾å€¼çš„ç¬¦å·å®šä¹‰ï¼Œè¿™ä¸äºŒæ¬¡å‹ç»“æœçš„ç¬¦å·ç›¸åŒã€‚æ˜ç¡®æ€§å¯¹çŸ©é˜µçš„å¯é€†æ€§ä»¥åŠé«˜çº§æ•°æ®åˆ†ææ–¹æ³•ï¼ˆå¦‚å¹¿ä¹‰ç‰¹å¾åˆ†è§£ï¼Œç”¨äºå¤šå˜é‡çº¿æ€§åˆ†ç±»å™¨å’Œä¿¡å·å¤„ç†ï¼‰æœ‰é‡è¦æ„ä¹‰ã€‚'
- en: There are five categories of definiteness, as shown in [TableÂ 13-1](#def-cat);
    the + and âˆ’ signs indicate the signs of the eigenvalues.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰äº”ç§æ˜ç¡®æ€§ç±»åˆ«ï¼Œå¦‚[è¡¨æ ¼Â 13-1](#def-cat)æ‰€ç¤ºï¼›+ å’Œ âˆ’ ç¬¦å·è¡¨ç¤ºç‰¹å¾å€¼çš„ç¬¦å·ã€‚
- en: Table 13-1\. Definiteness categories
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ 13-1\. æ˜ç¡®æ€§ç±»åˆ«
- en: '| Category | Quadratic form | Eigenvalues | Invertible |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| ç±»åˆ« | äºŒæ¬¡å‹ | ç‰¹å¾å€¼ | å¯é€† |'
- en: '| --- | --- | --- | --- |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| Positive definite | Positive | + | Yes |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| æ­£å®š | æ­£ | + | æ˜¯ |'
- en: '| Positive semidefinite | Nonnegative | + and 0 | No |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| æ­£åŠå®š | éè´Ÿ | + å’Œ 0 | ä¸ |'
- en: '| Indefinite | Positive and negative | + and âˆ’ | Depends |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ä¸å®š | æ­£è´Ÿ | + å’Œ âˆ’ | å–å†³äº |'
- en: '| Negative semidefinite | Nonpositive | âˆ’ and 0 | No |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| è´ŸåŠå®š | éæ­£ | âˆ’ å’Œ 0 | ä¸ |'
- en: '| Negative definite | Negative | âˆ’ | Yes |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| è´Ÿå®š | è´Ÿ | âˆ’ | æ˜¯ |'
- en: â€œDependsâ€ in the table means that the matrix can be invertible or singular depending
    on the numbers in the matrix, not on the definiteness category.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: è¡¨æ ¼ä¸­çš„â€œå–å†³äºâ€æ„å‘³ç€çŸ©é˜µçš„å¯é€†æ€§æˆ–å¥‡å¼‚æ€§å–å†³äºçŸ©é˜µä¸­çš„æ•°å­—ï¼Œè€Œä¸æ˜¯ç¡®å®šæ€§ç±»åˆ«ã€‚
- en: <math alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math> Is Positive (Semi)definite
  id: totrans-187
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: <math alttext="ç²—ä½“å¤§å†™Aä¸Šæ ‡å¤§å†™TåŸºçº¿ç²—ä½“å¤§å†™A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> æ˜¯æ­£ï¼ˆåŠï¼‰å®šçš„
- en: Any matrix that can be expressed as the product of a matrix and its transpose
    (that is, <math alttext="bold upper S equals bold upper A Superscript upper T
    Baseline bold upper A"><mrow><mi>ğ’</mi> <mo>=</mo> <msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> ) is guaranteed to be positive definite or positive semidefinite.
    The combination of these two categories is often written as â€œpositive (semi)definite.â€
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ä»»ä½•å¯ä»¥è¡¨ç¤ºä¸ºçŸ©é˜µåŠå…¶è½¬ç½®ä¹˜ç§¯ï¼ˆå³ï¼Œ<math alttext="ç²—ä½“å¤§å†™Sç­‰äºç²—ä½“å¤§å†™Aä¸Šæ ‡å¤§å†™TåŸºçº¿ç²—ä½“å¤§å†™A"><mrow><mi>ğ’</mi>
    <mo>=</mo> <msup><mi>ğ€</mi> <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math> ï¼‰çš„çŸ©é˜µéƒ½ä¿è¯æ˜¯æ­£å®šæˆ–æ­£åŠå®šçš„ã€‚è¿™ä¸¤ä¸ªç±»åˆ«çš„ç»“åˆé€šå¸¸è¢«å†™ä½œâ€œæ­£ï¼ˆåŠï¼‰å®šâ€ã€‚
- en: All data covariance matrices are positive (semi)definite, because they are defined
    as the data matrix times its transpose. This means that all covariance matrices
    have nonnegative eigenvalues. The eigenvalues will be all positive when the data
    matrix is full-rank (full column-rank if the data is stored as observations by
    features), and there will be at least one zero-valued eigenvalue if the data matrix
    is reduced-rank.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€æœ‰æ•°æ®åæ–¹å·®çŸ©é˜µéƒ½æ˜¯æ­£ï¼ˆåŠï¼‰å®šçš„ï¼Œå› ä¸ºå®ƒä»¬è¢«å®šä¹‰ä¸ºæ•°æ®çŸ©é˜µä¹˜ä»¥å…¶è½¬ç½®ã€‚è¿™æ„å‘³ç€æ‰€æœ‰åæ–¹å·®çŸ©é˜µå…·æœ‰éè´Ÿç‰¹å¾å€¼ã€‚å½“æ•°æ®çŸ©é˜µæ˜¯æ»¡ç§©æ—¶ï¼ˆå¦‚æœæ•°æ®ä»¥è§‚æµ‹å€¼å’Œç‰¹å¾å­˜å‚¨ï¼Œåˆ™ä¸ºæ»¡åˆ—ç§©ï¼‰ï¼Œç‰¹å¾å€¼å°†å…¨éƒ¨ä¸ºæ­£ï¼›å¦‚æœæ•°æ®çŸ©é˜µæ˜¯é™ç§©çš„ï¼Œåˆ™è‡³å°‘ä¼šæœ‰ä¸€ä¸ªé›¶å€¼ç‰¹å¾å€¼ã€‚
- en: The proof that <math alttext="bold upper S"><mi>ğ’</mi></math> is positive (semi)definite
    comes from writing out its quadratic form and applying some algebraic manipulations.
    (Observe that the transition from the first to the second equation simply involves
    moving parentheses around; such â€œproof by parenthesesâ€ is common in linear algebra.)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="ç²—ä½“å¤§å†™S"><mi>ğ’</mi></math> æ˜¯æ­£ï¼ˆåŠï¼‰å®šçš„è¯æ˜æ¥æºäºåˆ—å‡ºå…¶äºŒæ¬¡å‹å¹¶åº”ç”¨ä¸€äº›ä»£æ•°æ“ä½œã€‚ï¼ˆæ³¨æ„ï¼Œä»ç¬¬ä¸€ä¸ªåˆ°ç¬¬äºŒä¸ªæ–¹ç¨‹çš„è¿‡æ¸¡åªæ¶‰åŠç§»åŠ¨æ‹¬å·ï¼›è¿™ç§â€œæ‹¬å·è¯æ˜â€åœ¨çº¿æ€§ä»£æ•°ä¸­å¾ˆå¸¸è§ã€‚ï¼‰
- en: <math alttext="StartLayout 1st Row 1st Column bold w Superscript upper T Baseline
    bold upper S bold w 2nd Column equals bold w Superscript upper T Baseline left-parenthesis
    bold upper A Superscript upper T Baseline bold upper A right-parenthesis bold
    w 2nd Row 1st Column Blank 2nd Column equals left-parenthesis bold w Superscript
    upper T Baseline bold upper A Superscript upper T Baseline right-parenthesis left-parenthesis
    bold upper A bold w right-parenthesis 3rd Row 1st Column Blank 2nd Column equals
    left-parenthesis bold upper A bold w right-parenthesis Superscript upper T Baseline
    left-parenthesis bold upper A bold w right-parenthesis 4th Row 1st Column Blank
    2nd Column equals parallel-to bold upper A bold w parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup>
    <mi>ğ’</mi> <mi>ğ°</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ°</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi> <mo>)</mo></mrow> <mi>ğ°</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mrow><mo>(</mo> <msup><mi>ğ°</mi> <mtext>T</mtext></msup> <msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mi>ğ€</mi> <mi>ğ°</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi>ğ€</mi><mi>ğ°</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>ğ€</mi> <mi>ğ°</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>âˆ¥</mo><mi>ğ€</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold w Superscript upper T Baseline
    bold upper S bold w 2nd Column equals bold w Superscript upper T Baseline left-parenthesis
    bold upper A Superscript upper T Baseline bold upper A right-parenthesis bold
    w 2nd Row 1st Column Blank 2nd Column equals left-parenthesis bold w Superscript
    upper T Baseline bold upper A Superscript upper T Baseline right-parenthesis left-parenthesis
    bold upper A bold w right-parenthesis 3rd Row 1st Column Blank 2nd Column equals
    left-parenthesis bold upper A bold w right-parenthesis Superscript upper T Baseline
    left-parenthesis bold upper A bold w right-parenthesis 4th Row 1st Column Blank
    2nd Column equals parallel-to bold upper A bold w parallel-to EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mrow><msup><mi>ğ°</mi> <mtext>T</mtext></msup>
    <mi>ğ’</mi> <mi>ğ°</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo> <msup><mi>ğ°</mi>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi> <mo>)</mo></mrow> <mi>ğ°</mi></mrow></mtd></mtr> <mtr><mtd columnalign="left"><mrow><mo>=</mo>
    <mrow><mo>(</mo> <msup><mi>ğ°</mi> <mtext>T</mtext></msup> <msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mo>)</mo></mrow> <mrow><mo>(</mo> <mi>ğ€</mi> <mi>ğ°</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>(</mo><mi>ğ€</mi><mi>ğ°</mi><mo>)</mo></mrow>
    <mtext>T</mtext></msup> <mrow><mo>(</mo> <mi>ğ€</mi> <mi>ğ°</mi> <mo>)</mo></mrow></mrow></mtd></mtr>
    <mtr><mtd columnalign="left"><mrow><mo>=</mo> <msup><mrow><mo>âˆ¥</mo><mi>ğ€</mi><mi>ğ°</mi><mo>âˆ¥</mo></mrow>
    <mn>2</mn></msup></mrow></mtd></mtr></mtable></math>
- en: The point is that the quadratic form of <math alttext="bold upper A Superscript
    upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> equals the squared magnitude of a matrix times a vector.
    Magnitudes cannot be negative, and can be zero only when the vector is zero. And
    if <math alttext="bold upper A bold w equals bold 0"><mrow><mi>ğ€</mi> <mi>ğ°</mi>
    <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math> for a nontrival <math alttext="bold
    w"><mi>ğ°</mi></math> , then <math alttext="bold upper A"><mi>ğ€</mi></math> is
    singular.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: å…³é”®åœ¨äº<math alttext="ç²—ä½“å¤§å†™Aä¸Šæ ‡å¤§å†™TåŸºçº¿ç²—ä½“å¤§å†™A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> çš„äºŒæ¬¡å‹ç­‰äºçŸ©é˜µä¹˜ä»¥å‘é‡çš„å¹³æ–¹å¹…åº¦ã€‚å¹…åº¦ä¸èƒ½ä¸ºè´Ÿï¼Œåªæœ‰å½“å‘é‡ä¸ºé›¶æ—¶æ‰èƒ½ä¸ºé›¶ã€‚å¦‚æœ<math alttext="ç²—ä½“å¤§å†™Aç²—ä½“å°å†™wç­‰äºç²—ä½“
    0"><mrow><mi>ğ€</mi> <mi>ğ°</mi> <mo>=</mo> <mn mathvariant="bold">0</mn></mrow></math>å¯¹äºä¸€ä¸ªéå¹³å‡¡çš„<math
    alttext="ç²—ä½“å°å†™w"><mi>ğ°</mi></math> ï¼Œåˆ™<math alttext="ç²—ä½“å¤§å†™A"><mi>ğ€</mi></math> æ˜¯å¥‡å¼‚çš„ã€‚
- en: Keep in mind that although all <math alttext="bold upper A Superscript upper
    T Baseline bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math>
    matrices are symmetric, not all symmetric matrices can be expressed as <math alttext="bold
    upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup>
    <mi>ğ€</mi></mrow></math> . In other words, matrix symmetry on its own does not
    guarantee positive (semi)definiteness, because not all symmetric matrices can
    be expressed as the product of a matrix and its transpose.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: è¯·è®°ä½ï¼Œå°½ç®¡æ‰€æœ‰çš„<math alttext="bold upper A Superscript upper T Baseline bold upper
    A"><mrow><msup><mi>ğ€</mi> <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math>çŸ©é˜µéƒ½æ˜¯å¯¹ç§°çš„ï¼Œä½†å¹¶éæ‰€æœ‰å¯¹ç§°çŸ©é˜µéƒ½å¯ä»¥è¡¨ç¤ºä¸º<math
    alttext="bold upper A Superscript upper T Baseline bold upper A"><mrow><msup><mi>ğ€</mi>
    <mtext>T</mtext></msup> <mi>ğ€</mi></mrow></math> ã€‚æ¢å¥è¯è¯´ï¼Œä»…é çŸ©é˜µçš„å¯¹ç§°æ€§å¹¶ä¸èƒ½ä¿è¯æ­£å®šæ€§æˆ–åŠæ­£å®šæ€§ï¼Œå› ä¸ºå¹¶éæ‰€æœ‰å¯¹ç§°çŸ©é˜µéƒ½å¯ä»¥è¡¨ç¤ºä¸ºçŸ©é˜µä¸å…¶è½¬ç½®çš„ä¹˜ç§¯ã€‚
- en: Generalized Eigendecomposition
  id: totrans-194
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰ç‰¹å¾åˆ†è§£
- en: 'Consider that the following equation is the same as the fundamental eigenvalue
    equation:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: è€ƒè™‘ä»¥ä¸‹æ–¹ç¨‹ä¸åŸºæœ¬ç‰¹å¾å€¼æ–¹ç¨‹ç›¸åŒï¼š
- en: <math alttext="bold upper A bold v equals lamda bold upper I bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğˆ</mi> <mi>ğ¯</mi></mrow></math>
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold upper I bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğˆ</mi> <mi>ğ¯</mi></mrow></math>
- en: 'This is obvious because <math alttext="bold upper I bold v equals bold v"><mrow><mi>ğˆ</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>ğ¯</mi></mrow></math> . Generalized eigendecomposition
    involves replacing the identity matrix with another matrix (not the identity or
    zeros matrix):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™å¾ˆæ˜æ˜¾ï¼Œå› ä¸º<math alttext="bold upper I bold v equals bold v"><mrow><mi>ğˆ</mi> <mi>ğ¯</mi>
    <mo>=</mo> <mi>ğ¯</mi></mrow></math> ã€‚å¹¿ä¹‰ç‰¹å¾åˆ†è§£æ¶‰åŠç”¨å¦ä¸€ä¸ªçŸ©é˜µï¼ˆä¸æ˜¯å•ä½çŸ©é˜µæˆ–é›¶çŸ©é˜µï¼‰æ›¿æ¢å•ä½çŸ©é˜µï¼š
- en: <math alttext="bold upper A bold v equals lamda bold upper B bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğ</mi> <mi>ğ¯</mi></mrow></math>
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper A bold v equals lamda bold upper B bold v" display="block"><mrow><mi>ğ€</mi>
    <mi>ğ¯</mi> <mo>=</mo> <mi>Î»</mi> <mi>ğ</mi> <mi>ğ¯</mi></mrow></math>
- en: Generalized eigendecomposition is also called *simultaneous diagonalization
    of two matrices*. The resulting ( <math alttext="lamda comma bold v"><mrow><mi>Î»</mi>
    <mo>,</mo> <mi>ğ¯</mi></mrow></math> ) pair is not an eigenvalue/vector of <math
    alttext="bold upper A"><mi>ğ€</mi></math> alone nor of <math alttext="bold upper
    B"><mi>ğ</mi></math> alone. Instead, the two matrices share eigenvalue/vector
    pairs.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¿ä¹‰ç‰¹å¾åˆ†è§£ä¹Ÿç§°ä¸º*ä¸¤ä¸ªçŸ©é˜µçš„åŒæ—¶å¯¹è§’åŒ–*ã€‚äº§ç”Ÿçš„ï¼ˆ <math alttext="lamda comma bold v"><mrow><mi>Î»</mi>
    <mo>,</mo> <mi>ğ¯</mi></mrow></math> ï¼‰å¯¹ä¸æ˜¯ä»…å±äºçŸ©é˜µ<math alttext="bold upper A"><mi>ğ€</mi></math>æˆ–çŸ©é˜µ<math
    alttext="bold upper B"><mi>ğ</mi></math>çš„ç‰¹å¾å€¼/ç‰¹å¾å‘é‡ã€‚ç›¸åï¼Œè¿™ä¸¤ä¸ªçŸ©é˜µå…±äº«ç‰¹å¾å€¼/ç‰¹å¾å‘é‡å¯¹ã€‚
- en: 'Conceptually, you can think of generalized eigendecomposition as the â€œregularâ€
    eigendecomposition of a product matrix:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æ¦‚å¿µä¸Šæ¥è¯´ï¼Œä½ å¯ä»¥å°†å¹¿ä¹‰ç‰¹å¾åˆ†è§£çœ‹ä½œæ˜¯ä¸€ä¸ªä¹˜ç§¯çŸ©é˜µçš„â€œå¸¸è§„â€ç‰¹å¾åˆ†è§£ï¼š
- en: <math alttext="StartLayout 1st Row 1st Column bold upper C 2nd Column equals
    bold upper A bold upper B Superscript negative 1 Baseline 2nd Row 1st Column bold
    upper C bold v 2nd Column equals lamda bold v EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>ğ‚</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ€</mi> <msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ‚</mi> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr></mtable></math>
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartLayout 1st Row 1st Column bold upper C 2nd Column equals
    bold upper A bold upper B Superscript negative 1 Baseline 2nd Row 1st Column bold
    upper C bold v 2nd Column equals lamda bold v EndLayout" display="block"><mtable
    displaystyle="true"><mtr><mtd columnalign="right"><mi>ğ‚</mi></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>ğ€</mi> <msup><mi>ğ</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></mtd></mtr>
    <mtr><mtd columnalign="right"><mrow><mi>ğ‚</mi> <mi>ğ¯</mi></mrow></mtd> <mtd columnalign="left"><mrow><mo>=</mo>
    <mi>Î»</mi> <mi>ğ¯</mi></mrow></mtd></mtr></mtable></math>
- en: This is just conceptual; in practice, generalized eigendecomposition does not
    require <math alttext="bold upper B"><mi>ğ</mi></math> to be invertible.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯æ¦‚å¿µæ€§çš„ï¼›åœ¨å®è·µä¸­ï¼Œå¹¿ä¹‰ç‰¹å¾åˆ†è§£ä¸è¦æ±‚çŸ©é˜µ<math alttext="bold upper B"><mi>ğ</mi></math>æ˜¯å¯é€†çš„ã€‚
- en: It is not the case that any two matrices can be simultaneously diagonalized.
    But this diagonalization is possible if <math alttext="bold upper B"><mi>ğ</mi></math>
    is positive (semi)definite.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: å¹¶éæ‰€æœ‰ä¸¤ä¸ªçŸ©é˜µéƒ½å¯ä»¥åŒæ—¶å¯¹è§’åŒ–ã€‚ä½†æ˜¯å¦‚æœ<math alttext="bold upper B"><mi>ğ</mi></math>æ˜¯æ­£å®šï¼ˆåŠï¼‰å®šçš„ï¼Œåˆ™è¿™ç§å¯¹è§’åŒ–æ˜¯å¯èƒ½çš„ã€‚
- en: 'NumPy does not compute generalized eigendecomposition, but SciPy does. If you
    know that the two matrices are symmetric, you can use the function `eigh`, which
    is more numerically stable:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: NumPyä¸è®¡ç®—å¹¿ä¹‰ç‰¹å¾åˆ†è§£ï¼Œä½†SciPyè®¡ç®—ã€‚å¦‚æœä½ çŸ¥é“è¿™ä¸¤ä¸ªçŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œä½ å¯ä»¥ä½¿ç”¨å‡½æ•°`eigh`ï¼Œè¿™æ›´åŠ æ•°å€¼ç¨³å®šï¼š
- en: '[PRE8]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Be mindful of the order of inputs: the second input is the one that is conceptually
    inverted.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¾“å…¥é¡ºåºæ—¶è¦å°å¿ƒï¼šæ¦‚å¿µä¸Šï¼Œç¬¬äºŒä¸ªè¾“å…¥æ˜¯è¢«å€’ç½®çš„ã€‚
- en: In data science, generalized eigendecomposition is used in classification analysis.
    In particular, Fisherâ€™s linear discriminant analysis is based on the generalized
    eigendecomposition of two data covariance matrices. Youâ€™ll see an example in [ChapterÂ 15](ch15.xhtml#Chapter_15).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œå¹¿ä¹‰ç‰¹å¾åˆ†è§£è¢«ç”¨äºåˆ†ç±»åˆ†æã€‚ç‰¹åˆ«æ˜¯ï¼Œè´¹èˆå°”çº¿æ€§åˆ¤åˆ«åˆ†æåŸºäºä¸¤ä¸ªæ•°æ®åæ–¹å·®çŸ©é˜µçš„å¹¿ä¹‰ç‰¹å¾åˆ†è§£ã€‚ä½ å°†åœ¨[ç¬¬ 15 ç« ](ch15.xhtml#Chapter_15)ä¸­çœ‹åˆ°ä¸€ä¸ªä¾‹å­ã€‚
- en: Summary
  id: totrans-208
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ€»ç»“
- en: 'That was quite a chapter! Here is a reminder of the key points:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç¡®å®æ˜¯ä¸€ç« ï¼è¿™é‡Œæ˜¯å…³é”®ç‚¹çš„æé†’ï¼š
- en: Eigendecomposition identifies *M* scalar/vector pairs of an <math alttext="upper
    M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo> <mi>M</mi></mrow></math> matrix.
    Those pairs of eigenvalue/eigenvector reflect special directions in the matrix
    and have myriad applications in data science (principal components analysis being
    a common one), as well as in geometry, physics, computational biology, and myriad
    other technical disciplines.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾åˆ†è§£è¯†åˆ«*M*ä¸ªæ ‡é‡/å‘é‡å¯¹äºä¸€ä¸ª<math alttext="upper M times upper M"><mrow><mi>M</mi> <mo>Ã—</mo>
    <mi>M</mi></mrow></math>çŸ©é˜µã€‚è¿™äº›ç‰¹å¾å€¼/ç‰¹å¾å‘é‡å¯¹åæ˜ äº†çŸ©é˜µä¸­çš„ç‰¹æ®Šæ–¹å‘ï¼Œå¹¶åœ¨æ•°æ®ç§‘å­¦ï¼ˆä¸»æˆåˆ†åˆ†ææ˜¯ä¸€ä¸ªå¸¸è§åº”ç”¨ï¼‰ã€å‡ ä½•å­¦ã€ç‰©ç†å­¦ã€è®¡ç®—ç”Ÿç‰©å­¦ç­‰æŠ€æœ¯å­¦ç§‘ä¸­æœ‰ç€å¹¿æ³›çš„åº”ç”¨ã€‚
- en: Eigenvalues are found by assuming that the matrix shifted by an unknown scalar
    <math alttext="lamda"><mi>Î»</mi></math> is singular, setting its determinant to
    zero (called the *characteristic polynomial*), and solving for the <math alttext="lamda"><mi>Î»</mi></math>
    s.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾å€¼é€šè¿‡å‡è®¾ç”±æœªçŸ¥æ ‡é‡<math alttext="lamda"><mi>Î»</mi></math> è½¬ç§»çš„çŸ©é˜µæ˜¯å¥‡å¼‚çš„ï¼Œå°†å…¶è¡Œåˆ—å¼è®¾ä¸ºé›¶ï¼ˆç§°ä¸º*ç‰¹å¾å¤šé¡¹å¼*ï¼‰ï¼Œå¹¶è§£å‡º<math
    alttext="lamda"><mi>Î»</mi></math> sæ¥æ‰¾åˆ°ã€‚
- en: Eigenvectors are found by finding the basis vector for the null space of the
    <math alttext="lamda"><mi>Î»</mi></math> -shifted matrix.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾å‘é‡é€šè¿‡å¯»æ‰¾<math alttext="lamda"><mi>Î»</mi></math> -è½¬ç§»çŸ©é˜µçš„é›¶ç©ºé—´çš„åŸºå‘é‡æ¥æ‰¾åˆ°ã€‚
- en: '*Diagonalizing a matrix* means to represent the matrix as <math alttext="bold
    upper V Superscript negative 1 Baseline bold upper Lamda bold upper V"><mrow><msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mi mathvariant="bold">Î›</mi><mi
    mathvariant="bold">V</mi></mrow></math>, where <math alttext="bold upper V"><mi>ğ•</mi></math>
    is a matrix with eigenvectors in the columns and <math alttext="bold upper Lamda"><mi
    mathvariant="bold">Î›</mi></math> is a diagonal matrix with eigenvalues in the
    diagonal elements.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*å¯¹è§’åŒ–çŸ©é˜µ*æ„å‘³ç€å°†çŸ©é˜µè¡¨ç¤ºä¸º<math alttext="bold upper V Superscript negative 1 Baseline
    bold upper Lamda bold upper V"><mrow><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup>
    <mi mathvariant="bold">Î›</mi><mi mathvariant="bold">V</mi></mrow></math>ï¼Œå…¶ä¸­<math
    alttext="bold upper V"><mi>ğ•</mi></math>æ˜¯åˆ—ä¸­åŒ…å«ç‰¹å¾å‘é‡çš„çŸ©é˜µï¼Œ<math alttext="bold upper
    Lamda"><mi mathvariant="bold">Î›</mi></math>æ˜¯å¯¹è§’çº¿ä¸ŠåŒ…å«ç‰¹å¾å€¼çš„å¯¹è§’çŸ©é˜µã€‚'
- en: Symmetric matrices have several special properties in eigendecomposition; the
    most relevant for data science is that all eigenvectors are pair-wise orthogonal.
    This means that the matrix of eigenvectors is an orthogonal matrix (when the eigenvectors
    are unit normalized), which in turn means that the inverse of the eigenvectors
    matrix is its transpose.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å¯¹ç§°çŸ©é˜µåœ¨ç‰¹å¾åˆ†è§£ä¸­å…·æœ‰å‡ ä¸ªç‰¹æ®Šæ€§è´¨ï¼›åœ¨æ•°æ®ç§‘å­¦ä¸­æœ€ç›¸å…³çš„æ˜¯æ‰€æœ‰ç‰¹å¾å‘é‡æ˜¯ä¸¤ä¸¤æ­£äº¤çš„ã€‚è¿™æ„å‘³ç€ç‰¹å¾å‘é‡çŸ©é˜µæ˜¯æ­£äº¤çŸ©é˜µï¼ˆå½“ç‰¹å¾å‘é‡å•ä½å½’ä¸€åŒ–æ—¶ï¼‰ï¼Œè¿›è€Œæ„å‘³ç€ç‰¹å¾å‘é‡çŸ©é˜µçš„é€†æ˜¯å…¶è½¬ç½®ã€‚
- en: The *definiteness* of a matrix refers to the signs of its eigenvalues. In data
    science, the most relevant categories are positive (semi)definite, which means
    that all eigenvalues are either nonnegative or positive.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: çŸ©é˜µçš„*å®šæ€§*æŒ‡çš„æ˜¯å…¶ç‰¹å¾å€¼çš„ç¬¦å·ã€‚åœ¨æ•°æ®ç§‘å­¦ä¸­ï¼Œæœ€ç›¸å…³çš„ç±»åˆ«æ˜¯æ­£ï¼ˆåŠï¼‰å®šçš„ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰ç‰¹å¾å€¼è¦ä¹ˆéè´Ÿè¦ä¹ˆæ­£ã€‚
- en: A matrix times its transpose is always positive (semi)definite, which means
    all covariance matrices have nonnegative eigenvalues.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä¸€ä¸ªçŸ©é˜µä¹˜ä»¥å…¶è½¬ç½®å§‹ç»ˆæ˜¯æ­£ï¼ˆåŠï¼‰å®šçš„ï¼Œè¿™æ„å‘³ç€æ‰€æœ‰åæ–¹å·®çŸ©é˜µéƒ½å…·æœ‰éè´Ÿç‰¹å¾å€¼ã€‚
- en: The study of eigendecomposition is rich and detailed, and many fascinating subtleties,
    special cases, and applications have been discovered. I hope that the overview
    in this chapter provides a solid grounding for your needs as a data scientist,
    and may have inspired you to learn more about the fantastic beauty of eigendecomposition.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ç‰¹å¾åˆ†è§£çš„ç ”ç©¶å†…å®¹ä¸°å¯Œè€Œè¯¦ç»†ï¼Œå·²ç»å‘ç°äº†è®¸å¤šè¿·äººçš„ç»†èŠ‚ã€ç‰¹æ®Šæƒ…å†µå’Œåº”ç”¨ã€‚å¸Œæœ›æœ¬ç« æ¦‚è¿°èƒ½ä¸ºæ‚¨ä½œä¸ºæ•°æ®ç§‘å­¦å®¶çš„éœ€æ±‚æä¾›åšå®çš„åŸºç¡€ï¼Œå¹¶å¯èƒ½æ¿€å‘æ‚¨è¿›ä¸€æ­¥äº†è§£ç‰¹å¾åˆ†è§£çš„å¥‡å¦™ä¹‹ç¾ã€‚
- en: Code Exercises
  id: totrans-218
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç ç»ƒä¹ 
- en: Exercise 13-1\.
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  13-1\.
- en: Interestingly, the eigenvectors of <math alttext="bold upper A Superscript negative
    1"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> are the same
    as the eigenvectors of <math alttext="bold upper A"><mi>ğ€</mi></math> while the
    eigenvalues are <math alttext="lamda Superscript negative 1"><msup><mi>Î»</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> . Prove that this is the case
    by writing out the eigendecomposition of <math alttext="bold upper A"><mi>ğ€</mi></math>
    and <math alttext="bold upper A Superscript negative 1"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    . Then illustrate it using a random full-rank <math alttext="5 times 5"><mrow><mn>5</mn>
    <mo>Ã—</mo> <mn>5</mn></mrow></math> symmetric matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: æœ‰è¶£çš„æ˜¯ï¼Œ<math alttext="bold upper A Superscript negative 1"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    çš„ç‰¹å¾å‘é‡ä¸<math alttext="bold upper A"><mi>ğ€</mi></math> çš„ç‰¹å¾å‘é‡ç›¸åŒï¼Œè€Œç‰¹å¾å€¼æ˜¯<math alttext="lamda
    Superscript negative 1"><msup><mi>Î»</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>ã€‚é€šè¿‡å†™å‡º<math
    alttext="bold upper A"><mi>ğ€</mi></math> å’Œ<math alttext="bold upper A Superscript
    negative 1"><msup><mi>ğ€</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math> çš„ç‰¹å¾åˆ†è§£æ¥è¯æ˜è¿™ä¸€ç‚¹ã€‚ç„¶åä½¿ç”¨ä¸€ä¸ªéšæœºçš„å…¨ç§©<math
    alttext="5 times 5"><mrow><mn>5</mn> <mo>Ã—</mo> <mn>5</mn></mrow></math> å¯¹ç§°çŸ©é˜µè¿›è¡Œè¯´æ˜ã€‚
- en: Exercise 13-2\.
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  13-2\.
- en: 'Re-create the left-side panel of [FigureÂ 13-1](#fig_13_1), but using the *rows*
    of <math alttext="bold upper V"><mi>ğ•</mi></math> instead of *columns*. Of course
    you know that this is a coding error, but the results are insightful: it fails
    the geometry test that the matrix times its eigenvector only stretches.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: é‡æ–°åˆ›å»º[FigureÂ 13-1](#fig_13_1)çš„å·¦ä¾§é¢æ¿ï¼Œä½†ä½¿ç”¨<math alttext="bold upper V"><mi>ğ•</mi></math>çš„*è¡Œ*è€Œä¸æ˜¯*åˆ—*ã€‚å½“ç„¶ä½ çŸ¥é“è¿™æ˜¯ä¸€ä¸ªç¼–ç é”™è¯¯ï¼Œä½†ç»“æœå¾ˆæœ‰è§åœ°ï¼šå®ƒæœªèƒ½é€šè¿‡çŸ©é˜µä¹˜ä»¥å…¶ç‰¹å¾å‘é‡ä»…è¿›è¡Œæ‹‰ä¼¸çš„å‡ ä½•æµ‹è¯•ã€‚
- en: Exercise 13-3\.
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-3ã€‚
- en: The goal of this exercise is to demonstrate that eigenvalues are inextricably
    coupled to their eigenvectors. Diagonalize a symmetric random-integers matrix^([7](ch13.xhtml#idm45733291700640))
    created using the additive method (see [Exercise 5-9](ch05.xhtml#exercise_5_9)),
    but randomly reorder the eigenvalues (letâ€™s call this matrix <math alttext="bold
    upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Î›</mi> <mo>Ëœ</mo></mover></math>)
    without reordering the eigenvectors.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é¡¹ç»ƒä¹ çš„ç›®æ ‡æ˜¯å±•ç¤ºç‰¹å¾å€¼ä¸å®ƒä»¬çš„ç‰¹å¾å‘é‡å¯†åˆ‡ç›¸å…³ã€‚ä½¿ç”¨åŠ æ³•æ–¹æ³•åˆ›å»ºå¯¹ç§°éšæœºæ•´æ•°çŸ©é˜µ^([7](ch13.xhtml#idm45733291700640))ï¼ˆå‚è§[Exercise
    5-9](ch05.xhtml#exercise_5_9)ï¼‰ï¼Œä½†éšæœºé‡æ–°æ’åˆ—ç‰¹å¾å€¼ï¼ˆæˆ‘ä»¬å°†è¿™ä¸ªçŸ©é˜µç§°ä¸º<math alttext="bold upper Lamda
    overTilde"><mover accent="true"><mi mathvariant="bold">Î›</mi> <mo>Ëœ</mo></mover></math>ï¼‰ï¼Œè€Œä¸é‡æ–°æ’åˆ—ç‰¹å¾å‘é‡ã€‚
- en: First, demonstrate that you can reconstruct the original matrix as <math alttext="bold
    upper V bold upper Lamda bold upper V Superscript negative 1"><mrow><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Î›</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    . You can compute reconstruction accuracy as the Frobenius distance between the
    original and reconstructed matrix. Next, attempt to reconstruct the matrix using
    <math alttext="bold upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Î›</mi>
    <mo>Ëœ</mo></mover></math>. How close is the reconstructed matrix to the original?
    What happens if you only swap the two largest eigenvalues instead of randomly
    reordering them? How about the two smallest eigenvalues?
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè¯æ˜ä½ èƒ½å¤Ÿå°†åŸå§‹çŸ©é˜µé‡æ„ä¸º<math alttext="bold upper V bold upper Lamda bold upper V Superscript
    negative 1"><mrow><mi mathvariant="bold">V</mi><mi mathvariant="bold">Î›</mi><msup><mi
    mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>ã€‚ä½ å¯ä»¥é€šè¿‡è®¡ç®—åŸå§‹çŸ©é˜µå’Œé‡æ„çŸ©é˜µä¹‹é—´çš„Frobeniusè·ç¦»æ¥è¯„ä¼°é‡æ„ç²¾åº¦ã€‚æ¥ä¸‹æ¥ï¼Œå°è¯•ä½¿ç”¨<math
    alttext="bold upper Lamda overTilde"><mover accent="true"><mi mathvariant="bold">Î›</mi>
    <mo>Ëœ</mo></mover></math>é‡æ„çŸ©é˜µã€‚é‡æ„çŸ©é˜µä¸åŸå§‹çŸ©é˜µæœ‰å¤šæ¥è¿‘ï¼Ÿå¦‚æœä»…äº¤æ¢ä¸¤ä¸ªæœ€å¤§çš„ç‰¹å¾å€¼è€Œä¸æ˜¯éšæœºé‡æ–°æ’åºå‘¢ï¼Ÿé‚£ä¹ˆæœ€å°çš„ä¸¤ä¸ªç‰¹å¾å€¼å‘¢ï¼Ÿ
- en: Finally, create a bar plot showing the Frobenius distances to the original matrix
    for the different swapping options ([FigureÂ 13-3](#fig_13_3)). (Of course, because
    of the random matricesâ€”and thus, random eigenvaluesâ€”your plot wonâ€™t look exactly
    like mine.)
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œåˆ›å»ºä¸€ä¸ªæ¡å½¢å›¾ï¼Œæ˜¾ç¤ºä¸åŒäº¤æ¢é€‰é¡¹å¯¹äºåŸå§‹çŸ©é˜µçš„Frobeniusè·ç¦»ï¼ˆ[FigureÂ 13-3](#fig_13_3)ï¼‰ã€‚å½“ç„¶ï¼Œç”±äºéšæœºçŸ©é˜µâ€”â€”å› æ­¤ï¼Œéšæœºç‰¹å¾å€¼â€”â€”ä½ çš„å›¾è¡¨çœ‹èµ·æ¥ä¸ä¼šå®Œå…¨ä¸æˆ‘çš„ç›¸åŒã€‚
- en: '![exercise 13-3](assets/plad_1303.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 13-3](assets/plad_1303.png)'
- en: Figure 13-3\. Results of Exercise 13-3
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾13-3ã€‚Exercise 13-3çš„ç»“æœ
- en: Exercise 13-4\.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-4ã€‚
- en: One interesting property of random matrices is that their complex-valued eigenvalues
    are distributed in a circle with a radius proportional to the size of the matrix.
    To demonstrate this, compute 123 random <math alttext="42 times 42"><mrow><mn>42</mn>
    <mo>Ã—</mo> <mn>42</mn></mrow></math> matrices, extract their eigenvalues, divide
    by the square root of the matrix size (42), and plot the eigenvalues on the complex
    plane, as in [FigureÂ 13-4](#fig_13_4).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: éšæœºçŸ©é˜µçš„ä¸€ä¸ªæœ‰è¶£æ€§è´¨æ˜¯å®ƒä»¬çš„å¤å€¼ç‰¹å¾å€¼åœ¨ä¸€ä¸ªåŠå¾„ä¸çŸ©é˜µå¤§å°æˆæ¯”ä¾‹çš„åœ†å†…åˆ†å¸ƒã€‚ä¸ºäº†è¯æ˜è¿™ä¸€ç‚¹ï¼Œè®¡ç®—123ä¸ªéšæœº<math alttext="42 times
    42"><mrow><mn>42</mn> <mo>Ã—</mo> <mn>42</mn></mrow></math>çŸ©é˜µï¼Œæå–å®ƒä»¬çš„ç‰¹å¾å€¼ï¼Œé™¤ä»¥çŸ©é˜µå¤§å°çš„å¹³æ–¹æ ¹ï¼ˆ42ï¼‰ï¼Œå¹¶åœ¨å¤å¹³é¢ä¸Šç»˜åˆ¶ç‰¹å¾å€¼ï¼Œå¦‚[FigureÂ 13-4](#fig_13_4)ä¸­æ‰€ç¤ºã€‚
- en: '![exercise 13-4](assets/plad_1304.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![exercise 13-4](assets/plad_1304.png)'
- en: Figure 13-4\. Results of Exercise 13-4
  id: totrans-232
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾13-4ã€‚Exercise 13-4çš„ç»“æœ
- en: Exercise 13-5\.
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-5ã€‚
- en: This exercise will help you better understand that an eigenvector is the basis
    for the null space of the eigenvalue-shifted matrixâ€”and it will also reveal the
    risks of numerical precision errors. Eigendecompose a random <math alttext="3
    times 3"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>3</mn></mrow></math> symmetric matrix.
    Then for each eigenvalue, use `scipy.linalg.null_space()` to find a basis vector
    for the null space of each shifted matrix. Are those vectors the same as the eigenvectors?
    Note that you might need to take into consideration the norms and the sign indeterminacies
    of eigenvectors.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ å°†å¸®åŠ©ä½ æ›´å¥½åœ°ç†è§£ç‰¹å¾å‘é‡æ˜¯ç‰¹å¾å€¼åç§»çŸ©é˜µçš„é›¶ç©ºé—´çš„åŸºç¡€ï¼Œä¹Ÿå°†æ­ç¤ºæ•°å€¼ç²¾åº¦è¯¯å·®çš„é£é™©ã€‚ç‰¹å¾åˆ†è§£ä¸€ä¸ªéšæœº <math alttext="3 times
    3"><mrow><mn>3</mn> <mo>Ã—</mo> <mn>3</mn></mrow></math> å¯¹ç§°çŸ©é˜µã€‚ç„¶åå¯¹äºæ¯ä¸ªç‰¹å¾å€¼ï¼Œä½¿ç”¨ `scipy.linalg.null_space()`
    æ‰¾åˆ°æ¯ä¸ªåç§»çŸ©é˜µçš„é›¶ç©ºé—´çš„åŸºå‘é‡ã€‚è¿™äº›å‘é‡æ˜¯å¦ä¸ç‰¹å¾å‘é‡ç›¸åŒï¼Ÿè¯·æ³¨æ„ï¼Œä½ å¯èƒ½éœ€è¦è€ƒè™‘ç‰¹å¾å‘é‡çš„èŒƒæ•°å’Œç¬¦å·ä¸ç¡®å®šæ€§ã€‚
- en: When you run the code multiple times for different random matrices, you are
    likely to get Python errors. The error comes from an empty null space for the
    <math alttext="lamda"><mi>Î»</mi></math> -shifted matrix, which, upon investigation,
    comes from the shifted matrix being full rank. (Donâ€™t take my word for it; confirm
    this yourself!) That is not supposed to happen, which highlightsâ€”yet againâ€”that
    (1) finite-precision math on computers does not always conform to chalkboard math
    and (2) you should use the targeted and more numerically stable functions instead
    of trying to make direct translations of formulas into code.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ä½ å¤šæ¬¡è¿è¡Œä»£ç ä½¿ç”¨ä¸åŒçš„éšæœºçŸ©é˜µæ—¶ï¼Œä½ å¯èƒ½ä¼šé‡åˆ° Python é”™è¯¯ã€‚é”™è¯¯æ¥è‡ªäºåç§»çŸ©é˜µä¸ºç©ºé›¶ç©ºé—´ï¼Œè¿™ä¸ªé—®é¢˜åœ¨äºåç§»çŸ©é˜µæ˜¯æ»¡ç§©çš„ã€‚ ï¼ˆä¸è¦åªå¬æˆ‘çš„è¯ï¼Œè‡ªå·±ç¡®è®¤ä¸€ä¸‹ï¼ï¼‰è¿™æ˜¯ä¸åº”è¯¥å‘ç”Ÿçš„ï¼Œè¿™å†æ¬¡çªæ˜¾äº†ï¼ˆ1ï¼‰è®¡ç®—æœºä¸Šçš„æœ‰é™ç²¾åº¦æ•°å­¦å¹¶ä¸æ€»æ˜¯ç¬¦åˆé»‘æ¿æ•°å­¦å’Œï¼ˆ2ï¼‰ä½ åº”è¯¥ä½¿ç”¨æœ‰é’ˆå¯¹æ€§ä¸”æ›´ç¨³å®šçš„å‡½æ•°ï¼Œè€Œä¸æ˜¯å°è¯•ç›´æ¥å°†å…¬å¼ç¿»è¯‘æˆä»£ç ã€‚
- en: Exercise 13-6\.
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-6\.
- en: Iâ€™m going to teach you a third method to create random symmetric matrices.^([8](ch13.xhtml#idm45733291670912))
    Start by creating a <math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo> <mn>4</mn></mrow></math>
    diagonal matrix with positive numbers on the diagonals (they can be, for example,
    the numbers 1, 2, 3, 4). Then create a <math alttext="4 times 4"><mrow><mn>4</mn>
    <mo>Ã—</mo> <mn>4</mn></mrow></math> <math alttext="bold upper Q"><mi>ğ</mi></math>
    matrix from the QR decomposition of a random-numbers matrix. Use these matrices
    as the eigenvalues and eigenvectors, and multiply them appropriately to assemble
    a matrix. Confirm that the assembled matrix is symmetric, and that its eigenvalues
    equal the eigenvalues you specified.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å°†æ•™ä½ ç¬¬ä¸‰ç§æ–¹æ³•æ¥åˆ›å»ºéšæœºå¯¹ç§°çŸ©é˜µã€‚^([8](ch13.xhtml#idm45733291670912)) é¦–å…ˆåˆ›å»ºä¸€ä¸ªå¯¹è§’çº¿ä¸Šæœ‰æ­£æ•°ï¼ˆä¾‹å¦‚ï¼Œå¯ä»¥æ˜¯æ•°å­—
    1ã€2ã€3ã€4ï¼‰çš„ <math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo> <mn>4</mn></mrow></math>
    å¯¹è§’çŸ©é˜µã€‚ç„¶åä»ä¸€ä¸ªéšæœºæ•°çŸ©é˜µçš„QRåˆ†è§£ä¸­åˆ›å»ºä¸€ä¸ª <math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo>
    <mn>4</mn></mrow></math> çš„ <math alttext="bold upper Q"><mi>ğ</mi></math> çŸ©é˜µã€‚å°†è¿™äº›çŸ©é˜µç”¨ä½œç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡ï¼Œå¹¶é€‚å½“åœ°ç›¸ä¹˜ä»¥ç»„è£…ä¸€ä¸ªçŸ©é˜µã€‚ç¡®è®¤ç»„è£…çš„çŸ©é˜µæ˜¯å¯¹ç§°çš„ï¼Œå¹¶ä¸”å…¶ç‰¹å¾å€¼ç­‰äºä½ æŒ‡å®šçš„ç‰¹å¾å€¼ã€‚
- en: Exercise 13-7\.
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-7\.
- en: Letâ€™s revisit [Exercise 12-4](ch12.xhtml#exercise_12_4). Redo that exercise
    but use the average of the eigenvalues instead of the squared Frobenius norm of
    the design matrix (this is known as *shrinkage regularization*). How does the
    resulting figure compare with that from [ChapterÂ 12](ch12.xhtml#Chapter_12)?
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é‡æ–°å®¡è§†[ç»ƒä¹  12-4](ch12.xhtml#exercise_12_4)ã€‚é‡æ–°è¿›è¡Œè¯¥ç»ƒä¹ ï¼Œä½†ä½¿ç”¨ç‰¹å¾å€¼çš„å¹³å‡æ•°è€Œä¸æ˜¯è®¾è®¡çŸ©é˜µçš„å¹³æ–¹FrobeniusèŒƒæ•°ï¼ˆè¿™è¢«ç§°ä¸º*æ”¶ç¼©æ­£åˆ™åŒ–*ï¼‰ã€‚ç»“æœå›¾ä¸[ç¬¬
    12 ç« ](ch12.xhtml#Chapter_12)çš„å›¾ç›¸æ¯”å¦‚ä½•ï¼Ÿ
- en: Exercise 13-8\.
  id: totrans-240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Exercise 13-8\.
- en: 'This and the following exercise are closely linked. We will create surrogate
    data with a specified correlation matrix (this exercise), and then remove the
    correlation (next exercise). The formula to create data with a specified correlation
    structure is:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªç»ƒä¹ ä¸ä¸‹ä¸€ä¸ªç»ƒä¹ å¯†åˆ‡ç›¸å…³ã€‚æˆ‘ä»¬å°†åˆ›å»ºå…·æœ‰æŒ‡å®šç›¸å…³çŸ©é˜µçš„æ¨¡æ‹Ÿæ•°æ®ï¼ˆè¿™ä¸ªç»ƒä¹ ï¼‰ï¼Œç„¶åç§»é™¤ç›¸å…³æ€§ï¼ˆä¸‹ä¸€ä¸ªç»ƒä¹ ï¼‰ã€‚åˆ›å»ºå…·æœ‰æŒ‡å®šç›¸å…³ç»“æ„æ•°æ®çš„å…¬å¼æ˜¯ï¼š
- en: <math alttext="bold upper Y equals bold upper V bold upper Lamda Superscript
    1 slash 2 Baseline bold upper X" display="block"><mrow><mi mathvariant="bold">Y</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Î›</mi> <mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup>
    <mi mathvariant="bold">X</mi></mrow></math>
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Y equals bold upper V bold upper Lamda Superscript
    1 slash 2 Baseline bold upper X" display="block"><mrow><mi mathvariant="bold">Y</mi>
    <mo>=</mo> <mi mathvariant="bold">V</mi> <msup><mi mathvariant="bold">Î›</mi> <mrow><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup>
    <mi mathvariant="bold">X</mi></mrow></math>
- en: where <math alttext="bold upper V"><mi>ğ•</mi></math> and <math alttext="bold
    upper Lamda"><mi mathvariant="bold">Î›</mi></math> are the eigenvectors and eigenvalues
    of a correlation matrix, and <math alttext="bold upper X"><mi>ğ—</mi></math> is
    an <math alttext="upper N times upper T"><mrow><mi>N</mi> <mo>Ã—</mo> <mi>T</mi></mrow></math>
    matrix of uncorrelated random numbers (*N* channels and *T* time points).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­ <math alttext="bold upper V"><mi>ğ•</mi></math> å’Œ <math alttext="bold upper
    Lamda"><mi mathvariant="bold">Î›</mi></math> æ˜¯ç›¸å…³çŸ©é˜µçš„ç‰¹å¾å‘é‡å’Œç‰¹å¾å€¼ï¼Œ<math alttext="bold
    upper X"><mi>ğ—</mi></math> æ˜¯ä¸€ä¸ªæœªç›¸å…³éšæœºæ•°ï¼ˆ*N* é€šé“å’Œ *T* æ—¶é—´ç‚¹ï¼‰çš„ <math alttext="upper N
    times upper T"><mrow><mi>N</mi> <mo>Ã—</mo> <mi>T</mi></mrow></math> çŸ©é˜µã€‚
- en: 'Apply that formula to create a 3 Ã— 10,000 data matrix <math alttext="bold upper
    Y"><mi>ğ˜</mi></math> with the following correlation structure:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: åº”ç”¨è¯¥å…¬å¼åˆ›å»ºä¸€ä¸ª 3 Ã— 10,000 çš„æ•°æ®çŸ©é˜µ <math alttext="bold upper Y"><mi>ğ˜</mi></math>ï¼Œå…·æœ‰ä»¥ä¸‹ç›¸å…³ç»“æ„ï¼š
- en: <math display="block"><mrow><mi>ğ‘</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mrow><mn>.9</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mn>1</mn></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.9</mn></mrow></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mrow></math>
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>ğ‘</mi> <mo>=</mo> <mfenced close="]" open="["><mtable><mtr><mtd><mn>1</mn></mtd>
    <mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mrow><mn>.9</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.2</mn></mrow></mtd> <mtd><mn>1</mn></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd></mtr>
    <mtr><mtd><mrow><mn>.9</mn></mrow></mtd> <mtd><mrow><mn>.3</mn></mrow></mtd> <mtd><mn>1</mn></mtd></mtr></mtable></mfenced></mrow></math>
- en: Then compute the empirical correlation matrix of the data matrix <math alttext="bold
    upper X"><mi>ğ—</mi></math> . It wonâ€™t exactly equal <math alttext="bold upper
    R"><mi>ğ‘</mi></math> because we are randomly sampling a finite dataset. But it
    should be fairly close (e.g., within .01).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶åè®¡ç®—æ•°æ®çŸ©é˜µ <math alttext="bold upper X"><mi>ğ—</mi></math> çš„ç»éªŒç›¸å…³çŸ©é˜µã€‚å®ƒä¸ä¼šå®Œå…¨ç­‰äº <math
    alttext="bold upper R"><mi>ğ‘</mi></math>ï¼Œå› ä¸ºæˆ‘ä»¬æ˜¯ä»æœ‰é™æ•°æ®é›†ä¸­éšæœºæŠ½æ ·çš„ã€‚ä½†åº”è¯¥éå¸¸æ¥è¿‘ï¼ˆä¾‹å¦‚ï¼Œè¯¯å·®åœ¨ 0.01
    èŒƒå›´å†…ï¼‰ã€‚
- en: Exercise 13-9\.
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  13-9\.
- en: 'Now letâ€™s remove those imposed correlations by *whitening*. Whitening is a
    term in signal and image processing to remove correlations. A multivariate time
    series can be whitened by implementing the following formula:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬é€šè¿‡ *ç™½åŒ–* æ¥æ¶ˆé™¤è¿™äº›å¼ºåŠ çš„ç›¸å…³æ€§ã€‚ç™½åŒ–æ˜¯ä¿¡å·å’Œå›¾åƒå¤„ç†ä¸­çš„æœ¯è¯­ï¼Œç”¨äºæ¶ˆé™¤ç›¸å…³æ€§ã€‚å¯ä»¥é€šè¿‡å®æ–½ä»¥ä¸‹å…¬å¼å¯¹å¤šå˜é‡æ—¶é—´åºåˆ—è¿›è¡Œç™½åŒ–ï¼š
- en: <math alttext="bold upper Y overTilde equals bold upper Y Superscript upper
    T Baseline bold upper V bold upper Lamda Superscript negative 1 slash 2" display="block"><mrow><mover
    accent="true"><mi mathvariant="bold">Y</mi> <mo>Ëœ</mo></mover> <mo>=</mo> <msup><mi
    mathvariant="bold">Y</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi>
    <msup><mi mathvariant="bold">Î›</mi> <mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></math>
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="bold upper Y overTilde equals bold upper Y Superscript upper
    T Baseline bold upper V bold upper Lamda Superscript negative 1 slash 2" display="block"><mrow><mover
    accent="true"><mi mathvariant="bold">Y</mi> <mo>Ëœ</mo></mover> <mo>=</mo> <msup><mi
    mathvariant="bold">Y</mi> <mtext>T</mtext></msup> <mi mathvariant="bold">V</mi>
    <msup><mi mathvariant="bold">Î›</mi> <mrow><mo>-</mo><mn>1</mn><mo>/</mo><mn>2</mn></mrow></msup></mrow></math>
- en: Apply that formula to the data matrix from the previous exercise, and confirm
    that the correlation matrix is the identity matrix (again, within some tolerance
    for random sampling).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å°†è¯¥å…¬å¼åº”ç”¨äºä¸Šä¸€ä¸ªç»ƒä¹ çš„æ•°æ®çŸ©é˜µï¼Œå¹¶ç¡®è®¤ç›¸å…³çŸ©é˜µæ˜¯å•ä½çŸ©é˜µï¼ˆå†æ¬¡å¼ºè°ƒï¼Œå¯¹äºéšæœºæŠ½æ ·ï¼Œè¯¯å·®åœ¨æŸä¸ªå®¹å·®èŒƒå›´å†…ï¼‰ã€‚
- en: Exercise 13-10\.
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  13-10\.
- en: In generalized eigendecomposition, the eigenvectors are not orthogonal, even
    when both matrices are symmetric. Confirm in Python that <math alttext="bold upper
    V Superscript negative 1 Baseline not-equals bold upper V Superscript upper T"><mrow><msup><mi>ğ•</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>â‰ </mo> <msup><mi>ğ•</mi> <mtext>T</mtext></msup></mrow></math>
    . This happens because although both <math alttext="bold upper A"><mi>ğ€</mi></math>
    and <math alttext="bold upper B"><mi>ğ</mi></math> are symmetric, <math alttext="bold
    upper C equals bold upper A bold upper B"><mrow><mi>ğ‚</mi> <mo>=</mo> <mi>ğ€</mi>
    <mi>ğ</mi></mrow></math> is not symmetric.^([9](ch13.xhtml#idm45733291596240))
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¹¿ä¹‰ç‰¹å¾åˆ†è§£ä¸­ï¼Œå³ä½¿ä¸¤ä¸ªçŸ©é˜µéƒ½æ˜¯å¯¹ç§°çš„ï¼Œç‰¹å¾å‘é‡ä¹Ÿä¸æ˜¯æ­£äº¤çš„ã€‚åœ¨ Python ä¸­ç¡®è®¤ <math alttext="bold upper V Superscript
    negative 1 Baseline not-equals bold upper V Superscript upper T"><mrow><msup><mi>ğ•</mi>
    <mrow><mo>-</mo><mn>1</mn></mrow></msup> <mo>â‰ </mo> <msup><mi>ğ•</mi> <mtext>T</mtext></msup></mrow></math>
    ã€‚è¿™æ˜¯å› ä¸ºè™½ç„¶ <math alttext="bold upper A"><mi>ğ€</mi></math> å’Œ <math alttext="bold upper
    B"><mi>ğ</mi></math> éƒ½æ˜¯å¯¹ç§°çš„ï¼Œä½† <math alttext="bold upper C equals bold upper A bold
    upper B"><mrow><mi>ğ‚</mi> <mo>=</mo> <mi>ğ€</mi> <mi>ğ</mi></mrow></math> ä¸æ˜¯å¯¹ç§°çš„ã€‚^([9](ch13.xhtml#idm45733291596240))
- en: However, the eigenvectors are orthogonal with respect to <math alttext="bold
    upper B"><mi>ğ</mi></math> , which means that <math alttext="bold upper V Superscript
    upper T Baseline bold upper B bold upper V equals bold upper I"><mrow><msup><mi>ğ•</mi>
    <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ•</mi> <mo>=</mo> <mi>ğˆ</mi></mrow></math>
    . Confirm these properties by performing a generalized eigendecomposition on two
    symmetric matrices, and producing [FigureÂ 13-5](#fig_13_5).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ç„¶è€Œï¼Œç‰¹å¾å‘é‡åœ¨ <math alttext="bold upper B"><mi>ğ</mi></math> æ–¹é¢æ˜¯æ­£äº¤çš„ï¼Œè¿™æ„å‘³ç€ <math alttext="bold
    upper V Superscript upper T Baseline bold upper B bold upper V equals bold upper
    I"><mrow><msup><mi>ğ•</mi> <mtext>T</mtext></msup> <mi>ğ</mi> <mi>ğ•</mi> <mo>=</mo>
    <mi>ğˆ</mi></mrow></math> ã€‚é€šè¿‡å¯¹ä¸¤ä¸ªå¯¹ç§°çŸ©é˜µæ‰§è¡Œå¹¿ä¹‰ç‰¹å¾åˆ†è§£å¹¶ç”Ÿæˆ [å›¾ 13-5](#fig_13_5) æ¥ç¡®è®¤è¿™äº›å±æ€§ã€‚
- en: '![exercise 13-10](assets/plad_1305.png)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  13-10](assets/plad_1305.png)'
- en: Figure 13-5\. Results of Exercise 13-10
  id: totrans-255
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 13-5\. ç»ƒä¹  13-10 çš„ç»“æœ
- en: Exercise 13-11\.
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  13-11\.
- en: 'Letâ€™s explore the scaling of eigenvectors. Start by creating a <math alttext="4
    times 4"><mrow><mn>4</mn> <mo>Ã—</mo> <mn>4</mn></mrow></math> matrix of random
    integers drawn between âˆ’14 and +14\. Diagonalize the matrix and empirically confirm
    that <math alttext="bold upper A equals bold upper V bold upper Lamda bold upper
    V Superscript negative 1"><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Î›</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    . Confirm that the Euclidean norm of each eigenvector equals 1\. Note that the
    square of a complex number is computed as that number times its complex conjugate
    (hint: use `np.conj()`).'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ¢ç´¢ç‰¹å¾å‘é‡çš„ç¼©æ”¾ã€‚é¦–å…ˆåˆ›å»ºä¸€ä¸ªéšæœºæ•´æ•°çŸ©é˜µï¼Œå…¶å¤§å°ä¸º <math alttext="4 times 4"><mrow><mn>4</mn> <mo>Ã—</mo>
    <mn>4</mn></mrow></math>ï¼Œå–å€¼èŒƒå›´åœ¨ -14 åˆ° +14 ä¹‹é—´ã€‚å¯¹è¯¥çŸ©é˜µè¿›è¡Œå¯¹è§’åŒ–ï¼Œå¹¶é€šè¿‡å®éªŒè¯å® <math alttext="bold
    upper A equals bold upper V bold upper Lamda bold upper V Superscript negative
    1"><mrow><mi mathvariant="bold">A</mi><mo>=</mo><mi mathvariant="bold">V</mi><mi
    mathvariant="bold">Î›</mi><msup><mi mathvariant="bold">V</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></mrow></math>
    ã€‚ç¡®è®¤æ¯ä¸ªç‰¹å¾å‘é‡çš„æ¬§å‡ é‡Œå¾·èŒƒæ•°ç­‰äº 1ã€‚æ³¨æ„ï¼Œå¤æ•°çš„å¹³æ–¹é€šè¿‡å…¶å¤å…±è½­è¿›è¡Œè®¡ç®—ï¼ˆæç¤ºï¼šä½¿ç”¨ `np.conj()`ï¼‰ã€‚
- en: Next, multiply the eigenvectors matrix by any nonzero scalar. I used <math alttext="pi"><mi>Ï€</mi></math>
    for no particularly good reason other than it was fun to type. Does this scalar
    affect the accuracy of the reconstructed matrix and/or the norms of the eigenvectors?
    Why or why not?
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œå°†ç‰¹å¾å‘é‡çŸ©é˜µä¹˜ä»¥ä»»æ„éé›¶æ ‡é‡ã€‚æˆ‘é€‰æ‹©äº†<math alttext="pi"><mi>Ï€</mi></math>ï¼Œæ²¡æœ‰ç‰¹åˆ«å¥½çš„ç†ç”±ï¼Œåªæ˜¯æ‰“å­—æ—¶å¾ˆæœ‰è¶£ã€‚è¿™ä¸ªæ ‡é‡ä¼šå½±å“é‡æ„çŸ©é˜µçš„å‡†ç¡®æ€§å’Œ/æˆ–ç‰¹å¾å‘é‡çš„èŒƒæ•°å—ï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ
- en: Finally, repeat this but use a symmetric matrix, and replace <math alttext="bold
    upper V Superscript negative 1"><msup><mi>ğ•</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>
    with <math alttext="bold upper V Superscript upper T"><msup><mi>ğ•</mi> <mtext>T</mtext></msup></math>
    . Does this change the conclusion?
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œé‡å¤è¿™ä¸€è¿‡ç¨‹ï¼Œä½†ä½¿ç”¨å¯¹ç§°çŸ©é˜µï¼Œå¹¶ç”¨<math alttext="bold upper V Superscript upper T"><msup><mi>ğ•</mi>
    <mtext>T</mtext></msup></math>æ›¿æ¢<math alttext="bold upper V Superscript negative
    1"><msup><mi>ğ•</mi> <mrow><mo>-</mo><mn>1</mn></mrow></msup></math>ã€‚è¿™æ˜¯å¦æ”¹å˜äº†ç»“è®ºï¼Ÿ
- en: ^([1](ch13.xhtml#idm45733293579328-marker)) Approximately âˆ’.6 and 1.6.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch13.xhtml#idm45733293579328-marker)) å¤§çº¦æ˜¯âˆ’0.6å’Œ1.6ã€‚
- en: ^([2](ch13.xhtml#idm45733293558128-marker)) â€œOpen Letter to Kansas School Board,â€
    Church of the Flying Spaghetti Monster, [*spaghettimonâ sterâ€‹.org/about/open-letter*](https://www.spaghettimonster.org/about/open-letter).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch13.xhtml#idm45733293558128-marker)) â€œå¼€æ”¾ä¿¡ä»¶ç»™å ªè¨æ–¯å·æ•™è‚²å§”å‘˜ä¼šâ€ï¼Œé£è¡Œæ„å¤§åˆ©é¢æ€ªç‰©æ•™å ‚ï¼Œ[*spaghettimonâ sterâ€‹.org/about/open-letter*](https://www.spaghettimonster.org/about/open-letter)ã€‚
- en: ^([3](ch13.xhtml#idm45733293437712-marker)) As I wrote in [ChapterÂ 5](ch05.xhtml#Chapter_5),
    Python will return a result, but that is the scalar broadcast subtracted, which
    is not a linear algebra operation.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch13.xhtml#idm45733293437712-marker)) æ­£å¦‚æˆ‘åœ¨[ç¬¬5ç« ](ch05.xhtml#Chapter_5)ä¸­å†™çš„ï¼ŒPythonä¼šè¿”å›ä¸€ä¸ªç»“æœï¼Œä½†é‚£æ˜¯æ ‡é‡å¹¿æ’­å‡å»çš„ç»“æœï¼Œè¿™ä¸æ˜¯ä¸€ä¸ªçº¿æ€§ä»£æ•°æ“ä½œã€‚
- en: '^([4](ch13.xhtml#idm45733293126624-marker)) To quell suspense: it makes the
    eigenvectors matrix an orthogonal matrix.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch13.xhtml#idm45733293126624-marker)) ä¸ºäº†æ¶ˆé™¤æ‚¬å¿µï¼šå®ƒä½¿ç‰¹å¾å‘é‡çŸ©é˜µæˆä¸ºæ­£äº¤çŸ©é˜µã€‚
- en: ^([5](ch13.xhtml#idm45733292508688-marker)) By â€œstraightforwardâ€ I mean mathematically
    expected; interpreting complex solutions in eigendecomposition is far from straightforward.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch13.xhtml#idm45733292508688-marker)) â€œç›´è§‚â€æŒ‡çš„æ˜¯æ•°å­¦ä¸Šçš„é¢„æœŸï¼›è§£é‡Šç‰¹å¾åˆ†è§£ä¸­çš„å¤æ‚è§£å†³æ–¹æ¡ˆè¿œéç›´è§‚ã€‚
- en: ^([6](ch13.xhtml#idm45733292090752-marker)) In the interest of brevity, I am
    omitting some subtlety here about rare cases where the eigenvectors matrix does
    not span the entire *M*-dimensional subspace.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch13.xhtml#idm45733292090752-marker)) ä¸ºäº†ç®€æ´èµ·è§ï¼Œæˆ‘åœ¨è¿™é‡Œå¿½ç•¥äº†æœ‰å…³ç‰¹å¾å‘é‡çŸ©é˜µä¸æ¶µç›–æ•´ä¸ª*M*ç»´å­ç©ºé—´çš„ä¸€äº›ç»†å¾®å·®åˆ«ã€‚
- en: ^([7](ch13.xhtml#idm45733291700640-marker)) I often use symmetric matrices in
    exercises because they have real-valued eigenvalues, but that doesnâ€™t change the
    principle or the math; it merely facilitates visual inspection of the solutions.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch13.xhtml#idm45733291700640-marker)) åœ¨ç»ƒä¹ ä¸­æˆ‘ç»å¸¸ä½¿ç”¨å¯¹ç§°çŸ©é˜µï¼Œå› ä¸ºå®ƒä»¬å…·æœ‰å®æ•°ç‰¹å¾å€¼ï¼Œä½†è¿™å¹¶ä¸æ”¹å˜åŸåˆ™æˆ–æ•°å­¦ï¼Œåªæ˜¯ä¾¿äºè§£å†³æ–¹æ¡ˆçš„è§†è§‰æ£€æŸ¥ã€‚
- en: ^([8](ch13.xhtml#idm45733291670912-marker)) The first two were the multiplicative
    and additive methods.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch13.xhtml#idm45733291670912-marker)) ç¬¬ä¸€ä¸ªæ˜¯ä¹˜æ³•æ–¹æ³•ï¼Œç¬¬äºŒä¸ªæ˜¯åŠ æ³•æ–¹æ³•ã€‚
- en: ^([9](ch13.xhtml#idm45733291596240-marker)) The reason why the product of two
    symmetric matrices is not symmetric is the same as the reason why **R** from **QR**
    decomposition has zeros on the lower-diagonal.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch13.xhtml#idm45733291596240-marker)) ä¸¤ä¸ªå¯¹ç§°çŸ©é˜µçš„ä¹˜ç§¯ä¸å¯¹ç§°çš„åŸå› ä¸**QR**åˆ†è§£ä¸­**R**åœ¨å¯¹è§’çº¿ä¸‹æœ‰é›¶çš„åŸå› ç›¸åŒã€‚
