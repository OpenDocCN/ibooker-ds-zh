- en: 12 Case study 3 solution
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 12 案例研究 3 解决方案
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Extracting and visualizing locations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取和可视化位置
- en: Cleaning data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据清理
- en: Clustering locations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置聚类
- en: 'Our goal is to extract locations from disease-related headlines to uncover
    the largest active epidemics within and outside of the United States. We will
    do as follows:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是从疾病相关的标题中提取位置，以揭示美国境内外的最大活跃疫情。我们将按以下步骤进行：
- en: Load the data.
  id: totrans-6
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载数据。
- en: Extract locations from the text using regular expressions and the GeoNamesCache
    library.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用正则表达式和 GeoNamesCache 库从文本中提取位置。
- en: Check the location matches for errors.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查位置匹配是否存在错误。
- en: Cluster the locations based on geographic distance.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据地理距离对位置进行聚类。
- en: Visualize the clusters on a map, and remove any errors.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在地图上可视化簇，并删除任何错误。
- en: Output representative locations from the largest clusters to draw interesting
    conclusions.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从最大的簇中输出代表性位置以得出有趣的结论。
- en: Warning Spoiler alert! The solution to case study 3 is about to be revealed.
    I strongly encourage you to try to solve the problem before reading the solution.
    The original problem statement is available for reference at the beginning of
    the case study.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 揭示警报！案例研究 3 的解决方案即将揭晓。我强烈建议你在阅读解决方案之前尝试解决问题。原始问题陈述可在案例研究开头参考。
- en: 12.1 Extracting locations from headline data
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.1 从标题数据中提取位置
- en: We begin by loading the headline data.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先加载标题数据。
- en: Listing 12.1 Loading headline data
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.1 加载标题数据
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: We have loaded 650 headlines. Now we need a mechanism for extracting city and
    country names from the headline text. One naive solution is to match the locations
    in GeoNamesCache against each and every headline. However, this approach will
    fail to match locations whose capitalization and accent marks diverge from the
    stored GeoNamesCache data. For more optimal matching, we should transform each
    location name into a case-independent and accent-independent regular expression.
    We can execute these transformations using a custom `name_to_regex` function.
    That function takes a location name as input and returns a compiled regular expression
    capable of identifying any location of our choosing.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经加载了 650 个标题。现在我们需要一种机制从标题文本中提取城市和国家名称。一个简单的方法是将 GeoNamesCache 中的位置与每个标题进行匹配。然而，这种方法将无法匹配与存储在
    GeoNamesCache 数据中不同的首字母大小写和重音符号的位置。为了更优化的匹配，我们应该将每个位置名称转换为不区分大小写和不区分重音的正则表达式。我们可以使用自定义的
    `name_to_regex` 函数执行这些转换。该函数接受一个位置名称作为输入，并返回一个编译后的正则表达式，能够识别我们选择的任何位置。
- en: Listing 12.2 Converting names to regexes
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.2 将名称转换为正则表达式
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Using `name_to_regex`, we can create a mapping between regular expressions and
    the original names in GeoNamesCache. Let’s create two dictionaries, `country_to-_name`
    and `city_to_name`, which map regular expressions to country names and city names,
    respectively.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `name_to_regex`，我们可以创建正则表达式与 GeoNamesCache 中的原始名称之间的映射。让我们创建两个字典，`country_to_name`
    和 `city_to_name`，分别将正则表达式映射到国家名称和城市名称。
- en: Listing 12.3 Mapping names to regexes
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.3 将名称映射到正则表达式
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Next, we use our mappings to define a function that looks for location names
    in text. The function takes as input both a headline and a location dictionary.
    It iterates over each regex key in the dictionary, returning the associated value
    if the regex pattern matches the headline.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用我们的映射定义一个函数，该函数在文本中查找位置名称。该函数接受标题和位置字典作为输入。它遍历字典中的每个正则表达式键，如果正则表达式模式与标题匹配，则返回关联的值。
- en: Listing 12.4 Finding locations in text
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.4 在文本中查找位置
- en: '[PRE3]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Iterating over dictionaries gives us a nondeterministic sequence of results.
    A change in sequence order could alter which locations are matched to the inputted
    text. This is especially true if multiple locations are present in the text. Sorting
    by location name ensures that function output does not change from run to run.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历字典给我们一个非确定性的结果序列。序列顺序的改变可能会改变与输入文本匹配的位置。这在文本中存在多个位置时尤其如此。按位置名称排序确保函数输出在每次运行之间不会改变。
- en: We utilize `get_name_in_text` to discover the cities and countries mentioned
    in the `headlines` list. Then we store the results in a Pandas table for easier
    analysis.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `get_name_in_text` 来发现 `headlines` 列表中提到的城市和国家。然后我们将结果存储在 Pandas 表中，以便于分析。
- en: Listing 12.5 Finding locations in headlines
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.5 在标题中查找位置
- en: '[PRE4]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s explore our location table. We start by summarizing the contents of `df`
    using the `describe` method.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们探索我们的位置表。我们首先使用 `describe` 方法总结 `df` 的内容。
- en: Listing 12.6 Summarizing the location data
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.6 总结位置数据
- en: '[PRE5]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note Multiple countries in the data share the top occurrence frequency of 3\.
    Pandas does not have a deterministic method for selecting one top country over
    another. Depending on your local settings, a country other than Brazil could be
    returned as a top country, but it will still have a frequency of 3.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据中多个国家共享最高的发生频率为 3。Pandas 没有确定性的方法来选择一个国家而不是另一个国家作为顶级国家。根据您的本地设置，除了巴西以外的国家可能会被返回为顶级国家，但它仍然具有频率
    3。
- en: The table contains 619 mentions of cities representing 511 unique city names.
    It also contains just 15 countries representing 10 unique country names. The most
    frequently mentioned country is Brazil, which appears in three headlines.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 表格包含 619 个城市的提及，代表 511 个独特的城市名称。它还包含 15 个国家，代表 10 个独特的国家名称。最常提到的国家是巴西，它在三个标题中出现。
- en: The most frequently mentioned city is apparently “Of,” Turkey. That doesn’t
    seem right! The 45 instances of “Of” are more likely to match the preposition
    than the rarely referenced Turkish location. We will output some instances of
    “Of” to confirm the error.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 最常提到的城市显然是“Of”，土耳其。这似乎不太对！45 个“Of”的实例更有可能匹配介词而不是很少被引用的土耳其位置。我们将输出一些“Of”的实例以确认错误。
- en: Listing 12.7 Fetching cities named `"Of"`
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.7 获取名为 `"Of"` 的城市
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Converts df to a string in which the row indices have been removed. This leads
    to more concise printed output.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 df 转换为字符串，其中已删除行索引。这导致打印输出更加简洁。
- en: 'Yes, our matches to “Of” are definitely erroneous. We can fix the error by
    ensuring that all matches are capitalized. However, the observed bug is a symptom
    of a much bigger issue: in all the wrongly matched headlines, we matched to “Of”
    but not to the actual city name. This occurred because we didn’t account for multiple
    matches in a headline. How frequently do headlines contain more than one match?
    Let’s find out. We’ll track the list of all matched cities in a headline using
    an additional `Cities` column.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 是的，我们对“Of”的匹配肯定是有误的。我们可以通过确保所有匹配项都为大写来修复错误。然而，观察到的错误是一个更大问题的症状：在所有错误匹配的标题中，我们匹配到了“Of”，但没有匹配到实际的城市名称。这是因为我们没有考虑到标题中的多个匹配。标题中包含多个匹配的频率有多高？让我们找出答案。我们将使用额外的
    `Cities` 列跟踪标题中所有匹配城市的列表。
- en: Listing 12.8 Finding multicity headlines
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.8 查找多城市标题
- en: '[PRE7]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Returns a list of all unique cities in a headline
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回一个包含所有唯一城市的列表
- en: ❷ Makes sure the first letter of the city name is capitalized
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确保城市名称的首字母大写
- en: ❸ Adds a Cities column to the table by using the apply method, which applies
    an inputted function to all elements of a column to create a brand new column
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过使用 apply 方法向表格中添加一个 Cities 列，该方法将输入的函数应用于列中的所有元素以创建一个全新的列
- en: ❹ Adds a column counting the number of cities in a headline
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个列，用于计算标题中的城市数量
- en: ❺ Filters out rows that do not contain multiple city matches
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 过滤掉不包含多个城市匹配的行
- en: ❻ The city count may increase with data updates to the GeoNamesCache library.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 城市计数可能会随着 GeoNamesCache 库的数据更新而增加。
- en: We find that 67 headlines contain more than one city, which represents approximately
    10% of the data. Why do so many headlines match against multiple locations? Perhaps
    exploring some sample matches will yield an answer.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们发现 67 个标题包含多个城市，这代表了大约 10% 的数据。为什么这么多标题与多个位置匹配？也许探索一些样本匹配将得出答案。
- en: Listing 12.9 Sampling multicity headlines
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.9 多城市标题的采样
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: It appears that short, invalid city names are being matched to the headlines
    along with the longer, correct location names. For example, the city of `'San'`
    is always returned along with more legitimate city names like `'San Francisco'`
    and `'San Salvador'`. How do we fix this error? One solution is to just return
    the longest city name whenever more than one matched city is found.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，短的无效城市名称正在与较长的正确位置名称一起匹配标题。例如，城市 `'San'` 总是与更合法的城市名称如 `'San Francisco'`
    和 `'San Salvador'` 一起返回。我们如何修复这个错误？一个解决方案是在找到多个匹配城市时只返回最长的城市名称。
- en: Listing 12.10 Selecting the longest city names
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.10 选择最长的城市名称
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As a sanity check, we’ll output rows that contain a short city name (four characters
    or fewer) to ensure that no erroneous short name is assigned to one of our headlines.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项合理性检查，我们将输出包含短城市名称（四个字符或更少）的行，以确保不会将错误的短名称分配给我们的标题之一。
- en: Listing 12.11 Printing the shortest city names
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.11 打印最短的城市名称
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The results appear to be legitimate. Let’s now shift our attention from cities
    to countries. Only 15 of the total headlines contain actual country information.
    The count is low enough for us to manually examine all of these headlines.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来是合法的。现在让我们将注意力从城市转移到国家。只有15个总标题包含实际的国家信息。这个数量足够低，我们可以手动检查所有这些标题。
- en: Listing 12.12 Fetching headlines with countries
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.12 获取包含国家的标题
- en: '[PRE11]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The df.Country.notnull() method returns a list of Booleans. Each Boolean equals
    True only if a country is present in the associated row.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ df.Country.notnull()方法返回一个布尔值列表。每个布尔值只有在相关行中存在国家时才等于True。
- en: All of the country-bearing headlines also contain city information. Thus, we
    can assign a latitude and longitude without relying on the country’s central coordinates.
    Consequently, we can disregard the country names from our analysis.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 所有包含国家的标题也包含城市信息。因此，我们可以不依赖于国家的中心坐标来分配纬度和经度。因此，我们可以从我们的分析中忽略国家名称。
- en: Listing 12.13 Dropping countries from the table
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.13 从表中删除国家
- en: '[PRE12]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: We are nearly ready to add latitudes and longitudes to our table. However, we
    first need to consider the rows where no locations were detected. Let’s count
    the number of unmatched headlines and then print a subset of that data.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好将纬度和经度添加到我们的表中。然而，我们首先需要考虑没有检测到位置的那些行。让我们计算未匹配标题的数量，然后打印该数据的一个子集。
- en: Listing 12.14 Exploring unmatched headlines
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.14 探索未匹配的标题
- en: '[PRE13]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Approximately 6% of the headlines do not match any cities. Some of these headlines
    mention legitimate cities, which GeoNamesCache failed to identify. How should
    we treat the missing cities? Well, given their low frequency, perhaps we should
    delete the missing mentions. The price for those deletions is a slight reduction
    in data quality, but that loss will not significantly impact our results because
    our coverage of matched cities is quite high.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大约6%的标题与任何城市都不匹配。其中一些标题提到了合法的城市，GeoNamesCache未能识别。我们应该如何处理这些缺失的城市？嗯，鉴于它们的低频率，也许我们应该删除这些缺失的提及。这些删除的代价是数据质量略有下降，但这一损失不会显著影响我们的结果，因为我们对匹配城市的覆盖范围相当高。
- en: Listing 12.15 Dropping unmatched headlines
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.15 删除未匹配的标题
- en: '[PRE14]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ The ~ symbol reverses the Booleans in the list returned by the df.City.isnull()
    method. Thus, each reversed Boolean equals True only if a city is present in the
    associated row.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ ~符号反转了df.City.isnull()方法返回的列表中的布尔值。因此，每个反转的布尔值只有在相关行中存在城市时才等于True。
- en: 12.2 Visualizing and clustering the extracted location data
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.2 可视化和聚类提取的位置数据
- en: All the rows in our table contain a city name. Now we can assign a latitude
    and longitude to each row. We utilize `get_cities_by_name` to return the coordinates
    of the most populated city bearing the extracted city name.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们表格中的所有行都包含一个城市名称。现在我们可以为每一行分配纬度和经度。我们使用`get_cities_by_name`来返回具有提取的城市名称的最人口密集城市的坐标。
- en: Listing 12.16 Assigning geographic coordinates to cities
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.16 为城市分配地理坐标
- en: '[PRE15]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Chooses the matched city with the largest population
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择具有最大人口的匹配城市
- en: ❷ Extracts city latitudes and longitudes
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取城市纬度和经度
- en: ❸ Adds Latitude and Longitude columns to our table
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将纬度和经度列添加到我们的表中
- en: With latitudes and longitudes assigned, we can attempt to cluster the data.
    Let’s execute K-means across our set of 2D coordinates. We use the elbow method
    to choose a reasonable value for *K* (figure 12.1).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 分配了纬度和经度后，我们可以尝试对数据进行聚类。让我们在我们的2D坐标集中执行K-means。我们使用肘部方法来选择一个合理的*K*值（图12.1）。
- en: '![](../Images/12-01.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-01.png)'
- en: Figure 12.1 A geographic elbow curve points to a *K* of 3
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.1 地理肘部曲线指向一个*K*值为3
- en: Listing 12.17 Plotting a geographic elbow curve
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.17 绘制地理肘部曲线
- en: '[PRE16]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The “elbow” in our elbow plot points to a *K* of 3\. That *K* value is very
    low, limiting our scope to at most three different geographic territories. Still,
    we should maintain some faith in our analytic methodology. We cluster the locations
    into three groups and plot them on a map (figure 12.2).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的肘部图中的“肘部”指向一个*K*值为3。这个*K*值非常低，限制了我们的范围最多只有三个不同的地理区域。尽管如此，我们仍然应该对我们的分析方法论保持一定的信心。我们将位置聚类成三个组，并在地图上绘制它们（图12.2）。
- en: '![](../Images/12-02.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-02.png)'
- en: Figure 12.2 Mapped K-means city clusters. *K* is set to 3\. The three clusters
    are spread thin across six continents.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.2 地理K-means城市聚类图。*K*设置为3。三个聚类分布在六个大洲上。
- en: Listing 12.18 Using K-means to cluster cities into three groups
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.18 使用K-means将城市聚类成三个组
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ This function will be reused to plot clusters throughout the rest of our analysis.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 此函数将在我们分析的其他部分重复使用以绘制聚类。
- en: Note The marker shapes in figures 12.1 through 12.5 have been manually adjusted
    to discriminate among clusters in the black-and-white print version of the book.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：图12.1至12.5中的标记形状已被手动调整，以便在书的黑白印刷版本中区分聚类。
- en: The results look pretty ridiculous. Our three clusters cover
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 结果看起来相当荒谬。我们的三个聚类覆盖
- en: North and South America
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 北美洲和南美洲
- en: Africa and Europe
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非洲和欧洲
- en: Asia and Australia
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚洲和澳大利亚
- en: These continental categories are too broad to be useful. Furthermore, all South
    American cities on the eastern coast awkwardly cluster with African and European
    locations (despite the fact that an entire ocean lies between them). These clusters
    are not helpful for understanding the data. Perhaps our *K* was too low after
    all. Let’s disregard our elbow analysis and double the size of *K* to 6 (figure
    12.3).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这些大陆分类过于宽泛，没有实际用途。此外，所有南美洲东海岸的城市都尴尬地与非洲和欧洲的位置聚类在一起（尽管它们之间有一个整个海洋）。这些聚类对理解数据没有帮助。也许我们的*K*确实太低了。让我们忽略我们的肘部分析，将*K*的大小加倍到6（图12.3）。
- en: '![](../Images/12-03.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-03.png)'
- en: Figure 12.3 Mapped K-means city clusters. *K* is set to 6\. Africa’s clustered
    points are incorrectly split between the European and Asian continents.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.3 K-means城市聚类的映射。*K*设置为6。非洲的聚类点被错误地分在了欧洲和亚洲大陆之间。
- en: Listing 12.19 Using K-means to cluster cities into six groups
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.19 使用K-means将城市聚类成六个组
- en: '[PRE18]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Increasing *K* improves clustering in North America and South America. South
    America now falls in its own separate cluster, and North America is split between
    two Western and Eastern cluster groups. However, on the other side of the Atlantic,
    the clustering quality remains low. Africa’s geolocations are incorrectly split
    between Europe and Asia. K-mean’s sense of centrality is unable to properly distinguish
    between Africa, Europe, and Asia. Perhaps the algorithm’s reliance on Euclidean
    distance prevents it from capturing relationships between points distributed on
    our planet’s curved surface.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 增加参数*K*可以提高北美洲和南美洲的聚类效果。南美洲现在落入了它自己的单独聚类中，而北美洲则被分为两个西部和东部聚类组。然而，在大西洋的另一侧，聚类质量仍然很低。非洲的地理位置被错误地分在了欧洲和亚洲之间。K-means的中心感无法正确区分非洲、欧洲和亚洲。也许算法对欧几里得距离的依赖阻止了它捕捉到分布在我们地球曲面上点的相互关系。
- en: As an alternate approach, we can attempt to execute DBSCAN clustering. The DBSCAN
    algorithm takes as input any distance metric of our choosing, allowing us to cluster
    on the great-circle distance between points. We start by coding a great-circle
    distance function whose inputs are a pair of NumPy arrays.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种替代方法，我们可以尝试执行DBSCAN聚类。DBSCAN算法接受我们选择的任何距离度量，允许我们在点之间的大圆距离上进行聚类。我们首先编写一个以NumPy数组为输入的大圆距离函数。
- en: Listing 12.20 Defining a NumPy-based great-circle metric
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.20 定义基于NumPy的大圆度量
- en: '[PRE19]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ radius is preset to the radius of the Earth in miles.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 半径预设为地球的英里半径。
- en: 'We’ve defined our distance metric and are nearly ready to run the DBSCAN algorithm.
    However, first we need to choose reasonable values for the `eps` and `min_samples`
    parameters. Let’s assume the following: a global city cluster contains at least
    three cities that are on average no more than 250 miles apart. Based on these
    assumptions, we input values of 250 and 3 into `eps` and `min_samples`, respectively.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了我们的距离度量，并且几乎准备好运行DBSCAN算法。然而，首先我们需要为`eps`和`min_samples`参数选择合理的值。让我们假设以下情况：一个全球城市聚类至少包含三个城市，这些城市之间的平均距离不超过250英里。基于这些假设，我们将250和3分别输入到`eps`和`min_samples`中。
- en: Listing 12.21 Using DBSCAN to cluster cities
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.21 使用DBSCAN聚类城市
- en: '[PRE20]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: DBSCAN assigns –1 to outlier data points that do not cluster. Let’s remove these
    outliers from our table and then plot the remaining results (figure 12.4).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN将-1分配给那些没有聚类的异常数据点。让我们从我们的表中移除这些异常值，然后绘制剩余的结果（图12.4）。
- en: '![](../Images/12-04.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/12-04.png)'
- en: Figure 12.4 Mapped DBSCAN city clusters computed using the great-circle distance
    metric
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图12.4 使用大圆距离度量计算的城市DBSCAN聚类映射
- en: Listing 12.22 Plotting non-outlier DBSCAN clusters
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.22 绘制非异常DBSCAN聚类
- en: '[PRE21]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: DBSCAN has done a decent job generating discrete clusters in parts of South
    America, Asia, and southern Africa. The eastern United States, however, falls
    into a single overly dense cluster. Why is this the case? It is partially due
    to a certain narrative bias in Western media, which means American events are
    more likely to get coverage. This leads to a denser spread of mentioned locations.
    One way to overcome the geographic bias is to recluster US cities using a more
    rigorous epsilon parameter. Such a strategy seems sensible in the context of our
    problem statement, which asks for separate top clusters from American and globally
    grouped headlines. So, we’ll cluster US locations independently from the rest
    of the world. To do so, we first assign country codes across each of our cities.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: DBSCAN在南美洲、亚洲和南部非洲的部分地区生成离散集群做得相当不错。然而，美国东部却落入一个过于密集的单个集群中。这是为什么？部分原因是西方媒体中存在某种叙事偏见，这意味着美国事件更有可能获得报道。这导致提到的位置分布更密集。克服地理偏见的一种方法是用更严格的epsilon参数重新聚类美国城市。在我们的问题陈述的背景下，这种策略似乎是合理的，它要求从美国和全球分组标题中分别获取顶级集群。因此，我们将独立于世界其他地区聚类美国位置。为此，我们首先为每个城市分配国家代码。
- en: Listing 12.23 Assigning country codes to cities
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.23 为城市分配国家代码
- en: '[PRE22]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The country codes allow us to separate the data into two distinct `DataFrame`
    objects. The first object, `df_us`, holds the US locations. The second object,
    `df_not_us`, holds all the remaining global cities.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 国家代码使我们能够将数据分为两个不同的`DataFrame`对象。第一个对象`df_us`包含美国位置。第二个对象`df_not_us`包含所有剩余的全球城市。
- en: Listing 12.24 Separating US and global cities
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.24 分离美国和全球城市
- en: '[PRE23]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We’ve separated US and non-US cities. Now we need to recluster the coordinates
    in the two separated tables. Reclustering `df_not_us` is unavoidable due to density
    changes caused by deleting all the US locations. However, we maintain `eps` of
    250 while clustering that table. Meanwhile, we reduce `eps` for `df_us` by half
    (to 125) to acknowledge the tighter density of US locations. Finally, all outliers
    are deleted after we recluster.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将美国和非美国城市分开。现在我们需要重新聚类两个分离表中的坐标。由于删除所有美国位置导致的密度变化，重新聚类`df_not_us`是不可避免的。然而，我们在聚类该表时保持`eps`为250。同时，我们将`df_us`的`eps`减半（至125），以承认美国位置的更紧密密度。最后，我们在重新聚类后删除所有异常值。
- en: Listing 12.25 Reclustering extracted cities
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.25 重新聚类提取的城市
- en: '[PRE24]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 12.3 Extracting insights from location clusters
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 12.3 从位置集群中提取见解
- en: Let’s investigate the clustered data in the `df_not_us` table. We start by grouping
    the results using the Pandas `groupby` method.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查`df_not_us`表中的聚类数据。我们首先使用Pandas的`groupby`方法对结果进行分组。
- en: Listing 12.26 Grouping cities by cluster
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.26 按集群分组城市
- en: '[PRE25]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 31 global clusters have been detected. Let’s sort these groups by size and count
    the headlines in the largest cluster.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 已经检测到31个全球集群。让我们按大小对这些组进行排序，并计算最大集群中的标题数量。
- en: Listing 12.27 Finding the largest cluster
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.27 寻找最大集群
- en: '[PRE26]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: The largest cluster contains 51 total headlines. Reading all these headlines
    individually will be a time-consuming process. We can save time by outputting
    just those headlines that represent the most central locations in the cluster.
    Centrality can be captured by calculating the average latitude and longitude of
    a group. Then we can compute the distance between every location and the average
    coordinates. Lower distances indicate higher centrality.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的集群包含51个总标题。逐个阅读所有这些标题将是一个耗时过程。我们可以通过只输出代表集群中最中心位置的标题来节省时间。中心性可以通过计算一个组的平均纬度和经度来捕捉。然后我们可以计算每个位置与平均坐标之间的距离。较短的距离表示更高的中心性。
- en: Note As we discussed in section 11, the average latitude and longitude merely
    approximate the center since they do not consider the curvature of the Earth.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：正如我们在第11节中讨论的那样，平均纬度和经度仅近似中心，因为它们没有考虑地球的曲率。
- en: Next, we define a `compute_centrality` function that assigns a `Distance_to_center`
    column to an inputted group.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义一个`compute_centrality`函数，该函数将一个`Distance_to_center`列分配给一个输入的组。
- en: Listing 12.28 Computing cluster centrality
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.28 计算集群中心性
- en: '[PRE27]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: We can now sort all headlines by centrality. Let’s print the five most central
    headlines in our largest cluster.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以按中心性对所有标题进行排序。让我们打印出我们最大集群中五个最中心的标题。
- en: Listing 12.29 Finding the central headlines in the largest cluster
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表12.29 在最大集群中寻找中心标题
- en: '[PRE28]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The central headlines in `largest_group` focus on an outbreak of mad cow disease
    in various European cities. We can confirm that the cluster’s locale is centered
    in Europe by outputting the top countries associated with cities in the cluster.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`largest_group` 中的中心标题集中在欧洲各城市爆发疯牛病。我们可以通过输出与聚类中城市相关的顶级国家来确认该聚类的地理位置集中在欧洲。'
- en: Listing 12.30 Finding the top three countries in the largest cluster
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.30 在最大聚类中找到前三个国家
- en: '[PRE29]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The Counter class tracks the most-repeated elements in a list, along with
    their counts.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Counter 类跟踪列表中最常重复的元素及其计数。
- en: The most frequently mentioned cities in `largest_group` are located in the United
    Kingdom, France, and Germany. The majority of locations in `largest_group` are
    definitely in Europe.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `largest_group` 中最常提到的城市位于英国、法国和德国。`largest_group` 中的大多数位置肯定在欧洲。
- en: Let’s repeat this analysis across the four next-largest global clusters. The
    following code helps determine whether any other disease epidemics are currently
    threatening the globe.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在四个下一个最大的全球聚类中重复此分析。以下代码有助于确定是否有其他疾病流行目前正威胁着全球。
- en: Listing 12.31 Summarizing content in the largest clusters
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.31 总结最大聚类中的内容
- en: '[PRE30]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Oh no! Zika is spreading through the Philippines! There are also Zika outbreaks
    in Southeast Asia and in Central America. The Canadian cluster, however, contains
    a mix of random disease headlines, which implies that no dominant outbreak is
    occurring in that northern territory.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！Zika 正在菲律宾蔓延！东南亚和中美洲也有 Zika 爆发。然而，加拿大聚类包含了一系列随机的疾病标题，这表明在那个北方领土没有发生主要的爆发。
- en: Let’s turn our attention to the US clusters. We start by visualizing the clusters
    on a map of the United States (figure 12.5).
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把注意力转向美国聚类。我们首先在美国地图上可视化这些聚类（图 12.5）。
- en: '![](../Images/12-05.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-05.png)'
- en: Figure 12.5 Mapped DBSCAN location clusters within the boundaries of the United
    States
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.5 在美国边界内映射 DBSCAN 位置聚类
- en: Listing 12.32 Plotting US DBSCAN clusters
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.32 绘制 US DBSCAN 聚类
- en: '[PRE31]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The visualized map yields reasonable outputs. The eastern states no longer fall
    into a single dense cluster. We’ll analyze the top five US clusters by printing
    their centrality-sorted headlines.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化地图产生了合理的输出。东部各州不再落入一个单一的密集聚类中。我们将通过打印这些按中心性排序的标题来分析前五个美国聚类。
- en: Listing 12.33 Summarizing content within the largest US clusters
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.33 总结最大美国聚类中的内容
- en: '[PRE32]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'The Zika epidemic has hit both Florida and Texas! This is very troubling. However,
    no discernible disease patterns are present in the other top clusters. Currently,
    the spreading Zika outbreak is confined to the southern United States. We will
    immediately report this to our superiors so they can take appropriate action.
    As we prepare to present our findings, let’s plot one final image, which will
    appear on the front page of our report (figure 12.6). This image summarizes the
    menacing scope of the spreading Zika epidemic: it displays all US and global clusters
    where Zika is mentioned in more than 50% of article headlines.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Zika 疫情已经袭击了佛罗里达州和德克萨斯州！这非常令人担忧。然而，其他顶级聚类中并没有明显的疾病模式。目前，正在蔓延的 Zika 爆发仅限于美国南部。我们将立即向我们的上级报告，以便他们采取适当的行动。在我们准备展示我们的发现时，让我们绘制一个最终的图像，该图像将出现在我们报告的首页（图
    12.6）。此图像总结了正在蔓延的 Zika 疫情的威胁范围：它显示了所有提到 Zika 的文章标题中超过 50% 的美国和全球聚类。
- en: '![](../Images/12-06.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/12-06.png)'
- en: Figure 12.6 Mapped DBSCAN location clusters where Zika is mentioned in more
    than 50% of article headlines
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 12.6 提到 Zika 的文章标题中超过 50% 的 DBSCAN 位置聚类图
- en: Listing 12.34 Plotting Zika clusters
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 12.34 绘制 Zika 聚类图
- en: '[PRE33]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Counts the number of times Zika is mentioned in a list of headlines
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算标题列表中 Zika 被提及的次数
- en: ❷ Regex that matches an instance of the word "Zika" in a headline. The match
    is case insensitive.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 匹配标题中单词 "Zika" 实例的正则表达式。匹配不区分大小写。
- en: ❸ Iterates over both US and global clusters
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历美国和全球聚类
- en: ❹ Plots clusters where Zika is mentioned in more than 50% of article headlines
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制提到 Zika 的文章标题中超过 50% 的聚类图
- en: 'We have successfully clustered our headlines by location and plotted those
    clusters where the word *Zika* is dominant. This relationship between our clusters
    and their textual content leads to an interesting question: is it possible to
    cluster the headlines based on text similarity rather than geographic distance?
    In other words, can we group our headlines by text overlap so that all the references
    to Zika automatically appear in a single cluster? Yes, we can! In the subsequent
    case study, we learn how to measure similarity between texts to group documents
    by topic.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经根据地理位置成功地将标题进行了聚类，并在那些以单词 *Zika* 为主的区域绘制了这些聚类。我们聚类与它们的文本内容之间的关系引发了一个有趣的问题：是否可以根据文本相似性而不是地理距离来聚类标题？换句话说，我们能否通过文本重叠来对标题进行分组，使得所有关于Zika的引用都自动出现在一个单独的聚类中？是的，我们可以！在随后的案例研究中，我们学习了如何测量文本之间的相似性，以便根据主题对文档进行分组。
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Data science tools can fail in unexpected ways. When we ran GeoNamesCache on
    our news headlines, the library incorrectly matched short city names (such as
    “Of” and “San”) to the inputted text. Through data exploration, we were able to
    account for these mistakes. If, instead, we had blindly clustered the locations,
    our final output would have been junky. We must diligently explore our data prior
    to serious analysis.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据科学工具可能会以意想不到的方式失败。当我们对新闻标题运行GeoNamesCache时，该库错误地将短城市名称（如“Of”和“San”）与输入文本匹配。通过数据探索，我们能够解释这些错误。如果我们盲目地聚类位置，我们的最终输出将是垃圾。我们必须在认真分析之前勤奋地探索我们的数据。
- en: Sometimes, problematic data points are present in an otherwise good dataset.
    In our case, less than 6% of headlines incorrectly lacked a city assignment. Correcting
    for these headlines would have been difficult. Instead, we chose to delete the
    headlines from the dataset. Occasionally, it’s okay to delete problematic examples
    if their impact on the dataset is minimal. However, we should weigh the pros and
    cons of the deletion before making a final decision.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候，在原本良好的数据集中可能存在有问题的数据点。在我们的案例中，不到6%的标题错误地缺少城市分配。纠正这些标题将是困难的。相反，我们选择从数据集中删除这些标题。偶尔，如果这些有问题的例子对数据集的影响很小，删除它们是可以接受的。然而，在做出最终决定之前，我们应该权衡删除的利弊。
- en: The elbow method heuristically picks *K* for K-means clustering. Heuristic tools
    are not guaranteed to work correctly every time. In our analysis, an elbow plot
    returned a *K* of 3\. Obviously, this value was too low. Thus, we intervened and
    attempted to choose a different *K*. If we had indiscriminately trusted the elbow
    output, our final clustering would have been worthless.
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 肘部方法在K-means聚类中启发式地选择 *K*。启发式工具并不保证每次都能正确工作。在我们的分析中，一个肘部图返回了 *K* 为3。显然，这个值太低了。因此，我们介入并尝试选择不同的
    *K*。如果我们盲目地相信肘部输出，我们的最终聚类将毫无价值。
- en: Common sense should dictate our analysis of clustering outputs. Earlier, we
    examined a K-means output where *K* equaled 6\. We observed the clustering of
    Central African and European cities. This result was clearly wrong—Europe and
    Central Africa are very different locations. So, we transitioned to a different
    clustering approach. When common sense dictates that the clustering is wrong,
    we should try an alternate approach.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 常识应该指导我们对聚类输出的分析。之前，我们检查了一个K-means输出，其中 *K* 等于6。我们观察到了中非和欧洲城市的聚类。这个结果显然是错误的——欧洲和中非是截然不同的地理位置。因此，我们转向了不同的聚类方法。当常识告诉我们聚类是错误的，我们应该尝试另一种方法。
- en: Sometimes it’s acceptable to break a dataset into parts and analyze each part
    individually. In our initial DBSCAN analysis, the algorithm failed to correctly
    cluster US cities. Most eastern US cities fell into a single cluster. We could
    have abandoned our DBSCAN approach. Instead, we clustered the US cities separately,
    using more appropriate parameters. Analyzing the dataset in two separate parts
    led to better clustering results.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候，将数据集分成几部分并单独分析每一部分是可以接受的。在我们最初的DBSCAN分析中，算法未能正确地将美国城市进行聚类。大多数美国东部城市都落入了一个单独的聚类中。我们本可以放弃DBSCAN方法。相反，我们分别对美国的城市进行了聚类，使用了更合适的参数。将数据集分成两个独立的部分进行分析导致了更好的聚类结果。
