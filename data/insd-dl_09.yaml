- en: 7 Autoencoding and self-supervision
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 自动编码和自监督
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Training without labels
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无标签训练
- en: Autoencoding to project data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自动编码用于投影数据
- en: Constraining networks with bottlenecks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用瓶颈约束网络
- en: Adding noise to improve performance
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加噪声以提升性能
- en: Predicting the next item to make generative models
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测生成模型中的下一个项目
- en: You now know several approaches to specifying a neural network for classification
    and regression problems. These are the classic machine learning problems, where
    for each data point x (e.g., a picture of a fruit), we have an associated answer
    y (e.g., fresh or rotten). But what if we do not have a label y? Is there any
    useful way for us to learn? You should recognize this as an *unsupervised* learning
    scenario.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经了解了指定神经网络用于分类和回归问题的几种方法。这些是经典的机器学习问题，对于每个数据点x（例如，水果的图片），我们都有一个相关的答案y（例如，新鲜或腐烂）。但如果我们没有标签y呢？我们有没有任何有用的学习方法？你应该认识到这是一个*无监督*学习场景。
- en: People are interested in self-supervision because *labels are expensive*. It
    is often much easier to get lots of data, but knowing *what* each data point is
    requires a lot of work. Think about a sentiment classification problem where you
    try to predict if a sentence is conveying a positive notion (e.g., “I love this
    deep learning book I’m reading.”) or a negative one (e.g., “The author of this
    book is bad at making jokes.”). It’s not *hard* to read the sentence, make a determination,
    and save that information. But if you want to build a *good* sentiment classifier,
    you might want to label hundreds of thousands to millions of sentences. Do you
    really want to spend days or weeks labeling so many sentences? If we could somehow
    learn without needing these labels, it would make our lives much easier.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 人们之所以对自监督感兴趣，是因为*标签很昂贵*。通常更容易获取大量数据，但知道*每个数据点是什么*需要大量工作。想想一个情感分类问题，你试图预测一个句子是否传达了积极的观点（例如，“我爱我正在读的这本书。”）或消极的观点（例如，“这本书的作者不擅长讲笑话。”）。阅读句子、做出判断并保存信息并不*难*。但如果你想要构建一个*好的*情感分类器，你可能需要标记数十万到数百万个句子。你真的愿意花几天或几周时间标记这么多句子吗？如果我们能够以某种方式学习而不需要这些标签，那将使我们的生活变得更加容易。
- en: One strategy for unsupervised learning that has become increasingly common in
    deep learning is called *self-supervision*. The idea behind self-supervision is
    that we use a regression or classification loss function ℓ to do the learning,
    and we predict something about the input data x itself. In these cases, the labels
    come *implicitly with the data* and allow us to use the same tools we have already
    learned to use. Coming up with clever ways to get implicit labels is the trick
    to self-supervision.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习中越来越常见的无监督学习策略之一被称为*自监督*。自监督背后的想法是，我们使用回归或分类损失函数ℓ来进行学习，并预测关于输入数据x本身的一些信息。在这些情况下，标签*隐含地与数据一起存在*，并允许我们使用我们已经学会使用的相同工具。想出巧妙的方法来获取隐含标签是自监督的关键。
- en: 'Figure 7.1 shows three of the many ways self-supervision can be done: *inpainting*,
    where you obscure part of the input and then try to predict what was hidden; *image
    sorting*, where you break the image into multiple parts, shuffle them, and then
    try to put them back in the right order; and *autoencoding*, where you’re given
    the input image and predict the input. Using self-supervision, we can train models
    without labels and then use what the model has learned to do data clustering,
    perform tasks like identifying noisy/bad data, or build useful models with less
    data (the latter will be demonstrated in chapter 13).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1展示了自监督可以采取的许多方法中的三种：*修复*，即隐藏输入的一部分，然后尝试预测被隐藏的内容；*图像排序*，即将图像分解成多个部分，打乱它们的顺序，然后尝试将它们放回正确的顺序；以及*自动编码*，即给出输入图像并预测输入。使用自监督，我们可以训练没有标签的模型，然后使用模型学到的知识进行数据聚类，执行诸如识别噪声/不良数据等任务，或者用更少的数据构建有用的模型（后者将在第13章中演示）。
- en: '![](../Images/CH07_F01_Raff.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F01_Raff.png)'
- en: 'Figure 7.1 Three different types of self-supervised problems: inpainting, image
    sorting, and autoencoding. In each case, we do not need to know *what* the image
    is of because the network will try to predict the original image no matter what.
    In the first case (red), we randomly block out a portion of the image and ask
    the network to fill in the missing piece. In the second case, we break the image
    into pieces, shuffle them, and ask the network to put them back in the correct
    order. In the last case, we simply ask the network to predict the original image
    from the original image.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 三种不同类型的自监督问题：图像修复、图像排序和自动编码。在每种情况下，我们不需要知道图像的内容是什么，因为网络将尝试预测原始图像，无论其内容如何。在第一种情况（红色）中，我们随机遮挡图像的一部分，并要求网络填充缺失的部分。在第二种情况中，我们将图像分割成块，打乱它们的顺序，并要求网络将它们放回正确的顺序。在最后一种情况中，我们只是要求网络从原始图像中预测原始图像。
- en: There are numerous ways to create a self-supervised problem, and researchers
    are coming up with new approaches all the time. In this chapter, we focus on one
    specific kind of self-supervision called *autoencoding*, the third example in
    figure 7.1, because the key to autoencoding is *predicting the input from the
    input*. This may seem like an insane idea at first. Surely it is a trivial problem
    for the network to learn to return the input as it was given. This would be like
    defining a function as
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多方法可以创建自监督问题，研究人员一直在提出新的方法。在本章中，我们专注于一种特定的自监督方法，称为*自动编码*，这是图7.1中的第三个例子，因为自动编码的关键是*从输入预测输入*。一开始这可能看起来像是一个疯狂的想法。当然，对于网络来说，学习返回与给定相同的输入是一个简单的问题。这就像定义一个函数一样
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This `superUsefulFunction` would implement a *perfect* autoencoder. So if the
    problem is so easy, how can it be useful? That’s what we learn in this chapter.
    The trick is to *constrain* the network, giving it a handicap so that it is unable
    to learn the trivial solution. Think of it like a test in school, where the teacher
    has given you 100 questions on an open book exam. If you had all the time in the
    world, you could simply read the book, find the answers, and write them down.
    But if you are *constrained* to complete the exam in just an hour, you don’t have
    time to search the book for everything. Instead, you are forced to learn the *underlying
    concepts* that help you reconstruct the answers to all the questions. The constraints
    help encourage learning and understanding. The same idea is at work with autoencoders:
    with the trivial solution off the table, the network is forced to learn something
    more useful (underlying concepts) to solve the problem.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这个`superUsefulFunction`将实现一个*完美*的自动编码器。所以如果问题这么简单，它有什么用呢？这是我们本章要学习的内容。诀窍是*约束*网络，给它一个障碍，使其无法学习到平凡解。想象一下学校里的考试，老师给你一个开卷考试，有100个问题。如果你有足够的时间，你只需简单地阅读书籍，找到答案，并写下它们。但如果你被*约束*在仅一个小时的时间内完成考试，你就没有时间去书中查找所有内容。相反，你被迫学习*基本概念*，这些概念帮助你重建所有问题的答案。约束有助于促进学习和理解。同样的想法也适用于自动编码器：当平凡解被排除在外时，网络被迫学习一些更有用的东西（基本概念）来解决该问题。
- en: This chapter explains the concept of autoencoding further and shows that bread-and-butter
    principal component analysis (PCA) works by secretly being an autoencoder. We’ll
    make small changes to a PyTorch version of PCA to change it into a fully fledged
    autoencoding neural network. As we make an autoencoding network larger, it becomes
    more important to constrain it well, which we demonstrate with the *denoising*
    strategy. Finally we apply these concepts to sequential models like RNNs, which
    gives the *autoregressive* model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 本章进一步解释了自动编码的概念，并展示了面包和黄油般的主成分分析（PCA）实际上是通过秘密地作为一个自动编码器来工作的。我们将对PCA的PyTorch版本进行一些小的修改，将其变成一个完整的自动编码神经网络。当我们使自动编码网络更大时，更好地约束它变得更加重要，我们通过*降噪*策略来展示这一点。最后，我们将这些概念应用于序列模型，如RNN，这给出了*自回归*模型。
- en: 7.1 How autoencoding works
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 自动编码是如何工作的
- en: 'Let’s first describe the idea of autoencoding in a little more detail. Autoencoding
    means we generally learn *two* functions/networks: first a function *f*^(in)(**x**)
    = **z**, which transforms the input x into a new representation **z** ∈ ℝ^(*D*′),
    and then a function *f*^(out)(**z**) = **x**, which converts the new representation
    z back into the original representation. We call these two functions the *encoder*
    *f*^(in) and the *decoder* *f*^(out), and the process is summarized in figure
    7.2\. Without any labels, the new representation z is learned in such a way that
    it captures useful information about the structure of the data in a compact form
    that is friendly to ML algorithms.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先更详细地描述一下自动编码的概念。自动编码意味着我们通常学习*两个*函数/网络：首先是一个函数 *f*^(in)(**x**) = **z**，它将输入x转换成新的表示**z**
    ∈ ℝ^(*D*′)，然后是一个函数 *f*^(out)(**z**) = **x**，它将新的表示z转换回原始表示。我们称这两个函数为*编码器* *f*^(in)和*解码器*
    *f*^(out)，整个过程总结在图7.2中。在没有标签的情况下，新的表示z以这种方式学习，以便以紧凑的形式捕获有关数据结构的有用信息，这对机器学习算法来说是友好的。
- en: '![](../Images/CH07_F02_Raff.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图7.2 自动编码的过程](../Images/CH07_F02_Raff.png)'
- en: Figure 7.2 The process of autoencoding. The input goes through an encoder that
    produces a new representation of the data z. The representation z can be useful
    for lots of tasks like clustering, search, and even classification. The decoder
    attempts to reconstruct the original input x from the encoding z.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 自动编码的过程。输入通过编码器，生成数据z的新表示。表示z可以用于许多任务，如聚类、搜索，甚至分类。解码器试图从编码z中重建原始输入x。
- en: This assumes that there is a representation z to be found that somehow captures
    information about the data x but is unspecified. We don’t know what z should look
    like because we have never observed it. For this reason, we call it a *latent*
    representation of the data because it is not visible to us but emerges from training
    the model.[¹](#fn20)
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 这假设存在一个要找到的表示z，它以某种方式捕捉了关于数据x的信息，但未指定。因为我们从未观察过它，所以我们不知道z应该是什么样子。因此，我们称其为数据的*潜在*表示，因为它对我们来说是不可见的，但它从训练模型中产生。[¹](#fn20)
- en: 'This may seem very silly at first glance. Couldn’t we just learn a function
    *f*^(in) that does not do anything and returns the input as output? That would
    trivially solve the problem, but it’s like taking out a loan from the bank only
    to *immediately* pay it back. You have *technically* satisfied all the explicit
    goals, but you didn’t accomplish anything by doing it. This silliness is the dangerous
    shortcut that we want to avoid. The trick is to set up the problem so the network
    can’t cheat like this. There are a number of different approaches to doing this,
    and we learn about two of them in this chapter: bottlenecks and denoising. Both
    of these work by constraining the network so that it is impossible to learn the
    naive solution.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这乍一看可能显得非常愚蠢。我们难道不能学习一个什么也不做的函数 *f*^(in)，它将输入作为输出返回吗？这可以轻易地解决问题，但就像从银行贷款然后立即还清一样。你*技术上*满足了所有明确的目标，但这样做并没有完成任何事情。这种愚蠢是我们要避免的危险捷径。诀窍是设置问题，让网络不能这样作弊。有几种不同的方法可以做到这一点，我们将在本章中了解其中的两种：瓶颈和去噪。这两种方法都通过约束网络，使其不可能学习到原始解决方案。
- en: 7.1.1  Principle component analysis is a bottleneck autoencoder
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 原始成分分析是一个瓶颈自动编码器
- en: This block of the chapter is a little more challenging to work through, but
    it’s worth it. You’ll gain a new perspective on a classic algorithm that will
    help you further bridge how existing tools that aren’t generally thought of as
    deep learning are actually deeply related.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的这一部分内容在理解上可能稍微有些挑战，但这是值得的。你将获得对经典算法的新视角，这将帮助你进一步理解那些通常不被视为深度学习的现有工具实际上是如何深度相关的。
- en: 'To exemplify how you can constrain a network so that it can’t learn the naive
    `superUsefulFunction` approach, we first talk about a famous algorithm you probably
    know about but may not be aware is secretly an autoencoder: the feature engineering
    and dimensionality reduction technique known as *principle component analysis*
    (PCA). PCA is used to convert a feature vector in a D-dimensional space down to
    a lower dimension *D*′, which one might call an *encoder*. PCA also includes a
    seldom-used *decoder* step, where you can convert back (approximately) to the
    original D dimensional space. The following annotated equation defines the optimization
    problem that PCA solves:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明如何约束一个网络，使其不能学习简单的 `superUsefulFunction` 方法，我们首先讨论一个你可能知道但可能不知道实际上是自动编码器的著名算法：称为
    *主成分分析* (PCA) 的特征工程和降维技术。PCA用于将D维空间中的特征向量转换到较低维度 *D*′，这可以称为 *编码器*。PCA还包括一个很少使用的
    *解码器* 步骤，其中你可以（近似地）转换回原始的D维空间。以下注释的方程式定义了PCA解决的优化问题：
- en: '![](../Images/CH07_UN01_Raff.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN01_Raff.png)'
- en: 'PCA is an important and widely used algorithm, so if you want to use it, you
    should use one of the existing implementations. But PyTorch is flexible enough
    for us to implement PCA ourselves. If you look closely at the equation, PCA is
    a *regression* problem. How? Let’s look at the main part of the equation and try
    to annotate it again with deep learning style equations:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PCA 是一个重要且广泛使用的算法，因此如果您想使用它，您应该使用现有的实现之一。但 PyTorch 足够灵活，让我们可以自己实现 PCA。如果您仔细观察方程式，PCA
    是一个 *回归* 问题。如何？让我们看看方程式的主要部分，并尝试用深度学习风格的方程式再次注释它：
- en: '![](../Images/ch7-eqs-to-illustrator0x.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch7-eqs-to-illustrator0x.png)'
- en: 'We have one weight matrix W acting as the encoder, and its transpose *W*^⊤
    is acting as the decoder. That means PCA is using *weight sharing* (remember that
    concept from chapters 3 and 4?). The original input is on the left, and we have
    the 2-norm (∥ ⋅ ∥[2]²), which is used by the mean squared error loss. The “subject
    to” part of the equation is a *constraint* requiring the weight matrix to behave
    in a particular way. Let’s rewrite this equation in the same manner we have been
    doing for our neural networks:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个权重矩阵W作为编码器，其转置 *W*^⊤ 作为解码器。这意味着PCA正在使用 *权重共享*（记得第3章和第4章中的这个概念？）。原始输入在左侧，我们使用了2-范数（∥
    ⋅ ∥[2]²），这是均方误差损失中使用的。方程式中的“约束条件”部分是一个要求权重矩阵以特定方式行为的 *约束*。让我们以我们为神经网络所做的方式重写这个方程式：
- en: '![](../Images/CH07_UN02_Raff.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN02_Raff.png)'
- en: ∥*WW*^⊤ − *I*∥[2]² is a regularization penalty based on the “subject to” constraint.
    Personally, I think re-expressing PCA as an autoencoder using a loss function
    helps make it easier to understand it. It’s also more explicit that we are using
    *f*(⋅) as a single network that encompasses the sequence of the encoder *f*^(in)(⋅)
    and decoder *f*^(out)(⋅).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ∥*WW*^⊤ − *I*∥[2]² 是基于“约束条件”的正则化惩罚。我个人认为，使用损失函数将PCA重新表达为自动编码器有助于更好地理解它。这也更明确地表明，我们正在使用
    *f*(⋅) 作为包含编码器 *f*^(in)(⋅) 和解码器 *f*^(out)(⋅) 序列的单个网络。
- en: 'Now we have written our PCA as a loss function over all our data points. We
    know PCA works, and if PCA is an autoencoder, then the idea of autoencoding can’t
    be as crazy as it first seems. So how does PCA make it work? The insight that
    PCA provides is that we make the *intermediate representation too small*. Remember
    that the first thing PCA does it go from D dimensions down to *D*′ < *D*. Imagine
    if *D* = 1, 000, 000 and *D*′ = 2. There is *no possible way* to save enough information
    about 1 million features in just 2 features to be able to reconstruct the input
    perfectly. So the best that PCA can do is learn the best 2 features possible,
    which then forces PCA to learn something useful. This is the primary trick to
    make autoencoding work: push your data into a smaller representation than you
    started with.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将我们的PCA写成所有数据点的损失函数。我们知道PCA是有效的，如果PCA是一个自动编码器，那么自动编码的思路可能不像最初看起来那么疯狂。那么PCA是如何让它工作的呢？PCA提供的洞察是，我们使
    *中间表示过于小了*。记住PCA首先做的事情是从D维下降到 *D*′ < *D*。想象一下，如果 *D* = 1,000,000 且 *D*′ = 2，那么在只有2个特征的情况下保存足够关于一百万个特征的信息以完美重建输入是不可能的。所以PCA能做的最好的事情就是学习可能的最佳2个特征，这迫使PCA学习一些有用的东西。这是使自动编码工作起来的主要技巧：将你的数据推入比开始时更小的表示。
- en: 7.1.2  Implementing PCA
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 实现PCA
- en: Now that we understand how PCA is an autoencoder, let’s work on converting it
    into PyTorch code. We need to define our network function *f*(*x*), given by the
    following equation. But how do we implement the rightmost *W*^⊤ part?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 PCA 是一个自动编码器，让我们着手将其转换为 PyTorch 代码。我们需要定义我们的网络函数 *f*(*x*)，由以下方程给出。但我们如何实现最右边的
    *W*^⊤ 部分？
- en: '![](../Images/ch7-eqs-to-illustrator1x.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch7-eqs-to-illustrator1x.png)'
- en: 'The main trick we need to implement this is to reuse the weight from a `nn.Linear`
    layer in PyTorch. We walk through implementing PCA in PyTorch here to hammer home
    the fact that *PCA is an autoencoder*. Once we have implemented PCA, we will make
    a few changes to convert it into a deep autoencoder, similar to how we moved from
    linear regression into a neural network in chapter 2\. First, let’s quickly define
    some constants for the number of features we are working with, how large our hidden
    layers should be, and other standard items we want:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这个功能的主要技巧是重用 PyTorch 中 `nn.Linear` 层的权重。我们在这里通过实现 PyTorch 中的 PCA 来强调 *PCA
    是一个自动编码器* 的事实。一旦我们实现了 PCA，我们将对其进行一些修改，将其转换为深度自动编码器，类似于我们在第 2 章中从线性回归过渡到神经网络的方式。首先，让我们快速定义一些常数，包括我们正在处理的特征数量、隐藏层的大小以及其他标准项：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ How many values are in the input? 28 * 28 images. We use this to help determine
    the size of subsequent layers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入中有多少个值？28 * 28 张图像。我们使用这个信息来帮助确定后续层的尺寸。
- en: ❷ Hidden layer size
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 隐藏层大小
- en: ❸ How many channels are in the input?
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输入中有多少个通道？
- en: ❹ How many classes?
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 有多少个类别？
- en: Next, let’s implement this missing layer to represent *W*^⊤. We call this new
    layer a *transpose layer*. Why? Because the mathematical operation we are using
    is called a *transpose*. We also add some logic to have a custom bias term for
    the weight transposed layer because the input layer has a matrix with shape *W*
    ∈ ℝ^(*D* × *D*′) and a bias vector **b** ∈ ℝ^(*D*′). That means *W*^⊤ ∈ ℝ^(*D*′
    × *D*), but we can’t really take a meaningful transpose of b. So if someone wants
    a bias term, it has to be a new separate one.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们实现这个缺失的层来表示 *W*^⊤。我们称这个新层为 *转置层*。为什么？因为我们使用的数学操作被称为 *转置*。我们还添加了一些逻辑，以便为权重转置层提供一个自定义的偏置项，因为输入层有一个形状为
    *W* ∈ ℝ^(*D* × *D*′) 的矩阵和一个偏置向量 **b** ∈ ℝ^(*D*′)。这意味着 *W*^⊤ ∈ ℝ^(*D*′ × *D*)，但我们实际上无法对
    b 进行有意义的转置。因此，如果有人想要偏置项，它必须是一个新的独立项。
- en: Our new `TransposeLinear` module follows. This class implements the `Transpose`
    operation *W*^⊤. The matrix to transpose W must be passed in as the `linearLayer`
    in the constructor. This way, we can share weights between an original `nn.Linear`
    layer and this transposed version of that layer.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来介绍新的 `TransposeLinear` 模块。这个类实现了 `Transpose` 操作 *W*^⊤。需要将转置矩阵 W 作为构造函数中的
    `linearLayer` 传入。这样，我们就可以在原始 `nn.Linear` 层和这个转置版本之间共享权重。
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Our class extends nn.Module. All PyTorch layers must extend this.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们这个类扩展了 nn.Module。所有 PyTorch 层都必须扩展这个。
- en: ❷ Creates a new variable weight to store a reference to the original weight
    term
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个新的变量 weight 来存储对原始权重项的引用
- en: ❸ Creates a new bias vector. By default, PyTorch knows how to update Modules
    and Parameters. Since tensors are neither, the Parameter class wraps the Tensor
    class so PyTorch knows that the values in this tensor need to be updated by gradient
    descent.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个新的偏置向量。默认情况下，PyTorch 知道如何更新模块和参数。由于张量既不是模块也不是参数，参数类封装了张量类，这样 PyTorch 就知道这个张量中的值需要通过梯度下降进行更新。
- en: ❹ The Parameter class can’t take None as an input. So if we want the bias term
    to exist but be potentially unused, we can use the register_parameter function
    to create it. The important thing here is that PyTorch always sees the same parameters
    regardless of the arguments for the Module.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 参数类不能接受 None 作为输入。因此，如果我们想让偏置项存在但可能不被使用，我们可以使用 `register_parameter` 函数来创建它。这里重要的是，PyTorch
    不论模块的参数如何，总是看到相同的参数。
- en: ❺ The forward function is the code that takes an input and produces an output.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 前向函数是接收输入并产生输出的代码。
- en: ❻ The F directory of PyTorch contains many functions used by Modules. For example,
    the linear function performs a linear transform when given an input (we use the
    transpose of our weights) and a bias (if None, it knows to not do anything).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ PyTorch 的 F 目录包含许多模块使用的函数。例如，线性函数在给定输入（我们使用权重的转置）和偏置（如果为 None，它知道不执行任何操作）时执行线性变换。
- en: Now that we have our `TransposeLinear` layer completed, we can implement PCA.
    First is the architecture, which we break into the encoder and decoder portions.
    Because PyTorch `Module`s are also built from `Module`s, we can define the encoder
    and decoder as separate parts and use both as components in a final `Module`.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了`TransposeLinear`层的实现，我们可以实现PCA。首先，是架构，我们将它分解为编码器和解码器部分。因为PyTorch `Module`s也是由`Module`s构建的，所以我们可以将编码器和解码器定义为单独的部分，并将它们作为最终`Module`的组件使用。
- en: 'Note that because the input comes in as an image with shape (**B**,1,28,28),
    and we are using linear layers, we first need to flatten the input into a vector
    of shape (**B**,28*28). But in the decode step, we want to have the same shape
    as the original data. We can use the `View` layer I’ve provided to convert it
    back. It works just like the `.view` and `.reshape` functions on tensors, except
    as a `Module`, for convenience:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，因为输入以形状为(**B**,1,28,28)的图像形式传入，而我们使用的是线性层，我们首先需要将输入展平成一个形状为(**B**,28*28)的向量。但在解码步骤中，我们希望保持与原始数据相同的形状。我们可以使用我提供的`View`层将其转换回来。它的工作方式与张量上的`.view`和`.reshape`函数类似，但作为一个`Module`，为了方便：
- en: '[PRE3]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Since we will share the weights of the linear layer, let’s define it separately.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于我们将共享线性层的权重，让我们单独定义它。
- en: ❷ The encoder flattens and then uses the linear layer.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编码器展平后使用线性层。
- en: ❸ The decoder uses our TransposeLinear layer and the now-shared linearLayer
    object.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解码器使用我们的TransposeLinear层和现在共享的linearLayer对象。
- en: ❹ Shapes the data back to its original form
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将数据形状恢复到原始形式
- en: ❺ Defines a final PCA model that has the sequence of an encoder followed by
    a decoder
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义一个最终PCA模型，它由编码器序列后跟解码器组成
- en: PCA initialization and loss function
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: PCA初始化和损失函数
- en: 'We have everything we need to train up this autoencoder. But to make it *truly*
    PCA, we need to add the *WW*^⊤ = *I* constraint. This constraint has a name: *orthogonality*.
    We won’t go into the derivation of *why* PCA has this, but we will include it
    as a good exercise. We start our model in the right place by giving it an initial
    *random* set of orthogonal weights using the `nn.init.orthogonal_` function. That
    just takes one line of code:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了训练这个自动编码器所需的一切。但要使其*真正*成为PCA，我们需要添加*WW*^⊤ = *I*约束。这个约束有一个名字：*正交性*。我们不会深入探讨为什么PCA有这个，但我们会将其作为一个很好的练习包括在内。我们通过使用`nn.init.orthogonal_`函数给模型提供一个初始的*随机*正交权重集来开始我们的模型，这只需要一行代码：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We aren’t going to strictly enforce orthogonality during training because the
    code to do that would be a little uglier than I want. Instead, we take a common
    and simple approach to *encourage* orthogonality but not *require* it.[²](#fn21)
    This is done by converting the equality *W*^T*W* = *I* into a *penalty* or *regularizer*
    ∥*WW*^⊤ − *I*∥[2]². This works because if the penalty is 0, then W is orthogonal;
    and if the penalty is nonzero, it will increase the loss, and thus gradient decent
    will try to make W more orthogonal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练过程中不会严格强制正交性，因为实现这一功能的代码会比我想象的要丑陋一些。相反，我们采取了一种常见且简单的方法来*鼓励*正交性，但并不*要求*它。[²](#fn21)
    这通过将等式 *W*^T*W* = *I* 转换为一个*惩罚*或*正则化器* ∥*WW*^⊤ − *I*∥[2]² 来实现。这是因为如果惩罚为0，则W是正交的；如果惩罚不为零，它将增加损失，因此梯度下降将尝试使W更加正交。
- en: 'It is not hard to implement this. We are using the mean square error (MSE)
    loss function ℓ[MSE](*f*(**x**),**x**) to train the self-supervision part. We
    can just augment this loss function with the loss over the penalty:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点并不困难。我们使用均方误差（MSE）损失函数 ℓ[MSE](*f*(**x**),**x**) 来训练自监督部分。我们只需将惩罚的损失添加到这个损失函数中即可：
- en: '![](../Images/ch7-eqs-to-illustrator2x.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/ch7-eqs-to-illustrator2x.png)'
- en: 'The following block of code does this. As an additional step, we decrease the
    strength of the regularizer by a factor of 0.1 to reinforce that the autoencoding
    portion is more important than the orthogonality portion:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块实现了这一点。作为额外步骤，我们将正则化器的强度降低0.1倍，以强调自动编码部分比正交性部分更重要：
- en: '[PRE5]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Original loss function
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 原始损失函数
- en: ❷ Our PCA loss function
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的PCA损失函数
- en: ❸ Grabs W from the linearLayer object we saved earlier
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从我们之前保存的linearLayer对象中获取W
- en: ❹ The identity matrix that is the target for regularization
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 目标正则化的单位矩阵
- en: ❺ Computes the original loss ℓ[MSE](*f*(**x**),**x**)
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算原始损失 ℓ[MSE](*f*(**x**),**x**)
- en: ❻ Computes the regularizer penalty ℓ[MSE](*W*^⊤*W*,**I**)
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算正则化器惩罚 ℓ[MSE](*W*^⊤*W*,**I**)
- en: ❼ Returns the sum of the two losses
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 返回两个损失的加和
- en: 7.1.3  Implementing PCA with PyTorch
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 使用PyTorch实现PCA
- en: 'Now we want to create a wrapper for the MNIST datasets. Why? Because the default
    MNIST dataset will return data in pairs (**x**,*y*) for the input and label, respectively.
    But in our case, the *input is the label* because we are trying to predict the
    output from the input. So we extend the PyTorch `Dataset` class to take the original
    tuple **x**, *y* and instead return a tuple **x**, **x**. This way, our code keeps
    the convention that the first item in the tuple is the input, and the second item
    is the desired output/label:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想为MNIST数据集创建一个包装器。为什么？因为默认的MNIST数据集将分别以对(**x**,*y*)的形式返回输入和标签。但在这个案例中，*输入是标签*，因为我们正在尝试从输入预测输出。因此，我们扩展了PyTorch的
    `Dataset` 类，以接受原始元组 **x**, *y*，并返回一个元组 **x**, **x**。这样，我们的代码保持了元组中第一个元素是输入，第二个元素是期望的输出/标签的约定：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Throws away the original label
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 丢弃原始标签
- en: Note If you were implementing an autoencoder for a real-world problem, you would
    have code that looks more like `x = self.dataset.__getitem__(idx)`,because you
    wouldn’t know the label `y`. Then you could `return x, x`.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你正在为一个实际问题实现自动编码器，你的代码看起来可能更像是 `x = self.dataset.__getitem__(idx)`，因为你不知道标签
    `y`。然后你可以 `return x, x`。
- en: 'With this `AutoEncodeDataset` wrapper in hand, we can load the original MNIST
    dataset and wrap it with `AutoEncodeDataset`, and we’ll be ready to start training:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个 `AutoEncodeDataset` 包装器在手，我们可以加载原始MNIST数据集，并用 `AutoEncodeDataset` 包装它，然后我们就可以开始训练了：
- en: '[PRE7]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now we can train this PCA model the same way we have been training other neural
    networks. The `AutoEncodeDataset` makes the input also act as the label, `pca_model`
    combines the sequence of encoding and decoding the data, and `mseWithOrthoLoss`
    implements a PCA-specific loss function that combines: 1) making the output look
    like the input ℓ[MSE](*f*(*x*),*x*), and 2) maintaining the orthogonal weights
    that PCA desires (∥*W*^⊤*W* − *I*∥[2]² = 0):'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以以训练其他神经网络相同的方式训练这个PCA模型。`AutoEncodeDataset` 使得输入也充当标签，`pca_model` 结合了编码和解码数据的序列，而
    `mseWithOrthoLoss` 实现了一个PCA特定的损失函数，该函数结合了：1) 使输出看起来像输入 ℓ[MSE](*f*(*x*),*x*)，和
    2) 维护PCA所需的正交权重 (∥*W*^⊤*W* − *I*∥[2]² = 0)：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 7.1.4  Visualizing PCA results
  id: totrans-83
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4  可视化PCA结果
- en: 'You may have noticed that we used the hidden layer size *n* = 2. This was intentional
    because it lets us *plot* the results and build some good visual intuition about
    how autoencoders work. This is because we can use PCA to visualize our data in
    two dimensions when *n* = 2. This is a very common use case for PCA. Even if we
    used a larger target dimension, projecting the data down can make it faster and/or
    more accurate to search for similar data. So it is useful to have a function that
    will take a dataset and encode it all to the lower-dimensional space. The following
    function does that and copies the labels so that we can look at our results compared
    to the ground truth of the MNIST test data:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到我们使用了隐藏层大小 *n* = 2。这是故意的，因为这让我们能够 *绘制* 结果，并建立一些关于自动编码器如何工作的良好视觉直觉。这是因为当
    *n* = 2 时，我们可以使用PCA将数据可视化在二维空间中。这是PCA的一个非常常见的用例。即使我们使用了更大的目标维度，将数据投影下来可以使搜索相似数据更快和/或更准确。因此，有一个函数可以接受数据集并将其全部编码到低维空间中是有用的。以下函数就是这样做的，并复制了标签，这样我们就可以将我们的结果与MNIST测试数据的真实情况进行比较：
- en: '[PRE9]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Creates space to store the results
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建空间以存储结果
- en: ❷ Switches to eval mode
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 切换到评估模式
- en: ❸ Switches to CPU mode for simplicity, but you don’t have to
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了简单起见，切换到CPU模式，但你不一定需要这样做。
- en: ❹ We don’t want to train, so torch.no_grad!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们不想训练，所以使用 torch.no_grad!
- en: ❺ Encodes the original data
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对原始数据进行编码
- en: ❻ Stores the encoded version and label
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 存储编码版本和标签
- en: ❼ Turns the results into single large NumPy arrays
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 将结果转换为单个大型NumPy数组
- en: ❽ Returns the results
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 返回结果
- en: ❾ Projects our data
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 投影我们的数据
- en: 'Using the `encode_batch` function, we have now applied PCA to the dataset,
    and we can plot the results with seaborn. This should look like a very familiar
    PCA plot: some classes have decent separation from the others, while some are
    clumped together. The following code has this odd bit: `hue=[str(l) for l in labels], hue_order=[str(i) for i in range(10)]`,
    which is included to make the plot easier to read. If we used `hue=labels`, the
    code would work fine, but seaborn would give all the digits similar colors and
    that would be hard to read. By making the labels strings (`hue=[str(l) for l in labels]`),
    we get seaborn to give each class a more distinct color, and we use the `huge_order`
    to make seaborn plot the classes in the order we expect:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`encode_batch`函数，我们现在已经将PCA应用于数据集，并且可以使用seaborn绘制结果。这应该看起来像一个非常熟悉的PCA图：一些类与其他类有相当好的分离，而有些则聚集在一起。以下代码中有这样一个奇怪的片段：`hue=[str(l)
    for l in labels], hue_order=[str(i) for i in range(10)]`，这是为了使图表更容易阅读。如果我们使用`hue=labels`，代码将正常工作，但seaborn会给所有数字相似的色彩，这将很难阅读。通过将标签转换为字符串（`hue=[str(l)
    for l in labels]`），我们让seaborn给每个类一个更明显的颜色，并使用`hue_order`来让seaborn按我们期望的顺序绘制类：
- en: '[PRE10]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/CH07_UN03_Raff.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN03_Raff.png)'
- en: 'From this plot, we can get some ideas about the quality of the encoding. For
    example, it’s probably easy to differentiate the 0 and 1 classes from all the
    others. Some of the others might be much harder, though; and in a truly unsupervised
    scenario, where we don’t know the true labels, we won’t be able to easily discover
    the distinct concepts. Another thing we can use to help judge this is the encode/decode
    process. If we did a good job, the output should be the same as the input. First,
    we define a simple helper function to plot the original input x on the left and
    the encoded-decoded version on the right:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，我们可以得到一些关于编码质量的看法。例如，区分0和1类与其他所有类可能很容易。尽管如此，有些其他类可能很难区分；在一个真正无监督的场景中，如果我们不知道真正的标签，我们将无法轻易发现不同的概念。我们可以用来帮助判断的另一件事是编码/解码过程。如果我们做得好，输出应该与输入相同。首先，我们定义一个简单的辅助函数来在左侧绘制原始输入x，在右侧绘制编码-解码版本：
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Switches to eval mode
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 切换到评估模式
- en: ❷ Moves things to the CPU so we don’t have to think about what device anything
    is on and because this function is not performance-sensitive
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将操作移至CPU，这样我们就不必考虑任何设备是什么，因为这个函数的性能不敏感
- en: ❸ Always no_grad if you are not training
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果您不在训练，请始终使用no_grad
- en: ❹ Uses Matplotlib to create a side-by-side plot with the original on the left
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用Matplotlib创建一个左侧为原始数据的并排图
- en: 'We reuse this function throughout this chapter. First let’s look at some input-output
    combinations for a few different digits:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中重用这个函数。首先让我们看看一些不同数字的输入-输出组合：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Shows the input (left) and output (right) for three data points
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 展示了三个数据点的输入（左侧）和输出（右侧）
- en: '![](../Images/CH07_UN04_Raff.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN04_Raff.png)'
- en: '![](../Images/CH07_UN05_Raff.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN05_Raff.png)'
- en: '![](../Images/CH07_UN06_Raff.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN06_Raff.png)'
- en: These results match what we expected based on the 2D plot. The 0 and 1 classes
    look kind of like a 1 and 0 after we encode and decode them. The 7 . . . not so
    much. Why? Well, we are converting 784 dimensions down to 2\. That’s a *lot* of
    information compression—much more than we can reasonably expect poor PCA to be
    able to do.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 这些结果与我们根据二维图所预期的相符。经过编码和解码后，0和1类看起来就像1和0。至于7类，则不然。为什么？嗯，我们正在将784个维度压缩到2个。这是大量的信息压缩——远远超过我们合理期望的PCA能够做到的。
- en: 7.1.5  A simple nonlinear PCA
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.5  一个简单的非线性PCA
- en: 'PCA is one of the simplest autoencoders we could design because it is a completely
    linear model. What happens if we just add a little of what we have learned about?
    We can add a single nonlinearity and remove the weight sharing to turn this into
    a small nonlinear autoencoder. Let’s see how that looks:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是我们能设计的最简单的自动编码器之一，因为它是一个完全线性的模型。如果我们只添加一点我们所学的，会发生什么？我们可以添加一个单非线性并去除权重共享，将其转换为一个小的非线性自动编码器。让我们看看这会是什么样子：
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Augments the encoder with a Tanh nonlinearity
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用Tanh非线性增强编码器
- en: '❷ The only real change: adding a nonlinear operation at the end'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 唯一真正的变化：在最后添加一个非线性操作
- en: ❸ The decoder gets its own Linear layer, making it look more like a normal network.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 解码器获得自己的线性层，使其看起来更像一个正常网络。
- en: ❹ We are no longer tying the weights, for simplicity.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们不再绑定权重，为了简单起见。
- en: ❺ Combines them into the encoder-decoder function *f*(⋅)
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将它们组合成编码器-解码器函数 *f*(⋅)
- en: 'Since we are no longer sharing weights between the encoder and decoder, we
    do not care if the weights are orthogonal. So when we train this model, we use
    the normal MSE loss:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不再在编码器和解码器之间共享权重，所以我们不关心权重是否正交。因此，当我们训练这个模型时，我们使用正常的均方误差损失：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'In the following blocks of code, we again plot all the 2D encodings and our
    three encoded-decoded images to see visually what has changed. This lets us look
    subjectively to see if the quality is better:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码块中，我们再次绘制所有二维编码和我们的三个编码-解码图像，以直观地查看发生了什么变化。这让我们可以主观地判断质量是否有所提高：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](../Images/CH07_UN07_Raff.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN07_Raff.png)'
- en: '[PRE16]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](../Images/CH07_UN08_Raff.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN08_Raff.png)'
- en: '![](../Images/CH07_UN09_Raff.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN09_Raff.png)'
- en: '![](../Images/CH07_UN10_Raff.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN10_Raff.png)'
- en: 'Overall, the change is noticeable but not clearly different. The 2D plot still
    has a lot of overlap. The encode-decode images show a few artifacts: 0, 1, and
    7 are qualitatively similar but with different styles. What was the point? We
    have turned PCA into an autoencoder with one nonlinearity, so we have modified
    the PCA algorithm; and because we did so in PyTorch, it trained up and worked
    out of the box. Now we can try to make bigger changes to get a better result.
    As a theme of deep learning, if we make this model deeper by adding more layers,
    we should be able to successfully improve the results.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 总体来说，变化是明显的，但并不明显不同。二维图仍然有很多重叠。编码-解码图像显示了一些伪影：0、1和7在定性上相似，但风格不同。目的是什么？我们将PCA转换为一个具有一个非线性函数的自动编码器，因此我们修改了PCA算法；并且因为我们使用PyTorch进行操作，所以它训练起来并且直接工作。现在我们可以尝试进行更大的改动以获得更好的结果。作为深度学习的一个主题，如果我们通过添加更多层来使这个模型更深，我们应该能够成功地提高结果。
- en: 7.2 Designing autoencoding neural networks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 设计自动编码神经网络
- en: PCA is a very popular method for dimensionality reduction and visualization,
    and any situation where you would use PCA is one where you may want to instead
    use an *autoencoding network*. An autoencoding network is the same idea, but we
    will make the encoder and decoder larger networks with more layers so they can
    learn more powerful and complex encoders and decoders. Since we have seen that
    PCA is an autoencoder, an autoencoding network may be a more accurate choice by
    being able to learn more complex functions.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: PCA是一种非常流行的降维和可视化方法，任何你可能使用PCA的情况，你可能会想使用*自动编码网络*。自动编码网络有相同的概念，但我们将编码器和解码器设计成更大的网络，具有更多层，以便它们可以学习更强大和复杂的编码器和解码器。因为我们已经看到PCA是一个自动编码器，所以自动编码网络可能是一个更准确的选择，因为它能够学习更复杂的函数。
- en: Autoencoding networks are also useful for *outlier detection*. You want to detect
    outliers so that you can have them manually reviewed, because an outlier is unlikely
    to be handled well by a model. An autoencoder can detect outliers by looking at
    how well it reconstructed the input. If you can reconstruct the input well, the
    data probably looks like normal data you’ve seen. If you *cannot* successfully
    reconstruct the input, it is probably *unusual* and thus an outlier. This process
    is summarized in figure 7.3\. You can use this approach to find potentially bad
    data points in your training data or validate user-submitted data (e.g., if someone
    uploads a picture of their face to an application that looks for ear infections,
    you should detect the face as an outlier and not make a diagnosis).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码网络也适用于*异常值检测*。你想要检测异常值，以便你可以手动审查它们，因为异常值不太可能被模型很好地处理。自动编码器可以通过查看其重建输入的好坏来检测异常值。如果你可以很好地重建输入，数据可能看起来像你见过的正常数据。如果你*不能*成功重建输入，它可能*不寻常*，因此是一个异常值。这个过程总结在图7.3中。你可以使用这种方法来找到训练数据中的潜在不良数据点或验证用户提交的数据（例如，如果有人上传他们的脸部照片到一个寻找耳部感染的应用程序，你应该将脸部检测为异常值，而不是做出诊断）。
- en: '![](../Images/CH07_F03_Raff.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F03_Raff.png)'
- en: Figure 7.3 Example application of autoencoders to detect outliers. A dataset
    is used to train the autoencoder, and then you compute all the reconstruction
    errors. Assuming that 0.1% of the data is abnormal, it should be the most difficult
    to reconstruct. If you find the threshold for the top 0.1% of errors, you can
    apply that threshold to new data to detect outliers. Outliers are often ill-behaved,
    and it may be better to treat them differently or give them an extra review. Outlier
    detection can be done on training data or new testing data. You can change the
    0.1% to match what you believe is happening in your dataset.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 自动编码器检测异常值的示例应用。使用数据集来训练自动编码器，然后计算所有重建误差。假设0.1%的数据是异常的，它应该是最难重建的。如果你找到了前0.1%错误率的阈值，你可以将这个阈值应用于新数据以检测异常值。异常值通常表现不佳，可能最好以不同的方式处理它们或给予它们额外的审查。异常值检测可以在训练数据或新测试数据上执行。你可以将0.1%更改为与你在数据集中相信正在发生的情况相匹配。
- en: 'Now let’s talk about how to set up a deep learning-based autoencoder. The standard
    approach is to make a symmetric architecture between the encoder and decoder:
    keep the same number of layers in each and put them in the reverse order (encoder
    goes from big to small, and decoder goes from small to big). We also use a *bottleneck*
    style encoder, meaning the layers have progressively fewer neurons. This is shown
    in figure 7.4.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来谈谈如何设置基于深度学习的自动编码器。标准方法是在编码器和解码器之间建立一个对称架构：保持每边的层数相同，并将它们以相反的顺序放置（编码器从大到小，解码器从小到大）。我们还使用了一种*瓶颈*风格的编码器，这意味着层中的神经元数量逐渐减少。这如图7.4所示。
- en: '![](../Images/CH07_F04_Raff.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F04_Raff.png)'
- en: Figure 7.4 Example of a standard autoencoder design. The input comes in on the
    left. The encoder starts out large and tapers down the size of the hidden layers
    at each step. Because autoencoders are usually symmetric, our decoder will receive
    the small representation z and begin expanding it back to the original size.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 标准自动编码器设计的示例。输入从左侧进入。编码器一开始很大，然后在每一步逐渐减小隐藏层的大小。因为自动编码器通常是对称的，所以我们的解码器将接收小的表示z，并开始将其扩展回原始大小。
- en: Autoencoders do not *have* to be symmetric. They work *just fine* if they are
    not. This is done purely to make it easier to think and reason about the network.
    This way, you are making half as many decisions about how many layers are in the
    network, how many neurons are in each layer, and so on.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码器*不*必须是对称的。如果它们不是对称的，它们也能正常工作。这样做纯粹是为了使思考和理解网络更容易。这样，你将网络中层的数量、每层的神经元数量等决策减少了一半。
- en: The bottleneck in the encoder *is* important, though. Just as PCA did, by pushing
    down to a smaller representation, we make it impossible for the network to cheat
    and learn the naive solution of immediately returning the input as the output.
    Instead, the network must learn to identify high-level concepts like “there is
    a circle located at the center,” which could be used for encoding (and then decoding)
    the numbers 6 and 0\. By learning multiple high-level concepts, the network is
    forced to start learning useful representations.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器中的瓶颈*非常重要*。就像PCA所做的那样，通过将其压缩到更小的表示，我们使得网络无法作弊并学习到立即返回输入作为输出的天真解决方案。相反，网络必须学会识别高级概念，例如“有一个位于中心的圆圈”，这可以用于编码（然后解码）数字6和0。通过学习多个高级概念，网络被迫开始学习有用的表示。
- en: Note The autoencoder can also be seen as a way to produce embeddings. The encoder
    part of the network in particular makes an excellent candidate for an embedding
    construction. The approach is unsupervised, so you don’t need labels, and the
    encoder’s lower-dimensional output works better with common tools for visualization
    and nearest-neighbor search.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：自动编码器也可以被视为生成嵌入的一种方式。网络中的编码器部分尤其适合作为嵌入构建的候选者。这种方法是无监督的，因此你不需要标签，编码器的低维输出更适合与常见的可视化工具和最近邻搜索工具一起使用。
- en: 7.2.1  Implementing an autoencoder
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 实现自动编码器
- en: 'Since we have gone through the pain of implementing PCA, the autoencoder should
    be easier and more straightforward when done in this style, primarily because
    we do not share any weights across layers and no longer need the orthogonal constraint
    on the weights. For simplicity, we focus on fully connected networks for autoencoders,
    but the concepts are widely applicable. To start, let’s define another helper
    function `getLayer` that creates a single hidden layer for us to place in a network,
    similar to what we did in chapter 6:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经经历了实现PCA的痛苦，因此在以这种方式完成时，自动编码器应该更容易、更直接，主要是因为我们不再在层之间共享任何权重，也不再需要权重上的正交约束。为了简单起见，我们专注于全连接网络用于自动编码器，但这些概念具有广泛的应用性。首先，让我们定义另一个辅助函数`getLayer`，它为我们创建一个单独的隐藏层，以便放置在网络上，类似于我们在第6章中所做的：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Organizes the conceptual “block" of a hidden layer into a Sequential object
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将隐藏层的概念“块”组织成一个Sequential对象
- en: 'With this helper function in hand, the following code shows how easy it is
    to implement an autoencoder using the more advanced tools we have learned about,
    like batch normalization and ReLU activations. It uses a simple strategy of decreasing
    the number of neurons in each hidden layer by a fixed pattern. In this case, we
    divide the number of neurons by 2, then 3, then 4, and so on until the last layer
    of the decoder, where we jump straight to the target size *D*′. The pattern used
    to decrease the number of layers is not that important, as long as the size of
    the layers consistently decreases:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有这个辅助函数后，以下代码展示了如何使用我们所学到的更高级的工具（如批量归一化和ReLU激活）轻松实现自动编码器。它采用了一种简单的策略，即通过固定的模式减少每个隐藏层中的神经元数量。在这种情况下，我们将神经元数量除以2，然后是3，然后是4，依此类推，直到解码器的最后一层，我们直接跳到目标大小
    *D*′。用于减少层数量的模式并不重要，只要层的尺寸持续减小：
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Dividing by 2, 3, 4, etc., is one of many patterns that can be used.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将2、3、4等除以，是许多可用的模式之一。
- en: ❷ Each layer has a smaller output size than the previous one.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每一层的输出尺寸都比前一层小。
- en: ❸ Jumps down to the target dimension
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 跳到目标维度
- en: ❹ Decoder does the same layers/sizes in reverse to be symmetric
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 解码器以相反的顺序执行相同的层/尺寸，以保持对称性
- en: ❺ Each layer increases in size because we are in the decoder.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 由于我们处于解码器中，每一层都在增加尺寸。
- en: ❻ Reshapes to match the original shape
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 调整形状以匹配原始形状
- en: ❼ Combines into a deep autoencoder
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 结合成一个深度自动编码器
- en: 'As always, we can train this network using the exact same function. We stick
    with the mean squared error, which is very common in autoencoders:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 和往常一样，我们可以使用完全相同的函数来训练这个网络。我们坚持使用均方误差，这在自动编码器中非常常见：
- en: '[PRE19]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 7.2.2  Visualizing autoencoder results
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2  可视化自动编码器结果
- en: 'How did our new autoencoder do? The 2D plot shows *much* more separation in
    the projected dimension z. Classes 0, 6, and 3 are *very* well separated from
    all the others. In addition, the middle area where there are more classes next
    to each other at least has more continuity and uniformity in the class present.
    The classes have distinct homes in the middle area, rather than being smeared
    on top of each other:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的新自动编码器表现如何？2D图显示了投影维度z中*更多*的分离。类别0、6和3与其他类别*非常*好地分离。此外，中间区域（其中相邻的类别更多）在现有类别中至少具有更多的连续性和均匀性。类别在中部区域有独特的家园，而不是相互涂抹：
- en: '[PRE20]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![](../Images/CH07_UN11_Raff.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN11_Raff.png)'
- en: This is also one way to use autoencoders to explore unknown data. If we did
    not know the class labels, we might have concluded from this projection that there
    were likely at least two to four different subpopulations in the data.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这也是使用自动编码器探索未知数据的一种方法。如果我们不知道类别标签，我们可能会从这种投影中得出结论，数据中可能至少存在两个到四个不同的子群体。
- en: 'We can also look at some examples of the encode-decode cycle. Unlike before,
    the reconstructions are now crisp, with much less blur. But it’s not perfect:
    class 4 is usually hard to separate from others and has low-quality reconstructions:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以查看一些编码-解码周期的示例。与之前不同，重建现在清晰，模糊度大大减少。但并不完美：类别4通常很难与其他类别分离，并且重建质量较低：
- en: '[PRE21]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '![](../Images/CH07_UN12_Raff.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN12_Raff.png)'
- en: '![](../Images/CH07_UN13_Raff.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN13_Raff.png)'
- en: '![](../Images/CH07_UN14_Raff.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN14_Raff.png)'
- en: '![](../Images/CH07_UN15_Raff.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN15_Raff.png)'
- en: 'Try playing with the code and looking at the results for different data points.
    If you do, you may start to notice that the reconstructions do not always maintain
    the *style* of the inputs. This is more obvious with the 5 here: The reconstruction
    is smoother and more pristine than the original input. Is this a good thing or
    a bad thing?'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试与代码互动，观察不同数据点的结果。如果你这样做，你可能会开始注意到，重建并不总是保持输入的*风格*。这一点在数字5这里尤为明显：重建比原始输入更平滑、更纯净。这是好事还是坏事？
- en: From the perspective of how we trained the model, it’s a bad thing. The reconstruction
    *is different* from the input, and the goal was to *exactly* reconstruct the input.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们训练模型的角度来看，这是一个坏事情。重建与输入*不同*，而目标是*精确地*重建输入。
- en: 'However, our true goal is not to just learn to reconstruct inputs from themselves.
    We already have the input. Our goal was to learn a useful representation of the
    data without needing to know the data labels. From this perspective, the behavior
    is a good thing: it means there are multiple different potential “5”’s that could
    be the input and would be mapped to the same “5” reconstruction. In this sense,
    the network has on its own learned that there is a *canonical* or *prototypical*
    5, without being explicitly told about the concept of 5 or even that there are
    distinct numbers present.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的真正目标并不是仅仅学会从输入本身重建输入。我们已经有输入了。我们的目标是学习数据的有用表示，而无需知道数据的标签。从这个角度来看，这种行为是好事：这意味着可能有多个不同的潜在“5”可以作为输入，并且会被映射到相同的“5”重建。从这个意义上说，网络已经自学了存在一个*规范*或*典型*的5，而无需明确告知关于5的概念，甚至是否存在不同的数字。
- en: 'But the example of the digit 4 is a bad failure case. The network reconstructed
    a completely different digit because the restriction is *too strong*: the network
    was forced down to just two dimensions and couldn’t learn all the complexity in
    the data with such little space. This meant forcing out the concept of a 4\. Similar
    to PCA, if you give the network a larger bottleneck (more features to use), the
    reconstructions will steadily improve in quality. Using two dimensions is great
    for visualization in scatterplots, but for other applications, you probably want
    to use a few more features. (This, like most things in ML, is problem-specific.
    You should make sure you have a way to test your results to compare them, and
    then use that test to determine how many features you should use).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 但数字4的例子是一个失败的案例。网络重建了一个完全不同的数字，因为限制条件*太严格*：网络被迫降低到仅两个维度，并且无法在如此小的空间内学习数据中的所有复杂性。这意味着排除了4的概念。类似于PCA，如果你给网络更大的瓶颈（更多可用的特征），重建的质量将稳步提高。使用两个维度非常适合在散点图中进行可视化，但对于其他应用，你可能想使用更多一些的特征。（这，就像机器学习中的大多数事情一样，是特定于问题的。你应该确保你有方法来测试你的结果，以便进行比较，然后使用这个测试来确定你应该使用多少特征）。
- en: 7.3 Bigger autoencoders
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 更大的自动编码器
- en: 'All the autoencoding we have done so far has been based on projecting down
    to two dimensions, which we have already said makes the problem exceptionally
    hard. Your intuition should tell you that if we make the target dimension size
    *D*′ a little larger, our reconstructions should improve in quality. But what
    if we made the target size *larger* than the original input? Would this work?
    We can easily modify our autoencoder to try this and see what happens. In the
    following block of code, we simply jump up to *D*′ = 2 ⋅ *D* right after the first
    layer of the encoder and stay at that number of neurons for the entire process:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们迄今为止所做的所有自动编码都是基于将数据投影到两个维度，我们之前已经说过这使问题变得异常困难。你的直觉应该告诉你，如果我们把目标维度大小*D*′稍微增大，我们的重建质量应该会提高。但如果我们把目标大小做得比原始输入更大呢？这会起作用吗？我们可以轻松修改我们的自动编码器来尝试这一点，看看会发生什么。在下面的代码块中，我们只是在编码器的第一层之后简单地跳到*D*′
    = 2 ⋅ *D*，并在整个过程中保持这个神经元数量：
- en: '[PRE22]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We can’t make a 2D plot since we have too many dimensions. But we can still
    do the encode/decode comparisons on the data to see how our new autoencoder performs.
    If we plot some examples, it becomes clear that we now have *very good* reconstructions,
    which include even minute details from the original input. For example, the following
    7 has a slight uptick at the top left and a slightly thicker ending at the bottom,
    which are present in the reconstruction. The 4 that was completely mangled previously
    has a lot of unique curves and styles that are also faithfully preserved:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有很多维度，我们无法制作一个 2D 图。但我们仍然可以在数据上执行编码/解码比较，以查看我们的新自动编码器表现如何。如果我们绘制一些示例，就会变得明显，我们现在有*非常好*的重建，包括来自原始输入的细微细节。例如，下面的
    7 在左上角略有上升，底部略有加粗，这些都在重建中存在。之前完全混乱的 4 现在有大量独特的曲线和风格，这些也被忠实保留：
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH07_UN16_Raff.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN16_Raff.png)'
- en: '![](../Images/CH07_UN17_Raff.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN17_Raff.png)'
- en: '![](../Images/CH07_UN18_Raff.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN18_Raff.png)'
- en: 'So the question is, is this a better autoencoder than the previous one? Have
    we learned a useful representation? It’s a hard question to answer because we
    are using the input reconstruction as the loss of the network, but it is not what
    we truly care about. We want the network to learn *useful* representations. This
    is a variant of the classic unsupervised learning problem: if you don’t know what
    you are looking for, how do you know if you are doing a good job?'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，问题是，这个自动编码器是否比之前的更好？我们是否学习到了有用的表示？这是一个很难回答的问题，因为我们使用输入重建作为网络的损失，但这并不是我们真正关心的。我们希望网络学习到*有用*的表示。这是经典无监督学习问题的一个变体：如果你不知道你在找什么，你怎么知道你做得好不好？
- en: 7.3.1  Robustness to noise
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1  对噪声的鲁棒性
- en: To help us answer the question about which autoencoder is better, *D*′ = 2 or
    *D*′ = 2 ⋅ *D*, we will add some noise to our data. Why? One intuition we can
    use is that if a representation is good, it should be *robust*. Imagine if the
    clean data we have been using was like a road, and our model was the car. If the
    road is pristine and smooth, the car will handle well. But what if there are potholes
    and cracks (read, noise) in the road? A good car should still be able to drive
    successfully. Similarly, if we have noisy data, an ideal model will still perform
    well.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们回答哪个自动编码器更好的问题，*D*′ = 2 或 *D*′ = 2 ⋅ *D*，我们将向我们的数据添加一些噪声。为什么？我们可以使用的一个直觉是，如果一种表示法是好的，它应该是*鲁棒的*。想象一下，如果我们一直使用的干净数据就像一条路，我们的模型就像一辆车。如果这条路干净且平坦，车就能很好地行驶。但如果路上有坑洼和裂缝（即噪声）呢？一辆好车仍然应该能够成功驾驶。同样，如果我们有噪声数据，一个理想模型仍然会表现良好。
- en: There are *many* different ways we could make our data noisy. One of the easiest
    is to add noise from a normal distribution. We denote the normal distribution
    as *N*(*μ*,*σ*), where μ is the mean value returned and σ is the standard deviation.
    If s is a value sampled from the normal distribution, we denote that as *s* ∼
    *N*(*μ*,*σ*).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有*很多*种不同的方法可以使我们的数据变得噪声。其中一种最简单的方法是添加来自正态分布的噪声。我们用 *N*(*μ*,*σ*) 表示正态分布，其中
    μ 是返回的均值值，σ 是标准差。如果 s 是从正态分布中抽取的值，我们用 *s* ∼ *N*(*μ*,*σ*) 表示。
- en: 'To make our data noisy, we use PyTorch to construct an object that represents
    the normal distribution and perturbs the input data such that we get ![](../Images/tilde_x.png)
    = **x** + *s*, where *s* ∼ *N*(*μ*,*σ*). To represent the normal distribution
    *N*(*μ*,*σ*), PyTorch provides the `torch.distributions.Normal` class:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的数据变得噪声，我们使用 PyTorch 构造一个表示正态分布的对象，并扰动输入数据，以便我们得到 ![](../Images/tilde_x.png)
    = **x** + *s*，其中 *s* ∼ *N*(*μ*,*σ*)。为了表示正态分布 *N*(*μ*,*σ*)，PyTorch 提供了 `torch.distributions.Normal`
    类：
- en: '[PRE24]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ First argument is the mean μ; second is the standard deviation σ
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个参数是均值 μ；第二个是标准差 σ
- en: 'This class has a `sample` method that performs the *s*∼ step. We use the `sample_shape`
    argument to tell it that we want a tensor with a shape of `sample_shape` to be
    filled with random values from this distribution. The following function takes
    an input x and sample noise that is the same shape as x so we can add it, creating
    our noisy sample ![](../Images/tilde_x.png) = **x** + *s*:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这个类有一个执行 *s*∼ 步的 `sample` 方法。我们使用 `sample_shape` 参数来告诉它我们想要一个形状为 `sample_shape`
    的张量，用从这个分布中抽取的随机值填充。以下函数接受一个输入 x 和与 x 形状相同的噪声样本，这样我们就可以添加它，创建我们的噪声样本 ![](../Images/tilde_x.png)
    = **x** + *s*：
- en: '[PRE25]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ **x** + *s*
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ **x** + *s*
- en: 'With our simple `addNoise` function in place, we can try it with our big model.
    We have intentionally set the amount of noise to be fairly large to make the changes
    and differences between models obvious. For the following input data, you should
    see that the reconstructions are garbled, with extraneous lines. Since the noise
    is random, you can run the code multiple times to see different versions:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们放置了简单的 `addNoise` 函数之后，我们可以用我们的大型模型尝试它。我们故意将噪声量设置得相当大，以便使模型之间的变化和差异更加明显。对于以下输入数据，你应该看到重建结果混乱，有额外的线条。由于噪声是随机的，你可以多次运行代码以查看不同的版本：
- en: '[PRE26]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/CH07_UN19_Raff.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN19_Raff.png)'
- en: '![](../Images/CH07_UN20_Raff.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN20_Raff.png)'
- en: 'This would seem to indicate that our large autoencoder with *D*′ = 2 ⋅ *D*
    is not very robust. What happens if we apply the same noisy data to our original
    autoencoder, which uses *D* = 2? You can see that next. The 5 is reconstructed
    almost exactly as it was before: a little blurry, but clearly a 5:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎表明，我们的 *D*′ = 2 ⋅ *D* 的大型自动编码器并不非常鲁棒。如果我们将相同的噪声数据应用到原始自动编码器上，该编码器使用 *D* =
    2 会发生什么？你可以看到接下来。5 被重建得几乎与之前完全一样：有点模糊，但清楚地是一个 5：
- en: '[PRE27]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '![](../Images/CH07_UN21_Raff.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN21_Raff.png)'
- en: '![](../Images/CH07_UN22_Raff.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN22_Raff.png)'
- en: If you run the 4 through multiple times, sometimes you get a 4 back out of the
    decoder, and sometimes you get something else. This is because the noise is different
    every time, and our 2D plot showed that the 4 was being conflated with many other
    classes.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你多次运行 4，有时你会从解码器中得到一个 4，有时你会得到其他东西。这是因为噪声每次都不同，我们的 2D 图显示 4 正在被与其他许多类别混淆。
- en: Based on this experiment, we can see that as we make the encoding dimension
    *D*′ smaller, the model becomes *more robust*. If we let the encoding dimension
    become too large, it may be good at performing reconstructions on easy data, but
    it is not robust to changes and noise. Part of this is because when *D*′ ≥ *D*,
    it becomes easy for the model to learn a simple approach. It has more than enough
    capacity to copy the input and learn to regurgitate what it was given. By constraining
    the model with a smaller capacity (*D*′ ≤ *D*), the only way it can learn to solve
    the task is by creating a more compact representation of the input data. Ideally,
    you try to find a dimension *D*′ that fits a balance between reconstructing the
    data well but using as small an encoding dimension as possible.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这个实验，我们可以看到，随着我们使编码维度 *D*′ 变小，模型变得更加鲁棒。如果我们让编码维度变得过大，它可能在处理容易的数据时擅长重建，但它对变化和噪声不鲁棒。这部分原因是因为当
    *D*′ ≥ *D* 时，模型很容易学习一个简单的方法。它有足够的容量来复制输入并学会重复它所接受的内容。通过用较小的容量（*D*′ ≤ *D*）约束模型，它学习解决任务的唯一方法就是创建输入数据的更紧凑表示。理想情况下，你试图找到一个维度
    *D*′，它可以在很好地重建数据的同时，尽可能使用最小的编码维度。
- en: 7.4 Denoising autoencoders
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 去噪自动编码器
- en: It is not easy to balance having *D*′ small enough to be robust but large enough
    to do well at reconstruction. But there is a trick we can play that will allow
    us to have large *D*' > *D* *and* learn a robust model. The trick is to create
    what is called a *denoising autoencoder*. A denoising autoencoder adds noise to
    the encoder’s input while still expecting the decoder to produce a clean image.
    So our math goes from ℓ(*f*(**x**),**x**) to ℓ(*f*(![](../Images/tilde_x.png)),**x**.
    If we do this, there is no naive solution of just copying the input, because we
    perturb it before giving it to the network. The network must learn how to remove
    the noise, or denoise, the input and thus allows us to use *D*′ > *D* while still
    obtaining robust representations.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 要平衡使 *D*′ 足够小以保持鲁棒性，同时又要足够大以在重建方面表现良好，并非易事。但有一个技巧我们可以使用，这将允许我们拥有较大的 *D*′ > *D*
    并学习一个鲁棒模型。这个技巧就是创建所谓的 *去噪自动编码器*。去噪自动编码器在向编码器输入添加噪声的同时，仍然期望解码器能够生成一个干净的图像。因此，我们的数学从
    ℓ(*f*(**x**),**x**) 变为 ℓ(*f*(![](../Images/tilde_x.png)),**x**)。如果我们这样做，就没有简单的解决方案只是复制输入，因为我们是在将其交给网络之前对其进行扰动的。网络必须学习如何去除输入的噪声，或者去噪，从而允许我们在仍然获得鲁棒表示的同时使用
    *D*′ > *D*。
- en: Denoising networks have a lot of practical usages. If you can make synthetic
    noise that is realistic to the issues you might see in real life, you can create
    models that remove the noise and improve the accuracy by making the data cleaner.
    Libraries like scikit-image ([https://scikit-image.org](https://scikit-image.org))
    are available with many transforms that can be used to make noisy images, and
    I’ve personally used this approach to improve fingerprint recognition algorithms.[³](#fn22)
    How I used a denoising autoencoder is shown in figure 7.5, which is also a summary
    of how denoising autoencoders are typically set up. The original (or sometimes
    very clean) data comes in at the start, and we apply noise-generating processes
    to it. The more that noise looks like the kinds of issues you see in real data,
    the better. The noisy/corrupted version of the data is given as the input to the
    autoencoder, but the loss is computed against the original clean data.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪网络有很多实际应用。如果你可以创建出在现实生活中可能遇到的问题的合成噪声，你可以创建出去除噪声并通过使数据更干净来提高准确性的模型。例如，scikit-image
    ([https://scikit-image.org](https://scikit-image.org)) 这样的库提供了许多可以用来生成噪声图像的转换，我本人也使用这种方法来改进指纹识别算法。[³](#fn22)
    我如何使用去噪自动编码器在图7.5中展示，这也是去噪自动编码器通常设置的总结。原始（或有时非常干净）的数据在开始时进入，我们对其应用噪声生成过程。噪声看起来越像你在真实数据中看到的问题，效果越好。噪声/损坏的数据版本作为自动编码器的输入，但损失是针对原始干净数据计算的。
- en: '![](../Images/CH07_F05_Raff.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F05_Raff.png)'
- en: Figure 7.5 The denoising autoencoder processes applied to fingerprint images.
    This used special software that generates hyper-realistic fingerprint images,
    with the goal of removing that noise to make fingerprint processing less error
    prone. You can still get good results with simpler and unrealistic noise.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5展示了应用于指纹图像的去噪自动编码器处理过程。这使用了特殊的软件来生成超逼真的指纹图像，目的是去除噪声，使指纹处理更不易出错。即使使用更简单和不现实的噪声，你仍然可以得到良好的结果。
- en: 7.4.1  Denoising with Gaussian noise
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 使用高斯噪声进行去噪
- en: 'We will make only one change to our previous `auto_encoder_big` model: we will
    add a new layer at the beginning of the encoder subnetwork, which adds noise to
    the input *only when we are training*. The assumption is usually that our training
    data is relatively clean and prepared, and we are adding noise to make it more
    robust. If we are then *using* the model, and no longer training, we want to get
    the best answer we can—which means we want the cleanest data possible. Adding
    noise at that stage would make our lives more difficult, and if the input already
    had noise, we would just compound the problem.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将对之前的 `auto_encoder_big` 模型进行仅有的一个修改：在编码子网络的开始处添加一个新层，该层仅在训练时向输入添加噪声。通常的假设是，我们的训练数据相对干净且已准备就绪，我们添加噪声是为了使其更健壮。如果我们正在
    *使用* 模型，并且不再训练，我们希望得到最好的答案——这意味着我们希望得到尽可能干净的数据。在那个阶段添加噪声会使我们的生活更加困难，如果输入已经存在噪声，我们只会使问题更加复杂。
- en: 'So the first code we need is a new `AdditiveGaussNoise` layer. It takes the
    input x in. If we are in training mode (denoted by `self.training`), we add noise
    to the input; otherwise we return it unperturbed:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们首先需要的是一个新的 `AdditiveGaussNoise` 层。它接受输入 x in。如果我们处于训练模式（由 `self.training`
    表示），我们向输入添加噪声；否则，我们返回未扰动的输入：
- en: '[PRE28]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ We don’t need to do anything in the constructor of this object.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在此对象的构造函数中我们不需要做任何事情。
- en: ❷ Every PyTorch Module object has a self.training boolean that can be used to
    check if we are in training (True) or evaluation (False) mode.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每个PyTorch模块对象都有一个 `self.training` 布尔值，可以用来检查我们是否处于训练（True）或评估（False）模式。
- en: '❸ Now training: return the data as it was given.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 现在是训练阶段：返回给定的数据。
- en: 'Next, we redefine the same large autoencoder as before, where *D*′ = 2 ⋅ *D*.
    The only difference is that we insert the `AdditiveGaussNoise` layer at the start
    of the network:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们重新定义与之前相同的大型自动编码器，其中 *D*′ = 2 ⋅ *D*。唯一的区别是我们将 `AdditiveGaussNoise` 层插入到网络的开始部分：
- en: '[PRE29]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Only addition! We hope inserting noise here helps.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅添加！我们希望在这里添加噪声会有所帮助。
- en: ❷ Trains as usual
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 按常规训练
- en: 'How well does it do? Following, we can see the same data reconstructed when
    there is and is not noise. The new denoising model is clearly the best at creating
    reconstructions of all the models we have developed so far. In both cases, the
    denoising autoencoder captures most of the style of the individual digits. The
    denoising approach still misses small details, likely because they are so small
    that the model isn’t sure if they are a real part of the style or part of the
    noise. For example, the flourishes on the bottom of the 4 and the top of the 5
    are missing after reconstruction:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 它做得怎么样？接下来，我们可以看到在有噪声和无噪声的情况下重建的相同数据。新的去噪模型在创建我们迄今为止开发的各个模型的重建方面明显是最好的。在两种情况下，去噪自编码器都捕捉到了单个数字的大部分风格。去噪方法仍然遗漏了一些细节，这可能是由于它们太小，以至于模型不确定它们是风格的真实部分还是噪声的一部分。例如，4号字底部的装饰和5号字顶部的装饰在重建后缺失：
- en: '[PRE30]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '![](../Images/CH07_UN23_Raff.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN23_Raff.png)'
- en: '![](../Images/CH07_UN24_Raff.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN24_Raff.png)'
- en: '[PRE31]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '![](../Images/CH07_UN25_Raff.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN25_Raff.png)'
- en: '![](../Images/CH07_UN26_Raff.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN26_Raff.png)'
- en: The denoising approach is very popular for training autoencoders, and the trick
    of introducing your own perturbations into the data is widely used to build more
    accurate and robust models. As you learn more about deep learning and different
    applications, you will find many forms and spins on this approach.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 去噪方法在训练自编码器时非常受欢迎，将你自己的扰动引入数据中的技巧被广泛用于构建更准确和鲁棒的模型。随着你对深度学习和不同应用了解的更多，你会发现许多形式和变体都基于这种方法。
- en: Beyond helping learn more robust representations, the denoising approach can
    itself be a useful model. Noise can naturally occur in many situations. For example,
    when performing optical character recognition (OCR) to convert an image into searchable
    text, you can get noise from damage to the camera, damage to the document (e.g.,
    water or coffee stains), changes in lighting, objects casting shadows, and so
    on. Many OCR systems have been improved by learning to add noise that looks like
    the noise seen in real life and asking the model to learn in spite of it.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 除了帮助学习更鲁棒的表现形式之外，去噪方法本身也可以是一个有用的模型。噪声在许多情况下都可能自然发生。例如，在执行光学字符识别（OCR）以将图像转换为可搜索文本时，你可以从相机的损坏、文档的损坏（例如，水或咖啡污渍）、光照变化、物体投下的阴影等中获得噪声。许多OCR系统通过学习添加类似于真实生活中看到的噪声，并要求模型在噪声中学习而得到改进。
- en: Denoising with dropout
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 基于Dropout的去噪
- en: Adding Gaussian noise can be cumbersome because we need to figure out exactly
    how much noise to add, which can change from dataset to dataset. A second, more
    popular approach is to use *dropout*.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 添加高斯噪声可能很麻烦，因为我们需要确定确切需要添加多少噪声，这可能会从数据集到数据集而变化。第二种更受欢迎的方法是使用*dropout*。
- en: 'Dropout is a very simple idea: with some probability p, zero out any given
    feature value. This forces the network to be robust because it can *never* rely
    on any specific feature or neuron value, since p% of the time, the feature or
    value will not be there. Dropout is a very popular regularizer that can be applied
    to both the *input* of a network and to the *hidden layers*.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: Dropout是一个非常简单的想法：以一定的概率p，将任何给定的特征值置零。这迫使网络变得鲁棒，因为它*永远*不能依赖于任何特定的特征或神经元值，因为p%的时间，特征或值将不存在。Dropout是一个非常受欢迎的正则化器，可以应用于网络的*输入*和*隐藏层*。
- en: 'The following code block trains up a dropout-based denoising autoencoder. By
    default, dropout uses *p* = 50%, which is fine for hidden layers but on the aggressive
    size for the input. So for the input, we apply only *p* = 20%:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块训练了一个基于dropout的去噪自编码器。默认情况下，dropout使用*p* = 50%，这对于隐藏层来说是合适的，但对于输入来说过于激进。因此，对于输入，我们只应用*p*
    = 20%：
- en: '[PRE32]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ For the input, we usually drop only 5 to 20% of the values.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于输入，我们通常只丢弃5%到20%的值。
- en: ❷ By default, dropout uses 50% probability to zero out values.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 默认情况下，dropout使用50%的概率将值置零。
- en: ❸ Trains as usual
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 按常规训练
- en: 'Now that the model is trained, let’s apply it to some of the test data. Dropout
    can encourage a large degree of robustness, which we can show off by applying
    it to both dropout noise and Gaussian noise. The latter is something the network
    has never seen before, but that does not stop the autoencoder from faithfully
    determining an accurate reconstruction:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 现在模型已经训练好了，让我们将其应用于一些测试数据。Dropout可以鼓励很大的鲁棒性，我们可以通过将其应用于dropout噪声和高斯噪声来展示这一点。后者是网络以前从未见过的，但这并没有阻止自编码器忠实地确定一个准确的重建：
- en: '[PRE33]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Clean data
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 清洁数据
- en: ❷ Gaussian noise
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 高斯噪声
- en: ❸ Dropout noise
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Dropout噪声
- en: '![](../Images/CH07_UN27_Raff.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN27_Raff.png)'
- en: '![](../Images/CH07_UN28_Raff.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN28_Raff.png)'
- en: '![](../Images/CH07_UN29_Raff.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_UN29_Raff.png)'
- en: The rise and fall of dropout
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 退火与重生
- en: The origins of dropout began with denoising autoencoders way back in 2008[^a](#fn23)
    but was applied to only the input. Later it was developed into a more general-purpose
    regularizer[^b](#fn24) and played a significant role in the rebirth of neural
    networks as a field and research area.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: dropout的起源可以追溯到2008年早期去噪自编码器[^a](#fn23)，但最初只应用于输入。后来它被发展成为一种更通用的正则化器[^b](#fn24)，并在神经网络作为领域和研究领域的重生中发挥了重要作用。
- en: Like most regularizers, dropout had the goal of improving generalization and
    reducing overfitting. It was quite good at it and had an attractive and intuitive
    story about how it worked. For many years, dropout was a critical tool for getting
    good results, and implementing a network without it was almost impossible. Dropout
    still works as a regularizer and is useful and used, but it is not ubiquitous
    as it once was. The tools we have learned thus far, like normalization layers,
    better optimizers, and residual connections, give us most of the benefits of dropout.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 与大多数正则化方法一样，dropout的目标是提高泛化能力和减少过拟合。它在这一点上做得相当出色，并且有一个吸引人且直观的故事来解释它是如何工作的。多年来，dropout一直是获取良好结果的关键工具，没有它几乎不可能实现网络。dropout仍然作为一个正则化器发挥作用，并且有用且被广泛使用，但它不再像以前那样无处不在。我们迄今为止学到的工具，如归一化层、更好的优化器和残差连接，为我们提供了dropout的大部分好处。
- en: Using dropout is not a *bad* thing, and I’m being hyperbolic by referring to
    “the fall of dropout.” The technique has simply become less popular over time.
    My unsubstantiated theory as to why is that first, it is a little slower to train,
    requiring lots of random numbers and increased memory use when we can get most
    of its benefits now without those costs. Second, dropout is applied differently
    at training versus test time. During training, you lose ~50% of your neurons,
    making the network effectively smaller. But during validation, you get all 100%
    of your neurons. This can cause the confusing situation where testing performance
    looks better than training performance because train and test are being evaluated
    in different ways. (Technically, the same is true of batch normalization, but
    the inversion isn’t as common.) I think people opted for the slightly less expensive
    and less perplexing results of other methods. That said, dropout is still a great
    default choice to use as a regularizer for new architectures where you are not
    sure what does or does not work.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 使用dropout并不是一件“坏”事，我通过提到“dropout的衰落”而夸张了。这项技术只是随着时间的推移而变得不那么受欢迎。我未经证实的理论是，首先，它训练起来稍微慢一些，需要大量的随机数和增加的内存使用，而现在我们可以在不付出这些代价的情况下获得其大部分好处。其次，dropout在训练和测试时的应用方式不同。在训练期间，你失去了大约50%的神经元，使得网络实际上变得更小。但在验证期间，你得到所有的100%神经元。这可能导致测试性能看起来比训练性能更好的困惑情况，因为训练和测试是以不同的方式评估的。（技术上，批量归一化也是如此，但这种情况并不常见。）我认为人们选择了其他方法略微便宜且不那么令人困惑的结果。尽管如此，dropout仍然是一个很好的默认选择，可以作为新架构的正则化器使用，在这些架构中，你不确定哪些是有效的或无效的。
- en: '* * *'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '^a P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol, “Extracting and
    composing robustfeatures with denoising autoencoders,” in *Proceedings of the
    25th International Conference on Machine Learning*, New York: Association for
    Computing Machinery, 2008, pp. 1096–1103, [https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294).[↩](#fnref23)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '^a P. Vincent, H. Larochelle, Y. Bengio, and P.A. Manzagol, “Extracting and
    composing robust features with denoising autoencoders,” in *Proceedings of the
    25th International Conference on Machine Learning*, New York: Association for
    Computing Machinery, 2008, pp. 1096–1103, [https://doi.org/10.1145/1390156.1390294](https://doi.org/10.1145/1390156.1390294).[↩](#fnref23)'
- en: '^b N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The Journal
    of Machine Learning Research*, vol. 15, no. 1, pp. 1929–1958, 2014.[↩](#fnref24)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '^b N. Srivastava, G. Hinton, A. Krizhevsky, I. Sutskever, and R. Salakhutdinov,
    “Dropout: a simple way to prevent neural networks from overfitting,” *The Journal
    of Machine Learning Research*, vol. 15, no. 1, pp. 1929–1958, 2014.[↩](#fnref24)'
- en: 7.5 Autoregressive models for time series and sequences
  id: totrans-246
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 时间序列和序列的自动回归模型
- en: The autoencoding approach has been very successful for images, signals, and
    even fully connected models with tabular data. But what if our data is a sequence
    problem? Especially if our data is in a language represented by discrete tokens,
    it’s hard to add meaningful noise to things like a letter or word. Instead, we
    can use an *autoregressive model*, which is an approach specifically designed
    for time-series problems.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 自动编码方法在图像、信号以及带有表格数据的全连接模型中都非常成功。但如果我们面临的是一个序列问题呢？特别是如果我们的数据是由离散标记表示的语言，那么很难向字母或单词等添加有意义的噪声。相反，我们可以使用一个**自回归模型**，这是一种专门为时间序列问题设计的解决方案。
- en: You can use an autoregressive model for basically all the same applications
    for which you might use an autoencoding one. You can use the representation an
    autoregressive model learns as the input to another ML algorithm that doesn’t
    understand sequences. For example, you could train an autoregressive model on
    book reviews of *Inside Deep Learning* and then a clustering algorithm like k-means
    or HDBSCAN to cluster those reviews.[⁴](#fn25) Since these algorithms do not naturally
    take text as input, the autoregressive model is a nice way to quickly expand the
    reach of your favorite ML tools.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用自回归模型来基本上应用于所有你可能使用自动编码模型的应用。你可以使用自回归模型学习到的表示作为输入到另一个不理解序列的ML算法中。例如，你可以在《Inside
    Deep Learning》的书评上训练一个自回归模型，然后使用k-means或HDBSCAN等聚类算法对这些评论进行聚类。[⁴](#fn25) 由于这些算法不自然地接受文本作为输入，自回归模型是快速扩展你最喜欢的ML工具范围的好方法。
- en: 'Let’s say you have t steps of your data: **x**[1], **x**[2], …, **x**[*t* −
    1], **x**[t]. The goal of an autoregressive model is to predict **x**[*t* + 1]
    given all the previous items in the sequence. The mathy way to write this would
    be ℙ(**x**[*t* + 1]|**x**[1],**x**[2],…,**x**[t]), which is saying'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有t步的数据：**x**[1]，**x**[2]，…，**x**[*t* − 1]，**x**[t]。自回归模型的目标是在给定序列中所有前面的项目的情况下预测**x**[*t*
    + 1]。用数学方式写这个就是ℙ(**x**[*t* + 1]|**x**[1]，**x**[2]，…，**x**[t])，这意味着
- en: '![](../Images/CH07_UN30_Raff.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_UN30_Raff.png)'
- en: The autoregressive approach is still a form of self-supervision because the
    next item in a sequence is a trivial component of having the data in the first
    place. If you treat the sentence “This is a sentence” as a sequence of characters,
    you, by definition, know that *T* is the first item, *h* the second, *i* the third,
    and so on.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归方法仍然是一种自我监督的形式，因为序列中的下一个项目是数据本身的一个简单组成部分。如果你把“这是一个句子”看作是一系列字符的序列，根据定义，你知道*T*是第一个项目，*h*是第二个，*i*是第三个，以此类推。
- en: 'Figure 7.6 illustrates how autoregressive models happen at a high level. A
    sequence-based model is shown in the green blocks and takes in an input **x**[i].
    The prediction at the ith step is thus **x̂**[i]. We then use the loss function
    ℓ to compute the loss between the current prediction **x̂**[i] and the *next input*
    **x**[*i* + 1], ℓ(***x̂**[i]*, ***x**[i]*[+1]). So for an input with T time steps,
    we have *T* − 1 loss calculations: the last time step T can’t be used as an input
    because there is no *T* + 1th item to compare it against.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6说明了自回归模型在高级别上的发生情况。一个基于序列的模型显示在绿色块中，并接收输入**x**[i]。因此，第i步的预测是**x̂**[i]。然后我们使用损失函数ℓ来计算当前预测**x̂**[i]和下一个输入**x**[*i*
    + 1]之间的损失，ℓ(***x̂**[i]*, ***x**[i]*[+1])。所以对于一个有T个时间步长的输入，我们有T - 1次损失计算：最后一个时间步T不能用作输入，因为没有T
    + 1项可以与之比较。
- en: '![](../Images/CH07_F06_Raff.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F06_Raff.png)'
- en: Figure 7.6 Example of an autoregressive setup. The inputs are at the bottom,
    and the outputs are at the top. For an input **x**[i], the prediction from the
    autoregressive model is **x̂**[i], and the *label* **y**[i] = **x**[*i* + 1].
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6展示了自回归设置的示例。输入位于底部，输出位于顶部。对于一个输入**x**[i]，自回归模型的预测是**x̂**[i]，而**标签****y**[i]
    = **x**[*i* + 1]。
- en: You may have guessed from the look of this diagram that we will use a recurrent
    neural network to implement our autoregressive models. RNNs are great for sequence-based
    problems like this one. The big change compared to our previous use of RNNs is
    that we will make a prediction at *every* step, instead of just the *last* step.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经从这张图的形状中猜到，我们将使用循环神经网络来实现我们的自回归模型。RNNs非常适合像这样的基于序列的问题。与之前使用RNNs相比，一个大的变化是我们将在**每个**步骤进行预测，而不仅仅是最后一个步骤。
- en: A type of autoregressive model popularized by Andrej Karpathy ([http://karpathy.github.io](http://karpathy.github.io))
    is called *char-RNN* (character RNN). This is an autoregressive approach where
    the inputs/outputs are characters, and we’ll show a simple way to implement a
    char-RNN model on some Shakespeare data.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 一种由安德烈·卡帕西（Andrej Karpathy）普及的自动回归模型称为*char-RNN*（字符RNN）。这是一种自动回归方法，其中输入/输出是字符，我们将展示在莎士比亚数据上实现char-RNN模型的一种简单方法。
- en: Note While RNNs are an appropriate and common architecture to use for auto-
    regressive models, bidirectional RNNs are not. This is because an autoregressive
    model is making predictions about the future. If we used a bidirectional model,
    we would have information about the future content in the sequence, and knowing
    the future is cheating! Bi-directional RNNs were useful when we wanted to make
    a prediction about the *whole* sequence, but now that we are making predictions
    about the input, we need to enforce a no-bidirectional policy to make sure our
    models do not get to peek at information they should not see.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：虽然RNN是用于自动回归模型的适当且常见的架构，但双向RNN不是。这是因为自动回归模型正在预测未来。如果我们使用双向模型，我们将在序列中拥有关于未来内容的信息，而知道未来就是作弊！当我们要对整个序列进行预测时，双向RNN是有用的，但现在我们正在对输入进行预测，我们需要强制执行无双向策略，以确保我们的模型不会看到它们不应该看到的信息。
- en: 7.5.1  Implementing the char-RNN autoregressive text model
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 实现char-RNN自回归文本模型
- en: 'The first thing we need is our data. Andrej Karpathy has shared some text from
    Shakespeare online that we will download. There are about 100,000 characters in
    this text, so we store the data in a variable called `shakespear_100k`. We use
    this dataset to show the process of training an autoregressive model, as well
    as its generative capabilities:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先需要我们的数据。安德烈·卡帕西（Andrej Karpathy）在网上分享了一些莎士比亚的文本，我们将下载这些文本。这个文本中大约有10万个字符，所以我们把数据存储在一个名为`shakespear_100k`的变量中。我们使用这个数据集来展示训练自回归模型的过程，以及它的生成能力：
- en: '[PRE34]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Now we will build a vocabulary Σ of all the characters in this dataset. One
    change you could make is to not use the `lower()` function to convert everything
    to lowercase. As we are exploring deep learning, these early decisions are important
    for how our model will eventually be used and how useful it will be. So, you should
    learn to recognize choices like this *as choices*. I’ve chosen to use all lowercase
    data, and as a result, our vocabulary is smaller. This reduces the difficulty
    of the task but means our model can’t learn about capitalization.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将构建一个包含此数据集中所有字符的词汇 Σ。你可以做出的一个改变是不使用`lower()`函数将所有内容转换为小写。因为我们正在探索深度学习，这些早期的决定对我们模型最终的使用方式和有用性非常重要。所以，你应该学会识别这样的选择*作为选择*。我选择使用全部小写数据，因此我们的词汇量更小。这降低了任务的难度，但意味着我们的模型无法学习关于大写的信息。
- en: 'Here’s the code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE35]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ The vocab Σ
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 词汇 Σ
- en: ❷ Adds every new character to the vocab
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据当前词汇大小设置索引
- en: ❸ Sets the index based on the current vocab size
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将每个新字符添加到词汇中
- en: ❹ Useful code to go from the index back to the original characters
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从索引返回原始字符的有用代码
- en: ❺ Iterates over all key,value pairs and creates a dictionary with the inverse
    mapping
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 遍历所有键值对并创建一个具有逆映射的字典
- en: Next we take a very simple approach to build an autoregressive dataset. We have
    the original 100,000 characters in one long sequence since they are taken from
    one of Shakespeare’s plays. If we break this sequence into chunks that are sufficiently
    long, we can almost guarantee that each chunk will contain a few complete sentences.
    We obtain each chunk by indexing into a position `start` and grabbing a slice
    of the text `[start:start+chunk_size]`. Since the dataset is autoregressive, our
    *labels* are the tokens starting one character over. This can be done by grabbing
    a new slice shifted by one, `[start+1:start+1+chunk_size]`. This is shown in figure
    7.7.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们采取一个非常简单的方法来构建一个自动回归数据集。由于这些字符是从莎士比亚的一个剧中提取的，所以它们在一个长序列中。如果我们把这个序列分成足够长的块，我们几乎可以保证每个块都会包含几个完整的句子。我们通过索引到一个位置`start`并获取文本的切片`[start:start+chunk_size]`来获得每个块。由于数据集是自动回归的，所以我们的*标签*是比一个字符多出的标记。这可以通过获取一个偏移一个字符的新切片来实现，即`[start+1:start+1+chunk_size]`。这如图7.7所示。
- en: '![](../Images/CH07_F07_Raff.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F07_Raff.png)'
- en: Figure 7.7 Red shows grabbing the input, and yellow the output, using chunks
    of six characters. This makes it easy to create the dataset for the model where
    every batch size has the same length, simplifying our code and ensuring maximum
    GPU utilization (no work done on padded inputs/outputs).
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 红色表示获取输入，黄色表示输出，使用六个字符的块。这使得为模型创建数据集变得容易，其中每个批次的长度都相同，简化了我们的代码，并确保最大GPU利用率（在填充的输入/输出上不做任何工作）。
- en: 'The following code uses this strategy to implement the dataset for autoregressive
    problems from a large text corpus. We assume the corpus exists as one long string,
    and it is OK to concatenate multiple files together into one long string since
    our chunks are smaller than most documents. While we are giving our model the
    difficulty of having to learn *starting at a random position that is probably
    part-way into a word*, it makes it very easy for us to implement all of the code:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用此策略从大型文本语料库中实现自回归问题的数据集。我们假设语料库存在为一个长字符串，并且将多个文件连接成一个长字符串是可以接受的，因为我们的块的大小小于大多数文档。虽然我们给我们的模型增加了难度，需要从随机位置开始学习，这个位置可能是一个单词的中间部分，但它使我们能够轻松地实现所有代码：
- en: '[PRE36]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ The number of items is the number of characters divided by chunk size.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 项数是字符数除以块大小。
- en: ❷ Computes the starting position for the idx’th chunk
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算第idx个块的起始位置
- en: ❸ Grabs the input substring
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取输入子字符串
- en: ❹ Converts the substring into integers based on our vocab
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据我们的词汇表将子字符串转换为整数
- en: ❺ Grabs the label substring by shifting over by 1
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过移动1位来获取标签子字符串
- en: ❻ Converts the label substring into integers based on our vocab
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 根据我们的词汇表将标签子字符串转换为整数
- en: 'Now comes the tricky part: implementing an autoregressive RNN model. For this
    we use a gated recurrent unit (GRU) instead of long short-term memory (LSTM),
    because the code will be a little easier to read since the GRU has only the hidden
    states **h**[t] and does not have any context states **c**[t]. The high-level
    strategy for our implementation is given in figure 7.8.'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是棘手的部分：实现自回归RNN模型。为此，我们使用门控循环单元（GRU）而不是长短期记忆（LSTM），因为GRU只有隐藏状态**h**[t]，没有上下文状态**c**[t]，所以代码会更容易阅读。我们实现的高级策略在图7.8中给出。
- en: '![](../Images/CH07_F08_Raff.png)'
  id: totrans-281
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F08_Raff.png)'
- en: Figure 7.8 Autoregressive RNN design. The input (starting from the bottom) is
    in yellow, where an `nn.Embedding` layer converts each character into a vector.
    These vectors are fed into an RNN layer, shown in green, which sequentially processes
    each character. Then a set of fully connected layers processes each RNN hidden
    state **h**[t] *independently* (via weight sharing) to make a prediction about
    the next token.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 自回归RNN设计。输入（从底部开始）为黄色，其中`nn.Embedding`层将每个字符转换为向量。这些向量被送入绿色显示的RNN层，该层按顺序处理每个字符。然后一组全连接层独立地处理每个RNN隐藏状态**h**[t]，通过权重共享来做出关于下一个标记的预测。
- en: Defining an autoregressive constructor
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 定义自回归构造函数
- en: 'Our constructor takes some familiar arguments. We want to know the size of
    the vocabulary `num_embeddings`, how many dimensions in the embedding layer `embd_size`,
    the number of neurons in each hidden layer `hidden_size`, and the number of RNN
    layers `layers=1`:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的构造函数接受一些熟悉的参数。我们想知道词汇表的大小`num_embeddings`、嵌入层中的维度`embd_size`、每个隐藏层中的神经元数量`hidden_size`以及RNN层的数量`layers=1`：
- en: '[PRE37]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Our first major change to our architecture is that we do not use the normal
    `nn.GRU` module. The normal `nn.RNN`, `nn.LSTM` and `nn.GRU` modules take in *all*
    the time steps at once and return *all the outputs* at once. You can use these
    to implement an autoregressive model, but we are instead going to use the `nn.GRUCell`
    module. The `GRUCell` processes sequences *one item at a time*. This is slower
    but can make it easier to handle inputs with an unknown and variable length. This
    approach is summarized in figure 7.9\. The `Cell` classes will be useful once
    we are finished training the model, but I don’t want to ruin the surprise—we will
    come back to *why* we are doing it this way in a moment.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对架构的第一个主要改变是我们不使用正常的`nn.GRU`模块。正常的`nn.RNN`、`nn.LSTM`和`nn.GRU`模块一次接受所有时间步，并一次返回所有输出。你可以使用这些来实现自回归模型，但我们将使用`nn.GRUCell`模块。`GRUCell`一次处理一个序列项。这可能会慢一些，但可以更容易地处理未知和可变长度的输入。这种方法总结在图7.9中。`Cell`类在我们完成模型训练后将很有用，但我不想破坏惊喜——我们稍后会回到*为什么*我们这样做的原因。
- en: '![](../Images/CH07_F09_Raff.png)'
  id: totrans-287
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F09_Raff.png)'
- en: 'Figure 7.9 Example demonstrating the primary difference between the RNN and
    cell classes in PyTorch. Left: Normal RNNs process the entire sequence in a single
    operation, making them faster but requiring all data to be available at once.
    Right: Cell classes process items one at a time, making them slower but easier
    to use when you don’t have all the inputs already available.'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9展示了PyTorch中RNN和cell类之间主要区别的示例。左侧：正常的RNN在一次操作中处理整个序列，这使得它们更快，但需要一次性提供所有数据。右侧：cell类逐个处理项目，在没有所有输入已经可用的情况下更容易使用。
- en: 'If we want multiple `layers` of an RNN, we have to manually specify and run
    them ourselves. We can do this by using a `ModuleList` to specify multiple modules
    in a group. This means our initialization code after `self.embd` looks like this:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要多个`layers`的RNN，我们必须手动指定和运行它们。我们可以通过使用`ModuleList`来指定一组中的多个模块来实现这一点。这意味着我们的初始化代码在`self.embd`之后看起来像这样：
- en: '[PRE38]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: We broke up the specification of `GRUCell` layers into two parts. First is a
    list of one item for the first layer since it has to go from `embd_size` inputs
    to `hidden_size` outputs. Second is all remaining layers with `[nn.GRUCell(hidden_size, hidden_size) for i in range(layers-1)]`,
    which works because each of these layers has the same input and output size. For
    fun, I’ve also included a `LayerNorm` normalization layer for each RNN result.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`GRUCell`层的规范分为两部分。首先是第一个层的一个项目列表，因为它必须从`embd_size`输入到`hidden_size`输出。第二是所有剩余层，使用`[nn.GRUCell(hidden_size,
    hidden_size) for i in range(layers-1)]`，这之所以有效，是因为这些层的输入和输出大小相同。为了好玩，我还包括了一个`LayerNorm`归一化层，用于每个RNN结果。
- en: 'The last thing we need in our constructor is the purple layers that take in
    the hidden states **h**[t] and output a prediction for the class. This is done
    with a small fully connected network:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的构造函数中，我们还需要紫色层，这些层接受隐藏状态**h**[t]并输出类的预测。这是通过一个小型全连接网络完成的：
- en: '[PRE39]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ (B, *, D)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ (B, *, D)
- en: ❷ (B, *, D) -> B(B, *, VocabSize)
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ (B, *, D) -> B(B, *, VocabSize)
- en: Notice that we define a component of this module as an entire network. This
    will help us compartmentalize our design and make our code easier to read. If
    you ever want to go back and change the subnetwork that goes from hidden RNN states
    to predictions, you can change just the `pred_class` object, and the rest of the
    code will work fine.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将此模块的一个组件定义为整个网络。这将帮助我们模块化我们的设计，并使我们的代码更容易阅读。如果您想返回并更改从隐藏RNN状态到预测的子网络，您只需更改`pred_class`对象，其余代码将正常工作。
- en: Implementing the autoregressive forward function
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自回归前向函数
- en: 'The `forward` function of the module will organize the work done by two other
    helper functions. First we embed the input tokens into their vector forms, as
    this can all be done at once. Because we are using a `GRUCell` class, we need
    to keep track of the hidden states ourselves. So we use a `initHiddenStates(B)`
    function that creates the initial hidden states **h**[0] = ![](../Images/vec_0.png)
    for each GRU layer. Then we use a `for` loop to grab each of the t items and process
    them one step at a time using a `step` function that takes the inputs **x**[t]
    and a list of the GRU hidden states `h_prevs`. The GRU hidden states are stored
    in a list `last_activations` to get the predictions at every time step. Finally,
    we can return a single tensor by `stack`ing the results together:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 该模块的`forward`函数将组织两个其他辅助函数完成的工作。首先，我们将输入标记嵌入到它们的向量形式中，因为这可以一次性完成。因为我们使用的是`GRUCell`类，我们需要自己跟踪隐藏状态。因此，我们使用`initHiddenStates(B)`函数为每个GRU层创建初始隐藏状态**h**[0]
    = ![vec_0.png](../Images/vec_0.png)。然后，我们使用`for`循环获取每个t项目，并使用一个`step`函数逐个步骤处理它们，该函数接受输入**x**[t]和GRU隐藏状态列表`h_prevs`。GRU隐藏状态存储在列表`last_activations`中，以获取每个时间步的预测。最后，我们可以通过`stack`将结果组合在一起返回一个单一的tensor：
- en: '[PRE40]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Input should be (B, T).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入应为（B，T）。
- en: ❷ What is the batch size?
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 批量大小是多少？
- en: ❸ What is the maximum number of time steps?
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 最多有多少个时间步？
- en: ❹ (B, T, D)
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ （B，T，D）
- en: ❺ Initial hidden states
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 初始隐藏状态
- en: ❻ (B, D)
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ (B, D)
- en: ❼ (B, T, D)
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ （B，T，D）
- en: '`initHiddenStates` is easy to implement. We can use the `torch.zeros` function
    to create a tensor of all zero values. We just have the argument `B` for how large
    the batch is, and then we can grab the `hidden_size` and number of `layers` from
    the object’s members:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '`initHiddenStates`很容易实现。我们可以使用`torch.zeros`函数创建一个全零值的tensor。我们只需要一个参数`B`来指定批量大小的，然后我们可以从对象的成员中获取`hidden_size`和`layers`的数量：'
- en: '[PRE41]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The `step` function is a little more involved. First we check the shape of
    the input, and if it has only one dimension, we assume that we need to embed the
    token values to make vectors. Then we check the hidden states `h_prevs` and, if
    they are not provided, initialize them using `initHiddenStates`. These are both
    good defensive code steps to make sure our function can be versatile and avoid
    errors:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '`step` 函数稍微复杂一些。首先，我们检查输入的形状，如果它只有一个维度，我们假设我们需要嵌入标记值以生成向量。然后我们检查隐藏状态 `h_prevs`，如果它们没有提供，则使用
    `initHiddenStates` 初始化它们。这些都是好的防御性代码步骤，以确保我们的函数可以灵活且避免错误：'
- en: '[PRE42]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Preps all three arguments to be in the final form. First, (B); we need to
    embed it.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备所有三个参数以最终形式呈现。首先，(B)；我们需要嵌入它。
- en: ❷ Now (B, D)
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在 (B, D)
- en: ❸ Processes the input
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理输入
- en: ❹ Pushes in the current input with previous hidden states
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将当前输入与之前的隐藏状态一起推入
- en: ❺ Makes predictions about the token
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 对标记进行预测
- en: After those defensive coding steps, we simply loop through the number of layers
    and process the results. `x_in` is the input to a layer, which is passed into
    the current layer `self.layers[l]` and then the normalization layer `self.norms[l]`.
    After that, we do the minor bookkeeping of storing the new hidden state `h_prevs[l] = h`
    and setting `x_in = h` so that the next layer has its input ready to process.
    Once that loop is done, `x_in` has the result from the last RNN layer, so we can
    feed it directly to the `self.pred_class` object to go from the RNN hidden state
    to a prediction about the next character.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些防御性编码步骤之后，我们简单地遍历层数并处理结果。`x_in` 是层的输入，它被传递到当前层 `self.layers[l]` 和归一化层 `self.norms[l]`。之后，我们进行一些次要的记录工作，存储新的隐藏状态
    `h_prevs[l] = h` 并设置 `x_in = h`，以便下一层准备好输入以进行处理。一旦这个循环完成，`x_in` 就有了最后一个 RNN 层的结果，因此我们可以直接将其输入到
    `self.pred_class` 对象中，从 RNN 隐藏状态预测下一个字符。
- en: A shortcut for linear layers over time
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 线性层随时间的一个快捷方式
- en: 'You may notice a comment in this code about the tensor shapes with `(B, D)`.
    This is because `nn.Linear` layers have a special trick that allows them to be
    applied to multiple inputs *independently* at the same time. We have always used
    a linear model over a tensor with shape (*B*,*D*), and the linear model can take
    in D inputs and return *D*′ outputs. So we would go from (*B*,*D*) → (*B*,*D*′).
    If we have T items in a sequence, we have a tensor of shape (*B*,*T*,*D*). Applying
    a linear model to *each* time step naively would require a `for` loop and look
    like this:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到这段代码中关于张量形状的注释 `(B, D)`。这是因为 `nn.Linear` 层有一个特殊的技巧，允许它们同时独立地应用于多个输入。我们总是使用形状为
    (*B*, *D*) 的张量上的线性模型，线性模型可以接收 D 个输入并返回 *D*′ 个输出。因此，我们会从 (*B*, *D*) 变为 (*B*, *D*′)。如果我们有一个序列中的
    T 个项目，我们有一个形状为 (*B*, *T*, *D*) 的张量。将线性模型应用于每个时间步的原始方法需要 `for` 循环，看起来像这样：
- en: '[PRE43]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Place to store result for each step
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 存储每个步骤结果的地点
- en: ❷ Gets the result for each step
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取每个步骤的结果
- en: ❸ Stacks everything into one tensor and shapes it correctly
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将所有内容堆叠成一个张量并正确地调整形状
- en: That is more code than we would like, *and* it will run slower because of the
    `for` loop. PyTorch has a simple trick that `nn.Linear` layers are applied to
    the *last* axis of a tensor *regardless of the number of axes*. That means this
    whole function can be replaced with just `linearLayer`, and you will get the *exact
    same result*. That is less code *and* faster. This way, any fully connected network
    can be used on single time steps or groups of time steps, without having to do
    anything special. Still, it’s good to keep comments like `(B, D)` and `(B, T, D)`
    so that you can remind yourself *how* you are using your networks.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们想要的代码要多，而且由于 `for` 循环，它将运行得更慢。PyTorch 有一个简单的技巧，即 `nn.Linear` 层应用于张量的 *最后一个*
    轴，无论轴的数量是多少。这意味着整个函数可以用 `linearLayer` 代替，你将得到 *完全相同的结果*。这样，任何全连接网络都可以用于单个时间步或时间步组，而无需做任何特殊的事情。尽管如此，保留像
    `(B, D)` 和 `(B, T, D)` 这样的注释仍然很好，这样你可以提醒自己 *如何* 使用你的网络。
- en: 'With our model defined, we are almost finished. Next we quickly create our
    new `AutoRegressiveDataset` with the `shakespear_100k` data as the input, and
    we make a data loader using a respectable batch size. We also create our `AutoRegressive`
    model with 32 dimensions for the embedding, 128 hidden neurons, and 2 GRU layers.
    We include gradient clipping because RNNs are sensitive to that issue:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了我们的模型后，我们几乎完成了。接下来，我们快速创建一个新的 `AutoRegressiveDataset`，使用 `shakespear_100k`
    数据作为输入，并使用一个可观的批量大小创建数据加载器。我们还创建了一个具有 32 维嵌入、128 个隐藏神经元和 2 个 GRU 层的 `AutoRegressive`
    模型。我们包括梯度裁剪，因为 RNN 对这个问题很敏感：
- en: '[PRE44]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Implementing an autoregressive loss function
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 实现自回归损失函数
- en: 'The last thing we need is a loss function ℓ. We are making a prediction at
    every step, so we want to use the `CrossEntropyLoss` that is appropriate for classification
    problems. *However*, we have *multiple* losses to compute, one for every time
    step. We can solve this by writing our own loss function `CrossEntLossTime` that
    computes the cross-entropy for each step. Similar to our `forward` function, we
    slice each prediction `x[:,t,:]` and a corresponding label `y[:t]` so that we
    end up with the standard (*B*,*C*) and (*B*) shapes for the prediction and labels,
    respectively, and can call `CrossEntropyLoss` directly. Then we add the losses
    from every time step to get a single total loss to return:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后需要的是一个损失函数ℓ。我们在每个步骤都做出预测，因此我们想要使用适用于分类问题的`CrossEntropyLoss`。*然而*，我们需要计算多个损失，每个时间步一个。我们可以通过编写自己的损失函数`CrossEntLossTime`来解决，该函数计算每个步骤的交叉熵。类似于我们的`forward`函数，我们将每个预测`x[:,t,:]`和相应的标签`y[:t]`切片，以便我们最终得到预测和标签的标准(*B*,*C*)和(*B*)形状，可以直接调用`CrossEntropyLoss`。然后我们将每个时间步的损失相加，得到一个单一的返回总损失：
- en: '[PRE45]'
  id: totrans-328
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ For every item in the sequence . . .
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于序列中的每个项目 ...
- en: ❷ ... compute the sum of the prediction errors.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ ... 计算预测错误的总和。
- en: 'Now we can finally train our autoregressive model. We use our same `train_network`
    function but pass in our new `CrossEntLossTime` function as the loss function
    ℓ—and everything just works:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以最终训练我们的自回归模型了。我们使用相同的`train_network`函数，但将新的`CrossEntLossTime`函数作为损失函数ℓ传入——一切正常工作：
- en: '[PRE46]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 7.5.2  Autoregressive models are generative models
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 自回归模型是生成模型
- en: 'We saved one last detail for the end because it’s easier to *see* it than it
    is to explain. Autoregressive models are not only self-supervised; they also fall
    into a class known as *generative models*. This means they can *generate* new
    data that looks like the original data it was trained on. To do this, we switch
    our model to `eval` mode and create a tensor `sampling` that stores our generated
    output. Any output generated from a model can be called a *sample*, and the process
    of generating that sample is called *sampling*, if you want to sound cool (and
    this is good terminology for you to remember):'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将最后一个细节留到了最后，因为它比解释它更容易*看到*。自回归模型不仅是自监督的；它们还属于一类称为*生成模型*的类别。这意味着它们可以*生成*看起来像原始训练数据的新数据。为此，我们将我们的模型切换到`eval`模式，并创建一个存储我们的生成输出的张量`sampling`。从模型生成的任何输出都可以称为*样本*，生成该样本的过程称为*采样*，如果你想要听起来很酷（并且这是你需要记住的好术语）：
- en: '[PRE47]'
  id: totrans-335
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'To sample from an autoregressive model, we usually need to give the model a
    *seed*. This is some original text that the model is *given*; then the model is
    asked to make predictions about what comes next. The code for setting a seed is
    shown next, where “EMILIA:” is our initial seed, as if the character Emilia is
    about to speak in the play:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 要从自回归模型中采样，我们通常需要给模型一个*种子*。这是模型*给出*的一些原始文本；然后模型被要求预测接下来会发生什么。设置种子的代码如下，其中“EMILIA:”是我们的初始种子，就像角色Emilia即将在剧中说话一样：
- en: '[PRE48]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The sampling process of an autoregressive model is shown in figure 7.10\. Our
    seed is passed in as the initial inputs to the model, and we *ignore* the predictions
    being made. This is because our seed is helping to build the hidden states h of
    the RNN, which contain information about every previous input. Once we have processed
    the entire seed, we have no more inputs. After the seed has run out of inputs,
    we use the *previous* output of the model **x̂**[t] as the *input* for the next
    time step *t* + 1. This is possible because the autoregressive model has *learned
    to predict what comes next*. If it does a good job at this, its predictions can
    be used as inputs, and we end up *generating* new sequences in the process.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 自回归模型的采样过程如图7.10所示。我们将种子作为模型的初始输入传递，并*忽略*正在做出的预测。这是因为我们的种子正在帮助构建RNN的隐藏状态h，其中包含关于每个先前输入的信息。一旦我们处理完整个种子，我们就没有更多的输入了。种子用完输入后，我们使用模型的前一个输出**x̂**[t]作为下一个时间步*t*
    + 1的*输入*。这是因为自回归模型已经*学会了预测接下来会发生什么*。如果它在这一点上做得很好，它的预测可以用作输入，我们最终在过程中*生成*新的序列。
- en: '![](../Images/CH07_F10_Raff.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F10_Raff.png)'
- en: Figure 7.10 A seed is given to the model, and we ignore the predictions being
    made. Once the seed runs out, we use the predictions at time step t as the input
    to the next step *t* + 1.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 给模型一个种子，并忽略正在做出的预测。一旦种子用完，我们使用时间步t的预测作为下一个步骤*t* + 1的输入。
- en: 'But how do we use a prediction as an input? Our model is making a prediction
    about the probability of seeing *every* different character as the next possible
    output. But the next input needs to be a *specific* character. This can be done
    by *sampling* the predictions based on the model’s output probabilities. So if
    the character *a* has a 100% prediction of being next, the model *will* return
    *a*. If instead we have 80% *a*, 10% *b*, and 10% *c*, we will *probably* select
    *a* as the next class, but we could also pick *b* or *c*. The following code does
    just that:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们如何将预测作为输入使用呢？我们的模型正在预测看到 *每个* 不同字符作为下一个可能输出的概率。但下一个输入需要是一个 *特定* 的字符。这可以通过根据模型输出的概率
    *采样* 预测来实现。所以如果字符 *a* 有 100% 的预测概率是下一个，模型 *将* 返回 *a*。如果我们有 80% 的 *a*，10% 的 *b*
    和 10% 的 *c*，我们 *可能* 选择 *a* 作为下一个类别，但我们也可以选择 *b* 或 *c*。下面的代码就是这样做的：
- en: '[PRE49]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Processes all the previous items
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 处理所有前面的项目
- en: ❷ Grabs the last time step
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取最后一步
- en: ❸ Makes probabilities
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算概率
- en: ❹ Samples the next prediction
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 采样下一个预测
- en: ❺ Sets the next prediction
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置下一个预测
- en: ❻ Increases the length by one
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 增加长度为一
- en: Note Just as autoencoders make great embeddings, so do autoregressive models.
    The hidden state `h` in the previous code can be used as an embedding that summarizes
    the entire sequence processed thus far. This is a good way to go from word embeddings
    to sentence or paragraph embeddings.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 正如自编码器可以制作出优秀的嵌入，自回归模型也是如此。前述代码中的隐藏状态 `h` 可以用作一个嵌入，它总结了迄今为止处理过的整个序列。这是一种从词嵌入到句子或段落嵌入的好方法。
- en: 'Now we have a new sequence that we have predicted, but what does it look like?
    That is why we saved an *inverse mapping* from tokens back to our vocabulary with
    the `indx2vocab` `dict`: we can use that to map each integer back to a character,
    and then `join` them together to create an output. The following code converts
    our generative sample back into text we can read:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个我们预测的新序列，但它看起来是什么样子呢？这就是为什么我们使用 `indx2vocab` `dict` 保存了从标记到我们词汇表的反向映射：我们可以使用它将每个整数映射回一个字符，然后
    `join` 它们来创建输出。下面的代码将我们的生成样本转换回我们可以阅读的文本：
- en: '[PRE50]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: You should notice a few things about the output of our generation. While it
    kind of looks Shakespearian, it quickly devolves. This is because with each step
    of the data, we get further from *real* data, and our model *will* make unrealistic
    choices that thus become errors and negatively impact future predictions. So the
    longer we generate, the lower the quality will become.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该注意我们生成输出的几个方面。虽然它看起来有点像莎士比亚的风格，但它很快就会退化。这是因为随着每一步数据的增加，我们离 *真实* 数据就越远，我们的模型
    *将* 做出不切实际的选择，从而成为错误并负面影响未来的预测。所以，我们生成的越长，质量就会越低。
- en: 7.5.3  Changing samples with temperature
  id: totrans-353
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 使用温度改变样本
- en: The model rarely gives a probability of *zero* for any token, which means we
    will eventually select an incorrect or unrealistic next token. If you are 99%
    sure that the next character should be *a*, why give the model the 1% opportunity
    to pick something that is likely wrong? To encourage the model to go with the
    most likely predictions, we can add a *temperature* to the generation process.
    The `temperature` is a scalar that we divide the model’s prediction by before
    computing the `softmax` to make probabilities. This is shown in figure 7.11, where
    you can push the temperature to extreme values like infinity or zero. Something
    with infinite temperature will result in uniformly random behavior (not what we
    want), and something with zero temperature is frozen solid and returns the same
    (most likely) thing over and over again (also not what we want).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 模型很少为任何标记给出 *零* 的概率，这意味着我们最终会选择一个不正确或不切实际的下一个标记。如果你 99% 确定下一个字符应该是 *a*，为什么给模型
    1% 的机会去选择可能错误的东西呢？为了鼓励模型选择最可能的预测，我们可以在生成过程中添加一个 *温度*。这个 *温度* 是一个标量，我们在计算 `softmax`
    之前将模型的预测除以它来使概率。这如图 7.11 所示，你可以将温度推到极端值，如无穷大或零。具有无穷大温度的东西会导致均匀随机的行为（这不是我们想要的），而具有零温度的东西会冻结并反复返回（最可能）的相同东西（这也不是我们想要的）。
- en: '![](../Images/CH07_F11_Raff.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F11_Raff.png)'
- en: Figure 7.11 What food is Edward going to eat? If the temperature is set very
    high, Edward’s selection will be random, regardless of what the initial probabilities
    are. If the temperature is at or near zero, Edward will always pick BBQ because
    it is more likely than any other option. More temperature = more randomness, and
    no negative temperatures are allowed.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.11 爱德华将吃什么？如果温度设置得非常高，爱德华的选择将是随机的，无论初始概率如何。如果温度为零或接近零，爱德华将总是选择烧烤，因为它比任何其他选项更有可能。温度越高
    = 随机性越大，不允许有负温度。
- en: 'Instead of using these extremes, we can instead focus on adding a small effect
    by making the temperature a value slightly larger or smaller than `1.0`. The default
    value of `temperature=1.0` causes no change in the probabilities and so is the
    same as what we already did: compute the original probabilities and sample a prediction
    based on those probabilities. But if you use a `temperature < 1`, then items that
    originally had a larger chance of being selected (like BBQ) will get an even bigger
    advantage and increase their probability (think, “the rich get richer”). If we
    make `temperature > 1`, we end up giving lower-probability items more of a chance
    to be selected, at the cost of the originally higher probabilities. The effect
    of temperature is summarized in figure 7.12 with an example of picking my next
    meal.'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是使用这些极端值，我们可以通过将温度设置为略大于或略小于 `1.0` 的值来专注于添加一个小效果。`temperature=1.0` 的默认值不会改变概率，因此与我们已经做的事情相同：计算原始概率并根据这些概率采样预测。但如果你使用
    `temperature < 1`，那么原本有更大选择机会的项目（如烧烤）将获得更大的优势并增加其概率（想想，“富者愈富”）。如果我们使 `temperature
    > 1`，我们最终会给低概率项目更多的选择机会，这将以原本更高的概率为代价。温度的影响总结在图 7.12 的例子中，展示了选择下一顿饭的概率。
- en: '![](../Images/CH07_F12_Raff.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F12_Raff.png)'
- en: Figure 7.12 Example of how temperature impacts the probability of selecting
    my next meal. I’m most likely to eat BBQ by default because it is delicious. Raising
    the temperature encourages diversity, eventually selecting each item at random
    if we went to the extreme minimum temperature (zero). Lowering temperature reduces
    diversity, eventually selecting only the most likely original item at the extreme
    highest possible temperature (toward infinity).
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.12 温度如何影响选择下一顿饭的概率的例子。我默认最有可能吃烧烤，因为它很美味。提高温度鼓励多样性，最终在极端最低温度（零）时随机选择每个项目。降低温度减少多样性，最终在极端最高可能温度（趋向无穷大）时只选择最有可能的原始项目。
- en: 'In practice, a value of 0.75 is a good default[⁵](#fn26) (I usually see 0.65
    at the low end and 0.8 at the high end) because it maintains diversity but avoids
    picking items that were more unlikely originally (e.g., BBQ is my favorite food
    group, but some diversity is good for you and is more realistic). The following
    code adds the temperature to the sampling process:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，0.75 的值是一个很好的默认值[⁵](#fn26)（我通常看到最低端是 0.65，高端是 0.8），因为它保持了多样性但避免了选择原本不太可能的项目（例如，烧烤是我最喜欢的食物类别，但一些多样性对你有好处，也更现实）。以下代码将温度添加到采样过程中：
- en: '[PRE51]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '❶ Primary addition: controls the temperature and our sampling behavior'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 主要添加：控制温度和我们的采样行为
- en: ❷ Grabs the last time step
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取最后一步
- en: ❸ Makes probabilities
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 制作概率
- en: Why is temperature called temperature?
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么叫温度为温度？
- en: The analogy you should go with is a pot of water, and the temperature is literally
    the temperature of the water. If the temperature is very high, the water begins
    to boil, and the water’s atoms are in random positions as they bounce around.
    As the temperature decreases, the water begins to freeze, and the atoms stay still
    in an organized pattern. High temperature = chaotic (read, random), and low temperature
    is still (read, no randomness).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该遵循的类比是一个水壶，温度实际上是水的温度。如果温度非常高，水开始沸腾，水分子在四处弹跳时处于随机位置。随着温度的降低，水开始结冰，原子保持静止，形成有序的图案。高温
    = 混乱（读作，随机），低温是静止的（读作，没有随机性）。
- en: 'If we print out our predictions, we should see something that feels a little
    more reasonable. It is not perfect, and a bigger model with more epochs of training
    can help improve that. But adding a `temperature` is a common trick to help control
    the generation process:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们打印出我们的预测，我们应该看到一些感觉稍微更合理的东西。它并不完美，一个更大的模型和更多的训练轮次可以帮助提高这一点。但添加一个 `temperature`
    是一个常见的技巧，有助于控制生成过程：
- en: '[PRE52]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'So you may be thinking, why not lower the temperature even further? Should
    we not *always* want to go with the most likely prediction? This gets to some
    deep issues about the difficulty of evaluating generative and autoregressive models,
    especially when the input is something like human text, which is not all that
    predictable to begin with. If I told you I could *perfectly* predict what someone
    was going to say, you would likely be incredulous. How could that be possible?
    We should always apply that same standard to our model. If we *always* select
    the most likely prediction, we are assuming the model can *perfectly* predict
    what a person would say next. We can try that by setting the temperature to a
    very low value like 0.05:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你可能想，为什么不进一步降低温度呢？我们难道不应该**总是**选择最可能的预测吗？这涉及到一些关于评估生成和自回归模型难度的深层次问题，尤其是在输入类似于人类文本的情况下，这种文本一开始就并不那么可预测。如果我告诉你我能**完美**预测某人将要说什么，你可能会感到难以置信。这怎么可能呢？我们应该始终对我们的模型应用同样的标准。如果我们**总是**选择最可能的预测，我们就是在假设模型可以**完美**预测一个人接下来会说什么。我们可以通过将温度设置为非常低的值（如0.05）来尝试这一点：
- en: '[PRE53]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '❶ Very low temp: will almost always pick the most likely items'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 非常低的温度：几乎总是选择最可能的项
- en: ❷ Grabs the last time step
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取最后一个时间步
- en: ❸ Makes probabilities
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算概率
- en: The model has become *very* repetitive. This is what usually happens when you
    select the most likely token as the next one; the model devolves to selecting
    a sequence of the most common words/tokens over and over and over again.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型已经变得**非常**重复。当你选择最可能的标记作为下一个时，通常会发生这种情况；模型会退化到反复选择最常见的单词/标记的序列。
- en: Hot alternatives to temperature
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 温度的热门替代品
- en: 'Adjusting the temperature is but one of many possible techniques for selecting
    the generated output in a more realistic-looking manner. Each has pros and cons,
    but you should be aware of three alternatives: beam search, top-k sampling, and
    nucleus sampling. The fine folks at Hugging Face have a good blog post that introduces
    these at a high level ([https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate)).'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 调整温度只是选择更逼真的生成输出方式的可能技术之一。每种技术都有其优缺点，但你应该了解三种替代方案：束搜索、top-k采样和核采样。Hugging Face的团队有一篇很好的博客文章，从高层次介绍了这些方法（[https://huggingface.co/blog/how-to-generate](https://huggingface.co/blog/how-to-generate))。
- en: 7.5.4  Faster sampling
  id: totrans-377
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.4  更快的采样
- en: You may have noticed that the sampling process takes a surprisingly long time—about
    45 to 50 seconds to generate 500 characters—yet we could train over 100,000 characters
    in just seconds per epoch. This is because each time we make a prediction, we
    needed to refeed the entire generated sequence far back into the model to get
    the next prediction. Using Big O notation means we are doing *O*(*n*²) work to
    generate a sequence of *O*(*n*) length.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到采样过程需要的时间出奇地长——大约45到50秒才能生成500个字符——然而我们只需在每次epoch中几秒钟内就能训练超过10万个字符。这是因为每次我们做出预测时，我们需要将整个生成的序列重新输入到模型中，以获取下一个预测。使用大O符号表示，我们正在做**O**(*n*²)的工作来生成一个**O**(*n*)长度的序列。
- en: 'The `GRUCell` that processes sequences one step at a time makes it easy for
    us to solve this problem. We break our `for` loop up into two parts, each using
    the `step` function directly instead of the `forward` function of the module.
    The first loop pushes the seed into the model, updating an explicitly created
    set of hidden states `h_prevs`. After that, we can write a new loop that generates
    the new content and calls the `step` function to update the model after sampling
    the next character. This process is shown in the following code:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 处理序列的每个步骤的`GRUCell`使得我们能够轻松解决这个问题。我们将`for`循环分成两部分，每部分直接使用`step`函数而不是模块的`forward`函数。第一个循环将种子推入模型，更新一个明确创建的隐藏状态集合`h_prevs`。之后，我们可以编写一个新的循环来生成新内容，并在采样下一个字符后调用`step`函数来更新模型。这个过程在以下代码中展示：
- en: '[PRE54]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: ❶ Sets up our seed and the location to store the generated content
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置我们的种子和存储生成内容的存储位置
- en: ❷ Picks a temperature
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择温度
- en: ❸ Initializes the hidden state to avoid redundant work
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化隐藏状态以避免重复工作
- en: ❹ Pushes the seed through
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将种子推入
- en: ❺ Generates new text one character at a time
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 逐个字符生成新文本
- en: ❻ Makes probabilities
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算概率
- en: ❼ Now pushes only the new sample into the model
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 现在只将新的样本推入模型
- en: 'This new code runs in less than a second. This is much faster, and it gets
    faster the longer the sequence we want to generate, because it has a better Big
    O complexity of *O*(*n*) to generate *O*(*n*) tokens. Next we print out the generated
    result, which you can see is qualitatively the same as before:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 这段新代码的运行时间不到一秒。这要快得多，而且随着我们想要生成的序列越长，它运行得越快，因为它具有更好的Big O复杂度*O*(*n*)来生成*O*(*n*)个标记。接下来我们打印出生成的结果，你可以看到它从本质上与之前相同：
- en: '[PRE55]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'With that, you now know all the basics of autoencoders and autoregressive models,
    two related approaches that share the same fundamental idea: use the input data
    as the target output. These can be especially powerful techniques to mimic/replace
    expensive simulations (have the network learn to predict what happens next), clean
    up noisy data (inject realistic noise and learn how to remove it), and generally
    train useful models without any labeled data. The latter note will become important
    in chapter 13 when you learn how to do transfer learning, a technique allowing
    you to use unlabeled data to improve results on a smaller labeled dataset.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你现在已经了解了自动编码器和自回归模型的所有基础知识，这两种相关的方法共享相同的基本思想：使用输入数据作为目标输出。这些技术可以特别强大，可以模拟/替换昂贵的模拟（让网络学习预测接下来会发生什么），清理噪声数据（注入真实的噪声并学习如何去除它），并且通常在没有标记数据的情况下训练有用的模型。后者的说明将在第13章中变得重要，当你学习如何进行迁移学习时，这是一种允许你使用未标记数据来改进较小标记数据集上的结果的技术。
- en: Exercises
  id: totrans-391
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在Manning在线平台Inside Deep Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))上分享和讨论你的解决方案。一旦你提交了自己的答案，你将能够看到其他读者提交的解决方案，并看到作者认为哪些是最好的。
- en: Create a new version of the MNIST dataset that does not contain the numbers
    9 and 5, and train one of the autoencoders on this dataset. Then run the autoencoder
    on the test dataset, and record the average error (MSE) for each of the 10 classes.
    Do you see any patterns in the results, and can the autoencoder identify 9 and
    5 as outliers?
  id: totrans-393
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个新的MNIST数据集版本，其中不包含数字9和5，并在该数据集上训练一个自动编码器。然后在该测试数据集上运行自动编码器，并记录每个10个类别的平均误差（均方误差）。你在结果中看到任何模式吗？自动编码器能将9和5识别为异常值吗？
- en: Train the bottleneck-style autoencoder with a target size *D*′ = 64 dimensions.
    Then use k-means ([https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means))to
    create *k* = 10 clusters on the original version of MNIST and the version encoded
    using *D*′ = 64 dimensions. Use the homogeneity score from scikit-learn ([http://mng.bz/nYQV](http://mng.bz/nYQV))
    to evaluate these clusters. Which method does best:k-means on the original images
    or k-means on the encoded representations?
  id: totrans-394
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用目标大小*D*′ = 64维度的瓶颈式自动编码器进行训练。然后使用k-means ([https://scikit-learn.org/stable/modules/clustering.html#k-means](https://scikit-learn.org/stable/modules/clustering.html#k-means))在原始版本的MNIST和使用*D*′
    = 64维度编码的版本上创建*k* = 10个聚类。使用scikit-learn中的同质性得分 ([http://mng.bz/nYQV](http://mng.bz/nYQV))来评估这些聚类。哪种方法表现最好：原始图像上的k-means还是编码表示上的k-means？
- en: Use the denoising approach to implement a denoising convolutional network. This
    can be done by not having any pooling operations so that the input stays the same
    size.
  id: totrans-395
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用去噪方法实现去噪卷积网络。这可以通过不执行任何池化操作来实现，这样输入的大小保持不变。
- en: Sometimes people train deep autoencoders with weight sharing between the encoder
    and decoder. Try implementing a deep bottleneck autoencoder that uses the `TransposeLinear`
    layer for all of the decoder’s layers. Compare the weight shared versus the non-weight
    shared network when you have only n=1,024, 8,192, 32,768, and all 60,000 samples
    from MNIST.
  id: totrans-396
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有时人们会在编码器和解码器之间共享权重的情况下训练深度自动编码器。尝试实现一个使用`TransposeLinear`层的所有解码器层的深度瓶颈自动编码器。当只有n=1,024、8,192、32,768和MNIST的所有60,000个样本时，比较共享权重与不共享权重的网络。
- en: '**Challenging:** Train an asymmetric denoising autoencoder for MNIST where
    the encoder is a fully connected network and the decoder is a convolutional network.
    *Hint:* You will need to end the encoder with a `View` layer that changes the
    shape from (*B*,*D*) to (*B*,*C*,28,28), where D is the number of neurons in the
    last `nn.LinearLayer` of the encoder and *D* = *C* ⋅ 28 ⋅ 28. Do the results of
    this network look better or worse than the fully connected network in the chapter,
    and how do you think intermixing architectures impacts that result?'
  id: totrans-397
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挑战性任务:** 为 MNIST 训练一个非对称去噪自编码器，其中编码器是一个全连接网络，解码器是一个卷积网络。*提示:* 您需要在编码器末尾添加一个
    `View` 层，该层将形状从 (*B*,*D*) 改变为 (*B*,*C*,28,28)，其中 D 是编码器中最后一个 `nn.LinearLayer`
    的神经元数量，*D* = *C* ⋅ 28 ⋅ 28。这个网络的结果比章节中的全连接网络好还是差，您认为架构的混合如何影响这个结果？'
- en: '**Challenging:** Reshape the MNIST dataset as a sequence of pixels, and train
    an autoregressive model over the pixels. This requires using real-valued inputs
    and outputs, so you will not use an `nn.Embedding` layer, and you will need to
    switch to the MSE loss function. After training, try generating multiple digits
    from this autoregressive pixel model.'
  id: totrans-398
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**挑战性任务:** 将 MNIST 数据集重塑为像素序列，并在像素上训练一个自回归模型。这需要使用实值输入和输出，因此您将不会使用 `nn.Embedding`
    层，并且您需要切换到 MSE 损失函数。训练后，尝试从这个自回归像素模型生成多个数字。'
- en: Convert the `GRUCell`s in the autoregressive model to `LSTMCell`s, and train
    a new model. Which one do you think has better generated output?
  id: totrans-399
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将自回归模型中的 `GRUCell` 转换为 `LSTMCell`，并训练一个新的模型。你认为哪个模型的生成输出更好？
- en: The `AutoRegressiveDataset` can start an input in the middle of a sentence since
    it naively grabs subsequences of the input. Write a new version that will only
    select the start of a sequence at the start of a new line (i.e., after a carriage
    return ‘\n’) and then returns the next `max_chunk` characters (it’s OK if there
    is some overlap between chunks). Train a model on this new version of the dataset.
    Do you think it changes the characteristics of the generated output?
  id: totrans-400
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`AutoRegressiveDataset` 可以从句子中间开始输入，因为它天真地抓取输入的子序列。编写一个新版本，它只在新行的开始处（即换行符‘\n’之后）选择序列的开始，然后返回下一个
    `max_chunk` 个字符（如果块之间有重叠是可以的）。在这个数据集的新版本上训练一个模型。你认为这会改变生成输出的特征吗？'
- en: After training your autoregressive model on sentences, use the `LastTimeStep`
    class to extract a feature vector representing each sentence. Then feed these
    vectors into your favorite clustering algorithm and see if you can find any groups
    of similar styles or types of sentences. *Note:* You may want to sub-sample a
    smaller number of sentences to make your clustering algorithm run faster.
  id: totrans-401
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在对句子进行自回归模型训练后，使用 `LastTimeStep` 类提取代表每个句子的特征向量。然后将这些向量输入到您喜欢的聚类算法中，看看您是否可以找到任何具有相似风格或类型的句子组。*注意:*
    您可能需要子采样更少的句子以使聚类算法运行更快。
- en: Summary
  id: totrans-402
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Self-supervision is a means of training a neural network by using parts of the
    input *as the labels* we are trying to predict.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我监督是一种通过使用输入的一部分 *作为我们试图预测的标签* 来训练神经网络的方法。
- en: Self-supervision is considered unsupervised because it can be applied to any
    data and does not require any kind of process or human to manually label the data.
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自我监督被认为是无监督的，因为它可以应用于任何数据，并且不需要任何过程或人类手动标记数据。
- en: Autoencoding is one of the most popular forms of self-supervision. It works
    by having the network predict the input as the output but somehow constrains the
    network so that it can’t naively return the original input.
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自编码是自我监督最受欢迎的形式之一。它通过让网络预测输入作为输出，但以某种方式约束网络，使其不能天真地返回原始输入。
- en: Two popular constraints are a bottleneck design that forces the dimension to
    shrink before expanding back out and a denoising approach where the input is altered
    before being given to the network, but the network still must predict the unaltered
    output.
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种流行的约束是瓶颈设计，它强制维度在扩展回之前缩小，以及去噪方法，其中在输入被提供给网络之前改变输入，但网络仍然必须预测未改变输出。
- en: If we have a sequence problem, we can instead use autoregressive models, which
    look at every previous input in a sequence to predict the next input.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们有一个序列问题，我们可以使用自回归模型，这些模型查看序列中的每个先前输入来预测下一个输入。
- en: Autoregressive approaches have the benefit of being *generative*, which means
    we can create synthetic data from the model!
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自回归方法的好处是它是 *生成性的*，这意味着我们可以从模型中创建合成数据！
- en: '* * *'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ *Latent* generally means something that isn’t (easily) visible but does exist
    and/or can be enhanced. For example, a latent infection exists but has no symptoms,
    so you can’t see it. But untreated, it will emerge and become more visible. *Latent*
    in our context is fortunately not as morbid.[↩](#fnref20)
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ *潜在*通常指的是那些（不易）可见但确实存在且/或可以增强的事物。例如，潜在感染存在但没有症状，所以你看不见它。但如果不治疗，它将出现并变得更加明显。在我们这个语境中，“潜在”幸运地并没有那么可怕。[↩](#fnref20)
- en: ² Technically, that means we aren’t learning the PCA algorithm—but we are learning
    something *very* similar to it. Close enough is good enough.[↩](#fnref21)
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ² 技术上，这意味着我们不是在学习PCA算法——但我们正在学习与它非常相似的东西。足够接近就是好的。[↩](#fnref21)
- en: ³ E. Raff, “Neural fingerprint enhancement,” in *17th IEEE International Conference
    on Machine Learning and Applications (ICMLA)*, 2018, pp. 118–124, https://doi.org/10.1109/ICMLA.2018.00025.[↩](#fnref22)
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: ³ E. Raff, “神经指纹增强”，在 *第17届IEEE国际机器学习与应用会议（ICMLA）*，2018年，第118–124页，https://doi.org/10.1109/ICMLA.2018.00025。[↩](#fnref22)
- en: ⁴ You should look up “topic modeling” algorithms if you want to maximize your
    results with text data. There are deep topic models, but they are a bit beyond
    what we cover in this book. That said, I have seen people be reasonably successful
    with this autoregressive approach, which is more flexible with new situations
    and types of data than topic models are.[↩](#fnref25)
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 如果你想要通过文本数据最大化你的结果，你应该查找“主题建模”算法。有深度主题模型，但它们超出了我们在这本书中涵盖的范围。话虽如此，我见过有人用这种自回归方法取得了相当的成功，这种方法比主题模型更灵活，更适合新的情况和数据类型。[↩](#fnref25)
- en: ⁵ A good default for text generation, that is. Other tasks may need you to play
    around with the temperature to select a better value.[↩](#fnref26)
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: ⁵ 这是对文本生成的一个很好的默认设置。其他任务可能需要你调整温度以选择更好的值。[↩](#fnref26)
