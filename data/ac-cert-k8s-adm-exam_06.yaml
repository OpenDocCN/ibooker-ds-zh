- en: 6 Communication in a Kubernetes cluster
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 Kubernetes 集群中的通信
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: How nodes communicate via CNI and the different CNIs available
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点如何通过 CNI 和不同的 CNIs 进行通信
- en: How Pod-to-Pod communication happens
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 之间的通信是如何发生的
- en: Types of Services in Kubernetes and when they are used
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中服务类型及其使用情况
- en: Assigning IP addresses to Pods
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 IP 地址分配给 Pods
- en: Communication via DNS and how to use CoreDNS
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 DNS 进行通信以及如何使用 CoreDNS
- en: Using Ingress and Ingress controllers
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Ingress 和 Ingress 控制器
- en: Many find that networking in Kubernetes is complex, but we will break it down
    fully in this chapter, especially since it’s 20% of the CKA exam. There are a
    few important concepts that will clear up a lot of confusion, and because we’ve
    covered how bridge networking works within containers, I think it will all start
    to come together. By the end of this chapter, you’ll know how Pods talk to each
    other within a cluster, which is the essence of the Services and networking section
    of the exam.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人发现 Kubernetes 中的网络很复杂，但我们将在此章中全面解析它，特别是因为它占 CKA 考试的 20%。有几个重要概念可以澄清很多困惑，而且因为我们已经介绍了容器内桥接网络的工作方式，我认为所有这些都将开始整合。到本章结束时，你将知道
    Pods 在集群内部是如何相互通信的，这是考试服务和网络部分的核心。
- en: The Services and networking domain
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 服务和网络领域
- en: This chapter covers the Services and networking domain of the CKA curriculum.
    This domain includes the way nodes and Pods communicate with each other in the
    cluster. It encompasses the following competencies.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了 CKA 课程的服务和网络领域。该领域包括节点和 Pods 在集群中相互通信的方式。它包括以下能力。
- en: '| Competency | Chapter section |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| 能力 | 章节部分 |'
- en: '| --- | --- |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Understand host networking configuration on the cluster nodes. | 6.5 |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 理解集群节点上的主机网络配置。 | 6.5 |'
- en: '| Understand connectivity between Pods. | 6.2 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 理解 Pods 之间的连接性。 | 6.2 |'
- en: '| Understand ClusterIP, NodePort, and LoadBalancer Service types and Endpoints.
    | 6.4 |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 理解 ClusterIP、NodePort 和 LoadBalancer 服务类型和端点。 | 6.4 |'
- en: '| Know how to use Ingress controllers and Ingress resources. | 6.3 |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 了解如何使用 Ingress 控制器和 Ingress 资源。 | 6.3 |'
- en: '| Know how to configure and use CoreDNS. | 6.1, 6.2 |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 了解如何配置和使用 CoreDNS。 | 6.1, 6.2 |'
- en: '| Choose an appropriate container network interface plugin. | 6.5 |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 选择合适的容器网络接口插件。 | 6.5 |'
- en: 6.1 Configuring DNS
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 配置 DNS
- en: Inside a Kubernetes cluster, CoreDNS is responsible for resolving hostnames
    to IP addresses. As of version 1.12 of Kubernetes, CoreDNS has been the default
    DNS server and will be present on the exam. CoreDNS is also used in our kind Kubernetes
    cluster.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 集群内部，CoreDNS 负责将主机名解析为 IP 地址。截至 Kubernetes 1.12 版本，CoreDNS 已经成为默认的
    DNS 服务器，并将出现在考试中。CoreDNS 也用于我们的 kind Kubernetes 集群。
- en: The CKA exam asks you to configure and use CoreDNS, which includes resolving
    hostnames to IP addresses, making changes to how DNS works, and knowing where
    the CoreDNS configuration is located and how to change it. For example, the exam
    question will say something like the following.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: CKA 考试要求你配置和使用 CoreDNS，包括将主机名解析为 IP 地址、更改 DNS 的工作方式以及了解 CoreDNS 配置的位置和如何更改它。例如，考试问题可能会说如下内容。
- en: '| Exam Task In cluster `k8s`, change the IP addresses given to new Services
    to a CIDR range of 100.96.0.0/16\. Change the IP address associated with the cluster
    DNS Service to match this new Service range. Proceed to change the kubelet configuration
    so that new Pods can receive the new DNS Service IP address, and so they can resolve
    domain names. Edit the kubelet ConfigMap so that kubelet is updated in place and
    immediately reflected. Upgrade the node to receive the new kubelet configuration.
    Finally, test this by creating a new Pod and verifying that the Pod has the new
    IP address of the DNS Service. |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 考试任务 在集群 `k8s` 中，将分配给新服务的 IP 地址更改为 100.96.0.0/16 的 CIDR 范围。将与集群 DNS 服务关联的
    IP 地址更改为与新服务范围匹配。继续更改 kubelet 配置，以便新的 Pods 可以接收新的 DNS 服务 IP 地址，并且可以解析域名。编辑 kubelet
    ConfigMap，以便 kubelet 在原地更新并立即反映。升级节点以接收新的 kubelet 配置。最后，通过创建一个新的 Pod 并验证该 Pod
    是否具有 DNS 服务的新的 IP 地址来测试这一点。 |'
- en: If you don’t already have access to an existing Kubernetes cluster, creating
    a Kubernetes cluster with kind as explained in appendix A. A single-node cluster
    will suffice for this type of task. As soon as you have access to your kind cluster,
    get a shell to the control plane node with the command `docker exec -it kind-control-plane
    bash`. Once you have a shell, set your alias `k` to equal `kubectl` with the command
    `alias k=kubectl`. On the exam, they will already have this alias set for you,
    so it’s good to get used to using `k` as opposed to typing `kubectl` over and
    over again.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有访问现有的Kubernetes集群，可以按照附录A中解释的kind创建一个Kubernetes集群。单个节点集群就足够完成此类任务。一旦你有了对kind集群的访问权限，使用命令`docker
    exec -it kind-control-plane bash`获取控制平面节点的shell。一旦你有了shell，使用命令`alias k=kubectl`将别名`k`设置为等于`kubectl`。在考试中，他们已经为你设置了此别名，所以熟悉使用`k`而不是反复输入`kubectl`是很好的。
- en: Let’s change the Service CIDR block that is given to each Service that’s created
    in the cluster. This is a feature that’s controlled by the API server. We can
    locate the API server configuration in the directory `/etc/kubernetes/manifests`,
    and the name of the file is `kube-apiserver.yaml`. Let’s open this file to edit
    it with the command `vim /etc/kubernetes/manifests/kube-apiserver.yaml`; it will
    open in the Vim text editor. Under the command section of YAML, we’ll place our
    cursor at the line that begins with `- --service-cluster-ip-range` and change
    the CIDR range from 10.96.0.0/16 to 100.96.0.0/16 (just add a 0 after the 10).
    The result should be exactly like the YAML in figure 6.1.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更改分配给集群中创建的每个服务的Service CIDR块。这是一个由API服务器控制的特性。我们可以在目录`/etc/kubernetes/manifests`中找到API服务器配置，文件名为`kube-apiserver.yaml`。让我们使用命令`vim
    /etc/kubernetes/manifests/kube-apiserver.yaml`打开此文件进行编辑；它将在Vim文本编辑器中打开。在YAML命令部分，我们将光标置于以`-
    --service-cluster-ip-range`开头的行，并将CIDR范围从10.96.0.0/16更改为100.96.0.0/16（在10之后添加一个0）。结果应该与图6.1中的YAML完全相同。
- en: '![](../../OEBPS/Images/06-01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-01.png)'
- en: Figure 6.1 Change the Service CIDR range that gives each new Service a new IP
    address.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 更改分配给每个新服务的IP地址的Service CIDR范围。
- en: After you’ve changed the cluster IP range to 100.96.0.0/16, save and close the
    file. This will automatically restart the API server, so you will have to wait
    up to 2 minutes for the API server to reboot and run any additional `kubectl`
    commands.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在将集群IP范围更改为100.96.0.0/16后，保存并关闭文件。这将自动重启API服务器，因此你需要等待最多2分钟以等待API服务器重新启动并运行任何额外的`kubectl`命令。
- en: 'Next, let’s change the IP address associated with the cluster DNS Service.
    We examine Services in depth later in this chapter. For now, just know that it’s
    a cluster-wide communication mechanism for DNS. The DNS Service is in the `kube-system`
    namespace, and we can view the Service with the command `k -n kube-system get
    svc`. You will see the Service named `kube-dns`. The output looks like this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们更改与集群DNS服务关联的IP地址。我们将在本章后面深入探讨服务。现在，只需知道它是一个集群范围内的DNS通信机制。DNS服务位于`kube-system`命名空间中，我们可以使用命令`k
    -n kube-system get svc`查看服务。你会看到名为`kube-dns`的服务。输出看起来像这样：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: EXAM TIP Type the namespace just after the `k` to autocomplete the Kubernetes
    resources (press the Tab key to autocomplete the name of the resource). On the
    exam, they will have longer and more complicated names, which are prone to typos.
    Always copy and paste where you can, and use autocomplete to prevent typos!
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：在`k`之后直接输入命名空间以自动完成Kubernetes资源（按Tab键自动完成资源名称）。在考试中，它们的名称会更长且更复杂，容易出错。尽可能复制粘贴，并使用自动完成功能以防止出错！
- en: To edit this Service, we can type the command `k -n kube-system edit svc kube-dns`,
    and it will bring up the YAML in a Vi text editor (you may have to install Vim
    first with the command `apt update; apt install -y vim`). The Vi text editor will
    look similar to figure 6.2\. Bring your cursor down to the line that starts with
    `clusterIP`, press the I key (insert mode) on the keyboard, and replace the value
    10.96.0.10 with 100.96.0.10 (add a zero after the first 10). Do the same for the
    line under `clusterIPs`, so this will also have the new value of 100.96.0.10.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要编辑此服务，我们可以输入命令`k -n kube-system edit svc kube-dns`，它将在Vi文本编辑器中打开YAML文件（你可能需要先使用命令`apt
    update; apt install -y vim`安装Vim）。Vi文本编辑器将类似于图6.2。将光标移至以`clusterIP`开头的行，在键盘上按I键（插入模式），并将值10.96.0.10替换为100.96.0.10（在第一个10之后添加一个零）。对于`clusterIPs`下的行也做同样的操作，这样它也将具有新的值100.96.0.10。
- en: '![](../../OEBPS/Images/06-02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-02.png)'
- en: Figure 6.2 Edit the `kube-dns` Service in place, replacing the values for `clusterIP`
    and `clusterIPs``.`
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 在原地编辑 `kube-dns` Service，替换 `clusterIP` 和 `clusterIPs` 的值。`
- en: 'Once you’ve replaced the values, press the Esc key on the keyboard to get out
    of insert mode, followed by `:wq` to save and quit. You will receive the following
    message: “services ‘kube-dns’ was not valid: spec.clusterIPs[0]: Invalid value:
    []string{"100.96.0.10"}: may not change once set.” The message in figure 6.3 will
    appear at the top of the page.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '替换完值后，按键盘上的 Esc 键退出插入模式，然后输入 `:wq` 保存并退出。你会收到以下消息：“services ‘kube-dns’ was
    not valid: spec.clusterIPs[0]: Invalid value: []string{"100.96.0.10"}: may not
    change once set.” 图 6.3 中的消息将出现在页面顶部。'
- en: '![](../../OEBPS/Images/06-03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-03.png)'
- en: Figure 6.3 After editing the `kube-dns` Service, you will receive a message
    that the `clusterIP` value may not be changed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.3 编辑 `kube-dns` Service 后，你会收到一条消息，指出 `clusterIP` 的值可能无法更改。
- en: 'This is expected here because only certain types of parameters can be changed
    for Kubernetes objects that are in a running state, so proceed to type `:wq` once
    again and return to the command prompt, ignoring the error message. When you return
    to the command prompt, you will get a message that says the following:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 这在这里是预期的，因为只有某些类型的参数可以更改处于运行状态的 Kubernetes 对象，所以请再次输入 `:wq` 并返回到命令提示符，忽略错误信息。当你返回到命令提示符时，你会得到一条消息，内容如下：
- en: '[PRE1]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Disregard the error; this is also what we expected. The location in which the
    YAML was stored may be different for you, but we will use that YAML to replace
    the Service with force. To do this, we’ll type the command `k replace -f /tmp/kubectl-edit-3485293250.yaml
    --force`. The output will be similar to this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略错误；这也是我们预期的。YAML 存储的位置可能因你而异，但我们将使用该 YAML 强制替换 Service。为此，我们将输入命令 `k replace
    -f /tmp/kubectl-edit-3485293250.yaml --force`。输出将类似于以下内容：
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Now that we’ve changed the `kube-dns` Service, the new IP address is given
    with the command `k -n kube-system get svc`, as you see in the output here:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经更改了 `kube-dns` Service，新的 IP 地址可以通过命令 `k -n kube-system get svc` 获取，正如你在下面的输出中看到的那样：
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: For newly created Pods to receive the new DNS information, we need to modify
    the kubelet configuration. There are two places where we can adjust the kubelet
    configuration. The first place is the YAML file, which is the configuration YAML
    manifest for the kubelet. Let’s perform the command `vim /var/lib/kubelet/config.yaml`
    to open the kubelet YAML manifest. When you have it open, you’ll notice that there’s
    a section called `clusterDNS``:`. Change the value from 10.96.0.10 to 100.96.0.10
    (press the I key for insert mode), which is the new Service IP address for the
    Service named `kube-dns`. The result should be what you see in figure 6.4.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让新创建的 Pod 接收新的 DNS 信息，我们需要修改 kubelet 配置。我们可以调整 kubelet 配置的两个地方。第一个地方是 YAML
    文件，这是 kubelet 的配置 YAML 清单。让我们执行命令 `vim /var/lib/kubelet/config.yaml` 以打开 kubelet
    YAML 清单。当你打开它时，你会注意到有一个名为 `clusterDNS` 的部分。将值从 10.96.0.10 更改为 100.96.0.10（按 I
    键进入插入模式），这是名为 `kube-dns` 的 Service 的新 Service IP 地址。结果应该如图 6.4 所示。
- en: '![](../../OEBPS/Images/06-04.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-04.png)'
- en: Figure 6.4 The file `/var/lib/kubelet/config.yaml` replaces the `clusterDNS`
    with the new IP address of DNS.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.4 文件 `/var/lib/kubelet/config.yaml` 将 `clusterDNS` 替换为 DNS 的新 IP 地址。
- en: 'Once you’ve made the change, you can press the Esc key on your keyboard to
    get out of insert mode, followed by `:wq` to save the file and quit Vim. As you
    may have noticed, this didn’t do much; it just changed a file that didn’t affect
    the cluster. To affect the cluster and have our changes implemented immediately,
    you need to edit the ConfigMap associated with the kubelet configuration. To do
    this, perform the command `k -n kube-system edit cm kubelet-config`. You’ll see
    the same type of YAML structure as you saw in the `config.yaml` file. Again go
    to the line that starts with clusterDNS, and change the value from 10.96.0.10
    to 100.96.0.10 (don’t forget insert mode!). Once you’ve done this, press the Esc
    key and type `:wq` to save the file and quit to save the ConfigMap. The output
    will look similar to this:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成更改，您可以通过按键盘上的 Esc 键退出插入模式，然后输入 `:wq` 来保存文件并退出 Vim。如您所注意到的，这并没有做太多；它只是更改了一个不会影响集群的文件。要影响集群并立即实施我们的更改，您需要编辑与
    kubelet 配置关联的 ConfigMap。为此，执行命令 `k -n kube-system edit cm kubelet-config`。您将看到与
    `config.yaml` 文件中相同的 YAML 结构。再次找到以 clusterDNS 开头的行，并将值从 10.96.0.10 更改为 100.96.0.10（别忘了插入模式！）。完成此操作后，按
    Esc 键并输入 `:wq` 来保存文件并退出以保存 ConfigMap。输出将类似于以下内容：
- en: '[PRE4]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Because the kubelet is a daemon that’s currently running on the node, we must
    update the node of this configuration, as well as reload the daemon and restart
    the kubelet Service on the node. First, to update the kubelet configuration on
    the node, perform the command `kubeadm upgrade node phase kubelet-config`. The
    output will look like this:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 kubelet 是当前正在节点上运行的守护进程，我们必须更新此配置的节点，以及重新加载守护进程并在节点上重启 kubelet 服务。首先，要更新节点上的
    kubelet 配置，执行命令 `kubeadm upgrade node phase kubelet-config`。输出将类似于以下内容：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Now that we’ve upgraded the kubelet configuration for the node, we can reload
    the daemon with the command `systemctl daemon-reload` and restart the Service
    with the command `systemctl restart kubelet`. You will not get an output; you
    will just return to the command prompt, so as long as there are no error messages,
    you have successfully restarted the kubelet Service.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已升级了节点的 kubelet 配置，我们可以使用命令 `systemctl daemon-reload` 重新加载守护进程，并使用命令 `systemctl
    restart kubelet` 重新启动服务。您将不会得到输出；您将直接返回到命令提示符，所以只要没有错误消息，您已成功重新启动了 kubelet 服务。
- en: EXAM TIP You may be presented with tasks on the exam that require you to start,
    restart, or reload the kubelet daemon. The commands `systemctl stop kubelet systemctl
    start kubelet`, `systmectl restart kubelet`, and `systemctl daemon-reload` are
    good to memorize.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：您可能会在考试中遇到需要您启动、重启或重新加载 kubelet 守护进程的任务。命令 `systemctl stop kubelet systemctl
    start kubelet`、`systmectl restart kubelet` 和 `systemctl daemon-reload` 是值得记忆的。
- en: 'Finally, to test all the changes we made so far, we can create a new Pod and
    check that the DNS IP address is correct and that DNS is able to resolve example.com.
    Let’s perform the command `kubectl run netshoot --image=nicolaka/netshoot --command
    sleep --command "3600"` to create a new Pod in the default namespace. I like to
    use this image because it comes preinstalled with DNS utilities, which are handy
    for testing the network. If you’d like to know more details about this image,
    visit the DockerHub page here: [https://hub.docker.com/r/nicolaka/netshoot](https://hub.docker.com/r/nicolaka/netshoot).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了测试我们迄今为止所做的所有更改，我们可以创建一个新的 Pod 并检查 DNS IP 地址是否正确以及 DNS 是否能够解析 example.com。让我们执行命令
    `kubectl run netshoot --image=nicolaka/netshoot --command sleep --command "3600"`
    来在默认命名空间中创建一个新的 Pod。我喜欢使用这个镜像，因为它预先安装了 DNS 工具，这对于测试网络很有用。如果您想了解更多关于此镜像的详细信息，请访问
    DockerHub 页面：[https://hub.docker.com/r/nicolaka/netshoot](https://hub.docker.com/r/nicolaka/netshoot)。
- en: 'We run the two commands `sleep` and `3600` so that the Pod will stay in a running
    state (for 3600 seconds, or 60 minutes). We can check if the Pod is in a running
    state with the command `k get po`. The output should look similar to this:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们运行 `sleep` 和 `3600` 这两个命令，以便 Pod 保持运行状态（3600 秒，或 60 分钟）。我们可以使用命令 `k get po`
    检查 Pod 是否处于运行状态。输出应类似于以下内容：
- en: '[PRE6]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The Pod is running, so now you can get a Bash shell to the container. To do
    this, perform the command `k exec -it netshoot --bash`. You will notice that your
    prompt changes, which means you have successfully entered the container within
    the Pod. The output should look like this:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 正在运行，因此现在您可以通过命令 `k exec -it netshoot --bash` 获取容器的 Bash shell。您会注意到您的提示符已更改，这意味着您已成功进入
    Pod 内的容器。输出应类似于以下内容：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now that you have a Bash shell open in the container, you can run the command
    `cat /etc/resolv.conf` to check that the correct DNS IP address is listed. The
    output should be similar to this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经在容器中打开了Bash shell，您可以运行命令`cat /etc/resolv.conf`来检查是否列出了正确的DNS IP地址。输出应该类似于以下内容：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The DNS is correctly configured; therefore, Pods can resolve DNS names using
    CoreDNS in the cluster. You can check that this Pod can resolve a DNS query to
    example.com with the command `nslookup example.com`. `nslookup` is a DNS utility
    that allows you to query name servers. The output should look like this:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: DNS配置正确；因此，Pod可以在集群中使用CoreDNS解析DNS名称。您可以使用命令`nslookup example.com`检查这个Pod是否能够解析example.com的DNS查询。`nslookup`是一个DNS工具，允许您查询名称服务器。输出应该看起来像这样：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Because the container is using the correct DNS server (100.96.0.10), it was
    able to resolve example.com to 93.184.216.34; therefore, our configuration of
    CoreDNS was successful, and all the preceding steps were effective in customizing
    CoreDNS to correctly address the needs of this exam task.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 因为容器正在使用正确的DNS服务器（100.96.0.10），所以它能够将example.com解析为93.184.216.34；因此，我们的CoreDNS配置是成功的，并且所有前面的步骤都有效地自定义了CoreDNS以满足这个考试任务的需求。
- en: In summary, to complete this task, we modified the API configuration in `/etc/kubernetes/manifests/kube-apiserver.yaml`;
    we edited the Service named `kube-dns` in the `kube-system` namespace; we modified
    the `kubeclet` configuration at `/var/lib/kubelet/config.yaml`; we changed the
    kubelet ConfigMap named `kubelet-config` in the kube-system namespace; and then
    we upgraded the node and reloaded the kubelet daemon.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，为了完成这个任务，我们修改了`/etc/kubernetes/manifests/kube-apiserver.yaml`中的API配置；我们编辑了`kube-system`命名空间中的名为`kube-dns`的服务；我们修改了`/var/lib/kubelet/config.yaml`中的`kubeclet`配置；我们更改了`kube-system`命名空间中的名为`kubelet-config`的kubelet
    ConfigMap；然后我们升级了节点并重新加载了kubelet守护进程。
- en: 6.2 CoreDNS
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 CoreDNS
- en: The magic behind CoreDNS is the ability to resolve domain names quickly so that
    Services can talk to each other, and more importantly, so applications (running
    in Pods) can talk to each other and communicate as requests arise. To solve this
    locally, you can add values to our `/etc/hosts` file for each Pod, but that’s
    not scalable. Instead, we have a central location (central to the cluster) where
    Pods can query a cumulative list of hostnames and IP addresses, as in figure 6.5.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: CoreDNS背后的魔法是能够快速解析域名，以便服务之间可以通信，更重要的是，以便应用程序（在Pod中运行）可以相互通信并在请求出现时进行通信。为了本地解决这个问题，您可以为每个Pod添加到`/etc/hosts`文件中的值，但这不是可扩展的。相反，我们有一个中心位置（对集群来说是中心的），Pod可以查询主机名和IP地址的累积列表，如图6.5所示。
- en: '![](../../OEBPS/Images/06-05.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-05.png)'
- en: Figure 6.5 CoreDNS provides a central place for the mapping of hostnames to
    IP addresses in the Kubernetes cluster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 CoreDNS为Kubernetes集群中主机名到IP地址的映射提供了一个中心位置。
- en: 6.2.1 Config files
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 配置文件
- en: 'The kubelet plays a special role in the Kubernetes DNS configuration. Kubelet
    is a Service that exists directly on each node. As we saw in section 6.1, we can
    start, stop, and restart that Service via `systemctl`. Kubelet is responsible
    for creating the CoreDNS Pod and injecting the configuration into that Pod. That
    configuration file is in the `/var/lib/kubelet/config.yaml` directory. This is
    a special kind of configuration file, just for the kubelet, and will include the
    authentication for the kubelet, the health Endpoints, and, most importantly, the
    cluster DNS. Fun fact: `kube-dns` built with SkyDNS used to be the primary DNS
    resolution in Kubernetes. The name of the Service remained even after CoreDNS
    replaced it. CoreDNS became a much more efficient and well-rounded option and
    replaced `kube-dns` in Kubernetes version 1.11.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: kubelet在Kubernetes DNS配置中扮演着特殊角色。Kubelet是一个直接存在于每个节点上的服务。正如我们在6.1节中看到的，我们可以通过`systemctl`启动、停止和重启该服务。kubelet负责创建CoreDNS
    Pod并将配置注入到该Pod中。该配置文件位于`/var/lib/kubelet/config.yaml`目录中。这是一个特殊的配置文件，仅用于kubelet，并将包括kubelet的认证、健康端点和最重要的集群DNS。有趣的事实：使用SkyDNS构建的`kube-dns`曾经是Kubernetes中的主要DNS解析。即使CoreDNS取代了它，服务的名称仍然保留。CoreDNS成为了一个更高效、更全面的选项，并在Kubernetes版本1.11中取代了`kube-dns`。
- en: EXAM TIP There’s a special directory in the Kubernetes cluster for static Pods.
    This location is `/etc/kubernetes/manifests/`, and anything in this directory
    will autoprovision without the scheduler’s awareness.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 考试提示：在Kubernetes集群中有一个专门用于静态Pod的特殊目录。这个位置是`/etc/kubernetes/manifests/`，并且这个目录中的任何内容都将自动配置，无需调度器的意识。
- en: There are two ConfigMaps—one for the kubelet and one for the CoreDNS—and each
    contains their respective applied configurations because the kubelet is responsible
    for creating new Pods in a Kubernetes cluster, and CoreDNS will be the DNS server.
    You can view these ConfigMaps with the command `k -n kube-system get cm`.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 有两个 ConfigMap——一个用于 kubelet，一个用于 CoreDNS——并且每个都包含它们各自应用过的配置，因为 kubelet 负责在 Kubernetes
    集群中创建新的 Pods，而 CoreDNS 将是 DNS 服务器。你可以使用命令 `k -n kube-system get cm` 查看这些 ConfigMap。
- en: 6.2.2 Replicating DNS
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 DNS 复制
- en: 'As you may have noticed in our cluster, there are two instances of CoreDNS
    running as Pods in the `kube-system` namespace. You can view them with the command
    `k -n kube-system get po`. The output will look something like this:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如你可能在我们的集群中注意到的，有两个 CoreDNS 实例作为 `kube-system` 命名空间中的 Pods 运行。你可以使用命令 `k -n
    kube-system get po` 查看它们。输出将类似于以下内容：
- en: '[PRE10]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'As you see, there are two replicas of the CoreDNS Pods running. This is the
    benefit of a Deployment, as you can easily scale them up and down to get faster
    DNS resolution. Nobody likes to wait a long time for DNS queries. You can view
    the CoreDNS Deployment with the command `k -n kube-system get deploy`. The output
    will look similar to this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，有两个 CoreDNS Pods 正在运行。这是 Deployment 的好处，你可以轻松地将其扩展和缩减以获得更快的 DNS 解析。没有人喜欢长时间等待
    DNS 查询。你可以使用命令 `k -n kube-system get deploy` 查看CoreDNS Deployment。输出将类似于以下内容：
- en: '[PRE11]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This Deployment is called *CoreDNS*, and it is similar to any other Deployment
    in Kubernetes. Let’s say that DNS queries are taking a long time and causing delays
    for our application, we can scale the CoreDNS Deployment with the command `k -n
    kube-system scale deploy coredns --replicas 3` and see that there are now three
    Pod replicas. The output will be similar to this:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Deployment 被称为 *CoreDNS*，它类似于 Kubernetes 中的任何其他 Deployment。假设 DNS 查询花费了很长时间并导致我们的应用程序延迟，我们可以使用命令
    `k -n kube-system scale deploy coredns --replicas 3` 缩放 CoreDNS Deployment，并看到现在有三个
    Pod 副本。输出将类似于以下内容：
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Scaling the DNS server replicas simply allows for greater performance when making
    DNS queries, which makes managing your DNS servers much easier.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放 DNS 服务器副本只是允许在执行 DNS 查询时获得更高的性能，这使得管理你的 DNS 服务器变得更加容易。
- en: 'If this solution didn’t exist, we would have to apply the hostname to the IP
    address manually for every Pod. Let’s simulate this by scaling the CoreDNS Deployment
    down to zero and then trying to communicate with a Service in the cluster. To
    scale the Deployment down, we would run the command `k -n kube-system scale deploy
    Coredns --replicas 0`. This would terminate all of the CoreDNS Pods currently
    running in the cluster, which would look like the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个解决方案不存在，我们就必须手动将主机名应用到每个 Pod 的 IP 地址上。让我们通过将 CoreDNS Deployment 缩放到零并尝试与集群中的服务进行通信来模拟这种情况。要缩减
    Deployment，我们将运行命令 `k -n kube-system scale deploy Coredns --replicas 0`。这将终止集群中当前运行的所有
    CoreDNS Pods，看起来如下所示：
- en: '[PRE13]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'DNS is not available in the cluster, so we won’t be able to resolve hostnames.
    Let’s create a Deployment along with a Service. We’ll talk about Services later
    in this chapter, but for now, we’ll use this Service to communicate with the underlying
    Pods in the Deployment. Create a Deployment and Service with the command `k create
    deploy apache --image httpd; k expose deploy apache --name apache-svc --port 80`.
    The output will look similar to this:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 集群中没有可用的 DNS，因此我们无法解析主机名。让我们创建一个 Deployment 和一个 Service。我们将在本章后面讨论 Service，但到目前为止，我们将使用这个
    Service 与 Deployment 中的底层 Pods 进行通信。使用命令 `k create deploy apache --image httpd;
    k expose deploy apache --name apache-svc --port 80` 创建 Deployment 和 Service。输出将类似于以下内容：
- en: '[PRE14]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Grab the IP address of the Service by running the command `k get svc`. The
    output will look like this, where the cluster IP address is 100.96.102.73 for
    me:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行命令 `k get svc` 获取服务的 IP 地址。输出将类似于以下内容，其中我的集群 IP 地址为 100.96.102.73：
- en: '[PRE15]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you still have the `netshoot` container running from the exercise earlier,
    let’s get a Bash shell to it. If you’ve deleted the `netshoot` container, you
    can restart it with the command `kubectl run netshoot --image=nicolaka/netshoot
    --command sleep --command "3600"`. Let’s get a Bash shell to it by running the
    command `k exec --it netshoot --bash`. You should now have a new prompt that looks
    something like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前练习中还有运行的 `netshoot` 容器，让我们获取它的 Bash shell。如果你已经删除了 `netshoot` 容器，你可以使用命令
    `kubectl run netshoot --image=nicolaka/netshoot --command sleep --command "3600"`
    重新启动它。让我们通过运行命令 `k exec --it netshoot --bash` 来获取它。你现在应该有一个新的提示符，看起来类似于以下内容：
- en: '[PRE16]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To verify that DNS is not working, let’s try to communicate with the Service
    that we just created, called `apache-svc`. We can do this with the command `wget
    -O- apache-svc`. You should get the following output:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证 DNS 是否工作，让我们尝试与我们刚刚创建的名为 `apache-svc` 的 Service 进行通信。我们可以使用命令 `wget -O-
    apache-svc` 来做这件事。你应该得到以下输出：
- en: '[PRE17]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'To fix this, we’ll add the hostname-to-IP mapping to the `/etc/hosts` file
    within the Pod. We can do this with a single command: `echo “100.96.102.73` `apache-svc”
    >> /etc/hosts`. Now let’s run the `wget` command again, and you should get the
    standard Apache homepage, which looks something like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们将主机名到 IP 的映射添加到 Pod 内部的 `/etc/hosts` 文件中。我们可以用一条命令来完成这个操作：`echo “100.96.102.73
    apache-svc” >> /etc/hosts`。现在让我们再次运行 `wget` 命令，你应该得到标准的 Apache 主页，看起来像这样：
- en: '[PRE18]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This worked because we wrote our own DNS locally to the Pod. As you can see,
    it would be hard to do on every Pod if you have hundreds of Pods, and as we know,
    Pods are ephemeral, so doing this for every Pod would be a hassle. I hope this
    exercise proved the importance of CoreDNS and also gave you a few tools to test
    communication to Services.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效，是因为我们在 Pod 上本地编写了自己的 DNS。正如你所见，如果你有数百个 Pod，这会很困难，而且我们知道，Pod 是短暂的，因此为每个
    Pod 做这件事会非常麻烦。我希望这个练习证明了 CoreDNS 的重要性，并给你提供了一些测试 Service 通信的工具。
- en: 6.2.3 Pod-to-Pod connectivity
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 Pod 之间的连接性
- en: Let’s look at how Pods communicate across namespaces so we can become more familiar
    with how DNS works inside a Kubernetes cluster. Create a similar Deployment and
    Service, but create it inside of a new namespace called `c01383`.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 Pod 是如何跨命名空间进行通信的，这样我们就可以更熟悉 Kubernetes 集群内部的 DNS 的工作方式。创建一个类似的 Deployment
    和 Service，但将其创建在名为 `c01383` 的新命名空间内。
- en: EXAM TIP These complex names for namespaces will appear a lot on the exam, so
    it’s good to practice using autocomplete as much as possible. To install autocomplete
    in kind, see appendix B.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 考试技巧：这些复杂的命名空间名称在考试中会经常出现，所以尽可能多地练习使用自动补全是个好主意。有关在 kind 中安装自动补全的说明，请参阅附录 B。
- en: 'First, let’s create a namespace with the command `k create ns c01383`. We can
    view the namespaces in our cluster with the command `k get ns`. You will see an
    output similar to this:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们使用命令 `k create ns c01383` 创建一个命名空间。我们可以使用命令 `k get ns` 来查看我们集群中的命名空间。你会看到一个类似于以下内容的输出：
- en: '[PRE19]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Once we’ve created the namespace `c01383`, we can create a Deployment and Service
    inside of that namespace with the command `k -n c01383 create deploy nginx --image
    nginx; k -n c01383 expose deploy nginx --name nginx-svc --port 80`. You can see
    the Deployment and the Service in the `c01383` namespace by typing `k -n c01383
    get deploy,svc`. The output will be similar to this:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们创建了命名空间 `c01383`，我们就可以使用命令 `k -n c01383 create deploy nginx --image nginx;
    k -n c01383 expose deploy nginx --name nginx-svc --port 80` 在该命名空间内创建一个 Deployment
    和 Service。你可以通过输入 `k -n c01383 get deploy,svc` 来查看 `c01383` 命名空间中的 Deployment
    和 Service。输出将类似于以下内容：
- en: '[PRE20]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Do you think if we connect to one of the Pods from the Deployment, we’ll be
    able to reach the Service named `apache-svc` in the default namespace? Let’s try!
    Type the command `k -n c01383 get po` to retrieve the Pod name. It will start
    with `nginx`, followed by a dash, then a unique value that represents the ReplicaSet,
    another dash, and then another unique value that represents the Deployment. This
    is best demonstrated by the output shown in figure 6.6.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 你认为如果我们从 Deployment 中的一个 Pod 连接到，我们能否到达默认命名空间中名为 `apache-svc` 的 Service？让我们试试！输入命令
    `k -n c01383 get po` 来检索 Pod 名称。它将以 `nginx` 开头，然后是一个连字符，接着是一个代表 ReplicaSet 的唯一值，然后是另一个连字符，最后是代表
    Deployment 的另一个唯一值。这最好通过图 6.6 中的输出来说明。
- en: '![](../../OEBPS/Images/06-06.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-06.png)'
- en: Figure 6.6 The Pod name comes from an auto-assigned unique identifier that prevents
    conflicting Pod names.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.6 Pod 名称来自一个自动分配的唯一标识符，这可以防止 Pod 名称冲突。
- en: 'Now that we have the name of the Pod, we can open a Bash shell to it with the
    command `k -n c01383 exec -it nginx-76d6c9b8c-8lqgg --bash`. You will see your
    prompt change, which means you are now looking at the filesystem of the container
    inside the Pod. From inside the container, run the command `curl apache-svc`.
    You should get the result “Could not resolve host,” and it will look like this:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了 Pod 的名称，我们可以使用命令 `k -n c01383 exec -it nginx-76d6c9b8c-8lqgg --bash`
    打开一个 Bash shell 到它。你会看到你的提示符发生变化，这意味着你现在正在查看 Pod 内部的文件系统。在容器内部，运行命令 `curl apache-svc`。你应该得到结果“无法解析主机”，并且看起来像这样：
- en: '[PRE21]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Let’s try to run the command `curl apache-svc.default`. It worked! You should
    see the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试运行命令 `curl apache-svc.default`。它成功了！你应该看到以下内容：
- en: '[PRE22]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This is because each Service is given its own unique domain name in Kubernetes,
    which is different between namespaces. The *fully qualified domain name (FQDN**)*
    for Services in a Kubernetes cluster is <`service-name>.<namespace-name>.svc.cluster
    .local`, as depicted in figure 6.7.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是因为 Kubernetes 为每个服务分配了其唯一的域名，不同命名空间之间的域名是不同的。Kubernetes 集群中服务的 *完全限定域名（FQDN**）
    是 `<service-name>.<namespace-name>.svc.cluster .local`，如图 6.7 所示。
- en: '![](../../OEBPS/Images/06-07.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-07.png)'
- en: Figure 6.7 The domain-name convention applied to every Service in the cluster,
    with an example
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.7 集群中每个服务的域名约定示例
- en: 'We saw this previously when we looked at the `resolv.conf` within the Pod back
    in section 6.1 of this chapter. Let’s look at it again from the shell that we’re
    already in with the command `cat /etc/resolv.conf`. The output should look similar
    to this:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本书第 6.1 节中查看 Pod 内的 `resolv.conf` 时已经看到了这个。让我们再次从我们已经在的 shell 中使用命令 `cat
    /etc/resolv.conf` 来查看它。输出应该类似于以下内容：
- en: '[PRE23]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The line that begins with `search` will indicate the DNS names that are associated
    with the Services in that namespace. You don’t see `default.svc.cluster.local`
    in this list, which is why when we tried the name `apache-svc`, the name would
    not resolve.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 以 `search` 开头的行将指示与该命名空间中的服务关联的 DNS 名称。在这个列表中你不会看到 `default.svc.cluster.local`，这就是为什么当我们尝试
    `apache-svc` 这个名称时，名称无法解析。
- en: 'Pods also have an FQDN, which is the IP address, but CoreDNS converts the dots
    into dashes, so it looks like this: `10-244-0-14.default.pod.cluster.local`, where
    10.244.0.14 is the IP address of the Pod within the default namespace. We can
    perform the same curl command but use the Pod FQDN like so:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Pods 也有一个全限定域名（FQDN），它就是 IP 地址，但 CoreDNS 会将点转换为破折号，因此看起来像这样：`10-244-0-14.default.pod.cluster.local`，其中
    10.244.0.14 是默认命名空间内 Pod 的 IP 地址。我们可以执行相同的 curl 命令，但使用 Pod FQDN，如下所示：
- en: '[PRE24]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We could have also performed the command `10-244-0-14.default.pod`, and DNS
    would have resolved it due to `cluster.local` being in the `resolv.conf` search
    criteria.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以执行命令 `10-244-0-14.default.pod`，由于 `cluster.local` 包含在 `resolv.conf` 的搜索条件中，DNS
    会解析它。
- en: Exam exercises
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 练习考试
- en: '`exec` into a Pod and `cat` out the DNS resolver file to see the IP address
    of the DNS server that the Pod uses to resolve domain names.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `exec` 命令进入 Pod，并使用 `cat` 命令查看 DNS 解析器文件，以获取 Pod 用于解析域名的 DNS 服务器 IP 地址。
- en: Open the file that contains the configuration for the kubelet and change the
    value for `clusterDNS` to `100.96.0.10`. Save and quit the file.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 打开包含 kubelet 配置的文件，并将 `clusterDNS` 的值更改为 `100.96.0.10`。保存并退出文件。
- en: Stop and reload the kubelet daemon. Verify that the Service is active and running.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 停止并重新加载 kubelet 守护进程。验证服务是否处于活动状态并正在运行。
- en: Locate the `kube-dns` Service. Try to edit the Service in place by changing
    the value of both `clusterIP` and `ClusterIPs` to `100.96.0.10`. When the values
    cannot be updated, force a replacement of the Service with the correct `kubectl`
    command-line argument.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 定位 `kube-dns` 服务。尝试就地编辑服务，将 `clusterIP` 和 `ClusterIPs` 的值更改为 `100.96.0.10`。当值无法更新时，使用正确的
    `kubectl` 命令行参数强制替换服务。
- en: Edit the ConfigMap that contains the kubelet configuration. Change the IP address
    value that is set for `clusterDNS` to `100.96.0.10`. Make sure to edit the resource
    without writing a new YAML file.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑包含 kubelet 配置的 ConfigMap。将 `clusterDNS` 设置的 IP 地址值更改为 `100.96.0.10`。确保编辑资源时不要编写新的
    YAML 文件。
- en: Scale the CoreDNS Deployment to three replicas. Verify that the Pods have been
    created as a part of that Deployment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 将 CoreDNS 部署扩展到三个副本。验证 Pod 是否作为该部署的一部分被创建。
- en: Test access from a Pod to a Service by first creating a Deployment with the
    `apache` image, followed by exposing that Deployment. Create a Pod from the `netshoot`
    image, and verify that you can reach the Service that you just created.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 通过首先创建一个使用 `apache` 镜像的 Deployment，然后公开该 Deployment 来测试从 Pod 到服务的访问。从 `netshoot`
    镜像创建一个 Pod，并验证你是否可以到达你刚刚创建的服务。
- en: Using the `netshoot` Pod created in the previous exercise, locate the Service
    in the default namespace by its DNS name. Use as few letters as possible for DNS
    search functionality.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 使用在之前练习中创建的 `netshoot` Pod，通过 DNS 名称在默认命名空间中定位服务。为了 DNS 搜索功能，尽可能使用最少的字母。
- en: 6.3 Ingress and Ingress controllers
  id: totrans-124
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 入口和入口控制器
- en: When it comes to performing application layer (layer-7) routing to your application
    running in Kubernetes, the term that’s used is *Ingress*. Let’s cover Ingress
    first, before Services, as it is a preferred approach to exposing your app along
    with a Service, and the exam objective states that you must know how to use Ingress
    and Ingress controllers. The reason it’s a preferred method is that Ingress provides
    a single gateway (only one entry) into the cluster and can route traffic to multiple
    Services using simple path-based routes. Along with an Ingress controller, you
    can set these routes in the Ingress resource, and it will be directed to each
    Service, as is depicted in figure 6.8.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到对运行在 Kubernetes 中的应用程序执行应用层（第 7 层）路由时，使用的术语是 *Ingress*。在我们讨论 Service 之前，让我们先了解
    Ingress，因为它是在 Service 的基础上暴露应用程序的首选方法，并且考试目标指出您必须知道如何使用 Ingress 和 Ingress 控制器。它之所以是首选方法，是因为
    Ingress 提供了一个单一的网关（只有一个入口）进入集群，并且可以使用基于路径的路由将流量路由到多个服务。与 Ingress 控制器一起，您可以在 Ingress
    资源中设置这些路由，并且它们将被引导到每个服务，如图 6.8 所示。
- en: '![](../../OEBPS/Images/06-08.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/06-08.png)'
- en: Figure 6.8 Ingress flow of traffic and the redirection to multiple Services
    based on the path rules
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.8 展示了 Ingress 流量流向以及基于路径规则的多个服务的重定向
- en: You will be tested on creating Ingress and an Ingress controller, so let’s review
    what an exam task might look like.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您将接受创建 Ingress 和 Ingress 控制器的测试，因此让我们回顾一下考试任务可能的样子。
- en: '| Exam Task In cluster `ik8s`, install an Ingress controller to proxy communication
    into the cluster via an Ingress resource. Then, create a Deployment named `hello`
    using the image `nginxdemos/hello:plain-text`. The container is exposed on port
    80\. Create a Service named `hello-svc` that targets the `hello` Deployment on
    port 80\. Then, create an Ingress resource that will allow you to resolve the
    DNS name `hello.com` to the ClusterIP Service named `hello-svc` in Kubernetes.
    |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 在集群 `ik8s` 中，安装一个 Ingress 控制器以通过 Ingress 资源代理集群内的通信。然后，使用镜像 `nginxdemos/hello:plain-text`
    创建一个名为 `hello` 的 Deployment。容器在端口 80 上暴露。创建一个名为 `hello-svc` 的 Service，该 Service
    靶向 `hello` Deployment 的端口 80。然后，创建一个 Ingress 资源，这将允许您将 DNS 名称 `hello.com` 解析到
    Kubernetes 中名为 `hello-svc` 的 ClusterIP 服务。|'
- en: To complete these exam tasks, we’ll have to create a new kind cluster. See appendix
    A, section A.3, for instructions on how to build a single-node kind cluster with
    additional ports exposed and labels applied on the nodes. As soon as you have
    access to your kind cluster, get a shell to the control plane node with the command
    `docker exec -it ingress-control-plane bash`. Once you have a shell, set your
    alias `k` to equal `kubectl` with the command `alias k=kubectl`. On the exam,
    they will already have this alias set for you, so it’s good practice to use `k`
    as opposed to having to type `kubectl` over and over again.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 要完成这些考试任务，我们需要创建一个新的 kind 集群。请参阅附录 A，第 A.3 节，了解如何构建具有额外端口暴露和节点上应用标签的单节点 kind
    集群。一旦您有权访问您的 kind 集群，使用命令 `docker exec -it ingress-control-plane bash` 获取控制平面节点的
    shell。一旦您有了 shell，使用命令 `alias k=kubectl` 将您的别名 `k` 设置为等于 `kubectl`。在考试中，他们已经为您设置了此别名，因此使用
    `k` 而不是反复输入 `kubectl` 是一个好习惯。
- en: 'Now that we’ve built the cluster and have a shell to the control plane node,
    let’s begin the process of installing the Ingress controller, which will be a
    similar process on the exam. The Ingress controller YAML creates several resources.
    You can apply all of these resources at once with the command `k apply -f https://raw.githubusercontent
    .com/chadmcrowell/acing-the-cka-exam/main/ch_06/nginx-ingress-controller.yaml`.
    In the output of this command, the resources that have been created in the cluster
    will look similar to this:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经构建了集群并且有控制平面节点的 shell，让我们开始安装 Ingress 控制器的过程，这在考试中也是一个类似的过程。Ingress 控制器
    YAML 创建了几个资源。您可以使用命令 `k apply -f https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/nginx-ingress-controller.yaml`
    一次性应用所有这些资源。此命令的输出中，在集群中创建的资源将类似于以下内容：
- en: '[PRE25]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The Kubernetes objects have been created in their own namespace, called `ingress-nginx`,
    and you can view these resources all at once using the command `k -n ingress-nginx
    get all`. The output will look similar to this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 对象已在名为 `ingress-nginx` 的单独命名空间中创建，您可以使用命令 `k -n ingress-nginx get
    all` 一次性查看这些资源。输出将类似于以下内容：
- en: '[PRE26]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Verify that the Pod with the name that starts with `ingress-nginx-controller`
    is running. This means that you are ready to proceed and create a Deployment,
    a Service, and an Ingress.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 验证以 `ingress-nginx-controller` 开头的 Pod 是否正在运行。这意味着您已准备好继续并创建 Deployment、Service
    和 Ingress。
- en: '[PRE27]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can create a Service by exposing the Deployment with the command `k
    expose deploy hello -name hello-svc -port 80`. The output will look like the following:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过命令 `k expose deploy hello -name hello-svc -port 80` 来通过暴露 Deployment
    创建一个 Service。输出将如下所示：
- en: '[PRE28]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'You will now see the following when you type `k get deploy,svc`:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 当你输入 `k get deploy,svc` 时，你现在将看到以下内容：
- en: '[PRE29]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Let’s finish the exam task by creating an Ingress resource. Type the command
    `k apply -f` `https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/
    ch_06/hello-ingress.yaml` to create the Ingress resource. You will see the following
    output after you perform this command:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过创建 Ingress 资源来完成考试任务。输入命令 `k apply -f https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/hello-ingress.yaml`
    来创建 Ingress 资源。执行此命令后，你将看到以下输出：
- en: '[PRE30]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'After about 20 seconds, you will see the Ingress resource by typing the command
    `k get ing`, like so:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 大约20秒后，通过输入命令 `k get ing`，你将看到 Ingress 资源，如下所示：
- en: '[PRE31]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Finally, let’s test our setup by adding `hello.com` to our `/etc/hosts` file.
    Open the `hosts` file by typing `vim /etc/hosts` (you may need to run `apt update;
    apt install -y vim` first to install Vim). Type the IP address of the control
    plane node, which you can see next to the word `localhost` in the `hosts` file,
    followed by `hello.com`. The file will look similar to the `hosts` file contents
    in figure 6.9.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过将 `hello.com` 添加到我们的 `/etc/hosts` 文件中来测试我们的设置。通过输入 `vim /etc/hosts`
    打开 `hosts` 文件（你可能需要先运行 `apt update; apt install -y vim` 来安装 Vim）。在 `hosts` 文件中，输入控制平面节点的IP地址，你可以看到它位于
    `localhost` 旁边，然后输入 `hello.com`。文件将类似于图6.9中的 `hosts` 文件内容。
- en: '![](../../OEBPS/Images/06-09.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-09.png)'
- en: Figure 6.9 Adding a host entry to resolve the hostname `hello.com` to `172.18.0.2`
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 添加主机条目以解析主机名 `hello.com` 到 `172.18.0.2`
- en: 'Once you have saved this file, perform the command `curl hello.com`. You will
    get the following output, which will be the output of the hello application that
    we deployed earlier:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 保存此文件后，执行命令 `curl hello.com`。你将得到以下输出，这是之前部署的 hello 应用的输出：
- en: '[PRE32]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The output is the response that you get from the Pod, so the server address
    will match the IP address of the Pod. You can see that if you type the command
    `k get po -o wide`.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是从 Pod 获得的响应，因此服务器地址将与 Pod 的IP地址匹配。你可以通过输入命令 `k get po -o wide` 看到这一点。
- en: That completes our exam task of installing an Ingress controller and creating
    a Deployment named `hello`, a Service named `hello-svc`, and an Ingress resource
    that resolves `hello.com` to the `hello-svc` Service.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了我们的考试任务，安装了 Ingress 控制器，创建了一个名为 `hello` 的 Deployment，一个名为 `hello-svc` 的
    Service，以及一个将 `hello.com` 解析到 `hello-svc` Service 的 Ingress 资源。
- en: You’ll see from the exam task that we simply typed `curl hello.com`, and it
    magically routed us to the correct Service and, in turn, the correct Pod. This
    magic is performed by the admission controller. An Ingress controller is software
    that intercepts requests to the Kubernetes API. In our case, it’s a Pod running
    in the `ingress-nginx` namespace. This Pod intercepted the request that we made
    to `hello.com` and rerouted the request to the hello app with assistance from
    the Ingress resource. This was just a simple example, but we could have also added
    a path to the URL (e.g., `hello.com/app`) and directed the request to a different
    Service. This Service will be a ClusterIP Service, because the Ingress controller
    is already exposed to the outside, helping the traffic into the cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 从考试任务中我们可以看到，我们只是简单地输入了 `curl hello.com`，它神奇地将我们路由到正确的 Service，进而路由到正确的 Pod。这个魔法是由准入控制器执行的。Ingress
    控制器是一种拦截对 Kubernetes API 请求的软件。在我们的例子中，它是在 `ingress-nginx` 命名空间中运行的 Pod。这个 Pod
    拦截了我们向 `hello.com` 发出的请求，并在 Ingress 资源的帮助下将请求重定向到 hello 应用。这只是一个简单的例子，但我们也可以向
    URL 添加路径（例如，`hello.com/app`）并将请求重定向到不同的 Service。这个 Service 将是一个 ClusterIP Service，因为
    Ingress 控制器已经暴露在外部，帮助集群中的流量进入。
- en: Let’s go through the Ingress YAML and modify the Ingress resource. You can edit
    the Ingress resource with the command `k edit ingress hello`. The YAML will now
    appear in the Vim editor. Starting from the top, as you can see in figure 6.10,
    the host is the domain where clients will be accessing your application (hello.com).
    This could be a domain name that you’ve purchased from a domain registrar, or
    this could be something local to the cluster, as we have chosen. The HTTP section
    and the following text define the rules for resolving HTTP (port 80) traffic.
    The rule states that any request made to the host hello.com will be directed to
    the `hello-svc` Service on port 80\. This is the default route because an explicit
    path is not specified (e.g., `hello .com/path`). To add another rule—let’s say,
    to take requests from `hello.com/app` to a different Service named `nginx` over
    port 8080—we’d add another block under `path:` to specify that rule. The result
    will look similar to figure 6.10.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们遍历 Ingress YAML 并修改 Ingress 资源。您可以使用命令 `k edit ingress hello` 编辑 Ingress
    资源。现在 YAML 将会出现在 Vim 编辑器中。从顶部开始，如您在图 6.10 中所见，主机是客户端将访问您的应用程序（hello.com）的域名。这可能是一个您从域名注册商那里购买的域名，或者这可能是在集群本地的某个东西，正如我们选择的。HTTP
    部分以及随后的文本定义了解析 HTTP（端口 80）流量的规则。规则说明，对 hello.com 主机发出的任何请求都将被导向端口 80 上的 `hello-svc`
    服务。这是默认路由，因为没有指定显式路径（例如，`hello.com/path`）。要添加另一个规则——比如说，将来自 `hello.com/app` 的请求导向名为
    `nginx` 的不同服务，通过端口 8080——我们将在 `path:` 下添加另一个块来指定该规则。结果将类似于图 6.10。
- en: '![](../../OEBPS/Images/06-10.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.10 添加 Ingress 规则，路由到名为 `nginx` 的不同服务](../../OEBPS/Images/06-10.png)'
- en: Figure 6.10 Adding a rule to Ingress, routing to a different Service named `nginx`
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 添加 Ingress 规则，路由到名为 `nginx` 的不同服务
- en: In figure 6.10, the Ingress rules show routes to two different Services. Those
    Services are defined as *backends*, meaning once the traffic enters the Ingress,
    it will enter the backend (what happens behind the scenes). In this case, two
    backend routes define the specifics of the individual Services to which they route.
    The first goes to a Service named `hello-svc` and uses port 80\. This requires
    us to have a ClusterIP Service in our cluster that is exposed on port 80\. The
    second route is backend to a Service named `nginx`. The client would have to type
    `http://hello.com/app` to route to this Service. You will also see a `pathType`,
    as each Ingress path is required to have.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 6.10 中，Ingress 规则显示了路由到两个不同服务的路由。这些服务被定义为 *后端*，这意味着一旦流量进入 Ingress，它将进入后端（幕后发生的事情）。在这种情况下，两个后端路由定义了它们各自路由的个别服务的具体细节。第一个路由到名为
    `hello-svc` 的服务并使用端口 80。这要求我们在集群中有一个暴露在端口 80 上的 ClusterIP 服务。第二个路由是名为 `nginx`
    的服务的后端。客户端必须输入 `http://hello.com/app` 才能路由到这个服务。您还会看到一个 `pathType`，因为每个 Ingress
    路径都需要。
- en: There are three supported path types—`Exact`, `Prefix`, and `ImplementationSpecific`.
    The `Exact` path type has to match the path exactly (case-sensitive) for the path
    to be valid (e.g., `hello.com/app`). The `Prefix` path type matches the URL path
    split by a forward slash (`/`), where the characters between the slash are the
    element’s prefix. The `ImplementationSpecific` path type leaves it up to the `Ingress`
    class to decide a prefix or an exact path type.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 支持三种路径类型——`Exact`、`Prefix` 和 `ImplementationSpecific`。`Exact` 路径类型必须与路径完全匹配（区分大小写），路径才有效（例如，`hello.com/app`）。`Prefix`
    路径类型匹配由正斜杠（`/`）分隔的 URL 路径，斜杠之间的字符是元素的名称前缀。`ImplementationSpecific` 路径类型将决定前缀或精确路径类型留给
    `Ingress` 类来决定。
- en: 'In the YAML for an Ingress resource, you don’t have to only choose path-based
    routing (`hello.com/app`); you can also choose subdomain-type routing. For example,
    you can choose a different backend when the client types `http://app.hello.com`,
    which may be more flexible depending on the type of application you have. The
    YAML would then change to something like this:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Ingress 资源的 YAML 中，您不仅可以选择基于路径的路由（`hello.com/app`），还可以选择子域名类型的路由。例如，当客户端输入
    `http://app.hello.com` 时，您可以选择不同的后端，这可能会根据您拥有的应用程序类型更加灵活。然后 YAML 将变为类似以下内容：
- en: '[PRE33]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 6.4 Services
  id: totrans-160
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 服务
- en: Pods are ephemeral by nature and are meant to be killed and respawned at a moment’s
    notice. As a result, their IP addresses are constantly changing. Services are
    a way for the traffic to always reach the correct Pod, whether that Pod has been
    moved to a new node, killed, or scaled. Services provide a distributed load balancer
    for the Pods, and they evenly distribute the load between multiple Pods within
    the same Deployment and also help with detecting exactly where those Pods are
    in the cluster, as depicted in figure 6.11.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Pods本质上是短暂的，并且可以在任何时候被杀死并重新启动。因此，它们的IP地址不断变化。Service提供了一种让流量始终到达正确的Pod的方法，无论该Pod是否被移动到新的节点、被杀死或进行扩展。Service为Pods提供了一个分布式负载均衡器，它们在同一个Deployment内的多个Pod之间均匀分配负载，并有助于检测这些Pod在集群中的确切位置，如图6.11所示。
- en: '![](../../OEBPS/Images/06-11.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-11.png)'
- en: Figure 6.11 A Service accepts incoming traffic on behalf of each Pod, load balancing
    the traffic.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 Service代表每个Pod接受进入的流量，进行流量均衡。
- en: 'Services contain a consistent IP address and DNS name for the Pod or set of
    Pods. This consistent address helps maintain existing connections and manage the
    routing of traffic as Pods come and go. Pods can communicate with each other via
    Services, all within the cluster, so no matter what node the Pod is running on,
    it will be able to locate that Pod as a part of that cluster-wide Service. Endpoints
    are the exposed ports on Pods that associate with the Pods and represent a target
    where the Pods can be reached. We can see a list of Endpoints, which take on the
    same name of the Service, by typing the command `k get ep`. The output will look
    similar to this:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: Service为Pod或Pod集提供了一个一致的IP地址和DNS名称。这个一致的地址有助于保持现有连接并管理Pod来去时的流量路由。Pods可以通过Service在集群内相互通信，无论Pod运行在哪个节点上，它都将能够作为集群范围内的Service的一部分定位到该Pod。端点是Pod上暴露的端口，与Pod相关联，并代表Pod可以到达的目标。通过输入命令`k
    get ep`，我们可以看到端点列表，它们具有与Service相同的名称。输出将类似于以下内容：
- en: '[PRE34]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The types of Services are ClusterIP, NodePort, and LoadBalancer. ClusterIP Services
    are designed to make Pods internally available within the cluster only, whereas
    NodePort- and LoadBalancer-type Services are designed to expose a port and create
    outside access to the cluster. NodePort exposes a port on the node’s IP address,
    but LoadBalancer allows an external load balancer (outside the cluster) to control
    the traffic into each node port. This allows you to use more common ports (80,443)
    instead of the NodePort restriction of ports 30,000–32,768\. You’ll notice when
    you create a NodePort Service that you are extending a ClusterIP-type Service
    at the same time, as depicted in figure 6.12.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: Service的类型有ClusterIP、NodePort和LoadBalancer。ClusterIP Service旨在仅在集群内部使Pod可用，而NodePort和LoadBalancer类型的Service旨在暴露一个端口并创建对集群的外部访问。NodePort在节点的IP地址上暴露一个端口，但LoadBalancer允许外部负载均衡器（集群外部）控制每个节点端口的流量。这允许你使用更常见的端口（80、443）而不是NodePort限制的端口30,000-32,768。当你创建NodePort
    Service时，你实际上是在扩展一个ClusterIP类型的Service，如图6.12所示。
- en: '![](../../OEBPS/Images/06-12.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-12.png)'
- en: Figure 6.12 A LoadBalancer-type Service, which is a superset of the NodePort-
    and ClusterIP-type Services, provides a single entry point into the cluster and
    the underlying Pods.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 一个负载均衡器类型的Service，它是NodePort和ClusterIP类型Service的超集，提供了一个进入集群和底层Pods的单一点。
- en: 6.4.1 ClusterIP Service
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 ClusterIP Service
- en: The ClusterIP Service is for internal cluster communication only. You would
    usually create a Service like this to communicate from the frontend of your application,
    in one Pod in the cluster, to the backend, which is in a different Pod. You can
    specify the ports to use when you create the Service initially, or you can modify
    it later. Like a lot of other resources in Kubernetes, you can run an imperative
    command to create a Service like `k create svc clusterip internal-svc --tcp 8080:80`,
    or you can create a YAML spec file with the command `k create svc clusterip internal-svc
    --tcp 8080:80 -dry-run=client -o yaml > svc.yaml`. Go ahead and run the latter
    command and take a look at the YAML spec by opening in it Vim with the command
    `vim svc.yaml`. You’ll see, as in figure 6.13, in the ports section, that there’s
    a port and a target port. The target port is the port that’s exposed on the container
    itself. Not all containers have exposed ports, but in the case of the `nginx`
    image, for example, port 80 is exposed.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 集群IP服务仅用于集群内部通信。你通常会创建这样的服务来从应用程序的前端，在集群中的一个Pod中，与后端通信，后端位于不同的Pod中。你可以在创建服务时指定要使用的端口，或者你可以在之后修改它。像Kubernetes中的许多其他资源一样，你可以运行一个
    imperative 命令来创建服务，例如`k create svc clusterip internal-svc --tcp 8080:80`，或者你可以使用命令`k
    create svc clusterip internal-svc --tcp 8080:80 -dry-run=client -o yaml > svc.yaml`创建一个YAML规范文件。现在运行后者命令，并通过使用命令`vim
    svc.yaml`在Vim中打开它来查看YAML规范。你会发现，如图6.13所示，在端口部分，有一个端口和一个目标端口。目标端口是容器本身上暴露的端口。并非所有容器都有暴露的端口，但以`nginx`镜像为例，端口80是暴露的。
- en: The exposed port is defined in the Dockerfile (how the image is built), so if
    you’re ever wondering what ports the container is exposed on, look at the Dockerfile
    for that image. Going back to the Service YAML, the port is exposed within the
    cluster, as in figure 6.12 where the port is 80\. You can have multiple Services
    with the same port exposed because each Service has its own IP address and DNS
    name, which allows an incoming request to differentiate between Services. As long
    as the Service name is different, you can have as many Services as you want with
    port 80 exposed. Notice in the YAML spec that this Service has a selector, as
    in figure 6.13\. This is how the Service can *bind* itself to Pods. Any Pod with
    the label `app=internal-svc``,` in this case, will be associated with that Service.
    This works similarly to `nodeSelector`s, which we touched on in section 4.1.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 暴露的端口在Dockerfile中定义（镜像是如何构建的），所以如果你想知道容器暴露了哪些端口，请查看该镜像的Dockerfile。回到服务YAML，端口在集群内部暴露，如图6.12所示，端口是80。你可以有多个具有相同暴露端口的服务的服务，因为每个服务都有自己的IP地址和DNS名称，这允许传入请求区分不同的服务。只要服务名称不同，你就可以有任意多的服务，其端口80被暴露。注意在YAML规范中，这个服务有一个选择器，如图6.13所示。这就是服务如何*绑定*到Pods的方式。任何带有标签`app=internal-svc`的Pod，在这种情况下，都将与该服务相关联。这与我们在第4.1节中提到的`nodeSelector`类似。
- en: '![](../../OEBPS/Images/06-13.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-13.png)'
- en: Figure 6.13 YAML spec for a Service
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 服务YAML规范
- en: 'Let’s say that we want to expose another port within this Service; we could
    do that as well. Copy everything just below the word `ports` in the YAML file,
    and paste it just below the word `targetPort`. We can now change the name to `search`;
    the port and the target port can be 9200\. The result should be similar to this:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要在这个服务中暴露另一个端口；我们也可以这样做。复制YAML文件中“ports”一词下面的所有内容，并将其粘贴到“targetPort”一词下面。现在我们可以将名称更改为“search”；端口和目标端口可以是9200。结果应该类似于以下内容：
- en: '[PRE35]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This resembles an elastic search Pod that may be running in your cluster. If
    a client reaches out to this Service, both Pods that have port 80 exposed—as well
    as the Pods that have port 9200 exposed—can be reached by accessing just one Service.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这类似于可能运行在你的集群中的Elasticsearch Pod。如果客户端连接到这个服务，那么具有端口80暴露的Pod以及具有端口9200暴露的Pod都可以通过访问一个服务来访问。
- en: 'It is possible to bypass the Service altogether and go directly to the Pod.
    This is called a *headless Service*. To demonstrate this, let’s add a line to
    our YAML spec. Just below the spec, let’s insert the line `clusterIP: none`. The
    result should look like the following:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: '可以完全绕过服务，直接访问Pod。这被称为*无头服务*。为了演示这一点，让我们在我们的YAML规范中添加一行。在规范下方，让我们插入行“clusterIP:
    none”。结果应该看起来像以下内容：'
- en: '[PRE36]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This will not only remove the IP address from the Service, but when DNS is used
    to look up the Pod via its Service, the communication will go directly to the
    Pod. This is needed in the case of a database cluster, where only one of the database
    copies is responsible for writing to the database, and all others are only allowed
    to read. The client can easily look up the Pods that are associated with the headless
    Service and determine which Pod is responsible for writing to the database.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 这不仅会从服务中移除 IP 地址，而且当使用 DNS 通过服务查找 Pod 时，通信将直接发送到 Pod。在数据库集群的情况下，这是必需的，因为只有一个数据库副本负责写入数据库，而其他所有副本只允许读取。客户端可以轻松地查找与无头服务关联的
    Pods，并确定哪个 Pod 负责写入数据库。
- en: 'Let’s create this headless multiport Service with the command `k create -f
    svc.yaml`. The Service named `internal-svc` does not have a cluster IP like the
    others do. Also, there are two groups of ports—8080 and 9200\. The output will
    look similar to this:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用命令 `k create -f svc.yaml` 创建这个无头多端口服务。名为 `internal-svc` 的服务没有像其他服务那样的集群
    IP。此外，还有两组端口——8080 和 9200。输出将类似于以下内容：
- en: '[PRE37]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 6.4.2 NodePort Service
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 NodePort 服务
- en: A NodePort Service is not only able to communicate internally to all other cluster
    components but can also expose a static port on the node that can be used for
    external traffic. This is useful for testing the communication to a Pod but comes
    with a certain number of limitations. You will have to know the IP address of
    the node, and you will also have to know which port is exposed because the available
    port range for a NodePort Service is 30000–32768.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务不仅能够与集群中的所有其他组件进行内部通信，还可以在节点上公开一个静态端口，用于外部流量。这对于测试与 Pod 的通信很有用，但也有一些限制。您将需要知道节点的
    IP 地址，并且您还必须知道哪个端口被公开，因为 NodePort 服务的可用端口范围是 30000–32768。
- en: 'As with the ClusterIP Service type and many other resources in Kubernetes,
    you can create a Service by performing the imperative command `k create svc nodeport
    no --node-port 30000 --tcp 443:80`, or you can create a declarative YAML file
    with the command `k create svc nodeport no --node-port 30000 --tcp 443:80 --dry-run=client
    -o yaml > nodeport.yaml`. Let’s open the YAML in Vim and look at how this is different
    than the ClusterIP Service. To open the file `nodeport.yaml`, type `vim nodeport.yaml`.
    You’ll see that it’s very similar, but instead of the type being `ClusterIP`,
    it’s `NodePort`, and it has a line added to the port name section. The line `nodePort:
    30000` has been added, which is the static port number that will be exposed on
    each node in the cluster, as in the YAML displayed in figure 6.14.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '就像集群 IP 服务类型和 Kubernetes 中的许多其他资源一样，您可以通过执行命令 `k create svc nodeport no --node-port
    30000 --tcp 443:80` 来创建一个服务，或者您可以通过命令 `k create svc nodeport no --node-port 30000
    --tcp 443:80 --dry-run=client -o yaml > nodeport.yaml` 创建一个声明性的 YAML 文件。让我们在 Vim
    中打开 YAML 文件，看看这与集群 IP 服务有何不同。要打开文件 `nodeport.yaml`，请输入 `vim nodeport.yaml`。您会看到它们非常相似，但类型不是
    `ClusterIP`，而是 `NodePort`，并且在端口名称部分增加了一行。添加了 `nodePort: 30000` 这行，这是将在集群中的每个节点上公开的静态端口号，就像图
    6.14 中显示的 YAML 那样。'
- en: '![](../../OEBPS/Images/06-14.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-14.png)'
- en: Figure 6.14 YAML that creates a NodePort Service
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.14 创建 NodePort 服务的 YAML
- en: 'Just like in ClusterIP Services, we can have multiple ports and target ports
    assigned to this Service, but there is no option for headless Service. This is
    good because if the node is exposed, another user can gain an entryway directly
    into your Pod. Let’s create a NodePort Service by typing the command `k apply
    -f nodeport.yaml`, which will have an output similar to the following:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在集群 IP 服务中一样，我们可以为这个服务分配多个端口和目标端口，但没有为无头服务提供选项。这是好事，因为如果节点被暴露，其他用户可以直接进入您的
    Pod。让我们通过输入命令 `k apply -f nodeport.yaml` 创建一个 NodePort 服务，其输出将类似于以下内容：
- en: '[PRE38]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The NodePort Service has an additional parameter under ports that signifies
    the specific node port used out of 30,000\. The Pods will be available at `100.96.95.252:30000`
    on the node through the Service named `no`.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: NodePort 服务在端口下有一个额外的参数，表示从 30,000 个端口中使用的特定节点端口。Pods 将通过名为 `no` 的服务在节点的 `100.96.95.252:30000`
    上可用。
- en: 6.4.3 LoadBalancer Service
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 负载均衡器服务
- en: A LoadBalancer Service is just as it sounds—a load-balancing device that is
    either provisioned in the cloud or a bare metal device. For the CKA exam, you
    will not have to worry about the actual load balancer device; you will just have
    to know how to create a LoadBalancer Service and the associated YAML. Let’s do
    just that! Type the command `k create svc loadbalancer lb-svc --tcp 8080:8080
    --dry-run=client -o yaml > lb-svc.yaml`. Open the file by typing the command `vim
    lb-svc.yaml`. Look at the differences between this LoadBalancer Service and the
    NodePort and ClusterIP Services displayed in figure 6.15.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器服务正如其名——一个在云中或裸机设备上配置的负载均衡设备。对于CKA考试，你不必担心实际的负载均衡器设备；你只需要知道如何创建负载均衡器服务及其相关的YAML。让我们就这样做！输入命令`k
    create svc loadbalancer lb-svc --tcp 8080:8080 --dry-run=client -o yaml > lb-svc.yaml`。通过输入命令`vim
    lb-svc.yaml`打开文件。查看这个负载均衡器服务与图6.15中显示的NodePort和ClusterIP服务之间的差异。
- en: '![](../../OEBPS/Images/06-15.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-15.png)'
- en: Figure 6.15 YAML that creates a LoadBalancer Service
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 创建负载均衡器服务的YAML
- en: You’ll notice that there’s not much difference between the YAML for a LoadBalancer
    Service and a ClusterIP Service. The only difference is that the type is set to
    `LoadBalancer`. This means that the node port doesn’t have to be used to access
    the application, which therefore makes it slightly easier with which to communicate.
    There is an option to add a NodePort to the YAML here, which means that the port
    is still exposed, but the node port will not be exposed as a method to access
    the application.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到负载均衡器服务的YAML与ClusterIP服务的YAML之间没有太大的区别。唯一的区别是类型被设置为`LoadBalancer`。这意味着不需要使用节点端口来访问应用程序，这因此使得通信稍微容易一些。这里有一个选项可以将NodePort添加到YAML中，这意味着端口仍然被暴露，但节点端口不会作为访问应用程序的方法被暴露。
- en: 'Before we create a LoadBalancer Service, let’s create a load balancer in our
    kind cluster. One load balancer that works well with kind is MetalLB. You may
    remember installing the MetalLB load balancer with Helm in chapter 4\. Let’s deploy
    it again, but this time we’ll install it via a YAML manifest. Run the command
    `k apply -f https:// raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/metallb-native.yaml`
    to install the MetalLB load balancer. Next, we need to configure the load balancer
    for your cluster. To do this, type `k get no -o wide` to find the IP address of
    your nodes. The output will look similar to the following (but with different
    IP addresses):'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们创建负载均衡器服务之前，让我们在我们的kind集群中创建一个负载均衡器。一个与kind配合得很好的负载均衡器是MetalLB。你可能还记得在第4章中用Helm安装了MetalLB负载均衡器。现在我们再次部署它，但这次我们将通过YAML清单来安装。运行命令`k
    apply -f https:// raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/metallb-native.yaml`来安装MetalLB负载均衡器。接下来，我们需要为你的集群配置负载均衡器。为此，输入`k
    get no -o wide`以找到你的节点IP地址。输出将类似于以下内容（但IP地址不同）：
- en: '[PRE39]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Under the `Internal-IP` column, copy the first two octets (`172.18` for me),
    because you will change the values for the next file, which you can download with
    the command `curl -O https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/metallb-layer2-config.yaml`.
    Once you’ve downloaded the file, open it with the command `vim metallb-layer2-config.yaml`.
    Change the value just below the addresses to match what you found your node IP
    addresses to be. The output should be as follows:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在`Internal-IP`列下，复制前两个八位字节（对我来说是`172.18`），因为你将更改下一个文件的值，你可以使用命令`curl -O https://raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/metallb-layer2-config.yaml`下载该文件。一旦你下载了文件，使用命令`vim
    metallb-layer2-config.yaml`打开它。将地址下面的值更改为与你的节点IP地址匹配。输出应该如下所示：
- en: '[PRE40]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'In the file `metallb-layer2-config.yaml`, I changed the addresses to `172.18.255.1-172.18.255.50`
    to match the same first two octets from my node IP addresses. Now we can apply
    the YAML with the command `k create -f metallb-layer2-config.yaml`. You should
    see the following output:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在文件`metallb-layer2-config.yaml`中，我将地址更改为`172.18.255.1-172.18.255.50`以匹配我的节点IP地址的前两个八位字节。现在我们可以使用命令`k
    create -f metallb-layer2-config.yaml`应用YAML。你应该会看到以下输出：
- en: '[PRE41]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Now we can create our LoadBalancer Service. Instead of creating a new one, let’s
    modify the existing Service named `apache-svc` and change it from a ClusterIP
    Service to a LoadBalancer Service. To do this, type the command `k edit svc apache-svc`,
    which will open the YAML spec in Vim. Scroll down to the line starting with the
    word `type` and change `ClusterIP` to `LoadBalancer` (case-sensitive). Don’t forget
    to press the I key on the keyboard to enter insert mode!
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以创建我们的LoadBalancer服务。不是创建一个新的，而是修改现有的名为`apache-svc`的服务，将其从ClusterIP服务更改为LoadBalancer服务。为此，输入命令`k
    edit svc apache-svc`，这将打开Vim中的YAML规范。向下滚动到以单词`type`开头的行，将`ClusterIP`更改为`LoadBalancer`（区分大小写）。别忘了按键盘上的I键进入插入模式！
- en: 'Once you’ve made that change, press the Esc key on the keyboard to get out
    of insert mode and type `:wq` to save the file and quit. This will apply the changes.
    You can now perform the command `k get svc` and see the following:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您进行了更改，请按键盘上的Esc键退出插入模式，并输入`:wq`以保存文件并退出。这将应用更改。现在您可以执行命令`k get svc`并看到以下内容：
- en: '[PRE42]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The type changed to `LoadBalancer`, and now there’s an IP address in the `External-IP`
    column. This is the IP address of the MetalLB load balancer, which we can use
    to access our application (running in a Pod). Type `curl 172.18.255.1` and you
    should get the following result:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 类型已更改为`LoadBalancer`，现在在`External-IP`列中有一个IP地址。这是MetalLB负载均衡器的IP地址，我们可以使用它来访问我们的应用程序（在Pod中运行）。输入`curl
    172.18.255.1`，您应该得到以下结果：
- en: '[PRE43]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 6.5 Cluster node networking configuration
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 集群节点网络配置
- en: When trying to communicate within a Kubernetes cluster, it’s not much different
    than communicating from one server to another, with some added capabilities. These
    capabilities are generally called *network overlays*, and they are essentially
    network abstractions on top of the network you already have. Why is this? Because
    every Pod in the Kubernetes cluster requires its own IP address, you may not have
    enough IP addresses reserved for every Pod, especially as the cluster scales up
    to dozens of nodes and potentially hundreds of Pods. This overlay is sometimes
    referred to as a *VXLAN*.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当尝试在Kubernetes集群内部进行通信时，与从一台服务器到另一台服务器的通信并没有太大的区别，只是增加了一些功能。这些功能通常被称为*网络覆盖层*，它们是在您已有的网络之上的网络抽象。为什么是这样呢？因为Kubernetes集群中的每个Pod都需要自己的IP地址，您可能没有为每个Pod预留足够的IP地址，尤其是当集群扩展到数十个节点和可能数百个Pod时。这种覆盖层有时被称为*VXLAN*。
- en: A VXLAN sits on top of the existing physical network and uses an encapsulation
    protocol to tunnel layer-2 connections over a layer-3 network. As the communication
    goes back and forth from node to node, as in figure 6.16, when trying to reach
    a Pod, the encapsulation helps with routing to the correct destination by placing
    a header on the packet.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: VXLAN位于现有的物理网络之上，并使用封装协议在第三层网络上隧道第二层连接。当通信在节点之间来回传递时，如图6.16所示，当尝试到达一个Pod时，封装通过在数据包上放置一个头部来帮助进行正确的路由。
- en: '![](../../OEBPS/Images/06-16.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/06-16.png)'
- en: Figure 6.16 Encapsulated packet moving from node to node via CNI
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 通过CNI从节点到节点的封装数据包移动
- en: The overlay follows a set of specifications and libraries for writing plugins
    to configure network interfaces in Linux containers. The term for this overlay
    is the c*ontainer network interface (CNI**)*. CNI is a Cloud Native Computing
    Foundation (CNCF) project that is not exclusive to Kubernetes, as it is a general
    framework for creating networks between containers. However, when used with Kubernetes,
    it is a DaemonSet that runs in the cluster, creating a virtual network interface
    in the root namespace, which is an intermediary between packets flowing in and
    out of a node.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 该覆盖层遵循一组规范和库，用于编写插件以配置Linux容器中的网络接口。这个覆盖层的术语是*c*ontainer network interface (CNI**)。CNI是云原生计算基金会（CNCF）的一个项目，它不仅限于Kubernetes，因为它是一个创建容器之间网络的通用框架。然而，当与Kubernetes一起使用时，它是一个在集群中运行的DaemonSet，在根命名空间中创建一个虚拟网络接口，这是节点进出数据包流动的中介。
- en: 'Popular CNI plugins for Kubernetes are Flannel, Calico, Weavenet, Cilium, and
    more. On the CKA exam, only Flannel and Calico are used, so we’ll focus on them.
    The kindnet DaemonSet is running in our cluster and is the CNI used for a kind
    cluster. You can see this by typing the command `k get ds -n kube-system`. The
    output will look like this:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中流行的CNI插件包括Flannel、Calico、Weavenet、Cilium等。在CKA考试中，仅使用Flannel和Calico，因此我们将重点关注它们。kindnet
    DaemonSet正在我们的集群中运行，是用于kind集群的CNI。您可以通过输入命令`k get ds -n kube-system`来查看这一点。输出将类似于以下内容：
- en: '[PRE44]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'This serves as a VXLAN. For the exam, you’ll need to know how to choose an
    appropriate CNI plugin. We can install the flannel CNI by first creating a new
    kind cluster without a CNI, using the steps defined in appendix C. Once you’ve
    created a new kind cluster, get a shell to both of the node containers—one at
    a time–with the commands `docker exec -it kind-control-plane bash` and `docker
    exec -it kind-worker bash`. When you have a Bash shell to the container, go ahead
    and run the command `apt update; apt install wget` on both `kind-control-plane`
    and `kind-worker`. This will install wget, a command-line tool that we can use
    to download files from the web, which is what we’re going to do with the command
    `wget https:// github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz`.
    Because this file is a tarball, you’ll have to untar it with the command `tar
    -xvf cni-plugins-linux-amd64-v1.1.1.tgz`. The output will look similar to this:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这充当了一个VXLAN。对于考试，你需要知道如何选择合适的CNI插件。我们可以通过首先创建一个没有CNI的新kind集群，使用附录C中定义的步骤来安装flannel
    CNI。一旦你创建了一个新的kind集群，使用命令`docker exec -it kind-control-plane bash`和`docker exec
    -it kind-worker bash`分别获取到两个节点容器的一个shell。当你有一个Bash shell到容器时，继续在`kind-control-plane`和`kind-worker`上运行命令`apt
    update; apt install wget`。这将安装wget，这是一个我们可以用来从网络上下载文件的命令行工具，这正是我们将通过命令`wget https://github.com/containernetworking/plugins/releases/download/v1.1.1/cni-plugins-linux-amd64-v1.1.1.tgz`所做的事情。因为这是一个tarball文件，你需要使用命令`tar
    -xvf cni-plugins-linux-amd64-v1.1.1.tgz`来解压它。输出将类似于以下内容：
- en: '[PRE45]'
  id: totrans-215
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The bridge file is the most important for our case, as it will provide the necessary
    plugin for Kubernetes to use Flannel as a CNI. It also needs to be in a specific
    directory—`/opt/cni/bin/`—to be picked up for the cluster. We’ll move the file
    bridge to that directory with the command `mv bridge /opt/cni/bin``/`.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的情况，网桥文件是最重要的，因为它将为Kubernetes提供使用Flannel作为CNI所需的插件。它还需要位于特定的目录——`/opt/cni/bin/`——以便被集群识别。我们将使用命令`mv
    bridge /opt/cni/bin/`将文件bridge移动到该目录。
- en: 'Now that we’ve installed the bridge plugin on both `kind-control-plane` and
    `kind-worker`, we can install Flannel CNI by creating the `flannel` Kubernetes
    objects inside our cluster—while inside of a shell on the control plane node—with
    the command `kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/
    Documentation/kube-flannel.yml`. You can verify that the nodes are in a ready
    state with the command `kubectl get no`. The output will look similar to this:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在`kind-control-plane`和`kind-worker`上安装了网桥插件，我们可以通过在控制平面节点的shell内部创建`flannel`
    Kubernetes对象来安装Flannel CNI——使用命令`kubectl apply -f https://raw.githubusercontent.com/flannel-io/flannel/master/Documentation/kube-flannel.yml`。你可以使用命令`kubectl
    get no`来验证节点是否处于就绪状态。输出将类似于以下内容：
- en: '[PRE46]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We can also verify that the CoreDNS Pods are running—and that the `flannel`
    Pods are created and running—with the command `kubectl get po -A`. The output
    will look similar to this:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用命令`kubectl get po -A`来验证CoreDNS Pods正在运行，并且`flannel` Pods已被创建并正在运行。输出将类似于以下内容：
- en: '[PRE47]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: You now have the Flannel CNI installed in your kind cluster and are ready to
    communicate across nodes using the CNI, as well as to provide encapsulation to
    packets flowing back and forth.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你在你的kind集群中已经安装了Flannel CNI，并准备好使用CNI在节点之间进行通信，以及为往返的数据包提供封装。
- en: Exam exercises
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 考试练习
- en: Create a Deployment named `hello` using the image `nginxdemos/hello:plain-text`
    with the `kubectl` command line. Expose the Deployment to create a ClusterIP Service
    named `hello-svc` that can communicate over port 80 using the `kubectl` command
    line. Use the correct `kubectl` command to verify that it’s a ClusterIP Service
    with the correct port exposed.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl`命令行，通过`nginxdemos/hello:plain-text`镜像创建一个名为`hello`的Deployment。使用`kubectl`命令行暴露Deployment以创建一个名为`hello-svc`的ClusterIP服务，该服务可以通过端口80进行通信。使用正确的`kubectl`命令来验证它是一个具有正确端口暴露的ClusterIP服务。
- en: Change the `hello-svc` Service created in the previous exercise to a NodePort
    Service, where the NodePort should be 30000\. Be sure to edit the Service in place,
    without creating a new YAML or issuing a new imperative command. Communicate with
    the Pods within the hello Deployment via the NodePort Service using curl.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 将上一个练习中创建的`hello-svc`服务更改为NodePort服务，其中NodePort应为30000。确保就地编辑服务，而不是创建新的YAML或发出新的命令。使用curl通过NodePort服务与hello
    Deployment内的Pod进行通信。
- en: Install an Ingress controller in the cluster using the command `k apply -f https://
    raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/ nginx-ingress-controller.yaml`.
    Change the `hello-svc` Service back to a ClusterIP Service and create an Ingress
    resource that will route to the `hello-svc` Service when a client requests hello.com.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令 `k apply -f https:// raw.githubusercontent.com/chadmcrowell/acing-the-cka-exam/main/ch_06/
    nginx-ingress-controller.yaml` 在集群中安装 Ingress 控制器。将 `hello-svc` 服务改回 ClusterIP
    服务，并创建一个 Ingress 资源，当客户端请求 hello.com 时，将路由到 `hello-svc` 服务。
- en: Create a new kind cluster without a CNI. Install the bridge CNI, followed by
    the Calico CNI. After installing the CNI, verify that the CoreDNS Pods are up
    and running and the nodes are in a ready state.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的没有 CNI 的 kind 集群。安装 bridge CNI，然后安装 Calico CNI。在安装 CNI 后，验证 CoreDNS Pods
    是否正在运行，并且节点处于就绪状态。
- en: Summary
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: For Service discovery, DNS resolution happens via CoreDNS in a Kubernetes cluster.
    We explored how to discover both Pods and Services via DNS queries. For the exam,
    you will be expected to know how to configure and use CoreDNS.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于服务发现，在 Kubernetes 集群中，DNS 解析是通过 CoreDNS 进行的。我们探讨了如何通过 DNS 查询来发现 Pod 和服务。对于考试，你将需要知道如何配置和使用
    CoreDNS。
- en: CoreDNS is running in a Deployment in the `kube-system` namespace. We can replicate
    the Pods in the CoreDNS Deployment to get faster domain-name resolution.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CoreDNS 在 `kube-system` 命名空间中的 Deployment 上运行。我们可以复制 CoreDNS Deployment 中的 Pod
    以获得更快的域名解析。
- en: The kubelet is responsible for populating each Pod with the proper CoreDNS IP
    address to allow for the Pod’s DNS resolution. For the exam, remember that there’s
    the `/etc/kubernetes/manifests` directory on the control plane node where the
    kubelet will automatically start Pods (called static Pods).
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 负责为每个 Pod 提供正确的 CoreDNS IP 地址，以便允许 Pod 进行 DNS 解析。对于考试，请记住在控制平面节点上存在
    `/etc/kubernetes/manifests` 目录，kubelet 将自动启动 Pods（称为静态 Pods）。
- en: The DNS resolver file in a Pod is `/etc/resolv.conf`, which will have certain
    search criteria according to the Services in the cluster. The `resolv.conf` file
    also contains the DNS IP address. When resolving hostnames on the exam, don’t
    forget to use the Pod’s FQDN if it’s in a different namespace.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 中的 DNS 解析器文件是 `/etc/resolv.conf`，它将根据集群中的服务具有某些搜索标准。`resolv.conf` 文件还包含
    DNS IP 地址。在考试中解析主机名时，如果 Pod 在不同的命名空间中，别忘了使用 Pod 的 FQDN。
- en: Ingress and Ingress controllers allow layer-7 routing to the cluster to communicate
    to a Pod via a Service. Ingress can handle routing to multiple Services in the
    same Ingress resource, and you'll need to know how to configure this routing for
    the exam. Also, you’ll need to know how to use Ingress controllers and Ingress
    resources.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ingress 和 Ingress 控制器允许通过层 7 路由到集群，通过服务与 Pod 进行通信。Ingress 可以处理同一 Ingress 资源中多个服务的路由，对于考试，你需要知道如何配置这种路由。此外，你还需要知道如何使用
    Ingress 控制器和 Ingress 资源。
- en: There are three different types of Services in Kubernetes, all of which allow
    communication from Pod to Pod in the cluster. You will need to understand the
    connectivity between Pods for the exam.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中有三种不同类型的 Service，所有这些都允许集群内 Pod 与 Pod 之间的通信。对于考试，你需要了解 Pod 之间的连接性。
- en: The kubelet is a Service running on the node itself. You can reload the daemon
    and restart the Service, as you change the configuration for creating Pods. For
    the exam, you will be expected to know how to stop, start, and reload the kubelet
    daemon.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kubelet 是在节点本身上运行的服务。你可以重新加载守护进程并重启服务，就像你更改创建 Pod 的配置一样。对于考试，你将需要知道如何停止、启动和重新加载
    kubelet 守护进程。
- en: The cluster networking interface (CNI) in Kubernetes assists with communication
    from node to node by encapsulating the packet and adding a header for the source
    and destination route. For the exam, you’ll need to understand host networking
    configuration on cluster nodes.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 中的集群网络接口 (CNI) 通过封装数据包并为源和目标路由添加头部来帮助节点之间的通信。对于考试，你需要了解集群节点上的主机网络配置。
