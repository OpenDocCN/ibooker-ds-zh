- en: Chapter 6\. Rules and Rationality
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章：规则与理性
- en: Christof Wolf Brenner
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 克里斯多夫·沃尔夫·布伦纳
- en: '![](Images/christof_wolf-brenner.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/christof_wolf-brenner.png)'
- en: Consultant, Know-Center GmbH
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 咨询师，Know-Center GmbH
- en: In Isaac Asimov’s famous science fiction stories, a hierarchical set of laws
    acts as the centerpiece to ensure ethical behavior of artificial moral agents.
    These robots—part computer, part machine—can efficiently handle complex tasks
    that would otherwise require human-level minds to complete.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在艾萨克·阿西莫夫著名的科幻故事中，一套层次分明的法律作为核心，确保人工道德代理的伦理行为。这些机器人——既是计算机也是机器——能够高效处理复杂任务，而这些任务原本需要人类水平的智慧来完成。
- en: 'Asimov argues that his ruleset is the only suitable foundation for the interaction
    between rational human beings and robots that adapt and flexibly choose their
    own course of action. Today, almost 80 years after the first iteration of the
    laws was devised in 1942, die-hard fans still argue that Asimov’s laws are sufficient
    to guide moral decision making. However, looking at the ruleset as finalized by
    Asimov in 1985, it becomes clear that, applied exclusively, these laws might not
    produce what we would call “good decisions”:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 阿西莫夫认为，他的规则集是理性人类与适应并灵活选择自己行动路线的机器人之间互动的唯一合适基础。今天，几乎80年过去了，自从1942年第一次制定这些法律以来，死忠粉丝仍然认为阿西莫夫的法律足以指导道德决策。然而，看看阿西莫夫在1985年最终确定的规则集，就会清楚地发现，单独应用这些法律可能无法产生我们所称的“好决策”：
- en: Zeroth Law
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 零法则
- en: A robot may not harm humanity, or, by inaction, allow humanity to come to harm.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器人不得伤害人类，或者通过不作为，允许人类受到伤害。
- en: First Law
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第一法则
- en: A robot may not injure a human being, or, through inaction, allow a human being
    to come to harm.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器人不得伤害任何人类，或者通过不作为，允许任何人类受到伤害。
- en: Second Law
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第二法则
- en: A robot must obey the orders given it by human beings, except where such orders
    would conflict with the First Law.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 一台机器人必须遵循人类给它的命令，除非这些命令与第一法则相冲突。
- en: Third Law
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 第三法则
- en: A robot must protect its own existence, as long as such protection does not
    conflict with the First or Second Law.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人必须保护自己的存在，只要这种保护不与第一法则或第二法则相冲突。
- en: 'Asimov’s autonomous ethical agents can assess situations and act accordingly
    based on a combination of information about the world that they process and the
    aforementioned laws, which are imprinted in their artificial brains. However,
    as conflict between different laws arises, the robots are also able to reflect,
    reason, and reach sensible conclusions. This tiny and often overlooked detail
    provides a first inkling of how static rulesets might not be able to sufficiently
    support moral decision making on their own, and that Isaac Asimov was most likely
    aware of that. At the very least, even though he strongly promoted the exclusive
    application of the laws, his plots usually revolve around fringe cases in which
    no clear decision could be reached and therefore further reasoning would be required.
    Consider, if you will, accidents involving autonomous vehicles in a setup similar
    to the famous trolley problem:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 阿西莫夫的自治伦理代理能够根据它们处理的世界信息和上述嵌入其人工大脑中的法律组合来评估情况并采取相应行动。然而，随着不同法律之间的冲突出现，机器人也能够反思、推理并得出合理的结论。这一微小且常被忽视的细节提供了一个初步的迹象，说明静态规则集可能无法单独充分支持道德决策制定，而艾萨克·阿西莫夫很可能已经意识到这一点。至少，即使他强烈推广法律的唯一适用性，他的情节通常围绕无法作出明确决定的边缘案例展开，因此需要进一步的推理。请考虑，如果你愿意，涉及自动驾驶车辆的事故，设置类似著名的电车问题：
- en: 'A fully autonomous car—a robot in Asimovian terminology—is transporting a human
    being (A) to its desired destination. Suddenly, in a twist of fate, some living
    being (B) appears on the road. The artificial intelligence (i.e., the computer)
    that controls the vehicle (i.e., the machine) must come to a decision within a
    fraction of a second: take evasive action or continue straight ahead. If it does
    try to dodge B, the vehicle skids and hits a tree, A dies, and B survives. If
    not, A survives, but B dies. For simplification purposes, we shall assume that
    collateral damage is negligible or identical in both cases.'
  id: totrans-15
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 一辆完全自动驾驶的汽车——在阿西莫夫术语中是一个机器人——正将一个人类（A）运送到目的地。突然，命运弄人，一些生物（B）出现在道路上。控制车辆的人工智能（即计算机）必须在一秒钟的分秒之间做出决定：采取规避行动还是继续前进。如果它尝试躲避B，车辆打滑撞到树上，A死亡，B幸存。如果不躲避，A生存，B死亡。为了简化起见，我们假设附带损害是可以忽略不计的，或者在两种情况下是相同的。
- en: Based on this fringe scenario, we can derive two major issues with Asimov’s
    laws. First, if the robotic car had to decide between harm to human beings and
    harm to nonhuman beings, the nonhuman beings always lose. This results in speciesist
    robots—that is, robots that have a bias for or against a being because of its
    species. If B were a group or one of the last of a kind of animals, we surely
    should at least consider the implications of running them or it over.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这种边缘情景，我们可以推断出阿西莫夫法则存在两个主要问题。首先，如果机器人汽车必须在伤害人类和伤害非人类之间做出决定，非人类总是处于劣势。这导致了种族主义倾向的机器人——即，机器人因为某种生物的物种而对其偏爱或偏见。如果B是一组动物或某种动物的最后一批，我们肯定至少应该考虑撞死它们或它的后果。
- en: 'Second, the laws are not tailored to support decision making when different
    levels of harm to humans may occur. If all potential outcomes of a scenario were
    to involve harm to a human being, the ruleset would not be able to guide us to
    a decision: if all alternatives comply with the rules, they are equally good.
    If the outcome would involve either one human losing an arm or another human losing
    both arms, there would be no preference. Rather, Asimov’s laws are designed purely
    to prioritize groups of human beings or humanity as a whole over everything else.
    Even if it were easy for us to rationally argue for one course of action over
    another, the robot would not be able to do so if it were only following the laws.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，这些法则并不适合在可能发生对人类造成不同程度伤害的情况下支持决策。如果一个情景的所有潜在结果都涉及对人类的伤害，法则集将无法指导我们做出决定：如果所有替代方案都符合法则，它们是同等好的。如果结果可能导致一个人失去一只胳膊或另一个人失去两只胳膊，也没有优先选择。相反，阿西莫夫的法则纯粹设计用来优先考虑人类或整个人类群体。即使我们很容易基于理性辩护一个行动方案优于另一个，如果机器人只是遵循法则，它将无法这样做。
- en: If Asimov’s laws were to be taken as the basis for ethical decision making by
    robots, they would additionally need to be able to rationally argue for better
    outcomes or against worse outcomes in fringe cases. Rationality would need to
    be the glue between the laws. But then why use static laws in the first place?
    Wouldn’t it be easier to just use our own rationality to decide what acts are
    good?
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 如果阿西莫夫的法则被视为机器人进行道德决策的基础，它们还需要能够在边缘案例中理性地辩护以争取更好的结果或反对更坏的结果。理性必须成为法则之间的粘合剂。但那么为什么首先使用静态法则？使用我们自己的理性来决定哪些行为是好的不是更容易吗？
