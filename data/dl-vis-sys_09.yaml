- en: 7 Object detection with R-CNN, SSD, and YOLO
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用R-CNN、SSD和YOLO进行目标检测
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding image classification vs. object detection
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解图像分类与目标检测
- en: Understanding the general framework of object detection projects
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解目标检测项目的一般框架
- en: Using object detection algorithms like R-CNN, SSD, and YOLO
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用R-CNN、SSD和YOLO等目标检测算法
- en: In the previous chapters, we explained how we can use deep neural networks for
    image classification tasks. In image classification, we assume that there is only
    one main target object in the image, and the model’s sole focus is to identify
    the target category. However, in many situations, we are interested in multiple
    targets in the image. We want to not only classify them, but also obtain their
    specific positions in the image. In computer vision, we refer to such tasks as
    object detection. Figure 7.1 explains the difference between image classification
    and object detection tasks.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中，我们解释了如何使用深度神经网络进行图像分类任务。在图像分类中，我们假设图像中只有一个主要的目标对象，模型的主要焦点是识别目标类别。然而，在许多情况下，我们对图像中的多个目标感兴趣。我们不仅想要对它们进行分类，还想要获取它们在图像中的具体位置。在计算机视觉中，我们将此类任务称为目标检测。图7.1解释了图像分类和目标检测任务之间的区别。
- en: '![](../Images/7-1.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-1.png)'
- en: Figure 7.1 Image classification vs. object detection tasks. In classification
    tasks, the classifier outputs the class probability (cat), whereas in object detection
    tasks, the detector outputs the bounding box coordinates that localize the detected
    objects (four boxes in this example) and their predicted classes (two cats, one
    duck, and one dog).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 图像分类与目标检测任务。在分类任务中，分类器输出类别概率（猫），而在目标检测任务中，检测器输出定位检测到的对象的边界框坐标（本例中的四个框）及其预测类别（两只猫、一只鸭和一只狗）。
- en: 'Object detection is a CV task that involves both main tasks: localizing one
    or more objects within an image and classifying each object in the image (see
    table 7.1). This is done by drawing a bounding box around the identified object
    with its predicted class. This means the system doesn’t just predict the class
    of the image, as in image classification tasks; it also predicts the coordinates
    of the bounding box that fits the detected object. This is a challenging CV task
    because it requires both successful object localization, in order to locate and
    draw a bounding box around each object in an image, and object classification
    to predict the correct class of object that was localized.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测是一个涉及两个主要任务的计算机视觉任务：在图像中定位一个或多个对象，并对图像中的每个对象进行分类（见表7.1）。这是通过在识别的对象周围绘制一个包含其预测类别的边界框来完成的。这意味着系统不仅预测图像的类别，就像在图像分类任务中那样；它还预测适合检测到的对象的边界框坐标。这是一个具有挑战性的计算机视觉任务，因为它需要成功的目标定位，以便在图像中定位并绘制每个对象的边界框，以及目标分类来预测定位到的对象的正确类别。
- en: Table 7.1 Image classification vs. object detection
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 图像分类与目标检测
- en: '| Image classification | Object detection |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| 图像分类 | 目标检测 |'
- en: '| The goal is to predict the type or class of an object in an image.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '| 目标是预测图像中对象的类型或类别。'
- en: 'Input: an image with a single object'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：包含单个对象的图像
- en: 'Output: a class label (cat, dog, etc.)'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：类别标签（猫、狗等）
- en: 'Example output: class probability (for example, 84% cat)'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例输出：类别概率（例如，84%猫）
- en: '| The goal is to predict the location of objects in an image via bounding boxes
    and the classes of the located objects.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '| 目标是通过边界框预测图像中对象的位置，并预测定位到的对象的类别。|'
- en: 'Input: an image with one or more objects'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入：包含一个或多个对象的图像
- en: 'Output: one or more bounding boxes (defined by coordinates) and a class label
    for each bounding box'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出：一个或多个边界框（由坐标定义）以及每个边界框的类别标签
- en: 'Example output for an image with two objects:'
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含两个对象的图像的示例输出：
- en: box1 coordinates (*x, y, w, h*) and class probability
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: box1坐标（*x, y, w, h*）和类别概率
- en: box2 coordinates and class probability
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: box2坐标和类别概率
- en: 'Note that the image coordinates (*x, y, w, h*) are as follows: (*x* and *y*)
    are the coordinates of the bounding-box center point, and (*w* and *h*) are the
    width and height of the box. |'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图像坐标（*x, y, w, h*）如下：（*x* 和 *y*）是边界框中心点的坐标，而（*w* 和 *h*）是框的宽度和高度。|
- en: '|  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Object detection is widely used in many fields. For example, in self-driving
    technology, we need to plan routes by identifying the locations of vehicles, pedestrians,
    roads, and obstacles in a captured video image. Robots often perform this type
    of task to detect targets of interest. And systems in the security field need
    to detect abnormal targets, such as intruders or bombs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 目标检测在许多领域中得到广泛应用。例如，在自动驾驶技术中，我们需要通过识别捕获的视频图像中车辆、行人、道路和障碍物的位置来规划路线。机器人通常执行此类任务以检测感兴趣的目标。安全领域的系统需要检测异常目标，如入侵者或炸弹。
- en: 'This chapter’s layout is as follows:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的结构如下：
- en: We will explore the general framework of the object detection algorithms.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将探讨目标检测算法的通用框架。
- en: 'We will dive deep into three of the most popular detection algorithms: the
    R-CNN family of networks, SSD, and the YOLO family of networks.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将深入研究三种最受欢迎的检测算法：R-CNN系列网络、SSD和YOLO系列网络。
- en: We will use what we’ve learned in a real-world project to train an end-to-end
    object detector.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们将利用在现实世界项目中学到的知识来训练一个端到端的目标检测器。
- en: By the end of this chapter, we will have gained an understanding of how DL is
    applied to object detection, and how the different object detection models inspire
    and diverge from one another. Let’s get started!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，我们将了解深度学习在目标检测中的应用，以及不同的目标检测模型如何相互启发和分化。让我们开始吧！
- en: 7.1 General object detection framework
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 通用目标检测框架
- en: Before we jump into the object detection systems like R-CNN, SSD, and YOLO,
    let’s discuss the general framework of these systems to understand the high-level
    workflow that DL-based systems follow to detect objects and the metrics they use
    to evaluate their detection performance. Don’t worry about the code implementation
    details of object detectors yet. The goal of this section is to give you an overview
    of how different object detection systems approach this task and introduce you
    to a new way of thinking about this problem and a set of new concepts to set you
    up to understand the DL architectures that we will explain in sections 7.2, 7.3,
    and 7.4.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨R-CNN、SSD和YOLO等目标检测系统之前，让我们先讨论这些系统的通用框架，以便理解基于深度学习（DL）的系统在检测对象时遵循的高级工作流程以及它们用于评估检测性能的指标。目前不必担心目标检测器的代码实现细节。本节的目标是向您概述不同的目标检测系统如何处理这一任务，并介绍一种新的思考方式以及一系列新概念，以便您能够理解我们在第7.2节、第7.3节和第7.4节中将要解释的深度学习架构。
- en: 'Typically, an object detection framework has four components:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个目标检测框架包含四个组件：
- en: Region proposal --An algorithm or a DL model is used to generate regions of
    interest (RoIs) to be further processed by the system. These are regions that
    the network believes might contain an object; the output is a large number of
    bounding boxes, each of which has an objectness score. Boxes with large objectness
    scores are then passed along the network layers for further processing.
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 区域提议——使用算法或深度学习模型生成感兴趣区域（RoIs），以便系统进一步处理。这些是网络认为可能包含对象的区域；输出是大量边界框，每个边界框都有一个对象性分数。具有高对象性分数的边界框随后被传递到网络层进行进一步处理。
- en: Feature extraction and network predictions --Visual features are extracted for
    each of the bounding boxes. They are evaluated, and it is determined whether and
    which objects are present in the proposals based on visual features (for example,
    an object classification component).
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取和网络预测——为每个边界框提取视觉特征。根据视觉特征（例如，一个对象分类组件）评估这些特征，并确定在提议中是否存在以及哪些对象存在。
- en: Non-maximum suppression (NMS) --In this step, the model has likely found multiple
    bounding boxes for the same object. NMS helps avoid repeated detection of the
    same instance by combining overlapping boxes into a single bounding box for each
    object.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 非极大值抑制（NMS）——在这个步骤中，模型可能已经找到了同一对象的多个边界框。NMS通过将重叠的边界框合并为每个对象的单个边界框来帮助避免对同一实例的重复检测。
- en: Evaluation metrics --Similar to accuracy, precision, and recall metrics in image
    classification tasks (see chapter 4), object detection systems have their own
    metrics to evaluate their detection performance. In this section, we will explain
    the most popular metrics, like mean average precision (mAP), precision-recall
    curve (PR curve), and intersection over union (IoU).
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 评估指标——类似于图像分类任务中的准确率、精确率和召回率指标（见第4章），目标检测系统有自己的指标来评估它们的检测性能。在本节中，我们将解释最流行的指标，如平均精度均值（mAP）、精确率-召回率曲线（PR曲线）和交并比（IoU）。
- en: Now, let’s dive one level deeper into each one of these components to build
    an intuition about what their goals are.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入一层，了解这些组件的目标，以建立对这些组件的直观认识。
- en: 7.1.1 Region proposals
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 区域提议
- en: In this step, the system looks at the image and proposes RoIs for further analysis.
    RoIs are regions that the system believes have a high likelihood of containing
    an object, called the objectness score (figure 7.2). Regions with high objectness
    scores are passed to the next steps; regions with low scores are abandoned.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，系统查看图像并提议进行进一步分析的RoIs（感兴趣区域）。RoIs是系统认为有很高可能性包含对象的区域，称为对象性分数（图7.2）。具有高对象性分数的区域将传递到下一步；分数低的区域将被放弃。
- en: '![](../Images/7-2.png)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-2.png)'
- en: Figure 7.2 Regions of interest (RoIs) proposed by the system. Regions with high
    objectness score represent areas of high likelihood to contain objects (foreground),
    and the ones with low objectness score are ignored because they have a low likelihood
    of containing objects (background).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 系统提出的感兴趣区域（RoIs）。具有高对象性分数的区域代表有很高可能性包含对象（前景），而具有低对象性分数的区域被忽略，因为它们有很低的可能性包含对象（背景）。
- en: There are several approaches to generate region proposals. Originally, the selective
    search algorithm was used to generate object proposals; we will talk more about
    this algorithm when we discuss the R-CNN network. Other approaches use more complex
    visual features extracted from the image by a deep neural network to generate
    regions (for example, based on the features from a DL model).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 生成区域提议有几种方法。最初，选择性搜索算法被用来生成对象提议；当我们讨论R-CNN网络时，我们将更详细地介绍这个算法。其他方法使用从图像中提取的更复杂的视觉特征，这些特征由深度神经网络生成区域（例如，基于深度学习模型的特征）。
- en: We will talk in more detail about how different object detection systems approach
    this task. The important thing to note is that this step produces a lot (thousands)
    of bounding boxes to be further analyzed and classified by the network. During
    this step, the network analyzes these regions in the image and classifies each
    region as foreground (object) or background (no object) based on its objectness
    score. If the objectness score is above a certain threshold, then this region
    is considered a foreground and pushed forward in the network. Note that this threshold
    is configurable based on your problem. If the threshold is too low, your network
    will exhaustively generate all possible proposals, and you will have a better
    chance of detecting all objects in the image. On the flip side, this is very computationally
    expensive and will slow down detection. So, the trade-off with generating region
    proposals is the number of regions versus computational complexity--and the right
    approach is to use problem-specific information to reduce the number of RoIs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将更详细地讨论不同的目标检测系统如何处理这个任务。需要注意的是，这一步产生了大量（数千个）边界框，这些边界框将由网络进一步分析和分类。在这一步中，网络分析图像中的这些区域，并根据其对象性分数将每个区域分类为前景（对象）或背景（无对象）。如果对象性分数高于某个阈值，则该区域被认为是前景，并在网络中推进。请注意，这个阈值可以根据你的问题进行配置。如果阈值太低，你的网络将生成所有可能的提议，你将有更好的机会检测到图像中的所有对象。另一方面，这非常计算密集，会减慢检测速度。因此，生成区域提议的权衡是区域数量与计算复杂度——正确的做法是使用特定于问题的信息来减少RoIs的数量。
- en: 7.1.2 Network predictions
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 网络预测
- en: This component includes the pretrained CNN network that is used for feature
    extraction to extract features from the input image that are representative for
    the task at hand and to use these features to determine the class of the image.
    In object detection frameworks, people typically use pretrained image classification
    models to extract visual features, as these tend to generalize fairly well. For
    example, a model trained on the MS COCO or ImageNet dataset is able to extract
    fairly generic features.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 该组件包括用于特征提取的预训练CNN网络，从输入图像中提取具有代表性的特征，并使用这些特征来确定图像的类别。在目标检测框架中，人们通常使用预训练的图像分类模型来提取视觉特征，因为这些模型通常具有很好的泛化能力。例如，在MS
    COCO或ImageNet数据集上训练的模型能够提取相当通用的特征。
- en: 'In this step, the network analyzes all the regions that have been identified
    as having a high likelihood of containing an object and makes two predictions
    for each region:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步，网络分析所有被识别为有很高可能性包含对象的区域，并对每个区域做出两个预测：
- en: Bounding-box prediction--The coordinates that locate the box surrounding the
    object. The bounding box coordinates are represented as the tuple (*x, y, w, h*),
    where *x* and *y* are the coordinates of the center point of the bounding box
    and w and h are the width and height of the box.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框预测--定位围绕对象的框的坐标。边界框坐标表示为元组(*x, y, w, h*)，其中*x*和*y*是边界框中心点的坐标，w和h是框的宽度和高度。
- en: 'Class prediction : The classic softmax function that predicts the class probability
    for each object.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别预测：预测每个对象类别概率的经典softmax函数。
- en: 'Since thousands of regions are proposed, each object will always have multiple
    bounding boxes surrounding it with the correct classification. For example, take
    a look at the image of the dog in figure 7.3\. The network was clearly able to
    find the object (dog) and successfully classify it. But the detection fired a
    total of five times because the dog was present in the five RoIs produced in the
    previous step: hence the five bounding boxes around the dog in the figure. Although
    the detector was able to successfully locate the dog in the image and classify
    it correctly, this is not exactly what we need. We need just one bounding box
    for each object for most problems. In some problems, we only want the one box
    that fits the object the most. What if we are building a system to count dogs
    in an image? Our current system will count five dogs. We don’t want that. This
    is when the non-maximum suppression technique comes in handy.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 由于提出了数千个区域，每个对象周围都会有多个边界框，这些边界框具有正确的分类。例如，看看图7.3中的狗的图像。网络显然能够找到对象（狗）并成功对其进行分类。但是检测总共触发了五次，因为狗出现在之前步骤中产生的五个RoIs中：因此图中的狗周围有五个边界框。尽管检测器能够成功地在图像中定位狗并正确分类它，但这并不是我们需要的。对于大多数问题，我们只需要每个对象一个边界框。在某些问题中，我们只想得到最适合对象的那个框。如果我们正在构建一个用于在图像中计数狗的系统呢？我们当前的系统将计数五只狗。我们不想这样。这就是非极大值抑制技术派上用场的时候了。
- en: '![](../Images/7-3.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-3.png)'
- en: Figure 7.3 The bounding-box detector produces more than one bounding box for
    an object. We want to consolidate these boxes into one bounding box that fits
    the object the most.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 边界框检测器对一个对象产生多个边界框。我们希望将这些框合并成一个最适合对象的边界框。
- en: 7.1.3 Non-maximum suppression (NMS)
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 非极大值抑制（NMS）
- en: As you can see in figure 7.4, one of the problems of an object detection algorithm
    is that it may find multiple detections of the same object. So, instead of creating
    only one bounding box around the object, it draws multiple boxes for the same
    object. NMS is a technique that makes sure the detection algorithm detects each
    object only once. As the name implies, NMS looks at all the boxes surrounding
    an object to find the box that has the maximum prediction probability, and it
    suppresses or eliminates the other boxes (hence the name).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.4所示，对象检测算法的一个问题是它可能会找到同一对象的多个检测。因此，它不是只围绕对象创建一个边界框，而是为同一对象绘制多个框。NMS是一种确保检测算法只检测每个对象一次的技术。正如其名所示，NMS检查围绕一个对象的全部框，以找到具有最大预测概率的框，并抑制或消除其他框（因此得名）。
- en: '![](../Images/7-4.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-4.png)'
- en: Figure 7.4 Multiple regions are proposed for the same object. After NMS, only
    the box that fits the object the best remains; the rest are ignored, as they have
    large overlaps with the selected box.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 对于同一个对象提出了多个区域。经过NMS处理后，只剩下最适合对象的框；其余的都被忽略，因为它们与选定的框有较大的重叠。
- en: The general idea of NMS is to reduce the number of candidate boxes to only one
    bounding box for each object. For example, if the object in the frame is fairly
    large and more than 2,000 object proposals have been generated, it is quite likely
    that some of them will have significant overlap with each other and the object.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: NMS的一般思想是将候选框的数量减少到每个对象只有一个边界框。例如，如果帧中的对象相当大，并且已经生成了超过2,000个对象提议，那么其中一些很可能彼此之间有显著的重叠，并且与对象本身重叠。
- en: 'Let’s see the steps of how the NMS algorithm works:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看NMS算法的工作步骤：
- en: Discard all bounding boxes that have predictions that are less than a certain
    threshold, called the confidence threshold. This threshold is tunable, which means
    a box will be suppressed if the prediction probability is less than the set threshold.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抛弃所有预测值低于某个特定阈值（称为置信度阈值）的边界框。这个阈值是可调整的，这意味着如果预测概率低于设定的阈值，则该框将被抑制。
- en: Look at all the remaining boxes, and select the bounding box with the highest
    probability.
  id: totrans-62
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看所有剩余的框，并选择概率最高的边界框。
- en: Calculate the overlap of the remaining boxes that have the same class prediction.
    Bounding boxes that have high overlap with each other and that predict the same
    class are averaged together. This overlap metric is called intersection over union
    (IoU). IoU is explained in detail in the next section.
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算具有相同类别预测的剩余框的重叠。具有高度重叠并预测相同类别的边界框被平均在一起。这个重叠指标称为交并比（IoU）。IoU将在下一节中详细解释。
- en: Suppress any box that has an IoU value smaller than a certain threshold (called
    the NMS threshold). Usually the NMS threshold is equal to 0.5, but it is tunable
    as well if you want to output fewer or more bounding boxes.
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 抑制任何IoU值小于某个特定阈值（称为NMS阈值）的框。通常NMS阈值等于0.5，但如果你想输出更少或更多的边界框，它也是可调整的。
- en: NMS techniques are typically standard across the different detection frameworks,
    but it is an important step that may require tweaking hyperparameters such as
    the confidence threshold and the NMS threshold based on the scenario.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: NMS技术通常在不同检测框架中是标准的，但它是一个重要的步骤，可能需要根据场景调整超参数，如置信度阈值和NMS阈值。
- en: 7.1.4 Object-detector evaluation metrics
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 对象检测器评估指标
- en: 'When evaluating the performance of an object detector, we use two main evaluation
    metrics: frames per second and mean average precision.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 当评估对象检测器的性能时，我们使用两个主要的评估指标：每秒帧数和平均精度均值。
- en: Frames per second (FPS) to measure detection speed
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 每秒帧数（FPS）用于衡量检测速度
- en: 'The most common metric used to measure detection speed is the number of frames
    per second (FPS). For example, Faster R-CNN operates at only 7 FPS, whereas SSD
    operates at 59 FPS. In benchmarking experiments, you will see the authors of a
    paper state their network results as: “Network *x* achieves mAP of Y% at Z FPS,”
    where *x* is the network name, *y* is the mAP percentage, and Z is the FPS.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 用于衡量检测速度的最常用指标是每秒帧数（FPS）。例如，Faster R-CNN以仅7 FPS的速度运行，而SSD以59 FPS的速度运行。在基准测试实验中，你将看到论文的作者将他们的网络结果表述为：“网络*x*在Z
    FPS下实现了Y%的mAP，”其中*x*是网络名称，*y*是mAP百分比，Z是FPS。
- en: Mean average precision (mAP) to measure network precision
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 平均精度均值（mAP）用于衡量网络精度
- en: The most common evaluation metric used in object recognition tasks is mean average
    precision (mAP). It is a percentage from 0 to 100, and higher values are typically
    better, but its value is different from the accuracy metric used in classification.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在对象识别任务中最常用的评估指标是平均精度均值（mAP）。它是一个从0到100的百分比，通常值越高越好，但它的值与分类中使用的准确度指标不同。
- en: To understand how mAP is calculated, you first need to understand intersection
    over union (IoU) and the precision-recall curve (PR curve). Let’s explain IoU
    and the PR curve and then come back to mAP.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解mAP是如何计算的，你首先需要理解交并比（IoU）和精确度-召回率曲线（PR曲线）。让我们先解释IoU和PR曲线，然后再回到mAP。
- en: Intersection over union (IoU)
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 交并比（IoU）
- en: 'This measure evaluates the overlap between two bounding boxes: the ground truth
    bounding box (Bground truth) and the predicted bounding box (Bpredicted). By applying
    the IoU, we can tell whether a detection is valid (True Positive) or not (False
    Positive). Figure 7.5 illustrates the IoU between a ground truth bounding box
    and a predicted bounding box.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这个指标评估两个边界框的重叠：真实边界框（Bground truth）和预测边界框（Bpredicted）。通过应用IoU，我们可以判断一个检测是否有效（真阳性）或不是（假阳性）。图7.5说明了真实边界框和预测边界框之间的IoU。
- en: '![](../Images/7-5.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-5.png)'
- en: Figure 7.5 The IoU score is the overlap between the ground truth bounding box
    and the predicted bounding box.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 IoU分数是真实边界框和预测边界框之间的重叠。
- en: The intersection over the union value ranges from 0 (no overlap at all) to 1
    (the two bounding boxes overlap each other 100%). The higher the overlap between
    the two bounding boxes (IoU value), the better (figure 7.6).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 交并比（IoU）的值从0（完全没有重叠）到1（两个边界框重叠100%）不等。两个边界框之间的重叠程度（IoU值）越高，越好（见图7.6）。
- en: '![](../Images/7-6.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-6.png)'
- en: Figure 7.6 IoU scores range from 0 (no overlap) to 1 (100% overlap). The higher
    the overlap (IoU) between the two bounding boxes, the better.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 IoU分数范围从0（无重叠）到1（100%重叠）。两个边界框之间的重叠程度（IoU）越高，越好。
- en: 'To calculate the IoU of a prediction, we need the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算预测的IoU，我们需要以下信息：
- en: 'The ground truth bounding box (Bground truth): the hand-labeled bounding box
    created during the labeling process'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真实边界框（Bground truth）：在标注过程中创建的手动标注边界框
- en: The predicted bounding box (Bpredicted) from our model
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们模型预测的边界框（Bpredicted）
- en: 'We calculate IoU by dividing the area of overlap by the area of the union,
    as in the following equation:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将重叠面积除以并集面积来计算IoU，如下方程所示：
- en: '![](../Images/7-06_F1.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-06_F1.png)'
- en: IoU is used to define a correct prediction, meaning a prediction (True Positive)
    that has an IoU greater than some threshold. This threshold is a tunable value
    depending on the challenge, but 0.5 is a standard value. For example, some challenges,
    like Microsoft COCO, use mAP@0.5 (IoU threshold of 0.5) or mAP@0.75 (IoU threshold
    of 0.75). If the IoU value is above this threshold, the prediction is considered
    a True Positive (TP); and if it is below the threshold, it is considered a False
    Positive (FP).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: IoU用于定义一个正确的预测，意味着一个IoU值大于某个阈值的预测（真阳性）。这个阈值取决于挑战，但0.5是一个标准值。例如，一些挑战，如Microsoft
    COCO，使用mAP@0.5（IoU阈值为0.5）或mAP@0.75（IoU阈值为0.75）。如果IoU值高于这个阈值，预测被认为是真阳性（TP）；如果低于阈值，则被认为是假阳性（FP）。
- en: Precision-recall curve (PR curve)
  id: totrans-86
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 精度-召回曲线（PR曲线）
- en: 'With the TP and FP defined, we can now calculate the precision and recall of
    our detection for a given class across the testing dataset. As explained in chapter
    4, we calculate the precision and recall as follows (recall that FN stands for
    False Negative):'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了TP和FP后，我们现在可以计算在测试数据集上针对给定类别的检测的精度和召回率。如第4章所述，我们计算精度和召回率如下（FN代表假阴性）：
- en: '![](../Images/7-06_F2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-06_F2.png)'
- en: After calculating the precision and recall for all classes, the PR curve is
    then plotted as shown in figure 7.7.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算了所有类别的精度和召回率后，PR曲线如图7.7所示。
- en: '![](../Images/7-7.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-7.png)'
- en: Figure 7.7 A precision-recall curve is used to evaluate the performance of an
    object detector.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 使用精度-召回曲线来评估目标检测器的性能。
- en: The PR curve is a good way to evaluate the performance of an object detector,
    as the confidence is changed by plotting a curve for each object class. A detector
    is considered good if its precision stays high as recall increases, which means
    if you vary the confidence threshold, the precision and recall will still be high.
    On the other hand, a poor detector needs to increase the number of FPs (lower
    precision) in order to achieve a high recall. That’s why the PR curve usually
    starts with high precision values, decreasing as recall increases.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: PR曲线是评估目标检测器性能的好方法，因为通过为每个对象类别绘制曲线来改变置信度。如果一个检测器的精度在召回率增加时保持较高，则认为它是一个好的检测器，这意味着如果你改变置信度阈值，精度和召回率仍然很高。另一方面，一个差的检测器需要增加FP的数量（降低精度）以达到高的召回率。这就是为什么PR曲线通常以高精度值开始，随着召回率的增加而降低。
- en: Now that we have the PR curve, we can calculate the average precision (AP) by
    calculating the area under the curve (AUC). Finally, the mAP for object detection
    is the average of the AP calculated for all the classes. It is also important
    to note that some research papers use AP and mAP interchangeably.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了PR曲线，我们可以通过计算曲线下的面积（AUC）来计算平均精度（AP）。最后，目标检测的mAP是所有类别计算出的AP的平均值。值得注意的是，一些研究论文将AP和mAP互换使用。
- en: Recap
  id: totrans-94
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 概述
- en: 'To recap, the mAP is calculated as follows:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，mAP的计算如下：
- en: Get each bounding box’s associated objectness score (probability of the box
    containing an object).
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取每个边界框关联的对象性分数（该框包含对象的概率）。
- en: Calculate precision and recall.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算精度和召回率。
- en: Compute the PR curve for each class by varying the score threshold.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过改变分数阈值，为每个类别计算PR曲线。
- en: 'Calculate the AP: the area under the PR curve. In this step, the AP is computed
    for each class.'
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算平均精度（AP）：PR曲线下的面积。在这一步，为每个类别计算AP。
- en: 'Calculate the mAP: the average AP over all the different classes.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算平均精度（mAP）：所有不同类别的平均AP。
- en: 'The last thing to note about mAP is that it is more complicated to calculate
    than other traditional metrics like accuracy. The good news is that you don’t
    need to compute mAP values yourself: most DL object detection implementations
    handle computing the mAP for you, as you will see later in this chapter.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 关于mAP的最后一点是，它比其他传统指标（如准确率）更复杂。好消息是您不需要自己计算mAP值：大多数深度学习目标检测实现都为您处理mAP的计算，您将在本章后面看到。
- en: Now that we understand the general framework of object detection algorithms,
    let’s dive deeper into three of the most popular. In this chapter, we will discuss
    the R-CNN family of networks, SSD, and YOLO networks in detail to see how object
    detectors have evolved over time. We will also examine the pros and cons of each
    network so you can choose the most appropriate algorithm for your problem.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了目标检测算法的一般框架，让我们更深入地探讨其中三个最受欢迎的。在本章中，我们将详细讨论R-CNN系列网络、SSD和YOLO网络，以了解目标检测器是如何随着时间的推移而演变的。我们还将检查每个网络的优缺点，以便您可以选择最适合您问题的算法。
- en: 7.2 Region-based convolutional neural networks (R-CNNs)
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 基于区域的卷积神经网络（R-CNNs）
- en: The R-CNN family of object detection techniques usually referred to as R-CNNs,
    which is short for region-based convolutional neural networks, was developed by
    Ross Girshick et al. in 2014.[1](#pgfId-1171351) The R-CNN family expanded to
    include Fast-RCNN[2](#pgfId-1171354) and Faster-RCN[3](#pgfId-1171357) in 2015
    and 2016, respectively. In this section, I’ll quickly walk you through the evolution
    of the R-CNN family from R-CNNs to Fast R-CNN to Faster R-CNN, and then we will
    dive deeper into the Faster R-CNN architecture and code implementation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN系列的目标检测技术通常被称为R-CNNs，即基于区域的卷积神经网络，由Ross Girshick等人于2014年开发。[1](#pgfId-1171351)
    R-CNN系列在2015年和2016年分别扩展到包括Fast-RCNN[2](#pgfId-1171354)和Faster-RCN[3](#pgfId-1171357)。在本节中，我将快速为您介绍R-CNN系列从R-CNN到Fast
    R-CNN再到Faster R-CNN的演变过程，然后我们将更深入地探讨Faster R-CNN架构和代码实现。
- en: 7.2.1 R-CNN
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 R-CNN
- en: R-CNN is the least sophisticated region-based architecture in its family, but
    it is the basis for understanding how multiple object-recognition algorithms work
    for all of them. It was one of the first large, successful applications of convolutional
    neural networks to the problem of object detection and localization, and it paved
    the way for the other advanced detection algorithms. The approach was demonstrated
    on benchmark datasets, achieving then-state-of-the-art results on the PASCAL VOC-2012
    dataset and the ILSVRC 2013 object detection challenge. Figure 7.8 shows a summary
    of the R-CNN model architecture.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN是其家族中最简单的基于区域的架构，但它是理解所有多个目标识别算法如何工作的基础。它是卷积神经网络在目标检测和定位问题上的第一个大型、成功应用之一，并为其他高级检测算法铺平了道路。该方法在基准数据集上进行了演示，在PASCAL
    VOC-2012数据集和ILSVRC 2013目标检测挑战赛上实现了当时最先进的结果。图7.8展示了R-CNN模型架构的总结。
- en: '![](../Images/7-8.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-8.png)'
- en: Figure 7.8 Summary of the R-CNN model architecture. (Modified from Girshick
    et al., “Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation.”)
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 R-CNN模型架构总结。（改编自Girshick等人，“用于准确目标检测和语义分割的丰富特征层次。”）
- en: 'The R-CNN model consists of four components:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN模型由四个部分组成：
- en: '*Extract regions of interest* --Also known as extracting region proposals.
    These regions have a high probability of containing an object. An algorithm called
    selective search scans the input image to find regions that contain blobs, and
    proposes them as RoIs to be processed by the next modules in the pipeline. The
    proposed RoIs are then warped to have a fixed size; they usually vary in size,
    but as we learned in previous chapters, CNNs require a fixed input image size.'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*提取感兴趣区域* --也称为提取区域提议。这些区域有很高的概率包含一个对象。一种称为选择性搜索的算法扫描输入图像以找到包含块的区域，并将它们作为RoIs提议给管道中的下一个模块进行处理。提议的RoIs随后被变形为固定大小；它们通常大小不一，但正如我们在前面的章节中学到的，CNN需要固定大小的输入图像。'
- en: '*Feature extraction module* --We run a pretrained convolutional network on
    top of the region proposals to extract features from each candidate region. This
    is the typical CNN feature extractor that we learned about in previous chapters.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征提取模块* --我们在区域提议上运行一个预训练的卷积网络，以从每个候选区域中提取特征。这是我们之前章节中学到的典型的CNN特征提取器。'
- en: '*Classification module* --We train a classifier like a support vector machine
    (SVM), a traditional machine learning algorithm, to classify candidate detections
    based on the extracted features from the previous step.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类模块* --我们训练一个分类器，如支持向量机（SVM），一种传统的机器学习算法，根据上一步提取的特征对候选检测进行分类。'
- en: '*Localization module* --Also known as a bounding-box regressor. Let’s take
    a step back to understand regression. ML problems are categorized as classification
    or regression problems. Classification algorithms output discrete, predefined
    classes (dog, cat, elephant), whereas regression algorithms output continuous
    value predictions. In this module, we want to predict the location and size of
    the bounding box that surrounds the object. The bounding box is represented by
    identifying four values: the *x* and *y* coordinates of the box’s origin (*x,
    y*), the width, and the height of the box (*w, h*). Putting this together, the
    regressor predicts the four real-valued numbers that define the bounding box as
    the following tuple: (*x, y, w, h*).'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*定位模块* --也称为边界框回归器。让我们回顾一下回归的概念。机器学习问题被分类为分类或回归问题。分类算法输出离散的、预定义的类别（狗、猫、大象），而回归算法输出连续的值预测。在这个模块中，我们想要预测围绕对象的边界框的位置和大小。边界框通过识别四个值来表示：框原点的
    *x* 和 *y* 坐标（*x, y*）、框的宽度和高度（*w, h*）。将这些值组合起来，回归器预测定义边界框的四个实数值，如下面的元组所示：(*x, y,
    w, h*)。'
- en: Selective search
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性搜索
- en: Selective search is a greedy search algorithm that is used to provide region
    proposals that potentially contain objects. It tries to find areas that might
    contain an object by combining similar pixels and textures into rectangular boxes.
    Selective search combines the strength of both the exhaustive search algorithm
    (which examines all possible locations in the image) and the bottom-up segmentation
    algorithm (which hierarchically groups similar regions) to capture all possible
    object locations.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性搜索是一种贪婪搜索算法，用于提供可能包含物体的区域建议。它通过将相似的像素和纹理组合成矩形框来尝试找到可能包含物体的区域。选择性搜索结合了穷举搜索算法（检查图像中所有可能的位置）和从下而上的分割算法（将相似区域分层组合）的优点，以捕捉所有可能的对象位置。
- en: The selective search algorithm works by applying a segmentation algorithm to
    find blobs in an image, in order to figure out what could be an object (see the
    image on the right in the following figure).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性搜索算法通过在图像中应用分割算法来寻找块，以便确定可能是什么物体（参见以下图中右侧的图像）。
- en: '![](../Images/7-unnumb-1.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-unnumb-1.png)'
- en: The selective search algorithm looks for blob-like areas in the image to extract
    regions. At right, the segmentation algorithm defines blobs that could be objects.
    Then the selective search algorithm selects these areas to be passed along for
    further investigation.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 选择性搜索算法在图像中寻找类似块状的区域以提取区域。在右侧，分割算法定义了可能是物体的块。然后选择性搜索算法选择这些区域以供进一步调查。
- en: 'Bottom-up segmentation recursively combines these groups of regions together
    into larger ones to create about 2,000 areas to be investigated, as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 从下而上的分割递归地将这些区域组合成更大的区域，以创建大约2,000个需要调查的区域，如下所示：
- en: The similarities between all neighboring regions are calculated.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算所有相邻区域之间的相似性。
- en: The two most similar regions are grouped together, and new similarities are
    calculated between the resulting region and its neighbors.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将两个最相似的区域组合在一起，并计算结果区域与其邻居之间的新相似性。
- en: This process is repeated until the entire object is covered in a single region.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 此过程会重复进行，直到整个物体被单个区域覆盖。
- en: Note that a review of the selective search algorithm and how it calculates regions’
    similarity is outside the scope of this book. If you are interested in learning
    more
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，选择性搜索算法及其如何计算区域相似性的综述超出了本书的范围。如果你对此感兴趣，可以进一步学习。
- en: about this technique, you can refer to the original paper.a For the purpose
    of understanding R-CNNs, you can treat the selective search algorithm as a black
    box that intelligently scans the image and proposes RoI locations for us to use.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这项技术，你可以参考原始论文。[a] 为了理解R-CNN，你可以将选择性搜索算法视为一个智能扫描图像并为我们提出RoI位置的“黑盒”。
- en: '![](../Images/7-unnumb-2.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-unnumb-2.png)'
- en: An example of bottom-up segmentation using the selective search algorithm. It
    combines similar regions in every iteration until the entire object is covered
    in a single region.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择性搜索算法的从下而上的分割示例。它通过在每次迭代中组合相似区域，直到整个物体被单个区域覆盖。
- en: J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, and A.W.M. Smeulders, “Selective
    Search for Object Recognition,” 2012, [www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: J.R.R. Uijlings, K.E.A. van de Sande, T. Gevers, 和 A.W.M. Smeulders, “选择性搜索用于物体识别,”
    2012, [www.huppelen.nl/publications/selectiveSearchDraft.pdf](http://www.huppelen.nl/publications/selectiveSearchDraft.pdf).
- en: Figure 7.9 illustrates the R-CNN architecture in an intuitive way. As you can
    see, the network first proposes RoIs, then extracts features, and then classifies
    those regions based on their features. In essence, we have turned object detection
    into an image classification problem.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9以直观的方式展示了R-CNN架构。如图所示，网络首先提出RoIs，然后提取特征，接着根据这些特征对这些区域进行分类。本质上，我们将物体检测转化为一个图像分类问题。
- en: '![](../Images/7-9.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-9.png)'
- en: Figure 7.9 Illustration of the R-CNN architecture. Each proposed RoI is passed
    through the CNN to extract features, followed by a bounding-box regressor and
    an SVM classifier to produce the network output prediction.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 R-CNN架构的示意图。每个提出的RoI都会通过CNN提取特征，然后通过边界框回归器和SVM分类器产生网络输出预测。
- en: Training R-CNNs
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 训练R-CNN
- en: 'We learned in the previous section that R-CNNs are composed of four modules:
    selective search region proposal, feature extractor, classifier, and bounding-box
    regressor. All of the R-CNN modules need to be trained except the selective search
    algorithm. So, in order to train R-CNNs, we need to do the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解到R-CNN由四个模块组成：选择性搜索区域提议、特征提取器、分类器和边界框回归器。除了选择性搜索算法外，所有R-CNN模块都需要进行训练。因此，为了训练R-CNN，我们需要做以下几步：
- en: Train the feature extractor CNN. This is a typical CNN training process. We
    either train a network from scratch, which rarely happens, or fine-tune a pretrained
    network, as we learned to do in chapter 6.
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练特征提取器CNN。这是一个典型的CNN训练过程。我们或者从头开始训练一个网络，这种情况很少发生，或者微调一个预训练的网络，正如我们在第6章中学到的那样。
- en: Train the SVM classifier. The SVM algorithm is not covered in this book, but
    it is a traditional ML classifier that is no different from DL classifiers in
    the sense that it needs to be trained on labeled data.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练SVM分类器。SVM算法在本书中没有涉及，但它是一种传统的机器学习分类器，在需要训练标记数据的意义上与深度学习分类器没有区别。
- en: Train the bounding-box regressors. This model outputs four real-valued numbers
    for each of the K object classes to tighten the region bounding boxes.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练边界框回归器。该模型为每个K个物体类别输出四个实数值，以缩小区域边界框。
- en: Looking through the R-CNN learning steps, you could easily find out that training
    an R-CNN model is expensive and slow. The training process involves training three
    separate modules without much shared computation. This multistage pipeline training
    is one of the disadvantages of R-CNNs, as we will see next.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看R-CNN的学习步骤，你可以很容易地发现训练R-CNN模型既昂贵又缓慢。训练过程涉及训练三个独立的模块，共享计算很少。这种多阶段管道训练是R-CNN的一个缺点，我们将在下一节中看到。
- en: Disadvantages of R-CNN
  id: totrans-137
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R-CNN的缺点
- en: 'R-CNN is very simple to understand, and it achieved state-of-the-art results
    when it first came out, especially when using deep ConvNets to extract features.
    However, it is not actually a single end-to-end system that learns to localize
    via a deep neural network. Rather, it is a combination of standalone algorithms,
    added together to perform object detection. As a result, it has the following
    notable drawbacks:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN非常易于理解，并且它在首次推出时取得了最先进的结果，尤其是在使用深度卷积神经网络提取特征时。然而，它实际上不是一个通过深度神经网络学习定位的单个端到端系统。相反，它是由多个独立算法组合而成的，共同执行物体检测。因此，它有以下显著的缺点：
- en: Object detection is very slow. For each image, the selective search algorithm
    proposes about 2,000 RoIs to be examined by the entire pipeline (CNN feature extractor
    and classifier). This is very computationally expensive because it performs a
    ConvNet forward pass for each object proposal without sharing computation, which
    makes it incredibly slow. This high computation need means R-CNN is not a good
    fit for many applications, especially real-time applications that require fast
    inferences like self-driving cars and many others.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体检测非常慢。对于每张图像，选择性搜索算法会提出大约2,000个RoI供整个管道（CNN特征提取器和分类器）检查。由于它对每个物体提议执行卷积网络前向传递而不共享计算，这导致计算量非常大，因此非常慢。这种高计算需求意味着R-CNN不适合许多应用，尤其是需要快速推理的应用，如自动驾驶汽车等。
- en: 'Training is a multi-stage pipeline. As discussed earlier, R-CNNs require the
    training of three modules: CNN feature extractor, SVM classifier, and bounding-box
    regressors. Thus the training process is very complex and not an end-to-end training.'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练是一个多阶段管道。如前所述，R-CNN需要训练三个模块：CNN特征提取器、SVM分类器和边界框回归器。因此，训练过程非常复杂，不是一个端到端训练。
- en: Training is expensive in terms of space and time. When training the SVM classifier
    and bounding-box regressor, features are extracted from each object proposal in
    each image and written to disk. With very deep networks, such as VGG16, the training
    process for a few thousand images takes days using GPUs. The training process
    is expensive in space as well, because the extracted features require hundreds
    of gigabytes of storage.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练在空间和时间上都很昂贵。当训练SVM分类器和边界框回归器时，从每张图像中的每个对象提议中提取特征并写入磁盘。对于像VGG16这样的非常深的网络，使用GPU对几千张图像的训练过程需要几天时间。从空间上讲，训练过程也很昂贵，因为提取的特征需要数百GB的存储空间。
- en: What we need is an end-to-end DL system that fixes the disadvantages of R-CNN
    while improving its speed and accuracy.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一个端到端深度学习系统，它修正了R-CNN的缺点，同时提高了其速度和准确性。
- en: 7.2.2 Fast R-CNN
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 Fast R-CNN
- en: 'Fast R-CNN was an immediate descendant of R-CNN, developed in 2015 by Ross
    Girshick. Fast R-CNN resembled the R-CNN technique in many ways but improved on
    its detection speed while also increasing detection accuracy through two main
    changes:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN是R-CNN的直接后裔，由Ross Girshick于2015年开发。Fast R-CNN在许多方面与R-CNN技术相似，但在检测速度上有所改进，同时通过两个主要变化提高了检测准确性：
- en: Instead of starting with the regions proposal module and then using the feature
    extraction module, like R-CNN, Fast-RCNN proposes that we apply the CNN feature
    extractor first to the entire input image and then propose regions. This way,
    we run only one ConvNet over the entire image instead of 2,000 ConvNets over 2,000
    overlapping regions.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与R-CNN不同，它不是从区域提议模块开始，然后使用特征提取模块，Fast-RCNN建议我们首先将CNN特征提取器应用于整个输入图像，然后提出区域。这样，我们只需在整个图像上运行一个ConvNet，而不是在2,000个重叠区域上运行2,000个ConvNet。
- en: 'It extends the ConvNet’s job to do the classification part as well, by replacing
    the traditional SVM machine learning algorithm with a softmax layer. This way,
    we have a single model to perform both tasks: feature extraction and object classification.'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将ConvNet的任务扩展到分类部分，通过用softmax层替换传统的SVM机器学习算法来实现。这样，我们有一个单一模型来执行两个任务：特征提取和对象分类。
- en: Fast R-CNN architecture
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Fast R-CNN架构
- en: As shown in figure 7.10, Fast R-CNN generates region proposals based on the
    last feature map of the network, not from the original image like R-CNN. As a
    result, we can train just one ConvNet for the entire image. In addition, instead
    of training many different SVM algorithms to classify each object class, a single
    softmax layer outputs the class probabilities directly. Now we only have one neural
    net to train, as opposed to one neural net and many SVMs.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.10所示，Fast R-CNN基于网络的最后一个特征图生成区域提议，而不是像R-CNN那样从原始图像中生成。因此，我们只需为整个图像训练一个ConvNet。此外，我们不再需要训练许多不同的SVM算法来分类每个对象类别，而是单个softmax层直接输出类别概率。现在我们只有一个神经网络需要训练，而不是一个神经网络和多个SVM。
- en: 'The architecture of Fast R-CNN consists of the following modules:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN的架构由以下模块组成：
- en: Feature extractor module --The network starts with a ConvNet to extract features
    from the full image.
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 特征提取模块 -- 网络从全图像开始，使用ConvNet提取特征。
- en: RoI extractor --The selective search algorithm proposes about 2,000 region candidates
    per image.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoI提取器 -- 选择性搜索算法为每张图像提出大约2,000个区域候选。
- en: RoI pooling layer --This is a new component that was introduced to extract a
    fixed-size window from the feature map before feeding the RoIs to the fully connected
    layers. It uses max pooling to convert the features inside any valid RoI into
    a small feature map with a fixed spatial extent of height × width (H × W ). The
    RoI pooling layer will be explained in more detail in the Faster R-CNN section;
    for now, understand that it is applied on the last feature map layer extracted
    from the CNN, and its goal is to extract fixed-size RoIs to feed to the fully
    connected layers and then the output layers.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RoI池化层 -- 这是一个新组件，用于在将RoIs馈送到全连接层之前从特征图中提取一个固定大小的窗口。它使用最大池化将任何有效RoI内的特征转换为具有固定空间范围高度×宽度（H×W）的小特征图。RoI池化层将在Faster
    R-CNN部分中更详细地解释；现在，了解它应用于从CNN提取的最后一个特征图层，其目标是提取固定大小的RoIs以馈送到全连接层和输出层。
- en: 'Two-head output layer --The model branches into two heads:'
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 双头输出层 --模型分为两个头部：
- en: A softmax classifier layer that outputs a discrete probability distribution
    per RoI
  id: totrans-154
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 softmax 分类层，输出每个 RoI 的离散概率分布
- en: A bounding-box regressor layer to predict offsets relative to the original RoI
  id: totrans-155
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个边界框回归层，用于预测相对于原始 RoI 的偏移量
- en: '![](../Images/7-10.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-10.png)'
- en: Figure 7.10 The Fast R-CNN architecture consists of a feature extractor ConvNet,
    RoI extractor, RoI pooling layers, fully connected layers, and a two-head output
    layer. Note that, unlike R-CNNs, Fast R-CNNs apply the feature extractor to the
    entire input image before applying the regions proposal module.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.10 快速 R-CNN 架构由特征提取器 ConvNet、RoI 提取器、RoI 池化层、全连接层和双头输出层组成。注意，与 R-CNN 不同，快速
    R-CNN 在应用区域建议模块之前，将特征提取器应用于整个输入图像。
- en: Multi-task loss function in Fast R-CNNs
  id: totrans-158
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 快速 R-CNN 中的多任务损失函数
- en: Since Fast R-CNN is an end-to-end learning architecture to learn the class of
    an object as well as the associated bounding box position and size, the loss is
    multi-task loss. With multi-task loss, the output has the softmax classifier and
    bounding-box regressor, as shown in figure 7.10.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 由于快速 R-CNN 是一个端到端学习架构，用于学习对象的类别以及相关的边界框位置和大小，因此损失是多任务损失。在多任务损失的情况下，输出有 softmax
    分类器和边界框回归器，如图 7.10 所示。
- en: 'In any optimization problem, we need to define a loss function that our optimizer
    algorithm is trying to minimize. (Chapter 2 gives more details about optimization
    and loss functions.) In object detection problems, our goal is to optimize for
    two goals: object classification and object localization. Therefore, we have two
    loss functions in this problem: Lcls for the classification loss and Lloc for
    the bounding box prediction defining the object location.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何优化问题中，我们需要定义一个损失函数，我们的优化器算法试图最小化它。（第 2 章提供了更多关于优化和损失函数的细节。）在目标检测问题中，我们的目标是优化两个目标：对象分类和对象定位。因此，在这个问题中，我们有两个损失函数：Lcls
    用于分类损失，Lloc 用于定义对象位置的边界框预测。
- en: 'A Fast R-CNN network has two sibling output layers with two loss functions:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 快速 R-CNN 网络有两个兄弟输出层，带有两个损失函数：
- en: Classification --The first outputs a discrete probability distribution (per
    RoI) over K + 1 categories (we add one class for the background). The probability
    P is computed by a softmax over the *K* + 1 outputs of a fully connected layer.
    The classification loss function is a log loss for the true class u
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类 --第一个输出一个离散概率分布（每个 RoI），覆盖 K + 1 个类别（我们为背景添加一个类别）。概率 P 是通过全连接层的 *K* + 1 个输出的
    softmax 计算得到的。分类损失函数是对真实类别 u 的对数损失。
- en: '*L[cls]*(*p,u*) = −log*p[u]*'
  id: totrans-163
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*L[cls]*(*p,u*) = −log*p[u]*'
- en: where u is the true label, *u* ∈ 0, 1, 2, . . . (*K* + 1); where *u* = 0 is
    the background; and *p* is the discrete probability distribution per RoI over
    *K* + 1 classes.
  id: totrans-164
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中 u 是真实标签，*u* ∈ 0, 1, 2, . . . (*K* + 1)；其中 *u* = 0 表示背景；而 *p* 是每个 RoI 在 *K*
    + 1 个类别上的离散概率分布。
- en: Regression --The second sibling layer outputs bounding box regression offsets
    *v* = (*x, y, w, h*) for each of the *K* object classes. The loss function is
    the loss for bounding box for class u
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归 --第二个兄弟层输出每个 *K* 个对象类别的边界框回归偏移 *v* = (*x, y, w, h*)。损失函数是类别 u 的边界框损失。
- en: '*L[loc](t^u, u) = σ L1[smooth](t[i]^u - v[i])*'
  id: totrans-166
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*L[loc](t^u, u) = σ L1[smooth](t[i]^u - v[i])*'
- en: 'where:'
  id: totrans-167
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中：
- en: v is the true bounding box, *v* = (*x, y, w, h*).
  id: totrans-168
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: v 是真实边界框，*v* = (*x, y, w, h*)。
- en: '*t* u is the prediction bounding box correction:'
  id: totrans-169
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*t* u 是预测边界框校正：'
- en: '*t^u* = (*t[x]^u, t[y]^u, t[w]^u, t[h]^u*)'
  id: totrans-170
  prefs:
  - PREF_IND
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*t^u* = (*t[x]^u, t[y]^u, t[w]^u, t[h]^u*)'
- en: '*L1[smooth]* is the bounding box loss that measures the difference between
    tiu and vi using the smooth L1 loss function. It is a robust function and is claimed
    to be less sensitive to outliers than other regression losses like L2.'
  id: totrans-171
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*L1[smooth]* 是使用平滑 L1 损失函数来衡量 tiu 和 vi 之间差异的边界框损失。它是一个鲁棒函数，据称比其他回归损失（如 L2）对异常值更不敏感。'
- en: The overall loss function is
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 总体损失函数是
- en: '*L = L[cls] + L[loc]*'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '*L = L[cls] + L[loc]*'
- en: '*L*(*p, u, t^u, v*) = *L[cls]*(*p, u*) + [*u* ≥ 1] *l[box]*(*t^u, v*)'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '*L*(*p, u, t^u, v*) = *L[cls]*(*p, u*) + [*u* ≥ 1] *l[box]*(*t^u, v*)'
- en: Note that [*u* ≥ 1] is added before the regression loss to indicate 0 when the
    region inspected doesn’t contain any object and contains a background. It is a
    way of ignoring the bounding box regression when the classifier labels the region
    as a background. The indicator function [*u* ≥ 1] is defined as
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，[*u* ≥ 1] 在回归损失之前添加，表示当检查的区域不包含任何对象且包含背景时为 0。这是一种在分类器将区域标记为背景时忽略边界框回归的方法。指示函数
    [*u* ≥ 1] 定义为
- en: '![](../Images/7-12_F1.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-12_F1.png)'
- en: Disadvantages of Fast R-CNN
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Fast R-CNN的缺点
- en: 'Fast R-CNN is much faster in terms of testing time, because we don’t have to
    feed 2,000 region proposals to the convolutional neural network for every image.
    Instead, a convolution operation is done only once per image, and a feature map
    is generated from it. Training is also faster because all the components are in
    one CNN network: feature extractor, object classifier, and bounding-box regressor.
    However, there is a big bottleneck remaining: the selective search algorithm for
    generating region proposals is very slow and is generated separately by another
    model. The last step to achieve a complete end-to-end object detection system
    using DL is to find a way to combine the region proposal algorithm into our end-to-end
    DL network. This is what Faster R-CNN does, as we will see next.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: Fast R-CNN在测试时间上要快得多，因为我们不需要为每张图像将2,000个区域提议输入到卷积神经网络中。相反，每个图像只进行一次卷积操作，并从中生成一个特征图。训练也更快，因为所有组件都在一个CNN网络中：特征提取器、物体分类器和边界框回归器。然而，仍然存在一个大的瓶颈：用于生成区域提议的选择搜索算法非常慢，并且由另一个模型单独生成。使用深度学习实现完整端到端物体检测系统的最后一步是找到一种方法将区域提议算法结合到我们的端到端深度学习网络中。这就是Faster
    R-CNN所做的事情，我们将在下面看到。
- en: 7.2.3 Faster R-CNN
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 Faster R-CNN
- en: Faster R-CNN is the third iteration of the R-CNN family, developed in 2016 by
    Shaoqing Ren et al. Similar to Fast R-CNN, the image is provided as an input to
    a convolutional network that provides a convolutional feature map. Instead of
    using a selective search algorithm on the feature map to identify the region proposals,
    a region proposal network (RPN) is used to predict the region proposals as part
    of the training process. The predicted region proposals are then reshaped using
    an RoI pooling layer and used to classify the image within the proposed region
    and predict the offset values for the bounding boxes. These improvements both
    reduce the number of region proposals and accelerate the test-time operation of
    the model to near real-time with then-state-of-the-art performance.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN是R-CNN家族的第三次迭代，由Shaoqing Ren等人于2016年开发。与Fast R-CNN类似，图像被提供给一个卷积网络，该网络提供卷积特征图。而不是在特征图上使用选择搜索算法来识别区域提议，使用区域提议网络（RPN）来预测作为训练过程一部分的区域提议。然后使用RoI池化层重塑预测的区域提议，并在提议的区域中对图像进行分类，并预测边界框的偏移值。这些改进既减少了区域提议的数量，又加速了模型的测试时间操作，接近实时，并具有当时最先进的性能。
- en: Faster R-CNN architecture
  id: totrans-181
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: Faster R-CNN架构
- en: 'The architecture of Faster R-CNN can be described using two main networks:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Faster R-CNN的架构可以用两个主要网络来描述：
- en: 'Region proposal network (RPN) --Selective search is replaced by a ConvNet that
    proposes RoIs from the last feature maps of the feature extractor to be considered
    for investigation. The RPN has two outputs: the objectness score (object or no
    object) and the box location.'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域提议网络（RPN）--选择搜索被一个卷积网络所取代，该网络从特征提取器的最后特征图中提出RoIs以供调查。RPN有两个输出：物体分数（物体或无物体）和边界框位置。
- en: 'Fast R-CNN --It consists of the typical components of Fast R-CNN:'
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast R-CNN --它由Fast R-CNN的典型组件组成：
- en: 'Base network for the feature extractor: a typical pretrained CNN model to extract
    features from the input image'
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取器的基网络：一个典型的预训练CNN模型，用于从输入图像中提取特征。
- en: RoI pooling layer to extract fixed-size RoIs
  id: totrans-186
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoI池化层用于提取固定大小的RoIs
- en: 'Output layer that contains two fully connected layers: a softmax classifier
    to output the class probability and a bounding box regression CNN for the bounding
    box predictions'
  id: totrans-187
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层包含两个全连接层：一个softmax分类器用于输出类别概率，一个边界框回归CNN用于预测边界框。
- en: 'As you can see in figure 7.11, the input image is presented to the network,
    and its features are extracted via a pretrained CNN. These features, in parallel,
    are sent to two different components of the Faster R-CNN architecture:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.11所示，输入图像被提供给网络，并通过预训练的CNN提取其特征。这些特征并行发送到Faster R-CNN架构的两个不同组件：
- en: The RPN to determine where in the image a potential object could be. At this
    point, we do not know what the object is, just that there is potentially an object
    at a certain location in the image.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RPN用于确定图像中可能存在物体的位置。在此阶段，我们并不知道物体是什么，只知道图像的某个位置可能存在一个潜在的物体。
- en: RoI pooling to extract fixed-size windows of features.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoI池化用于提取固定大小的特征窗口。
- en: '![](../Images/7-11.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-11.png)'
- en: 'Figure 7.11 The Faster R-CNN architecture has two main components: an RPN that
    identifies regions that may contain objects of interest and their approximate
    location, and a Fast R-CNN network that classifies objects and refines their location
    defined using bounding boxes. The two components share the convolutional layers
    of the pretrained VGG16.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 Faster R-CNN架构有两个主要组件：一个RPN，用于识别可能包含感兴趣对象及其大致位置的区域，以及一个Fast R-CNN网络，用于分类对象并细化使用边界框定义的位置。这两个组件共享预训练的VGG16的卷积层。
- en: 'The output is then passed into two fully connected layers: one for the object
    classifier and one for the bounding box coordinate predictions to obtain our final
    localizations.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将输出传递到两个全连接层：一个用于物体分类器，一个用于边界框坐标预测，以获得我们的最终定位。
- en: 'This architecture achieves an end-to-end trainable, complete object detection
    pipeline where all of the required components are inside the network:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 该架构实现了一个端到端可训练的完整目标检测流程，其中所有必需的组件都在网络内部：
- en: Base network feature extractor
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础网络特征提取器
- en: Regions proposal
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 区域建议
- en: RoI pooling
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RoI池化
- en: Object classification
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体分类
- en: Bounding-box regressor
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 边界框回归器
- en: Base network to extract features
  id: totrans-200
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 从基础网络提取特征
- en: Similar to Fast R-CNN, the first step is to use a pretrained CNN and slice off
    its classification part. The base network is used to extract features from the
    input image. We covered how this works in detail in chapter 6\. In this component,
    you can use any of the popular CNN architectures based on the problem you are
    trying to solve. The original Faster R-CNN paper used ZF[4](#pgfId-1171567) and
    VGG[5](#pgfId-1171570) pretrained networks on ImageNet; but since then, there
    have been lots of different networks with a varying number of weights. For example,
    MobileNet,[6](#pgfId-1171573) a smaller and efficient network architecture optimized
    for speed, has approximately 3.3 million parameters, whereas ResNet-152 (152 layers)--once
    the state of the art in the ImageNet classification competition--has around 60
    million. Most recently, new architectures like DenseNet[7](#pgfId-1171576) are
    both improving results and reducing the number of parameters.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 与Fast R-CNN类似，第一步是使用预训练的CNN并切掉其分类部分。基础网络用于从输入图像中提取特征。我们在第6章详细介绍了这一点。在这个组件中，你可以根据你试图解决的问题使用任何流行的CNN架构。原始Faster
    R-CNN论文使用了在ImageNet上预训练的ZF[4](#pgfId-1171567)和VGG[5](#pgfId-1171570)网络；但自那时起，已经出现了许多具有不同数量权重的不同网络。例如，MobileNet[6](#pgfId-1171573)，一个更小、更高效的针对速度优化的网络架构，大约有330万个参数，而ResNet-152（152层）--曾是ImageNet分类竞赛的顶尖技术--大约有6000万个。最近，新的架构如DenseNet[7](#pgfId-1171576)在提高结果的同时，也在减少参数数量。
- en: VGGNet vs. ResNet
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: VGGNet与ResNet
- en: Nowadays, ResNet architectures have mostly replaced VGG as a base network for
    extracting features. The obvious advantage of ResNet over VGG is that it has many
    more layers (is deeper), giving it more capacity to learn very complex features.
    This is true for the classification task and should be equally true in the case
    of object detection. In addition, ResNet makes it easy to train deep models with
    the use of residual connections and batch normalization, which was not invented
    when VGG was first released. Please revisit chapter 5 for a more detailed review
    of the different CNN architectures.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，ResNet架构大多已取代VGG作为提取特征的基础网络。ResNet相对于VGG的明显优势是它有更多的层（更深），这使它能够学习非常复杂的功能。这在分类任务中是正确的，在目标检测的情况下也应该同样正确。此外，ResNet通过使用残差连接和批量归一化（这是在VGG首次发布时没有发明的）来训练深度模型变得容易。请回顾第5章，以获得不同CNN架构的更详细审查。
- en: As we learned in earlier chapters, each convolutional layer creates abstractions
    based on the previous information. The first layer usually learns edges, the second
    finds patterns in edges to activate for more complex shapes, and so forth. Eventually
    we end up with a convolutional feature map that can be fed to the RPN to extract
    regions that contain objects.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中学到的，每个卷积层都是基于之前的信息创建抽象。第一层通常学习边缘，第二层在边缘中寻找激活以形成更复杂形状的模式，依此类推。最终我们得到一个卷积特征图，可以输入到RPN中以提取包含对象的区域。
- en: Region proposal network (RPN)
  id: totrans-205
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 区域建议网络（RPN）
- en: The RPN identifies regions that could potentially contain objects of interest,
    based on the last feature map of the pretrained convolutional neural network.
    An RPN is also known as an attention network because it guides the network’s attention
    to interesting regions in the image. Faster R-CNN uses an RPN to bake the region
    proposal directly into the R-CNN architecture instead of running a selective search
    algorithm to extract RoIs.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: RPN根据预训练卷积神经网络的最后一个特征图识别可能包含感兴趣对象的区域。RPN也被称为注意力网络，因为它引导网络将注意力集中在图像中的有趣区域。Faster
    R-CNN使用RPN将区域提议直接嵌入到R-CNN架构中，而不是运行选择性搜索算法来提取RoIs。
- en: 'The architecture of the RPN is composed of two layers (figure 7.12):'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: RPN的架构由两层组成（图7.12）：
- en: A 3 × 3 fully convolutional layer with 512 channels
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有512个通道的3 × 3全卷积层
- en: 'Two parallel 1 × 1 convolutional layers: a classification layer that is used
    to predict whether the region contains an object (the score of it being background
    or foreground), and a layer for regression or bounding box prediction.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两个平行的1 × 1卷积层：一个分类层，用于预测区域是否包含对象（其分数为背景或前景），以及一个用于回归或边界框预测的层。
- en: Fully convolutional networks (FCNs)
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 全卷积网络（FCNs）
- en: One important aspect of object detection networks is that they should be fully
    convolutional. A fully convolutional neural network means that the network does
    not contain any fully connected layers, typically found at the end of a network
    prior to making output predictions.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 物体检测网络的一个重要方面是它们应该是全卷积的。全卷积神经网络意味着网络不包含任何全连接层，通常在网络末尾用于输出预测之前。
- en: 'In the context of image classification, removing the fully connected layers
    is normally accomplished by applying average pooling across the entire volume
    prior to using a single dense softmax classifier to output the final predictions.
    An FCN has two main benefits:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像分类的背景下，通过在整个体积上应用平均池化来移除全连接层，然后在使用单个密集softmax分类器输出最终预测之前。FCN有两个主要优点：
- en: It is faster because it contains only convolution operations and no fully connected
    layers.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它更快，因为它只包含卷积操作，没有全连接层。
- en: It can accept images of any spatial resolution (width and height), provided
    the image and network can fit into the available memory.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以接受任何空间分辨率的图像（宽度和高度），只要图像和网络可以适应可用的内存。
- en: Being an FCN makes the network invariant to the size of the input image. However,
    in practice, we might want to stick to a constant input size due to issues that
    only become apparent when we are implementing the algorithm. A significant such
    problem is that if we want to process images in batches (because images in batches
    can be processed in parallel by the GPU, leading to speed boosts), all of the
    images must have a fixed height and width.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 作为FCN，使网络对输入图像的大小不变。然而，在实践中，我们可能希望坚持一个固定的输入大小，因为只有在我们实现算法时才会出现的问题才会变得明显。这样一个重大问题是，如果我们想批量处理图像（因为批量的图像可以通过GPU并行处理，从而提高速度），所有图像都必须具有固定的高度和宽度。
- en: '![](../Images/7-12.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-12.png)'
- en: Figure 7.12 Convolutional implementation of an RPN architecture, where k is
    the number of anchors
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 RPN架构的卷积实现，其中k是锚的数量
- en: 'The 3 × 3 convolutional layer is applied on the last feature map of the base
    network where a sliding window of size 3 × 3 is passed over the feature map. The
    output is then passed to two 1 × 1 convolutional layers: a classifier and a bounding-box
    regressor. Note that the classifier and the regressor of the RPN are not trying
    to predict the class of the object and its bounding box; this comes later, after
    the RPN. Remember, the goal of the RPN is to determine whether the region has
    an object to be investigated afterward by the fully connected layers. In the RPN,
    we use a binary classifier to predict the objectness score of the region, to determine
    the probability of this region being a foreground (contains an object) or a background
    (doesn’t contain an object). It basically looks at the region and asks, “Does
    this region contain an object?” If the answer is yes, then the region is passed
    along for further investigation by RoI pooling and the final output layers (see
    figure 7.13).'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 3 × 3卷积层应用于基础网络的最后一个特征图，其中3 × 3大小的滑动窗口在特征图上移动。然后，输出被传递到两个1 × 1卷积层：一个分类器和边界框回归器。请注意，RPN的分类器和回归器并不是试图预测对象及其边界框的类别；这将在RPN之后进行，即在RPN之后。记住，RPN的目标是确定该区域是否有对象需要由全连接层之后进一步研究。在RPN中，我们使用二进制分类器来预测区域的物体得分，以确定该区域是前景（包含对象）还是背景（不包含对象）的概率。它基本上是查看该区域并问，“这个区域包含对象吗？”如果答案是肯定的，那么该区域将通过RoI池化和最终输出层进行进一步研究（见图7.13）。
- en: '![](../Images/7-13.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-13.png)'
- en: Figure 7.13 The RPN classifier predicts the objectness score, which is the probability
    of an image containing an object (foreground) or a background.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 RPN分类器预测目标得分，即图像包含对象（前景）或背景的概率。
- en: How does the regressor predict the bounding box?
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 回归器是如何预测边界框的？
- en: To answer this question, let’s first define the bounding box. It is the box
    that surrounds the object and is identified by the tuple (*x, y, w, h*), where
    *x* and *y* are the coordinates in the image that describes the center of the
    bounding box and h and w are the height and width of the bounding box. Researchers
    have found that defining the (*x, y*) coordinates of the center point can be challenging
    because we have to enforce some rules to make sure the network predicts values
    inside the boundaries of the image. Instead, we can create reference boxes called
    anchor boxes in the image and make the regression layer predict offsets from these
    boxes called deltas (*Δx, Δy, Δw, Δh*) to adjust the anchor boxes to better fit
    the object to get final proposals (figure 7.14).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答这个问题，我们首先定义边界框。它是围绕对象的框，由元组(*x, y, w, h*)标识，其中*x*和*y*是图像中的坐标，描述边界框的中心，h和w是边界框的高度和宽度。研究人员发现，定义中心点的(*x,
    y*)坐标可能具有挑战性，因为我们必须强制执行一些规则以确保网络预测的值在图像边界内。相反，我们可以在图像中创建称为锚框的参考框，并使回归层预测从这些框的偏移量，称为delta
    (*Δx, Δy, Δw, Δh*)，以调整锚框以更好地适应对象并获得最终提议（见图7.14）。
- en: '![](../Images/7-14.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-14.png)'
- en: Figure 7.14 Illustration of predicting the delta shift from the anchor boxes
    and the bounding box coordinates
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14展示了从锚框预测delta偏移量和边界框坐标的说明
- en: Anchor boxes
  id: totrans-225
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 锚框
- en: Using a sliding window approach, the RPN generates k regions for each location
    in the feature map. These regions are represented as anchor boxes. The anchors
    are centered in the middle of their corresponding sliding window and differ in
    terms of scale and aspect ratio to cover a wide variety of objects. They are fixed
    bounding boxes that are placed throughout the image to be used for reference when
    first predicting object locations. In their paper, Ren et. al. generated nine
    anchor boxes that all had the same center but that had three different aspect
    ratios and three different scales.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滑动窗口方法，RPN为特征图中的每个位置生成k个区域。这些区域表示为锚框。锚位于其对应滑动窗口的中间，在尺寸和宽高比方面有所不同，以覆盖广泛的各种对象。它们是固定边界框，放置在整个图像中，用于在首次预测对象位置时作为参考。在其论文中，Ren等人生成了九个锚框，它们都具有相同的中心，但具有三种不同的宽高比和三种不同的尺寸。
- en: Figure 7.15 shows an example of how anchor boxes are applied. Anchors are at
    the center of the sliding windows; each window has k anchor boxes with the anchor
    at their center.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15展示了锚框应用的一个示例。锚位于滑动窗口的中心；每个窗口有k个锚框，锚位于其中心。
- en: '![](../Images/7-15.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-15.png)'
- en: Figure 7.15 Anchors are at the center of each sliding window. IoU is calculated
    to select the bounding box that overlaps the most with the ground truth.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 锚点位于每个滑动窗口的中心。IoU被计算出来以选择与真实值重叠最多的边界框。
- en: Training the RPN
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 训练RPN
- en: The RPN is trained to classify an anchor box to output an objectness score and
    to approximate the four coordinates of the object (location parameters). It is
    trained using human annotators to label the bounding boxes. A labeled box is called
    the ground truth.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: RPN被训练来对锚框进行分类，输出物体存在分数，并近似物体的四个坐标（位置参数）。它使用人工标注员来标注边界框进行训练。标注的框被称为真实值。
- en: 'For each anchor box, the overlap probability value (*p*) is computed, which
    indicates how much these anchors overlap with the ground-truth bounding boxes:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个锚框，计算重叠概率值（*p*），这表示这些锚框与真实值边界框重叠的程度：
- en: '![](../Images/7-17_F1.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-17_F1.png)'
- en: If an anchor has high overlap with a ground-truth bounding box, then it is likely
    that the anchor box includes an object of interest, and it is labeled as positive
    with respect to the object versus no object classification task. Similarly, if
    an anchor has small overlap with a ground-truth bounding box, it is labeled as
    negative. During the training process, the positive and negative anchors are passed
    as input to two fully connected layers corresponding to the classification of
    anchors as containing an object or no object, and to the regression of location
    parameters (four coordinates), respectively. Corresponding to the k number of
    anchors from a location, the RPN network outputs 2k scores and 4k coordinates.
    Thus, for example, if the number of anchors per sliding window (k) is 9, then
    the RPN outputs 18 objectness scores and 36 location coordinates (figure 7.16).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个锚框与真实值边界框的重叠度很高，那么它很可能包含感兴趣的对象，并且在与无对象分类任务中标记为正。同样，如果一个锚框与真实值边界框的重叠度很小，它将被标记为负。在训练过程中，正负锚框作为输入传递到两个全连接层，分别对应于锚框是否包含对象的分类以及位置参数（四个坐标）的回归。对应于位置的一个锚框数量（k），RPN网络输出2k个分数和4k个坐标。例如，如果每个滑动窗口的锚框数量（k）是9，那么RPN输出18个物体存在分数和36个位置坐标（图7.16）。
- en: RPN as a standalone application
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: RPN作为一个独立的应用程序
- en: An RPN can be used as a standalone application. For example, in problems with
    a single class of objects, the objectness probability can be used as the final
    class probability. This is because in such a case, foreground means single class,
    and background means not a single class.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: RPN可以用作独立的应用程序。例如，在只有一个对象类别的問題中，可以使用物体存在概率作为最终的类别概率。这是因为在这种情况下，前景意味着单个类别，而背景意味着不是单个类别。
- en: The reason you would want to use RPN for cases like single-class detection is
    the gain in speed in both training and prediction. Since the RPN is a very simple
    network that only uses convolutional layers, the prediction time can be faster
    than using the classification base network.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要使用RPN处理单类检测等案例的原因是它在训练和预测过程中的速度提升。由于RPN是一个非常简单的网络，仅使用卷积层，其预测时间可以比使用分类基础网络更快。
- en: '![](../Images/7-16.png)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-16.png)'
- en: Figure 7.16 Region proposal network
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 区域建议网络
- en: Fully connected layer
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 全连接层
- en: 'The output fully connected layer takes two inputs: the feature maps coming
    from the base ConvNet and the RoIs coming from the RPN. It then classifies the
    selected regions and outputs their prediction class and the bounding box parameters.
    The object classification layer in Faster R-CNN uses softmax activation, while
    the location regression layer uses linear regression over the coordinates defining
    the location as a bounding box. All of the network parameters are trained together
    using multi-task loss.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输出全连接层接收两个输入：来自基础ConvNet的特征图和来自RPN的RoIs。然后它对选定的区域进行分类，并输出它们的预测类别和边界框参数。Faster
    R-CNN中的物体分类层使用softmax激活，而位置回归层使用线性回归来定义边界框的位置坐标。所有网络参数都使用多任务损失一起进行训练。
- en: Multi-task loss function
  id: totrans-242
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多任务损失函数
- en: 'Similar to Fast R-CNN, Faster R-CNN is optimized for a multi-task loss function
    that combines the losses of classification and bounding box regression:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与Fast R-CNN类似，Faster R-CNN针对一个多任务损失函数进行了优化，该函数结合了分类和边界框回归的损失：
- en: '![](../Images/7-18_F1.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-18_F1.png)'
- en: The loss equation might look a little overwhelming at first, but it is simpler
    than it appears. Understanding it is not necessary to be able to run and train
    Faster R-CNNs, so feel free to skip this section. But I encourage you to power
    through this explanation, because it will add a lot of depth to your understanding
    of how the optimization process works under the hood. Let’s go through the symbols
    first; see table 7.2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 初始时，损失方程可能看起来有些令人眼花缭乱，但实际上它比看起来要简单。理解它并不是运行和训练 Faster R-CNN 的必要条件，所以您可以自由地跳过这一部分。但我鼓励您努力理解这一解释，因为它将大大加深您对优化过程内部工作原理的理解。让我们首先看看符号；参见表
    7.2。
- en: Multi-task loss function symbols
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 多任务损失函数符号
- en: '| Symbol | Explanation |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '| 符号 | 说明 |'
- en: '| *p[i]* and *p[i]^** | pi is the predicted probability of the anchor (*i*)
    being an object and the ground, and p*i is the binary ground truth (0 or 1) of
    the anchor being an object. |'
  id: totrans-248
  prefs: []
  type: TYPE_TB
  zh: '| *p[i]* 和 *p[i]^** | pi 是预测的锚点 (*i*) 是对象的概率和背景的概率，p*i 是锚点是否为对象的二元真实值（0 或 1）。|'
- en: '| *t[i]* and *t[i]^** | *t[i]* is the predicted four parameters that define
    the bounding box, and *t[i]^** is the ground-truth parameters. |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '| *t[i]* 和 *t[i]^** | *t[i]* 是定义边界框的四个预测参数，而 *t[i]^** 是真实参数。|'
- en: '| *N*[cls] | Normalization term for the classification loss. Ren et al. set
    it to be a mini-batch size of ~256. |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| *N*[cls] | 分类损失的归一化项。Ren 等人将其设置为约 256 个 mini-batch 的大小。|'
- en: '| *N*[loc] | Normalization term for the bounding box regression. Ren et al.
    set it to the number of anchor locations, ~2400. |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| *N*[loc] | 边界框回归的归一化项。Ren 等人将其设置为锚点位置的数量，约 2400。|'
- en: '| *L[cls]*(*p[i], p[i]^**) | The log loss function over two classes. We can
    easily translate a multi-class classification into a binary classification by
    predicting whether a sample is a target object:*L[cls]*(*p[i]*, *p[i]^** ) = −*p[i]^*
    log p[i] - (1 - p[i]^*) log (1 − p[i])* |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| *L[cls]*(*p[i], p[i]^**) | 两个类别的对数损失函数。我们可以通过预测样本是否为目标对象将多类分类轻松转换为二分类：*L[cls]*(*p[i]*,
    *p[i]^**) = −*p[i]^* log p[i] - (1 - p[i]^*) log (1 − p[i])* |'
- en: '| *L*1*[smooth]* | As described in section 7.2.2, the bounding box loss measures
    the difference between the predicted and true location parameters (*t[i]*, *t[i]^**)
    using the smooth L1 loss function. It is a robust function and is claimed to be
    less sensitive to outliers than other regression losses like L2. |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| *L*1*[smooth]* | 如 7.2.2 节所述，边界框损失使用平滑 L1 损失函数来衡量预测和真实位置参数 (*t[i]*, *t[i]^**)
    之间的差异。它是一个鲁棒函数，据称比其他回归损失（如 L2）对异常值更不敏感。|'
- en: '| λ | A balancing parameter, set to be ~10 in Ren et al. (so the Lcls and Lloc
    terms are roughly equally weighted). |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| λ | 一个平衡参数，Ren 等人将其设置为约 10（因此 Lcls 和 Lloc 项大致同等权重）。|'
- en: 'Now that you know the definitions of the symbols, let’s try to read the multi-task
    loss function again. To help understand this equation, just for a moment, ignore
    the normalization terms and the (*i*) terms. Here’s the simplified loss function
    for each instance (*i*):'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经知道了符号的定义，让我们再次尝试阅读多任务损失函数。为了帮助理解这个方程，暂时忽略归一化项和 (*i*) 项。以下是每个实例 (*i*) 的简化损失函数：
- en: Loss = *L[cls]*(*p, p^**) + *p^** · *L*1*[smooth]*(*t - t^**)
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 损失 = *L[cls]*(*p, p^**) + *p^** · *L*1*[smooth]*(*t - t^**)
- en: 'This simplified function is the summation of two loss functions: the classification
    loss and the location loss (bounding box). Let’s look at them one at a time:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简化的函数是两个损失函数的和：分类损失和位置损失（边界框）。让我们逐一看看它们：
- en: 'The idea of any loss function is that it subtracts the predicted value from
    the true value to find the amount of error. The classification loss is the cross-entropy
    function explained in chapter 2\. Nothing new. It is a log loss function that
    calculates the error between the prediction probability (*p*) and the ground truth
    (*p, p^**):'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何损失函数的想法都是从预测值中减去真实值以找到误差量。分类损失是第 2 章中解释的交叉熵函数。没有什么新的。它是一个对数损失函数，用于计算预测概率 (*p*)
    和真实值 (*p, p^**) 之间的误差：
- en: '*L[cls]*(*p[i]*, *p[i]^** ) = −*p[i]^** log *p[i]* − (1 − *p[i]^**) log (1
    - *p[i]*)'
  id: totrans-259
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*L[cls]*(*p[i]*, *p[i]^**) = −*p[i]^** log *p[i]* − (1 − *p[i]^**) log (1 -
    *p[i]*)'
- en: The location loss is the difference between the predicted and true location
    parameters (*t[i]* , *t[i]^**) using the smooth L1 loss function. The difference
    is then multiplied by the ground truth probability of the region containing an
    object *p^**. If it is not an object, *p^** is 0 to eliminate the entire location
    loss for non-object regions.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位置损失是使用平滑L1损失函数预测的位置参数（*t[i]*，*t[i]^**）与真实位置参数之间的差异。然后将差异乘以包含对象的区域的地面真实概率*p^**。如果不是对象，*p^**为0，以消除非对象区域的整个位置损失。
- en: 'Finally, we add the values of both losses to create the multi-loss function:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将两个损失的值相加以创建多损失函数：
- en: '*L* = *L[cls]* + *L[loc]*'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '*L* = *L[cls]* + *L[loc]*'
- en: 'There you have it: the multi-loss function for each instance (*i*). Put back
    the (*i*) and σ symbols to calculate the summation of losses for each instance.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 你看，这就是每个实例（*i*）的多损失函数。将（*i*）和σ符号放回以计算每个实例的损失总和。
- en: 7.2.4 Recap of the R-CNN family
  id: totrans-264
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 R-CNN家族回顾
- en: 'Table 7.3 recaps the evolution of the R-CNN architecture:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.3总结了R-CNN架构的演变：
- en: R-CNN --Bounding boxes are proposed by the selective search algorithm. Each
    is warped, and features are extracted via a deep convolutional neural network
    such as AlexNet, before a final set of object classifications and bounding box
    predictions is made with linear SVMs and linear regressors.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R-CNN -- 边界框由选择性搜索算法提出。每个边界框都被扭曲，并通过如AlexNet之类的深度卷积神经网络提取特征，然后使用线性SVM和线性回归器进行最终的一组对象分类和边界框预测。
- en: Fast R-CNN --A simplified design with a single model. An RoI pooling layer is
    used after the CNN to consolidate regions. The model predicts both class labels
    and RoIs directly.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fast R-CNN -- 一种具有单个模型的单一代计设计。在CNN之后使用RoI池化层来巩固区域。该模型直接预测类别标签和RoIs。
- en: Faster R-CNN --A fully end-to-end DL object detector. It replaces the selective
    search algorithm to propose RoIs with a region proposal network that interprets
    features extracted from the deep CNN and learns to propose RoIs directly.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Faster R-CNN -- 一个完全端到端的深度学习目标检测器。它用区域提议网络替换选择性搜索算法来提议RoIs，该网络解释从深度卷积神经网络中提取的特征，并学习直接提议RoIs。
- en: The evolution of the CNN family of networks from R-CNN to Fast R-CNN to Faster
    R-CNN
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 从R-CNN到Fast R-CNN再到Faster R-CNN的CNN网络家族的演变
- en: '|  | R-CNN | Fast R-CNN | Faster R-CNN |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '|  | R-CNN | Fast R-CNN | Faster R-CNN |'
- en: '|  | ![](../Images/7-unnumb-3.png) | ![](../Images/7-unnumb-4.png) | ![](../Images/7-unnumb-5.png)
    |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片1](../Images/7-unnumb-3.png) | ![图片2](../Images/7-unnumb-4.png) |
    ![图片3](../Images/7-unnumb-5.png) |'
- en: '| mAP on the PASCAL Visual Object Classes Challenge 2007 | 66.0% | 66.9% |
    66.9% |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| PASCAL视觉对象类别挑战赛2007的mAP | 66.0% | 66.9% | 66.9% |'
- en: '| Features |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| 特征 |'
- en: Applies selective search to extract RoIs (~2,000) from each image.
  id: totrans-274
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对每个图像应用选择性搜索以提取RoIs（约2,000个）。
- en: A ConvNet is used to extract features from each of the ~2,000 regions extracted.
  id: totrans-275
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络从提取的约2,000个区域中提取特征。
- en: Uses classification and bounding box predictions.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用分类和边界框预测。
- en: '| Each image is passed only once to the CNN, and feature maps are extracted.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: '| 每个图像只通过CNN一次，并提取特征图。'
- en: A ConvNet is used to extract feature maps from the input image.
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 卷积神经网络（ConvNet）用于从输入图像中提取特征图。
- en: Selective search is used on these maps to generate predictions.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这些图上使用选择性搜索来生成预测。
- en: This way, we run only one ConvNet over the entire image instead of ~2,000 ConvNets
    over 2000 overlapping regions. | Replaces the selective search method with a region
    proposal network, which makes the algorithm much faster.An end-to-end DL network.
    |
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们只需在整个图像上运行一个卷积神经网络，而不是在2000个重叠区域上运行约2000个卷积神经网络。 | 用区域提议网络替换选择性搜索方法，这使得算法速度更快。一个端到端的深度学习网络。|
- en: '| Limitations | High computation time, as each region is passed to the CNN
    separately. Also, uses three different models for making predictions. | Selective
    search is slow and, hence, computation time is still high. | Object proposal takes
    time. And as there are different systems working one after the other, the performance
    of systems depends on how the previous system performed. |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '| 局限性 | 计算时间高，因为每个区域都单独通过CNN。此外，使用三个不同的模型进行预测。 | 选择性搜索慢，因此计算时间仍然很高。 | 对象提议需要时间。由于有不同系统依次工作，系统的性能取决于前一个系统的表现。
    |'
- en: '| Test time per image | 50 seconds | 2 seconds | 0.2 seconds |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 每个图像的测试时间 | 50秒 | 2秒 | 0.2秒 |'
- en: '| Speed-up from R-CNN | 1x | 25x | 250x |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| R-CNN加速 | 1x | 25x | 250x |'
- en: R-CNN limitations
  id: totrans-284
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: R-CNN局限性
- en: 'As you might have noticed, each paper proposes improvements to the seminal
    work done in R-CNN to develop a faster network, with the goal of achieving real-time
    object detection. The achievements displayed through this set of work is truly
    amazing, yet none of these architectures manages to create a real-time object
    detector. Without going into too much detail, the following problems have been
    identified with these networks:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 如您可能已经注意到的，每一篇论文都提出了对R-CNN早期工作的改进，旨在开发一个更快的网络，目标是实现实时目标检测。通过这一系列工作所展示的成就确实令人惊叹，然而，这些架构中的任何一个都没有成功创建一个真正的实时目标检测器。不深入细节，以下这些问题已经与这些网络相关联：
- en: Training the data is unwieldy and takes too long.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据难以处理，耗时过长。
- en: Training happens in multiple phases (such as the training region proposal versus
    a classifier).
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练发生在多个阶段（例如，训练区域提议与分类器）。
- en: The network is too slow at inference time.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络在推理时的速度太慢。
- en: Fortunately, in the last few years, new architectures have been created to address
    the bottlenecks of R-CNN and its successors, enabling real-time object detection.
    The most famous are the single-shot detector (SSD) and you only look once (YOLO),
    which we will explain in sections 7.3 and 7.4.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在过去的几年里，已经创建了新的架构来解决R-CNN及其后续产品的瓶颈，从而实现了实时目标检测。最著名的是单次检测器（SSD）和YOLO（你只看一次），我们将在第7.3节和第7.4节中解释。
- en: Multi-stage vs. single-stage detector
  id: totrans-290
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多阶段检测器与单阶段检测器
- en: 'Models in the R-CNN family are all region-based. Detection happens in two stages,
    and thus these models are called two-stage detectors:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: R-CNN家族中的所有模型都是基于区域的。检测分为两个阶段，因此这些模型被称为两阶段检测器：
- en: The model proposes a set of RoIs using selective search or an RPN. The proposed
    regions are sparse because the potential bounding-box candidates can be infinite.
  id: totrans-292
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型提出了一组RoIs，使用选择性搜索或RPN。提出的区域是稀疏的，因为潜在的边界框候选者可能是无限的。
- en: A classifier only processes the region candidates.
  id: totrans-293
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分类器只处理区域候选者。
- en: One-stage detectors take a different approach. They skip the region proposal
    stage and run detection directly over a dense sampling of possible locations.
    This approach is faster and simpler but can potentially drag down performance
    a bit. In the next two sections, we will examine the SSD and YOLO one-stage object
    detectors. In general, single-stage detectors tend to be less accurate than two-stage
    detectors but are significantly faster.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 单阶段检测器采取不同的方法。它们跳过了区域提议阶段，并在可能的密集采样位置上直接运行检测。这种方法更快、更简单，但可能会略微降低性能。在接下来的两个部分中，我们将检查SSD和YOLO单阶段目标检测器。一般来说，单阶段检测器通常比两阶段检测器精度低，但速度要快得多。
- en: 7.3 Single-shot detector (SSD)
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 单次检测器（SSD）
- en: The SSD paper was released in 2016 by Wei Liu et al.[8](#pgfId-1171881) The
    SSD network reached new records in terms of performance and precision for object
    detection tasks, scoring over 74% mAP at 59 FPS on standard datasets such as the
    PASCAL VOC and Microsoft COCO.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: SSD论文由Wei Liu等人于2016年发布[8](#pgfId-1171881)。SSD网络在目标检测任务中的性能和精度方面达到了新的记录，在标准数据集如PASCAL
    VOC和Microsoft COCO上得分超过74% mAP，在59 FPS的速度下运行。
- en: 'Measuring detector speed (FPS: Frames per second)'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 测量检测器速度（FPS：每秒帧数）
- en: As discussed at the beginning of this chapter, the most common metric for measuring
    detection speed is the number of frames per second. For example, Faster R-CNN
    operates at only 7 frames per second (FPS). There have been many attempts to build
    faster detectors by attacking each stage of the detection pipeline, but so far,
    significantly increased speed has come only at the cost of significantly decreased
    detection accuracy. In this section, you will see why single-stage networks like
    SSD can achieve faster detections that are more suitable for real-time detection.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章开头所讨论的，测量检测速度最常用的指标是每秒帧数。例如，Faster R-CNN每秒只能运行7帧（FPS）。已经有很多尝试通过攻击检测管道的每个阶段来构建更快的检测器，但到目前为止，显著提高速度的代价是显著降低检测精度。在本节中，您将了解为什么像SSD这样的单阶段网络可以实现更快的检测，这更适合实时检测。
- en: For benchmarking, SSD300 achieves 74.3% mAP at 59 FPS, while SSD512 achieves
    76.8% mAP at 22 FPS, which outperforms Faster R-CNN (73.2% mAP at 7 FPS). SSD300
    refers to an input image of size 300 × 300, and SSD512 refers to an input image
    of size 512 × 512.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基准测试，SSD300在59 FPS时实现了74.3% mAP，而SSD512在22 FPS时实现了76.8% mAP，这超过了Faster R-CNN（在7
    FPS时实现73.2% mAP）。SSD300指的是300 × 300大小的输入图像，而SSD512指的是512 × 512大小的输入图像。
- en: 'We learned earlier that the R-CNN family are multi-stage detectors: the network
    first predicts the objectness score of the bounding box and then passes this box
    through a classifier to predict the class probability. In single-stage detectors
    like SSD and YOLO (discussed in section 7.4), the convolutional layers make both
    predictions directly in one shot: hence the name single-shot detector. The image
    is passed once through the network, and the objectness score for each bounding
    box is predicted using logistic regression to indicate the level of overlap with
    the ground truth. If the bounding box overlaps 100% with the ground truth, the
    objectness score is 1; and if there is no overlap, the objectness score is 0\.
    We then set a threshold value (0.5) that says, “If the objectness score is above
    50%, this bounding box likely has an object of interest, and we get predictions.
    If it is less than 50%, we ignore the predictions.”'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前了解到，R-CNN系列是多阶段检测器：网络首先预测边界框的对象分数，然后将这个框通过分类器传递以预测类别概率。在单阶段检测器如SSD和YOLO（在第7.4节中讨论）中，卷积层直接在一次操作中做出两种预测：因此得名单次检测器。图像只通过一次网络，每个边界框的对象分数使用逻辑回归预测，以指示与真实值的重叠程度。如果边界框与真实值重叠100%，则对象分数为1；如果没有重叠，则对象分数为0。然后我们设置一个阈值值（0.5），即“如果对象分数高于50%，则这个边界框很可能包含感兴趣的对象，我们得到预测。如果低于50%，则忽略预测。”
- en: 7.3.1 High-level SSD architecture
  id: totrans-301
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 高级SSD架构
- en: 'The SSD approach is based on a feed-forward convolutional network that produces
    a fixed-size collection of bounding boxes and scores for the presence of object
    class instances in those boxes, followed by a NMS step to produce the final detections.
    The architecture of the SSD model is composed of three main parts:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: SSD方法基于一个前馈卷积网络，该网络生成固定大小的边界框和分数集合，用于预测这些框中对象类实例的存在，然后通过NMS步骤生成最终检测。SSD模型的架构由三个主要部分组成：
- en: Base network to extract feature maps --A standard pretrained network used for
    high-quality image classification, which is truncated before any classification
    layers. In their paper, Liu et al. used a VGG16 network. Other networks like VGG19
    and ResNet can be used and should produce good results.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础网络提取特征图——用于高质量图像分类的标准预训练网络，在分类层之前被截断。在他们的论文中，Liu等人使用了VGG16网络。其他网络如VGG19和ResNet也可以使用，并且应该产生良好的结果。
- en: Multi-scale feature layers --A series of convolution filters are added after
    the base network. These layers decrease in size progressively to allow predictions
    of detections at multiple scales.
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多尺度特征层——在基础网络之后添加了一系列卷积滤波器。这些层的大小逐渐减小，以允许预测多个尺度的检测。
- en: Non-maximum suppression --NMS is used to eliminate overlapping boxes and keep
    only one box for each object detected.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非最大值抑制——NMS用于消除重叠的框，并为每个检测到的对象保留一个框。
- en: As you can see in figure 7.17, layers 4_3, 7, 8_2, 9_2, 10_2, and 11_2 make
    predictions directly to the NMS layer. We will talk about why these layers progressively
    decrease in size in section 7.3.3\. For now, let’s follow along to understand
    the end-to-end flow of data in SSD.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图7.17中可以看到，层4_3、7、8_2、9_2、10_2和11_2直接对NMS层做出预测。我们将在第7.3.3节中讨论为什么这些层的大小逐渐减小。现在，让我们继续了解SSD中的数据端到端流程。
- en: '![](../Images/7-17.png)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-17.png)'
- en: 'Figure 7.17 The SSD architecture is composed of a base network (VGG16), extra
    convolutional layers for object detection, and a non-maximum suppression (NMS)
    layer for final detections. Note that convolution layers 7, 8, 9, 10, and 11 make
    predictions that are directly fed to the NMS layer. (Source: Liu et al., 2016.)'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 SSD架构由基础网络（VGG16）、用于对象检测的额外卷积层以及用于最终检测的非最大值抑制（NMS）层组成。注意，卷积层7、8、9、10和11直接做出预测，并将其直接输入到NMS层。来源：Liu等人，2016年。
- en: You can see in figure 7.17, that the network makes a total of 8,732 detections
    per class that are then fed to an NMS layer to reduce down to one detection per
    object. Where did the number 8,732 come from?
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图7.17中看到，网络对每个类别进行了总共8,732次检测，然后将其输入到NMS层以减少到每个对象一次检测。这个数字8,732是从哪里来的？
- en: 'To have more accurate detection, different layers of feature maps also go through
    a small 3 × 3 convolution for object detection. For example, Conv4_3 is of size
    38 × 38 × 512, and a 3 × 3 convolutional is applied. There are four bounding boxes,
    each of which has (number of classes + 4 box values) outputs. Suppose there are
    20 object classes plus 1 background class; then the output number of bounding
    boxes is 38 × 38 × 4 = 5,776 bounding boxes. Similarly, we calculate the number
    of bounding boxes for the other convolutional layers:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现更精确的检测，特征图的各个层也通过一个小型的3 × 3卷积进行物体检测。例如，Conv4_3的大小为38 × 38 × 512，并应用了一个3
    × 3的卷积。有四个边界框，每个边界框有（类别数量 + 4个边界框值）个输出。假设有20个物体类别加上1个背景类别；那么输出边界框的数量是38 × 38 ×
    4 = 5,776个边界框。同样，我们计算其他卷积层的边界框数量：
- en: 'Conv7: 19 × 19 × 6 = 2,166 boxes (6 boxes for each location)'
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv7: 19 × 19 × 6 = 2,166个边界框（每个位置6个边界框）'
- en: 'Conv8_2: 10 × 10 × 6 = 600 boxes (6 boxes for each location)'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv8_2: 10 × 10 × 6 = 600个边界框（每个位置6个边界框）'
- en: 'Conv9_2: 5 × 5 × 6 = 150 boxes (6 boxes for each location)'
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv9_2: 5 × 5 × 6 = 150个边界框（每个位置6个边界框）'
- en: 'Conv10_2: 3 × 3 × 4 = 36 boxes (4 boxes for each location)'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv10_2: 3 × 3 × 4 = 36个边界框（每个位置4个边界框）'
- en: 'Conv11_2: 1 × 1 × 4 = 4 boxes (4 boxes for each location)'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'Conv11_2: 1 × 1 × 4 = 4个边界框（每个位置4个边界框）'
- en: 'If we sum them up, we get 5,776 + 2,166 + 600 + 150 + 36 + 4 = 8,732 boxes
    produced. This is a huge number of boxes to show for our detector. That’s why
    we apply NMS to reduce the number of the output boxes. As you will see in section
    7.4, in YOLO there are 7 × 7 locations at the end with two bounding boxes for
    each location: 7 × 7 × 2 = 98 boxes.'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将它们加起来，我们得到5,776 + 2,166 + 600 + 150 + 36 + 4 = 8,732个边界框产生。这对于我们的检测器来说是一个巨大的数字。这就是为什么我们应用NMS来减少输出边界框的数量。正如你将在7.4节中看到的，在YOLO中，最后有7
    × 7个位置，每个位置有两个边界框：7 × 7 × 2 = 98个边界框。
- en: What does the output prediction look like?
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 输出预测看起来是什么样子？
- en: 'For each feature, the network predicts the following:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个特征，网络预测以下内容：
- en: 4 values that describe the bounding box (*x, y, w, h*)
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 描述边界框的4个值（*x, y, w, h*）
- en: 1 value for the objectness score
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体得分的1个值
- en: C values that represent the probability of each class
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示每个类别概率的C值
- en: 'That’s a total of 5 + C prediction values. Suppose there are four object classes
    in our problem. Then each prediction will be a vector that looks like this: [x,
    y, w, h, objectness score, C1, C2, C3, C4].'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 这总共是5 + C个预测值。假设我们的问题中有四个物体类别。那么每个预测将是一个看起来像这样的向量：[x, y, w, h, 物体得分, C1, C2,
    C3, C4]。
- en: '![](../Images/7-unnumb-6.png)'
  id: totrans-323
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-unnumb-6.png)'
- en: 'An example visualization of the output prediction when we have four classes
    in our problem. The convolutional layer predicts the bounding box coordinates,
    objectness score, and four class probabilities: C1, C2, C3, and C4.'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在问题中有四个类别时，输出预测的示例可视化。卷积层预测边界框坐标、物体得分和四个类别的概率：C1、C2、C3和C4。
- en: Now, let’s dive a little deeper into each component of the SSD architecture.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更深入地探讨SSD架构的每个组件。
- en: 7.3.2 Base network
  id: totrans-326
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 基础网络
- en: As you can see in figure 7.17, the SSD architecture builds on the VGG16 architecture
    after slicing off the fully connected classification layers (VGG16 is explained
    in detail in chapter 5). VGG16 was used as the base network because of its strong
    performance in high-quality image classification tasks and its popularity for
    problems where transfer learning helps to improve results. Instead of the original
    VGG fully connected layers, a set of supporting convolutional layers (from Conv6
    onward) was added to enable us to extract features at multiple scales and progressively
    decrease the size of the input to each subsequent layer.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 如你在图7.17中看到的，SSD架构在切除了全连接分类层（VGG16在第五章中有详细解释）之后建立在VGG16架构之上。VGG16被用作基础网络，因为它在高质量图像分类任务中表现出色，并且因其对迁移学习有帮助提高结果的问题而受到欢迎。与原始VGG全连接层不同，增加了一系列支持卷积层（从Conv6开始），使我们能够提取多个尺度的特征，并逐步减小每个后续层的输入大小。
- en: 'Following is a simplified code implementation of the VGG16 network used in
    SSD using Keras. You will not need to implement this from scratch; my goal in
    including this code snippet is to show you that this is a typical VGG16 network
    like the one implemented in chapter 5:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是使用Keras在SSD中使用的VGG16网络的简化代码实现。你不需要从头开始实现这个；我包括这个代码片段的目标是向你展示这是一个典型的VGG16网络，就像第五章中实现的那样：
- en: '[PRE0]'
  id: totrans-329
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'You saw VGG16 implemented in Keras in chapter 5\. The two main takeaways from
    adding this here are as follows:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 你在第五章中看到了VGG16在Keras中的实现。在这里添加这个的主要收获如下：
- en: Layer conv4_3 will be used again to make direct predictions.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层conv4_3将再次用于直接预测。
- en: Layer pool5 will be fed to the next layer (conv6), which is the first of the
    multiscale features layers.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 层pool5将被输入到下一层（conv6），这是多尺度特征层的第一个层。
- en: How the base network makes predictions
  id: totrans-333
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 基础网络如何进行预测
- en: 'Consider the following example. Suppose you have the image in figure 7.18,
    and the network’s job is to draw bounding boxes around all the boats in the image.
    The process goes as follows:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下示例。假设你有图7.18中的图像，网络的任务是在图像中围绕所有船只绘制边界框。过程如下：
- en: Similar to the anchors concept in R-CNN, SSD overlays a grid of anchors around
    the image. For each anchor, the network creates a set of bounding boxes at its
    center. In SSD, anchors are called priors.
  id: totrans-335
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 与R-CNN中的锚点概念类似，SSD在图像周围叠加了一个锚点网格。对于每个锚点，网络在其中心创建一组边界框。在SSD中，锚点被称为先验。
- en: The base network looks at each bounding box as a separate image. For each bounding
    box, the network asks, “Is there a boat in this box?” Or in other words, “Did
    I extract any features of a boat in this box?”
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 基础网络将每个边界框视为一个单独的图像。对于每个边界框，网络会问：“这个框里有没有船？”或者换句话说，“我在这个框里提取了任何船只特征吗？”
- en: When the network finds a bounding box that contains boat features, it sends
    its coordinates prediction and object classification to the NMS layer.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当网络找到一个包含船只特征的边界框时，它会将其坐标预测和对象分类发送到NMS层。
- en: NMS eliminates all the boxes except the one that overlaps the most with the
    ground-truth bounding box.
  id: totrans-338
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: NMS消除了所有与真实边界框重叠最少的框。
- en: NOTE Liu et al. used VGG16 because of its strong performance in complex image
    classification tasks. You can use other networks like the deeper VGG19 or ResNet
    for the base network, and it should perform as well if not better in accuracy;
    but it could be slower if you chose to implement a deeper network. MobileNet is
    a good choice if you want a balance between a complex, high-performing deep network
    and being fast.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Liu等人使用VGG16是因为它在复杂图像分类任务中表现出色。您可以使用其他网络，如更深的VGG19或ResNet作为基础网络，如果不在准确性上，应该表现得更好；但如果您选择实现更深的网络，它可能会更慢。如果您想要在复杂、高性能的深度网络和快速之间取得平衡，MobileNet是一个不错的选择。
- en: 'Now, on to the next component of the SSD architecture: multi-scale feature
    layers.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续SSD架构的下一个组件：多尺度特征层。
- en: '![](../Images/7-18.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-18.png)'
- en: Figure 7.18 The SSD base network looks at the anchor boxes to find features
    of a boat. Solid boxes indicate that the network has found boat features. Dotted
    boxes indicate no boat features.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 SSD基础网络查看锚点框以找到船只特征。实线框表示网络已找到船只特征。虚线框表示没有船只特征。
- en: 7.3.3 Multi-scale feature layers
  id: totrans-343
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 多尺度特征层
- en: These are convolutional feature layers that are added to the end of the truncated
    base network. These layers decrease in size progressively to allow predictions
    of detections at multiple scales.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是添加到截断基础网络末尾的卷积特征层。这些层的大小逐渐减小，以便能够在多个尺度上进行检测预测。
- en: Multi-scale detections
  id: totrans-345
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多尺度检测
- en: To understand the goal of the multi-scale feature layers and why they vary in
    size, let’s look at the image of horses in figure 7.19\. As you can see, the base
    network may be able to detect the horse features in the background, but it may
    fail to detect the horse that is closest to the camera. To understand why, take
    a close look at the dotted bounding box and try to imagine this box alone outside
    the context of the full image (see figure 7.20)
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解多尺度特征层的目标以及它们为什么大小不同，让我们看看图7.19中的马匹图像。如图所示，基础网络可能能够检测到背景中的马匹特征，但它可能无法检测到离相机最近的马匹。为了理解原因，仔细看看虚线边界框，并尝试想象这个框在完整图像之外（见图7.20）。
- en: '![](../Images/7-19.png)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-19.png)'
- en: Figure 7.19 Horses at different scales in an image. The horses that are far
    from the camera are easier to detect because they are small in size and can fit
    inside the priors (anchor boxes). The base network might fail to detect the horse
    closest to the camera because it needs a different scale of anchors to be able
    to create priors that cover more identifiable features.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 图像中不同尺度的马匹。离相机远的马匹更容易检测，因为它们体积小，可以适应先验（锚点框）。基础网络可能无法检测到离相机最近的马匹，因为它需要不同尺度的锚点来创建能够覆盖更多可识别特征的先验。
- en: '![](../Images/7-20.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-20.png)'
- en: Figure 7.20 An isolated horse feature
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 一个孤立的马特征
- en: Can you see horse features in the bounding box in figure 7.20? No. To deal with
    objects of different scales in an image, some methods suggest preprocessing the
    image at different sizes and combining the results afterward (figure 7.21). However,
    by using different convolution layers that vary in size, we can use feature maps
    from several different layers in a single network; for prediction we can mimic
    the same effect, while also sharing parameters across all object scales. As CNN
    reduces the spatial dimension gradually, the resolution of the feature maps also
    decreases. SSD uses lower-resolution layers to detect larger-scale objects. For
    example, 4 × 4 feature maps are used for larger scale objects.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在图7.20的边界框中看到马的特征吗？不。为了处理图像中不同尺度的对象，一些方法建议在不同大小上预处理图像，并在之后合并结果（图7.21）。然而，通过使用不同大小的卷积层，我们可以在单个网络中使用来自几个不同层的特征图；对于预测，我们可以模仿相同的效果，同时在整个对象尺度上共享参数。随着CNN逐渐减少空间维度，特征图的分辨率也降低。SSD使用低分辨率层来检测较大尺度的对象。例如，4
    × 4特征图用于较大尺度的对象。
- en: '![](../Images/7-21.png)'
  id: totrans-352
  prefs: []
  type: TYPE_IMG
  zh: '![图7.21](../Images/7-21.png)'
- en: Figure 7.21 Lower-resolution feature maps detect larger-scale objects (right);
    higher-resolution feature maps detect smaller-scale objects (left).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 低分辨率特征图检测较大尺度的对象（右侧）；高分辨率特征图检测较小尺度的对象（左侧）。
- en: To visualize this, imagine that the network reduces the image dimensions to
    be able to fit all of the horses inside its bounding boxes (figure 7.22). The
    multi-scale feature layers resize the image dimensions and keep the bounding-box
    sizes so that they can fit the larger horse. In reality, convolutional layers
    do not literally reduce the size of the image; this is just for illustration to
    help us intuitively understand the concept. The image is not just resized, it
    actually goes through the convolutional process and thus won’t look anything like
    itself anymore. It will be a completely random-looking image, but it will preserve
    its features. The convolutional process is explained in detail in chapter 3.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这一点，想象一下网络将图像维度减小，以便所有马都能在其边界框内（图7.22）。多尺度特征层调整图像维度并保持边界框大小，以便它们可以适应较大的马。实际上，卷积层并不是字面上减小图像的大小；这只是为了说明，帮助我们直观地理解这个概念。图像不仅被调整大小，它实际上经过了卷积过程，因此看起来不再像自己。它将是一个完全随机的图像，但它将保留其特征。卷积过程在第3章中详细解释。
- en: '![](../Images/7-22.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![图7.22](../Images/7-22.png)'
- en: Figure 7.22 Multi-scale feature layers reduce the spatial dimensions of the
    input image to detect objects with different scales. In this image, you can see
    that the new priors are kind of zoomed out to cover more identifiable features
    of the horse close to the camera.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 多尺度特征层减小输入图像的空间维度以检测不同尺度的对象。在这张图像中，你可以看到新的先验值有点放大，以覆盖靠近摄像头的马的可识别特征。
- en: Using multi-scale feature maps improves network accuracy significantly. Liu
    et al. ran an experiment to measure the advantage gained by adding the multi-scale
    feature layers. Figure 7.23 shows a decrease in accuracy with fewer layers; you
    can see the accuracy with different numbers of feature map layers used for object
    detection.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 使用多尺度特征图可以显著提高网络精度。刘等人进行了一项实验，以衡量通过添加多尺度特征层所获得的优势。图7.23显示了随着层数减少，精度下降；你可以看到使用不同数量的特征图层进行对象检测时的精度。
- en: '![](../Images/7-23.png)'
  id: totrans-358
  prefs: []
  type: TYPE_IMG
  zh: '![图7.23](../Images/7-23.png)'
- en: 'Figure 7.23 Effects of using multiple output layers from the original paper.
    The detector’s accuracy (mAP) increases when the authors add multi-scale features.
    (Source: Liu et al., 2016.)'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 使用原始论文中多个输出层的效果。当作者添加多尺度特征时，检测器的精度（mAP）提高。（来源：刘等人，2016。）
- en: Notice that network accuracy drops from 74.3% when having the prediction source
    from all six layers to 62.4% for one source layer. When using only the conv7 layer
    for prediction, performance is the worst, reinforcing the message that it is critical
    to spread boxes of different scales over different layers.
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当预测源来自所有六个层时，网络精度从74.3%下降到62.4%，对于单一源层。当仅使用conv7层进行预测时，性能最差，这强化了这样一个信息：在不同层上分布不同尺度的边界框是至关重要的。
- en: Architecture of the multi-scale layers
  id: totrans-361
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 多尺度层的架构
- en: Liu et al. decided to add six convolutional layers that decrease in size. They
    did this with a lot of tuning and trial and error until they produced the best
    results. As you saw in figure 7.17, convolutional layers 6 and 7 are pretty straightforward.
    Conv6 has a kernel size of 3 × 3, and conv7 has a kernel size of 1 × 1\. Layers
    8 through 11, on the other hand, are treated more like blocks, where each block
    consists of two convolutional layers of kernel sizes 1 × 1 and 3 × 3.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 刘等人决定添加六个逐渐变小的卷积层。他们通过大量的调整和试错，直到产生最佳结果。正如你在图 7.17 中看到的，卷积层 6 和 7 非常简单。Conv6
    的内核大小为 3 × 3，而 conv7 的内核大小为 1 × 1。另一方面，8 到 11 层更像是一系列块，其中每个块由两个内核大小为 1 × 1 和 3
    × 3 的卷积层组成。
- en: 'Here is the code implementation in Keras for layers 6 through 11 (you can see
    the full implementation in the book’s downloadable code):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 这是 Keras 中 6 到 11 层的代码实现（你可以在书中可下载的代码中看到完整的实现）：
- en: '[PRE1]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As mentioned before, if you are not working in research or academia, you most
    probably won’t need to implement object detection architectures yourself. In most
    cases, you will download an open source implementation and build on it to work
    on your problem. I just added these code snippets to help you internalize the
    information discussed about different layer architectures.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，如果你不在研究或学术界工作，你很可能不需要自己实现目标检测架构。在大多数情况下，你会下载一个开源实现，并在其基础上工作以解决你的问题。我只是添加了这些代码片段，帮助你内化关于不同层架构讨论的信息。
- en: Atrous (or dilated) convolutions
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀（或膨胀）卷积
- en: 'Dilated convolutions introduce another parameter to convolutional layers: the
    dilation rate. This defines the spacing between the values in a kernel. A 3 ×
    3 kernel with a dilation rate of 2 has the same field of view as a 5 × 5 kernel
    while only using nine parameters. Imagine taking a 5 × 5 kernel and deleting every
    second column and row.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积为卷积层引入了另一个参数：膨胀率。这定义了内核中值的间隔。具有膨胀率 2 的 3 × 3 内核具有与 5 × 5 内核相同的视野，而只使用九个参数。想象一下，取一个
    5 × 5 内核，并删除每行的第二列和每列的第二行。
- en: This delivers a wider field of view at the same computational cost.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这在相同的计算成本下提供了更宽的视野。
- en: '![](../Images/7-unnumb-7.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-unnumb-7.png)'
- en: A 3 × 3 kernel with a dilation rate of 2 has the same field of view as a 5 ×
    5 kernel while only using nine parameters.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 具有膨胀率 2 的 3 × 3 内核具有与 5 × 5 内核相同的视野，而只使用九个参数。
- en: Dilated convolutions are particularly popular in the field of real-time segmentation.
    Use them if you need a wide field of view and cannot afford multiple convolutions
    or larger kernels.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 膨胀卷积在实时分割领域特别受欢迎。如果你需要一个宽视野且无法承担多个卷积或更大的内核，请使用它们。
- en: 'The following code builds a dilated 3 × 3 convolution layer with a dilation
    rate of 2 using Keras:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码使用 Keras 构建了一个具有 2 倍膨胀率的 3 × 3 膨胀卷积层：
- en: '[PRE2]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, we discuss the third and last component of the SSD architecture: NMS.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论 SSD 架构的第三和最后一个组件：NMS。
- en: 7.3.4 Non-maximum suppression
  id: totrans-375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 非极大值抑制
- en: Given the large number of boxes generated by the detection layer per class during
    a forward pass of SSD at inference time, it is essential to prune most of the
    bounding box by applying the NMS technique (explained earlier in this chapter).
    Boxes with a confidence loss and IoU less than a certain threshold are discarded,
    and only the top N predictions are kept (figure 7.24). This ensures that only
    the most likely predictions are retained by the network, while the noisier ones
    are removed.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SSD 在推理时每类生成的框数量很大，因此在使用 NMS 技术剪枝（本章前面已解释）时，剪枝大部分边界框至关重要。置信度损失和 IoU 小于某个阈值的框被丢弃，只保留前
    N 个预测（图 7.24）。这确保了网络只保留最可能的预测，而噪声预测被移除。
- en: How does SSD use NMS to prune the bounding boxes? SSD sorts the predicted boxes
    by their confidence scores. Starting from the top confidence prediction, SSD evaluates
    whether there are any previously predicted boundary boxes for the same class that
    overlap with each other above a certain threshold by calculating their IoU. (The
    IoU threshold value is tunable. Liu et al. chose 0.45 in their paper.) Boxes with
    IoU above the threshold are ignored because they overlap too much with another
    box that has a higher confidence score, so they are most likely detecting the
    same object. At most, we keep the top 200 predictions per image.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: SSD是如何使用NMS来剪枝边界框的？SSD按置信度分数对预测框进行排序。从置信度最高的预测开始，SSD通过计算它们的IoU来评估是否存在任何先前预测的与同一类别的边界框重叠超过某个阈值。
    (IoU阈值是可调的。Liu等人在他们的论文中选择了0.45。) IoU高于阈值的框被忽略，因为它们与置信度更高的另一个框重叠太多，因此它们最有可能检测到同一对象。每个图像最多保留200个预测。
- en: '![](../Images/7-24.png)'
  id: totrans-378
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-24.png)'
- en: Figure 7.24 Non-maximum suppression reduces the number of bounding boxes to
    only one box for each object.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 非极大值抑制将每个对象的边界框数量减少到仅一个。
- en: 7.4 You only look once (YOLO)
  id: totrans-380
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 只看一次 (YOLO)
- en: 'Similar to the R-CNN family, YOLO is a family of object detection networks
    developed by Joseph Redmon et al. and improved over the years through the following
    versions:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 与R-CNN系列类似，YOLO是由Joseph Redmon等人开发的目标检测网络系列，并通过以下版本逐年改进：
- en: 'YOLOv1, published in 2016[9](#pgfId-1172116)--Called “unified, real-time object
    detection” because it is a single-detection network that unifies the two components
    of a detector: object detector and class predictor.'
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv1，于2016年发布[9](#pgfId-1172116)--被称为“统一、实时目标检测”，因为它是一个统一的检测网络，统一了检测器的两个组件：目标检测器和类别预测器。
- en: YOLOv2 (also known as YOLO9000), published later in 2016[10](#pgfId-1172120)--Capable
    of detecting over 9,000 objects; hence the name. It has been trained on ImageNet
    and COCO datasets and has achieved 16% mAP, which is not good; but it was very
    fast during test time.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv2（也称为YOLO9000），稍后于2016年发布[10](#pgfId-1172120)--能够检测超过9,000个对象；因此得名。它已在ImageNet和COCO数据集上训练，并实现了16%的mAP，这并不好；但它在测试时非常快。
- en: YOLOv3, published in 2018[11](#pgfId-1172125)--Significantly larger than previous
    models and has achieved a mAP of 57.9%, which is the best result yet out of the
    YOLO family of object detectors.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLOv3，于2018年发布[11](#pgfId-1172125)--比先前模型大得多，并实现了57.9%的mAP，这是YOLO系列目标检测器中的最佳结果。
- en: The YOLO family is a series of end-to-end DL models designed for fast object
    detection, and it was among the first attempts to build a fast real-time object
    detector. It is one of the faster object detection algorithms out there. Although
    the accuracy of the models is close but not as good as R-CNNs, they are popular
    for object detection because of their detection speed, often demonstrated in real-time
    video or camera feed input.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO系列是一系列端到端深度学习模型，专为快速目标检测而设计，并且是构建快速实时目标检测器的首次尝试之一。它是现有最快的目标检测算法之一。尽管模型的精度接近但不如R-CNNs，但由于它们的检测速度，它们在目标检测中很受欢迎，通常在实时视频或相机输入中演示。
- en: The creators of YOLO took a different approach than the previous networks. YOLO
    does not undergo the region proposal step like R-CNNs. Instead, it only predicts
    over a limited number of bounding boxes by splitting the input into a grid of
    cells; each cell directly predicts a bounding box and object classification. The
    result is a large number of candidate bounding boxes that are consolidated into
    a final prediction using NMS (figure 7.25).
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO的创造者采用了与先前网络不同的方法。YOLO不经过与R-CNNs类似的区域提议步骤。相反，它通过将输入分割成单元格网格来仅预测有限数量的边界框；每个单元格直接预测一个边界框和对象分类。结果是大量候选边界框，这些边界框通过NMS（图7.25）合并成最终的预测。
- en: '![](../Images/7-25.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-25.png)'
- en: Figure 7.25 YOLO splits the image into grids, predicts objects for each grid,
    and then uses NMS to finalize predictions.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25 YOLO将图像分割成网格，预测每个网格的对象，然后使用NMS最终确定预测。
- en: YOLOv1 proposed the general architecture, YOLOv2 refined the design and made
    use of predefined anchor boxes to improve bounding-box proposals, and YOLOv3 further
    refined the model architecture and training process. In this section, we are going
    to focus on YOLOv3 because it is currently the state-of-the-art architecture in
    the YOLO family.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv1提出了通用架构，YOLOv2优化了设计并利用预定义的锚框来改进边界框建议，YOLOv3进一步优化了模型架构和训练过程。在本节中，我们将重点关注YOLOv3，因为它目前在YOLO家族中是最先进的架构。
- en: 7.4.1 How YOLOv3 works
  id: totrans-390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 YOLOv3的工作原理
- en: 'The YOLO network splits the input image into a grid of S × S cells. If the
    center of the ground-truth box falls into a cell, that cell is responsible for
    detecting the existence of that object. Each grid cell predicts B number of bounding
    boxes and their objectness score along with their class predictions, as follows:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: YOLO网络将输入图像分割成S × S的网格。如果真实值框的中心落在单元格中，则该单元格负责检测该物体的存在。每个网格单元格预测B个边界框及其物体得分和类别预测，如下所示：
- en: Coordinates of B bounding boxes --Similar to previous detectors, YOLO predicts
    four coordinates for each bounding box (*b[x] , b[y] , b[w] , b[h]*), where *x*
    and *y* are set to be offsets of a cell location.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: B个边界框的坐标--与之前的检测器类似，YOLO为每个边界框(*b[x] , b[y] , b[w] , b[h]*)预测四个坐标，其中*x*和*y*被设置为单元格位置的偏移量。
- en: 'Objectness score (*P*[0])--indicates the probability that the cell contains
    an object. The objectness score is passed through a sigmoid function to be treated
    as a probability with a value range between 0 and 1\. The objectness score is
    calculated as follows:'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 物体得分 (*P*[0])--表示该单元格包含物体的概率。物体得分通过sigmoid函数转换为介于0和1之间的概率。物体得分的计算如下：
- en: '*P*[0] = P[r] (containing an object) × IoU (pred, truth)'
  id: totrans-394
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '*P*[0] = P[r] (包含物体) × IoU (预测，真实)'
- en: Class prediction --If the bounding box contains an object, the network predicts
    the probability of K number of classes, where K is the total number of classes
    in your problem.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别预测--如果边界框包含物体，网络预测K个类别的概率，其中K是您问题中的类别总数。
- en: It is important to note that before v3, YOLO used a softmax function for the
    class scores. In v3, Redmon et al. decided to use sigmoid instead. The reason
    is that softmax imposes the assumption that each box has exactly one class, which
    is often not the case. In other words, if an object belongs to one class, then
    it’s guaranteed not to belong to another class. While this assumption is true
    for some datasets, it may not work when we have classes like Women and Person.
    A multilabel approach models the data more accurately.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在v3之前，YOLO使用softmax函数对类别得分进行计算。在v3中，Redmon等人决定使用sigmoid。原因是softmax假设每个框恰好有一个类别，这通常并不成立。换句话说，如果一个物体属于一个类别，那么它肯定不属于另一个类别。虽然这个假设对于某些数据集是正确的，但在我们处理像“女性”和“人”这样的类别时可能不起作用。多标签方法可以更准确地建模数据。
- en: '![](../Images/7-26.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-26.png)'
- en: 'Figure 7.26 Example of a YOLOv3 workflow when applying a 13 × 13 grid to the
    input image. The input image is split into 169 cells. Each cell predicts B number
    of bounding boxes and their objectness score along with their class predictions.
    In this example, we show the cell at the center of the ground-truth making predictions
    for 3 boxes (B = 3). Each prediction has the following attributes: bounding box
    coordinates, objectness score, and class predictions.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26展示了将13 × 13网格应用于输入图像时的YOLOv3工作流程示例。输入图像被分割成169个单元格。每个单元格预测B个边界框及其物体得分，以及它们的类别预测。在这个例子中，我们展示了中心真实值单元格对3个边界框进行预测（B
    = 3）。每个预测具有以下属性：边界框坐标、物体得分和类别预测。
- en: 'As you can see in figure 7.26, for each bounding box (*b*), the prediction
    looks like this: [(bounding box coordinates), (objectness score), (class predictions)].
    We’ve learned that the bounding box coordinates are four values plus one value
    for the objectness score and K values for class predictions. Then the total number
    of values predicted for all bounding boxes is 5B + K multiplied by the number
    of cells in the grid S × S :'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 如图7.26所示，对于每个边界框 (*b*)，预测结果如下：[(边界框坐标)，(物体得分)，(类别预测)]。我们已经了解到，边界框坐标是四个值加上一个物体得分值和K个类别预测值。因此，所有边界框预测的总值是5B
    + K乘以网格S × S中的单元格数：
- en: Total predicted values = S × S × (5B + K)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 总预测值 = S × S × (5B + K)
- en: Predictions across different scales
  id: totrans-401
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 不同尺度的预测
- en: Look closely at figure 7.26\. Notice that the prediction feature map has three
    boxes. You might have wondered why there are three boxes. Similar to the anchors
    concept in SSD, YOLOv3 has nine anchors to allow for prediction at three different
    scales per cell. The detection layer makes detections at feature maps of three
    different sizes having strides 32, 16, and 8, respectively. This means that with
    an input image of size 416 × 416, we make detections on scales 13 × 13, 26 × 26,
    and 52 × 52 (figure 7.27). The 13 × 13 layer is responsible for detecting large
    objects, the 26 × 26 layer is for detecting medium objects, and the 52 × 52 layer
    detects smaller objects.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察图7.26。注意预测特征图有三个框。你可能想知道为什么有三个框。与SSD中的锚框概念类似，YOLOv3有九个锚框，允许每个细胞在三个不同尺度上进行预测。检测层在具有步长32、16和8的三个不同大小的特征图上进行检测。这意味着对于大小为416
    × 416的输入图像，我们在13 × 13、26 × 26和52 × 52的尺度上进行检测（图7.27）。13 × 13层负责检测大对象，26 × 26层负责检测中等对象，52
    × 52层检测小对象。
- en: '![](../Images/7-27.png)'
  id: totrans-403
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-27.png)'
- en: Figure 7.27 Prediction feature maps at different scales
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27不同尺度的预测特征图
- en: This results in the prediction of three bounding boxes for each cell (B = 3).
    That’s why in figure 7.26, the prediction feature map is predicting Box 1, Box
    2, and Box 3\. The bounding box responsible for detecting the dog will be the
    one whose anchor has the highest IoU with the ground-truth box.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致每个细胞预测三个边界框（B = 3）。这就是为什么在图7.26中，预测特征图正在预测框1、框2和框3。负责检测狗的边界框将是与真实框具有最高IoU的锚框。
- en: NOTE Detections at different layers help address the issue of detecting small
    objects, which was a frequent complaint with YOLOv2\. The upsampling layers can
    help the network preserve and learn fine-grained features, which are instrumental
    for detecting small objects.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在不同层上的检测有助于解决YOLOv2中常见的检测小对象的问题。上采样层可以帮助网络保留和学会细粒度特征，这对于检测小对象至关重要。
- en: The network does this by downsampling the input image until the first detection
    layer, where a detection is made using feature maps of a layer with stride 32\.
    Further, layers are upsampled by a factor of 2 and concatenated with feature maps
    of previous layers having identical feature-map sizes. Another detection is now
    made at layer with stride 16\. The same upsampling procedure is repeated, and
    a final detection is made at the layer of stride 8.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 网络通过下采样输入图像直到第一个检测层来实现这一点，在该层使用步长为32的特征图进行检测。进一步，层通过2倍上采样并与具有相同特征图大小的先前层的特征图连接。现在在步长为16的层进行另一个检测。相同的上采样过程被重复，并在步长为8的层进行最终检测。
- en: YOLOv3 output bounding boxes
  id: totrans-408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: YOLOv3输出边界框
- en: For an input image of size 416 × 416, YOLO predicts ((52 × 52) + (26 × 26) +
    13 × 13)) × 3 = 10,647 bounding boxes. That is a huge number of boxes for an output.
    In our dog example, we have only one object. We want only one bounding box around
    this object. How do we reduce the boxes from 10,647 down to 1?
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大小为416 × 416的输入图像，YOLO预测 ((52 × 52) + (26 × 26) + 13 × 13)) × 3 = 10,647个边界框。对于一个输出来说，这是一个巨大的框数量。在我们的狗示例中，我们只有一个对象。我们只想在这个对象周围有一个边界框。我们如何将框从10,647个减少到1个？
- en: First, we filter the boxes based on their objectness score. Generally, boxes
    having scores below a threshold are ignored. Second, we use NMS to cure the problem
    of multiple detections of the same image. For example, all three bounding boxes
    of the outlined grid cell at the center of the image may detect a box, or the
    adjacent cells may detect the same object.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们根据它们的对象性分数过滤框。通常，分数低于阈值的框被忽略。其次，我们使用NMS来解决同一图像的多次检测问题。例如，图像中心突出网格细胞的三个边界框都可能检测到一个框，或者相邻的单元格可能检测到同一个对象。
- en: 7.4.2 YOLOv3 architecture
  id: totrans-411
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 YOLOv3架构
- en: Now that you understand how YOLO works, going through the architecture will
    be very simple and straightforward. YOLO is a single neural network that unifies
    object detection and classifications into one end-to-end network. The neural network
    architecture was inspired by the GoogLeNet model (Inception) for feature extraction.
    Instead of the Inception modules, YOLO uses 1 × 1 reduction layers followed by
    3 × 3 convolutional layers. Redmon and Farhadi called this DarkNet (figure 7.28).
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了YOLO的工作原理，那么通过其架构将会非常简单直接。YOLO是一个将目标检测和分类统一到一个端到端网络的单一神经网络。神经网络架构受到了GoogLeNet模型（Inception）在特征提取方面的启发。YOLO不是使用Inception模块，而是使用1
    × 1降维层后跟3 × 3卷积层。Redmon和Farhadi将这个称为DarkNet（图7.28）。
- en: '![](../Images/7-28.png)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图片7-28](../Images/7-28.png)'
- en: Figure 7.28 High-level architecture of YOLO
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.28 YOLO 的高级架构
- en: 'YOLOv2 used a custom deep architecture darknet-19, an originally 19-layer network
    supplemented with 11 more layers for object detection. With a 30-layer architecture,
    YOLOv2 often struggled with small object detections. This was attributed to loss
    of fine-grained features as the layers downsampled the input. However, YOLOv2’s
    architecture was still lacking some of the most important elements that are now
    stable in most state-of-the art algorithms: no residual blocks, no skip connections,
    and no upsampling. YOLOv3 incorporates all of these updates.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv2 使用了一个定制的深度架构 darknet-19，一个原本 19 层的网络，增加了 11 个额外的层用于目标检测。具有 30 层架构的 YOLOv2
    常常在小型目标检测上遇到困难。这归因于随着层对输入进行下采样而丢失了细粒度特征。然而，YOLOv2 的架构仍然缺少现在大多数最先进算法中稳定的一些最重要的元素：没有残差块，没有跳跃连接，没有上采样。YOLOv3
    包含了所有这些更新。
- en: YOLOv3 uses a variant of DarkNet called Darknet-53 (figure 7.29). It has a 53-layer
    network that is trained on ImageNet. For the task of detection, 53 more layers
    are stacked onto it, giving us a 106-layer fully convolutional underlying architecture
    for YOLOv3\. This is the reason behind the slowness of YOLOv3 compared to YOLOv2--but
    this comes with a great boost in detection accuracy.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: YOLOv3 使用了名为 Darknet-53 的 DarkNet 变体（如图 7.29）。它有一个在 ImageNet 上训练的 53 层网络。为了检测任务，在其上又堆叠了
    53 个额外的层，为 YOLOv3 提供了一个 106 层的全卷积底层架构。这就是 YOLOv3 相比 YOLOv2 慢的原因——但这也带来了检测精度的显著提升。
- en: '![](../Images/7-29.png)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![图片7-29](../Images/7-29.png)'
- en: 'Figure 7.29 DarkNet-53 feature extractor architecture. (Source: Redmon and
    Farhadi, 2018.)'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.29 DarkNet-53 特征提取器架构。（来源：Redmon 和 Farhadi，2018。）
- en: Full architecture of YOLOv3
  id: totrans-419
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: YOLOv3 的完整架构
- en: We just learned that YOLOv3 makes predictions across three different scales.
    This becomes a lot clearer when you see the full architecture, shown in figure
    7.30.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚了解到 YOLOv3 在三个不同的尺度上进行预测。当您看到完整的架构时，这会变得更加清晰，如图 7.30 所示。
- en: '![](../Images/7-30.png)'
  id: totrans-421
  prefs: []
  type: TYPE_IMG
  zh: '![图片7-30](../Images/7-30.png)'
- en: Figure 7.30 YOLOv3 network architecture. (Inspired by the diagram in Ayoosh
    Kathuria’s post “What’s new in YOLO v3?” Medium, 2018, [http://mng.bz/lGN2](http://mng.bz/lGN2).)
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.30 YOLOv3 网络架构。（灵感来源于 Ayoosh Kathuria 在 Medium 上的帖子“YOLO v3 的新特性是什么？”2018
    年，[http://mng.bz/lGN2](http://mng.bz/lGN2)。）
- en: The input image goes through the DarkNet-53 feature extractor, and then the
    image is downsampled by the network until layer 79\. The network branches out
    and continues to downsample the image until it makes its first prediction at layer
    82\. This detection is made on a grid scale of 13 × 13 that is responsible for
    detecting large objects, as we explained before.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像通过 DarkNet-53 特征提取器，然后网络将图像下采样到第 79 层。网络分支并继续下采样图像，直到在第 82 层进行第一次预测。这种检测是在
    13 × 13 的网格尺度上进行的，正如我们之前解释的那样，负责检测大型物体。
- en: Next the feature map from layer 79 is upsampled by 2x to dimensions 26 × 26
    and concatenated with the feature map from layer 61\. Then the second detection
    is made by layer 94 on a grid scale of 26 × 26 that is responsible for detecting
    medium objects.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，第 79 层的特征图上采样 2 倍到 26 × 26 的尺寸，并与第 61 层的特征图连接。然后，第 94 层在第 26 × 26 的网格尺度上进行第二次检测，负责检测中等物体。
- en: Finally, a similar procedure is followed again, and the feature map from layer
    91 is subjected to few upsampling convolutional layers before being depth concatenated
    with a feature map from layer 36\. A third prediction is made by layer 106 on
    a grid scale of 52 × 52, which is responsible for detecting small objects.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，再次遵循类似的程序，第 91 层的特征图在经过少量上采样卷积层后，与第 36 层的特征图进行深度连接。第 106 层在第 52 × 52 的网格尺度上进行第三次预测，负责检测小型物体。
- en: '7.5 Project: Train an SSD network in a self-driving car application'
  id: totrans-426
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 项目：在自动驾驶汽车应用中训练 SSD 网络
- en: The code for this project was created by Pierluigi Ferrari in his GitHub repository
    ([https://github.com/pierluigiferrari/ssd_keras](https://github.com/pierluigiferrari/ssd_keras)).
    The project was adapted for this chapter; you can find this implementation with
    the book’s downloadable code.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的代码由 Pierluigi Ferrari 在他的 GitHub 仓库 ([https://github.com/pierluigiferrari/ssd_keras](https://github.com/pierluigiferrari/ssd_keras))
    创建。该项目已为本章改编；您可以在本书的可下载代码中找到此实现。
- en: Note that for this project, we are going to build a smaller SSD network called
    SSD7\. SSD7 is a seven-layer version of the SSD300 network. It is important to
    note that while an SSD7 network would yield some acceptable results, this is not
    an optimized network architecture. The goal is just to build a low-complexity
    network that is fast enough for you to train on your personal computer. It took
    me around 20 hours to train this network on the road traffic dataset; training
    could take a lot less time on a GPU.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于这个项目，我们将构建一个名为SSD7的小型SSD网络。SSD7是SSD300网络的七层版本。需要注意的是，虽然SSD7网络会产生一些可接受的结果，但这并不是一个优化的网络架构。目标是构建一个低复杂度的网络，足够快，以便您可以在个人计算机上训练。我在道路交通数据集上训练这个网络大约花费了20个小时；在GPU上训练可能需要的时间少得多。
- en: NOTE The original repository created by Pierluigi Ferrari comes with implementation
    tutorials for SSD7, SSD300, and SSD512 networks. I encourage you to check it out.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** Pierluigi Ferrari创建的原始仓库包含SSD7、SSD300和SSD512网络的实现教程。我鼓励您查看。'
- en: 'In this project, we will use a toy dataset created by Udacity. You can visit
    Udacity’s GitHub repository for more information on the dataset ([https://github.com/udacity/
    self-driving-car/tree/master/annotations](https://github.com/udacity/self-driving-car/tree/master/annotations)).
    It has more than 22,000 labeled images and 5 object classes: car, truck, pedestrian,
    bicyclist, and traffic light. All of the images have been resized to a height
    of 300 pixels and a width of 480 pixels. You can download the dataset as part
    of the book’s code.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个项目中，我们将使用由Udacity创建的玩具数据集。您可以访问Udacity的GitHub仓库以获取有关数据集的更多信息（[https://github.com/udacity/self-driving-car/tree/master/annotations](https://github.com/udacity/self-driving-car/tree/master/annotations)）。该数据集包含超过22,000个标记的图像和5个对象类别：汽车、卡车、行人、骑自行车的人和交通灯。所有图像都已调整大小，高度为300像素，宽度为480像素。您可以将数据集作为本书代码的一部分下载。
- en: NOTE The GitHub data repository is owned by Udacity, and it may be updated after
    this writing. To avoid any confusion, I downloaded the dataset that I used to
    create this project and provided it with the book’s code to allow you to replicate
    the results in this project.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '**注意** GitHub数据仓库归Udacity所有，并且在此写作之后可能会更新。为了避免任何混淆，我下载了用于创建此项目的数据集，并将其与本书的代码一起提供，以便您可以在项目中复制这些结果。'
- en: What makes this dataset very interesting is that these are real-time images
    taken while driving in Mountain View, California, and neighboring cities during
    daylight conditions. No image cleanup was done. Take a look at the image examples
    in figure 7.31.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 使这个数据集非常有趣的是，这些是在加利福尼亚州山景城及其邻近城市白天驾驶时拍摄的真实时间图像。没有进行图像清理。请查看图7.31中的图像示例。
- en: '![](../Images/7-31.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-31.png)'
- en: Figure 7.31 Example images from the Udacity self-driving dataset (Image copyright
    © 2016 Udacity and published under MIT License.)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.31 Udacity自动驾驶数据集的示例图像（图像版权©2016 Udacity，在MIT许可证下发布。）
- en: 'As stated on Udacity’s page, the dataset was labeled by CrowdAI and Autti.
    You can find the labels in CSV format in the folder, split into three files: training,
    validation, and test datasets. The labeling format is straightforward, as follows:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 如Udacity页面所述，数据集由CrowdAI和Autti标记。您可以在文件夹中找到CSV格式的标签，分为三个文件：训练集、验证集和测试集。标记格式简单，如下所示：
- en: '| frame | xmin | xmax | ymin | ymax | class_id |'
  id: totrans-436
  prefs: []
  type: TYPE_TB
  zh: '| frame | xmin | xmax | ymin | ymax | class_id |'
- en: '| 1478019952686311006.jpg | 237 | 251 | 143 | 155 | 1 |'
  id: totrans-437
  prefs: []
  type: TYPE_TB
  zh: '| 1478019952686311006.jpg | 237 | 251 | 143 | 155 | 1 |'
- en: Xmin, xmax, ymin, and ymax are the bounding box coordinates. Class_id is the
    correct label, and frame is the image name.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: Xmin、xmax、ymin和ymax是边界框坐标。Class_id是正确的标签，frame是图像名称。
- en: Data annotation using LabelImg
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 使用LabelImg进行数据标注
- en: If you are annotating your own data, there are several open source labeling
    applications that you can use, like LabelImg ([https://pypi.org/project/labelImg](https://pypi.org/project/labelImg)).
    They are very easy to set up and use.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在标注自己的数据，有几个开源标注应用程序可供使用，例如LabelImg ([https://pypi.org/project/labelImg](https://pypi.org/project/labelImg))。它们非常容易设置和使用。
- en: '![](../Images/7-unnumb-8.png)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/7-unnumb-8.png)'
- en: Example of using the labelImg application to annotate images
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 使用labelImg应用程序标注图像的示例
- en: '7.5.1 Step 1: Build the model'
  id: totrans-443
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 步骤1：构建模型
- en: Before jumping into the model training, take a close look at the `build_model`
    method in the `keras_ssd7.py` file. This file builds a Keras model with the SSD
    architecture. As we learned earlier in this chapter, the model consists of convolutional
    feature layers and a number of convolutional predictor layers that make their
    input from different feature layers.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始模型训练之前，仔细查看`keras_ssd7.py`文件中的`build_model`方法。此文件使用SSD架构构建Keras模型。正如我们在本章前面所学，该模型由卷积特征层和多个从不同特征层获取输入的卷积预测层组成。
- en: 'Here is what the `build_model` method looks like. Please read the comments
    in the keras_ssd7.py file to understand the arguments passed:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`build_model`方法的样子。请阅读`keras_ssd7.py`文件中的注释以了解传递的参数：
- en: '[PRE3]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '7.5.2 Step 2: Model configuration'
  id: totrans-447
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 步骤 2：模型配置
- en: 'In this section, we set the model configuration parameters. First we set the
    height, width, and number of color channels to whatever we want the model to accept
    as image input. If your input images have a different size than defined here,
    or if your images have non-uniform size, you must use the data generator’s image
    transformations (resizing and/or cropping) so that your images end up having the
    required input size before they are fed to the model:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们设置模型配置参数。首先，我们将高度、宽度和颜色通道数设置为模型接受的图像输入。如果您的输入图像的大小与这里定义的不同，或者如果您的图像大小不均匀，您必须使用数据生成器的图像变换（调整大小和/或裁剪），以确保在输入模型之前，您的图像达到所需的输入大小：
- en: '[PRE4]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Height, width, and channels of the input images
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入图像的高度、宽度和通道数
- en: ❷ Set to your preference (maybe None). The current settings transform the input
    pixel values to the interval [-1,1].
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置为您的偏好（可能是None）。当前设置将输入像素值转换为区间[-1,1]。
- en: 'The number of classes is the number of positive classes in your dataset: for
    example, 20 for PASCAL VOC or 80 for COCO. Class ID 0 must always be reserved
    for the background class:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 类别数量是您数据集中正类别的数量：例如，PASCAL VOC为20，COCO为80。类别ID 0必须始终保留为背景类别：
- en: '[PRE5]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Number of classes in our dataset
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们数据集中的类别数量
- en: ❷ An explicit list of anchor box scaling factors. If this is passed, it overrides
    the min_scale and max_scale arguments.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 明确的锚框缩放因子列表。如果传递了此参数，它将覆盖min_scale和max_scale参数。
- en: ❸ List of aspect ratios for the anchor boxes
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 锚框的纵横比列表
- en: ❹ In case you’d like to set the step sizes for the anchor box grids manually;
    not recommended
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果您想手动设置锚框网格的步长大小；不推荐
- en: ❺ In case you’d like to set the offsets for the anchor box grids manually; not
    recommended
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 如果您想手动设置锚框网格的偏移量；不推荐
- en: ❻ Specifies whether to generate two anchor boxes for aspect ratio 1
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 指定是否为纵横比1生成两个锚框
- en: ❼ Specifies whether to clip the anchor boxes to lie entirely within the image
    boundaries
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 指定是否将锚框裁剪到图像边界内
- en: ❽ List of variances by which the encoded target coordinates are scaled
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 通过编码目标坐标缩放的方差列表
- en: ❾ Specifies whether the model is supposed to use coordinates relative to the
    image size
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 指定模型是否应使用相对于图像大小的坐标
- en: '7.5.3 Step 3: Create the model'
  id: totrans-463
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 步骤 3：创建模型
- en: 'Now we call the `build_model()` function to build our model:'
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们调用`build_model()`函数来构建我们的模型：
- en: '[PRE6]'
  id: totrans-465
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'You can optionally load saved weights. If you don’t want to load weights, skip
    the following code snippet:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以可选地加载保存的权重。如果您不想加载权重，请跳过以下代码片段：
- en: '[PRE7]'
  id: totrans-467
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Instantiate an Adam optimizer and the SSD loss function, and compile the model.
    Here, we will use a custom Keras function called `SSDLoss`. It implements the
    multi-task log loss for classification and smooth L1 loss for localization. `neg_pos_ratio`
    and `alpha` are set as in the SSD paper (Liu et al., 2016):'
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 实例化一个Adam优化器和SSD损失函数，并编译模型。在这里，我们将使用一个名为`SSDLoss`的自定义Keras函数。它实现了多任务对数损失用于分类和光滑L1损失用于定位。`neg_pos_ratio`和`alpha`设置为SSD论文（Liu
    et al., 2016）中所述：
- en: '[PRE8]'
  id: totrans-469
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '7.5.4 Step 4: Load the data'
  id: totrans-470
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.4 步骤 4：加载数据
- en: 'To load the data, follow these steps:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载数据，请按照以下步骤操作：
- en: 'Instantiate two `DataGenerator` objects--one for training and one for validation:'
  id: totrans-472
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化两个`DataGenerator`对象——一个用于训练，一个用于验证：
- en: '[PRE9]'
  id: totrans-473
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Parse the image and label lists for the training and validation datasets:'
  id: totrans-474
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析训练和验证数据集的图像和标签列表：
- en: '[PRE10]'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Ground truth
  id: totrans-476
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 真实值
- en: ❷ Gets the number of samples in the training and validation datasets
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 获取训练和验证数据集的样本数量
- en: 'This cell should print out the size of your training and validation datasets
    as follows:'
  id: totrans-478
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 此单元格应按如下方式打印出您的训练和验证数据集的大小：
- en: '[PRE11]'
  id: totrans-479
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Set the batch size:'
  id: totrans-480
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 设置批量大小：
- en: '[PRE12]'
  id: totrans-481
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: As you learned in chapter 4, you can increase the batch size to get a boost
    in the computing speed based on the hardware that you are using for this training.
  id: totrans-482
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如您在第4章所学，您可以根据用于此训练的硬件增加批处理大小以获得计算速度的提升。
- en: 'Define the data augmentation process:'
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义数据增强过程：
- en: '[PRE13]'
  id: totrans-484
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Instantiate an encoder that can encode ground-truth labels into the format
    needed by the SSD loss function. Here, the encoder constructor needs the spatial
    dimensions of the model’s predictor layers to create the anchor boxes:'
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个编码器，可以将真实标签编码成SSD损失函数所需的格式。在这里，编码器构造函数需要模型预测层的空间维度来创建锚框：
- en: '[PRE14]'
  id: totrans-486
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Create the generator handles that will be passed to Keras’s `fit_generator()`
    function:'
  id: totrans-487
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建将传递给Keras的`fit_generator()`函数的生成器处理程序：
- en: '[PRE15]'
  id: totrans-488
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '7.5.5 Step 5: Train the model'
  id: totrans-489
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.5 步骤5：训练模型
- en: 'Everything is set, and we are ready to train our SSD7 network. We’ve already
    chosen an optimizer and a learning rate and set the batch size; now let’s set
    the remaining training parameters and train the network. There are no new parameters
    here that you haven’t learned already. We will set the model checkpoint, early
    stopping, and learning rate reduction rate:'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 一切准备就绪，我们现在可以训练我们的SSD7网络了。我们已经选择了一个优化器和学习率，并设置了批处理大小；现在让我们设置剩余的训练参数并训练网络。这里没有新的参数是您之前没有学过的。我们将设置模型检查点、提前停止和学习率降低率：
- en: '[PRE16]'
  id: totrans-491
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Early stopping if val_loss did not improve for 10 consecutive epochs
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果验证损失在10个连续的epoch中没有提高，则提前停止
- en: ❷ Learning rate reduction rate when it plateaus
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当学习率达到平台期时的学习率降低率
- en: 'Set one epoch to consist of 1,000 training steps. I’ve arbitrarily set the
    number of epochs to 20 here. This does not necessarily mean that 20,000 training
    steps is the optimum number. Depending on the model, dataset, learning rate, and
    so on, you might have to train much longer (or less) to achieve convergence:'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个epoch设置为包含1,000个训练步骤。我在这里任意地将epoch数设置为20。这并不一定意味着20,000个训练步骤是最佳数量。根据模型、数据集、学习率等因素，您可能需要更长时间（或更短）的训练才能达到收敛：
- en: '[PRE17]'
  id: totrans-495
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ If you’re resuming previous training, set initial_epoch and final_epoch accordingly.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果您正在继续之前的训练，请相应地设置initial_epoch和final_epoch。
- en: ❷ Starts training
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开始训练
- en: '7.5.6 Step 6: Visualize the loss'
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.6 步骤6：可视化损失
- en: 'Let’s visualize the `loss` and `val_loss` values to look at how the training
    and validation loss evolved and check whether our training is going in the right
    direction (figure 7.32):'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们可视化`loss`和`val_loss`值，看看训练和验证损失是如何演变的，并检查我们的训练是否朝着正确的方向进行（图7.32）：
- en: '[PRE18]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/7-32.png)'
  id: totrans-501
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-32.png)'
- en: Figure 7.32 Visualized `loss` and `val_loss` values during SSD7 training for
    20 epochs
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.32 SSD7训练20个epoch期间可视化的`loss`和`val_loss`值
- en: '7.5.7 Step 7: Make predictions'
  id: totrans-503
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.7 步骤7：进行预测
- en: 'Now let’s make some predictions on the validation dataset with the trained
    model. For convenience, we’ll use the validation generator that we’ve already
    set up. Feel free to change the batch size:'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用训练好的模型在验证数据集上进行一些预测。为了方便，我们将使用我们已设置的验证生成器。您可以随意更改批处理大小：
- en: '[PRE19]'
  id: totrans-505
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ 1\. Set the generator for the predictions.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 1. 设置预测的生成器。
- en: ❷ 2\. Generate samples.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 2. 生成样本。
- en: ❸ 3\. Make a prediction.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 3. 进行预测。
- en: ❹ 4\. Decode the raw prediction y_pred.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 4. 解码原始预测 y_pred。
- en: This code snippet prints the predicted bounding boxes along with their class
    and the level of confidence for each one, as shown in figure 7.33.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码片段打印出预测的边界框及其类别和每个边界框的置信度，如图7.33所示。
- en: '![](../Images/7-33.png)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-33.png)'
- en: Figure 7.33 Predicted bounding boxes, confidence level, and class
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.33 预测的边界框、置信度和类别
- en: When we draw these predicted boxes onto the image, as shown in figure 7.34,
    each predicted box has its confidence next to the category name. The ground-truth
    boxes are also drawn onto the image for comparison.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将这些预测框绘制到图像上时，如图7.34所示，每个预测框旁边都有其置信度以及类别名称。真实框也被绘制到图像上进行比较。
- en: '![](../Images/7-34.png)'
  id: totrans-514
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/7-34.png)'
- en: Figure 7.34 Predicted boxes drawn onto the image
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.34 将预测框绘制到图像上
- en: Summary
  id: totrans-516
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Image classification is the task of predicting the type or class of an object
    in an image.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像分类是预测图像中对象类型或类别的任务。
- en: Object detection is the task of predicting the location of objects in an image
    via bounding boxes and the classes of the located objects.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测是通过边界框预测图像中对象的位置以及定位对象的类别。
- en: 'The general framework of object detection systems consists of four main components:
    region proposals, feature extraction and predictions, non-maximum suppression,
    and evaluation metrics.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测系统的一般框架包括四个主要组件：区域提议，特征提取和预测，非极大值抑制，以及评估指标。
- en: 'Object detection algorithms are evaluated using two main metrics: frame per
    second (FPS) to measure the network’s speed, and mean average precision (mAP)
    to measure the network’s precision.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目标检测算法使用两个主要指标进行评估：每秒帧数（FPS）来衡量网络的速率，以及平均精度均值（mAP）来衡量网络的精度。
- en: The three most popular object detection systems are the R-CNN family of networks,
    SSD, and the YOLO family of networks.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最受欢迎的三种目标检测系统是R-CNN系列网络，SSD和YOLO系列网络。
- en: 'The R-CNN family of networks has three main variations: R-CNN, Fast R-CNN,
    and Faster R-CNN. R-CNN and Fast R-CNN use a selective search algorithm to propose
    RoIs, whereas Faster R-CNN is an end-to-end DL system that uses a region proposal
    network to propose RoIs.'
  id: totrans-522
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R-CNN系列网络有三个主要变体：R-CNN，Fast R-CNN和Faster R-CNN。R-CNN和Fast R-CNN使用选择性搜索算法来提议RoIs，而Faster
    R-CNN是一个端到端深度学习系统，它使用区域提议网络来提议RoIs。
- en: The YOLO family of networks include YOLOv1, YOLOv2 (or YOLO9000), and YOLOv3.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: YOLO网络家族包括YOLOv1，YOLOv2（或YOLO9000）和YOLOv3。
- en: 'R-CNN is a multi-stage detector: it separates the process to predict the objectness
    score of the bounding box and the object class into two different stages.'
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: R-CNN是一个多阶段检测器：它将预测边界框对象分数和对象类别的过程分为两个不同的阶段。
- en: 'SSD and YOLO are single-stage detectors: the image is passed once through the
    network to predict the objectness score and the object class.'
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SSD和YOLO是单阶段检测器：图像通过网络一次来预测对象分数和对象类别。
- en: In general, single-stage detectors tend to be less accurate than two-stage detectors
    but are significantly faster.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常，单阶段检测器比双阶段检测器精度低，但速度要快得多。
- en: '* * *'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.Ross Girshick, Jeff Donahue, Trevor Darrell, and Jitendra Malik, “Rich Feature
    Hierarchies for Accurate Object Detection and Semantic Segmentation,” 2014, [http://arxiv.org/abs/1311.2524](http://arxiv.org/abs/1311.2524).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 1. 罗斯·吉什克，杰夫·多纳休，特雷弗·达尔，和吉滕德拉·马利克， “用于准确目标检测和语义分割的丰富特征层次结构，” 2014，[http://arxiv.org/abs/1311.2524](http://arxiv.org/abs/1311.2524).
- en: 2.Ross Girshick, “Fast R-CNN,” 2015, [http://arxiv.org/abs/1504.08083](http://arxiv.org/abs/1504.08083).
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 罗斯·吉什克， “Fast R-CNN，” 2015，[http://arxiv.org/abs/1504.08083](http://arxiv.org/abs/1504.08083).
- en: '3.Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun, “Faster R-CNN: Towards
    Real-Time Object Detection with Region Proposal Networks,” 2016, [http://arxiv.org/abs/1506.01497](http://arxiv.org/abs/1506.01497).'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '3. 邵庆庆，何凯明，罗斯·吉什克，和孙剑， “Faster R-CNN: 使用区域提议网络的实时目标检测，” 2016，[http://arxiv.org/abs/1506.01497](http://arxiv.org/abs/1506.01497).'
- en: 4.Matthew D. Zeiler and Rob Fergus, “Visualizing and Understanding Convolutional
    Networks,” 2013, [http://arxiv.org/abs/1311.2901](http://arxiv.org/abs/1311.2901).
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 马修·D·泽勒和罗布·弗格森， “可视化与理解卷积网络，” 2013，[http://arxiv.org/abs/1311.2901](http://arxiv.org/abs/1311.2901).
- en: 5.Karen Simonyan and Andrew Zisserman, “Very Deep Convolutional Networks for
    Large-Scale Image Recognition,” 2014, [http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556).
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 卡伦·西蒙扬和安德鲁·齐塞曼， “用于大规模图像识别的非常深的卷积网络，” 2014，[http://arxiv.org/abs/1409.1556](http://arxiv.org/abs/1409.1556).
- en: '6.Andrew G. Howard, Menglong Zhu, Bo Chen, Dmitry Kalenichenko, Weijun Wang,
    Tobias Weyand, Marco Andreetto, and Hartwig Adam, “MobileNets: Efficient Convolutional
    Neural Networks for Mobile Vision Applications,” 2017, [http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861).'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '6. 安德鲁·G·豪厄德，朱孟龙，陈波，卡列宁琴科·德米特里，王伟军，韦安德·托比亚斯，安德烈托·马可，亚当·哈特维格， “MobileNets: 用于移动视觉应用的效率卷积神经网络，”
    2017，[http://arxiv.org/abs/1704.04861](http://arxiv.org/abs/1704.04861).'
- en: 7.Gao Huang, Zhuang Liu, Laurens van der Maaten, and Kilian Q. Weinberger, “Densely
    Connected Convolutional Networks,” 2016, [http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 高翔，刘壮，范德马滕·劳伦斯，和奎因伯格·基利安， “密集连接卷积网络，” 2016，[http://arxiv.org/abs/1608.06993](http://arxiv.org/abs/1608.06993).
- en: '8.Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed,
    Cheng-Yang Fu, and Alexander C. Berg, “SSD: Single Shot MultiBox Detector,” 2016,
    [http://arxiv.org/abs/1512.02325](http://arxiv.org/abs/1512.02325).'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: '8. 刘伟，安格洛夫·德拉戈米尔，厄尔汉·杜米特鲁，塞格迪·克里斯蒂安，里德·斯科特，傅成阳，亚历山大·C·伯格， “SSD: 单阶段多框检测器，”
    2016，[http://arxiv.org/abs/1512.02325](http://arxiv.org/abs/1512.02325).'
- en: '9.Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi, “You Only
    Look Once: Unified, Real-Time Object Detection,” 2016, [http://arxiv.org/abs/1506.02640](http://arxiv.org/abs/1506.02640).'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 9.约瑟夫·雷德蒙、桑托什·迪瓦拉、罗斯·吉什克和阿里·法哈迪，“你只需看一次：统一、实时目标检测”，2016年，[http://arxiv.org/abs/1506.02640](http://arxiv.org/abs/1506.02640)。
- en: '10.Joseph Redmon and Ali Farhadi, “YOLO9000: Better, Faster, Stronger,” 2016,
    [http://arxiv.org/abs/ 1612.08242](http://arxiv.org/abs/1612.08242).'
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 10.约瑟夫·雷德蒙和阿里·法哈迪，“YOLO9000：更好、更快、更强”，2016年，[http://arxiv.org/abs/1612.08242](http://arxiv.org/abs/1612.08242)。
- en: '11.Joseph Redmon and Ali Farhadi, “YOLOv3: An Incremental Improvement,” 2018,
    [http://arxiv.org/abs/ 1804.02767](http://arxiv.org/abs/1804.02767).'
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 11.约瑟夫·雷德蒙和阿里·法哈迪，“YOLOv3：渐进式改进”，2018年，[http://arxiv.org/abs/1804.02767](http://arxiv.org/abs/1804.02767)。
