- en: 8 Running data-heavy apps with StatefulSets and Jobs
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用状态集（StatefulSets）和作业（Jobs）运行数据密集型应用程序
- en: “Data heavy” isn’t a very scientific term, but this chapter is about running
    a class of application that isn’t just stateful but is also demanding about how
    it uses state. Databases are one example of this class. They need to run across
    multiple instances for high availability, each instance needs a local data store
    for fast access, and those independent data stores need to be kept in sync. The
    data has its own availability requirements, and you’ll need to run backups periodically
    to guard against terminal failure or corruption. Other data-intensive applications,
    like message queues and distributed caches, have similar requirements.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: “数据密集型”不是一个非常科学的术语，但本章是关于运行一类不仅具有状态性，而且对如何使用状态有较高要求的程序。数据库是这类程序的一个例子。它们需要在多个实例上运行以实现高可用性，每个实例需要一个本地数据存储以实现快速访问，而这些独立的数据存储需要保持同步。数据有其自己的可用性要求，并且你需要定期运行备份以防止永久性故障或损坏。其他数据密集型应用程序，如消息队列和分布式缓存，也有类似的要求。
- en: 'You can run those kinds of app in Kubernetes, but you need to design around
    an inherent conflict: Kubernetes is a dynamic environment, and data-heavy apps
    typically expect to run in a stable environment. Clustered applications, which
    expect to find peers at a known network address, won’t work nicely in a ReplicaSet,
    and backup jobs, which expect to read from a disk drive, won’t work well with
    PersistentVolumeClaims. You need to model your app differently if it has strict
    data requirements, and we’ll cover how to do that in this chapter with some more
    advanced controllers: StatefulSets, Jobs, and CronJobs.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在Kubernetes中运行这类应用程序，但你需要围绕一个固有的冲突进行设计：Kubernetes是一个动态环境，而数据密集型应用程序通常期望在一个稳定的环境中运行。期望在已知网络地址找到对等体的集群应用程序在ReplicaSet中不会运行良好，而期望从磁盘驱动器读取的备份作业与PersistentVolumeClaims也不会很好地工作。如果你的应用程序有严格的数据要求，你需要以不同的方式建模你的应用程序，我们将在本章中介绍如何使用一些更高级的控制器：状态集（StatefulSets）、作业（Jobs）和定时作业（CronJobs）来做到这一点。
- en: 8.1 How Kubernetes models stability with StatefulSets
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 Kubernetes如何使用状态集（StatefulSets）来建模稳定性
- en: 'A StatefulSet is a Pod controller with predictable management features: it
    lets you run applications at scale within a stable framework. When you deploy
    a ReplicaSet, it creates Pods with random names, which are not individually addressable
    over the domain name system (DNS), and it starts them in parallel. When you deploy
    a StatefulSet, it creates Pods with predictable names, which can be individually
    accessed over DNS, and starts them in order; the first Pod needs to be up and
    running before the second Pod is created.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 状态集（StatefulSet）是一种具有可预测管理功能的Pod控制器：它允许你在稳定的框架内以规模运行应用程序。当你部署ReplicaSet时，它会创建具有随机名称的Pod，这些Pod在域名系统（DNS）中无法单独寻址，并且并行启动它们。当你部署状态集（StatefulSet）时，它会创建具有可预测名称的Pod，这些Pod可以通过DNS单独访问，并且按顺序启动；第一个Pod需要启动并运行，第二个Pod才能创建。
- en: Clustered applications are a great candidate for StatefulSets. Typically they’re
    designed with a primary instance and one or more secondaries, which gives them
    high availability. You might be able to scale the secondaries, but they all need
    to reach the primary and then use it to synchronize their own data. You can’t
    model that with a Deployment because in the ReplicaSet, there is no way to identify
    a single Pod as the primary, so you’d end up with bizarre and unpredictable conditions
    with multiple primaries or zero primaries.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 集群应用程序是状态集（StatefulSet）的绝佳候选者。通常，它们设计有一个主实例和一个或多个辅助实例，这使它们具有高可用性。你可能能够扩展辅助实例，但它们都需要达到主实例并使用它来同步它们自己的数据。你不能用Deployment来建模，因为在ReplicaSet中，没有方法可以识别单个Pod作为主实例，因此你最终会得到奇怪且不可预测的条件，有多个主实例或没有主实例。
- en: Figure 8.1 shows an example of that, which could be used to run the Postgres
    database we’ve used in previous chapters for the to-do list application, but it
    uses a StatefulSet to achieve replicated data and high availability.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1展示了这样一个例子，它可以用来运行我们在前几章中用于待办事项应用程序的Postgres数据库，但它使用状态集（StatefulSet）来实现数据复制和高可用性。
- en: '![](../Images/8-1.jpg)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1](../Images/8-1.jpg)'
- en: Figure 8.1 In a StatefulSet, each Pod can have its own copy of data replicated
    from the first Pod.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 在状态集中，每个Pod都可以拥有从第一个Pod复制的自己的数据副本。
- en: The setup for this is quite involved, and we’ll spend a couple of sections getting
    there in stages, so you learn how all the pieces of a working StatefulSet fit
    together. It’s a pattern that is useful for more than just databases—many older
    applications were designed for a static runtime environment and made assumptions
    about stability that don’t hold true in Kubernetes. StatefulSets allow you to
    model that stability, and if your goal is to move your existing apps to Kubernetes,
    then they may be something you use early in that journey.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 这个设置相当复杂，我们将分几个部分逐步进行，以便你了解一个工作状态集的所有组件是如何组合在一起的。这是一个不仅对数据库有用的模式——许多旧应用都是为静态运行时环境设计的，并假设了稳定性，而这种稳定性在
    Kubernetes 中并不成立。StatefulSets 允许你模拟这种稳定性，如果你的目标是迁移现有的应用到 Kubernetes，那么它们可能是你在旅途中早期就会使用的东西。
- en: Let’s start with a simple StatefulSet that shows the basics. Listing 8.1 shows
    that StatefulSets have pretty much the same specs as other Pod controllers, except
    that they also need to include the name of a Service.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的 StatefulSet 开始，它展示了基础知识。列表 8.1 显示，StatefulSets 几乎与其他 Pod 控制器具有相同的规范，只是它们还需要包含一个服务的名称。
- en: Listing 8.1 todo-db.yaml, a simple StatefulSet
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 todo-db.yaml，一个简单的 StatefulSet
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: When you deploy this YAML file, you’ll get a StatefulSet running two Postgres
    pods, but don’t get too excited—they’re just two separate database servers that
    happen to be managed by the same controller. There’s more work needed to get two
    Pods to be a replicated database cluster, and we’ll get there over the next few
    sections.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署这个 YAML 文件时，你会得到一个运行两个 Postgres Pod 的 StatefulSet，但不要过于兴奋——它们只是两个由同一个控制器管理的独立数据库服务器。要使两个
    Pod 成为复制的数据库集群，还需要做更多的工作，我们将在接下来的几节中实现这一点。
- en: Try it now Deploy the StatefulSet from listing 8.1, and see how the Pods it
    creates compare to Pods managed by a ReplicaSet.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 从列表 8.1 中部署 StatefulSet，并观察它创建的 Pod 与由 ReplicaSet 管理的 Pod 的比较。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You can see from figure 8.2 that a StatefulSet works in a very different way
    from a ReplicaSet or a DaemonSet. The Pods have a predictable name, which is the
    StatefulSet name followed by the index of the Pod, so you can manage the Pods
    using their names instead of having to use a label selector.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从图 8.2 中你可以看到，StatefulSet 的工作方式与 ReplicaSet 或 DaemonSet 非常不同。Pod 有一个可预测的名称，即
    StatefulSet 名称后跟 Pod 的索引，因此你可以使用 Pod 的名称来管理它们，而无需使用标签选择器。
- en: '![](../Images/8-2.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-2.jpg)'
- en: Figure 8.2 A StatefulSet can create the environment for a clustered application,
    but the app needs to configure itself.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 StatefulSet 可以为集群应用创建环境，但应用需要自行配置。
- en: The Pods are still managed by the controller, but in a more predictable way
    than with a ReplicaSet. Pods are created in order from zero up to n; if you scale
    down the set, the controller will remove them in the reverse order, starting from
    n and working down. If you delete a Pod, the controller will create a replacement.
    It will have the same name and configuration as the original, but it will be a
    new Pod.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 仍然由控制器管理，但比 ReplicaSet 更可预测。Pod 按顺序从零到 n 创建；如果你缩小集合，控制器将按相反的顺序删除它们，从 n 开始向下工作。如果你删除一个
    Pod，控制器将创建一个替换。它将具有与原始 Pod 相同的名称和配置，但将是一个新的 Pod。
- en: Try it now Delete Pod 0 of the StatefulSet, and see that Pod 0 comes back again.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 删除 StatefulSet 的 Pod 0，并观察 Pod 0 是否会再次出现。
- en: '[PRE2]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: You can see in figure 8.3 that a StatefulSet provides a stable environment for
    the app. Pod 0 is replaced with an identical Pod 0, but that doesn’t trigger a
    whole new set; the original Pod 1 remains. Ordering is applied only for creation
    and scaling, not for replacing missing Pods.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从图 8.3 中看到，StatefulSet 为应用提供了一个稳定的环境。Pod 0 被一个相同的 Pod 0 替换，但这不会触发整个新集；原始的
    Pod 1 仍然存在。排序仅应用于创建和扩展，不应用于替换缺失的 Pod。
- en: '![](../Images/8-3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-3.jpg)'
- en: Figure 8.3 StatefulSets replace missing replicas exactly as they were.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 StatefulSets 正确替换了缺失的副本。
- en: The StatefulSet is only the first part of modeling a stable environment. You
    can get DNS names for each Pod linking the StatefulSet to a service, and that
    means you can configure Pods to initialize themselves by working with other replicas
    at known addresses.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSet 只是建模稳定环境的第一步。你可以为每个 Pod 获取 DNS 名称，将 StatefulSet 连接到服务，这意味着你可以配置
    Pod 通过与其他已知地址的副本一起工作来自行初始化。
- en: 8.2 Bootstrapping Pods with init containers in StatefulSets
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 在 StatefulSets 中使用 init 容器引导 Pod
- en: 'The Kubernetes API composes objects from other objects: the Pod template in
    a StatefulSet definition is the same object type you use in the template for a
    Deployment and in a bare Pod definition. That means all the Pod features are available
    for StatefulSets even though the Pods themselves are managed in a different way.
    We learned about init containers in chapter 7, and they’re a perfect tool for
    the complicated initialization steps you often need in clustered applications.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes API由其他对象组成对象：状态集定义中的Pod模板与你在Deployment模板和裸Pod定义中使用的对象类型相同。这意味着所有Pod功能都可用于状态集，尽管Pod本身是以不同的方式管理的。我们在第7章中学习了初始化容器，它们是解决集群应用程序中经常需要的复杂初始化步骤的完美工具。
- en: Listing 8.2 shows the first init container for an update to the Postgres deployment.
    Multiple init containers in this Pod spec run in sequence, and because the Pods
    also start in sequence, you can guarantee that the first init container in Pod
    1 won’t run until Pod 0 is fully initialized and ready.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2显示了Postgres部署更新的第一个初始化容器。在这个Pod规范中，多个初始化容器按顺序运行，因为Pod也是按顺序启动的，所以你可以保证Pod
    1中的第一个初始化容器不会在Pod 0完全初始化和准备好之前运行。
- en: Listing 8.2 todo-db.yaml, the replicated Postgres setup with initialization
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.2 todo-db.yaml，带有初始化的复制Postgres设置
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The script that runs in this init container has two functions: if it’s running
    in Pod 0, it just prints a log to confirm that this is the database primary, and
    then the container exits; if it’s running in any other Pod, it makes a DNS lookup
    call to the primary, to make sure it’s accessible before continuing. The next
    init container will start the replication process, so this one makes sure everything
    is in place.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个初始化容器中运行的脚本有两个功能：如果它在Pod 0中运行，它只是打印一条日志以确认这是数据库主节点，然后容器退出；如果它在任何其他Pod中运行，它会对主节点进行DNS查找调用，以确保在继续之前它是可访问的。下一个初始化容器将启动复制过程，因此这个容器确保一切就绪。
- en: The exact steps in this example are specific to Postgres, but the pattern is
    the same for many clustered and replicated applications—MySQL, Elasticsearch,
    RabbitMQ, and NATS all have broadly similar requirements. Figure 8.4 shows how
    you can model that pattern using init containers in a StatefulSet.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中的确切步骤是针对Postgres的，但对于许多集群和复制应用程序（如MySQL、Elasticsearch、RabbitMQ和NATS）的模式是相同的。图8.4显示了如何使用状态集中的初始化容器来模拟该模式。
- en: '![](../Images/8-4.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图8-4](../Images/8-4.jpg)'
- en: Figure 8.4 The stable environment of a StatefulSet gives guarantees you can
    use in initialization.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 状态集的稳定环境为你提供了初始化时可以使用的保证。
- en: You define DNS names for the individual Pods in a StatefulSet by identifying
    a Service in the spec, but it needs to be a special configuration of headless
    Service. Listing 8.3 shows how the database Service is configured with no ClusterIP
    address and with a selector for the Pods.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过在规范中标识一个服务来为状态集中的单个Pod定义DNS名称，但它需要是特殊配置的无头服务。列表8.3显示了数据库服务如何配置没有ClusterIP地址以及具有Pod选择器。
- en: Listing 8.3 todo-db-service.yaml, a headless Service for a StatefulSet
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 todo-db-service.yaml，状态集的无头服务
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: A Service with no ClusterIP is still available as a DNS entry in the cluster,
    but it doesn’t use a fixed IP address for the Service. There’s no virtual IP that
    is routed to the real destination by the networking layer. Instead, the DNS entry
    for the service returns an IP address for each Pod in the StatefulSet, and each
    Pod additionally gets its own DNS entry.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 没有ClusterIP的服务在集群中仍然可以作为DNS条目使用，但它不使用固定的IP地址作为服务。没有虚拟IP是通过网络层路由到实际目的地的。相反，服务的DNS条目为状态集中的每个Pod返回一个IP地址，并且每个Pod还额外获得自己的DNS条目。
- en: Try it now We’ve already deployed the headless Service, so we can use a sleep
    Deployment to query DNS for the StatefulSet and see how it compares to a typical
    ClusterIP service.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 我们已经部署了无头服务，因此我们可以使用sleep Deployment来查询DNS以查看状态集与典型ClusterIP服务相比如何。
- en: '[PRE5]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ll see in this exercise that the DNS lookup for the service returns two
    IP addresses, which are the internal Pod IPs. The Pods themselves have their own
    DNS entry in the format `pod-name.service-name` with the usual cluster domain
    suffix. Figure 8.5 shows my output.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会发现对该服务的DNS查找返回了两个IP地址，这些是内部Pod IP。Pod本身在`pod-name.service-name`的格式下有自己的DNS条目，带有常规的集群域名后缀。图8.5显示了我的输出。
- en: '![](../Images/8-5.jpg)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图8-5](../Images/8-5.jpg)'
- en: Figure 8.5 StatefulSets give each Pod its own DNS entry, so they are individually
    addressable.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 状态集为每个Pod提供自己的DNS条目，因此它们可以单独寻址。
- en: 'Predictable startup order and individually addressable Pods are the foundation
    for initializing a clustered app in a StatefulSet. The details will differ wildly
    between applications, but broadly, the startup logic for the Pod will be something
    like this: if I am Pod 0, then I’m the primary, so I do all the primary setup
    stuff; otherwise, I’m a secondary, so I’ll give the primary some time to get set
    up, check that everything’s working, and then synchronize using the Pod 0 address.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可预测的启动顺序和可单独寻址的Pod是初始化StatefulSet中集群应用的基石。不同应用之间的细节可能会有很大差异，但总体上，Pod的启动逻辑可能如下：如果我是Pod
    0，那么我是主节点，所以我将执行所有主节点的设置工作；否则，我是辅助节点，所以我将给主节点一些时间来设置，检查一切是否正常工作，然后使用Pod 0的地址进行同步。
- en: The actual setup for Postgres is quite involved, so I’ll skip over it here.
    It uses scripts in ConfigMaps with init containers to set up the primary and secondaries.
    I use various techniques we’ve covered in the book so far in the spec for the
    StatefulSet, which is worth exploring, but the details of the scripts are all
    specific to Postgres.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres的实际设置相当复杂，所以在这里我将跳过它。它使用ConfigMap中的脚本在初始化容器中设置主节点和辅助节点。我在StatefulSet的规范中使用了我们在书中已经介绍过的各种技术，这值得探索，但脚本的细节都是特定于Postgres的。
- en: Try it now Update the database to make it a replicated setup. There are configuration
    files and startup scripts in ConfigMaps, and the StatefulSet is updated to use
    them in init containers.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：更新数据库以使其成为复制设置。ConfigMap中有配置文件和启动脚本，StatefulSet被更新以在初始化容器中使用它们。
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Postgres uses an active-passive model for replication, so the primary is used
    for database reads and writes, and the secondaries sync data from the primary
    and can be used by clients, but only for read access. Figure 8.6 shows how the
    init containers recognize the role for each Pod and initialize them.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres使用主动-被动模型进行复制，因此主节点用于数据库的读取和写入，辅助节点从主节点同步数据，并且可以被客户端使用，但仅限于读访问。图8.6展示了初始化容器如何识别每个Pod的角色并初始化它们。
- en: '![](../Images/8-6.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-6.jpg)'
- en: Figure 8.6 Pods are replicas, but they can have different behavior, using init
    containers to choose a role.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6显示Pod是副本，但它们可以有不同的行为，使用初始化容器来选择角色。
- en: Most of the complexity in initializing replicated apps like this is around modelling
    the workflow, which is specific to the app. The init container scripts here use
    the `pg_isready` tool to verify that the primary is ready to receive connections
    and the `pb_basebackup` tool to start the replication. Those implementation details
    are abstracted away from operators managing the system. They can add more replicas
    by scaling up the StatefulSet, like with any other replication controller.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化此类复制应用的大部分复杂性在于对工作流程的建模，这是特定于应用的。这里的初始化容器脚本使用`pg_isready`工具来验证主节点是否准备好接收连接，并使用`pb_basebackup`工具来启动复制。这些实现细节被抽象化，以便系统管理员管理。他们可以通过扩展StatefulSet来添加更多副本，就像使用任何其他复制控制器一样。
- en: Try it now Scale up the database to add another replica, and confirm that the
    new Pod also starts as a secondary.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：扩展数据库以添加另一个副本，并确认新的Pod也以辅助节点的方式启动。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: I wouldn’t call this an enterprise-grade production setup, but it’s a good starting
    point where a real Postgres expert could take over. You now have a functional,
    replicated Postgres database cluster with a primary and two secondaries—Postgres
    calls them *standbys*. As you can see in figure 8.7, all the standbys start in
    the same way, syncing data from the primary, and they can all be used by clients
    for read-only access.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会称这为一个企业级的生产设置，但这是一个很好的起点，真正的Postgres专家可以在这里接管。你现在有一个功能齐全的、具有主节点和两个辅助节点的复制Postgres数据库集群——Postgres称它们为*备用节点*。如图8.7所示，所有备用节点都以相同的方式启动，从主节点同步数据，并且它们都可以被客户端用于只读访问。
- en: '![](../Images/8-7.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-7.jpg)'
- en: Figure 8.7 Using individually addressable Pods means secondaries can always
    find the primary.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7使用可单独寻址的Pod意味着辅助节点总能找到主节点。
- en: 'One obvious part is missing here—the actual storage of the data. The setup
    we have isn’t really usable because it doesn’t have any volumes for storage, so
    each database container writes data in its own writable layer, not in a persistent
    volume. StatefulSets have a neat way of defining volume requirements: you can
    include a set of Persistent Volume Claim (PVC) templates in the spec.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这里缺少一个明显的部分——实际的数据存储。我们设置的配置并不是真正可用的，因为它没有存储卷，所以每个数据库容器都在自己的可写层中写入数据，而不是在持久卷中。StatefulSets有一种定义卷需求的好方法：你可以在规范中包含一组持久卷声明（PVC）模板。
- en: 8.3 Requesting storage with volume claim templates
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 使用卷声明模板请求存储
- en: Volumes are part of the standard Pod spec, and you can load ConfigMaps and Secrets
    into the Pods for a StatefulSet. You can even include a PVC and mount it into
    the app container, but that gives you volumes that are shared among all the Pods.
    That’s fine for read-only configuration settings where you want every Pod to have
    the same data, but if you mount a standard PVC for data storage, then every Pod
    will try to write to the same volume.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 卷是标准 Pod 规范的一部分，你可以将 ConfigMaps 和 Secrets 加载到 Pod 中以供 StatefulSet 使用。你甚至可以包括一个
    PVC 并将其挂载到应用程序容器中，但这会给所有 Pod 提供共享的卷。这对于只读配置设置来说是可以的，你希望每个 Pod 都有相同的数据，但如果挂载标准
    PVC 用于数据存储，那么每个 Pod 都会尝试写入相同的卷。
- en: You actually want each Pod to have its own PVC, and Kubernetes provides that
    for StatefulSets with the `volumeClaimTemplates` field in the spec. Volume claim
    templates can include a storage class as well as capacity and access mode requirements.
    When you deploy a StatefulSet with volume claim templates, it creates a PVC for
    each Pod, and they’re linked, so if Pod 0 is replaced, the new Pod 0 will attach
    to the PVC used by the previous Pod 0.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 你实际上希望每个 Pod 都有自己的 PVC，Kubernetes 通过 spec 中的 `volumeClaimTemplates` 字段为 StatefulSet
    提供了这一点。卷声明模板可以包括存储类以及容量和访问模式要求。当你使用卷声明模板部署 StatefulSet 时，它会为每个 Pod 创建一个 PVC，并且它们是链接的，所以如果
    Pod 0 被替换，新的 Pod 0 将连接到之前 Pod 0 使用的 PVC。
- en: Listing 8.4 shows a simple sleep spec that uses volume claim templates. As we
    learned in chapter 5, different Kubernetes platforms offer different storage classes,
    and I can’t be sure what your cluster provides. This spec omits the storage class,
    which means the volume will be dynamically provisioned using your cluster’s default
    storage class.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 显示了一个简单的 sleep 规范，它使用了卷声明模板。正如我们在第 5 章中学到的，不同的 Kubernetes 平台提供不同的存储类，我无法确定你的集群提供什么。这个规范省略了存储类，这意味着卷将使用你的集群的默认存储类动态配置。
- en: Listing 8.4 sleep-with-pvc.yaml, a StatefulSet with volume claim templates
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 sleep-with-pvc.yaml，一个带有卷声明模板的 StatefulSet
- en: '[PRE8]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: We’ll use this exercise to see how volume claim templates in StatefulSets work
    in a simple environment before adding them as the storage layer for our database
    cluster.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个练习来查看在简单环境中 StatefulSet 中的卷声明模板是如何工作的，然后再将其作为数据库集群的存储层添加。
- en: Try it now Deploy the StatefulSet from listing 8.4, and explore the PVCs it
    creates.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 从列表 8.4 中部署 StatefulSet，并探索它创建的 PVC。
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You’ll see that each Pod in the set gets a PVC created dynamically, which in
    turn creates a PersistentVolume using the default storage class (or the requested
    storage class, if I had included one in the spec). The PVCs all have the same
    configuration, and they use the same stable approach as Pods in the StatefulSet:
    they have a predictable name, and, as you see in figure 8.8, each Pod has its
    own PVC, giving the replicas independent storage.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 你会看到集合中的每个 Pod 都会动态创建一个 PVC，这反过来又使用默认存储类（或者如果我在规范中包含了一个，则是请求的存储类）创建一个 PersistentVolume。所有
    PVC 都有相同的配置，并且它们使用与 StatefulSet 中的 Pod 相同的稳定方法：它们有一个可预测的名称，正如你在图 8.8 中看到的，每个 Pod
    都有自己的 PVC，这为副本提供了独立的存储。
- en: '![](../Images/8-8.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图 8-8](../Images/8-8.jpg)'
- en: Figure 8.8 Volume claim templates dynamically create storage for Pods in StatefulSets.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.8 卷声明模板在 StatefulSet 中动态为 Pods 创建存储。
- en: The link between the Pod and its PVC is maintained when Pods are replaced, which
    is what really gives StatefulSets the power to run data-heavy applications. When
    you roll out an update to your app, the new Pod 0 will attach to the PVC from
    the previous Pod 0, and the new app container will have access to the exact same
    state as the replaced app container.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pod 被替换时，Pod 和其 PVC 之间的链接被保留，这正是 StatefulSet 能够运行数据密集型应用程序的真正原因。当你推出应用程序的更新时，新的
    Pod 0 将连接到之前 Pod 0 的 PVC，新的应用程序容器将能够访问与被替换的应用程序容器完全相同的状态。
- en: Try it now Trigger a Pod replacement by removing Pod 0\. It will be replaced
    with another Pod 0 that attaches to the same PVC.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 通过移除 Pod 0 触发 Pod 替换。它将被另一个 Pod 0 替换，该 Pod 0 将连接到相同的 PVC。
- en: '[PRE10]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: This simple example makes this clear—you can see in figure 8.9 that the new
    Pod 0 has access to all the data from the original Pod. In a production cluster,
    you would specify a storage class that uses a volume type that any node can access,
    so replacement Pods can run on any node, and the app container can still mount
    the PVC.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的例子使这一点变得清晰——你可以在图 8.9 中看到新的 Pod 0 可以访问原始 Pod 的所有数据。在生产集群中，你会指定一个使用任何节点都可以访问的卷类型的存储类，这样替换的
    Pod 就可以在任何节点上运行，应用程序容器仍然可以挂载 PVC。
- en: '![](../Images/8-9.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图8-9](../Images/8-9.jpg)'
- en: Figure 8.9 Stability in a StatefulSet extends to preserving the PVC link between
    Pod replacements.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 StatefulSet的稳定性延伸到保留Pod替换之间的PVC链接。
- en: Volume claim templates are the final piece we need to add to the Postgres deployment
    to model a fully reliable database. StatefulSets are intended to present a stable
    environment for your app, so they’re less flexible than other controllers when
    it comes to updates—you can’t update an existing StatefulSet and make a fundamental
    change, like adding volume claims. You need to make sure your design meets the
    app requirements for a StatefulSet because it’s hard to maintain service levels
    during big changes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 卷声明模板是我们需要添加到Postgres部署中的最后一部分，以模拟一个完全可靠的数据库。StatefulSets旨在为您的应用程序提供一个稳定的环境，因此它们在更新方面不如其他控制器灵活——您不能更新现有的StatefulSet并做出基本更改，例如添加卷声明。您需要确保您的设计满足StatefulSet的应用程序要求，因为在重大变化期间很难维护服务水平。
- en: Try it now We’ll update the Postgres deployment, but first we need to remove
    the existing StatefulSet.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 我们将更新Postgres部署，但首先我们需要删除现有的StatefulSet。
- en: '[PRE11]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: When you run this exercise, you should see clearly how the StatefulSet preserves
    order and waits for each Pod to be running before it starts the next Pod. PVCs
    are created for each Pod in sequence, too, as you can see from my output, shown
    in figure 8.10.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行这个练习时，你应该清楚地看到StatefulSet如何保持顺序，并在启动下一个Pod之前等待每个Pod运行。从我的输出中，你可以看到PVCs也是按顺序为每个Pod创建的，如图8.10所示。
- en: '![](../Images/8-10.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图8-10](../Images/8-10.jpg)'
- en: Figure 8.10 PVCs are created and allocated to the Postgres Pods.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 PVCs被创建并分配给Postgres Pods。
- en: It feels like we’ve spent a long time on StatefulSets, but it’s a topic you
    should understand well so you’re not taken by surprise when someone asks you to
    move their database to Kubernetes (which they will). StatefulSets come with a
    good deal of complexity, and you’ll avoid using them most of the time. But if
    you are looking to migrate existing apps to Kubernetes, StatefulSets could be
    the difference between being able to run everything on the same platform or having
    to keep a handful of VMs just to run one or two apps.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 感觉我们已经在StatefulSets上花费了很长时间，但这是一个你应该很好地理解的课题，这样当有人要求你将他们的数据库迁移到Kubernetes（他们会的）时，你不会感到惊讶。StatefulSets附带了很多复杂性，你大部分时间都不会使用它们。但如果你正在寻找将现有应用程序迁移到Kubernetes，StatefulSets可能是能够在同一平台上运行所有内容或必须保留几个VM来运行一个或两个应用程序之间的区别。
- en: We’ll finish the section with an exercise to show the power of our clustered
    database. The Postgres secondary replicates all the data from the primary, and
    it can be used by clients for read-only access. If we had a serious production
    issue with our to-do list app that was causing it to lose data, we have the option
    to switch to read-only mode and use the secondary while we investigate the problem.
    That keeps the app running safely with minimal functionality, which is definitely
    preferable to taking it offline.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用一项练习来结束本节，以展示我们集群数据库的力量。Postgres从主副本复制所有数据，并且可以被客户端用于只读访问。如果我们的事务列表应用出现了严重的生产问题，导致它丢失数据，我们可以选择切换到只读模式并使用副本来调查问题。这样可以在最小功能的情况下安全地运行应用程序，这绝对比将其关闭要好。
- en: Try it now Run the to-do web app and enter some items. In the default configuration,
    it connects to the Postgres primary in Pod 0 of the StatefulSet. Then we’ll switch
    the app configuration to put it into read-only mode. This makes it connect to
    the read-only Postgres standby in Pod 1, which has replicated all the data from
    Pod 0.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 运行待办事项Web应用并输入一些条目。在默认配置中，它连接到StatefulSet的Pod 0中的Postgres主副本。然后我们将切换应用程序配置以将其置于只读模式。这使得它连接到Pod
    1中的只读Postgres备用副本，该副本已复制了Pod 0的所有数据。
- en: '[PRE12]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: You can see my output in figure 8.11, with some tiny screenshots to show the
    app running in read-only mode but still with access to all the data.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图8.11中看到我的输出，其中包含一些小截图，展示了应用程序在只读模式下运行，但仍能访问所有数据。
- en: '![](../Images/8-11.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图8-11](../Images/8-11.jpg)'
- en: Figure 8.11 Switching an app to read-only mode is a useful option if there's
    a data issue
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 如果存在数据问题，将应用程序切换到只读模式是一个有用的选项
- en: Postgres has existed as a SQL database engine since 1996—it predates Kubernetes
    by almost 25 years. Using a StatefulSet, you can model an application environment
    that suits Postgres and other clustered applications like it, providing stable
    networking, storage, and initialization in the dynamic world of containers.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: Postgres自1996年以来一直作为SQL数据库引擎存在——它比Kubernetes早了近25年。使用StatefulSet，你可以创建一个适合Postgres和其他类似集群应用的应用环境，提供稳定的网络、存储和初始化，在容器动态世界中。
- en: 8.4 Running maintenance tasks with Jobs and CronJobs
  id: totrans-90
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 使用作业和CronJobs运行维护任务
- en: 'Data-intensive apps need replicated data with storage aligned to compute, and
    they usually also need some independent nurturing of the storage layer. Data backups
    and reconciliation are well suited to another type of Pod controller: the Job.
    Kubernetes Jobs are defined with a Pod spec, and they run the Pod as a batch job,
    ensuring it runs to completion.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 数据密集型应用需要与计算对齐的复制数据存储，并且通常还需要对存储层进行一些独立的管理。数据备份和校验非常适合另一种类型的Pod控制器：作业。Kubernetes作业通过Pod规范定义，并以批处理作业的方式运行Pod，确保其运行至完成。
- en: Jobs aren’t just for stateful apps; they’re a great way to bring a standard
    approach to any batch-processing problems, where you can hand off all the scheduling
    and monitoring and retry logic to the cluster. You can run any container image
    in the Pod for a Job, but it should start a process that ends; otherwise, your
    jobs will keep running forever. Listing 8.5 shows a Job spec that runs the Pi
    application in batch mode.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 作业不仅仅适用于有状态的应用；它们是解决任何批处理问题的标准方法，在这些情况下，你可以将所有调度、监控和重试逻辑交给集群。你可以在Pod中运行任何容器镜像作为作业，但应该启动一个结束进程；否则，你的作业将永远运行。列表8.5展示了运行Pi应用程序的批处理模式的作业规范。
- en: Listing 8.5 pi-job.yaml, a simple Job to calculate Pi
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 pi-job.yaml，一个简单的计算π的作业
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The Job template contains a standard Pod spec, with the addition of a required
    `restartPolicy` field. That field controls the behavior of the Job in response
    to failure. You can choose to have Kubernetes restart the same Pod with a new
    container if the run fails or always create a replacement Pod, potentially on
    a different node. In a normal run of the Job where the Pod completes successfully,
    the Job and the Pod are retained so the container logs are available.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 作业模板包含一个标准的Pod规范，并添加了一个必需的`restartPolicy`字段。该字段控制作业在失败时的行为。你可以选择在运行失败时让Kubernetes使用新容器重启相同的Pod，或者始终创建一个替换Pod，可能是在不同的节点上。在作业的正常运行中，如果Pod成功完成，作业和Pod将被保留，以便容器日志可用。
- en: Try it now Run the Pi Job from listing 8.5, and check the output from the Pod.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试运行Pi作业，并检查Pod的输出。
- en: '[PRE14]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Jobs add their own labels to the Pods they create. The `job-name` label is always
    added, so you can navigate to Pods from the Job. My output in figure 8.12 shows
    that the Job has had one successful completion and the calculation result is available
    in the logs.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 作业会给它们创建的Pod添加自己的标签。`job-name`标签始终被添加，这样你就可以从作业导航到Pod。我在图8.12中的输出显示，作业已经成功完成一次，计算结果可在日志中找到。
- en: '![](../Images/8-12.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-12.jpg)'
- en: Figure 8.12 Jobs create Pods, make sure they complete, and then leave them in
    the cluster.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12作业创建Pod，确保它们完成，然后将其留在集群中。
- en: 'It’s always useful to have different options for computing Pi, but this is
    just a simple example. You can use any container image in the Pod spec so you
    can run any kind of batch process with a Job. You might have a set of input items
    that need the same work done on them; you can create one Job for the whole set,
    which creates a Pod for each item, and Kubernetes distributes the work all throughout
    the cluster. The Job spec supports this with the following two optional fields:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有不同的计算π的选项总是很有用，但这只是一个简单的例子。你可以在Pod规范中使用任何容器镜像，因此你可以使用作业运行任何类型的批处理过程。你可能有一组需要执行相同工作的输入项；你可以为整个集合创建一个作业，它为每个项目创建一个Pod，Kubernetes将工作分布在整个集群中。作业规范通过以下两个可选字段支持这一点：
- en: '`completions`—specifies how many times the Job should run. If your Job is processing
    a work queue, then the app container needs to understand how to fetch the next
    item to work on. The Job itself just ensures that it runs a number of Pods equal
    to the desired number of completions.'
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`completions`——指定作业应运行多少次。如果你的作业正在处理工作队列，那么应用容器需要理解如何获取下一个要处理的项目。作业本身只确保运行与所需完成次数相等的Pod数量。'
- en: '`parallelism`—specifies how many Pods to run in parallel for a Job with multiple
    completions set. This setting lets you tweak the speed of running the Job, balancing
    that with the compute requirements on the cluster.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`parallelism`——指定对于具有多个完成设置的作业，并行运行多少个Pod。此设置允许您调整作业的运行速度，同时平衡集群的计算需求。'
- en: 'One last Pi example for this chapter: a new Job spec that runs multiple Pods
    in parallel, each computing Pi to a random number of decimal places. This spec
    uses an init container to generate the number of decimal places to use, and the
    app container reads that input using a shared `EmptyDir` mount. This is a nice
    approach because the app container doesn’t need to be modified to work in a parallel
    environment. You could extend this with an init container that fetched a work
    item from a queue, so the app itself wouldn’t need to be aware of the queue.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 本章最后一个Pi示例：一个新作业规格，它并行运行多个Pod，每个Pod计算到随机位数的π。此规格使用初始化容器生成要使用的位数，应用程序容器使用共享`EmptyDir`挂载读取该输入。这是一个很好的方法，因为应用程序容器不需要修改就可以在并行环境中工作。您可以使用初始化容器从队列中获取工作项，这样应用程序本身就不需要知道队列的存在。
- en: Try it now Run an alternative Pi Job that uses parallelism and shows that multiple
    Pods from the same spec can process different workloads.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看：运行一个使用并行处理并展示来自同一规格的多个Pod可以处理不同工作负载的替代Pi作业。
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This exercise may take a while to run, depending on your hardware and the number
    of decimal places it generates. You’ll see all the Pods running in parallel, working
    on their own calculations. The final output will be three sets of Pi, probably
    to thousands of decimal places. I’ve abbreviated my results, shown in figure 8.13.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习可能需要一段时间才能运行，具体取决于您的硬件和生成的位数数量。您将看到所有Pod并行运行，各自进行计算。最终输出将是三组π，可能精确到数千位。我已经将我的结果简化，如图8.13所示。
- en: '![](../Images/8-13.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-13.jpg)'
- en: Figure 8.13 Jobs can run multiple Pods from the same spec that each process
    different workloads.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 作业可以运行来自同一规格的多个Pod，每个Pod处理不同的工作负载。
- en: Jobs are a great tool to have in your pocket. They’re perfect for anything compute
    intensive or IO intensive, where you want to make sure a process completes but
    don’t mind when. You can even submit Jobs from your own application-a web app
    running in Kubernetes has access to the Kubernetes API server, and it can create
    Jobs to run work for users.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作业是您口袋里的一大法宝。它们非常适合任何计算密集型或I/O密集型任务，您希望确保一个进程完成，但不介意何时完成。您甚至可以从自己的应用程序提交作业——运行在Kubernetes中的Web应用程序可以访问Kubernetes
    API服务器，并创建作业来为用户运行工作。
- en: The real power of Jobs is that they run in the context of the cluster, so they
    have all the cluster resources available to them. Back to the Postgres example,
    we can run a database-backup process in a Job, and the Pod it runs can access
    the Pods in the StatefulSet or the PVCs, depending on what it needs to do. That
    takes care of the nurturing aspect of these data-intensive apps, but those Jobs
    need to be run regularly, which is where the CronJob comes in. The CronJob is
    a Job controller, which creates Jobs on a regular schedule. Figure 8.14 shows
    the workflow.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 作业的真正力量在于它们在集群的上下文中运行，因此它们可以使用集群的所有资源。回到Postgres示例，我们可以在作业中运行数据库备份过程，并且运行的Pod可以根据需要访问StatefulSet中的Pod或PVC。这解决了这些数据密集型应用程序的培育方面，但这些作业需要定期运行，这就是CronJob的作用。CronJob是一个作业控制器，它按固定的时间表创建作业。图8.14显示了工作流程。
- en: '![](../Images/8-14.jpg)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/8-14.jpg)'
- en: Figure 8.14 CronJobs are the ultimate owner of the Job Pods, so everything can
    be removed with cascading deletes.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 CronJobs是作业Pod的最终所有者，因此可以通过级联删除删除所有内容。
- en: CronJob specs include a Job spec, so you can do anything in a CronJob that you
    can do in a Job, including running multiple completions in parallel. The schedule
    for running the Job uses the Linux Cron format, which lets you express everything
    from simple “every minute” or “every day” schedules to more complex “at 4 a.m.
    and 6 a.m. every Sunday” routines. Listing 8.6 shows part of the CronJob spec
    for running database backups.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob规格包括作业规格，因此您可以在CronJob中执行任何在作业中可以执行的操作，包括并行运行多个完成。作业运行的计划使用Linux Cron格式，允许您表达从简单的“每分钟”或“每天”计划到更复杂的“每周日早上4点和6点”等复杂计划。列表8.6显示了运行数据库备份的CronJob规格的一部分。
- en: Listing 8.6 todo-db-backup-cronjob.yaml, a CronJob for database backups
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6 todo-db-backup-cronjob.yaml，数据库备份的CronJob
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The full spec uses the Postgres Docker image, with a command to run the `pg_dump`
    backup tool. The Pod loads environment variables and passwords from the same ConfigMaps
    and Secrets that the StatefulSet uses, so there’s no duplication in the config
    file. It also uses its own PVC as the storage location to write the backup files.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 完整规范使用 Postgres Docker 镜像，并包含一个运行 `pg_dump` 备份工具的命令。Pod 从与 StatefulSet 使用相同的
    ConfigMaps 和 Secrets 加载环境变量和密码，因此在配置文件中没有重复。它还使用自己的 PVC 作为写入备份文件的存储位置。
- en: Try it now Create a CronJob from the spec in listing 8.6 to run a database backup
    Job every two minutes.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：根据列表 8.6 中的规范创建一个 CronJob，每两分钟运行一次数据库备份作业。
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The CronJob is set to run every two minutes, so you’ll need to give it time
    to fire up during this exercise. On schedule, the CronJob creates a Job, which
    creates a Pod, which runs the `backup` command. The Job ensures the Pod completes
    successfully. You can confirm the backup file is written by mounting the same
    PVC in another Pod. You can see it all works correctly in figure 8.15.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: CronJob 设置为每两分钟运行一次，所以在这次练习中你需要给它一些时间来启动。按照计划，CronJob 创建一个作业，该作业创建一个 Pod，并运行
    `backup` 命令。作业确保 Pod 成功完成。你可以通过在另一个 Pod 中挂载相同的 PVC 来确认备份文件已写入。你可以在图 8.15 中看到所有操作都正确无误。
- en: '![](../Images/8-15.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-15.jpg)'
- en: Figure 8.15 CronJobs run Pods, which can access other Kubernetes objects. This
    one connects to a database Pod.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.15 CronJobs 运行 Pods，这些 Pods 可以访问其他 Kubernetes 对象。这个示例连接到一个数据库 Pod。
- en: CronJobs don’t perform an automatic cleanup for Pods and Jobs. The time-to-live
    (TTL) controller does this, but it’s an alpha-grade feature that isn’t available
    in many Kubernetes platforms. Without it you need to manually delete the child
    objects when you’re sure you no longer need them. You can also move CronJobs to
    a suspended state, which means the object spec still exists in the cluster, but
    it doesn’t run until the CronJob is activated again.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: CronJobs 不会自动清理 Pods 和 Jobs。生存时间 (TTL) 控制器负责这项工作，但它是一个 alpha 级别的功能，在许多 Kubernetes
    平台上不可用。如果没有它，当你确定不再需要它们时，你需要手动删除子对象。你还可以将 CronJobs 移动到挂起状态，这意味着对象规范仍然存在于集群中，但直到
    CronJob 再次激活，它不会运行。
- en: Try it now Suspend the CronJob so it doesn’t keep creating backup Jobs, and
    then explore the status of the CronJob and its Jobs.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：挂起 CronJob，使其不再创建备份作业，然后探索 CronJob 及其作业的状态。
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: If you explore the object hierarchy, you’ll see that CronJobs don’t follow the
    standard controller model, with a label selector to identify the Jobs it owns.
    You can add your own labels in the Job template for the CronJob, but if you don’t
    do that, you need to identify Jobs where the owner reference is the CronJob, as
    shown in figure 8.16.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你探索对象层次结构，你会看到 CronJobs 不遵循标准的控制器模型，没有使用标签选择器来识别它拥有的作业。你可以在 CronJob 的作业模板中添加自己的标签，但如果你不这样做，你需要识别所有所有者引用为
    CronJob 的作业，如图 8.16 所示。
- en: '![](../Images/8-16.jpg)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/8-16.jpg)'
- en: Figure 8.16 CronJobs don’t use a label selector to model ownership, because
    they don’t keep track of Jobs.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.16 CronJobs 不使用标签选择器来建模所有权，因为它们不跟踪作业。
- en: As you start to make more use of Jobs and CronJobs, you’ll realize that the
    simplicity of the spec masks some complexity in the process and presents some
    interesting failure modes. Kubernetes does its best to make sure your batch jobs
    start when you want them to and run to completion, which means your containers
    need to be resilient. Completing a Job might mean restarting a Pod with a new
    container or replacing the Pod on a new node, and for CronJobs, multiple Pods
    could be running if the process takes longer than the schedule interval. Your
    container logic needs to allow for all those scenarios.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 当你开始更多地使用 Jobs 和 CronJobs 时，你会发现规范的简单性掩盖了过程中的某些复杂性，并呈现了一些有趣的故障模式。Kubernetes
    尽力确保你的批处理作业在你想要的时候启动并运行到完成，这意味着你的容器需要具有弹性。完成作业可能意味着重启一个带有新容器的 Pod 或者在新的节点上替换 Pod，而对于
    CronJobs，如果过程持续时间超过计划间隔，则可能运行多个 Pod。你的容器逻辑需要允许所有这些场景。
- en: Now you know how to run data-heavy apps in Kubernetes, with StatefulSets to
    model a stable runtime environment and initialize the app, and CronJobs to process
    data backups and other regular maintenance work. We’ll close out the chapter thinking
    about whether this is really a good idea.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何在 Kubernetes 中运行数据密集型应用了，使用 StatefulSets 来建模稳定的运行时环境并初始化应用，以及使用 CronJobs
    来处理数据备份和其他定期维护工作。我们将思考这是否真的是一个好主意来结束本章。
- en: 8.5 Choosing your platform for stateful apps
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 选择状态化应用的平台
- en: The great promise of Kubernetes is that it gives you a single platform that
    can run all your apps, on any infrastructure. It’s hugely appealing to think that
    you can model all the aspects of any application in a chunk of YAML, deploy it
    with some kubectl commands, and know it will run in the same way on any cluster,
    taking advantage of all the extended features the platform offers. But data is
    precious and usually irreplaceable, so you need to think carefully before you
    decide that Kubernetes is the place to run data-heavy apps.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的伟大承诺是它为你提供了一个单一的平台，可以在任何基础设施上运行所有应用程序。想象一下，你可以在一小块YAML中模拟任何应用程序的所有方面，使用一些kubectl命令部署它，并知道它将在任何集群中以相同的方式运行，利用平台提供的所有扩展功能，这非常吸引人。但是数据是宝贵的，通常不可替代，所以在你决定Kubernetes是运行数据密集型应用程序的地方之前，你需要仔细思考。
- en: 'Figure 8.17 shows the full setup we’ve built in this chapter to run an almost-production-grade
    SQL database in Kubernetes. Just look at all the moving parts—do you really want
    to manage all that? And how much time will you need to invest just testing this
    setup with your own data sizes: validating that the replicas are syncing correctly,
    verifying the backups can be restored, running chaos experiments to be sure that
    failures are handled in the way you expect?'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17显示了我们在本章中构建的完整设置，以在Kubernetes中运行几乎生产级别的SQL数据库。只需看看所有移动部件——你真的想管理所有这些吗？而且你需要投入多少时间来测试这个设置与你的数据大小：验证副本是否正确同步，验证备份可以恢复，运行混沌实验以确保失败以你期望的方式处理？
- en: '![](../Images/8-17.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![图8.17](../Images/8-17.jpg)'
- en: Figure 8.17 Yikes! And this is a simplification that doesn’t show volumes or
    init containers.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 哎呀！而且这是一个简化版本，没有显示卷或初始化容器。
- en: Compare that to a managed database in the cloud. Azure, AWS, and GCP all offer
    managed services for Postgres, MySQL, and SQL Server, as well as their own custom
    cloud-scale databases. The cloud provider takes care of scale and high availability,
    including features for backups to cloud storage and more advanced options like
    threat detection. An alternative architecture just uses Kubernetes for compute
    and plugs in to managed cloud services for data and communication.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 将其与云中的托管数据库进行比较。Azure、AWS和GCP都为Postgres、MySQL和SQL Server提供托管服务，以及他们自己的定制云规模数据库。云服务提供商负责扩展和高可用性，包括备份到云存储和更高级的选项，如威胁检测。另一种架构只是使用Kubernetes进行计算，并连接到托管云服务以处理数据和通信。
- en: 'Which is the better option? Well, I’m a consultant by day, and I know the only
    real answer is: “It depends.” If you’re running in the cloud, then I think you
    need a very good reason *not* to use managed services in production, where data
    is critical. In nonproduction environments, it often makes sense to run equivalent
    services in Kubernetes instead, so you run a containerized database and message
    queue in your development and test environments for lower costs and ease of deployment
    and swap out to managed versions in production. Kubernetes makes a swap like that
    very simple, with all the Pod and Service configuration options.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个选项更好？嗯，我白天是一名顾问，我知道唯一真正的答案是：“这取决于。”如果你在云中运行，那么我认为你需要一个非常充分的理由*不*在生产中使用托管服务，因为数据至关重要。在非生产环境中，通常在Kubernetes中运行等效服务是有意义的，这样你就可以在开发和测试环境中以较低的成本和易于部署的方式运行容器化的数据库和消息队列，并在生产中切换到托管版本。Kubernetes通过所有Pod和服务配置选项使这种切换变得非常简单。
- en: In the data center, the picture is a little different. If you’re already invested
    in running Kubernetes on your own infrastructure, you’re taking on a lot of management,
    and it might make sense to maximize utilization of your clusters and use them
    for everything. If you choose to go that way, Kubernetes gives you the tools to
    migrate data-heavy apps into the cluster and run them with the levels of availability
    and scale that you need. Just don’t underestimate the complexity of getting there.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中心，情况略有不同。如果你已经在自己的基础设施上运行Kubernetes，那么你承担了大量的管理工作，这可能意味着最大化集群的利用率并将它们用于一切。如果你选择这样做，Kubernetes为你提供了将数据密集型应用程序迁移到集群并使用所需可用性和扩展级别的工具。只是不要低估达到这一目标复杂性。
- en: We’re done with StatefulSets and Jobs now, so we can clean up before going on
    to the lab.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经完成了StatefulSets和Jobs，所以在进入实验室之前我们可以进行清理。
- en: Try it now All the top-level objects are labeled, so we can remove everything
    with cascading deletes.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 所有顶层对象都被标记了，所以我们可以通过级联删除移除所有内容。
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 8.6 Lab
  id: totrans-142
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.6 实验室
- en: So, how much of your lunchtime do you have left for this lab? Can you model
    a MySQL database from scratch, with backups? Probably not, but don’t worry—this
    lab isn’t as involved as that. The goal is just to give you some experience working
    with StatefulSets and PVCs, so we’ll use a much simpler app. You’re going to run
    the Nginx web server in a StatefulSet, where each Pod writes log files to a PVC
    of its own, and then you’ll run a Job that prints the size of each Pod’s log file.
    The basic pieces are there for you, so it’s about applying some of the techniques
    from the chapter.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你还有多少午餐时间可以用来做这个实验？你能从头开始构建一个 MySQL 数据库，包括备份吗？可能不行，但别担心——这个实验并没有那么复杂。目标只是让你获得一些使用
    StatefulSets 和 PVCs 的工作经验，所以我们将会使用一个更简单的应用程序。你将在 StatefulSet 中运行 Nginx 网络服务器，其中每个
    Pod 将日志文件写入自己的 PVC，然后你将运行一个作业来打印每个 Pod 日志文件的大小。基本组件已经为你准备好了，所以这主要是应用章节中的一些技术。
- en: The starting point is the Nginx spec in `ch08/lab/nginx`, which runs a single
    Pod writing logs to an `EmptyDir` volume.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始点是 `ch08/lab/nginx` 中的 Nginx 规范，它运行一个 Pod 将日志写入一个 `EmptyDir` 卷。
- en: The Pod spec needs to be migrated to a StatefulSet definition, which is configured
    to run with three Pods and provide separate storage for each of them.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod 规范需要迁移到 StatefulSet 定义，该定义配置为运行三个 Pod，并为每个 Pod 提供单独的存储。
- en: When you have the StatefulSet working, you should be able to make calls to your
    Service and see the log files being written in the Pods.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你的 StatefulSet 运行正常时，你应该能够调用你的 Service 并看到 Pod 中正在写入的日志文件。
- en: Then you can complete the Job spec in the file `disk-calc-job.yaml`, adding
    the volume mounts so it can read the log files from the Nginx Pods.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，你可以在文件 `disk-calc-job.yaml` 中完成作业规范，添加卷挂载，以便它可以从 Nginx Pods 中读取日志文件。
- en: 'It’s not as bad as it looks, and it will get you thinking about storage and
    Jobs. My solution is on GitHub for you to check in the usual place: [https://github.com/sixeyed/kiamol/blob/master/ch08/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch08/lab/README.md).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 实际情况并没有看起来那么糟糕，这会让你开始思考存储和作业。我的解决方案在 GitHub 上，你可以像往常一样在以下位置查看：[https://github.com/sixeyed/kiamol/blob/master/ch08/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch08/lab/README.md)。
