- en: '7 Bilingual PySpark: Blending Python and SQL code'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 双语PySpark：混合Python和SQL代码
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Drawing a parallel between PySpark’s instruction set and the SQL vocabulary
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将PySpark的指令集与SQL词汇进行类比
- en: Registering data frames as temporary views or tables to query them using Spark
    SQL
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据帧注册为临时视图或表，以便使用Spark SQL进行查询
- en: Using the catalog to create, reference, and delete registered tables for SQL
    querying
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用目录来创建、引用和删除用于SQL查询的已注册表
- en: Translating common data manipulations instructions from Python to SQL, and vice
    versa
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将常见的Python到SQL的数据操作指令翻译，反之亦然
- en: Using SQL-style clauses inside certain PySpark methods
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些PySpark方法中使用SQL风格的子句
- en: My answer to the question “Python versus SQL, which one should I learn?” is
    “both.”
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“Python与SQL，我应该学习哪一个？”这个问题，我的答案是“两者都要学”。
- en: When it comes to manipulating tabular data, SQL is the reigning king. For multiple
    decades now, it has been the workhorse language for relational databases, and
    even today, learning how to tame it is a worthwhile exercise. Spark acknowledges
    the power of SQL head-on. You can seamlessly blend SQL code within your Spark
    or PySpark program, making it easier than ever to migrate those old SQL ETL jobs
    without reinventing the wheel.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到操作表格数据时，SQL是当之无愧的王者。几十年来，它一直是关系型数据库的工作语言，即使今天，学习如何驾驭它仍然是一项值得的练习。Spark直面SQL的力量。你可以在Spark或PySpark程序中无缝地混合SQL代码，这使得迁移那些旧的SQL
    ETL作业变得前所未有的容易。
- en: This chapter is dedicated to using SQL with, and on top of, PySpark. I cover
    how we can move from one language to the other. I also cover how we can use a
    SQL-like syntax within data frame methods to speed up your code, and some of the
    trade-offs you may face. Finally, we blend Python and SQL code to get the best
    of both worlds.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章致力于使用SQL与PySpark结合，以及在其之上使用。我介绍了如何从一个语言转换到另一个语言。我还介绍了如何在数据帧方法中使用类似SQL的语法来加快你的代码，以及你可能面临的某些权衡。最后，我们将Python和SQL代码结合起来，以获得两者的最佳效果。
- en: If you already have notable exposure to SQL, this chapter will be a breeze for
    you. Feel free to skim over the SQL-specific section (7.4), but don’t skip the
    sections on Python and SQL interoperability (7.5 and after), as I cover some PySpark
    idiosyncrasies. For those new to SQL, this will be—I hope—an eye-opening moment,
    and you’ll add another tool under your belt. If you’d like a deeper dive into
    SQL, *SQL in Motion*, by Ben Brumm (Manning, 2017), is a good video source. If
    you prefer a book, a very exhaustive reference is Joe Celko’s *SQL for Smarties*
    (Morgan Kauffman, 2014).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经对SQL有显著的接触，那么这一章对你来说将会轻松自如。你可以自由地浏览SQL特定部分（7.4），但不要跳过Python和SQL互操作性部分（7.5及之后），因为我将涵盖一些PySpark的特殊性。对于那些刚开始接触SQL的人来说，这可能会是一个令人耳目一新的时刻，你将掌握另一个工具。如果你想深入了解SQL，Ben
    Brumm的《SQL in Motion》（Manning，2017）是一个很好的视频资源。如果你更喜欢书籍，Joe Celko的《SQL for Smarties》（Morgan
    Kauffman，2014）是一本非常详尽的参考书。
- en: 'Here are the imports I use in this chapter’s examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我在本章示例中使用的导入：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ We will deal with some AnalysisException, so I import it right at the start.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将处理一些AnalysisException异常，因此我在一开始就导入了它。
- en: Spark SQL vs. ANSI SQL vs. HiveQL
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: Spark SQL与ANSI SQL与HiveQL的比较
- en: Spark supports both ANSI SQL (experimentally, as of right now; see [http://mng.bz/oapr](http://mng.bz/oapr))
    and the vast majority of HiveQL^a as a SQL dialect. Spark SQL also has some Spark-specific
    functions baked in to ensure common functionality across languages.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Spark支持ANSI SQL（目前实验性地支持；参见[http://mng.bz/oapr](http://mng.bz/oapr)）以及绝大多数HiveQL^a作为SQL方言。Spark
    SQL还内置了一些Spark特定的函数，以确保跨语言的功能一致性。
- en: In a nutshell, Hive is a SQL-like interface that can be used over a variety
    of data storage options. It became very popular because it provided the ability
    to query files in HDFS (Hadoop Distributed File System) as if they were a table.
    Spark can integrate with Hive when your environment has it installed. Spark SQL
    also provides additional syntax to work with larger data sets, a topic covered
    in this chapter.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，Hive是一个可以在多种数据存储选项上使用的类似SQL的接口。它之所以非常受欢迎，是因为它提供了查询HDFS（Hadoop分布式文件系统）中文件的能力，就像它们是一个表一样。当你的环境中安装了Hive时，Spark可以与之集成。Spark
    SQL还提供了额外的语法来处理更大的数据集，这是本章讨论的主题。
- en: Because of the amount of material and its longevity, and also because its syntax
    is similar to basic and intermediate queries, I usually recommend learning ANSI
    SQL first and then learning HiveQL as you go along. This way, your knowledge will
    transfer to other SQL-based products. Since Hive is not a component of Spark,
    I won’t cover Hive-specific functionality in this book and will instead focus
    on SQL with Spark.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于材料数量庞大且历史悠久，以及它的语法与基本和中级查询相似，我通常建议首先学习ANSI SQL，然后在学习过程中学习HiveQL。这样，你的知识就可以转移到其他基于SQL的产品上。由于Hive不是Spark的组件，因此我不会在本书中涵盖Hive特定的功能，而是将重点放在与Spark一起使用的SQL上。
- en: '^a You can see the functionality supported (and unsupported) on the Spark website:
    [http://mng.bz/nYMg](http://mng.bz/nYMg).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: ^a 你可以在Spark网站上查看支持（和不支持）的功能：[http://mng.bz/nYMg](http://mng.bz/nYMg)。
- en: '7.1 Banking on what we know: pyspark.sql vs. plain SQL'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 借助我们所知：pyspark.sql与普通SQL
- en: In this section, we draw parallels between Spark’s function and method names
    and SQL keywords. Since both share a base vocabulary, it becomes easy to read
    Spark and SQL code and understand their behavior. More specifically, we break
    down a simple set of instructions in both languages to recognize the similarities
    and differences.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将Spark的功能和方法名称与SQL关键字进行类比。由于两者共享一个基础词汇，因此阅读Spark和SQL代码并理解它们的行为变得容易。更具体地说，我们将两种语言中的简单指令分解开来，以识别它们的相似之处和不同之处。
- en: 'PySpark’s SQL heritage is more than skin deep: the name of the module—`pyspark.sql`—is
    a dead giveaway. PySpark developers recognized the heritage of the SQL programming
    language for data manipulation and used the same keywords to name their method.
    Let’s look at a quick example in both SQL and plain PySpark and look at similarities
    between the keywords used. In listing 7.1, I load a CSV with information about
    the periodic table of elements, and I query the data set to find the number of
    entries with a `liquid` state per period. The code is presented both in PySpark
    and SQL form, and, without much context, we can see similarities. I draw the parallels
    between the two versions in figure 7.1.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的SQL遗产不仅仅在表面：模块的名称——`pyspark.sql`——就是一个明显的提示。PySpark开发者认识到SQL编程语言在数据处理方面的遗产，并使用相同的关键字来命名他们的方法。让我们快速看一下SQL和纯PySpark中的示例，并观察所使用关键字之间的相似性。在列表7.1中，我加载了一个包含元素周期表信息的CSV文件，并查询数据集以找到每个周期中具有`液态`状态的条目数量。代码以PySpark和SQL的形式呈现，并且在不多的上下文中，我们可以看到相似之处。我在图7.1中对比了这两个版本。
- en: '![](../Images/07-01.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-01.png)'
- en: Figure 7.1 PySpark and SQL share the same keywords, but the order of operations
    differs. PySpark looks like an ordered list of operations (take this table, do
    these transformations, then finally show the result), while SQL has a more descriptive
    approach (show me the results from that table after performing these transformations).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 PySpark和SQL共享相同的关键字，但操作顺序不同。PySpark看起来像一系列有序的操作（取这个表，执行这些转换，然后最终显示结果），而SQL则采用更描述性的方法（在执行这些转换后，显示该表的结果）。
- en: 'Compared to the SQL language, PySpark’s data manipulation API differs in two
    main ways:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 与SQL语言相比，PySpark的数据操作API在两个方面有所不同：
- en: PySpark will always start with the name of the data frame you are working with.
    SQL refers to the table (or *target*) using a `from` keyword.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark将以你正在处理的数据帧的名称开头。SQL使用`from`关键字来引用表（或*目标*）。
- en: 'PySpark chains the transformations and actions as methods on the data frame,
    whereas SQL splits them into two groups: the *operation* group and the *condition*
    group. The first one is before the `from` clause and operates on columns. The
    second is after the `from` clause and groups, filters, and orders the structure
    of the result table.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark将转换和操作作为数据帧上的方法链，而SQL将它们分为两组：*操作*组和*条件*组。前者在`from`子句之前，操作列。后者在`from`子句之后，对结果表的结构进行分组、过滤和排序。
- en: Tip SQL is not case-sensitive, so you can either use lowercase or uppercase.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 提示 SQL对大小写不敏感，因此你可以使用小写或大写。
- en: Listing 7.1 Reading and counting the liquid elements by period
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 通过周期读取和计算液态元素
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Both would return the same results: one element in period four (Bromine) and
    one in period six (Mercury).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 两者都会返回相同的结果：第四周期（溴）有一个元素，第六周期（汞）有一个元素。
- en: Whether you prefer the order of operations from PySpark or SQL will depend on
    how you build queries mentally and how familiar you are with the respective languages.
    Fortunately, PySpark makes it easy to move from one to the other, and even to
    work with both at once.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你更喜欢 PySpark 或 SQL 的操作顺序将取决于你如何心理构建查询以及你对相应语言的熟悉程度。幸运的是，PySpark 使你很容易从一种语言切换到另一种语言，甚至可以同时使用这两种语言。
- en: 7.2 Preparing a data frame for SQL
  id: totrans-32
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 准备数据框以供 SQL 查询
- en: Since we can think of PySpark data frames like tables on steroids, it’s not
    far-fetched to think about querying them using a language designed to query tables.
    Spark provides a full SQL API that is documented in the same fashion as the PySpark
    API ([http://mng.bz/vozJ](http://mng.bz/vozJ)). The Spark SQL API also defines
    the functions used in the `pyspark.sql` API, such as `substr()` or `size()`.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们可以将 PySpark 数据框视为类固醇上的表，因此考虑使用用于查询表的语言来查询它们并不牵强。Spark 提供了一个完整的 SQL API，其文档方式与
    PySpark API 相同（[http://mng.bz/vozJ](http://mng.bz/vozJ)）。Spark SQL API 还定义了在 `pyspark.sql`
    API 中使用的函数，例如 `substr()` 或 `size()`。
- en: Note Spark’s SQL API only covers the data manipulation subset of Spark. For
    instance, you won’t be able to do machine learning using SQL (see chapter 13).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Spark 的 SQL API 仅涵盖 Spark 的数据操作子集。例如，你将无法使用 SQL 进行机器学习（见第 13 章）。
- en: 7.2.1 Promoting a data frame to a Spark table
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 将数据框提升为 Spark 表
- en: In this section, I cover the simple steps to get a Spark data frame using SQL.
    PySpark maintains boundaries between its own name spacing and Spark SQL’s name
    spacing; we, therefore, have to explicitly promote them.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍使用 SQL 获取 Spark 数据框的简单步骤。PySpark 在其自己的命名空间和 Spark SQL 的命名空间之间保持边界；因此，我们必须明确提升它们。
- en: First, let’s see what happens when we don’t do anything. The code in listing
    7.2 shows an example.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看如果我们什么都不做会发生什么。列表 7.2 中的代码展示了示例。
- en: Listing 7.2 Trying (and failing) at querying a data frame SQL style
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 尝试（并失败）以 SQL 风格查询数据框
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Here, PySpark doesn’t make the link between the python variable `elements`,
    which points to the data frame, and a potential table `elements` that can be queried
    by Spark SQL. To allow a data frame to be queried via SQL, we need to *register*
    it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，PySpark 并没有在 Python 变量 `elements`（它指向数据框）和可以由 Spark SQL 查询的潜在表 `elements`
    之间建立链接。为了允许数据框通过 SQL 进行查询，我们需要对其进行 *注册*。
- en: When we assign a data frame to a variable, Python points to the data frame.
    Spark SQL does not have visibility over the variables Python assigns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将数据框分配给变量时，Python 指向数据框。Spark SQL 无法看到 Python 分配的变量。
- en: When you want to create a table/view to query with Spark SQL, use the `createOrReplaceTempView()`
    method. This method takes a single string parameter, which is the name of the
    table you want to use. This transformation will look at the data frame referenced
    by the Python variable on which the method was applied and will create a Spark
    SQL reference to the same data frame.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当你想创建一个用于 Spark SQL 查询的表/视图时，请使用 `createOrReplaceTempView()` 方法。此方法接受一个字符串参数，即你想要使用的表名称。这种转换将查看由
    Python 变量引用的数据框，并将创建一个指向相同数据框的 Spark SQL 引用。
- en: Note Although you can name the table the same name as the variable you are using,
    you are not forced to do so.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：尽管你可以将表命名为与正在使用的变量相同的名称，但你并不被强迫这样做。
- en: Once we have registered our `elements` table that points to the same data frame
    as our Python variable of the same name, we can query our table without any problems.
    Let’s rerun the same code block as in listing 7.2 and see if it succeeds.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们注册了指向与我们的同名 Python 变量相同的数据框的 `elements` 表，我们就可以没有任何问题地查询我们的表。让我们重新运行列表 7.2
    中的相同代码块，看看它是否成功。
- en: 'Note In this chapter, I use the term *table* and *view* pretty loosely. In
    SQL, they are distinct concepts: the table is materialized in memory and on disk,
    and the view is computed on the fly. Spark’s temp views are conceptually closer
    to a view than a table. Spark SQL also has tables, but we will not be using them,
    and will instead read and materialize our data into a data frame.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在本章中，我相当宽松地使用术语 *表* 和 *视图*。在 SQL 中，它们是不同的概念：表在内存和磁盘上物化，而视图是即时计算的。Spark 的临时视图在概念上更接近视图而不是表。Spark
    SQL 也有表，但我们将不会使用它们，而是将我们的数据读取和物化到数据框中。
- en: Listing 7.3 Trying (and succeeding at) querying a data frame SQL style
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 尝试（并成功）以 SQL 风格查询数据框
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ We register our table using the createOrReplaceTempView() method on the element
    data frame.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用元素数据框上的 createOrReplaceTempView() 方法注册我们的表。
- en: ❷ The same query works once Spark is able to de-reference the SQL view name.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当Spark能够解析SQL视图名称时，相同的查询才会生效。
- en: Now we have a view registered. In the case of a low number of views to manage,
    it’s pretty easy to keep the name in memory. What about if you have dozens of
    views or you need to delete some? Enter the catalog, Spark’s way of managing its
    SQL namespace.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经注册了一个视图。在管理少量视图的情况下，将其名称保持在内存中相当容易。如果你有数十个视图或需要删除一些，怎么办？进入目录，这是Spark管理其SQL命名空间的方式。
- en: 'Advanced-ish topic: Spark SQL views and persistence'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 高级话题：Spark SQL视图和持久化
- en: 'PySpark has four methods to create temporary views, and they look quite similar
    at first glance:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark有四种创建临时视图的方法，乍一看它们看起来相当相似：
- en: '`createGlobalTempView()`'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createGlobalTempView()`'
- en: '`createOrReplaceGlobalTempView()`'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createOrReplaceGlobalTempView()`'
- en: '`createOrReplaceTempView()`'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createOrReplaceTempView()`'
- en: '`createTempView()`'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`createTempView()`'
- en: 'We can see that there is a two-by-two matrix of possibilities:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，存在一个两行两列的可能性矩阵：
- en: Do I want to replace an existing view (`OrReplace`)?
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否想要替换现有的视图（`OrReplace`）？
- en: Do I want to create a global view (`Global`)?
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我是否想要创建一个全局视图（`Global`）？
- en: 'The first one is relatively easy to answer: if you use `createTempView` with
    a name already being used for another table, the method will fail. On the other
    hand, if you use `createOrReplaceTempView()`, Spark will replace the old table
    with a new one. In SQL, it is equivalent to using `CREATE` `VIEW` versus `CREATE`
    `OR` `REPLACE` `VIEW`. I personally always use the latter, as it mimics Python’s
    way of doing things: when reassigning a variable, you comply.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个问题相对容易回答：如果你使用`createTempView`与另一个表已经使用的名称，该方法将失败。另一方面，如果你使用`createOrReplaceTempView()`，Spark将用新表替换旧表。在SQL中，这相当于使用`CREATE`
    `VIEW`与`CREATE` `OR` `REPLACE` `VIEW`。我个人总是使用后者，因为它模仿了Python处理事情的方式：在重新赋值变量时，你会遵守。
- en: What about `Global`? The difference between a local view and a global view has
    to do with how long it will last in memory. A local table is tied to your `SparkSession`,
    while a global table is tied to the Spark application. The differences at this
    time are not significant, as we are not using multiple `SparkSession`s that need
    to share data. In the context of data analysis with Spark, you won’t deal with
    multiple `SparkSession`s at once, so I usually don’t use the `Global` methods.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 那么`Global`呢？本地视图和全局视图之间的区别在于它们在内存中持续的时间。本地表与你的`SparkSession`相关联，而全局表与Spark应用程序相关联。目前这些差异并不显著，因为我们没有使用需要共享数据的多个`SparkSession`s。在Spark进行数据分析的上下文中，你不会同时处理多个`SparkSession`s，所以我通常不使用`Global`方法。
- en: 7.2.2 Using the Spark catalog
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 使用Spark目录
- en: The Spark catalog is an object that allows working with Spark SQL tables and
    views. A lot of its methods have to do with managing the metadata of those tables,
    such as their names and the level of caching (which I’ll cover in detail in chapter
    11). We will look at the most basic set of functionality in this section, setting
    the stage for more advanced material, such as table caching (chapter 11) and UDF
    (chapter 8).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Spark目录是一个允许与Spark SQL表和视图一起工作的对象。它的大多数方法都与管理这些表的元数据有关，例如它们的名称和缓存级别（我将在第11章中详细说明）。在本节中，我们将查看最基本的功能集，为更高级的内容打下基础，例如表缓存（第11章）和UDF（第8章）。
- en: We can use the catalog to list the tables/views we have registered and drop
    them if we are done. The code in the next listing provides the simple methods
    to do those tasks. Since they are mostly mimicking PySpark’s data frame functionality,
    I think that an example shows it best.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用目录来列出我们已注册的表/视图，并在完成时删除它们。下一列表中的代码提供了执行这些任务的简单方法。由于它们主要模仿PySpark的数据帧功能，我认为示例是最好的说明。
- en: Listing 7.4 Using the catalog to display our registered view and then drop it
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.4 使用目录显示我们的注册视图然后删除它
- en: '[PRE4]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The catalog is reached through the catalog property of our SparkSession.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 目录是通过我们的SparkSession的目录属性访问的。
- en: ❷ The listTables method gives us a list of Table objects that contain the information
    we want.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `listTables`方法给我们一个包含我们所需信息的Table对象的列表。
- en: ❸ To delete a view, we use the method dropTempView() and pass the name of the
    view as a parameter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 要删除视图，我们使用`dropTempView()`方法，并将视图名称作为参数传递。
- en: ❹ Our catalog now has no table for us to query.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们现在没有表可以查询。
- en: Now that we understand how we can manage a Spark SQL view within PySpark, we
    can start looking at manipulating data using both languages.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了如何在PySpark中管理Spark SQL视图，我们可以开始探讨使用这两种语言来操作数据。
- en: 7.3 SQL and PySpark
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 SQL和PySpark
- en: The integration between Python (through PySpark) and SQL is well thought-out
    and can improve the speed at which we can write code. This section focuses on
    using only SQL for manipulating data, with Python playing a coordinating role
    with the catalog and instructions. I will review the most common operations from
    a pure SQL and PySpark perspective to illustrate how basic manipulations are written.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Python（通过PySpark）与SQL之间的集成考虑得非常周到，可以提高我们编写代码的速度。本节重点介绍仅使用SQL来操作数据，Python在目录和指令中扮演协调角色。我将从纯SQL和PySpark的角度回顾最常见的操作，以说明基本操作是如何编写的。
- en: For the remainder of the chapter, we will use a public data set provided by
    Backblaze, which provided hard-drive data and statistics. Backblaze is a company
    that provides cloud storage and backup. Since 2013, they have provided data on
    the drives in their data center, and over time have moved to a focus on failures
    and diagnosis. Their (clean) data is in the gigabytes range, which, although not
    that big yet, is certainly Spark-worthy, as it’ll be more than the memory available
    on your home computer. Backblaze has a lot more historical data available on their
    website, should you want a larger data set. A convenience shell script is also
    provided for downloading everything in one fell swoop. For those working locally
    and afraid of blowing your memory, you can use Q3 2019\. The syntax will differ
    marginally between both workflows. A computer with at least 16 GB of RAM should
    be able to handle all files. Backblaze provides documentation mostly in the form
    of SQL statements, which is perfect for what we’re learning.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将使用Backblaze提供的一个公共数据集，该数据集提供了硬盘驱动器和统计数据。Backblaze是一家提供云存储和备份的公司。自2013年以来，他们提供了数据中心中驱动器的数据，并且随着时间的推移，他们转向了关注故障和诊断。他们的（清洁）数据量在千兆字节范围内，虽然目前还不是很大，但绝对适合Spark，因为它将超过您家庭电脑上的内存。如果您需要更大的数据集，Backblaze网站上还有更多历史数据可供使用。还提供了一个方便的shell脚本，可以一次性下载所有内容。对于在本地上工作的用户，如果担心内存不足，可以使用Q3
    2019。两种工作流程的语法略有不同。至少有16GB RAM的计算机应该能够处理所有文件。Backblaze主要提供SQL语句形式的文档，这对于我们正在学习的内容来说非常合适。
- en: To get the files, you can either download them from the website ([http://mng.bz/4jZa](http://mng.bz/4jZa))
    or use the `backblaze_download_data.py` available in the code repository, which
    requires the `wget` package to be installed. The data needs to be in the `./data/`
    `backblaze` directory.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取文件，您可以从网站下载它们（[http://mng.bz/4jZa](http://mng.bz/4jZa)）或者使用代码仓库中可用的`backblaze_download_data.py`，这需要安装`wget`包。数据需要放在`./data/`目录下的`backblaze`文件夹中。
- en: Listing 7.5 Downloading the data from Backblaze
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.5 从Backblaze下载数据
- en: '[PRE5]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Windows users, use "dir data\backblaze".
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Windows用户，使用"dir data\backblaze"。
- en: Make sure you unzip the files into the directory before trying to read them.
    Unlike many other codecs (e.g., Gzip, Bzip2, Snappy, and LZO), PySpark will not
    decompress zip files automatically when reading them, so we need to do it ahead
    of time. The `unzip` command can be used if you are using the command line (you
    might need to install the tool on Linux). On Windows, I usually use Windows Explorer
    and unzip by hand.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试读取文件之前，请确保将文件解压缩到该目录中。与许多其他编解码器（例如，Gzip、Bzip2、Snappy和LZO）不同，PySpark在读取时不会自动解压缩zip文件，因此我们需要提前进行解压缩。如果您使用的是命令行（您可能需要在Linux上安装此工具），可以使用`unzip`命令。在Windows上，我通常使用Windows资源管理器手动解压缩。
- en: The code to ingest and prep the data is pretty straightforward. We read each
    data source separately, and then we make sure that each data frame has the same
    columns as its peers. In our case, the data for the fourth quarter has two more
    columns than the others, so we add the missing columns. When joining the four
    data frames, we use a select method so that their column order is the same. We
    continue by casting all the columns containing a SMART measurement as a long,
    since they are documented as integral values. Finally, we register our data frame
    as a view so that we can use SQL statements on it.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 读取和准备数据的代码相当简单。我们分别读取每个数据源，然后确保每个数据帧与其同伴具有相同的列。在我们的案例中，第四季度的数据比其他数据多两个列，因此我们添加缺失的列。在连接四个数据帧时，我们使用选择方法以确保它们的列顺序相同。然后，我们将包含SMART测量的所有列转换为长整型，因为它们被记录为整数值。最后，我们将我们的数据帧注册为视图，这样我们就可以在它上面使用SQL语句。
- en: Listing 7.6 Reading Backblaze data into a data frame and registering a view
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.6 将Backblaze数据读取到数据帧中并注册视图
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 7.4 Using SQL-like syntax within data frame methods
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 在数据帧方法中使用类似SQL的语法
- en: Our goal in this section is to perform a quick exploratory data analysis on
    a subset of the columns presented. We will reproduce the failure rates that Backblaze
    computes and identify the models with the greatest and least amount of failures
    in 2019.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 本节的目标是对列子集进行快速探索性数据分析。我们将重现Backblaze计算出的故障率，并确定2019年故障最多和最少的型号。
- en: '7.4.1 Get the rows and columns you want: select and where'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 获取你想要的行和列：select和where
- en: '`select` and `where` are used to narrow down the columns (`select`) and the
    rows (`where`) you want to keep in your data frame. In listing 7.7, I use `select`
    and `where` to show a few hard drives’ serial numbers that have failed at some
    point (`failure` `=` `1`). Both `select()` and `where()` were introduced in chapter
    2 and have been used since; I want to draw the focus once more to the differences
    between the SQL and Python syntax.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '`select`和`where`用于缩小你想要保留在数据框中的列（`select`）和行（`where`）。在列表7.7中，我使用`select`和`where`来展示一些在某个时刻失败的硬盘序列号（`failure`
    `=` `1`）。`select()`和`where()`都是在第2章中引入的，并且自那时起一直在使用；我想再次强调SQL和Python语法的差异。'
- en: To use SQL within your PySpark program, use the `sql` method of the `SparkSession`
    object. This method takes a string containing a SQL statement. It’s as simple
    as that!
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要在PySpark程序中使用SQL，请使用`SparkSession`对象的`sql`方法。这个方法接受一个包含SQL语句的字符串。就这么简单！
- en: Listing 7.7 Comparing `select` and `where` in PySpark and SQL
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.7 在PySpark和SQL中比较`select`和`where`
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Since a SQL statement returns a data frame, we still have to show() it to
    see the results.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 由于SQL语句返回一个数据框，我们仍然需要使用`show()`来查看结果。
- en: 'Let’s recap the differences between Python and SQL code using the example in
    listing 7.7\. PySpark makes you think about how you want to chain the operations.
    In our case, we start by filtering the data frame and then select the column of
    interest. SQL presents an alternative construction:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下使用列表7.7中的示例，Python和SQL代码之间的差异。PySpark让你思考如何链式操作。在我们的例子中，我们首先过滤数据框，然后选择感兴趣的列。SQL提供了一个不同的结构：
- en: 'You put the columns you want to select at the beginning of your statement.
    This is called the *SQL operation*: `select` `serial_number`.'
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你将想要选择的列放在语句的开头。这被称为*SQL操作*：`select` `serial_number`。
- en: 'You add one or more tables to query, called the *target*: `from` `backblaze_`
    `stats_2019`.'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你添加一个或多个表到查询中，称为*目标*：`from` `backblaze_` `stats_2019`。
- en: 'You add the *conditions*, such as filtering: `where` `failure` `=` `1`.'
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你可以添加*条件*，例如过滤：`where` `failure` `=` `1`。
- en: Every operation we will look at in this chapter will be classified as an operation,
    a target, or a condition so you can know where it fits in the statement.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们将讨论的每个操作都将被分类为操作、目标或条件，这样你可以知道它在语句中的位置。
- en: As a final note, SQL has no notion of creating columns with `withColumns()`
    or renaming them with `withColumnRenamed`. Everything has to go through `SELECT`.
    In the next section, I cover grouping records together; I also take the opportunity
    to cover aliasing there.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的注意事项，SQL没有使用`withColumns()`创建列或使用`withColumnRenamed`重命名列的概念。所有操作都必须通过`SELECT`来完成。在下一节中，我将介绍如何将记录分组在一起；同时，我也将机会用来介绍别名。
- en: Tip If you have a table you want to extract as a data frame, you assign the
    result of a `SELECT` statement to a variable. As an example, you could do `failures`
    `=` `spark.sql("select` `serial_number` `.` `.` `.")`, and the resulting data
    frame would be assigned to the variable `failures`.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你有一个想要提取为数据框的表，你可以将`SELECT`语句的结果赋值给一个变量。例如，你可以这样做：`failures` `=` `spark.sql("select`
    `serial_number` `.` `.` `.")`，然后结果数据框将被分配给变量`failures`。
- en: '7.4.2 Grouping similar records together: group by and order by'
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 将相似记录分组在一起：group by和order by
- en: The PySpark syntax for `groupby()` and `orderby()` was covered in detail in
    chapter 5\. In this section, I introduce the SQL syntax—through a comparison with
    the Python syntax—by looking at the capacity, in gigabytes, of the hard drives
    included in the data, by model. For this, we use a little arithmetic and the `pow()`
    function (available in `pyspark.sql.functions`) that elevates its first argument
    to the power of the second. We can see similarities between the SQL and PySpark
    vocabulary, but once again, the order of the transformations is different.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中详细介绍了PySpark的`groupby()`和`orderby()`语法。在本节中，我通过将Python语法与SQL语法进行比较，介绍了SQL语法——通过查看数据中包含的硬盘的容量（以千兆字节为单位），按型号分组。为此，我们使用一点算术和`pow()`函数（在`pyspark.sql.functions`中可用），它将第一个参数提升为第二个参数的幂。我们可以看到SQL和PySpark词汇之间的相似性，但再次强调，转换的顺序是不同的。
- en: Listing 7.8 Grouping and ordering in PySpark and SQL
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 PySpark 和 SQL 中的分组和排序
- en: '[PRE8]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In PySpark, once again, we look at the logical order of operations. We `groupby`
    the `capacity_GB` column, which is a computed column. Just like in PySpark, arithmetic
    operations can be performed using the usual syntax in SQL. Furthermore, the `pow()`
    function is also implemented in Spark SQL. If you need to see which functions
    can be used out of the box, the Spark SQL API doc contains the necessary information
    ([http://mng.bz/vozJ](http://mng.bz/vozJ)).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，我们再次查看操作的逻辑顺序。我们按 `capacity_GB` 列进行 `groupby`，这是一个计算列。就像在 PySpark
    中一样，可以使用常规语法在 SQL 中执行算术运算。此外，Spark SQL 也实现了 `pow()` 函数。如果您需要查看可以使用的函数，Spark SQL
    API 文档包含必要的信息（[http://mng.bz/vozJ](http://mng.bz/vozJ)）。
- en: To alias a column, we just add the name after the column descriptor, preceded
    by a space. In our case, `min(capacity_bytes` `/` `pow(1024,` `3))` is aliased
    to `min_GB`—a much friendlier name! Some will prefer to use the keyword `as` so
    that the line reads `min(capacity_bytes` `/` `pow(1024,` `3))` as min_GB; in Spark
    SQL, it’s a matter of personal preference.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要给列起别名，我们只需在列描述符后添加名称，前面加一个空格。在我们的例子中，`min(capacity_bytes` `/` `pow(1024,` `3))`
    被别名为 `min_GB`——一个更友好的名称！有些人可能更喜欢使用关键字 `as`，这样行就变成了 `min(capacity_bytes` `/` `pow(1024,`
    `3))` as min_GB；在 Spark SQL 中，这完全是个人喜好问题。
- en: 'Grouping and ordering are conditions in SQL, so they are at the end of the
    statement. They both follow the same convention as PySpark:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 分组和排序是 SQL 中的条件，因此它们位于语句的末尾。它们都遵循与 PySpark 相同的约定：
- en: We provide the columns to `group` `by`, separated by a comma.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过逗号分隔列来提供 `group by`。
- en: For `order` `by`, we provide the column names to the clause with an optional
    `DESC` argument if we want to order by descending order (the default is `ASC`
    for ascending).
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `order by`，我们向子句提供列名，如果想要按降序排序（默认为升序 `ASC`），则可选地提供 `DESC` 参数。
- en: One thing worth noting is that we group by `1` and order by `3` `DESC`. This
    is a shorthand way of referring to the columns in the SQL operation by position
    rather than name. In this case, it saves us from writing `group` `by` `capacity_bytes`
    `/` `pow(1024,` `3)` or `order` `by` `max(capacity_bytes` `/` `pow(1024,3))` `DESC`
    in the conditions block. We can use numerical aliases in `group` `by` and `order`
    `by` clauses. While they are nifty, abusing them makes your code more fragile
    and hard to maintain should you change the query.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我们按 `1` 进行分组，按 `3` `DESC` 排序。这是一种通过位置而不是名称来引用 SQL 操作中列的简写方式。在这种情况下，它使我们免于在条件块中编写
    `group by` `capacity_bytes` `/` `pow(1024,3)` 或 `order by` `max(capacity_bytes`
    `/` `pow(1024,3))` `DESC`。我们可以在 `group by` 和 `order by` 子句中使用数字别名。虽然它们很方便，但过度使用它们会使你的代码更加脆弱，且在更改查询时难以维护。
- en: Looking at the results from our query, there are some drives that report more
    than one capacity. Furthermore, we have some drives that report negative capacity,
    which is really odd. Let’s focus on seeing how prevalent this is.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 观察查询结果，有一些驱动器报告了多个容量。此外，我们还有一些报告负容量的驱动器，这真的很奇怪。让我们关注一下这种情况的普遍性。
- en: 7.4.3 Filtering after grouping using having
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 使用 `having` 在分组后进行过滤
- en: 'Because of the order of the evaluation of operations in SQL, `where` is always
    applied before `group` `by`. What happens if we want to filter the values of columns
    created after the `group` `by` operation? We use a new keyword: `having`!'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 SQL 中操作评估的顺序，`where` 总是在 `group by` 之前应用。如果我们想过滤在 `group by` 操作之后创建的列的值，会发生什么？我们使用一个新的关键字：`having`！
- en: As an example, let’s assume that, for each model, the maximum reported capacity
    is the correct one. The code in the next listing shows how we can accomplish this
    in both languages.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设对于每个模型，报告的最大容量是正确的。下一条列表中的代码展示了我们如何在两种语言中实现这一点。
- en: Listing 7.9 Using `having` in SQL and relying on `where` in PySpark
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 在 SQL 中使用 `having` 并在 PySpark 中依赖 `where`
- en: '[PRE9]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`having` is a syntax unique to SQL: it can be thought of as a `where` clause
    that can only be applied to aggregate fields, such as `count(*)` or `min(date)`.
    Since it is equivalent in functionality to `where`, `having` is in the condition
    block after the `group` `by` clause. In PySpark, we do not have `having` as a
    method. Since each method returns a new data frame, we do not have to have a different
    keyword, and can simply use `where` with the column we created instead.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '`having` 是 SQL 独有的语法：它可以被视为只能应用于聚合字段（如 `count(*)` 或 `min(date)`）的 `where` 子句。由于它在功能上与
    `where` 相等，`having` 位于 `group by` 子句之后的条件块中。在 PySpark 中，我们没有 `having` 作为方法。由于每个方法都返回一个新的数据框，我们不需要不同的关键字，可以直接使用我们创建的列的
    `where`。'
- en: Note We will ignore (for now) those capacity-reporting inconsistencies. They’ll
    come back as exercises.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们暂时忽略那些容量报告的不一致性。它们将在练习中再次出现。
- en: 'So far, we’ve covered the most important SQL operation: selecting columns with
    `select`. Next, let’s materialize our work, SQL-style.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经涵盖了最重要的 SQL 操作：使用 `select` 选择列。接下来，让我们以 SQL 的方式实现我们的工作。
- en: 7.4.4 Creating new tables/views using the CREATE keyword
  id: totrans-117
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 使用 CREATE 关键字创建新表/视图
- en: Now that we have queried the data and are getting the hang of it in SQL, we
    might want to check our work and save some data so that we do not have to process
    everything from scratch the next time. For this, we can create either a table
    or a view, which we can then query directly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经查询了数据，并在 SQL 中掌握了它的使用方法，我们可能想要检查我们的工作并保存一些数据，这样我们就不必下次从头开始处理所有内容。为此，我们可以创建一个表或一个视图，然后我们可以直接查询它。
- en: 'Creating a table or a view is very easy in SQL: prefix our query by `CREATE`
    `TABLE/VIEW`. Here, creating a table or a view will have a different impact. If
    you have a Hive metastore connected, creating a table will materialize the data,
    whereas a view will only keep the query. To use a baking analogy, `CREATE` `TABLE`
    will store a cake, whereas `CREATE` `VIEW` will refer to the ingredients (the
    original data) and the recipe (the query).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 SQL 中创建表或视图非常简单：在我们的查询前加上 `CREATE` `TABLE/VIEW`。在这里，创建一个表或视图将产生不同的影响。如果您连接了
    Hive 元数据存储，创建表将物化数据，而视图将只保留查询。用烘焙的比喻来说，`CREATE TABLE` 将存储蛋糕，而 `CREATE VIEW` 将引用原料（原始数据）和食谱（查询）。
- en: 'To demonstrate this, I will reproduce the `drive_days` and `failures` that
    compute the number of days of operation that a model has and the number of drive
    failures it has had, respectively. The code in listing 7.10 shows how it is done:
    prefix your select query with a `CREATE` `[TABLE/VIEW]`.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示这一点，我将重现 `drive_days` 和 `failures`，它们分别计算模型运行的天数和已发生的驱动器故障数。列表 7.10 中的代码显示了如何实现：在您的
    select 查询前加上 `CREATE` `[TABLE/VIEW]`。
- en: In PySpark, we do not have to rely on extra syntax. A newly created data frame
    has to be assigned to a variable and then we are good to go.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，我们不需要依赖额外的语法。一个新创建的数据框必须分配给一个变量，然后我们就可以继续了。
- en: Listing 7.10 Creating a view in Spark SQL and in PySpark
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 在 Spark SQL 和 PySpark 中创建视图
- en: '[PRE10]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Creating tables from data in SQL
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 从 SQL 中的数据创建表
- en: 'You can also create a table from data on a hard drive or HDFS. For this, you
    can use a modified SQL query. Since we are reading a CSV file, we prefix our path
    with `csv.`:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以从硬盘或 HDFS 上的数据创建一个表。为此，您可以使用修改后的 SQL 查询。由于我们正在读取 CSV 文件，我们在路径前加上 `csv.`：
- en: '[PRE11]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: I much prefer relying on PySpark syntax for reading and setting the schema from
    my data source and then using SQL, but the option is there for the taking.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 我更倾向于依赖 PySpark 语法来从我的数据源读取和设置模式，然后使用 SQL，但这个选项是可用的。
- en: 7.4.5 Adding data to our table using UNION and JOIN
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.5 使用 UNION 和 JOIN 向我们的表中添加数据
- en: So far, we’ve seen how to query a single table at a time. In practice, you’ll
    often get multiple tables related to one another. We already witnessed this problem
    by having one historical table per quarter, which needed to be stacked together
    (or unioned), and with our `drive_days` and `failures` tables, which each paint
    a single dimension of the story until they are merged (or joined).
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了如何一次查询一个表。在实践中，你经常会得到多个相互关联的表。我们已经通过每个季度有一个历史表来见证这个问题，这些表需要堆叠在一起（或联合），以及我们的
    `drive_days` 和 `failures` 表，每个表都描绘了故事的一个维度，直到它们合并（或连接）。
- en: 'Joins and unions are the only clauses we’ll see that modify the target piece
    in our SQL statement. In SQL, a query is operating on a single target at a time.
    We already saw at the beginning of the chapter how to use PySpark to union tables
    together. In SQL, we follow the same blueprint: `SELECT` `columns` `FROM` `table1`
    `UNION` `ALL` `SELECT` `columns` `FROM` `table2`.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 连接和联合是我们将看到的唯一修改SQL语句目标部分的子句。在SQL中，查询一次只对一个目标进行操作。我们在本章开头已经看到了如何使用PySpark联合表。在SQL中，我们遵循相同的蓝图：`SELECT
    columns FROM table1 UNION ALL SELECT columns FROM table2`。
- en: PySpark’s union() vs. SQL UNION
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的union() 与 SQL UNION
- en: In SQL, `UNION` removes the duplicate records. PySpark’s `union()` doesn’t,
    which is why it’s equivalent to a SQL `UNION` `ALL`. If you want to drop the duplicates,
    which is an expensive operation when working in a distributed context, use the
    `distinct()` function after your `union()`. This is one of the rare cases where
    PySpark’s vocabulary doesn’t follow SQL’s, but it’s for a good reason. Most of
    the time, you’ll want the `UNION` `ALL` behavior.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，`UNION`会移除重复的记录。PySpark的`union()`不会，这就是为什么它与SQL的`UNION ALL`等价。如果你想要删除重复项，这在分布式环境中是一个昂贵的操作，请在`union()`之后使用`distinct()`函数。这是PySpark词汇不遵循SQL的罕见情况之一，但这是出于一个很好的原因。大多数时候，你都会想要`UNION
    ALL`的行为。
- en: It is always a good idea to make sure that your data frames have the same columns,
    with the same types, in the same order, before attempting a union. In the PySpark
    solution, we used the fact that we could extract the columns in a list to `select`
    the data frames in the same fashion. Spark SQL does not have a simple way to do
    the same, so you would have to type all the columns. This is okay when you just
    have a few, but we’re talking hundreds here.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试进行合并之前，确保你的数据框具有相同的列、相同的类型以及相同的顺序总是一个好主意。在PySpark解决方案中，我们利用了可以提取列列表的事实，以相同的方式`select`数据框。Spark
    SQL没有简单的方法来做同样的事情，所以你可能需要输入所有列。当你只有少数几个列时，这没问题，但我们这里讨论的是数百个列。
- en: One easy way to circumvent this is to use the fact that a Spark SQL statement
    is a string. We can take our list of columns, transform it into a SQL-esque string,
    and be done with it. This is exactly what I did in listing 7.11\. It’s not a pure
    Spark SQL solution, but it’s much friendlier than making you type all the columns
    one by one.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一种绕过这个问题的简单方法就是利用Spark SQL语句是一个字符串的事实。我们可以将我们的列列表转换成一个类似SQL的字符串，然后完成它。这正是我在列表7.11中所做的。这不是一个纯Spark
    SQL解决方案，但它比让你逐个输入所有列要友好得多。
- en: Warning Do not allow for plain string insertion if you are processing user input!
    This is the best way to have a SQL injection, where a user can craft a string
    that will wreak havoc on your data. For more information about SQL injections
    and why they can be so dangerous, review the Open Web Application Security Project
    article on the subject ([http://mng.bz/XWdG](http://mng.bz/XWdG)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：在处理用户输入时，不要允许直接插入纯字符串！这是导致SQL注入的最佳方式，用户可以构建一个字符串，对你的数据造成破坏。有关SQL注入及其为何如此危险的信息，请参阅Open
    Web Application Security Project关于此主题的文章([http://mng.bz/XWdG](http://mng.bz/XWdG))。
- en: Listing 7.11 Unioning tables together in Spark SQL and in PySpark
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.11 在Spark SQL和PySpark中联合表
- en: '[PRE12]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ We use the join() method on a separator string to create a string containing
    all the elements in the list, separated by ,.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们使用`join()`方法在一个分隔符字符串上创建一个包含列表中所有元素的字符串，元素之间用逗号分隔。
- en: ❷ We promote our quarterly data frames to Spark SQL views so we can use them
    in our query.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将我们的季度数据框提升为Spark SQL视图，以便我们可以在查询中使用它们。
- en: ❸ This is taken from listing 7.6.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这是从列表7.6中摘取的。
- en: Joins are equally as simple in SQL. We add a `[DIRECTION]` `JOIN` `table` `[ON]`
    `[LEFT` `COLUMN]` `[COMPARISON` `OPERATOR]` `[RIGHT` `COLUMN]` in the target portion
    of our statement. The direction is the same parameter of our `how` in PySpark.
    The `on` clause is a series of comparisons between columns. In the example in
    listing 7.12, we join the records where the value in the `model` column is equal
    (`=`) on both `drive_days` and `failures` tables. More than one condition? Use
    parentheses and logical operators (`AND`, `OR`), just like when working in Python
    (for more information, see chapter 5).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，连接同样简单。我们在语句的目标部分添加一个`[DIRECTION] JOIN table [ON] [LEFT COLUMN] [COMPARISON
    OPERATOR] [RIGHT COLUMN]`。方向与PySpark中的`how`参数相同。`on`子句是一系列列之间的比较。在列表7.12的例子中，我们连接了`model`列的值在`drive_days`和`failures`表中都相等（`=`）的记录。有多个条件？使用括号和逻辑运算符（`AND`，`OR`），就像在Python中工作一样（更多信息，请参阅第5章）。
- en: Listing 7.12 Joining tables in Spark SQL and in PySpark
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 在 Spark SQL 和 PySpark 中连接表
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 7.4.6 Organizing your SQL code better through subqueries and common table expressions
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.6 通过子查询和公用表表达式更好地组织你的 SQL 代码
- en: The last pieces of SQL syntax we will look at on their own are the subquery
    and the common table expression. A lot of SQL references do not talk about them
    until very late, which is a shame because they are (a) easy to understand and
    (b) very helpful in keeping your code clean. In a nutshell, they allow you to
    create tables local to your query. In Python, this is similar to using the `with`
    statement or using a function block to limit the scope of a query. I will show
    the function approach, as it is much more common.[¹](#pgfId-1031540)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将单独查看的最后一些 SQL 语法是子查询和公用表表达式。许多 SQL 参考文献直到很晚才讨论它们，这是很遗憾的，因为它们（a）易于理解，并且（b）在保持代码整洁方面非常有帮助。简而言之，它们允许你在查询本地创建表。在
    Python 中，这类似于使用 `with` 语句或使用函数块来限制查询的作用域。我将展示函数方法，因为它更为常见。[¹](#pgfId-1031540)
- en: For our example, we will take our `drive_days` and `failures` table definitions
    and bundle them into a single query that will measure the models with the highest
    rate of failure in 2019\. The code in listing 7.13 shows how we can do this using
    a subquery. A subquery simply replaces a table name with a standalone SQL query.
    In the example, we can see that the name of the table has been replaced by the
    `SELECT` query that formed the table. We can alias the table referred to in the
    subquery by adding the name at the end of the statement, after the closing parenthesis.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例，我们将使用 `drive_days` 和 `failures` 表定义，并将它们捆绑成一个查询，该查询将衡量 2019 年故障率最高的模型。列表
    7.13 中的代码显示了我们可以如何使用子查询来完成这项工作。子查询简单地用一个独立的 SQL 查询替换了一个表名。在示例中，我们可以看到表名已被形成该表的
    `SELECT` 查询所取代。我们可以通过在语句的末尾添加名称来给子查询中引用的表起别名，在括号关闭之后。
- en: Listing 7.13 Finding drive models with highest failure rates using subqueries
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.13 使用子查询查找故障率最高的驾驶模型
- en: '[PRE14]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Subqueries are cool but can be hard to read and debug, since you are adding
    complexity into the main query. This is where common table expressions, or CTEs,
    are especially useful. A CTE is a table definition, just like in the subquery
    case. The difference here is that you put them at the top of your main statement
    (before your main `SELECT`) and prefix with the word `WITH`. In the next listing,
    I take the same statement as the subquery case but use two CTE instead. These
    can also be considered makeshift `CREATE` statements that get dropped at the end
    of the query, just like the `with` keyword in Python.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 子查询很酷，但可能难以阅读和调试，因为你在主查询中增加了复杂性。这就是公用表表达式（CTE）特别有用之处。CTE 是一个表定义，就像在子查询的情况下一样。这里的区别在于你将它们放在主语句的顶部（在主
    `SELECT` 之前），并以前缀词 `WITH` 开头。在下一个列表中，我使用了与子查询相同的语句，但使用了两个 CTE。这些也可以被视为临时 `CREATE`
    语句，在查询结束时被删除，就像 Python 中的 `with` 关键字一样。
- en: Listing 7.14 Finding highest failure rates using common table expressions
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.14 使用公用表表达式查找最高故障率
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ We can refer to drive_days and failures in our main query.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以在主查询中参考 drive_days 和 failures。
- en: In Python, the best alternative I’ve found is to wrap statements in a function.
    Any intermediate variable created in the scope of the function would not be kept
    once the function returns. My version of the query using PySpark is in the next
    listing.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我发现最好的替代方案是将语句包装在一个函数中。在函数的作用域内创建的任何中间变量，一旦函数返回，就不会被保留。我使用 PySpark
    的查询版本将在下一个列表中。
- en: Listing 7.15 Finding the highest failure rate using Python scope rules
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.15 使用 Python 范围规则查找最高故障率
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ We are creating intermediate data frames within the body of the function to
    avoid having a monster query.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在函数体内部创建中间数据框，以避免有一个庞大的查询。
- en: ❷ Our answer data frame uses both intermediate data frames.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们的答案数据框使用了两个中间数据框。
- en: ❸ We are testing if we have a variable drive_days in scope once the function
    returned confirms that our intermediate frames are neatly confined inside the
    function scope.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 当函数返回确认我们的中间框架被整洁地限制在函数范围内时，我们正在测试是否有一个变量 drive_days 在范围内。
- en: In this section, we transformed data using both the PySpark/Python data transformation
    APIs, as well as Spark SQL. PySpark gives the floor to SQL without too much ceremony.
    This can be very convenient if you happen to hang out with DBAs and SQL developers,
    as you can collaborate using their preferred language, knowing that Python is
    right around the corner. Everybody wins!
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 PySpark/Python 数据转换 API 和 Spark SQL 对数据进行转换。PySpark 在不太多仪式的情况下将优先权给了
    SQL。如果你经常与 DBA 和 SQL 开发人员打交道，这会非常方便，因为你可以使用他们首选的语言进行协作，同时知道 Python 就在附近。大家都赢了！
- en: 7.4.7 A quick summary of PySpark vs. SQL syntax
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.7 PySpark 与 SQL 语法快速总结
- en: 'PySpark borrowed a lot of vocabulary from the SQL world. I think this was a
    very smart idea: there are generations of programmers who know SQL, and adopting
    the same keywords makes it easy to communicate. Where we see a lot of difference
    is in the order of the operations: PySpark will naturally encourage you to think
    about the order in which the operations should be performed. SQL follows a more
    rigid framework that requires you to remember if your operation belongs in the
    operation, the target, or the condition clause.'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 从 SQL 世界借用了很多词汇。我认为这是一个非常聪明的想法：有代际的程序员知道 SQL，采用相同的关键字使得沟通变得容易。我们在操作顺序上看到很多差异：PySpark
    会自然地鼓励你思考操作应该执行的顺序。SQL 遵循一个更严格的框架，要求你记住你的操作属于操作、目标还是条件子句。
- en: I find PySpark’s way of treating data manipulation more intuitive, but will
    rely on my years of SQL experience as a data analyst when convenient. When writing
    SQL, I usually write my query out of order, starting with the target and building
    as I go. Not everything needs to be top to bottom!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我发现 PySpark 处理数据操作的方式更直观，但在方便的时候，我会依靠我作为数据分析师多年的 SQL 经验。在编写 SQL 时，我通常先写出目标，然后逐步构建。不是所有东西都需要从头到尾！
- en: So far, I’ve tried to keep both languages in a vacuum. We’ll now break the barrier
    and unleash the power of Python + SQL. This will simplify how we write certain
    transformations and make our code easier to write and a lot less busy.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我尽量使两种语言保持独立。现在，我们将打破这种障碍，释放 Python + SQL 的力量。这将简化我们编写某些转换的方式，并使我们的代码更容易编写，且更加简洁。
- en: Exercise 7.1
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 7.1
- en: Taking the `elements` data frame, which PySpark code is equivalent to the following
    SQL statement?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 以 `elements` 数据帧为例，以下哪个 PySpark 代码与以下 SQL 语句等价？
- en: '[PRE17]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: a) `element.groupby("Radioactive").count().show()`
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: a) `element.groupby("Radioactive").count().show()`
- en: b) `elements.where(F.col("Radioactive").isNotNull()).groupby().count().show()`
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: b) `elements.where(F.col("Radioactive").isNotNull()).groupby().count().show()`
- en: c) `elements.groupby("Radioactive").where(F.col("Radioactive").isNotNull()).show()`
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: c) `elements.groupby("Radioactive").where(F.col("Radioactive").isNotNull()).show()`
- en: d) `elements.where(F.col("Radioactive").isNotNull()).count()`
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: d) `elements.where(F.col("Radioactive").isNotNull()).count()`
- en: e) None of the queries above
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: e) 以上查询均不适用
- en: '7.5 Simplifying our code: Blending SQL and Python'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 简化我们的代码：融合 SQL 和 Python
- en: 'PySpark is rather accommodating when taking method and function parameters:
    you can pass a column name (as a string) instead of a column object (`F.col()`)
    when using `groupby()` (see chapter 4). In addition, there are a few methods we
    can use to cram a little SQL syntax into our PySpark code. You’ll see that there
    aren’t many methods in which you can use this, but it’s so useful and well done
    that you’ll end up using it all the time.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 在处理方法和函数参数时相当灵活：在使用 `groupby()` 时，你可以传递一个列名（作为字符串），而不是列对象（`F.col()`）（参见第
    4 章）。此外，还有一些方法我们可以使用，将一点 SQL 语法塞入我们的 PySpark 代码中。你会发现能使用这种方法的地方不多，但它非常实用且做得很好，你最终会经常使用它。
- en: This section will build on the code we’ve written so far. We’re going to write
    a function that, for a given capacity, will return the top three most reliable
    drives according to our failure rate. We’ll leverage the code we’ve already written
    and simplify it.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将基于我们迄今为止编写的代码。我们将编写一个函数，对于给定的容量，将根据我们的故障率返回最可靠的三个驱动器。我们将利用我们已编写的代码并简化它。
- en: 7.5.1 Using Python to increase the resiliency and simplifying the data reading
    stage
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 使用 Python 提高鲁棒性和简化数据读取阶段
- en: We start by simplifying the code to read the data. The data ingestion part of
    the program is displayed in listing 7.16\. There are a few changes compared to
    our original data ingestion.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先简化了读取数据的代码。程序的数据摄取部分显示在列表 7.16 中。与我们的原始数据摄取相比，有一些变化。
- en: First, I put all the directories in a list so that I could read them using a
    list comprehension. This removes some repetitive code and will also work easily
    if I remove or add files (if you are only using Q3 2019, you can remove the other
    entries in the list).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我将所有目录放入一个列表中，这样我就可以使用列表推导来读取它们。这消除了一些重复的代码，并且如果我要删除或添加文件（如果你只使用Q3 2019，你可以从列表中删除其他条目）的话，它也会很容易工作。
- en: 'Second, since we do not need the SMART (*Self-Monitoring, Analysis, and Reporting
    Technology*, a monitoring system included in most hard drives; see [http://mng.bz/jydV](http://mng.bz/jydV)
    for more information.) measurements, I take the intersection of the columns instead
    of trying to fill in the missing columns with `null` values. In order to create
    a common intersection that will apply to any number of data sources, I use `reduce`,
    which applies the anonymous function on all the column sets, resulting in the
    common columns between all the data frames. (For those unfamiliar with `reduce`,
    I find the Python documentation very explicit and easy to follow: [http://mng.bz/y4YG](http://mng.bz/y4YG).)
    I also add an assertion on the common set of columns, as I want to make sure it
    contains the columns I need for the analysis. Assertions are a good way to short-circuit
    an analysis if certain conditions are not met. In this case, if I am missing one
    of the columns, I’d rather have my program fail early with an `AssertionError`
    than have a huge stack trace later.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，由于我们不需要SMART（*自我监控、分析和报告技术*，大多数硬盘驱动器中包含的监控系统；更多信息请见[http://mng.bz/jydV](http://mng.bz/jydV)）的测量，我选择列的交集而不是尝试用`null`值填充缺失的列。为了创建适用于任何数量数据源的通用交集，我使用了`reduce`，它将匿名函数应用于所有列集，从而得到所有数据帧之间的公共列。（对于那些不熟悉`reduce`的人，我发现Python文档非常明确且易于理解：[http://mng.bz/y4YG](http://mng.bz/y4YG)。）我还添加了一个关于公共列集的断言，因为我想要确保它包含我用于分析所需的列。断言是在某些条件未满足时短路分析的好方法。在这种情况下，如果缺少一个列，我宁愿程序在`AssertionError`中提前失败，也不愿在之后出现巨大的堆栈跟踪。
- en: Finally, I use a second `reduce` for unioning all the distinct data frames into
    a cohesive one. The same principle is used as when I created the common variables.
    This makes the code a lot cleaner, and it will work without any modifications
    should I want to add more sources or remove some.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我使用第二个`reduce`来合并所有不同的数据帧成为一个统一的数据帧。这与我创建公共变量时使用的原则相同。这使得代码更加简洁，如果我想添加更多来源或删除一些，它将无需任何修改即可工作。
- en: Listing 7.16 The data ingestion part of our program
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.16 我们程序的数据摄入部分
- en: '[PRE18]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 7.5.2 Using SQL-style expressions in PySpark
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 在PySpark中使用SQL风格的表达式
- en: 'Now that our data has been read and is in a steady state, we can process it
    so that it can easily answer our question. Three methods accept SQL-type statements:
    `selectExpr()`, `expr()`, and `where()`/`filter()`. In this section, we use SQL-style
    expressions when appropriate to showcase when it makes sense to fuse both languages.
    At the end of this section, we have code that'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的数据已经被读取并且处于稳定状态，我们可以处理它，使其能够轻松回答我们的问题。三种方法接受SQL类型的语句：`selectExpr()`、`expr()`和`where()`/`filter()`。在本节中，我们适当地使用SQL风格的表达式来展示何时融合这两种语言是有意义的。在本节的末尾，我们有代码
- en: Selects only the useful columns for our query
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅选择查询中使用的有用列
- en: Gets our drive capacity in gigabytes
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取我们的驱动器容量（以千兆字节为单位）
- en: Computes the `drive_days` and `failures` data frames
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算出`drive_days`和`failures`数据帧
- en: Joins the two data frames into a summarized one and computes the failure rate
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将两个数据帧合并成一个汇总的数据帧并计算故障率
- en: The code is available in the following listing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 代码在以下列表中可用。
- en: Listing 7.17 Processing our data so it’s ready for the query function
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.17 处理我们的数据以便为查询函数做好准备
- en: '[PRE19]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '`selectExpr()` is just like the `select()` method with the exception that it
    will process SQL-style operations. I am quite a fan of this method since it removes
    a bit of syntax when manipulating columns with functions and arithmetic. In our
    case, the PySpark alternative (displayed in the next listing) is a little more
    verbose and cumbersome to write and read, especially since we have to create a
    literal `1024` column to apply the `pow()` function.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`selectExpr()`与`select()`方法类似，只是它将处理SQL风格的运算。我非常喜爱这个方法，因为它在用函数和算术操作处理列时减少了语法。在我们的案例中，PySpark的替代方案（在下一条列表中显示）稍微有点冗长且难以编写和阅读，特别是我们不得不创建一个字面量`1024`列来应用`pow()`函数。'
- en: Listing 7.18 Replacing `selectExpr()` with a regular `select()`
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.18 将`selectExpr()`替换为常规的`select()`
- en: '[PRE20]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The second method is simply called `expr()`. It wraps a SQL-style expression
    into a column. This is kind of a generalized `selectExpr()` that you can use in
    lieu of `F.col()` (or the column name) when you want to modify a column. If we
    take our `failures` table from listing 7.17, we can use an `expr` (or *expression*)
    as the `agg()` argument. This alternative syntax is shown in the next listing.
    I like doing it in `agg()` parameters, because it saves a lot of `alias()`.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法简单地称为 `expr()`。它将 SQL 风格的表达式包装成一个列。这有点像通用的 `selectExpr()`，当你想要修改一个列时，你可以用它来代替
    `F.col()`（或列名）。如果我们从列表 7.17 中的 `failures` 表开始，我们可以使用 `expr`（或 *表达式*）作为 `agg()`
    参数。这种替代语法在下一个列表中展示。我喜欢在 `agg()` 参数中这样做，因为它可以节省很多 `alias()`。
- en: Listing 7.19 Using a SQL expression in our `failures` data frame code
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.19 在我们的 `failures` 数据框代码中使用 SQL 表达式
- en: '[PRE21]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The third method, and my favorite, is the `where()/filter()` method. I find
    the syntax for filtering in SQL much less verbose than regular PySpark; being
    able to use the SQL syntax as the argument of the `filter()` method with no ceremony
    is a godsend. In our final program, I am able to use `full_data.where("failure`
    `=` `1")` instead of wrapping the column name in `F.col()` like we’ve been doing.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法，也是我最喜欢的，是 `where()/filter()` 方法。我发现 SQL 中过滤的语法比常规 PySpark 更简洁；能够无需任何仪式地将
    SQL 语法作为 `filter()` 方法的参数使用，真是一个福音。在我们的最终程序中，我能够使用 `full_data.where("failure`
    `=` `1")` 而不是像我们之前做的那样将列名包裹在 `F.col()` 中。
- en: 'I reuse this convenience in the query function, which is displayed in listing
    7.20\. This time, I use string interpolation in conjunction with `between`. This
    doesn’t save many key strokes, but it’s easy to understand, and you don’t get
    as much line noise as when using the `data.capacity_GB.between(capacity_min,`
    `capacity_max)` (if you prefer using the column function, you can also use this
    syntax: `F.col("capacity_GB") .between(capacity_min,` `capacity_max)`). At this
    point, it’s very much a question of personal style and how familiar you are with
    each approach.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我在查询函数中重用了这个便利性，该函数在列表 7.20 中展示。这次，我结合了字符串插值和 `between`。这并不节省很多按键，但它很容易理解，而且当你使用
    `data.capacity_GB.between(capacity_min,` `capacity_max)`（如果你更喜欢使用列函数，你也可以使用这种语法：`F.col("capacity_GB")
    .between(capacity_min,` `capacity_max)`）时，你不会得到那么多的行噪声。在这个阶段，这很大程度上是一个个人风格的问题，以及你对每种方法有多熟悉。
- en: Listing 7.20 The `most_reliable_drive_for_capacity()` function
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20 `most_reliable_drive_for_capacity()` 函数
- en: '[PRE22]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ I used a SQL-style expression in my where() method, without having to use
    any other special syntax or method.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我在 `where()` 方法中使用了 SQL 风格的表达式，而不需要使用任何其他特殊语法或方法。
- en: ❷ Since we want to return the top N results, not just show them, I use limit()
    instead of show().
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 由于我们想要返回前 N 个结果，而不仅仅是显示它们，所以我使用 `limit()` 而不是 `show()`。
- en: 7.6 Conclusion
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 结论
- en: You do not need to learn or use SQL to effectively work with PySpark. That being
    said, since the data manipulation API shares so much vocabulary and functionality
    with SQL, you will have a much more productive time with PySpark if you have a
    basic understanding of the syntax and query structure.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要学习或使用 SQL 就能有效使用 PySpark。话虽如此，由于数据操作 API 与 SQL 共享了如此多的词汇和功能，如果你对语法和查询结构有基本的了解，你将会有更多富有成效的时间使用
    PySpark。
- en: My family speaks both English and French, and sometimes you don’t always know
    where one language starts and one ends. I tend to think in both languages, and
    sometimes blend them in a single sentence. Likewise, I find that some problems
    are easier to solve with Python, and some are more in SQL’s territory. You will
    find your own balance, as well, which is why it’s nice to have the option. Just
    like spoken languages, the goal is to express your thoughts and intentions as
    clearly as possible while keeping your audience in mind.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我的家庭说英语和法语，有时你并不总是知道一种语言从哪里开始，在哪里结束。我倾向于用两种语言思考，有时在单个句子中混合它们。同样，我发现一些问题用 Python
    更容易解决，而另一些则更适合 SQL 的领域。你也会找到自己的平衡点，这就是为什么有这个选项是件好事。就像口语语言一样，目标是以尽可能清晰的方式表达你的思想和意图，同时考虑到你的听众。
- en: Summary
  id: totrans-206
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: Spark provides a SQL API for data manipulation. This API supports ANSI SQL.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 提供了一个用于数据操作的数据 API。此 API 支持 ANSI SQL。
- en: Spark (and PySpark, by extension) borrows a lot of vocabulary and expected functionality
    from the way SQL manipulates tables. This is especially evident since the data
    manipulation module is called `pyspark.sql`.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark（以及通过扩展的 PySpark）从 SQL 操作表的方式中借用了许多词汇和预期的功能。这一点在数据操作模块被称为 `pyspark.sql`
    时尤为明显。
- en: PySpark’s data frames need to be registered as views or tables before they can
    be queried with Spark SQL. You can give them a different name than the data frame
    you’re registering.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark的数据帧在可以使用Spark SQL查询之前需要注册为视图或表。你可以给它们一个不同于你注册的数据帧的名字。
- en: PySpark’s own data frame manipulation methods and functions borrow SQL functionality,
    for the most part. Some exceptions, such as `union()`, are present and documented
    in the API.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark自己的数据帧操作方法和函数大部分借鉴了SQL功能。一些例外，如`union()`，在API中有文档说明。
- en: Spark SQL queries can be inserted in a PySpark program through the `spark.sql`
    function, where `spark` is the running `SparkSession`.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以通过`spark.sql`函数将Spark SQL查询插入到PySpark程序中，其中`spark`是正在运行的`SparkSession`。
- en: Spark SQL table references are kept in a `Catalog`, which contains the metadata
    for all tables accessible to Spark SQL.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark SQL表引用保存在`Catalog`中，它包含所有Spark SQL可访问表的元数据。
- en: PySpark will accept SQL-style clauses in `where()`, `expr()`, and `selectExpr()`,
    which can simplify the syntax for complex filtering and selection.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark接受`where()`、`expr()`和`selectExpr()`中的SQL风格子句，这可以简化复杂过滤和选择的语法。
- en: When using Spark SQL queries with user-provided input, be careful with sanitizing
    the inputs to avoid potential SQL injection attacks.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用Spark SQL查询与用户提供的输入时，请注意清理输入以避免潜在的SQL注入攻击。
- en: Additional exercises
  id: totrans-215
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 7.2
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.2
- en: 'If we look at the code that follows, we can simplify it even further and avoid
    creating two tables outright. Can you write a `summarized_data` without having
    to use a table other than `full_data` and no join? (Bonus: Try using pure PySpark,
    then pure Spark SQL, and then a combo of both.)'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看下面的代码，我们可以进一步简化它，并避免直接创建两个表。你能写一个`summarized_data`，而不需要使用除了`full_data`之外的任何表，也不需要连接吗？（加分：尝试使用纯PySpark，然后纯Spark
    SQL，最后是两者的组合。）
- en: '[PRE23]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Exercise 7.3
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.3
- en: 'The analysis in the chapter is flawed in that the age of a drive is not taken
    into consideration. Instead of ordering the model by failure rate, order by average
    age at failure (assume that every drive fails on the maximum date reported if
    they are still alive). (Hint: Remember that you need to count the age of each
    drive first.)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的分析存在缺陷，因为未考虑驱动器的年龄。与其按故障率对模型进行排序，不如按平均故障年龄排序（假设如果它们仍然存活，每个驱动器都会在报告的最大日期上失败）。（提示：记住，你需要首先计算每个驱动器的年龄。）
- en: Exercise 7.4
  id: totrans-221
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.4
- en: What is the total capacity (in TB) that Backblaze records at the beginning of
    each month?
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: Backblaze在每个月初记录的总容量（TB）是多少？
- en: Exercise 7.5
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.5
- en: Note There is a much more elegant way to solve this problem that we see in chapter
    10 using window functions. In the meantime, this exercise can be solved with the
    judicious usage of `group` `by`s and joins.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在第10章中，我们看到了一个使用窗口函数解决这个问题的更优雅的方法。在此期间，这个练习可以通过谨慎使用`group by`和连接来解决。
- en: If you look at the data, you’ll see that some drive models can report an erroneous
    capacity. In the data preparation stage, restage the `full_data` data frame so
    that the most common capacity for each drive is used.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看数据，你会发现一些驱动模型可能会报告错误的容量。在数据准备阶段，重新调整`full_data`数据帧，以便使用每个驱动器最常见的容量。
- en: '* * *'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: ¹ The `with` statement is usually used with resources that need to be cleaned
    up at the end. It doesn’t really apply here, but I felt like the comparison was
    worth mentioning.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ `with`语句通常与需要在结束时清理的资源一起使用。它在这里并不适用，但我认为这个比较值得一提。
