- en: 5 A gentle introduction to classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 对分类的温和介绍
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Writing formal notation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编写正式符号
- en: Using logistic regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑回归
- en: Working with a confusion matrix
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵
- en: Understanding multiclass classification
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解多类别分类
- en: Imagine an advertisement agency collecting information about user interactions
    to decide what type of ad to show. That’s not uncommon. Google, Twitter, Facebook,
    and other big tech giants that rely on ads have creepy-good personal profiles
    of their users to help deliver personalized ads. A user who’s recently searched
    for gaming keyboards or graphics cards is probably more likely to click ads about
    the latest and greatest video games.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个广告机构收集有关用户互动的信息，以决定显示哪种类型的广告。这不是不常见的事情。谷歌、推特、Facebook和其他依赖广告的大科技公司拥有令人毛骨悚然的优秀用户个人资料，以帮助提供个性化广告。最近搜索过游戏键盘或显卡的用户可能更有可能点击关于最新和最伟大的视频游戏的广告。
- en: Delivering an advertisement specially crafted to each person may be difficult,
    so grouping users into categories is a common technique. A user may be categorized
    as a gamer to receive relevant video game-related ads, for example.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 向每个人发送专门定制的广告可能很困难，因此将用户分组到类别中是一种常见的技术。例如，一个用户可能被归类为游戏玩家，以接收相关的视频游戏相关广告。
- en: Machine learning is the go-to tool for accomplishing such a task. At the most
    fundamental level, machine-learning practitioners want to build a tool to help
    them understand data. Labeling data items as belonging in separate categories
    is an excellent way to characterize data for specific needs.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是实现此类任务的常用工具。在最基本层面上，机器学习从业者希望构建一个工具来帮助他们理解数据。将数据项标记为属于不同的类别是针对特定需求表征数据的一种绝佳方式。
- en: Chapter 4 dealt with regression, which is about fitting a curve to data. As
    you recall, the best-fit curve is a function that takes as input a data item and
    assigns it a number drawn from a continuous distribution. Creating a machine-learning
    model that instead assigns discrete labels to its inputs is called *classification*.
    Classification is a supervised learning algorithm for dealing with discrete output.
    (Each discrete value is called a *class*.) The input is typically a feature vector,
    and the output is a class. If there are only two class labels (`True`/`False`,
    `On`/`Off`, `Yes`/`No`), we call this learning algorithm a *binary classifier*;
    otherwise, it’s called a *multiclass classifier*.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 第四章讨论了回归，这是关于将曲线拟合到数据上。正如你所回忆的，最佳拟合曲线是一个函数，它接受一个数据项作为输入，并给它分配一个来自连续分布的数字。创建一个将离散标签分配给其输入的机器学习模型称为*分类*。分类是处理离散输出的监督学习算法。（每个离散值称为一个*类别*。）输入通常是特征向量，输出是一个类别。如果只有两个类别标签（`True`/`False`，`On`/`Off`，`Yes`/`No`），我们称这种学习算法为*二元分类器*；否则，它被称为*多类别分类器*。
- en: There are many types of classifiers, but this chapter focuses on the ones outlined
    in table 5.1\. Each has advantages and disadvantages, which we’ll delve into deeper
    after we start implementing each one in TensorFlow.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多种类的分类器，但本章重点介绍表5.1中概述的类别。每个都有其优点和缺点，我们将在开始在每个TensorFlow中实现每个类别之后更深入地探讨。
- en: Linear regression is the easiest type to implement because we did most of the
    hard work in chapters 3 and 4, but as you’ll see, it’s a terrible classifier.
    A much better classifier is the logistic regression algorithm. As the name suggests,
    it uses logarithmic properties to define a better cost function. Finally, softmax
    regression is a direct approach to solving multiclass classification. It’s a natural
    generalization of logistic regression and is called softmax regression because
    a function called `softmax` is applied as the last step.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是最容易实现的类型，因为我们已经在第3章和第4章中做了大部分艰苦的工作，但正如你将看到的，它是一个糟糕的分类器。一个更好的分类器是逻辑回归算法。正如其名所示，它使用对数属性来定义更好的成本函数。最后，softmax回归是解决多类别分类的直接方法。它是逻辑回归的自然推广，被称为softmax回归，因为最后一步应用了一个名为`softmax`的函数。
- en: Table 5.1 Classifiers
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 表5.1 分类器
- en: '| Type | Pros | Cons |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 优点 | 缺点 |'
- en: '| Linear regression | Simple to implement | Not guaranteed to workSupports
    only binary labels |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 实现简单 | 不保证有效支持只支持二元标签 |'
- en: '| Logistic regression | Highly accurateFlexible ways to regularize model for
    custom adjustmentModel responses are measures of probability.Easy-to-update model
    with new data | Supports only binary labels |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 高度准确灵活的方式对模型进行正则化以进行自定义调整模型响应是概率的度量。易于用新数据更新模型 | 只支持二元标签 |'
- en: '| Softmax regression | Supports multiclass classificationModel responses are
    measures of probability. | More complicated to implement |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 软最大化回归 | 支持多类分类模型响应是概率的度量。 | 实现起来更复杂 |'
- en: 5.1 Formal notation
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 形式化表示
- en: In mathematical notation, a classifier is a function *y* = *f*(*x*), where *x*
    is the input data item and *y* is the output category (figure 5.1). Adopting from
    traditional scientific literature, we often refer to the input vector *x* as the
    *independent variable* and the output *y* as the *dependent variable*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在数学符号中，分类器是一个函数 *y* = *f*(*x*)，其中 *x* 是输入数据项，而 *y* 是输出类别（图5.1）。从传统的科学文献中借鉴，我们通常将输入向量
    *x* 称为 *自变量*，将输出 *y* 称为 *因变量*。
- en: '![CH05_F01_Mattmann2](../Images/CH05_F01_Mattmann2.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Mattmann2](../Images/CH05_F01_Mattmann2.png)'
- en: Figure 5.1 A classifier produces discrete outputs but may take either continuous
    or discrete inputs.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 分类器产生离散输出，但可能接受连续或离散输入。
- en: Formally, a category label is restricted to a range of possible values. You
    can think of two-valued labels as being like Boolean variables in Python. When
    the input features have only a fixed set of possible values, you need to ensure
    that your model can understand how to handle those values. Because the functions
    in a model typically deal with continuous real numbers, you need to preprocess
    the dataset to account for discrete variables, which are either ordinal or nominal
    (figure 5.2).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 形式上，类别标签被限制在可能的值范围内。你可以将二元标签视为Python中的布尔变量。当输入特征只有一组可能的值时，你需要确保你的模型能够理解如何处理这些值。因为模型中的函数通常处理连续的实数，你需要预处理数据集以考虑离散变量，这些变量可以是序数或名义的（图5.2）。
- en: '![CH05_F02_Mattmann2](../Images/CH05_F02_Mattmann2.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Mattmann2](../Images/CH05_F02_Mattmann2.png)'
- en: 'Figure 5.2 There are two types of discrete sets: those with values that can
    be ordered (ordinal) and those with values that can’t be ordered (nominal).'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 存在两种类型的离散集合：那些可以排序的值（序数）和那些不能排序的值（名义）。
- en: Values of an ordinal type, as the name suggests, can be ordered. The values
    in a set of even numbers from 1 to 10 are ordinal, for example, because integers
    can be compared with one another. On the other hand, an element from a set of
    fruits `{banana,` `apple,` `orange}` might not come with a natural ordering. We
    call values from such a set *nominal* because they can be described by only their
    names.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，序数类型的值可以排序。例如，从1到10的偶数集合中的值是序数，因为整数可以相互比较。另一方面，来自 `{banana,` `apple,`
    `orange}` 水果集合的元素可能没有自然的排序。我们称此类集合的值为 *名义值*，因为它们只能通过其名称来描述。
- en: A simple approach to representing nominal variables in a dataset is to assign
    a number to each label. The set `{banana,` `apple,` `orange}` could instead be
    processed as `{0,` `1,` `2}`. But some classification models may have a strong
    bias about how the data behaves. Linear regression, for example, would interpret
    our apple as being midway between a banana and an orange, which makes no natural
    sense.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中表示名义变量的一个简单方法是为每个标签分配一个数字。集合 `{banana,` `apple,` `orange}` 可以被处理为 `{0,`
    `1,` `2}`。但是，某些分类模型可能对数据的行为有强烈的偏见。例如，线性回归会将我们的苹果解释为香蕉和橙子的中间值，这没有任何自然的意义。
- en: 'A simple workaround to represent nominal categories of a dependent variable
    is to add dummy variables for each value of the nominal variable. In this example,
    the `fruit` variable would be removed and replaced by three separate variables:
    `banana`, `apple`, and `orange`. Each variable holds a value of `0` or `1` (figure
    5.3), depending on whether the category for that fruit holds true. This process
    is often referred to as *one-hot encoding*.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了表示因变量的名义类别，一个简单的解决方案是为名义变量的每个值添加虚拟变量。在这个例子中，`fruit` 变量将被移除，并替换为三个单独的变量：`banana`、`apple`
    和 `orange`。每个变量持有 `0` 或 `1` 的值（图5.3），具体取决于该水果的类别是否为真。这个过程通常被称为 *独热编码*。
- en: As in linear regression from chapters 3 and 4, the learning algorithm must traverse
    the possible functions supported by the underlying model, called `M`. In linear
    regression, the model was parameterized by `w`. So the function `y = M(w)` can
    be tried to measure its cost. In the end, we choose a value of `w` with the least
    cost. The only difference between regression and classification is that the output
    is no longer a continuous spectrum, but a discrete set of class labels.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第3章和第4章中的线性回归一样，学习算法必须遍历底层模型支持的可能的函数，该模型称为`M`。在线性回归中，模型由`w`参数化。因此，函数`y = M(w)`可以尝试来衡量其成本。最终，我们选择具有最小成本的`w`值。回归和分类之间的唯一区别是输出不再是连续的谱，而是一组离散的类别标签。
- en: '![CH05_F03_Mattmann2](../Images/CH05_F03_Mattmann2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_Mattmann2](../Images/CH05_F03_Mattmann2.png)'
- en: Figure 5.3 If the values of a variable are nominal, they may need to be preprocessed.
    One solution is to treat each nominal value as a Boolean variable, as shown on
    the right; `banana`, `apple`, and `orange` are three newly added variables, each
    having a value of `0` or `1`. The original `fruit` variable is removed.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 如果变量的值是名义的，可能需要进行预处理。一个解决方案是将每个名义值视为一个布尔变量，如图中所示；“香蕉”、“苹果”和“橙子”是三个新添加的变量，每个变量都有`0`或`1`的值。原始的`fruit`变量被移除。
- en: Exercise 5.1
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.1
- en: Is it a better idea to treat each of the following as a regression or classification
    task?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 将以下每个任务视为回归或分类任务，哪个更好？
- en: (a) Predicting stock prices
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 预测股票价格
- en: (b) Deciding which stocks you should buy, sell, or hold
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 决定你应该买入、卖出或持有的股票
- en: (c) Rating the quality of a computer on a 1-10 scale
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 在1-10的尺度上评估计算机的质量
- en: '**Answers**'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: (a) Regression
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 回归
- en: (b) Classification
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: (b) 分类
- en: (c) Either
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: (c) 或者
- en: Because the input/output types for regression are even more general than those
    of classification, nothing prevents you from running a linear regression algorithm
    on a classification task. In fact, that’s exactly what you’ll do in section 5.3.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 由于回归的输入/输出类型比分类更通用，没有任何东西阻止你在分类任务上运行线性回归算法。实际上，这正是你将在第5.3节中做的事情。
- en: Before you begin implementing TensorFlow code, however, it’s important to gauge
    the strength of a classifier. Section 5.2 covers state-of-the-art approaches for
    measuring a classifier’s success.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始实现TensorFlow代码之前，然而，重要的是要评估分类器的强度。第5.2节涵盖了衡量分类器成功度的最先进方法。
- en: 5.2 Measuring performance
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 衡量性能
- en: Before you begin writing classification algorithms, you should be able to check
    the success of your results. This section covers essential techniques for measuring
    performance in classification problems.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始编写分类算法之前，你应该能够检查你结果的成功率。本节涵盖了在分类问题中衡量性能的基本技术。
- en: 5.2.1 Accuracy
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 准确度
- en: Do you remember those multiple-choice exams in high school or college? Classification
    problems in machine learning are similar. Given a statement, your job is to classify
    it as one of the given multiple-choice “answers.” If you have only two choices,
    as in a true-or-false exam, we call it a *binary classifier*. In a graded exam
    in school, the typical way to measure your score is to count the number of correct
    answers and divide that number by the total number of questions.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得高中或大学时的那些多项选择题吗？机器学习中的分类问题与此类似。给定一个陈述，你的任务是将其分类为给定的多项选择题“答案”之一。如果你只有两个选择，就像在是非考试中一样，我们称之为*二元分类器*。在学校里的评分考试中，衡量你的分数的典型方式是计算正确答案的数量，并将其除以总问题数。
- en: 'Machine learning adopts the same scoring strategy and calls it *accuracy*.
    Accuracy is measured by the following formula:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习采用相同的评分策略，称之为*准确度*。准确度通过以下公式来衡量：
- en: '![CH05_F03EQ01_Mattmann2](../Images/CH05_F03EQ01_Mattmann2.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03EQ01_Mattmann2](../Images/CH05_F03EQ01_Mattmann2.png)'
- en: This formula provides a crude summary of performance, which may be sufficient
    if you’re worried only about the overall correctness of the algorithm. But the
    accuracy measure doesn’t reveal a breakdown of correct and incorrect results for
    each label.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式提供了一个粗略的性能总结，如果你只关心算法的整体正确性，这可能就足够了。但准确度度量并没有揭示每个标签的正确和错误结果的细分。
- en: To account for this limitation, a confusion matrix provides a more detailed
    report on a classifier’s success. A useful way to describe how well a classifier
    performs is to inspect the way it performs on each of the classes.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了弥补这一限制，混淆矩阵为分类器的成功提供了更详细的报告。描述分类器表现好坏的一个有用方法是检查它在每个类别上的表现。
- en: Consider a binary classifier with positive and negative labels, for example.
    As shown in figure 5.4, a *confusion matrix* is a table that compares how the
    predicted responses compare with actual ones. Data items that are correctly predicted
    as positive are called *true positives* (TP). Those that are incorrectly predicted
    as positive are called *false positives* (FP). If the algorithm accidentally predicts
    an element to be negative when in reality it is positive, we call this situation
    a *false negative* (FN). Finally, when prediction and reality agree that a data
    item is a negative label, it’s called a *true negative* (TN). As you can see,
    a confusion matrix enables you to see easily how often a model confuses two classes
    that it’s trying to differentiate.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个具有正负标签的二分类器，例如。如图5.4所示，*混淆矩阵*是一个表格，它比较了预测响应与实际响应的比较。被正确预测为正的数据项称为*真阳性*（TP）。那些被错误预测为正的数据项称为*假阳性*（FP）。如果算法错误地将一个元素预测为负，而实际上它是正的，我们称这种情况为*假阴性*（FN）。最后，当预测和现实都认为一个数据项是负标签时，我们称之为*真阴性*（TN）。正如你所看到的，混淆矩阵使你能够轻松地看到模型在区分两个类别时混淆的频率。
- en: '![CH05_F04_Mattmann2](../Images/CH05_F04_Mattmann2.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04_Mattmann2](../Images/CH05_F04_Mattmann2.png)'
- en: Figure 5.4 You can compare predicted results with actual results by using a
    matrix of positive (green check mark) and negative (red forbidden) labels.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 你可以通过使用正（绿色勾号）和负（红色禁止）标签的矩阵来比较预测结果与实际结果。
- en: NOTE Many graphics in this book include color, which can be viewed in the e-book
    versions. To get your free e-book in PDF, ePub, or Kindle format, go to [http://mng.bz/JxPo](http://mng.bz/JxPo)
    to register your print book.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本书中包含许多彩色图形，这些图形可以在电子书版本中查看。要获取免费电子书（PDF、ePub或Kindle格式），请访问[http://mng.bz/JxPo](http://mng.bz/JxPo)注册您的印刷版书籍。
- en: 5.2.2 Precision and recall
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 精确度和召回率
- en: Although the definitions of true positives (TP), false positives (FP), true
    negatives (TN), and false negatives (FN) are all useful individually, the power
    comes in the interplay among them.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然真阳性（TP）、假阳性（FP）、真阴性（TN）和假阴性（FN）的定义都是单独有用的，但它们的相互作用才是力量的源泉。
- en: The ratio of true positives to total positive examples is *precision*—a score
    of how likely a positive prediction is to be correct. The left column in figure
    5.4 is the total number of positive predictions (TP + FP), so the equation for
    precision is
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性与总正例的比率是*精确度*——一个衡量正预测可能正确的分数。图5.4中的左侧列是总正预测数（TP + FP），因此精确度的方程是
- en: '![CH05_F04EQ02_Mattmann2](../Images/CH05_F04EQ02_Mattmann2.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04EQ02_Mattmann2](../Images/CH05_F04EQ02_Mattmann2.png)'
- en: The ratio of true positives to all possible positives is *recall*, which measures
    the ratio of true positives found. It’s a score of how many true positives were
    successfully predicted (that is, recalled). The top row in figure 5.4 is the total
    number of positives (TP + FN), so the equation for recall is
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性与所有可能正性的比率是*召回率*，它衡量了找到真阳性的比率。这是一个衡量成功预测（即召回）多少真阳性的分数。图5.4中的顶部行是正性的总数（TP
    + FN），因此召回率的方程是
- en: '![CH05_F04EQ03_Mattmann2](../Images/CH05_F04EQ03_Mattmann2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04EQ03_Mattmann2](../Images/CH05_F04EQ03_Mattmann2.png)'
- en: Simply put, *precision* is a measure of the predictions the algorithm got right,
    and *recall* is a measure of the right things the algorithm identified in the
    final set. If the precision is higher than the recall, the model is better at
    successfully identifying correct items than not identifying some wrong items,
    and vice versa.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，*精确度*是算法预测正确的预测的度量，而*召回率*是算法在最终集中识别出的正确事物的度量。如果精确度高于召回率，则模型在成功识别正确项目方面比未识别某些错误项目方面做得更好，反之亦然。
- en: Here’s a quick example. Suppose that you’re trying to identify cats in a set
    of 100 pictures, in which 40 of the pictures are of cats, and 60 are of dogs.
    When you run your classifier, 10 of the cats are identified as dogs, and 20 of
    the dogs are identified as cats. Your confusion matrix looks like figure 5.5.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个快速示例。假设你正在尝试识别一组100张图片中的猫，其中40张是猫，60张是狗。当你运行你的分类器时，10只猫被识别为狗，20只狗被识别为猫。你的混淆矩阵看起来像图5.5。
- en: '![CH05_F05_Mattmann2](../Images/CH05_F05_Mattmann2.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Mattmann2](../Images/CH05_F05_Mattmann2.png)'
- en: Figure 5.5 An example confusion matrix for evaluating the performance of a classification
    algorithm
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 用于评估分类算法性能的混淆矩阵示例
- en: 'You can see the total number of cats on the left side of the prediction column:
    30 identified correctly and 10 not, totaling 40.'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在预测列的左侧看到猫的总数：30个被正确识别，10个未被识别，总共40个。
- en: Exercise 5.2
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.2
- en: What are the precision and recall for cats? What’s the accuracy of the system?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 猫的精确度和召回率是多少？系统的准确率是多少？
- en: '**Answers**'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: For cats, the precision is 30 / (30 + 20) or 3/5, or 60%. The recall is 30 /
    (30 + 10) or 3/4, or 75%. The accuracy is (30 + 40) / 100, or 70%.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 对于猫，精确度是30 / (30 + 20) 或 3/5，或60%。召回率是30 / (30 + 10) 或 3/4，或75%。准确率是(30 + 40)
    / 100，或70%。
- en: 5.2.3 Receiver operating characteristic curve
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 受试者工作特征曲线
- en: Because binary classifiers are among the most popular tools, many mature techniques
    exist for measuring their performance, such as the *receiver operating characteristic
    (ROC) curve*, a plot that lets you compare the trade-offs between false positives
    and true positives. The x-axis is the measure of false-positive values, and the
    y-axis is the measure of true-positive values.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因为二元分类器是最受欢迎的工具之一，所以存在许多成熟的测量它们性能的技术，例如*受试者工作特征（ROC）曲线*，这是一种让你比较假正例和真正例之间权衡的图表。x轴是假正例值的度量，y轴是真正例值的度量。
- en: A binary classifier reduces its input feature vector to a number and then decides
    the class based on whether the number is greater than or less than a specified
    threshold. As you adjust a threshold of the machine-learning classifier, you plot
    the various values of false-positive and true-positive rates.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类器将输入特征向量减少到一个数字，然后根据这个数字是否大于或小于指定的阈值来决定类别。当你调整机器学习分类器的阈值时，你会在图表上绘制各种假正例和真正例率的值。
- en: A robust way to compare various classifiers is to compare their ROC curves.
    When two curves don’t intersect, one method is certainly better than the other.
    Good algorithms are far above the baseline. For two choices, they do better than
    a random choice, or a 50/50 guess. A quantitative way to compare classifiers is
    to measure the area under the ROC curve. If a model has an area-under-curve (AUC)
    value higher than 0.9, it’s an excellent classifier. A model that randomly guesses
    the output will have an AUC value of about 0.5\. See figure 5.6 for an example.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 比较各种分类器的一种稳健方法是比较它们的ROC曲线。当两条曲线不相交时，一种方法肯定比另一种方法好。好的算法远高于基线。对于两种选择，它们比随机选择或50/50的猜测要好。比较分类器的一种定量方法是测量ROC曲线下的面积。如果一个模型的曲线下面积（AUC）值高于0.9，它是一个优秀的分类器。随机猜测输出的模型将有一个大约0.5的AUC值。见图5.6的示例。
- en: '![CH05_F06_Mattmann2](../Images/CH05_F06_Mattmann2.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F06_Mattmann2](../Images/CH05_F06_Mattmann2.png)'
- en: Figure 5.6 The principled way to compare algorithms is to examine their ROC
    curves. When the true-positive rate is greater than the false-positive rate in
    every situation, it’s straightforward to declare that one algorithm is dominant
    in terms of its performance. If the true-positive rate is less than the false-positive
    rate, the plot dips below the baseline shown by the dotted line.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 比较算法的原理方法是通过检查它们的ROC曲线。当每个情况下的真正率都大于假正率时，可以简单地声明一个算法在性能方面占主导地位。如果真正率小于假正率，图表会低于由虚线表示的基线。
- en: Exercise 5.3
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.3
- en: How would a 100% correct rate (all true positives, no false positives) look
    as a point on an ROC curve?
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 100%正确率（所有真正例，没有假正例）在ROC曲线上看起来会是什么样子？
- en: '**Answers**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: The point for a 100% correct rate would be located on the positive y-axis of
    the ROC curve, as shown in figure 5.7.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 100%正确率的点将位于ROC曲线的正y轴上，如图5.7所示。
- en: '![CH05_F07_Mattmann2](../Images/CH05_F07_Mattmann2.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Mattmann2](../Images/CH05_F07_Mattmann2.png)'
- en: Figure 5.7 A 100% correct classifier with the blue ROC curve alongside the vertical
    true-positive (y) axis
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 具有蓝色ROC曲线的100%正确分类器，与垂直的真正例（y）轴并排
- en: 5.3 Using linear regression for classification
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 使用线性回归进行分类
- en: 'One of the simplest ways to implement a classifier is to tweak a linear regression
    algorithm, as discussed in chapter 3\. As a reminder, a linear regression model
    is a set of functions that look linear: `f(x) = wx`. The function `f(x``)` takes
    continuous real numbers as input and produces continuous real numbers as output.
    Remember that classification is all about discrete outputs. So one way to force
    the regression model to produce a two-valued (binary) output is to set values
    above a certain threshold to a number (such as `1`) and values below that threshold
    to a different number (such as `0`).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实现分类器的一种最简单的方法是调整线性回归算法，如第3章所述。作为提醒，线性回归模型是一组看起来线性的函数：`f(x) = wx`。函数`f(x)`接受连续的实数作为输入，并产生连续的实数作为输出。记住，分类全都是关于离散输出的。因此，强制回归模型产生一个双值（二元）输出的方法之一是将高于某个阈值的值设置为数字（例如`1`），将低于该阈值的值设置为不同的数字（例如`0`）。
- en: We’ll proceed with the following motivating example. Suppose that Alice is an
    avid chess player, and you have records of her win/loss history. Moreover, each
    game has a time limit ranging from 1 to 10 minutes. You can plot the outcome of
    each game as shown in figure 5.8\. The x-axis represents the time limit of the
    game, and the y-axis signifies whether she won (`y` `= 1`) or lost (`y` `= 0`).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用以下激励性示例继续进行。假设Alice是一位热衷的国际象棋玩家，你拥有她胜负历史的记录。此外，每场比赛都有一个从1到10分钟的时间限制。你可以将每场比赛的结果绘制成如图5.8所示。横轴代表比赛的时间限制，纵轴表示她是否获胜（`y`
    `= 1`）或失败（`y` `= 0`）。
- en: '![CH05_F08_Mattmann2](../Images/CH05_F08_Mattmann2.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F08_Mattmann2](../Images/CH05_F08_Mattmann2.png)'
- en: 'Figure 5.8 A visualization of a binary classification training dataset. The
    values are divided into two classes: all points where y = 1 and all points where
    y = 0.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 二元分类训练数据集的可视化。值被分为两类：所有y = 1的点以及所有y = 0的点。
- en: 'As you see from the data, Alice is a quick thinker: she always wins short games.
    But she usually loses games that have longer time limits. From the plot, you’d
    like to predict the critical game time-limit that decides whether she’ll win.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据中可以看出，Alice是一位思维敏捷的人：她总是赢得短局比赛。但她通常会在时间限制较长的比赛中失败。从图中，你可能会想要预测决定她是否会赢的关键比赛时间限制。
- en: You want to challenge her to a game that you’re sure of winning. If you choose
    an obviously long game, such as one that takes 10 minutes, she’ll refuse to play.
    Let’s set up the game time to be as short as possible so she’ll be willing to
    play against you while tilting the balance to your advantage.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 你想挑战她进行一场你确信能赢的比赛。如果你选择一个明显很长的比赛，比如10分钟，她会拒绝比赛。让我们将比赛时间设定得尽可能短，这样她就会愿意和你比赛，同时将平衡倾斜到你这边。
- en: A linear fit on the data gives you something to work with. Figure 5.9 shows
    the best-fit line computed by using linear regression from listing 5.1 (appearing
    later in this section). The value of the line is closer to `1` than it is to `0`
    for games that Alice will likely win. It appears that if you pick a time corresponding
    to when the value of the line is less than `0.5` (that is, when Alice is more
    likely to lose than to win), you have a good chance of winning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据上的线性拟合为你提供了一些可以工作的东西。图5.9显示了使用列表5.1（本节稍后出现）中的线性回归计算出的最佳拟合线。对于Alice可能会赢的比赛，这条线的值更接近`1`而不是`0`。看起来如果你选择一个当线的值小于`0.5`的时间（即Alice更有可能输而不是赢的时候），你就有很大的赢的机会。
- en: '![CH05_F09_Mattmann2](../Images/CH05_F09_Mattmann2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F09_Mattmann2](../Images/CH05_F09_Mattmann2.png)'
- en: Figure 5.9 The diagonal line is the best-fit line on a classification dataset.
    Clearly, the line doesn’t fit the data well, but it provides an imprecise approach
    for classifying new data.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.9 在分类数据集上的最佳拟合线。显然，这条线并不很好地拟合数据，但它为分类新数据提供了一个不精确的方法。
- en: The line is trying to fit the data as best as possible. Due to the nature of
    the training data, the model will respond with values near `1` for positive examples
    and values near `0` for negative examples. Because you’re modeling this data with
    a line, some input may produce values between `0` and `1`. As you may imagine,
    values too far into one category will result in values greater than `1` or less
    than `0`. You need a way to decide when an item belongs to one category more than
    another. Typically, you choose the midpoint, `0.5`, as a deciding boundary (also
    called the *threshold* ). As you’ve seen, this procedure uses linear regression
    to perform classification.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线试图尽可能好地拟合数据。由于训练数据的性质，模型将对正例响应值为接近 `1`，对负例响应值为接近 `0`。由于你使用线来模拟这些数据，某些输入可能产生介于
    `0` 和 `1` 之间的值。正如你可能想象的那样，太靠近某一类别的值将导致值大于 `1` 或小于 `0`。你需要一种方法来决定一个项目属于某一类别更多。通常，你选择中点
    `0.5` 作为决定边界（也称为 *阈值*）。正如你所见，此过程使用线性回归进行分类。
- en: Exercise 5.4
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: What are the disadvantages of using linear regression as a tool for classification?
    (See listing 5.4 for a hint.)
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使用线性回归作为分类工具的缺点是什么？（参见列表 5.4 以获取提示。）
- en: '**Answers**'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Linear regression is sensitive to outliers in your data, so it isn’t an accurate
    classifier.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归对数据中的异常值敏感，因此它不是一个准确的分类器。
- en: Let’s write your first classifier! Open a new Python source file, and call it
    linear.py. Use listing 5.1 to write the code. In the TensorFlow code, you’ll need
    to first define placeholder nodes and then inject values into them from the `session.run()`
    statement.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编写你的第一个分类器！打开一个新的 Python 源文件，并将其命名为 linear.py。使用列表 5.1 编写代码。在 TensorFlow
    代码中，你首先需要定义占位符节点，然后从 `session.run()` 语句中注入值。
- en: Listing 5.1 Using linear regression for classification
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1 使用线性回归进行分类
- en: '[PRE0]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports TensorFlow for the core learning algorithm, NumPy for manipulating
    data, and Matplotlib for visualizing
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 TensorFlow 用于核心学习算法，NumPy 用于数据处理，以及 Matplotlib 用于可视化
- en: ❷ Initializes fake data, 10 instances of each label centered at 5 and 2, with
    stddev 1, respectively
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化假数据，每个标签有 10 个实例，分别以 5 和 2 为中心，stddev 为 1
- en: ❸ Initializes the corresponding labels
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 初始化相应的标签
- en: ❹ Plots the data
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 绘制数据
- en: ❺ Declares the hyperparameters
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 声明超参数
- en: ❻ Sets up the placeholder nodes for the input/output pairs
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 设置输入/输出对的占位符节点
- en: ❼ Defines a linear y = w1 * x + w0 model
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义线性模型 y = w1 * x + w0
- en: ❽ Sets up the parameter variables
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 设置参数变量
- en: ❾ Defines a helper variable, because you’ll refer to it multiple times
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义一个辅助变量，因为你将多次引用它
- en: ❿ Defines the cost function
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 定义成本函数
- en: ⓫ Defines the rule to learn the parameters
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 定义学习参数的规则
- en: After designing the TensorFlow graph, see listing 5.2 to find out how to open
    a new session and execute the graph. `train_op` updates the model’s parameters
    to better guesses. You run `train_op` multiple times in a loop because each step
    iteratively improves the parameter estimate. Listing 5.2 generates a plot similar
    to figure 5.8.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计 TensorFlow 图之后，参见列表 5.2 了解如何打开新会话并执行图。`train_op` 更新模型的参数以更好地猜测。你通过循环多次运行
    `train_op`，因为每一步都会迭代地改进参数估计。列表 5.2 生成与图 5.8 类似的图表。
- en: Listing 5.2 Executing the graph
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 执行图
- en: '[PRE1]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Opens a new session and initializes the variables
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打开一个新的会话并初始化变量
- en: ❷ Runs the learning operation multiple times
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 多次运行学习操作
- en: ❸ Records the cost computed with the current parameters
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 记录使用当前参数计算的成本
- en: ❹ Prints log info while the code runs
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在代码运行时打印日志信息
- en: ❺ Prints the learned parameters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印学习到的参数
- en: ❻ Closes the session when no longer in use
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 当不再使用时关闭会话
- en: ❼ Shows the best-fit line
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 显示最佳拟合线
- en: 'To measure success, you can count the number of correct predictions and compute
    a success rate. In listing 5.3, you add two more nodes to the previous code in
    linear.py: `correct_prediction` and `accuracy`. Then you can print the value of
    `accuracy` to see the success rate. The code can be executed right before closing
    the session.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 要衡量成功，你可以计算正确预测的数量并计算成功率。在列表 5.3 中，你向之前的 linear.py 代码中添加了两个节点：`correct_prediction`
    和 `accuracy`。然后你可以打印 `accuracy` 的值来查看成功率。代码可以在关闭会话之前执行。
- en: Listing 5.3 Measuring accuracy
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 测量准确率
- en: '[PRE2]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ When the model’s response is greater than 0.5, it should be a positive label,
    and vice versa.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当模型的响应大于 0.5 时，它应该是一个正标签，反之亦然。
- en: ❷ Computes the percent of success
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算成功率
- en: ❸ Prints the success measure from provided input
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印从提供的输入中得出的成功度量
- en: ❹ Prints the correct predictions
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印正确的预测
- en: 'The code in listing 5.3 produces the following output:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.3 中的代码产生了以下输出：
- en: '[PRE3]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If classification were that easy, this chapter would be over by now. Unfortunately,
    the linear regression approach fails miserably if you train on more-extreme data,
    also called *outliers*.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果分类那么简单，这一章现在就已经结束了。不幸的是，如果你在更极端的数据（也称为 *异常值*）上训练，线性回归方法会彻底失败。
- en: Suppose that Alice lost a game that took 20 minutes. You train the classifier
    on a dataset that includes this new outlier data point. Listing 5.4 replaces one
    of the game times with the value of `20`. Let’s see how introducing an outlier
    affects the classifier’s performance.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 Alice 失去了一场耗时 20 分钟的游戏。你在一个包含这个新异常数据点的数据集上训练分类器。列表 5.4 将游戏时间中的一个值替换为 `20`。让我们看看引入异常值如何影响分类器的性能。
- en: Listing 5.4 Linear regression failing miserably for classification
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 线性回归在分类中彻底失败
- en: '[PRE4]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When you rerun the code with these changes, you see a result similar to figure
    5.10.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当你使用这些更改重新运行代码时，你会看到一个类似于图 5.10 的结果。
- en: '![CH05_F10_Mattmann2](../Images/CH05_F10_Mattmann2.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Mattmann2](../Images/CH05_F10_Mattmann2.png)'
- en: Figure 5.10 A new training element of value `20` greatly influences the best-fit
    line. The line is too sensitive to outlying data; therefore, linear regression
    is a sloppy classifier.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 新的训练元素值 `20` 对最佳拟合线有重大影响。该线对异常值数据过于敏感；因此，线性回归是一个粗略的分类器。
- en: The original classifier suggested that you could beat Alice in a three-minute
    game. She’d probably agree to play such a short game. But the revised classifier,
    if you stick with the same `0.5` threshold, is suggesting that the shortest game
    she’ll lose is a five-minute game. She’ll likely refuse to play such a long game!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 原始分类器建议你可以在三分钟的游戏中打败 Alice。她可能会同意玩这样短的游戏。但修订后的分类器，如果你坚持相同的 `0.5` 阈值，则暗示她最短会输掉的游戏是五分钟的。她可能会拒绝玩这么长时间的游戏！
- en: 5.4 Using logistic regression
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 使用逻辑回归
- en: Logistic regression provides an analytic function with theoretical guarantees
    on accuracy and performance. It’s like linear regression, except that you use
    a different cost function and slightly transform the model response function.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归提供了一个具有关于准确性和性能的理论保证的分析函数。它类似于线性回归，只是你使用不同的代价函数并对模型响应函数进行轻微变换。
- en: 'Let’s revisit the linear function shown here:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们重新审视这里显示的线性函数：
- en: '*y* (*x) = wx*'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* (*x) = wx*'
- en: In linear regression, a line with a nonzero slope may range from negative infinity
    to infinity. If the only sensible result for classification is `0` or `1`, it
    would be intuitive to fit a function with that property instead. Fortunately,
    the sigmoid function depicted in figure 5.11 works well because it converges to
    `0` or `1` quickly.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在线性回归中，具有非零斜率的线可能从负无穷大到无穷大。如果分类的唯一合理结果是 `0` 或 `1`，那么拟合具有该特性的函数将是直观的。幸运的是，图 5.11
    中描绘的 Sigmoid 函数迅速收敛到 `0` 或 `1`。
- en: '![CH05_F11_Mattmann2](../Images/CH05_F11_Mattmann2.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F11_Mattmann2](../Images/CH05_F11_Mattmann2.png)'
- en: Figure 5.11 A visualization of the sigmoid function
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 Sigmoid 函数的可视化
- en: When `x` is `0`, the sigmoid function results in `0.5`. As `x` increases, the
    function converges to `1`. And as `x` decreases to negative infinity, the function
    converges to `0`.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `x` 为 `0` 时，Sigmoid 函数的结果为 `0.5`。随着 `x` 的增加，函数收敛到 `1`。而当 `x` 减少到负无穷大时，函数收敛到
    `0`。
- en: In logistic regression, our model is `sig(linear(``x``))`. As it turns out,
    the best-fit parameters of this function imply a linear separation between the
    two classes. This separating line is also called a *linear decision boundary*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中，我们的模型是 `sig(linear(``x``))`。结果证明，该函数的最佳拟合参数暗示了两个类别之间的线性分离。这条分离线也称为 *线性决策边界*。
- en: 5.4.1 Solving 1D logistic regression
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 解决一维逻辑回归
- en: The cost function used in logistic regression is a bit different from the one
    you used in linear regression. Although you could use the same cost function as
    before, it won’t be as fast or guarantee an optimal solution. The sigmoid function
    is the culprit here, because it causes the cost function to have many “bumps.”
    TensorFlow and most other machine-learning libraries work best with simple cost
    functions. Scholars have found a neat way to modify the cost function to use sigmoids
    for logistic regression.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在逻辑回归中使用的成本函数与你在线性回归中使用的不同。虽然你可以使用之前相同的成本函数，但它不会那么快，也不能保证得到最优解。这里的罪魁祸首是sigmoid函数，因为它使得成本函数有很多“峰值”。TensorFlow和大多数其他机器学习库与简单的成本函数配合得最好。学者们已经找到了一种巧妙的方法来修改成本函数，以便在逻辑回归中使用sigmoid函数。
- en: 'The new cost function between the actual value (`y`) and model response (`h`)
    will be a two-part equation:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 实际值(`y`)和模型响应(`h`)之间新的成本函数将是一个两部分方程：
- en: '![CH05_F11EQ04_Mattmann2](../Images/CH05_F11EQ04_Mattmann2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F11EQ04_Mattmann2](../Images/CH05_F11EQ04_Mattmann2.png)'
- en: 'You can condense the two equations into one long equation:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将这两个方程压缩成一个长方程：
- en: '*Cost*(*y, h*) = *–y*log(*h*) – (1 – *y*)log(1 – *h*)'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*Cost*(*y, h*) = *–y*log(*h*) – (1 – *y*)log(1 – *h*)'
- en: 'This function has exactly the qualities needed for efficient and optimal learning.
    Specifically, it’s convex, but don’t worry too much about what that means. You’re
    trying to minimize the cost: think of cost as an altitude and the cost function
    as a terrain. You’re trying to find the lowest point in the terrain. It’s a lot
    easier to find the lowest point in the terrain if there’s no place you can ever
    go uphill. Such a place is called *convex*. There are no hills.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数恰好具有高效和最优学习的所需品质。具体来说，它是凸的，但不必过于担心这意味着什么。你试图最小化成本：把成本想成高度，把成本函数想成地形。你试图找到地形中的最低点。如果没有任何地方可以向上爬，那么在地形中找到最低点会容易得多。这样的地方被称为*凸性*。这里没有山丘。
- en: You can think of this function as being like a ball rolling down a hill. Eventually,
    the ball will settle to the bottom, which is the *optimal point*. A nonconvex
    function might have a rugged terrain, making it difficult to predict where a ball
    will roll. It might not even end up at the lowest point. Your function is convex,
    so the algorithm will easily figure out how to minimize this cost and “roll the
    ball downhill.”
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以把这个函数想象成一个球从山上滚下来。最终，球会落在底部，这是*最优点*。非凸函数可能有一片崎岖的地形，这使得很难预测球会滚到哪里。它甚至可能不会落在最低点。你的函数是凸的，所以算法会很容易地找出如何最小化这个成本并将“球滚下山”。
- en: 'Convexity is nice, but correctness is also an important criterion when picking
    a cost function. How do you know this cost function does exactly what you intended
    it to do? To answer that question most intuitively, take a look at figure 5.12\.
    You use `-log(``x``)` to compute the cost when you want your desired value to
    be `1` (`notice: -log(1) = 0`). The algorithm strays away from setting the value
    to 0, because the cost approaches infinity. Adding these functions together gives
    a curve that approaches infinity at both `0` and `1`, with the negative parts
    canceling out.'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 凸性很好，但在选择成本函数时，正确性也是一个重要的标准。你怎么知道这个成本函数确实做了你想要它做的事情呢？为了直观地回答这个问题，请看图5.12。当你希望你的期望值为`1`时，你使用`-log(x)`来计算成本（注意：-log(1)
    = 0）。算法不会将值设置为0，因为成本会趋向于无穷大。将这些函数相加得到一个曲线，在`0`和`1`处都趋向于无穷大，负的部分相互抵消。
- en: '![CH05_F12_Mattmann2](../Images/CH05_F12_Mattmann2.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F12_Mattmann2](../Images/CH05_F12_Mattmann2.png)'
- en: Figure 5.12 Here’s a visualization of how the two cost functions penalize values
    at `0` and `1`. Notice that the left function penalizes `0` heavily but has no
    cost at `1`. The right cost function displays the opposite phenomena.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 这里展示了两个成本函数如何惩罚`0`和`1`处的值。注意，左边的函数对`0`的惩罚很重，但在`1`处没有成本。右边的成本函数显示了相反的现象。
- en: Sure, figures are an informal way to persuade you of the importance of convexity
    in picking a cost function, but a technical discussion of why the cost function
    is optimal is beyond the scope of this book. If you’re interested in the mathematics,
    you’ll be interested to learn that the cost function is derived from the principle
    of maximum entropy, which you can look up anywhere online.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，图表是一种非正式的方式来说服你选择成本函数时凸性的重要性，但关于为什么成本函数是最优的技术的讨论超出了本书的范围。如果你对数学感兴趣，你可能会对学习到成本函数是从最大熵原理推导出来的感到兴趣，你可以在网上任何地方查找相关信息。
- en: See figure 5.13 for a best-fit result from logistic regression on a 1D dataset.
    The sigmoid curve that you’ll generate will provide a better linear decision boundary
    than that from linear regression.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅图5.13，以查看一维数据集上逻辑回归的最佳拟合结果。你生成的sigmoid曲线将提供比线性回归更好的线性决策边界。
- en: '![CH05_F13_Mattmann2](../Images/CH05_F13_Mattmann2.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F13_Mattmann2](../Images/CH05_F13_Mattmann2.png)'
- en: Figure 5.13 Here’s a best-fit sigmoid curve for a binary classification dataset.
    Notice that the curve resides within `y` `=` `0` and `y` `=` `1`. That way, this
    curve isn’t too sensitive to outliers.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 这里是一个二元分类数据集的最佳拟合sigmoid曲线。注意曲线位于`y` `=` `0`和`y` `=` `1`之间。这样，这条曲线对异常值不太敏感。
- en: You’ll start to notice a pattern in the code listings. In simple/typical use
    of TensorFlow, you generate a fake dataset, define placeholders, define variables,
    define a model, define a cost function on that model (which is often a mean squared
    error or a mean squared log error), create a `train_op` by using gradient descent,
    iteratively feed it example data (possibly with a label or output), and finally
    collect the optimized values. Create a new source file called logistic_1d.py,
    and copy into it listing 5.5, which generates figure 5.13.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 你会开始注意到代码列表中的模式。在TensorFlow的简单/典型使用中，你生成一个假数据集，定义占位符，定义变量，定义模型，在该模型上定义一个成本函数（通常是均方误差或均方对数误差），通过使用梯度下降创建`train_op`，迭代地提供示例数据（可能带有标签或输出），并最终收集优化值。创建一个新的源文件名为logistic_1d.py，并将列表5.5复制到其中，它生成了图5.13。
- en: Listing 5.5 Using 1D logistic regression
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5 使用一维逻辑回归
- en: '[PRE5]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Imports relevant libraries
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入相关库
- en: ❷ Sets the hyperparameters
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置超参数
- en: ❸ Defines a helper function to calculate the sigmoid function
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个辅助函数来计算sigmoid函数
- en: ❹ Initializes fake data
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化假数据
- en: ❺ Visualizes the data
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 可视化数据
- en: ❻ Defines the input/output placeholders
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义输入/输出占位符
- en: ❼ Defines the parameter node
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义参数节点
- en: ❽ Defines the model by using TensorFlow’s sigmoid function
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 使用TensorFlow的sigmoid函数定义模型
- en: ❾ Defines the cross-entropy loss function
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 定义交叉熵损失函数
- en: ❿ Defines the minimizer to use
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 定义要使用的最小化器
- en: ⓫ Opens a session and defines all variables
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 打开一个会话并定义所有变量
- en: ⓬ Defines a variable to keep track of the previous error
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 定义一个变量来跟踪前一个错误
- en: ⓭ Iterates until convergence or until the maximum number of epochs is reached
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ⓭ 迭代直到收敛或达到最大epoch数
- en: ⓮ Computes the cost and updates the learning parameters
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ⓮ 计算成本并更新学习参数
- en: ⓯ Checks for convergence. If you’re changing by < .01% per iteration, you’re
    done.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ⓯ 检查收敛性。如果你每次迭代的改变小于< .01%，则完成。
- en: ⓰ Updates the previous error value
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ⓰ 更新前一个错误值
- en: ⓱ Obtains the learned parameter value
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ⓱ 获取学习到的参数值
- en: ⓲ Plots the learned sigmoid function
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ⓲ 绘制学习到的sigmoid函数
- en: And there you have it! If you were playing chess against Alice, you’d now have
    a binary classifier to decide the threshold indicating when a chess match might
    result in a win or loss.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！如果你在与Alice下棋，你现在将有一个二元分类器来决定何时棋局可能赢得或输掉。
- en: Cross-entropy loss in TensorFlow
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow中的交叉熵损失
- en: As shown in listing 5.5, the cross-entropy loss is averaged over each input/output
    pair by using the `tf.reduce_mean` op. Another handy and more general function
    is provided by the TensorFlow library, called `tf.nn.softmax_cross_entropy_with_logits`[.
    You can find more about it in the official documentation at](http://mng.bz/8mEk)
    http://mng.bz/8mEk.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表5.5所示，使用`tf.reduce_mean`操作对每个输入/输出对进行交叉熵损失的求平均值。TensorFlow库还提供了一个方便且更通用的函数，称为`tf.nn.softmax_cross_entropy_with_logits`[。你可以在官方文档中了解更多信息，链接为](http://mng.bz/8mEk)
    http://mng.bz/8mEk。
- en: 5.4.2 Solving 2D regression
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 解决二维回归
- en: Now we’ll explore how to use logistic regression with multiple independent variables.
    The number of independent variables corresponds to the number of dimensions. In
    our case, a 2D logistic regression problem will try to label a pair of independent
    variables. The concepts you learn in this section extrapolate to arbitrary dimensions.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将探索如何使用多个独立变量进行逻辑回归。独立变量的数量对应于维度数。在我们的情况下，二维逻辑回归问题将尝试标记一对独立变量。本节中你学到的概念可以推广到任意维度。
- en: NOTE Suppose that you’re thinking about buying a new phone. The only attributes
    you care about are (1) operating system, (2) size, and (3) cost. The goal is to
    decide whether a phone is a worthwhile purchase. In this case, there are three
    independent variables (the attributes of the phone) and one dependent variable
    (whether it’s worth buying). So we regard this problem as a classification problem
    in which the input vector is 3D.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：假设你在考虑购买一部新手机。你唯一关心的属性是（1）操作系统，（2）尺寸和（3）成本。目标是决定一部手机是否值得购买。在这种情况下，有三个独立变量（手机的属性）和一个因变量（是否值得购买）。因此，我们将这个问题视为一个分类问题，其中输入向量是
    3D。
- en: Consider the dataset shown in figure 5.14, which represents the crime activity
    of two gangs in a city. The first dimension is the x-axis, which can be thought
    of as the latitude, and the second dimension is the y-axis, representing longitude.
    There’s one cluster around (3, 2) and another around (7, 6). Your job is to decide
    which gang is most likely responsible for a new crime that occurred at location
    (6, 4).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑图 5.14 所示的数据集，它代表了一个城市中两个帮派的犯罪活动。第一维是 x 轴，可以认为是纬度，第二维是 y 轴，代表经度。在 (3, 2) 附近有一个簇，在
    (7, 6) 附近还有一个簇。你的任务是决定哪个帮派最有可能对发生在位置 (6, 4) 的新犯罪负责。
- en: '![CH05_F14_Mattmann2](../Images/CH05_F14_Mattmann2.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F14_Mattmann2](../Images/CH05_F14_Mattmann2.png)'
- en: Figure 5.14 The x-axis and y-axis represent the two independent variables. The
    dependent variable holds two possible labels, represented by the shape and color
    of the plotted points.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.14 x 轴和 y 轴代表两个独立变量。因变量持有两个可能的标签，由绘制点的形状和颜色表示。
- en: Another way to visualize figure 5.14 is to project the independent variables
    `x=latitude` and `y=longitude` as a 2D plane and then draw the vertical axis as
    the result of the sigmoid function for the one-hot-encoded categories. You could
    visualize the function as shown in figure 5.15.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化图 5.14 的方法是，将独立变量 `x=latitude` 和 `y=longitude` 投影为 2D 平面，然后绘制垂直轴作为 sigmoid
    函数的结果，该函数代表了一热编码的类别。你可以将函数可视化如图 5.15 所示。
- en: '![CH05_F15_Mattmann2](../Images/CH05_F15_Mattmann2.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F15_Mattmann2](../Images/CH05_F15_Mattmann2.png)'
- en: Figure 5.15 Another way to visualize the two independent variables, this time
    taking into account the dependent variable defined by the sigmoid function, which
    represents the one-hot-encoded classes for gang 1 (green) and gang 2 (red).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.15 另一种可视化两个独立变量的方法，这次考虑了由 sigmoid 函数定义的因变量，该函数代表帮派 1（绿色）和帮派 2（红色）的一热编码类别。
- en: Create a new source file called logistic_2d.py, and follow along with listing
    5.6.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 logistic_2d.py 的新源文件，并按照列表 5.6 进行操作。
- en: Listing 5.6 Setting up data for 2D logistic regression
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.6 设置 2D 逻辑回归数据
- en: '[PRE6]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Imports relevant libraries
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入相关库
- en: ❷ Sets the hyperparameters
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置超参数
- en: ❸ Defines a helper sigmoid function
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个辅助 sigmoid 函数
- en: ❹ Initializes fake data
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 初始化假数据
- en: 'You have two independent variables (`x[1]` and `x[2]`). A simple way to model
    the mapping between the input `x`’s and output `M(x)` is the following equation,
    where `w` is the parameter to be found with TensorFlow:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 你有两个独立变量 (`x[1]` 和 `x[2]`)。一个简单的方式来建模输入 `x` 和输出 `M(x)` 之间的映射是以下方程，其中 `w` 是 TensorFlow
    要找到的参数：
- en: '*M*(*x, v*) = *sig*(*w[2]x[2]* + *w[1]x[1]* + *w[0]*)'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '*M*(*x, v*) = *sig*(*w[2]x[2]* + *w[1]x[1]* + *w[0]*)'
- en: In listing 5.7, you’ll implement the equation and its corresponding cost function
    to learn the parameters.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 5.7 中，你将实现方程及其相应的成本函数来学习参数。
- en: Listing 5.7 Using TensorFlow for multidimensional logistic regression
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.7 使用 TensorFlow 进行多维逻辑回归
- en: '[PRE7]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Defines the input/output placeholder nodes
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义输入/输出占位符节点
- en: ❷ Defines the parameter node
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义参数节点
- en: ❸ Defines the sigmoid model, using both input variables
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义 sigmoid 模型，使用两个输入变量
- en: ❹ Defines the learning step
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义学习步长
- en: ❺ Creates a new session, initializes variables, and learns parameters until
    convergence
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建一个新的会话，初始化变量，并学习参数直到收敛
- en: ❻ Obtains the learned parameter value before closing the session
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 在关闭会话之前获取学习到的参数值
- en: ❼ Defines arrays to hold boundary points
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 定义用于存储边界点的数组
- en: ❽ Loops through a window of points
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 遍历点窗口
- en: ❾ If the model response is close to 0.5, updates the boundary points
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 如果模型响应接近 0.5，则更新边界点
- en: ❿ Shows the boundary line along with the data
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 显示边界线以及数据
- en: Figure 5.16 depicts the linear boundary line learned from the training data.
    A crime that occurs on this line has an equal chance of being committed by either
    gang.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 描述了从训练数据中学习到的线性边界线。发生在这条线上的犯罪事件，被两个帮派犯下的可能性是相等的。
- en: '![CH05_F16_Mattmann2](../Images/CH05_F16_Mattmann2.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F16_Mattmann2](../Images/CH05_F16_Mattmann2.png)'
- en: Figure 5.16 The diagonal dotted line represents when the probability between
    the two decisions is split equally. Confidence in a decision increases as data
    lies farther from the line.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 对角虚线表示两个决策之间的概率平均分割。当数据点远离该线时，决策的信心增加。
- en: 5.5 Multiclass classifier
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 多类分类器
- en: So far, you’ve dealt with multidimensional input but not with multivariate output,
    shown in figure 5.17\. Instead of binary labels on the data, what if you have
    3, 4, or 100 classes? Logistic regression requires two labels—no more.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经处理了多维输入，但没有处理多变量输出，如图 5.17 所示。在数据上不是二进制标签，而是有 3、4 或 100 个类别怎么办？逻辑回归需要两个标签——不再多了。
- en: '![CH05_F17_Mattmann2](../Images/CH05_F17_Mattmann2.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F17_Mattmann2](../Images/CH05_F17_Mattmann2.png)'
- en: Figure 5.17 The independent variable is 2D, indicated by the x-axis and y-axis.
    The dependent variable can be one of three labels, shown by the color and shape
    of the data points.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.17 独立变量是二维的，由 x 轴和 y 轴表示。因变量可以是三个标签之一，由数据点的颜色和形状表示。
- en: Image classification, for example, is a popular multivariate classification
    problem because the goal is to decide the class of an image from a collection
    of candidates. A photograph may be bucketed into one of hundreds of categories.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图像分类是一个流行的多变量分类问题，因为目标是决定图像的类别。一张照片可能被归类到数百个类别之一。
- en: To handle more than two labels, you may reuse logistic regression in a clever
    way (using a one-versus-all or one-versus-one approach) or develop a new approach
    (softmax regression). We look at each of the approaches in the following sections.
    The logistic regression approaches require a decent amount of ad hoc engineering,
    so let’s focus on softmax regression.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要处理超过两个标签，你可以巧妙地重复使用逻辑回归（使用一对多或一对一方法）或开发一种新的方法（softmax 回归）。我们将在以下各节中查看每种方法。逻辑回归方法需要相当多的临时工程，所以让我们专注于
    softmax 回归。
- en: 5.5.1 One-versus-all
  id: totrans-224
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 一对多
- en: 'First, you train a classifier for each of the labels, as shown in figure 5.18\.
    If there are three labels, you have three classifiers available to use: `f1`,
    `f2`, and `f3`. To test on new data, you run each of the classifiers to see which
    one produces the most confident response. Intuitively, you label the new point
    by the label of the classifier that responded most confidently.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为每个标签训练一个分类器，如图 5.18 所示。如果有三个标签，你就有三个可用的分类器：`f1`、`f2` 和 `f3`。要在新数据上进行测试，运行每个分类器以查看哪个分类器产生了最自信的响应。直观地说，通过响应最自信的分类器的标签来标记新的点。
- en: '![CH05_F18_Mattmann2](../Images/CH05_F18_Mattmann2.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F18_Mattmann2](../Images/CH05_F18_Mattmann2.png)'
- en: Figure 5.18 One-versus-all is a multiclass classifier approach that requires
    a detector for each class.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.18 一对多是一种多类分类方法，它需要每个类别的检测器。
- en: 5.5.2 One-versus-one
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 一对一
- en: Then you train a classifier for each pair of labels (see figure 5.19). If there
    are three labels, that’s three unique pairs. But for *k* number of labels, that’s
    *k*(*k* - 1)/2 pairs of labels. On new data, you run all the classifiers and choose
    the class with the most wins.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，为每对标签训练一个分类器（见图 5.19）。如果有三个标签，那就是三个独特的对。但对于 *k* 个标签，那就是 *k*(*k* - 1)/2 对标签。在新数据上，运行所有分类器并选择获胜最多的类别。
- en: '![CH05_F19_Mattmann2](../Images/CH05_F19_Mattmann2.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F19_Mattmann2](../Images/CH05_F19_Mattmann2.png)'
- en: Figure 5.19 In one-versus-one multiclass classification, there’s a detector
    for each pair of classes.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.19 在一对一多类分类中，每个类别对都有一个检测器。
- en: 5.5.3 Softmax regression
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 Softmax 回归
- en: '*Softmax* *regression* is named after the traditional `max` function, which
    takes a vector and returns the max value. But softmax isn’t exactly the `max`
    function, because it has the added benefit of being continuous and differentiable.
    As a result, it has the helpful properties for stochastic gradient descent to
    work efficiently.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: '*Softmax* *回归*是以传统的 `max` 函数命名的，它接受一个向量并返回最大值。但 softmax 并不是 `max` 函数，因为它具有连续性和可微分的额外优势。因此，它具有对随机梯度下降有效工作的有益特性。'
- en: In this type of multiclass classification setup, each class has a confidence
    (or probability) score for each input vector. The softmax step picks the highest-scoring
    output.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种多类分类设置中，每个类别对每个输入向量都有一个置信度（或概率）分数。softmax步骤选择得分最高的输出。
- en: Open a new file called softmax.py, and follow along with listing 5.8\. First,
    you’ll visualize fake data to reproduce figure 5.17 (also reproduced here in figure
    5.20)
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个名为softmax.py的新文件，并按照列表5.8进行操作。首先，你将可视化假数据以重现图5.17（也在此处重现为图5.20）
- en: '![CH05_F20_Mattmann2](../Images/CH05_F20_Mattmann2.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F20_Mattmann2](../Images/CH05_F20_Mattmann2.png)'
- en: Figure 5.20 2D training data for multi-output classification
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 多输出分类的2D训练数据
- en: Listing 5.8 Visualizing multiclass data
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8 可视化多类数据
- en: '[PRE8]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Imports NumPy and Matplotlib
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入NumPy和Matplotlib
- en: ❷ Generates points near (1, 1)
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成靠近（1, 1）的点
- en: ❸ Generates points near (5, 4)
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成靠近（5, 4）的点
- en: ❹ Generates points near (8, 0)
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 生成靠近（8, 0）的点
- en: ❺ Visualizes the three labels on a scatter plot
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在散点图上可视化三个标签
- en: 'Next, in listing 5.9, you set up the training and test data to prepare for
    the softmax regression step. The labels must be represented as a vector in which
    only one element is 1 and the rest are 0s. This representation is called *one-hot
    encoding*. If there are three labels, they’d be represented as the following vectors:
    [1, 0, 0], [0, 1, 0], and [0, 0, 1].'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在列表5.9中，你设置了训练和测试数据，为softmax回归步骤做准备。标签必须表示为一个向量，其中只有一个元素是1，其余都是0。这种表示称为*one-hot编码*。如果有三个标签，它们将表示为以下向量：[1,
    0, 0]，[0, 1, 0]，和[0, 0, 1]。
- en: Exercise 5.5
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.5
- en: One-hot encoding might appear to be an unnecessary step. Why not have 1D output
    with values of `1`, `2`, and `3` representing the three classes?
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot编码可能看起来是一个不必要的步骤。为什么不使用1D输出，其中值为`1`、`2`和`3`来表示三个类别呢？
- en: '**Answer**'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Regression may induce a semantic structure in the output. If outputs are similar,
    regression implies that their inputs were also similar. If you use one dimension,
    you’re implying that labels 2 and 3 are more similar to each other than 1 and
    3\. You must be careful about making unnecessary or incorrect assumptions, so
    it’s a safe bet to use one-hot encoding.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 回归可能在输出中诱导出语义结构。如果输出相似，回归意味着它们的输入也相似。如果你使用一个维度，你就是在暗示标签2和3比1和3更相似。你必须小心不要做出不必要的或错误的假设，因此使用one-hot编码是一个安全的赌注。
- en: Listing 5.9 Setting up training and test data for multiclass classification
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 设置多类分类的训练和测试数据
- en: '[PRE9]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Combines all input data into one big matrix
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将所有输入数据合并到一个大矩阵中
- en: ❷ Creates the corresponding one-hot labels
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建相应的one-hot标签
- en: ❸ Shuffles the dataset
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打乱数据集
- en: ❹ Constructs the test dataset and labels
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 构建测试数据集和标签
- en: ❺ The shape of the dataset tells you the number of examples and features per
    example.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 数据集的形状告诉你每个示例的示例数量和特征数量。
- en: 'You see the use of the `hstack` and `vstack` methods in listing 5.9, corresponding
    to horizontal stack and vertical stack, respectively—two functions available from
    the NumPy library. The `hstack` function takes arrays and stacks them in sequence
    horizontally (column-wise), and the `vstack` function takes arrays and stacks
    them in sequence vertically (row-wise). As an example, `x1_label0` and `x2_label0`
    from listing 5.8 look similar to this when printed:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 你在列表5.9中看到了`hstack`和`vstack`方法的用法，分别对应水平堆叠和垂直堆叠——这两个函数来自NumPy库。`hstack`函数接收数组并将它们按顺序水平（列向）堆叠，而`vstack`函数接收数组并将它们按顺序垂直（行向）堆叠。例如，列表5.8中的`x1_label0`和`x2_label0`打印出来时看起来像这样：
- en: '[PRE10]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The resulting value of the `xs_label0` variable would look something like this:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '`xs_label0`变量的结果值可能看起来像这样：'
- en: '[PRE11]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Finally, in listing 5.10, you use softmax regression. Unlike the sigmoid function
    in logistic regression, here you use the `softmax` function provided by the TensorFlow
    library. The `softmax` function is similar to the `max` function, which outputs
    the maximum value from a list of numbers. It’s called *softmax* because it’s a
    “soft” or “smooth” approximation of the `max` function, which is not smooth or
    continuous (and that’s bad). Continuous and smooth functions facilitate learning
    the correct weights of a neural network by backpropagation.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在列表5.10中，你使用了softmax回归。与逻辑回归中的sigmoid函数不同，这里你使用了TensorFlow库提供的`softmax`函数。`softmax`函数类似于`max`函数，它从一系列数字中输出最大值。它被称为*softmax*，因为它是对`max`函数的“软”或“平滑”近似，而`max`函数是不平滑或不连续的（这是不好的）。连续且平滑的函数通过反向传播促进了神经网络正确权重的学习。
- en: Exercise 5.6
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 练习5.6
- en: Which of the following functions is continuous?
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下哪个函数是连续的？
- en: '[PRE12]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Answer**'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: The first two are continuous. The last one, `tan(x)`, has periodic asymptotes,
    so there are some values for which there are no valid results.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 前两个是连续的。最后一个 `tan(x)` 有周期性渐近线，因此对于某些值没有有效的结果。
- en: Listing 5.10 Using softmax regression
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 使用 softmax 回归
- en: '[PRE13]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Defines hyperparameters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义超参数
- en: ❷ Defines the input/output placeholder nodes
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义输入/输出占位符节点
- en: ❸ Defines the model parameters
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义模型参数
- en: ❹ Designs the softmax model
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设计 softmax 模型
- en: ❺ Sets up the learning algorithm
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 设置学习算法
- en: ❻ Defines an op to measure success rate
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 定义一个操作来测量成功率
- en: Now that you’ve defined the TensorFlow computation graph, execute it from a
    session. You’ll try a new form of iteratively updating the parameters this time,
    called *batch learning*. Instead of passing in the data one piece at a time, you’ll
    run the optimizer on batches of data. This technique speeds things but introduces
    a risk of converging to a local optimum solution instead of the global best. Use
    listing 5.11 to run the optimizer in batches.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经定义了 TensorFlow 计算图，从会话中执行它。这次你将尝试一种新的迭代更新参数的形式，称为 *批量学习*。你不会一次只传递一个数据片段，而是会对数据批次运行优化器。这种技术可以加快速度，但引入了收敛到局部最优解而不是全局最优解的风险。使用列表
    5.11 以批量运行优化器。
- en: Listing 5.11 Executing the graph
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 执行图
- en: '[PRE14]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Opens a new session and initializes all variables
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打开一个新的会话并初始化所有变量
- en: ❷ Loops only enough times to complete a single pass through the dataset
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅循环足够次数以完成对数据集的单次遍历
- en: ❸ Retrieves a subset of the dataset corresponding to the current batch
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取与当前批次对应的数据集子集
- en: ❹ Runs the optimizer on this batch
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在这个批次上运行优化器
- en: ❺ Prints ongoing results
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 打印持续结果
- en: ❻ Prints the final learned parameters
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 打印最终的学到的参数
- en: ❼ Prints the success rate
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印成功率
- en: The final output of running the softmax regression algorithm on the dataset
    is
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集上运行 softmax 回归算法的最终输出是
- en: '[PRE15]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: You’ve learned the weights and biases of the model. You can reuse these learned
    parameters to infer on test data. A simple way to do so is to save and load the
    variables by using TensorFlow’s `Saver` object (see [https://www.tensorflow.org/guide/saved_
    model](https://www.tensorflow.org/guide/saved_model)). You can run the model (called
    `y_model` in our code) to obtain the model responses on your test data.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学会了模型的权重和偏差。你可以使用这些学习到的参数来对测试数据进行推断。这样做的一个简单方法是使用 TensorFlow 的 `Saver` 对象（见
    [https://www.tensorflow.org/guide/saved_model](https://www.tensorflow.org/guide/saved_model)）保存和加载变量。你可以运行模型（在我们的代码中称为
    `y_model`）以获得测试数据上的模型响应。
- en: 5.6 Application of classification
  id: totrans-288
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 分类应用
- en: Emotion is a difficult concept to operationalize. Happiness, sadness, anger,
    excitement, and fear are examples of emotions that are subjective. What comes
    across as exciting to one person might appear sarcastic to another. Text that
    appears to convey anger to some people might convey fear to others. If humans
    have so much trouble, what luck can computers have?
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 情感是一个难以量化的概念。快乐、悲伤、愤怒、兴奋和恐惧是主观性情感的例子。对某个人来说可能令人兴奋的事情，对另一个人来说可能显得讽刺。对一些人来说似乎传达愤怒的文本，对其他人来说可能传达恐惧。如果人类都有这么多麻烦，那么计算机又能有什么运气呢？
- en: At the very least, machine-learning researchers have figured out ways to classify
    positive and negative sentiments within text. Suppose that you’re building an
    Amazon-like website on which each item has user reviews. You want your intelligent
    search engine to prefer items with positive reviews. Perhaps the best metric you
    have available is the average star rating or number of thumbs ups. But what if
    you have a lot of heavy-text reviews without explicit ratings?
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 至少，机器学习研究人员已经找到了在文本中分类正面和负面情感的方法。假设你正在构建一个类似亚马逊的网站，每个商品都有用户评论。你希望你的智能搜索引擎优先选择正面评论的商品。你可能拥有的最佳指标是平均星级评分或点赞数量。但如果你有很多没有明确评分的重文本评论怎么办呢？
- en: 'Sentiment analysis can be considered to be a binary classification problem.
    The input is natural language text, and the output is a binary decision that infers
    positive or negative sentiment. Following are datasets you can find online to
    solve this exact problem:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析可以被视为一个二元分类问题。输入是自然语言文本，输出是一个二元决策，推断出正面或负面情感。以下是一些你可以在线找到的用于解决这个确切问题的数据集：
- en: Large Movie Review Dataset—[http://mng.bz/60nj](http://mng.bz/60nj)
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大型电影评论数据集—[http://mng.bz/60nj](http://mng.bz/60nj)
- en: Sentiment Labelled Sentences Data Set—[http://mng.bz/CzSM](http://mng.bz/CzSM)
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 情感标注句子数据集—[http://mng.bz/CzSM](http://mng.bz/CzSM)
- en: Twitter Sentiment Analysis Dataset—[http://mng.bz/2M4d](http://mng.bz/2M4d)
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Twitter情感分析数据集—[http://mng.bz/2M4d](http://mng.bz/2M4d)
- en: The biggest hurdle is to figure out how to represent raw text as an input to
    a classification algorithm. Throughout this chapter, the input to classification
    has always been a feature vector. One of the oldest methods of converting raw
    text into a feature vector is called Bag of Words. You can find a nice tutorial
    and code implementation for it at [http://mng.bz/K8yz](http://mng.bz/K8yz).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的挑战是如何将原始文本表示为分类算法的输入。在整个这一章中，分类的输入始终是一个特征向量。将原始文本转换为特征向量的最古老的方法之一被称为词袋模型。你可以在[http://mng.bz/K8yz](http://mng.bz/K8yz)找到关于它的良好教程和代码实现。
- en: Summary
  id: totrans-296
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: There are many ways to solve classification problems, but logistic regression
    and softmax regression are two of the most robust in terms of accuracy and performance.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决分类问题有许多方法，但在准确性和性能方面，逻辑回归和softmax回归是最稳健的两种。
- en: It’s important to preprocess data before running classification. Discrete independent
    variables can be readjusted into binary variables, for example.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在运行分类之前预处理数据是很重要的。例如，离散独立变量可以被调整为二元变量。
- en: So far, you’ve approached classification from the point of view of regression.
    In later chapters, you’ll revisit classification by using neural networks.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到目前为止，你都是从回归的角度来处理分类的。在后面的章节中，你将通过使用神经网络重新审视分类。
- en: 'There are various ways to approach multiclass classification. There’s no clear
    answer as to which one you should try first: one-versus-one, one-versus-all, or
    softmax regression. But the softmax approach is a little more hands-free and allows
    you to fiddle with hyperparameters.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有多种方法可以处理多类分类。没有明确的答案说明你应该先尝试哪一种：一对多、多对一，还是softmax回归。但softmax方法稍微更少需要手动操作，并允许你调整超参数。
