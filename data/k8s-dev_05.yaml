- en: 4 Automated operations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 自动化操作
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating long-lasting, reliable application deployments
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建持久、可靠的部署
- en: Having Kubernetes keep your applications running without your intervention
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 让Kubernetes在无需您干预的情况下保持应用运行
- en: Updating applications without downtime
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无停机更新应用
- en: Kubernetes can automate many operations, like restarting your container if it
    crashes and migrating your application in the case of hardware failure. Thus,
    Kubernetes helps to make your deployments more reliable without you needing to
    monitor them 24/7\. These automated operations are one of the key value propositions
    of Kubernetes, and understanding them is an essential step to taking full advantage
    of everything Kubernetes has to offer.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以自动化许多操作，如容器崩溃时重启容器，以及在硬件故障的情况下迁移应用。因此，Kubernetes可以帮助使您的部署更加可靠，而无需您24/7进行监控。这些自动化操作是Kubernetes的核心价值主张之一，理解它们是充分利用Kubernetes提供的一切的必要步骤。
- en: Kubernetes can also help you update your application without outages and glitches
    by booting the new version and monitoring its status to ensure it’s ready to serve
    traffic before removing the old version.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes可以通过启动新版本并监控其状态，确保在删除旧版本之前它已准备好服务流量，从而帮助您在不出现中断和故障的情况下更新应用。
- en: To help Kubernetes help keep your application running without downtime during
    normal operations and upgrades, you need to provide certain information about
    the state of your application with a process known as health checks. In the next
    section, we’ll go through adding the various health checks to your application
    and, in a later section, how these can be used with Kubernetes’ built-in rollout
    strategies to update your application without glitches or downtime.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助Kubernetes在正常操作和升级期间保持应用的无停机运行，您需要通过称为健康检查的过程提供有关应用状态的信息。在下一节中，我们将介绍如何将各种健康检查添加到您的应用中，在稍后的章节中，我们将介绍如何使用Kubernetes内置的滚动策略来更新应用，而不会出现故障或停机。
- en: 4.1 Automated uptime with health checks
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 带健康检查的自动化正常运行时间
- en: There are some conditions that Kubernetes can detect and repair on its own.
    If your application crashes, Kubernetes will restart it automatically. Likewise,
    if the node running your container fails or is removed, Kubernetes will notice
    that your Deployment is missing replicas and boot new replicas on available space
    in the cluster.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 有些条件Kubernetes可以自行检测和修复。如果您的应用崩溃，Kubernetes会自动重启它。同样，如果运行容器的节点失败或被移除，Kubernetes会注意到您的部署缺少副本，并在集群中可用空间上启动新的副本。
- en: But what about other types of application failure, like a hung process, a web
    service that stops accepting connections, or an application that depends on an
    external service when that service becomes inaccessible? Kubernetes can gracefully
    detect and attempt to recover from all these conditions, but it needs you to provide
    signals on the health of your application and whether it is ready to receive traffic.
    The process used to provide these signals are named *health checks*, which Kubernetes
    refers to as *liveness and readiness probes*.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 但其他类型的应用故障怎么办，比如挂起的进程、停止接受连接的Web服务，或者当外部服务变得不可访问时依赖该服务的应用？Kubernetes可以优雅地检测并尝试从所有这些条件中恢复，但它需要您提供有关应用健康状况以及它是否准备好接收流量的信号。提供这些信号的过程称为健康检查，Kubernetes将其称为**活跃性和就绪性探测**。
- en: Since Kubernetes can’t know what it means for each and every service that runs
    on the platform to be down or up, ready or unready to receive traffic, apps must
    themselves implement this test. Simply put, the probe queries the container for
    its status, and the container checks its own internal state and returns a success
    code if everything is good. If the request times out (e.g., if the application
    is under too much load) or the container itself determines that there’s a problem
    (such as with a critical dependency), the probe is considered a fail.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Kubernetes无法知道平台上每个运行的服务处于关闭或开启、准备好或未准备好接收流量的具体含义，因此应用必须自行实现此测试。简单来说，探测会查询容器状态，容器会检查其内部状态，如果一切正常则返回成功代码。如果请求超时（例如，如果应用负载过重）或容器本身确定存在问题（例如，关键依赖项问题），则探测被视为失败。
- en: 4.1.1 Liveness and readiness probes
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 活跃性和就绪性探测
- en: 'In Kubernetes, the health of a container is determined by two separate probes:
    *liveness*, which determines whether the container is running, and *readiness*,
    which indicates when the container is able to receive traffic. Both probes use
    the same techniques to perform the checks, but how Kubernetes uses the result
    of the probe is different (see table 4.1).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，容器的健康状态由两个独立的探针确定：*生存性*，它确定容器是否正在运行，以及*就绪性*，它指示容器何时能够接收流量。这两个探针使用相同的技巧进行检查，但
    Kubernetes 使用探针结果的方式不同（见表 4.1）。
- en: Table 4.1 Differences between liveness and readiness
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 生存性和就绪性的区别
- en: '|  | Liveness | Readiness |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  | 生存性 | 就绪性 |'
- en: '| Semantic meaning | Is the container running? | Is the container ready to
    receive traffic? |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| 语义含义 | 容器是否正在运行？ | 容器是否准备好接收流量？ |'
- en: '| Implication of probe failures exceeding threshold | Pod is terminated and
    replaced. | Pod is removed from receiving traffic until the probe passes. |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 探针失败超过阈值的含义 | Pod 被终止并替换。 | Pod 在探针通过之前被移除以接收流量。 |'
- en: '| Time to recover from a failed probe | Slow: Pod is rescheduled on failure
    and needs time to boot. | Fast: Pod is already running and can immediately receive
    traffic once the probe passes. |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 从失败的探针中恢复的时间 | 慢：Pod 在失败时重新调度并需要时间启动。 | 快：Pod 已经运行，一旦探针通过即可立即接收流量。 |'
- en: '| Default state at container boot | Passing (live). | Failing (unready). |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 容器启动时的默认状态 | 通过（活动）。 | 失败（未就绪）。 |'
- en: There are a few reasons for having the two probe types. One is the state at
    boot. Note how the liveness probe starts in the passing, or live, state (the container
    is assumed to be live until the Pod proves otherwise), whereas the readiness probe
    starts in the unready state (the container is assumed to not be able to serve
    traffic until it proves it can).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种探针类型的原因有几个。一个是启动状态。注意生存性探针从通过或活动状态开始（假设容器在 Pod 证明其不活动之前是活动的），而就绪性探针从未就绪状态开始（假设容器在证明其能够服务流量之前无法服务流量）。
- en: Without a readiness check, Kubernetes has no way of knowing when the container
    is ready to receive traffic, so it has to assume it’s ready the moment the container
    starts up, and it will be added to the Service’s load-balancing rotation immediately.
    Most containers take tens of seconds or even minutes to start up—so sending traffic
    right away would result in some traffic loss during startup. The readiness check
    solves this by only reporting “Ready” when the internal tests are passing.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 没有就绪性检查，Kubernetes 没有办法知道容器何时准备好接收流量，因此它必须假设容器在启动的那一刻就绪，并且它将立即被添加到服务的负载均衡轮询中。大多数容器需要数十秒甚至数分钟才能启动——因此立即发送流量会导致启动期间一些流量损失。就绪性检查通过仅在内部测试通过时才报告“就绪”来解决此问题。
- en: Likewise, with a liveness check, the conditions that require a container restart
    may differ from those that indicate the container is not ready to receive traffic.
    The best example is a container waiting for an external dependency, like a database
    connection. Until the container has the database connection, it should not be
    serving traffic (therefore it’s unready), but internally, the container is good
    to go. You don’t want to replace this container too hastily so that it has enough
    time to establish the database connection it depends on.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，与生存性检查一样，需要容器重启的条件可能与指示容器未准备好接收流量的条件不同。最好的例子是等待外部依赖项（如数据库连接）的容器。直到容器建立了数据库连接，它才不应该提供服务（因此它是未就绪的），但内部容器是良好的。您不希望太急切地替换此容器，以便它有足够的时间建立其依赖的数据库连接。
- en: Other reasons for having two types of probes are the sensitivity and recovery
    times. Readiness checks are typically tuned to quickly remove the Pod from the
    load balancer (as this is a fast and cheap operation to initiate) and add it back
    when the check is passing again, whereas liveness checks are often tuned to be
    a little less hasty as the time needed to re-create a container is longer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 存在两种类型探针的其他原因包括敏感性和恢复时间。就绪性检查通常调整得很快，以便快速将 Pod 从负载均衡器中移除（因为这是一种快速且成本低的操作来启动），并在检查再次通过时将其添加回去，而生存性检查通常调整得稍微不那么急迫，因为重新创建容器所需的时间更长。
- en: 4.1.2 Adding a readiness probe
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 添加就绪性探针
- en: For a web service, a rudimentary health check could simply test “Is the service
    serving traffic?” Before building a dedicated health check endpoint for your service,
    you could just find any endpoint on the service that returns an HTTP 200 status
    code and use it as the health check.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个网络服务，一个基本的健康检查可以简单地测试“服务是否在处理流量？”在为你的服务构建一个专门的健康检查端点之前，你可以在服务上找到任何返回HTTP
    200状态码的端点，并将其用作健康检查。
- en: If the root path returns HTTP 200 on all responses, you can just use that path.
    Since the root path behaves that way in our example container, the following readiness
    probe will work just fine.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果根路径在所有响应中都返回HTTP 200，你就可以直接使用该路径。由于在我们的示例容器中根路径表现是这样的，所以下面的就绪检查将正常工作。
- en: Listing 4.1 Chapter04/4.1.2_Readiness/deploy.yaml
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 第04章/4.1.2_Readiness/deploy.yaml
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ After an initial delay
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始延迟后
- en: ❷ Every 30 seconds
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每30秒
- en: ❸ Perform this HTTP request.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行此HTTP请求。
- en: ❹ Time out after 2 seconds.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 2秒后超时。
- en: ❺ Consider one error response to indicate the container is not ready.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将一个错误响应视为容器未就绪。
- en: ❻ Consider one successful response to indicate the container is ready after
    being considered unready.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将一个成功的响应视为容器在被视为未就绪后已准备好。
- en: From the root directory, update the `timeserver` Deployment with
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 从根目录更新`timeserver` Deployment：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now, any time the container fails to respond to the readiness check, that Pod
    will be temporarily removed from the Service. Say you have three replicas of a
    Pod, and one of them fails to respond. Any traffic to the Service will be routed
    to the remaining two healthy Pods. Once the Pod returns success (an HTTP 200 response
    in this case), it will be added back into service.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，任何容器未能响应就绪检查时，该Pod将被临时从服务中移除。假设你有三个Pod副本，其中一个未能响应。任何访问服务的流量都将被路由到剩余的两个健康Pod。一旦Pod返回成功（在这种情况下是一个HTTP
    200响应），它将被重新加入到服务中。
- en: This readiness check is particularly important during updates, as you don’t
    want Pods to be receiving traffic while they are booting (as these requests will
    fail). With correctly implemented readiness checks, you can get zero downtime
    updates, as traffic is only routed to Pods that are ready and not ones in the
    process of being created.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这种就绪检查在更新期间尤为重要，因为你不希望Pod在启动时接收流量（因为这些请求将失败）。通过正确实现的就绪检查，你可以实现零停机更新，因为流量只被路由到已就绪的Pod，而不是正在创建的Pod。
- en: Observing the difference
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 观察差异
- en: 'If you want to see the difference between having a readiness check and not
    with your own experimentation, try the following test. In one shell window, create
    a Deployment without a readiness check (let’s use the one from chapter 3):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想通过自己的实验来查看有无就绪检查之间的差异，请尝试以下测试。在一个shell窗口中，创建一个没有就绪检查的Deployment（让我们使用第3章中的那个）：
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Wait for the Service to be assigned an external IP:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 等待服务被分配一个外部IP：
- en: '[PRE3]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now, set your IP and set up a watch on the service endpoint in a separate console
    window:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，设置你的IP，并在单独的控制台窗口中设置对服务端点的监视：
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Back in the first window, trigger a rollout:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个窗口中，触发一个回滚：
- en: '[PRE5]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As the Pods restart, you should see some intermittent connection problems in
    the curl window.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 随着Pod的重启，你应该在curl窗口中看到一些间歇性的连接问题。
- en: Now update the Deployment with a readiness check (like the one in this section)
    and apply
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在更新Deployment以包含就绪检查（如本节中所示）并应用：
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This time, since the Deployment has a readiness check, you shouldn’t see any
    connection problems on the `curl` window.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，由于部署有一个就绪检查，你不应该在`curl`窗口中看到任何连接问题。
- en: 4.1.3 Adding a liveness probe
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.3 添加存活探针
- en: Liveness probes have the same specification as readiness probes but are specified
    with the key `livenessProbe`. How the probes are *used*, on the other hand, is
    quite different. The result of the readiness probe governs whether the Pod receives
    traffic, whereas a failing liveness probe will cause the Pod to be restarted (once
    the failure threshold is met).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 存活探针与就绪探针具有相同的规范，但使用键`livenessProbe`指定。另一方面，探针的使用方式相当不同。就绪探针的结果决定了Pod是否接收流量，而失败的存活探针将导致Pod重启（一旦达到失败阈值）。
- en: The readiness check we added to our Deployment in the previous section was rudimentary
    in that it just used the root path of the service rather than a dedicated endpoint.
    We can continue that practice for now and use the same endpoint from the readiness
    probe for the liveness probe in the following example, with minor changes to increase
    the failure tolerance. Since the container gets restarted when the liveness probe
    fails the threshold and can take some time to come back, we don’t want the liveness
    probe set up on a hair trigger. Let’s add a liveness probe to our Deployment that
    will restart it if it fails for 180 seconds (six failures at a 30-second interval).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前一节中添加到部署的就绪检查是基本的，因为它只是使用了服务的根路径而不是一个专门的端点。我们现在可以继续这种做法，并在以下示例中使用就绪探测的相同端点作为活跃探测，进行一些小的修改以增加故障容忍度。由于容器在活跃探测失败时达到阈值会重启，并且需要一些时间才能恢复，我们不希望活跃探测被设置为过于敏感。让我们为我们的部署添加一个活跃探测，如果它失败180秒（在30秒间隔内六次失败），则将其重启。
- en: Listing 4.2 Chapter04/4.1.3_Liveness/deploy.yaml
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 第04章/4.1.3_Liveness/deploy.yaml
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Specifies a liveness probe this time
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这次指定一个活跃探测
- en: ❷ After an initial delay of 30 seconds
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在初始延迟30秒后
- en: ❸ Every 30 seconds
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每30秒
- en: ❹ Perform this HTTP request.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 执行此HTTP请求。
- en: ❺ Time out after 5 seconds (more tolerant than the readiness check).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 5秒后超时（比就绪检查更宽容）。
- en: ❻ Consider 10 error responses in a row to indicate the container is not ready.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 连续10次错误响应以指示容器未就绪。
- en: ❼ Consider one successful response to indicate the container is ready after
    being considered unready.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 考虑一个成功的响应以指示容器在被视为未就绪后已就绪。
- en: 'Update the `timeserver` Deployment with these latest changes:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些最新的更改更新`timeserver`部署：
- en: '[PRE8]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Now, your Deployment has a readiness and liveness probe. Even these rudimentary
    probes improve the reliability of your deployment drastically. If you stop here,
    it’s probably enough for a basic application. The next section details some further
    design considerations to bulletproof your probes for production use.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您的部署有了就绪和活跃探测。即使这些基本的探测也能极大地提高部署的可靠性。如果您就此止步，这可能对于一个基本的应用程序来说已经足够了。下一节将详细说明一些进一步的设计考虑，以确保您的探测在生产使用中更加稳固。
- en: 4.1.4 Designing good health checks
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.4 设计良好的健康检查
- en: While using an existing endpoint as we did in the previous two sections, as
    the health check path is better than nothing, it’s generally better to add dedicated
    health check endpoints to your application. These health checks should implement
    the specific semantics of readiness and liveness and be as lightweight as possible.
    Without understanding the semantic differences between liveness and readiness,
    you could see instability due to excessive restarts and cascading failures. In
    addition, if you’re reusing some other endpoint, chances are it’s heavier-weight
    than needed. Why pay the cost of rendering an entire HTML page when a simple HTTP
    header response would suffice?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用现有端点，就像我们在前两个章节中所做的那样，尽管健康检查路径总比没有好，但通常最好为您的应用程序添加专用的健康检查端点。这些健康检查应该实现就绪和活跃的具体语义，并尽可能轻量。如果不理解活跃和就绪之间的语义差异，可能会因为重启过多和级联故障而看到不稳定性。此外，如果您正在重用其他端点，那么它可能比所需的更重。为什么要在整个HTML页面渲染的成本上付费，而一个简单的HTTP头部响应就足够了呢？
- en: When creating the HTTP endpoints to implement these checks, it’s important to
    take into account any external dependencies being tested. Generally, you don’t
    want external dependencies to be checked in the liveness probe; rather, it should
    test only whether the container itself is running (assuming your container will
    retry the connections to its external connections). There’s not really any value
    in restarting a container that’s running just fine or only because it can’t connect
    to another service that is having trouble. This could cause unnecessary restarts
    that create churn and lead to cascading failures, particularly if you have a complex
    dependency graph. However, there is an exception to this principle of not testing
    dependencies in liveness probes, which I cover later in the section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建HTTP端点以实现这些检查时，考虑任何正在测试的外部依赖项非常重要。通常，您不希望在活跃探测中检查外部依赖项；相反，它应该只测试容器本身是否正在运行（假设您的容器将重试其外部连接的连接）。对于运行良好的容器或仅因为无法连接到另一个有问题的服务而重启的容器，实际上并没有太多价值。这可能导致不必要的重启，从而产生波动并导致级联故障，尤其是如果您有一个复杂的依赖图。然而，对于活跃探测中不测试依赖项的原则有一个例外，我将在后面的章节中介绍。
- en: Since the liveness probe is only testing whether the server is responding, the
    result can and should be extremely simple, generally just an HTTP 200 status response,
    even one with no response body text. If the request can get through to the server
    code, then it must be live, which is enough information.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存活探测仅测试服务器是否响应，结果可以且应该是极其简单的，通常只是一个HTTP 200状态响应，甚至可以没有响应体文本。如果请求能够到达服务器代码，那么它必须是活跃的，这已经足够了。
- en: For readiness probes on the other hand, it’s generally desirable that they test
    their external dependencies like a database connection (see figure 4.1). Say you
    have three replicas of a Pod, and only two can connect to your database. It makes
    sense to only have those two fully functional Pods in the load-balancer rotation.
    One way to test the connection is to look up a single row from the database in
    your readiness check.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 对于就绪性探测而言，通常希望它们测试它们的外部依赖，如数据库连接（见图4.1）。假设你有三个Pod副本，但只有两个可以连接到你的数据库。只让那些两个完全功能的Pod在负载均衡器轮询中是有意义的。测试连接的一种方法是在就绪性检查中从数据库中查找单行。
- en: '![04-01](../../OEBPS/Images/04-01.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![04-01](../../OEBPS/Images/04-01.png)'
- en: Figure 4.1 Liveness and readiness checks and external dependencies
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 存活性和就绪性检查以及外部依赖
- en: For example, the pseudocode for a database connection check could look something
    like
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，数据库连接检查的伪代码可能看起来像这样
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Performing a simple SQL query should be enough to ensure that the database is
    both connected and responsive. Rather than using a `SELECT` query, you could perform
    any other database operation, but I personally like the legitimacy of a `SELECT`
    statement. If it works, I’m confident the other queries will work, too.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 执行一个简单的SQL查询应该足以确保数据库既已连接又可响应。与其使用`SELECT`查询，你还可以执行任何其他数据库操作，但我个人更喜欢`SELECT`语句的合法性。如果它有效，我就有信心其他查询也会有效。
- en: The Python `timeserver` example app doesn’t have a database dependency. However,
    let’s refactor the code to include specific paths, which we will name `/healthz`
    and `/readyz`, as it is best practice to have dedicated endpoints for these probes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Python的`timeserver`示例应用没有数据库依赖。但是，让我们重构代码以包括特定的路径，我们将它们命名为`/healthz`和`/readyz`，因为为这些探测保留专用端点是最佳实践。
- en: Listing 4.3 Chapter04/timeserver2/server.py
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 第04章/timeserver2/server.py
- en: '[PRE10]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The new health check paths
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新的健康检查路径
- en: After updating our Deployment configuration for these new endpoints, we get
    the code in the following listing.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在更新了这些新端点的部署配置之后，我们得到以下列表中的代码。
- en: Listing 4.4 Chapter04/4.1.4_GoodHealthChecks/deploy.yaml
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.4 第04章/4.1.4_良好的健康检查/deploy.yaml
- en: '[PRE11]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Updated endpoints
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 更新后的端点
- en: ❷ Now that the liveness probe is lightweight, we can reduce the failure threshold.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在存活探测变得轻量级，我们可以降低失败阈值。
- en: Apply this new config in the usual way. Your own application may have more complex
    readiness and liveness logic. The `healthz` endpoint here probably works for many
    HTTP applications (simply testing that the HTTP server is responding to requests
    is sufficient). However, every application with dependencies like databases should
    define its own readiness test to determine whether your application is truly ready
    to serve user requests.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以通常的方式应用这个新配置。你的应用程序可能具有更复杂的就绪性和存活性逻辑。这里的`healthz`端点可能适用于许多HTTP应用程序（简单地测试HTTP服务器是否响应请求就足够了）。然而，每个具有数据库等依赖的应用程序都应该定义自己的就绪性测试，以确定你的应用程序是否真正准备好服务用户请求。
- en: 4.1.5 Rescheduling unready containers
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.5 重新调度未就绪的容器
- en: The previous section detailed the standard way to set up liveness and readiness
    checks in Kubernetes where you only verify service dependencies in the readiness
    check. There is one problematic condition that can arise from not testing dependencies
    in the liveness check. By separating the concerns into readiness (“Is the container
    ready to receive traffic?”) and liveness (“Is the container running?”), there
    could be a condition where the container is running, but due to a bug in the container’s
    retry logic, the external connections are never resolved. In other words, your
    container could stay unready forever, something that a restart might resolve.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 前一节详细介绍了在Kubernetes中设置存活和就绪检查的标准方法，其中你只需验证就绪检查中的服务依赖。不测试存活检查中的依赖可能会出现一个有问题的条件。通过将关注点分离为就绪性（“容器是否准备好接收流量？”）和存活性（“容器是否正在运行？”），可能会出现容器正在运行，但由于容器重试逻辑中的错误，外部连接从未解决的情况。换句话说，你的容器可能会永远处于未就绪状态，这可能需要重启来解决。
- en: Recall that we don’t generally test readiness in the liveness check, as this
    could cause the Pod to be re-created too quickly and not provide any time for
    the external dependencies to resolve. Still, it might make sense to have this
    Pod be re-created if it stays unready for too long. Sometimes it’s best just to
    turn it off and on again!
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们通常不在存活性检查中测试就绪性，因为这可能会导致Pod被太快地重新创建，没有为外部依赖项的解决提供任何时间。然而，如果Pod长时间不可用，重新创建这个Pod可能是有意义的。有时最好的办法就是关掉它再打开它！
- en: Unfortunately, Kubernetes doesn’t have a way to express this logic directly,
    but it’s easy enough to add it to our own liveness check so that it will fail
    if the Pod doesn’t become ready in a certain amount of time. You can simply record
    the time of each readiness success response and then fail your liveness check
    if too much time passes (e.g., 5 minutes). The following listing provides a simple
    implementation of this logic into the `timeserver` container.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，Kubernetes没有直接表达这种逻辑的方法，但很容易将其添加到我们自己的存活性检查中，以便如果Pod在一段时间内没有就绪，它就会失败。你可以简单地记录每个就绪性成功响应的时间，然后如果时间过长（例如，5分钟），就失败存活性检查。以下列表提供了将此逻辑简单实现到`timeserver`容器的示例。
- en: Listing 4.5 Chapter04/timeserver3/server.py
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.5 第04章/timeserver3/server.py
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The “last ready” time is initialized at the current time to allow for 5 minutes
    after startup.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ “最后就绪”时间初始化为当前时间，以便在启动后允许5分钟。
- en: ❷ If 5 minutes have passed since the last successful readiness result (or since
    boot), fail the liveness check.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果自上次成功的就绪性结果（或自启动以来）已过去5分钟，则失败存活性检查。
- en: ❸ Each time the readiness passes, the time is updated.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每次就绪性通过时，时间都会更新。
- en: 'Having the liveness check fail eventually if the container doesn’t become ready
    within a given timeframe gives it a chance to restart. Now, we have the best of
    both worlds: we don’t test the external dependencies in the liveness check, but
    we do in the readiness one. That means our container won’t receive traffic when
    its dependencies are not connected, but it’s not rebooted either, giving it some
    time to self-heal. But if, after 5 minutes, the container is still not ready,
    it will fail the liveness check and be restarted.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果容器在给定时间内没有就绪，存活性检查最终会失败，这给了它重启的机会。现在，我们拥有了两者之最：我们不在存活性检查中测试外部依赖项，但在就绪性检查中测试。这意味着当依赖项未连接时，我们的容器不会收到流量，但它也不会重启，这给了它一些时间来自我修复。但是，如果在5分钟后容器仍然没有就绪，它将失败存活性检查并被重启。
- en: An alternative approach to achieve this (restarting the container after a prolonged
    period of unreadiness) is to use the readiness endpoint for both the liveness
    and readiness probes but with different tolerances. That is, for example, the
    readiness check fails after 30 seconds, but liveness fails only after 5 minutes.
    This approach still gives the container some time to resolve any interdependent
    services before eventually rebooting in the event of continued downtime, which
    may indicate a problem with the container itself. This technique is not technically
    idiomatic Kubernetes, as you’re still testing dependencies in the liveness check,
    but it gets the job done.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一目标（在长时间不可用后重启容器）的另一种方法是同时使用存活性和就绪性探针的存活性端点，但具有不同的容忍度。也就是说，例如，就绪性检查在30秒后失败，但存活性检查仅在5分钟后失败。这种方法仍然给容器一些时间来解决任何相互依赖的服务，在最终重启之前，这可能会表明容器本身存在问题。这种技术从技术上讲不是Kubernetes的惯用方法，因为你在存活性检查中仍在测试依赖项，但它完成了工作。
- en: In conclusion, these two probes are incredibly important to giving Kubernetes
    the information it needs to automate the reliability of your application. Understanding
    the difference between them and implementing appropriate checks that take into
    account the specific details of your application are crucial.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，这两个探针对于向Kubernetes提供它需要的信息以自动化应用程序的可靠性至关重要。理解它们之间的区别并实施适当的检查，考虑到应用程序的具体细节，是至关重要的。
- en: 4.1.6 Probe types
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.6 探针类型
- en: Until now, the examples have assumed an HTTP service, and the probes therefore
    were implemented as HTTP requests. Kubernetes can be used to host many different
    types of services, as well as batch jobs with no service endpoints at all. Fortunately,
    there are a number of ways to expose health checks.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，示例都假设了HTTP服务，因此探针被实现为HTTP请求。Kubernetes可以用于托管许多不同类型的服务，以及没有任何服务端点的批处理作业。幸运的是，有几种方法可以公开健康检查。
- en: HTTP
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP
- en: HTTP is recommended for any container that provides an HTTP service. The service
    exposes an endpoint, such as `/healthz`. An HTTP 200 response indicates success;
    any other response (or timeout) indicates a failure.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于提供HTTP服务的任何容器，建议使用HTTP。服务公开一个端点，例如`/healthz`。HTTP 200响应表示成功；任何其他响应（或超时）表示失败。
- en: TCP
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: TCP
- en: 'TCP is recommended for TCP-based services other than HTTP (e.g., a SMTP service).
    The probe succeeds if the connection can be opened:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 对于除了HTTP之外的基于TCP的服务（例如，SMTP服务），建议使用TCP。如果可以打开连接，则探针成功。
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ TCP probe specification
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ TCP探针规范
- en: Bash script
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: Bash脚本
- en: A bash script is recommended for any container not providing an HTTP or a TCP
    service, such as batch jobs that don’t run service endpoints. Kubernetes will
    execute the script you specify, allowing you to perform whatever tests you need.
    A nonzero exit code indicates failure. Section 10.4 has a complete example of
    a liveness probe for a background task.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 对于不提供HTTP或TCP服务的任何容器，如不运行服务端点的批处理作业，建议使用bash脚本。Kubernetes将执行您指定的脚本，允许您执行所需的任何测试。非零退出代码表示失败。第10.4节有一个后台任务存活探针的完整示例。
- en: 4.2 Updating live applications
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 更新运行中的应用程序
- en: Once you’ve implemented readiness checks, you can now roll out changes to your
    application without downtime. Kubernetes uses the readiness check during updates
    to know when the new Pod is ready to receive traffic and to govern the rate of
    the rollout according to the parameters you set. You can choose from several different
    rollout strategies, each with its own characteristics.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 实施就绪性检查后，现在您可以无停机地推出应用程序更改。Kubernetes在更新期间使用就绪性检查来确定新Pod何时准备好接收流量，并根据您设置的参数控制部署的速度。您可以选择几种不同的部署策略，每种策略都有其自身的特点。
- en: 4.2.1 Rolling update strategy
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 滚动更新策略
- en: The default zero-downtime update strategy offered by Kubernetes is a rolling
    update. In a rolling update, Pods with the new version are created in groups (the
    size of which is tunable). Kubernetes waits for the new group of Pods to become
    available and then terminates the same number of Pods running the old version,
    repeating this until all Pods are running the new version (figure 4.2).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供的默认零停机更新策略是滚动更新。在滚动更新中，会以组的形式创建具有新版本的新Pod（组的大小是可调的）。Kubernetes等待新组的Pod变得可用，然后终止运行旧版本相同数量的Pod，重复此过程，直到所有Pod都运行新版本（图4.2）。
- en: '![04-02](../../OEBPS/Images/04-02.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![04-02](../../OEBPS/Images/04-02.png)'
- en: Figure 4.2 Pod status during a rolling update. With this strategy, requests
    can be served by either the old or the new version of the app until the rollout
    is complete.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2滚动更新期间的Pod状态。使用此策略，在部署完成之前，请求可以由应用程序的旧版或新版提供服务。
- en: 'The goal of such a strategy is twofold:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 此策略的目标有两个：
- en: Providing continuous uptime during the rollout
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在部署过程中提供连续的运行时间
- en: Using as few extra resources as possible during the update
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更新过程中尽可能少地使用额外资源
- en: Importantly, with this strategy, the two versions of your application (old and
    new) need to be able to coexist as they will both be running for a time. That
    is, your backend or any other dependencies must be able to handle these two different
    versions, and users may get alternating versions when they make different requests.
    Imagine reloading the page and seeing the new version and then reloading it and
    seeing the old version again. Depending on how many replicas you have, a rolling
    update can take a while to complete (and, therefore, any rollback can also take
    a while).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，使用此策略时，您的应用程序的两个版本（旧版和新版）需要能够共存，因为它们将同时运行一段时间。也就是说，您的后端或任何其他依赖项必须能够处理这两个不同的版本，并且当用户进行不同请求时，可能会得到交替的版本。想象一下，重新加载页面看到新版本，然后再次重新加载看到旧版本。根据您拥有的副本数量，滚动更新可能需要一段时间才能完成（因此，任何回滚也可能需要一段时间）。
- en: Let’s configure our Deployment to use the rolling update strategy in the following
    listing.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '让我们配置我们的Deployment以使用以下列表中的滚动更新策略。 '
- en: Listing 4.6 Chapter04/4.2.1_RollingUpdate/deploy.yaml
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.6 Chapter04/4.2.1_RollingUpdate/deploy.yaml
- en: '[PRE14]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Rolling update strategy
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 滚动更新策略
- en: ❷ Optional configuration
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 可选配置
- en: The options `maxSurge` and `maxUnavailable` can be used to govern how quickly
    the rollout happens.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`maxSurge`和`maxUnavailable`选项来控制部署的速度。
- en: MaxSurge
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: MaxSurge
- en: '`maxSurge` governs how many additional Pods you’re willing to create during
    the rollout. For example, if you set a replica count of `5`, and a `maxSurge`
    of `2`, then it may be possible to have seven Pods (of different versions) scheduled.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxSurge` 控制在滚动更新过程中你愿意创建多少额外的 Pods。例如，如果你设置了副本数量为 `5`，并且 `maxSurge` 为 `2`，那么可能会有七个
    Pods（不同版本）被调度。'
- en: The tradeoff is that the higher this number is, the faster the rollout will
    complete, but the more resources it will (temporarily) use. If you’re highly optimizing
    your costs, you could set `maxSurge` to `0`. Alternatively, for a large deployment,
    you could temporarily increase the resources available in your cluster during
    the rollout by adding nodes and removing them when the rollout is complete.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 权衡是，这个数字越高，滚动更新完成得越快，但（暂时）使用的资源也越多。如果你高度优化成本，可以将 `maxSurge` 设置为 `0`。或者，对于大型部署，你可以在滚动更新期间暂时增加集群中可用的资源，通过添加节点，并在滚动更新完成后移除它们。
- en: MaxUnavailable
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 最大不可用数
- en: '`maxUnavailable` sets the maximum number of Pods that can be unavailable during
    updates (percentage values are also accepted and are rounded down to the nearest
    integer). If you’ve tuned your replica count to handle your expected traffic,
    you may not want to set this value much higher than `0`, as your service quality
    could degrade during updates. The tradeoff here is that the higher the value,
    the more Pods can be replaced at once and the faster the rollout completes, while
    temporarily reducing the number of ready Pods that are able to process traffic.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`maxUnavailable` 设置了在更新过程中可以不可用的最大 Pod 数量（也接受百分比值，并向下取整到最接近的整数）。如果你已经调整了副本数量以处理预期的流量，你可能不希望将此值设置得比
    `0` 高得多，因为在更新期间你的服务质量可能会下降。这里的权衡是，值越高，一次可以替换的 Pods 越多，滚动更新完成得越快，但暂时会减少能够处理流量的就绪
    Pods 数量。'
- en: A rollout could coincide with another event that lowers availability, like a
    node failure. Thus, for production workloads, I recommend setting `maxUnavailable`
    to `0`. The caveat is that if you set it to `0` and your cluster has no schedulable
    resources, the rollout will get stuck, and you will see Pods in the `Pending`
    state until resources become available. When `maxUnavailable` is `0`, `maxSurge`
    cannot also be `0` because, to preserve the full availability, the system needs
    to temporarily increase the replica count to allow time for the new Pods to become
    ready.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新可能与降低可用性的其他事件同时发生，例如节点故障。因此，对于生产工作负载，我建议将 `maxUnavailable` 设置为 `0`。需要注意的是，如果你将其设置为
    `0` 而你的集群没有可调度的资源，滚动更新将会卡住，你将看到 Pod 处于 `Pending` 状态，直到资源变得可用。当 `maxUnavailable`
    为 `0` 时，`maxSurge` 不能也为 `0`，因为为了保持完全可用性，系统需要暂时增加副本数量，以便为新 Pods 提供就绪时间。
- en: Recommendation
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 建议
- en: A rolling update is a good go-to strategy for most services. For production
    services, `maxUnavailable` is best set to `0`. `maxSurge` should be at least `1`,
    or higher if you have enough spare capacity and want faster rollouts.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是大多数服务的一个很好的策略。对于生产服务，`maxUnavailable` 最好设置为 `0`。`maxSurge` 至少应该是 `1`，或者如果你有足够的备用容量并且希望更快地滚动更新，可以设置得更高。
- en: Deploying changes with a rolling update
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用滚动更新部署更改
- en: Once your Deployment is configured to use a rolling update, deploying your changes
    is as simple as updating the Deployment manifest (e.g., with a new container version)
    and applying the changes with `kubectl apply`. Most changes made to the Deployment,
    including readiness and liveness checks, are also versioned and will be rolled
    out just like a new container image version. If you ever need, you can also force
    a rollout without changing anything in the Deployment object with `kubectl` `rollout`
    `restart` `deploy` `$DEPLOYMENT_NAME`.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你的 Deployment 配置为使用滚动更新，部署你的更改就像更新 Deployment 清单（例如，使用新的容器版本）并用 `kubectl apply`
    应用更改一样简单。对 Deployment 所做的几乎所有更改，包括就绪和存活检查，也都是版本化的，并且会像新的容器镜像版本一样进行滚动更新。如果你需要，你也可以使用
    `kubectl rollout restart deploy $DEPLOYMENT_NAME` 强制进行滚动更新，而无需在 Deployment 对象中做任何更改。
- en: 4.2.2 Re-create strategy
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 重新创建策略
- en: Another approach—some might say, the old-fashion approach—is to cut the application
    over directly, deleting all Pods of the old version and scheduling replacements
    of the new version. Unlike the other strategies discussed here, this approach
    is *not* a zero-downtime one. It will almost certainly result in some unavailability
    (figure 4.3). With the right readiness checks in place, this downtime could be
    as short as the time to boot the first Pod, assuming it can handle the client
    traffic at that moment in time. The benefit of this strategy is that it does not
    require compatibility between the new version and the old version (since the two
    versions won’t be running at the same time), and it doesn’t require any additional
    compute capacity (since it’s a direct replacement).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法——有些人可能会说是老式方法——是直接切换应用程序，删除旧版本的所有 Pod 并安排新版本的替换。与这里讨论的其他策略不同，这种方法**不是**零停机时间。它几乎肯定会导致一些不可用（见图
    4.3）。如果有适当的就绪性检查，这种停机时间可以短到启动第一个 Pod 的时间，前提是它能在那一刻处理客户端流量。这种策略的好处是它不需要新旧版本之间的兼容性（因为两个版本不会同时运行），也不需要任何额外的计算能力（因为它是一个直接替换）。
- en: '![04-03](../../OEBPS/Images/04-03.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![04-03](../../OEBPS/Images/04-03.png)'
- en: Figure 4.3 A Pod status during a rollout with the re-create strategy. During
    this type of rollout, the app will experience a period of total downtime and a
    period of degraded capacity.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.3 使用重新创建策略的 Pod 状态。在这种类型的滚动更新中，应用将经历一段完全停机的时间和一段容量下降的时间。
- en: 'This strategy may be useful for development and staging environments to avoid
    needing to overprovision compute capacity to handle rolling updates, and to increase
    speed, but otherwise, it should generally be avoided. To use this, in the `strategy`
    field given in listing 4.6, you would instead declare:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略可能适用于开发和测试环境，以避免需要过度配置计算能力来处理滚动更新，并提高速度，但除此之外，通常应避免使用。要使用此策略，在列表 4.6 中给出的
    `strategy` 字段中，你将声明：
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 4.2.3 Blue/green strategy
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 蓝绿策略
- en: The blue/green strategy is a rollout strategy where the new application version
    is deployed alongside the existing version (figure 4.4). These versions are given
    the names “blue” and “green.” When the new version is fully deployed, tested,
    and ready to go, the service is cut over. If there’s a problem, it can be immediately
    cut back. After a time, if everything looks good, the old version can be removed.
    Unlike the prior two strategies, the old version remains ready to serve and is
    removed only when the new version is validated (and often with a human decision
    involved).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿策略是一种滚动更新策略，其中新应用程序版本与现有版本一起部署（见图 4.4）。这些版本被命名为“蓝色”和“绿色”。当新版本完全部署、测试并准备好使用时，服务将切换。如果出现问题，可以立即切换回旧版本。经过一段时间，如果一切看起来都很好，可以删除旧版本。与前面的两种策略不同，旧版本仍然可以提供服务，只有在新版本经过验证（并且通常涉及人类决策）后才会被删除。
- en: '![04-04](../../OEBPS/Images/04-04.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![04-04](../../OEBPS/Images/04-04.png)'
- en: Figure 4.4 A Pod status during a blue/green rollout. Unlike the previous strategies,
    there are two action points where other systems, potentially including human actors,
    make decisions.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.4 蓝绿滚动更新中的 Pod 状态。与之前的策略不同，有两个操作点，其他系统（可能包括人类操作者）会做出决策。
- en: 'The benefits of this strategy include the following:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的好处包括以下内容：
- en: Only one version of the app is running at a time for a consistent user experience.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一次只运行一个版本的程序，以确保用户体验的一致性。
- en: The rollout is fast (within seconds).
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 滚动更新非常快（在几秒内完成）。
- en: Rollbacks are similarly fast.
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回滚操作也很快。
- en: 'The drawbacks include the following:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 其缺点包括以下内容：
- en: It temporarily consumes double the compute resources.
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它暂时消耗了双倍的计算资源。
- en: It is not supported directly by Kubernetes Deployments.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不是由 Kubernetes Deployments 直接支持的。
- en: This approach is an advanced rollout strategy, popular with large deployments.
    There are often several other processes included. For example, when the new version
    is ready, it can be tested first by a set of internal users, followed by a percentage
    of external traffic prior to the 100% cutover—a process known as *canary analysis*.
    After the cutover, there is often a period of time where the new version continues
    to be evaluated prior to the old version being scaled down (this could last days).
    Of course, keeping both versions scaled up doubles the resource usage, with the
    tradeoff that near-instant rollbacks are possible during that window.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是一种高级推出策略，在大规模部署中很受欢迎。通常还包括其他几个过程。例如，当新版本准备好时，可以先由一组内部用户进行测试，然后在外部流量的百分比之前进行测试，在
    100% 切换之前——这个过程被称为 *金丝雀分析*。切换后，通常有一个时间段，在新版本在旧版本缩放之前继续评估（这可能需要几天）。当然，同时保持两个版本缩放会增加资源使用量，但权衡是可以在那个窗口期间实现几乎即时的回滚。
- en: Unlike the prior two strategies—rolling updates and re-create—there is no in-built
    Kubernetes support for blue/green. Typically, users will use additional tooling
    to help with the complexities of such a rollout. Such tools include Istio, to
    split traffic at a fine-grained level, and Spinnaker, to help automate the deployment
    pipeline with the canary analysis and decision points.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 与前两种策略——滚动更新和重新创建——不同，Kubernetes 并没有内置对蓝/绿部署的支持。通常，用户会使用额外的工具来帮助处理这种部署的复杂性。这些工具包括
    Istio，用于在细粒度级别分割流量，以及 Spinnaker，用于通过金丝雀分析和决策点帮助自动化部署管道。
- en: Despite the lack of in-built support, it is possible to perform a blue/green
    rollout in Kubernetes. Without the aforementioned tools to help with the pipeline
    and traffic splitting, it is a slightly manual process and missing some benefits,
    like being able to do canary analysis on a tiny percentage of production traffic.
    However, that doesn’t mean it’s hard to implement.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管缺乏内置支持，但在 Kubernetes 中执行蓝/绿部署是可能的。如果没有上述工具来帮助处理管道和流量分割，这将是一个稍微手动的过程，并且会失去一些好处，比如能够在极小比例的生产流量上进行金丝雀分析。然而，这并不意味着它难以实现。
- en: Recall the Deployment and Service we deployed in chapter 3\. Employing a blue/green
    strategy for this application simply requires having one extra Deployment. Duplicate
    the Deployment and give one a suffix of `-blue`, and the other `-green`. This
    suffix should be applied to the name of the Deployment and the Pod’s label. You
    can then direct traffic from your Service by selecting either the `-blue` or the
    `-green` labeled Pods.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下我们在第 3 章中部署的部署和服务。对于这个应用程序采用蓝/绿策略只需要有一个额外的部署。复制部署，给其中一个添加后缀 `-blue`，另一个添加
    `-green`。这个后缀应该应用于部署的名称和 Pod 的标签。然后您可以通过选择带有 `-blue` 或 `-green` 标签的 Pod 来通过服务引导流量。
- en: The update strategy you would specify in the Deployment configuration in this
    case is the `Recreate` strategy. Since only the Pods in the inactive Deployment
    are updated, deleting all the old versions and creating Pods with the new version
    won’t result in downtime and is faster than a rolling update.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您在部署配置中指定的更新策略是 `Recreate` 策略。由于只有非活动部署中的 Pod 被更新，删除所有旧版本并创建带有新版本的 Pod
    不会导致停机，并且比滚动更新更快。
- en: The Service’s selector is used to decide which version to route traffic to (figure
    4.5). In this two-Deployment system, one version is live, and one is not live
    at any given time. The Service selects the Pods of the live Deployment with the
    label selector.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的选择器用于决定将流量路由到哪个版本（图 4.5）。在这个双部署系统中，一个版本是活动的，另一个在任何给定时间都不是活动的。服务通过标签选择器选择活动部署的
    Pod。
- en: '![04-05](../../OEBPS/Images/04-05.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![04-05](../../OEBPS/Images/04-05.png)'
- en: Figure 4.5 A single Service alternates between two Deployments, each with a
    different version of the container.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.5 一个服务在两个不同的部署之间交替，每个部署都有一个不同的容器版本。
- en: 'The steps to roll out a new version with blue/green are as follows:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蓝/绿部署推出新版本的步骤如下：
- en: Identify the nonlive Deployment (the one not selected by the Service).
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 识别非活动部署（服务未选择的那个）。
- en: Update the image path of the nonlive Deployment with the new container image
    version.
  id: totrans-162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新的容器镜像版本更新非活动部署的镜像路径。
- en: Wait until the Deployment is fully rolled out (`kubectl` `get` `deploy`).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 等待部署完全推出（使用 `kubectl get deploy`）。
- en: Update the Service’s selector to point to the new version’s Pod labels.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新服务的选择器，使其指向新版本的 Pod 标签。
- en: The update steps are performed by modifying the YAML configuration for the resource
    in question and applying the changes with `kubectl apply`. The next time you want
    to roll out a change to this application, the steps are the same, but the labels
    are reversed (if blue was live for the last update, green will be live next time).
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 更新步骤是通过修改相关资源的 YAML 配置并使用 `kubectl apply` 应用更改来执行的。下次您想对此应用程序进行更改发布时，步骤相同，但标签会反转（如果上一次更新中蓝色是活动的，则绿色将是下一次的活动）。
- en: As mentioned, this strategy doubles the number of Pods used by the Deployment,
    which will likely affect your resource usage. To minimize resource costs, you
    can scale the nonlive Deployment to `0` when you’re not currently doing a rollout
    and scale it back up to match the live version when you’re about to do a rollout.
    You’ll likely need to adjust the number of nodes in your cluster as well (covered
    in chapter 6).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此策略将 Deployment 使用的 Pod 数量翻倍，这可能会影响您的资源使用。为了最小化资源成本，当您当前没有进行发布时，可以将非活动
    Deployment 缩放到 `0`，当您即将进行发布时，将其缩放回与活动版本匹配。您可能还需要调整集群中的节点数量（在第 6 章中介绍）。
- en: 4.2.4 Choosing a rollout strategy
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 选择滚动发布策略
- en: For most Deployments, one of the built-in rollout strategies should suffice.
    Use `RollingUpdate` as an easy way to get zero-downtime updates on Kubernetes.
    To achieve zero downtime or disruption, you will also need to have a readiness
    check implemented; otherwise, traffic can be sent to your container before it
    has fully booted. Two versions of your application can serve traffic simultaneously,
    so you must design attributes like data formats with that in mind. Being able
    to support at least the current and previous version is generally good practice
    anyway, as it also allows you to roll back to the previous version if something
    goes wrong.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大多数 Deployment，内置的滚动发布策略应该足够。使用 `RollingUpdate` 作为在 Kubernetes 上实现零停机时间更新的简单方法。为了实现零停机时间或中断，您还需要实施就绪检查；否则，流量可能会在容器完全启动之前发送到您的容器。您的应用程序的两个版本可以同时处理流量，因此您必须考虑到这一点来设计属性，如数据格式。能够支持至少当前版本和上一个版本通常是良好的实践，因为它还允许您在出现问题的情况下回滚到上一个版本。
- en: '`Recreate` is a useful strategy when you really don’t want two application
    versions running at the same time. It can be useful for things like legacy single-instance
    services where only one copy can exist at a time.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 当您真的不希望同时运行两个应用程序版本时，`Recreate` 策略非常有用。它可以用于像遗留的单实例服务这样的东西，其中一次只能存在一个副本。
- en: Blue/green is an advanced-level strategy that requires additional tooling or
    processes but comes with the advantage of near-instant cutovers while offering
    the best of both worlds in that only one version is live at a time but without
    the downtime of the `Recreate` strategy. I recommend getting started with the
    in-built strategies but keep this one in mind for when you need something more.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿是一种高级策略，需要额外的工具或流程，但具有几乎即时切换的优势，同时提供了两个世界的最佳之处，即一次只有一个版本是活动的，但没有 `Recreate`
    策略的停机时间。我建议从内置策略开始，但记住当您需要更多功能时可以考虑这个策略。
- en: Summary
  id: totrans-171
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Kubernetes provides many tools to help you keep your deployments running and
    updated.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 提供了许多工具来帮助您保持您的部署运行和更新。
- en: It’s important to define health checks so that Kubernetes has the signals it
    needs to keep your application running by rebooting containers that are stuck
    or nonresponsive.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义健康检查很重要，这样 Kubernetes 就有信号来通过重启卡住或无响应的容器来保持您的应用程序运行。
- en: Liveness probes are used by Kubernetes to determine when your application needs
    restarting.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 使用存活探针来确定您的应用程序何时需要重启。
- en: The readiness probe governs which replicas receive traffic from the Service,
    which is particularly important during updates to prevent dropped requests.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 就绪探针控制哪些副本从服务接收流量，这在更新期间尤其重要，可以防止请求丢失。
- en: Kubernetes can also help update your application without downtime.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes 还可以帮助您在不中断服务的情况下更新应用程序。
- en: '`RollingUpdate` is the default rollout strategy in Kubernetes, giving you a
    zero-downtime rollout while using minimal additional resources.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`RollingUpdate` 是 Kubernetes 中的默认滚动发布策略，在最小化额外资源使用的同时，提供零停机时间的发布。'
- en: '`Recreate` is an alternative rollout strategy that does an in-place update
    with some downtime but no additional resource usage.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Recreate` 是一种替代的滚动发布策略，它通过一些停机时间进行就地更新，但不会使用额外的资源。'
- en: Blue/green is a rollout strategy that isn’t directly supported by Kubernetes
    but can still be performed using standard Kubernetes constructs.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署是一种Kubernetes不支持直接使用的滚动发布策略，但仍然可以通过标准的Kubernetes结构来执行。
- en: Blue/green offers some of the highest-quality guarantees but is more complex
    and temporarily doubles the resources needed by the deployment.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 蓝绿部署提供了一些最高品质的保证，但更为复杂，并且暂时将部署所需的资源数量翻倍。
