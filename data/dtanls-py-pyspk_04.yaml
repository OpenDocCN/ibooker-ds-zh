- en: 3 Submitting and scaling your first PySpark program
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 提交和扩展你的第一个 PySpark 程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Summarizing data using `groupby` and a simple aggregate function
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `groupby` 和简单的聚合函数总结数据
- en: Ordering results for display
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对结果进行排序以便显示
- en: Writing data from a data frame
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从数据框中写入数据
- en: Using `spark-submit` to launch your program in batch mode
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `spark-submit` 以批处理模式启动你的程序
- en: Simplifying PySpark writing using method chaining
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用方法链简化 PySpark 写入
- en: Scaling your program to multiple files at once
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将程序扩展到多个文件
- en: 'Chapter 2 dealt with all the data preparation work for our word frequency program.
    We *read* the input data, *tokenized* each word, and *cleaned* our records to
    only keep lowercase words. If we bring out our outline, we only have steps 4 and
    5 to complete:'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章处理了我们单词频率程序的所有数据准备工作。我们 *读取* 输入数据，*分词* 每个单词，并 *清理* 记录以仅保留小写单词。如果我们拿出我们的大纲，我们只需要完成步骤
    4 和 5：
- en: '**[DONE]***Read*: Read the input data (we’re assuming a plain text file).'
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取：读取输入数据（我们假设是一个纯文本文件）。'
- en: '**[DONE]***Token*: Tokenize each word.'
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***分词*：对每个单词进行分词。'
- en: '**[DONE]***Clean*: Remove any punctuation and/or tokens that aren’t words.
    Lowercase each word.'
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***清理：移除任何标点符号和/或非单词标记。将每个单词转换为小写。'
- en: '*Count*: Count the frequency of each word present in the text.'
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*：计算文本中每个单词的频率。'
- en: '*Answer:* Return the top 10 (or 20, 50, 100).'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*：返回前 10（或 20、50、100）个。'
- en: After tackling those two last steps, we look at packaging our code in a single
    file to be able to submit it to Spark without having to launch a REPL. We also
    take a look at our completed program and at simplifying it by removing intermediate
    variables. We finish with scaling our program to accommodate more data sources.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在解决最后两个步骤之后，我们看看将代码打包到一个文件中以便提交给 Spark，而无需启动 REPL。我们还看一下我们的完整程序，并探讨通过删除中间变量来简化它。我们以扩展程序以适应更多数据源结束。
- en: '3.1 Grouping records: Counting word frequencies'
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 记录分组：计算单词频率
- en: If you take our data frame in the same shape as it was at the end of chapter
    2 (you can find the code in a single file in the book’s code repository at `code/Ch02/end_of_
    chapter.py`), there is just a little more work to be done. With a data frame containing
    a single word per record, we just have to count the word occurrences and take
    the top contenders. This section shows you how to count records using the `GroupedData`
    object and perform an aggregation function—here, counting the items—on each group.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用与第二章结尾相同形状的数据框（你可以在书的代码仓库中的单个文件 `code/Ch02/end_of_ chapter.py` 中找到代码），只需做一点额外的工作。对于包含每条记录一个单词的数据框，我们只需计算单词出现次数并选择最频繁的单词。本节将展示如何使用
    `GroupedData` 对象来计数记录并执行聚合函数——在这里，是计数项——在每个组上。
- en: 'Intuitively, we count the number of each word by creating *groups*: one for
    each word. Once those groups are formed, we can perform an *aggregation function*
    on each one of them. In this specific case, we count the number of records for
    each group, which will give us the number of occurrences for each word in the
    data frame. Under the hood, PySpark represents a grouped data frame in a `GroupedData`
    object; think of it as a transitional object that awaits an aggregation function
    to become a transformed data frame.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地，我们通过创建 *组* 来计数每个单词的数量：每个单词一个组。一旦这些组形成，我们就可以对每个组执行 *聚合函数*。在这个特定的情况下，我们计算每个组的记录数，这将给我们数据框中每个单词的出现次数。在底层，PySpark
    使用 `GroupedData` 对象表示分组数据框；将其视为一个过渡对象，等待聚合函数将其转换为转换后的数据框。
- en: '![](../Images/03-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03-01.png)'
- en: Listing 3.1 A schematic representation of our `groups` object. Each small box
    represents a record.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 `groups` 对象的示意图。每个小框代表一条记录。
- en: The easiest way to count record occurrence is to use the `groupby()` method,
    passing the columns we wish to group as a parameter. The `groupby()` method in
    listing 3.1 returns a `GroupedData` and awaits further instructions. Once we apply
    the `count()` method, we get back a data frame containing the grouping column
    `word`, as well as the `count` column containing the number of occurrences for
    each word.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 计算记录出现次数的最简单方法是使用 `groupby()` 方法，将我们希望分组的列作为参数传递。列表 3.1 中的 `groupby()` 方法返回一个
    `GroupedData` 并等待进一步指令。一旦我们应用 `count()` 方法，我们就会得到一个包含分组列 `word` 以及包含每个单词出现次数的
    `count` 列的数据框。
- en: Listing 3.1 Counting word frequencies using `groupby()` and `count()`
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 使用 `groupby()` 和 `count()` 计算单词频率
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Peeking at the `results` data frame in listing 3.1, we see that the results
    are in no specific order. As a matter of fact, I’d be very surprised if you had
    the exact same order of words that I do! This has to do with how PySpark manages
    data. In chapter 1, we learned that PySpark distributes the data across multiple
    nodes. When performing a grouping function, such as `groupby()`, each worker performs
    the work on its assigned data. `groupby()` and `count()` are transformations,
    so PySpark will queue them lazily until we request an action. When we pass the
    `show` method to our results data frame, it triggers the chain of computation
    that we see in figure 3.2.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 查看列表3.1中的`results`数据帧，我们发现结果没有特定的顺序。事实上，如果你和我有完全相同的单词顺序，我会非常惊讶！这与PySpark管理数据的方式有关。在第1章中，我们了解到PySpark将数据分布到多个节点。当执行分组函数，如`groupby()`时，每个工作节点在其分配的数据上执行工作。`groupby()`和`count()`是转换操作，因此PySpark会懒惰地将它们排队，直到我们请求一个动作。当我们向结果数据帧传递`show`方法时，它触发了我们在图3.2中看到的计算链。
- en: '![](../Images/03-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-02.png)'
- en: Listing 3.2 A distributed group by on our `words_nonull` data frame. The work
    is performed in a distributed fashion until we need to assemble the results in
    a cohesive display via `show()`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2：在`words_nonull`数据帧上执行分布式分组。工作以分布式方式执行，直到我们需要通过`show()`方法组装结果以进行连贯显示。
- en: Tip If you need to create groups based on the values of multiple columns, you
    can pass multiple columns as parameters to `groupby()`. We see this in action
    in chapter 5.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你需要根据多个列的值创建组，可以将多个列作为参数传递给`groupby()`。我们将在第5章中看到这个操作的实例。
- en: 'Because Spark is lazy, it doesn’t care about the order of records unless we
    explicitly ask it to. Since we wish to see the top words on display, let’s put
    a little order in our data frame and, at the same time, complete the last step
    of our program: return the top word frequencies.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Spark是懒惰的，它不关心记录的顺序，除非我们明确要求它。由于我们希望在显示中看到最常出现的单词，让我们在我们的数据帧中添加一些顺序，同时完成我们程序的最后一步：返回最常出现的单词频率。
- en: Exercise 3.1
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3.1
- en: Starting with the `word_nonull` seen in this section, which of the following
    expressions would return the number of words per letter count (e.g., there are
    *X* one-letter words, *Y* two-letter words, etc.)?
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节中看到的`word_nonull`开始，以下哪个表达式会返回每个字母计数的单词数量（例如，有*X*个单字母单词，*Y*个双字母单词等）？
- en: Assume that `pyspark.sql.functions.col,` `pyspark.sql.functions.length` are
    imported.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 假设`pyspark.sql.functions.col,`和`pyspark.sql.functions.length`已被导入。
- en: a) `words_nonull.select(length(col("word"))).groupby("length").count()`
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: a) `words_nonull.select(length(col("word"))).groupby("length").count()`
- en: b) `words_nonull.select(length(col("word")).alias("length")).groupby("length").count()`
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: b) `words_nonull.select(length(col("word")).alias("length")).groupby("length").count()`
- en: c) `words_nonull.groupby("length").select("length").count()`
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: c) `words_nonull.groupby("length").select("length").count()`
- en: d) None of those options would work.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: d) 这些选项中没有一个会工作。
- en: 3.2 Ordering the results on the screen using orderBy
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 使用orderBy在屏幕上排序结果
- en: 'In 3.1, we explained why PySpark doesn’t necessarily maintain an order of records
    when performing transformations. If we look at our five-step blueprint, the last
    step is to return the top *N* records for different values of *N*. We already
    know how to show a specific number of records, so this section focuses on ordering
    the records in a data frame before displaying them:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在3.1中，我们解释了为什么PySpark在执行转换时不一定维护记录的顺序。如果我们查看我们的五步蓝图，最后一步是返回不同*N*值的*N*条记录。我们已经知道如何显示特定数量的记录，所以本节重点介绍在显示之前对数据帧中的记录进行排序：
- en: '**[DONE]***Read*: Read the input data (we’re assuming a plain text file).'
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取*：读取输入数据（我们假设是一个纯文本文件）。'
- en: '**[DONE]***Token*: Tokenize each word.'
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***标记*：对每个单词进行标记化。'
- en: '**[DONE]***Clean*: Remove any punctuation and/or tokens that aren’t words.
    Lowercase each word.'
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***清理*：移除任何标点符号和/或非单词标记。将每个单词转换为小写。'
- en: '**[DONE]***Count*: Count the frequency of each word present in the text.'
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***计数*：计算文本中每个单词出现的频率。'
- en: '*Answer:* Return the top 10 (or 20, 50, 100).'
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*：返回前10个（或20个、50个、100个）。'
- en: 'Just like we use `groupby()` to group a data frame by the values in one or
    many columns, we use `orderBy()` to order a data frame by the values of one or
    many columns. PySpark provides two different syntaxes to order records:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们使用`groupby()`按一列或多列的值对数据帧进行分组一样，我们使用`orderBy()`按一列或多列的值对数据帧进行排序。PySpark提供了两种不同的语法来排序记录：
- en: We can provide the column names as parameters, with an optional `ascending`
    parameter. By default, we order a data frame in ascending order; by setting `ascending`
    to false, we reverse the order, getting the largest values first.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将列名作为参数提供，还可以有一个可选的`ascending`参数。默认情况下，我们按升序对数据框进行排序；通过将`ascending`设置为false，我们可以反转顺序，首先得到最大的值。
- en: Or we can use the `Column` object directly, via the `col` function. When we
    want to reverse the ordering, we use the `desc()` method on the column.
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者，我们可以通过`col`函数直接使用`Column`对象。当我们想要反转排序时，我们使用列上的`desc()`方法。
- en: PySpark orders the data frame using each column, one at a time. If you pass
    multiple columns (see chapter 5), PySpark uses the first column’s values to order
    the data frame, then the second (and then third, etc.) when there are identical
    values. Since we have a single column—and no duplicates because of `groupby()`—the
    application of `orderBy()` in the next listing is simple, regardless of the syntax
    we pick.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark按列顺序对数据框进行排序，一次一列。如果您传递多个列（见第5章），PySpark将使用第一列的值来排序数据框，然后是第二列（然后是第三列，等等），当有相同值时。由于我们只有一个列——由于`groupby()`而没有重复——因此，在下一个列表中应用`orderBy()`很简单，无论我们选择哪种语法。
- en: Listing 3.2 Displaying the top 10 words in Jane Austen’s *Pride and Prejudice*
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 显示简·奥斯汀的《傲慢与偏见》中的前10个单词
- en: '[PRE1]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'The list is very unsurprising: even though we can’t argue with Austen’s vocabulary,
    she isn’t immune to the fact that the English language needs pronouns and other
    common words. In natural language processing, those words are called *stop words*
    and could be removed. We solved our original query and can rest easy. Should you
    want to get the top 20, top 50, or even top 1,000, it’s easily done by changing
    the parameter to `show()`.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表非常不出所料：尽管我们无法质疑奥斯汀的词汇量，但她无法避免这样一个事实，即英语语言需要代词和其他常用词。在自然语言处理中，这些词被称为*停用词*，可以被移除。我们解决了原始查询，可以安心休息了。如果您想获取前20名、前50名，甚至前1000名，只需更改参数为`show()`即可。
- en: PySpark’s method naming convention zoo
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的方法命名约定动物园
- en: If you are detail-oriented, you might have noticed we used `groupby` (lowercase),
    but `orderBy` (lowerCamelCase, where you capitalize the first letter of each word
    but the first word). This seems like an odd design choice.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您注重细节，您可能会注意到我们使用了`groupby`（小写），但`orderBy`（小驼峰式，每个单词的首字母大写，但第一个单词除外）。这似乎是一个奇怪的设计选择。
- en: '`groupby()` is an alias for `groupBy()`, just like `where()` is an alias of
    `filter()`. I guess that the PySpark developers found that a lot of typing mistakes
    were avoided by accepting the two cases. `orderBy()` didn’t have that luxury,
    for a reason that escapes my understanding, so we need to be mindful of this.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`groupby()`是`groupBy()`的别名，就像`where()`是`filter()`的别名一样。我想PySpark的开发者发现，通过接受这两种情况，可以避免很多打字错误。`orderBy()`没有这样的奢侈，原因超出了我的理解，因此我们需要注意这一点。'
- en: 'Part of this incoherence is due to Spark’s heritage. Scala prefers camelCase
    for methods. On the other hand, we saw `regexp_extract`, which uses Python’s preferred
    snake_case (words separated by an underscore) in chapter 2\. There is no magic
    secret here: you’ll have to be mindful of the different case conventions at play
    in PySpark.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分不一致的部分是由于Spark的遗产。Scala更喜欢驼峰式命名方法。另一方面，我们在第2章中看到了`regexp_extract`，它使用Python首选的蛇形命名法（单词由下划线分隔）。这里没有魔法秘诀：您必须注意PySpark中正在使用的不同大小写约定。
- en: Showing results on the screen is great for a quick assessment, but most of the
    time you’ll want them to have some sort of longevity. It’s much better to save
    those results to a file so that we’ll be able to reuse them without having to
    compute everything each time. The next section covers writing a data frame to
    a file.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在屏幕上显示结果对于快速评估来说很棒，但大多数时候您希望它们有一定的持久性。将结果保存到文件中会更好，这样我们就可以在不每次都进行计算的情况下重用它们。下一节将介绍将数据框写入文件。
- en: Exercise 3.2
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 练习3.2
- en: Why isn’t the order preserved in the following code block?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么以下代码块中的顺序没有被保留？
- en: '[PRE2]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 3.3 Writing data from a data frame
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 从数据框写入数据
- en: Having the data on the screen is great for interactive development, but you’ll
    often want to export your results. For this, we write our results in a *comma-separated
    value* (CSV) file. I chose this format because it’s a human-readable format, meaning
    that we can review the results of our operations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在屏幕上拥有数据对于交互式开发来说很棒，但您通常会想要导出结果。为此，我们将结果写入*逗号分隔值*（CSV）文件。我选择这种格式，因为它是一种人类可读的格式，这意味着我们可以审查我们操作的结果。
- en: Just like we use `read()` and the `SparkReader` to read data in Spark, we use
    `write()` and the `SparkWriter` object to write back our data frame to disk. In
    listing 3.3, I specialize the `SparkWriter` to export text into a CSV file, naming
    the output `simple_count.csv`. If we look at the results, we can see that PySpark
    didn’t create a results.csv file. Instead, it created a directory of the same
    name, and put 201 files inside the directory (200 CSVs + 1 _SUCCESS file).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在 Spark 中使用 `read()` 和 `SparkReader` 来读取数据一样，我们使用 `write()` 和 `SparkWriter`
    对象将我们的数据框写回到磁盘。在列表 3.3 中，我将 `SparkWriter` 特化以将文本导出为 CSV 文件，输出命名为 `simple_count.csv`。如果我们查看结果，我们可以看到
    PySpark 没有创建一个 results.csv 文件。相反，它创建了一个同名目录，并在目录中放置了 201 个文件（200 个 CSV 文件 + 1
    个 _SUCCESS 文件）。
- en: Listing 3.3 Writing our results in multiple CSV files, one per partition
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 在多个 CSV 文件中写入我们的结果，每个分区一个
- en: '[PRE3]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The results are written in a directory called simple_count.csv.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 结果被写入一个名为 simple_count.csv 的目录中。
- en: ❷ The _SUCCESS file means the operation was successful.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ _SUCCESS 文件意味着操作成功。
- en: ❸ We have part-00000 to part-00199, which means our results are split across
    200 files.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们有 part-00000 到 part-00199，这意味着我们的结果分布在 200 个文件中。
- en: There it is, folks! The first moment where we have to care about PySpark’s distributed
    nature. Just like PySpark will distribute the transformation work across multiple
    workers, it’ll do the same for writing data. While it might look like a nuisance
    for our simple program, it is tremendously useful when working in distributed
    environments. When you have a large cluster of nodes, having many smaller files
    makes it easy to logically distribute reading and writing the data, making it
    way faster than having a single massive file.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，朋友们！这是我们需要关注 PySpark 分布式特性的第一个时刻。就像 PySpark 会将转换工作分布到多个工作节点上一样，它也会在写入数据时做同样的事情。虽然对于我们的简单程序来说可能看起来有些麻烦，但在分布式环境中工作时会非常有用。当你有一个由大量节点组成的大集群时，拥有许多较小的文件使得逻辑上分布读取和写入数据变得容易，这使得它比拥有一个单一的巨大文件要快得多。
- en: By default, PySpark will give you one file per partition. This means that our
    program, as run on my machine, yields 200 partitions at the end. This isn’t the
    best for portability. To reduce the number of partitions, we apply the `coalesce()`
    method with the desired number of partitions. The next listing shows the difference
    when using `coalesce(1)` on our data frame before writing to disk. We still get
    a directory, but there is a single CSV file inside of it. Mission accomplished!
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，PySpark 会为每个分区提供一个文件。这意味着在我机器上运行的我们的程序最终产生了 200 个分区。这并不利于可移植性。为了减少分区的数量，我们应用
    `coalesce()` 方法并指定所需的分区数。下一个列表显示了在使用 `coalesce(1)` 在写入磁盘之前对数据框进行操作时的差异。我们仍然得到一个目录，但里面只有一个
    CSV 文件。任务完成！
- en: Listing 3.4 Writing our results under a single partition
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 在单个分区下写入我们的结果
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note You might have realized that we’re not ordering the file before writing
    it. Since our data here is pretty small, we could have written the words by decreasing
    order of frequency. If you have a large data set, this operation will be quite
    expensive. Furthermore, since reading is a potentially distributed operation,
    what guarantees that it’ll get read the same way? Never assume that your data
    frame will keep the same ordering of records unless you explicitly ask via `orderBy()`
    right before the showing step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可能已经意识到我们在写入文件之前并没有对文件进行排序。由于我们这里的数据相当小，我们可以按频率递减的顺序写入单词。如果你有一个大的数据集，这个操作将会相当昂贵。此外，由于读取是一个可能分布的操作，有什么保证它会以相同的方式被读取？除非你通过在显示步骤之前显式地使用
    `orderBy()` 来请求，否则永远不要假设你的数据框会保持记录相同的顺序。
- en: Our workflow has been pretty interactive so far. We write one or two lines of
    text before showing the result to the terminal. As we get more and more confident
    with operating on the data frame’s structure, those showings will become fewer.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的工作流程相当交互式。我们在向终端显示结果之前先写入一行或两行文本。随着我们对操作数据框结构的信心越来越强，这些显示将变得越来越少。
- en: Now that we’ve performed all the necessary steps interactively, let’s look at
    putting our program in a single file and at refactoring opportunities.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经交互式地执行了所有必要的步骤，让我们看看将我们的程序放入一个单独的文件和重构的机会。
- en: '3.4 Putting it all together: Counting'
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 将所有内容整合起来：计数
- en: Interactive development is fantastic for the rapid iteration of our code. When
    developing programs, it’s great to experiment and validate our thoughts through
    rapid code inputs to a shell. When the experimentation is over, it’s good to bring
    our program into a cohesive body of code. This section takes all the code we have
    written in this chapter and chapter 2, and brings it into something runnable end
    to end.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 交互式开发非常适合我们代码的快速迭代。在开发程序时，通过快速向 shell 输入代码来实验和验证我们的想法是非常好的。当实验结束后，将我们的程序整合成一段连贯的代码是很好的。本节将本章和第
    2 章中我们编写的所有代码整合成一段可运行的代码。
- en: The REPL allows you to go back in history using the directional arrows on your
    keyboard, just like a regular Python REPL. To make things a bit easier, I am providing
    the step-by-step program in the next listing. This section is dedicated to streamlining
    and making our code more succinct and readable.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: REPL 允许您使用键盘上的方向箭头回溯历史，就像常规的 Python REPL 一样。为了使事情变得更容易一些，我在下一个列表中提供了逐步的程序。本节致力于简化我们的代码，使其更加简洁和易读。
- en: Listing 3.5 Our first PySpark program, dubbed “Counting Jane Austen”
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 我们的第一段 PySpark 程序，被称为“计数简·奥斯汀”
- en: '[PRE5]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This program runs perfectly if you paste its entirety into the `pyspark` shell.
    With everything in the same file, we can make our code more friendly and make
    it easier for future you to come back to it. First, we adopt common import conventions
    when working with PySpark. We then rearrange our code to make it more readable,
    as seen in chapter 1.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将整个程序粘贴到 `pyspark` shell 中，程序将完美运行。将所有内容放在同一个文件中，我们可以使我们的代码更加友好，并使其更容易为未来的您所理解。首先，我们采用与
    PySpark 一起工作时常用的导入约定。然后，我们重新排列代码，使其更易于阅读，正如第 1 章中所示。
- en: 3.4.1 Simplifying your dependencies with PySpark’s import conventions
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 使用 PySpark 的导入约定简化依赖
- en: This section covers the general conventions when using PySpark modules. We review
    the most relevant import—the transformation function—and how qualifying it helps
    with knowing what is coming from where.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了使用 PySpark 模块时的通用约定。我们回顾了最相关的导入——转换函数——以及如何通过限定导入来帮助我们了解内容来自哪里。
- en: This program uses five distinct functions from the `pyspark.sql.functions` modules.
    We should probably replace this with a qualified import, which is Python’s way
    of importing a module by assigning a keyword to it. While there is no hard rule,
    the common wisdom is to use `F` to refer to PySpark’s functions. The next listing
    shows the before and after.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这个程序使用了来自 `pyspark.sql.functions` 模块中的五个不同的函数。我们可能需要将其替换为有条件的导入，这是 Python 通过为模块分配一个关键字来导入模块的方式。虽然没有硬性规则，但普遍的智慧是使用
    `F` 来指代 PySpark 的函数。下一个列表显示了前后变化。
- en: Listing 3.6 Simplifying our PySpark functions import
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 简化我们的 PySpark 函数导入
- en: '[PRE6]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Since `col`, `explode`, `lower`, `regexp_extract`, and `split` are all in `pyspark.sql.functions`,
    we can import the whole module. Since the new import statement imports the entirety
    of the `pyspark.sql.functions` module, we assign the keyword (or key letter) `F`.
    The PySpark community seems to have implicitly settled on using `F` for `pyspark.sql.functions`,
    and I encourage you to do the same. It’ll make your programs consistent, and since
    many functions in the module share their name with pandas or Python built-in functions,
    you’ll avoid name clashes. Each function application in the program will then
    be prefixed by `F`, just like with regular Python-qualified imports.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `col`、`explode`、`lower`、`regexp_extract` 和 `split` 都在 `pyspark.sql.functions`
    中，我们可以导入整个模块。由于新的导入语句导入了 `pyspark.sql.functions` 模块的全部内容，我们将关键字（或关键字母）赋值为 `F`。PySpark
    社区似乎已经隐式地决定使用 `F` 来指代 `pyspark.sql.functions`，我鼓励您也这样做。这将使您的程序保持一致性，并且由于模块中的许多函数与
    pandas 或 Python 内置函数共享名称，您将避免名称冲突。程序中的每个函数应用都将以 `F` 为前缀，就像常规的 Python-qualified
    导入一样。
- en: Warning It can be very tempting to start an import like `from` `pyspark.sql
    .functions` `import` `*`. Do not fall into that trap! It’ll make it hard for your
    readers to know which functions come from PySpark and which come from regular
    Python. In chapter 8, where we’ll use user-defined functions (UDFs), this separation
    will become even more important. This is a good coding hygiene rule!
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：开始导入时使用 `from` `pyspark.sql .functions` `import` `*` 可能非常诱人！不要陷入这个陷阱！这将使读者难以知道哪些函数来自
    PySpark，哪些来自常规 Python。在第 8 章中，我们将使用用户定义函数（UDFs），这种分离将变得更加重要。这是一条好的编码卫生规则！
- en: In the subsequent chapters, and more specifically chapter 6, I introduce other
    functionality that warrants its qualified import. Whether you choose to import
    functions one by one or the whole qualified module depends on your use case; I
    usually value consistency over terseness and prefer using a qualified import for
    data transformation API.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在随后的章节中，特别是第 6 章，我将介绍其他需要合格导入的功能。你选择逐个导入函数还是导入整个合格模块取决于你的用例；我通常更重视一致性而不是简洁性，并倾向于使用合格导入进行数据转换
    API。
- en: 'We simplified our program’s preamble; let’s now focus on where the action is
    by simplifying our program flow using one of my favorite aspects of PySpark: its
    chaining abilities.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们简化了程序的前置部分；现在让我们通过使用 PySpark 的我最喜欢的特性之一——它的链式能力——来简化我们的程序流程。
- en: 3.4.2 Simplifying our program via method chaining
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 通过方法链简化我们的程序
- en: 'If we look at the transformation methods we applied to our data frames (`select()`,
    `where()`, `groupBy()`, and `count()`), they all have something in common: they
    take a structure as a parameter—the data frame or `GroupedData` in the case of
    `count()`—and return a structure. All transformations can be seen as pipes that
    ingest a structure and return a modified structure. This section will look at
    method chaining and how it makes a program less verbose and thus easier to read
    by eliminating intermediate variables.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们看看我们应用在我们数据框上的转换方法（`select()`、`where()`、`groupBy()` 和 `count()`），它们都有一些共同点：它们都接受一个结构作为参数——在
    `count()` 的情况下是 `GroupedData` 数据框——并返回一个结构。所有转换都可以看作是管道，它们消耗一个结构并返回一个修改后的结构。本节将探讨方法链以及它是如何通过消除中间变量来使程序更简洁、更易于阅读的。
- en: 'Our program uses intermediate variables quite a lot: every time we perform
    a transformation, we assign the result to a new variable. This is useful when
    using the shell as we keep the state of our transformation and can peek at our
    work at the end of every step. On the other hand, once our program works, this
    multiplication of variables is not as useful and can visually clutter our program.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序中使用了大量的中间变量：每次我们执行转换时，我们都将结果赋值给一个新的变量。当使用 shell 时这很有用，因为我们保持转换的状态，并且可以在每一步结束时查看我们的工作。另一方面，一旦我们的程序运行起来，这种变量的倍增就不再那么有用，并且可能会使我们的程序在视觉上变得杂乱。
- en: 'In PySpark, every transformation returns an object, which is why we need to
    assign a variable to the result. This means that PySpark doesn’t perform modifications
    *in place*. For instance, the following code block by itself in a program wouldn’t
    do anything because we don’t assign the result to a variable. In a REPL, on the
    other hand, you would get the return value printed as output, so this would count
    as work:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 中，每个转换都返回一个对象，这就是为什么我们需要将结果赋值给一个变量的原因。这意味着 PySpark 不执行就地修改。例如，以下代码块在程序中单独使用时不会做任何事情，因为我们没有将结果赋值给变量。另一方面，在
    REPL 中，你会得到打印出来的返回值作为输出，所以这可以算作工作：
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can avoid intermediate variables by *chaining* the results of one method
    to the next. Since each transformation returns a data frame (or `GroupedData`,
    when we perform the `groupby()` method), we can directly append the next method
    without assigning the result to a variable. This means that we can eschew all
    but one variable assignment. The code in the next listing shows the before and
    after. Note that we also added the `F` prefix to our functions to respect the
    import convention we outlined in section 3.4.1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将一个方法的结果链接到下一个方法来避免中间变量。由于每个转换都返回一个数据框（或者当我们执行 `groupby()` 方法时是 `GroupedData`），我们可以直接附加下一个方法，而不需要将结果赋值给一个变量。这意味着我们可以避免除了一个变量赋值之外的所有赋值。下一列表中的代码展示了前后变化。注意，我们还添加了
    `F` 前缀到我们的函数中，以尊重我们在 3.4.1 节中概述的导入约定。
- en: Listing 3.7 Removing intermediate variables by chaining transformation methods
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 通过链式转换方法移除中间变量
- en: '[PRE8]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'It’s like night and day: the “after” is much more terse and readable, and we’re
    able to easily follow the list of steps. Visually, we can also see the difference
    in figure 3.3.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像是白天和黑夜：修改后的代码更加简洁易读，我们能够轻松地跟随步骤列表。从视觉上，我们也可以在图 3.3 中看到这种差异。
- en: '![](../Images/03-03.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03-03.png)'
- en: Listing 3.3 Method chaining eliminates the need for intermediate variables.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 方法链消除了对中间变量的需求。
- en: I am not saying that intermediate variables are evil and are to be avoided.
    But they can hinder your code readability, so you have to make sure they serve
    a purpose. A lot of burgeoning PySpark developers make it a habit of always writing
    on top of the same variable. While not dangerous in itself, it makes the code
    redundant and harder to reason about. If you see yourself doing something like
    the first two lines of the next listing, chain your methods. You’ll get the same
    result and more aesthetically pleasing code.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我并不是说中间变量是邪恶的，应该避免使用。但它们可能会阻碍你的代码可读性，所以你必须确保它们有存在的意义。许多初出茅庐的PySpark开发者养成了总是在同一个变量上写代码的习惯。虽然这本身并不危险，但它使得代码冗余，更难以推理。如果你发现自己正在做类似下一列表的前两行的事情，请链式调用你的方法。你将得到相同的结果，并且代码看起来更美观。
- en: Listing 3.8 Chaining for writing over the same variable
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 链式操作以覆盖相同变量
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Instead of doing this . . .
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 不要这样做 . . .
- en: ❷ . . . you can do this—no variable repetition!
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 你可以这样做——没有变量重复！
- en: Make your life easier by using Python’s parentheses
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用Python的括号使你的生活更轻松
- en: 'If you look at the “after” code in listing 3.7, you’ll notice that I start
    the right side of the equal sign with an opening parenthesis (`spark` `=` `(`
    `[...]`). This is a trick I use when I need to chain methods in Python. If you
    don’t wrap your result into a pair of parentheses, you’ll need to add a `\` character
    at the end of each line, which adds visual noise to your program. PySpark code
    is especially prone to line breaks when you use method chaining:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看列表3.7中的“之后”代码，你会注意到我以一个开括号(`spark` `=` `(` `[...]`)开始等号右边。这是一个我在需要链式调用Python中的方法时使用的技巧。如果你不将你的结果包裹在一对括号中，你需要在每一行的末尾添加一个`\`字符，这会给你的程序增加视觉噪音。PySpark代码在链式调用方法时尤其容易发生行中断：
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: As a lazy alternative, I am a big fan of using Black as a Python code formatting
    tool ([https://black.readthedocs.io/](https://black.readthedocs.io/)). It removes
    a lot of the guesswork involved in having your code logically laid out and consistent.
    Since we read code more than we write it, readability matters.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一种懒惰的替代方案，我非常喜欢使用Black作为Python代码格式化工具([https://black.readthedocs.io/](https://black.readthedocs.io/))。它消除了在代码逻辑布局和一致性方面所需的大量猜测工作。由于我们读代码比写代码多，可读性很重要。
- en: Since we are performing two actions on `results` (displaying the top 10 words
    on the screen and writing the data frame to a CSV file), we have to use a variable.
    If you only have one action to perform on your data frame, you can channel your
    inner code golfer[¹](#pgfId-1015507) by not using any variable name. Most of the
    time, I prefer lumping my transformations together and keeping the action visually
    separate, like we are doing now.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在`results`上执行了两个操作（在屏幕上显示前10个单词并将数据帧写入CSV文件），我们必须使用一个变量。如果你只需要对你的数据帧执行一个操作，你可以通过不使用任何变量名来发挥你内心的代码高尔夫选手[¹](#pgfId-1015507)的精神。大多数时候，我更喜欢将我的转换组合在一起，并将操作在视觉上分开，就像我们现在所做的那样。
- en: Our program is looking much more polished now. The last step will be to add
    PySpark’s plumbing to prepare it for batch mode.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的项目现在看起来更加精致了。最后一步将是添加PySpark的管道，以便为批量模式做准备。
- en: 3.5 Using spark-submit to launch your program in batch mode
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.5 使用spark-submit以批量模式启动你的程序
- en: If we start PySpark with the `pyspark` program, the launcher takes care of creating
    the `SparkSession` for us. In chapter 2, we started from a basic Python REPL,
    so we created our entry point and named it `spark`. This section takes our program
    and submits it in batch mode. It is the equivalent of running a Python script;
    if you only need the result and not the REPL, this will do the trick.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用`pyspark`程序启动PySpark，启动器会为我们创建`SparkSession`。在第2章中，我们从基本的Python REPL开始，因此我们创建了我们的入口点并将其命名为`spark`。本节将我们的程序以批量模式提交。这相当于运行一个Python脚本；如果你只需要结果而不需要REPL，这将是一个好方法。
- en: Unlike the interactive REPL, where the choice of language triggers the program
    to run, as in listing 3.10, we see that Spark provides a single program, named
    `spark-submit`, to submit Spark (Scala, Java, SQL), PySpark (Python), and SparkR
    (R) programs. The full code for our program is available on the book’s repository
    under `code/Ch02/word_count_submit.py`.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 与交互式REPL不同，在REPL中，语言的选择会触发程序的运行，就像列表3.10中所示，我们看到Spark提供了一个名为`spark-submit`的单个程序，用于提交Spark（Scala、Java、SQL）、PySpark（Python）和SparkR（R）程序。我们程序的完整代码可以在本书的仓库中的`code/Ch02/word_count_submit.py`下找到。
- en: Listing 3.9 Submitting our job in batch mode
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.9 提交我们的作业到批量模式
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Tip If you get a deluge of `INFO` messages, don’t forget that you have control
    over this: use `spark.sparkContext.setLogLevel("WARN")` right after your `spark`
    definition. If your local configuration has `INFO` as a default, you’ll still
    get a slew of messages until it catches this line, but it won’t obscure your results.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：如果你收到大量的`INFO`消息，别忘了你对此有控制权：在定义`spark`之后立即使用`spark.sparkContext.setLogLevel("WARN")`。如果你的本地配置默认为`INFO`，你仍然会收到一系列消息，直到它捕获到这一行，但不会遮挡你的结果。
- en: 'Once this step is completed, we’re done! Our program successfully *ingests*
    the book, *transforms* it into a cleaned list of word frequencies, and then *exports*
    it two ways: as a top-10 list on the screen and as a CSV file.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成这一步，我们就完成了！我们的程序成功地将书籍*摄入*，将其*转换*为清洗后的单词频率列表，然后以两种方式*导出*：屏幕上的前10名列表和CSV文件。
- en: If we look at our process, we applied one transformation interactively at the
    time, `show()`-ing the process after each one. This will often be your modus operandi
    when working with a new data file. Once you’re confident about a block of code,
    you can remove the intermediate variables. Out of the box, PySpark gives you a
    productive environment to explore large data sets interactively and provides an
    expressive and terse vocabulary to manipulate data. It’s also easy to go from
    interactive development to batch deployment—you just have to define your `SparkSession`,
    if you haven’t already, and you’re good to go.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们审视我们的过程，我们一次交互式地应用一个转换，每次转换后都使用`show()`来显示过程。当你处理一个新的数据文件时，这通常会是你的操作模式。一旦你对一段代码有信心，你可以移除中间变量。PySpark默认为你提供了一个高效的环境，可以交互式地探索大型数据集，并提供了一种表达性和简洁的词汇来操作数据。它还很容易从交互式开发过渡到批量部署——你只需定义你的`SparkSession`（如果你还没有定义的话），然后就可以开始了。
- en: 3.6 What didn’t happen in this chapter
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.6 本章没有发生的事情
- en: Chapter 2 and 3 were pretty dense. We learned how to read text data, process
    it to answer any question, display the results on the screen, and write them to
    a CSV file. On the other hand, there are many elements we left out on purpose.
    Let’s quickly look at what we *didn’t* do in this chapter.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章和第三章内容相当密集。我们学习了如何读取文本数据，将其处理以回答任何问题，在屏幕上显示结果，并将它们写入CSV文件。另一方面，我们故意省略了许多元素。让我们快速看一下在本章中我们没有做什么。
- en: Except for coalescing the data frame to write it into a single file, we didn’t
    do much with the distribution of the data. We saw in chapter 1 that PySpark distributes
    data across multiple worker nodes, but our code didn’t pay much attention to this.
    Not having to constantly think about partitions, data locality, and fault tolerance
    made our data discovery process much faster.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 除了将数据帧合并到一个文件中以便写入外，我们对数据的分布没有做太多处理。我们在第一章中看到，PySpark会将数据分布到多个工作节点上，但我们的代码并没有过多关注这一点。不必不断思考分区、数据局部性和容错性，使得我们的数据发现过程变得更快。
- en: We didn’t spend much time configuring PySpark. Other than providing a name for
    our application, no additional configuration was inputted in our `SparkSession`.
    It’s not to say we’ll never broach this, but we can start with a bare-bones configuration
    and tweak as we go. The subsequent chapters will customize the `SparkSession`
    to optimize resources (chapter 11) or create connectors to external data repositories
    (chapter 9).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有花太多时间配置PySpark。除了为我们的应用程序提供一个名称外，我们在`SparkSession`中没有输入任何额外的配置。这并不是说我们永远不会触及这一点，但我们可以从一个基本的配置开始，并在过程中进行调整。后续章节将定制`SparkSession`以优化资源（第11章）或创建连接到外部数据存储库的连接器（第9章）。
- en: Finally, we didn’t obsess about planning the order of operations as it relates
    to processing, focusing instead on readability and logic. We made a point to describe
    our transformations as logically as they appear to us, and we’re letting Spark
    optimize this into efficient processing steps. We could potentially reorder some
    and get the same output, but our program reads well, is easy to reason about,
    and works correctly.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们没有过分关注与处理相关的操作顺序规划，而是专注于可读性和逻辑。我们确保以逻辑上尽可能清晰的方式描述我们的转换，并让Spark优化这些步骤为高效的处理步骤。我们可能重新排列一些步骤并得到相同的结果，但我们的程序可读性好，易于推理，并且运行正确。
- en: 'This echoes the statement I made in chapter 1: PySpark is remarkable not only
    in what it provides, but also in what it can abstract over. You most often can
    write your code as a sequence of transformations that will get you to your destination
    most of the time. For those cases where you want a more finely tuned performance
    or more control over the physical layout of your data, we’ll see in part 3 that
    PySpark won’t hold you back. Because Spark is in constant evolution, there are
    still cases where you need to be a little more careful about how your program
    translates to physical execution on the cluster. For this, chapter 11 covers the
    Spark UI, which shows you the work being performed on your data and how you can
    influence processing.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我在第 1 章中做出的声明相呼应：PySpark 不仅在它提供的内容上令人瞩目，而且在它能够抽象的内容上也是如此。你通常可以将你的代码编写为一系列转换，这些转换大多数情况下都能带你到达目的地。对于那些想要更精细的性能或更多控制数据物理布局的情况，在第
    3 部分中我们将看到 PySpark 不会阻碍你。因为 Spark 持续发展，仍然有一些情况下你需要对你的程序如何转换为集群上的物理执行更加小心。为此，第
    11 章涵盖了 Spark UI，它显示了正在你的数据上执行的工作以及你如何影响处理。
- en: 3.7 Scaling up our word frequency program
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.7 扩展我们的词频程序
- en: That example wasn’t big data. I’ll be the first to say it.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 那个例子并不是大数据。我首先得承认这一点。
- en: Teaching big data processing has a catch-22\. While I want to show the power
    of PySpark to work with massive data sets, I don’t want you to purchase a cluster
    or rack up a massive cloud bill. It’s easier to show you the ropes using a smaller
    set of data, knowing that we can scale using the same code.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 教授大数据处理有一个二难困境。虽然我想展示 PySpark 与大规模数据集一起工作的能力，但我不想让你购买一个集群或产生巨大的云费用。使用较小的数据集展示会更简单，因为我们知道我们可以使用相同的代码进行扩展。
- en: 'Let’s take our word-counting example: How can we scale this to a larger corpus
    of text? Let’s download more files from Project Gutenberg and place them in the
    same directory:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以我们的词数示例为例：我们如何将其扩展到更大的文本语料库？让我们从 Project Gutenberg 下载更多文件并将它们放在同一个目录中：
- en: '[PRE12]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: While this is not enough to claim “we’re doing big data,” it’ll be enough to
    explain the general concept. If you want to scale, you can use appendix B to provision
    a powerful cluster on the cloud, download more books or other text files, and
    run the same program for a few dollars.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这不足以宣称“我们在做大数据”，但它足以解释一般概念。如果你想扩展，你可以使用附录 B 在云上配置一个强大的集群，下载更多书籍或其他文本文件，并以几美元的价格运行相同的程序。
- en: 'We modify our `word_count_submit.py` in a very subtle way. Where we `.read.text()`,
    we’ll change the path to account for all files in the directory. The next listing
    shows the before and after: we are only changing the `1342-0.txt` to a `*.txt`,
    which is called a *glob pattern*. The `*` means that Spark selects all the `.txt`
    files in the directory.'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以非常微妙的方式修改了我们的 `word_count_submit.py`。在 `.read.text()` 的地方，我们将路径更改为包含目录中的所有文件。接下来的列表显示了前后变化：我们只将
    `1342-0.txt` 改为 `*.txt`，这被称为 *glob 模式*。`*` 表示 Spark 选择目录中的所有 `.txt` 文件。
- en: Listing 3.10 Scaling our word count program using the glob pattern
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.10 使用 glob 模式扩展我们的词数程序
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Here we have a single file passed as a parameter . . .
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这里我们传递了一个参数的单个文件 . . .
- en: ❷ . . . and here the star (or glob) picks all the text files within the directory.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ . . . 并且在这里，星号（或球体）选择目录内的所有文本文件。
- en: Note You can also just pass the name of the directory if you want PySpark to
    ingest all the files within the directory.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你想让 PySpark 读取目录内的所有文件，你也可以只传递目录的名称。
- en: The results of running the program over all the files in the directory are available
    in the following listing.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下列表中提供了在目录中运行程序的所有文件的结果。
- en: Listing 3.11 Results of scaling our program to multiple files
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.11 扩展我们的程序到多个文件的结果
- en: '[PRE14]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: With this, you can confidently say that you can scale a simple data analysis
    program using PySpark. You can use the general formula we’ve outlined here and
    modify some of the parameters and methods to fit your use case. Chapters 4 and
    5 will dig a little deeper into some interesting and common data transformations,
    building on what we’ve learned here.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方法，你可以自信地说，你可以使用 PySpark 扩展一个简单的数据分析程序。你可以使用我们在这里概述的通用公式，并修改一些参数和方法以适应你的用例。第
    4 章和第 5 章将进一步深入探讨一些有趣且常见的数据转换，基于我们在这里学到的内容。
- en: Summary
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: You can group records using the `groupby` method, passing the column names you
    want to group against as a parameter. This returns a `GroupedData` object that
    waits for an aggregation method to return the results of computation over the
    groups, such as the `count()` of records.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用`groupby`方法对记录进行分组，将你想要分组的列名作为参数传递。这将返回一个`GroupedData`对象，该对象等待聚合方法返回对组进行计算的结果，例如记录的`count()`。
- en: PySpark’s repertoire of functions that operates on columns is located in `pyspark.sql.functions`.
    The unofficial but well-respected convention is to qualify this import in your
    program using the `F` keyword.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark中操作列的功能集合位于`pyspark.sql.functions`。虽然这不是官方规定，但被广泛尊重的做法是在你的程序中使用`F`关键字来限定这个导入。
- en: When writing a data frame to a file, PySpark will create a directory and put
    one file per partition. If you want to write a single file, use the `coaslesce(1)`
    method.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当将数据帧写入文件时，PySpark将创建一个目录，并为每个分区创建一个文件。如果你想写入单个文件，请使用`coalesce(1)`方法。
- en: To prepare your program to work in batch mode via `spark-submit`, you need to
    create a `SparkSession`. PySpark provides a builder pattern in the `pyspark.sql`
    module.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了使你的程序通过`spark-submit`以批处理模式运行，你需要创建一个`SparkSession`。PySpark在`pyspark.sql`模块中提供了一个构建器模式。
- en: If your program needs to scale across multiple files within the same directory,
    you can use a glob pattern to select many files at once. PySpark will collect
    them in a single data frame.
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的程序需要在同一目录下的多个文件中进行扩展，你可以使用glob模式一次性选择多个文件。PySpark将它们收集到一个单独的数据帧中。
- en: Additional Exercises
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 补充练习
- en: For these exercises, you’ll need the `word_count_submit.py` program we worked
    on in this chapter. You can pick it from the book’s code repository (`Code/Ch03/word_`
    `count_submit.py`).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些练习，你需要我们在本章中工作的`word_count_submit.py`程序。你可以从本书的代码仓库（`Code/Ch03/word_` `count_submit.py`）中获取它。
- en: Exercise 3.3
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.3
- en: 'By modifying the `word_count_submit.py` program, return the number of distinct
    words in Jane Austen’s *Pride and Prejudice*. (Hint: `results` contains one record
    for each unique word.)'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过修改`word_count_submit.py`程序，返回简·奥斯汀的《*傲慢与偏见*》中不同单词的数量。（提示：`results`包含每个唯一单词的一条记录。）
- en: (Challenge) Wrap your program in a function that takes a file name as a parameter.
    It should return the number of distinct words.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: （挑战）将你的程序封装在一个函数中，该函数接受一个文件名作为参数。它应该返回不同单词的数量。
- en: Exercise 3.4
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.4
- en: Taking `word_count_submit.py`, modify the script to return a sample of five
    words that appear only once in Jane Austen’s *Pride and Prejudice*.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`word_count_submit.py`，修改脚本以返回简·奥斯汀的《*傲慢与偏见*》中只出现一次的五个单词的样本。
- en: Exercise 3.5
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.5
- en: Using the `substring` function (refer to PySpark’s API or the `pyspark` shell
    if needed), return the top five most popular first letters (keep only the first
    letter of each word).
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`substring`函数（如有需要，请参考PySpark的API或`pyspark` shell），返回最常见的五个首字母（只保留每个单词的首字母）。
- en: 'Compute the number of words starting with a consonant or a vowel. (Hint: The
    `isin()` function might be useful.)'
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算以辅音或元音开头的单词数量。（提示：`isin()`函数可能很有用。）
- en: Exercise 3.6
  id: totrans-155
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习3.6
- en: Let’s say you want to get both the `count()` and `sum()` of a `GroupedData`
    object. Why doesn’t this code work? Map the inputs and outputs of each method.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要获取`GroupedData`对象的`count()`和`sum()`。为什么这段代码不起作用？映射每个方法的输入和输出。
- en: '[PRE15]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Multiple aggregate function applications will be covered in chapter 4.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 第4章将涵盖多个聚合函数的应用。
- en: '* * *'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ Writing a program using the lowest possible number of characters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 使用尽可能少的字符编写程序。
