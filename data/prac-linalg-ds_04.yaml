- en: Chapter 4\. Vector Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç¬¬å››ç« ã€‚å‘é‡åº”ç”¨
- en: While working through the previous two chapters, you may have felt that some
    of the material was esoteric and abstract. Perhaps you felt that the challenge
    of learning linear algebra wouldnâ€™t pay off in understanding real-world applications
    in data science and machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨é˜…è¯»å‰ä¸¤ç« çš„è¿‡ç¨‹ä¸­ï¼Œä½ å¯èƒ½è§‰å¾—å…¶ä¸­ä¸€äº›å†…å®¹è¿‡äºæ·±å¥¥å’ŒæŠ½è±¡ã€‚ä¹Ÿè®¸ä½ æ„Ÿåˆ°å­¦ä¹ çº¿æ€§ä»£æ•°çš„æŒ‘æˆ˜åœ¨ç†è§£æ•°æ®ç§‘å­¦å’Œæœºå™¨å­¦ä¹ ä¸­çš„å®é™…åº”ç”¨æ–¹é¢å¹¶æ²¡æœ‰å¤šå¤§å¸®åŠ©ã€‚
- en: I hope that this chapter dispels you of these doubts. In this chapter, you will
    learn how vectors and vector operations are used in data science analyses. And
    you will be able to extend this knowledge by working through the exercises.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›æœ¬ç« èƒ½æ¶ˆé™¤ä½ çš„è¿™äº›ç–‘è™‘ã€‚åœ¨æœ¬ç« ä¸­ï¼Œä½ å°†å­¦ä¹ å‘é‡åŠå…¶æ“ä½œå¦‚ä½•åœ¨æ•°æ®ç§‘å­¦åˆ†æä¸­ä½¿ç”¨ã€‚é€šè¿‡å®Œæˆç»ƒä¹ ï¼Œä½ å°†èƒ½å¤Ÿæ‰©å±•è¿™äº›çŸ¥è¯†ã€‚
- en: Correlation and Cosine Similarity
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ç›¸å…³æ€§å’Œä½™å¼¦ç›¸ä¼¼åº¦
- en: Correlation is one of the most fundamental and important analysis methods in
    statistics and machine learning. A *correlation coefficient* is a single number
    that quantifies the linear relationship between two variables. Correlation coefficients
    range from âˆ’1 to +1, with âˆ’1 indicating a perfect negative relationship, +1 a
    perfect positive relationships, and 0 indicating no linear relationship. [FigureÂ 4-1](#fig_4_1)
    shows a few examples of pairs of variables and their correlation coefficients.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å…³æ€§æ˜¯ç»Ÿè®¡å­¦å’Œæœºå™¨å­¦ä¹ ä¸­æœ€åŸºæœ¬å’Œé‡è¦çš„åˆ†ææ–¹æ³•ä¹‹ä¸€ã€‚*ç›¸å…³ç³»æ•°*æ˜¯ä¸€ä¸ªå•ä¸€æ•°å­—ï¼Œç”¨äºé‡åŒ–ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„çº¿æ€§å…³ç³»ã€‚ç›¸å…³ç³»æ•°çš„èŒƒå›´ä» âˆ’1 åˆ° +1ï¼Œå…¶ä¸­
    âˆ’1 è¡¨ç¤ºå®Œç¾çš„è´Ÿç›¸å…³ï¼Œ+1 è¡¨ç¤ºå®Œç¾çš„æ­£ç›¸å…³ï¼Œè€Œ 0 è¡¨ç¤ºæ²¡æœ‰çº¿æ€§å…³ç³»ã€‚[å›¾4-1](#fig_4_1)å±•ç¤ºäº†å‡ å¯¹å˜é‡åŠå…¶ç›¸å…³ç³»æ•°çš„ç¤ºä¾‹ã€‚
- en: '![Correlation examples](assets/plad_0401.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![ç›¸å…³æ€§ç¤ºä¾‹](assets/plad_0401.png)'
- en: Figure 4-1\. Examples of data exhibiting positive correlation, negative correlation,
    and zero correlation. The lower-right panel illustrates that correlation is a
    linear measure; nonlinear relationships between variables can exist even if their
    correlation is zero.
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾4-1ã€‚å±•ç¤ºäº†è¡¨ç°å‡ºæ­£ç›¸å…³ã€è´Ÿç›¸å…³å’Œé›¶ç›¸å…³çš„æ•°æ®ç¤ºä¾‹ã€‚å³ä¸‹æ–¹çš„é¢æ¿è¯´æ˜äº†ç›¸å…³æ€§æ˜¯çº¿æ€§æµ‹é‡ï¼›å³ä½¿å˜é‡ä¹‹é—´çš„ç›¸å…³æ€§ä¸ºé›¶ï¼Œå®ƒä»¬ä¹‹é—´ä¹Ÿå¯èƒ½å­˜åœ¨éçº¿æ€§å…³ç³»ã€‚
- en: 'In [ChapterÂ 2](ch02.xhtml#Chapter_2), I mentioned that the dot product is involved
    in the correlation coefficient, and that the magnitude of the dot product is related
    to the magnitude of the numerical values in the data (remember the discussion
    about using grams versus pounds for measuring weight). Therefore, the correlation
    coefficient requires some normalizations to be in the expected range of âˆ’1 to
    +1\. Those two normalizations are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨[ç¬¬äºŒç« ](ch02.xhtml#Chapter_2)ä¸­ï¼Œæˆ‘æåˆ°ç‚¹ç§¯æ¶‰åŠåˆ°ç›¸å…³ç³»æ•°ï¼Œå¹¶ä¸”ç‚¹ç§¯çš„å¤§å°ä¸æ•°æ®ä¸­æ•°å€¼çš„å¤§å°ç›¸å…³ï¼ˆè®°å¾—æˆ‘ä»¬è®¨è®ºè¿‡ä½¿ç”¨å…‹ä¸ç£…æ¥è¡¡é‡é‡é‡ï¼‰ã€‚å› æ­¤ï¼Œç›¸å…³ç³»æ•°éœ€è¦ä¸€äº›æ ‡å‡†åŒ–ä»¥ä¿æŒåœ¨é¢„æœŸèŒƒå›´å†…çš„
    âˆ’1 åˆ° +1 ä¹‹é—´ã€‚è¿™ä¸¤ç§æ ‡å‡†åŒ–æ–¹æ³•æ˜¯ï¼š
- en: Mean center each variable
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹æ¯ä¸ªå˜é‡è¿›è¡Œå‡å€¼ä¸­å¿ƒåŒ–
- en: '*Mean centering* means to subtract the average value from each data value.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*å‡å€¼ä¸­å¿ƒåŒ–*æ„å‘³ç€ä»æ¯ä¸ªæ•°æ®å€¼ä¸­å‡å»å¹³å‡å€¼ã€‚'
- en: Divide the dot product by the product of the vector norms
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å°†ç‚¹ç§¯é™¤ä»¥å‘é‡èŒƒæ•°çš„ä¹˜ç§¯
- en: This divisive normalization cancels the measurement units and scales the maximum
    possible correlation magnitude to |1|.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§é™¤æ³•å¼æ ‡å‡†åŒ–å–æ¶ˆäº†æµ‹é‡å•ä½å¹¶å°†æœ€å¤§å¯èƒ½çš„ç›¸å…³æ€§å¹…åº¦ç¼©æ”¾ä¸º|1|ã€‚
- en: '[Equation 4-1](#pearson-corr1) shows the full formula for the Pearson correlation
    coefficient.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[æ–¹ç¨‹4-1](#pearson-corr1)å±•ç¤ºäº†çš®å°”é€Šç›¸å…³ç³»æ•°çš„å®Œæ•´å…¬å¼ã€‚'
- en: Equation 4-1\. Formula for Pearson correlation coefficient
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹4-1ã€‚çš®å°”é€Šç›¸å…³ç³»æ•°çš„å…¬å¼
- en: <math alttext="rho equals StartFraction sigma-summation Underscript i equals
    1 Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar
    right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis
    Over StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis x Subscript i Baseline minus x overbar right-parenthesis squared
    EndRoot StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis squared
    EndRoot EndFraction" display="block"><mrow><mi>Ï</mi> <mo>=</mo> <mfrac><mrow><msubsup><mo>âˆ‘</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>y</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><msqrt><mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt> <msqrt><mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="rho equals StartFraction sigma-summation Underscript i equals
    1 Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar
    right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis
    Over StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis x Subscript i Baseline minus x overbar right-parenthesis squared
    EndRoot StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis squared
    EndRoot EndFraction" display="block"><mrow><mi>Ï</mi> <mo>=</mo> <mfrac><mrow><msubsup><mo>âˆ‘</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>y</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><msqrt><mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt> <msqrt><mrow><msubsup><mo>âˆ‘</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>Â¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'It may not be obvious that the correlation is simply three dot products. [Equation
    4-2](#pearson-corr2) shows this same formula rewritten using the linear algebra
    dot-product notation. In this equation, <math alttext="bold x overTilde"><mover
    accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover></math> is the mean-centered version
    of <math alttext="bold x"><mi>ğ±</mi></math> (that is, variable <math alttext="bold
    x"><mi>ğ±</mi></math> with normalization #1 applied).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ–è®¸ä¸å¤ªæ˜æ˜¾çš„æ˜¯ï¼Œç›¸å…³æ€§å…¶å®å°±æ˜¯ä¸‰ä¸ªç‚¹ç§¯ã€‚[æ–¹ç¨‹4-2](#pearson-corr2)å±•ç¤ºäº†ä½¿ç”¨çº¿æ€§ä»£æ•°ç‚¹ç§¯ç¬¦å·é‡å†™çš„ç›¸åŒå…¬å¼ã€‚åœ¨è¿™ä¸ªæ–¹ç¨‹ä¸­ï¼Œ<math
    alttext="bold x overTilde"><mover accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover></math>
    æ˜¯<math alttext="bold x"><mi>ğ±</mi></math>çš„å‡å€¼ä¸­å¿ƒåŒ–ç‰ˆæœ¬ï¼ˆå³åº”ç”¨äº†æ ‡å‡†åŒ–æ–¹æ³•#1çš„å˜é‡<math alttext="bold
    x"><mi>ğ±</mi></math>ï¼‰ã€‚
- en: Equation 4-2\. The Pearson correlation expressed in the parlance of linear algebra
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: æ–¹ç¨‹4-2ã€‚ç”¨çº¿æ€§ä»£æ•°æœ¯è¯­è¡¨ç¤ºçš„çš®å°”é€Šç›¸å…³æ€§
- en: <math alttext="rho equals StartFraction bold x overTilde Superscript upper T
    Baseline bold y overTilde Over parallel-to bold x overTilde parallel-to parallel-to
    bold y overTilde parallel-to EndFraction" display="block"><mrow><mi>Ï</mi> <mo>=</mo>
    <mfrac><mrow><msup><mover accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover> <mtext>T</mtext></msup>
    <mover accent="true"><mi>ğ²</mi> <mo>Ëœ</mo></mover></mrow> <mrow><mrow><mo>âˆ¥</mo></mrow><mover
    accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover><mrow><mo>âˆ¥</mo><mo>âˆ¥</mo></mrow><mover
    accent="true"><mi>ğ²</mi> <mo>Ëœ</mo></mover><mrow><mo>âˆ¥</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="rho equals StartFraction bold x overTilde Superscript upper T
    Baseline bold y overTilde Over parallel-to bold x overTilde parallel-to parallel-to
    bold y overTilde parallel-to EndFraction" display="block"><mrow><mi>Ï</mi> <mo>=</mo>
    <mfrac><mrow><msup><mover accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover> <mtext>T</mtext></msup>
    <mover accent="true"><mi>ğ²</mi> <mo>Ëœ</mo></mover></mrow> <mrow><mrow><mo>âˆ¥</mo></mrow><mover
    accent="true"><mi>ğ±</mi> <mo>Ëœ</mo></mover><mrow><mo>âˆ¥</mo><mo>âˆ¥</mo></mrow><mover
    accent="true"><mi>ğ²</mi> <mo>Ëœ</mo></mover><mrow><mo>âˆ¥</mo></mrow></mrow></mfrac></mrow></math>
- en: 'So there you go: the famous and widely used Pearson correlation coefficient
    is simply the dot product between two variables, normalized by the magnitudes
    of the variables. (By the way, you can also see from this formula that if the
    variables are unit normed such that <math alttext="parallel-to bold x parallel-to
    equals parallel-to bold y parallel-to equals 1"><mrow><mo>âˆ¥</mo> <mi>ğ±</mi> <mo>âˆ¥</mo>
    <mo>=</mo> <mo>âˆ¥</mo> <mi>ğ²</mi> <mo>âˆ¥</mo> <mo>=</mo> <mn>1</mn></mrow></math>
    , then their correlation equals their dot product. (Recall from [Exercise 2-6](ch02.xhtml#exercise_2_6)
    that <math alttext="parallel-to bold x parallel-to equals StartRoot bold x Superscript
    upper T Baseline bold x EndRoot"><mrow><mrow><mo>âˆ¥</mo> <mi>ğ±</mi> <mo>âˆ¥</mo></mrow>
    <mo>=</mo> <msqrt><mrow><msup><mi>ğ±</mi> <mtext>T</mtext></msup> <mi>ğ±</mi></mrow></msqrt></mrow></math>
    .)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: æ‰€ä»¥ï¼Œè‘—åä¸”å¹¿æ³›ä½¿ç”¨çš„Pearsonç›¸å…³ç³»æ•°ç®€å•åœ°æ˜¯ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç‚¹ç§¯ï¼Œç”±å˜é‡çš„å¤§å°å½’ä¸€åŒ–å¾—åˆ°ã€‚ï¼ˆé¡ºä¾¿è¯´ä¸€å¥ï¼Œæ‚¨è¿˜å¯ä»¥ä»è¿™ä¸ªå…¬å¼çœ‹å‡ºï¼Œå¦‚æœå˜é‡è¢«å•ä½å½’ä¸€åŒ–ï¼Œä½¿å¾—<math
    alttext="parallel-to bold x parallel-to equals parallel-to bold y parallel-to
    equals 1"><mrow><mo>âˆ¥</mo> <mi>ğ±</mi> <mo>âˆ¥</mo> <mo>=</mo> <mo>âˆ¥</mo> <mi>ğ²</mi>
    <mo>âˆ¥</mo> <mo>=</mo> <mn>1</mn></mrow></math>ï¼Œåˆ™å®ƒä»¬çš„ç›¸å…³æ€§ç­‰äºå®ƒä»¬çš„ç‚¹ç§¯ã€‚ï¼ˆå›æƒ³ä¸€ä¸‹ï¼Œæ¥è‡ª[Exercise
    2-6](ch02.xhtml#exercise_2_6)ï¼Œå…¶ä¸­<math alttext="parallel-to bold x parallel-to
    equals StartRoot bold x Superscript upper T Baseline bold x EndRoot"><mrow><mrow><mo>âˆ¥</mo>
    <mi>ğ±</mi> <mo>âˆ¥</mo></mrow> <mo>=</mo> <msqrt><mrow><msup><mi>ğ±</mi> <mtext>T</mtext></msup>
    <mi>ğ±</mi></mrow></msqrt></mrow></math>ã€‚ï¼‰
- en: 'Correlation is not the only way to assess similarity between two variables.
    Another method is called *cosine similarity*. The formula for cosine similarity
    is simply the geometric formula for the dot product ([Equation 2-11](ch02.xhtml#dp-geom)),
    solved for the cosine term:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: ç›¸å…³æ€§ä¸æ˜¯è¯„ä¼°ä¸¤ä¸ªå˜é‡ç›¸ä¼¼æ€§çš„å”¯ä¸€æ–¹æ³•ã€‚å¦ä¸€ç§æ–¹æ³•ç§°ä¸º*ä½™å¼¦ç›¸ä¼¼åº¦*ã€‚ä½™å¼¦ç›¸ä¼¼åº¦çš„å…¬å¼åªæ˜¯ç‚¹ç§¯çš„å‡ ä½•å½¢å¼ï¼ˆ[Equation 2-11](ch02.xhtml#dp-geom)ï¼‰ï¼Œè§£å‡ºä½™å¼¦é¡¹ï¼š
- en: <math alttext="cosine left-parenthesis theta Subscript x comma y Baseline right-parenthesis
    equals StartFraction alpha Over parallel-to bold x parallel-to parallel-to bold
    y parallel-to EndFraction" display="block"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <msub><mi>Î¸</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mi>Î±</mi> <mrow><mo>âˆ¥</mo><mi>ğ±</mi><mo>âˆ¥</mo><mo>âˆ¥</mo><mi>ğ²</mi><mo>âˆ¥</mo></mrow></mfrac></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="cosine left-parenthesis theta Subscript x comma y Baseline right-parenthesis
    equals StartFraction alpha Over parallel-to bold x parallel-to parallel-to bold
    y parallel-to EndFraction" display="block"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <msub><mi>Î¸</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mi>Î±</mi> <mrow><mo>âˆ¥</mo><mi>ğ±</mi><mo>âˆ¥</mo><mo>âˆ¥</mo><mi>ğ²</mi><mo>âˆ¥</mo></mrow></mfrac></mrow></math>
- en: where <math alttext="alpha"><mi>Î±</mi></math> is the dot product between <math
    alttext="bold x"><mi>ğ±</mi></math> and <math alttext="bold y"><mi>ğ²</mi></math>
    .
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="alpha"><mi>Î±</mi></math>æ˜¯<math alttext="bold x"><mi>ğ±</mi></math>å’Œ<math
    alttext="bold y"><mi>ğ²</mi></math>çš„ç‚¹ç§¯ã€‚
- en: It may seem like correlation and cosine similarity are exactly the same formula.
    However, remember that [Equation 4-1](#pearson-corr1) is the full formula, whereas
    [Equation 4-2](#pearson-corr2) is a simplification under the assumption that the
    variables have already been mean centered. Thus, cosine similarity does not involve
    the first normalization factor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: çœ‹èµ·æ¥ç›¸å…³æ€§å’Œä½™å¼¦ç›¸ä¼¼æ€§çš„å…¬å¼å®Œå…¨ç›¸åŒã€‚ä½†è¯·è®°ä½ï¼Œ[Equation 4-1](#pearson-corr1)æ˜¯å®Œæ•´çš„å…¬å¼ï¼Œè€Œ[Equation 4-2](#pearson-corr2)æ˜¯åœ¨å˜é‡å·²ç»è¢«å‡å€¼ä¸­å¿ƒåŒ–çš„å‡è®¾ä¸‹çš„ç®€åŒ–ã€‚å› æ­¤ï¼Œä½™å¼¦ç›¸ä¼¼åº¦ä¸æ¶‰åŠç¬¬ä¸€ä¸ªå½’ä¸€åŒ–å› å­ã€‚
- en: 'From this section, you can understand why the Pearson correlation and cosine
    similarity reflects the *linear* relationship between two variables: they are
    based on the dot product, and the dot product is a linear operation.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ä»æœ¬èŠ‚å¯ä»¥ç†è§£ä¸ºä»€ä¹ˆPearsonç›¸å…³ç³»æ•°å’Œä½™å¼¦ç›¸ä¼¼åº¦åæ˜ äº†ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„*çº¿æ€§*å…³ç³»ï¼šå®ƒä»¬åŸºäºç‚¹ç§¯ï¼Œè€Œç‚¹ç§¯æ˜¯çº¿æ€§æ“ä½œã€‚
- en: There are four coding exercises associated with this section, which appear at
    the end of the chapter. You can choose whether you want to solve those exercises
    before reading the next section, or continue reading the rest of the chapter and
    then work through the exercises. (My personal recommendation is the former, but
    you are the master of your linear algebra destiny!)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬èŠ‚æœ‰å››ä¸ªç¼–ç¨‹ç»ƒä¹ ï¼Œä½äºç« èŠ‚æœ«å°¾ã€‚æ‚¨å¯ä»¥é€‰æ‹©åœ¨é˜…è¯»ä¸‹ä¸€èŠ‚ä¹‹å‰è§£å†³è¿™äº›ç»ƒä¹ ï¼Œæˆ–è€…ç»§ç»­é˜…è¯»æ•´ç« ç„¶åå†è§£å†³è¿™äº›ç»ƒä¹ ã€‚ï¼ˆæˆ‘ä¸ªäººæ¨èé€‰æ‹©å‰è€…ï¼Œä½†æ‚¨æ˜¯çº¿æ€§ä»£æ•°å‘½è¿çš„ä¸»å®°ï¼ï¼‰
- en: Time Series Filtering and Feature Detection
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: æ—¶é—´åºåˆ—è¿‡æ»¤å’Œç‰¹å¾æ£€æµ‹
- en: The dot product is also used in time series filtering. Filtering is essentially
    a feature-detection method, whereby a templateâ€”called a *kernel* in the parlance
    of filteringâ€”is matched against portions of a time series signal, and the result
    of filtering is another time series that indicates how much the characteristics
    of the signal match the characteristics of the kernel. Kernels are carefully constructed
    to optimize certain criteria, such as smooth fluctuations, sharp edges, particular
    waveform shapes, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ç‚¹ç§¯è¿˜ç”¨äºæ—¶é—´åºåˆ—è¿‡æ»¤ã€‚è¿‡æ»¤æœ¬è´¨ä¸Šæ˜¯ä¸€ç§ç‰¹å¾æ£€æµ‹æ–¹æ³•ï¼Œå…¶ä¸­æ¨¡æ¿â€”â€”åœ¨è¿‡æ»¤æœ¯è¯­ä¸­ç§°ä¸º*æ ¸*â€”â€”ä¸æ—¶é—´åºåˆ—ä¿¡å·çš„éƒ¨åˆ†åŒ¹é…ï¼Œè¿‡æ»¤çš„ç»“æœæ˜¯å¦ä¸€ä¸ªæ—¶é—´åºåˆ—ï¼ŒæŒ‡ç¤ºä¿¡å·ç‰¹å¾ä¸æ ¸ç‰¹å¾åŒ¹é…ç¨‹åº¦ã€‚æ ¸è¢«ç²¾å¿ƒæ„é€ ä»¥ä¼˜åŒ–ç‰¹å®šæ ‡å‡†ï¼Œå¦‚å¹³æ»‘æ³¢åŠ¨ã€é”åˆ©è¾¹ç¼˜ã€ç‰¹å®šæ³¢å½¢å½¢çŠ¶ç­‰ã€‚
- en: The mechanism of filtering is to compute the dot product between the kernel
    and the time series signal. But filtering usually requires *local* feature detection,
    and the kernel is typically much shorter than the entire time series. Therefore,
    we compute the dot product between the kernel and a short snippet of the data
    of the same length as the kernel. This procedure produces one time point in the
    filtered signal ([FigureÂ 4-2](#fig_4_2)), and then the kernel is moved one time
    step to the right to compute the dot product with a different (overlapping) signal
    segment. Formally, this procedure is called convolution and involves a few additional
    steps that Iâ€™m omitting to focus on the application of the dot product in signal
    processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿‡æ»¤çš„æœºåˆ¶æ˜¯è®¡ç®—æ ¸ä¸æ—¶é—´åºåˆ—ä¿¡å·ä¹‹é—´çš„ç‚¹ç§¯ã€‚ä½†é€šå¸¸è¿‡æ»¤éœ€è¦*å±€éƒ¨*ç‰¹å¾æ£€æµ‹ï¼Œè€Œæ ¸é€šå¸¸æ¯”æ•´ä¸ªæ—¶é—´åºåˆ—è¦çŸ­å¾—å¤šã€‚å› æ­¤ï¼Œæˆ‘ä»¬è®¡ç®—æ ¸ä¸ä¸æ ¸é•¿åº¦ç›¸åŒçš„æ•°æ®çš„ä¸€ä¸ªçŸ­ç‰‡æ®µä¹‹é—´çš„ç‚¹ç§¯ã€‚è¿™ä¸ªè¿‡ç¨‹äº§ç”Ÿè¿‡æ»¤ä¿¡å·ä¸­çš„ä¸€ä¸ªæ—¶é—´ç‚¹ï¼ˆ[å›¾Â 4-2](#fig_4_2)ï¼‰ï¼Œç„¶åå°†æ ¸å‘å³ç§»åŠ¨ä¸€ä¸ªæ—¶é—´æ­¥æ¥è®¡ç®—ä¸ä¸åŒï¼ˆé‡å ï¼‰ä¿¡å·æ®µçš„ç‚¹ç§¯ã€‚æ­£å¼åœ°è¯´ï¼Œè¿™ä¸ªè¿‡ç¨‹ç§°ä¸ºå·ç§¯ï¼Œæ¶‰åŠå‡ ä¸ªé¢å¤–çš„æ­¥éª¤ï¼Œæˆ‘åœ¨è¿™é‡Œçœç•¥ä»¥ä¾¿ä¸“æ³¨äºåœ¨ä¿¡å·å¤„ç†ä¸­åº”ç”¨ç‚¹ç§¯ã€‚
- en: '![Time series filtering](assets/plad_0402.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![æ—¶é—´åºåˆ—æ»¤æ³¢](assets/plad_0402.png)'
- en: Figure 4-2\. Illustration of time series filtering
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 4-2\. æ—¶é—´åºåˆ—æ»¤æ³¢ç¤ºä¾‹
- en: Temporal filtering is a major topic in science and engineering. Indeed, without
    temporal filtering there would be no music, radio, telecommunications, satellites,
    etc. And yet, the mathematical heart that keeps your music pumping is the vector
    dot product.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: æ—¶é—´æ»¤æ³¢æ˜¯ç§‘å­¦å’Œå·¥ç¨‹ä¸­çš„ä¸€ä¸ªé‡è¦ä¸»é¢˜ã€‚äº‹å®ä¸Šï¼Œæ²¡æœ‰æ—¶é—´æ»¤æ³¢å°±ä¸ä¼šæœ‰éŸ³ä¹ã€æ— çº¿ç”µã€ç”µä¿¡ã€å«æ˜Ÿç­‰ã€‚è€Œæ”¯æ’‘ä½ çš„éŸ³ä¹æ’­æ”¾çš„æ•°å­¦æ ¸å¿ƒæ˜¯å‘é‡ç‚¹ä¹˜ã€‚
- en: In the exercises at the end of the chapter, you will discover how dot products
    are used to detect features (edges) and to smooth time series data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æœ¬ç« æœ«å°¾çš„ç»ƒä¹ ä¸­ï¼Œä½ å°†äº†è§£åˆ°å¦‚ä½•åˆ©ç”¨ç‚¹ä¹˜æ¥æ£€æµ‹ç‰¹å¾ï¼ˆè¾¹ç¼˜ï¼‰å’Œå¹³æ»‘æ—¶é—´åºåˆ—æ•°æ®ã€‚
- en: '*k*-Means Clustering'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-Means Clustering'
- en: '*k-means clustering* is an unsupervised method of classifying multivariate
    data into a relatively small number of groups, or categories, based on minimizing
    distance to the group center.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*k-means clustering* æ˜¯ä¸€ç§æ— ç›‘ç£çš„æ–¹æ³•ï¼Œæ ¹æ®è·ç¦»æœ€å°åŒ–åŸåˆ™ï¼Œå°†å¤šå˜é‡æ•°æ®åˆ†æˆç›¸å¯¹è¾ƒå°‘çš„å‡ ç»„æˆ–ç±»åˆ«ã€‚'
- en: '*k*-means clustering is an important analysis method in machine learning, and
    there are sophisticated variants of *k*-means clustering. Here we will implement
    a simple version of *k*-means, with the goal of seeing how concepts about vectors
    (in particular: vectors, vector norms, and broadcasting) are used in the *k*-means
    algorithm.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means clustering æ˜¯æœºå™¨å­¦ä¹ ä¸­é‡è¦çš„åˆ†ææ–¹æ³•ï¼Œæœ‰å¤šç§å¤æ‚çš„*k*-means clustering å˜ä½“ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å°†å®ç°ä¸€ä¸ªç®€å•ç‰ˆæœ¬çš„*k*-meansï¼Œæ—¨åœ¨äº†è§£å‘é‡ï¼ˆç‰¹åˆ«æ˜¯ï¼šå‘é‡ã€å‘é‡èŒƒæ•°å’Œå¹¿æ’­ï¼‰åœ¨*k*-means
    ç®—æ³•ä¸­çš„åº”ç”¨ã€‚'
- en: 'Here is a brief description of the algorithm that we will write:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯æˆ‘ä»¬å°†è¦ç¼–å†™çš„ç®—æ³•çš„ç®€è¦æè¿°ï¼š
- en: Initialize *k* centroids as random points in the data space. Each centroid is
    a *class*, or category, and the next steps will assign each data observation to
    each class. (A *centroid* is a center generalized to any number of dimensions.)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†*k*ä¸ªè´¨å¿ƒåˆå§‹åŒ–ä¸ºæ•°æ®ç©ºé—´ä¸­çš„éšæœºç‚¹ã€‚æ¯ä¸ªè´¨å¿ƒéƒ½æ˜¯ä¸€ä¸ª*ç±»*æˆ–*ç±»åˆ«*ï¼Œæ¥ä¸‹æ¥çš„æ­¥éª¤å°†æŠŠæ¯ä¸ªæ•°æ®è§‚æµ‹å€¼åˆ†é…åˆ°æ¯ä¸ªç±»åˆ«ä¸­ã€‚ï¼ˆ*è´¨å¿ƒ*æ˜¯é€šç”¨åŒ–åˆ°ä»»æ„ç»´åº¦çš„ä¸­å¿ƒã€‚ï¼‰
- en: Compute the Euclidean distance between each data observation and each centroid.^([1](ch04.xhtml#idm45733310634320))
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¯ä¸ªæ•°æ®è§‚æµ‹å€¼ä¸æ¯ä¸ªè´¨å¿ƒä¹‹é—´çš„æ¬§æ°è·ç¦»ã€‚^([1](ch04.xhtml#idm45733310634320))
- en: Assign each data observation to the group with the closest centroid.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¯ä¸ªæ•°æ®è§‚æµ‹å€¼åˆ†é…ç»™æœ€è¿‘è´¨å¿ƒæ‰€åœ¨çš„ç»„ã€‚
- en: Update each centroid as the average of all data observations assigned to that
    centroid.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: å°†æ¯ä¸ªè´¨å¿ƒæ›´æ–°ä¸ºåˆ†é…ç»™è¯¥è´¨å¿ƒçš„æ‰€æœ‰æ•°æ®è§‚æµ‹å€¼çš„å¹³å‡å€¼ã€‚
- en: Repeat steps 2â€“4 until a convergence criteria is satisfied, or for *N* iterations.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: é‡å¤æ­¥éª¤ 2â€“4ï¼Œç›´åˆ°æ»¡è¶³æ”¶æ•›æ ‡å‡†æˆ–è¿›è¡Œ*N*æ¬¡è¿­ä»£ã€‚
- en: If you are comfortable with Python coding and would like to implement this algorithm,
    then I encourage you to do that before continuing. Next, we will work through
    the math and code for each of these steps, with a particular focus on using vectors
    and broadcasting in NumPy. We will also test the algorithm using randomly generated
    2D data to confirm that our code is correct.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœä½ å¯¹Pythonç¼–ç æ„Ÿåˆ°èˆ’é€‚ï¼Œå¹¶å¸Œæœ›å®ç°è¿™ä¸ªç®—æ³•ï¼Œé‚£ä¹ˆæˆ‘é¼“åŠ±ä½ åœ¨ç»§ç»­ä¹‹å‰è¿™æ ·åšã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å°†è¯¦ç»†è®²è§£æ¯ä¸ªæ­¥éª¤çš„æ•°å­¦å’Œä»£ç ï¼Œç‰¹åˆ«æ˜¯åœ¨NumPyä¸­ä½¿ç”¨å‘é‡å’Œå¹¿æ’­çš„æ¦‚å¿µã€‚æˆ‘ä»¬è¿˜å°†ä½¿ç”¨éšæœºç”Ÿæˆçš„äºŒç»´æ•°æ®æ¥æµ‹è¯•ç®—æ³•ï¼Œä»¥ç¡®è®¤æˆ‘ä»¬çš„ä»£ç æ˜¯æ­£ç¡®çš„ã€‚
- en: 'Letâ€™s start with step 1: initialize *k* random cluster centroids. *k* is a
    parameter of *k*-means clustering; in real data, it is difficult to determine
    the optimal *k*, but here we will fix *k* = 3\. There are several ways to initialize
    random cluster centroids; to keep things simple, I will randomly select *k* data
    samples to be centroids. The data are contained in variable `data` (this variable
    is 150 Ã— 2, corresponding to 150 observations and 2 features) and visualized in
    the upper-left panel of [FigureÂ 4-3](#fig_4_3) (the online code shows how to generate
    these data):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ä»ç¬¬1æ­¥å¼€å§‹ï¼šåˆå§‹åŒ–*k*ä¸ªéšæœºèšç±»ä¸­å¿ƒã€‚*k*æ˜¯*k*-å‡å€¼èšç±»çš„ä¸€ä¸ªå‚æ•°ï¼›åœ¨çœŸå®æ•°æ®ä¸­ï¼Œç¡®å®šæœ€ä½³*k*æ˜¯å›°éš¾çš„ï¼Œä½†åœ¨è¿™é‡Œæˆ‘ä»¬å°†*k*å›ºå®šä¸º3ã€‚æœ‰å‡ ç§åˆå§‹åŒ–éšæœºèšç±»ä¸­å¿ƒçš„æ–¹æ³•ï¼›ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘å°†éšæœºé€‰æ‹©*k*ä¸ªæ•°æ®æ ·æœ¬ä½œä¸ºèšç±»ä¸­å¿ƒã€‚æ•°æ®åŒ…å«åœ¨å˜é‡`data`ä¸­ï¼ˆæ­¤å˜é‡ä¸º150Ã—2ï¼Œå¯¹åº”150ä¸ªè§‚æµ‹å’Œ2ä¸ªç‰¹å¾ï¼‰ï¼Œå¹¶ä¸”åœ¨[å›¾Â 4-3](#fig_4_3)çš„å·¦ä¸Šæ–¹é¢æ¿ä¸­è¿›è¡Œäº†å¯è§†åŒ–ï¼ˆåœ¨çº¿ä»£ç æ˜¾ç¤ºå¦‚ä½•ç”Ÿæˆè¿™äº›æ•°æ®ï¼‰ï¼š
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now for step 2: compute the distance between each data observation and each
    cluster centroid. Here is where we use linear algebra concepts you learned in
    the previous chapters. For one data observation and centroid, Euclidean distance
    is computed as'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨åˆ°ç¬¬2æ­¥ï¼šè®¡ç®—æ¯ä¸ªæ•°æ®è§‚æµ‹ç‚¹ä¸æ¯ä¸ªèšç±»ä¸­å¿ƒä¹‹é—´çš„è·ç¦»ã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä½ åœ¨å‰å‡ ç« å­¦åˆ°çš„çº¿æ€§ä»£æ•°æ¦‚å¿µã€‚
- en: <math alttext="delta Subscript i comma j Baseline equals StartRoot left-parenthesis
    d Subscript i Superscript x Baseline minus c Subscript j Superscript x Baseline
    right-parenthesis squared plus left-parenthesis d Subscript i Superscript y Baseline
    minus c Subscript j Superscript y Baseline right-parenthesis squared EndRoot"
    display="block"><mrow><msub><mi>Î´</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi> <mi>x</mi></msubsup>
    <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>x</mi></msubsup> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi>
    <mi>y</mi></msubsup> <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>y</mi></msubsup>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="delta Subscript i comma j Baseline equals StartRoot left-parenthesis
    d Subscript i Superscript x Baseline minus c Subscript j Superscript x Baseline
    right-parenthesis squared plus left-parenthesis d Subscript i Superscript y Baseline
    minus c Subscript j Superscript y Baseline right-parenthesis squared EndRoot"
    display="block"><mrow><msub><mi>Î´</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi> <mi>x</mi></msubsup>
    <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>x</mi></msubsup> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi>
    <mi>y</mi></msubsup> <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>y</mi></msubsup>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: where <math alttext="delta Subscript i comma j"><msub><mi>Î´</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    indicates the distance from data observation *i* to centroid *j*, *d*^x[i] is
    feature *x* of the *i*th data observation, and *c*^x[j] is the *x*-axis coordinate
    of centroid *j*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: å…¶ä¸­<math alttext="delta Subscript i comma j"><msub><mi>Î´</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>è¡¨ç¤ºæ•°æ®è§‚æµ‹*i*åˆ°èšç±»ä¸­å¿ƒ*j*çš„è·ç¦»ï¼Œ*d*^x[i]æ˜¯ç¬¬*i*ä¸ªæ•°æ®è§‚æµ‹çš„*x*ç‰¹å¾ï¼Œ*c*^x[j]æ˜¯èšç±»ä¸­å¿ƒ*j*çš„*x*è½´åæ ‡ã€‚
- en: 'You might think that this step needs to be implemented using a double `for`
    loop: one loop over *k* centroids and a second loop over *N* data observations
    (you might even think of a third `for` loop over data features). However, we can
    use vectors and broadcasting to make this operation compact and efficient. This
    is an example of how linear algebra often looks different in equations compared
    to in code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½è®¤ä¸ºè¿™ä¸€æ­¥éœ€è¦ä½¿ç”¨åŒé‡`for`å¾ªç¯æ¥å®ç°ï¼šä¸€ä¸ªå¾ªç¯ç”¨äº*k*ä¸ªèšç±»ä¸­å¿ƒï¼Œç¬¬äºŒä¸ªå¾ªç¯ç”¨äº*N*ä¸ªæ•°æ®è§‚æµ‹ï¼ˆä½ ç”šè‡³å¯èƒ½è€ƒè™‘ç¬¬ä¸‰ä¸ª`for`å¾ªç¯ç”¨äºæ•°æ®ç‰¹å¾ï¼‰ã€‚ç„¶è€Œï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å‘é‡å’Œå¹¿æ’­æ¥ä½¿è¿™ä¸ªæ“ä½œæ›´åŠ ç®€æ´å’Œé«˜æ•ˆã€‚è¿™æ˜¯çº¿æ€§ä»£æ•°åœ¨å…¬å¼å’Œä»£ç ä¸­çœ‹èµ·æ¥ä¸åŒçš„ä¸€ä¸ªä¾‹å­ï¼š
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Letâ€™s think about the sizes of these variables: `data` is 150 Ã— 2 (observations
    by features) and `centroids[ci,:]` is 1 Ã— 2 (cluster `ci` by features). Formally,
    it is not possible to subtract these two vectors. However, Python will implement
    broadcasting by repeating the cluster centroids 150 times, thus subtracting the
    centroid from each data observation. The exponent operation `**` is applied element-wise,
    and the `axis=1` input tells Python to sum across the columns (separately per
    row). So, the output of `np.sum()` will be a 150 Ã— 1 array that encodes the Euclidean
    distance of each point to centroid `ci`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬è€ƒè™‘è¿™äº›å˜é‡çš„å¤§å°ï¼š`data`ä¸º150Ã—2ï¼ˆè§‚æµ‹ç‚¹ä¹˜ç‰¹å¾ï¼‰ï¼Œ`centroids[ci,:]`ä¸º1Ã—2ï¼ˆèšç±»`ci`ä¹˜ç‰¹å¾ï¼‰ã€‚ä¸¥æ ¼æ¥è¯´ï¼Œä¸èƒ½ä»è¿™ä¸¤ä¸ªå‘é‡ä¸­å‡å»ã€‚ç„¶è€Œï¼ŒPythonå°†é€šè¿‡å°†èšç±»ä¸­å¿ƒé‡å¤150æ¬¡æ¥å®ç°å¹¿æ’­ï¼Œå› æ­¤ä»æ¯ä¸ªæ•°æ®è§‚æµ‹ä¸­å‡å»èšç±»ä¸­å¿ƒã€‚æŒ‡æ•°æ“ä½œ`**`æ˜¯é€å…ƒç´ åº”ç”¨çš„ï¼Œ`axis=1`è¾“å…¥å‘Šè¯‰Pythonåœ¨åˆ—ä¸Šæ±‚å’Œï¼ˆæ¯è¡Œåˆ†åˆ«ï¼‰ã€‚å› æ­¤ï¼Œ`np.sum()`çš„è¾“å‡ºå°†æ˜¯ä¸€ä¸ª150Ã—1çš„æ•°ç»„ï¼Œç¼–ç æ¯ä¸ªç‚¹åˆ°èšç±»ä¸­å¿ƒ`ci`çš„æ¬§å‡ é‡Œå¾·è·ç¦»ã€‚
- en: 'Take a moment to compare the code to the distance formula. Are they really
    the same? In fact, they are not: the square root in Euclidean distance is missing
    from the code. So is the code wrong? Think about this for a moment; Iâ€™ll discuss
    the answer later.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: æ‹¿å‡ºä¸€æ®µæ—¶é—´æ¥æ¯”è¾ƒä»£ç å’Œè·ç¦»å…¬å¼ã€‚å®ƒä»¬çœŸçš„ä¸€æ ·å—ï¼Ÿäº‹å®ä¸Šï¼Œå®ƒä»¬å¹¶ä¸ä¸€æ ·ï¼šä»£ç ä¸­ç¼ºå°‘æ¬§å‡ é‡Œå¾·è·ç¦»ä¸­çš„å¹³æ–¹æ ¹ã€‚æ‰€ä»¥ä»£ç é”™äº†å—ï¼Ÿæ€è€ƒä¸€ä¸‹è¿™ä¸ªé—®é¢˜ï¼›æˆ‘ç¨åä¼šè®¨è®ºç­”æ¡ˆã€‚
- en: 'Step 3 is to assign each data observation to the group with minimum distance.
    This step is quite compact in Python, and can be implemented using one function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬3æ­¥æ˜¯å°†æ¯ä¸ªæ•°æ®è§‚æµ‹åˆ†é…åˆ°æœ€å°è·ç¦»çš„ç»„ä¸­ã€‚è¿™ä¸€æ­¥åœ¨Pythonä¸­éå¸¸ç®€æ´ï¼Œå¯ä»¥ä½¿ç”¨ä¸€ä¸ªå‡½æ•°å®ç°ï¼š
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note the difference between `np.min`, which returns the minimum *value*, versus
    `np.argmin`, which returns the *index* at which the minimum occurs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„`np.min`å’Œ`np.argmin`ä¹‹é—´çš„åŒºåˆ«ï¼Œå‰è€…è¿”å›æœ€å°*å€¼*ï¼Œè€Œåè€…è¿”å›æœ€å°å€¼å‡ºç°çš„*ç´¢å¼•*ã€‚
- en: We can now return to the inconsistency between the distance formula and its
    code implementation. For our *k*-means algorithm, we use distance to assign each
    data point to its closest centroid. Distance and squared distance are monotonically
    related, so both metrics give the same answer. Adding the square root operation
    increases code complexity and computation time with no impact on the results,
    so it can simply be omitted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥å›åˆ°è·ç¦»å…¬å¼åŠå…¶ä»£ç å®ç°ä¹‹é—´çš„ä¸ä¸€è‡´æ€§ã€‚å¯¹äºæˆ‘ä»¬çš„*k*-meansç®—æ³•ï¼Œæˆ‘ä»¬ä½¿ç”¨è·ç¦»å°†æ¯ä¸ªæ•°æ®ç‚¹åˆ†é…åˆ°å…¶æœ€è¿‘çš„è´¨å¿ƒã€‚è·ç¦»å’Œå¹³æ–¹è·ç¦»æ˜¯å•è°ƒç›¸å…³çš„ï¼Œå› æ­¤è¿™ä¸¤ä¸ªåº¦é‡ç»™å‡ºç›¸åŒçš„ç­”æ¡ˆã€‚å¢åŠ å¹³æ–¹æ ¹æ“ä½œä¼šå¢åŠ ä»£ç å¤æ‚æ€§å’Œè®¡ç®—æ—¶é—´ï¼Œè€Œä¸”å¯¹ç»“æœæ²¡æœ‰å½±å“ï¼Œå› æ­¤å¯ä»¥ç®€å•åœ°çœç•¥ã€‚
- en: 'Step 4 is to recompute the centroids as the mean of all data points within
    the class. Here we can loop over the *k* clusters, and use Python indexing to
    find all data points assigned to each cluster:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬ 4 æ­¥æ˜¯é‡æ–°è®¡ç®—æ¯ä¸ªç±»ä¸­æ‰€æœ‰æ•°æ®ç‚¹çš„å¹³å‡å€¼ä½œä¸ºè´¨å¿ƒã€‚åœ¨è¿™é‡Œï¼Œæˆ‘ä»¬å¯ä»¥å¾ªç¯éå†*k*ä¸ªèšç±»ï¼Œå¹¶ä½¿ç”¨ Python ç´¢å¼•æ¥æ‰¾åˆ°åˆ†é…ç»™æ¯ä¸ªèšç±»çš„æ‰€æœ‰æ•°æ®ç‚¹ï¼š
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, Step 5 is to put the previous steps into a loop that iterates until
    a good solution is obtained. In production-level *k*-means algorithms, the iterations
    continue until a stopping criteria is reached, e.g., that the cluster centroids
    are no longer moving around. For simplicity, here we will iterate three times
    (an arbitrary number selected to make the plot visually balanced).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åï¼Œç¬¬ 5 æ­¥æ˜¯å°†å‰é¢çš„æ­¥éª¤æ”¾å…¥å¾ªç¯ä¸­ï¼Œç›´åˆ°è·å¾—è‰¯å¥½çš„è§£å†³æ–¹æ¡ˆã€‚åœ¨ç”Ÿäº§çº§*k*-meansç®—æ³•ä¸­ï¼Œè¿­ä»£å°†ç»§ç»­è¿›è¡Œï¼Œç›´åˆ°è¾¾åˆ°åœæ­¢æ¡ä»¶ï¼Œä¾‹å¦‚é›†ç¾¤è´¨å¿ƒä¸å†ç§»åŠ¨ã€‚ä¸ºäº†ç®€å•èµ·è§ï¼Œè¿™é‡Œæˆ‘ä»¬å°†è¿­ä»£ä¸‰æ¬¡ï¼ˆé€‰æ‹©è¿™ä¸ªä»»æ„æ•°é‡æ˜¯ä¸ºäº†ä½¿ç»˜å›¾è§†è§‰ä¸Šå¹³è¡¡ï¼‰ã€‚
- en: The four panels in [FigureÂ 4-3](#fig_4_3) show the initial random cluster centroids
    (iteration 0), and their updated locations after each of three iterations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[å›¾Â 4-3](#fig_4_3) ä¸­çš„å››ä¸ªé¢æ¿æ˜¾ç¤ºäº†åˆå§‹éšæœºèšç±»è´¨å¿ƒï¼ˆè¿­ä»£ 0ï¼‰ï¼Œä»¥åŠæ¯æ¬¡è¿­ä»£åå®ƒä»¬çš„æ›´æ–°ä½ç½®ã€‚'
- en: If you study clustering algorithms, you will learn sophisticated methods for
    centroid initialization and stopping criteria, as well as quantitative methods
    to select an appropriate *k* parameter. Nonetheless, all *k*-means methods are
    essentially extensions of the above algorithm, and linear algebra is at the heart
    of their implementations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæ‚¨å­¦ä¹ èšç±»ç®—æ³•ï¼Œæ‚¨å°†å­¦ä¹ åˆ°ç”¨äºè´¨å¿ƒåˆå§‹åŒ–å’Œåœæ­¢æ¡ä»¶çš„å¤æ‚æ–¹æ³•ï¼Œä»¥åŠé€‰æ‹©é€‚å½“*k*å‚æ•°çš„å®šé‡æ–¹æ³•ã€‚å°½ç®¡å¦‚æ­¤ï¼Œæ‰€æœ‰*k*-meansæ–¹æ³•æœ¬è´¨ä¸Šéƒ½æ˜¯ä¸Šè¿°ç®—æ³•çš„æ‰©å±•ï¼Œè€Œçº¿æ€§ä»£æ•°æ˜¯å®ƒä»¬å®ç°çš„æ ¸å¿ƒã€‚
- en: '![k-means](assets/plad_0403.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![k-means](assets/plad_0403.png)'
- en: Figure 4-3\. k-means
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 4-3\. k-means
- en: Code Exercises
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: ä»£ç ç»ƒä¹ 
- en: Correlation Exercises
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç›¸å…³æ€§ç»ƒä¹ 
- en: Exercise 4-1\.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'ç»ƒä¹  4-1\. '
- en: 'Write a Python function that takes two vectors as input and provides two numbers
    as output: the Pearson correlation coefficient and the cosine similarity value.
    Write code that follows the formulas presented in this chapter; donâ€™t simply call
    `np.corrcoef` and `spatial.distance.cosine`. Check that the two output values
    are identical when the variables are already mean centered and different when
    the variables are not mean centered.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ç¼–å†™ä¸€ä¸ªPythonå‡½æ•°ï¼Œè¯¥å‡½æ•°ä»¥ä¸¤ä¸ªå‘é‡ä½œä¸ºè¾“å…¥å¹¶æä¾›ä¸¤ä¸ªæ•°å­—ä½œä¸ºè¾“å‡ºï¼šçš®å°”é€Šç›¸å…³ç³»æ•°å’Œä½™å¼¦ç›¸ä¼¼åº¦å€¼ã€‚ç¼–å†™ç¬¦åˆæœ¬ç« ä»‹ç»çš„å…¬å¼çš„ä»£ç ï¼›ä¸è¦ç®€å•åœ°è°ƒç”¨`np.corrcoef`å’Œ`spatial.distance.cosine`ã€‚æ£€æŸ¥å½“å˜é‡å·²ç»å‡å€¼ä¸­å¿ƒåŒ–æ—¶ï¼Œè¿™ä¸¤ä¸ªè¾“å‡ºå€¼æ˜¯å¦ç›¸åŒï¼Œå½“å˜é‡æœªå‡å€¼ä¸­å¿ƒåŒ–æ—¶æ˜¯å¦ä¸åŒã€‚
- en: Exercise 4-2\.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'ç»ƒä¹  4-2\. '
- en: Letâ€™s continue exploring the difference between correlation and cosine similarity.
    Create a variable containing the integers 0 through 3, and a second variable equaling
    the first variable plus some offset. You will then create a simulation in which
    you systematically vary that offset between âˆ’50 and +50 (that is, the first iteration
    of the simulation will have the second variable equal to [âˆ’50, âˆ’49, âˆ’48, âˆ’47]).
    In a `for` loop, compute the correlation and cosine similarity between the two
    variables and store these results. Then make a line plot showing how the correlation
    and cosine similarity are affected by the mean offset. You should be able to reproduce
    [FigureÂ 4-4](#fig_4_4).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬ç»§ç»­æ¢è®¨ç›¸å…³æ€§å’Œä½™å¼¦ç›¸ä¼¼åº¦ä¹‹é—´çš„å·®å¼‚ã€‚åˆ›å»ºä¸€ä¸ªåŒ…å«æ•´æ•° 0 åˆ° 3 çš„å˜é‡ï¼Œä»¥åŠä¸€ä¸ªç­‰äºç¬¬ä¸€ä¸ªå˜é‡åŠ ä¸ŠæŸäº›åç§»é‡çš„ç¬¬äºŒä¸ªå˜é‡ã€‚ç„¶åï¼Œæ‚¨å°†åˆ›å»ºä¸€ä¸ªæ¨¡æ‹Ÿï¼Œåœ¨è¯¥æ¨¡æ‹Ÿä¸­ç³»ç»Ÿåœ°æ”¹å˜è¯¥åç§»é‡åœ¨âˆ’50åˆ°+50ä¹‹é—´ï¼ˆå³ï¼Œæ¨¡æ‹Ÿçš„ç¬¬ä¸€æ¬¡è¿­ä»£å°†ä½¿ç¬¬äºŒä¸ªå˜é‡ç­‰äº[âˆ’50,
    âˆ’49, âˆ’48, âˆ’47]ï¼‰ã€‚åœ¨`for`å¾ªç¯ä¸­ï¼Œè®¡ç®—ä¸¤ä¸ªå˜é‡ä¹‹é—´çš„ç›¸å…³æ€§å’Œä½™å¼¦ç›¸ä¼¼åº¦ï¼Œå¹¶å­˜å‚¨è¿™äº›ç»“æœã€‚ç„¶ååˆ¶ä½œä¸€å¼ çº¿å›¾ï¼Œå±•ç¤ºç›¸å…³æ€§å’Œä½™å¼¦ç›¸ä¼¼åº¦å¦‚ä½•å—å¹³å‡åç§»çš„å½±å“ã€‚æ‚¨åº”è¯¥èƒ½å¤Ÿé‡ç°[å›¾Â 4-4](#fig_4_4)ã€‚
- en: '![Cor and cos](assets/plad_0404.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Cor å’Œ cos](assets/plad_0404.png)'
- en: Figure 4-4\. Results of Exercise 4-2
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 4-4\. ç»ƒä¹  4-2 çš„ç»“æœ
- en: Exercise 4-3\.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 'ç»ƒä¹  4-3\. '
- en: 'There are several Python functions to compute the Pearson correlation coefficient.
    One of them is called `pearsonr` and is located in the `stats` module of the SciPy
    library. Open the source code for this file (hint: `??functionname`) and make
    sure you understand how the Python implementation maps onto the formulas introduced
    in this chapter.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Python ä¸­æœ‰å‡ ä¸ªå‡½æ•°å¯ä»¥è®¡ç®—çš®å°”é€Šç›¸å…³ç³»æ•°ã€‚å…¶ä¸­ä¸€ä¸ªå«åš`pearsonr`ï¼Œä½äºSciPyåº“çš„`stats`æ¨¡å—ä¸­ã€‚æ‰“å¼€è¿™ä¸ªæ–‡ä»¶çš„æºä»£ç ï¼ˆæç¤ºï¼š`??functionname`ï¼‰ï¼Œç¡®ä¿ä½ ç†è§£Pythonå®ç°æ˜¯å¦‚ä½•æ˜ å°„åˆ°æœ¬ç« ä»‹ç»çš„å…¬å¼ä¸­çš„ã€‚
- en: Exercise 4-4\.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-4\.
- en: Why do you ever need to code your own functions when they already exist in Python?
    Part of the reason is that writing your own functions has huge educational value,
    because you see that (in this case) the correlation is a simple computation and
    not some incredibly sophisticated black-box algorithm that only a computer-science
    PhD could understand. But another reason is that built-in functions are sometimes
    slower because of myriad input checks, dealing with additional input options,
    converting data types, etc. This increases usability but at the expense of computation
    time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåœ¨Pythonä¸­å·²ç»å­˜åœ¨å‡½æ•°çš„æƒ…å†µä¸‹ï¼Œä½ ä»ç„¶éœ€è¦ç¼–å†™è‡ªå·±çš„å‡½æ•°ï¼Ÿéƒ¨åˆ†åŸå› æ˜¯ç¼–å†™è‡ªå·±çš„å‡½æ•°å…·æœ‰å·¨å¤§çš„æ•™è‚²ä»·å€¼ï¼Œå› ä¸ºä½ ä¼šçœ‹åˆ°ï¼ˆåœ¨æœ¬ä¾‹ä¸­ï¼‰ç›¸å…³æ€§æ˜¯ä¸€ä¸ªç®€å•çš„è®¡ç®—ï¼Œè€Œä¸æ˜¯æŸç§åªæœ‰è®¡ç®—æœºç§‘å­¦åšå£«æ‰èƒ½ç†è§£çš„å¤æ‚é»‘ç›’ç®—æ³•ã€‚ä½†å¦ä¸€ä¸ªåŸå› æ˜¯ï¼Œå†…ç½®å‡½æ•°æœ‰æ—¶ä¼šæ›´æ…¢ï¼Œå› ä¸ºéœ€è¦è¿›è¡Œå¤§é‡çš„è¾“å…¥æ£€æŸ¥ã€å¤„ç†é¢å¤–çš„è¾“å…¥é€‰é¡¹ã€æ•°æ®ç±»å‹è½¬æ¢ç­‰ã€‚è¿™å¢åŠ äº†å¯ç”¨æ€§ï¼Œä½†ä»¥è®¡ç®—æ—¶é—´ä¸ºä»£ä»·ã€‚
- en: Your goal in this exercise is to determine whether your own bare-bones correlation
    function is faster than NumPyâ€™s `corrcoef` function. Modify the function from
    [Exercise 4-2](#exercise_4_2) to compute only the correlation coefficient. Then,
    in a `for` loop over 1,000 iterations, generate two variables of 500 random numbers
    and compute the correlation between them. Time the `for` loop. Then repeat but
    using `np.corrcoef`. In my tests, the custom function was about 33% faster than
    `np.corrcoef`. In these toy examples, the differences are measured in milliseconds,
    but if you are running billions of correlations with large datasets, those milliseconds
    really add up! (Note that writing your own functions without input checks has
    the risk of input errors that would be caught by `np.corrcoef`.) (Also note that
    the speed advantage breaks down for larger vectors. Try it!)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨è¿™ä¸ªç»ƒä¹ ä¸­ï¼Œä½ çš„ç›®æ ‡æ˜¯ç¡®å®šä½ è‡ªå·±ç¼–å†™çš„ç®€å•ç›¸å…³å‡½æ•°æ˜¯å¦æ¯”NumPyçš„`corrcoef`å‡½æ•°æ›´å¿«ã€‚ä¿®æ”¹[ç»ƒä¹  4-2](#exercise_4_2)ä¸­çš„å‡½æ•°ï¼Œåªè®¡ç®—ç›¸å…³ç³»æ•°ã€‚ç„¶åï¼Œåœ¨ä¸€ä¸ªåŒ…å«
    1000 æ¬¡è¿­ä»£çš„`for`å¾ªç¯ä¸­ï¼Œç”Ÿæˆä¸¤ä¸ªåŒ…å« 500 ä¸ªéšæœºæ•°çš„å˜é‡ï¼Œå¹¶è®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ã€‚è®¡æ—¶è¿™ä¸ª`for`å¾ªç¯ã€‚ç„¶åé‡å¤ï¼Œä½†ä½¿ç”¨`np.corrcoef`ã€‚åœ¨æˆ‘çš„æµ‹è¯•ä¸­ï¼Œè‡ªå®šä¹‰å‡½æ•°æ¯”`np.corrcoef`å¿«çº¦
    33%ã€‚åœ¨è¿™äº›ç©å…·ç¤ºä¾‹ä¸­ï¼Œå·®å¼‚ä»¥æ¯«ç§’è®¡ï¼Œä½†å¦‚æœä½ åœ¨å¤§æ•°æ®é›†ä¸Šè¿è¡Œæ•°åäº¿ä¸ªç›¸å…³æ€§è®¡ç®—ï¼Œè¿™äº›æ¯«ç§’çš„ç´¯ç§¯éå¸¸æ˜¾è‘—ï¼ï¼ˆè¯·æ³¨æ„ï¼Œç¼–å†™æ²¡æœ‰è¾“å…¥æ£€æŸ¥çš„è‡ªå®šä¹‰å‡½æ•°å­˜åœ¨è¾“å…¥é”™è¯¯çš„é£é™©ï¼Œè€Œè¿™äº›é”™è¯¯åœ¨`np.corrcoef`ä¸­ä¼šè¢«æ•è·ã€‚ï¼‰ï¼ˆè¿˜è¦æ³¨æ„ï¼Œå¯¹äºè¾ƒå¤§çš„å‘é‡ï¼Œé€Ÿåº¦ä¼˜åŠ¿ä¼šæ¶ˆå¤±ã€‚è¯•ä¸€è¯•ï¼ï¼‰
- en: Filtering and Feature Detection Exercises
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: è¿‡æ»¤å’Œç‰¹å¾æ£€æµ‹ç»ƒä¹ 
- en: Exercise 4-5\.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-5\.
- en: 'Letâ€™s build an edge detector. The kernel for an edge detector is very simple:
    [âˆ’1 +1]. The dot product of that kernel with a snippet of a time series signal
    with constant value (e.g., [10 10]) is 0\. But that dot product is large when
    the signal has a steep change (e.g., [1 10] would produce a dot product of 9).
    The signal weâ€™ll work with is a plateau function. Graphs A and B in [FigureÂ 4-5](#fig_4_5)
    show the kernel and the signal. The first step in this exercise is to write code
    that creates these two time series.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬æ„å»ºä¸€ä¸ªè¾¹ç¼˜æ£€æµ‹å™¨ã€‚è¾¹ç¼˜æ£€æµ‹å™¨çš„æ ¸éå¸¸ç®€å•ï¼š[âˆ’1 +1]ã€‚è¯¥æ ¸ä¸æ—¶é—´åºåˆ—ä¿¡å·ç‰‡æ®µçš„ç‚¹ç§¯åœ¨ä¿¡å·å…·æœ‰æ’å®šå€¼ï¼ˆä¾‹å¦‚ï¼Œ[10 10]ï¼‰æ—¶ä¸º 0ã€‚ä½†å½“ä¿¡å·æœ‰é™¡å³­å˜åŒ–æ—¶ï¼ˆä¾‹å¦‚ï¼Œ[1
    10]ä¼šäº§ç”Ÿç‚¹ç§¯ä¸º 9ï¼‰ï¼Œè¯¥ç‚¹ç§¯å°±ä¼šå¾ˆå¤§ã€‚æˆ‘ä»¬å°†ä½¿ç”¨çš„ä¿¡å·æ˜¯ä¸€ä¸ªå¹³å°å‡½æ•°ã€‚å›¾ A å’Œ B åœ¨[å›¾ 4-5](#fig_4_5)ä¸­å±•ç¤ºäº†æ ¸å’Œä¿¡å·ã€‚è¿™ä¸ªç»ƒä¹ çš„ç¬¬ä¸€æ­¥æ˜¯ç¼–å†™ä»£ç åˆ›å»ºè¿™ä¸¤ä¸ªæ—¶é—´åºåˆ—ã€‚
- en: '![What does this do?](assets/plad_0405.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![è¿™æ˜¯åšä»€ä¹ˆçš„ï¼Ÿ](assets/plad_0405.png)'
- en: Figure 4-5\. Results of Exercise 4-5
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾ 4-5\. ç»ƒä¹  4-5 çš„ç»“æœ
- en: Next, write a `for` loop over the time points in the signal. At each time point,
    compute the dot product between the kernel and a segment of the time series data
    that has the same length as the kernel. You should produce a plot that looks like
    graph C in [FigureÂ 4-5](#fig_4_5). (Focus more on the result than on the aesthetics.)
    Notice that our edge detector returned 0 when the signal was flat, +1 when the
    signal jumped up, and âˆ’1 when the signal jumped down.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œåœ¨ä¿¡å·çš„æ—¶é—´ç‚¹ä¸Šç¼–å†™ä¸€ä¸ª`for`å¾ªç¯ã€‚åœ¨æ¯ä¸ªæ—¶é—´ç‚¹ï¼Œè®¡ç®—æ ¸ä¸ä¸æ ¸ç›¸åŒé•¿åº¦çš„æ—¶é—´åºåˆ—æ•°æ®æ®µä¹‹é—´çš„ç‚¹ç§¯ã€‚ä½ åº”è¯¥ç”Ÿæˆä¸€ä¸ªçœ‹èµ·æ¥åƒå›¾ C åœ¨[å›¾ 4-5](#fig_4_5)ä¸­çš„å›¾çš„ç»˜å›¾ç»“æœã€‚ï¼ˆæ›´å…³æ³¨ç»“æœè€Œä¸æ˜¯ç¾è§‚åº¦ã€‚ï¼‰æ³¨æ„ï¼Œæˆ‘ä»¬çš„è¾¹ç¼˜æ£€æµ‹å™¨åœ¨ä¿¡å·å¹³å¦æ—¶è¿”å›
    0ï¼Œåœ¨ä¿¡å·è·³å˜æ—¶è¿”å› +1ï¼Œåœ¨ä¿¡å·è·³é™æ—¶è¿”å› âˆ’1ã€‚
- en: Feel free to continue exploring this code. For example, does anything change
    if you pad the kernel with zeros ([0 âˆ’1 1 0])? What about if you flip the kernel
    to be [1 âˆ’1]? How about if the kernel is asymmetric ([âˆ’1 2])?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: éšæ„ç»§ç»­æ¢ç´¢è¿™æ®µä»£ç ã€‚ä¾‹å¦‚ï¼Œå¦‚æœç”¨é›¶ï¼ˆ[0 âˆ’1 1 0]ï¼‰å¡«å……æ ¸å¿ƒä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿå¦‚æœå°†æ ¸å¿ƒç¿»è½¬ä¸º[1 âˆ’1]ä¼šæ€æ ·ï¼Ÿå¦‚æœæ ¸å¿ƒæ˜¯ä¸å¯¹ç§°çš„ï¼ˆ[âˆ’1 2]ï¼‰ä¼šå‘ç”Ÿä»€ä¹ˆå˜åŒ–ï¼Ÿ
- en: Exercise 4-6\.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-6ã€‚
- en: Now we will repeat the same procedure but with a different signal and kernel.
    The goal will be to smooth a rugged time series. The time series will be 100 random
    numbers generated from a Gaussian distribution (also called a normal distribution).
    The kernel will be a bell-shaped function that approximates a Gaussian function,
    defined as the numbers [0, .1, .3, .8, 1, .8, .3, .1, 0] but scaled so that the
    sum over the kernel is 1\. Your kernel should match graph A in [FigureÂ 4-6](#fig_4_6),
    although your signal wonâ€™t look exactly like graph B due to random numbers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å°†é‡å¤ç›¸åŒçš„è¿‡ç¨‹ï¼Œä½†ä½¿ç”¨ä¸åŒçš„ä¿¡å·å’Œæ ¸å‡½æ•°ã€‚ç›®æ ‡æ˜¯å¹³æ»‘ä¸€ä¸ªå´å²–ä¸å¹³çš„æ—¶é—´åºåˆ—ã€‚æ—¶é—´åºåˆ—å°†æ˜¯ä»é«˜æ–¯åˆ†å¸ƒï¼ˆä¹Ÿç§°ä¸ºæ­£æ€åˆ†å¸ƒï¼‰ç”Ÿæˆçš„100ä¸ªéšæœºæ•°ã€‚æ ¸å‡½æ•°å°†æ˜¯ä¸€ä¸ªé’Ÿå½¢å‡½æ•°ï¼Œè¿‘ä¼¼äºé«˜æ–¯å‡½æ•°ï¼Œå…¶å®šä¹‰ä¸ºæ•°å­—[0,
    .1, .3, .8, 1, .8, .3, .1, 0]ï¼Œä½†ç¼©æ”¾ä½¿å¾—æ ¸çš„æ€»å’Œä¸º1ã€‚ä½ çš„æ ¸åº”è¯¥ä¸[å›¾Â 4-6](#fig_4_6)ä¸­çš„Aå›¾ç›¸åŒ¹é…ï¼Œå°½ç®¡ç”±äºéšæœºæ•°çš„åŸå› ï¼Œä½ çš„ä¿¡å·çœ‹èµ·æ¥ä¸ä¼šå®Œå…¨åƒBå›¾ã€‚
- en: 'Copy and adapt the code from the previous exercise to compute the sliding time
    series of dot productsâ€”the signal filtered by the Gaussian kernel. Warning: be
    mindful of the indexing in the `for` loop. Graph C in [FigureÂ 4-6](#fig_4_6) shows
    an example result. You can see that the filtered signal is a smoothed version
    of the original signal. This is also called low-pass filtering.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: å¤åˆ¶å¹¶è°ƒæ•´ä¸Šä¸€ä¸ªç»ƒä¹ ä¸­çš„ä»£ç ï¼Œè®¡ç®—é€šè¿‡é«˜æ–¯æ ¸è¿‡æ»¤çš„æ»‘åŠ¨æ—¶é—´åºåˆ—çš„ç‚¹ç§¯ã€‚è­¦å‘Šï¼šè¦æ³¨æ„`for`å¾ªç¯ä¸­çš„ç´¢å¼•ã€‚[å›¾Â 4-6](#fig_4_6)ä¸­çš„Cå›¾æ˜¾ç¤ºäº†ä¸€ä¸ªç¤ºä¾‹ç»“æœã€‚ä½ å¯ä»¥çœ‹åˆ°ï¼Œç»è¿‡æ»¤æ³¢çš„ä¿¡å·æ˜¯åŸå§‹ä¿¡å·çš„å¹³æ»‘ç‰ˆæœ¬ã€‚è¿™ä¹Ÿè¢«ç§°ä¸ºä½é€šæ»¤æ³¢ã€‚
- en: '![Exercise 4-6](assets/plad_0406.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![ç»ƒä¹  4-6](assets/plad_0406.png)'
- en: Figure 4-6\. Results of Exercise 4-6
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: å›¾Â 4-6ã€‚ç»ƒä¹  4-6çš„ç»“æœ
- en: Exercise 4-7\.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-7ã€‚
- en: Replace the 1 in the center of the kernel with âˆ’1 and mean center the kernel.
    Then rerun the filtering and plotting code. What is the result? It actually accentuates
    the sharp features! In fact, this kernel is now a high-pass filter, meaning it
    dampens the smooth (low-frequency) features and highlights the rapidly changing
    (high-frequency) features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æ ¸å¿ƒä¸­çš„1æ›¿æ¢ä¸º-1å¹¶å¯¹æ ¸å¿ƒè¿›è¡Œå¹³å‡ä¸­å¿ƒåŒ–ã€‚ç„¶åé‡æ–°è¿è¡Œè¿‡æ»¤å’Œç»˜å›¾ä»£ç ã€‚ç»“æœæ˜¯ä»€ä¹ˆï¼Ÿå®é™…ä¸Šï¼Œè¿™ä¼šçªæ˜¾å‡ºå°–é”çš„ç‰¹å¾ï¼äº‹å®ä¸Šï¼Œè¿™ä¸ªæ ¸ç°åœ¨æ˜¯ä¸€ä¸ªé«˜é€šæ»¤æ³¢å™¨ï¼Œæ„å‘³ç€å®ƒå‡å¼±äº†å¹³æ»‘ï¼ˆä½é¢‘ï¼‰ç‰¹å¾å¹¶çªå‡ºæ˜¾ç¤ºå¿«é€Ÿå˜åŒ–ï¼ˆé«˜é¢‘ï¼‰ç‰¹å¾ã€‚
- en: '*k*-Means Exercises'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*k*-å‡å€¼èšç±»ç»ƒä¹ '
- en: Exercise 4-8\.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-8ã€‚
- en: One way to determine an optimal *k* is to repeat the clustering multiple times
    (each time using randomly initialized cluster centroids) and assess whether the
    final clustering is the same or different. Without generating new data, rerun
    the *k*-means code several times using *k* = 3 to see whether the resulting clusters
    are similar (this is a qualitative assessment based on visual inspection). Do
    the final cluster assignments generally seem similar even though the centroids
    are randomly selected?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ç¡®å®šä¸€ä¸ªæœ€ä¼˜*k*çš„ä¸€ç§æ–¹æ³•æ˜¯å¤šæ¬¡é‡å¤èšç±»ï¼ˆæ¯æ¬¡ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„èšç±»ä¸­å¿ƒï¼‰å¹¶è¯„ä¼°æœ€ç»ˆçš„èšç±»æ˜¯å¦ç›¸åŒæˆ–ä¸åŒã€‚ä¸ç”Ÿæˆæ–°æ•°æ®çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨*k* = 3å¤šæ¬¡é‡æ–°è¿è¡Œ*k*-å‡å€¼ä»£ç ï¼Œçœ‹çœ‹æœ€ç»ˆçš„èšç±»åˆ†é…æ˜¯å¦é€šå¸¸çœ‹èµ·æ¥ç›¸ä¼¼ï¼ˆè¿™æ˜¯åŸºäºè§†è§‰æ£€æŸ¥çš„å®šæ€§è¯„ä¼°ï¼‰ã€‚å³ä½¿èšç±»ä¸­å¿ƒæ˜¯éšæœºé€‰æ‹©çš„ï¼Œæœ€ç»ˆçš„èšç±»åˆ†é…æ˜¯å¦é€šå¸¸çœ‹èµ·æ¥ç›¸ä¼¼ï¼Ÿ
- en: Exercise 4-9\.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: ç»ƒä¹  4-9ã€‚
- en: Repeat the multiple clusterings using *k* = 2 and *k* = 4\. What do you think
    of these results?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨*k* = 2å’Œ*k* = 4é‡å¤å¤šæ¬¡èšç±»ã€‚ä½ è§‰å¾—è¿™äº›ç»“æœå¦‚ä½•ï¼Ÿ
- en: '^([1](ch04.xhtml#idm45733310634320-marker)) Reminder: Euclidean distance is
    the square root of the sum of squared distances from the data observation to the
    centroid.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45733310634320-marker)) æç¤ºï¼šæ¬§æ°è·ç¦»æ˜¯ä»æ•°æ®è§‚æµ‹åˆ°è´¨å¿ƒçš„å¹³æ–¹è·ç¦»ä¹‹å’Œçš„å¹³æ–¹æ ¹ã€‚
