- en: Chapter 4\. Vector Applications
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第四章。向量应用
- en: While working through the previous two chapters, you may have felt that some
    of the material was esoteric and abstract. Perhaps you felt that the challenge
    of learning linear algebra wouldn’t pay off in understanding real-world applications
    in data science and machine learning.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读前两章的过程中，你可能觉得其中一些内容过于深奥和抽象。也许你感到学习线性代数的挑战在理解数据科学和机器学习中的实际应用方面并没有多大帮助。
- en: I hope that this chapter dispels you of these doubts. In this chapter, you will
    learn how vectors and vector operations are used in data science analyses. And
    you will be able to extend this knowledge by working through the exercises.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 希望本章能消除你的这些疑虑。在本章中，你将学习向量及其操作如何在数据科学分析中使用。通过完成练习，你将能够扩展这些知识。
- en: Correlation and Cosine Similarity
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 相关性和余弦相似度
- en: Correlation is one of the most fundamental and important analysis methods in
    statistics and machine learning. A *correlation coefficient* is a single number
    that quantifies the linear relationship between two variables. Correlation coefficients
    range from −1 to +1, with −1 indicating a perfect negative relationship, +1 a
    perfect positive relationships, and 0 indicating no linear relationship. [Figure 4-1](#fig_4_1)
    shows a few examples of pairs of variables and their correlation coefficients.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性是统计学和机器学习中最基本和重要的分析方法之一。*相关系数*是一个单一数字，用于量化两个变量之间的线性关系。相关系数的范围从 −1 到 +1，其中
    −1 表示完美的负相关，+1 表示完美的正相关，而 0 表示没有线性关系。[图4-1](#fig_4_1)展示了几对变量及其相关系数的示例。
- en: '![Correlation examples](assets/plad_0401.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![相关性示例](assets/plad_0401.png)'
- en: Figure 4-1\. Examples of data exhibiting positive correlation, negative correlation,
    and zero correlation. The lower-right panel illustrates that correlation is a
    linear measure; nonlinear relationships between variables can exist even if their
    correlation is zero.
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图4-1。展示了表现出正相关、负相关和零相关的数据示例。右下方的面板说明了相关性是线性测量；即使变量之间的相关性为零，它们之间也可能存在非线性关系。
- en: 'In [Chapter 2](ch02.xhtml#Chapter_2), I mentioned that the dot product is involved
    in the correlation coefficient, and that the magnitude of the dot product is related
    to the magnitude of the numerical values in the data (remember the discussion
    about using grams versus pounds for measuring weight). Therefore, the correlation
    coefficient requires some normalizations to be in the expected range of −1 to
    +1\. Those two normalizations are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第二章](ch02.xhtml#Chapter_2)中，我提到点积涉及到相关系数，并且点积的大小与数据中数值的大小相关（记得我们讨论过使用克与磅来衡量重量）。因此，相关系数需要一些标准化以保持在预期范围内的
    −1 到 +1 之间。这两种标准化方法是：
- en: Mean center each variable
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对每个变量进行均值中心化
- en: '*Mean centering* means to subtract the average value from each data value.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*均值中心化*意味着从每个数据值中减去平均值。'
- en: Divide the dot product by the product of the vector norms
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 将点积除以向量范数的乘积
- en: This divisive normalization cancels the measurement units and scales the maximum
    possible correlation magnitude to |1|.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这种除法式标准化取消了测量单位并将最大可能的相关性幅度缩放为|1|。
- en: '[Equation 4-1](#pearson-corr1) shows the full formula for the Pearson correlation
    coefficient.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程4-1](#pearson-corr1)展示了皮尔逊相关系数的完整公式。'
- en: Equation 4-1\. Formula for Pearson correlation coefficient
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-1。皮尔逊相关系数的公式
- en: <math alttext="rho equals StartFraction sigma-summation Underscript i equals
    1 Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar
    right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis
    Over StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis x Subscript i Baseline minus x overbar right-parenthesis squared
    EndRoot StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis squared
    EndRoot EndFraction" display="block"><mrow><mi>ρ</mi> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="rho equals StartFraction sigma-summation Underscript i equals
    1 Overscript n Endscripts left-parenthesis x Subscript i Baseline minus x overbar
    right-parenthesis left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis
    Over StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis x Subscript i Baseline minus x overbar right-parenthesis squared
    EndRoot StartRoot sigma-summation Underscript i equals 1 Overscript n Endscripts
    left-parenthesis y Subscript i Baseline minus y overbar right-parenthesis squared
    EndRoot EndFraction" display="block"><mrow><mi>ρ</mi> <mo>=</mo> <mfrac><mrow><msubsup><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></msubsup> <mrow><mo>(</mo><msub><mi>x</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow><mrow><mo>(</mo><msub><mi>y</mi>
    <mi>i</mi></msub> <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow></mrow>
    <mrow><msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>x</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt> <msqrt><mrow><msubsup><mo>∑</mo> <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow>
    <mi>n</mi></msubsup> <msup><mrow><mo>(</mo><msub><mi>y</mi> <mi>i</mi></msub>
    <mo>-</mo><mover accent="true"><mi>y</mi> <mo>¯</mo></mover><mo>)</mo></mrow>
    <mn>2</mn></msup></mrow></msqrt></mrow></mfrac></mrow></math>
- en: 'It may not be obvious that the correlation is simply three dot products. [Equation
    4-2](#pearson-corr2) shows this same formula rewritten using the linear algebra
    dot-product notation. In this equation, <math alttext="bold x overTilde"><mover
    accent="true"><mi>𝐱</mi> <mo>˜</mo></mover></math> is the mean-centered version
    of <math alttext="bold x"><mi>𝐱</mi></math> (that is, variable <math alttext="bold
    x"><mi>𝐱</mi></math> with normalization #1 applied).'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 或许不太明显的是，相关性其实就是三个点积。[方程4-2](#pearson-corr2)展示了使用线性代数点积符号重写的相同公式。在这个方程中，<math
    alttext="bold x overTilde"><mover accent="true"><mi>𝐱</mi> <mo>˜</mo></mover></math>
    是<math alttext="bold x"><mi>𝐱</mi></math>的均值中心化版本（即应用了标准化方法#1的变量<math alttext="bold
    x"><mi>𝐱</mi></math>）。
- en: Equation 4-2\. The Pearson correlation expressed in the parlance of linear algebra
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4-2。用线性代数术语表示的皮尔逊相关性
- en: <math alttext="rho equals StartFraction bold x overTilde Superscript upper T
    Baseline bold y overTilde Over parallel-to bold x overTilde parallel-to parallel-to
    bold y overTilde parallel-to EndFraction" display="block"><mrow><mi>ρ</mi> <mo>=</mo>
    <mfrac><mrow><msup><mover accent="true"><mi>𝐱</mi> <mo>˜</mo></mover> <mtext>T</mtext></msup>
    <mover accent="true"><mi>𝐲</mi> <mo>˜</mo></mover></mrow> <mrow><mrow><mo>∥</mo></mrow><mover
    accent="true"><mi>𝐱</mi> <mo>˜</mo></mover><mrow><mo>∥</mo><mo>∥</mo></mrow><mover
    accent="true"><mi>𝐲</mi> <mo>˜</mo></mover><mrow><mo>∥</mo></mrow></mrow></mfrac></mrow></math>
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="rho equals StartFraction bold x overTilde Superscript upper T
    Baseline bold y overTilde Over parallel-to bold x overTilde parallel-to parallel-to
    bold y overTilde parallel-to EndFraction" display="block"><mrow><mi>ρ</mi> <mo>=</mo>
    <mfrac><mrow><msup><mover accent="true"><mi>𝐱</mi> <mo>˜</mo></mover> <mtext>T</mtext></msup>
    <mover accent="true"><mi>𝐲</mi> <mo>˜</mo></mover></mrow> <mrow><mrow><mo>∥</mo></mrow><mover
    accent="true"><mi>𝐱</mi> <mo>˜</mo></mover><mrow><mo>∥</mo><mo>∥</mo></mrow><mover
    accent="true"><mi>𝐲</mi> <mo>˜</mo></mover><mrow><mo>∥</mo></mrow></mrow></mfrac></mrow></math>
- en: 'So there you go: the famous and widely used Pearson correlation coefficient
    is simply the dot product between two variables, normalized by the magnitudes
    of the variables. (By the way, you can also see from this formula that if the
    variables are unit normed such that <math alttext="parallel-to bold x parallel-to
    equals parallel-to bold y parallel-to equals 1"><mrow><mo>∥</mo> <mi>𝐱</mi> <mo>∥</mo>
    <mo>=</mo> <mo>∥</mo> <mi>𝐲</mi> <mo>∥</mo> <mo>=</mo> <mn>1</mn></mrow></math>
    , then their correlation equals their dot product. (Recall from [Exercise 2-6](ch02.xhtml#exercise_2_6)
    that <math alttext="parallel-to bold x parallel-to equals StartRoot bold x Superscript
    upper T Baseline bold x EndRoot"><mrow><mrow><mo>∥</mo> <mi>𝐱</mi> <mo>∥</mo></mrow>
    <mo>=</mo> <msqrt><mrow><msup><mi>𝐱</mi> <mtext>T</mtext></msup> <mi>𝐱</mi></mrow></msqrt></mrow></math>
    .)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，著名且广泛使用的Pearson相关系数简单地是两个变量之间的点积，由变量的大小归一化得到。（顺便说一句，您还可以从这个公式看出，如果变量被单位归一化，使得<math
    alttext="parallel-to bold x parallel-to equals parallel-to bold y parallel-to
    equals 1"><mrow><mo>∥</mo> <mi>𝐱</mi> <mo>∥</mo> <mo>=</mo> <mo>∥</mo> <mi>𝐲</mi>
    <mo>∥</mo> <mo>=</mo> <mn>1</mn></mrow></math>，则它们的相关性等于它们的点积。（回想一下，来自[Exercise
    2-6](ch02.xhtml#exercise_2_6)，其中<math alttext="parallel-to bold x parallel-to
    equals StartRoot bold x Superscript upper T Baseline bold x EndRoot"><mrow><mrow><mo>∥</mo>
    <mi>𝐱</mi> <mo>∥</mo></mrow> <mo>=</mo> <msqrt><mrow><msup><mi>𝐱</mi> <mtext>T</mtext></msup>
    <mi>𝐱</mi></mrow></msqrt></mrow></math>。）
- en: 'Correlation is not the only way to assess similarity between two variables.
    Another method is called *cosine similarity*. The formula for cosine similarity
    is simply the geometric formula for the dot product ([Equation 2-11](ch02.xhtml#dp-geom)),
    solved for the cosine term:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 相关性不是评估两个变量相似性的唯一方法。另一种方法称为*余弦相似度*。余弦相似度的公式只是点积的几何形式（[Equation 2-11](ch02.xhtml#dp-geom)），解出余弦项：
- en: <math alttext="cosine left-parenthesis theta Subscript x comma y Baseline right-parenthesis
    equals StartFraction alpha Over parallel-to bold x parallel-to parallel-to bold
    y parallel-to EndFraction" display="block"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mi>α</mi> <mrow><mo>∥</mo><mi>𝐱</mi><mo>∥</mo><mo>∥</mo><mi>𝐲</mi><mo>∥</mo></mrow></mfrac></mrow></math>
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="cosine left-parenthesis theta Subscript x comma y Baseline right-parenthesis
    equals StartFraction alpha Over parallel-to bold x parallel-to parallel-to bold
    y parallel-to EndFraction" display="block"><mrow><mo form="prefix">cos</mo> <mrow><mo>(</mo>
    <msub><mi>θ</mi> <mrow><mi>x</mi><mo>,</mo><mi>y</mi></mrow></msub> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mi>α</mi> <mrow><mo>∥</mo><mi>𝐱</mi><mo>∥</mo><mo>∥</mo><mi>𝐲</mi><mo>∥</mo></mrow></mfrac></mrow></math>
- en: where <math alttext="alpha"><mi>α</mi></math> is the dot product between <math
    alttext="bold x"><mi>𝐱</mi></math> and <math alttext="bold y"><mi>𝐲</mi></math>
    .
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="alpha"><mi>α</mi></math>是<math alttext="bold x"><mi>𝐱</mi></math>和<math
    alttext="bold y"><mi>𝐲</mi></math>的点积。
- en: It may seem like correlation and cosine similarity are exactly the same formula.
    However, remember that [Equation 4-1](#pearson-corr1) is the full formula, whereas
    [Equation 4-2](#pearson-corr2) is a simplification under the assumption that the
    variables have already been mean centered. Thus, cosine similarity does not involve
    the first normalization factor.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来相关性和余弦相似性的公式完全相同。但请记住，[Equation 4-1](#pearson-corr1)是完整的公式，而[Equation 4-2](#pearson-corr2)是在变量已经被均值中心化的假设下的简化。因此，余弦相似度不涉及第一个归一化因子。
- en: 'From this section, you can understand why the Pearson correlation and cosine
    similarity reflects the *linear* relationship between two variables: they are
    based on the dot product, and the dot product is a linear operation.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 从本节可以理解为什么Pearson相关系数和余弦相似度反映了两个变量之间的*线性*关系：它们基于点积，而点积是线性操作。
- en: There are four coding exercises associated with this section, which appear at
    the end of the chapter. You can choose whether you want to solve those exercises
    before reading the next section, or continue reading the rest of the chapter and
    then work through the exercises. (My personal recommendation is the former, but
    you are the master of your linear algebra destiny!)
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 本节有四个编程练习，位于章节末尾。您可以选择在阅读下一节之前解决这些练习，或者继续阅读整章然后再解决这些练习。（我个人推荐选择前者，但您是线性代数命运的主宰！）
- en: Time Series Filtering and Feature Detection
  id: totrans-25
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 时间序列过滤和特征检测
- en: The dot product is also used in time series filtering. Filtering is essentially
    a feature-detection method, whereby a template—called a *kernel* in the parlance
    of filtering—is matched against portions of a time series signal, and the result
    of filtering is another time series that indicates how much the characteristics
    of the signal match the characteristics of the kernel. Kernels are carefully constructed
    to optimize certain criteria, such as smooth fluctuations, sharp edges, particular
    waveform shapes, and so on.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 点积还用于时间序列过滤。过滤本质上是一种特征检测方法，其中模板——在过滤术语中称为*核*——与时间序列信号的部分匹配，过滤的结果是另一个时间序列，指示信号特征与核特征匹配程度。核被精心构造以优化特定标准，如平滑波动、锐利边缘、特定波形形状等。
- en: The mechanism of filtering is to compute the dot product between the kernel
    and the time series signal. But filtering usually requires *local* feature detection,
    and the kernel is typically much shorter than the entire time series. Therefore,
    we compute the dot product between the kernel and a short snippet of the data
    of the same length as the kernel. This procedure produces one time point in the
    filtered signal ([Figure 4-2](#fig_4_2)), and then the kernel is moved one time
    step to the right to compute the dot product with a different (overlapping) signal
    segment. Formally, this procedure is called convolution and involves a few additional
    steps that I’m omitting to focus on the application of the dot product in signal
    processing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤的机制是计算核与时间序列信号之间的点积。但通常过滤需要*局部*特征检测，而核通常比整个时间序列要短得多。因此，我们计算核与与核长度相同的数据的一个短片段之间的点积。这个过程产生过滤信号中的一个时间点（[图 4-2](#fig_4_2)），然后将核向右移动一个时间步来计算与不同（重叠）信号段的点积。正式地说，这个过程称为卷积，涉及几个额外的步骤，我在这里省略以便专注于在信号处理中应用点积。
- en: '![Time series filtering](assets/plad_0402.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![时间序列滤波](assets/plad_0402.png)'
- en: Figure 4-2\. Illustration of time series filtering
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-2\. 时间序列滤波示例
- en: Temporal filtering is a major topic in science and engineering. Indeed, without
    temporal filtering there would be no music, radio, telecommunications, satellites,
    etc. And yet, the mathematical heart that keeps your music pumping is the vector
    dot product.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 时间滤波是科学和工程中的一个重要主题。事实上，没有时间滤波就不会有音乐、无线电、电信、卫星等。而支撑你的音乐播放的数学核心是向量点乘。
- en: In the exercises at the end of the chapter, you will discover how dot products
    are used to detect features (edges) and to smooth time series data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾的练习中，你将了解到如何利用点乘来检测特征（边缘）和平滑时间序列数据。
- en: '*k*-Means Clustering'
  id: totrans-32
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '*k*-Means Clustering'
- en: '*k-means clustering* is an unsupervised method of classifying multivariate
    data into a relatively small number of groups, or categories, based on minimizing
    distance to the group center.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '*k-means clustering* 是一种无监督的方法，根据距离最小化原则，将多变量数据分成相对较少的几组或类别。'
- en: '*k*-means clustering is an important analysis method in machine learning, and
    there are sophisticated variants of *k*-means clustering. Here we will implement
    a simple version of *k*-means, with the goal of seeing how concepts about vectors
    (in particular: vectors, vector norms, and broadcasting) are used in the *k*-means
    algorithm.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*k*-means clustering 是机器学习中重要的分析方法，有多种复杂的*k*-means clustering 变体。在这里，我们将实现一个简单版本的*k*-means，旨在了解向量（特别是：向量、向量范数和广播）在*k*-means
    算法中的应用。'
- en: 'Here is a brief description of the algorithm that we will write:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们将要编写的算法的简要描述：
- en: Initialize *k* centroids as random points in the data space. Each centroid is
    a *class*, or category, and the next steps will assign each data observation to
    each class. (A *centroid* is a center generalized to any number of dimensions.)
  id: totrans-36
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将*k*个质心初始化为数据空间中的随机点。每个质心都是一个*类*或*类别*，接下来的步骤将把每个数据观测值分配到每个类别中。（*质心*是通用化到任意维度的中心。）
- en: Compute the Euclidean distance between each data observation and each centroid.^([1](ch04.xhtml#idm45733310634320))
  id: totrans-37
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个数据观测值与每个质心之间的欧氏距离。^([1](ch04.xhtml#idm45733310634320))
- en: Assign each data observation to the group with the closest centroid.
  id: totrans-38
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据观测值分配给最近质心所在的组。
- en: Update each centroid as the average of all data observations assigned to that
    centroid.
  id: totrans-39
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个质心更新为分配给该质心的所有数据观测值的平均值。
- en: Repeat steps 2–4 until a convergence criteria is satisfied, or for *N* iterations.
  id: totrans-40
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2–4，直到满足收敛标准或进行*N*次迭代。
- en: If you are comfortable with Python coding and would like to implement this algorithm,
    then I encourage you to do that before continuing. Next, we will work through
    the math and code for each of these steps, with a particular focus on using vectors
    and broadcasting in NumPy. We will also test the algorithm using randomly generated
    2D data to confirm that our code is correct.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对Python编码感到舒适，并希望实现这个算法，那么我鼓励你在继续之前这样做。接下来，我们将详细讲解每个步骤的数学和代码，特别是在NumPy中使用向量和广播的概念。我们还将使用随机生成的二维数据来测试算法，以确认我们的代码是正确的。
- en: 'Let’s start with step 1: initialize *k* random cluster centroids. *k* is a
    parameter of *k*-means clustering; in real data, it is difficult to determine
    the optimal *k*, but here we will fix *k* = 3\. There are several ways to initialize
    random cluster centroids; to keep things simple, I will randomly select *k* data
    samples to be centroids. The data are contained in variable `data` (this variable
    is 150 × 2, corresponding to 150 observations and 2 features) and visualized in
    the upper-left panel of [Figure 4-3](#fig_4_3) (the online code shows how to generate
    these data):'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第1步开始：初始化*k*个随机聚类中心。*k*是*k*-均值聚类的一个参数；在真实数据中，确定最佳*k*是困难的，但在这里我们将*k*固定为3。有几种初始化随机聚类中心的方法；为了简单起见，我将随机选择*k*个数据样本作为聚类中心。数据包含在变量`data`中（此变量为150×2，对应150个观测和2个特征），并且在[图 4-3](#fig_4_3)的左上方面板中进行了可视化（在线代码显示如何生成这些数据）：
- en: '[PRE0]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now for step 2: compute the distance between each data observation and each
    cluster centroid. Here is where we use linear algebra concepts you learned in
    the previous chapters. For one data observation and centroid, Euclidean distance
    is computed as'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在到第2步：计算每个数据观测点与每个聚类中心之间的距离。在这里，我们使用了你在前几章学到的线性代数概念。
- en: <math alttext="delta Subscript i comma j Baseline equals StartRoot left-parenthesis
    d Subscript i Superscript x Baseline minus c Subscript j Superscript x Baseline
    right-parenthesis squared plus left-parenthesis d Subscript i Superscript y Baseline
    minus c Subscript j Superscript y Baseline right-parenthesis squared EndRoot"
    display="block"><mrow><msub><mi>δ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi> <mi>x</mi></msubsup>
    <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>x</mi></msubsup> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi>
    <mi>y</mi></msubsup> <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>y</mi></msubsup>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="delta Subscript i comma j Baseline equals StartRoot left-parenthesis
    d Subscript i Superscript x Baseline minus c Subscript j Superscript x Baseline
    right-parenthesis squared plus left-parenthesis d Subscript i Superscript y Baseline
    minus c Subscript j Superscript y Baseline right-parenthesis squared EndRoot"
    display="block"><mrow><msub><mi>δ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub>
    <mo>=</mo> <msqrt><mrow><msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi> <mi>x</mi></msubsup>
    <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>x</mi></msubsup> <mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>+</mo> <msup><mrow><mo>(</mo><msubsup><mi>d</mi> <mi>i</mi>
    <mi>y</mi></msubsup> <mo>-</mo><msubsup><mi>c</mi> <mi>j</mi> <mi>y</mi></msubsup>
    <mo>)</mo></mrow> <mn>2</mn></msup></mrow></msqrt></mrow></math>
- en: where <math alttext="delta Subscript i comma j"><msub><mi>δ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>
    indicates the distance from data observation *i* to centroid *j*, *d*^x[i] is
    feature *x* of the *i*th data observation, and *c*^x[j] is the *x*-axis coordinate
    of centroid *j*.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 其中<math alttext="delta Subscript i comma j"><msub><mi>δ</mi> <mrow><mi>i</mi><mo>,</mo><mi>j</mi></mrow></msub></math>表示数据观测*i*到聚类中心*j*的距离，*d*^x[i]是第*i*个数据观测的*x*特征，*c*^x[j]是聚类中心*j*的*x*轴坐标。
- en: 'You might think that this step needs to be implemented using a double `for`
    loop: one loop over *k* centroids and a second loop over *N* data observations
    (you might even think of a third `for` loop over data features). However, we can
    use vectors and broadcasting to make this operation compact and efficient. This
    is an example of how linear algebra often looks different in equations compared
    to in code:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能认为这一步需要使用双重`for`循环来实现：一个循环用于*k*个聚类中心，第二个循环用于*N*个数据观测（你甚至可能考虑第三个`for`循环用于数据特征）。然而，我们可以使用向量和广播来使这个操作更加简洁和高效。这是线性代数在公式和代码中看起来不同的一个例子：
- en: '[PRE1]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Let’s think about the sizes of these variables: `data` is 150 × 2 (observations
    by features) and `centroids[ci,:]` is 1 × 2 (cluster `ci` by features). Formally,
    it is not possible to subtract these two vectors. However, Python will implement
    broadcasting by repeating the cluster centroids 150 times, thus subtracting the
    centroid from each data observation. The exponent operation `**` is applied element-wise,
    and the `axis=1` input tells Python to sum across the columns (separately per
    row). So, the output of `np.sum()` will be a 150 × 1 array that encodes the Euclidean
    distance of each point to centroid `ci`.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑这些变量的大小：`data`为150×2（观测点乘特征），`centroids[ci,:]`为1×2（聚类`ci`乘特征）。严格来说，不能从这两个向量中减去。然而，Python将通过将聚类中心重复150次来实现广播，因此从每个数据观测中减去聚类中心。指数操作`**`是逐元素应用的，`axis=1`输入告诉Python在列上求和（每行分别）。因此，`np.sum()`的输出将是一个150×1的数组，编码每个点到聚类中心`ci`的欧几里德距离。
- en: 'Take a moment to compare the code to the distance formula. Are they really
    the same? In fact, they are not: the square root in Euclidean distance is missing
    from the code. So is the code wrong? Think about this for a moment; I’ll discuss
    the answer later.'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 拿出一段时间来比较代码和距离公式。它们真的一样吗？事实上，它们并不一样：代码中缺少欧几里德距离中的平方根。所以代码错了吗？思考一下这个问题；我稍后会讨论答案。
- en: 'Step 3 is to assign each data observation to the group with minimum distance.
    This step is quite compact in Python, and can be implemented using one function:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 第3步是将每个数据观测分配到最小距离的组中。这一步在Python中非常简洁，可以使用一个函数实现：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Note the difference between `np.min`, which returns the minimum *value*, versus
    `np.argmin`, which returns the *index* at which the minimum occurs.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`np.min`和`np.argmin`之间的区别，前者返回最小*值*，而后者返回最小值出现的*索引*。
- en: We can now return to the inconsistency between the distance formula and its
    code implementation. For our *k*-means algorithm, we use distance to assign each
    data point to its closest centroid. Distance and squared distance are monotonically
    related, so both metrics give the same answer. Adding the square root operation
    increases code complexity and computation time with no impact on the results,
    so it can simply be omitted.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以回到距离公式及其代码实现之间的不一致性。对于我们的*k*-means算法，我们使用距离将每个数据点分配到其最近的质心。距离和平方距离是单调相关的，因此这两个度量给出相同的答案。增加平方根操作会增加代码复杂性和计算时间，而且对结果没有影响，因此可以简单地省略。
- en: 'Step 4 is to recompute the centroids as the mean of all data points within
    the class. Here we can loop over the *k* clusters, and use Python indexing to
    find all data points assigned to each cluster:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 第 4 步是重新计算每个类中所有数据点的平均值作为质心。在这里，我们可以循环遍历*k*个聚类，并使用 Python 索引来找到分配给每个聚类的所有数据点：
- en: '[PRE3]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Finally, Step 5 is to put the previous steps into a loop that iterates until
    a good solution is obtained. In production-level *k*-means algorithms, the iterations
    continue until a stopping criteria is reached, e.g., that the cluster centroids
    are no longer moving around. For simplicity, here we will iterate three times
    (an arbitrary number selected to make the plot visually balanced).
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第 5 步是将前面的步骤放入循环中，直到获得良好的解决方案。在生产级*k*-means算法中，迭代将继续进行，直到达到停止条件，例如集群质心不再移动。为了简单起见，这里我们将迭代三次（选择这个任意数量是为了使绘图视觉上平衡）。
- en: The four panels in [Figure 4-3](#fig_4_3) show the initial random cluster centroids
    (iteration 0), and their updated locations after each of three iterations.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 4-3](#fig_4_3) 中的四个面板显示了初始随机聚类质心（迭代 0），以及每次迭代后它们的更新位置。'
- en: If you study clustering algorithms, you will learn sophisticated methods for
    centroid initialization and stopping criteria, as well as quantitative methods
    to select an appropriate *k* parameter. Nonetheless, all *k*-means methods are
    essentially extensions of the above algorithm, and linear algebra is at the heart
    of their implementations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您学习聚类算法，您将学习到用于质心初始化和停止条件的复杂方法，以及选择适当*k*参数的定量方法。尽管如此，所有*k*-means方法本质上都是上述算法的扩展，而线性代数是它们实现的核心。
- en: '![k-means](assets/plad_0403.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![k-means](assets/plad_0403.png)'
- en: Figure 4-3\. k-means
  id: totrans-61
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-3\. k-means
- en: Code Exercises
  id: totrans-62
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 代码练习
- en: Correlation Exercises
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 相关性练习
- en: Exercise 4-1\.
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '练习 4-1\. '
- en: 'Write a Python function that takes two vectors as input and provides two numbers
    as output: the Pearson correlation coefficient and the cosine similarity value.
    Write code that follows the formulas presented in this chapter; don’t simply call
    `np.corrcoef` and `spatial.distance.cosine`. Check that the two output values
    are identical when the variables are already mean centered and different when
    the variables are not mean centered.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 编写一个Python函数，该函数以两个向量作为输入并提供两个数字作为输出：皮尔逊相关系数和余弦相似度值。编写符合本章介绍的公式的代码；不要简单地调用`np.corrcoef`和`spatial.distance.cosine`。检查当变量已经均值中心化时，这两个输出值是否相同，当变量未均值中心化时是否不同。
- en: Exercise 4-2\.
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '练习 4-2\. '
- en: Let’s continue exploring the difference between correlation and cosine similarity.
    Create a variable containing the integers 0 through 3, and a second variable equaling
    the first variable plus some offset. You will then create a simulation in which
    you systematically vary that offset between −50 and +50 (that is, the first iteration
    of the simulation will have the second variable equal to [−50, −49, −48, −47]).
    In a `for` loop, compute the correlation and cosine similarity between the two
    variables and store these results. Then make a line plot showing how the correlation
    and cosine similarity are affected by the mean offset. You should be able to reproduce
    [Figure 4-4](#fig_4_4).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续探讨相关性和余弦相似度之间的差异。创建一个包含整数 0 到 3 的变量，以及一个等于第一个变量加上某些偏移量的第二个变量。然后，您将创建一个模拟，在该模拟中系统地改变该偏移量在−50到+50之间（即，模拟的第一次迭代将使第二个变量等于[−50,
    −49, −48, −47]）。在`for`循环中，计算两个变量之间的相关性和余弦相似度，并存储这些结果。然后制作一张线图，展示相关性和余弦相似度如何受平均偏移的影响。您应该能够重现[图 4-4](#fig_4_4)。
- en: '![Cor and cos](assets/plad_0404.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![Cor 和 cos](assets/plad_0404.png)'
- en: Figure 4-4\. Results of Exercise 4-2
  id: totrans-69
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-4\. 练习 4-2 的结果
- en: Exercise 4-3\.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '练习 4-3\. '
- en: 'There are several Python functions to compute the Pearson correlation coefficient.
    One of them is called `pearsonr` and is located in the `stats` module of the SciPy
    library. Open the source code for this file (hint: `??functionname`) and make
    sure you understand how the Python implementation maps onto the formulas introduced
    in this chapter.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中有几个函数可以计算皮尔逊相关系数。其中一个叫做`pearsonr`，位于SciPy库的`stats`模块中。打开这个文件的源代码（提示：`??functionname`），确保你理解Python实现是如何映射到本章介绍的公式中的。
- en: Exercise 4-4\.
  id: totrans-72
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-4\.
- en: Why do you ever need to code your own functions when they already exist in Python?
    Part of the reason is that writing your own functions has huge educational value,
    because you see that (in this case) the correlation is a simple computation and
    not some incredibly sophisticated black-box algorithm that only a computer-science
    PhD could understand. But another reason is that built-in functions are sometimes
    slower because of myriad input checks, dealing with additional input options,
    converting data types, etc. This increases usability but at the expense of computation
    time.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在Python中已经存在函数的情况下，你仍然需要编写自己的函数？部分原因是编写自己的函数具有巨大的教育价值，因为你会看到（在本例中）相关性是一个简单的计算，而不是某种只有计算机科学博士才能理解的复杂黑盒算法。但另一个原因是，内置函数有时会更慢，因为需要进行大量的输入检查、处理额外的输入选项、数据类型转换等。这增加了可用性，但以计算时间为代价。
- en: Your goal in this exercise is to determine whether your own bare-bones correlation
    function is faster than NumPy’s `corrcoef` function. Modify the function from
    [Exercise 4-2](#exercise_4_2) to compute only the correlation coefficient. Then,
    in a `for` loop over 1,000 iterations, generate two variables of 500 random numbers
    and compute the correlation between them. Time the `for` loop. Then repeat but
    using `np.corrcoef`. In my tests, the custom function was about 33% faster than
    `np.corrcoef`. In these toy examples, the differences are measured in milliseconds,
    but if you are running billions of correlations with large datasets, those milliseconds
    really add up! (Note that writing your own functions without input checks has
    the risk of input errors that would be caught by `np.corrcoef`.) (Also note that
    the speed advantage breaks down for larger vectors. Try it!)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你的目标是确定你自己编写的简单相关函数是否比NumPy的`corrcoef`函数更快。修改[练习 4-2](#exercise_4_2)中的函数，只计算相关系数。然后，在一个包含
    1000 次迭代的`for`循环中，生成两个包含 500 个随机数的变量，并计算它们之间的相关性。计时这个`for`循环。然后重复，但使用`np.corrcoef`。在我的测试中，自定义函数比`np.corrcoef`快约
    33%。在这些玩具示例中，差异以毫秒计，但如果你在大数据集上运行数十亿个相关性计算，这些毫秒的累积非常显著！（请注意，编写没有输入检查的自定义函数存在输入错误的风险，而这些错误在`np.corrcoef`中会被捕获。）（还要注意，对于较大的向量，速度优势会消失。试一试！）
- en: Filtering and Feature Detection Exercises
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 过滤和特征检测练习
- en: Exercise 4-5\.
  id: totrans-76
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-5\.
- en: 'Let’s build an edge detector. The kernel for an edge detector is very simple:
    [−1 +1]. The dot product of that kernel with a snippet of a time series signal
    with constant value (e.g., [10 10]) is 0\. But that dot product is large when
    the signal has a steep change (e.g., [1 10] would produce a dot product of 9).
    The signal we’ll work with is a plateau function. Graphs A and B in [Figure 4-5](#fig_4_5)
    show the kernel and the signal. The first step in this exercise is to write code
    that creates these two time series.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个边缘检测器。边缘检测器的核非常简单：[−1 +1]。该核与时间序列信号片段的点积在信号具有恒定值（例如，[10 10]）时为 0。但当信号有陡峭变化时（例如，[1
    10]会产生点积为 9），该点积就会很大。我们将使用的信号是一个平台函数。图 A 和 B 在[图 4-5](#fig_4_5)中展示了核和信号。这个练习的第一步是编写代码创建这两个时间序列。
- en: '![What does this do?](assets/plad_0405.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![这是做什么的？](assets/plad_0405.png)'
- en: Figure 4-5\. Results of Exercise 4-5
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-5\. 练习 4-5 的结果
- en: Next, write a `for` loop over the time points in the signal. At each time point,
    compute the dot product between the kernel and a segment of the time series data
    that has the same length as the kernel. You should produce a plot that looks like
    graph C in [Figure 4-5](#fig_4_5). (Focus more on the result than on the aesthetics.)
    Notice that our edge detector returned 0 when the signal was flat, +1 when the
    signal jumped up, and −1 when the signal jumped down.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在信号的时间点上编写一个`for`循环。在每个时间点，计算核与与核相同长度的时间序列数据段之间的点积。你应该生成一个看起来像图 C 在[图 4-5](#fig_4_5)中的图的绘图结果。（更关注结果而不是美观度。）注意，我们的边缘检测器在信号平坦时返回
    0，在信号跳变时返回 +1，在信号跳降时返回 −1。
- en: Feel free to continue exploring this code. For example, does anything change
    if you pad the kernel with zeros ([0 −1 1 0])? What about if you flip the kernel
    to be [1 −1]? How about if the kernel is asymmetric ([−1 2])?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 随意继续探索这段代码。例如，如果用零（[0 −1 1 0]）填充核心会发生什么变化？如果将核心翻转为[1 −1]会怎样？如果核心是不对称的（[−1 2]）会发生什么变化？
- en: Exercise 4-6\.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-6。
- en: Now we will repeat the same procedure but with a different signal and kernel.
    The goal will be to smooth a rugged time series. The time series will be 100 random
    numbers generated from a Gaussian distribution (also called a normal distribution).
    The kernel will be a bell-shaped function that approximates a Gaussian function,
    defined as the numbers [0, .1, .3, .8, 1, .8, .3, .1, 0] but scaled so that the
    sum over the kernel is 1\. Your kernel should match graph A in [Figure 4-6](#fig_4_6),
    although your signal won’t look exactly like graph B due to random numbers.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将重复相同的过程，但使用不同的信号和核函数。目标是平滑一个崎岖不平的时间序列。时间序列将是从高斯分布（也称为正态分布）生成的100个随机数。核函数将是一个钟形函数，近似于高斯函数，其定义为数字[0,
    .1, .3, .8, 1, .8, .3, .1, 0]，但缩放使得核的总和为1。你的核应该与[图 4-6](#fig_4_6)中的A图相匹配，尽管由于随机数的原因，你的信号看起来不会完全像B图。
- en: 'Copy and adapt the code from the previous exercise to compute the sliding time
    series of dot products—the signal filtered by the Gaussian kernel. Warning: be
    mindful of the indexing in the `for` loop. Graph C in [Figure 4-6](#fig_4_6) shows
    an example result. You can see that the filtered signal is a smoothed version
    of the original signal. This is also called low-pass filtering.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 复制并调整上一个练习中的代码，计算通过高斯核过滤的滑动时间序列的点积。警告：要注意`for`循环中的索引。[图 4-6](#fig_4_6)中的C图显示了一个示例结果。你可以看到，经过滤波的信号是原始信号的平滑版本。这也被称为低通滤波。
- en: '![Exercise 4-6](assets/plad_0406.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![练习 4-6](assets/plad_0406.png)'
- en: Figure 4-6\. Results of Exercise 4-6
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-6。练习 4-6的结果
- en: Exercise 4-7\.
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-7。
- en: Replace the 1 in the center of the kernel with −1 and mean center the kernel.
    Then rerun the filtering and plotting code. What is the result? It actually accentuates
    the sharp features! In fact, this kernel is now a high-pass filter, meaning it
    dampens the smooth (low-frequency) features and highlights the rapidly changing
    (high-frequency) features.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 将核心中的1替换为-1并对核心进行平均中心化。然后重新运行过滤和绘图代码。结果是什么？实际上，这会突显出尖锐的特征！事实上，这个核现在是一个高通滤波器，意味着它减弱了平滑（低频）特征并突出显示快速变化（高频）特征。
- en: '*k*-Means Exercises'
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '*k*-均值聚类练习'
- en: Exercise 4-8\.
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-8。
- en: One way to determine an optimal *k* is to repeat the clustering multiple times
    (each time using randomly initialized cluster centroids) and assess whether the
    final clustering is the same or different. Without generating new data, rerun
    the *k*-means code several times using *k* = 3 to see whether the resulting clusters
    are similar (this is a qualitative assessment based on visual inspection). Do
    the final cluster assignments generally seem similar even though the centroids
    are randomly selected?
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 确定一个最优*k*的一种方法是多次重复聚类（每次使用随机初始化的聚类中心）并评估最终的聚类是否相同或不同。不生成新数据的情况下，使用*k* = 3多次重新运行*k*-均值代码，看看最终的聚类分配是否通常看起来相似（这是基于视觉检查的定性评估）。即使聚类中心是随机选择的，最终的聚类分配是否通常看起来相似？
- en: Exercise 4-9\.
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 练习 4-9。
- en: Repeat the multiple clusterings using *k* = 2 and *k* = 4\. What do you think
    of these results?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 使用*k* = 2和*k* = 4重复多次聚类。你觉得这些结果如何？
- en: '^([1](ch04.xhtml#idm45733310634320-marker)) Reminder: Euclidean distance is
    the square root of the sum of squared distances from the data observation to the
    centroid.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch04.xhtml#idm45733310634320-marker)) 提示：欧氏距离是从数据观测到质心的平方距离之和的平方根。
