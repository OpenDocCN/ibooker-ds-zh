- en: 2 Types of machine learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 机器学习的两种类型
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: 'three different types of machine learning: supervised, unsupervised, and reinforcement
    learning'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种不同的机器学习类型：监督学习、无监督学习和强化学习
- en: the difference between labeled and unlabeled data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签数据和未标记数据之间的区别
- en: the difference between regression and classification, and how they are used
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归和分类之间的区别以及它们的使用方法
- en: '![](../Images/2-unnumb.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-unnumb.png)'
- en: As we learned in chapter 1, machine learning is common sense for a computer.
    Machine learning roughly mimics the process by which humans make decisions based
    on experience, by making decisions based on previous data. Naturally, programming
    computers to mimic the human thinking process is challenging, because computers
    are engineered to store and process numbers, not make decisions. This is the task
    that machine learning aims to tackle. Machine learning is divided into several
    branches, depending on the type of decision to be made. In this chapter, we overview
    some of the most important among these branches.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第1章中学到的，机器学习对计算机来说是常识。机器学习大致模仿了人类根据经验做出决策的过程，即基于以往的数据做出决策。自然地，编程计算机模仿人类思维过程是具有挑战性的，因为计算机被设计用来存储和处理数字，而不是做出决策。这正是机器学习旨在解决的问题。机器学习根据要做出的决策类型分为几个分支。在本章中，我们将概述这些分支中最重要的一些。
- en: 'Machine learning has applications in many fields, such as the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在许多领域都有应用，例如以下：
- en: Predicting house prices based on the house’s size, number of rooms, and location
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据房屋的大小、房间数量和位置预测房价
- en: Predicting today’s stock market prices based on yesterday’s prices and other
    factors of the market
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据昨天的股价和市场其他因素预测今天的股市价格
- en: Detecting spam and non-spam emails based on the words in the e-mail and the
    sender
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据电子邮件中的文字和发件人检测垃圾邮件和非垃圾邮件
- en: Recognizing images as faces or animals, based on the pixels in the image
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据图像中的像素识别图像为面孔或动物
- en: Processing long text documents and outputting a summary
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理长文本文档并输出摘要
- en: Recommending videos or movies to a user (e.g., on YouTube or Netflix)
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向用户推荐视频或电影（例如，在YouTube或Netflix上）
- en: Building chatbots that interact with humans and answer questions
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建与人类互动并回答问题的聊天机器人
- en: Training self-driving cars to navigate a city by themselves
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练自动驾驶汽车自行在城市中导航
- en: Diagnosing patients as sick or healthy
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断患者为生病或健康
- en: Segmenting the market into similar groups based on location, acquisitive power,
    and interests
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据地理位置、购买力和兴趣将市场划分为相似的群体
- en: Playing games like chess or Go
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 玩像象棋或围棋这样的游戏
- en: Try to imagine how we could use machine learning in each of these fields. Notice
    that some of these applications are different but can be solved in a similar way.
    For example, predicting housing prices and predicting stock prices can be done
    using similar techniques. Likewise, predicting whether an email is spam and predicting
    whether a credit card transaction is legitimate or fraudulent can also be done
    using similar techniques. What about grouping users of an app based on their similarity?
    That sounds different from predicting housing prices, but it could be done similarly
    to grouping newspaper articles by topic. And what about playing chess? That sounds
    different from all the other previous applications, but it could be like playing
    Go.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试想象我们如何在每个这些领域中应用机器学习。注意，这些应用虽然不同，但可以用类似的方法解决。例如，预测房价和预测股价可以使用类似的技术。同样，预测一封电子邮件是否为垃圾邮件以及预测信用卡交易是否合法或欺诈也可以使用类似的技术。那么，根据用户的相似性对应用程序的用户进行分组呢？这听起来与预测房价不同，但可以类似地根据主题对报纸文章进行分组。那么下棋呢？这听起来与所有其他先前应用都不同，但可能像下围棋一样。
- en: Machine learning models are grouped into different types, according to the way
    they operate. The main three families of machine learning models are
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 根据它们操作的方式，机器学习模型被分为不同的类型。机器学习模型的主要三个家族是
- en: '*supervised learning*,'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*监督学习*，'
- en: '*unsupervised learning*, and'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*无监督学习*，'
- en: '*reinforcement learning*.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*强化学习*。'
- en: In this chapter, we overview all three. However, in this book, we focus only
    on supervised learning because it is the most natural one to start learning and
    arguably the most used right now. Look up the other types in the literature and
    learn about them, too, because they are all interesting and useful! In the resources
    in appendix C, you can find some interesting links, including several videos created
    by the author.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们概述了所有三个概念。然而，在这本书中，我们只关注监督学习，因为它是最自然的学习起点，并且可以说是目前最常用的。查阅文献中的其他类型，并了解它们，因为它们都很有趣且很有用！在附录C的资源中，你可以找到一些有趣的链接，包括作者创建的几个视频。
- en: What is the difference between labeled and unlabeled data?
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 标记数据和未标记数据有什么区别？
- en: What is data?
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据？
- en: We talked about data in chapter 1, but before we go any further, let’s first
    establish a clear definition of what we mean by *data* in this book. Data is simply
    information. Any time we have a table with information, we have data. Normally,
    each row in our table is a data point. Say, for example, that we have a dataset
    of pets. In this case, each row represents a different pet. Each pet in the table
    is described by certain features of that pet.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第一章中讨论了数据，但在我们进一步讨论之前，让我们首先明确我们在这本书中提到的*数据*的定义。数据仅仅是信息。每次我们有一个包含信息的表格时，我们就有数据。通常，我们表格中的每一行都是一个数据点。比如说，我们有一个宠物数据集。在这种情况下，每一行代表不同的宠物。表格中的每个宠物都由该宠物的某些特征来描述。
- en: And what are features?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 那么什么是特征？
- en: In chapter 1, we defined features as the properties or characteristics of the
    data. If our data is in a table, the features are the columns of the table. In
    our pet example, the features may be size, name, type, or weight. Features could
    even be the colors of the pixels in an image of the pet. This is what describes
    our data. Some features are special, though, and we call them *labels*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我们将特征定义为数据的属性或特征。如果我们的数据在一个表格中，特征就是表格的列。在我们的宠物例子中，特征可能是大小、名字、类型或重量。特征甚至可以是宠物图像中像素的颜色。这就是描述我们的数据的内容。尽管如此，一些特征是特殊的，我们称它们为*标签*。
- en: Labels?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 标签？
- en: This one is a bit less straightforward, because it depends on the context of
    the problem we are trying to solve. Normally, if we are trying to predict a particular
    feature based on the other ones, that feature is the label. If we are trying to
    predict the type of pet (e.g., cat or dog) based on information on that pet, then
    the label is the type of pet (cat/dog). If we are trying to predict if the pet
    is sick or healthy based on symptoms and other information, then the label is
    the state of the pet (sick/healthy). If we are trying to predict the age of the
    pet, then the label is the age (a number).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题稍微有点复杂，因为它取决于我们试图解决的问题的上下文。通常，如果我们试图根据其他特征预测一个特定的特征，那么这个特征就是标签。如果我们试图根据宠物的信息预测宠物的类型（例如，猫或狗），那么标签就是宠物的类型（猫/狗）。如果我们试图根据症状和其他信息预测宠物是否生病或健康，那么标签就是宠物的状态（生病/健康）。如果我们试图预测宠物的年龄，那么标签就是年龄（一个数字）。
- en: Predictions
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 预测
- en: We have been using the concept of making predictions freely, but let’s now pin
    it down. The goal of a predictive machine learning model is to guess the labels
    in the data. The guess that the model makes is called a *prediction*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直在使用自由预测的概念，但现在让我们将其明确下来。预测机器学习模型的目标是猜测数据中的标签。模型做出的猜测称为*预测*。
- en: 'Now that we know what labels are, we can understand there are two main types
    of data: *labeled* and *unlabeled* data.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了标签是什么，我们可以理解主要有两种类型的数据：*标记*数据和*未标记*数据。
- en: Labeled and unlabeled data
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和未标记数据
- en: Labeled data is data that comes with labels. Unlabeled data is data that comes
    with no labels. An example of labeled data is a dataset of emails that comes with
    a column that records whether the emails are spam or ham, or a column that records
    whether the email is work related. An example of unlabeled data is a dataset of
    emails that has no particular column we are interested in predicting.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 标记数据是带有标签的数据。未标记数据是没有任何标签的数据。标记数据的例子是一个带有记录邮件是否为垃圾邮件或工作相关的列的电子邮件数据集。未标记数据的例子是一个没有我们感兴趣预测的特定列的电子邮件数据集。
- en: In figure 2.1, we see three datasets containing images of pets. The first dataset
    has a column recording the type of pet, and the second dataset has a column specifying
    the weight of the pet. These two are examples of labeled data. The third dataset
    consists only of images, with no label, making it unlabeled data.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.1中，我们看到包含宠物图像的三个数据集。第一个数据集有一列记录宠物的类型，第二个数据集有一列指定宠物的重量。这两个是标记数据的例子。第三个数据集仅包含图像，没有标签，因此是无标签数据。
- en: '![](../Images/2-1.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-1.png)'
- en: Figure 2.1 Labeled data is data that comes with a tag, or label. That label
    can be a type or a number. Unlabeled data is data that comes with no tag. The
    dataset on the left is labeled, and the label is the type of pet (dog/cat). The
    dataset in the middle is also labeled, and the label is the weight of the pet
    (in pounds). The dataset on the right is unlabeled.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 标记数据是带有标签或标记的数据。这个标签可以是类型或数字。无标签数据是没有标签的数据。左侧的数据集是标记的，标签是宠物的类型（狗/猫）。中间的数据集也是标记的，标签是宠物的重量（以磅为单位）。右侧的数据集是无标签的。
- en: Of course, this definition contains some ambiguity, because depending on the
    problem, we decide whether a particular feature qualifies as a label. Thus, determining
    if data is labeled or unlabeled, many times, depends on the problem we are trying
    to solve.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这个定义包含一些歧义，因为根据问题，我们决定一个特定的特征是否可以作为标签。因此，确定数据是否标记或无标签，很多时候取决于我们试图解决的问题。
- en: Labeled and unlabeled data yield two different branches of machine learning
    called *supervised* and *unsupervised* learning, which are defined in the next
    three sections.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 标记和无标签数据产生了两种不同的机器学习分支，称为**监督学习**和**无监督学习**，这些将在接下来的三个部分中定义。
- en: 'Supervised learning: The branch of machine learning that works with labeled
    data'
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习：与标记数据一起工作的机器学习分支
- en: We can find supervised learning in some of the most common applications nowadays,
    including image recognition, various forms of text processing, and recommendation
    systems. Supervised learning is a type of machine learning that uses labeled data.
    In short, the goal of a supervised learning model is to predict (guess) the labels.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在当今一些最常见应用中找到监督学习，包括图像识别、各种形式的文本处理和推荐系统。监督学习是一种使用标记数据的机器学习类型。简而言之，监督学习模型的目标是预测（猜测）标签。
- en: In the example in figure 2.1, the dataset on the left contains images of dogs
    and cats, and the labels are “dog” and “cat.” For this dataset, the machine learning
    model would use previous data to predict the label of new data points. This means,
    if we bring in a new image *without* a label, the model will guess whether the
    image is of a dog or a cat, thus predicting the label of the data point (figure
    2.2).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.1的示例中，左侧的数据集包含狗和猫的图像，标签是“狗”和“猫”。对于这个数据集，机器学习模型将使用先前数据来预测新数据点的标签。这意味着，如果我们引入一个没有标签的新图像，模型将猜测该图像是狗还是猫，从而预测数据点的标签（图2.2）。
- en: '![](../Images/2-2.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-2.png)'
- en: Figure 2.2 A supervised learning model predicts the label of a new data point.
    In this case, the data point corresponds to a dog, and the supervised learning
    algorithm is trained to predict that this data point does, indeed, correspond
    to a dog.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 一个监督学习模型预测新数据点的标签。在这种情况下，数据点对应于狗，监督学习算法被训练来预测这个数据点确实对应于狗。
- en: If you recall from chapter 1, the framework we learned for making a decision
    was remember-formulate-predict. This is precisely how supervised learning works.
    The model first **remembers** the dataset of dogs and cats. Then it **formulates**
    a model, or a rule, for what it believes constitutes a dog and a cat. Finally,
    when a new image comes in, the model makes a **prediction** about what it thinks
    the label of the image is, namely, a dog or a cat (figure 2.3).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您还记得第一章中我们学习用于做决策的框架是“记住-制定-预测”。这正是监督学习的工作方式。模型首先**记住**狗和猫的数据集。然后它**制定**一个模型或规则，即它认为构成狗和猫的特征。最后，当一个新的图像进来时，模型对它认为的图像标签做出**预测**，即狗或猫（图2.3）。
- en: '![](../Images/2-3.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-3.png)'
- en: Figure 2.3 A supervised learning model follows the remember-formulate-predict
    framework from chapter 1\. First, it remembers the dataset. Then, it formulates
    rules for what would constitute a dog and a cat. Finally, it predicts whether
    a new data point is a dog or a cat.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 一个监督学习模型遵循第一章中提到的“记住-制定-预测”框架。首先，它记住数据集。然后，它制定构成狗和猫的规则。最后，它预测新的数据点是否是狗或猫。
- en: Now, notice that in figure 2.1, we have two types of labeled datasets. In the
    dataset in the middle, each data point is labeled with the weight of the animal.
    In this dataset, the labels are numbers. In the dataset on the left, each data
    point is labeled with the type of animal (dog or cat). In this dataset, the labels
    are states. Numbers and states are the two types of data that we’ll encounter
    in supervised learning models. We call the first type *numerical data* and the
    second type *categorical data*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，请注意在图2.1中，我们有两种类型的标记数据集。在中间的数据集中，每个数据点都标记了动物的重量。在这个数据集中，标签是数字。在左侧的数据集中，每个数据点都标记了动物的类型（狗或猫）。在这个数据集中，标签是状态。数字和状态是我们将在监督学习模型中遇到的两种类型的数据。我们称第一种类型为**数值数据**，第二种类型为**分类数据**。
- en: numerical data is any type of data that uses numbers such as 4, 2.35, or –199\.
    Examples of numerical data are prices, sizes, or weights.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '**数值数据**是任何使用数字的数据类型，例如4、2.35或-199。数值数据的例子包括价格、大小或重量。'
- en: categorical data is any type of data that uses categories, or states, such as
    male/female or cat/dog/bird. For this type of data, we have a finite set of categories
    to associate to each of the data points.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**分类数据**是任何使用类别或状态的类型的数据，例如男/女或猫/狗/鸟。对于这类数据，我们有一个有限集合的类别可以与每个数据点关联。'
- en: 'This gives rise to the following two types of supervised learning models:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 这导致了以下两种类型的监督学习模型：
- en: regression models are the types of models that predict **numerical data**. The
    output of a regression model is a *number*, such as the weight of the animal.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型是预测**数值数据**的模型类型。回归模型的输出是一个**数字**，例如动物的重量。
- en: classification models are the types of models that predict **categorical data**.
    The output of a classification model is a *category*, or a *state*, such as the
    type of animal (cat or dog).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型是预测**分类数据**的模型类型。分类模型的输出是一个**类别**，或一个**状态**，例如动物的类型（猫或狗）。
- en: Let’s look at two examples of supervised learning models, one regression and
    one classification.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两个监督学习模型的例子，一个是回归，一个是分类。
- en: '**Model 1: housing prices model** **(regression).** In this model, each data
    point is a house. The label of each house is its price. Our goal is that when
    a new house (data point) comes on the market, we would like to predict its label,
    namely, its price.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型1：房价模型** **（回归）**。在这个模型中，每个数据点是一栋房屋。每栋房屋的标签是它的价格。我们的目标是，当一栋新的房屋（数据点）上市时，我们希望预测其标签，即它的价格。'
- en: '**Model 2: email spam–detection model** **(classification).** In this model,
    each data point is an email. The label of each email is either spam or ham. Our
    goal is that when a new email (data point) comes into our inbox, we would like
    to predict its label, namely, whether it is spam or ham.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型2：电子邮件垃圾邮件检测模型** **（分类）**。在这个模型中，每个数据点是一封电子邮件。每封电子邮件的标签是垃圾邮件或非垃圾邮件。我们的目标是，当一封新的电子邮件（数据点）进入我们的收件箱时，我们希望预测其标签，即它是垃圾邮件还是非垃圾邮件。'
- en: Notice the difference between models 1 and 2.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意模型1和模型2之间的区别。
- en: The housing prices model is a model that can return a number from many possibilities,
    such as $100, $250,000, or $3,125,672.33\. Thus, it is a *regression* model.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房价模型是一个可以从许多可能性中返回数字的模型，例如$100、$250,000或$3,125,672.33。因此，它是一个**回归**模型。
- en: 'The spam detection model, on the other hand, can return only two things: spam
    or ham. Thus, it is a *classification* model.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与此相反，垃圾邮件检测模型只能返回两件事：垃圾邮件或非垃圾邮件。因此，它是一个**分类**模型。
- en: In the following subsections, we elaborate more on regression and classification.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将更详细地介绍回归和分类。
- en: Regression models predict numbers
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 回归模型预测数字
- en: As we discussed previously, regression models are those in which the label we
    want to predict is a number. This number is predicted based on the features. In
    the housing example, the features can be anything that describes a house, such
    as the size, the number of rooms, the distance to the closest school, or the crime
    rate in the neighborhood.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前讨论的，回归模型是我们想要预测的标签是一个数字的模型。这个数字是基于特征预测的。在房屋的例子中，特征可以是描述房屋的任何东西，例如大小、房间数量、距离最近学校的距离或社区的犯罪率。
- en: 'Other places where one can use regression models follow:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 其他可以使用回归模型的地方包括：
- en: '**Stock market**: predicting the price of a certain stock based on other stock
    prices and other market signals'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**股市**：根据其他股票价格和其他市场信号预测某只股票的价格'
- en: '**Medicine**: predicting the expected life span of a patient or the expected
    recovery time, based on symptoms and the medical history of the patient'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学**：根据患者的症状和病史预测患者的预期寿命或预期恢复时间'
- en: '**Sales**: predicting the expected amount of money a customer will spend, based
    on the client’s demographics and past purchase behavior'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**销售**：根据客户的人口统计信息和过去的购买行为预测客户预期的消费金额'
- en: '**Video recommendations**: predicting the expected amount of time a user will
    watch a video, based on the user’s demographics and other videos they have watched'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频推荐**：根据用户的人口统计信息和用户观看的其他视频预测用户预期观看视频的时间'
- en: The most common method used for regression is linear regression, which uses
    linear functions (lines or similar objects) to make our predictions based on the
    features. We study linear regression in chapter 3\. Other popular methods used
    for regression are decision tree regression, which we learn in chapter 9, and
    several ensemble methods such as random forests, AdaBoost, gradient boosted trees,
    and XGBoost, which we learn in chapter 12.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 用于回归的最常见方法是线性回归，它使用线性函数（直线或类似对象）根据特征进行预测。我们在第3章中学习了线性回归。其他用于回归的流行方法包括决策树回归，我们在第9章中学习，以及几种集成方法，如随机森林、AdaBoost、梯度提升树和XGBoost，我们在第12章中学习。
- en: Classification models predict a state
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型预测一个状态
- en: Classification models are those in which the label we want to predict is a state
    belonging to a finite set of states. The most common classification models predict
    a “yes” or a “no,” but many other models use a larger set of states. The example
    we saw in figure 2.3 is an example of classification, because it predicts the
    type of the pet, namely, “cat” or “dog.”
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型是那些我们想要预测的标签属于有限状态集合的模型。最常见的分类模型预测“是”或“否”，但许多其他模型使用更大的状态集合。我们在图2.3中看到的例子是分类的一个例子，因为它预测宠物的类型，即“猫”或“狗”。
- en: In the email spam recognition example, the model predicts the state of the email
    (namely, spam or ham) from the features of the email. In this case, the features
    of the email can be the words on it, the number of spelling mistakes, the sender,
    or anything else that describes the email.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子邮件垃圾邮件识别的例子中，模型根据电子邮件的特征预测电子邮件的状态（即垃圾邮件或正常邮件）。在这种情况下，电子邮件的特征可以是邮件上的单词、拼写错误的数量、发件人或任何描述电子邮件的其他内容。
- en: 'Another common application of classification is image recognition. The most
    popular image recognition models take as input the pixels in the image, and they
    output a prediction of what the image depicts. Two of the most famous datasets
    for image recognition are MNIST and CIFAR-10\. MNIST contains approximately 60,000
    28-by-28-pixel black-and-white images of handwritten digits which are labelled
    0–9\. These images come from a combination of sources, including the American
    Census Bureau and a repository of handwritten digits written by American high
    school students. The MNIST dataset can be found in the following link: [http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)**.**
    The CIFAR-10 dataset contains 60,000 32-by-32-pixel colored images of different
    things. These images are labeled with 10 different objects (thus the 10 in its
    name), namely airplanes, cars, birds, cats, deer, dogs, frogs, horses, ships,
    and trucks. This database is maintained by the Canadian Institute for Advanced
    Research (CIFAR), and it can be found in the following link: [https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)**.**'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个常见的分类应用是图像识别。最流行的图像识别模型将图像中的像素作为输入，并输出对图像内容的预测。两个最著名的图像识别数据集是MNIST和CIFAR-10。MNIST包含大约60,000张28×28像素的黑白手写数字图像，这些图像被标记为0到9。这些图像来自多个来源的组合，包括美国人口普查局和美国高中生手写数字的存储库。MNIST数据集可以在以下链接找到：[http://yann.lecun.com/exdb/mnist/](http://yann.lecun.com/exdb/mnist/)**。**
    CIFAR-10数据集包含60,000张32×32像素的不同事物的彩色图像。这些图像被标记为10个不同的对象（因此其名称中的10），即飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船只和卡车。该数据库由加拿大高级研究研究所（CIFAR）维护，可以在以下链接找到：[https://www.cs.toronto.edu/~kriz/cifar.html](https://www.cs.toronto.edu/~kriz/cifar.html)**。**
- en: 'Some additional powerful applications of classification models follow:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 分类模型的一些其他强大应用如下：
- en: '**Sentiment analysis**: predicting whether a movie review is positive or negative,
    based on the words in the review'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**情感分析**：根据评论中的词语预测电影评论是正面还是负面'
- en: '**Website traffic**: predicting whether a user will click a link or not, based
    on the user’s demographics and past interaction with the site'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**网站流量**：根据用户的 demographics 和他们与网站的过往互动预测用户是否会点击链接'
- en: '**Social media**: predicting whether a user will befriend or interact with
    another user, based on their demographics, history, and friends in common'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**社交媒体**：根据用户的 demographics、历史和共同朋友预测用户是否会成为朋友或与另一个用户互动'
- en: '**Video recommendations**: predicting whether a user will watch a video, based
    on the user’s demographics and other videos they have watched'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频推荐**：根据用户的 demographics 和他们观看的其他视频预测用户是否会观看视频'
- en: The bulk of this book (chapters 5, 6, 8, 9, 10, 11, and 12) covers classification
    models. In these chapters we learn the perceptrons (chapter 5), logistic classifiers
    (chapter 6), the naive Bayes algorithm (chapter 8), decision trees (chapter 9),
    neural networks (chapter 10), support vector machines (chapter 11), and ensemble
    methods (chapter 12).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的大部分内容（第5、6、8、9、10、11和12章）涵盖了分类模型。在这些章节中，我们学习了感知器（第5章）、逻辑分类器（第6章）、朴素贝叶斯算法（第8章）、决策树（第9章）、神经网络（第10章）、支持向量机（第11章）和集成方法（第12章）。
- en: 'Unsupervised learning: The branch of machine learning that works with unlabeled
    data'
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习：与无标签数据一起工作的机器学习分支
- en: Unsupervised learning is also a common type of machine learning. It differs
    from supervised learning in that the data is unlabeled. In other words, the goal
    of a machine learning model is to extract as much information as possible from
    a dataset that has no labels, or targets to predict.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习也是一种常见的机器学习类型。它与监督学习的不同之处在于数据是无标签的。换句话说，机器学习模型的目标是从没有标签或预测目标的数据集中提取尽可能多的信息。
- en: What could such a dataset be, and what could we do with it? In principle, we
    can do a little less than what we can do with a labeled dataset, because we have
    no labels to predict. However, we can still extract a lot of information from
    an unlabeled dataset. For example, let’s go back to the cats and dogs example
    on the rightmost dataset in figure 2.1\. This dataset consists of images of cats
    and dogs, but it has no labels. Therefore, we don’t know what type of pet each
    image represents, so we can’t predict if a new image corresponds to a dog or a
    cat. However, we can do other things, such as determine if two pictures are similar
    or different. This is something unsupervised learning algorithms do. An unsupervised
    learning algorithm can group the images based on similarity, even without knowing
    what each group represents (figure 2.4). If done properly, the algorithm could
    separate the dog images from the cat images, or even group each of them by breed!
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的数据集可能是什么，我们能用它做什么？原则上，我们可能比使用有标签的数据集能做的少一些，因为我们没有标签来预测。然而，我们仍然可以从无标签的数据集中提取大量信息。例如，让我们回到图2.1最右侧数据集上的猫和狗的例子。这个数据集由猫和狗的图像组成，但没有标签。因此，我们不知道每张图像代表哪种宠物类型，所以无法预测新图像是否对应狗或猫。然而，我们还可以做其他事情，例如确定两张图片是否相似或不同。这是无监督学习算法所做的事情。无监督学习算法可以根据相似性对图像进行分组，即使不知道每个组代表什么（图2.4）。如果做得恰当，算法可以将狗图像与猫图像分开，甚至可以根据品种将它们各自分组！
- en: '![](../Images/2-4.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-4.png)'
- en: Figure 2.4 An unsupervised learning algorithm can still extract information
    from data. For example, it can group similar elements together.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 无监督学习算法仍然可以从数据中提取信息。例如，它可以把相似元素分组在一起。
- en: As a matter of fact, even if the labels are there, we can still use unsupervised
    learning techniques on our data to preprocess it and apply supervised learning
    methods more effectively.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，即使标签存在，我们仍然可以在我们的数据上使用无监督学习技术进行预处理，并更有效地应用监督学习方法。
- en: The main branches of unsupervised learning are clustering, dimensionality reduction,
    and generative learning.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督学习的主要分支是聚类、维度约简和生成学习。
- en: clustering algorithms The algorithms that group data into clusters based on
    similarity
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法 根据相似性将数据分组到簇中的算法
- en: dimensionality reduction algorithms The algorithms that simplify our data and
    faithfully describe it with fewer features
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 维度约简算法 简化我们的数据，并用更少的特征忠实描述它的算法
- en: generative algorithms The algorithms that can generate new data points that
    resemble the existing data
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 生成算法 可以生成与现有数据相似的新数据点的算法
- en: In the following three subsections, we study these three branches in more detail.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的三个小节中，我们将更详细地研究这三个分支。
- en: Clustering algorithms split a dataset into similar groups
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类算法将数据集分割成相似组
- en: As we stated previously, clustering algorithms are those that split the dataset
    into similar groups. To illustrate this, let’s go back to the two datasets in
    the section “Supervised learning”—the housing dataset and the spam email dataset—but
    imagine that they have no labels. This means that the housing dataset has no prices,
    and the email dataset has no information on the emails being spam or ham.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前所述，聚类算法是将数据集分割成相似组的算法。为了说明这一点，让我们回到“监督学习”部分中的两个数据集——住房数据集和垃圾邮件数据集——但想象一下它们没有标签。这意味着住房数据集没有价格，而邮件数据集没有关于邮件是否为垃圾邮件或非垃圾邮件的信息。
- en: 'Let’s begin with the housing dataset. What can we do with this dataset? Here
    is an idea: we could somehow group the houses by similarity. For example, we could
    group them by location, price, size, or a combination of these factors. This process
    is called *clustering*. Clustering is a branch of unsupervised machine learning
    that consists of the tasks that group the elements in our dataset into clusters
    where all the data points are similar.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从住房数据集开始。我们能用这个数据集做什么？这里有一个想法：我们可以以某种方式根据相似性对房屋进行分组。例如，我们可以根据位置、价格、大小或这些因素的组合来分组。这个过程被称为*聚类*。聚类是无监督机器学习的一个分支，它包括将我们的数据集中的元素分组到簇中的任务，其中所有数据点都是相似的。
- en: Now let’s look at the second example, the dataset of emails. Because the dataset
    is unlabeled, we don’t know whether each email is spam or ham. However, we can
    still apply some clustering to our dataset. A clustering algorithm splits our
    images into a few different groups based on different features of the email. These
    features could be the words in the message, the sender, the number and size of
    the attachments, or the types of links inside the email. After clustering the
    dataset, a human (or a combination of a human and a supervised learning algorithm)
    could label these clusters by categories such as “Personal,” “Social,” and “Promotions.”
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看第二个例子，邮件数据集。因为数据集没有标签，我们不知道每封邮件是否为垃圾邮件。然而，我们仍然可以对我们数据集应用一些聚类。聚类算法根据邮件的不同特征将我们的图像分割成几个不同的组。这些特征可能是信息中的单词、发件人、附件的数量和大小，或者邮件内部的链接类型。聚类数据集后，一个人（或人与监督学习算法的组合）可以根据“个人”、“社交”和“促销”等类别对这些簇进行标记。
- en: As an example, let’s look at the dataset in table 2.1, which contains nine emails
    that we would like to cluster. The features of the dataset are the size of the
    email and the number of recipients.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们看看表2.1中的数据集，其中包含九封我们想要聚类的邮件。数据集的特征是邮件的大小和收件人数量。
- en: Table 2.1 A table of emails with their size and number of recipients
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 包含大小和收件人数量的邮件表
- en: '| Email | Size | Recipients |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 邮件 | 大小 | 收件人 |'
- en: '| 1 | 8 | 1 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 8 | 1 |'
- en: '| 2 | 12 | 1 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 12 | 1 |'
- en: '| 3 | 43 | 1 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 43 | 1 |'
- en: '| 4 | 10 | 2 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 10 | 2 |'
- en: '| 5 | 40 | 2 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 40 | 2 |'
- en: '| 6 | 25 | 5 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 25 | 5 |'
- en: '| 7 | 23 | 6 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 23 | 6 |'
- en: '| 8 | 28 | 6 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 28 | 6 |'
- en: '| 9 | 26 | 7 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 26 | 7 |'
- en: 'To the naked eye, it looks like we could group the emails by their number of
    recipients. This would result in two clusters: one with emails having two or fewer
    recipients, and one with emails having five or more recipients. We could also
    try to group them into three groups by size. But you can imagine that as the table
    gets larger and larger, eyeballing the groups gets harder and harder. What if
    we plot the data? Let’s plot the emails in a graph, where the horizontal axis
    records the size and the vertical axis records the number of recipients. This
    gives us the plot in figure 2.5.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 用肉眼来看，我们似乎可以根据收件人数量将邮件分组。这将导致形成两个簇：一个包含两个或更少收件人的邮件，另一个包含五个或更多收件人的邮件。我们也可以尝试根据大小将它们分成三个组。但你可以想象，随着表格越来越大，用眼睛识别组变得越来越困难。如果我们绘制数据呢？让我们在图表中绘制邮件，其中水平轴记录大小，垂直轴记录收件人数量。这给我们带来了图2.5中的图表。
- en: '![](../Images/2-5.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-5.png)'
- en: Figure 2.5  A plot of the email dataset. The horizontal axis corresponds to
    the size of the email and the vertical axis to the number of recipients. We can
    see three well-defined clusters in this dataset.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5  邮件数据集的图表。水平轴对应邮件的大小，垂直轴对应收件人数量。我们可以在这个数据集中看到三个定义良好的簇。
- en: In figure 2.5 we can see three well-defined clusters, which are highlighted
    in figure 2.6.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.5中，我们可以看到三个定义良好的簇，这些簇在图2.6中被突出显示。
- en: '![](../Images/2-6.png)'
  id: totrans-112
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-6.png)'
- en: Figure 2.6 We can cluster the emails into three categories based on size and
    number of recipients.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6我们可以根据大小和收件人数量将电子邮件聚类成三类。
- en: This last step is what clustering is all about. Of course, for us humans, it’s
    easy to eyeball the three groups once we have the plot. But for a computer, this
    task is not easy. Furthermore, imagine if our data contained millions of points,
    with hundreds or thousands of features. With more than three features, it is impossible
    for humans to see the clusters, because they would be in dimensions that we cannot
    visualize. Luckily, computers can do this type of clustering for huge datasets
    with multiple rows and columns.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 最后这一步就是聚类的全部内容。当然，对于我们人类来说，一旦有了图表，很容易就能目测出三个组。但对于计算机来说，这项任务并不容易。此外，想象一下，如果我们的数据包含数百万个点，有数百或数千个特征。当特征超过三个时，人类无法看到簇，因为它们会处于我们无法可视化的维度。幸运的是，计算机可以为具有多行和多列的巨大数据集进行此类聚类。
- en: 'Other applications of clustering are the following:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类的一些其他应用如下：
- en: '**Market segmentation**: dividing customers into groups based on demographics
    and previous purchasing behavior to create different marketing strategies for
    the groups'
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**市场细分**：根据人口统计和以往购买行为将客户分成组，为这些组创建不同的营销策略'
- en: '**Genetics**: clustering species into groups based on gene similarity'
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**遗传学**：根据基因相似性将物种聚类成组'
- en: '**Medical imaging**: splitting an image into different parts to study different
    types of tissue'
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**医学成像**：将图像分割成不同的部分以研究不同类型的组织'
- en: '**Video recommendations**: dividing users into groups based on demographics
    and previous videos watched and using this to recommend to a user the videos that
    other users in their group have watched'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**视频推荐**：根据人口统计和以往观看的视频将用户分成组，并据此向用户推荐其组中其他用户观看的视频'
- en: More on unsupervised learning models
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 更多关于无监督学习模型
- en: In the rest of this book, we don’t cover unsupervised learning. However, I strongly
    encourage you to study it on your own. Here are some of the most important clustering
    algorithms out there. Appendix C lists several more (including some videos of
    mine) where you can learn these algorithms in detail.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的其余部分，我们不涉及无监督学习。然而，我强烈建议您自学。以下是一些最重要的聚类算法。附录C列出了更多（包括一些我的视频）的算法，您可以在其中详细了解这些算法。
- en: '**K****-means clustering**: this algorithm groups points by picking some random
    centers of mass and moving them closer and closer to the points until they are
    at the right spots.'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**K-均值聚类**：此算法通过选择一些随机质心并将它们移动到越来越接近点的位置来分组点，直到它们处于正确的位置。'
- en: '**Hierarchical clustering**: this algorithm starts by grouping the closest
    points together and continuing in this fashion, until we have some well-defined
    groups.'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**层次聚类**：此算法首先将最近的点分组在一起，并继续这种方式，直到我们有一些定义良好的组。'
- en: '**Density-based spatial clustering (DBSCAN****)**: this algorithm starts grouping
    points together in places with high density, while labeling the isolated points
    as noise.'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**基于密度的空间聚类（DBSCAN**）：此算法从高密度区域开始将点分组在一起，同时将孤立点标记为噪声。'
- en: '**Gaussian mixture models**: this algorithm does not assign a point to one
    cluster but instead assigns fractions of the point to each of the existing clusters.
    For example, if there are three clusters, A, B, and C, then the algorithm could
    determine that 60% of a particular point belongs to group A, 25% to group B, and
    15% to group C.'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**高斯混合模型**：此算法不会将一个点分配给一个簇，而是将点的分数分配给现有的每个簇。例如，如果有三个簇，A、B和C，那么算法可以确定一个特定点的60%属于组A，25%属于组B，15%属于组C。'
- en: Dimensionality reduction simplifies data without losing too much information
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低简化数据而不丢失太多信息
- en: 'Dimensionality reduction is a useful preprocessing step that we can apply to
    vastly simplify our data before applying other techniques. As an example, let’s
    go back to the housing dataset. Imagine that the features are the following:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 维度降低是一个有用的预处理步骤，我们可以将其应用于大量简化我们的数据，然后再应用其他技术。例如，让我们回到住房数据集。想象一下，特征如下：
- en: Size
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尺寸
- en: Number of bedrooms
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卧室数量
- en: Number of bathrooms
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卫生间数量
- en: Crime rate in the neighborhood
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社区犯罪率
- en: Distance to the closest school
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到最近学校的距离
- en: This dataset has five columns of data. What if we wanted to turn the dataset
    into a simpler one with fewer columns, without losing a lot of information? Let’s
    do this by using common sense. Take a closer look at the five features. Can you
    see any way to simplify them—perhaps to group them into some smaller and more
    general categories?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有五个数据列。如果我们想将数据集转换成一个列更少、信息损失不多的简单数据集，会怎么样呢？让我们通过常识来做这件事。仔细看看这五个特征。你能看到任何简化它们的方法——也许是将它们分组到一些更小、更一般的类别中？
- en: After a careful look, we can see that the first three features are similar,
    because they are all related to the size of the house. Similarly, the fourth and
    fifth features are similar to each other, because they are related to the quality
    of the neighborhood. We could condense the first three features into a big “size”
    feature, and the fourth and fifth into a big “neighborhood quality” feature. How
    do we condense the size features? We could forget about rooms and bedrooms and
    consider only the size, we could add the number of bedrooms and bathrooms, or
    maybe take some other combination of the three features. We could also condense
    the area quality features in similar ways. Dimensionality reduction algorithms
    will find good ways to condense these features, losing as little information as
    possible and keeping our data as intact as possible while managing to simplify
    it for easier process and storage (figure 2.7).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察后，我们可以发现前三个特征是相似的，因为它们都与房屋的大小有关。同样，第四和第五个特征彼此相似，因为它们与社区的素质有关。我们可以将前三个特征浓缩成一个大的“大小”特征，第四和第五个特征浓缩成一个大的“社区素质”特征。我们如何浓缩大小特征呢？我们可以忽略房间和卧室的数量，只考虑大小，我们可以加上卧室和浴室的数量，或者可能取这三个特征的某种组合。我们也可以以类似的方式浓缩区域素质特征。降维算法将找到好的方法来浓缩这些特征，尽可能减少信息损失，同时尽可能保持我们的数据完整，以便在简化数据的同时便于处理和存储（图2.7）。
- en: '![](../Images/2-7.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![图2.7](../Images/2-7.png)'
- en: Figure 2.7 Dimensionality reduction algorithms help us simplify our data. On
    the left, we have a housing dataset with many features. We can use dimensionality
    reduction to reduce the number of features in the dataset without losing much
    information and obtain the dataset on the right.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 降维算法帮助我们简化数据。在左侧，我们有一个具有许多特征的房屋数据集。我们可以使用降维来减少数据集中的特征数量，同时损失的信息很少，并得到右侧的数据集。
- en: 'Why is it called dimensionality reduction if all we’re doing is reducing the
    number of columns in our data? The fancy word for the number of columns in a dataset
    is *dimension*. Think about this: if our data has one column, then each data point
    is one number. A collection of numbers can be plotted as a collection of points
    in a line, which has precisely one dimension. If our data has two columns, then
    each data point is formed by a pair of numbers. We can imagine a collection of
    pairs of numbers as a collection of points in a city, where the first number is
    the street number and the second number is the avenue. Addresses on a map are
    two-dimensional, because they are in a plane. What happens when our data has three
    columns? In this case, then each data point is formed by three numbers. We can
    imagine that if every address in our city is a building, then the first and second
    numbers are the street and avenue, and the third one is the floor in the building.
    This looks more like a three-dimensional city. We can keep going. What about four
    numbers? Well, now we can’t really visualize it, but if we could, this set of
    points would look like places in a four-dimensional city, and so on. The best
    way to imagine a four-dimensional city is by imagining a table with four columns.
    What about a 100-dimensional city? This would be a table with 100 columns, in
    which each person has an address that consists of 100 numbers. The mental picture
    we could have when thinking of higher dimensions is shown in figure 2.8\. Therefore,
    as we went from five dimensions down to two, we reduced our five-dimensional city
    into a two-dimensional city. This is why it is called dimensionality reduction.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只是在减少数据中的列数，为什么称之为降维？数据集中列数的术语是*维度*。想想看：如果我们的数据有一列，那么每个数据点就是一个数字。一组数字可以绘制成一条线上的点集，这条线恰好是一维的。如果我们的数据有两列，那么每个数据点由一对数字组成。我们可以想象一对数字的集合就像一个城市中的点集，第一个数字是街号，第二个数字是大道。地图上的地址是二维的，因为它们在平面上。当我们的数据有三列时会发生什么？在这种情况下，每个数据点由三个数字组成。我们可以想象，如果我们的城市中的每个地址都是一个建筑，那么前两个数字是街道和大道，第三个数字是建筑中的楼层。这看起来更像是一个三维城市。我们可以继续这样想。四个数字呢？好吧，现在我们真的无法可视化它，但如果我们可以，这个点集将看起来像四维城市中的地方，以此类推。想象四维城市最好的方式是通过想象一个有四列的表格。那么一百维城市呢？这将是一个有100列的表格，其中每个人都有一个由100个数字组成的地址。当我们思考高维时，我们可以在图2.8中看到这种心理图像。因此，当我们从五维降到二维时，我们将五维城市简化为二维城市。这就是为什么称之为降维。
- en: '![](../Images/2-8.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-8.png)'
- en: 'Figure 2.8 How to imagine higher dimensional spaces: One dimension is like
    a street, in which each house only has one number. Two dimensions is like a flat
    city, in which each address has two numbers, a street and an avenue. Three dimensions
    is like a city with buildings, in which each address has three numbers: a street,
    an avenue, and a floor. Four dimensions is like an imaginary place in which each
    address has four numbers. We can imagine higher dimensions as another imaginary
    city in which addresses have as many coordinates as we need.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 如何想象高维空间：一维就像一条街道，其中每栋房子只有一个号码。二维就像一个平面城市，其中每个地址有两个号码，一个街道和一个大道。三维就像一个有建筑的城市，其中每个地址有三个号码：一个街道，一个大道和一个楼层。四维就像一个有四个号码的想象中的地方。我们可以将高维想象为另一个想象中的城市，其中地址有我们需要的那么多坐标。
- en: 'Other ways of simplifying our data: Matrix factorization and singular value
    decomposition'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 简化我们数据的其他方法：矩阵分解和奇异值分解
- en: It seems that clustering and dimensionality reduction are nothing like each
    other, but, in reality, they are not so different. If we have a table full of
    data, each row corresponds to a data point, and each column corresponds to a feature.
    Therefore, we can use clustering to reduce the number of rows in our dataset and
    dimensionality reduction to reduce the number of columns, as figures 2.9 and 2.10
    illustrate.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来聚类和降维完全不同，但实际上它们并不那么不同。如果我们有一个充满数据的数据表，每一行对应一个数据点，每一列对应一个特征。因此，我们可以使用聚类来减少数据集中的行数，使用降维来减少列数，如图2.9和2.10所示。
- en: '![](../Images/2-9.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-9.png)'
- en: Figure 2.9 Clustering can be used to simplify our data by reducing the number
    of rows in our dataset by grouping several rows into one.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 通过将多个行分组为一个，聚类可以用来简化我们的数据，减少数据集中行的数量。
- en: '![](../Images/2-10.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-10.png)'
- en: Figure 2.10 Dimensionality reduction can be used to simplify our data by reducing
    the number of columns in our dataset.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 可以通过减少数据集中列的数量来简化我们的数据，从而进行降维。
- en: You may be wondering, is there a way that we can reduce both the rows and the
    columns at the same time? And the answer is yes! Two common ways we can do this
    are *matrix factorization* and *singular value decomposition*. These two algorithms
    express a big matrix of data into a product of smaller matrices.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道，我们是否可以同时减少行和列？答案是肯定的！我们可以通过两种常见的方法来实现这一点：*矩阵分解*和*奇异值分解*。这两个算法将大数据矩阵表示为较小矩阵的乘积。
- en: Places like Netflix use matrix factorization extensively to generate recommendations.
    Imagine a large table where each row corresponds to a user, each column to a movie,
    and each entry in the matrix is the rating that the user gave the movie. With
    matrix factorization, one can extract certain features, such as type of movie,
    actors appearing in the movie, and others, and be able to predict the rating that
    a user gives a movie, based on these features.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 类似Netflix这样的地方广泛使用矩阵分解来生成推荐。想象一个大的表格，其中每一行对应一个用户，每一列对应一部电影，矩阵中的每个条目都是用户对电影的评分。通过矩阵分解，可以提取某些特征，例如电影类型、电影中的演员等等，并能够根据这些特征预测用户对电影的评分。
- en: Singular value decomposition is used in image compression. For example, a black-and-white
    image can be seen as a large table of data, where each entry contains the intensity
    of the corresponding pixel. Singular value decomposition uses linear algebra techniques
    to simplify this table of data, thus allowing us to simplify the image and store
    its simpler version using fewer entries.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 奇异值分解用于图像压缩。例如，黑白图像可以看作是一个大数据表，其中每个条目包含对应像素的强度。奇异值分解使用线性代数技术简化这个数据表，从而允许我们简化图像，并使用更少的条目存储其简化版本。
- en: Generative machine learning
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 生成式机器学习
- en: '*Generative* *machine learning* is one of the most astonishing fields of machine
    learning. If you have seen ultra-realistic faces, images, or videos created by
    computers, then you have seen generative machine learning in action.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '*生成式* *机器学习*是机器学习中最令人惊讶的领域之一。如果你看到过计算机生成的超逼真的人脸、图像或视频，那么你就看到了生成式机器学习的实际应用。'
- en: The field of generative learning consists of models that, given a dataset, can
    output new data points that look like samples from that original dataset. These
    algorithms are forced to learn how the data looks to produce similar data points.
    For example, if the dataset contains images of faces, then the algorithm will
    produce realistic-looking faces. Generative algorithms have been able to create
    tremendously realistic images, paintings, and so on. They have also generated
    video, music, stories, poetry, and many other wonderful things. The most popular
    generative algorithm is generative adversarial networks (GANs), developed by Ian
    Goodfellow and his coauthors. Other useful and popular generative algorithms are
    variational autoencoders, developed by Kingma and Welling, and restricted Boltzmann
    machines (RBMs), developed by Geoffrey Hinton.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 生成学习领域包括一些模型，这些模型在给定一个数据集的情况下，可以输出看起来像原始数据集样本的新数据点。这些算法被迫学习数据的外观以产生相似的数据点。例如，如果数据集包含人脸图像，那么算法将产生看起来逼真的面孔。生成算法已经能够创造出极其逼真的图像、绘画等等。它们还生成了视频、音乐、故事、诗歌以及许多其他奇妙的事物。最受欢迎的生成算法是生成对抗网络（GANs），由伊恩·古德费洛及其合作者开发。其他有用且流行的生成算法包括由Kingma和Welling开发的变分自编码器，以及由杰弗里·辛顿开发的受限玻尔兹曼机（RBMs）。
- en: As you can imagine, generative learning is quite hard. For a human, it is much
    easier to determine if an image shows a dog than it is to draw a dog. This task
    is just as hard for computers. Thus, the algorithms in generative learning are
    complicated, and lots of data and computing power are needed to make them work
    well. Because this book is on supervised learning, we won’t cover generative learning
    in detail, but in chapter 10, we get an idea of how some of these generative algorithms
    work, because they tend to use neural networks. Appendix C contains recommendations
    of resources, including a video by the author, if you’d like to explore this topic
    further.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，生成学习相当困难。对于人类来说，确定一张图片是否显示了一只狗比画一只狗要容易得多。这项任务对计算机来说同样困难。因此，生成学习中的算法很复杂，需要大量的数据和计算能力才能使它们工作得很好。因为这本书是关于监督学习的，所以我们不会详细讨论生成学习，但在第10章中，我们将了解一些这些生成算法是如何工作的，因为它们倾向于使用神经网络。附录C包含了一些资源推荐，包括作者的一个视频，如果你想进一步探索这个话题的话。
- en: What is reinforcement learning?
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是强化学习？
- en: Reinforcement learning is a different type of machine learning in which no data
    is given, and we must get the computer to perform a task. Instead of data, the
    model receives an environment and an agent who is supposed to navigate in this
    environment. The agent has a goal or a set of goals. The environment has rewards
    and punishments that guide the agent to make the right decisions to reach its
    goal. This all sounds a bit abstract, but let’s look at an example.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习是一种不同类型的机器学习，其中没有提供数据，我们必须让计算机执行一个任务。不是数据，模型接收一个环境和应该在这个环境中导航的智能体。智能体有一个目标或一系列目标。环境有奖励和惩罚，引导智能体做出正确的决策以达到其目标。这一切听起来可能有点抽象，但让我们看看一个例子。
- en: 'Example: Grid world'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：网格世界
- en: In figure 2.11, we see a grid world with a robot at the bottom-left corner.
    That is our agent. The goal is to get to the treasure chest in the top right of
    the grid. In the grid, we can also see a mountain, which means we cannot go through
    that square, because the robot cannot climb mountains. We also see a dragon, which
    will attack the robot, should the robot dare to land in its square, which means
    that part of our goal is to not land over there. This is the game. And to give
    the robot information about how to proceed, we keep track of a score. The score
    starts at zero. If the robot gets to the treasure chest, then we gain 100 points.
    If the robot reaches the dragon, we lose 50 points. And to make sure our robot
    moves quickly, we can say that for every step the robot makes, we lose 1 point,
    because the robot loses energy as it walks.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.11中，我们看到一个网格世界，其中有一个机器人在左下角。那就是我们的智能体。目标是到达网格右上角的宝箱。在网格中，我们还可以看到一个山丘，这意味着我们不能通过那个方块，因为机器人不能爬山。我们还看到一个龙，如果机器人敢降落在它的方块上，龙将会攻击机器人，这意味着我们的目标之一是不降落在那里。这就是游戏。为了给机器人提供如何进行的信息，我们记录一个分数。分数从零开始。如果机器人到达宝箱，我们就获得100分。如果机器人遇到龙，我们就会失去50分。为了确保我们的机器人移动得快，我们可以这样说，对于机器人每走一步，我们就失去1分，因为机器人走路时会消耗能量。
- en: '![](../Images/2-11.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/2-11.png)'
- en: Figure 2.11 A grid world in which our agent is a robot. The goal of the robot
    is to find the treasure chest, while avoiding the dragon. The mountain represents
    a place through which the robot can’t pass.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11展示了我们的智能体是一个机器人的网格世界。机器人的目标是找到宝箱，同时避开龙。山丘代表机器人无法通过的地方。
- en: 'The way to train this algorithm, in very rough terms, follows: The robot starts
    walking around, recording its score and remembering what steps took it there.
    After some point, it may meet the dragon, losing many points. Therefore, it learns
    to associate the dragon square and the squares close to it with low scores. At
    some point it may also hit the treasure chest, and it learns to start associating
    that square and the squares close to it to high scores. After playing this game
    for a long time, the robot will have a good idea of how good each square is, and
    it can take the path following the squares all the way to the treasure chest.
    Figure 2.12 shows'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这个算法的方法，非常粗略地说，是这样的：机器人开始四处走动，记录它的分数并记住是什么步骤带它到那里。在某个时候，它可能会遇到龙，失去很多分数。因此，它学会了将龙方块及其附近的方块与低分数联系起来。在某个时候，它也可能击中宝箱，并学会将那个方块及其附近的方块与高分数联系起来。经过长时间玩这个游戏，机器人将很好地了解每个方块的好坏，并且它可以沿着跟随方块直到宝箱的路径前进。图2.12展示了
- en: a possible path, although this one is not ideal, because it passes too close
    to the dragon. Can you think of a better one?
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: a possible path, although this one is not ideal, because it passes too close
    to the dragon. Can you think of a better one?
- en: '![](../Images/2-12.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/2-12.png)'
- en: Figure 2.12 Here is a path that the robot could take to find the treasure chest.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 这里是机器人可能采取的寻找宝箱的路径。
- en: Of course, this is a very brief explanation, and there is a lot more to reinforcement
    learning. Appendix C recommends some resources for further study, including a
    deep reinforcement learning video.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这是一个非常简短的说明，强化学习还有很多其他内容。附录C推荐了一些进一步学习的资源，包括一个深度强化学习的视频。
- en: 'Reinforcement learning has numerous cutting-edge applications, including the
    following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习有许多尖端应用，包括以下内容：
- en: '**Games**: recent advances in teaching computers how to win at games, such
    as Go or chess, use reinforcement learning. Also, agents have been taught to win
    at Atari games such as *Breakout* or *Super Mario*.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**游戏**：最近在教计算机如何赢得游戏（如围棋或象棋）的进步，使用了强化学习。此外，已经教会代理赢得像*Breakout*或*超级马里奥*这样的Atari游戏。'
- en: '**Robotics**: reinforcement learning is used extensively to help robots carry
    out tasks such as picking up boxes, cleaning a room, or even dancing!'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**机器人技术**：强化学习被广泛用于帮助机器人执行拾取箱子、打扫房间或甚至跳舞等任务！'
- en: '**Self-driving cars**: reinforcement learning techniques are used to help the
    car carry out many tasks such as path planning or behaving in particular environments.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**自动驾驶汽车**：强化学习技术被用来帮助汽车执行许多任务，例如路径规划或在特定环境中行为。'
- en: Summary
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Several types of machine learning exist, including supervised learning, unsupervised
    learning, and reinforcement learning.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在几种机器学习类型，包括监督学习、无监督学习和强化学习。
- en: Data can be labeled or unlabeled. Labeled data contains a special feature, or
    label, that we aim to predict. Unlabeled data doesn’t contain this feature.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据可以是标记的或未标记的。标记数据包含一个特殊特征或标签，我们旨在预测。未标记数据不包含此特征。
- en: Supervised learning is used on labeled data and consists of building models
    that predict the labels for unseen data.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督学习用于标记数据，并包括构建模型来预测未见数据的标签。
- en: Unsupervised learning is used on unlabeled data and consists of algorithms that
    simplify our data without losing a lot of information. Unsupervised learning is
    often used as a preprocessing step.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无监督学习用于未标记的数据，并包括简化我们的数据而不丢失大量信息的算法。无监督学习通常用作预处理步骤。
- en: Two common types of supervised learning algorithms are called regression and
    classification.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种常见的监督学习算法被称为回归和分类。
- en: Regression models are those in which the answer is any number.
  id: totrans-174
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归模型是那些答案可以是任何数字的模型。
- en: Classification models are those in which the answer is of a type or a class.
  id: totrans-175
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型是那些答案属于某种类型或类别的模型。
- en: Two common types of unsupervised learning algorithms are clustering and dimensionality
    reduction.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 两种常见的无监督学习算法是聚类和维度约简。
- en: Clustering is used to group data into similar clusters to extract information
    or make it easier to handle.
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类用于将数据分组到相似的簇中，以提取信息或使其更容易处理。
- en: Dimensionality reduction is a way to simplify our data, by joining certain similar
    features and losing as little information as possible.
  id: totrans-178
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度约简是一种通过合并某些相似特征并尽可能少地丢失信息来简化我们的数据的方法。
- en: Matrix factorization and singular value decomposition are other algorithms that
    can simplify our data by reducing both the number of rows and columns.
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩阵分解和奇异值分解是其他可以通过减少行和列的数量来简化我们的数据的算法。
- en: Generative machine learning is an innovative type of unsupervised learning,
    consisting of generating data that is similar to our dataset. Generative models
    can paint realistic faces, compose music, and write poetry.
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成式机器学习是一种创新的无监督学习类型，它由生成与我们的数据集相似的数据组成。生成模型可以绘制逼真的面孔，创作音乐，并写诗。
- en: Reinforcement learning is a type of machine learning in which an agent must
    navigate an environment and reach a goal. It is extensively used in many cutting-edge
    applications.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是一种机器学习类型，其中代理必须在一个环境中导航并达到目标。它在许多尖端应用中得到了广泛的应用。
- en: Exercises
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 2.1
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.1
- en: For each of the following scenarios, state if it is an example of supervised
    or unsupervised learning. Explain your answers. In cases of ambiguity, pick one,
    and explain why you picked it.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下每个场景，说明它是一个监督学习还是无监督学习的例子。解释你的答案。在模糊的情况下，选择一个，并解释你为什么选择它。
- en: A recommendation system on a social network that recommends potential friends
    to a user
  id: totrans-185
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在社交网络上的推荐系统，向用户推荐潜在的朋友
- en: A system in a news site that divides the news into topics
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在新闻网站上，将新闻分为主题的系统
- en: The Google autocomplete feature for sentences
  id: totrans-187
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Google自动补全功能的句子
- en: A recommendation system on an online retailer that recommends to users what
    to buy based on their past purchasing history
  id: totrans-188
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在在线零售商上的推荐系统，根据用户的过去购买历史向用户推荐购买商品
- en: A system in a credit card company that captures fraudulent transactions
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一家信用卡公司中用于捕捉欺诈交易的系统
- en: Exercise 2.2
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.2
- en: For each of the following applications of machine learning, would you use regression
    or classification to solve it? Explain your answers. In cases of ambiguity, pick
    one, and explain why you picked it.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 对于以下机器学习的应用，你会使用回归还是分类来解决它？解释你的答案。在存在歧义的情况下，选择一个，并解释你为什么选择它。
- en: An online store predicting how much money a user will spend on their site
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线商店预测用户在其网站上将花费多少钱
- en: A voice assistant decoding voice and turning it into text
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 语音助手解码语音并将其转换为文本
- en: Selling or buying stock from a particular company
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从特定公司买卖股票
- en: YouTube recommending a video to a user
  id: totrans-195
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: YouTube向用户推荐视频
- en: Exercise 2.3
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.3
- en: Your task is to build a self-driving car. Give at least three examples of machine
    learning problems that you would have to solve to build it. In each example, explain
    if you are using supervised/unsupervised learning, and, if supervised, whether
    you are using regression or classification. If you are using other types of machine
    learning, explain which ones, and why.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 你的任务是构建一辆自动驾驶汽车。至少给出三个你必须解决的机器学习问题来构建它。在每个例子中，解释你是否使用监督学习/无监督学习，如果是监督学习，是否使用回归或分类。如果你使用其他类型的机器学习，解释是哪一种，以及为什么。
