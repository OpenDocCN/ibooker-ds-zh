- en: Part 2\. Core concepts
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2部分\. 核心概念
- en: Through these next several chapters, we’ll dig into standard Redis commands,
    how they manipulate data, and how to configure Redis. In the latter chapters,
    we’ll build ever-growing pieces of support tools and applications, until we finally
    build a simple social network completely within Redis.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几章中，我们将深入研究标准的Redis命令，它们如何操作数据，以及如何配置Redis。在后几章中，我们将构建不断增长的支持工具和应用程序，直到我们最终在Redis中构建一个简单的社交网络。
- en: Chapter 3\. Commands in Redis
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章\. Redis中的命令
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: String, list, and set commands
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字符串、列表和集合命令
- en: Hash and sorted set commands
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哈希和有序集合命令
- en: Publish/subscribe commands
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布/订阅命令
- en: Other commands
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他命令
- en: In this chapter, we’ll primarily cover commands that we haven’t already covered
    in [chapters 1](kindle_split_011.html#ch01) and [2](kindle_split_012.html#ch02).
    By learning about Redis through its commands, you’ll be able to build on the examples
    provided and have a better understanding of how to solve your own problems. If
    you’re looking for short examples that are more than the simple interactions I
    show here, you’ll find some in [chapter 2](kindle_split_012.html#ch02).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将主要介绍在[第1章](kindle_split_011.html#ch01)和[第2章](kindle_split_012.html#ch02)中尚未涉及到的命令。通过学习Redis的命令，你将能够基于提供的示例进行构建，并更好地理解如何解决你自己的问题。如果你在寻找比我在这里展示的简单交互更短的示例，你可以在[第2章](kindle_split_012.html#ch02)中找到一些。
- en: The commands that are highlighted in this chapter are broken down by structure
    or concept, and were chosen because they include 95% or more of the typical Redis
    calls in a variety of applications. The examples are interactions in the console,
    similar to the way I introduced each of the structures in [chapter 1](kindle_split_011.html#ch01).
    Where appropriate, I’ll reference earlier or later sections that use those commands.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中突出显示的命令是根据结构或概念划分的，并且被选中是因为它们包含了各种应用中95%或更多的典型Redis调用。示例是控制台中的交互，类似于我在[第1章](kindle_split_011.html#ch01)中介绍每个结构的方式。在适当的地方，我会引用使用这些命令的早期或后期部分。
- en: In the section for each of the different data types, I’ll show commands that
    are unique to the different structures, primarily focusing on what makes those
    structures and commands distinct. Let’s start by seeing how Redis `STRING`s offer
    more than just `GET` and `SET` operations.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个不同数据类型的部分，我会展示那些独特于不同结构的命令，主要关注那些使这些结构和命令与众不同的特点。让我们先看看Redis的`STRING`类型除了`GET`和`SET`操作之外还能提供什么。
- en: '|  |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Additional documentation for commands not covered
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 未涵盖命令的附加文档
- en: In this chapter, I only cover the most commonly used commands or those commands
    that we’ll use in later chapters. If you’re looking for a full command and documentation
    reference, you can visit [http://redis.io/commands](http://redis.io/commands).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我只介绍最常用的命令或我们将在后续章节中使用的命令。如果你在寻找完整的命令和文档参考，你可以访问[http://redis.io/commands](http://redis.io/commands)。
- en: '|  |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Redis 2.4 and 2.6
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Redis 2.4和2.6
- en: As mentioned in [appendix A](kindle_split_024.html#app01), as of the time of
    this writing, precompiled versions of Redis for Windows are from the 2.4 series.
    In this and other chapters, we use features that are only available in Redis 2.6
    and later. The primary differences between Redis 2.4 and 2.6 include (but aren’t
    limited to) Lua scripting (which we’ll discuss in [chapter 11](kindle_split_023.html#ch11)),
    millisecond-level precision for expiration (`PTTL`, `PEXPIRE`, and `PEXPIREAT`,
    described in this chapter), some bit operations (`BITOP` and `BITCOUNT`), and
    some commands now taking multiple arguments where they previously only took one
    argument (`RPUSH`, `LPUSH`, `SADD`, `SREM`, `HDEL`, `ZADD`, and `ZREM`).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如[附录A](kindle_split_024.html#app01)中所述，截至本书写作之时，Windows上的预编译Redis版本来自2.4系列。在本章和其他章节中，我们使用仅在Redis
    2.6及以后版本中可用的功能。Redis 2.4和2.6之间的主要区别包括（但不限于）Lua脚本（我们将在[第11章](kindle_split_023.html#ch11)中讨论）、毫秒级精度的过期时间（`PTTL`、`PEXPIRE`和`PEXPIREAT`，在本章中描述）、一些位操作（`BITOP`和`BITCOUNT`），以及一些现在接受多个参数而之前只接受一个参数的命令（`RPUSH`、`LPUSH`、`SADD`、`SREM`、`HDEL`、`ZADD`和`ZREM`）。
- en: '|  |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.1\. Strings
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1\. 字符串
- en: 'You’ll remember from [chapters 1](kindle_split_011.html#ch01) and [2](kindle_split_012.html#ch02)
    that `STRING`s hold sequences of bytes, not significantly different from strings
    in many programming languages, or even C/C++–style char arrays. In Redis, `STRING`s
    are used to store three types of values:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 你会记得从[第1章](kindle_split_011.html#ch01)和[第2章](kindle_split_012.html#ch02)中，`STRING`存储的是一系列字节，与许多编程语言中的字符串或甚至C/C++风格的char数组没有太大区别。在Redis中，`STRING`用于存储三种类型的值：
- en: Byte string values
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字节字符串值
- en: Integer values
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整数值
- en: Floating-point values
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 浮点值
- en: Integers and floats can be incremented or decremented by an arbitrary numeric
    value (integers turning into floats as necessary). Integers have ranges that are
    equivalent to the platform’s long integer range (signed 32-bit integers on 32-bit
    platforms, and signed 64-bit integers on 64-bit platforms), and floats have ranges
    and values limited to IEEE 754 floating-point doubles. This three-way ability
    to look at the simplest of Redis values can be an advantage; it offers more flexibility
    in data representation than if only byte string values were allowed.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 整数和浮点数可以通过任意数值递增或递减（整数在必要时转换为浮点数）。整数的范围与平台的长整数范围相当（32位平台上的有符号32位整数，64位平台上的有符号64位整数），而浮点数的范围和值限于IEEE
    754浮点双精度数。这种查看Redis最简单值的三种方式可以是一个优势；它比只允许字节字符串值提供了更多的数据表示灵活性。
- en: In this section, we’ll talk about the simplest structure available to Redis,
    the `STRING`. We’ll cover the basic numeric increment and decrement operations,
    followed later by the bit and substring manipulation calls, and you’ll come to
    understand that even the simplest of structures has a few surprises that can make
    it useful in a variety of powerful ways.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论Redis中最简单的结构，即`STRING`。我们将介绍基本的数值递增和递减操作，随后将介绍位和子字符串操作调用，你将了解到即使是最简单的结构也有一些惊喜，可以使它在各种强大的方式中变得有用。
- en: In [table 3.1](#ch03table01), you can see the available integer and float increment/decrement
    operations available on Redis `STRING`s.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[表3.1](#ch03table01)中，你可以看到Redis `STRING`上可用的整数和浮点数递增/递减操作。
- en: Table 3.1\. Increment and decrement commands in Redis
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.1\. Redis中的递增和递减命令
- en: '| Command | Example use and description |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| INCR | INCR key-name—Increments the value stored at the key by 1 |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| INCR | INCR key-name—通过1递增存储在键中的值 |'
- en: '| DECR | DECR key-name—Decrements the value stored at the key by 1 |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| DECR | DECR key-name—通过1递减存储在键中的值 |'
- en: '| INCRBY | INCRBY key-name amount—Increments the value stored at the key by
    the provided integer value |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| INCRBY | INCRBY key-name amount—通过提供的整数值递增存储在键中的值 |'
- en: '| DECRBY | DECRBY key-name amount—Decrements the value stored at the key by
    the provided integer value |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| DECRBY | DECRBY key-name amount—通过提供的整数值递减存储在键中的值 |'
- en: '| INCRBYFLOAT | INCRBYFLOAT key-name amount—Increments the value stored at
    the key by the provided float value (available in Redis 2.6 and later) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| INCRBYFLOAT | INCRBYFLOAT key-name amount—通过提供的浮点值递增存储在键中的值（Redis 2.6及以后版本可用）
    |'
- en: When setting a `STRING` value in Redis, if that value could be interpreted as
    a base-10 integer or a floating-point value, Redis will detect this and allow
    you to manipulate the value using the various `INCR*` and `DECR*` operations.
    If you try to increment or decrement a key that doesn’t exist or is an empty string,
    Redis will operate as though that key’s value were zero. If you try to increment
    or decrement a key that has a value that can’t be interpreted as an integer or
    float, you’ll receive an error. In the next listing, you can see some interactions
    with these commands.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当在Redis中设置`STRING`值时，如果该值可以被解释为十进制整数或浮点数，Redis将检测到这一点，并允许你使用各种`INCR*`和`DECR*`操作来操作该值。如果你尝试递增或递减一个不存在或为空字符串的键，Redis将像该键的值是零一样操作。如果你尝试递增或递减一个值不能被解释为整数或浮点数的键，你将收到一个错误。在下一个列表中，你可以看到这些命令的一些交互。
- en: Listing 3.1\. A sample interaction showing `INCR` and `DECR` operations in Redis
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 显示Redis中`INCR`和`DECR`操作的示例交互
- en: '![](041fig01_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](041fig01_alt.jpg)'
- en: After reading other chapters, you may notice that we really only call `incr()`.
    Internally, the Python Redis libraries call `INCRBY` with either the optional
    second value passed, or 1 if the value is omitted. As of this writing, the Python
    Redis client library supports the full command set of Redis 2.6, and offers `INCRBYFLOAT`
    support via an `incrbyfloat()` method that works the same as `incr()`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了其他章节之后，你可能注意到我们实际上只调用了`incr()`。在内部，Python Redis库使用`INCRBY`调用，要么使用可选的第二个值，要么如果省略值为1。截至本文撰写时，Python
    Redis客户端库支持Redis 2.6的完整命令集，并通过`incrbyfloat()`方法提供`INCRBYFLOAT`支持，该方法与`incr()`的工作方式相同。
- en: Redis additionally offers methods for reading and writing parts of byte string
    values (integer and float values can also be accessed as though they’re byte strings,
    though that use is somewhat uncommon). This can be useful if we were to use Redis
    `STRING` values to pack structured data in an efficient fashion, which we’ll talk
    about in [chapter 9](kindle_split_021.html#ch09). [Table 3.2](#ch03table02) shows
    some methods that can be used to manipulate substrings and individual bits of
    `STRING`s in Redis.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: Redis还提供了读取和写入字节字符串值部分的方法（整数和浮点值也可以像字节字符串一样访问，尽管这种用法相对不常见）。如果我们使用Redis `STRING`值以高效的方式打包结构化数据，这可能很有用，我们将在第9章中讨论这一点。[表3.2](#ch03table02)显示了可以用于在Redis中操作`STRING`的子字符串和单个位的某些方法。
- en: Table 3.2\. Substring manipulation commands available to Redis
  id: totrans-40
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.2\. Redis可用的子字符串操作命令
- en: '| Command | Example use and description |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| APPEND | APPEND key-name value—Concatenates the provided value to the string
    already stored at the given key |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| APPEND | APPEND key-name value—将提供的值连接到给定键已存储的字符串上|'
- en: '| GETRANGE | GETRANGE key-name start end—Fetches the substring, including all
    characters from the start offset to the end offset, inclusive |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| GETRANGE | GETRANGE key-name start end—获取子字符串，包括从起始偏移量到结束偏移量（包括）的所有字符|'
- en: '| SETRANGE | SETRANGE key-name offset value—Sets the substring starting at
    the provided offset to the given value |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| SETRANGE | SETRANGE key-name offset value—将提供的偏移量开始的子字符串设置为给定的值|'
- en: '| GETBIT | GETBIT key-name offset—Treats the byte string as a bit string, and
    returns the value of the bit in the string at the provided bit offset |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| GETBIT | GETBIT key-name offset—将字节字符串视为位字符串，并返回字符串在提供的位偏移量处的位值|'
- en: '| SETBIT | SETBIT key-name offset value—Treats the byte string as a bit string,
    and sets the value of the bit in the string at the provided bit offset |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| SETBIT | SETBIT key-name offset value—将字节字符串视为位字符串，并在提供的位偏移量处设置字符串中的位值|'
- en: '| BITCOUNT | BITCOUNT key-name [start end]—Counts the number of 1 bits in the
    string, optionally starting and finishing at the provided byte offsets |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| BITCOUNT | BITCOUNT key-name [start end]—计算字符串中1位的数量，可选地从提供的字节偏移量开始和结束|'
- en: '| BITOP | BITOP operation dest-key key-name [key-name ...]—Performs one of
    the bitwise operations, AND, OR, XOR, or NOT, on the strings provided, storing
    the result in the destination key |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| BITOP | BITOP操作dest-key key-name [key-name ...]—对提供的字符串执行位运算之一，AND、OR、XOR或NOT，并将结果存储在目标键中|'
- en: '|  |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '`GETRANGE` and `SUBSTR`'
  id: totrans-51
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`GETRANGE`和`SUBSTR`'
- en: In the past, `GETRANGE` was named `SUBSTR`, and the Python client continues
    to use the `substr()` method name to fetch ranges from the string. When using
    a version of Redis later than 2.6, you should use the `getrange()` method, and
    use `substr()` for Redis versions before 2.6.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，`GETRANGE`被命名为`SUBSTR`，Python客户端继续使用`substr()`方法名从字符串中获取范围。当使用2.6版本之后的Redis版本时，应使用`getrange()`方法，对于2.6版本之前的Redis版本，使用`substr()`。
- en: '|  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: When writing to strings using `SETRANGE` and `SETBIT`, if the `STRING` wasn’t
    previously long enough, Redis will automatically extend the `STRING` with nulls
    before updating and writing the new data. When reading `STRING`s with `GETRANGE`,
    any request for data beyond the end of the `STRING` won’t be returned, but when
    reading bits with `GETBIT`, any bit beyond the end of the `STRING` is considered
    zero. In the following listing, you can see some uses of these `STRING` manipulation
    commands.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`SETRANGE`和`SETBIT`写入字符串时，如果`STRING`之前不够长，Redis将在更新和写入新数据之前自动用空字符扩展`STRING`。当使用`GETRANGE`读取`STRING`时，任何超出`STRING`末尾的数据请求都不会返回，但当使用`GETBIT`读取位时，任何超出`STRING`末尾的位都被视为零。在下面的列表中，你可以看到这些`STRING`操作命令的一些用法。
- en: Listing 3.2\. A sample interaction showing substring and bit operations in Redis
  id: totrans-55
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 一个示例交互，展示了Redis中的子字符串和位操作
- en: '![](042fig01_alt.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![](042fig01_alt.jpg)'
- en: In many other key-value databases, data is stored as a plain string with no
    opportunities for manipulation. Some other key-value databases do allow you to
    prepend or append bytes, but Redis is unique in its ability to read and write
    substrings. In many ways, even if Redis only offered `STRING`s and these methods
    to manipulate strings, Redis would be more powerful than many other systems; enterprising
    users could use the substring and bit manipulation calls along with `WATCH`/`MULTI`/`EXEC`
    (which we’ll briefly introduce in [section 3.7.2](#ch03lev2sec2), and talk about
    extensively in [chapter 4](kindle_split_015.html#ch04)) to build arbitrary data
    structures. In [chapter 9](kindle_split_021.html#ch09), we’ll talk about using
    `STRING`s to store a type of simple mappings that can greatly reduce memory use
    in some situations.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多其他键值数据库中，数据以纯字符串的形式存储，没有操作的机会。一些其他键值数据库允许您预拼接或后拼接字节，但 Redis 在其读取和写入子字符串的能力方面是独一无二的。在许多方面，即使
    Redis 只提供 `STRING` 和这些字符串操作方法，Redis 也会比许多其他系统更强大；有创业精神的用户可以使用子字符串和位操作调用以及 `WATCH`/`MULTI`/`EXEC`（我们将在[第3.7.2节](#ch03lev2sec2)中简要介绍，并在[第4章](kindle_split_015.html#ch04)中详细讨论）来构建任意数据结构。在第9章[kindle_split_021.html#ch09]，我们将讨论使用
    `STRING` 存储一种可以大大减少某些情况下内存使用的简单映射类型。
- en: With a little work, we can store some types of sequences, but we’re limited
    in the kinds of manipulations we can perform. But if we use `LIST`s, we have a
    wider range of commands and ways to manipulate `LIST` items.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 经过一些努力，我们可以存储某些类型的序列，但我们能进行的操作种类有限。但如果我们使用 `LIST`，我们就有更广泛的命令和操作 `LIST` 项的方式。
- en: 3.2\. Lists
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 列表
- en: As you may remember from [chapter 1](kindle_split_011.html#ch01), `LIST`s allow
    you to push and pop items from both ends of a sequence, fetch individual items,
    and perform a variety of other operations that are expected of lists. `LIST`s
    by themselves can be great for keeping a queue of work items, recently viewed
    articles, or favorite contacts.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从[第1章](kindle_split_011.html#ch01)中可能记得，`LIST` 允许您从序列的两端推送和弹出项，获取单个项，并执行列表预期的一系列其他操作。`LIST`
    本身非常适合用于保持工作项队列、最近查看的文章或最喜欢的联系人。
- en: In this section, we’ll talk about `LIST`s, which store an ordered sequence of
    `STRING` values. We’ll cover some of the most commonly used `LIST` manipulation
    commands for pushing and popping items from `LIST`s. After reading this section,
    you’ll know how to manipulate `LIST`s using the most common commands. We’ll start
    by looking at [table 3.3](#ch03table03), where you can see some of the most frequently
    used `LIST` commands.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论 `LIST`，它存储了一个有序的 `STRING` 值序列。我们将介绍一些最常用的 `LIST` 操作命令，用于从 `LIST`
    中推送和弹出项。阅读本节后，您将了解如何使用最常见的命令来操作 `LIST`。我们将首先查看[表3.3](#ch03table03)，在那里您可以查看一些最常用的
    `LIST` 命令。
- en: Table 3.3\. Some commonly used `LIST` commands
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.3\. 一些常用的 `LIST` 命令
- en: '| Command | Example use and description |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| RPUSH | RPUSH key-name value [value ...]—Pushes the value(s) onto the right
    end of the list |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| RPUSH | RPUSH key-name value [value ...]—将值（们）推送到列表的右侧 |'
- en: '| LPUSH | LPUSH key-name value [value ...]—Pushes the value(s) onto the left
    end of the list |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| LPUSH | LPUSH key-name value [value ...]—将值（们）推送到列表的左侧 |'
- en: '| RPOP | RPOP key-name—Removes and returns the rightmost item from the list
    |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| RPOP | RPOP key-name—从列表中移除并返回最右侧的项 |'
- en: '| LPOP | LPOP key-name—Removes and returns the leftmost item from the list
    |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| LPOP | LPOP key-name—从列表中移除并返回最左侧的项 |'
- en: '| LINDEX | LINDEX key-name offset—Returns the item at the given offset |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| LINDEX | LINDEX key-name offset—返回给定偏移量的项 |'
- en: '| LRANGE | LRANGE key-name start end—Returns the items in the list at the offsets
    from start to end, inclusive |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| LRANGE | LRANGE key-name start end—返回从 start 到 end 索引的列表项，包括 start 和 end
    |'
- en: '| LTRIM | LTRIM key-name start end—Trims the list to only include items at
    indices between start and end, inclusive |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| LTRIM | LTRIM key-name start end—修剪列表，只包含从 start 到 end 索引之间的项，包括 start 和
    end |'
- en: The semantics of the `LIST` push commands shouldn’t be surprising, and neither
    should the pop commands. We covered a couple of these, along with both `LINDEX`
    and `LRANGE`, back in [chapter 1](kindle_split_011.html#ch01). The next listing
    shows some uses of these push and pop commands.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '`LIST` 推送命令的语义不应该令人惊讶，弹出命令也是如此。我们之前在[第1章](kindle_split_011.html#ch01)中介绍了一些这些命令，包括
    `LINDEX` 和 `LRANGE`。下面的列表显示了这些推送和弹出命令的一些用法。'
- en: Listing 3.3\. A sample interaction showing `LIST` push and pop commands in Redis
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.3\. Redis中`LIST`推送和弹出命令的示例交互
- en: '![](044fig01_alt.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](044fig01_alt.jpg)'
- en: The `LTRIM` command is new in this example, and we can combine it with `LRANGE`
    to give us something that functions much like an `LPOP` or `RPOP` call that returns
    and pops multiple items at once. We’ll talk more about how to make these kinds
    of composite commands atomic^([[1](#ch03fn01)]) later in this chapter, as well
    as dive deeper into more advanced Redis-style transactions in [chapter 4](kindle_split_015.html#ch04).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`LTRIM` 命令在本例中是新的，我们可以将其与 `LRANGE` 结合使用，以提供类似于 `LPOP` 或 `RPOP` 调用并一次性返回和弹出多个项目。我们将在本章后面更详细地讨论如何使这些类型的复合命令原子化^([[1](#ch03fn01)]),
    以及在第 4 章（[chapter 4](kindle_split_015.html#ch04)）中深入探讨更高级的 Redis 风格事务。'
- en: ¹ In Redis, when we talk about a group of commands as being *atomic*, we mean
    that no other client can read or change data while we’re reading or changing that
    same data.
  id: totrans-76
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 在 Redis 中，当我们说一组命令是 *原子* 时，我们的意思是，在我们读取或更改相同数据时，没有其他客户端可以读取或更改数据。
- en: Among the `LIST` commands we didn’t introduce in [chapter 1](kindle_split_011.html#ch01)
    are a few commands that allow you to move items from one list to another, and
    even block while waiting for other clients to add items to `LIST`s. [Table 3.4](#ch03table04)
    shows our blocking pop and item moving commands.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 1 章（[chapter 1](kindle_split_011.html#ch01)）中我们没有介绍的一些 `LIST` 命令允许您将项目从一个列表移动到另一个列表，甚至可以在等待其他客户端向
    `LIST`s 添加项目时阻塞。[表 3.4](#ch03table04) 展示了我们的阻塞弹出和项目移动命令。
- en: Table 3.4\. Some `LIST` commands for blocking `LIST` pops and moving items between
    `LIST`s
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**表 3.4\. 一些用于阻塞 `LIST` 弹出和在 `LIST`s 之间移动项目的 `LIST` 命令**'
- en: '| Command | Example use and description |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用途和描述 |'
- en: '| --- | --- |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '| BLPOP | BLPOP key-name [key-name ...] timeout—Pops the leftmost item from
    the first non-empty LIST, or waits the timeout in seconds for an item |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| BLPOP | BLPOP key-name [key-name ...] timeout—从第一个非空列表的左侧弹出项目，或者等待超时秒数以获取项目
    |'
- en: '| BRPOP | BRPOP key-name [key-name ...] timeout—Pops the rightmost item from
    the first non-empty LIST, or waits the timeout in seconds for an item |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| BRPOP | BRPOP key-name [key-name ...] timeout—从第一个非空列表的右侧弹出项目，或者等待超时秒数以获取项目
    |'
- en: '| RPOPLPUSH | RPOPLPUSH source-key dest-key—Pops the rightmost item from the
    source and LPUSHes the item to the destination, also returning the item to the
    user |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| RPOPLPUSH | RPOPLPUSH source-key dest-key—从源列表的右侧弹出项目，并将其 LPUSH 到目标列表，同时也将项目返回给用户
    |'
- en: '| BRPOPLPUSH | BRPOPLPUSH source-key dest-key timeout—Pops the rightmost item
    from the source and LPUSHes the item to the destination, also returning the item
    to the user, and waiting up to the timeout if the source is empty |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| BRPOPLPUSH | BRPOPLPUSH source-key dest-key timeout—从源列表的右侧弹出项目，并将其 LPUSH
    到目标列表，同时也将项目返回给用户，如果源列表为空，则等待超时 |'
- en: This set of commands is particularly useful when we talk about queues in [chapter
    6](kindle_split_017.html#ch06). The following listing shows some examples of moving
    items around with `BRPOPLPUSH` and popping items from multiple lists with `BLPOP`.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 这组命令在我们讨论第 6 章（[chapter 6](kindle_split_017.html#ch06)）中的队列时特别有用。以下列表展示了使用 `BRPOPLPUSH`
    在多个列表中移动项目以及使用 `BLPOP` 从多个列表中弹出项目的示例。
- en: Listing 3.4\. Blocking `LIST` pop and movement commands in Redis
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '**列表 3.4\. Redis 中阻塞 `LIST` 弹出和移动命令**'
- en: '![](045fig01_alt.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](045fig01_alt.jpg)'
- en: The most common use case for using blocking pop commands as well as the pop/push
    combination commands is in the development of messaging and task queues, which
    we’ll cover in [chapter 6](kindle_split_017.html#ch06).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 使用阻塞弹出命令以及弹出/推送组合命令的最常见用例是在消息传递和任务队列的开发中，我们将在第 6 章（[chapter 6](kindle_split_017.html#ch06)）中介绍。
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Reducing memory use with `LIST`s**'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：使用 `LIST` 减少内存使用**'
- en: 'Back in [sections 2.1](kindle_split_012.html#ch02lev1sec1) and [2.5](kindle_split_012.html#ch02lev1sec5),
    we used `ZSET`s to keep a listing of recently viewed items. Those recently viewed
    items included timestamps as scores to allow us to perform analytics during cleanup
    or after purchase. But including these timestamps takes space, and if timestamps
    aren’t necessary for our analytics, then using a `ZSET` just wastes space. Try
    to replace the use of `ZSET`s in `update_token()` with `LIST`s, while keeping
    the same semantics. Hint: If you find yourself stuck, you can skip ahead to [section
    6.1.1](kindle_split_017.html#ch06lev2sec1) for a push in the right direction.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在[2.1节](kindle_split_012.html#ch02lev1sec1)和[2.5节](kindle_split_012.html#ch02lev1sec5)中，我们使用了`ZSET`s来保存最近查看项目的列表。这些最近查看的项目包括时间戳作为分数，以便我们在清理或购买后进行数据分析。但是，包括这些时间戳会占用空间，如果时间戳对我们分析不是必需的，那么使用`ZSET`只是浪费空间。尝试用`LIST`s替换`update_token()`中的`ZSET`s的使用，同时保持相同的语义。提示：如果你发现自己陷入了困境，可以跳到[6.1.1节](kindle_split_017.html#ch06lev2sec1)以获得正确的方向。
- en: '|  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: One of the primary benefits of `LIST`s is that they can contain multiple string
    values, which can allow you to group data together. `SET`s offer a similar feature,
    but with the caveat that all items in a given `SET` are unique. Let’s look at
    how that changes what we can do with `SET`s.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '`LIST`s的主要优点之一是它们可以包含多个字符串值，这可以让你将数据分组在一起。`SET`s提供类似的功能，但有一个前提，即给定`SET`中的所有项目都是唯一的。让我们看看这如何改变我们可以用`SET`s做什么。'
- en: 3.3\. Sets
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 集合
- en: You’ll remember from [chapter 1](kindle_split_011.html#ch01) that `SET`s hold
    unique items in an unordered fashion. You can quickly add, remove, and determine
    whether an item is in the `SET`. Among the many uses of `SET`s are storing who
    voted for an article and which articles belong to a specific group, as seen in
    [chapter 1](kindle_split_011.html#ch01).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从[第1章](kindle_split_011.html#ch01)中回忆起，`SET`s以无序方式存储独特的项目。你可以快速添加、删除并确定一个项目是否在`SET`中。`SET`s的许多用途包括存储为文章投票的人以及属于特定组的文章，如第1章[第1章](kindle_split_011.html#ch01)中所示。
- en: In this section, we’ll discuss some of the most frequently used commands that
    operate on `SET`s. You’ll learn about the standard operations for inserting, removing,
    and moving members between `SET`s, as well as commands to perform intersection,
    union, and differences on `SET`s. When finished with this section, you’ll be better
    prepared to fully understand how our search examples in [chapter 7](kindle_split_018.html#ch07)
    work.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一些最常用的操作`SET`s的命令。你将了解在`SET`s中插入、删除和移动成员的标准操作，以及执行集合交集、并集和差集的命令。完成本节后，你将更好地了解我们的搜索示例在[第7章](kindle_split_018.html#ch07)中是如何工作的。
- en: Let’s take a look at [table 3.5](#ch03table05) to see some of the more commonly
    used set commands.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下[表3.5](#ch03table05)来了解一些更常用的集合命令。
- en: Table 3.5\. Some commonly used `SET` commands
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.5. 一些常用的`SET`命令
- en: '| Command | Example use and description |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SADD | SADD key-name item [item ...]—Adds the items to the set and returns
    the number of items added that weren’t already present |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| SADD | SADD key-name item [item ...]—将项目添加到集合中，并返回添加的项目数量，这些项目之前不存在 |'
- en: '| SREM | SREM key-name item [item ...]—Removes the items and returns the number
    of items that were removed |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| SREM | SREM key-name item [item ...]—删除项目并返回被删除的项目数量 |'
- en: '| SISMEMBER | SISMEMBER key-name item—Returns whether the item is in the SET
    |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| SISMEMBER | SISMEMBER key-name item—返回项目是否在集合中 |'
- en: '| SCARD | SCARD key-name—Returns the number of items in the SET |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| SCARD | SCARD key-name—返回集合中的项目数量 |'
- en: '| SMEMBERS | SMEMBERS key-name—Returns all of the items in the SET as a Python
    set |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| SMEMBERS | SMEMBERS key-name—以Python集合的形式返回集合中的所有项目 |'
- en: '| SRANDMEMBER | SRANDMEMBER key-name [count]—Returns one or more random items
    from the SET. When count is positive, Redis will return count distinct randomly
    chosen items, and when count is negative, Redis will return count randomly chosen
    items that may not be distinct. |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| SRANDMEMBER | SRANDMEMBER key-name [count]—从集合中返回一个或多个随机项目。当count为正数时，Redis将返回count个不同的随机选择的项目，当count为负数时，Redis将返回count个随机选择的项目，这些项目可能不是不同的。
    |'
- en: '| SPOP | SPOP key-name—Removes and returns a random item from the SET |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| SPOP | SPOP key-name—从集合中随机删除并返回一个项目 |'
- en: '| SMOVE | SMOVE source-key dest-key item—If the item is in the source, removes
    the item from the source and adds it to the destination, returning if the item
    was moved |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| SMOVE | SMOVE source-key dest-key item—如果项目在源中，则从源中删除该项目并将其添加到目标，如果项目已移动则返回
    |'
- en: Some of those commands should be familiar from [chapter 1](kindle_split_011.html#ch01),
    so let’s jump to the next listing to see some of these commands in action.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些命令在第1章（kindle_split_011.html#ch01）中应该已经熟悉，所以让我们跳到下一个列表，看看这些命令的实际应用。
- en: Listing 3.5\. A sample interaction showing some common `SET` commands in Redis
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.5\. 显示Redis中一些常见`SET`命令的示例交互
- en: '![](047fig01_alt.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](047fig01_alt.jpg)'
- en: Using just these commands, we can keep track of unique events and items like
    we did in [chapter 1](kindle_split_011.html#ch01) with voting and article groups.
    But the real power of `SET`s is in the commands that combine multiple `SET`s at
    the same time. [Table 3.6](#ch03table06) shows some of the ways that you can relate
    multiple `SET`s to each other.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用这些命令，我们可以跟踪独特的事件和项目，就像我们在第1章（kindle_split_011.html#ch01）中通过投票和文章组所做的那样。但`SET`的真正力量在于同时组合多个`SET`的命令。[表3.6](#ch03table06)展示了你可以将多个`SET`相互关联的一些方式。
- en: Table 3.6\. Operations for combining and manipulating `SET`s in Redis
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.6\. Redis中组合和操作`SET`的操作
- en: '| Command | Example use and description |'
  id: totrans-114
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用途和描述 |'
- en: '| --- | --- |'
  id: totrans-115
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SDIFF | SDIFF key-name [key-name ...]—Returns the items in the first SET
    that weren’t in any of the other SETs (mathematical set difference operation)
    |'
  id: totrans-116
  prefs: []
  type: TYPE_TB
  zh: '| SDIFF | SDIFF key-name [key-name ...]—返回第一个SET中不在任何其他SET中的项目（数学集合差集操作）|'
- en: '| SDIFFSTORE | SDIFFSTORE dest-key key-name [key-name ...]—Stores at the dest-key
    the items in the first SET that weren’t in any of the other SETs (mathematical
    set difference operation) |'
  id: totrans-117
  prefs: []
  type: TYPE_TB
  zh: '| SDIFFSTORE | SDIFFSTORE dest-key key-name [key-name ...]—将第一个SET中不在其他任何SET中的项目存储到dest-key（数学集合差集操作）|'
- en: '| SINTER | SINTER key-name [key-name ...]—Returns the items that are in all
    of the SETs (mathematical set intersection operation) |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '| SINTER | SINTER key-name [key-name ...]—返回所有SET中都存在的项目（数学集合交集操作）|'
- en: '| SINTERSTORE | SINTERSTORE dest-key key-name [key-name ...]—Stores at the
    dest-key the items that are in all of the SETs (mathematical set intersection
    operation) |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| SINTERSTORE | SINTERSTORE dest-key key-name [key-name ...]—将所有SET中都存在的项目存储到dest-key（数学集合交集操作）|'
- en: '| SUNION | SUNION key-name [key-name ...]—Returns the items that are in at
    least one of the SETs (mathematical set union operation) |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '| SUNION | SUNION key-name [key-name ...]—返回至少在一个SET中的项目（数学集合并集操作）|'
- en: '| SUNIONSTORE | SUNIONSTORE dest-key key-name [key-name ...]—Stores at the
    dest-key the items that are in at least one of the SETs (mathematical set union
    operation) |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| SUNIONSTORE | SUNIONSTORE dest-key key-name [key-name ...]—将至少在一个SET中的项目存储到dest-key（数学集合并集操作）|'
- en: This group of commands are three fundamental `SET` operations, with both “return
    the result” and “store the result” versions. Let’s see a sample of what these
    commands are able to do.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这组命令是三个基本的`SET`操作，既有“返回结果”版本，也有“存储结果”版本。让我们看看这些命令能做什么的一个示例。
- en: Listing 3.6\. A sample interaction showing `SET` difference, intersection, and
    union in Redis
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.6\. 显示Redis中`SET`差集、交集和并集的示例交互
- en: '![](048fig01_alt.jpg)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](048fig01_alt.jpg)'
- en: If you’re comparing with Python sets, Redis `SET`s offer many of the same semantics
    and functionality, but are available remotely to potentially many clients. We’ll
    dig more deeply into what `SET`s are capable of in [chapter 7](kindle_split_018.html#ch07),
    where we build a type of search engine with them.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你与Python集合进行比较，Redis的`SET`提供了许多相同的语义和功能，但可以远程提供给可能许多客户端。我们将在第7章（kindle_split_018.html#ch07）中更深入地探讨`SET`的能力，在那里我们将使用它们构建一种搜索引擎。
- en: Coming up next, we’ll talk about commands that manipulate `HASH`es, which allow
    us to group related keys and values together for easy fetching and updating.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论操作`HASH`的命令，这些命令允许我们将相关的键和值组合在一起，以便于检索和更新。
- en: 3.4\. Hashes
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4\. 哈希表
- en: As introduced in [chapter 1](kindle_split_011.html#ch01), `HASH`es in Redis
    allow you to store groups of key-value pairs in a single higher-level Redis key.
    Functionally, the values offer some of the same features as values in `STRING`s
    and can be useful to group related data together. This data grouping can be thought
    of as being similar to a row in a relational database or a document in a document
    store.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1章（kindle_split_011.html#ch01）中所述，Redis中的`HASH`允许你在单个高级Redis键中存储键值对组。功能上，这些值提供了与`STRING`中的值类似的一些功能，并且可以用来将相关数据分组在一起。这种数据分组可以被视为类似于关系数据库中的一行或文档存储中的一个文档。
- en: In this section, we’ll talk about the most commonly used commands that manipulate
    `HASH`es. You’ll learn more about the operations for adding and removing key-value
    pairs to `HASH`es, as well as commands to fetch all of the `HASH` contents along
    with the ability to increment or decrement values. When finished with this section,
    you’ll better understand the usefulness of storing your data in `HASH`es and how
    to do so. Look at [table 3.7](#ch03table07) to see some commonly used `HASH` commands.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论最常用的操作 `HASH` 的命令。您将了解更多关于向 `HASH` 中添加和删除键值对的操作，以及获取所有 `HASH` 内容的命令，以及增加或减少值的能力。完成本节后，您将更好地理解将数据存储在
    `HASH` 中的有用性以及如何这样做。查看 [表 3.7](#ch03table07) 以了解一些常用的 `HASH` 命令。
- en: Table 3.7\. Operations for adding and removing items from `HASH`es
  id: totrans-130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.7\. 向 `HASH` 中添加和删除项的操作
- en: '| Command | Example use and description |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例使用和描述 |'
- en: '| --- | --- |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| HMGET | HMGET key-name key [key ...]—Fetches the values at the fields in
    the HASH |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| HMGET | HMGET key-name key [key ...]—从 HASH 中获取字段的值|'
- en: '| HMSET | HMSET key-name key value [key value ...]—Sets the values of the fields
    in the HASH |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| HMSET | HMSET key-name key value [key value ...]—设置 HASH 中字段的值|'
- en: '| HDEL | HDEL key-name key [key ...]—Deletes the key-value pairs in the HASH,
    returning the number of pairs that were found and deleted |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| HDEL | HDEL key-name key [key ...]—从 HASH 中删除键值对，返回找到并删除的对的数量|'
- en: '| HLEN | HLEN key-name—Returns the number of key-value pairs in the HASH |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| HLEN | HLEN key-name—返回 HASH 中的键值对数量|'
- en: Some of those commands should be familiar from [chapter 1](kindle_split_011.html#ch01),
    but we have a couple of new ones for getting and setting multiple keys at the
    same time. These bulk commands are mostly a matter of convenience and to improve
    Redis’s performance by reducing the number of calls and round trips between a
    client and Redis. Look at the next listing to see some of them in action.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一些命令可能来自 [第 1 章](kindle_split_011.html#ch01)，但我们有一些新的命令用于同时获取和设置多个键。这些批量命令主要是为了方便，并通过减少客户端和
    Redis 之间的调用和往返次数来提高 Redis 的性能。查看下一个列表以查看其中的一些命令。
- en: Listing 3.7\. A sample interaction showing some common `HASH` commands in Redis
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.7\. 显示 Redis 中一些常见 `HASH` 命令的示例交互
- en: '![](049fig01_alt.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](049fig01_alt.jpg)'
- en: The `HMGET`/`HMSET` commands are similar to their single-argument versions that
    we introduced in [chapter 1](kindle_split_011.html#ch01), only differing in that
    they take a list or dictionary for arguments instead of the single entries.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`HMGET`/`HMSET` 命令与我们在 [第 1 章](kindle_split_011.html#ch01) 中介绍的单一参数版本类似，只是它们接受列表或字典作为参数，而不是单个条目。'
- en: '[Table 3.8](#ch03table08) shows some other bulk commands and more `STRING`-like
    operations on `HASH`es.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3.8](#ch03table08) 展示了一些其他批量命令以及 `HASH` 上的更多类似 `STRING` 的操作。'
- en: Table 3.8\. More bulk operations and `STRING`-like calls over `HASH`es
  id: totrans-142
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.8\. `HASH` 上的更多批量操作和类似 `STRING` 的调用
- en: '| Command | Example use and description |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例使用和描述 |'
- en: '| --- | --- |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| HEXISTS | HEXISTS key-name key—Returns whether the given key exists in the
    HASH |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| HEXISTS | HEXISTS key-name key—返回给定键是否存在于 HASH 中|'
- en: '| HKEYS | HKEYS key-name—Fetches the keys in the HASH |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| HKEYS | HKEYS key-name—从 HASH 中获取键|'
- en: '| HVALS | HVALS key-name—Fetches the values in the HASH |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| HVALS | HVALS key-name—从 HASH 中获取值|'
- en: '| HGETALL | HGETALL key-name—Fetches all key-value pairs from the HASH |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| HGETALL | HGETALL key-name—从 HASH 中获取所有键值对|'
- en: '| HINCRBY | HINCRBY key-name key increment—Increments the value stored at the
    given key by the integer increment |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| HINCRBY | HINCRBY key-name key increment—将给定键存储的值增加整数增量|'
- en: '| HINCRBYFLOAT | HINCRBYFLOAT key-name key increment—Increments the value stored
    at the given key by the float increment |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| HINCRBYFLOAT | HINCRBYFLOAT key-name key increment—将给定键存储的值增加浮点增量|'
- en: With the availability of `HGETALL`, it may not seem as though `HKEYS` and `HVALUES`
    would be that useful, but when you expect your values to be large, you can fetch
    the keys, and then get the values one by one to keep from blocking other requests.
    `HINCRBY` and `HINCRBYFLOAT` should remind you of the `INCRBY` and `INCRBYFLOAT`
    operations available on `STRING` keys, and they have the same semantics, applied
    to `HASH` values. Let’s look at some of these commands being used in the next
    listing.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 随着 `HGETALL` 的可用性，`HKEYS` 和 `HVALUES` 可能看起来不那么有用，但当你预期你的值很大时，你可以获取键，然后逐个获取值，以避免阻塞其他请求。`HINCRBY`
    和 `HINCRBYFLOAT` 应该会让你想起在 `STRING` 键上可用的 `INCRBY` 和 `INCRBYFLOAT` 操作，并且它们具有相同的语义，应用于
    `HASH` 值。让我们在下一个列表中查看一些这些命令的使用示例。
- en: Listing 3.8\. A sample interaction showing some more advanced features of Redis
    `HASH`es
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.8\. 一个展示Redis `HASH`的高级功能的示例交互
- en: '![](050fig01_alt.jpg)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![](050fig01_alt.jpg)'
- en: As we described earlier, when confronted with a large value in a `HASH`, we
    can fetch the keys and only fetch values that we’re interested in to reduce the
    amount of data that’s transferred. We can also perform key checks, as we could
    perform member checks on `SET`s with `SISMEMBER`. And back in [chapter 1](kindle_split_011.html#ch01),
    we used `HINCRBY` to keep track of the number of votes an article had received,
    which we just revisited.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所述，当面对`HASH`中的大值时，我们可以获取键，并且只获取我们感兴趣的值，以减少传输的数据量。我们还可以执行键检查，就像我们可以在`SET`上使用`SISMEMBER`执行成员检查一样。在[第1章](kindle_split_011.html#ch01)中，我们使用了`HINCRBY`来跟踪文章收到的投票数，这正是我们刚刚回顾的内容。
- en: 'Let’s look at a structure that we’ll be using fairly often in the remaining
    chapters: sorted sets.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看在接下来的章节中我们将非常频繁使用的一种结构：排序集合。
- en: 3.5\. Sorted sets
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 排序集合
- en: '`ZSET`s offer the ability to store a mapping of members to scores (similar
    to the keys and values of `HASH`es). These mappings allow us to manipulate the
    numeric scores,^([[2](#ch03fn02)]) and fetch and scan over both members and scores
    based on the sorted order of the scores. In [chapter 1](kindle_split_011.html#ch01),
    we showed a brief example that used `ZSET`s as a way of sorting submitted articles
    based on time and how many up-votes they had received, and in [chapter 2](kindle_split_012.html#ch02),
    we had an example that used `ZSET`s as a way of handling the expiration of old
    cookies.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZSET`提供了将成员映射到分数的能力（类似于`HASH`的键和值）。这些映射允许我们操作数值分数，^([[2](#ch03fn02)])并根据分数的排序顺序检索和扫描成员和分数。在[第1章](kindle_split_011.html#ch01)中，我们展示了使用`ZSET`根据时间和收到的点赞数对提交的文章进行排序的简短示例，在[第2章](kindle_split_012.html#ch02)中，我们有一个使用`ZSET`处理旧cookie过期示例。'
- en: ² Scores are actually stored inside Redis as IEEE 754 floating-point doubles.
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 分数实际上以IEEE 754浮点双精度格式存储在Redis中。
- en: In this section, we’ll talk about commands that operate on `ZSET`s. You’ll learn
    how to add and update items in `ZSET`s, as well as how to use the `ZSET` intersection
    and union commands. When finished with this section, you’ll have a much clearer
    understanding about how `ZSET`s work, which will help you to better understand
    what we did with them in [chapter 1](kindle_split_011.html#ch01), and how we’ll
    use them in [chapters 5](kindle_split_016.html#ch05), [6](kindle_split_017.html#ch06),
    and [7](kindle_split_018.html#ch07).
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论操作`ZSET`的命令。您将学习如何在`ZSET`中添加和更新项目，以及如何使用`ZSET`的交集和并集命令。完成本节后，您将更清楚地了解`ZSET`的工作原理，这将帮助您更好地理解我们在[第1章](kindle_split_011.html#ch01)中对其所做的工作，以及我们将在[第5章](kindle_split_016.html#ch05)、[第6章](kindle_split_017.html#ch06)和[第7章](kindle_split_018.html#ch07)中使用它们的方式。
- en: Let’s look at some commonly used `ZSET` commands in [table 3.9](#ch03table09).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看表3.9中的一些常用`ZSET`命令。
- en: Table 3.9\. Some common `ZSET` commands
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.9\. 一些常见的`ZSET`命令
- en: '| Command | Example use and description |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ZADD | ZADD key-name score member [score member ...]—Adds members with the
    given scores to the ZSET |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| ZADD | ZADD key-name score member [score member ...]—将具有给定分数的成员添加到ZSET中 |'
- en: '| ZREM | ZREM key-name member [member ...]—Removes the members from the ZSET,
    returning the number of members that were removed |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| ZREM | ZREM key-name member [member ...]—从ZSET中删除成员，返回被删除成员的数量 |'
- en: '| ZCARD | ZCARD key-name—Returns the number of members in the ZSET |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| ZCARD | ZCARD key-name—返回ZSET中的成员数量 |'
- en: '| ZINCRBY | ZINCRBY key-name increment member—Increments the member in the
    ZSET |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| ZINCRBY | ZINCRBY key-name increment member—在ZSET中对指定成员的分数进行增量操作 |'
- en: '| ZCOUNT | ZCOUNT key-name min max—Returns the number of members with scores
    between the provided minimum and maximum |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| ZCOUNT | ZCOUNT key-name min max—返回分数介于min和max之间的成员数量 |'
- en: '| ZRANK | ZRANK key-name member—Returns the position of the given member in
    the ZSET |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| ZRANK | ZRANK key-name member—返回给定成员在ZSET中的位置 |'
- en: '| ZSCORE | ZSCORE key-name member—Returns the score of the member in the ZSET
    |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| ZSCORE | ZSCORE key-name member—返回ZSET中成员的分数 |'
- en: '| ZRANGE | ZRANGE key-name start stop [WITHSCORES]—Returns the members and
    optionally the scores for the members with ranks between start and stop |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| ZRANGE | ZRANGE key-name start stop [WITHSCORES]—返回排名在start和stop之间的成员，以及可选的分数
    |'
- en: We’ve used some of these commands in [chapters 1](kindle_split_011.html#ch01)
    and [2](kindle_split_012.html#ch02), so they should already be familiar to you.
    Let’s quickly revisit the use of some of our commands.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 [1](kindle_split_011.html#ch01) 和 [2](kindle_split_012.html#ch02) 章节中使用了一些这些命令，所以它们应该已经对你很熟悉了。让我们快速回顾一下我们的一些命令的使用。
- en: Listing 3.9\. A sample interaction showing some common `ZSET` commands in Redis
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.9\. 显示 Redis 中一些常见 `ZSET` 命令的示例交互
- en: '![](051fig01_alt.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](051fig01_alt.jpg)'
- en: You’ll likely remember our use of `ZADD`, `ZREM`, `ZINCRBY`, `ZSCORE`, and `ZRANGE`
    from [chapters 1](kindle_split_011.html#ch01) and [2](kindle_split_012.html#ch02),
    so their semantics should come as no surprise. The `ZCOUNT` command is a little
    different than the others, primarily meant to let you discover the number of values
    whose scores are between the provided minimum and maximum scores.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得我们在第 [1](kindle_split_011.html#ch01) 和 [2](kindle_split_012.html#ch02)
    章节中提到的 `ZADD`, `ZREM`, `ZINCRBY`, `ZSCORE`, 和 `ZRANGE` 命令，因此它们的语义应该不会让你感到意外。`ZCOUNT`
    命令与其他命令略有不同，主要是为了让你能够发现分数在提供的最小和最大分数之间的值的数量。
- en: '[Table 3.10](#ch03table10) shows several more `ZSET` commands in Redis that
    you’ll find useful.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 3.10](#ch03table10) 展示了 Redis 中一些你可能会发现很有用的 `ZSET` 命令。'
- en: Table 3.10\. Commands for fetching and deleting ranges of data from `ZSET`s
    and offering `SET`-like intersections
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.10\. 从 `ZSET` 中获取和删除数据范围以及提供类似 `SET` 的交集的命令
- en: '| Command | Example use and description |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例使用和描述 |'
- en: '| --- | --- |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| ZREVRANK | ZREVRANK key-name member—Returns the position of the member in
    the ZSET, with members ordered in reverse |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| ZREVRANK | ZREVRANK key-name member—返回成员在 ZSET 中的位置，成员按逆序排列 |'
- en: '| ZREVRANGE | ZREVRANGE key-name start stop [WITHSCORES]—Fetches the given
    members from the ZSET by rank, with members in reverse order |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| ZREVRANGE | ZREVRANGE key-name start stop [WITHSCORES]—通过排名从 ZSET 中获取指定的成员，成员按逆序排列
    |'
- en: '| ZRANGEBYSCORE | ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]—Fetches
    the members between min and max |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| ZRANGEBYSCORE | ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count]—获取
    min 和 max 之间的成员 |'
- en: '| ZREVRANGEBYSCORE | ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset
    count]—Fetches the members in reverse order between min and max |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| ZREVRANGEBYSCORE | ZREVRANGEBYSCORE key max min [WITHSCORES] [LIMIT offset
    count]—按逆序获取 min 和 max 之间的成员 |'
- en: '| ZREMRANGEBYRANK | ZREMRANGEBYRANK key-name start stop—Removes the items from
    the ZSET with ranks between start and stop |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| ZREMRANGEBYRANK | ZREMRANGEBYRANK key-name start stop—从 ZSET 中删除排名在 start
    和 stop 之间的项目 |'
- en: '| ZREMRANGEBYSCORE | ZREMRANGEBYSCORE key-name min max—Removes the items from
    the ZSET with scores between min and max |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ZREMRANGEBYSCORE | ZREMRANGEBYSCORE key-name min max—从 ZSET 中删除分数在 min 和
    max 之间的项目 |'
- en: '| ZINTERSTORE | ZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight
    [weight ...]] [AGGREGATE SUM&#124;MIN&#124;MAX]—Performs a SET-like intersection
    of the provided ZSETs |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ZINTERSTORE | ZINTERSTORE dest-key key-count key [key ...] [WEIGHTS weight
    [weight ...]] [AGGREGATE SUM|MIN|MAX]—执行提供的 ZSET 的类似 SET 的交集操作 |'
- en: '| ZUNIONSTORE | ZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight
    [weight ...]] [AGGREGATE SUM&#124;MIN&#124;MAX]—Performs a SET-like union of the
    provided ZSETs |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| ZUNIONSTORE | ZUNIONSTORE dest-key key-count key [key ...] [WEIGHTS weight
    [weight ...]] [AGGREGATE SUM|MIN|MAX]—执行提供的 ZSET 的类似 SET 的并集操作 |'
- en: This is the first time that you’ve seen a few of these commands. If some of
    the `ZREV*` commands are confusing, remember that they work the same as their
    nonreversed counterparts, except that the `ZSET` behaves as if it were in reverse
    order (sorted by score from high to low). You can see a few examples of their
    use in the next listing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你第一次看到这些命令中的几个。如果一些 `ZREV*` 命令让你感到困惑，请记住，它们的工作方式与它们的非逆序对应命令相同，只是 `ZSET` 的行为就像它是逆序排列的（按分数从高到低排序）。你可以在下一列表中看到它们的一些使用示例。
- en: Listing 3.10\. A sample interaction showing `ZINTERSTORE` and `ZUNIONSTORE`
  id: totrans-189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.10\. 显示 `ZINTERSTORE` 和 `ZUNIONSTORE` 的示例交互
- en: '![](053fig01_alt.jpg)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![](053fig01_alt.jpg)'
- en: '`ZSET` union and intersection can be difficult to understand at first glance,
    so let’s look at some figures that show what happens during the processes of both
    intersection and union. [Figure 3.1](#ch03fig01) shows the intersection of the
    two `ZSET`s and the final `ZSET` result. In this case, our aggregate is the default
    of `sum`, so scores are added.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZSET` 的并集和交集一开始可能难以理解，所以让我们看看一些图示，展示在交集和并集过程中发生的情况。[图 3.1](#ch03fig01) 展示了两个
    `ZSET` 的交集和最终的 `ZSET` 结果。在这种情况下，我们的聚合是默认的 `sum`，所以分数是相加的。'
- en: Figure 3.1\. What happens when calling `conn.zinterstore('zset-i', ['zset-1',
    'zset-2'])`; elements that exist in both zset-1 and zset-2 are added together
    to get zset-i
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1\. 调用`conn.zinterstore('zset-i', ['zset-1', 'zset-2'])`时会发生什么；存在于zset-1和zset-2中的元素会被相加得到zset-i
- en: '![](03fig01_alt.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig01_alt.jpg)'
- en: Unlike intersection, when we perform a union operation, items that exist in
    at least one of the input `ZSET`s are included in the output. [Figure 3.2](#ch03fig02)
    shows the result of performing a union operation with a different aggregate function,
    `min`, which takes the minimum score if a member is in multiple input `ZSET`s.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 与交集不同，当我们执行并集操作时，至少存在于一个输入`ZSET`中的项目会被包含在输出中。[图3.2](#ch03fig02)显示了使用不同的聚合函数`min`执行并集操作的结果，如果成员在多个输入`ZSET`中，则取最小分数。
- en: Figure 3.2\. What happens when calling `conn.zunionstore('zset-u', ['zset-1',
    'zset-2'], aggregate='min')`; elements that exist in either zset-1 or zset-2 are
    combined with the `minimum` function to get zset-u
  id: totrans-195
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2\. 调用`conn.zunionstore('zset-u', ['zset-1', 'zset-2'], aggregate='min')`时会发生什么；存在于zset-1或zset-2中的元素与`最小`函数结合得到zset-u
- en: '![](03fig02_alt.jpg)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig02_alt.jpg)'
- en: In [chapter 1](kindle_split_011.html#ch01), we used the fact that we can include
    `SET`s as part of `ZSET` union and intersection operations. This feature allowed
    us to easily add and remove articles from groups without needing to propagate
    scoring and insertion times into additional `ZSET`s. [Figure 3.3](#ch03fig03)
    shows a `ZUNIONSTORE` call that combines two `ZSET`s with one `SET` to produce
    a final `ZSET`.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_011.html#ch01)中，我们使用了可以将`SET`作为`ZSET`并集和交集操作一部分的事实。这个特性允许我们轻松地在不将评分和插入时间传播到额外的`ZSET`的情况下添加和删除文章。![图3.3](#ch03fig03)显示了将两个`ZSET`和一个`SET`组合起来产生最终`ZSET`的`ZUNIONSTORE`调用。
- en: Figure 3.3\. What happens when calling `conn.zunionstore('zset-u2', ['zset-1',
    'zset-2', 'set-1'])`; elements that exist in any of zset-1, zset-2, or set-1 are
    combined via addition to get zset-u2
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3\. 调用`conn.zunionstore('zset-u2', ['zset-1', 'zset-2', 'set-1'])`时会发生什么；存在于zset-1、zset-2或set-1中的元素通过相加组合得到zset-u2
- en: '![](03fig03_alt.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![](03fig03_alt.jpg)'
- en: In [chapter 7](kindle_split_018.html#ch07), we’ll use `ZINTERSTORE` and `ZUNIONSTORE`
    as parts of a few different types of search. We’ll also talk about a few different
    ways to combine `ZSET` scores with the optional `WEIGHTS` parameter to further
    extend the types of problems that can be solved with `SET`s and `ZSET`s.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](kindle_split_018.html#ch07)中，我们将使用`ZINTERSTORE`和`ZUNIONSTORE`作为几种不同类型搜索的一部分。我们还将讨论几种不同的方法来结合`ZSET`分数与可选的`WEIGHTS`参数，以进一步扩展可以使用`SET`和`ZSET`解决的问题类型。
- en: As you’re developing applications, you may have come upon a pattern known as
    publish/subscribe, also referred to as *pub/sub*. Redis includes this functionality,
    which we’ll cover next.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发应用程序时，你可能已经遇到了一种称为发布/订阅的模式，也称为*pub/sub*。Redis包含了这个功能，我们将在下一节中介绍。
- en: 3.6\. Publish/subscribe
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6\. 发布/订阅
- en: If you’re confused because you can’t remember reading about publish or subscribe
    yet, don’t be—this is the first time we’ve talked about it. Generally, the concept
    of publish/subscribe, also known as pub/sub, is characterized by listeners *subscribing*
    to channels, with publishers sending binary string messages to channels. Anyone
    listening to a given channel will receive all messages sent to that channel while
    they’re connected and listening. You can think of it like a radio station, where
    subscribers can listen to multiple radio stations at the same time, and publishers
    can send messages on any radio station.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你因为记不起阅读过关于发布或订阅的内容而感到困惑，请不要担心——这是我们第一次讨论它。通常，发布/订阅的概念，也称为pub/sub，其特点是听众*订阅*频道，发布者向频道发送二进制字符串消息。任何正在监听特定频道的听众都会在连接并监听时接收到发送到该频道的所有消息。你可以将其想象成一个广播电台，订阅者可以同时收听多个电台，发布者可以在任何电台发送消息。
- en: In this section, we’ll discuss and use operations involving publish and subscribe.
    When finished with this section, you’ll know how to use these commands, and why
    we use other similar solutions in later chapters.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论和使用涉及发布和订阅的操作。完成本节后，你将知道如何使用这些命令，以及为什么在后面的章节中我们会使用其他类似解决方案。
- en: In Redis, the pub/sub concept has been included through the use of a collection
    of the five commands shown in [table 3.11](#ch03table11).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中，通过使用[表3.11](#ch03table11)中显示的五个命令集合来包含pub/sub概念。
- en: Table 3.11\. Commands for handling pub/sub in Redis
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.11\. Redis中处理pub/sub的命令
- en: '| Command | Example use and description |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用法和描述 |'
- en: '| --- | --- |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SUBSCRIBE | SUBSCRIBE channel [channel ...]—Subscribes to the given channels
    |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 订阅 | 订阅频道 [频道 ...]—订阅给定的频道 |'
- en: '| UNSUBSCRIBE | UNSUBSCRIBE [channel [channel ...]]—Unsubscribes from the provided
    channels, or unsubscribes all channels if no channel is given |'
  id: totrans-210
  prefs: []
  type: TYPE_TB
  zh: '| 取消订阅 | 取消订阅 [频道 [频道 ...]]—取消订阅提供的频道，如果没有提供频道则取消订阅所有频道 |'
- en: '| PUBLISH | PUBLISH channel message—Publishes a message to the given channel
    |'
  id: totrans-211
  prefs: []
  type: TYPE_TB
  zh: '| PUBLISH | PUBLISH 频道 消息—向给定的频道发布消息 |'
- en: '| PSUBSCRIBE | PSUBSCRIBE pattern [pattern ...]—Subscribes to messages broadcast
    to channels that match the given pattern |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '| PSUBSCRIBE | PSUBSCRIBE 模式 [模式 ...]—订阅发送到匹配给定模式的频道的消息 |'
- en: '| PUNSUBSCRIBE | PUNSUBSCRIBE [pattern [pattern ...]]—Unsubscribes from the
    provided patterns, or unsubscribes from all subscribed patterns if none are given
    |'
  id: totrans-213
  prefs: []
  type: TYPE_TB
  zh: '| PUNSUBSCRIBE | PUNSUBSCRIBE [模式 [模式 ...]]—取消订阅提供的模式，如果没有提供模式则取消订阅所有已订阅的模式
    |'
- en: With the way the `PUBLISH` and `SUBSCRIBE` commands are implemented on the Python
    side of things, it’s easier to demonstrate the feature if we use a helper thread
    to handle the `PUBLISH`ing. You can see an example of `PUBLISH`/`SUBSCRIBE` in
    the next listing.^([[3](#ch03fn03)])
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 端实现 `PUBLISH` 和 `SUBSCRIBE` 命令的方式中，如果我们使用一个辅助线程来处理 `PUBLISH`，那么演示这个特性会更容易。你可以在下一列表中看到一个
    `PUBLISH`/`SUBSCRIBE` 的例子.^([[3](#ch03fn03)])
- en: '³ If you’d like to run this code yourself, you can: I included the `publisher()`
    and `run_pubsub()` functions in the source code for this chapter.'
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 如果你想亲自运行此代码，你可以：我在本章的源代码中包含了 `publisher()` 和 `run_pubsub()` 函数。
- en: Listing 3.11\. Using `PUBLISH` and `SUBSCRIBE` in Redis
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.11\. 在 Redis 中使用 `PUBLISH` 和 `SUBSCRIBE`
- en: '![](ch03ex11-0.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex11-0.jpg)'
- en: '![](ch03ex11-1.jpg)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![](ch03ex11-1.jpg)'
- en: The publish/subscribe pattern and its Redis implementation can be useful. If
    you skip ahead and scan around other chapters, you’ll notice that we only use
    publish/subscribe in one other section, [section 8.5](kindle_split_019.html#ch08lev1sec5).
    If `PUBLISH` and `SUBSCRIBE` are so useful, why don’t we use them very much? There
    are two reasons.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 发布/订阅模式和其在 Redis 中的实现可能很有用。如果你跳过并浏览其他章节，你会注意到我们只在另一个部分使用发布/订阅，[第 8.5 节](kindle_split_019.html#ch08lev1sec5)。如果
    `PUBLISH` 和 `SUBSCRIBE` 是如此有用，为什么我们不用得很多？有两个原因。
- en: One reason is because of Redis system reliability. In older versions of Redis,
    a client that had subscribed to channels but didn’t read sent messages fast enough
    could cause Redis itself to keep a large outgoing buffer. If this outgoing buffer
    grew too large, it could cause Redis to slow down drastically or crash, could
    cause the operating system to kill Redis, and could even cause the operating system
    itself to become unusable. Modern versions of Redis don’t have this issue, and
    will disconnect subscribed clients that are unable to keep up with the `client-output-buffer-limit
    pubsub` configuration option (which we’ll talk about in [chapter 8](kindle_split_019.html#ch08)).
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一个原因是由于 Redis 系统的可靠性。在 Redis 的旧版本中，如果一个客户端订阅了频道但未能足够快地读取发送的消息，这可能导致 Redis 本身保持一个很大的输出缓冲区。如果这个输出缓冲区变得过大，可能会导致
    Redis 迅速减慢或崩溃，可能导致操作系统杀死 Redis，甚至可能导致操作系统本身变得不可用。Redis 的现代版本没有这个问题，并且会断开无法跟上 `client-output-buffer-limit
    pubsub` 配置选项（我们将在[第 8 章](kindle_split_019.html#ch08)中讨论）的已订阅客户端。
- en: The second reason is for data transmission reliability. Within any sort of networked
    system, you must operate under the assumption that your connection could fail
    at some point. Typically, this is handled by one side or the other reconnecting
    as a result of a connection error. Our Python Redis client will normally handle
    connection issues well by automatically reconnecting on failure, automatically
    handling connection pooling (we’ll talk about this more in [chapter 4](kindle_split_015.html#ch04)),
    and more. But in the case of clients that have subscribed, if the client is disconnected
    and a message is sent before it can reconnect, the client will never see the message.
    When you’re relying on receiving messages over a channel, the semantics of `PUBLISH`/`SUBSCRIBE`
    in Redis may let you down.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是数据传输的可靠性。在任何网络化系统中，你必须假设你的连接可能在某个时刻失败。通常，这通过任一方的重新连接来处理，作为连接错误的结果。我们的
    Python Redis 客户端通常会通过在失败时自动重新连接、自动处理连接池（我们将在[第 4 章](kindle_split_015.html#ch04)中更多地讨论这一点）以及更多来很好地处理连接问题。但在已订阅的客户端的情况下，如果客户端断开连接，并且在它重新连接之前发送了消息，客户端将永远看不到这条消息。当你依赖于通过频道接收消息时，Redis
    中 `PUBLISH`/`SUBSCRIBE` 的语义可能会让你失望。
- en: It’s for these two reasons that we write two different methods to handle reliable
    message delivery in [chapter 6](kindle_split_017.html#ch06), which works in the
    face of network disconnections, and which won’t cause Redis memory to grow (even
    in older versions of Redis) unless you want it to.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 正是因为这两个原因，我们在 [第 6 章](kindle_split_017.html#ch06) 中编写了两种不同的方法来处理可靠的消息传递，这种方法在面对网络断开连接的情况下也能工作，并且不会导致
    Redis 内存增长（即使在 Redis 的旧版本中），除非你希望它增长。
- en: If you like the simplicity of using `PUBLISH`/`SUBSCRIBE`, and you’re okay with
    the chance that you may lose a little data, then feel free to use pub/sub instead
    of our methods, as we also do in [section 8.5](kindle_split_019.html#ch08lev1sec5);
    just remember to configure `client-output-buffer-limit pubsub` reasonably before
    starting.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你喜欢使用 `PUBLISH`/`SUBSCRIBE` 的简单性，并且可以接受可能会丢失一些数据的风险，那么你可以自由地使用 pub/sub 而不是我们的方法，就像我们在
    [第 8.5 节](kindle_split_019.html#ch08lev1sec5) 中做的那样；只是记得在开始之前合理地配置 `client-output-buffer-limit
    pubsub`。
- en: At this point, we’ve covered the majority of commands that you’ll use on a regular
    basis that are related to individual data types. There are a few more commands
    that you’ll also likely use, which don’t fit into our nice five structures-plus-pub/sub
    theme.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了你将定期使用的与单个数据类型相关的多数命令。还有一些你可能会使用的其他命令，它们不适合我们那五种结构加 pub/sub 的主题。
- en: 3.7\. Other commands
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 其他命令
- en: So far we’ve gone through the five structures that Redis provides, as well as
    shown a bit of pub/sub. The commands in this section are commands that operate
    on multiple types of data. We’ll first cover `SORT`, which can involve `STRING`s,
    `SET`s or `LIST`s, and `HASH`es all at the same time. We’ll then cover basic transactions
    with `MULTI` and `EXEC`, which can allow you to execute multiple commands together
    as though they were just one command. Finally, we’ll cover the variety of automatic
    expiration commands for automatically deleting unnecessary data.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经介绍了 Redis 提供的五种结构，以及展示了一些 pub/sub 的内容。本节中的命令是操作多种类型数据的命令。我们首先介绍 `SORT`，它可以同时涉及
    `STRING`s、`SET`s、`LIST`s 和 `HASH`es。然后，我们将介绍使用 `MULTI` 和 `EXEC` 的基本事务，这可以让你像执行一个命令一样一起执行多个命令。最后，我们将介绍各种自动过期命令，用于自动删除不必要的数据。
- en: After reading this section, you should have a better idea of how to combine
    and manipulate multiple data types at the same time.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读本节之后，你应该对如何同时组合和操作多种数据类型有了更好的了解。
- en: 3.7.1\. Sorting
  id: totrans-228
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.1\. 排序
- en: 'Sorting in Redis is similar to sorting in other languages: we want to take
    a sequence of items and order them according to some comparison between elements.
    `SORT` allows us to sort `LIST`s, `SET`s, and `ZSET`s according to data in the
    `LIST`/`SET`/`ZSET` data stored in `STRING` keys, or even data stored in `HASH`es.
    If you’re coming from a relational database background, you can think of `SORT`
    as like the `order by` clause in a SQL statement that can reference other rows
    and tables. [Table 3.12](#ch03table12) shows the `SORT` command definition.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Redis 中的排序与其他语言的排序类似：我们希望对一系列项目进行排序，根据元素之间的比较来排列它们。`SORT` 命令允许我们根据存储在 `STRING`
    键中的 `LIST`/`SET`/`ZSET` 数据来排序 `LIST`s、`SET`s 和 `ZSET`s，甚至可以排序存储在 `HASH` 中的数据。如果你来自关系型数据库的背景，你可以将
    `SORT` 视为类似于 SQL 语句中的 `order by` 子句，它可以引用其他行和表。[表 3.12](#ch03table12) 展示了 `SORT`
    命令的定义。
- en: Table 3.12\. The `SORT` command definition
  id: totrans-230
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 3.12\. `SORT` 命令定义
- en: '| Command | Example use and description |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| Command | 示例使用和描述 |'
- en: '| --- | --- |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| SORT | SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GET
    pattern ...]] [ASC&#124;DESC] [ALPHA] [STORE dest-key]—Sorts the input LIST, SET,
    or ZSET according to the options provided, and returns or stores the result |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| SORT | SORT source-key [BY pattern] [LIMIT offset count] [GET pattern [GET
    pattern ...]] [ASC|DESC] [ALPHA] [STORE dest-key]—根据提供的选项对输入的 LIST、SET 或 ZSET
    进行排序，并返回或存储结果 |'
- en: Some of the more basic options with `SORT` include the ability to order the
    results in descending order rather than the default ascending order, consider
    items as though they were numbers, compare as though items were binary strings
    (the sorted order of the strings `'110'` and `'12'` are different than the sorted
    order of the numbers 110 and 12), sorting by values not included in the original
    sequence, and even fetching values outside of the input `LIST`, `SET`, or `ZSET`.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: '`SORT` 的一些更基本选项包括能够以降序而不是默认的升序对结果进行排序，将项目视为数字，将项目视为二进制字符串进行比较（字符串 `''110''`
    和 `''12''` 的排序顺序与数字 110 和 12 的排序顺序不同），根据原始序列中未包含的值进行排序，甚至可以检索输入 `LIST`、`SET` 或
    `ZSET` 之外的值。'
- en: You can see some examples that use `SORT` in [listing 3.12](#ch03ex12). The
    first few lines of the listing show the addition of some initial data, and basic
    sorting (by numeric value and by string order). The remaining parts show how we
    can store data to be sorted by and/or fetched inside `HASH`es using a special
    syntax.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 [列表 3.12](#ch03ex12) 中看到一些使用 `SORT` 的示例。列表的前几行显示了初始数据的添加和基本排序（按数值和字符串顺序）。其余部分显示了如何使用特殊语法将数据存储到要排序的，以及/或从
    `HASH` 中检索的内部。
- en: Listing 3.12\. A sample interaction showing some uses of `SORT`
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 3.12\. 显示 `SORT` 一些使用的示例交互
- en: '![](057fig01_alt.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](057fig01_alt.jpg)'
- en: Sorting can be used to sort `LIST`s, but it can also sort `SET`s, turning the
    result into a `LIST`. In this example, we sorted numbers character by character
    (via the `alpha` keyword argument), we sorted some items based on external data,
    and we were even able to fetch external data to return. When combined with `SET`
    intersection, union, and difference, along with storing data externally inside
    `HASH`es, `SORT` is a powerful command. We’ll spend some time talking about how
    to combine `SET` operations with `SORT` in [chapter 7](kindle_split_018.html#ch07).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 排序可以用来排序 `LIST`s，但它也可以排序 `SET`s，将结果转换为 `LIST`。在这个例子中，我们按字符逐个排序数字（通过 `alpha`
    关键字参数），我们根据外部数据排序了一些项目，甚至能够获取外部数据返回。当与 `SET` 的交集、并集和差集操作以及在外部 `HASH` 中存储数据相结合时，`SORT`
    是一个强大的命令。我们将在第 7 章（kindle_split_018.html#ch07）中花时间讨论如何将 `SET` 操作与 `SORT` 结合使用。
- en: Though `SORT` is the only command that can manipulate three types of data at
    the same time, basic Redis transactions can let you manipulate multiple data types
    with a series of commands without interruption.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `SORT` 是唯一一个可以同时操作三种数据类型的命令，但基本的 Redis 事务可以通过一系列命令让您在不被打断的情况下操作多种数据类型。
- en: 3.7.2\. Basic Redis transactions
  id: totrans-240
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.2\. 基本Redis事务
- en: 'Sometimes we need to make multiple calls to Redis in order to manipulate multiple
    structures at the same time. Though there are a few commands to copy or move items
    between keys, there isn’t a single command to move items between types (though
    you can copy from a `SET` to a `ZSET` with `ZUNIONSTORE`). For operations involving
    multiple keys (of the same or different types), Redis has five commands that help
    us operate on multiple keys without interruption: `WATCH`, `MULTI`, `EXEC`, `UNWATCH`,
    and `DISCARD`.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们需要多次调用 Redis 来同时操作多个结构。尽管有一些命令可以复制或移动键之间的项目，但没有一个命令可以移动不同类型之间的项目（尽管您可以使用
    `ZUNIONSTORE` 从 `SET` 复制到 `ZSET`）。对于涉及多个键（相同或不同类型）的操作，Redis 有五个命令可以帮助我们在不中断的情况下操作多个键：`WATCH`、`MULTI`、`EXEC`、`UNWATCH`
    和 `DISCARD`。
- en: For now, we’ll only talk about the simplest version of a Redis transaction,
    which uses `MULTI` and `EXEC`. If you want to see an example that uses `WATCH`,
    `MULTI`, `EXEC`, and `UNWATCH`, you can skip ahead to [section 4.4](kindle_split_015.html#ch04lev1sec4),
    where I explain why you’d need to use `WATCH` and `UNWATCH` with `MULTI` and `EXEC`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只讨论 Redis 事务的最简单版本，它使用 `MULTI` 和 `EXEC`。如果您想看使用 `WATCH`、`MULTI`、`EXEC`
    和 `UNWATCH` 的示例，可以跳到第 4.4 节（kindle_split_015.html#ch04lev1sec4），在那里我解释了为什么需要在使用
    `MULTI` 和 `EXEC` 时使用 `WATCH` 和 `UNWATCH`。
- en: What is a basic transaction in Redis?
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Redis 中的基本事务是什么？
- en: In Redis, a basic transaction involving `MULTI` and `EXEC` is meant to provide
    the opportunity for one client to execute multiple commands A, B, C, ... without
    other clients being able to interrupt them. This isn’t the same as a relational
    database transaction, which can be executed partially, and then rolled back or
    committed. In Redis, every command passed as part of a basic `MULTI`/`EXEC` transaction
    is executed one after another until they’ve completed. After they’ve completed,
    other clients may execute their commands.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Redis 中，涉及 `MULTI` 和 `EXEC` 的基本事务旨在为单个客户端提供执行多个命令 A、B、C、... 的机会，而其他客户端无法中断它们。这不同于关系型数据库事务，它可以部分执行，然后回滚或提交。在
    Redis 中，作为基本 `MULTI`/`EXEC` 事务一部分传递的每个命令都会依次执行，直到完成。完成之后，其他客户端可以执行它们的命令。
- en: To perform a transaction in Redis, we first call `MULTI`, followed by any sequence
    of commands we intend to execute, followed by `EXEC`. When seeing `MULTI`, Redis
    will queue up commands from that same connection until it sees an `EXEC`, at which
    point Redis will execute the queued commands sequentially without interruption.
    Semantically, our Python library handles this by the use of what’s called a *pipeline*.
    Calling the `pipeline()` method on a connection object will create a transaction,
    which when used correctly will automatically wrap a sequence of commands with
    `MULTI` and `EXEC`. Incidentally, the Python Redis client will also store the
    commands to send until we actually want to send them. This reduces the number
    of round trips between Redis and the client, which can improve the performance
    of a sequence of commands.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Redis中执行事务，我们首先调用`MULTI`，然后是任何我们打算执行的命令序列，最后调用`EXEC`。当看到`MULTI`时，Redis将排队等待来自同一连接的命令，直到它看到`EXEC`，此时Redis将顺序执行排队的命令，而不会中断。从语义上讲，我们的Python库通过使用所谓的*管道*来处理这个问题。在连接对象上调用`pipeline()`方法将创建一个事务，如果使用得当，它将自动将一系列命令用`MULTI`和`EXEC`包裹起来。顺便提一下，Python
    Redis客户端也会存储要发送的命令，直到我们真正想要发送它们。这减少了Redis和客户端之间的往返次数，可以提高一系列命令的性能。
- en: As was the case with `PUBLISH` and `SUBSCRIBE`, the simplest way to demonstrate
    the result of using a transaction is through the use of threads. In the next listing,
    you can see the result of parallel increment operations without a transaction.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 与`PUBLISH`和`SUBSCRIBE`的情况一样，通过使用线程来演示使用事务的结果是最简单的方法。在下一个列表中，你可以看到在没有事务的情况下并行增加操作的结果。
- en: Listing 3.13\. What can happen without transactions during parallel execution
  id: totrans-247
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.13\. 在并行执行期间没有事务可能发生的情况
- en: '![](059fig01_alt.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '|  |'
- en: Without transactions, each of the three threads are able to increment the `notrans:`
    counter before the decrement comes through. We exaggerate potential issues here
    by including a 100ms sleep, but if we needed to be able to perform these two calls
    without other commands getting in the way, we’d have issues. The following listing
    shows these same operations with a transaction.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 没有事务的情况下，三个线程中的每一个都能在减量操作到来之前增加`notrans:`计数器。我们在这里通过包含一个100毫秒的休眠来夸大潜在的问题，但如果我们需要在没有其他命令干扰的情况下执行这两个调用，我们就会遇到问题。下面的列表显示了使用事务执行这些相同操作的结果。
- en: Listing 3.14\. What can happen with transactions during parallel execution
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.14\. 在并行执行期间使用事务可能发生的情况
- en: '![](059fig02_alt.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![](059fig02_alt.jpg)'
- en: As you can see, by using a transaction, each thread is able to execute its entire
    sequence of commands without other threads interrupting it, despite the delay
    between the two calls. Again, this is because Redis waits to execute all of the
    provided commands between `MULTI` and `EXEC` until all of the commands have been
    received and followed by an `EXEC`.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，通过使用事务，每个线程都能够在其没有其他线程干扰的情况下执行其整个命令序列，尽管这两个调用之间存在延迟。再次强调，这是因为Redis会等待在`MULTI`和`EXEC`之间执行所有提供的命令，直到所有命令都已接收并跟随一个`EXEC`。
- en: There are both benefits and drawbacks to using transactions, which we’ll discuss
    further in [section 4.4](kindle_split_015.html#ch04lev1sec4).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 使用事务既有好处也有坏处，我们将在[第4.4节](kindle_split_015.html#ch04lev1sec4)中进一步讨论。
- en: '|  |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Removing of race conditions**'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：移除竞争条件**'
- en: 'One of the primary purposes of `MULTI`/`EXEC` transactions is removing what
    are known as *race conditions*, which you saw exposed in [listing 3.13](#ch03ex13).
    It turns out that the `article_vote()` function from [chapter 1](kindle_split_011.html#ch01)
    has a race condition and a second related bug. The race condition can cause a
    memory leak, and the bug can cause a vote to not be counted correctly. The chances
    of either of them happening is very small, but can you spot and fix them? Hint:
    If you’re having difficulty finding the memory leak, check out [section 6.2.5](kindle_split_017.html#ch06lev2sec7)
    while consulting the `post_article()` function.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '`MULTI`/`EXEC`事务的主要目的之一是移除所谓的*竞争条件*，这在[列表3.13](#ch03ex13)中已经暴露出来。结果是，[第1章](kindle_split_011.html#ch01)中的`article_vote()`函数有一个竞争条件和一个相关的第二个错误。竞争条件可能导致内存泄漏，而错误可能导致投票计数不正确。它们发生的可能性非常小，但你能否找到并修复它们？提示：如果你在寻找内存泄漏方面有困难，请在查阅`post_article()`函数时查看[第6.2.5节](kindle_split_017.html#ch06lev2sec7)。'
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Improving performance**'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：提高性能**'
- en: A secondary purpose of using pipelines in Redis is to improve performance (we’ll
    talk more about this in [sections 4.4](kindle_split_015.html#ch04lev1sec4)–[4.6](kindle_split_015.html#ch04lev1sec6)).
    In particular, by reducing the number of round trips between Redis and our client
    that occur over a sequence of commands, we can significantly reduce the amount
    of time our client is waiting for a response. In the `get_articles()` function
    we defined in [chapter 1](kindle_split_011.html#ch01), there will actually be
    26 round trips between Redis and the client to fetch a full page of articles.
    This is a waste. Can you change `get_articles()` so that it only makes two round
    trips?
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中使用管道的次要目的是提高性能（我们将在第[4.4](kindle_split_015.html#ch04lev1sec4)–[4.6](kindle_split_015.html#ch04lev1sec6)节中更多地讨论这一点）。特别是，通过减少在一系列命令中Redis和我们的客户端之间的往返次数，我们可以显著减少客户端等待响应的时间。在我们定义在[第1章](kindle_split_011.html#ch01)中的`get_articles()`函数中，实际上会有26次Redis和客户端之间的往返来获取一整页的文章。这是浪费。你能修改`get_articles()`使其只进行两次往返吗？
- en: '|  |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When writing data to Redis, sometimes the data is only going to be useful for
    a short period of time. We can manually delete this data after that time has elapsed,
    or we can have Redis automatically delete the data itself by using key expiration.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当向Redis写入数据时，有时数据只会在短时间内有用。我们可以在时间过去后手动删除这些数据，或者我们可以让Redis自动通过使用键过期来删除数据。
- en: 3.7.3\. Expiring keys
  id: totrans-263
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.3\. 过期键
- en: When writing data into Redis, there may be a point at which data is no longer
    needed. We can remove the data explicitly with `DEL`, or if we want to remove
    an entire key after a specified timeout, we can use what’s known as *expiration*.
    When we say that a key has a *time to live*, or that it’ll *expire* at a given
    time, we mean that Redis will automatically delete the key when its expiration
    time has arrived.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当将数据写入Redis时，可能会有数据不再需要的情况。我们可以使用`DEL`显式地删除数据，或者如果我们想在指定超时后删除整个键，我们可以使用所谓的*过期时间*。当我们说一个键有*生存时间*，或者它将在给定时间*过期*时，我们的意思是当它的过期时间到达时，Redis将自动删除该键。
- en: Having keys that will expire after a certain amount of time can be useful to
    handle the cleanup of cached data. If you look through other chapters, you won’t
    see the use of key expiration in Redis often (except in [sections 6.2](kindle_split_017.html#ch06lev1sec2),
    [7.1](kindle_split_018.html#ch07lev1sec1), and [7.2](kindle_split_018.html#ch07lev1sec2)).
    This is mostly due to the types of structures that are used; few of the commands
    we use offer the ability to set the expiration time of a key automatically. And
    with containers (`LIST`s, `SET`s, `HASH`es, and `ZSET`s), we can only expire entire
    keys, not individual items (this is also why we use `ZSET`s with timestamps in
    a few places).
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 拥有将在一定时间后过期的键可以用来处理缓存数据的清理。如果你查看其他章节，你不会经常看到Redis中键过期的使用（除了在第[6.2](kindle_split_017.html#ch06lev1sec2)、[7.1](kindle_split_018.html#ch07lev1sec1)和[7.2](kindle_split_018.html#ch07lev1sec2)节中）。这主要是因为使用的结构类型；我们使用的命令中很少有能够自动设置键过期时间的。而且对于容器（`LIST`s、`SET`s、`HASH`es和`ZSET`s），我们只能使整个键过期，而不能使单个项目过期（这也是为什么我们在一些地方使用带时间戳的`ZSET`s）。
- en: In this section, we’ll cover commands that are used to expire and delete keys
    from Redis automatically after a specified timeout, or at a specified time. After
    reading this section, you’ll be able to use expiration as a way of keeping Redis
    memory use low, and for cleaning up data you no longer need.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍用于在指定超时后或指定时间自动过期和删除Redis中键的命令。阅读本节后，你将能够使用过期时间来保持Redis内存使用率低，并清理不再需要的数据。
- en: '[Table 3.13](#ch03table13) shows the list of commands that we use to set and
    check the expiration times of keys in Redis.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '[表3.13](#ch03table13) 展示了我们用来设置和检查Redis中键的过期时间的命令列表。'
- en: Table 3.13\. Commands for handling expiration in Redis
  id: totrans-268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表3.13\. Redis中处理过期的命令
- en: '| Command | Example use and description |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 命令 | 示例用途和描述 |'
- en: '| --- | --- |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| --- | ---'
- en: '| PERSIST | PERSIST key-name—Removes the expiration from a key |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| PERSIST | PERSIST key-name—从键中移除过期时间 |'
- en: '| TTL | TTL key-name—Returns the amount of time remaining before a key will
    expire |'
  id: totrans-272
  prefs: []
  type: TYPE_TB
  zh: '| TTL | TTL key-name—返回键过期前剩余的时间 |'
- en: '| EXPIRE | EXPIRE key-name seconds—Sets the key to expire in the given number
    of seconds |'
  id: totrans-273
  prefs: []
  type: TYPE_TB
  zh: '| EXPIRE | EXPIRE key-name seconds—将键设置为在给定秒数后过期 |'
- en: '| EXPIREAT | EXPIREAT key-name timestamp—Sets the expiration time as the given
    Unix timestamp |'
  id: totrans-274
  prefs: []
  type: TYPE_TB
  zh: '| EXPIREAT | EXPIREAT key-name timestamp—将过期时间设置为给定的Unix时间戳 |'
- en: '| PTTL | PTTL key-name—Returns the number of milliseconds before the key will
    expire (available in Redis 2.6 and later) |'
  id: totrans-275
  prefs: []
  type: TYPE_TB
  zh: '| PTTL | PTTL key-name—返回键将在多少毫秒后过期（自 Redis 2.6 及以后版本可用）|'
- en: '| PEXPIRE | PEXPIRE key-name milliseconds—Sets the key to expire in the given
    number of milliseconds (available in Redis 2.6 and later) |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '| PEXPIRE | PEXPIRE key-name milliseconds—将键设置为在给定数量的毫秒后过期（自 Redis 2.6 及以后版本可用）|'
- en: '| PEXPIREAT | PEXPIREAT key-name timestamp-milliseconds—Sets the expiration
    time to be the given Unix timestamp specified in milliseconds (available in Redis
    2.6 and later) |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '| PEXPIREAT | PEXPIREAT key-name timestamp-milliseconds—将过期时间设置为给定的以毫秒为单位的
    Unix 时间戳（自 Redis 2.6 及以后版本可用）|'
- en: You can see a few examples of using expiration times on keys in the next listing.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在下一个列表中看到一些在键上使用过期时间的示例。
- en: Listing 3.15\. A sample interaction showing the use of expiration-related commands
    in Redis
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.15\. 一个示例交互，展示了在 Redis 中使用与过期相关的命令
- en: '![](061fig01_alt.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](061fig01_alt.jpg)'
- en: '|  |'
  id: totrans-281
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Replacing timestamp `ZSET`s with `EXPIRE`**'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：用 `EXPIRE` 替换时间戳 `ZSET`**'
- en: In [sections 2.1](kindle_split_012.html#ch02lev1sec1), [2.2](kindle_split_012.html#ch02lev1sec2),
    and [2.5](kindle_split_012.html#ch02lev1sec5), we used a `ZSET` with timestamps
    to keep a listing of session IDs to clean up. By using this `ZSET`, we could optionally
    perform analytics over our items when we cleaned sessions out. But if we aren’t
    interested in analytics, we can instead get similar semantics with expiration,
    without needing a cleanup function. Can you update the `update_token()` and `add_to_cart()`
    functions to expire keys instead of using a “recent” `ZSET` and cleanup function?
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [2.1节](kindle_split_012.html#ch02lev1sec1)、[2.2节](kindle_split_012.html#ch02lev1sec2)
    和 [2.5节](kindle_split_012.html#ch02lev1sec5) 中，我们使用带时间戳的 `ZSET` 来保存会话 ID 的列表以进行清理。通过使用这个
    `ZSET`，我们可以在清理会话时可选地对我们的事项进行数据分析。但如果我们对分析不感兴趣，我们可以通过过期来获得类似的语义，而不需要清理函数。您能否更新
    `update_token()` 和 `add_to_cart()` 函数，使键过期而不是使用“最近”的 `ZSET` 和清理函数？
- en: '|  |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.8\. Summary
  id: totrans-285
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8\. 摘要
- en: In this chapter, we’ve looked at commands that typically should cover at least
    95% of your command usage in Redis. We started with each of the different datatypes,
    and then discussed `PUBLISH` and `SUBSCRIBE`, followed by `SORT`, `MULTI`/`EXEC`
    transactions, and key expiration.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们查看了一些通常应该涵盖您在 Redis 中至少95%的命令使用情况的命令。我们从每种不同的数据类型开始，然后讨论了 `PUBLISH` 和
    `SUBSCRIBE`，接着是 `SORT`、`MULTI`/`EXEC` 事务和键过期。
- en: If there’s one thing that you should learn from this chapter, it’s that a wide
    variety of commands can be used to manipulate Redis structures in numerous ways.
    Although this chapter presents more than 70 of the most important commands, still
    more are listed and described at [http://redis.io/commands](http://redis.io/commands).
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您应该从本章中学到一件事，那就是可以使用各种命令以多种方式操作 Redis 结构。尽管本章介绍了70多个最重要的命令，但还有更多命令被列出并描述在
    [http://redis.io/commands](http://redis.io/commands)。
- en: If there’s a second thing you should take away from this chapter, it’s that
    I sometimes don’t offer the perfect answer to every problem. In revisiting a few
    of our examples from [chapters 1](kindle_split_011.html#ch01) and [2](kindle_split_012.html#ch02)
    in the exercises (whose answers you can see in the downloadable source code),
    I’m giving you an opportunity to try your hand at taking our already pretty-good
    answers, and making them better overall, or making them suit your problems better.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您应该从本章中得到的第二件事，那就是我有时并不能为每个问题提供完美的答案。在回顾一些我们在练习中来自 [第1章](kindle_split_011.html#ch01)
    和 [第2章](kindle_split_012.html#ch02) 的例子（其答案可在可下载的源代码中查看）时，我给了您一个机会来尝试改进我们已有的相当不错的答案，或者使它们更适合您的问题。
- en: One large group of commands that we didn’t cover in this chapter was configuration-related
    commands. In the next chapter, we get into configuring Redis to ensure your data
    stays healthy, and we give pointers on how to ensure that Redis performs well.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 本章未涵盖的一组大量命令是与配置相关的命令。在下一章中，我们将配置 Redis 以确保您的数据保持健康，并提供有关如何确保 Redis 性能良好的指南。
- en: Chapter 4\. Keeping data safe and ensuring performance
  id: totrans-290
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章\. 保持数据安全并确保性能
- en: '*This chapter covers*'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Persisting data to disk
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据持久化到磁盘
- en: Replicating data to other machines
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据复制到其他机器
- en: Dealing with system failures
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理系统故障
- en: Redis transactions
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis 事务
- en: Non-transactional pipelines
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非事务性管道
- en: Diagnosing performance issues
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 诊断性能问题
- en: In the last few chapters, you’ve learned about the variety of commands available
    in Redis and how they manipulate structures, and you’ve even solved a few problems
    using Redis. This chapter will prepare you for building real software with Redis
    by showing you how to keep your data safe, even in the face of system failure,
    and I’ll point out methods that you can use to improve Redis performance while
    preserving data integrity.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，您已经了解了Redis中可用的各种命令以及它们如何操作结构，您甚至使用Redis解决了一些问题。本章将通过向您展示如何在系统故障的情况下保持数据安全，为您准备使用Redis构建真实软件，同时我会指出您可以用来提高Redis性能并保持数据完整性的方法。
- en: We’ll start by exploring the various Redis persistence options available to
    you for getting your data on disk. We’ll then talk about the use of replication
    to keep up-to-date copies of your data on additional machines for both performance
    and data reliability. Combining replication and persistence, we’ll talk about
    trade-offs you may need to make, and we’ll walk through a few examples of choosing
    persistence and replication options to suit your needs. We’ll then talk about
    Redis transactions and pipelines, and we’ll finish out the chapter by discussing
    how to diagnose some performance issues.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先探索您可用于将数据存储在磁盘上的各种Redis持久化选项。然后，我们将讨论使用复制来在额外的机器上保持数据的最新副本，以实现性能和数据可靠性。结合复制和持久化，我们将讨论您可能需要做出的权衡，并演示一些选择持久化和复制选项以适应您需求的示例。然后，我们将讨论Redis事务和管道，并以讨论如何诊断一些性能问题结束本章。
- en: As we go through this chapter, our focus is understanding more about how Redis
    works so that we can first ensure that our data is correct, and then work toward
    making our operations on the data fast.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们阅读本章的过程中，我们的重点是了解Redis的工作原理，以便我们首先确保我们的数据是正确的，然后努力使我们的数据操作变得快速。
- en: To start, let’s examine how Redis stores our information on disk so that, after
    restart, it’s still there.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看Redis如何将我们的信息存储在磁盘上，以便在重启后它仍然存在。
- en: 4.1\. Persistence options
  id: totrans-302
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1. 持久化选项
- en: Within Redis, there are two different ways of persisting data to disk. One is
    a method called *snapshotting* that takes the data as it exists at one moment
    in time and writes it to disk. The other method is called *AOF*, or *append-only
    file*, and it works by copying incoming write commands to disk as they happen.
    These methods can be used together, separately, or not at all in some circumstances.
    Which to choose will depend on your data and your application.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中，有两种不同的方法将数据持久化到磁盘。一种被称为*快照*的方法，它将某一时刻存在的数据写入磁盘。另一种方法被称为*AOF*，或*追加文件*，它通过将发生的写入命令复制到磁盘来实现。在某些情况下，这些方法可以一起使用，单独使用，或者根本不使用。选择哪种方法将取决于您的数据和应用程序。
- en: One of the primary reasons why you’d want to store in-memory data on disk is
    so that you have it later, or so that you can back it up to a remote location
    in the case of failure. Additionally, the data that’s stored in Redis may have
    taken a long time to compute, or may be in the process of computation, and you
    may want to have access to it later without having to compute it again. For some
    Redis uses, “computation” may simply involve an act of copying data from another
    database into Redis (as was the case in [section 2.4](kindle_split_012.html#ch02lev1sec4)),
    but for others, Redis could be storing aggregate analytics data from billions
    of log lines.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 您想要将内存中的数据存储在磁盘上的一个主要原因是，以便以后使用，或者以防故障，您可以将它备份到远程位置。此外，存储在Redis中的数据可能已经花费了很长时间进行计算，或者可能正处于计算过程中，您可能希望在以后能够访问它而无需再次计算。对于某些Redis用途，“计算”可能只是将数据从另一个数据库复制到Redis的行为（如[2.4节](kindle_split_012.html#ch02lev1sec4)中所述），但对于其他用途，Redis可能存储来自数十亿日志行的聚合分析数据。
- en: Two different groups of configuration options control how Redis will write data
    to disk. All of these configuration options with example configuration values
    can be seen in the following listing. We’ll talk about them all more specifically
    in [sections 4.1.1](#ch04lev2sec1) and [4.1.2](#ch04lev2sec2), but for now, we’ll
    just look at the options so you can get familiar with them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 两种不同的配置选项组控制着Redis如何将数据写入磁盘。所有这些配置选项及其示例配置值可以在以下列表中看到。我们将在[4.1.1节](#ch04lev2sec1)和[4.1.2节](#ch04lev2sec2)中更具体地讨论它们，但就目前而言，我们将仅查看这些选项，以便您能熟悉它们。
- en: Listing 4.1\. Options for persistence configuration available in Redis
  id: totrans-306
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1. Redis中可用的持久化配置选项
- en: '![](064fig01_alt.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](064fig01_alt.jpg)'
- en: As you saw in [listing 4.1](#ch04ex01), the first few options deal with basic
    snapshotting, like what to name the snapshot on disk, how often to perform an
    automatic snapshot, whether to compress the snapshot, and whether to keep accepting
    writes on failure. The second group of options configure the AOF subsystem, telling
    Redis whether to use it, how often to sync writes to disk, whether to sync during
    AOF compaction, and how often to compact the AOF. In the next section, we’ll talk
    about using snapshots to keep our data safe.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[列表4.1](#ch04ex01)中看到的，前几个选项处理基本的快照功能，比如在磁盘上命名快照，多久执行一次自动快照，是否压缩快照，以及是否在失败时继续接受写入。第二组选项配置AOF子系统，告诉Redis是否使用它，多久同步一次写入到磁盘，是否在AOF压缩期间同步，以及多久压缩一次AOF。在下一节中，我们将讨论使用快照来保护我们的数据安全。
- en: 4.1.1\. Persisting to disk with snapshots
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 使用快照持久化到磁盘
- en: In Redis, we can create a point-in-time copy of in-memory data by creating a
    snapshot. After creation, these snapshots can be backed up, copied to other servers
    to create a clone of the server, or left for a future restart.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中，我们可以通过创建快照来创建内存数据的点时间副本。创建后，这些快照可以被备份，复制到其他服务器以创建服务器的克隆，或者保留以供将来重启使用。
- en: On the configuration side of things, snapshots are written to the file referenced
    as `dbfilename` in the configuration, and stored in the path referenced as `dir`.
    Until the next snapshot is performed, data written to Redis since the last snapshot
    started (and completed) would be lost if there were a crash caused by Redis, the
    system, or the hardware.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置的配置方面，快照被写入配置中引用的`dbfilename`文件，并存储在引用的`dir`路径中。在执行下一个快照之前，如果由于Redis、系统或硬件故障导致崩溃，自上次快照开始（并完成）以来写入Redis的数据将会丢失。
- en: As an example, say that we have Redis running with 10 gigabytes of data currently
    in memory. A previous snapshot had been started at 2:35 p.m. and had finished.
    Now a snapshot is started at 3:06 p.m., and 35 keys are updated before the snapshot
    completes at 3:08 p.m. If some part of the system were to crash and prevent Redis
    from completing its snapshot operation between 3:06 p.m. and 3:08 p.m., any data
    written between 2:35 p.m. and now would be lost. But if the system were to crash
    just *after* the snapshot had completed, then only the updates to those 35 keys
    would be lost.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个Redis实例正在运行，当前内存中有10GB的数据。之前的一个快照在下午2:35开始并完成。现在在下午3:06开始了一个快照，并在下午3:08快照完成之前更新了35个键。如果系统在下午3:06和下午3:08之间崩溃，阻止Redis完成快照操作，那么从下午2:35到现在写入的所有数据将会丢失。但如果系统在快照完成后立即崩溃，那么只会丢失那35个键的更新。
- en: 'There are five methods to initiate a snapshot, which are listed as follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 有五种方法可以启动快照，如下所示：
- en: Any Redis client can initiate a snapshot by calling the `BGSAVE` command. On
    platforms that support `BGSAVE` (basically all platforms except for Windows),
    Redis will *fork*,^([[1](#ch04fn01)]) and the child process will write the snapshot
    to disk while the parent process continues to respond to commands.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何Redis客户端都可以通过调用`BGSAVE`命令来启动快照。在支持`BGSAVE`的平台（基本上是除了Windows以外的所有平台）上，Redis将**分叉**，^([[1](#ch04fn01)]),
    子进程将快照写入磁盘，而父进程继续响应命令。
- en: ¹ When a process forks, the underlying operating system makes a copy of the
    process. On Unix and Unix-like systems, the copying process is optimized such
    that, initially, all memory is shared between the child and parent processes.
    When either the parent or child process writes to memory, that memory will stop
    being shared.
  id: totrans-315
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 当一个进程分叉时，底层操作系统会复制该进程。在Unix和Unix-like系统上，复制过程被优化，使得最初，所有内存都在父进程和子进程之间共享。当父进程或子进程写入内存时，该内存将停止共享。
- en: A Redis client can also initiate a snapshot by calling the `SAVE` command, which
    causes Redis to stop responding to any/all commands until the snapshot completes.
    This command isn’t commonly used, except in situations where we need our data
    on disk, and either we’re okay waiting for it to complete, or we don’t have enough
    memory for a `BGSAVE`.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Redis客户端也可以通过调用`SAVE`命令来启动快照，这将导致Redis停止响应任何/所有命令，直到快照完成。这个命令并不常用，除非我们需要磁盘上的数据，并且我们愿意等待其完成，或者我们没有足够的内存来执行`BGSAVE`。
- en: If Redis is configured with `save` lines, such as `save 60 10000`, Redis will
    automatically trigger a `BGSAVE` operation if 10,000 writes have occurred within
    60 seconds since the last successful save has started (using the configuration
    option described). When multiple `save` lines are present, any time one of the
    rules match, a `BGSAVE` is triggered.
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果 Redis 配置了 `save` 行，例如 `save 60 10000`，Redis 将在自上次成功保存开始后的 60 秒内发生 10,000
    次写入时自动触发一个 `BGSAVE` 操作（使用所描述的配置选项）。当存在多个 `save` 行时，只要其中一个规则匹配，就会触发一个 `BGSAVE`。
- en: When Redis receives a request to shut down by the `SHUTDOWN` command, or it
    receives a standard `TERM` signal, Redis will perform a `SAVE`, blocking clients
    from performing any further commands, and then shut down.
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当 Redis 通过 `SHUTDOWN` 命令收到关闭请求，或者它收到标准的 `TERM` 信号时，Redis 将执行 `SAVE`，阻止客户端执行任何进一步的命令，然后关闭。
- en: If a Redis server connects to another Redis server and issues the `SYNC` command
    to begin replication, the master Redis server will start a `BGSAVE` operation
    if one isn’t already executing or recently completed. See [section 4.2](#ch04lev1sec2)
    for more information about replication.
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个 Redis 服务器连接到另一个 Redis 服务器并发出 `SYNC` 命令以开始复制，如果主 Redis 服务器尚未执行或最近已完成 `BGSAVE`
    操作，它将启动一个 `BGSAVE` 操作。有关复制的更多信息，请参阅[第 4.2 节](#ch04lev1sec2)。
- en: When using only snapshots for saving data, you must remember that if a crash
    were to happen, you’d lose any data changed since the last snapshot. For some
    applications, this kind of loss isn’t acceptable, and you should look into using
    append-only file persistence, as described in [section 4.1.2](#ch04lev2sec2).
    But if your application can live with data loss, snapshots can be the right answer.
    Let’s look at a few scenarios and how you may want to configure Redis to get the
    snapshot persistence behavior you’re looking for.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 当仅使用快照来保存数据时，你必须记住，如果发生崩溃，你将丢失自上次快照以来更改的任何数据。对于某些应用程序，这种损失是不可接受的，你应该考虑使用描述在[第
    4.1.2 节](#ch04lev2sec2)中的仅追加文件持久性。但是，如果你的应用程序可以接受数据损失，快照可能是正确的答案。让我们看看一些场景以及你可能想要如何配置
    Redis 以获得所需的快照持久性行为。
- en: Development
  id: totrans-321
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 开发
- en: 'For my personal development server, I’m mostly concerned with minimizing the
    overhead of snapshots. To this end, and because I generally trust my hardware,
    I have a single rule: `save 900 1`. The `save` option tells Redis that it should
    perform a `BGSAVE` operation based on the subsequent two values. In this case,
    if at least one write has occurred in at least 900 seconds (15 minutes) since
    the last `BGSAVE`, Redis will automatically start a new `BGSAVE`.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的个人开发服务器，我主要关注最小化快照的开销。为此，并且因为我通常信任我的硬件，我只有一个规则：`save 900 1`。`save` 选项告诉
    Redis 应根据后续的两个值执行 `BGSAVE` 操作。在这种情况下，如果自上次 `BGSAVE` 以来至少 900 秒（15 分钟）内至少发生了一次写入，Redis
    将自动启动一个新的 `BGSAVE`。
- en: If you’re planning on using snapshots on a production server, and you’re going
    to be storing a lot of data, you’ll want to try to run a development server with
    the same or similar hardware, the same `save` options, a similar set of data,
    and a similar expected load. By setting up an environment equivalent to what you’ll
    be running in production, you can make sure that you’re not snapshotting too often
    (wasting resources) or too infrequently (leaving yourself open for data loss).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你计划在生产服务器上使用快照，并且你将存储大量数据，你将想要尝试使用相同或类似硬件、相同的 `save` 选项、类似的数据集和类似的预期负载运行一个开发服务器。通过设置一个与你在生产中运行的环境等效的环境，你可以确保你不会过于频繁地快照（浪费资源）或过于不频繁（使自己面临数据损失的风险）。
- en: Aggregating logs
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 日志聚合
- en: In the case of aggregating log files and analysis of page views, we really only
    need to ask ourselves how much time we’re willing to lose if something crashes
    between dumps. If we’re okay with losing up to an hour of work, then we can use
    `save 3600 1` (there are 3600 seconds in an hour). But how might we recover if
    we were processing logs?
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 在聚合日志文件和分析页面浏览量的情况下，我们实际上只需要问自己，如果两次转储之间发生崩溃，我们愿意丢失多少时间。如果我们对丢失最多一小时的工具有所容忍，那么我们可以使用
    `save 3600 1`（一小时有 3600 秒）。但是，如果我们正在处理日志，我们如何恢复呢？
- en: To recover from data loss, we need to know what we lost in the first place.
    To know what we lost, we need to keep a record of our progress while processing
    logs. Let’s imagine that we have a function that’s called when new logs are ready
    to be processed. This function is provided with a Redis connect, a path to where
    log files are stored, and a callback that will process individual lines in the
    log file. With our function, we can record which file we’re working on and the
    file position information as we’re processing. A log-processing function that
    records this information can be seen in the next listing.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从数据丢失中恢复，我们首先需要知道我们最初失去了什么。为了知道我们失去了什么，我们需要在处理日志时记录我们的进度。让我们想象一下，我们有一个函数，当新的日志准备好处理时会被调用。这个函数提供了一个Redis连接，一个指向存储日志文件的路径，以及一个回调，该回调将处理日志文件中的单个行。有了我们的函数，我们可以在处理过程中记录我们正在处理的文件以及文件位置信息。一个记录此信息的日志处理函数可以在下一列表中看到。
- en: Listing 4.2\. The `process_logs()` function that keeps progress information
    in Redis
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.2. 在Redis中记录进度信息的`process_logs()`函数
- en: '![](ch04ex02-0.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch04ex02-0.jpg)'
- en: '![](ch04ex02-1.jpg)'
  id: totrans-329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch04ex02-1.jpg)'
- en: By keeping a record of our progress in Redis, we can pick up with processing
    logs if at any point some part of the system crashes. And because we used `MULTI`/`EXEC`
    pipelines as introduced in [chapter 3](kindle_split_014.html#ch03), we ensure
    that the dump will only include processed log information when it also includes
    progress information.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在Redis中记录我们的进度，如果系统中的任何部分在任何时候崩溃，我们都可以继续处理日志。而且因为我们使用了在第3章中介绍过的`MULTI`/`EXEC`管道，我们确保转储将只包括包含进度信息的处理日志信息。
- en: Big data
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 大数据
- en: When the amount of data that we store in Redis tends to be under a few gigabytes,
    snapshotting can be the right answer. Redis will fork, save to disk, and finish
    the snapshot faster than you can read this sentence. But as our Redis memory use
    grows over time, so does the time to perform a fork operation for the `BGSAVE`.
    In situations where Redis is using tens of gigabytes of memory, there isn’t a
    lot of free memory, or if we’re running on a virtual machine, letting a `BGSAVE`
    occur may cause the system to pause for extended periods of time, or may cause
    heavy use of system virtual memory, which could degrade Redis’s performance to
    the point where it’s unusable.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们存储在Redis中的数据量趋于几GB时，快照可能是正确的答案。Redis将进行分叉，保存到磁盘，并且比您读完这句话的时间更快地完成快照。但随着我们的Redis内存使用随着时间的增长，执行`BGSAVE`的分叉操作所需的时间也会增加。在Redis使用数十GB内存、没有太多空闲内存或我们在虚拟机上运行的情况下，让`BGSAVE`发生可能会导致系统长时间暂停，或者可能导致系统虚拟内存的过度使用，这可能会降低Redis的性能到无法使用的程度。
- en: This extended pausing (and how significant it is) will depend on what kind of
    system we’re running on. Real hardware, VMWare virtualization, or KVM virtualization
    will generally allow us to create a fork of a Redis process at roughly 10–20ms
    per gigabyte of memory that Redis is using. If our system is running within Xen
    virtualization, those numbers can be closer to 200–300ms per gigabyte of memory
    used by Redis, depending on the Xen configuration. So if we’re using 20 gigabytes
    of memory with Redis, running `BGSAVE` on standard hardware will pause Redis for
    200–400 milliseconds for the fork. If we’re using Redis inside a Xen-virtualized
    machine (as is the case with Amazon EC2 and some other cloud providers), that
    same fork will cause Redis to pause for 4–6 seconds. You need to decide for your
    application whether this pause is okay.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这种延长的暂停（以及它有多重要）将取决于我们在什么类型的系统上运行。真实硬件、VMware虚拟化或KVM虚拟化通常允许我们以大约每GB内存10-20毫秒的速度创建Redis进程的分叉。如果我们的系统在Xen虚拟化中运行，这些数字可以接近每GB
    Redis使用的内存200-300毫秒，具体取决于Xen配置。所以如果我们使用20GB的Redis内存，在标准硬件上运行`BGSAVE`将使Redis暂停200-400毫秒进行分叉。如果我们使用Redis在Xen虚拟化的机器内部（如Amazon
    EC2和一些其他云服务提供商的情况），同样的分叉将使Redis暂停4-6秒。您需要决定对于您的应用程序，这种暂停是否可以接受。
- en: To prevent forking from causing such issues, we may want to disable automatic
    saving entirely. When automatic saving is disabled, we then need to manually call
    `BGSAVE` (which has all of the same potential issues as before, only now we know
    when they will happen), or we can call `SAVE`. With `SAVE`, Redis does block until
    the save is completed, but because there’s no fork, there’s no fork delay. And
    because Redis doesn’t have to fight with itself for resources, the snapshot will
    finish faster.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止派生引起此类问题，我们可能希望完全禁用自动保存。当自动保存被禁用时，我们需要手动调用`BGSAVE`（它具有与之前相同的潜在问题，但现在我们知道它们何时会发生），或者我们可以调用`SAVE`。使用`SAVE`，Redis会阻塞直到保存完成，但由于没有派生，所以没有派生延迟。并且由于Redis不需要与其他自身争夺资源，快照将更快完成。
- en: As a point of personal experience, I’ve run Redis servers that used 50 gigabytes
    of memory on machines with 68 gigabytes of memory inside a cloud provider running
    Xen virtualization. When trying to use `BGSAVE` with clients writing to Redis,
    forking would take 15 seconds or more, followed by 15–20 minutes for the snapshot
    to complete. But with `SAVE`, the snapshot would finish in 3–5 minutes. For our
    use, a daily snapshot at 3 a.m. was sufficient, so we wrote scripts that would
    stop clients from trying to access Redis, call `SAVE`, wait for the `SAVE` to
    finish, back up the resulting snapshot, and then signal to the clients that they
    could continue.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 作为个人经验，我运行过在运行Xen虚拟化的云提供商的68GB内存机器上使用50GB内存的Redis服务器。当尝试使用`BGSAVE`与向Redis写入客户端一起使用时，派生需要15秒或更长时间，然后是15-20分钟来完成快照。但使用`SAVE`，快照会在3-5分钟内完成。对于我们的使用，凌晨3点的每日快照就足够了，所以我们编写了脚本，阻止客户端尝试访问Redis，调用`SAVE`，等待`SAVE`完成，备份生成的快照，然后向客户端发出信号，告知他们可以继续。
- en: Snapshots are great when we can deal with potentially substantial data loss
    in Redis, but for many applications, 15 minutes or an hour or more of data loss
    or processing time is too much. To allow Redis to keep more up-to-date information
    about data in memory stored on disk, we can use append-only file persistence.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们能够处理Redis中可能的大量数据丢失时，快照是非常有用的，但对于许多应用程序来说，15分钟或更长时间的数据丢失或处理时间是不够的。为了使Redis能够保持关于存储在磁盘上的内存中数据的更及时信息，我们可以使用仅追加文件持久化。
- en: 4.1.2\. Append-only file persistence
  id: totrans-337
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 仅追加文件持久化
- en: In basic terms, append-only log files keep a record of data changes that occur
    by writing each change to the end of the file. In doing this, anyone could recover
    the entire dataset by replaying the append-only log from the beginning to the
    end. Redis has functionality that does this as well, and it’s enabled by setting
    the configuration option `appendonly yes`, as shown in [listing 4.1](#ch04ex01).
    [Table 4.1](#ch04table01) shows the `appendfsync` options and how they affect
    file-write syncing to disk.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，仅追加日志文件通过将每个更改写入文件的末尾来记录数据变化。通过这种方式，任何人都可以通过从开始到结束重放仅追加日志来恢复整个数据集。Redis也有这样的功能，并且可以通过设置配置选项`appendonly
    yes`来启用，如[列表4.1](#ch04ex01)所示。[表4.1](#ch04table01)显示了`appendfsync`选项及其对文件写入磁盘同步的影响。
- en: Table 4.1\. Sync options to use with `appendfsync`
  id: totrans-339
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.1\. 与`appendfsync`一起使用的同步选项
- en: '| Option | How often syncing will occur |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '| 选项 | 同步发生的频率 |'
- en: '| --- | --- |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| always | Every write command to Redis results in a write to disk. This slows
    Redis down substantially if used. |'
  id: totrans-342
  prefs: []
  type: TYPE_TB
  zh: '| always | 每个写入Redis的命令都会导致磁盘上的写入。如果使用，这会显著减慢Redis的速度。|'
- en: '| everysec | Once per second, explicitly syncs write commands to disk. |'
  id: totrans-343
  prefs: []
  type: TYPE_TB
  zh: '| everysec | 每秒一次，显式地将写命令同步到磁盘。|'
- en: '| no | Lets the operating system control syncing to disk. |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '| no | 允许操作系统控制同步到磁盘。|'
- en: '|  |'
  id: totrans-345
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: File syncing
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文件同步
- en: When writing files to disk, at least three things occur. The first is writing
    to a buffer, and this occurs when calling `file.write()` or its equivalent in
    other languages. When the data is in the buffer, the operating system can take
    that data and write it to disk at some point in the future. We can optionally
    take a second step and ask the operating system to write the data provided to
    disk when it next has a chance, with `file.flush()`, but this is only a request.
    Because data isn’t actually on disk until the operating system writes it to disk,
    we can tell the operating system to “sync” the files to disk, which will block
    until it’s completed. When that sync is completed, we can be fairly certain that
    our data is on disk and we can read it later if the system otherwise fails.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 当将文件写入磁盘时，至少会发生三件事情。第一是写入缓冲区，这发生在调用`file.write()`或其它语言中的等效函数时。当数据在缓冲区中时，操作系统可以在未来的某个时刻将数据写入磁盘。我们可以选择性地进行第二步，并请求操作系统在有机会时将提供的数据写入磁盘，使用`file.flush()`，但这只是一个请求。因为数据实际上只有在操作系统将其写入磁盘后才会存在于磁盘上，所以我们可以告诉操作系统“同步”文件到磁盘，这将阻塞直到完成。当同步完成时，我们可以相当确信数据已经写入磁盘，并且如果系统出现故障，我们可以在以后读取它。
- en: '|  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: If we were to set `appendfsync always`, every write to Redis would result in
    a write to disk, and we can ensure minimal data loss if Redis were to crash. Unfortunately,
    because we’re writing to disk with every write to Redis, we’re limited by disk
    performance, which is roughly 200 writes/second for a spinning disk, and maybe
    a few tens of thousands for an SSD (a solid-state drive).
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将`appendfsync always`设置为，每次写入Redis都会导致写入磁盘，并且如果Redis崩溃，我们可以确保最小化数据丢失。不幸的是，因为我们每次写入Redis都会写入磁盘，所以我们受限于磁盘性能，对于旋转磁盘大约是每秒200次写入，而对于SSD（固态驱动器）可能是几千次。
- en: '|  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Warning: SSDs and `appendfsync always`'
  id: totrans-351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告：SSD和`appendfsync always`
- en: You’ll want to be careful if you’re using SSDs with `appendfsync always`. Writing
    every change to disk as they happen, instead of letting the operating system group
    writes together as is the case with the other `appendfsync` options, has the potential
    to cause an extreme form of what is known as *write amplification*. By writing
    small amounts of data to the end of a file, you can reduce the lifetime of SSDs
    from years to just a few months in some cases.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用的是SSD与`appendfsync always`，你需要小心。将每次更改即时写入磁盘，而不是像其他`appendfsync`选项那样让操作系统将写入分组在一起，可能会引起一种称为*写入放大*的极端形式。通过将少量数据写入文件的末尾，你可以在某些情况下将SSD的使用寿命从几年缩短到仅仅几个月。
- en: '|  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As a reasonable compromise between keeping data safe and keeping our write performance
    high, we can also set `appendfsync everysec`. This configuration will sync the
    append-only log once every second. For most common uses, we’ll likely not find
    significant performance penalties for syncing to disk every second compared to
    not using any sort of persistence. By syncing to disk every second, if the system
    were to crash, we could lose at most one second of data that had been written
    or updated in Redis. Also, in the case where the disk is unable to keep up with
    the write volume that’s happening, Redis would gracefully slow down to accommodate
    the maximum write rate of the drive.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 作为在保证数据安全与保持高写入性能之间的一种合理折衷，我们也可以设置`appendfsync everysec`。这种配置将每秒同步一次只追加日志。对于大多数常见用途，我们可能不会发现与不使用任何持久化方式相比，每秒同步到磁盘会有显著的性能损失。通过每秒同步到磁盘，如果系统崩溃，我们最多可能丢失一秒钟内写入或更新的数据。此外，在磁盘无法跟上写入量时，Redis会优雅地降低速度以适应驱动器的最大写入速率。
- en: As you may guess, when setting `appendfsync no`, Redis doesn’t perform any explicit
    file syncing, leaving everything up to the operating system. There should be no
    performance penalties in this case, though if the system were to crash in one
    way or another, we’d lose an unknown and unpredictable amount of data. And if
    we’re using a hard drive that isn’t fast enough for our write load, Redis would
    perform fine until the buffers to write data to disk were filled, at which point
    Redis would get very slow as it got blocked from writing. For these reasons, I
    generally discourage the use of this configuration option, and include its description
    and semantics here for completeness.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，当设置`appendfsync no`时，Redis不会执行任何显式的文件同步，将一切留给操作系统。在这种情况下，不应该有任何性能损失，尽管如果系统以某种方式崩溃，我们可能会丢失未知且不可预测的数据量。如果我们使用的是无法满足我们写入负载的硬盘，Redis将表现良好，直到写入磁盘的数据缓冲区被填满，此时Redis会因为无法写入而变得非常慢。出于这些原因，我通常不建议使用此配置选项，并在此处包含其描述和语义，以保持完整性。
- en: Append-only files are flexible, offering a variety of options to ensure that
    almost every level of paranoia can be addressed. But there’s a dark side to AOF
    persistence, and that is file size.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 只追加文件具有灵活性，提供了各种选项以确保几乎可以解决每个级别的偏执。但AOF持久化有一个阴暗面，那就是文件大小。
- en: 4.1.3\. Rewriting/compacting append-only files
  id: totrans-357
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.3. 重写/压缩只追加文件
- en: 'After reading about AOF persistence, you’re probably wondering why snapshots
    exist at all. If by using append-only files we can minimize our data losses to
    one second (or essentially none at all), and minimize the time it takes to have
    data persisted to disk on a regular basis, it would seem that our choice should
    be clear. But the choice is actually not so simple: because every write to Redis
    causes a log of the command to be written to disk, the append-only log file will
    continuously grow. Over time, a growing AOF could cause your disk to run out of
    space, but more commonly, upon restart, Redis will be executing every command
    in the AOF in order. When handling large AOFs, Redis can take a very long time
    to start up.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了关于AOF持久化的内容后，你可能想知道为什么快照存在。如果我们通过使用只追加文件可以将数据损失最小化到一秒（或者实际上没有损失），并且最小化定期将数据持久化到磁盘所需的时间，那么我们的选择似乎应该是明确的。但实际上，选择并不那么简单：因为每次写入Redis都会导致将命令日志写入磁盘，只追加日志文件将不断增长。随着时间的推移，不断增长的AOF可能会导致你的磁盘空间不足，但更常见的情况是，在重启时，Redis将按顺序执行AOF中的每个命令。在处理大型AOF时，Redis启动可能需要非常长的时间。
- en: 'To solve the growing AOF problem, we can use `BGREWRITEAOF`, which will rewrite
    the AOF to be as short as possible by removing redundant commands. `BGREWRITEAOF`
    works similarly to the snapshotting `BGSAVE`: performing a fork and subsequently
    rewriting the append-only log in the child. As such, all of the same limitations
    with snapshotting performance regarding fork time, memory use, and so on still
    stand when using append-only files. But even worse, because AOFs can grow to be
    many times the size of a dump (if left uncontrolled), when the AOF is rewritten,
    the OS needs to delete the AOF, which can cause the system to hang for multiple
    seconds while it’s deleting an AOF of tens of gigabytes.'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决日益增长的AOF问题，我们可以使用`BGREWRITEAOF`，它通过删除冗余命令将AOF重写为尽可能短。`BGREWRITEAOF`的工作方式与快照的`BGSAVE`类似：执行一个fork操作，随后在子进程中重写只追加日志。因此，在使用只追加文件时，关于快照性能的所有相同限制，如fork时间、内存使用等，仍然存在。但更糟糕的是，因为AOF可以增长到比转储文件大许多倍（如果不受控制），当AOF被重写时，操作系统需要删除AOF，这可能导致系统在删除数十GB的AOF时挂起数秒。
- en: 'With snapshots, we could use the `save` configuration option to enable the
    automatic writing of snapshots using `BGSAVE`. Using AOFs, there are two configuration
    options that enable automatic `BGREWRITEAOF` execution: `auto-aof-rewrite-percentage`
    and `auto-aof-rewrite-min-size`. Using the example values of `auto-aof-rewrite-percentage`
    `100` and `auto-aof-rewrite-min-size 64mb`, when AOF is enabled, Redis will initiate
    a `BGREWRITEAOF` when the AOF is at least 100% larger than it was when Redis last
    finished rewriting the AOF, and when the AOF is at least 64 megabytes in size.
    As a point of configuration, if our AOF is rewriting too often, we can increase
    the `100` that represents 100% to something larger, though it will cause Redis
    to take longer to start up if it has been a while since a rewrite happened.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 使用快照时，我们可以使用`save`配置选项来启用使用`BGSAVE`自动写入快照。使用AOFs，有两个配置选项可以启用自动`BGREWRITEAOF`执行：`auto-aof-rewrite-percentage`和`auto-aof-rewrite-min-size`。使用`auto-aof-rewrite-percentage`的示例值`100`和`auto-aof-rewrite-min-size
    64mb`，当AOF启用时，Redis将在AOF至少比Redis上次完成AOF重写时大100%时，以及AOF大小至少为64兆字节时启动`BGREWRITEAOF`。作为一个配置点，如果我们的AOF重写过于频繁，我们可以将代表100%的`100`增加到更大的值，尽管这会导致Redis在重写发生一段时间后启动时间更长。
- en: Regardless of whether we choose append-only files or snapshots, having the data
    on disk is a great first step. But unless our data has been backed up somewhere
    else (preferably to multiple locations), we’re still leaving ourselves open to
    data loss. Whenever possible, I recommend backing up snapshots and newly rewritten
    append-only files to other servers.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们选择只读文件还是快照，将数据存储在磁盘上都是迈出的重要一步。但除非我们的数据已经在其他地方（最好是多个位置）进行了备份，否则我们仍然面临数据丢失的风险。在可能的情况下，我建议将快照和重新编写的只读文件备份到其他服务器。
- en: By using either append-only files or snapshots, we can keep our data between
    system reboots or crashes. As load increases, or requirements for data integrity
    become more stringent, we may need to look to replication to help us.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用只读文件或快照，我们可以在系统重启或崩溃之间保持数据。随着负载的增加或对数据完整性的要求变得更加严格，我们可能需要考虑复制来帮助我们。
- en: 4.2\. Replication
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2. 复制
- en: Over their years of scaling platforms for higher loads, engineers and administrators
    have added *replication* to their bag of tricks to help systems scale. Replication
    is a method by which other servers receive a continuously updated copy of the
    data as it’s being written, so that the replicas can service read queries. In
    the relational database world, it’s not uncommon for a single *master* database
    to send writes out to multiple *slaves*, with the slaves performing all of the
    read queries. Redis has adopted this method of replication as a way of helping
    to scale, and this section will discuss configuring replication in Redis, and
    how Redis operates during replication.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在多年的平台扩展经验中，工程师和管理员已经将*复制*添加到他们的工具箱中，以帮助系统扩展。复制是一种方法，其他服务器可以接收正在写入的数据的持续更新的副本，以便副本可以处理读取查询。在关系型数据库领域，单个*主*数据库向多个*从*数据库发送写入操作，从数据库执行所有读取查询的情况并不少见。Redis采用了这种复制方法作为帮助扩展的一种方式，本节将讨论配置Redis的复制，以及Redis在复制过程中的操作。
- en: Though Redis may be fast, there are situations where one Redis server running
    isn’t fast enough. In particular, operations over `SET`s and `ZSET`s can involve
    dozens of `SET`s/`ZSET`s over tens of thousands or even millions of items. When
    we start getting millions of items involved, set operations can take seconds to
    finish, instead of milliseconds or microseconds. But even if single commands can
    complete in 10 milliseconds, that still limits us to 100 commands/second from
    a single Redis instance.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Redis可能很快，但在某些情况下，单个Redis服务器运行可能不够快。特别是，对`SET`和`ZSET`的操作可能涉及数十个`SET`和`ZSET`，这些操作可能涉及数万个甚至数百万个项目。当我们开始处理数百万个项目时，集合操作可能需要几秒钟才能完成，而不是毫秒或微秒。但即使单个命令可以在10毫秒内完成，这也限制了单个Redis实例每秒只能执行100个命令。
- en: '|  |'
  id: totrans-366
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Example performance for `SUNIONSTORE`
  id: totrans-367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`SUNIONSTORE`的性能示例'
- en: As a point to consider for the performance to expect from Redis, on a 2.4 GHz
    Intel Core 2 Duo, Redis will take 7–8 milliseconds to perform a `SUNIONSTORE`
    of two 10,000-item `SET`s that produces a single 20,000 item `SET`.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 作为考虑Redis性能的一个点，在2.4 GHz Intel Core 2 Duo处理器上，Redis执行两个包含10,000个元素的`SET`的`SUNIONSTORE`操作，将产生一个包含20,000个元素的`SET`，这个过程需要7-8毫秒。
- en: '|  |'
  id: totrans-369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For situations where we need to scale out read queries, or where we may need
    to write temporary data (we’ll talk about some of those in [chapter 7](kindle_split_018.html#ch07)),
    we can set up additional slave Redis servers to keep copies of our dataset. After
    receiving an initial copy of the data from the master, slaves are kept up to date
    in real time as clients write data to the master. With a master/slave setup, instead
    of connecting to the master for reading data, clients will connect to one of the
    slaves to read their data (typically choosing them in a random fashion to try
    to balance the load).
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 对于需要扩展读查询的情况，或者可能需要写入临时数据（我们将在[第7章](kindle_split_018.html#ch07)中讨论一些此类情况），我们可以设置额外的从服务器Redis来保存我们数据集的副本。从服务器在接收到主服务器上的初始数据副本后，会实时更新，以保持与客户端写入主服务器数据的同步。在主/从服务器配置中，客户端将连接到从服务器之一来读取数据（通常随机选择以尝试平衡负载），而不是连接到主服务器读取数据。
- en: Let’s talk about configuring Redis for master/slave operation, and how Redis
    behaves during the entire process.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们讨论如何配置Redis以实现主/从操作，以及Redis在整个过程中的行为。
- en: 4.2.1\. Configuring Redis for replication
  id: totrans-372
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 配置Redis以实现复制
- en: As I mentioned in [section 4.1.1](#ch04lev2sec1), when a slave connects to the
    master, the master will start a `BGSAVE` operation. To configure replication on
    the master side of things, we only need to ensure that the path and filename listed
    under the `dir` and `dbfilename` configuration options shown in [listing 4.1](#ch04ex01)
    are to a path and file that are writable by the Redis process.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[4.1.1节](#ch04lev2sec1)中提到的，当从服务器连接到主服务器时，主服务器将启动`BGSAVE`操作。为了在主服务器端配置复制，我们只需要确保[列表4.1](#ch04ex01)中显示的`dir`和`dbfilename`配置选项下的路径和文件是Redis进程可写路径和文件。
- en: 'Though a variety of options control behavior of the slave itself, only one
    option is really necessary to enable slaving: `slaveof`. If we were to set `slaveof
    host port` in our configuration file, the Redis that’s started with that configuration
    will use the provided host and port as the master Redis server it should connect
    to. If we have an already running system, we can tell a Redis server to stop slaving,
    or even to slave to a new or different master. To connect to a new master, we
    can use the `SLAVEOF host port` command, or if we want to stop updating data from
    the master, we can use `SLAVEOF no one`.'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有许多选项控制从服务器本身的行为，但只有一个选项真正必要以启用从属：`slaveof`。如果我们要在配置文件中设置`slaveof host port`，那么使用该配置启动的Redis将使用提供的主机和端口作为它应该连接的主服务器Redis。如果我们有一个已经运行的系统，我们可以告诉Redis服务器停止从属，甚至将其从属到一个新的或不同的主服务器。要连接到新的主服务器，我们可以使用`SLAVEOF
    host port`命令，或者如果我们想停止从主服务器更新数据，我们可以使用`SLAVEOF no one`。
- en: There’s not a lot to configuring Redis for master/slave operation, but what’s
    interesting and useful to know is what happens to Redis when it becomes a master
    or slave.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 配置Redis以实现主/从操作并没有太多内容，但有趣且值得了解的是，当Redis成为主服务器或从服务器时会发生什么。
- en: 4.2.2\. Redis replication startup process
  id: totrans-376
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. Redis复制启动过程
- en: I briefly described what happens when a slave connects—that the master starts
    a snapshot and sends that to the slave—but that’s the simple version. [Table 4.2](#ch04table02)
    lists all of the operations that occur on both the master and slave when a slave
    connects to a master.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 我简要描述了当从服务器连接时发生的情况——主服务器开始创建快照并将其发送到从服务器，但这只是简单版本。[表4.2](#ch04table02)列出了当从服务器连接到主服务器时，主服务器和从服务器上发生的所有操作。
- en: Table 4.2\. What happens when a slave connects to a master
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.2\. 当从服务器连接到主服务器时发生的情况
- en: '| Step | Master operations | Slave operations |'
  id: totrans-379
  prefs: []
  type: TYPE_TB
  zh: '| 步骤 | 主服务器操作 | 从服务器操作 |'
- en: '| --- | --- | --- |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | (waiting for a command) | (Re-)connects to the master; issues the SYNC
    command |'
  id: totrans-381
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (等待命令) | (重新)连接到主服务器；发出SYNC命令 |'
- en: '| 2 | Starts BGSAVE operation; keeps a backlog of all write commands sent after
    BGSAVE | Serves old data (if any), or returns errors to commands (depending on
    configuration) |'
  id: totrans-382
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 开始BGSAVE操作；保留所有在BGSAVE之后发送的写命令的回压 | 提供旧数据（如果有），或根据配置返回错误 |'
- en: '| 3 | Finishes BGSAVE; starts sending the snapshot to the slave; continues
    holding a backlog of write commands | Discards all old data (if any); starts loading
    the dump as it’s received |'
  id: totrans-383
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 完成BGSAVE；开始向从服务器发送快照；继续保留写命令的回压 | 抛弃所有旧数据（如果有）；开始加载接收到的转储 |'
- en: '| 4 | Finishes sending the snapshot to the slave; starts sending the write
    command backlog to the slave | Finishes parsing the dump; starts responding to
    commands normally again |'
  id: totrans-384
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 完成将快照发送到从属节点；开始向从属节点发送写命令积压 | 完成解析转储；再次正常响应命令 |'
- en: '| 5 | Finishes sending the backlog; starts live streaming of write commands
    as they happen | Finishes executing backlog of write commands from the master;
    continues executing commands as they happen |'
  id: totrans-385
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 完成发送积压；开始实时流式传输发生的写命令 | 完成执行来自主节点的写命令积压；继续实时执行命令 |'
- en: With the method outlined in [table 4.2](#ch04table02), Redis manages to keep
    up with most loads during replication, except in cases where network bandwidth
    between the master and slave instances isn’t fast enough, or when the master doesn’t
    have enough memory to fork and keep a backlog of write commands. Though it isn’t
    necessary, it’s generally considered to be a good practice to have Redis masters
    only use about 50–65% of the memory in our system, leaving approximately 30–45%
    for spare memory during `BGSAVE` and command backlogs.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 使用[表4.2](#ch04table02)中概述的方法，Redis在复制过程中能够跟上大多数负载，除了在主从实例之间的网络带宽不够快，或者当主节点没有足够的内存来分叉并保持写命令积压的情况下。虽然这不是必需的，但通常认为将Redis主节点仅使用系统内存的约50-65%是一个好的做法，在`BGSAVE`和命令积压期间留出大约30-45%的备用内存。
- en: On the slave side of things, configuration is also simple. To configure the
    slave for master/slave replication, we can either set the configuration option
    `SLAVEOF host port`, or we can configure Redis during runtime with the `SLAVEOF`
    command. If we use the configuration option, Redis will initially load whatever
    snapshot/AOF is currently available (if any), and then connect to the master to
    start the replication process outlined in [table 4.2](#ch04table02). If we run
    the `SLAVEOF` command, Redis will immediately try to connect to the master, and
    upon success, will start the replication process outlined in [table 4.2](#ch04table02).
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在从属节点方面，配置也很简单。要配置从属节点进行主从复制，我们可以设置配置选项`SLAVEOF host port`，或者我们可以在运行时使用`SLAVEOF`命令配置Redis。如果我们使用配置选项，Redis将最初加载当前可用的任何快照/AOF（如果有），然后连接到主节点以启动[表4.2](#ch04table02)中概述的复制过程。如果我们运行`SLAVEOF`命令，Redis将立即尝试连接到主节点，如果成功，将启动[表4.2](#ch04table02)中概述的复制过程。
- en: '|  |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: During sync, the slave flushes all of its data
  id: totrans-389
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在同步过程中，从属节点刷新其所有数据
- en: 'Just to make sure that we’re all on the same page (some users forget this the
    first time they try using slaves): when a slave initially connects to a master,
    any data that had been in memory will be lost, to be replaced by the data coming
    from the master.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们都在同一页面上（有些用户在第一次尝试使用从属节点时可能会忘记这一点）：当从属节点最初连接到主节点时，之前存储在内存中的任何数据都将丢失，将被来自主节点的数据所取代。
- en: '|  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Warning: Redis doesn’t support master-master replication'
  id: totrans-393
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告：Redis不支持主-主复制
- en: When shown master/slave replication, some people get the mistaken idea that
    because we can set slaving options after startup using the `SLAVEOF` command,
    that means we can get what’s known as *multi-master replication* by setting two
    Redis instances as being `SLAVEOF` each other (some have even considered more
    than two in a loop). Unfortunately, *this does not work*. At best, our two Redis
    instances will use as much processor as they can, will be continually communicating
    back and forth, and depending on which server we connect and try to read/write
    data from/to, we may get inconsistent data or no data.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 当展示主从复制时，有些人会错误地认为，因为我们可以在启动后使用`SLAVEOF`命令设置从属节点选项，这意味着我们可以通过将两个Redis实例设置为相互作为`SLAVEOF`来获得所谓的*多主复制*。不幸的是，*这行不通*。最坏的情况是，我们的两个Redis实例将尽可能使用处理器，将不断进行双向通信，并且根据我们连接的服务器以及尝试从/向其读写数据，我们可能会得到不一致的数据或没有数据。
- en: '|  |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: When multiple slaves attempt to connect to Redis, one of two different scenarios
    can occur. [Table 4.3](#ch04table03) describes them.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 当多个从属节点尝试连接到Redis时，可能会出现两种不同的场景。[表4.3](#ch04table03)描述了它们。
- en: Table 4.3\. When a slave connects to an existing master, sometimes it can reuse
    an existing dump file.
  id: totrans-397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.3。当从属节点连接到现有的主节点时，有时它可以重用现有的转储文件。
- en: '| When additional slaves connect | Master operation |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '| 当附加从属节点连接 | 主节点操作 |'
- en: '| --- | --- |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Before step 3 in [table 4.2](#ch04table02) | All slaves will receive the
    same dump and same backlogged write commands. |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '| 在[表4.2](#ch04table02)的第3步之前 | 所有从属节点将接收到相同的转储和相同的积压写命令。 |'
- en: '| On or after step 3 in [table 4.2](#ch04table02) | While the master is finishing
    up the five steps for earlier slaves, a new sequence of steps 1-5 will start for
    the new slave(s). |'
  id: totrans-401
  prefs: []
  type: TYPE_TB
  zh: '| 在[表4.2](#ch04table02)的第3步或之后 | 当主节点完成早期从节点的五个步骤时，新的从节点将开始执行步骤1-5的新序列。 |'
- en: For the most part, Redis does its best to ensure that it doesn’t have to do
    more work than is necessary. In some cases, slaves may try to connect at inopportune
    times and cause the master to do more work. On the other hand, if multiple slaves
    connect at the same time, the outgoing bandwidth used to synchronize all of the
    slaves initially may cause other commands to have difficulty getting through,
    and could cause general network slowdowns for other devices on the same network.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，Redis会尽力确保它不需要做比必要更多的工作。在某些情况下，从节点可能会在不合适的时间尝试连接，导致主节点做更多的工作。另一方面，如果多个从节点同时连接，用于最初同步所有从节点的出带宽可能会导致其他命令难以通过，并可能造成同一网络上的其他设备的一般网络速度降低。
- en: 4.2.3\. Master/slave chains
  id: totrans-403
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 主/从链
- en: Some developers have found that when they need to replicate to more than a handful
    of slaves, some networks are unable to keep up—especially when replication is
    being performed over the internet or between data centers. Because there’s nothing
    particularly special about being a master or a slave in Redis, slaves can have
    their own slaves, resulting in master/slave chaining.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 一些开发者发现，当他们需要将数据复制到超过几个从节点时，某些网络无法跟上——尤其是在通过互联网或在数据中心之间进行复制时。由于在Redis中，主节点和从节点并没有什么特别之处，从节点可以有它们自己的从节点，从而形成主/从链。
- en: Operationally, the only difference in the replication process that occurs is
    that if a slave X has its own slave Y, when slave X hits step 4 from [table 4.2](#ch04table02),
    slave X will disconnect slave Y, causing Y to reconnect and resync.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 在操作上，复制过程中发生的唯一区别是，如果从节点X有一个自己的从节点Y，当从节点X从[表4.2](#ch04table02)的步骤4开始时，从节点X将断开与从节点Y的连接，导致Y重新连接并重新同步。
- en: When read load significantly outweighs write load, and when the number of reads
    pushes well beyond what a single Redis server can handle, it’s common to keep
    adding slaves to help deal with the load. As load continues to increase, we can
    run into situations where the single master can’t write to all of its slaves fast
    enough, or is overloaded with slaves reconnecting and resyncing. To alleviate
    such issues, we may want to set up a layer of intermediate Redis master/slave
    nodes that can help with replication duties similar to [figure 4.1](#ch04fig01).
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 当读负载远大于写负载，并且当读操作的数量远远超过单个Redis服务器可以处理的能力时，通常需要继续添加从节点来帮助处理负载。随着负载的持续增加，我们可能会遇到单个主节点无法足够快地将数据写入所有从节点，或者被过多的从节点重新连接和重新同步所超载的情况。为了缓解这些问题，我们可能需要设置一层中间Redis主/从节点，以帮助执行类似于[图4.1](#ch04fig01)的复制任务。
- en: Figure 4.1\. An example Redis master/slave replica tree with nine lowest-level
    slaves and three intermediate replication helper servers
  id: totrans-407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1\. 一个包含九个最低级从节点和三个中间复制辅助服务器的Redis主/从副本树示例
- en: '![](04fig01_alt.jpg)'
  id: totrans-408
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig01_alt.jpg)'
- en: Though the example shown in [figure 4.1](#ch04fig01) may not necessarily need
    to be in a tree structure, remembering and understanding that this is both possible
    and reasonable for Redis replication can help you later.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然[图4.1](#ch04fig01)中显示的示例可能不一定需要是树形结构，但记住和理解这对于Redis复制来说既是可能的也是合理的，这有助于你以后。
- en: Back in [section 4.1.2](#ch04lev2sec2), we talked about using append-only files
    with syncing to limit the opportunities for us to lose data. We could prevent
    data loss almost entirely (except for system or hard drive crashes) by syncing
    every write to disk, but then we end up limiting performance severely. If we tell
    Redis to sync every second, we’re able to get the performance we need, but we
    could lose up to a second of writes if bad things happen. But by combining replication
    and append-only files, we can ensure that data gets to disk on multiple machines.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4.1.2节](#ch04lev2sec2)中，我们讨论了使用带有同步的只写文件来限制我们丢失数据的机会。我们可以通过将每次写入同步到磁盘来几乎完全防止数据丢失（除了系统或硬盘故障），但这样会严重限制性能。如果我们告诉Redis每秒同步一次，我们就能获得所需的性能，但如果发生不好的事情，我们可能会丢失最多一秒的写入。但是，通过结合复制和只写文件，我们可以确保数据被发送到多台机器的磁盘上。
- en: 'In order to ensure that data gets to disk on multiple machines, we must obviously
    set up a master with slaves. By configuring our slaves (and optionally our master)
    with `appendonly yes` and `appendfsync everysec`, we now have a group of machines
    that will sync to disk every second. But that’s only the first part: we must wait
    for the write to reach the slave(s) and check to make sure that the data reached
    disk before we can continue.'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保数据在多台机器上到达磁盘，我们显然需要设置一个带有从节点的主节点。通过将我们的从节点（以及可选的主节点）配置为 `appendonly yes`
    和 `appendfsync everysec`，我们现在有一组每秒钟都会同步到磁盘的机器。但这只是第一部分：我们必须等待写入到达从节点并检查数据是否在我们可以继续之前已到达磁盘。
- en: 4.2.4\. Verifying disk writes
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 验证磁盘写入
- en: 'Verifying that the data we wrote to the master made it to the slave is easy:
    we merely need to write a unique dummy value after our important data, and then
    check for it on the slave. But verifying that the data made it to disk is more
    difficult. If we wait at least one second, we know that our data made it to disk.
    But if we’re careful, we may be able to wait less time by checking the output
    of `INFO` for the value of `aof_pending_bio_fsync`, which will be 0 if all data
    that the server knows about has been written to disk. To automate this check,
    we can use the function provided in the next listing, which we’d call after writing
    our data to the master by passing both the master and slave connections.'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 验证我们写入主节点的数据是否已到达从节点是很容易的：我们只需在我们的重要数据之后写入一个唯一的虚拟值，然后在从节点上检查它。但是，验证数据是否已到达磁盘则更为困难。如果我们至少等待一秒钟，我们就知道我们的数据已到达磁盘。但如果我们小心，我们可能通过检查
    `INFO` 输出的 `aof_pending_bio_fsync` 值来等待更少的时间，该值将在服务器所知道的所有数据都已写入磁盘时为 0。为了自动化此检查，我们可以使用下一列表中提供的函数，在将数据写入主节点后通过传递主节点和从节点连接来调用此函数。
- en: Listing 4.3\. The `wait_for_sync()` function
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. `wait_for_sync()` 函数
- en: '![](075fig01_alt.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![075fig01_alt.jpg](075fig01_alt.jpg)'
- en: '|  |'
  id: totrans-416
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Other information from the `INFO` command
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`INFO` 命令的其他信息'
- en: The `INFO` command can offer a wide range of information about the current status
    of a Redis server—memory used, the number of connected clients, the number of
    keys in each database, the number of commands executed since the last snapshot,
    and more. Generally speaking, `INFO` is a good source of information about the
    general state of our Redis servers, and many resources online can explain more.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '`INFO` 命令可以提供有关 Redis 服务器当前状态的广泛信息——使用的内存、连接的客户端数量、每个数据库中的键的数量、自上次快照以来执行的命令数量等。一般来说，`INFO`
    是我们 Redis 服务器一般状态的不错信息来源，许多在线资源可以解释更多。'
- en: '|  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: To ensure correct operation, this function will first verify that the slave
    is connected to the master. It’ll then poll the slave, looking for the value that
    it had added to the sync wait `ZSET`. After it has found that the value has made
    it to the slave, it’ll then check on the status of the Redis write buffer, waiting
    for it to either say that there are no pending syncs to disk (signaling that the
    change had made it to disk), or wait for up to one second. We wait for one second
    under the assumption that after one second, the data had been synced to disk,
    but there’s so much writing to Redis that we didn’t catch when the data had been
    synced. After verifying the write to disk, we then clean up after ourselves.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保正确操作，此函数将首先验证从节点是否连接到主节点。然后它将轮询从节点，寻找它添加到同步等待 `ZSET` 的值。在它发现该值已到达从节点后，它将检查
    Redis 写入缓冲区的状态，等待它要么表示没有挂起的同步到磁盘（表示更改已到达磁盘），要么等待最多一秒钟。我们等待一秒钟是基于假设，在一秒钟后，数据已同步到磁盘，但由于我们对数据同步时发生的写入没有捕捉到，所以有大量的写入到
    Redis。在验证写入磁盘后，我们然后清理我们的工作。
- en: By combining replication and append-only files, we can configure Redis to be
    resilient against system failures.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合复制和只追加文件，我们可以配置 Redis 以抵御系统故障。
- en: 4.3\. Handling system failures
  id: totrans-422
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3\. 处理系统故障
- en: In order to be able to handle system failures in Redis, we need to prepare ourselves
    for the failure. The reason we’ve spent so much time talking about these topics
    is because if we’re going to rely on Redis as the sole data store for our application,
    then we must ensure that we never lose any data. Unlike a traditional relational
    database that offers ACID^([[2](#ch04fn02)]) guarantees, when choosing to architect
    on top of a Redis back end, we need to do a little extra work to ensure data consistency.
    Redis is software, and it runs on hardware, and even if both were designed perfectly
    and couldn’t fail, power can fail, generators can run out of fuel, and batteries
    can run out of power. In looking at what Redis offers, we spent a lot of time
    preparing for potential system failures. This section will talk about what we
    can do when failure does happen.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 为了能够处理 Redis 中的系统故障，我们需要为故障做好准备。我们之所以花费这么多时间讨论这些主题，是因为如果我们打算将 Redis 作为我们应用程序的唯一数据存储，那么我们必须确保我们永远不会丢失任何数据。与传统提供
    ACID^([[2](#ch04fn02)]) 保证的数据库不同，当我们选择在 Redis 后端之上进行架构时，我们需要做一些额外的工作来确保数据一致性。Redis
    是软件，它运行在硬件上，即使两者都设计得完美无缺且不会失败，电力也可能中断，发电机可能会耗尽燃料，电池可能会耗尽电量。在查看 Redis 提供的功能时，我们花费了大量时间来准备潜在的系统故障。本节将讨论当故障发生时我们可以做什么。
- en: ² ACID—or atomicity, consistency, isolation, and durability—is a functional
    description of what a database must guarantee to offer reliable transactions over
    data.
  id: totrans-424
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² ACID——或原子性、一致性、隔离性和持久性——是对数据库必须保证提供可靠事务的函数描述。
- en: 4.3.1\. Verifying snapshots and append-only files
  id: totrans-425
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1\. 验证快照和仅追加文件
- en: 'When confronted with system failures, we have tools to help us recover when
    either snapshotting or append-only file logging had been enabled. Redis includes
    two command-line applications for testing the status of a snapshot and an append-only
    file. These commands are `redis-check-aof` and `redis-check-dump`. If we run either
    command without arguments, we’ll see the basic help that’s provided:'
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 面对系统故障时，我们拥有工具来帮助我们恢复，无论是启用快照功能还是仅追加文件日志。Redis 包含两个命令行应用程序，用于测试快照和仅追加文件的状态。这些命令是
    `redis-check-aof` 和 `redis-check-dump`。如果我们不带参数运行这些命令，我们会看到提供的基本帮助信息：
- en: '[PRE0]'
  id: totrans-427
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'If we provide `--fix` as an argument to `redis-check-aof`, the command will
    fix the file. Its method to fix an append-only file is simple: it scans through
    the provided AOF, looking for an incomplete or incorrect command. Upon finding
    the first bad command, it trims the file to just before that command would’ve
    been executed. For most situations, this will discard the last partial write command.'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将 `--fix` 作为参数传递给 `redis-check-aof`，该命令将修复文件。它修复仅追加文件的方法很简单：它扫描提供的 AOF，寻找不完整或不正确的命令。一旦找到第一个错误的命令，它就会将文件修剪到该命令应该被执行之前。在大多数情况下，这将丢弃最后的部分写入命令。
- en: Unfortunately, there’s no currently supported method of repairing a corrupted
    snapshot. Though there’s the potential to discover where the first error had occurred,
    because the snapshot itself is compressed, an error partway through the dump has
    the potential to make the remaining parts of the snapshot unreadable. It’s for
    these reasons that I’d generally recommend keeping multiple backups of important
    snapshots, and calculating the SHA1 or SHA256 hashes to verify content during
    restoration. (Modern Linux and Unix platforms will have available `sha1sum` and
    `sha256sum` command-line applications for generating and verifying these hashes.)
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，目前没有支持修复损坏快照的方法。尽管有可能发现第一个错误发生的位置，但由于快照本身是压缩的，如果在转储过程中出现错误，可能会使快照的剩余部分变得不可读。正因为如此，我通常建议保留多个重要快照的备份，并在恢复过程中计算
    SHA1 或 SHA256 哈希值以验证内容。（现代 Linux 和 Unix 平台将提供 `sha1sum` 和 `sha256sum` 命令行应用程序，用于生成和验证这些哈希值。）
- en: '|  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Checksums and hashes
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 校验和与哈希
- en: Redis versions including 2.6 and later include a CRC64 checksum of the snapshot
    as part of the snapshot. The use of a CRC-family checksum is useful to discover
    errors that are typical in some types of network transfers or disk corruption.
    The SHA family of cryptographic hashes is much better suited for discovering arbitrary
    errors. To the point, if we calculated the CRC64 of a file, then flipped any number
    of bits inside the file, we could later flip a subset of the last 64 bits of the
    file to produce the original checksum. There’s no currently known method for doing
    the same thing with SHA1 or SHA256.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 包括 2.6 版本及以后的 Redis 版本，将快照的 CRC64 校验和作为快照的一部分。使用 CRC 家族校验和对于发现某些类型网络传输或磁盘损坏中典型的错误很有用。加密散列的
    SHA 家族更适合发现任意错误。具体来说，如果我们计算了一个文件的 CRC64，然后翻转文件内部的任意位，我们可以在以后翻转文件最后 64 位的一个子集来产生原始校验和。目前还没有已知的方法可以用
    SHA1 或 SHA256 做同样的事情。
- en: '|  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: After we’ve verified that our backups are what we had saved before, and we’ve
    corrected the last write to AOF as necessary, we may need to replace a Redis server.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们验证我们的备份是我们之前保存的内容，并且我们已经根据需要更正了 AOF 的最后写入后，我们可能需要替换一个 Redis 服务器。
- en: 4.3.2\. Replacing a failed master
  id: totrans-435
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2\. 替换失败的 master
- en: When we’re running a group of Redis servers with replication and persistence,
    there may come a time when some part of our infrastructure stops working for one
    reason or another. Maybe we get a bad hard drive, maybe bad memory, or maybe the
    power just went out. Regardless of what causes the system to fail, we’ll eventually
    need to replace a Redis server. Let’s look at an example scenario involving a
    master, a slave, and needing to replace the master.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行一组具有复制和持久性的 Redis 服务器时，可能会有某个时候，我们的基础设施的某个部分因为某种原因停止工作。也许我们得到了一个坏硬盘，也许内存有问题，或者电力突然中断。无论是什么原因导致系统失败，我们最终都需要替换一个
    Redis 服务器。让我们看看一个涉及 master、从服务器和需要替换 master 的示例场景。
- en: Machine A is running a copy of Redis that’s acting as the master, and machine
    B is running a copy of Redis that’s acting as the slave. Unfortunately, machine
    A has just lost network connectivity for some reason that we haven’t yet been
    able to diagnose. But we have machine C with Redis installed that we’d like to
    use as the new master.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 机器 A 正在运行一个充当 master 的 Redis 复制，机器 B 正在运行一个充当从服务器的 Redis 复制。不幸的是，机器 A 由于我们尚未诊断出的某种原因刚刚失去了网络连接。但我们有一个安装了
    Redis 的机器 C，我们希望将其用作新的 master。
- en: 'Our plan is simple: We’ll tell machine B to produce a fresh snapshot with `SAVE`.
    We’ll then copy that snapshot over to machine C. After the snapshot has been copied
    into the proper path, we’ll start Redis on machine C. Finally, we’ll tell machine
    B to become a slave of machine C.^([[3](#ch04fn03)]) Some example commands to
    make this possible on this hypothetical set of systems are shown in the following
    listing.'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的计划很简单：我们将告诉机器 B 使用 `SAVE` 生成一个新的快照。然后我们将该快照复制到机器 C。在快照被复制到正确的路径后，我们将在机器 C
    上启动 Redis。最后，我们将告诉机器 B 成为机器 C 的从服务器。[^[[3](#ch04fn03)]] 在以下列表中展示了使这成为可能的一些示例命令。
- en: ³ Because B was originally a slave, our clients shouldn’t have been writing
    to B, so we won’t have any race conditions with clients writing to B after the
    snapshot operation was started.
  id: totrans-439
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 因为 B 最初是奴隶，我们的客户不应该向 B 写信，所以在快照操作开始后，我们不会与向 B 写信的客户有任何竞态条件。
- en: Listing 4.4\. An example sequence of commands for replacing a failed master
    node
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.4\. 替换失败的 master 节点的命令示例
- en: '![](077fig01_alt.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![077fig01_alt.jpg]'
- en: Most of these commands should be familiar to those who have experience using
    and maintaining Unix or Linux systems. The only interesting things in the commands
    being run here are that we can initiate a `SAVE` on machine B by running a command,
    and we later set up machine B to be a slave of machine C by running a command.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些命令对于那些有使用和维护 Unix 或 Linux 系统经验的人来说应该是熟悉的。这里运行的命令中唯一有趣的事情是我们可以通过运行一个命令在机器
    B 上启动 `SAVE`，我们后来通过运行一个命令设置机器 B 成为机器 C 的从服务器。
- en: As an alternative to creating a new master, we may want to turn the slave into
    a master and create a new slave. Either way, Redis will be able to pick up where
    it left off, and our only job from then on is to update our client configuration
    to read and write to the proper servers, and optionally update the on-disk server
    configuration if we need to restart Redis.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 作为创建新 master 的替代方案，我们可能希望将从服务器转换为 master 并创建一个新的从服务器。无论哪种方式，Redis 都能从上次停止的地方继续，我们此后唯一的任务就是更新我们的客户端配置以读取和写入正确的服务器，并且如果需要重启
    Redis，还可以选择更新磁盘上的服务器配置。
- en: '|  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Redis Sentinel
  id: totrans-445
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Redis Sentinel
- en: A relatively recent addition to the collection of tools available with Redis
    is *Redis Sentinel*. By the final publishing of this manuscript, Redis Sentinel
    should be complete. Generally, Redis Sentinel pays attention to Redis masters
    and the slaves of the masters and automatically handles failover if the master
    goes down. We’ll discuss Redis Sentinel in [chapter 10](kindle_split_022.html#ch10).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 可用工具集合中相对较新的补充是 *Redis Sentinel*。到这份手稿最终出版时，Redis Sentinel 应该已经完成。通常，Redis
    Sentinel 会关注 Redis 主节点及其从节点，并在主节点故障时自动处理故障转移。我们将在第 [10 章](kindle_split_022.html#ch10)
    中讨论 Redis Sentinel。
- en: '|  |'
  id: totrans-447
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the next section, we’ll talk about keeping our data from being corrupted
    by multiple writers working on the same data, which is a necessary step toward
    keeping our data safe.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将讨论如何防止多个作者在处理相同数据时导致数据损坏，这是确保数据安全所必需的步骤。
- en: 4.4\. Redis transactions
  id: totrans-449
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. Redis 事务
- en: Part of keeping our data correct is understanding that when other clients are
    working on the same data, if we aren’t careful, we may end up with data corruption.
    In this section, we’ll talk about using Redis transactions to prevent data corruption
    and, in some cases, to improve performance.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 保持数据正确的一部分是理解，当其他客户端正在处理相同的数据时，如果我们不小心，我们可能会遇到数据损坏。在本节中，我们将讨论使用 Redis 事务来防止数据损坏，在某些情况下，还可以提高性能。
- en: Transactions in Redis are different from transactions that exist in more traditional
    relational databases. In a relational database, we can tell the database server
    `BEGIN`, at which point we can perform a variety of read and write operations
    that will be consistent with respect to each other, after which we can run either
    `COMMIT` to make our changes permanent or `ROLLBACK` to discard our changes.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: Redis 中的事务与更传统的关系型数据库中的事务不同。在关系型数据库中，我们可以告诉数据库服务器 `BEGIN`，此时我们可以执行各种读和写操作，这些操作将相互一致，之后我们可以运行
    `COMMIT` 使我们的更改永久化，或者运行 `ROLLBACK` 来丢弃我们的更改。
- en: Within Redis, there’s a simple method for handling a sequence of reads and writes
    that will be consistent with each other. We begin our transaction by calling the
    special command `MULTI`, passing our series of commands, followed by `EXEC` (as
    introduced in [section 3.7.2](kindle_split_014.html#ch03lev2sec2)). The problem
    is that this simple transaction doesn’t actually do anything until `EXEC` is called,
    which means that we can’t use data we read to make decisions until after we may
    have needed it. This may not seem important, but there’s a class of problems that
    become difficult to solve because of not being able to read the data in a consistent
    fashion, or allow for transactions to fail where they should succeed (as is the
    case when we have multiple simultaneous transactions against a single object when
    using two-phase commit, a common solution to the problem). One of these problems
    is the process of purchasing an item from a marketplace. Let’s see an example
    of this in action.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Redis 中，有一种简单的方法可以处理一系列读和写操作，这些操作将相互一致。我们通过调用特殊命令 `MULTI` 开始事务，传递我们的命令序列，然后是
    `EXEC`（如第 [3.7.2 节](kindle_split_014.html#ch03lev2sec2) 中所述）。问题是这个简单的交易实际上在调用
    `EXEC` 之前并不做任何事情，这意味着我们可能需要在之后才能使用我们读取的数据来做决定。这看起来可能并不重要，但有一些问题由于无法以一致的方式读取数据或允许事务在应该成功的地方失败（例如，当我们使用两阶段提交时，这是解决该问题的常见方法）而变得难以解决。这些问题之一是从市场中购买商品的过程。让我们看看这个过程的例子。
- en: '|  |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Delayed execution with `MULTI/EXEC` can improve performance
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 `MULTI/EXEC` 进行延迟执行可以提高性能
- en: Because of Redis’s delaying execution of commands until `EXEC` is called when
    using `MULTI`/`EXEC`, many clients (including the Python client that we’re using)
    will hold off on even sending commands until all of them are known. When all of
    the commands are known, the client will send `MULTI`, followed by the series of
    commands to be executed, and `EXEC`, all at the same time. The client will then
    wait until all of the replies from all of the commands are received. This method
    of sending multiple commands at once and waiting for all of the replies is generally
    referred to as *pipelining*, and has the ability to improve Redis’s performance
    when executing multiple commands by reducing the number of network round trips
    that a client needs to wait for.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Redis在`MULTI`/`EXEC`中使用时，直到调用`EXEC`才延迟执行命令，因此许多客户端（包括我们使用的Python客户端）甚至会在所有命令都已知之前推迟发送命令。当所有命令都已知时，客户端将同时发送`MULTI`，然后是执行的一系列命令，最后是`EXEC`。然后客户端将等待直到收到所有命令的回复。这种方法通常被称为*管道化*，它能够通过减少客户端需要等待的网络往返次数来提高Redis执行多个命令时的性能。
- en: '|  |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In the last few months, Fake Game Company has seen major growth in their web-based
    RPG that’s played on YouTwitFace, a fictional social network. Because it pays
    attention to the needs and desires of its community, it has determined that the
    players need the ability to buy and sell items in a marketplace. It’s our job
    to design and build a marketplace that can scale to the needs of the community.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去几个月里，假游戏公司在YouTwitFace（一个虚构的社会网络）上玩的游戏RPG中看到了重大增长。因为它关注其社区的需求和愿望，它已经确定玩家需要在市场上买卖物品。我们的任务是设计和构建一个能够扩展到社区需求的市场。
- en: 4.4.1\. Defining users and their inventory
  id: totrans-458
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1. 定义用户及其库存
- en: We’ll start by showing some structures that define our users and their inventory.
    User information is stored as a `HASH`, with keys and values that store user attributes
    like name, funds, and anything else. A user’s inventory will be a `SET` that holds
    unique identifiers for each item, which can be seen in [figure 4.2](#ch04fig02).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从展示一些定义我们的用户及其库存的结构开始。用户信息以`HASH`存储，键和值存储用户属性，如姓名、资金等。用户的库存将是一个`SET`，它包含每个物品的唯一标识符，如图4.2所示。
- en: Figure 4.2\. Example user inventory and user information. Frank has 43 e-dollars
    and an item that he’s considering selling from his inventory.
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2. 示例用户库存和用户信息。Frank有43个电子美元，并且他正在考虑从他的库存中出售一个物品。
- en: '![](04fig02_alt.jpg)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig02_alt.jpg)'
- en: 'Our requirements for the market are simple: a user can list an item for a given
    price, and when another user purchases the item, the seller receives the money.
    We’ll also say that the part of the market we’ll be worrying about only needs
    to be ordered by selling price. In [chapter 7](kindle_split_018.html#ch07), we’ll
    cover some topics for handling other orders.'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对市场的要求很简单：用户可以以特定价格列出物品，当另一个用户购买该物品时，卖家收到钱。我们还将说，我们将关注的市场的部分只需要按售价排序。在[第7章](kindle_split_018.html#ch07)中，我们将介绍一些处理其他排序的主题。
- en: To include enough information to sell a given item in the market, we’ll concatenate
    the item ID for the item with the user ID of the seller and use that as a member
    of a market `ZSET`, with the score being the item’s selling price. By including
    all of this information together, we greatly simplify our data structures and
    what we need to look up, and get the benefit of being able to easily paginate
    through a presorted market. A small version of the marketplace is shown in [figure
    4.3](#ch04fig03).
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在市场上销售特定物品，我们将物品ID与卖家用户ID连接起来，并将其作为市场`ZSET`的一个成员使用，分数为物品的售价。通过将所有这些信息一起包含，我们大大简化了我们的数据结构和需要查找的内容，并获得了能够轻松分页浏览预排序市场的优势。市场的小版本如图4.3所示。
- en: Figure 4.3\. Our basic marketplace that includes an ItemA being sold by user
    4 for 35 e-dollars
  id: totrans-464
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3. 我们的基本市场，其中包括用户4以35个电子美元出售的物品A
- en: '![](04fig03.jpg)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![图片](04fig03.jpg)'
- en: Now that we know what structures our marketplace uses, let’s list items in the
    market.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道了我们的市场使用哪些结构，让我们在市场上列出物品。
- en: 4.4.2\. Listing items in the marketplace
  id: totrans-467
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2. 在市场上列出物品
- en: In the process of listing, we’ll use a Redis operation called `WATCH`, which
    we combine with `MULTI` and `EXEC`, and sometimes `UNWATCH` or `DISCARD`. When
    we’ve watched keys with `WATCH`, if at any time some other client replaces, updates,
    or deletes any keys that we’ve `WATCH`ed before we have performed the `EXEC` operation,
    our operations against Redis will fail with an error message when we try to `EXEC`
    (at which point we can retry or abort the operation). By using `WATCH`, `MULTI`/`EXEC`,
    and `UNWATCH`/`DISCARD`, we can ensure that the data that we’re working with doesn’t
    change while we’re doing something important, which protects us from data corruption.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出过程中，我们将使用Redis的一个操作`WATCH`，我们将它与`MULTI`和`EXEC`结合使用，有时还会使用`UNWATCH`或`DISCARD`。当我们用`WATCH`监视了键，如果在执行`EXEC`操作之前，有其他客户端替换、更新或删除了我们之前监视的任何键，那么当我们尝试`EXEC`时，我们的Redis操作将因错误信息而失败（此时我们可以重试或中止操作）。通过使用`WATCH`、`MULTI/EXEC`和`UNWATCH/DISCARD`，我们可以确保我们在做重要的事情时，所处理的数据不会发生变化，这可以保护我们免受数据损坏。
- en: '|  |'
  id: totrans-469
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: What is `DISCARD?`
  id: totrans-470
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: '`DISCARD`是什么意思？'
- en: In the same way that `UNWATCH` will let us reset our connection if sent after
    `WATCH` but before `MULTI`, `DISCARD` will also reset the connection if sent after
    `MULTI` but before `EXEC`. That is to say, if we’d `WATCH`ed a key or keys, fetched
    some data, and then started a transaction with `MULTI` followed by a group of
    commands, we could cancel the `WATCH` and clear out any queued commands with `DISCARD`.
    We don’t use `DISCARD` here, primarily because we know whether we want to perform
    a `MULTI`/`EXEC` or `UNWATCH`, so a `DISCARD` is unnecessary for our purposes.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 与`UNWATCH`在`WATCH`之后但在`MULTI`之前发送时可以让我们重置连接一样，`DISCARD`在`MULTI`之后但在`EXEC`之前发送时也会重置连接。也就是说，如果我们用`WATCH`监视了一个或多个键，获取了一些数据，然后使用`MULTI`启动了一个事务，随后是一组命令，我们可以通过`DISCARD`取消`WATCH`并清除任何排队的命令。我们在这里不使用`DISCARD`主要是因为我们知道是否要执行`MULTI/EXEC`或`UNWATCH`，所以对于我们的目的来说，`DISCARD`是不必要的。
- en: '|  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s go about listing an item in the marketplace. To do so, we add the item
    to the market `ZSET`, while `WATCH`ing the seller’s inventory to make sure that
    the item is still available to be sold. The function to list an item is shown
    here.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何在市场上列出商品。为此，我们将商品添加到市场`ZSET`中，同时监视卖家的库存以确保该商品仍然可供出售。列出商品的函数如下所示。
- en: Listing 4.5\. The `list_item()` function
  id: totrans-474
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5\. `list_item()`函数
- en: '![](080fig01_alt.jpg)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![](080fig01_alt.jpg)'
- en: After some initial setup, we’ll do what we described earlier. We’ll tell Redis
    that we want to watch the seller’s inventory, verify that the seller can still
    sell the item, and if so, add the item to the market and remove the item from
    their inventory. If there’s an update or change to the inventory while we’re looking
    at it, we’ll receive an error and retry, as is shown by the `while` loop outside
    of our actual operation.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些初始设置之后，我们将执行之前描述的操作。我们将告诉Redis我们想要监视卖家的库存，验证卖家是否仍然可以出售该商品，如果是这样，就将商品添加到市场并从他们的库存中移除。如果我们查看库存时库存有更新或变化，我们将收到错误并重试，正如我们实际操作之外的`while`循环所示。
- en: Let’s look at the sequence of operations that are performed when Frank (user
    17) wants to sell ItemM for 97 e-dollars in [figure 4.4](#ch04fig04).
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当Frank（用户17）想要以97个电子美元的价格出售ItemM时，在[图4.4](#ch04fig04)中执行的操作序列。
- en: Figure 4.4\. `list_item(conn, "ItemM", 17, 97)`
  id: totrans-478
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4\. `list_item(conn, "ItemM", 17, 97)`
- en: '![](04fig04_alt.jpg)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![](04fig04_alt.jpg)'
- en: Generally, listing an item should occur without any significant issue, since
    only the user should be selling their own items (which is enforced farther up
    the application stack). But as I mentioned before, if a user’s inventory were
    to change between the `WATCH` and `EXEC`, our attempt to list the item would fail,
    and we’d retry.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，列出商品应该不会出现任何重大问题，因为只有用户应该出售他们自己的商品（这在应用堆栈的更高层得到了强制执行）。但如我之前提到的，如果用户的库存在与`WATCH`和`EXEC`之间发生变化，我们尝试列出商品的努力将失败，我们将重试。
- en: Now that you know how to list an item, it’s time to purchase an item.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何列出商品，是时候购买商品了。
- en: 4.4.3\. Purchasing items
  id: totrans-482
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.3\. 购买商品
- en: To process the purchase of an item, we first `WATCH` the market and the user
    who’s buying the item. We then fetch the buyer’s total funds and the price of
    the item, and verify that the buyer has enough money. If they don’t have enough
    money, we cancel the transaction. If they do have enough money, we perform the
    transfer of money between the accounts, move the item into the buyer’s inventory,
    and remove the item from the market. On `WATCH` error, we retry for up to 10 seconds
    in total. We can see the function which handles the purchase of an item in the
    following listing.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理物品的购买，我们首先监视市场以及购买该物品的用户。然后我们获取买家的总资金和物品的价格，并验证买家是否有足够的钱。如果他们没有足够的钱，我们取消交易。如果他们有足够的钱，我们执行账户间的转账，将物品移入买家的库存，并从市场中移除该物品。在`WATCH`错误的情况下，我们将重试，总时间不超过
    10 秒。我们可以在以下列表中看到处理物品购买的功能。
- en: Listing 4.6\. The `purchase_item()` function
  id: totrans-484
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6。`purchase_item()` 函数
- en: '![](082fig01_alt.jpg)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![图片 8.2.1](082fig01_alt.jpg)'
- en: To purchase an item, we need to spend more time preparing the data, and we need
    to watch both the market and the buyer’s information. We watch the market to ensure
    that the item can still be bought (or that we can notice that it has already been
    bought), and we watch the buyer’s information to verify that they have enough
    money. When we’ve verified that the item is still there, and that the buyer has
    enough money, we go about actually moving the item into their inventory, as well
    as moving money from the buyer to the seller.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 要购买物品，我们需要花更多的时间准备数据，并且我们需要监视市场和买家的信息。我们监视市场以确保物品仍然可以购买（或者我们可以注意到它已经被购买），我们监视买家的信息以验证他们是否有足够的钱。当我们验证了物品仍然存在，并且买家有足够的钱时，我们就开始将物品移入他们的库存，并将钱从买家转移到卖家。
- en: After seeing the available items in the market, Bill (user 27) decides that
    he wants to buy ItemM from Frank through the marketplace. Let’s follow along to
    see how our data changes through [figures 4.5](#ch04fig05) and [4.6](#ch04fig06).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 在看到市场中的可用物品后，比尔（用户 27）决定他想通过市场从弗兰克那里购买 ItemM。让我们跟随这些图（[图 4.5](#ch04fig05) 和
    [4.6](#ch04fig06)）来了解我们的数据是如何变化的。
- en: Figure 4.5\. Before the item can be purchased, we must watch the market and
    the buyer’s information to verify that the item is still available, and that the
    buyer has enough money.
  id: totrans-488
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.5。在物品可以购买之前，我们必须监视市场以及买家的信息，以验证该物品是否仍然可用，以及买家是否有足够的钱。
- en: '![](04fig05_alt.jpg)'
  id: totrans-489
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.5](04fig05_alt.jpg)'
- en: Figure 4.6\. In order to complete the item purchase, we must actually transfer
    money from the buyer to the seller, and we must remove the item from the market
    while adding it to the buyer’s inventory.
  id: totrans-490
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.6。为了完成物品购买，我们必须实际上从买家向卖家转账，同时从市场中移除该物品，并将其添加到买家的库存中。
- en: '![](04fig06_alt.jpg)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![图片 4.6](04fig06_alt.jpg)'
- en: If either the market `ZSET` or Bill’s account information changes between our
    `WATCH` and our `EXEC`, the `purchase_item()` function will either retry or abort,
    based on how long it has been trying to purchase the item, as shown in [listing
    4.6](#ch04ex06).
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在“监视”（`WATCH`）和“执行”（`EXEC`）之间，市场中的`ZSET`或比尔的账户信息发生变化，`purchase_item()`函数将根据尝试购买该物品的时间长短来决定是重试还是中止，具体请参考[列表
    4.6](#ch04ex06)。
- en: '|  |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why doesn’t Redis implement typical locking?
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么 Redis 不实现典型的锁定机制？
- en: When accessing data for writing (`SELECT FOR UPDATE` in SQL), relational databases
    will place a lock on rows that are accessed until a transaction is completed with
    `COMMIT` or `ROLLBACK`. If any other client attempts to access data for writing
    on any of the same rows, that client will be blocked until the first transaction
    is completed. This form of locking works well in practice (essentially all relational
    databases implement it), though it can result in long wait times for clients waiting
    to acquire locks on a number of rows if the lock holder is slow.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 当访问数据以进行写入操作（SQL 中的 `SELECT FOR UPDATE`）时，关系型数据库将对访问的行进行锁定，直到事务通过 `COMMIT` 或
    `ROLLBACK` 完成。如果任何其他客户端尝试访问同一行的数据以进行写入操作，该客户端将被阻塞，直到第一个事务完成。这种锁定机制在实践中效果良好（实际上所有关系型数据库都实现了它），尽管如果锁定持有者操作缓慢，这可能会导致等待获取多个行锁的客户出现长时间的等待。
- en: Because there’s potential for long wait times, and because the design of Redis
    minimizes wait time for clients (except in the case of blocking `LIST` pops),
    Redis doesn’t lock data during `WATCH`. Instead, Redis will notify clients if
    someone else modified the data first, which is called *optimistic locking* (the
    actual locking that relational databases perform could be viewed as *pessimistic*).
    Optimistic locking also works well in practice because clients are never waiting
    on the first holder of the lock; instead they retry if some other client was faster.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 由于存在长时间等待的可能性，并且 Redis 的设计最小化了客户端的等待时间（除了阻塞 `LIST` 弹出之外），Redis 在 `WATCH` 期间不会锁定数据。相反，如果有人先修改了数据，Redis
    将通知客户端，这被称为*乐观锁*（关系型数据库实际执行的锁定可以被视为*悲观锁*）。乐观锁在实践中也表现良好，因为客户端永远不会等待锁的第一个持有者；相反，如果其他客户端更快，它们会重试。
- en: '|  |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In this section, we’ve discussed combining `WATCH`, `MULTI`, and `EXEC` to handle
    the manipulation of multiple types of data so that we can implement a marketplace.
    Given this functionality as a basis, it wouldn’t be out of the question to make
    our marketplace into an auction, add alternate sorting options, time out old items
    in the market, or even add higher-level searching and filtering based on techniques
    discussed in [chapter 7](kindle_split_018.html#ch07).
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何结合 `WATCH`、`MULTI` 和 `EXEC` 来处理多种类型数据的操作，以便我们可以实现一个市场。基于这个功能作为基础，将我们的市场转变为拍卖，添加其他排序选项，使市场中的旧项目超时，或者甚至添加基于第
    7 章中讨论的技术的高级搜索和过滤功能，这些都是可以想象的事情。
- en: As long as we consistently use transactions in Redis, we can keep our data from
    being corrupted while being operated on by multiple clients. Let’s look at how
    we can make our operations even faster when we don’t need to worry about other
    clients altering our data.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们在 Redis 中持续使用事务，我们就可以防止我们的数据在多个客户端操作时被损坏。让我们看看当我们不需要担心其他客户端更改我们的数据时，我们如何使我们的操作更快。
- en: 4.5\. Non-transactional pipelines
  id: totrans-500
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 非事务性管道
- en: When we first introduced `MULTI`/`EXEC` in [chapter 3](kindle_split_014.html#ch03),
    we talked about them as having a “transaction” property—everything between the
    `MULTI` and `EXEC` commands will execute without other clients being able to do
    anything. One benefit to using transactions is the underlying library’s use of
    a pipeline, which improves performance. This section will show how to use a pipeline
    without a transaction to further improve performance.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第 3 章中首次介绍 `MULTI`/`EXEC` 时，我们将其描述为具有“事务”属性——`MULTI` 和 `EXEC` 命令之间的所有内容都将执行，而其他客户端无法进行任何操作。使用事务的一个好处是底层库使用管道，这提高了性能。本节将展示如何在不使用事务的情况下使用管道来进一步提高性能。
- en: You’ll remember from [chapter 2](kindle_split_012.html#ch02) that some commands
    take multiple arguments for adding/updating—commands like `MGET`, `MSET`, `HMGET`,
    `HMSET`, `RPUSH`/`LPUSH`, `SADD`, `ZADD`, and others. Those commands exist to
    streamline calls to perform the same operation repeatedly. As you saw in [chapter
    2](kindle_split_012.html#ch02), this can result in significant performance improvements.
    Though not as drastic as these commands, the use of non-transactional pipelines
    offers many of the same performance advantages, and allows us to run a variety
    of commands at the same time.
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会记得，在第 2 章中，一些命令需要多个参数来进行添加/更新操作——例如 `MGET`、`MSET`、`HMGET`、`HMSET`、`RPUSH`/`LPUSH`、`SADD`、`ZADD`
    等命令。这些命令的存在是为了简化执行相同操作的多次调用。正如你在第 2 章中看到的，这可以带来显著的性能提升。尽管这些命令的效果不如这些命令那么显著，但使用非事务性管道也能提供许多相同的性能优势，并允许我们同时运行各种命令。
- en: 'In the case where we don’t need transactions, but where we still want to do
    a lot of work, we could still use `MULTI`/`EXEC` for their ability to send all
    of the commands at the same time to minimize round trips and latency. Unfortunately,
    `MULTI` and `EXEC` aren’t free, and can delay other important commands from executing.
    But we can gain all the benefits of pipelining without using `MULTI`/`EXEC`. When
    we used `MULTI`/`EXEC` in Python in [chapter 3](kindle_split_014.html#ch03) and
    in [section 4.4](#ch04lev1sec4), you may have noticed that we did the following:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们不需要事务，但仍然想要做大量工作的场合，我们仍然可以使用 `MULTI`/`EXEC` 来利用它们同时发送所有命令的能力，以最小化往返次数和延迟。不幸的是，`MULTI`
    和 `EXEC` 并非免费，并且可能会延迟其他重要命令的执行。但我们可以不使用 `MULTI`/`EXEC` 就获得管道的所有好处。当你我们在第 3 章和第
    4.4 节中使用 Python 的 `MULTI`/`EXEC` 时，你可能已经注意到我们做了以下操作：
- en: '[PRE1]'
  id: totrans-504
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: By passing `True` to the `pipeline()` method (or omitting it), we’re telling
    our client to wrap the sequence of commands that we’ll call with a `MULTI`/`EXEC`
    pair. If instead of passing `True` we were to pass `False`, we’d get an object
    that prepared and collected commands to execute similar to the transactional pipeline,
    only it wouldn’t be wrapped with `MULTI`/`EXEC`. For situations where we want
    to send more than one command to Redis, the result of one command doesn’t affect
    the input to another, and we don’t need them all to execute transactionally, passing
    `False` to the `pipeline()` method can further improve overall Redis performance.
    Let’s look at an example.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `True` 传递给 `pipeline()` 方法（或省略它），我们告诉我们的客户端将我们将要调用的命令序列用 `MULTI`/`EXEC`
    对包裹起来。如果我们传递 `False` 而不是 `True`，我们会得到一个准备并收集要执行的命令的对象，类似于事务性管道，但它不会被 `MULTI`/`EXEC`
    包裹。对于需要向 Redis 发送多个命令的情况，一个命令的结果不会影响另一个命令的输入，并且我们不需要它们都作为事务执行，将 `False` 传递给 `pipeline()`
    方法可以进一步提高 Redis 的整体性能。让我们看一个例子。
- en: Way back in [sections 2.1](kindle_split_012.html#ch02lev1sec1) and [2.5](kindle_split_012.html#ch02lev1sec5),
    we wrote and updated a function called `update_token()`, which kept a record of
    recent items viewed and recent pages viewed, and kept the user’s login cookie
    updated. The updated code from [section 2.5](kindle_split_012.html#ch02lev1sec5)
    is shown in [listing 4.7](#ch04ex07). Note how the function will make three or
    five calls to Redis for every call of the function. As written, that will result
    in three or five round trips between Redis and our client.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在很久以前，[第 2.1 节](kindle_split_012.html#ch02lev1sec1) 和 [第 2.5 节](kindle_split_012.html#ch02lev1sec5)
    中，我们编写并更新了一个名为 `update_token()` 的函数，该函数记录了最近查看的项目和最近查看的页面，并更新了用户的登录 Cookie。第 2.5
    节的更新代码显示在 [列表 4.7](#ch04ex07) 中。注意该函数在每次函数调用时都会对 Redis 进行三次或五次调用。按照目前的编写方式，这将导致
    Redis 和我们的客户端之间进行三次或五次往返。
- en: Listing 4.7\. The `update_token()` function from [section 2.5](kindle_split_012.html#ch02lev1sec5)
  id: totrans-507
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7\. 来自 [第 2.5 节](kindle_split_012.html#ch02lev1sec5) 的 `update_token()`
    函数
- en: '![](085fig01_alt.jpg)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![图片](085fig01_alt.jpg)'
- en: If our Redis and web servers are connected over LAN with only one or two steps,
    we could expect that the round trip between the web server and Redis would be
    around 1–2 milliseconds. With three to five round trips between Redis and the
    web server, we could expect that it would take 3–10 milliseconds for `update_token()`
    to execute. At that speed, we could only expect a single web server thread to
    be able to handle 100–333 requests per second. This is great, but we could do
    better. Let’s quickly create a non-transactional pipeline and make all of our
    requests over that pipeline. You can see the updated function in the next listing.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的 Redis 和 Web 服务器通过局域网连接，且只有一两个步骤，我们可以预期 Web 服务器和 Redis 之间的往返时间大约为 1–2 毫秒。在
    Redis 和 Web 服务器之间进行三次到五次往返后，我们可以预期 `update_token()` 的执行时间大约为 3–10 毫秒。在这个速度下，我们只能期望单个
    Web 服务器线程每秒能够处理 100–333 个请求。这已经很不错了，但我们还能做得更好。让我们快速创建一个非事务性管道，并通过该管道发送所有请求。您可以在下一个列表中看到更新后的函数。
- en: Listing 4.8\. The `update_token_pipeline()` function
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.8\. `update_token_pipeline()` 函数
- en: '![](086fig01_alt.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![图片](086fig01_alt.jpg)'
- en: By replacing our standard Redis connection with a pipelined connection, we can
    reduce our number of round trips by a factor of 3–5, and reduce the expected time
    to execute `update_token_pipeline()` to 1–2 milliseconds. At that speed, a single
    web server thread could handle 500–1000 requests per second if it only had to
    deal with updating item view information. Theoretically, this is great, but what
    about in reality?
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将我们的标准 Redis 连接替换为管道连接，我们可以将往返次数减少 3–5 倍，并将 `update_token_pipeline()` 的预期执行时间减少到
    1–2 毫秒。在这个速度下，如果 Web 服务器线程只需要处理更新项目视图信息，那么它每秒可以处理 500–1000 个请求。从理论上讲，这很棒，但在现实中又会怎样呢？
- en: Let’s test both of these functions by performing a simple benchmark. We’ll test
    the number of requests that can be processed per second against a copy of Redis
    that’s on the same machine, across a fast and low-latency network connection,
    and across a slow and higher latency connection. We’ll first start with the benchmark
    code that we’ll use to test the performance of these connections. In our benchmark,
    we’ll call either `update_token()` or `update_token_pipeline()` repeatedly until
    we reach a prespecified timeout, and then calculate the number of requests we
    can service at a given time. The following listing shows the code that we’ll use
    to run our two `update_token` commands.
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过执行一个简单的基准测试来测试这两个函数。我们将测试每秒可以处理多少个请求，与同一台机器上的Redis副本进行测试，通过快速和低延迟的网络连接，以及通过慢速和更高延迟的连接进行测试。我们将首先从我们将用于测试这些连接性能的基准代码开始。在我们的基准测试中，我们将反复调用`update_token()`或`update_token_pipeline()`，直到达到预定的超时时间，然后计算在给定时间内可以服务的请求数量。以下列表显示了我们将用于运行两个`update_token`命令的代码。
- en: Listing 4.9\. The `benchmark_update_token()` function
  id: totrans-514
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.9. `benchmark_update_token()`函数
- en: '![](086fig02_alt.jpg)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![](086fig02_alt.jpg)'
- en: When we run the benchmark function across a variety of connections with the
    given available bandwidth (gigabits or megabits) and latencies, we get data as
    shown in [table 4.4](#ch04table04).
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在具有给定可用带宽（千兆比特或兆比特）和延迟的各种连接上运行基准函数时，我们得到[表4.4](#ch04table04)中所示的数据。
- en: Table 4.4\. Performance of pipelined and nonpipelined connections over different
    types of connections. For high-speed connections, we’ll tend to run at the limit
    of what a single processor can perform for encoding/decoding commands in Redis.
    For slower connections, we’ll run at the limit of bandwidth and/or latency.
  id: totrans-517
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表4.4. 不同类型连接的管道和非管道连接的性能。对于高速连接，我们将倾向于运行单个处理器在Redis中编码/解码命令的性能极限。对于较慢的连接，我们将运行带宽和/或延迟的极限。
- en: '| Description | Bandwidth | Latency | update_table() calls per second | update_table_pipeline()
    calls per second |'
  id: totrans-518
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 带宽 | 延迟 | 每秒update_table()调用次数 | 每秒update_table_pipeline()调用次数 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-519
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Local machine, Unix domain socket | >1 gigabit | 0.015ms | 3,761 | 6,394
    |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '| 本地机器，Unix域套接字 | >1千兆比特 | 0.015毫秒 | 3,761 | 6,394 |'
- en: '| Local machine, localhost | >1 gigabit | 0.015ms | 3,257 | 5,991 |'
  id: totrans-521
  prefs: []
  type: TYPE_TB
  zh: '| 本地机器，localhost | >1千兆比特 | 0.015毫秒 | 3,257 | 5,991 |'
- en: '| Remote machine, shared switch | 1 gigabit | 0.271ms | 739 | 2,841 |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '| 远程机器，共享交换机 | 1千兆比特 | 0.271毫秒 | 739 | 2,841 |'
- en: '| Remote machine, connected through VPN | 1.8 megabit | 48ms | 3.67 | 18.2
    |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '| 远程机器，通过VPN连接 | 1.8兆比特 | 48毫秒 | 3.67 | 18.2 |'
- en: Looking at the table, note that for high-latency connections, we can multiply
    performance by a factor of five using pipelines over not using pipelines. Even
    with very low-latency remote connections, we’re able to improve performance by
    almost four times. For local connections, we actually run into the single-core
    performance limit of Python sending and receiving short command sequences using
    the Redis protocol (we’ll talk about this more in [section 4.6](#ch04lev1sec6)).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 观察表格，请注意，对于高延迟连接，我们可以通过使用管道而不是不使用管道将性能提高五倍。即使是非常低延迟的远程连接，我们也能将性能提高近四倍。对于本地连接，我们实际上遇到了Python使用Redis协议发送和接收短命令序列的单核性能限制（我们将在[第4.6节](#ch04lev1sec6)中更多地讨论这个问题）。
- en: You now know how to push Redis to perform better without transactions. Beyond
    using pipelines, are there any other standard ways of improving the performance
    of Redis?
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在知道如何在没有事务的情况下推动Redis表现得更好。除了使用管道之外，还有没有其他标准方法可以提高Redis的性能？
- en: 4.6\. Performance considerations
  id: totrans-526
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 性能考虑
- en: When coming from a relational database background, most users will be so happy
    with improving performance by a factor of 100 times or more by adding Redis, they
    won’t realize that they can make Redis perform even better. In the previous section,
    we introduced non-transactional pipelines as a way to minimize the number of round
    trips between our application and Redis. But what if we’ve already built an application,
    and we know that it could perform better? How do we find ways to improve performance?
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 当来自关系型数据库背景时，大多数用户会非常高兴地通过添加Redis将性能提高100倍或更多，以至于他们没有意识到他们可以使Redis的性能表现得更好。在前一节中，我们介绍了非事务性管道作为减少应用程序和Redis之间往返次数的一种方式。但如果我们已经构建了一个应用程序，并且我们知道它可以表现得更好，我们应该怎么办？我们如何找到提高性能的方法？
- en: Improving performance in Redis requires having an understanding of what to expect
    in terms of performance for the types of commands that we’re sending to Redis.
    To get a better idea of what to expect from Redis, we’ll quickly run a benchmark
    that’s included with Redis, `redis-benchmark`, as can be seen in [listing 4.10](#ch04ex10).
    Feel free to explore `redis-benchmark` on your own to discover the performance
    characteristics of your server and of Redis.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Redis 中提高性能需要了解我们发送到 Redis 的命令类型方面的性能预期。为了更好地了解 Redis 的预期性能，我们将快速运行 Redis
    中包含的基准测试，即 `redis-benchmark`，如[代码列表 4.10](#ch04ex10)所示。请随意探索 `redis-benchmark`
    以发现您服务器和 Redis 的性能特征。
- en: Listing 4.10\. Running `redis-benchmark` on an Intel Core-2 Duo 2.4 GHz desktop
  id: totrans-529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.10\. 在 Intel Core-2 Duo 2.4 GHz 桌面上运行 `redis-benchmark`
- en: '![](088fig01_alt.jpg)'
  id: totrans-530
  prefs: []
  type: TYPE_IMG
  zh: '![](088fig01_alt.jpg)'
- en: The output of `redis-benchmark` shows a group of commands that are typically
    used in Redis, as well as the number of commands of that type that can be run
    in a single second. A standard run of this benchmark without any options will
    try to push Redis to its limit using 50 clients, but it’s a lot easier to compare
    performance of a single benchmark client against one copy of our own client, rather
    than many.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '`redis-benchmark` 的输出显示了一组在 Redis 中通常使用的命令，以及每秒可以运行的该类型命令的数量。在没有任何选项的情况下运行此基准测试将尝试使用
    50 个客户端将 Redis 推向其极限，但与多个客户端相比，将单个基准测试客户端的性能与我们的客户端的一个副本进行比较要容易得多。'
- en: When looking at the output of `redis-benchmark`, we must be careful not to try
    to directly compare its output with how quickly our application performs. This
    is because `redis-benchmark` doesn’t actually process the result of the commands
    that it performs, which means that the results of some responses that require
    substantial parsing overhead aren’t taken into account. Generally, compared to
    `redis-benchmark` running with a single client, we can expect the Python Redis
    client to perform at roughly 50–60% of what `redis-benchmark` will tell us for
    a single client and for nonpipelined commands, depending on the complexity of
    the command to call.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在查看 `redis-benchmark` 的输出时，我们必须小心不要直接将其输出与我们的应用程序的性能进行比较。这是因为 `redis-benchmark`
    实际上并没有处理它执行的命令的结果，这意味着一些需要大量解析开销的响应结果没有被考虑在内。通常，与使用单个客户端运行的 `redis-benchmark`
    相比，我们可以预期 Python Redis 客户端在调用复杂命令时，其性能大约是 `redis-benchmark` 对单个客户端和非管道命令报告的 50–60%。
- en: If you find that your commands are running at about half of what you’d expect
    given `redis-benchmark` (about 25–30% of what `redis-benchmark` reports), or if
    you get errors reporting “Cannot assign requested address,” you may be accidentally
    creating a new connection for every command.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您发现您的命令运行速度大约是您预期的 `redis-benchmark` 速度的一半（大约是 `redis-benchmark` 报告的 25–30%），或者如果您收到“无法分配请求的地址”的错误报告，您可能意外地为每个命令创建了一个新的连接。
- en: I’ve listed some performance numbers relative to a single `redis-benchmark`
    client using the Python client, and have described some of the most likely causes
    of slowdowns and/or errors in [table 4.5](#ch04table05).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我列出了一些相对于单个 `redis-benchmark` 客户端使用 Python 客户端时的性能数据，并描述了一些可能导致性能下降和/或错误的最可能原因，见[表
    4.5](#ch04table05)。
- en: Table 4.5\. A table of general performance comparisons against a single `redis-benchmark`
    client and what may be causing potential slowdowns
  id: totrans-535
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 4.5\. 与单个 `redis-benchmark` 客户端的一般性能比较表以及可能导致潜在性能下降的原因
- en: '| Performance or error | Likely cause | Remedy |'
  id: totrans-536
  prefs: []
  type: TYPE_TB
  zh: '| 性能或错误 | 可能原因 | 解决方案 |'
- en: '| --- | --- | --- |'
  id: totrans-537
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 50–60% of redis-benchmark for a single client | Expected performance without
    pipelining | N/A |'
  id: totrans-538
  prefs: []
  type: TYPE_TB
  zh: '| 单个客户端的 50–60% 的 redis-benchmark | 无管道时的预期性能 | 无需操作 |'
- en: '| 25–30% of redis-benchmark for a single client | Connecting for every command/group
    of commands | Reuse your Redis connections |'
  id: totrans-539
  prefs: []
  type: TYPE_TB
  zh: '| 单个客户端的 25–30% 的 redis-benchmark | 每个命令/命令组都进行连接 | 重复使用您的 Redis 连接 |'
- en: '| Client error: “Cannot assign requested address” | Connecting for every command/group
    of commands | Reuse your Redis connections |'
  id: totrans-540
  prefs: []
  type: TYPE_TB
  zh: '| 客户端错误：“无法分配请求的地址” | 每个命令/命令组都进行连接 | 重复使用您的 Redis 连接 |'
- en: This list of possible performance issues and solutions is short, but these issues
    amount to easily 95% of the performance-related problems that users report on
    a regular basis (aside from using Redis data structures incorrectly). If we’re
    experiencing slowdowns that we’re having difficulty in diagnosing, and we know
    it isn’t one of the problems listed in [table 4.5](#ch04table05), we should request
    help by one of the ways described in [section 1.4](kindle_split_011.html#ch01lev1sec4).
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 这份可能的性能问题和解决方案列表很短，但这些问题的数量占到了用户定期报告的性能相关问题的 95% 以上（除了使用 Redis 数据结构不正确之外）。如果我们遇到难以诊断的减速，并且我们知道它不是
    [表 4.5](#ch04table05) 中列出的问题之一，我们应该通过 [第 1.4 节](kindle_split_011.html#ch01lev1sec4)
    中描述的某种方式请求帮助。
- en: Most client libraries that access Redis offer some level of connection pooling
    built in. For Python, we only need to create a single `redis.Redis()` for every
    unique Redis server we need to connect to (we need to create a new connection
    for each numbered database we’re using). The `redis.Redis()` object itself will
    handle creating connections as necessary, reusing existing connections, and discarding
    timed-out connections. As written, the Python client connection pooling is both
    thread safe and `fork()` safe.
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数访问 Redis 的客户端库都内置了一定级别的连接池。对于 Python，我们只需要为每个需要连接的唯一 Redis 服务器创建一个 `redis.Redis()`（我们为每个使用编号数据库都需要创建一个新的连接）。`redis.Redis()`
    对象本身将负责根据需要创建连接，重用现有连接，并丢弃超时的连接。按照目前的编写，Python 客户端连接池既线程安全也 `fork()` 安全。
- en: 4.7\. Summary
  id: totrans-543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7\. 摘要
- en: Through this chapter, we’ve covered topics that can help keep Redis performing
    well while keeping your data secure against system failures. The first half of
    the chapter primarily discussed the use of persistence and replication to prepare
    for failures and deal with failures. The latter half dealt with keeping your data
    from being corrupted, using pipelines to improve performance, and diagnosing potential
    performance problems.
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 通过本章，我们涵盖了可以帮助 Redis 保持良好性能同时确保数据在系统故障中安全性的主题。本章的前半部分主要讨论了使用持久化和复制来准备故障处理和故障处理。后半部分处理了防止数据损坏，使用管道提高性能，以及诊断潜在的性能问题。
- en: If there are two things you should take from this chapter, they are that the
    use of replication and append-only files can go a long way toward keeping your
    data safe, and that using `WATCH`/`MULTI`/`EXEC` can keep your data from being
    corrupted by multiple clients working on the same data.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您应该从本章中吸取两点，那么它们是：使用复制和只追加文件可以大大提高数据的安全性，以及使用 `WATCH`/`MULTI`/`EXEC` 可以防止多个客户端在相同数据上工作时数据被破坏。
- en: Hopefully our discussion of `WATCH`/`MULTI`/`EXEC` introduced in [chapter 3](kindle_split_014.html#ch03)
    has helped you to better understand how to fully utilize transactions in Redis.
    In [chapter 6](kindle_split_017.html#ch06), we’ll revisit transactions, but now
    let’s move on to [chapter 5](kindle_split_016.html#ch05), where you’ll learn more
    about using Redis to help with system administration tasks.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 希望我们在 [第 3 章](kindle_split_014.html#ch03) 中引入的 `WATCH`/`MULTI`/`EXEC` 的讨论能帮助您更好地理解如何充分利用
    Redis 中的事务。在 [第 6 章](kindle_split_017.html#ch06) 中，我们将重新讨论事务，但现在让我们转到 [第 5 章](kindle_split_016.html#ch05)，在那里您将了解更多关于如何使用
    Redis 来帮助系统管理任务的信息。
- en: Chapter 5\. Using Redis for application support
  id: totrans-547
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章\. 使用 Redis 支持应用程序
- en: '*This chapter covers*'
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Logging to Redis
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将日志记录到 Redis
- en: Counters and statistics
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计数器和统计信息
- en: Discovering city and country from IP address
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 IP 地址发现城市和国家
- en: Service discovery and configuration
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务发现和配置
- en: 'In the last chapter, we spent most of our time talking about how to keep Redis
    up and running as part of a larger group of systems. In this chapter, we’ll talk
    about using Redis to support other parts of your environment: from gathering information
    about the current state of the system with logs and counters, to discovering information
    about the clients using your system, all the way to configuring your system by
    using Redis as a directory.'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们大部分时间都在讨论如何将 Redis 作为更大系统组的一部分保持运行。在本章中，我们将讨论如何使用 Redis 来支持您环境中的其他部分：从使用日志和计数器收集关于系统当前状态的信息，到发现使用您系统的客户端信息，一直到使用
    Redis 作为目录来配置您的系统。
- en: Overall, this chapter offers control of and insight into how your system operates
    during runtime. As you read along, keep in mind that we’re looking to support
    the continued running of higher-level applications—that the components we build
    in this chapter aren’t the applications themselves, but will help to support those
    applications. This support comes by way of recording information about the applications
    and application visitors, and a method of configuring applications. Let’s look
    at the first level of monitoring that we can add through logging.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，本章提供了对系统在运行时操作的控制和洞察。在你阅读的过程中，请记住，我们正在寻求支持高级应用程序的持续运行——本章中构建的组件本身不是应用程序，而是将帮助支持这些应用程序。这种支持是通过记录有关应用程序和应用程序访问者的信息以及配置应用程序的方法来实现的。让我们看看我们可以通过日志记录添加的第一层监控。
- en: 5.1\. Logging to Redis
  id: totrans-555
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1. 将日志记录到Redis
- en: As we build applications and services, being able to discover information about
    the running system becomes increasingly important. Being able to dig into that
    information to diagnose problems, discover problems before they become severe,
    or even just to discover information about users—these all necessitate logging.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们构建应用程序和服务，能够发现有关运行系统的信息变得越来越重要。能够深入挖掘这些信息以诊断问题、在问题变得严重之前发现它们，或者仅仅是发现有关用户的信息——所有这些都需要记录日志。
- en: In the world of Linux and Unix, there are two common logging methods. The first
    is logging to a file, where over time we write individual log lines to a file,
    and every once in a while, we write to a new file. Many thousands of pieces of
    software have been written do this (including Redis itself). But this method can
    run into issues because we have many different services writing to a variety of
    log files, each with a different way of rolling them over, and no common way of
    easily taking all of the log files and doing something useful with them.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux和Unix的世界里，有两种常见的日志记录方法。第一种是将日志记录到文件中，随着时间的推移，我们将单个日志行写入文件，偶尔我们会写入到一个新文件中。已经编写了成千上万件软件来做这件事（包括Redis本身）。但是这种方法可能会遇到问题，因为我们有多个不同的服务正在将信息写入各种日志文件，每个文件都有不同的滚动方式，而且没有一种简单的方法可以轻松地处理所有日志文件并从中获取有用的信息。
- en: Running on TCP and UDP port 514 of almost every Unix and Linux server available
    is a service called *syslog*, the second common logging method. Syslog accepts
    log messages from any program that sends it a message and routes those messages
    to various on-disk log files, handling rotation and deletion of old logs. With
    configuration, it can even forward messages to other servers for further processing.
    As a service, it’s far more convenient than logging to files directly, because
    all of the special log file rotation and deletion is already handled for us.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎所有Unix和Linux服务器都运行在TCP和UDP端口514上的一个名为*syslog*的服务，这是第二种常见的日志记录方法。Syslog接受任何向它发送消息的程序发送的日志消息，并将这些消息路由到各种磁盘日志文件中，处理旧日志的轮换和删除。通过配置，它甚至可以将消息转发到其他服务器进行进一步处理。作为一个服务，它比直接将日志记录到文件要方便得多，因为所有特殊的日志文件轮换和删除都已经为我们处理好了。
- en: '|  |'
  id: totrans-559
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Replacing syslog
  id: totrans-560
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 替换syslog
- en: Whether you end up using the logging methods described here, you owe it to yourself
    to consider replacing your current syslog daemon (which is likely `Rsyslogd`)
    with `syslog-ng`. Having used and configured both systems, I find that the configuration
    language that `syslog-ng` provides for directing log messages is easier to use.
    And though I don’t have the space or time to build it in this book, building a
    service that consumes syslog messages and puts them into Redis is a great way
    to offer a layer of indirection between what needs to happen now for processing
    a request, and what can happen later (like logging or updating counters).
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你最终是否使用这里描述的日志记录方法，你都应该考虑用`syslog-ng`替换你当前的syslog守护进程（可能是`Rsyslogd`）。在使用和配置了这两个系统之后，我发现`syslog-ng`提供的用于指导日志消息的配置语言更容易使用。尽管我没有足够的空间和时间在这本书中构建它，但构建一个消费syslog消息并将它们放入Redis的服务是一个很好的方法，可以在处理请求的当前需要发生的事情和可以稍后发生的事情（如日志记录或更新计数器）之间提供一层间接层。
- en: '|  |'
  id: totrans-562
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Having logs available in files on a single server (thanks to syslog forwarding)
    is a great long-term plan with logging (remember to back them up). In this section,
    we’ll talk about using Redis as a way of keeping more time-sensitive logs, to
    function as a replacement for syslog messages being stored in the short term.
    Our first view into changing logs is a continuously updated stream of recent log
    messages.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 由于syslog转发，在单个服务器上的文件中（感谢syslog转发）提供日志是一个很好的长期计划（记得备份它们）。在本节中，我们将讨论使用Redis作为保持更时间敏感的日志的方式，作为短期存储syslog消息的替代。我们第一次查看日志变化的方式是连续更新的最近日志消息流。
- en: 5.1.1\. Recent logs
  id: totrans-564
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 近期日志
- en: When building a system, knowing what’s important to record can be difficult.
    Do you record every time someone logs in? What about when they log out? Do you
    log every time someone changes their account information? Or do you only log errors
    and exceptions? I can’t answer those questions for you directly, but I can offer
    a method of keeping a recent list of log messages in Redis, which will let you
    get a snapshot view of your logs at any time.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建系统时，知道需要记录什么可能会很困难。你是否每次有人登录时都记录？当他们注销时呢？你是否每次有人更改账户信息时都记录？或者你只记录错误和异常？我无法直接为你回答这些问题，但我可以提供一个方法，让你在Redis中保持最近日志消息的列表，这样你就可以在任何时候获取日志的快照视图。
- en: To keep a recent list of logs, we’ll `LPUSH` log messages to a `LIST` and then
    trim that `LIST` to a fixed size. Later, if we want to read the log messages,
    we can perform a simple `LRANGE` to fetch the messages. We’ll take a few extra
    steps to support different named log message queues and to support the typical
    log severity levels, but you can remove either of those in your own code if you
    need to. The code for writing recent logs to Redis is shown in the next listing.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 为了保持最近日志的列表，我们将日志消息`LPUSH`到一个`LIST`中，然后将其修剪到固定大小。稍后，如果我们想读取日志消息，我们可以执行一个简单的`LRANGE`来获取消息。我们将采取一些额外的步骤来支持不同命名的日志消息队列和支持典型的日志严重级别，但如果你需要，你可以在自己的代码中移除其中任何一个。将最近日志写入Redis的代码将在下一列表中展示。
- en: Listing 5.1\. The `log_recent()` function
  id: totrans-567
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.1\. `log_recent()`函数
- en: '![](092fig01_alt.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![图片](092fig01_alt.jpg)'
- en: Aside from the part that handles turning the different log levels into useful
    strings like `info` and `debug`, the `log_recent()` function is simple—a quick
    `LPUSH` followed by an `LTRIM`. Now that you have a better idea of what’s going
    on right now, can we discover the most common (and maybe the most important) messages?
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 除了处理将不同日志级别转换为有用的字符串（如`info`和`debug`）的部分之外，`log_recent()`函数很简单——先进行一次`LPUSH`操作，然后进行一次`LTRIM`。现在你已经对当前正在发生的事情有了更好的了解，我们能否发现最常见（也许是最重要的）消息？
- en: 5.1.2\. Common logs
  id: totrans-570
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 常见日志
- en: If you’ve been running `log_recent()`, you’ll probably discover that although
    it’s useful for getting an idea of what’s happening right now, it’s not very good
    at telling you whether any important messages were lost in the noise. By recording
    information about how often a particular message appears, you could then look
    through the messages ordered by how often they happened to help you determine
    what’s important.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一直在运行`log_recent()`，你可能会发现，虽然它对于了解当前正在发生的事情很有用，但它并不擅长告诉你是否有任何重要消息在噪音中丢失。通过记录特定消息出现的频率信息，然后你可以按发生的频率顺序查看消息，以帮助你确定什么重要。
- en: A simple and useful way of knowing how often a message appears is by storing
    the message as a member of a `ZSET`, with the score being how often the message
    appears. To make sure that we only see recent common messages, we’ll rotate our
    record of common messages every hour. So that we don’t lose *everything*, we’ll
    keep the previous hour’s worth of common messages. Our code for keeping track
    of and rotating common log messages is shown next.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 知道一个消息出现频率的一种简单而实用的方法是将消息存储为`ZSET`的一个成员，其中分数表示消息出现的频率。为了确保我们只看到最近常见的消息，我们将每小时轮换一次常见消息的记录。为了不丢失*所有*内容，我们将保留前一个小时的常见消息。我们用于跟踪和轮换常见日志消息的代码将在下面展示。
- en: Listing 5.2\. The `log_common()` function
  id: totrans-573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.2\. `log_common()`函数
- en: '![](093fig01_alt.jpg)'
  id: totrans-574
  prefs: []
  type: TYPE_IMG
  zh: '![图片](093fig01_alt.jpg)'
- en: This logging function was more involved than the recent logging function, primarily
    due to being careful when taking care of old logs. That’s why we used the `WATCH`/`MULTI`/`EXEC`
    transaction to rename the `ZSET` and rewrite the key that tells us what hour the
    current data is for. We also passed the pipeline through to the `log_recent()`
    function to minimize round trips to Redis while keeping track of common and recent
    logs.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 这个日志功能比最近的日志功能更复杂，主要是因为在处理旧日志时非常小心。这就是为什么我们使用了 `WATCH`/`MULTI`/`EXEC` 事务来重命名
    `ZSET` 并重写告诉我们当前数据是哪个小时的键。我们还通过管道将 `log_recent()` 函数传递过去，以在跟踪常见和最近日志的同时最小化对 Redis
    的往返次数。
- en: Now that we’ve started to gather information about running software in Redis
    by storing recent and common logs, what other kinds of information would be useful
    to keep in Redis?
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经开始通过存储最近和常见的日志来收集关于在 Redis 中运行软件的信息，那么还有哪些其他类型的信息对保留在 Redis 中会有用？
- en: 5.2\. Counters and statistics
  id: totrans-577
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 计数器和统计数据
- en: As you saw way back in [chapter 2](kindle_split_012.html#ch02) when I introduced
    the concept of counting individual page visits, having basic hit count information
    can (for example) change the way we choose our caches. But our example from [chapter
    2](kindle_split_012.html#ch02) was very simple, and reality is rarely that simple,
    especially when it involves a real website.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 就像你在[第 2 章](kindle_split_012.html#ch02)中看到的那样，当我介绍了计数单个页面访问的概念，拥有基本的点击次数信息可以（例如）改变我们选择缓存的方式。但我们的[第
    2 章](kindle_split_012.html#ch02)中的例子非常简单，现实很少那么简单，尤其是在涉及到真实网站的时候。
- en: The fact that our site received 10,000 hits in the last 5 minutes, or that the
    database handled 200 writes and 600 reads in the last 5 seconds, is useful information
    to know. If we add the ability to see that information over time, we can notice
    sudden or gradual increases in traffic, predict when server upgrades are necessary,
    and ultimately save ourselves from downtime due to an overloaded system.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 我们网站在过去的 5 分钟内收到了 10,000 次点击，或者数据库在过去的 5 秒内处理了 200 次写入和 600 次读取，这些是有用的信息。如果我们能够随着时间的推移看到这些信息，我们就可以注意到流量突然或逐渐的增加，预测何时需要服务器升级，并最终避免因系统过载而导致的停机。
- en: This section will work through two different methods for recording both counters
    and statistics in Redis, and will finish by discussing how to simplify the collection
    of our example statistics. Both of these examples are driven by real use cases
    and requirements. Our next stop on the road of application introspection is collecting
    time series counters in Redis.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍两种不同的方法来记录 Redis 中的计数器和统计数据，并通过讨论如何简化我们示例统计数据的收集来结束。这两个示例都由实际的使用案例和需求驱动。我们在应用内省之路上的下一个目的地是收集
    Redis 中的时间序列计数器。
- en: 5.2.1\. Storing counters in Redis
  id: totrans-581
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 在 Redis 中存储计数器
- en: As we monitor our application, being able to gather information over time becomes
    ever more important. Code changes (that can affect how quickly our site responds,
    and subsequently how many pages we serve), new advertising campaigns, or new users
    to our system can all radically change the number of pages that are loaded on
    a site. Subsequently, any number of other performance metrics may change. But
    if we aren’t recording any metrics, then it’s impossible to know how they’re changing,
    or whether we’re doing better or worse.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们监控我们的应用程序，能够随着时间的推移收集信息变得越来越重要。代码更改（可能会影响我们网站响应的速度，从而影响我们服务的页面数量），新的广告活动，或新用户到我们的系统中，都可能极大地改变网站上加载的页面数量。随后，许多其他性能指标可能会改变。但如果我们没有记录任何指标，那么我们就无法知道它们是如何变化的，或者我们是在变得更好还是更差。
- en: In an attempt to start gathering metrics to watch and analyze, we’ll build a
    tool to keep named counters over time (counters with names like *site hits*, *sales*,
    or *database queries* can be crucial). Each of these counters will store the most
    recent 120 samples at a variety of time precisions (like 1 second, 5 seconds,
    1 minute, and so on). Both the number of samples and the selection of precisions
    to record can be customized as necessary.
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 为了开始收集用于监控和分析的指标，我们将构建一个工具来随着时间的推移保持命名的计数器（例如 *site hits*、*sales* 或 *database
    queries* 的计数器可能至关重要）。每个计数器都将存储最近 120 个样本，以各种时间精度（如 1 秒、5 秒、1 分钟等）存储。样本的数量和记录精度的选择可以根据需要定制。
- en: The first step for keeping counters is actually storing the counters themselves.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 保持计数器的第一步实际上是存储计数器本身。
- en: Updating a counter
  id: totrans-585
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更新计数器
- en: In order to update counters, we’ll need to store the actual counter information.
    For each counter and precision, like *site hits* and *5 seconds*, we’ll keep a
    `HASH` that stores information about the number of site hits that have occurred
    in each 5-second time slice. The keys in the hash will be the start of the time
    slice, and the value will be the number of hits. [Figure 5.1](#ch05fig01) shows
    a selection of data from a hit counter with 5-second time slices.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更新计数器，我们需要存储实际的计数器信息。对于每个计数器和精度，例如 *网站点击* 和 *5 秒*，我们将保留一个 `HASH`，其中存储了每个 5
    秒时间切片内发生的网站点击次数。`HASH` 中的键将是时间切片的开始，值将是点击次数。[图 5.1](#ch05fig01) 展示了一个具有 5 秒时间切片的点击计数器的数据选择。
- en: Figure 5.1\. A `HASH` that shows the number of web page hits over 5-second time
    slices around 7:40 a.m. on May 7, 2012
  id: totrans-587
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1\. 一个 `HASH`，显示了 2012 年 5 月 7 日上午 7:40 左右 5 秒时间切片内的网页点击次数
- en: '![](05fig01.jpg)'
  id: totrans-588
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig01.jpg)'
- en: As we start to use counters, we need to record what counters have been written
    to so that we can clear out old data. For this, we need an ordered sequence that
    lets us iterate one by one over its entries, and that also doesn’t allow duplicates.
    We could use a `LIST` combined with a `SET`, but that would take extra code and
    round trips to Redis. Instead, we’ll use a `ZSET`, where the members are the combinations
    of precisions and names that have been written to, and the scores are all 0\.
    By setting all scores to 0 in a `ZSET`, Redis will try to sort by score, and finding
    them all equal, will then sort by member name. This gives us a fixed order for
    a given set of members, which will make it easy to sequentially scan them. An
    example `ZSET` of known counters can be seen in [figure 5.2](#ch05fig02).
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始使用计数器时，我们需要记录哪些计数器已被写入，以便我们可以清除旧数据。为此，我们需要一个有序序列，它允许我们逐个迭代其条目，并且不允许重复。我们可以使用
    `LIST` 与 `SET` 结合，但这将需要额外的代码和 Redis 的往返。相反，我们将使用 `ZSET`，其中成员是已写入的精度和名称的组合，分数都是
    0。通过在 `ZSET` 中将所有分数设置为 0，Redis 将尝试按分数排序，由于它们都相等，因此将按成员名称排序。这为我们给定的一组成员提供了一个固定顺序，这将使它们按顺序扫描变得容易。一个已知的计数器的
    `ZSET` 示例可以在 [图 5.2](#ch05fig02) 中看到。
- en: Figure 5.2\. A `ZSET` that shows some known counters
  id: totrans-590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.2\. 一个 `ZSET`，显示了某些已知的计数器
- en: '![](05fig02.jpg)'
  id: totrans-591
  prefs: []
  type: TYPE_IMG
  zh: '![图片](05fig02.jpg)'
- en: Now that we know what our structures for counters look like, what goes on to
    make that happen? For each time slice precision, we’ll add a reference to the
    precision and the name of the counter to the known `ZSET`, and we’ll increment
    the appropriate time window by the count in the proper `HASH`. Our code for updating
    a counter looks like this.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道了计数器的结构是什么样的，接下来会发生什么呢？对于每个时间切片精度，我们将向已知的 `ZSET` 添加对精度和计数器名称的引用，并将适当的时窗计数增加适当的
    `HASH` 中的计数。我们的更新计数器的代码如下。
- en: Listing 5.3\. The `update_counter()` function
  id: totrans-593
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. `update_counter()` 函数
- en: '![](095fig01_alt.jpg)'
  id: totrans-594
  prefs: []
  type: TYPE_IMG
  zh: '![图片](095fig01_alt.jpg)'
- en: Updating the counter information isn’t so bad; just a `ZADD` and `HINCRBY` for
    each time slice precision. And fetching the information for a named counter and
    a specific precision is also easy. We fetch the whole `HASH` with `HGETALL`, convert
    our time slices and counters back into numbers (they’re all returned as strings),
    sort them by time, and finally return the values. The next listing shows our code
    for fetching counter data.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 更新计数器信息并不那么糟糕；只需为每个时间切片精度执行一个 `ZADD` 和 `HINCRBY`。并且获取命名计数器和特定精度的信息也很简单。我们使用
    `HGETALL` 获取整个 `HASH`，将我们的时间切片和计数器转换回数字（它们都作为字符串返回），按时间排序，并最终返回值。下一个列表显示了获取计数器数据的代码。
- en: Listing 5.4\. The `get_counter()` function
  id: totrans-596
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. `get_counter()` 函数
- en: '![](095fig02_alt.jpg)'
  id: totrans-597
  prefs: []
  type: TYPE_IMG
  zh: '![图片](095fig02_alt.jpg)'
- en: We did exactly what we said we were going to do. We fetched the data, ordered
    it sequentially based on time, and converted it back into integers. Let’s look
    at how we prevent these counters from keeping too much data.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 我们确实做了我们所说的。我们获取了数据，根据时间顺序排列，并将其转换回整数。让我们看看我们是如何防止这些计数器保留太多数据的。
- en: Cleaning out old counters
  id: totrans-599
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 清理旧计数器
- en: Now we have all of our counters written to Redis and we can fetch our counters
    with ease. But as we update our counters, at some point we’re going to run out
    of memory if we don’t perform any cleanup. Because we were thinking ahead, we
    wrote to our known `ZSET` the listing of known counters. To clean out the counters,
    we need to iterate over that listing and clean up old entries.
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有计数器写入 Redis，并且可以轻松地获取我们的计数器。但是，随着我们更新计数器，如果我们不执行任何清理，我们最终会耗尽内存。因为我们事先考虑到了这一点，所以我们把我们已知的计数器列表写入到已知的
    `ZSET` 中。要清理计数器，我们需要遍历这个列表并清理旧条目。
- en: '|  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Why not use `EXPIRE`?
  id: totrans-602
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么不使用 `EXPIRE` 命令呢？
- en: One limitation of the `EXPIRE` command is that it only applies to whole keys;
    we can’t expire parts of keys. And because we chose to structure our data so that
    counter X of precision Y is in a single key for all time, we have to clean out
    the counters periodically. If you feel ambitious, you may want to try restructuring
    the counters to change the data layout to use standard Redis expiration instead.
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: '`EXPIRE` 命令的一个限制是它只适用于整个键；我们无法使键的部分过期。而且因为我们选择将数据结构化，使得计数器 X 的精度 Y 在所有时间都存储在单个键中，我们必须定期清理计数器。如果你有雄心壮志，你可能想尝试重新结构化计数器，以改变数据布局以使用标准的
    Redis 过期机制。'
- en: '|  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'As we process and clean up old counters, a few things are important to pay
    attention to. The following list shows a few of the more important items that
    we need to be aware of as we clean out old counters:'
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理和清理旧计数器时，有一些事情需要注意。以下列表显示了一些我们在清理旧计数器时需要特别注意的重要事项：
- en: New counters can be added at any time.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 新的计数器可以随时添加。
- en: Multiple cleanups may be occurring at the same time.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能会同时进行多次清理。
- en: Trying to clean up daily counters every minute is a waste of effort.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每分钟尝试清理每日计数器是一种浪费。
- en: If a counter has no more data, we shouldn’t try to clean it up any more.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果计数器没有更多数据，我们就不应该再尝试清理它。
- en: With all of those things in mind, we’ll build a daemon function similar in operation
    to the daemon functions that we wrote back in [chapter 2](kindle_split_012.html#ch02).
    As before, we’ll repeatedly loop until the system is told to quit. To help minimize
    load during cleanup, we’ll attempt to clean out old counters roughly once per
    minute, and will also clean up old counters at roughly the schedule that they’re
    creating new entries, except for counters that get new entries more often than
    once per minute. If a counter has a time slice of 5 minutes, we’ll try to clean
    out old entries from that counter every 5 minutes. Counters that have new entries
    more often (1 second and 5 seconds in our example), we’ll clean out every minute.
  id: totrans-610
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到所有这些因素，我们将构建一个类似于我们在第 2 章中编写的守护进程函数。像以前一样，我们将反复循环，直到系统被告知退出。为了帮助在清理期间最小化负载，我们将尝试大约每分钟清理一次旧计数器，并且也会按照它们创建新条目的大致时间表来清理旧计数器，除了每分钟获得新条目次数超过一次的计数器。如果一个计数器的时长为
    5 分钟，我们将尝试每 5 分钟从这个计数器中清理旧条目。对于新条目频率更高的计数器（在我们的例子中是 1 秒和 5 秒），我们将每分钟清理一次。
- en: To iterate over the counters, we’ll fetch known counters one by one with `ZRANGE`.
    To clean a counter, we’ll fetch all of the start times for a given counter, calculate
    which items are before a calculated cutoff (120 samples ago), and remove them.
    If there’s no more data for a given counter, we’ll remove the counter reference
    from the known `ZSET`. Explaining what goes on is simple, but the details of the
    code show some corner cases. Check out this listing to see the cleanup code in
    full detail.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 要遍历计数器，我们将使用 `ZRANGE` 逐个获取已知的计数器。要清理计数器，我们将获取给定计数器的所有起始时间，计算哪些项目是在计算截止时间（120个样本之前）之前的，并将它们删除。如果给定计数器没有更多数据，我们将从已知的
    `ZSET` 中删除计数器引用。解释这个过程很简单，但代码的细节显示了某些边缘情况。查看此列表以详细了解清理代码。
- en: Listing 5.5\. The `clean_counters()` function
  id: totrans-612
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5. `clean_counters()` 函数
- en: '![](ch05ex05-0.jpg)'
  id: totrans-613
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5-0](ch05ex05-0.jpg)'
- en: '![](ch05ex05-1.jpg)'
  id: totrans-614
  prefs: []
  type: TYPE_IMG
  zh: '![图 5.5-1](ch05ex05-1.jpg)'
- en: As described earlier, we iterate one by one over the `ZSET` of counters, looking
    for items to clean out. We only clean out counters that should be cleaned in this
    pass, so we perform that check early. We then fetch the counter data and determine
    what (if anything) should be cleaned up. After cleaning out old data as necessary,
    if we don’t believe that there should be any remaining data, we verify that there’s
    no more data for the counter and remove it from our `ZSET` of counters. Finally,
    after passing over all of the counters, we calculate how long it took to perform
    a pass, and sleep roughly the remainder of the minute we left for ourselves to
    perform the full cleanup, until the next pass.
  id: totrans-615
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们逐个遍历计数器的`ZSET`，寻找需要清理的项目。我们只清理这次遍历中应该清理的计数器，因此我们提前进行这个检查。然后我们获取计数器数据并确定（如果有的话）应该清理什么。在必要时清理旧数据后，如果我们认为不应该有任何剩余数据，我们将验证计数器没有更多数据，并将其从我们的计数器`ZSET`中删除。最后，在遍历所有计数器之后，我们计算完成一次遍历所需的时间，然后睡去剩余的分钟，直到下一次遍历。
- en: Now that we have counter data, are cleaning it up, and can fetch it, it’s just
    a matter of building an interface for consuming the data. Sadly, that part is
    out of the scope of this book, but there are a few usable JavaScript plotting
    libraries that can help you out on the web side of things (I’ve had good experiences
    with jqplot [[http://www.jqplot.com/](http://www.jqplot.com/)], Highcharts [[http://www.highcharts.com/](http://www.highcharts.com/)],
    dygraphs [[http://dygraphs.com/](http://dygraphs.com/)], and D3 [[http://d3js.org/](http://d3js.org/)]
    for personal and professional uses).
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了计数器数据，正在对其进行清理，并且可以获取它，现在的问题只是构建一个用于消费数据的接口。遗憾的是，这部分内容超出了本书的范围，但有一些可用的JavaScript绘图库可以帮助你在网络端进行（我在个人和专业使用中与jqplot
    [[http://www.jqplot.com/](http://www.jqplot.com/)]、Highcharts [[http://www.highcharts.com/](http://www.highcharts.com/)]、dygraphs
    [[http://dygraphs.com/](http://dygraphs.com/)]和D3 [[http://d3js.org/](http://d3js.org/)]有很好的体验）。
- en: When dealing with the depth of complexity in a real website, knowing that a
    page gets hit thousands of times a day can help us to decide that the page should
    be cached. But if that page takes 2 milliseconds to render, whereas another page
    gets one tenth the traffic but takes 2 seconds to render, we can instead direct
    our attention to optimizing the slower page. In the next section, we change our
    approach from keeping precise counters that give us data over time, to keeping
    aggregate statistics to help us make more nuanced decisions about what to optimize.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理真实网站中的复杂深度时，知道一个页面每天被点击数千次可以帮助我们决定该页面应该被缓存。但如果该页面渲染需要2毫秒，而另一个页面流量只有十分之一，但渲染需要2秒，我们则可以将注意力转向优化较慢的页面。在下一节中，我们将方法从保持精确的计数器（它给我们随时间变化的数据）转变为保持汇总统计数据，以帮助我们做出更细致的优化决策。
- en: 5.2.2\. Storing statistics in Redis
  id: totrans-618
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2. 在Redis中存储统计数据
- en: Truth be told, I’ve personally implemented five different methods of storing
    statistics in Redis. The method described here takes many of the good ideas from
    those methods and combines them in a way that allows for the greatest flexibility
    and opportunity to scale. What are we going to build?
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 实话实说，我个人已经实现了五种不同的在Redis中存储统计数据的方法。这里描述的方法从那些方法中吸取了许多好的想法，并以一种允许最大灵活性和扩展机会的方式将它们结合起来。我们将要构建什么？
- en: We’ll build a method to store statistics that have a similar scope to our `log_common()`
    function from [section 5.1.2](#ch05lev2sec2) (the current hour and the last hour).
    We’ll collect enough information to keep track of the minimum, maximum, average
    value, standard deviation, sample count, and the sum of values that we’re recording.
    We record so much information because we can just about guarantee that if we aren’t
    recording it, we’ll probably need it.
  id: totrans-620
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个方法来存储与我们的`log_common()`函数（第5.1.2节中的当前小时和上一小时）具有相似范围的统计数据。我们将收集足够的信息来跟踪记录的最小值、最大值、平均值、标准差、样本计数和值的总和。我们记录如此多的信息是因为我们可以几乎保证，如果我们没有记录它，我们可能需要它。
- en: For a given named context and type, we’ll store a group of values in a `ZSET`.
    We won’t use the `ZSET` for its ability to sort scores, but instead for its ability
    to be unioned against another `ZSET`, keeping only the `MIN` or `MAX` of items
    that intersect. The precise information that we’ll store for that context and
    type is the minimum value, the maximum value, the count of values, the sum of
    the values, and the sum of the squares of the values. With that information, we
    can calculate the average and standard deviation. [Figure 5.3](#ch05fig03) shows
    an example of a `ZSET` holding this information for the `ProfilePage` context
    with statistics on `AccessTime`.
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定的命名上下文和类型，我们将在`ZSET`中存储一组值。我们不会使用`ZSET`来排序分数，而是使用其与其他`ZSET`进行并集的能力，仅保留交集中项目的`MIN`或`MAX`。我们将为该上下文和类型存储的精确信息是最小值、最大值、值的计数、值的总和以及值的平方和的总和。有了这些信息，我们可以计算平均值和标准差。[图5.3](#ch05fig03)显示了`ProfilePage`上下文中包含此信息的`ZSET`示例，以及`AccessTime`的统计信息。
- en: Figure 5.3\. Example access time stats for the profile page. Remember that `ZSET`s
    are sorted by score, which is why our order seems strange compared to our description.
  id: totrans-622
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3.配置文件页面的示例访问时间统计。记住，`ZSET`是按分数排序的，这就是为什么我们的顺序看起来与我们描述的不一样。
- en: '![](05fig03.jpg)'
  id: totrans-623
  prefs: []
  type: TYPE_IMG
  zh: '![图片4](05fig03.jpg)'
- en: Now that we know the type of data that we’ll be storing, how do we get the data
    in there? We’ll start like we did with our common logs by checking to make sure
    that our current data is for the correct hour, moving the old data to an archive
    if it’s not for the current hour. We’ll then construct two temporary `ZSET`s—one
    with the minimum value, the other with the maximum value—and `ZUNIONSTORE` them
    with the current stats with an aggregate of `MIN` and `MAX`, respectively. That’ll
    allow us to quickly update the data without needing to `WATCH` a potentially heavily
    updated stats key. After cleaning up those temporary `ZSET`s, we’ll then `ZINCRBY`
    the `count`, `sum`, and `sumsq` members of the `statsZSET`. Our code for performing
    this operation is shown next.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经知道了将要存储的数据类型，我们该如何将这些数据放入其中呢？我们将像处理我们常见的日志一样开始，检查当前数据是否为正确的小时，如果不是，则将旧数据移动到存档中。然后，我们将构建两个临时的`ZSET`——一个包含最小值，另一个包含最大值——并使用`ZUNIONSTORE`将它们与当前的统计信息结合，分别进行`MIN`和`MAX`的聚合。这将使我们能够快速更新数据，而无需`WATCH`一个可能频繁更新的统计键。在清理这些临时`ZSET`之后，我们将然后对`statsZSET`的`count`、`sum`和`sumsq`成员执行`ZINCRBY`操作。执行此操作的代码如下所示。
- en: Listing 5.6\. The `update_stats()` function
  id: totrans-625
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.6. `update_stats()`函数
- en: '![](ch05ex06-0.jpg)'
  id: totrans-626
  prefs: []
  type: TYPE_IMG
  zh: '![图片2](ch05ex06-0.jpg)'
- en: '![](ch05ex06-1.jpg)'
  id: totrans-627
  prefs: []
  type: TYPE_IMG
  zh: '![图片3](ch05ex06-1.jpg)'
- en: 'We can ignore almost all of the first half of the code listing, since it’s
    a verbatim copy of the rollover code from our `log_common()` function from [section
    5.1.2](#ch05lev2sec2). The latter half does exactly what we described: creating
    temporary `ZSET`s, `ZUNIONSTORE`ing them with our destination `ZSET` with the
    proper aggregates, cleaning the temporary `ZSET`s, and then adding our standard
    statistics information. But what about pulling the statistics information back
    out?'
  id: totrans-628
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以忽略代码列表的前半部分，因为它是我们`log_common()`函数中滚动代码的逐字复制，来自[第5.1.2节](#ch05lev2sec2)。后半部分正好做了我们描述的事情：创建临时的`ZSET`，使用适当的聚合与目标`ZSET`进行`ZUNIONSTORE`操作，清理临时`ZSET`，然后添加我们的标准统计信息。但是，关于如何提取统计信息呢？
- en: To pull the information back out, we need to pull all of the values from the
    `ZSET` and then calculate the average and standard deviation. The average is simply
    the `sum` member divided by the `count` member. But the standard deviation is
    more difficult. With a bit of work, we can derive the standard deviation from
    the information we have, though for the sake of brevity I won’t explain the math
    behind it. Our code for fetching stats is shown here.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提取信息，我们需要从`ZSET`中提取所有值，然后计算平均值和标准差。平均值简单地是`sum`成员除以`count`成员。但标准差更复杂。通过一些工作，我们可以从我们拥有的信息中推导出标准差，尽管为了简洁起见，我不会解释背后的数学。我们获取统计信息的代码如下所示。
- en: Listing 5.7\. The `get_stats()` function
  id: totrans-630
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.7. `get_stats()`函数
- en: '![](100fig01_alt.jpg)'
  id: totrans-631
  prefs: []
  type: TYPE_IMG
  zh: '![图片1](100fig01_alt.jpg)'
- en: Aside from the calculation of the standard deviation, the `get_stats()` function
    isn’t surprising. And for those who’ve spent some time on the Wikipedia page for
    standard deviation, even calculating the standard deviation shouldn’t be all that
    surprising. But with all of this statistical information being stored, how do
    we know what information to look at? We’ll be answering that question and more
    in the next section.
  id: totrans-632
  prefs: []
  type: TYPE_NORMAL
  zh: 除了计算标准差之外，`get_stats()`函数并不令人惊讶。对于那些花了一些时间研究标准差维基百科页面的人来说，甚至计算标准差也不应该那么令人惊讶。但是，由于所有这些统计信息都被存储起来，我们如何知道应该查看哪些信息呢？我们将在下一节回答这个问题以及更多问题。
- en: 5.2.3\. Simplifying our statistics recording and discovery
  id: totrans-633
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 简化我们的统计记录和发现
- en: Now we have our statistics stored in Redis—what next? More specifically, now
    that we have information about (for example) access time on every page, how do
    we discover which pages take a long time on average to generate? Or how do we
    know when it takes significantly longer to generate a page than it did on previous
    occasions? The simple answer is that we need to store more information in a way
    that lets us discover when both situations happen, which we’ll explore in this
    section.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将统计数据存储在Redis中——接下来是什么？更具体地说，现在我们有了关于（例如）每个页面的访问时间的信息，我们如何发现哪些页面的平均生成时间较长？或者我们如何知道生成页面比以往任何时候都要长？简单的答案是，我们需要以让我们发现这两种情况发生的方式存储更多信息，我们将在本节中探讨这一点。
- en: If we want to record access times, then we need to calculate access times. We
    can spend our time adding access time calculations in various places and then
    adding code to record the access times, or we can implement something to help
    us to calculate and record the access times. That same helper could then also
    make that information available in (for example) a `ZSET` of the slowest pages
    to access on average, and could even report on pages that take a long time to
    access compared to other times that page was accessed.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要记录访问时间，那么我们需要计算访问时间。我们可以花费时间在各个地方添加访问时间计算，然后添加代码来记录访问时间，或者我们可以实现一些帮助我们计算和记录访问时间的功能。同样的辅助工具还可以使这些信息在（例如）最慢的页面访问`ZSET`中可用，甚至可以报告与其他访问时间相比访问时间较长的页面。
- en: To help us calculate and record access times, we’ll write a Python context manager^([[1](#ch05fn01)])
    that will wrap our code that we want to calculate and record access times for.
    This context manager will get the current time, let the wrapped code execute,
    and then calculate the total time of execution, record it in Redis, and also update
    a `ZSET` of the highest access time contexts. The next listing shows our context
    manager for performing this set of operations.
  id: totrans-636
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助我们计算和记录访问时间，我们将编写一个Python上下文管理器^([[1](#ch05fn01)])，它将包装我们想要计算和记录访问时间的代码。这个上下文管理器将获取当前时间，让包装的代码执行，然后计算执行的总时间，将其记录在Redis中，并更新最高访问时间上下文的`ZSET`。下面的列表显示了执行这些操作的上下文管理器。
- en: ¹ In Python, a *context manager* is a specially defined function or class that
    will have parts of it executed before and after a given block of code is executed.
    This allows, for example, the easy opening and automatic closing of files.
  id: totrans-637
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 在Python中，*上下文管理器*是一个特别定义的函数或类，它会在执行给定代码块之前和之后执行其部分。例如，这允许轻松打开文件并自动关闭文件。
- en: Listing 5.8\. The `access_time()` context manager
  id: totrans-638
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.8\. `access_time()`上下文管理器
- en: '![](101fig01_alt.jpg)'
  id: totrans-639
  prefs: []
  type: TYPE_IMG
  zh: '![](101fig01_alt.jpg)'
- en: 'There’s some magic going on in the `access_time()` context manager, and it’ll
    probably help to see it in use to understand what’s going on. The following code
    shows the `access_time()` context manager being used to record access times of
    web pages that are served through a similar kind of callback method as part of
    a middleware layer or plugin that was used in our examples from [chapter 2](kindle_split_012.html#ch02):'
  id: totrans-640
  prefs: []
  type: TYPE_NORMAL
  zh: 在`access_time()`上下文管理器中发生了一些魔法般的事情，看到它实际应用可能会帮助我们理解其工作原理。以下代码展示了如何使用`access_time()`上下文管理器来记录通过类似回调方法（作为中间件或插件的一部分）提供的网页的访问时间，这些方法在我们第2章的示例中使用过：
- en: '![](101fig02_alt.jpg)'
  id: totrans-641
  prefs: []
  type: TYPE_IMG
  zh: '![](101fig02_alt.jpg)'
- en: After seeing the example, even if you don’t yet understand how to create a context
    manager, you should at least know how to use one. In this example, we used the
    access time context manager to calculate the total time to generate a web page.
    This context manager could also be used to record the time it takes to make a
    database query or the amount of time it takes to render a template. As an exercise,
    can you think of other types of context managers that could record statistics
    that would be useful? Or can you add reporting of access times that are more than
    two standard deviations above average to the `recent_log()`?
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 即使你还不理解如何创建上下文管理器，看到这个例子后，你也应该至少知道如何使用它。在这个例子中，我们使用了访问时间上下文管理器来计算生成网页的总时间。这个上下文管理器也可以用来记录数据库查询所需的时间或渲染模板所需的时间。作为一个练习，你能想到其他类型的上下文管理器，它们可以记录有用的统计数据吗？或者你能在`recent_log()`中添加报告超过平均两个标准差以上的访问时间？
- en: '|  |'
  id: totrans-643
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Gathering statistics and counters in the real world
  id: totrans-644
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在现实世界中收集统计数据和计数器
- en: I know that we just spent several pages talking about how to gather fairly important
    statistics about how our production systems operate, but let me remind you that
    there are preexisting software packages designed for collecting and plotting counters
    and statistics. My personal favorite is Graphite ([http://graphite.wikidot.com/](http://graphite.wikidot.com/)),
    which you should probably download and install before spending too much time building
    your own data-plotting library.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道我们刚刚花费了几页的篇幅来讨论如何收集关于我们生产系统操作的重要统计数据，但让我提醒你，有一些现成的软件包是专门用于收集和绘制计数器和统计数据的。我个人最喜欢的是Graphite
    ([http://graphite.wikidot.com/](http://graphite.wikidot.com/))，在你花费太多时间构建自己的数据绘图库之前，你可能需要下载并安装它。
- en: '|  |'
  id: totrans-646
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we’ve been recording diverse and important information about the state
    of our application into Redis, knowing more about our visitors can help us to
    answer other questions.
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将关于我们应用程序状态的各种重要信息记录到Redis中，了解更多的访客信息可以帮助我们回答其他问题。
- en: 5.3\. IP-to-city and -country lookup
  id: totrans-648
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. IP到城市和国家的查找
- en: While we’ve been collecting statistics and logs in Redis, we’ve been gathering
    information about visitor behavior in our system. But we’ve been ignoring one
    of the most important parts of visitor behavior—where the visitor is coming from.
    In this section, we’ll build a set of functions that we can use to parse an IP-to-location
    database, and we’ll write a function to look up IP addresses to determine the
    visitor’s city, region (state), and country. Let’s look at an example.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在Redis中收集统计数据和日志时，我们一直在收集关于我们系统中访客行为的资料。但我们一直忽略了访客行为中最重要的一部分——访客来自哪里。在本节中，我们将构建一组函数，我们可以使用这些函数来解析IP到位置数据库，并且我们将编写一个函数来查找IP地址以确定访客的城市、地区（州）和国家。让我们来看一个例子。
- en: As visitors to Fake Game Company’s game have multiplied, players have been coming
    from all over the world to visit and play. Though tools like Google Analytics
    have helped Fake Game Company to understand which major countries their users
    are from, they want to know cities and states to better understand their users.
    It’s our job to use one of the IP address-to-city databases and combine it with
    Redis to discover the locations of players.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 随着假游戏公司游戏的访客数量增加，玩家来自世界各地来访问和玩游戏。尽管像Google Analytics这样的工具已经帮助假游戏公司了解他们的用户来自哪些主要国家，但他们想了解城市和州以更好地了解他们的用户。我们的任务是使用IP地址到城市数据库之一，并将其与Redis结合，以发现玩家的位置。
- en: We use Redis instead of a typical relational database because Redis will generally
    be faster for this (and other) use cases. And we use Redis over local lookup tables
    because the amount of information necessary to locate users is large enough to
    make loading tables on application startup a relatively expensive operation. To
    start using our lookup tables, we first need to load the tables into Redis.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Redis而不是典型的关系型数据库，因为Redis通常在这些（以及其他）用例中会更快。我们使用Redis而不是本地查找表，因为定位用户所需的信息量足够大，以至于在应用程序启动时加载表是一个相对昂贵的操作。要开始使用我们的查找表，我们首先需要将表加载到Redis中。
- en: 5.3.1\. Loading the location tables
  id: totrans-652
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.1\. 加载位置表
- en: 'For development data, I’ve downloaded a free IP-to-city database available
    from [http://dev.maxmind.com/geoip/geolite](http://dev.maxmind.com/geoip/geolite).
    This database contains two important files: GeoLiteCity-Blocks.csv, which contains
    information about ranges of IP addresses and city IDs for those ranges, and GeoLiteCity-Location.csv,
    which contains a mapping of city IDs to the city name, the name of the region/state/province,
    the name of the country, and some other information that we won’t use.'
  id: totrans-653
  prefs: []
  type: TYPE_NORMAL
  zh: 对于开发数据，我已经下载了来自 [http://dev.maxmind.com/geoip/geolite](http://dev.maxmind.com/geoip/geolite)
    的免费 IP 到城市数据库。此数据库包含两个重要文件：GeoLiteCity-Blocks.csv，其中包含 IP 地址范围和这些范围的城市 ID 的信息，以及
    GeoLiteCity-Location.csv，其中包含城市 ID 到城市名称、地区/州/省名称、国家名称以及一些我们不会使用的信息的映射。
- en: We’ll first construct the lookup table that allows us to take an IP address
    and convert it to a city ID. We’ll then construct a second lookup table that allows
    us to take the city ID and convert it to actual city information (city information
    will also include region and country information).
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先构建一个查找表，使我们能够将 IP 地址转换为城市 ID。然后我们构建第二个查找表，使我们能够将城市 ID 转换为实际的城市信息（城市信息还将包括地区和国家信息）。
- en: The table that allows us to find an IP address and turn it into a city ID will
    be constructed from a single `ZSET`, which has a special city ID as the member,
    and an integer value of the IP address as the score. To allow us to map from IP
    address to city ID, we convert dotted-quad format IP addresses to an integer score
    by taking each octet as a byte in an unsigned 32-bit integer, with the first octet
    being the highest bits. Code to perform this operation can be seen here.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 允许我们找到 IP 地址并将其转换为城市 ID 的表将从单个 `ZSET` 构建，该 `ZSET` 以特殊城市 ID 作为成员，并以 IP 地址的整数值作为分数。为了使我们能够从
    IP 地址映射到城市 ID，我们将点分十进制格式的 IP 地址转换为整数分数，通过将每个八位字节作为无符号 32 位整数中的一个字节，第一个八位字节是最高位。执行此操作的代码如下。
- en: Listing 5.9\. The `ip_to_score()` function
  id: totrans-656
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.9\. `ip_to_score()` 函数
- en: '[PRE2]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After we have the score, we’ll add the IP address mapping to city IDs first.
    To construct a unique city ID from each normal city ID (because multiple IP address
    ranges can map to the same city ID), we’ll append a `_` character followed by
    the number of entries we’ve added to the `ZSET` already, as can be seen in the
    next listing.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们获得分数后，首先会将 IP 地址映射到城市 ID。为了从每个普通城市 ID 构建一个唯一的城市 ID（因为多个 IP 地址范围可以映射到同一个城市
    ID），我们会在 `_` 字符后面附加已经添加到 `ZSET` 中的条目数量，如下一列表所示。
- en: Listing 5.10\. The `import_ips_to_redis()` function
  id: totrans-659
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.10\. `import_ips_to_redis()` 函数
- en: '![](103fig01_alt.jpg)'
  id: totrans-660
  prefs: []
  type: TYPE_IMG
  zh: '![图片](103fig01_alt.jpg)'
- en: When our IP addresses have all been loaded by calling `import_ips_to_redis()`,
    we’ll create a `HASH` that maps city IDs to city information, as shown in the
    next listing. We’ll store the city information as a list encoded with JSON, because
    all of our entries are of a fixed format that won’t be changing over time.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们通过调用 `import_ips_to_redis()` 函数将所有 IP 地址加载后，我们将创建一个 `HASH`，将城市 ID 映射到城市信息，如下一列表所示。我们将以
    JSON 编码的列表形式存储城市信息，因为我们的条目都是固定格式，并且不会随时间改变。
- en: Listing 5.11\. The `import_cities_to_redis()` function
  id: totrans-662
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.11\. `import_cities_to_redis()` 函数
- en: '![](103fig02_alt.jpg)'
  id: totrans-663
  prefs: []
  type: TYPE_IMG
  zh: '![图片](103fig02_alt.jpg)'
- en: Now that we have all of our information in Redis, we can start looking up IP
    addresses.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经将所有信息存储在 Redis 中，我们可以开始查找 IP 地址。
- en: 5.3.2\. Looking up cities
  id: totrans-665
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.3.2\. 查找城市
- en: To support looking up IP addresses, we added integer scores to a `ZSET` to represent
    the beginning of IP address ranges for a given city ID. In order to find a city
    given an IP address, we map the IP address to a similarly calculated score and
    then find the city ID that has the largest starting IP address less than or equal
    to the IP address we pass. We can use `ZREVRANGEBYSCORE` with the optional `START`
    and `NUM` arguments set to 0 and 1, respectively, to fetch this information. After
    we’ve discovered the city ID, we can fetch the city information from our `HASH`.
    Our function for finding which city an IP address is in can be seen next.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持查找 IP 地址，我们在 `ZSET` 中添加了整数分数来表示给定城市 ID 的 IP 地址范围的开始。为了找到给定 IP 地址的城市，我们将
    IP 地址映射到类似计算出的分数，然后找到具有最大起始 IP 地址小于或等于我们传递的 IP 地址的城市 ID。我们可以使用 `ZREVRANGEBYSCORE`
    并将可选的 `START` 和 `NUM` 参数分别设置为 0 和 1 来获取此信息。在我们发现城市 ID 后，我们可以从我们的 `HASH` 中获取城市信息。我们用于查找
    IP 地址所在城市的函数如下。
- en: Listing 5.12\. The `find_city_by_ip()` function
  id: totrans-667
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.12\. `find_city_by_ip()` 函数
- en: '![](104fig01_alt.jpg)'
  id: totrans-668
  prefs: []
  type: TYPE_IMG
  zh: '![图片](104fig01_alt.jpg)'
- en: We can now look up city information based on IP address and begin to analyze
    where our users are coming from. This method of converting data into integers
    for use with a `ZSET` is useful, and can greatly simplify the discovery of individual
    items or ranges. We’ll talk more about these kinds of data transformations in
    [chapter 7](kindle_split_018.html#ch07). But for now, let’s look at how we can
    use Redis to help us find and connect to other servers and services.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以根据IP地址查找城市信息，并开始分析我们的用户来自哪里。这种将数据转换为整数以用于`ZSET`的方法很有用，可以极大地简化单个项目或范围的发现。我们将在[第7章](kindle_split_018.html#ch07)中更多地讨论这类数据转换。但就目前而言，让我们看看如何使用Redis帮助我们找到并连接到其他服务器和服务。
- en: 5.4\. Service discovery and configuration
  id: totrans-670
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4\. 服务发现和配置
- en: As your use of Redis and other services grows over time, you’ll eventually come
    to a situation where keeping configuration information can get out of hand. It’s
    not a big deal when you have one Redis server, one database server, and one web
    server. But when you have a Redis master with a few slaves, or different Redis
    servers for different applications, or even master and slave database servers,
    keeping all of that configuration can be a pain.
  id: totrans-671
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你对Redis和其他服务的使用随着时间的推移而增长，你最终会面临一个情况，即保持配置信息可能会变得难以控制。当你只有一个Redis服务器、一个数据库服务器和一个Web服务器时，这并不是什么大问题。但是，当你有一个Redis主服务器和一些从服务器，或者为不同的应用程序有不同的Redis服务器，甚至有主从数据库服务器时，保持所有这些配置可能会变得很麻烦。
- en: Typically, configuration information for connecting to different services and
    servers is contained in configuration files that are stored on disk. And in situations
    where a machine breaks down, a network connection goes down, or something else
    causes us to need to connect to a different server, we’ll usually need to update
    a number of configuration files in one of a number of locations. In this section,
    we’ll talk about how we can move much of our configuration out of files and into
    Redis, which will let applications almost configure themselves.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，连接到不同服务和服务器所需的配置信息包含在存储在磁盘上的配置文件中。在机器故障、网络连接中断或其他原因导致我们需要连接到不同的服务器时，我们通常需要更新多个位置中的一个或多个配置文件。在本节中，我们将讨论如何将大部分配置从文件中移出，放入Redis中，这将使应用程序几乎可以自行配置。
- en: Let’s start with a simple live configuration to see how Redis can help us.
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单的实时配置开始，看看Redis如何帮助我们。
- en: 5.4.1\. Using Redis to store configuration information
  id: totrans-674
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.1\. 使用Redis存储配置信息
- en: 'To see how generally difficult configuration management can be, we only need
    to look at the simplest of configurations: a flag to tell our web servers whether
    we’re under maintenance. If so, we shouldn’t make requests against the database,
    and should instead return a simple “Sorry, we’re under maintenance; try again
    later” message to visitors. If the site isn’t under maintenance, all of the normal
    web-serving behavior should happen.'
  id: totrans-675
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解配置管理可能有多困难，我们只需看看最简单的配置：一个标志来告诉我们的Web服务器我们是否在维护中。如果是这样，我们就不应向数据库发送请求，而应向访客返回一个简单的“抱歉，我们正在维护中；稍后再试”的消息。如果网站不在维护中，所有正常的Web服务行为都应该发生。
- en: In a typical situation, updating that single flag can force us to push updated
    configuration files to all of our web servers, and may force us to reload configurations
    on all of our servers, if not force us to restart our application servers themselves.
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型情况下，更新那个单个标志可能会迫使我们向所有Web服务器推送更新的配置文件，并可能迫使我们重新加载所有服务器的配置，如果不是迫使我们的应用程序服务器重启。
- en: Instead of trying to write and maintain configuration files as our number of
    services grows, let’s instead write our configuration to Redis. By putting our
    configuration in Redis and by writing our application to fetch configuration information
    from Redis, we no longer need to write tools to push out configuration information
    and cause our servers and services to reload that configuration.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是试图随着服务数量的增长编写和维护配置文件，让我们将配置写入Redis。通过将配置放入Redis并将我们的应用程序编写为从Redis获取配置信息，我们不再需要编写推送配置信息的工具，并导致我们的服务器和服务重新加载该配置。
- en: To implement this simple behavior, we’ll assume that we’ve built a middleware
    layer or plugin like we used for caching in [chapter 2](kindle_split_012.html#ch02)
    that will return our maintenance page if a simple `is_under_maintenance()` function
    returns `True`, or will handle the request like normal if it returns `False`.
    Our actual function will check for a key called `is-under-maintenance`. If the
    key has any value stored there, we’ll return `True`; otherwise, we’ll return `False`.
    To help minimize the load to Redis under heavy web server load (because people
    love to hit Refresh when they get maintenance pages), we’ll only update our information
    once per second. Our function can be seen in this listing.
  id: totrans-678
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这种简单的行为，我们假设我们已经构建了一个中间件层或插件，就像我们在第2章中用于缓存的那样，如果简单的`is_under_maintenance()`函数返回`True`，它将返回我们的维护页面；如果返回`False`，它将像正常一样处理请求。我们的实际函数将检查一个名为`is-under-maintenance`的键。如果该键中存储了任何值，我们将返回`True`；否则，我们返回`False`。为了帮助减少在Web服务器负载下对Redis的负载（因为人们喜欢在得到维护页面时刷新），我们只每秒更新一次信息。我们的函数可以在下面的列表中看到。
- en: Listing 5.13\. The `is_under_maintenance()` function
  id: totrans-679
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.13\. `is_under_maintenance()`函数
- en: '![](105fig01_alt.jpg)'
  id: totrans-680
  prefs: []
  type: TYPE_IMG
  zh: '![](105fig01_alt.jpg)'
- en: With that one function plugged into the right place in our application, we could
    affect the behavior of thousands of web servers within 1 second. We chose 1 second
    to help reduce load against Redis for very heavily trafficked web sites, but we
    can reduce or remove that part of the function if our needs require faster updates.
    This seems like a toy example, but it demonstrates the power of keeping configuration
    information in a commonly accessible location. But what about more intricate configuration
    options?
  id: totrans-681
  prefs: []
  type: TYPE_NORMAL
  zh: 将该功能插入到我们应用程序的正确位置后，我们可以在1秒内影响数千个Web服务器的行为。我们选择1秒是为了帮助减少对Redis的负载，这对于流量非常大的网站来说是有帮助的，但如果我们的需求需要更快的更新，我们可以减少或移除该功能的一部分。这看起来像是一个玩具示例，但它展示了将配置信息保存在一个常用位置中的力量。但关于更复杂的配置选项呢？
- en: 5.4.2\. One Redis server per application component
  id: totrans-682
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.2\. 每个应用程序组件一个 Redis 服务器
- en: As countless developers have discovered during our increasing use of Redis,
    at some point we outgrow our first Redis server. Maybe we need to log more information,
    maybe we need more space for caching, or maybe we’ve already skipped ahead and
    are using one of the more advanced services described in later chapters. For whatever
    reason, we’ll need more servers.
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们不断增加Redis的使用过程中，无数的开发者发现，在某个时刻，我们会超出我们最初的一个Redis服务器。也许我们需要记录更多信息，也许我们需要更多的缓存空间，或者也许我们已经跳到了下一章中描述的更高级的服务之一。无论出于什么原因，我们都需要更多的服务器。
- en: To help with the ease of transitioning to more servers, I recommend running
    one Redis server for every separate part of your application—one for logging,
    one for statistics, one for caching, one for cookies, and so forth. Note that
    you can run multiple Redis servers on a single machine; they just need to run
    on different ports. Alternatively, if you want to reduce your system administration
    load, you can also use different “databases” in Redis. Either way, by having different
    data split up into different key spaces, your transition to more or larger servers
    is somewhat simplified. Unfortunately, as your number of servers and/or Redis
    databases increases, managing and distributing configuration information for all
    of those servers becomes more of a chore.
  id: totrans-684
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助简化向更多服务器过渡的过程，我建议为应用程序的每个独立部分运行一个Redis服务器——一个用于日志记录，一个用于统计，一个用于缓存，一个用于cookies，等等。请注意，您可以在一台机器上运行多个Redis服务器；它们只需要在不同的端口上运行。或者，如果您想减少系统管理负担，您也可以在Redis中使用不同的“数据库”。无论如何，通过将不同的数据分散到不同的键空间中，您向更多或更大的服务器过渡的过程将得到一定程度的简化。不幸的是，随着服务器和/或Redis数据库数量的增加，管理和分配所有这些服务器的配置信息变得更加繁琐。
- en: In the previous section, we used Redis as our source for configuration information
    about whether we should serve a maintenance page. We can again use Redis to tell
    us information about other Redis servers. More specifically, let’s use a single
    known Redis server as a directory of configuration information to discover how
    to connect to all of the other Redis servers that provide data for different application
    or service components. While we’re at it, we’ll build it in such a way that when
    configurations change, we’ll connect to the correct servers. Our implementation
    will be more generic than this example calls for, but I’m sure that after you
    start using this method for getting configuration information, you’ll start using
    it for non-Redis servers and services.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们使用了 Redis 作为我们配置信息的来源，以确定是否应该提供维护页面。我们同样可以使用 Redis 来获取关于其他 Redis 服务器的信息。更具体地说，让我们使用一个已知的
    Redis 服务器作为配置信息的目录，以发现如何连接到所有提供不同应用程序或服务组件数据的其他 Redis 服务器。在此过程中，我们将以这种方式构建它，以便当配置发生变化时，我们将连接到正确的服务器。我们的实现将比这个示例所要求的更通用，但我相信，在你开始使用这种方法获取配置信息后，你将开始将其用于非
    Redis 服务器和服务。
- en: We’ll build a function that will fetch a JSON-encoded configuration value from
    a key that’s named after the type of service and the application component that
    service is for. For example, if we wanted to fetch connection information for
    the Redis server that holds statistics, we’d fetch the key `config:redis:statistics`.
    The following listing shows the code for setting configurations.
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将构建一个函数，它将从以服务类型和该服务针对的应用程序组件命名的键中获取 JSON 编码的配置值。例如，如果我们想获取存储统计数据的 Redis 服务器的连接信息，我们将获取键
    `config:redis:statistics`。以下列表显示了设置配置的代码。
- en: Listing 5.14\. The `set_config()` function
  id: totrans-687
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.14. `set_config()` 函数
- en: '[PRE3]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: With this `set_config()` function, we can set any JSON-encodable configuration
    that we may want. With a slight change in semantics and a `get_config()` function
    structured similarly to our earlier `is_under_maintenance()` function, we could
    replace `is_under_maintenance()`. Consult the following listing for a function
    that matches `set_config()` and will allow us to locally cache configuration information
    for 1 second, 10 seconds, or 0 seconds, depending on our needs.
  id: totrans-689
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个 `set_config()` 函数，我们可以设置任何可 JSON 编码的配置。通过语义的轻微变化以及一个与我们的早期 `is_under_maintenance()`
    函数结构相似的 `get_config()` 函数，我们可以替换 `is_under_maintenance()`。请参考以下列表，以获取与 `set_config()`
    匹配的函数，并允许我们根据需要本地缓存配置信息 1 秒、10 秒或 0 秒。
- en: Listing 5.15\. The `get_config()` function
  id: totrans-690
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.15. `get_config()` 函数
- en: '![](107fig01_alt.jpg)'
  id: totrans-691
  prefs: []
  type: TYPE_IMG
  zh: '![图片](107fig01_alt.jpg)'
- en: Now that we have a pair of functions for getting and setting configurations,
    we can go farther. We started down this path of storing and fetching configurations
    in order to set up and create connections to a variety of different Redis servers.
    But the first argument to almost every function that we’ve written so far is a
    connection argument. Rather than needing to manually fetch connections for the
    variety of services that we’re using, let’s build a method to help us automatically
    connect to these services.
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一对获取和设置配置的函数，我们可以更进一步。我们开始存储和检索配置，以便设置和创建连接到各种不同的 Redis 服务器。但到目前为止，我们几乎每个函数的第一个参数都是一个连接参数。而不是需要手动获取我们使用的各种服务的连接，让我们构建一个方法来帮助我们自动连接到这些服务。
- en: 5.4.3\. Automatic Redis connection management
  id: totrans-693
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.4.3. 自动 Redis 连接管理
- en: Manually creating and passing connections to Redis can be tough. Not only do
    we need to repeatedly refer to configuration information, but if we’re using our
    configuration management functions from the last section, we still need to fetch
    the configuration, connect to Redis, and somehow deal with the connection when
    we’re done. To simplify the management of all of these connections, we’ll write
    a decorator that will take care of connecting to all of our Redis servers (except
    for the configuration server).
  id: totrans-694
  prefs: []
  type: TYPE_NORMAL
  zh: 手动创建和传递 Redis 连接可能会很困难。我们不仅需要反复引用配置信息，而且如果我们使用上一节中的配置管理函数，我们仍然需要获取配置，连接到 Redis，并在完成后以某种方式处理连接。为了简化所有这些连接的管理，我们将编写一个装饰器，它将负责连接到我们所有的
    Redis 服务器（除了配置服务器）。
- en: '|  |'
  id: totrans-695
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Decorators
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 装饰器
- en: Within Python there’s a syntax for passing a function X into another function
    Y. This function Y is called a *decorator*. Decorators are given an opportunity
    to alter the behavior of function X. Some decorators validate arguments, other
    decorators register callbacks, and even others manage connections like we intend
    to.
  id: totrans-697
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，有一种将函数X传递给另一个函数Y的语法。这个函数Y被称为*装饰器*。装饰器有机会改变函数X的行为。一些装饰器验证参数，其他装饰器注册回调，甚至还有其他装饰器管理连接，就像我们打算做的那样。
- en: '|  |'
  id: totrans-698
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Our decorator will take a named configuration as an argument, which will generate
    a wrapper that, when called on the actual function, will wrap the function such
    that later calls will automatically connect to the appropriate Redis server, and
    that connection will be passed to the wrapped function with all of the other arguments
    that were later provided. The next listing has the source for our `redis_connection()`
    function.
  id: totrans-699
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的装饰器将接受一个命名配置作为参数，这将生成一个包装器，当在实际函数上调用时，将包装该函数，使得后续调用将自动连接到适当的Redis服务器，并且该连接将连同后来提供的所有其他参数一起传递给包装函数。下一列表中包含了我们的`redis_connection()`函数的源代码。
- en: Listing 5.16\. The `redis_connection()` function/decorator
  id: totrans-700
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.16。`redis_connection()`函数/装饰器
- en: '![](108fig01_alt.jpg)'
  id: totrans-701
  prefs: []
  type: TYPE_IMG
  zh: '![](108fig01_alt.jpg)'
- en: '|  |'
  id: totrans-702
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Combining `*args` and `**kwargs`
  id: totrans-703
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 结合`*args`和`**kwargs`
- en: 'Way back in [chapter 1](kindle_split_011.html#ch01), we first looked at default
    arguments in Python. But here, we’re combining two different forms of argument
    passing. If you’re having difficulty understanding what’s going on (which is essentially
    capturing all positional and named arguments in the `args` and `kwargs` variables
    in the function definition, and passing all positional and named parameters to
    the called function), then you should spend some time with the Python language
    tutorial via this shortened URL: [http://mng.bz/KM5x](http://mng.bz/KM5x).'
  id: totrans-704
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_011.html#ch01)中，我们首先了解了Python中的默认参数。但在这里，我们正在结合两种不同的参数传递形式。如果你理解起来有困难（这本质上是在函数定义中的`args`和`kwargs`变量中捕获所有位置参数和命名参数，并将所有位置参数和命名参数传递给被调用的函数），那么你应该花些时间通过这个缩短的URL学习Python语言教程：[http://mng.bz/KM5x](http://mng.bz/KM5x)。
- en: '|  |'
  id: totrans-705
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: I know that this group of nested functions can be confusing at first, but it
    really isn’t that bad. We have a function, `redis_connection()`, that takes the
    named application component and returns a wrapper function. That wrapper function
    is then called with the function we want to pass a connection to (the wrapped
    function), which then returns the function caller. This caller handles all of
    the work of getting configuration information, connecting to Redis, and calling
    our wrapped function. Though it’s a mouthful to describe, actually using it is
    convenient, as you can see by applying it in the next listing to our `log_recent()`
    function from [section 5.1.1](#ch05lev2sec1).
  id: totrans-706
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道这个嵌套函数组一开始可能会让人感到困惑，但实际上并没有那么糟糕。我们有一个函数，`redis_connection()`，它接受命名应用程序组件并返回一个包装函数。然后，这个包装函数被用来调用我们想要传递连接的函数（即包装函数），然后返回函数调用者。这个调用者处理获取配置信息、连接到Redis和调用我们的包装函数的所有工作。虽然描述起来很复杂，但实际上使用它很方便，就像你在下一列表中看到的那样，将其应用于[第5.1.1节](#ch05lev2sec1)中的`log_recent()`函数。
- en: Listing 5.17\. The decorated `log_recent()` function
  id: totrans-707
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表5.17。装饰过的`log_recent()`函数
- en: '![](109fig01_alt.jpg)'
  id: totrans-708
  prefs: []
  type: TYPE_IMG
  zh: '![](109fig01_alt.jpg)'
- en: '|  |'
  id: totrans-709
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Decorators
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 装饰器
- en: In addition to the strange argument passing with `*args` and `**kwargs` from
    [listing 5.16](#ch05ex16), we’re also using syntax to “decorate” the log function.
    That is to say, we pass a function to a decorator, which performs some manipulation
    on the function before returning the original function, or something else. You
    can read up on the details of what’s going on and why at [http://www.python.org/dev/peps/pep-0318/](http://www.python.org/dev/peps/pep-0318/).
  id: totrans-711
  prefs: []
  type: TYPE_NORMAL
  zh: 除了[列表5.16](#ch05ex16)中的`*args`和`**kwargs`的奇怪参数传递之外，我们还使用了语法来“装饰”日志函数。也就是说，我们传递一个函数给装饰器，装饰器在返回原始函数或其它东西之前对函数进行一些操作。你可以阅读有关正在发生的事情及其原因的详细信息，请参阅[http://www.python.org/dev/peps/pep-0318/](http://www.python.org/dev/peps/pep-0318/)。
- en: '|  |'
  id: totrans-712
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that you’ve seen how to use the `redis_connection()` decorator on `log_recent()`,
    it doesn’t seem so bad, does it? With this better method of handling connections
    and configuration, we’ve just removed a handful of lines from almost every function
    that we’ll be calling. As an exercise, try to add this decorator to the `access_time()`
    context manager from [section 5.2.3](#ch05lev2sec5) so that we don’t need to pass
    a connection. Feel free to reuse this decorator with all of the other examples
    in the book.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了如何在`log_recent()`上使用`redis_connection()`装饰器，它并不那么糟糕，对吧？有了这种更好的处理连接和配置的方法，我们几乎从我们将要调用的每个函数中移除了一小部分代码。作为一个练习，尝试将这个装饰器添加到[第5.2.3节](#ch05lev2sec5)中的`access_time()`上下文管理器中，这样我们就不需要传递连接了。请随意将此装饰器与其他书中的所有其他示例一起重用。
- en: 5.5\. Summary
  id: totrans-714
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5. 摘要
- en: All of the topics that we’ve covered in this chapter have directly or indirectly
    been written to support applications. These functions and decorators are meant
    to help you start using Redis as a way of supporting different parts of your application
    over time. Logging, counters, and statistics are there to offer direct insight
    into how your application is performing. IP-to-location lookup can tell you where
    your consumers are located. And storing service discovery and configuration can
    save a lot of effort because of not needing to manually handle connections.
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中我们直接或间接讨论的所有主题都是为了支持应用程序而编写的。这些函数和装饰器旨在帮助您随着时间的推移开始使用Redis作为支持应用程序不同部分的方式。日志记录、计数器和统计信息可以提供对应用程序性能的直接洞察。IP到位置查找可以告诉您您的消费者位于何处。存储服务发现和配置可以节省大量精力，因为不需要手动处理连接。
- en: Now that we have a solid foundation for supporting applications in Redis, [chapter
    6](kindle_split_017.html#ch06) will continue down this path to functions that
    can be used as building blocks of your application.
  id: totrans-716
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经为支持Redis中的应用程序打下了坚实的基础，[第6章](kindle_split_017.html#ch06)将继续沿着这条路径深入到可以作为应用程序构建块使用的功能。
- en: Chapter 6\. Application components in Redis
  id: totrans-717
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章. Redis中的应用组件
- en: '*This chapter covers*'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Building two prefix-matching autocomplete methods
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建两个前缀匹配自动完成方法
- en: Creating a distributed lock to improve performance
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建分布式锁以提升性能
- en: Developing counting semaphores to control concurrency
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发用于控制并发的计数信号量
- en: Two task queues for different use cases
  id: totrans-722
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适用于不同用例的两个任务队列
- en: Pull messaging for delayed message delivery
  id: totrans-723
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于延迟消息传递的拉取消息
- en: Handling file distribution
  id: totrans-724
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理文件分发
- en: In the last few chapters, we’ve gone through some basic use cases and tools
    to help build applications in Redis. In this chapter, we’ll get into more useful
    tools and techniques, working toward building bigger pieces of applications in
    Redis.
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们介绍了一些基本用例和工具，以帮助在Redis中构建应用程序。在本章中，我们将探讨更多有用的工具和技术，致力于在Redis中构建更大的应用程序组件。
- en: We’ll begin by building autocomplete functions to quickly find users in short
    and long lists of items. We’ll then take some time to carefully build two different
    types of locks to reduce data contention, improve performance, prevent data corruption,
    and reduce wasted work. We’ll construct a delayed task queue, only to augment
    it later to allow for executing a task at a specific time with the use of the
    lock we just created. Building on the task queues, we’ll build two different messaging
    systems to offer point-to-point and broadcast messaging services. We’ll then reuse
    our earlier IP-address-to-city/-country lookup from [chapter 5](kindle_split_016.html#ch05),
    and apply it to billions of log entries that are stored and distributed via Redis.
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先构建自动完成函数，以便快速在短列表和长列表中查找用户。然后，我们将花一些时间仔细构建两种不同类型的锁，以减少数据竞争，提高性能，防止数据损坏，并减少浪费的工作。我们将构建一个延迟任务队列，稍后将其增强以允许使用我们刚刚创建的锁在特定时间执行任务。在任务队列的基础上，我们将构建两个不同的消息系统，以提供点对点和广播消息服务。然后，我们将重用[第5章](kindle_split_016.html#ch05)中较早的IP地址到城市/国家查找，并将其应用于通过Redis存储和分发的数十亿条日志条目。
- en: Each component offers usable code and solutions for solving these specific problems
    in the context of two example companies. But our solutions contain techniques
    that can be used for other problems, and our specific solutions can be applied
    to a variety of personal, public, or commercial projects.
  id: totrans-727
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组件都提供了可用的代码和解决方案，用于解决在两个示例公司背景下这些特定问题的解决方案。但我们的解决方案包含可用于其他问题的技术，并且我们的特定解决方案可以应用于各种个人、公共或商业项目。
- en: To start, let’s look at a fictional web-based game company called Fake Game
    Company, which currently has more than a million daily players of its games on
    YouTwitFace, a fictional social network. Later we’ll look at a web/mobile startup
    called Fake Garage Startup that does mobile and web instant messaging.
  id: totrans-728
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看一个名为假游戏公司的虚构网络游戏公司，该公司目前在YouTwitFace（一个虚构的社会网络）上拥有超过一百万的日活跃玩家。稍后我们将探讨一个名为假车库创业公司的虚构网络/移动初创公司，它提供移动和网页即时通讯服务。
- en: 6.1\. Autocomplete
  id: totrans-729
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. 自动完成
- en: In the web world, *autocomplete* is a method that allows us to quickly look
    up things that we want to find without searching. Generally, it works by taking
    the letters that we’re typing and finding all words that start with those letters.
    Some autocomplete tools will even let us type the beginning of a phrase and finish
    the phrase for us. As an example, autocomplete in Google’s search shows us that
    Betty White’s SNL appearance is still popular, even years later (which is no surprise—she’s
    a firecracker). It shows us the URLs we’ve recently visited and want to revisit
    when we type in the address bar, and it helps us remember login names. All of
    these functions and more are built to help us access information faster. Some
    of them, like Google’s search box, are backed by many terabytes of remote information.
    Others, like our browser history and login boxes, are backed by much smaller local
    databases. But they all get us what we want with less work.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络世界中，*自动完成*是一种允许我们快速查找我们想要找到的东西的方法，而无需搜索。通常，它是通过获取我们输入的字母并找到所有以这些字母开头的单词来工作的。一些自动完成工具甚至可以让我们输入短语的开头，然后为我们完成整个短语。例如，谷歌搜索中的自动完成显示贝蒂·怀特在周六夜现场（SNL）的亮相仍然很受欢迎，即使是在多年之后（这并不令人惊讶——她是个火花）。它显示了我们最近访问并希望重新访问的URL，并帮助我们记住登录名。所有这些功能以及更多都是为了帮助我们更快地获取信息。其中一些，如谷歌的搜索框，背后支持着数以TB计的远程信息。其他一些，如我们的浏览器历史记录和登录框，背后支持着更小的本地数据库。但它们都能以更少的劳动为我们提供我们想要的东西。
- en: We’ll build two different types of autocomplete in this section. The first uses
    lists to remember the most recent 100 contacts that a user has communicated with,
    trying to minimize memory use. Our second autocomplete offers better performance
    and scalability for larger lists, but uses more memory per list. They differ in
    their structure, the methods used, and the time it takes for the operations to
    complete. Let’s first start with an autocomplete for recent contacts.
  id: totrans-731
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建两种不同的自动完成类型。第一种使用列表来记住用户最近沟通的100个联系人，试图最小化内存使用。我们的第二种自动完成提供了更好的性能和可扩展性，适用于更大的列表，但每个列表使用的内存更多。它们在结构、使用的方法以及操作完成所需的时间上有所不同。让我们首先从最近联系人的自动完成开始。
- en: 6.1.1\. Autocomplete for recent contacts
  id: totrans-732
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1. 最近联系人自动完成
- en: The purpose of this autocomplete is to keep a list of the most recent users
    that each player has been in contact with. To increase the social aspect of the
    game and to allow people to quickly find and remember good players, Fake Game
    Company is looking to create a contact list for their client to remember the most
    recent 100 people that each user has chatted with. On the client side, when someone
    is trying to start a chat, they can start typing the name of the person they want
    to chat with, and autocomplete will show the list of users whose screen names
    start with the characters they’ve typed. [Figure 6.1](#ch06fig01) shows an example
    of this kind of autocompletion.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 这个自动完成的目的是保留每个玩家最近接触过的用户列表。为了增加游戏的社会性，并让人们快速找到并记住优秀的玩家，假游戏公司正在为其客户创建一个联系人列表，以便记住每个用户最近聊天的100个人。在客户端，当有人尝试开始聊天时，他们可以开始输入他们想要聊天的人的名字，自动完成将显示以他们输入的字符开头的用户屏幕名列表。[图6.1](#ch06fig01)展示了这种自动完成的示例。
- en: Figure 6.1\. A recent contacts autocomplete showing users with names starting
    with *je*
  id: totrans-734
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1. 最近联系人自动完成示例，显示以*je*开头的用户
- en: '![](06fig01.jpg)'
  id: totrans-735
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig01.jpg)'
- en: Because each of the millions of users on the server will have their own list
    of their most recent 100 contacts, we need to try to minimize memory use, while
    still offering the ability to quickly add and remove users from the list. Because
    Redis `LIST`s keep the order of items consistent, and because `LIST`s use minimal
    memory compared to some other structures, we’ll use them to store our autocomplete
    lists. Unfortunately, `LIST`s don’t offer enough functionality to actually perform
    the autocompletion inside Redis, so we’ll perform the actual autocomplete outside
    of Redis, but inside of Python. This lets us use Redis to store and update these
    lists using a minimal amount of memory, leaving the relatively easy filtering
    to Python.
  id: totrans-736
  prefs: []
  type: TYPE_NORMAL
  zh: 由于服务器上数百万的用户将各自拥有他们最近的 100 个联系人的列表，我们需要尽量减少内存使用，同时仍然提供快速添加和删除用户的能力。因为 Redis
    `LIST` 保持项目顺序的一致性，并且与一些其他结构相比，`LIST` 使用最少的内存，我们将使用它们来存储我们的自动完成列表。不幸的是，`LIST` 并没有提供足够的功能来在
    Redis 内部实际执行自动完成，所以我们将实际的自动完成操作放在 Python 中执行。这让我们可以使用 Redis 以最少的内存存储和更新这些列表，而将相对容易的过滤操作留给
    Python。
- en: 'Generally, three operations need to be performed against Redis in order to
    deal with the recent contacts autocomplete lists. The first operation is to add
    or update a contact to make them the most recent user contacted. To perform this
    operation, we need to perform these steps:'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们需要对 Redis 执行三个操作来处理最近的联系人自动完成列表。第一个操作是添加或更新联系人，使他们成为最近联系的用户。要执行此操作，我们需要执行以下步骤：
- en: '**1**.  Remove the contact from the list if it exists.'
  id: totrans-738
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 如果存在，从列表中删除联系人。'
- en: '**2**.  Add the contact to the beginning of the list.'
  id: totrans-739
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 将联系人添加到列表的起始位置。'
- en: '**3**.  Trim the list if it now has more than 100 items.'
  id: totrans-740
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 如果列表现在有超过 100 项，则对其进行修剪。'
- en: We can perform these operations with `LREM`, `LPUSH`, and `LTRIM`, in that order.
    To make sure that we don’t have any race conditions, we’ll use a `MULTI`/`EXEC`
    transaction around our commands like I described in [chapter 3](kindle_split_014.html#ch03).
    The complete function is shown in this next listing.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 `LREM`、`LPUSH` 和 `LTRIM` 按此顺序执行这些操作。为了确保我们没有任何竞争条件，我们将使用 `MULTI`/`EXEC`
    事务来包围我们的命令，就像我在第 3 章中描述的那样。完整的函数在接下来的列表中展示。
- en: Listing 6.1\. The `add_update_contact()` function
  id: totrans-742
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.1\. `add_update_contact()` 函数
- en: '![](112fig01_alt.jpg)'
  id: totrans-743
  prefs: []
  type: TYPE_IMG
  zh: '![图片](112fig01_alt.jpg)'
- en: As I mentioned, we removed the user from the `LIST` (if they were present),
    pushed the user onto the left side of the `LIST`; then we trimmed the `LIST` to
    ensure that it didn’t grow beyond our limit.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我提到的，我们从 `LIST` 中移除了用户（如果他们存在），然后将用户推送到 `LIST` 的左侧；然后我们修剪 `LIST` 以确保它不会超过我们的限制。
- en: 'The second operation that we’ll perform is to remove a contact if the user
    doesn’t want to be able to find them anymore. This is a quick `LREM` call, which
    can be seen as follows:'
  id: totrans-745
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要执行的第二个操作是，如果用户不想再能找到他们，就删除联系人。这是一个快速的 `LREM` 调用，如下所示：
- en: '[PRE4]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The final operation that we need to perform is to fetch the autocomplete list
    itself to find the matching users. Again, because we’ll perform the actual autocomplete
    processing in Python, we’ll fetch the whole `LIST`, and then process it in Python,
    as shown next.
  id: totrans-747
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要执行的最后一个操作是获取自动完成列表本身以找到匹配的用户。同样，由于我们将在 Python 中执行实际的自动完成处理，我们将获取整个 `LIST`，然后在
    Python 中进行处理，如下所示。
- en: Listing 6.2\. The `fetch_autocomplete_list()` function
  id: totrans-748
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. `fetch_autocomplete_list()` 函数
- en: '![](113fig01_alt.jpg)'
  id: totrans-749
  prefs: []
  type: TYPE_IMG
  zh: '![图片](113fig01_alt.jpg)'
- en: Again, we fetch the entire autocomplete `LIST`, filter it by whether the name
    starts with the necessary prefix, and return the results. This particular operation
    is simple enough that we could even push it off to the client if we find that
    our server is spending too much time computing it, only requiring a refetch on
    update.
  id: totrans-750
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们获取整个自动完成 `LIST`，通过是否以必要的前缀开头进行过滤，并返回结果。这个特定的操作足够简单，以至于如果我们发现服务器在计算它上花费了太多时间，我们甚至可以将它推到客户端，只需要在更新时重新获取即可。
- en: This autocomplete will work fine for our specific example. It won’t work as
    well if the lists grow significantly larger, because to remove an item takes time
    proportional to the length of the list. But because we were concerned about space,
    and have explicitly limited our lists to 100 users, it’ll be fast enough. If you
    find yourself in need of much larger most- or least-recently-used lists, you can
    use `ZSET`s with timestamps instead.
  id: totrans-751
  prefs: []
  type: TYPE_NORMAL
  zh: 这种自动完成对于我们的特定例子来说将工作得很好。如果列表显著增大，它可能不会工作得很好，因为删除一个项目所需的时间与列表的长度成比例。但由于我们关注空间，并且明确将我们的列表限制在100个用户，它将足够快。如果你发现自己需要更大的最常使用或最少使用列表，你可以使用带有时间戳的`ZSET`。
- en: 6.1.2\. Address book autocomplete
  id: totrans-752
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2. 地址簿自动完成
- en: In the previous example, Redis was used primarily to keep track of the contact
    list, not to actually perform the autocomplete. This is okay for short lists,
    but for longer lists, fetching thousands or millions of items to find just a handful
    would be a waste. Instead, for autocomplete lists with many items, we must find
    matches inside Redis.
  id: totrans-753
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，Redis主要用于跟踪联系人列表，而不是实际执行自动完成。对于短列表来说，这是可以接受的，但对于较长的列表，检索成千上万或数百万个项目以找到少数几个将会是浪费。相反，对于包含许多项目的自动完成列表，我们必须在Redis内部找到匹配项。
- en: Going back to Fake Game Company, the recent contacts chat autocomplete is one
    of the most-used social features of our game. Our number-two feature, in-game
    mailing, has been gaining momentum. To keep the momentum going, we’ll add an autocomplete
    for mailing. But in our game, we only allow users to send mail to other users
    that are in the same in-game social group as they are, which we call a *guild*.
    This helps to prevent abusive and unsolicited messages between users.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 回到“假游戏公司”，最近的联系人聊天自动完成是我们游戏中使用最频繁的社会功能之一。我们的第二大功能，游戏内邮件系统，正在逐渐受到关注。为了保持这种势头，我们将为邮件系统添加自动完成功能。但在我们的游戏中，我们只允许用户向与他们处于同一游戏内社交团体（我们称之为“公会”）的其他用户发送邮件。这有助于防止用户之间发送滥用和不请自来的信息。
- en: Guilds can grow to thousands of members, so we can’t use our old `LIST`-based
    autocomplete method. But because we only need one autocomplete list per guild,
    we can use more space per member. To minimize the amount of data to be transferred
    to clients who are autocompleting, we’ll perform the autocomplete prefix calculation
    inside Redis using `ZSET`s.
  id: totrans-755
  prefs: []
  type: TYPE_NORMAL
  zh: 公会可以扩展到数千名成员，所以我们不能使用我们旧的基于`LIST`的自动完成方法。但由于我们只需要每个公会一个自动完成列表，我们可以为每个成员使用更多的空间。为了最小化需要传输到正在自动完成的客户端的数据量，我们将在Redis内部使用`ZSET`执行自动完成前缀计算。
- en: 'To store each autocomplete list will be different than other `ZSET` uses that
    you’ve seen before. Mostly, we’ll use `ZSET`s for their ability to quickly tell
    us whether an item is in the `ZSET`, what position (or index) a member occupies,
    and to quickly pull ranges of items from anywhere inside the `ZSET`. What makes
    this use different is that all of our scores will be zero. By setting our scores
    to zero, we use a secondary feature of `ZSET`s: `ZSET`s sort by member names when
    scores are equal. When all scores are zero, all members are sorted based on the
    binary ordering of the strings. In order to actually perform the autocomplete,
    we’ll insert lowercased contact names. Conveniently enough, we’ve only ever allowed
    users to have letters in their names, so we don’t need to worry about numbers
    or symbols.'
  id: totrans-756
  prefs: []
  type: TYPE_NORMAL
  zh: 存储每个自动完成列表的方式将不同于你之前看到的其他`ZSET`使用方式。大多数情况下，我们将使用`ZSET`来快速判断一个项目是否在`ZSET`中，一个成员占据什么位置（或索引），以及快速从`ZSET`内部任何位置拉取一系列项目。这种使用方式的不同之处在于，我们所有的分数都将为零。通过将分数设置为零，我们使用了`ZSET`的二级特性：当分数相等时，`ZSET`按成员名称排序。当所有分数都是零时，所有成员将根据字符串的二元排序进行排序。为了实际执行自动完成，我们将插入小写的联系人名称。幸运的是，我们只允许用户在他们的名字中使用字母，所以我们不需要担心数字或符号。
- en: What do we do? Let’s start by thinking of names as a sorted sequence of strings
    like `abc`, `abca`, `abcb`, ... `abd`. If we’re looking for words with a prefix
    of *abc*, we’re really looking for strings that are after `abbz`... and before
    `abd`. If we knew the rank of the first item that is before `abbz`... and the
    last item after `abd`, we could perform a `ZRANGE` call to fetch items between
    them. But, because we don’t know whether either of those items are there, we’re
    stuck. To become unstuck, all we really need to do is to insert items that we
    know are after `abbz`... and before `abd`, find their ranks, make our `ZRANGE`
    call, and then remove our start and end members.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 我们该怎么办？让我们首先将名称视为一个排序后的字符串序列，如 `abc`、`abca`、`abcb`、... `abd`。如果我们正在寻找以 *abc*
    为前缀的单词，我们实际上是在寻找在 `abbz`... 之后并且 `abd`... 之前的字符串。如果我们知道在 `abbz`... 之前的第一项和 `abd`...
    之后的最后一项的排名，我们可以执行一个 `ZRANGE` 调用来获取它们之间的项。但是，因为我们不知道这两项中是否有任何一项存在，所以我们陷入了困境。为了摆脱困境，我们真正需要做的只是插入我们知道在
    `abbz`... 之后并且 `abd`... 之前的项，找到它们的排名，执行我们的 `ZRANGE` 调用，然后移除我们的起始和结束成员。
- en: 'The good news is that finding an item that’s before `abd` but still after all
    valid names with a prefix of `abc` is easy: we concatenate a `{` (left curly brace)
    character onto the end of our prefix, giving us `abc{`. Why `{`? Because it’s
    the next character in ASCII after *z*. To find the start of our range for `abc`,
    we could also concatenate `{` to `abb`, getting `abb{`, but what if our prefix
    was `aba` instead of `abc`? How do we find a character before `a`? We take a hint
    from our use of the curly brace, and note that the character that precedes *a*
    in ASCII is `` ` `` (back quote). So if our prefix is `aba`, our start member
    will be `` ab` ``, and our end member will be `aba{`.'
  id: totrans-758
  prefs: []
  type: TYPE_NORMAL
  zh: 好消息是，找到一个在 `abd` 之前但仍然在所有以 `abc` 为前缀的有效名称之后的项是很容易的：我们在前缀的末尾添加一个 `{`（左花括号）字符，得到
    `abc{`。为什么是 `{`？因为它在 ASCII 码中是字母 *z* 后面的下一个字符。为了找到 `abc` 的起始范围，我们也可以将 `{` 添加到
    `abb`，得到 `abb{`，但如果我们的前缀是 `aba` 而不是 `abc` 呢？我们如何找到一个在 `a` 之前的字符？我们从使用花括号中得到提示，并注意
    ASCII 码中在 *a* 之前的是 `` ` ``（反引号）。所以如果我们的前缀是 `aba`，我们的起始成员将是 `` ab` ``，而我们的结束成员将是
    `aba{`。
- en: Putting it all together, we’ll find the predecessor of our prefix by replacing
    the last character of our prefix with the character that came right before it.
    We’ll find the successor of our prefix by concatenating a curly brace. To prevent
    any issues with two prefix searches happening at the same time, we’ll concatenate
    a curly brace onto our prefix (for post-filtering out endpoint items if necessary).
    A function that will generate these types of ranges can be seen next.
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些放在一起，我们将通过将前缀的最后一个字符替换为它前面的字符来找到前缀的前驱。我们将通过连接一个花括号来找到前缀的后继。为了防止同时发生两个前缀搜索的问题，我们将一个花括号连接到我们的前缀（如果需要，用于后过滤端点项）。下面将展示一个可以生成这些类型范围的函数。
- en: Listing 6.3\. The `find_prefix_range()` function
  id: totrans-760
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. `find_prefix_range()` 函数
- en: '![](114fig01_alt.jpg)'
  id: totrans-761
  prefs: []
  type: TYPE_IMG
  zh: '![](114fig01_alt.jpg)'
- en: I know, it can be surprising to have spent so many paragraphs describing what
    we’re going to do, only to end up with just a few lines that actually implement
    it. But if we look at what we’re doing, we’re just finding the last character
    in the prefix in our presorted sequence of characters (using the bisect module),
    and then looking up the character that came just before it.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，花这么多段落描述我们要做什么，最后只留下几行实际实现它的代码，可能会让人感到惊讶。但如果我们看看我们在做什么，我们只是在我们的预排序字符序列中找到前缀的最后一个字符（使用
    bisect 模块），然后查找它前面的字符。
- en: '|  |'
  id: totrans-763
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Character sets and internationalization
  id: totrans-764
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 字符集和国际化
- en: This method of finding the preceding and following characters in ASCII works
    really well for languages with characters that only use characters *a*-*z*. But
    when confronted with characters that aren’t in this range, you’ll find a few new
    challenges.
  id: totrans-765
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在 ASCII 中查找前后字符的方法对于只使用字符 *a*-*z* 的语言来说效果非常好。但是，当面对不在该范围内的字符时，你会遇到一些新的挑战。
- en: First, you’ll have to find a method that turns all of your characters into bytes;
    three common encodings include UTF-8, UTF-16, and UTF-32 (big-endian and little-endian
    variants of UTF-16 and UTF-32 are used, but only big-endian versions work in this
    situation). Second, you’ll need to find the range of characters that you intend
    to support, ensuring that your chosen encoding leaves at least one character before
    your supported range and one character after your selected range in the encoded
    version. Third, you need to use these characters to replace the back quote character
    `` ` `` and the left curly brace character `{` in our example.
  id: totrans-766
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要找到一个方法将所有字符转换为字节；三种常见的编码包括UTF-8、UTF-16和UTF-32（UTF-16和UTF-32的大端和小端变体都被使用，但在此情况下只有大端版本有效）。其次，您需要找到您打算支持的字符范围，确保所选编码在编码版本中在您的支持范围之前至少留有一个字符，在所选范围之后也至少留有一个字符。第三，您需要使用这些字符来替换示例中的反引号字符
    `` ` `` 和左花括号字符 `{`。
- en: Thankfully, our algorithm doesn’t care about the native sort order of the characters,
    only the encodings. So you can pick UTF-8 or big-endian UTF-16 or UTF-32, use
    a null to replace the back quote, and use the maximum value that your encoding
    and language supports to replace the left curly brace. (Some language bindings
    are somewhat limited, allowing only up to Unicode code point U+ffff for UTF-16
    and Unicode code point U+2ffff for UTF-32.)
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们的算法并不关心字符的本地排序顺序，只关心编码。因此，您可以选择UTF-8或大端UTF-16或UTF-32，使用空字符来替换反引号，并使用您的编码和语言支持的最大的值来替换左花括号。（某些语言绑定有些受限，只允许UTF-16最多到Unicode代码点U+ffff，UTF-32最多到Unicode代码点U+2ffff。）
- en: '|  |'
  id: totrans-768
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: After we have the range of values that we’re looking for, we need to insert
    our ending points into the `ZSET`, find the rank of those newly added items, pull
    some number of items between them (we’ll fetch at most 10 to avoid overwhelming
    the user), and then remove our added items. To ensure that we’re not adding and
    removing the same items, as would be the case if two members of the same guild
    were trying to message the same user, we’ll also concatenate a 128-bit randomly
    generated UUID to our start and end members. To make sure that the `ZSET` isn’t
    being changed when we try to find and fetch our ranges, we’ll use `WATCH` with
    `MULTI` and `EXEC` after we’ve inserted our endpoints. The full autocomplete function
    is shown here.
  id: totrans-769
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们得到我们正在寻找的值的范围之后，我们需要将我们的结束点插入到 `ZSET` 中，找到这些新添加项目的排名，从中拉取一些项目（我们将最多获取10个项目以避免压倒用户），然后移除我们添加的项目。为了确保我们没有添加和移除相同的项，就像同一公会中的两个成员试图给同一用户发消息的情况一样，我们还将一个128位随机生成的UUID连接到我们的起始点和结束成员。为了确保当我们尝试查找和获取我们的范围时，`ZSET`
    没有被更改，我们在插入端点后使用 `WATCH` 与 `MULTI` 和 `EXEC`。完整的自动完成功能在此处显示。
- en: Listing 6.4\. The `autocomplete_on_prefix()` function
  id: totrans-770
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4\. `autocomplete_on_prefix()` 函数
- en: '![](115fig01_alt.jpg)'
  id: totrans-771
  prefs: []
  type: TYPE_IMG
  zh: '![](115fig01_alt.jpg)'
- en: Most of this function is bookkeeping and setup. The first part is just getting
    our start and ending points, followed by adding them to the guild’s autocomplete
    `ZSET`. When we have everything in the `ZSET`, we `WATCH` the `ZSET` to ensure
    that we discover if someone has changed it, fetch the ranks of the start and end
    points in the `ZSET`, fetch items between the endpoints, and clean up after ourselves.
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这个功能都是记账和设置。第一部分只是获取我们的起始点和结束点，然后将其添加到公会的自动完成 `ZSET` 中。当我们把所有东西都放在 `ZSET`
    中后，我们 `WATCH` `ZSET` 以确保我们能够发现是否有人更改了它，获取起始点和结束点在 `ZSET` 中的排名，获取端点之间的项目，并在自己之后清理。
- en: 'To add and remove members from a guild is straightforward: we only need to
    `ZADD` and `ZREM` the user from the guild’s `ZSET`. Both of these functions are
    shown here.'
  id: totrans-773
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加和移除公会成员，很简单：我们只需要将用户从公会的 `ZSET` 中 `ZADD` 和 `ZREM`。这两个函数在此处显示。
- en: Listing 6.5\. The `join_guild()` and `leave_guild()` functions
  id: totrans-774
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5\. `join_guild()` 和 `leave_guild()` 函数
- en: '[PRE5]'
  id: totrans-775
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Joining or leaving a guild, at least when it comes to autocomplete, is straightforward.
    We only need to add or remove names from the `ZSET`.
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 加入或离开公会，至少在自动完成方面，很简单。我们只需要在 `ZSET` 中添加或移除名称。
- en: This method of adding items to a `ZSET` to create a range—fetching items in
    the range and then removing those added items—can be useful. Here we use it for
    autocomplete, but this technique can also be used for arbitrary sorted indexes.
    In [chapter 7](kindle_split_018.html#ch07), we’ll talk about a technique for improving
    these kinds of operations for a few different types of range queries, which removes
    the need to add and remove items at the endpoints. We’ll wait to talk about the
    other method, because it only works on some types of data, whereas this method
    works on range queries over any type of data.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 这种向`ZSET`添加项目以创建范围的方法——获取范围内的项目然后移除这些添加的项目——可能很有用。在这里，我们用它来实现自动完成，但这项技术也可以用于任意排序索引。在第7章（kindle_split_018.html#ch07）中，我们将讨论一种改进这些操作的技术，适用于几种不同类型的范围查询，这样可以避免在端点添加和移除项目。我们将稍后再讨论另一种方法，因为它只适用于某些类型的数据，而这种方法适用于任何类型数据的范围查询。
- en: When we added our endpoints to the `ZSET`, we needed to be careful about other
    users trying to autocomplete at the same time, which is why we use the `WATCH`
    command. As our load increases, we may need to retry our operations often, which
    can be wasteful. The next section talks about a way to avoid retries, improve
    performance, and sometimes simplify our code by reducing and/or replacing `WATCH`
    with locks.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将端点添加到`ZSET`时，我们需要小心其他用户同时尝试自动完成，这就是为什么我们使用`WATCH`命令的原因。随着负载的增加，我们可能需要经常重试我们的操作，这可能是浪费的。下一节将讨论一种避免重试、提高性能以及有时通过减少和/或用锁替换`WATCH`来简化我们代码的方法。
- en: 6.2\. Distributed locking
  id: totrans-779
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 分布式锁
- en: Generally, when you “lock” data, you first *acquire* the lock, giving you exclusive
    access to the data. You then perform your operations. Finally, you *release* the
    lock to others. This sequence of acquire, operate, release is pretty well known
    in the context of shared-memory data structures being accessed by threads. In
    the context of Redis, we’ve been using `WATCH` as a replacement for a lock, and
    we call it *optimistic locking*, because rather than actually preventing others
    from modifying the data, we’re notified if someone else changes the data before
    we do it ourselves.
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当你“锁定”数据时，你首先*获取*锁，这让你可以独占访问数据。然后你执行你的操作。最后，你*释放*锁，让其他人可以访问。这种获取、操作、释放的顺序在由线程访问的共享内存数据结构的上下文中相当知名。在Redis的上下文中，我们一直使用`WATCH`作为锁的替代品，我们称之为*乐观锁*，因为我们不是真正阻止其他人修改数据，而是在我们操作之前通知我们数据已被其他人更改。
- en: With distributed locking, we have the same sort of acquire, operate, release
    operations, but instead of having a lock that’s only known by threads within the
    same process, or processes on the same machine, we use a lock that different Redis
    clients on different machines can acquire and release. When and whether to use
    locks or `WATCH` will depend on a given application; some applications don’t need
    locks to operate correctly, some only require locks for parts, and some require
    locks at every step.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式锁中，我们有相同的获取、操作、释放操作，但与只有同一进程内的线程或同一机器上的进程知道的锁不同，我们使用不同机器上的不同Redis客户端可以获取和释放的锁。何时以及是否使用锁或`WATCH`将取决于特定的应用程序；有些应用程序不需要锁来正确运行，有些只需要在部分操作中使用锁，而有些则需要在每个步骤中都使用锁。
- en: One reason why we spend so much time building locks with Redis instead of using
    operating system–level locks, language-level locks, and so forth, is a matter
    of scope. Clients want to have exclusive access to data stored on Redis, so clients
    need to have access to a lock defined in a scope that all clients can see—Redis.
    Redis *does* have a basic sort of lock already available as part of the command
    set (`SETNX`), which we use, but it’s not full-featured and doesn’t offer advanced
    functionality that users would expect of a distributed lock.
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之所以花费大量时间用Redis而不是使用操作系统级锁、语言级锁等来构建锁，是因为范围的问题。客户端希望独占访问存储在Redis上的数据，因此客户端需要访问一个所有客户端都可以看到的锁——Redis。Redis*确实*有一个基本的锁作为命令集的一部分（`SETNX`）可用，我们使用了它，但它不是功能齐全的，并且不提供用户期望的分布式锁的高级功能。
- en: Throughout this section, we’ll talk about how an overloaded `WATCH`ed key can
    cause performance issues, and build a lock piece by piece until we can replace
    `WATCH` for some situations.
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一个过载的`WATCH`ed键如何导致性能问题，并逐步构建锁，直到我们可以替换某些情况下的`WATCH`。
- en: 6.2.1\. Why locks are important
  id: totrans-784
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 为什么锁很重要
- en: In the first version of our autocomplete, we added and removed items from a
    `LIST`. We did so by wrapping our multiple calls with a `MULTI`/`EXEC` pair. Looking
    all the way back to [section 4.6](kindle_split_015.html#ch04lev1sec6), we first
    introduced `WATCH`/`MULTI`/`EXEC` transactions in the context of an in-game item
    marketplace. If you remember, the market is structured as a single `ZSET`, with
    members being an object and owner ID concatenated, along with the item price as
    the score. Each user has their own `HASH`, with columns for user name, currently
    available funds, and other associated information. [Figure 6.2](#ch06fig02) shows
    an example of the marketplace, user inventories, and user information.
  id: totrans-785
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们自动完成功能的第一版中，我们从`LIST`中添加和移除商品。我们通过将多个调用包裹在`MULTI`/`EXEC`对中来做到这一点。回顾到[第4.6节](kindle_split_015.html#ch04lev1sec6)，我们首先在游戏内商品市场的背景下介绍了`WATCH`/`MULTI`/`EXEC`事务。如果你记得，市场结构为一个单一的`ZSET`，成员是一个对象和所有者ID的拼接，以及商品价格作为分数。每个用户都有自己的`HASH`，包含用户名、当前可用资金和其他相关信息的列。[图6.2](#ch06fig02)展示了市场、用户库存和用户信息的一个示例。
- en: Figure 6.2\. The structure of our marketplace from [section 4.6](kindle_split_015.html#ch04lev1sec6).
    There are four items in the market on the left—ItemA, ItemC, ItemE, and ItemG—with
    prices 35, 48, 60, and 73, and seller IDs of 4, 7, 2, and 3, respectively. In
    the middle we have two users, Frank and Bill, and their current funds, with their
    inventories on the right.
  id: totrans-786
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2\. 来自[第4.6节](kindle_split_015.html#ch04lev1sec6)的市场结构。市场左侧有四个商品——ItemA、ItemC、ItemE和ItemG，价格分别为35、48、60和73，卖家ID分别为4、7、2和3。中间有两个用户，Frank和Bill，以及他们的当前资金，他们的库存位于右侧。
- en: '![](06fig02_alt.jpg)'
  id: totrans-787
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig02_alt.jpg)'
- en: You remember that to add an item to the marketplace, we `WATCH` the seller’s
    inventory to make sure the item is still available, add the item to the market
    `ZSET`, and remove it from the user’s inventory. The core of our earlier `list_item()`
    function from [section 4.4.2](kindle_split_015.html#ch04lev2sec11) is shown next.
  id: totrans-788
  prefs: []
  type: TYPE_NORMAL
  zh: 你记得，为了将商品添加到市场，我们需要`WATCH`卖家的库存以确保商品仍然可用，然后将商品添加到市场的`ZSET`中，并从用户的库存中移除。我们之前在[第4.4.2节](kindle_split_015.html#ch04lev2sec11)中展示的`list_item()`函数的核心如下。
- en: Listing 6.6\. The `list_item()` function from [section 4.4.2](kindle_split_015.html#ch04lev2sec11)
  id: totrans-789
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.6\. 来自[第4.4.2节](kindle_split_015.html#ch04lev2sec11)的`list_item()`函数
- en: '![](118fig01_alt.jpg)'
  id: totrans-790
  prefs: []
  type: TYPE_IMG
  zh: '![图片](118fig01_alt.jpg)'
- en: The short comments in this code just hide a lot of the setup and `WATCH`/`MULTI`/`EXEC`
    handling that hide the core of what we’re doing, which is why I omitted it here.
    If you feel like you need to see that code again, feel free to jump back to [section
    4.4.2](kindle_split_015.html#ch04lev2sec11) to refresh your memory.
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码中的简短注释只是隐藏了大量的设置和`WATCH`/`MULTI`/`EXEC`处理，这些处理隐藏了我们所做事情的核心，这就是为什么我在这里省略了它。如果你觉得你需要再次查看那段代码，请随时跳转到[第4.4.2节](kindle_split_015.html#ch04lev2sec11)以刷新你的记忆。
- en: Now, to review our purchasing of an item, we `WATCH` the market and the buyer’s
    `HASH`. After fetching the buyer’s total funds and the price of the item, we verify
    that the buyer has enough money. If the buyer has enough money, we transfer money
    between the accounts, add the item to the buyer’s inventory, and remove the item
    from the market. If the buyer doesn’t have enough money, we cancel the transaction.
    If a `WATCH` error is caused by someone else writing to the market `ZSET` or the
    buyer `HASH` changing, we retry. The following listing shows the core of our earlier
    `purchase_item()` function from [section 4.4.3](kindle_split_015.html#ch04lev2sec12).
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，为了回顾我们的商品购买过程，我们`WATCH`市场和买家的`HASH`。在获取买家的总资金和商品价格后，我们验证买家是否有足够的钱。如果买家有足够的钱，我们在账户间转账，将商品添加到买家的库存中，并从市场中移除商品。如果买家没有足够的钱，我们取消交易。如果由于其他人向市场`ZSET`或买家`HASH`更改而引起`WATCH`错误，我们重试。以下列表展示了我们之前在[第4.4.3节](kindle_split_015.html#ch04lev2sec12)中展示的`purchase_item()`函数的核心。
- en: Listing 6.7\. The `purchase_item()` function from [section 4.4.3](kindle_split_015.html#ch04lev2sec12)
  id: totrans-793
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7\. 来自[第4.4.3节](kindle_split_015.html#ch04lev2sec12)的`purchase_item()`函数
- en: '![](118fig02_alt.jpg)'
  id: totrans-794
  prefs: []
  type: TYPE_IMG
  zh: '![图片](118fig02_alt.jpg)'
- en: As before, we omit the setup and `WATCH`/`MULTI`/`EXEC` handling to focus on
    the core of what we’re doing.
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们省略了设置和`WATCH`/`MULTI`/`EXEC`处理，以专注于我们所做事情的核心。
- en: 'To see the necessity of locks at scale, let’s take a moment to simulate the
    marketplace in a few different loaded scenarios. We’ll have three different runs:
    one listing and one buying process, then five listing processes and one buying
    process, and finally five listing and five buying processes. [Table 6.1](#ch06table01)
    shows the result of running this simulation.'
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到在规模上锁的必要性，让我们花一点时间模拟在不同负载场景下的市场。我们将进行三次不同的运行：一个发布和一个购买过程，然后是五个发布过程和一个购买过程，最后是五个发布和五个购买过程。[表6.1](#ch06table01)显示了这次模拟的结果。
- en: Table 6.1\. Performance of a heavily loaded marketplace over 60 seconds
  id: totrans-797
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.1\. 60秒内高负载市场的性能
- en: '|   | Listed items | Bought items | Purchase retries | Average wait per purchase
    |'
  id: totrans-798
  prefs: []
  type: TYPE_TB
  zh: '|   | 发布的商品 | 购买的商品 | 购买重试次数 | 每次购买的平均等待时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-799
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 lister, 1 buyer | 145,000 | 27,000 | 80,000 | 14ms |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '| 1个发布者，1个买家 | 145,000 | 27,000 | 80,000 | 14ms |'
- en: '| 5 listers, 1 buyer | 331,000 | <200 | 50,000 | 150ms |'
  id: totrans-801
  prefs: []
  type: TYPE_TB
  zh: '| 5个发布者，1个买家 | 331,000 | <200 | 50,000 | 150ms |'
- en: '| 5 listers, 5 buyers | 206,000 | <600 | 161,000 | 498ms |'
  id: totrans-802
  prefs: []
  type: TYPE_TB
  zh: '| 5个发布者，5个买家 | 206,000 | <600 | 161,000 | 498ms |'
- en: As our overloaded system pushes its limits, we go from roughly a 3-to-1 ratio
    of retries per completed sale with one listing and buying process, all the way
    up to 250 retries for every completed sale. As a result, the latency to complete
    a sale increases from under 10 milliseconds in the moderately loaded system, all
    the way up to nearly 500 milliseconds in the overloaded system. This is a perfect
    example of why `WATCH`/`MULTI`/`EXEC` transactions sometimes don’t scale at load,
    and it’s caused by the fact that while trying to complete a transaction, we fail
    and have to retry over and over. Keeping our data correct is important, but so
    is actually getting work done. To get past this limitation and actually start
    performing sales at scale, we must make sure that we only list or sell one item
    in the marketplace at any one time. We do this by using a lock.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的系统负载过重，我们从一个列表和购买过程完成的每个销售的重试比率从大约3比1增加到250次重试。因此，完成一个销售的平均延迟从中等负载系统下的不到10毫秒增加到负载过重系统下的近500毫秒。这是一个完美的例子，说明了为什么`WATCH`/`MULTI`/`EXEC`事务有时在负载下无法扩展，这是由于在尝试完成事务时，我们失败并不得不一次又一次地重试。保持数据正确很重要，但完成工作也同样重要。为了克服这一限制并真正开始以规模进行销售，我们必须确保在任何时候只在一个市场上列出或销售一个商品。我们通过使用锁来实现这一点。
- en: 6.2.2\. Simple locks
  id: totrans-804
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 简单锁
- en: In our first simple version of a lock, we’ll take note of a few different potential
    failure scenarios. When we actually start building the lock, we won’t handle all
    of the failures right away. We’ll instead try to get the basic acquire, operate,
    and release process working right. After we have that working and have demonstrated
    how using locks can actually improve performance, we’ll address any failure scenarios
    that we haven’t already addressed.
  id: totrans-805
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们锁的第一个简单版本中，我们将注意几个不同的潜在故障场景。当我们真正开始构建锁时，我们不会立即处理所有故障。相反，我们将尝试使基本的获取、操作和释放过程正确工作。在我们完成这项工作并证明使用锁实际上可以改善性能之后，我们将解决我们尚未解决的任何故障场景。
- en: While using a lock, sometimes clients can fail to release a lock for one reason
    or another. To protect against failure where our clients may crash and leave a
    lock in the acquired state, we’ll eventually add a *timeout*, which causes the
    lock to be released automatically if the process that has the lock doesn’t finish
    within the given time.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用锁的过程中，有时客户可能因为各种原因而无法释放锁。为了防止我们的客户崩溃并留下已获取状态的锁，我们最终会添加一个**超时**，如果在给定时间内持有锁的进程没有完成，锁将自动释放。
- en: 'Many users of Redis already know about locks, locking, and lock timeouts. But
    sadly, many implementations of locks in Redis are only *mostly* correct. The problem
    with mostly correct locks is that they’ll fail in ways that we don’t expect, precisely
    when we don’t expect them to fail. Here are some situations that can lead to incorrect
    behavior, and in what ways the behavior is incorrect:'
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 许多Redis用户已经了解锁、锁定和锁超时。但遗憾的是，Redis中许多锁的实现只**基本上**是正确的。基本上正确的锁的问题是它们会在我们意想不到的情况下失败，正是我们不想它们失败的时候。以下是一些可能导致不正确行为的情况，以及行为不正确的方式：
- en: A process acquired a lock, operated on data, but took too long, and the lock
    was automatically released. The process doesn’t know that it lost the lock, or
    may even release the lock that some other process has since acquired.
  id: totrans-808
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个进程获取了锁，操作了数据，但耗时过长，锁被自动释放。进程不知道它已经失去了锁，甚至可能释放了其他进程已经获取的锁。
- en: A process acquired a lock for an operation that takes a long time and crashed.
    Other processes that want the lock don’t know what process had the lock, so can’t
    detect that the process failed, and waste time waiting for the lock to be released.
  id: totrans-809
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个进程获取了一个耗时较长的操作锁并崩溃了。其他想要锁的进程不知道哪个进程持有锁，因此无法检测到进程失败，并浪费了等待锁释放的时间。
- en: One process had a lock, but it timed out. Other processes try to acquire the
    lock simultaneously, and multiple processes are able to get the lock.
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个进程持有锁，但超时了。其他进程尝试同时获取锁，并且多个进程能够获取到锁。
- en: Because of a combination of the first and third scenarios, many processes now
    hold the lock and all believe that they are the only holders.
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于第一和第三种情况的综合作用，现在许多进程都持有锁，并且所有进程都认为自己是唯一的持有者。
- en: Even if each of these problems had a one-in-a-million chance of occurring, because
    Redis can perform 100,000 operations per second on recent hardware (and up to
    225,000 operations per second on high-end hardware), those problems can come up
    when under heavy load,^([[1](#ch06fn01)]) so it’s important to get locking right.
  id: totrans-812
  prefs: []
  type: TYPE_NORMAL
  zh: 即使这些问题的发生概率仅为百万分之一，因为Redis在最新硬件上每秒可以执行100,000次操作（在高端硬件上高达225,000次操作），在重负载下这些问题仍然可能发生，^([[1](#ch06fn01)])
    因此正确地实现锁机制非常重要。
- en: ¹ Having tested a few available Redis lock implementations that include support
    for timeouts, I was able to induce lock duplication on at least half of the lock
    implementations with just five clients acquiring and releasing the same lock over
    10 seconds.
  id: totrans-813
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 经过测试几个可用的Redis锁实现，这些实现包括对超时的支持，我能够通过五个客户端在10秒内获取和释放相同的锁，至少在半数锁实现中诱导出锁重复。
- en: 6.2.3\. Building a lock in Redis
  id: totrans-814
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 在Redis中构建锁
- en: Building a *mostly* correct lock in Redis is easy. Building a *completely* correct
    lock in Redis isn’t much more difficult, but requires being extra careful about
    the operations we use to build it. In this first version, we’re not going to handle
    the case where a lock times out, or the case where the holder of the lock crashes
    and doesn’t release the lock. Don’t worry; we’ll get to those cases in the next
    section, but for now, let’s just get basic locking correct.
  id: totrans-815
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中构建一个*基本正确*的锁是容易的。在Redis中构建一个*完全正确*的锁并不困难，但需要格外小心我们用来构建它的操作。在这个第一个版本中，我们不会处理锁超时的情况，或者锁持有者崩溃而没有释放锁的情况。不用担心；我们将在下一节中讨论这些情况，但现在，让我们先确保基本的锁机制正确无误。
- en: The first part of making sure that no other code can run is to acquire the lock.
    The natural building block to use for acquiring a lock is the `SETNX` command,
    which will only set a value if the key doesn’t already exist. We’ll set the value
    to be a unique identifier to ensure that no other process can get the lock, and
    the unique identifier we’ll use is a 128-bit randomly generated UUID.
  id: totrans-816
  prefs: []
  type: TYPE_NORMAL
  zh: 确保没有其他代码可以运行的第一个步骤是获取锁。用于获取锁的自然构建块是`SETNX`命令，它只有在键不存在时才会设置值。我们将设置一个唯一的标识符以确保没有其他进程可以获取锁，我们将使用的唯一标识符是一个128位的随机生成的UUID。
- en: If we fail to acquire the lock initially, we’ll retry until we acquire the lock,
    or until a specified timeout has passed, whichever comes first, as shown here.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们最初未能获取锁，我们将重试，直到获取锁或直到指定的超时时间过去，以先到者为准，如下所示。
- en: Listing 6.8\. The `acquire_lock()` function
  id: totrans-818
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.8\. `acquire_lock()`函数
- en: '![](120fig01_alt.jpg)'
  id: totrans-819
  prefs: []
  type: TYPE_IMG
  zh: '![120fig01_alt.jpg](120fig01_alt.jpg)'
- en: As described, we’ll attempt to acquire the lock by using `SETNX` to set the
    value of the lock’s key only if it doesn’t already exist. On failure, we’ll continue
    to attempt this until we’ve run out of time (which defaults to 10 seconds).
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 如描述所述，我们将尝试使用`SETNX`来设置锁键的值，前提是它尚未存在。在失败的情况下，我们将继续尝试，直到耗尽时间（默认为10秒）。
- en: Now that we have the lock, we can perform our buying or selling without `WATCH`
    errors getting in our way. We’ll acquire the lock and, just like before, check
    the price of the item, make sure that the buyer has enough money, and if so, transfer
    the money and item. When completed, we release the lock. The code for this can
    be seen next.
  id: totrans-821
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了锁，我们可以执行买卖操作而不会因为`WATCH`错误而受阻。我们将获取锁，就像之前一样，检查物品的价格，确保买家有足够的钱，如果有，就转移钱和物品。完成后，我们释放锁。这个代码的示例见下文。
- en: Listing 6.9\. The `purchase_item_with_lock()` function
  id: totrans-822
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.9. `purchase_item_with_lock()`函数
- en: '![](ch06ex09-0.jpg)'
  id: totrans-823
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex09-0.jpg)'
- en: '![](ch06ex09-1.jpg)'
  id: totrans-824
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex09-1.jpg)'
- en: Looking through the code listing, it almost seems like we’re locking the operation.
    But don’t be fooled—we’re locking the market data, and the lock must exist while
    we’re operating on the data, which is why it surrounds the code performing the
    operation.
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看代码列表，几乎感觉我们是在锁定操作。但不要被误导——我们是在锁定市场数据，并且锁必须在操作数据时存在，这就是为什么它围绕着执行操作的代码。
- en: To release the lock, we have to be at least as careful as when acquiring the
    lock. Between the time when we acquired the lock and when we’re trying to release
    it, someone may have done bad things to the lock. To release the lock, we need
    to `WATCH` the lock key, and then check to make sure that the value is still the
    same as what we set it to before we delete it. This also prevents us from releasing
    a lock multiple times. The `release_lock()` function is shown next.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 要释放锁，我们必须至少像获取锁时那样小心。在我们获取锁和尝试释放锁之间，有人可能对锁做了坏事。为了释放锁，我们需要`WATCH`锁键，然后检查确保删除之前设置的值仍然是相同的。这也防止了我们多次释放锁。下一个示例是`release_lock()`函数。
- en: Listing 6.10\. The `release_lock()` function
  id: totrans-827
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.10. `release_lock()`函数
- en: '![](122fig01_alt.jpg)'
  id: totrans-828
  prefs: []
  type: TYPE_IMG
  zh: '![图片](122fig01_alt.jpg)'
- en: We take many of the same steps to ensure that our lock hasn’t changed as we
    did with our money transfer in the first place. But if you think about our release
    lock function for long enough, you’ll (reasonably) come to the conclusion that,
    except in very rare situations, we don’t need to repeatedly loop. But the next
    version of the acquire lock function that supports timeouts, if accidentally mixed
    with earlier versions (also unlikely, but anything is possible with code), could
    cause the release lock transaction to fail and could leave the lock in the acquired
    state for longer than necessary. So, just to be extra careful, and to guarantee
    correctness in as many situations as possible, we’ll err on the side of caution.
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采取了与最初进行货币转移时相同的许多步骤来确保我们的锁没有改变。但如果你长时间思考我们的释放锁函数，你将（合理地）得出结论，除了非常罕见的情况外，我们不需要重复循环。但下一个支持超时的获取锁函数版本，如果意外地与早期版本（也不太可能，但代码中任何事都可能发生），可能会导致释放锁事务失败，并且可能使锁保持在获取状态的时间比必要的更长。所以，为了格外小心，并尽可能保证在各种情况下正确，我们将采取谨慎的态度。
- en: After we’ve wrapped our calls with locks, we can perform the same simulation
    of buying and selling as we did before. In [table 6.2](#ch06table02), we have
    new rows that use the lock-based buying and selling code, which are shown below
    our earlier rows.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们用锁封装了调用之后，我们可以执行与之前相同的买卖模拟。在[表6.2](#ch06table02)中，我们添加了使用基于锁的买卖代码的新行，这些行位于我们之前的行下方。
- en: Table 6.2\. Performance of locking over 60 seconds
  id: totrans-831
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.2. 60秒内锁定的性能
- en: '|   | Listed items | Bought items | Purchase retries | Average wait per purchase
    |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '|   | 列出的物品 | 购买的物品 | 购买重试次数 | 每次购买的平均等待时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-833
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 lister, 1 buyer, no lock | 145,000 | 27,000 | 80,000 | 14ms |'
  id: totrans-834
  prefs: []
  type: TYPE_TB
  zh: '| 1个列表者，1个买家，无锁 | 145,000 | 27,000 | 80,000 | 14ms |'
- en: '| 1 lister, 1 buyer, with lock | 51,000 | 50,000 | 0 | 1ms |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '| 1个列表者，1个买家，使用锁 | 51,000 | 50,000 | 0 | 1ms |'
- en: '| 5 listers, 1 buyer, no lock | 331,000 | <200 | 50,000 | 150ms |'
  id: totrans-836
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表者，1个买家，无锁 | 331,000 | <200 | 50,000 | 150ms |'
- en: '| 5 listers, 1 buyer, with lock | 68,000 | 13,000 | <10 | 5ms |'
  id: totrans-837
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表者，1个买家，使用锁 | 68,000 | 13,000 | <10 | 5ms |'
- en: '| 5 listers, 5 buyers, no lock | 206,000 | <600 | 161,000 | 498ms |'
  id: totrans-838
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表者，5个买家，无锁 | 206,000 | <600 | 161,000 | 498ms |'
- en: '| 5 listers, 5 buyers, with lock | 21,000 | 20,500 | 0 | 14ms |'
  id: totrans-839
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表者，5个买家，使用锁 | 21,000 | 20,500 | 0 | 14ms |'
- en: Though we generally have lower total number of items that finished being listed,
    we never retry, and our number of listed items compared to our number of purchased
    items is close to the ratio of number of listers to buyers. At this point, we’re
    running at the limit of contention between the different listing and buying processes.
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们通常完成的列表项目总数较少，但我们从未重试，并且我们的列表项目与购买项目的数量接近列表者与买家的数量比率。此时，我们正在运行不同列表和购买过程之间的竞争极限。
- en: 6.2.4\. Fine-grained locking
  id: totrans-841
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4. 细粒度锁定
- en: When we introduced locks and locking, we only worried about providing the same
    type of locking granularity as the available `WATCH` command—on the level of the
    market key that we were updating. But because we’re constructing locks manually,
    and we’re less concerned about the market in its entirety than we are with whether
    an item is still in the market, we can actually lock on a finer level of detail.
    If we replace the market-level lock with one specific to the item to be bought
    or sold, we can reduce lock contention and increase performance.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们引入锁和锁定时，我们只担心提供与可用的`WATCH`命令相同的锁定粒度——在更新的市场键级别。但由于我们手动构建锁，并且我们更关心项目是否仍在市场上，而不是整个市场，我们实际上可以在更细的级别上锁定。如果我们用针对要购买或出售的特定项目的锁替换市场级别的锁，我们可以减少锁定竞争并提高性能。
- en: Let’s look at the results in [table 6.3](#ch06table03), which is the same simulation
    as produced [table 6.2](#ch06table02), only with locks over just the items being
    listed or sold individually, and not over the entire market.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看[表6.3](#ch06table03)中的结果，该表与[表6.2](#ch06table02)产生的模拟相同，只是对所列或单独出售的项目进行了锁定，而不是对整个市场进行锁定。
- en: Table 6.3\. Performance of fine-grained locking over 60 seconds
  id: totrans-844
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表6.3. 60秒内细粒度锁定的性能
- en: '|   | Listed items | Bought items | Purchase retries | Average wait per purchase
    |'
  id: totrans-845
  prefs: []
  type: TYPE_TB
  zh: '|   | 列表项目 | 购买项目 | 购买重试 | 平均每次购买等待时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-846
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 lister, 1 buyer, no lock | 145,000 | 27,000 | 80,000 | 14ms |'
  id: totrans-847
  prefs: []
  type: TYPE_TB
  zh: '| 1个列表，1个买家，无锁定 | 145,000 | 27,000 | 80,000 | 14ms |'
- en: '| 1 lister, 1 buyer, with lock | 51,000 | 50,000 | 0 | 1ms |'
  id: totrans-848
  prefs: []
  type: TYPE_TB
  zh: '| 1个列表，1个买家，带锁定 | 51,000 | 50,000 | 0 | 1ms |'
- en: '| 1 lister, 1 buyer, with fine-grained lock | 113,000 | 110,000 | 0 | <1ms
    |'
  id: totrans-849
  prefs: []
  type: TYPE_TB
  zh: '| 1个列表，1个买家，带细粒度锁定 | 113,000 | 110,000 | 0 | <1ms |'
- en: '| 5 listers, 1 buyer, no lock | 331,000 | <200 | 50,000 | 150ms |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，1个买家，无锁定 | 331,000 | <200 | 50,000 | 150ms |'
- en: '| 5 listers, 1 buyer, with lock | 68,000 | 13,000 | <10 | 5ms |'
  id: totrans-851
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，1个买家，带锁定 | 68,000 | 13,000 | <10 | 5ms |'
- en: '| 5 listers, 1 buyer, with fine-grained lock | 192,000 | 36,000 | 0 | <2ms
    |'
  id: totrans-852
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，1个买家，带细粒度锁定 | 192,000 | 36,000 | 0 | <2ms |'
- en: '| 5 listers, 5 buyers, no lock | 206,000 | <600 | 161,000 | 498ms |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，5个买家，无锁定 | 206,000 | <600 | 161,000 | 498ms |'
- en: '| 5 listers, 5 buyers, with lock | 21,000 | 20,500 | 0 | 14ms |'
  id: totrans-854
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，5个买家，带锁定 | 21,000 | 20,500 | 0 | 14ms |'
- en: '| 5 listers, 5 buyers, with fine-grained lock | 116,000 | 111,000 | 0 | <3ms
    |'
  id: totrans-855
  prefs: []
  type: TYPE_TB
  zh: '| 5个列表，5个买家，带细粒度锁定 | 116,000 | 111,000 | 0 | <3ms |'
- en: With fine-grained locking, we’re performing 220,000–230,000 listing and buying
    operations regardless of the number of listing and buying processes. We have no
    retries, and even under a full load, we’re seeing less than 3 milliseconds of
    latency. Our listed-to-sold ratio is again almost exactly the same as our ratio
    of listing-to-buying processes. Even better, we never get into a situation like
    we did without locks where there’s so much contention that latencies shoot through
    the roof and items are rarely sold.
  id: totrans-856
  prefs: []
  type: TYPE_NORMAL
  zh: 在细粒度锁定的情况下，无论列表和购买过程的数量如何，我们都执行了220,000–230,000次列表和购买操作。我们没有重试，即使在满载的情况下，我们看到的延迟也少于3毫秒。我们的列表到售出比率再次几乎与我们的列表到购买过程比率完全相同。更好的是，我们从未遇到过没有锁时的情况，那时竞争如此激烈，延迟飙升，商品很少售出。
- en: Let’s take a moment to look at our data as a few graphs so that we can see the
    relative scales. In [figure 6.3](#ch06fig03), we can see that both locking methods
    result in much higher numbers of items being purchased over all relative loads
    than the `WATCH`-based method.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花点时间看看我们的数据，通过几个图表来观察相对规模。在[图6.3](#ch06fig03)中，我们可以看到，与基于`WATCH`的方法相比，两种锁定方法在所有相对负载下都导致购买的商品数量显著增加。
- en: Figure 6.3\. Items purchased completed in 60 seconds. This graph has an overall
    V shape because the system is overloaded, so when we have five listing processes
    to only one buying process (shown as 5L/1B in the middle samples), the ratio of
    listed items to bought items is roughly the same ratio, 5 to 1.
  id: totrans-858
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3. 在60秒内完成的购买物品。由于系统过载，此图呈V形，因为当我们有五个列表过程对应一个购买过程（在中间样本中显示为5L/1B）时，列表物品与购买物品的比例大致相同，为5比1。
- en: '![](06fig03_alt.jpg)'
  id: totrans-859
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig03_alt.jpg)'
- en: Looking at [figure 6.4](#ch06fig04), we can see that the `WATCH`-based method
    has to perform many thousands of expensive retries in order to complete what few
    sales are completed.
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 观察图6.4，我们可以看到基于`WATCH`的方法必须执行成千上万昂贵的重试才能完成少量的销售。
- en: Figure 6.4\. The number of retries when trying to purchase an item in 60 seconds.
    There are no retries for either types of locks, so we can’t see the line for “with
    lock” because it’s hidden behind the line for fine-grained locks.
  id: totrans-861
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4. 在60秒内尝试购买物品时的重试次数。两种类型的锁都没有重试，因此我们看不到“带锁”的线条，因为它被细粒度锁的线条隐藏了。
- en: '![](06fig04.jpg)'
  id: totrans-862
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig04.jpg)'
- en: And in [figure 6.5](#ch06fig05), we can see that because of the `WATCH` contention,
    which caused the huge number of retries and the low number of purchase completions,
    latency without using a lock went up significantly.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图6.5](#ch06fig05)中，我们可以看到，由于`WATCH`竞争，导致了大量的重试和购买完成的数量低，不使用锁的延迟显著增加。
- en: Figure 6.5\. Average latency for a purchase; times are in milliseconds. The
    maximum latency for either kind of lock is under 14ms, which is why both locking
    methods are difficult to see and hugging the bottom—our overloaded system without
    a lock has an average latency of nearly 500ms.
  id: totrans-864
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5. 购买的平均延迟；时间以毫秒为单位。两种类型的锁的最大延迟都低于14ms，这就是为什么两种锁定方法都难以看到并且紧贴底部——我们没有锁的过载系统的平均延迟接近500ms。
- en: '![](06fig05.jpg)'
  id: totrans-865
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig05.jpg)'
- en: What these simulations and these charts show overall is that when under heavy
    load, using a lock can reduce retries, reduce latency, improve performance, and
    be tuned at the granularity that we need.
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模拟和这些图表总体上表明，在重负载下，使用锁可以减少重试次数，减少延迟，提高性能，并且可以在我们需要的粒度上进行调整。
- en: Our simulation is limited. One major case that it doesn’t simulate is where
    many more buyers are unable to buy items because they’re waiting for others. It
    also doesn’t simulate an effect known as *dogpiling*, when, as transactions take
    longer to complete, more transactions are overlapping and trying to complete.
    That will increase the time it takes to complete an individual transaction, and
    subsequently increase the chances for a time-limited transaction to fail. This
    will substantially increase the failure and retry rates for all transactions,
    but it’s especially harmful in the `WATCH`-based version of our market buying
    and selling.
  id: totrans-867
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模拟是有限的。它没有模拟的一个主要情况是，由于许多买家在等待他人而无法购买物品。它也没有模拟一种称为*狗群效应*的现象，当事务完成时间变长时，更多的交易重叠并尝试完成。这将增加单个事务完成所需的时间，从而增加时间限制事务失败的机会。这将大大增加所有事务的失败和重试率，但在我们基于`WATCH`的市场买卖版本中尤其有害。
- en: The choice to use a lock over an entire structure, or over just a small portion
    of a structure, can be easy. In our case, the critical data that we were watching
    was a small piece of the whole (one item in a marketplace), so locking that small
    piece made sense. There are situations where it’s not just one small piece, or
    when it may make sense to lock multiple parts of structures. That’s where the
    decision to choose locks over small pieces of data or an entire structure gets
    difficult; the use of multiple small locks can lead to deadlocks, which can prevent
    any work from being performed at all.
  id: totrans-868
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个结构或结构的小部分上使用锁的选择可能是容易的。在我们的案例中，我们关注的临界数据是整个结构的一小部分（市场中的一个物品），因此锁定这一小部分是有意义的。有些情况下，不仅仅是一小部分，或者当锁定结构的多个部分可能是有意义的时候。这就是选择锁定小部分数据或整个结构变得困难的地方；使用多个小锁可能导致死锁，这可能会阻止任何工作执行。
- en: 6.2.5\. Locks with timeouts
  id: totrans-869
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5. 带超时的锁
- en: As mentioned before, our lock doesn’t handle cases where a lock holder crashes
    without releasing the lock, or when a lock holder fails and holds the lock forever.
    To handle the crash/failure cases, we add a timeout to the lock.
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们的锁不处理锁持有者在未释放锁的情况下崩溃或锁持有者失败并永久持有锁的情况。为了处理崩溃/故障情况，我们在锁中添加了超时。
- en: In order to give our lock a timeout, we’ll use `EXPIRE` to have Redis time it
    out automatically. The natural place to put the `EXPIRE` is immediately after
    the lock is acquired, and we’ll do that. But if our client happens to crash (and
    the worst place for it to crash for us is between `SETNX` and `EXPIRE`), we still
    want the lock to eventually time out. To handle that situation, any time a client
    fails to get the lock, the client will check the expiration on the lock, and if
    it’s not set, set it. Because clients are going to be checking and setting timeouts
    if they fail to get a lock, the lock will always have a timeout, and will eventually
    expire, letting other clients get a timed-out lock.
  id: totrans-871
  prefs: []
  type: TYPE_NORMAL
  zh: 为了给我们的锁定设置超时，我们将使用 `EXPIRE` 来让 Redis 自动计时。将 `EXPIRE` 放在锁定获取后立即执行是自然的，我们会这样做。但如果我们的客户端意外崩溃（对我们来说最糟糕的情况是发生在
    `SETNX` 和 `EXPIRE` 之间），我们仍然希望锁定最终能够超时。为了处理这种情况，任何客户端在无法获取锁定时，都会检查锁定的过期时间，如果尚未设置，则设置它。因为客户端在无法获取锁定时会检查和设置超时，所以锁定将始终具有超时，最终会过期，让其他客户端获取超时锁。
- en: What if multiple clients set expiration times simultaneously? They’ll run at
    essentially the same time, so expiration will be set for the same time.
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 如果多个客户端同时设置过期时间会怎样？它们将几乎同时运行，因此过期时间将被设置为相同的时间。
- en: Adding expiration to our earlier `acquire_lock()` function gets us the updated
    `acquire_lock_with_timeout()` function shown here.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前的 `acquire_lock()` 函数中添加过期时间，得到了这里显示的更新后的 `acquire_lock_with_timeout()`
    函数。
- en: Listing 6.11\. The `acquire_lock_with_timeout()` function
  id: totrans-874
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11. `acquire_lock_with_timeout()` 函数
- en: '![](126fig01_alt.jpg)'
  id: totrans-875
  prefs: []
  type: TYPE_IMG
  zh: '![图片](126fig01_alt.jpg)'
- en: This new `acquire_lock_with_timeout()` handling timeouts. It ensures that locks
    expire as necessary, and that they won’t be stolen from clients that rightfully
    have them. Even better, we were smart with our release lock function earlier,
    which still works.
  id: totrans-876
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的 `acquire_lock_with_timeout()` 处理超时。它确保锁定按需过期，并且不会被从合法持有客户端那里窃取。更好的是，我们之前在释放锁定函数中做得很聪明，它仍然有效。
- en: '|  |'
  id: totrans-877
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-878
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: As of Redis 2.6.12, the `SET` command added options to support a combination
    of `SETNX` and `SETEX` functionality, which makes our lock acquire function trivial.
    We still need the complicated release lock to be correct.
  id: totrans-879
  prefs: []
  type: TYPE_NORMAL
  zh: 到 Redis 2.6.12 版本为止，`SET` 命令增加了选项来支持 `SETNX` 和 `SETEX` 功能的组合，这使得我们的锁定获取函数变得简单。我们仍然需要复杂的释放锁定函数来保持正确性。
- en: '|  |'
  id: totrans-880
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In [section 6.1.2](#ch06lev2sec2) when we built the address book autocomplete
    using a `ZSET`, we went through a bit of trouble to create start and end entries
    to add to the `ZSET` in order to fetch a range. We also postprocessed our data
    to remove entries with curly braces (`{}`), because other autocomplete operations
    could be going on at the same time. And because other operations could be going
    on at the same time, we used `WATCH` so that we could retry. Each of those pieces
    added complexity to our functions, which could’ve been simplified if we’d used
    a lock instead.
  id: totrans-881
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第6.1.2节](#ch06lev2sec2) 中，当我们使用 `ZSET` 构建地址簿自动完成功能时，我们经历了一些麻烦来创建起始和结束条目以添加到
    `ZSET` 中以获取范围。我们还对数据进行后处理，以删除带有花括号 `{}` 的条目，因为可能同时进行其他自动完成操作。而且因为可能同时进行其他操作，我们使用了
    `WATCH` 以便可以重试。这些操作中的每一个都增加了我们函数的复杂性，如果使用锁定的话，这些复杂性是可以简化的。
- en: In other databases, locking is a basic operation that’s supported and performed
    automatically. As I mentioned earlier, using `WATCH`, `MULTI`, and `EXEC` is a
    way of having an optimistic lock—we aren’t actually locking data, but we’re notified
    and our changes are canceled if someone else modifies it before we do. By adding
    explicit locking on the client, we get a few benefits (better performance, a more
    familiar programming concept, easier-to-use API, and so on), but we need to remember
    that Redis itself doesn’t respect our locks. It’s up to us to consistently use
    our locks in addition to or instead of `WATCH`, `MULTI`, and `EXEC` to keep our
    data consistent and correct.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 在其他数据库中，锁定是一个基本操作，它被支持和自动执行。正如我之前提到的，使用 `WATCH`、`MULTI` 和 `EXEC` 是实现乐观锁的一种方式——我们实际上并没有锁定数据，但如果在我们操作之前有人修改了它，我们会收到通知并且我们的更改会被取消。通过在客户端添加显式锁定，我们可以获得一些好处（更好的性能、更熟悉的编程概念、易于使用的API等等），但我们需要记住，Redis
    本身并不尊重我们的锁定。我们需要一致地使用我们的锁定，以及或代替 `WATCH`、`MULTI` 和 `EXEC`，以保持数据的一致性和正确性。
- en: Now that we’ve built a lock with timeouts, let’s look at another kind of lock
    called a *counting semaphore*. It isn’t used in as many places as a regular lock,
    but when we need to give multiple clients access to the same information at the
    same time, it’s the perfect tool for the job.
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了一个带有超时的锁，让我们看看另一种类型的锁，称为*计数信号量*。它不像常规锁那样被广泛使用，但当我们需要同时给多个客户端提供相同的信息时，它就是这项工作的完美工具。
- en: 6.3\. Counting semaphores
  id: totrans-884
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3. 计数信号量
- en: A *counting semaphore* is a type of lock that allows you to limit the number
    of processes that can concurrently access a resource to some fixed number. You
    can think of the lock that we just created as being a counting semaphore with
    a limit of 1\. Generally, counting semaphores are used to limit the amount of
    resources that can be used at one time.
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: '*计数信号量*是一种锁，允许你将可以并发访问资源的进程数量限制为某个固定数。你可以将我们刚刚创建的锁视为一个限制为1的计数信号量。通常，计数信号量用于限制一次可以使用的资源数量。'
- en: Like other types of locks, counting semaphores need to be acquired and released.
    First, we acquire the semaphore, then we perform our operation, and then we release
    it. But where we’d typically wait for a lock if it wasn’t available, it’s common
    to fail immediately if a semaphore isn’t immediately available. For example, let’s
    say that we wanted to allow for five processes to acquire the semaphore. If a
    sixth process tried to acquire it, we’d want that call to fail early and report
    that the resource is busy.
  id: totrans-886
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他类型的锁一样，计数信号量需要被获取和释放。首先，我们获取信号量，然后执行我们的操作，最后释放它。但是，如果我们通常在锁不可用时会等待，那么如果信号量不能立即可用，通常会立即失败。例如，假设我们想要允许五个进程获取信号量。如果第六个进程尝试获取它，我们希望该调用尽早失败并报告资源正忙。
- en: We’ll move through this section similarly to how we went through distributed
    locking in [section 6.2](#ch06lev1sec2). We’ll build a counting semaphore piece
    by piece until we have one that’s complete and correct.
  id: totrans-887
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将像在[第6.2节](#ch06lev1sec2)中处理分布式锁定一样，逐步通过这一节。我们将逐步构建计数信号量，直到我们有一个完整且正确的信号量。
- en: Let’s look at an example with Fake Game Company. With the success of its marketplace
    continuously growing, Fake Game Company has had requests from users wanting to
    access information about the marketplace from outside the game so that they can
    buy and sell items without being logged into the game. The API to perform these
    operations has already been written, but it’s our job to construct a mechanism
    that limits each account from accessing the marketplace from more than five processes
    at a time.
  id: totrans-888
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以假游戏公司为例。随着其市场成功的持续增长，假游戏公司收到了用户请求，希望从游戏外部访问有关市场的信息，以便他们可以在不登录游戏的情况下买卖物品。执行这些操作的API已经编写完成，但我们的任务是构建一个机制，限制每个账户一次从超过五个进程访问市场。
- en: After we’ve built our counting semaphore, we make sure to wrap incoming API
    calls with a proper `acquire_semaphore()` and `release_semaphore()` pair.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们构建了计数信号量之后，我们确保用适当的`acquire_semaphore()`和`release_semaphore()`对传入的API调用进行包装。
- en: 6.3.1\. Building a basic counting semaphore
  id: totrans-890
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.1. 构建基本的计数信号量
- en: When building a counting semaphore, we run into many of the same concerns we
    had with other types of locking. We must decide who got the lock, how to handle
    processes that crashed with the lock, and how to handle timeouts. If we don’t
    care about timeouts, or handling the case where semaphore holders can crash without
    releasing semaphores, we could build semaphores fairly conveniently in a few different
    ways. Unfortunately, those methods don’t lead us to anything useful in the long
    term, so I’ll describe one method that we’ll incrementally improve to offer a
    full range of functionality.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建计数信号量时，我们会遇到与其他类型锁定相同的一些问题。我们必须决定谁获得了锁，如何处理持有锁的进程崩溃的情况，以及如何处理超时。如果我们不关心超时，或者处理信号量持有者可以不释放信号量而崩溃的情况，我们可以以几种不同的方式方便地构建信号量。不幸的是，这些方法在长期内不会带给我们任何有用的东西，所以我将描述一种方法，我们将逐步改进它以提供完整的功能集。
- en: In almost every case where we want to deal with timeouts in Redis, we’ll generally
    look to one of two different methods. Either we’ll use `EXPIRE` like we did with
    our standard locks, or we’ll use `ZSET`s. In this case, we want to use `ZSET`s,
    because that allows us to keep information about multiple semaphore holders in
    a single structure.
  id: totrans-892
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们想要在Redis中处理超时的大多数情况下，我们通常会考虑两种不同的方法。要么我们会使用`EXPIRE`，就像我们使用标准锁那样，要么我们会使用`ZSET`s。在这种情况下，我们想使用`ZSET`s，因为这允许我们在单个结构中保留关于多个信号量持有者的信息。
- en: More specifically, for each process that attempts to acquire the semaphore,
    we’ll generate a unique identifier. This identifier will be the member of a `ZSET`.
    For the score, we’ll use the timestamp for when the process attempted to acquire
    the semaphore. Our semaphore `ZSET` will look something like [figure 6.6](#ch06fig06).
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，对于每个尝试获取信号量的进程，我们将生成一个唯一的标识符。这个标识符将是`ZSET`的成员。对于分数，我们将使用进程尝试获取信号量的时间戳。我们的信号量`ZSET`将类似于[图6.6](#ch06fig06)。
- en: Figure 6.6\. Basic semaphore `ZSET`
  id: totrans-894
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 基本信号量 `ZSET`
- en: '![](06fig06.jpg)'
  id: totrans-895
  prefs: []
  type: TYPE_IMG
  zh: '![](06fig06.jpg)'
- en: When a process wants to attempt to acquire a semaphore, it first generates an
    identifier, and then the process adds the identifier to the `ZSET` using the current
    timestamp as the score. After adding the identifier, the process then checks for
    its identifier’s *rank*. If the rank returned is lower than the total allowed
    count (Redis uses 0-indexing on rank), then the caller has acquired the semaphore.
    Otherwise, the caller doesn’t have the semaphore and must delete its identifier
    from the `ZSET`. To handle timeouts, before adding our identifier to the `ZSET`,
    we first clear out any entries that have timestamps that are older than our timeout
    number value. The code to acquire the semaphore can be seen next.
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个进程想要尝试获取信号量时，它首先生成一个标识符，然后进程使用当前时间戳作为分数将标识符添加到`ZSET`中。在添加标识符后，进程然后检查其标识符的*排名*。如果返回的排名低于允许的总数（Redis在排名上使用0索引），则调用者已获取信号量。否则，调用者没有信号量，必须从`ZSET`中删除其标识符。为了处理超时，在我们将标识符添加到`ZSET`之前，我们首先清除任何时间戳早于我们的超时数值的条目。获取信号量的代码将在下一部分展示。
- en: Listing 6.12\. The `acquire_semaphore()` function
  id: totrans-897
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.12\. `acquire_semaphore()`函数
- en: '![](128fig01_alt.jpg)'
  id: totrans-898
  prefs: []
  type: TYPE_IMG
  zh: '![](128fig01_alt.jpg)'
- en: 'Our code proceeds as I’ve already described: generating the identifier, cleaning
    out any timed-out semaphores, adding its identifier to the `ZSET`, and checking
    its rank. Not too surprising.'
  id: totrans-899
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的代码按照我之前描述的方式进行：生成标识符，清除任何超时的信号量，将其标识符添加到`ZSET`中，并检查其排名。这并不令人惊讶。
- en: 'Releasing the semaphore is easy: we remove the identifier from the `ZSET`,
    as can be seen in the next listing.'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: 释放信号量很简单：我们从`ZSET`中移除标识符，如下一列表所示。
- en: Listing 6.13\. The `release_semaphore()` function
  id: totrans-901
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.13\. `release_semaphore()`函数
- en: '![](129fig01_alt.jpg)'
  id: totrans-902
  prefs: []
  type: TYPE_IMG
  zh: '![](129fig01_alt.jpg)'
- en: This basic semaphore works well—it’s simple, and it’s very fast. But relying
    on every process having access to the same system time in order to get the semaphore
    can cause problems if we have multiple hosts. This isn’t a huge problem for our
    specific use case, but if we had two systems A and B, where A ran even 10 milliseconds
    faster than B, then if A got the last semaphore, and B tried to get a semaphore
    within 10 milliseconds, B would actually “steal” A’s semaphore without A knowing
    it.
  id: totrans-903
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本的信号量工作得很好——它简单，而且非常快。但是，如果我们有多个主机，依赖于每个进程都能访问相同的系统时间来获取信号量可能会引起问题。对于我们特定的用例来说，这不是一个大问题，但如果有两个系统A和B，其中A比B快10毫秒，那么如果A获得了最后一个信号量，而B试图在10毫秒内获取一个信号量，B实际上会“偷走”A的信号量，而A却不知道。
- en: Any time we have a lock or a semaphore where such a slight difference in the
    system clock can drastically affect who can get the lock, the lock or semaphore
    is considered *unfair*. Unfair locks and semaphores can cause clients that should’ve
    gotten the lock or semaphore to never get it, and this is something that we’ll
    fix in the next section.
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时候，当我们有一个锁或信号量，其中系统时钟的微小差异会极大地影响谁能获得锁时，这个锁或信号量被认为是*不公平的*。不公平的锁和信号量可能导致本应获得锁或信号量的客户端永远无法获得，这是我们将在下一节中解决的问题。
- en: 6.3.2\. Fair semaphores
  id: totrans-905
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.2\. 公平信号量
- en: Because we can’t assume that all system clocks are exactly the same on all systems,
    our earlier basic counting semaphore will have issues where clients on systems
    with slower system clocks can steal the semaphore from clients on systems with
    faster clocks. Any time there’s this kind of sensitivity, locking itself becomes
    unfair. We want to reduce the effect of incorrect system times on acquiring the
    semaphore to the point where as long as systems are within 1 second, system time
    doesn’t cause semaphore theft or early semaphore expiration.
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们无法假设所有系统的系统时钟在所有系统上都是完全相同的，我们之前的基本计数信号量将会有问题，即系统时钟较慢的系统的客户端可能会从系统时钟较快的系统的客户端那里窃取信号量。每当有这种敏感性时，锁定本身就会变得不公平。我们希望将不正确系统时间对获取信号量的影响减少到只要系统在
    1 秒内，系统时间就不会导致信号量盗窃或提前过期。
- en: In order to minimize problems with inconsistent system times, we’ll add a counter
    and a second `ZSET`. The counter creates a steadily increasing timer-like mechanism
    that ensures that whoever incremented the counter first should be the one to get
    the semaphore. We then enforce our requirement that clients that want the semaphore
    who get the counter first also get the semaphore by using an “owner” `ZSET` with
    the counter-produced value as the score, checking our identifier’s rank in the
    new `ZSET` to determine which client got the semaphore. The new owner `ZSET` appears
    in [figure 6.7](#ch06fig07).
  id: totrans-907
  prefs: []
  type: TYPE_NORMAL
  zh: 为了最小化不一致的系统时间问题，我们将添加一个计数器和第二个 `ZSET`。计数器创建了一个稳步增加的计时器机制，确保首先增加计数器的那个应该获得信号量。然后我们通过使用一个“所有者”
    `ZSET`，其中计数器产生的值作为分数，检查我们的标识符在新 `ZSET` 中的排名，以确定哪个客户端获得了信号量。新的所有者 `ZSET` 出现在 [图
    6.7](#ch06fig07) 中。
- en: Figure 6.7\. Fair semaphore owner `ZSET`
  id: totrans-908
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.7\. 公平信号量所有者 `ZSET`
- en: '![](06fig07.jpg)'
  id: totrans-909
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig07.jpg)'
- en: We continue to handle timeouts the same way as our basic semaphore, by removing
    entries from the system time `ZSET`. We propagate those timeouts to the new owner
    `ZSET` by the use of `ZINTERSTORE` and the `WEIGHTS` argument.
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续以与我们的基本信号量相同的方式处理超时，即通过从系统时间 `ZSET` 中删除条目。我们通过使用 `ZINTERSTORE` 和 `WEIGHTS`
    参数将那些超时传播到新的所有者 `ZSET`。
- en: Bringing it all together in [listing 6.14](#ch06ex14), we first time out an
    entry by removing old entries from the timeout `ZSET` and then intersect the timeout
    `ZSET` with the owner `ZSET`, saving to and overwriting the owner `ZSET`. We then
    increment the counter and add our counter value to the owner `ZSET`, while at
    the same time adding our current system time to the timeout `ZSET`. Finally, we
    check whether our rank in the owner `ZSET` is low enough, and if so, we have a
    semaphore. If not, we remove our entries from the owner and timeout `ZSET`s.
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 6.14](#ch06ex14) 中将所有内容整合在一起，我们首先通过从超时 `ZSET` 中删除旧条目来超时一个条目，然后交集超时 `ZSET`
    与所有者 `ZSET`，保存到并覆盖所有者 `ZSET`。然后我们增加计数器并将我们的计数器值添加到所有者 `ZSET`，同时将我们的当前系统时间添加到超时
    `ZSET`。最后，我们检查我们在所有者 `ZSET` 中的排名是否足够低，如果是这样，我们就有了信号量。如果不是，我们从所有者和超时 `ZSET` 中删除我们的条目。
- en: Listing 6.14\. The `acquire_fair_semaphore()` function
  id: totrans-912
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.14\. `acquire_fair_semaphore()` 函数
- en: '![](130fig01_alt.jpg)'
  id: totrans-913
  prefs: []
  type: TYPE_IMG
  zh: '![图片](130fig01_alt.jpg)'
- en: This function has a few different pieces. We first clean up timed-out semaphores,
    updating the owner `ZSET` and fetching the next counter ID for this item. After
    we’ve added our time to the timeout `ZSET` and our counter value to the owner
    `ZSET`, we’re ready to check to see whether our rank is low enough.
  id: totrans-914
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数有几个不同的部分。我们首先清理超时的信号量，更新所有者 `ZSET` 并获取此项目的下一个计数器 ID。在我们将时间添加到超时 `ZSET` 并将我们的计数器值添加到所有者
    `ZSET` 之后，我们准备检查我们的排名是否足够低。
- en: '|  |'
  id: totrans-915
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Fair semaphores on 32-bit platforms
  id: totrans-916
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 32 位平台上的公平信号量
- en: On 32-bit Redis platforms, integer counters are limited to 2^(31) - 1, the standard
    signed integer limit. An overflow situation could occur on heavily used semaphores
    roughly once every 2 hours in the worst case. Though there are a variety of workarounds,
    the simplest is to switch to a 64-bit platform for any machine using any counter-based
    ID.
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: 在 32 位 Redis 平台上，整数计数器限制为 2^(31) - 1，即标准有符号整数限制。在最坏的情况下，在高度使用的信号量上，溢出情况可能大约每
    2 小时发生一次。尽管有多种解决方案，但最简单的是将任何使用基于计数器 ID 的机器切换到 64 位平台。
- en: '|  |'
  id: totrans-918
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s look at [figure 6.8](#ch06fig08), which shows the sequence of operations
    that are performed when process ID 8372 wants to acquire the semaphore at time
    1326437039.100 when there’s a limit of 5.
  id: totrans-919
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 [图 6.8](#ch06fig08)，它显示了当进程 ID 8372 在时间 1326437039.100 想要获取信号量，并且限制为 5
    时执行的操作序列。
- en: Figure 6.8\. Call sequence for `acquire_fair_semaphore()`
  id: totrans-920
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8. `acquire_fair_semaphore()` 的调用序列
- en: '![](06fig08_alt.jpg)'
  id: totrans-921
  prefs: []
  type: TYPE_IMG
  zh: '![06fig08_alt.jpg](06fig08_alt.jpg)'
- en: Releasing the semaphore is almost as easy as before, only now we remove our
    identifier from both the owner and timeout `ZSET`s, as can be seen in this next
    listing.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 释放信号量几乎和以前一样简单，但现在我们从所有者和超时 `ZSET`s 中移除我们的标识符，如以下列表所示。
- en: Listing 6.15\. The `release_fair_semaphore()` function
  id: totrans-923
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.15. `release_fair_semaphore()` 函数
- en: '![](131fig01_alt.jpg)'
  id: totrans-924
  prefs: []
  type: TYPE_IMG
  zh: '![131fig01_alt.jpg](131fig01_alt.jpg)'
- en: If we wanted to be lazy, in most situations we could just remove our semaphore
    identifier from the timeout `ZSET`; one of our steps in the acquire sequence is
    to refresh the owner `ZSET` to remove identifiers that are no longer in the timeout
    `ZSET`. But by only removing our identifier from the timeout `ZSET`, there’s a
    chance (rare, but possible) that we removed the entry, but the `acquire_fair_semaphore()`
    was between the part where it updated the owner `ZSET` and when it added its own
    identifiers to the timeout and owner `ZSET`s. If so, this could prevent it from
    acquiring the semaphore when it should’ve been able to. To ensure correct behavior
    in as many situations as possible, we’ll stick to removing the identifier from
    both `ZSET`s.
  id: totrans-925
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想偷懒，在大多数情况下，我们只需从超时 `ZSET` 中移除我们的信号量标识符；我们在获取序列中的一步是刷新所有者 `ZSET` 以移除不再超时
    `ZSET` 中的标识符。但仅仅通过从超时 `ZSET` 中移除我们的标识符，就有可能（虽然很少，但有可能）我们移除了条目，而 `acquire_fair_semaphore()`
    正在更新所有者 `ZSET` 的部分和它将自身标识符添加到超时和所有者 `ZSET`s 之间。如果是这样，这可能会阻止它在应该能够获取信号量时获取它。为了确保尽可能多的情况下行为正确，我们将坚持从两个
    `ZSET`s 中移除标识符。
- en: Now we have a semaphore that doesn’t require that all hosts have the same system
    time, though system times do need to be within 1 or 2 seconds in order to ensure
    that semaphores don’t time out too early, too late, or not at all.
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个不需要所有主机具有相同系统时间的信号量，尽管系统时间需要在1到2秒内，以确保信号量不会过早、过晚或根本不会超时。
- en: 6.3.3\. Refreshing semaphores
  id: totrans-927
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.3. 刷新信号量
- en: As the API for the marketplace was being completed, it was decided that there
    should be a method for users to stream all item listings as they happen, along
    with a stream for all purchases that actually occur. The semaphore method that
    we created only supports a timeout of 10 seconds, primarily to deal with timeouts
    and possible bugs on our side of things. But users of the streaming portion of
    the API will want to keep connected for much longer than 10 seconds, so we need
    a method for refreshing the semaphore so that it doesn’t time out.
  id: totrans-928
  prefs: []
  type: TYPE_NORMAL
  zh: 当市场API正在完成时，决定应该有一个方法让用户流式传输所有发生的商品列表，以及所有实际发生的购买流。我们创建的信号量方法仅支持10秒的超时，主要是为了处理超时和可能在我们这边的问题。但API流部分的用户希望保持连接超过10秒，因此我们需要一个刷新信号量的方法，以便它不会超时。
- en: Because we already separated the timeout `ZSET` from the owner `ZSET`, we can
    actually refresh timeouts quickly by updating our time in the timeout `ZSET`,
    shown in the following listing.
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经将超时 `ZSET` 与所有者 `ZSET` 分离，我们实际上可以通过更新超时 `ZSET` 中的时间来快速刷新超时，如下面的列表所示。
- en: Listing 6.16\. The `refresh_fair_semaphore()` function
  id: totrans-930
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.16. `refresh_fair_semaphore()` 函数
- en: '![](132fig01_alt.jpg)'
  id: totrans-931
  prefs: []
  type: TYPE_IMG
  zh: '![132fig01_alt.jpg](132fig01_alt.jpg)'
- en: As long as we haven’t already timed out, we’ll be able to refresh the semaphore.
    If we were timed out in the past, we’ll go ahead and let the semaphore be lost
    and report to the caller that it was lost. When using a semaphore that may be
    refreshed, we need to be careful to refresh often enough to not lose the semaphore.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们没有超时，我们就能刷新信号量。如果我们以前超时了，我们将让信号量丢失并报告给调用者它已经丢失。当使用可能刷新的信号量时，我们需要小心，以确保经常刷新以避免信号量丢失。
- en: Now that we have the ability to acquire, release, and refresh a fair semaphore,
    it’s time to deal with our final race condition.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了获取、释放和刷新公平信号量的能力，是时候处理我们的最后一个竞争条件了。
- en: 6.3.4\. Preventing race conditions
  id: totrans-934
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.3.4. 防止竞争条件
- en: As you saw when building locks in [section 6.2](#ch06lev1sec2), dealing with
    race conditions that cause retries or data corruption can be difficult. In this
    case, the semaphores that we created have race conditions that we alluded to earlier,
    which can cause incorrect operation.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在[第6.2节](#ch06lev1sec2)中构建锁时，处理导致重试或数据损坏的竞争条件可能很困难。在这种情况下，我们创建的信号量具有我们之前提到的竞争条件，这可能导致操作不正确。
- en: We can see the problem in the following example. If we have two processes A
    and B that are trying to get one remaining semaphore, and A increments the counter
    first but B adds its identifier to the `ZSET`s and checks its identifier’s rank
    first, then B will get the semaphore. When A then adds its identifier and checks
    its rank, it’ll “steal” the semaphore from B, but B won’t know until it tries
    to release or renew the semaphore.
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在以下示例中看到问题。如果我们有两个进程 A 和 B 正在尝试获取一个剩余的信号量，并且 A 首先增加计数器，但 B 将其标识符添加到 `ZSET`s
    并首先检查其标识符的排名，那么 B 将获得信号量。当 A 然后添加其标识符并检查其排名时，它将从 B 那里“窃取”信号量，但 B 不会知道直到它尝试释放或更新信号量。
- en: When we were using the system clock as a way of getting a lock, the likelihood
    of this kind of a race condition coming up and resulting in more than the desired
    number of semaphore owners was related to the difference in system times—the greater
    the difference, the greater the likelihood. After introducing the counter with
    the owner `ZSET`, this problem became less likely (just by virtue of removing
    the system clock as a variable), but because we have multiple round trips, it’s
    still possible.
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用系统时钟作为获取锁的方式时，这种类型的竞态条件出现的可能性以及导致超过期望的信号量所有者数量的情况与系统时间之间的差异有关——差异越大，可能性越大。在引入带有所有者
    `ZSET` 的计数器后，这个问题变得不太可能（仅仅是因为移除了系统时钟作为变量），但由于我们有多次往返，它仍然可能。
- en: To fully handle all possible race conditions for semaphores in Redis, we need
    to reuse the earlier distributed lock with timeouts that we built in [section
    6.2.5](#ch06lev2sec7). We need to use our earlier lock to help build a correct
    counting semaphore. Overall, to acquire the semaphore, we’ll first try to acquire
    the lock for the semaphore with a short timeout. If we got the lock, we then perform
    our normal semaphore acquire operations with the counter, owner `ZSET`, and the
    system time `ZSET`. If we failed to acquire the lock, then we say that we also
    failed to acquire the semaphore. The code for performing this operation is shown
    next.
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完全处理 Redis 中信号量的所有可能的竞态条件，我们需要重用 [第 6.2.5 节](#ch06lev2sec7) 中构建的带有超时的早期分布式锁。我们需要使用我们之前的锁来帮助构建一个正确的计数信号量。总的来说，为了获取信号量，我们首先尝试使用短超时获取信号量的锁。如果我们获得了锁，然后我们使用计数器、所有者
    `ZSET` 和系统时间 `ZSET` 执行正常的信号量获取操作。如果我们未能获取锁，那么我们说我们也未能获取信号量。执行此操作的代码如下。
- en: Listing 6.17\. The `acquire_semaphore_with_lock()` function
  id: totrans-939
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.17\. `acquire_semaphore_with_lock()` 函数
- en: '[PRE6]'
  id: totrans-940
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'I know, it can be disappointing to come so far only to end up needing to use
    a lock at the end. But that’s the thing with Redis: there are usually a few ways
    to solve the same or a similar problem, each with different trade-offs. Here are
    some of the trade-offs for the different counting semaphores that we’ve built:'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道，走了这么远最后却需要使用锁来结束，这可能会让人失望。但这就是 Redis 的特点：通常有几种方法可以解决相同或类似的问题，每种方法都有不同的权衡。以下是我们构建的不同计数信号量的权衡之一：
- en: If you’re happy with using the system clock, never need to refresh the semaphore,
    and are okay with occasionally going over the limit, then you can use the first
    semaphore we created.
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你对使用系统时钟感到满意，不需要刷新信号量，并且可以偶尔超出限制，那么你可以使用我们创建的第一个信号量。
- en: If you can only really trust system clocks to be within 1 or 2 seconds, but
    are still okay with occasionally going over your semaphore limit, then you can
    use the second one.
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只能真正信任系统时钟在 1 或 2 秒内，但仍然可以偶尔超出信号量限制，那么你可以使用第二个。
- en: If you need your semaphores to be correct every single time, then you can use
    a lock to guarantee correctness.
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你需要你的信号量每次都正确，那么你可以使用锁来保证正确性。
- en: Now that we’ve used our lock from [section 6.2](#ch06lev1sec2) to help us fix
    the race condition, we have varying options for how strict we want to be with
    our semaphore limits. Generally it’s a good idea to stick with the last, strictest
    version. Not only is the last semaphore actually correct, but whatever time we
    may save using a simpler semaphore, we could lose by using too many resources.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经使用 [第 6.2 节](#ch06lev1sec2) 中的锁来帮助我们解决竞态条件，我们可以根据需要选择对信号量限制的严格程度。通常来说，坚持使用最后一个、最严格的版本是个好主意。不仅最后一个信号量实际上是正确的，而且我们可能通过使用更简单的信号量节省的时间，我们可能会因为使用过多的资源而失去。
- en: In this section, we used semaphores to limit the number of API calls that can
    be running at any one time. Another common use case is to limit concurrent requests
    to databases to reduce individual query times and to prevent dogpiling like we
    talked about at the end of [section 6.2.4](#ch06lev2sec6). One other common situation
    is when we’re trying to download many web pages from a server, but their robots.txt
    says that we can only make (for example) three requests at a time. If we have
    many clients downloading web pages, we can use a semaphore to ensure that we aren’t
    pushing a given server too hard.
  id: totrans-946
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了信号量来限制在任何时候可以运行的API调用数量。另一个常见的用例是限制对数据库的并发请求，以减少单个查询时间并防止我们像在[第6.2.4节](#ch06lev2sec6)结尾所讨论的那样发生堆叠。另一种常见的情况是我们试图从服务器下载许多网页，但它们的robots.txt文件表示我们一次只能进行（例如）三个请求。如果我们有多个客户端下载网页，我们可以使用信号量来确保我们不会过度推压特定的服务器。
- en: As we finish with building locks and semaphores to help improve performance
    for concurrent execution, it’s now time to talk about using them in more situations.
    In the next section, we’ll build two different types of task queues for delayed
    and concurrent task execution.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们完成构建锁和信号量以帮助提高并发执行性能的工作，现在是时候讨论在更多情况下使用它们了。在下一节中，我们将构建两种不同类型的任务队列，用于延迟和并发任务执行。
- en: 6.4\. Task queues
  id: totrans-948
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 任务队列
- en: When handling requests from web clients, sometimes operations take more time
    to execute than we want to spend immediately. We can defer those operations by
    putting information about our task to be performed inside a queue, which we process
    later. This method of deferring work to some task processor is called a *task
    queue*. Right now there are many different pieces of software designed specifically
    for task queues (ActiveMQ, RabbitMQ, Gearman, Amazon SQS, and others), but there
    are also ad hoc methods of creating task queues in situations where queues aren’t
    expected. If you’ve ever had a cron job that scans a database table for accounts
    that have been modified/checked before or after a specific date/time, and you
    perform some operation based on the results of that query, you’ve already created
    a task queue.
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理来自网络客户端的请求时，有时操作所需的时间比我们愿意立即花费的时间更长。我们可以通过将关于要执行的任务的信息放入队列中，稍后处理，来推迟这些操作。这种方法将工作推迟到某个任务处理器称为*任务队列*。目前，有许多专门为任务队列设计的软件（ActiveMQ、RabbitMQ、Gearman、Amazon
    SQS等），但在不需要队列的情况下，也有创建任务队列的临时方法。如果你曾经有过一个cron作业，它会扫描数据库表以查找在特定日期/时间之前或之后被修改/检查过的账户，并根据查询结果执行某些操作，那么你已经创建了一个任务队列。
- en: In this section we’ll talk about two different types of task queues. Our first
    queue will be built to execute tasks as quickly as possible in the order that
    they were inserted. Our second type of queue will have the ability to schedule
    tasks to execute at some specific time in the future.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论两种不同类型的任务队列。我们的第一个队列将构建成以尽可能快的速度按插入顺序执行任务。我们的第二种队列类型将具有将任务调度到未来某个特定时间执行的能力。
- en: 6.4.1\. First-in, first-out queues
  id: totrans-951
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 先进先出队列
- en: In the world of queues beyond task queues, normally a few different kinds of
    queues are discussed—first-in, first-out (FIFO), last-in first-out (LIFO), and
    priority queues. We’ll look first at a first-in, first-out queue, because it offers
    the most reasonable semantics for our first pass at a queue, can be implemented
    easily, and is fast. Later, we’ll talk about adding a method for coarse-grained
    priorities, and even later, time-based queues.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 在队列的世界中，除了任务队列之外，通常讨论几种不同的队列类型——先进先出（FIFO）、后进先出（LIFO）和优先队列。我们将首先探讨先进先出队列，因为它为我们对队列的第一轮尝试提供了最合理的语义，易于实现，并且速度快。稍后，我们将讨论添加粗粒度优先级的方法，甚至更晚，基于时间的队列。
- en: Let’s again look back to an example from Fake Game Company. To encourage users
    to play the game when they don’t normally do so, Fake Game Company has decided
    to add the option for users to opt-in to emails about marketplace sales that have
    completed or that have timed out. Because outgoing email is one of those internet
    services that can have very high latencies and can fail, we need to keep the act
    of sending emails for completed or timed-out sales out of the typical code flow
    for those operations. To do this, we’ll use a task queue to keep a record of people
    who need to be emailed and why, and will implement a worker process that can be
    run in parallel to send multiple emails at a time if outgoing mail servers become
    slow.
  id: totrans-953
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾一下 Fake Game Company 的一个例子。为了鼓励用户在他们通常不会玩游戏的时候玩游戏，Fake Game Company 决定添加一个选项，让用户可以选择接收有关已完成或超时的市场销售的电子邮件。由于外发电子邮件是那些可能具有非常高的延迟并且可能失败的互联网服务之一，我们需要将发送已完成或超时销售电子邮件的操作从这些操作的典型代码流中分离出来。为此，我们将使用任务队列来记录需要发送电子邮件的人以及原因，并实现一个可以并行运行的工作进程，以便在发送邮件服务器变慢时一次发送多封电子邮件。
- en: The queue that we’ll write only needs to send emails out in a first-come, first-served
    manner, and will log both successes and failures. As we talked about in [chapters
    3](kindle_split_014.html#ch03) and [5](kindle_split_016.html#ch05), Redis `LIST`s
    let us push and pop items from both ends with `RPUSH`/`LPUSH` and `RPOP`/`LPOP`.
    For our email queue, we’ll push emails to send onto the right end of the queue
    with `RPUSH`, and pop them off the left end of the queue with `LPOP`. (We do this
    because it makes sense visually for readers of left-to-right languages.) Because
    our worker processes are only going to be performing this emailing operation,
    we’ll use the blocking version of our list pop, `BLPOP`, with a timeout of 30
    seconds. We’ll only handle item-sold messages in this version for the sake of
    simplicity, but adding support for sending timeout emails is also easy.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要编写的队列只需要以先进先出的方式发送电子邮件，并记录成功和失败的情况。正如我们在第 3 章 [chapters 3](kindle_split_014.html#ch03)
    和第 5 章 [chapters 5](kindle_split_016.html#ch05) 中所讨论的，Redis `LIST`s 允许我们使用 `RPUSH`/`LPUSH`
    和 `RPOP`/`LPOP` 从队列的两端推送和弹出项目。对于我们的电子邮件队列，我们将使用 `RPUSH` 将要发送的电子邮件推送到队列的右侧，并使用
    `LPOP` 从队列的左侧弹出它们。（我们这样做是因为对于从左到右阅读的语言的读者来说，这在视觉上是有意义的。）由于我们的工作进程只执行此电子邮件操作，我们将使用带有
    30 秒超时的阻塞版本列表弹出，即 `BLPOP`。为了简单起见，我们将只处理项目已售出的消息，但添加发送超时电子邮件的支持也很容易。
- en: Our queue will simply be a list of JSON-encoded blobs of data, which will look
    like [figure 6.9](#ch06fig09).
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的队列将简单地是一个 JSON 编码的数据块列表，其外观类似于 [图 6.9](#ch06fig09)。
- en: Figure 6.9\. A first-in, first-out queue using a `LIST`
  id: totrans-956
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.9\. 使用 `LIST` 的先进先出队列
- en: '![](06fig09.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9](06fig09.jpg)'
- en: To add an item to the queue, we’ll get all of the necessary information together,
    serialize it with JSON, and `RPUSH` the result onto our email queue. As in previous
    chapters, we use JSON because it’s human readable and because there are fast libraries
    for translation to/from JSON in most languages. The function that pushes an email
    onto the item-sold email task queue appears in the next listing.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 要将项目添加到队列中，我们将收集所有必要的信息，使用 JSON 进行序列化，并将结果 `RPUSH` 到我们的电子邮件队列中。与前面的章节一样，我们使用
    JSON，因为它可读性好，并且大多数语言都有快速库用于 JSON 的转换。将电子邮件推送到项目已售出电子邮件任务队列的函数将在下一个列表中显示。
- en: Listing 6.18\. The `send_sold_email_via_queue()` function
  id: totrans-959
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.18\. `send_sold_email_via_queue()` 函数
- en: '![](135fig01_alt.jpg)'
  id: totrans-960
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9](135fig01_alt.jpg)'
- en: Adding a message to a `LIST` queue shouldn’t be surprising.
  id: totrans-961
  prefs: []
  type: TYPE_NORMAL
  zh: 向 `LIST` 队列中添加消息不应令人惊讶。
- en: Sending emails from the queue is easy. We use `BLPOP` to pull items from the
    email queue, prepare the email, and finally send it. The next listing shows our
    function for doing so.
  id: totrans-962
  prefs: []
  type: TYPE_NORMAL
  zh: 从队列中发送电子邮件很简单。我们使用 `BLPOP` 从电子邮件队列中提取项目，准备电子邮件，最后发送。下面的列表显示了我们的执行此操作的函数。
- en: Listing 6.19\. The `process_sold_email_queue()` function
  id: totrans-963
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.19\. `process_sold_email_queue()` 函数
- en: '![](135fig02_alt.jpg)'
  id: totrans-964
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.9](135fig02_alt.jpg)'
- en: Similarly, actually sending the email after pulling the message from the queue
    is also not surprising. But what about executing more than one type of task?
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，从队列中拉取消息后实际发送电子邮件也不足为奇。但是，执行多种类型的任务呢？
- en: Multiple executable tasks
  id: totrans-966
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 多个可执行任务
- en: 'Because Redis only gives a single caller a popped item, we can be sure that
    none of the emails are duplicated and sent twice. Because we only put email messages
    to send in the queue, our worker process was simple. Having a single queue for
    each type of message is not uncommon for some situations, but for others, having
    a single queue able to handle many different types of tasks can be much more convenient.
    Take the worker process in [listing 6.20](#ch06ex20): it watches the provided
    queue and dispatches the JSON-encoded function call to one of a set of known registered
    callbacks. The item to be executed will be of the form `[''FUNCTION_NAME'', [ARG1,
    ARG2, ...]]`.'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Redis 只给单个调用者提供一个弹出的项目，我们可以确信没有邮件被重复发送两次。因为我们只把要发送的邮件消息放入队列，所以我们的工作进程很简单。对于某些情况，为每种类型的消息使用单个队列并不罕见，但对于其他情况，有一个能够处理许多不同类型任务的单一队列可能更加方便。以
    [列表 6.20](#ch06ex20) 中的工作进程为例：它监视提供的队列，并将 JSON 编码的函数调用派发到一组已知的已注册回调之一。要执行的项目将具有以下形式：`['FUNCTION_NAME',
    [ARG1, ARG2, ...]]`。
- en: Listing 6.20\. The `worker_watch_queue()` function
  id: totrans-968
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.20\. `worker_watch_queue()` 函数
- en: '![](136fig01_alt.jpg)'
  id: totrans-969
  prefs: []
  type: TYPE_IMG
  zh: '![图片](136fig01_alt.jpg)'
- en: With this generic worker process, our email sender could be written as a callback
    and passed with other callbacks.
  id: totrans-970
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个通用的工作进程，我们的邮件发送器可以写成回调函数，并与其他回调函数一起传递。
- en: Task priorities
  id: totrans-971
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 任务优先级
- en: Sometimes when working with queues, it’s necessary to prioritize certain operations
    before others. In our case, maybe we want to send emails about sales that completed
    before we send emails about sales that expired. Or maybe we want to send password
    reset emails before we send out emails for an upcoming special event. Remember
    the `BLPOP`/`BRPOP` commands—we can provide multiple `LIST`s in which to pop an
    item from; the first `LIST` to have any items in it will have its first item popped
    (or last if we’re using `BRPOP`).
  id: totrans-972
  prefs: []
  type: TYPE_NORMAL
  zh: 有时在处理队列时，有必要在执行其他操作之前优先处理某些操作。在我们的例子中，我们可能想在发送关于已完成的销售的邮件之前发送关于即将到期的销售的邮件。或者，我们可能在发送即将到来的特别活动的邮件之前发送密码重置邮件。记住
    `BLPOP`/`BRPOP` 命令——我们可以提供多个 `LIST`，从中弹出项目；第一个有项目的 `LIST` 将会弹出其第一个项目（或使用 `BRPOP`
    时的最后一个项目）。
- en: 'Let’s say that we want to have three priority levels: high, medium, and low.
    High-priority items should be executed if they’re available. If there are no high-priority
    items, then items in the medium-priority level should be executed. If there are
    neither high- nor medium-priority items, then items in the low-priority level
    should be executed. Looking at our earlier code, we can change two lines to make
    that possible in the updated listing.'
  id: totrans-973
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要设置三个优先级级别：高、中、低。如果高优先级的项目可用，则应执行高优先级项目。如果没有高优先级项目，则应执行中优先级级别的项目。如果没有高优先级和中优先级的项目，则应执行低优先级级别的项目。查看我们之前的代码，我们可以更改两行，以便在更新的列表中实现这一点。
- en: Listing 6.21\. The `worker_watch_queues()` function
  id: totrans-974
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.21\. `worker_watch_queues()` 函数
- en: '![](137fig01_alt.jpg)'
  id: totrans-975
  prefs: []
  type: TYPE_IMG
  zh: '![图片](137fig01_alt.jpg)'
- en: By using multiple queues, priorities can be implemented easily. There are situations
    where multiple queues are used as a way of separating different queue items (announcement
    emails, notification emails, and so forth) without any desire to be “fair.” In
    such situations, it can make sense to reorder the queue list occasionally to be
    more fair to all of the queues, especially in the case where one queue can grow
    quickly relative to the other queues.
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用多个队列，可以轻松实现优先级。在某些情况下，使用多个队列作为分离不同队列项目（如公告邮件、通知邮件等）的方式，而不希望“公平”。在这种情况下，偶尔重新排序队列列表以使所有队列都更加公平是有意义的，特别是在一个队列相对于其他队列可以快速增长的情况下。
- en: If you’re using Ruby, you can use an open source package called Resque that
    was put out by the programmers at GitHub. It uses Redis for Ruby-based queues
    using lists, which is similar to what we’ve talked about here. Resque offers many
    additional features over the 11-line function that we provided here, so if you’re
    using Ruby, you should check it out. Regardless, there are many more options for
    queues in Redis, and you should keep reading.
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Ruby，你可以使用 GitHub 程序员发布的开源包 Resque。它使用 Redis 为基于 Ruby 的队列使用列表，这与我们在这里讨论的类似。Resque
    在我们提供的 11 行函数之上提供了许多额外的功能，所以如果你使用 Ruby，你应该检查一下。无论如何，Redis 中还有更多关于队列的选项，你应该继续阅读。
- en: 6.4.2\. Delayed tasks
  id: totrans-978
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2\. 延迟任务
- en: 'With list-based queues, we can handle single-call per queue, multiple callbacks
    per queue, and we can handle simple priorities. But sometimes, we need a bit more.
    Fake Game Company has decided that they’re going to add a new feature in their
    game: delayed selling. Rather than putting an item up for sale now, players can
    tell the game to put an item up for sale in the future. It’s our job to change
    or replace our task queue with something that can offer this feature.'
  id: totrans-979
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于列表的队列，我们可以处理每个队列的单次调用、每个队列的多个回调，并且我们可以处理简单的优先级。但有时我们需要更多。假想游戏公司决定在他们的游戏中添加一个新功能：延迟销售。玩家不是现在就上架物品，而是可以告诉游戏在未来上架物品。我们的任务是更改或替换我们的任务队列，使其能够提供此功能。
- en: 'There are a few different ways that we could potentially add delays to our
    queue items. Here are the three most straightforward ones:'
  id: totrans-980
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有几种不同的方法可以为队列项添加延迟。以下是三种最直接的方法：
- en: We could include an execution time as part of queue items, and if a worker process
    sees an item with an execution time later than now, it can wait for a brief period
    and then re-enqueue the item.
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以在队列项中包含一个执行时间，如果工作进程看到执行时间晚于现在的项，它可以等待一段时间，然后重新入队该项。
- en: The worker process could have a local waiting list for any items it has seen
    that need to be executed in the future, and every time it makes a pass through
    its while loop, it could check that list for any outstanding items that need to
    be executed.
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作进程可以为任何它已经看到并需要在将来执行的项目保留一个本地等待列表，并且每次它通过 while 循环时，它都可以检查该列表以查找任何需要执行的超出项。
- en: Normally when we talk about times, we usually start talking about `ZSET`s. What
    if, for any item we wanted to execute in the future, we added it to a `ZSET` instead
    of a `LIST`, with its score being the time when we want it to execute? We then
    have a process that checks for items that should be executed now, and if there
    are any, the process removes it from the `ZSET`, adding it to the proper `LIST`
    queue.
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通常当我们谈论时间时，我们通常会从 `ZSET` 开始谈论。如果我们想要在未来执行任何项目，我们将其添加到 `ZSET` 而不是 `LIST` 中，其分数为我们希望它执行的时间，会怎样呢？然后我们有一个检查现在应该执行的项目的过程，如果有，该过程将该项目从
    `ZSET` 中移除，并将其添加到适当的 `LIST` 队列中。
- en: We can’t wait/re-enqueue items as described in the first, because that’ll waste
    the worker process’s time. We also can’t create a local waiting list as described
    in the second option, because if the worker process crashes for an unrelated reason,
    we lose any pending work items it knew about. We’ll instead use a secondary `ZSET`
    as described in the third option, because it’s simple, straightforward, and we
    can use a lock from [section 6.2](#ch06lev1sec2) to ensure that the move is safe.
  id: totrans-984
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能像第一种方法那样等待/重新入队项，因为这会浪费工作进程的时间。我们也不能像第二种方法那样创建一个本地等待列表，因为如果工作进程因无关原因崩溃，我们将丢失它所知道的任何挂起的任务项。我们将使用第三种方法中描述的二级
    `ZSET`，因为它简单、直接，并且我们可以使用 [第 6.2 节](#ch06lev1sec2) 中的锁来确保移动是安全的。
- en: 'Each delayed item in the `ZSET` queue will be a JSON-encoded list of four items:
    a unique identifier, the queue where the item should be inserted, the name of
    the callback to call, and the arguments to pass to the callback. We include the
    unique identifier in order to differentiate all calls easily, and to allow us
    to add possible reporting features later if we so choose. The score of the item
    will be the time when the item should be executed. If the item can be executed
    immediately, we’ll insert the item into the list queue instead. For our unique
    identifier, we’ll again use a 128-bit randomly generated UUID. The code to create
    an (optionally) delayed task can be seen next.'
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: '`ZSET` 队列中的每个延迟项都将是一个包含四个元素的 JSON 编码列表：一个唯一标识符、应插入该项的队列名称、要调用的回调函数名称以及传递给回调函数的参数。我们包含唯一标识符是为了方便区分所有调用，并允许我们在需要时添加可能的报告功能。该项的分数将是该项应该执行的时间。如果该项可以立即执行，我们将该项插入到列表队列中。对于我们的唯一标识符，我们再次使用一个
    128 位随机生成的 UUID。创建（可选）延迟任务的代码将在下面展示。'
- en: Listing 6.22\. The `execute_later()` function
  id: totrans-986
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.22\. `execute_later()` 函数
- en: '![](138fig01_alt.jpg)'
  id: totrans-987
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](138fig01_alt.jpg)'
- en: When the queue item is to be executed without delay, we continue to use the
    old list-based queue. But if we need to delay the item, we add the item to the
    delayed `ZSET`. An example of the delayed queue emails to be sent can be seen
    in [figure 6.10](#ch06fig10).
  id: totrans-988
  prefs: []
  type: TYPE_NORMAL
  zh: 当队列项需要立即执行时，我们继续使用旧的基于列表的队列。但如果我们需要延迟该项，我们将该项添加到延迟的 `ZSET` 中。延迟队列中要发送的电子邮件示例可以在
    [图 6.10](#ch06fig10) 中看到。
- en: Figure 6.10\. A delayed task queue using a `ZSET`
  id: totrans-989
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10\. 使用`ZSET`的延迟任务队列
- en: '![](06fig10.jpg)'
  id: totrans-990
  prefs: []
  type: TYPE_IMG
  zh: '![图片](06fig10.jpg)'
- en: Unfortunately, there isn’t a convenient method in Redis to block on `ZSET`s
    until a score is lower than the current Unix timestamp, so we need to manually
    poll. Because delayed items are only going into a single queue, we can just fetch
    the first item with the score. If there’s no item, or if the item still needs
    to wait, we’ll wait a brief period and try again. If there is an item, we’ll acquire
    a lock based on the identifier in the item (a fine-grained lock), remove the item
    from the `ZSET`, and add the item to the proper queue. By moving items into queues
    instead of executing them directly, we only need to have one or two of these running
    at any time (instead of as many as we have workers), so our polling overhead is
    kept low. The code for polling our delayed queue is in the following listing.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在Redis中并没有一个方便的方法可以阻塞在`ZSET`上，直到分数低于当前的Unix时间戳，因此我们需要手动轮询。由于延迟项只进入单个队列，我们只需获取具有分数的第一个项。如果没有项，或者项仍然需要等待，我们将等待一段时间后再次尝试。如果有项，我们将根据项中的标识符（一个细粒度锁）获取锁，将项从`ZSET`中移除，并将项添加到适当的队列中。通过将项移动到队列而不是直接执行，我们只需要在任何时候运行一两个这样的任务（而不是像我们有那么多工作者那样），因此我们的轮询开销保持较低。轮询我们的延迟队列的代码如下所示。
- en: Listing 6.23\. The `poll_queue()` function
  id: totrans-992
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.23\. `poll_queue()`函数
- en: '![](139fig01_alt.jpg)'
  id: totrans-993
  prefs: []
  type: TYPE_IMG
  zh: '![图片](139fig01_alt.jpg)'
- en: As is clear from [listing 6.23](#ch06ex23), because `ZSET`s don’t have a blocking
    pop mechanism like `LIST`s do, we need to loop and retry fetching items from the
    queue. This can increase load on the network and on the processors performing
    the work, but because we’re only using one or two of these pollers to move items
    from the `ZSET` to the `LIST` queues, we won’t waste too many resources. If we
    further wanted to reduce overhead, we could add an adaptive method that increases
    the sleep time when it hasn’t seen any items in a while, or we could use the time
    when the next item was scheduled to help determine how long to sleep, capping
    it at 100 milliseconds to ensure that tasks scheduled only slightly in the future
    are executed in a timely fashion.
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 如[列表6.23](#ch06ex23)所示，由于`ZSET`没有像`LIST`那样的阻塞弹出机制，我们需要循环并重试从队列中获取项。这可能会增加网络和执行工作的处理器的负载，但由于我们只使用一两个这样的轮询器将项从`ZSET`移动到`LIST`队列，我们不会浪费太多资源。如果我们进一步想要减少开销，我们可以添加一个自适应方法，在一段时间内没有看到任何项时增加睡眠时间，或者我们可以使用下一个项计划的时间来帮助确定睡眠时间，将其限制在100毫秒以内，以确保仅稍后计划的任务能够及时执行。
- en: Respecting priorities
  id: totrans-995
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 尊重优先级
- en: In the basic sense, delayed tasks have the same sort of priorities that our
    first-in, first-out queue had. Because they’ll go back on their original destination
    queues, they’ll be executed with the same sort of priority. But what if we wanted
    delayed tasks to execute as soon as possible after their time to execute has come
    up?
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本意义上，延迟任务具有与我们的先进先出队列相同的优先级。因为它们将回到它们原始的目的地队列，所以将以相同的优先级执行。但如果我们想让延迟任务在执行时间到来后尽快执行呢？
- en: The simplest way to do this is to add some extra queues to make scheduled tasks
    jump to the front of the queue. If we have our high-, medium-, and low-priority
    queues, we can also create high-delayed, medium-delayed, and low-delayed queues,
    which are passed to the `worker_watch_queues()` function as `["high-delayed",
    "high", "medium-delayed", "medium", "low-delayed", "low"]`. Each of the delayed
    queues comes just before its nondelayed equivalent.
  id: totrans-997
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一点的最简单方法是为调度任务添加一些额外的队列，使它们能够跳到队列的前面。如果我们有高、中、低优先级的队列，我们还可以创建高延迟、中延迟和低延迟队列，这些队列作为`["高延迟",
    "高", "中延迟", "中", "低延迟", "低"]`传递给`worker_watch_queues()`函数。每个延迟队列都紧接在其非延迟等效队列之前。
- en: Some of you may be wondering, “If we’re having them jump to the front of the
    queue, why not just use `LPUSH` instead of `RPUSH`?” Suppose that all of our workers
    are working on tasks for the medium queue, and will take a few seconds to finish.
    Suppose also that we have three delayed tasks that are found and `LPUSH`ed onto
    the front of the medium queue. The first is pushed, then the second, and then
    the third. But on the medium queue, the third task to be pushed will be executed
    first, which violates our expectations that things that we want to execute earlier
    should be executed earlier.
  id: totrans-998
  prefs: []
  type: TYPE_NORMAL
  zh: 一些朋友可能会想，“如果我们让他们跳到队列的前面，为什么不直接使用 `LPUSH` 而不是 `RPUSH` 呢？”假设我们的所有工作者都在处理中等队列的任务，并且需要几秒钟才能完成。假设我们还有三个找到的延迟任务，它们被
    `LPUSH` 到中等队列的前面。第一个被推入，然后是第二个，然后是第三个。但在中等队列中，即将被推入的第三个任务将首先执行，这违反了我们希望尽早执行的任务应该尽早执行的想法。
- en: If you use Python and you’re interested in a queue like this, I’ve written a
    package called `RPQueue` that offers delayed task execution semantics similar
    to the preceding code snippets. It does include more functionality, so if you
    want a queue and are already using Redis, give `RPQueue` a look at [http://github.com/josiahcarlson/rpqueue/](http://github.com/josiahcarlson/rpqueue/).
  id: totrans-999
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Python 并且对这种队列感兴趣，我编写了一个名为 `RPQueue` 的包，它提供了类似于前面代码片段的延迟任务执行语义。它还包括更多功能，所以如果你需要一个队列并且已经在使用
    Redis，请查看 [http://github.com/josiahcarlson/rpqueue/](http://github.com/josiahcarlson/rpqueue/)。
- en: When we use task queues, sometimes we need our tasks to report back to other
    parts of our application with some sort of messaging system. In the next section,
    we’ll talk about creating message queues that can be used to send to a single
    recipient, or to communicate between many senders and receivers.
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用任务队列时，有时我们需要我们的任务通过某种消息系统向应用程序的其他部分报告。在下一节中，我们将讨论创建可以用于向单个收件人发送消息或用于许多发送者和接收者之间通信的消息队列。
- en: 6.5\. Pull messaging
  id: totrans-1001
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5. 拉取消息
- en: When sending and receiving messages between two or more clients, there are two
    common ways of looking at how the messages are delivered. One method, called *push
    messaging*, causes the sender to spend some time making sure that all of the recipients
    of the message receive it. Redis has built-in commands for handling push messaging
    called `PUBLISH` and `SUBSCRIBE`, whose drawbacks and use we discussed in [chapter
    3](kindle_split_014.html#ch03).^([[2](#ch06fn02)]) The second method, called *pull
    messaging*, requires that the recipients of the message fetch the messages instead.
    Usually, messages will wait in a sort of mailbox for the recipient to fetch them.
  id: totrans-1002
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个或多个客户端之间发送和接收消息时，有两种常见的方式来观察消息的传递方式。一种方法称为 *推送消息*，它使得发送者花费一些时间确保所有消息的收件人都收到了消息。Redis
    内置了处理推送消息的命令，称为 `PUBLISH` 和 `SUBSCRIBE`，我们在第 3 章中讨论了它们的缺点和使用方法。[第 3 章](kindle_split_014.html#ch03).^([[2](#ch06fn02)])
    第二种方法称为 *拉取消息*，它要求消息的收件人去获取消息。通常，消息会在一种类似邮箱的地方等待收件人去取。
- en: ² Briefly, these drawbacks are that the client must be connected at all times
    to receive messages, disconnections can cause the client to lose messages, and
    older versions of Redis could become unusable, crash, or be killed if there was
    a slow subscriber.
  id: totrans-1003
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 简而言之，这些缺点是客户端必须始终连接才能接收消息，断开连接可能导致客户端丢失消息，而且 Redis 的旧版本如果有一个慢速订阅者，可能会变得不可用、崩溃或被杀死。
- en: Though push messaging can be useful, we run into problems when clients can’t
    stay connected all the time for one reason or another. To address this limitation,
    we’ll write two different pull messaging methods that can be used as a replacement
    for `PUBLISH`/`SUBSCRIBE`.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然推送消息可能很有用，但当我们因为某种原因无法始终连接客户端时，我们会遇到问题。为了解决这个问题，我们将编写两种不同的拉取消息方法，这些方法可以用作
    `PUBLISH`/`SUBSCRIBE` 的替代。
- en: We’ll first start with single-recipient messaging, since it shares much in common
    with our first-in, first-out queues. Later in this section, we’ll move to a method
    where we can have multiple recipients of a message. With multiple recipients,
    we can replace Redis `PUBLISH` and `SUBSCRIBE` when we need our messages to get
    to all recipients, even if they were disconnected.
  id: totrans-1005
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从单收件人消息开始，因为它与我们的先进先出队列有很多共同之处。在本节的后半部分，我们将转向一种可以有多位消息收件人的方法。有了多位收件人，当我们需要消息到达所有收件人时，即使他们已经断开连接，我们也可以替换
    Redis 的 `PUBLISH` 和 `SUBSCRIBE`。
- en: 6.5.1\. Single-recipient publish/subscribe replacement
  id: totrans-1006
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1. 单收件人发布/订阅替代
- en: One common pattern that we find with Redis is that we have clients of one kind
    or another (server processes, users in a chat, and so on) that listen or wait
    for messages on their own channel. They’re the only ones that receive those messages.
    Many programmers will end up using Redis `PUBLISH` and `SUBSCRIBE` commands to
    send messages and wait for messages, respectively. But if we need to receive messages,
    even in the face of connection issues, `PUBLISH` and `SUBSCRIBE` don’t help us
    much.
  id: totrans-1007
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在Redis中发现的常见模式是，我们有一类或另一类的客户端（服务器进程、聊天中的用户等）会监听或等待它们自己的通道上的消息。他们是唯一接收这些消息的人。许多程序员最终会使用Redis的`PUBLISH`和`SUBSCRIBE`命令来发送消息和等待消息。但如果我们需要接收消息，即使在面对连接问题时，`PUBLISH`和`SUBSCRIBE`也帮不上我们太多忙。
- en: Breaking from our game company focus, Fake Garage Startup wants to release a
    mobile messaging application. This application will connect to their web servers
    to send and receive SMS/MMS-like messages (basically a text or picture messaging
    replacement). The web server will be handling authentication and communication
    with the Redis back end, and Redis will be handling the message routing/storage.
  id: totrans-1008
  prefs: []
  type: TYPE_NORMAL
  zh: 脱离我们游戏公司的焦点，假车库创业公司希望发布一个移动消息应用。这个应用将连接到他们的网络服务器，发送和接收类似SMS/MMS的消息（基本上是文本或图片消息的替代品）。网络服务器将处理身份验证和与Redis后端的通信，Redis将处理消息的路由/存储。
- en: Each message will only be received by a single client, which greatly simplifies
    our problem. To handle messages in this way, we use a single `LIST` for each mobile
    client. Senders cause messages to be placed in the recipient’s `LIST`, and any
    time the recipient’s client makes a request, it fetches the most recent messages.
    With HTTP 1.1’s ability to pipeline requests, or with more modern web socket support,
    our mobile client can either make a request for all waiting messages (if any),
    can make requests one at a time, or can fetch 10 and use `LTRIM` to remove the
    first 10 items.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 每条消息只会被单个客户端接收，这极大地简化了我们的问题。为了以这种方式处理消息，我们为每个移动客户端使用一个单独的`LIST`。发送者将消息放入收件人的`LIST`中，而每当收件人的客户端发起请求时，它会获取最新的消息。利用HTTP
    1.1的请求管道能力，或者更现代的WebSocket支持，我们的移动客户端可以请求所有等待的消息（如果有），可以一次请求一个，或者可以获取10条消息并使用`LTRIM`来移除前10条。
- en: Because you already know how to push and pop items from lists from earlier sections,
    most recently from our first-in, first-out queues from [section 6.4.1](#ch06lev2sec12),
    we’ll skip including code to send messages, but an example incoming message queue
    for user jack451 is illustrated in [figure 6.11](#ch06fig11).
  id: totrans-1010
  prefs: []
  type: TYPE_NORMAL
  zh: 由于你已经知道如何从前面的章节中推入和弹出列表项，最近的是从[6.4.1节](#ch06lev2sec12)中的先进先出队列，我们将跳过发送消息的代码，但一个用户jack451的示例传入消息队列在[图6.11](#ch06fig11)中展示。
- en: Figure 6.11\. jack451 has some messages from Jill and his mother waiting for
    him.
  id: totrans-1011
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11\. jack451有一些来自Jill和他母亲的待读消息。
- en: '![](06fig11.jpg)'
  id: totrans-1012
  prefs: []
  type: TYPE_IMG
  zh: '![图6.11](06fig11.jpg)'
- en: With `LIST`s, senders can also be notified if the recipient hasn’t been connecting
    recently, hasn’t received their previous messages, or maybe has too many pending
    messages; all by checking the messages in the recipient’s `LIST`. If the system
    were limited by a recipient needing to be connected all the time, as is the case
    with `PUBLISH`/`SUBSCRIBE`, messages would get lost, clients wouldn’t know if
    their message got through, and slow clients could result in outgoing buffers growing
    potentially without limit (in older versions of Redis) or getting disconnected
    (in newer versions of Redis).
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`LIST`，如果收件人最近没有连接，没有收到他们的上一条消息，或者可能有太多待处理消息，发送者也可以得到通知；所有这些只需检查收件人的`LIST`中的消息。如果系统限制在收件人需要始终连接的情况下，就像`PUBLISH`/`SUBSCRIBE`那样，消息就会丢失，客户端不知道他们的消息是否送达，慢速客户端可能导致输出缓冲区无限增长（在Redis的旧版本中）或断开连接（在Redis的新版本中）。
- en: With single-recipient messaging out of the way, it’s time to talk about replacing
    `PUBLISH` and `SUBSCRIBE` when we want to have multiple listeners to a given channel.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完单收件人消息后，现在是时候讨论当我们想要给定的通道有多个监听者时，如何替换`PUBLISH`和`SUBSCRIBE`了。
- en: 6.5.2\. Multiple-recipient publish/subscribe replacement
  id: totrans-1015
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 多收件人发布/订阅替代方案
- en: Single-recipient messaging is useful, but it doesn’t get us far in replacing
    the `PUBLISH` and `SUBSCRIBE` commands when we have multiple recipients. To do
    that, we need to turn our problem around. In many ways, Redis `PUBLISH`/`SUBSCRIBE`
    is like group chat where whether someone’s connected determines whether they’re
    in the group chat. We want to remove that “need to be connected all the time”
    requirement, and we’ll implement it in the context of chatting.
  id: totrans-1016
  prefs: []
  type: TYPE_NORMAL
  zh: 单个接收者消息很有用，但当我们有多个接收者时，它并不能帮助我们完全替代`PUBLISH`和`SUBSCRIBE`命令。为了做到这一点，我们需要转变我们的问题。在许多方面，Redis的`PUBLISH`/`SUBSCRIBE`就像群聊，其中某人是否连接决定了他们是否在群聊中。我们希望消除“始终需要连接”的要求，我们将在聊天的上下文中实现它。
- en: Let’s look at Fake Garage Startup’s next problem. After quickly implementing
    their user-to-user messaging system, Fake Garage Startup realized that replacing
    SMS is good, but they’ve had many requests to add group chat functionality. Like
    before, their clients may connect or disconnect at any time, so we can’t use the
    built-in `PUBLISH`/`SUBSCRIBE` method.
  id: totrans-1017
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看假车库创业公司接下来的问题。在快速实现了他们的用户对用户消息系统后，假车库创业公司意识到，虽然取代短信是好的，但他们收到了许多添加群聊功能的要求。就像以前一样，他们的客户可能随时连接或断开连接，所以我们不能使用内置的`PUBLISH`/`SUBSCRIBE`方法。
- en: Each new group chat will have a set of original recipients of the group messages,
    and users can join or leave the group if they want. Information about what users
    are in the chat will be stored as a `ZSET` with members being the usernames of
    the recipients, and values being the highest message ID the user has received
    in the chat. Which chats an individual user is a part of will also be stored as
    a `ZSET`, with members being the groups that the user is a part of, and scores
    being the highest message ID that the user has received in that chat. Information
    about some users and chats can be seen in [figure 6.12](#ch06fig12).
  id: totrans-1018
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新的群聊都将有一组群消息的原始接收者，用户可以根据需要加入或离开群组。关于哪些用户在聊天中的信息将存储为一个`ZSET`，其中成员是接收者的用户名，值是用户在聊天中收到的最高消息ID。单个用户参与的聊天也将存储为一个`ZSET`，其中成员是用户所属的组，分数是用户在该聊天中收到的最高消息ID。一些用户和聊天的信息可以在[图6.12](#ch06fig12)中看到。
- en: Figure 6.12\. Some example chat and user data. The chat `ZSET`s show users and
    the maximum IDs of messages in that chat that they’ve seen. The seen `ZSET`s list
    chat IDs per user, again with the maximum message ID in the given chat that they’ve
    seen.
  id: totrans-1019
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12。一些示例聊天和用户数据。聊天`ZSET`s显示了用户和他们已看到的该聊天中的最大消息ID。已查看`ZSET`s按用户列出聊天ID，同样，它们在给定聊天中已看到的最大消息ID。
- en: '![](06fig12.jpg)'
  id: totrans-1020
  prefs: []
  type: TYPE_IMG
  zh: '![06fig12.jpg](06fig12.jpg)'
- en: As you can see, user jason22 has seen five of six chat messages sent in chat:827,
    in which jason22 and jeff24 are participating.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，用户jason22已经看到了在聊天:827中发送的六条聊天消息中的五条，其中jason22和jeff24正在参与。
- en: Creating a chat session
  id: totrans-1022
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建聊天会话
- en: The content of chat sessions themselves will be stored in `ZSET`s, with messages
    as members and message IDs as scores. To create and start a chat, we’ll increment
    a global counter to get a new chat ID. We’ll then create a `ZSET` with all of
    the users that we want to include with seen IDs being 0, and add the group to
    each user’s group list `ZSET`. Finally, we’ll send the initial message to the
    users by placing the message in the chat `ZSET`. The code to create a chat is
    shown here.
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 聊天会话的内容本身将存储在`ZSET`s中，其中消息作为成员，消息ID作为分数。为了创建并启动一个聊天，我们将增加一个全局计数器以获取一个新的聊天ID。然后，我们将创建一个包含所有我们想要包含的用户`ZSET`，其中查看ID为0，并将该组添加到每个用户的组列表`ZSET`中。最后，我们将通过将消息放置在聊天`ZSET`中来向用户发送初始消息。创建聊天的代码如下所示。
- en: Listing 6.24\. The `create_chat()` function
  id: totrans-1024
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.24。`create_chat()`函数
- en: '![](142fig01_alt.jpg)'
  id: totrans-1025
  prefs: []
  type: TYPE_IMG
  zh: '![142fig01_alt.jpg](142fig01_alt.jpg)'
- en: About the only thing that may be surprising is our use of what’s called a *generator
    expression* from within a call to the `dict()` object constructor. This shortcut
    lets us quickly construct a dictionary that maps users to an initially 0-valued
    score, which `ZADD` can accept in a single call.
  id: totrans-1026
  prefs: []
  type: TYPE_NORMAL
  zh: 关于可能令人惊讶的唯一事情可能就是我们在调用`dict()`对象构造函数时使用所谓的*生成器表达式*。这个快捷方式让我们可以快速构建一个字典，该字典将用户映射到一个初始值为0的分数，`ZADD`可以在单个调用中接受。
- en: '|  |'
  id: totrans-1027
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Generator expressions and dictionary construction
  id: totrans-1028
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 生成器表达式和字典构建
- en: Python dictionaries can be easily constructed by passing a sequence of pairs
    of values. The first item in the pair becomes the key; the second item becomes
    the value. [Listing 6.24](#ch06ex24) shows some code that looks odd, where we
    actually generate the sequence to be passed to the dictionary in-line. This type
    of sequence generation is known as a *generator expression*, which you can read
    more about at [http://mng.bz/TTKb](http://mng.bz/TTKb).
  id: totrans-1029
  prefs: []
  type: TYPE_NORMAL
  zh: Python 字典可以通过传递一系列值对的序列轻松构建。一对中的第一个项目成为键；第二个项目成为值。[列表 6.24](#ch06ex24) 展示了一些看起来有些奇怪的代码，其中我们实际上在行内生成传递给字典的序列。这种类型的序列生成称为*生成器表达式*，你可以在
    [http://mng.bz/TTKb](http://mng.bz/TTKb) 上了解更多相关信息。
- en: '|  |'
  id: totrans-1030
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Sending messages
  id: totrans-1031
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 发送消息
- en: To send a message, we must get a new message ID, and then add the message to
    the chat’s messages `ZSET`. Unfortunately, there’s a race condition in sending
    messages, but it’s easily handled with the use of a lock from [section 6.2](#ch06lev1sec2).
    Our function for sending a message using a lock is shown next.
  id: totrans-1032
  prefs: []
  type: TYPE_NORMAL
  zh: 要发送消息，我们必须获取一个新的消息 ID，然后将消息添加到聊天消息的 `ZSET` 中。不幸的是，发送消息存在竞争条件，但使用 [第 6.2 节](#ch06lev1sec2)
    中的锁可以轻松处理。我们使用锁发送消息的函数如下所示。
- en: Listing 6.25\. The `send_message()` function
  id: totrans-1033
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.25\. `send_message()` 函数
- en: '![](143fig01_alt.jpg)'
  id: totrans-1034
  prefs: []
  type: TYPE_IMG
  zh: '![图 143](143fig01_alt.jpg)'
- en: Most of the work involved in sending a chat message is preparing the information
    to be sent itself; actually sending the message involves adding it to a `ZSET`.
    We use locking around the packed message construction and addition to the `ZSET`
    for the same reasons that we needed a lock for our counting semaphore earlier.
    Generally, when we use a value from Redis in the construction of another value
    we need to add to Redis, we’ll either need to use a `WATCH`/`MULTI`/`EXEC` transaction
    or a lock to remove race conditions. We use a lock here for the same performance
    reasons that we developed it in the first place.
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: 发送聊天消息所涉及的大部分工作都是准备要发送的信息本身；实际上发送消息涉及将其添加到 `ZSET`。我们在打包消息构建和添加到 `ZSET` 时使用锁定，原因与我们需要在之前的计数信号量中使用锁的原因相同。通常，当我们需要将
    Redis 中的值用于构建要添加到 Redis 的另一个值时，我们可能需要使用 `WATCH`/`MULTI`/`EXEC` 事务或锁来消除竞争条件。我们在这里使用锁，原因与最初开发它的原因相同。
- en: Now that we’ve created the chat and sent the initial message, users need to
    find out information about the chats they’re a part of and how many messages are
    pending, and they need to actually receive the messages.
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了聊天并发送了初始消息，用户需要了解他们参与的聊天信息以及待处理消息的数量，并且他们需要实际接收这些消息。
- en: Fetching messages
  id: totrans-1037
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 获取消息
- en: To fetch all pending messages for a user, we need to fetch group IDs and message
    IDs seen from the user’s `ZSET` with `ZRANGE`. When we have the group IDs and
    the messages that the user has seen, we can perform `ZRANGEBYSCORE` operations
    on all of the message `ZSET`s. After we’ve fetched the messages for the chat,
    we update the seen `ZSET` with the proper ID and the user entry in the group `ZSET`,
    and we go ahead and clean out any messages from the group chat that have been
    received by everyone in the chat, as shown in the following listing.
  id: totrans-1038
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取用户的全部待处理消息，我们需要使用 `ZRANGE` 从用户的 `ZSET` 中获取群组 ID 和已看到的消息 ID。当我们有了群组 ID 和用户已看到的消息后，我们可以在所有的消息
    `ZSET` 上执行 `ZRANGEBYSCORE` 操作。在获取聊天消息后，我们使用正确的 ID 和群组 `ZSET` 中的用户条目更新已看到 `ZSET`，然后我们继续清理群聊中所有用户已收到的消息，如下所示列表所示。
- en: Listing 6.26\. The `fetch_pending_messages()` function
  id: totrans-1039
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.26\. `fetch_pending_messages()` 函数
- en: '![](144fig01_alt.jpg)'
  id: totrans-1040
  prefs: []
  type: TYPE_IMG
  zh: '![图 144](144fig01_alt.jpg)'
- en: Fetching pending messages is primarily a matter of iterating through all of
    the chats for the user, pulling the messages, and cleaning up messages that have
    been seen by all users in a chat.
  id: totrans-1041
  prefs: []
  type: TYPE_NORMAL
  zh: 获取待处理消息主要是迭代用户的所有聊天，提取消息，并清理所有用户在聊天中已看到的消息。
- en: Joining and leaving the chat
  id: totrans-1042
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 加入和离开聊天
- en: We’ve sent and fetched messages from group chats; all that remains is joining
    and leaving the group chat. To join a group chat, we fetch the most recent message
    ID for the chat, and we add the chat information to the user’s seen `ZSET` with
    the score being the most recent message ID. We also add the user to the group’s
    member list, again with the score being the most recent message ID. See the next
    listing for the code for joining a group.
  id: totrans-1043
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经从群聊中发送和获取了消息；剩下的是加入和离开群聊。要加入群聊，我们获取聊天最近的消息 ID，并将聊天信息添加到用户的已看到 `ZSET` 中，分数为最近的消息
    ID。我们还将用户添加到群组的成员列表中，同样分数为最近的消息 ID。请参阅下一列表中的加入群聊的代码。
- en: Listing 6.27\. The `join_chat()` function
  id: totrans-1044
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.27\. `join_chat()`函数
- en: '![](145fig01_alt.jpg)'
  id: totrans-1045
  prefs: []
  type: TYPE_IMG
  zh: '![图片](145fig01_alt.jpg)'
- en: Joining a chat only requires adding the proper references to the user to the
    chat, and the chat to the user’s seen `ZSET`.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 加入聊天只需要将适当的引用添加到用户的聊天中，以及将聊天添加到用户已看到的`ZSET`中。
- en: To remove a user from the group chat, we remove the user ID from the chat `ZSET`,
    and we remove the chat from the user’s seen `ZSET`. If there are no more users
    in the chat `ZSET`, we delete the messages `ZSET` and the message ID counter.
    If there are users remaining, we’ll again take a pass and clean out any old messages
    that have been seen by all users. The function to leave a chat is shown in the
    following listing.
  id: totrans-1047
  prefs: []
  type: TYPE_NORMAL
  zh: 要从群聊中移除用户，我们从聊天`ZSET`中移除用户ID，并从用户已看到的`ZSET`中移除聊天。如果没有更多用户在聊天`ZSET`中，我们将删除消息`ZSET`和消息ID计数器。如果有用户剩余，我们再次遍历并清理所有用户都已看到的任何旧消息。离开聊天的函数如下所示。
- en: Listing 6.28\. The `leave_chat()` function
  id: totrans-1048
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.28\. `leave_chat()`函数
- en: '![](145fig02_alt.jpg)'
  id: totrans-1049
  prefs: []
  type: TYPE_IMG
  zh: '![图片](145fig02_alt.jpg)'
- en: Cleaning up after a user when they leave a chat isn’t that difficult, but requires
    taking care of a lot of little details to ensure that we don’t end up leaking
    a `ZSET` or ID somewhere.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户离开聊天时进行清理并不困难，但需要处理许多细节，以确保我们不会在某处泄露`ZSET`或ID。
- en: We’ve now finished creating a complete multiple-recipient pull messaging system
    in Redis. Though we’re looking at it in terms of chat, this same method can be
    used to replace the `PUBLISH`/`SUBSCRIBE` functions when you want your recipients
    to be able to receive messages that were sent while they were disconnected. With
    a bit of work, we could replace the `ZSET` with a `LIST`, and we could move our
    lock use from sending a message to old message cleanup. We used a `ZSET` instead,
    because it saves us from having to fetch the current message ID for every chat.
    Also, by making the sender do more work (locking around sending a message), the
    multiple recipients are saved from having to request more data and to lock during
    cleanup, which will improve performance overall.
  id: totrans-1051
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经在Redis中完成了一个完整的多个接收者拉取消息系统的创建。虽然我们是从聊天的角度来考虑的，但这个相同的方法可以用来自动替换`PUBLISH`/`SUBSCRIBE`功能，当你希望接收者在断开连接期间也能接收到发送的消息时。通过一些工作，我们可以用`LIST`替换`ZSET`，并将我们的锁使用从发送消息转移到旧消息清理。我们使用`ZSET`的原因是它使我们免于为每次聊天获取当前消息ID。此外，通过让发送者做更多的工作（在发送消息时锁定），多个接收者可以免于在清理时请求更多数据并锁定，这将提高整体性能。
- en: We now have a multiple-recipient messaging system to replace `PUBLISH` and `SUBSCRIBE`
    for group chat. In the next section, we’ll use it as a way of sending information
    about key names available in Redis.
  id: totrans-1052
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个多个接收者消息系统来替换群聊中的`PUBLISH`和`SUBSCRIBE`。在下一节中，我们将用它作为发送Redis中可用键名信息的方式。
- en: 6.6\. Distributing files with Redis
  id: totrans-1053
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 使用Redis分发文件
- en: When building distributed software and systems, it’s common to need to copy,
    distribute, or process data files on more than one machine. There are a few different
    common ways of doing this with existing tools. If we have a single server that
    will always have files to be distributed, it’s not uncommon to use NFS or Samba
    to mount a path or drive. If we have files whose contents change little by little,
    it’s also common to use a piece of software called Rsync to minimize the amount
    of data to be transferred between systems. Occasionally, when many copies need
    to be distributed among machines, a protocol called BitTorrent can be used to
    reduce the load on the server by partially distributing files to multiple machines,
    which then share their pieces among themselves.
  id: totrans-1054
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建分布式软件和系统时，通常需要在多台机器上复制、分发或处理数据文件。使用现有工具，有几种不同的常见方法来完成这项工作。如果我们有一个始终会存储要分发文件的单一服务器，使用NFS或Samba挂载路径或驱动器并不罕见。如果我们有内容逐渐变化的小文件，也常见使用名为Rsync的软件来最小化系统间传输的数据量。偶尔，当需要在多台机器之间分发许多副本时，可以使用名为BitTorrent的协议，通过将文件部分分发到多台机器来减少服务器的负载，然后这些机器之间共享它们的部分。
- en: Unfortunately, all of these methods have a significant setup cost and value
    that’s somewhat relative. NFS and Samba can work well, but both can have significant
    issues when network connections aren’t perfect (or even if they are perfect),
    due to the way both of these technologies are typically integrated with operating
    systems. Rsync is designed to handle intermittent connection issues, since each
    file or set of files can be partially transferred and resumed, but it suffers
    from needing to download complete files before processing can start, and requires
    interfacing our software with Rsync in order to fetch the files (which may or
    may not be a problem). And though BitTorrent is an amazing technology, it only
    really helps if we’re running into limits sending from our server, or if our network
    is underutilized. It also relies on interfacing our software with a BitTorrent
    client that may not be available on all platforms, and which may not have a convenient
    method to fetch files.
  id: totrans-1055
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，所有这些方法都有显著的设置成本和相对价值。NFS和Samba可以很好地工作，但两者在网络连接不完美（甚至如果它们是完美的）时都可能存在重大问题，因为这两种技术通常与操作系统集成。Rsync旨在处理间歇性连接问题，因为每个文件或文件集可以部分传输并恢复，但它需要在处理开始之前下载完整的文件，并且需要将我们的软件与Rsync接口以获取文件（这可能是也可能不是问题）。尽管BitTorrent是一项惊人的技术，但它只有在我们的服务器发送遇到限制或我们的网络利用率不足时才能真正帮助。它还依赖于将我们的软件与可能不在所有平台上都可用且可能没有方便的文件获取方法的BitTorrent客户端接口。
- en: 'Each of the three methods described also require setup and maintenance of users,
    permissions, and/or servers. Because we already have Redis installed, running,
    and available, we’ll use Redis to distribute files instead. By using Redis, we
    bypass issues that some other software has: our client handles connection issues
    well, we can fetch the data directly with our clients, and we can start processing
    data immediately (no need to wait for an entire file).'
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 所述的三个方法都需要设置和维护用户、权限和/或服务器。因为我们已经安装、运行并可用Redis，我们将使用Redis来分发文件。通过使用Redis，我们绕过了某些其他软件存在的问题：我们的客户端很好地处理连接问题，我们可以直接使用我们的客户端获取数据，并且可以立即开始处理数据（无需等待整个文件）。
- en: 6.6.1\. Aggregating users by location
  id: totrans-1057
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.1\. 按地理位置聚合用户
- en: 'Let’s take a moment and look back at an earlier problem that we solved for
    Fake Game Company. With the ability to discover where users are accessing the
    game from thanks to our IP-to-city lookup in [chapter 5](kindle_split_016.html#ch05),
    Fake Game Company has found itself needing to reparse many gigabytes of log files.
    They’re looking to aggregate user visitation patterns over time in a few different
    dimensions: per country, per region, per city, and more. Because we need this
    to be run in real time over new data, we’ve already implemented callbacks to perform
    the aggregate operations.'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们花一点时间回顾一下我们之前为假游戏公司解决的问题。得益于我们在第5章中提到的IP到城市的查找功能，假游戏公司发现自己需要重新解析许多GB的日志文件。他们希望在不同维度上对用户访问模式进行时间聚合：按国家、按地区、按城市等。由于我们需要在实时新数据上运行此操作，我们已实现了回调来执行聚合操作。
- en: As you may remember from [chapter 5](kindle_split_016.html#ch05), Fake Game
    Company has been around for about 2 years. They have roughly 1,000,000 users per
    day, but they have roughly 10 events per user per day. That gives us right around
    7.3 billion log lines to process. If we were to use one of the earlier methods,
    we’d copy the log files to various machines that need to process the data, and
    then go about processing the log files. This works, but then we need to copy the
    data, potentially delaying processing, and using storage space on every machine
    that processes the data, which later needs to be cleaned up.
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从第5章（kindle_split_016.html#ch05）中可能记得的那样，假游戏公司已经存在大约2年了。他们每天大约有100万用户，但每个用户每天大约有10个事件。这给我们带来了大约73亿条日志行需要处理。如果我们使用之前的方法之一，我们会将日志文件复制到需要处理数据的各种机器上，然后开始处理日志文件。这可以工作，但我们需要复制数据，这可能会延迟处理，并在处理数据的每台机器上使用存储空间，这些空间后来需要清理。
- en: In this particular case, instead of copying files around, we could write a one-time
    map-reduce^([[3](#ch06fn03)]) process to handle all of this data. But because
    map-reduces are designed to not share memory between items to be processed (each
    item is usually one log line), we can end up taking more time with map-reduce
    than if we spent some time writing it by hand to share memory. More specifically,
    if we load our IP-to-city lookup table into memory in Python (which we’d only
    want to do if we had a lot of processing to do, and we do), we can perform about
    200k IP-to-city ID lookups per second, which is faster than we could expect a
    single Redis instance to respond to the same queries. Also, to scale with map-reduce,
    we’d have to run at least a few instances of Redis to keep up with the map-reduces.
  id: totrans-1060
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定案例中，我们不是复制文件，而是可以编写一个一次性的map-reduce^([[3](#ch06fn03)])过程来处理所有这些数据。但是，由于map-reduces旨在在要处理的项目之间不共享内存（每个项目通常是一条日志行），我们可能会花费比手动编写并共享内存更多的时间。更具体地说，如果我们把IP到城市的查找表加载到Python内存中（我们只想在有很多处理要做的情况下这样做，我们确实有），我们每秒可以执行大约200k个IP到城市ID的查找，这比我们期望单个Redis实例响应相同查询的速度要快。此外，为了与map-reduce扩展，我们至少需要运行几个Redis实例来跟上map-reduces。
- en: ³ MapReduce (or Map/Reduce) is a type of distributed computation popularized
    by Google, which can offer high performance and simplicity for some problems.
  id: totrans-1061
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ MapReduce（或Map/Reduce）是由谷歌推广的一种分布式计算类型，它可以为某些问题提供高性能和简单性。
- en: With the three standard methods of handling this already discounted (NFS/Samba,
    copying files, map-reduce), let’s look at some other practical pieces that we’ll
    need to solve to actually perform all of our lookups.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 已经排除了处理这种问题的三种标准方法（NFS/Samba、复制文件、map-reduce），让我们看看我们实际上需要解决的其他实际问题。
- en: Aggregating data locally
  id: totrans-1063
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 本地聚合数据
- en: In order to process that many log entries efficiently, we’ll need to locally
    cache aggregates before updating Redis in order to minimize round trips. Why?
    If we have roughly 10 million log lines to process for each day, then that’s roughly
    10 million writes to Redis. If we perform the aggregates locally on a per-country
    basis for the entire day (being that there are around 300 countries), we can instead
    write 300 values to Redis. This will significantly reduce the number of round
    trips between Redis, reducing the number of commands processed, which in turn
    will reduce the total processing time.
  id: totrans-1064
  prefs: []
  type: TYPE_NORMAL
  zh: 为了高效处理那么多的日志条目，我们需要在更新Redis之前本地缓存聚合数据，以最小化往返次数。为什么？如果我们每天需要处理大约1000万条日志行，那么这就意味着大约有1000万次写入Redis。如果我们按国家每天进行本地聚合（考虑到大约有300个国家），我们就可以写入Redis的值只有300个。这将显著减少Redis之间的往返次数，减少处理的命令数量，从而减少总处理时间。
- en: If we don’t do anything intelligent about local caching, and we have 10 aggregates
    that we want to calculate, we’re looking at around 10 days to process all of the
    data. But anything on the country or region level can be aggregated completely
    (for the day) before being sent to Redis. And generally because the top 10% of
    cities (there are roughly 350,000 cities in our sample dataset) amount for more
    than 90% of our game’s users, we can also locally cache any city-level aggregates.
    So by performing local caching of aggregates, we’re not limited by Redis on aggregate
    throughput.
  id: totrans-1065
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不进行任何关于本地缓存的智能操作，并且我们想要计算10个聚合，我们可能需要大约10天的时间来处理所有数据。但是，任何国家或地区级别的东西都可以在发送到Redis之前完全（对于一天）进行聚合。而且，由于我们样本数据集中大约有35万个城市，这10%的城市（用户占我们游戏用户的90%以上），我们也可以本地缓存任何城市级别的聚合。因此，通过执行本地缓存聚合，我们不会受到Redis聚合吞吐量的限制。
- en: 'Assuming that we’ve already cached a copy of our `ZSET` and `HASH` table for
    IP lookups from [section 5.3](kindle_split_016.html#ch05lev1sec3), we only need
    to worry about aggregating the data. Let’s start with the log lines that contain
    an IP address, date, time, and the operation that was performed, similar to the
    following snippet:'
  id: totrans-1066
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经从[第5.3节](kindle_split_016.html#ch05lev1sec3)缓存了IP查找的`ZSET`和`HASH`表副本，我们只需要担心聚合数据。让我们从包含IP地址、日期、时间和执行的操作的日志行开始，类似于以下片段：
- en: '[PRE7]'
  id: totrans-1067
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Given log lines of that form, let’s aggregate those lines on a daily basis per
    country. To do this, we’ll receive the line as part of a call and increment the
    appropriate counter. If the log line is empty, then we’ll say that the day’s worth
    of data is done, and we should write to Redis. The source code for performing
    this aggregate is shown next.
  id: totrans-1068
  prefs: []
  type: TYPE_NORMAL
  zh: 给定这种形式的日志行，让我们按国家每天汇总这些行。为此，我们将接收行作为调用的一部分并增加适当的计数器。如果日志行是空的，那么我们将说这一天的数据已经完成，我们应该写入
    Redis。执行此聚合的源代码如下所示。
- en: Listing 6.29\. A locally aggregating callback for a daily country-level aggregate
  id: totrans-1069
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.29\. 用于每日国家级别聚合的本地聚合回调
- en: '![](148fig01_alt.jpg)'
  id: totrans-1070
  prefs: []
  type: TYPE_IMG
  zh: '![图片](148fig01_alt.jpg)'
- en: Now that we’ve written and seen one of these aggregate functions, the rest are
    fairly similar and just as easy to write. Let’s move on to more interesting topics,
    like how we’re going to send files through Redis.
  id: totrans-1071
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经编写并看到了这些聚合函数中的一个，其余的相当相似，并且同样容易编写。让我们继续探讨更有趣的话题，比如我们如何通过 Redis 发送文件。
- en: 6.6.2\. Sending files
  id: totrans-1072
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.2\. 发送文件
- en: In order to get the log data to our logs processors, we’ll have two different
    components operating on the data. The first is a script that will be taking the
    log files and putting them in Redis under named keys, publishing the names of
    the keys to a chat channel using our group chat method from [section 6.5.2](#ch06lev2sec15),
    and waiting for notification when they’re complete (to not use more memory than
    our Redis machine has). It’ll be waiting for a notification that a key with a
    name similar to the file stored in Redis has a value equal to 10, which is our
    number of aggregation processes. The function that copies logs and cleans up after
    itself is shown in the following listing.
  id: totrans-1073
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将日志数据传递给日志处理器，我们将有两个不同的组件在数据上操作。第一个是一个脚本，它将取日志文件并将它们放入 Redis 下的命名键中，使用我们的第
    6.5.2 节中的群聊方法发布键的名称到聊天频道，并在它们完成时等待通知（以不使用比我们的 Redis 机器更多的内存）。它将等待一个通知，即与存储在 Redis
    中的文件名称相似的键的值等于 10，这是我们的聚合过程数量。复制日志并清理自身的函数如下所示。
- en: Listing 6.30\. The `copy_logs_to_redis()` function
  id: totrans-1074
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.30\. `copy_logs_to_redis()` 函数
- en: '![](ch06ex30-0.jpg)'
  id: totrans-1075
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex30-0.jpg)'
- en: '![](ch06ex30-1.jpg)'
  id: totrans-1076
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch06ex30-1.jpg)'
- en: Copying logs to Redis requires a lot of detailed steps, mostly involving being
    careful to not put too much data into Redis at one time and properly cleaning
    up after ourselves when a file has been read by all clients. The actual aspect
    of notifying logs processors that there’s a new file ready is easy, but the setup,
    sending, and cleanup are pretty detailed.
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: 将日志复制到 Redis 需要很多详细的步骤，主要涉及注意不要一次性将太多数据放入 Redis，并在所有客户端读取完文件后适当地清理。通知日志处理器有新文件准备好的实际方面是容易的，但设置、发送和清理相当详细。
- en: 6.6.3\. Receiving files
  id: totrans-1078
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.3\. 接收文件
- en: The second part of the process is a group of functions and generators that will
    fetch log filenames from the group chat. After receiving each name, it’ll process
    the log files directly from Redis, and will update the keys that the copy process
    is waiting on. This will also call our callback on each incoming line, updating
    our aggregates. The next listing shows the code for the first of these functions.
  id: totrans-1079
  prefs: []
  type: TYPE_NORMAL
  zh: 处理过程的第二部分是一组函数和生成器，将从群聊中获取日志文件名。在接收到每个名称后，它将直接从 Redis 处理日志文件，并更新复制过程等待的键。这还将对每条传入的行调用我们的回调函数，更新我们的聚合。下一个列表显示了这些函数中的第一个代码。
- en: Listing 6.31\. The `process_logs_from_redis()` function
  id: totrans-1080
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.31\. `process_logs_from_redis()` 函数
- en: '![](150fig01_alt.jpg)'
  id: totrans-1081
  prefs: []
  type: TYPE_IMG
  zh: '![图片](150fig01_alt.jpg)'
- en: Receiving information about log files is straightforward, though we do defer
    a lot of the hard work of actually reading the file from Redis to helper functions
    that generate sequences of log lines. We also need to be careful to notify the
    file sender by incrementing the counter for the log file; otherwise the file sending
    process won’t know to clean up finished log files.
  id: totrans-1082
  prefs: []
  type: TYPE_NORMAL
  zh: 接收关于日志文件的信息是直接的，尽管我们将实际从 Redis 读取文件的繁重工作推迟到生成日志行序列的辅助函数中。我们还需要小心地通过增加日志文件的计数器来通知文件发送者；否则，文件发送过程将不知道清理完成的日志文件。
- en: 6.6.4\. Processing files
  id: totrans-1083
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.6.4\. 处理文件
- en: We’re deferring some of the work of decoding our files to functions that return
    generators over data. The `readlines()` function takes the connection, key, and
    a block-iterating callback. It’ll iterate over blocks of data yielded by the block-iterating
    callback, discover line breaks, and yield lines. When provided with blocks as
    in [listing 6.32](#ch06ex32), it finds the last line ending in the block, and
    then splits the lines up to that last line ending, yielding the lines one by one.
    When it’s done, it keeps any partial lines to prepend onto the next block. If
    there’s no more data, it yields the last line by itself. There are other ways
    of finding line breaks and extracting lines in Python, but the `rfind()`/`split()`
    combination is faster than other methods.
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在将一些解码文件的工怍推迟到返回数据生成器的函数中。`readlines()` 函数接受连接、键和一个块迭代回调函数。它将迭代由块迭代回调函数产生的数据块，发现行分隔符，并产生行。当提供如
    [列表 6.32](#ch06ex32) 中的块时，它找到块中最后一个行结束符，然后分割到该最后一个行结束符的行，逐行产生。完成时，它保留任何部分行以附加到下一个块的开头。如果没有更多数据，它单独产生最后一个行。Python
    中还有其他查找行分隔符和提取行的方法，但 `rfind()`/`split()` 组合比其他方法更快。
- en: Listing 6.32\. The `readlines()` function
  id: totrans-1085
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.32\. `readlines()` 函数
- en: '![](150fig02_alt.jpg)'
  id: totrans-1086
  prefs: []
  type: TYPE_IMG
  zh: '![](150fig02_alt.jpg)'
- en: For our higher-level line-generating function, we’re iterating over blocks produced
    by one of two readers, which allows us to focus on finding line breaks.
  id: totrans-1087
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们更高层次的行生成函数，我们正在迭代由两个读取器之一产生的块，这使我们能够专注于查找行分隔符。
- en: '|  |'
  id: totrans-1088
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Generators with yield
  id: totrans-1089
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 带有 `yield` 的生成器
- en: '[Listing 6.32](#ch06ex32) offers our first real use of Python generators with
    the `yield` statement. Generally, this allows Python to suspend and resume execution
    of code primarily to allow for easy iteration over sequences or pseudo-sequences
    of data. For more details on how generators work, you can visit the Python language
    tutorial with this short URL: [http://mng.bz/Z2b1](http://mng.bz/Z2b1).'
  id: totrans-1090
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.32](#ch06ex32) 展示了使用 `yield` 语句的 Python 生成器的第一次实际应用。通常，这允许 Python 暂停和恢复代码的执行，主要是为了允许轻松地对数据序列或伪序列进行迭代。有关生成器如何工作的更多详细信息，您可以访问带有此简短网址的
    Python 语言教程：[http://mng.bz/Z2b1](http://mng.bz/Z2b1)。'
- en: '|  |'
  id: totrans-1091
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Each of the two block-yielding callbacks, `readblocks()` and `readblocks_gz()`,
    will read blocks of data from Redis. The first yields the blocks directly, whereas
    the other automatically decompresses gzip files. We’ll use this particular layer
    separation in order to offer the most useful and reusable data reading method
    possible. The following listing shows the `readblocks()` generator.
  id: totrans-1092
  prefs: []
  type: TYPE_NORMAL
  zh: 两个产生块数据的回调函数，`readblocks()` 和 `readblocks_gz()`，将从 Redis 读取数据块。第一个函数直接产生块，而另一个函数会自动解压缩
    gzip 文件。我们将使用这种特定的分层分离，以便提供最有用和可重用的数据读取方法。下面的列表展示了 `readblocks()` 生成器。
- en: Listing 6.33\. The `readblocks()` generator
  id: totrans-1093
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.33\. `readblocks()` 生成器
- en: '![](151fig01_alt.jpg)'
  id: totrans-1094
  prefs: []
  type: TYPE_IMG
  zh: '![](151fig01_alt.jpg)'
- en: The `readblocks()` generator is primarily meant to offer an abstraction over
    our block reading, which allows us to replace it later with other types of readers,
    like maybe a filesystem reader, a memcached reader, a `ZSET` reader, or in our
    case, a block reader that handles gzip files in Redis. The next listing shows
    the `readblocks_gz()` generator.
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: '`readblocks()` 生成器主要是为了在我们的块读取上提供抽象，这允许我们稍后用其他类型的读取器替换它，比如可能是文件系统读取器、memcached
    读取器、`ZSET` 读取器，或者在我们的情况下，处理 Redis 中 gzip 文件的块读取器。下一个列表展示了 `readblocks_gz()` 生成器。'
- en: Listing 6.34\. The `readblocks_gz()` generator
  id: totrans-1096
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.34\. `readblocks_gz()` 生成器
- en: '![](ch06ex34-0.jpg)'
  id: totrans-1097
  prefs: []
  type: TYPE_IMG
  zh: '![](ch06ex34-0.jpg)'
- en: '![](ch06ex34-1.jpg)'
  id: totrans-1098
  prefs: []
  type: TYPE_IMG
  zh: '![](ch06ex34-1.jpg)'
- en: Much of the body of `readblocks_gz()` is gzip header parsing code, which is
    unfortunately necessary. For log files (like we’re parsing), gzip can offer a
    reduction of 2–5 times in storage space requirements, while offering fairly high-speed
    decompression. Though more modern compression methods are able to compress better
    (bzip2, lzma/xz, and many others) or faster (lz4, lzop, snappy, QuickLZ, and many
    others), no other method is as widely available (bzip2 comes close) or has such
    a useful range of compression ratio and CPU utilization trade-off options.
  id: totrans-1099
  prefs: []
  type: TYPE_NORMAL
  zh: '`readblocks_gz()` 函数的大部分代码是 gzip 头部解析代码，这是不幸必要的。对于日志文件（如我们正在解析的），gzip 可以提供
    2-5 倍的存储空间需求减少，同时提供相当高速的解压缩。尽管更现代的压缩方法能够更好地压缩（bzip2、lzma/xz 以及许多其他方法）或更快（lz4、lzop、snappy、QuickLZ
    以及许多其他方法），但没有其他方法像 bzip2 那样广泛可用，或者具有如此有用的压缩比和 CPU 利用率权衡选项。'
- en: 6.7\. Summary
  id: totrans-1100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7\. 概述
- en: In this chapter we’ve gone through six major topics, but in looking at those
    topics, we actually solved nine different problems. Whenever possible, we’ve taken
    steps to borrow ideas and functionality from previous sections to keep building
    more useful tools, while trying to highlight that many of the techniques that
    we use in one solution can also be used to solve other problems.
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们已经探讨了六个主要主题，但在审视这些主题时，我们实际上解决了九个不同的问题。在可能的情况下，我们采取了步骤从前面的部分借用想法和功能，以构建更多有用的工具，同时试图强调我们用于一个解决方案的技术也可以用于解决其他问题。
- en: If there’s one concept that you should take away from this entire chapter, it’s
    that although `WATCH` is a useful command, is built in, convenient, and so forth,
    having access to a working distributed lock implementation from [section 6.2](#ch06lev1sec2)
    can make concurrent Redis programming so much easier. Being able to lock at a
    finer level of detail than an entire key can reduce contention, and being able
    to lock around related operations can reduce operation complexity. We saw both
    performance improvements and operation simplicity in our revisited marketplace
    example from [section 4.6](kindle_split_015.html#ch04lev1sec6), and in our delayed
    task queue from [section 6.4.2](#ch06lev2sec13).
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这个章节中带走一个概念，那就是尽管`WATCH`是一个有用的命令，它是内置的，方便的，等等，但能够从[第6.2节](#ch06lev1sec2)访问一个工作分发的锁实现可以使并发Redis编程变得容易得多。能够在比整个键更细的粒度上锁定可以减少竞争，并且能够围绕相关操作进行锁定可以减少操作复杂性。我们在回顾的市场示例[第4.6节](kindle_split_015.html#ch04lev1sec6)和我们的延迟任务队列[第6.4.2节](#ch06lev2sec13)中看到了性能改进和操作简化。
- en: If there’s a second concept that you should remember, take to heart, and apply
    in the future, it’s that with a little work, you can build reusable components
    with Redis. We reused locks explicitly in counting semaphores, delayed task queues,
    and in our multiple-recipient pub/sub replacement. And we reused our multiple-recipient
    pub/sub replacement when we distributed files with Redis.
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有一个概念你应该记住，并认真对待，并在未来应用，那就是通过一点努力，你可以用Redis构建可重用的组件。我们在计数信号量、延迟任务队列和我们的多接收者pub/sub替换中明确重用了锁。当我们使用Redis分发文件时，我们也重用了我们的多接收者pub/sub替换。
- en: In the next chapter, we’ll continue with building more advanced tools with Redis,
    writing code that can be used to back entire applications from document indexing
    and search with scored indexing and sorting, all the way to an ad targeting system,
    and a job search system. Going forward, we’ll reuse some of these components in
    later chapters, so keep an eye out, and remember that it’s not difficult to build
    reusable components for Redis.
  id: totrans-1104
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用Redis构建更高级的工具，编写可以用于从文档索引和搜索（带有评分索引和排序）到广告定位系统，再到求职搜索系统的代码。向前看，我们将在后面的章节中重用一些这些组件，所以请留意，并记住构建Redis的可重用组件并不困难。
- en: Chapter 7\. Search-based applications
  id: totrans-1105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章\. 基于搜索的应用
- en: '*This chapter covers*'
  id: totrans-1106
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Searching in Redis
  id: totrans-1107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Redis中进行搜索
- en: Scoring your search results
  id: totrans-1108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评分搜索结果
- en: Ad targeting
  id: totrans-1109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告定位
- en: Job search
  id: totrans-1110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 求职
- en: Over the last several chapters, I’ve introduced a variety of topics and problems
    that can be solved with Redis. Redis is particularly handy in solving a class
    of problems that I generally refer to as *search-based problems*. These types
    of problems primarily involve the use of `SET` and `ZSET` intersection, union,
    and difference operations to find items that match a specified criteria.
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我介绍了一系列可以用Redis解决的问题和问题。Redis在解决我通常称之为*基于搜索的问题*的一类问题中特别有用。这类问题主要涉及使用`SET`和`ZSET`交集、并集和差集操作来找到符合指定标准的项目。
- en: In this chapter, I’ll introduce the concept of searching for content with Redis
    `SET`s. We’ll then talk about scoring and sorting our search results based on
    a few different options. After getting all of the basics out of the way, we’ll
    dig into creating an ad-targeting engine using Redis, based on what we know about
    search. Before finishing the chapter, we’ll talk about a method for matching or
    exceeding a set of requirements as a part of job searching.
  id: totrans-1112
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将介绍使用Redis `SET`s进行内容搜索的概念。然后，我们将讨论根据几个不同的选项对搜索结果进行评分和排序。在解决所有基础知识之后，我们将深入探讨如何使用Redis创建一个基于搜索知识的广告定位引擎。在结束本章之前，我们将讨论一种在求职过程中匹配或超过一组要求的方法。
- en: Overall, the set of problems in this chapter will show you how to search and
    filter data quickly and will expand your knowledge of techniques that you can
    use to organize and search your own information. First, let’s talk about what
    we mean by searching in Redis.
  id: totrans-1113
  prefs: []
  type: TYPE_NORMAL
  zh: 整个章节中的问题集将向您展示如何快速搜索和过滤数据，并扩展您对可用于组织和搜索您自己的信息的技术的了解。首先，让我们谈谈在 Redis 中我们所说的搜索是什么意思。
- en: 7.1\. Searching in Redis
  id: totrans-1114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 在 Redis 中搜索
- en: In a text editor or word processor, when you want to search for a word or phrase,
    the software will scan the document for that word or phrase. If you’ve ever used
    `grep` in Linux, Unix, or OS X, or if you’ve used Windows’ built-in file search
    capability to find files with words or phrases, you’ve noticed that as the number
    and size of documents to be searched increases, it takes longer to search your
    files.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本编辑器或文字处理软件中，当您想要搜索一个单词或短语时，软件将扫描文档以查找该单词或短语。如果您曾经使用过 Linux、Unix 或 OS X 中的
    `grep`，或者如果您使用过 Windows 内置的文件搜索功能来查找包含单词或短语的文件，您会注意到，随着要搜索的文档数量和大小增加，搜索文件所需的时间会更长。
- en: Unlike searching our local computer, searching for web pages or other content
    on the web is very fast, despite the significantly larger size and number of documents.
    In this section, we’ll examine how we can change the way we think about searching
    through data, and subsequently reduce search times over almost any kind of word
    or keyword-based content search with Redis.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 与在本地计算机上搜索不同，尽管网络上的网页或其他内容的大小和数量显著更大，但搜索网页或其他内容仍然非常快。在本节中，我们将探讨如何改变我们对数据搜索的看法，并随后通过
    Redis 几乎任何基于单词或关键词的内容搜索来减少搜索时间。
- en: As part of their effort to help their customers find help in solving their problems,
    Fake Garage Startup has created a knowledge base of troubleshooting articles for
    user support. As the number and variety of articles has increased over the last
    few months, the previous database-backed search has slowed substantially, and
    it’s time to come up with a method to search over these documents quickly. We’ve
    decided to use Redis, because it has all of the functionality necessary to build
    a content search engine.
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 作为帮助客户解决问题的一部分努力，假车库创业公司为用户支持创建了一个故障排除文章的知识库。随着过去几个月文章数量和种类的增加，之前的基于数据库的搜索速度大幅下降，现在是时候想出一个快速搜索这些文档的方法了。我们决定使用
    Redis，因为它拥有构建内容搜索引擎所需的所有功能。
- en: Our first step is to talk about how it’s even possible for us to search so much
    faster than scanning documents word by word.
  id: totrans-1118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是讨论我们如何能够比逐字扫描文档快得多地进行搜索。
- en: 7.1.1\. Basic search theory
  id: totrans-1119
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 基本搜索理论
- en: 'Searching for words in documents faster than scanning over them requires preprocessing
    the documents in advance. This preprocessing step is generally known as *indexing*,
    and the structures that we create are called *inverted indexes*. In the search
    world, inverted indexes are well known and are the underlying structure for almost
    every search engine that we’ve used on the internet. In a lot of ways, we can
    think of this process as producing something similar to the index at the end of
    this book. We create inverted indexes in Redis primarily because Redis has native
    structures that are ideally suited for inverted indexes: the `SET` and `ZSET`.^([[1](#ch07fn01)])'
  id: totrans-1120
  prefs: []
  type: TYPE_NORMAL
  zh: 要比扫描文档更快地搜索单词，需要在事先对文档进行预处理。这个预处理步骤通常被称为 *索引*，而我们创建的结构被称为 *倒排索引*。在搜索领域，倒排索引是众所周知的，并且是我们互联网上几乎每个搜索引擎的基础结构。在许多方面，我们可以将这个过程视为产生类似于本书末尾索引的东西。我们在
    Redis 中创建倒排索引主要是因为 Redis 具有适合倒排索引的本地结构：`SET` 和 `ZSET`。^([[1](#ch07fn01)])
- en: ¹ Though `SET`s and `ZSET`s could be emulated with a properly structured table
    and unique index in a relational database, emulating a `SET` or `ZSET` intersection,
    union, or difference using SQL for more than a few terms is cumbersome.
  id: totrans-1121
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 尽管在关系数据库中，可以通过结构良好的表和唯一索引来模拟 `SET` 和 `ZSET`，但使用 SQL 模拟 `SET` 或 `ZSET` 的交集、并集或差集对于超过几个术语来说很繁琐。
- en: More specifically, an inverted index of a collection of documents will take
    the words from each of the documents and create tables that say which documents
    contain what words. So if we have two documents, docA and docB, containing just
    the titles *lord of the rings* and *lord of the dance*, respectively, we’ll create
    a `SET` in Redis for *lord* that contains both docA and docB. This signifies that
    both docA and docB contain the word *lord*. The full inverted index for our two
    documents appears in [figure 7.1](#ch07fig01).
  id: totrans-1122
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，文档集合的倒排索引将取每个文档中的单词，并创建表格来说明哪些文档包含哪些单词。因此，如果我们有两个文档，docA 和 docB，分别包含标题`lord
    of the rings`和`lord of the dance`，我们将为`lord`创建一个包含 docA 和 docB 的`SET`。这表示 docA
    和 docB 都包含单词`lord`。我们两个文档的完整倒排索引显示在[图 7.1](#ch07fig01)中。
- en: Figure 7.1\. The inverted index for docA and docB
  id: totrans-1123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.1\. docA 和 docB 的倒排索引
- en: '![](07fig01_alt.jpg)'
  id: totrans-1124
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig01_alt.jpg)'
- en: Knowing that the ultimate result of our index operation is a collection of Redis
    `SET`s is helpful, but it’s also important to know how we get to those `SET`s.
  id: totrans-1125
  prefs: []
  type: TYPE_NORMAL
  zh: 知道我们索引操作的最终结果是Redis `SET`的集合是有帮助的，但了解我们如何到达这些`SET`也很重要。
- en: Basic indexing
  id: totrans-1126
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基本索引
- en: In order to construct our `SET`s of documents, we must first examine our documents
    for words. The process of extracting words from documents is known as *parsing*
    and *tokenization*; we’re producing a set of tokens (or words) that identify the
    document.
  id: totrans-1127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建我们的文档`SET`，我们必须首先检查我们的文档中的单词。从文档中提取单词的过程称为*解析*和*标记化*；我们正在生成一组标记（或单词），以标识文档。
- en: There are many different ways of producing tokens. The methods used for web
    pages could be different from methods used for rows in a relational database,
    or from documents from a document store. We’ll keep it simple and consider words
    of alphabetic characters and apostrophes (`'`) that are at least two characters
    long. This accounts for the majority of words in the English language, except
    for *I* and *a*, which we’ll ignore.
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 产生标记化的方法有很多种。用于网页的方法可能与用于关系数据库行的方法不同，也可能与文档存储中的文档不同。我们将保持简单，考虑至少有两个字符长的字母字符和撇号(`'`)的单词。这涵盖了英语语言中的大多数单词，除了*I*和*a*，我们将忽略它们。
- en: One common addition to a tokenization process is the removal of words known
    as *stop words*. Stop words are words that occur so frequently in documents that
    they don’t offer a substantial amount of information, and searching for those
    words returns too many documents to be useful. By removing stop words, not only
    can we improve the performance of our searches, but we can also reduce the size
    of our index. [Figure 7.2](#ch07fig02) shows the process of tokenization and stop
    word removal.
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 在标记化过程中常见的附加操作是移除称为*停用词*的单词。停用词是那些在文档中频繁出现，不提供大量信息的单词，搜索这些单词会返回太多文档，没有实际用途。通过移除停用词，我们不仅可以提高搜索性能，还可以减少索引的大小。[图
    7.2](#ch07fig02)显示了标记化和停用词移除的过程。
- en: Figure 7.2\. The process of tokenizing text into words, then removing stop words,
    as run on a paragraph from an early version of this section
  id: totrans-1130
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.2\. 将文本标记化为单词的过程，然后移除停用词，如图所示，是在本节早期版本的一个段落上运行的结果
- en: '![](07fig02.jpg)'
  id: totrans-1131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig02.jpg)'
- en: One challenge in this process is coming up with a useful list of stop words.
    Every group of documents will have different statistics on what words are most
    common, which may or may not affect stop words. As part of [listing 7.1](#ch07ex01),
    we include a list of stop words (fetched from [http://www.textfixer.com/resources/](http://www.textfixer.com/resources/)),
    as well as functions to both tokenize and index a document, taking into consideration
    the stop words that we want to remove.
  id: totrans-1132
  prefs: []
  type: TYPE_NORMAL
  zh: 在此过程中的一个挑战是制定一个有用的停用词列表。每个文档组都会有不同的统计数据，说明哪些单词最常见，这些统计数据可能或可能不会影响停用词。作为[列表 7.1](#ch07ex01)的一部分，我们包括了一个停用词列表（从[http://www.textfixer.com/resources/](http://www.textfixer.com/resources/)获取），以及用于标记化和索引文档的函数，同时考虑了我们想要移除的停用词。
- en: Listing 7.1\. Functions to tokenize and index a document
  id: totrans-1133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.1\. 用于标记化和索引文档的函数
- en: '![](157fig01_alt.jpg)'
  id: totrans-1134
  prefs: []
  type: TYPE_IMG
  zh: '![图片](157fig01_alt.jpg)'
- en: If we were to run our earlier docA and docB examples through the updated tokenization
    and indexing step in [listing 7.1](#ch07ex01), instead of having the five `SET`s
    corresponding to `lord`, `of`, `the`, `rings`, and `dance`, we’d only have `lord`,
    `rings`, and `dance`, because `of` and `the` are both stop words.
  id: totrans-1135
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将先前的 docA 和 docB 示例通过更新的标记化和索引步骤[列表 7.1](#ch07ex01)运行，而不是有五个对应于`lord`、`of`、`the`、`rings`和`dance`的`SET`，我们只会得到`lord`、`rings`和`dance`，因为`of`和`the`都是停用词。
- en: '|  |'
  id: totrans-1136
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Removing a document from the index
  id: totrans-1137
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从索引中删除文档
- en: If you’re in a situation where your document may have its content changed over
    time, you’ll want to add functionality to automatically remove the document from
    the index prior to reindexing the item, or a method to intelligently update only
    those inverted indexes that the document should be added or removed from. A simple
    way of doing this would be to use the `SET` command to update a key with a JSON-encoded
    list of words that the document had been indexed under, along with a bit of code
    to un-index as necessary at the start of `index_document()`.
  id: totrans-1138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你处于文档内容可能随时间变化的情况，你将想要添加功能，在重新索引项目之前自动从索引中删除文档，或者一个智能更新那些文档应该添加或删除的倒排索引的方法。一种简单的方法是使用`SET`命令更新一个键，该键包含文档被索引下的JSON编码单词列表，以及一些代码，在`index_document()`开始时根据需要取消索引。
- en: '|  |'
  id: totrans-1139
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we have a way of generating inverted indexes for our knowledge base
    documents, let’s look at how we can search our documents.
  id: totrans-1140
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了为我们的知识库文档生成倒排索引的方法，让我们看看我们如何可以搜索我们的文档。
- en: Basic searching
  id: totrans-1141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基本搜索
- en: 'Searching the index for a single word is easy: we fetch the set of documents
    in that word’s `SET`. But what if we wanted to find documents that contained two
    or more words? We could fetch the `SET`s of documents for all of those words,
    and then try to find those documents that are in all of the `SET`s, but as we
    discussed in [chapter 3](kindle_split_014.html#ch03), there are two commands in
    Redis that do this directly: `SINTER` and `SINTERSTORE`. Both commands will discover
    the items that are in all of the provided `SET`s and, for our example, will discover
    the `SET` of documents that contain all of the words.'
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 在索引中搜索单个单词很简单：我们从该单词的`SET`中获取文档集合。但如果我们想找到包含两个或更多单词的文档呢？我们可以获取所有这些单词的文档`SET`，然后尝试找到所有`SET`中都存在的文档，但正如我们在[第3章](kindle_split_014.html#ch03)中讨论的那样，Redis中有两个直接执行此操作的命令：`SINTER`和`SINTERSTORE`。这两个命令将发现所有提供的`SET`中的项目，并且在我们的例子中，将发现包含所有单词的文档`SET`。
- en: One of the amazing things about using inverted indexes with `SET` intersections
    is not so much what we can find (the documents we’re looking for), and it’s not
    even how quickly it can find the results—it’s how much information the search
    completely ignores. When searching text the way a text editor does, a lot of useless
    data gets examined. But with inverted indexes, we already know what documents
    have each individual word, and it’s only a matter of filtering through the documents
    that have *all* of the words we’re looking for.
  id: totrans-1143
  prefs: []
  type: TYPE_NORMAL
  zh: 使用倒排索引和`SET`交集的一个令人惊奇之处不在于我们可以找到什么（我们正在寻找的文档），甚至不是它如何快速找到结果——而是搜索完全忽略了多少信息。当像文本编辑器一样搜索文本时，会检查很多无用的数据。但使用倒排索引，我们已知哪些文档包含每个单独的单词，而问题仅仅在于过滤掉那些包含我们正在寻找的所有单词的文档。
- en: 'Sometimes we want to search for items with similar meanings and have them considered
    the same, which we call *synonyms* (at least in this context). To handle that
    situation, we could again fetch all of the document `SET`s for those words and
    find all of the unique documents, or we could use another built-in Redis operation:
    `SUNION` or `SUNIONSTORE`.'
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: 有时我们想要搜索具有相似含义的项目，并将它们视为相同，这我们称之为*同义词*（至少在这个上下文中）。为了处理这种情况，我们再次获取所有这些单词的文档`SET`，并找到所有唯一的文档，或者我们可以使用另一个内置的Redis操作：`SUNION`或`SUNIONSTORE`。
- en: 'Occasionally, there are times when we want to search for documents with certain
    words or phrases, but we want to remove documents that have other words. There
    are also Redis `SET` operations for that: `SDIFF` and `SDIFFSTORE`.'
  id: totrans-1145
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，我们想要搜索包含某些单词或短语的文档，但想要排除包含其他单词的文档。Redis也有针对这种情况的`SET`操作：`SDIFF`和`SDIFFSTORE`。
- en: With Redis `SET` operations and a bit of helper code, we can perform arbitrarily
    intricate word queries over our documents. [Listing 7.2](#ch07ex02) provides a
    group of helper functions that will perform `SET` intersections, unions, and differences
    over the given words, storing them in temporary `SET`s with an expiration time
    that defaults to 30 seconds.
  id: totrans-1146
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Redis的`SET`操作和一些辅助代码，我们可以对文档执行任意复杂的单词查询。[列表7.2](#ch07ex02)提供了一组辅助函数，这些函数将执行给定单词的`SET`交集、并集和差集操作，并将它们存储在具有默认30秒过期时间的临时`SET`中。
- en: Listing 7.2\. `SET` intersection, union, and difference operation helpers
  id: totrans-1147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2\. `SET`交集、并集和差集操作辅助函数
- en: '![](158fig01_alt.jpg)'
  id: totrans-1148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](158fig01_alt.jpg)'
- en: 'Each of the `intersect()`, `union()`, and `difference()` functions calls another
    helper function that actually does all of the heavy lifting. This is because they
    all essentially do the same thing: set up the keys, make the appropriate `SET`
    call, update the expiration, and return the new `SET`’s ID. Another way of visualizing
    what happens when we perform the three different `SET` operations `SINTER`, `SUNION`,
    and `SDIFF` can be seen in [figure 7.3](#ch07fig03), which shows the equivalent
    operations on Venn diagrams.'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 每个`intersect()`、`union()`和`difference()`函数都会调用另一个辅助函数，该函数实际上执行所有繁重的工作。这是因为它们本质上都做同样的事情：设置键，进行适当的`SET`调用，更新过期时间，并返回新的`SET`的ID。另一种可视化我们在执行三个不同的`SET`操作`SINTER`、`SUNION`和`SDIFF`时发生的事情的方法是查看[图7.3](#ch07fig03)，它显示了在维恩图上的等效操作。
- en: Figure 7.3\. The SET intersection, union, and difference calls as if they were
    operating on Venn diagrams
  id: totrans-1150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3\. SET交集、并集和差集调用，就像它们在维恩图中操作一样
- en: '![](07fig03_alt.jpg)'
  id: totrans-1151
  prefs: []
  type: TYPE_IMG
  zh: '![07fig03_alt.jpg](07fig03_alt.jpg)'
- en: This is everything necessary for programming the search engine, but what about
    parsing a search query?
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 这一切对于编程搜索引擎都是必要的，但解析搜索查询又是怎么回事呢？
- en: Parsing and executing a search
  id: totrans-1153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解析和执行搜索
- en: We almost have all of the pieces necessary to perform indexing and search. We
    have tokenization, indexing, and the basic functions for intersection, union,
    and differences. The remaining piece is to take a text query and turn it into
    a search operation. Like our earlier tokenization of documents, there are many
    ways to tokenize search queries. We’ll use a method that allows for searching
    for documents that contain all of the provided words, supporting both synonyms
    and unwanted words.
  id: totrans-1154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎拥有了执行索引和搜索所需的所有部件。我们有分词、索引以及交集、并集和差集的基本函数。剩下的部分是将文本查询转换为搜索操作。就像我们之前对文档的分词一样，有许多方法可以对搜索查询进行分词。我们将使用一种允许搜索包含所有提供单词的文档的方法，支持同义词和不想要的单词。
- en: A basic search will be about finding documents that contain all of the provided
    words. If we have just a list of words, that’s a simple `intersect()` call. In
    the case where we want to remove unwanted words, we’ll say that any word with
    a leading minus character (`-`) will be removed with `difference()`. To handle
    synonyms, we need a way of saying “This word is a synonym,” which we’ll denote
    by the use of the plus (`+`) character prefix on a word. If we see a plus character
    leading a word, it’s considered a synonym of the word that came just before it
    (skipping over any unwanted words), and we’ll group synonyms together to perform
    a `union()` prior to the higher-level `intersect()` call.
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 基本搜索将关于找到包含所有提供单词的文档。如果我们只有一个单词列表，那就是一个简单的`intersect()`调用。在我们想要移除不想要的单词的情况下，我们将说任何以减号字符（`-`）开头的单词都将通过`difference()`被移除。为了处理同义词，我们需要一种方式来说“这个词是同义词”，我们将通过在单词前使用加号（`+`）字符前缀来表示。如果我们看到一个加号字符引导一个单词，它将被认为是紧随其后的单词的同义词（跳过任何不想要的单词），并且我们将同义词分组在一起，在高级`intersect()`调用之前执行`union()`。
- en: Putting it all together where `+` denotes synonyms and `-` denotes unwanted
    words, the next listing shows the code for parsing a query into a Python list
    of lists that describes words that should be considered the same, and a list of
    words that are unwanted.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 将所有这些组合起来，其中`+`表示同义词，`-`表示不想要的词，下一个列表显示了将查询解析为Python列表的代码，该列表描述了应被视为相同的词，以及一个包含不想要的词的列表。
- en: Listing 7.3\. A function for parsing a search query
  id: totrans-1157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3\. 解析搜索查询的函数
- en: '![](160fig01_alt.jpg)'
  id: totrans-1158
  prefs: []
  type: TYPE_IMG
  zh: '![160fig01_alt.jpg](160fig01_alt.jpg)'
- en: 'To give this parsing function a bit of exercise, let’s say that we wanted to
    search our knowledge base for chat connection issues. What we really want to search
    for is an article with `connect`, `connection`, `disconnect`, or `disconnection`,
    along with `chat`, but because we aren’t using a proxy, we want to skip any documents
    that include `proxy` or `proxies`. Here’s an example interaction that shows the
    query (formatted into groups for easier reading):'
  id: totrans-1159
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让这个解析函数得到一些锻炼，让我们假设我们想要在我们的知识库中搜索聊天连接问题。我们真正想要搜索的是包含`connect`、`connection`、`disconnect`或`disconnection`以及`chat`的文章，但由于我们不使用代理，我们想要跳过包含`proxy`或`proxies`的任何文档。以下是一个示例交互，展示了查询（格式化为组以便于阅读）：
- en: '[PRE8]'
  id: totrans-1160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Our parse function properly extracted the synonyms for connect/disconnect, kept
    `chat` separate, and discovered our unwanted `proxy` and `proxies`. We aren’t
    going to be passing that parsed result around (except for maybe debugging as necessary),
    but instead are going to be calling our `parse()` function as part of a `parse_and_search()`
    function that `union()`s the individual synonym lists as necessary, `intersect()`ing
    the final list, and removing the unwanted words with `difference()` as necessary.
    The full `parse_and_search()` function is shown in the next listing.
  id: totrans-1161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解析函数正确提取了 connect/disconnect 的同义词，将 `chat` 单独列出，并发现了我们不想要的 `proxy` 和 `proxies`。我们不会将解析结果传递出去（除非需要调试），而是将
    `parse()` 函数作为 `parse_and_search()` 函数的一部分调用，该函数根据需要 `union()` 合并单个同义词列表，`intersect()`
    合并最终列表，并使用 `difference()` 必要时移除不想要的单词。完整的 `parse_and_search()` 函数在下一列表中展示。
- en: Listing 7.4\. A function to parse a query and search documents
  id: totrans-1162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4\. 解析查询和搜索文档的函数
- en: '![](161fig01_alt.jpg)'
  id: totrans-1163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](161fig01_alt.jpg)'
- en: Like before, the final result will be the ID of a `SET` that includes the documents
    that match the parameters of our search. Assuming that Fake Garage Startup has
    properly indexed all of their documents using our earlier `index_document()` function,
    `parse_and_search()` will perform the requested search.
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 和之前一样，最终结果将是包含匹配我们搜索参数的文档的 `SET` 的 ID。假设 Fake Garage Startup 已经正确使用我们之前的 `index_document()`
    函数索引了所有文档，`parse_and_search()` 将执行所需的搜索。
- en: We now have a method that’s able to search for documents with a given set of
    criteria. But ultimately, when there are a large number of documents, we want
    to see them in a specific order. To do that, we need to learn how to sort the
    results of our searches.
  id: totrans-1165
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个能够根据给定一组标准搜索文档的方法。但最终，当有大量文档时，我们希望按特定顺序查看它们。为了做到这一点，我们需要学习如何对搜索结果进行排序。
- en: 7.1.2\. Sorting search results
  id: totrans-1166
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 排序搜索结果
- en: We now have the ability to arbitrarily search for words in our indexed documents.
    But searching is only the first step in retrieving information that we’re looking
    for. After we have a list of documents, we need to decide what’s important enough
    about each of the documents to determine its position relative to other matching
    documents. This question is generally known as *relevance* in the search world,
    and one way of determining whether one article is more relevant than another is
    which article has been updated more recently. Let’s see how we could include this
    as part of our search results.
  id: totrans-1167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了在索引文档中任意搜索单词的能力。但搜索只是检索我们所需信息的第一步。在我们获得文档列表之后，我们需要决定每个文档中哪些信息足够重要，以确定其相对于其他匹配文档的位置。这个问题在搜索领域通常被称为
    *相关性*，确定一篇文章是否比另一篇文章更相关的一种方法是比较哪篇文章更新得更频繁。让我们看看我们如何将这一点包含在我们的搜索结果中。
- en: If you remember from [chapter 3](kindle_split_014.html#ch03), the Redis `SORT`
    call allows us to sort the contents of a `LIST` or `SET`, possibly referencing
    external data. For each article in Fake Garage Startup’s knowledge base, we’ll
    also include a `HASH` that stores information about the article. The information
    we’ll store about the article includes the title, the creation timestamp, the
    timestamp for when the article was last updated, and the document’s ID. An example
    document appears in [figure 7.4](#ch07fig04).
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还记得 [第 3 章](kindle_split_014.html#ch03)，Redis 的 `SORT` 调用允许我们对 `LIST` 或 `SET`
    的内容进行排序，可能还会引用外部数据。对于 Fake Garage Startup 知识库中的每篇文章，我们还会包含一个 `HASH`，用于存储有关文章的信息。我们将存储的文章信息包括标题、创建时间戳、文章最后更新时间戳以及文档的
    ID。一个示例文档出现在 [图 7.4](#ch07fig04) 中。
- en: Figure 7.4\. An example document stored in a `HASH`
  id: totrans-1169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.4\. 存储在 `HASH` 中的示例文档
- en: '![](07fig04.jpg)'
  id: totrans-1170
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig04.jpg)'
- en: With documents stored in this format, we can then use the `SORT` command to
    sort by one of a few different attributes. We’ve been giving our result `SET`s
    expiration times as a way of cleaning them out shortly after we’ve finished using
    them. But for our final `SORT`ed result, we could keep that result around longer,
    while at the same time allowing for the ability to re-sort, and even paginate
    over the results without having to perform the search again. Our function for
    integrating this kind of caching and re-sorting can be seen in the following listing.
  id: totrans-1171
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种格式存储文档后，我们可以使用 `SORT` 命令按几个不同的属性之一进行排序。我们一直将结果 `SET`s 的过期时间作为清理它们的一种方式，在我们完成使用后不久就清理掉。但对我们最终的
    `SORT` 排序结果，我们可以保留这个结果更长时间，同时允许重新排序，甚至在不重新执行搜索的情况下分页浏览结果。我们用于集成这种缓存和重新排序的函数可以在以下列表中看到。
- en: Listing 7.5\. A function to parse and search, sorting the results
  id: totrans-1172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.5\. 解析和搜索的函数，对结果进行排序
- en: '![](162fig01_alt.jpg)'
  id: totrans-1173
  prefs: []
  type: TYPE_IMG
  zh: '![图片](162fig01_alt.jpg)'
- en: When searching and sorting, we can paginate over results by updating the `start`
    and `num` arguments; alter the sorting attribute (and order) with the `sort` argument;
    cache the results for longer or shorter with the `ttl` argument; and reference
    previous search results (to save time) with the `id` argument.
  id: totrans-1174
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索和排序时，我们可以通过更新 `start` 和 `num` 参数来分页浏览结果；使用 `sort` 参数更改排序属性（和顺序）；使用 `ttl`
    参数缓存结果更长时间或更短时间；并使用 `id` 参数引用先前的搜索结果（以节省时间）。
- en: Though these functions won’t let us create a search engine to compete with Google,
    this problem and solution are what brought me to use Redis in the first place.
    Limitations on `SORT` lead to using `ZSET`s to support more intricate forms of
    document sorting, including combining scores for a composite sort order.
  id: totrans-1175
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这些函数不会让我们创建一个能与 Google 竞争的搜索引擎，但这个问题和解决方案最初让我开始使用 Redis。`SORT` 的限制导致使用 `ZSET`s
    来支持更复杂的文档排序形式，包括组合得分以实现复合排序顺序。
- en: 7.2\. Sorted indexes
  id: totrans-1176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 排序索引
- en: In the previous section, we talked primarily about searching, with the ability
    to sort results by referencing data stored in `HASH`es. This kind of sorting works
    well when we have a string or number that represents the actual sort order we’re
    interested in. But what if our sort order is a composite of a few different scores?
    In this section, we’ll talk about ways to combine multiple scores using `SET`s
    and `ZSET`s, which can offer greater flexibility than calling `SORT`.
  id: totrans-1177
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们主要讨论了搜索，以及通过引用存储在 `HASH` 中的数据来排序结果的能力。这种排序在我们有一个表示我们感兴趣的排序顺序的实际字符串或数字时效果很好。但如果我们想要排序的顺序是由几个不同的得分组合而成的呢？在本节中，我们将讨论使用
    `SET`s 和 `ZSET`s 结合多个得分的方法，这比调用 `SORT` 提供更大的灵活性。
- en: Stepping back for a moment, when we used `SORT` and fetched data to sort by
    from `HASH`es, the `HASH`es behaved much like rows in a relational database. If
    we were to instead pull all of the updated times for our articles into a `ZSET`,
    we could similarly order our articles by updated times by intersecting our earlier
    result `SET` with our update time `ZSET` with `ZINTERSTORE`, using an aggregate
    of `MAX`. This works because `SET`s can participate as part of a `ZSET` intersection
    or union as though every element has a score of 1.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾一下，当我们使用 `SORT` 并从 `HASH` 中获取数据以进行排序时，`HASH` 的行为类似于关系数据库中的行。如果我们将我们文章的所有更新时间拉入一个
    `ZSET`，我们可以通过使用 `ZINTERSTORE` 将我们的早期结果 `SET` 与更新时间 `ZSET` 进行交集来按更新时间对文章进行排序，使用
    `MAX` 进行聚合。这是因为 `SET` 可以作为 `ZSET` 交集或联合的一部分参与，就像每个元素都有一个得分为 1 一样。
- en: 7.2.1\. Sorting search results with ZSETs
  id: totrans-1179
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 使用 ZSET 对搜索结果进行排序
- en: As we saw in [chapter 1](kindle_split_011.html#ch01) and talked about in [chapter
    3](kindle_split_014.html#ch03), `SET`s can actually be provided as arguments to
    the `ZSET` commands `ZINTERSTORE` and `ZUNIONSTORE`. When we pass `SET`s to these
    commands, Redis will consider the `SET` members to have scores of 1\. For now,
    we aren’t going to worry about the scores of `SET`s in our operations, but we
    will later.
  id: totrans-1180
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在 [第 1 章](kindle_split_011.html#ch01) 中所见并在 [第 3 章](kindle_split_014.html#ch03)
    中所讨论的，`SET`s 实际上可以作为 `ZSET` 命令 `ZINTERSTORE` 和 `ZUNIONSTORE` 的参数提供。当我们向这些命令传递
    `SET`s 时，Redis 会将 `SET` 成员视为具有得分为 1。目前，我们不会担心我们操作中 `SET` 的得分，但稍后我们会关注这个问题。
- en: In this section, we’ll talk about using `SET`s and `ZSET`s together for a two-part
    search-and-sort operation. When you’ve finished reading this section, you’ll understand
    why and how we’d want to combine scores together as part of a document search.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何使用 `SET`s 和 `ZSET`s 一起进行两部分的搜索和排序操作。当你阅读完本节后，你将了解为什么以及如何将分数组合到文档搜索中作为一部分。
- en: Let’s consider a situation in which we’ve already performed a search and have
    our result `SET`. We can sort our results with the `SORT` command, but that means
    we can only sort based on a single value at a time. Being able to easily sort
    by a single value is one of the reasons why we started out sorting with our indexes
    in the first place.
  id: totrans-1182
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一种情况，即我们已经执行了搜索并得到了我们的结果 `SET`。我们可以使用 `SORT` 命令来排序我们的结果，但这意味着我们一次只能根据单个值进行排序。能够轻松地根据单个值进行排序是我们最初开始使用索引进行排序的原因之一。
- en: But say that we want to add the ability to vote on our knowledge base articles
    to indicate if they were useful. We could put the vote count in the article hash
    and use `SORT` as we did before. That’s reasonable. But what if we also wanted
    to sort based on a combination of recency *and* votes? We could do as we did in
    [chapter 1](kindle_split_011.html#ch01) and predefine the score increase for each
    vote. But if we don’t have enough information about how much scores should increase
    with each vote, then picking a score early on will force us to have to recalculate
    later when we find the right number.
  id: totrans-1183
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设我们想要添加对知识库文章进行投票的功能，以表明它们是否有用。我们可以将投票计数放入文章散列中，并像以前一样使用 `SORT`。这是合理的。但如果我们还想根据最新性和投票的组合进行排序呢？我们可以像在第
    1 章（kindle_split_011.html#ch01）中做的那样，预先定义每个投票的分数增加。但如果我们没有足够的信息来了解分数应该随着每次投票增加多少，那么在早期选择分数将迫使我们后来在找到正确的数字时重新计算。
- en: Instead, we’ll keep a `ZSET` of the times that articles were last updated, as
    well as a `ZSET` for the number of votes that an article has received. Both will
    use the article IDs of the knowledge base articles as members of the `ZSET`s,
    with update times or vote count as scores, respectively. We’ll also pass similar
    arguments to an updated `search_and_zsort()` function defined in the next listing,
    in order to calculate the resulting sort order for only update times, only vote
    counts, or almost any relative balance between the two.
  id: totrans-1184
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们将保留文章最后更新时间的 `ZSET` 以及文章收到的投票数的 `ZSET`。这两个都将使用知识库文章的文章 ID 作为 `ZSET` 的成员，分别以更新时间或投票计数作为分数。我们还将向下一个列表中定义的更新后的
    `search_and_zsort()` 函数传递类似的参数，以计算仅针对更新时间、仅针对投票计数或几乎任何两种之间的相对平衡的排序顺序。
- en: Listing 7.6\. An updated function to search and sort based on votes and updated
    times
  id: totrans-1185
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.6\. 基于投票和更新时间进行搜索和排序的更新函数
- en: '![](164fig01_alt.jpg)'
  id: totrans-1186
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](164fig01_alt.jpg)'
- en: Our `search_and_zsort()` works much like the earlier `search_and_sort()`, differing
    primarily in how we sort/order our results. Rather than calling `SORT`, we perform
    a `ZINTERSTORE` operation, balancing the search result `SET`, the updated time
    `ZSET`, and the vote `ZSET`.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `search_and_zsort()` 工作方式与早期的 `search_and_sort()` 类似，主要区别在于我们如何排序/排序我们的结果。我们不是调用
    `SORT`，而是执行 `ZINTERSTORE` 操作，平衡搜索结果 `SET`、更新时间 `ZSET` 和投票 `ZSET`。
- en: As part of `search_and_zsort()`, we used a helper function for handling the
    creation of a temporary ID, the `ZINTERSTORE` call, and setting the expiration
    time of the result `ZSET`. The `zintersect()` and `zunion()` helper functions
    are shown next.
  id: totrans-1188
  prefs: []
  type: TYPE_NORMAL
  zh: 作为 `search_and_zsort()` 的一部分，我们使用了一个辅助函数来处理临时 ID 的创建、`ZINTERSTORE` 调用和结果 `ZSET`
    的过期时间设置。`zintersect()` 和 `zunion()` 辅助函数将在下面展示。
- en: Listing 7.7\. Some helper functions for performing `ZSET` intersections and
    unions
  id: totrans-1189
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.7\. 执行 `ZSET` 交集和并集的一些辅助函数
- en: '![](165fig01_alt.jpg)'
  id: totrans-1190
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](165fig01_alt.jpg)'
- en: These helper functions are similar to our `SET`-based helpers, the primary difference
    being that we’re passing a dictionary through to specify scores, so we need to
    do more work to properly prefix all of our input keys.
  id: totrans-1191
  prefs: []
  type: TYPE_NORMAL
  zh: 这些辅助函数与我们的 `SET`-based 辅助函数类似，主要区别在于我们传递一个字典来指定分数，因此我们需要做更多的工作来正确地前缀所有输入键。
- en: '|  |'
  id: totrans-1192
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Article voting**'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：文章投票**'
- en: In this section, we used `ZSET`s to handle combining a time and a vote count
    for an article. You remember that we did much the same thing back in [chapter
    1](kindle_split_011.html#ch01) without search, though we did handle groups of
    articles. Can you update `article_vote()`, `post_articles()`, `get_articles()`,
    and `get_group_articles()` to use this new method so that we can update our score
    per vote whenever we want?
  id: totrans-1194
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用了`ZSET`来处理文章的时间和投票数的组合。你还记得在[第1章](kindle_split_011.html#ch01)中我们也做了类似的事情，尽管我们没有进行搜索，但我们确实处理了文章组。你能更新`article_vote()`、`post_articles()`、`get_articles()`和`get_group_articles()`，以便我们可以随时更新每个投票的分数吗？
- en: '|  |'
  id: totrans-1195
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In this section, we talked about how to combine `SET`s and `ZSET`s to calculate
    a simple composite score based on vote count and updated time. Though we used
    2 `ZSET`s as sources for scores, there’s no reason why we couldn’t have used 1
    or 100\. It’s all a question of what we want to calculate.
  id: totrans-1196
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论了如何将`SET`和`ZSET`结合起来，根据投票数和更新时间计算一个简单的复合分数。尽管我们使用了2个`ZSET`作为分数的来源，但我们没有理由不能使用1个或100个。这完全取决于我们想要计算什么。
- en: 'If we try to fully replace `SORT` and `HASH`es with the more flexible `ZSET`,
    we run into one problem almost immediately: scores in `ZSET`s must be floating-point
    numbers. But we can handle this issue in many cases by converting our non-numeric
    data to numbers.'
  id: totrans-1197
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图完全用更灵活的`ZSET`来替换`SORT`和`HASH`，我们几乎会立即遇到一个问题：`ZSET`中的分数必须是浮点数。但我们可以通过将我们的非数字数据转换为数字来处理这个问题。
- en: 7.2.2\. Non-numeric sorting with ZSETs
  id: totrans-1198
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2. 使用ZSET进行非数值排序
- en: Typical comparison operations between strings will examine two strings character
    by character until one character is different, one string runs out of characters,
    or until they’re found to be equal. In order to offer the same sort of functionality
    with string data, we need to turn strings into numbers. In this section, we’ll
    talk about methods of converting strings into numbers that can be used with Redis
    `ZSET`s in order to sort based on string prefixes. After reading this section,
    you should be able to sort your `ZSET` search results with strings.
  id: totrans-1199
  prefs: []
  type: TYPE_NORMAL
  zh: 字符串之间的典型比较操作将逐个字符地检查两个字符串，直到找到一个不同的字符，一个字符串没有字符了，或者它们被发现是相等的。为了提供与字符串数据相同的排序功能，我们需要将字符串转换为数字。在本节中，我们将讨论将字符串转换为数字的方法，这些方法可以与Redis的`ZSET`一起使用，以便根据字符串前缀进行排序。阅读本节后，你应该能够使用字符串对`ZSET`搜索结果进行排序。
- en: Our first step in translating strings into numbers is understanding the limitations
    of what we can do. Because Redis uses IEEE 754 double-precision floating-point
    values to store scores, we’re limited to at most 64 bits worth of storage. Due
    to some subtleties in the way doubles represent numbers, we can’t use all 64 bits.
    Technically, we could use more than the equivalent of 63 bits, but that doesn’t
    buy us significantly more than 63 bits, and for our case, we’ll only use 48 bits
    for the sake of simplicity. Using 48 bits limits us to prefixes of 6 bytes on
    our data, which is often sufficient.
  id: totrans-1200
  prefs: []
  type: TYPE_NORMAL
  zh: 在将字符串转换为数字的第一步中，我们需要了解我们能做什么的限制。因为Redis使用IEEE 754双精度浮点值来存储分数，所以我们最多只能存储64位。由于双精度表示数字的方式中存在一些细微差别，我们无法使用所有64位。技术上，我们可以使用超过63位的值，但这并不会给我们带来比63位显著更多的空间，并且在我们的情况下，我们将为了简单起见只使用48位。使用48位将我们的数据限制在6个字节的前缀，这通常足够了。
- en: To convert our string into an integer, we’ll trim our string down to six characters
    (as necessary), converting each character into its ASCII value. We’ll then extend
    the values to six entries for strings shorter than six characters. Finally, we’ll
    combine all of the values into an integer. Our code for converting a string into
    a score can be seen next.
  id: totrans-1201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将我们的字符串转换为整数，我们将字符串修剪到六个字符（如果需要的话），将每个字符转换为它的ASCII值。然后我们将值扩展到六个条目，对于少于六个字符的字符串。最后，我们将所有值组合成一个整数。我们将字符串转换为分数的代码将在下面看到。
- en: Listing 7.8\. A function to turn a string into a numeric score
  id: totrans-1202
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8。一个将字符串转换为数值分数的函数
- en: '![](166fig01_alt.jpg)'
  id: totrans-1203
  prefs: []
  type: TYPE_IMG
  zh: '![](166fig01_alt.jpg)'
- en: Most of our `string_to_score()` function should be straightforward, except for
    maybe our use of -1 as a filler value for strings shorter than six characters,
    and our use of 257 as a multiplier before adding each character value to the score.
    For many applications, being able to differentiate between `hello\\0` and `hello`
    can be important, so we take steps to differentiate the two, primarily by adding
    1 to all ASCII values (making null 1), and using 0 (-1 + 1) as a filler value
    for short strings. As a bonus, we use an extra bit to tell us whether a string
    is more than six characters long, which helps us with similar six-character prefixes.^([[2](#ch07fn02)])
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大多数 `string_to_score()` 函数应该是直截了当的，除了可能使用 -1 作为小于六个字符的字符串的填充值，以及我们在将每个字符值添加到分数之前使用
    257 作为乘数。对于许多应用来说，能够区分 `hello\\0` 和 `hello` 可能很重要，因此我们采取措施来区分这两个字符串，主要是通过将所有 ASCII
    值加 1（使空字符为 1），并使用 0（-1 + 1）作为短字符串的填充值。作为额外的好处，我们使用一个额外的位来告诉我们字符串是否超过六个字符长，这有助于我们处理类似的六字符前缀。[^([2](#ch07fn02))]
- en: ² If we didn’t care about differentiating between `hello\\0` and `hello`, then
    we wouldn’t need the filler. If we didn’t need the filler, we could replace our
    multiplier of 257 with 256 and get rid of the +1 adjustment. But with the filler,
    we actually use .0337 additional bits to let us differentiate short strings from
    strings that have nulls. And when combined with the extra bit we used to distinguish
    whether we have strings longer than six characters, we actually use 49.0337 total
    bits.
  id: totrans-1205
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 如果我们不关心区分 `hello\\0` 和 `hello`，那么我们就不需要填充值。如果我们不需要填充值，我们可以将我们的乘数 257 替换为 256，并去掉
    +1 调整。但是，有了填充值，我们实际上使用了额外的 .0337 比特来让我们区分短字符串和包含空字符的字符串。并且当与用于区分我们是否有超过六个字符的字符串的额外比特结合时，我们实际上使用了
    49.0337 个总比特。
- en: By mapping strings to scores, we’re able to get a prefix comparison of a little
    more than the first six characters of our string. For non-numeric data, this is
    more or less what we can reasonably do without performing extensive numeric gymnastics,
    and without running into issues with how a non-Python library transfers large
    integers (that may or may not have been converted to a double).
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将字符串映射到分数，我们能够得到比我们字符串前六个字符更长的前缀比较。对于非数值数据，这基本上是我们可以在不进行大量数值运算的情况下合理做到的，而且不会遇到非
    Python 库传输大整数（可能或可能未转换为双精度浮点数）的问题。
- en: When using scores derived from strings, the scores aren’t always useful for
    combining with other scores and are typically only useful for defining a single
    sort order. Note that this is because the score that we produced from the string
    doesn’t really mean anything, aside from defining a sort order.
  id: totrans-1207
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用从字符串派生出的分数时，这些分数并不总是适用于与其他分数结合，通常仅适用于定义单个排序顺序。请注意，这是因为我们从字符串中产生的分数实际上并不代表任何意义，除了定义一个排序顺序。
- en: '|  |'
  id: totrans-1208
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Autocompleting with strings as scores**'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：使用字符串作为分数自动完成**'
- en: Back in [section 6.1.2](kindle_split_017.html#ch06lev2sec2), we used `ZSET`s
    with scores set to 0 to allow us to perform prefix matching on user names. We
    had to add items to the `ZSET` and either use `WATCH`/`MULTI`/`EXEC` or the lock
    that we covered in [section 6.2](kindle_split_017.html#ch06lev1sec2) to make sure
    that we fetched the correct range. But if instead we added names with scores being
    the result of `string_to_score()` on the name itself, we could bypass the use
    of `WATCH`/ `MULTI`/`EXEC` and locks when someone is looking for a prefix of at
    most six characters by using `ZRANGEBYSCORE`, with the endpoints we had calculated
    before being converted into scores as just demonstrated. Try rewriting our `find_prefix_range()`
    and `autocomplete_on_prefix()` functions to use `ZRANGEBYSCORE` instead.
  id: totrans-1210
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6.1.2 节](kindle_split_017.html#ch06lev2sec2) 中，我们使用了分数设置为 0 的 `ZSET`，以便我们可以在用户名上执行前缀匹配。我们必须向
    `ZSET` 中添加项目，并使用 `WATCH`/`MULTI`/`EXEC` 或我们在 [第 6.2 节](kindle_split_017.html#ch06lev1sec2)
    中提到的锁来确保我们获取正确的范围。但如果我们向名称本身使用 `string_to_score()` 得到的分数添加名称，我们就可以通过使用 `ZRANGEBYSCORE`
    来绕过在某人寻找最多六个字符的前缀时使用 `WATCH`/`MULTI`/`EXEC` 和锁。我们之前计算出的端点被转换成分数，正如刚刚演示的那样。尝试重写我们的
    `find_prefix_range()` 和 `autocomplete_on_prefix()` 函数，以使用 `ZRANGEBYSCORE` 代替。
- en: '|  |'
  id: totrans-1211
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-1212
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Autocompleting with longer strings**'
  id: totrans-1213
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：使用较长的字符串自动完成**'
- en: In this section and for the previous exercise, we converted arbitrary binary
    strings to scores, which limited us to six-character prefixes. By reducing the
    number of valid characters in our input strings, we don’t need to use a full 8+
    bits per input character. Try to come up with a method that would allow us to
    use more than a six-character prefix if we only needed to autocomplete on lowercase
    alphabetic characters.
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节和之前的练习中，我们将任意二进制字符串转换为分数，这限制了我们只能使用六字符前缀。通过减少输入字符串中的有效字符数量，我们不需要为每个输入字符使用完整的8+位。尝试想出一个方法，如果我们只需要在小型字母字符上进行自动完成，我们可以使用超过六字符的前缀。
- en: '|  |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we can sort on arbitrary data, and you’ve seen how to use weights to
    adjust and combine numeric data, you’re ready to read about how we can use Redis
    `SET`s and `ZSET`s to target ads.
  id: totrans-1216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在任意数据上排序，你已经看到了如何使用权重来调整和组合数值数据，你现在可以阅读关于我们如何使用Redis `SET`s和`ZSET`s进行广告定位的内容。
- en: 7.3\. Ad targeting
  id: totrans-1217
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 广告定位
- en: On countless websites around the internet, we see advertisements in the form
    of text snippets, images, and videos. Those ads exist as a method of paying website
    owners for their service—whether it’s search results, information about travel
    destinations, or even finding the definition of a word.
  id: totrans-1218
  prefs: []
  type: TYPE_NORMAL
  zh: 在互联网上无数的网站上，我们看到以文本片段、图片和视频形式存在的广告。这些广告作为支付网站所有者服务的一种方式——无论是搜索结果、关于旅游目的地的信息，甚至是查找单词的定义。
- en: In this section, we’ll talk about using `SET`s and `ZSET`s to implement an ad-targeting
    engine. When you finish reading this section, you’ll have at least a basic understanding
    of how to build an ad-serving platform using Redis. Though there are a variety
    of ways to build an ad-targeting engine without Redis (custom solutions written
    with C++, Java, or C# are common), building an ad-targeting engine with Redis
    is one of the quickest methods to get an ad network running.
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论使用`SET`s和`ZSET`s来实现广告定位引擎。当你阅读完本节后，你至少会基本了解如何使用Redis构建广告投放平台。尽管有各种方法可以在没有Redis的情况下构建广告定位引擎（使用C++、Java或C#编写的自定义解决方案很常见），但使用Redis构建广告定位引擎是快速启动广告网络的一种方法。
- en: If you’ve been reading these chapters sequentially, you’ve seen a variety of
    problems and solutions, almost all of which were simplified versions of much larger
    projects and problems. But in this section, we won’t be simplifying anything.
    We’ll build an almost-complete ad-serving platform based on software that I built
    and ran in a production setting for a number of months. The only major missing
    parts are the web server, ads, and traffic.
  id: totrans-1220
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经按顺序阅读了这些章节，你已经看到了各种问题和解决方案，几乎所有这些问题和解决方案都是更大项目和大问题的简化版本。但在这个部分，我们不会简化任何东西。我们将基于我在生产环境中构建和运行了几个月的软件，构建一个几乎完整的广告投放平台。唯一缺少的主要部分是网络服务器、广告和流量。
- en: Before we get into building the ad server itself, let’s first talk about what
    an ad server is and does.
  id: totrans-1221
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建广告服务器本身之前，让我们首先谈谈广告服务器是什么以及它做什么。
- en: 7.3.1\. What’s an ad server?
  id: totrans-1222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1. 什么是广告服务器？
- en: When we talk about an ad server, what we really mean is a sometimes-small, but
    sophisticated piece of technology. Whenever we visit a web page with an ad, either
    the web server itself or our web browser makes a request to a remote server for
    that ad. This ad server will be provided a variety of information about how to
    find an ad that can earn the most money through clicks, views, or actions (I’ll
    explain these shortly).
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们谈论广告服务器时，我们真正指的是一种有时规模不大但复杂的科技。每次我们访问带有广告的网页时，无论是网络服务器本身还是我们的网络浏览器，都会向远程服务器请求该广告。这个广告服务器将提供各种信息，关于如何找到可以赚取最多点击、浏览或行动（我稍后会解释这些）的广告。
- en: In order to choose a specific ad, our server must be provided with targeting
    parameters. Servers will typically receive at least basic information about the
    viewer’s location (based on our IP address at minimum, and occasionally based
    on GPS information from our phone or computer), what operating system and web
    browser we’re using, maybe the content of the page we’re on, and maybe the last
    few pages we’ve visited on the current website.
  id: totrans-1224
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择特定的广告，我们的服务器必须提供定位参数。服务器通常会接收至少关于观众位置的基本信息（至少基于我们的IP地址，有时基于我们的手机或电脑的GPS信息），我们正在使用的操作系统和网页浏览器，可能是我们所在页面的内容，以及我们可能在当前网站上访问的最后几个页面。
- en: We’ll focus on building an ad-targeting platform that has a small amount of
    basic information about viewer location and the content of the page visited. After
    we’ve seen how to pick an ad from this information, we can add other targeting
    parameters later.
  id: totrans-1225
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将专注于构建一个广告定位平台，该平台只有少量关于观众位置和访问页面内容的基本信息。在我们看到如何从这些信息中选择广告之后，我们可以在以后添加其他定位参数。
- en: '|  |'
  id: totrans-1226
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Ads with budgets
  id: totrans-1227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 有预算的广告
- en: In a typical ad-targeting platform, each ad is provided with a budget to be
    spent over time. We don’t address budgeting or accounting here, so both need to
    be built. Generally, budgets should at least attempt to be spread out over time,
    and as a practical approach, I’ve found that adding a portion of the ad’s total
    budget on an hourly basis (with different ads getting budgeted at different times
    through the hour) works well.
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的广告定位平台中，每个广告都会分配一个预算，用于在一定时间内花费。我们在此不涉及预算或会计问题，因此两者都需要构建。通常，预算至少应尝试在时间上分散，作为一个实际的方法，我发现每小时添加广告总预算的一部分（不同广告在不同时间通过小时进行预算）效果很好。
- en: '|  |'
  id: totrans-1229
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Our first step in returning ads to the user is getting the ads into our platform
    in the first place.
  id: totrans-1230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将广告返回给用户的第一步是首先将广告放入我们的平台中。
- en: 7.3.2\. Indexing ads
  id: totrans-1231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2\. 索引广告
- en: The process of indexing an ad is not so different from the process of indexing
    any other content. The primary difference is that we aren’t looking to return
    a list of ads (or search results); we want to return a single ad. There are also
    some secondary differences in that ads will typically have required targeting
    parameters such as location, age, or gender.
  id: totrans-1232
  prefs: []
  type: TYPE_NORMAL
  zh: 广告索引的过程与其他内容的索引过程没有太大区别。主要区别在于我们不是要返回一系列广告（或搜索结果）；我们只想返回单个广告。还有一些次要的区别，即广告通常需要一些定位参数，如位置、年龄或性别。
- en: As mentioned before, we’ll only be targeting based on location and content,
    so this section will discuss how to index ads based on location and content. When
    you’ve seen how to index and target based on location and content, targeting based
    on, for example, age, gender, or recent behavior should be similar (at least on
    the indexing and targeting side of things).
  id: totrans-1233
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们只会根据位置和内容进行定位，因此本节将讨论如何根据位置和内容索引广告。当你了解了如何根据位置和内容进行索引和定位后，根据年龄、性别或最近的行为进行定位应该类似（至少在索引和定位方面）。
- en: Before we can talk about indexing an ad, we must first determine how to measure
    the value of an ad in a consistent manner.
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论如何索引广告之前，我们必须首先确定如何以一致的方式衡量广告的价值。
- en: Calculating the value of an ad
  id: totrans-1235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算广告的价值
- en: 'Three major types of ads are shown on web pages: *cost per view*, *cost per
    click*, and *cost per action* (or acquisition). Cost per view ads are also known
    as *CPM* or *cost per mille*, and are paid a fixed rate per 1,000 views of the
    ad itself. Cost per click, or *CPC*, ads are paid a fixed rate per click on the
    ad itself. Cost per action, or *CPA*, ads are paid a sometimes varying rate based
    on actions performed on the ad-destination site.'
  id: totrans-1236
  prefs: []
  type: TYPE_NORMAL
  zh: 网页上显示的三大广告类型是：*按观看付费*、*按点击付费*和*按行动付费*（或获取）。按观看付费广告也称为*CPM*或*每千次展示成本*，按广告本身每1,000次观看支付固定费用。按点击付费，或称*CPC*，广告按点击广告本身支付固定费用。按行动付费，或称*CPA*，广告根据在广告目标网站上执行的操作支付有时变化的费用。
- en: Making values consistent
  id: totrans-1237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使价值一致
- en: To greatly simplify our calculations as to the value of showing a given ad,
    we’ll convert all of our types of ads to have values relative to 1,000 views,
    generating what’s known as an *estimated CPM*, or *eCPM*. CPM ads are the easiest
    because their value per thousand views is already provided, so eCPM = CPM. But
    for both CPC and CPA ads, we must calculate the eCPMs.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了极大地简化我们对展示特定广告价值的计算，我们将所有类型的广告转换为相对于1,000次观看的价值，生成所谓的*估计CPM*，或*预估CPM*。CPM广告是最简单的，因为它们每千次观看的价值已经提供，所以eCPM
    = CPM。但对于CPC和CPA广告，我们必须计算eCPM。
- en: Calculating the estimated CPM of a CPC ad
  id: totrans-1239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算CPC广告的估计CPM
- en: 'If we have a CPC ad, we start with its cost per click, say $.25\. We then multiply
    that cost by the click-through rate (CTR) on the ad. Click-through rate is the
    number of clicks that an ad received divided by the number of views the ad received.
    We then multiply that result by 1,000 to get our estimated CPM for that ad. If
    our ad gets .2% CTR, or .002, then our calculation looks something like this:
    .25 × .002 × 1000 = $.50 eCPM.'
  id: totrans-1240
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有一个CPC广告，我们从其每次点击成本开始，比如说0.25美元。然后我们乘以广告的点击率（CTR）。点击率是广告收到的点击次数除以广告收到的展示次数。然后我们乘以这个结果，再乘以1,000来得到该广告的预估CPM。如果我们的广告有0.2%的CTR，即0.002，那么我们的计算可能如下：0.25
    × 0.002 × 1000 = $0.50 eCPM。
- en: Calculating the estimated CPM of a CPA ad
  id: totrans-1241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算CPA广告的预估CPM
- en: 'When we have a CPA ad, the calculation is somewhat similar to the CPC value
    calculation. We start with the CTR of the ad, say .2%. We multiply that against
    the probability that the user will perform an action on the advertiser’s destination
    page, maybe 10% or .1\. We then multiply that times the value of the action performed,
    and again multiply that by 1,000 to get our estimated CPM. If our CPA is $3, our
    calculation would look like this: .002 × .1 × 3 × 1000 = $.60 eCPM.'
  id: totrans-1242
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个CPA广告时，计算方法与CPC值计算有些相似。我们从广告的点击率（CTR）开始，比如说0.2%。然后我们乘以用户在广告商目标页面上执行动作的概率，可能是10%或0.1。然后我们再乘以执行动作的价值，再次乘以1,000来得到我们的预估CPM。如果我们CPA是3美元，我们的计算如下：0.002
    × 0.1 × 3 × 1000 = $0.60 eCPM。
- en: Two helper functions for calculating the eCPM of CPC and CPA ads are shown next.
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例展示了计算CPC和CPA广告eCPM的两个辅助函数。
- en: Listing 7.9\. Helper functions for turning information about CPC and CPA ads
    into eCPM
  id: totrans-1244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9\. 将CPC和CPA广告信息转换为eCPM的辅助函数
- en: '![](169fig01_alt.jpg)'
  id: totrans-1245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](169fig01_alt.jpg)'
- en: Notice that in our helper functions we used clicks, views, and actions directly
    instead of the calculated CTR. This lets us keep these values directly in our
    accounting system, only calculating the eCPM as necessary. Also notice that for
    our uses, CPC and CPA are similar, the major difference being that for most ads,
    the number of actions is significantly lower than the number of clicks, but the
    value per action is typically much larger than the value per click.
  id: totrans-1246
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在我们的辅助函数中，我们直接使用了点击次数、展示次数和动作，而不是计算的CTR。这使得我们可以将这些值直接保留在我们的会计系统中，只在需要时计算eCPM。另外，请注意，对于我们的用途，CPC和CPA是相似的，主要区别在于对于大多数广告，动作的数量通常远低于点击次数，但每个动作的价值通常远大于每次点击的价值。
- en: Now that we’ve calculated the basic value of an ad, let’s index an ad in preparation
    for targeting.
  id: totrans-1247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经计算了广告的基本价值，让我们索引一个广告，为定位做准备。
- en: Inserting an ad into the index
  id: totrans-1248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将广告插入索引
- en: When targeting an ad, we’ll have a group of optional and required targeting
    parameters. In order to properly target an ad, our indexing of the ad must reflect
    the targeting requirements. Since we have two targeting options—location and content—we’ll
    say that location is required (either on the city, state, or country level), but
    any matching terms between the ad and the content of the page will be optional
    and a bonus.^([[3](#ch07fn03)])
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 当定位广告时，我们将有一组可选和必选的定位参数。为了正确定位广告，我们的广告索引必须反映定位要求。由于我们有两种定位选项——位置和内容——我们将说位置是必需的（在市、州或国家层面），但广告与页面内容之间的任何匹配项将是可选的，并且是额外的奖励。[3](#ch07fn03)
- en: ³ If ad copy matches page content, then the ad looks like the page and will
    be more likely to be clicked on than an ad that doesn’t look like the page content.
  id: totrans-1250
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³ 如果广告文案与页面内容匹配，那么广告看起来就像页面一样，并且比看起来不像页面内容的广告更有可能被点击。
- en: We’ll use the same search functions we defined in [sections 7.1](#ch07lev1sec1)
    and [7.2](#ch07lev1sec2), with slightly different indexing options. We’ll also
    assume that you’ve taken my advice from [chapter 4](kindle_split_015.html#ch04)
    by splitting up your different types of services to different machines (or databases)
    as necessary, so that your ad-targeting index doesn’t overlap with your other
    content indexes.
  id: totrans-1251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用我们在[第7.1节](#ch07lev1sec1)和[7.2节](#ch07lev1sec2)中定义的相同搜索函数，但索引选项略有不同。我们还将假设你已经按照[第4章](kindle_split_015.html#ch04)中的建议，根据需要将不同类型的服务分配到不同的机器（或数据库）上，这样你的广告定位索引就不会与其他内容索引重叠。
- en: As in [section 7.1](#ch07lev1sec1), we’ll create inverted indexes that use `SET`s
    and `ZSET`s to hold ad IDs. Our `SET`s will hold the required location targeting,
    which provides no additional bonus. When we talk about learning from user behavior,
    we’ll get into how we calculate our per-matched-word bonus, but initially we won’t
    include any of our terms for targeting bonuses, because we don’t know how much
    they may contribute to the overall value of the ad. Our ad-indexing function is
    shown here.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第7.1节](#ch07lev1sec1)中所述，我们将创建使用`SET`和`ZSET`来存储广告ID的倒排索引。我们的`SET`将包含所需的位置定位，这不会提供额外的奖金。当我们谈论从用户行为中学习时，我们将深入了解我们如何计算每个匹配词的奖金，但最初我们不会包括任何我们的定位奖金术语，因为我们不知道它们可能对广告的整体价值贡献多少。我们的广告索引函数如下所示。
- en: Listing 7.10\. A method for indexing an ad that’s targeted on location and ad
    content
  id: totrans-1253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.10\. 在位置和广告内容上定位广告的索引方法
- en: '![](170fig01_alt.jpg)'
  id: totrans-1254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](170fig01_alt.jpg)'
- en: As shown in the listing and described in the annotations, we made three important
    additions to the listing. The first is that an ad can actually have multiple targeted
    locations. This is necessary to allow a single ad to be targeted for any one of
    multiple locations at the same time (like multiple cities, states, or countries).
  id: totrans-1255
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表所示并在注释中描述的，我们对列表做了三个重要的补充。第一个是广告实际上可以有多个目标位置。这是为了允许一个广告同时针对多个位置（如多个城市、州或国家）中的任何一个进行定位。
- en: The second is that we’ll keep a dictionary that holds information about the
    average number of clicks and actions across the entire system. This lets us come
    up with a reasonable estimate on the eCPM for CPC and CPA ads before they’ve even
    been seen in the system.^([[4](#ch07fn04)])
  id: totrans-1256
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个是我们将保留一个字典，其中包含整个系统中平均点击和动作的数量信息。这使我们能够在广告甚至在系统中被看到之前，对CPC和CPA广告的eCPM做出合理的估计。[^4](#ch07fn04)
- en: ⁴ It may seem strange to be estimating an expectation (which is arguably an
    estimate), but everything about targeting ads is fundamentally predicated on statistics
    of one kind or another. This is one of those cases where the basics can get us
    pretty far.
  id: totrans-1257
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴ 估计期望值（这本身可能就是一个估计）可能看起来有些奇怪，但关于定位广告的一切本质上都是基于某种统计学的。这就是那种基本原理可以让我们走得很远的情况之一。
- en: Finally, we’ll also keep a `SET` of all of the terms that we can optionally
    target in the ad. I include this information as a precursor to learning about
    user behavior a little later.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们还将保留一个`SET`，其中包含我们可以在广告中可选定位的所有术语。我将这些信息作为稍后了解用户行为的先导。
- en: It’s now time to search for and discover ads that match an ad request.
  id: totrans-1259
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候搜索和发现与广告请求匹配的广告了。
- en: 7.3.3\. Targeting ads
  id: totrans-1260
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.3\. 定位广告
- en: As described earlier, when we receive a request to target an ad, we’re generally
    looking to find the highest eCPM ad that matches the viewer’s location. In addition
    to matching an ad based on a location, we’ll gather and use statistics about how
    the content of a page matches the content of an ad, and what that can do to affect
    the ad’s CTR. Using these statistics, content in an ad that matches a web page
    can result in bonuses that contribute to the calculated eCPM of CPC and CPA ads,
    which can result in those types of ads being shown more.
  id: totrans-1261
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，当我们收到定位广告的请求时，我们通常是在寻找与观看者位置匹配的最高eCPM广告。除了根据位置匹配广告外，我们还将收集并使用有关页面内容与广告内容匹配的统计数据，以及这些统计数据如何影响广告的CTR。使用这些统计数据，与网页内容匹配的广告中的内容可以产生额外的奖金，从而有助于计算CPC和CPA广告的eCPM，这可能导致这些类型的广告被展示得更多。
- en: Before showing any ads, we won’t have any bonus scoring for any of our web page
    content. But as we show ads, we’ll learn about what terms in the ads help or hurt
    the ad’s expected performance, which will allow us to change the relative value
    of each of the optional targeting terms.
  id: totrans-1262
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示任何广告之前，我们不会对我们的网页内容有任何额外的评分。但随着我们展示广告，我们将了解哪些广告中的术语有助于或损害广告的预期性能，这将使我们能够改变每个可选定位术语的相对价值。
- en: To execute the targeting, we’ll union all of the relevant location `SET`s to
    produce an initial group of ads that should be shown to the viewer. Then we’ll
    parse the content of the page on which the ad will be shown, add any relevant
    bonuses, and finally calculate a total eCPM for all ads that a viewer could be
    shown. After we’ve calculated those eCPMs, we’ll fetch the ad ID for the highest
    eCPM ad, record some statistics about our targeting, and return the ad itself.
    Our code for targeting an ad looks like this.
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行定位，我们将所有相关的位置`SET`进行联合，以生成一组初始广告，这些广告应该展示给观众。然后我们将解析将要展示广告的页面内容，添加任何相关的奖金，并最终计算观众可能看到的所有广告的总eCPM。在我们计算出这些eCPM之后，我们将获取最高eCPM广告的广告ID，记录一些关于我们定位的统计数据，并返回广告本身。我们的定位广告代码如下所示。
- en: Listing 7.11\. Ad targeting by location and page content bonuses
  id: totrans-1264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.11\. 基于位置和页面内容奖金的广告定位
- en: '![](171fig01_alt.jpg)'
  id: totrans-1265
  prefs: []
  type: TYPE_IMG
  zh: '![图片](171fig01_alt.jpg)'
- en: In this first version, we hide away the details of exactly how we’re matching
    based on location and adding bonuses based on terms so that we can understand
    the general flow. The only part I didn’t mention earlier is that we’ll also generate
    a *target ID*, which is an ID that represents this particular execution of the
    ad targeting. This ID will allow us to track clicks later, helping us learn about
    which parts of the ad targeting may have contributed to the overall total clicks.
  id: totrans-1266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个第一个版本中，我们隐藏了基于位置匹配和基于术语添加奖金的详细细节，以便我们能够理解整体流程。我之前没有提到的是，我们还将生成一个*目标ID*，这是一个代表广告定位特定执行的ID。这个ID将允许我们跟踪后续的点击，帮助我们了解哪些广告定位部分可能对总点击量做出了贡献。
- en: As mentioned earlier, in order to match based on location, we’ll perform a `SET`
    union over the locations (city, state, country) that the viewer is coming from.
    While we’re here, we’ll also calculate the base eCPM of these ads without any
    bonuses applied. The code for performing this operation is shown next.
  id: totrans-1267
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，为了根据位置进行匹配，我们将对观众来源的位置（城市、州、国家）执行`SET`联合操作。在此期间，我们还将计算这些广告的基础eCPM，而不应用任何奖金。执行此操作的代码如下所示。
- en: Listing 7.12\. A helper function for targeting ads based on location
  id: totrans-1268
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.12\. 基于位置定位广告的辅助函数
- en: '![](172fig01_alt.jpg)'
  id: totrans-1269
  prefs: []
  type: TYPE_IMG
  zh: '![图片](172fig01_alt.jpg)'
- en: 'This code listing does exactly what I said it would do: it finds ads that match
    the location of the viewer, and it calculates the eCPM of all of those ads without
    any bonuses based on page content applied. The only thing that may be surprising
    about this listing is our passing of the funny `_execute` keyword argument to
    the `zintersect()` function, which delays the actual execution of calculating
    the eCPM of the ad until later. The purpose of waiting until later is to help
    minimize the number of round trips between our client and Redis.'
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码列表确实做了我所说的：它找到与观众位置匹配的广告，并基于页面内容计算所有这些广告的eCPM，而不应用任何基于内容的奖金。这个列表可能令人惊讶的唯一一点是我们将一个有趣的自定义`_execute`关键字参数传递给`zintersect()`函数，这延迟了计算广告eCPM的实际操作直到稍后。等待直到稍后的目的是帮助最小化客户端和Redis之间的往返次数。
- en: Calculating targeting bonuses
  id: totrans-1271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算定位奖金
- en: The interesting part of targeting isn’t the location matching; it’s calculating
    the bonuses. We’re trying to discover the amount to add to an ad’s eCPM, based
    on words in the page content that matched words in the ad. We’ll assume that we’ve
    precalculated a bonus for each word in each ad (as part of the learning phase),
    stored in `ZSET`s for each word, with members being ad IDs and scores being what
    should be added to the eCPM.
  id: totrans-1272
  prefs: []
  type: TYPE_NORMAL
  zh: 定位有趣的部分不是位置匹配，而是计算奖金。我们试图发现要添加到广告eCPM中的金额，基于页面内容中的单词与广告中匹配的单词。我们假设我们已经为每个广告中的每个单词预先计算了一个奖金（作为学习阶段的一部分），存储在每个单词的`ZSET`中，成员是广告ID，分数是要添加到eCPM中的金额。
- en: These word-based, per-ad eCPM bonuses have values such that the average eCPM
    of the ad, when shown on a page with that word, is the eCPM bonus from the word
    plus the known average CPM for the ad. Unfortunately, when more than one word
    in an ad matches the content of the page, adding all of the eCPM bonuses gives
    us a total eCPM bonus that probably isn’t close to reality.
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 这些基于单词、每个广告的eCPM奖金具有这样的值，即广告在包含该单词的页面上展示时的平均eCPM是单词的eCPM奖金加上广告已知的平均CPM。不幸的是，当广告中的多个单词与页面内容匹配时，添加所有eCPM奖金给我们的是一个总eCPM奖金，这可能与现实相差甚远。
- en: We have eCPM bonuses based on word matching for each word that are based on
    single word matching page content alone, or with any one or a number of other
    words in the ad. What we really want to find is the weighted average of the eCPMs,
    where the weight of each word is the number of times the word matched the page
    content. Unfortunately, we can’t perform the weighted average calculation with
    Redis `ZSET`s, because we can’t divide one `ZSET` by another.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有基于单词匹配的eCPM奖金，每个单词都是基于单个单词匹配页面内容，或者与广告中的任何一个或多个其他单词。我们真正想要找到的是eCPM的加权平均值，其中每个单词的权重是单词匹配页面内容的次数。不幸的是，我们无法使用Redis
    `ZSET`s执行加权平均值计算，因为我们不能将一个`ZSET`除以另一个。
- en: Numerically speaking, the weighted average lies between the geometric average
    and the arithmetic average, both of which would be reasonable estimates of the
    combined eCPM. But we can’t calculate either of those averages when the count
    of matching words varies. The best estimate of the ad’s true eCPM is to find the
    maximum and minimum bonuses, calculate the average of those two, and use that
    as the bonus for multiple matching words.
  id: totrans-1275
  prefs: []
  type: TYPE_NORMAL
  zh: 从数值上讲，加权平均值位于几何平均值和算术平均值之间，这两个值都是对组合eCPM的合理估计。但是，当匹配词的数量变化时，我们无法计算这两个平均值。广告的真实eCPM的最佳估计是找到最大和最小奖金，计算这两个的平均值，并将其作为多个匹配词的奖金。
- en: '|  |'
  id: totrans-1276
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Being mathematically rigorous
  id: totrans-1277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 具有数学上的严谨性
- en: Mathematically speaking, our method of averaging the maximum and minimum word
    bonuses to determine an overall bonus isn’t rigorous. The true mathematical expectation
    of the eCPM with a collection of matched words is different than what we calculated.
    We chose to use this mathematically nonrigorous method because it gets us to a
    reasonable answer (the weighted average of the words *is* between the minimum
    and maximum), with relatively little work. If you choose to use this bonus method
    along with our later learning methods, remember that there are better ways to
    target ads and to learn from user behavior. I chose this method because it’s easy
    to write, easy to learn, and easy to improve.
  id: totrans-1278
  prefs: []
  type: TYPE_NORMAL
  zh: 从数学上讲，我们通过平均最大和最小单词奖金来确定整体奖金的方法并不严谨。具有一组匹配词的eCPM的真实数学期望与我们计算的不同。我们选择使用这种数学上不严谨的方法，因为它能让我们得到一个合理的答案（单词的加权平均值确实在最小值和最大值之间），而且相对较少的工作。如果你选择使用这种奖金方法以及我们后面的学习方法，请记住，有更好的方法来定位广告和从用户行为中学习。我选择这种方法是因为它易于编写，易于学习，也易于改进。
- en: '|  |'
  id: totrans-1279
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We can calculate the maximum and minimum bonuses by using `ZUNIONSTORE` with
    the `MAX` and `MIN` aggregates. And we can calculate their average by using them
    as part of a `ZUNIONSTORE` operation with an aggregate of `SUM`, and their weights
    each being .5\. Our function for combining bonus word eCPMs with the base eCPM
    of the ad can be seen in the next listing.
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用带有`MAX`和`MIN`聚合的`ZUNIONSTORE`来计算最大和最小奖金。我们还可以通过将它们作为`ZUNIONSTORE`操作的一部分，使用聚合`SUM`，并且每个权重为.5来计算它们的平均值。我们的函数用于将奖金词eCPMs与广告的基础eCPM结合，可以在下一列表中看到。
- en: Listing 7.13\. Calculating the eCPM of ads including content match bonuses
  id: totrans-1281
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.13. 计算包括内容匹配奖金的广告的eCPM
- en: '![](173fig01_alt.jpg)'
  id: totrans-1282
  prefs: []
  type: TYPE_IMG
  zh: '![图片描述](173fig01_alt.jpg)'
- en: As before, we continue to pass the `_execute` parameter to delay execution of
    our various `ZINTERSTORE` and `ZUNIONSTORE` operations until after the function
    returns to the calling `target_ads()`. One thing that may be confusing is our
    use of `ZINTERSTORE` between the location-targeted ads and the bonuses, followed
    by a final `ZUNIONSTORE` call. Though we could perform fewer calls by performing
    a `ZUNIONSTORE` over all of our bonus `ZSET`s, followed by a single `ZINTERSTORE`
    call at the end (to match location), the majority of ad-targeting calls will perform
    better by performing many smaller intersections followed by a union.
  id: totrans-1283
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们继续将`_execute`参数传递给`target_ads()`调用，以延迟执行我们的各种`ZINTERSTORE`和`ZUNIONSTORE`操作。可能令人困惑的一点是我们使用`ZINTERSTORE`在位置定向广告和奖金之间，然后是一个最终的`ZUNIONSTORE`调用。虽然我们可以通过对所有奖金`ZSET`s执行一个`ZUNIONSTORE`，然后是一个单一的`ZINTERSTORE`调用（以匹配位置）来执行更少的调用，但大多数广告定向调用通过执行许多较小的交集然后进行合并将表现得更好。
- en: The difference between the two methods is illustrated in [figure 7.5](#ch07fig05),
    which shows that essentially all of the data in all of the relevant word bonus
    `ZSET`s is examined when we union and then intersect. Compare that with [figure
    7.6](#ch07fig06), which shows that when we intersect and then union, Redis will
    examine far less data to produce the same result.
  id: totrans-1284
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种方法的区别在[图7.5](#ch07fig05)中得到了说明，该图显示，当我们进行并集操作后再进行交集操作时，会检查所有相关词语奖励`ZSET`中的所有数据。与之相比，[图7.6](#ch07fig06)显示，当我们先进行交集操作后再进行并集操作时，Redis将检查的数据量要少得多，以产生相同的结果。
- en: Figure 7.5\. The data that’s examined during a union-then-intersect calculation
    of ad-targeting bonuses includes all ads in the relevant word bonus `ZSET`s, even
    ads that don’t meet the location matching requirements.
  id: totrans-1285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5。在执行并集-交集计算广告定位奖励时检查的数据包括所有相关词语奖励`ZSET`中的广告，即使这些广告不符合位置匹配要求。
- en: '![](07fig05_alt.jpg)'
  id: totrans-1286
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig05_alt.jpg)'
- en: Figure 7.6\. The data that’s examined during an intersect-then-union calculation
    of ad-targeting bonuses only includes those ads that previously matched, which
    significantly cuts down on the amount of data that Redis will process.
  id: totrans-1287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6。在执行交集-并集计算广告定位奖励时检查的数据仅包括之前匹配的广告，这显著减少了Redis需要处理的数据量。
- en: '![](07fig06_alt.jpg)'
  id: totrans-1288
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig06_alt.jpg)'
- en: After we have an ad targeted, we’re given a `target_id` and an `ad_id`. These
    IDs would then be used to construct an ad response that includes both pieces of
    information, in addition to fetching the ad copy, formatting the result, and returning
    the result to the web page or client that requested it.
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们定位广告后，我们得到了一个`target_id`和一个`ad_id`。这些ID将被用来构建一个包含这两条信息的广告响应，除了获取广告副本、格式化结果并将其返回到请求网页或客户端外。
- en: '|  |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: No matching content**'
  id: totrans-1291
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：无匹配内容**'
- en: If you pay careful attention to the flow of `target_ads()` through `finish_scoring()`
    in [listings 7.11](#ch07ex11) and [7.13](#ch07ex13), you’ll notice that we don’t
    make any effort to deal with the case where an ad has zero matched words in the
    content. In that situation, the eCPM produced will actually be the average eCPM
    over all calls that returned that ad itself. It’s not difficult to see that this
    may result in an ad being shown that shouldn’t. Can you alter `finish_scoring()`
    to take into consideration ads that don’t match any content?
  id: totrans-1292
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细关注`target_ads()`在[列表7.11](#ch07ex11)和[7.13](#ch07ex13)中的`finish_scoring()`流程，你会注意到我们没有努力处理广告在内容中没有匹配词语的情况。在这种情况下，产生的eCPM实际上是返回该广告的所有调用平均eCPM。不难看出，这可能会导致展示不应该展示的广告。你能修改`finish_scoring()`以考虑那些没有匹配任何内容的广告吗？
- en: '|  |'
  id: totrans-1293
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The only part of our `target_ads()` function from [listing 7.11](#ch07ex11)
    that we haven’t defined is `record_targeting_result()`, which we’ll now examine
    as part of the learning phase of ad targeting.
  id: totrans-1294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从[列表7.11](#ch07ex11)中尚未定义的`target_ads()`函数的部分是`record_targeting_result()`，我们现在将作为广告定位的学习阶段来检查它。
- en: 7.3.4\. Learning from user behavior
  id: totrans-1295
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.4. 从用户行为中学习
- en: As ads are shown to users, we have the opportunity to gain insight into what
    can cause someone to click on an ad. In the last section, we talked about using
    words as bonuses to ads that have already matched the required location. In this
    section, we’ll talk about how we can record information about those words and
    the ads that were targeted to discover basic patterns about user behavior in order
    to develop per-word, per-ad-targeting bonuses.
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 当广告展示给用户时，我们有机会深入了解什么因素会导致有人点击广告。在上一个章节中，我们讨论了使用词语作为奖励来提升已经匹配到所需位置的广告。在本节中，我们将讨论如何记录这些词语和被定位的广告的信息，以发现关于用户行为的基本模式，从而开发出按词语、按广告定位的奖励。
- en: A crucial question you should be asking yourself is “Why are we using words
    in the web page content to try to find better ads?” The simple reason is that
    ad placement is all about context. If a web page has content related to the safety
    of children’s toys, showing an ad for a sports car probably won’t do well. By
    matching words in the ad with words in the web page content, we get a form of
    context matching quickly and easily.
  id: totrans-1297
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该问自己一个关键问题：“为什么我们在网页内容中使用词语来尝试找到更好的广告？”简单的理由是广告投放完全关乎上下文。如果一个网页的内容与儿童玩具的安全相关，展示一辆跑车广告可能不会效果很好。通过将广告中的词语与网页内容中的词语相匹配，我们可以快速简单地实现一种上下文匹配。
- en: One thing to remember during this discussion is that we aren’t trying to be
    perfect. We aren’t trying to solve the ad-targeting and learning problem completely;
    we’re trying to build something that will work “pretty well” with simple and straightforward
    methods. As such, our note about the fact that this isn’t mathematically rigorous
    still applies.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 在这次讨论中需要记住的一点是我们并不试图做到完美。我们并不是试图完全解决广告定位和学习问题；我们试图构建一些能够通过简单直接的方法“相当好”工作的东西。因此，我们关于这并不数学上严谨的笔记仍然适用。
- en: Recording views
  id: totrans-1299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记录观看次数
- en: 'The first step in our learning process is recording the results of our ad targeting
    with the `record_targeting_result()` function that we called earlier from [listing
    7.11](#ch07ex11). Overall, we’ll record some information about the ad-targeting
    results, which we’ll later use to help us calculate click-through rates, action
    rates, and ultimately eCPM bonuses for each word. We’ll record the following:'
  id: totrans-1300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习过程的第一步是记录我们之前从[列表7.11](#ch07ex11)中调用的`record_targeting_result()`函数的广告定位结果。总的来说，我们将记录一些关于广告定位结果的信息，我们稍后会使用这些信息来帮助我们计算点击率、行动率和最终每个单词的eCPM奖励。我们将记录以下内容：
- en: Which words were targeted with the given ad
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与给定广告一起被定位的单词
- en: The total number of times that a given ad has been targeted
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个特定广告被定位的总次数
- en: The total number of times that a word in the ad was part of the bonus calculation
  id: totrans-1303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广告中某个单词在奖励计算中出现的总次数
- en: To record this information, we’ll store a `SET` of the words that were targeted
    and keep counts of the number of times that the ad and words were seen as part
    of a single `ZSET` per ad. Our code for recording this information is shown next.
  id: totrans-1304
  prefs: []
  type: TYPE_NORMAL
  zh: 为了记录这些信息，我们将存储一个表示被定位单词的`SET`，并为每个广告的单词和广告被看作是单一`ZSET`的一部分的次数进行计数。我们记录这些信息的代码如下。
- en: Listing 7.14\. A method for recording the result after we’ve targeted an ad
  id: totrans-1305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.14。记录我们定位广告后的结果的方法
- en: '![](176fig01_alt.jpg)'
  id: totrans-1306
  prefs: []
  type: TYPE_IMG
  zh: '![](176fig01_alt.jpg)'
- en: That function does everything we said it would do, and you’ll notice a call
    to `update_cpms()`. This `update_cpms()` function is called every 100th time the
    ad is returned from a call. This function really is the core of the learning phase—it
    writes back to our per-word, per-ad-targeting bonus `ZSET`s.
  id: totrans-1307
  prefs: []
  type: TYPE_NORMAL
  zh: 那个函数做了我们所说的所有事情，你会注意到一个对`update_cpms()`的调用。这个`update_cpms()`函数在广告每次从调用中返回时被调用第100次。这个函数实际上是学习阶段的核心——它将数据写回到我们每个单词、每个广告定位的奖励`ZSET`s中。
- en: We’ll get to updating the eCPM of an ad in a moment, but first, let’s see what
    happens when an ad is clicked.
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很快就会更新广告的eCPM，但首先，让我们看看点击广告会发生什么。
- en: Recording clicks and actions
  id: totrans-1309
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记录点击和行动
- en: 'As we record views, we’re recording half of the data for calculating CTRs.
    The other half of the data that we need to record is information about the clicks
    themselves, or in the case of a cost per action ad, the action. Numerically, this
    is because our eCPM calculations are based on this formula: (value of a click
    or action) × (clicks or actions) / views. Without recording clicks and actions,
    the numerator of our value calculation is 0, so we can’t discover anything useful.'
  id: totrans-1310
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们记录观看次数时，我们正在记录计算CTR所需数据的一半。我们需要记录的另一半数据是关于点击本身的信息，或者在按行动付费的广告的情况下，是关于行动的信息。从数值上来说，这是因为我们的eCPM计算基于以下公式：（点击或行动的价值）×（点击或行动次数）/观看次数。如果没有记录点击和行动，我们价值计算的分母为0，因此我们无法发现任何有用的信息。
- en: When someone actually clicks on an ad, prior to redirecting them to their final
    destination, we’ll record the click in the total aggregates for the type of ad,
    as well as whether the ad got a click and which words matched the clicked ad.
    We’ll record the same information for actions. Our function for recording clicks
    is shown next.
  id: totrans-1311
  prefs: []
  type: TYPE_NORMAL
  zh: 当有人实际点击广告，在他们被重定向到最终目的地之前，我们将记录点击在广告类型的总聚合中，以及广告是否被点击以及哪些单词与被点击的广告匹配。我们还将记录关于行动的相同信息。我们记录点击的函数如下。
- en: Listing 7.15\. A method for recording clicks on an ad
  id: totrans-1312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.15。记录广告点击的方法
- en: '![](177fig01_alt.jpg)'
  id: totrans-1313
  prefs: []
  type: TYPE_IMG
  zh: '![](177fig01_alt.jpg)'
- en: You’ll notice there are a few parts of the recording function that we didn’t
    mention earlier. In particular, when we receive a click or an action for a CPA
    ad, we’ll refresh the expiration of the words that were a part of the ad-targeting
    call. This will let an action following a click count up to 15 minutes after the
    initial click-through to the destination site happened.
  id: totrans-1314
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到记录函数中有一些我们之前没有提到的部分。特别是，当我们收到CPA广告的点击或操作时，我们将刷新广告定位调用中作为一部分的单词的过期时间。这将允许在点击到目标网站后的15分钟内计算随后的操作。
- en: Another change is that we’ll optionally be recording actions in this call for
    CPA ads; we’ll assume that this function is called with the `action` parameter
    set to `True` in that case.
  id: totrans-1315
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个变化是我们将可选地记录CPA广告的此调用中的操作；在这种情况下，我们将假设此函数通过将`action`参数设置为`True`来调用。
- en: And finally, we’ll call the `update_cpms()` function for every click/action
    because they should happen roughly once every 100–2000 views (or more), so each
    individual click/action is important relative to a view.
  id: totrans-1316
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将为每个点击/操作调用`update_cpms()`函数，因为它们大约每100-2000次浏览（或更多）发生一次，所以相对于浏览量，每个单独的点击/操作都很重要。
- en: '|  |'
  id: totrans-1317
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Changing the way we count clicks and actions**'
  id: totrans-1318
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：改变我们计数点击和操作的方式**'
- en: 'In [listing 7.15](#ch07ex15), we define a `record_click()` function to add
    1 to every word that was targeted as part of an ad that was clicked on. Can you
    think of a different number to add to a word that may make more sense? Hint: You
    may want to consider this number to be related to the count of matched words.
    Can you update `finish_scoring()` and `record_click()` to take into consideration
    this new click/ action value?'
  id: totrans-1319
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.15](#ch07ex15)中，我们定义了一个`record_click()`函数，为作为被点击广告一部分的目标单词增加1。你能想到一个可能更有意义的数字添加到单词中吗？提示：你可能想考虑这个数字与匹配单词的计数相关。你能更新`finish_scoring()`和`record_click()`以考虑这个新的点击/操作值吗？
- en: '|  |'
  id: totrans-1320
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: To complete our learning process, we only need to define our final `update_cpms()`
    function.
  id: totrans-1321
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成我们的学习过程，我们只需要定义我们的最终`update_cpms()`函数。
- en: Updating eCPMs
  id: totrans-1322
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更新eCPM
- en: We’ve been talking about and using the `update_cpms()` function for a couple
    of sections now, and hopefully you already have an idea of what happens inside
    of it. Regardless, we’ll walk through the different parts of how we’ll update
    our per-word, per-ad bonus eCPMs, as well as how we’ll update our per-ad eCPMs.
  id: totrans-1323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经讨论并使用`update_cpms()`函数好几个部分了，希望你已经对它内部发生的事情有了大概的了解。无论如何，我们将逐步讲解如何更新我们的按词、按广告的额外eCPM，以及如何更新我们的按广告eCPM。
- en: The first part to updating our eCPMs is to know the click-through rate of an
    ad by itself. Because we’ve recorded both the clicks and views for each ad overall,
    we have the click-through rate by pulling both of those scores from the relevant
    `ZSET`s. By combining that click-through rate with the ad’s actual value, which
    we fetch from the ad’s base value `ZSET`, we can calculate the eCPM of the ad
    over all clicks and views.
  id: totrans-1324
  prefs: []
  type: TYPE_NORMAL
  zh: 更新我们的eCPM的第一步是知道广告本身的点击率。因为我们记录了每个广告的点击和浏览总数，所以我们有通过从相关的`ZSET`s中提取这两个分数来计算点击率的点击率。通过将这个点击率与广告的实际价值相结合，我们从广告的基础值`ZSET`中获取它，我们可以计算广告在所有点击和浏览中的eCPM。
- en: The second part to updating our eCPMs is to know the CTR of words that were
    matched in the ad itself. Again, because we recorded all views and clicks involving
    the ad, we have that information. And because we have the ad’s base value, we
    can calculate the eCPM. When we have the word’s eCPM, we can subtract the ad’s
    eCPM from it to determine the bonus that the word matching contributes. This difference
    is what’s added to the per-word, per-ad bonus `ZSET`s.
  id: totrans-1325
  prefs: []
  type: TYPE_NORMAL
  zh: 更新我们的eCPM的第二步是知道广告本身匹配的单词的CTR。同样，因为我们记录了涉及广告的所有浏览和点击，所以我们有这些信息。并且因为我们有广告的基础值，我们可以计算eCPM。当我们有单词的eCPM时，我们可以从它中减去广告的eCPM来确定匹配单词带来的奖金。这个差异就是添加到按词、按广告额外`ZSET`s中的值。
- en: The same calculation is performed for actions as was performed for clicks, the
    only difference being that we use the action count `ZSET`s instead of the click
    count `ZSET`s. Our method for updating eCPMs for clicks and actions can be seen
    in the next listing.
  id: totrans-1326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于操作，我们执行与点击相同的计算，唯一的区别是我们使用操作计数`ZSET`s而不是点击计数`ZSET`s。我们的更新点击和操作eCPM的方法可以在下一个列表中看到。
- en: Listing 7.16\. A method for updating eCPMs and per-word eCPM bonuses for ads
  id: totrans-1327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.16\. 更新广告eCPM和按词eCPM奖金的方法
- en: '![](ch07ex16-0.jpg)'
  id: totrans-1328
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch07ex16-0.jpg)'
- en: '![](ch07ex16-1.jpg)'
  id: totrans-1329
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch07ex16-1.jpg)'
- en: '|  |'
  id: totrans-1330
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Optimizing eCPM calculations**'
  id: totrans-1331
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：优化 eCPM 计算**'
- en: In [listing 7.16](#ch07ex16), we perform a number of round trips to Redis that’s
    relative to the number of words that were targeted. More specifically, we perform
    the number of words plus three round trips. In most cases, this should be relatively
    small (considering that most ads won’t have a lot of content or related keywords).
    But even so, some of these round trips can be avoided. Can you update the `update_cpms()`
    function to perform a total of only three round trips?
  id: totrans-1332
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表7.16](#ch07ex16)中，我们根据被定位的单词数量执行了相对数量的 Redis 往返操作。更具体地说，我们执行了单词数量加三次往返。在大多数情况下，这应该是相对较小的（考虑到大多数广告不会有很多内容或相关关键词）。但即便如此，其中一些往返操作是可以避免的。你能更新
    `update_cpms()` 函数，使其总共只进行三次往返吗？
- en: '|  |'
  id: totrans-1333
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: In our `update_cpms()` function, we updated the global per-type click-through
    and action rates, the per-ad eCPMs, and the per-word, per-ad bonus eCPMs.
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的 `update_cpms()` 函数中，我们更新了全局的按类型点击率和行动率，每个广告的 eCPM，以及每个单词、每个广告的奖金 eCPM。
- en: 'With the learning portion of our ad-targeting process now complete, we’ve now
    built a complete ad-targeting engine from scratch. This engine will learn over
    time, adapt the ads it returns over time, and more. We can make many possible
    additions or changes to this engine to make it perform even better, some of which
    are mentioned in the exercises, and several of which are listed next. These are
    just starting points for building with Redis:'
  id: totrans-1335
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的广告定位过程的学习部分现已完成，我们现在从头开始构建了一个完整的广告定位引擎。这个引擎将随着时间的推移而学习，随着时间的推移调整它返回的广告，以及更多。我们可以对这个引擎进行许多可能的添加或更改，以使其表现更好，其中一些在练习中提到，还有一些将在下面列出。这些都是使用
    Redis 构建的开始点：
- en: Over time, the total number of clicks and views for each ad will stabilize around
    a particular ratio, and subsequent clicks and views won’t alter that ratio significantly.
    The real CTR of an ad will vary based on time of day, day of week, and more. Consider
    degrading an ad’s click/action and view count on a regular basis, as we did in
    [section 2.5](kindle_split_012.html#ch02lev1sec5) with our `rescale_viewed()`
    call. This same concept applies to our global expected CTRs.
  id: totrans-1336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随着时间的推移，每个广告的总点击次数和浏览次数将稳定在某个特定的比率周围，后续的点击和浏览不会显著改变这个比率。广告的实际点击率（CTR）将根据一天中的时间、一周中的日子等因素而变化。考虑定期降低广告的点击/行动和浏览计数，就像我们在[第2.5节](kindle_split_012.html#ch02lev1sec5)中的
    `rescale_viewed()` 调用所做的那样。这个相同的概念也适用于我们的全球预期点击率。
- en: To extend the learning ability beyond just a single count, consider keeping
    counts over the last day, week, and other time slices. Also consider ways of weighing
    those time slices differently, depending on their age. Can you think of a method
    to learn proper weights of different ages of counts?
  id: totrans-1337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了将学习能力扩展到仅仅一个计数之外，考虑对过去一天、一周和其他时间切片进行计数。同时考虑根据它们的时间长度以不同的方式权衡这些时间切片。你能想出一个方法来学习不同年龄的计数的适当权重吗？
- en: All of the big ad networks use second-price auctions in order to charge for
    a given ad placement. More specifically, rather than charging a fixed rate per
    click, per thousand views, or per action, you charge a rate that’s relative to
    the second-highest value ad that was targeted.
  id: totrans-1338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有的大型广告网络都使用第二价格拍卖来对特定的广告位置进行收费。更具体地说，而不是按每次点击、每千次浏览或每次行动收取固定费用，你收取的费率与被定位的第二高价值广告相关。
- en: In most ad networks, there’ll be a set of ads that rotate through the highest-value
    slot for a given set of keywords as each of them runs out of money. These ads
    are there because their high value and CTR earn the top slot. This means that
    new ads that don’t have sufficiently high values will never be seen, so the network
    will never discover them. Rather than picking the highest-eCPM ads 100% of the
    time, fetch the top 100 ads and choose ads based on the relative values of their
    eCPMs anywhere from 10%-50% of the time (depending on how you want to balance
    learning true eCPMs and earning the most money).
  id: totrans-1339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数广告网络中，将有一组广告在给定的关键词组用完资金后轮流占据最高价值的位置。这些广告之所以存在，是因为它们的高价值和点击率赢得了顶级位置。这意味着没有足够高价值的新的广告永远不会被看到，因此网络永远不会发现它们。而不是100%的时间选择最高
    eCPM 的广告，取而代之的是，取前100个广告，并在10%-50%的时间内（根据你想要如何平衡学习真实 eCPM 和赚取最多金钱）根据它们 eCPM 的相对价值来选择广告。
- en: When ads are initially placed into the system, we know little about what to
    expect in terms of eCPM. We addressed this briefly by using the average CTR of
    all ads of the same type, but this is moot the moment a single click comes in.
    Another method mixes the average CTR for a given ad type, along with the seen
    CTR for the ad based on the number of views that the ad has seen. A simple inverse
    linear or inverse sigmoid relationship between the two can be used until the ad
    has had sufficient views (2,000–5,000 views is typically enough to determine a
    reliable CTR).
  id: totrans-1340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当广告最初被放入系统中时，我们对预期的eCPM知之甚少。我们通过使用相同类型所有广告的平均CTR来简要解决这个问题，但一旦有单个点击进来，这就不重要了。另一种方法是将给定广告类型的平均CTR与基于广告已看到的查看次数的广告看到的CTR混合。在广告有足够的查看次数（通常是2,000-5,000次查看）之前，可以使用简单的倒数线性或倒数S形关系。
- en: In addition to mixing the average CTR for a given ad type with the CTR of the
    ad during the learning process, ads that are in the process of reaching an initial
    2,000–5,000 views can have their CTR/eCPM artificially boosted. This can ensure
    sufficient traffic for the system to learn the ads’ actual eCPMs.
  id: totrans-1341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了在学习过程中将给定广告类型的平均CTR与广告的CTR混合之外，正在达到初始2,000-5,000次查看的广告可以人为地提高其CTR/eCPM。这可以确保系统有足够的流量来学习广告的实际eCPM。
- en: Our method of learning per-word bonuses is similar to Bayesian statistics. We
    could use real Bayesian statistics, neural networks, association rule learning,
    clustering, or other techniques to calculate the bonuses. These other methods
    may offer more mathematically rigorous results, which may result in better CTRs,
    and ultimately more money earned.
  id: totrans-1342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们学习每个单词奖励的方法类似于贝叶斯统计。我们可以使用真实的贝叶斯统计、神经网络、关联规则学习、聚类或其他技术来计算奖励。这些其他方法可能提供更数学上严谨的结果，这可能会导致更好的点击率（CTR），最终赚取更多的钱。
- en: In our code listings, we recorded views, clicks, and actions as part of the
    calls that return the ads or handle any sort of redirection. These operations
    may take a long time to execute, so should be handled after the calls have returned
    by being executed as an external task, as we discussed in [section 6.4](kindle_split_017.html#ch06lev1sec4).
  id: totrans-1343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在我们的代码列表中，我们记录了查看、点击和操作，作为返回广告或处理任何类型重定向的调用的一部分。这些操作可能需要很长时间才能执行，因此应该在调用返回后作为外部任务处理，正如我们在[第6.4节](kindle_split_017.html#ch06lev1sec4)中讨论的那样。
- en: As you can see from our list, many additions and improvements can and should
    be made to this platform. But as an initial pass, what we’ve provided can get
    you started in learning about and building the internet’s next-generation ad-targeting
    platform.
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，我们可以对这个平台进行许多改进。但作为一个初步的尝试，我们提供的内容可以帮助你开始了解和构建互联网下一代广告定位平台。
- en: Now that you’ve learned about how to build an ad-targeting platform, let’s keep
    going to see how to use search tools to find jobs that candidates are qualified
    for as part of a job-search tool.
  id: totrans-1345
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了如何构建广告定位平台，让我们继续看看如何使用搜索工具来寻找候选人符合资格的工作，作为求职工具的一部分。
- en: 7.4\. Job search
  id: totrans-1346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4. 求职
- en: If you’re anything like me, at some point in your past you’ve spent time looking
    through classifieds and online job-search pages, or have used a recruiting agency
    to try to find work. One of the first things that’s checked (after location) is
    required experience and/or skills.
  id: totrans-1347
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你和我一样，你过去某个时候一定花时间浏览过分类广告和在线求职页面，或者使用招聘机构试图找工作。检查的第一件事（在位置之后）是所需的经验和/或技能。
- en: In this section, we’ll talk about using Redis `SET`s and `ZSET`s to find jobs
    for which a candidate has all of the required skills. When you’re finished reading
    this section, you’ll understand another way of thinking about your problem that
    fits the Redis data model.
  id: totrans-1348
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论使用Redis的`SET`s和`ZSET`s来寻找候选人拥有所有所需技能的工作。当你阅读完本节后，你将了解另一种思考问题的方法，这符合Redis数据模型。
- en: As a way of approaching this problem, we’ll say that Fake Garage Startup is
    branching out in their offerings, trying to pull their individual and group chat
    customers into using their system to find work. Initially, they’re only offering
    the ability for users to search for positions in which they’re qualified.
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 作为解决这个问题的一种方法，我们将说假车库创业公司正在扩展其服务，试图将他们的个人和群组聊天客户吸引到使用他们的系统来找工作。最初，他们只提供用户搜索他们有资格担任的职位的可能性。
- en: 7.4.1\. Approaching the problem one job at a time
  id: totrans-1350
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1. 逐个处理问题
- en: At first glance, we might consider a straightforward solution to this problem.
    Start with every job having its own `SET`, with members being the skills that
    the job requires. To check whether a candidate has all of the requirements for
    a given job, we’d add the candidate’s skills to a `SET` and then perform the `SDIFF`
    of the job and the candidate’s skills. If there are no skills in the resulting
    `SDIFF`, then the user has all of the qualifications necessary to complete the
    job. The code for adding a job and checking whether a given set of skills is sufficient
    for that job looks like this next listing.
  id: totrans-1351
  prefs: []
  type: TYPE_NORMAL
  zh: 初看这个问题，我们可能会考虑一个直接的解决方案。从每个工作都有其自己的`SET`开始，其中成员是该工作所需的技能。为了检查候选人是否具备特定工作的所有要求，我们会将候选人的技能添加到一个`SET`中，然后执行工作和候选人技能的`SDIFF`。如果结果`SDIFF`中没有技能，那么用户就有完成这份工作所需的所有资格。添加工作并检查给定技能集是否足够完成该工作的代码如下所示。
- en: Listing 7.17\. A potential solution for finding jobs when a candidate meets
    all requirements
  id: totrans-1352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.17\. 当候选人满足所有要求时寻找工作的潜在解决方案
- en: '![](181fig01_alt.jpg)'
  id: totrans-1353
  prefs: []
  type: TYPE_IMG
  zh: '![](181fig01_alt.jpg)'
- en: Explaining that again, we’re checking whether a job requires any skills that
    the candidate doesn’t have. This solution is okay, but it suffers from the fact
    that to find all of the jobs for a given candidate, we must check each job individually.
    This won’t scale, but there are solutions that will.
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 再次解释，我们是在检查这份工作是否需要候选人没有的任何技能。这个解决方案是可以的，但它有一个缺点，那就是为了找到给定候选人的所有工作，我们必须逐个检查每份工作。这无法扩展，但存在解决方案。
- en: 7.4.2\. Approaching the problem like search
  id: totrans-1355
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2\. 以搜索的方式处理问题
- en: In [section 7.3.3](#ch07lev2sec7), we used `SET`s and `ZSET`s as holders for
    additive bonuses for optional targeting parameters. If we’re careful, we can do
    the same thing for groups of required targeting parameters.
  id: totrans-1356
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7.3.3节](#ch07lev2sec7)中，我们使用了`SET`s和`ZSET`s作为可选定位参数的累加奖励的持有者。如果我们小心，我们也可以为一系列必需的定位参数组做同样的事情。
- en: Rather than talk about jobs with skills, we need to flip the problem around
    like we did with the other search problems described in this chapter. We start
    with one `SET` per skill, which stores all of the jobs that require that skill.
    In a required skills `ZSET`, we store the total number of skills that a job requires.
    The code that sets up our index looks like the next listing.
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是讨论具有技能的工作，我们需要像本章中描述的其他搜索问题一样，将问题反过来。我们以每个技能一个`SET`开始，其中存储了所有需要该技能的工作。在一个所需技能`ZSET`中，我们存储了工作所需的技能总数。设置我们索引的代码如下所示。
- en: Listing 7.18\. A function for indexing jobs based on the required skills
  id: totrans-1358
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.18\. 基于所需技能索引工作的函数
- en: '![](182fig01_alt.jpg)'
  id: totrans-1359
  prefs: []
  type: TYPE_IMG
  zh: '![](182fig01_alt.jpg)'
- en: This indexing function should remind you of the text indexing function we used
    in [section 7.1](#ch07lev1sec1). The only major difference is that we’re providing
    `index_job()` with pretokenized skills, and we’re adding a member to a `ZSET`
    that keeps a record of the number of skills that each job requires.
  id: totrans-1360
  prefs: []
  type: TYPE_NORMAL
  zh: 这个索引函数应该会让你想起我们在[第7.1节](#ch07lev1sec1)中使用的文本索引函数。唯一的重大区别是我们向`index_job()`提供了预处理过的技能，并且我们在一个`ZSET`中添加了一个成员，该`ZSET`记录了每个工作所需的技能数量。
- en: To perform a search for jobs that a candidate has all of the skills for, we
    need to approach the search like we did with the bonuses to ad targeting in [section
    7.3.3](#ch07lev2sec7). More specifically, we’ll perform a `ZUNIONSTORE` operation
    over skill `SET`s to calculate a total score for each job. This score represents
    how many skills the candidate has for each of the jobs.
  id: totrans-1361
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行搜索以找到候选人具备所有技能的工作，我们需要像在[第7.3.3节](#ch07lev2sec7)中为广告定位的奖励进行搜索那样处理搜索。更具体地说，我们将对技能`SET`s执行`ZUNIONSTORE`操作，以计算每个工作的总分数。这个分数代表了候选人为每个工作拥有的技能数量。
- en: Because we have a `ZSET` with the total number of skills required, we can then
    perform a `ZINTERSTORE` operation between the candidate’s `ZSET` and the required
    skills `ZSET` with weights -1 and 1, respectively. Any job ID with a score equal
    to 0 in that final result `ZSET` is a job that the candidate has all of the required
    skills for. The code for implementing the search operation is shown in the following
    listing.
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们有一个包含所需技能总数的`ZSET`，我们可以然后执行候选人的`ZSET`和所需技能`ZSET`之间的`ZINTERSTORE`操作，权重分别为-1和1。在最终结果`ZSET`中，任何得分为0的工作ID都是候选人具备所有所需技能的工作。实现搜索操作的代码如下所示。
- en: Listing 7.19\. Find all jobs that a candidate is qualified for
  id: totrans-1363
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.19\. 找到候选人符合资格的所有工作
- en: '![](183fig01_alt.jpg)'
  id: totrans-1364
  prefs: []
  type: TYPE_IMG
  zh: '![](183fig01_alt.jpg)'
- en: Again, we first find the scores for each job. After we have the scores for each
    job, we subtract each job score from the total score necessary to match. In that
    final result, any job with a `ZSET` score of 0 is a job that the candidate has
    all of the skills for.
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，我们首先为每个职位找到分数。在得到每个职位的分数后，我们从匹配所需的总分数中减去每个职位的分数。在最终结果中，任何`ZSET`分数为0的职位都是候选人拥有所有技能的职位。
- en: Depending on the number of jobs and searches that are being performed, our job-search
    system may or may not perform as fast as we need it to, especially with large
    numbers of jobs or searches. But if we apply sharding techniques that we’ll discuss
    in [chapter 9](kindle_split_021.html#ch09), we can break the large calculations
    into smaller pieces and calculate partial results bit by bit. Alternatively, if
    we first find the `SET` of jobs in a location to search for jobs, we could perform
    the same kind of optimization that we performed with ad targeting in [section
    7.3.3](#ch07lev2sec7), which could greatly improve job-search performance.
  id: totrans-1366
  prefs: []
  type: TYPE_NORMAL
  zh: 根据正在进行的职位和搜索的数量，我们的职位搜索系统可能或可能不会像我们需要的那么快，特别是当有大量职位或搜索时。但如果我们应用第9章中将要讨论的分区技术，我们可以将大计算分解成小块，并逐步计算部分结果。或者，如果我们首先找到要搜索职位的地点中的`SET`，我们可以执行与第7.3.3节中广告定位相同的优化，这可能会大大提高职位搜索性能。
- en: '|  |'
  id: totrans-1367
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Levels of experience**'
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：经验水平**'
- en: A natural extension to the simple required skills listing is an understanding
    that skill levels vary from beginner to intermediate, to expert, and beyond. Can
    you come up with a method using additional `SET`s to offer the ability, for example,
    for someone who has as intermediate level in a skill to find jobs that require
    either beginner or intermediate-level candidates?
  id: totrans-1369
  prefs: []
  type: TYPE_NORMAL
  zh: 简单所需技能列表的自然扩展是对技能水平从入门级到中级，再到专家，以及更高级别的理解。你能想出一个使用额外的`SET`来提供能力的方法，例如，让一个技能处于中级水平的人找到需要入门级或中级水平候选人的职位吗？
- en: '|  |'
  id: totrans-1370
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-1371
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Years of experience**'
  id: totrans-1372
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：工作经验年数**'
- en: Levels of expertise can be useful, but another way to look at the amount of
    experience someone has is the number of years they’ve used it. Can you build an
    alternate version that supports handling arbitrary numbers of years of experience?
  id: totrans-1373
  prefs: []
  type: TYPE_NORMAL
  zh: 专业知识水平可能很有用，但另一种看待某人经验量的方式是他们使用该技能的年数。你能构建一个支持处理任意年数工作经验的替代版本吗？
- en: '|  |'
  id: totrans-1374
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.5\. Summary
  id: totrans-1375
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 概述
- en: In this chapter, you’ve learned how to perform basic searching using `SET` operations,
    and then ordered the results based on either values in `HASH`es, or potentially
    composite values with `ZSET`s. You continued through the steps necessary to build
    and update information in an ad-targeting network, and you finished with job searching
    that turned the idea of scoring search results on its head.
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何使用`SET`操作执行基本搜索，然后根据`HASH`中的值或潜在的复合值（使用`ZSET`）对结果进行排序。你继续执行构建和更新广告定位网络信息的步骤，并以对搜索结果评分的想法进行职位搜索作为结束。
- en: Though the problems introduced in this chapter may have been new, one thing
    that you should’ve gotten used to by now is Redis’s ability to help you solve
    an unexpectedly wide variety of problems. With the data modeling options available
    in other databases, you really only have one tool to work with. But with Redis,
    the five data structures and pub/sub are an entire toolbox for you to work with.
  id: totrans-1377
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章介绍的问题可能都是新的，但你现在应该已经习惯了Redis帮助你解决意外广泛的各类问题的能力。在其他数据库中，你实际上只有一个工具可以使用。但与Redis相比，五种数据结构和pub/sub为你提供了一个完整的工具箱来使用。
- en: In the next chapter, we’ll continue to use Redis `HASH`es and `ZSET`s as building
    blocks in our construction of a fully functional Twitter clone.
  id: totrans-1378
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将继续使用Redis `HASH`和`ZSET`作为构建一个完全功能性的Twitter克隆的基础。
- en: Chapter 8\. Building a simple social network
  id: totrans-1379
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章\. 构建简单的社交网络
- en: '*This chapter covers*'
  id: totrans-1380
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Users and statuses
  id: totrans-1381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户和状态
- en: Home timeline
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主时间线
- en: Followers/following lists
  id: totrans-1383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关注者/被关注者列表
- en: Posting or deleting a status update
  id: totrans-1384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布或删除状态更新
- en: Streaming API
  id: totrans-1385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流式API
- en: In this chapter, we’ll cover the data structures and concepts necessary to build
    a system that offers almost all of the back-end-level functionality of Twitter.
    This chapter isn’t intended to allow you to build a site that scales to the extent
    of Twitter, but the methods that we cover should give you a much better understanding
    of how social networking sites can be built from simple structures and data.
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍构建一个几乎提供所有Twitter后端级别功能系统的数据结构和概念。本章的目的不是让你构建一个可以扩展到Twitter规模的网站，但我们介绍的方法应该能让你更好地理解社交网络是如何从简单的结构和数据构建起来的。
- en: We’ll begin this chapter by talking about user and status objects, which are
    the basis of almost all of the information in our application. From there, we’ll
    discuss the home timeline and followers/following lists, which are sequences of
    status messages or users. Continuing on, we’ll work through posting status messages,
    following/unfollowing someone, and deleting posts, which involves manipulating
    those lists. Finally, we’ll build out a fully functioning streaming API with web
    server to encourage users of the social network to use and play with the data.
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，讨论用户和状态对象，它们是我们应用程序中几乎所有信息的基石。从那里，我们将讨论主页时间线和关注者/被关注者列表，它们是状态消息或用户的序列。继续下去，我们将处理发布状态消息、关注/取消关注某人以及删除帖子，这涉及到操作这些列表。最后，我们将构建一个完整的流式API，配合Web服务器，以鼓励社交网络用户使用和玩弄数据。
- en: In the last chapter, we spent much of our time building an ad-targeting engine
    that combined user-entered data (the ads and their prices) with click behavior
    data in order to optimize ad earnings. The ad-targeting engine was query-intensive,
    in that every request could cause a lot of computation. In this Twitter-like platform,
    we’ll do our best to perform as little work as possible when someone is interested
    in viewing a page.
  id: totrans-1388
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们花费了大量时间构建一个广告定位引擎，该引擎将用户输入的数据（广告及其价格）与点击行为数据相结合，以优化广告收入。广告定位引擎是查询密集型的，因为每个请求都可能引起大量的计算。在这个类似Twitter的平台中，我们将尽力在有人感兴趣查看页面时做尽可能少的工作。
- en: To get started, let’s build the basic structures that will hold much of the
    data that our users are interested in.
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，让我们构建将包含我们用户感兴趣的大部分数据的基结构。
- en: 8.1\. Users and statuses
  id: totrans-1390
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 用户和状态
- en: 'As users interact with Twitter, two types of objects hold the most important
    information: users and status messages. User objects hold basic identity information,
    as well as aggregate data about the number of followers, number of status messages
    posted, and more. The user objects are important because they’re the starting
    point for every other kind of data that’s available or interesting. Status messages
    are also important because they’re how individuals express themselves and interact
    with each other, and are the true content of social networks.'
  id: totrans-1391
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户与Twitter互动时，两种类型的对象包含最重要的信息：用户和状态消息。用户对象包含基本身份信息，以及关于关注者数量、发布的状态消息数量等聚合数据。用户对象之所以重要，是因为它们是其他所有可用或有趣的数据的起点。状态消息也很重要，因为它们是个人表达自己和相互交流的方式，也是社交网络的真实内容。
- en: In this section, we’ll talk about what data will be stored in the user and status
    message objects and how we’ll store them. We’ll also look at a function to create
    a new user.
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论将存储在用户和状态消息对象中的数据以及我们将如何存储它们。我们还将查看一个创建新用户的函数。
- en: Our first step is to define and create the structure for a user.
  id: totrans-1393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第一步是定义和创建用户结构。
- en: 8.1.1\. User information
  id: totrans-1394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1\. 用户信息
- en: In a variety of online services and social networks, user objects can be the
    basic building blocks from which everything else is derived. Our Twitter work-alike
    is no different.
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 在各种在线服务和社交网络中，用户对象可以是其他所有东西的基本构建块。我们的Twitter类似产品也不例外。
- en: We’ll store user information inside of Redis as a `HASH`, similar to how we
    stored articles in [chapter 1](kindle_split_011.html#ch01). Data that we’ll store
    includes the username of the user, how many followers they have, how many people
    they’re following, how many status messages they’ve posted, their sign-up date,
    and any other meta-information we decide to store down the line. A sample `HASH`
    that includes this information for a user with the username of dr_josiah (my Twitter
    username) is shown in [figure 8.1](#ch08fig01).
  id: totrans-1396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 Redis 中以 `HASH` 的形式存储用户信息，类似于我们在[第 1 章](kindle_split_011.html#ch01)中存储文章的方式。我们将存储的数据包括用户的用户名、他们有多少关注者、他们关注了多少人、他们发布了多少条状态消息、他们的注册日期以及我们决定存储的其他任何元信息。一个包含用户
    dr_josiah（我的 Twitter 用户名）信息的示例 `HASH` 如[图 8.1](#ch08fig01)所示。
- en: Figure 8.1\. Example user information stored in a `HASH`
  id: totrans-1397
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.1\. 存储在 `HASH` 中的示例用户信息
- en: '![](08fig01.jpg)'
  id: totrans-1398
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig01.jpg)'
- en: From this figure, you can see that I have a modest number of followers, along
    with other information. When a new user signs up, we only need to create an object
    with the following, followers, and post count set to zero, a new timestamp for
    the sign-up time, and the relevant username. The function to perform this initial
    creation is shown next.
  id: totrans-1399
  prefs: []
  type: TYPE_NORMAL
  zh: 从这张图中，你可以看到我有一些谦虚的关注者数量，以及其他信息。当新用户注册时，我们只需要创建一个具有以下内容的对象，关注者和帖子计数设置为零，一个新的注册时间戳，以及相关的用户名。执行此初始创建的函数如下所示。
- en: Listing 8.1\. How to create a new user profile `HASH`
  id: totrans-1400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.1\. 如何创建新的用户资料 `HASH`
- en: '![](187fig01_alt.jpg)'
  id: totrans-1401
  prefs: []
  type: TYPE_IMG
  zh: '![](187fig01_alt.jpg)'
- en: 'In our function, we perform the expected setting of the initial user information
    in the user’s `HASH`, but we also acquire a lock around the user’s login name.
    This lock is necessary: it guarantees that we won’t have two requests trying to
    create a user with the same login at the same time. After locking, we verify that
    the login name hasn’t been taken by another user. If the name hasn’t been taken,
    we generate a new unique ID for the user, add the login name to the mapping of
    login names to user IDs, and then create the user’s `HASH`.'
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的函数中，我们执行了在用户的 `HASH` 中设置初始用户信息的预期操作，但我们还获取了用户登录名的锁。这个锁是必要的：它保证了我们不会有两个请求同时尝试创建具有相同登录名的用户。锁定后，我们验证登录名是否已被其他用户占用。如果该名称未被占用，我们将为用户生成一个新的唯一
    ID，将登录名添加到登录名到用户 ID 的映射中，然后创建用户的 `HASH`。
- en: '|  |'
  id: totrans-1403
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Sensitive user information
  id: totrans-1404
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 敏感用户信息
- en: Because the user `HASH` will be fetched countless times for rendering a template,
    or for returning directly as a response to an API request, we don’t store sensitive
    user information in this `HASH`. For now, we’ll assume that hashed passwords,
    email addresses, and more are stored at other keys, or in a different database
    entirely.
  id: totrans-1405
  prefs: []
  type: TYPE_NORMAL
  zh: 由于用户 `HASH` 将会被无数次地用于渲染模板或直接作为 API 请求的响应，我们不会在这个 `HASH` 中存储敏感用户信息。目前，我们将假设散列密码、电子邮件地址等存储在其他键或完全不同的数据库中。
- en: '|  |'
  id: totrans-1406
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We’ve finished creating the user and setting all of the necessary meta-information
    about them. From here, the next step in building our Twitter work-alike is the
    status message itself.
  id: totrans-1407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了用户的创建和设置他们所有必要的元信息。从这里开始，构建我们的类似 Twitter 的下一个步骤是状态消息本身。
- en: 8.1.2\. Status messages
  id: totrans-1408
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2\. 状态消息
- en: As we mentioned earlier, whereas user profiles store information about an individual,
    the ideas that people are trying to express are stored in status messages. As
    was the case with user information, we’ll store status message information inside
    a `HASH`.
  id: totrans-1409
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前提到的，虽然用户资料存储了关于个人的信息，但人们试图表达的想法存储在状态消息中。与用户信息一样，我们将在 `HASH` 中存储状态消息信息。
- en: In addition to the message itself, we’ll store when the status message was posted,
    the user ID and login of the user who posted it (so that if we have a status object,
    we don’t need to fetch the user object of the poster to discover their login name),
    and any additional information that should be stored about the status message.
    [Figure 8.2](#ch08fig02) shows an example status message.
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 除了消息本身之外，我们还将存储状态消息发布的时间、发布该消息的用户 ID 和登录名（这样如果我们有一个状态对象，我们就不需要获取发布者的用户对象来发现他们的登录名），以及关于状态消息应存储的任何其他信息。[图
    8.2](#ch08fig02)显示了示例状态消息。
- en: Figure 8.2\. Example status message stored in a `HASH`
  id: totrans-1411
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2\. 存储在 `HASH` 中的示例状态消息
- en: '![](08fig02.jpg)'
  id: totrans-1412
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig02.jpg)'
- en: And that’s everything necessary for a basic status message. The code to create
    such a status message can be seen in the next listing.
  id: totrans-1413
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是基本状态消息所需的所有内容。创建此类状态消息的代码可以在下一列表中看到。
- en: Listing 8.2\. How to create a status message `HASH`
  id: totrans-1414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.2\. 如何创建状态消息 `HASH`
- en: '![](188fig01_alt.jpg)'
  id: totrans-1415
  prefs: []
  type: TYPE_IMG
  zh: '![](188fig01_alt.jpg)'
- en: There isn’t anything surprising going on in the status creation function. The
    function fetches the login name of the user, gets a new ID for the status message,
    and then combines everything together and stores it as a `HASH`.
  id: totrans-1416
  prefs: []
  type: TYPE_NORMAL
  zh: 在状态创建函数中并没有什么令人惊讶的事情发生。该函数检索用户的登录名，为状态消息获取一个新的 ID，然后将所有这些内容组合在一起并存储为 `HASH`。
- en: 'We’ll talk about making the status message visible to followers in [section
    8.4](#ch08lev1sec4), so sit tight for now, as we now examine the most commonly
    used view into lists of status messages: a user’s *home timeline*.'
  id: totrans-1417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在 [第 8.4 节](#ch08lev1sec4) 中讨论使状态消息对关注者可见，所以请耐心等待，因为我们现在检查最常用的状态消息列表视图：用户的
    *主时间线*。
- en: 8.2\. Home timeline
  id: totrans-1418
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 主时间线
- en: When people visit Twitter after logging in, the first view that they see is
    what’s referred to as their *home timeline*. This is a list of status messages
    that have been posted by the user and all of the people they’re following. As
    the primary entry point to what users see, this data should be as easy to retrieve
    as possible.
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们登录 Twitter 后，他们首先看到的是所谓的 *主时间线*。这是一份由用户及其所有关注的用户发布的状态消息列表。作为用户看到的主要入口点，这些数据应该尽可能容易检索。
- en: In this section, we’ll talk about the data to be stored in the home timeline
    and how to fetch information to display the home timeline quickly. We’ll also
    talk about other important status message timelines.
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论要存储在主时间线中的数据以及如何快速检索显示主时间线的信息。我们还将讨论其他重要的状态消息时间线。
- en: As mentioned earlier in this chapter, we want to be able to fetch all of the
    data required for a given view as quickly as possible. For the home timeline,
    which will store the list of status messages that have been posted by the people
    that the current user is following, we’ll use a `ZSET` to store status IDs as
    `ZSET` members, with the timestamp of when the message was posted being used as
    the score. [Figure 8.3](#ch08fig03) shows an example home timeline.
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，我们希望尽可能快地检索给定视图所需的所有数据。对于主时间线，它将存储当前用户关注的用户发布的状态消息列表，我们将使用 `ZSET` 来存储状态
    ID 作为 `ZSET` 成员，消息发布的时间戳用作分数。[图 8.3](#ch08fig03) 展示了一个示例主时间线。
- en: Figure 8.3\. When someone visits their home timeline on a site like Twitter,
    they see the most recently posted messages that people they follow have written.
    This information is stored as a `ZSET` of status ID/timestamp pairs. Timestamp
    information provides the sort order, and the status ID shows us what information
    to pull in a second step.
  id: totrans-1422
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.3\. 当有人访问类似 Twitter 网站上的主时间线时，他们会看到他们关注的用户最近发布的消息。这些信息存储为状态 ID/时间戳对的 `ZSET`。时间戳信息提供排序顺序，状态
    ID 显示我们第二步要检索的信息。[图 8.3](#ch08fig03) 展示了一个示例主时间线。
- en: '![](08fig03.jpg)'
  id: totrans-1423
  prefs: []
  type: TYPE_IMG
  zh: '![](08fig03.jpg)'
- en: Because the home timeline is just referencing status messages—it doesn’t contain
    the status messages themselves—our function to fetch the most recently posted
    status messages must also fetch the status message data. The next listing shows
    the code to fetch a page of messages from the home timeline.
  id: totrans-1424
  prefs: []
  type: TYPE_NORMAL
  zh: 由于主时间线仅引用状态消息——它不包含状态消息本身——因此我们检索最新发布状态消息的函数也必须检索状态消息数据。下一个列表显示了从主时间线检索一页消息的代码。
- en: Listing 8.3\. A function to fetch a page of recent status messages from a timeline
  id: totrans-1425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.3\. 一个从时间线中检索一页最近状态消息的函数
- en: '![](189fig01_alt.jpg)'
  id: totrans-1426
  prefs: []
  type: TYPE_IMG
  zh: '![](189fig01_alt.jpg)'
- en: That function will fetch status messages in reverse chronological order from
    the provided timeline, which defaults to the home timeline.
  id: totrans-1427
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数将从提供的时序中按倒序时间顺序检索状态消息，默认为主时间线。
- en: A second important timeline is the timeline of posts that a user has posted.
    Where the home timeline includes posts from other people, the user’s timeline
    will include only those posts from the user. These timelines can be seen when
    visiting a user’s profile, and are the primary entry point for finding someone
    interesting. To fetch a page of statuses from a given user, we can call the same
    `get_messages()` function, but we’ll pass `profile:` as the `timeline` argument
    in our call.
  id: totrans-1428
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个重要的时间线是用户发布的状态时间线。主时间线包括其他人的帖子，而用户的时间线将仅包括用户的帖子。这些时间线可以在访问用户个人资料时看到，并且是寻找有趣人的主要入口点。要从一个特定用户那里检索一页状态，我们可以调用相同的
    `get_messages()` 函数，但在我们的调用中，我们将 `profile:` 作为 `timeline` 参数传递。
- en: Now that a user can fetch the home timeline, we should discuss how to manage
    the list of users that someone is following, and the list of users that are following
    them.
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: 现在用户可以获取主页时间线，我们应该讨论如何管理某人关注的用户列表以及关注他们的用户列表。
- en: 8.3\. Followers/following lists
  id: totrans-1430
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3. 关注者/被关注者列表
- en: One of the primary services of a platform like Twitter is for users to share
    their thoughts, ideas, and dreams with others. Following someone means that you’re
    interested in reading about what they’re saying, with the hope that others will
    want to follow you.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Twitter这样的平台的主要服务之一是让用户与他人分享他们的想法、观点和梦想。关注某人意味着你对阅读他们所说的话感兴趣，希望其他人也会想关注你。
- en: In this section, we’ll discuss how to manage the lists of users that each user
    follows, and the users that follow them. We’ll also discuss what happens to a
    user’s home timeline when they start or stop following someone.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何管理每个用户关注的用户列表以及关注他们的用户列表。我们还将讨论当用户开始或停止关注某人时，他们的主页时间线会发生什么变化。
- en: When we looked at the home and profile timelines in the last section, we stored
    status IDs and timestamps in a `ZSET`. To keep a list of followers and a list
    of those people that a user is following, we’ll also store user IDs and timestamps
    in `ZSET`s as well, with members being user IDs, and scores being the timestamp
    of when the user was followed. [Figure 8.4](#ch08fig04) shows an example of the
    followers and those that a user is following.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，当我们查看主页和简介时间线时，我们在`ZSET`中存储了状态ID和时间戳。为了保持关注者和被关注者列表，我们也将用户ID和时间戳存储在`ZSET`s中，成员是用户ID，分数是用户被关注的时间戳。[图8.4](#ch08fig04)展示了关注者和被关注者的一个示例。
- en: Figure 8.4\. To know who’s following a user, we store user ID/timestamp pairs
    in a `ZSET`. The user IDs are the people who’re following that user, and the timestamp
    is when they started following the user. Similarly, the users that a user is following
    are stored as a `ZSET` of user ID/timestamp pairs of the user ID of the person
    being followed, and the timestamp of when the user followed them.
  id: totrans-1434
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4。为了知道谁在关注一个用户，我们存储用户ID/时间戳对在`ZSET`中。用户ID是关注该用户的人，时间戳是他们开始关注用户的时间。同样，一个用户关注的用户存储为用户ID/时间戳对的`ZSET`，其中用户ID是被关注的人的用户ID，时间戳是用户开始关注他们的时间。
- en: '![](08fig04_alt.jpg)'
  id: totrans-1435
  prefs: []
  type: TYPE_IMG
  zh: '![图片](08fig04_alt.jpg)'
- en: As we start or stop following a user, there are following and followers `ZSET`s
    that need to be updated, as well as counts in the two user profile `HASH`es. After
    those `ZSET`s and `HASH`es have been updated, we then need to copy the newly followed
    user’s status message IDs from their profile timeline into our home timeline.
    This is to ensure that after we’ve followed someone, we get to see their status
    messages immediately. The next listing shows the code for following someone.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始或停止关注一个用户时，需要更新关注者和被关注者的`ZSET`s，以及两个用户个人资料`HASH`中的计数。在这些`ZSET`s和`HASH`更新后，我们需要将新关注的用户的状态消息ID从他们的个人资料时间线复制到我们的主页时间线中。这是为了确保我们在关注某人之后，能够立即看到他们的状态消息。下面的列表显示了关注某人的代码。
- en: Listing 8.4\. Update the following user’s home timeline
  id: totrans-1437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4。更新被关注者的主页时间线
- en: '![](190fig01_alt.jpg)'
  id: totrans-1438
  prefs: []
  type: TYPE_IMG
  zh: '![图片](190fig01_alt.jpg)'
- en: '|  |'
  id: totrans-1439
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Converting a list of tuples into a dictionary
  id: totrans-1440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将元组列表转换为字典
- en: As part of our `follow_user()` function, we fetched a list of status message
    IDs along with their timestamp scores. Because this is a sequence of pairs, we
    can pass them directly to the `dict()` type, which will create a dictionary of
    keys and values, as passed.
  id: totrans-1441
  prefs: []
  type: TYPE_NORMAL
  zh: 作为`follow_user()`函数的一部分，我们获取了状态消息ID及其时间戳分数。因为这是一个成对的序列，我们可以直接将它们传递给`dict()`类型，这将创建一个键值对的字典。
- en: '|  |'
  id: totrans-1442
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'This function proceeds in the way we described earlier: we add the appropriate
    user IDs to the following and followers `ZSET`s, get the size of the following
    and followers `ZSET`s, and fetch the recent status message IDs from the followed
    user’s profile timeline. After we’ve fetched all of the data, we then update counts
    inside the user profile `HASH`es, and update the following user’s home timeline.'
  id: totrans-1443
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数按照我们之前描述的方式进行：我们将适当的用户ID添加到关注者和被关注者的`ZSET`s中，获取关注者和被关注者的`ZSET`s的大小，并从被关注者的个人资料时间线中获取最近的状态消息ID。在获取所有数据后，我们更新用户个人资料`HASH`中的计数，并更新被关注者的主页时间线。
- en: 'After following someone and reading their status messages for a while, we may
    get to a point where we decide we don’t want to follow them anymore. To stop following
    someone, we perform essentially the reverse operations of what we’ve discussed:
    removing UIDs from followers and following lists, removing status messages, and
    again updating the followers/following counts. The code to stop following someone
    is shown in the following listing.'
  id: totrans-1444
  prefs: []
  type: TYPE_NORMAL
  zh: 在跟随某人并阅读他们的状态消息一段时间后，我们可能会达到一个决定不再跟随他们的点。要停止跟随某人，我们执行本质上与我们之前讨论的相反的操作：从关注者和关注列表中移除UID，移除状态消息，并再次更新关注者和关注者计数。停止跟随某人的代码在下面的列表中显示。
- en: Listing 8.5\. A function to stop following a user
  id: totrans-1445
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.5. 停止关注用户的函数
- en: '![](191fig01_alt.jpg)'
  id: totrans-1446
  prefs: []
  type: TYPE_IMG
  zh: '![](191fig01_alt.jpg)'
- en: In that function, we updated the following and followers lists, updated the
    followers and following counts, and updated the home timeline to remove status
    messages that should no longer be there. As of now, that completes all of the
    steps necessary to start and stop following a user.
  id: totrans-1447
  prefs: []
  type: TYPE_NORMAL
  zh: 在那个函数中，我们更新了关注者和关注者列表，更新了关注者和关注者计数，并更新了主页时间线以删除不再应该存在的状态消息。到目前为止，这完成了开始和停止关注用户所需的所有步骤。
- en: '|  |'
  id: totrans-1448
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Refilling timelines**'
  id: totrans-1449
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：刷新时间线**'
- en: 'When someone stops following another user, some number of status messages will
    be removed from the former follower’s home timeline. When this happens, we can
    either say that it’s okay that fewer than the desired number of status messages
    are in the timeline, or we can make an effort to add status messages from the
    other people that the user is still following. Can you write a function that will
    add status messages to the user’s timeline to keep it full? Hint: You may want
    to use tasks like we defined in [section 6.4](kindle_split_017.html#ch06lev1sec4)
    to reduce the time it takes to return from an unfollow call.'
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 当某人停止关注另一用户时，一些状态消息将从前关注者的主页时间线中删除。当这种情况发生时，我们可以说，时间线中状态消息的数量少于预期是可以接受的，或者我们可以努力添加用户仍然关注的其他人的状态消息。你能编写一个函数，将状态消息添加到用户的时间线中，以保持其完整吗？提示：你可能想使用我们在[第6.4节](kindle_split_017.html#ch06lev1sec4)中定义的任务来减少从取消关注调用返回所需的时间。
- en: '|  |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-1452
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Lists of users**'
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：用户列表**'
- en: 'In addition to the list of users that someone follows, Twitter also supports
    the ability to create additional named lists of users that include the timeline
    of posts for just those users. Can you update `follow_user()` and `unfollow_user()`
    to take an optional “list ID” for storing this new information, create functions
    to create a custom list, and fetch the custom list? Hint: Think of it like a different
    type of follower. Bonus points: can you also update your function from the “Refilling
    timelines” exercise?'
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 除了某人关注的用户列表之外，Twitter还支持创建包含仅那些用户帖子时间线的额外命名用户列表。你能更新`follow_user()`和`unfollow_user()`以接受可选的“列表ID”来存储这些新信息，创建创建自定义列表和获取自定义列表的函数吗？提示：把它想象成不同类型的关注者。加分项：你也能更新“刷新时间线”练习中的函数吗？
- en: '|  |'
  id: totrans-1455
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now that we can start or stop following a user while keeping the home timeline
    updated, it’s time to see what happens when someone posts a new status update.
  id: totrans-1456
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以在保持主页时间线更新的同时开始或停止关注用户，是时候看看当某人发布新的状态更新时会发生什么。
- en: 8.4\. Posting or deleting a status update
  id: totrans-1457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 发布或删除状态更新
- en: One of the most fundamental operations on a service like Twitter is posting
    status messages. People post to share their ideas, and people read because they’re
    interested in what’s going on with others. [Section 8.1.2](#ch08lev2sec2) showed
    how to create a status message as a prerequisite for knowing the types of data
    that we’ll be storing, but didn’t show how to get that status message into a profile
    timeline or the home timeline of the user’s followers.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: 在像Twitter这样的服务上，最基本的一项操作是发布状态消息。人们发布以分享他们的想法，人们阅读是因为他们对其他人正在发生的事情感兴趣。[第8.1.2节](#ch08lev2sec2)展示了如何创建状态消息作为了解我们将存储的数据类型的先决条件，但没有展示如何将状态消息放入个人时间线或用户关注者的主页时间线中。
- en: In this section, we’ll discuss what happens to a status message when it’s posted
    so it can find its way into the home timelines of that user’s followers. We’ll
    also talk about how to delete a status message.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论状态消息发布后会发生什么，以便它能够进入该用户关注者的主页时间线。我们还将讨论如何删除状态消息。
- en: You already know how to create the status message itself, but we now need to
    get the status message ID into the home timeline of all of our followers. How
    we should perform this operation will depend on the number of followers that the
    posting user happens to have. If the user has a relatively small number of followers
    (say, up to 1,000 or so), we can update their home timelines immediately. But
    for users with larger number of followers (like 1 million, or even the 25 million
    that some users have on Twitter), attempting to perform those insertions directly
    will take longer than is reasonable for a user to wait.
  id: totrans-1460
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经知道如何创建状态消息本身，但现在我们需要将状态消息ID放入我们所有粉丝的主时间线中。我们如何执行这个操作将取决于发布用户恰好拥有的粉丝数量。如果用户粉丝数量相对较少（比如说，最多1,000左右），我们可以立即更新他们的主时间线。但对于粉丝数量较多的用户（比如1百万，甚至有些用户在Twitter上有2,500万），直接尝试执行这些插入操作将比用户等待的时间更长。
- en: To allow for our call to return quickly, we’ll do two things. First, we’ll add
    the status ID to the home timelines of the first 1,000 followers as part of the
    call that posts the status message. Based on statistics from a site like Twitter,
    that should handle at least 99.9% of all users who post (Twitter-wide analytics
    suggest that there are roughly 100,000–250,000 users with more than 1,000 followers,
    which amounts to roughly .1% of the active user base). This means that only the
    top .1% of users will need another step.
  id: totrans-1461
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的调用快速返回，我们将做两件事。首先，我们将状态ID添加到前1,000名粉丝的主时间线中，作为发布状态消息的调用的一部分。根据类似Twitter这样的网站的统计数据，这应该可以处理至少99.9%的所有发布用户（Twitter的全局分析表明，大约有10万到25万用户拥有超过1,000名粉丝，这大约占活跃用户基础的0.1%）。这意味着只有前0.1%的用户需要额外的步骤。
- en: Second, for those users with more than 1,000 followers, we’ll start a deferred
    task using a system similar to what we built back in [section 6.4](kindle_split_017.html#ch06lev1sec4).
    The next listing shows the code for pushing status updates to followers.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，对于拥有超过1,000名粉丝的用户，我们将启动一个延迟任务，使用一个类似于我们在[第6.4节](kindle_split_017.html#ch06lev1sec4)中构建的系统。下面的列表显示了将状态更新推送到粉丝的代码。
- en: Listing 8.6\. Update a user’s profile timeline
  id: totrans-1463
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.6\. 更新用户的主时间线
- en: '![](193fig01_alt.jpg)'
  id: totrans-1464
  prefs: []
  type: TYPE_IMG
  zh: '![图片](193fig01_alt.jpg)'
- en: Notice that we broke our status updating into two parts. The first part calls
    the `create_status()` function from [listing 8.2](#ch08ex02) to actually create
    the status message, and then adds it to the poster’s profile timeline. The second
    part actually adds the status message to the timelines of the user’s followers,
    which can be seen next.
  id: totrans-1465
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将状态更新分为两部分。第一部分调用[列表8.2](#ch08ex02)中的`create_status()`函数来实际创建状态消息，并将其添加到发布者的个人资料时间线中。第二部分实际上将状态消息添加到用户的粉丝时间线中，这将在下面展示。
- en: Listing 8.7\. Update a user’s followers’ home timelines
  id: totrans-1466
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.7\. 更新用户粉丝的主时间线
- en: '![](194fig01_alt.jpg)'
  id: totrans-1467
  prefs: []
  type: TYPE_IMG
  zh: '![图片](194fig01_alt.jpg)'
- en: This second function is what actually handles pushing status messages to the
    first 1,000 followers’ home timelines, and starts a delayed task using the API
    we defined in [section 6.4](kindle_split_017.html#ch06lev1sec4) for followers
    past the first 1,000\. With those new functions, we’ve now completed the tools
    necessary to actually post a status update and send it to all of a user’s followers.
  id: totrans-1468
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个函数实际上是处理将状态消息推送到前1,000名粉丝的主时间线，并使用我们在[第6.4节](kindle_split_017.html#ch06lev1sec4)中定义的API启动一个延迟任务，用于超过前1,000名粉丝的粉丝。有了这些新函数，我们现在已经完成了发布状态更新并将其发送给所有用户粉丝所需的所有工具。
- en: '|  |'
  id: totrans-1469
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Updating lists**'
  id: totrans-1470
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：更新列表**'
- en: In the last section, I suggested an exercise to build named lists of users.
    Can you extend the `syndicate_message()` function to also support updating the
    list timelines from before?
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我建议进行一个构建用户命名列表的练习。你能扩展`syndicate_message()`函数，使其也支持更新之前的状态列表时间线吗？
- en: '|  |'
  id: totrans-1472
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s imagine that we posted a status message that we weren’t proud of; what
    would we need to do to delete it?
  id: totrans-1473
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们想象一下，我们发布了一条我们并不自豪的状态消息；我们需要做什么来删除它？
- en: It turns out that deleting a status message is pretty easy. Before returning
    the fetched status messages from a user’s home or profile timeline in `get_messages()`,
    we’re already filtering “empty” status messages with the Python `filter()` function.
    So to delete a status message, we only need to delete the status message `HASH`
    and update the number of status messages posted for the user. The function that
    deletes a status message is shown in the following listing.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 删除状态消息实际上相当简单。在`get_messages()`函数中从用户的首页或个人资料时间线返回获取到的状态消息之前，我们已经在使用Python的`filter()`函数过滤掉“空”状态消息。因此，要删除状态消息，我们只需要删除状态消息的`HASH`并更新用户发布的状态消息数量。删除状态消息的函数如下所示。
- en: Listing 8.8\. A function to delete a previously posted status message
  id: totrans-1475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.8. 删除先前发布的状态消息的函数
- en: '![](195fig01_alt.jpg)'
  id: totrans-1476
  prefs: []
  type: TYPE_IMG
  zh: '![](195fig01_alt.jpg)'
- en: While deleting the status message and updating the status count, we also went
    ahead and removed the message from the user’s home timeline and profile timeline.
    Though this isn’t technically necessary, it does allow us to keep both of those
    timelines a little cleaner without much effort.
  id: totrans-1477
  prefs: []
  type: TYPE_NORMAL
  zh: 在删除状态消息并更新状态计数的同时，我们还从用户的主时间线和个人资料时间线中移除了该消息。虽然这从技术上讲不是必需的，但它确实允许我们在不费太多力气的情况下使这两个时间线保持更干净。
- en: '|  |'
  id: totrans-1478
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Cleaning out deleted IDs**'
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：清理已删除的ID**'
- en: 'As status messages are deleted, “zombie” status message IDs will still be in
    the home timelines of all followers. Can you clean out these status IDs? Hint:
    Think about how we sent the messages out in the first place. Bonus points: also
    handle lists.'
  id: totrans-1480
  prefs: []
  type: TYPE_NORMAL
  zh: 随着状态消息被删除，"僵尸"状态消息ID仍然会出现在所有关注者的主时间线中。你能清理掉这些状态ID吗？提示：想想我们最初是如何发送这些消息的。加分项：也处理列表。
- en: '|  |'
  id: totrans-1481
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Being able to post or delete status messages more or less completes the primary
    functionality of a Twitter-like social network from a typical user’s perspective.
    But to complete the experience, you may want to consider adding a few other features:'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 能够发布或删除状态消息，从普通用户的角度来看，基本上完成了类似Twitter的社会网络的初级功能。但为了完成体验，你可能还想考虑添加一些其他功能：
- en: Private users, along with the ability to request to follow someone
  id: totrans-1483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 私人用户，以及请求关注某人的能力
- en: Favorites (keeping in mind the privacy of a tweet)
  id: totrans-1484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收藏（考虑到推文的隐私性）
- en: Direct messaging between users
  id: totrans-1485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户之间的直接消息
- en: Replying to messages resulting in conversation flow
  id: totrans-1486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对消息进行回复，从而形成对话流程
- en: Reposting/retweeting of messages
  id: totrans-1487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 消息的转发/转推
- en: 'The ability to @mention users or #tag ideas'
  id: totrans-1488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 能够@提到用户或#标签想法
- en: Keeping a record of who @mentions someone
  id: totrans-1489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记录谁@提到了某人
- en: Spam and abuse reporting and controls
  id: totrans-1490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 举报和管控垃圾邮件和滥用行为
- en: 'These additional features would help to round out the functionality of a site
    like Twitter, but may not be necessary in every situation. Expanding beyond those
    features that Twitter provides, some social networks have chosen to offer additional
    functionality that you may want to consider:'
  id: totrans-1491
  prefs: []
  type: TYPE_NORMAL
  zh: 这些附加功能将有助于完善类似Twitter这样的网站的功能，但在某些情况下可能不是必需的。在超越Twitter提供的功能之外，一些社交网络已经选择了提供额外的功能，你可能需要考虑：
- en: Liking/+1 voting status messages
  id: totrans-1492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对状态消息进行点赞/+1投票
- en: Moving status messages around the timeline depending on “importance”
  id: totrans-1493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据重要性在时间线中移动状态消息
- en: Direct messaging between a prespecified group of people (like in [section 6.5.2](kindle_split_017.html#ch06lev2sec15))
  id: totrans-1494
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预指定的用户组之间进行直接消息（如[第6.5.2节](kindle_split_017.html#ch06lev2sec15)中所述）
- en: Groups where users can post to and/or follow a group timeline (public groups,
    private groups, or even announcement-style groups)
  id: totrans-1495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以发布到和/或关注组时间线的组（公开组、私有组，甚至是公告风格的组）
- en: Now that we’ve built the last piece of the standard functional API for actually
    servicing a site like Twitter, let’s see what it’d take to build a system for
    processing streaming API requests.
  id: totrans-1496
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经构建了服务类似Twitter这样的网站的标准功能API的最后一块，让我们看看构建处理流式API请求的系统需要什么。
- en: 8.5\. Streaming API
  id: totrans-1497
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5. 流式API
- en: As development of our social network continues, at some point we’ll want to
    learn more about what’s going on—maybe to discover how many posts are made every
    hour, the most-talked-about topics, or even who’s being mentioned all the time.
    One way of doing this is to make calls to gather this information. Another way
    is to record this information inside the functions that perform all of the operations.
    The third way, which we’ll explore in this section, is to build our functions
    to broadcast simple events, which are received and processed by event listeners
    to analyze the data.
  id: totrans-1498
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的社交网络开发继续进行，在某个时候，我们可能想了解更多关于正在发生的事情——也许是为了发现每小时发布多少帖子，最热门的话题，或者甚至是谁一直在被提及。一种方法是调用以收集这些信息。另一种方法是在执行所有操作的功能中记录这些信息。第三种方法，我们将在本节中探讨，是构建我们的函数以广播简单的事件，这些事件由事件监听器接收和处理以分析数据。
- en: In this section, I’ll describe how to build the back end for a streaming API
    that functions similar to the streaming API offered by Twitter.
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将描述如何构建一个与Twitter提供的流式API功能相似的流式API的后端。
- en: Unlike the other parts of the system that we’ve already built, the streaming
    API is a different group of functionalities altogether. The functions that we
    built to support the typical operations of a site like Twitter in the last several
    sections were meant to execute and complete quickly. On the other hand, a streaming
    API request is meant to return data over a longer period of time.
  id: totrans-1500
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们已构建的系统的其他部分不同，流式API是完全不同的功能组。我们在上一节中构建的函数旨在支持像Twitter这样的网站典型操作的目的是快速执行和完成。另一方面，流式API请求的目的是在更长的时间内返回数据。
- en: Most modern social networks offer the ability to gather information from their
    system via some sort of API. One advantage that Twitter has shown over the last
    several years is that by offering real-time events to third parties, those third
    parties can develop unique and interesting analyses of the data that Twitter itself
    may not have had the time or interest to develop.
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现代社交网络都提供通过某种形式的API从其系统中收集信息的能力。过去几年中，Twitter显示出的一个优势是，通过向第三方提供实时事件，这些第三方可以开发出独特的、有趣的数据分析，而这些数据可能是Twitter本身没有时间或兴趣去开发的。
- en: The first step in building a streaming API is understanding what kind of data
    we’ll be processing and producing.
  id: totrans-1502
  prefs: []
  type: TYPE_NORMAL
  zh: 构建流式API的第一步是了解我们将要处理和生成哪些数据。
- en: 8.5.1\. Data to be streamed
  id: totrans-1503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.1\. 要流式传输的数据
- en: As people perform a variety of actions within our social network, those actions
    are seen at the various functions that defined our API. In particular, we spent
    most of our time building out the ability to follow/unfollow users, and post/delete
    messages. If we’d built other pieces of our social network, we’d also find a variety
    of other *events* that occur as the result of user behavior. A streaming API is
    meant to produce a sequence of these events over time as a way of keeping clients
    or other services updated about a subset of what’s going on across the entire
    network.
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 当人们在我们的社交网络中执行各种操作时，这些操作会在我们API定义的各种功能中被看到。特别是，我们花了大部分时间来构建关注/取消关注用户和发布/删除消息的能力。如果我们构建了社交网络的其他部分，我们也会发现由于用户行为而产生的各种其他*事件*。流式API的目的是随着时间的推移产生这些事件的序列，以便让客户端或其他服务了解整个网络中发生的一部分情况。
- en: 'In the process of building a streaming API, a variety of decisions must be
    made, which can be generally reduced to three major questions:'
  id: totrans-1505
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建流式API的过程中，必须做出各种决定，这些决定可以概括为三个主要问题：
- en: Which events should be exposed?
  id: totrans-1506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该公开哪些事件？
- en: What access restrictions (if any) should exist?
  id: totrans-1507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该存在哪些访问限制（如果有）？
- en: What kinds of filtering options should be provided?
  id: totrans-1508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该提供哪些过滤选项？
- en: For now, I won’t answer the second question about access restrictions. That’s
    a question that we need to answer when we’re building our social network based
    on expectations of privacy and system resources. But I’ll answer the other two
    questions.
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，我不会回答关于访问限制的第二个问题。这是一个我们需要在基于隐私和系统资源预期构建我们的社交网络时回答的问题。但我将回答其他两个问题。
- en: Because we focused on posting/deleting messages and following/unfollowing users,
    we should offer at least some of those events. To keep things simple for now,
    we’ll only produce message posting and deletion events. But based on the structures
    that we create and pass around, adding functionality to support follow/unfollow
    events or events for other actions that we’ve added should be easy.
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们专注于发布/删除消息和关注/取消关注用户，我们应该至少提供其中的一些事件。为了保持简单，我们现在只产生消息发布和删除事件。但基于我们创建和传递的结构，添加支持关注/取消关注事件或其他我们添加的动作的事件的功能应该是容易的。
- en: The types of filtering options that we’ll provide will overlap significantly
    with the API features and functionality that Twitter provides on the public side
    of things. In particular, we’ll offer the ability to filter over messages with
    an equivalent of follow (users), track (keywords), and location filters, in addition
    to a randomly selected subset of messages, similar to Twitter’s *firehose* and
    *sample* streams.
  id: totrans-1511
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将提供的过滤选项类型将与Twitter在公共方面提供的API功能和特性有显著的重叠。特别是，我们将提供过滤消息的能力，这些能力包括与关注（用户）、跟踪（关键词）和位置过滤等效的功能，以及类似于Twitter的*firehose*和*sample*流的一个随机选择的子集消息。
- en: Now that we know what data we’ll have access to, let’s start looking at how
    we’ll serve the data.
  id: totrans-1512
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们知道我们将能够访问哪些数据，让我们开始看看我们将如何提供这些数据。
- en: 8.5.2\. Serving the data
  id: totrans-1513
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.2\. 提供数据服务
- en: In preceding sections and chapters, when we showed functions that made calls
    to Redis, we built on the assumption that we had an existing web server that would
    be calling these functions at just the right time. In the case of a streaming
    API, the details of streaming data to a client can be more complicated than just
    plugging these functions into an existing web service stack. In particular, most
    web servers operate under the assumption that we’ll be returning the entire response
    to a request at once, but this is definitely not the case with a streaming API.
  id: totrans-1514
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，当我们展示了调用Redis的函数时，我们基于这样一个假设：我们有一个现有的网络服务器，它将在正确的时间调用这些函数。在流式API的情况下，将流式数据传输到客户端的细节可能比将这些函数插入现有的网络服务堆栈要复杂得多。特别是，大多数网络服务器假设我们将一次性返回请求的整个响应，但这与流式API的情况肯定不同。
- en: Responses from a streaming API are received status message by status message
    as they’re produced and matched. Though modern technologies like WebSockets and
    SPDY can offer incremental data production, or even server-side push messages,
    the protocols involved are still in the process of being finalized, and client-side
    support in many programming languages is incomplete. But there is a method of
    producing incremental content with an HTTP server—sending data using the *chunked*
    transfer encoding.
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: 从流式API收到的响应是按状态消息逐个产生和匹配的。尽管现代技术如WebSockets和SPDY可以提供增量数据生产，甚至服务器端推送消息，但涉及的协议仍在最终确定过程中，许多编程语言中的客户端支持尚不完整。但是，有一种方法可以在HTTP服务器上产生增量内容——使用*分块*传输编码发送数据。
- en: In this section, we’ll build a simple web server that supports streaming to
    clients that can handle chunked HTTP responses. This is to support our later sections
    which will actually implement filtering options for streamed message data.
  id: totrans-1516
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将构建一个简单的网络服务器，它支持向能够处理分块HTTP响应的客户端进行流式传输。这是为了支持我们后面的章节，这些章节将实际实现流式消息数据的过滤选项。
- en: To build this streaming HTTP web server, we have to delve deeper into the Python
    programming language. In the past, we’ve attempted to keep everything to standard
    functions, and in [chapter 6](kindle_split_017.html#ch06), we even started using
    generators (that was the code that included `yield`). But here, we’ll have to
    use Python classes. This is primarily because we don’t want to have to build an
    entire web server from scratch, and Python already includes servers that we can
    mix together to handle all of the difficult parts of web serving. If you’ve used
    classes in other languages, you’ll be comfortable with Python, because classes
    in Python are similar. They’re meant to encapsulate data, with methods to manipulate
    the data. In our case, most of the functionality that we want to use is already
    available in existing libraries; we just need to plug them together.
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建这个流式 HTTP 网络服务器，我们必须更深入地了解 Python 编程语言。在过去，我们试图将所有内容都保持为标准函数，甚至在 [第 6 章](kindle_split_017.html#ch06)
    中，我们甚至开始使用生成器（那是在代码中包含 `yield` 的部分）。但在这里，我们必须使用 Python 类。这主要是因为我们不希望从头开始构建整个网络服务器，Python
    已经包含了我们可以混合使用来处理网络服务所有困难部分的服务器。如果你在其他语言中使用过类，你将感到 Python 很舒适，因为 Python 中的类很相似。它们旨在封装数据，并提供操作数据的方法。在我们的情况下，我们想要使用的功能的大部分已经存在于现有的库中；我们只需要将它们连接起来。
- en: A streaming HTTP server
  id: totrans-1518
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 流式 HTTP 服务器
- en: Within Python we have a series of socket server libraries that can be mixed
    together to offer varying types of functionality. To start, we’ll create a server
    that uses threads in order to process each incoming request separately. When the
    server receives a request, the server will create a thread to execute a request
    handler. This request handler is where we’ll perform some initial basic routing
    for `GET` and `POST` HTTP requests. Both the threaded server and the request handler
    are shown in the next listing.
  id: totrans-1519
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Python 中，我们有一系列可以混合使用的套接字服务器库，可以提供不同类型的功能。首先，我们将创建一个使用线程来分别处理每个传入请求的服务器。当服务器收到请求时，服务器将创建一个线程来执行请求处理器。这个请求处理器是我们将执行一些初始基本路由的地方，用于
    `GET` 和 `POST` HTTP 请求。接下来的列表中展示了线程服务器和请求处理器。
- en: Listing 8.9\. Server and request handler for our streaming HTTP server
  id: totrans-1520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.9\. 我们流式 HTTP 服务器的服务器和请求处理器
- en: '![](198fig01_alt.jpg)'
  id: totrans-1521
  prefs: []
  type: TYPE_IMG
  zh: '![图片](198fig01_alt.jpg)'
- en: 'What we didn’t write is the code that actually starts up the server, but we’ll
    get to that in a moment. For now, you can see that we defined a server that created
    threads on each request. Those threads execute methods on a request handler object,
    which eventually lead to either `do_GET()` or `do_POST()`, which handle the two
    major types of streaming API requests: filtered and sampled.'
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有编写启动服务器的代码，但稍后我们会涉及到这一点。现在，你可以看到我们定义了一个在每次请求时创建线程的服务器。这些线程在请求处理器对象上执行方法，最终导致执行
    `do_GET()` 或 `do_POST()`，这两个方法处理两种主要的流式 API 请求类型：过滤和采样。
- en: To actually run this server, we’ll use a bit of Python magic. This magic allows
    us to later import a module to use these predefined classes, or it allows us to
    run the module directly in order to start up a streaming API server. The code
    that lets us both import the module and run it as a daemon can be seen in the
    next listing.
  id: totrans-1523
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实际运行这个服务器，我们将使用一些 Python 魔法。这种魔法允许我们稍后导入一个模块来使用这些预定义的类，或者它允许我们直接运行模块以启动流式
    API 服务器。让我们可以导入模块并以守护进程方式运行的代码可以在下一个列表中看到。
- en: Before you put these two blocks of code into a file and run them, remember that
    we’re still missing two functions that are called as part of the streaming API
    server, `parse_identifier()` and `process_filters()`, which we’ll cover next.
  id: totrans-1524
  prefs: []
  type: TYPE_NORMAL
  zh: 在你将这些两块代码放入文件并运行之前，请记住我们仍然缺少两个作为流式 API 服务器一部分调用的函数，`parse_identifier()` 和 `process_filters()`，我们将在下一部分介绍。
- en: Listing 8.10\. The code to actually start and run the streaming HTTP server
  id: totrans-1525
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.10\. 实际启动和运行流式 HTTP 服务器的代码
- en: '![](199fig01_alt.jpg)'
  id: totrans-1526
  prefs: []
  type: TYPE_IMG
  zh: '![图片](199fig01_alt.jpg)'
- en: Identifying the client
  id: totrans-1527
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 识别客户端
- en: The first of these two functions is a way of fetching identifying information
    about the client. This basic method extracts an identifier from the request query
    arguments. For a production scenario, we’d want to perform some amount of client
    validation of the identifier. Our simple method to parse an identifier from the
    request can be seen in the next listing.
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个函数中的第一个是一种获取客户端识别信息的方法。这个基本方法从请求查询参数中提取一个标识符。对于生产场景，我们可能想要对标识符进行一些客户端验证。我们解析请求中标识符的简单方法可以在下一个列表中看到。
- en: Listing 8.11\. An example function to parse and store the client identifier
  id: totrans-1529
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.11\. 解析和存储客户端标识符的示例函数
- en: '![](199fig02_alt.jpg)'
  id: totrans-1530
  prefs: []
  type: TYPE_IMG
  zh: '![图片199fig02_alt.jpg](199fig02_alt.jpg)'
- en: That function shouldn’t do anything surprising; we set some initial values for
    the query arguments (if we want to use them later) and the identifier, parse the
    query arguments, and then store the identifier from the query if it was available.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数不应该有任何令人惊讶的操作；我们为查询参数（如果我们以后想使用它们）和标识符设置了一些初始值，解析查询参数，然后如果可用，从查询中存储标识符。
- en: Handling HTTP streaming
  id: totrans-1532
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理HTTP流
- en: There’s one final piece to the HTTP server portion of our request—actually sending
    the filtered responses. To prepare to send these filtered messages one by one,
    we first need to verify the requests are valid. Assuming that everything is okay,
    we must then send to the client the notification that we’ll be entering an HTTP
    mode called *chunked transfer encoding*, which will allow us to send messages
    one at a time as they come in. The function that performs this validation and
    the actual transfer of streamed messages to the client is shown next.
  id: totrans-1533
  prefs: []
  type: TYPE_NORMAL
  zh: 我们请求的HTTP服务器部分还有一个最后的环节——实际上是发送过滤后的响应。为了准备逐个发送这些过滤消息，我们首先需要验证请求是否有效。假设一切正常，我们必须然后向客户端发送通知，我们将进入一个名为*分块传输编码*的HTTP模式，这将允许我们逐个发送消息，就像它们到来时一样。执行此验证并将流式消息实际传输到客户端的函数将在下面展示。
- en: Listing 8.12\. A function that will verify the request and stream data to the
    client
  id: totrans-1534
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.12\. 验证请求并将数据流式传输到客户端的函数
- en: '![](ch08ex12-0.jpg)'
  id: totrans-1535
  prefs: []
  type: TYPE_IMG
  zh: '![图片ch08ex12-0.jpg](ch08ex12-0.jpg)'
- en: '![](ch08ex12-1.jpg)'
  id: totrans-1536
  prefs: []
  type: TYPE_IMG
  zh: '![图片ch08ex12-1.jpg](ch08ex12-1.jpg)'
- en: A few details in this function are tricky, but the basic idea is that we make
    sure that we have an identifier for the client and fetch the filtering arguments
    for the specific calls. If everything is okay, we then announce to the client
    that we’ll be streaming responses and pass the actual filtering off to a generator,
    which will produce the sequence of messages that match the filter criteria.
  id: totrans-1537
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数中的一些细节比较棘手，但基本思路是我们确保有一个标识符用于客户端，并获取特定调用所需的过滤参数。如果一切正常，我们随后会通知客户端我们将进行响应流传输，并将实际的过滤工作交给一个生成器，该生成器将产生符合过滤标准的消息序列。
- en: And that’s it for the streaming HTTP server. In the next section, we’ll build
    the methods that will filter messages that pass through the system.
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，流式HTTP服务器就完成了。在下一节中，我们将构建过滤通过系统的消息的方法。
- en: 8.5.3\. Filtering streamed messages
  id: totrans-1539
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.5.3\. 过滤流式消息
- en: So far we’ve built a server to serve the streamed messages; now it’s time to
    filter through the messages for streaming. We filter the messages so that a client
    making a request only sees the messages they’re interested in. Though our social
    network may not have a lot of traffic, sites like Twitter, Facebook, or even Google+
    will see tens to hundreds of thousands of events every second. And for both third
    parties and ourselves, the cost of bandwidth to send all of that information can
    be quite high, so only sending messages that match up is important.
  id: totrans-1540
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了一个服务器来提供流式消息；现在，我们需要对消息进行过滤以进行流传输。我们过滤消息，使得发出请求的客户端只能看到他们感兴趣的消息。尽管我们的社交网络可能没有太多的流量，但像Twitter、Facebook甚至Google+这样的网站每秒都会看到成千上万的事件。对于第三方和我们自己来说，发送所有这些信息的带宽成本可能相当高，因此只发送匹配的消息非常重要。
- en: In this section, we’ll write functions and classes that will filter posted messages
    to be streamed to clients. These filters will plug into the streaming web server
    we wrote in [section 8.5.2](#ch08lev2sec4). As I mentioned at the beginning of
    [section 8.5](#ch08lev1sec5), we’ll support random sampling of all messages and
    access to the full firehose, as well as filtering for specific users, words, and
    the location of messages.
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将编写函数和类，以过滤要流式传输到客户端的发布消息。这些过滤器将连接到我们在[8.5.2节](#ch08lev2sec4)中编写的流式Web服务器。正如我在[8.5节](#ch08lev1sec5)开头提到的，我们将支持对所有消息的随机抽样和访问完整的数据流，以及针对特定用户、单词和消息位置的过滤。
- en: As mentioned way back in [chapter 3](kindle_split_014.html#ch03), we’ll use
    Redis `PUBLISH` and `SUBSCRIBE` to implement at least part of the streaming functionality.
    More specifically, when users post messages, we’ll `PUBLISH` the posted message
    information to a channel in Redis. Our filters will `SUBSCRIBE` to that same channel,
    receive the message, and yield messages that match the filters back to the web
    server for sending to the client.
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 如[第3章](kindle_split_014.html#ch03)中提到的，我们将使用Redis的`PUBLISH`和`SUBSCRIBE`来实现至少部分流式功能。更具体地说，当用户发布消息时，我们将`PUBLISH`发布消息信息到Redis中的通道。我们的过滤器将`SUBSCRIBE`到该通道，接收消息，并将匹配过滤器的消息返回给Web服务器以发送给客户端。
- en: Updating status message posting and deletion
  id: totrans-1543
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 更新状态消息的发布和删除
- en: Before we get ahead of ourselves, let’s first update our message posting function
    from [section 8.1.2](#ch08lev2sec2) and message deletion function from [section
    8.4](#ch08lev1sec4) to start producing messages to filter. We’ll start with posting
    in the next listing, which shows that we’ve added a line to our function that
    sends messages out to be filtered.
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续前进之前，让我们首先更新我们的消息发布函数[第8.1.2节](#ch08lev2sec2)和消息删除函数[第8.4节](#ch08lev1sec4)，以便开始生成过滤消息。我们将在下一个列表中开始发布，显示我们已经添加了一行代码，该代码将消息发送出去进行过滤。
- en: Listing 8.13\. Updated `create_status()` from [listing 8.2](#ch08ex02) to support
    streaming filters
  id: totrans-1545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.13。更新`create_status()`以支持流式过滤器，来自[列表8.2](#ch08ex02)
- en: '![](201fig01_alt.jpg)'
  id: totrans-1546
  prefs: []
  type: TYPE_IMG
  zh: '![图片](201fig01_alt.jpg)'
- en: All it took was one more line to add streaming support on the posting side.
    But what about deletion? The update to status message deletion is shown in the
    following listing.
  id: totrans-1547
  prefs: []
  type: TYPE_NORMAL
  zh: 只需再添加一行代码就可以在发布端添加流式支持。但删除怎么办？状态消息删除的更新如下所示。
- en: Listing 8.14\. Updated `delete_status()` from [listing 8.8](#ch08ex08) to support
    streaming filters
  id: totrans-1548
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.14。更新`delete_status()`以支持流式过滤器，来自[列表8.8](#ch08ex08)
- en: '![](201fig02_alt.jpg)'
  id: totrans-1549
  prefs: []
  type: TYPE_IMG
  zh: '![图片](201fig02_alt.jpg)'
- en: At first glance, you’re probably wondering why we’d want to send the entire
    status message that’s to be deleted to the channel for filtering. Conceptually,
    we should only need to send message-deleted information to clients that received
    the status message when it was posted. If we perform the same filtering on deleted
    messages as we do on newly posted messages, then we can always send message-deleted
    notifications to those clients that would’ve received the original message. This
    ensures that we don’t need to keep a record of the status IDs for messages sent
    to all clients, which simplifies our server and reduces memory use.
  id: totrans-1550
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，你可能想知道为什么我们想要将整个要删除的状态消息发送到通道进行过滤。从概念上讲，我们只需要将消息删除信息发送给在消息发布时收到状态消息的客户端。如果我们对删除消息执行与对新发布消息相同的过滤，那么我们可以始终向那些原本会收到原始消息的客户端发送消息删除通知。这确保了我们不需要保留发送给所有客户端的消息状态ID记录，从而简化了我们的服务器并减少了内存使用。
- en: Receiving streamed messages for filtering
  id: totrans-1551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 接收用于过滤的流式消息
- en: Now that we’re sending information about status messages being posted and deleted
    to a channel in Redis, we only need to subscribe to that channel to start receiving
    messages to filter. As was the case in [chapter 3](kindle_split_014.html#ch03),
    we’ll need to construct a special `pubsub` object in order to subscribe to a channel.
    When we’ve subscribed to the channel, we’ll perform our filtering, and produce
    one of two different messages depending on whether the message was posted or deleted.
    The code for handling these operations is next.
  id: totrans-1552
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将状态消息的发布和删除信息发送到Redis中的通道，我们只需要订阅该通道即可开始接收过滤消息。正如[第3章](kindle_split_014.html#ch03)中所述，我们需要构建一个特殊的`pubsub`对象来订阅通道。当我们订阅了通道后，我们将执行过滤操作，并根据消息是发布还是删除产生两种不同的消息。处理这些操作的代码如下。
- en: Listing 8.15\. A function to receive and process streamed messages
  id: totrans-1553
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.15。接收和处理流式消息的函数
- en: '![](202fig01_alt.jpg)'
  id: totrans-1554
  prefs: []
  type: TYPE_IMG
  zh: '![图片](202fig01_alt.jpg)'
- en: As I said before, this function needs to subscribe to a channel in Redis in
    order to receive posted/deleted notifications for status messages. But it also
    needs to handle cases where the streaming client has disconnected, and it needs
    to properly clean up the connection if Redis has been trying to send it too much
    data.
  id: totrans-1555
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所说，这个函数需要订阅Redis中的通道以接收状态消息的发布/删除通知。但它还需要处理流式客户端断开连接的情况，并且如果Redis尝试发送太多数据，它需要正确清理连接。
- en: As we covered in [chapter 3](kindle_split_014.html#ch03), there’s a Redis server
    setting to determine the maximum outgoing buffer for subscriptions to support.
    To ensure that our Redis server stays up even under heavy load, we’ll probably
    want to set `client-output-buffer-limit pubsub` to lower than the default 32 megabytes
    per connection. Where to set the limit will depend on how many clients we expect
    to support and how much other data is in Redis.
  id: totrans-1556
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第3章](kindle_split_014.html#ch03)中所述，Redis服务器有一个设置可以确定支持订阅的最大输出缓冲区。为了确保我们的Redis服务器即使在重负载下也能正常运行，我们可能需要将`client-output-buffer-limit
    pubsub`设置得低于默认的每个连接32兆字节。设置限制的位置将取决于我们期望支持多少客户端以及Redis中还有多少其他数据。
- en: Filtering messages
  id: totrans-1557
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 消息过滤
- en: At this point we’ve built every other layer; it now remains to actually write
    filtering. I know, there was a lot of build-up, but you may be surprised to find
    out that actually filtering messages isn’t difficult for any of our cases. To
    create filters, we’ll first define our `create_filters()` function in [listing
    8.16](#ch08ex16), which will delegate off to one of a variety of filtering classes,
    depending on the filter that we want to build. We’ll assume that clients are sending
    reasonable arguments, but if you’re considering using any of this in a production
    scenario, you’ll want to add validation and verification.
  id: totrans-1558
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经构建了其他所有层；现在剩下的就是实际编写过滤代码了。我知道，之前的准备工作很多，但你可能会惊讶地发现，实际上过滤消息在我们的任何情况下都不困难。为了创建过滤器，我们首先在[列表8.16](#ch08ex16)中定义我们的`create_filters()`函数，它将根据我们想要构建的过滤器将任务委托给多种过滤类之一。我们假设客户端发送的是合理的参数，但如果你考虑在生产环境中使用任何这些，你将想要添加验证和验证。
- en: Listing 8.16\. A factory function to dispatch to the actual filter creation
  id: totrans-1559
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.16\. 一个用于实际过滤器创建的分派工厂函数
- en: '![](203fig01_alt.jpg)'
  id: totrans-1560
  prefs: []
  type: TYPE_IMG
  zh: '![图片](203fig01_alt.jpg)'
- en: 'Nothing surprising there: we’re distinguishing the different kinds of filters.
    The first filter we’ll create will be the `sample` filter, which will actually
    implement the functionality of the Twitter-style `firehose`, `gardenhose`, and
    `spritzer` access levels, and anything in between. The implementation of the sampling
    filter is shown next.'
  id: totrans-1561
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么令人惊讶的：我们正在区分不同的过滤器类型。我们将创建的第一个过滤器将是`sample`过滤器，它实际上将实现Twitter风格的`firehose`、`gardenhose`和`spritzer`访问级别的功能，以及介于它们之间的任何功能。采样过滤器的实现将在下面展示。
- en: Listing 8.17\. The function to handle `firehose`, `gardenhose`, and `spritzer`
  id: totrans-1562
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.17\. 处理`firehose`、`gardenhose`和`spritzer`的函数
- en: '![](204fig01_alt.jpg)'
  id: totrans-1563
  prefs: []
  type: TYPE_IMG
  zh: '![图片](204fig01_alt.jpg)'
- en: As you can see, we started using classes again, primarily because we need to
    encapsulate data and behavior together. This first class that defines sampling
    does one interesting thing—it uses a random number generator seeded with the user-provided
    identifier to choose the IDs of status messages that it should accept. This allows
    the sampling filters to receive a `deleted` notification for a message, even if
    the client had disconnected (as long as the client reconnected before the delete
    notification came through). We use Python sets here to quickly determine whether
    the ID modulo 100 is in the group that we want to accept, as Python sets offer
    O(1) lookup time, compared to O(n) for a Python list.
  id: totrans-1564
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们又开始使用类了，主要是因为我们需要将数据和行为封装在一起。这个定义采样的第一个类做了一件事——它使用一个由用户提供的标识符随机数生成器来选择它应该接受的状态消息的ID。这允许采样过滤器接收一个消息的`deleted`通知，即使客户端已经断开连接（只要客户端在删除通知到来之前重新连接）。我们在这里使用Python集合来快速确定ID除以100的余数是否在我们想要接受的那组中，因为Python集合提供了O(1)的查找时间，而Python列表是O(n)。
- en: Continuing on, we’ll now build the `track` filter, which will allow users to
    track words or phrases in status messages. Similar to our sample filter in [listing
    8.17](#ch08ex17), we’ll use a class to encapsulate the data and filtering functionality
    together. The filter class definition is shown in the following listing.
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 继续进行，我们现在将构建`track`过滤器，它将允许用户跟踪状态消息中的单词或短语。类似于我们的[列表8.17](#ch08ex17)中的采样过滤器，我们将使用一个类来封装数据和过滤功能。过滤器类的定义在下面的列表中展示。
- en: Listing 8.18\. A filter that matches groups of words that are posted in status
    messages
  id: totrans-1566
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.18\. 匹配在状态消息中发布的单词组的过滤器
- en: '![](204fig02_alt.jpg)'
  id: totrans-1567
  prefs: []
  type: TYPE_IMG
  zh: '![图片](204fig02_alt.jpg)'
- en: About the only interesting thing about the tracking filter is to make sure that
    if someone wants to match a group of words, the filter matches *all* of the words
    in the message and not just *some* of them. We again use Python sets, which, like
    Redis `SET`s, offer the ability to calculate intersections.
  id: totrans-1568
  prefs: []
  type: TYPE_NORMAL
  zh: 关于跟踪过滤器唯一有趣的事情是确保如果有人想要匹配一组单词，过滤器将匹配消息中的*所有*单词，而不仅仅是*一些*单词。我们再次使用Python集合，就像Redis
    `SET`s一样，提供了计算交集的能力。
- en: Moving on to the `follow` filter, we’re trying to match status messages that
    were posted by one of a group of users, or where one of the users is mentioned
    in the message. The class that implements user matching is shown here.
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是 `follow` 过滤器，我们试图匹配由一组用户中的一位用户发布的或消息中提及了其中一位用户的状态消息。实现用户匹配的类在此处展示。
- en: Listing 8.19\. Messages posted by or mentioning any one of a list of users
  id: totrans-1570
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.19\. 由用户列表中的任何一位用户发布的或提及的消息
- en: '![](205fig01_alt.jpg)'
  id: totrans-1571
  prefs: []
  type: TYPE_IMG
  zh: '![图片](205fig01_alt.jpg)'
- en: As before, we continue to use Python sets as a fast way to check whether a name
    is in the set of names that we’re looking for, or whether any of the names to
    match are also contained in a status message.
  id: totrans-1572
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们继续使用Python集合作为快速检查一个名称是否在我们寻找的名称集合中，或者是否任何要匹配的名称也包含在状态消息中的方法。
- en: We finally get to the location filter. This filter is different from the others
    in that we didn’t explicitly talk about adding location information to our status
    messages. But because of the way we wrote our `create_status()` and `post_status()`
    functions to take additional optional keyword arguments, we can add additional
    information without altering our status creation and posting functions. The location
    filter for this optional data is shown next.
  id: totrans-1573
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终到达了位置过滤器。这个过滤器与其他过滤器不同之处在于我们没有明确讨论将位置信息添加到我们的状态消息中。但由于我们编写了 `create_status()`
    和 `post_status()` 函数以接受额外的可选关键字参数，我们可以在不更改状态创建和发布函数的情况下添加更多信息。此可选数据的过滤器如下所示。
- en: Listing 8.20\. Messages within boxes defined by ranges of latitudes and longitudes
  id: totrans-1574
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.20\. 由经纬度范围定义的框内的消息
- en: '![](205fig02_alt.jpg)'
  id: totrans-1575
  prefs: []
  type: TYPE_IMG
  zh: '![图片](205fig02_alt.jpg)'
- en: About the only thing that may surprise you about this particular filter is how
    we’re preparing the boxes for filtering. We expect that requests will provide
    location boxes as comma-separated sequences of numbers, where each chunk of four
    numbers defines latitude and longitude ranges (minimum longitude, minimum latitude,
    maximum longitude, maximum latitude—the same order as Twitter’s API).
  id: totrans-1576
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个特定过滤器可能让您感到惊讶的唯一事情可能就是我们在准备用于过滤的框的方式。我们预计请求将提供以逗号分隔的数字序列作为位置框，其中每个四数字块定义了纬度和经度范围（最小经度、最小纬度、最大经度、最大纬度——顺序与Twitter的API相同）。
- en: With all of our filters built, a working web server, and the back-end API for
    everything else, it’s now up to you to get traffic!
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建了所有过滤器、一个工作的Web服务器以及所有其他后端API之后，现在轮到您来获取流量了！
- en: 8.6\. Summary
  id: totrans-1578
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.6\. 摘要
- en: In this chapter, we’ve built the majority of functionality that makes a site
    like Twitter work. Though these structures won’t scale to the extent that Twitter
    does, the methods used can be used to build a small social network easily. With
    a front end for users to interact with, you can start your own social network
    with your friends!
  id: totrans-1579
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们构建了使像Twitter这样的网站工作的主要功能。尽管这些结构不会扩展到Twitter的程度，但使用的方法可以轻松构建一个小型社交网络。有了用户与之交互的前端，您可以用您的朋友开始自己的社交网络！
- en: If there’s one thing that you should take away from this chapter, it’s that
    even immensely popular websites have functionality that can be built with the
    tools available inside of Redis.
  id: totrans-1580
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您应该从本章中带走一件事，那就是即使是极其流行的网站也有可以用Redis内部可用的工具构建的功能。
- en: In the upcoming [chapters 9](kindle_split_021.html#ch09) through [11](kindle_split_023.html#ch11),
    we’ll look into methods to help reduce memory use, methods to help scaling Redis
    read and write loads, and scripting Redis to simplify (and sometimes help scale)
    applications. These things will help to scale Redis applications, like our social
    network, beyond expected single-machine limits. Our first step down this path
    is [chapter 9](kindle_split_021.html#ch09), where I’ll show you how to reduce
    Redis’s memory use.
  id: totrans-1581
  prefs: []
  type: TYPE_NORMAL
  zh: 在即将到来的第9章（kindle_split_021.html#ch09）到第11章（kindle_split_023.html#ch11）中，我们将探讨帮助减少内存使用的方法、帮助扩展Redis读写负载的方法以及将Redis脚本化以简化（有时帮助扩展）应用程序的方法。这些事情将帮助扩展Redis应用程序，如我们的社交网络，超越预期的单机限制。我们沿着这条道路的第一步是第9章（kindle_split_021.html#ch09），在那里我将向您展示如何减少Redis的内存使用。
