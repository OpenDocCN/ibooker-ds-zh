- en: 6 Memory hierarchy, storage, and networking
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 内存层次结构、存储和网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Making efficient use of CPU cache and main memory
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有效利用CPU缓存和主内存
- en: Using Blosc to access compressed array data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Blosc访问压缩数组数据
- en: Using NumExpr to accelerate NumPy expressions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用NumExpr加速NumPy表达式
- en: Designing client/server architectures for very fast networks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为非常快速的网络设计客户端/服务器架构
- en: It goes without saying that hardware affects performance. But how hardware interacts
    with performance is not always so obvious. The goal of this chapter is to help
    you get a better grasp of how, exactly, your machinery can affect your speed and
    what you can do on the hardware end to improve performance. To that end, we will
    take a close look at the effects of modern hardware and network architectures
    on efficient data processing with Python.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 众所周知，硬件会影响性能。但硬件如何与性能互动并不总是那么明显。本章的目标是帮助您更好地理解您的机器如何影响您的速度，以及您可以在硬件端做些什么来提高性能。为此，我们将仔细研究现代硬件和网络架构对Python高效数据处理的影响。
- en: There are many counterintuitive implications for software development stemming
    from hardware considerations. For example, there are quite a few cases where working
    with compressed data is *faster* than dealing with uncompressed data. Conventional
    wisdom suggests that the cost of both decompressing and analyzing data would be
    much more expensive than just analyzing data. After all, when we decompress, we
    are adding more computations. So how can this be computationally more efficient?
    It turns out that modern hardware architectures can play tricks with “obvious”
    observations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于硬件考虑，软件开发中存在许多反直觉的后果。例如，有相当多的情况，处理压缩数据比处理未压缩数据要快。传统观点认为，解压缩和分析数据的花费将远远高于仅分析数据。毕竟，当我们解压缩时，我们是在增加更多的计算。那么，如何才能在计算上更有效率呢？事实证明，现代硬件架构可以对“明显”的观察进行一些小把戏。
- en: 'To make the most of modern hardware performance, we need to understand what
    makes some default assumptions so counterintuitive. To gain this understanding,
    we will start with a introduction to modern computer architectures from a performance
    perspective. The topic itself would merit a book, but we will concentrate on the
    less intuitive features: we will study memory hierarchy, from CPU caches to wide
    area networks, going through RAM, hard disks, and local networks.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 为了充分利用现代硬件的性能，我们需要理解是什么使得一些默认假设如此反直觉。为了获得这种理解，我们将从性能角度出发，对现代计算机架构进行介绍。这个主题本身就可以写成一本书，但我们将专注于不那么直观的特性：我们将研究内存层次结构，从CPU缓存到广域网，经过RAM、硬盘和本地网络。
- en: We are interested in making computing faster and storage processing more efficient,
    both from a size and a speed perspective. After we understand some of the implications
    of modern hardware architectures, we will see how some Python libraries make the
    most of hardware. We’ll first explore how Blosc, a high-performance library to
    compress binary data, can be used to generate compact representations of NumPy
    arrays that can be accessed in about the same time as uncompressed arrays. As
    we break down this process, you will see how intelligent use of CPU caches can
    make the compression and decompression times virtually irrelevant. Then, we will
    look at how NumExpr can accelerate NumPy expressions over very large arrays by,
    again, being intelligent about processing data in a cache-minded way.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对使计算更快、存储处理更高效感兴趣，从大小和速度两个角度来看。在理解了现代硬件架构的一些影响之后，我们将看到一些Python库如何充分利用硬件。首先，我们将探索Blosc，一个用于压缩二进制数据的高性能库，如何被用来生成与未压缩数组访问时间大致相同的紧凑表示的NumPy数组。在我们分解这个过程时，您将看到智能地使用CPU缓存可以使压缩和解压缩时间几乎无关紧要。然后，我们将探讨NumExpr如何通过智能处理数据的方式，在缓存意识上加速处理非常大的NumPy数组。
- en: Finally, we will change gears and discuss the effect of performing computations
    on *clusters* or clouds based on very fast local networks. Much of the code that
    we use to conduct data analysis is run on clusters or *clouds*, which may be implemented
    on these types of networks, so it is useful for you to know how this is done.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将转换话题，讨论在基于非常快速本地网络的集群或云上执行计算的影响。我们用于进行数据分析的大部分代码都是在集群或云上运行的，这些集群或云可能基于这些类型的网络实现，因此了解如何进行这一操作是有用的。
- en: Interpreting performance
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 解释性能
- en: Because this is a hardware-dependent chapter, the results that you get may be
    *qualitatively* different from the ones I present, given that your hardware is
    different from mine. What might fit in the cache of my machine might not fit into
    yours. Furthermore, if you run this code on the machine with a user interface,
    cache usage will be mostly unpredictable due to all the other processes running
    concurrently with your code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因为这是一本硬件相关的章节，所以你得到的结果可能与我的不同，因为你的硬件与我的不同。可能适合我机器缓存的，可能不适合你的。此外，如果你在带有用户界面的机器上运行此代码，由于所有其他进程与你代码的同时运行，缓存使用将主要不可预测。
- en: 'All the benchmarks presented here were run on a server without a user interface
    or other large processes running. The specifications are: Intel Xeon 8375C CPU
    @ 2.90GHz, 32 cores, L1 Cache 2 MB, L2 Cache 40 MB, L3 cache 54 MB, DRAM 16 GB.
    We will give a concrete example of how results can vary tremendously with your
    hardware in the NumExpr section.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 这里展示的所有基准测试都是在没有用户界面或其他大型进程运行的服务器上运行的。规格如下：Intel Xeon 8375C CPU @ 2.90GHz，32
    核心，L1 缓存 2 MB，L2 缓存 40 MB，L3 缓存 54 MB，DRAM 16 GB。我们将在 NumExpr 部分给出一个具体示例，说明结果如何因你的硬件而大幅变化。
- en: Let’s start with a review of modern hardware architecture, focusing on the problems
    that may have counterintuitive consequences on efficient Python coding. This chapter
    will require the installation of Blosc (`conda install blosc`). If you use Docker,
    the main image includes everything you need.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从现代硬件架构的回顾开始，重点关注可能对高效 Python 编码产生反直觉后果的问题。这一章需要安装 Blosc (`conda install
    blosc`)。如果你使用 Docker，主镜像包含了你需要的一切。
- en: 6.1 How modern hardware architectures affect Python performance
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 现代硬件架构如何影响 Python 性能
- en: 'In this section, we will survey the current landscape of hardware architectures,
    focusing on their less intuitive, but crucial, implications for efficient Python
    development. Hardware architecture includes what’s inside the computer—CPU, memory,
    and local storage—as well as the network. When we look at local storage, and especially
    network architecture, we will also sometimes dabble in system software architecture
    problems: namely, file systems and network protocols. Again, the topics here could
    easily fill several books, so we will narrow our concentration to problems that
    have a direct effect on Python performance and for which there are Python libraries
    to address those problems.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述当前硬件架构的现状，重点关注它们对高效 Python 开发的较少直观但至关重要的影响。硬件架构包括计算机内部的内容——CPU、内存和本地存储——以及网络。当我们查看本地存储，尤其是网络架构时，我们有时也会涉猎系统软件架构问题：即文件系统和网络协议。再次强调，这里的话题可以轻易地填满几本书，因此我们将我们的关注点缩小到对
    Python 性能有直接影响的问题，并且有 Python 库可以解决这些问题。
- en: We will start with a seemingly trivial example that will serve as motivation
    and, I hope, convince you of the very real benefits of understanding how these
    hardware and system problems affect performance—and what you can do about it.
    If you are interested in having performance gains of up to two orders of magnitude
    on some operations where you were expecting to have none, read on.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从看似微不足道的例子开始，这个例子将作为动机，并希望说服你理解这些硬件和系统问题如何影响性能——以及你可以做些什么。如果你对在某些操作中获得高达两个数量级的性能提升感兴趣，其中你原本预期没有任何提升，请继续阅读。
- en: 6.1.1 The counterintuitive effect of modern architectures on performance
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 现代架构对性能的反直觉影响
- en: Our trivial example with be simply taking a NumPy *square* matrix and duplicating
    the value of a row and the value of a column. From a performance perspective,
    doubling a row should take exactly the same time as doubling a column, given that
    the matrix is square. It’s obvious! Or is it?
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的简单例子将是简单地取一个 NumPy *平方* 矩阵并复制一行和一列的值。从性能的角度来看，加倍一行应该与加倍一列花费的时间完全相同，前提是矩阵是平方的。这很明显！或者不是吗？
- en: 'To find out, let’s evaluate the performance cost of doubling a row and of doubling
    a column:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出答案，让我们评估加倍一行和加倍一列的性能成本：
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① IPython on Jupyter allows us to conduct a performance analysis by adding %timeit
    before the last two lines.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在 Jupyter 上的 IPython 允许我们在最后两行之前添加 %timeit 来进行性能分析。
- en: We create a random matrix with values between 0 and 9\. We start with a size
    of 100\. Later, we’ll vary this, using 1000 and 10,000.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们创建一个介于 0 和 9 之间的随机矩阵。我们开始时的大小为 100。稍后，我们将改变这个大小，使用 1000 和 10,000。
- en: Again, notice that the matrix is square. That means that `double_column` and
    `double_ row` will require the same number of operations. Common sense suggests
    that this is a trivial problem (i.e., not worth our time), and that the time cost
    of doubling a column or doubling a row should be mostly the same. In this case,
    common sense is wrong.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，请注意矩阵是方阵。这意味着 `double_column` 和 `double_row` 将需要相同数量的操作。常识表明这是一个微不足道的问题（即不值得我们花费时间），并且加倍一列或加倍一行的时间成本应该大致相同。在这种情况下，常识是错误的。
- en: Let’s start with the previous code, a square matrix of size 100—hence, 10,000
    elements. Given that the default integer representation is 8 bytes, we have 80
    KB. On my computer, doubling a column takes a mean time of 750 ns; doubling a
    row takes 715 ns. Not a big difference, and given the granularity of the operation,
    the difference may be caused by the instrumentation to profile this. So far, so
    obvious.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从之前的代码开始，一个大小为 100 的方阵——因此，有 10,000 个元素。鉴于默认整数表示为 8 字节，我们有 80 KB。在我的电脑上，加倍一列的平均时间为
    750 纳秒；加倍一行，715 纳秒。没有太大的差异，考虑到操作的粒度，差异可能是由用于分析此操作的仪器引起的。到目前为止，一切都很明显。
- en: Let’s increase the size of our matrix to 1000, thus 1 million elements and 8
    MB. We now have 1.99 μs and 1.5 μs respectively. Again, nothing really striking.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们增加矩阵的大小到 1000，因此有 100 万个元素和 8 MB。现在我们有 1.99 微秒和 1.5 微秒。再次，并没有什么真正引人注目的。
- en: Let’s increase the size to 10,000\. Such a matrix will take approximately 800
    MB, so make sure you have enough memory to perform this. Doubling a column takes
    4.51 μs; doubling a row, 74.9 µs!
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将大小增加到 10,000。这样一个矩阵将占用大约 800 MB，所以请确保你有足够的内存来执行此操作。加倍一列需要 4.51 微秒；加倍一行，74.9
    微秒！
- en: 'For this bigger matrix, it is 16 times faster to double a column than to double
    a row. Let this sink in: there is *something* regarding the hardware architecture
    and NumPy’s internal representation that makes two apparently equal operations
    have a difference in performance of more than one order of magnitude!'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个更大的矩阵，加倍一列比加倍一行快 16 倍。让这个事实沉淀下来：有关硬件架构和 NumPy 内部表示的某些方面使得两个看似相同的操作在性能上相差一个数量级以上！
- en: There are two problems here at work. One is the relationship between the CPU
    cache and the main memory. The other is the internal representation of the matrix.
    They conspire to cause the difference in performance. We will delve into both
    these problems later in this chapter.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两个问题在起作用。一个是 CPU 缓存和主存储器之间的关系。另一个是矩阵的内部表示。它们共同导致了性能差异。我们将在本章后面深入探讨这两个问题。
- en: 6.1.2 How CPU caching affects algorithm efficiency
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 CPU 缓存如何影响算法效率
- en: Let’s first consider transient memory. We typically think in terms of DRAM,
    but computation happens in CPU registers (i.e., the lowest level of memory) and
    passes through several layers of CPU caches. Table 6.1 shows an example of what
    a modern machine might look like.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先考虑瞬态记忆。我们通常从 DRAM 的角度思考，但计算实际上发生在 CPU 寄存器（即最低级别的内存）中，并经过 CPU 缓存的几层。表 6.1
    展示了一个现代机器可能的样子。
- en: Table 6.1 Memory hierarchy with sizes and access times for a hypothetical, but
    realistic modern desktop
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.1 假设但现实的现代桌面计算机的内存层次结构，大小和访问时间
- en: '| Type | Size | Access time |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 大小 | 访问时间 |'
- en: '| CPU |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| CPU |'
- en: '| L1 cache | 256 KB | 2 ns |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| L1 缓存 | 256 KB | 2 ns |'
- en: '| L2 cache | 1 MB | 5 ns |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| L2 缓存 | 1 MB | 5 ns |'
- en: '| L3 cache | 6 MB | 30 ns |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| L3 缓存 | 6 MB | 30 ns |'
- en: '| RAM |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| RAM |'
- en: '| DIMM | 8 GB | 100 ns |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| DIMM | 8 GB | 100 ns |'
- en: L1 cache times are near the cycle speed of modern CPUs. Remember that 2 GHz
    means 2 × 10^9 cycles/s, and a nanosecond is 10^(-9) seconds.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: L1 缓存时间接近现代 CPU 的周期速度。记住，2 GHz 意味着 2 × 10^9 个周期/秒，而纳秒是 10^(-9) 秒。
- en: 'If the data that the CPU needs can be found in the L1 cache (hit rate), then
    the speeds will match. However, using DRAM means that the CPU will be idle for
    a long time: it’s not impossible that 90% of the time, it’s just waiting for data
    to be fetched.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 CPU 需要的数据可以在 L1 缓存中找到（命中率），则速度将匹配。然而，使用 DRAM 意味着 CPU 将长时间空闲：90% 的时间可能只是在等待数据读取，这并非不可能。
- en: 'Now we can explain our original example: why the doubling of a column in a
    square can have a completely different time cost than doubling a row. So, if you
    have a matrix like'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以解释我们的原始示例：为什么方阵中列的加倍与行的加倍可能具有完全不同的时间成本。所以，如果你有一个像
- en: '| I11 | I12 | I13 | I14 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| I11 | I12 | I13 | I14 |'
- en: '| I21 | I22 | I23 | I24 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| I21 | I22 | I23 | I24 |'
- en: '| I31 | I32 | I33 | I34 |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| I31 | I32 | I33 | I34 |'
- en: '| I41 | I42 | I43 | I44 |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| I41 | I42 | I43 | I44 |'
- en: 'it will have to be represented in memory sequentially:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 它必须在内存中顺序表示：
- en: '| I11 | I12 | I13 | I14 | I21 | I22 | I23 | I24 | I31 | I32 | I33 | I34 | I41
    | I42 | I43 | I44 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| I11 | I12 | I13 | I14 | I21 | I22 | I23 | I24 | I31 | I32 | I33 | I34 | I41
    | I42 | I43 | I44 |'
- en: When you access element I11, the CPU will bring a few more elements into the
    memory, not just a single one. So, if you do 2*I11, 2*I12, 2*I13, 2*I14, there
    will only be a single move from memory to cache. But if you do 2*I11, 2*I21, 2*I31,
    2*I41, because they are not contiguous, every time you do an operation there will
    be a memory move, which is comparatively a very expensive operation. So, the first
    case is four doubling operations with one memory move, whereas the second case
    is four doubling operations with four memory moves.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 当你访问元素I11时，CPU会带来一些额外的元素进入内存，而不仅仅是单个元素。所以，如果你做2*I11，2*I12，2*I13，2*I14，从内存到缓存的移动将只有一次。但是，如果你做2*I11，2*I21，2*I31，2*I41，因为它们不是连续的，每次你进行操作时都会有内存移动，这相对是一个非常昂贵的操作。所以，第一种情况是四个加倍操作加一次内存移动，而第二种情况是四个加倍操作加四次内存移动。
- en: Our example is, of course, a simplification. Depending on the size of the matrix
    and the size of the caches, it might be possible that the CPU brings all the data
    in a single move; that is why we don’t see a difference with very small matrices,
    But if the matrix is large enough, the effect becomes so pronounced that it can
    make one operation one order of magnitude more expensive than the other.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们的例子是一个简化。根据矩阵的大小和缓存的大小，CPU可能一次就能将所有数据都带入；这就是为什么我们看不到非常小的矩阵有差异。但是，如果矩阵足够大，这种影响就会变得非常明显，以至于可以使一个操作比另一个操作贵一个数量级。
- en: 'Tip There is another problem to take into account with the representation of
    matrices: we can have each row contiguously represented or have each column contiguously
    represented. The former is common in C-based code, whereas the latter is common
    in Fortran-based code. This is quite important for us because the backend of NumPy
    can be implemented in both languages, so we must be mindful of the backend implementation
    to devise how to access data.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：在矩阵表示方面，还有一个需要考虑的问题：我们可以连续表示每一行，或者连续表示每一列。前者在基于C的代码中很常见，而后者在基于Fortran的代码中很常见。这对我们来说非常重要，因为NumPy的后端可以用这两种语言之一实现，因此我们必须注意后端实现，以便设计如何访问数据。
- en: The next two sections will demonstrate how to use two libraries, Blosc and NumExpr,
    to make efficient use of CPU caches.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 下两个部分将演示如何使用两个库，Blosc和NumExpr，来有效地利用CPU缓存。
- en: 6.1.3 Modern persistent storage
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 现代持久存储
- en: 'Another potential problem area is persistent storage. The most common is local
    storage, either hard disk drives (HDDs) or solid-state drives (SDDs). Persistent
    memory is orders of magnitude slower to access than transient memory: SSDs have
    access times in the microsecond range, and HDDs, in the millisecond range. While
    we are not going into this topic further here (although some of these topics will
    resurface in a different form in chapter 8), note that techniques for transient
    memory like the one presented in the next section are equally valid for storage.
    For example, there are cases where it is faster to work with compressed files
    than with raw files; the cost of decompressing can be substantially lower than
    reading more (raw) data from disk.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个潜在的问题领域是持久存储。最常见的是本地存储，无论是硬盘驱动器（HDD）还是固态驱动器（SDD）。持久性内存的访问速度比瞬态内存慢得多：SSD的访问时间在微秒级别，而HDD在毫秒级别。虽然我们在这里不会进一步探讨这个话题（尽管这些话题中的某些将在第8章以不同形式出现），请注意，下一节中介绍的瞬态内存的技术同样适用于存储。例如，有些情况下，处理压缩文件比处理原始文件更快；解压缩的成本可以大大低于从磁盘读取更多（原始）数据。
- en: In addition to transient memory and local persistent memory, we have remote
    storage and remote computation. In theory, remote storage and remote computation
    are substantially slower than local storage. For example, access times when you
    are accessing a storage server on the Internet are long and unpredictable. That
    being said, modern local computation clusters can have very fast backbones. How
    fast? It can be faster to access a remote server than to get data from a local
    disk! We will discuss the implications of that in the last section of the chapter.
    As you will see, standard network protocols that we use to access long-distance
    web services might not be fast enough when we are working on a local fast network.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 除了瞬态内存和本地持久内存之外，我们还有远程存储和远程计算。从理论上讲，远程存储和远程计算比本地存储慢得多。例如，当你访问互联网上的存储服务器时，访问时间很长且不可预测。尽管如此，现代本地计算集群可以拥有非常快的骨干网络。有多快？访问远程服务器可能比从本地磁盘获取数据更快！我们将在本章的最后部分讨论这一点的影响。正如你将看到的，当我们在一个本地快速网络上工作时，我们用来访问远程网络服务的标准网络协议可能不够快。
- en: The takeaway
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的经验
- en: What I hope you take away from this section is the realization that some old
    assumptions about computation and memory locality can be wrong. As we’ve seen,
    the apparently same operation can have dramatically different costs, depending
    on how memory is allocated. Therefore, if we want to be CPU-efficient, we need
    to make sure that the information that the algorithm needs is as close to the
    CPU as possible. Furthermore, DRAM proximity is not enough, as accessing it can
    cause CPU starvation and cause the CPU to be idle for multiple cycles. Having
    data available in the L1 cache is our aim whenever possible.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你能从这一节中得到这样的认识：关于计算和内存局部性的某些旧假设可能是错误的。正如我们所见，同样的操作可能具有截然不同的成本，这取决于内存是如何分配的。因此，如果我们想提高CPU效率，我们需要确保算法所需的信息尽可能接近CPU。此外，DRAM的接近性还不够，因为访问它可能导致CPU饥饿，并使CPU空闲多个周期。尽可能让数据在L1缓存中可用是我们的目标。
- en: 'But the rabbit hole goes deeper: sometimes it can be faster to decompress data
    on the fly (i.e., use expensive decompression algorithms) than to use raw data.
    This is exactly what Blosc allows us to do and we will explore it in the next
    section.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 但这个洞还要深：有时在运行时解压缩数据（即使用昂贵的解压缩算法）可能比使用原始数据更快。这正是Blosc允许我们做到的，我们将在下一节中探讨这一点。
- en: 6.2 Efficient data storage with Blosc
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 使用Blosc高效存储数据
- en: Blosc is a high-performance compressor framework that is designed to make processing
    compressed data faster than processing its uncompressed counterpart. How is that
    even possible? Remember from the previous section that the CPU can be starved
    most of the time if the data that it needs to process is located far away in DRAM.
    If the number of CPU cycles that we use to (un)compress data is small enough that
    they occur during the starvation time of the CPU, then compression is actually
    “free.”
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Blosc是一个高性能的压缩框架，旨在使处理压缩数据比处理其未压缩版本更快。这怎么可能呢？记得上一节提到的，如果需要处理的数据位于DRAM的较远位置，CPU大部分时间都会处于饥饿状态。如果我们用于（解）压缩数据的CPU周期数足够少，以至于它们发生在CPU饥饿时间内，那么压缩实际上就是“免费的”。
- en: 6.2.1 Compress data; save time
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 压缩数据；节省时间
- en: To see how dealing with compressed data can be faster than raw data in some
    cases, we will look at three alternative ways to create NumPy arrays and then
    store and retrieve them from the disk using NumPy and Blosc. We will be looking
    at the time and disk space implications of each approach. This is a substantially
    more complex task than it seems.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解在某些情况下处理压缩数据如何比处理原始数据更快，我们将探讨三种创建NumPy数组的替代方法，然后使用NumPy和Blosc将它们存储到磁盘上并检索它们。我们将研究每种方法的时序和磁盘空间影响。这比看起来要复杂得多。
- en: 'Let’s start with array creation and support functions:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从创建数组和支持函数开始：
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① NumPy can take care of disk persistence natively.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ① NumPy可以原生处理磁盘持久性。
- en: ② sync forces the disk to flush.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ② sync强制磁盘刷新。
- en: ③ If we want to write a NumPy array in Blosc, we need to pack it.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 如果我们想在Blosc中写入NumPy数组，我们需要将其打包。
- en: ④ If we want to read a NumPy array in Blosc, we need to unpack it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如果我们想在Blosc中读取NumPy数组，我们需要将其解包。
- en: 'We start by creating three arrays: one has only zeroes, another has tile values
    of up to 256, and one has random values. We use these three array types because
    they represent very different situations regarding compression: a zero-array is
    trivial to compress, a tiled array takes an average amount of time to compress,
    and a random one is mostly impossible to compress.'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建三个数组：一个只包含零，另一个包含最多256的瓦片值，还有一个包含随机值。我们使用这三种数组类型，因为它们代表了关于压缩的非常不同的情境：零数组很容易压缩，瓦片数组平均压缩时间，而随机数组基本上无法压缩。
- en: 'We then create a set of helper functions to read and write arrays to the disk.
    This is not trivial on the write side: because we want to fairly benchmark the
    total cost of the writing function, we need to force the operating system to flush
    buffers—hence, the use of `sync`. `sync` is not available on Windows.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们创建一组辅助函数来读取和写入磁盘上的数组。在写入方面，这并不简单：因为我们想公平地基准测试写入函数的总成本，我们需要强制操作系统刷新缓冲区——因此，我们使用了`sync`。`sync`在Windows上不可用。
- en: 'Let’s now benchmark the write part:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在基准测试写入部分：
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: We start by calling `sync`, which will clean the operating system IO buffers
    as much as possible. We then use `%time` to time the write functions. While it
    might be safe, at least for the write part, to use `%timeit`, we want to avoid
    any possibility that the operating system optimizes our calls, which would make
    the benchmarks difficult to interpret. Table 6.2 provides the time results.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调用`sync`，这将尽可能清理操作系统的IO缓冲区。然后我们使用`%time`来计时写入函数。虽然对于写入部分来说可能安全，至少对于写入部分，我们可以使用`%timeit`，但我们想避免任何操作系统优化我们的调用的可能性，这会使基准测试难以解释。表6.2提供了时间结果。
- en: Table 6.2 Writing time in seconds of different array times using NumPy and Blosc
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 使用NumPy和Blosc不同数组类型写入时间的秒数
- en: '| Array | NumPy | Blosc |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 数组 | NumPy | Blosc |'
- en: '| `zero` | 7.49 | 0.53 |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| `zero` | 7.49 | 0.53 |'
- en: '| `rep_tile` | 7.49 | 0.53 |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| `rep_tile` | 7.49 | 0.53 |'
- en: '| `random` | 7.5 | 8.13 |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| `random` | 7.5 | 8.13 |'
- en: 'For the `zero` and `rep_tile` arrays, Blosc is around 15 times faster. For
    the `random` array, NumPy is slightly faster. Which case is more common? The most
    distant case from reality is the random one: data in tables tends to have some
    sort of pattern. It is not overly optimistic to assume the `zero` and `rep_tile`
    cases.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`zero`和`rep_tile`数组，Blosc大约快15倍。对于`random`数组，NumPy略快。哪种情况更常见？与现实最不接近的情况是随机情况：表格中的数据往往有一定的模式。对于`zero`和`rep_tile`情况，并不过于乐观。
- en: So, unintuitively, Blosc proves more efficient in terms of time. But what about
    disk space occupation? This is another important metric for very large datasets
    where we expect Blosc to outperform. `rep_tile` is 200 times smaller, `zero` is
    250 times smaller, and `random` has the same size.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，出人意料的是，Blosc在时间效率方面更高效。但关于磁盘空间占用呢？这是对于非常大的数据集的一个重要指标，我们预计Blosc将优于其他。`rep_tile`是200倍小，`zero`是250倍小，而`random`大小相同。
- en: 6.2.2 Read speeds (and memory buffers)
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 读取速度（和内存缓冲区）
- en: Now we will check read speeds. In theory, this is a question of just reading
    the files, right? Well, given that we have written them to disk, the operating
    system might have them in intermediate memory buffers, thus providing a biased
    view of performance. In other words, caches can make profiling unreliable. To
    have a fair comparison, we need to make sure we are *really* reading from disk
    and not from temporary memory buffers, which would be much faster. So we have
    to flush the buffers.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将检查读取速度。从理论上讲，这只是读取文件的问题，对吧？然而，鉴于我们已经将它们写入磁盘，操作系统可能将它们存储在中间内存缓冲区中，从而提供了一个有偏差的性能视图。换句话说，缓存可能会使分析不可靠。为了进行公平的比较，我们需要确保我们真正是从磁盘而不是从临时内存缓冲区中读取，这会快得多。因此，我们必须刷新缓冲区。
- en: A brutal way to solve this problem is to reboot the computer. Another way is
    to tell the operating system to invalidate all caches. Unfortunately, this is
    OS-dependent. Here I will give you a way to do this on Debian/Ubuntu and derivatives.
    This does not work on Windows or Macs or in some other Linux distributions. If
    you are in such a situation, you will have to investigate how it can be done with
    your operating system.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的粗暴方法是通过重启计算机。另一种方法是告诉操作系统使所有缓存失效。不幸的是，这取决于操作系统。在这里，我将给你一个在Debian/Ubuntu及其衍生版本上这样做的方法。这不能在Windows或Mac上工作，或在某些其他Linux发行版上。如果你处于这种情况，你将不得不调查如何使用你的操作系统来完成它。
- en: 'As the root, use the command:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 作为root用户，使用以下命令：
- en: '[PRE3]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Now we can read with the expectation that the data is not in transient buffer
    memory:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以预期数据不在瞬态缓冲区内存中读取：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The times are provided in table 6.3.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 时间信息在表6.3中提供。
- en: Table 6.3 Reading time in seconds of different array times using NumPy and Blosc
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 使用NumPy和Blosc的不同数组时间读取时间（秒）
- en: '| Array | NumPy | Blosc |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| 数组 | NumPy | Blosc |'
- en: '| `zero` | 7.02 | 0.63 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| `zero` | 7.02 | 0.63 |'
- en: '| `rep_tile` | 7.04 | 0.61 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| `rep_tile` | 7.04 | 0.61 |'
- en: '| `random` | 7.37 | 8.58 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| `random` | 7.37 | 8.58 |'
- en: This is a similar pattern to the write times. For nonrandom data, Blosc clearly
    outperforms NumPy, which means that you should consider it for large data sizes.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 这与写入时间有类似的模式。对于非随机数据，Blosc明显优于NumPy，这意味着你应该考虑它用于大数据量。
- en: Up to this point, we haven’t cared much about which compression algorithm we
    are using; we just accepted the default. But Blosc allows you to choose from many
    algorithms.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们并没有太在意我们使用的是哪种压缩算法；我们只是接受了默认设置。但是Blosc允许你从许多算法中选择。
- en: 6.2.3 The effect of different compression algorithms on storage performance
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 不同压缩算法对存储性能的影响
- en: 'The objective here is not to make an exhaustive survey of existing algorithms
    and their benchmark speeds. Instead, I simply want to make you aware that different
    algorithms exist and more may be added in the future. These algorithms vary in
    speed and efficiency, depending on how and when they are used. With this awareness,
    you should be able to choose the one that best suits your particular data and
    needs. To illustrate the potential variation in performance, let’s compare two
    algorithms: LZ4 and Zstandard.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的目标不是对现有算法及其基准速度进行详尽的调查。相反，我只想让你知道存在不同的算法，未来可能会添加更多。这些算法的速度和效率因使用方式和时间而异。有了这种认识，你应该能够选择最适合你特定数据和需求的一个。为了说明性能的潜在变化，让我们比较两种算法：LZ4和Zstandard。
- en: 'At this stage, we will stop writing data to disk, as it should be clear by
    now that it is cumbersome to benchmark it. We will perform only in-memory operations;
    namely, we will compress the data with BLOSC because we’ve already seen its superior
    performance. First, we’ll use LZ4 and then Zstandard:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们将停止将数据写入磁盘，因为现在应该很明显，对其进行基准测试很繁琐。我们只进行内存操作；也就是说，我们将使用BLOSC压缩数据，因为我们已经看到了其优越的性能。首先，我们将使用LZ4，然后是Zstandard：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① We create an in-memory representation using LZ4 as a compression algorithm.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们使用LZ4作为压缩算法创建一个内存表示。
- en: ② We create an in-memory representation using Zstandard as a compression algorithm.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们使用Zstandard作为压缩算法创建一个内存表示。
- en: Table 6.4 provides the time and size results.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4提供了时间和大小结果。
- en: Table 6.4 Compression time and size using LZ4 and Zstandard
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.4 使用LZ4和Zstandard的压缩时间和大小
- en: '|  | LZ4 | Zstandard |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '|  | LZ4 | Zstandard |'
- en: '| Time (ms) | 527 | 919 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 时间 (ms) | 527 | 919 |'
- en: '| Size (KB) | 5204 | 366 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 大小 (KB) | 5204 | 366 |'
- en: If you recall from the previous section, the LZ4 representation is 200 times
    smaller than the standard NumPy representation. This means that the Zstandard
    compression is 2800 times smaller than NumPy (200 times 14—the ratio between LZ4
    and Zstandard).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你回忆一下上一节，LZ4表示形式比标准的NumPy表示形式小200倍。这意味着Zstandard压缩算法比NumPy小2800倍（200倍乘以14——LZ4和Zstandard的比例）。
- en: 'Blosc still has an extra trick up its sleeve: not only does it offer a variety
    of algorithms, but it also allows you to change the representation of your input
    on the fly. And this can further reduce the size of the compressed data. Let’s
    see how that works.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: Blosc还有额外的技巧：它不仅提供各种算法，还允许你在运行时更改输入数据的表示。这可以进一步减小压缩数据的大小。让我们看看它是如何工作的。
- en: 6.2.4 Using insights about data representation to increase compression
  id: totrans-109
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.4 利用数据表示的见解来提高压缩
- en: 'Imagine that you know that your data has some patterns of regularity; for example,
    numbers in sequence are common. Say your data includes the following sequence
    of 8-bit encoded numbers:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你知道你的数据有一些规律性模式；例如，序列中的数字很常见。比如说，你的数据包括以下8位编码数字的序列：
- en: '[PRE6]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'This will be encoded typically in binary as:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常以二进制形式编码如下：
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Now imagine that you take the highest order bit of each number and encode it,
    then the second highest, and so on until the 8 bit of each number. This will end
    up looking like:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下，你取每个数的最高位并对其进行编码，然后是次高位，依此类推，直到每个数的第8位。这将看起来像：
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The second pattern looks much more regular. This is exactly what enables compressors
    to be efficient. Blosc allows you to do precisely this:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种模式看起来规律得多。这正是压缩器能够高效工作的原因。Blosc允许你做 precisely this：
- en: '[PRE9]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The shuffled version comes at 4,600,034 bytes, whereas the unprocessed version
    comes at 5,345,500\. Shuffle is marginally slower at 596 ms versus 524 ms.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 打乱版本的文件大小为4,600,034字节，而未经处理的版本为5,345,500字节。打乱操作略微慢一些，596毫秒比524毫秒。
- en: The takeaway
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 吸取的教训
- en: Intelligent use of memory hierarchy and CPU processing can speed up some basic
    array operations. Blosc allows us, in many cases, to access compressed stored
    representations of data faster than using raw data. This is in addition to the
    usual benefit of having smaller persistent datasets.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 智能地使用内存层次结构和CPU处理可以加速一些基本的数组操作。Blosc允许我们在许多情况下，比使用原始数据更快地访问压缩存储的数据表示。这除了通常的具有更小的持久数据集的好处之外。
- en: Let’s take this a step further and use similar techniques while analyzing data.
    For that, we will explore the NumExpr library.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更进一步，在分析数据时使用类似的技术。为此，我们将探索NumExpr库。
- en: 6.3 Accelerating NumPy with NumExpr
  id: totrans-122
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 使用NumExpr加速NumPy
- en: Blosc is one example of how we can make intelligent use of memory hierarchy
    to accelerate data processing. But we can take this approach much further by accelerating
    NumPy expressions with NumExpr.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: Blosc是我们如何智能地使用内存层次结构来加速数据处理的例子之一。但我们可以通过使用NumExpr加速NumPy表达式来进一步采取这种方法。
- en: 'NumExpr is a numerical expression evaluator for NumPy, which can be faster
    than NumPy. It takes an expression—say, `a + b`—and computes its result. But wait:
    what is the point of it? Isn’t that what NumPy does anyway? Indeed, NumExpr replaces
    some of NumPy’s functionality with an engine that tries to reorganize computation
    more efficiently when working with large datasets. One technique used by NumExpr
    relies on *not* generating full intermediate representations for parts of an expression:
    computation is made in chunks that are designed to fit into the L1 cache.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: NumExpr是NumPy的数值表达式评估器，它可以比NumPy更快。它接受一个表达式——比如说，`a + b`——并计算其结果。但是等等：这有什么意义呢？NumPy不是也在做这件事吗？确实，NumExpr用一种试图在处理大型数据集时更有效地重新组织计算的引擎替换了NumPy的一些功能。NumExpr使用的一种技术依赖于**不**生成表达式部分的全中间表示：计算是在设计用来适合L1缓存的块中进行的。
- en: 6.3.1 Fast expression processing
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 快速表达式处理
- en: 'Now let’s see a few examples of how NumExpr changes the performance of evaluating
    expressions:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看几个NumExpr如何改变表达式评估性能的例子：
- en: '[PRE10]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① This matrix is represented using Fortran standard.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这个矩阵使用Fortran标准表示。
- en: ② NumExpr provides the evaluate function to process an expression.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ② NumExpr提供了evaluate函数来处理表达式。
- en: We first create three square matrices to support our performance evaluation.
    The last one has a Fortran organization. The output of the `%timeit` benchmarks
    is summarized in table 6.5.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先创建三个正方形矩阵来支持我们的性能评估。最后一个具有Fortran组织。`%timeit`基准测试的输出总结在表6.5中。
- en: Table 6.5 Comparison of execution times between NumPy and NumExpr. Values are
    in milliseconds.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 NumPy与NumExpr执行时间比较。数值以毫秒为单位。
- en: '| Expression | Mean Numpy time | Mean NumExpr time | Speed up |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 表达式 | Numpy平均时间 | NumExpr平均时间 | 加速比 |'
- en: '| a + a | 224 | 58 | 3.8 |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| a + a | 224 | 58 | 3.8 |'
- en: '| f + f | 224 | 58 | 3.8 |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| f + f | 224 | 58 | 3.8 |'
- en: '| a + f | 577 | 153 | 3.7 |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| a + f | 577 | 153 | 3.7 |'
- en: '| a**5 + f | 1690 | 87 | 19.4 |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| a**5 + f | 1690 | 87 | 19.4 |'
- en: '| a**5 + f + sin(a) + cos(a) | 3840 | 153 | 25.1 |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| a**5 + f + sin(a) + cos(a) | 3840 | 153 | 25.1 |'
- en: In our hardware setup, NumExpr is more efficient. Take a look at the operations
    involving matrices represented in different formats, C versus Fortran. These are
    more expensive than when all the arrays are homogeneous in format. As expressions
    become more complex, the benefit of NumExpr increases as more space becomes available
    for optimization techniques to be used.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的硬件配置中，NumExpr更高效。看看以不同格式表示的矩阵操作，C与Fortran相比。当所有数组格式统一时，这些操作的成本更高。随着表达式的复杂化，NumExpr的优势增加，因为更多的空间可用于优化技术。
- en: The examples, however, paint an excessively rosy picture of NumExpr. There are
    some cases where NumExpr can decrease performance. The rest of this section will
    showcase some downsides. We will start with the qualitative variations in performance
    caused by the hardware.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些例子过于美化NumExpr。有些情况下，NumExpr可能会降低性能。本节的其余部分将展示一些缺点。我们将从由硬件引起的性能的定性变化开始。
- en: 6.3.2 How hardware architecture affects our results
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 硬件架构如何影响我们的结果
- en: 'As I alluded to at the start of this chapter, your results can vary widely
    from the ones shown here. To make this clear, I will compare the machine that
    I am using to write this text (i.e., a machine that is running a Linux GUI with
    a text editor). I will not put the cache sizes of my CPU here because that might
    be misleading: there are so many processes hitting the cache simultaneously that
    it is completely impossible to estimate the L1 cache anyway. Table 6.6 compares
    the performance speedup of using NumExpr on our arithmetic operations on the server
    versus my laptop.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在本章开头提到的，你的结果可能与这里显示的结果大相径庭。为了使这一点清楚，我将比较我用来编写此文本的机器（即运行Linux GUI和文本编辑器的机器）。我不会在这里列出我的CPU的缓存大小，因为这可能会产生误导：有如此多的进程同时访问缓存，以至于无论如何都无法估计L1缓存。表6.6比较了在服务器上使用NumExpr进行算术运算与我的笔记本电脑的性能提升速度。
- en: 'Table 6.6 The effect of hardware architecture on performance: Speedup as a
    function of hardware'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.6 硬件架构对性能的影响：硬件作为速度提升的函数
- en: '| Expression | Server speedup | Laptop speedup |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 表达式 | 服务器速度提升 | 笔记本速度提升 |'
- en: '| a + a | 3.8 | 0.7 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| a + a | 3.8 | 0.7 |'
- en: '| f + f | 3.8 | 0.8 |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| f + f | 3.8 | 0.8 |'
- en: '| a + f | 3.7 | 1.3 |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| a + f | 3.7 | 1.3 |'
- en: '| a**5 + f | 19.4 | 11.5 |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| a**5 + f | 19.4 | 11.5 |'
- en: '| a**5 + f + sin(a) + cos(a) | 25.1 | 6.7 |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| a**5 + f + sin(a) + cos(a) | 25.1 | 6.7 |'
- en: The performance benefit from using NumExpr is severely reduced. Indeed, for
    some of the operations, the NumExpr implementation suddenly becomes *slower* than
    the NumPy one. One of the main reasons is related to the unpredictable availability
    of the CPU cache on a typical local because many processes are running and competing
    for it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumExpr带来的性能优势严重降低。实际上，对于一些操作，NumExpr的实现突然变得**比NumPy**慢。其中一个主要原因是与典型本地机器上CPU缓存不可预测的可用性有关，因为许多进程正在运行并竞争缓存。
- en: Tip Don’t expect to have large speedups based on cache optimization on local
    machines that are running a lot of other applications (e.g. all the UI-based apps
    like your text editor or your browser). There will be massive competition for
    the CPU cache, and the results can vary widely from run to run. These techniques
    will shine on the server but not on a typical development machine. Hence, when
    testing the advantages of using any techniques that make use of the CPU cache,
    test on the server.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：不要期望在运行大量其他应用程序（例如所有基于UI的应用程序，如你的文本编辑器或浏览器）的本地机器上基于缓存优化获得大的速度提升。CPU缓存将会有巨大的竞争，运行结果可能会有很大的差异。这些技术将在服务器上发挥作用，但在典型的开发机器上则不会。因此，在测试任何利用CPU缓存的技术优势时，请在服务器上进行测试。
- en: As the previous example demonstrates, not all scenarios are appropriate for
    NumExpr. Let’s elaborate on when, where, and why NumExpr isn’t the best choice.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如前例所示，并非所有场景都适合使用NumExpr。让我们详细说明何时、何地以及为什么NumExpr不是最佳选择。
- en: 6.3.3 When NumExpr is not appropriate
  id: totrans-152
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 当NumExpr不适用时
- en: There are several scenarios where NumExpr can be deleterious. Let’s discuss
    them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种情况下NumExpr可能会产生不利影响。让我们来讨论一下。
- en: 'The most important factor is the size of your arrays: NumExpr tends to perform
    better with larger arrays. Let’s repeat some of the previous examples but with
    small arrays:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要因素是数组的大小：NumExpr通常在较大的数组上表现更好。让我们重复一些之前的例子，但这次使用小型数组：
- en: '[PRE11]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The addition is 15 times slower with NumExpr, whereas the complex expression
    is still 30% slower if you use NumExpr. However, this is not a serious problem:
    in most cases, you will not be trying to optimize small arrays: big data, not
    small data, is our problem.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumExpr进行加法运算会慢15倍，而如果你使用NumExpr进行复杂表达式计算，速度仍然会慢30%。然而，这并不是一个严重的问题：在大多数情况下，你不会试图优化小型数组：大数据，而不是小数据，才是我们的问题。
- en: Another source of performance degradation for NumExpr is when you perform benchmarks
    on machines, like your own local machine, that have other processes running. NumExpr
    performs better on servers, especially if you can control the number of applications
    running. This means that on shared clusters, which are common in academia, NumExpr
    performance will vary. Finally, NumExpr only supports a subset of NumPy’s operators,
    so some operations cannot be boosted by NumExpr.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: NumExpr性能下降的另一个原因是当你在对运行其他进程的机器进行基准测试时，比如你自己的本地机器。NumExpr在服务器上表现更好，特别是如果你可以控制运行的应用程序数量。这意味着在学术界常见的共享集群上，NumExpr的性能会有所变化。最后，NumExpr只支持NumPy操作符的一个子集，因此某些操作无法通过NumExpr得到提升。
- en: Now that we have discussed several ways of optimizing the use of transient memory,
    we will change our focus completely and discuss local networks. Modern local networks
    can be faster than accessing local persistent storage, and that fact, once again,
    turns some common assumptions upside down.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经讨论了几种优化使用瞬态内存的方法，我们将完全改变我们的焦点，并讨论本地网络。现代本地网络可以比访问本地持久存储更快，这个事实再次颠覆了一些常见的假设。
- en: 6.4 The performance implications of using the local network
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 使用本地网络性能的影响
- en: 'When we code for the network, we are dealing with an infrastructure that can
    have completely different properties. Many times we assume that the network is
    something far away—with speed, latency, and resilience problems. But in many high-performance
    scenarios, when we use the network locally, these assumptions do not apply. Modern
    network switches can support up to 2 Tb/s of backbone communication with up to
    56 Gb/s per network port. As a reference, most local disks support 6 Gb/s per
    second. Think about this for a second: in a high-performance local network, it
    is faster to talk with another computer than it is to talk with a local disk.
    If you are in a situation where you have a fast network between your nodes, read
    on.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们为网络编写代码时，我们正在处理一个可能具有完全不同特性的基础设施。很多时候我们假设网络是远方的——存在速度、延迟和弹性问题。但在许多高性能场景中，当我们本地使用网络时，这些假设并不适用。现代网络交换机可以支持高达2
    Tb/s的主干通信，每个网络端口高达56 Gb/s。作为一个参考，大多数本地磁盘每秒支持6 Gb/s。思考一下：在一个高性能的本地网络中，与另一台计算机交谈比与本地磁盘交谈要快。如果你处于节点之间有快速网络的情况，请继续阅读。
- en: The typical software network framework for communication is wholly inappropriate
    to deal with the speed of a modern local network. Do you think interrogating your
    local disk using a REST call over HTTPS will be efficient? Given that high-performance
    local networks are faster than local disk access, we have to find a more efficient
    way to communicate.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 用于通信的典型软件网络框架完全不适用于处理现代本地网络的速度。你认为使用HTTPS上的REST调用查询本地磁盘会有效率吗？鉴于高性能本地网络比本地磁盘访问更快，我们必须找到一种更有效的方式进行通信。
- en: Before we devise a solution, let’s understand why a standard approach will not
    be efficient. In this section, we will implement the backend of a pastebin service
    *not* based on REST. A pastebin service allows you to store pieces of text to
    share with others over the internet. We will write a client that sends text for
    storage and requests texts for reading. We will also write the server, which stores
    the texts and serves those on request. For an example of a real-life service like
    this, see [https://pastebin.com/](https://pastebin.com/). We will assume that
    our client and server will be run on a very fast local network.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设计解决方案之前，让我们了解为什么标准方法将不会有效率。在本节中，我们将实现一个基于REST的*非*后端的服务。一个代码片段服务允许你将文本存储起来，以便通过互联网与他人分享。我们将编写一个客户端，用于发送文本进行存储和请求读取文本。我们还将编写服务器，用于存储文本并在请求时提供服务。有关此类实际服务的示例，请参阅[https://pastebin.com/](https://pastebin.com/)。我们将假设我们的客户端和服务器将在一个非常快速的本地网络上运行。
- en: 6.4.1 The sources of inefficiency with REST calls
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 REST调用中的低效来源
- en: Before we devise an efficient solution, let’s understand the performance bottlenecks
    of typical REST implementations. Client/server communication in REST is typically
    done using JSON payloads over HTTPS. JSON is a text format and thus requires computing
    time for parsing and a lot of space. HTTPS augments the HTTP protocol with authentication
    and encryption using public key cryptography. HTTPS thus adds substantial processing
    on top of HTTP.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们设计一个高效解决方案之前，让我们先了解典型REST实现中的性能瓶颈。在REST中，客户端/服务器通信通常是通过HTTPS上的JSON有效载荷来完成的。JSON是一种文本格式，因此需要解析时间，并且占用大量空间。HTTPS通过使用公钥加密技术增加了HTTP协议的认证和加密。因此，HTTPS在HTTP之上增加了大量的处理。
- en: 'The HTTP protocol does all its work on top of an Internet protocol called Transmission
    Control Protocol (TCP). TCP establishes a connection abstraction between two endpoints—in
    our case, the client and the server. The connection assures that the data arrives
    in order with no loss. But the protocol is heavy, at least for our very fast network:
    just establishing a connection will require at least three data packets to go
    backward and forward between the client and the server.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP协议在其之上的一个名为传输控制协议（TCP）的互联网协议上完成所有工作。TCP在两个端点之间建立连接抽象——在我们的例子中，是客户端和服务器。连接确保数据按顺序到达且无丢失。但这个协议很重，至少对于我们的非常快的网络来说是这样：仅仅建立连接就需要至少三个数据包在客户端和服务器之间来回传递。
- en: 'After we have our TCP connection established, the security part of HTTP needs
    to be done, which delegated to the Transport Layer Security (TLS) protocol. This
    protocol performs a handshake, which requires several packets to go backward and
    forwards between the client and the server. Given that it involves cryptography,
    it will be computationally intensive. Note that the computational time cost is
    relative: in a very fast network, it will be a big fraction of the computation;
    if you are computing with a server on the other side of the world, the same computing
    time will be negligible in the overall computation.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们建立了TCP连接之后，HTTP的安全部分需要完成，这部分由传输层安全（TLS）协议委托处理。这个协议执行握手，这需要在客户端和服务器之间来回传递几个数据包。鉴于它涉及到加密，这将非常计算密集。请注意，计算时间成本是相对的：在一个非常快的网络中，它将是计算的一个很大部分；如果你在与世界另一端的服务器进行计算，同样的计算时间在整体计算中将是微不足道的。
- en: Now we are ready to send our payload, a verbose JSON payload that will require
    text parsing. Finally, we will have to close the connection at both the HTTPS
    and TCP levels.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好发送我们的有效载荷，这是一个需要文本解析的详细JSON有效载荷。最后，我们还需要在HTTPS和TCP级别关闭连接。
- en: 'From a message perspective, the communication will require at least 20 network
    packets to be exchanged, probably more. Given the speed of the local network,
    the overwhelming majority of the exchanges will be spent on protocols. Let’s implement
    this using just two packets: the bare minimum—one for the request and another
    for the response.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 从消息的角度来看，通信至少需要交换20个网络数据包，可能更多。考虑到本地网络的速率，绝大多数的交换都将花费在协议上。让我们使用仅两个数据包来实现这一点：最基本的一组——一个用于请求，另一个用于响应。
- en: 6.4.2 A naive client based on UDP and msgpack
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 基于UDP和msgpack的简单客户端
- en: Our implementation will be quite simple. More important than understanding the
    code is understanding the tradeoffs that our implementation imposes.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现将会非常简单。比起理解代码，理解我们的实现带来的权衡更为重要。
- en: 'Let’s start with the client. Our client will post a text to the pastebin server
    and then retrieve it. It starts like this:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从客户端开始。我们的客户端将向pastebin服务器发送文本，然后检索它。它开始如下：
- en: '[PRE12]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Creates a new UDP, SOCK_DGRAM socket
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个新的UDP，SOCK_DGRAM套接字
- en: Instead of using the whole HTTPS over TCP stack, we dispense with application
    protocols altogether and replace TCP with UDP, the User Datagram protocol. UDP
    doesn’t establish connections; it just sends packets. Think of UDP as the postal
    service, whereas TCP is the phone service. In the postal service, letters can
    be lost, delivered in the wrong order, and routed wrongly. In a phone call, the
    stream is delivered in order with no lost information. From an overhead perspective,
    UDP (postal) is less burdensome than TCP (phone call).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是使用整个基于TCP的HTTPS堆栈，而是完全摒弃了应用协议，并用UDP（用户数据报协议）替换了TCP。UDP不建立连接；它只是发送数据包。将UDP想象成邮政服务，而TCP则是电话服务。在邮政服务中，信件可能会丢失，顺序错误地交付，或者错误地路由。在电话通话中，流是按顺序交付的，没有丢失信息。从开销角度来看，UDP（邮政）比TCP（电话）更轻。
- en: The previous code snippet uses the low-level module socket to create a UDP communication
    endpoint. We specify that the server address is 127.0.0.1—in this case, the address
    of the local machine—and we will use port 54321.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的代码片段使用了低级模块socket来创建UDP通信端点。我们指定服务器地址为127.0.0.1——在本例中，是本地机器的地址——我们将使用端口54321。
- en: 'This solution implies a couple of assumptions, which you need to be sure are
    acceptable for your case:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个解决方案隐含了一些假设，你需要确保这些假设适用于你的情况：
- en: '*By not using an encrypted channel, we are opening our implementation to eavesdropping
    and data changing*. When communicating in a local high-performance network, this
    is less of a problem than over the internet. Quite pragmatically, if a security
    threat can access the backbone of your network, you have bigger problems than
    access to the data: you have a compromised infrastructure.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*不使用加密通道，我们使我们的实现容易受到窃听和数据更改的影响*。在本地高性能网络中通信时，这比在互联网上要小得多的问题。相当实用地，如果一个安全威胁可以访问你的网络骨干，你面临的问题比访问数据更大：你有一个被破坏的基础设施。'
- en: '*The UDP protocol doesn’t ensure that packets are delivered*. This means that
    our solution might lose data between the client and the server. This problem is
    substantially rarer in a high-performance local network than on the internet.
    That being said, it can realistically occur, and we will address it in the last
    subsection of this chapter.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*UDP协议不保证数据包的交付*。这意味着我们的解决方案可能在客户端和服务器之间丢失数据。在高性能的本地网络中，这个问题比在互联网上要少得多。尽管如此，它确实可能发生，我们将在本章的最后一个小节中解决这个问题。'
- en: 'Let’s now complete our client by sending a text to the server and retrieving
    it:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在通过向服务器发送文本并检索它来完善我们的客户端：
- en: '[PRE13]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① We use the external msgpack library to encode complex data structures.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们使用外部msgpack库来编码复杂的数据结构。
- en: ② We pack a dictionary into a byte array using msgpack.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们使用msgpack将字典打包到字节数组中。
- en: ③ We send a UDP message to the server.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们向服务器发送UDP消息。
- en: ④ We receive the response from the server.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们从服务器接收响应。
- en: We use a function called `send_text` to send text to the server. The request
    includes the command `0`, which means it stores a text and the text proper. We
    could have encoded the command more explicitly, for example, by using the string
    “store text,” but that would be more verbose and thus less efficient. We send
    the text as-is, but given what we have seen in the previous sections, compressing
    the text might be a viable option, especially if we expect that large texts will
    be transferred.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用一个名为`send_text`的函数来向服务器发送文本。请求包括命令`0`，表示存储文本和文本本身。我们本可以更明确地编码命令，例如使用字符串“store
    text”，但这将更加冗长，因此效率更低。我们以原样发送文本，但根据我们在前几节中看到的内容，压缩文本可能是一个可行的选项，尤其是如果我们预计将传输大量文本。
- en: 'The answer that we get from the server is not encoded with msgpack. Given that
    we will get the numeric id with which the text was stored, we use something even
    simpler: a reconstruction of an integer from a stream of bytes. This should be
    even faster than msgpack.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从服务器获得的答案没有使用msgpack进行编码。鉴于我们将获得存储文本的数字ID，我们使用更简单的方法：从字节流中重建一个整数。这应该比msgpack更快。
- en: The `request_text` function has a command code of `1` and a numeric id that
    is packaged with msgpack. After sending the message, we receive the text.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`request_text`函数有一个命令代码`1`和一个用msgpack打包的数字ID。发送消息后，我们收到文本。'
- en: Finally, we send a text to the server and then get the text back by using the
    text id. Now, we will implement the server side. After that, we will revisit the
    client to make it more robust to message losses.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们向服务器发送文本，然后通过使用文本ID来获取文本。现在，我们将实现服务器端。之后，我们将重新访问客户端，使其对消息丢失更加健壮。
- en: 6.4.3 A UDP-based server
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 基于UDP的服务器
- en: 'The server code is based on the built-in module `socketserver`, which provides
    utility classes to write servers based on sockets:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 服务器代码基于内置模块`socketserver`，它提供了编写基于套接字的服务器的实用类：
- en: '[PRE14]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ① We implement our server processing code inside a handler class.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们在处理类内部实现我们的服务器处理代码。
- en: ② The handler class requires the creation of the handle method to implement
    functionality.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ② 处理类需要创建处理方法来实现功能。
- en: ③ We create a UDP server.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 我们创建一个UDP服务器。
- en: ④ We initiate an internal variable for text ids.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 我们初始化一个用于文本ID的内部变量。
- en: The handle function starts by getting the command to decide which operation
    to perform. A storage request will take the supplied text and write it to disk.
    A retrieve request will get the text pertaining to the supplied id.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 处理函数首先获取命令以决定执行哪个操作。存储请求将接受提供的文本并将其写入磁盘。检索请求将获取与提供的ID相关的文本。
- en: 'The performance-related concepts of this code were introduced in the previous
    subsection: they use msgpack and UDP. Now that we have a server and a client,
    let’s make the client more robust.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 之前小节中介绍了与性能相关的概念：它们使用msgpack和UDP。现在我们有了服务器和客户端，让我们使客户端更加健壮。
- en: 6.4.4 Dealing with basic recovery on the client side
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 客户端处理基本恢复
- en: Our original client sends a message and waits for a response. UDP doesn’t assure
    packet delivery, so we need to add a timeout mechanism. That being said, in a
    high-performance local network, UDP packet loss should be a rare event.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的原生客户端发送一条消息并等待响应。UDP不保证数据包的交付，因此我们需要添加超时机制。话虽如此，在一个高性能的本地网络中，UDP数据包丢失应该是一个罕见的事件。
- en: 'Here is our implementation, using a decorator:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是我们的实现，使用装饰器：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① We set a timeout for the socket.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ① 我们为套接字设置了超时。
- en: We simply apply our decorator to `send_text` and `request_text`. By default,
    the socket is blocking; that is, it waits until a message is received, so after
    `socket`, we use `settimeout` to make it nonblocking and return after 1.0 s if
    no message is received. This simple timeout mechanism should be enough to deal
    with the client side precisely because the network should be reliable enough to
    ensure most UDP packets won’t be lost.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需将我们的装饰器应用到`send_text`和`request_text`上。默认情况下，套接字是阻塞的；也就是说，它等待直到收到消息，因此我们在`socket`之后使用`settimeout`使其非阻塞，并在1.0秒后如果没有收到消息则返回。这种简单的超时机制应该足以精确处理客户端，因为网络应该足够可靠，以确保大多数UDP数据包不会丢失。
- en: 'It would also make sense to do something similar on the server side. On the
    server side, there is a problem with the semantics of repeating an operation:
    if you end up saving a pastebin twice, you are expending disk resources. As a
    general rule, be careful with the semantics of the operations that you are creating.
    They may not be repeatable without causing harm.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务器端也做类似的事情也是合理的。在服务器端，重复操作的问题在于语义：如果您最终保存了两次pastebin，您正在消耗磁盘资源。作为一般规则，请小心处理您创建的操作的语义。它们可能在没有造成损害的情况下不可重复。
- en: 6.4.5 Other suggestions for optimizing network computing
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.5 网络计算优化的其他建议
- en: 'Our implementation uses UDP to substantially reduce message overhead, but sometimes
    you might need to use TCP, or even HTTPS or other protocols on top of TCP. Here
    are a few tips if that is the case:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的实现使用UDP显著减少了消息开销，但有时您可能需要使用TCP，甚至是在TCP之上的HTTPS或其他协议。如果这种情况发生，这里有一些提示：
- en: '*If your client will send several requests to a server, try to use the same
    connection for all the requests*. This way, you only pay the price of setting
    up and tearing down the connection once.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果您的客户端将向服务器发送多个请求，请尝试为所有请求使用相同的连接。* 这样，您只需支付一次建立和拆除连接的成本。'
- en: '*Sometimes it is possible to pre-open TCP connections before peak usage.* This
    would allow the cost of establishing a connection to be paid outside a critical
    time path. This technique, as well as the previous technique, is commonly used
    with database connections and is called *connection pooling*.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*有时在高峰使用之前预先打开TCP连接是可能的。* 这样就可以将建立连接的成本支付在关键时间路径之外。这种技术与之前的技术一样，通常与数据库连接一起使用，并被称为
    *连接池*。'
- en: '*If UDP is too simple and TCP is too heavy, consider using the new QUIC protocol*.
    QUIC originally stood for “quick UDP internet connections.” As the old name indicates,
    it attempts to bridge the gap between having the advantages of connections on
    top of UDP.'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*如果UDP太简单而TCP太重，考虑使用新的QUIC协议。* QUIC最初代表“快速UDP互联网连接”。正如旧名称所表明的，它试图弥合在UDP之上拥有连接优势的差距。'
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Being mindful of the memory hierarchy is fundamental to the design of efficient
    programs. Most programmers are at least somewhat aware of the effect of RAM, disk
    storage, and network access, but are often less aware of the effect of the relationship
    between CPU cache and transient RAM memory.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 注意内存层次结构对于高效程序的设计至关重要。大多数程序员至少对RAM、磁盘存储和网络访问的影响有所了解，但往往对CPU缓存和瞬态RAM内存之间的关系的影响了解较少。
- en: DRAM memory access causes CPU starvation, making the CPU idle for many cycles.
    Making sure that as much data is available in the CPU cache as possible can significantly
    increase processing speed.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DRAM内存访问会导致CPU饥饿，使CPU空闲许多周期。确保尽可能多的数据在CPU缓存中可用可以显著提高处理速度。
- en: Algorithms that avoid CPU starvation will potentially be more efficient but
    sometimes in unintuitive ways. For example, it might be faster to process compressed
    data than raw data as the cost of the (de)compression algorithm might be less
    than the cost of getting (larger) uncompressed data from RAM.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免CPU饥饿的算法可能会更高效，但有时是以不直观的方式。例如，处理压缩数据可能比处理原始数据更快，因为（解）压缩算法的成本可能低于从RAM获取（更大的）未压缩数据所需的成本。
- en: In many cases, Blosc allows you to access compressed stored representations
    of data faster than using the raw data counterpart. This is in addition to the
    usual benefit of having smaller persistent datasets.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在许多情况下，Blosc允许你以比使用原始数据更快的速度访问数据的压缩存储表示。这除了通常的拥有更小持久数据集的好处之外。
- en: NumExpr can process NumPy-like expressions in less time and with less memory
    than NumPy itself. NumExpr makes use of smart L1 cache, among other techniques,
    to speed up evaluation, sometimes more than one order of magnitude.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: NumExpr可以在比NumPy本身更短的时间内，使用更少的内存处理类似NumPy的表达式。NumExpr利用智能L1缓存等技巧来加速评估，有时甚至可以超过一个数量级。
- en: Some modern local networks are so fast that accessing other computers over the
    network can be faster than accessing local disks.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些现代的本地网络速度如此之快，以至于通过网络访问其他计算机可能比访问本地磁盘更快。
- en: The standard REST APIs are too slow and inefficient to make performant use of
    fast local networks.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准的REST API太慢且效率低下，无法充分利用快速本地网络。
- en: 'Network communication can be made faster by introducing changes at several
    levels: selecting the best transport protocol (TCP versus UDP), not using HTTPS,
    and using faster ways than JSON to serialize data.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过在多个层面进行改进，网络通信可以变得更快：选择最佳的传输协议（TCP与UDP），不使用HTTPS，以及使用比JSON更快的序列化数据方式。
