- en: 2 White-box models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 白盒模型
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Characteristics that make white-box models inherently transparent and interpretable
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使白盒模型天生透明和可解释的特征
- en: How to interpret simple white-box models such as linear regression and decision
    trees
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何解释简单的白盒模型，如线性回归和决策树
- en: What generalized additive models (GAMs) are and their properties that give them
    high predictive power and high interpretability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 广义加性模型（GAMs）是什么以及赋予它们高预测能力和高可解释性的特性
- en: How to implement and interpret GAMs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何实现和解释GAMs
- en: What black-box models are and their characteristics that make them inherently
    opaque
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒模型是什么以及使它们天生不透明的特征
- en: To build an interpretable AI system, we must understand the different types
    of models that we can use to drive the AI system and techniques that we can apply
    to interpret them. In this chapter, I cover three key white-box models—linear
    regression, decision trees, and generalized additive models (GAMs)—that are inherently
    transparent. You will learn how they can be implemented, when they can be applied,
    and how they can be interpreted. I also briefly introduce black-box models. You
    will learn when they can be applied and their characteristics that make them hard
    to interpret. This chapter focuses on interpreting white-box models, and the rest
    of the book will be dedicated to interpreting complex black-box models.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要构建一个可解释的人工智能系统，我们必须了解我们可以用来驱动人工智能系统的不同类型模型以及我们可以应用来解释它们的技巧。在本章中，我将介绍三种关键的白盒模型——线性回归、决策树和广义加性模型（GAMs），它们天生透明。您将学习它们如何实现、何时可以应用以及如何解释。我还简要介绍了黑盒模型。您将学习它们何时可以应用以及使它们难以解释的特征。本章重点在于解释白盒模型，而本书的其余部分将致力于解释复杂的黑盒模型。
- en: In chapter 1, you learned how to build a robust, interpretable AI system. The
    process is shown again in figure 2.1\. The main focus of chapter 2 and the rest
    of the book will be on implementing interpretability techniques to gain a much
    better understanding of machine learning models that cover both white-box and
    black-box models. The relevant blocks are highlighted in figure 2.1\. We will
    apply these interpretability techniques during model development and testing.
    We will also learn about model training and testing, especially the implementation
    aspects. Because the model learning, testing, and understanding stages are quite
    iterative, it is important to cover all three stages together. Readers who are
    already familiar with model training and testing are free to skip those sections
    and jump straight into interpretability.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，您学习了如何构建一个健壮、可解释的人工智能系统。该过程在图2.1中再次展示。第二章以及本书的其余部分的主要重点将在于实现可解释性技巧，以更好地理解涵盖白盒和黑盒模型的人工智能模型。相关的模块在图2.1中被突出显示。我们将在模型开发和测试期间应用这些可解释性技巧。我们还将了解模型训练和测试，特别是实现方面。由于模型学习、测试和理解阶段相当迭代，因此同时涵盖这三个阶段很重要。对于已经熟悉模型训练和测试的读者，可以自由跳过那些部分，直接进入可解释性部分。
- en: When applying interpretability techniques in production, we also need to consider
    building an explanation-producing system to generate a human-readable explanation
    for the end users of your system. Explainability is, however, beyond the scope
    of this book, and the focus will be exclusively on interpretability during model
    development and testing.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产中应用可解释性技巧时，我们还需要考虑构建一个生成解释的系统，为您的系统最终用户提供人类可读的解释。然而，可解释性超出了本书的范围，本书将专注于模型开发和测试期间的可解释性。
- en: '![](../Images/CH02_F01_Thampi.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F01_Thampi.png)'
- en: Figure 2.1 The process to build a robust AI system, focusing mainly on interpretation
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 构建健壮人工智能系统的过程，主要关注解释
- en: 2.1 White-box models
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 白盒模型
- en: White-box models are inherently transparent, and the characteristics that make
    them transparent are
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒模型天生透明，使它们透明的特征是
- en: The algorithm used for machine learning is straightforward to understand, and
    we can clearly interpret how the input features are transformed into the output
    or target variable.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于机器学习的算法易于理解，我们可以清楚地解释输入特征是如何转换为输出或目标变量的。
- en: We can identify the most important features to predict the target variable,
    and those features are understandable.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以识别出预测目标变量最重要的特征，并且这些特征是可理解的。
- en: Examples of white-box models include linear regression, logistic regression,
    decision trees, and generalized additive models (GAMs). Table 2.1 shows the machine
    learning tasks to which these models can be applied.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 白盒模型的例子包括线性回归、逻辑回归、决策树和广义加性模型（GAMs）。表2.1显示了这些模型可以应用到的机器学习任务。
- en: Table 2.1 Mapping of a white-box model to a machine learning task
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 白盒模型到机器学习任务的映射
- en: '| White-box model | Machine learning task(s) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 白盒模型 | 机器学习任务（s） |'
- en: '| Linear regression | Regression |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 回归 |'
- en: '| Logistic regression | Classification |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 逻辑回归 | 分类 |'
- en: '| Decision trees | Regression and classification |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 回归和分类 |'
- en: '| GAMs | Regression and classification |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 广义加性模型（GAMs） | 回归和分类 |'
- en: In this chapter, we focus on linear regression, decision trees, and GAMs. In
    figure 2.2, I have plotted these techniques on a 2-D plane with interpretability
    on the *x*-axis and predictive power on the *y*-axis. As you go from left to right
    on this plane, the models go from the low interpretability regime to the high
    interpretability regime. As you go from bottom to top on this plane, the models
    go from the low predictive power regime to the high predictive power regime. Linear
    regression and decision trees are highly interpretable but have low to medium
    predictive power. GAMs, on the other hand, have high predictive power and are
    highly interpretable as well. The figure also shows black-box models in gray and
    italic. We cover those in section 2.6.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们重点关注线性回归、决策树和广义加性模型（GAMs）。在图2.2中，我将这些技术绘制在了一个二维平面上，其中可解释性在*x*轴上，预测能力在*y*轴上。当你从左到右移动这个平面时，模型从低可解释性状态过渡到高可解释性状态。当你从底部向上移动这个平面时，模型从低预测能力状态过渡到高预测能力状态。线性回归和决策树高度可解释，但预测能力较低到中等。另一方面，广义加性模型（GAMs）具有高预测能力，并且也是高度可解释的。该图还以灰色和斜体显示了黑盒模型。我们将在第2.6节中介绍这些。
- en: '![](../Images/CH02_F02_Thampi.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F02_Thampi.png)'
- en: Figure 2.2 White-box models on the interpretability versus predictive power
    plane
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 可解释性对预测能力平面的白盒模型
- en: We start off with interpreting the simpler linear regression and decision tree
    models and then go deep into the world of GAMs. For each of these white-box models,
    we learn how the algorithm works and the characteristics that make them inherently
    interpretable. For white-box models, it is important to understand the details
    of the algorithm because it will help us interpret how the input features are
    transformed into the final model output or prediction. It will also help us quantify
    the importance of each input feature. You’ll learn how to train and evaluate all
    of the models in this book in Python first, before we dive into interpretability.
    As mentioned earlier, because the model learning, testing, and understanding stages
    are iterative, it is important to cover all three stages together.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先从解释更简单的线性回归和决策树模型开始，然后深入到广义加性模型（GAMs）的世界。对于这些白盒模型中的每一个，我们学习算法是如何工作的以及使它们本质上可解释的特征。对于白盒模型，理解算法的细节非常重要，因为它将帮助我们解释输入特征是如何转换为最终模型输出或预测的。它还将帮助我们量化每个输入特征的重要性。你将首先学习如何在Python中训练和评估本书中的所有模型，然后再深入研究可解释性。如前所述，由于模型学习、测试和理解阶段是迭代的，因此这三个阶段一起考虑非常重要。
- en: 2.2 Diagnostics+—diabetes progression
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 诊断+—糖尿病进展
- en: Let’s look at white-box models in the context of a concrete example. Recall
    the Diagnostics+ AI example from chapter 1\. The Diagnostics+ center would now
    like to determine the progression of diabetes in their patients one year after
    a baseline measurement is taken, as shown in figure 2.3\. The center has tasked
    you, as a newly minted data scientist, to build a model for Diagnostics+ AI to
    predict diabetes progression one year out. This prediction will be used by doctors
    to determine a proper treatment plan for their patients. To gain the doctors’
    confidence in the model, it is important not just to provide an accurate prediction
    but also to be able to show how the model arrived at that prediction. How would
    you begin this task?
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个具体的例子来探讨白盒模型。回忆一下第1章中的Diagnostics+ AI例子。Diagnostics+中心现在希望确定在基线测量后一年内他们的患者的糖尿病进展情况，如图2.3所示。中心已经指派你，作为一位新晋数据科学家，为Diagnostics+
    AI构建一个模型，以预测一年后的糖尿病进展。医生将使用这个预测来确定他们的患者的适当治疗方案。为了赢得医生对模型的信心，不仅要提供准确的预测，还要能够展示模型是如何得出这个预测的。你将如何开始这项任务？
- en: '![](../Images/CH02_F03_Thampi.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F03_Thampi.png)'
- en: Figure 2.3 Diagnostics+ AI for diabetes
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 Diagnostics+ AI用于糖尿病
- en: 'First, let’s look at what data is available. The Diagnostics+ center has collected
    from around 440 patients data that consists of patient metadata such as age, sex,
    body mass index (BMI), and blood pressure (BP). Blood tests were also performed
    on these patients, and the following six measurements were collected:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看有哪些数据可用。Diagnostics+中心收集了大约440名患者的数据，这些数据包括患者的元数据，如年龄、性别、体重指数（BMI）和血压（BP）。还对这些患者进行了血液检查，并收集了以下六个测量值：
- en: LDL (bad cholesterol)
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低密度脂蛋白（坏胆固醇）
- en: HDL (good cholesterol)
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高密度脂蛋白（好胆固醇）
- en: Total cholesterol
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总胆固醇
- en: Thyroid-stimulating hormone
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 甲状腺刺激激素
- en: Low-tension glaucoma
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低眼压性青光眼
- en: Fasting blood glucose
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 空腹血糖
- en: The data also contains the fasting glucose levels for all patients one year
    after the baseline measurement was taken. This is the target for the model. How
    would you formulate this as a machine learning problem? Because labeled data is
    available, where you are given 10 input features and one target variable that
    you have to predict, you can formulate this problem as a supervised learning problem.
    The target variable is real valued or continuous, so it is a regression task.
    The objective is to learn a function *f* that will help predict the target variable
    *y* given the input features *x*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 数据还包含了所有患者在基线测量后一年内的空腹血糖水平。这是模型的目标。你将如何将这个问题表述为一个机器学习问题？因为提供了标记数据，其中你被给出了10个输入特征和一个你必须预测的目标变量，你可以将这个问题表述为一个监督学习问题。目标变量是实值或连续的，因此这是一个回归任务。目标是学习一个函数
    *f*，它将帮助根据输入特征 *x* 预测目标变量 *y*。
- en: 'Let’s now load the data in Python and explore how correlated the input features
    are with each other and the target variable. If the input features are highly
    correlated with the target variable, then we can use them to train a model to
    make the prediction. If, however, they are not correlated with the target variable,
    then we will need to explore further to determine whether there is some noise
    in the data. The data can be loaded in Python as follows:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们在Python中加载数据，并探索输入特征与彼此以及目标变量的相关性。如果输入特征与目标变量高度相关，那么我们可以使用它们来训练一个模型进行预测。然而，如果它们与目标变量不相关，那么我们需要进一步探索以确定数据中是否存在一些噪声。数据可以在Python中如下加载：
- en: '[PRE0]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Imports the scikit-learn function to load the open diabetes dataset
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入scikit-learn函数以加载公开的糖尿病数据集
- en: ② Loads the diabetes dataset
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ② 加载糖尿病数据集
- en: ③ Extracts the features and the target variable
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 提取特征和目标变量
- en: 'We will now create a Pandas DataFrame, which is a two-dimensional data structure
    that contains all the features and the target variable. The diabetes dataset provided
    by Scikit-Learn comes with feature names that are not easy to understand. The
    six blood sample measurements are named s1, s2, s3, s4, s5, and s6, which makes
    it hard for us to understand what each feature is measuring. The documentation
    provides this mapping, however, and we use that to rename the columns to something
    that is more understandable, as shown here:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将创建一个Pandas DataFrame，它是一个包含所有特征和目标变量的二维数据结构。Scikit-Learn提供的糖尿病数据集包含的特征名称不易理解。六个血液样本测量值分别命名为s1、s2、s3、s4、s5和s6，这使得我们难以理解每个特征测量的是什么。然而，文档提供了这种映射，我们使用它将列重命名为更易理解的形式，如下所示：
- en: '[PRE1]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Mapping the feature names provided by Scikit-Learn to a more readable form
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ① 将Scikit-Learn提供的特征名称映射到更易读的形式
- en: ② Loads all the features (x) into a DataFrame
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将所有特征（x）加载到DataFrame中
- en: ③ Uses the Scikit-Learn feature names as column names
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用Scikit-Learn特征名称作为列名称
- en: ④ Renames the Scikit-Learn feature names to a more readable form
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将Scikit-Learn的特征名称重命名为更易读的形式
- en: ⑤ Includes the target variable (y) as a separate column
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将目标变量（y）作为一个单独的列包含
- en: 'Now let’s compute the pairwise correlation of columns so that we can determine
    how correlated each of the input features is with each other and the target variable.
    This can be done easily in Pandas as follows:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算列之间的成对相关性，以便我们可以确定每个输入特征与其他输入特征和目标变量的相关性。这可以在Pandas中轻松完成，如下所示：
- en: '[PRE2]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'By default, the `corr()` function in pandas computes the Pearson or standard
    correlation coefficient. This coefficient measures the linear correlation between
    two variables and has a value between +1 and –1. If the magnitude of the coefficient
    is above 0.7, that means it’s a really high correlation. If the magnitude of the
    coefficient is between 0.5 and 0.7, that indicates a moderately high correlation.
    If the magnitude of the coefficient is between 0.3 and 0.5, that means a low correlation,
    and a magnitude less than 0.3 means there is little to no correlation. We can
    now plot the correlation matrix in Python as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，pandas中的`corr()`函数计算皮尔逊或标准相关系数。这个系数衡量两个变量之间的线性相关性，其值介于+1和-1之间。如果系数的绝对值大于0.7，这意味着它具有非常高的相关性。如果系数的绝对值介于0.5和0.7之间，则表示中等程度的高相关性。如果系数的绝对值介于0.3和0.5之间，则表示低相关性，而系数的绝对值小于0.3则意味着几乎没有相关性。现在我们可以在Python中如下绘制相关矩阵：
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Imports Matplotlib and Seaborn to plot the correlation matrix
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入Matplotlib和Seaborn以绘制相关矩阵
- en: ② Initializes a Matplotlib plot with a predefined size
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用预定义的大小初始化Matplotlib图表
- en: ③ Uses Seaborn to plot a heatmap of the correlation coefficients
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 使用Seaborn绘制相关系数的热图
- en: ④ Rotates the labels on the x-axis by 90 degrees
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将x轴上的标签旋转90度
- en: The resulting plot is shown in figure 2.4\. Let’s first focus on either the
    last row or the last column in the figure. This shows us the correlation of each
    of the inputs with the target variable. We can see that seven features—BMI, BP,
    Total Cholesterol, HDL, Thyroid, Glaucoma, and Glucose—have moderately high to
    high correlation with the target variable. We can also observe that the good cholesterol
    (HDL) also has a negative correlation with the progression of diabetes. This means
    that the higher the HDL value, the lower the fasting glucose level for the patient
    one year out. The features seem to have pretty good signal in being able to predict
    the disease progression, and we can go ahead and train a model using them. As
    an exercise, observe how each of the features is correlated with each other. Total
    cholesterol, for instance, seems very highly correlated with the bad cholesterol,
    LDL. We will come back to this when we start to interpret the linear regression
    model in section 2.3.1.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图表显示在图2.4中。让我们首先关注图中的最后一行或最后一列。这显示了每个输入与目标变量之间的相关性。我们可以看到七个特征——BMI、血压、总胆固醇、高密度脂蛋白、甲状腺、青光眼和葡萄糖——与目标变量具有中等程度到高度的相关性。我们还可以观察到良好的胆固醇（HDL）也与糖尿病的进展呈负相关。这意味着HDL值越高，患者一年后的空腹血糖水平就越低。这些特征似乎在预测疾病进展方面具有很好的信号，我们可以继续使用它们来训练模型。作为练习，观察每个特征之间是如何相互关联的。例如，总胆固醇似乎与坏胆固醇LDL高度相关。当我们开始在第2.3.1节中解释线性回归模型时，我们将回到这一点。
- en: '![](../Images/CH02_F04_Thampi.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F04_Thampi.png)'
- en: Figure 2.4 Correlation plot of the features and the target variable for the
    diabetes dataset
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 糖尿病数据集中特征与目标变量的相关性图
- en: 2.3 Linear regression
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 线性回归
- en: Linear regression is one of the simplest models you can train for regression
    tasks. In linear regression, the function *f* is represented as a linear combination
    of all the input features, as depicted in figure 2.5\. The known variables are
    shown in gray, and the idea is to represent the target variable as a linear combination
    of the inputs. The unknown variables are the weights that must be learned by the
    learning algorithm.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是您可以训练的最简单的回归任务模型之一。在线性回归中，函数*f*表示为所有输入特征的线性组合，如图2.5所示。已知变量用灰色表示，目标是表示目标变量为输入的线性组合。未知变量是学习算法必须学习的权重。
- en: '![](../Images/CH02_F05_Thampi.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F05_Thampi.png)'
- en: Figure 2.5 Disease progression represented as a linear combination of inputs
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 将疾病进展表示为输入的线性组合
- en: 'In general, the function *f* for linear regression is shown mathematically
    as follows, where *n* is the total number of features:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在一般情况下，线性回归的函数*f*可以用以下数学公式表示，其中*n*是特征的总数：
- en: '![](../Images/CH02_F05_Thampi_equation01.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![方程式图片](../Images/CH02_F05_Thampi_equation01.png)'
- en: 'The objective of the linear regression learning algorithm is to determine the
    weights that accurately predict the target variable for all patients in the training
    set. We can apply the following techniques here:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归学习算法的目标是确定权重，以准确预测训练集中所有患者的目标变量。我们可以应用以下技术：
- en: Gradient descent
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度下降
- en: Closed-form solution (e.g., the Newton equation)
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 闭合形式解（例如，牛顿方程）
- en: Gradient descent is commonly applied because it scales well to a large number
    of features and training examples. The general idea is to update the weights such
    that the squared error of the predicted target variable with respect to the actual
    target variable is minimized.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降法通常被应用，因为它能够很好地扩展到大量特征和训练示例。其基本思想是更新权重，使得预测目标变量与实际目标变量之间的平方误差最小化。
- en: 'The objective of the gradient descent algorithm is to minimize the squared
    error or squared difference between the predicted target variable and the actual
    target variable across all the examples in the training set. This algorithm is
    guaranteed to find the optimum set of weights, and because the algorithm minimizes
    the squared error, it is said to be based on least squares. A linear regression
    model can be easily trained using the Scikit-Learn package in Python. The code
    to train the model is shown next. Note that the open diabetes dataset provided
    by Scikit-Learn is used here, and this dataset has been standardized, having zero
    mean and unit variance for all the input features. Feature standardization is
    a widely used form of preprocessing done on datasets used in many machine learning
    models like linear regression, logistic regression, and more complex models based
    on neural networks. It allows the learning algorithms that drive these models
    to converge faster to an optimum solution:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降算法的目的是在整个训练集的所有示例中，最小化预测目标变量与实际目标变量之间的平方误差或平方差。该算法保证找到最优的权重集，并且因为算法最小化平方误差，所以它被称为基于最小二乘法。可以使用Python中的Scikit-Learn包轻松训练线性回归模型。下面的代码展示了训练模型的代码。请注意，这里使用的是Scikit-Learn提供的开放型糖尿病数据集，并且该数据集已经标准化，所有输入特征都具有零均值和单位方差。特征标准化是在许多机器学习模型（如线性回归、逻辑回归以及基于神经网络的更复杂模型）中广泛使用的预处理形式。它允许驱动这些模型的机器学习算法更快地收敛到最优解：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① Imports the scikit-learn function to split the data into training and test
    sets
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入scikit-learn函数以将数据分为训练集和测试集
- en: ② Imports the scikit-learn class for linear regression
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入scikit-learn的线性回归类
- en: ③ Imports numpy to evaluate the performance of model
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 导入numpy库以评估模型的性能
- en: ④ Splits the data into training and test sets, where 80% of the data is used
    for training and 20% of the data for testing, and ensures that the seed for the
    random-number generator is set using the random_state parameter to ensure consistent
    train-test splits
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将数据分为训练集和测试集，其中80%的数据用于训练，20%的数据用于测试，并确保使用random_state参数设置随机数生成器的种子，以保证训练集和测试集的分割一致性
- en: ⑤ Initializes the linear regression model, which is based on least squares
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 初始化基于最小二乘法的线性回归模型
- en: ⑥ Learns the weights for the model by fitting on the training set
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 通过在训练集上拟合来学习模型的权重
- en: ⑦ Uses the learned weights to predict the disease progression for patients in
    the test set
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 使用学习到的权重来预测测试集中患者的疾病进展
- en: ⑧ Evaluates the model performance using the mean absolute error (MAE) metric
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 使用平均绝对误差（MAE）指标评估模型性能
- en: The performance of the trained linear regression model can be quantified by
    comparing the predictions with the actual values on the test set. We can use multiple
    metrics, such as root mean squared error (RMSE), mean absolute error (MAE), and
    mean absolute percentage error (MAPE). Each of these metrics offers pros and cons,
    and it helps to quantify the performance using multiple metrics to measure the
    goodness of a model. Both MAE and RMSE are in the same units as the target variable
    and are easy to understand in that regard. The magnitude of the error, however,
    cannot be easily understood using these two metrics. For example, an error of
    10 may seem small at first, but if the actual value you are comparing with is,
    say, 100, then that error is not small in relation to that. This is where MAPE
    is useful for understanding these relative differences because the error is expressed
    in terms of percentage (%) error. The topic of measuring model goodness is important
    but is beyond the scope of this book. You can find a lot of resources online.
    I have written a comprehensive two-part blog post ([http://mng.bz/ZzNP](http://mng.bz/ZzNP))
    to cover this topic.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的线性回归模型的性能可以通过将预测值与测试集中的实际值进行比较来量化。我们可以使用多个指标，例如均方根误差（RMSE）、平均绝对误差（MAE）和平均绝对百分比误差（MAPE）。这些指标中的每一个都有其优缺点，并且使用多个指标来衡量模型的好坏有助于量化性能。MAE和RMSE与目标变量具有相同的单位，并且在这方面易于理解。然而，使用这两个指标很难理解误差的大小。例如，一个10的误差可能一开始看起来很小，但如果你要比较的实际值是100，那么这个误差相对于那个值来说就不小了。这就是MAPE在这里很有用，因为它以百分比（%）的形式表达误差，有助于理解这些相对差异。测量模型好坏的话题很重要，但超出了本书的范围。你可以在网上找到很多资源。我已经写了一篇综合的两部分博客文章([http://mng.bz/ZzNP](http://mng.bz/ZzNP))来涵盖这个主题。
- en: The previous trained linear regression model was evaluated using the MAE metric,
    and the performance was determined to be 42.8\. But is this performance good?
    To check whether the performance of a model is good, we need to compare it with
    a baseline. For Diagnostics+, the doctors have been using a baseline model that
    predicts the median diabetes progression across all patients. The MAE of this
    baseline model was determined to be 62.2\. If we now compare this baseline with
    the linear regression model, we notice a drop in MAE by 19.4, which is a pretty
    good improvement. We have now trained a decent model, but it doesn’t tell us how
    the model arrived at the prediction and which input features are most important.
    I cover this in the following section.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 之前训练的线性回归模型使用MAE指标进行了评估，其性能被确定为42.8。但是，这个性能好吗？为了检查一个模型的性能是否良好，我们需要将其与基线进行比较。对于Diagnostics+，医生们一直在使用一个基线模型，该模型预测所有患者糖尿病进展的中位数。这个基线模型的MAE被确定为62.2。如果我们现在将这个基线与线性回归模型进行比较，我们会注意到MAE下降了19.4，这是一个相当好的改进。我们现在已经训练了一个不错的模型，但它并没有告诉我们模型是如何得出预测的，以及哪些输入特征是最重要的。我将在下一节中介绍这一点。
- en: 2.3.1 Interpreting linear regression
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 解释线性回归
- en: In the earlier section, we trained a linear regression model during model development
    and then evaluated the model performance during testing using the MAE metric.
    As a data scientist building Diagnostics+ AI, you now share these results with
    the doctors, and they are reasonably happy with the performance. But there is
    something missing. The doctors don’t have a clear understanding of how the model
    arrived at the final prediction. Explaining the gradient descent algorithm does
    not help with this understanding because you are dealing with a pretty large feature
    space in this example—10 input features in total. It is impossible to visualize
    how the algorithm converges to the final prediction in a 10-dimensional space.
    In general, the ability to describe and explain a machine learning algorithm does
    not guarantee interpretability. So, what is the best way of interpreting a model?
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们在模型开发期间训练了一个线性回归模型，然后在测试期间使用MAE指标评估了模型性能。作为构建Diagnostics+ AI的数据科学家，你现在将这些结果与医生们分享，他们对性能表示满意。但是，还有一些不足之处。医生们对模型如何得出最终预测没有清晰的理解。解释梯度下降算法并不能帮助理解这一点，因为在这个例子中你处理的是一个相当大的特征空间——总共10个输入特征。在10维空间中可视化算法如何收敛到最终预测是不可能的。一般来说，描述和解释机器学习算法的能力并不能保证其可解释性。那么，最佳的解释模型的方法是什么呢？
- en: For linear regression, because the final prediction is just a weighted sum of
    the input features, all we have to look at are the learned weights. This is what
    makes linear regression a white-box model. What do the weights tell us? If the
    weight of a feature is positive, a positive change in that input will result in
    a proportional positive change in the output, and a negative change in the input
    will result in a proportional negative change in the output. Similarly, if the
    weight is negative, a positive change in the input will result in a proportional
    negative change in the output, and a negative change in the input will result
    in a proportional positive change in the output. Such a learned function, shown
    in figure 2.6, is called a linear, monotonic function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 对于线性回归，因为最终的预测只是输入特征的加权求和，我们只需要关注学习到的权重。这就是为什么线性回归是一个白盒模型。权重告诉我们什么？如果一个特征的权重是正的，那么输入的正变化将导致输出的正变化成比例增加，输入的负变化将导致输出的负变化成比例增加。同样，如果权重是负的，输入的正变化将导致输出的负变化成比例增加，输入的负变化将导致输出的正变化成比例增加。这种在图2.6中显示的学到的函数被称为线性、单调函数。
- en: '![](../Images/CH02_F06_Thampi.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F06_Thampi.png)'
- en: Figure 2.6 A representation of a linear, monotonic function
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 线性、单调函数的表示
- en: We can also look at the impact or importance of a feature in predicting the
    target variable by looking at the absolute value of the corresponding weight.
    The larger the absolute value of the weight, the greater the importance. The weights
    for each of the 10 features are shown in descending order of importance in figure
    2.7.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以通过查看对应权重的绝对值来观察一个特征在预测目标变量中的影响或重要性。权重的绝对值越大，其重要性就越高。图2.7显示了10个特征按重要性降序排列的权重。
- en: '![](../Images/CH02_F07_Thampi.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F07_Thampi.png)'
- en: Figure 2.7 Feature importance for the diabetes linear regression model
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 糖尿病线性回归模型的特征重要性
- en: The most important feature is the Total Cholesterol measurement. It has a large
    negative value for the weight. This means that a positive change in the cholesterol
    level has a large negative influence on predicting diabetes progression. This
    could be because Total Cholesterol also accounts for the good kind of cholesterol.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的特征是总胆固醇测量值。其权重具有较大的负值。这意味着胆固醇水平的任何增加都会对预测糖尿病进展产生较大的负面影响。这可能是因为总胆固醇也包含了好的胆固醇类型。
- en: If we now look at the bad cholesterol, or LDL, feature, it has a large positive
    weight, and it is also the fourth most important feature in predicting the progression
    of diabetes. This means that a positive change in LDL cholesterol level results
    in a large positive influence in predicting diabetes one year out. The good cholesterol,
    or HDL, feature has a small positive weight and is the third least important feature.
    Why is that? Recall the exploratory analysis that we did in section 2.2 where
    we plotted the correlation matrix in figure 2.4\. If we observe the correlation
    among total cholesterol, LDL, and HDL, we see a very high correlation between
    total cholesterol and LDL and moderately high correlation between total cholesterol
    and HDL. Because of this correlation, the HDL feature is deemed redundant by the
    model.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在查看坏胆固醇，或LDL，特征，它具有较大的正权重，并且也是预测糖尿病进展的第四个最重要的特征。这意味着LDL胆固醇水平的任何增加都会对预测一年后糖尿病的进展产生较大的正影响。好的胆固醇，或HDL，特征具有较小的正权重，并且是第三个最不重要的特征。为什么是这样？回想一下我们在2.2节中进行的探索性分析，我们在图2.4中绘制了相关矩阵。如果我们观察总胆固醇、LDL和HDL之间的相关性，我们会看到总胆固醇和LDL之间有非常高的相关性，总胆固醇和HDL之间有中等程度的高相关性。由于这种相关性，模型认为HDL特征是冗余的。
- en: It also looks like the baseline Glucose measurement for the patient has a very
    small impact on predicting the progression of diabetes a year out. If we again
    go back to the correlation plot shown in figure 2.4, we can see that Glucose measurement
    is very highly correlated with the baseline Glaucoma measurement (the second most
    important feature for the model) and highly correlated with Total Cholesterol
    (the most important feature for the model). The model, therefore, treats Glucose
    as a redundant feature because a lot of the signal is obtained from the Total
    Cholesterol and Glaucoma features.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来，对于该患者的基线葡萄糖测量对预测一年后糖尿病进展的影响非常小。如果我们再次回到图2.4所示的关联图，我们可以看到葡萄糖测量与基线青光眼测量（模型中第二重要的特征）高度相关，并且与总胆固醇（模型中最重要的特征）高度相关。因此，模型将葡萄糖视为一个冗余特征，因为大部分信号都来自总胆固醇和青光眼特征。
- en: If an input feature is highly correlated with one or more other features, they
    are said to be multicollinear. *Multicollinearity* could be detrimental to the
    performance of a linear regression model based on least squares. Let’s suppose
    we use two features, *x*[1] and *x*[2], to predict the target variable *y*. In
    a linear regression model, we are essentially estimating weights for each of the
    features that will help predict the target variable such that the squared error
    is minimized. Using least squares, the weight for feature *x*[1], or the effect
    of *x*[1] on the target variable *y*, is estimated by holding *x*[2] constant.
    Similarly, the weight for *x*[2] is estimated by holding *x*[1] constant. If *x*[1]
    and *x*[2] are collinear, then they vary together, and it becomes very difficult
    to accurately estimate their effects on the target variable. One of the features
    becomes completely redundant for the model. We saw the effects of collinearity
    on our diabetes model earlier where features such as HDL and Glucose that were
    pretty highly correlated with the target variable had very low importance in the
    final model. The problem of multicollinearity can be overcome by removing the
    redundant features for the model. As an exercise, I highly recommend doing that
    to see if you can improve the performance of the linear regression model.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个输入特征与一个或多个其他特征高度相关，则称它们为多重共线性。*多重共线性*可能会损害基于最小二乘法的线性回归模型的性能。假设我们使用两个特征，*x*[1]和*x*[2]，来预测目标变量*y*。在线性回归模型中，我们实际上是在估计每个特征的权重，这些权重将有助于预测目标变量，从而最小化平方误差。使用最小二乘法，特征*x*[1]的权重，或*x*[1]对目标变量*y*的影响，是通过保持*x*[2]不变来估计的。同样，*x*[2]的权重是通过保持*x*[1]不变来估计的。如果*x*[1]和*x*[2]是共线的，那么它们会一起变化，这就使得准确估计它们对目标变量的影响变得非常困难。其中一个特征对于模型来说变得完全冗余。我们之前在糖尿病模型中看到了共线性对模型的影响，其中像HDL和葡萄糖这样的特征与目标变量高度相关，但在最终模型中的重要性非常低。可以通过移除模型中的冗余特征来解决多重共线性问题。作为一个练习，我强烈建议你尝试这样做，看看你是否能提高线性回归模型的性能。
- en: 'In the process of training a machine learning model, it is important to explore
    the data first and determine how correlated features are with each other and with
    the target variable. The problem of multicollinearity must be uncovered early
    in the process, before training the model, but if it has been overlooked, interpreting
    the model will help expose such issues. The plot in figure 2.7 can be generated
    in Python using the following code snippet:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练机器学习模型的过程中，首先探索数据并确定特征之间以及它们与目标变量之间的相关性非常重要。多重共线性问题必须在模型训练之前早期发现，但如果被忽略，解释模型将有助于揭示这些问题。图2.7所示的图表可以使用以下代码片段在Python中生成：
- en: '[PRE5]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Imports numpy to perform operation on vectors in an optimized way
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入numpy以优化方式对向量进行操作
- en: ② Imports matplotlib and seaborn to plot the feature importance
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入matplotlib和seaborn以绘制特征重要性
- en: ③ Obtains the weights from the linear regression model trained earlier using
    the coef_ parameter
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 从之前训练的线性回归模型中通过coef_参数获取权重
- en: ④ Sorts the weights in descending order of importance and gets their indices
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 按重要性降序排序权重并获取它们的索引
- en: ⑤ Uses the ordered indices to get the feature names and the corresponding weight
    values
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用有序索引获取特征名称和相应的权重值
- en: ⑥ Generates the plot shown in figure 2.7
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 生成图2.7所示的图表
- en: 2.3.2 Limitations of linear regression
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 线性回归的局限性
- en: In the previous section, we saw how easy it is to interpret a linear regression
    model. It is highly transparent and easy to understand. However, it has poor predictive
    power, especially in cases where the relationship between the input features and
    target is nonlinear. Consider the example shown in figure 2.8.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了解释线性回归模型是多么容易。它非常透明，易于理解。然而，它的预测能力较差，尤其是在输入特征与目标之间的关系是非线性的情况下。考虑图2.8中显示的示例。
- en: '![](../Images/CH02_F08_Thampi.png)'
  id: totrans-106
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F08_Thampi.png)'
- en: Figure 2.8 Illustration of a nonlinear dataset
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 非线性数据集的说明
- en: If we were to fit a linear regression model to this dataset, we would get a
    straight-line linear fit, as shown in figure 2.9\. As you can see, the model does
    not properly fit the data and does not capture the nonlinear relationship. This
    limitation of linear regression is called *underfitting*, and the model is said
    to have *high bias*. In the following sections, we will see how this problem can
    be overcome by using more complex models with higher predictive power.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将线性回归模型拟合到这个数据集，我们会得到一个直线线性拟合，如图2.9所示。如图所示，该模型没有正确拟合数据，也没有捕捉到非线性关系。线性回归的这个局限性被称为*欠拟合*，并且模型被认为具有*高偏差*。在接下来的几节中，我们将看到如何通过使用具有更高预测能力的更复杂模型来克服这个问题。
- en: '![](../Images/CH02_F09_Thampi.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F09_Thampi.png)'
- en: Figure 2.9 The problem of underfitting (high bias)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 过拟合问题（高偏差）
- en: 2.4 Decision trees
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 决策树
- en: A decision tree is a great machine learning algorithm that can be used to model
    complex nonlinear relationships. It can be applied to both regression and classification
    tasks. It has relatively higher predictive power than linear regression and is
    highly interpretable, too. The basic idea behind a decision tree is to find optimum
    splits in the data that best predict the output or target variable. In figure
    2.10, I have illustrated this by considering only two features, BMI and Age. The
    decision tree splits the dataset into five groups in total, three age groups and
    two BMI groups.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是一种优秀的机器学习算法，可以用来建模复杂的非线性关系。它可以应用于回归和分类任务。它比线性回归具有相对更高的预测能力，并且易于解释。决策树背后的基本思想是在数据中找到最佳分割，以最好地预测输出或目标变量。在图2.10中，我通过仅考虑两个特征，BMI和年龄，来展示这一点。决策树将数据集分为总共五个组，三个年龄组和两个BMI组。
- en: '![](../Images/CH02_F10_Thampi.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F10_Thampi.png)'
- en: Figure 2.10 Decision tree splitting strategy
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 决策树分割策略
- en: 'The algorithm that is commonly applied in determining the optimum splits is
    the classification and regression tree (CART) algorithm. This algorithm first
    chooses a feature and a threshold for that feature. Based on that feature and
    threshold, the algorithm splits the dataset into the following two subsets:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定最佳分割时，通常应用的算法是分类和回归树（CART）算法。该算法首先选择一个特征和该特征的阈值。基于该特征和阈值，算法将数据集分割成以下两个子集：
- en: Subset 1, where the value of the feature is less than or equal to the threshold
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集1，其中特征的值小于或等于阈值
- en: Subset 2, where the value of the feature is greater than the threshold
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 子集2，其中特征的值大于阈值
- en: The algorithm picks the feature and threshold that minimizes a cost function
    or criterion. For regression tasks, this criterion is typically the mean squared
    error (MSE), and for classification tasks, it is typically either Gini impurity
    or entropy. The algorithm then continues to recursively split the data until the
    criterion is reduced further or until a maximum depth is reached. The splitting
    strategy in figure 2.10 is shown as a binary tree in figure 2.11.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 算法选择特征和阈值，以最小化成本函数或标准。对于回归任务，这个标准通常是均方误差（MSE），而对于分类任务，通常是基尼不纯度或熵。然后算法继续递归地分割数据，直到标准进一步降低或达到最大深度。图2.10中的分割策略在图2.11中显示为一个二叉树。
- en: '![](../Images/CH02_F11_Thampi.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F11_Thampi.png)'
- en: Figure 2.11 Decision tree data splitting visualized as a binary tree
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 决策树数据分割以二叉树形式可视化
- en: 'A decision tree model can be trained in Python using the Scikit-Learn package
    as follows. The code to learn the open diabetes dataset and to split it into the
    training and test sets is the same as the one used for linear regression in section
    2.3, so, this code is not repeated here:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用Python中的Scikit-Learn包训练决策树模型，如下所示。学习开放糖尿病数据集并将其分割为训练集和测试集的代码与第2.3节中用于线性回归的代码相同，因此这里不再重复：
- en: '[PRE6]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Imports the scikit-learn class for the decision tree regressor
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入scikit-learn的决策树回归器类
- en: ② Initializes the decision tree regressor. It is important to set the random_state
    to ensure that consistent, reproducible results can be obtained.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化决策树回归器。设置random_state非常重要，以确保可以得到一致且可重复的结果。
- en: ③ Trains the decision tree model
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 训练决策树模型
- en: ④ Uses the trained decision tree model to predict the disease progression for
    patients in the test set
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用训练好的决策树模型预测测试集中患者的疾病进展
- en: ⑤ Evaluates the model performance using the mean absolute error (MAE) metric
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用平均绝对误差（MAE）指标评估模型性能
- en: The decision tree model trained here was evaluated using the MAE metric, and
    the performance was determined to be 54.7\. If we tune the *max_depth* hyperparameter
    and set it to 3, we can improve the MAE performance further to 48.6\. This performance,
    however, is poorer than the regression model trained in section 2.2\. I will discuss
    the reasons for this in section 2.4.2, but first, let’s look at how to interpret
    a decision tree in the following section.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这里训练的决策树模型使用MAE指标进行评估，性能确定为54.7。如果我们调整*max_depth*超参数并将其设置为3，我们可以进一步提高MAE性能到48.6。然而，这种性能比第2.2节中训练的回归模型要差。我将在第2.4.2节中讨论这种差异的原因，但首先，让我们在下一节中看看如何解释决策树。
- en: Decision tree for classification tasks
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 用于分类任务的决策树
- en: 'As mentioned in this section, decision trees can also be used for classification
    tasks. In the CART algorithm, Gini impurity or entropy is used as the cost function.
    In Scikit-Learn, you can easily train a decision tree classifier as follows:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如本节所述，决策树也可以用于分类任务。在CART算法中，Gini不纯度或熵被用作成本函数。在Scikit-Learn中，你可以轻松地训练一个决策树分类器，如下所示：
- en: '[PRE7]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `criterion` parameter in the `DecisionTreeClassifier` can be used to specify
    the cost function for the CART algorithm. By default, it is set to `gini`, but
    it can be changed to `entropy`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '`DecisionTreeClassifier`中的`criterion`参数可以用来指定CART算法的成本函数。默认情况下，它设置为`gini`，但可以更改为`entropy`。'
- en: 2.4.1 Interpreting decision trees
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 解释决策树
- en: Decision trees are great at modeling nonlinear relationships between the input
    and the output. By finding splits in the data across features, the model tends
    to learn a function that is nonlinear in nature. The function could be monotonic,
    where a change in the input results in a change in the output in the same direction,
    or nonmonotonic, where a change in the input could result in a change in the output
    in any direction and at a varying rate. This is illustrated in figure 2.12.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树擅长建模输入和输出之间的非线性关系。通过在特征间找到数据分割，模型倾向于学习一个本质上非线性的函数。这个函数可以是单调的，其中输入的变化导致输出以相同方向的变化，或者非单调的，其中输入的变化可能导致输出以任何方向和不同的速率变化。这如图2.12所示。
- en: '![](../Images/CH02_F12_Thampi.png)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F12_Thampi.png)'
- en: Figure 2.12 Representation of nonlinear, monotonic, and nonmonotonic functions
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.12 非线性、单调和非单调函数的表示
- en: How do we interpret such a learned nonlinear function? As seen in the previous
    section, a decision tree can be visualized as a bunch of if-else conditions strung
    together, where each condition splits the data in two. Such a model can be easily
    visualized as a binary tree, as illustrated in figure 2.11\. For the decision
    tree model trained for diabetes, the visualization of the binary tree is shown
    in figure 2.13\. The tree can be interpreted as follows.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何解释这样一个学习到的非线性函数？如前所述，决策树可以被视为一系列串联的if-else条件，其中每个条件将数据分成两部分。这样的模型可以很容易地可视化为一个二叉树，如图2.11所示。对于为糖尿病训练的决策树模型，二叉树的可视化如图2.13所示。树可以解释如下。
- en: Starting at the root of the tree, check if the normalized BMI is <= 0\. If true,
    go to the left part of the tree. If false, go to the right part of the tree. Because
    we are starting at the root of the tree, this node accounts for 100% of the data.
    This is why *samples* is equal to 100%. Also, if we were to set the *max_depth*
    to 0 and predict the disease progression, then we would use the average value
    of all the samples in the data, which is 153.7, represented as *value* in the
    tree. By predicting 153.7, we would get an MSE of 6076.4.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 从树的根节点开始，检查标准化后的BMI是否小于等于0。如果是，则进入树的左侧部分。如果不是，则进入树的右侧部分。因为我们是从树的根节点开始的，这个节点代表了100%的数据。这就是为什么“样本”等于100%的原因。此外，如果我们把*max_depth*设置为0并预测疾病进展，那么我们会使用数据中所有样本的平均值，即153.7，在树中表示为*value*。通过预测153.7，我们会得到一个均方误差（MSE）为6076.4。
- en: 'If the normalized BMI is <= 0, then we go to the left part of the tree and
    check if the normalized Glaucoma is <= 0\. If BMI is <= 0, we would account for
    approximately 59% of the data, and the MSE would reduce from 6076.4 for the parent
    node to 3612.7\. We can repeat this process until we have reached the leaf nodes
    in the tree. If we look at, say, the right-most leaf node, this corresponds to
    the following condition: if BMI > 0 and BMI > 0.1 and LDL > 0, then predict 225.8
    for 2.3% of the data, resulting in an MSE of 2757.9.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果标准化后的BMI <= 0，则我们进入树的左侧部分并检查标准化后的青光眼是否 <= 0。如果BMI <= 0，我们将处理大约59%的数据，MSE将从父节点的6076.4减少到3612.7。我们可以重复此过程，直到达到树的叶节点。如果我们查看最右侧的叶节点，这对应以下条件：如果BMI
    > 0且BMI > 0.1且LDL > 0，则对2.3%的数据预测225.8，导致MSE为2757.9。
- en: Please note that the *max_depth* for the decision tree in figure 2.13 was set
    to 3\. The complexity of this tree will increase as *max_depth* increases or as
    the number of input features increases.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，图2.13中决策树的*max_depth*被设置为3。随着*max_depth*的增加或输入特征数量的增加，此树的复杂性将增加。
- en: '![](../Images/CH02_F13_Thampi.png)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F13_Thampi.png)'
- en: Figure 2.13 Visualization of the diabetes decision tree model
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 糖尿病决策树模型可视化
- en: 'The visualization in figure 2.13 can be generated in Python using the following
    code snippet:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13中的可视化可以使用以下代码片段在Python中生成：
- en: '[PRE8]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Imports all the necessary libraries to generate and visualize the binary tree
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入所有必要的库以生成和可视化二叉树
- en: ② Initializes a string buffer to store the binary tree/graph in DOT format
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化一个字符串缓冲区以存储DOT格式的二叉树/图
- en: ③ Exports the decision tree model as a binary tree in DOT format
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 将决策树模型导出为DOT格式的二叉树
- en: ④ Generates an image of the binary tree using the DOT format string
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 使用DOT格式字符串生成二叉树的图像
- en: ⑤ Visualizes the binary tree using the Image class
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 使用Image类可视化二叉树
- en: 'Because decision trees learn a nonlinear relationship between the input features
    and the target, it is hard to understand what effects changes to each of the inputs
    have on the output. It is not as straightforward as linear regression. We can,
    however, compute the relative importance of each of the features in predicting
    the target at a global level. To compute the feature importance, we first need
    to compute the importance of a node in the binary tree. The importance of a node
    is computed as the decrease in the cost function or impurity measure for that
    node weighted by the probability of reaching that node in the tree. This is shown
    mathematically next:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 由于决策树学习输入特征与目标之间的非线性关系，很难理解每个输入的变化对输出的影响。这不如线性回归直观。然而，我们可以在全局层面上计算每个特征在预测目标时的相对重要性。为了计算特征重要性，我们首先需要计算二叉树中节点的权重。节点的权重是通过该节点在树中的概率加权，计算该节点成本函数或纯度度量的减少。这将在下面的数学公式中展示：
- en: '![](../Images/CH02_F13_Thampi_equation02.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F13_Thampi_equation02.png)'
- en: 'We can then compute the feature importance by summing up the importance of
    the nodes that split on that feature normalized by the importance of all the nodes
    in the tree. This is shown mathematically next. The feature importance for the
    decision tree is between 0 and 1, where a higher value implies greater importance:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以通过将具有该特征分割的节点的重要性求和并除以树中所有节点的重要性来计算特征重要性。这将在下面的数学公式中展示。决策树的特征重要性介于0和1之间，其中更高的值表示更大的重要性：
- en: '![](../Images/CH02_F13_Thampi_equation03.png)'
  id: totrans-153
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F13_Thampi_equation03.png)'
- en: 'In Python, the feature importance can be obtained from the Scikit-Learn decision
    tree model and plotted as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，可以从Scikit-Learn决策树模型中获取特征重要性，并按如下方式绘制：
- en: '[PRE9]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Gets feature importance from the trained decision tree model
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从训练好的决策树模型中获取特征重要性
- en: ② Sorts indices of feature weights in descending order of importance
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ② 按重要性降序排序特征权重的索引
- en: ③ Gets the feature names and feature weights in descending order of importance
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 按重要性降序获取特征名称和特征权重
- en: ④ Generates the plot shown in figure 2.14
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 生成图2.14所示的图表
- en: The features ordered in descending order of importance and their corresponding
    weights are shown in figure 2.14\. As can be seen from the figure, the order of
    important features is different from linear regression. The most important feature
    is BMI, accounting for roughly 42% of the overall model importance. The Glaucoma
    measurement is the next most important feature, accounting for roughly 15% of
    the model importance. These importance values are useful in determining what features
    have the most signal in predicting the target variable. Decision trees are immune
    to the problem of multicollinearity because the algorithm picks the feature that
    is highly correlated with the target and that most reduces the cost function or
    impurity. As a data scientist, it is important to visualize the learned decision
    tree, as shown in figure 2.13, because this will help you understand how the model
    arrived at the final prediction. You could reduce the complexity of the tree by
    setting the *max_depth* hyperparameter or by pruning the number of features you
    feed into the model. You can determine what features to prune by visualizing the
    global feature importance, as shown in figure 2.14.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 按照重要性降序排列的特征及其对应权重在图2.14中显示。如图所示，重要特征的顺序与线性回归不同。最重要的特征是BMI，占整体模型重要性的约42%。青光眼测量是下一个最重要的特征，占模型重要性的约15%。这些重要性值有助于确定哪些特征在预测目标变量时具有最大的信号。决策树算法对多重共线性问题具有免疫力，因为它选择与目标高度相关的特征，并且最能减少成本函数或纯度。作为一名数据科学家，可视化学习到的决策树（如图2.13所示）非常重要，因为这有助于你理解模型是如何得出最终预测的。你可以通过设置*max_depth*超参数或通过修剪输入到模型中的特征数量来降低树的复杂性。你可以通过可视化全局特征重要性（如图2.14所示）来确定要修剪的特征。
- en: '![](../Images/CH02_F14_Thampi.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F14_Thampi.png)'
- en: Figure 2.14 Diabetes feature importance for decision tree
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 决策树对糖尿病特征的重要性
- en: 2.4.2 Limitations of decision trees
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 决策树的局限性
- en: Decision trees are quite versatile because they can be applied to both regression
    and classification tasks, and they also have the ability to model nonlinear relationships.
    The algorithm, however, is prone to the problem of *overfitting* and the model
    is said to have *high variance*.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树非常灵活，因为它们可以应用于回归和分类任务，并且它们还具有建模非线性关系的能力。然而，该算法容易受到*过拟合*问题的影响，并且模型被认为具有*高方差*。
- en: The problem of overfitting occurs when the model *f*its the training data almost
    perfectly and, therefore, does not generalize well to data that it hasn’t seen
    before, such as the test set. This is illustrated in figure 2.15\. When a model
    overfits, you will notice really good performance on the training set but poor
    performance on the test set. This could explain why the decision tree model trained
    on the diabetes dataset performed poorer than the linear regression model.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型*拟合*训练数据几乎完美时，就会发生过拟合问题，因此它对之前未见过的数据（如测试集）的泛化能力不好。这如图2.15所示。当模型过拟合时，你会在训练集上注意到非常好的性能，但在测试集上表现较差。这可以解释为什么在糖尿病数据集上训练的决策树模型的表现不如线性回归模型。
- en: '![](../Images/CH02_F15_Thampi.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F15_Thampi.png)'
- en: Figure 2.15 The problem of overfitting (high variance)
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15 过拟合问题（高方差）
- en: The problem of overfitting can be overcome by tuning certain hyperparameters
    in the decision tree, like *max_depth*, and the minimum number of samples required
    for the leaf nodes. As shown in the visualization of the decision tree model in
    figure 2.13, one leaf node accounts for only 0.8% of the samples. This means that
    the prediction for this node is based on the data from roughly only three patients.
    By increasing the minimum number of samples required to 5 or 10, we could improve
    the performance of the model on the test set.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 通过调整决策树中的某些超参数，如*max_depth*和叶节点所需的最小样本数，可以克服过拟合问题。如图2.13中决策树模型的可视化所示，一个叶节点只占样本的0.8%。这意味着这个节点的预测仅基于大约三个患者的数据。通过将所需的最小样本数增加到5或10，我们可以提高模型在测试集上的性能。
- en: 2.5 Generalized additive models (GAMs)
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 广义加性模型（GAMs）
- en: Diagnostics+ and the doctors are reasonably happy with the two models built
    so far, but the performance is not that good. By interpreting the models, we have
    also uncovered some shortcomings. The linear regression model does not seem to
    handle features that are highly correlated with each other, such as Total Cholesterol,
    LDL, and HDL. The decision tree model performs worse than linear regression, and
    it seems to have overfit on the training data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: Diagnostics+ 和医生们对迄今为止构建的两个模型相当满意，但性能并不那么好。通过解释模型，我们也发现了某些不足。线性回归模型似乎无法处理彼此高度相关的特征，例如总胆固醇、LDL和HDL。决策树模型的表现不如线性回归，并且似乎在训练数据上过度拟合。
- en: Let’s look at one specific feature from the diabetes data. Figure 2.16 shows
    a contrived example of a nonlinear relationship between age and the target variable,
    where both variables are normalized. How would you best model this relationship
    without overfitting? One possible approach is to extend the linear regression
    model where the target variable is modeled as an *n^(th)* degree polynomial of
    the feature set. This form of regression is called polynomial regression.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看糖尿病数据中的一个特定特征。图2.16展示了年龄与目标变量之间非线性关系的虚构示例，其中两个变量都已归一化。你将如何最好地建模这种关系而不过度拟合？一个可能的方法是扩展线性回归模型，其中目标变量被建模为特征集的*n*次多项式。这种回归形式称为多项式回归。
- en: '![](../Images/CH02_F16_Thampi.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16](../Images/CH02_F16_Thampi.png)'
- en: Figure 2.16 An illustration of a nonlinear relationship for Diagnostics+ AI
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '![图2.16](../Images/CH02_F16_Thampi.png)'
- en: 'Polynomial regression for various-degree polynomials is shown in the following
    equations. In these equations, we are considering only one feature, *x*[1,] to
    model the target variable *y*. The degree 1 polynomial is the same as linear regression.
    For the degree 2 polynomial, we would add an additional feature, which is the
    square of *x*[1]. For the degree 3 polynomial, we would add two additional features—one
    that is the square of *x*[1] and the other that is the cube of *x*[1]:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 以下方程展示了不同次数的多项式回归。在这些方程中，我们只考虑一个特征*x*[1]来建模目标变量*y*。一次多项式与线性回归相同。对于二次多项式，我们会添加一个额外的特征，即*x*[1]的平方。对于三次多项式，我们会添加两个额外的特征——一个是*x*[1]的平方，另一个是*x*[1]的立方：
- en: '![](../Images/CH02_F16_Thampi_equation04.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![图2.16](../Images/CH02_F16_Thampi_equation04.png)'
- en: The weights for the polynomial regression model can be obtained using the same
    algorithm as linear regression, that is, the method of least squares using gradient
    descent. The best fit learned by each of the three polynomials is shown in figure
    2.17\. We can see that the degree 3 polynomial fits the raw data better than degrees
    2 and 1\. We can interpret a polynomial regression model the same way as we would
    a linear regression because the model is essentially a linear combination of the
    features including the higher degree features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用与线性回归相同的算法获得多项式回归模型的权重，即使用梯度下降的最小二乘法。图2.17中显示了三个多项式各自学习到的最佳拟合。我们可以看到，三次多项式比二次和一次多项式更好地拟合原始数据。我们可以像解释线性回归模型一样解释多项式回归模型，因为模型本质上是由包括高次特征在内的特征进行线性组合的。
- en: '![](../Images/CH02_F17_Thampi.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17](../Images/CH02_F17_Thampi.png)'
- en: Figure 2.17 Polynomial regression for modeling a nonlinear relationship
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17展示了用于建模非线性关系的多项式回归
- en: Polynomial regression has some limitations, however. The complexity of the model
    increases as the number of features or the dimension of the feature space increases.
    It, therefore, has a tendency to overfit on the data. It is also hard to determine
    the degree for each feature in the polynomial, especially in a higher-dimensional
    feature space.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，多项式回归也有一些局限性。随着特征数量或特征空间维度的增加，模型的复杂性也会增加。因此，它倾向于在数据上过度拟合。此外，在多项式中确定每个特征的次数也很困难，尤其是在高维特征空间中。
- en: 'So, what model can be applied to overcome all these limitations and is also
    interpretable? Enter generalized additive models (GAMs)! GAMs are models with
    medium to high predictive power and are highly interpretable. Nonlinear relationships
    are modeled by using smoothing functions for each feature and adding all of them,
    as shown in the following equation:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，哪种模型可以应用于克服所有这些限制，并且也是可解释的？欢迎广义加性模型（GAMs）！GAMs是具有中等至高预测能力和高度可解释性的模型。非线性关系通过为每个特征使用平滑函数并将它们全部相加来建模，如下方程所示：
- en: '![](../Images/CH02_F17_Thampi_equation05.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17](../Images/CH02_F17_Thampi_equation05.png)'
- en: In this equation, each feature has its own associated smoothing function that
    best models the relationship between that feature and the target. You can choose
    from many types of smoothing functions, but a widely used smoothing function is
    called *regression splines* because it is practical and computationally efficient.
    I will focus on regression splines in this book. Let’s now go deep into the world
    of GAMs using regression splines!
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程式中，每个特征都有其关联的平滑函数，该函数最好地建模了该特征与目标之间的关系。你可以选择许多类型的平滑函数，但一种广泛使用的平滑函数被称为 *回归样条*，因为它既实用又计算高效。本书将重点关注回归样条。现在，让我们深入到使用回归样条的GAMs世界吧！
- en: 2.5.1 Regression splines
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 回归样条
- en: Regression splines are represented as a weighted sum of basis functions. This
    is shown mathematically in the next equation. In this equation, *f*[j] is the
    function that models the relationship between the feature *x*[j] and the target
    variable. This function is represented as a weighted sum of basis functions where
    the weight is represented as *w*[k] and the basis function is represented as *b*[k].
    In the context of GAMs, the function *f*[j] is called a smoothing function.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 回归样条表示为基函数的加权和。这在下一个方程式中以数学形式展示。在这个方程式中，*f*[j] 是一个函数，它建模了特征 *x*[j] 与目标变量之间的关系。这个函数表示为基函数的加权和，其中权重表示为
    *w*[k]，基函数表示为 *b*[k]。在GAMs的上下文中，函数 *f*[j] 被称为平滑函数。
- en: '![](../Images/CH02_F17_Thampi_equation07.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F17_Thampi_equation07.png)'
- en: Now, what is a basis function? A basis function is a family of transformations
    that can be used to capture a general shape or nonlinear relationship. For regression
    splines, as the name suggests, splines are used as the basis function. A spline
    is a polynomial of degree *n* with *n* – `1` derivatives. It will be much easier
    to understand splines using an illustration. Figure 2.18 shows splines of various
    degrees. The top-left graph shows the simplest spline of degree 0, from which
    higher degree splines can be generated. As you can see from the top-left graph,
    six splines have been placed on a grid. The idea is to split the distribution
    of the data into portions and fit a spline on each of those portions. So, in this
    illustration, the data has been split into six portions, and we are modeling each
    portion as a degree 0 spline.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，什么是基函数？基函数是一组变换，可以用来捕捉一般形状或非线性关系。对于回归样条，正如其名称所暗示的，样条被用作基函数。样条是一个具有 *n* –
    `1` 个导数的 *n* 阶多项式。使用插图来理解样条将更容易。图2.18显示了不同阶数的样条。左上角的图表显示了最简单的0阶样条，由此可以生成更高阶的样条。正如你可以从左上角的图表中看到，六个样条被放置在一个网格上。想法是将数据的分布分割成部分，并在每一部分上拟合一个样条。因此，在这个插图中，数据被分割成六个部分，我们正在将每一部分建模为0阶样条。
- en: '![](../Images/CH02_F18_Thampi.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F18_Thampi.png)'
- en: Figure 2.18 An illustration of degree 0, degree 1, degree 2, and degree 3 splines
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.18 0阶、1阶、2阶和3阶样条的示意图
- en: A degree 1 spline, shown in the top-right graph, can be generated by convolving
    a degree 0 spline with itself. Convolution is a mathematical operation that takes
    in two functions and creates a third function that represents the correlation
    of the first function and a delayed copy of the second function. When we convolve
    a function with itself, we are essentially looking at the correlation of the function
    with a delayed copy of itself. There is a nice blog post by Christopher Olah on
    convolutions ([http://mng.bz/5Kdq](http://mng.bz/5Kdq)). By convolving a degree
    0 spline with itself, we get a degree 1 spline, which is triangular, and this
    has a continuous 0^(th)-order derivative.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 一个1阶样条，如右上角图表所示，可以通过将0阶样条与其自身卷积生成。卷积是一种数学运算，它接受两个函数并创建一个第三函数，该函数表示第一个函数与第二个函数延迟副本的相关性。当我们对一个函数与其自身卷积时，我们实际上是在查看该函数与其自身延迟副本的相关性。Christopher
    Olah有一篇关于卷积的很好的博客文章（[http://mng.bz/5Kdq](http://mng.bz/5Kdq)）。通过将0阶样条与其自身卷积，我们得到一个1阶样条，它是三角形的，并且具有连续的0阶导数。
- en: If we now convolve a degree 1 spline with itself, we will get a degree 2 spline,
    shown in the bottom-left graph. This degree 2 spline has a first-order derivative.
    Similarly, we can get a degree 3 spline by convolving a degree 2 spline, and this
    has a second-order derivative. In general, a degree *n* spline has an *n* – `1`
    derivative. In the limit, as *n* approaches infinity, we will obtain a spline
    that has the shape of a Gaussian distribution. In practice, a *degree 3 spline*,
    or *cubic spline*, is used because it can capture most general shapes.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在将一个一次样条与其自身卷积，我们将得到一个二次样条，如图中左下角的图所示。这个二次样条有一个一阶导数。同样，我们可以通过卷积一个二次样条来得到一个三次样条，它有一个二阶导数。一般来说，一个*n*次样条有一个*n*
    – `1`阶导数。在极限情况下，当*n*趋向于无穷大时，我们将获得一个具有高斯分布形状的样条。在实践中，使用三次样条，或称为三次样条，因为它可以捕捉到大多数一般形状。
- en: As mentioned earlier, in figure 2.18, we have divided the distribution of data
    into six portions and have placed six splines on the grid. In the earlier mathematical
    equation, the number of portions or splines was represented as variable *K*. The
    idea behind regression splines is to learn the weights for each of the splines
    so that you can model the distribution of the data in each of the portions. The
    number of portions or splines in the grid, *K*, is also called *degrees of freedom*.
    In general, if we place these *K* splines on a grid, we will have *K* + `3` points
    of division, also known as *knots*.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在图2.18中，我们将数据的分布分成了六个部分，并在网格上放置了六个样条。在早期的数学方程中，部分数或样条的数量用变量*K*表示。回归样条背后的想法是学习每个样条的权重，这样你就可以在每个部分中建模数据的分布。网格中部分数或样条的数量*K*也称为自由度。一般来说，如果我们在这*K*个样条上放置网格，我们将有*K*
    + `3`个分割点，也称为节点。
- en: Let’s now zoom in on cubic splines, as shown in figure 2.19\. We can see that
    there are six splines, or six degrees of freedom, resulting in nine points of
    division or knots.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来聚焦于三次样条，如图2.19所示。我们可以看到有六个样条，或者说六个自由度，导致有九个分割点或节点。
- en: '![](../Images/CH02_F19_Thampi.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F19_Thampi.png)'
- en: Figure 2.19 An illustration of splines and knots
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.19 样条和节点的示意图
- en: To capture a general shape, we need to take a weighted sum of the splines. We
    will use cubic splines here. In figure 2.20, we are using the same six splines
    overlaid to create nine knots. For the graph on the left, I have set the same
    weights for all six splines. As you can imagine, if we take an equally weighted
    sum of all six splines, we will get a horizontal straight line. This is an illustration
    of a poor fit to the raw data. For the graph on the right, however, I have taken
    an unequal weighted sum of the six splines generating a shape that perfectly fits
    the raw data. This shows the power of regression splines and GAMs. By increasing
    the number of splines or by dividing the data into more portions, we can model
    more complex nonlinear relationships. In GAMs based on regression splines, we
    individually model nonlinear relationships of each feature with the target variable
    and then add them all up to come up with the final prediction.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 为了捕捉一个一般形状，我们需要对样条进行加权求和。在这里我们将使用三次样条。在图2.20中，我们使用相同的六个样条叠加来创建九个节点。对于左边的图，我为所有六个样条设置了相同的权重。正如你可以想象的那样，如果我们对所有六个样条进行等权重求和，我们将得到一条水平直线。这是对原始数据拟合不良的说明。然而，对于右边的图，我取了六个样条的不等权重求和，生成一个完美拟合原始数据的形状。这展示了回归样条和GAMs的强大功能。通过增加样条的数量或将数据分成更多部分，我们可以建模更复杂的非线性关系。在基于回归样条的GAMs中，我们分别对每个特征与目标变量的非线性关系进行建模，然后将它们全部加起来以得出最终的预测。
- en: '![](../Images/CH02_F20_Thampi.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F20_Thampi.png)'
- en: Figure 2.20 Splines for modeling a nonlinear relationship
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.20 用于建模非线性关系的样条
- en: In figure 2.20, the weights were determined using trial and error to best describe
    the raw data. But, how do you algorithmically determine the weights for a regression
    spline that best captures the relationship between the features and the target?
    Recall from the start of this section that a regression spline is a weighted sum
    of basis functions or splines. This is essentially a linear regression problem,
    and you can learn the weights using the method of least squares and gradient descent.
    We would, however, need to specify the number of knots, or degrees of freedom.
    We can treat this as a hyperparameter and determine it using a technique called
    *cross-validation*. Using cross-validation, we would remove a portion of the data
    and fit a regression spline with a certain number of predetermined knots on the
    remaining data. This regression spline is then evaluated on the held-out set.
    The optimum number of knots is the one that results in the best performance on
    the held-out set.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 2.20 中，权重是通过试错法确定的，以最好地描述原始数据。但是，如何算法性地确定回归样条的最佳权重，以捕捉特征与目标之间的关系呢？回想一下本节开头提到的，回归样条是基函数或样条的有权总和。这本质上是一个线性回归问题，你可以使用最小二乘法和梯度下降法来学习权重。然而，我们需要指定结点的数量，或者说自由度。我们可以将其视为超参数，并使用称为
    *交叉验证* 的技术来确定它。使用交叉验证，我们会移除一部分数据，并在剩余数据上拟合一个具有预定的结点数量的回归样条。然后，在这个保留的集合上评估这个回归样条。最佳的结点数量是导致在保留集合上性能最佳的数量。
- en: In GAMs, you can easily overfit by increasing the number of splines or degrees
    of freedom. If the number of splines is high, the resulting smoothing function,
    which is a weighted sum of the splines, would be quite “wiggly”—it would start
    to fit some of the noise in the data. How can we control this wiggliness or prevent
    overfitting? We can use a technique called *regularization*. In regularization,
    we would add a term to the least square cost function that quantifies the wiggliness.
    We could then quantify the wiggliness of a smoothing function by taking the integral
    of the square of the second-order derivative of the function. Then, using a hyperparameter
    (also called regularization parameter) represented by *λ*, we could adjust the
    intensity of wiggliness. A high value for *λ* penalizes wiggliness heavily. We
    can determine *λ* the same way we determine other hyperparameters using cross-validation.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GAMs 中，通过增加样条或自由度的数量，很容易过度拟合。如果样条的数量很高，得到的平滑函数（样条的有权总和）会非常“扭曲”——它开始拟合数据中的噪声。我们如何控制这种扭曲或防止过度拟合呢？我们可以使用一种称为
    *正则化* 的技术。在正则化中，我们会在最小二乘成本函数中添加一个量化扭曲的项。然后，我们可以通过取函数的二阶导数的平方的积分来量化平滑函数的扭曲程度。然后，使用一个超参数（也称为正则化参数）*λ*
    来调整扭曲的强度。*λ* 的值越高，对扭曲的惩罚就越重。我们可以使用与确定其他超参数相同的方式，通过交叉验证来确定 *λ*。
- en: Summary of GAMs
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs 概述
- en: 'A GAM is a powerful model where the target variable is represented as a sum
    of smoothing functions representing the relationship of each of the features and
    the target. We can use the smoothing function to capture any nonlinear relationship.
    This is shown mathematically here:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: GAM 是一种强大的模型，其中目标变量被表示为一系列平滑函数的总和，这些平滑函数代表了每个特征与目标之间的关系。我们可以使用平滑函数来捕捉任何非线性关系。这在此处用数学公式表示：
- en: '*y = w*[0] + *f[1]*(*x*[1]) + *f[2]*(*x*[2]) +...+ *f[n]*(*x[n]*)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*y = w*[0] + *f[1]*(*x*[1]) + *f[2]*(*x*[2]) +...+ *f[n]*(*x[n]*)'
- en: This is a white-box model—we can easily see how each feature is transformed
    to the output using the smoothing function. A common way of representing the smoothing
    function is by using regression splines. A regression spline is represented as
    a simple weighted sum of basis functions. A basis function that is widely used
    for GAMs is the cubic spline. By increasing the number of splines or degrees of
    freedom, we can divide the distribution of data into small portions and model
    each portion piecewise. This way, we can capture very complex nonlinear relationships.
    The learning algorithm essentially has to determine the weights for the regression
    spline. We can do this the same way as for linear regression, using the method
    of least squares and gradient descent. We can determine the number of splines
    using the cross-validation technique. As the number of splines increases, GAMs
    have a tendency to overfit on the data. We can safeguard against this by using
    the regularization technique. Using a regularization parameter *λ*, we can control
    the amount of wiggliness. A higher *λ* ensures a smoother function. The parameter
    *λ* can also be determined using cross-validation.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个白盒模型——我们可以很容易地看到每个特征是如何通过平滑函数转换为输出的。表示平滑函数的一种常见方式是使用回归样条。回归样条表示为基函数的简单加权求和。GAMs中广泛使用的基函数是三次样条。通过增加样条的数量或自由度，我们可以将数据的分布划分为小部分，并逐部分建模。这样，我们可以捕捉非常复杂的非线性关系。学习算法本质上必须确定回归样条的权重。我们可以像线性回归一样使用最小二乘法和梯度下降法来做这件事。我们可以使用交叉验证技术来确定样条的数量。随着样条数量的增加，GAMs倾向于在数据上过拟合。我们可以通过使用正则化技术来防止这种情况。使用正则化参数*λ*，我们可以控制曲线的波动程度。较高的*λ*确保函数更加平滑。参数*λ*也可以通过交叉验证来确定。
- en: 'GAMs can also be used to model interactions between variables. GA2M, shown
    mathematically next, is a type of GAM that models pairwise interactions:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: GAMs也可以用于建模变量之间的交互。GA2M，如数学上所示，是一种建模成对交互的GAM：
- en: '![](../Images/CH02_F20_Thampi_equation08.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F20_Thampi_equation08.png)'
- en: With the help of subject matter experts (SMEs)—the doctors in the Diagnostics+
    example—you can determine what feature interactions need to be modeled. You could
    also look at the correlation between features to understand what features need
    to be modeled together.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在主题专家（SMEs）——Diagnostics+示例中的医生——的帮助下，你可以确定需要建模哪些特征交互。你也可以查看特征之间的相关性，以了解需要一起建模哪些特征。
- en: 'In Python, you can use a package called pyGAM to build and train GAMs. It is
    inspired by the GAM implementation in the popular mgcv package in R. You can install
    pyGAM in your Python environment using the pip package as follows:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中，你可以使用一个名为pyGAM的包来构建和训练GAMs。它受到了R中流行的mgcv包中GAM实现的启发。你可以使用pip包在你的Python环境中安装pyGAM，如下所示：
- en: '[PRE10]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 2.5.2 GAM for Diagnostics+ diabetes
  id: totrans-209
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 GAM用于Diagnostics+糖尿病
- en: 'Let’s now go back to the Diagnostics+ example to train a GAM to predict diabetes
    progression using all 10 features. Note that the Sex of the patient is a categorical
    or discrete feature. It does not make sense to model this feature using a smoothing
    function. We can treat such categorical features in GAMs as factor terms. We can
    train the GAM using the pyGAM package as follows. As with decision trees, I’m
    not going to repeat the code that loads the diabetes dataset and splits it into
    the train and test sets. Please refer to section 2.2 for that snippet of code:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们回到Diagnostics+示例，使用所有10个特征来训练一个GAM以预测糖尿病进展。请注意，患者的性别是一个分类或离散特征。使用平滑函数来模拟这个特征是没有意义的。我们可以在GAM中将此类分类特征视为因子项。我们可以使用pyGAM包如下训练GAM。与决策树一样，我不会重复加载糖尿病数据集并将其拆分为训练集和测试集的代码。请参阅第2.2节以获取该代码片段：
- en: '[PRE11]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ① Imports the LinearGAM class from pygam that can be used to train a GAM for
    regression tasks
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从pygam导入LinearGAM类，可用于训练回归任务的GAM
- en: ② Imports the smoothing term function to be used for numerical features
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ② 导入用于数值特征的平滑项函数
- en: ③ Imports the factor term function to be used for categorical features
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 导入用于分类特征的因子项函数
- en: ④ Cubic spline term for the Age feature
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 年龄特征的立方样条项
- en: ⑤ Factor term for the Sex feature, which is categorical
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 性别特征的因子项，这是一个分类特征
- en: ⑥ Cubic spline term for the BMI feature
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 体质指数（BMI）特征的立方样条项
- en: ⑦ Cubic spline term for the BP feature
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 血压（BP）特征的立方样条项
- en: ⑧ Cubic spline term for the Total Cholesterol feature
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 总胆固醇特征的立方样条项
- en: ⑨ Cubic spline term for the LDL feature
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 低密度脂蛋白（LDL）特征的立方样条项
- en: ⑩ Cubic spline term for the HDL feature
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ HDL特征的立方样条项
- en: ⑪ Cubic spline term for the Thyroid feature
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ Thyroid特征的立方样条项
- en: ⑫ Cubic spline term for the Glaucoma feature
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ Glaucoma特征的立方样条项
- en: ⑬ Cubic spline term for the Glucose feature
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ Glucose特征的立方样条项
- en: ⑭ Maximum number of splines to be used for each feature
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 每个特征要使用的最大样条数量
- en: ⑮ Uses grid search to perform training and cross-validation to determine the
    number of splines, the regularization parameter lambda, and the optimum weights
    for the regression splines for each feature
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 使用网格搜索进行训练和交叉验证，以确定每个特征的样条数量、正则化参数lambda以及回归样条的优化权重
- en: ⑯ Uses the trained GAM model to predict on the test
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ⑯ 使用训练好的GAM模型在测试上进行预测
- en: ⑰ Evaluates the performance of the model on the test set using the MAE metric
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ⑰ 使用MAE指标评估模型在测试集上的性能
- en: Now for the moment of truth! How did the GAM perform? The MAE performance of
    the GAM is 41.4—a pretty good improvement when compared to the linear regression
    and decision tree models. A comparison of the performance of all three models
    is summarized in table 2.2\. I have also included the performance of a baseline
    model that Diagnostics+ and the doctors have been using where they look at the
    median diabetes progression across all patients. All models are compared against
    the baseline to show how much of an improvement the models give to the doctors.
    It looks like GAM is the best model across all performance metrics.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是检验真伪的时刻！GAM的表现如何？GAM的MAE性能为41.4，与线性回归和决策树模型相比，这是一个相当不错的改进。所有三个模型性能的比较总结在表2.2中。我还包括了Diagnostics+和医生们一直在使用的基线模型的表现，他们在所有患者中查看糖尿病进展的中位数。所有模型都与基线进行比较，以显示模型为医生带来的改进程度。看起来GAM在所有性能指标上都是最好的模型。
- en: Table 2.2 Performance comparison of linear regression, decision tree, and GAM
    against a baseline for Diagnostics+ A
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.2线性回归、决策树和GAM相对于Diagnostics+基线的性能比较
- en: '|  | MAE | RMSE | MAPE |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '|  | MAE | RMSE | MAPE |'
- en: '| Baseline | 62.2 | 74.7 | 51.6 |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '| 基线 | 62.2 | 74.7 | 51.6 |'
- en: '| Linear regression | 42.8 (–19.4) | 53.8 (–20.9) | 37.5 (–14.1) |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '| 线性回归 | 42.8 (–19.4) | 53.8 (–20.9) | 37.5 (–14.1) |'
- en: '| Decision tree | 48.6 (–13.6) | 60.5 (–14.2) | 44.4 (–7.2) |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '| 决策树 | 48.6 (–13.6) | 60.5 (–14.2) | 44.4 (–7.2) |'
- en: '| GAM | 41.4 (–20.8) | 52.2 (–22.5) | 35.7 (–15.9) |'
  id: totrans-235
  prefs: []
  type: TYPE_TB
  zh: '| GAM | 41.4 (–20.8) | 52.2 (–22.5) | 35.7 (–15.9) |'
- en: We have now seen the predictive power of GAMs. We could potentially get further
    improvement in the performance by modeling feature interactions, especially the
    cholesterol features with each other and with other features that are potentially
    highly correlated, like BMI. As an exercise, I encourage you to try modeling feature
    interactions using GAMs.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经看到了GAM的预测能力。通过建模特征交互，特别是胆固醇特征之间的交互，以及与其他可能高度相关的特征（如BMI）的交互，我们有可能进一步提高性能。作为一个练习，我鼓励你尝试使用GAM建模特征交互。
- en: GAMs are white box and can be easily interpreted. In the following section,
    we will see how GAMs can be interpreted.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: GAM是白盒模型，可以很容易地解释。在下一节中，我们将看到如何解释GAM。
- en: GAMs for classification tasks
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: GAM分类任务
- en: 'GAMs can also be used to train a binary classifier by using the logistic link
    function where the response *y* can be either 0 or 1\. In the pyGAM package, you
    can make use of the logistic GAM for binary classification problems as follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: GAM也可以通过使用逻辑链接函数来训练二元分类器，其中响应*y*可以是0或1。在pyGAM包中，你可以使用逻辑GAM来解决二元分类问题，如下所示：
- en: '[PRE12]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 2.5.3 Interpreting GAMs
  id: totrans-241
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.3 解释GAM
- en: Although each smoothing function is obtained as a linear combination of basis
    functions, the final smoothing function for each feature is nonlinear, and, therefore,
    we cannot interpret the weights the same way as we do for linear regression. We
    can, however, easily visualize the effects of each feature on the target using
    partial dependence or partial effects plots. Partial dependence looks at the effect
    of each feature by marginalizing on the rest. It is highly interpretable because
    we can see the average effect of each feature value on the target variable. We
    can see whether the target response to the feature is linear, nonlinear, monotonic,
    or nonmonotonic. Figure 2.21 shows the effect of each of the patient features
    on the target variable. The 95% confidence interval around them have also been
    plotted. This will help us determine the sensitivity of the model to data points
    with a low sample size.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然每个平滑函数都是通过基函数的线性组合获得的，但每个特征的最终平滑函数是非线性的，因此我们不能像线性回归那样解释权重。然而，我们可以通过部分依赖或部分效应图轻松地可视化每个特征对目标的影响。部分依赖通过边缘化其他特征来观察每个特征的影响。它非常易于解释，因为我们可以看到每个特征值对目标变量的平均影响。我们可以看到目标对特征的反应是线性、非线性、单调还是非单调。图2.21显示了每个病人特征对目标变量的影响。它们周围的95%置信区间也已绘制。这将帮助我们确定模型对样本量较小的数据点的敏感性。
- en: Let’s now look at a couple of features in figure 2.21, namely, BMI and BP. The
    effect of BMI on the target variable is shown in the bottom-left graph. On the
    *x*-axis, we see the normalized values of BMI, and on the *y*-axis, we see the
    effect that BMI has on the progression of diabetes for the patient. We see that
    as BMI increases, the effect on the progression of diabetes also increases. We
    see a similar trend for BP shown by the bottom-right graph. We see that the higher
    the BP, the higher the impact on the progression of diabetes. If we look at the
    95% confidence interval lines (the dashed lines in figure 2.21), we see a wider
    confidence interval around the lower and higher ends of BMI and BP. This is because
    fewer samples of patients exist at this range of values, resulting in higher uncertainty
    in understanding the effects of these features at those ranges.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看图2.21中的几个特性，即BMI和BP。BMI对目标变量的影响显示在左下角的图表中。在*x*轴上，我们看到BMI的标准化值，而在*y*轴上，我们看到BMI对病人糖尿病进展的影响。我们看到随着BMI的增加，对糖尿病进展的影响也增加。右下角的图表显示了类似的趋势，BP越高，对糖尿病进展的影响也越大。如果我们观察95%置信区间线（图2.21中的虚线），我们会看到BMI和BP的上下端附近的置信区间更宽。这是因为在这个值范围内的病人样本较少，导致对这些特征在该范围内的效果的认知存在更高的不确定性。
- en: '![](../Images/CH02_F21_Thampi.png)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F21_Thampi.png)'
- en: Figure 2.21 The effect of each of the patient features on the target variable
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.21 每个病人特征对目标变量的影响
- en: 'The code to generate figure 2.21 follows:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图2.21的代码如下：
- en: '[PRE13]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① Locations of the four graphs in the 2x2 Matplotlib grid
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ① 四个图表在2x2 Matplotlib网格中的位置
- en: ② Creates a 2x2 grid of Matplotlib graphs
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个2x2的Matplotlib图表网格
- en: ③ Iterates through the four patient metadata features
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 遍历四个病人元数据特征
- en: ④ Gets the location of feature in the 2x2 grid
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取特征在2x2网格中的位置
- en: ⑤ Generates the partial dependence of the feature values with the target marginalizing
    on the other features
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 生成特征值的部分依赖，目标对其他特征进行边缘化
- en: ⑥ Plots the partial dependence values as a solid line
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 以实线绘制部分依赖值
- en: ⑦ Plots the 95% confidence interval around the partial dependence values as
    a dashed line
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 以虚线绘制部分依赖值的95%置信区间
- en: ⑧ Adds labels for the x- and y-axes
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 为x轴和y轴添加标签
- en: 'Figure 2.22 shows the effect of each of the six blood test measurements on
    the target. As an exercise, observe the effects that features like Total Cholesterol,
    LDL, HDL, and Glaucoma have on the progression of diabetes. What can you say about
    the impact of higher LDL values (or bad cholesterol) on the target variable? Why
    does higher Total Cholesterol have a lower impact on the target variable? To answer
    these questions, let’s look at a few patient cases with very high cholesterol
    values. The following code snippet will help you zoom in on those patients:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.22显示了六个血液检测指标对目标的影响。作为一个练习，观察总胆固醇、LDL、HDL和青光眼等特征对糖尿病进展的影响。你能说些什么关于更高LDL值（或坏胆固醇）对目标变量的影响？为什么更高总胆固醇对目标变量的影响较小？为了回答这些问题，让我们看看一些胆固醇值非常高的病人案例。以下代码片段将帮助您聚焦这些病人：
- en: '[PRE14]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: If you execute this code, you will see only one patient out of 442 that has
    a Total Cholesterol reading greater than 0.15 and an LDL reading greater than
    0.19\. The fasting glucose level for this patient one year out (the target variable)
    seems to be 84, which is in the normal range. This could explain why in figure
    2.22 we are seeing a very large negative effect for Total Cholesterol on the target
    variable for a range that is greater than 0.15\. The negative effect of Total
    Cholesterol seems to be greater than the positive effect the bad LDL cholesterol
    seems to have on the target. The confidence interval seems much wider in these
    range of values. The model may have overfit on this one outlier patient record,
    and so, we should not read too much into these effects. By observing these effects,
    we can identify cases or a range of values where the model is sure of the prediction
    and cases where there is high uncertainty. For high uncertainty cases, we can
    go back to the diagnostics center to collect more patient data so that we have
    a representative sample.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你执行此代码，你将看到 442 个病人中只有一个病人的总胆固醇读数大于 0.15，并且低密度脂蛋白读数大于 0.19。这位病人在一年后的空腹血糖水平（目标变量）似乎为
    84，处于正常范围内。这可以解释为什么在图 2.22 中，我们看到了总胆固醇对目标变量在大于 0.15 的范围内的非常显著的负影响。总胆固醇的负影响似乎大于坏的低密度脂蛋白胆固醇对目标的正面影响。这些值范围内的置信区间似乎要宽得多。模型可能对这个异常病人的记录过度拟合，因此，我们不应过分解读这些影响。通过观察这些影响，我们可以识别出模型对预测有信心和存在高度不确定性的案例或值范围。对于高度不确定的案例，我们可以回到诊断中心收集更多病人数据，以便我们有代表性的样本。
- en: '![](../Images/CH02_F22_Thampi.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH02_F22_Thampi.png)'
- en: Figure 2.22 The effect of each of the blood test measurements on the target
    variable
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.22 每个血液检测测量对目标变量的影响
- en: 'The code to generate figure 2.22 follows:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 生成图 2.22 的代码如下：
- en: '[PRE15]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ① Locations of the six graphs in the 3 × 2 Matplotlib grid
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: ① 六个图表在 3 × 2 Matplotlib 网格中的位置
- en: ② Creates a 3 × 2 grid of Matplotlib graphs
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建一个 3 × 2 网格的 Matplotlib 图表
- en: ③ Iterates through the six blood test measurement features
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 遍历六个血液检测测量特征
- en: ④ Gets the location of the feature in the 3 × 2 grid
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 获取特征在 3 × 2 网格中的位置
- en: ⑤ Generates the partial dependence of the feature values with the target marginalizing
    on the other features grid of Matplotlib graphs
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 生成特征值与目标变量对其他特征网格的边际化相关的部分依赖图
- en: ⑥ Plots the partial dependence values as a solid line
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 以实线绘制部分依赖值
- en: ⑦ Plots the 95% confidence interval around the partial dependence values as
    a dashed line
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 以虚线绘制部分依赖值周围的 95% 置信区间
- en: ⑧ Adds labels for the x- and y-axes
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 为 x 轴和 y 轴添加标签
- en: Through figures 2.21 and 2.22, we can gain a much deeper understanding of the
    marginal effect of each of the feature values on the target. The partial dependence
    plots are useful for debugging any issues with the model. By plotting the 95%
    confidence interval around the partial dependence values, we can also see data
    points with low sample sizes. If a feature value with a low sample size has a
    dramatic effect on the target, then there could be an overfitting problem. We
    can also visualize the wiggliness of the smoothing function to determine whether
    the model has fit on the noise in the data. We can fix these overfitting problems
    by increasing the value of the regularization parameter. These partial dependence
    plots can also be shared with the SME—doctors, in this case—for validation which
    will help gain their trust.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 通过图 2.21 和 2.22，我们可以更深入地了解每个特征值对目标的边际效应。部分依赖图对于调试模型中的任何问题都很有用。通过绘制部分依赖值周围的 95%
    置信区间，我们还可以看到样本量较小的数据点。如果一个特征值在样本量较小的情况下对目标有显著影响，那么可能存在过度拟合问题。我们还可以可视化平滑函数的波动性，以确定模型是否拟合了数据中的噪声。我们可以通过增加正则化参数的值来解决这些过度拟合问题。这些部分依赖图也可以与
    SME（在这种情况下是医生）共享，以进行验证，这将有助于赢得他们的信任。
- en: 2.5.4 Limitations of GAMs
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.4 GAMs 的局限性
- en: 'We have so far seen the advantages of GAMs in terms of predictive power and
    interpretability. GAMs have a tendency to overfit, although this can be overcome
    with regularization. You do need to be aware of the following other limitations,
    however:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了 GAMs 在预测能力和可解释性方面的优势。GAMs 有过度拟合的倾向，尽管可以通过正则化来克服。然而，你需要注意以下其他局限性：
- en: GAMs are sensitive to feature values outside of the range in the training set
    and tend to lose predictive power when exposed to outlier values.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs对训练集中范围之外的特性值敏感，并且当暴露于异常值时往往会失去预测能力。
- en: For mission-critical tasks, GAMs may sometimes have limited predictive power,
    in which case you may need to consider more powerful black-box models.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于关键任务，GAMs有时可能预测能力有限，在这种情况下，你可能需要考虑更强大的黑盒模型。
- en: 2.6 Looking ahead to black-box models
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 展望黑盒模型
- en: 'Black-box models are models with really high predictive power and are typically
    applied in tasks for which model performance (such as accuracy) is extremely important.
    They are, however, inherently opaque, and the characteristics that make them opaque
    include the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒模型是具有极高预测能力的模型，通常应用于模型性能（如准确度）极为重要的任务。然而，它们本质上是不可透见的，使它们不可透见的特征包括以下内容：
- en: The machine learning process is complicated, and you can’t easily understand
    how the input features are transformed into the output or target variable.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习过程很复杂，你无法轻易理解输入特征是如何转换为输出或目标变量的。
- en: You can’t easily identify the most important features to predict the target
    variable.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你无法轻易识别出预测目标变量最重要的特征。
- en: Examples of black-box models are tree ensembles such as random forest and gradient-boosted
    trees, deep neural networks (DNNs), convolutional neural networks (CNNs), and
    recurrent neural networks (RNNs). Table 2.3 shows the machine learning tasks for
    which these models are typically applied.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒模型的例子包括随机森林和梯度提升树等树集成模型，深度神经网络（DNNs），卷积神经网络（CNNs）和循环神经网络（RNNs）。表2.3显示了这些模型通常应用的机器学习任务。
- en: Table 2.3 Mapping of black-box model to machine learning tasks
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.3 黑盒模型到机器学习任务的映射
- en: '| Black-box model | Machine learning tasks |'
  id: totrans-282
  prefs: []
  type: TYPE_TB
  zh: '| 黑盒模型 | 机器学习任务 |'
- en: '| Tree ensembles (random forest, gradient-boosted trees) | Regression and classification
    |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '| 树集成（随机森林、梯度提升树） | 回归和分类 |'
- en: '| Deep neural networks (DNNs) | Regression and classification |'
  id: totrans-284
  prefs: []
  type: TYPE_TB
  zh: '| 深度神经网络（DNNs） | 回归和分类 |'
- en: '| Convolutional neural networks (CNNs) | Image classification, object detection
    |'
  id: totrans-285
  prefs: []
  type: TYPE_TB
  zh: '| 卷积神经网络（CNNs） | 图像分类、目标检测 |'
- en: '| Recurrent neural networks (RNNs) | Sequence modeling, language understanding
    |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| 循环神经网络（RNNs） | 序列建模、语言理解 |'
- en: I have now plotted in the black-box models in the same predictive power versus
    interpretability plane as introduced in section 2.1, shown in figure 2.23.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 我现在已经在与2.1节中介绍的预测能力与可解释性平面相同的黑盒模型中进行了绘制，如图2.23所示。
- en: '![](../Images/CH02_F23_Thampi.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH02_F23_Thampi.png)'
- en: Figure 2.23 Black-box models on the interpretability versus predictive power
    plane
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.23 黑盒模型在可解释性与预测能力平面上的分布
- en: The black-box models are clustered in the top left of the plane because they
    have high predictive power but low interpretability. For mission-critical tasks,
    it is important not to trade off model performance (such as accuracy) for interpretability
    by applying white-box models. We will need to apply black-box models for such
    tasks and will need to find ways to interpret them. We can interpret black-box
    models in multiple ways, and doing so is the main focus of the remaining chapters
    in this book. In the next chapter, we will specifically focus on tree ensembles
    and how to interpret them using global, model-agnostic techniques.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 黑盒模型因为具有高预测能力但低可解释性而被聚类在平面的左上角。对于关键任务，重要的是不要通过应用白盒模型来牺牲模型性能（如准确度）以换取可解释性。我们需要应用黑盒模型来完成这些任务，并且需要找到解释它们的方法。我们可以以多种方式解释黑盒模型，而这本书剩余章节的主要焦点。在下一章中，我们将特别关注树集成以及如何使用全局、模型无关的技术来解释它们。
- en: Summary
  id: totrans-291
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: White-box models are inherently transparent. The machine learning process is
    straightforward to understand, and you can clearly interpret how the input features
    are transformed into the output. Using white-box models, you can identify the
    most important features, and those features are understandable.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 白盒模型本质上是透明的。机器学习过程易于理解，你可以清楚地解释输入特征是如何转换为输出的。使用白盒模型，你可以识别出最重要的特征，而这些特征是可理解的。
- en: Linear regression is one of the simplest white-box models, where the target
    variable is modeled as a linear combination of the input features. You can determine
    the weights using the method of least squares and gradient descent.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归是最简单的白盒模型之一，其中目标变量被建模为输入特征的线性组合。你可以使用最小二乘法和梯度下降法确定权重。
- en: We can implement linear regression in Python using the `LinearRegression` class
    in the Scikit-Learn package. You can interpret the model by inspecting the coefficients
    or learned weights. The weights can also be used to determine the importance of
    each of the features. Linear regression, however, suffers from the problems of
    multicollinearity and underfitting.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用Scikit-Learn包中的`LinearRegression`类在Python中实现线性回归。你可以通过检查系数或学习到的权重来解释模型。权重也可以用来确定每个特征的重要性。然而，线性回归却存在多重共线性和高偏差的问题。
- en: A decision tree is a slightly more advanced white-box model that can be used
    for both regression and classification tasks. You can predict the target variable
    by splitting the data across all features to minimize a cost function. You have
    learned the CART algorithm to learn the splits.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是一种稍微高级一点的白盒模型，可以用于回归和分类任务。你可以通过将数据分割到所有特征上以最小化成本函数来预测目标变量。你已经学习了CART算法来学习分割。
- en: A decision tree for regression tasks can be implemented in Python using the
    `DecisionTreeRegressor` class in Scikit-Learn. You can implement a decision tree
    for classification tasks using the `DecisionTreeClassifier` class in Scikit-Learn.
    You can interpret a decision tree learned using CART by visualizing it as a binary
    tree. The Scikit-Learn implementation also computes the feature importance for
    you. A decision tree can be used to model nonlinear relationships but tends to
    suffer from overfitting.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn中的`DecisionTreeRegressor`类，可以在Python中实现回归任务的决策树。你可以使用Scikit-Learn中的`DecisionTreeClassifier`类实现分类任务的决策树。你可以通过将CART学习到的决策树可视化成二叉树来解释它。Scikit-Learn的实现还为你计算了特征重要性。决策树可以用来建模非线性关系，但往往容易过度拟合。
- en: GAMs are a powerful white-box model where the target variable is represented
    as a sum of smoothing functions representing the relationship of each of the features
    and the target. You know that regression splines and cubic splines are widely
    used to represent the smoothing function.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAMs是一种强大的白盒模型，其中目标变量被表示为一系列平滑函数的总和，这些平滑函数代表了每个特征与目标之间的关系。你知道回归样条和三次样条广泛用于表示平滑函数。
- en: Regression splines and GAMs can be implemented using the pyGAM package in Python.
    We can use the `LinearGAM` class for regression tasks and the `LogisticGAM` class
    for classification tasks. You can interpret a GAM by plotting the partial dependence
    of each of the features on the target. GAMs have a tendency to overfit, but this
    problem can be mitigated through regularization.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归样条和广义加性模型（GAMs）可以使用Python中的pyGAM包实现。我们可以使用`LinearGAM`类进行回归任务，使用`LogisticGAM`类进行分类任务。你可以通过绘制每个特征对目标变量的部分依赖性来解释GAM。GAMs有过度拟合的倾向，但这个问题可以通过正则化来缓解。
- en: Black-box models are models with really high predictive power and are typically
    applied to tasks for which model performance (such as accuracy) is extremely important.
    They are, however, inherently opaque. The machine learning process is complicated,
    and you can’t easily understand how the input features are transformed into the
    output or target variable. As a result, you can’t easily identify the most important
    features to predict the target variable.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黑盒模型是具有极高预测能力的模型，通常应用于模型性能（如准确率）极其重要的任务。然而，它们本质上是透明的。机器学习过程很复杂，你无法轻易理解输入特征是如何转换成输出或目标变量的。因此，你无法轻易识别出预测目标变量最重要的特征。
