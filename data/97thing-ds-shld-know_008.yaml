- en: Chapter 7\. Understanding Passive Versus Proactive Ethics
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第7章。理解被动与主动伦理
- en: Bill Schmarzo
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 比尔·施马尔佐
- en: '![](Images/Bill_Schmarzo.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Bill_Schmarzo.png)'
- en: Chief Innovation Officer, Hitachi Vantara
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 日立研究院首席创新官
- en: Several friends have challenged me to get involved in the AI ethics discussion.
    I certainly do not have any special ethics training. But then again, maybe I do.
    I’ve been going to church most Sundays (not just on Christmas Eve) since I was
    a kid, and have been taught a multitude of “ethics” lessons from the Bible. So,
    respectfully, let me take my best shot at sharing my thoughts about the critical
    importance of the AI ethics topic.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 几位朋友挑战我参与 AI 伦理讨论。我确实没有接受过特殊的伦理培训。但话又说回来，也许我有。我从小就几乎每个星期天都去教堂（不仅仅是圣诞节前夕），并从《圣经》中学到了众多的“伦理”教训。所以，请恕我尽力分享我对
    AI 伦理主题重要性的看法。
- en: What Is AI Ethics?
  id: totrans-5
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 什么是 AI 伦理？
- en: '*Ethics* is defined as the *moral principles* that govern a person’s behavior
    or actions—the principles of “right and wrong” that are generally accepted by
    an individual or a social group. “Right or wrong” behaviors are not easily codified
    in a simple mathematical equation. And this is what makes the AI ethics discussion
    so challenging and so important.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*伦理*被定义为统治一个人行为或行动的*道德原则* —— 一个个人或社会群体普遍接受的“对和错”原则。 “对或错”的行为不容易用简单的数学方程式来编码。这也是为什么
    AI 伦理讨论如此具有挑战性和重要性的原因。'
- en: 'To understand the AI ethics quandary, one must first understand how an AI model
    makes decisions:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 AI 伦理困境，首先必须理解 AI 模型如何做出决策：
- en: The AI model relies on the creation of “AI rational agents” that interact with
    the environment to learn the *rewards and penalties* associated with actions.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: AI 模型依赖于创建与环境交互的“AI 合理代理人”，以学习与动作相关的*奖励和惩罚*。
- en: The rewards and penalties against which the “AI rational agents” seek to make
    the “right” decisions are framed by the *AI utility function*.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: “AI 合理代理人”寻求做出“正确”决策的奖励和惩罚是由*AI 效用函数*界定的。
- en: To create an “AI rational agent” that makes the “right” decision, the AI utility
    function must comprise a holistic definition of *“value”* that includes financial/economic,
    operational, customer, societal, environmental, and spiritual values.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要创建一个“AI 合理代理人”来做出“正确”决策，AI 效用函数必须包含一个综合定义的*“价值”*，其中包括财务/经济、运营、客户、社会、环境和精神价值。
- en: 'Bottom line: the “AI rational agent” determines “right” and “wrong” based on
    the definition of “value” as articulated in the AI utility function.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 底线：AI 合理代理人根据AI 效用函数中表达的“价值”定义来确定“对”和“错”。
- en: Simple, right?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 简单吧？
- en: It isn’t the AI models that scare me. My experience to date is that AI models
    work great. But the thing is, AI models will strive to optimize exactly what humans
    have programmed them to optimize via the AI utility function.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 吓唬我的不是 AI 模型。迄今为止，我的经验是 AI 模型表现得非常出色。但问题是，AI 模型将努力通过 AI 效用函数所编程的方式来优化精确地。
- en: And that’s where we should focus the AI ethics conversation, because *humans
    tend to make poor decisions.* Just visit Las Vegas if you doubt that statement.
    The effort by humans to define the rules against which actions will be measured
    sometimes results in unintended consequences.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们应该集中 AI 伦理讨论的地方，因为*人类往往做出糟糕的决定*。如果你怀疑这一说法，只需访问拉斯维加斯即可。人类努力定义衡量行动的规则有时会导致意外后果。
- en: The Ramifications of Unintended Consequences
  id: totrans-15
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 意外后果的后果
- en: 'Shortcutting the process of defining the measures against which to monitor
    any complicated business initiative is naive...and ultimately dangerous. The article
    [“10 Fascinating Examples of Unintended Consequences”](https://oreil.ly/sExdR)
    details actions “believed to be good” that ultimately led to disastrous outcomes,
    including:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监控任何复杂商业计划的定义措施抄近路是幼稚的…最终是危险的。文章[“10个令人着迷的意外后果示例”](https://oreil.ly/sExdR)详细描述了被认为是好的行动，最终导致灾难性后果，包括：
- en: The SS *Eastland,* a badly designed and ungainly vessel, was intended to be
    made safer by adding several lifeboats. Unfortunately, the extra weight of the
    lifeboats caused the ship to capsize, thereby trapping and killing 800 passengers
    below the decks.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SS *Eastland*，一艘设计不良且笨重的船只，本意是通过增加几艘救生艇来增加安全性。不幸的是，救生艇的额外重量导致船只倾覆，从而使下层800名乘客被困并丧生。
- en: The Treaty of Versailles dictated surrender terms to Germany to end World War
    I. Unfortunately, the terms empowered Adolf Hitler and his followers, leading
    to World War II.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凡尔赛条约规定了德国的投降条件，以结束第一次世界大战。不幸的是，这些条件赋予了阿道夫·希特勒及其追随者权力，导致了第二次世界大战的爆发。
- en: The Smokey Bear Wildfire Prevention campaign created decades of highly successful
    fire prevention. Unfortunately, this disrupted normal fire processes that are
    vital to the health of forests. The result is megafires that destroy everything
    in their path, even huge pine trees that had stood for several thousand years
    through normal fire conditions.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Smokey Bear 森林火灾预防运动创造了几十年来极其成功的防火效果。不幸的是，这扰乱了对森林健康至关重要的正常火灾过程。结果是超级大火摧毁了一切，甚至是那些在正常火灾条件下已经屹立数千年的巨大松树。
- en: One can mitigate unintended consequences and the costs associated with false
    positives and false negatives by *bringing together diverse and even conflicting
    perspectives* to thoroughly debate and define the AI utility function.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过*汇聚多样化甚至是冲突的观点*，全面讨论和定义AI效用函数，可以减轻意外后果和假阳性与假阴性相关的成本。
- en: Defining the AI Utility Function
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 定义AI效用函数
- en: As mentioned earlier, to create a “rational AI agent” that understands how to
    differentiate between “right” and “wrong” actions, the AI model must work off
    of a holistic AI utility function that contemplates “value” across a variety of
    often conflicting dimensions—for example, increase financial value, while also
    reducing operational costs and risks, *and* improving customer satisfaction and
    likelihood to recommend, *and* improving societal value and quality of life, *and*
    reducing environmental impact and carbon footprint.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，要创建一个能够区分“对”与“错”行为的“理性AI代理”，AI模型必须依赖于一个全面的AI效用函数，考虑到“价值”在各种常常冲突的维度上——例如，增加财务价值，同时降低运营成本和风险，*并且*提高客户满意度和推荐可能性，*并且*提高社会价值和生活质量，*并且*减少环境影响和碳足迹。
- en: 'And ethics *must* be one of those value dimensions if we are to create AI utility
    functions that can lead AI to the right decisions. This brings us to a very important
    concept: the difference between *passive ethics* and *proactive ethics*.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 伦理学必须成为这些价值维度之一，如果我们要创建能够引导人工智能做出正确决策的AI效用函数。这让我们思考一个非常重要的概念：*被动伦理*与*主动伦理*之间的区别。
- en: Passive Ethics Versus Proactive Ethics
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 被动伦理与主动伦理
- en: When debating ethics, we must contemplate the dilemma of passive ethics versus
    proactive ethics. And it starts with a story that many of us learned at a very
    young age—the parable of the Good Samaritan.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在讨论伦理问题时，我们必须考虑被动伦理与主动伦理的两难境地。这一切都始于我们很小的时候就听过的一个故事——善 Samaritan 的寓言。
- en: The story is of a Jewish traveler who is stripped of his clothing, beaten, and
    left for dead alongside the road. First a priest and then a Levite come by, but
    both cross the road to avoid the man. Finally, a Samaritan happens upon the battered
    traveler and helps him. The Samaritan bandages his wounds, transports him to an
    inn on his beast of burden to rest and heal, and pays for the traveler’s care
    and accommodations at the inn.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 故事讲述了一个犹太旅行者被剥夺衣物，被打败，倒在路旁等待死亡。先是一个祭司，然后是一个利未人路过，但他们都选择绕道而行，避开那个人。最后，一个撒玛利亚人路过，看到这个受伤的旅行者，决定帮助他。撒玛利亚人包扎他的伤口，用自己的牲口把他送到旅馆休息和疗养，并为旅行者在旅馆里的照料和住宿支付费用。
- en: The priest and the Levite both operated under the passive ethics philosophy
    of “do no harm.” Technically, they did nothing wrong. The Samaritan operated under
    the proactive ethics philosophy of seeking to “do good.”
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 祭司和利未人都秉持着“不造成伤害”的被动伦理哲学。从技术上讲，他们并没有做错什么。而撒玛利亚人则奉行主动伦理，积极寻求“做好事”。
- en: The “do no harm” mindset is totally insufficient in a world driven by AI models.
    Our AI models must embrace proactive ethics by seeking to “do good”; that is,
    every AI model and the AI utility function that guides the operations of the AI
    model must proactively seek to do good.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在由AI模型驱动的世界中，“不造成伤害”的思维方式是完全不足够的。我们的AI模型必须采纳主动伦理，努力“做好事”；也就是说，每一个AI模型以及指导其运作的AI效用函数必须积极地寻求做好事。
- en: There is a *huge* difference between “do no harm” and “do good,” as the parable
    of the Good Samaritan well demonstrates.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: “不造成伤害”和“做好事”之间有着巨大的差别，善 Samaritan 的寓言很好地展示了这一点。
- en: Summary
  id: totrans-30
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 总结
- en: 'Let’s consider a simple ethics test that I call the “Mom test.” Here’s how
    it works: if you were to tell your mom of a decision or action you took in a particular
    matter, would she be proud of or disappointed in your choice? That simple test
    would probably minimize many of our AI ethics concerns.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个简单的伦理测试，我称之为“妈妈测试”。它是这样工作的：如果你告诉你的妈妈某个特定事务中你做出的决定或行动，她会为你的选择感到骄傲还是失望？这个简单的测试可能会减少我们许多关于
    AI 伦理的担忧。
- en: As humans define the AI utility function that serves to distinguish between
    right and wrong decisions, we *must* understand the differences between passive
    ethics and proactive ethics.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 作为人类定义 AI 效用函数以区分正确和错误决策的工具，我们*必须*理解被动伦理与主动伦理之间的差异。
