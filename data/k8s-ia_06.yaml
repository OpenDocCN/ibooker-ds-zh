- en: 'Chapter 5\. Services: enabling clients to discover and talk to pods'
  id: totrans-0
  prefs: []
  type: TYPE_NORMAL
  zh: 第5章. 服务：使客户端能够发现并与服务Pod通信
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Creating Service resources to expose a group of pods at a single address
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建服务资源以在单个地址公开一组Pod
- en: Discovering services in the cluster
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中查找服务
- en: Exposing services to external clients
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务暴露给外部客户端
- en: Connecting to external services from inside the cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从集群内部连接到外部服务
- en: Controlling whether a pod is ready to be part of the service or not
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 控制Pod是否准备好成为服务的一部分
- en: Troubleshooting services
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务故障排除
- en: You’ve learned about pods and how to deploy them through ReplicaSets and similar
    resources to ensure they keep running. Although certain pods can do their work
    independently of an external stimulus, many applications these days are meant
    to respond to external requests. For example, in the case of microservices, pods
    will usually respond to HTTP requests coming either from other pods inside the
    cluster or from clients outside the cluster.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了Pod以及如何通过ReplicaSet等资源部署它们以确保它们持续运行。尽管某些Pod可以在没有外部刺激的情况下独立工作，但如今许多应用程序都是为了响应外部请求而设计的。例如，在微服务的情况下，Pod通常会响应来自集群内部其他Pod或来自集群外部客户端的HTTP请求。
- en: Pods need a way of finding other pods if they want to consume the services they
    provide. Unlike in the non-Kubernetes world, where a sysadmin would configure
    each client app by specifying the exact IP address or hostname of the server providing
    the service in the client’s configuration files, doing the same in Kubernetes
    wouldn’t work, because
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Pod想要消费它们提供的服务，它们需要一种方式来找到其他Pod。与Kubernetes世界之外的情况不同，在那里系统管理员会通过指定客户端配置文件中提供服务的服务器确切的IP地址或主机名来配置每个客户端应用程序，在Kubernetes中这样做是不行的，因为
- en: Pods are ephemeral—They may come and go at any time, whether it’s because a
    pod is removed from a node to make room for other pods, because someone scaled
    down the number of pods, or because a cluster node has failed.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pods是短暂的——它们可能随时出现或消失，无论是由于Pod被从节点中移除以腾出空间给其他Pod，有人减少了Pod的数量，还是因为集群节点故障。
- en: Kubernetes assigns an IP address to a pod after the pod has been scheduled to
    a node and before it’s started—Clients thus can’t know the IP address of the server
    pod up front.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes在Pod被调度到节点并启动之前为其分配一个IP地址——因此客户端无法事先知道服务器Pod的IP地址。
- en: Horizontal scaling means multiple pods may provide the same service—Each of
    those pods has its own IP address. Clients shouldn’t care how many pods are backing
    the service and what their IPs are. They shouldn’t have to keep a list of all
    the individual IPs of pods. Instead, all those pods should be accessible through
    a single IP address.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平扩展意味着多个Pod可能提供相同的服务——每个Pod都有自己的IP地址。客户端不需要关心支持服务的Pod数量以及它们的IP地址。他们不需要保留所有Pod的IP地址列表。相反，所有这些Pod都应该可以通过一个单一的IP地址访问。
- en: To solve these problems, Kubernetes also provides another resource type—Services—that
    we’ll discuss in this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些问题，Kubernetes还提供了另一种资源类型——服务，我们将在本章中讨论。
- en: 5.1\. Introducing services
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 5.1. 介绍服务
- en: A Kubernetes Service is a resource you create to make a single, constant point
    of entry to a group of pods providing the same service. Each service has an IP
    address and port that never change while the service exists. Clients can open
    connections to that IP and port, and those connections are then routed to one
    of the pods backing that service. This way, clients of a service don’t need to
    know the location of individual pods providing the service, allowing those pods
    to be moved around the cluster at any time.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes服务是一种资源，你创建它来为提供相同服务的多个Pod提供一个单一、恒定的入口点。每个服务都有一个IP地址和端口，在服务存在期间这些地址和端口不会改变。客户端可以打开到该IP和端口的连接，然后这些连接会被路由到支持该服务的某个Pod。这样，服务的客户端不需要知道提供服务的单个Pod的位置，允许这些Pod在集群中随时移动。
- en: Explaining services with an example
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 用例子解释服务
- en: 'Let’s revisit the example where you have a frontend web server and a backend
    database server. There may be multiple pods that all act as the frontend, but
    there may only be a single backend database pod. You need to solve two problems
    to make the system function:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下这个例子：你有一个前端Web服务器和一个后端数据库服务器。可能有多个Pod都充当前端，但可能只有一个后端数据库Pod。你需要解决两个问题才能使系统正常工作：
- en: External clients need to connect to the frontend pods without caring if there’s
    only a single web server or hundreds.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 外部客户端需要连接到前端Pod，无需关心是否只有一个Web服务器或数百个。
- en: The frontend pods need to connect to the backend database. Because the database
    runs inside a pod, it may be moved around the cluster over time, causing its IP
    address to change. You don’t want to reconfigure the frontend pods every time
    the backend database is moved.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前端 Pod 需要连接到后端数据库。因为数据库运行在 Pod 内部，它可能会随着时间的推移在集群中移动，导致其 IP 地址发生变化。你不想每次后端数据库移动时都重新配置前端
    Pod。
- en: By creating a service for the frontend pods and configuring it to be accessible
    from outside the cluster, you expose a single, constant IP address through which
    external clients can connect to the pods. Similarly, by also creating a service
    for the backend pod, you create a stable address for the backend pod. The service
    address doesn’t change even if the pod’s IP address changes. Additionally, by
    creating the service, you also enable the frontend pods to easily find the backend
    service by its name through either environment variables or DNS. All the components
    of your system (the two services, the two sets of pods backing those services,
    and the interdependencies between them) are shown in [figure 5.1](#filepos474192).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 通过为前端 Pod 创建一个服务并配置它可以从集群外部访问，你暴露了一个单一的、恒定的 IP 地址，外部客户端可以通过这个 IP 地址连接到 Pod。同样，通过为后端
    Pod 也创建一个服务，你为后端 Pod 创建了一个稳定的地址。即使 Pod 的 IP 地址发生变化，服务地址也不会改变。此外，通过创建服务，你还可以使前端
    Pod能够通过环境变量或 DNS 通过名称轻松找到后端服务。你的系统中的所有组件（两个服务、支持这些服务的两个 Pod 集合以及它们之间的相互依赖关系）都在[图
    5.1](#filepos474192)中展示。
- en: Figure 5.1\. Both internal and external clients usually connect to pods through
    services.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1\. 内部和外部客户端通常通过服务连接到 Pod。
- en: '![](images/00163.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00163.jpg)'
- en: You now understand the basic idea behind services. Now, let’s dig deeper by
    first seeing how they can be created.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你已经理解了服务背后的基本概念。现在，让我们通过首先了解它们是如何被创建的来深入探讨。
- en: 5.1.1\. Creating services
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 5.1.1\. 创建服务
- en: As you’ve seen, a service can be backed by more than one pod. Connections to
    the service are load-balanced across all the backing pods. But how exactly do
    you define which pods are part of the service and which aren’t?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，服务可以由多个 Pod 支持。对服务的连接在所有支持 Pod 之间进行负载均衡。但你是如何定义哪些 Pod 是服务的一部分，哪些不是的呢？
- en: You probably remember label selectors and how they’re used in Replication-Controllers
    and other pod controllers to specify which pods belong to the same set. The same
    mechanism is used by services in the same way, as you can see in [figure 5.2](#filepos475228).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得标签选择器和它们如何在 Replication-Controllers 和其他 Pod 控制器中使用，以指定哪些 Pod 属于同一个集合。服务以相同的方式使用相同的机制，正如你在[图
    5.2](#filepos475228)中可以看到的那样。
- en: Figure 5.2\. Label selectors determine which pods belong to the Service.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2\. 标签选择器确定哪些 Pod 属于服务。
- en: '![](images/00182.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00182.jpg)'
- en: In the previous chapter, you created a ReplicationController which then ran
    three instances of the pod containing the Node.js app. Create the ReplicationController
    again and verify three pod instances are up and running. After that, you’ll create
    a Service for those three pods.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你创建了一个 ReplicationController，然后运行了包含 Node.js 应用的 Pod 的三个实例。再次创建 ReplicationController
    并验证三个 Pod 实例是否启动并运行。之后，你将为这三个 Pod 创建一个 Service。
- en: Creating a service through kubectl expose
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 kubectl expose 创建服务
- en: The easiest way to create a service is through `kubectl expose`, which you’ve
    already used in [chapter 2](index_split_022.html#filepos185841) to expose the
    ReplicationController you created earlier. The `expose` command created a Service
    resource with the same pod selector as the one used by the ReplicationController,
    thereby exposing all its pods through a single IP address and port.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 创建服务最简单的方法是通过 `kubectl expose`，你已经在[第 2 章](index_split_022.html#filepos185841)中使用它来暴露你之前创建的
    ReplicationController。`expose` 命令创建了一个与 ReplicationController 使用相同的 pod 选择器的 Service
    资源，从而通过单个 IP 地址和端口暴露了所有 Pod。
- en: Now, instead of using the `expose` command, you’ll create a service manually
    by posting a YAML to the Kubernetes API server.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将不再使用 `expose` 命令，而是通过向 Kubernetes API 服务器提交 YAML 来手动创建服务。
- en: Creating a service through a YAML descriptor
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 YAML 描述符创建服务
- en: Create a file called kubia-svc.yaml with the following listing’s contents.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 kubia-svc.yaml 的文件，并包含以下内容列表。
- en: 'Listing 5.1\. A definition of a service: kubia-svc.yaml'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.1\. 服务的定义：kubia-svc.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia spec:   ports:   - port:
    80` `1` `targetPort: 8080` `2` `selector:` `3` `app: kubia` `3`'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia spec:   ports:   - port:
    80` `1` `targetPort: 8080` `2` `selector:` `3` `app: kubia` `3`'
- en: 1 The port this service will be available on
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 该服务将可用的端口
- en: 2 The container port the service will forward to
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 服务将转发的容器端口
- en: 3 All pods with the app=kubia label will be part of this service.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 所有带有app=kubia标签的Pod都将成为此服务的一部分。
- en: You’re defining a service called `kubia`, which will accept connections on port
    80 and route each connection to port 8080 of one of the pods matching the `app=kubia`
    label selector.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在定义一个名为`kubia`的服务，该服务将在端口80上接受连接，并将每个连接路由到匹配`app=kubia`标签选择器的Pod的端口8080。
- en: Go ahead and create the service by posting the file using `kubectl create`.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kubectl create`上传文件来创建服务。
- en: Examining your new service
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 检查您的新服务
- en: 'After posting the YAML, you can list all Service resources in your namespace
    and see that an internal cluster IP has been assigned to your service:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 上传YAML文件后，您可以列出您命名空间中的所有服务资源，并看到已为您的服务分配了一个内部集群IP：
- en: '`$ kubectl get svc` `NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)  
    AGE kubernetes   10.111.240.1     <none>        443/TCP   30d kubia        10.111.249.153  
    <none>        80/TCP    6m` `1`'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get svc` `NAME         CLUSTER-IP       EXTERNAL-IP   PORT(S)  
    AGE kubernetes   10.111.240.1     <none>        443/TCP   30d kubia        10.111.249.153  
    <none>        80/TCP    6m` `1`'
- en: 1 Here’s your service.
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这里是您的服务。
- en: The list shows that the IP address assigned to the service is 10.111.249.153\.
    Because this is the cluster IP, it’s only accessible from inside the cluster.
    The primary purpose of services is exposing groups of pods to other pods in the
    cluster, but you’ll usually also want to expose services externally. You’ll see
    how to do that later. For now, let’s use your service from inside the cluster
    and see what it does.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表显示分配给服务的IP地址是10.111.249.153。因为这是集群IP，所以它只能在集群内部访问。服务的主要目的是将Pod组暴露给集群中的其他Pod，但您通常还希望将服务外部暴露。您将在稍后看到如何做到这一点。现在，让我们从集群内部使用您的服务并查看它的工作情况。
- en: Testing your service from within the cluster
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群内部测试您的服务
- en: 'You can send requests to your service from within the cluster in a few ways:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过几种方式在集群内部向您的服务发送请求：
- en: The obvious way is to create a pod that will send the request to the service’s
    cluster IP and log the response. You can then examine the pod’s log to see what
    the service’s response was.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 明显的方法是创建一个Pod，它会将请求发送到服务的集群IP并记录响应。然后您可以检查Pod的日志以查看服务的响应。
- en: You can `ssh` into one of the Kubernetes nodes and use the `curl` command.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以`ssh`连接到Kubernetes的一个节点并使用`curl`命令。
- en: You can execute the `curl` command inside one of your existing pods through
    the `kubectl exec` command.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过`kubectl exec`命令在您的现有Pod中执行`curl`命令。
- en: Let’s go for the last option, so you also learn how to run commands in existing
    pods.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择最后一个选项，这样您也可以学习如何在现有Pod中运行命令。
- en: Remotely executing commands in running containers
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行容器中远程执行命令
- en: 'The `kubectl exec` command allows you to remotely run arbitrary commands inside
    an existing container of a pod. This comes in handy when you want to examine the
    contents, state, and/or environment of a container. List the pods with the `kubectl
    get pods` command and choose one as your target for the `exec` command (in the
    following example, I’ve chosen the `kubia-7nog1` pod as the target). You’ll also
    need to obtain the cluster IP of your service (using `kubectl get svc`, for example).
    When running the following commands yourself, be sure to replace the pod name
    and the service IP with your own:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl exec`命令允许您在Pod的现有容器中远程运行任意命令。当您想检查容器的内容、状态和/或环境时，这非常有用。使用`kubectl
    get pods`命令列出Pod，并选择一个作为`exec`命令的目标（在以下示例中，我选择了`kubia-7nog1` Pod作为目标）。您还需要获取您服务的集群IP（例如，使用`kubectl
    get svc`）。当您自己运行以下命令时，请确保将Pod名称和服务IP替换为您自己的：'
- en: '`$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153` `You''ve hit
    kubia-gzwli`'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec kubia-7nog1 -- curl -s http://10.111.249.153` `您已连接到kubia-gzwli`'
- en: If you’ve used `ssh` to execute commands on a remote system before, you’ll recognize
    that `kubectl exec` isn’t much different.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您之前使用`ssh`在远程系统上执行过命令，您会认识到`kubectl exec`并没有太大区别。
- en: '|  |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why the double dash?
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么是双横线？
- en: 'The double dash (`--`) in the command signals the end of command options for
    `kubectl`. Everything after the double dash is the command that should be executed
    inside the pod. Using the double dash isn’t necessary if the command has no arguments
    that start with a dash. But in your case, if you don’t use the double dash there,
    the `-s` option would be interpreted as an option for `kubectl exec` and would
    result in the following strange and highly misleading error:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 命令中的双横线（`--`）表示`kubectl`命令选项的结束。双横线之后是应该在Pod内部执行的命令。如果没有以短横线开头的参数，则不需要使用双横线。但在你的情况下，如果你不使用双横线，`-s`选项将被解释为`kubectl
    exec`的选项，并导致以下奇怪且极具误导性的错误：
- en: '`$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153` `The connection
    to the server 10.111.249.153 was refused – did you      specify the right host
    or port?`'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec kubia-7nog1 curl -s http://10.111.249.153` `连接到服务器10.111.249.153被拒绝
    – 你是否指定了正确的宿主或端口？`'
- en: This has nothing to do with your service refusing the connection. It’s because
    `kubectl` is not able to connect to an API server at 10.111.249.153 (the `-s`
    option is used to tell `kubectl` to connect to a different API server than the
    default).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这与你的服务拒绝连接无关。这是因为`kubectl`无法连接到10.111.249.153（API服务器）的API服务器（`-s`选项用于告诉`kubectl`连接到非默认的API服务器）。
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s go over what transpired when you ran the command. [Figure 5.3](#filepos483310)
    shows the sequence of events. You instructed Kubernetes to execute the `curl`
    command inside the container of one of your pods. Curl sent an HTTP request to
    the service IP, which is backed by three pods. The Kubernetes service proxy intercepted
    the connection, selected a random pod among the three pods, and forwarded the
    request to it. Node.js running inside that pod then handled the request and returned
    an HTTP response containing the pod’s name. Curl then printed the response to
    the standard output, which was intercepted and printed to its standard output
    on your local machine by `kubectl`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下当你运行命令时发生了什么。[图5.3](#filepos483310)显示了事件的序列。你指示Kubernetes在其中一个Pod的容器内执行`curl`命令。Curl向服务IP发送了一个HTTP请求，该IP由三个Pod支持。Kubernetes服务代理拦截了连接，从三个Pod中随机选择了一个Pod，并将请求转发给它。然后在该Pod内部运行的Node.js处理了请求，并返回了一个包含Pod名称的HTTP响应。Curl随后将响应打印到标准输出，然后由`kubectl`拦截并打印到你的本地机器的标准输出。
- en: Figure 5.3\. Using kubectl exec to test out a connection to the service by running
    curl in one of the pods
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3\. 使用kubectl exec通过在Pod中运行curl测试对服务的连接
- en: '![](images/00151.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00151.jpg)'
- en: In the previous example, you executed the `curl` command as a separate process,
    but inside the pod’s main container. This isn’t much different from the actual
    main process in the container talking to the service.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一个例子中，你作为单独的进程执行了`curl`命令，但它在Pod的主容器内部。这与容器中的实际主进程与服务的通信并没有太大的区别。
- en: Configuring session affinity on the service
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在服务上配置会话亲和性
- en: If you execute the same command a few more times, you should hit a different
    pod with every invocation, because the service proxy normally forwards each connection
    to a randomly selected backing pod, even if the connections are coming from the
    same client.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你多次执行相同的命令，你应该在每次调用时遇到不同的Pod，因为服务代理通常将每个连接转发到随机选择的支撑Pod，即使连接来自同一客户端。
- en: If, on the other hand, you want all requests made by a certain client to be
    redirected to the same pod every time, you can set the service’s `sessionAffinity`
    property to `ClientIP` (instead of `None`, which is the default), as shown in
    the following listing.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果你希望某个客户端发出的所有请求每次都重定向到同一个Pod，你可以将服务的`sessionAffinity`属性设置为`ClientIP`（而不是默认的`None`），如下所示。
- en: Listing 5.2\. A example of a service with `ClientIP` session affinity configured
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.2\. 配置了`ClientIP`会话亲和性的服务示例
- en: '`apiVersion: v1 kind: Service spec:` `sessionAffinity: ClientIP` `...`'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service spec:` `sessionAffinity: ClientIP` `...`'
- en: This makes the service proxy redirect all requests originating from the same
    client IP to the same pod. As an exercise, you can create an additional service
    with session affinity set to `ClientIP` and try sending requests to it.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得服务代理将所有来自同一客户端IP的请求重定向到同一个Pod。作为一个练习，你可以创建一个额外的服务，将会话亲和性设置为`ClientIP`，并尝试向它发送请求。
- en: 'Kubernetes supports only two types of service session affinity: `None` and
    `ClientIP`. You may be surprised it doesn’t have a cookie-based session affinity
    option, but you need to understand that Kubernetes services don’t operate at the
    HTTP level. Services deal with TCP and UDP packets and don’t care about the payload
    they carry. Because cookies are a construct of the HTTP protocol, services don’t
    know about them, which explains why session affinity cannot be based on cookies.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes只支持两种类型的服务会话亲和性：`None`和`ClientIP`。你可能对它没有基于cookie的会话亲和性选项感到惊讶，但你需要理解Kubernetes服务不在HTTP级别上操作。服务处理TCP和UDP数据包，并不关心它们携带的负载。因为cookie是HTTP协议的一部分，服务不知道它们，这也解释了为什么会话亲和性不能基于cookie。
- en: Exposing multiple ports in the same service
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在同一服务中暴露多个端口
- en: Your service exposes only a single port, but services can also support multiple
    ports. For example, if your pods listened on two ports—let’s say 8080 for HTTP
    and 8443 for HTTPS—you could use a single service to forward both port 80 and
    443 to the pod’s ports 8080 and 8443\. You don’t need to create two different
    services in such cases. Using a single, multi-port service exposes all the service’s
    ports through a single cluster IP.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 你的服务只暴露单个端口，但服务也可以支持多个端口。例如，如果你的Pod监听两个端口——假设8080用于HTTP和8443用于HTTPS——你可以使用单个服务将端口80和443都转发到Pod的端口8080和8443。在这种情况下，你不需要创建两个不同的服务。使用单个多端口服务可以通过单个集群IP暴露所有服务端口。
- en: '|  |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When creating a service with multiple ports, you must specify a name for each
    port.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建具有多个端口的 服务时，你必须为每个端口指定一个名称。
- en: '|  |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The spec for a multi-port service is shown in the following listing.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 多端口服务的规范如下所示。
- en: Listing 5.3\. Specifying multiple ports in a service definition
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3\. 在服务定义中指定多个端口
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia spec:   ports:   - name:
    http` `1` `port: 80` `1` `targetPort: 8080` `1` `- name: https` `2` `port: 443`
    `2` `targetPort: 8443` `2` `selector:` `3` `app: kubia` `3`'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia spec:   ports:   - name:
    http` `1` `port: 80` `1` `targetPort: 8080` `1` `- name: https` `2` `port: 443`
    `2` `targetPort: 8443` `2` `selector:` `3` `app: kubia` `3`'
- en: 1 Port 80 is mapped to the pods’ port 8080\.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口80映射到Pod的端口8080。
- en: 2 Port 443 is mapped to pods’ port 8443\.
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口443映射到Pod的端口8443。
- en: 3 The label selector always applies to the whole service.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签选择器始终应用于整个服务。
- en: '|  |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The label selector applies to the service as a whole—it can’t be configured
    for each port individually. If you want different ports to map to different subsets
    of pods, you need to create two services.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 标签选择器应用于整个服务——不能为每个端口单独配置。如果你想让不同的端口映射到不同的Pod子集，你需要创建两个服务。
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Because your `kubia` pods don’t listen on multiple ports, creating a multi-port
    service and a multi-port pod is left as an exercise to you.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你的`kubia` Pods没有监听多个端口，创建多端口服务和多端口Pod留作练习。
- en: Using named ports
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命名端口
- en: In all these examples, you’ve referred to the target port by its number, but
    you can also give a name to each pod’s port and refer to it by name in the service
    spec. This makes the service spec slightly clearer, especially if the port numbers
    aren’t well-known.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有这些示例中，你都是通过端口号来引用目标端口的，但你也可以为每个Pod的端口命名，并在服务规范中通过名称引用它。这使得服务规范稍微清晰一些，尤其是如果端口号不是很知名的话。
- en: For example, suppose your pod defines names for its ports as shown in the following
    listing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你的Pod定义了如下所示端口名称。
- en: Listing 5.4\. Specifying port names in a pod definition
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.4\. 在Pod定义中指定端口名称
- en: '`kind: Pod spec:   containers:   - name: kubia     ports:     - name: http`
    `1` `containerPort: 8080` `1` `- name: https` `2` `containerPort: 8443` `2`'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '`kind: Pod spec:   containers:   - name: kubia     ports:     - name: http`
    `1` `containerPort: 8080` `1` `- name: https` `2` `containerPort: 8443` `2`'
- en: 1 Container’s port 8080 is called http
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容器的端口8080被称为http
- en: 2 Port 8443 is called https.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口8443被称为https。
- en: You can then refer to those ports by name in the service spec, as shown in the
    following listing.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在服务规范中通过名称引用这些端口，如下所示。
- en: Listing 5.5\. Referring to named ports in a service
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.5\. 在服务中引用命名端口
- en: '`apiVersion: v1 kind: Service spec:   ports:   - name: http` `1` `port: 80`
    `1` `targetPort: http` `1` `- name: https` `2` `port: 443` `2` `targetPort: https`
    `2`'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service spec:   ports:   - name: http` `1` `port: 80`
    `1` `targetPort: http` `1` `- name: https` `2` `port: 443` `2` `targetPort: https`
    `2`'
- en: 1 Port 80 is mapped to the container’s port called http.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 端口80映射到名为http的容器端口。
- en: 2 Port 443 is mapped to the container’s port, whose name is https.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 端口443映射到名为https的容器端口。
- en: But why should you even bother with naming ports? The biggest benefit of doing
    so is that it enables you to change port numbers later without having to change
    the service spec. Your pod currently uses port 8080 for http, but what if you
    later decide you’d like to move that to port 80?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 但你为什么要费心去命名端口呢？这样做最大的好处是，它允许你在以后更改端口号，而无需更改服务规范。你的Pod当前使用8080端口进行http通信，但如果你后来决定想将其移动到80端口怎么办？
- en: If you’re using named ports, all you need to do is change the port number in
    the pod spec (while keeping the port’s name unchanged). As you spin up pods with
    the new ports, client connections will be forwarded to the appropriate port numbers,
    depending on the pod receiving the connection (port 8080 on old pods and port
    80 on the new ones).
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用命名端口，你只需要更改Pod规范中的端口号（同时保持端口号名称不变）。当你启动具有新端口号的Pod时，客户端连接将根据接收连接的Pod（旧Pod上的8080端口和新Pod上的80端口）被转发到相应的端口号。
- en: 5.1.2\. Discovering services
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 5.1.2\. 发现服务
- en: By creating a service, you now have a single and stable IP address and port
    that you can hit to access your pods. This address will remain unchanged throughout
    the whole lifetime of the service. Pods behind this service may come and go, their
    IPs may change, their number can go up or down, but they’ll always be accessible
    through the service’s single and constant IP address.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建服务，你现在有一个单一且稳定的IP地址和端口号，你可以通过它来访问你的Pod。在整个服务生命周期中，这个地址将保持不变。位于此服务后面的Pod可能会来来去去，它们的IP可能会改变，它们的数量可能会增加或减少，但它们将通过服务的单一且恒定的IP地址始终可访问。
- en: But how do the client pods know the IP and port of a service? Do you need to
    create the service first, then manually look up its IP address and pass the IP
    to the configuration options of the client pod? Not really. Kubernetes also provides
    ways for client pods to discover a service’s IP and port.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 但客户端Pod如何知道服务的IP和端口号呢？你需要先创建服务，然后手动查找其IP地址，并将其传递给客户端Pod的配置选项吗？实际上并不需要。Kubernetes还提供了客户端Pod发现服务IP和端口号的方法。
- en: Discovering services through environment variables
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 通过环境变量发现服务
- en: When a pod is started, Kubernetes initializes a set of environment variables
    pointing to each service that exists at that moment. If you create the service
    before creating the client pods, processes in those pods can get the IP address
    and port of the service by inspecting their environment variables.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod启动时，Kubernetes初始化一组环境变量，指向当时存在的每个服务。如果你在创建客户端Pod之前创建服务，那些Pod中的进程可以通过检查它们的环境变量来获取服务的IP地址和端口号。
- en: Let’s see what those environment variables look like by examining the environment
    of one of your running pods. You’ve already learned that you can use the `kubectl
    exec` command to run a command in the pod, but because you created the service
    only after your pods had been created, the environment variables for the service
    couldn’t have been set yet. You’ll need to address that first.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过检查你运行中的Pod的环境来查看这些环境变量是什么样的。你已经了解到你可以使用`kubectl exec`命令在Pod中运行命令，但由于你是在Pod创建后才创建服务的，因此服务环境变量还没有被设置。首先你需要解决这个问题。
- en: 'Before you can see environment variables for your service, you first need to
    delete all the pods and let the ReplicationController create new ones. You may
    remember you can delete all pods without specifying their names like this:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在你能够看到服务环境变量之前，首先需要删除所有Pod，并让ReplicationController创建新的Pod。你可能记得你可以这样不指定Pod名称来删除所有Pod：
- en: '`$ kubectl delete po --all` `pod "kubia-7nog1" deleted pod "kubia-bf50t" deleted
    pod "kubia-gzwli" deleted`'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl delete po --all` `pod "kubia-7nog1" deleted pod "kubia-bf50t" deleted
    pod "kubia-gzwli" deleted`'
- en: Now you can list the new pods (I’m sure you know how to do that) and pick one
    as your target for the `kubectl exec` command. Once you’ve selected your target
    pod, you can list environment variables by running the `env` command inside the
    container, as shown in the following listing.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以列出新的Pod（我确信你知道如何做），并选择一个作为`kubectl exec`命令的目标。一旦你选择了目标Pod，你可以在容器内部运行`env`命令来列出环境变量，如下所示。
- en: Listing 5.6\. Service-related environment variables in a container
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6\. 容器中的服务相关环境变量
- en: '`$ kubectl exec kubia-3inly env` `PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    HOSTNAME=kubia-3inly KUBERNETES_SERVICE_HOST=10.111.240.1 KUBERNETES_SERVICE_PORT=443
    ...` `KUBIA_SERVICE_HOST=10.111.249.153``1``KUBIA_SERVICE_PORT=80``2` `...`'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec kubia-3inly env` `PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
    HOSTNAME=kubia-3inly KUBERNETES_SERVICE_HOST=10.111.240.1 KUBERNETES_SERVICE_PORT=443
    ...` `KUBIA_SERVICE_HOST=10.111.249.153``1``KUBIA_SERVICE_PORT=80``2` `...`'
- en: 1 Here’s the cluster IP of the service.
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这里是服务的集群IP。
- en: 2 And here’s the port the service is available on.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这里是服务可用的端口。
- en: 'Two services are defined in your cluster: the `kubernetes` and the `kubia`
    service (you saw this earlier with the `kubectl get svc` command); consequently,
    two sets of service-related environment variables are in the list. Among the variables
    that pertain to the `kubia` service you created at the beginning of the chapter,
    you’ll see the `KUBIA_SERVICE_HOST` and the `KUBIA_SERVICE_PORT` environment variables,
    which hold the IP address and port of the `kubia` service, respectively.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的集群中定义了两个服务：`kubernetes`和`kubia`服务（你之前通过`kubectl get svc`命令看到了这一点）；因此，有两个与服务相关的环境变量集。在章节开头创建的`kubia`服务相关的变量中，你会看到`KUBIA_SERVICE_HOST`和`KUBIA_SERVICE_PORT`环境变量，分别持有`kubia`服务的IP地址和端口。
- en: Turning back to the frontend-backend example we started this chapter with, when
    you have a frontend pod that requires the use of a backend database server pod,
    you can expose the backend pod through a service called `backend-database` and
    then have the frontend pod look up its IP address and port through the environment
    variables `BACKEND_DATABASE_SERVICE_HOST` and `BACKEND_DATABASE_SERVICE_PORT.`
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 回到本章开始时我们讨论的前端-后端示例，当你有一个需要使用后端数据库服务器Pod的前端Pod时，你可以通过一个名为`backend-database`的服务来暴露后端Pod，然后让前端Pod通过环境变量`BACKEND_DATABASE_SERVICE_HOST`和`BACKEND_DATABASE_SERVICE_PORT`查找其IP地址和端口。
- en: '|  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Dashes in the service name are converted to underscores and all letters are
    uppercased when the service name is used as the prefix in the environment variable’s
    name.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当服务名称用作环境变量名称的前缀时，服务名称中的破折号会被转换为下划线，并且所有字母都会转换为大写。
- en: '|  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Environment variables are one way of looking up the IP and port of a service,
    but isn’t this usually the domain of DNS? Why doesn’t Kubernetes include a DNS
    server and allow you to look up service IPs through DNS instead? As it turns out,
    it does!
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量是查找服务IP和端口的一种方式，但这通常不是DNS的领域吗？为什么Kubernetes不包含一个DNS服务器并允许你通过DNS查找服务IP呢？实际上，它确实包含了！
- en: Discovering services through DNS
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 通过DNS发现服务
- en: Remember in [chapter 3](index_split_028.html#filepos271328) when you listed
    pods in the `kube-system` namespace? One of the pods was called `kube-dns`. The
    `kube-system` namespace also includes a corresponding service with the same name.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第3章](index_split_028.html#filepos271328)中你列出了`kube-system`命名空间中的Pod吗？其中一个Pod被称作`kube-dns`。`kube-system`命名空间还包括一个同名的对应服务。
- en: As the name suggests, the pod runs a DNS server, which all other pods running
    in the cluster are automatically configured to use (Kubernetes does that by modifying
    each container’s `/etc/resolv.conf` file). Any DNS query performed by a process
    running in a pod will be handled by Kubernetes’ own DNS server, which knows all
    the services running in your system.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名所示，该Pod运行一个DNS服务器，集群中所有其他运行的Pod都会自动配置使用该服务器（Kubernetes通过修改每个容器的`/etc/resolv.conf`文件来实现这一点）。在Pod中运行的任何进程执行的DNS查询都将由Kubernetes自己的DNS服务器处理，该服务器了解系统中运行的所有服务。
- en: '|  |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Whether a pod uses the internal DNS server or not is configurable through the
    `dnsPolicy` property in each pod’s spec.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 是否使用内部DNS服务器是由每个Pod的spec中的`dnsPolicy`属性配置的。
- en: '|  |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Each service gets a DNS entry in the internal DNS server, and client pods that
    know the name of the service can access it through its fully qualified domain
    name (FQDN) instead of resorting to environment variables.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 每个服务在内部DNS服务器中都有一个DNS条目，知道服务名称的客户Pod可以通过其完全限定域名（FQDN）访问它，而不是求助于环境变量。
- en: Connecting to the service through its FQDN
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 通过FQDN连接到服务
- en: 'To revisit the frontend-backend example, a frontend pod can connect to the
    backend-database service by opening a connection to the following FQDN:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾前端-后端示例，前端Pod可以通过打开以下FQDN的连接来连接到后端-database服务：
- en: '`backend-database.default.svc.cluster.local`'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '`backend-database.default.svc.cluster.local`'
- en: '`backend-database` corresponds to the service name, `default` stands for the
    namespace the service is defined in, and `svc.cluster.local` is a configurable
    cluster domain suffix used in all cluster local service names.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`backend-database`对应于服务名，`default`表示服务定义的命名空间，而`svc.cluster.local`是用于所有集群本地服务名的可配置集群域名后缀。'
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The client must still know the service’s port number. If the service is using
    a standard port (for example, 80 for HTTP or 5432 for Postgres), that shouldn’t
    be a problem. If not, the client can get the port number from the environment
    variable.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 客户端仍然需要知道服务的端口号。如果服务使用标准端口（例如，HTTP的80或Postgres的5432），这通常不会成问题。如果不是，客户端可以从环境变量中获取端口号。
- en: '|  |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Connecting to a service can be even simpler than that. You can omit the `svc.cluster.local`
    suffix and even the namespace, when the frontend pod is in the same namespace
    as the database pod. You can thus refer to the service simply as `backend-database`.
    That’s incredibly simple, right?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到服务甚至可以比这更简单。当前端pod与数据库pod位于同一命名空间时，你可以省略`svc.cluster.local`后缀甚至命名空间。因此，你可以简单地通过`backend-database`来引用服务。这真是太简单了，对吧？
- en: Let’s try this. You’ll try to access the `kubia` service through its FQDN instead
    of its IP. Again, you’ll need to do that inside an existing pod. You already know
    how to use `kubectl exec` to run a single command in a pod’s container, but this
    time, instead of running the `curl` command directly, you’ll run the `bash` shell
    instead, so you can then run multiple commands in the container. This is similar
    to what you did in [chapter 2](index_split_022.html#filepos185841) when you entered
    the container you ran with Docker by using the `docker exec -it bash` command.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一下。你将尝试通过其FQDN而不是IP访问`kubia`服务。同样，你需要在现有的pod内部进行此操作。你已经知道如何使用`kubectl exec`在pod的容器中运行单个命令，但这次，你将运行`bash`
    shell而不是直接运行`curl`命令，这样你就可以在容器中运行多个命令。这与你使用`docker exec -it bash`命令进入使用Docker运行的容器时在第二章中做的事情类似。
- en: Running a shell in a pod’s container
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在pod的容器中运行shell
- en: You can use the `kubectl exec` command to run `bash` (or any other shell) inside
    a pod’s container. This way you’re free to explore the container as long as you
    want, without having to perform a `kubectl exec` for every command you want to
    run.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用`kubectl exec`命令在pod的容器内运行`bash`（或任何其他shell）。这样，你可以自由地探索容器，而无需为每个要运行的命令执行`kubectl
    exec`。
- en: '|  |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: The shell’s binary executable must be available in the container image for this
    to work.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使shell正常工作，shell的二进制可执行文件必须在容器镜像中可用。
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'To use the shell properly, you need to pass the `-it` option to `kubectl exec`:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要正确使用shell，你需要将`-it`选项传递给`kubectl exec`：
- en: '`$ kubectl exec -it kubia-3inly bash` `root@kubia-3inly:/#`'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec -it kubia-3inly bash` `root@kubia-3inly:/#`'
- en: 'You’re now inside the container. You can use the `curl` command to access the
    `kubia` service in any of the following ways:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经在容器内部了。你可以使用`curl`命令以下任何一种方式访问`kubia`服务：
- en: '`root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local` `You''ve
    hit kubia-5asi2` `root@kubia-3inly:/# curl http://kubia.default` `You''ve hit
    kubia-3inly` `root@kubia-3inly:/# curl http://kubia` `You''ve hit kubia-8awf3`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`root@kubia-3inly:/# curl http://kubia.default.svc.cluster.local` `You''ve
    hit kubia-5asi2` `root@kubia-3inly:/# curl http://kubia.default` `You''ve hit
    kubia-3inly` `root@kubia-3inly:/# curl http://kubia` `You''ve hit kubia-8awf3`'
- en: 'You can hit your service by using the service’s name as the hostname in the
    requested URL. You can omit the namespace and the `svc.cluster.local` suffix because
    of how the DNS resolver inside each pod’s container is configured. Look at the
    /etc/resolv.conf file in the container and you’ll understand:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用服务名作为请求URL中的主机名来访问你的服务。你可以省略命名空间和`svc.cluster.local`后缀，因为每个pod容器内部的DNS解析器是如何配置的。查看容器中的`/etc/resolv.conf`文件，你就会明白：
- en: '`root@kubia-3inly:/# cat /etc/resolv.conf` `search default.svc.cluster.local
    svc.cluster.local cluster.local ...`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`root@kubia-3inly:/# cat /etc/resolv.conf` `search default.svc.cluster.local
    svc.cluster.local cluster.local ...`'
- en: Understanding why you can’t ping a service IP
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么无法ping通服务IP
- en: One last thing before we move on. You know how to create services now, so you’ll
    soon create your own. But what if, for whatever reason, you can’t access your
    service?
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，还有最后一件事。你现在知道如何创建服务了，所以你很快就会创建自己的服务。但如果你因为任何原因无法访问你的服务怎么办？
- en: 'You’ll probably try to figure out what’s wrong by entering an existing pod
    and trying to access the service like you did in the last example. Then, if you
    still can’t access the service with a simple `curl` command, maybe you’ll try
    to ping the service IP to see if it’s up. Let’s try that now:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会尝试通过进入现有的Pod并尝试像上一个例子中那样访问服务来找出问题所在。然后，如果你仍然无法使用简单的`curl`命令访问服务，你可能尝试ping服务IP以查看它是否在线。我们现在试试看：
- en: '`root@kubia-3inly:/# ping kubia` `PING kubia.default.svc.cluster.local (10.111.249.153):
    56 data bytes ^C--- kubia.default.svc.cluster.local ping statistics --- 54 packets
    transmitted, 0 packets received, 100% packet loss`'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '`root@kubia-3inly:/# ping kubia` `PING kubia.default.svc.cluster.local (10.111.249.153):
    56 data bytes ^C--- kubia.default.svc.cluster.local ping statistics --- 54 packets
    transmitted, 0 packets received, 100% packet loss`'
- en: Hmm. `curl`-ing the service works, but pinging it doesn’t. That’s because the
    service’s cluster IP is a virtual IP, and only has meaning when combined with
    the service port. We’ll explain what that means and how services work in [chapter
    11](index_split_087.html#filepos1036287). I wanted to mention that here because
    it’s the first thing users do when they try to debug a broken service and it catches
    most of them off guard.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯。使用`curl`访问服务是可行的，但ping它却不行。这是因为服务的集群IP是一个虚拟IP，只有与服务端口结合时才有意义。我们将在第11章（index_split_087.html#filepos1036287）中解释这意味着什么以及服务是如何工作的。我想在这里提一下，因为这是用户在尝试调试损坏的服务时做的第一件事，而且它会让大多数人感到措手不及。
- en: 5.2\. Connecting to services living outside the cluster
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 5.2. 连接到集群外部的服务
- en: Up to now, we’ve talked about services backed by one or more pods running inside
    the cluster. But cases exist when you’d like to expose external services through
    the Kubernetes services feature. Instead of having the service redirect connections
    to pods in the cluster, you want it to redirect to external IP(s) and port(s).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了由集群内部运行的一个或多个Pod支持的服务。但存在一些情况，你可能希望通过Kubernetes服务功能公开外部服务。你不想让服务将连接重定向到集群中的Pod，而是希望它重定向到外部IP和端口。
- en: This allows you to take advantage of both service load balancing and service
    discovery. Client pods running in the cluster can connect to the external service
    like they connect to internal services.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这允许你利用服务负载均衡和服务发现的优势。运行在集群中的客户端Pod可以像连接内部服务一样连接到外部服务。
- en: 5.2.1\. Introducing service endpoints
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 5.2.1. 介绍服务端点
- en: Before going into how to do this, let me first shed more light on services.
    Services don’t link to pods directly. Instead, a resource sits in between—the
    Endpoints resource. You may have already noticed endpoints if you used the `kubectl
    describe` command on your service, as shown in the following listing.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在进入如何实现这一点之前，让我首先更详细地解释一下服务。服务并不直接链接到Pod。相反，一个资源位于其中间——Endpoints资源。如果你在服务上使用了`kubectl
    describe`命令，你可能已经注意到了端点，如下所示。
- en: Listing 5.7\. Full details of a service displayed with `kubectl describe`
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7. 使用`kubectl describe`显示的服务详细信息
- en: '`$ kubectl describe svc kubia` `Name:                kubia Namespace:          
    default Labels:              <none> Selector:            app=kubia` `1` `Type:               
    ClusterIP IP:                  10.111.249.153 Port:                <unset> 80/TCP
    Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080` `2` `Session
    Affinity:    None No events.`'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl describe svc kubia` `Name:                kubia Namespace:          
    default Labels:              <none> Selector:            app=kubia` `1` `Type:               
    ClusterIP IP:                  10.111.249.153 Port:                <unset> 80/TCP
    Endpoints:           10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080` `2` `Session
    Affinity:    None No events.`'
- en: 1 The service’s pod selector is used to create the list of endpoints.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 该服务的Pod选择器用于创建端点列表。
- en: 2 The list of pod IPs and ports that represent the endpoints of this service
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 代表此服务端点的Pod IP和端口列表
- en: 'An Endpoints resource (yes, plural) is a list of IP addresses and ports exposing
    a service. The Endpoints resource is like any other Kubernetes resource, so you
    can display its basic info with `kubectl get`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: Endpoints资源（是的，复数形式）是一个列出暴露服务的IP地址和端口的列表。Endpoints资源就像其他Kubernetes资源一样，因此你可以使用`kubectl
    get`来显示其基本信息：
- en: '`$ kubectl get endpoints kubia` `NAME    ENDPOINTS                                        
    AGE kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h`'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get endpoints kubia` `NAME    ENDPOINTS                                        
    AGE kubia   10.108.1.4:8080,10.108.2.5:8080,10.108.2.6:8080   1h`'
- en: Although the pod selector is defined in the service spec, it’s not used directly
    when redirecting incoming connections. Instead, the selector is used to build
    a list of IPs and ports, which is then stored in the Endpoints resource. When
    a client connects to a service, the service proxy selects one of those IP and
    port pairs and redirects the incoming connection to the server listening at that
    location.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然pod选择器在服务规范中定义，但在重定向传入连接时并不直接使用。相反，选择器用于构建一个IP和端口号列表，然后存储在端点资源中。当客户端连接到服务时，服务代理会从这些IP和端口号对中选择一个，并将传入的连接重定向到在该位置监听的服务器。
- en: 5.2.2\. Manually configuring service endpoints
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 5.2.2\. 手动配置服务端点
- en: You may have probably realized this already, but having the service’s endpoints
    decoupled from the service allows them to be configured and updated manually.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经意识到了这一点，但将服务的端点与服务解耦允许它们手动配置和更新。
- en: If you create a service without a pod selector, Kubernetes won’t even create
    the Endpoints resource (after all, without a selector, it can’t know which pods
    to include in the service). It’s up to you to create the Endpoints resource to
    specify the list of endpoints for the service.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你创建一个没有pod选择器的服务，Kubernetes甚至不会创建端点资源（毕竟，没有选择器，它不知道要包含哪些pod在服务中）。创建端点资源以指定服务端点列表的责任在你。
- en: To create a service with manually managed endpoints, you need to create both
    a Service and an Endpoints resource.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建手动管理的端点服务，你需要创建一个服务和端点资源。
- en: Creating a service without a selector
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 创建没有选择器的服务
- en: You’ll first create the YAML for the service itself, as shown in the following
    listing.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要创建服务本身的YAML，如下所示。
- en: 'Listing 5.8\. A service without a pod selector: external-service.yaml'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.8\. 没有pod选择器的服务：external-service.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: external-service` `1` `spec:`
    `2` `ports:   - port: 80`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: external-service` `1` `spec:`
    `2` `ports:   - port: 80`'
- en: 1 The name of the service must match the name of the Endpoints object (see next
    listing).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 服务的名称必须与端点对象的名称匹配（见下一列表）。
- en: 2 This service has no selector defined.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 此服务未定义选择器。
- en: You’re defining a service called `external-service` that will accept incoming
    connections on port 80\. You didn’t define a pod selector for the service.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在定义一个名为`external-service`的服务，该服务将在端口80上接受传入连接。你没有为服务定义pod选择器。
- en: Creating an Endpoints resource for a service without a selector
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为没有选择器的服务创建端点资源
- en: Endpoints are a separate resource and not an attribute of a service. Because
    you created the service without a selector, the corresponding Endpoints resource
    hasn’t been created automatically, so it’s up to you to create it. The following
    listing shows its YAML manifest.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 端点是单独的资源，而不是服务的属性。因为你创建的服务没有选择器，相应的端点资源还没有自动创建，所以你需要自己创建它。下面的列表显示了它的YAML表示。
- en: 'Listing 5.9\. A manually created Endpoints resource: external-service-endpoints.yaml'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9\. 手动创建的端点资源：external-service-endpoints.yaml
- en: '`apiVersion: v1 kind: Endpoints metadata:   name: external-service` `1` `subsets:
      - addresses:     - ip: 11.11.11.11` `2` `- ip: 22.22.22.22` `2` `ports:    
    - port: 80` `3`'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Endpoints metadata:   name: external-service` `1` `subsets:
      - addresses:     - ip: 11.11.11.11` `2` `- ip: 22.22.22.22` `2` `ports:    
    - port: 80` `3`'
- en: 1 The name of the Endpoints object must match the name of the service (see previous
    listing).
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 端点对象的名称必须与服务的名称匹配（见上一列表）。
- en: 2 The IPs of the endpoints that the service will forward connections to
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 服务将转发连接到的端点IP
- en: 3 The target port of the endpoints
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 端点的目标端口
- en: The Endpoints object needs to have the same name as the service and contain
    the list of target IP addresses and ports for the service. After both the Service
    and the Endpoints resource are posted to the server, the service is ready to be
    used like any regular service with a pod selector. Containers created after the
    service is created will include the environment variables for the service, and
    all connections to its IP:port pair will be load balanced between the service’s
    endpoints.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 端点对象需要与服务的名称相同，并包含服务的目标IP地址和端口号列表。在服务和端点资源都提交到服务器后，服务就可以像任何带有pod选择器的常规服务一样使用了。在服务创建后创建的容器将包含服务的环境变量，并且所有连接到其IP:端口对的连接将在服务的端点之间进行负载均衡。
- en: '[Figure 5.4](#filepos511617) shows three pods connecting to the service with
    external endpoints.'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.4](#filepos511617)显示了三个pod连接到具有外部端点的服务。'
- en: Figure 5.4\. Pods consuming a service with two external endpoints.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4\. 消费具有两个外部端点的服务的 Pod。
- en: '![](images/00064.jpg)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00064.jpg)'
- en: If you later decide to migrate the external service to pods running inside Kubernetes,
    you can add a selector to the service, thereby making its Endpoints managed automatically.
    The same is also true in reverse—by removing the selector from a Service, Kubernetes
    stops updating its Endpoints. This means a service IP address can remain constant
    while the actual implementation of the service is changed.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您后来决定将外部服务迁移到在 Kubernetes 内运行的 Pod 中，您可以在服务中添加一个选择器，从而自动管理其端点。反之亦然——通过从服务中移除选择器，Kubernetes
    停止更新其端点。这意味着服务 IP 地址可以保持不变，而服务的实际实现可以更改。
- en: 5.2.3\. Creating an alias for an external service
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 5.2.3\. 创建外部服务的别名
- en: Instead of exposing an external service by manually configuring the service’s
    Endpoints, a simpler method allows you to refer to an external service by its
    fully qualified domain name (FQDN).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动配置服务的端点以暴露外部服务相比，一种更简单的方法允许您通过其完全限定域名（FQDN）来引用外部服务。
- en: Creating an ExternalName service
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 ExternalName 服务
- en: To create a service that serves as an alias for an external service, you create
    a Service resource with the `type` field set to `ExternalName`. For example, let’s
    imagine there’s a public API available at [api.somecompany.com](http://api.somecompany.com).
    You can define a service that points to it as shown in the following listing.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个作为外部服务别名的服务，您需要创建一个 `type` 字段设置为 `ExternalName` 的服务资源。例如，让我们假设有一个公开的 API
    在 [api.somecompany.com](http://api.somecompany.com) 上可用。您可以定义一个指向它的服务，如下所示。
- en: 'Listing 5.10\. An `ExternalName`-type service: external-service-externalname.yaml'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10\. `ExternalName` 类型的服务：external-service-externalname.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: external-service spec:   type:
    ExternalName` `1` `externalName: someapi.somecompany.com` `2` `ports:   - port:
    80`'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: external-service spec:   type:
    ExternalName` `1` `externalName: someapi.somecompany.com` `2` `ports:   - port:
    80`'
- en: 1 Service type is set to ExternalName
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 服务类型设置为 ExternalName
- en: 2 The fully qualified domain name of the actual service
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 实际服务的完全限定域名
- en: After the service is created, pods can connect to the external service through
    the `external-service.default.svc.cluster.local` domain name (or even `external-service`)
    instead of using the service’s actual FQDN. This hides the actual service name
    and its location from pods consuming the service, allowing you to modify the service
    definition and point it to a different service any time later, by only changing
    the `externalName` attribute or by changing the type back to `ClusterIP` and creating
    an Endpoints object for the service—either manually or by specifying a label selector
    on the service and having it created automatically.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 服务创建后，Pod 可以通过 `external-service.default.svc.cluster.local` 域名（或甚至 `external-service`）连接到外部服务，而不是使用服务的实际
    FQDN。这隐藏了实际服务名称及其位置，从而允许您在以后任何时候仅通过更改 `externalName` 属性或将类型更改为 `ClusterIP` 并为服务创建一个端点对象（手动或通过在服务上指定标签选择器并自动创建）来修改服务定义并指向不同的服务。
- en: '`ExternalName` services are implemented solely at the DNS level—a simple `CNAME`
    DNS record is created for the service. Therefore, clients connecting to the service
    will connect to the external service directly, bypassing the service proxy completely.
    For this reason, these types of services don’t even get a cluster IP.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '`ExternalName` 服务仅在 DNS 层面实现——为服务创建一个简单的 `CNAME` DNS 记录。因此，连接到服务的客户端将直接连接到外部服务，完全绕过服务代理。因此，这些类型的服务甚至没有集群
    IP。'
- en: '|  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A `CNAME` record points to a fully qualified domain name instead of a numeric
    IP address.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`CNAME` 记录指向一个完全限定域名，而不是一个数字 IP 地址。'
- en: '|  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.3\. Exposing services to external clients
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 5.3\. 向外部客户端暴露服务
- en: Up to now, we’ve only talked about how services can be consumed by pods from
    inside the cluster. But you’ll also want to expose certain services, such as frontend
    webservers, to the outside, so external clients can access them, as depicted in
    [figure 5.5](#filepos515841).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只讨论了服务如何被集群内部的 Pod 消费。但您可能还想将某些服务，如前端 web 服务器，暴露给外部，以便外部客户端可以访问它们，如图
    5.5 所示。
- en: Figure 5.5\. Exposing a service to external clients
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5\. 向外部客户端暴露服务
- en: '![](images/00072.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00072.jpg)'
- en: 'You have a few ways to make a service accessible externally:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 您有几种方法可以使服务对外部可访问：
- en: Setting the service type to `NodePort`—For a `NodePort` service, each cluster
    node opens a port on the node itself (hence the name) and redirects traffic received
    on that port to the underlying service. The service isn’t accessible only at the
    internal cluster IP and port, but also through a dedicated port on all nodes.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务类型设置为 `NodePort`——对于 `NodePort` 服务，每个集群节点在其自身上打开一个端口（因此得名），并将该端口接收到的流量重定向到底层服务。服务不仅可以通过内部集群
    IP 和端口访问，还可以通过所有节点上的专用端口访问。
- en: Setting the service type to `LoadBalancer`, an extension of the `NodePort` type—This
    makes the service accessible through a dedicated load balancer, provisioned from
    the cloud infrastructure Kubernetes is running on. The load balancer redirects
    traffic to the node port across all the nodes. Clients connect to the service
    through the load balancer’s IP.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将服务类型设置为 `LoadBalancer`，这是 `NodePort` 类型的扩展——这使得服务可以通过一个专用的负载均衡器访问，该负载均衡器由 Kubernetes
    运行的云基础设施提供。负载均衡器将流量重定向到所有节点上的节点端口。客户端通过负载均衡器的 IP 连接到服务。
- en: Creating an Ingress resource, a radically different mechanism for exposing multiple
    services through a single IP address—It operates at the HTTP level (network layer
    7) and can thus offer more features than layer 4 services can. We’ll explain Ingress
    resources in [section 5.4](index_split_050.html#filepos536948).
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 Ingress 资源，这是一种通过单个 IP 地址公开多个服务的根本不同的机制——它在 HTTP 层（网络层 7）上运行，因此可以提供比层 4
    服务更多的功能。我们将在 [第 5.4 节](index_split_050.html#filepos536948) 解释 Ingress 资源。
- en: 5.3.1\. Using a NodePort service
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 5.3.1\. 使用 NodePort 服务
- en: The first method of exposing a set of pods to external clients is by creating
    a service and setting its type to `NodePort`. By creating a `NodePort` service,
    you make Kubernetes reserve a port on all its nodes (the same port number is used
    across all of them) and forward incoming connections to the pods that are part
    of the service.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将一组 pod 公开给外部客户端的第一种方法是创建一个服务并将其类型设置为 `NodePort`。通过创建 `NodePort` 服务，你让 Kubernetes
    在所有节点上保留一个端口（所有这些节点都使用相同的端口号）并将传入的连接转发到属于该服务的 pod。
- en: This is similar to a regular service (their actual type is `ClusterIP`), but
    a `NodePort` service can be accessed not only through the service’s internal cluster
    IP, but also through any node’s IP and the reserved node port.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 这与常规服务类似（它们的实际类型是 `ClusterIP`），但 `NodePort` 服务不仅可以通过服务的内部集群 IP 访问，还可以通过任何节点的
    IP 和保留的节点端口访问。
- en: This will make more sense when you try interacting with a `NodePort` service.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当你尝试与 `NodePort` 服务交互时，这会更有意义。
- en: Creating a NodePort service
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 NodePort 服务
- en: You’ll now create a `NodePort` service to see how you can use it. The following
    listing shows the YAML for the service.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在将创建一个 `NodePort` 服务来查看如何使用它。以下列表显示了服务的 YAML 格式。
- en: 'Listing 5.11\. A `NodePort` service definition: kubia-svc-nodeport.yaml'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11\. `NodePort` 服务定义：kubia-svc-nodeport.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia-nodeport spec:   type:
    NodePort` `1` `ports:   - port: 80` `2` `targetPort: 8080` `3` `nodePort: 30123`
    `4` `selector:     app: kubia`'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia-nodeport spec:   type:
    NodePort` `1` `ports:   - port: 80` `2` `targetPort: 8080` `3` `nodePort: 30123`
    `4` `selector:     app: kubia`'
- en: 1 Set the service type to NodePort.
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 将服务类型设置为 NodePort。
- en: 2 This is the port of the service’s internal cluster IP.
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 这是服务内部集群 IP 的端口。
- en: 3 This is the target port of the backing pods.
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 这是支持 pod 的目标端口。
- en: 4 The service will be accessible through port 30123 of each of your cluster
    nodes.
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 4 服务将通过您每个集群节点的 30123 端口访问。
- en: You set the type to `NodePort` and specify the node port this service should
    be bound to across all cluster nodes. Specifying the port isn’t mandatory; Kubernetes
    will choose a random port if you omit it.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你将类型设置为 `NodePort` 并指定该服务应在所有集群节点上绑定的节点端口。指定端口不是强制性的；如果你省略它，Kubernetes 将选择一个随机端口。
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When you create the service in GKE, `kubectl` prints out a warning about having
    to configure firewall rules. We’ll see how to do that soon.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在 GKE 中创建服务时，`kubectl` 会打印出一个警告，关于需要配置防火墙规则。我们很快就会看到如何做。
- en: '|  |'
  id: totrans-233
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Examining your NodePort service
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 检查你的 NodePort 服务
- en: 'Let’s see the basic information of your service to learn more about it:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们查看您服务的详细信息，以了解更多信息：
- en: '`$ kubectl get svc kubia-nodeport` `NAME             CLUSTER-IP       EXTERNAL-IP  
    PORT(S)        AGE kubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP  
    2m`'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get svc kubia-nodeport` `NAME             CLUSTER-IP       EXTERNAL-IP  
    PORT(S)        AGE kubia-nodeport   10.111.254.223   <nodes>       80:30123/TCP  
    2m`'
- en: 'Look at the `EXTERNAL-IP` column. It shows `<nodes>`, indicating the service
    is accessible through the IP address of any cluster node. The `PORT(S)` column
    shows both the internal port of the cluster IP (`80`) and the node port (`30123`).
    The service is accessible at the following addresses:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 查看外部IP列。它显示`<nodes>`，表示服务可以通过任何集群节点的IP地址访问。端口（PORT(S)）列显示了集群IP的内部端口（`80`）和节点端口（`30123`）。服务可通过以下地址访问：
- en: '`10.11.254.223:80`'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`10.11.254.223:80`'
- en: '`<1st node''s IP>:30123`'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<1st node''s IP>:30123`'
- en: '`<2nd node''s IP>:30123`, and so on.'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`<2nd node''s IP>:30123`，等等。'
- en: '[Figure 5.6](#filepos521984) shows your service exposed on port 30123 of both
    of your cluster nodes (this applies if you’re running this on GKE; Minikube only
    has a single node, but the principle is the same). An incoming connection to one
    of those ports will be redirected to a randomly selected pod, which may or may
    not be the one running on the node the connection is being made to.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.6](#filepos521984)显示了您的服务在两个集群节点的30123端口上公开（如果您在GKE上运行此操作，则适用；Minikube只有一个节点，但原理相同）。连接到这些端口之一的传入连接将被重定向到随机选择的Pod，这可能或可能不是连接到的节点上运行的Pod。'
- en: Figure 5.6\. An external client connecting to a NodePort service either through
    Node 1 or 2
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6\. 外部客户端通过节点1或2连接到NodePort服务
- en: '![](images/00091.jpg)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00091.jpg)'
- en: A connection received on port 30123 of the first node might be forwarded either
    to the pod running on the first node or to one of the pods running on the second
    node.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个节点的30123端口上接收到的连接可能会被转发到第一个节点上运行的Pod，或者转发到第二个节点上运行的Pod之一。
- en: Changing firewall rules to let external clients access our NodePort service
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 更改防火墙规则以允许外部客户端访问我们的NodePort服务
- en: 'As I’ve mentioned previously, before you can access your service through the
    node port, you need to configure the Google Cloud Platform’s firewalls to allow
    external connections to your nodes on that port. You’ll do this now:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所述，在您可以通过节点端口访问服务之前，您需要配置Google Cloud Platform的防火墙，以允许在该端口上对您的节点进行外部连接。您现在将这样做：
- en: '`$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123` `Created
    [https://www.googleapis.com/compute/v1/projects/kubia-      1295/global/firewalls/kubia-svc-rule].
    NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS kubia-svc-rule 
    default  0.0.0.0/0   tcp:30123`'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ gcloud compute firewall-rules create kubia-svc-rule --allow=tcp:30123` `Created
    [https://www.googleapis.com/compute/v1/projects/kubia-1295/global/firewalls/kubia-svc-rule].
    NAME            NETWORK  SRC_RANGES  RULES      SRC_TAGS  TARGET_TAGS kubia-svc-rule 
    default  0.0.0.0/0   tcp:30123`'
- en: You can access your service through port 30123 of one of the node’s IPs. But
    you need to figure out the IP of a node first. Refer to the sidebar on how to
    do that.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过节点IP的30123端口之一访问您的服务。但您首先需要找出节点的IP。有关如何操作的说明，请参考侧边栏。
- en: '|  |'
  id: totrans-249
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using JSONPath to get the IPs of all your nodes
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 使用JSONPath获取所有节点的IP地址
- en: 'You can find the IP in the JSON or YAML descriptors of the nodes. But instead
    of sifting through the relatively large JSON, you can tell `kubectl` to print
    out only the node IP instead of the whole service definition:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在节点的JSON或YAML描述符中找到IP地址。但您不必在相对较大的JSON中筛选，您可以告诉`kubectl`仅打印节点IP而不是整个服务定义：
- en: '`$ kubectl get nodes -o jsonpath=''{.items[*].status.`![](images/00006.jpg)`addresses[?(@.type=="ExternalIP")].address}''`
    `130.211.97.55 130.211.99.206`'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get nodes -o jsonpath=''{.items[*].status.`![](images/00006.jpg)`addresses[?(@.type=="ExternalIP")].address}''`
    `130.211.97.55 130.211.99.206`'
- en: 'You’re telling `kubectl` to only output the information you want by specifying
    a JSONPath. You’re probably familiar with XPath and how it’s used with XML. JSONPath
    is basically XPath for JSON. The JSONPath in the previous example instructs `kubectl`
    to do the following:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 您通过指定JSONPath来告诉`kubectl`仅输出您想要的信息。您可能熟悉XPath及其在XML中的应用。JSONPath基本上是XPath的JSON版本。上一个示例中的JSONPath指示`kubectl`执行以下操作：
- en: Go through all the elements in the `items` attribute.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历`items`属性中的所有元素。
- en: For each element, enter the `status` attribute.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个元素，进入`status`属性。
- en: Filter elements of the `addresses` attribute, taking only those that have the
    `type` attribute set to `ExternalIP`.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过滤`addresses`属性的元素，仅选择那些将`type`属性设置为`ExternalIP`的元素。
- en: Finally, print the `address` attribute of the filtered elements.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，打印过滤元素的`address`属性。
- en: To learn more about how to use JSONPath with `kubectl`, refer to the documentation
    at [http://kubernetes.io/docs/user-guide/jsonpath](http://kubernetes.io/docs/user-guide/jsonpath).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解有关如何使用`kubectl`与JSONPath的更多信息，请参阅[http://kubernetes.io/docs/user-guide/jsonpath](http://kubernetes.io/docs/user-guide/jsonpath)文档。
- en: '|  |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Once you know the IPs of your nodes, you can try accessing your service through
    them:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您知道了节点的 IP 地址，您可以通过它们尝试访问您的服务：
- en: '`$ curl http://130.211.97.55:30123` `You''ve hit kubia-ym8or` `$ curl http://130.211.99.206:30123`
    `You''ve hit kubia-xueq1`'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl http://130.211.97.55:30123` `您已访问 kubia-ym8or` `$ curl http://130.211.99.206:30123`
    `您已访问 kubia-xueq1`'
- en: '|  |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: When using Minikube, you can easily access your `NodePort` services through
    your browser by running `minikube service <service-name> [-n <namespace>]`.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Minikube 时，您可以通过运行 `minikube service <service-name> [-n <namespace>]` 命令，轻松通过浏览器访问您的
    `NodePort` 服务。
- en: '|  |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: As you can see, your pods are now accessible to the whole internet through port
    30123 on any of your nodes. It doesn’t matter what node a client sends the request
    to. But if you only point your clients to the first node, when that node fails,
    your clients can’t access the service anymore. That’s why it makes sense to put
    a load balancer in front of the nodes to make sure you’re spreading requests across
    all healthy nodes and never sending them to a node that’s offline at that moment.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，您的 pod 现在可以通过您任何节点的 30123 端口访问整个互联网。客户端发送请求到哪个节点无关紧要。但是，如果您只将客户端指向第一个节点，当该节点失败时，客户端将无法再访问服务。这就是为什么在节点前放置负载均衡器以确保您正在将请求分散到所有健康节点，并且永远不会将请求发送到当时离线的节点是有意义的。
- en: If your Kubernetes cluster supports it (which is mostly true when Kubernetes
    is deployed on cloud infrastructure), the load balancer can be provisioned automatically
    by creating a `LoadBalancer` instead of a `NodePort` service. We’ll look at this
    next.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的 Kubernetes 集群支持此功能（当 Kubernetes 部署在云基础设施上时通常支持），则可以通过创建一个 `LoadBalancer`
    而不是 `NodePort` 服务来自动配置负载均衡器。我们将在下一节中探讨这一点。
- en: 5.3.2\. Exposing a service through an external load balancer
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 5.3.2\. 通过外部负载均衡器公开服务
- en: Kubernetes clusters running on cloud providers usually support the automatic
    provision of a load balancer from the cloud infrastructure. All you need to do
    is set the service’s type to `LoadBalancer` instead of `NodePort`. The load balancer
    will have its own unique, publicly accessible IP address and will redirect all
    connections to your service. You can thus access your service through the load
    balancer’s IP address.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在云提供商上运行的 Kubernetes 集群通常支持从云基础设施自动配置负载均衡器。您需要做的只是将服务的类型设置为 `LoadBalancer` 而不是
    `NodePort`。负载均衡器将拥有自己的唯一、公开可访问的 IP 地址，并将所有连接重定向到您的服务。因此，您可以通过负载均衡器的 IP 地址访问您的服务。
- en: If Kubernetes is running in an environment that doesn’t support `LoadBalancer`
    services, the load balancer will not be provisioned, but the service will still
    behave like a `NodePort` service. That’s because a `LoadBalancer` service is an
    extension of a `NodePort` service. You’ll run this example on Google Kubernetes
    Engine, which supports `LoadBalancer` services. Minikube doesn’t, at least not
    as of this writing.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 Kubernetes 运行在不支持 `LoadBalancer` 服务的环境中，则不会配置负载均衡器，但服务仍将像 `NodePort` 服务一样运行。这是因为
    `LoadBalancer` 服务是 `NodePort` 服务的扩展。您将在支持 `LoadBalancer` 服务的 Google Kubernetes
    Engine 上运行此示例。Minikube 不支持，至少在本写作时如此。
- en: Creating a LoadBalancer service
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 创建负载均衡器服务
- en: To create a service with a load balancer in front, create the service from the
    following YAML manifest, as shown in the following listing.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个前面有负载均衡器的服务，请从以下 YAML 清单创建服务，如下所示。
- en: 'Listing 5.12\. A `LoadBalancer`-type service: kubia-svc-loadbalancer.yaml'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.12\. `LoadBalancer` 类型服务：kubia-svc-loadbalancer.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia-loadbalancer spec:  
    type: LoadBalancer` `1` `ports:   - port: 80     targetPort: 8080   selector:
        app: kubia`'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia-loadbalancer spec:  
    type: LoadBalancer` `1` `ports:   - port: 80     targetPort: 8080   selector:
        app: kubia`'
- en: 1 This type of service obtains a load balancer from the infrastructure hosting
    the Kubernetes cluster.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此类服务从托管 Kubernetes 集群的底层基础设施中获取负载均衡器。
- en: The service type is set to `LoadBalancer` instead of `NodePort`. You’re not
    specifying a specific node port, although you could (you’re letting Kubernetes
    choose one instead).
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 服务类型设置为 `LoadBalancer` 而不是 `NodePort`。您没有指定特定的节点端口，尽管您可以（您让 Kubernetes 选择一个）。
- en: Connecting to the service through the load balancer
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 通过负载均衡器连接到服务
- en: 'After you create the service, it takes time for the cloud infrastructure to
    create the load balancer and write its IP address into the Service object. Once
    it does that, the IP address will be listed as the external IP address of your
    service:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建服务后，云基础设施需要时间来创建负载均衡器并将它的IP地址写入服务对象。一旦完成，IP地址将作为服务的公网IP地址列出：
- en: '`$ kubectl get svc kubia-loadbalancer` `NAME                 CLUSTER-IP      
    EXTERNAL-IP      PORT(S)         AGE kubia-loadbalancer   10.111.241.153` `130.211.53.173``  
    80:32143/TCP    1m`'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get svc kubia-loadbalancer` `NAME                 CLUSTER-IP      
    EXTERNAL-IP      PORT(S)         AGE kubia-loadbalancer   10.111.241.153` `130.211.53.173``  
    80:32143/TCP    1m`'
- en: 'In this case, the load balancer is available at IP 130.211.53.173, so you can
    now access the service at that IP address:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，负载均衡器在IP 130.211.53.173上可用，因此你现在可以通过该IP地址访问服务：
- en: '`$ curl http://130.211.53.173` `You''ve hit kubia-xueq1`'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl http://130.211.53.173` `你访问了kubia-xueq1`'
- en: Success! As you may have noticed, this time you didn’t need to mess with firewalls
    the way you had to before with the `NodePort` service.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 成功！正如你可能注意到的，这次你不需要像之前使用`NodePort`服务那样去配置防火墙。
- en: '|  |'
  id: totrans-283
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Session affinity and web browsers
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 会话亲和性与网页浏览器
- en: Because your service is now exposed externally, you may try accessing it with
    your web browser. You’ll see something that may strike you as odd—the browser
    will hit the exact same pod every time. Did the service’s session affinity change
    in the meantime? With `kubectl explain`, you can double-check that the service’s
    session affinity is still set to `None`, so why don’t different browser requests
    hit different pods, as is the case when using `curl`?
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你的服务现在已对外公开，你可以尝试使用你的网页浏览器访问它。你会发现一些可能让你觉得奇怪的事情——浏览器每次都会访问到完全相同的Pod。服务在这期间是否改变了会话亲和性？使用`kubectl
    explain`，你可以再次确认服务的会话亲和性仍然设置为`None`，那么为什么不同的浏览器请求没有击中不同的Pod，就像使用`curl`时那样呢？
- en: Let me explain what’s happening. The browser is using keep-alive connections
    and sends all its requests through a single connection, whereas `curl` opens a
    new connection every time. Services work at the connection level, so when a connection
    to a service is first opened, a random pod is selected and then all network packets
    belonging to that connection are all sent to that single pod. Even if session
    affinity is set to `None`, users will always hit the same pod (until the connection
    is closed).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我解释一下正在发生的事情。浏览器正在使用持久连接，并通过单个连接发送所有请求，而`curl`每次都会打开一个新的连接。服务在连接级别上工作，所以当第一次打开到服务的连接时，会随机选择一个Pod，然后所有属于该连接的网络包都会发送到那个单个Pod。即使会话亲和性设置为`None`，用户也总是会击中同一个Pod（直到连接关闭）。
- en: '|  |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: See [figure 5.7](#filepos532150) to see how HTTP requests are delivered to the
    pod. External clients (`curl` in your case) connect to port 80 of the load balancer
    and get routed to the implicitly assigned node port on one of the nodes. From
    there, the connection is forwarded to one of the pod instances.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 见[图5.7](#filepos532150)了解HTTP请求是如何被发送到Pod的。外部客户端（在你的情况下是`curl`）连接到负载均衡器的80端口，并被路由到某个节点上隐式分配的节点端口。从那里，连接被转发到Pod实例之一。
- en: Figure 5.7\. An external client connecting to a `LoadBalancer` service
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7\. 一个外部客户端连接到`LoadBalancer`服务
- en: '![](images/00192.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00192.jpg)'
- en: As already mentioned, a `LoadBalancer`-type service is a `NodePort` service
    with an additional infrastructure-provided load balancer. If you use `kubectl
    describe` to display additional info about the service, you’ll see that a node
    port has been selected for the service. If you were to open the firewall for this
    port, the way you did in the previous section about `NodePort` services, you could
    access the service through the node IPs as well.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，`LoadBalancer`类型的服务是一个带有额外基础设施提供的负载均衡器的`NodePort`服务。如果你使用`kubectl describe`来显示有关服务的更多信息，你会看到为服务选择了一个节点端口。如果你像在关于`NodePort`服务的上一节中那样为该端口打开防火墙，你也可以通过节点IP访问服务。
- en: '|  |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you’re using Minikube, even though the load balancer will never be provisioned,
    you can still access the service through the node port (at the Minikube VM’s IP
    address).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用Minikube，即使负载均衡器永远不会被配置，你仍然可以通过节点端口（在Minikube虚拟机的IP地址）访问服务。
- en: '|  |'
  id: totrans-295
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 5.3.3\. Understanding the peculiarities of external connections
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 5.3.3\. 理解外部连接的特有之处
- en: You must be aware of several things related to externally originating connections
    to services.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你必须注意与外部发起的服务连接相关的几个事项。
- en: Understanding and preventing unnecessary network hops
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 理解和防止不必要的网络跳转
- en: When an external client connects to a service through the node port (this also
    includes cases when it goes through the load balancer first), the randomly chosen
    pod may or may not be running on the same node that received the connection. An
    additional network hop is required to reach the pod, but this may not always be
    desirable.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 当外部客户端通过节点端口连接到服务（这也包括首先通过负载均衡器的情况），随机选择的Pod可能或可能不在接收连接的同一节点上运行。需要额外的网络跳转才能到达Pod，但这可能并不总是希望的。
- en: 'You can prevent this additional hop by configuring the service to redirect
    external traffic only to pods running on the node that received the connection.
    This is done by setting the `externalTrafficPolicy` field in the service’s `spec`
    section:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过配置服务，仅将外部流量重定向到运行在接收连接的节点上的Pod来防止这种额外的跳转。这是通过在服务的`spec`部分设置`externalTrafficPolicy`字段来完成的：
- en: '`spec:   externalTrafficPolicy: Local   ...`'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec: externalTrafficPolicy: Local ...`'
- en: If a service definition includes this setting and an external connection is
    opened through the service’s node port, the service proxy will choose a locally
    running pod. If no local pods exist, the connection will hang (it won’t be forwarded
    to a random global pod, the way connections are when not using the annotation).
    You therefore need to ensure the load balancer forwards connections only to nodes
    that have at least one such pod.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务定义包括此设置，并且通过服务的节点端口打开外部连接，服务代理将选择本地运行的Pod。如果没有本地Pod存在，连接将挂起（不会像不使用注释时那样将连接转发到随机全局Pod）。因此，您需要确保负载均衡器仅将连接转发到至少有一个此类Pod的节点。
- en: Using this annotation also has other drawbacks. Normally, connections are spread
    evenly across all the pods, but when using this annotation, that’s no longer the
    case.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此注释也有其他缺点。通常，连接会在所有Pod之间均匀分配，但使用此注释时，情况就不再是这样了。
- en: Imagine having two nodes and three pods. Let’s say node A runs one pod and node
    B runs the other two. If the load balancer spreads connections evenly across the
    two nodes, the pod on node A will receive 50% of all connections, but the two
    pods on node B will only receive 25% each, as shown in [figure 5.8](#filepos535550).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 想象有两个节点和三个Pod。假设节点A运行一个Pod，节点B运行另外两个Pod。如果负载均衡器将连接均匀地分配到两个节点，节点A上的Pod将接收所有连接的50%，但节点B上的两个Pod每个只能接收25%，如图[5.8](#filepos535550)所示。
- en: Figure 5.8\. A Service using the `Local` external traffic policy may lead to
    uneven load distribution across pods.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8\. 使用`Local`外部流量策略的服务可能导致Pod之间的负载分布不均。
- en: '![](images/00034.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00034.jpg)'
- en: Being aware of the non-preservation of the client’s IP
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 了解客户端IP不保留的情况
- en: Usually, when clients inside the cluster connect to a service, the pods backing
    the service can obtain the client’s IP address. But when the connection is received
    through a node port, the packets’ source IP is changed, because Source Network
    Address Translation (SNAT) is performed on the packets.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当集群内的客户端连接到服务时，支持服务的Pod可以获取客户端的IP地址。但通过节点端口接收连接时，数据包的源IP会改变，因为数据包在源网络地址转换（SNAT）上执行。
- en: The backing pod can’t see the actual client’s IP, which may be a problem for
    some applications that need to know the client’s IP. In the case of a web server,
    for example, this means the access log won’t show the browser’s IP.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 支持Pod看不到实际的客户端IP，这可能对一些需要知道客户端IP的应用程序来说是个问题。例如，对于Web服务器来说，这意味着访问日志不会显示浏览器的IP。
- en: The `Local` external traffic policy described in the previous section affects
    the preservation of the client’s IP, because there’s no additional hop between
    the node receiving the connection and the node hosting the target pod (SNAT isn’t
    performed).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 上一个章节中描述的`Local`外部流量策略会影响客户端IP的保留，因为没有在接收连接的节点和托管目标Pod的节点之间增加额外的跳转（不会执行SNAT）。
- en: 5.4\. Exposing services externally through an Ingress resource
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4\. 通过Ingress资源外部公开服务
- en: You’ve now seen two ways of exposing a service to clients outside the cluster,
    but another method exists—creating an Ingress resource.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经看到了两种将服务公开给集群外客户端的方法，但还存在另一种方法——创建Ingress资源。
- en: '|  |'
  id: totrans-313
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Definition
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 定义
- en: Ingress (noun)—The act of going in or entering; the right to enter; a means
    or place of entering; entryway.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress（名词）——进入或进入的行为；进入的权利；进入的手段或地方；入口。
- en: '|  |'
  id: totrans-316
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let me first explain why you need another way to access Kubernetes services
    from the outside.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 让我先解释一下为什么你需要另一种从外部访问Kubernetes服务的方法。
- en: Understanding why Ingresses are needed
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么需要Ingress
- en: One important reason is that each `LoadBalancer` service requires its own load
    balancer with its own public IP address, whereas an Ingress only requires one,
    even when providing access to dozens of services. When a client sends an HTTP
    request to the Ingress, the host and path in the request determine which service
    the request is forwarded to, as shown in [figure 5.9](#filepos538264).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的原因是，每个 `LoadBalancer` 服务都需要自己的负载均衡器及其自己的公网 IP 地址，而 Ingress 只需要一个是足够的，即使它提供了对数十个服务的访问。当客户端向
    Ingress 发送 HTTP 请求时，请求中的主机和路径决定了请求被转发到哪个服务，如图 5.9 所示。
- en: Figure 5.9\. Multiple services can be exposed through a single Ingress.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9\. 可以通过单个 Ingress 暴露多个服务。
- en: '![](images/00046.jpg)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](images/00046.jpg)'
- en: Ingresses operate at the application layer of the network stack (HTTP) and can
    provide features such as cookie-based session affinity and the like, which services
    can’t.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress 在网络堆栈的应用层（HTTP）中运行，可以提供诸如基于 cookie 的会话亲和力等功能，这是服务所不能提供的。
- en: Understanding that an Ingress controller is required
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 理解需要 Ingress 控制器
- en: Before we go into the features an Ingress object provides, let me emphasize
    that to make Ingress resources work, an Ingress controller needs to be running
    in the cluster. Different Kubernetes environments use different implementations
    of the controller, but several don’t provide a default controller at all.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入了解 Ingress 对象提供的功能之前，让我强调一点，为了使 Ingress 资源工作，集群中需要运行 Ingress 控制器。不同的 Kubernetes
    环境使用不同的控制器实现，但其中一些根本不提供默认控制器。
- en: For example, Google Kubernetes Engine uses Google Cloud Platform’s own HTTP
    load-balancing features to provide the Ingress functionality. Initially, Minikube
    didn’t provide a controller out of the box, but it now includes an add-on that
    can be enabled to let you try out the Ingress functionality. Follow the instructions
    in the following sidebar to ensure it’s enabled.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，Google Kubernetes Engine 使用 Google Cloud Platform 自身的 HTTP 负载均衡功能来提供 Ingress
    功能。最初，Minikube 并没有提供默认的控制器，但现在它包含了一个可以启用的插件，让你可以尝试 Ingress 功能。按照以下侧边栏中的说明确保已启用。
- en: '|  |'
  id: totrans-326
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Enabling the Ingress add-on in Minikube
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Minikube 中启用 Ingress 插件
- en: 'If you’re using Minikube to run the examples in this book, you’ll need to ensure
    the Ingress add-on is enabled. You can check whether it is by listing all the
    add-ons:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 Minikube 运行本书中的示例，你需要确保 Ingress 插件已启用。你可以通过列出所有插件来检查它是否已启用：
- en: '`$ minikube addons list` `- default-storageclass: enabled - kube-dns: enabled
    - heapster: disabled - ingress: disabled` `1` `- registry-creds: disabled - addon-manager:
    enabled - dashboard: enabled`'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ minikube addons list` `- default-storageclass: enabled - kube-dns: enabled
    - heapster: disabled - ingress: disabled` `1` `- registry-creds: disabled - addon-manager:
    enabled - dashboard: enabled`'
- en: 1 The Ingress add-on isn’t enabled.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 Ingress 插件尚未启用。
- en: 'You’ll learn about what these add-ons are throughout the book, but it should
    be pretty clear what the `dashboard` and the `kube-dns` add-ons do. Enable the
    Ingress add-on so you can see Ingresses in action:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本书中了解这些插件是什么，但应该很清楚 `dashboard` 和 `kube-dns` 插件的作用。启用 Ingress 插件以便你可以看到 Ingress
    的实际操作：
- en: '`$ minikube addons enable ingress` `ingress was successfully enabled`'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ minikube addons enable ingress` `ingress was successfully enabled`'
- en: 'This should have spun up an Ingress controller as another pod. Most likely,
    the controller pod will be in the `kube-system` namespace, but not necessarily,
    so list all the running pods across all namespaces by using the `--all-namespaces`
    option:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该已经启动了一个 Ingress 控制器作为另一个 pod。最可能的情况是，控制器 pod 将位于 `kube-system` 命名空间中，但不一定是这样，所以使用
    `--all-namespaces` 选项列出所有命名空间中的所有正在运行的 pod：
- en: '`$ kubectl get po --all-namespaces` `NAMESPACE    NAME                           
    READY  STATUS    RESTARTS AGE default      kubia-rsv5m                     1/1   
    Running   0        13h default      kubia-fe4ad                     1/1    Running  
    0        13h default      kubia-ke823                     1/1    Running   0       
    13h kube-system  default-http-backend-5wb0h      1/1    Running   0        18m
    kube-system  kube-addon-manager-minikube     1/1    Running   3        6d kube-system 
    kube-dns-v20-101vq              3/3    Running   9        6d kube-system  kubernetes-dashboard-jxd9l     
    1/1    Running   3        6d kube-system  nginx-ingress-controller-gdts0  1/1   
    Running   0        18m`'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po --all-namespaces` `NAMESPACE    NAME                           
    READY  STATUS    RESTARTS AGE default      kubia-rsv5m                     1/1   
    Running   0        13h default      kubia-fe4ad                     1/1    Running  
    0        13h default      kubia-ke823                     1/1    Running   0       
    13h kube-system  default-http-backend-5wb0h      1/1    Running   0        18m
    kube-system  kube-addon-manager-minikube     1/1    Running   3        6d kube-system 
    kube-dns-v20-101vq              3/3    Running   9        6d kube-system  kubernetes-dashboard-jxd9l     
    1/1    Running   3        6d kube-system  nginx-ingress-controller-gdts0  1/1   
    Running   0        18m`'
- en: At the bottom of the output, you see the Ingress controller pod. The name suggests
    that Nginx (an open-source HTTP server and reverse proxy) is used to provide the
    Ingress functionality.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在输出底部，你可以看到Ingress控制器Pod。名称表明Nginx（一个开源的HTTP服务器和反向代理）被用来提供Ingress功能。
- en: '|  |'
  id: totrans-336
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-337
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: The `--all-namespaces` option mentioned in the sidebar is handy when you don’t
    know what namespace your pod (or other type of resource) is in, or if you want
    to list resources across all namespaces.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 侧边栏中提到的`--all-namespaces`选项在你不知道你的Pod（或其他类型的资源）位于哪个命名空间，或者你想列出所有命名空间中的资源时很有用。
- en: '|  |'
  id: totrans-340
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.4.1\. Creating an Ingress resource
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4.1\. 创建Ingress资源
- en: You’ve confirmed there’s an Ingress controller running in your cluster, so you
    can now create an Ingress resource. The following listing shows what the YAML
    manifest for the Ingress looks like.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 你已确认你的集群中正在运行Ingress控制器，因此你现在可以创建Ingress资源。以下列表显示了Ingress的YAML清单的外观。
- en: 'Listing 5.13\. An Ingress resource definition: kubia-ingress.yaml'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.13\. Ingress资源定义：kubia-ingress.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: kubia spec:
      rules:   - host: kubia.example.com` `1` `http:       paths:       - path: /`
    `2` `backend:           serviceName: kubia-nodeport` `2` `servicePort: 80` `2`'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: kubia spec:
      rules:   - host: kubia.example.com` `1` `http:       paths:       - path: /`
    `2` `backend:           serviceName: kubia-nodeport` `2` `servicePort: 80` `2`'
- en: 1 This Ingress maps the kubia.example.com domain name to your service.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 此Ingress将kubia.example.com域名映射到你的服务。
- en: 2 All requests will be sent to port 80 of the kubia-nodeport service.
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 所有请求都将发送到kubia-nodeport服务的80端口。
- en: This defines an Ingress with a single rule, which makes sure all HTTP requests
    received by the Ingress controller, in which the host `kubia.example.com` is requested,
    will be sent to the `kubia-nodeport` service on port `80`.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这定义了一个包含单个规则的Ingress，确保所有通过Ingress控制器接收到的、请求主机`kubia.example.com`的HTTP请求都将发送到端口`80`上的`kubia-nodeport`服务。
- en: '|  |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Ingress controllers on cloud providers (in GKE, for example) require the Ingress
    to point to a `NodePort` service. But that’s not a requirement of Kubernetes itself.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 云提供商上的Ingress控制器（例如在GKE上）需要Ingress指向一个`NodePort`服务。但这不是Kubernetes本身的必要条件。
- en: '|  |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.4.2\. Accessing the service through the Ingress
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4.2\. 通过Ingress访问服务
- en: To access your service through http://kubia.example.com, you’ll need to make
    sure the domain name resolves to the IP of the Ingress controller.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过http://kubia.example.com访问你的服务，你需要确保域名解析到Ingress控制器的IP地址。
- en: Obtaining the IP address of the Ingress
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 获取Ingress的IP地址
- en: 'To look up the IP, you need to list Ingresses:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找IP，你需要列出Ingress：
- en: '`$ kubectl get ingresses` `NAME      HOSTS               ADDRESS          PORTS    
    AGE kubia     kubia.example.com   192.168.99.100   80        29m`'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get ingresses` `NAME      HOSTS               ADDRESS          PORTS    
    AGE kubia     kubia.example.com   192.168.99.100   80        29m`'
- en: '|  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: When running on cloud providers, the address may take time to appear, because
    the Ingress controller provisions a load balancer behind the scenes.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当在云提供商上运行时，地址可能需要一段时间才能出现，因为Ingress控制器在幕后配置了一个负载均衡器。
- en: '|  |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The IP is shown in the `ADDRESS` column.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: IP地址显示在`ADDRESS`列中。
- en: Ensuring the host configured in the Ingress points to the Ingress’ IP address
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Ingress中配置的主机指向Ingress的IP地址
- en: 'Once you know the IP, you can then either configure your DNS servers to resolve
    kubia.example.com to that IP or you can add the following line to `/etc/hosts`
    (or `C:\windows\system32\drivers\etc\hosts` on Windows):'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您知道了IP，您可以选择配置您的DNS服务器将kubia.example.com解析到该IP，或者您可以在`/etc/hosts`（或在Windows上的`C:\windows\system32\drivers\etc\hosts`）中添加以下行：
- en: '`192.168.99.100    kubia.example.com`'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '`192.168.99.100    kubia.example.com`'
- en: Accessing pods through the Ingress
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 通过Ingress访问Pod
- en: 'Everything is now set up, so you can access the service at http://kubia.example.com
    (using a browser or `curl`):'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 一切都已设置好，因此您可以通过http://kubia.example.com（使用浏览器或`curl`）访问服务：
- en: '`$ curl http://kubia.example.com` `You''ve hit kubia-ke823`'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl http://kubia.example.com` `您已访问kubia-ke823`'
- en: You’ve successfully accessed the service through an Ingress. Let’s take a better
    look at how that unfolded.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 您已成功通过Ingress访问了服务。让我们更详细地看看这是如何展开的。
- en: Understanding how Ingresses work
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 理解Ingress的工作原理
- en: '[Figure 5.10](#filepos548026) shows how the client connected to one of the
    pods through the Ingress controller. The client first performed a DNS lookup of
    kubia.example.com, and the DNS server (or the local operating system) returned
    the IP of the Ingress controller. The client then sent an HTTP request to the
    Ingress controller and specified `kubia.example.com` in the `Host` header. From
    that header, the controller determined which service the client is trying to access,
    looked up the pod IPs through the Endpoints object associated with the service,
    and forwarded the client’s request to one of the pods.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[图5.10](#filepos548026)显示了客户端如何通过Ingress控制器连接到一个Pod。客户端首先对kubia.example.com执行DNS查找，DNS服务器（或本地操作系统）返回Ingress控制器的IP。然后客户端向Ingress控制器发送HTTP请求，并在`Host`头中指定`kubia.example.com`。从该头信息中，控制器确定了客户端试图访问哪个服务，通过服务关联的Endpoints对象查找Pod
    IP，并将客户端的请求转发到其中一个Pod。'
- en: Figure 5.10\. Accessing pods through an Ingress
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10\. 通过Ingress访问Pod
- en: '![](images/00065.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00065.jpg)'
- en: As you can see, the Ingress controller didn’t forward the request to the service.
    It only used it to select a pod. Most, if not all, controllers work like this.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，Ingress控制器没有将请求转发到服务。它只是用它来选择一个Pod。大多数，如果不是所有控制器都是这样工作的。
- en: 5.4.3\. Exposing multiple services through the same Ingress
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4.3\. 通过同一Ingress暴露多个服务
- en: If you look at the Ingress spec closely, you’ll see that both `rules` and `paths`
    are arrays, so they can contain multiple items. An Ingress can map multiple hosts
    and paths to multiple services, as you’ll see next. Let’s focus on `paths` first.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细查看Ingress规范，您会看到`rules`和`paths`都是数组，因此它们可以包含多个项目。Ingress可以将多个主机和路径映射到多个服务，您将在下面看到。让我们首先关注`paths`。
- en: Mapping different services to different paths of the same host
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同的服务映射到同一主机的不同路径
- en: You can map multiple `paths` on the same host to different services, as shown
    in the following listing.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将同一主机上的多个`path`映射到不同的服务，如下列所示。
- en: Listing 5.14\. Ingress exposing multiple services on same host, but different
    `path`s
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.14\. 在同一主机上暴露多个服务，但不同的`path`
- en: '`...   - host: kubia.example.com     http:       paths:       - path: /kubia`
    `1` `backend:` `1` `serviceName: kubia` `1` `servicePort: 80` `1` `- path: /foo`
    `2` `backend:` `2` `serviceName: bar` `2` `servicePort: 80` `2`'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: '`...   - host: kubia.example.com   http:   paths:   - path: /kubia   1   backend:  
    1   serviceName: kubia   1   servicePort: 80   1   - path: /foo   2   backend:  
    2   serviceName: bar   2   servicePort: 80   2`'
- en: 1 Requests to kubia.example.com/kubia will be routed to the kubia service.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 请求到kubia.example.com/kubia将被路由到kubia服务。
- en: 2 Requests to kubia.example.com/bar will be routed to the bar service.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 请求到kubia.example.com/bar将被路由到bar服务。
- en: In this case, requests will be sent to two different services, depending on
    the path in the requested URL. Clients can therefore reach two different services
    through a single IP address (that of the Ingress controller).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，根据请求的URL路径，请求将被发送到两个不同的服务。因此，客户端可以通过单个IP地址（即Ingress控制器所在的IP地址）访问两个不同的服务。
- en: Mapping different services to different hosts
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同的服务映射到不同的主机
- en: Similarly, you can use an Ingress to map to different services based on the
    host in the HTTP request instead of (only) the path, as shown in the next listing.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 类似地，您可以使用Ingress根据HTTP请求中的主机而不是（仅）路径来映射到不同的服务，如下列所示。
- en: Listing 5.15\. Ingress exposing multiple services on different hosts
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.15\. 在不同主机上暴露多个服务的Ingress
- en: '`spec:   rules:   - host: foo.example.com` `1` `http:       paths:       -
    path: /         backend:           serviceName: foo` `1` `servicePort: 80   -
    host: bar.example.com` `2` `http:       paths:       - path: /         backend:
              serviceName: bar` `2` `servicePort: 80`'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '`spec:    rules:    - host: foo.example.com` `1` `http:        paths:        -
    path: /        backend:            serviceName: foo` `1` `servicePort: 80    -
    host: bar.example.com` `2` `http:        paths:        - path: /        backend:            serviceName:
    bar` `2` `servicePort: 80`'
- en: 1 Requests for foo.example.com will be routed to service foo.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 请求 foo.example.com 将被路由到服务 foo。
- en: 2 Requests for bar.example.com will be routed to service bar.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 请求 bar.example.com 将被路由到服务 bar。
- en: Requests received by the controller will be forwarded to either service `foo`
    or `bar`, depending on the `Host` header in the request (the way virtual hosts
    are handled in web servers). DNS needs to point both the foo.example.com and the
    bar.example.com domain names to the Ingress controller’s IP address.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 控制器接收到的请求将被转发到服务 `foo` 或 `bar`，具体取决于请求中的 `Host` 头（类似于在 Web 服务器中处理虚拟主机的方式）。DNS
    需要将 foo.example.com 和 bar.example.com 域名都指向 Ingress 控制器的 IP 地址。
- en: 5.4.4\. Configuring Ingress to handle TLS traffic
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 5.4.4. 配置 Ingress 以处理 TLS 流量
- en: You’ve seen how an Ingress forwards HTTP traffic. But what about HTTPS? Let’s
    take a quick look at how to configure Ingress to support TLS.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经看到了 Ingress 如何转发 HTTP 流量。那么 HTTPS 呢？让我们快速了解一下如何配置 Ingress 以支持 TLS。
- en: Creating a TLS certificate for the Ingress
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Ingress 创建 TLS 证书
- en: When a client opens a TLS connection to an Ingress controller, the controller
    terminates the TLS connection. The communication between the client and the controller
    is encrypted, whereas the communication between the controller and the backend
    pod isn’t. The application running in the pod doesn’t need to support TLS. For
    example, if the pod runs a web server, it can accept only HTTP traffic and let
    the Ingress controller take care of everything related to TLS. To enable the controller
    to do that, you need to attach a certificate and a private key to the Ingress.
    The two need to be stored in a Kubernetes resource called a Secret, which is then
    referenced in the Ingress manifest. We’ll explain Secrets in detail in [chapter
    7](index_split_063.html#filepos687721). For now, you’ll create the Secret without
    paying too much attention to it.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 当客户端打开到 Ingress 控制器的 TLS 连接时，控制器将终止 TLS 连接。客户端与控制器之间的通信是加密的，而控制器与后端 Pod 之间的通信则不是。Pod
    中运行的应用程序不需要支持 TLS。例如，如果 Pod 运行的是 Web 服务器，它只能接受 HTTP 流量，而让 Ingress 控制器处理所有与 TLS
    相关的事情。为了使控制器能够这样做，您需要将证书和私钥附加到 Ingress 上。这两个文件需要存储在名为 Secret 的 Kubernetes 资源中，然后在
    Ingress 清单中引用它。我们将在第 7 章（[index_split_063.html#filepos687721](index_split_063.html#filepos687721)）中详细解释
    Secrets。现在，您将创建 Secret，但不必过分关注它。
- en: 'First, you need to create the private key and certificate:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要创建私钥和证书：
- en: '`$ openssl genrsa -out tls.key 2048``$ openssl req -new -x509 -key tls.key
    -out tls.cert -days 360 -subj`![](images/00006.jpg)`/CN=kubia.example.com`'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ openssl genrsa -out tls.key 2048` `$ openssl req -new -x509 -key tls.key
    -out tls.cert -days 360 -subj`![](images/00006.jpg)`/CN=kubia.example.com`'
- en: 'Then you create the Secret from the two files like this:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以根据这两个文件创建 Secret，如下所示：
- en: '`$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key` `secret
    "tls-secret" created`'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl create secret tls tls-secret --cert=tls.cert --key=tls.key` `secret
    "tls-secret" created`'
- en: '|  |'
  id: totrans-398
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Signing certificates through the CertificateSigningRequest resource
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 CertificateSigningRequest 资源签名证书
- en: 'Instead of signing the certificate ourselves, you can get the certificate signed
    by creating a `CertificateSigningRequest` (CSR) resource. Users or their applications
    can create a regular certificate request, put it into a CSR, and then either a
    human operator or an automated process can approve the request like this:'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过创建 `CertificateSigningRequest`（CSR）资源来获取证书的签名，而不是自己签名证书。用户或他们的应用程序可以创建一个常规证书请求，将其放入
    CSR 中，然后由人工操作员或自动化流程批准请求，如下所示：
- en: '`$ kubectl certificate approve <name of the CSR>`'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl certificate approve <CSR 名称>`'
- en: The signed certificate can then be retrieved from the CSR’s `status.certificate`
    field.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 签名的证书可以从 CSR 的 `status.certificate` 字段中检索。
- en: Note that a certificate signer component must be running in the cluster; otherwise
    creating `CertificateSigningRequest` and approving or denying them won’t have
    any effect.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，必须在集群中运行证书签名组件；否则，创建 `CertificateSigningRequest` 以及批准或拒绝它们将没有任何效果。
- en: '|  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The private key and the certificate are now stored in the Secret called `tls-secret`.
    Now, you can update your Ingress object so it will also accept HTTPS requests
    for kubia.example.com. The Ingress manifest should now look like the following
    listing.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 私钥和证书现在存储在名为`tls-secret`的Secret中。现在，您可以更新Ingress对象，使其也接受kubia.example.com的HTTPS请求。Ingress清单现在应如下所示。
- en: 'Listing 5.16\. Ingress handling TLS traffic: kubia-ingress-tls.yaml'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.16\. Ingress处理TLS流量：kubia-ingress-tls.yaml
- en: '`apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: kubia spec:
      tls:` `1` `- hosts:` `2` `- kubia.example.com` `2` `secretName: tls-secret`
    `3` `rules:   - host: kubia.example.com     http:       paths:       - path: /
            backend:           serviceName: kubia-nodeport           servicePort:
    80`'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: extensions/v1beta1 kind: Ingress metadata:   name: kubia spec:
      tls:` `1` `- hosts:` `2` `- kubia.example.com` `2` `secretName: tls-secret`
    `3` `rules:   - host: kubia.example.com     http:       paths:       - path: /
            backend:           serviceName: kubia-nodeport           servicePort:
    80`'
- en: 1 The whole TLS configuration is under this attribute.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 整个TLS配置都包含在这个属性中。
- en: 2 TLS connections will be accepted for the kubia.example.com hostname.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 2 将接受对kubia.example.com主机名的TLS连接。
- en: 3 The private key and the certificate should be obtained from the tls-secret
    you created previously.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 3 应从之前创建的tls-secret中获取私钥和证书。
- en: '|  |'
  id: totrans-411
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 提示
- en: Instead of deleting the Ingress and re-creating it from the new file, you can
    invoke `kubectl apply -f kubia-ingress-tls.yaml`, which updates the Ingress resource
    with what’s specified in the file.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 您无需删除Ingress并从新文件中重新创建它，而是可以调用`kubectl apply -f kubia-ingress-tls.yaml`，这将使用文件中指定的内容更新Ingress资源。
- en: '|  |'
  id: totrans-414
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'You can now use HTTPS to access your service through the Ingress:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以通过Ingress使用HTTPS访问您的服务：
- en: '`$ curl -k -v https://kubia.example.com/kubia` `* About to connect() to kubia.example.com
    port 443 (#0) ... * Server certificate: *   subject: CN=kubia.example.com ...
    > GET /kubia HTTP/1.1 > ... You''ve hit kubia-xueq1`'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl -k -v https://kubia.example.com/kubia` `* 即将连接()到kubia.example.com端口443
    (#0) ... * 服务器证书： *   主题: CN=kubia.example.com ... > GET /kubia HTTP/1.1 > ...
    您已访问kubia-xueq1`'
- en: The command’s output shows the response from the app, as well as the server
    certificate you configured the Ingress with.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 命令的输出显示了应用响应以及与Ingress配置的服务器证书。
- en: '|  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Support for Ingress features varies between the different Ingress controller
    implementations, so check the implementation-specific documentation to see what’s
    supported.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 不同Ingress控制器实现之间对Ingress特性的支持各不相同，因此请检查特定实现的文档以了解支持的内容。
- en: '|  |'
  id: totrans-421
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Ingresses are a relatively new Kubernetes feature, so you can expect to see
    many improvements and new features in the future. Although they currently support
    only L7 (HTTP/HTTPS) load balancing, support for L4 load balancing is also planned.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: Ingress是相对较新的Kubernetes特性，因此您预计在将来会看到许多改进和新特性。尽管它们目前仅支持L7（HTTP/HTTPS）负载均衡，但计划也支持L4负载均衡。
- en: 5.5\. Signaling when a pod is ready to accept connections
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 5.5\. 当Pod准备好接受连接时发出信号
- en: There’s one more thing we need to cover regarding both Services and Ingresses.
    You’ve already learned that pods are included as endpoints of a service if their
    labels match the service’s pod selector. As soon as a new pod with proper labels
    is created, it becomes part of the service and requests start to be redirected
    to the pod. But what if the pod isn’t ready to start serving requests immediately?
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 关于服务和Ingress，我们还需要讨论一点。您已经了解到，如果Pod的标签与服务的Pod选择器匹配，则Pod被视为服务的端点。一旦创建了带有适当标签的新Pod，它就成为服务的一部分，请求开始被重定向到该Pod。但如果Pod尚未准备好立即开始处理请求怎么办？
- en: The pod may need time to load either configuration or data, or it may need to
    perform a warm-up procedure to prevent the first user request from taking too
    long and affecting the user experience. In such cases you don’t want the pod to
    start receiving requests immediately, especially when the already-running instances
    can process requests properly and quickly. It makes sense to not forward requests
    to a pod that’s in the process of starting up until it’s fully ready.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: Pod可能需要时间来加载配置或数据，或者可能需要执行预热程序以防止第一个用户请求耗时过长并影响用户体验。在这种情况下，您不希望Pod立即开始接收请求，尤其是当已运行的实例可以正确且快速地处理请求时。在Pod完全准备好之前不将请求转发到正在启动的Pod是有意义的。
- en: 5.5.1\. Introducing readiness probes
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 5.5.1\. 介绍就绪探针
- en: In the previous chapter you learned about liveness probes and how they help
    keep your apps healthy by ensuring unhealthy containers are restarted automatically.
    Similar to liveness probes, Kubernetes allows you to also define a readiness probe
    for your pod.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了存活探针以及它们如何通过确保不健康的容器自动重启来帮助保持应用程序的健康状态。与存活探针类似，Kubernetes 允许你为你的 Pod
    定义一个准备探针。
- en: The readiness probe is invoked periodically and determines whether the specific
    pod should receive client requests or not. When a container’s readiness probe
    returns success, it’s signaling that the container is ready to accept requests.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 准备探针定期调用，并确定特定的 Pod 是否应该接收客户端请求。当一个容器的准备探针返回成功时，它表示容器已准备好接受请求。
- en: This notion of being ready is obviously something that’s specific to each container.
    Kubernetes can merely check if the app running in the container responds to a
    simple `GET /` request or it can hit a specific URL path, which causes the app
    to perform a whole list of checks to determine if it’s ready. Such a detailed
    readiness probe, which takes the app’s specifics into account, is the app developer’s
    responsibility.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 准备这一概念显然是针对每个容器特定的。Kubernetes 可以检查容器中的应用程序是否响应简单的 `GET /` 请求，或者它可以直接访问特定的 URL
    路径，这会导致应用程序执行一系列检查以确定其是否已准备好。这种详细的准备探针，考虑到应用程序的特定情况，是应用程序开发者的责任。
- en: Types of readiness probes
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 准备探针的类型
- en: 'Like liveness probes, three types of readiness probes exist:'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 与存活探针一样，存在三种类型的准备探针：
- en: An Exec probe, where a process is executed. The container’s status is determined
    by the process’ exit status code.
  id: totrans-432
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行探针，其中执行一个进程。容器的状态由进程的退出状态码确定。
- en: An HTTP GET probe, which sends an HTTP `GET` request to the container and the
    HTTP status code of the response determines whether the container is ready or
    not.
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP GET 探针，它向容器发送 HTTP `GET` 请求，响应的 HTTP 状态码确定容器是否已准备好。
- en: A TCP Socket probe, which opens a TCP connection to a specified port of the
    container. If the connection is established, the container is considered ready.
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TCP Socket 探针，它打开到容器指定端口的 TCP 连接。如果连接建立，则认为容器已准备好。
- en: Understanding the operation of readiness probes
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 理解准备探针的操作
- en: When a container is started, Kubernetes can be configured to wait for a configurable
    amount of time to pass before performing the first readiness check. After that,
    it invokes the probe periodically and acts based on the result of the readiness
    probe. If a pod reports that it’s not ready, it’s removed from the service. If
    the pod then becomes ready again, it’s re-added.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 当容器启动时，Kubernetes 可以配置为在执行第一次准备检查之前等待一段可配置的时间。之后，它定期调用探针并根据准备探针的结果采取行动。如果一个
    Pod 报告它未准备好，它将被从服务中移除。如果 Pod 之后再次准备好，它将被重新添加。
- en: Unlike liveness probes, if a container fails the readiness check, it won’t be
    killed or restarted. This is an important distinction between liveness and readiness
    probes. Liveness probes keep pods healthy by killing off unhealthy containers
    and replacing them with new, healthy ones, whereas readiness probes make sure
    that only pods that are ready to serve requests receive them. This is mostly necessary
    during container start up, but it’s also useful after the container has been running
    for a while.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 与存活探针不同，如果容器失败准备检查，它不会被杀死或重启。这是存活探针和准备探针之间的重要区别。存活探针通过杀死不健康的容器并替换为新的、健康的容器来保持
    Pod 的健康状态，而准备探针确保只有准备好接收请求的 Pod 才能接收它们。这在容器启动期间是必要的，但容器运行一段时间后也非常有用。
- en: As you can see in [figure 5.11](#filepos563477), if a pod’s readiness probe
    fails, the pod is removed from the Endpoints object. Clients connecting to the
    service will not be redirected to the pod. The effect is the same as when the
    pod doesn’t match the service’s label selector at all.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [图 5.11](#filepos563477) 所示，如果 Pod 的准备探针失败，Pod 将从 Endpoints 对象中移除。连接到服务的客户端不会重定向到该
    Pod。效果与 Pod 完全不匹配服务标签选择器时相同。
- en: Figure 5.11\. A pod whose readiness probe fails is removed as an endpoint of
    a service.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11\. 准备探针失败的 Pod 作为服务的一个端点被移除。
- en: '![](images/00084.jpg)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![图片](images/00084.jpg)'
- en: Understanding why readiness probes are important
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 理解为什么准备探针很重要
- en: Imagine that a group of pods (for example, pods running application servers)
    depends on a service provided by another pod (a backend database, for example).
    If at any point one of the frontend pods experiences connectivity problems and
    can’t reach the database anymore, it may be wise for its readiness probe to signal
    to Kubernetes that the pod isn’t ready to serve any requests at that time. If
    other pod instances aren’t experiencing the same type of connectivity issues,
    they can serve requests normally. A readiness probe makes sure clients only talk
    to those healthy pods and never notice there’s anything wrong with the system.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有一组 Pod（例如，运行应用程序服务器的 Pod）依赖于另一个 Pod（例如，后端数据库）提供的服务。如果任何一个前端 Pod 在任何时候遇到连接问题，无法再访问数据库，那么它的就绪探测向
    Kubernetes 信号表明 Pod 在当时无法处理任何请求可能是明智的。如果其他 Pod 实例没有遇到相同的连接问题，它们可以正常处理请求。就绪探测确保客户端只与健康的
    Pod 通信，并且永远不会注意到系统有任何问题。
- en: 5.5.2\. Adding a readiness probe to a pod
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 5.5.2\. 向 Pod 添加就绪探测
- en: Next you’ll add a readiness probe to your existing pods by modifying the Replication-Controller’s
    pod template.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将通过修改 Replication-Controller 的 Pod 模板来向现有的 Pod 添加就绪探测。
- en: Adding a readiness probe to the pod template
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 向 Pod 模板添加就绪探测
- en: 'You’ll use the `kubectl edit` command to add the probe to the pod template
    in your existing ReplicationController:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 您将使用 `kubectl edit` 命令将探测添加到现有 ReplicationController 的 Pod 模板中：
- en: '`$ kubectl edit rc kubia`'
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl edit rc kubia`'
- en: When the ReplicationController’s YAML opens in the text editor, find the container
    specification in the pod template and add the following readiness probe definition
    to the first container under `spec.template.spec.containers.` The YAML should
    look like the following listing.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 当 ReplicationController 的 YAML 在文本编辑器中打开时，找到 Pod 模板中的容器规范，并在 `spec.template.spec.containers.`
    下的第一个容器中添加以下就绪探测定义。YAML 应该看起来像以下列表。
- en: 'Listing 5.17\. RC creating a pod with a readiness probe: kubia-rc-readinessprobe.yaml'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.17\. RC 创建带有就绪探测的 Pod：kubia-rc-readinessprobe.yaml
- en: '`apiVersion: v1 kind: ReplicationController ... spec:   ...   template:    
    ...     spec:       containers:       - name: kubia         image: luksa/kubia`
    `readinessProbe:``1``exec:``1``command:``1``- ls``1``- /var/ready``1` `...`'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: ReplicationController ... spec:   ...   template:    
    ...     spec:       containers:       - name: kubia         image: luksa/kubia`
    `readinessProbe:``1``exec:``1``command:``1``- ls``1``- /var/ready``1` `...`'
- en: 1 A readinessProbe may be defined for each container in the pod.
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 每个 Pod 中的容器都可以定义一个就绪探测。
- en: The readiness probe will periodically perform the command `ls /var/ready` inside
    the container. The `ls` command returns exit code zero if the file exists, or
    a non-zero exit code otherwise. If the file exists, the readiness probe will succeed;
    otherwise, it will fail.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探测将定期在容器内执行 `ls /var/ready` 命令。如果文件存在，`ls` 命令返回退出代码零，否则返回非零退出代码。如果文件存在，就绪探测将成功；否则，它将失败。
- en: The reason you’re defining such a strange readiness probe is so you can toggle
    its result by creating or removing the file in question. The file doesn’t exist
    yet, so all the pods should now report not being ready, right? Well, not exactly.
    As you may remember from the previous chapter, changing a ReplicationController’s
    pod template has no effect on existing pods.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 您定义这样一个奇怪的就绪探测的原因是可以通过创建或删除相关文件来切换其结果。该文件尚不存在，因此所有 Pod 应该现在都报告未就绪，对吧？嗯，并不完全是这样。如您在前一章中记得的那样，更改
    ReplicationController 的 Pod 模板对现有 Pod 没有影响。
- en: In other words, all your existing pods still have no readiness probe defined.
    You can see this by listing the pods with `kubectl get pods` and looking at the
    `READY` column. You need to delete the pods and have them re-created by the Replication-Controller.
    The new pods will fail the readiness check and won’t be included as endpoints
    of the service until you create the /var/ready file in each of them.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，您现有的所有 Pod 都还没有定义就绪探测。您可以通过使用 `kubectl get pods` 列出 Pod 并查看 `READY` 列来查看这一点。您需要删除
    Pod，并让 Replication-Controller 重新创建它们。新的 Pod 将会失败就绪检查，并且直到您在每个 Pod 中创建 /var/ready
    文件，它们都不会被包括为服务的端点。
- en: Observing and modifying the pods’ readiness status
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 观察和修改 Pod 的就绪状态
- en: 'List the pods again and inspect whether they’re ready or not:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 再次列出 Pod 并检查它们是否就绪：
- en: '`$ kubectl get po` `NAME          READY     STATUS    RESTARTS   AGE kubia-2r1qb`
    `0/1``Running   0          1m kubia-3rax1` `0/1``Running   0          1m kubia-3yw4s`
    `0/1``       Running   0          1m`'
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po` `NAME          READY     STATUS    RESTARTS   AGE kubia-2r1qb`
    `0/1``Running   0          1m kubia-3rax1` `0/1``Running   0          1m kubia-3yw4s`
    `0/1``       Running   0          1m`'
- en: 'The `READY` column shows that none of the containers are ready. Now make the
    readiness probe of one of them start returning success by creating the `/var/ready`
    file, whose existence makes your mock readiness probe succeed:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: '`READY`列显示没有任何容器就绪。现在通过创建`/var/ready`文件来使其中一个就绪探针开始返回成功状态，该文件的存在使得模拟的就绪探针成功：'
- en: '`$ kubectl exec kubia-2r1qb -- touch /var/ready`'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec kubia-2r1qb -- touch /var/ready`'
- en: 'You’ve used the `kubectl exec` command to execute the `touch` command inside
    the container of the `kubia-2r1qb` pod. The `touch` command creates the file if
    it doesn’t yet exist. The pod’s readiness probe command should now exit with status
    code 0, which means the probe is successful, and the pod should now be shown as
    ready. Let’s see if it is:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经使用`kubectl exec`命令在`kubia-2r1qb` Pod的容器内执行了`touch`命令。`touch`命令会在文件不存在时创建该文件。现在Pod的就绪探针命令应该以状态码0退出，这意味着探针成功，Pod现在应该显示为就绪。让我们看看它是否就绪：
- en: '`$ kubectl get po kubia-2r1qb` `NAME          READY     STATUS    RESTARTS  
    AGE kubia-2r1qb` `0/1``       Running   0          2m`'
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl get po kubia-2r1qb` `NAME          READY     STATUS    RESTARTS  
    AGE kubia-2r1qb` `0/1``       运行   0          2m`'
- en: 'The pod still isn’t ready. Is there something wrong or is this the expected
    result? Take a more detailed look at the pod with `kubectl describe`. The output
    should contain the following line:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: Pod仍然没有就绪。是出了问题还是这是预期的结果？使用`kubectl describe`查看Pod的详细信息。输出应包含以下行：
- en: '`Readiness: exec [ls /var/ready] delay=0s timeout=1s period=10s #success=1`
    ![](images/00006.jpg) `#failure=3`'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: '`就绪状态：执行 [ls /var/ready] 延迟=0s 超时=1s 周期=10s #成功=1` ![图片](images/00006.jpg)
    `#失败=3`'
- en: The readiness probe is checked periodically—every 10 seconds by default. The
    pod isn’t ready because the readiness probe hasn’t been invoked yet. But in 10
    seconds at the latest, the pod should become ready and its IP should be listed
    as the only endpoint of the service (run `kubectl get endpoints kubia-loadbalancer`
    to confirm).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 就绪探针会定期检查——默认情况下每10秒检查一次。由于就绪探针尚未被调用，因此Pod尚未就绪。但最迟在10秒后，Pod应该变为就绪状态，并且其IP应该被列为服务的唯一端点（运行`kubectl
    get endpoints kubia-loadbalancer`以确认）。
- en: Hitting the service with the single ready pod
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 单个就绪Pod访问服务
- en: 'You can now hit the service URL a few times to see that each and every request
    is redirected to this one pod:'
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以多次访问服务URL，以查看每个请求都被重定向到这个Pod：
- en: '`$ curl http://130.211.53.173` `You''ve hit kubia-2r1qb` `$ curl http://130.211.53.173`
    `You''ve hit kubia-2r1qb ...` `$ curl http://130.211.53.173` `You''ve hit kubia-2r1qb`'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ curl http://130.211.53.173` `您已访问kubia-2r1qb` `$ curl http://130.211.53.173`
    `您已访问kubia-2r1qb ...` `$ curl http://130.211.53.173` `您已访问kubia-2r1qb`'
- en: Even though there are three pods running, only a single pod is reporting as
    being ready and is therefore the only pod receiving requests. If you now delete
    the file, the pod will be removed from the service again.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管有三个Pod正在运行，但只有单个Pod报告为就绪状态，因此它是唯一接收请求的Pod。如果你现在删除该文件，Pod将再次从服务中移除。
- en: 5.5.3\. Understanding what real-world readiness probes should do
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 5.5.3. 理解现实世界的就绪探针应该做什么
- en: This mock readiness probe is useful only for demonstrating what readiness probes
    do. In the real world, the readiness probe should return success or failure depending
    on whether the app can (and wants to) receive client requests or not.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模拟的就绪探针仅用于演示就绪探针的作用。在现实世界中，就绪探针应根据应用是否能够（并且愿意）接收客户端请求来返回成功或失败。
- en: Manually removing pods from services should be performed by either deleting
    the pod or changing the pod’s labels instead of manually flipping a switch in
    the probe.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 手动从服务中删除Pod应通过删除Pod或更改Pod的标签来完成，而不是手动在探针中切换开关。
- en: '|  |'
  id: totrans-472
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you want to add or remove a pod from a service manually, add `enabled=true`
    as a label to your pod and to the label selector of your service. Remove the label
    when you want to remove the pod from the service.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想手动添加或删除Pod到服务中，请将`enabled=true`作为标签添加到你的Pod和服务的标签选择器中。当你想从服务中删除Pod时，请移除该标签。
- en: '|  |'
  id: totrans-475
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Always define a readiness probe
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 总是定义一个就绪探针
- en: Before we conclude this section, there are two final notes about readiness probes
    that I need to emphasize. First, if you don’t add a readiness probe to your pods,
    they’ll become service endpoints almost immediately. If your application takes
    too long to start listening for incoming connections, client requests hitting
    the service will be forwarded to the pod while it’s still starting up and not
    ready to accept incoming connections. Clients will therefore see “Connection refused”
    types of errors.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本节之前，还有两个关于就绪探针的最终注意事项需要强调。首先，如果你没有为你的Pod添加就绪探针，它们几乎会立即成为服务端点。如果你的应用程序启动并开始监听传入连接需要太长时间，那么击中服务的客户端请求将在Pod仍在启动且尚未准备好接受传入连接时被转发到Pod。因此，客户端将看到“连接被拒绝”类型的错误。
- en: '|  |'
  id: totrans-478
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士
- en: You should always define a readiness probe, even if it’s as simple as sending
    an HTTP request to the base URL.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该始终定义一个就绪探针，即使它只是发送一个HTTP请求到基本URL也是如此。
- en: '|  |'
  id: totrans-481
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Don’t include pod shutdown logic into your readiness probes
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 不要将Pod关闭逻辑包含在就绪探针中
- en: The other thing I need to mention applies to the other end of the pod’s life
    (pod shutdown) and is also related to clients experiencing connection errors.
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 我需要提到的另一件事与Pod生命周期的另一端（Pod关闭）有关，并且也与客户端遇到连接错误有关。
- en: When a pod is being shut down, the app running in it usually stops accepting
    connections as soon as it receives the termination signal. Because of this, you
    might think you need to make your readiness probe start failing as soon as the
    shutdown procedure is initiated, ensuring the pod is removed from all services
    it’s part of. But that’s not necessary, because Kubernetes removes the pod from
    all services as soon as you delete the pod.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 当Pod正在关闭时，运行在其内的应用程序通常会一收到终止信号就停止接受连接。正因为如此，你可能会认为你需要让你的就绪探针在关闭程序启动时立即开始失败，确保Pod从它所属的所有服务中被移除。但这是不必要的，因为Kubernetes在你删除Pod时立即将其从所有服务中移除。
- en: 5.6\. Using a headless service for discovering individual pods
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 5.6. 使用无头服务来发现单个Pod
- en: You’ve seen how services can be used to provide a stable IP address allowing
    clients to connect to pods (or other endpoints) backing each service. Each connection
    to the service is forwarded to one randomly selected backing pod. But what if
    the client needs to connect to all of those pods? What if the backing pods themselves
    need to each connect to all the other backing pods? Connecting through the service
    clearly isn’t the way to do this. What is?
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到服务可以用来提供稳定的IP地址，允许客户端连接到每个服务背后的Pod（或其他端点）。每个对服务的连接都会被转发到随机选择的支持Pod。但是，如果客户端需要连接到所有这些Pod怎么办？如果支持Pod本身需要每个都连接到所有其他支持Pod怎么办？通过服务连接显然不是这样做的方式。那是什么？
- en: For a client to connect to all pods, it needs to figure out the the IP of each
    individual pod. One option is to have the client call the Kubernetes API server
    and get the list of pods and their IP addresses through an API call, but because
    you should always strive to keep your apps Kubernetes-agnostic, using the API
    server isn’t ideal.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让客户端连接到所有Pod，它需要找出每个单独Pod的IP。一个选项是让客户端调用Kubernetes API服务器，并通过API调用获取Pod列表及其IP地址，但因为你应该始终努力保持你的应用程序与Kubernetes无关，所以使用API服务器并不是最佳选择。
- en: Luckily, Kubernetes allows clients to discover pod IPs through DNS lookups.
    Usually, when you perform a DNS lookup for a service, the DNS server returns a
    single IP—the service’s cluster IP. But if you tell Kubernetes you don’t need
    a cluster IP for your service (you do this by setting the `clusterIP` field to
    `None` in the service specification`)`, the DNS server will return the pod IPs
    instead of the single service IP.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Kubernetes允许客户端通过DNS查找来发现Pod IP。通常，当你对服务执行DNS查找时，DNS服务器会返回单个IP——服务的集群IP。但是，如果你告诉Kubernetes你不需要为你的服务提供集群IP（你通过在服务规范中将`clusterIP`字段设置为`None`来完成此操作）`，DNS服务器将返回Pod
    IP而不是单个服务IP。
- en: Instead of returning a single DNS `A` record, the DNS server will return multiple
    `A` records for the service, each pointing to the IP of an individual pod backing
    the service at that moment. Clients can therefore do a simple DNS `A` record lookup
    and get the IPs of all the pods that are part of the service. The client can then
    use that information to connect to one, many, or all of them.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: DNS服务器不会返回单个DNS `A`记录，而是为服务返回多个`A`记录，每个记录都指向在那一刻支持该服务的单个Pod的IP。因此，客户端可以执行简单的DNS
    `A`记录查找，并获取所有属于该服务的Pod的IP。客户端然后可以使用该信息连接到其中一个、多个或所有Pod。
- en: 5.6.1\. Creating a headless service
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 5.6.1. 创建无头服务
- en: Setting the `clusterIP` field in a service spec to `None` makes the service
    headless, as Kubernetes won’t assign it a cluster IP through which clients could
    connect to the pods backing it.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 将服务规范中的`clusterIP`字段设置为`None`使得服务成为无头服务，因为Kubernetes不会为其分配一个客户端可以通过它连接到后端pod的集群IP。
- en: You’ll create a headless service called `kubia-headless` now. The following
    listing shows its definition.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你将创建一个名为`kubia-headless`的无头服务。以下列表显示了其定义。
- en: 'Listing 5.18\. A headless service: kubia-svc-headless.yaml'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.18. 无头服务：kubia-svc-headless.yaml
- en: '`apiVersion: v1 kind: Service metadata:   name: kubia-headless spec:   clusterIP:
    None` `1` `ports:   - port: 80     targetPort: 8080   selector:     app: kubia`'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: '`apiVersion: v1 kind: Service metadata:   name: kubia-headless spec:   clusterIP:
    None` `1` `ports:   - port: 80     targetPort: 8080   selector:     app: kubia`'
- en: 1 This makes the service headless.
  id: totrans-495
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 1 这使得服务成为无头服务。
- en: 'After you create the service with `kubectl create`, you can inspect it with
    `kubectl get` and `kubectl describe`. You’ll see it has no cluster IP and its
    endpoints include (part of) the pods matching its pod selector. I say “part of”
    because your pods contain a readiness probe, so only pods that are ready will
    be listed as endpoints of the service. Before continuing, please make sure at
    least two pods report being ready, by creating the `/var/ready` file, as in the
    previous example:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 在你使用`kubectl create`创建服务后，你可以使用`kubectl get`和`kubectl describe`来检查它。你会发现它没有集群IP，其端点包括（部分）匹配其pod选择器的pod。我说“部分”，因为你的pod包含就绪性检查，所以只有就绪的pod才会被列为服务的端点。在继续之前，请确保至少有两个pod报告为就绪，通过创建与上一个示例中的`/var/ready`文件一样的方法：
- en: '`$ kubectl exec <pod name> -- touch /var/ready`'
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec <pod name> -- touch /var/ready`'
- en: 5.6.2\. Discovering pods through DNS
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 5.6.2. 通过DNS发现pod
- en: With your pods ready, you can now try performing a DNS lookup to see if you
    get the actual pod IPs or not. You’ll need to perform the lookup from inside one
    of the pods. Unfortunately, your `kubia` container image doesn’t include the `nslookup`
    (or the `dig`) binary, so you can’t use it to perform the DNS lookup.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在你的pod就绪后，你现在可以尝试执行DNS查找，看看你是否能得到实际的pod IP。你需要从其中一个pod内部执行查找。不幸的是，你的`kubia`容器镜像不包括`nslookup`（或`dig`）二进制文件，所以你不能用它来执行DNS查找。
- en: All you’re trying to do is perform a DNS lookup from inside a pod running in
    the cluster. Why not run a new pod based on an image that contains the binaries
    you need? To perform DNS-related actions, you can use the `tutum/dnsutils` container
    image, which is available on Docker Hub and contains both the `nslookup` and the
    `dig` binaries. To run the pod, you can go through the whole process of creating
    a YAML manifest for it and passing it to `kubectl create`, but that’s too much
    work, right? Luckily, there’s a faster way.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 你所尝试做的只是从集群中运行的pod内部执行DNS查找。为什么不基于包含所需二进制的镜像运行一个新的pod呢？为了执行与DNS相关的操作，你可以使用Docker
    Hub上可用的`tutum/dnsutils`容器镜像，它包含`nslookup`和`dig`二进制文件。要运行pod，你可以通过为其创建YAML清单并将其传递给`kubectl
    create`的整个过程，但这太费事了，对吧？幸运的是，有一个更快的方法。
- en: Running a pod without writing a YAML manifest
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 不编写YAML清单运行pod
- en: 'In [chapter 1](index_split_017.html#filepos122588), you already created pods
    without writing a YAML manifest by using the `kubectl run` command. But this time
    you want to create only a pod—you don’t need to create a ReplicationController
    to manage the pod. You can do that like this:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](index_split_017.html#filepos122588)中，你已经通过使用`kubectl run`命令创建了不需要编写YAML清单的pod。但这次你只想创建一个pod——你不需要创建ReplicationController来管理pod。你可以这样做：
- en: '`$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1`![](images/00006.jpg)`--command
    -- sleep infinity` `pod "dnsutils" created`'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl run dnsutils --image=tutum/dnsutils --generator=run-pod/v1`![](images/00006.jpg)`--command
    -- sleep infinity` `pod "dnsutils" created`'
- en: The trick is in the `--generator=run-pod/v1` option, which tells `kubectl` to
    create the pod directly, without any kind of ReplicationController or similar
    behind it.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 关键在于`--generator=run-pod/v1`选项，它告诉`kubectl`直接创建pod，而不需要任何类型的ReplicationController或类似的后台。
- en: Understanding DNS A records returned for a headless service
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 理解无头服务返回的DNS A记录
- en: 'Let’s use the newly created pod to perform a DNS lookup:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用新创建的pod来执行DNS查找：
- en: '`$ kubectl exec dnsutils nslookup kubia-headless` `... Name:    kubia-headless.default.svc.cluster.local
    Address: 10.108.1.4 Name:    kubia-headless.default.svc.cluster.local Address:
    10.108.2.5`'
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec dnsutils nslookup kubia-headless` `... Name:    kubia-headless.default.svc.cluster.local
    Address: 10.108.1.4 Name:    kubia-headless.default.svc.cluster.local Address:
    10.108.2.5`'
- en: The DNS server returns two different IPs for the `kubia-headless.default.svc.cluster.local`
    FQDN. Those are the IPs of the two pods that are reporting being ready. You can
    confirm this by listing pods with `kubectl get pods -o wide`, which shows the
    pods’ IPs.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: DNS服务器为`kubia-headless.default.svc.cluster.local` FQDN返回两个不同的IP。这些是报告已准备就绪的两个Pod的IP。你可以通过使用`kubectl
    get pods -o wide`列出Pod来确认这一点，它显示了Pod的IP。
- en: 'This is different from what DNS returns for regular (non-headless) services,
    such as for your `kubia` service, where the returned IP is the service’s cluster
    IP:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 这与DNS为常规（非无头）服务返回的内容不同，例如你的`kubia`服务，其中返回的IP是服务的集群IP：
- en: '`$ kubectl exec dnsutils nslookup kubia` `... Name:    kubia.default.svc.cluster.local
    Address: 10.111.249.153`'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '`$ kubectl exec dnsutils nslookup kubia` `... Name:    kubia.default.svc.cluster.local
    Address: 10.111.249.153`'
- en: Although headless services may seem different from regular services, they aren’t
    that different from the clients’ perspective. Even with a headless service, clients
    can connect to its pods by connecting to the service’s DNS name, as they can with
    regular services. But with headless services, because DNS returns the pods’ IPs,
    clients connect directly to the pods, instead of through the service proxy.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管无头服务看起来与常规服务不同，但从客户端的角度来看，它们并没有那么不同。即使是无头服务，客户端也可以通过连接到服务的DNS名称来连接到其Pod，就像它们可以连接到常规服务一样。但在无头服务中，因为DNS返回Pod的IP，客户端直接连接到Pod，而不是通过服务代理。
- en: '|  |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: A headless services still provides load balancing across pods, but through the
    DNS round-robin mechanism instead of through the service proxy.
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 无头服务仍然通过DNS轮询机制而不是通过服务代理在Pod之间提供负载均衡。
- en: '|  |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 5.6.3\. Discovering all pods—even those that aren’t ready
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 5.6.3\. 发现所有Pod——即使那些尚未准备就绪的Pod
- en: You’ve seen that only pods that are ready become endpoints of services. But
    sometimes you want to use the service discovery mechanism to find all pods matching
    the service’s label selector, even those that aren’t ready.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经看到只有准备就绪的Pod才会成为服务的端点。但有时你希望使用服务发现机制来找到所有匹配服务标签选择器的Pod，即使它们尚未准备就绪。
- en: 'Luckily, you don’t have to resort to querying the Kubernetes API server. You
    can use the DNS lookup mechanism to find even those unready pods. To tell Kubernetes
    you want all pods added to a service, regardless of the pod’s readiness status,
    you must add the following annotation to the service:'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，你不必求助于查询Kubernetes API服务器。你可以使用DNS查找机制来找到甚至那些未准备就绪的Pod。为了告诉Kubernetes你希望将所有Pod添加到服务中，无论Pod的准备状态如何，你必须将以下注释添加到服务中：
- en: '`kind: Service metadata:   annotations:` `service.alpha.kubernetes.io/tolerate-unready-endpoints:
    "true"`'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: '`kind: Service metadata:   annotations:` `service.alpha.kubernetes.io/tolerate-unready-endpoints:
    "true"`'
- en: '|  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 警告
- en: As the annotation name suggests, as I’m writing this, this is an alpha feature.
    The Kubernetes Service API already supports a new service spec field called `publishNotReadyAddresses`,
    which will replace the `tolerate-unready-endpoints` annotation. In Kubernetes
    version 1.9.0, the field is not honored yet (the annotation is what determines
    whether unready endpoints are included in the DNS or not). Check the documentation
    to see whether that’s changed.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 如注释名称所暗示的，在我写这篇文章的时候，这是一个alpha特性。Kubernetes服务API已经支持一个新的服务规范字段，称为`publishNotReadyAddresses`，它将取代`tolerate-unready-endpoints`注释。在Kubernetes版本1.9.0中，该字段尚未得到尊重（注释是确定未准备端点是否包含在DNS中的因素）。请查看文档以了解是否已更改。
- en: '|  |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 5.7\. Troubleshooting services
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: 5.7\. 服务故障排除
- en: Services are a crucial Kubernetes concept and the source of frustration for
    many developers. I’ve seen many developers lose heaps of time figuring out why
    they can’t connect to their pods through the service IP or FQDN. For this reason,
    a short look at how to troubleshoot services is in order.
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 服务是Kubernetes的一个关键概念，也是许多开发者的挫折之源。我见过许多开发者花费大量时间试图弄清楚为什么他们无法通过服务IP或FQDN连接到他们的Pod。因此，简要了解如何故障排除服务是有必要的。
- en: 'When you’re unable to access your pods through the service, you should start
    by going through the following list:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 当你无法通过服务访问你的Pod时，你应该首先检查以下列表：
- en: First, make sure you’re connecting to the service’s cluster IP from within the
    cluster, not from the outside.
  id: totrans-527
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，确保你是在集群内部连接到服务的集群IP，而不是从外部连接。
- en: Don’t bother pinging the service IP to figure out if the service is accessible
    (remember, the service’s cluster IP is a virtual IP and pinging it will never
    work).
  id: totrans-528
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不要费心ping服务IP来检查服务是否可访问（记住，服务的集群IP是一个虚拟IP，ping它永远不会起作用）。
- en: If you’ve defined a readiness probe, make sure it’s succeeding; otherwise the
    pod won’t be part of the service.
  id: totrans-529
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你已定义就绪探测，请确保它成功；否则，pod 不会成为服务的一部分。
- en: To confirm that a pod is part of the service, examine the corresponding Endpoints
    object with `kubectl get endpoints`.
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要确认一个 pod 是否是服务的一部分，使用 `kubectl get endpoints` 检查相应的 Endpoints 对象。
- en: If you’re trying to access the service through its FQDN or a part of it (for
    example, myservice.mynamespace.svc.cluster.local or myservice.mynamespace) and
    it doesn’t work, see if you can access it using its cluster IP instead of the
    FQDN.
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你试图通过其 FQDN 或其一部分（例如，myservice.mynamespace.svc.cluster.local 或 myservice.mynamespace）来访问服务，但不起作用，请尝试使用其集群
    IP 而不是 FQDN 来访问。
- en: Check whether you’re connecting to the port exposed by the service and not the
    target port.
  id: totrans-532
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查你是否连接到了服务暴露的端口，而不是目标端口。
- en: Try connecting to the pod IP directly to confirm your pod is accepting connections
    on the correct port.
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试直接连接到 pod IP 以确认你的 pod 正在正确的端口上接受连接。
- en: If you can’t even access your app through the pod’s IP, make sure your app isn’t
    only binding to localhost.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你甚至无法通过 pod 的 IP 访问你的应用程序，请确保你的应用程序不仅绑定到 localhost。
- en: This should help you resolve most of your service-related problems. You’ll learn
    much more about how services work in [chapter 11](index_split_087.html#filepos1036287).
    By understanding exactly how they’re implemented, it should be much easier for
    you to troubleshoot them.
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该有助于你解决大部分与服务相关的问题。你将在第 11 章 [chapter 11](index_split_087.html#filepos1036287)
    中学习更多关于服务如何工作的内容。通过了解它们的确切实现方式，你应该更容易对它们进行故障排除。
- en: 5.8\. Summary
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 5.8. 摘要
- en: In this chapter, you’ve learned how to create Kubernetes Service resources to
    expose the services available in your application, regardless of how many pod
    instances are providing each service. You’ve learned how Kubernetes
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了如何创建 Kubernetes Service 资源以暴露应用程序中可用的服务，无论有多少个 pod 实例提供每个服务。你学习了 Kubernetes
- en: Exposes multiple pods that match a certain label selector under a single, stable
    IP address and port
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在单个稳定的 IP 地址和端口下暴露匹配特定标签选择器的多个 pod
- en: Makes services accessible from inside the cluster by default, but allows you
    to make the service accessible from outside the cluster by setting its type to
    either `NodePort` or `LoadBalancer`
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下使服务在集群内部可访问，但允许你通过将其类型设置为 `NodePort` 或 `LoadBalancer` 来使服务从集群外部可访问
- en: Enables pods to discover services together with their IP addresses and ports
    by looking up environment variables
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过查找环境变量，使 pod 能够与其 IP 地址和端口一起发现服务
- en: Allows discovery of and communication with services residing outside the cluster
    by creating a Service resource without specifying a selector, by creating an associated
    Endpoints resource instead
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过创建一个不指定选择器的 Service 资源，而不是创建关联的 Endpoints 资源，允许发现和与集群外部的服务进行通信
- en: Provides a DNS `CNAME` alias for external services with the `ExternalName` service
    type
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `ExternalName` 服务类型为外部服务提供 DNS `CNAME` 别名
- en: Exposes multiple HTTP services through a single Ingress (consuming a single
    IP)
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过单个 Ingress（消耗单个 IP）暴露多个 HTTP 服务
- en: Uses a pod container’s readiness probe to determine whether a pod should or
    shouldn’t be included as a service endpoint
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 pod 容器的就绪探测来确定 pod 是否应该或不应作为服务端点包含
- en: Enables discovery of pod IPs through DNS when you create a headless service
  id: totrans-545
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你创建一个无头服务时，通过 DNS 发现 pod IP
- en: Along with getting a better understanding of services, you’ve also learned how
    to
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 除了更好地理解服务外，你还学习了如何
- en: Troubleshoot them
  id: totrans-547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除它们
- en: Modify firewall rules in Google Kubernetes/Compute Engine
  id: totrans-548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 修改 Google Kubernetes/Compute Engine 的防火墙规则
- en: Execute commands in pod containers through `kubectl exec`
  id: totrans-549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `kubectl exec` 在 pod 容器中执行命令
- en: Run a `bash` shell in an existing pod’s container
  id: totrans-550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现有 pod 的容器中运行 `bash` shell
- en: Modify Kubernetes resources through the `kubectl apply` command
  id: totrans-551
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 `kubectl apply` 命令修改 Kubernetes 资源
- en: Run an unmanaged ad hoc pod with `kubectl run --generator=run-pod/v1`
  id: totrans-552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `kubectl run --generator=run-pod/v1` 运行一个未管理的 ad hoc pod
