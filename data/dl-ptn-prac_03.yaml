- en: 2 Deep neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 深度神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Breaking down the structure of neural networks and deep neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分析神经网络和深度神经网络的架构
- en: Using feed-forward and backward propagation during training to learn model weights
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中使用前向和反向传播来学习模型权重
- en: Coding neural network models in both TF.Keras sequential and functional APIs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在TF.Keras顺序和功能API中编码神经网络模型
- en: Understanding the various types of model tasks
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解各种模型任务类型
- en: Using strategies to prevent overfitting
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用策略防止过拟合
- en: 'This chapter starts with some basics on neural networks. Once you’ve gotten
    the basics down, I’ll introduce you to how deep neural networks (DNNs) can be
    easily coded using TF.Keras, which has two styles for coding neural networks:
    a sequential API and functional API. We will code examples using both styles.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节从神经网络的一些基础知识开始。一旦你掌握了基础知识，我将向你介绍如何使用TF.Keras轻松地编码深度神经网络（DNNs），TF.Keras提供了两种编码神经网络的风格：顺序API和功能API。我们将使用这两种风格来编写示例代码。
- en: This chapter also covers the fundamental types of models. Each model type, such
    as regression and classification, learns different types of tasks. The task you
    want to learn determines the model type you will design. You’ll also learn the
    fundamentals of weights, biases, activations, and optimizers, and how they contribute
    to the accuracy of the model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 本章还涵盖了模型的基本类型。每种模型类型，如回归和分类，都学习不同类型的任务。你想要学习的任务决定了你将设计的模型类型。你还将学习权重、偏差、激活和优化器的根本原理，以及它们如何有助于提高模型的准确性。
- en: To wrap up this chapter, we’ll code an image classifier. And finally, I’ll introduce
    the problem of overfitting when training along with an early approach to solving
    overfitting with dropout.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结本章内容，我们将编写一个图像分类器。最后，我将介绍在训练过程中出现的过拟合问题，以及使用dropout的早期解决过拟合的方法。
- en: 2.1 Neural network basics
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2 神经网络基础知识
- en: Let’s start with some basics on neural networks. First, this section covers
    the input layer to a neural network, then how this is connected to an output layer,
    and then how hidden layers are added in between to become a deep neural network.
    From there, we cover how the layers are made of nodes, what nodes do, and how
    layers are connected to each other to form fully connected neural networks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从神经网络的一些基础知识开始。首先，本节涵盖了神经网络的输入层，然后是如何将其连接到输出层，接着是如何在之间添加隐藏层以形成一个深度神经网络。从那里，我们将介绍层由节点组成，节点的作用，以及层如何相互连接以形成全连接神经网络。
- en: 2.1.1 Input layer
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 输入层
- en: The *input layer* to a neural network takes numbers! All the input data is converted
    to numbers. Everything is a number. The text becomes numbers, speech becomes numbers,
    pictures become numbers, and things that are already numbers are just numbers.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络输入层接受数字！所有输入数据都被转换为数字。一切都是数字。文本变成数字，语音变成数字，图片变成数字，已经是数字的东西仍然是数字。
- en: Neural networks take numbers as vectors, matrices, or tensors. These are simply
    names for the number of dimensions in an array. A *vector* is a one-dimensional
    array, such as a list of numbers. A *matrix* is a two-dimensional array, like
    the pixels in a black-and-white image. And a *tensor* is any array of three or
    more dimensions—for example, a stack of matrices in which each matrix is the same
    dimension. That’s it. Figure 2.1 illustrates these concepts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络以向量、矩阵或张量作为输入。这些只是表示数组中维度数量的名称。一个*向量*是一维数组，例如数字列表。一个*矩阵*是二维数组，例如黑白图像中的像素。而一个*张量*是三维或更多维度的数组——例如，矩阵堆叠，其中每个矩阵的维度相同。就是这样。图2.1展示了这些概念。
- en: '![](Images/CH02_F01_Ferlitsch.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH02_F01_Ferlitsch.png)'
- en: Figure 2.1 Types of arrays in deep learning
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 深度学习中数组的类型
- en: Speaking of numbers, you might have heard terms like *normalization* or *standardization*.
    In standardization, numbers are converted to be centered around a mean of zero,
    with one standard deviation on each side of the mean. If you’re saying, “I don’t
    do statistics” right about now, I know how you feel. But don’t worry. Packages
    such as scikit-learn ([https://scikit-learn.org](https://scikit-learn.org)) and
    NumPy ([https://numpy.org](https://numpy.org)) have library calls that do this
    for you. Standardization is basically a button to push, and it doesn’t even need
    a lever, so there are no parameters to set.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 说到数字，你可能听说过像*规范化*或*标准化*这样的术语。在标准化中，数字被转换成以零为中心，每侧的均值有一个标准差。如果你现在正在说，“我不做统计学”，我知道你的感受。但别担心。像scikit-learn([https://scikit-learn.org](https://scikit-learn.org))和NumPy([https://numpy.org](https://numpy.org))这样的包提供了库调用，为你完成这项工作。标准化基本上是一个可以按的按钮，甚至不需要杠杆，所以没有需要设置的参数。
- en: 'Speaking of packages, you’re going to be using NumPy a lot. What is NumPy,
    and why is it so popular? Given the interpretive nature of Python, the language
    handles large arrays poorly—like really big, super-big arrays of numbers—thousands,
    tens of thousands, millions of numbers. Think of Carl Sagan’s infamous quote on
    the size of the universe: billions and billions of stars. That’s a tensor!'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 说到包，你将大量使用NumPy。NumPy是什么，为什么它如此受欢迎？鉴于Python的解释性特性，该语言处理大型数组不佳——就像真正的大、超级大的数字数组——成千上万的、数以万计的、数以百万计的数字。想想卡尔·萨根关于宇宙大小的著名引言：亿万个星星。那是一个张量！
- en: One day a C programmer got the idea to write, in low-level C, a high-performance
    implementation for handling super-big arrays, and then added an external Python
    wrapper. NumPy was born. Today NumPy is a library with lots of useful methods
    and properties, like the property `shape`, which tells you the shape (or dimensions)
    of the array, and the `where()` method, which allows you to do SQL-like queries
    on your super-big array.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 有一天，一个C程序员有了这样的想法，用低级的C编写一个处理超级大数组的性能实现，然后添加了一个外部Python包装器。NumPy就这样诞生了。如今，NumPy是一个包含许多有用方法和属性的库，如`shape`属性，它告诉你数组的形状（或维度），以及`where()`方法，它允许你在超级大数组上进行类似SQL的查询。
- en: All Python ML frameworks, such as TensorFlow and PyTorch, will take as input
    on the input layer a NumPy multidimensional array. And speaking of C, or Java,
    or C+, . . . , the input layer in a neural network is just like the parameters
    passed to a function in a programming language. That’s it.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 所有Python机器学习框架，如TensorFlow和PyTorch，都会将NumPy多维数组作为输入层输入。至于C、Java或C++等，神经网络中的输入层就像在编程语言中传递给函数的参数一样。就是这样。
- en: Let’s get started by installing Python packages you will need. I assume you
    have version 3.*x* of Python installed ([www.python.org/downloads/](https://www.python.org/downloads/)).
    Whether you directly installed it or had it installed as part of a larger package
    such as Anaconda ([www.anaconda.com/products/enterprise](https://www.anaconda.com/products/enterprise)),
    you got with it a nifty command-line tool called `pip`. This tool is used to install
    any Python package you will ever need again, from a single command invocation.
    You use `pip install` and then the name of the package. It goes to the Python
    Package Index (PyPI), the global repository of Python packages, and downloads
    and installs the package for you. It’s quite easy.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始安装你需要的Python包。我假设你已经安装了Python 3.*x*版本([www.python.org/downloads/](https://www.python.org/downloads/))。无论你是直接安装它，还是作为Anaconda([www.anaconda.com/products/enterprise](https://www.anaconda.com/products/enterprise))等更大包的一部分安装它，你都会得到一个叫做`pip`的便捷命令行工具。这个工具用于安装你将来需要的任何Python包，只需一个命令调用。你使用`pip
    install`然后是包名。它会去Python包索引（PyPI），Python包的全局存储库，为你下载并安装包。这相当简单。
- en: 'We want to start off by downloading and installing the TensorFlow framework
    and the NumPy package. Guess what? Their names are in the registry, *tensorflow*
    and *numpy*—thankfully, very obvious. Let’s do it together. Go to the command
    line and issue the following:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想从下载和安装TensorFlow框架和NumPy包开始。猜猜看？它们的名称在注册表中是*tensorflow*和*numpy*——幸运的是，非常明显。让我们一起来做。打开命令行，输入以下命令：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With TensorFlow 2.0, Keras is built in and the recommended model API, referred
    to now as *TF.Keras*. TF.Keras is based on object-oriented programming with a
    collection of classes and associated methods and properties.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在TensorFlow 2.0中，Keras被内置为推荐的模型API，现在被称为*TF.Keras*。TF.Keras基于面向对象编程，包含一系列类及其相关的方法和属性。
- en: Let’s start simply. Say we have a dataset of housing data. Each row has 14 columns
    of data. One column indicates the sale price of a home. We are going to call that
    the *label*. The other 13 columns have information about the house, such as the
    square footage and property tax. It’s all numbers. We are going to call those
    the *features*. What we want to do is learn to predict (or estimate) the label
    from the features.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始。假设我们有一个房屋数据集。每一行有14个数据列。一列表示房屋的销售价格。我们将称之为*标签*。其他13列包含有关房屋的信息，如面积和财产税。这些都是数字。我们将称之为*特征*。我们想要做的是学习从特征中预测（或估计）标签。
- en: Now, before we had all this computing power and these awesome ML frameworks,
    data analysts did this stuff by hand or by using formulas in a Microsoft Excel
    spreadsheet with a certain amount of data and lots and lots of linear algebra.
    We, however, will use Keras and TensorFlow.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在我们拥有所有这些计算能力和这些令人惊叹的机器学习框架之前，数据分析师是通过手工或使用Microsoft Excel电子表格中的公式来处理这些事情的，这些电子表格包含一定量的数据和大量的线性代数。然而，我们将使用Keras和TensorFlow。
- en: 'We will start by first importing the Keras module from TensorFlow, and then
    instantiate an `Input` class object. For this class object, we define the shape
    or dimensions of the input. In our example, the input is a one-dimensional array
    (a vector) of 13 elements, one for each feature:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先从TensorFlow导入Keras模块，然后实例化一个`Input`类对象。对于这个类对象，我们定义输入的形状或维度。在我们的例子中，输入是一个包含13个元素的向量（数组），每个元素对应一个特征：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When you run these two lines in a notebook, you will see this output:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在笔记本中运行这两行代码时，你会看到这个输出：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: This output shows you what `Input(shape=(13,))` evaluates to. It produces a
    tensor object named `input_1:0`. This name will be useful later in assisting you
    in debugging your models. The `?` in `shape` shows that the input object takes
    an unbounded number of entries (examples or rows) of 13 elements each. That is,
    at runtime it will bind the number of one-dimensional vectors of 13 elements to
    the actual number of examples (rows) you pass in, referred to as the (mini) *batch
    size*. The `dtype` shows the default data type of the elements, which in this
    case is a 32-bit float (single precision).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出显示了`Input(shape=(13,))`的评估结果。它产生了一个名为`input_1:0`的张量对象。这个名称将在稍后帮助你调试模型时很有用。`shape`中的`?`表示输入对象可以接受任意数量的条目（示例或行），每个条目包含13个元素。也就是说，在运行时，它将绑定13个元素的向量数量到实际传递的示例（行）数量，这被称为（迷你）*批量大小*。`dtype`显示了元素的默认数据类型，在这种情况下是一个32位的浮点数（单精度）。
- en: 2.1.2 Deep neural networks
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 深度神经网络
- en: DeepMind, deep learning, deep, deep, deep. Oh my, what’s all this? *Deep* in
    this context just means that the neural network has one or more layers between
    the input layer and the output layer. As you will read later, by going deeper
    into hidden layers, researchers have been able to get higher accuracy.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: DeepMind, 深度学习，深，深，深。哦，这一切都是什么意思？在这里，“深”的意思是指神经网络在输入层和输出层之间有一层或多层。正如你稍后将要读到的，通过深入隐藏层，研究人员已经能够获得更高的准确率。
- en: 'Visualize a directed graph in layers of depth. The root nodes are the input
    layer, and the terminal nodes are the output layer. The layers between are known
    as the *hidden*, or *deep*, layers. So a four-layer DNN architecture would look
    like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 将有向图可视化成深度层。根节点是输入层，终端节点是输出层。中间的层被称为*隐藏层*或*深层层*。所以一个四层的深度神经网络（DNN）架构看起来会是这样：
- en: Input layer
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层
- en: Hidden layer
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Hidden layer
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层
- en: Output layer
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层
- en: To get started, we’ll assume that every neural network node in every layer,
    except the output layer, is the same type of neural network node. We’ll also assume
    that every node on each layer is connected to every other node on the next layer.
    This is known as a *fully connected neural network* (FCNN), depicted in figure
    2.2\. For example, if the input layer has three nodes, and the next (hidden) layer
    has four nodes, then each node on the first layer is connected to all four nodes
    on the next layer—for a total of 12 connections (3 × 4).
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，我们将假设除了输出层之外，每一层的每个神经网络节点都是相同类型的神经网络节点。我们还将假设每一层的每个节点都与下一层的每个节点相连。这被称为*全连接神经网络*（FCNN），如图2.2所示。例如，如果输入层有三个节点，下一个（隐藏）层有四个节点，那么第一层的每个节点都与下一层的所有四个节点相连——总共12个连接（3
    × 4）。
- en: '![](Images/CH02_F02_Ferlitsch.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH02_F02_Ferlitsch.png)'
- en: Figure 2.2 Deep neural networks have one or more hidden layers between the input
    and output layers. This is a fully connected network, so the nodes at each level
    are all connected to one another.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 深度神经网络在输入层和输出层之间有一个或多个隐藏层。这是一个全连接网络，因此每个层的节点都相互连接。
- en: 2.1.3 Feed-forward networks
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 前馈网络
- en: The DNN and CNN (you’ll learn more about the CNN in chapter 3) are known as
    *feed-forward neural networks*. *Feed-forward* means that data moves through the
    network sequentially, in one direction, from the input layer to the output layer.
    This is analogous to a function in procedural programming. The inputs are passed
    as parameters in the input layer, and the function performs a sequenced set of
    actions based on the inputs (in the hidden layers) and then outputs a result (the
    output layer).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: DNN和CNN（你将在第3章中了解更多关于CNN的内容）被称为*前馈神经网络*。*前馈*意味着数据按顺序通过网络，单向地从输入层流向输出层。这与过程式编程中的函数类似。输入作为参数在输入层中传递，函数根据输入（在隐藏层中）执行一系列按顺序的动作，然后输出结果（输出层）。
- en: When coding a feed-forward network in TF.Keras, you will see two distinctive
    styles in blogs and other tutorials. I will briefly touch on both so when you
    see a code snippet in one style, you can translate it to the other.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当在TF.Keras中编码前馈网络时，你会在博客和其他教程中看到两种不同的风格。我将简要介绍这两种风格，这样当你看到一种风格的代码片段时，你可以将其翻译成另一种风格。
- en: 2.1.4 Sequential API method
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.4 顺序API方法
- en: 'The *sequential API method* is easier to read and follow for beginners, but
    the tradeoff is that it is less flexible. Essentially, you create an empty feed-forward
    neural network with the `Sequential` class object, and then “add” one layer at
    a time, until the output layer. In the following examples, the ellipses represent
    pseudocode:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '*顺序API方法*对于初学者来说更容易阅读和遵循，但代价是它的灵活性较低。本质上，你使用`Sequential`类对象创建一个空的 feed-forward
    神经网络，然后逐个“添加”一层，直到输出层。在以下示例中，省略号代表伪代码：'
- en: '[PRE3]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Creates an empty model
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个空模型
- en: ❷ Placeholders for adding layers in sequential order
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 按顺序添加层的占位符
- en: 'Alternatively, the layers can be specified in sequential order as a list passed
    as a parameter when instantiating the `Sequential` class object:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以在实例化`Sequential`类对象时，以列表的形式按顺序指定层，作为参数传递：
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: So, you might ask, when would you use the `add()` method versus specifying as
    a list in the instantiation of the `Sequential` object? Well, both methods generate
    the same model and behavior, so it’s a matter of personal preference. I tend to
    use the more verbose `add()` method in instructional and demonstration material
    for clarity. But if I am writing code for production, I use the sparser list method,
    where I can visualize and edit the code more easily.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可能会问，何时使用`add()`方法与在`Sequential`对象实例化时指定列表？嗯，两种方法生成相同的模型和行为，所以这是一个个人偏好的问题。我倾向于在教程和演示材料中使用更详尽的`add()`方法以提高清晰度。但如果是编写生产代码，我使用更简洁的列表方法，这样我可以更容易地可视化和编辑代码。
- en: 2.1.5 Functional API method
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.5 功能API方法
- en: 'The *functional API method* is more advanced, allowing you to construct models
    that are nonsequential in flow—such as branches, skip links, and multiple inputs
    and outputs (you’ll see how multiple inputs and outputs work in section 2.4).
    You build the layers separately and then tie them together. This latter step gives
    you the freedom to connect layers in creative ways. Essentially, for a feed-forward
    neural network, you create the layers, bind them to another layer or layers, and
    then pull all the layers together in a final instantiation of a `Model` class
    object:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '*功能API方法*更为高级，允许你构建非顺序流模型——例如分支、跳转链接、多个输入和输出（你将在第2.4节中看到多个输入和输出的工作方式）。你分别构建层，然后将它们连接起来。这一步骤给你提供了以创新方式连接层的自由。本质上，对于前馈神经网络，你创建层，将它们绑定到另一个或多个层，然后在`Model`类对象的最终实例化中拉取所有层。'
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Constructs the input layer
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建输入层
- en: ❷ Constructs the hidden layer and binds it to the input layer
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建隐藏层并将其绑定到输入层
- en: ❸ Constructs the output layer and binds it to the hidden layer
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建输出层并将其绑定到隐藏层
- en: ❹ Assembles the model by following the bindings from the input to output layers
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 按照从输入层到输出层的绑定组装模型
- en: 2.1.6 Input shape vs. input layer
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.6 输入形状与输入层
- en: The input *shape* and input *layer* can be confusing at first. They are not
    the same thing. More specifically, the number of nodes in the input layer does
    not need to match the shape of the input vector. That’s because every element
    in the input vector will be passed to every node in the input layer, as shown
    in figure 2.3.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 输入 *形状* 和输入 *层* 在一开始可能会让人困惑。它们不是同一件事。更具体地说，输入层中的节点数量不需要与输入向量的形状相匹配。这是因为输入向量中的每个元素都将传递到输入层中的每个节点，如图
    2.3 所示。
- en: '![](Images/CH02_F03_Ferlitsch.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH02_F03_Ferlitsch.png)'
- en: Figure 2.3 The input (shape) and input layer differ. Every element in the input
    is connected to every node in the input layer.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.3 输入（形状）和输入层不同。输入中的每个元素都与输入层中的每个节点相连。
- en: For example, if our input layer is 10 nodes, and we use our earlier example
    of a 13-element input vector, we will have 130 connections (10 × 13) between the
    input vector and the input layer.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的输入层有 10 个节点，并且我们使用之前提到的 13 元素输入向量作为例子，我们将在输入向量和输入层之间有 130 个连接（10 × 13）。
- en: Each connection between an element in the input vector and a node in the input
    layer has a *weight*, and each node in the input layer has a *bias*. Think of
    each connection between the input vector and input layer, as well as connections
    between layers, as sending a signal forward indicating how strongly it believes
    the input value will contribute to the model’s predictions. We need to have a
    measurement of the strength of this signal, and that is what the weight does.
    It is a coefficient that is multiplied against the input value for the input layer,
    and previous value for subsequent layers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 输入向量中的每个元素与输入层中的每个节点之间的每个连接都有一个 *权重*，输入层中的每个节点都有一个 *偏差*。将输入向量与输入层之间的每个连接以及层之间的连接视为发送一个信号，表示它对输入值将如何对模型的预测做出贡献的强烈信念。我们需要有一个测量这个信号强度的指标，这就是权重的作用。它是一个系数，用于乘以输入层的输入值和后续层的先前值。
- en: Now, each one of these connections is like a vector on an x-y plane. Ideally,
    we want each of these vectors to cross the y-axis at the same central point (for
    example, the 0 origin). But they don’t. To make the vectors relative to one another,
    the bias is the offset of each vector from the central point on the y-axis.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这些连接中的每一个都像是在 x-y 平面上的一个向量。理想情况下，我们希望这些向量在 y 轴上的同一个中心点（例如，0 原点）相交。但它们并没有。为了使这些向量相互关联，偏差是每个向量从
    y 轴中心点的偏移量。
- en: The weights and biases are what the neural network will “learn” during training.
    The weights and biases are also referred to as *parameters*. These values stay
    with the model after it is trained. This operation will otherwise be invisible
    to you.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 权重和偏差是神经网络在训练过程中将“学习”的内容。权重和偏差也被称为 *参数*。这些值在模型训练后将与模型一起保留。否则，这个操作对你来说是不可见的。
- en: 2.1.7 Dense layer
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.7 密集层
- en: In TF.Keras, layers in an FCNN are called *dense layers*. A dense layer has
    *n* number of nodes and is fully connected to the previous layer.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在 TF.Keras 中，FCNN（全连接神经网络）的层被称为 *密集层*。一个密集层有 *n* 个节点，并且与前一层完全连接。
- en: 'Let’s continue by defining in TF.Keras a three-layer neural network, using
    the sequential API method, for our example. Our input layer has 10 nodes and takes
    as input a 13-element vector (the 13 features), which is connected to a second
    (hidden) layer of 10 nodes, which is then connected to a third (output) layer
    of one node. Our output layer needs to be only one node since it will be outputting
    a single real value (for example, the predicted price of the house). In this example,
    we are going to use a neural network as a *regressor*, which means the neural
    network will output a single real number:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续，通过在 TF.Keras 中定义一个三层神经网络，使用 sequential API 方法，来构建我们的例子。我们的输入层有 10 个节点，接受一个
    13 元素向量（13 个特征）作为输入，该向量连接到第二个（隐藏）层，该层有 10 个节点，然后连接到第三个（输出）层，该层只有一个节点。由于输出层将输出单个实数值（例如，房屋的预测价格），因此输出层只需要一个节点。在这个例子中，我们将使用神经网络作为
    *回归器*，这意味着神经网络将输出一个单一的实数：
- en: Input layer = 10 nodes
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 输入层 = 10 个节点
- en: Hidden layer = 10 nodes
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层 = 10 个节点
- en: Output layer = 1 node
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 输出层 = 1 个节点
- en: For input and hidden layers, we can pick any number of nodes. The more nodes
    we have, the better the neural network can learn. But more nodes means more complexity,
    and more time in training and predicting.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于输入层和隐藏层，我们可以选择任意数量的节点。节点越多，神经网络的学习能力越强。但节点越多也意味着复杂性增加，训练和预测所需的时间也越长。
- en: 'In the following code example, we have three `add()` calls to the class object
    `Dense`. The `add()` method adds the layers in the same sequential order that
    we specified them. The first (positional) parameter is the number of nodes, 10
    in the first and second layer, and 1 in the third layer. Notice that in the first
    `Dense` layer, we add the (keyword) parameter `input_shape`. This is where we
    will define the input vector and connect it to the first (input) layer in a single
    instantiation of the `Dense` layer:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们对 `Dense` 类对象进行了三次 `add()` 调用。`add()` 方法以我们指定的相同顺序添加层。第一个（位置）参数是节点的数量，第一层和第二层是10个，第三层是1个。请注意，在第一个
    `Dense` 层中，我们添加了（关键字）参数 `input_shape`。这就是我们定义输入向量并将其连接到 `Dense` 层的第一个（输入）层的地方：
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The first layer requires the input_shape parameter in a sequential model.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在顺序模型中，第一层需要 `input_shape` 参数。
- en: ❷ Constructs the hidden layer
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建隐藏层
- en: ❸ Constructs the output layer as a regressor—single node
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将输出层构建为一个回归器——单个节点
- en: 'Alternatively, we can define the sequential sequence of the layers as a list
    parameter when instantiating the `Sequential` class object:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以在实例化 `Sequential` 类对象时，将层的顺序序列定义为列表参数：
- en: '[PRE7]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ The layers are specified in sequential order as a list.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 层以顺序列表的形式指定。
- en: 'Let’s now do the same but use the functional API method. First, we create an
    input vector by instantiating an `Input` class object. The (positional) parameter
    to the `Input` object is the shape of the input, which can be a vector, matrix,
    or tensor. In our example, we have a vector that is 13 elements long. So our shape
    is `(13,)`. I am sure you noticed the trailing comma. That’s to overcome a quirk
    in Python. Without the comma, `(13)` would be evaluated as an expression: the
    integer value 13 surrounded by parentheses. Adding a comma tells the interpreter
    this is a *tuple* (an ordered set of values).'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在做同样的事情，但使用功能API方法。首先，我们通过实例化一个 `Input` 类对象来创建一个输入向量。`Input` 对象的（位置）参数是输入的形状，它可以是一个向量、矩阵或张量。在我们的例子中，我们有一个13个元素的向量。因此，我们的形状是
    `(13,)`。我确信你已经注意到了尾随的逗号。这是为了克服Python中的一个怪癖。如果没有逗号，`(13)` 将被评估为一个表达式：一个由括号包围的整数值13。添加一个逗号告诉解释器这是一个
    *元组*（一个有序值集）。
- en: 'Next, we create the input layer by instantiating a `Dense` class object. The
    positional parameter to the `Dense` object is the number of nodes, which in our
    example is 10\. Note the peculiar syntax that follows: `(inputs)`. The `Dense`
    object is a callable; the object returned by instantiating `Dense` can be callable
    as a function. So we call it as a function, and in this case, the function takes
    as a (positional) parameter the input vector (or layer output) to connect it to;
    hence we pass it `inputs` so the input vector is bound to the 10-node input layer.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过实例化一个 `Dense` 类对象来创建输入层。`Dense` 对象的位置参数是节点的数量，在我们的例子中是10个。注意以下特殊的语法：`(inputs)`。`Dense`
    对象是一个可调用的对象；通过实例化 `Dense` 返回的对象可以作为函数调用。因此，我们将其作为函数调用，在这种情况下，该函数将输入向量（或层输出）作为（位置）参数来连接它；因此我们传递给它
    `inputs`，这样输入向量就被绑定到10节点的输入层。
- en: Next, we create the hidden layer by instantiating another `Dense` object with
    10 nodes. Using it as a callable, we (fully) connect it to the input layer.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过实例化另一个具有10个节点的 `Dense` 对象来创建隐藏层。使用它作为可调用的对象，我们将它（完全）连接到输入层。
- en: Then we create the output layer by instantiating another `Dense` object with
    one node. Using it as a callable, we (fully) connect it to the hidden layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们通过实例化另一个具有一个节点的 `Dense` 对象来创建输出层。使用它作为可调用的对象，我们将它（完全）连接到隐藏层。
- en: 'Finally, we put it all together by instantiating a `Model` class object, passing
    it the (positional) parameters for the input vector and output layer. Remember,
    all the other layers between are already connected, so we don’t need to specify
    them when instantiating the `Model()` object:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们通过实例化一个 `Model` 类对象，传递输入向量和输出层的（位置）参数来将这些全部组合起来。记住，所有其他层之间已经连接，因此在实例化 `Model()`
    对象时我们不需要指定它们：
- en: '[PRE8]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Constructs the input vector (13 elements)
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建输入向量（13个元素）
- en: ❷ Constructs the first (input) layer (10 nodes) and connects it to the input
    vector
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建第一个（输入）层（10个节点）并将其连接到输入向量
- en: ❸ Constructs the next (hidden) layer (10 nodes) and connects it to the input
    layer
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 构建下一个（隐藏）层（10个节点）并将其连接到输入层
- en: ❹ Constructs the output layer (1 node) and connects it to the previous (hidden)
    layer
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 构建输出层（1个节点）并将其连接到前一个（隐藏）层
- en: ❺ Constructs the neural network, specifying the input and output layers
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 构建神经网络，指定输入和输出层
- en: 2.1.8 Activation functions
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.8 激活函数
- en: When training or predicting (via inference), each node in a layer will output
    a value to the nodes in the next layer. We don’t want to pass the value as-is,
    but instead sometimes want to change the value in a particular manner. This process
    is called an *activation function*.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练或预测（通过推理）时，层中的每个节点都会向下一层的节点输出一个值。我们不想原样传递值，而有时希望以特定方式更改值。这个过程称为*激活函数*。
- en: 'Think of a function that returns a result, like `return result`. In the case
    of an activation function, instead of returning `result`, we would return the
    result of passing the result value to another (activation) function, like `return
    A(result``)`, where `A()` is the activation function. Conceptually, you can think
    of this as follows:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个返回结果的函数，比如`return result`。在激活函数的情况下，我们不会返回`result`，而是会返回将结果值传递到另一个（激活）函数的结果，例如`return
    A(result``)`，其中`A()`是激活函数。从概念上讲，你可以这样想：
- en: '[PRE9]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Activation functions assist neural networks in learning faster and better. By
    default, when no activation function is specified, the values from one layer are
    passed as-is (unchanged) to the next layer. The most basic activation function
    is a *step function*. If the value is greater than 0, a 1 is outputted; otherwise,
    a 0 is outputted. The step function hasn’t been used in a long, long time.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数帮助神经网络更快、更好地学习。默认情况下，当未指定激活函数时，将一层中的值原样（未改变）传递到下一层。最基本的激活函数是*阶跃函数*。如果值大于0，则输出1；否则，输出0。阶跃函数已经很久没有使用了。
- en: Let’s pause for a moment and discuss the purpose of an activation function.
    You likely have heard the term *nonlinearity**.* What is this? To me, more importantly,
    what is it not?
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂停一下，讨论激活函数的目的。你可能已经听说过*非线性*这个术语。这是什么？对我来说，更重要的是，它不是什么？
- en: In traditional statistics, we worked in low-dimensional space with a strong
    linear correlation between the input and output. This correlation could be computed
    as a polynomial transformation of the input that, when transformed, had a linear
    correlation to the output. The most fundamental example is the slope of a line,
    which is represented as *y = mx + b*. In this case, *x* and *y* are coordinates
    of the line, and we want to fit the value of *m*, the slope, and *b*, where the
    line intercepts the y-axis.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统统计学中，我们在低维空间中工作，输入和输出之间存在强烈的线性相关性。这种相关性可以通过输入的多项式变换来计算，当变换后，与输出有线性相关性。最基本的一个例子是直线的斜率，表示为*y
    = mx + b*。在这种情况下，*x*和*y*是直线的坐标，我们想要拟合*m*的值，即斜率，以及*b*，即直线与y轴的交点。
- en: 'In deep learning, we work in high-dimensional space with substantial nonlinearity
    between the input and output. This nonlinearity means that an input is not uniformly
    related to (not near) an output based on a polynomial transformation of the input.
    For example, let’s say property tax is a fixed percentage rate (*r*) of the house
    value. The property tax can be represented by a function that multiplies the rate
    by the house value. Thus we have a linear (straight-line) relationship between
    the value (input) and property tax (output):'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习中，我们在高维空间中工作，输入和输出之间存在显著的非线性。这种非线性意味着输入不是基于输入的多项式变换均匀地与输出相关（不接近）。例如，假设财产税是房屋价值的固定百分比率（*r*）。财产税可以用一个函数表示，该函数将税率乘以房屋价值。因此，我们有一个线性（直线）的关系，即价值（输入）与财产税（输出）之间的关系：
- en: tax = *f* (*value*) = *r* × *value*
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: tax = *f* (*value*) = *r* × *value*
- en: 'Let’s look at the logarithmic scale for measuring earthquakes, in which an
    increase of 1 means the power released is 10 times greater. For example, an earthquake
    of 4 is 10 times stronger than a 3\. By applying a logarithmic transform to the
    input power, we have a linear relationship between power and scale:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看用于测量地震的对数刻度，其中增加1表示释放的能量是10倍。例如，4级地震比3级地震强10倍。通过对输入功率应用对数变换，我们得到功率和刻度之间的线性关系：
- en: scale = *f* (*power*) = log(*power*)
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: scale = *f* (*power*) = log(*power*)
- en: 'In a nonlinear relationship, sequences within the input have different linear
    relationships to the output, and in deep learning we want to learn the separation
    points as well as the linear functions for each input sequence. For example, consider
    age versus income to demonstrate a nonlinear relationship. In general, toddlers
    have no income, elementary-school children have an allowance, early teens earn
    an allowance plus money for chores, later teens earn money from jobs, and then
    when they go to college, their income drops to zero! After college, their income
    gradually increases until retirement, when it becomes fixed. We could model this
    nonlinearity as sequences across age and learn a linear function for each sequence,
    as depicted here:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在非线性关系中，输入序列中的序列与输出有不同的线性关系，在深度学习中，我们希望学习到每个输入序列的分离点以及线性函数。例如，考虑年龄与收入的关系来展示非线性关系。一般来说，幼儿没有收入，小学生有零花钱，青少年有零花钱加上家务钱，稍大一点的青少年通过工作赚钱，然后当他们上大学时，他们的收入降到零！大学毕业后，他们的收入逐渐增加，直到退休，这时收入变得固定。我们可以将这种非线性建模为年龄序列，并学习每个序列的线性函数，如图所示：
- en: '| income = F1(age) = 0 | for age [0..5] |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F1(年龄) = 0 | 对于年龄 [0..5] |'
- en: '| income = F2(age) = c1 | for age [6..9] |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F2(年龄) = c1 | 对于年龄 [6..9] |'
- en: '| income = F3(age) = c1 + (w1 × age) | for age [10..15] |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F3(年龄) = c1 + (w1 × 年龄) | 对于年龄 [10..15] |'
- en: '| income = F4(age) = (w2 × age) | for age [16..18] |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F4(年龄) = (w2 × 年龄) | 对于年龄 [16..18] |'
- en: '| income = F5(age) = 0 | for age [19..22] |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F5(年龄) = 0 | 对于年龄 [19..22] |'
- en: '| income = F6(age) = (w3 × age) | for age [23..64] |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F6(年龄) = (w3 × 年龄) | 对于年龄 [23..64] |'
- en: '| income = F7(age) = c2 | for age [65+] |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| 收入 = F7(年龄) = c2 | 对于年龄 [65+] |'
- en: 'Activation functions assist in finding the nonlinear separations and corresponding
    clustering of nodes within input sequences, which then learn the (near) linear
    relationship to the output. Most of the time, you will use three activation functions:
    the rectified linear unit (ReLU), sigmoid, and softmax.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数有助于在输入序列中找到非线性分离以及节点的对应聚类，然后学习到与输出的（近）线性关系。大多数情况下，你会使用三种激活函数：修正线性单元（ReLU）、Sigmoid和Softmax。
- en: We will start with the ReLU, since it is the one that is most used in all but
    the output layer of a model. The sigmoid and softmax activations are covered in
    sections 2.2 and 2.3\. The ReLU, depicted in figure 2.4, passes values greater
    than zero as-is (unchanged); otherwise, it passes zero (no signal).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从ReLU开始，因为它是除了输出层之外所有模型中最常用的。Sigmoid和Softmax激活将在第2.2节和第2.3节中介绍。ReLU如图2.4所示，将大于零的值原样通过（不变）；否则，它通过零（无信号）。
- en: '![](Images/CH02_F04_Ferlitsch.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH02_F04_Ferlitsch.png)'
- en: Figure 2.4 The function for a rectified linear unit clips all negative values
    to zero. In essence, any negative value is the same as no signal, or ~ zero.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 矩形线性单元函数将所有负值裁剪为零。本质上，任何负值都等同于无信号，或~零。
- en: 'The ReLU is generally used between layers. While early researchers used different
    activation functions (such as a hyperbolic tangent) between layers, researchers
    found that the ReLU produced the best result in training a model. In our example,
    we will add a ReLU between each layer:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ReLU通常用于层之间。虽然早期研究人员在层之间使用了不同的激活函数（例如双曲正切函数），但研究人员发现ReLU在训练模型时产生了最佳结果。在我们的例子中，我们将在每一层之间添加一个ReLU：
- en: '[PRE10]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Convention is to add ReLU activation to each non-output layer
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 习惯上，将ReLU激活添加到每个非输出层
- en: 'Let’s take a look inside our model object to see if we constructed what we
    think we did. You can do this by using the `summary()` method. This method is
    useful for visualizing the layers you constructed and verifying that what you
    intended to build is what you actually built. It will show in sequential order
    a summary of each layer:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们的模型对象内部，看看我们是否构建了我们认为的那样。你可以通过使用`summary()`方法来完成这个操作。这个方法对于可视化你构建的层以及验证你打算构建的内容是否实际构建了非常有用。它将按顺序显示每个层的摘要：
- en: '[PRE11]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: For this code example, you see that the summary starts with a dense layer of
    10 nodes (input layer), followed by a ReLU activation function, followed by a
    second dense layer (hidden) of 10 nodes, followed by a ReLU activation function,
    and finally followed by a dense layer (output) of 1 node. So, yes, we got what
    we expected.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个代码示例，你可以看到总结从10个节点的密集层（输入层）开始，接着是一个ReLU激活函数，然后是一个10个节点的第二个密集层（隐藏层），接着是一个ReLU激活函数，最后是一个1个节点的密集层（输出层）。所以，是的，我们得到了我们预期的结果。
- en: 'Next, let’s look at the parameter field in the summary. The input layer shows
    140 parameters. How is that calculated? We have 13 inputs and 10 nodes, so 13
    × 10 is 130\. Where does 140 come from? Each connection between the inputs and
    each node has a weight, which adds up to 130\. But each node has an additional
    bias. That’s 10 nodes, so 130 + 10 = 140\. As I’ve said, it’s the weights and
    biases that the neural network will “learn” during training. A *bias* is a learned
    offset, conceptually equivalent to the y-intercept (*b*) in the slope of a line,
    which is where the line intercepts the y-axis:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看摘要中的参数字段。输入层显示140个参数。这是如何计算的？我们有13个输入和10个节点，所以13 × 10是130。140是从哪里来的？输入和每个节点之间的每个连接都有一个权重，这些权重加起来是130。但是每个节点还有一个额外的偏差。这是10个节点，所以130
    + 10 = 140。正如我所说的，这是神经网络在训练期间将“学习”的权重和偏差。*偏差*是一个学习到的偏移量，在概念上等同于线的斜率中的y截距（*b*），这是线与y轴相交的地方：
- en: '*y* = *b* + *mx*'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*y* = *b* + *mx*'
- en: At the next (hidden) layer, you see 110 parameters. That’s 10 outputs from the
    input layer connected to each of the 10 nodes from the hidden layer (10 × 10)
    plus the 10 biases for the nodes in the hidden layers, for a total of 110 parameters
    to learn.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个（隐藏）层，您可以看到110个参数。这是从输入层连接到隐藏层每个节点的10个输出（10 × 10）加上隐藏层中节点的10个偏差，总共110个参数需要学习。
- en: 2.1.9 Shorthand syntax
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.9 简写语法
- en: TF.Keras provides a shorthand syntax when specifying layers. You don’t actually
    need to separately specify activation functions between layers, as we did in the
    previous example. Instead, you can specify the activation function as a (keyword)
    parameter when instantiating a `Dense` layer.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: TF.Keras在指定层时提供了简写语法。实际上，您不需要像上一个例子那样在层之间单独指定激活函数。相反，您可以在实例化`Dense`层时将激活函数指定为一个（关键字）参数。
- en: 'You might ask, why not always use the shorthand syntax? As you will see in
    chapter 3, in today’s model architecture, the activation function is preceded
    by another intermediate layer (batch normalization) or precedes the layer altogether
    (pre-activation batch normalization). The following code example does exactly
    the same thing as the previous one:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能会问，为什么不总是使用简写语法？正如您将在第3章中看到的，在今天的模型架构中，激活函数位于另一个中间层（批归一化）之前，或者根本位于层之前（预激活批归一化）。以下代码示例与上一个示例完全相同：
- en: '[PRE12]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The activation function is specified as a keyword parameter in the layer.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 激活函数在层中指定为关键字参数。
- en: 'Let’s call the `summary()` method on this model:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调用这个模型的`summary()`方法：
- en: '[PRE13]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Hmm, you don’t see the activations between the layers as you did in the earlier
    example. Why not? It’s a quirk in the way the `summary()` method displays output.
    They are still there.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，您没有看到层之间的激活，就像在之前的例子中那样。为什么？这是因为`summary()`方法显示输出的方式有点奇怪。它们仍然存在。
- en: 2.1.10 Improving accuracy with an optimizer
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.10 使用优化器提高准确度
- en: Once you’ve completed building the feed-forward portion of your neural network,
    as we have for our simple example, you need to add a few things for training the
    model. This is done with the `compile()` method. This step adds the *backward
    propagation* during training. Let’s define and explore this concept.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您完成了神经网络前馈部分的构建，就像我们在简单示例中所做的那样，您需要添加一些东西来训练模型。这是通过`compile()`方法完成的。这一步在训练期间添加了*反向传播*。让我们定义并探讨这个概念。
- en: Each time we send data (or a batch of data) forward through the neural network,
    it calculates the errors in the predicted results (known as the *loss*) from the
    actual values (called *labels*) and uses that information to incrementally adjust
    the weights and biases of the nodes. This, for a model, is the process of learning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 每次我们将数据（或数据批次）通过神经网络向前传递时，它都会计算预测结果中的误差（称为*损失*），这些误差与实际值（称为*标签*）进行比较，并使用这些信息来逐步调整节点的权重和偏差。对于模型来说，这个过程就是学习的过程。
- en: The calculation of the error, as I’ve said, is called a *loss*. It can be calculated
    in many ways. Since we designed our example neural network to be a *regresso**r*
    (meaning that the output, house price, is a real value), we want to use a loss
    function that is best suited for a regressor. Generally, for this type of neural
    network, we use the *mean square error* method of calculating a loss. In Keras,
    the `compile()` method takes a (keyword) parameter `loss` used to specify how
    we want to calculate the loss. We are going to pass it the value `mse` `(`for
    *mean square error*).
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 如我所说，误差的计算称为**损失**。它可以以多种方式计算。由于我们设计的示例神经网络是一个**回归器**（意味着输出，房价，是一个实数值），我们希望使用最适合回归器的损失函数。通常，对于这种类型的神经网络，我们使用**均方误差**方法来计算损失。在Keras中，`compile()`方法接受一个（关键字）参数`loss`，用于指定我们想要如何计算损失。我们将传递给它`mse`（代表**均方误差**）的值。
- en: The next step in the process is minimizing the loss with an optimizer that occurs
    during backward propagation. The optimizer is based on *gradient descent* ; different
    variations of the *gradient descent* *algorithm* can be selected. These terms
    can be hard to understand at first. Essentially, each time we pass data through
    the neural network, we use the calculated loss to decide how much to change the
    weights and biases in the layers. The goal is to gradually get closer and closer
    to the correct values for the weights and biases, to accurately predict or estimate
    the label for each example. This process of progressively getting closer and closer
    to the accurate values is called *convergence*. The job of the optimizer is to
    calculate the updates to the weights to progressively get closer to the accurate
    values to reach convergence.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 过程中的下一步是使用在反向传播过程中出现的优化器来最小化损失。优化器基于**梯度下降**；可以选择不同的**梯度下降**算法变体。这些术语一开始可能难以理解。本质上，每次我们通过神经网络传递数据时，我们都会使用计算出的损失来决定如何改变层中的权重和偏置。目标是逐渐接近权重和偏置的正确值，以准确预测或估计每个示例的标签。这个过程被称为**收敛**。优化器的任务是计算权重的更新，以逐渐接近正确的值并达到收敛。
- en: As the loss gradually decreases, we are *converging*. After the loss plateaus
    out, we have *convergence*. The result is the accuracy of the neural network.
    Before using gradient descent, the methods used by early AI researchers could
    take years on a supercomputer to find convergence on a nontrivial problem. After
    the discovery of using the gradient descent algorithm, this time was reduced to
    days, hours, and even just minutes on ordinary computing power. Let’s skip the
    math and just say that gradient descent is the data scientist’s pixie dust that
    makes converging on a good local optimum possible.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 随着损失的逐渐降低，我们正在**收敛**。当损失达到平台期后，我们实现了**收敛**。结果是神经网络的准确率。在梯度下降之前，早期AI研究人员使用的方法可能需要超级计算机数年才能在非平凡问题上找到收敛。梯度下降算法的发现之后，这个时间缩短到了几天、几小时，甚至在普通计算能力上只需几分钟。让我们跳过数学，只说梯度下降是数据科学家的一种魔法，它使得在良好的局部最优解上收敛成为可能。
- en: 'For our regressor neural network, we will use the `rmsprop` method (*root mean
    square property*):'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的回归器神经网络，我们将使用`rmsprop`方法（**均方根属性**）：
- en: '[PRE14]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now we have completed building your first trainable neural network. Before
    embarking on preparing data and training the model, we will cover several more
    neural network designs. These designs use the two other activation functions I
    mentioned earlier: sigmoid and softmax.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了您第一个可训练神经网络的构建。在开始准备数据和训练模型之前，我们将介绍更多关于神经网络的设计。这些设计使用了之前提到的两种激活函数：sigmoid和softmax。
- en: 2.2 DNN binary classifier
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 DNN二分类器
- en: 'Another form of a DNN is a *binary classifier*, also known as a *logistic classifier*.
    When we use a binary classifier, we want the neural network to predict whether
    the input is or is not something. The output can have two states or classes: yes/no,
    true/false, 0/1, and so forth.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: DNN的另一种形式是**二分类器**，也称为**逻辑分类器**。当我们使用二分类器时，我们希望神经网络预测输入是否是某物。输出可以有两种状态或类别：是/否、真/假、0/1等等。
- en: For example, let’s say we have a dataset of credit card transactions, and each
    transaction is labeled as fraudulent or not fraudulent. Remember, the label is
    what we want to predict.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们有一个信用卡交易数据集，每个交易都被标记为欺诈或非欺诈。记住，标签是我们想要预测的内容。
- en: Overall, the design approach we’ve learned so far doesn’t change, with the exception
    of the activation function of the single-node output layer and the loss/optimizer
    method. Instead of using a *linear* activation function, as for a regressor, we
    will use a *sigmoid* activation function on the output node. The sigmoid squashes
    all values so they are between 0 and 1, as shown in figure 2.5\. As values move
    away from the center, they quickly move to the extremes of 0 and 1 (the asymptotes).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们迄今为止学到的设计方法没有改变，除了单节点输出层的激活函数以及损失/优化器方法。与回归器不同，我们将在输出节点上使用一个**sigmoid**激活函数，而不是**线性**激活函数。sigmoid函数将所有值压缩到0和1之间，如图2.5所示。随着值远离中心，它们会迅速移动到0和1的极端（渐近线）。
- en: '![](Images/CH02_F05_Ferlitsch.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH02_F05_Ferlitsch.png)'
- en: Figure 2.5 The function for a sigmoid
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 sigmoid函数
- en: We will now code this in the two styles we’ve discussed. Let’s start with our
    previous code example, in which we specify the activation function as a (keyword)
    parameter. In this example, we add to the output `Dense` layer the parameter `activation=
    'sigmoid'` to pass the output result from the final node through a sigmoid function.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们讨论过的两种风格来编写这段代码。让我们从之前的代码示例开始，其中我们将激活函数指定为一个（关键字）参数。在这个例子中，我们在输出`Dense`层中添加了参数`activation=
    'sigmoid'`，以便将最终节点的输出结果通过sigmoid函数传递。
- en: 'Next, we change our loss parameter to `binary_crossentropy`. This is the loss
    function that is generally used in a binary classifier:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将我们的损失参数更改为`binary_crossentropy`。这是在二分类器中通常使用的损失函数：
- en: '[PRE15]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ A sigmoid is used for a binary classification.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Sigmoid函数用于二分类。
- en: ❷ Common convention for loss and optimizer for binary classifier
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 二分类器中损失函数和优化器的常见约定
- en: 'Not all activation functions have their own class, like `ReLU`. This is another
    quirk in the TF.Keras framework. Instead, a class called `Activation` creates
    any of the supported activations. The parameter is the predefined name of the
    activation function. In our example, `relu` is for the rectified linear unit,
    and `sigmoid` is for the sigmoid. The following code does the same as the preceding
    code:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 并非所有激活函数都有自己的类，例如`ReLU`。这是TF.Keras框架中的另一个特性。相反，一个名为`Activation`的类可以创建任何受支持的激活函数。参数是预定义的激活函数名称。在我们的例子中，`relu`代表修正线性单元，而`sigmoid`代表sigmoid。以下代码与前面的代码执行相同的操作：
- en: '[PRE16]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Activations can be specified using the Activation() method.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 可以使用Activation()方法指定激活函数。
- en: Now we will rewrite the same code using the functional API approach. Notice
    that we repeatedly used the variable `x`. This is a common practice. We want to
    avoid creating lots of one-time-use variables. Since we know that in this type
    of neural network, the output of every layer is the input to the next layer (or
    activation), except for the input and output, we continuously use `x` as the connecting
    variable.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用功能API方法重写相同的代码。请注意，我们反复使用了变量`x`。这是一个常见的做法。我们希望避免创建大量的一次性使用变量。由于我们知道在这种类型的神经网络中，除了输入和输出之外，每一层的输出都是下一层的输入（或激活），因此我们持续使用`x`作为连接变量。
- en: 'By now, you should start becoming familiar with the two approaches:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，你应该开始熟悉这两种方法：
- en: '[PRE17]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Activations specified using functional API
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用功能API指定的激活函数
- en: 2.3 DNN multiclass classifier
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 DNN多分类器
- en: Let’s say we have a set of body measurements (height and weight, for instance)
    and the gender associated with each set of measurements, and we want to predict
    whether someone is a baby, toddler, preteen, teenager, or adult. We want our model
    to classify, or predict, from more than one class or label—in this example, we
    have a total of five classes of age categories. To do this, we can use another
    form of a DNN, called a *multiclass classifier*.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一组身体测量值（例如身高和体重）以及与每组测量值相关的性别，我们想要预测某人是否是婴儿、幼儿、儿童、青少年或成人。我们希望我们的模型能够从多个类别或标签中进行分类或预测——在这个例子中，我们有总共五个年龄类别的类别。为此，我们可以使用DNN的另一种形式，称为**多分类器**。
- en: We can already see we will have some complications. For example, men on average
    as adults are taller than women. But during the preteen years, girls tend to be
    taller than boys. We know on average that men get heavier early in their adult
    years in comparison to their teenage years, but women on average are less likely
    to become heavier. So we should anticipate problems in predicting around the preteen
    years for girls, teenage years for boys, and adult years for women.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经可以看到我们将会有一些复杂性。例如，成年男性平均身高比女性高。但在青春期前，女孩往往比男孩高。我们知道男性在成年早期比在青少年时期体重增加得更多，但女性平均来说不太可能增重。因此，我们应该预料到在青春期前预测女孩、青春期预测男孩和成年预测女性时会出现问题。
- en: These problems are examples of *nonlinearity* ; the relationship between a feature
    and a prediction is not linear. Instead, the relationship can be broken into segments
    of disjointed linearity. This is the type of problem neural networks are good
    at.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题是**非线性**的例子；特征与预测之间的关系不是线性的。相反，这种关系可以被分解为不连续的线性段。这正是神经网络擅长的类型的问题。
- en: Let’s add a fourth measurement, the nose surface area. Studies such as one from
    the *Annals of Plastic Surgery* ([https://pubmed.ncbi.nlm.nih.gov/3579170/](https://pubmed.ncbi.nlm.nih.gov/3579170/))
    have shown that for both girls and boys, the surface area of the nose continues
    to grow from ages 6 to 18 and essentially stops at 18.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们增加一个第四个测量值，即鼻子的表面积。例如，《整形外科学年鉴》*（Annals of Plastic Surgery）*的研究([https://pubmed.ncbi.nlm.nih.gov/3579170/](https://pubmed.ncbi.nlm.nih.gov/3579170/))表明，对于女孩和男孩来说，鼻子的表面积从6岁到18岁持续增长，并在18岁时基本停止增长。
- en: So now we have four features, and a label that consists of five classes. We
    will change our input vector in the next example to 4, to match the number of
    features, and change our output layer to 5 nodes, to match the number of classes.
    In this case, each output node corresponds to one unique class (baby, toddler,
    and so forth). We want to train the neural network so each output node outputs
    a value between 0 and 1 as a prediction. For example, 0.75 would mean that the
    node is 75% confident that the prediction is the corresponding class.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现在我们有四个特征，一个由五个类别组成的标签。在下一个例子中，我们将改变我们的输入向量到4，以匹配特征的数量，并将我们的输出层改变为5个节点，以匹配类别的数量。在这种情况下，每个输出节点对应一个独特的类别（婴儿、幼儿等等）。我们希望训练神经网络，使每个输出节点输出一个介于0到1之间的值作为预测。例如，0.75意味着该节点有75%的信心认为预测是相应的类别。
- en: 'Each output node will independently learn and predict its confidence on whether
    the input is the corresponding class. This process leads to a problem, however:
    because the values are independent, they won’t add up to 1 (100%). This is where
    the *softmax* function is useful. This mathematical function will take a set of
    values (the outputs from the output layer) and squash them into a range from 0
    to 1 while also ensuring that all the values add up to 1\. Perfect. This way,
    we can take the output node with the highest value and say what is predicted as
    well as the confidence level in that prediction. So if the highest value is 0.97,
    we can say we estimated the confidence at 97% in our prediction.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每个输出节点将独立学习和预测其对应类别的置信度。然而，这个过程导致了一个问题：因为值是独立的，它们不会加起来等于1（100%）。这就是softmax函数有用的地方。这个数学函数将一组值（输出层的输出）压缩到0到1的范围内，同时确保所有值加起来等于1。完美。这样，我们可以选择具有最高值的输出节点，并说出预测的内容以及该预测的置信度。所以如果最高值是0.97，我们可以说我们在预测中估计的置信度为97%。
- en: Figure 2.6 is a diagram of a multiclass model. In this example, the output layer
    has two nodes, each corresponding to predicting a different class. Each node makes
    an independent prediction of how strongly it believes that the input is the corresponding
    class. The two independent predictions are then passed through a softmax activation,
    which squashes the values to add up to 1 (100%). In this example, one class is
    predicted at 97%, and the other at 3% confidence level.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6是一个多类模型的图示。在这个例子中，输出层有两个节点，每个节点对应预测一个不同的类别。每个节点独立地预测它对输入属于相应类别的信念强度。这两个独立的预测随后通过softmax激活函数，将值压缩到总和为1（100%）。在这个例子中，一个类别以97%的置信度被预测，另一个以3%的置信度被预测。
- en: '![](Images/CH02_F06_Ferlitsch.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH02_F06_Ferlitsch.png)'
- en: Figure 2.6 Adding softmax activation to the output layer for a multiclass classifier
    helps improve the confidence level in the model’s prediction.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6为多类分类器的输出层添加softmax激活有助于提高模型预测的置信度。
- en: The following code shows an example of constructing a multiclass classifier
    DNN. We start by setting up our input and output layers with the multiple features
    and multiple classes, respectively. Then we change the activation function from
    `sigmoid` to `softmax`. Next we set our loss function to `categorical_crossentropy`.
    This is generally the most recommended for a multiclass classification. We won’t
    go deep into the statistics behind cross-entropy, other than cross-entropy computes
    a loss from multiple probability distributions. In a binary classifier, we have
    two probability distributions and use the `binary_crossentropy` calculation; and
    in a multiclass classifier, we use `categorical_ crossentropy` for calculating
    the loss from multiple (more than two) probability distributions.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码展示了构建多类分类器DNN的示例。我们首先设置我们的输入层和输出层，分别使用多个特征和多个类别。然后我们将激活函数从`sigmoid`改为`softmax`。接下来，我们将损失函数设置为`categorical_crossentropy`。这通常是最推荐用于多类分类的。我们不会深入探讨交叉熵背后的统计学原理，除了交叉熵计算多个概率分布的损失。在二元分类器中，我们有两个概率分布并使用`binary_crossentropy`计算；而在多类分类器中，我们使用`categorical_crossentropy`来计算多个（多于两个）概率分布的损失。
- en: 'Finally, we will use a popular and widely used variant of gradient descent
    called the *Adam optimizer* (`adam`). *Adam* incorporates several aspects of other
    methods, such as `rmsprop` (*root mean square*) and `adagrad` (*adaptive gradien*t),
    along with an adaptive learning rate. It’s generally considered the best-in-class
    optimizer for a wide variety of neural networks:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将使用一种流行且广泛使用的梯度下降变体，称为*Adam优化器*（`adam`）。*Adam*结合了其他方法的一些方面，如`rmsprop`（*均方根*）和`adagrad`（*自适应梯度*），以及自适应学习率。它通常被认为是适用于各种神经网络的最佳优化器：
- en: '[PRE18]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ The input layer for an input shape of 1D vector of four features
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入层为四个特征的1D向量
- en: ❷ In the output layer, a softmax activation is used for a multiclass classifier.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在输出层，多类分类器使用softmax激活函数。
- en: ❸ Common conventions for the loss function and optimizer for a multiclass classifier
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 多类分类器损失函数和优化器的常用约定
- en: 2.4 DNN multilabel multiclass classifier
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 DNN多标签多类分类器
- en: 'Now let’s look at predicting two or more classes (labels) per input. Let’s
    use our previous example of predicting whether someone is a baby, toddler, preteen,
    teenager, or adult. This time, we will remove gender from one of the features
    and instead make it one of the labels to predict. Our input will be the height,
    weight, and nose surface area, and our outputs will be two classes: age category
    (baby, toddler, etc.) and gender (male or female). An example prediction might
    look like this:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看预测每个输入的两个或更多类别（标签）。让我们使用我们之前的例子，预测某人是否是婴儿、幼儿、儿童、青少年或成人。这次，我们将从特征中移除性别，并将其作为要预测的一个标签。我们的输入将是身高、体重和鼻表面面积，我们的输出将是两个类别：年龄类别（婴儿、幼儿等）和性别（男性或女性）。一个预测示例可能看起来像这样：
- en: '[height, weight, nose surface area] -> neural network -> [preteen, female]'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '[身高，体重，鼻表面面积] -> 神经网络 -> [儿童，女性]'
- en: 'To predict two or more labels from multiple inputs, as we do here, we use—you
    guessed it—a *multilabel multiclass classifier*. To do this, we need to make a
    few changes from our previous multiclass classifier*.* On our output layer, our
    number of output classes is the sum of all the output categories. In this case,
    we previously had five and now we add two more for gender, for a total of seven.
    We also want to treat each output class as a binary classifier, meaning we want
    a yes/no type of answer, so we change the activation function to `sigmoid`. For
    our compile statement, we mimic what we’ve done with simpler DNNs in this chapter,
    and set the loss function to `binary_crossentropy`, and the optimizer to `rmsprop`.
    You can see the implementation of each step here:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从多个输入中预测两个或更多标签，正如我们在这里所做的那样，我们使用——你猜对了——一个*多标签多类分类器*。为此，我们需要对我们之前的多类分类器做一些修改。在我们的输出层，我们的输出类别数是所有输出类别的总和。在这种情况下，我们之前有五个，现在我们再增加两个用于性别，总共七个。我们还希望将每个输出类别视为二元分类器，这意味着我们希望得到一个是/否类型的答案，因此我们将激活函数改为`sigmoid`。对于我们的编译语句，我们模仿了本章中更简单的DNN所做的事情，将损失函数设置为`binary_crossentropy`，并将优化器设置为`rmsprop`。你可以在这里看到每个步骤的实现：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Input vector is just the height, weight, and nose surface area
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入向量仅仅是身高、体重和鼻表面面积
- en: ❷ Both age demographic and gender categories combined into one classifier output
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将年龄人口统计和性别类别合并为一个分类器输出
- en: ❸ Uses a sigmoid activation with binary_crossentropy loss function to independently
    predict each class as either, or close to, 0 or 1.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用sigmoid激活函数和二元交叉熵损失函数来独立预测每个类别为0或1，或者接近0或1。
- en: Do you see a potential problem with this design? Let’s assume we output the
    two classes (labels) with the highest values (from 0 to 1); that is, the most
    confident predictions. What if, on a prediction, the neural network predicts both
    preteen and teenager with high confidence, and both male and female with lower
    confidence? Well, we could fix this with some post-logic by selecting the highest
    confidence from the first five output classes (age demographic), and selecting
    the highest confidence from the last two classes (gender). In other words, we
    separate the seven output classes into the two corresponding categories, and from
    each category we select the output with the highest confidence level.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 你看到这个设计可能存在潜在问题吗？让我们假设我们输出两个具有最高值（从0到1）的类别（标签），即最自信的预测。如果在一个预测中，神经网络以高置信度预测了青少年和青少年，以及男性和女性以较低置信度，会怎样呢？嗯，我们可以通过一些后逻辑来修复这个问题，通过从前五个输出类别（年龄人口统计）中选择最高置信度，并从最后两个类别（性别）中选择最高置信度。换句话说，我们将七个输出类别分为两个相应的类别，并从每个类别中选择置信度最高的输出。
- en: The functional API gives us the ability to fix this without adding any post-logic.
    In this case, we want to replace the output layer, which combines the two sets
    of classes, with *two parallel output layers*, one for the first set of classes
    (age categories) and one for the second set of classes (gender). You can see this
    setup in figure 2.7.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 功能API使我们能够在不添加任何后逻辑的情况下修复这个问题。在这种情况下，我们想要用*两个并行输出层*替换结合两个类别集的输出层，一个用于第一组类别（年龄类别），另一个用于第二组类别（性别）。您可以在图2.7中看到这个设置。
- en: '![](Images/CH02_F07_Ferlitsch.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH02_F07_Ferlitsch.png)'
- en: Figure 2.7 Output layers of a multilabel multiclassifier with two parallel output
    layers
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 多标签多类分类器的两个并行输出层
- en: In the following code example, only the final output layer differs from the
    previous code listing. Here, instead of a single output layer, we have the two
    parallel layers.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，只有最终输出层与之前的代码列表不同。在这里，我们不是只有一个输出层，而是有两个并行层。
- en: 'Then when we put it all together with the `Model` class, instead of passing
    in a single output layer, we pass in a list of output layers: `[output1, output2]`.
    Finally, since each output layer makes independent predictions, we can return
    to treating them as a *multiclass classifier*—meaning we return to using `categorical_crossentropy`
    as the loss function and `adam` as the optimizer.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后当我们使用`Model`类将所有内容组合在一起时，我们传递的是一个输出层的列表：`[output1, output2]`。最后，由于每个输出层都做出独立的预测，我们可以将它们作为*多类分类器*来处理——这意味着我们返回使用`categorical_crossentropy`作为损失函数和`adam`作为优化器。
- en: 'This design of a multilabel multiclassifier can also be referred to as a *neural
    network* *with multiple outputs*, in which each output learns a different task.
    Since we will be training this model to do multiple independent predictions, this
    is also known as a *multitask* model:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这种多标签多类分类器的设计也可以称为*具有多个输出的神经网络*，其中每个输出学习不同的任务。由于我们将训练这个模型进行多个独立的预测，这也被称为*多任务模型*：
- en: '[PRE20]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Each of the two categories has a separate output layer and gets a copy of
    the same input.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个类别都有一个独立的输出层，并得到相同输入的副本。
- en: So which design is correct (or better) for a multilabel multiclass classifier?
    It depends on the application. If all the classes are from a single category—such
    as age demographic—use the first pattern, the single task. If the classes come
    from different categories—such as age demographic and gender—use the second pattern,
    the multitask. In this example, we use the multitask pattern, because we want
    to learn two categories as output.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 那么哪种设计对于多标签多类分类器是正确的（或更好的）？这取决于应用。如果所有类别都属于单个类别——例如年龄人口统计——使用第一个模式，即单一任务。如果类别来自不同的类别——例如年龄人口统计和性别——使用第二个模式，即多任务。在这个例子中，我们使用多任务模式，因为我们想要学习两个类别作为输出。
- en: 2.5 Simple image classifier
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 简单图像分类器
- en: You’ve now seen the basic types of DNNs and how to code them with TF.Keras.
    So now let’s build our first simple model for image classification.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在已经看到了DNN的基本类型以及如何使用TF.Keras来编码它们。所以现在让我们构建我们的第一个简单的图像分类模型。
- en: Neural networks are used for image classification throughout computer vision.
    Let’s start with the basics. For small grayscale images, as depicted in figure
    2.8, we can use a DNN similar to that we have already described for the multiclass
    classifier for predicting the age demographic. This type of DNN has been widely
    published using the Modified National Institute of Standards and Technology (MNIST)
    dataset, which is a dataset for recognizing handwritten digits. The dataset consists
    of grayscale images of size 28 × 28 pixels. Each pixel is represented by an integer
    value from 0 to 255 (0 is black, 255 is white, and values between are shades of
    gray).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机视觉中，神经网络被用于图像分类。让我们从基础知识开始。对于小灰度图像，如图2.8所示，我们可以使用一个类似于我们之前描述的多类分类器的深度神经网络来预测年龄人口统计。这种类型的DNN已经在使用修改后的国家标准与技术研究院（MNIST）数据集的文献中广泛发表，这是一个用于识别手写数字的数据集。该数据集由28
    × 28像素大小的灰度图像组成。每个像素由一个从0到255的整数值表示（0是黑色，255是白色，中间的值是灰色）。
- en: '![](Images/CH02_F08_Ferlitsch.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH02_F08_Ferlitsch.png)'
- en: Figure 2.8 Matrix representation of a grayscale image
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 灰度图像的矩阵表示
- en: We need to make one change, though. A grayscale image is a *matrix* (2D array).
    Think of a matrix as a grid, of size height × width, where the width represents
    the columns and the height represents the rows. A DNN, though, takes as input
    a *vector*, which is a 1D array. So what can we do? We can *flatten* the two-dimensional
    matrix into a one-dimensional vector.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，我们需要进行一个修改。灰度图像是一个*矩阵*（二维数组）。将矩阵想象成一个网格，大小为高度 × 宽度，其中宽度代表列，高度代表行。然而，深度神经网络将*向量*作为输入，这是一个一维数组。那么我们能做什么呢？我们可以将二维矩阵展平成一维向量。
- en: 2.5.1 Flattening
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 展平
- en: We are going to do classification by treating each pixel as a feature. Using
    the example of the MNIST dataset, the 28 × 28 images will have 784 pixels, and
    thus 784 features. We convert the matrix (2D) into a vector (1D) by flattening
    it.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将通过将每个像素视为一个特征来进行分类。以MNIST数据集为例，28 × 28的图像将有784个像素，因此有784个特征。我们通过展平将矩阵（二维）转换为向量（一维）。
- en: '*Flattening* is the process of placing each row in sequential order into a
    vector. So the vector starts with the first row of pixels, followed by the second
    row of pixels, and continues by ending with the last row of pixels. Figure 2.9
    depicts flattening a matrix into a vector.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '*展平*是将每一行按顺序放入向量的过程。因此，向量从像素的第一行开始，然后是第二行像素，以此类推，最后以最后一行像素结束。图2.9展示了将矩阵展平成向量的过程。'
- en: '![](Images/CH02_F09_Ferlitsch.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH02_F09_Ferlitsch.png)'
- en: Figure 2.9 A matrix flattened into a vector
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 将矩阵展平成向量
- en: You might be asking at this point, why do we need to flatten the 2D matrix into
    a 1D vector? It’s because in a DNN, the input to a dense layer must be a 1D vector.
    In the next chapter, when we introduce CNNs, you will see examples of convolutional
    layers that take 2D input.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会在这个时候问，为什么我们需要将二维矩阵展平成一维向量？这是因为在一个深度神经网络中，密集层的输入必须是一维向量。在下一章中，当我们介绍卷积神经网络（CNN）时，你会看到一些使用二维输入的卷积层示例。
- en: 'In the next example, we add a layer at the beginning of our neural network
    to flatten the input, using the class `Flatten`. The remaining layers and activations
    are typical for the MNIST dataset. Note that the input shape to the `Flatten`
    object is the 2D shape `(28, 28)`. The output from this object will be a 1D shape
    of `(784,)`:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个示例中，我们在神经网络的开头添加一个层来展平输入，使用`Flatten`类。剩余的层和激活对于MNIST数据集来说是典型的。请注意，`Flatten`对象的输入形状是二维形状`(28,
    28)`。该对象输出的形状将是一维的`(784,)`：
- en: '[PRE21]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ The 2D grayscale image is flattened into a 1D vector for input to the DNN.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将二维灰度图像展平成一维向量作为DNN的输入。
- en: ❷ MNIST is typically done as an input and one hidden dense layer between 128,
    256, and 512 nodes.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ MNIST通常作为输入，并在128、256和512个节点之间有一个隐藏的密集层。
- en: 'Let’s now look at the layers by using the `summary()` method. As you can see,
    the first layer in the summary is the flattened layer and shows that the output
    from the layer is 784 nodes. That’s what we want. Also notice how many parameters
    the network will need to learn during training, nearly 700,000:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过使用`summary()`方法来查看层。正如你所见，摘要中的第一层是展平层，显示该层的输出是784个节点。这正是我们想要的。同时注意网络在训练过程中需要学习的参数数量，接近70万个：
- en: '[PRE22]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 2.5.2 Overfitting and dropout
  id: totrans-213
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 过拟合和dropout
- en: During training, a dataset is split into training data and test data (also known
    as *holdout data*). Only the training data is used during the training of the
    neural network. Once the neural network has reached convergence*,* which we discuss
    in detail in chapter 4, training stops, as shown in figure 2.10.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，数据集被分为训练数据和测试数据（也称为*保留数据*）。在神经网络的训练过程中只使用训练数据。一旦神经网络达到收敛*，*，我们在第4章中详细讨论了这一点，训练就会停止，如图2.10所示。
- en: '![](Images/CH02_F10_Ferlitsch.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH02_F10_Ferlitsch.png)'
- en: Figure 2.10 Convergence happens when the slope of the loss plateaus.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 收敛发生在损失曲线平缓时。
- en: Afterward, to obtain the accuracy of the model on the training data, the training
    data is forward-fed again without backward propagation enabled, so there is no
    learning. This is also known as running the trained neural network in *inference*
    or *prediction mode*. In a train/test split, the test data, which has been set
    aside and not used as part of training, is forward-fed again without backward
    propagation enabled to obtain an accuracy.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，为了获得模型在训练数据上的准确率，训练数据再次正向传递，但不启用反向传播，因此没有学习。这也被称为在训练好的神经网络中运行*推理*或*预测模式*。在训练/测试分割中，之前保留并未作为训练一部分的测试数据再次正向传递，不启用反向传播，以获得准确率。
- en: Why do we split and hold out the test data from training? Ideally, the accuracy
    on the training data and the test data will be nearly identical. In reality, the
    test data will always be a little less. There is a reason for this.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要将测试数据从训练数据中分割出来并保留？理想情况下，训练数据和测试数据的准确率将几乎相同。实际上，测试数据的准确率总是略低。这有一个原因。
- en: Once you reach convergence, continually passing the training data through the
    neural network will cause the neurons to more and more memorize the training samples
    versus generalizing to samples that are never seen during training. This is known
    as *overfitting*. When the neural network is overfitted to the training data,
    you will get high training accuracy, but substantially lower accuracy on the test/evaluation
    data.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦达到收敛，持续将训练数据通过神经网络，会导致神经元越来越多地记住训练样本，而不是泛化到训练过程中从未见过的样本。这被称为*过拟合*。当神经网络对训练数据过拟合时，你将获得很高的训练准确率，但在测试/评估数据上的准确率会显著降低。
- en: Even without training past the convergence, you will have some overfitting.
    The dataset/problem is likely to have nonlinearity (hence why you’re using a neural
    network). As such, the individual neurons will converge at an unequal rate. When
    measuring convergence, you’re looking at the overall system. Prior to that, some
    neurons have already converged, and the continued training will cause them to
    overfit. This is why the test/evaluation accuracy will always be at least a bit
    less than the accuracy of the training data.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 即使没有训练到收敛，你也会有一些过拟合。数据集/问题可能存在非线性（这就是为什么你使用神经网络）。因此，单个神经元将以不同的速率收敛。在测量收敛时，你是在看整个系统。在此之前，一些神经元已经收敛，而持续的训练将导致它们过拟合。这就是为什么测试/评估准确率总是至少略低于训练数据的准确率。
- en: To address overfitting when training neural networks, we can use *regularization*.
    This adds small amounts of random noise during training to prevent the model from
    memorizing samples and better generalize to unseen samples after the model is
    trained.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决训练神经网络时的过拟合问题，我们可以使用*正则化*。这会在训练过程中添加少量的随机噪声，以防止模型记住样本，并在模型训练后更好地泛化到未见过的样本。
- en: The most basic type of regularization is called *dropout*. Dropout is like forgetting.
    When we teach young children, we use rote memorization, as when we ask them to
    memorize the multiplication table of numbers 1 through 12\. We have them iterate,
    iterate, iterate, until they can recite in any order the correct answer 100% of
    the time. But if we ask them “What is 13 times 13?” they will likely give us a
    blank look. At this point, the times table is overfitted in their memory. The
    answer to each multiplication pair, the samples, is hardwired in the brain’s memory
    cells, and they don’t have a way to transfer that knowledge beyond 1 through 12.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的正则化类型被称为*dropout*。Dropout就像遗忘。当我们教小孩子时，我们使用死记硬背，就像我们要求他们记住1到12的乘法表。我们让他们反复练习，反复练习，直到他们能够100%正确地按任何顺序背诵出正确的答案。但如果我们问他们“13乘以13等于多少？”他们可能会给我们一个茫然的表情。在这个时候，乘法表已经在他们的记忆中过拟合了。每个乘法对的答案，即样本，被硬编码在大脑的记忆细胞中，他们没有方法将那种知识扩展到1到12之外。
- en: As children get older, we switch to abstraction. Instead of teaching a child
    to memorize the answers, we teach them how to compute the answer—albeit they may
    make computation mistakes. During this second teaching phase, some neurons related
    to the rote memorization will die. The combination of the death of those neurons
    (which means forgetting) and abstraction allows the child’s brain to generalize
    and now solve arbitrary multiplication problems, though at times they will make
    a mistake, even at times in the 12 × 12 times table, with some probabilistic distribution.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 随着孩子们的成长，我们转向抽象。我们不是教孩子记忆答案，而是教他们如何计算答案——尽管他们可能会犯计算错误。在这个第二个教学阶段，一些与死记硬背相关的神经元会死亡。这些神经元的死亡（意味着遗忘）与抽象的结合使得孩子的头脑能够进行概括，现在可以解决任意的乘法问题，尽管有时他们会犯错误，甚至在12×12乘法表中，以某种概率分布。
- en: The dropout technique in neural networks mimics this process of moving to abstraction,
    and learning, with probabilistic distribution of uncertainty. Between any layer,
    you can add a dropout layer, where you specify a percentage (between 0 and 1)
    to forget. The nodes themselves won’t be dropped, but instead a random selection
    on each forward feed during training will not pass a signal forward. The signal
    from the randomly selected node will be forgotten. So, for example, if you specify
    a dropout of 50% (0.5), on each forward feed of data, a random selection of half
    of the nodes will not send a signal.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中的dropout技术模拟了这种向抽象和学习的迁移过程，通过概率分布的不确定性进行学习。在任意层之间，你可以添加一个dropout层，在那里你指定一个百分比（介于0和1之间）来遗忘。节点本身不会被丢弃，而是在训练过程中，每个前向传递的随机选择不会传递信号。随机选择的节点的信号将被遗忘。例如，如果你指定50%的dropout（0.5），在每次数据的前向传递中，随机选择一半的节点将不会发送信号。
- en: The advantage here is that we minimize the effect of localized overfitting while
    continuously training the neural network for overall convergence. A common practice
    for dropout is setting values between 20% and 50%.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 优势在于，我们最小化了局部过拟合的影响，同时持续训练神经网络以实现整体收敛。dropout的常见做法是设置20%到50%之间的值。
- en: 'In the following code example, we’ve added a 50% dropout to the input and hidden
    layers. Notice that we placed it before the activation (ReLU) function. Since
    dropout will cause the signal from the node, when dropped out, to be zero, it
    doesn’t matter whether you add the `Dropout` layer before or after the activation
    function:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码示例中，我们在输入层和隐藏层中添加了50%的dropout。注意，我们在激活函数（ReLU）之前放置了它。由于dropout会导致节点信号为零，因此添加`Dropout`层在激活函数之前或之后无关紧要：
- en: '[PRE23]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Dropout is added to prevent overfitting.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加dropout是为了防止过拟合。
- en: Summary
  id: totrans-229
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The neural network’s input and input layer are not the same thing and do not
    need to be the same size. The input is the features of the samples, while the
    input layer is the first layer of weights and biases to learn to predict the corresponding
    label.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络的输入和输入层不是同一回事，也不需要相同的大小。输入是样本的特征，而输入层是学习预测相应标签的第一个权重和偏差层。
- en: A deep neural network has one or more layers between the input and output layers,
    which are called hidden layers. Using a programming function as an analogy, the
    input layer is the parameters to the function, the output layer is the return
    value from the function, and the hidden layers are the code within the function
    that transform the input parameters to an output return value.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络在输入层和输出层之间有一个或多个层，这些层被称为隐藏层。用编程函数作类比，输入层是函数的参数，输出层是函数的返回值，而隐藏层是函数内部的代码，它将输入参数转换为输出返回值。
- en: Neural networks are directed acyclic graphs, in that data is fed forward from
    the input layer to the output layer.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是有向无环图，数据从输入层正向传递到输出层。
- en: Activations, like ReLU and softmax, squash the output signals of layers, which
    researchers found help models learn better.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活函数，如ReLU和softmax，压缩了层的输出信号，研究人员发现这有助于模型更好地学习。
- en: The role of the optimizer is to update weights from the current batch loss so
    that the loss on subsequent batches gets smaller.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化器的作用是更新权重，从当前批次的损失中，以便后续批次的损失更小。
- en: A regressor uses a linear activation to predict a continuous real value, like
    predicting the sale price of a house.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归器使用线性激活函数来预测连续的实数值，例如预测房屋的销售价格。
- en: 'A binary classifier uses a sigmoid activation to predict a binary state: true/false,
    1/0, yes/no.'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二元分类器使用sigmoid激活函数来预测二元状态：真/假，1/0，是/否。
- en: A multiclass classifier uses a softmax activation to predict a class from a
    set of classes, like predicting the age demographic of a person.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多类分类器使用softmax激活函数从一组类别中预测一个类别，例如预测一个人的年龄人口统计。
- en: The sequential API is easy to get started but limited in that it does not support
    branching in the model.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 顺序型API易于入门，但它的局限性在于不支持模型中的分支。
- en: The functional API is preferred over the sequential API for production models.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 功能型API比顺序型API更适合用于生产模型。
- en: Overfitting occurs when the model memorizes training samples during training,
    which prevents the model from generalizing to samples it was not trained on. Regularization
    methods inject small amounts of random noise during training, which has shown
    to be effective in preventing memorization.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当模型在训练过程中记住训练样本时，会发生过拟合，这阻止了模型泛化到它未训练过的样本。正则化方法在训练过程中注入少量随机噪声，这已被证明在防止记忆化方面是有效的。
