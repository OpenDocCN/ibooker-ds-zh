- en: 7 More experiments with the trained model
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 使用训练模型的更多实验
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Validating whether removing bad values improves the model performance
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证移除不良值是否可以提高模型性能
- en: Validating whether embeddings for the categorical columns improve the model
    performance
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证分类列的嵌入是否可以提高模型性能
- en: Possible approaches to improving the performance of the model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提高模型性能的可能方法
- en: Comparing the performance of the deep learning model with a non-deep-learning
    model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较深度学习模型与非深度学习模型的性能
- en: 'In chapter 6, we trained the deep learning model and did a series of experiments
    to measure and improve its performance. In this chapter, we will go through a
    set of additional experiments to validate two key aspects of the model: removing
    bad values (a step that we took as part of the data preparation described in chapters
    3 and 4) and including embeddings for the categorical columns (as described in
    chapter 5). Then we will describe an experiment to compare the deep learning solution
    using the streetcar delay prediction deep learning model with a solution that
    uses a non-deep-learning approach called XGBoost.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们训练了深度学习模型并做了一系列实验来衡量和改进其性能。在本章中，我们将进行一系列额外的实验来验证模型的两个关键方面：移除不良值（这是我们作为第3章和第4章中描述的数据准备的一部分所采取的步骤）以及包括分类列的嵌入（如第5章所述）。然后我们将描述一个实验，比较使用电车延误预测深度学习模型的深度学习解决方案与使用称为XGBoost的非深度学习方法的解决方案。
- en: 7.1 Code for more experiments with the model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 模型更多实验的代码
- en: When you have cloned the GitHub repo ([http://mng.bz/v95x](http://mng.bz/v95x))
    associated with this book, you’ll find the code related to the experiments in
    the notebooks subdirectory. The following listing shows the files used in the
    experiments described in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当你克隆了与本书相关的GitHub仓库([http://mng.bz/v95x](http://mng.bz/v95x))时，你将在notebooks子目录中找到与实验相关的代码。以下列表显示了本章中描述的实验所使用的文件。
- en: Listing 7.1 Code in the repo related to the model training experiments
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.1 存储库中与模型训练实验相关的代码
- en: '[PRE0]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Directory for pickled intermediate datasets
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 存储中间数据集的目录
- en: ❷ Directory for saving trained models
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存训练模型的目录
- en: ❸ Contains definitions of pipeline classes
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 包含管道类定义
- en: ❹ Notebook containing dataset refactoring and deep learning model training code
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 包含数据集重构和深度学习模型训练代码的笔记本
- en: ❺ Notebook containing dataset refactoring and XGBoost model training code
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 包含数据集重构和XGBoost模型训练代码的笔记本
- en: '❻ Config file for model training: definitions of hyperparameter values, train/validate/test
    proportions, and other configuration parameters. Note that we use a common config
    file for training both the deep learning model and the XGBoost model.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 模型训练配置文件：超参数值的定义、训练/验证/测试比例以及其他配置参数。请注意，我们使用一个通用的配置文件来训练深度学习模型和XGBoost模型。
- en: ❼ Directory for saved pipelines
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 保存管道的目录
- en: 7.2 Validating whether removing bad values improves the model
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 验证移除不良值是否可以提高模型
- en: 'Back in chapter 4, we reviewed the number of records in the dataset with bad
    values—records with a value in one of the columns that is not valid. The input
    record may have had a route value that doesn’t exist, for example, or a direction
    value that isn’t one of the compass points. By default, we remove these values
    before saving the output dataframe at the end of the streetcar_data_preparation
    notebook. We want to do an experiment to validate that this choice is the best
    one to make for the performance of the model. Following is such an experiment:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 回到第4章，我们回顾了数据集中不良值记录的数量——那些在一列中的值无效的记录。输入记录可能有一个不存在的路线值，例如，或者一个不是罗盘方向的值。默认情况下，我们在streetcar_data_preparation笔记本的末尾保存输出数据框之前移除这些值。我们想要进行一个实验来验证这个选择对于模型性能来说是最好的选择。以下是这样的一个实验：
- en: Rerun the streetcar_data_preparation notebook with the following values set
    in streetcar_data_preparation_config.yml to save a cleaned output dataframe that
    includes records with bad values, as shown in the next listing.
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下值设置streetcar_data_preparation_config.yml重新运行streetcar_data_preparation笔记本，以保存包含不良值记录的清洁输出数据框，如下一列表所示。
- en: Listing 7.2 Parameters in the data preparation config for bad values experiment
  id: totrans-21
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表7.2 数据准备配置中不良值实验的参数
- en: '[PRE1]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Specify that the output dataframe should be saved.
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 指定输出数据框应保存。
- en: ❷ Specify that the bad values should not be removed from the output dataframe.
  id: totrans-24
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 指定不良值不应从输出数据框中移除。
- en: ❸ Set a unique filename for the output dataframe.
  id: totrans-25
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❸ 为输出数据框设置一个唯一的文件名。
- en: Rerun the streetcar_model_training notebook with the following values set in
    streetcar_model_training_config.yml to use a control file for refactoring the
    dataset that includes route/direction combinations with “bad route” and “bad direction,”
    as the next listing shows.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用以下值在streetcar_model_training_config.yml中重新运行streetcar_model_training笔记本，以使用控制文件重构数据集，该数据集包括带有“坏路线”和“坏方向”组合的路线/方向，如下所示。
- en: Listing 7.3 Parameter settings for the bad values experiment
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 列表7.3 恶值实验的参数设置
- en: '[PRE2]'
  id: totrans-28
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Specify the same filename that you specified for the pickled_output_dataframe
    in the data preparation config file.
  id: totrans-29
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❶ 在数据准备配置文件中指定与pickled_output_dataframe相同的文件名。
- en: ❷ Control file that includes route/direction combinations with “bad route” and
    “bad direction”.
  id: totrans-30
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: ❷ 包含“坏路线”和“坏方向”组合的控制文件。
- en: Now when we run experiment 5 from chapter 6 with these changes (to use the input
    dataset that includes bad values), we get the result shown in figure 7.1\. Validation
    accuracy is not that different, but the model trained with an input dataset that
    includes bad values has much worse recall.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用这些更改（使用包含坏值的输入数据集）从第6章运行实验5，我们得到图7.1中所示的结果。验证准确率没有太大差异，但使用包含坏值输入数据集训练的模型的召回率要差得多。
- en: '![CH07_F01_Ryan](../Images/CH07_F01_Ryan.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F01_Ryan](../Images/CH07_F01_Ryan.png)'
- en: Figure 7.1 Comparison of model performance with and without bad values in the
    training dataset
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 比较训练数据集中有无坏值时的模型性能
- en: Overall, we get better performance from the model trained with a dataset that
    excludes bad values. This experiment confirms our decision in chapter 4 to exclude
    by default records with bad values from the model training process.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，我们从排除坏值的训练数据集训练的模型中获得了更好的性能。这个实验证实了我们在第4章中决定默认排除模型训练过程中的坏值记录的决定。
- en: 7.3 Validating whether embeddings for columns improve the performance of the
    model
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 验证列嵌入是否提高了模型的性能
- en: Embeddings play an important role in the performance of the deep learning model
    we created in chapter 5 and trained in chapter 6\. The model incorporates embedding
    layers for all the categorical columns. As an experiment, I removed only these
    layers and trained the model to compare its performance with and without embedding
    layers for the categorical columns.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入在我们的第5章中创建并在第6章中训练的深度学习模型中起着重要作用。该模型为所有分类列包含嵌入层。作为一个实验，我只移除了这些层，并训练了模型，以比较其具有和没有分类列嵌入层的性能。
- en: To perform this experiment, I replaced these two lines in the model building
    section in the streetcar_model_training notebook
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行这个实验，我在streetcar_model_training笔记本中的模型构建部分替换了这两行
- en: '[PRE3]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: with the line
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与以下行
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: and reran experiment 5, described in chapter 6\. This experiment is a 50-epoch
    training run with early stopping defined based on validation accuracy. Figure
    7.2 shows the results of this experiment run with and without embeddings for categorical
    columns.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 并重新运行了第6章中描述的实验5。这个实验是一个50个epoch的训练运行，基于验证准确率定义了提前停止。图7.2显示了带有和不带有分类列嵌入的实验运行结果。
- en: '![CH07_F02_Ryan](../Images/CH07_F02_Ryan.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F02_Ryan](../Images/CH07_F02_Ryan.png)'
- en: Figure 7.2 Comparing performance for the model with categorical columns without
    and with embedding layers
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 比较具有和没有嵌入层的分类列的模型性能
- en: Every performance measure is much worse when the embedding layers are removed
    from the model for categorical columns. This example demonstrates the value of
    embeddings even in a simple deep learning model like the one we have defined for
    streetcar delay.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当从模型中移除分类列的嵌入层时，每个性能指标都变得非常糟糕。这个例子展示了嵌入在像我们为电车延迟定义的简单深度学习模型中的价值。
- en: 7.4 Comparing the deep learning model with XGBoost
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 比较深度学习模型与XGBoost
- en: The thesis of this book is that it’s worth considering deep learning to be an
    option for doing machine learning on structured, tabular data. In chapter 6, we
    trained a deep learning model on the streetcar delay dataset and examined the
    model’s performance. What if we were to train a non-deep-learning model with the
    same streetcar delay dataset? In this section, we are going to show the results
    for such an experiment. We will replace the deep learning model with XGBoost,
    a gradient-boosting decision-tree algorithm that has gained a reputation as the
    “go to” machine learning approach for problems involving structured, tabular data.
    We will compare the results for both models and determine what these results tell
    us about the viability of deep learning as a solution for problems involving structured
    data.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主张是，值得考虑将深度学习视为在结构化、表格数据上执行机器学习的一个选项。在第6章中，我们在streetcar delay数据集上训练了一个深度学习模型，并检查了模型的表现。如果我们用同样的streetcar
    delay数据集训练一个非深度学习模型会怎样呢？在本节中，我们将展示这种实验的结果。我们将用XGBoost替换深度学习模型，XGBoost是一种在处理涉及结构化、表格数据的机器学习问题中赢得“首选”机器学习方法的声誉的梯度提升决策树算法。我们将比较这两个模型的结果，并确定这些结果告诉我们关于深度学习作为解决涉及结构化数据问题的解决方案的可行性的什么信息。
- en: In the same way that a book about Batman would not be complete without describing
    the Joker, a book about using deep learning with structured data would be incomplete
    if it didn’t say something about XGBoost. In terms of dealing with structured,
    tabular data, XGBoost is the archnemesis of deep learning, and it is the approach
    most often recommended instead of deep learning for dealing with structured data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 就像一本关于蝙蝠侠的书如果没有描述小丑就不会完整一样，一本关于使用结构化数据深度学习的书如果没有提到XGBoost也会显得不完整。在处理结构化、表格数据方面，XGBoost是深度学习的宿敌，并且它是最常被推荐用来处理结构化数据的替代深度学习的方法。
- en: XGBoost is an example of a kind of non-deep-learning machine learning called
    a gradient-boosting machine. In gradient boosting, predictions from a set of simple
    models are aggregated to get a consolidated prediction. It is worth noting that
    XGBoost offers you a set of capabilities that is not identical to those offered
    by deep learning models. XGBoost has a built-in feature, importance capability
    ([http://mng.bz/awwJ](http://mng.bz/awwJ)), that can help you determine how much
    each feature contributes to the model, although this capability should be used
    judiciously, as demonstrated by the article at [http:// mng.bz/5pa8](http://mng.bz/5pa8).
    A detailed description of all the features of XGBoost is beyond the scope of this
    book, but the XGBoost section in *Machine Learning for Business (* [http://mng.bz/
    EEGo](http://mng.bz/EEGo) *)* provides an excellent and accessible description
    of how XGBoost works.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost是一种非深度学习机器学习，称为梯度提升机。在梯度提升中，从一组简单模型中聚合预测以获得一个综合预测。值得注意的是，XGBoost提供了一套与深度学习模型不完全相同的特性。XGBoost内置了一个特征重要性功能（[http://mng.bz/awwJ](http://mng.bz/awwJ)），可以帮助你确定每个特征对模型贡献的大小，尽管这个功能应该谨慎使用，正如[http://
    mng.bz/5pa8](http://mng.bz/5pa8)上的文章所展示的。XGBoost的所有特性的详细描述超出了本书的范围，但*《商业机器学习》（Machine
    Learning for Business）*中的XGBoost部分（[http://mng.bz/EEGo](http://mng.bz/EEGo)）提供了一个优秀且易于理解的XGBoost工作原理的描述。
- en: To get a comparison between the deep learning model and XGBoost, I updated the
    model training notebook streetcar_model_training to replace the deep learning
    model with XGBoost. My intention was to make minimal changes to the code. If you
    think of the entire model training notebook as a car, I wanted to swap out the
    existing engine (the deep learning model) and put in another engine (XGBoost)
    without changing the body panels, wheels, tires, interior, or anything else about
    the car, as shown in figure 7.3.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较深度学习模型和XGBoost之间的差异，我更新了模型训练笔记本streetcar_model_training，将深度学习模型替换为XGBoost。我的意图是尽量减少对代码的修改。如果你把整个模型训练笔记本比作一辆车，我希望在不改变车身面板、车轮、轮胎、内饰或其他任何车辆部件的情况下，更换现有的引擎（深度学习模型）并安装另一个引擎（XGBoost），如图7.3所示。
- en: '![CH07_F03_Ryan](../Images/CH07_F03_Ryan.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F03_Ryan](../Images/CH07_F03_Ryan.png)'
- en: Figure 7.3 Replacing the deep learning engine with the XGBoost engine
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 替换深度学习引擎为XGBoost引擎
- en: When I had the new engine working, I wanted to take the car out on the track
    to assess what it was like to drive compared with the same car with its original
    engine. If I keep everything else on the car the same and change only the engine,
    I can expect to get a fair comparison of what it is like to drive the car with
    each engine. Similarly, I hoped that by keeping code changes in the notebook to
    a minimum, I would be able to get a fair comparison between the deep learning
    model and XGBoost.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当我让新引擎工作后，我想把车开到赛道上，评估与原引擎相同的车的驾驶感受。如果我在车上保持其他一切不变，只更换引擎，我可以期望得到一个公平的比较，比较两种引擎的驾驶感受。同样，我希望通过将笔记本中的代码更改保持在最低限度，我能够得到深度学习模型和
    XGBoost 之间的公平比较。
- en: 'You can find the code for training the XGBoost model in the streetcar_model
    _training_xgb notebook, and if you examine this notebook, you will see that the
    car analogy holds: I changed engines, but the rest of the car remains the same.The
    first part of this notebook is identical to the deep learning model training notebook
    streetcar _model_training, with the exception of including the import statement
    for the XGBoost model:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在 streetcar_model_training_xgb 笔记本中找到训练 XGBoost 模型的代码，如果您检查这个笔记本，您会发现汽车类比是成立的：我更换了引擎，但汽车的其他部分保持不变。这个笔记本的前部分与深度学习模型训练笔记本
    streetcar_model_training 相同，除了包含 XGBoost 模型的导入语句：
- en: '[PRE5]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The XGBoost-specific content begins after the master block to invoke the pipeline.
    At this point, the dataset is a list of numpy arrays, with one numpy array for
    each column in the dataset:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在调用管道的主块之后开始 XGBoost 特定的内容。此时，数据集是一个 numpy 数组的列表，每个数据集的列对应一个 numpy 数组：
- en: '[PRE6]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The multi-input Keras model in the deep learning training code expects this
    format. XGBoost, however, expects the dataset to be a numpy array of lists, so
    before we can train an XGBoost model with this data, we need to convert it to
    the format that XGBoost expects. We begin by converting the train and test datasets
    from lists of numpy arrays to lists of lists, as shown in the following listing.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习训练代码中的多输入 Keras 模型期望这种格式。然而，XGBoost 期望数据集是一个列表的 numpy 数组，因此在我们能够使用这些数据训练
    XGBoost 模型之前，我们需要将其转换为 XGBoost 所期望的格式。我们首先将训练和测试数据集从 numpy 数组的列表转换为列表的列表，如下所示。
- en: Listing 7.4 Code to convert the train and test datasets to lists of lists
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 将训练和测试数据集转换为列表的列表的代码
- en: '[PRE7]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ For both the train and test datasets, iterate through the numpy arrays, and
    convert them to lists so we end up with two lists of lists.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于训练和测试数据集，遍历 numpy 数组，并将它们转换为列表，最终得到两个列表的列表。
- en: 'Next, for the test and train datasets, we convert the lists to numpy arrays
    and transpose the numpy arrays:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于测试和训练数据集，我们将列表转换为 numpy 数组，并转置 numpy 数组：
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Here is what the resulting training dataset `xgb_X_train` looks like:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是生成的训练数据集 `xgb_X_train` 的样子：
- en: '[PRE9]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The dataset that came out of the pipelines as a list of numpy arrays has been
    converted to a numpy array of lists with the content transposed—exactly what we
    need to train the XGBoost model in the next block, as shown in the next listing.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 作为列表的 numpy 数组从管道中输出的数据集已经被转换成列表的 numpy 数组，并且内容已经转置——这正是我们在下一个块中训练 XGBoost 模型所需要的，如下所示。
- en: Listing 7.5 Code to train an XGBoost model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 训练 XGBoost 模型的代码
- en: '[PRE10]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Build the path where the trained XGBoost model will be saved.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建训练好的 XGBoost 模型将保存的路径。
- en: ❷ Define the XGB model object, using defaults for all parameters except scale_pos_weight,
    which is used to account for the imbalance between positive (delay) and negative
    (no delay) targets. This value is identical to the one used in the deep learning
    model to account for the imbalance.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义 XGB 模型对象，使用所有参数的默认值，除了 scale_pos_weight，该参数用于解决正（延迟）和负（无延迟）目标之间的不平衡。此值与用于解决深度学习模型不平衡的值相同。
- en: ❸ Fit the model, using the training dataset that we transformed into a numpy
    array of lists.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用我们转换成列表的 numpy 数组的训练数据集来拟合模型。
- en: ❹ Save the trained model.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 保存训练好的模型。
- en: ❺ Apply the trained model to the test dataset.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将训练好的模型应用于测试数据集。
- en: ❻ Calculate the accuracy of the model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算模型的准确率。
- en: Now that we have seen what changes needed to be made to get the model training
    notebook to work with XGBoost, what happened when we trained and assessed the
    XGBoost model? Figure 7.4 summarizes both the results of comparative training
    and evaluation runs with XGBoost and deep learning, as well as high-level differences
    between the two approaches.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了为了让模型训练笔记本与XGBoost一起工作需要做出哪些改变，那么当我们训练和评估XGBoost模型时发生了什么？图7.4总结了使用XGBoost和深度学习进行的比较训练和评估运行的结果，以及两种方法之间的高级差异。
- en: '![CH07_F04_Ryan](../Images/CH07_F04_Ryan.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![CH07_F04_Ryan](../Images/CH07_F04_Ryan.png)'
- en: Figure 7.4 Comparison of XGBoost and Keras deep learning models
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 XGBoost与Keras深度学习模型比较
- en: '*Performance* —Right out of the box, with no tuning, the XGBoost model got
    better performance than deep learning. For accuracy on the test set, the high-water
    mark was 78.1% for deep learning and 80.1% for XGBoost. For recall and volume
    of false negatives (as we stated in chapter 6, this factor is key to the performance
    of the model in terms of the user’s final experience), XGBoost was also better.
    Compare the confusion matrix for XGBoost in figure 7.5 with the confusion matrix
    in figure 7.6 for a high-accuracy deep learning run, and it’s clear that XGBoost
    is ahead.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*性能* —XGBoost模型在没有任何调整的情况下，性能就优于深度学习。在测试集的准确性方面，深度学习的最高记录是78.1%，而XGBoost是80.1%。在召回率和假阴性数量（如我们在第6章中所述，这个因素是用户最终体验中模型性能的关键）方面，XGBoost也表现更好。将图7.5中XGBoost的混淆矩阵与图7.6中高精度深度学习运行的混淆矩阵进行比较，可以看出XGBoost领先。'
- en: '![CH07_F05_Ryan](../Images/CH07_F05_Ryan.png)'
  id: totrans-78
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH07_F05_Ryan](../Images/CH07_F05_Ryan.png)'
- en: Figure 7.5 XGBoost confusion matrix
  id: totrans-79
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.5 XGBoost混淆矩阵
- en: '![CH07_F06_Ryan](../Images/CH07_F06_Ryan.png)'
  id: totrans-80
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![CH07_F06_Ryan](../Images/CH07_F06_Ryan.png)'
- en: Figure 7.6 Deep learning confusion matrix
  id: totrans-81
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图7.6 深度学习混淆矩阵
- en: '*Training time* —The deep learning model training time is more hardware-dependent
    than the training time for XGBoost. On a mediocre Windows system, XGBoost takes
    in the range of 1.5 minutes to train, and the deep learning model takes in the
    range of 3 minutes to run experiment 5\. But deep learning training time for experiment
    5 (50 epochs, with early-stopping patience parameter set at 15) varies widely
    depending on the patience parameter (how many epochs are run in training when
    the optimized performance measurement, such as validation accuracy, stops improving
    before the training run is stopped) and the hardware available in the environment.
    Although XGBoost has shorter training time, the gap is close enough, and the training
    performance of the deep learning model is variable enough, that I would call the
    comparison a draw.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*训练时间* —深度学习模型的训练时间比XGBoost的训练时间更依赖于硬件。在一个普通的Windows系统上，XGBoost的训练时间大约为1.5分钟，而深度学习模型运行实验5的时间大约为3分钟。但深度学习实验5的训练时间（50个epoch，提前停止的耐心参数设置为15）会根据耐心参数（在训练运行停止之前，优化性能测量，如验证准确性，停止改进时运行的epoch数量）和环境中的硬件而大幅变化。尽管XGBoost有更短的训练时间，但差距足够小，深度学习模型的训练性能变化足够大，因此我会将这种比较称为平局。'
- en: '*Code complexity* —There is little difference in terms of complexity between
    the fitting code of the deep learning model and XGBoost. The code leading up to
    the fitting statements is different. Before the fitting code, the deep learning
    model has complex code in the `get_model()` function to construct the model itself.
    As described in chapter 5, this function assembles different layers for different
    types of input columns. XGBoost does not need this complex code block, but it
    requires additional steps to convert the dataset from a list of numpy arrays (the
    format required by the deep learning model) to a numpy array of lists (the format
    required by XGBoost). I would call this category a draw as well, although an argument
    could be made that XGBoost is simpler because the additional data preparation
    code it requires is simpler than the model building code. But model building code
    is a key part of deep learning’s rating in the last category.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码复杂度* —在复杂度方面，深度学习模型的拟合代码与XGBoost之间几乎没有区别。在拟合语句之前的代码有所不同。在拟合代码之前，深度学习模型在`get_model()`函数中有复杂的代码来构建模型本身。如第5章所述，该函数为不同类型的输入列组装不同的层。XGBoost不需要这个复杂的代码块，但它需要额外的步骤将数据集从深度学习模型所需的numpy数组列表格式转换为XGBoost所需的numpy数组列表格式。我也可以将这一类别称为平局，尽管可以争论说XGBoost更简单，因为其所需的数据准备代码比模型构建代码简单。但模型构建代码是深度学习在最后一个类别中的评分的关键部分。'
- en: '*Flexibility* —As described in chapter 5, the deep learning model is built
    to work with a wide variety of structured datasets. Because of the way that XGBoost
    was retrofitted into the code, it benefits from this flexibility, and its implementation
    in the streetcar_model_training_xgb notebook will also work with a variety of
    structured datasets. There is one important exception: the deep learning model
    would work with datasets that include freeform text columns. As discussed in chapter
    4, the streetcar delay dataset does not have any such columns, but they are common
    in many structured datasets.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*灵活性* —— 如第5章所述，深度学习模型被构建来与各种结构化数据集一起工作。由于 XGBoost 被重新整合到代码中，它受益于这种灵活性，并且其在
    streetcar_model_training_xgb 笔记本中的实现也将与各种结构化数据集一起工作。有一个重要的例外：深度学习模型将能够处理包含自由文本列的数据集。如第4章所述，街车延误数据集没有这样的列，但它们在许多结构化数据集中很常见。'
- en: Consider, for example, a table that tracked items for sale in an online footwear
    retail site. This table could include continuous columns (like price) and categorical
    columns (like color and shoe size). It could also include a description of each
    item in freeform text. The deep learning model would be able to incorporate and
    get training data from such a text column. XGBoost would need to exclude this
    column from its analysis. In this important respect, the deep learning model is
    more flexible than XGBoost.
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 例如，考虑一个追踪在线鞋类零售网站销售物品的表格。这个表格可以包括连续列（如价格）和分类列（如颜色和鞋码）。它还可以包括每个物品的自由文本描述。深度学习模型能够整合并从这样的文本列中获取训练数据。XGBoost
    需要从其分析中排除这个列。在这个重要的方面，深度学习模型比 XGBoost 更灵活。
- en: It’s worth noting that I was able to adapt the code in streetcar_model_training
    to work with XGBoost with minimal extra work. When I began training the XGBoost
    model, I found that with no tuning other than setting the `scale_pos_weight` parameter
    to account for the much higher number of “no delay” records than “delay” records
    in the input dataset, the XGBoost model consistently outperformed the deep learning
    model.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，我能够以最小的额外工作将 streetcar_model_training 中的代码调整为与 XGBoost 一起工作。当我开始训练 XGBoost
    模型时，我发现，除了将 `scale_pos_weight` 参数设置为考虑到输入数据集中“无延误”记录比“延误”记录多得多的数量之外，无需其他调整，XGBoost
    模型始终优于深度学习模型。
- en: What’s the ultimate conclusion of comparing deep learning and XGBoost for the
    streetcar delay prediction problem? XGBoost has better performance than the deep
    learning model, and it was relatively easy to integrate into the existing code
    structure that I created for the deep learning model. Returning to the car analogy,
    XGBoost was an easy engine to fit into the car and get running. Does that mean
    that the conclusion of the comparison has to align with the conventional wisdom
    that non-deep-learning methods—in particular, XGBoost—are better than deep learning
    for structured data problems?
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 比较深度学习和 XGBoost 在街车延误预测问题上的最终结论是什么？XGBoost 的性能优于深度学习模型，并且它相对容易集成到我为深度学习模型创建的现有代码结构中。回到汽车的比喻，XGBoost
    是一个容易安装到汽车中并使其运行的引擎。这意味着比较的结论必须与传统的智慧一致，即非深度学习方法——特别是 XGBoost——对于结构化数据问题比深度学习更好吗？
- en: If we freeze the streetcar delay prediction problem where it is now, the answer
    probably is yes, but it would be naïve to expect that in a real-world situation,
    the model would not change. As you will see in chapter 9, many options are available
    to extend and improve the streetcar delay prediction model by considering additional
    datasets, and as soon as we consider these improvements, we run into the limitations
    of XGBoost. Any dataset that includes freeform text columns (such as the weather
    dataset described in chapter 9), for example, could be incorporated easily into
    the deep learning model but could not be incorporated directly into XGBoost. XGBoost
    would not be suitable for some of the extensions to the streetcar delay prediction
    problem described in chapter 9; neither would it be suitable for any problem in
    which the structured data includes columns with any kind of BLOB data ([https://techterms
    .com/definition/blob](https://techterms.com/definition/blob)), such as images,
    video, or audio. If we want a solution that is generally applicable to a truly
    wide range of tabular, structured data, XGBoost isn’t going to be the solution.
    Although XGBoost can beat deep learning for performance on a given application,
    deep learning has the flexibility to exploit the full variety of structured data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将电车延误预测问题冻结在当前状态，答案可能确实是肯定的，但在现实世界情况下，期望模型不会发生变化则过于天真。正如你在第9章中将会看到的，有许多选项可以通过考虑额外的数据集来扩展和改进电车延误预测模型，一旦我们考虑这些改进，我们就会遇到XGBoost的限制。例如，任何包含自由文本列（如第9章中描述的天气数据集）的数据集都很容易被整合到深度学习模型中，但无法直接整合到XGBoost中。XGBoost可能不适合第9章中描述的电车延误预测问题的某些扩展；它也不适合任何结构化数据中包含任何类型的BLOB数据（[https://techterms.com/definition/blob](https://techterms.com/definition/blob)）的问题，例如图像、视频或音频。如果我们想要一个适用于真正广泛范围表格化、结构化数据的通用解决方案，XGBoost可能不是正确答案。尽管XGBoost在特定应用上的性能可以击败深度学习，但深度学习具有利用结构化数据全范围的灵活性。
- en: 7.5 Possible next steps for improving the deep learning model
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 改进深度学习模型的可能下一步
- en: 'After all the experiments described in chapter 6, we end up with a trained
    model that is a little better than 78% accurate on the test set and has a recall
    of 0.79\. What steps could we take to improve the performance on subsequent iterations?
    Here are some ideas:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中描述的所有实验之后，我们最终得到一个训练好的模型，在测试集上的准确率略高于78%，召回率为0.79。为了在后续迭代中提高性能，我们可以采取哪些步骤？以下是一些想法：
- en: '*Adjust the mix of features.* The set of features used in the deep learning
    model is relatively limited to keep the training process as simple as possible.
    One feature that could be added is a geospatial measurement based on the latitude
    and longitude values generated from the addresses in chapter 4\. As described
    in chapter 9, you could use bounding boxes for each route to divide each route
    into lateral segments (for example, 10 sections per route) and use these segments
    as one of the features of the model. Such an approach would isolate delays to
    specific subsets of each route and could improve the performance of the model.'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调整特征组合*。在深度学习模型中使用的特征集相对有限，以尽可能简化训练过程。可以添加的一个特征是基于第4章中地址生成的纬度和经度值的地理空间测量。正如第9章所述，你可以为每条路线使用边界框将其划分为横向段（例如，每条路线10个部分），并将这些段作为模型的一个特征。这种方法可以将延误隔离到每条路线的特定子集中，并可能提高模型的表现。'
- en: '*Adjust the threshold for delay* *.* If the threshold for what constitutes
    a delay is too small, the model will be less useful, because transitory delays
    will count as incidents. On the other hand, if the threshold is set too high,
    the model’s predictions would have less value, because delays that would constitute
    an inconvenience for travelers would not be captured. This boundary is set in
    the model by the `targetthresh` parameter in the model training config file. Adjusting
    this threshold could provide improvements in the model’s performance, particularly
    as the input data evolves over time. As noted in chapter 3, over the years there
    has been an overall trend for delays to be more frequent but shorter, so a smaller
    value for the `targetthresh` parameter could be worth exploring.'
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调整延误的阈值* *.* 如果构成延误的阈值太小，模型将不太有用，因为短暂的延误将被计为事件。另一方面，如果阈值设置得太高，模型的预测将失去价值，因为对旅客构成不便的延误将不会被捕捉到。这个边界在模型训练配置文件中的
    `targetthresh` 参数中设置。调整这个阈值可能会提高模型性能，尤其是在输入数据随时间演变的情况下。如第3章所述，多年来，延误的整体趋势是更加频繁但时间更短，因此，`targetthresh`
    参数的较小值值得探索。'
- en: '*Adjust the learning rate* *.* The learning rate controls how much the weights
    are adjusted in each training iteration. If it’s set too high, the training process
    could skip right over a minimum loss point. If it’s set too low, training progress
    will be slow, and you’ll consume more time and system resources than you need
    to train the model. In the early stages of the model training process, I adjusted
    the learning rate and settled on the value that is currently in the model training
    config file because it produced stable results. Further experiments to fine-tune
    the learning rate could yield improvements in the model’s performance.'
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调整学习率* *.* 学习率控制每次训练迭代中权重调整的程度。如果设置得太高，训练过程可能会跳过最小损失点。如果设置得太低，训练进度会变慢，你将消耗比训练模型所需更多的时间和系统资源。在模型训练过程的早期阶段，我调整了学习率，并确定了目前模型训练配置文件中的值，因为它产生了稳定的结果。进一步调整学习率的实验可能会提高模型性能。'
- en: The model meets the performance goal we set for it at the outset (at least 70%
    accuracy), but there is always room for improvement. The preceding list shows
    some of the potential tweaks that could be made to the deep learning model to
    improve its performance. If you have worked through the code to this point, I
    encourage you to try some of the recommendations in this section and run experiments
    to see whether these tweaks improve the performance of the model.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 模型达到了我们最初为其设定的性能目标（至少70%的准确率），但总有改进的空间。上述列表显示了一些可以调整深度学习模型以改进其性能的潜在调整。如果你已经完成了到这一点的代码，我鼓励你尝试本节中的建议，并运行实验以查看这些调整是否提高了模型的性能。
- en: Summary
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: By default, we discard records with bad values from the dataset. You can perform
    an experiment to validate this choice and demonstrate that a model trained on
    a dataset with bad values removed has superior performance to a model trained
    on a dataset that keeps the bad values.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，我们从数据集中丢弃具有不良值的记录。你可以进行一个实验来验证这个选择，并证明在移除不良值后训练的模型比保留不良值的模型具有更好的性能。
- en: By default, the model incorporates embeddings for categorical columns. You can
    perform an experiment to confirm and show that a model that incorporates embeddings
    for categorical columns has better performance than a model that does not have
    embeddings for categorical columns.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，模型包含分类列的嵌入。你可以进行一个实验来确认并展示包含分类列嵌入的模型比没有分类列嵌入的模型具有更好的性能。
- en: XGBoost is currently the default machine learning approach for problems involving
    tabular, structured data. You can perform a series of experiments on a version
    of the model training notebook in which the deep learning model has been replaced
    with XGBoost.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 目前是处理涉及表格化、结构化数据的机器学习问题的默认方法。你可以在一个模型训练笔记本的版本上执行一系列实验，其中深度学习模型已被 XGBoost
    替换。
- en: XGBoost does have somewhat better performance than deep learning on the streetcar
    delay prediction problem, but performance isn’t the only consideration, and deep
    learning beats XGBoost in the key area of flexibility.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost 在街车延误预测问题上确实比深度学习有更好的性能，但性能并不是唯一要考虑的因素，在灵活性这一关键领域，深度学习胜过 XGBoost。
