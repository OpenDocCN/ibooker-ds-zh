- en: 3 Convolutional and residual neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 卷积和残差神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Understanding the structure of convolutional neural networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解卷积神经网络的结构
- en: Constructing a ConvNet model
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建ConvNet模型
- en: Designing and constructing a VGG model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和构建VGG模型
- en: Designing and constructing a residual network model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设计和构建残差网络模型
- en: Chapter 2 introduced the fundamentals behind deep neural networks (DNNs), a
    network architecture based on dense layers. We also demonstrated how to make a
    simple image classifier using dense layers, and discussed the limitations when
    attempting to scale a DNN to larger sizes of images. The introduction of neural
    networks using convolutional layers for feature extraction and learning, known
    as *convolutional neural networks* (*CNNs*), made it possible to scale image classifiers
    for practical applications.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第二章介绍了深度神经网络（DNN）背后的基本原理，这是一种基于密集层的网络架构。我们还演示了如何使用密集层制作一个简单的图像分类器，并讨论了尝试将DNN扩展到更大图像尺寸时的局限性。使用卷积层进行特征提取和学习的神经网络，称为“卷积神经网络”（*CNN*），使得将图像分类器扩展到实际应用成为可能。
- en: 'This chapter covers the design patterns, and evolution in the design patterns,
    for early SOTA convolutional neural networks. We cover three design patterns in
    this chapter, in sequence of their evolution:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了早期SOTA卷积神经网络的架构设计模式和设计模式的发展。本章按其演变的顺序，介绍了三个设计模式：
- en: ConvNet
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ConvNet
- en: VGG
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG
- en: Residual networks
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差网络
- en: Each of these design patterns made contributions toward today’s modern CNN designs.
    ConvNet, with AlexNet as an early example, introduced a pattern of alternating
    feature extraction and dimensionality reduction through pooling, and sequentially
    increasing the number of filters as we went deeper in layers. VGG introduced grouping
    convolutions into blocks of one or more convolutions and delaying dimensionality
    reduction with pooling until the end of the block. Residual networks introduced
    further grouping blocks into groups, delaying dimensionality reduction to the
    end of the group, and using feature pooling as well as pooling for the reduction,
    as well as the concept of branched paths—the identity link—for feature reuse between
    blocks.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设计模式中的每一个都对当今现代CNN设计做出了贡献。ConvNet，以AlexNet作为早期例子，引入了通过池化交替进行特征提取和降维的模式，并随着层深度的增加而逐次增加滤波器的数量。VGG将卷积分组到由一个或多个卷积组成的块中，并在块的末尾延迟降维操作。残差网络进一步将分组块分组，将降维延迟到组末，并使用特征池化以及池化进行降维，以及分支路径——即恒等链接——的概念，用于块之间的特征重用。
- en: 3.1 Convolutional neural networks
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 卷积神经网络
- en: Early convolutional neural networks are a type of neural network that can be
    viewed as consisting of two parts, a frontend and a backend. The backend is a
    DNN, which we have already covered. The name *convolutional neural network* comes
    from the frontend, referred to as a *convolutional layer*. The frontend acts as
    a preprocessor. The DNN backend does the *classification learning*. The CNN frontend
    preprocesses the image data into a form that is computationally practical for
    the DNN to learn from. The CNN frontend does the *feature learning*.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 早期卷积神经网络是一种可以看作由两部分组成的神经网络，即前端和后端。后端是一个深度神经网络（DNN），我们之前已经讨论过。名称“卷积神经网络”来源于前端，称为“卷积层”。前端充当预处理器的角色。DNN后端执行“分类学习”。CNN前端将图像数据预处理成DNN可以学习的计算上实用的形式。CNN前端执行“特征学习”。
- en: Figure 3.1 depicts a CNN in which the convolutional layers act as a frontend
    for learning features from the image, which is then passed to a backend DNN for
    classification from the features.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1描述了一个CNN，其中卷积层作为前端从图像中学习特征，然后将这些特征传递给后端DNN进行基于特征的分类。
- en: '![](Images/CH03_F01_Ferlitsch.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F01_Ferlitsch.png)'
- en: Figure 3.1 A convolution acts as a frontend to a deep neural network to learn
    features instead of pixels.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 卷积作为前端，用于从图像中学习特征，然后将这些特征传递给后端DNN进行分类。
- en: This section covers the basic steps and components in assembling these earlier
    CNNs. While we do not specifically cover AlexNet, its success as the 2012 ILSVCR
    winner for image classification can be considered the catalyst for researchers
    to explore and develop convolutional designs. The assembly and design principles
    of the frontend of AlexNet were incorporated into the earliest design pattern,
    ConvNet, for practical application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了组装这些早期卷积神经网络的基本步骤和组件。虽然我们没有特别介绍AlexNet，但其作为2012年ILSVRC图像分类冠军的成功，可以被视为研究人员探索和发展卷积设计的催化剂。AlexNet前端的组装和设计原则被纳入了最早的设计模式，即ConvNet，以供实际应用。
- en: 3.1.1 Why we use a CNN over a DNN for image models
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 为什么我们使用CNN而不是DNN进行图像模型
- en: Once we get to larger image sizes, the number of pixels for a DNN becomes computationally
    too expensive to be feasible. Presume you have a 1 MB image, in which each pixel
    is represented by a single byte (0..255 value). At 1 MB, you have 1 million pixels.
    That would require an input vector of 1 million elements. And let’s assume that
    the input layer has 1024 nodes. The number of weights to *update and learn* would
    be over a billion (1 million × 1024) at just the input layer! Yikes. Back to a
    supercomputer and a lifetime of computing power.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们处理到更大的图像尺寸，对于深度神经网络（DNN）来说，像素的数量在计算上变得过于昂贵，以至于无法实现。假设你有一个1MB的图像，其中每个像素由一个字节（0..255值）表示。在1MB中，你有100万个像素。这将需要一个包含100万个元素的输入向量。假设输入层有1024个节点。仅在输入层就需要更新和学习的权重数量就超过10亿（100万
    × 1024）！天哪。回到超级计算机和一生的计算能力。
- en: Let’s contrast this to our Chapter 2 MNIST example of 784 pixels × 512 nodes
    on our input layer. That’s 400,000 weights to learn, which is considerably smaller
    than 1 billion. You can do the former on your laptop, but don’t dare try the latter.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与我们第二章的MNIST示例进行对比，输入层有784像素 × 512节点。这意味着有40万个权重需要学习，这比10亿小得多。你可以在你的笔记本电脑上完成前者，但不要尝试后者。
- en: In the following subsections, we will look at how the components of CNNs networks
    solve the problem of what would otherwise have been a computational impractical
    number of weights, also referred to as *parameters*, for image classification.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将探讨CNN网络组件是如何解决原本可能是一个计算上不切实际的权重数量（也称为参数）的问题，这对于图像分类来说。
- en: 3.1.2 Downsampling (resizing)
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 下采样（调整大小）
- en: To solve the problem of having too many parameters, one approach is to reduce
    the resolution of the image through a process called *downsampling*. But if we
    reduce the image resolution too far, at some point we may lose the ability to
    clearly distinguish what’s in the image; it becomes fuzzy and/or has artifacts.
    So, the first step is to reduce the resolution down to a level that we still have
    enough details.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决参数过多的问题，一种方法是通过称为下采样的过程降低图像的分辨率。但如果我们过度降低图像分辨率，在某个点上，我们可能失去清晰区分图像中内容的能力；它变得模糊，或者有伪影。因此，第一步是将分辨率降低到我们仍然有足够细节的水平。
- en: A common convention for everyday computer vision is around 224 × 224 pixels.
    We do this by resizing. Even at this lower resolution and three channels for color
    images, and an input layer of 1024 nodes, we still have 154 million weights to
    update and learn (224 × 224 × 3 × 1024); see figure 3.2.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于日常计算机视觉来说，一个常见的约定是224 × 224像素。我们通过调整大小来实现这一点。即使在这个较低的分辨率和三个通道的颜色图像，以及1024节点的输入层，我们仍然有1.54亿个权重需要更新和学习（224
    × 224 × 3 × 1024）；参见图3.2。
- en: '![](Images/CH03_F02_Ferlitsch.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F02_Ferlitsch.png)'
- en: 'Figure 3.2 Number of parameters at the input layer before and after resizing
    (image source: Pixabay, Stockvault)'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2 输入层在调整大小前后的参数数量（图片来源：Pixabay，Stockvault）
- en: So training on real-world images was out of reach with neural networks until
    the introduction of using convolutional layers. To begin with, a convolutional
    layer is a frontend to a neural network, which transforms the images from a high-dimensional
    pixel-based image to a substantially lower-dimensionality feature-based image.
    The substantially lower-dimensionality features can then be the input vector to
    a DNN. Thus, a convolutional frontend is a frontend between the image data and
    the DNN.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在引入使用卷积层之前，使用神经网络在真实世界图像上进行训练是不可能的。首先，卷积层是神经网络的前端，它将图像从基于高维像素的图像转换为基于显著低维度的特征图像。这些显著低维度的特征可以成为DNN的输入向量。因此，卷积前端是图像数据和DNN之间的前端。
- en: But let’s say we have enough computational power to use just a DNN and learn
    154 million weights at the input layer, as in our preceding example. Well, the
    pixels are very position-dependent on the input layer. So we learn to recognize
    a cat on the left side of the picture. But then we shift the cat to the middle
    of the picture. Now we have to learn to recognize a cat from a new set of pixel
    positions—wah! Now move it to the right, add the cat lying down, jumping in the
    air, and so forth.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 但假设我们拥有足够的计算能力，只使用深度神经网络（DNN）并在输入层学习1.54亿个权重，就像我们前面的例子一样。嗯，像素在输入层的位置非常依赖。所以，我们学会了识别图片左边的猫。但然后我们把猫移到图片中间。现在我们必须学会从一组新的像素位置识别猫——哇！现在把它移到右边，加上躺着的猫、在空中跳跃等等。
- en: Learning to recognize an image from various perspectives is referred to as *translational
    invariance*. For basic 2D renderings like digits and letters, this works (brute-force),
    but for everything else, it’s not going to work. Early research showed that when
    you flattened the initial image into a 1D vector, you lost the spatial relationship
    of the features that make up the object being classified, like a cat. Even if
    you successfully trained a DNN to, say, recognize a cat, based on pixels, in the
    middle of the picture, that DNN is unlikely to recognize the object if it is shifted
    in the image.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 从各种角度学习识别图像被称为*平移不变性*。对于基本的二维渲染，如数字和字母，这是可行的（暴力法），但对于其他所有东西，这都不行。早期的研究表明，当你将初始图像展平成1D向量时，你失去了构成被分类对象的特征的空间关系，比如猫。即使你成功地训练了一个DNN，比如基于像素在图片中间识别猫，那么如果这个对象在图像中移动了位置，这个DNN不太可能识别出该对象。
- en: Next, we will discuss how convolutions learn features instead of pixels, along
    with retaining the 2D shape for spatial relationships, which solved this problem.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将讨论卷积如何学习特征而不是像素，同时保留二维形状以进行空间关系，从而解决了这个问题。
- en: 3.1.3 Feature detection
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.3 特征检测
- en: For these higher-resolution and more-complex images, we perform recognition
    by detecting and classifying *features* instead of classifying pixel positions.
    Visualize an image, and ask yourself what makes you recognize what’s there? Go
    beyond the high level of asking, “Is that a person, a cat, or a building?” to
    ask why you can distinguish a person from the building they’re standing in front
    of, or separate a person from the cat they’re holding. Your eyes are recognizing
    low-level features, such as edges, blurs, and contrast.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些更高分辨率和更复杂的图像，我们通过检测和分类*特征*来进行识别，而不是对像素位置进行分类。可视化一张图像，问问自己是什么让你识别出那里的东西？超越问“那是一个人、一只猫还是一栋建筑？”这样的高级问题，去问为什么你能区分站在建筑物前面的人，或者从他们手中分离出一只猫。你的眼睛正在识别低级特征，如边缘、模糊和对比度。
- en: As depicted in figure 3.3, these low-level features are built up into contours
    and then spatial relationships. Suddenly, the eye/brain has the ability to recognize
    nose, ears, eyes—and perceive that’s a cat face, or that’s a human face.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.3所示，这些低级特征被构建成轮廓，然后是空间关系。突然之间，眼睛/大脑有了识别鼻子、耳朵、眼睛的能力——感知到那是一只猫脸，或者那是一个人脸。
- en: '![](Images/CH03_F03_Ferlitsch.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F03_Ferlitsch.png)'
- en: Figure 3.3 Flow of the human eye’s recognition of low-level features to high-level
    features
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 人眼识别低级特征到高级特征的流程
- en: In a computer, a *convolutional layer* performs the task of feature detection
    within an image. Each convolution consists of a set of filters. These filters
    are *N* × *M* matrices of values that are used to detect the likely presence of
    a feature. Think of them as little windows. They are slid across the image, and
    at each location, a comparison is made between the filter and the pixel values
    at that location. That comparison is done with a matrix dot product, but we will
    skip the statistics here. What’s important is that the result of this operation
    will generate a value that indicates how strongly the feature was detected at
    that location in the image. For example, a value of 4 would indicate a stronger
    presence of the feature than the value of 1.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算机中，*卷积层*负责在图像中进行特征检测。每个卷积由一组过滤器组成。这些过滤器是*N* × *M*的值矩阵，用于检测特征可能存在的情况。把它们想象成小窗口。它们在图像上滑动，并在每个位置，将过滤器与该位置的像素值进行比较。这种比较是通过矩阵点积完成的，但在这里我们将跳过统计。重要的是，这个操作的结果将生成一个值，表示在图像的该位置检测到特征的程度有多强。例如，4的值表示特征的检测比1的值更强。
- en: Prior to neural networks, imaging scientists hand-designed these filters. Today,
    the filters, along with the weights in the neural network, are *learned*. In a
    convolutional layer, we specify the size of the filter and the number of filters.
    Typical filter sizes are 3 × 3 and 5 × 5, with 3 × 3 the most common. The number
    of filters varies more, but they are typically multiples of 16, such as 16, 32,
    or 64 for shallow CNNs, and 256, 512, and 1024 in deep CNNs.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络之前，成像科学家手动设计这些过滤器。今天，过滤器以及神经网络中的权重都是*学习*得到的。在卷积层中，我们指定过滤器的尺寸和过滤器的数量。典型的过滤器尺寸是3
    × 3和5 × 5，其中3 × 3是最常见的。过滤器的数量变化更多，但它们通常是16的倍数，例如浅层CNN的16、32或64，以及深层CNN的256、512和1024。
- en: Additionally, we specify a *stride*, which is the rate that the filter is slid
    across the image. For example, if the stride is 1, the filter advances 1 pixel
    at a time; thus the filter would partially overlap with the previous step in a
    3 × 3 filter (and consequently so would a stride of 2). A stride of 3 has no overlap.
    The most common practice is to use strides of 1 and 2\. Each filter that is *learned*
    produces a feature map, which is a mapping that indicates how strongly the feature
    is detected at a particular location in the image, as depicted in figure 3.4.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们指定一个*步长*，这是过滤器在图像上滑动的速率。例如，如果步长是1，则过滤器每次前进1个像素；因此，过滤器在3 × 3的过滤器中会部分重叠前一步（并且因此步长为2的过滤器也是如此）。步长为3没有重叠。最常见的方法是使用步长1和2。每个*学习*到的过滤器都会产生一个特征图，这是一个映射，表示在图像的特定位置检测到特征的程度，如图3.4所示。
- en: '![](Images/CH03_F04_Ferlitsch.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH03_F04_Ferlitsch.png)'
- en: Figure 3.4 A filter is slid across an image to produce a feature map of detected
    features.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 过滤器在图像上滑动以产生检测到的特征的特征图。
- en: The filter can either stop when it gets to the edge of the image, or continue
    until the last column is covered, as shown in figure 3.5\. The former case is
    called *no padding*. The latter case is called *padding*. When the filter goes
    partially over the edge, we want to give a value for these imaginary pixels. Typical
    values are zero or same—the same as the last column.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 过滤器可以在到达图像边缘时停止，或者继续直到覆盖最后一列，如图3.5所示。前者称为*无填充*。后者称为*填充*。当过滤器部分超出边缘时，我们想要为这些虚拟像素提供一个值。典型值是零或相同——与最后一列相同。
- en: '![](Images/CH03_F05_Ferlitsch.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH03_F05_Ferlitsch.png)'
- en: Figure 3.5 Where the filter stops depends on padding.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 过滤器停止的位置取决于填充。
- en: When you have multiple convolutional layers, a common practice is to either
    keep the same number or increase the number of filters on deeper layers, and to
    use a stride of 1 on the first layer and 2 on deeper layers. The increase in filters
    provides the means to go from coarse detection of features to more detailed detection
    within coarse features. The increase in stride offsets the increase in the size
    of retained data; this process is referred to as *feature pooling ,* where the
    feature maps are downsampled.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有多个卷积层时，一个常见的做法是在深层层中保持相同的过滤器数量或增加过滤器数量，并在第一层使用步长1，在深层层使用步长2。过滤器数量的增加提供了从粗略检测特征到在粗略特征内进行更详细检测的手段。步长的增加抵消了保留数据大小的增加；这个过程被称为*特征池化*，其中特征图被下采样。
- en: 'CNNs use two types of downsampling: pooling and feature pooling. In *pooling*,
    a fixed algorithm is used to downsample the size of the image data. In *feature
    pooling*, the best downsampling algorithm for the specific dataset is learned:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: CNNs使用两种类型的下采样：池化和特征池化。在*池化*中，使用一个固定的算法来下采样图像数据的大小。在*特征池化*中，学习特定数据集的最佳下采样算法：
- en: More filters => More data
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 更多的过滤器 => 更多的数据
- en: Bigger strides => Less data
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的步长 => 更少的数据
- en: We’ll look at pooling in more detail next. We’ll delve into feature pooling
    in section 3.2.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节更详细地研究池化。我们将在3.2节深入研究特征池化。
- en: 3.1.4 Pooling
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.4 池化
- en: Even though each feature map that’s generated is typically equal in size to
    or smaller than the image, the total amount of data increases because we generate
    multiple feature maps (for example, 16). Yikes! The next step is to reduce the
    total amount of data, while retaining the features detected and corresponding
    spatial relationships among the detected features.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管生成的每个特征图通常与图像大小相等或更小，但由于我们生成了多个特征图（例如，16个），总数据量会增加。哎呀！下一步是减少总数据量，同时保留检测到的特征及其对应的空间关系。
- en: As I’ve said, this step is referred to as *pooling*, which is the same as *downsampling*
    (or *subsampling*). In this process, the feature maps are resized to a smaller
    dimension using either the max (downsampling) or mean pixel average (subsampling)
    within the feature map. In pooling, as depicted in figure 3.6, we set the size
    of the area to pool as an *N* × *M* matrix as well as a stride. The common practice
    is a 2 × 2 pool size with a stride of 2\. This will result in a 75% reduction
    in pixel data, while still preserving enough resolution that the detected features
    are not lost.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说的，这一步被称为*池化*，这与*下采样*（或*子采样*）相同。在这个过程中，特征图通过在特征图内部使用最大值（下采样）或平均像素平均值（子采样）调整到更小的维度。在池化中，如图3.6所示，我们将要池化的区域大小设置为*N*
    × *M*矩阵以及步长。常见的做法是2 × 2的池化大小和2的步长。这将导致像素数据的75%减少，同时仍然保留足够的分辨率，以确保检测到的特征不会丢失。
- en: '![](Images/CH03_F06_Ferlitsch.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F06_Ferlitsch.png)'
- en: Figure 3.6 Pooling resizes feature maps to a smaller dimension.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 池化将特征图调整到更小的维度。
- en: Another way to look at pooling is in the context of information gain. By reducing
    unwanted or less-informative pixels (for example, from the background) we are
    reducing entropy and making the remaining pixels more informative.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种看待池化的方式是在信息增益的背景下。通过减少不需要或不那么有信息的像素（例如，背景中的像素），我们正在减少熵，并使剩余的像素更有信息量。
- en: 3.1.5 Flattening
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.5 展平
- en: 'Recall that deep neural networks take vectors as input—one-dimensional arrays
    of numbers. In the case of the pooled maps, we have a list (plurality) of 2D matrices,
    so we need to transform them into a single 1D vector, which then becomes the input
    vector to the DNN. This process is called *flattening* : we flatten the list of
    2D matrices into a single 1D vector.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，深度神经网络以向量作为输入——数字的一维数组。在池化图的情况下，我们有一个2D矩阵的列表（复数），因此我们需要将它们转换成一个单一的1D向量，然后它成为DNN的输入向量。这个过程被称为*展平*：我们将2D矩阵的列表展平成一个单一的1D向量。
- en: It’s pretty straightforward. We start with the first row of the first pooled
    map as the beginning of the 1D vector. We then take the second row and append
    it to the end, and then the third row, and so forth. We then proceed to the second
    pooled map and do the same, continuously appending each row until we’ve completed
    the last pooled map. As long as we follow the same sequencing through pooled maps,
    the spatial relationships among detected features will be maintained across images
    for training and inference (prediction), as depicted in figure 3.7.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这相当直接。我们以第一个池化图的第一行为1D向量的开始。然后我们取第二行并将其附加到末尾，接着是第三行，以此类推。然后我们继续到第二个池化图，并执行相同的操作，持续地将每一行附加到我们完成最后一个池化图。只要我们通过池化图遵循相同的顺序，检测到的特征之间的空间关系将在训练和推理（预测）过程中保持一致，如图3.7所示。
- en: For example, if we have 16 pooled maps of size 20 × 20 and three channels per
    pooled map (for example, RGB channels in a color image), our 1D vector size will
    be 16 × 20 × 20 × 3 = 19,200 elements.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有16个大小为20 × 20的池化图，每个池化图有3个通道（例如，彩色图像中的RGB通道），我们的1D向量大小将是16 × 20 × 20
    × 3 = 19,200个元素。
- en: '![](Images/CH03_F07_Ferlitsch.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F07_Ferlitsch.png)'
- en: Figure 3.7 Spatial relationships are maintained when pooled maps are flattened.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 当池化图被展平时，空间关系得以保持。
- en: 3.2 The ConvNet design for a CNN
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 CNN的ConvNet设计
- en: Let’s get started now with TF.Keras. Let’s assume a situation that’s hypothetical
    but resembles the real world today. Your company’s application supports human
    interfaces and currently can be accessed through voice activation. You’ve been
    tasked with developing a proof of concept to demonstrate expanding the human interface
    to include sign language in order to comply with federal accessibility laws. The
    relevant law, Section 503 of the Rehabilitation Act of 1973, “prohibits federal
    contractors and subcontractors from discriminating in employment against individuals
    with disabilities and requires employers take affirmative action to recruit, hire,
    promote, and retain these individuals” ([https://www.dol.gov/agencies/ofccp/section-503](https://www.dol.gov/agencies/ofccp/section-503)).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们开始使用TF.Keras。让我们假设一个假设但与现实世界相似的情况。贵公司的应用程序支持人机界面，并且目前可以通过语音激活访问。你被分配了一个开发概念证明的任务，以展示将手语纳入人机界面，以符合联邦无障碍法律。相关的法律，1973年康复法案的第503节，“禁止联邦承包商和分包商在就业中歧视残疾人，并要求雇主采取积极行动招募、雇佣、晋升和留住这些个人”([https://www.dol.gov/agencies/ofccp/section-503](https://www.dol.gov/agencies/ofccp/section-503))。
- en: What you should not do is assume that you can train the model by using arbitrary
    labeled sign-language images and image augmentation. The data, its preparation,
    and the design of the model must match the actual “in the wild” deployment. Otherwise,
    beyond resulting in disappointing accuracy, the model might learn noise, exposing
    it to false positives that could lead to unexpected consequences, and be vulnerable
    to hacking. Chapter 12 covers this in more detail.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 你不应该假设你可以通过使用任意标记的手语图像和图像增强来训练模型。数据、其准备和模型的设计必须与实际的“野外”部署相匹配。否则，除了导致令人失望的准确率外，模型可能会学习噪声，使其暴露于可能导致意外后果的假阳性，并且容易受到黑客攻击。第12章将更详细地介绍这一点。
- en: For our proof of concept, we are going to show only recognizing hand signs for
    the letters of the English alphabet (A to Z). Additionally, we assume that the
    individual will be signing directly in front of the camera from a dead-on perspective.
    We don’t want the model to learn, for example, the ethnicity of the hand signer.
    So for this and other reasons, color is not important.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的概念验证，我们将仅展示识别英文字母（从A到Z）的手势。此外，我们假设个人将直接在摄像头前从正面的角度进行手势。我们不希望模型学习，例如，手势者的种族。因此，出于这个和其他原因，颜色并不重要。
- en: To make our model not learn color (the noise), we will train it in grayscale
    mode. We will design the model to learn and predict, a process also referred to
    as *inference*, in grayscale. What we do want the model to learn are contours
    of the hand. We will design the model in two parts, the convolutional frontend
    and the DNN backend, as shown in figure 3.8.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的模型不学习颜色（噪声），我们将以灰度模式对其进行训练。我们将设计模型以在灰度下学习和预测，这个过程也被称为*推理*。我们希望模型学习的是手的轮廓。我们将设计模型为两部分，即卷积前端和DNN后端，如图3.8所示。
- en: '![](Images/CH03_F08_Ferlitsch.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F08_Ferlitsch.png)'
- en: Figure 3.8 A ConvNet with a convolutional frontend and DNN backend
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 带有卷积前端和DNN后端的ConvNet
- en: The following code sample is written in the sequential API method and in long
    form; activation functions are specified using the corresponding method (instead
    of specifying them as a parameter when adding the corresponding layer).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例是用顺序API方法编写的，并且是长格式；激活函数是通过相应的方法指定的（而不是在添加相应层时将其作为参数指定）。
- en: We start by adding a convolutional layer of 16 filters as the first layer by
    using the `Conv2D` class object. Recall that the number of filters equals the
    number of feature maps that will be generated (in this case, 16). The size of
    each filter will be 3 × 3, which is specified by the parameter `kernel_size`,
    and a stride of 2 by the parameter `strides`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先通过使用`Conv2D`类对象添加一个16个滤波器的卷积层作为第一层。回想一下，滤波器的数量等于将要生成的特征图的数量（在这种情况下，16）。每个滤波器的大小将是3
    × 3，这是通过`kernel_size`参数指定的，步长为2，由`strides`参数指定。
- en: Note that for `strides`, a tuple of `(2, 2)` is specified instead of a single
    value 2\. The first digit is the horizontal stride (across), and the second digit
    is the vertical stride (down). It’s a common convention for these horizontal and
    vertical values to be the same; therefore, we commonly say a “stride of 2” instead
    of “a 2 × 2 stride.”
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于`strides`，指定了一个`(2, 2)`的元组而不是单个值2。第一个数字是水平步长（横跨），第二个数字是垂直步长（向下）。这些水平和垂直值通常是相同的，因此我们通常说“步长为2”而不是“2
    × 2步长”。
- en: You may ask, what’s with the 2D part of the name `Conv2D`? The 2D means that
    input to the convolutional layer will be a stack of matrices (two-dimensional
    array). For this chapter, we will stick with 2D convolutionals, which are a common
    practice for computer vision.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，`Conv2D`这个名字中的2D部分是什么意思？2D表示卷积层的输入将是一堆矩阵（二维数组）。对于本章，我们将坚持使用2D卷积，这是计算机视觉中的常见做法。
- en: Let’s calculate what the output size will be from this layer. As you recall,
    at a stride of 1, each output feature map will be the same size as the image.
    With 16 filters, that would be 16 times the input. But since we used a stride
    of 2 (feature pooling), each feature map will be reduced by 75%, so the total
    output size will be 4 times the input.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算从这个层输出的尺寸将会是多少。如您所回忆的，在步长为1的情况下，每个输出特征图的大小将与图像相同。有16个滤波器，那将是输入的16倍。但由于我们使用了步长为2（特征池化），每个特征图将减少75%，因此总输出大小将是输入的4倍。
- en: The output from the convolutional layer is then passed through a ReLU activation
    function, which is then passed to the max pooling layer, using the `MaxPool2D`
    class object. The size of the pooling region will be 2 × 2, specified by the parameter
    `pool_ size`, with a stride of 2 by the parameter `strides`. The pooling layer
    will reduce the feature maps by 75% into pooled feature maps.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层的输出随后通过ReLU激活函数，然后传递给最大池化层，使用`MaxPool2D`类对象。池化区域的大小将是2 × 2，由参数`pool_size`指定，步长为2，由参数`strides`指定。池化层将特征图减少75%到池化特征图。
- en: Let’s calculate the output size after the pooling layer. We know that the size
    coming in is 4 times the input. With an additional 75% reduction, the output size
    is the same as the input. So what have we gained here? First, we have trained
    a set of filters to learn a first set of coarse features (resulting in information
    gain), eliminated nonessential pixel information (reducing entropy), and learned
    the best method to downsample the feature maps. Hmm, seems we gained a lot.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算池化层之后的输出大小。我们知道输入的大小是输入大小的4倍。再额外减少75%，输出大小与输入相同。那么我们在这里得到了什么？首先，我们训练了一组滤波器来学习第一组粗糙特征（从而获得信息增益），消除了非必要的像素信息（减少熵），并学会了下采样特征图的最佳方法。嗯，看起来我们得到了很多。
- en: The pooled feature maps are then flattened, using the `Flatten` class object,
    into a 1D vector for input into the DNN. We will glance over the parameter `padding`.
    It is sufficient for our purposes to say that in almost all cases, you will use
    the value `same`; it’s just that the default is `valid`, and therefore you need
    to explicitly add it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将池化特征图展平，使用`Flatten`类对象，形成一个1D向量，用于输入到深度神经网络（DNN）。我们将简要介绍一下参数`padding`。对于我们的目的来说，可以说在几乎所有情况下，你都会使用值`same`；只是默认值是`valid`，因此你需要明确地添加它。
- en: 'Finally, we pick an input size for our images. We like to reduce the size to
    as small as possible without losing detection of the features needed for recognizing
    the contours of the hand. In this case, we choose 128 × 128\. The `Conv2D` class
    has a quirk: it always requires specifying the number of channels, instead of
    defaulting to 1 for grayscale; thus we specified it as (128, 128, 1) instead of
    (128, 128).'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们为我们的图像选择一个输入大小。我们希望尽可能减小大小，同时不丢失识别手部轮廓所需的特征检测。在这种情况下，我们选择128 × 128。`Conv2D`类有一个特性：它总是要求指定通道数，而不是默认为灰度图的1；因此我们将其指定为(128,
    128, 1)而不是(128, 128)。
- en: 'Here’s the code:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE0]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Image data is inputted to a convolutional layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 图像数据输入到一个卷积层。
- en: ❷ The size the feature maps is reduced by pooling.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过池化减少了特征图的大小。
- en: ❸ The 2D feature maps are flattened into a 1D vector before the output layer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在输出层之前，2D特征图被展平成一个1D向量。
- en: 'Let’s look at the details of the layers in our model by using the `summary()`
    method:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用`summary()`方法，让我们来看看我们模型中各层的详细信息：
- en: '[PRE1]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The output from the convolutional layer is 16 feature maps of 2D size 64 ×
    64.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 卷积层的输出是16个2D大小为64 × 64的特征图。
- en: ❷ The output from the pooling layer reduces the feature map sizes to 32 × 32.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 池化层的输出将特征图大小减少到32 × 32。
- en: ❸ The number of parameters for the 512-node dense layer is over 8 million; every
    node in the flatten layer is connected to every node in the dense layer.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 512节点密集层的参数数量超过800万；展平层中的每个节点都与密集层中的每个节点相连。
- en: ❹ The final dense layer has 26 nodes, one for each letter in the English alphabet.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 最终的密集层有26个节点，每个节点对应英文字母表中的一个字母。
- en: Here’s how to read the Output Shape column. For the `Conv2D` input layer, the
    output shape shows (None, 64, 64, 16). The first value in the tuple is the number
    of examples (batch size) that will be passed through on a single forward feed.
    Since this is determined at training time, it is set to `None` to indicate it
    will be bound when the model is being fed data. The last number is the number
    of filters, which we set to 16\. The two numbers in the middle (64, 64) are the
    output size of the feature maps—in this case, 64 × 64 pixels each (for a total
    of 16). The output size is determined by the filter size (3 × 3), the stride (2
    × 2) and the padding (same). The combination that we specified will result in
    the height and width being halved, for a total reduction of 75% in size.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是如何读取输出形状列。对于`Conv2D`输入层，输出形状显示为（None, 64, 64, 16）。元组中的第一个值是单个前向传递中将通过的示例（批大小）数量。由于这是在训练时确定的，因此设置为`None`以表示当模型被喂数据时将绑定。最后一个数字是过滤器的数量，我们将其设置为16。中间的两个数字（64,
    64）是特征图的输出大小——在这种情况下，每个为64 × 64像素（总共16）。输出大小由过滤器大小（3 × 3）、步长（2 × 2）和填充（same）决定。我们指定的组合将使高度和宽度减半，总大小减少75%。
- en: For the `MaxPooling2D` layer, the output size of the pooled feature maps will
    be 32 × 32\. By specifying a pooling region of 2 × 2 and stride of 2, the height
    and width of the pooled feature maps will be halved, for a total reduction of
    75% in size.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`MaxPooling2D`层，池化特征图的输出大小将是32 × 32。通过指定2 × 2的池化区域和2的步长，池化特征图的高度和宽度将减半，总大小减少75%。
- en: The flattened output from the pooled feature maps is a 1D vector of size 16,384,
    calculated as 16 × (32 × 32). Let’s see if this adds up to what we calculated
    earlier, that the output size of the feature maps should be the same as the input
    size. Our input is 128 × 128, which is 16,384 which matches the output size from
    the `Flatten` layer.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 从池化特征图得到的展平输出是一个大小为16,384的1D向量，计算方式为16 × (32 × 32)。让我们看看这加起来是否等于我们之前计算的，即特征图的输出大小应该与输入大小相同。我们的输入是128
    × 128，也就是16,384，这与`Flatten`层的输出大小相匹配。
- en: Each element (pixel) in the flattened pooled feature maps is then inputted to
    each node in the input layer of the DNN, which has 512 nodes. The number of connections
    between the flattened layer and the input layer is therefore 16,384 × 512 = ~8.4
    million. That’s the number of weights to learn at that layer, and where most of
    the computation will (overwhelmingly) occur.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 展平后的池化特征图中的每个元素（像素）随后被输入到DNN输入层的每个节点中，该层有512个节点。因此，展平层和输入层之间的连接数是16,384 × 512
    = ~8.4百万。这就是该层需要学习的权重数量，并且大部分计算将（压倒性地）发生在这里。
- en: 'Let’s now show the same code example in a variation of the sequential method
    style. Here, the activation methods are specified using the parameter `activation`
    in each instantiation of a layer (such as `Conv2D(), Dense()`):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们以序列方法风格的变体来展示相同的代码示例。在这里，激活方法是通过在每个层的实例化中使用参数`activation`来指定的（例如`Conv2D(),
    Dense()`）：
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s now show the same code example in a third way, using the functional API
    method. In this approach, we separately define each layer, starting with the input
    vector and proceeding to the output layer. At each layer, we use polymorphism
    to invoke the instantiated class (layer) object as a callable and pass in the
    object of the previous layer to connect it to.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们以第三种方式展示相同的代码示例，使用功能API方法。在这种方法中，我们分别定义每个层，从输入向量开始，到输出层结束。在每一层，我们使用多态来调用实例化的类（层）对象作为可调用对象，并传入前一个层的对象来连接它。
- en: 'For example, for the first `Dense` layer, when invoked as a callable, we pass
    as the parameter the layer object for the `Flatten` layer. As a callable, this
    will cause the `Flatten` layer and the first `Dense` layer to be fully connected
    (each node in the `Flatten` layer will be connected to every node in the `Dense`
    layer):'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于第一个`Dense`层，当作为可调用对象调用时，我们将`Flatten`层的层对象作为参数传递。作为一个可调用对象，这将导致`Flatten`层和第一个`Dense`层完全连接（`Flatten`层的每个节点将连接到`Dense`层的每个节点）：
- en: '[PRE3]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The input vector for a convolutional layer requires specifying the number
    of channels.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对于卷积层，需要指定通道数。
- en: ❷ Constructs the convolutional layer
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建卷积层
- en: ❸ Reduces the size of feature maps by pooling
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过池化减少特征图的大小
- en: 3.3 VGG networks
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 VGG网络
- en: The *VGG* type of CNN was designed by the Visual Geometry Group at Oxford. It
    was designed to compete in the international ILSVRC competition for image recognition
    for 1000 classes of images. The VGGNet in the 2014 contest took first place on
    the image location task and second place on the image classification task.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '*VGG* 类型的卷积神经网络是由牛津大学的视觉几何组设计的。它是为了在国际ILSVRC图像识别竞赛中竞争1000类图像而设计的。2014年的VGGNet在图像定位任务中获得了第一名，在图像分类任务中获得了第二名。'
- en: 'While AlexNet (and its corresponding ConvNet design pattern) is considered
    the granddaddy of convolutional networks, the VGGNet (and its corresponding VGG
    design pattern) is considered the father of formalizing a design pattern based
    on groups of convolutions. Like its AlexNet predecessors, it continued to view
    the convolutional layers as a frontend, and to retain a large DNN backend for
    the classification task. The fundamental principles behind the VGG design pattern
    are as follows:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 AlexNet（及其相应的卷积网络设计模式）被认为是卷积网络的鼻祖，但 VGGNet（及其相应的 VGG 设计模式）被认为是基于卷积组正规化设计模式的之父。像它的
    AlexNet 先辈一样，它继续将卷积层视为前端，并保留一个大的 DNN 后端用于分类任务。VGG 设计模式背后的基本原理如下：
- en: Grouping multiple convolutions into blocks, with the same number of filters
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个卷积分组为具有相同数量的过滤器的块
- en: Progressively doubling the number of filters across blocks
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在块之间逐步加倍过滤器数量
- en: Delaying pooling to the end of a block
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将池化延迟到块的末尾
- en: When discussing a VGG design pattern in today’s context, initial confusion may
    arise over the terms *group* and *block*. In their research for VGGNet, the authors
    used the term *convolutional group*. Subsequently, researchers refined the grouping
    patterns into convolutional groups, consisting of convolutional blocks. In today’s
    nomenclature, the VGG group would be called a *block*.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当在当今的背景下讨论 VGG 设计模式时，可能会对术语 *group* 和 *block* 产生初始混淆。在为 VGGNet 进行研究时，作者使用了术语
    *卷积组*。随后，研究人员将分组模式细化成由卷积块组成的卷积组。在今天的命名法中，VGG 组会被称为 *块*。
- en: It is designed using a handful of principles that are easy to learn. The convolutional
    frontend consists of a sequence of pairs (and later triples) of convolutions of
    the same size, followed by a max pooling. The max pooling layer downsamples the
    generated feature maps by 75%, and the next pair (or triple) of convolutional
    layers then doubles the number of learned filters. The principle behind the convolution
    design was that the early layers learn coarse features, and subsequent layers,
    by increasing the filters, learn finer and finer features, and the max pooling
    is used between the layers to minimize growth in size (and subsequently parameters
    to learn) of the feature maps. Finally, the DNN backend consists of two identically-sized
    dense hidden layers of 4096 nodes each, and a final dense output layer of 1000
    nodes for classification. Figure 3.9 depicts the first convolutional groups in
    a VGG architecture.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 它是使用一些易于学习的原则设计的。卷积前端由一系列相同大小的卷积对（后来是三对）组成，随后是最大池化。最大池化层将生成的特征图下采样75%，然后下一对（或三对）卷积层将学习到的过滤器数量加倍。卷积设计背后的原理是，早期层学习粗略特征，后续层通过增加过滤器，学习越来越精细的特征，而最大池化用于层之间以最小化特征图大小的增长（以及随后学习的参数）。最后，深度神经网络（DNN）后端由两个大小相同、每个有4096个节点的密集隐藏层和一个用于分类的1000个节点的最终密集输出层组成。图3.9描述了VGG架构中的第一个卷积组。
- en: '![](Images/CH03_F09_Ferlitsch.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH03_F09_Ferlitsch.png)'
- en: Figure 3.9 In the VGG architecture, convolutions are grouped, and pooling is
    delayed to the end of the group.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 在VGG架构中，卷积被分组，池化被延迟到组末尾。
- en: The best-known versions are the VGG16 and VGG19\. The VGG16 and VGG19 that were
    used in the competition, along with their trained weights from the competition,
    were made publicly available. As they have been frequently used in transfer learning,
    others have kept the convolutional frontend of an ImageNet pretrained VGG16 or
    VGG19, and corresponding weights, and attached a new DNN backend for retraining
    for new classes of images. Figure 3.10 is an architectural depiction of a VGG16.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最著名的版本是 VGG16 和 VGG19。在竞赛中使用的 VGG16 和 VGG19，以及它们的竞赛训练权重，都已公开发布。由于它们在迁移学习中经常被使用，其他人保留了
    ImageNet 预训练的 VGG16 或 VGG19 的卷积前端和相应的权重，并附加了一个新的 DNN 后端，用于重新训练新的图像类别。图3.10是VGG16的架构描述。
- en: '![](Images/CH03_F10_Ferlitsch.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH03_F10_Ferlitsch.png)'
- en: Figure 3.10 A VGG16 architecture consists of a convolutional frontend of VGG
    groups, followed by a DNN backend.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 VGG16架构由VGG组的卷积前端组成，后面跟着DNN后端。
- en: 'So, let’s go ahead and code a VGG16 in two coding styles: the first in a sequential
    flow, and the second procedurally using *reuse* functions for duplicating the
    common blocks of layers, and parameters for their specific settings. We will also
    change specifying `kernel_size` and `pool_size` as keyword parameters and instead
    specify them as positional parameters:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们继续用两种编码风格来编写VGG16：第一种是顺序流，第二种是使用*重用*函数来复制层的公共块，并指定它们特定设置的参数。我们还将更改指定`kernel_size`和`pool_size`的方式，将它们作为关键字参数指定，而不是位置参数：
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ First convolutional block
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个卷积块
- en: ❷ Second convolutional block—double the number of filters
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第二个卷积块——过滤器数量加倍
- en: ❸ Third convolutional block—double the number of filters
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第三个卷积块——过滤器数量加倍
- en: ❹ Fourth convolutional block—double the number of filters
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第四个卷积块——过滤器数量加倍
- en: ❺ Fifth (final) convolutional block
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 第五（最终）个卷积块
- en: ❻ DNN backend
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ DNN后端
- en: ❼ Output layer for classification (1000 classes)
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 用于分类的输出层（1000个类别）
- en: 'You just coded a VGG16—nice. Let’s now code the same using a procedural reuse
    style. In this example, we create a procedure (function) `conv_block``()`, which
    builds the convolutional blocks and takes as parameters the number of layers in
    the block (2 or 3), and number of filters (64, 128, 256, or 512). Note that we
    keep the first convolutional layer outside `conv_block`. The first layer needs
    the `input_shape` parameter. We could have coded this as a flag to `conv_block`,
    but since it would occur only one time, that’s not reuse. So we inline it instead:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚编写了一个VGG16——不错。现在让我们用过程重用风格来编写相同的代码。在这个例子中，我们创建了一个过程（函数）`conv_block``()`，它构建卷积块，并接受块中层数（2或3）和过滤器数量（64、128、256或512）作为参数。注意，我们将第一个卷积层放在`conv_block`之外。第一层需要`input_shape`参数。我们本可以将它编码为`conv_block`的标志，但由于它只会出现一次，这不是重用。所以我们将其内联：
- en: '[PRE5]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Convolutional block implemented as a procedure
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 卷积块作为过程实现
- en: ❷ First convolutional specified separately since it requires the input_shape
    parameter
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个卷积块单独指定，因为它需要`input_shape`参数
- en: ❸ Remainder of first convolutional block
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第一个卷积块的剩余部分
- en: ❹ Second through fifth convolutional blocks
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 第二至第五个卷积块
- en: Try running `model.summary()` on both examples, and you will see that the output
    is identical.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试在两个示例上运行 `model.summary()`，你会发现输出是相同的。
- en: 3.4 ResNet networks
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 ResNet网络
- en: The *ResNet* type of CNN was designed by Microsoft Research to compete in the
    international ILSVRC competition. The ResNet in the 2015 contest took first place
    in all categories for the ImageNet and Common Objects in Context (COCO) competition.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 微软研究院设计的*ResNet*类型CNN是为了在国际ILSVRC竞赛中竞争。2015年的比赛中，ResNet在ImageNet和Common Objects
    in Context (COCO)竞赛的所有类别中都获得了第一名。
- en: The VGGNet design pattern covered in the previous section had limitations in
    how deep the model architecture could go in layers, before suffering from vanishing
    and exploding gradients. In addition, different layers converging at different
    rates could lead to divergence during training.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中介绍的VGGNet设计模式在模型架构的层数深度上存在局限性，在遭受梯度消失和梯度爆炸之前，模型架构的深度是有限的。此外，不同层以不同速率收敛可能导致训练过程中的发散。
- en: The researchers for the residual block design pattern component of the residual
    network proposed a new novel layer connection they called an *identity link*.
    The identity link introduced the earliest concept of feature reuse. Prior to the
    identity link, each convolutional block did feature extraction on the previous
    convolutional output, without retaining any knowledge from prior outputs. The
    identity link can be seen as a coupling between the current and prior convolutional
    outputs to reuse feature information gained from earlier extraction. Concurrently
    along with ResNet, other researchers—such as at Google, with Inception v1 (GoogLeNet)—further
    refined convolutional design patterns into groups and blocks. In parallel to these
    design improvements was the introduction of batch normalization.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于残差网络中残差块设计模式组件的研究人员提出了一个他们称之为*恒等链接*的新颖层连接。恒等链接引入了特征重用的最早概念。在恒等链接之前，每个卷积块都对前一个卷积输出进行特征提取，而不保留任何先前输出的知识。恒等链接可以看作是当前和先前卷积输出之间的耦合，以重用从早期提取中获得的特征信息。同时，与ResNet一起，其他研究人员——例如谷歌的Inception
    v1（GoogLeNet）——进一步将卷积设计模式细化成组和块。与这些设计改进并行的是批量归一化的引入。
- en: Using identity links along with batch normalization provided more stability
    across layers, reducing both vanishing and exploding gradients and divergence
    between layers, allowing model architectures to go deeper in layers to increase
    accuracy in prediction.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 使用身份链接以及批量归一化在层之间提供了更多的稳定性，减少了梯度消失和爆炸以及层间的发散，使得模型架构可以在层中更深，从而提高预测的准确性。
- en: 3.4.1 Architecture
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 架构
- en: ResNet, and other architectures within this class, use different layer-to-layer
    connection patterns. The patterns we’ve discussed so far (ConvNet and VGG) use
    the fully connected layer-to-layer pattern.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet以及这个类别中的其他架构使用不同的层到层连接模式。我们之前讨论的模式（ConvNet和VGG）使用的是全连接层到层模式。
- en: ResNet34 introduced a new block layer and layer-connection pattern, residual
    blocks, and identity connection, respectively. The residual block in ResNet34
    consists of blocks of two identical convolutional layers without a pooling layer.
    Each block has an identity connection that creates a parallel path between the
    input of the residual block and its output, as depicted in figure 3.11\. As in
    VGG, each successive block doubles the number of filters. Pooling is done at the
    end of the sequence of blocks.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet34引入了新的块层和层连接模式，分别是残差块和恒等连接。ResNet34中的残差块由两个没有池化层的相同卷积层组成。每个块都有一个恒等连接，它创建了一个在残差块的输入和输出之间的并行路径，如图3.11所示。与VGG一样，每个后续块将滤波器的数量加倍。在块序列的末尾进行池化。
- en: '![](Images/CH03_F11_Ferlitsch.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH03_F11_Ferlitsch.png)'
- en: Figure 3.11 A residual block in which the input is a matrix added to the output
    of the convolution
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 一个残差块，其中输入是一个矩阵加到卷积的输出上
- en: 'One of the problems with neural networks is that as we add deeper layers (under
    the presumption of increasing accuracy), their performance can degrade. It can
    get worse, not better. This occurs for several reasons. As we go deeper, we are
    adding more parameters (weights). The more parameters, the more places that each
    input in the training data will fit to the excess parameters. Instead of generalizing,
    the neural network will simply learn each training example (rote memorization).
    The other issue is *covariate shift*: the distribution of the weights will widen
    (spread further apart) as we go deeper, resulting in making it more difficult
    for the neural network to converge. The former case causes a degradation in performance
    on the test (holdout) data, and the latter, on the training data as well as a
    vanishing or exploding gradient.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的一个问题是，当我们增加更深的层（在假设增加准确性的前提下），它们的性能可能会下降。情况可能会变得更糟，而不是更好。这有几个原因。当我们深入时，我们正在添加更多的参数（权重）。参数越多，每个训练数据中的输入适合过多参数的地方就越多。不是泛化，神经网络将简单地学习每个训练示例（死记硬背）。另一个问题是*协变量偏移*：随着我们深入，权重的分布会变宽（进一步分散），这使得神经网络收敛变得更加困难。前者导致测试（保留）数据上的性能下降，后者在训练数据上也是如此，以及梯度消失或爆炸。
- en: Residual blocks allow neural networks to be built with deeper layers without
    a degradation in performance on the test data. A ResNet block could be viewed
    as a VGG block with the addition of the identity link. While the VGG style of
    the block performs feature detection, the identity link retains the input for
    the next subsequent block, whereby the input to the next block consists of both
    the previous features’ detection and input.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块允许神经网络构建更深的层，而不会降低测试数据上的性能。一个ResNet块可以看作是一个添加了恒等连接的VGG块。虽然块的VGG风格执行特征检测，但恒等连接保留了输入给下一个后续块，因此下一个块的输入包括前一个特征检测和输入。
- en: 'By retaining information from the past (previous input), this block design
    allows neural networks to go deeper than the VGG counterpart, with an increase
    in accuracy. Mathematically, we could represent the VGG and ResNet as follows.
    For both cases, we want to learn a formula for *h*(*x*) that is the distribution
    (for example, labels) of the test data. For VGG, we are learning a function *f(x*,
    {*W*}), where {*W*} represents the weights. For ResNet, we modify the equation
    by adding the term “+ *x*”, which is the identity:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过保留过去（前一个输入）的信息，这种块设计使得神经网络可以比VGG对应物更深，同时提高准确性。从数学上讲，我们可以将VGG和ResNet表示如下。对于这两种情况，我们希望学习一个*h*(*x*)的公式，它是测试数据的分布（例如，标签）。对于VGG，我们学习一个函数*f(x*,
    {*W*})，其中{*W*}代表权重。对于ResNet，我们通过添加“+ *x*”这一项来修改方程，其中*x*是恒等：
- en: 'VGG: *h*(*x*) = *f*(*x*, {*W*})'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 'VGG: *h*(*x*) = *f*(*x*, {*W*})'
- en: 'ResNet: *h*(*x*) = *f*(*x*, {*W*}) + *x*'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 'ResNet: *h*(*x*) = *f*(*x*, {*W*}) + *x*'
- en: 'The following code snippet shows how a residual block can be coded in TF.Keras
    by using the functional API method approach. The variable `x` represents the output
    of a layer, which is the input to the next layer. At the beginning of the block,
    we retain a copy of the previous block/layer output as the variable `shortcut`.
    We then pass the previous block/layer output (`x`) through two convolutional layers,
    each time taking the output from the previous layer as input into the next layer.
    Finally, the last output from the block (retained in the variable `x`) is added
    (matrix addition) with the original value of `x` (shortcut). This is the identity
    link, which is commonly referred to as a *shortcut*:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码片段展示了如何通过使用功能API方法在TF.Keras中编码一个残差块。变量`x`代表一个层的输出，它是下一层的输入。在块的开始，我们保留前一个块/层的输出作为变量`shortcut`。然后我们将前一个块/层的输出(`x`)通过两个卷积层，每次都将前一个层的输出作为下一层的输入。最后，块的最后一个输出（保留在变量`x`中）与原始的`x`值（快捷方式）相加（矩阵加法）。这是恒等连接，通常被称为*快捷方式*：
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Remember the input to the block.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记住块的输入。
- en: ❷ The output of the convolutional sequence
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 卷积序列的输出
- en: ❸ Matrix addition of the input to the output
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输入到输出的矩阵加法
- en: Let’s now put the whole network together, using a procedural style. Additionally,
    we need to add the entry convolutional layer of ResNet and then the DNN classifier.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用过程式风格将整个网络组合起来。此外，我们还需要添加ResNet的入口卷积层，然后是DNN分类器。
- en: As we did for the VGG example, we define a procedure (function) for generating
    the residual block pattern, following the pattern we used in the preceding code
    snippet. For our procedure `residual_block``()`, we pass in the number of filters
    for the block and the input layer (the output from the previous layer).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在VGG示例中所做的那样，我们定义了一个生成残差块模式的程序（函数），遵循我们在前面的代码片段中使用的模式。对于我们的`residual_block``()`过程，我们传入块的滤波器数量和输入层（前一个层的输出）。
- en: The ResNet architectures take as input a (224, 224, 3) vector—an RGB image (3
    channels) of 224 (height) × 224 (width) pixels. The first layer is a basic convolutional
    layer, consisting of a convolution using a fairly large filter size of 7 × 7\.
    The output (feature maps) is then reduced in size by a max pooling layer.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构将一个(224, 224, 3)向量作为输入——一个224（高度）× 224（宽度）像素的RGB图像（3个通道）。第一层是一个基本的卷积层，使用一个相当大的7
    × 7的滤波器。然后通过一个最大池化层减小输出（特征图）的大小。
- en: After the initial convolutional layer is a succession of groups of residual
    blocks. Each successive group doubles the number of filters (similar to VGG).
    Unlike VGG, though, there is no pooling layer between the groups that would reduce
    the size of the feature maps. Now, if we connect these blocks directly with each
    other, we have a problem. The input to the next block has a shape based on the
    previous block’s filter size (let’s call it *X*). The next block, by doubling
    the filters, will cause the output of that residual block to be double in size
    (let’s call it 2*X*). The identity link would attempt to add the input matrix
    (*X* ) and the output matrix (2*X* ). Yikes—we get an error, indicating we can’t
    broadcast (for the add operation) matrices of different sizes.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始卷积层之后是一系列残差块组。每个后续组将过滤器数量加倍（类似于VGG）。然而，与VGG不同，组之间没有池化层来减少特征图的大小。现在，如果我们直接将这些块连接起来，我们会遇到问题。下一个块的输入形状基于前一个块的过滤器大小（让我们称它为*X*）。下一个块通过加倍过滤器，将导致该残差块的输出大小加倍（让我们称它为2*X*）。恒等链接将尝试将输入矩阵(*X*)和输出矩阵(2*X*)相加。哎呀——我们得到一个错误，表示我们无法广播（对于加法操作）不同大小的矩阵。
- en: For ResNet, this is solved by adding a convolutional block between each “doubling”
    group of residual blocks. As depicted in figure 3.12, the convolutional block
    doubles the filters to reshape the size and doubles the stride to reduce the feature
    map size by 75% (performs feature pooling).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 对于ResNet，这是通过在每个“加倍”的残差块组之间添加一个卷积块来解决的。如图3.12所示，卷积块将过滤器加倍以改变大小，并将步长加倍以将特征图大小减少75%（执行特征池化）。
- en: '![](Images/CH03_F12_Ferlitsch.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F12_Ferlitsch.png)'
- en: Figure 3.12 The convolution block performs pooling and doubles the number of
    feature maps for the next convolutional group.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 卷积块执行池化并将特征图数量加倍，为下一个卷积组做准备。
- en: 'The output of the last residual block group is passed to a pooling and flattening
    layer (`GlobalAveragePooling2D`), which is then passed to a single `Dense` layer
    of 1000 nodes (number of classes):'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个残差块组的输出传递到一个池化和展平层(`GlobalAveragePooling2D`)，然后传递到一个有1000个节点的单个`Dense`层（类别数量）：
- en: '[PRE7]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ The residual block as a procedure
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 残差块作为过程
- en: ❷ The convolutional block as a procedure
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 卷积块作为过程
- en: ❸ The input tensor
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输入张量
- en: ❹ First convolutional layer, where pooled feature maps will be reduced by 75%
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 首个卷积层，其中池化特征图将减少75%
- en: ❺ First residual block group of 64 filters
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 64个过滤器的第一个残差块组
- en: ❻ Doubles the size of filters and reduces feature maps by 75% (stride s = 2,
    2) to fit the next residual group
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将过滤器大小加倍并减少特征图75%（步长s = 2, 2）以适应下一个残差组
- en: Let’s now run `model.summary``()`. We see that the total number of parameters
    to learn is 21 million. This is in contrast to the VGG16, which has 138 million
    parameters. So the ResNet architecture is six times computationally faster. This
    reduction is mostly achieved by the construction of the residual blocks. Notice
    that the DNN backend is just a single output `Dense` layer. In effect, there is
    no backend. The early residual block groups act as the CNN frontend doing the
    feature detection, while the latter residual blocks perform the classification.
    In doing so, unlike in VGG, there was no need for several fully connected dense
    layers, which would have substantially increased the number of parameters.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行`model.summary()`。我们看到需要学习的总参数数量是2100万。这与拥有1.38亿参数的VGG16形成对比。所以ResNet架构在计算上快了六倍。这种减少主要是由残差块的结构实现的。注意，深度神经网络后端只是一个单独的输出`Dense`层。实际上，没有后端。早期的残差块组充当CNN前端进行特征检测，而后面的残差块执行分类。这样做时，与VGG不同，不需要几个全连接的密集层，这会大大增加参数数量。
- en: 'Unlike the previous example of pooling, in which the size of each feature map
    is reduced according to the size of the stride, `GlobalAveragePooling2D` is like
    a supercharged version of pooling: each feature map is replaced by a single value,
    which in this case is the average of all values in the corresponding feature map.
    For example, if the input is 256 feature maps, the output will be a 1D vector
    of size 256\. After ResNet, it became the general practice for deep convolutional
    neural networks to use `GlobalAveragePooling2D` at the last pooling stage, which
    benefited from a substantial reduction of the number of parameters coming into
    the classifier, without significant loss in representational power.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前示例中的池化不同，在池化中每个特征图的尺寸根据步长的大小而减小，`GlobalAveragePooling2D`就像一个超级充电版的池化：每个特征图被一个单一值所替代，在这种情况下是相应特征图中所有值的平均值。例如，如果输入是256个特征图，输出将是一个大小为256的1D向量。在ResNet之后，使用`GlobalAveragePooling2D`在最后一个池化阶段成为深度卷积神经网络的通用实践，这显著减少了进入分类器的参数数量，而没有在表示能力上造成重大损失。
- en: Another advantage is the identity link, which provided the ability to add deeper
    layers, without degradation, for higher accuracy.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个优点是恒等连接，它提供了在不降低性能的情况下添加更深层的功能，以实现更高的准确率。
- en: ResNet50 **i**ntroduced a variation of the residual block referred to as the
    *bottleneck residual block*. In this version, the group of two 3 × 3 convolutional
    layers is replaced by a group of 1 × 1, then 3 × 3, and then 1 × 1 convolutional
    layers. The first 1 × 1 convolution performs a dimensionality reduction, reducing
    the computational complexity, and the last convolution restores the dimensionality,
    increasing the number of filters by a factor of 4\. The middle 3 × 3 convolution
    is referred to as the *bottleneck convolution*, like the neck of a bottle. The
    bottleneck residual block, depicted in figure 3.13, allows for deeper neural networks,
    without degradation, and further reduction in computational complexity.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet50 **引入**了一种称为*瓶颈残差块*的残差块变体。在这个版本中，两个3 × 3卷积层组被一组1 × 1、然后3 × 3、最后1 × 1卷积层组所取代。第一个1
    × 1卷积执行降维操作，降低计算复杂度，最后一个卷积恢复维度，通过4倍增加滤波器的数量。中间的3 × 3卷积被称为*瓶颈卷积*，就像瓶子的颈部。如图3.13所示的瓶颈残差块允许构建更深层的神经网络，而不降低性能，并进一步降低计算复杂度。
- en: '![](Images/CH03_F13_Ferlitsch.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH03_F13_Ferlitsch.png)'
- en: Figure 3.13 The bottleneck design uses 1 × 1 convolutions for dimensionality
    reduction and expansion.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 瓶颈设计使用1 × 1卷积进行降维和扩展。
- en: 'Here is a code snippet for writing a bottleneck residual block as a reusable
    function:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个将瓶颈残差块作为可重用函数编写的代码片段：
- en: '[PRE8]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ A 1 × 1 bottleneck convolution for dimensionality reduction
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于降维的1 × 1瓶颈卷积
- en: ❷ A 3 × 3 convolution for feature extraction
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于特征提取的3 × 3卷积
- en: ❸ A 1 × 1 projection convolution for dimensionality expansion
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于降维的1 × 1投影卷积
- en: ❹ Matrix addition of the input to the output
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 输入到输出的矩阵加法
- en: Residual blocks introduced the concepts of representational power and representational
    equivalence. *Representational power* is a measure of how powerful a block is
    as a feature extractor. *Representational equivalence* is the idea that a block
    can be factored into a lower computational complexity, while maintaining representational
    power. The design of the residual bottleneck block was demonstrated to maintain
    representational power of the ResNet34 block, with a lower computational complexity.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 残差块引入了表示能力和表示等价性的概念。*表示能力*是衡量一个块作为特征提取器强大程度的一个指标。*表示等价性*是指一个块可以被分解成具有更低计算复杂度的形式，同时保持其表示能力。残差瓶颈块的设计被证明可以保持ResNet34块的表示能力，同时降低计算复杂度。
- en: 3.4.2 Batch normalization
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 批标准化
- en: Another problem with adding deeper layers in a neural network is the *vanishing
    gradient* problem. This is actually about computer hardware. During training (the
    process of backward propagation and gradient descent), at each layer the weights
    are multiplied by very small numbers—specifically, numbers less than 1\. As you
    know, two numbers less than 1 multiplied together make an even smaller number.
    When these tiny values are propagated through deeper layers, they continuously
    get smaller. At some point, the computer hardware can’t represent the value anymore—hence,
    the *vanishing gradient*.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在神经网络中添加更深层的问题还包括*梯度消失*问题。这实际上是关于计算机硬件的。在训练过程中（反向传播和梯度下降的过程），在每一层，权重都会乘以非常小的数字——具体来说，是小于1的数字。正如你所知，两个小于1的数字相乘会得到一个更小的数字。当这些微小的值通过更深层传播时，它们会持续变小。在某个点上，计算机硬件无法再表示这个值——因此，出现了*梯度消失*。
- en: The problem is further exacerbated if we try to use half-precision floats (16-bit
    floats) for the matrix operations versus single-precision floats (32-bit floats).
    The advantage of the former is that the weights (and data) are stored in half
    the amount of space—and using a general rule of thumb, by reducing the computational
    size in half, we can execute four times as many instructions per computing cycle.
    The problem, of course, is that with even smaller precision, we will encounter
    the *vanishing gradient* even sooner.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们尝试使用半精度浮点数（16位浮点数）进行矩阵运算，而不是单精度浮点数（32位浮点数），问题会进一步加剧。前者的优势在于权重（和数据）存储的空间减少了一半——按照一般经验，将计算大小减半，我们可以在每个计算周期内执行四倍的指令。当然，问题是，即使精度更小，我们也会更早地遇到*梯度消失*问题。
- en: '*Batch normalization* is a technique applied to the output of a layer (before
    or after the activation function). Without going into the statistics aspect, it
    normalizes the shift in the weights as they are being trained. This has several
    advantages: it smooths out (across a batch) the amount of change, thus slowing
    the possibility of getting a number so small that it can’t be represented by the
    hardware. Additionally, by narrowing the amount of shift between the weights,
    convergence can happen sooner by using a higher learning rate and reducing the
    overall amount of training time. Batch normalization is added to a layer in TF.Keras
    with the `BatchNormalization` class.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '*批归一化*是一种应用于层输出（在激活函数之前或之后）的技术。不深入统计学方面，它在训练过程中对权重的偏移进行归一化。这有几个优点：它平滑了（在整个批次中）变化量，从而减缓了得到一个无法由硬件表示的极小数字的可能性。此外，通过缩小权重之间的偏移量，可以使用更高的学习率并减少总的训练时间，从而更快地收敛。在
    TF.Keras 中，使用 `BatchNormalization` 类将批归一化添加到层中。'
- en: 'In earlier implementations, batch normalization was implemented post-activation.
    The batch normalization would occur after the convolution and dense layers. At
    the time, it was debated whether the batch normalization should be before or after
    the activation function. This code example uses post-activation batch normalization
    both before and after an activation function, in both a convolution and dense
    layer:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期的实现中，批归一化是在激活函数之后实现的。批归一化发生在卷积和密集层之后。当时，人们争论批归一化应该在激活函数之前还是之后。此代码示例在卷积和密集层中，在激活函数之前和之后都使用了后激活批归一化：
- en: '[PRE9]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Adds batchnorm before the activation
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在激活函数之前添加批归一化
- en: ❷ Adds batchnorm after the activation
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在激活函数之后添加批归一化
- en: 3.4.3 ResNet50
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 ResNet50
- en: '*ResNet50* is a well-known model, which is commonly reused as a stock model,
    such as for transfer learning, as shared layers in objection detection, and for
    performance benchmarking. The model has three versions: v1, v1.5 and v2.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '*ResNet50* 是一个广为人知的模型，通常被用作通用模型，例如用于迁移学习、作为目标检测中的共享层，以及用于性能基准测试。该模型有三个版本：v1、v1.5
    和 v2。'
- en: '*ResNet50 v1* formalized the concept of a *convolutional group*. This is a
    set of convolutional blocks that share a common configuration, such as the number
    of filters. In v1, the neural network is decomposed into groups, and each group
    doubles the number of filters from the previous group.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '*ResNet50 v1* 正式化了*卷积组*的概念。这是一个共享相同配置（如滤波器数量）的卷积块集合。在 v1 中，神经网络被分解成组，每个组将前一个组的滤波器数量翻倍。'
- en: Additionally, the concept of a separate convolution block to double the number
    of filters was removed and replaced by a residual block that uses *linear projection*.
    Each group starts with a residual block using linear projection on the identity
    link to double the number of filters, while the remaining residual blocks pass
    the input directly to the output for the matrix add operation. Additionally, the
    first 1 × 1 convolution in the residual block with linear projection uses a stride
    of 2 (feature pooling), which is also known as a *strided convolution*, reducing
    the feature map sizes by 75%, as depicted in figure 3.14.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，移除了单独的卷积块以将滤波器数量加倍的概念，并替换为使用*线性投影*的残差块。每个组从使用线性投影在标识连接上进行的残差块开始，以加倍滤波器数量，而其余的残差块直接将输入传递到输出以进行矩阵加法操作。此外，具有线性投影的残差块中的第一个1
    × 1卷积使用步长为2（特征池化），这也称为*带步长的卷积*，如图3.14所示，减少了特征图大小75%。
- en: '![](Images/CH03_F14_Ferlitsch.png)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F14_Ferlitsch.png)'
- en: Figure 3.14 The identity link is replaced with a 1 × 1 projection to match the
    number of feature maps on the convolutional output for the matrix add operation.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 标识连接被替换为1 × 1投影以匹配卷积输出上的特征图数量，以便进行矩阵加法操作。
- en: 'The following is an implementation of ResNet50 v1 using the bottleneck block
    combined with batch normalization:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对ResNet50 v1使用瓶颈块与批量归一化相结合的实现：
- en: '[PRE10]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ The projection block as a procedure
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 投影块作为过程
- en: ❷ 1 × 1 projection convolution on shortcut to match size of output
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在快捷连接上进行1 × 1投影卷积以匹配输出大小
- en: ❸ Each convolutional group after the first group starts with a projection block.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 首个组之后的每个卷积组都以投影块开始。
- en: As depicted in figure 3.15, v1.5 introduced a refactoring of the bottleneck
    design and further reducing of computational complexity, while maintaining representational
    power. The feature pooling (strides = 2) in the residual block with linear projection
    is moved from the first 1 × 1 convolution to the 3 × 3 convolution, reducing computational
    complexity and increasing results on ImageNet by 0.5%.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3.15所示，v1.5引入了对瓶颈设计的重构，进一步降低了计算复杂度，同时保持了表示能力。在具有线性投影的残差块中的特征池化（步长=2）从第一个1
    × 1卷积移动到3 × 3卷积，降低了计算复杂度，并在ImageNet上提高了0.5%的结果。
- en: '![](Images/CH03_F15_Ferlitsch.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH03_F15_Ferlitsch.png)'
- en: Figure 3.15 The dimensionality reduction moved from the 1 × 1 to the 3 × 3 convolution.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15 维度降低从1 × 1卷积移动到3 × 3卷积。
- en: 'The following is an implementation of ResNet50 v1 residual block with a projection
    link:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对具有投影连接的ResNet50 v1残差块的实现：
- en: '[PRE11]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Bottleneck is moved to 3 × 3 convolution using a stride of 2
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用步长为2将瓶颈移动到3 × 3卷积
- en: '*ResNet50 v2* introduced *preactivation batch normalization* (*BN-RE-Conv*),
    in which the batch normalization and activation functions are placed before (instead
    of after) the corresponding convolution or dense layer. This has now become a
    common practice, as depicted here for implementation of the residual block with
    the identity link in v2:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*ResNet50 v2*引入了*预激活批量归一化*（*BN-RE-Conv*），其中批量归一化和激活函数被放置在相应的卷积或密集层之前（而不是之后）。现在这已成为一种常见做法，如图中所示，v2中实现了具有标识连接的残差块：'
- en: '[PRE12]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Batchnorm before the convolution
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在卷积之前进行批量归一化
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A convolutional neural network can be described as adding a frontend to a deep
    neural network.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络可以描述为向深度神经网络添加前端。
- en: The purpose of the CNN frontend is to reduce the high-dimensional pixel input
    to low-dimensional feature representation.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNN前端的目的是将高维像素输入降低到低维特征表示。
- en: The lower dimensionality of the feature representation makes it practical to
    do deep learning with real-world images.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征表示的较低维度使得使用真实世界图像进行深度学习变得实用。
- en: Image resizing and pooling are used to reduce the number of parameters in the
    model, without information loss.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图像缩放和池化来减少模型中的参数数量，而不损失信息。
- en: Using a cascading set of filters to detect features has similarities to the
    human eye.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一系列级联的滤波器来检测特征与人类眼睛有相似之处。
- en: VGG formalized the concept of a convolutional pattern that is repeated.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: VGG形式化了重复的卷积模式的概念。
- en: Residual networks introduced the concept of feature reuse and demonstrated the
    ability to obtain higher accuracy at the same number of layers as a VGG, and go
    deeper in layers for more accuracy.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 残差网络引入了特征重用的概念，并证明了在相同层数的情况下，与VGG相比可以获得更高的准确率，并且可以加深层数以获得更高的准确率。
- en: Batch normalization allowed models to go deeper in layers for more accuracy
    before being exposed to vanishing or exploding gradients.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化允许模型在暴露于梯度消失或梯度爆炸之前，在层中更深入地学习以获得更高的准确性。
