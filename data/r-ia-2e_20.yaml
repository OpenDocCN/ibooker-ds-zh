- en: 16 Cluster analysis
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 16 聚类分析
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Identifying cohesive subgroups (clusters) of observations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别观察值的紧密子组（聚类）
- en: Determining the number of clusters present
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定存在的聚类数量
- en: Obtaining a nested hierarchy of clusters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得聚类的嵌套层次结构
- en: Obtaining discrete clusters
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获得离散的聚类
- en: '*Cluster analysis* is a data-reduction technique designed to uncover subgroups
    of observations within a dataset. It allows you to reduce a large number of observations
    to a much smaller number of clusters or types. A *cluster* is defined as a group
    of observations that are more similar to each other than they are to the observations
    in other groups. This isn’t a precise definition, and that fact has given rise
    to an enormous variety of clustering methods.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: '*聚类分析*是一种数据降维技术，旨在揭示数据集中观察值的子组。它允许您将大量观察值减少到更少的聚类或类型。*聚类*被定义为比其他组中的观察值更相似的观察值组。这不是一个精确的定义，这一事实导致了大量聚类方法的出现。'
- en: Cluster analysis is widely used in the biological and behavioral sciences, marketing,
    and medical research. For example, a psychological researcher might cluster data
    on the symptoms and demographics of depressed patients, seeking to uncover subtypes
    of depression. The hope would be that finding such subtypes might lead to more
    targeted and effective treatments and a better understanding of the disorder.
    Marketing researchers use cluster analysis as a customer-segmentation strategy.
    Customers are arranged into clusters based on the similarity of their demographics
    and buying behaviors. Marketing campaigns are then tailored to appeal to one or
    more of these subgroups. Medical researchers use cluster analysis to help catalog
    gene expression patterns obtained from DNA microarray data. This can help them
    understand normal growth and development and the underlying causes of many human
    diseases.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析在生物和行为科学、市场营销和医学研究中得到广泛应用。例如，一位心理学家可能会对抑郁患者的症状和人口统计数据进行聚类，试图揭示抑郁的亚型。希望找到这样的亚型可能会导致更具有针对性和有效的治疗方法，以及对这种疾病的更好理解。市场营销研究人员使用聚类分析作为客户细分策略。客户根据其人口统计数据和购买行为的相似性被安排到不同的群体中。随后，营销活动将针对一个或多个这些子群体进行定制。医学研究人员使用聚类分析来帮助整理从DNA微阵列数据中获得的基因表达模式。这有助于他们理解正常的生长和发育以及许多人类疾病的潜在原因。
- en: 'The two most popular clustering approaches are *hierarchical agglomerative
    clustering* and *partitioning clustering*. In agglomerative hierarchical clustering,
    each observation starts as its own cluster. Clusters are then combined, two at
    a time, until all clusters are merged into a single cluster. In the partitioning
    approach, you specify *K*: the number of clusters you’re seeking. Observations
    are then randomly divided into *K* groups and reshuffled to form cohesive clusters.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最受欢迎的两种聚类方法是*层次聚类法*和*划分聚类法*。在层次聚类法中，每个观察值最初都是一个单独的聚类。然后，每次合并两个聚类，直到所有聚类合并成一个单一的聚类。在划分方法中，您指定*K*：您要寻找的聚类数量。然后，观察值被随机分成*K*组，并重新排列以形成紧密的聚类。
- en: Within each of these broad approaches, there are many clustering algorithms
    to choose from. For hierarchical clustering, the most popular are single linkage,
    complete linkage, average linkage, centroid, and Ward’s method. For partitioning,
    the two most popular are k-means and partitioning around medoids (PAM). Each clustering
    method has advantages and disadvantages, which we’ll discuss.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些广泛的方法中，有许多聚类算法可供选择。对于层次聚类，最受欢迎的是单链接、完全链接、平均链接、质心法和沃德法。对于划分聚类，最受欢迎的是k-means和基于中位数划分（PAM）。每种聚类方法都有其优缺点，我们将在后面讨论。
- en: 'The examples in this chapter focus on food and wine (I suspect my friends aren’t
    surprised). Hierarchical clustering is applied to the `nutrient` dataset contained
    in the `flexclust` package to answer the following questions:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的例子集中在食品和葡萄酒上（我怀疑我的朋友们不会感到惊讶）。层次聚类被应用于`flexclust`包中包含的`nutrient`数据集，以回答以下问题：
- en: What are the similarities and differences among 27 types of fish, fowl, and
    meat, based on 5 nutrient measures?
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据5种营养指标，27种鱼类、家禽和肉类的相似性和差异是什么？
- en: Can these foods be meaningfully clustered into a smaller number of groups?
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些食品能否有意义的聚类成更少的组？
- en: Partitioning methods will be used to evaluate 13 chemical analyses of 178 Italian
    wine samples. The data are contained in the `wine` dataset available with the
    `rattle` package. Here, the questions are
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 将使用分区方法评估178个意大利葡萄酒样品的13项化学分析。数据包含在`rattle`包提供的`wine`数据集中。在这里，问题是
- en: Are there subtypes of wine in the data?
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中是否存在葡萄酒亚型？
- en: If so, how many subtypes are there, and what are their characteristics?
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果存在，那么有多少亚型，它们的特征是什么？
- en: In fact, the wine samples represent three varietals (recorded as `Type`). This
    will allow you to evaluate how well the cluster analysis recovers the underlying
    structure.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，葡萄酒样品代表三种品种（记录为`Type`）。这将允许你评估聚类分析恢复潜在结构的效果。
- en: Although there are many approaches to cluster analysis, they usually follow
    a similar set of steps. These common steps are described in section 16.1\. Hierarchical
    agglomerative clustering is described in section 16.3, and partitioning methods
    are covered in section 16.4\. Some final advice and cautionary statements are
    provided in section 16.6\. To run the examples in this chapter, be sure to install
    the `cluster`, `NbClust`, `flexclust`, `fMultivar`, `ggplot2`, `ggdendro`, `factoextra`,
    `clusterability`, and `rattle` packages. The `rattle` package will also be used
    in chapter 17.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管聚类分析有许多方法，但它们通常遵循一系列相似的步骤。这些常见步骤在16.1节中描述。层次聚类在16.3节中描述，分区方法在16.4节中介绍。一些最终建议和注意事项在16.6节中提供。要运行本章的示例，请确保安装`cluster`、`NbClust`、`flexclust`、`fMultivar`、`ggplot2`、`ggdendro`、`factoextra`、`clusterability`和`rattle`包。`rattle`包也将在第17章中使用。
- en: 16.1 Common steps in cluster analysis
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.1 聚类分析的常见步骤
- en: 'Like factor analysis (chapter 14), an effective cluster analysis is a multistep
    process with numerous decision points. Each decision can affect the quality and
    usefulness of the results. This section describes the 11 typical steps in a comprehensive
    cluster analysis:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 与因子分析（第14章）一样，有效的聚类分析是一个多步骤过程，有多个决策点。每个决策都可能影响结果的质量和有用性。本节描述了全面聚类分析的11个典型步骤：
- en: '*Choose appropriate attributes.* The first (and perhaps most important) step
    is to select variables that you feel may be important for identifying and understanding
    differences among groups of observations within the data. For example, in a study
    of depression, you might want to assess one or more of the following: psychological
    symptoms; physical symptoms; age at onset; the number, duration, and timing of
    episodes; the number of hospitalizations; functional status with regard to self-care;
    social and work history; current age; gender; ethnicity; socioeconomic status;
    marital status; family medical history; and response to previous treatments. A
    sophisticated cluster analysis can’t compensate for a poor choice of variables.'
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择合适的属性。* 第一步（也许是最重要的一步）是选择你认为可能对识别和理解数据中观察组之间差异重要的变量。例如，在抑郁症研究中，你可能想评估以下一个或多个：心理症状；身体症状；发病年龄；发作的数量、持续时间和时间；住院次数；自我护理的功能状态；社会和工作历史；当前年龄；性别；种族；社会经济地位；婚姻状况；家族医疗史；以及先前治疗反应。复杂的聚类分析无法弥补变量选择不当的缺点。'
- en: '*Scale the data.* If the variables in the analysis vary in range, the variables
    with the largest range will have the greatest impact on the results. This is often
    undesirable, so analysts scale the data before continuing. The most popular approach
    is to standardize each variable to a mean of 0 and a standard deviation of 1\.
    Other alternatives include dividing each variable by its maximum value or subtracting
    the variable’s mean and dividing by the variable’s median absolute deviation.
    The three approaches are illustrated with the following code snippets:'
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*缩放数据。* 如果分析中的变量范围不同，范围最大的变量将对结果产生最大影响。这通常是不希望的，因此分析师在继续之前会缩放数据。最流行的方法是将每个变量标准化到均值为0和标准差为1。其他替代方法包括将每个变量除以其最大值或减去变量的均值并除以变量的中位数绝对偏差。以下代码片段展示了这三种方法：'
- en: '[PRE0]'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: In this chapter, you’ll use the `scale()` function to standardize the variables
    to a mean of 0 and a standard deviation of 1\. This is equivalent to the first
    code snippet (`df1`).
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在本章中，你将使用`scale()`函数将变量标准化到均值为0和标准差为1。这与第一个代码片段（`df1`）等效。
- en: '*Screen for outliers.* Many clustering techniques are sensitive to outliers,
    which can distort the cluster solutions obtained. You can screen for (and remove)
    univariate outliers using functions from the `outliers` package. The `mvoutlier`
    package contains functions that can be used to identify multivariate outliers.
    An alternative is to use a clustering method that is robust to the presence of
    outliers. Partitioning around medoids (section 16.4.2) is an example of the latter
    approach.'
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*筛选异常值。* 许多聚类技术对异常值敏感，这可能会扭曲获得的聚类解决方案。您可以使用`outliers`包中的函数来筛选（并删除）单变量异常值。`mvoutlier`包包含可以用来识别多变量异常值的函数。另一种选择是使用对异常值存在具有鲁棒性的聚类方法。基于中位数划分（第16.4.2节）是后一种方法的例子。'
- en: '*Calculate distances.* Although clustering algorithms vary widely, they typically
    require a measure of the distance among the entities to be clustered. The most
    popular measure of the distance between two observations is the Euclidean distance,
    but the Manhattan, Canberra, asymmetric binary, maximum, and Minkowski distance
    measures are also available (see `?dist` for details). In this chapter, the Euclidean
    distance is used throughout. Calculating Euclidean distances is covered in section
    16.2.'
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计算距离。* 尽管聚类算法差异很大，但它们通常需要测量要聚类的实体之间的距离。两个观测值之间最流行的距离度量是欧几里得距离，但曼哈顿、Canberra、非对称二进制、最大和Minkowski距离度量也是可用的（有关详细信息，请参阅`?dist`）。在本章中，始终使用欧几里得距离。计算欧几里得距离在第16.2节中进行了介绍。'
- en: '*Select a clustering algorithm.* Next, you select a method of clustering the
    data. Hierarchical clustering is useful for smaller problems (say, 150 observations
    or less) and when a nested hierarchy of groupings is desired. The partitioning
    method can handle much larger problems but requires that the number of clusters
    be specified in advance.'
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*选择聚类算法。* 接下来，您选择一种聚类数据的方法。层次聚类适用于较小的问题（例如，150个观测值或更少）以及当需要嵌套分组层次结构时。划分方法可以处理更大的问题，但需要提前指定聚类数量。'
- en: Once you’ve chosen the hierarchical or partitioning approach, you must select
    a specific clustering algorithm. Again, each has advantages and disadvantages.
    Sections 16.3 and 16.4 describe the most popular. You may wish to try more than
    one algorithm to see how robust the results are to the choice of methods.
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 一旦您选择了层次或划分方法，您必须选择一个特定的聚类算法。同样，每个算法都有其优点和缺点。第16.3节和第16.4节描述了最流行的算法。您可能希望尝试多个算法，以查看结果对方法选择的鲁棒性。
- en: '*Obtain one or more cluster solutions.* This step uses the method(s) selected
    in step 5.'
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*获得一个或多个聚类解决方案。* 此步骤使用步骤5中选择的方 法。'
- en: '*Determine the number of clusters present.* To obtain a final cluster solution,
    you must decide how many clusters are present in the data. This is a thorny problem,
    and many approaches have been proposed. It usually involves extracting various
    numbers of clusters (say, 2 to *K*) and comparing the quality of the solutions.
    The `NbClust()` function in the `NbClust` package provides 26 different indices
    to help you make this decision (which elegantly demonstrates how unresolved this
    issue is). `NbClust` is used throughout this chapter.'
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*确定存在的聚类数量。* 为了获得最终的聚类解决方案，您必须决定数据中存在多少个聚类。这是一个棘手的问题，已经提出了许多方法。通常涉及提取各种数量的聚类（例如，2到*K*），并比较解决方案的质量。`NbClust`包中的`NbClust()`函数提供了26个不同的指标，以帮助您做出这个决定（巧妙地展示了这个问题是如何悬而未决的）。`NbClust`在本章中得到了广泛应用。'
- en: '*Obtain a final clustering solution.* Once the number of clusters has been
    determined, a final clustering is performed to extract that number of subgroups.'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*获得最终的聚类解决方案。* 一旦确定了聚类数量，就会执行最终的聚类以提取该数量的子组。'
- en: '*Visualize the results.* Visualization can help you determine the meaning and
    usefulness of the cluster solution. The results of a hierarchical clustering are
    usually presented as a dendrogram. Partitioning results are typically visualized
    using a bivariate cluster plot.'
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*可视化结果。* 可视化可以帮助您确定聚类解决方案的意义和有用性。层次聚类的结果通常以树状图的形式呈现。划分结果通常使用双变量聚类图进行可视化。'
- en: '*Interpret the clusters.* Once a cluster solution has been obtained, you must
    interpret (and possibly name) the clusters. What do the observations in a cluster
    have in common? How do they differ from the observations in other clusters? This
    step is typically accomplished by obtaining summary statistics for each variable
    by cluster. For continuous data, the mean or median for each variable within each
    cluster is calculated. For mixed data (data that contains categorical variables),
    the summary statistics will also include modes or category distributions.'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*解释聚类。* 一旦获得聚类解决方案，就必须解释（并可能命名）聚类。聚类中的观测值有什么共同之处？它们如何与其他聚类中的观测值不同？这一步骤通常通过为每个变量按聚类获得汇总统计来实现。对于连续数据，计算每个聚类中每个变量的均值或中位数。对于混合数据（包含分类变量的数据），汇总统计还将包括众数或类别分布。'
- en: '*Validate the results.* Validating the cluster solution involves asking the
    question, “Are these groupings in some sense real and not a manifestation of unique
    aspects of this dataset or statistical technique?” If a different cluster method
    or different sample is employed, would the same clusters be obtained? The `fpc`,
    `clv`, and `clValid` packages each contain functions for evaluating the stability
    of a clustering solution.'
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*验证结果。* 验证聚类解决方案涉及提出问题：“这些分组在某种意义上是真实的，而不是这个数据集或统计技术的独特方面的表现吗？”如果使用不同的聚类方法或不同的样本，是否会得到相同的聚类？`fpc`、`clv`和`clValid`每个包都包含用于评估聚类解决方案稳定性的函数。'
- en: Because the calculations of distances between observations is such an integral
    part of cluster analysis, it’s described next in some detail.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 由于观测值之间的距离计算是聚类分析的一个基本组成部分，因此下面将详细描述。
- en: 16.2 Calculating distances
  id: totrans-35
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.2 计算距离
- en: Every cluster analysis begins with the calculation of a distance, dissimilarity,
    or proximity between each entity to be clustered. The Euclidean distance between
    two observations is given by
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个聚类分析都以计算每个要聚类的实体之间的距离、相似度或邻近度开始。两个观测值之间的欧几里得距离由以下公式给出
- en: '![](Images/CH16_F00_Kabacoff3-EQ01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F00_Kabacoff3-EQ01.png)'
- en: where *i* and *j* are observations and *P* is the number of variables. In other
    words, the Euclidean distance between two observations is the square root of the
    sum of squared differences on each variable.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *i* 和 *j* 是观测值，*P* 是变量的数量。换句话说，两个观测值之间的欧几里得距离是每个变量上平方差的和的平方根。
- en: Consider the `nutrient` dataset provided with the `flexclust` package. The dataset
    contains measurements on the nutrients of 27 types of meat, fish, and fowl. The
    first few observations are given by
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑`flexclust`包提供的`nutrient`数据集。该数据集包含27种肉类、鱼类和家禽的营养成分测量值。前几个观测值如下所示
- en: '[PRE1]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: and the Euclidean distance between the first two (beef braised and hamburger)
    is
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 和前两个（红烧牛肉和汉堡）之间的欧几里得距离是
- en: '![](Images/CH16_F00_Kabacoff3-EQ02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F00_Kabacoff3-EQ02.png)'
- en: The `dist()` function in the base R installation can be used to calculate the
    distances between all rows (observations) of a matrix or data frame. The format
    is `dist(``x``, method=)`, where `x` is the input data and `method="euclidean"`
    by default. The function returns a lower triangle matrix by default, but the `as.matrix()`
    function can be used to access the distances using standard bracket notation.
    For the `nutrient` data frame,
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 基础R安装中的`dist()`函数可用于计算矩阵或数据框中所有行（观测值）之间的距离。格式为`dist(``x``，method=)`，其中`x`是输入数据，默认`method="euclidean"`。该函数默认返回一个下三角矩阵，但可以使用`as.matrix()`函数使用标准括号符号访问距离。对于`nutrient`数据框，
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Larger distances indicate larger dissimilarities between observations. The distance
    between an observation and itself is 0\. As expected, the `dist()` function provides
    the same distance between beef braised and hamburger as the hand calculations.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 较大的距离表示观测值之间的差异较大。一个观测值与自身的距离是0。正如预期的那样，`dist()`函数提供了红烧牛肉和汉堡之间的相同距离，与手工计算相同。
- en: Cluster analysis with mixed data types
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 混合数据类型的聚类分析
- en: Euclidean distances are usually the distance measure of choice for continuous
    data. But if other variable types are present, alternative dissimilarity measures
    are required. You can use the `daisy()` function in the `cluster` package to obtain
    a dissimilarity matrix among observations that have any combination of binary,
    nominal, ordinal, and continuous attributes. Other functions in the `cluster`
    package can use these dissimilarities to carry out a cluster analysis. For example,
    `agnes()` offers agglomerative hierarchical clustering, and `pam()` provides partitioning
    around medoids.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 欧几里得距离通常是连续数据的距离度量选择。但如果存在其他变量类型，则需要其他相似性度量。您可以使用 `cluster` 包中的 `daisy()` 函数获取具有任何组合的二进制、名义、有序和连续属性的观测值之间的相似性矩阵。`cluster`
    包中的其他函数可以使用这些相似性进行聚类分析。例如，`agnes()` 提供了聚合层次聚类，而 `pam()` 提供了基于中位数划分。
- en: Note that distances in the `nutrient` data frame are heavily dominated by the
    contribution of the `energy` variable, which has a much larger range. Scaling
    the data will help equalize the impact of each variable. In the next section,
    you’ll apply hierarchical cluster analysis to this dataset.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`nutrient` 数据框中的距离在很大程度上受 `energy` 变量的贡献所主导，该变量的范围要大得多。缩放数据将有助于平衡每个变量的影响。在下一节中，您将应用层次聚类分析到此数据集。
- en: 16.3 Hierarchical cluster analysis
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.3 层次聚类分析
- en: 'As stated, in agglomerative hierarchical clustering, each case or observation
    starts as its own cluster. Clusters are then combined two at a time until all
    clusters are merged into a single cluster. The algorithm is as follows:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 如所述，在聚合层次聚类中，每个案例或观测值最初都是一个单独的簇。然后每次合并两个簇，直到所有簇合并成一个包含所有观测值的单个簇。算法如下：
- en: Define each observation (row, case) as a cluster.
  id: totrans-51
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观测值（行、案例）定义为簇。
- en: Calculate the distances between every cluster and every other cluster.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个簇与每个其他簇之间的距离。
- en: Combine the two clusters that have the smallest distance. This reduces the number
    of clusters by one.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将具有最小距离的两个簇合并。这减少了簇的数量一个。
- en: Repeat steps 2 and 3 until all clusters have been merged into a single cluster
    containing all observations.
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤 2 和 3，直到所有簇都合并成一个包含所有观测值的单个簇。
- en: The primary difference among hierarchical clustering algorithms is their definitions
    of cluster distances (step 2). Table 16.1 lists five of the most common hierarchical
    clustering methods and their definitions of the distance between two clusters.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 层次聚类算法之间的主要区别在于它们对簇距离的定义（步骤 2）。表 16.1 列出了五种最常见的层次聚类方法及其定义的两个簇之间的距离。
- en: Table 16.1 Hierarchical clustering methods
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 表 16.1 层次聚类方法
- en: '| Cluster method | Definition of the distance between two clusters |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 簇方法 | 定义两个簇之间的距离 |'
- en: '| Single linkage | Shortest distance between a point in one cluster and a point
    in the other cluster |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 单链接 | 一个簇中的点与另一个簇中的点的最短距离 |'
- en: '| Complete linkage | Longest distance between a point in one cluster and a
    point in the other cluster |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 完全链接 | 一个簇中的点与另一个簇中的点的最长距离 |'
- en: '| Average linkage | Average distance between each point in one cluster and
    each point in the other cluster (*also called UPGMA [unweighted pair group mean
    averaging]*) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 平均链接 | 一个簇中的每个点与另一个簇中的每个点的平均距离（也称为 UPGMA [未加权配对组平均]）|'
- en: '| Centroid | Distance between the centroids (vector of variable means) of the
    two clusters. For a single observation, the centroid is the variable’s values.
    |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| 质心 | 两个簇的质心（变量均值向量）之间的距离。对于单个观测值，质心是变量的值。|'
- en: '| Ward | The ANOVA sum of squares between the two clusters added up over all
    the variables |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 瓦德（Ward） | 两个簇之间所有变量的方差分析平方和的总和 |'
- en: Single-linkage clustering tends to find elongated, cigar-shaped clusters. It
    also commonly displays a phenomenon called *chaining*—dissimilar observations
    are joined in the same cluster because they’re similar to intermediate observations
    between them. Complete-linkage clustering tends to find compact clusters of approximately
    equal diameter. It can also be sensitive to outliers. Average-linkage clustering
    offers a compromise between the two. It’s less likely to chain and is less susceptible
    to outliers. It also has a tendency to join clusters with small variances.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 单链聚类倾向于找到细长的、雪茄形的聚类。它也常见到一种称为*链式*的现象——不相似的观测值被合并到同一个聚类中，因为它们与它们之间的中间观测值相似。完全链聚类倾向于找到直径大致相等的紧凑聚类。它也可能对异常值敏感。平均链聚类在这两者之间提供了一个折中方案。它不太可能形成链式结构，并且对异常值不太敏感。它还倾向于将具有小方差差异的聚类合并在一起。
- en: Ward’s method tends to join clusters with small numbers of observations and
    tends to produce clusters with roughly equal numbers of observations. It can also
    be sensitive to outliers. The centroid method offers an attractive alternative
    due to its simple and easily understood definition of cluster distances. It’s
    also less sensitive to outliers than other hierarchical methods. But it may not
    perform as well as the average-linkage or Ward method.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 沃德方法倾向于将具有少量观测值的聚类合并在一起，并且倾向于产生观测值数量大致相等的聚类。它也可能对异常值敏感。由于其对聚类距离定义简单且易于理解，中心点方法提供了一个有吸引力的替代方案。它也比其他层次聚类方法对异常值不太敏感。但它可能不如平均链或沃德方法表现得好。
- en: Hierarchical clustering can be accomplished using the `hclust()` function. The
    format is `hclust(*d*, method=)`, where *`d`* is a distance matrix produced by
    the `dist()` function, and methods include `"single"`, `"complete"`, `"average"`,
    `"centroid"`, `and "ward"`.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`hclust()`函数实现层次聚类。其格式为`hclust(*d*, method=)`，其中*`d`*是由`dist()`函数生成的距离矩阵，方法包括`"single"`、`"complete"`、`"average"`、`"centroid"`和`"ward"`。
- en: In this section, you’ll apply average-linkage clustering to the `nutrient` data
    introduced in section 16.2 to identify similarities, differences, and groupings
    among 27 food types based on nutritional information. The following listing provides
    the code for carrying out the clustering.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将应用平均链聚类方法对第16.2节中引入的`nutrient`数据进行处理，以识别基于营养信息的27种食品类型之间的相似性、差异和分组。以下列表提供了执行聚类的代码。
- en: Listing 16.1 Average-linkage clustering of the nutrient data
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.1 平均链聚类营养数据
- en: '[PRE3]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: First, the data are imported, and the row names are set to lowercase (because
    I hate UPPERCASE LABELS). Because the variables differ widely in range, they’re
    standardized to a mean of 0 and a standard deviation of 1\. Euclidean distances
    between each of the 27 food types are calculated, and an average-linkage clustering
    is performed. Finally, the results are plotted as a dendrogram using the `ggplot2`
    and `ggdendro` packages (see figure 16.1).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入数据，并将行名设置为小写（因为我讨厌大写标签）。由于变量范围差异很大，它们被标准化为均值为0和标准差为1。计算27种食品类型之间的欧几里得距离，并执行平均链聚类。最后，使用`ggplot2`和`ggdendro`包将结果绘制成树状图（见图16.1）。
- en: '![](Images/CH16_F01_Kabacoff3.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH16_F01_Kabacoff3.png)'
- en: Figure 16.1 Average-linkage clustering of nutrient data
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.1 营养数据的平均链聚类
- en: The dendrogram displays how items are combined into clusters and is read from
    the bottom up. Each observation starts as its own cluster. Then the two observations
    that are closest (beef braised and smoked ham) are combined. Next, pork roast
    and pork simmered are combined, followed by chicken canned and tuna canned. In
    the fourth step, the beef braised/smoked ham cluster and the pork roast/pork simmered
    clusters are combined (and the cluster now contains four food items). This continues
    until all observations are combined into a single cluster. The height dimension
    indicates the criterion value at which clusters are joined. For average-linkage
    clustering, this criterion is the average distance between each point in one cluster
    and each point in the other cluster.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 树状图显示了项目如何组合成聚类，并且是从下往上读取的。每个观测值最初都是它自己的聚类。然后，最接近的两个观测值（红烧牛肉和熏火腿）被合并。接下来，烤猪肉和炖猪肉被合并，然后是罐装鸡肉和罐装金枪鱼。在第四步中，红烧牛肉/熏火腿聚类和烤猪肉/炖猪肉聚类被合并（现在该聚类包含四个食品项目）。这个过程一直持续到所有观测值都被合并成一个单一的聚类。高度维度表示聚类合并的准则值。对于平均链聚类，这个准则值是每个聚类中每个点与其他聚类中每个点之间的平均距离。
- en: If your goal is to understand how food types are similar or different in terms
    of their nutrients, then figure 16.1 may be sufficient. It creates a hierarchical
    view of the similarity/dissimilarity among the 27 items. Canned tuna and chicken
    are similar, and both differ greatly from canned clams. But if the end goal is
    to assign these foods to a smaller number of (hopefully meaningful) groups, additional
    analyses are required to select an appropriate number of clusters.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的目标是了解食物类型在营养方面的相似性或差异性，那么图16.1可能就足够了。它创建了27个项目中相似性/差异性的层次视图。罐装金枪鱼和鸡肉相似，并且两者与罐装蛤蜊都大相径庭。但如果最终目标是将这些食物分配到更少的（希望是有意义的）组中，则需要额外的分析来选择合适的簇数。
- en: The `NbClust` package offers numerous indices for determining the best number
    of clusters in a cluster analysis. There is no guarantee that they will agree
    with each other. In fact, they probably won’t. But the results can be used as
    a guide for selecting possible candidate values for *K*, the number of clusters.
    Input to the `NbClust()` function includes the matrix or data frame to be clustered,
    the distance measure and clustering method to employ, and the minimum and maximum
    number of clusters to consider. It returns each of the clustering indices along
    with the best number of clusters proposed by each. The next listing applies this
    approach to the average-linkage clustering of the nutrient data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`NbClust` 包提供了许多指标来确定聚类分析中最佳簇数。它们之间并不保证会达成一致。事实上，它们可能不会。但可以使用这些结果作为选择可能的候选值
    *K*（簇数）的指南。`NbClust()` 函数的输入包括要聚类的矩阵或数据框、要使用的距离度量以及聚类方法，以及要考虑的最小和最大簇数。它返回每个聚类指标以及每个指标提出的最佳簇数。下面的列表将此方法应用于营养数据的平均链聚类。'
- en: Listing 16.2 Selecting the number of clusters
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.2 选择簇数
- en: '[PRE4]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Here, two criteria favor zero clusters, one criterion favors one cluster, four
    criteria favor two clusters, and so on. The results are plotted using the `fviz_nbclust()`
    function (figure 16.2). The optimal number of clusters has the most votes. In
    the case of a tie, the solution with fewer clusters is usually favored.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，有两个标准有利于零个簇，一个标准有利于一个簇，四个标准有利于两个簇，依此类推。结果使用 `fviz_nbclust()` 函数绘制（图16.2）。拥有最多投票数的簇数被认为是最佳簇数。在出现平局的情况下，通常会选择簇数较少的解决方案。
- en: '![](Images/CH16_F02_Kabacoff3.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH16_F02_Kabacoff3.png)'
- en: Figure 16.2 Recom-mended number of clusters using 26 criteria provided by the
    `NbClust` package
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.2 使用 `NbClust` 包提供的26个标准推荐的簇数
- en: Although two clusters are suggested in the graph, you could also try 3-, 5-,
    and 15-cluster solutions and select the one that makes the most interpretive sense.
    The following listing explores the 5-cluster solution.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图中建议有两个簇，但你也可以尝试3个、5个和15个簇的解决方案，并选择最具解释意义的那个。下面的列表探讨了5个簇的解决方案。
- en: Listing 16.3 Obtaining the final cluster solution
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.3 获取最终的簇解决方案
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Assigns cases
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分配案例
- en: ❷ Describes clusters
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 描述簇
- en: ❸ Plots results
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 绘制结果
- en: The `cutree()` function is used to cut the tree into 5 clusters ❶. The first
    cluster has 7 observations, the second cluster has 16 observations, and so on.
    The `dplyr` functions are then used to obtain the median profile for each cluster
    ❷. Finally, the dendrogram is replotted, and the `colorhcplot` function is used
    to identify the 5 clusters ❸. Here, `cl` is a factor with the cluster labels,
    `hang=-1` aligns the labels at the bottom of the graph, `lab.cex` controls the
    size of the labels (80% of default here), and `lwd` controls the width of the
    dendrogram lines. Figure 16.3 displays the results. If you are using the printed
    version of this text, be sure to run this code. In greyscale, the color distinctions
    are difficult to see.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cutree()` 函数将树切割成5个簇 ❶。第一个簇有7个观测值，第二个簇有16个观测值，依此类推。然后使用 `dplyr` 函数为每个簇获取中位数轮廓
    ❷。最后，重新绘制树状图，并使用 `colorhcplot` 函数来识别5个簇 ❸。在这里，`cl` 是一个带有簇标签的因子，`hang=-1` 将标签对齐在图的底部，`lab.cex`
    控制标签的大小（这里为默认值的80%），`lwd` 控制树状图线的宽度。图16.3显示了结果。如果你使用的是文本的打印版，请确保运行此代码。在灰度图中，颜色区分难以辨认。
- en: '![](Images/CH16_F03_Kabacoff3.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH16_F03_Kabacoff3.png)'
- en: Figure 16.3 Average-linkage clustering of the nutrient data with a 5-cluster
    solution
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.3 使用5簇解决方案的营养数据平均链聚类
- en: Sardines form their own cluster and are much higher in calcium than the other
    food groups. Beef heart is also a singleton and is high in protein and iron. The
    clam cluster is low in protein and high in iron. The items in the cluster containing
    beef roast to pork simmered are high in energy and fat. Finally, the largest group
    (mackerel to bluefish) is relatively low in iron.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 鲨鱼形成自己的簇，其钙含量比其他食物组高得多。牛肉心也是一个单例，富含蛋白质和铁。蛤蜊簇蛋白质含量低，铁含量高。包含烤牛肉和炖猪肉的簇中的项目能量和脂肪含量高。最后，最大的组（鲭鱼到蓝鱼）铁含量相对较低。
- en: Hierarchical clustering can be particularly useful when you expect nested clustering
    and a meaningful hierarchy. This is often the case in the biological sciences.
    But the hierarchical algorithms are greedy in the sense that once an observation
    is assigned to a cluster, it can’t be reassigned later. Additionally, hierarchical
    clustering is difficult to apply in large samples that may include hundreds or
    even thousands of observations. Partitioning methods can work well in these situations.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当你预期嵌套聚类和有意义层次结构时，层次聚类特别有用。这在生物科学中通常是这种情况。但是，层次算法在贪婪的意义上，一旦一个观测值被分配到某个簇，它就不能在以后重新分配。此外，层次聚类在可能包括数百甚至数千个观测值的大样本中难以应用。分区方法在这些情况下可以很好地工作。
- en: 16.4 Partitioning-cluster analysis
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.4 分区聚类分析
- en: 'In the partitioning approach, observations are divided into *K* groups and
    reshuffled to form the most cohesive clusters possible according to a given criterion.
    This section considers two methods: k-means and partitioning around medoids (PAM).'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在分区方法中，观测值被分为*K*组，并重新排列以形成根据给定标准可能的最紧密的簇。本节考虑两种方法：k-means和基于中位数（PAM）的分区。
- en: 16.4.1 K-means clustering
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.1 K-means聚类
- en: 'The most common partitioning method is the k-means cluster analysis. Conceptually,
    the k-means algorithm is as follows:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的分区方法是k-means聚类分析。从概念上讲，k-means算法如下：
- en: Select *K* centroids (*K* rows chosen at random).
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择*K*个质心（随机选择*K*行）。
- en: Assign each data point to its closest centroid.
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个数据点分配到其最近的质心。
- en: Recalculate the centroids as the average of all data points in a cluster (that
    is, the centroids are *p*-length mean vectors where *p* is the number of variables).
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将质心重新计算为簇中所有数据点的平均值（即，质心是*p*-长度均值向量，其中*p*是变量的数量）。
- en: Assign data points to their closest centroids.
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据点分配到其最近的质心。
- en: Continue steps 3 and 4 until the observations aren’t reassigned or the maximum
    number of iterations (R uses 10 as a default) is reached.
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续执行步骤3和4，直到观测值不再重新分配或达到最大迭代次数（R默认为10）。
- en: Implementation details for this approach can vary.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法的实现细节可能有所不同。
- en: R uses an efficient algorithm by Hartigan and Wong (1979) that partitions the
    observations into *k* groups such that the sum of squares of the observations
    to their assigned cluster centers is a minimum. This means that in steps 2 and
    4, each observation is assigned to the cluster with the smallest value of
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: R使用Hartigan和Wong（1979年）的高效算法，将观测值划分为*k*组，使得观测值到其分配的簇中心的平方和最小。这意味着在步骤2和4中，每个观测值被分配到具有最小值的簇中，
- en: '![](Images/CH16_F03_Kabacoff3-EQ03.png)'
  id: totrans-102
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F03_Kabacoff3-EQ03.png)'
- en: where *k* is the cluster, *x[ij]* is the value of the *j^(th)* variable for
    the *i^(th)* observation, *x̄[kj]* is the mean of the *j^(th)* variable for the
    *k^(th)* cluster, and *p* is the number of variables.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*k*是簇，*x[ij]*是第*i*个观测值的第*j*个变量的值，*x̄[kj]*是第*k*个簇的第*j*个变量的均值，*p*是变量的数量。
- en: K-means clustering can handle larger datasets than hierarchical clustering approaches.
    Additionally, observations aren’t permanently committed to a cluster—they’re moved
    when doing so improves the overall solution. But the use of means implies that
    all variables must be continuous, and the approach can be severely affected by
    outliers. It also performs poorly in the presence of nonconvex (for example, U-shaped)
    clusters.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: K-means聚类方法可以处理比层次聚类方法更大的数据集。此外，观测值不会被永久性地分配到某个簇中——当这样做能改善整体解决方案时，它们会被移动。但是，使用均值意味着所有变量都必须是连续的，并且这种方法可能会受到异常值的影响。它还在存在非凸（例如，U形）簇的情况下表现不佳。
- en: The format of the k-means function in R is `kmeans(*x, centers*)`, where *`x`*
    is a numeric dataset (matrix or data frame) and *`centers`* is the number of clusters
    to extract. The function returns the cluster memberships, centroids, sums of squares
    (within, between, total), and cluster sizes.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: R中k-means函数的格式为`kmeans(*x, centers*)`，其中*`x`*是数值数据集（矩阵或数据框），而*`centers`*是要提取的簇的数量。该函数返回簇成员资格、质心、平方和（组内、组间、总平方和）以及簇大小。
- en: Because k-means cluster analysis starts with *k* randomly chosen centroids,
    a different solution can be obtained each time the function is invoked. Use the
    `set.seed()` function to guarantee that the results are reproducible. Additionally,
    this clustering approach can be sensitive to the initial selection of centroids.
    The `kmeans()` function has an `nstart` option that attempts multiple initial
    configurations and reports on the best one. For example, adding `nstart=25` generates
    25 initial configurations. This approach is often recommended.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 由于k-means聚类分析从随机选择的k个质心开始，每次调用函数时都可能得到不同的解决方案。使用`set.seed()`函数以确保结果可重复。此外，这种聚类方法可能对质心的初始选择敏感。`kmeans()`函数有一个`nstart`选项，它尝试多个初始配置并报告最佳配置。例如，添加`nstart=25`生成25个初始配置。这种方法通常被推荐。
- en: Unlike hierarchical clustering, k-means clustering requires that you specify
    in advance the number of clusters to extract. Again, the `NbClust` package can
    be used as a guide. Additionally, a plot of the total within-groups sums of squares
    against the number of clusters in a k-means solution can be helpful. A bend in
    the graph (similar to the bend in the Scree test described in section 14.2.1)
    can suggest the appropriate number of clusters.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 与层次聚类不同，k-means聚类要求您提前指定要提取的簇的数量。同样，`NbClust`包可以用作指南。此外，k-means解决方案中组内总平方和与簇数量之间的图表可能很有帮助。图表中的弯曲（类似于第14.2.1节中描述的Scree测试中的弯曲）可以表明合适的簇数量。
- en: 'The graph can be produced with the following function:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 该图可以使用以下函数生成：
- en: '[PRE6]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The `data` parameter is the numeric dataset to be analyzed, `nc` is the maximum
    number of clusters to consider, and `seed` is a random number seed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '`data`参数是要分析的数值数据集，`nc`是考虑的最大簇数，而`seed`是随机数种子。'
- en: Let’s apply k-means clustering to a dataset containing 13 chemical measurements
    on 178 Italian wine samples. The data originally comes from the UCI Machine Learning
    Repository ([www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html)),
    but you’ll access them here via the `rattle` package. In this dataset, the observations
    represent three wine varietals, as indicated by the first variable (`Type`). You’ll
    drop this variable, perform the cluster analysis, and see if you can recover the
    known structure.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将k-means聚类应用于包含178个意大利葡萄酒样品的13个化学测量的数据集。数据最初来自UCI机器学习仓库([www.ics.uci.edu/~mlearn/MLRepository.html](http://www.ics.uci.edu/~mlearn/MLRepository.html))，但您将通过`rattle`包在这里访问它们。在此数据集中，观测值代表三种葡萄酒品种，如第一个变量（`Type`）所示。您将删除此变量，执行聚类分析，并查看您是否可以恢复已知的结构。
- en: Listing 16.4 K-means clustering of wine data
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.4 葡萄酒数据的k-means聚类
- en: '[PRE7]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Standardizes the data
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标准化数据
- en: ❷ Determines the number of clusters
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 确定簇的数量
- en: ❸ Performs the k-means cluster analysis
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行k-means聚类分析
- en: Because the variables vary in range, they’re standardized prior to clustering
    ❶. Next, the number of clusters is determined using the `wssplot``()` and `NbClust()`
    functions ❷. Figure 16.4 indicates that there is a distinct drop in the within-groups
    sum of squares when moving from one to three clusters. After three clusters, this
    decrease drops off, suggesting that a three-cluster solution may be a good fit
    to the data. In figure 16.5, 19 of 23 criteria provided by the `NbClust` package
    suggest a three-cluster solution. Note that not all 30 criteria can be calculated
    for every dataset.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 由于变量范围不同，它们在聚类之前需要进行标准化 ❶。接下来，使用`wssplot()`和`NbClust()`函数确定簇的数量 ❷。图16.4表明，当从一到三个簇移动时，组内平方和有明显的下降。在三个簇之后，这种下降趋势减弱，表明三个簇的解决方案可能适合数据。在图16.5中，`NbClust`包提供的23个标准中有19个建议三个簇的解决方案。请注意，并非所有30个标准都可以用于每个数据集。
- en: '![](Images/CH16_F04_Kabacoff3.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH16_F04_Kabacoff3.png)'
- en: Figure 16.4 Plotting the within-groups sums of squares vs. the number of clusters
    extracted. The sharp decreases from one to three clusters (with little decrease
    thereafter) suggests a three-cluster solution.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.4 绘制组内平方和与提取簇数的关系图。从一到三个簇（之后减少很少）的急剧下降表明了三簇解决方案。
- en: A final cluster solution is obtained with the `kmeans()` function, and the cluster
    centroids are printed ❸. Because the centroids provided by the function are based
    on standardized data, the `aggregate()` function is used along with the cluster
    memberships to determine variable means for each cluster in the original metric.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`kmeans()`函数获得最终的簇解决方案，并打印簇中心点❸。因为函数提供的中心点基于标准化数据，所以使用`aggregate()`函数以及簇成员资格来确定原始度量中每个簇的变量均值。
- en: '![](Images/CH16_F05_Kabacoff3.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH16_F05_Kabacoff3.png)'
- en: Figure 16.5 Recommended number of clusters using 26 criteria provided by the
    `NbClust` package
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.5 使用`NbClust`包提供的26个标准推荐簇数
- en: The easiest way to compare the clusters is with a cluster profile plot. The
    following listing continues the example from listing 16.4.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 比较簇的最简单方法是通过簇轮廓图。以下列表继续了列表16.4中的示例。
- en: Listing 16.5 Cluster profile plots
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.5 簇轮廓图
- en: '[PRE8]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Prepares mean profiles
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备均值轮廓
- en: ❷ Converts data to long format
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据转换为长格式
- en: ❸ Plots profiles as faceted bar charts
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将轮廓绘制为分面条形图
- en: First, we get the cluster means on the standardized variables and add a variable
    representing cluster membership ❶. Then, we convert this wide-format data frame
    to long form (wide-to-long form conversion is described in section 5.5.2) ❷. Finally,
    we plot the profiles as faceted bar charts ❸. Figure 16.6 shows the results. The
    mean cluster profiles help you see what makes each cluster unique. For example,
    compared to clusters 2 and 4, cluster 1 has high mean scores on alcohol, phenols,
    proanthocyanins, and proline.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们得到标准化变量上的簇均值并添加一个表示簇成员资格的变量❶。然后，我们将这个宽格式数据框转换为长格式（宽到长格式转换在第5.5.2节中描述）❷。最后，我们将轮廓绘制为分面条形图❸。图16.6显示了结果。平均簇轮廓有助于您了解使每个簇独特的东西。例如，与簇2和簇4相比，簇1在酒精、酚类、原花青素和脯氨酸上的平均得分较高。
- en: '![](Images/CH16_F06_Kabacoff3.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH16_F06_Kabacoff3.png)'
- en: Figure 16.6 Mean profiles for each cluster based on standardized data. This
    graph helps identify the distinctive features of each cluster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.6 标准化数据中每个簇的均值轮廓。此图表有助于识别每个簇的独特特征。
- en: Another approach to visualizing the results of a cluster analysis is the *bivariate
    cluster plot*. The plot is created by plotting the coordinates of each observation
    (wine) on the first two principal components derived from the 13 assay variables.
    (Principal components are described chapter 14.) The color and shape of each point
    identifies its cluster membership. The point labels represent each wine’s row
    number in the data. Additionally, each cluster is surrounded by the smallest ellipse
    that can contain all the points in that cluster.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种可视化簇分析结果的方法是双变量簇图。该图通过将每个观测值（酒）的坐标绘制在由13个检测变量得出的前两个主成分上创建。 （主成分在第14章中描述。）每个点的颜色和形状标识其簇成员资格。点标签代表数据中每款酒行的编号。此外，每个簇周围都围绕着可以包含该簇所有点的最小椭圆。
- en: 'A bivariate cluster plot can be created with the `fviz_cluster()` function
    from the `factoextra` package:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用`factoextra`包中的`fviz_cluster()`函数创建双变量簇图：
- en: '[PRE9]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Figure 16.7 shows the graph. We can see that cluster 1 and cluster 3 are most
    dissimilar. Wines 4 and 19 are similar, while wines 4 and 171 are very different.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7显示了图表。我们可以看到簇1和簇3最不相似。酒4和酒19相似，而酒4和酒171非常不同。
- en: '![](Images/CH16_F07_Kabacoff3.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH16_F07_Kabacoff3.png)'
- en: Figure 16.7 Cluster plot of 178 wines clustered into 3 groups. Each wine is
    plotted against the first two principal components of the data. The graphs can
    help us see the similarities/differences between wines and between clusters.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.7 将178种酒分为3组的簇图。每种酒都绘制在数据的第一个和第二个主成分上。这些图表可以帮助我们看到酒与酒之间以及簇与簇之间的相似性/差异性。
- en: Cluster analysis is usually an unsupervised technique in that there is no outcome
    variable we are trying to predict. However, in the wine example, there are actually
    three wine varietals (`Type`) in the dataset.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 簇分析通常是一种无监督技术，因为我们没有试图预测的输出变量。然而，在葡萄酒示例中，数据集中实际上有三个葡萄酒品种（`Type`）。
- en: How well did k-means clustering uncover the actual structure of the data contained
    in the `Type` variable? A cross-tabulation of `Type` (wine varietal) and cluster
    membership is given by
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: k-means聚类如何揭示`Type`变量中包含的数据的实际结构？`Type`（葡萄酒品种）和聚类成员资格的交叉表如下所示
- en: '[PRE10]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'You can quantify the agreement between type and cluster using an adjusted Rand
    index, provided by the `flexclust` package:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`flexclust`包提供的调整后的兰德指数来量化类型和聚类之间的一致性。
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The adjusted Rand index provides a measure of the agreement between two partitions,
    adjusted for chance. It ranges from –1 (no agreement) to 1 (perfect agreement).
    Agreement between the wine varietal type and the cluster solution is 0.9\. Not
    bad—shall we have some wine?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的兰德指数提供了两个分区之间一致性的度量，考虑了偶然性。它介于-1（无一致性）到1（完全一致性）之间。葡萄酒品种类型与聚类解决方案之间的一致性为0.9。不错——我们喝点酒如何？
- en: 16.4.2 Partitioning around medoids
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 16.4.2 基于中位数的聚类
- en: Because it’s based on means, the k-means clustering approach can be sensitive
    to outliers. A more robust solution is provided by partitioning around medoids
    (PAM). Rather than representing each cluster using a centroid (a vector of variable
    means), each cluster is identified by its most representative observation (called
    a *medoid*). Whereas k-means uses Euclidean distances, PAM can be based on any
    distance measure. It can therefore accommodate mixed data types and isn’t limited
    to continuous variables.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它基于均值，k-means聚类方法可能对异常值敏感。基于中位数的聚类（PAM）提供了一种更稳健的解决方案。PAM不是使用质心（变量均值的向量）来表示每个聚类，而是通过其最具代表性的观测值（称为*中位数*）来识别每个聚类。与k-means使用欧几里得距离不同，PAM可以基于任何距离度量。因此，它可以适应混合数据类型，并且不仅限于连续变量。
- en: 'The PAM algorithm is as follows:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PAM算法如下：
- en: Randomly select *K* observations (call each a medoid).
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择*K*个观测值（每个称为中位数）。
- en: Calculate the distance/dissimilarity of every observation to each medoid.
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个观测值与每个中位数之间的距离/不相似度。
- en: Assign each observation to its closest medoid.
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个观测值分配到其最近的中位数。
- en: Calculate the sum of the distances of each observation from its medoid (total
    cost).
  id: totrans-150
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个观测值与其中位数之间的距离之和（总成本）。
- en: Select a point that isn’t a medoid and swap it with its medoid.
  id: totrans-151
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个不是中位数的点，并将其与其中位数交换。
- en: Reassign every point to its closest medoid.
  id: totrans-152
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将每个点重新分配到其最近的中位数。
- en: Calculate the total cost.
  id: totrans-153
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算总成本。
- en: If this total cost is smaller, keep the new point as a medoid.
  id: totrans-154
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这个总成本更小，则保留新点作为中位数。
- en: Repeat steps 5-8 until the medoids don’t change.
  id: totrans-155
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤5-8，直到中位数不再改变。
- en: A good, worked example of the underlying math in the PAM approach can be found
    at [http://en.wikipedia.org/wiki/k-medoids](http://en.wikipedia.org/wiki/k-medoids)
    (I don’t usually cite Wikipedia, but this is a great example).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在[PAM方法中潜在数学的好的、工作过的例子可以在[http://en.wikipedia.org/wiki/k-medoids](http://en.wikipedia.org/wiki/k-medoids)找到（我通常不引用维基百科，但这是一个很好的例子）。
- en: You can use the `pam()` function in the `cluster` package to partition around
    medoids. The format is `pam(*x*, *k*, metric="euclidean", stand=FALSE)`, where
    *`x`* is a data matrix or data frame, *`k`* is the number of clusters, `metric`
    is the type of distance/dissimilarity measure to use, and `stand` is a logical
    value indicating whether the variables should be standardized before calculating
    this metric. In the following listing, PAM is applied to the wine data.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用`cluster`包中的`pam()`函数进行基于中位数聚类。格式为`pam(*x*, *k*, metric="euclidean", stand=FALSE)`，其中*`x`*是一个数据矩阵或数据框，*`k`*是聚类数量，`metric`是使用的距离/不相似度度量类型，而`stand`是一个逻辑值，表示在计算此度量之前是否应对变量进行标准化。在以下列表中，PAM应用于葡萄酒数据。
- en: Listing 16.6 Partitioning around medoids for the wine data
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表16.6葡萄酒数据的基于中位数的聚类
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Clusters standardized data
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标准化聚类数据
- en: ❷ Prints the medoids
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印中位数
- en: Note that the medoids are actual observations contained in the `wine` dataset.
    In this case, they’re observations 36, 107, and 175, and they have been chosen
    to represent the three clusters.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，中位数是包含在`wine`数据集中的实际观测值。在这种情况下，它们是观测值36、107和175，并且它们已被选中以代表三个聚类。
- en: 'Also note that PAM didn’t perform as well as k-means in this instance:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，在这个例子中，PAM的表现不如k-means好：
- en: '[PRE13]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The adjusted Rand index has decreased from 0.9 (for k-means) to 0.7\. Creation
    of the cluster profile plot and bivariate cluster plot is left as an exercise.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 调整后的兰德指数已从0.9（k-means）降至0.7。创建聚类轮廓图和双变量聚类图留作练习。
- en: 16.5 Avoiding nonexistent clusters
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.5 避免不存在的聚类
- en: Before I finish this discussion, a word of caution is in order. Cluster analysis
    is a methodology designed to identify cohesive subgroups in a dataset. It’s very
    good at doing this. In fact, it’s so good, it can find clusters where none exist.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在我完成这次讨论之前，需要提醒一点。聚类分析是一种旨在识别数据集中凝聚子群的方法。它在这方面做得非常好。事实上，它太好了，甚至可以在不存在聚类的地方找到聚类。
- en: 'Consider the following code:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下代码：
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `rnorm2d()` function in the `fMultivar` package is used to sample 1,000
    observations from a bivariate normal distribution with a correlation of 0.5\.
    Figure 16.8 shows the resulting graph. Clearly, there are no clusters in this
    data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`fMultivar`包中的`rnorm2d()`函数用于从相关系数为0.5的双变量正态分布中抽取1,000个观测值。图16.8显示了生成的图形。显然，这些数据中没有聚类。'
- en: '![](Images/CH16_F08_Kabacoff3.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F08_Kabacoff3.png)'
- en: Figure 16.8 Bivariate normal data (n = 1000). There are no clusters in this
    data.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.8 双变量正态数据（n = 1000）。这些数据中没有聚类。
- en: 'The `wssplot``()` and `NbClust()` functions are then used to determine the
    number of clusters present:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用`wssplot()`和`NbClust()`函数来确定存在的聚类数量：
- en: '[PRE15]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Figures 16.9 and 16.10 plot the results.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9和16.10展示了结果。
- en: '![](Images/CH16_F09_Kabacoff3.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F09_Kabacoff3.png)'
- en: Figure 16.9 Plot of within-groups sums of squares vs. number of k-means clusters
    for bivariate normal data
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.9 双变量正态数据中组内平方和与k-means聚类数量的关系图
- en: '![](Images/CH16_F10_Kabacoff3.png)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F10_Kabacoff3.png)'
- en: Figure 16.10 Number of clusters recommended for bivariate normal data by criteria
    in the `NbClust` package. Two clusters are suggested.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.10 `NbClust`包中根据标准推荐的用于双变量正态数据的聚类数量。建议有两个聚类。
- en: Both approaches suggest at least two clusters. If you carry out a two-cluster
    analysis with k-means,
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 两种方法都表明至少有两个聚类。如果你使用k-means进行两聚类分析，
- en: '[PRE16]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: you get the two-cluster plot shown in figure 16.11.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你会得到图16.11中显示的两个聚类图。
- en: '![](Images/CH16_F11_Kabacoff3.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F11_Kabacoff3.png)'
- en: Figure 16.11 K-means cluster analysis of bivariate normal data, extracting two
    clusters. Note that the clusters are an arbitrary division of the data.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.11 双变量正态数据的K-means聚类分析，提取出两个聚类。请注意，聚类是对数据的任意划分。
- en: 'Clearly, the partitioning is artificial. There are no real clusters here. How
    can you avoid this mistake? Although it isn’t foolproof, I have found two approaches
    to be helpful. The first is the DIP test provided by the `clusterability` package:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这种划分是人为的。这里没有真实的聚类。你如何避免这种错误？虽然这不是万无一失的，但我发现两种方法是有帮助的。第一种是`clusterability`包提供的DIP测试：
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The `df[-3]` drops the factor variable (cluster membership) from the data. The
    null hypothesis is that there is a single cluster (mode). Since p > .05, we can’t
    reject the hypothesis. The data do not support a cluster structure.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '`df[-3]`从数据中删除了因子变量（聚类成员）。零假设是只有一个聚类（模态）。由于p > .05，我们不能拒绝这个假设。数据不支持聚类结构。'
- en: The other approach uses the Cubic Clustering Criteria (CCC) reported by `NbClust`.
    CCC can often help uncover situations where no structure exists. The code is
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法使用由`NbClust`报告的立方聚类准则（CCC）。CCC通常有助于揭示不存在结构的情况。代码如下：
- en: '[PRE18]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: and the resulting graph is displayed in figure 16.12\. When the CCC values are
    all negative and decreasing for two or more clusters, the distribution is typically
    unimodal.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的图形显示在图16.12中。当两个或更多聚类的CCC值都是负数且递减时，分布通常是单峰的。
- en: '![](Images/CH16_F12_Kabacoff3.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH16_F12_Kabacoff3.png)'
- en: Figure 16.12 Cubic clustering criteria plot for bivariate normal data. It correctly
    suggests that no clusters are present.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图16.12 双变量正态数据的立方聚类准则图。它正确地表明不存在聚类。
- en: The ability of cluster analysis (or your interpretation of it) to find erroneous
    clusters makes the validation step of cluster analysis important. If you’re trying
    to identify clusters that are real in some sense (rather than a convenient partitioning),
    be sure the results are robust and repeatable. Try different clustering methods,
    and replicate the findings with new samples. If the same clusters are consistently
    recovered, you can be more confident in the results.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析（或你对它的解释）能够找到错误聚类的能力使得聚类分析的验证步骤变得重要。如果你试图识别在某种意义上真实的聚类（而不是一个方便的划分），确保结果稳健且可重复。尝试不同的聚类方法，并使用新的样本重复研究结果。如果相同的聚类始终被恢复，你可以更有信心地相信结果。
- en: 16.6 Going further
  id: totrans-194
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 16.6 进一步探讨
- en: Cluster analysis is a broad topic, and R has some of the most comprehensive
    facilities currently available for applying this methodology. To learn more about
    these capabilities, see the CRAN Task View for Cluster Analysis & Finite Mixture
    Models ([http://cran.r-project.org/web/views/Cluster.html](http://cran.r-project.org/web/views/Cluster.html)).
    Additionally, Tan, Steinbach, and Kumar (2006) have an excellent book on data
    mining techniques that includes a lucid chapter on cluster analysis that you can
    downloaded freely ([www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf](http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf)).
    Finally, Everitt, Landau, Leese, and Stahl (2011) have written a practical and
    highly regarded textbook on this subject.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 聚类分析是一个广泛的主题，R 语言目前提供了应用这一方法的最全面的功能之一。要了解更多关于这些功能的信息，请参阅CRAN任务视图中的聚类分析与有限混合模型（[http://cran.r-project.org/web/views/Cluster.html](http://cran.r-project.org/web/views/Cluster.html)）。此外，Tan、Steinbach和Kumar（2006）有一本关于数据挖掘技术的优秀书籍，其中包括一个关于聚类分析的清晰章节，您可以免费下载（[www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf](http://www-users.cs.umn.edu/~kumar/dmbook/ch8.pdf)）。最后，Everitt、Landau、Leese和Stahl（2011）编写了一本关于这个主题的实用且备受推崇的教科书。
- en: Summary
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Cluster analysis is a common approach for arranging observations into cohesive
    groups.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分析是将观测值排列成紧密群体的常见方法。
- en: Since there is no single definition of what we mean by a “cluster,” or by what
    we mean by the “distance between clusters,” many clustering approaches have been
    developed.
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们对“聚类”或“聚类之间的距离”没有统一的定义，因此已经开发了许多聚类方法。
- en: Two of the most popular categories of cluster analysis are hierarchical clustering
    and partitioning. There are many clustering approaches within each category. No
    single approach is best in all situations.
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分析中最受欢迎的两个类别是层次聚类和划分聚类。每个类别中都有许多聚类方法。没有一种方法在所有情况下都是最好的。
- en: There is also no one best approach for determining the number of clusters in
    a dataset. It may be valuable to try several different approaches and settle on
    the one that is most meaningful or practically useful.
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在确定数据集中聚类数量时，也没有一种最佳方法。尝试几种不同的方法并选择最有意义或最实用的方法可能是有价值的。
- en: Cluster analysis can uncover clusters whether they exist are not! If your goal
    is to divide the data into convenient, cohesive groups (e.g., customer segmentation),
    this may be fine. However, if you are seeking to uncover naturally occurring groups
    with theoretically meaningful differences (e.g., subtypes of depression based
    on symptoms and history), it’s important to validate your findings by replicating
    them with new data.
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类分析可以揭示是否存在聚类！如果您的目标是将数据划分为方便的、紧密的群体（例如，客户细分），这可能就足够了。然而，如果您正在寻求揭示具有理论意义的自然发生的群体（例如，基于症状和病史的抑郁症亚型），那么通过用新数据重复验证您的发现是很重要的。
