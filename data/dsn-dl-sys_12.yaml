- en: Appendix B. Survey of existing solutions
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 B. 现有解决方案概述
- en: Deep learning systems are huge undertakings when implemented from scratch. In
    some situations, special requirements may warrant spending the extra effort to
    build a deep learning system from scratch. In other cases, given finite resources
    and time, it may make sense to use existing components, or even systems as a whole,
    and tailor them to your own needs.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 从零开始实施深度学习系统是一项巨大的工作。在某些情况下，特殊要求可能需要投入额外努力从头开始构建深度学习系统。在其他情况下，考虑到有限的资源和时间，使用现有组件，甚至整个系统，并根据自身需求进行定制可能是有意义的。
- en: The purpose of this appendix is to examine a few deep learning systems that
    have been implemented by different cloud vendors and open source communities.
    These operations range from serverless deployment to custom service container
    deployment. You will gain a sense of which pieces of these operations you can
    use to design your own project by comparing them with our reference architecture
    and highlighting their similarities and differences.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录的目的是检查不同云供应商和开源社区实施的一些深度学习系统。这些操作范围从无服务器部署到自定义服务容器部署。通过将它们与我们的参考架构进行比较，并突出它们的相似之处和不同之处，你将获得一种感觉，了解你可以使用哪些操作来设计你自己的项目。
- en: If you want to see a quick summarized comparison of every solution that we will
    cover, feel free to skip ahead to section B.5\. Also, for your convenience, the
    reference architecture introduced in section 1.2.1 is reposted in figure B.1.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想快速总结比较我们将涵盖的每个解决方案，请随意跳转到 B.5 部分。此外，为了方便起见，第 1.2.1 节中引入的参考架构在图 B.1 中重新发布。
- en: '![](../Images/B-1.png)'
  id: totrans-4
  prefs: []
  type: TYPE_IMG
  zh: '![图 B-1](../Images/B-1.png)'
- en: Figure B.1 An overview of a typical deep learning system that includes basic
    components to support a deep learning development cycle. This reference architecture
    can be used as a starting point and further tailored.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 图 B.1 一个典型的深度学习系统概述，包括支持深度学习开发周期的基本组件。这个参考架构可以作为起点，并进一步定制。
- en: B.1 Amazon SageMaker
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.1 Amazon SageMaker
- en: Amazon SageMaker is the umbrella term for its collection of artificial intelligence
    products that can be used together to form a complete deep learning system. In
    this section, we will review the suite of products and see how they compare with
    our key components. As mentioned at the beginning of this section, we make these
    comparisons so that you will learn what product will best help to build your own
    system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 是其人工智能产品集合的统称，这些产品可以一起使用，形成一个完整的深度学习系统。在本节中，我们将回顾产品套件，并了解它们如何与我们的关键组件进行比较。如本节开头所述，我们进行这些比较是为了让你了解哪个产品将最能帮助你构建自己的系统。
- en: B.1.1 Dataset management
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.1 数据集管理
- en: Amazon SageMaker does not offer a dataset management component that provides
    a unified interface to help manage the complex interaction of data preparation
    with the different types of users of a deep learning system. Amazon, however,
    does provide a collection of data storage, transformation, and querying solutions
    that can be used to build a data management component.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker 不提供提供统一接口的数据集管理组件，以帮助管理数据准备与深度学习系统不同类型用户之间的复杂交互。然而，亚马逊确实提供了一系列数据存储、转换和查询解决方案，可用于构建数据管理组件。
- en: It is possible to build a data management component that collects raw data for
    Amazon S3, an object storage product. Metadata tagging can be backed by AWS Glue
    Data Catalog, which can be used by AWS Glue ETL for further processing into datasets
    that can be used for training. After reading chapter 2, you should be able to
    identify how you can use these Amazon products to build your own data management
    component.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能构建一个数据管理组件，用于收集 Amazon S3 的原始数据，这是一个对象存储产品。元数据标记可以由 AWS Glue 数据目录支持，它可以被
    AWS Glue ETL 用于进一步处理成可用于训练的数据集。阅读第 2 章后，你应该能够识别出如何使用这些 Amazon 产品来构建你自己的数据管理组件。
- en: B.1.2 Model training
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.2 模型训练
- en: Amazon SageMaker supports both built-in algorithms and externally provided custom
    code for training deep learning models. It also supports containers for training
    runs. It exposes an API that can be called to launch training jobs on demand.
    This is largely similar to the compute backend that powers the training component
    of a deep learning system, which is covered in this book. To implement the resource
    management portion of the training component, you may use the existing tools provided
    by Amazon, such as assigning resource limits and policies to different AWS Identity
    and Access Management (IAM) users or roles. If your organization requires extra
    control or sophistication or already has an identity provider implementation,
    you may need to spend more time building a custom solution. After reading chapters
    3 and 4, you should be able to figure out how you can build your own training
    component with existing Amazon tools.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 支持用于训练深度学习模型的内置算法和外部提供的自定义代码。它还支持训练运行的容器。它公开了一个 API，可以调用以按需启动训练作业。这基本上与本书中涵盖的深度学习系统训练组件的计算后端类似。为了实现训练组件的资源管理部分，您可以使用亚马逊提供的现有工具，例如为不同的
    AWS 身份和访问管理 (IAM) 用户或角色分配资源限制和政策。如果您的组织需要额外的控制或复杂性或已经实施了身份提供者实现，您可能需要花费更多时间构建自定义解决方案。阅读第
    3 章和第 4 章后，您应该能够弄清楚如何使用现有的亚马逊工具构建自己的训练组件。
- en: B.1.3 Model serving
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.3 模型托管
- en: Amazon SageMaker, in its most basic form, supports deploying trained models
    as web services that are accessible over the internet. For scaling out to host
    multiple models without deploying each of them to separate endpoints, SageMaker
    provides a multimodel endpoint that also comes with configurable model-caching
    behavior. These tools can come in handy if they fit your bill. As of the time
    of this writing, SageMaker supports multi-container endpoints and serial inference
    pipelines, which are similar to the serving architecture and DAG support described
    in this book. Chapters 6 and 7 review model-serving principles so that you will
    understand what existing tools you can use and how you can build your own when
    you encounter limitations with existing tools.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊 SageMaker 在其最基本的形式下，支持将训练好的模型作为可通过互联网访问的 Web 服务进行部署。为了在不将每个模型部署到单独端点的情况下扩展以托管多个模型，SageMaker
    提供了一个多模型端点，该端点还附带可配置的模型缓存行为。如果这些工具符合您的需求，它们将非常有用。截至本文撰写之时，SageMaker 支持多容器端点和串行推理管道，这与本书中描述的托管架构和
    DAG 支持类似。第 6 章和第 7 章回顾了模型托管原则，以便您了解您可以使用哪些现有工具，以及当您遇到现有工具的限制时，您如何构建自己的工具。
- en: B.1.4 Metadata and artifacts store
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.4 元数据和工件存储
- en: As the component that centers around trained models, it is not surprising to
    see cloud vendors make products that do just that. The SageMaker Model Registry
    provides functionalities that map to many key concepts of the metadata and artifacts
    store of a deep learning system. For example, metadata, such as training metrics
    of a model and model versions, can be tracked using Model Registry. It does not,
    however, provide a storage solution for artifacts in the same component. You can
    easily build an interface on top of Model Registry and other Amazon storage products
    to provide the artifact storage aspect of this component.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为围绕训练模型的核心组件，云服务提供商推出仅此而已的产品并不令人惊讶。SageMaker 模型注册表提供了与深度学习系统的元数据和工件存储的关键概念相对应的功能。例如，可以使用模型注册表跟踪元数据，如模型的训练指标和模型版本。然而，它并不提供同一组件中工件存储的解决方案。您可以在模型注册表和其他亚马逊存储产品之上轻松构建一个接口，以提供该组件的工件存储方面。
- en: Another important type of metadata that is tracked between artifacts is their
    lineage information. SageMaker provides ML Lineage Tracking as a separate feature
    that keeps tabs on this information automatically.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在工件之间跟踪的另一种重要类型的元数据是它们的血缘信息。SageMaker 提供了作为独立功能的 ML 血缘跟踪，它会自动跟踪这些信息。
- en: In chapter 8, we will discuss key concerns in building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you to build your own quickly.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们将讨论构建元数据和工件存储的关键问题。阅读该章节后，您将了解该组件背后的设计原则以及现有产品如何帮助您快速构建自己的工具。
- en: B.1.5 Workflow orchestration
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.5 工作流编排
- en: On Amazon SageMaker, you can use the Model Building Pipelines product to manage
    your workflows, or *pipelines* (which is SageMaker terminology). Using this product,
    you can execute a set of actions, such as data preparation steps, training steps,
    and model validation steps, as one unit with a predefined order in an arbitrary
    fashion. To allow multiple types of users to work on the same problem, SageMaker
    also provides a Project product to help organize relationships between workflows,
    code versions, lineage information, and different access permissions for each
    user type.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在Amazon SageMaker上，您可以使用模型构建管道产品来管理您的工作流程，或者称为*管道*（这是SageMaker的术语）。使用此产品，您可以以预定义的顺序以任意方式将一系列操作，如数据准备步骤、训练步骤和模型验证步骤，作为一个单元执行。为了允许多种类型的用户在同一问题上工作，SageMaker还提供了一个项目产品，以帮助组织工作流程、代码版本、谱系信息和不同用户类型的访问权限之间的关系。
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system, as well
    as its role in an enterprise environment.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我们回顾了如何使用工作流程管理器来启用不同的训练模式。阅读完本章后，您将理解深度学习系统中工作流程管理器的设计和实用性的原因，以及它在企业环境中的作用。
- en: B.1.6 Experimentation
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.1.6 实验
- en: Amazon SageMaker provides a feature called Experiments that tags experimental
    runs with relevant tracking information and metrics. Indeed, this type of tracking
    information is also a kind of metadata, which is important to users of a deep
    learning system who need to evaluate the performance of different combinations
    of data input, training algorithms, and hyperparameters.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon SageMaker提供了一个名为“实验”的功能，该功能使用相关的跟踪信息和指标标记实验运行。实际上，这种跟踪信息也是一种元数据，对于需要评估不同数据输入组合、训练算法和超参数性能的深度学习系统用户来说非常重要。
- en: B.2 Google Vertex AI
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.2 Google Vertex AI
- en: Google Vertex AI, a combination of Google’s AI platform offering and its AutoML
    product, provides a collection of tools and services that can be used as a deep
    learning system. In this section, we will review its offerings and compare them
    with key components introduced in this book.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI，结合了谷歌的AI平台服务及其AutoML产品，提供了一系列工具和服务，这些工具和服务可以用作深度学习系统。在本节中，我们将回顾其提供的功能，并将它们与本书中介绍的关键组件进行比较。
- en: B.2.1 Dataset management
  id: totrans-26
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.1 数据集管理
- en: Google Vertex AI provides a simple API to manage datasets, though you must first
    upload your object data to Google Cloud Storage and then upload metadata and annotation
    files that reference object data in Google Cloud Storage via the Vertex AI API.
    The dataset API is similar across different types of datasets (images, text, video,
    etc.) that provide a unified experience to developers. The API, however, does
    not provide versioning information and other lineage tracking information. In
    chapter 2, we explore core data management principles. After reading the chapter,
    you will be able to compare existing solutions and extend them or build from scratch
    for your own needs.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI提供了一个简单的API来管理数据集，尽管您必须首先将您的对象数据上传到谷歌云存储，然后通过Vertex AI API上传元数据和注释文件，这些文件引用谷歌云存储中的对象数据。数据集API在不同类型的数据集（图像、文本、视频等）之间相似，为开发者提供了一致的使用体验。然而，该API不提供版本信息和其他谱系跟踪信息。在第2章中，我们探讨了核心数据管理原则。阅读完本章后，您将能够比较现有解决方案，并根据您的需求进行扩展或从头开始构建。
- en: B.2.2 Model training
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.2 模型训练
- en: Google Vertex AI supports training with Docker containers. It provides prebuilt
    training containers for those who do not need further customization and also supports
    custom-built training containers for those who require more than what the prebuilt
    flavor provides. Its training service exposes an interface that allows the launching
    of training runs on either a single node or on multiple nodes for distributed
    training. When running distributed training, Vertex AI provides additional support
    with reduction to accelerate training. In chapters 3 and 4, we explore these features
    and the principles behind them. After reading these chapters, you will be able
    to determine what existing offerings you can use, how to extend them if you need
    more, and how to build it from scratch if you have more specific requirements.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI支持使用Docker容器进行训练。它为不需要进一步定制的用户提供预构建的训练容器，同时也支持为需要比预构建版本提供更多功能的用户提供自定义构建的训练容器。其训练服务提供了一个接口，允许在单个节点或多个节点上启动训练运行，以实现分布式训练。在运行分布式训练时，Vertex
    AI提供额外的支持，通过减少来加速训练。在第3章和第4章中，我们探讨了这些功能和背后的原理。阅读这些章节后，您将能够确定可以使用哪些现有产品，如果需要更多功能，如何扩展它们，以及如果您有更具体的要求，如何从头开始构建。
- en: B.2.3 Model serving
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.3 模型服务
- en: Google Vertex AI supports serving online inference requests to trained models,
    either with prebuilt inference containers or custom inference containers. Trained
    models are decoupled from containers and must be deployed with compute resources
    to form an endpoint that can serve online inference requests. Vertex AI supports
    deploying one model to multiple endpoints and supports deploying multiple models
    to a single endpoint. Different from other solutions that support various model
    types, deploying multiple models to a single endpoint in Vertex AI is primarily
    used for canarying new model versions using split traffic patterns. In Vertex
    AI, if you train a Vertex AI video model, it cannot be made to serve online inference
    requests.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI支持使用预构建推理容器或自定义推理容器将在线推理请求服务于训练好的模型。训练好的模型与容器解耦，必须与计算资源一起部署，形成一个可以服务于在线推理请求的端点。Vertex
    AI支持将单个模型部署到多个端点，并支持将多个模型部署到单个端点。与支持各种模型类型的其他解决方案不同，Vertex AI中将多个模型部署到单个端点主要用于使用分割流量模式进行新模型版本的灰度发布。在Vertex
    AI中，如果您训练了一个Vertex AI视频模型，则无法使其服务于在线推理请求。
- en: In chapters 6 and 7, we learn the fundamental principles behind model serving.
    After completing these chapters, you will have a good understanding of model serving
    and will be able to decide whether existing solutions are sufficient for your
    needs. You will be able to build your own, as well as understand how to operate
    a model server efficiently and at scale.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章和第7章中，我们学习了模型服务的根本原则。完成这些章节后，您将对模型服务有一个良好的理解，并能够判断现有解决方案是否满足您的需求。您将能够构建自己的解决方案，以及了解如何高效且大规模地操作模型服务器。
- en: B.2.4 Metadata and artifacts store
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.4 元数据和工件存储
- en: Vertex ML Metadata is Google’s metadata store solution that can be used in a
    deep learning system. It uses a graph to describe relationships between artifacts
    such as datasets, training runs, and trained models. Each node and edge in the
    graph can be tagged with a list of key-value pairs to describe any metadata. When
    used properly, this can provide comprehensive lineage information for everything
    in a deep learning system.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex ML Metadata是Google的元数据存储解决方案，可以在深度学习系统中使用。它使用图来描述数据集、训练运行和训练模型等工件之间的关系。图中的每个节点和边都可以用键值对列表进行标记，以描述任何元数据。当正确使用时，这可以为深度学习系统中的所有内容提供全面的血缘信息。
- en: Artifacts are not stored directly in Vertex ML Metadata. Artifacts are stored
    in Google Cloud Storage. Vertex ML Metadata uses a URI reference to point to these
    artifacts.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 工件不是直接存储在Vertex ML Metadata中。工件存储在Google Cloud Storage中。Vertex ML Metadata使用URI引用来指向这些工件。
- en: In chapter 8, we will explore a similar approach in building a metadata and
    artifacts store, where both can be managed through a single, unified interface.
    After reading the chapter, you will be able to tell how to leverage and extend
    existing solutions for your needs.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章中，我们将探讨在构建元数据和工件存储时采用的一种类似方法，其中两者都可以通过一个单一、统一的接口进行管理。阅读该章节后，您将能够了解如何利用和扩展现有解决方案来满足您的需求。
- en: B.2.5 Workflow orchestration
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.5 工作流程编排
- en: With Google, you can use Vertex Pipelines to manage and operate your deep learning
    workflows. You can represent data preparation and training operations as steps
    in a pipeline. In Vertex Pipelines, steps are organized as nodes in a directed
    acyclic graph. Each step of the pipeline is implemented by a container. A run
    of a pipeline is in fact an orchestration of executions of containers.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 使用谷歌，你可以使用Vertex Pipelines来管理和操作你的深度学习工作流程。你可以将数据准备和训练操作表示为管道中的步骤。在Vertex Pipelines中，步骤被组织为有向无环图中的节点。管道的每个步骤都由一个容器实现。实际上，管道的运行是容器执行编排。
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在第9章中，我们回顾了如何使用工作流程管理器来启用不同的训练模式。阅读完本章后，你将理解深度学习系统中工作流程管理器的设计和实用性的原因，以及它在企业环境中的作用。
- en: B.2.6 Experimentation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.2.6 实验
- en: Google Vertex AI Experiments provides a unified UI to create, track, and manage
    experiments. The Vertex AI SDK provides autologging support for model training
    code to record hyperparameters, metrics, and data lineage. When paired with Vertex
    ML Metadata, you can get a complete overview of all your model training experiment
    runs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Google Vertex AI Experiments提供了一个统一的用户界面来创建、跟踪和管理实验。Vertex AI SDK为模型训练代码提供自动记录支持，以记录超参数、指标和数据血缘。当与Vertex
    ML Metadata结合使用时，你可以获得所有模型训练实验运行的完整概述。
- en: B.3 Microsoft Azure Machine Learning
  id: totrans-42
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.3 微软Azure机器学习
- en: Different from the classic ML Studio offering from Microsoft that focuses on
    a GUI approach to machine learning, Azure Machine Learning is a new suite of tools
    and services that also supports a wide variety of customization using code and
    established open source frameworks. In this section, we will compare their offerings
    to key components that are described in this book. After completing this section,
    you will gain a sense of what you can use as is, what you can extend, and what
    you need to build from scratch to fulfill your requirements.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 与微软经典的ML Studio提供的产品不同，ML Studio侧重于机器学习的GUI方法，Azure机器学习是一套新的工具和服务，它还支持使用代码和成熟的开源框架进行广泛的定制。在本节中，我们将比较它们的提供内容与本书中描述的关键组件。完成本节后，你将了解你可以直接使用什么，你可以扩展什么，以及你需要从头开始构建什么以满足你的需求。
- en: B.3.1 Dataset management
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.1 数据集管理
- en: On Azure Machine Learning, datasets are first-class objects that are inputs
    and outputs of data processing and training tasks. Datasets are defined as a collection
    of metadata associated with the raw data of the dataset. The dataset references
    its raw data via a URI reference to underlying data storage. Once a dataset is
    created, it becomes immutable. The underlying data, however, does not have the
    same guarantee, and it is up to you to manage its immutability.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在Azure机器学习中，数据集是数据处理和训练任务的一级对象，是输入和输出。数据集被定义为与数据集原始数据相关联的元数据集合。数据集通过URI引用到底层数据存储来引用其原始数据。一旦创建数据集，它就变为不可变。然而，底层数据并没有相同的保证，管理其不可变性取决于你。
- en: Once datasets are defined, data processing and training codes can access them
    through a unified client API. Data can either be downloaded for local access or
    mounted as network storage for direct access. After reading chapter 2, you will
    be able to identify similarities between this paradigm and the one that is described
    in the book. You will learn how you can use this existing product as is and how
    you can extend it for your own needs.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦定义了数据集，数据处理和训练代码可以通过统一的客户端API访问它们。数据可以下载到本地进行访问，或者挂载为网络存储以实现直接访问。阅读完第2章后，你将能够识别出这种范式与书中描述的范式之间的相似之处。你将学习如何直接使用现有产品，以及如何根据自身需求对其进行扩展。
- en: B.3.2 Model training
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.2 模型训练
- en: Azure Machine Learning supplies prebuilt containers with Python distributions
    and allows users to define a custom-base image that conforms to specific requirements.
    As of this writing, only Python is supported for defining custom training code.
    To launch a training run, you need to specify a runtime container and a reference
    to training code that conforms to a certain convention. If you need something
    other than this setup, you will need to build your own training service. Chapters
    3 and 4 will show you the key principles of a training service and an example
    that you can use as a starting point for your own training service.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习提供了预构建的容器，包含 Python 发行版，并允许用户定义符合特定要求的自定义基础镜像。到目前为止，仅支持 Python 用于定义自定义训练代码。要启动训练运行，您需要指定一个运行时容器和符合特定约定的训练代码的引用。如果您需要除这种设置之外的其他东西，您将需要构建自己的训练服务。第
    3 章和第 4 章将向您展示训练服务的关键原则以及您可以将其作为自己训练服务起点的一个示例。
- en: B.3.3 Model serving
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.3 模型服务
- en: On Azure Machine Learning v2, an endpoint can be created to serve online inference
    requests. The endpoint can either be configured to load a certain model and use
    a Python script to produce inferences or be configured to use a completely custom
    container image—such as TensorFlow Serving—to produce inferences. Azure Machine
    Learning also integrates with NVIDIA Triton Inference Server, which provides additional
    performance gain when GPU is used to produce inferences.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 机器学习 v2 中，可以创建一个端点来处理在线推理请求。该端点可以配置为加载特定模型并使用 Python 脚本生成推理，或者配置为使用完全自定义的容器镜像（例如
    TensorFlow Serving）来生成推理。Azure 机器学习还与 NVIDIA Triton 推理服务器集成，当使用 GPU 生成推理时，可以提供额外的性能提升。
- en: If you need to deploy multiple models to a single endpoint or manage models
    and inference production on edge devices, you will need to build your own. In
    chapters 6 and 7, we discuss model serving in depth. After completing these chapters,
    you will be able to build your own model server should you require additional
    features that existing offerings do not support.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您需要将多个模型部署到单个端点或管理边缘设备上的模型和推理生产，您将需要构建自己的。在第 6 章和第 7 章中，我们将深入讨论模型服务。完成这些章节后，如果您需要现有服务不支持的功能，您将能够构建自己的模型服务器。
- en: B.3.4 Metadata and artifacts store
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.4 元数据和工件存储
- en: In Azure Machine Learning, metadata can be tagged along many objects, such as
    models, training runs, etc., in the form of tags. While not a standalone product,
    the model registration capability supports additional metadata when registering
    a model. The interface receives the metadata as well as the model file (artifact)
    at the same time during registration, taking one less step when compared to other
    solutions that require models to be registered to already exist in their cloud
    storage.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Azure 机器学习中，元数据可以以标签的形式附加到许多对象上，例如模型、训练运行等。虽然它不是一个独立的产品，但模型注册功能在注册模型时支持额外的元数据。在注册过程中，接口同时接收元数据和模型文件（工件），与其他需要将模型注册到其云存储中已存在的解决方案相比，减少了步骤。
- en: As of this writing, a preview feature called registry can be used to centralize
    ML related metadata to one place. If you want to track lineage between different
    artifacts, though you may need to build your own solution.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，可以使用名为“注册表”的预览功能将机器学习相关的元数据集中到一个地方。如果您想跟踪不同工件之间的血缘关系，尽管您可能需要构建自己的解决方案。
- en: After reading chapter 8, you will gain an in-depth understanding of the metadata
    and artifacts store. You will learn its fundamentals and will be able to quickly
    build one yourself.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读第 8 章之后，您将深入理解元数据和工件存储。您将学习其基础原理，并能够快速构建自己的存储。
- en: B.3.5 Workflow orchestration
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.5 工作流编排
- en: Azure Machine Learning provides a feature called *ML pipelines* that allows
    you to define data, training, and other tasks as steps. These steps are put together
    programmatically to form a pipeline that can be executed periodically based on
    a schedule or trigger or be launched once manually. Compute resources, execution
    environment, and access permissions can be configured programmatically when the
    pipeline is defined.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Azure 机器学习提供了一个名为“ML 管道”的功能，允许您将数据、训练和其他任务定义为步骤。这些步骤通过编程方式组合在一起，形成一个可以定期根据计划或触发器执行或手动启动的管道。当定义管道时，可以编程配置计算资源、执行环境和访问权限。
- en: In chapter 9, we review how to use a workflow manager to enable different modes
    of training. After reading the chapter, you will understand the reasoning behind
    the design and utility of a workflow manager in a deep learning system and its
    role in an enterprise environment.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9 章中，我们回顾了如何使用工作流程管理器来启用不同的训练模式。阅读本章后，您将了解深度学习系统中工作流程管理器的设计和实用性的原因，以及它在企业环境中的作用。
- en: B.3.6 Experimentation
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.3.6 实验
- en: Azure Machine Learning provides a feature for defining and tracking experiments.
    When model training is being performed as part of an experiment, metrics can be
    logged from the training code and visualized through the web interface. It also
    supports arbitrary tagging and parent-child relationships between experiment runs
    for a hierarchical organization and lookup.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: Azure Machine Learning 提供了定义和跟踪实验的功能。当模型训练作为实验的一部分进行时，可以从训练代码中记录指标并通过网页界面进行可视化。它还支持实验运行之间的任意标记和父子关系，以实现分层组织和查找。
- en: B.4 Kubeflow
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.4 Kubeflow
- en: Kubeflow is an open source suite of tools that provides many useful components
    for building a deep learning system without being locked into a particular cloud
    vendor. In this section, we walk through the list of key components that are introduced
    in this book and compare them with similar components provided by Kubeflow.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是一个开源工具套件，它提供了许多构建深度学习系统时非常有用的组件，而无需锁定特定的云供应商。在本节中，我们将介绍本书中引入的关键组件列表，并将它们与
    Kubeflow 提供的类似组件进行比较。
- en: B.4.1 Dataset management
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.1 数据集管理
- en: Kubeflow’s vision is to not reinvent any existing tools, so it should not be
    a surprise that it does not come with a data management component, as other open
    source solutions exist. In chapter 2, we review some open source data management
    solutions and explore how they can be further extended to implement key principles
    described in that chapter.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 的愿景是不重新发明任何现有工具，因此它不附带数据管理组件并不令人惊讶，因为其他开源解决方案已经存在。在第 2 章中，我们回顾了一些开源数据管理解决方案，并探讨了如何进一步扩展以实现该章节中描述的关键原则。
- en: B.4.2 Model training
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.2 模型训练
- en: Kubeflow, being a suite of tools that is based on Kubernetes, has the luxury
    of being backed by a sophisticated resource scheduler. Unlike cloud vendors that
    provide prebuilt model training containers, you must build your own and manage
    their launches. In chapters 3 and 4, we talk about the principles of a training
    service and how it helps to abstract the complexity in resource assignment and
    scheduling. We go over a reference training service, and you learn how to build
    one yourself for your requirements.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基于 Kubernetes 的工具套件，Kubeflow 拥有由复杂的资源调度器支持的奢侈。与提供预构建模型训练容器的云供应商不同，您必须自己构建并管理它们的启动。在第
    3 章和第 4 章中，我们讨论了训练服务的原理以及它是如何帮助抽象资源分配和调度的复杂性的。我们将介绍一个参考训练服务，并学习如何根据您的需求自行构建。
- en: B.4.3 Model serving
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.3 模型服务
- en: As of this writing, Kubeflow provides a KServe component that can be used to
    deploy trained models as an inference service, which serves inference requests
    over the network. It is an interface that sits on top of existing serving frameworks
    such as TensorFlow Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server.
    The main benefit of using KServe is the additional abstraction of operational
    complexity such as autoscaling, health checks, and auto-recovery. Because it is
    an open source solution, it is possible to host either one model or multiple models
    with the same endpoint. In chapters 6 and 7, we will go through model serving
    principles so that you will understand the reason behind the design of popular
    serving interfaces and how you can customize them to fit your own needs.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，Kubeflow 提供了一个 KServe 组件，可以用来将训练好的模型作为推理服务部署，通过网络处理推理请求。它是一个位于现有服务框架（如
    TensorFlow Serving、PyTorch TorchServe 和 NVIDIA Triton Inference Server）之上的接口。使用
    KServe 的主要好处是增加了操作复杂性的抽象，例如自动扩展、健康检查和自动恢复。由于它是一个开源解决方案，因此可以托管一个或多个模型，并使用相同的端点。在第
    6 章和第 7 章中，我们将探讨模型服务原理，以便您了解流行服务接口的设计原因以及如何根据您的需求进行定制。
- en: B.4.4 Metadata and artifacts store
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.4 元数据和工件存储
- en: Starting with Kubeflow version 1.3, metadata and artifacts become an integral
    part of Kubeflow Pipelines. Kubeflow Pipelines consist of a graph of pipeline
    components. Between each component, parameters and artifacts can be passed along.
    Similar to the description in this book, artifacts encapsulate any kind of data
    that is the side effect of a deep learning system, such as the model itself, training
    metrics, and data distribution metrics. Metadata is any data that describes pipeline
    components and artifacts. With these constructs, you can deduce the lineage between
    input training datasets, trained models, experiment results, and served inferences.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从 Kubeflow 版本 1.3 开始，元数据和工件成为 Kubeflow Pipelines 的一个组成部分。Kubeflow Pipelines
    由一系列管道组件的图组成。在各个组件之间，可以传递参数和工件。类似于本书中的描述，工件封装了深度学习系统的任何副作用数据，例如模型本身、训练指标和数据分布指标。元数据是描述管道组件和工件的任何数据。有了这些结构，你可以推断输入训练数据集、训练模型、实验结果和服务的推理之间的血缘关系。
- en: In chapter 8, we discuss key concerns for building the metadata and artifacts
    store. After reading the chapter, you will understand the design principles behind
    this component and how existing products can help you build your own quickly.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 8 章中，我们讨论了构建元数据和工件存储的关键关注点。阅读本章后，您将了解该组件背后的设计原则以及现有产品如何帮助您快速构建自己的系统。
- en: B.4.5 Workflow orchestration
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.5 工作流程编排
- en: Also described in the previous section, Kubeflow Pipelines can be used to help
    manage deep learning data preparation and training workflows. Metadata and versioning
    are built into pipelines, and native users and access permissions from Kubernetes
    can be used to restrict access.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中也描述了，Kubeflow Pipelines 可以用来帮助管理深度学习数据准备和训练工作流程。元数据和版本控制内置到管道中，并且可以使用 Kubernetes
    的本地用户和访问权限来限制访问。
- en: In chapter 9, we review how a workflow manager enables different modes of training.
    After reading the chapter, you will understand the reasoning behind the design
    and utility of a workflow manager in a deep learning system.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 9 章中，我们回顾了工作流程管理器如何启用不同的训练模式。阅读本章后，您将了解深度学习系统中工作流程管理器设计和实用性的背后的推理。
- en: B.4.6 Experimentation
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: B.4.6 实验
- en: Kubeflow Pipelines provides the Experiment construct in which multiple training
    runs can be organized into a logical group, where it provides additional visualization
    tools for differences between each experimental run. This fits well with offline
    experimentation. If you need to perform online experimentation, you will need
    to roll your own solution.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines 提供了实验结构，可以将多个训练运行组织成一个逻辑组，其中它提供了用于比较每个实验运行的额外可视化工具。这非常适合离线实验。如果您需要执行在线实验，您将需要自行开发解决方案。
- en: B.5 Side-by-side comparison
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: B.5 并列比较
- en: We think it will be handy to provide a summarized overview table of every solution
    grouped by components that we have covered previously. We hope that table B.1
    will make it easier for you to pick the right solution.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为提供一个按组件分组的每个解决方案的总结概述表将很有用。我们希望表 B.1 将使您更容易选择正确的解决方案。
- en: Table B.1 Side-by-side comparison
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表 B.1 并列比较
- en: '|  | Amazon SageMaker | Google Vertex AI | Microsoft Azure Machine Learning
    | Kubeflow |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|  | 亚马逊 SageMaker | 谷歌 Vertex AI | 微软 Azure Machine Learning | Kubeflow |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| Comparing dataset management solutions | AWS components, such as S3, Glue
    Data Catalog, and Glue ETL, can be used to build a dataset management component.
    | APIs for managing datasets are ready to use. Data content upload and metadata
    tagging are separate operations. | Datasets are first-class objects and are immutable
    once created. A unified client API is provided for training jobs to access training
    datasets. | Does not provide a dataset management solution. Other open source
    alternatives are readily available. |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 比较数据集管理解决方案 | AWS 组件，如 S3、Glue 数据目录和 Glue ETL，可以用来构建数据集管理组件。 | 管理数据集的 API
    已准备好使用。数据内容上传和元数据标记是独立的操作。 | 数据集是第一类对象，一旦创建就不可变。为训练作业提供了一个统一的客户端 API，用于访问训练数据集。
    | 不提供数据集管理解决方案。其他开源替代方案 readily 可用。 |'
- en: '| Comparing model training solutions | Supports built-in algorithms, externally
    provided custom code, and custom containers for training. Exposes an API for launching
    training jobs on demand. | Provides prebuilt training containers that can be used
    as is. Supports custom training containers. Exposes an API that supports launching
    training containers on multiple nodes. | Provides prebuilt training containers
    with Python that can be customized. Training containers must conform to a certain
    convention. | Has native access to Kubernetes scheduling capabilities. No prebuilt
    training containers are provided. |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 比较模型训练解决方案 | 支持内置算法、外部提供的自定义代码和自定义容器进行训练。提供API以按需启动训练作业。 | 提供预构建的训练容器，可以直接使用。支持自定义训练容器。提供API，支持在多个节点上启动训练容器。
    | 提供预构建的带有Python的自定义训练容器。训练容器必须符合一定的约定。 | 具有对Kubernetes调度能力的原生访问。不提供预构建的训练容器。
    |'
- en: '| Comparing model serving solutions | Models can be deployed as web endpoints.
    Multiple models can be deployed to the same endpoint for better utilization, with
    some limitations when GPUs are used. Configurable model caching behavior. | Models
    and inference containers are decoupled. They must be deployed together to form
    a web endpoint for serving. Custom inference containers are supported. Multiple
    models per endpoint are primarily used for canarying new versions of models. Video
    models are not supported. | Endpoints can be deployed to serve models over the
    web. Endpoints are configured to use a particular model with a custom Python script
    for producing inferences. NVIDIA Triton Inference Server integration is available.
    | KServe is the Kubeflow component that serves models. It provides a serverless
    inferencing abstraction on top of popular serving frameworks such as TensorFlow
    Serving, PyTorch TorchServe, and NVIDIA Triton Inference Server. |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 比较模型服务解决方案 | 模型可以作为网络端点部署。可以将多个模型部署到同一端点以实现更好的利用率，当使用GPU时有一些限制。可配置模型缓存行为。
    | 模型和推理容器是解耦的。它们必须一起部署以形成用于服务的网络端点。支持自定义推理容器。每个端点可以部署多个模型，主要用于测试模型的新版本。不支持视频模型。
    | 端点可以部署到网络上提供服务。端点配置为使用特定模型，并使用自定义Python脚本来生成推理。提供NVIDIA Triton推理服务器集成。 | KServe是Kubeflow组件，用于提供模型服务。它在上面的流行服务框架（如TensorFlow
    Serving、PyTorch TorchServe和NVIDIA Triton推理服务器）之上提供无服务器推理抽象。 |'
- en: '| Comparing metadata and artifacts store solutions | SageMaker Model Registry
    provides a central metadata store solution. Artifacts are stored separately in
    Amazon’s object store. | Vertex ML Metadata provides a central metadata store
    solution. Metadata is stored as graphs that can describe complex relationships.
    Artifacts are stored in Google’s object store. | A preview feature called registry
    can be used to centralize ML metadata. Metadata exists as tags of different objects
    (training runs, models, etc.), and objects can be artifacts. Lineage information
    can be deduced using these object tags. | Does not have a central repository of
    metadata or artifacts. Metadata and artifacts are integral parts of Kubeflow Pipelines.
    Each stage in the pipeline can be annotated with metadata and produce artifacts
    that can be tracked. Lineage information can be deduced from this information
    that can be retrieved from the Pipelines API. |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 比较元数据和工件存储解决方案 | SageMaker模型注册表提供了一个中央元数据存储解决方案。工件存储在Amazon的对象存储中。 | Vertex
    ML元数据提供了一个中央元数据存储解决方案。元数据存储为可以描述复杂关系的图。工件存储在Google的对象存储中。 | 可以使用名为注册表的预览功能来集中管理ML元数据。元数据作为不同对象的标签（如训练运行、模型等）存在，对象可以是工件。可以使用这些对象标签推断出血缘信息。
    | 没有元数据或工件的中央存储库。元数据和工件是Kubeflow Pipelines的组成部分。管道中的每个阶段都可以用元数据注释，并生成可以跟踪的工件。可以从Pipelines
    API检索到的信息中推断出血缘信息。 |'
- en: '| Comparing workflow orchestration solutions | Model Building Pipelines can
    be used to build and manage deep learning workflows. | Vertex ML Metadata provides
    a central metadata store solution. Metadata is stored as graphs that can describe
    complex relationships. Artifacts are stored in Google’s object store. | A preview
    feature called registry can be used to centralize ML metadata. Metadata exists
    as tags of different objects (training runs, models, etc.), and objects can be
    artifacts. Lineage information can be deduced using these object tags. | Does
    not have a central repository of metadata or artifacts. Metadata and artifacts
    are integral parts of Kubeflow Pipelines. Each stage in the pipeline can be annotated
    with metadata and produce artifacts that can be tracked. Lineage information can
    be deduced from the details that can be retrieved from the Pipelines API. |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 比较工作流编排解决方案 | 模型构建管道可用于构建和管理深度学习工作流。 | Vertex ML元数据提供了一种中央元数据存储解决方案。元数据存储为可以描述复杂关系的图。工件存储在Google的对象存储中。
    | 可以使用名为注册表的预览功能来集中管理ML元数据。元数据以不同对象的标签（训练运行、模型等）的形式存在，对象可以是工件。可以使用这些对象标签推断出血缘信息。
    | 没有元数据或工件的中央存储库。元数据和工件是Kubeflow管道的组成部分。管道中的每个阶段都可以用元数据注释，并产生可以跟踪的工件。可以从可以从中检索到的详细信息中推断出血缘信息。
    |'
- en: '| Comparing experimentation solutions | The Experiments feature provides grouping
    and tracking for training runs. | Provides Vertex AI Experiments for tracking
    and visualizing experiment setups and run results. | Provides features for defining
    and tracking experiments. Experiments can be associated with a parent-child relationship.
    The web interface supports visualization. | Provides an Experiment construct for
    logical grouping of Kubeflow Pipelines that belong to the same experiment group.
    Visualization tools are provided to highlight differences between each pipeline
    run in the same experiment. |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 比较实验解决方案 | 实验功能提供了对训练运行的分组和跟踪。 | 提供Vertex AI实验功能，用于跟踪和可视化实验设置和运行结果。 | 提供定义和跟踪实验的功能。实验可以与父-子关系相关联。Web界面支持可视化。
    | 提供一个实验结构，用于对属于同一实验组的Kubeflow管道进行逻辑分组。提供可视化工具以突出显示同一实验中每个管道运行之间的差异。 |'
