- en: 1 Introduction
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 1 简介
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: What PySpark is
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是PySpark
- en: Why PySpark is a useful tool for analytics
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么PySpark是数据分析的有用工具
- en: The versatility of the Spark platform and its limitations
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark平台的多功能性及其局限性
- en: PySpark’s way of processing data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark处理数据的方式
- en: 'According to pretty much every news outlet, data is everything, everywhere.
    It’s the new oil, the new electricity, the new gold, plutonium, even bacon! We
    call it powerful, intangible, precious, dangerous. At the same time, data itself
    is not enough: it is what you do with it that matters. After all, for a computer,
    any piece of data is a collection of zeroes and ones, and it is our responsibility,
    as users, to make sense of how it translates to something useful.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎每家新闻机构都认为，数据无处不在，数据就是一切。它是新的石油，新的电力，新的黄金，钚，甚至是培根！我们称之为强大、无形、珍贵、危险。然而，数据本身并不足够：重要的是你如何使用它。毕竟，对于计算机来说，任何数据都是零和一的集合，作为用户，我们有责任理解它如何转化为有用的东西。
- en: Just like oil, electricity, gold, plutonium, and bacon (especially bacon!),
    our appetite for data is growing. So much, in fact, that computers aren’t following.
    Data is growing in size and in complexity, yet consumer hardware has been stalling
    a little. RAM is hovering for most laptops at around 8 to 16 GB, and SSDs are
    getting prohibitively expensive past a few terabytes. Is the solution for the
    burgeoning data analyst to triple-mortgage their life to afford top-of-the-line
    hardware to tackle big data problems?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 就像石油、电力、黄金、钚和培根（尤其是培根！）一样，我们对数据的渴望在增长。事实上，增长得如此之快，以至于计算机跟不上了。数据在规模和复杂性上都在增长，而消费级硬件的增长却有所停滞。对于大多数笔记本电脑来说，RAM徘徊在8到16GB左右，而SSD在几TB之后变得过于昂贵。对于日益增长的数据分析师来说，解决方案是不是要三倍抵押他们的生活，才能负担得起顶级的硬件来处理大数据问题？
- en: Here is where Apache Spark (which I’ll call Spark throughout the book) and its
    companion PySpark are introduced. They take a few pages of the supercomputer playbook—powerful,
    but manageable compute units meshed in a network of machines—and bring them to
    the masses. Add on top a powerful set of data structures ready for any work you’re
    willing to throw at them, and you have a tool that will *grow* (pun intended)
    with you.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍了Apache Spark（我在本书中会称之为Spark）及其配套的PySpark。它们借鉴了超级计算机操作手册中的几页——强大的计算单元在网络中的机器上交织在一起——并将它们带给大众。再加上一套强大的数据结构，可以应对您愿意投入的任何工作，您就拥有了一个会*成长*（有意为之）的工具。
- en: A goal for this book is to provide you with the tools to analyze data using
    PySpark, whether you need to answer a quick data-driven question or build an ML
    model. It covers just enough theory to get you comfortable while giving you enough
    opportunities to practice. Most chapters contain a few exercises to anchor what
    you just learned. The exercises are all solved and explained in appendix A.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的一个目标是为您提供使用PySpark分析数据的工具，无论您需要回答一个快速的数据驱动问题还是构建一个机器学习模型。它涵盖了足够的理论知识，让您感到舒适，同时给您足够的机会进行实践。大多数章节都包含一些练习来巩固您刚刚学到的知识。这些练习都在附录A中解决并解释。
- en: 1.1 What is PySpark?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.1 什么是PySpark？
- en: What’s in a name? Actually, quite a lot. Just by separating PySpark in two,
    you can already deduce that this will be related to Spark and Python. And you
    would be right!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 名称中有什么含义？实际上，很多。仅仅通过将PySpark分成两部分，你就可以推断出这将与Spark和Python相关。而且你是对的！
- en: At its core, PySpark can be summarized as being the Python API to Spark. While
    this is an accurate definition, it doesn’t give much unless you know the meaning
    of Python and Spark. Still, let’s break down the summary definition by first answering
    “What is Spark?” With that under our belt, we then will look at why Spark becomes
    especially powerful when combined with Python and its incredible array of analytical
    (and machine learning) libraries.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在其核心，PySpark可以概括为Spark的Python API。虽然这是一个准确的定义，但如果不知道Python和Spark的含义，它并没有太多意义。不过，让我们通过首先回答“什么是Spark？”来分解这个总结定义。有了这个基础，我们接下来将探讨为什么Spark与Python及其令人难以置信的分析（和机器学习）库结合使用时变得特别强大。
- en: '1.1.1 Taking it from the start: What is Spark?'
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.1 从一开始：什么是Spark？
- en: According to the authors of the software, Apache Spark™, which I’ll call Spark
    throughout this book, is a “unified analytics engine for large-scale data processing”
    (see [https://spark.apache.org/](https://spark.apache.org/)). This is a very accurate,
    if a little dry, definition. As a mental image, we can compare Spark to an *analytics
    factory*. The raw material—here, data—comes in, and data, insights, visualizations,
    models, you name it, comes out.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Apache Spark™软件的作者，我将在本书中称之为Spark，它是一个“用于大规模数据处理的一体化分析引擎”（见[https://spark.apache.org/](https://spark.apache.org/))。这是一个非常准确，但略显枯燥的定义。作为一个心理图像，我们可以将Spark比作一个*分析工厂*。原材料——在这里是数据——进入，而数据、洞察、可视化、模型，等等，都会产出。
- en: Just like a factory will often gain more capacity by increasing its footprint,
    Spark can process an increasingly vast amount of data by *scaling out* (across
    multiple smaller machines) instead of *scaling up* (adding more resources, such
    as CPU, RAM, and disk space, to a single machine). RAM, unlike most things in
    this world, gets *more* expensive the more you buy (e.g., one stick of 128 GB
    is more than the price of two sticks of 64 GB). This means that, instead of buying
    thousands of dollars of RAM to accommodate your data set, you’ll rely on multiple
    computers, splitting the job between them. In a world where two modest computers
    are less costly than one large one, scaling out is less expensive than scaling
    up, which keeps more money in your pockets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 就像工厂通过扩大其占地面积来增加产能一样，Spark可以通过*横向扩展*（跨多个较小的机器）而不是*纵向扩展*（向单一机器添加更多资源，如CPU、RAM和磁盘空间）来处理越来越多的数据。与世界上大多数事物不同，RAM的价格随着购买量的增加而*增加*（例如，一根128GB的内存条比两根64GB的内存条价格要高）。这意味着，你不需要购买数千美元的RAM来容纳你的数据集，而是会依赖多台计算机，将工作分配给它们。在一个两台普通计算机的成本低于一台大型计算机的世界里，横向扩展比纵向扩展更经济，这可以让你省下更多的钱。
- en: Cloud cost and RAM
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 云端成本和RAM
- en: In the cloud, prices will often be more consequential. For instance, as of January
    2022, a 16-Core/128-GB RAM machine can be about twice the cost of an 8 Core/64
    GB of RAM machine. As the data size grows, Spark can help control costs by scaling
    the number of workers and executors for a given job. As an example, if you have
    a data transformation job on a modest data set (a few TB), you can limit yourself
    to a lower number—say, five—machines, scaling up to 60 when you want to do machine
    learning. Some vendors, such as Databricks (see appendix B), offer *auto-scaling*,
    meaning that they increase and decrease the number of machines during a job depending
    on the pressure on the cluster. The implementation of auto-scaling/cost controlling
    is 100% vendor-dependent. (Check out chapter 11 for an introduction to the resources
    making up a Spark cluster, as well as their purpose.)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在云端，价格往往更为重要。例如，截至2022年1月，一个16核/128GB RAM的机器可能比一个8核/64GB RAM的机器贵一倍。随着数据量的增长，Spark可以通过扩展特定任务的工人和执行器的数量来帮助控制成本。例如，如果你有一个在适度数据集（几TB）上的数据转换任务，你可以限制自己使用更少的机器——比如说五台——当你想要进行机器学习时，可以扩展到60台。一些供应商，如Databricks（见附录B），提供*自动扩展*，这意味着他们会根据集群的压力在任务期间增加或减少机器的数量。自动扩展/成本控制的实现完全取决于供应商。（查看第11章，了解构成Spark集群的资源以及它们的作用。）
- en: A single computer can crash or behave unpredictably at times. If instead of
    one you have one hundred, the chance that at least one of them goes down is now
    much higher.[¹](#pgfId-1108245) Spark therefore has a lot of hoops to manage,
    scale, and babysit so that you can focus on what you want, which is to work with
    data.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 一台计算机有时可能会崩溃或表现出不可预测的行为。如果你有100台而不是一台，那么至少有一台出现故障的概率现在要高得多。[¹](#pgfId-1108245)
    因此，Spark 需要管理、扩展和照看很多环节，以便你能专注于你真正想要的事情，那就是与数据工作。
- en: 'This is, in fact, one of the key things about Spark: it’s a good tool because
    of what you can do with it, but especially because of what you *don’t have to
    do* with it. Spark provides a powerful API (*application programming interface*,
    the set of functions, classes, and variables provided for you to interact with)
    that makes it look like you’re working with a cohesive source of data while also
    working hard in the background to optimize your program to use all the power available.
    You don’t have to be an expert in the arcane art of distributed computing; you
    just need to be familiar with the language you’ll use to build your program.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是Spark的关键特性之一：它是一个好工具，因为你可以用它做很多事情，但更重要的是，它让你*不需要做*的事情很多。Spark提供了一个强大的API（应用程序编程接口，为你提供的一组函数、类和变量，用于与数据交互），这使得你看起来像是在与一个统一的数据源一起工作，同时在后台努力优化你的程序以使用所有可用的功能。你不需要成为分布式计算神秘艺术的专家；你只需要熟悉你将用来构建程序的语言。
- en: 1.1.2 PySpark = Spark + Python
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.2 PySpark = Spark + Python
- en: PySpark provides an entry point to Python in the computational model of Spark.
    Spark itself is coded in Scala.[²](#pgfId-1108284) The authors did a great job
    of providing a coherent interface between languages while preserving the idiosyncrasies
    of each language where appropriate. It will, therefore, be quite easy for a Scala/Spark
    programmer to read your PySpark program, as well as for a fellow Python programmer
    who hasn’t jumped into the deep end (yet).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark为Spark计算模型中的Python提供了一个入口点。Spark本身是用Scala编写的。[²](#pgfId-1108284) 作者在提供连贯的接口的同时，适当地保留了每种语言的独特性，做得非常出色。因此，对于Scala/Spark程序员来说，阅读你的PySpark程序将会非常容易，同样对于还没有深入水中的Python程序员也是如此。
- en: Python is a dynamic, general-purpose language, available on many platforms and
    for a variety of tasks. Its versatility and expressiveness make it an especially
    good fit for PySpark. The language is one of the most popular for a variety of
    domains, and currently it is a major force in data analysis and science. The syntax
    is easy to learn and read, and the number of libraries available means that you’ll
    often find one (or more!) that’s just the right fit for your problem.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: Python是一种动态的通用语言，可在许多平台上运行，适用于各种任务。它的多功能性和表现力使其非常适合PySpark。这种语言在各种领域都非常受欢迎，目前它是数据分析和科学领域的主要力量。其语法易于学习和阅读，可用的库数量意味着你通常会找到一个（或更多！）非常适合你问题的库。
- en: PySpark provides access not only to the core Spark API but also to a set of
    bespoke functionality to scale out regular Python code, as well as pandas transformations.
    In Python’s data analysis ecosystem, pandas is the de facto data frame library
    for memory-bound data frames (the entire data frame needs to reside on a single
    machine’s memory). It’s not a matter of PySpark or pandas now, but PySpark *and*
    pandas. Chapters 8 and 9 are dedicated to combining Python, pandas, and PySpark
    in one happy program. For those really committed to the pandas syntax (or if you
    have a large pandas program you want to scale to PySpark), Koalas (now called
    `pyspark.pandas` and part of Spark as of version 3.2.0; [https://koalas.readthedocs.io/](https://koalas.readthedocs.io/))
    provides a pandas-like porcelain on top of PySpark. If you are starting a new
    Spark program in Python, I recommend using the PySpark syntax—covered thoroughly
    in this book—reserving Koalas for when you want to ease the transition from pandas
    to PySpark. Your program will work faster and, in my opinion, will read better.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark不仅提供了对核心Spark API的访问，还提供了一套专门的功能，用于扩展常规Python代码以及pandas转换。在Python的数据分析生态系统中，pandas是内存受限数据框的事实上的库（整个数据框需要驻留在单个机器的内存中）。现在不是PySpark或pandas的问题，而是PySpark
    *和* pandas。第8章和第9章专门介绍了将Python、pandas和PySpark结合在一个快乐程序中的方法。对于那些真正致力于pandas语法（或者如果你有一个大的pandas程序想要扩展到PySpark），Koalas（现在称为`pyspark.pandas`，自3.2.0版本起成为Spark的一部分；[https://koalas.readthedocs.io/](https://koalas.readthedocs.io/))在PySpark之上提供了一个类似pandas的瓷器。如果你正在用Python开始一个新的Spark程序，我建议使用本书彻底介绍的PySpark语法，将Koalas保留在你想要从pandas过渡到PySpark时使用。你的程序将运行得更快，在我看来，可读性也会更好。
- en: 1.1.3 Why PySpark?
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.1.3 为什么选择PySpark？
- en: There is no shortage of libraries and frameworks to work with data. Why should
    one spend their time learning PySpark specifically?
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据一起工作的库和框架并不缺乏。为什么人们应该花时间专门学习PySpark呢？
- en: PySpark has a lot of advantages for modern data workloads. It sits at the intersection
    of fast, expressive, and versatile. This section covers the many advantages of
    PySpark, why its value proposition goes beyond just “Spark, with Python,” and
    when it is better to reach for another tool.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark对现代数据工作负载有很多优势。它位于快速、表达和灵活的交汇点。本节涵盖了PySpark的许多优势，为什么其价值主张不仅仅局限于“Spark，加上Python”，以及何时应该选择其他工具。
- en: PySpark is fast
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark速度快
- en: 'If you search for “big data” in a search engine, there is a very good chance
    that Hadoop will come up within the first few results. There is a good reason
    for this: Hadoop popularized the famous *MapReduce* framework that Google pioneered
    in 2004 and inspired how data is processed at scale (we touch on MapReduce in
    chapter 8, when talking about PySpark’s low-level data structure, the resilient
    distributed data set).'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个搜索引擎中搜索“大数据”，有很大可能性Hadoop会在前几个结果中出现。这有一个很好的原因：Hadoop普及了Google在2004年开创的著名*MapReduce*框架，并启发了大规模数据处理的方式（我们在第8章中提到MapReduce，当时我们讨论了PySpark的低级数据结构，弹性分布式数据集）。
- en: Spark was created a few years later, sitting on Hadoop’s incredible legacy.
    With an aggressive query optimizer, a judicious usage of RAM (reducing disk I/O;
    see chapter 11), and some other improvements we’ll touch on in the next chapters,
    Spark can run up to 100 times faster than plain Hadoop. Because of the integration
    between the two frameworks, you can easily switch your Hadoop workflow to Spark
    and gain some performance boost without changing your hardware.[³](#pgfId-1108395)
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Spark是在几年后创建的，建立在Hadoop令人难以置信的遗产之上。凭借积极的查询优化器、合理的RAM使用（减少磁盘I/O；见第11章）以及我们将在下一章中提到的其他改进，Spark可以比普通的Hadoop快100倍。由于这两个框架之间的集成，你可以轻松地将你的Hadoop工作流程切换到Spark，并获得一些性能提升，而无需更改你的硬件。[³](#pgfId-1108395)
- en: PySpark is expressive
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark具有表达性
- en: 'Beyond Python being one of the most popular and easy-to-learn languages, PySpark’s
    API has been designed from the ground up to be easy to understand. PySpark borrows
    and extends the vocabulary for data manipulation from SQL. It does so in a *fluent*
    manner: each operation on a data frame returns a “new” data frame, so you can
    chain operations one after the other. Although we are just in the early stages
    of learning PySpark, listing 1.1 shows how readable, well-crafted PySpark looks.
    Even with no prior knowledge, the vocabulary choices and the consistency of the
    syntax makes it read like prose. We read a CSV file, create a new column that
    contains a value conditional to an *old column*, *filter* (using `where`), group
    by the values of the column, generate the count for each group, and finally write
    the results back to a CSV file. All these methods are covered throughout part
    1 of the book, but we can already deduce what this code is doing.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 除了Python是最受欢迎和易于学习的语言之一之外，PySpark的API从头开始设计，易于理解。PySpark从SQL中借用并扩展了数据操作词汇。它以流畅的方式进行：对数据框的每次操作都会返回一个“新”数据框，因此你可以连续执行操作。尽管我们目前只是处于学习PySpark的早期阶段，但列表1.1展示了PySpark的可读性和精心设计的特性。即使没有先前的知识，词汇选择和语法的连贯性也使得它读起来像散文。我们读取CSV文件，创建一个包含基于*旧列*的值的条件的新列，*过滤*（使用`where`），按列值分组，为每个组生成计数，最后将结果写回CSV文件。所有这些方法都在本书的第一部分中进行了介绍，但我们已经可以推断出这段代码的作用。
- en: Listing 1.1 Simple ETL pipeline showing expressiveness of PySpark
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 列表1.1 显示PySpark表达性的简单ETL管道
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Under the hood, Spark optimizes these operations so that we don’t get an intermediate
    data frame after each method. Because of this, we can program our data transformation
    code in a very succinct and self-describing way, relying on Spark to optimize
    the end results—a programmer’s comfort at its finest.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，Spark对这些操作进行了优化，以确保我们不会在每次方法调用后得到一个中间数据框。正因为如此，我们可以用非常简洁和自我描述的方式编写我们的数据转换代码，依靠Spark来优化最终结果——这是程序员最舒适的体验。
- en: You will see many (more complex!) examples throughout this book. As I was writing
    the examples, I was pleased about how close to my initial (pen-and-paper) reasoning
    the code ended up looking. After understanding the fundamentals of the framework,
    I’m confident you’ll be in the same situation.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你将在本书中看到许多（更复杂！）的示例。当我编写这些示例时，我很高兴代码最终看起来与我最初的（笔和纸）推理非常接近。在理解了框架的基本原理之后，我坚信你也会处于同样的情况。
- en: PySpark is versatile
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark功能丰富
- en: 'A key advantage of PySpark is its versatility: you learn one tool and use it
    in a variety of settings. There are two components to this versatility. First,
    there is the *availability* of the framework. Second, there is the diversified
    *ecosystem* surrounding Spark.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的一个关键优势是其多功能性：您学习一个工具，并在各种环境中使用它。这种多功能性有两个组成部分。首先，是框架的*可用性*。其次，是围绕Spark的多元化*生态系统*。
- en: PySpark is everywhere. All three major cloud providers (Amazon Web Services
    [AWS], Google Cloud Platform [GCP], Microsoft Azure) have a managed Spark cluster
    as part of their offerings, which means you have a fully provisioned cluster at
    the click of a few buttons. You can also easily install Spark on your computer
    to nail down your program before scaling on a more powerful cluster. Appendix
    B covers how to get your local Spark running and succinctly walks you through
    the current main cloud offerings.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark无处不在。所有三大云服务提供商（亚马逊网络服务 [AWS]、谷歌云平台 [GCP]、微软Azure）都将其服务中包含一个管理的Spark集群，这意味着您只需点击几个按钮就可以获得一个完全配置的集群。您还可以轻松地在您的计算机上安装Spark，以便在更强大的集群上扩展之前确定您的程序。附录B涵盖了如何启动本地Spark以及简要介绍了当前主要的云服务。
- en: 'PySpark is open source. Unlike other analytical software, you aren’t tied to
    a single company. You can inspect the source code if you’re curious and even contribute
    if you have an idea for new functionality or find a bug. It also gives a low barrier
    to adoption: download, learn, profit!'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark是开源的。与其他分析软件不同，您不需要绑定到单个公司。如果您对源代码感兴趣，可以检查它，如果您有新功能的想法或发现了错误，甚至可以贡献。它还提供了一个低门槛的采用率：下载、学习、获利！
- en: Finally, Spark’s ecosystem doesn’t stop at PySpark. There is also an API for
    Scala, Java, and R, as well as a state-of-the-art SQL layer. This makes it easy
    to write a polyglot program in Spark. A Java software engineer can tackle the
    data transformation pipeline in Spark using Java, while a data scientist can build
    a model using PySpark.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，Spark的生态系统不仅限于PySpark。还有Scala、Java和R的API，以及一个最先进的SQL层。这使得在Spark中编写多语言程序变得容易。一个Java软件工程师可以使用Java在Spark中处理数据转换管道，而数据科学家可以使用PySpark构建模型。
- en: Where PySpark falls short
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的不足之处
- en: It would be awesome if PySpark was the answer to every data problem. Unfortunately,
    there are some caveats. None of them are deal breakers, but they are to be considered
    when you’re selecting a tool for your next project.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如果PySpark是每个数据问题的答案，那将是极好的。不幸的是，有一些注意事项。它们都不是决定性的，但在选择您下一个项目的工具时应该考虑。
- en: PySpark isn’t the right choice if you’re dealing with rapid processing of (very)
    small data sets. Executing a program on multiple machines requires a level of
    coordination between the nodes, which comes with some overhead. If you’re just
    using a single node, you’re paying the price but aren’t using the benefits. As
    an example, a PySpark shell will take a few seconds to launch; this is often more
    than enough time to process data that fits within your RAM. As new PySpark versions
    get released, though, this small data set performance gap gets narrower and narrower.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您处理的是（非常）小的数据集的快速处理，PySpark不是正确的选择。在多台机器上执行程序需要在节点之间进行一些协调，这会带来一些开销。如果您只使用单个节点，您正在付出代价，但没有使用其好处。例如，PySpark
    shell将花费几秒钟来启动；这通常足以处理适合您RAM的数据。然而，随着新的PySpark版本的发布，这种小数据集的性能差距越来越小。
- en: 'PySpark also has a small disadvantage compared to the Java and Scala API. Since
    Spark is at the core of a Scala program, pure Python code has to be translated
    to and from JVM (Java Virtual Machine, the runtime that powers Java and Scala
    code) instructions. Since the `DataFrame` API is available with PySpark, the differences
    between languages have been narrowed significantly: data frame operations are
    mapped to highly efficient Spark operations that work at the same speed, whether
    your program is written in Scala, Java, or Python. You will still witness slower
    operations when you’re using the resilient distributed data set (RDD) data structure
    or when you define your Python user-defined functions. This does not mean that
    we will avoid them: I cover both topics in chapter 8.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Java 和 Scala API 相比，PySpark 也有一些小缺点。由于 Spark 是 Scala 程序的核心，纯 Python 代码必须转换为
    JVM（Java 虚拟机，Java 和 Scala 代码的运行时）指令。由于 `DataFrame` API 在 PySpark 中可用，语言之间的差异已经显著缩小：数据帧操作映射到高度高效的
    Spark 操作，无论您的程序是用 Scala、Java 还是 Python 编写的，这些操作都以相同的速度运行。当您使用弹性分布式数据集（RDD）数据结构或定义您的
    Python 用户自定义函数时，您仍然会见证较慢的操作。这并不意味着我们将避免使用它们：我在第 8 章中涵盖了这两个主题。
- en: 'Finally, while programming PySpark can feel straightforward, managing a cluster
    can be a little arcane. Spark is a pretty complicated piece of software; while
    the code base matured remarkably over the past few years, we are not yet to the
    point that we can manage a 100-machine cluster as easily as a single node. Understanding
    how Spark is configured and tuning for performance is introduced in chapter 11,
    and cloud options are making it easier than ever (see appendix B). For hairier
    problems, do what I do: befriend your operations team.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，虽然编写 PySpark 可能感觉直接，但管理集群可能有点复杂。Spark 是一个相当复杂的软件；尽管在过去几年中代码库成熟度显著提高，但我们还没有达到可以像管理单个节点那样轻松管理
    100 台机器集群的程度。Spark 的配置和性能调优将在第 11 章中介绍，云选项使得这比以往任何时候都更容易（参见附录 B）。对于更复杂的问题，做像我一样的事情：与您的运维团队交朋友。
- en: This section provided the *why* of PySpark, but also some *why not*, as knowing
    where and when to use PySpark is key to having a great development experience
    and processing performance. In the next section, we delve a little deeper into
    how Spark processes data and makes distributed data processing look like you’re
    controlling a single factory.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不仅提供了 PySpark 的“为什么”，还提供了一些“为什么不”，因为了解何时何地使用 PySpark 对于获得良好的开发体验和高效的处理性能至关重要。在下一节中，我们将更深入地探讨
    Spark 如何处理数据，以及如何使分布式数据处理看起来就像您在控制一个单一工厂。
- en: '1.2 Your very own factory: How PySpark works'
  id: totrans-47
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.2 您自己的工厂：PySpark 的工作原理
- en: In this section, we cover how Spark processes a program. It can be a little
    odd to present the workings and underpinnings of a system that we claimed, a few
    paragraphs ago, hides that complexity. Still, it is important to have a working
    knowledge of how Spark is set up, how it manages data, and how it optimizes queries.
    With this, you will be able to reason with the system, improve your code, and
    figure out quickly when it doesn’t perform the way you want.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍 Spark 如何处理程序。虽然我们之前声称该系统隐藏了复杂性，但展示其工作原理和基础可能有点奇怪。然而，了解 Spark 的设置、数据管理方式和查询优化方式是非常重要的。有了这些知识，您将能够与系统进行推理，改进您的代码，并快速找出它没有按您期望的方式执行的情况。
- en: 'If we keep the factory analogy, we can imagine that the cluster of computers
    Spark is sitting on is the building. If we look at figure 1.1, we can see two
    different ways to interpret a data factory. On the left, we see how it looks from
    the outside: a cohesive unit where projects come in and results come out. This
    is how it will appear to you most of the time. Under the hood, it looks more like
    what’s on the right: you have some workbenches that some workers are assigned
    to. The workbenches are like the computers in our Spark cluster: there is a fixed
    amount of them. Some modern Spark implementations, such as Databricks (see appendix
    B), allow for auto-scaling the number of machines at runtime. Some require more
    planning, especially if you run on the premises and own your hardware. The workers
    are called *executors* in Spark’s literature: they perform the actual work on
    the machines/nodes.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们保持工厂的类比，我们可以想象Spark所坐的计算机集群就是大楼。如果我们看图1.1，我们可以看到解释数据工厂的两种不同方式。在左边，我们看到它从外表看起来是什么样的：一个统一的单元，项目进来，结果出来。这通常是它在你面前呈现的样子。在底层，它看起来更像是右边的东西：你有一些工作台，一些工人被分配到那里。这些工作台就像我们Spark集群中的计算机：它们有固定数量的工作台。一些现代的Spark实现，如Databricks（见附录B），允许在运行时自动扩展机器的数量。一些需要更多的规划，特别是如果你在本地运行并拥有自己的硬件。在Spark的文献中，这些工人被称为*执行器*：它们在机器/节点上执行实际的工作。
- en: '![](../Images/01-01.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图1.1](../Images/01-01.png)'
- en: Figure 1.1 A totally relatable data factory, outside and in. Ninety percent
    of the time we care about the whole factory, but knowing how it’s laid out helps
    when reflecting on our code performance.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.1 一个完全相关的数据工厂，从外到内。百分之九十的时间我们关心整个工厂，但了解它的布局有助于我们反思代码性能。
- en: One of the little workers looks spiffier than the other. That top hat definitely
    makes him stand out from the crowd. In our data factory, he’s the manager of the
    work floor. In Spark terms, we call this the *master*.[⁴](#pgfId-1108599) The
    master here sits on one of the workbenches/machines, but it can also sit on a
    distinct machine (or even your computer!) depending on the cluster manager and
    deployment mode. The role of the master is crucial to the efficient execution
    of your program, so section 1.2.2 is dedicated to this.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 其中一个小工人看起来比其他工人更精神。那顶高帽子确实让他从人群中脱颖而出。在我们的数据工厂中，他是工作场所的管理者。在Spark术语中，我们称这个为*主节点*。[⁴](#pgfId-1108599)
    这个主节点坐在一个工作台/机器上，但它也可以坐在一个不同的机器上（甚至你的电脑上！）这取决于集群管理器和部署模式。主节点的角色对于你程序的效率执行至关重要，所以第1.2.2节专门讨论了这一点。
- en: Tip In the cloud, you can have a *high-availability* cluster, meaning that your
    master will be replicated on more than one machine.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在云中，你可以有一个*高可用性*集群，这意味着你的主节点将在多个机器上复制。
- en: 1.2.1 Some physical planning with the cluster manager
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.1 使用集群管理器进行一些物理规划
- en: Upon reception of the task, which is called a *driver program* in the Spark
    world, the factory starts running. This doesn’t mean that we get straight to processing.
    Before that, the cluster needs to *plan the capacity* it will allocate for your
    program. The entity or program taking care of this is aptly called the *cluster
    manager*. In our factory, this cluster manager will look at the workbenches with
    available space and secure as many as necessary, and then start hiring workers
    to fill the capacity. In Spark, it will look at the machines with available computing
    resources and secure what’s necessary before launching the required number of
    executors across them.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在接收到任务后，在Spark世界中被称为*驱动程序*，工厂开始运行。这并不意味着我们直接进入处理。在那之前，集群需要*规划分配给你的程序的容量*。负责这个任务的实体或程序恰当地被称为*集群管理器*。在我们的工厂中，这个集群管理器将查看有可用空间的工作台，并确保尽可能多的空间，然后开始雇佣工人来填补这些空间。在Spark中，它将查看有可用计算资源的机器，并在它们上启动所需数量的执行器之前确保所需资源。
- en: Note Spark provides its own cluster manager, called Standalone, but can also
    play well with other ones when working in conjunction with Hadoop or another big
    data platform. If you read about YARN, Mesos, or Kubernetes in the wild, know
    that they are used (as far as Spark is concerned) as cluster managers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Spark提供了一个自己的集群管理器，称为Standalone，但在与Hadoop或其他大数据平台协同工作时也能很好地与其他集群管理器协同工作。如果你在野外读到关于YARN、Mesos或Kubernetes的内容，要知道它们（就Spark而言）被用作集群管理器。
- en: Any directions about capacity (machines and executors) are encoded in a `SparkContext`
    representing the connection to our Spark cluster. If our instructions don’t mention
    any specific capacity, the cluster manager will allocate the default capacity
    prescribed by our Spark installation.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 关于容量（机器和执行器）的任何指示都编码在表示与我们的 Spark 集群连接的 `SparkContext` 中。如果我们的指示没有提到任何特定的容量，集群管理器将分配由我们的
    Spark 安装规定的默认容量。
- en: 'As an example, let’s try the following operation. Using the same sample.csv
    file in listing 1.1 (available in the book’s repository), let’s compute a simplified
    version of the program: return the arithmetic average of the values of `old_column`.
    Let’s assume that our Spark instance has four executors, each working on its own
    worker node. The data processing will be approximately split between the four
    executors: each will have a small portion of the data frame that it will work
    with.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们尝试以下操作。使用与列表 1.1 中相同的 sample.csv 文件（可在本书的存储库中找到），让我们计算程序的简化版本：返回 `old_column`
    的值的算术平均值。假设我们的 Spark 实例有四个执行器，每个执行器在其自己的工作节点上工作。数据处理将大致在四个执行器之间分配：每个执行器将处理数据框的一小部分。
- en: Listing 1.2 Content of the sample.csv file
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 1.2 sample.csv 文件的内容
- en: '[PRE1]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Figure 1.2 depicts one way that PySpark could process the average of our `old_column`
    in our small data frame. I chose the average because it is not trivially distributable,
    unlike the sum or the count, where you sum the intermediate values from each worker.
    In the case of computing the average, each worker independently computes the sum
    of the values and their counts before moving the result—not all the data!—over
    to a single worker (or the master directly, when the intermediate result is really
    small) that will process the aggregation into a single number, the average.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 描述了 PySpark 处理我们小型数据框中 `old_column` 平均值的一种方式。我选择平均值，因为它不像求和或计数那样可以轻易分配，在求和或计数的情况下，你需要从每个工作节点求中间值的和。在计算平均值的情况下，每个工作节点独立计算值的总和及其计数，然后在将结果（不是所有数据！）移动到单个工作节点之前，该节点将处理聚合到一个数字，即平均值。
- en: For a simple example like this, mapping the thought process of PySpark is an
    easy and fun exercise. The size of our data and the complexity of our programs
    will grow and will get more complicated, and we will not be able to easily map
    our code to exact physical steps performed by our Spark instance. Chapter 11 covers
    the mechanism Spark uses to give us visibility into the work performed as well
    as the health of our factory.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这样的简单例子，映射 PySpark 的思维过程是一个简单而有趣的练习。我们数据的大小和程序的复杂性将会增长，并且会变得更加复杂，我们将无法轻易地将我们的代码映射到
    Spark 实例执行的精确物理步骤。第 11 章介绍了 Spark 用来让我们了解工作执行情况以及工厂健康状况的机制。
- en: '![](../Images/01-02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-02.png)'
- en: 'Figure 1.2 Computing the average of our small data frame, PySpark style: each
    worker works on a distinct piece of data. As necessary, the data gets moved/shuffled
    around to complete the instructions.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 1.2 以 PySpark 风格计算我们小型数据框的平均值：每个工作节点处理不同的数据。根据需要，数据会被移动/洗牌以完成指令。
- en: 'This section took a simple example—computing the average of a data frame of
    numbers—and we mapped a blueprint of the physical steps performed by Spark to
    give us the right answer. In the next section, we get to one of Spark’s best,
    and most misunderstood, features: laziness. In the case of big data analysis,
    hard work pays off, but smart work is better!'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过一个简单的例子——计算数字数据框的平均值——将 Spark 执行的物理步骤蓝图映射给我们正确的答案。在下一节中，我们将介绍 Spark 最好的、也是最受误解的功能之一：惰性。在大数据分析的情况下，辛勤的工作会得到回报，但聪明的工作更好！
- en: 'Some language convention: Data frame vs. DataFrame'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一些语言约定：Data frame 与 DataFrame
- en: Since this book will talk about data frames more than anything else, I prefer
    using the noncapitalized nomenclature (i.e., “data frame”). I find this more readable
    than using capital letters or even “dataframe” without a space.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 由于本书将更多地讨论数据框，我更喜欢使用非首字母大写的命名法（即，“data frame”）。我发现这比使用大写字母甚至没有空格的“dataframe”更易读。
- en: When referring to the PySpark object directly, I’ll use `DataFrame` but with
    a fixed-width font. This will help differentiate between “data frame” the concept
    and `DataFrame` the object.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当直接引用 PySpark 对象时，我将使用 `DataFrame` 但字体为固定宽度。这将有助于区分“data frame”这个概念和 `DataFrame`
    这个对象。
- en: 1.2.2 A factory made efficient through a lazy leader
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1.2.2 通过惰性领导者使工厂变得高效
- en: 'This section introduces one of the most fundamental aspects of Spark: its lazy
    evaluation capabilities. In my time teaching PySpark and troubleshooting data
    scientists’ programs, I would say that laziness is the concept in Spark that creates
    the most confusion. It’s a real shame because laziness is (in part) how Spark
    achieves its incredible processing speed. By understanding at a high level how
    Spark makes laziness work, you will be able to explain a lot of its behavior and
    better tune for performance.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了Spark最基本的功能之一：其懒加载能力。在我教授PySpark和解决数据科学家程序的问题时，我会说，懒加载是Spark中最容易引起混淆的概念。这真的很遗憾，因为懒加载（部分）是Spark实现其惊人处理速度的原因。通过从高层次理解Spark如何实现懒加载，你将能够解释很多其行为，并更好地调整性能。
- en: 'Just like in a large-scale factory, you don’t go to each employee and give
    them a list of tasks. No, here, the master/manager is responsible for the workers.
    The driver is where the action happens. Think of a driver as a floor lead: you
    provide them your list of steps and let them deal with it. In Spark, the driver/floor
    lead takes your instructions (carefully written in Python code), translates them
    into Spark steps, and then processes them across the worker. The driver also manages
    which worker/table has which slice of the data, and makes sure you don’t lose
    some bits in the process. The executor/factory worker sits atop the workers/tables
    and performs the actual work on the data.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 就像在大型工厂中一样，你不会去每个员工那里给他们一个任务清单。不，在这里，主节点/管理者负责工人。驱动程序是动作发生的地方。将驱动程序想象成一个楼层领导：你给他们你的步骤清单，让他们处理。在Spark中，驱动程序/楼层领导将你的指令（用Python代码仔细编写）转换为Spark步骤，然后在工作节点上处理它们。驱动程序还管理哪个工作节点/表拥有哪些数据切片，并确保在处理过程中不丢失任何数据。执行器/工厂工人位于工作节点/表之上，并执行实际的数据工作。
- en: 'As a summary:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 作为总结：
- en: The master is like the factory owner, allocating resources as needed to complete
    the jobs.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大师就像工厂主，根据需要分配资源以完成工作。
- en: The driver is responsible for completing a given job. It requests resources
    from the master as needed.
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序负责完成给定的工作。它根据需要从主节点请求资源。
- en: A worker is a set of computing/memory resources, like a workbench in our factory.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点是一组计算/内存资源，就像我们工厂中的工作台一样。
- en: Executors sit atop a worker and perform the work sent by the driver, like employees
    at a workbench.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 执行器位于工作之上，执行由驱动程序发送的工作，就像工作台上的员工一样。
- en: We’ll review the terminology in practice in chapter 11.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第11章中回顾实践中的术语。
- en: Taking the example of listing 1.1 and breaking each instruction one by one,
    PySpark won’t start performing the work until the `write` instruction. If you
    use regular Python or a pandas data frame, which are not lazy (we call this *eager*
    evaluation), each instruction is performed one by one as it’s being read.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 以列表1.1为例，逐条分解每个指令，PySpark不会开始执行工作，直到`write`指令。如果你使用常规Python或pandas数据框（它们不是懒加载的，我们称之为*急切*评估），每个指令都会在读取时逐个执行。
- en: 'Your floor lead/driver has all the qualities a good manager has: it’s smart,
    cautious, and lazy. Wait, what? You read me right. *Laziness* in a programming
    context—and, one could argue, in the real world too—can be a very good thing.
    Every instruction you’re providing in Spark can be classified into two categories:
    transformations and actions. *Actions* are what many programming languages would
    consider I/O. The most typical actions are the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你的楼层领导/驱动程序拥有优秀管理者所具备的所有品质：它聪明、谨慎、懒散。等等？你读对了吗。在编程环境中——甚至可以说在现实世界中——*懒散*可能是一件非常好的事情。你提供给Spark的每个指令都可以分为两类：转换和操作。*操作*是许多编程语言会考虑的I/O。最典型的操作如下：
- en: Printing information on the screen
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在屏幕上打印信息
- en: Writing data to a hard drive or cloud bucket
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据写入硬盘或云存储桶
- en: Counting the number of records
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算记录数
- en: In Spark, we’ll see those instructions most often via the `show()`, `write()`,
    and `count()` methods on a data frame.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在Spark中，我们通常通过数据框上的`show()`、`write()`和`count()`方法看到这些指令。
- en: '![](../Images/01-03.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/01-03.png)'
- en: Figure 1.3 Breaking down the data frame instructions as a series of transformations
    and one action. Each “job” Spark will perform consists of zero or more transformations
    and **one** action.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.3 将数据框指令分解为一系列转换和一个操作。Spark将执行的操作由零个或多个转换和一个**操作**组成。
- en: '*Transformations* are pretty much everything else. Some examples of transformations
    are as follows:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '*转换*几乎涵盖了所有其他内容。以下是一些转换的例子：'
- en: Adding a column to a table
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向表中添加列
- en: Performing an aggregation according to certain keys
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据某些键进行聚合操作
- en: Computing summary statistics
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算汇总统计量
- en: Training a machine learning model
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练一个机器学习模型
- en: Why the distinction, you might ask? When thinking about computation over data,
    you, as the developer, are only concerned about the computation leading to an
    action. You’ll always interact with the results of an action because this is something
    you can see. Spark, with its lazy computation model, will take this to the extreme
    and avoid performing data work until an action triggers the computation chain.
    Before that, the driver will store your instructions. This way of dealing with
    computation has many benefits when dealing with large-scale data.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会问，为什么要有这种区分？当思考对数据进行计算时，作为开发者的你，你只关心计算导致的行为。你将始终与行为的结果进行交互，因为这是你可以看到的东西。Spark凭借其懒计算模型，会将这一点推向极致，直到有行为触发计算链之前，它都会避免执行数据工作。在此之前，驱动程序会存储你的指令。在处理大规模数据时，这种方式处理计算有许多好处。
- en: Note As we see in chapter 5, `count()` is a transformation when applied as an
    aggregation function (where it counts the number of records of each group) but
    an action when applied on a data frame (where it counts the number of records
    in a data frame).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：正如我们在第5章中看到的，`count()` 当作为聚合函数应用时（在这里它计算每个组的记录数）是一个转换，但当它应用于数据框时（在这里它计算数据框中的记录数）则是一个行为。
- en: First, storing instructions in memory takes much less space than storing intermediate
    data results. If you are performing many operations on a data set and are materializing
    the data each step of the way, you’ll blow your storage much faster, although
    you don’t need the intermediate results. We can all agree that less waste is better.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，将指令存储在内存中比存储中间数据结果占用更少的空间。如果你在数据集上执行许多操作，并且每一步都物化数据，那么你会更快地耗尽存储空间，尽管你不需要中间结果。我们都可以同意，减少浪费是更好的。
- en: Second, by having the full list of tasks to be performed available, the driver
    can optimize the work between executors much more efficiently. It can use the
    information available at run time, such as the node where specific parts of the
    data are located. It can also reorder, eliminate useless transformations, combine
    multiple operations, and rewrite some portion of the program more effectively,
    if necessary.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，通过拥有要执行的任务的完整列表，驱动程序可以更有效地优化执行器之间的工作。它可以使用运行时可用信息，例如数据特定部分所在的节点。它还可以重新排序、消除无用的转换、合并多个操作，并在必要时更有效地重写程序的一部分。
- en: '![](../Images/01-04.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/01-04.png)'
- en: 'Figure 1.4 Eager versus lazy evaluation: storing (and computing on the fly)
    transformation saves memory by reducing the need for intermediate data frames.
    It also makes it easier to recreate the data frame if one of the nodes fails.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图1.4：急切与懒计算：存储（和即时计算）转换通过减少对中间数据框的需求来节省内存。它还使得在某个节点失败时更容易重新创建数据框。
- en: Third, should one node fail during processing—computers fail!—Spark will be
    able to recreate the missing chunks of data since it has the instructions cached.
    It’ll read the relevant chunk of data and process it up to where you are without
    the need for you to do anything. With this, you can focus on the data-processing
    aspect of your code, offloading the disaster and recovery part to Spark. Check
    out chapter 11 for more information about compute and memory resources, and how
    to monitor for failures.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，如果在处理过程中某个节点失败——计算机会出故障！——Spark将能够重新创建缺失的数据块，因为它已经缓存了指令。它将读取相关的数据块并处理它，直到你所在的位置，而无需你做任何事情。有了这个，你可以专注于代码的数据处理方面，将灾难恢复部分委托给Spark。有关计算和内存资源以及如何监控失败的信息，请参阅第11章。
- en: Finally, during interactive development, you don’t have to submit a huge block
    of commands and wait for the computation to happen. Instead, you can iteratively
    build your chain of transformation, one at a time, and when you’re ready to launch
    the computation, you can add an action and let Spark work its magic.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在交互式开发过程中，你不必提交一大块命令并等待计算发生。相反，你可以迭代地构建你的转换链，一次一个，当你准备好启动计算时，你可以添加一个行为，让Spark施展其魔法。
- en: Lazy computation is a fundamental aspect of Spark’s operating model and part
    of the reason it’s so fast. Most programming languages, including Python, R, and
    Java, are eagerly evaluated. This means that they process instructions as soon
    as they receive them. With PySpark, you get to use an eager language—Python—with
    a lazy framework—Spark. This can look a little foreign and intimidating, but you
    don’t need to worry. The best way to learn is by doing, and this book provides
    explicit examples of laziness when relevant. You’ll be a lazy pro in no time!
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 惰性计算是Spark操作模型的基本方面，也是其速度如此之快的原因之一。大多数编程语言，包括Python、R和Java，都是即时求值的。这意味着它们在接收到指令后立即处理指令。在PySpark中，你可以使用一个即时语言——Python——和一个惰性框架——Spark。这可能会显得有些陌生和令人生畏，但无需担心。最好的学习方式是通过实践，本书在相关情况下提供了惰性的具体示例。你将很快成为一个懒惰的专家！
- en: 'One aspect to remember is that Spark will not preserve the results of actions
    (or the intermediate data frames) for subsequent computations. If you submit the
    same program twice, PySpark will process the data twice. We use caching to change
    this behavior and optimize certain hot spots in our code (most noticeably when
    training an ML model), and chapter 11 provides you with how and when to cache
    (spoiler: not as often as you’d think).'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 需要记住的一个方面是，Spark不会保留动作（或中间数据帧）的结果以供后续计算使用。如果你两次提交相同的程序，PySpark会两次处理数据。我们使用缓存来改变这种行为，并优化代码中的某些热点（最明显的是在训练机器学习模型时），第11章将向你介绍如何以及何时缓存（剧透：没有你想象的那么频繁）。
- en: Note Reading data, although being I/O, is considered a transformation by Spark.
    In most cases, reading data doesn’t perform any visible work for the user. You,
    therefore, won’t read data until you need to perform some work on it (writing,
    reading, inferring schema; see chapter 6 for more information).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：读取数据，虽然属于I/O操作，但在Spark中被视为转换。在大多数情况下，读取数据不会对用户执行任何可见的工作。因此，你只有在需要对其执行某些操作（写入、读取、推断模式）时才会读取数据（更多信息请参阅第6章）。
- en: What’s a manager without competent employees? Once the task, with its action,
    has been received, the driver starts allocating data to what Spark calls *executors*.
    Executors are processes that run computations and store data for the application.
    Those executors sit on what’s called a *worker node*, which is the actual computer.
    In our factory analogy, an executor is an employee performing the work, while
    the worker node is a workbench where many employees/executors can work.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 没有能干的员工，经理又有什么用？一旦接收到任务及其动作，驱动程序开始将数据分配给Spark所说的*执行器*。执行器是运行计算并存储应用程序数据的进程。这些执行器位于所谓的*工作节点*上，即实际的计算机。在我们的工厂类比中，执行器是执行工作的员工，而工作节点是许多员工/执行器可以工作的工坊。
- en: 'That concludes our factory tour. Let’s summarize our typical PySpark program:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的工厂之旅到此结束。让我们总结一下典型的PySpark程序：
- en: We first encode our instructions in Python code, forming a driver program.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们首先用Python代码编码我们的指令，形成一个驱动程序。
- en: When submitting our program (or launching a PySpark shell), the cluster manager
    allocates resources for us to use. Those will mostly stay constant (with the exception
    of auto-scaling) for the duration of the program.
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们提交程序（或启动PySpark shell）时，集群管理器为我们分配资源。这些资源在程序运行期间将保持不变（除了自动扩展）。
- en: The driver ingests your code and translates it into Spark instructions. Those
    instructions are either transformations or actions.
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 驱动程序将你的代码摄入并转换为Spark指令。这些指令要么是转换，要么是动作。
- en: Once the driver reaches an action, it optimizes the whole computation chain
    and splits the work between executors. Executors are processes performing the
    actual data work, and they reside on machines labeled worker nodes.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当驱动程序遇到动作时，它会优化整个计算链，并在执行器之间分配工作。执行器是执行实际数据工作的进程，它们位于标记为工作节点的机器上。
- en: That’s it! As we can see, the overall process is quite simple, but it’s obvious
    that Spark hides a lot of the complexity that arises from efficient distributed
    processing. For a developer, this means shorter and clearer code, and a faster
    development cycle.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！正如我们所见，整个过程相当简单，但很明显，Spark隐藏了由高效分布式处理产生的许多复杂性。对于开发者来说，这意味着代码更短、更清晰，并且开发周期更快。
- en: 1.3 What will you learn in this book?
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.3 你将在本书中学到什么？
- en: This book will use PySpark to solve a variety of tasks that a data analyst,
    engineer, or scientist will encounter during their day-to-day life. We will therefore
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将使用PySpark解决数据分析师、工程师或科学家在日常工作中可能会遇到的各种任务。因此
- en: Read and write data from (and to) a variety of sources and formats
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从（和到）各种来源和格式读取和写入数据
- en: Deal with messy data with PySpark’s data manipulation functionality
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PySpark的数据操作功能处理杂乱的数据
- en: Discover new data sets and perform exploratory data analysis
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发现新的数据集并执行数据探索性分析
- en: Build data pipelines that transform, summarize, and get insights from data in
    an automated fashion
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建数据管道，以自动化的方式转换、总结并从数据中获得洞察
- en: Troubleshoot common PySpark errors and how to recover from them and avoid them
    in the first place
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 故障排除常见的PySpark错误以及如何从中恢复以及如何从一开始就避免它们
- en: 'After covering those fundamentals, we’ll also tackle different tasks that aren’t
    as frequent but are interesting and excellent ways to showcase the power and versatility
    of PySpark:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在介绍完这些基础知识之后，我们还将处理一些不那么常见但有趣且展示PySpark强大和多功能性的绝佳方式的不同任务：
- en: We’ll build machine learning models, from simple throwaway experiments to robust
    ML pipelines.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将构建机器学习模型，从简单的实验到健壮的机器学习管道。
- en: We’ll work with multiple data formats, from text to tabular to JSON.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将处理多种数据格式，从文本到表格到JSON。
- en: We’ll seamlessly blend Python, pandas, and PySpark code, leveraging the strengths
    of each, and most importantly will scale pandas code to new territories.
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将无缝融合Python、pandas和PySpark代码，利用各自的优点，最重要的是将pandas代码扩展到新的领域。
- en: We are trying to cater to many potential readers but are focusing on people
    with little to no exposure to Spark and/or PySpark. More seasoned practitioners
    might find useful analogies for when they need to explain difficult concepts and
    maybe learn a thing or two!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们试图满足许多潜在读者的需求，但重点是那些对Spark和/或PySpark接触很少或没有接触的人。经验更丰富的从业者可能会在需要解释复杂概念时找到有用的类比，也许还能学到一些新东西！
- en: 1.4 What do I need to get started?
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 1.4 我需要什么来开始？
- en: The book focuses on Spark version 3.2, which is the most recent. The data frame
    made its appearance in Spark 1.3, so some code will work on Spark versions as
    old as this one. For this book, to avoid any headaches, I recommend you use Spark
    version 3.0 or later; if impossible, aim for the most recent version available
    to you.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本书专注于Spark 3.2版本，这是最新的版本。DataFrame首次出现在Spark 1.3版本中，因此一些代码可以在比这更旧的Spark版本上运行。为了避免任何麻烦，我建议您使用Spark
    3.0或更高版本；如果不可能，请尝试使用您可用的最新版本。
- en: We’re assuming some basic Python knowledge; some useful concepts are outlined
    in appendix C. If you want a more in-depth introduction to Python, I recommend
    *The Quick Python Book*, by Naomi Ceder (Manning, 2018; [https://www.manning.com/books/the-quick-python-book-third-edition](https://www.manning.com/books/the-quick-python-book-third-edition)),
    or *Python Workout*, by Reuven M. Lerner (Manning, 2020; [https://www.manning.com/books/python-workout](https://www.manning.com/books/python-workout)).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 我们假设您具备一些基本的Python知识；一些有用的概念在附录C中进行了概述。如果您想要更深入地了解Python，我推荐Naomi Ceder所著的《The
    Quick Python Book》（Manning, 2018；[https://www.manning.com/books/the-quick-python-book-third-edition](https://www.manning.com/books/the-quick-python-book-third-edition)），或者Reuven
    M. Lerner所著的《Python Workout》（Manning, 2020；[https://www.manning.com/books/python-workout](https://www.manning.com/books/python-workout)）。
- en: To get started, the only thing required is a working installation of Spark.
    It can either be on your computer or on a cloud provider (see appendix B). Most
    examples in the book are doable using a local installation of Spark, but some
    may require more horsepower and will be identified as such.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，您只需要一个可工作的Spark安装。它可以是您的电脑上的，也可以是云服务提供商上的（参见附录B）。本书中的大多数示例都可以使用Spark的本地安装来完成，但有些可能需要更强的计算能力，并将被标记出来。
- en: A code editor will also be very useful for writing, reading, and editing scripts
    as you go through the examples and craft your programs. A Python-aware editor,
    such as PyCharm, VS Code, or even Emacs/Vim, is nice to have but is in no way
    necessary. All the examples will work with Jupyter as well; check out appendix
    B to set up your notebook environment.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在您通过示例进行编写、阅读和编辑脚本以及构建程序的过程中，代码编辑器也将非常有用。拥有一个Python感知的编辑器，如PyCharm、VS Code，甚至是Emacs/Vim，会很方便，但并非必需。所有示例都可以与Jupyter一起使用；请查看附录B以设置您的笔记本环境。
- en: The book’s code examples are available on GitHub ([http://mng.bz/6ZOR](http://mng.bz/6ZOR)),
    so Git will be a useful piece of software to have. If you don’t know Git or don’t
    have it handy, GitHub provides a way to download all the book’s code in a zip
    file. Make sure you check regularly for updates!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的代码示例可在GitHub上找到（[http://mng.bz/6ZOR](http://mng.bz/6ZOR)），因此Git将是一个有用的软件工具。如果您不了解Git或没有它，GitHub提供了一个下载所有本书代码的zip文件的方法。请确保定期检查更新！
- en: 'Finally, I recommend that you have an analog way of drafting your code and
    schema. I am a compulsive notetaker and doodler, and even if my drawings are very
    basic and crude, I find that working through a new piece of software via drawings
    helps in clarifying my thoughts. This means rewriting less code and a happier
    programmer! Nothing spiffy is required: some scrap paper and a pencil will do
    wonders.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我建议您有一种模拟的方式来草拟您的代码和模式。我是一个强迫性的笔记记录者和涂鸦者，即使我的绘画非常基础和粗糙，我发现通过绘画来处理新的软件可以帮助澄清我的思路。这意味着需要重写的代码更少，程序员也更快乐！不需要任何花哨的东西：一些废纸和铅笔就能产生奇迹。
- en: Summary
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: PySpark is the Python API for Spark, a distributed framework for large-scale
    data analysis. It provides the expressiveness and dynamism of the Python programming
    language to Spark.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 是 Spark 的 Python API，是一个用于大规模数据分析的分布式框架。它将 Python 编程语言的丰富性和动态性带给 Spark。
- en: 'Spark is fast: it owes its speed to a judicious usage of the RAM available
    and an aggressive and lazy query optimizer.'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 很快：它的速度归功于对可用 RAM 的明智使用以及一个积极和懒加载的查询优化器。
- en: You can use Spark in Python, Scala, Java, R, and more. You can also use SQL
    for data manipulation.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 Spark 在 Python、Scala、Java、R 等多种语言中。您还可以使用 SQL 进行数据处理。
- en: Spark uses a driver that processes the instructions and orchestrates the work.
    The executors receive the instructions from the master and perform the work.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 使用一个驱动程序来处理指令并协调工作。执行器从主节点接收指令并执行工作。
- en: All instructions in PySpark are either transformations or actions. Because Spark
    is lazy, only actions will trigger the computation of a chain of instructions.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 中的所有指令要么是转换，要么是动作。因为 Spark 是懒加载的，只有动作才会触发指令链的计算。
- en: '* * *'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ It can be a fun probability exercise to compute, but I will try to keep the
    math to a minimum.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 这可以是一个有趣的概率练习来计算，但我将尽量将数学简化到最小。
- en: ² Databricks, the company behind Spark, has a project called Photon, which is
    a rewrite of the Spark execution engine in C++.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ² Spark 背后的公司 Databricks 有一个名为 Photon 的项目，这是用 C++ 重写的 Spark 执行引擎。
- en: '³ As always, the standard disclaimer applies: not every Hadoop job will get
    faster in Spark. Your mileage may vary. Always test your job before making large
    architectural changes.'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ³ 如同往常，标准的免责声明适用：并不是每个 Hadoop 作业在 Spark 中都会变快。效果因人而异。在做出大的架构变更之前，请始终测试您的作业。
- en: '⁴ The term *master* is getting phased out. The replacement has not been decided,
    but you can follow the conversation here: [https://issues.apache.org/jira/browse/SPARK-32333](https://issues.apache.org/jira/browse/SPARK-32333).'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ⁴ 术语 *master* 正在逐步淘汰。替代方案尚未确定，但您可以在以下链接中关注讨论：[https://issues.apache.org/jira/browse/SPARK-32333](https://issues.apache.org/jira/browse/SPARK-32333)。
