- en: Chapter 2\. Transformations in Action
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第二章转换实战
- en: In this chapter, we will explore the most important Spark transformations (mappers
    and reducers) in the context of data summarization design patterns, and examine
    how to select specific transformations for targeted problems.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨最重要的Spark转换（映射器和减少器），在数据总结设计模式的背景下，并分析如何选择特定的转换来解决目标问题。
- en: As you will see, for a given problem (we’ll use the DNA base count problem here)
    there are multiple possible PySpark solutions using different Spark transformations,
    but the efficiency of these transformations differs due to their implementation
    and shuffle processes (when the grouping of values by key happens). The DNA base
    count problem is very similar to the classic word count problem (finding the frequency
    of unique words in a set of files/documents), with the difference that in DNA
    base counting you find the frequencies of DNA letters (`A`, `T`, `C`, `G`).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您将看到的，对于给定的问题（我们将在这里使用DNA碱基计数问题），可以使用不同的Spark转换来实现多种可能的PySpark解决方案，但这些转换的效率因其实现和洗牌过程（键的分组发生时）而异。DNA碱基计数问题与经典的单词计数问题非常相似（在一组文件/文档中找到唯一单词的频率），其区别在于在DNA碱基计数中，您会找到DNA字母（`A`，`T`，`C`，`G`）的频率。
- en: I chose this problem because in solving it we will learn about data summarization,
    condensing a large quantity of information (here, DNA data strings/sequences)
    into a much smaller set of useful information (the frequency of DNA letters).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择这个问题是因为在解决它时，我们将了解数据总结，将大量信息（这里是DNA数据字符串/序列）压缩成更小的一组有用信息（DNA字母的频率）。
- en: This chapter provides three complete end-to-end solutions in PySpark, using
    different mappers and reductions to solve the DNA base count problem. We’ll discuss
    the performance differences between them, and explore data summarization design
    patterns.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了三种完整的PySpark端到端解决方案，使用不同的映射器和减少器来解决DNA碱基计数问题。我们将讨论它们之间的性能差异，并探讨数据总结设计模式。
- en: The DNA Base Count Example
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNA碱基计数示例
- en: The purpose of our example in this chapter is to count DNA bases in a given
    set of DNA strings/sequences. Don’t worry, you don’t need to be an expert in DNA,
    biology, or genomics to understand this example. I’ll cover the basics, which
    should be all you need to get the idea.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节中我们的示例目的是计算一组DNA字符串/序列中的DNA碱基数。不用担心，您不需要成为DNA、生物学或基因组学的专家来理解这个例子。我会涵盖基础知识，这应该足以让您理解。
- en: Human DNA consists of about 3 billion bases, and more than 99% of those bases
    are the same in all people. To understand DNA base counting, we need to first
    understand DNA strings. DNA strings are constructed from the alphabet `{A, C,
    G, T}`, whose symbols represent the bases adenine (`A`), cytosine (`C`), guanine
    (`G`), and thymine (`T`). Our DNA is composed of a set of DNA strings. The question
    we want to answer is how many times each base letter occurs in a set of DNA strings.
    For example, if we have the DNA string `"AAATGGCATTA"` and we ask how many times
    the base `A` occurs in this string, the answer is 5; if we ask how many times
    the base `T` occurs in this string, the answer is 3\. So, we want to count the
    number of occurrences of each base letter, ignoring case. Since DNA machines might
    produce uppercase and lowercase letters, we will convert all of them to lowercase.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 人类DNA由大约30亿个碱基组成，超过99%的碱基在所有人中都是相同的。要理解DNA碱基计数，我们首先需要理解DNA字符串。DNA字符串由字母 `{A,
    C, G, T}` 组成，其符号代表腺嘌呤（`A`）、胞嘧啶（`C`）、鸟嘌呤（`G`）和胸腺嘧啶（`T`）的碱基。我们的DNA由一组DNA字符串组成。我们想要回答的问题是在一组DNA字符串中每个碱基字母出现的次数。例如，如果我们有DNA字符串
    `"AAATGGCATTA"` 并询问在这个字符串中碱基 `A` 出现的次数，答案是5；如果我们询问在这个字符串中碱基 `T` 出现的次数，答案是3。因此，我们要计算每个碱基字母的出现次数，忽略大小写。由于DNA机器可能产生大写和小写字母，我们将把它们全部转换为小写。
- en: For this problem, I will provide three distinct solutions using different combinations
    of powerful and efficient Spark transformations. Even though all the solutions
    generate the same results, their performance will be different due to the transformations
    used.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个问题，我将提供三种不同的解决方案，使用不同组合的强大和高效的Spark转换。尽管所有解决方案都生成相同的结果，但由于使用的转换不同，它们的性能将有所不同。
- en: '[Figure 2-1](#dna_base_count) illustrates the process of solving the DNA base
    count problem using Spark. For each solution, we will write a driver program in
    Python using the PySpark API (a series of Spark transformations and actions) and
    submit the program to a Spark cluster. All the solutions will read input (FASTA
    files format, to be defined shortly) and produce a dictionary, where the key is
    a DNA letter and the value is the associated frequency.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '[图2-1](#dna_base_count)说明了使用Spark解决DNA碱基计数问题的过程。对于每个解决方案，我们将使用PySpark API编写一个驱动程序（一系列Spark转换和操作），并将程序提交到一个Spark集群。所有的解决方案都将读取输入（FASTA文件格式，稍后定义）并生成一个字典，其中键是DNA字母，值是相关的频率。'
- en: These three solutions will show that we have choices in selecting Spark transformations
    for solving this problem (and any data problem you are trying to solve) and that
    the performance of the different transformations varies. A summary of the three
    PySpark solutions is provided in [Table 2-1](#solutions_for_dna_base_count).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这三种解决方案将展示我们在选择解决此问题的Spark转换时（以及你尝试解决的任何数据问题时）有多种选择，并且不同转换的性能也会有所不同。关于三种PySpark解决方案的摘要在[表2-1](#solutions_for_dna_base_count)中提供。
- en: '![daws 0201](Images/daws_0201.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0201](Images/daws_0201.png)'
- en: Figure 2-1\. Solving the DNA base count problem
  id: totrans-12
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-1\. 解决DNA碱基计数问题
- en: Table 2-1\. Solutions for the DNA base count problem
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-1\. DNA碱基计数问题的解决方案
- en: '|  | Solution 1 | Solution 2 | Solution 3 |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '|  | 解决方案 1 | 解决方案 2 | 解决方案 3 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| **Program** | *dna_bc_ver_1.py* | *dna_bc_ver_2.py* | *dna_bc_ver_3.py* |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| **程序** | *dna_bc_ver_1.py* | *dna_bc_ver_2.py* | *dna_bc_ver_3.py* |'
- en: '| **Design pattern** | *Basic MapReduce* | *In-mapper combiner* | *Mapping
    partitions* |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **设计模式** | *基本MapReduce* | *In-mapper combiner* | *Mapping partitions* |'
- en: '| **Transformations** | `textFile()` | `textFile()` | `textFile()` |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| **转换** | `textFile()` | `textFile()` | `textFile()` |'
- en: '|  | `flatMap()` | `flatMap()` | `mapPartitions()` |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  | `flatMap()` | `flatMap()` | `mapPartitions()` |'
- en: '|  | `reduceByKey()` | `reduceByKey()` | `reduceByKey()` |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '|  | `reduceByKey()` | `reduceByKey()` | `reduceByKey()` |'
- en: As [Table 2-2](#performance_for_dna_base_count) shows, the three programs performed
    very differently on my machine (a MacBook with 16 GB RAM, a 2.3 GHz Intel processor,
    and a 500 GB hard disk). Note I used the default parameters with the `$SPARK_HOME/bin/spark-submit`
    command for all of them; no optimization was done for any solution.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如[表2-2](#performance_for_dna_base_count)所示，这三个程序在我的机器上表现非常不同（一台配备16 GB RAM、2.3
    GHz英特尔处理器和500 GB硬盘的MacBook）。注意，我对所有解决方案使用了`$SPARK_HOME/bin/spark-submit`命令的默认参数；对任何解决方案都没有进行优化。
- en: Table 2-2\. Performance of the three solutions
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表2-2\. 三种解决方案的性能
- en: '| Input data (in bytes) | Version 1 | Version 2 | Version 3 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 输入数据（以字节为单位） | 版本 1 | 版本 2 | 版本 3 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 253,935,557 | 72 seconds | 27 seconds | 18 seconds |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| 253,935,557 | 72 秒 | 27 秒 | 18 秒 |'
- en: '| 1,095,573,358 | 258 seconds | 79 seconds | 57 seconds |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| 1,095,573,358 | 258 秒 | 79 秒 | 57 秒 |'
- en: 'What does this basic performance table tell you? When you write your PySpark
    applications, you have a lot of choices. There are no hard and fast rules for
    which transformations or actions to use; this will depend on the specifics of
    your data and your program. In general, when you write a PySpark application,
    you can choose from a variety of arrangements of transformations and actions that
    will produce the same results. However, not all these arrangements will result
    in the same performance: avoiding common pitfalls and picking the right combination
    can make a world of difference in an application’s performance.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本性能表告诉你什么？当你编写PySpark应用程序时，你有很多选择。没有硬性规则适用于使用哪些转换或操作；这取决于你的数据和程序的具体情况。一般来说，当你编写PySpark应用程序时，你可以从各种转换和操作的排列中选择，它们会产生相同的结果。然而，并不是所有这些排列都会导致相同的性能：避免常见的陷阱并选择正确的组合可以在应用程序的性能上产生天壤之别。
- en: 'For example, for a large set of (key, value) pairs, using `reduceByKey()` or
    `combineByKey()` is typically more efficient than using the combination of `groupByKey()`
    and `mapValues()`, because they reduce the shuffling time. If your RDD (represented
    by the variable `rdd`) is an `RDD[(String, Integer)]` (an RDD where each element
    is a pair of `(key-as-String, value-as-Integer)`), then this:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于一个大型的(key, value)对集合，通常使用`reduceByKey()`或`combineByKey()`比使用`groupByKey()`和`mapValues()`的组合更高效，因为它们减少了洗牌时间。如果你的RDD（由变量`rdd`表示）是一个`RDD[(String,
    Integer)]`（每个元素都是一个`(key-as-String, value-as-Integer)`对），那么这样做：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'will produce the same results as this:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 将产生与此相同的结果：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: However, the `groupByKey()` operation will transfer the entire dataset across
    the cluster network (incurring a large performance penalty), while the `reduceByKey()`
    operation will compute local sums for each key in each partition and combine those
    local sums into larger sums after shuffling. Therefore, `reduceByKey()` will transfer
    much less data across the cluster network than `groupByKey()`, which means that
    in most situations `reduceByKey()` will outperform the combination of `groupByKey()`
    and `mapValues()`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`groupByKey()` 操作将整个数据集传输到集群网络上（导致性能损失很大），而 `reduceByKey()` 操作将在每个分区中计算每个键的本地总和，并在洗牌后将这些本地总和组合成较大的总和。因此，在大多数情况下，`reduceByKey()`
    将比 `groupByKey()` 和 `mapValues()` 的组合传输更少的数据到集群网络上，这意味着 `reduceByKey()` 在性能上将表现更好。
- en: Now, let’s dig into a little more detail on our DNA base count problem.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们更详细地讨论一下我们的DNA碱基计数问题。
- en: The DNA Base Count Problem
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DNA碱基计数问题
- en: The goal of this example is to find the frequencies (or percentages) of the
    letters `A`, `T`, `C`, `G`, and `N` (the letter `N` denotes any letter other than
    `A`, `T`, `C`, or `G`—i.e., an error) in a given set of DNA sequences. As I mentioned
    earlier, `{'A', 'T', 'C', 'G'}` stand for the four nitrogenous bases associated
    with DNA.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例的目标是找出给定一组DNA序列中字母 `A`、`T`、`C`、`G` 和 `N`（字母 `N` 表示除了 `A`、`T`、`C` 或 `G` 之外的任何字母——即一个错误）的频率（或百分比）。正如我之前提到的，`{'A',
    'T', 'C', 'G'}` 代表与DNA相关的四个含氮碱基。
- en: DNA sequences can be huge—for example, the human genome consists of three billion
    DNA base pairs, while the diploid genome (found in somatic cells) has twice the
    DNA content—and can contain both uppercase and lowercase letters. For consistency,
    we will convert all letters to lowercase. The goal of DNA base counting for our
    example is to generate the frequencies for each DNA base. [Table 2-3](#dna_base_count_example)
    shows the result for the example sequence `"ACGGGTACGAAT"`. Note that I am using
    the key `z` to find the total number of DNA sequences processed.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: DNA序列可能非常庞大——例如，人类基因组由三十亿个DNA碱基对组成，而二倍体基因组（存在于体细胞中）则具有两倍的DNA含量——并且可以包含大小写字母。为了保持一致性，我们将所有字母转换为小写。我们示例中DNA碱基计数的目标是为每个DNA碱基生成频率。[表 2-3](#dna_base_count_example)
    展示了示例序列 `"ACGGGTACGAAT"` 的结果。注意，我正在使用键 `z` 来找出处理的DNA序列的总数。
- en: Table 2-3\. DNA base count example
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-3\. DNA碱基计数示例
- en: '| Base | Count |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| Base | Count |'
- en: '| --- | --- |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `a` | 4 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| `a` | 4 |'
- en: '| `t` | 2 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| `t` | 2 |'
- en: '| `c` | 2 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| `c` | 2 |'
- en: '| `g` | 4 |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| `g` | 4 |'
- en: '| `n` | 0 |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| `n` | 0 |'
- en: '| `z` | 1 (the total number of DNA sequences) |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| `z` | 1（DNA序列的总数） |'
- en: FASTA Format
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: FASTA格式
- en: DNA sequences can be represented in many different formats, including [FASTA](https://oreil.ly/wF0fe)
    and FASTQ. These are popular text-based formats where the input is given as a
    text file. Our solutions will only handle FASTA format, since it is much easier
    to read FASTA files. Both the FASTA and FASTQ formats store sequence data and
    sequence metadata. With some minor modifications to the presented solutions, you
    can use them with inputs in FASTQ format; a FASTQ solution is provided on [GitHub](https://oreil.ly/ogZEA).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: DNA序列可以用许多不同的格式表示，包括[FASTA](https://oreil.ly/wF0fe)和FASTQ。这些是流行的基于文本的格式，其中输入是作为文本文件提供的。我们的解决方案仅处理FASTA格式，因为读取FASTA文件要容易得多。FASTA和FASTQ格式都存储序列数据和序列元数据。通过对呈现的解决方案进行一些小修改，您可以将其用于FASTQ格式的输入；有一个FASTQ解决方案在[GitHub](https://oreil.ly/ogZEA)上提供。
- en: A sequence file in FASTA format can contain many DNA sequences. Each sequence
    begins with a single-line description, followed by one or many lines of sequence
    data. According to the FASTA format specification, the description line must begin
    with a greater-than symbol (>) in the first column. Note that the description
    line may be used for counting the number of sequences and does not contain any
    DNA sequence data.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: FASTA格式的序列文件可以包含许多DNA序列。每个序列都以单行描述开始，后跟一个或多个序列数据行。根据FASTA格式规范，描述行必须以大于号（>）开头的第一列开始。请注意，描述行可用于计算序列的数量，而不包含任何DNA序列数据。
- en: Sample Data
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 样本数据
- en: 'We’ll use the *sample.fasta* file, available in the book’s [GitHub repository](https://oreil.ly/qQLCq),
    as a test case for our PySpark programs. This small FASTA file contains four sample
    DNA sequences (remember, the case of the characters is irrelevant):'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用书籍的[GitHub存储库](https://oreil.ly/qQLCq)中的*sample.fasta*文件作为我们PySpark程序的测试案例。这个小的FASTA文件包含四个样本DNA序列（请记住，字符的大小写是无关紧要的）：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: To test the DNA base count programs provided in this chapter with larger files,
    you can download FASTA data from the [University of California, Santa Cruz website](https://oreil.ly/sv3fs).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试本章提供的 DNA 碱基计数程序与更大文件，请从 [加州大学圣塔克鲁兹分校网站](https://oreil.ly/sv3fs) 下载 FASTA
    数据。
- en: Next, we’ll walk through three distinct PySpark solutions for the DNA base count
    problem, using different Spark transformations. Remember that while the outcome
    of all the solutions is the same (they produce the same results), the performance
    of each solution will differ due to the nature of the data and the transformations
    used.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍三种不同的 PySpark 解决方案，用于 DNA 碱基计数问题，使用不同的 Spark 转换。请记住，尽管所有解决方案的结果相同（它们产生相同的结果），但由于数据的性质和使用的转换方式不同，每个解决方案的性能也会有所不同。
- en: DNA Base Count Solution 1
  id: totrans-54
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNA 碱基计数解决方案 1
- en: The first version I’ll present is a very basic solution for the DNA base count
    problem. The high-level workflow is shown in [Figure 2-2](#dna_base_count_solution_version_1).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我将首先介绍的版本是 DNA 碱基计数问题的非常基本的解决方案。高级工作流程显示在 [图 2-2](#dna_base_count_solution_version_1)
    中。
- en: '![daws 0202](Images/daws_0202.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0202](Images/daws_0202.png)'
- en: Figure 2-2\. DNA base count solution
  id: totrans-57
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-2\. DNA 碱基计数解决方案
- en: 'It consists of three simple steps:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 它由三个简单步骤组成：
- en: Read FASTA input data and create an `RDD[String]`, where each RDD element is
    a FASTA record (it can be either a comment line or an actual DNA sequence).
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取 FASTA 输入数据并创建一个 `RDD[String]`，其中每个 RDD 元素都是一个 FASTA 记录（可以是注释行或实际的 DNA 序列）。
- en: 'Define a mapper function: for every DNA letter in a FASTA record, emit a pair
    of `(dna_letter, 1)`, where `dna_letter` is in `{A, T, C, G}` and `1` is a frequency
    (similar to a word count solution).'
  id: totrans-60
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义一个映射函数：对于 FASTA 记录中的每个 DNA 字母，发出一对 `(dna_letter, 1)`，其中 `dna_letter` 在 `{A,
    T, C, G}` 中，`1` 是频率（类似于单词计数解决方案）。
- en: Sum up the frequencies for all DNA letters (this is a reduction step). For each
    unique `dna_letter`, group and add all frequencies.
  id: totrans-61
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 汇总所有 DNA 字母的频率（这是一个归约步骤）。对于每个唯一的 `dna_letter`，分组并添加所有频率。
- en: To test this solution, I will use the *sample.fasta* file presented earlier.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 要测试这个解决方案，我将使用前面提到的 *sample.fasta* 文件。
- en: 'Step 1: Create an RDD[String] from the Input'
  id: totrans-63
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 1 步：从输入创建一个 RDD[String]
- en: 'The `SparkContext.textFile()` function is used to create an `RDD[String]` for
    input in FASTA text-based format. `textFile()` can be used to read a text file
    from HDFS, Amazon S3, a local filesystem (available on all Spark nodes), or any
    Hadoop-supported filesystem URI, and return it as an `RDD[String]`. If `spark`
    is an instance of the `SparkSession` class, then to create a FASTA records RDD
    (as denoted by `records_rdd`), we have at least two options. We can use the `SparkSession`:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `SparkContext.textFile()` 函数创建一个输入格式为 FASTA 文本的 `RDD[String]`。`textFile()`
    可以用于从 HDFS、Amazon S3、本地文件系统（所有 Spark 节点都可用）或任何支持 Hadoop 文件系统 URI 的文件中读取文本文件，并将其作为
    `RDD[String]` 返回。如果 `spark` 是 `SparkSession` 类的实例，则要创建一个 FASTA 记录的 RDD（如 `records_rdd`
    所示），我们至少有两个选项。我们可以使用 `SparkSession`：
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO1-1)'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO1-1)'
- en: Define the input path.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入路径。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO1-2)'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO1-2)'
- en: Use the `DataFrameReader` interface (accessed with `spark.read`) to create a
    DataFrame and then convert it to an `RDD[String]`.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `DataFrameReader` 接口（通过 `spark.read` 访问）创建一个 DataFrame，然后将其转换为 `RDD[String]`。
- en: DataFrameReader and DataFrameWriter
  id: totrans-70
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DataFrameReader 和 DataFrameWriter
- en: Spark’s `DataFrameReader` class is an interface to read data from external data
    sources—such as text, CSV, and JSON files, Parquet and ORC files, Hive tables,
    or Java Database Connectivity (JDBC)-compliant database tables—into a DataFrame.
    Its `DataFrameWriter` class is an interface to write a DataFrame into an external
    data source.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '`DataFrameReader` 类是一个接口，用于从外部数据源（如文本、CSV 和 JSON 文件、Parquet 和 ORC 文件、Hive 表或符合
    Java 数据库连接（JDBC）的数据库表）读取数据到 DataFrame 中。其 `DataFrameWriter` 类是一个接口，用于将 DataFrame
    写入外部数据源。'
- en: 'Or we can use the `SparkContext`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 或者我们可以使用 `SparkContext`：
- en: '[PRE4]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO2-1)'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO2-1)'
- en: Define the input path.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 定义输入路径。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO2-2)'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO2-2)'
- en: Create an instance of `SparkContext` (as `sc`).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 `SparkContext` 的一个实例（作为 `sc`）。
- en: '[![3](Images/3.png)](#co_transformations_in_action_CO2-3)'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformations_in_action_CO2-3)'
- en: Use the `SparkContext` to read input and create an `RDD[String]`.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `SparkContext` 读取输入并创建 `RDD[String]`。
- en: The second option is preferable, because it is easy and efficient. The first
    one works too, but it’s less efficient because it first creates a DataFrame, then
    converts it to an RDD, and eventually performs another mapper transformation.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项更可取，因为它简单且高效。第一个选项也可以使用，但效率较低，因为它首先创建一个DataFrame，然后将其转换为RDD，最后执行另一个映射器转换。
- en: 'Next we’ll examine the contents of the created RDD. Each RDD element (as a
    `String`) is denoted by `u''*<element>*''`:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将检查创建的RDD的内容。每个RDD元素（作为`String`）由`u'*<element>*'`表示：
- en: '[PRE5]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Tip
  id: totrans-83
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `RDD.collect()` method is used here to get the content as a list of `String`
    objects and display it. As mentioned in [Chapter 1](ch01.xhtml#Chapter-01), for
    large RDDs you should not use `collect()`, which might cause OOM errors as well
    as incurring a performance penalty. To just view the first *`N`* elements of an
    RDD, you may use `RDD.take(*N*)`.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此处使用`RDD.collect()`方法获取内容作为`String`对象列表并显示它。如[第1章](ch01.xhtml#Chapter-01)所述，对于大型RDD，不应使用`collect()`，这可能导致OOM错误，并带来性能损失。要仅查看RDD的前*N*个元素，可以使用`RDD.take(*N*)`。
- en: 'Step 2: Define a Mapper Function'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤2：定义映射函数
- en: 'To map RDD elements into a set of pairs `(dna_letter, 1)`, we’ll need to define
    a Python function that will be passed to the `flatMap()` transformation. `flatMap()`
    is a 1-to-many transformation; it returns a new RDD by first applying a function
    to all elements of the source RDD and then flattening the results. For example,
    if the Python function we pass to the `flatMap()` transformation returns a list
    as `[V[1], V[2], V[3]]`, then that will be flattened into three target RDD elements,
    `V[1]`, `V[2]`, and `V[3]`. Informally, we can write this as:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 要将RDD元素映射为一组`(dna_letter, 1)`对，我们需要定义一个Python函数，该函数将传递给`flatMap()`转换。`flatMap()`是一种一对多的转换方式；它通过首先对源RDD的所有元素应用函数，然后展平结果来返回一个新的RDD。例如，如果我们传递给`flatMap()`转换的Python函数返回一个列表，如`[V[1],
    V[2], V[3]]`，那么这将被展平为三个目标RDD元素，`V[1]`、`V[2]`和`V[3]`。非正式地说，我们可以将其写成：
- en: 'Create an iterable list:'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个可迭代列表：
- en: '[PRE6]'
  id: totrans-88
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Flatten the list into many elements (here, three target elements):'
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将列表展平为多个元素（此处为三个目标元素）：
- en: '[PRE7]'
  id: totrans-90
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'For this solution we’ll define a function, `process_FASTA_record()`, that accepts
    an RDD element (a single record of the FASTA file as a `String`) and returns a
    list of pairs as `(dna_letter, 1)`. For example, given the input record `"AATTG"`,
    it will emit the following (key, value) pairs (recall that we’re converting all
    the DNA letters to lowercase):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于此解决方案，我们将定义一个名为`process_FASTA_record()`的函数，该函数接受一个RDD元素（FASTA文件的单个记录作为`String`）并返回一个`(dna_letter,
    1)`对的列表。例如，给定输入记录`"AATTG"`，它将发出以下`(key, value)`对（请记住，我们将所有DNA字母转换为小写）：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If the input is a description record (which contains no sequence data and begins
    with `>seq`), then we emit `(z, 1)`. This will enable us to find the number of
    sequences as well. If the input is a DNA sequence we first tokenize it by characters
    and then, for each DNA letter (denoted by `dna_letter`), we emit `(dna_letter,
    1)`. Finally, we return a list of these pairs. The function definition follows.
    Note that I have included some `print` statements for debugging purposes, but
    in a production environment these should be removed as they will cause performance
    penalties:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果输入是描述记录（不包含序列数据并以`>seq`开头），则我们发出`(z, 1)`。这将使我们能够找到序列的数量。如果输入是DNA序列，我们首先按字符令牌化它，然后对每个DNA字母（由`dna_letter`表示）发出`(dna_letter,
    1)`。最后，我们返回这些对的列表。函数定义如下。请注意，我包含了一些用于调试目的的`print`语句，但在生产环境中，这些应该删除，因为它们会导致性能损失：
- en: '[PRE9]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO3-1)'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO3-1)'
- en: Create an empty list, to which we will add (key, value) pairs (this is our output
    from this function).
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个空列表，我们将向其添加`(key, value)`对（这是此函数的输出）。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO3-2)'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO3-2)'
- en: Append `(z, 1)` to the list.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将`(z, 1)`添加到列表中。
- en: '[![3](Images/3.png)](#co_transformations_in_action_CO3-3)'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformations_in_action_CO3-3)'
- en: Append `(c, 1)` to the list, where `c` is a DNA letter.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将`(c, 1)`添加到列表中，其中`c`是一个DNA字母。
- en: '[![4](Images/4.png)](#co_transformations_in_action_CO3-4)'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformations_in_action_CO3-4)'
- en: For debugging purposes only.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 仅供调试目的。
- en: '[![5](Images/5.png)](#co_transformations_in_action_CO3-5)'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_transformations_in_action_CO3-5)'
- en: Return a list of (key, value) pairs, which will be flattened by the `flatMap()`
    transformation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 返回一个`(key, value)`对的列表，这将由`flatMap()`转换展平。
- en: 'Now, we will use this function to apply the `flatMap()` transformation to the
    `records_rdd` (`RDD[String]`) we just created:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用此函数将`flatMap()`转换应用于刚刚创建的`records_rdd`（`RDD[String]`）。
- en: '[PRE10]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO4-1)'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO4-1)'
- en: The source RDD (`records_rdd`) is an `RDD[String]`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 源RDD（`records_rdd`）是一个`RDD[String]`。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO4-2)'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO4-2)'
- en: We use a lambda expression, where `rec` denotes a single element of `records_rdd`.
    The target RDD (`pairs_rdd`) is an `RDD[(String, Integer)]`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用lambda表达式，其中`rec`表示`records_rdd`的单个元素。目标RDD（`pairs_rdd`）是一个`RDD[(String,
    Integer)]`。
- en: 'Alternatively, we can write it as follows (without using a lambda expression):'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以按以下方式编写（不使用lambda表达式）：
- en: '[PRE11]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'For example, if an element of `records_rdd` contains the DNA sequence as `"gaattcg"`,
    then it will be flattened into the following (key, value) pairs:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果`records_rdd`的元素包含DNA序列`"gaattcg"`，那么它将被展开为以下的（键，值）对：
- en: '[PRE12]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'If an element of `records_rdd` contains `>seq`, then it will be flattened into
    the following (key, value) pair (recall that we use the key `z` to find the total
    number of DNA sequences for a given input):'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果`records_rdd`的元素包含`>seq`，那么它将被展开为以下的（键，值）对（请记住我们使用键`z`来找到给定输入的DNA序列的总数）：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Step 3: Find the Frequencies of DNA Letters'
  id: totrans-117
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤3：查找DNA字母的频率
- en: '`pairs_rdd` now contains a set of (key, value) pairs where the key is a DNA
    letter and the value is its frequency (`1`). Next, we apply the `reduceByKey()`
    transformation to `pairs_rdd` to find the aggregated frequencies for all DNA letters.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '`pairs_rdd`现在包含一组（键，值）对，其中键是DNA字母，值是其频率（`1`）。接下来，我们将`reduceByKey()`转换应用于`pairs_rdd`以找到所有DNA字母的聚合频率。'
- en: 'The `reduceByKey()` transformation merges the values for each unique key using
    an associative and commutative reduce function (we’ll use addition as our reduction
    function). Therefore, we can now see that we are simply taking an *accumulated
    value* for the given key and summing it with the *next value* of that key. In
    other words, if key `K` has five pairs in the RDD, `(K, 2)`, `(K, 3)`, `(K, 6)`,
    `(K, 7)`, and `(K, 8)`, then the `reduceByKey()` transformation will transform
    these five pairs into a single pair, `(K, 26)` (because 2 + 3 + 6 + 7 + 8 = 26).
    If these five pairs were stored on two partitions, each partition would be processed
    in parallel and independently:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`reduceByKey()`转换使用可结合和可交换的减少函数合并每个唯一键的值。因此，我们现在可以看到，我们只是为给定键获取了一个累积值，并将其与该键的下一个值相加。换句话说，如果键`K`在RDD中有五对，`(K,
    2)`，`(K, 3)`，`(K, 6)`，`(K, 7)`和`(K, 8)`，那么`reduceByKey()`转换将这五对转换为一对，`(K, 26)`（因为2
    + 3 + 6 + 7 + 8 = 26）。如果这五对存储在两个分区上，则每个分区将并行和独立地处理：'
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[PRE15]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'And then the partitions would be merged:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将合并分区：
- en: '[PRE16]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'To produce the final result, we use the `reduceByKey()` transformation:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 要生成最终结果，我们使用`reduceByKey()`转换：
- en: '[PRE17]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that the source and target data types for `reduceByKey()` are the same.
    That is, if the source RDD is an `RDD[(K, V)]` then the target RDD will also be
    an `RDD[(K, V)]`. Spark’s `combineByKey()` transformation does not have the data
    type restrictions for values imposed by `reduceByKey()`.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`reduceByKey()`的源和目标数据类型是相同的。也就是说，如果源RDD是`RDD[(K, V)]`，那么目标RDD也将是`RDD[(K,
    V)]`。Spark的`combineByKey()`转换对`reduceByKey()`所强加的值的数据类型限制并不适用。
- en: 'There are several ways that you can view the final output. For example, you
    can use the `RDD.collect()` function to get the final RDD’s elements as a list
    of pairs:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过使用`RDD.collect()`函数将最终RDD的元素作为一组对获取最终输出的几种方法：
- en: '[PRE18]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Or you can use the `RDD.collectAsMap()` action to return the result as a hash
    map:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用`RDD.collectAsMap()`操作将结果作为哈希映射返回：
- en: '[PRE19]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can also use other Spark transformations to aggregate frequencies of DNA
    letters. For example, you could group the frequencies (using `groupByKey()`) by
    DNA letter and then add all the frequencies together. This solution is, however,
    less efficient than using the `reduceByKey()` transformation:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用其他Spark转换来聚合DNA字母的频率。例如，您可以通过DNA字母对其频率进行分组（使用`groupByKey()`）然后将所有频率相加。但是，这种解决方案比使用`reduceByKey()`转换效率低：
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO5-1)'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO5-1)'
- en: '`grouped_rdd` is an `RDD[(String, [Integer])]`, where the key is a `String`
    and the value is a list/iterable of `Integers` (as frequencies).'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`grouped_rdd`是一个`RDD[(String, [Integer])]`，其中键是一个`String`，值是一个整数列表/可迭代对象（作为频率）。'
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO5-2)'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO5-2)'
- en: '`frequencies_rdd` is an `RDD[(String, Integer)]`.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`frequencies_rdd` 是一个 `RDD[(String, Integer)]`。'
- en: For example, if `pairs_rdd` contains four pairs of `('z', 1)`, then `grouped_rdd`
    will have a single pair of `('z', [1, 1, 1, 1])`. That is, it groups values for
    the same key. While both of these transformations (`reduceByKey()` and `groupByKey()`)
    produce the correct answer, `reduceByKey()` works much better on a large FASTA
    dataset. That’s because Spark knows it can combine output with a common key (DNA
    letter) on each partition before shuffling the data. Spark experts recommend that
    we avoid `groupByKey()` and use `reduceByKey()` and `combineByKey()` whenever
    possible, as they scale out better than `groupByKey()`.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果 `pairs_rdd` 包含四对 `('z', 1)`，那么 `grouped_rdd` 将有一个单一的对 `('z', [1, 1, 1,
    1])`。即，它会对相同的键进行值的分组。虽然 `reduceByKey()` 和 `groupByKey()` 这两种转换都能产生正确的答案，但在大型 FASTA
    数据集上，`reduceByKey()` 的效果要好得多。这是因为 Spark 知道可以在每个分区上在数据洗牌之前将具有相同键（DNA 字母）的输出进行组合。Spark
    专家建议我们尽可能避免使用 `groupByKey()`，而是在可能的情况下使用 `reduceByKey()` 和 `combineByKey()`，因为它们比
    `groupByKey()` 更适合扩展。
- en: If you want to save your created RDD to disk, you can use `RDD.saveAsTextFile(*path*)`,
    where *`path`* is your output directory name.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想将创建的 RDD 保存到磁盘，可以使用 `RDD.saveAsTextFile(*path*)`，其中 *`path`* 是你的输出目录名称。
- en: Pros and Cons of Solution 1
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 1 的优缺点
- en: 'Let’s take a look at some of the pros and cons of this solution:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个解决方案的一些优缺点：
- en: Pros
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: The provided solution works and is simple. It uses minimal code to get the job
    done with Spark’s `map()` and `reduceByKey()` transformations.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的解决方案可行且简单。它使用最少的代码来完成任务，使用了 Spark 的 `map()` 和 `reduceByKey()` 转换。
- en: There is no scalability issue since we use `reduceByKey()` to reduce all the
    (key, value) pairs. This transformation will automatically perform the `combine()`
    optimization (local aggregation) on all worker nodes.
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `reduceByKey()` 来减少所有 (key, value) 对，不存在可扩展性问题。此转换将自动在所有工作节点上执行 `combine()`
    优化（局部聚合）。
- en: Cons
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点
- en: This solution emits a large number of (key, value) pairs (one for each letter
    in the input), which might cause memory problems. If you get an error because
    too many (key, value) pairs are produced, try adjusting the RDD’s `StorageLevel`.
    By default, Spark uses `MEMORY_ONLY`, but you can set the `StorageLevel` to `MEMORY_AND_DISK`
    for this RDD.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个解决方案会产生大量的 (key, value) 对（每个输入字母一个）。这可能会导致内存问题。如果因为产生了太多 (key, value) 对而出现错误，请尝试调整
    RDD 的 `StorageLevel`。默认情况下，Spark 使用 `MEMORY_ONLY`，但你可以为这个 RDD 设置 `StorageLevel`
    为 `MEMORY_AND_DISK`。
- en: Performance is not optimal because emitting a large number of (key, value) pairs
    will place a high load on the network and prolong the shuffle time. The network
    will be a bottleneck when scaling this solution.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能并不理想，因为发出大量的 (key, value) 对会对网络造成高负载并延长洗牌时间。当扩展此解决方案时，网络将成为瓶颈。
- en: Next, I’ll present a second solution for the DNA base count problem.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将为 DNA 基数计数问题提出第二个解决方案。
- en: DNA Base Count Solution 2
  id: totrans-148
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNA 基数计数 解决方案 2
- en: Solution 2 is an improved version of solution 1\. In solution 1, we emitted
    pairs of `(dna_letter, 1)` for each DNA letter in the input DNA sequences. FASTA
    sequences can be very long, with multiple `(dna_letter, 1)` pairs per DNA letter.
    So, in this version we will perform an in-mapper combining optimization (a design
    pattern discussed in much greater depth in [Chapter 10](ch10.xhtml#unique_chapter_id_10))
    to reduce the number of intermediate (key, value) pairs that are emitted by the
    mapper. We will aggregate the `(dna_letter, 1)` pairs into a hash map (an unordered
    collection of (key, value) pairs stored in a hash table, where the keys are unique),
    then flatten the hash map into a list and finally aggregate the frequencies. For
    example, given the FASTA sequence record `"aaatttcggggaa"`, the values in column
    2 of [Table 2-4](#emitted_pairs) will be emitted instead of the values in column
    1 (as in solution 1).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案 2 是解决方案 1 的改进版。在解决方案 1 中，我们对输入的 DNA 序列中的每个 DNA 字母发出了 `(dna_letter, 1)`
    对。FASTA 序列可能非常长，每个 DNA 字母可能有多个 `(dna_letter, 1)` 对。因此，在这个版本中，我们将执行一种内部映射器组合优化（在[第10章](ch10.xhtml#unique_chapter_id_10)中详细讨论的设计模式），以减少映射器发出的中间
    (key, value) 对的数量。我们将把 `(dna_letter, 1)` 对聚合到一个哈希映射（存储在哈希表中的无序 (key, value) 对集合，其中键是唯一的），然后将哈希映射扁平化为一个列表，并最终聚合频率。例如，给定
    FASTA 序列记录 `"aaatttcggggaa"`，[表2-4](#emitted_pairs)的第2列中的值将被发出，而不是第1列中的值（就像解决方案
    1 中一样）。
- en: Table 2-4\. Emitted (key, value) pairs for the sequence “aaatttcggggaa”
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 表 2-4\. 序列 "aaatttcggggaa" 的发出的 (key, value) 对
- en: '| Solution 1 | Solution 2 |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| 解决方案 1 | 解决方案 2 |'
- en: '| --- | --- |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `(a, 1)` | `(a, 5)` |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `(a, 1)` | `(a, 5)` |'
- en: '| `(a, 1)` | `(t, 3)` |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `(a, 1)` | `(t, 3)` |'
- en: '| `(a, 1)` | `(c, 1)` |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `(a, 1)` | `(c, 1)` |'
- en: '| `(t, 1)` | `(g, 4)` |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `(t, 1)` | `(g, 4)` |'
- en: '| `(t, 1)` |  |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `(t, 1)` |  |'
- en: '| `(t, 1)` |  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| `(t, 1)` |  |'
- en: '| `(c, 1)` |  |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| `(c, 1)` |  |'
- en: '| `(g, 1)` |  |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| `(g, 1)` |  |'
- en: '| `(g, 1)` |  |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| `(g, 1)` |  |'
- en: '| `(g, 1)` |  |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `(g, 1)` |  |'
- en: '| `(g, 1)` |  |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `(g, 1)` |  |'
- en: '| `(a, 1)` |  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `(a, 1)` |  |'
- en: '| `(a, 1)` |  |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `(a, 1)` |  |'
- en: The advantage of this solution is that it will emit many fewer (key, value)
    pairs, which will reduce the cluster network traffic and hence improve the overall
    performance of our program.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 此解决方案的优点在于它将发出较少的`(key, value)`对，从而减少集群网络流量，提高程序的整体性能。
- en: 'Solution 2 can be summarized as follows:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案2可以总结如下：
- en: Read FASTA input data and create an `RDD[String]`, where each RDD element is
    a FASTA record. This step is the same as in solution 1.
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取FASTA输入数据并创建一个`RDD[String]`，其中每个RDD元素都是一个FASTA记录。此步骤与解决方案1中的步骤相同。
- en: For every FASTA record, create a `HashMap[Key, Value]` (a dictionary or hash
    table) where the `key` is a DNA letter and the `value` is an aggregated frequency
    for that letter. Then, flatten the hash map (using Spark’s `flatMap()`) into a
    list of (key, value) pairs. This step is different from solution 1 and enables
    us to emit fewer (key, value) pairs.
  id: totrans-169
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个FASTA记录，创建一个`HashMap[Key, Value]`（字典或哈希表），其中`key`是一个DNA字母，`value`是该字母的聚合频率。然后，展开哈希映射（使用Spark的`flatMap()`）为`(key,
    value)`对的列表。此步骤与解决方案1不同，并且使我们能够发出较少的`(key, value)`对。
- en: For each DNA letter, aggregate and sum all the frequencies. This is a reduction
    step, and it is the same as in solution 1.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个DNA字母，聚合并求出所有频率的总和。这是一个归约步骤，与解决方案1中的步骤相同。
- en: The workflow is presented visually in [Figure 2-3](#dna_base_count_solution_version_2).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 工作流程在[图2-3](#dna_base_count_solution_version_2)中以图像方式呈现。
- en: '![daws 0203](Images/daws_0203.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0203](Images/daws_0203.png)'
- en: Figure 2-3\. DNA base count solution 2
  id: totrans-173
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-3. DNA碱基计数解决方案2
- en: Let’s dig into the details of each step.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解每个步骤的详细信息。
- en: 'Step 1: Create an RDD[String] from the Input'
  id: totrans-175
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：从输入创建RDD[String]。
- en: 'The `SparkContext.textFile()` function is used to create an RDD for input in
    FASTA text-based format. Let `spark` be a `SparkSession` object:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext.textFile()`函数用于创建基于FASTA文本格式的输入的RDD。让`spark`成为一个`SparkSession`对象：'
- en: '[PRE21]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO6-1)'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO6-1)'
- en: '`records_rdd` is an `RDD[String]`.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`records_rdd`是一个`RDD[String]`。'
- en: 'Step 2: Define a Mapper Function'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：定义一个映射函数
- en: Next, we’ll map each RDD element (which represents a single FASTA record as
    a `String`) into a list of (key, value) pairs, where the key is a unique DNA letter
    and the value is an aggregated frequency for the entire record.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将每个RDD元素（代表单个FASTA记录的字符串）映射为`(key, value)`对的列表，其中key是唯一的DNA字母，value是整个记录的聚合频率。
- en: We define a Python function, which is passed to the `flatMap()` transformation
    to return a new RDD, by first applying a function to all elements of this RDD
    and then flattening the results.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了一个Python函数，该函数传递给`flatMap()`转换，以返回一个新的RDD，首先将函数应用于该RDD的所有元素，然后展开结果。
- en: 'To process the RDD elements, we’ll define a Python function, `p⁠r⁠o⁠c⁠e⁠s⁠s​_⁠F⁠A⁠S⁠T⁠A⁠_as_hashmap`,
    which accepts an RDD element as a `String` and returns a list of `(dna_letter,
    frequency)`. Note that I’ve included some `print` statements here for debugging
    and teaching purposes, which should be removed for production environments:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理RDD元素，我们将定义一个Python函数，`p⁠r⁠o⁠c⁠e⁠s⁠s​_⁠F⁠A⁠S⁠T⁠A⁠_as_hashmap`，它接受一个RDD元素作为`String`并返回`(dna_letter,
    frequency)`的列表。请注意，我在这里包含了一些用于调试和教学目的的`print`语句，这些语句应在生产环境中删除：
- en: '[PRE22]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO7-1)'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO7-1)'
- en: '`>` indicates a comment line in a DNA sequence.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: '`>`表示DNA序列中的注释行。'
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO7-2)'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO7-2)'
- en: Create a `dict[String, Integer]`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`dict[String, Integer]`。
- en: '[![3](Images/3.png)](#co_transformations_in_action_CO7-3)'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformations_in_action_CO7-3)'
- en: Aggregate the DNA letters.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 聚合DNA字母。
- en: '[![4](Images/4.png)](#co_transformations_in_action_CO7-4)'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformations_in_action_CO7-4)'
- en: Flatten the dictionary into a list of `(dna_letter, frequency)` pairs.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 将字典展平为`(dna_letter, frequency)`对的列表。
- en: '[![5](Images/5.png)](#co_transformations_in_action_CO7-5)'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_transformations_in_action_CO7-5)'
- en: Return the flattened list of `(dna_letter, frequency)` pairs.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 返回展平的`(dna_letter, frequency)`对列表。
- en: 'Now, we will use this Python function, to apply the `flatMap()` transformation
    to the `records_rdd` (an `RDD[String]`) created earlier:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将使用这个Python函数，对之前创建的`records_rdd`（一个`RDD[String]`）应用`flatMap()`转换：
- en: '[PRE23]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Alternatively, we can write this as follows without the lambda expression:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以这样写，而不使用 lambda 表达式：
- en: '[PRE24]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'For example, if the `records_rdd` element contains `''gggggaaattccccg''`, then
    it will be flattened into the following (key, value) pairs:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果`records_rdd`元素包含`'gggggaaattccccg'`，则它将被展开为以下的（键，值）对：
- en: '[PRE25]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To enable us to count the total number of DNA sequences, any `records_rdd`
    elements that begin with `">seq"` will be flattened into the following (key, value)
    pair:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们能够计算DNA序列的总数，任何以`">seq"`开头的`records_rdd`元素将被展开为以下的（键，值）对：
- en: '[PRE26]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Step 3: Find the Frequencies of DNA Letters'
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 步骤 3：查找DNA字母的频率
- en: 'Now, `pairs_rdd` contains (key, value) pairs where the key is a `dna_letter`
    and the value is the frequency of that letter. Next, we apply the `reduceByKey()`
    transformation to `pairs_rdd` to find the aggregated frequencies for all DNA letters.
    Recall that `''n''` is the key used to denote any letter other than `a`, `t`,
    `c`, or `g`:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`pairs_rdd`包含了（键，值）对，其中键是`dna_letter`，值是该字母的频率。接下来，我们对`pairs_rdd`应用`reduceByKey()`转换，以找到所有DNA字母的聚合频率。请记住，`'n'`是用来表示除了`a`、`t`、`c`或`g`之外的任何字母的键：
- en: '[PRE27]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO8-1)'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO8-1)'
- en: '`pairs_rdd` is an `RDD[(String, Integer)]`.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '`pairs_rdd`是一个`RDD[(String, Integer)]`。'
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO8-2)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO8-2)'
- en: '`frequencies_rdd` is an `RDD[(String, Integer)]`.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`frequencies_rdd`是一个`RDD[(String, Integer)]`。'
- en: 'Alternatively, we can use the `collectAsMap()` action to return the result
    as a hash map:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用`collectAsMap()`操作将结果返回为一个哈希映射：
- en: '[PRE28]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Pros and Cons of Solution 2
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案 2 的优缺点
- en: 'Let’s examine the pros and cons of this solution:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析这种解决方案的优缺点：
- en: Pros
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: The provided solution works and is simple and semi-efficient. It improves on
    the previous version by emitting many fewer (key, value) pairs—at most six per
    DNA sequence, since we create a dictionary per input record and then flatten it
    into a list of (key, value) pairs, where the key is a DNA letter and the value
    is the aggregated frequency of that letter.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的解决方案有效且简单，半高效。它通过发射的（键，值）对数量大大减少——每个DNA序列最多只有六个，因为我们为每个输入记录创建一个字典，然后将其展开为（键，值）对列表，其中键是DNA字母，值是该字母的聚合频率。
- en: Network traffic demands are lower due to the reduction in the number of (key,
    value) pairs emitted.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于发出的（键，值）对数量减少，网络流量需求较低。
- en: There is no scalability issue since we use `reduceByKey()` for reducing all
    the (key, value) pairs.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们使用`reduceByKey()`来减少所有（键，值）对，因此不存在可扩展性问题。
- en: Cons
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点
- en: Performance is not optimal, since we are still emitting up to six (key, value)
    pairs per DNA string.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 性能不佳，因为我们仍然会发出每个DNA字符串最多六个（键，值）对。
- en: With large datasets or limited resources this solution might still use too much
    memory due to the creation of a dictionary per DNA sequence.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大型数据集或资源有限的情况，这种解决方案可能仍然会因为每个DNA序列创建一个字典而占用过多内存。
- en: DNA Base Count Solution 3
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: DNA碱基计数解决方案 3
- en: This final solution improves on versions 1 and 2 and is an optimal solution
    with no scalability issues at all. Here, we’ll solve the DNA base count problem
    using a powerful and efficient Spark transformation called `mapPartitions()`.
    Before I present the solution itself, let’s take a closer look at this transformation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终解决方案改进了版本 1 和 2，并且是一个没有任何可扩展性问题的最佳解决方案。在这里，我们将使用一种名为`mapPartitions()`的强大且高效的Spark转换来解决DNA碱基计数问题。在我介绍解决方案本身之前，让我们更仔细地看看这个转换。
- en: The mapPartitions() Transformation
  id: totrans-223
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: mapPartitions() 转换
- en: 'If the source RDD is `RDD[T]` and the target RDD is `RDD[U]`, the `mapPartitions()`
    transformation is defined as:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 如果源RDD是`RDD[T]`，目标RDD是`RDD[U]`，则`mapPartitions()`转换定义如下：
- en: '[PRE29]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO9-1)'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO9-1)'
- en: The function `f()` accepts a pointer to a single partition (as an `iterator`
    of type `T`) and returns an object of type `U`; `T` and `U` can be any data types
    and they do not have to be the same.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f()`接受一个指向单个分区的指针（作为`iterator`类型的`T`）并返回一个类型为`U`的对象；`T`和`U`可以是任何数据类型，它们不必相同。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO9-2)'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO9-2)'
- en: Transform an `RDD[T]` to `RDD[U]`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 将`RDD[T]`转换为`RDD[U]`。
- en: To understand the semantics of the `mapPartitions()` transformation, first you
    must understand the concept of a partition and partitioning in Spark. Informally,
    using Spark’s terminology, input data (in this case, DNA sequences in FASTA format)
    is represented as an RDD. Spark automatically partitions RDDs and distributes
    the partitions across nodes. As an example, say that we have 6 billion records,
    and the Spark partitioner partitions the input data into 3,000 chunks/partitions.
    Each partition will have roughly 2 million records and will be processed by a
    single `mapPartitions()` transformation. Therefore, the function `f()` that is
    used in the `mapPartitions()` transformation will accept an iterator (as an argument)
    to handle one partition.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解 `mapPartitions()` 转换的语义，首先必须理解 Spark 中分区和分区的概念。简单地说，使用 Spark 的术语，输入数据（在本例中为
    FASTA 格式的 DNA 序列）表示为 RDD。Spark 自动分区 RDD，并将分区分布在节点上。例如，假设我们有 60 亿条记录，并且 Spark 的分区器将输入数据分为
    3,000 个块/分区。每个分区将大约有 2 百万条记录，并且将由单个 `mapPartitions()` 转换处理。因此，用于 `mapPartitions()`
    转换中的函数 `f()` 将接受一个迭代器（作为参数），以处理一个分区。
- en: In solution 3, we will create one dictionary per partition rather than a dictionary
    per FASTA record to aggregate DNA letters and their associated frequencies. This
    is a huge improvement over solutions 1 and 2, as the creation of 3,000 hash tables
    in a cluster uses very little memory compared to creating a dictionary per input
    record. This solution is highly scalable and fast, due to the concurrent and independent
    processing of all partitions in the cluster.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三种解决方案中，我们将为每个分区创建一个字典，而不是每个 FASTA 记录创建一个字典，以聚合 DNA 字母及其关联的频率。这比解决方案 1 和 2
    要好得多，因为在集群中创建 3,000 个哈希表几乎不会使用任何内存，与为每个输入记录创建字典相比。由于集群中所有分区的并行和独立处理，此解决方案具有高度的可扩展性和速度。
- en: The `mapPartitions()` transformation semantics for DNA base count solution 3
    are illustrated in [Figure 2-4](#the_mappartitions_transformation).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 用于解决 DNA 碱基计数问题的 `mapPartitions()` 转换语义在 [图 2-4](#the_mappartitions_transformation)
    中进行了说明。
- en: '![The mapPartitions() Transformation](Images/daws_0204.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![mapPartitions() 转换](Images/daws_0204.png)'
- en: Figure 2-4\. The `mapPartitions()` transformation
  id: totrans-234
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-4\. `mapPartitions()` 转换
- en: 'Let’s walk through [Figure 2-4](#the_mappartitions_transformation):'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看 [图 2-4](#the_mappartitions_transformation)：
- en: The source RDD represents all of our input as an `RDD[String]`, since each record
    of a FASTA file is a `String` object.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 源 RDD 表示所有输入作为 `RDD[String]`，因为 FASTA 文件的每条记录都是 `String` 对象。
- en: The entire input is partitioned into *`N`* chunks or partitions (where *`N`*
    can be `100`, `200`, `1000`, …, based on the data size and cluster resources),
    each of which may hold thousands or millions of DNA sequences (each DNA sequence
    is a record of type `String`). Partitioning of the source RDD is similar to the
    Linux `split` command, which splits a file into pieces.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 整个输入被分割成 *`N`* 个块或分区（其中 *`N`* 可以是 `100`，`200`，`1000`，…，根据数据大小和集群资源的情况），每个分区可能包含数千或数百万条
    DNA 序列（每个 DNA 序列都是 `String` 类型的记录）。源 RDD 的分区类似于 Linux 的 `split` 命令，该命令将文件分割成片段。
- en: Each partition is sent to a `mapPartitions()` mapper/worker/executor to be processed
    by your provided `func()`. Your `func()` accepts a partition (as an iterator of
    type `String`) and returns at most six (key, value) pairs, where the key is a
    DNA-letter and the value is the total frequency of that letter for that partition.
    Note that partitions are processed in parallel and independently.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个分区都会被发送到一个 `mapPartitions()` 的映射器/工作者/执行器中，以便由您提供的 `func()` 处理。您的 `func()`
    接受一个分区（作为 `String` 类型的迭代器），并返回最多六对（键，值）对，其中键是 DNA 字母，值是该分区中该字母的总频率。请注意，分区是并行和独立处理的。
- en: Once processing of all partitions is complete, the results are merged into the
    target RDD, which is an `RDD[(String, Integer)]`, where the key is a DNA letter
    and the value is the frequency of that DNA letter.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦所有分区的处理完成，结果将合并到目标 RDD 中，该 RDD 是一个 `RDD[(String, Integer)]`，其中键是 DNA 字母，值是该
    DNA 字母的频率。
- en: The detailed `mapPartitions()` transformation semantics for the DNA base count
    problem are presented in [Figure 2-5](#detailed_mappartitions_transformation).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 详细的 `mapPartitions()` 转换语义用于解决 DNA 碱基计数问题，如 [图 2-5](#detailed_mappartitions_transformation)
    所示。
- en: '![The mapPartitions() Transformation](Images/daws_0205.png)'
  id: totrans-241
  prefs: []
  type: TYPE_IMG
  zh: '![mapPartitions() 转换](Images/daws_0205.png)'
- en: Figure 2-5\. Using `mapPartitions()` to solve the DNA base count problem
  id: totrans-242
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 2-5\. 使用 `mapPartitions()` 解决 DNA 碱基计数问题
- en: As this figure shows, our input (FASTA-format data) has been partitioned into
    *`N`* chunks/partitions, each of which can be handled by a mapper/worker/executor
    independently and in parallel. For example, if our input has a total of 5 billion
    records and `*N* = 50,000`, then each partition will have about 100,000 FASTA
    records (5 billion = 50,000 × 100,000). Therefore, each `func()` will process
    (by means of iteration) about 100,000 FASTA records. Each partition will emit
    at most six (key, value) pairs, where the keys will be in `{"a", "t", "c", "g",
    "n", "z"}` (the four letters, `"n"` as a key for non-DNA letters, and `"z"` as
    a key for the number of processed DNA strings/sequences.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本图所示，我们的输入（FASTA格式数据）已经被分割成*N*个块/分区，每个块可以由一个独立的mapper/worker/executor并行处理。例如，如果我们的输入总共有50亿条记录，*N*
    = 50,000，则每个分区将包含约100,000个FASTA记录（50亿 = 50,000 × 100,000）。因此，每个`func()`将处理（通过迭代）约100,000个FASTA记录。每个分区最多会生成六个（键，值）对，其中键将是`{"a",
    "t", "c", "g", "n", "z"}`（四个字母，“n”作为非DNA字母的键，“z”作为处理过的DNA字符串/序列的数量的键）。
- en: 'Because the `mapPartitions(func)` transformation runs separately on each partition
    (block) of the RDD, `func()` must be of type `iterator`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 因为`mapPartitions(func)`变换在RDD的每个分区（块）上分别运行，所以`func()`必须是`iterator`类型：
- en: '[PRE30]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '[![1](Images/1.png)](#comarker1)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#comarker1)'
- en: Each element of the source RDD is of type `T`.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 源RDD的每个元素的类型为`T`。
- en: '[![2](Images/2.png)](#comarker2)'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#comarker2)'
- en: Parameter `p` is an `iterator<T>`, which represents a single partition.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`p`是一个`iterator<T>`，表示一个单独的分区。
- en: '[![3](Images/3.png)](#comarker3)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#comarker3)'
- en: Each iteration will return an object of type `T`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 每次迭代将返回一个类型为`T`的对象。
- en: '[![4](Images/4.png)](#comarker4)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#comarker4)'
- en: Define a `func()`, which accepts a single partition as an `iterator<T>` (an
    iterator of type `T` that iterates over a single partition of the source `RDD[T]`)
    and returns an object of type of `U`.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 定义一个`func()`，接受一个单独的分区作为`iterator<T>`（一个类型为`T`的迭代器，用于遍历源`RDD[T]`的单个分区），并返回一个类型为`U`的对象。
- en: '[![5](Images/5.png)](#comarker5)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#comarker5)'
- en: Apply the transformation.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 应用变换。
- en: '[![6](Images/6.png)](#comarker6)'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#comarker6)'
- en: The result is an `RDD[U]`, where each partition has been converted (using `func()`)
    into a single object type of `U`.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个`RDD[U]`，其中每个分区已经被转换（使用`func()`）为单个类型为`U`的对象。
- en: Let’s assume that we have a source `RDD[T]`. Therefore, for our example, `T`
    represents a `String` type (a DNA sequence record) and `U` represents a hash table
    (a.k.a. dictionary in Python) as `HashMap[String, Integer]`, where the key is
    a DNA letter (as a `String` object) and the value is the associated frequency
    (as an `Integer`).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个源`RDD[T]`。因此，对于我们的示例，`T`表示`String`类型（DNA序列记录），而`U`表示哈希表（Python中的字典）作为`HashMap[String,
    Integer]`，其中键是DNA字母（作为`String`对象），值是关联的频率（作为`Integer`）。
- en: 'We can define `func()` (as a generic template) in Python as shown here:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在Python中定义`func()`（作为通用模板），如下所示：
- en: '[PRE31]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO10-1)'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO10-1)'
- en: '`iterator` is a pointer to a single partition, which you can use to iterate
    through elements of a partition.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterator`是指向单个分区的指针，可用于遍历分区的元素。'
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO10-2)'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO10-2)'
- en: '`record` is of type `T`.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`record`的类型为`T`。'
- en: '[![3](Images/3.png)](#co_transformations_in_action_CO10-3)'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformations_in_action_CO10-3)'
- en: '`result_for_single_partition` is of type `U`.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: '`result_for_single_partition`的类型为`U`。'
- en: Summarization Design Pattern
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要设计模式
- en: Spark’s `mapPartitions()` transformation can be used to implement the summarization
    design pattern, which is useful when you’re working with big data and you want
    to get a summary view so you can glean insights that are not available from looking
    at a localized set of records alone. This design pattern involves grouping similar
    data together and then performing an operation such as calculating a statistic,
    building an index, or simply counting.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`mapPartitions()`变换可用于实现摘要设计模式，当处理大数据并希望获取汇总视图以获取不仅限于查看局部记录的洞察时，这是非常有用的。此设计模式涉及将相似的数据组合在一起，然后执行操作，如计算统计量、构建索引或简单计数。
- en: 'So when should you use the `mapPartitions()` transformation? It’s particularly
    useful when you want to extract some condensed or minimal amount of information
    from each partition, where each partition is a large set of data. For example,
    if you want to find the minimum and maximum of all numbers in your input, using
    `map()` can be pretty inefficient, since you will be generating tons of intermediate
    (key, value) pairs but the bottom line is that you want to find just two numbers.
    It’s also useful if you want to find the top 10 (or bottom 10) values in your
    input. `mapPartitions()` does this efficiently: you find the top (or bottom) 10
    per partition, then the top (or bottom) 10 for all partitions. This way, you avoid
    emitting too many intermediate (key, value) pairs.'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 当应该使用`mapPartitions()`转换时呢？当你想从每个分区中提取一些简化或最小数量的信息时，这个转换非常有用，每个分区都是一个大数据集。例如，如果你想找出输入中所有数字的最小值和最大值，使用`map()`会相当低效，因为你会生成大量的中间（键，值）对，但实际上你只想找到两个数字。如果你想要找出输入中的前10个（或后10个）值，那么`mapPartitions()`就非常有用了：它可以高效地实现这个目标，首先找到每个分区的前（或后）10个值，然后再找到所有分区的前（或后）10个值。这样一来，你就避免了生成过多的中间（键，值）对。
- en: For counting DNA bases, the `mapPartitions()` transformation is an ideal solution
    that scales out very well even when the number of partitions is in the high thousands.
    Let’s say you partition your input into 100,000 chunks (which is a very high number
    of partitions—typically the number of partitions will not be this high). Aggregating
    the resulting 100,000 dictionaries (hash maps) is a trivial task that can be accomplished
    in seconds, with no danger of OOM errors or scalability problems.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 对于计数DNA碱基来说，`mapPartitions()`转换是一个理想的解决方案，即使分区数目非常高（高达数千个），也能很好地扩展。假设你将输入分成100,000个块（这是一个非常高的分区数目——通常情况下分区数目不会这么高）。聚合这100,000个字典（哈希映射）的结果是一个简单的任务，可以在几秒钟内完成，不会出现OOM错误或可扩展性问题。
- en: 'I will mention one more tip about using `mapPartitions()` before presenting
    a complete DNA base count solution using this powerful transformation. Suppose
    that you will be accessing a database for some of your data transformations, so
    you need a connection to your database. As you know, creating a connection object
    is expensive, and it will take some time (maybe a second or two) to create this
    object. If you create a connection object per source RDD element, then your solution
    will not scale at all: you will quickly run out of connections and resources.
    Whenever you have to perform heavyweight initialization (such as creating a database
    connection object), ideally this should be done once for many RDD elements rather
    than once per RDD element. If this initialization cannot be serialized (so that
    Spark can transmit it across the cluster to the worker nodes), as in the case
    of creating objects from an external library, you should use `mapPartitions()`
    instead of `map()`. The `mapPartitions()` transformation allows the initialization
    to be done once per worker task/partition instead of once per RDD data element.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在展示使用这种强大转换完成的完整DNA碱基计数解决方案之前，我将提及关于使用`mapPartitions()`的另一个技巧。假设你将要访问数据库来进行一些数据转换，因此需要连接到数据库。正如你所知，创建连接对象是昂贵的，可能需要一两秒钟的时间来创建这个对象。如果你为每个源RDD元素创建一个连接对象，那么你的解决方案将无法扩展：你很快就会用完连接和资源。每当需要执行重量级初始化（比如创建数据库连接对象）时，最好是为许多RDD元素而不是每个RDD元素执行一次。如果这种初始化无法序列化（以便Spark可以将其传输到工作节点上），例如从外部库创建对象的情况，那么应该使用`mapPartitions()`而不是`map()`。`mapPartitions()`转换允许一次在工作任务/分区中初始化而不是每个RDD数据元素一次。
- en: 'This concept of initialization per partition/worker is presented by the following
    example:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这种按分区/工作器初始化的概念通过以下示例来展示：
- en: '[PRE32]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[![1](Images/1.png)](#co01)'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co01)'
- en: The `partition` parameter is an `iterator<T>`, which represents a single partition
    of `source_rdd`; this `func()` returns an object of type `U`.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`partition`参数是一个`iterator<T>`，表示`source_rdd`的一个分区；`func()`返回一个类型为`U`的对象。'
- en: '[![2](Images/2.png)](#co02)'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co02)'
- en: Create a single `connection` object to be used by all elements in a given partition.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个单一的`connection`对象，供给给定分区中的所有元素使用。
- en: '[![3](Images/3.png)](#co03)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co03)'
- en: '`data_structures` can be a list or dictionary or whatever you desire.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`data_structures`可以是列表、字典或任何你想要的数据结构。'
- en: '[![4](Images/4.png)](#co04)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co04)'
- en: '`rdd_element` is a single element of type `T`.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd_element`是类型为`T`的单个元素。'
- en: '[![5](Images/5.png)](#co05)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co05)'
- en: Close the `connection` object (to release allocated resources).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 关闭`connection`对象（释放分配的资源）。
- en: '[![6](Images/6.png)](#co06)'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co06)'
- en: Create an object of type `U` from your created `data_structures`.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 从创建的`data_structures`创建类型为`U`的对象。
- en: '[![7](Images/7.png)](#co07)'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co07)'
- en: Return a single object of type `U` per partition.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分区返回一个类型为`U`的单个对象。
- en: Now that you understand the basics of the summarization design pattern (to be
    implemented by Spark’s `mapPartitions()`), let’s get into the specifics of using
    it to solve our DNA base count problem.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了摘要设计模式的基础（由Spark的`mapPartitions()`实现），让我们深入使用它来解决我们的DNA碱基计数问题的具体细节。
- en: The high-level workflow for solution 3 is presented in [Figure 2-6](#dna_base_count_solution_version_3).
    We’ll again use the *sample.fasta* file to test this solution.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案3的高级工作流程在[图2-6](#dna_base_count_solution_version_3)中展示。我们将再次使用*sample.fasta*文件来测试这个解决方案。
- en: '![daws 0206](Images/daws_0206.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0206](Images/daws_0206.png)'
- en: Figure 2-6\. DNA base count solution 3
  id: totrans-291
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图2-6\. DNA碱基计数解决方案3
- en: 'There are a few important points to keep in mind here:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有几个重要点需要记住：
- en: I show only four records (two FASTA sequences) per partition in this figure,
    but in reality, each partition may contain thousands or millions of records. If
    your total input is *`N`* records and you have *`P`* partitions, then each partition
    will have about `(*N*/*P*)` records.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本图中，每个分区仅显示四条记录（两个FASTA序列），但实际上，每个分区可能包含数千或数百万条记录。如果您的总输入为*`N`*条记录，并且有*`P`*个分区，则每个分区将包含约`(*N*/*P*)`条记录。
- en: If you have enough resources in your Spark cluster, then each partition can
    be processed in parallel and independently.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的Spark集群有足够的资源，那么每个分区可以并行和独立地处理。
- en: As a general rule, if you have a lot of data but you only need to extract small
    amount of information from that data, `mapPartitions()` is likely to be a good
    choice and will outperform the `map()` and `flatMap()` transformations.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为一般规则，如果您有大量数据，但只需从该数据中提取少量信息，则`mapPartitions()`很可能是一个很好的选择，并且会优于`map()`和`flatMap()`转换。
- en: With that said, let’s look at the main steps of solution 3.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 说了这么多，让我们来看看解决方案3的主要步骤。
- en: 'Step 1: Create an RDD[String] from the Input'
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第1步：从输入创建一个RDD[String]
- en: 'The `SparkContext.textFile()` function is used to create an RDD for input in
    FASTA text-based format. This step is identical to step 1 of the previous solutions:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkContext.textFile()`函数用于创建以FASTA文本格式输入的RDD。此步骤与先前解决方案的第1步相同：'
- en: '[PRE33]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO11-1)'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO11-1)'
- en: Create records as an `RDD[String]`.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 将记录创建为`RDD[String]`。
- en: 'Step 2: Define a Function to Handle a Partition'
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第2步：定义处理分区的函数
- en: 'Let your RDD be an `RDD[T]` (in our example, `T` is a `String`). Spark splits
    our input data into partitions (where each partition is a set of elements of type
    `T`—in our example, `T` is `String`) and then executes computations on the partitions
    independently and in parallel. This is called the divide and conquer model. With
    the `mapPartitions()` transformation, the source RDD is partitioned into *`N`*
    partitions (the number of partitions is determined by the size and number of resources
    available in the Spark cluster) and each partition is passed to a function (this
    can be a user-defined function). You can control the number of partitions by using
    `coalesce()`:'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 让您的RDD成为`RDD[T]`（在我们的例子中，`T`是`String`）。Spark将我们的输入数据分割成分区（其中每个分区是类型为`T`的元素集合——在我们的例子中，`T`是`String`），然后在分区上独立并并行执行计算。这称为分而治之模型。使用`mapPartitions()`转换，源RDD被分割为*`N`*个分区（分区的数量由Spark集群中可用的资源大小和数量决定），每个分区被传递给一个函数（这可以是用户定义的函数）。您可以使用`coalesce()`来控制分区的数量：
- en: '[PRE34]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'which partitions the source RDD into `numOfPartitions` partitions. For example,
    here we create an RDD and partition it into three partitions:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 将源RDD分区为`numOfPartitions`个分区。例如，在这里我们创建了一个RDD并将其分区为三个分区：
- en: '[PRE35]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO12-1)'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO12-1)'
- en: Create an RDD and set the number of partitions to 3.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个RDD并将分区数设置为3。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO12-2)'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO12-2)'
- en: Check the number of partitions for the RDD.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 检查RDD的分区数。
- en: 'Next, I’ll define a `scan()` function in Python to iterate a given iterator—you
    can use this function to debug small RDDs and check the partitioning:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将在Python中定义一个`scan()`函数来迭代给定的迭代器——您可以使用此函数调试小的RDD并检查分区：
- en: '[PRE36]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO13-1)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO13-1)'
- en: Iterate the elements of a partition.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 迭代分区的元素。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO13-2)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO13-2)'
- en: Apply the `scan()` function to a given partition. From the output, we can see
    that there are three partitions here.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 将`scan()`函数应用于给定分区。从输出中，我们可以看到这里有三个分区。
- en: Warning
  id: totrans-317
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Do not use `scan()` for production environments; this is for teaching purposes
    only.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 不要在生产环境中使用`scan()`；这仅用于教学目的。
- en: 'Now let’s take a look at the results if we define an `adder()` function in
    Python that adds the values in each partition:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如果在Python中定义一个`adder()`函数来对每个分区中的值进行加法操作的结果：
- en: '[PRE37]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO14-1)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO14-1)'
- en: '`yield` is a keyword that is used like `return`, except the function will return
    a generator that can be iterated.'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '`yield`是一个关键字，类似于`return`，但函数将返回一个可以迭代的生成器。'
- en: 'For the DNA base counting problem, to handle (i.e., process all elements in)
    an RDD partition we’ll define a function, `process_FASTA_partition()`, which accepts
    a single partition (represented as an `iterator`). We then iterate on the `iterator`
    to process all the elements in the given partition. This produces a dictionary,
    which we map into a list of `(dna_letter, frequency)` pairs:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DNA碱基计数问题，为了处理（即处理RDD分区中的所有元素），我们将定义一个名为`process_FASTA_partition()`的函数，它接受一个分区（表示为`iterator`）。然后，我们在给定分区上进行迭代以处理所有给定分区中的元素。这将生成一个字典，我们将其映射为`(dna_letter,
    frequency)`对的列表：
- en: '[PRE38]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](Images/1.png)](#co_transformations_in_action_CO15-1)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_transformations_in_action_CO15-1)'
- en: The input parameter `iterator` is a handle/pointer to a single partition.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 输入参数`iterator`是单个分区的句柄/指针。
- en: '[![2](Images/2.png)](#co_transformations_in_action_CO15-2)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_transformations_in_action_CO15-2)'
- en: Create a hash table of `[String, Integer]`.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个`[String, Integer]`的哈希表。
- en: '[![3](Images/3.png)](#co_transformations_in_action_CO15-3)'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_transformations_in_action_CO15-3)'
- en: Handle comments for input data.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 处理输入数据的注释。
- en: '[![4](Images/4.png)](#co_transformations_in_action_CO15-4)'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_transformations_in_action_CO15-4)'
- en: Handle a DNA sequence.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 处理DNA序列。
- en: '[![5](Images/5.png)](#co_transformations_in_action_CO15-5)'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: '[![5](Images/5.png)](#co_transformations_in_action_CO15-5)'
- en: Populate the hash table.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 填充哈希表。
- en: '[![6](Images/6.png)](#co_transformations_in_action_CO15-6)'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: '[![6](Images/6.png)](#co_transformations_in_action_CO15-6)'
- en: Flatten the hash table into a list of `(dna_letter, frequency)` pairs.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 将哈希表展平为`(dna_letter, frequency)`对的列表。
- en: '[![7](Images/7.png)](#co_transformations_in_action_CO15-7)'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '[![7](Images/7.png)](#co_transformations_in_action_CO15-7)'
- en: Return list of `(dna_letter, frequency)` pairs.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 返回`(dna_letter, frequency)`对的列表。
- en: In defining the `process_FASTA_partition()` function, we used a `defaultdict(int)`,
    which works exactly like a normal dictionary (as an associative array) but is
    initialized with a function (the “default factory”) that takes no arguments and
    provides the default value for a nonexistent key. In our case, the `defaultdict`
    is used for counting DNA letters and the default factory is `int` (as in the `Integer`
    data type), which in turn has a default value of zero. For each character in the
    list, the value of the corresponding key (a DNA letter) is incremented by one.
    We do not need to make sure the DNA letter is already a key; if it is not, it
    will use the default value of zero.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义`process_FASTA_partition()`函数时，我们使用了`defaultdict(int)`，它的工作原理与普通字典完全相同（作为关联数组），但初始化时使用了一个函数（“默认工厂”），该函数不带参数并提供不存在键的默认值。在我们的情况下，`defaultdict`用于计数DNA碱基，其默认工厂是`int`（即整数数据类型），默认值为零。对于列表中的每个字符，相应键（DNA碱基）的值将增加一。我们无需确保DNA碱基已经是一个键；如果不是，它将使用默认值零。
- en: 'Step 3: Apply the Custom Function to Each Partition'
  id: totrans-340
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三步：将自定义函数应用于每个分区
- en: 'In this step, we apply the `process_FASTA_partition()` function to each partition.
    I have formatted the output and added some comments to show the output per partition
    (we have two partitions):'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们将`process_FASTA_partition()`函数应用于每个分区。我已经格式化了输出，并添加了一些注释以显示每个分区的输出（我们有两个分区）：
- en: '[PRE39]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Note that for this solution, each partition returns at most six (key, value)
    pairs:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，对于此解决方案，每个分区最多返回六个`(key, value)`对：
- en: '[PRE40]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'For our sample data, the final collection from all partitions will be:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的示例数据，所有分区的最终集合将是：
- en: '[PRE41]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Finally, we aggregate and sum up the output (generated by `mapPartitions()`)
    for all partitions:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们聚合并汇总（由`mapPartitions()`生成的）所有分区的输出：
- en: '[PRE42]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Pros and Cons of Solution 3
  id: totrans-349
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案3的优缺点
- en: 'Let’s examine the pros and cons of solution 3:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来分析一下解决方案3的优缺点：
- en: Pros
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 优点
- en: This is the optimal solution for the DNA base count problem. The provided solution
    works and is both simple and efficient. It improves on solutions 1 and 2 by emitting
    the least number of (key, value) pairs, since we create a dictionary per partition
    (rather than per record) and then flatten it into a list of (key, value) pairs.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这是DNA碱基计数问题的最佳解决方案。提供的解决方案有效且简单高效。它通过每个分区创建字典（而不是每条记录）并将其展平为（键，值）对列表，从而改进了解决方案1和2，减少了发出的（键，值）对数量。
- en: There are no scalability issues since we use `mapPartitions()` for handling
    each partition and `reduceByKey()` for reducing all the (key, value) pairs emitted
    by the partitions.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们使用`mapPartitions()`处理每个分区和`reduceByKey()`来减少所有分区发出的（键，值）对，因此不存在可伸缩性问题。
- en: At most we will create `*N*` dictionaries, where `*N*` is the total number of
    partitions for all the input data (this can be in the hundreds or thousands).
    This will not be a threat to scalability and will not use too much memory.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们最多会创建`*N*`个字典，其中`*N*`是所有输入数据的分区总数（可能达到数百或数千个）。这不会对可伸缩性构成威胁，也不会使用过多内存。
- en: Cons
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 缺点
- en: This solution requires custom code.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此解决方案需要自定义代码。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: 'To recap:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：
- en: There are usually multiple ways to solve big data problems, using a variety
    of actions and transformations. Although they all achieve the same result, their
    performance can differ. When selecting transformations to solve a specific data
    problem, make sure that you test it with “real” big data rather than toy data.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解决大数据问题通常有多种方法，使用各种操作和转换。尽管它们都能达到相同的结果，但它们的性能可能不同。在选择解决特定数据问题的转换时，请确保使用“真实”大数据进行测试，而不是玩具数据。
- en: For large volumes of (key, value) pairs, overall, the `reduceByKey()` transformation
    performs better than `groupByKey()` due to different shuffling algorithms.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大量的（键，值）对，总体上，由于不同的洗牌算法，`reduceByKey()`转换比`groupByKey()`表现更好。
- en: When you have big data and you want to extract and aggregate or derive a small
    amount of information (e.g., finding the minimum and maximum or top 10 values,
    or counting values like in the DNA base count problem), the `mapPartitions()`
    transformation is often a good choice.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当您拥有大数据并希望提取、聚合或派生少量信息（例如，查找最小值和最大值或前10个值，或计数DNA碱基问题中的值）时，`mapPartitions()`转换通常是一个不错的选择。
- en: Emitting fewer (key, value) pairs improves the performance of your data solutions.
    This reduces the time required for the sort and shuffle phase of your Spark application.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少（键，值）对数量可提高数据解决方案的性能。这减少了 Spark 应用程序排序和洗牌阶段所需的时间。
- en: Next, we’ll dig deeper into mapper transformations.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将深入探讨映射器转换。
