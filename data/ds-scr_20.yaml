- en: Chapter 19\. Deep Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第19章 深度学习
- en: A little learning is a dangerous thing; Drink deep, or taste not the Pierian
    spring.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 略知一二是危险的；要么深入探索，要么不要触碰那泊里亚之泉。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Alexander Pope
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 亚历山大·蒲柏
- en: '*Deep learning* originally referred to the application of “deep” neural networks
    (that is, networks with more than one hidden layer), although in practice the
    term now encompasses a wide variety of neural architectures (including the “simple”
    neural networks we developed in [Chapter 18](ch18.html#neural_networks)).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*深度学习*最初是指“深度”神经网络的应用（即具有多个隐藏层的网络），尽管实际上这个术语现在包含了各种各样的神经网络架构（包括我们在[第18章](ch18.html#neural_networks)中开发的“简单”神经网络）。'
- en: In this chapter we’ll build on our previous work and look at a wider variety
    of neural networks. To do so, we’ll introduce a number of abstractions that allow
    us to think about neural networks in a more general way.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将在之前的工作基础上继续，并查看更广泛的神经网络。为此，我们将介绍一些允许我们以更一般方式思考神经网络的抽象概念。
- en: The Tensor
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 张量
- en: Previously, we made a distinction between vectors (one-dimensional arrays) and
    matrices (two-dimensional arrays). When we start working with more complicated
    neural networks, we’ll need to use higher-dimensional arrays as well.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们区分了向量（一维数组）和矩阵（二维数组）。当我们开始处理更复杂的神经网络时，我们还需要使用更高维度的数组。
- en: In many neural network libraries, *n*-dimensional arrays are referred to as
    *tensors*, which is what we’ll call them too. (There are pedantic mathematical
    reasons not to refer to *n*-dimensional arrays as tensors; if you are such a pedant,
    your objection is noted.)
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多神经网络库中，*n*维数组被称为*张量*，这也是我们将它们称为的方式。（有一些学术严谨的数学原因不将*n*维数组称为张量；如果你是这样的学究，我们注意到你的反对。）
- en: If I were writing an entire book about deep learning, I’d implement a full-featured
    `Tensor` class that overloaded Python’s arithmetic operators and could handle
    a variety of other operations. Such an implementation would take an entire chapter
    on its own. Here we’ll cheat and say that a `Tensor` is just a `list`. This is
    true in one direction—all of our vectors and matrices and higher-dimensional analogues
    *are* lists. It is certainly not true in the other direction—most Python `list`s
    are not *n*-dimensional arrays in our sense.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我要写一本关于深度学习的整本书，我会实现一个功能齐全的`Tensor`类，重载Python的算术运算符，并能处理各种其他操作。这样的实现将需要一个完整的章节。在这里，我们将简单处理，并说一个`Tensor`只是一个`list`。在某种意义上是正确的——我们所有的向量、矩阵和更高维度的模拟
    *都* 是列表。但在另一方面则不正确——大多数Python的`list`不是我们所说的*n*维数组。
- en: Note
  id: totrans-10
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: 'Ideally you’d like to do something like:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，你想做这样的事情：
- en: '[PRE0]'
  id: totrans-12
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'However, Python won’t let you define recursive types like that. And even if
    it did that definition is still not right, as it allows for bad “tensors” like:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，Python不允许你定义这样的递归类型。即使它允许，那个定义仍然是错误的，因为它允许像这样的坏“张量”：
- en: '[PRE1]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: whose rows have different sizes, which makes it not an *n*-dimensional array.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 其行具有不同的大小，这使得它不是一个*n*维数组。
- en: 'So, like I said, we’ll just cheat:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，正如我所说的，我们将简单地作弊：
- en: '[PRE2]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And we’ll write a helper function to find a tensor’s *shape*:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，我们将编写一个辅助函数来找到张量的*形状*：
- en: '[PRE3]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Because tensors can have any number of dimensions, we’ll typically need to
    work with them recursively. We’ll do one thing in the one-dimensional case and
    recurse in the higher-dimensional case:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 因为张量可以具有任意数量的维度，我们通常需要递归地处理它们。在一维情况下我们会做一件事，在更高维情况下我们会递归处理：
- en: '[PRE4]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'which we can use to write a recursive `tensor_sum` function:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用这一点编写一个递归的`tensor_sum`函数：
- en: '[PRE5]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'If you’re not used to thinking recursively, you should ponder this until it
    makes sense, because we’ll use the same logic throughout this chapter. However,
    we’ll create a couple of helper functions so that we don’t have to rewrite this
    logic everywhere. The first applies a function elementwise to a single tensor:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不习惯递归思维，你应该思考直到理解为止，因为我们将在本章中始终使用相同的逻辑。但是，我们将创建几个辅助函数，这样我们就不必在每个地方重写这个逻辑。首先，将一个函数逐元素地应用于单个张量：
- en: '[PRE6]'
  id: totrans-25
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'We can use this to write a function that creates a zero tensor with the same
    shape as a given tensor:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用它来编写一个函数，创建一个与给定张量形状相同的零张量：
- en: '[PRE7]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We’ll also need to apply a function to corresponding elements from two tensors
    (which had better be the exact same shape, although we won’t check that):'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要将函数应用于两个张量对应的元素（它们最好具有完全相同的形状，尽管我们不会检查这一点）：
- en: '[PRE8]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Layer Abstraction
  id: totrans-30
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 层的抽象
- en: In the previous chapter we built a simple neural net that allowed us to stack
    two layers of neurons, each of which computed `sigmoid(dot(weights, inputs))`.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个简单的神经网络，允许我们堆叠两层神经元，每层计算 `sigmoid(dot(weights, inputs))`。
- en: Although that’s perhaps an idealized representation of what an actual neuron
    does, in practice we’d like to allow a wider variety of things. Perhaps we’d like
    the neurons to remember something about their previous inputs. Perhaps we’d like
    to use a different activation function than `sigmoid`. And frequently we’d like
    to use more than two layers. (Our `feed_forward` function actually handled any
    number of layers, but our gradient computations did not.)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这可能是对实际神经元功能的理想化表示，但实际上，我们希望允许更多种类的操作。也许我们希望神经元记住它们以前的输入。也许我们想使用不同的激活函数而不是
    `sigmoid`。而且通常情况下，我们希望使用超过两层的网络。（我们的 `feed_forward` 函数实际上可以处理任意数量的层，但我们的梯度计算不能。）
- en: In this chapter we’ll build machinery for implementing such a variety of neural
    networks. Our fundamental abstraction will be the `Layer`, something that knows
    how to apply some function to its inputs and that knows how to backpropagate gradients.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建用于实现各种神经网络的机制。我们的基本抽象将是 `Layer`，它知道如何将某个函数应用到其输入上，并知道如何反向传播梯度。
- en: 'One way of thinking about the neural networks we built in [Chapter 18](ch18.html#neural_networks)
    is as a “linear” layer, followed by a “sigmoid” layer, then another linear layer
    and another sigmoid layer. We didn’t distinguish them in these terms, but doing
    so will allow us to experiment with much more general structures:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第18章](ch18.html#neural_networks)构建的神经网络可以理解为一个“线性”层，后跟一个“sigmoid”层，然后是另一个线性层和另一个sigmoid层。在这些术语中没有区分它们，但这样做将允许我们尝试更通用的结构：
- en: '[PRE9]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The `forward` and `backward` methods will have to be implemented in our concrete
    subclasses. Once we build a neural net, we’ll want to train it using gradient
    descent, which means we’ll want to update each parameter in the network using
    its gradient. Accordingly, we insist that each layer be able to tell us its parameters
    and gradients.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward` 和 `backward` 方法需要在我们的具体子类中实现。一旦我们建立了神经网络，我们将希望使用梯度下降来训练它，这意味着我们希望使用其梯度更新网络中的每个参数。因此，我们要求每一层能够告诉我们它的参数和梯度。'
- en: Some layers (for example, a layer that applies `sigmoid` to each of its inputs)
    have no parameters to update, so we provide a default implementation that handles
    that case.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 一些层（例如，将 `sigmoid` 应用于每个输入的层）没有需要更新的参数，因此我们提供了一个处理这种情况的默认实现。
- en: 'Let’s look at that layer:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看那层：
- en: '[PRE10]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: There are a couple of things to notice here. One is that during the forward
    pass we saved the computed sigmoids so that we could use them later in the backward
    pass. Our layers will typically need to do this sort of thing.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事情需要注意。一是在前向传播期间，我们保存了计算出的 sigmoid 值，以便稍后在反向传播中使用。我们的层通常需要执行这种操作。
- en: Second, you may be wondering where the `sig * (1 - sig) * grad` comes from.
    This is just the chain rule from calculus and corresponds to the `output * (1
    - output) * (output - target)` term in our previous neural networks.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，您可能想知道 `sig * (1 - sig) * grad` 是从哪里来的。这只是微积分中的链式法则，对应于我们先前神经网络中的 `output
    * (1 - output) * (output - target)` 项。
- en: Finally, you can see how we were able to make use of the `tensor_apply` and
    the `tensor_combine` functions. Most of our layers will use these functions similarly.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您可以看到我们如何使用 `tensor_apply` 和 `tensor_combine` 函数。我们的大多数层将类似地使用这些函数。
- en: The Linear Layer
  id: totrans-43
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 线性层
- en: The other piece we’ll need to duplicate the neural networks from [Chapter 18](ch18.html#neural_networks)
    is a “linear” layer that represents the `dot(weights, inputs)` part of the neurons.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要从[第18章](ch18.html#neural_networks)复制神经网络所需的“线性”层，该层代表神经元中的 `dot(weights,
    inputs)` 部分。
- en: This layer will have parameters, which we’d like to initialize with random values.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这个层将有参数，我们希望用随机值初始化它们。
- en: It turns out that the initial parameter values can make a huge difference in
    how quickly (and sometimes *whether*) the network trains. If weights are too big,
    they may produce large outputs in a range where the activation function has near-zero
    gradients. And parts of the network that have zero gradients necessarily can’t
    learn anything via gradient descent.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，初始参数值可能极大地影响网络的训练速度（有时甚至影响网络是否能训练）。如果权重过大，它们可能会在激活函数梯度接近零的范围内产生大的输出。具有零梯度部分的网络必然无法通过梯度下降学习任何内容。
- en: 'Accordingly, we’ll implement three different schemes for randomly generating
    our weight tensors. The first is to choose each value from the random uniform
    distribution on [0, 1]—that is, as a `random.random()`. The second (and default)
    is to choose each value randomly from a standard normal distribution. And the
    third is to use *Xavier initialization*, where each weight is initialized with
    a random draw from a normal distribution with mean 0 and variance 2 / (`num_inputs`
    + `num_outputs`). It turns out this often works nicely for neural network weights.
    We’ll implement these with a `random_uniform` function and a `random_normal` function:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将实现三种不同的方案来随机生成我们的权重张量。第一种是从[0, 1]上的随机均匀分布中选择每个值，即使用`random.random()`。第二种（默认）是从标准正态分布中随机选择每个值。第三种是使用*Xavier初始化*，其中每个权重都从均值为0、方差为2
    / (`num_inputs` + `num_outputs`)的正态分布中随机抽取。事实证明，这通常对神经网络权重效果很好。我们将使用`random_uniform`函数和`random_normal`函数来实现这些：
- en: '[PRE11]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'And then wrap them all in a `random_tensor` function:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将它们全部包装在一个`random_tensor`函数中：
- en: '[PRE12]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Now we can define our linear layer. We need to initialize it with the dimension
    of the inputs (which tells us how many weights each neuron needs), the dimension
    of the outputs (which tells us how many neurons we should have), and the initialization
    scheme we want:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的线性层了。我们需要用输入的维度来初始化它（这告诉我们每个神经元需要多少个权重），输出的维度（这告诉我们应该有多少个神经元），以及我们想要的初始化方案：
- en: '[PRE13]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In case you’re wondering how important the initialization schemes are, some
    of the networks in this chapter I couldn’t get to train at all with different
    initializations than the ones I used.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道初始化方案有多重要，这一章中一些网络如果使用与我使用的不同初始化方法，我根本无法训练它们。
- en: 'The `forward` method is easy to implement. We’ll get one output per neuron,
    which we stick in a vector. And each neuron’s output is just the `dot` of its
    weights with the input, plus its bias:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`方法很容易实现。我们将得到每个神经元的一个输出，将其放入一个向量中。每个神经元的输出只是其权重与输入的`dot`积，加上偏置：'
- en: '[PRE14]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The `backward` method is more involved, but if you know calculus it’s not difficult:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: '`backward`方法更复杂一些，但如果你懂微积分，它并不难：'
- en: '[PRE15]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Note
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: In a “real” tensor library, these (and many other) operations would be represented
    as matrix or tensor multiplications, which those libraries are designed to do
    very quickly. Our library is *very* slow.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个“真正”的张量库中，这些（以及许多其他）操作将被表示为矩阵或张量乘法，这些库被设计成能够非常快速地执行。我们的库*非常*慢。
- en: 'Finally, here we do need to implement `params` and `grads`. We have two parameters
    and two corresponding gradients:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们确实需要实现`params`和`grads`。我们有两个参数和两个相应的梯度：
- en: '[PRE16]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Neural Networks as a Sequence of Layers
  id: totrans-63
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 神经网络作为层序列
- en: 'We’d like to think of neural networks as sequences of layers, so let’s come
    up with a way to combine multiple layers into one. The resulting neural network
    is itself a layer, and it implements the `Layer` methods in the obvious ways:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望将神经网络视为层序列，因此让我们想出一种将多个层组合成一个的方法。得到的神经网络本身就是一个层，它以明显的方式实现了`Layer`方法：
- en: '[PRE17]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'So we could represent the neural network we used for XOR as:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以将用于XOR的神经网络表示为：
- en: '[PRE18]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: But we still need a little more machinery to train it.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们仍然需要一些更多的机制来训练它。
- en: Loss and Optimization
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 损失和优化
- en: 'Previously we wrote out individual loss functions and gradient functions for
    our models. Here we’ll want to experiment with different loss functions, so (as
    usual) we’ll introduce a new `Loss` abstraction that encapsulates both the loss
    computation and the gradient computation:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们为我们的模型编写了单独的损失函数和梯度函数。在这里，我们将想要尝试不同的损失函数，所以（像往常一样）我们将引入一个新的`Loss`抽象，它封装了损失计算和梯度计算：
- en: '[PRE19]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We’ve already worked many times with the loss that’s the sum of the squared
    errors, so we should have an easy time implementing that. The only trick is that
    we’ll need to use `tensor_combine`:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经多次使用过损失函数，即平方误差的和，所以实现它应该很容易。唯一的技巧是我们需要使用`tensor_combine`：
- en: '[PRE20]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: (We’ll look at a different loss function in a bit.)
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: （稍后我们将看一个不同的损失函数。）
- en: 'The last piece to figure out is gradient descent. Throughout the book we’ve
    done all of our gradient descent manually by having a training loop that involves
    something like:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 最后需要弄清楚的是梯度下降。在整本书中，我们通过一个训练循环手动进行所有的梯度下降，类似于以下操作：
- en: '[PRE21]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Here that won’t quite work for us, for a couple reasons. The first is that our
    neural nets will have many parameters, and we’ll need to update all of them. The
    second is that we’d like to be able to use more clever variants of gradient descent,
    and we don’t want to have to rewrite them each time.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们来说不太适用，原因有几个。首先是我们的神经网络将有很多参数，我们需要更新所有这些参数。其次是我们希望能够使用更聪明的梯度下降的变体，而不想每次都重新编写它们。
- en: 'Accordingly, we’ll introduce a (you guessed it) `Optimizer` abstraction, of
    which gradient descent will be a specific instance:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们将引入一个（你猜对了）`Optimizer` 抽象，梯度下降将是一个具体的实例：
- en: '[PRE22]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'After that it’s easy to implement gradient descent, again using `tensor_combine`:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，使用 `tensor_combine` 再次实现梯度下降就很容易了：
- en: '[PRE23]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The only thing that’s maybe surprising is the “slice assignment,” which is a
    reflection of the fact that reassigning a list doesn’t change its original value.
    That is, if you just did `param = tensor_combine(. . .)`, you would be redefining
    the local variable `param`, but you would not be affecting the original parameter
    tensor stored in the layer. If you assign to the slice `[:]`, however, it actually
    changes the values inside the list.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 可能令人惊讶的唯一一件事是“切片赋值”，这反映了重新分配列表不会改变其原始值的事实。也就是说，如果你只是做了 `param = tensor_combine(.
    . .)`，你会重新定义局部变量 `param`，但你不会影响存储在层中的原始参数张量。然而，如果你分配给切片 `[:]`，它实际上会改变列表内的值。
- en: 'Here’s a simple example to demonstrate:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个简单的示例来演示：
- en: '[PRE24]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you are somewhat inexperienced in Python, this behavior may be surprising,
    so meditate on it and try examples yourself until it makes sense.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在Python方面有些经验不足，这种行为可能会让你感到惊讶，所以要沉思一下，并尝试自己的例子，直到它变得清晰为止。
- en: 'To demonstrate the value of this abstraction, let’s implement another optimizer
    that uses *momentum*. The idea is that we don’t want to overreact to each new
    gradient, and so we maintain a running average of the gradients we’ve seen, updating
    it with each new gradient and taking a step in the direction of the average:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这种抽象的价值，让我们实现另一个使用 *动量* 的优化器。其思想是我们不希望对每一个新的梯度过于反应，因此我们保持已看到的梯度的运行平均，每次新的梯度更新它，并朝平均梯度的方向迈出一步：
- en: '[PRE25]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Because we used an `Optimizer` abstraction, we can easily switch between our
    different optimizers.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用了 `Optimizer` 抽象，我们可以轻松地在不同的优化器之间切换。
- en: 'Example: XOR Revisited'
  id: totrans-89
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：重新思考XOR
- en: 'Let’s see how easy it is to use our new framework to train a network that can
    compute XOR. We start by re-creating the training data:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看使用我们的新框架来训练一个能计算XOR的网络有多容易。我们首先重新创建训练数据：
- en: '[PRE26]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'and then we define the network, although now we can leave off the last sigmoid
    layer:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们定义网络，尽管现在我们可以省略最后的sigmoid层：
- en: '[PRE27]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can now write a simple training loop, except that now we can use the abstractions
    of `Optimizer` and `Loss`. This allows us to easily try different ones:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写一个简单的训练循环，除了现在我们可以使用 `Optimizer` 和 `Loss` 的抽象。这使得我们可以轻松尝试不同的优化方法：
- en: '[PRE28]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This should train quickly, and you should see the loss go down. And now we
    can inspect the weights:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会快速训练，并且你应该会看到损失下降。现在我们可以检查权重了：
- en: '[PRE29]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'For my network I find roughly:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我的网络，我发现大致上：
- en: '[PRE30]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: So `hidden1` activates if neither input is 1\. `hidden2` activates if both inputs
    are 1. And `output` activates if neither hidden output is 1—that is, if it’s not
    the case that neither input is 1 and it’s also not the case that both inputs are
    1\. Indeed, this is exactly the logic of XOR.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果 `hidden1` 激活，那么没有一个输入是1。如果 `hidden2` 激活，那么两个输入都是1。如果 `output` 激活，那么既不是
    `hidden` 输出是1，也不是两个输入都是1。确实，这正是XOR的逻辑。
- en: Notice that this network learned different features than the one we trained
    in [Chapter 18](ch18.html#neural_networks), but it still manages to do the same
    thing.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个网络学习了不同于我们在 [第18章](ch18.html#neural_networks) 中训练的网络的特征，但它仍然能够执行相同的操作。
- en: Other Activation Functions
  id: totrans-102
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 其他激活函数
- en: The `sigmoid` function has fallen out of favor for a couple of reasons. One
    reason is that `sigmoid(0)` equals 1/2, which means that a neuron whose inputs
    sum to 0 has a positive output. Another is that its gradient is very close to
    0 for very large and very small inputs, which means that its gradients can get
    “saturated” and its weights can get stuck.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '`sigmoid` 函数因为几个原因已经不再流行。其中一个原因是 `sigmoid(0)` 等于 1/2，这意味着输入总和为0的神经元具有正的输出。另一个原因是对于非常大和非常小的输入，它的梯度非常接近0，这意味着它的梯度可能会“饱和”，它的权重可能会被困住。'
- en: 'One popular replacement is `tanh` (“hyperbolic tangent”), which is a different
    sigmoid-shaped function that ranges from –1 to 1 and outputs 0 if its input is
    0\. The derivative of `tanh(x)` is just `1 - tanh(x) ** 2`, which makes the layer
    easy to write:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的替代方案是`tanh`（“双曲正切”），这是一个不同的S型函数，范围从–1到1，并且如果其输入为0，则输出为0。`tanh(x)`的导数就是`1
    - tanh(x) ** 2`，这样写起来很容易：
- en: '[PRE31]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'In larger networks another popular replacement is `Relu`, which is 0 for negative
    inputs and the identity for positive inputs:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在更大的网络中，另一个流行的替代方案是`Relu`，对于负输入为0，对于正输入为恒等：
- en: '[PRE32]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: There are many others. I encourage you to play around with them in your networks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有许多其他的函数。我鼓励你在你的网络中尝试它们。
- en: 'Example: FizzBuzz Revisited'
  id: totrans-109
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：FizzBuzz再探讨
- en: 'We can now use our “deep learning” framework to reproduce our solution from
    [“Example: Fizz Buzz”](ch18.html#fizzbuzz). Let’s set up the data:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用我们的“深度学习”框架来重新生成我们在[“示例：Fizz Buzz”](ch18.html#fizzbuzz)中的解决方案。让我们设置数据：
- en: '[PRE33]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'and create the network:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 并创建网络：
- en: '[PRE34]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'As we’re training, let’s also track our accuracy on the training set:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们训练时，让我们也跟踪一下训练集上的准确率：
- en: '[PRE35]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '[PRE36]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: After 1,000 training iterations, the model gets 90% accuracy on the test set;
    if you keep training it longer, it should do even better. (I don’t think it’s
    possible to train to 100% accuracy with only 25 hidden units, but it’s definitely
    possible if you go up to 50 hidden units.)
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 经过1000次训练迭代后，模型在测试集上达到了90%的准确率；如果你继续训练，它应该能表现得更好。（我认为只用25个隐藏单元不可能训练到100%的准确率，但如果增加到50个隐藏单元，这是可能的。）
- en: Softmaxes and Cross-Entropy
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Softmax函数和交叉熵
- en: The neural net we used in the previous section ended in a `Sigmoid` layer, which
    means that its output was a vector of numbers between 0 and 1. In particular,
    it could output a vector that was entirely 0s, or it could output a vector that
    was entirely 1s. Yet when we’re doing classification problems, we’d like to output
    a 1 for the correct class and a 0 for all the incorrect classes. Generally our
    predictions will not be so perfect, but we’d at least like to predict an actual
    probability distribution over the classes.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前面章节使用的神经网络以`Sigmoid`层结束，这意味着其输出是介于0和1之间的向量。特别是，它可以输出一个完全是0的向量，或者一个完全是1的向量。然而，在分类问题中，我们希望输出一个1代表正确类别，0代表所有不正确的类别。通常我们的预测不会那么完美，但我们至少希望预测一个实际的类别概率分布。
- en: For example, if we have two classes, and our model outputs `[0, 0]`, it’s hard
    to make much sense of that. It doesn’t think the output belongs in either class?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有两个类别，而我们的模型输出`[0, 0]`，很难理解这意味着什么。它不认为输出属于任何类别？
- en: But if our model outputs `[0.4, 0.6]`, we can interpret it as a prediction that
    there’s a probability of 0.4 that our input belongs to the first class and 0.6
    that our input belongs to the second class.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们的模型输出`[0.4, 0.6]`，我们可以将其解释为预测我们的输入属于第一类的概率为0.4，属于第二类的概率为0.6。
- en: In order to accomplish this, we typically forgo the final `Sigmoid` layer and
    instead use the `softmax` function, which converts a vector of real numbers to
    a vector of probabilities. We compute `exp(x)` for each number in the vector,
    which results in a vector of positive numbers. After that, we just divide each
    of those positive numbers by the sum, which gives us a bunch of positive numbers
    that add up to 1—that is, a vector of probabilities.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现这一点，我们通常放弃最后的`Sigmoid`层，而是使用`softmax`函数，将实数向量转换为概率向量。我们对向量中的每个数计算`exp(x)`，得到一组正数。然后，我们将这些正数除以它们的和，得到一组加起来为1的正数——即概率向量。
- en: 'If we ever end up trying to compute, say, `exp(1000)` we will get a Python
    error, so before taking the `exp` we subtract off the largest value. This turns
    out to result in the same probabilities; it’s just safer to compute in Python:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们试图计算，比如`exp(1000)`，我们会得到一个Python错误，所以在计算`exp`之前，我们要减去最大值。这样做结果是相同的概率；在Python中这样计算更安全：
- en: '[PRE37]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Once our network produces probabilities, we often use a different loss function
    called *cross-entropy* (or sometimes “negative log likelihood”).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的网络产生了概率，我们通常使用另一种称为*交叉熵*（有时称为“负对数似然”）的损失函数。
- en: You may recall that in [“Maximum Likelihood Estimation”](ch14.html#maximum_likelihood_estimation),
    we justified the use of least squares in linear regression by appealing to the
    fact that (under certain assumptions) the least squares coefficients maximized
    the likelihood of the observed data.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能记得，在[“最大似然估计”](ch14.html#maximum_likelihood_estimation)中，我们通过引用最小二乘在线性回归中的使用来证明（在某些假设下）最小二乘系数最大化了观察数据的似然。
- en: 'Here we can do something similar: if our network outputs are probabilities,
    the cross-entropy loss represents the negative log likelihood of the observed
    data, which means that minimizing that loss is the same as maximizing the log
    likelihood (and hence the likelihood) of the training data.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们可以做类似的事情：如果我们的网络输出是概率，交叉熵损失表示观察数据的负对数似然，这意味着最小化该损失等同于最大化（因而最大化）训练数据的似然。
- en: Typically we won’t include the `softmax` function as part of the neural network
    itself. This is because it turns out that if `softmax` is part of your loss function
    but not part of the network itself, the gradients of the loss with respect to
    the network outputs are very easy to compute.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们不会将`softmax`函数作为神经网络本身的一部分。这是因为事实证明，如果`softmax`是你的损失函数的一部分，但不是网络本身的一部分，那么损失关于网络输出的梯度计算非常容易。
- en: '[PRE38]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If I now train the same Fizz Buzz network using `SoftmaxCrossEntropy` loss,
    I find that it typically trains much faster (that is, in many fewer epochs). Presumably
    this is because it is much easier to find weights that `softmax` to a given distribution
    than it is to find weights that `sigmoid` to a given distribution.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在我使用`SoftmaxCrossEntropy`损失训练相同的Fizz Buzz网络，我发现它通常训练速度快得多（即在更少的周期内）。这可能是因为找到使`softmax`到给定分布的权重比找到使`sigmoid`到给定分布的权重更容易。
- en: 'That is, if I need to predict class 0 (a vector with a 1 in the first position
    and 0s in the remaining positions), in the `linear` + `sigmoid` case I need the
    first output to be a large positive number and the remaining outputs to be large
    negative numbers. In the `softmax` case, however, I just need the first output
    to be *larger than* the remaining outputs. Clearly there are a lot more ways for
    the second case to happen, which suggests that it should be easier to find weights
    that make it so:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，如果我需要预测类别0（一个向量，第一个位置为1，其余位置为0），在`linear` + `sigmoid`的情况下，我需要第一个输出为一个较大的正数，其余的输出为较大的负数。然而，在`softmax`的情况下，我只需要第一个输出比其余的输出*大*。显然，第二种情况发生的方式更多，这表明找到使其成为可能的权重应该更容易：
- en: '[PRE39]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Dropout
  id: totrans-133
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Dropout
- en: Like most machine learning models, neural networks are prone to overfitting
    to their training data. We’ve previously seen ways to ameliorate this; for example,
    in [“Regularization”](ch15.html#regularization) we penalized large weights and
    that helped prevent overfitting.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 像大多数机器学习模型一样，神经网络容易过拟合其训练数据。我们之前已经看到了一些缓解这种情况的方法；例如，在[“正则化”](ch15.html#regularization)中，我们对大的权重进行了惩罚，这有助于防止过拟合。
- en: A common way of regularizing neural networks is using *dropout*. At training
    time, we randomly turn off each neuron (that is, replace its output with 0) with
    some fixed probability. This means that the network can’t learn to depend on any
    individual neuron, which seems to help with overfitting.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的神经网络正则化方法之一是使用*dropout*。在训练时，我们随机关闭每个神经元（即将其输出替换为0），关闭的概率固定。这意味着网络不能学会依赖任何单个神经元，这似乎有助于防止过拟合。
- en: 'At evaluation time, we don’t want to dropout any neurons, so a `Dropout` layer
    will need to know whether it’s training or not. In addition, at training time
    a `Dropout` layer only passes on some random fraction of its input. To make its
    output comparable during evaluation, we’ll scale down the outputs (uniformly)
    using that same fraction:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在评估时，我们不希望关闭任何神经元，因此`Dropout`层需要知道它是在训练还是不训练。此外，在训练时，`Dropout`层仅传递其输入的一些随机部分。为了在评估期间使其输出可比较，我们将使用相同的比例缩减输出（均匀地）：
- en: '[PRE40]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: We’ll use this to help prevent our deep learning models from overfitting.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用这个来帮助防止我们的深度学习模型过拟合。
- en: 'Example: MNIST'
  id: totrans-139
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 示例：MNIST
- en: '[MNIST](http://yann.lecun.com/exdb/mnist/) is a dataset of handwritten digits
    that everyone uses to learn deep learning.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '[MNIST](http://yann.lecun.com/exdb/mnist/) 是一个手写数字数据集，每个人都用它来学习深度学习。'
- en: It is available in a somewhat tricky binary format, so we’ll install the `mnist`
    library to work with it. (Yes, this part is technically not “from scratch.”)
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这种数据以一种有些棘手的二进制格式提供，因此我们将安装`mnist`库来处理它。（是的，这部分从技术上讲并非“从头开始”。）
- en: '[PRE41]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'And then we can load the data:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以加载数据：
- en: '[PRE42]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Let’s plot the first 100 training images to see what they look like ([Figure 19-1](#mnist_images)):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制前 100 张训练图像，看看它们的样子（[图 19-1](#mnist_images)）：
- en: '[PRE43]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '![MNIST images](assets/dsf2_1901.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![MNIST 图像](assets/dsf2_1901.png)'
- en: Figure 19-1\. MNIST images
  id: totrans-148
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 19-1\. MNIST 图像
- en: You can see that indeed they look like handwritten digits.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到它们确实看起来像手写数字。
- en: Note
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: My first attempt at showing the images resulted in yellow numbers on black backgrounds.
    I am neither clever nor subtle enough to know that I needed to add `cmap=*Greys*`
    to get black-and-white images; I Googled it and found the solution on Stack Overflow.
    As a data scientist you will become quite adept at this workflow.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 我第一次尝试显示图像时，结果是黄色数字在黑色背景上。我既不聪明也不够细心，不知道我需要添加 `cmap=*Greys*` 才能获得黑白图像；我在 Stack
    Overflow 上搜索找到了解决方案。作为一名数据科学家，你将变得非常擅长这种工作流程。
- en: 'We also need to load the test images:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要加载测试图像：
- en: '[PRE44]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Each image is 28 × 28 pixels, but our linear layers can only deal with one-dimensional
    inputs, so we’ll just flatten them (and also divide by 256 to get them between
    0 and 1). In addition, our neural net will train better if our inputs are 0 on
    average, so we’ll subtract out the average value:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 每个图像是 28 × 28 像素，但我们的线性层只能处理一维输入，因此我们将它们展平（同时除以 256 以使它们在 0 到 1 之间）。此外，如果我们的输入平均值为
    0，我们的神经网络将更好地训练，因此我们将减去平均值：
- en: '[PRE45]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We also want to one-hot-encode the targets, since we have 10 outputs. First
    let’s write a `one_hot_encode` function:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还希望对目标进行独热编码，因为我们有 10 个输出。首先让我们编写一个 `one_hot_encode` 函数：
- en: '[PRE46]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'and then apply it to our data:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其应用到我们的数据中：
- en: '[PRE47]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: One of the strengths of our abstractions is that we can use the same training/evaluation
    loop with a variety of models. So let’s write that first. We’ll pass it our model,
    the data, a loss function, and (if we’re training) an optimizer.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我们抽象化的一个优势是，我们可以用多种模型来使用相同的训练/评估循环。因此，让我们首先编写它。我们会传入我们的模型、数据、损失函数和（如果我们在训练）一个优化器。
- en: 'It will make a pass through our data, track performance, and (if we passed
    in an optimizer) update our parameters:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 它将遍历我们的数据，跟踪性能，并且（如果我们传入了优化器）更新我们的参数：
- en: '[PRE48]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: As a baseline, we can use our deep learning library to train a (multiclass)
    logistic regression model, which is just a single linear layer followed by a softmax.
    This model (in essence) just looks for 10 linear functions such that if the input
    represents, say, a 5, then the 5th linear function produces the largest output.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 作为基准，我们可以使用我们的深度学习库训练（多类别）逻辑回归模型，这只是一个单一线性层，后跟一个 softmax。该模型（实质上）只是寻找 10 个线性函数，如果输入代表，比如说，一个
    5，那么第 5 个线性函数将产生最大的输出。
- en: 'One pass through our 60,000 training examples should be enough to learn the
    model:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 通过我们的 60,000 个训练样本的一次遍历应该足以学习模型：
- en: '[PRE49]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'This gets about 89% accuracy. Let’s see if we can do better with a deep neural
    network. We’ll use two hidden layers, the first with 30 neurons, and the second
    with 10 neurons. And we’ll use our `Tanh` activation:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这个准确率约为 89%。让我们看看是否可以通过深度神经网络做得更好。我们将使用两个隐藏层，第一个有 30 个神经元，第二个有 10 个神经元。我们将使用我们的
    `Tanh` 激活函数：
- en: '[PRE50]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: And we can just use the same training loop!
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以只使用相同的训练循环！
- en: '[PRE51]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: Our deep model gets better than 92% accuracy on the test set, which is a nice
    improvement from the simple logistic model.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的深度模型在测试集上的准确率超过了 92%，这比简单的逻辑回归模型有了显著提升。
- en: The [MNIST website](http://yann.lecun.com/exdb/mnist/) describes a variety of
    models that outperform these. Many of them could be implemented using the machinery
    we’ve developed so far but would take an extremely long time to train in our lists-as-tensors
    framework. Some of the best models involve *convolutional* layers, which are important
    but unfortunately quite out of scope for an introductory book on data science.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: '[MNIST 网站](http://yann.lecun.com/exdb/mnist/) 描述了多种优于这些模型的模型。其中许多模型可以使用我们迄今为止开发的机制实现，但在我们的列表作为张量的框架中训练时间将非常长。一些最佳模型涉及到
    *卷积* 层，这很重要，但不幸的是对于数据科学入门书籍来说有些超出范围。'
- en: Saving and Loading Models
  id: totrans-172
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 保存和加载模型
- en: These models take a long time to train, so it would be nice if we could save
    them so that we don’t have to train them every time. Luckily, we can use the `json`
    module to easily serialize model weights to a file.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型需要很长时间来训练，因此如果我们能保存它们以免每次都重新训练就太好了。幸运的是，我们可以使用 `json` 模块轻松地将模型权重序列化到文件中。
- en: 'For saving, we can use `Layer.params` to collect the weights, stick them in
    a list, and use `json.dump` to save that list to a file:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 对于保存，我们可以使用 `Layer.params` 收集权重，将它们放入列表中，并使用 `json.dump` 将该列表保存到文件中：
- en: '[PRE52]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Loading the weights back is only a little more work. We just use `json.load`
    to get the list of weights back from the file and slice assignment to set the
    weights of our model.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 将权重加载回来只需要多做一点工作。我们只需使用`json.load`从文件中获取权重列表，然后使用切片赋值将权重设置到我们的模型中。
- en: (In particular, this means that we have to instantiate the model ourselves and
    *then* load the weights. An alternative approach would be to also save some representation
    of the model architecture and use that to instantiate the model. That’s not a
    terrible idea, but it would require a lot more code and changes to all our `Layer`s,
    so we’ll stick with the simpler way.)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: (具体来说，这意味着我们必须自己实例化模型，*然后*加载权重。另一种方法是保存模型架构的某种表示，并使用它来实例化模型。这并不是一个糟糕的想法，但这将需要大量的代码和对所有我们的`Layer`进行更改，所以我们将坚持使用更简单的方法。)
- en: Before we load the weights, we’d like to check that they have the same shapes
    as the model params we’re loading them into. (This is a safeguard against, for
    example, trying to load the weights for a saved deep network into a shallow network,
    or similar issues.)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载权重之前，我们希望检查它们与我们加载到其中的模型参数具有相同的形状。（这是为了防止例如将保存的深度网络的权重加载到浅网络中，或类似的问题。）
- en: '[PRE53]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note
  id: totrans-180
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: JSON stores your data as text, which makes it an extremely inefficient representation.
    In real applications you’d probably use the `pickle` serialization library, which
    serializes things to a more efficient binary format. Here I decided to keep it
    simple and human-readable.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: JSON将数据存储为文本，这使其成为极其低效的表示形式。在实际应用中，您可能会使用`pickle`序列化库，它将事物序列化为更有效的二进制格式。在这里，我决定保持简单和人类可读性。
- en: You can download the weights for the various networks we train from [the book’s
    GitHub repository](https://github.com/joelgrus/data-science-from-scratch).
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从[该书的GitHub存储库](https://github.com/joelgrus/data-science-from-scratch)下载我们训练的各种网络的权重。
- en: For Further Exploration
  id: totrans-183
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 深入探索
- en: Deep learning is really hot right now, and in this chapter we barely scratched
    its surface. There are many good books and blog posts (and many, many bad blog
    posts) about almost any aspect of deep learning you’d like to know about.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 现在深度学习非常火热，在本章中我们只是简单介绍了一下。关于几乎任何您想了解的深度学习方面，都有许多好书和博客文章（以及很多很多糟糕的博客文章）。
- en: The canonical textbook [*Deep Learning*](https://www.deeplearningbook.org/),
    by Ian Goodfellow, Yoshua Bengio, and Aaron Courville (MIT Press), is freely available
    online. It is very good, but it involves quite a bit of mathematics.
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[*深度学习*](https://www.deeplearningbook.org/) 这本经典教材，作者是Ian Goodfellow、Yoshua
    Bengio和Aaron Courville（MIT Press），可以在线免费获取。这本书非常好，但涉及到相当多的数学。'
- en: Francois Chollet’s [*Deep Learning with Python*](https://www.manning.com/books/deep-learning-with-python)
    (Manning) is a great introduction to the Keras library, after which our deep learning
    library is sort of patterned.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Francois Chollet的[*Python深度学习*](https://www.manning.com/books/deep-learning-with-python)（Manning）是了解Keras库的绝佳入门书籍，我们的深度学习库就是基于这种模式设计的。
- en: I myself mostly use [PyTorch](https://pytorch.org/) for deep learning. Its website
    has lots of documentation and tutorials.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我自己大多使用[PyTorch](https://pytorch.org/)进行深度学习。它的网站有大量的文档和教程。
