- en: Appendix B. The backpropagation algorithm
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 附录 B. 反向传播算法
- en: '[Chapter 5](kindle_split_017.xhtml#ch05) introduced sequential neural networks
    and feed-forward networks in particular. We briefly talked about the *backpropagation
    algorithm*, which is used to train neural networks. This appendix explains in
    a bit more detail how to arrive at the gradients and parameter updates that we
    simply stated and used in [chapter 5](kindle_split_017.xhtml#ch05).'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '[第五章](kindle_split_017.xhtml#ch05)介绍了顺序神经网络和特别的前馈网络。我们简要地谈到了**反向传播算法**，它是用来训练神经网络的。这个附录更详细地解释了如何得到我们简单陈述和使用的梯度以及参数更新。'
- en: We’ll first derive the backpropagation algorithm for feed-forward neural networks
    and then discuss how to extend the algorithm to more-general sequential and nonsequential
    networks. Before going deeper into the math, let’s define our setup and introduce
    notation that will help along the way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先推导前馈神经网络的反向传播算法，然后讨论如何将算法扩展到更一般的顺序和非顺序网络。在深入数学之前，让我们定义我们的设置并介绍沿途将帮助的符号。
- en: A bit of notation
  id: totrans-3
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一些符号
- en: 'In this section, you’ll work with a feed-forward neural network with *l* layers.
    Each of the *l* layers has a sigmoid activation function. Weights of the *i*th
    layer are referred to as *W^i*, and bias terms by *b^i*. You use *x* to denote
    a mini-batch of size *k* of input data to the network, and *y* to indicate the
    output of it. It’s safe to think of both *x* and *y* as vectors here, but all
    operations carry over to mini-batches. Moreover, we introduce the following notation:'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将使用一个具有 *l* 层的前馈神经网络。每一层都有一个 Sigmoid 激活函数。第 *i* 层的权重被称为 *W^i*，偏差项为 *b^i*。你用
    *x* 表示网络输入数据的批大小为 *k* 的小批量，用 *y* 表示输出。在这里，安全地将 *x* 和 *y* 视为向量，但所有操作都适用于小批量。此外，我们引入以下符号：
- en: We indicate the output of the *i*th layer with activation *y^(i+1)*; that is,
    *y^(i+1)* = s(*W^iy^i* + *b^i*). Note that *y^(i+1)* is also the *input* to layer
    *i* +1.
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用 *y^(i+1)* 表示带有激活的第 *i* 层的输出；即，*y^(i+1)* = s(*W^iy^i* + *b^i*)。请注意，*y^(i+1)*
    也是第 *i* +1 层的**输入**。
- en: We indicate the output of the *i*th dense layer without activation as *z^i*;
    that is, *z^i* = *W^i* · *y^i* + *b^i*.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们用 *z^i* 表示没有激活的第 *i* 层的输出；即，*z^i* = *W^i* · *y^i* + *b^i*。
- en: Introducing this convenient way of writing intermediate output, you can now
    write *z^i* = *W^i* · *y^i* + *b^i* and *y^(i+1)* = s(*z^i*). Note that with this
    notation, you could also write the output as *y* = *y^l* and the input as *x*
    = *y⁰*, but we won’t use this notation in what follows.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍这种方便的中间输出写法后，你现在可以写出 *z^i* = *W^i* · *y^i* + *b^i* 和 *y^(i+1)* = s(*z^i*)。请注意，使用这个符号，你也可以将输出写成
    *y* = *y^l*，输入写成 *x* = *y⁰*，但我们在以下内容中不会使用这个符号。
- en: As a last piece of notation, we’ll sometimes write *f^i*(*y^i*) for s(*W^iy^i*
    + *b^i*).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作为最后一个符号，我们有时会写 *f^i*(*y^i*) 表示 s(*W^iy^i* + *b^i*)。
- en: The backpropagation algorithm for feed-forward networks
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 前馈网络的反向传播算法
- en: 'Following the preceding conventions, the forward pass for the *i*th layer of
    your neural network can now be written as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面的约定，现在可以写出你的神经网络第 *i* 层的前向传递如下：
- en: '| *y*^(*i*+1) = σ(*W^iy^i* + *b^i*) = *f^i* o *y^i* |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| *y*^(*i*+1) = σ(*W^iy^i* + *b^i*) = *f^i* · *y^i* |'
- en: 'You can use this definition recursively for each layer to write your predictions
    like this:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用这个定义递归地对每一层进行预测，如下所示：
- en: '| *y* = *f^n* o ··· o *f*¹(*x*) |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| *y* = *f^n* ··· *f*¹(*x*) |'
- en: 'Because you compute your loss function *Loss* from predictions *y* and labels
    *ŷ*, you can split the loss function in a similar fashion:'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 因为你是从预测 *y* 和标签 *ŷ* 计算损失函数 *Loss*，所以你可以以类似的方式拆分损失函数：
- en: '| Loss(*y*,*ŷ*) = Loss o *f^n* o ··· o *f*¹(*x*) |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| 损失(*y*,*ŷ*) = 损失函数的 *f^n* ··· *f*¹(*x*) |'
- en: 'Computing and using the derivative of the loss function as shown here is done
    by a smart application of the *chain rule* for functions, a fundamental result
    from multivariable calculus. Directly applying the chain rule to the preceding
    formula yields this:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 计算和使用如上所示的损失函数的导数是通过智能应用函数的**链式法则**，这是多元微积分的一个基本结果。直接将链式法则应用于前面的公式得到以下结果：
- en: '![](Images/p0319_01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0319_01.jpg)'
- en: 'Now, you define the *delta* of the *i*th layer as follows:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你定义第 *i* 层的 *delta* 如下：
- en: '![](Images/p0319_02.jpg)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0319_02.jpg)'
- en: 'Then you can express deltas in a similar fashion to the previous forward pass,
    which you call the *backward pass*—namely, by the following relationship:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你可以以类似的方式表达 delta，就像之前的正向传递，你称之为**反向传递**——即，通过以下关系：
- en: '![](Images/p0319_03.jpg)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0319_03.jpg)'
- en: 'Note that for deltas, the indices go down, as you pass backward through the
    computation. Formally, computing the backward pass is structurally equivalent
    to the simple forward pass. You’ll now proceed to explicitly computing the actual
    derivatives involved. Derivatives of both sigmoid and affine linear functions
    with respect to their input are quickly derived:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于delta值，索引是递减的，因为你在反向通过计算时。形式上，计算反向传播在结构上等同于简单的正向传播。你现在将明确计算涉及的导数。sigmoid和affine线性函数相对于其输入的导数可以快速推导出来：
- en: '![](Images/p0319_04.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0319_04.jpg)'
- en: 'Using these last two equations, you can now write down how to propagate back
    the error term D*^(i+1)* of the (*i* + 1)th layer to the *i*th layer:'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这两个最后方程，你现在可以写下如何将第(*i* + 1)层的误差项D*^(i+1)*反向传播到第*i*层：
- en: '| Δ^i = (*W^i*)^⊤ · (Δ^(*i*+1) ⊙ σ′ (*z^i))* |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| Δ^i = (*W^i*)^⊤ · (Δ^(*i*+1) ⊙ σ′ (*z^i))* |'
- en: 'In this formula, the superscript *T* denotes matrix transposition. The ⊙, or
    *Hadamard product*, indicates element-wise multiplication of the two vectors.
    The preceding computation splits into two parts, one for the dense layer and one
    for the activation:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，上标 *T* 表示矩阵转置。⊙，或*Hadamard积*，表示两个向量的逐元素乘法。前面的计算分为两部分，一部分是密集层，另一部分是激活：
- en: '| Δ^σ = Δ^(*i*+1) ⊙ σ′ (*z^i*) Δ^(*i*) = (*W^i*)^⊤ · Δ^σ |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| Δ^σ = Δ^(*i*+1) ⊙ σ′ (*z^i*) Δ^(*i*) = (*W^i*)^⊤ · Δ^σ |'
- en: 'The last step is to compute the gradients of your parameters *W^i* and *b^i*
    for every layer. Now that you have D*^i* readily computed, you can immediately
    read off parameter gradients from there:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是计算每一层的参数 *W^i* 和 *b^i* 的梯度。现在你已经准备好了D*^i*，你可以立即从那里读取参数梯度：
- en: '![](Images/p0320_01.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0320_01.jpg)'
- en: With these error terms, you can update your neural network parameters as you
    wish, meaning with any optimizer or update rule you like.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些误差项，你可以根据需要更新你的神经网络参数，这意味着使用任何你喜欢的优化器或更新规则。
- en: Backpropagation for sequential neural networks
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 顺序神经网络的反向传播
- en: 'In general, sequential neural networks can have more-interesting layers than
    what we’ve discussed so far. For instance, you could be concerned with convolutional
    layers, as described in [chapter 6](kindle_split_018.xhtml#ch06), or other activation
    functions, such as the softmax activation discussed in [chapter 6](kindle_split_018.xhtml#ch06)
    as well. Regardless of the actual layers in a sequential network, backpropagation
    follows the same general outline. If *g^i* denotes the forward pass without activation,
    and Act*^i* denotes the respective activation function, propagating Δ*^(i+1)*
    to the *i*th layer requires you to compute the following transition:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，顺序神经网络可以比我们之前讨论的更有趣的层。例如，你可能关心卷积层，如第6章[第6章](kindle_split_018.xhtml#ch06)中描述的，或者其他激活函数，如第6章[第6章](kindle_split_018.xhtml#ch06)中讨论的softmax激活函数。无论顺序网络中的实际层是什么，反向传播都遵循相同的一般概述。如果
    *g^i* 表示没有激活的前向传播，而 *Act^i* 表示相应的激活函数，将Δ*^(i+1)*传播到第*i*层需要你计算以下转换：
- en: '![](Images/p0320_02.jpg)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/p0320_02.jpg)'
- en: You need to compute the derivative of the activation function evaluated at the
    intermediate output *z^i* and the derivative of the layer function *g^i* with
    respect to the input of the *i*th layer. Knowing all the deltas, you can usually
    quickly deduce gradients for all parameters involved in the layer, just as you
    did for weights and bias terms in the feed-forward layer. Seen this way, each
    layer knows how to pass data forward and propagate an error backward, without
    explicitly knowing anything about the structure of the surrounding layers.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要计算在中间输出*z^i*处评估的激活函数的导数以及第*i*层输入的层函数*g^i*的导数。知道了所有的delta值，你通常可以快速推导出层中所有参数的梯度，就像你在前馈层中为权重和偏置项所做的那样。从这个角度来看，每一层都知道如何正向传递数据并反向传播错误，而不需要明确知道周围层的结构。
- en: Backpropagation for neural networks in general
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 通用神经网络的反向传播
- en: In this book, we’re concerned solely with sequential neural networks, but it’s
    still interesting to discuss what happens when you move away from the sequentiality
    constraint. In a nonsequential network, a layer has multiple outputs, multiple
    inputs, or both.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这本书中，我们只关注顺序神经网络，但讨论当你离开顺序约束时会发生什么仍然很有趣。在非顺序网络中，一个层可以有多个输出、多个输入，或者两者都有。
- en: Let’s say a layer has *m* outputs. A prototypical example might be to split
    up a vector into *m* parts. Locally for this layer, the forward pass can be split
    into *k separate* functions. On the backward pass, the derivative of each of these
    functions can be computed separately as well, and each derivative contributes
    equally to the delta that’s being passed on to the previous layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个层有 *m* 个输出。一个典型的例子可能是将一个向量分成 *m* 个部分。对于这个层，前向传递可以分成 *k 个独立的* 函数。在反向传递中，这些函数的导数也可以分别计算，并且每个导数都对传递给前一层的
    delta 贡献相同。
- en: In the situation that we have to deal with, *n* inputs and one output, the situation
    is somewhat reversed. The forward pass is computed from *n* input components by
    means of a single function that outputs a single value. On the backward pass,
    you receive one delta from the next layer and have to compute *n* output deltas
    to pass on to each one of the incoming *n* layers. Those derivatives can be computed
    independently of each other, evaluated at each of the respective inputs.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们必须处理的情况下，*n* 个输入和一个输出，情况有些相反。前向传递是通过一个输出单个值的单一函数，从 *n* 个输入组件计算得出的。在反向传递中，你从下一层接收一个
    delta，并必须计算 *n* 个输出 delta 以传递给每个进入的 *n* 层。这些导数可以独立计算，并在各自的输入上进行评估。
- en: The general case of *n* inputs and *m* outputs works by combining the two previous
    steps. Each neural network, no matter how complicated the setup or how many layers
    in total, locally looks like this.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 *n* 个输入和 *m* 个输出的通用情况，是通过结合前两个步骤来实现的。每个神经网络，无论设置多么复杂或总共有多少层，在局部看起来都像这样。
- en: Computational challenges with backpropagation
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 反向传播的计算挑战
- en: You could argue that backpropagation is just a simple application of the chain
    rule to a specific class of machine-learning algorithms. Although on a theoretical
    level it may be seen like this, in practice there’s a lot to consider when implementing
    backpropagation.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会争辩说，反向传播只是链式法则对特定类别机器学习算法的简单应用。尽管在理论上可能看起来是这样，但在实践中实现反向传播时还有很多需要考虑的因素。
- en: Most notably, to compute deltas and gradient updates for any layer, you have
    to have the respective inputs of the forward pass ready for evaluation. If you
    simply discard results from the forward pass, you have to recompute them on the
    backward pass. Thus, you’d do well by caching those values in an efficient way.
    In your implementation from scratch in [chapter 5](kindle_split_017.xhtml#ch05),
    each layer persisted its own state, for input and output data, as well as for
    input and output deltas. Building networks that rely on processing massive amounts
    of data, you should make sure to have an implementation in place that’s both computationally
    efficient and has a low memory footprint.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 最值得注意的是，为了计算任何层的 delta 和梯度更新，你必须准备好前向传递的相应输入以供评估。如果你简单地丢弃前向传递的结果，你必须在反向传递中重新计算它们。因此，通过高效地缓存这些值会是一个不错的选择。在你的[第5章](kindle_split_017.xhtml#ch05)从头开始实现中，每一层都持续保存自己的状态，包括输入和输出数据，以及输入和输出
    delta。构建依赖于处理大量数据的网络时，你应该确保有一个既计算效率高又内存占用低的实现。
- en: 'Another related, interesting consideration is that of reusing intermediate
    values. For instance, we’ve argued that in the simple case of a feed-forward network,
    we can either see affine linear transformation and sigmoid activation as a unit
    or split them into two layers. The output of the affine linear transformation
    is needed to compute the backward pass of the activation function, so you should
    keep that intermediate information from the forward pass. On the other hand, because
    the sigmoid function doesn’t have parameters, you compute the backward pass in
    one go:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个相关的有趣考虑是重用中间值。例如，我们曾争论，在简单的前馈网络简单情况下，我们可以将仿射线性变换和 sigmoid 激活视为一个单元，或者将它们分成两层。仿射线性变换的输出需要用于计算激活函数的反向传递，因此你应该保留前向传递中的这些中间信息。另一方面，因为
    sigmoid 函数没有参数，你一次就可以计算反向传递：
- en: '| Δ^i = (*W^i*)^⊤ (Δ^(*i*+1) ⊙ σ′ (*z^i))* |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| Δ^i = (*W^i*)^⊤ (Δ^(i+1) ⊙ σ′ (z^i))* |'
- en: This might computationally be more efficient than doing it in two steps. Automatically
    detecting which operations can be carried out together can bring a lot of speed
    gains. In more-complicated situations (such as that of recurrent neural networks,
    in which a layer will essentially compute a *loop* with inputs from the last step),
    managing intermediate state becomes even more important.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这在计算上可能比分两步来做更有效率。自动检测哪些操作可以一起执行可以带来很多速度上的提升。在更复杂的情况下（例如循环神经网络的情况，其中一层将基本上使用来自最后一步的输入进行*循环*计算），管理中间状态变得尤为重要。
