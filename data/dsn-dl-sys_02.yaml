- en: 2 Dataset management service
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 数据集管理服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding dataset management
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解数据集管理
- en: Using design principles to build a dataset management service
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用设计原则构建数据集管理服务
- en: Building a sample dataset management service
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建示例数据集管理服务
- en: Using open source approaches to dataset management
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用开源方法进行数据集管理
- en: After our general discussion of deep learning systems, we are ready for the
    rest of the chapters, which focus on specific components in those systems. We
    present dataset management first not only because deep learning projects are data-driven
    but also because we want to remind you how important it is to think about data
    management before building other services.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对深度学习系统的总体讨论之后，我们准备进入其他章节，这些章节将专注于这些系统中的特定组件。我们首先介绍数据集管理，不仅因为深度学习项目是数据驱动的，而且因为我们想提醒您在构建其他服务之前考虑数据管理的重要性。
- en: Dataset management (DM) often gets overlooked in the deep learning model development
    process, whereas data processing and model training and serving attract the most
    attention. A common thought in data engineering is that good data processing pipelines,
    such as ETL (extract, transform, and load) pipelines, are all we need. But if
    you avoid managing your datasets as your project proceeds, your data collection
    and dataset consumption logic become more and more complicated, model performance
    improvement becomes difficult, and eventually, the entire project slows down.
    A good DM system can expedite model development by decoupling training data collection
    and consumption; it also enables model reproducibility by versioning the training
    data.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型开发过程中，数据集管理（DM）往往被忽视，而数据处理、模型训练和部署则吸引了最多的关注。在数据工程中，一个常见的想法是，良好的数据处理流程，如ETL（提取、转换和加载）流程，就是我们所需要的全部。但是，如果您在项目进行过程中避免管理您的数据集，您的数据收集和数据集消耗逻辑会变得越来越复杂，模型性能提升变得困难，最终整个项目会减慢。一个好的DM系统可以通过解耦训练数据收集和消耗来加速模型开发；它还可以通过版本控制训练数据来提高模型的可重复性。
- en: We guarantee that you will appreciate your wise decision to build or at least
    set up a dataset management component in addition to your existing data processing
    pipelines. And build it before working on the training and serving components.
    Your deep learning project development will go faster and can produce better results
    and simpler models in the long run. Because the DM component shields the upstream
    data complexity from your model training code, your model algorithm development
    and data development can run parallel.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们保证您会欣赏您明智的决定，即在现有的数据处理管道之外构建或至少设置一个数据集管理组件。并且在开始处理训练和部署组件之前构建它。您的深度学习项目开发将更快，并且从长远来看可以产生更好的结果和更简单的模型。因为DM组件可以屏蔽上游数据复杂性对您的模型训练代码的影响，您的模型算法开发和数据开发可以并行运行。
- en: This chapter is about building dataset management functionality for your deep
    learning project. Because of the variety of deep learning algorithms, data pipelines,
    and data sources, dataset management is an often-discussed topic in the deep learning
    industry. There is no unified approach, and it seems there will never be one.
    To be beneficial to you in practice, therefore, we will focus on teaching the
    design principles instead of advocating one single approach. The sample dataset
    management service we build in this chapter demonstrates one possible way to implement
    these principles.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章主要介绍为您的深度学习项目构建数据集管理功能。由于深度学习算法、数据处理流程和数据源种类繁多，数据集管理在深度学习行业中是一个经常被讨论的话题。目前还没有统一的方法，而且似乎永远不会有一个统一的方法。因此，为了在实践中对您有益，我们将专注于教授设计原则，而不是提倡单一的方法。本章中我们构建的示例数据集管理服务展示了实现这些原则的一种可能方式。
- en: In section 2.1, you will learn why dataset management is needed, the challenges
    it should address, and the crucial role it plays in a deep learning system. We
    will also introduce its key design principles to prepare you for the concrete
    examples in the next section.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.1节中，您将了解为什么需要数据集管理，它应该解决哪些挑战，以及它在深度学习系统中的关键作用。我们还将介绍其关键设计原则，以便您为下一节的具体示例做好准备。
- en: In section 2.2, we will demonstrate a dataset management service based on the
    concepts and design principles introduced in section 2.1\. First, we will set
    up the service on your local machine and experiment with it. Second, we will discuss
    the internal dataset storage and data schema, user scenarios, data ingestion API,
    and dataset fetching API, as well as provide an overview of design and user scenarios.
    During the tour, we will also discuss the pros and cons of some important decisions
    we made in the service design.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.2节中，我们将演示一个基于2.1节中介绍的概念和设计原则的数据集管理服务。首先，我们将在您的本地机器上设置此服务并进行实验。其次，我们将讨论内部数据集存储和数据模式、用户场景、数据摄入API和数据集获取API，并提供设计和用户场景的概述。在参观过程中，我们还将讨论我们在服务设计中做出的某些重要决策的优缺点。
- en: In section 2.3, we will look at two open source approaches. If you don’t want
    a DIY dataset management service, you can use the components that are already
    built, available, and adaptable. For instance, you can use Delta Lake with Petastorm
    for dataset management if your existing data pipeline is built on top of Apache
    Spark. Or you can adopt Pachyderm if your data comes directly from a cloud object
    storage such as AWS Simple Storage Service (S3) or Azure Blob. We use image dataset
    preparation as an example to show how these two approaches can work with unstructured
    data in practice. By the end of this chapter, you will have a deep understanding
    of the intrinsic characteristics of dataset management and its design principles,
    so you can either build a dataset management service on your own or improve an
    existing system in your work.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在2.3节中，我们将探讨两种开源方法。如果您不想自己动手构建数据集管理服务，您可以使用已经构建、可用且可适应的组件。例如，如果您现有的数据管道建立在Apache
    Spark之上，您可以使用Delta Lake与Petastorm进行数据集管理。或者，如果您的数据直接来自云对象存储，如AWS Simple Storage
    Service (S3)或Azure Blob，您可以选择Pachyderm。我们以图像数据集准备为例，展示这两种方法在实际中如何处理非结构化数据。到本章结束时，您将对数据集管理的内在特性和设计原则有深入的理解，因此您可以选择自己构建数据集管理服务，或者改进您工作中现有的系统。
- en: 2.1 Understanding dataset management service
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 理解数据集管理服务
- en: A dataset management component or service is a specialized data store for organizing
    data in favor of model training and model performance troubleshooting. It processes
    raw data fed from upstream data sources and returns training data in a well-defined
    structure—a dataset—for use in model training. Figure 2.1 shows the core value
    a dataset management service delivers. In the figure, we see that a dataset management
    component converts the raw data into a consistent data format that favors model
    training, so downstream model training applications can just focus on algorithm
    development.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集管理组件或服务是一种专门的数据存储，旨在组织数据以利于模型训练和模型性能故障排除。它处理来自上游数据源的原始数据，并以定义良好的结构——数据集——的形式返回训练数据，用于模型训练。图2.1显示了数据集管理服务提供的核心价值。在图中，我们看到数据集管理组件将原始数据转换为有利于模型训练的一致数据格式，因此下游模型训练应用程序只需关注算法开发。
- en: '![](../Images/02-01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-01.png)'
- en: Figure 2.1 A dataset management service is a specialized data store; it ingests
    data into its internal storage with its own raw data format. During training,
    it converts the raw data into training data in a consistent data format that favors
    model training.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 数据集管理服务是一种专门的数据存储；它以自己的原始数据格式将数据摄入其内部存储。在训练过程中，它将原始数据转换为有利于模型训练的一致数据格式。
- en: 2.1.1 Why deep learning systems need dataset management
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 为什么深度学习系统需要数据集管理
- en: Let’s take a moment to explain why DM is a crucial part of any deep learning
    system before we start looking at the sample dataset management service. This
    section is important because, from our experience, it is impossible to design
    a system that solves a real problem unless you fully understand the *why*.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始查看示例数据集管理服务之前，让我们花一点时间解释为什么DM是任何深度学习系统的一个关键部分。这一部分很重要，因为根据我们的经验，除非您完全理解“为什么”，否则您无法设计出解决实际问题的系统。
- en: There are two answers to the why question. First, DM can help to expedite model
    development by decoupling the *collection* of training data from the *consumption*
    of that data. Second, a well-designed DM service supports model reproducibility
    by having version tracking on training datasets. Let’s look at both of these points
    in detail.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于“为什么”这个问题有两个答案。首先，DM可以通过将训练数据的*收集*与数据的*消费*解耦来加速模型开发。其次，一个设计良好的DM服务通过在训练数据集上实现版本跟踪来支持模型的可重复性。让我们详细探讨这两个观点。
- en: Decoupling the training data collection and consumption
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 解耦训练数据收集和消费
- en: 'If you work on a deep learning project completely by yourself, the project
    development workflow is an iterative loop of the following steps: data collection,
    dataset preprocess, training, and evaluation (see figure 2.2). Although you can
    break the downstream dataset preprocess code or training code if you change the
    data format in the data collection component, it’s not a big problem. Because
    you are the single code owner, you make free changes; no other people are affected.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你完全独立地从事深度学习项目，项目开发工作流程是以下步骤的迭代循环：数据收集、数据集预处理、训练和评估（见图2.2）。尽管你可以在数据收集组件更改数据格式时中断下游数据预处理代码或训练代码，但这不是一个大问题。因为你是唯一的代码所有者，你可以自由更改；没有其他人受到影响。
- en: '![](../Images/02-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-02.png)'
- en: Figure 2.2 The workflow for a single-person deep learning project development
    is an iterative loop of linear steps.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 单人深度学习项目开发的工作流程是一个线性的迭代循环步骤。
- en: When we are building a serious deep learning platform catering to tens of different
    deep learning projects and opening it to multiple people and teams, the simple
    data flow chart will dilate quickly to a bewildering 3D diagram (figure 2.3).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建一个服务于数十个不同深度学习项目并对外开放给多个人员和团队的严肃深度学习平台时，简单的数据流程图会迅速膨胀成一个令人困惑的3D图（图2.3）。
- en: '![](../Images/02-03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-03.png)'
- en: Figure 2.3 Deep learning model development in enterprise runs in multidimensions.
    Multiple teams of people work together to ship a project in different phases.
    Each team focuses on one step of the workflow and also works on multiple projects
    in parallel.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 企业中深度学习模型开发是多维度的。多个团队协同工作，在不同阶段交付项目。每个团队专注于工作流程的一个步骤，并且并行处理多个项目。
- en: 'Figure 2.3 shows the complexity of an enterprise deep learning development
    environment. In this setting, each person only works on a single step instead
    of the entire workflow, and they develop their work for multiple projects. Ideally,
    this process is efficient because people build their expertise by focusing on
    one particular problem. But here is the catch: the communication cost is often
    ignored.'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3展示了企业深度学习开发环境的复杂性。在这个环境中，每个人只专注于工作流程的一个步骤，而不是整个工作流程，他们为多个项目开发自己的工作。理想情况下，这个过程是高效的，因为人们通过专注于特定问题来建立他们的专业知识。但这里有一个问题：沟通成本经常被忽视。
- en: When we divide the steps of a workflow (figure 2.2) between multiple teams,
    data schemas are needed for the handshake. Without a data contract, the downstream
    team doesn’t know how to read the data sent from the upstream team. Let’s go back
    to figure 2.3\. Imagine how many data schemas we need to communicate between teams
    if there are 10 projects developed by four teams in parallel, especially if every
    team handles different steps of the workflow.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将工作流程的步骤（图2.2）分配给多个团队时，需要数据模式进行握手。没有数据合同，下游团队不知道如何读取上游团队发送的数据。让我们回到图2.3。想象一下，如果有四个团队并行开发10个项目，每个团队处理工作流程的不同步骤，我们需要在团队之间沟通多少数据模式。
- en: Now, if we want to add a new feature or attribute (such as text language) to
    a training dataset, we need to gather every team, obtain a consensus on the new
    data format, and implement the change. This is a huge effort because cross-team
    collaboration in corporations is complicated. It often takes months to make a
    small change; because each team has its own priority, you have to wait on its
    to-do list.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们想向训练数据集中添加一个新功能或属性（如文本语言），我们需要召集每个团队，就新的数据格式达成共识，并实施更改。这是一个巨大的努力，因为企业中的跨团队合作很复杂。一个小小的改变通常需要数月时间；因为每个团队都有自己的优先级，你必须等待它的待办事项列表。
- en: To make the situation worse, deep learning model development is an iterative
    process. It demands constantly tuning the training dataset (including the upstream
    data pipelines) to improve model accuracy. This requires data scientists, data
    developers, and platform developers to interact at a high frequency, but because
    of the cross-team workflow setting, the data iteration happens slowly, which is
    one of the reasons why model development is so slow in a production environment.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，深度学习模型开发是一个迭代过程。它要求不断调整训练数据集（包括上游数据管道）以提高模型精度。这需要数据科学家、数据开发人员和平台开发者以高频率互动，但由于跨团队工作流程设置，数据迭代进展缓慢，这是模型在生产环境中开发缓慢的一个原因。
- en: Another problem is that when we have multiple types of projects (image, video,
    and text) developing in parallel, the number of data schemas will explode. If
    we let each team define new data schemas freely and don’t manage them properly,
    keeping the system backward compatible is almost impossible. The new data updates
    will become more and more difficult because we have to spend extra time to make
    sure the new data update doesn’t break the projects built in the past. As a consequence,
    the project development velocity will slow down significantly.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是我们现在有多个类型的项目（图像、视频和文本）并行开发时，数据模式的数量会激增。如果我们让每个团队自由定义新的数据模式并且没有妥善管理它们，保持系统向后兼容几乎是不可能的。新的数据更新将变得越来越困难，因为我们必须花费额外的时间来确保新的数据更新不会破坏过去构建的项目。因此，项目开发速度将显著减慢。
- en: To address the slow iteration and data schema management problem, we can build
    a dataset management service. Let’s look at figure 2.4 to help determine the changes
    in the project development workflow after introducing the dataset management service.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决迭代缓慢和数据模式管理问题，我们可以构建一个数据集管理服务。让我们看看图2.4，以帮助确定在引入数据集管理服务后项目开发工作流程的变化。
- en: 'In figure 2.4, we see a dataset management service that splits the model development
    workflow into two separate spaces: data developer space and data scientist space.
    The long iteration loop (figure 2.2) is now divided into two small loops (figure
    2.4), and each loop is owned by a single team, so the data developer and data
    scientist can iterate on data collection and model training separately; therefore,
    the deep learning project can iterate much faster.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2.4中，我们看到一个数据集管理服务将模型开发工作流程分为两个独立的空间：数据开发者空间和数据科学家空间。长的迭代循环（图2.2）现在被划分为两个小的循环（图2.4），每个循环由一个团队独立拥有，因此数据开发者和数据科学家可以分别迭代数据收集和模型训练；因此，深度学习项目可以迭代得更快。
- en: '![](../Images/02-04.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图2.4](../Images/02-04.png)'
- en: Figure 2.4 A dataset management component creates a good separation between
    training data collection and consumption by defining strongly typed schemas for
    both, which allows data development and model algorithm development to iterate
    in their own loop, thus expediting the project development.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 数据集管理组件通过为两者定义强类型模式来创建训练数据收集和消费之间的良好分离，这允许数据开发和模型算法开发在自己的循环中迭代，从而加速项目开发。
- en: 'You may also notice that we now have all data schemas in one place: a dataset
    management service, which manages two strongly typed data schemas—the ingestion
    data schema and the training data schema—for each type of dataset. By having two
    separate data schemas for data ingestion and training while doing data transformation
    inside DM, you ensure that the data changes in the upstream data collection won’t
    break the downstream model training. Because the data schemas are strongly typed,
    future data upgrades can easily be made backward compatible.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还会注意到，我们现在所有的数据模式都在一个地方：一个数据集管理服务，它管理每种类型的数据集的两个强类型数据模式——摄入数据模式和解训数据模式。通过在DM内部进行数据转换时为数据摄入和解训定义两个独立的数据模式，你确保了上游数据收集中的数据变化不会破坏下游模型训练。因为数据模式是强类型的，未来的数据升级可以轻松地实现向后兼容。
- en: Defining a strongly typed dataset may not be a good idea for projects in the
    beginning or experimental phase because we are still exploring all kinds of data
    options. Therefore, we also recommend defining a special schema-free dataset type,
    such as `GENERIC` type, which has no strong schema restriction. For data in this
    dataset type, DM just accepts the data as is and does not perform data validation
    and transformation (for a detailed example, see section 2.2.6). The data collected
    from the data processing pipeline can be consumed directly by the training process.
    Although the whole workflow would be fragile, a free dataset type addresses the
    need to be flexible for projects in the early phase. Once the project matures,
    we can create strongly typed schemas and define a dataset type for them.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 对于处于初期或实验阶段的项目，定义强类型数据集可能不是一个好主意，因为我们仍在探索各种数据选项。因此，我们也建议定义一个特殊的无模式数据集类型，例如`GENERIC`类型，它没有强模式限制。对于此类数据集中的数据，DM仅接受数据原样，不执行数据验证和转换（有关详细示例，请参阅第2.2.6节）。从数据处理管道收集的数据可以直接由训练过程消费。尽管整个工作流程可能会变得脆弱，但自由的数据集类型满足了项目在早期阶段灵活性的需求。一旦项目成熟，我们可以为它们创建强类型模式并定义数据集类型。
- en: To summarize this section, managing two data schemas of a dataset type is the
    secret sauce that decouples data scientists and data developers. In section 2.2.6,
    we will show how these schemas can be implemented in our sample dataset management
    service.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 总结本节内容，管理数据集类型的数据模式是使数据科学家和数据开发者解耦的秘诀。在2.2.6节中，我们将展示这些模式如何在我们的样本数据集管理服务中实现。
- en: Enabling model reproducibility
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 启用模型可复现性
- en: A well-designed dataset management service supports model reproducibility by
    having version tracking on training datasets—for example, using a version string
    to obtain the exact training files used in previous model training runs. The advantage
    of model reproducibility with respect to the data scientist (model algorithm development)
    is that you can repeatedly run a deep learning algorithm (such as the self-attention
    transformer in NLP) on a certain dataset and gain the same or similar quality
    of results. This is called *algorithm reproducibility*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一个设计良好的数据集管理服务通过在训练数据集上实现版本跟踪来支持模型可复现性——例如，使用版本字符串来获取先前模型训练运行中使用的确切训练文件。对于数据科学家（模型算法开发）而言，模型可复现性的优势在于你可以在某个数据集上反复运行深度学习算法（如NLP中的自注意力转换器）并得到相同或相似的质量结果。这被称为*算法可复现性*。
- en: From the view of a deep learning system developer, model reproducibility is
    the superset of algorithm reproducibility. It requires the dataset management
    system to be able to reproduce its output artifacts (datasets). For example, we
    need to obtain the exact training data and training configuration to reproduce
    models that were trained in the past.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 从深度学习系统开发者的角度来看，模型可复现性是算法可复现性的超集。它要求数据集管理系统能够复现其输出工件（数据集）。例如，我们需要获取确切的训练数据和训练配置来复现过去训练的模型。
- en: Model reproducibility is crucial to machine learning projects for two main reasons.
    The first is trust. Reproducibility creates trust and credibility for the system
    that produces the model. For any system, if the output can’t be reproduced, people
    simply won’t trust the system. This is extremely relevant to machine learning
    projects because applications will make decisions based on model output—for example,
    a chatbot will transfer a user call to proper service departments according to
    the user intent prediction. If we can’t reproduce a model, the applications built
    on top of the model are nondeterministic and untrustworthy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可复现性对于机器学习项目至关重要，主要原因有两个。第一个是信任。可复现性为生成模型的系统创造了信任和信誉。对于任何系统，如果输出无法复现，人们就不会信任该系统。这对于机器学习项目尤其相关，因为应用将基于模型输出做出决策——例如，聊天机器人将根据用户意图预测将用户呼叫转接到适当的服务部门。如果我们无法复现模型，建立在模型之上的应用将是不确定和不可信的。
- en: The second reason is that model reproducibility facilitates performance troubleshooting.
    When detecting a model performance regression, people first want to find out what
    has changed in the training dataset and training algorithm code. If model reproducibility
    is not supported, performance troubleshooting will be very difficult.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是模型可复现性有助于性能故障排除。当检测到模型性能退化时，人们首先想了解训练数据集和训练算法代码中发生了什么变化。如果不支持模型可复现性，性能故障排除将非常困难。
- en: 2.1.2 Dataset management design principles
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 数据集管理设计原则
- en: We want to outline five design principles for DM before we start building one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始构建之前，我们想概述五个DM的设计原则。
- en: Note We consider these five principles to be the most important elements in
    this chapter. For data applications, the principles we follow in design are more
    important than the actual design. Because data could be anything in any form,
    there is no paradigm for data storage, in general, and there is no standard design
    that suits all kinds of data processing use cases. So, in practice, we build our
    own data application by following certain general principles. Therefore, these
    principles are critical.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们认为这五个原则是本章最重要的元素。对于数据应用而言，我们在设计中所遵循的原则比实际的设计更为重要。因为数据可以是任何形式，所以没有通用的数据存储范式，也没有适用于所有数据处理的通用设计标准。因此，在实践中，我们通过遵循某些一般原则来构建自己的数据应用。因此，这些原则至关重要。
- en: The five principles here will give you clear design targets for building a new
    DM service or improving your existing DM service.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这五个原则将为构建新的DM服务或改进现有的DM服务提供明确的设计目标。
- en: 'Principle 1: Support dataset reproducibility for reproducing models'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 原则1：支持数据集可复现性以复现模型
- en: Dataset reproducibility means that the DM always returns the same exact training
    examples it has returned in the past. For instance, when the training team starts
    training a model, the DM provides a dataset with a version string. Anytime the
    training team—or any other team—needs to retrieve the same training data, it can
    use this version string to query DM to retrieve the same training data.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集可重复性意味着数据管理器（DM）总是返回过去返回过的完全相同的训练示例。例如，当训练团队开始训练模型时，DM 提供一个带有版本字符串的数据集。无论何时训练团队或任何其他团队需要检索相同的训练数据，都可以使用此版本字符串查询
    DM 以检索相同的训练数据。
- en: We believe all DM systems should support dataset reproducibility. Even better
    would be to also offer data diff functionally, so we can see the data difference
    between two different dataset versions easily. This is very convenient for troubleshooting.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们认为所有 DM 系统都应该支持数据集可重复性。更好的是，还可以提供数据差异功能，这样我们就可以轻松地看到不同数据集版本之间的数据差异。这对于故障排除非常方便。
- en: 'Principle 2: Provide unified API across different types of datasets'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 2：提供跨不同类型数据集的统一 API
- en: A dataset of deep learning might be structured (text, such as sales records
    or the transcript of a user conversation) or unstructured (image, voice recording
    file). No matter how a DM system processes and stores these different forms of
    data internally, it should provide a unified API interface for uploading and fetching
    different types of datasets. The API interface also abstracts away the data source
    from the data consumer; no matter what happens under the hood, such as data parsing
    changes and internal storage format changes, downstream consumers shouldn’t be
    affected.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习的数据集可能是结构化的（如销售记录或用户对话的记录文本）或非结构化的（如图像、声音录制文件）。无论 DM 系统如何内部处理和存储这些不同形式的数据，它都应该提供统一的
    API 接口来上传和检索不同类型的数据集。API 接口还抽象化了数据源与数据消费者之间的关系；无论底层发生什么，例如数据解析变化和内部存储格式变化，下游消费者都不应受到影响。
- en: Therefore, our users, both data scientists and data developers, only need to
    learn one API to work with all the different types of datasets. This makes the
    system simple and easy to use. Also, the code maintenance cost will be greatly
    reduced because we only expose one public API.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的用户，无论是数据科学家还是数据开发者，只需要学习一个 API 就可以处理所有不同类型的数据集。这使得系统简单易用。此外，代码维护成本将大大降低，因为我们只公开一个
    API。
- en: 'Principle 3: Adopt a strongly typed data schema'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 3：采用强类型数据模式
- en: A strongly typed data schema is the key to avoiding unexpected failures caused
    by data changes. With data schema enforcement, the DM service can guarantee that
    the raw data it ingests and the training data it produces are consistent with
    our specs.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 强类型数据模式是避免由数据变化引起意外失败的关键。有了数据模式强制执行，DM 服务可以保证其摄取的原始数据和产生的训练数据与我们的规范一致。
- en: The strongly typed data schema acts as a safety guard to ensure the downstream
    model training code does not get affected by the upstream data collection changes,
    and it also ensures backward compatibility for both upstream and downstream clients
    of DM. Without data schema protection, the dataset consumer—the downstream model
    training code—can easily be broken by upstream data changes.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 强类型数据模式充当安全防护，确保下游模型训练代码不受上游数据收集变化的影响，并确保 DM 的上游和下游客户端的向后兼容性。没有数据模式保护，数据集消费者——下游模型训练代码——很容易被上游数据变化破坏。
- en: Data schemas can be versioned as well, but this will add another layer of complexity
    to management. An additional option is to only have one schema per dataset. When
    introducing new data changes, make sure that the schema update is backward compatible.
    If a new data requirement requires a breaking change, create a new dataset type
    with a new schema instead of updating the existing one.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模式也可以进行版本控制，但这将为管理增加另一层复杂性。另一个选项是每个数据集只保留一个模式。在引入新的数据变更时，确保模式更新是向后兼容的。如果新的数据需求需要破坏性变更，则创建一个新的数据集类型，并使用新的模式，而不是更新现有的模式。
- en: 'Principle 4: Ensure API consistency and handle scaling internally'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 原则 4：确保 API 一致性并内部处理扩展
- en: The current trend in the deep learning field is model architecture that keeps
    getting bigger as datasets continue to grow larger. For example, GPT-3 (a generative
    pretrained transformer language model for language understanding) uses more than
    250 TB of text materials with hundreds of billions of words; in Tesla, the autonomous
    driving model consumes an immense amount of data at the petabyte level. On the
    other hand, we still use small datasets (around 50 MB) for some easy tasks in
    narrow domains, such as customer support ticket classification. Dataset management
    systems should handle the data scaling challenges internally, and the API exposed
    to users (data developers and data scientists) should be consistent for both large-
    and small-sized datasets.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习领域的当前趋势是随着数据集的不断扩大，模型架构也在不断增大。例如，GPT-3（一种用于语言理解的生成预训练转换器语言模型）使用了超过250 TB的文本材料，包含数百亿个单词；在特斯拉，自动驾驶模型消耗了海量数据，达到PB级别。另一方面，我们仍然使用小型数据集（大约50
    MB）来处理一些狭窄领域的简单任务，例如客户支持工单分类。数据集管理系统应在内部处理数据扩展挑战，并且向用户（数据开发者和数据科学家）公开的API应适用于大型和小型数据集。
- en: 'Principle 5: Guarantee data persistency'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 原则5：保证数据持久性
- en: Ideally, datasets used for deep learning training should be stored immutably
    to reproduce training data and troubleshoot. Data removal should be soft deletions
    with only a few exceptions for hard deletions, such as deleting customer data
    permanently when a customer chooses to opt out of or cancel their account.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，用于深度学习训练的数据集应存储为不可变，以便重现训练数据和故障排除。数据删除应该是软删除，只有少数例外是硬删除，例如当客户选择退出或取消其账户时，永久删除客户数据。
- en: 2.1.3 The paradoxical character of datasets
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.3 数据集的矛盾特性
- en: To close out our conceptual discussion on dataset management, we would like
    to clarify an ambiguous aspect of datasets. We have seen dozens of poorly designed
    dataset management systems fail on this point.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 为了结束我们对数据集管理的概念讨论，我们想澄清数据集的一个模糊方面。我们看到了数十个设计不佳的数据集管理系统在这方面失败。
- en: 'A dataset has a paradoxical trait: it is both dynamic and static. From a data
    scientist’s point of view, a dataset is static: it is a fixed set of files with
    annotations (also known as labels). From a data developer’s point of view, a dataset
    is dynamic: it is a file-saving destination in a remote storage to which we keep
    adding data.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有一个矛盾的特性：它既是动态的又是静态的。从数据科学家的角度来看，数据集是静态的：它是一组带有注释（也称为标签）的固定文件。从数据开发者的角度来看，数据集是动态的：它是一个远程存储中的文件保存目标，我们不断向其中添加数据。
- en: So, from a DM perspective, a dataset should be a logic file group and satisfy
    both data collection and data training needs. To help you get a concrete understanding
    of how to accommodate both the dynamic and static nature of datasets, let’s look
    at figure 2.5.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，从DM的角度来看，数据集应该是一个逻辑文件组，并满足数据收集和数据训练的需求。为了帮助您具体理解如何适应数据集的动态和静态特性，让我们看看图2.5。
- en: '![](../Images/02-05.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图2.5](../Images/02-05.png)'
- en: 'Figure 2.5 A dataset is a logic file group: it’s both dynamic and static, and
    it''s editable for data collection but fixed for model training.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 数据集是一个逻辑文件组：它既是动态的又是静态的，对于数据收集是可编辑的，但对于模型训练是固定的。
- en: 'We can read figure 2.5 from two angles: data ingestion and data fetching. First,
    from the data ingestion side, we see that the data collection pipeline (from the
    left of the graph) keeps pumping in new data, such as text utterances and labels.
    For example, at time T0, an example data batch (example batch T0) is created in
    the dataset—the same for time T1, T2, and T3; we have a total of four data batches
    created over time. So, from the data developer’s view, this dataset is mutable,
    because the pipeline keeps adding data to it.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从两个角度来阅读图2.5：数据摄取和数据检索。首先，从数据摄取的角度来看，我们看到数据收集管道（从图的左侧）持续地注入新的数据，例如文本语句和标签。例如，在时间T0时，数据集中创建了一个示例数据批次（示例批次T0）——同样适用于时间T1、T2和T3；我们在一段时间内总共创建了四个数据批次。因此，从数据开发者的角度来看，这个数据集是可变的，因为管道持续向其中添加数据。
- en: Second, on the training data fetching side (from the top of the graph), we see
    that when fetching training data, the DM reads all the current data from the dataset
    at the same time point. We see that the data is returned as a static versioned
    snapshot, which has a version string to uniquely identify the actual data it picked
    from a dataset. For example, when we fetch training data from the dataset at time
    T2, the dataset has three data batches (batch T0, batch T1, and batch T2). We
    package these three data batches into a snapshot, assign a version string (“version1”),
    and return it as training data.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，在训练数据获取方面（从图的顶部），我们看到在获取训练数据时，DM会同时从数据集的同一时间点读取所有当前数据。我们看到数据以静态版本化的快照形式返回，该快照有一个版本字符串来唯一标识它从数据集中选择的数据。例如，当我们从时间T2的数据集获取训练数据时，数据集有三个数据批次（批次T0、批次T1和批次T2）。我们将这三个数据批次打包成一个快照，分配一个版本字符串（“version1”），并将其作为训练数据返回。
- en: From a model training perspective, the dataset fetched from DM is a static snapshot
    of the dataset—a time-filtered plus customer logic-filtered dataset. The static
    snapshot is crucial to model reproducibility because it represents the exact training
    files used in a training run. When we need to rebuild the model, we can use the
    snapshot version string to find the snapshot that was used in the past model training.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 从模型训练的角度来看，从DM获取的数据集是数据集的静态快照——一个经过时间过滤和客户逻辑过滤的数据集。静态快照对于模型的可重复性至关重要，因为它代表了训练运行中使用的确切训练文件。当我们需要重建模型时，我们可以使用快照版本字符串来找到过去模型训练中使用的快照。
- en: Our theoretical introductions have been thoroughly covered, and you should be
    able to grasp the needs, goals, and unique characteristics of the dataset management
    component. The next section is a concrete example of how to design a dataset management
    service.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的理论介绍已经彻底覆盖，您应该能够掌握数据集管理组件的需求、目标和独特特性。下一节将具体说明如何设计数据集管理服务。
- en: 2.2 Touring a sample dataset management service
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 浏览样本数据集管理服务
- en: In this section, we will walk you through a sample DM service. We built this
    sample to give you an idea of how the principles presented in section 2.1.2 can
    be implemented. We will first run the service locally, play with it, and then
    look at its API design and internal implementation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将向您展示一个样本DM服务。我们构建这个样本是为了让您了解第2.1.2节中提出的原理如何实现。我们首先将在本地运行该服务，进行操作，然后查看其API设计和内部实现。
- en: 2.2.1 Playing with the sample service
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.1 操练样本服务
- en: To make this easy for you, we built seven shell scripts to automate the entire
    DM lab. These shell scripts are the recommended way to experience the demo scenarios
    in this section because they not only automate services’ local setup but also
    take care of setting the environment variables, preparing sample data, and initializing
    the local network.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您更容易操作，我们构建了七个shell脚本来自动化整个DM实验室。这些shell脚本是体验本节中演示场景的推荐方式，因为它们不仅自动化了服务的本地设置，还负责设置环境变量、准备样本数据和初始化本地网络。
- en: 'You can find these scripts at [https://github.com/orca3/MiniAutoML/tree/main/scripts](https://github.com/orca3/MiniAutoML/tree/main/scripts),
    starting with the search: “dm”. The “function demo” doc in our GitHub repo ([https://github.com/orca3/MiniAutoML/tree/main/data-management](https://github.com/orca3/MiniAutoML/tree/main/data-management))
    provides detailed instructions for how to complete the lab and sample outputs
    of these scripts.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[https://github.com/orca3/MiniAutoML/tree/main/scripts](https://github.com/orca3/MiniAutoML/tree/main/scripts)中找到这些脚本，搜索“dm”。我们GitHub仓库中的“function
    demo”文档([https://github.com/orca3/MiniAutoML/tree/main/data-management](https://github.com/orca3/MiniAutoML/tree/main/data-management))提供了如何完成实验室和这些脚本的样本输出的详细说明。
- en: Note Before running the function demo, the system requirement should be met
    first. Please refer to [https://github.com/orca3/MiniAutoML#system-requirements](https://github.com/orca3/MiniAutoML#system-requirements).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在运行函数演示之前，应首先满足系统要求。请参阅[https://github.com/orca3/MiniAutoML#system-requirements](https://github.com/orca3/MiniAutoML#system-requirements)。
- en: 'This lab consists of three sections: first, run the sample dataset management
    service; second, create a dataset and upload data to it; and third, fetch training
    data from the dataset just created.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 本实验分为三个部分：首先，运行样本数据集管理服务；其次，创建一个数据集并将数据上传到其中；最后，从刚刚创建的数据集中获取训练数据。
- en: Setting up service locally
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 本地设置服务
- en: The sample service is written in Java 11\. It uses MinIO as the file blob server
    to mimic cloud object storage (such as Amazon S3), so we can run everything locally
    without any remote dependency. If you have set up the lab in appendix A, you can
    run the following commands (listing 2.1) in your terminal at the root of the scripts
    folder to start the service.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 样例服务是用 Java 11 编写的。它使用 MinIO 作为文件 blob 服务器来模拟云对象存储（如 Amazon S3），因此我们可以在本地运行一切，而无需任何远程依赖。如果您已在附录
    A 中设置了实验室，您可以在脚本文件夹的根目录下通过终端运行以下命令（列表 2.1）来启动服务。
- en: Note Starting with a clean setup is highly recommended before running DM demo
    scripts. You can execute `./scripts/lab-999-tear-down.sh` to clean up previous
    labs.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在运行 DM 演示脚本之前，建议从干净的环境开始。您可以通过执行 `./scripts/lab-999-tear-down.sh` 来清理之前的实验室。
- en: Listing 2.1 Starting the service locally
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 在本地启动服务
- en: '[PRE0]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Note To keep the service setup to a bare minimum, we maintain all the dataset
    records in memory to avoid using databases. Please be aware that you will lose
    all datasets if you restart the dataset management service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了将服务设置保持在最低限度，我们保持所有数据集记录在内存中，以避免使用数据库。请注意，如果您重新启动数据集管理服务，您将丢失所有数据集。
- en: Creating and updating a language intent dataset
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 创建和更新语言意图数据集
- en: Our sample DM service offers three API methods for users to create/update a
    dataset and check the result. These API methods are `CreateDataset`, `UpdateDataset`,
    and `GetDatasetSummary`. We will discuss them in detail in the next few sections.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的示例 DM 服务为用户提供三种 API 方法来创建/更新数据集并检查结果。这些 API 方法是 `CreateDataset`、`UpdateDataset`
    和 `GetDatasetSummary`。我们将在接下来的几节中详细讨论它们。
- en: In this example scenario, first we call the `CreateDataset` API method on the
    data management service to create a new language intent dataset; then we use the
    `UpdateDataset` API method to append more data to the dataset. Finally, we use
    the `GetDatasetSummary` API method to obtain the dataset’s statistics and commit
    (data change) history.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在本示例场景中，首先我们在数据管理服务上调用 `CreateDataset` API 方法来创建一个新的语言意图数据集；然后我们使用 `UpdateDataset`
    API 方法向数据集中追加更多数据。最后，我们使用 `GetDatasetSummary` API 方法获取数据集的统计信息和提交（数据变更）历史。
- en: Note The scripts dm-003-create-dataset.sh and dm-004-add-commits.sh automate
    the previous steps. Please use them to run the demo scenario. Please note that
    the following code listings are only for illustration purposes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：脚本 dm-003-create-dataset.sh 和 dm-004-add-commits.sh 自动执行前面的步骤。请使用它们来运行演示场景。请注意，以下代码列表仅用于说明目的。
- en: Let’s run the lab now. First, we’ll create a dataset using the following listing.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在运行实验室。首先，我们将使用以下列表创建一个数据集。
- en: Listing 2.2 Creating a language intent dataset
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 创建语言意图数据集
- en: '[PRE1]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Uploads raw data (upload/001.csv) to cloud storage
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将原始数据（upload/001.csv）上传到云存储
- en: ❷ gRPC request to create a dataset
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建数据集的 gRPC 请求
- en: ❸ Dataset type
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据集类型
- en: ❹ Data URL of the raw data in MinIO, for example, upload/001.csv
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ MinIO 中原始数据的 URL，例如，upload/001.csv
- en: ❺ API name
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ API 名称
- en: 'It should be noted that the `CreateDataset` API expects users to provide a
    downloadable URL in the gRPC request, not the actual data, which is why we first
    upload the 001.csv file to the local MinIO server. After the dataset is created,
    the `CreateDataset` API will return a JSON object that contains a data summary
    and commits the history of the dataset. See a sample result as follows:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 应注意，`CreateDataset` API 期望用户在 gRPC 请求中提供可下载的 URL，而不是实际数据，这就是为什么我们首先将 001.csv
    文件上传到本地 MinIO 服务器。数据集创建后，`CreateDataset` API 将返回一个包含数据摘要和数据集提交历史的 JSON 对象。以下是一个示例结果：
- en: '[PRE2]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Commits are the snapshot dataset updates.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 提交是数据集快照更新。
- en: ❷ Commit ID; this commit captures the data from upload/001.csv.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提交 ID；此提交捕获了从 upload/001.csv 上传的数据。
- en: ❸ Commit tags are used to filter commits when building a training dataset.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提交标签用于在构建训练数据集时过滤提交。
- en: ❹ Data summary of the commit
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 提交的数据摘要
- en: After creating a dataset, you can keep updating it by appending more data; see
    the dataset update gRPC request as follows.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 创建数据集后，您可以通过追加更多数据来持续更新它；请参见以下数据集更新 gRPC 请求。
- en: Listing 2.3 Updating a language intent dataset
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 更新语言意图数据集
- en: '[PRE3]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Uploads raw data (upload/002.csv) to cloud storage
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将原始数据（upload/002.csv）上传到云存储
- en: ❷ A request to append more data (upload/002.csv)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 追加更多数据（upload/002.csv）的请求
- en: ❸ Replace the dataset ID with the value returned from the CreateDataset API.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将数据集 ID 替换为 `CreateDataset` API 返回的值。
- en: ❹ The data URL of raw data, created by raw data upload
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 原始数据的 URL，由原始数据上传创建
- en: ❺ Updates the dataset API name
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 更新数据集 API 名称
- en: 'Once the dataset update completes, the `UpdateDataset` API returns a data summary
    JSON object in the same way as the `CreateDataset` API does; see a sample responsible
    object as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据集更新完成，`UpdateDataset` API将以与`CreateDataset` API相同的方式返回一个数据摘要JSON对象；请参见以下示例响应对象。
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The commit created by the create dataset request
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建数据集请求创建的提交
- en: ❷ Commit ID; this commit captures the data from upload/002.csv.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提交ID；这个提交捕获了上传/002.csv中的数据。
- en: 'You can also fetch the data summary and commit history of a dataset by using
    the `GetDatasetSummary` API. See the following sample gRPC request:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以通过使用`GetDatasetSummary` API来获取数据摘要和提交历史。请参见以下示例gRPC请求：
- en: '[PRE5]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The ID of the dataset to query
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要查询的数据集ID
- en: Fetch training dataset
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集
- en: Now we have a dataset (ID = 1) created with raw data; let’s try to build a training
    dataset from it. In our sample service, it’s a two-step process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了一个包含原始数据的数据集（ID = 1）；让我们尝试从它构建一个训练数据集。在我们的示例服务中，这是一个两步过程。
- en: We first call the `PrepareTrainingDataset` API to start the dataset-building
    process. And then we use the `FetchTrainingDataset` API to query the dataset preparation
    progress until the request completes.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调用`PrepareTrainingDataset` API来启动数据集构建过程。然后我们使用`FetchTrainingDataset` API查询数据集准备进度，直到请求完成。
- en: Note Scripts dm-005-prepare-dataset.sh, dm-006-prepare-partial-dataset.sh, and
    dm-007-fetch-dataset-version.sh automate the steps that follow. Please try to
    use them to run the sample dataset fetching demo in code listings 2.4 and 2.5.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 注意脚本dm-005-prepare-dataset.sh、dm-006-prepare-partial-dataset.sh和dm-007-fetch-dataset-version.sh自动化了以下步骤。请尝试使用它们在代码列表2.4和2.5中运行示例数据集获取演示。
- en: To use the `PrepareTrainingDataset` API, we only need to provide a dataset ID.
    If you just want a portion of data to be in the training dataset, you can use
    `tag` as a filter in the request. See a sample request as follows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`PrepareTrainingDataset` API，我们只需要提供一个数据集ID。如果您只想将部分数据包含在训练数据集中，您可以在请求中使用`tag`作为过滤器。请参见以下示例请求。
- en: Listing 2.4 Preparing a training dataset
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4准备训练数据集
- en: '[PRE6]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ A request to prepare the training dataset with all data commits
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备包含所有数据提交的训练数据集的请求
- en: ❷ A request to prepare the training dataset with partial data commits by defining
    filter tags
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过定义过滤器标签准备包含部分数据提交的训练数据集的请求
- en: ❸ Data filters
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据过滤器
- en: 'Once the data preparation gRPC request succeeds, it returns a JSON object as
    follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备gRPC请求成功，它将返回以下JSON对象：
- en: '[PRE7]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ ID of the training dataset snapshot
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集快照的ID
- en: ❷ The selected data commits of the raw dataset
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从原始数据集中选择的数据提交
- en: Among the data that the `PrepareTrainingDataset` API returns is the `"version_hash"`
    string. It is used to identify the data snapshot produced by the API. Using this
    hash as an ID, we can call the `FetchTrainingDatasetc` API to track the progress
    of building the training dataset; see the following example.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在`PrepareTrainingDataset` API返回的数据中，有一个名为`"version_hash"`的字符串。它用于识别API产生的数据快照。使用这个哈希值作为ID，我们可以调用`FetchTrainingDatasetc`
    API来跟踪构建训练数据集的进度；请参见以下示例。
- en: Listing 2.5 Checking dataset prepare progress
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5检查数据集准备进度
- en: '[PRE8]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ ID of the training dataset snapshot
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集快照的ID
- en: 'The `FetchTrainingDatasetc` API returns a JSON object that describes the training
    dataset. It tells us the status of the background dataset-building process: `RUNNING`,
    `READY`, or `FAILED`. If the training data is ready for consumption, the response
    object will display a list of downloadable URLs for the training data. In this
    demo, the URLs point to the local MinIO server. See a sample response as follows:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`FetchTrainingDatasetc` API返回一个描述训练数据集的JSON对象。它告诉我们后台数据集构建过程的状况：`RUNNING`、`READY`或`FAILED`。如果训练数据已准备好供消费，响应对象将显示训练数据的可下载URL列表。在这个演示中，URL指向本地MinIO服务器。请参见以下示例响应。'
- en: '[PRE9]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Status of the training dataset
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据集的状态
- en: ❷ Data URLs of the training data
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据的URL
- en: Good job! You just experienced all the major data APIs offered by our sample
    dataset management service. By trying to upload data and build training datasets
    by yourself, we hope you have gained a feeling for how this service can be used.
    In the next few sections, we will look at user scenarios, service architecture
    overview, and code implementation of our sample dataset management services.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！您刚刚体验了我们示例数据集管理服务提供的所有主要数据API。通过尝试自己上传数据和构建训练数据集，我们希望您已经对如何使用此服务有了感觉。在接下来的几节中，我们将探讨用户场景、服务架构概述以及我们示例数据集管理服务的代码实现。
- en: Note If you encounter any problems when running the mentioned scripts, please
    refer to the instructions in the “function demo” doc of our GitHub repo. Also,
    if you want to try the labs in chapters 3 and 4, please keep the containers running
    because they are the prerequisites for the model training labs.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果在运行提到的脚本时遇到任何问题，请参阅我们GitHub仓库中“功能演示”文档中的说明。此外，如果您想尝试第3章和第4章中的实验室，请保持容器运行，因为它们是模型训练实验室的先决条件。
- en: 2.2.2 Users, user scenarios, and the big picture
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.2 用户、用户场景和整体情况
- en: When designing backend services, the method we found very useful is thinking
    from the outside in. First, figure out who the users are, what value the service
    will provide, and how customers will interact with the service. Then the inner
    logic and storage layout should come naturally to you. For touring this sample
    DM service, we will show you using the same approach. So let’s look at our users
    and user scenario first.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在设计后端服务时，我们发现非常有用的方法是“从外向内”思考。首先，确定用户是谁，服务将提供什么价值，以及客户将如何与该服务互动。然后，内部逻辑和存储布局将自然而然地呈现出来。在参观这个样例DM服务时，我们将使用相同的方法。因此，让我们首先看看我们的用户和用户场景。
- en: Note The reason we look at the use cases first is that we believe any system
    design should consider the user the most. Our approach to efficiency and scalability
    will come up naturally if we identify how customers use the system. If the design
    is taken in the reverse order (consider technology first and usability second),
    the system often is clumsy to use because it’s designed for technology and not
    for customers.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们首先考虑用例的原因是，我们相信任何系统设计都应该把用户放在首位。如果我们确定了客户如何使用系统，我们的效率和可扩展性方法将自然而然地出现。如果设计顺序相反（首先考虑技术，其次考虑可用性），系统通常使用起来会很笨拙，因为它是为技术而不是为顾客设计的。
- en: Users and user scenarios
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 用户和用户场景
- en: 'Our sample DM service is built for two fictional users: Jianguo, a data engineer,
    and Julia, a data scientist. They work together to train a language-intent classification
    model.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样例DM服务是为两位虚构用户构建的：Jianguo，一位数据工程师，和Julia，一位数据科学家。他们一起工作以训练一个语言意图分类模型。
- en: Jianguo works on training data collection. He continuously collects data from
    different data sources (such as parsing user activity logs and conducting customer
    surveys) and labels them. Jianguo uses a DM data ingestion API to create datasets,
    append new data to existing datasets, and query datasets’ summary and status.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Jianguo负责训练数据收集。他持续地从不同的数据源（例如解析用户活动日志和进行客户调查）收集数据，并对它们进行标记。Jianguo使用DM数据摄取API创建数据集，向现有数据集追加新数据，并查询数据集的摘要和状态。
- en: Julia uses the dataset built by Jianguo to train intent classification models
    (usually written in PyTorch or Tensorflow). At the training time, Julia’s training
    code will first call the DM service’s fetch training data API to get the training
    dataset from the DM and then start the training process.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Julia使用Jianguo构建的数据集来训练意图分类模型（通常用PyTorch或Tensorflow编写）。在训练时，Julia的训练代码将首先调用DM服务的获取训练数据API从DM获取训练数据集，然后开始训练过程。
- en: The service’s overall architecture
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的整体架构
- en: 'Our sample DM service is built in three layers: the data ingestion layer, dataset
    fetching layer, and dataset internal storage layer. The data ingestion API set
    is built so that Jianguo can upload new training data and query dataset status.
    The dataset fetching API is built so that Julia can obtain the training dataset.
    See figures 2.6 and 2.7 for the whole picture.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的样例DM服务构建在三层：数据摄取层、数据集获取层和数据集内部存储层。数据摄取API集构建得使得Jianguo可以上传新的训练数据并查询数据集状态。数据集获取API构建得使得Julia可以获得训练数据集。参见图2.6和2.7以获得整体情况。
- en: '![](../Images/02-06.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-06.png)'
- en: Figure 2.6 System overview of the sample dataset management service. The sample
    service contains three main components, a data ingestion API, internal storage,
    and a dataset fetching API.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 样例数据集管理服务的系统概述。该样例服务包含三个主要组件，一个数据摄取API、内部存储和一个数据集获取API。
- en: 'The central big box in figure 2.6 shows the overall design of our sample dataset
    management service. It has an internal dataset storage system and two public-facing
    interfaces: a data ingestion API and a dataset fetching API—one for data ingestion
    and another for dataset fetching. The system supports both strongly typed schema
    datasets (text and image types) and nonschema datasets (`GENERIC` type).'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6中的中心大框展示了我们的样本数据集管理服务的整体设计。它有一个内部数据集存储系统，以及两个面向公众的接口：一个数据摄取API和一个数据集检索API——一个用于数据摄取，另一个用于数据集检索。该系统支持强类型模式数据集（文本和图像类型）以及非模式数据集（`GENERIC`类型）。
- en: '![](../Images/02-07.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-07.png)'
- en: Figure 2.7 The internal storage structure for storing a dataset
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 存储数据集的内部存储结构
- en: Figure 2.7 displays the overall data structure the sample DM service uses to
    store a dataset. The commits are created by the data ingestion API, and versioned
    snapshots are created by the data fetching API. The concepts of commit and versioned
    snapshot are introduced to address the dynamic and static nature of a dataset.
    We will discuss storage in detail in section 2.2.5.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7显示了样本DM服务存储数据集所使用的整体数据结构。提交是由数据摄取API创建的，而版本化快照是由数据检索API创建的。提交和版本化快照的概念被引入以应对数据集的动态和静态特性。我们将在2.2.5节中详细讨论存储。
- en: In the remaining subsections, we will walk you through every detail of the previous
    two diagrams, component by component. We first start with the API and then move
    to the internal storage and data schema.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的小节中，我们将逐个细节地引导你了解前两个图，从组件到组件。我们首先从API开始，然后转向内部存储和数据模式。
- en: 2.2.3 Data ingestion API
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.3 数据摄取API
- en: The data ingestion API allows creating, updating, and querying datasets in the
    sample dataset management service. The gray box in figure 2.8 shows the definition
    of four service methods in the data ingestion layer that support ingesting data
    into DM. Their names are self-explanatory; let’s look at their gRPC method definition
    in listing 2.6.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取API允许在样本数据集管理服务中创建、更新和查询数据集。图2.8中的灰色框显示了数据摄取层支持将数据摄取到DM的四个服务方法的定义。它们的名字具有自解释性；让我们看看它们在列表2.6中的gRPC方法定义。
- en: '![](../Images/02-08.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-08.png)'
- en: 'Figure 2.8 Four methods to support data ingestion: create the dataset, update
    the dataset, get the dataset summary, and list the datasets'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 支持数据摄取的四种方法：创建数据集、更新数据集、获取数据集摘要和列出数据集
- en: Note To reduce boilerplate code, we chose gRPC to implement the public interface
    for our sample DM service. This doesn’t mean gRPC is the best approach for a dataset
    management service, but compared to the RESTful interface, gRPC’s concise coding
    style is perfect for demonstrating our idea without exposing you to unnecessary
    Spring Framework details.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了减少样板代码，我们选择了gRPC来实现我们样本DM服务的公共接口。这并不意味着gRPC是数据集管理服务的最佳方法，但与RESTful接口相比，gRPC简洁的编码风格非常适合展示我们的想法，而不会让你接触到不必要的Spring框架细节。
- en: Definition of data ingestion methods
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄取方法定义
- en: Let's take a look at what our sample data ingestion API looks like.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的样本数据摄取API的样子。
- en: Listing 2.6 Data ingestion API service definition
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 数据摄取API服务定义
- en: '[PRE10]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Defines dataset type, "TEXT_INTENT" or "GENERIC"
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义数据集类型，"TEXT_INTENT"或"GENERIC"
- en: ❷ Defines the file URL of the uploading data in MinIO server
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义上传数据的文件URL在MinIO服务器上
- en: ❸ Sets data filter by using tags
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用标签设置数据过滤器
- en: Note The topic of data deletion and modification is not covered in this sample
    service, but the service can be easily extended to support them.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：数据删除和修改的话题未在本样本服务中涵盖，但该服务可以轻松扩展以支持这些功能。
- en: Data URL vs. data streaming
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 数据URL与数据流
- en: 'You may notice in our API design that we require users to provide data URLs
    as raw data input instead of uploading files directly to our service. In section
    2.2.4, we also choose to return data URLs as a training dataset instead of returning
    files directly via a streaming endpoint. The main reason is that we want to offload
    the file-transferring work to a cloud object storage service, such as Amazon S3
    or Azure Blob. Doing this has two benefits: first, it saves network bandwidth
    because there are no actual files passed between client and service, and second,
    it reduces code complexity because keeping data streaming working with high availability
    can be complicated when files are large and API usage is high.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到在我们的API设计中，我们要求用户提供数据URL作为原始数据输入，而不是直接将文件上传到我们的服务。在第2.2.4节中，我们也选择返回数据URL作为训练数据集，而不是通过流式端点直接返回文件。主要原因是我们希望将文件传输工作卸载到云对象存储服务，如Amazon
    S3或Azure Blob。这样做有两个好处：首先，它节省了网络带宽，因为客户端和服务之间没有实际文件传递；其次，它减少了代码复杂性，因为当文件很大且API使用率高时，保持数据流的高可用性可能会很复杂。
- en: Creating a new dataset
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个新的数据集
- en: Let’s look at how the gRPC `CreateDataset` method is implemented. Before calling
    the DM (c`reateDataset` API) to create a dataset, the user (Jianguo) needs to
    prepare a downloadable URL for the data they want to upload (steps 1 and 2); the
    URL can be a downloadable link in a cloud object storage service, like Amazon
    S3 or Azure Blob. In our sample service, we use the MinIO server to run on your
    local to mock Amazon S3\. Jianguo also can name the dataset and assign tags in
    the dataset creation request. Listing 2.7 highlights the key pieces of code (`dataManagement/DataManagementService
    .java`) that implement the workflow pictured in figure 2.9.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看gRPC `CreateDataset`方法的实现。在调用DM（`createDataset` API）创建数据集之前，用户（Jianguo）需要准备一个他们想要上传的数据的可下载URL（步骤1和2）；该URL可以是云对象存储服务中的可下载链接，如Amazon
    S3或Azure Blob。在我们的示例服务中，我们使用MinIO服务器在本地运行来模拟Amazon S3。Jianguo还可以在数据集创建请求中命名数据集并分配标签。列表2.7突出了实现图2.9中所示工作流程的关键代码片段（`dataManagement/DataManagementService.java`）。
- en: Listing 2.7 New dataset creation implementation
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 新数据集创建实现
- en: '[PRE11]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Receives dataset creation request (step 3)
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接收数据集创建请求（步骤3）
- en: ❷ Creates a dataset object with metadata from user request (step 4a)
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 根据用户请求创建一个包含元数据的dataset对象（步骤4a）
- en: ❸ Downloads data from URL and uploads it to DM’s cloud storage (step 4b)
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从URL下载数据并将其上传到DM的云存储（步骤4b）
- en: ❹ Saves the dataset with downloaded data as the initial commit (step 5)
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将下载的数据集作为初始提交（步骤5）保存
- en: ❺ Returns the dataset summary to the client (steps 6 and 7)
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据集摘要返回给客户端（步骤6和7）
- en: '![](../Images/02-09.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图2.9](../Images/02-09.png)'
- en: 'Figure 2.9 A high-level overview of the seven steps to creating a new dataset:
    (1) upload data to the cloud object storage; (2) get a data link; (3) call `createDataset`
    API with a data link as the payload; (4) DM first downloads data from the data
    link and then finds the right dataset transformer (`IntentTextTransformer``)`
    to do data parsing and conversion; (5) DM saves the transformed data; and (6 and
    7) DM returns the dataset summary (ID, commit history, data statistics) to the
    user.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 创建新数据集的七个步骤的高级概述：（1）将数据上传到云对象存储；（2）获取数据链接；（3）使用数据链接作为有效负载调用`createDataset`
    API；（4）DM首先从数据链接下载数据，然后找到正确的数据集转换器（`IntentTextTransformer`）进行数据解析和转换；（5）DM保存转换后的数据；（6和7）DM将数据集摘要（ID、提交历史、数据统计）返回给用户。
- en: The implementation details of `DatasetIngestion.ingest()` will be discussed
    in section 2.2.5.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '`DatasetIngestion.ingest()`方法的实现细节将在第2.2.5节中讨论。'
- en: Updating an existing dataset
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 更新现有数据集
- en: Deep learning model development is a continuous process. Once we create a dataset
    for a model training project, data engineers (like Jianguo) will keep adding data
    to it. To accommodate this need, we provide the `UpdateDataset` API.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习模型开发是一个持续的过程。一旦我们为模型训练项目创建了一个数据集，数据工程师（如Jianguo）会持续向其中添加数据。为了满足这一需求，我们提供了`UpdateDataset`
    API。
- en: To use the `UpdateDataset` API, we need to prepare a data URL for the new data.
    We can also pass in a commit message and some customer tags to describe the data
    change; these metadata are useful for data history tracking and data filtering.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`UpdateDataset` API，我们需要为新数据准备一个数据URL。我们还可以传递一个提交消息和一些客户标签来描述数据变更；这些元数据对于数据历史跟踪和数据过滤很有用。
- en: The dataset update workflow is almost identical to the dataset creation workflow
    (figure 2.9). It creates a new commit with the given data and appends the commit
    to the dataset’s commit list. The only difference is that the dataset update workflow
    won’t create a new dataset but will work on an existing dataset. See the following
    code listing.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集更新工作流程几乎与数据集创建工作流程（图2.9）相同。它创建一个新的提交并附加到数据集的提交列表中。唯一的区别是数据集更新工作流程不会创建新的数据集，而是将工作在现有数据集上。请参阅以下代码列表。
- en: Note Because every dataset update is saved as a commit, we could easily remove
    or soft delete those commits with some dataset management API if Jianguo mistakenly
    uploads some mislabeled data to a dataset. Because of space limitations, these
    management APIs are not discussed.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于每个数据集更新都保存为一个提交，因此我们可以轻松地使用一些数据集管理API删除或软删除那些提交。由于空间限制，这些管理API没有讨论。
- en: Listing 2.8 Dataset update implementation
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 数据集更新实现
- en: '[PRE12]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Receives dataset creation request (step 3)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 接收数据集创建请求（步骤3）
- en: ❷ Finds the existing dataset and creates a new commit object (step 4a)
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 找到现有数据集并创建一个新的提交对象（步骤4a）
- en: We will talk more about the concept of commits in section 2.2.3\. For now, you
    just need to be aware that every dataset update request creates a new commit object.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在2.2.3节中更多地讨论提交的概念。现在，您只需知道每个数据集更新请求都会创建一个新的提交对象。
- en: 'Note Why save data updates in commits? Can we merge the new data with the current
    data so we only store the latest state? In our update dataset implementation,
    we create a new commit every time the `UpdateDataset` API is called. There are
    two reasons we want to avoid an in-place data merge: first, an in-place data merge
    can cause irreversible data modification and silent data loss. Second, to reproduce
    the training dataset used in the past, we need to make sure the data batches DM
    receives are stored immutably because they are the source data we used to create
    the training dataset at any time.'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为什么在提交中保存数据更新？我们能否将新数据与当前数据合并，只存储最新状态？在我们的更新数据集实现中，每次调用`UpdateDataset` API时，我们都会创建一个新的提交。我们想要避免原地数据合并的两个原因：首先，原地数据合并可能导致不可逆的数据修改和静默数据丢失。其次，为了重现过去使用的训练数据集，我们需要确保DM接收到的数据批次是存储不可变的，因为它们是我们用于创建训练数据集的源数据。
- en: List datasets and get datasets summary
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 列出数据集和获取数据集概览
- en: 'Besides `CreateDataset` and `UpdateDataset` API, our users need methods to
    list existing datasets and query the overview of a dataset, such as the number
    of a dataset’s examples and labels and its audit history. To accommodate these
    needs, we build two APIs: `ListDatasets` and `GetDatasetSummary`. The first one
    can list all the existing datasets, and the second one provides detailed information
    about a dataset, such as commit history, example and label count, and dataset
    ID and type. The implementation for these two APIs is straightforward; you can
    find them in our Git repo (`miniAutoML/DataManagementService.java)`.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `CreateDataset` 和 `UpdateDataset` API，我们的用户还需要列出现有数据集和查询数据集概览的方法，例如数据集的示例数量和标签以及其审计历史。为了满足这些需求，我们构建了两个API：`ListDatasets`
    和 `GetDatasetSummary`。第一个可以列出所有现有数据集，第二个提供有关数据集的详细信息，例如提交历史、示例和标签计数以及数据集ID和类型。这两个API的实现很简单；您可以在我们的Git仓库（`miniAutoML/DataManagementService.java`）中找到它们。
- en: 2.2.4 Training dataset fetching API
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.4 训练数据集获取API
- en: In this section, we will look at the dataset fetching layer, which is highlighted
    as a gray box in figure 2.10\. To build training data, we designed two APIs. The
    data scientist (Julia) first calls the `PrepareTrainingDataset` API to issue a
    training data preparation request; our DM service will kick off a background thread
    to start building the training data and return a version string as a reference
    handle for the training data. Next, Julia can call the `FetchTrainingDataset`
    API to obtain the training data if the background thread is completed.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将查看数据集获取层，这在图2.10中被突出显示为灰色框。为了构建训练数据，我们设计了两个API。数据科学家（Julia）首先调用`PrepareTrainingDataset`
    API来发出训练数据准备请求；我们的DM服务将启动一个后台线程开始构建训练数据，并返回一个版本字符串作为训练数据的引用句柄。接下来，Julia可以调用`FetchTrainingDataset`
    API来获取训练数据，如果后台线程已完成。
- en: '![](../Images/02-10.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图2.10](../Images/02-10.png)'
- en: 'Figure 2.10 Two methods in the dataset fetching layer to support dataset fetching:
    `PrepareTrainingDataset` and `FetchTrainingDataset`'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.10 数据集获取层中的两种方法以支持数据集获取：`PrepareTrainingDataset` 和 `FetchTrainingDataset`
- en: Definition of dataset fetching methods
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集获取方法定义
- en: First, let’s see the gRPC service method definition (`grpc-contract/src/main/proto/`
    `data_management.proto)` for the two dataset fetching methods—`PrepareTrainingDataset`
    and `FetchTrainingDataset`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看两个数据集获取方法——`PrepareTrainingDataset` 和 `FetchTrainingDataset` 的 gRPC
    服务方法定义（`grpc-contract/src/main/proto/` `data_management.proto`）。
- en: Listing 2.9 Training dataset fetching service definition
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 训练数据集获取服务定义
- en: '[PRE13]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Prepares training dataset API
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 准备训练数据集 API
- en: ❷ Fetches training dataset API
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取训练数据集 API
- en: ❸ The payload of dataset preparation API
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 数据集准备 API 的有效载荷
- en: ❹ Specifies which dataset to build training data
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 指定要构建训练数据集的数据集
- en: ❺ Specifies which commit of a dataset to build training data, optional
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 指定构建训练数据集所需的数据集的提交，可选
- en: ❻ Filters data by commit tags, optional
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过提交标签过滤数据，可选
- en: ❼ The payload of the training dataset fetch API
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 训练数据集获取 API 的有效载荷
- en: ❽ Version hash string represents the training dataset snapshot.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 版本哈希字符串代表训练数据集快照。
- en: Why we need two APIs (two steps) to fetch a dataset
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么需要两个 API（两个步骤）来获取数据集
- en: If we only publish one API for acquiring training data, the caller needs to
    wait on the API call until the backend data preparation completes to obtain the
    final training data. If the data preparation takes a long time, this request will
    time out.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只发布一个用于获取训练数据的 API，调用者需要等待 API 调用直到后端数据准备完成以获取最终训练数据。如果数据准备耗时较长，此请求将超时。
- en: A deep learning dataset is normally big (at the gigabyte level); it can take
    minutes or hours to complete the network I/O data transfer and local data aggregation.
    So the common solution to acquiring large data is to offer two APIs—one for submitting
    data preparation requests and another for querying the data status—and pull down
    the result when the request is complete. In this way, the dataset fetching API
    performs consistently regardless of the size of the dataset.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习数据集通常很大（以千兆字节为单位）；完成网络 I/O 数据传输和本地数据聚合可能需要几分钟或几小时。因此，获取大量数据的常见解决方案是提供两个
    API——一个用于提交数据准备请求，另一个用于查询数据状态——并在请求完成时拉取结果。这样，数据集获取 API 的性能不受数据集大小的影响。
- en: Sending the prepare training dataset request
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 发送准备训练数据集请求
- en: Now let’s look at the code workflow of the `PrepareTrainingDataset` API. Figure
    2.11 shows how our sample service handles Julia’s preparation training dataset
    request.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看 `PrepareTrainingDataset` API 的代码工作流程。图 2.11 展示了我们的示例服务如何处理 Julia 的准备训练数据集请求。
- en: '![](../Images/02-11.png)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-11.png)'
- en: 'Figure 2.11 A high-level overview of the eight steps to responding to a dataset
    build request: (1) the user submits a dataset preparation request with data filters;
    (2) DM selects data from commits that satisfy the data filters; (3 and 4) DM generates
    a version string to represent the training data; and (5–8) DM starts a background
    job to produce the training data.'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.11 响应数据集构建请求的八个步骤的高级概述：（1）用户提交带有数据过滤器的数据集准备请求；（2）DM 从满足数据过滤器的提交中选择数据；（3
    和 4）DM 生成一个版本字符串来表示训练数据；（5–8）DM 启动一个后台作业来生成训练数据。
- en: 'When DM receives a dataset preparation request (figure 2.11, step 1), it carries
    out three acts:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 当 DM 收到数据集准备请求（图 2.11，步骤 1），它执行以下三个动作：
- en: Tries to find the dataset in its storage with the given dataset ID.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试使用给定的数据集 ID 在其存储中查找数据集。
- en: Applies the given data filter to select commits from the dataset.
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用给定的数据过滤器从数据集中选择提交。
- en: Creates a `versionedSnapshot` object to track training data in its internal
    storage (`versionHashRegistry)`. The ID of the `versionedSnapshot` object is a
    hash string generated from the selected commits’ ID list.
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建一个 `versionedSnapshot` 对象来跟踪其内部存储中的训练数据（`versionHashRegistry`）。`versionedSnapshot`
    对象的 ID 是从所选提交的 ID 列表中生成的哈希字符串。
- en: The `versionedSnapshot` object is the training dataset Julia wants; it is a
    group of immutable static files from the selected commits. Julia could use the
    hash string (snapshot ID) returned at step 3 to query the dataset preparation
    status and get the data-downloadable URL when the training dataset is ready. With
    this version string, Julia can always obtain the same training data (`versionedSnapshot`)
    from any time in the future, which is how dataset reproducibility is supported.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`versionedSnapshot` 对象是 Julia 想要的训练数据集；它是一组来自所选提交的不可变静态文件。Julia 可以使用步骤 3 返回的哈希字符串（快照
    ID）查询数据集准备状态，并在训练数据集准备就绪时获取可下载的数据 URL。有了这个版本字符串，Julia 可以从未来的任何时间点获取相同的训练数据（`versionedSnapshot`），这就是数据集可重复性的支持方式。'
- en: A side benefit of `versionedSnapshot` is that it can be used as a cache across
    different `PrepareTrainingDataset` API calls. If the snapshot ID—a hash string
    of a list of commits—already exists, we return the existing `versionedSnapshot`
    without rebuilding the same data, which can save computation time and network
    bandwidth.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '`versionedSnapshot` 的一个附带好处是它可以作为不同 `PrepareTrainingDataset` API 调用之间的缓存。如果快照
    ID——提交列表的哈希字符串——已经存在，我们将返回现有的 `versionedSnapshot` 而不是重建相同的数据，这样可以节省计算时间和网络带宽。'
- en: Note In our design, the data filtering happens at the commit level, not at the
    individual example level; for example, having a filter tag `"DataType=Training"`
    in the preparation request indicates that the user only wants data from the commits
    that are labeled `"DataType=Training"`.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在我们的设计中，数据过滤发生在提交级别，而不是在单个示例级别；例如，在准备请求中有一个过滤器标签 `"DataType=Training"` 表示用户只想从标记为
    `"DataType=Training"` 的提交中获取数据。
- en: After step 3, DM will spawn a background thread to build the training dataset.
    In the background job, DM will download the files of each dataset commit from
    the MinIO server to the local, aggregate and compress them into one file in a
    predefined format, and upload them back to the MinIO server in a different bucket
    (steps 6 and 7). Next, DM will put the data URL of the actual training data in
    the `versionedSnapshot` object and update its status to `"READY"` (step 8). Now
    Julia can find the data URLs from the returned `versionedSnapshot` object and
    start to download the training data.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在步骤 3 之后，DM 将启动一个后台线程来构建训练数据集。在后台任务中，DM 将从 MinIO 服务器下载每个数据集提交的文件到本地，将它们聚合并压缩成一个预定义格式的文件，然后将它们上传回
    MinIO 服务器上的不同存储桶（步骤 6 和 7）。接下来，DM 将实际训练数据的数据 URL 放入 `versionedSnapshot` 对象中，并更新其状态为
    `"READY"`（步骤 8）。现在 Julia 可以从返回的 `versionedSnapshot` 对象中找到数据 URL 并开始下载训练数据。
- en: What we haven’t covered is the data schema. In the dataset management service,
    we save the ingested data (`commit`) and the generated training data (`versionedSnapshot`)
    in two different data formats. A data merge operation (figure 2.11, steps 6 and
    7) aggregates the raw ingested data (the selected commits) and converts it into
    training data in an intent classification training data schema. We will discuss
    data schemas in detail in section 2.2.6\. Listing 2.10 highlights the code implemented
    for figure 2.11.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还没有涉及数据模式。在数据集管理服务中，我们将摄入的数据（`commit`）和生成的训练数据（`versionedSnapshot`）保存为两种不同的数据格式。数据合并操作（图
    2.11，步骤 6 和 7）将原始摄入数据（选定的提交）聚合，并将其转换为意图分类训练数据模式中的训练数据。我们将在 2.2.6 节中详细讨论数据模式。列表
    2.10 突出了为图 2.11 实现的代码。
- en: Listing 2.10 Preparing training data request API
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 准备训练数据请求 API
- en: '[PRE14]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Creates VersionedSnapshot object to represent the training dataset
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建表示训练数据集的版本化快照对象
- en: Fetching the training dataset
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集
- en: Once the DM service receives a training dataset preparation request on the `prepareTrainingDataset`
    API, it will spawn a background job to build the training data and return a `version_hash`
    string for tracking purposes. Julia can use the `FetchTrainingDataset` API and
    the `version_hash` string to query the dataset-building progress and eventually
    get the training dataset. Figure 2.12 shows how dataset fetching requests are
    handled in DM.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 DM 服务在 `prepareTrainingDataset` API 上收到训练数据集准备请求，它将启动一个后台任务来构建训练数据，并返回一个用于跟踪的
    `version_hash` 字符串。Julia 可以使用 `FetchTrainingDataset` API 和 `version_hash` 字符串来查询数据集构建进度，并最终获取训练数据集。图
    2.12 展示了 DM 中如何处理数据集获取请求。
- en: '![](../Images/02-12.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-12.png)'
- en: 'Figure 2.12 A high-level overview of the three steps to serving a dataset fetching
    request: (1) the user calls the `FetchTrainingDataset` API with a dataset ID and
    a version string; (2) DM will search the `versionHashRegistry` of the dataset
    in its internal storage and return a `versionedSnapshot` object; and (3) the `versionedSnapshot`
    object will have a download URL when the data preparation job is completed.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.12 三个步骤的高级概述，用于处理数据集获取请求：（1）用户使用数据集 ID 和版本字符串调用 `FetchTrainingDataset` API；（2）DM
    将在其内部存储中搜索数据集的 `versionHashRegistry` 并返回一个 `versionedSnapshot` 对象；（3）当数据准备作业完成时，`versionedSnapshot`
    对象将具有下载 URL。
- en: The fetch training dataset is essentially querying the training data preparation
    request status. For each dataset, the DM service creates a `versionedSnapshot`
    object to track each training dataset produced by the `prepareTrainingDataset`
    request.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 获取训练数据集本质上是在查询训练数据准备请求的状态。对于每个数据集，DM 服务创建一个 `versionedSnapshot` 对象来跟踪 `prepareTrainingDataset`
    请求产生的每个训练数据集。
- en: When a user sends a fetch dataset query, we simply use the hash string in the
    request to search its corresponding `versionedSnapshot` object in the dataset’s
    training snapshots (`versionHashRegistry`) and return it to the user if it exists.
    The `versionedSnapshot` object will keep being updated by the background training
    data process job (figure 2.11, steps 5–8). When the job completes, it will write
    the training data URL to the `versionedSnapshot` object; therefore, the user gets
    the training data at the end. See the code implementation in the following listing.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 当用户发送获取数据集查询时，我们只需使用请求中的哈希字符串在数据集的训练快照（`versionHashRegistry`）中搜索其对应的`versionedSnapshot`对象，如果存在则将其返回给用户。`versionedSnapshot`对象将由后台训练数据过程作业持续更新（图2.11，步骤5–8）。当作业完成时，它将训练数据URL写入`versionedSnapshot`对象；因此，用户最终获得训练数据。请参见以下列表中的代码实现。
- en: Listing 2.11 Preparing the training data request API
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.11 准备训练数据请求API
- en: '[PRE15]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Searches versionedSnapshot in a dataset’s training snapshots
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在数据集的训练快照中搜索versionedSnapshot
- en: ❷ Returns versionedSnapshot; it contains the latest progress of dataset preparation.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回versionedSnapshot；它包含数据集准备的最新进度。
- en: 2.2.5 Internal dataset storage
  id: totrans-243
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.5 内部数据集存储
- en: The internal storage of the sample service is simply a list of in-memory dataset
    objects. Earlier we talked about how a dataset can be both dynamic and static.
    On one hand, a dataset is a logical file group, changing dynamically as it continuously
    absorbs new data from a variety of sources. On the other hand, it’s static and
    reproducible for training.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 样式服务的内部存储只是一个内存中的数据集对象列表。我们之前讨论了数据集可以是动态的也可以是静态的。一方面，数据集是一个逻辑文件组，随着它从各种来源持续吸收新数据而动态变化。另一方面，对于训练来说，它是静态的且可重复的。
- en: 'To showcase this concept, we design each dataset containing a list of commits
    and a list of versioned snapshots. A commit represents the dynamically ingested
    data: data added by a data ingestion call (`CreateDataset` or `UpdateDataset`);
    a commit also has tags and messages for annotation purposes. A versioned snapshot
    represents the static training data, which, produced by the prepare training dataset
    request (`PrepareTrainingDataset`), is converted from a list of selected commits.
    Each snapshot is associated with a version; once the training dataset is built,
    you can use this version string to fetch the corresponding training data (`FetchTrainingDataset`)
    at any time to reuse. Figure 2.13 visualizes the internal storage structure of
    a dataset.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这个概念，我们设计每个数据集包含一个提交列表和一个版本化快照列表。一个提交代表动态摄取的数据：通过数据摄取调用（`CreateDataset`或`UpdateDataset`）添加的数据；提交还具有标签和消息用于注释目的。一个版本化快照代表静态训练数据，它由准备训练数据集请求（`PrepareTrainingDataset`）产生，并从所选提交列表转换而来。每个快照都与一个版本相关联；一旦训练数据集构建完成，您就可以使用这个版本字符串在任何时候获取相应的训练数据（`FetchTrainingDataset`）以进行重用。图2.13展示了数据集的内部存储结构。
- en: '![](../Images/02-13.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![图2.13](../Images/02-13.png)'
- en: 'Figure 2.13 An internal dataset storage overview. A dataset stores two types
    of data: commits for the ingested raw data and versioned snapshots for the training
    dataset. The dataset metadata and data URLs are stored in the dataset management
    service, and the actual data is stored in the cloud object storage service.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.13 内部数据集存储概览。数据集存储两种类型的数据：摄取的原始数据的提交和用于训练数据集的版本化快照。数据集元数据和数据URL存储在数据集管理服务中，实际数据存储在云对象存储服务中。
- en: Note Although the individual training examples of different types of datasets
    can be in different forms, such as images, audios, and text sentences, the dataset’s
    operations (creating, updating, and querying dataset summary) and its dynamic/static
    characters are the same. Because we designed a unified API set across all dataset
    types, we can use one uniformed storage structure to store all different kinds
    of datasets.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：尽管不同类型的数据集的个别训练示例可能具有不同的形式，例如图像、音频和文本句子，但数据集的操作（创建、更新和查询数据集摘要）及其动态/静态特性是相同的。因为我们为所有数据集类型设计了一个统一的API集，所以我们可以使用一个统一的存储结构来存储所有不同类型的数据集。
- en: In our storage, the actual files (commit data, snapshot data) are stored in
    cloud object storage (such as Amazon S3), and we only keep dataset metadata (see
    explanation later) in our DM system. By offloading file storage work and only
    tracking the file links, we can focus on organizing the datasets and tracking
    their metadata, such as edit history, data statistics, training snapshots, and
    ownership.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的存储中，实际文件（提交数据、快照数据）存储在云对象存储（如Amazon S3）中，我们只在我们的DM系统中保留数据集元数据（稍后解释）。通过卸载文件存储工作并仅跟踪文件链接，我们可以专注于组织数据集和跟踪它们的元数据，如编辑历史、数据统计、训练快照和所有权。
- en: Dataset metadata
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集元数据
- en: We define dataset metadata as everything except actual data files, such as the
    dataset ID, data owner, change history (audits), training snapshots, commits,
    data statistics, and so on.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将数据集元数据定义为除了实际数据文件之外的所有内容，例如数据集ID、数据所有者、变更历史（审计）、训练快照、提交、数据统计等。
- en: For demonstration purposes, we store the datasets’ metadata in a memory dictionary
    with the ID as key and put all data files into the MinIO server. But you can extend
    it to use a database or NoSQL database to store the dataset’s metadata.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们将数据集的元数据存储在一个以ID为键的内存字典中，并将所有数据文件放入MinIO服务器。但你可以扩展它以使用数据库或NoSQL数据库来存储数据集的元数据。
- en: So far, we have talked about dataset storage concepts, but how do the actual
    dataset writing and reading work? How do we serialize commits and snapshots for
    different dataset types, such as `GENERIC` and `TEXT_INTENT` types?
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了数据集存储的概念，但实际的读写操作是如何进行的呢？我们如何序列化不同数据集类型（如`GENERIC`和`TEXT_INTENT`类型）的提交和快照？
- en: 'In the storage backend implementation, we use a simple inheritance concept
    to handle file operations for different dataset types. We define a `DatasetTransformer`
    interface as follows: the `ingest()` function saves input data into internal storage
    as a commit, and the `compress()` function merges data from selected commits into
    a version snapshot (training data).'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在存储后端实现中，我们使用简单的继承概念来处理不同数据集类型的文件操作。我们定义了一个`DatasetTransformer`接口，如下所示：`ingest()`函数将输入数据保存到内部存储作为一个提交，而`compress()`函数将选定的提交中的数据合并到一个版本快照（训练数据）中。
- en: More specifically, for the `"TEXT_INTENT"` type dataset, we have `IntentTextTransformer`
    to apply the strong type of file schema on file conversion. For a `"GENERIC"`
    type dataset, we have `GenericTransformer` to save data in the original format
    without any checks or format conversions. Figure 2.14 illustrates these.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，对于`"TEXT_INTENT"`类型的数据集，我们有一个`IntentTextTransformer`来在文件转换上应用强类型文件模式。对于`"GENERIC"`类型的数据集，我们有一个`GenericTransformer`来以原始格式保存数据，而不进行任何检查或格式转换。图2.14展示了这些。
- en: '![](../Images/02-14.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-14.png)'
- en: Figure 2.14 Implement `DatasetTransformer` interface to handle different dataset
    types; implement ingest function to save raw input data as commit; and implement
    compress function to aggregate commit data to training data.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.14 实现`DatasetTransformer`接口以处理不同数据集类型；实现`ingest`函数以保存原始输入数据为提交；并实现`compress`函数以聚合提交数据到训练数据。
- en: From figure 2.14, we see that the raw intent classification data from data ingestion
    API (section 2.2.3) is saved as a commit by `IntentTextTransformer:Ingest()`;
    the intent classification training data produced by training dataset fetching
    API (section 2.2.4) is saved as a versioned snapshot by `IntentTextTransformer:Compress()`.
    Because they are plain Java code, we leave it for your own discovery; you can
    find the implementation code at our Git repo (org/orca3/miniAutoML/dataManagement/
    transformers/IntentTextTransformer.java).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 从图2.14中，我们看到来自数据摄取API（第2.2.3节）的原始意图分类数据通过`IntentTextTransformer:Ingest()`保存为一个提交；由训练数据集获取API（第2.2.4节）产生的意图分类训练数据通过`IntentTextTransformer:Compress()`保存为一个版本化的快照。因为它们是纯Java代码，我们留给你自己探索；你可以在我们的Git仓库（org/orca3/miniAutoML/dataManagement/transformers/IntentTextTransformer.java）中找到实现代码。
- en: 2.2.6 Data schemas
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.6 数据模式
- en: 'So far, we have seen all the APIs, workflows, and internal storage structures.
    Now let’s consider what the data looks like in the DM service. For each kind of
    strongly typed dataset, such as a “`TEXT_INTENT`” dataset, we defined two data
    schemas: one for data ingestion and one for training data fetching (figure 2.15).'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到了所有的API、工作流程和内部存储结构。现在让我们考虑一下DM服务中的数据看起来是什么样子。对于每种强类型数据集，例如“`TEXT_INTENT`”数据集，我们定义了两个数据模式：一个用于数据摄取，一个用于训练数据获取（图2.15）。
- en: '![](../Images/02-15.png)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-15.png)'
- en: 'Figure 2.15 Each type of dataset has two data schemas: ingestion data schema
    and training data schema. These two schemas will ensure that the data we accept
    and the data we produce follow our data spec.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15每种类型的数据集都有两个数据模式：摄入数据模式和解剖数据模式。这两个模式将确保我们接受和产生的数据遵循我们的数据规范。
- en: Figure 2.15 shows how the DM service uses two data schemas to implement its
    data contract. Step 1 uses the ingestion data schema to validate the raw input
    data; step 2 uses the training data schema to convert the raw data to the training
    data format; step 3 saves the converted data as a commit; and step 4 merges the
    selected commits into one versioned snapshot when building a training dataset
    but still obeys the training data schema.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.15展示了DM服务如何使用两个数据模式来实现其数据合约。步骤1使用摄入数据模式验证原始输入数据；步骤2使用训练数据模式将原始数据转换为训练数据格式；步骤3将转换后的数据保存为提交；步骤4在构建训练数据集时将选定的提交合并为一个版本化的快照，但仍遵守训练数据模式。
- en: 'These two different data schemas are the data contract that DM service provides
    to our two different users: Jianguo and Julia. No matter how Jianguo collects
    the data, it needs to be converted to the ingestion data format to insert into
    DM. Alternatively, because DM guarantees that the output training data follows
    the training data schema, Julia feels comfortable consuming the dataset without
    worrying about being affected by the data collection changes made by Jianguo.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个不同的数据模式是DM服务为我们两个不同的用户：Jianguo和Julia提供的数据合约。无论Jianguo如何收集数据，它都需要转换为摄入数据格式以插入DM。或者，由于DM保证输出训练数据遵循训练数据模式，Julia可以放心消费数据集，无需担心受到Jianguo所做的数据收集变化的影响。
- en: A data ingestion schema
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 数据摄入模式
- en: 'We have seen the data schema concept; now let’s look at the ingestion data
    schema we defined for the `TEXT_INTENT` dataset:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了数据模式的概念；现在让我们看看为`TEXT_INTENT`数据集定义的摄入数据模式：
- en: '[PRE16]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'For simplicity, our ingestion data schema requires that all the input data
    for the `TEXT_INTENT` dataset must be in a CSV file format. The first column is
    text utterance, and the remainder of the columns are labels. See a sample CSV
    file as follows:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，我们的摄入数据模式要求`TEXT_INTENT`数据集的所有输入数据必须以CSV文件格式。第一列是文本表述，其余列是标签。以下是一个示例CSV文件：
- en: '[PRE17]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Labels
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 标签
- en: A training dataset schema
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据集模式
- en: 'For `TEXT_INTENT` training data, our schema defines the output data as a zip
    file that contains two files: examples.csv and labels.csv. Labels.csv defines
    a label name to a label ID mapping, and the examples.csv defines training text
    (utterance) to label ID mapping. See the following examples:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`TEXT_INTENT`训练数据，我们的模式定义输出数据为一个包含两个文件：examples.csv和labels.csv的zip文件。labels.csv定义了标签名称到标签ID的映射，而examples.csv定义了训练文本（表述）到标签ID的映射。以下是一些示例：
- en: '[PRE18]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Why we use a self-defined data structure
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们使用自定义的数据结构
- en: We build `TEXT_INTENT` with the self-defined data schema instead of using the
    PyTorch or Tensorflow dataset format (like TFRecordDataset) to create abstraction
    from model training frameworks.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用自定义数据模式构建`TEXT_INTENT`，而不是使用PyTorch或Tensorflow数据集格式（如TFRecordDataset）从模型训练框架中创建抽象。
- en: By choosing a framework-specific dataset format, your training code will also
    need to be written in the framework, which is not ideal. Introducing a self-defined
    intermediate dataset format can make the DM framework-neutral, so no framework-specific
    training codes are required.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 通过选择框架特定的数据集格式，您的训练代码也需要在框架中编写，这并不理想。引入自定义的中间数据集格式可以使DM框架中立，因此不需要框架特定的训练代码。
- en: The benefit of having two strongly typed data schemas in one dataset
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个数据集中拥有两个强类型数据模式的好处
- en: By having two strongly typed data schemas in a dataset and letting DM do the
    data transformation from the ingestion data format to the training data format,
    we could parallelize data collection development and training code development.
    For example, when Jianguo wants to add a new feature—“text language”—to the `TEXT_INTENT`
    dataset, he can work with the DM service developers to update the data ingestion
    schema to add a new data field.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在数据集中拥有两个强类型数据模式，并让DM从摄入数据格式到训练数据格式进行数据转换，我们可以并行化数据收集开发和训练代码开发。例如，当Jianguo想要向`TEXT_INTENT`数据集添加一个新功能——“文本语言”——时，他可以与DM服务开发者合作更新数据摄入模式以添加新的数据字段。
- en: Julia won’t be affected because the training data schema is not changed. Julia
    may come to us later to update the training data schema when she has the bandwidth
    to consume the new feature in her training code. The key point is that Jianguo
    and Julia don’t have to work synchronously to introduce a new dataset enhancement;
    they can work independently.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Julia不会受到影响，因为训练数据模式没有改变。当Julia有空闲时间来消费其训练代码中的新功能时，她可能会后来向我们更新训练数据模式。关键点是Jianguo和Julia不需要同步工作来引入新的数据集增强；他们可以独立工作。
- en: Note For simplicity and demo purpose, we choose to use a CSV file to store data.
    The problem with using plain CSV files is their lack of backward compatibility
    support and data-type validation support. In production, we recommend using Parquet,
    Google protobuf, or Avro to define data schemas and store data. They come with
    a set of libraries for data validation, data serialization, and schema backward-compatible
    support.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了简单和演示目的，我们选择使用CSV文件来存储数据。使用纯CSV文件的问题在于它们缺乏向后兼容性支持和数据类型验证支持。在生产环境中，我们建议使用Parquet、Google
    protobuf或Avro来定义数据模式并存储数据。它们提供了一套用于数据验证、数据序列化和模式向后兼容性支持的库。
- en: 'A generic dataset: A dataset with no schema'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一个通用的数据集：一个没有模式的数据集
- en: Although we emphasize at multiple places that defining strongly typed dataset
    schemas is fundamental to dataset management service, we will make an exception
    here by adding a free format dataset type—the `GENERIC` dataset. Unlike the strongly
    typed TEXT_ INENT dataset, a `GENERIC`-type dataset has no data schema validation.
    Our service will save any raw input data as is, and when building training data,
    the service simply packs all the raw data together in its original format into
    a training dataset.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在多个地方强调定义强类型数据集模式对于数据集管理服务至关重要，但在这里我们将做出例外，添加一个自由格式的数据集类型——`GENERIC`数据集。与强类型的TEXT_
    INENT数据集不同，`GENERIC`类型的数据集没有数据模式验证。我们的服务将保存任何原始输入数据，并在构建训练数据时，将所有原始数据以原始格式打包成一个训练数据集。
- en: A `GENERIC` dataset type may sound like a bad idea because we basically pass
    whatever data we receive from upstream data sources to the downstream training
    application, which can break the data parsing logic in the training code easily.
    This is definitely not an option for production, but it provides the agility necessary
    for experimental projects.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`GENERIC`类型的数据集可能听起来像是一个坏主意，因为我们基本上将我们从上游数据源接收到的任何数据都传递给下游的训练应用程序，这很容易破坏训练代码中的数据解析逻辑。这绝对不是一个适合生产的选项，但它为实验项目提供了必要的灵活性。'
- en: Although a strongly typed data schema offers good data type safety protection,
    it comes at the cost of maintaining it. It is quite annoying when you have to
    make frequent schema changes in the DM service to adopt the new data format required
    by a new experimentation.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管强类型数据模式提供了良好的数据类型安全性保护，但维护它却付出了代价。当你不得不在DM服务中频繁更改模式以适应新实验所需的新数据格式时，这相当令人烦恼。
- en: At the beginning of a deep learning project, a lot of things are uncertain,
    such as which deep learning algorithm works the best, what kind of data we can
    collect, and what data schema we should choose. To move forward with all these
    uncertainties, we need a flexible way to handle arbitrary data to enable model
    training experimentations. This is what `GENERIC` dataset type designs are for.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习项目的初期，许多事情都是不确定的，比如哪种深度学习算法效果最好，我们能收集到什么类型的数据，以及我们应该选择哪种数据模式。为了应对所有这些不确定性，我们需要一种灵活的方式来处理任意数据，以便进行模型训练实验。这正是`GENERIC`数据集类型设计的目的。
- en: Once the business value is proven and the deep learning algorithm is chosen,
    we are now clear about how the training data looks; then it’s time for us to define
    a strongly typed dataset in the dataset management service. In the next section,
    we will discuss how to add a new strongly typed dataset.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦证明了业务价值并选择了深度学习算法，我们现在就清楚训练数据的样子了；那么，我们就需要在数据集管理服务中定义一个强类型数据集。在下一节中，我们将讨论如何添加一个新的强类型数据集。
- en: 2.2.7 Adding new dataset type (IMAGE_CLASS)
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.7 添加新的数据集类型（IMAGE_CLASS）
- en: Let’s imagine one day Julia asks us (the platform developers) to promote her
    experimental image classification project to a formal project. Julia and her team
    is developing an image classification model by using a `GENERIC` dataset, and
    because they get good results, they now want to define a strongly typed dataset
    (`IMAGE_CLASS`) to stabilize the data schema for raw data collection and training
    data consumption. This will protect the training code from future dataset updates.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们设想有一天朱莉娅（平台开发者）要求我们将她的实验性图像分类项目提升为正式项目。朱莉娅及其团队正在使用 `GENERIC` 数据集开发图像分类模型，由于他们取得了良好的成果，他们现在希望定义一个强类型数据集（`IMAGE_CLASS`），以稳定原始数据收集和训练数据消费的数据模式。这将保护训练代码免受未来数据集更新的影响。
- en: 'To add a new dataset type—`IMAGE_CLASS`—we can follow three steps. First, we
    must define the training data format. After discussing with Julia, we decide the
    training data produced by `FetchTrainingDataset` API will be a zip file; it will
    contain these three files:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 要添加新的数据集类型—`IMAGE_CLASS`，我们可以遵循三个步骤。首先，我们必须定义训练数据格式。在与朱莉娅讨论后，我们决定 `FetchTrainingDataset`
    API 生成的训练数据将是一个 zip 文件；它将包含以下三个文件：
- en: '[PRE19]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: The examples.csv and labels.csv files are manifest files that define labels
    for each training image. The actual image files are stored in the examples folder.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: examples.csv 和 labels.csv 文件是定义每个训练图像标签的清单文件。实际的图像文件存储在 examples 文件夹中。
- en: 'Second, define the ingestion data format. We need to discuss the ingestion
    data schema with Jianguo, the data engineer who collects images and labels them.
    We agree that the payload data for each `CreateDataset` and `UpdateDataset` request
    is also a zip file; its directory looks as follows: the zip file should be a folder
    with only subdirectories. Each subdirectory under the root folder represents a
    label; the images under it belong to this label. The subdirectory should only
    contain images and not any nested directories:'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，定义摄入数据格式。我们需要与收集图像并为其标记的工程师江国讨论摄入数据模式。我们同意每个 `CreateDataset` 和 `UpdateDataset`
    请求的负载数据也是一个 zip 文件；其目录如下：zip 文件应该是一个只包含子目录的文件夹。根文件夹下的每个子目录代表一个标签；该子目录下的图像属于此标签。子目录应只包含图像，不包含任何嵌套目录：
- en: '[PRE20]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The last step is the code change. After having two data schemas in mind, we
    need to create an `ImageClassTransformer` class that implements the `DatasetTransformer`
    interface to build the data reads and writes logic.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是代码更改。在心中有了两种数据模式之后，我们需要创建一个 `ImageClassTransformer` 类，该类实现 `DatasetTransformer`
    接口以构建数据读取和写入逻辑。
- en: We first implement the `ImageClassTransformer.ingest()` function. The logic
    needs to use the input data format—defined in step 2—to parse the input data in
    the dataset creation and update requests and then convert the input data to a
    training data format and save it as a commit of the dataset.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先实现 `ImageClassTransformer.ingest()` 函数。逻辑需要使用第 2 步中定义的输入数据格式来解析数据集创建和更新请求中的输入数据，然后将输入数据转换为训练数据格式并保存为数据集的提交。
- en: We then implement the `ImageClassTransformer.compress()` function, which first
    selects commits by matching data filters and then merges the matched commits into
    a single training snapshot. As the last step, we register the `ImageClassTransformer
    .ingest()` function to the `DatasetIngestion.ingestion()` function with an IMAGE_
    CLASS type and register `ImageClassTransformer.compress()` to `DatasetCompressor
    .run()` with an `IMAGE_CLASS` type.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们实现 `ImageClassTransformer.compress()` 函数，该函数首先通过匹配数据过滤器选择提交，然后将匹配的提交合并成一个单独的训练快照。最后一步，我们将
    `ImageClassTransformer.ingest()` 函数注册到 `DatasetIngestion.ingestion()` 函数，并使用 `IMAGE_CLASS`
    类型，将 `ImageClassTransformer.compress()` 注册到 `DatasetCompressor.run()`，并使用 `IMAGE_CLASS`
    类型。
- en: As you can see, with proper dataset structure, we can support new dataset types
    by just adding a few new code snippets. The existing types of datasets and the
    public data ingestion and fetching APIs are not affected.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，通过适当的数据集结构，我们只需添加几个新的代码片段就可以支持新的数据集类型。现有的数据集类型和公共数据摄入和检索 API 不会受到影响。
- en: 2.2.8 Service design recap
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.2.8 服务设计回顾
- en: 'Let’s recap how this sample dataset management service addresses the five design
    principles introduced in section 2.1.2:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下本节 2.1.2 中介绍的五个设计原则是如何通过这个示例数据集管理服务来解决的：
- en: '*Principle 1*—Support dataset reproducibility. Our sample DM service saves
    all the generated training data as a versioned snapshot with a version hash string
    as key. Users can apply this version string to obtain the training data snapshot
    at any time.'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 1*—支持数据集可重复性。我们的示例 DM 服务将所有生成的训练数据保存为带有版本哈希字符串作为键的版本化快照。用户可以在任何时候应用此版本字符串以获取训练数据快照。'
- en: '*Principle 2*—Provide a unified experience across different dataset types.
    The data ingestion API and training data fetching API work the same way for all
    dataset types and sizes.'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 2*—在不同数据集类型之间提供统一的使用体验。数据摄取 API 和训练数据获取 API 对于所有数据集类型和大小都采用相同的工作方式。'
- en: '*Principle 3*—Adopt strongly typed data schema. Our sample TEXT_INENT type
    and `IMAGE_CLASS` type datasets apply a self-defined data schema to both raw ingestion
    data and training data.'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 3*—采用强类型数据模式。我们的示例 TEXT_INENT 类型数据和 `IMAGE_CLASS` 类型数据集对原始摄取数据和训练数据都应用了一个自定义的数据模式。'
- en: '*Principle 4*—Ensure API consistency and handle scaling internally. Although
    we save all datasets’ metadata in memory in our sample code (for simplicity),
    we can easily implement the dataset storage structure in cloud object storage;
    in theory, it has infinite capacity. Also, we require data URLs to send data and
    return data, so no matter how large a dataset is, our API remains consistent.'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 4*—确保 API 一致性并内部处理扩展。尽管在我们的示例代码中（为了简化），我们将所有数据集的元数据保存在内存中，但我们可以轻松地将数据集存储结构实现到云对象存储中；理论上，它具有无限容量。此外，我们要求使用数据
    URL 发送数据和返回数据，因此无论数据集有多大，我们的 API 都保持一致。'
- en: '*Principle 5*—Guarantee data persistency. Every dataset creation request and
    update request creates a new commit; every training data prepare request creates
    a versioned snapshot. Both commit and snapshot are immutable and persist with
    no data expiration limits.'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原则 5*—保证数据持久性。每个数据集创建请求和更新请求都会创建一个新的提交；每个训练数据准备请求都会创建一个版本化快照。提交和快照都是不可变的，并且没有数据过期限制。'
- en: Note We have trimmed many important features from the sample dataset management
    service to keep it simple. Management APIs, for example, allow you to delete data,
    revert data commits, and view data audit history. Feel free to fork the repo and
    try to implement them.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了保持简单，我们从示例数据集管理服务中删除了许多重要功能。例如，管理 API 允许你删除数据、回滚数据提交和查看数据审计历史。请随意克隆仓库并尝试实现它们。
- en: 2.3 Open source approaches
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 开源方法
- en: 'If you are interested in employing open source approaches to set up dataset
    management functionality, we select two approaches for you: Delta Lake and Pachyderm.
    Let’s look at them individually.'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对使用开源方法来设置数据集管理功能感兴趣，我们为你选择了两种方法：Delta Lake 和 Pachyderm。让我们分别看看它们。
- en: 2.3.1 Delta Lake and Petastorm with Apache Spark family
  id: totrans-308
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 Delta Lake 和 Petastorm 与 Apache Spark 家族
- en: In this approach, we propose to save data in a Delta Lake table and use the
    Petastorm library to convert the table data to PyTorch and Tensorflow dataset
    objects. The dataset can be consumed in training code seamlessly.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种方法中，我们建议将数据保存在 Delta Lake 表中，并使用 Petastorm 库将表数据转换为 PyTorch 和 Tensorflow
    数据集对象。数据集可以在训练代码中无缝消费。
- en: Delta Lake
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake
- en: Delta Lake is a storage layer that brings scalable, ACID (atomicity, consistency,
    isolation, durability) transactions to Apache Spark and other cloud object stores
    (e.g., Amazon S3). Delta Lake is developed as open source by Databricks, a respected
    data and AI company.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 是一个存储层，它为 Apache Spark 和其他云对象存储（例如 Amazon S3）带来了可扩展的、ACID（原子性、一致性、隔离性、持久性）事务。Delta
    Lake 由备受尊敬的数据和 AI 公司 Databricks 以开源方式开发。
- en: Cloud storage services, such as Amazon S3, are some of the most scalable and
    cost-effective storage systems in the IT industry. They are ideal places to build
    large data warehouses, but their key-values store design makes it difficult to
    achieve ACID transactions and high performance. The metadata operations such as
    listing objects are expensive, and consistency guarantees are limited.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 云存储服务，如 Amazon S3，是 IT 行业中最可扩展和最具成本效益的存储系统之一。它们是构建大型数据仓库的理想之地，但它们的键值存储设计使得实现
    ACID 事务和高性能变得困难。例如，列出对象的元数据操作成本高昂，一致性保证有限。
- en: Delta Lake is designed to fill the previously discussed gaps. It works as a
    file system that stores batch and streaming data in object storage (such as Amazon
    S3). In addition, Delta Lake manages metadata, caching, and indexing for its table
    structure and schema enforcement. It provides ACID properties, time travel, and
    significantly faster metadata operations for large tabular datasets. See figure
    2.16 for the Delta Lake concept graph.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake旨在填补之前讨论的差距。它作为一个文件系统，在对象存储（如Amazon S3）中存储批量和流数据。此外，Delta Lake管理其表结构和模式执行的数据元数据、缓存和索引。它为大型表格数据集提供ACID属性、时间旅行和显著更快的元数据操作。参见图2.16的Delta
    Lake概念图。
- en: '![](../Images/02-16.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-16.png)'
- en: Figure 2.16 Delta Lake data ingestion and processing workflow. Both stream data
    and batch data can be saved as Delta Lake tables, and the Delta Lake tables are
    stored in the cloud object storage, such as Amazon S3.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.16 Delta Lake数据摄取和处理工作流程。流数据批数据都可以保存为Delta Lake表，Delta Lake表存储在云对象存储中，如Amazon
    S3。
- en: The Delta Lake table is the core concept of the system. When working with Delta
    Lake, you are usually dealing with Delta Lake tables. They are like SQL tables;
    you can query, insert, update, and merge table content. Schema protection in Delta
    Lake is one of its advantages. It supports schema validation on table writing,
    which prevents data pollution. It also tracks table history, so you can roll back
    a table to any of its past stages (known as time travel).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake表是系统的核心概念。当与Delta Lake一起工作时，你通常处理的是Delta Lake表。它们类似于SQL表；你可以查询、插入、更新和合并表内容。Delta
    Lake中的模式保护是其优点之一。它支持表写入时的模式验证，从而防止数据污染。它还跟踪表历史，因此你可以将表回滚到其任何过去阶段（称为时间旅行）。
- en: 'For building data processing pipelines, Delta Lake recommends naming your tables
    in three categories: bronze, silver, and gold. First, we use bronze tables to
    keep the raw input from different sources (some of which are not so clean). Then
    the data flows constantly from bronze tables to silver tables with data cleaning
    and transformation (ETL). Finally, we perform data filtering and purification
    and save the results to gold tables. Each table is in a machine learningstate;
    they are reproducible and type-safe.'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 对于构建数据处理管道，Delta Lake建议将您的表命名为三类：青铜、银和金。首先，我们使用青铜表来保存来自不同来源的原始输入（其中一些并不那么干净）。然后，数据不断从青铜表流向银表，进行数据清洗和转换（ETL）。最后，我们执行数据过滤和净化，并将结果保存到金表中。每个表都处于机器学习状态；它们是可重复的且类型安全的。
- en: Why Delta Lake is a good option for deep learning dataset management
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么Delta Lake是深度学习数据集管理的良好选择
- en: The following are three features that make Delta Lake a good option for managing
    datasets for deep learning projects.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 以下三个特性使Delta Lake成为深度学习项目数据集管理的良好选择。
- en: First, Delta Lake supports dataset reproducibility. It has a “time travel” feature
    that has the ability to query the data as it existed at a certain point in time
    using data versioning. Imagine you have set up a continuously running ETL pipeline
    to keep your training dataset (gold table) up to date. Because Delta Lake tracks
    table updates as snapshots, every operation is automatically versioned as the
    pipeline writes into the dataset. This means all the training data snapshots are
    kept for free, and you can browse table update history and roll back to past stages
    easily. The following listing provides a few sample commands.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，Delta Lake支持数据集的可重复性。它具有“时间旅行”功能，能够使用数据版本查询在某个时间点存在的数据。想象一下，你已经设置了一个持续运行的ETL管道来保持你的训练数据集（金表）的最新状态。因为Delta
    Lake将表更新作为快照跟踪，所以每次操作都会自动作为管道写入数据集时进行版本控制。这意味着所有训练数据快照都免费保留，你可以浏览表更新历史并轻松回滚到过去阶段。以下列表提供了一些示例命令。
- en: Listing 2.12 Delta Lake time travel commands
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.12 Delta Lake时间旅行命令
- en: '[PRE21]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Finds the dataset in Delta Lake
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在Delta Lake中查找数据集
- en: ❷ Lists the full history of the data
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 列出数据的完整历史记录
- en: ❸ Gets the last operation on the dataset
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取数据集的最后一个操作
- en: ❹ Rolls back the dataset by time stamp
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过时间戳回滚数据集
- en: ❺ Rolls back dataset by version
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 回滚数据集到指定版本
- en: Second, Delta Lake supports continuously streaming data processing. Its tables
    can handle the continuous flow of data from both historical and real-time streaming
    sources seamlessly. For example, your data pipeline or stream data source can
    keep adding data to the Delta Lake table while querying data from the table at
    the same time. This saves you extra steps when writing code to merge the new data
    with existing data.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，Delta Lake支持持续流式数据处理。其表可以无缝处理来自历史和实时流源的数据连续流动。例如，您的数据管道或流数据源可以持续向Delta Lake表添加数据，同时从表中查询数据。这为您在编写代码时合并新数据与现有数据节省了额外的步骤。
- en: Third, Delta Lake offers schema enforcement and evolvement. It applies schema
    validation on write. It will ensure that new data records match the table’s predefined
    schema; if the new data isn’t compatible with the table’s schema, Delta Lake will
    raise an exception. Having data type validation at writing time is better than
    at reading time because it’s difficult to clean data if it’s polluted.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 第三，Delta Lake提供了模式强制和演变。它在写入时应用模式验证。它将确保新数据记录与表的预定义模式匹配；如果新数据与表的模式不兼容，Delta
    Lake将引发异常。在写入时进行数据类型验证比在读取时更好，因为如果数据被污染，清理数据就变得困难。
- en: Besides strong schema enforcement, Delta Lake also allows you to add new columns
    to existing data tables without causing breaking changes. The dataset schema enforcement
    and adjustment (evolvement) capabilities are critical to deep learning projects.
    These capabilities protect the training data from being polluted by unintended
    data writes and offer safe data updates.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 除了强大的模式强制外，Delta Lake还允许您在不引起破坏性更改的情况下向现有数据表添加新列。数据集模式强制和调整（演变）能力对于深度学习项目至关重要。这些能力保护训练数据免受意外数据写入的污染，并提供了安全的数据更新。
- en: Petastorm
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm
- en: Petastorm is an open source data access library developed at Uber ATG (Advanced
    Technologies Group). It enables single-machine or distributed training and evaluation
    of deep learning models directly from datasets in the Apache Parquet format (a
    data file format designed for efficient data storage and retrieval).
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm是Uber ATG（高级技术集团）开发的开源数据访问库。它使您能够直接从Apache Parquet格式（一种为高效数据存储和检索而设计的文件格式）的数据集中进行单机或分布式深度学习模型的训练和评估。
- en: Petastorm can convert Delta Lake tables to Tensorflow and PyTorch format datasets
    easily, and it also supports distributed training data partitions. With Petastorm,
    the training data from a Delta Lake table can be simply consumed by downstream
    training applications without worrying about the details of data conversion for
    a specific training framework. It also creates good isolation between the dataset
    format and training frameworks (Tensorflow, PyTorch, and PySpark). Figure 2.17
    visualizes the data conversion process.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: Petastorm可以轻松地将Delta Lake表转换为Tensorflow和PyTorch格式数据集，并且它还支持分布式训练数据分区。使用Petastorm，Delta
    Lake表中的训练数据可以简单地被下游训练应用程序消费，无需担心特定训练框架的数据转换细节。它还在数据集格式和训练框架（Tensorflow、PyTorch和PySpark）之间创建了良好的隔离。图2.17可视化了数据转换过程。
- en: '![](../Images/02-17.png)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图2.17](../Images/02-17.png)'
- en: Figure 2.17 Petastorm converts the Delta Lake table to datasets that can be
    read by the PyTorch or Tensorflow framework.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17 Petastorm将Delta Lake表转换为PyTorch或Tensorflow框架可读取的数据集。
- en: Figure 2.17 depicts the Petastorm data conversion workflow. You can create a
    Petastorm spark converter that reads Delta Lake tables into its cache as Parquet
    files and generates Tensorflow or Pytorch dataset.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.17展示了Petastorm数据转换工作流程。您可以创建一个Petastorm Spark转换器，该转换器将Delta Lake表读取到其缓存中作为Parquet文件，并生成Tensorflow或Pytorch数据集。
- en: 'Example: Preparing training data for a flower image classification'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：为花卉图像分类准备训练数据
- en: Now that we have a general idea of Delta Lake and Petastorm, let’s see a concrete
    model training example. The following code snippets—code listings 2.13 and 2.14—demonstrate
    an end-to-end image classification model training workflow in two steps. First,
    they define an image process ETL pipeline that parses a group of image files into
    the Delta Lake table as an image dataset. Second, they use Petastorm to convert
    the Delta Lake table to a dataset that can be loaded into the PyTorch framework
    directly to start model training.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们对Delta Lake和Petastorm有了大致的了解，让我们看看一个具体的模型训练示例。以下代码片段——代码列表2.13和2.14——展示了两个步骤的端到端图像分类模型训练工作流程。首先，它们定义了一个图像处理ETL管道，将一组图像文件解析为Delta
    Lake表中的图像数据集。其次，它们使用Petastorm将Delta Lake表转换为可以直接加载到PyTorch框架中以开始模型训练的数据集。
- en: Let’s first visit the four-step ETL data processing pipeline in code listing
    2.13\. You can also find the complete code at [http://mng.bz/JVPz](http://mng.bz/JVPz).
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看代码列表 2.13 中的四步 ETL 数据处理管道。您也可以在 [http://mng.bz/JVPz](http://mng.bz/JVPz)
    找到完整的代码。
- en: In the beginning step of the pipeline, we load the images from a folder, `flower_`
    `photos`, to spark as binary files. Second, we define the extract functions to
    obtain metadata from each image file, such as label name, file size, and image
    size. Third, we construct the data processing pipeline with the extract functions
    and then pass the image files to the pipeline, which will produce a data frame.
    Each row of the data frame represents an image file and its metadata, including
    file content, label name, image size, and file path. In the last step, we save
    this data frame as a Delta Lake table—`gold_table_training_dataset`. You can also
    see this Delta Lake table’s data schema at the end of the following code listing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在管道的初始步骤中，我们从文件夹中加载图像到 `flower_photos`，作为二进制文件。其次，我们定义提取函数以从每个图像文件中获取元数据，例如标签名称、文件大小和图像大小。第三，我们使用提取函数构建数据处理管道，然后将图像文件传递到管道中，这将生成一个数据框。数据框的每一行代表一个图像文件及其元数据，包括文件内容、标签名称、图像大小和文件路径。最后一步，我们将这个数据框保存为
    Delta Lake 表格——`gold_table_training_dataset`。您也可以在以下代码列表的末尾看到这个 Delta Lake 表格的数据模式。
- en: Listing 2.13 An ETL to create an image dataset in Delta Lake
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.13 创建 Delta Lake 中的图像数据集的 ETL
- en: '[PRE22]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Reads images as binaryFile
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以二进制文件读取图像
- en: ❷ Extracts labels from the image’s subdirectory name
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从图像子目录名称中提取标签
- en: ❸ Extracts image dimensions
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取图像尺寸
- en: ❹ Data schema of the Delta Lake table—gold_table_training_dataset
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Delta Lake 表格的数据模式——gold_table_training_dataset
- en: Note The raw data used in the demo is the flowers dataset from the TensorFlow
    team. It contains flower photos stored under five subdirectories, one per class.
    The subdirectory name is the label name for the images it contains.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在演示中使用的原始数据是 TensorFlow 团队提供的花朵数据集。它包含存储在五个子目录下的花朵照片，每个子目录对应一个类别。子目录的名称是该目录包含图像的标签名称。
- en: 'Now that we have an image dataset built in a Delta Lake table, we can start
    to train a PyTorch model by using this dataset with the help of Petastorm. In
    code listing 2.14, we first read the Delta Lake table `gold_table_training_dataset`
    produced by the ETL pipeline defined in code listing 2.13 and then split the data
    into two data frames: one for training and one for validation. Next, we load these
    two data frames to two Petastorm spark converters; the data will be converted
    to Parquet files inside the converter. At the end, we use the Petastorm API `make_torch_dataloader`
    to read training examples in PyTorch for model training. See the following code
    for the entire three-step process. You can also find the full sample code at:
    [http://mng.bz/wy4B](http://mng.bz/wy4B).'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在 Delta Lake 表格中构建了一个图像数据集，我们可以开始使用 Petastorm 通过这个数据集来训练一个 PyTorch 模型。在代码列表
    2.14 中，我们首先读取由代码列表 2.13 中定义的 ETL 管道生成的 Delta Lake 表格 `gold_table_training_dataset`，然后将数据拆分为两个数据框：一个用于训练，一个用于验证。接下来，我们将这两个数据框加载到两个
    Petastorm Spark 转换器中；数据将在转换器内部转换为 Parquet 文件。最后，我们使用 Petastorm API `make_torch_dataloader`
    读取 PyTorch 中的训练示例以进行模型训练。以下代码展示了整个三个步骤的过程。您也可以在：[http://mng.bz/wy4B](http://mng.bz/wy4B)
    找到完整的示例代码。
- en: Listing 2.14 Consuming a Delta Lake image dataset in PyTorch with Petastorm
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.14 使用 Petastorm 在 PyTorch 中消耗 Delta Lake 图像数据集
- en: '[PRE23]'
  id: totrans-350
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '❶ Splits Delta Lake table data into two data frames: training and validation'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 Delta Lake 表格数据拆分为两个数据框：训练和验证
- en: ❷ Creates the PyTorch data loader from the Petastorm converter for training
    and evaluation
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从 Petastorm 转换器创建用于训练和评估的 PyTorch 数据加载器
- en: ❸ Consumes the training data in the training iterations
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在训练迭代中消耗训练数据
- en: When to use Delta Lake
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用 Delta Lake
- en: The common misconception about Delta Lake is that it can only handle structured
    text data, such as sales records and user profiles. But the previous example shows
    it can also deal with unstructured data like images and audio files; you can write
    the file content as a bytes column into a table with other file properties and
    build datasets from them.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 关于 Delta Lake 的常见误解是它只能处理结构化文本数据，例如销售记录和用户资料。但前面的例子显示它也可以处理非结构化数据，如图像和音频文件；您可以将文件内容作为字节数据列写入一个表格，其中包含其他文件属性，并从它们构建数据集。
- en: Delta Lake is a great choice for doing dataset management if you already use
    Apache Spark to build your data pipeline; it supports both structured and unstructured
    data. It’s also cost-effective because Delta Lake keeps data in cloud object storage
    (e.g., Amazon S3, Azure Blob), and Delta Lake’s data schema enforcement and live
    data updated table support mechanism simplify your ETL pipeline development and
    maintenance. Last but not least, the time travel function keeps track of all the
    table updates automatically, so you can feel safe to make data changes and roll
    back to previous versions of the training dataset.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经使用 Apache Spark 构建数据管道，Delta Lake 是进行数据集管理的绝佳选择；它支持结构化和非结构化数据。它还具有良好的成本效益，因为
    Delta Lake 将数据存储在云对象存储中（例如，Amazon S3、Azure Blob），Delta Lake 的数据模式强制执行和实时数据更新表支持机制简化了您的
    ETL 管道开发和维护。最后但同样重要的是，时间旅行功能会自动跟踪所有表更新，因此您可以放心地进行数据更改并回滚到训练数据集的先前版本。
- en: The limitations of Delta Lake
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: Delta Lake 的局限性
- en: 'The biggest risks of using Delta Lake are lock-in technology and its steep
    learning curve. Delta Lake stores tables in its own mechanism: a combination of
    Parquet-based storage, a transaction log, and indexes, which means it can only
    be written/read by a Delta cluster. You need to use Delta ACID API for data ingestion
    and Delta JDBC to run queries; thus, the data migration cost would be high if
    you decide to move away from Delta Lake in the future. Also, because Delta Lake
    goes with Spark, there is a lot of learning ahead of you if you are new to Spark.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Delta Lake 的最大风险是技术锁定和学习曲线陡峭。Delta Lake 使用自己的机制存储表：基于 Parquet 的存储、事务日志和索引的组合，这意味着它只能由
    Delta 集群写入/读取。您需要使用 Delta ACID API 进行数据摄取，并使用 Delta JDBC 运行查询；因此，如果您决定将来离开 Delta
    Lake，数据迁移成本会很高。此外，由于 Delta Lake 与 Spark 一起使用，如果您是 Spark 新手，那么您面前还有大量的学习内容。
- en: Regarding data ingestion performance, Delta Lake stores data to the underlying
    cloud object store, and it’s difficult to achieve low-latency streaming (millisecond
    scale) when using object store operations, such as table creation and saving.
    In addition, Delta Lake needs to update indexes for each ACID transaction; it
    also introduces latency compared with some ETLs performing append-only data writes.
    But in our opinion, data ingestion latency at the second level is not a problem
    for deep learning projects. If you are unfamiliar with Spark and don’t want the
    heavy lifting of setting up Spark and Delta Lake clusters, we have another lightweight
    approach for you—Pachyderm.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 关于数据摄取性能，Delta Lake 将数据存储到其底层云对象存储中，当使用对象存储操作（如表创建和保存）时，难以实现低延迟流（毫秒级）。此外，Delta
    Lake 需要为每个 ACID 事务更新索引；与一些仅执行追加数据写入的 ETL 相比，它也引入了延迟。但据我们看来，对于深度学习项目来说，第二级的数据摄取延迟并不是问题。如果您不熟悉
    Spark 且不想承担设置 Spark 和 Delta Lake 集群的繁重工作，我们为您提供了另一种轻量级的方法——Pachyderm。
- en: 2.3.2 Pachyderm with cloud object storage
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 使用云对象存储的 Pachyderm
- en: In this section, we want to propose a lightweight, Kubernetes-based tool—Pachyderm—to
    handle dataset management. We will show you two examples of how to use Pachyderm
    to accomplish image data processing and labeling. But before that, let’s look
    at what Pachyderm is.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们希望提出一个基于 Kubernetes 的轻量级工具——Pachyderm，用于处理数据集管理。我们将向您展示如何使用 Pachyderm
    完成图像数据处理和标记的两个示例。但在那之前，让我们看看 Pachyderm 是什么。
- en: Pachyderm
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm
- en: Pachyderm is a tool for building version-controlled, automated, end-to-end data
    pipelines for data science. It runs on Kubernetes and is backed by an object store
    of your choice (e.g., Amazon S3). You can write your own Docker images for data
    scraping, ingestion, cleaning, munging, and wrangling and use the Pachyderm pipeline
    to chain them together. Once you define your pipelines, Pachyderm will handle
    the pipeline scheduling, executing, and scaling.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 是一个用于构建版本控制、自动化、端到端数据管道的工具，适用于数据科学。它运行在 Kubernetes 上，并支持您选择的云对象存储（例如，Amazon
    S3）。您可以编写自己的 Docker 镜像用于数据抓取、摄取、清洗、整理和整理，并使用 Pachyderm 管道将它们串联起来。一旦定义了您的管道，Pachyderm
    将处理管道调度、执行和扩展。
- en: Pachyderm offers dataset version control and provenance (data lineage) management.
    It sees every data update (create, write, delete, etc.) as a commit, and it also
    tracks the data source that generates the data update. So you not only can see
    the change history of a dataset, but you can also roll back the dataset to a past
    version and find the data provenance of the change. Figure 2.18 gives a high-level
    view of how Pachyderm works.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 提供数据集版本控制和溯源（数据血缘）管理。它将每次数据更新（创建、写入、删除等）视为一个提交，并且还跟踪生成数据更新的数据源。因此，您不仅可以查看数据集的变更历史，还可以将数据集回滚到过去版本并找到变更的数据溯源。图
    2.18 给出了 Pachyderm 的工作的高级视图。
- en: '![](../Images/02-18.png)'
  id: totrans-365
  prefs: []
  type: TYPE_IMG
  zh: '![图片 2.18](../Images/02-18.png)'
- en: Figure 2.18 The Pachyderm platform runs with two kinds of objects—a pipeline
    and versioned data. The pipeline is the computational component, and the data
    is the version-control primitive. A data change in the “raw dataset” can trigger
    a pipeline job to process the new data and save the result to the “mature dataset.”
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.18 Pachyderm 平台运行时涉及两种对象——管道和版本化数据。管道是计算组件，数据是版本控制的基本单元。在“原始数据集”中的数据变更可以触发管道作业来处理新数据并将结果保存到“成熟数据集”中。
- en: In Pachyderm, data is version-controlled with a Git style. Each dataset is a
    repository (repo) in Pachyderm, which is the highest-level data object. A repo
    contains commits, files, and branches. Pachyderm only keeps metadata (such as
    audit history and branch) internally and stores the actual files in cloud object
    storage.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 中，数据以 Git 风格进行版本控制。每个数据集在 Pachyderm 中都是一个仓库（repo），这是最高级别的数据对象。仓库包含提交、文件和分支。Pachyderm
    仅在内部保留元数据（如审计历史和分支），并将实际文件存储在云对象存储中。
- en: The Pachyderm pipeline performs various data transformations. The pipelines
    execute a user-defined piece of code—for example, a docker container—to perform
    an operation and process the data. Each of these executions is called a job. Listing
    2.15 shows a simple pipeline definition. This “edges” pipeline watches an “images”
    dataset. When there is a new image added to the images dataset, the pipeline will
    launch a job to run the `"pachyderm/opencv"` docker image to parse the image and
    save its edge picture into the edges dataset.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 管道执行各种数据转换。管道执行用户定义的代码片段——例如，一个 docker 容器——以执行操作并处理数据。这些执行中的每一个都称为一个作业。列表
    2.15 展示了一个简单的管道定义。这个“edges”管道监视一个“images”数据集。当有新的图像添加到 images 数据集时，管道将启动一个作业来运行
    `"pachyderm/opencv"` docker 镜像来解析图像并将边缘图像保存到 edges 数据集中。
- en: Listing 2.15 A Pachyderm pipeline definition
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.15 Pachyderm 管道定义
- en: '[PRE24]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ A Pachyderm pipeline
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Pachyderm 管道
- en: ❷ A Pachyderm dataset
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Pachyderm 数据集
- en: Version and data provenance
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 版本和数据溯源
- en: 'In Pachyderm, any changes applied to both the dataset and pipeline are versioned
    automatically, and you can use the Pachyderm command tool `pachctl` to connect
    to the Pachyderm workspace to check file history and even roll back those changes.
    See the following example for using the `pachctl` command to check the edges dataset’s
    change history and the change provenance. First, we run the `pachctl` `list` command
    to list all the commits in the edges dataset. In our example, there are three
    changes (commits) applied to the edges dataset:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 中，对数据集和管道应用的所有更改都会自动进行版本控制，您可以使用 Pachyderm 命令工具 `pachctl` 连接到 Pachyderm
    工作区以检查文件历史记录，甚至可以回滚这些更改。以下示例展示了如何使用 `pachctl` 命令检查 edges 数据集的变更历史和变更溯源。首先，我们运行
    `pachctl` `list` 命令列出 edges 数据集中的所有提交。在我们的示例中，有三个更改（提交）应用于 edges 数据集：
- en: '[PRE25]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To get the provenance of a data change, we can use pachctl inspect command to
    check on the commit. For example, we can use the following command to check the
    data origin of commit.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取数据变更的溯源，我们可以使用 pachctl inspect 命令检查提交。例如，我们可以使用以下命令来检查提交的数据来源。
- en: '[PRE26]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'From the following response, we can see the commit `eb58294a976347abaf06e35fe3b0da5b`
    of the edges dataset is computed from the images dataset’s `66f4ff89a017412090dc4a542d9b1142`
    commit:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下响应中，我们可以看到 edges 数据集的 `eb58294a976347abaf06e35fe3b0da5b` 提交是由 images 数据集的
    `66f4ff89a017412090dc4a542d9b1142` 提交计算得出的：
- en: '[PRE27]'
  id: totrans-379
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Data provenance
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 数据溯源
- en: The data provenance feature is great for reproducibility and troubleshooting
    datasets, as you can always find the exact data that was used in the past, along
    with the data process code that created it.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 数据溯源功能非常适合数据集的可重复性和故障排除，因为您总能找到过去使用的确切数据，以及创建它的数据处理代码。
- en: 'Example: Using Pachyderm for labeling and training an image dataset'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：使用 Pachyderm 对图像数据集进行标注和训练
- en: Having seen how Pachyderm works, let’s see a design proposal for using Pachyderm
    to build an automated object detection training pipeline. For object detection
    model training, we first need to prepare the training dataset by labeling the
    target object with a bounding box on each image and then send the dataset—the
    bounding box label file and images—to the training code to start the model training.
    Figure 2.19 shows the process of using Pachyderm to automate this workflow.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了 Pachyderm 的工作原理后，让我们看看一个使用 Pachyderm 构建自动化目标检测训练管道的设计方案。对于目标检测模型训练，我们首先需要通过在每个图像上用边界框标记目标对象来准备训练数据集，然后将数据集——边界框标签文件和图像——发送到训练代码以启动模型训练。图
    2.19 展示了使用 Pachyderm 自动化此工作流程的过程。
- en: '![](../Images/02-19.png)'
  id: totrans-384
  prefs: []
  type: TYPE_IMG
  zh: '![图 2.19](../Images/02-19.png)'
- en: Figure 2.19 Automated object detection model training in Pachyderm. The training
    process starts automatically when new images are labeled.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.19 Pachyderm 中的自动化目标检测模型训练。当新图像被标记时，训练过程会自动开始。
- en: In this design, we use two pipelines, the labeling pipeline and the training
    pipeline, and two datasets to build this training workflow. In step 1, we upload
    image files to the “raw image dataset.” In step 2, we kick off the labeling pipeline
    to launch a labeling application that opens up a UI for the user to label objects
    by drawing bounding boxes on the images; these images are read from the raw image
    dataset. Once the user finishes the labeling work, the image and the generated
    label data will be saved to the “labeled dataset.” In step 3, we add new training
    data to the labeled dataset, which will trigger the training pipeline to launch
    the training container and start the model training. In step 4, we save the model
    file.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个设计中，我们使用两个管道，即标记管道和训练管道，以及两个数据集来构建这个训练工作流程。在第 1 步中，我们将图像文件上传到“原始图像数据集”。在第
    2 步中，我们启动标记管道以启动一个标记应用程序，该应用程序为用户打开一个 UI，用户可以在图像上绘制边界框来标记对象；这些图像是从原始图像数据集中读取的。一旦用户完成标记工作，图像和生成的标签数据将被保存到“标记数据集”中。在第
    3 步中，我们将新的训练数据添加到标记数据集中，这将触发训练管道启动训练容器并开始模型训练。在第 4 步中，我们保存模型文件。
- en: Besides the automation, data including the raw image dataset, the labeled dataset,
    and the model files are all versioned by Pachyderm automatically. Also, by leveraging
    the data provenance feature, we can tell with any given model file which version
    of the labeled dataset is used in its training and from which version of the raw
    image dataset this training data is made.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 除了自动化之外，包括原始图像数据集、标记数据集和模型文件在内的所有数据都由 Pachyderm 自动进行版本控制。此外，通过利用数据溯源功能，我们可以确定任何给定的模型文件在训练中使用了哪个版本的标记数据集，以及这些训练数据是从哪个版本的原始图像数据集中生成的。
- en: When to use Pachyderm
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 何时使用 Pachyderm
- en: Pachyderm is a lightweight approach to help you build data engineering pipelines
    easily and offers data versioning support in Git style. It is data scientist–centric
    and easy to use. Pachyderm is Kubernetes based and uses cloud object storage as
    a data store, so it’s cost-effective, simple to set up, and easy to maintain for
    small teams. We would suggest using Pachyderm, and not using Spark, for any data
    science teams that own their infrastructure. Pachyderm works really well with
    unstructured data, like image and audio files.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 是一种轻量级的方法，可以帮助您轻松构建数据工程管道，并提供类似 Git 风格的数据版本支持。它是以数据科学家为中心且易于使用的。Pachyderm
    基于 Kubernetes，并使用云对象存储作为数据存储，因此对于小型团队来说，它既经济高效，又易于设置和维护。我们建议对于拥有自己基础设施的数据科学团队，应使用
    Pachyderm，而不是 Spark。Pachyderm 与非结构化数据，如图像和音频文件，配合得非常好。
- en: Limitations with Pachyderm
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: Pachyderm 的局限性
- en: What is missing in Pachyderm are schema protection and data analysis efficiency.
    Pachyderm sees everything as files; it keeps snapshots for each file version but
    doesn’t care about the file content. There is no data type validation on data
    writing or reading; it completely depends on the pipeline to protect the data
    consistency.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pachyderm 中缺少的是模式保护和数据分析效率。Pachyderm 将一切视为文件；它为每个文件版本保留快照，但并不关心文件内容。在数据写入或读取时没有数据类型验证；它完全依赖于管道来保护数据一致性。
- en: Lack of schema awareness and protection introduces a lot of risk for any continuous-running
    deep learning training pipeline because any code changes in the upstream data
    processing code might break the downstream data processing or training code. Also,
    without knowing the schema of the data, dataset comparison is hard to implement.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 缺乏模式意识和保护为任何持续运行的深度学习训练管道引入了大量的风险，因为上游数据处理代码中的任何代码更改都可能破坏下游数据处理或训练代码。此外，不知道数据模式，数据集比较难以实现。
- en: Summary
  id: totrans-393
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The primary goal of dataset management is to continuously receive fresh data
    from a variety of data sources and deliver datasets to model training while supporting
    training reproducibility (data version tracking).
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理的首要目标是持续从各种数据源接收新鲜数据，并将数据集交付给模型训练，同时支持训练可重复性（数据版本跟踪）。
- en: Having a dataset management component can expedite deep learning project development
    by parallelizing model algorithm development and data engineering development.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 拥有数据集管理组件可以通过并行化模型算法开发和数据工程开发来加速深度学习项目开发。
- en: 'The principles to validate the design of a dataset management service are as
    follows: supporting dataset reproducibility; employing strongly typed data schema;
    designing unified API and keeping API behavior consistent across different dataset
    types and sizes; and guaranteeing data persistence.'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据集管理服务设计的原则如下：支持数据集可重复性；使用强类型数据模式；设计统一的 API 并保持 API 行为在不同数据集类型和大小上的一致性；并保证数据持久性。
- en: A dataset management system should at least support (training) dataset versioning,
    which is crucial for model reproducibility and performance troubleshooting.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集管理系统至少应支持（训练）数据集版本控制，这对于模型的可重复性和性能故障排除至关重要。
- en: A dataset is a logic file group for a deep learning task; it’s static from the
    model training perspective and dynamic from the data collection perspective.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是深度学习任务的逻辑文件组；从模型训练的角度来看它是静态的，而从数据收集的角度来看它是动态的。
- en: The sample dataset management service is made of three layers—the data ingestion
    layer, internal dataset storage layer, and training dataset fetching layer.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据集管理服务由三层组成——数据摄取层、内部数据集存储层和训练数据集检索层。
- en: We define two data schemas for each dataset type in the sample dataset management
    service, one for data ingestion and one for dataset fetching. Each data update
    is stored as a commit, and each training dataset is stored as a versioned snapshot.
    Users can employ a version hash string to fetch the related training data at any
    time (dataset reproducibility).
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在示例数据集管理服务中，我们为每种数据集类型定义了两种数据模式，一种用于数据摄取，另一种用于数据集检索。每次数据更新都存储为一个提交，每个训练数据集都存储为一个版本化的快照。用户可以使用版本哈希字符串在任何时候检索相关的训练数据（数据集可重复性）。
- en: The sample dataset management service supports a special dataset type—a `GENERIC`
    dataset. A `GENERIC` dataset has no schema and no data validation, and users can
    upload and download data freely, so it’s good for prototyping new algorithms.
    Once the training code and dataset requirements become mature, the dataset format
    can be promoted to a strongly typed dataset.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 示例数据集管理服务支持一种特殊的数据集类型——`GENERIC` 数据集。`GENERIC` 数据集没有模式且没有数据验证，用户可以自由上传和下载数据，因此它适用于原型设计新算法。一旦训练代码和数据集要求成熟，数据集格式可以升级为强类型数据集。
- en: Delta Lake and Petastorm can work together to set up a dataset management service
    for Spark-based, deep learning projects.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Delta Lake 和 Petastorm 可以协同工作，为基于 Spark 的深度学习项目设置数据集管理服务。
- en: Pachyderm is a lightweight, Kubernetes-based data platform that offers data
    versioning support in Git style and allows easy pipeline setup. A pipeline is
    made by docker containers; it can be used to automate data process workflow and
    training workflow for a deep learning project.
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pachyderm 是一个基于 Kubernetes 的轻量级数据平台，它以 Git 风格提供数据版本控制支持，并允许轻松设置管道。管道由 Docker
    容器组成；它可以用于自动化深度学习项目的数据处理流程和训练流程。
