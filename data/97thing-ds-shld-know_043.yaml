- en: Chapter 39\. Algorithms Are Used Differently than Human Decision Makers
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第39章。算法的使用方式与人类决策者不同
- en: Rachel Thomas
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Rachel Thomas
- en: '![](Images/Rachel_Thomas.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Rachel_Thomas.png)'
- en: Cofounder, fast.ai; Director, USF Center for Applied Data Ethics
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: fast.ai的联合创始人；USF应用数据伦理中心主任
- en: People often discuss algorithms as though they are plug-and-play, interchangeable
    with human decision makers—just comparing error rates, for instance, when deciding
    whether to replace a human decision maker with an algorithmic result. However,
    in practice, algorithms and human decision makers are used differently, and failure
    to address those differences can lead to a number of ethical risks and harms.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人们经常讨论算法，好像它们是即插即用的，可以与人类决策者互换——例如，仅仅比较错误率来决定是否用算法结果替换人类决策者。然而，在实践中，算法和人类决策者的使用方式是不同的，如果不解决这些差异可能会导致许多伦理风险和危害。
- en: 'Here are a few common ways that algorithms and human decision makers are used
    differently in practice:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些算法和人类决策者在实际运用中不同的常见方式：
- en: Algorithms are more likely to be implemented with *no recourse process* in place.
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法更有可能在实施时没有*追索过程*。
- en: Algorithms are often used *at scale*.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法经常*规模化应用*。
- en: Algorithmic systems are *cheap*.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法系统*成本低廉*。
- en: People are more likely to assume algorithms are *objective* or *error-free*.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人们更容易认为算法是*客观*或*无误的*。
- en: There is a lot of overlap between these factors. If the main motivation for
    implementing an algorithm is cost cutting, then adding an appeals process (or
    even diligently checking for errors) may be considered an “unnecessary” expense.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这些因素之间存在很大的重叠。如果实施算法的主要动机是削减成本，那么增加上诉流程（甚至是认真检查错误）可能被视为“不必要”的开支。
- en: 'Consider one case study: after the state of [Arkansas implemented software](https://oreil.ly/m9kHi)
    to determine people’s health care benefits, many people saw a drastic reduction
    in the amount of care they received but were given no explanation and no way to
    appeal. Tammy Dobbs, a woman with cerebral palsy who needs an aide to help her
    to get out of bed, go to the bathroom, and more, had her hours of help suddenly
    reduced by 20 hours a week, transforming her life for the worse. Eventually, a
    lengthy court case uncovered errors in the software implementation, and Tammy’s
    hours were restored (along with those of many others who were impacted by the
    errors).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个案例研究：阿肯色州实施软件来确定人们的医疗保健福利后，许多人看到他们接收护理的数量急剧减少，但没有得到解释和上诉的途径。患有脑瘫的女士Tammy
    Dobbs需要助理帮助她起床、上厕所等，但她的帮助时间突然减少了20小时每周，生活变得更糟。最终，一场长时间的法庭审理揭示了软件实施中的错误，Tammy的工作时间恢复了（以及其他受错误影响的人们）。
- en: Another real-world case study comes from an algorithm that was used to fire
    public school teachers. Observations of fifth-grade teacher Sarah Wysocki’s classroom
    yielded positive reviews. Her [assistant principal wrote](https://oreil.ly/ALVe-),
    “It is a pleasure to visit a classroom in which the elements of sound teaching,
    motivated students, and a positive learning environment are so effectively combined.”
    Two months later, she was fired by an opaque algorithm, along with more than 200
    other teachers. The head of the PTA and a parent of one of Wysocki’s students
    described her as “one of the best teachers I’ve ever come in contact with. Every
    time I saw her, she was attentive to the children, went over their schoolwork;
    she took time with them.” That people are losing needed health care or being fired
    without mechanisms for recourse is truly dystopian!
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个现实案例来自于一个用于解雇公立学校教师的算法。对于第五年级教师莎拉·维索奇的课堂观察得到了积极的评价。她的[副校长写道](https://oreil.ly/ALVe-)，“访问一个教室是一种愉快的体验，在这个教室中，合理教学的要素、积极学习环境和正面的学习氛围被如此有效地结合在一起。”
    两个月后，她被一种不透明的算法与其他200多名教师一起解雇。PTA主席和维索奇学生的家长称她为“我曾经接触过的最好的老师之一。每次我看到她，她都在关心孩子们，帮助他们复习功课；她花时间和他们在一起。”
    那些因为没有追索机制而失去必要的医疗保健或被解雇的人们真是一个真正的反乌托邦！
- en: Mathematician Cathy O’Neil wrote in her 2016 book [*Weapons of Math Destruction*](https://weaponsofmathdestructionbook.com)
    (Crown) that many algorithmic systems
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数学家凯西·奥尼尔在她2016年的著作[*数学毁灭之武器*](https://weaponsofmathdestructionbook.com)（Crown）中写道，许多算法系统
- en: tend to punish the poor. They specialize in bulk, and they’re cheap. That’s
    part of their appeal. The wealthy, by contrast, often benefit from personal input.
    A white-shoe law firm or an exclusive prep school will lean far more on recommendations
    and face-to-face interviews than will a fast-food chain or a cash-strapped urban
    school district. The privileged, we’ll see time and again, are processed more
    by people, the masses by machines.
  id: totrans-14
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 倾向于惩罚贫困者。它们专门进行大宗交易，而且价格便宜。这就是它们吸引人的部分原因。相比之下，富人通常受益于个性化的建议。一家白手套律师事务所或独家预备学校将更多地依赖推荐和面对面的面试，而不像快餐连锁店或财政困难的城市学区那样。我们会一再看到，特权阶层更多地通过人工处理，而大众更多地通过机器处理。
- en: This harm can be compounded by the fact that many people mistakenly believe
    that computers are objective and error-free. A [city official in Lancaster](https://oreil.ly/mY_If),
    California, where an IBM Watson dashboard is being used for predictive policing,
    said, “With machine learning, with automation, there’s a 99% success, so that
    robot is—will be—99% accurate in telling us what is going to happen next, which
    is really interesting.” This statement is completely false. It is a dangerous
    yet common misconception that can lead people to overlook harmful errors in computer
    output.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这种伤害可能会因为许多人错误地认为计算机是客观且无误的事实而加剧。在加利福尼亚州兰开斯特市，一位[市官员](https://oreil.ly/mY_If)称，“通过机器学习、自动化，成功率达到99%，因此那个机器人将会——将会——在预测下一步发生的事情时有99%的准确率，这真的很有趣。”这种说法完全不正确。这是一个危险而常见的误解，可能导致人们忽视计算机输出中的有害错误。
- en: As roboticist [Peter Haas said](https://oreil.ly/dIvsh) in a TEDx talk, “In
    AI, we have Milgram’s ultimate authority figure,” referring to Stanley Milgram’s
    [famous experiments](https://oreil.ly/yWvhA) showing that most people will obey
    orders from authority figures, even to the point of harming or killing other humans.
    How much more likely will people be to trust algorithms perceived as objective
    and correct?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 作为机器人学家[彼得·哈斯](https://oreil.ly/dIvsh)在一次TEDx演讲中所说，“在AI领域，我们拥有米尔格拉姆的终极权威人物”，指的是斯坦利·米尔格拉姆的[著名实验](https://oreil.ly/yWvhA)，显示大多数人会服从权威人物的命令，甚至可能伤害或杀害其他人类。那么，人们更有可能信任被认为是客观和正确的算法？
- en: Since algorithms are often used at a larger scale, mass-producing identical
    biases, and are assumed to be error-proof or objective, we can’t compare them
    to human decision makers in an apples-to-apples way. Moreover, it is important
    that we address these differences when implementing algorithms for decision making.
    It is essential to implement systems for identifying errors, and mechanisms for
    recourse, alongside any algorithmic implementation. It is also necessary to make
    sure that those using the output of the algorithms understand that computers are
    not error-free, and that they are empowered to raise any issues they spot.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于算法通常用于更大规模的应用，大规模生产相同的偏见，并且被假定为无误或客观，我们无法以苹果和苹果的方式将它们与人类决策者进行比较。此外，在实施决策算法时，解决这些差异非常重要。必须实施系统来识别错误，并在任何算法实施之外实施追索机制。同样重要的是，确保使用算法输出的人员明白计算机并非无误，他们有能力提出任何他们发现的问题。
