- en: Chapter 22\. Arrow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第 22 章 Arrow
- en: Introduction
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 介绍
- en: 'CSV files are designed to be easily read by humans. They’re a good interchange
    format because they’re simple and they can be read by every tool under the sun.
    But CSV files aren’t efficient: you have to do quite a lot of work to read the
    data into R. In this chapter, you’ll learn about a powerful alternative: the [parquet
    format](https://oreil.ly/ClE7D), an open standards–based format widely used by
    big data systems.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: CSV 文件设计成易于人类阅读。它们是一种很好的交换格式，因为它们简单且可以被各种工具读取。但 CSV 文件不够高效：您需要做相当多的工作才能将数据读入
    R。在本章中，您将了解到一个强大的替代方案：[parquet 格式](https://oreil.ly/ClE7D)，这是一个基于开放标准的格式，在大数据系统中被广泛使用。
- en: We’ll pair parquet files with [Apache Arrow](https://oreil.ly/TGrH5), a multilanguage
    toolbox designed for efficient analysis and transport of large datasets. We’ll
    use Apache Arrow via the [arrow package](https://oreil.ly/g60F8), which provides
    a dplyr backend allowing you to analyze larger-than-memory datasets using familiar
    dplyr syntax. As an additional benefit, arrow is extremely fast; you’ll see some
    examples later in the chapter.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 parquet 文件与 [Apache Arrow](https://oreil.ly/TGrH5) 配对使用，后者是一款专为高效分析和传输大数据集而设计的多语言工具包。我们将通过
    [arrow 软件包](https://oreil.ly/g60F8) 使用 Apache Arrow，它提供了 dplyr 后端，允许您使用熟悉的 dplyr
    语法分析超内存数据集。此外，arrow 运行速度极快；本章后面将展示一些示例。
- en: Both arrow and dbplyr provide dplyr backends, so you might wonder when to use
    each. In many cases, the choice is made for you, as in the data is already in
    a database or in parquet files, and you’ll want to work with it as is. But if
    you’re starting with your own data (perhaps CSV files), you can either load it
    into a database or convert it to parquet. In general, it’s hard to know what will
    work best, so in the early stages of your analysis, we encourage you to try both
    and pick the one that works the best for you.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: arrow 和 dbplyr 都提供了 dplyr 后端，因此您可能会想知道何时使用每一个。在许多情况下，选择已经为您做出，例如数据已经在数据库中或者在
    parquet 文件中，并且您希望按原样处理它。但如果您从自己的数据开始（也许是 CSV 文件），您可以将其加载到数据库中或将其转换为 parquet。总之，在早期分析阶段，很难知道哪种方法最有效，因此我们建议您尝试两种方法，并选择最适合您的那一种。
- en: (A big thanks to Danielle Navarro who contributed the initial version of this
    chapter.)
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: （特别感谢 Danielle Navarro 提供本章的初始版本。）
- en: Prerequisites
  id: totrans-6
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 先决条件
- en: 'In this chapter, we’ll continue to use the tidyverse, particularly dplyr, but
    we’ll pair it with the arrow package, which was designed specifically for working
    with large data:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续使用 tidyverse，特别是 dplyr，但我们将与 arrow 软件包配对使用，该软件包专门设计用于处理大数据：
- en: '[PRE0]'
  id: totrans-8
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Later in the chapter, we’ll also see some connections between arrow and duckdb,
    so we’ll also need dbplyr and duckdb:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本章稍后我们还将看到 arrow 和 duckdb 之间的一些连接，因此我们还需要 dbplyr 和 duckdb。
- en: '[PRE1]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Getting the Data
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'We begin by getting a dataset worthy of these tools: a dataset of item checkouts
    from Seattle public libraries, available online at [Seattle Open Data](https://oreil.ly/u56DR).
    This dataset contains 41,389,465 rows that tell you how many times each book was
    checked out each month from April 2005 to October 2022.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先获取一个值得使用这些工具的数据集：来自西雅图公共图书馆的项目借阅数据集，可在线获取，网址为 [Seattle Open Data](https://oreil.ly/u56DR)。该数据集包含
    41,389,465 行数据，告诉您每本书从 2005 年 4 月至 2022 年 10 月每月被借阅的次数。
- en: 'The following code will get you a cached copy of the data. The data is a 9
    GB CSV file, so it will take some time to download. I highly recommend using `curl::multidownload()`
    to get very large files as it’s built for exactly this purpose: it gives you a
    progress bar, and it can resume the download if it’s interrupted.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将帮助您获取数据的缓存副本。数据是一个 9 GB 的 CSV 文件，因此下载可能需要一些时间。我强烈建议您使用 `curl::multidownload()`
    来获取非常大的文件，因为它专为此目的而设计：它会提供进度条，并且如果下载中断，它可以恢复下载。
- en: '[PRE2]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Opening a Dataset
  id: totrans-15
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 打开数据集
- en: 'Let’s start by taking a look at the data. At 9 GB, this file is large enough
    that we probably don’t want to load the whole thing into memory. A good rule of
    thumb is that you usually want at least twice as much memory as the size of the
    data, and many laptops top out at 16 GB. This means we want to avoid [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.xhtml)
    and instead use [`arrow::open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml):'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来看一下数据。9 GB 的文件足够大，我们可能不希望将整个文件加载到内存中。一个好的经验法则是，您通常希望内存大小至少是数据大小的两倍，而许多笔记本电脑最大只能支持
    16 GB。这意味着我们应避免使用 [`read_csv()`](https://readr.tidyverse.org/reference/read_delim.xhtml)，而应使用
    [`arrow::open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml)：
- en: '[PRE3]'
  id: totrans-17
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'What happens when this code is run? [`open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml)
    will scan a few thousand rows to figure out the structure of the dataset. Then
    it records what it’s found and stops; it will only read further rows as you specifically
    request them. This metadata is what we see if we print `seattle_csv`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行此代码时会发生什么？[`open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml)将扫描几千行以确定数据集的结构。然后记录它所找到的内容并停止；只有在您明确请求时才会继续读取更多行。这些元数据是我们在打印`seattle_csv`时看到的内容：
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The first line in the output tells you that `seattle_csv` is stored locally
    on disk as a single CSV file; it will be loaded into memory only as needed. The
    remainder of the output tells you the column type that arrow has imputed for each
    column.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 输出的第一行告诉您`seattle_csv`作为单个CSV文件存储在本地磁盘上，只有在需要时才会加载到内存中。其余输出告诉您Arrow为每列推断的列类型。
- en: We can see what’s actually in with [`glimpse()`](https://pillar.r-lib.org/reference/glimpse.xhtml).
    This reveals that there are ~41 million rows and 12 columns and shows us a few
    values.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过[`glimpse()`](https://pillar.r-lib.org/reference/glimpse.xhtml)看到实际情况。这显示了大约4100万行和12列，并展示了一些值。
- en: '[PRE5]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'We can start to use this dataset with dplyr verbs, using [`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml)
    to force arrow to perform the computation and return some data. For example, this
    code tells us the total number of checkouts per year:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以开始使用dplyr动词处理此数据集，使用[`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml)强制Arrow执行计算并返回一些数据。例如，此代码告诉我们每年的总借阅次数：
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Thanks to arrow, this code will work regardless of how large the underlying
    dataset is. But it’s currently rather slow: on Hadley’s computer, it took ~10s
    to run. That’s not terrible given how much data we have, but we can make it much
    faster by switching to a better format.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Arrow的支持，无论基础数据集有多大，此代码都将有效运行。但目前速度较慢：在Hadley的计算机上运行大约需要10秒。考虑到我们有多少数据，这并不糟糕，但如果切换到更好的格式，可以使其运行速度更快。
- en: The Parquet Format
  id: totrans-26
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Parquet格式
- en: To make this data easier to work with, let’s switch to the parquet file format
    and split it up into multiple files. The following sections will first introduce
    you to parquet and partitioning and then apply what we learned to the Seattle
    library data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些数据更易于处理，让我们切换到Parquet文件格式，并将其拆分成多个文件。接下来的部分将首先介绍Parquet和分区，然后应用我们学到的内容到Seattle图书馆数据。
- en: Advantages of Parquet
  id: totrans-28
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: Parquet的优势
- en: 'Like CSV, parquet is used for rectangular data, but instead of being a text
    format that you can read with any file editor, it’s a custom binary format designed
    specifically for the needs of big data. This means that:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 像CSV一样，Parquet用于矩形数据，但不是可以用任何文件编辑器读取的文本格式，而是一个专门为大数据需求设计的自定义二进制格式。这意味着：
- en: Parquet files are usually smaller than the equivalent CSV file. Parquet relies
    on [efficient encodings](https://oreil.ly/OzpFo) to keep file size down and supports
    file compression. This helps make parquet files fast because there’s less data
    to move from disk to memory.
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet文件通常比等价的CSV文件更小。Parquet依赖于[高效编码](https://oreil.ly/OzpFo)来减少文件大小，并支持文件压缩。这有助于使Parquet文件快速，因为需要从磁盘移到内存的数据更少。
- en: Parquet files have a rich type system. As we talked about in [“Controlling Column
    Types”](ch07.xhtml#sec-col-types), a CSV file does not provide any information
    about column types. For example, a CSV reader has to guess whether `"08-10-2022"`
    should be parsed as a string or a date. In contrast, parquet files store data
    in a way that records the type along with the data.
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet文件具有丰富的类型系统。正如我们在[“控制列类型”](ch07.xhtml#sec-col-types)中讨论的那样，CSV文件不提供任何关于列类型的信息。例如，CSV读取器必须猜测`"08-10-2022"`应该被解析为字符串还是日期。相比之下，Parquet文件以记录类型及其数据的方式存储数据。
- en: Parquet files are “column-oriented.” This means they’re organized column by
    column, much like R’s data frame. This typically leads to better performance for
    data analysis tasks compared to CSV files, which are organized row by row.
  id: totrans-32
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet文件是“列导向”的。这意味着它们按列组织，类似于R的数据框架。与按行组织的CSV文件相比，这通常会导致数据分析任务的性能更好。
- en: Parquet files are “chunked,” which makes it possible to work on different parts
    of the file at the same time and, if you’re lucky, to skip some chunks altogether.
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet文件是“分块”的，这使得可以同时处理文件的不同部分，并且如果幸运的话，甚至可以跳过一些块。
- en: Partitioning
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 分区
- en: As datasets get larger and larger, storing all the data in a single file gets
    increasingly painful, and it’s often useful to split large datasets across many
    files. When this structuring is done intelligently, this strategy can lead to
    significant improvements in performance because many analyses will require only
    a subset of the files.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据集变得越来越大，将所有数据存储在单个文件中变得越来越痛苦，将大型数据集分割成许多文件通常非常有用。当这种结构化工作得聪明时，这种策略可以显著提高性能，因为许多分析只需要一部分文件的子集。
- en: 'There are no hard and fast rules about how to partition your dataset: the results
    will depend on your data, access patterns, and the systems that read the data.
    You’re likely to need to do some experimentation before you find the ideal partitioning
    for your situation. As a rough guide, arrow suggests that you avoid files smaller
    than 20 MB and larger than 2 GB and avoid partitions that produce more than 10,000
    files. You should also try to partition by variables that you filter by; as you’ll
    see shortly, that allows arrow to skip a lot of work by reading only the relevant
    files.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 没有关于如何分区数据集的硬性规则：结果将取决于您的数据、访问模式和读取数据的系统。在找到适合您情况的理想分区之前，您可能需要进行一些实验。作为一个粗略的指南，Arrow
    建议避免小于 20 MB 和大于 2 GB 的文件，并避免产生超过 10,000 个文件的分区。您还应尝试按照您进行过滤的变量进行分区；正如您马上将看到的那样，这样可以使
    Arrow 跳过许多工作，仅读取相关文件。
- en: Rewriting the Seattle Library Data
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重写西雅图图书馆数据
- en: Let’s apply these ideas to the Seattle library data to see how they play out
    in practice. We’re going to partition by `CheckoutYear`, since it’s likely some
    analyses will want to look at only recent data and partitioning by year yields
    18 chunks of a reasonable size.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这些想法应用到西雅图图书馆的数据中，看看它们在实践中如何运作。我们将按照 `CheckoutYear` 进行分区，因为一些分析可能只想查看最近的数据，并且按年份分区得到了
    18 个合理大小的块。
- en: 'To rewrite the data, we define the partition using [`dplyr::group_by()`](https://dplyr.tidyverse.org/reference/group_by.xhtml)
    and then save the partitions to a directory with [`arrow::write_dataset()`](https://arrow.apache.org/docs/r/reference/write_dataset.xhtml).
    [`write_dataset()`](https://arrow.apache.org/docs/r/reference/write_dataset.xhtml)
    has two important arguments: a directory where we’ll create the files and the
    format we’ll use.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 为了重写数据，我们使用 [`dplyr::group_by()`](https://dplyr.tidyverse.org/reference/group_by.xhtml)
    定义分区，然后使用 [`arrow::write_dataset()`](https://arrow.apache.org/docs/r/reference/write_dataset.xhtml)
    将分区保存到目录中。[`write_dataset()`](https://arrow.apache.org/docs/r/reference/write_dataset.xhtml)
    有两个重要的参数：我们将创建文件的目录和我们将使用的格式。
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: '[PRE8]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This takes about a minute to run; as we’ll see shortly this is an initial investment
    that pays off by making future operations much much faster.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 这需要大约一分钟才能运行；正如我们马上将看到的，这是一个初始投资，通过使未来的操作更快来得到回报。
- en: 'Let’s take a look at what we just produced:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们刚刚产生的内容：
- en: '[PRE9]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Our single 9 GB CSV file has been rewritten into 18 parquet files. The filenames
    use a “self-describing” convention used by the [Apache Hive project](https://oreil.ly/kACzC).
    Hive-style partitions name folders with a “key=value” convention, so as you might
    guess, the `CheckoutYear=2005` directory contains all the data where `CheckoutYear`
    is 2005\. Each file is between 100 and 300 MB and the total size is now around
    4 GB, a little more than half the size of the original CSV file. This is as we
    expect since parquet is a much more efficient format.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的单个 9 GB CSV 文件已经被重写为 18 个 parquet 文件。文件名使用了由 [Apache Hive 项目](https://oreil.ly/kACzC)
    使用的“自描述”约定。Hive 风格的分区将文件夹命名为“key=value”的约定，因此您可能猜到，`CheckoutYear=2005` 目录包含所有
    `CheckoutYear` 是 2005 年的数据。每个文件大小在 100 到 300 MB 之间，总大小现在约为 4 GB，略大于原始 CSV 文件大小的一半。这正如我们预期的那样，因为
    parquet 是一种更高效的格式。
- en: Using dplyr with Arrow
  id: totrans-46
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 dplyr 与 Arrow
- en: 'Now that we’ve created these parquet files, we’ll need to read them in again.
    We use [`open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml)
    again, but this time we give it a directory:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了这些 parquet 文件，我们需要再次读取它们。我们再次使用 [`open_dataset()`](https://arrow.apache.org/docs/r/reference/open_dataset.xhtml)，但这次我们给它一个目录：
- en: '[PRE10]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now we can write our dplyr pipeline. For example, we could count the total
    number of books checked out in each month for the last five years:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以编写我们的 dplyr 流水线。例如，我们可以计算过去五年每个月借出的书籍总数：
- en: '[PRE11]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Writing dplyr code for arrow data is conceptually similar to dbplyr, as discussed
    in [Chapter 21](ch21.xhtml#chp-databases): you write dplyr code, which is automatically
    transformed into a query that the Apache Arrow C++ library understands, which
    is then executed when you call [`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml).
    If we print out the `query` object, we can see a little information about what
    we expect Arrow to return when the execution takes place:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为 Arrow 数据编写 dplyr 代码在概念上与 dbplyr 类似，如 [第 21 章](ch21.xhtml#chp-databases) 中讨论的：你编写
    dplyr 代码，它会自动转换为 Apache Arrow C++ 库理解的查询，当你调用 [`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml)
    时执行。
- en: '[PRE12]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'And we can get the results by calling [`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml):'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 [`collect()`](https://dplyr.tidyverse.org/reference/compute.xhtml) 即可获取结果：
- en: '[PRE13]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Like dbplyr, arrow understands only some R expressions, so you may not be able
    to write exactly the same code you usually would. However, the list of operations
    and functions supported is fairly extensive and continues to grow; find a complete
    list of currently supported functions in [`?acero`](https://arrow.apache.org/docs/r/reference/acero.xhtml).
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 与 dbplyr 类似，Arrow 仅理解某些 R 表达式，因此可能无法像平常那样编写完全相同的代码。不过，支持的操作和函数列表相当广泛，并且在不断增加中；可以在
    [`?acero`](https://arrow.apache.org/docs/r/reference/acero.xhtml) 中找到目前支持的完整列表。
- en: Performance
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能
- en: 'Let’s take a quick look at the performance impact of switching from CSV to
    parquet. First, let’s time how long it takes to calculate the number of books
    checked out in each month of 2021, when the data is stored as a single large CSV
    file:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下从 CSV 切换到 Parquet 对性能的影响。首先，让我们计算将数据存储为单个大型 CSV 文件时，在 2021 年每个月的借阅书籍数量所需的时间：
- en: '[PRE14]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now let’s use our new version of the dataset in which the Seattle library checkout
    data has been partitioned into 18 smaller parquet files:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使用我们的新版本数据集，在这个版本中，西雅图图书馆的借阅数据已经被分成了18个更小的 Parquet 文件：
- en: '[PRE15]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The ~100x speedup in performance is attributable to two factors: the multifile
    partitioning and the format of individual files:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提升约100倍归因于两个因素：多文件分区和单个文件的格式：
- en: Partitioning improves performance because this query uses `CheckoutYear == 2021`
    to filter the data, and arrow is smart enough to recognize that it needs to read
    only 1 of the 18 parquet files.
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分区可以提高性能，因为此查询使用 `CheckoutYear == 2021` 来过滤数据，而 Arrow 足够智能，能识别出它只需读取18个 Parquet
    文件中的一个。
- en: The parquet format improves performance by storing data in a binary format that
    can be read more directly into memory. The column-wise format and rich metadata
    means that arrow needs to read only the four columns actually used in the query
    (`CheckoutYear`, `MaterialType`, `CheckoutMonth`, and `Checkouts`).
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Parquet 格式通过以二进制格式存储数据来提高性能，可以更直接地读入内存。按列存储的格式和丰富的元数据意味着 Arrow 只需读取查询中实际使用的四列（`CheckoutYear`、`MaterialType`、`CheckoutMonth`
    和 `Checkouts`）。
- en: This massive difference in performance is why it pays off to convert large CSVs
    to parquet!
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这种性能上的巨大差异是将大型 CSV 文件转换为 Parquet 格式是值得的！
- en: Using dbplyr with Arrow
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用 dbplyr 和 Arrow
- en: 'There’s one last advantage of parquet and arrow—it’s easy to turn an arrow
    dataset into a DuckDB database ([Chapter 21](ch21.xhtml#chp-databases)) by calling
    [`arrow::to_duckdb()`](https://arrow.apache.org/docs/r/reference/to_duckdb.xhtml):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Parquet 和 Arrow 的最后一个优势是很容易将 Arrow 数据集转换成 DuckDB 数据库（[第 21 章](ch21.xhtml#chp-databases)）只需调用
    [`arrow::to_duckdb()`](https://arrow.apache.org/docs/r/reference/to_duckdb.xhtml)：
- en: '[PRE16]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The neat thing about [`to_duckdb()`](https://arrow.apache.org/docs/r/reference/to_duckdb.xhtml)
    is that the transfer doesn’t involve any memory copying and speaks to the goals
    of the arrow ecosystem: enabling seamless transitions from one computing environment
    to another.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '[`to_duckdb()`](https://arrow.apache.org/docs/r/reference/to_duckdb.xhtml)
    的好处在于转移过程中不涉及任何内存复制，并且符合 Arrow 生态系统的目标：实现从一个计算环境到另一个计算环境的无缝过渡。'
- en: Summary
  id: totrans-69
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, you got a taste of the arrow package, which provides a dplyr
    backend for working with large on-disk datasets. It can work with CSV files, and
    it’s much much faster if you convert your data to parquet. Parquet is a binary
    data format that’s designed specifically for data analysis on modern computers.
    Far fewer tools can work with parquet files compared to CSV, but its partitioned,
    compressed, and columnar structure makes it much more efficient to analyze.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经初步了解了arrow包，它为处理大型磁盘数据集提供了dplyr后端支持。它可以处理CSV文件，并且如果您将数据转换为parquet格式，速度会快得多。Parquet是一种二进制数据格式，专为在现代计算机上进行数据分析而设计。与CSV文件相比，能够处理parquet文件的工具较少，但是其分区、压缩和列式结构使其分析效率大大提高。
- en: Next up you’ll learn about your first nonrectangular data source, which you’ll
    handle using tools provided by the tidyr package. We’ll focus on data that comes
    from JSON files, but the general principles apply to tree-like data regardless
    of its source.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将学习有关您的第一个非矩形数据源的内容，您将使用tidyr包提供的工具处理此类数据。我们将重点关注来自JSON文件的数据，但是无论数据源如何，一般的原则都适用于类似树形的数据。
