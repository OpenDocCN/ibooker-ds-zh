- en: 7 Alternative connectivity patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7种交替连接模式
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding alternative connectivity patterns for deeper and wider layers
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解更深更宽层的交替连接模式
- en: Increasing accuracy with feature map reuse, further refactoring convolutions,
    and squeeze-excitation
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过特征图重用、进一步重构卷积和squeeze-excitation来提高准确性
- en: Coding alternatively connected models (DenseNet, Xception, SE-Net) with the
    procedural design pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用过程设计模式编码交替连接的模型（DenseNet、Xception、SE-Net）
- en: So far, we’ve looked at convolutional networks with deep layers and convolutional
    networks with wide layers. In particular, we’ve seen how the corresponding connectivity
    patterns both between and within convolutional blocks addressed issues of vanishing
    and exploding gradients and the problem of memorization from overcapacity.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了具有深层卷积网络的卷积网络和具有宽层卷积网络的卷积网络。特别是，我们看到了相应的连接模式如何在卷积块之间和内部解决梯度消失和爆炸以及过度容量导致的记忆问题。
- en: Those methods of increasing deep and wide layers, along with regularization
    (adding noise to reduce overfitting) at the deeper layers, reduced the problem
    with memorization but certainly did not eliminate it. So researchers explored
    other connectivity patterns within and between residual convolutional blocks to
    further reduce memorization without substantially increasing the number of parameters
    and compute operations.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 这些增加深度和宽度层的方法，以及深层层中的正则化（添加噪声以减少过拟合），减少了记忆问题，但当然并没有消除它。因此，研究人员探索了残差卷积块内部和之间的其他连接模式，以进一步减少记忆，而不会显著增加参数数量和计算操作。
- en: 'We’ll cover three of those alternative connectivity patterns in this chapter:
    DenseNet, Xception, and SE-Net. These patterns all had similar goals: reducing
    compute complexity in the connectivity component. But they differed in their approaches
    to the problem. Let’s first get an overview of those differences. Then we’ll spend
    the rest of the chapter looking at the specifics of each pattern.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍其中的三种交替连接模式：DenseNet、Xception和SE-Net。这些模式都有相似的目标：减少连接组件的计算复杂性。但它们在解决问题的方法上有所不同。让我们首先概述这些差异。然后，我们将在本章的剩余部分查看每种模式的细节。
- en: In 2017, researchers from Cornell University, Tsinghua University, and Facebook
    AI Research argued that the residual link in conventional residual blocks only
    partially allowed deeper layers to use feature extraction from earlier layers.
    By doing a matrix addition of the input to the output, the feature information
    from the input is gradually diluted as it progresses to deeper layers. The authors
    proposed using a feature map concatenation, which they referred to as *feature
    reuse*, in place of the matrix addition. Their reasoning was that the feature
    maps at the output of each residual block would be reused at all the remaining
    (deeper) layers. To keep the model parameters exploding in size as feature maps
    accumulate through deeper layers, they introduced an aggressive feature map dimensionality
    reduction between convolutional groups. In their ablation study, DenseNet obtained
    better performance than previous residual block networks.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年，康奈尔大学、清华大学和Facebook人工智能研究部门的学者们认为，传统残差块中的残差连接只部分允许深层层使用早期层的特征提取。通过将输入与输出进行矩阵相加，输入的特征信息在向深层层进展的过程中逐渐稀释。作者们提出使用特征图拼接，他们称之为*特征重用*，来代替矩阵相加。他们的理由是，每个残差块输出的特征图将在所有剩余的（深层）层中重用。为了防止模型参数随着特征图在深层层中累积而爆炸性增长，他们在卷积组之间引入了激进的降维特征图。在他们进行的消融研究中，DenseNet比之前的残差块网络获得了更好的性能。
- en: In the same year, François Chollet, who created Keras, introduced Xception,
    which redesigned the Inception v3 model into a new flow pattern. The new pattern
    consisted of entry, middle, and exit, as opposed to the previous Inception designs.
    While other researchers did not adopt this new flow pattern, they did adopt Chollet’s
    further refactoring of normal and separable convolutions into depthwise separable
    convolutions. This process reduces the number of matrix operations, while maintaining
    representational equivalence (more on this shortly). This refactoring continues
    to appear in many SOTA models—particularly those designed for memory and computationally
    constrained devices, such as mobile devices.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在同年，Keras的创造者François Chollet引入了Xception，它将Inception v3模型重新设计成新的流程模式。新的模式由入口、中间和出口组成，与之前的Inception设计不同。虽然其他研究人员没有采用这种新的流程模式，但他们确实采用了Chollet对正常和可分离卷积进一步重构为深度可分离卷积的改进。这个过程减少了矩阵操作的数量，同时保持了表示等价性（稍后将有更多介绍）。这种重构继续出现在许多SOTA模型中——尤其是那些为内存和计算受限设备（如移动设备）设计的模型。
- en: Later in 2017, researchers from the Chinese Academy of Sciences and Oxford University
    introduced another connectivity pattern for residual blocks, which could be retrofitted
    into conventional residual networks. The connectivity pattern of SE-Net, as it
    became known, inserted a micro-block (referred to as an *SE link*) between the
    output of the residual block and the matrix add operation with the block input.
    This micro-block did an aggressive dimensionality reduction, or *squeeze*, on
    the output feature maps, followed by a dimensionality expansion, or *excitation*.
    The researchers hypothesized that this squeeze-and-excitation step would cause
    the feature maps to become more generalized. They inserted the SE links into ResNet
    and ResNeXt and showed performance improvements averaging 2% on examples not seen
    during testing (holdout data).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2017年稍后，中国科学院和牛津大学的研究人员为残差块引入了另一种连接模式，该模式可以集成到传统的残差网络中。SE-Net的连接模式，正如其名称所示，在残差块的输出和与块输入的矩阵加法操作之间插入了一个微块（称为*SE链接*）。这个微块对输出特征图进行了积极的维度缩减，或*挤压*，随后是维度扩展，或*激励*。研究人员假设这个挤压-激励步骤会使特征图变得更加通用。他们将SE链接插入到ResNet和ResNeXt中，并在测试中未看到的示例上（保留数据）展示了平均2%的性能提升。
- en: Now that we have the big picture, let’s look at the details of how these three
    patterns address the problem of reducing complexity at the connection level.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了整体图景，让我们来看看这三种模式如何解决在连接级别降低复杂性的问题。
- en: '7.1 DenseNet: Densely connected convolutional neural network'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 DenseNet：密集连接卷积神经网络
- en: The *DenseNet* model introduced the concept of a densely connected convolutional
    network. The corresponding paper, “Densely Connected Convolutional Networks,”
    by Gao Huang et al. ([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993)),
    won the Conference on Computer Vision and Pattern Recognition (CVPR) 2017 Best
    Paper Award. The design is based on the principle that the output of each residual
    block layer is connected to the input of every subsequent residual block layer.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '*DenseNet*模型引入了密集连接卷积网络的概念。相应的论文，“Densely Connected Convolutional Networks”，由Gao
    Huang等人撰写([https://arxiv.org/abs/1608.06993](https://arxiv.org/abs/1608.06993))，获得了2017年计算机视觉和模式识别会议（CVPR）最佳论文奖。该设计基于每个残差块层输出连接到每个后续残差块层输入的原则。'
- en: This extends the concept of identity links in residual blocks (covered in chapter
    4). This section provides details on the macro-architecture, group and block components,
    and corresponding design principles.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这扩展了残差块中身份链接的概念（在第4章中介绍）。本节提供了关于宏观架构、组和块组件以及相应设计原则的详细信息。
- en: 7.1.1 Dense group
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 密集组
- en: Prior to DenseNet, an identity link between the input and output of a residual
    block was combined by matrix addition. In contrast, in a dense block, the input
    to the residual block is concatenated to the output of the residual block. This
    change introduced the concept of *feature (map) reuse*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在DenseNet之前，残差块的输入和输出之间的身份链接是通过矩阵加法组合的。相比之下，在密集块中，残差块的输入被连接到残差块的输出。这种变化引入了*特征（映射）重用*的概念。
- en: In figure 7.1, you can see the difference in connectivity between a residual
    block and a dense residual block. In a residual block, the values in input feature
    maps are added to the output feature maps. While this retained some of the information
    in the block, it can be seen as being diluted by the addition operation. In the
    DenseNet residual block version, the input feature maps are fully retained, so
    no dilution occurs.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.1中，你可以看到残差块和密集残差块之间的连接性差异。在残差块中，输入特征图中的值被加到输出特征图上。虽然这保留了一些块中的信息，但它可以被看作是通过加法操作稀释了。在DenseNet残差块版本中，输入特征图被完全保留，因此没有发生稀释。
- en: '![](Images/CH07_F01_Ferlitsch.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F01_Ferlitsch.png)'
- en: 'Figure 7.1 Residual block vs. dense block: the dense block uses a matrix concatenation
    instead of a matrix add operation.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 残差块与密集块对比：密集块使用矩阵连接而不是矩阵加法操作。
- en: 'The replacing of the matrix addition with concatenation has these advantages:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 将矩阵加法替换为连接的优点：
- en: Further alleviating the vanishing gradient problem over deeper layers
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进一步缓解深层网络中的消失梯度问题
- en: Further reducing the computational complexity (parameters) with narrower feature
    maps
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过较窄的特征图进一步降低计算复杂度（参数）
- en: With concatenation, the distance between the output (classifier) and the feature
    maps is shorter. The shortened distance reduces the vanishing gradient problem,
    allowing for deeper networks that could produce higher accuracy.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 使用连接，输出（分类器）和特征图之间的距离更短。缩短的距离减少了消失梯度问题，允许构建更深层的网络，从而产生更高的精度。
- en: The reuse of feature maps has representational equivalence with the former operation
    of a matrix addition, but with substantially fewer filters. The authors refer
    to this arrangement as *narrower* layers. With the narrower layers, the overall
    number of parameters to train is reduced. The authors theorized that the feature
    reuse allowed the model to go deeper in layers for more accuracy, without being
    exposed to vanishing gradients or memorization.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 特征图的复用与矩阵加法的前一操作具有表示等价性，但具有显著更少的过滤器。作者将这种配置称为*较窄*的层。使用较窄的层，可以减少训练所需的总体参数数量。作者理论认为，特征复用允许模型在更深层的网络中达到更高的精度，而不会暴露于消失梯度或记忆化。
- en: Here is an example for comparison. Let’s assume the outputs of a layer are feature
    maps of size 28 × 28 × 10\. After a matrix addition, the outputs continue to be
    28 × 28 × 10 feature maps. The values within them are the addition of the residual
    block’s input and output, and thus do not retain the original values—in other
    words, they have been merged. In the dense block, the input feature maps are concatenated—not
    merged—to the residual block output, thus preserving the original value of the
    identity link. In our example, with input and output of 28 × 28 × 10, the output,
    after the concatenation, will be 28 × 28 × 20\. Continuing to the next block,
    the output will be 28 × 28 × 40.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个比较的例子。假设某层的输出是大小为28 × 28 × 10的特征图。经过矩阵加法后，输出仍然是28 × 28 × 10的特征图。它们内部的价值是残差块输入和输出的加和，因此没有保留原始值——换句话说，它们已经被合并。在密集块中，输入特征图是连接到残差块输出的，而不是合并，从而保留了恒等连接的原始值。在我们的例子中，输入和输出为28
    × 28 × 10，连接后的输出将是28 × 28 × 20。继续到下一个块，输出将是28 × 28 × 40。
- en: In this way, the output of each layer is concatenated into the input of each
    subsequent layer, giving rise to the phrasing *densely connected* to describe
    these kinds of models. Figure 7.2 depicts the general construction and identity
    linking between residual blocks in a dense group.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，每一层的输出都连接到下一层的输入，从而产生了描述这类模型的短语*密集连接*。图7.2展示了密集组中残差块的一般构造和恒等连接。
- en: '![](Images/CH07_F02_Ferlitsch.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F02_Ferlitsch.png)'
- en: Figure 7.2 In this dense group micro-architecture, a matrix concatenation operation
    is used between the output of the residual block and the input (identity link).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 在这个密集的组微架构中，残差块输出和输入（恒等连接）之间使用矩阵连接操作。
- en: As you can see, a dense group consists of multiple dense blocks. Each dense
    block consists of a residual block (without an identity link) and the identity
    link from the input to the residual block to the output. The input and output
    feature maps are then concatenated into a single output, which becomes the input
    to the next dense block. This way, the feature maps outputted from each dense
    block are reused (shared) with every subsequent dense block.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，一个密集组由多个密集块组成。每个密集块由一个残差块（没有身份链接）和从输入到残差块再到输出的身份链接组成。然后，输入和输出特征图被连接成一个单一的输出，这成为下一个密集块的输入。这样，每个密集块输出的特征图都会被后续的每个密集块重用（共享）。
- en: The DenseNet researchers introduced a metaparameter *k*, which specified the
    number of filters in each convolutional group. They tried *k* = 12, 24, and 32\.
    For ImageNet, they used *k* = 32 with four dense groups. They found that they
    could get comparable results with ResNet networks with half the parameters. For
    example, they trained a DenseNet with comparable parameters to a ResNet50, with
    20 million parameters, and got comparable results to a deeper ResNet101, with
    40 million parameters.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet研究人员引入了一个元参数*k*，它指定了每个卷积组中的滤波器数量。他们尝试了*k* = 12, 24, 和 32。对于ImageNet，他们使用了*k*
    = 32，并设置了四个密集组。他们发现，他们可以用一半的参数得到与ResNet网络相当的结果。例如，他们训练了一个与ResNet50相当参数的DenseNet，参数数量为2000万，并得到了与更深层的ResNet101相当的结果，参数数量为4000万。
- en: 'The following code is an example implementation of a dense group. The number
    of dense residual blocks is specified by the parameter `n_blocks`, the number
    of output filters by `n_filters`, and the compression factor by `compression`.
    For the last group, the lack of a transition block (we’ll look at those next)
    is indicated by setting the parameter `compression` set to `None`:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个密集组的示例实现。密集残差块的数量由参数`n_blocks`指定，输出滤波器的数量由`n_filters`指定，压缩因子由`compression`指定。对于最后一组，由于缺少过渡块（我们将在下一节中讨论），将参数`compression`设置为`None`来表示：
- en: '[PRE0]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Constructs a group of densely connected residual blocks
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建一组密集连接的残差块
- en: ❷ Constructs the interceding transition block
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建中间过渡块
- en: Let’s discuss again why DenseNet and other SOTA models have no final pooling
    of the feature maps before the task component (for example, the classifier). These
    models do feature extraction within the blocks and feature summarization at the
    end of the group, a process we refer to as *feature learning*. Each group summarizes
    the features it has learned to reduce the computational complexity for further
    processing of the feature maps by subsequent groups. The final (nonpooled) feature
    maps of the last group are optimized in size for representing the features as
    a high-dimensional encoding in the latent space. Recall here that in multitask
    models, such as in object detection, it is the latent space that is shared between
    tasks (or in the case of model amalgamation, between model interfaces).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次讨论为什么DenseNet和其他SOTA模型在任务组件（例如，分类器）之前没有对特征图进行最终池化。这些模型在块内进行特征提取，并在组末进行特征汇总，我们称这个过程为*特征学习*。每个组总结它所学习的特征，以减少后续组对特征图进一步处理的计算复杂度。最后一组（非池化）的特征图在大小上进行了优化，以表示在潜在空间中的高维编码。在此提醒，在多任务模型中，例如在目标检测中，潜在空间是在任务之间（或模型融合的情况下，在模型接口之间）共享的。
- en: Once the final feature maps enter the task component, they are pooled one final
    time—but they are pooled in a manner that is optimal for learning the task instead
    of feature summarization. This last pooling step in the task component is the
    bottleneck layer, and the output is referred to as *the low-dimensional embedding*
    of the latent space, which may also be shared with other tasks and models.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦最终特征图进入任务组件，它们将进行最后一次池化——但这次池化的方式是为了学习任务而不是特征汇总。在任务组件中的这个最后池化步骤是瓶颈层，输出被称为潜在空间的*低维嵌入*，这也可能与其他任务和模型共享。
- en: The DenseNet architecture has four dense groups, and each consists of a configurable
    number of dense blocks. Let’s now look into the construction and design of a dense
    block.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet架构有四个密集组，每个组都由可配置数量的密集块组成。现在让我们来看看密集块的结构和设计。
- en: 7.1.2 Dense block
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 密集块
- en: 'The residual block in DenseNet uses the B(1, 3) pattern, which is a 1 × 1 convolution
    followed by a 3 × 3 convolution. However, the 1 × 1 convolution is a linear projection
    instead of a bottleneck: the 1 × 1 expands the number of output feature maps (filters)
    by an expansion factor of 4\. The 3 × 3 then performs a dimensionality reduction,
    restoring the number of output feature maps to the same number as the input feature
    maps.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet中的残差块使用B(1, 3)模式，这是一个1×1卷积后跟一个3×3卷积。然而，1×1卷积是一个线性投影而不是瓶颈：1×1通过4倍的扩展因子扩展了输出特征图（滤波器）的数目。然后3×3执行维度缩减，将输出特征图的数目恢复到与输入特征图相同的数目。
- en: Figure 7.3 depicts the dimensionality expansion and reduction of feature maps
    in the residual dense block. Note that the number and size of the input and output
    feature maps stay the same. Within the block, the 1 × 1 linear projection expands
    the number of feature maps, while the subsequent 3 × 3 convolution does both feature
    extraction and feature map reduction. It is this last convolutional that restores
    the number and size of feature maps at the output to be the same as the input—a
    process called *dimensionality restoration**.*
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3展示了残差密集块中特征图的空间扩展和缩减。请注意，输入和输出特征图的数目和大小保持不变。在块内部，1×1线性投影扩展了特征图的数目，而随后的3×3卷积则同时进行特征提取和特征图缩减。正是这个最后的卷积将输出特征图的数目和大小恢复到与输入相同——这个过程被称为*维度恢复***。
- en: '![](Images/CH07_F03_Ferlitsch.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH07_F03_Ferlitsch.png)'
- en: Figure 7.3 In dimensionality expansion and reduction in convolution layers of
    a residual dense block, the number and size of feature maps at the input and output
    are the same.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 在残差密集块的卷积层中进行维度扩展和缩减时，输入和输出特征图的数目和大小保持不变。
- en: 'Figure 7.4 illustrates the residual dense block, which consists of the following:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4说明了残差密集块，它由以下部分组成：
- en: A 1 × 1 linear projection convolution that increases the number of feature maps
    by four times
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将特征图数目增加四倍的1×1线性投影卷积
- en: A 3 × 3 convolution that both performs feature extraction and restores the number
    of feature maps
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个既执行特征提取又恢复特征图数目的3×3卷积
- en: An operation that concatenates the residual block’s input feature maps and its
    output
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个将残差块的输入特征图和输出特征图连接的操作
- en: '![](Images/CH07_F04_Ferlitsch.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH07_F04_Ferlitsch.png)'
- en: Figure 7.4 Residual dense block with identity shortcut using a concatenation
    operation for feature reuse
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 使用连接操作进行特征重用的具有恒等快捷方式的残差密集块
- en: DenseNet also adopted the modern convention of using a pre-activation batch
    normalization (BN-ReLU-Conv) to improve accuracy. In a post-activation, the ReLU
    activation and batch normalization occur *after* the convolution. In pre-activation,
    the batch normalization and ReLU occur *before* the convolution.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet还采用了现代惯例，使用前激活批量归一化（BN-ReLU-Conv）来提高准确度。在后激活中，ReLU激活和批量归一化发生在卷积之后。在前激活中，批量归一化和ReLU发生在卷积之前。
- en: Previous researchers had found that by moving from a post- to a pre-activation,
    models gained from 0.5 to 2 percent in accuracy. (For example, the ResNet v2 researchers,
    as presented in “Identity Mappings in Deep Residual Networks” [[https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)].)
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 前人研究者发现，通过从后激活转换为前激活，模型在准确度上提高了0.5到2个百分点。（例如，ResNet v2研究者，如“深度残差网络中的恒等映射”[[https://arxiv.org/abs/1603.05027](https://arxiv.org/abs/1603.05027)]中所述。）
- en: 'The following code is an example implementation of a dense residual block,
    which consists of these steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是一个密集残差块的示例实现，它包括以下步骤：
- en: Saving a copy of the input feature maps in the variable `shortcut`.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在变量`shortcut`中保存输入特征图的副本。
- en: A pre-activation 1 × 1 linear projection that increases the number of feature
    maps by four times
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个将特征图数目增加四倍的前激活1×1线性投影
- en: A pre-activation 3 × 3 convolution for feature extraction and restoring the
    number of feature maps
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个用于特征提取和恢复特征图数目的前激活3×3卷积
- en: A concatenation of the saved input feature maps with the output feature maps
    for feature reuse
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将保存的输入特征图与输出特征图进行连接，以实现特征重用
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Remembers the input
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记录输入
- en: ❷ Dimensionality expansion, expands filters by 4 (DenseNet-B)
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 维度扩展，通过4倍扩展滤波器（DenseNet-B）
- en: ❸ 3 × 3 bottleneck convolution with padding='same' to preserve shape of feature
    maps
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用padding='same'填充的3×3瓶颈卷积以保持特征图形状
- en: ❹ Concatenates the input (identity) with the output of the residual block, where
    concatenation provides feature reuse between layers
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将输入（恒等变换）与残差块的输出连接起来，其中连接在层之间提供了特征重用。
- en: 7.1.3 DenseNet macro-architecture
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 DenseNet宏观架构
- en: In the learner component, a *transition block* is inserted between each dense
    group to further reduce computational complexity. The transition block is a strided
    convolution, also referred to as *feature pooling*, used to reduce the overall
    size of the concatenated feature maps (feature reuse) as they move from one dense
    group to the next. Without the reduction, the overall size of the feature maps
    would progressively double per dense block, which would have resulted in an explosion
    in the number of parameters to train. By reducing the number of parameters, a
    DenseNet can go deeper in layers with only a linear increase in the number of
    parameters.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在学习组件中，在每个密集组之间插入一个*过渡块*以进一步减少计算复杂度。过渡块是一个步长卷积，也称为*特征池化*，用于在从一个密集组移动到下一个密集组时减少连接特征图的整体大小（特征重用）。如果没有这种减少，特征图的整体大小会随着每个密集块的逐步增加而逐渐翻倍，这将导致训练参数数量的爆炸性增长。通过减少参数数量，DenseNet可以在参数数量仅线性增加的情况下更深入地扩展层。
- en: Before we look at the architecture of the transition block, let’s first see
    where it fits into the learner component. You can see, in figure 7.5, that the
    learner component consists of four dense groups, and the transition block is between
    each of the dense groups.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们查看过渡块的架构之前，让我们首先看看它在学习组件中的位置。如图7.5所示，学习组件由四个密集组组成，过渡块位于每个密集组之间。
- en: '![](Images/CH07_F05_Ferlitsch.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F05_Ferlitsch.png)'
- en: Figure 7.5 DenseNet macro-architecture showing transition blocks between dense
    groups
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 展示了密集组之间的过渡块的大规模DenseNet架构
- en: Now let’s look up close at a transition block between each dense group.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们近距离观察每个密集组之间的过渡块。
- en: 7.1.4 Dense transition block
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 密集过渡块
- en: 'The transition block consists of two steps:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 过渡块由两个步骤组成：
- en: A 1 × 1 bottleneck convolution that reduces the number of output feature maps
    (channels) by a compression factor *C*.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个1 × 1瓶颈卷积，通过压缩因子*C*减少了输出特征图（通道）的数量。
- en: A strided average pooling that follows the bottleneck and reduces the size of
    each feature map by 75%. When we say *strided*, we generally mean a stride of
    2\. And a stride of 2 will reduce the height and width dimensions of the feature
    by one-half, which reduces the number of pixels by one-quarter (25%).
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在瓶颈之后跟随的步长平均池化，将每个特征图的大小减少75%。当我们说*步长*时，我们通常指的是步长为2。步长为2将特征图的高度和宽度维度减少一半，这将像素数量减少四分之一（25%）。
- en: Figure 7.6 depicts this process. Here, filters / *C* represents the feature
    map compression in the 1 × 1 bottleneck convolution that reduces the *number*
    of feature maps. The average pooling that follows is strided, which reduces the
    *size* of those reduced number of feature maps.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6描述了此过程。在这里，过滤器/*C*代表1 × 1瓶颈卷积中减少特征图*数量*的特征图压缩。随后的平均池化是步长的，它减少了减少数量后的特征图*大小*。
- en: '![](Images/CH07_F06_Ferlitsch.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F06_Ferlitsch.png)'
- en: Figure 7.6 In the dense transition block, feature map dimensionality is reduced
    by both the 1 × 1 bottleneck convolution and the strided average pooling layer.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 在密集过渡块中，特征图维度通过1 × 1瓶颈卷积和步长平均池化层同时减少。
- en: Now, how does that compression actually work? As you can see in figure 7.7,
    we start with input of eight feature maps, each of size *H* × *W*, which, in total,
    can be represented as *H* × *W* × 8\. The compression ratio in the 1 × 1 bottleneck
    convolution is 2\. So the bottleneck takes the input and then outputs half the
    number of feature maps, which is 4 in this example. We can represent that as *H*
    × W × *4*. The strided average pooling then reduces the dimension of the 4 feature
    maps by one-half, resulting in a final output size of 0.5*H* × 0.5*W* × 4.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这种压缩是如何实际工作的呢？如图7.7所示，我们开始时有八个特征图，每个大小为*H* × *W*，总共可以表示为*H* × *W* × 8。1 ×
    1瓶颈卷积中的压缩比是2。因此，瓶颈将输入并输出一半数量的特征图，在这个例子中是4。我们可以将其表示为*H* × *W* × *4*。然后步长平均池化将4个特征图的维度减少一半，最终输出大小为0.5*H*
    × 0.5*W* × 4。
- en: '![](Images/CH07_F07_Ferlitsch.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F07_Ferlitsch.png)'
- en: Figure 7.7 Demonstrating the progress of reducing feature maps (compression)
    in a transition block
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 展示了过渡块中特征图（压缩）减少的过程
- en: 'To compress the number of feature maps, we need to know the number of feature
    maps (channels) coming into the transition block. In the following code example,
    this is obtained by `x.shape[-1]`. We use an index of –1 to refer to the last
    dimension in the shape of the input tensor (*B*, *H*, *W*, *C*), which is the
    number of channels. The number of feature maps in the input tensor is then multiplied
    by the `compression` factor (which is from 0 to 1). Note that in Python, multiplication
    operations are done as floating-point values, so we cast the result back to an
    integer:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要压缩特征图的数量，我们需要知道进入过渡块的特征图数量（通道数）。在下面的代码示例中，这是通过 `x.shape[-1]` 获得的。我们使用索引-1来引用输入张量（*B*,
    *H*, *W*, *C*）的最后一个维度，这是通道数。输入张量中的特征图数量然后乘以`compression`因子（范围从0到1）。请注意，在Python中，乘法操作以浮点值执行，因此我们将结果转换回整数：
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Calculates the reduction (compression) of the number of feature maps (DenseNet-C)
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算特征图数量（DenseNet-C）的减少（压缩）
- en: ❷ 1 × 1 bottleneck convolution using BN-LI-Conv form of batch normalization
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用BN-LI-Conv形式的批量归一化进行1 × 1瓶颈卷积
- en: ❸ Uses mean value (average) when pooling to reduce by 75%
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在池化时使用平均值（平均）来减少75%
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for DenseNet is available on GitHub ([http://mng.bz/6N0o](http://mng.bz/6N0o)).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上提供了使用Idiomatic procedure reuse设计模式为DenseNet编写的完整代码示例 ([http://mng.bz/6N0o](http://mng.bz/6N0o)).
- en: '7.2 Xception: Extreme Inception'
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 Xception：极端Inception
- en: 'The *Xception* (*Extreme Inception*) architecture, as noted previously, was
    introduced by Keras creator François Chollet at Google in 2017 as a proposed further
    improvement over the Inception v3 architecture. In his paper, “Xception: Deep
    Learning with Depthwise Separable Convolutions” ([https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf)),
    Chollet argued that the success of the Inception-style module was based on a factorization
    that substantially decoupled the spatial correlations from the channel correlations.
    This decoupling resulted in fewer parameters while still maintaining representational
    power. He proposed that we could reduce parameters even more, maintaining representational
    power, by fully decoupling the spatial and channel correlations. Don’t worry if
    these ideas on decoupling seem a bit complicated; you’ll find a more detailed
    explanation in section 7.2.5.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '如前所述，*Xception* (*极端Inception*) 架构是由Keras的创造者François Chollet于2017年在谷歌提出的，作为对Inception
    v3架构的进一步改进建议。在他的论文“Xception: Deep Learning with Depthwise Separable Convolutions”
    ([https://arxiv.org/pdf/1610.02357.pdf](https://arxiv.org/pdf/1610.02357.pdf))
    中，Chollet认为Inception风格模块的成功基于一个将空间相关性从通道相关性中大量解耦的因子化。这种解耦导致参数数量减少，同时仍然保持表示能力。他提出，我们可以通过完全解耦空间和通道相关性来进一步减少参数，同时保持表示能力。如果你觉得这些关于解耦的想法有点复杂，你会在7.2.5节中找到一个更详细的解释。'
- en: 'Chollet made another important statement in his paper: he claimed that his
    redesign of the architecture for Xception was actually simpler than Inception’s
    architecture, and could be coded in only 30 to 40 lines using a high-level library
    such as Keras.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: Chollet在他的论文中做出了另一个重要的声明：他声称他对Xception架构的重设计实际上比Inception架构更简单，并且可以使用像Keras这样的高级库仅用30到40行代码来实现。
- en: Chollet based his conclusion on experiments comparing the accuracy of Inception
    v3 to Xception on ImageNet and Google’s internal Joint Foto Tree (JFT) datasets.
    He used the same number of parameters in both models, so he believed any accuracy
    improvement was due to *more-efficient use of the parameters*. The JFT dataset
    consists of 350 million images, and 17,000 categories; Xception outperformed Inception
    by 4.3% on the JFT dataset. In his experiments on ImageNet, which consists of
    1.2 million images and 1000 categories, the difference in accuracy was negligible.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Chollet的结论基于比较Inception v3和Xception在ImageNet和谷歌的内部Joint Foto Tree (JFT)数据集上的准确性的实验。他在两个模型中使用了相同数量的参数，因此他认为任何准确性的提高都是由于*更有效地使用参数*。JFT数据集包含3.5亿张图片和17,000个类别；Xception在JFT数据集上比Inception高出4.3%。在他的ImageNet实验中，该数据集包含1.2百万张图片和1000个类别，准确性的差异可以忽略不计。
- en: 'There were two major changes from Inception v3 to Xception:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 从Inception v3到Xception有两个主要变化：
- en: Reorganization of the Inception architecture’s use of three inception-style
    residual groups (A, B, and C) into an entry, middle, and exit flow instead. Under
    this new approach, the stem group becomes part of the entry, and the classifier
    becomes part of the exit, which reduces the structural complexity of the Inception-style
    residual blocks.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将Inception架构中使用的三个Inception风格的残差组（A、B和C）重新组织为入口、中间和出口流。在这种新方法下，主干组成为入口的一部分，分类器成为出口的一部分，这降低了Inception风格残差块的结构复杂性。
- en: The factorization of a convolution into a spatial separable convolution in an
    Inception v3 block is replaced with a depthwise separable convolution, which reduces
    the number of matrix multiply operations by 83%.
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Inception v3块中将卷积分解为空间可分离卷积的操作被替换为深度可分离卷积，这减少了83%的矩阵乘法操作。
- en: Like Inception v3, Xception uses a post-activation batch normalization (Conv-BN-ReLU).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 与Inception v3一样，Xception使用后激活批量归一化（Conv-BN-ReLU）。
- en: Let’s take a look at the overall macro-architecture, and then look at the details
    of the redesigned components (the entry, exit, and middle flow). At the end of
    this section, we’ll come back to where we started, and I’ll explain the factorization
    of spatial convolutions into depthwise separable convolutions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下整体宏架构，然后看看重新设计组件（入口、出口和中间流）的细节。在本节末尾，我们将回到起点，我会解释空间卷积分解为深度可分离卷积的过程。
- en: 7.2.1 Xception architecture
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 Xception架构
- en: Chollet took the traditional stem-learner-classifier arrangement and regrouped
    it into an entry flow, middle flow, and exit flow. You can see this in figure
    7.8, which shows the Xception architecture, regrouped and retrofitted into the
    procedural reuse design pattern. The entry and middle represent the feature learning,
    and the exit flow represents the classification learning.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: Chollet将传统的主干-学习者-分类器排列重新组合为入口流、中间流和出口流。您可以在图7.8中看到这一点，该图显示了重新组合并回溯到过程重用设计模式的Xception架构。入口和中间代表特征学习，出口流代表分类学习。
- en: While I have read Chollet’s paper multiple times, I can’t find a justification
    for describing the architecture as having an entry, middle, and exit flow. I think
    it would be clearer to just call these three *styles* of residual groups in the
    learner component, maintaining the convention of referencing them as A, B, and
    C. The paper seems to hint that his decision was an attempt to simplify what he
    referred to as the complex architecture of Inception. He wanted this simplification
    so the architecture could be written in 30 to 40 lines of a high-level library,
    like Keras or TensorFlow-Slim, while maintaining the comparable number of parameters.
    In any case, subsequent researchers have not adopted Chollet’s terminology of
    entry, middle, and exit flow.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我已经多次阅读了Chollet的论文，但我找不到描述架构为具有入口、中间和出口流的理由。我认为直接将这些残差组在学习者组件中的三种*风格*称为A、B和C会更清晰。论文似乎暗示他的决定是为了简化他所说的Inception的复杂架构。他希望这种简化使得架构可以用30到40行高级库（如Keras或TensorFlow-Slim）的代码实现，同时保持参数数量相当。无论如何，后续的研究者并没有采用Chollet关于入口、中间和出口流的术语。
- en: '![](Images/CH07_F08_Ferlitsch.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F08_Ferlitsch.png)'
- en: Figure 7.8 The Xception macro-architecture regrouped the major components into
    entry, middle, and exit flow. Here’s how they fit into the stem, learner, and
    task components.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 Xception宏架构将主要组件重新组合为入口、中间和出口流。以下是它们如何适应主干、学习者和任务组件。
- en: As you can see, the stem component is incorporated into the entry flow, and
    the classifier component into the exit flow. The residual convolutional groups
    from the entry to exit flow collectively form the equivalent of the learner component.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，主干组件被纳入入口流，分类器组件被纳入出口流。从入口到出口流的残差卷积组共同构成了学习者组件的等效部分。
- en: 'The skeleton implementation of the Xception architecture shows how the code
    is partitioned into an entry flow, middle flow, and exit flow section. The entry
    flow is further subpartitioned into a stem and body, and the exit flow is further
    subpartitioned into a classifier and body. These partitions are denoted in the
    code template with three top-level functions: `entryFlow``()`, `middleFlow``()`,
    and `exitFlow``()`. The `entryFlow()` function has the nested `stem()` to denote
    the inclusion of the stem in the entry flow, and the `exitFlow``()` has the nested
    function `classifier``()` to denote the inclusion of the classifier in the exit
    flow.'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Xception架构的骨架实现显示了代码是如何分为输入流程、中间流程和输出流程部分的。输入流程进一步细分为主干和主体，输出流程进一步细分为分类器和主体。这些部分在代码模板中用三个顶级函数表示：`entryFlow()`、`middleFlow()`和`exitFlow()`。`entryFlow()`函数包含嵌套的`stem()`函数，表示主干包含在输入流程中，而`exitFlow()`包含嵌套的函数`classifier()`，表示分类器包含在输出流程中。
- en: I’ve left the function body details out for brevity. A complete code rendition
    using the Idiomatic procedure reuse design pattern for Xception is available on
    GitHub ([http://mng.bz/5WzB](https://shortener.manning.com/5WzB)).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁，省略了函数体细节。使用Idiomatic procedure reuse设计模式为Xception提供的完整代码版本可在GitHub上找到 ([http://mng.bz/5WzB](https://shortener.manning.com/5WzB))。
- en: '[PRE3]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The stem component is part of the entry flow.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 主干组件是输入流程的一部分。
- en: ❷ Code removed for brevity
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为了简洁，代码已被删除。
- en: ❸ The stem component is part of the entry flow.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 主干组件是输入流程的一部分。
- en: ❹ Constructs three residual blocks using linear projection
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用线性投影构建三个残差块。
- en: ❺ Middle flow constructs 8 identical residual blocks.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 中间流程构建8个相同的残差块。
- en: ❻ The classifier component is part of the exit flow.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 分类器组件是输出流程的一部分。
- en: ❼ Code removed for brevity
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 为了简洁，代码已被删除。
- en: ❽ Middle flow constructs 8 identical residual blocks.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 中间流程构建8个相同的残差块。
- en: ❾ Creates the input vector of shape (229, 229, 3)
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 创建形状为(229, 229, 3)的输入向量。
- en: ❿ Constructs the entry flow
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 构建输入流程。
- en: ⓫ Constructs the middle flow
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 构建中间流程。
- en: ⓬ Constructs the exit flow for 1000 classes
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ⓬ 构建用于1000个类别的输出流程。
- en: 7.2.2 Entry flow of Xception
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 Xception的输入流程
- en: The entry flow component consists of the stem convolutional group, followed
    by three Xception entry-flow-style residual blocks, successively outputting 128,
    256, and 728 feature maps. Figure 7.9 shows the entry flow and how the stem group
    fits into it as a subcomponent.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 输入流程组件包括主干卷积组，随后是三个Xception输入流程风格的残差块，依次输出128、256和728个特征图。图7.9显示了输入流程以及主干组如何作为子组件嵌入其中。
- en: '![](Images/CH07_F09_Ferlitsch.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F09_Ferlitsch.png)'
- en: Figure 7.9 Xception entry flow micro-architecture
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 Xception输入流程的微架构。
- en: The stem consists of a stack of two 3 × 3 convolutions, as shown in figure 7.10\.
    The second 3 × 3 doubles the number of output feature maps (dimensionality expansion),
    and one of the convolutions is strided for feature pooling (dimensionality reduction).
    For Xception, the number of filters for the stack is 32 and 64, respectively,
    which is a common convention. The first convolution in the stack, which is strided,
    would’ve been chosen to reduce the number of parameters to the second 3 × 3 convolution
    in the stack. The other convention is with the second 3 × 3 convolution, which
    is strided for feature summarization, and forgoes the reduction in parameters.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 主干部分由两个3 × 3卷积层的堆叠组成，如图7.10所示。第二个3 × 3卷积层将输出特征图的数量加倍（维度扩展），其中一个卷积层采用步进以进行特征池化（维度减少）。对于Xception，堆叠中的滤波器数量分别为32和64，这是一个常见的约定。堆叠中的第一个卷积层，采用步进，原本是为了减少堆叠中第二个3
    × 3卷积层的参数数量。另一种约定是在第二个3 × 3卷积层，采用步进进行特征汇总，并放弃了参数减少。
- en: '![](Images/CH07_F10_Ferlitsch.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F10_Ferlitsch.png)'
- en: Figure 7.10 Xception stem group’s layer construction for the stack of two 3
    × 3 convolutions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 Xception主干组的层构建，用于两个3 × 3卷积层的堆叠。
- en: Next is the entry-flow-style residual block, shown in figure 7.11\. The entry
    flow style uses a B(3, 3) residual block followed by a max pooling and 1 × 1 linear
    projection on the identity link. The 3 × 3 convolutions are depthwise separable
    convolutions (`SeparableConv2D`), in contrast to Inception v3, which used a combination
    of normal and spatially separable convolutions. The max pooling uses a 3 × 3 pooling
    size, thus outputting the maximum value from a 9-pixel window (versus 4 pixels
    for 2 × 2). Note that the 1 × 1 linear projection is also strided, reducing the
    size of the feature maps, to match the size of reduction of the feature maps from
    the residual path by the max pooling layer.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是入口流式残差块，如图7.11所示。入口流式使用一个B(3, 3)残差块，随后在身份链路上进行最大池化和1 × 1线性投影。3 × 3卷积是深度可分离卷积（`SeparableConv2D`），与Inception
    v3不同，后者使用了正常和空间可分离卷积的组合。最大池化使用3 × 3的池化大小，因此从9像素窗口中输出最大值（与2 × 2的4像素相比）。请注意，1 ×
    1线性投影也是步进的，以减少特征图的大小，以匹配最大池化层从残差路径中减少的特征图大小。
- en: '![](Images/CH07_F11_Ferlitsch.png)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F11_Ferlitsch.png)'
- en: Figure 7.11 Xception residual block with linear projection shortcut
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 带线性投影快捷方式的Xception残差块
- en: 'Now let’s look at an example implementation of the entry-flow-style residual
    block. This codes for the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看一个入口流式残差块的示例实现。以下代码表示：
- en: A 1 × 1 linear projection to increase the number of feature maps and reduce
    the size to match output from the residual path (`shortcut`)
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个1 × 1线性投影来增加特征图的数量并减小大小以匹配残差路径的输出（`shortcut`）
- en: Two 3 × 3 depthwise separable convolutions
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 两个3 × 3深度可分离卷积
- en: A matrix add operation of the feature maps from the linear projection link (`shortcut`)
    to the output of the residual path
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 线性投影链路（`shortcut`）的特征图与残差路径输出的矩阵加法操作
- en: '[PRE4]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Projection shortcut uses strided convolution to reduce the size of feature
    maps while doubling the number of filters to match output of block for the matrix
    add operation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 投影快捷方式使用步进卷积来减小特征图的大小，同时将滤波器数量加倍以匹配块的输出，以便进行矩阵加法操作。
- en: ❷ First depthwise separable convolution
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 首个深度可分离卷积
- en: ❸ Second depthwise separable convolution
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第二个深度可分离卷积
- en: ❹ Reduces size of feature maps by 75%
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将特征图的大小减少75%
- en: ❺ Adds the projection shortcut to the output of the block
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将投影快捷方式添加到块的输出
- en: 7.2.3 Middle flow of Xception
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 Xception的中间流
- en: The middle flow consists of eight middle-flow-style residual blocks, each outputting
    728 feature maps. It is the convention to maintain the same number of input/output
    feature maps across blocks in a group; while between groups, the number of feature
    maps progressively increases. In contrast, with Xception, the number of output
    feature maps in the entry and middle flow stayed the same, instead of increasing.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 中间流由八个中间流式残差块组成，每个块输出728个特征图。在组内保持输入/输出特征图数量相同是惯例；而在组之间，特征图的数量逐渐增加。相比之下，Xception中入口和中间流的输出特征图数量保持不变，而不是增加。
- en: The middle-flow-style residual block, shown in figure 7.12, uses eight B(3,
    3, 3) residual blocks. Unlike the entry-flow residual block, there is no 1 × 1
    strided linear projection on the identity link, since the number of input and
    output feature maps stays the same across all blocks, and no pooling occurs.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 中间流式残差块，如图7.12所示，使用八个B(3, 3, 3)残差块。与入口流残差块不同，在所有块之间输入和输出特征图的数量保持相同，因此没有池化操作，也没有在身份链路上进行1
    × 1步进线性投影。
- en: '![](Images/CH07_F12_Ferlitsch.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F12_Ferlitsch.png)'
- en: Figure 7.12 The Xception middle flow micro-architecture has a sequence of eight
    identical residual blocks.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 Xception中间流微架构由八个相同的残差块组成。
- en: Now let’s take a look at what’s happening in each of those residual blocks.
    Figure 7.13 shows the three 3 × 3 convolutions, which are depthwise separable
    convolutions (`SeparableConv2D`). (Again, we’ll soon get to what exactly a depthwise
    separable convolution is.)
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看每个残差块中发生了什么。图7.13显示了三个3 × 3卷积，它们是深度可分离卷积（`SeparableConv2D`）。（我们很快就会了解到深度可分离卷积究竟是什么。）
- en: '![](Images/CH07_F13_Ferlitsch.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F13_Ferlitsch.png)'
- en: 'Figure 7.13 Residual block middle flow with identity shortcut: the number and
    size of both the input feature maps and the residual path are the same for the
    matrix add operation.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 带身份快捷方式的残差块中间流：矩阵加法操作中输入特征图和残差路径的数量和大小相同。
- en: 'The following code is an example implementation of the middle-flow-style residual
    block, where the B(3, 3, 3) style is implemented using depthwise separable convolutions
    (`SeparableConv2D`):'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是中间流样式残差块的示例实现，其中B(3, 3, 3)样式使用深度可分离卷积(`SeparableConv2D`)实现：
- en: '[PRE5]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Sequence of three 3 × 3 depthwise separable convolutions
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 三个3 × 3深度可分离卷积的序列
- en: ❷ Adds the identity link to the output of the block
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将恒等连接添加到块的输出
- en: 7.2.4 Exit flow of Xception
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 Xception的出口流
- en: Now for the exit flow. It consists of a single exit-flow-style residual block,
    followed by a convolutional (nonresidual) block, and then the classifier. The
    classifier group, as shown in figure 7.14, is a subcomponent of the exit flow.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看出口流。它由一个出口流样式的残差块组成，后面跟着一个卷积块（非残差块），然后是分类器。如图7.14所示，分类器组是出口流的子组件。
- en: '![](Images/CH07_F14_Ferlitsch.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F14_Ferlitsch.png)'
- en: Figure 7.14 The Xception exit flow progressively increases the number of feature
    maps.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 Xception出口流逐步增加特征图的数量。
- en: The exit flow takes as input 728 feature maps, and output from the middle flow,
    and progressively increases the number of feature maps to 2048 before the classifier.
    Compare this with the convention for large CNNs, such as Inception v3 and ResNet,
    which generate 2048 final feature maps before the bottleneck layer, creating what’s
    known as *high-dimensional encoding*.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 出口流以728个特征图和中间流的输出作为输入，并在分类器之前逐步增加特征图的数量到2048。与Inception v3和ResNet等大型CNN的常规做法相比，这些CNN在瓶颈层之前生成2048个最终特征图，形成了所谓的*高维编码*。
- en: Let’s take a close-up look at that single exit-flow-style residual block, in
    figure 7.15\. This residual block is a B(3,3) with the two convolutions outputting
    728 and 1024 feature maps, respectively. The two convolutions are followed by
    a max pooling with a pooling size of 3 × 3, and then a 1 × 1 linear projection
    for the identity link. In contrast to the middle flow, the exit-flow block increases
    the number of feature maps and delays pooling between the single residual and
    convolution block in the exit flow.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们仔细看看那个单独的出口流样式残差块，如图7.15所示。这个残差块是一个B(3,3)，两个卷积分别输出728和1024个特征图。两个卷积后面跟着一个3
    × 3的最大池化，然后是一个1 × 1的线性投影用于恒等连接。与中间流相比，出口流块增加了特征图的数量，并在出口流中的单个残差块和卷积块之间延迟了池化。
- en: '![](Images/CH07_F15_Ferlitsch.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F15_Ferlitsch.png)'
- en: Figure 7.15 The Xception exit flow residual block with linear projection shortcut
    delays the progression of increasing the final number of feature maps and pooling.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 Xception出口流残差块带有线性投影快捷连接延迟了最终特征图数量增加和池化的进程。
- en: Note that the exit flow residual block structure is identical to the entry flow,
    except the exit flow style does a dimensionality expansion with the block, going
    from 728 to 1024 feature maps, while the entry flow does not do any dimensionality
    expansion.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，出口流残差块结构与入口流相同，除了出口流样式在块中进行维度扩展，从728个特征图扩展到1024个特征图，而入口流不进行任何维度扩展。
- en: Now for the exit flow convolutional block, which follows the residual block,
    and is shown in figure 7.16\. This block consists of two 3 × 3 depthwise separable
    convolutions, each doing a dimensionality expansion. That expansion increases
    the number of feature maps to 1156 and 2048, respectively, which completes the
    delayed progression of increasing the final number of feature maps before the
    bottleneck layer.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看出口流卷积块，它位于残差块之后，如图7.16所示。此块由两个3 × 3深度可分离卷积组成，每个卷积都进行维度扩展。这种扩展将特征图的数量增加到1156和2048，分别，这完成了在瓶颈层之前增加最终特征图数量的延迟增长。
- en: '![](Images/CH07_F16_Ferlitsch.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F16_Ferlitsch.png)'
- en: Figure 7.16 The Xception exit flow convolutional block completes the delayed
    progression of the number of final feature maps.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 Xception出口流卷积块完成了最终特征图数量的延迟增长。
- en: The final group in the exit flow is the classifier, composed of a `GlobalAveragePooling2D`
    layer, which pools and flattens the final feature maps into a 1D vector, followed
    by a `Dense` layer with softmax activation for the classification.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 出口流的最后一个组是分类器，由一个`GlobalAveragePooling2D`层组成，该层将最终特征图池化和展平成一个1D向量，然后是一个具有softmax激活的分类`Dense`层。
- en: 7.2.5 Depthwise separable convolution
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.5 深度可分离卷积
- en: As promised, we are finally going to get to the bottom of the depthwise separable
    convolution in the Xception architecture. Since their introduction, depthwise
    separable convolutions have been frequently used in convolutional neural networks,
    because of their ability to be computationally less expensive while maintaining
    representational power. Originally proposed by Laurent Sifre and Stephane Mallat
    in 2014 while working at Google Brain (see [https://arxiv.org/abs/1403.1687](https://arxiv.org/abs/1403.1687)),
    depthwise separable convolutions have since been studied and adopted in a variety
    of SOTA models, including Xception, MobileNet, and ShuffleNet.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 如承诺的那样，我们终于要深入探讨Xception架构中的深度可分离卷积了。自从它们被引入以来，深度可分离卷积在卷积神经网络中得到了广泛的应用，因为它们能够在保持表示能力的同时降低计算成本。深度可分离卷积最初由Laurent
    Sifre和Stéphane Mallat于2014年在Google Brain工作期间提出（参见[https://arxiv.org/abs/1403.1687](https://arxiv.org/abs/1403.1687)），自那时起，深度可分离卷积在各种SOTA模型中得到了研究和应用，包括Xception、MobileNet和ShuffleNet。
- en: 'Put simply, a depthwise separable convolution factors a 2D kernel into two
    2D kernels; the first is a depthwise convolution, and the second is a pointwise
    convolution. To fully understand this, we first need to understand two related
    concepts: depthwise convolution and pointwise convolution, from which a depthwise
    convolution is constructed.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，深度可分离卷积将一个2D核分解为两个2D核；第一个是深度卷积，第二个是点卷积。为了完全理解这一点，我们首先需要了解两个相关概念：深度卷积和点卷积，深度卷积就是由这两个概念构建的。
- en: 7.2.6 Depthwise convolution
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.6 深度卷积
- en: In a *depthwise convolution*, the kernel is split into a single *H* × *W* ×
    1 kernel, one per channel, with each kernel operating on a single channel instead
    of across all channels. In this arrangement, the cross-channel relationships are
    decoupled from the spatial relationships. As Chollet suggested, fully decoupling
    the spatial and channel convolutions results in fewer matmul operations, and accuracy
    comparable to that of models with no decoupling and normal convolution, as well
    as of models with partial decoupling and spatial separable convolution.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在*深度卷积*中，核被分割成一个单独的*H* × *W* × 1核，每个通道一个，每个核只对一个通道进行操作，而不是对所有通道进行操作。在这种安排中，跨通道关系与空间关系解耦。正如Chollet所建议的，完全解耦空间和通道卷积会导致更少的matmul操作，并且精度与没有解耦和正常卷积的模型相当，以及与部分解耦和空间可分离卷积的模型相当。
- en: So, in the RGB example with a 3 × 3 kernel shown in figure 7.17, a depthwise
    convolution would be three 3 × 3 × 1 kernels. The number of multiply operations
    as the kernel is moved is the same as in the normal convolution (for example,
    27 for 3 × 3 on three channels). The output, however, is a D-depth feature map,
    rather than a 2D (`depth=1`) feature map.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在图7.17中展示的RGB示例中，使用3 × 3核，深度卷积将会有三个3 × 3 × 1的核。当核移动时，乘法操作的次数与正常卷积相同（例如，在三个通道上为27）。然而，输出是一个D深度的特征图，而不是一个2D（`depth=1`）的特征图。
- en: '![](Images/CH07_F17_Ferlitsch.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F17_Ferlitsch.png)'
- en: Figure 7.17 In this depthwise convolution, the kernel gets split into single
    *H* × *W* × 1 kernels.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 在这个深度卷积中，核被分割成单个*H* × *W* × 1核。
- en: 7.2.7 Pointwise convolution
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.7 点卷积
- en: The output from a depthwise convolution is then passed as the input to a *pointwise
    convolution*, which forms a depthwise separable convolution. The pointwise convolution
    performs the decoupled spatial convolution. The pointwise convolution combines
    the outputs of the depthwise convolution and expands the number of feature maps
    to match the specified number of filters (feature maps). The combination outputs
    the same number of feature maps as a normal or separable convolution (89), but
    with fewer matrix multiply operations (83% reduction).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 深度卷积的输出随后作为*点卷积*的输入，形成深度可分离卷积。点卷积执行解耦的空间卷积。点卷积结合深度卷积的输出，并将特征图的数量扩展到匹配指定的过滤器（特征图）数量。组合输出与正常或可分离卷积（89）相同数量的特征图，但矩阵乘法操作更少（减少了83%）。
- en: A pointwise convolution, shown in figure 7.18, has a 1 × 1 × *D* (number of
    channels). It will iterate through each pixel producing an *N* × *M* × 1 feature
    map, which replaces the *N* × *M* × *D* feature map.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 点卷积，如图7.18所示，具有1 × 1 × *D*（通道数）。它将遍历每个像素，生成一个*N* × *M* × 1的特征图，这取代了*N* × *M*
    × *D*的特征图。
- en: '![](Images/CH07_F18_Ferlitsch.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F18_Ferlitsch.png)'
- en: Figure 7.18 Pointwise convolution
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 点卷积
- en: In the pointwise convolution, we use 1 × 1 × D kernels, one for each output.
    As in the previous example in figure 7.17, if our output is 256 filters (feature
    maps), we will use 256 1 × 1 × D kernels.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在点卷积中，我们使用1 × 1 × D核，每个输出一个。正如图7.17中的前一个例子一样，如果我们的输出是256个滤波器（特征图），我们将使用256个1
    × 1 × D核。
- en: In the RGB example using a 3 × 3 × 3 kernel for the depthwise convolution in
    figure 7.17, we have 27 multiply operations each time the kernel moves. This would
    be followed by a 1 × 1 × 3 × 256 (where 256 is the number of output filters)—which
    is 768\. The total number of multiply operations would be 795, instead of 6912
    for a normal convolution and 4608 for a spatial separable convolution.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.17中使用的RGB示例中，深度卷积使用了3 × 3 × 3核，每次核移动时都有27次乘法操作。这将随后是一个1 × 1 × 3 × 256（其中256是输出滤波器的数量）——这是768。总的乘法操作数将是795，而不是正常卷积的6912次和空间可分离卷积的4608次。
- en: In the Xception architecture, the spatial separable convolutions in the inception
    module are replaced with a depthwise separable convolution, reducing computational
    complexity (number of multiply operations) by 83%. A complete code rendition using
    the Idiomatic procedure reuse design pattern for Xception is on GitHub ([http://mng.bz/5WzB](http://mng.bz/5WzB)).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在Xception架构中，inception模块中的空间可分离卷积被深度可分离卷积所取代，通过83%的计算复杂度（乘法操作数）减少。使用Idiomatic
    procedure reuse设计模式对Xception的完整代码实现可在GitHub上找到（[http://mng.bz/5WzB](http://mng.bz/5WzB)）。
- en: '7.3 SE-Net: Squeeze and excitation'
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 SE-Net：挤压和激励
- en: Now we will turn to another alternative connectivity design, the *squeeze-excitation-scale
    pattern*, or *SE-Net*, which can be added to existing residual networks to increase
    accuracy by adding only a few parameters.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将转向另一种替代连接设计，即*squeeze-excitation-scale pattern*或*SE-Net*，它可以通过仅添加少量参数来添加到现有的残差网络中以提高准确率。
- en: Introducing the pattern in “Squeeze-and-Excitation Networks” ([https://arxiv.org/
    abs/1709.01507](https://arxiv.org/abs/1709.01507)), Jie Hu et al. explained that
    previous improvements to models focused on spatial relationships between convolutional
    layers. So they decided to take a different tack and investigate a new network
    design based on the relationship between *channels*. Their idea was that the feature
    recalibration could use global information to selectively emphasize important
    features and de-emphasize less-important features.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在“Squeeze-and-Excitation Networks”中引入了这种模式（[https://arxiv.org/abs/1709.01507](https://arxiv.org/abs/1709.01507)），Jie
    Hu等人解释说，之前对模型的改进主要集中在卷积层之间的空间关系上。因此，他们决定采取不同的方法，并研究基于*通道*之间关系的新网络设计。他们的想法是，特征重校准可以使用全局信息来选择性地强调重要特征并降低不太重要的特征的重要性。
- en: To achieve the ability to selectively emphasize features, the authors came up
    with the concept of adding a *squeeze-excitation* (*SE*) *link* inside a residual
    block. This block would go between the output of the convolution layer (or layers),
    and the matrix add operation with the identity link. This concept won the 2017
    ILSVRC competition for ImageNet.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现选择性强调特征的能力，作者提出了在残差块内添加一个*squeeze-excitation* (*SE*) *链接*的概念。这个块将位于卷积层（或多个层）的输出和与恒等链接的矩阵加法操作之间。这个概念赢得了2017年ILSVRC竞赛的ImageNet。
- en: 'Their ablation study indicated several benefits of the SE-Net approach, including
    these:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 他们的消融研究指出了SE-Net方法的一些好处，包括以下这些：
- en: Can be added to existing SOTA architectures, such as ResNet, ResNeXt, and Inception.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以添加到现有的SOTA架构中，例如ResNet、ResNeXt和Inception。
- en: 'Adds a minimal increase in parameters while achieving higher accuracy results.
    For example:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实现更高准确率的同时，参数增加最小。例如：
- en: ImageNet top-5 error rate was 7.48% for ResNet50 and 6.62% for SE-ResNet50
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNet50的ImageNet top-5错误率为7.48%，SE-ResNet50为6.62%
- en: ImageNet top-5 error rate was 5.9% for ResNeXt50 and 5.49% for SE-ResNeXt50
  id: totrans-182
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNeXt50的ImageNet top-5错误率为5.9%，SE-ResNeXt50为5.49%
- en: ImageNet top-5 error rate was 7.89% for Inception and 7.14% for SE-Inception
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Inception的ImageNet top-5错误率为7.89%，SE-Inception为7.14%
- en: 7.3.1 Architecture of SE-Net
  id: totrans-184
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 SE-Net的架构
- en: SE-Net architecture, shown in figure 7.19, consists of an existing residual
    network that is then retrofitted by inserting an SE link into the residual blocks.
    The retrofitted ResNet and ResNeXt architectures are referred to as *SE-ResNet*
    and *SE-ResNeXt*, respectively.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: SE-Net架构，如图7.19所示，由一个现有的残差网络组成，然后通过在残差块中插入SE链接进行改造。改造后的ResNet和ResNeXt架构分别称为*SE-ResNet*和*SE-ResNeXt*。
- en: '![](Images/CH07_F19_Ferlitsch.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH07_F19_Ferlitsch.png)'
- en: Figure 7.19 SE-Net macro-architecture showing addition of SE-Link to each residual
    block
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.19 SE-Net 宏架构展示了将 SE-链接添加到每个残差块
- en: 7.3.2 Group and block of SE-Net
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 SE-Net 的组和块
- en: If we break down the macro-architecture, we find that each convolutional group
    in figure 7.19 consists of one or more residual blocks, composing a residual group.
    Each residual block has an SE link. This close-up look at the residual group is
    depicted in figure 7.20.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们分解宏观架构，我们会发现图 7.19 中的每个卷积组都由一个或多个残差块组成，构成一个残差组。每个残差块都有一个 SE 链接。这种对残差组的近距离观察在图
    7.20 中展示。
- en: '![](Images/CH07_F20_Ferlitsch.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F20_Ferlitsch.png)'
- en: Figure 7.20 In the residual group, each residual block has an inserted SE link.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.20 在残差组中，每个残差块都插入了一个 SE 链接。
- en: Now let’s break down the residual group. Figure 7.21 shows how the SE link is
    inserted into a residual block, between the convolution layer(s) output and the
    matrix add operation.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们分解残差组。图 7.21 展示了 SE 链接是如何插入到残差块中，位于卷积层（s）输出和矩阵加法操作之间。
- en: '![](Images/CH07_F21_Ferlitsch.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F21_Ferlitsch.png)'
- en: Figure 7.21 Residual block showing how the SE link is inserted between the residual
    path and the matrix add operation (denoted as *Add*) for the identity link
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.21 残差块展示了 SE 链接是如何插入到残差路径和矩阵加法操作（标记为 *Add*）之间的（对于身份链接）
- en: The following code is an example implementation of adding an SE link to a ResNet
    residual block. At the end of the block, a call to `squeeze_excite_link()` is
    inserted between the output from the B(3,3) output and the matrix add operation
    (`Add()`). In the `squeeze_excite_link()` function, we implement the SE link (detailed
    in the next subsection).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是向 ResNet 残差块添加 SE 链接的示例实现。在块的末尾，在 B(3,3) 输出和矩阵加法操作（`Add()`）之间插入对 `squeeze_excite_link()`
    的调用。在 `squeeze_excite_link()` 函数中，我们实现了 SE 链接（在下一小节中详细介绍）。
- en: The parameter `ratio` is the amount (ratio) of dimensionality reduction on the
    squeeze operation on the input prior to the subsequent dimensionality restoration
    by the excitation operation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 参数 `ratio` 是在激励操作之前对输入进行挤压操作的维度降低量（比率）。
- en: '[PRE6]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ 1 × 1 convolution for dimensionality reduction
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于维度降低的 1 × 1 卷积
- en: ❷ 3 × 3 convolution for bottleneck layer
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 瓶颈层使用 3 × 3 卷积
- en: ❸ 1 × 1 convolution increases filters by 4 times for dimensionality restoration
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 1 × 1 卷积通过增加过滤器数量 4 倍来实现维度恢复
- en: ❹ Passes the output through the squeeze-and-excitation link
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将输出通过挤压-激励链接
- en: ❺ Adds the identity link (input) to the output of the residual block
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将身份链接（输入）添加到残差块的输出
- en: 7.3.3 SE link
  id: totrans-203
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 SE 链接
- en: Now, let’s go into detail on the SE link (figure 7.22). The link consists of
    three layers. The first two layers perform the squeeze operation. A global average
    pooling is used to reduce each input feature map (channel) to a single value,
    outputting a 1D vector of size *C* (channels), which is then reshaped into a 1-×-1-pixel
    2D matrix of size *C* (channels). The dense layer then further reduces the output
    by the reduction ratio *r*, resulting in a 1-×-1-pixel 2D matrix of size *C* /
    *r* (channels).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们详细探讨 SE 链接（图 7.22）。该链接由三个层组成。前两层执行挤压操作。使用全局平均池化将每个输入特征图（通道）减少到一个单一值，输出一个大小为
    *C*（通道）的 1D 向量，然后将其重塑为一个大小为 *C*（通道）的 1-×-1 像素的 2D 矩阵。密集层随后通过减少比率 *r* 进一步减少输出，结果是一个大小为
    *C* / *r*（通道）的 1-×-1 像素的 2D 矩阵。
- en: '![](Images/CH07_F22_Ferlitsch.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH07_F22_Ferlitsch.png)'
- en: Figure 7.22 Squeeze-excitation block showing the squeeze, excitation, and then
    scale operation
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.22 挤压-激励块展示了挤压、激励然后缩放操作
- en: The squeezed output is then passed to the third layer, which performs the excitation
    by restoring to the number of channels (*C*) inputted to the link. Note that this
    is comparable to using a 1 × 1 linear projection convolution, but is instead done
    with a dense layer.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 挤压后的输出随后传递到第三层，该层通过恢复到链接输入的通道数（*C*）来进行激励。请注意，这与使用 1 × 1 线性投影卷积相当，但这里使用的是密集层。
- en: The final step is a scale operation that consists of an identity link from the
    input, where the 1 × 1 × *C* vector from the squeeze-excitation operation is matrix-multiplied
    against the input (*H* × *W* × *C* ). After the scale operation, the output dimension
    (number and size of feature maps) is restored to the original dimension of the
    input (scale).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是一个缩放操作，它由一个从输入的身份链接组成，其中来自挤压-激励操作的 1 × 1 × *C* 向量与输入（*H* × *W* × *C*）进行矩阵乘法。缩放操作之后，输出维度（特征图的数量和大小）恢复到输入的原始维度（缩放）。
- en: 'Now let’s see an example implementation of the SE link, consisting of the squeeze,
    excitation, and scale operations. Note the `Reshape` operation after `GlobalAveragePooling2D`
    to convert the pooled 1D vector into a 1-×-1-pixel 2D vector for the subsequent
    two `Dense` layers, which perform the squeeze and excitation operations, respectively.
    The 1 × 1 × *C* matrix produced by the excitation is then matrix-multiplied with
    the input (`shortcut`), for the scale operation:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看SE链接的一个示例实现，包括挤压、激励和缩放操作。注意在`GlobalAveragePooling2D`之后的`Reshape`操作，将池化的1D向量转换为1-×-1像素的2D向量，以便后续的两个`Dense`层执行挤压和激励操作。激励产生的1
    × 1 × *C*矩阵随后与输入（`shortcut`）进行矩阵乘法，以进行缩放操作：
- en: '[PRE7]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Gets the number of feature maps (filters) in the input to the SE link
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取SE链接输入中的特征图（滤波器）数量
- en: ❷ Squeeze operation for dimensionality reduction using global average pooling,
    which will output a 1D vector
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用全局平均池化进行维度减少的挤压操作，将输出一个1D向量
- en: ❸ Reshapes the output into 1 × 1 feature maps (1 × 1 × C)
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将输出重塑为1 × 1特征图（1 × 1 × C）
- en: ❹ Reduces the number of filters (1 × 1 × C / r) by the reduction ratio
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过减少比例将滤波器数量（1 × 1 × C / r）减少
- en: ❺ Excitation operation for dimensionality restoration by restoring the number
    of filters (1 × 1 × C)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过恢复滤波器数量（1 × 1 × C）进行维度恢复的激励操作
- en: ❻ Scale operation, multiply the squeeze/excitation output with the input (W
    × H × C)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 缩放操作，将挤压/激励输出与输入（W × H × C）相乘
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for SE-Net is on GitHub ([http://mng.bz/vea7](https://shortener.manning.com/vea7)).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic程序重用设计模式在GitHub上提供了一个SE-Net的完整代码实现（[http://mng.bz/vea7](https://shortener.manning.com/vea7)）。
- en: Summary
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Feature reuse in DenseNet replaces the matrix addition with a feature map concatenation
    of the input to the output from a residual block. This classification increases
    accuracy over existing SOTA models.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: DenseNet中的特征重用通过将残差块的输入到输出的特征图拼接代替矩阵加法。这种分类提高了现有SOTA模型的准确性。
- en: Using 1 × 1 convolutions in DenseNet to learn is the best way to upsample and
    downsample feature maps for a specific dataset.
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在DenseNet中使用1 × 1卷积来学习是针对特定数据集进行上采样和下采样的最佳方式。
- en: Further refactoring a spatial separable convolution to a depthwise separable
    convolution in Xception further reduces computational cost while maintaining representational
    equivalence.
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Xception中将空间可分离卷积进一步重构为深度可分离卷积，进一步降低计算成本，同时保持表示等价。
- en: Adding a squeeze-excite-scale pattern in SE-Net to existing residual networks
    increases the accuracy while adding only a few parameters.
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在SE-Net中添加squeeze-excite-scale模式，同时仅增加少量参数，可以提高准确性。
