- en: '8 Platform capabilities II: Enabling teams to experiment'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 平台功能 II：使团队能够进行实验
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章节涵盖
- en: Enabling teams by providing release strategies capabilities
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过提供发布策略功能来使团队能力得到提升
- en: Identifying the challenges of using Kubernetes built-in mechanisms to implement
    release strategies
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别使用 Kubernetes 内置机制实现发布策略的挑战
- en: Using Knative Serving advanced traffic management to release our cloud-native
    applications
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Knative Serving 高级流量管理来发布我们的云原生应用程序
- en: Leveraging Argo Rollouts out-of-the-box release strategies
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用 Argo Rollouts 的开箱即用发布策略
- en: In chapter 7, we looked at how enabling development teams with application-level
    APIs can reduce the cognitive load on developers to solve common distributed application
    challenges while at the same time enabling platform teams to wire and configure
    these components to be accessible for applications to consume. We also evaluated
    using feature flags to enable developers to keep releasing new features and enable
    other teams closer to the business to decide when these new features are exposed
    to customers.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章中，我们探讨了如何通过为开发团队提供应用级 API 来降低开发者解决常见分布式应用程序挑战的认知负荷，同时使平台团队能够连接和配置这些组件，以便应用程序可以访问。我们还评估了使用功能标志，使开发者能够继续发布新功能，并使更接近业务的团队决定何时将这些新功能向客户公开。
- en: In this chapter, we will look at how introducing different release strategies
    can help the organization catch errors earlier in the process, validate assumptions,
    and enable teams to experiment with different versions of the same application
    running simultaneously.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨引入不同的发布策略如何帮助组织在流程中更早地捕捉错误，验证假设，并使团队能够同时实验同一应用程序的不同版本。
- en: We want to avoid teams being worried about deploying a new version of your services,
    as this slows down your release cadence and causes stress to everyone involved
    in the release process. Reducing risk and having the proper mechanisms to deploy
    new versions drastically improves confidence in the system. It also reduces the
    time from a requested change until it is live in front of your users. New releases
    with fixes and new features directly correlate to business value, because software
    is not valuable unless it serves our company’s users.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望避免团队担心部署您服务的全新版本，因为这会减慢您的发布节奏，并给参与发布过程的每个人带来压力。降低风险并拥有适当的机制来部署新版本，可以极大地提高对系统的信心。它还缩短了从请求更改到在用户面前上线的时间。带有修复和新功能的全新发布与商业价值直接相关，因为软件如果不为我们公司的用户提供服务，就没有价值。
- en: 'While Kubernetes built-in resources such as deployments, services, and ingresses
    provide us with the basic building blocks to deploy and expose our services to
    our users, a lot of manual and error-prone work must happen to implement well-known
    release strategies. For these reasons, the cloud-native communities have created
    specialized tools to help teams be more productive by providing mechanisms to
    implement the most common release strategy patterns we will discuss in this chapter.
    This chapter is divided into three main sections:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Kubernetes 内置资源如部署、服务和入口为我们提供了部署和向用户公开服务的基本构建块，但为了实现众所周知的发布策略，必须进行大量手动且容易出错的工作。因此，云原生社区创建了专门的工具，通过提供机制来实现本章中我们将讨论的最常见的发布策略模式，以帮助团队提高生产力。本章节分为三个主要部分：
- en: 'Release strategies fundamentals:'
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发布策略基础：
- en: Canary releases, blue/green deployments, and A/B testing
  id: totrans-11
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金丝雀发布、蓝/绿部署和 A/B 测试
- en: Limitations and complexities of using Kubernetes built-in mechanisms
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 内置机制的限制和复杂性
- en: 'Knative Serving: Autoscaling, advanced traffic management, and release strategies'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative Serving：自动缩放、高级流量管理和发布策略
- en: Introduction to Knative Serving
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative Serving 简介
- en: Release strategies in action with Knative Serving and the Conference application
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Knative Serving 和 Conference 应用程序的实际发布策略
- en: 'Argo Rollouts: Release strategies automated with GitOps'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo Rollouts：使用 GitOps 自动化的发布策略
- en: Introducing Argo Rollouts
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍 Argo Rollouts
- en: Argo Rollouts and progressive delivery
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo Rollouts 和渐进式交付
- en: The first section of this chapter covers the most common and well-documented
    release strategies from a high level, and we’ll quickly look at why implementing
    these release strategies with Kubernetes building blocks can be challenging. Section
    8.2 looks at Knative Serving, which provides higher-level building blocks that
    highly simplify how to implement these release strategies while at the same time
    providing advanced traffic management and dynamic autoscaling for our workloads.
    Section 8.3 introduces Argo Rollouts, another project from the Argo family that
    focuses on enabling teams with out-of-the-box release strategies and progressive
    delivery. Let’s start covering the fundamentals of release strategies.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的第一部分从高层次概述了最常见且记录良好的发布策略，我们将快速探讨为什么使用Kubernetes构建块实现这些发布策略可能具有挑战性。第8.2节探讨了Knative
    Serving，它提供了更高层次的构建块，极大地简化了实现这些发布策略的方法，同时为我们的工作负载提供高级流量管理和动态自动缩放。第8.3节介绍了Argo
    Rollouts，这是Argo家族的另一个项目，专注于为团队提供开箱即用的发布策略和渐进式交付。让我们开始介绍发布策略的基础。
- en: 8.1 Release strategies fundamentals
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 发布策略基础
- en: If you look for the most common release strategies teams implement to promote
    services to sensitive environments, you will find canary releases, blue/green
    deployments, and A/B testing. Each release strategy has a different purpose and
    can be applied to various scenarios. In the following short sections, we will
    look at what is expected for each release strategy, the expected benefits of having
    these mechanisms in place, and how they relate to Kubernetes. Let’s start by looking
    into canary releases.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你寻找团队在推广服务到敏感环境时最常采用的发布策略，你会发现有金丝雀发布、蓝绿部署和A/B测试。每种发布策略都有不同的目的，并且可以应用于各种场景。在接下来的简短部分中，我们将探讨每种发布策略的预期效果，这些机制实施后的预期好处，以及它们与Kubernetes的关系。让我们首先了解一下金丝雀发布。
- en: 8.1.1 Canary releases
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 金丝雀发布
- en: With canary releases, we want to enable teams to deploy a new version of a service
    and have full control over how much live traffic is routed to this new version.
    This allows teams to slowly route traffic to the new version to validate that
    no problems were introduced before routing all the production traffic to it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在金丝雀发布中，我们希望使团队能够部署服务的新版本，并完全控制有多少实时流量被路由到这个新版本。这允许团队缓慢地将流量路由到新版本以验证在将所有生产流量路由到它之前没有引入任何问题。
- en: Figure 8.1 shows users accessing our software, where 95% of the requests are
    forwarded to the service that we know is stable and only 5% are forwarded to the
    new version of the service.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1显示了用户访问我们的软件，其中95%的请求被转发到我们已知稳定的版本，只有5%被转发到服务的新版本。
- en: '![](../../OEBPS/Images/08-01.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图8.1](../../OEBPS/Images/08-01.png)'
- en: Figure 8.1 Releasing a new version (canary) of the service with 5% traffic routed
    to it
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 显示了将5%的流量路由到新版本（金丝雀）的服务新版本发布
- en: The term *canary release* comes from coal miners who used canary birds to alert
    them when toxic gasses reached dangerous levels. In this case, our canary release
    can help us identify problems or regressions introduced by the new version early
    on, where rolling back 100% of the traffic to the stable version doesn’t include
    a full deployment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: “金丝雀发布”这个术语来自煤矿工人，他们使用金丝雀鸟来警告他们当有毒气体达到危险水平时。在这种情况下，我们的金丝雀发布可以帮助我们在新版本引入问题或回归的早期阶段就识别出来，而将100%的流量回滚到稳定版本并不包括完整的部署。
- en: In the context of Kubernetes, and as shown in figure 8.2, you can implement
    a sort of canary release by using two Kubernetes deployments resources (one with
    the stable version and one with the new version) and a single Kubernetes service
    that matches these two deployments. If each deployment has a single replica, there
    will be a 50% and 50% traffic split. Adding more replicas to each version creates
    a different percentage traffic split (for example, three replicas for the stable
    version and only one replica for the new version will give you a 75% to 25% traffic
    split ratio), as the Kubernetes service route requests using a round-robin fashion
    to all pods matching the service label.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes的上下文中，如图8.2所示，你可以通过使用两个Kubernetes部署资源（一个用于稳定版本，一个用于新版本）和一个匹配这两个部署的单个Kubernetes服务来实现一种金丝雀发布。如果每个部署只有一个副本，则流量将平分，即50%和50%。向每个版本添加更多副本将创建不同的流量分割百分比（例如，稳定版本有三个副本，而新版本只有一个副本，将给出75%到25%的流量分割比率），因为Kubernetes服务使用轮询方式将请求路由到所有匹配服务标签的Pod。
- en: '![](../../OEBPS/Images/08-02.png)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图8.2](../../OEBPS/Images/08-02.png)'
- en: Figure 8.2 Canary release in Kubernetes using two deployments and one service.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 使用两个部署和一个服务在 Kubernetes 中进行金丝雀发布。
- en: Tools like Istio ([https://istio.io/](https://istio.io/)) or Linkerd ([https://linkerd.io/](https://linkerd.io/))
    service meshes can give you finer-grained control of how traffic gets routed to
    each service. I strongly recommend you check Martin Fowler’s website, which explains
    this release strategy in more detail at [https://martinfowler.com/bliki/CanaryRelease.xhtml](https://martinfowler.com/bliki/CanaryRelease.xhtml).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 Istio ([https://istio.io/](https://istio.io/)) 或 Linkerd ([https://linkerd.io/](https://linkerd.io/))
    服务网格可以让你更精细地控制流量如何路由到每个服务。我强烈建议你查看马丁·福勒的网站，其中更详细地解释了这种发布策略，网址为 [https://martinfowler.com/bliki/CanaryRelease.xhtml](https://martinfowler.com/bliki/CanaryRelease.xhtml)。
- en: 8.1.2 Blue/green deployments
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 蓝绿部署
- en: With blue/green deployments, we aim to enable teams to switch between two versions
    of their services or applications that are running parallel. This parallel version
    can act as a staging instance for testing, and when the team is confident enough,
    they can switch traffic to this parallel instance. This approach gives the team
    the safety of having another instance ready if the new version starts experiencing
    problems. This approach requires having enough resources to run both versions
    simultaneously, which can be expensive, but it gives your teams the freedom to
    experiment with an instance that is running with the same resources as your production
    workloads.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 使用蓝绿部署，我们的目标是让团队能够在两个并行运行的服务或应用程序版本之间切换。这个并行版本可以作为测试的预发布实例，当团队足够自信时，他们可以将流量切换到这个并行实例。这种方法给团队提供了在新版本开始出现问题时，有另一个实例准备好的安全性。这种方法需要足够的资源同时运行两个版本，这可能很昂贵，但它给了你的团队在具有与生产工作负载相同资源的实例上进行实验的自由。
- en: Figure 8.3 shows internal teams testing a production-like setup of the service’s
    new version. Whenever this new version is ready, the team can decide to switch
    production traffic to the new version while still having the stable version to
    rollback if things go wrong.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 展示了内部团队在测试服务新版本的生产环境配置。每当这个新版本准备就绪时，团队可以决定将生产流量切换到新版本，同时仍然保留稳定的版本以备出错时回滚。
- en: '![](../../OEBPS/Images/08-03.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-03.png)'
- en: Figure 8.3 Blue/green deployments run in parallel with production-grade setups,
    allowing teams to switch traffic when they feel confident in the new version.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 蓝绿部署与生产级设置并行运行，允许团队在他们对新版本有信心时切换流量。
- en: In the Kubernetes context, you can implement blue/green deployments by using
    two Kubernetes deployment resources and a Kubernetes service, but in this case,
    the service should only match the pods of a single deployment. Updating the service
    configuration to match the green deployment label(s) will automatically switch
    the traffic to the new version.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 的上下文中，你可以通过使用两个 Kubernetes 部署资源和 Kubernetes 服务来实现蓝绿部署，但在此情况下，服务应仅匹配单个部署的
    pod。将服务配置更新为匹配绿色部署的标签将自动将流量切换到新版本。
- en: Figure 8.4 shows how by changing the `matchLabel` of the service to “green,”
    the traffic will be automatically routed to the new version of the service. In
    the meantime, for testing, internal teams can use a different service to match
    the new version’s deployment.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 展示了通过将服务的 `matchLabel` 更改为“绿色”，流量将自动路由到服务的新版本。在此同时，为了测试，内部团队可以使用不同的服务来匹配新版本的部署。
- en: '![](../../OEBPS/Images/08-04.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-04.png)'
- en: Figure 8.4 Blue/green deployments run in parallel. The service matchLabel is
    used to define where to route requests.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 显示蓝绿部署并行运行。服务 matchLabel 用于定义请求的路由位置。
- en: Once again, I strongly recommend you check Martin Fowler’s website ([https://martinfowler.com/bliki/BlueGreenDeployment.xhtml](https://martinfowler.com/bliki/BlueGreenDeployment.xhtml))
    on blue/green deployments, because there are links and more context that you might
    find useful.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我强烈建议你查看马丁·福勒关于蓝绿部署的网站 ([https://martinfowler.com/bliki/BlueGreenDeployment.xhtml](https://martinfowler.com/bliki/BlueGreenDeployment.xhtml))，因为那里有链接和更多可能对你有用的上下文。
- en: 8.1.3 A/B testing
  id: totrans-42
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 A/B 测试
- en: A/B testing is different from canary releases and blue/green deployments because
    it focuses more on end users than internal teams. With A/B testing, we want to
    enable other teams closer to the business to try different approaches to solve
    a business problem. Examples are having two different page layouts to see which
    one works better for the users or having different registration flows to validate
    which one takes users less time and causes less frustration. As discussed in chapter
    7 with feature flags, we want to enable other teams and not only developers to
    experiment, in this case by providing different groups of users access to different
    versions of the application. These teams can then validate how effective each
    feature is and then decide which one to keep.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: A/B测试与金丝雀发布和蓝绿部署不同，因为它更关注最终用户而不是内部团队。通过A/B测试，我们希望使业务相关的其他团队能够尝试不同的方法来解决业务问题。例如，有两个不同的页面布局以查看哪一个对用户更有效，或者有不同的注册流程以验证哪一个花费用户更少的时间并引起更少的挫败感。正如第7章中讨论的功能标志一样，我们希望使其他团队（而不仅仅是开发者）能够进行实验，在这种情况下，通过为不同的用户组提供不同版本的应用程序。然后，这些团队可以验证每个功能的有效性，并决定保留哪一个。
- en: Figure 8.5 shows two different service implementations providing alternative
    registration flows for users. Using A/B testing, we can run both in parallel and
    collect data to enable business teams to decide which option works better.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5展示了两种不同的服务实现，为用户提供替代的注册流程。通过A/B测试，我们可以同时运行这两种实现并收集数据，以便业务团队决定哪种选项效果更好。
- en: '![](../../OEBPS/Images/08-05.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图8.6](../../OEBPS/Images/08-05.png)'
- en: Figure 8.5 A/B testing enables teams closer to the business to evaluate different
    approaches and gather data to make decisions to improve business outcomes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 A/B测试使业务相关的团队能够评估不同的方法并收集数据，以便做出改善业务成果的决策。
- en: Because A/B testing is not a technical release strategy, it can be implemented
    in different ways depending on the application’s requirements. Having two separate
    Kubernetes services and deployments would make sense to run and access two different
    versions of the same application. Figure 8.6 shows the use of two Kubernetes services
    and two deployments to route users to different versions of the same functionality.
    It also shows that an application-level router will be needed to define the rules
    on how users are routed to each of the alternatives.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 由于A/B测试不是一种技术发布策略，它可以根据应用程序的需求以不同的方式实现。拥有两个独立的Kubernetes服务和部署来运行和访问同一应用程序的两个不同版本是有意义的。图8.6展示了使用两个Kubernetes服务和两个部署将用户路由到同一功能的不同版本。它还显示需要一个应用程序级别的路由器来定义用户如何路由到每个替代方案。
- en: '![](../../OEBPS/Images/08-06.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图8.5](../../OEBPS/Images/08-06.png)'
- en: Figure 8.6 A/B testing requires some business and application-level rules to
    define how to route users to different options.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 A/B测试需要一些业务和应用级别的规则来定义如何将用户路由到不同的选项。
- en: A/B testing can be implemented using similar mechanisms as canary releases,
    and we will look at several options in the following sections. *Continuous Delivery*
    by Jez Humble and David Farley (Addison-Wesley Professional, 2010) covers these
    release strategies in detail, so I strongly recommend you check that book.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: A/B测试可以使用与金丝雀发布类似的机制实现，我们将在以下几节中探讨几个选项。"Continuous Delivery" by Jez Humble and
    David Farley (Addison-Wesley Professional, 2010)详细介绍了这些发布策略，所以我强烈建议你查看那本书。
- en: 8.1.4 Limitations and complexities of using built-in Kubernetes building blocks
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 使用内置Kubernetes构建块的局限性和复杂性
- en: Canary releases, blue/green deployments, and A/B testing can be implemented
    using built-in Kubernetes resources. But as you have seen, this requires creating
    different deployments, changing labels, and calculating the number of replicas
    needed to achieve percentage-based distribution of the requests is quite a major
    and error-prone task. Even if you use a GitOps approach, as shown with ArgoCD
    or other similar tools in chapter 4, creating the required resources with the
    right configurations is quite hard and takes a lot of effort.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀发布、蓝绿部署和A/B测试可以使用内置的Kubernetes资源实现。但正如你所见，这需要创建不同的部署、更改标签和计算请求的百分比分布所需的副本数量，这是一项相当重大且容易出错的任务。即使你使用GitOps方法，如第4章中展示的ArgoCD或其他类似工具，创建所需资源并配置正确也是相当困难且需要大量工作。
- en: 'We can summarize the drawbacks of implementing these patterns using Kubernetes
    building blocks as follows:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以总结使用Kubernetes构建块实现这些模式的缺点如下：
- en: Manual creation of more Kubernetes resources, such as deployments, services,
    and ingress rules, to implement these different strategies can be error-prone
    and cumbersome. The team implementing the release strategies must understand how
    Kubernetes behaves to achieve the desired output.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 手动创建更多的 Kubernetes 资源，例如部署、服务和入口规则，以实现这些不同的策略可能会出错且繁琐。实施发布策略的团队必须了解 Kubernetes
    的行为，以实现预期的输出。
- en: No automated mechanisms are provided out of the box to coordinate and implement
    the resources required by each release strategy.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开箱即用的自动化机制不提供协调和实施每个发布策略所需资源的功能。
- en: They can be error-prone, because multiple changes need to be applied at the
    same time in different resources for everything to work as expected.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可能会出错，因为需要同时在不同资源中应用多个更改，以确保一切按预期工作。
- en: Suppose we notice a demand increase or decrease in our services. In that case,
    we need to manually change the number of replicas for our deployments or install
    and configure a custom auto scaler (more on this later in this chapter). Unfortunately,
    if you set the number of replicas to 0, there will not be any instance to answer
    requests, requiring you to have at least one replica running all the time.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设我们注意到我们的服务需求有所增加或减少。在这种情况下，我们需要手动更改部署的副本数量或安装和配置自定义自动缩放器（关于这一点，本章后面会详细介绍）。不幸的是，如果您将副本数量设置为
    0，则不会有任何实例来响应请求，这意味着您至少需要一直运行一个副本。
- en: Out of the box, Kubernetes doesn’t include any mechanism to automate or facilitate
    these release strategies, which becomes a problem quite quickly if you are dealing
    with many services that depend on each other.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 开箱即用的 Kubernetes 不包括任何自动化或简化这些发布策略的机制，如果您处理许多相互依赖的服务，这会迅速成为一个问题。
- en: 'Note One thing is clear: your teams need to be aware of the implicit contracts
    imposed by Kubernetes regarding 12-factor apps and how their services APIs evolve
    to avoid downtime. Your developers need to know how Kubernetes’ built-in mechanisms
    work to have more control over how your applications are upgraded.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有一点很清楚：您的团队需要了解 Kubernetes 对 12 因素应用施加的隐含合同以及他们的服务 API 如何演变，以避免停机。您的开发者需要了解
    Kubernetes 内置机制的工作原理，以便更好地控制应用程序的升级。
- en: If we want to reduce the risk of releasing new versions, we want to empower
    our developers to have these release strategies available for their daily experimentation.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想降低发布新版本的风险，我们希望赋予我们的开发者这些发布策略，以便他们在日常实验中使用。
- en: In the next sections, we will look at Knative Serving and Argo Rollouts, tools
    and mechanisms built on top of Kubernetes to simplify all the manual work and
    limitations that we will find when trying to set up Kubernetes building blocks
    to enable teams with different release mechanisms. Let’s start first with Knative
    Serving, which extends our Kubernetes clusters with a set of building blocks that
    simplifies the implementation of the release strategies described before.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将探讨 Knative Serving 和 Argo Rollouts，这些是在 Kubernetes 之上构建的工具和机制，旨在简化我们在尝试设置
    Kubernetes 构建块以使具有不同发布机制的团队受益时遇到的所有手动工作和限制。让我们首先从 Knative Serving 开始，它通过一系列构建块扩展了我们的
    Kubernetes 集群，简化了之前描述的发布策略的实施。
- en: '8.2 Knative Serving: Advanced traffic management and release strategies'
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 Knative Serving：高级流量管理和发布策略
- en: Knative is one of these technologies that are hard not to use when you learn
    what it can do for you. After working with the project for almost three years
    and observing the evolution of some of its components, every Kubernetes cluster
    should have Knative Serving installed; your teams will appreciate it. Knative
    Serving is a Kubernetes extension that provides higher-level abstractions on top
    of Kubernetes built-in resources to implement good practices and common patterns
    that enable your teams to go faster and have more control over their services.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 是那些一旦了解其能为您做什么就很难不使用的科技之一。在与该项目合作了近三年并观察其某些组件的演变后，每个 Kubernetes 集群都应该安装
    Knative Serving；您的团队会感激它的。Knative Serving 是一个 Kubernetes 扩展，它在上层 Kubernetes 内置资源之上提供高级抽象，以实现良好的实践和常见模式，使您的团队能够更快地工作并对其服务拥有更多控制。
- en: 'While this chapter focuses on release strategies, you should look into Knative
    Serving if you are interested in the following topics:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章重点介绍发布策略，但如果您对以下主题感兴趣，您应该考虑研究 Knative Serving：
- en: Providing a containers-as-a-service approach for your teams to use.
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的团队提供容器即服务的方法。
- en: Dynamic autoscaling for your workloads to provide a functions-as-a-service approach
    for your teams. Knative Serving installs its own autoscaler, which is automatically
    available for all Knative Services.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的负载提供动态自动缩放，为您的团队提供一种函数即服务（Functions-as-a-Service）的方法。Knative Serving会安装自己的自动缩放器，该缩放器对所有Knative服务自动可用。
- en: Advanced and fine-grained traffic management for your services.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为您的服务提供高级和细粒度的流量管理。
- en: As the title of this section specifies, the following sections focus on a subset
    of the functionality provided by Knative, called Knative Serving. Knative Serving
    allows you to define *Knative Services*, which dramatically simplifies implementing
    the release strategies exemplified in the previous sections. Knative Services
    will create Kubernetes built-in resources for you and keep track of their changes
    and versions, enabling scenarios that require multiple versions to be present
    simultaneously. Knative Services also provides advanced traffic handling and autoscaling
    to scale down to zero replicas for a serverless approach.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 正如本节标题所指定的，以下章节将重点介绍Knative提供的功能子集，称为Knative Serving。Knative Serving允许您定义*Knative服务*，这极大地简化了实现前几节中展示的发布策略。Knative服务将为您创建Kubernetes内置资源，并跟踪其更改和版本，从而实现需要同时存在多个版本的场景。Knative服务还提供高级流量处理和自动缩放，以实现无服务器方法，将副本数缩放到零。
- en: Note A step-by-step tutorial on how to use Knative Serving with the Conference
    application to implement different release strategies can be found at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关如何使用Knative Serving与会议应用程序一起实现不同发布策略的逐步教程，请参阅[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md)。
- en: It is outside of the scope of this book to explain how Knative Serving components
    and resources work; my recommendation is that if I manage to get your attention
    with the examples in the following sections, you should check out *Knative in
    Action* by Jacques Chester (Manning Publications, 2021).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 解释Knative Serving组件和资源的工作原理超出了本书的范围；我的建议是，如果我在以下章节中的示例中成功吸引了您的注意，您应该查阅Jacques
    Chester所著的《Knative实战》（Manning Publications, 2021）。
- en: '8.2.1 Knative Services: Containers-as-a-Service'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 Knative服务：容器即服务
- en: 'Once you have Knative Serving installed, you can create Knative Services. I
    can hear you thinking: “But we already have Kubernetes services. Why do we need
    Knative Services?” Believe me, I had the same feeling when I saw the same name,
    but follow along—it does make sense.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您安装了Knative Serving，您就可以创建Knative服务。我能听到您在想：“但我们已经有了Kubernetes服务。为什么我们还需要Knative服务？”相信我，当我看到相同的名称时，我也有同样的感觉，但请继续阅读——这确实是有道理的。
- en: 'When we deployed our walking skeleton in chapter 2 (the Conference application),
    we created at least two Kubernetes resources: a Kubernetes deployment and a Kubernetes
    service. As we discussed in chapter 2, by using ReplicaSets, a deployment can
    perform rolling updates by keeping track of the configuration changes in the deployment
    resources. We also discussed in chapter 2 the need for creating an ingress resource
    to route traffic from outside the cluster. Usually, you only create an ingress
    resource to map the publicly available services, such as the Frontend of the Conference
    application or the Conference Admin Portal.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第2章（会议应用程序）中部署我们的“行走骨架”时，我们至少创建了两个Kubernetes资源：一个Kubernetes部署和一个Kubernetes服务。正如我们在第2章中讨论的，通过使用ReplicaSets，部署可以通过跟踪部署资源中的配置更改来执行滚动更新。我们还在第2章中讨论了创建ingress资源以将集群外部的流量路由到集群的需求。通常，您只创建ingress资源来映射公开可用的服务，例如会议应用程序的前端或会议管理门户。
- en: Note The Ingress resource that we created routes all the traffic straight to
    the in-cluster Kubernetes service, and the ingress controller used in the tutorials
    works as a simple reverse proxy. It doesn’t have any advanced capability to split
    traffic, rate limit, or inspect the request headers to make dynamic decisions
    about it.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们创建的Ingress资源将所有流量直接路由到集群内的Kubernetes服务，教程中使用的ingress控制器作为一个简单的反向代理工作。它没有任何高级功能来分割流量、速率限制或检查请求头以对其进行动态决策。
- en: You can follow a step-by-step tutorial to create a cluster, install Knative
    Serving, and deploy the application services at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#installation](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#installation).
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以遵循逐步教程来创建集群、安装 Knative Serving 并部署应用程序服务，请参阅 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#installation](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#installation)。
- en: Knative Services are built on top of these resources (services, deployments,
    ReplicaSets) to simplify how we define and manage the lifecycle of our application’s
    services. While it simplifies the task and reduces the amount of YAML that we
    need to maintain, it also adds some exciting features. Before jumping into the
    features, let’s look at how a Knative Service looks in action.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 服务是在这些资源（服务、部署、ReplicaSets）之上构建的，以简化我们定义和管理应用程序服务生命周期的过程。虽然这简化了任务并减少了我们需要维护的
    YAML 文件数量，但它也增加了一些令人兴奋的功能。在深入探讨这些功能之前，让我们看看 Knative 服务在实际操作中的样子。
- en: Knative Services expose a simplified contract to its users that resembles a
    *container-as-a-service* interface such as AWS App Runner and Azure Container
    Apps. In fact, Knative Services share the interface used by Google Cloud Run to
    enable users to run containers on-demand without the need to understand Kubernetes.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: Knative 服务向用户提供了类似于 AWS App Runner 和 Azure Container Apps 这样的 *容器即服务* 接口的简化合约。实际上，Knative
    服务与 Google Cloud Run 使用的接口相同，使用户能够按需运行容器，而无需了解 Kubernetes。
- en: Because Knative Serving installs its own autoscaler, Knative Services are automatically
    configured to scale based on demand. This makes Knative Serving a very good way
    to implement a *function-as-a-service* platform, because workloads that are not
    being used will be automatically downscaled to zero.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Knative Serving 安装了自己的自动扩展器，Knative 服务会自动根据需求进行配置以进行扩展。这使得 Knative Serving
    成为实现 *函数即服务* 平台的一种非常好的方式，因为未使用的负载将自动缩减到零。
- en: Let’s see these features in action, beginning with the Knative Service Kubernetes
    resource. We will start simple and use the notification service from the Conference
    application to demonstrate how Knative Services work. Check the notifications-service.yaml
    resource definition (available at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/notifications-service.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/notifications-service.yaml)),
    as shown in the following listing.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这些功能在实际操作中的表现，从 Knative 服务 Kubernetes 资源开始。我们将从简单开始，并使用来自 Conference 应用程序的通告服务来演示
    Knative 服务的工作原理。检查 notifications-service.yaml 资源定义（可在 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/notifications-service.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/notifications-service.yaml)
    获取），如下所示。
- en: Listing 8.1 Knative Service definition
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 Knative 服务定义
- en: '[PRE0]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① You need to specify a name for the resource, as with any other Kubernetes
    resource.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ① 您需要为资源指定一个名称，就像任何其他 Kubernetes 资源一样。
- en: ② You need to specify which container image you want to run.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ② 您需要指定您想要运行的容器镜像。
- en: ③ You can parameterize your containers using environment variables.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 您可以使用环境变量来参数化您的容器。
- en: In the same way as a deployment will pick the `spec.template.spec` field to
    cookie-cut pods, a Knative Service defines the configuration for creating other
    resources using the same field.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 就像部署会选择 `spec.template.spec` 字段来裁剪 pod 一样，Knative 服务定义了使用相同字段创建其他资源的配置。
- en: Nothing too strange so far, but how is this different from a Kubernetes Service?
    If you create this resource using `kubectl apply -f`, you can start exploring
    the differences.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，没有什么太奇怪的，但这与 Kubernetes 服务有何不同？如果您使用 `kubectl apply -f` 创建此资源，您就可以开始探索它们之间的差异。
- en: Note All the examples in this section are based on running the step-by-step
    tutorial on a KinD cluster. Outputs will be different if you run in a cloud provider.
    See [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#knative-services-quick-intro](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#knative-services-quick-intro).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节中的所有示例都是基于在 KinD 集群上运行逐步教程。如果您在云提供商上运行，输出将不同。请参阅 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#knative-services-quick-intro](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/knative/README.md#knative-services-quick-intro)。
- en: 'You can also list all Knative Services using `kubectl get ksvc` (`ksvc` stands
    for Knative Service), and you should see your newly created Knative Service there:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用 `kubectl get ksvc`（`ksvc` 代表 Knative 服务）列出所有 Knative 服务，并且您应该在那里看到您新创建的
    Knative 服务：
- en: '[PRE1]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There are a couple of details to notice right here; first, there is a URL that
    you can copy into your browser and access the service. If you were running in
    a cloud provider and configured DNS while installing Knative, this URL should
    be accessible immediately. The `LASTCREATED` column shows the name of the latest
    Knative Revision of the Service. Knative Revisions are pointers to the specific
    configuration of our service, meaning that we can route traffic to them.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里有几个细节需要注意；首先，有一个可以复制到浏览器中访问服务的 URL。如果您在云提供商上运行并安装 Knative 时配置了 DNS，这个 URL
    应该可以立即访问。`LASTCREATED` 列显示服务的最新 Knative 修订版本名。Knative 修订是指向我们服务特定配置的指针，这意味着我们可以将流量路由到它们。
- en: You can go ahead and test the Knative Service URL by using `curl` or by pointing
    your browser to http://notifications-service.default.127.0.0.1.sslip.io/service/info.
    Notice that we are using jq ([https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/)),
    a very popular JSON utility, to pretty-print the output. You should see the output
    in listing 8.2.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 `curl` 或将浏览器指向 http://notifications-service.default.127.0.0.1.sslip.io/service/info
    来测试 Knative 服务 URL。请注意，我们正在使用 jq ([https://jqlang.github.io/jq/download/](https://jqlang.github.io/jq/download/))，一个非常流行的
    JSON 工具，来美化输出。您应该在列表 8.2 中看到输出。
- en: Listing 8.2 Interacting with our newly created Knative Service
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 与我们新创建的 Knative 服务交互
- en: '[PRE2]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: As with any other Kubernetes resource, you can also use `kubectl describe ksvc
    notifications-service` to get a more detailed description of the resource. If
    you list other well-known resources such as deployment, services, and pods, you
    will find out that Knative Serving is creating them for you and managing them.
    Because these are managed resources now, it is usually not recommended to change
    them manually. If you want to change your application configurations, you should
    edit the Knative Service resource.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他 Kubernetes 资源一样，您也可以使用 `kubectl describe ksvc notifications-service` 来获取资源的更详细描述。如果您列出其他知名资源，例如部署、服务和
    Pod，您会发现 Knative Serving 正在为您创建并管理它们。因为这些现在是托管资源，通常不建议手动更改它们。如果您想更改应用程序配置，您应该编辑
    Knative 服务资源。
- en: 'A Knative Service, as we applied it before to our cluster, by default behaves
    differently from creating a service, a deployment, and an ingress manually. A
    Knative Service by default:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前在集群中应用 Knative 服务时，默认行为与手动创建服务、部署和入口不同。Knative 服务默认：
- en: '*Is accessible:* It exposes itself under a public URL so you can access it
    from outside the cluster. It doesn’t create an ingress resource, because it uses
    the available Knative Networking stack that you installed previously. Because
    Knative has more control over the network stack and manages deployments and services,
    it knows when the service is ready to serve requests, reducing configuration errors
    between services and deployments.'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*可访问：* 它在公共 URL 下暴露自己，因此您可以从集群外部访问它。它不会创建入口资源，因为它使用您之前安装的可用 Knative 网络堆栈。因为
    Knative 对网络堆栈有更多控制权，并管理部署和服务，所以它知道服务何时准备好处理请求，从而减少了服务和部署之间的配置错误。'
- en: '*Manages Kubernetes resources:* It creates two services and a deployment. Knative
    Serving allows us to run multiple versions of the same service simultaneously.
    Hence, it will create a new Kubernetes service for each version (which in Knative
    Serving is called a revision).'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理 Kubernetes 资源：* 它创建两个服务和一项部署。Knative Serving 允许我们同时运行同一服务的多个版本。因此，它将为每个版本创建一个新的
    Kubernetes 服务（在 Knative Serving 中称为修订版）。'
- en: '*Collects service usage:* It creates a pod with the specified `user-container`
    and a sidecar container called `queue-proxy`.'
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*收集服务使用情况：* 它创建一个具有指定 `user-container` 的 Pod 和一个名为 `queue-proxy` 的边车容器。'
- en: '*Scales-up and down based on demand:* It automatically downscales itself to
    zero if no requests are hitting the service (by default after 90 seconds):'
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*根据需求进行扩展和缩减：* 如果没有请求击中服务（默认情况下 90 秒后），它会自动将自己缩减到零：'
- en: It achieves this by downscaling the Deployment replicas to 0 using the data
    collected by the `queue-proxy`.
  id: totrans-100
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通过使用 `queue-proxy` 收集的数据将 Deployment 副本缩减到 0 来实现这一点。
- en: If a request arrives and there is no replica available, it scales up while queuing
    the request, so it doesn’t get lost.
  id: totrans-101
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果有请求到达但没有可用的副本，它会排队请求的同时进行扩展，所以它不会丢失。
- en: Our notification service has set a minimum number of replicas to 1 to be kept
    running at all times.
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们的通知服务已将副本的最小数量设置为1，以确保始终运行。
- en: '*Configuration changes history is managed by Knative Serving:* If you change
    the Knative Service configuration, a new *Revision* will be created. By default,
    all traffic will be routed to the latest revision.'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*配置更改历史由Knative Serving管理：* 如果您更改Knative服务配置，将创建一个新的*修订版*。默认情况下，所有流量都将路由到最新的修订版。'
- en: Of course, these are the defaults, but you can fine-tune each of your Knative
    Services to serve your purpose and, for example, implement the previously described
    release strategies.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些都是默认设置，但您可以微调每个Knative服务以满足您的需求，例如实现之前描述的发布策略。
- en: In the next section, we will look at how Knative Serving advanced traffic-handling
    features can be used to implement canary releases, blue/green deployments, A/B
    testing, and header-based routing.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨如何使用Knative Serving的高级流量处理功能来实现金丝雀发布、蓝/绿部署、A/B测试和基于头部的路由。
- en: 8.2.2 Advanced traffic-splitting features
  id: totrans-106
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 高级流量分配功能
- en: Let’s start by looking at how you can implement a canary release for one of
    our application’s services with a Knative Service. This section starts by looking
    into doing canary releases using percentage-based traffic splitting. Then it goes
    into A/B testing by using tag-based and header-based traffic splitting.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先看看如何使用Knative服务实现我们应用程序服务的一个金丝雀发布。本节首先探讨使用基于百分比的流量分配进行金丝雀发布。然后，它将进入基于标签和基于头部的流量分配的A/B测试。
- en: Canary releases using percentage-based traffic splitting
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 基于百分比的流量分配的金丝雀发布
- en: If you get the Knative Service resource (with `kubectl get ksvc notifications-service
    -oyaml`), you will notice that the `spec` section now also contains a `spec.traffic`
    section (as shown in listing 8.3) that was created by default, because we didn’t
    specify anything. By default, 100% of the traffic is being routed to the latest
    Knative Revision of the service.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您获取Knative服务资源（使用`kubectl get ksvc notifications-service -oyaml`），您会注意到`spec`部分现在还包含一个默认创建的`spec.traffic`部分（如列表8.3所示），因为我们没有指定任何内容。默认情况下，100%的流量将被路由到服务的最新Knative修订版。
- en: Listing 8.3 The Knative Service allows us to set traffic rules
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.3 Knative服务允许我们设置流量规则
- en: '[PRE3]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now imagine that you made a change in your service to improve how emails are
    sent, but your team is not sure how well it will be received by people, and we
    want to avoid having any backlash from people not wanting to sign into our conference
    because of the website. Hence, we can run both versions side-by-side and control
    how much of the traffic is being routed to each version (Revision in Knative terms).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设您在服务中对发送电子邮件的方式进行了更改，以提高其效果，但您的团队不确定人们会接受得如何，我们希望避免因网站问题导致人们不愿意注册我们的会议。因此，我们可以同时运行两个版本，并控制将多少流量路由到每个版本（在Knative术语中称为修订版）。
- en: Let’s edit (`kubectl edit ksvc notifications-service`) the Knative Service and
    apply the changes, as shown in listing 8.4.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们编辑Knative服务（`kubectl edit ksvc notifications-service`），并应用列表8.4中所示的变化。
- en: Listing 8.4 Changing our Knative Service
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.4 更改我们的Knative服务
- en: '[PRE4]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ① You have updated the container image that the service will use from “notifications-service-0e27884e01429ab7e350cb5dff61b525:v1.0.0”
    to “notifications-service-0e27884e01429ab7e350cb5dff61b525:v1.1.0”.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ① 您已将服务将使用的容器镜像从“notifications-service-0e27884e01429ab7e350cb5dff61b525:v1.0.0”更新到“notifications-service-0e27884e01429ab7e350cb5dff61b525:v1.1.0”。
- en: ② You have created a 50% / 50% traffic split where 50% of the traffic will keep
    going to your stable version and 50% to the newest version that you just updated.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ② 您已创建了一个50% / 50%的流量分配，其中50%的流量将继续流向您的稳定版本，另外50%流向您刚刚更新的最新版本。
- en: If you try now with `curl,` you should be able to see the traffic split in action.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您现在使用`curl`尝试，您应该能够看到流量分配的实际操作。
- en: Listing 8.5 New requests hitting different versions that are running parallel
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.5 新请求正在影响并行运行的不同版本
- en: '[PRE5]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① One in five requests will go to the new “NOTIFICATIONS-IMPROVED” version.
    Notice that this can take a while until the new Knative Revision is running.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ① 每5个请求中有一个将流向新的“NOTIFICATIONS-IMPROVED”版本。请注意，这可能需要一段时间，直到新的Knative修订版开始运行。
- en: Once you have validated that the new version of your service is working correctly,
    you can start sending more traffic until you feel confident to move 100% of the
    traffic to it. If things go wrong, you can revert the traffic split to the stable
    version.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你验证了服务的新版本运行正确，你就可以开始发送更多流量，直到你确信可以将100%的流量移动到它。如果出现问题，你可以将流量分割回稳定版本。
- en: Notice that you are not limited to just two service revisions; you can create
    as many as you want as long as the traffic percentage sum of all the revisions
    is 100%. Knative will follow these rules and scale up the required revisions of
    your services to serve requests. You don’t need to create any new Kubernetes resources,
    as Knative will create those for you, reducing the likelihood of errors that come
    with modifying multiple resources simultaneously.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，你不仅限于只有两个服务版本；只要你所有版本的流量百分比总和为100%，你就可以创建尽可能多的版本。Knative将遵循这些规则，并扩展所需的服务版本以处理请求。你不需要创建任何新的Kubernetes资源，因为Knative会为你创建这些资源，从而降低同时修改多个资源时出现错误的可能性。
- en: Figure 8.7 shows some challenges that you will face when using this feature.
    By using percentages, you don’t have control over where subsequent requests will
    land. Knative will just make sure to maintain a fair distribution based on the
    percentages that you have specified. This can become a problem if, for example,
    you have a user interface instead of a simple REST endpoint.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7显示了在使用此功能时你将面临的挑战。通过使用百分比，你无法控制后续请求将落在何处。Knative将确保根据你指定的百分比保持公平的分布。如果，例如，你有用户界面而不是简单的REST端点，这可能会成为一个问题。
- en: '![](../../OEBPS/Images/08-07.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-07.png)'
- en: Figure 8.7 Percentage-based traffic split scenarios and challenges
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7基于百分比的流量分割场景和挑战
- en: User interfaces are complex because a browser will perform several correlated
    GET requests to render the page HTML, CSS, images, and so forth. You can quickly
    end up in a situation where each request hits a different version of your application.
    Let’s look at a different approach that might be better suited for testing user
    interfaces or scenarios when we need to ensure that several requests end up in
    the correct version of our application.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 用户界面很复杂，因为浏览器将执行多个相关的GET请求来渲染页面HTML、CSS、图像等。你可能会迅速陷入每个请求都击中你应用程序不同版本的情况。让我们看看一种可能更适合测试用户界面或需要确保多个请求最终进入正确版本的应用程序的场景的不同方法。
- en: A/B testing with tag-based routing
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 基于标签的路由A/B测试
- en: If you want to perform A/B testing of different versions of the user interface
    included with the Conference application, you will need to give Knative some way
    to differentiate where to send the requests. You have two options. First, you
    can point to a special URL for the service you want to try out, and the second
    is to use a request header to differentiate where to send the request. Let’s look
    at these two alternatives in action.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要对会议应用包含的不同版本的用户界面进行A/B测试，你需要为Knative提供一种区分请求发送位置的方法。你有两种选择。首先，你可以指向一个用于尝试服务的特殊URL，第二种是使用请求头区分请求发送的位置。让我们看看这两个替代方案的实际操作。
- en: The step-by-step tutorial ([https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/knative#run-the-conference-application-with-knative-services](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/knative#run-the-conference-application-with-knative-services))
    defines all the Conference application services to be Knative Services and deploys
    them to the cluster. The Frontend Knative Services looks like listing 8.6.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤分解教程([https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/knative#run-the-conference-application-with-knative-services](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/knative#run-the-conference-application-with-knative-services))定义了所有要作为Knative服务的会议应用服务并将它们部署到集群中。前端Knative服务看起来像列表8.6。
- en: Listing 8.6 Knative Service definition for the Frontend application
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.6前端应用程序的Knative服务定义
- en: '[PRE6]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① You need to specify a name for this service.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ①你需要为这个服务指定一个名称。
- en: ② We don’t want Knative Serving to downscale the Frontend service if nobody
    is using it. We want to keep at least one instance running all the time.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ②我们不希望Knative Serving在没有人在使用时缩小前端服务。我们希望始终保持至少一个实例运行。
- en: ③ You now define the Frontend container image, because we are going to test
    multiple requests going to the same version.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ③你现在定义前端容器镜像，因为我们将要测试多个请求发送到同一版本。
- en: Once again, we have just created a Knative Service, but we cannot specify percentage-based
    routing rules because this container image contains a web application composed
    of HTML, CSS, images, and JavaScript files. Knative will not stop you from doing
    so. Still, you will notice requests going to different versions and errors popping
    up because a given image is not in one of the versions, or you end up with the
    wrong stylesheet (CSS) coming from the wrong version of the application.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们刚刚创建了一个Knative服务，但由于这个容器镜像包含由HTML、CSS、图像和JavaScript文件组成的Web应用程序，我们无法指定基于百分比的路由规则。Knative不会阻止您这样做。不过，您会发现请求被路由到不同的版本，并出现错误，因为给定的镜像不在任何一个版本中，或者您最终得到了来自应用程序错误版本的样式表（CSS）。
- en: 'Let’s start by defining a Tag that you can use to test a new stylesheet and
    also include the Debug tab in the Back Office section. You can do that by modifying
    the Knative Service resource as we did before. First, change the image to `salaboy/frontend-go-1739aa83b5e69d4ccb8a5615830ae66c:v1.1.0`,
    add the `FEATURE_DEBUG_ENABLED` environment variable with value `true` and then
    create some new traffic rules using the `traffic.tag` property:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义一个标签开始，这个标签可以用来测试新的样式表，并在后台办公室部分包含调试标签。您可以通过修改Knative服务资源来实现这一点，就像我们之前做的那样。首先，将镜像更改为`salaboy/frontend-go-1739aa83b5e69d4ccb8a5615830ae66c:v1.1.0`，添加值为`true`的`FEATURE_DEBUG_ENABLED`环境变量，然后使用`traffic.tag`属性创建一些新的流量规则：
- en: '[PRE7]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① 100% of the traffic will go to our stable version, and no request will be
    sent to our newly updated revision with version v1.1.0.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ① 100%的流量将流向我们的稳定版本，不会有请求发送到我们的新修订版本，版本号为v1.1.0。
- en: ② We created a new tag called “color”; you can find the URL for this new tag
    by describing the Knative Service resource.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们创建了一个名为“color”的新标签；您可以通过描述Knative服务资源来找到这个新标签的URL。
- en: As shown in listing 8.7, if you describe the Knative Service (`kubectl describe
    ksvc frontend`) you will find the URL for the tag that we just created, as shown
    in the following listing.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 如列表8.7所示，如果您描述Knative服务（`kubectl describe ksvc frontend`），您将找到我们刚刚创建的标签的URL，如下所示。
- en: Listing 8.7 Traffic rules when using tags
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.7使用标签时的流量规则
- en: '[PRE8]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① You can find the tag and its generated URL in the ksvc traffic section.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ① 您可以在ksvc流量部分找到标签及其生成的URL。
- en: Figure 8.8 shows how the Knative Service will route 100% of the traffic to version
    v1.0.0 when no tags are specified. If the tag ”version110” is specified, the Knative
    Service will route traffic to the version v1.1.0.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8展示了当未指定标签时，Knative服务将100%的流量路由到版本v1.0.0。如果指定了标签“version110”，Knative服务将流量路由到版本v1.1.0。
- en: '![](../../OEBPS/Images/08-08.png)'
  id: totrans-146
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-08.png)'
- en: Figure 8.8 Knative Serving tag-based routing for version v1.1.0.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 Knative Serving基于标签的路由版本v1.1.0。
- en: Using a web browser, check that you can consistently access version v1.1.0 by
    using the following URL (http://version110-frontend.default.127.0.0.1.sslip.io)
    and version v.1.0.0 using the original service URL (http://frontend.default.127.0.0.1.sslip.io).
    Figure 8.9 shows both side by side using a different color palette.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 使用网络浏览器，检查您是否可以通过以下URL（http://version110-frontend.default.127.0.0.1.sslip.io）一致地访问版本v1.1.0，并使用原始服务URL（http://frontend.default.127.0.0.1.sslip.io）访问版本v.1.0.0。图8.9展示了两者并排使用不同的调色板。
- en: '![](../../OEBPS/Images/08-09.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-09.png)'
- en: Figure 8.9 A/B testing with tag-based routing
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9基于标签的路由A/B测试
- en: Using tags guarantees that all requests are hitting the URL to the correct version
    of your service. One more option avoids you pointing to a different URL for doing
    A/B testing, and it might be useful for debugging purposes. The next section looks
    at tag-based routing using HTTP headers instead of different URLs.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用标签可以确保所有请求都击中了服务正确版本的URL。还有一个选项可以避免您在A/B测试时指向不同的URL，这可能对调试很有用。下一节将探讨使用HTTP头而不是不同URL进行基于标签的路由。
- en: A/B testing with header-based routing
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 基于头部的A/B测试
- en: Finally, let’s look at a Knative Serving feature ([https://knative.dev/docs/serving/configuration/feature-flags/#tag-header-based-routing](https://knative.dev/docs/serving/configuration/feature-flags/#tag-header-based-routing))
    that allows you to use HTTP headers to route requests. This feature also uses
    tags to know where to route traffic, but instead of using a different URL to access
    a specific revision, you can add an HTTP header that will do the trick.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们看看一个 Knative Serving 功能（[https://knative.dev/docs/serving/configuration/feature-flags/#tag-header-based-routing](https://knative.dev/docs/serving/configuration/feature-flags/#tag-header-based-routing)），该功能允许您使用
    HTTP 头部来路由请求。此功能也使用标签来确定路由流量，但不是使用不同的 URL 来访问特定的修订版本，而是可以添加一个 HTTP 头部来完成这项工作。
- en: Imagine that you want to enable developers to access a debugging version of
    the application. Application developers can set a special header in their browsers
    and then access a specific revision.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想启用开发者访问应用程序的调试版本。应用程序开发者可以在他们的浏览器中设置一个特殊头部，然后访问特定的修订版本。
- en: 'To enable this experimental feature, you or the administrator that installs
    Knative needs to patch a ConfigMap inside the `knative-serving` namespace:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用此实验性功能，您或安装 Knative 的管理员需要修补 `knative-serving` 命名空间内的 ConfigMap：
- en: '[PRE9]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Once the feature is enabled, you can test this by using the tag `version110`
    that we created before. Listing 8.8 shows the traffic rules that we have defined.
    The tag name that we want to target using HTTP header-based routing is highlighted.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用此功能，您可以通过使用我们之前创建的 `version110` 标签来测试它。列表 8.8 展示了我们定义的流量规则。我们想要使用 HTTP 头部路由来针对的标签名称已突出显示。
- en: Listing 8.8 HTTP headers-based routing using the name of the tag
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.8 使用标签名称进行基于 HTTP 头部的路由
- en: '[PRE10]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you point your browser to the Knative Service URL (`kubectl get ksvc`),
    you will see the same application as always, as shown in figure 8.10, but if you
    use a tool like ModHeader extension ([https://chrome.google.com/webstore/detail/modheader/idgpnmonknjnojddfkpgkljpfnnfcklj?hl=en](https://chrome.google.com/webstore/detail/modheader/idgpnmonknjnojddfkpgkljpfnnfcklj?hl=en))
    for Chrome, you can set your custom HTTP headers that will be included in every
    request that the browser produces. For this example, and because the tag that
    you created is called `version110`, you need to set the following HTTP header:
    `Knative-Serving-Tag: version110`. As soon as the HTTP header is present, Knative
    Serving will route the incoming request to the `version110` tag.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '如果您将浏览器指向 Knative 服务 URL (`kubectl get ksvc`)，您将看到与之前相同的应用程序，如图 8.10 所示，但如果您使用像
    ModHeader 扩展程序（[https://chrome.google.com/webstore/detail/modheader/idgpnmonknjnojddfkpgkljpfnnfcklj?hl=en](https://chrome.google.com/webstore/detail/modheader/idgpnmonknjnojddfkpgkljpfnnfcklj?hl=en)）这样的
    Chrome 工具，您可以设置浏览器产生的每个请求中都将包含的自定义 HTTP 头部。对于此示例，并且因为您创建的标签名为 `version110`，您需要设置以下
    HTTP 头部：`Knative-Serving-Tag: version110`。一旦 HTTP 头部存在，Knative Serving 将将传入请求路由到
    `version110` 标签。'
- en: Figure 8.10 shows how Knative Serving routes the request to our `version110`
    tag by using an HTTP header set using ModHeader. Notice that we are using the
    default service URL http://frontend.default.127.0.0.1.sslip.io.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 展示了 Knative Serving 如何通过使用 ModHeader 设置的 HTTP 头部将请求路由到我们的 `version110`
    标签。请注意，我们正在使用默认服务 URL http://frontend.default.127.0.0.1.sslip.io。
- en: '![](../../OEBPS/Images/08-10.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-10.png)'
- en: Figure 8.10 Using ModHeader Chrome extension to set custom HTTP headers for
    header-based routing.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 使用 ModHeader Chrome 扩展程序设置基于头部的自定义 HTTP 头部以进行路由。
- en: Both tag and header-based routing are designed to ensure that all requests will
    be routed to the same revision if you hit a specific URL (created for the tag)
    or if one particular header is present. Finally, let’s look at how to do blue/green
    deployments with Knative Serving.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 标签和基于头部的路由都旨在确保如果访问特定的 URL（为标签创建的）或存在特定的头部，所有请求都将被路由到相同的修订版本。最后，让我们看看如何使用 Knative
    Serving 进行蓝绿部署。
- en: Blue/green deployments
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署
- en: For situations where we need to change from one version to the next at a very
    specific point in time, because there is no backward compatibility, we can still
    use tag-based routing with percentages. Instead of going gradually from one version
    to the next, we use percentages as a switch from 0 to 100 on the new version and
    from 100 to 0 on the old version.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们需要在特定时间点从一个版本切换到下一个版本的情况，因为不存在向后兼容性，我们仍然可以使用基于标签的路由和百分比。我们不是逐渐从一个版本过渡到下一个版本，而是使用百分比作为新版本从
    0 到 100 的开关，以及旧版本从 100 到 0 的开关。
- en: Most blue/green deployment scenarios require coordination between different
    teams and services to make sure that both the service and the clients are updated
    at the same time. Knative Serving allows you to declaratively define when to switch
    from one version to the next in a controller way. Figure 8.11 shows the scenario
    where we want to deploy a new version of the notifications service `v2.0.0` that
    is not backward compatible with `v1.x` versions. This means that this upgrade
    will require changes to the clients. By using Knative Serving traffic rules and
    tags, we can decide when the switch happens. Teams responsible for the clients
    and the upgrade of the notification service `v2.0.0` will need to coordinate the
    upgrade.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数蓝绿部署场景需要不同团队和服务的协调，以确保服务和客户端同时更新。Knative Serving允许你以声明式的方式定义何时从当前版本切换到下一个版本。图8.11显示了我们要部署一个与`v1.x`版本不兼容的新版本`v2.0.0`的通知服务场景。这意味着这次升级将需要修改客户端。通过使用Knative
    Serving流量规则和标签，我们可以决定何时进行切换。负责客户端和通知服务`v2.0.0`升级的团队需要协调升级。
- en: '![](../../OEBPS/Images/08-11.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-11.png)'
- en: Figure 8.11 Blue/green deployments using Knative Serving tag-based routing
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 使用Knative Serving基于标签的路由进行蓝绿部署
- en: To achieve the scenario described in figure 8.11, we can create the “green”
    tag for the new version inside our Knative Service, as shown in listing 8.9.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实现图8.11中描述的场景，我们可以在Knative服务内部为新版本创建“green”标签，如图表8.9所示。
- en: Listing 8.9 Using tags to define blue and green revisions
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.9 使用标签定义蓝绿版本
- en: '[PRE11]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: By creating a new tag (called “green”), we will now have a new URL to access
    the new version for testing. This is particularly useful for testing new versions
    of the clients, because if the Service API is changing with a non-backward compatible
    change, clients might need to be updated as well. Once all tests are performed,
    we can safely switch all traffic to the “green” revision of our service, as shown
    in listing 8.10\. Notice that we removed the tag from the "green" revision and
    created a new tag for the "blue" revision.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 通过创建一个新的标签（称为“green”），我们现在将有一个新的URL来访问用于测试的新版本。这对于测试客户端的新版本特别有用，因为如果服务API发生了非向后兼容的更改，客户端可能也需要更新。一旦所有测试完成，我们可以安全地将所有流量切换到服务的“green”版本，如图表8.10所示。注意，我们从“green”版本中移除了标签，并为“blue”版本创建了一个新的标签。
- en: Listing 8.10 Switching traffic using the Knative declarative approach
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.10 使用Knative声明式方法切换流量
- en: '[PRE12]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Notice that the “blue” original version before the update is now accessible
    using header–or tag–based routing and is receiving all the traffic sent to the
    service.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，更新前的“blue”原始版本现在可以通过基于头部或标签的路由访问，并接收发送到服务的所有流量。
- en: Generally, we cannot progressively move traffic from one version to the next,
    because the client consuming the service will need to understand that requests
    might land in different (and non-compatible) versions of the service.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们不能逐步将流量从当前版本移动到下一个版本，因为消费服务的客户端需要理解请求可能会落在不同的（且不兼容的）服务版本上。
- en: In the previous sections, we have been looking into how Knative Serving simplifies
    the implementation of different release strategies for your teams to deliver features
    and new versions of your services continuously. Knative Serving reduces the need
    to create several Kubernetes built-in resources to manually implement the release
    strategies described in this chapter. It provides high-level abstractions such
    as Knative Services, which creates and manages Kubernetes built-in resources and
    a network stack for advanced traffic management.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们一直在探讨Knative Serving如何简化团队实现不同发布策略的过程，以便持续交付功能和服务的最新版本。Knative Serving减少了手动实现本章中描述的发布策略所需的创建多个Kubernetes内置资源的需要。它提供了高级抽象，例如Knative服务，它创建和管理Kubernetes内置资源以及用于高级流量管理的网络栈。
- en: Let’s switch to another alternative for managing release strategies in Kubernetes
    with Argo Rollouts.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们切换到另一种使用Argo Rollouts在Kubernetes中管理发布策略的替代方案。
- en: '8.3 Argo Rollouts: Release strategies automated with GitOps'
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 Argo Rollouts：使用GitOps自动化的发布策略
- en: In most cases you will see Argo Rollouts working hand in hand with ArgoCD. This
    makes sense because we want to enable a delivery pipeline that removes the need
    to interact with our environments to apply configuration changes manually. For
    the examples in the following sections, we will focus only on Argo Rollouts, but
    in real-life scenarios, you shouldn’t apply resources to the environments using
    `kubectl`, because Argo CD will do it for you.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，你会看到 Argo Rollouts 与 ArgoCD 一起协同工作。这很有道理，因为我们希望实现一个交付管道，该管道可以消除手动手动应用配置更改与我们的环境交互的需求。在以下章节的示例中，我们将仅关注
    Argo Rollouts，但在实际场景中，你不应该使用 `kubectl` 将资源应用到环境中，因为 Argo CD 会为你完成这项工作。
- en: As defined on the website, Argo Rollouts is “a Kubernetes controller and set
    of CRDs which provide advanced deployment capabilities such as blue-green, canary,
    canary analysis, experimentation, and progressive delivery features to Kubernetes.”
    As we have seen with other projects, Argo Rollouts extend Kubernetes with the
    concepts of `Rollouts`, `Analysis`, and `Experimentations` to enable progressive
    delivery features. The main idea with Argo Rollouts is to use the Kubernetes built-in
    blocks without the need to manually modify and keep track of deployment and services
    resources.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 如网站所定义，Argo Rollouts 是“一个 Kubernetes 控制器和一组 CRDs，它们提供了高级部署功能，如蓝绿部署、金丝雀发布、金丝雀分析、实验和渐进式交付功能。”正如我们通过其他项目所看到的那样，Argo
    Rollouts 通过引入 `Rollouts`、`Analysis` 和 `Experimentations` 的概念来扩展 Kubernetes，以实现渐进式交付功能。Argo
    Rollouts 的主要思想是使用 Kubernetes 内置的块，而无需手动修改和跟踪部署和服务资源。
- en: 'Argo Rollouts is composed of two big parts: the Kubernetes controller that
    implements the logic to deal with our rollouts, definitions (also analysis and
    experimentations) and a `kubectl` plugin that allows you to control how these
    rollouts progress, enabling manual promotions and rollbacks. Using the `kubectl`
    Argo Rollouts plugin, you can also install the Argo Rollouts Dashboard and run
    it locally.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Rollouts 由两部分组成：一个 Kubernetes 控制器，它实现了处理我们的发布、定义（以及分析和实验）的逻辑，以及一个允许你控制这些发布如何进展的
    `kubectl` 插件，它使得手动升级和回滚成为可能。使用 `kubectl` Argo Rollouts 插件，你还可以安装 Argo Rollouts
    仪表板并在本地运行它。
- en: Note You can follow a tutorial on how to install Argo Rollouts on a local Kubernetes
    KinD cluster at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/README.md).
    Notice that this tutorial requires creating a different KinD cluster than the
    one we used for Knative Serving.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以在 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/README.md)
    上找到一个关于如何在本地 Kubernetes KinD 集群上安装 Argo Rollouts 的教程。请注意，这个教程需要创建一个不同于我们用于 Knative
    Serving 的不同 KinD 集群。
- en: Let’s start by looking at how we can implement canary releases with Argo Rollouts
    to see how it compares with using plain Kubernetes resources or Knative Services.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看如何使用 Argo Rollouts 实现金丝雀发布，以了解它与使用纯 Kubernetes 资源或 Knative 服务相比如何。
- en: 8.3.1 Argo Rollouts canary rollouts
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 Argo Rollouts 金丝雀发布
- en: We’ll begin by creating our first `Rollout` resource. With Argo Rollouts, we
    will be not defining deployments, because we will delegate this responsibility
    to the Argo Rollouts controller. Instead, we define an Argo Rollouts resource
    that also provides our pod specification (`PodSpec` in the same way that a Deployment
    defines how pods need to be created).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从创建我们的第一个 `Rollout` 资源开始。使用 Argo Rollouts，我们不会定义部署，因为我们将会将这项责任委托给 Argo Rollouts
    控制器。相反，我们定义一个 Argo Rollouts 资源，它也提供了我们的 pod 规范（与 Deployment 以相同的方式定义 pod 需要如何创建的
    `PodSpec`）。
- en: For these examples, we will use only the notifications service from the Conference
    platform application, and we will not use Helm. When using Argo Rollouts, we need
    to deal with a different resource type currently not included in the Conference
    application Helm charts. Argo Rollouts can work perfectly fine with Helm, but
    we will create files to test how Argo Rollouts behave for these examples. You
    can take a look at an Argo Rollouts example using Helm at [https://argoproj.github.io/argo-rollouts/features/helm/](https://argoproj.github.io/argo-rollouts/features/helm/).
    Let’s start by creating an Argo Rollouts resource for the notifications service
    in listing 8.11.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些示例中，我们将只使用来自 Conference 平台应用程序的通知服务，并且不会使用 Helm。当使用 Argo Rollouts 时，我们需要处理目前未包含在
    Conference 应用程序 Helm 图表中的不同资源类型。Argo Rollouts 可以与 Helm 完美地协同工作，但我们将创建文件来测试 Argo
    Rollouts 在这些示例中的行为。你可以在 [https://argoproj.github.io/argo-rollouts/features/helm/](https://argoproj.github.io/argo-rollouts/features/helm/)
    查看使用 Helm 的 Argo Rollouts 示例。让我们首先在列表 8.11 中创建一个用于通知服务的 Argo Rollouts 资源。
- en: Listing 8.11 Argo Rollouts resource definition
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.11 Argo Rollouts 资源定义
- en: '[PRE13]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ① The Rollouts resource definition allows us to configure our workloads to use
    different releases.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ① Rollouts 资源定义允许我们配置我们的工作负载使用不同的发布。
- en: ② Notice that as with deployments, we can set up the number of replicas that
    we want for our notification service.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ② 注意，与部署类似，我们可以设置我们想要的通知服务的副本数量。
- en: ③ This example sets the spec.strategy property to canary, which requires a set
    of specific steps to configure how the canary release will behave for this specific
    service.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 此示例将 spec.strategy 属性设置为 canary，这需要一组特定的步骤来配置 canary 发布将如何为这个特定的服务行为。
- en: ④ The steps defined will be executed in sequence when we make any update on
    our service. For this example, the canary will start with 25% of the traffic and
    wait for manual promotion and then switch to 75%, wait for 10 seconds, and finally
    move to 100%.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 当我们对服务进行任何更新时，定义的步骤将按顺序执行。在这个例子中，canary 将从 25% 的流量开始，等待手动提升，然后切换到 75%，等待 10
    秒，最后移动到 100%。
- en: Note You can find the full file at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/canary-release/rollout.yam](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/canary-release/rollout.yaml)l.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以在这里找到完整的文件 [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/canary-release/rollout.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/canary-release/rollout.yaml)。
- en: This Rollout resource manages the creation of Pods using what we define inside
    the `spec.template` and `spec.replicas` fields. But it adds the `spec.strategy`
    section, which for this case is set to `canary` and defines the steps (amount
    traffic (weight) that will be sent to the canary) in which the rollout will happen.
    As you can see, you can also define a pause between each step. The `duration`
    is expressed in seconds and allows us to have a fine-grain control of how the
    traffic is shifted to the canary version. If you don’t specify the `duration`
    parameter, the rollout will wait there until manual intervention happens. Let’s
    see how this rollout works in action.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Rollout 资源通过我们在 `spec.template` 和 `spec.replicas` 字段中定义的内容来管理 Pods 的创建。但它还添加了
    `spec.strategy` 部分，在这个情况下设置为 `canary`，并定义了 rollout 将发生的步骤（将发送到 canary 的流量（权重））。正如你所看到的，你还可以定义每个步骤之间的暂停。`duration`
    以秒为单位表示，允许我们精细控制流量如何转移到 canary 版本。如果你不指定 `duration` 参数，rollout 将会等待直到手动干预发生。让我们看看这个
    rollout 是如何实际工作的。
- en: 'Let’s apply the Rollout resource to our Kubernetes cluster (check the step-by-step
    tutorial available at [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#canary-releases](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#canary-releases)
    for all the steps):'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将 Rollout 资源应用到我们的 Kubernetes 集群中（请查看在 [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#canary-releases](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#canary-releases)
    可用的逐步教程）：
- en: '[PRE14]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Note This command will also create a Kubernetes service and a Kubernetes ingress
    resource.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此命令还将创建一个 Kubernetes 服务和一个 Kubernetes 入口资源。
- en: Remember, if you are using ArgoCD, instead of manually applying the resource,
    you will push this resource to your Git repository that Argo CD is monitoring.
    Once the resource is applied, we can see that a new Rollout resource is available
    by using `kubectl`, as shown in listing 8.12.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，如果你正在使用 ArgoCD，那么你将不会手动应用资源，而是将此资源推送到 Argo CD 监控的 Git 仓库。一旦资源被应用，我们可以通过使用
    `kubectl` 来看到一个新的 Rollout 资源可用，如列表 8.12 所示。
- en: Listing 8.12 Getting all Argo Rollouts resources
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.12 获取所有 Argo Rollouts 资源
- en: '[PRE15]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This looks pretty much like a normal Kubernetes deployment, but it is not. If
    you use `kubectl get deployments`, you shouldn’t see any deployment resource available
    for our `email-service`. Argo Rollouts replace the use of Kubernetes deployments
    by using Rollouts resources, which are in charge of creating and manipulating
    replica sets, we can check using `kubectl get rs` that our Rollout has created
    a new ReplicaSet. See listing 8.13.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来几乎就像一个正常的 Kubernetes 部署，但它不是。如果您使用 `kubectl get deployments`，您不应该看到任何针对我们的
    `email-service` 的部署资源。Argo Rollouts 通过使用 Rollouts 资源来替代 Kubernetes 部署的使用，这些资源负责创建和操作副本集，我们可以使用
    `kubectl get rs` 检查我们的 Rollout 是否创建了一个新的 ReplicaSet。请参阅列表 8.13。
- en: Listing 8.13 Getting the ReplicaSet created by our Rollout
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.13 获取由我们的 Rollout 创建的 ReplicaSet
- en: '[PRE16]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Argo Rollouts will create and manage these replica sets that we used to manage
    with deployment resources, but in a way that enables us to smoothly perform canary
    releases.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Rollouts 将创建和管理我们以前用部署资源管理的这些副本集，但以一种使我们能够平滑地进行金丝雀发布的方式。
- en: If you have installed the Argo Rollouts Dashboard, you should see our Rollout
    on the main page (see figure 8.12).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已安装了 Argo Rollouts Dashboard，您应该在主页上看到我们的 Rollout（见图 8.12）。
- en: '![](../../OEBPS/Images/08-12.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-12.png)'
- en: Figure 8.12 Argo Rollouts Dashboard
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.12 Argo Rollouts Dashboard
- en: As with deployments, we still need a service and an ingress to route traffic
    to our service from outside the cluster; these resources are included in the step-by-step
    tutorial ([https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts/canary-release](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts/canary-release)).
    If you create the following resources, you can start interacting with the stable
    service and with the canary, as shown in figure 8.13.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 与部署一样，我们仍然需要一个服务和入口来将流量从集群外部路由到我们的服务；这些资源包含在逐步教程中（[https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts/canary-release](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts/canary-release)）。如果您创建了以下资源，您就可以开始与稳定服务以及金丝雀进行交互，如图
    8.13 所示。
- en: '![](../../OEBPS/Images/08-13.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-13.png)'
- en: Figure 8.13 Argo Rollouts canary release Kubernetes resources. The Rollout controls
    the ReplicaSets and manage the approximate weights based on the number of pods
    in each ReplicaSet.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 Argo Rollouts 金丝雀发布 Kubernetes 资源。Rollout 控制副本集，并根据每个副本集中的 Pod 数量管理近似权重。
- en: 'If you create a service and an ingress, you should be able to query the notifications
    service `service/info` endpoint by using the following `curl` command:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您创建了一个服务和入口，您应该能够使用以下 `curl` 命令查询通知服务的 `service/info` 端点：
- en: '[PRE17]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The output should look like listing 8.14.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于列表 8.14。
- en: Listing 8.14 Interacting with version v1.0.0 of the notification service
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.14 与通知服务版本 v1.0.0 交互
- en: '[PRE18]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The request shows the output of the `service/info` endpoint of our notifications
    service. Because we have just created this Rollout resource, the Rollout canary
    strategy mechanism didn’t kick in just yet. Now if we want to update the Rollout
    `spec.template` section with a new container image reference or change environment
    variables, a new revision will be created, and the canary strategy will kick in.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 请求显示了我们的通知服务 `service/info` 端点的输出。因为我们刚刚创建了此 Rollout 资源，所以 Rollout 金丝雀策略机制尚未启动。现在，如果我们想通过更新
    Rollout `spec.template` 部分使用新的容器镜像引用或更改环境变量，将创建一个新的修订版，并启动金丝雀策略。
- en: 'In a new terminal, we can watch the Rollout status before doing any modification,
    so we can see the Rollout mechanism in action when we change the Rollout specification.
    If we want to watch how the rollout progresses after we make some changes, you
    can run the following command in a separate terminal:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的终端中，我们可以在进行任何修改之前查看 Rollout 状态，这样我们就可以在更改 Rollout 规范时看到 Rollout 机制的实际操作。如果我们想查看在做出一些更改后
    Rollout 的进度，您可以在另一个终端中运行以下命令：
- en: '[PRE19]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You should see something like figure 8.14.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于图 8.14 的内容。
- en: '![](../../OEBPS/Images/08-14.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-14.png)'
- en: Figure 8.14 Rollout details using the `argo` plugin for `kubectl`
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 使用 `kubectl` 的 `argo` 插件查看 Rollout 详细信息
- en: 'Let’s modify our `notification-service-canary` rollout by running the following
    command:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过运行以下命令修改我们的 `notification-service-canary` Rollout：
- en: '[PRE20]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'As soon as we replace the container image used by the Rollout, the rollout
    strategy will kick in. If you go back to the terminal where you are watching the
    rollout, you should see that a new `# revision: 2` was created; see figure 8.15.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '一旦我们替换了Rollout使用的容器镜像，滚动部署策略就会启动。如果你回到你在那里观察滚动部署的终端，你应该会看到创建了一个新的`# revision:
    2`；参见图8.15。'
- en: '![](../../OEBPS/Images/08-15.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-15.png)'
- en: Figure 8.15 Rollout progress after updating the service
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 更新服务后的滚动部署进度
- en: You can see that revision 2 is labeled as the “canary” and the status of the
    rollout is “॥ Paused” and only one pod is created for the canary. So far, the
    rollout has only executed the first step, as in listing 8.15.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到修订版2被标记为“金丝雀”，滚动部署的状态为“॥ 暂停”，并且只为金丝雀创建了一个Pod。到目前为止，滚动部署只执行了第一步，如列表8.15所示。
- en: Listing 8.15 Steps definition in our Rollout
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.15 Rollout中的步骤定义
- en: '[PRE21]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can also check the status of the canary Rollout in the dashboard, as shown
    in figure 8.16.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以在仪表板中检查金丝雀滚动部署的状态，如图8.16所示。
- en: '![](../../OEBPS/Images/08-16.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-16.png)'
- en: Figure 8.16 A canary release has been created with approximately 20% of the
    traffic being routed to it.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 已创建金丝雀发布，大约20%的流量被路由到它。
- en: The Rollout is currently paused waiting for manual intervention. We can now
    test that our canary is receiving traffic to see if we are happy with how the
    canary is working before continuing the rollout process. To do that, we can query
    the “service/info” endpoint again to see that approximately 25% of the time we
    hit the canary, as in listing 8.16.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: Rollout目前处于暂停状态，等待人工干预。我们现在可以测试我们的金丝雀是否正在接收流量，以查看我们是否对金丝雀的工作情况满意，然后再继续滚动部署过程。为此，我们可以再次查询“service/info”端点，以查看大约25%的时间我们命中了金丝雀，如列表8.16所示。
- en: Listing 8.16 Example output hitting version v1.1.10 from our notification service
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.16 从我们的通知服务中命中版本v1.1.10的示例输出
- en: '[PRE22]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We can see that one request hit our stable version and one went to the canary.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到有一个请求击中了我们的稳定版本，另一个请求则去了金丝雀。
- en: Argo Rollouts is not dealing with traffic management; in this case, the Rollout
    resource is only dealing with the underlying ReplicaSet objects and their replicas.
    You can check the `ReplicaSets` by running `kubectl get rs`, as in listing 8.17.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Argo Rollouts不处理流量管理；在这种情况下，Rollout资源仅处理底层的ReplicaSet对象及其副本。你可以通过运行`kubectl
    get rs`来检查`ReplicaSets`，如列表8.17所示。
- en: Listing 8.17 Checking the ReplicaSets associated to our Rollout
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.17 检查与我们的Rollout关联的ReplicaSets
- en: '[PRE23]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The traffic management between these different pods (canary and stable pods)
    is being managed by the Kubernetes Service resource, so to see our request hitting
    both the canary and the stable version pods, we need to go through the Kubernetes
    service. I am only mentioning this because if you use `kubectl port-forward svc/notifications-service
    8080:80`, for example, you might be tempted to think that traffic is being forwarded
    to the Kubernetes service (because we are using `svc/notifications-service`),
    but `kubectl port-forward` resolves to a pod instance and connects to a single
    pod, allowing you only to hit the canary or a stable pod. For this reason, we
    have used an ingress, which will use the service to load balance traffic and hit
    all the pods that are matching to the service selector.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这些不同Pod（金丝雀和稳定Pod）之间的流量管理是由Kubernetes服务资源管理的，因此要看到我们的请求同时击中了金丝雀和稳定版本Pod，我们需要通过Kubernetes服务。我之所以提到这一点，是因为如果你使用`kubectl
    port-forward svc/notifications-service 8080:80`，例如，你可能会想交通是被转发到了Kubernetes服务（因为我们使用了`svc/notifications-service`），但`kubectl
    port-forward`解析为一个Pod实例并连接到单个Pod，这仅允许你击中金丝雀或稳定Pod。因此，我们使用了ingress，它将使用服务来负载均衡流量并击中所有匹配服务选择器的Pod。
- en: 'If we are happy with the results, we can continue the rollout process by executing
    the following command, which promotes the canary to be the stable version:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对结果满意，我们可以通过执行以下命令继续滚动部署过程，该命令将金丝雀提升为稳定版本：
- en: '[PRE24]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Although we have just manually promoted the rollout, the best practice would
    be utilizing Argo Rollouts automated analysis steps, which we will dig into in
    section 8.3.2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们刚刚手动提升了滚动部署，但最佳实践是利用Argo Rollouts的自动化分析步骤，我们将在第8.3.2节中深入探讨。
- en: If you look at the Argo Rollouts Dashboard, you will notice that you can also
    promote the rollout to move forward using the Button Promote in the Rollout. Promotion
    in this context only means that the rollout can continue to execute the next steps
    defined in the `spec.strategy` section, as shown in listing 8.18.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您查看 Argo Rollouts 仪表板，您会注意到您还可以使用 Rollout 中的“提升”按钮来提升 rollout 以继续前进。在这个上下文中，提升仅意味着
    rollout 可以继续执行在 `spec.strategy` 部分中定义的下一步，如图表 8.18 所示。
- en: Listing 8.18 Rollouts steps definition with 10 seconds pause
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.18 带有 10 秒暂停的 Rollouts 步骤定义
- en: '[PRE25]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: After the manual promotion, the weight is going to be set to 75%, followed by
    a pause of 10 seconds, to finally set the wait to 100%. At that point, you should
    see that revision 1 is being downscaled while progressively revision 2 is being
    upscaled to take all the traffic. See figure 8.17, which shows the final state
    of the rollout.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 在手动提升后，权重将被设置为 75%，然后暂停 10 秒，最终将等待时间设置为 100%。此时，您应该看到修订版 1 正在逐步缩放，而修订版 2 正在逐步提升以接管所有流量。请参阅图
    8.17，它显示了 rollout 的最终状态。
- en: '![](../../OEBPS/Images/08-17.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-17.png)'
- en: Figure 8.17 Rollout finished with all the traffic shifted to revision 2
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.17 所有流量已切换到修订版 2 的 Rollout 完成
- en: You can see this rollout progression live in the dashboard as well in figure
    8.18.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以在图 8.18 中的仪表板中实时查看此 rollout 进展。
- en: '![](../../OEBPS/Images/08-18.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-18.png)'
- en: Figure 8.18 The canary revision is promoted to be the stable version.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.18 金丝雀修订版被提升为稳定版本。
- en: As you can see, revision 1 was downscaled to have zero pods, and revision 2
    is now marked as the stable version. If you check the ReplicaSets, you will see
    the same output, as shown in listing 8.19.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，修订版 1 已缩放到零个 pod，修订版 2 现在标记为稳定版本。如果您检查 ReplicaSets，您将看到与列表 8.19 中相同的输出。
- en: Listing 8.19 The ReplicaSet responsible for revision 1 is downscaled to 0
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.19 负责修订版 1 的 ReplicaSet 已缩放到 0
- en: '[PRE26]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We have successfully created, tested, and promoted a canary release with Argo
    Rollouts!
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经成功创建、测试并提升了 Argo Rollouts 的金丝雀发布！
- en: Compared to what we have seen in section 8.1 for canary releases, using two
    deployment resources to implement the same pattern with Argo Rollouts, you have
    full control over how your canary release is promoted, how much time you want
    to wait before shifting more traffic to the canary and how many manual interventions
    steps do you want to add. Let’s now see how a blue/green deployment works with
    Argo Rollouts.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们在 8.1 节中看到的金丝雀发布相比，使用两个部署资源通过 Argo Rollouts 实现相同的模式，你可以完全控制金丝雀发布如何提升，你希望在将更多流量切换到金丝雀之前等待多少时间，以及你希望添加多少手动干预步骤。现在让我们看看
    Argo Rollouts 中的蓝绿部署是如何工作的。
- en: 8.3.2 Argo Rollouts blue/green deployments
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 Argo Rollouts 蓝绿部署
- en: In section 8.1 we covered the advantages and the reasons why you would be interested
    in doing blue/green deployments using Kubernetes basic building blocks. We have
    also seen how manual the process is and how these manual steps can open the door
    for silly mistakes that can bring our services down. In this section, we will
    look at how Argo Rollouts allows us to implement blue/green deployments following
    the same approach we previously used for canary deployments. Check the step-by-step
    tutorial for Argo Rollouts blue/green deployments at [https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#bluegreen-deployments](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#bluegreen-deployments).
    Let’s look at what our Rollout with a BlueGreen strategy looks like in listing
    8.20.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在 8.1 节中，我们介绍了使用 Kubernetes 基本构建块进行蓝绿部署的优势以及您可能感兴趣的原因。我们还看到了这个过程是多么手动，以及这些手动步骤如何打开可能导致我们的服务崩溃的愚蠢错误的门。在本节中，我们将探讨
    Argo Rollouts 如何使我们能够以与之前用于金丝雀部署相同的方法实现蓝绿部署。请查看 Argo Rollouts 蓝绿部署的逐步教程[https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#bluegreen-deployments](https://github.com/salaboy/platforms-on-k8s/tree/main/chapter-8/argo-rollouts#bluegreen-deployments)。让我们看看具有蓝绿策略的
    Rollout 在列表 8.20 中的样子。
- en: Listing 8.20 Rollout defining a BlueGreen strategy
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.20 定义蓝绿策略的 Rollout
- en: '[PRE27]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Note You can find the full file at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/blue-green/rollout.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/blue-green/rollout.yaml).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以在[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/blue-green/rollout.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-8/argo-rollouts/blue-green/rollout.yaml)找到完整的文件。
- en: 'Let’s apply the resources for this Rollout resource to work (two Kubernetes
    services and an ingress):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个Rollout资源的资源，使其工作（两个Kubernetes服务和入口）：
- en: '[PRE28]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'We are using the same `spec.template` as before, but now we are setting the
    strategy of the rollout to be `blueGreen`, and because of that, we need to configure
    the reference to two Kubernetes services. One service will be the Active service
    (Blue), which is serving production traffic, and the other one is the Green service
    that we want to preview but without routing production traffic to it. The `autoPromotionEnabled`:
    `false` is required to allow for manual intervention for the promotion to happen.
    By default, the rollout will be automatically promoted as soon as the new ReplicaSet
    is ready/available. You can watch the rollout running the following command or
    in the Argo Rollouts Dashboard:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '我们正在使用与之前相同的`spec.template`，但现在我们将Rollout的策略设置为`blueGreen`，因此我们需要配置对两个Kubernetes服务的引用。一个服务将是活动服务（蓝色），它正在处理生产流量，另一个是我们想要预览的绿色服务，但不将其路由到生产流量。`autoPromotionEnabled`:
    `false`是必需的，以允许手动干预以进行提升。默认情况下，一旦新的ReplicaSet准备好/可用，部署将自动提升。你可以通过以下命令或Argo Rollouts仪表板来监视部署：'
- en: '[PRE29]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: In the following figure, you should see output similar to the output we saw
    for the canary release.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下图中，你应该看到与我们在金丝雀发布中看到的输出类似的输出。
- en: '![](../../OEBPS/Images/08-19.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-19.png)'
- en: Figure 8.19 Checking the state of our BlueGreen Rollout
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19检查我们的蓝绿部署状态
- en: And in the dashboard, see figure 8.20.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在仪表板中，见图8.20。
- en: '![](../../OEBPS/Images/08-20.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-20.png)'
- en: Figure 8.20 Blue/green deployment in the Argo Rollouts Dashboard.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 Argo Rollouts仪表板中的蓝/绿部署
- en: 'We can interact with revision #1 using an ingress to the service and then send
    a request like listing 8.21.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用服务入口与修订版#1进行交互，然后发送类似于列表8.21的请求。
- en: Listing 8.21 Hitting revision 1 of our service
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.21访问我们的服务的修订版1
- en: '[PRE30]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'If we now make changes to our Rollout `spec.template` the blueGreen strategy
    will kick in. For this example, the expected result that we want to see is that
    the previewService is now routing traffic to the second revision that is created
    when we make changes to the rollout:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们现在更改我们的Rollout `spec.template`，蓝绿策略将启动。对于这个例子，我们想要看到的结果是previewService现在正在路由流量到我们在更改部署时创建的第二修订版：
- en: '[PRE31]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The rollout mechanism will kick in, and it will automatically create a new ReplicaSet
    with revision 2 that includes our changes. Argo Rollouts for blue/green deployments
    will use selectors to route traffic to our new revision by modifying the `previewService`
    that we referenced in our Rollout definition.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 部署机制将启动，并且它将自动创建一个新的修订版2的ReplicaSet，其中包含我们的更改。Argo Rollouts用于蓝/绿部署将使用选择器通过修改我们在Rollout定义中引用的`previewService`来路由流量到我们的新修订版。
- en: If you describe the `notifications-service-green` Kubernetes service, you will
    notice that a new selector was added, as in listing 8.22.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你描述`notifications-service-green` Kubernetes服务，你会注意到添加了一个新的选择器，如列表8.22所示。
- en: Listing 8.22 Kubernetes service selectors managed by Argo Rollouts
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.22 Argo Rollouts管理的Kubernetes服务选择器
- en: '[PRE32]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: This selector matches with the revision 2 ReplicaSet that was created when we
    made the changes, as shown in listing 8.23.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 此选择器与我们在进行更改时创建的修订版2的ReplicaSet匹配，如列表8.23所示。
- en: Listing 8.23 The ReplicaSet uses the same labels to match the service definition
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.23 ReplicaSet使用相同的标签来匹配服务定义
- en: '[PRE33]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: By using the selector and labels, the Rollout with the `blueGreen` strategy
    is handling these links automatically for us. This avoids the need to create these
    labels manually and makes sure they match. As shown in figure 8.21, you can check
    now that there are two revisions (and ReplicaSets) with two pods each.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用选择器和标签，具有`blueGreen`策略的Rollout正在为我们自动处理这些链接。这避免了手动创建这些标签的需要，并确保它们匹配。如图8.21所示，你现在可以检查现在有两个修订版（和ReplicaSets），每个都有两个Pod。
- en: '![](../../OEBPS/Images/08-21.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-21.png)'
- en: Figure 8.21 Both Blue and Green services have the same amount of replicas running
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.21蓝色和绿色服务运行着相同数量的副本
- en: In the Argo Rollouts Dashboard you should see the same information as in figure
    8.22.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在Argo Rollouts仪表板中，你应该看到与图8.22相同的信息。
- en: '![](../../OEBPS/Images/08-22.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-22.png)'
- en: Figure 8.22 Argo Rollouts Dashboard Blue and Green revisions are up
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22 Argo Rollouts仪表板蓝绿修订版已启动
- en: 'We can now interact with the Green service (revision #2) using a different
    path in our Ingress in the following listing.'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过以下列表中的不同路径与Green服务（修订版#2）进行交互。
- en: Listing 8.24 interacting with revision 2 (our Green service)
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.24与修订版2（我们的Green服务）交互
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Once we have the Green service running, the Rollout is in a Paused state until
    we decide to promote it to be the stable service. Figure 8.23 shows how the Rollout
    resource will orchestrate the many replicas the Green and Blue services will have
    depending on the progress of the rollout.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦Green服务启动运行，Rollout将处于暂停状态，直到我们决定将其提升为稳定服务。图8.23展示了Rollout资源将如何根据Rollout的进度来协调Green和Blue服务所拥有的多个副本。
- en: '![](../../OEBPS/Images/08-23.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![图8-23](../../OEBPS/Images/08-23.png)'
- en: Figure 8.23 Blue/green deployment with Kubernetes resources
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.23 使用Kubernetes资源的蓝绿部署
- en: 'Because we now have two services, we can access both at the same time and make
    sure that our Green (green-service) is working as expected before promoting it
    to be our main (blue) service. While the service is in preview, other services
    in the cluster can start routing traffic to it for testing purposes, but to route
    all the traffic and replace our Blue service with our Green service, we can use
    once again the Argo Rollouts promotion mechanism from the terminal using the CLI
    or from the Argo Rollouts Dashboard. Try to promote the Rollout using the Dashboard
    now instead of using `kubectl`. Remember that the command for promoting the rollout
    from the terminal looks like this:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们现在有两个服务，我们可以同时访问它们，并在将其提升为主要（蓝色）服务之前确保我们的Green（green-service）按预期工作。当服务处于预览状态时，集群中的其他服务可以开始将其用于测试目的的路由流量，但要路由所有流量并用我们的Green服务替换Blue服务，我们还可以再次使用终端中的CLI或从Argo
    Rollouts仪表板使用Argo Rollouts提升机制。现在尝试使用仪表板而不是`kubectl`来提升Rollout。请记住，从终端提升Rollout的命令看起来像这样：
- en: '[PRE35]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Notice that a 30-second delay is added by default before the scaling down of
    our revision #1 (this can be controlled using the property called `scaleDownDelaySeconds`),
    but the promotion (switching labels to the services) happens the moment we hit
    the `PROMOTE` button, as shown in figure 8.24.'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在缩小修订版#1之前默认添加了30秒的延迟（这可以通过名为`scaleDownDelaySeconds`的属性来控制），但提升（切换标签到服务）发生在我们点击`PROMOTE`按钮的瞬间，如图8.24所示。
- en: '![](../../OEBPS/Images/08-24.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图8-24](../../OEBPS/Images/08-24.png)'
- en: Figure 8.24 Green service promotion using the Argo Rollouts Dashboard
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.24 使用Argo Rollouts仪表板进行Green服务提升
- en: This promotion only switches labels to the services’ resources, which automatically
    changes the routing tables to now forward all the traffic from the Active service
    to our Green service. If we make more changes to our Rollout, the process will
    start again, and the preview service will point to a new revision which will include
    these changes. Now that we have seen the basics of canary releases and blue/green
    deployments with Argo Rollouts, let’s take a look at more advanced mechanisms
    provided by Argo Rollouts.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这次提升仅将标签切换到服务的资源上，这会自动更改路由表，现在将所有来自活动服务的流量转发到我们的Green服务。如果我们对Rollout进行更多更改，过程将重新开始，预览服务将指向一个包含这些更改的新修订版。现在我们已经了解了使用Argo
    Rollouts进行金丝雀发布和蓝绿部署的基本知识，让我们来看看Argo Rollouts提供的更多高级机制。
- en: 8.3.3 Argo Rollouts analysis for progressive delivery
  id: totrans-305
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 Argo Rollouts的渐进式交付分析
- en: So far, we have managed to have more control over our different release strategies,
    but Argo Rollouts shine by providing the AnalysisTemplate CRD, which lets us ensure
    that our canary and Green services are working as expected when progressing through
    our rollouts. These analyses are automated and serve as gates for our Rollouts
    not to progress unless the analysis probes are successful.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经能够更好地控制我们的不同发布策略，但Argo Rollouts通过提供AnalysisTemplate CRD而显得出色，这使我们能够确保在Rollout过程中，我们的金丝雀和Green服务按预期工作。这些分析是自动化的，并作为Rollout的关卡，确保分析探测成功后才会继续进行。
- en: These analyses can use different providers to run the probes, ranging from Prometheus,
    Datadog ([https://www.datadoghq.com/](https://www.datadoghq.com/)), New Relic
    ([https://newrelic.com/](https://newrelic.com/)), and Dynatrace ([https://www.dynatrace.com/](https://www.dynatrace.com/)),
    among others, providing maximum flexibility to define these automated tests against
    the new revisions of our services.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分析可以使用不同的提供商来运行探针，包括 Prometheus、Datadog ([https://www.datadoghq.com/](https://www.datadoghq.com/)）、New
    Relic ([https://newrelic.com/](https://newrelic.com/)）和 Dynatrace ([https://www.dynatrace.com/](https://www.dynatrace.com/)）等，从而提供最大的灵活性来定义针对我们服务新版本的这些自动化测试。
- en: Figure 8.25 shows how AnalysisTemplates allows Argo Rollouts to create AnalysisRuns
    to validate that the new version that is rolled out is behaving as expected by
    looking at service metrics. `AnalysisRuns` will probe the service for metrics
    and only proceed with the `Rollout` steps if the metrics match the success conditions
    defined in the `AnalysisTemplate`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25 展示了 AnalysisTemplates 如何允许 Argo Rollouts 创建 AnalysisRuns 以验证推出的新版本是否通过查看服务指标按预期运行。`AnalysisRuns`
    将对服务进行指标探测，并且只有当指标匹配在 `AnalysisTemplate` 中定义的成功条件时，才会继续执行 `Rollout` 步骤。
- en: '![](../../OEBPS/Images/08-25.png)'
  id: totrans-309
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-25.png)'
- en: Figure 8.25 Argo Rollouts and analysis working together to make sure that our
    new revisions are sound before shifting more traffic to them. When receiving the
    signal to move forward to the next step of the Rollout, an `AnalysisRun` is created
    to probe the service by running a query defined in the `AnalysisTemplate`. The
    `AnalysisRun` result affect if the `Rollout`’s update will continue, abort, or
    pause.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.25 Argo Rollouts 和分析协同工作以确保我们的新版本在流量转移之前是可靠的。当收到前进到 Rollout 下一个步骤的信号时，将创建一个
    `AnalysisRun` 来通过运行在 `AnalysisTemplate` 中定义的查询来探测服务。`AnalysisRun` 的结果将影响 `Rollout`
    的更新是继续、中止还是暂停。
- en: For canary release, the analysis can be triggered as part of the step definitions,
    meaning between arbitrary steps, to start at a predefined step or for every step
    defined in the Rollout. An `AnalysisTemplate` using the Prometheus provider definition
    looks like listing 8.25.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 对于金丝雀发布，分析可以作为步骤定义的一部分触发，这意味着在任意步骤之间，或者在 Rollout 中定义的每个步骤开始，或者为每个步骤定义。使用 Prometheus
    提供商定义的 `AnalysisTemplate` 看起来像列表 8.25。
- en: Listing 8.25 AnalysisTemplate resource provided by Argo Rollouts
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.25 Argo Rollouts 提供的 AnalysisTemplate 资源
- en: '[PRE36]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then in our Rollout, we can refer to this template and define when a new AnalysisRun
    will be created, for example, if we want to run the first analysis after step
    2 (listing 8.26).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们的 Rollout 中，我们可以引用此模板并定义何时创建新的 AnalysisRun，例如，如果我们想在步骤 2 之后运行第一次分析（列表
    8.26）。
- en: Listing 8.26 Selecting analysis template when defining canary release
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.26 在定义金丝雀发布时选择分析模板
- en: '[PRE37]'
  id: totrans-316
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: As mentioned before, the analysis can also be defined as part of the steps.
    In that case, our steps definition will look like listing 8.27.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，分析也可以定义为步骤的一部分。在这种情况下，我们的步骤定义将类似于列表 8.27。
- en: Listing 8.27 Using AnalysisTemplate reference as a step in the rollout
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.27 在 Rollout 中使用 AnalysisTemplate 引用作为步骤
- en: '[PRE38]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: For rollouts using a BlueGreen strategy, we can trigger Analysis runs pre- and
    post-promotion. Figure 8.26 shows the PrePromotionAnalysis step by running the
    SmokeTestTemplate. This will gate the rollout to switch traffic to the Green service
    if the AnalysisRun fails.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对于使用蓝绿策略的 Rollout，我们可以在推广前后触发分析运行。图 8.26 显示了通过运行 SmokeTestTemplate 来执行 PrePromotionAnalysis
    步骤。如果分析运行失败，这将阻止 Rollout 切换流量到绿色服务。
- en: '![](../../OEBPS/Images/08-26.png)'
  id: totrans-321
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/08-26.png)'
- en: Figure 8.26 Argo Rollouts with blueGreen deployments and PrePromotionAnalysis
    in action. When the promotion is triggered on the Rollout it will create a new
    `AnalysisRun` using the `SmokeTestsTemplate` before switching the labels to route
    traffic to the Preview Service. Only if the `AnalysisRun` is successful the preview
    service becomes the new Active Service.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.26 Argo Rollouts 与蓝绿部署和 PrePromotionAnalysis 一起工作。当在 Rollout 上触发推广时，它将使用
    `SmokeTestsTemplate` 创建一个新的 `AnalysisRun`，然后在切换标签以路由流量到预览服务之前。只有当 `AnalysisRun`
    成功时，预览服务才成为新的活动服务。
- en: Here is an example of PrePromotionAnalysis configured in our Rollout in listing
    8.28.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是我们在 Rollout 中配置的 PrePromotionAnalysis 的示例，见列表 8.28。
- en: Listing 8.28 Defining a PrePromotionAnalysis as part of a BlueGreen rollout
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.28 在蓝绿 Rollout 中定义 PrePromotionAnalysis
- en: '[PRE39]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: For PrePromotion tests, run a new AnalysisRun test before switching traffic
    to the Green service, and only if the test is successful will the labels be updated.
    For PostPromotion, the test will run after the labels were switched to the Green
    service, and if the AnalysisRun fails, the rollout can automatically revert the
    labels to the previous version. This is possible because the Blue service will
    not be downscaled until the AnalysisRun finishes.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 对于预促销测试，在将流量切换到绿色服务之前，运行一个新的AnalysisRun测试，并且只有当测试成功时才会更新标签。对于后促销，测试将在标签切换到绿色服务之后运行，如果AnalysisRun失败，Rollout可以自动将标签回滚到上一个版本。这是可能的，因为蓝色服务不会在AnalysisRun完成之前进行缩放。
- en: 'I recommend you check the Analysis section of the official documentation as
    it contains a detailed explanation of all the providers and knobs that you can
    use to make sure that your Rollouts go smoothly: [https://argoproj.github.io/argo-rollouts/features/analysis/](https://argoproj.github.io/argo-rollouts/features/analysis/).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议您查看官方文档中的分析部分，因为它包含了所有提供者和旋钮的详细解释，这些提供者和旋钮可以帮助确保您的Rollouts顺利运行：[https://argoproj.github.io/argo-rollouts/features/analysis/](https://argoproj.github.io/argo-rollouts/features/analysis/)。
- en: 8.3.4 Argo Rollouts and traffic management
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 Argo Rollouts和流量管理
- en: Finally, it is worth mentioning that Rollouts used the number of pods available
    to approximate the weights we define for canary releases. While this is a good
    start and a simple mechanism, sometimes we need more control over how traffic
    is routed to different revisions. We can use the power of service meshes and load
    balancers to write more precise rules about which traffic is routed to our canary
    releases.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，值得一提的是，Rollouts使用可用的Pod数量来近似我们为金丝雀发布定义的权重。虽然这是一个良好的开始，也是一个简单的机制，但有时我们需要对如何将流量路由到不同的版本有更多的控制。我们可以利用服务网格和负载均衡器的力量来编写更精确的规则，关于哪些流量被路由到我们的金丝雀发布。
- en: 'Argo Rollouts can be configured with different `trafficRouting` rules, depending
    on which traffic management tool we have available in our Kubernetes cluster.
    Argo Rollouts today supports: Istio, AWS ALB Ingress Controller, Ambassador Edge
    Stack, Nginx Ingress Controller, Service Mesh Interface (SMI), and Traefik Proxy,
    among others. As described in the documentation, if we have more advanced traffic
    management capabilities, we can implement techniques like:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 根据我们Kubernetes集群中可用的流量管理工具，Argo Rollouts可以配置不同的`trafficRouting`规则。目前，Argo Rollouts支持：Istio、AWS
    ALB Ingress Controller、Ambassador Edge Stack、Nginx Ingress Controller、Service
    Mesh Interface (SMI)和Traefik Proxy等。如文档所述，如果我们有更高级的流量管理功能，我们可以实现如下技术：
- en: Raw percentages (i.e., 5% of traffic should go to the new version while the
    rest goes to the stable version)
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始百分比（即，5%的流量应发送到新版本，其余发送到稳定版本）
- en: Header-based routing (i.e., send requests with a specific header to the new
    version)
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于头的路由（即，发送带有特定头的请求到新版本）
- en: Mirrored traffic where all the traffic is copied and sent to the new version
    in parallel (but the response is ignored)
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 镜像流量，其中所有流量都并行复制并发送到新版本（但忽略响应）
- en: By using tools like Istio in conjunction with Argo Rollouts, we can enable developers
    to test features that are only available by setting specific headers or to forward
    copies of the production traffic to the canaries to validate that they are behaving
    as they should.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 通过结合使用Istio等工具和Argo Rollouts，我们可以使开发者能够测试只有通过设置特定头或转发生产流量的副本到金丝雀以验证它们是否按预期行为的功能。
- en: Here is an example of configuring a Rollout to mirror 35% of the traffic to
    the canary release, which has a 25% weight. This means that 35% of the traffic
    routed to the stable service will be copied and forwarded to the canary. By using
    this technique, we don’t risk any of the production traffic, because Istio is
    copying requests for testing purposes, as shown in listing 8.29.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个配置Rollout以将35%的流量镜像到具有25%权重的金丝雀发布的示例。这意味着将有35%的流量被路由到稳定服务后，会复制并转发到金丝雀。通过使用这种技术，我们不会冒任何生产流量的风险，因为Istio正在复制请求以进行测试，如列表8.29所示。
- en: Listing 8.29 Using Istio for advanced (weight-based) traffic split
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.29 使用Istio进行高级（基于权重的）流量分割
- en: '[PRE40]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: As you can see, this simple example already requires knowledge of Istio Virtual
    Services and a more advanced configuration that is out of the scope of this section.
    I strongly recommend checking *Istio in Action* by Christian Posta and Rinor Maloku
    (Manning Publications, 2022) if you want to learn about Istio. Figure 8.27 shows
    Rollouts configured to use Istio traffic management capabilities to do weight-based
    routing.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，这个简单的例子已经需要了解 Istio 虚拟服务以及超出本节范围的更高级配置。如果您想了解 Istio，我强烈推荐阅读 Christian Posta
    和 Rinor Maloku（Manning Publications，2022 年）的《Istio in Action》。图 8.27 显示了配置为使用
    Istio 流量管理能力进行基于权重的路由的 Rollouts。
- en: '![](../../OEBPS/Images/08-27.png)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/08-27.png)'
- en: Figure 8.27 Traffic mirroring to canary release using Istio. Using tools like
    Istio to set `trafficRouting` enables our canary workloads to experience real
    life traffic that the stable service is receiving. The Rollout Controller is in
    charge of configuring Istio Virtual Services to do the work for us and has a fine-grained
    control about which traffic is delivered to the Service.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.27 使用 Istio 进行流量镜像到金丝雀发布。使用 Istio 等工具设置 `trafficRouting` 可以使我们的金丝雀工作负载体验到稳定服务正在接收的真实流量。Rollout
    控制器负责配置 Istio 虚拟服务为我们完成工作，并对哪些流量被发送到服务有精细的控制。
- en: When using “trafficManagement” features, the Rollout canary strategy will behave
    differently than when we are not using any rules. More specifically, the Stable
    version of the service will not be downscaled when going through a canary release
    rollout. This ensures that the Stable service can handle 100% of the traffic.
    The usual calculations apply to the canary replica count.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用“trafficManagement”功能时，Rollout 金丝雀策略的行为将与不使用任何规则时不同。更具体地说，当通过金丝雀发布滚动时，服务的稳定版本不会被缩放。这确保了稳定服务可以处理
    100% 的流量。通常的计算适用于金丝雀副本数量。
- en: I strongly recommend checking the official documentation ([https://argoproj.github.io/argo-rollouts/features/traffic-management/](https://argoproj.github.io/argo-rollouts/features/traffic-management/))
    and following the examples there, because the rollouts need to be configured differently
    depending on the service mesh that you have available.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议查看官方文档（[https://argoproj.github.io/argo-rollouts/features/traffic-management/](https://argoproj.github.io/argo-rollouts/features/traffic-management/)）并遵循那里的示例，因为根据您可用的服务网格，Rollouts
    的配置可能需要不同。
- en: 8.4 Linking back to platform engineering
  id: totrans-343
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 回到平台工程
- en: In this chapter, we have seen what can be achieved with basic Kubernetes building
    blocks and how tools like Argo Rollouts or Knative Serving simplify the life of
    teams by releasing new versions of their applications to Kubernetes.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们看到了使用基本的 Kubernetes 构建块可以实现什么，以及像 Argo Rollouts 或 Knative Serving 这样的工具如何通过发布它们应用程序的新版本到
    Kubernetes 来简化团队的生活。
- en: It is unfortunate that as of today, in 2023, Argo Rollouts and Knative Serving
    haven’t been integrated yet ([https://github.com/argoproj/argo-rollouts/issues/2186](https://github.com/argoproj/argo-rollouts/issues/2186)),
    because both communities would benefit from a consolidated way of defining release
    strategies instead of duplicating functionality. I like the Knative Serving building
    blocks that facilitate implementing these release strategies. On the other hand,
    I like how Argo Rollouts takes things to the next level with the concepts of `AnalysisTemplates`
    to ensure we can automatically test and validate new releases. The future is promising,
    because both projects are looking for further integrations with the Gateway API
    standard ([https://gateway-api.sigs.k8s.io/](https://gateway-api.sigs.k8s.io/))
    to unify how advanced traffic routing capabilities are managed in Kubernetes.
    Tools like Istio, Knative Serving, and Argo Rollouts have active initiatives to
    support this new standard.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，截至 2023 年今天，Argo Rollouts 和 Knative Serving 尚未集成（[https://github.com/argoproj/argo-rollouts/issues/2186](https://github.com/argoproj/argo-rollouts/issues/2186)），因为这两个社区都将从定义发布策略的统一方式中受益，而不是重复功能。我喜欢
    Knative Serving 的构建块，它有助于实现这些发布策略。另一方面，我喜欢 Argo Rollouts 通过 `AnalysisTemplates`
    的概念将事物提升到下一个层次，以确保我们可以自动测试和验证新版本。未来是光明的，因为这两个项目都在寻求与 Gateway API 标准的进一步集成（[https://gateway-api.sigs.k8s.io/](https://gateway-api.sigs.k8s.io/)），以统一在
    Kubernetes 中管理高级流量路由能力。像 Istio、Knative Serving 和 Argo Rollouts 这样的工具都有积极的倡议来支持这个新标准。
- en: I firmly believe that you will face delivery challenges sooner or later in your
    Kubernetes journey, and having these mechanisms available inside your clusters
    will increase your confidence to release more software faster. Hence, I don’t
    take the evaluation of these tools lightly. Make sure you plan time for your teams
    to research and choose which tools they will use to implement these release strategies;
    many software vendors can assist you and provide recommendations too.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我坚信，你在Kubernetes之旅中迟早会遇到交付挑战，而在你的集群内部拥有这些机制将增加你更快发布更多软件的信心。因此，我不会轻率地评估这些工具。确保你为你的团队规划时间，让他们研究和选择他们将用于实施这些发布策略的工具；许多软件供应商也可以提供帮助和建议。
- en: From a platform engineering perspective, we have looked into how to enable developers
    to be more efficient by providing them application-level APIs that they can consume
    no matter their language. We have enabled other teams, like product managers or
    more business-focused teams, to decide when certain features are enabled and how
    to perform different release strategies depending on their needs. We enabled operations
    teams to define the rules safely to validate that new Rollouts are safe and working
    as expected.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 从平台工程的角度来看，我们研究了如何通过提供开发者可以消费的应用级API来提高他们的效率，无论他们的语言是什么。我们已经使其他团队，如产品经理或更多以业务为导向的团队，能够决定何时启用某些功能以及如何根据他们的需求执行不同的发布策略。我们还使运维团队能够安全地定义规则，以验证新的Rollouts是否安全且按预期工作。
- en: While the focus of this chapter wasn’t analyzing tools like Knative Serving
    in detail, it is important to mention containers-as-a-service and function-as-a-service
    features when building platforms, because these represent common traits that platform
    teams might want to expose to their users. I would also recommend checking Knative
    Functions ([https://knative.dev/docs/functions/](https://knative.dev/docs/functions/)),
    now an official Knative module, because the project highlights the importance
    of building a function-based development workflow based on Knative and leveraging
    the polyglot approach of Kubernetes.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然本章的重点不是详细分析像Knative Serving这样的工具，但在构建平台时，提及容器即服务（container-as-a-service）和函数即服务（function-as-a-service）功能是很重要的，因为这些代表了平台团队可能希望向用户公开的常见特性。我还建议检查Knative
    Functions（[https://knative.dev/docs/functions/](https://knative.dev/docs/functions/)），现在是一个官方的Knative模块，因为这个项目强调了基于Knative构建基于函数的开发工作流程并利用Kubernetes的多语言方法的重要性。
- en: Figure 8.28 shows tools like Knative Serving provide basic building blocks for
    platform teams to expose different ways to deploy and run different teams’ workloads.
    By adding advanced traffic management, teams can implement more complex release
    strategies. Argo Rollouts and Knative Serving work with Istio Service Mesh, which
    will cover other important aspects, such as mTLS for encryption and observability.
    Tools like Dapr and OpenFeature fit perfectly in this picture by providing standard
    interfaces for teams to use while at the same time enabling platform teams to
    define the backing implementations without committing to a single solution.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28展示了像Knative Serving这样的工具为平台团队提供了基本的构建块，以便以不同的方式部署和运行不同团队的负载。通过添加高级流量管理，团队可以实现更复杂的发布策略。Argo
    Rollouts和Knative Serving与Istio服务网格协同工作，这将涵盖其他重要方面，例如用于加密和可观察性的mTLS。像Dapr和OpenFeature这样的工具通过为团队提供标准接口来使用，同时使平台团队能够定义后端实现，而不必承诺单一解决方案。
- en: '![](../../OEBPS/Images/08-28.png)'
  id: totrans-350
  prefs: []
  type: TYPE_IMG
  zh: '![图8.28](../../OEBPS/Images/08-28.png)'
- en: Figure 8.28 Platform capabilities defined to manage environments.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.28 定义的平台能力以管理环境。
- en: I do see tools like Knative, Argo Rollouts, Dapr, Istio, and OpenFeature leading
    the way in this space, and still, even if teams need to figure out all the details
    of each of these tools, patterns are emerging. These tools have been around for
    over three years, and you can notice the maturity of their features, roadmaps,
    and the people involved. With some of these projects graduating from the incubation
    process at the CNCF, I expect more integrations to help users with common workflows
    that most companies are implementing by hand today.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 我确实看到像Knative、Argo Rollouts、Dapr、Istio和OpenFeature这样的工具在这个领域引领潮流，尽管如此，即使团队需要弄清楚这些工具的每个细节，模式也在出现。这些工具已经存在了三年多，你可以注意到它们的功能、路线图以及参与人员的成熟度。随着一些项目从CNCF的孵化过程中毕业，我预计会有更多的集成来帮助用户处理今天大多数公司手动实施的标准工作流程。
- en: Finally, to recap our journey so far, figure 8.29 shows how release strategies
    fit into our platform walking skeleton and how teams closer to the business (product
    teams, stakeholders) can use these mechanisms to validate new versions before
    fully moving all customers to the latest version.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了回顾到目前为止的旅程，图 8.29 展示了发布策略如何融入我们的平台骨架，以及业务团队（产品团队、利益相关者）如何使用这些机制在将所有客户完全迁移到最新版本之前验证新版本。
- en: '![](../../OEBPS/Images/08-29.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.29 允许团队实验新版本的 环境](../../OEBPS/Images/08-29.png)'
- en: Figure 8.29 Environments that enable teams to experiment with new versions
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.29 允许团队实验新版本的 环境
- en: In the next chapter, to close the book, I’ve decided to talk about how we can
    measure the platforms we are building on top of Kubernetes. The platform capabilities
    described in these last two chapters and the combinations of tools described in
    this book are good because we are improving our team’s velocity for delivering
    software. Therefore, using metrics that focus on how efficient our teams are in
    delivering software directly correlates with the tools offered by the platform
    for these teams to use.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，为了结束本书，我决定谈谈我们如何衡量我们构建在 Kubernetes 之上的平台。这两章中描述的平台功能以及本书中描述的工具组合都是好的，因为我们正在提高我们团队交付软件的速度。因此，使用关注我们团队在交付软件方面效率的指标与平台为这些团队提供的工具直接相关。
- en: Summary
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Implementing common release strategies such as canary releases, blue/green deployments,
    and A/B testing can be challenging using Kubernetes built-in resources.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubernetes 内置资源实现常见的发布策略，如金丝雀发布、蓝/绿部署和 A/B 测试可能具有挑战性。
- en: Knative Serving introduces an advanced networking layer that gives us fine-grain
    control over how traffic is routed to different versions of our services that
    can be deployed simultaneously. This feature is implemented on top of Knative
    Services and reduces the manual work of creating several Kubernetes resources
    for implementing canary releases, blue/green deployments, and A/B testing release
    strategies. Knative Serving simplifies the operational burden of moving traffic
    to new versions and, with the help of the Knative autoscaler, can scale up and
    down based on demand.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Knative Serving 引入了一个高级网络层，使我们能够精细控制流量如何路由到可以同时部署的不同版本的服务。此功能是在 Knative 服务之上实现的，减少了创建多个
    Kubernetes 资源以实现金丝雀发布、蓝/绿部署和 A/B 测试发布策略的手动工作。Knative Serving 简化了将流量移动到新版本的运营负担，并且借助
    Knative 自动扩展器，可以根据需求进行扩展和缩减。
- en: Argo Rollouts integrates with ArgoCD (discussed in chapter 4) and provides an
    alternative to implement release strategies using the concept of Rollouts. Argo
    Rollouts also include features to automate testing new releases to ensure we move
    safely between versions (AnalysisTemplates and AnalysisRuns).
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo Rollouts 与 ArgoCD（在第 4 章中讨论）集成，并提供了使用 Rollouts 概念实现发布策略的替代方案。Argo Rollouts
    还包括自动化测试新版本的功能，以确保我们在版本之间安全迁移（AnalysisTemplates 和 AnalysisRuns）。
- en: Platform teams must enable stakeholders (business, product managers, operations)
    to experiment by providing flexible mechanisms and workflows that reduce the risk
    of releasing new versions of the applications that they are working with.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平台团队必须通过提供灵活的机制和工作流程，使利益相关者（业务、产品经理、运营）能够通过实验来降低他们正在工作的应用程序新版本发布的风险。
- en: Following the step-by-step tutorials, you gain hands-on experience using Knative
    Services and different patterns to route traffic to the Conference application.
    You also gained experience using Argo Rollouts to implement canary releases and
    blue/green deployments.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按照逐步的教程，您可以通过使用 Knative 服务和不同的模式将流量路由到会议应用程序来获得实际操作经验。您还获得了使用 Argo Rollouts
    来实现金丝雀发布和蓝/绿部署的经验。
