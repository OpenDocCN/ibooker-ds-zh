- en: 2 The end-to-end pipeline of an ML project
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 机器学习项目的端到端管道
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Getting familiar with the end-to-end pipeline for conducting an ML project
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 熟悉进行机器学习项目的端到端管道
- en: Preparing data for ML models (data collection and preprocessing)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为机器学习模型准备数据（数据收集和预处理）
- en: Generating and selecting features to enhance the performance of the ML algorithm
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成和选择特征以增强机器学习算法的性能
- en: Building up linear regression and decision tree models
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建线性回归和决策树模型
- en: Fine-tuning an ML model with grid search
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索微调机器学习模型
- en: Now that the first chapter has set the scene, it’s time to get familiar with
    the basic concepts of ML and AutoML. Because AutoML is grounded in ML, learning
    the fundamentals of ML will help you better understand and make use of AutoML
    techniques. This is especially the case when it comes to designing the search
    space in an AutoML algorithm, which characterizes the ML components to be used
    and the ranges of their hyperparameters. In this chapter, we will walk through
    a concrete example of solving an ML problem. This will help you gain a deeper
    understanding of the overall process of building up an ML pipeline, especially
    if you have little experience working on ML projects. You will also learn a naive
    way of tuning the hyperparameters of an ML model. This can be thought of as one
    of the simplest applications of AutoML, showing how it can help you find a better
    ML solution. More advanced AutoML tasks and solutions will be introduced in the
    second part of the book.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 现在第一章已经设定了场景，是时候熟悉机器学习（ML）和自动化机器学习（AutoML）的基本概念了。因为自动化机器学习建立在机器学习的基础上，学习机器学习的基本原理将帮助你更好地理解和利用自动化机器学习技术。这在设计自动化机器学习算法中的搜索空间时尤为重要，这决定了要使用的机器学习组件及其超参数的范围。在本章中，我们将通过一个具体的例子来解决问题。这将帮助你更深入地理解构建机器学习管道的整体过程，尤其是如果你在机器学习项目中经验不足的话。你还将学习一种调整机器学习模型超参数的简单方法。这可以被视为自动化机器学习最简单的应用之一，展示了它如何帮助你找到更好的机器学习解决方案。本书的第二部分将介绍更高级的自动化机器学习任务和解决方案。
- en: Note All of the code snippets included in this and later chapters are written
    in Python, in the form of Jupyter notebooks. They are all generated by Jupyter
    Notebook ([https://jupyter.org](https://jupyter.org)), an open source web application
    that offers features such as interactive code design, data processing and visualization,
    narrative text, and so on. It is widely popular in the machine learning and data
    science communities. If you’re not familiar with the environmental setup or do
    not have sufficient hardware resources, you can also run the code in Google Colaboratory
    ([http://colab.research.google.com/](http://colab.research.google.com/)), a free
    Jupyter notebook environment where anyone can run ML experiments. Detailed instructions
    for setting up the environment in Google Colaboratory, or Colab for short, are
    provided in appendix A. The notebooks are available at [https://github.com/datamllab/automl-in-action-notebooks](https://github.com/datamllab/automl-in-action-notebooks).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节及后续章节中包含的所有代码片段均以Python编写，以Jupyter笔记本的形式呈现。它们都是由Jupyter Notebook（[https://jupyter.org](https://jupyter.org)）生成的，这是一个开源的Web应用程序，提供交互式代码设计、数据处理和可视化、叙述文本等功能。它在机器学习和数据科学社区中非常受欢迎。如果你不熟悉环境设置或没有足够的硬件资源，你还可以在Google
    Colaboratory（[http://colab.research.google.com/](http://colab.research.google.com/)）中运行代码，这是一个免费的Jupyter笔记本环境，任何人都可以在其中运行机器学习实验。有关在Google
    Colaboratory（简称Colab）中设置环境的详细说明请参阅附录A。笔记本可在[https://github.com/datamllab/automl-in-action-notebooks](https://github.com/datamllab/automl-in-action-notebooks)找到。
- en: 2.1 An overview of the end-to-end pipeline
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 端到端管道概述
- en: 'An *ML pipeline* is a sequence of steps for conducting an ML project. Those
    steps follow:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*机器学习管道*是一系列用于执行机器学习项目的步骤。这些步骤依次为：'
- en: '*Problem framing and data collection*—Frame the problem as an ML problem and
    collect the data you need.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*问题界定和数据收集*——将问题界定为机器学习问题并收集所需的数据。'
- en: '*Data preprocessing and feature engineering*—Process the data into a suitable
    format that can be input into the ML algorithms. Select or generate features that
    are related to the target output to improve the performance of the algorithms.
    This step is usually done by first exploring the dataset to get a sense of its
    characteristics. The operations should accommodate the specific ML algorithms
    you are considering.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据预处理和特征工程*—将数据处理成适合输入到机器学习算法的格式。选择或生成与目标输出相关的特征，以提高算法的性能。这一步骤通常通过首先探索数据集来了解其特征来完成。操作应适应你考虑的具体机器学习算法。'
- en: '*ML algorithm selection*—Select ML algorithms appropriate for the task that
    you would like to test, based on your prior knowledge of the problem and your
    experience.'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器学习算法选择*—根据你对问题的先验知识和经验，选择适合你想要测试的任务的机器学习算法。'
- en: '*Model training and evaluation*—Apply the selected ML algorithm (or algorithms)
    to train an ML model with your training data, and evaluate its performance on
    the validation dataset.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*模型训练和评估*—将选定的机器学习算法（或算法）应用于训练数据，以训练机器学习模型，并在验证数据集上评估其性能。'
- en: '*Hyperparameter tuning*—Attempt to achieve better performance by iteratively
    tuning the model’s hyperparameters.'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*超参数调整*—通过迭代调整模型的超参数来尝试实现更好的性能。'
- en: '*Service deployment and model monitoring*—Deploy the final ML solution, and
    monitor its performance so you can maintain and improve the pipeline continuously.'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*服务部署和模型监控*—部署最终的机器学习解决方案，并监控其性能，以便你可以持续维护和改进管道。'
- en: As you can see, an ML project is a human-in-the-loop process. Starting from
    problem framing and data collection, the pipeline involves multiple data processing
    steps, which typically happen asynchronously (see figure 2.1). We will focus on
    the steps before service deployment and monitoring in the rest of the book. To
    learn more about deploying and serving models, please refer to a reference such
    as *Machine Learning Systems* by Jeff Smith (Manning, 2018) or *Machine Learning
    for Business* by Doug Hudgeon and Richard Nichol (Manning, 2019).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，机器学习项目是一个人机交互的过程。从问题定义和数据处理开始，管道涉及多个数据处理步骤，这些步骤通常异步发生（见图2.1）。本书的其余部分将重点关注服务部署和监控之前的步骤。要了解更多关于部署和提供模型的信息，请参考杰夫·史密斯（Manning，2018年）的《机器学习系统》或道格·哈奇顿和理查德·尼科尔（Manning，2019年）的《商业机器学习》等参考资料。
- en: '![02-01](../Images/02-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![02-01](../Images/02-01.png)'
- en: Figure 2.1 The end-to-end ML project pipeline
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 全端机器学习项目管道
- en: Let’s start working on a real problem to get you familiar with each component
    in the pipeline. The problem we explore here is predicting the average house price
    in a housing block, given features of the houses such as their locations and number
    of rooms. The data we use is the California housing dataset featured in R. Kelley
    Pace and Ronald Barry’s 1997 article “Sparse Spatial Autoregressions,” collected
    via the 1990 census. This is a representative problem used in many practical ML
    books as a starter, due to the small scale of the data and the simplicity of the
    data preparation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始解决一个实际问题，以便你熟悉管道中的每个组件。我们在这里探索的问题是预测一个住宅区的平均房价，给定房屋的特征，如位置和房间数量。我们使用的数据是R.凯利·佩斯和罗纳德·巴里1997年文章“稀疏空间自回归”中提到的加利福尼亚住房数据集，该数据集是通过1990年的人口普查收集的。这是一个在许多实际机器学习书籍中作为入门问题使用的代表性问题，因为数据规模小，数据准备简单。
- en: Note Selecting the right problem to work on can be difficult. It depends on
    multiple factors, such as your business need and research objectives. Before really
    engaging in a problem, ask yourself what solutions you expect to achieve and how
    they will benefit your downstream applications and whether any existing work has
    already fulfilled the need. This will help you decide whether the problem is worth
    investing in.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意选择合适的问题进行工作是困难的。这取决于多个因素，例如您的业务需求和研究目标。在真正投入问题之前，问问自己你期望实现哪些解决方案，以及它们将如何惠及你的下游应用，以及是否已有工作已经满足了这一需求。这将帮助你决定这个问题是否值得投资。
- en: 2.2 Framing the problem and assembling the dataset
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 定义问题和组装数据集
- en: The first thing you need to do in any ML project is frame the problem and collect
    the corresponding data. Framing the problem requires you to specify the inputs
    and outputs of the ML model. In the California housing problem, the inputs are
    the set of features describing the housing blocks.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何机器学习项目中，你需要做的第一件事是定义问题和收集相应的数据。定义问题需要你指定机器学习模型的输入和输出。在加利福尼亚住宅问题中，输入是描述住宅区块的特征集合。
- en: In this dataset, a housing block is a group of, on average, 1,425 individuals
    living in a geographically compact area. The features are the average number of
    rooms per house in the housing block, the latitude and longitude of the center
    of the block, and so on. The outputs should be the average housing prices of the
    blocks. We are trying to train an ML model given housing blocks with known median
    prices and predict the unknown prices of housing blocks based on their features.
    The returned predicted values are also called the *targets* (or *annotations*)
    of the model. Generally, any problem aiming to learn the relationship between
    data inputs and targets based on existing annotated examples is called a *supervised
    learning* *problem*. This is the most widely studied branch of ML and will be
    our main focus in the rest of this book.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，一个住宅区块是一群平均有1,425人居住在地理上紧凑区域的人群。特征包括住宅区块中每栋房子的平均房间数、区块中心的纬度和经度等。输出应该是区块的平均房价。我们试图训练一个机器学习模型，给定已知中位数的住宅区块，并基于其特征预测未知房价。返回的预测值也被称为模型的*目标*（或*注释*）。通常，任何旨在根据现有注释示例学习数据输入和目标之间关系的问题都被称为*监督学习*问题。这是机器学习研究最广泛的分支，也是本书余下的主要关注点。
- en: We can further classify supervised learning problems into different categories
    based on the type of target value. For example, any supervised learning problem
    with continuous targets can be categorized as a *regression* *problem*. Because
    price is a continuous variable, predicting California housing prices is, therefore,
    essentially a regression problem. If instead the target values in a supervised
    learning problem are discrete values with limited categories, we call the problem
    a *classification* problem. You can find some examples of classification problems
    in appendix B, and we will also explore them in the next chapter.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以根据目标值的类型进一步将监督学习问题分类为不同的类别。例如，任何具有连续目标值的监督学习问题都可以归类为*回归*问题。因为价格是一个连续变量，所以预测加利福尼亚房价本质上是一个回归问题。如果监督学习问题中的目标值是有限类别的离散值，我们称该问题为*分类*问题。你可以在附录B中找到一些分类问题的例子，我们也会在下一章中探讨它们。
- en: After framing the problem, the next step is to collect the data. Because the
    California housing dataset is one of the most-used ML datasets, you can easily
    access it with scikit-learn, a popular ML library. It should be noted, however,
    that in real life, discovering and acquiring datasets is a nontrivial activity
    and might require additional skills, such as knowledge of *Structured Query Language*
    (SQL), which is beyond the scope of this book. (To learn more about this, see
    Jeff Smith’s book *Machine Learning Systems*.) The following code will load the
    dataset for our problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在定义问题之后，下一步是收集数据。因为加利福尼亚住宅数据集是使用最广泛的机器学习数据集之一，你可以通过流行的机器学习库scikit-learn轻松访问它。然而，需要注意的是，在现实生活中，发现和获取数据集是一项非同寻常的活动，可能需要额外的技能，例如对*结构化查询语言*（SQL）的了解，这超出了本书的范围。（有关更多信息，请参阅Jeff
    Smith的书籍《机器学习系统》）。以下代码将加载我们问题的数据集。
- en: Listing 2.1 Loading the California housing dataset
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.1 加载加利福尼亚住宅数据集
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Imports the dataset-loading function from the scikit-learn library
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从scikit-learn库导入数据集加载函数
- en: ❷ Loads the California housing dataset
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载加利福尼亚住宅数据集
- en: 'The original data is a dictionary containing the data points formatted as an
    instance-feature matrix. Each data point is a housing block described by the features
    in a row of the matrix. Their targets are formatted as a vector. The dictionary
    also contains the feature names and descriptions indicating the features’ meanings
    and creation information for the dataset, as shown next:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 原始数据是一个包含数据点的字典，这些数据点以实例-特征矩阵的格式进行格式化。每个数据点都是一个由矩阵中的一行特征描述的住宅区块。它们的标签以向量的格式进行格式化。该字典还包含特征名称和描述，这些描述说明了特征的含义和数据集的创建信息，如下所示：
- en: '[PRE1]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: After loading the original dataset, we extract the data points and convert them
    into a *DataFrame*, which is a primary structure of the pandas library. pandas
    is a potent tool for data analysis and manipulation in Python. As shown in listing
    2.2, the targets are formatted as a *Series* object; it’s a vector with the label
    “MedPrice,” standing for the median price of the housing block in millions of
    dollars.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在加载原始数据集之后，我们提取数据点和将它们转换为 *DataFrame*，这是 pandas 库的基本结构。pandas 是 Python 中进行数据分析和操作的有力工具。如图
    2.2 所示，目标以 *Series* 对象的格式表示；它是一个带有标签“MedPrice”的向量，代表该住宅区的中位房价（以百万美元为单位）。
- en: Listing 2.2 Extracting the data samples and the targets
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.2 提取数据样本和目标
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Imports the pandas package
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 pandas 包
- en: ❷ Extracts the features with their names into a DataFrame
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将带有其名称的特征提取到 DataFrame 中
- en: ❸ Extracts the targets into a Series object with the name "MedPrice"
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将目标提取为名为 "MedPrice" 的 Series 对象
- en: 'Let’s print out the first five samples of the data (shown in figure 2.2). The
    first row indicates the feature names, details of which can be found at [https://scikit-learn.org/
    stable/datasets.html](https://scikit-learn.org/stable/datasets.html). For example,
    the “AveRooms” feature indicates the average number of rooms within a housing
    block. We can also check the values of the target data the same way, shown here:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出数据的前五个样本（如图 2.2 所示）。第一行表示特征名称，详细信息可以在 [https://scikit-learn.org/ stable/datasets.html](https://scikit-learn.org/stable/datasets.html)
    找到。例如，“AveRooms”特征表示一个住宅区内的平均房间数。我们也可以以同样的方式检查目标数据的值，如下所示：
- en: '[PRE3]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![02-02](../Images/02-02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![02-02](../Images/02-02.png)'
- en: Figure 2.2 Features of the first five samples in the California housing dataset
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.2 加利福尼亚住房数据集的前五个样本的特征
- en: Before moving on to the data-preprocessing step, let’s first do a data split
    to separate the training data and test set. As you learned in the previous chapter,
    the primary purpose of doing this is to avoid testing your model on the same data
    that you use to conduct your analysis and train your model. The code for splitting
    the data into training and test sets is shown in the following listing.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据预处理步骤之前，我们首先进行数据拆分，以分离训练数据和测试集。正如你在上一章所学，这样做的主要目的是避免在用于进行分析和训练模型的数据上测试你的模型。拆分数据到训练集和测试集的代码如下所示。
- en: Listing 2.3 Splitting the data into training and test sets
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.3 将数据拆分为训练集和测试集
- en: '[PRE4]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Imports the data split function from scikit-learn
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 scikit-learn 导入数据拆分函数
- en: ❷ Randomly splits 20% of the data into the test set
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机将 20% 的数据拆分为测试集
- en: 'We split a random 20% of the data into the test set. Now let’s do a quick check
    of the split. Looking at the full dataset, you’ll see that it contains 20,640
    data points. The number of features for each housing block is eight. The training
    set contains 16,512 samples, and the test set contains 4,128 samples, as depicted
    in the next code snippet:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将 20% 的数据随机拆分为测试集。现在让我们快速检查拆分情况。查看完整数据集，你会发现它包含 20,640 个数据点。每个住宅区的特征数量为八个。训练集包含
    16,512 个样本，测试集包含 4,128 个样本，如以下代码片段所示：
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You should not touch the target data in the test set until you have your final
    ML solution. Otherwise, all your analysis, including the data preparation and
    model training, may overfit to the test data, resulting in the final solution
    performing badly on unseen data when it is deployed. It is feasible to combine
    the features in the test set with the training features in data preprocessing
    and feature engineering, however, as we will do in the following sections. It
    can be helpful to aggregate the feature information, especially when the dataset
    size is small.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在你得到最终的机器学习解决方案之前，不要触碰测试集中的目标数据。否则，包括数据准备和模型训练在内的所有分析可能会过度拟合测试数据，导致最终解决方案在部署时在未见过的数据上表现不佳。在数据预处理和特征工程中，将测试集中的特征与训练特征相结合是可行的，正如我们将在以下章节中所做的那样。当数据集大小较小时，聚合特征信息可能很有帮助。
- en: 2.3 Data preprocessing
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 数据预处理
- en: 'Our next step is to do some preprocessing to transform the data into a suitable
    format for feeding into the ML algorithms. The procedure usually involves some
    *exploratory data analysis* (EDA) based on prior assumptions or questions about
    the data. EDA can help us get familiar with the dataset and gain more insights
    into it, to enable better data preparation. Some commonly asked questions include
    the following:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们下一步要做的是进行一些预处理，将数据转换成适合输入到机器学习算法的格式。这个过程通常涉及基于先前的假设或对数据的疑问进行的某些*探索性数据分析*（EDA）。EDA可以帮助我们熟悉数据集，并对其有更深入的了解，以便更好地准备数据。常见的问题包括以下内容：
- en: What are the data types of the values in each feature? Are they strings or other
    objects that can be used in later steps in the pipeline, or do they need to be
    transformed?
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征中的值的数据类型是什么？它们是字符串或其他可以在管道后续步骤中使用的对象，还是需要转换？
- en: How many distinct values does each feature have? Are they numerical values,
    categorical values, or something else?
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征有多少个不同的值？它们是数值、分类值还是其他类型？
- en: What are the scales and basic statistics of each feature? Can we gain some insights
    by visualizing the distribution of the values or correlations between them?
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个特征的尺度及基本统计信息是什么？通过可视化值的分布或它们之间的相关性，我们能获得一些洞察吗？
- en: Are there missing values in the data? If so, do we have to remove them or fill
    them in?
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据中是否存在缺失值？如果是，我们需要移除它们还是填充它们？
- en: In practice, different data usually requires tailored data preprocessing techniques
    depending on its format and features, the problems we are concerned with, the
    selected ML models, and so on. This is generally a heuristic, empirical process,
    resulting in various ad hoc operations being proposed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，不同的数据通常需要根据其格式和特征、我们关注的问题、选定的机器学习模型等因素定制数据预处理技术。这通常是一个启发式、经验的过程，导致提出了各种临时操作。
- en: 'We’ll use the four previously mentioned questions as the basis for some preliminary
    data preprocessing in this example. You can find more examples in appendix B.
    The first question we’re concerned about is the data types of the feature values.
    In this example, all the features and their targets are floating-point values,
    which can be directly fed into the ML algorithms without further manipulation,
    as shown next:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将使用前面提到的四个问题作为初步数据预处理的依据。更多示例可以在附录B中找到。我们首先关注的问题是特征值的类型。在这个例子中，所有特征及其目标都是浮点值，可以直接输入到机器学习算法中，无需进一步操作，如下所示：
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The second thing we’re concerned about is the number of distinct values in the
    features. Counting the distinct values can be helpful for distinguishing the feature
    types so that we can design tailored strategies to process them. This may also
    help us remove any redundant features. For example, if all the data samples have
    the same value for a feature, that feature cannot provide any useful information
    for prediction. It’s also possible that every data point has a unique value for
    a feature, but we’re confident that these values will not be helpful for classification.
    This is often the case for the ID feature of the data points, if it indicates
    only the order of the data samples. In listing 2.4, we can see that in this dataset,
    there is no feature whose value is identical for all the points, and there are
    no features where each data point has a unique value. Although some of them have
    large numbers of distinct values, such as “MedInc,” “AveRooms,” and “AveBedrms,”
    because these are numerical features whose values are useful for comparing the
    housing blocks and predicting the price, we should not remove them.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们关注的第二点是特征中不同值的数量。计算不同值可以帮助我们区分特征类型，以便我们可以设计定制的策略来处理它们。这也有助于我们移除任何冗余特征。例如，如果所有数据样本的特征值都相同，那么这个特征就不能为预测提供任何有用的信息。也有可能每个数据点的特征值都是唯一的，但我们确信这些值对分类没有帮助。这通常是数据点的ID特征的情况，如果它只表示数据样本的顺序。在列表2.4中，我们可以看到在这个数据集中，没有特征值对所有点都是相同的，也没有特征值是每个数据点都唯一的。尽管其中一些特征具有大量不同的值，例如“MedInc”、“AveRooms”和“AveBedrms”，因为这些是数值特征，其值对于比较住宅区和预测价格是有用的，所以我们不应该移除它们。
- en: Listing 2.4 Checking the number of unique values in each feature
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 检查每个特征中唯一值的数量
- en: '[PRE7]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: We can further display some basic statistics of the features to gain more insights
    (as shown in figure 2.3). For example, the average population in a housing block
    is 1,425, but the most densely populated block in this dataset has over 35,000
    inhabitants and the most sparsely populated block has just 3.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步显示一些特征的基本统计信息以获得更多见解（如图2.3所示）。例如，一个住宅区的平均人口为1,425，但在这个数据集中人口最密集的住宅区有超过35,000居民，而人口最稀疏的住宅区仅有3人。
- en: '![02-03](../Images/02-03.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![02-03](../Images/02-03.png)'
- en: Figure 2.3 Feature statistics of the California housing data
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3 加利福尼亚住宅数据特征统计
- en: One of the key challenges in real-world applications is missing values in the
    data. This problem can be introduced during the collection or transmission of
    the data, or it can be caused by corruption, failure to correctly load the data,
    and so on. If not handled properly, missing values can affect the performance
    of the ML solution, or even cause the program to crash. The process of replacing
    missing and invalid values in the data with substitute values is called *imputation*.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实世界的应用中，数据中的缺失值是一个关键挑战。这个问题可能在数据收集或传输过程中引入，也可能由损坏、未能正确加载数据等原因引起。如果处理不当，缺失值可能会影响机器学习解决方案的性能，甚至导致程序崩溃。用替代值替换数据中的缺失和无效值的过程称为*插补*。
- en: The following listing checks for the existence of missing values in our training
    and test datasets.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表检查我们的训练和测试数据集中是否存在缺失值。
- en: Listing 2.5 Checking for missing values in the training and test sets
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 检查训练和测试集中的缺失值
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Copies the training data to avoid changing it in place
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将训练数据复制以避免就地更改
- en: ❷ Combines the features and the target by adding a column 'MedPrice' for the
    target
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过添加一个名为'MedPrice'的目标列将特征和目标合并
- en: ❸ Checks whether the training set has missing values
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查训练集是否存在缺失值
- en: ❹ Checks whether the test set has missing values
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查测试集是否存在缺失值
- en: 'The following results show that the dataset has no missing values, so we can
    proceed with our analysis without considering this problem further (you will see
    an example of dealing with missing values in chapter 3):'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的结果显示数据集中没有缺失值，因此我们可以继续我们的分析，无需进一步考虑这个问题（你将在第3章中看到一个处理缺失值的例子）：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: For simplicity, we won’t do any additional data preprocessing here. You will
    often want to take some other common steps, such as checking for *outliers*, which
    are distant points that can affect the training of ML models, and removing them
    if they exist in your data. Also, real-world datasets are often not formatted
    quite as well as the one used in this example. More examples of data-preprocessing
    techniques dealing with different data types are provided in appendix B; I recommend
    you go over those examples before going on to the next chapter if you’re not familiar
    with this topic. Next we’ll move on to the feature-engineering step, which is
    often carried out concurrently with data preprocessing.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们在这里不会进行任何额外的数据预处理。你通常会想采取一些其他常见步骤，例如检查*异常值*，这些是可能影响机器学习模型训练的远离中心的点，如果它们存在于你的数据中，则应将其移除。此外，现实世界的数据集通常不如本例中使用的格式好。附录B提供了处理不同数据类型的预处理技术示例；如果你不熟悉这个主题，我建议你在继续下一章之前先浏览那些示例。接下来，我们将继续进行特征工程步骤，这通常与数据预处理同时进行。
- en: 2.4 Feature engineering
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 特征工程
- en: 'Different from data preprocessing, which focuses on getting the raw data into
    a useful or efficient format, feature engineering aims to generate and select
    a set of good features to boost the performance of ML algorithms. It often relies
    on specific domain knowledge and proceeds iteratively with the following two steps:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据预处理不同，数据预处理侧重于将原始数据转换为有用的或高效的格式，特征工程旨在生成和选择一组良好的特征以提升机器学习算法的性能。它通常依赖于特定的领域知识，并迭代进行以下两个步骤：
- en: '*Feature generation* aims at generating new features by transforming existing
    ones. It can be done on a single feature, such as by substituting a categorical
    feature with the frequency count in each category to obtain a measurable numerical
    feature, or on multiple features. For example, by counting the number of male
    and female employees with different occupations, we may arrive at a more instructive
    feature for analyzing the fairness of recruiting across different industries.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征生成*旨在通过转换现有特征来生成新特征。这可以在单个特征上完成，例如，通过将分类特征替换为每个类别的频率计数来获得可测量的数值特征，或者可以在多个特征上完成。例如，通过计算不同职业的男性和女性员工的数量，我们可能得到一个更有助于分析不同行业招聘公平性的特征。'
- en: '*Feature selection* aims at selecting the most useful subset of the existing
    features, to improve the efficiency and accuracy of the ML algorithms.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*特征选择*旨在选择现有特征中最有用的子集，以提高机器学习算法的效率和准确性。'
- en: Feature selection and generation are often done in an iterative manner, leveraging
    immediate feedback from certain measures, such as the correlation of the generated
    features and the targets, or delayed feedback based on the performance of the
    trained ML model on the evaluation dataset.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择和生成通常以迭代方式进行，利用某些度量（如生成的特征与目标之间的相关性）的即时反馈，或者基于在评估数据集上训练的机器学习模型的性能的延迟反馈。
- en: In listing 2.6, we perform a simple feature selection by measuring the correlation
    between each feature and the target using *Pearson’s correlation coefficient*.
    Pearson’s correlation coefficient measures the linear correlation between two
    variables (feature and target). The value can range from -1 to 1, where -1 and
    1 indicate a perfect negative and a perfect positive linear relationship, respectively.
    A coefficient of 0 indicates no relationship exists.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.6中，我们通过使用*皮尔逊相关系数*来测量每个特征与目标之间的相关性，进行简单的特征选择。皮尔逊相关系数衡量两个变量（特征和目标）之间的线性相关性。其值范围从-1到1，其中-1和1分别表示完美的负线性关系和完美的正线性关系。系数为0表示不存在关系。
- en: Listing 2.6 Plotting the Pearson’s correlation coefficient matrix
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 绘制皮尔逊相关系数矩阵
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Imports the library for general plotting configuration
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入用于通用绘图配置的库
- en: ❷ Imports the seaborn library to plot the heatmap
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 导入seaborn库以绘制热图
- en: ❸ Pretties the figure display in Jupyter notebooks
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在Jupyter笔记本中美化图形显示
- en: ❹ Sets the figure size
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置图形大小
- en: ❺ Calculates the Pearson’s correlation coefficient matrix
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算皮尔逊相关系数矩阵
- en: ❻ Plots the correlations between all the features and the target
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 绘制所有特征与目标之间的相关性图
- en: We’ll focus on the last row of the matrix (see figure 2.4), which shows the
    pairwise correlation between the target housing price and each feature. We’ll
    then discuss the two features we’ve chosen.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将关注矩阵的最后一行（见图2.4），它显示了目标房价与每个特征之间的成对相关性。然后我们将讨论我们选择的两个特征。
- en: '![02-04](../Images/02-04.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![02-04](../Images/02-04.png)'
- en: Figure 2.4 Pearson’s correlation coefficient matrix among all the features and
    the target
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.4 所有特征与目标之间的皮尔逊相关系数矩阵
- en: 'Based on the coefficient matrix and the following assumptions, we select the
    top two correlated features:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 基于系数矩阵和以下假设，我们选择前两个相关性最高的特征：
- en: '*MedInc*—This feature, which indicates the median income for households within
    a block of houses, shows a high positive linear correlation with the target values.
    This is aligned with the intuition that people with higher incomes are more likely
    to live in the blocks with higher housing prices (positive correlation).'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*MedInc*—这个特征表示一个街区内房屋的中等收入，它与目标值显示出高度的正线性相关性。这与直觉相符，即收入较高的人更有可能住在房价较高的街区（正相关）。'
- en: '*AveRooms*—This feature indicates the average number of rooms in each house
    in a block. Houses with more rooms are more likely to have higher prices (positive
    correlation).'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*AveRooms*—这个特征表示每个街区房屋的平均房间数。房间数较多的房屋更有可能价格更高（正相关）。'
- en: We select only two features here as examples for simplicity. The feature selection
    is implemented in listing 2.7\. It is also possible to automate the selection
    of features, instead of basing it on a visual inspection, by choosing a threshold
    for the calculated Pearson’s correlation coefficients (such as 0.5). The number
    of features to select is a hyperparameter that we need to carefully decide. We
    can try different feature combinations to train our ML model and select the best
    one by trial and error.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化，这里我们只选择两个特征作为示例。特征选择在列表2.7中实现。也可以通过选择计算出的皮尔逊相关系数的阈值（例如0.5）来自动化特征选择，而不是基于视觉检查。要选择多少个特征是一个需要我们仔细决定的超参数。我们可以尝试不同的特征组合来训练我们的机器学习模型，并通过试错法选择最佳的一个。
- en: Listing 2.7 Feature selection
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 特征选择
- en: '[PRE11]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The selected feature set
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选定的特征集
- en: ❷ Extracts the new training features
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 提取新的训练特征
- en: ❸ Drops the target and keeps only the training features in X_train
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在X_train中删除目标并仅保留训练特征
- en: ❹ Selects the same feature set for the test data
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为测试数据选择相同的特征集
- en: 'After selecting the two features, we can draw scatterplots to display the pairwise
    correlations between them and the target. Their distributions can be jointly shown
    via histogram plots using the following code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择了两个特征之后，我们可以绘制散点图来显示它们之间的成对相关性以及与目标的相关性。它们的分布可以通过以下代码使用直方图来共同展示：
- en: '[PRE12]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The scatterplots show a strong positive correlation between the “MedInc” feature
    and the target, “MedPrice.” The correlation between the “AveRooms” feature and
    “MedPrice” is comparably less conspicuous, due to the scale differences between
    the features and the outliers (see figure 2.5).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 散点图显示“MedInc”特征与目标“MedPrice”之间存在强烈的正相关。由于特征和异常值之间的比例差异，“AveRooms”特征与“MedPrice”之间的相关性相对不那么明显（见图2.5）。
- en: '![02-05](../Images/02-05.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![02-05](../Images/02-05.png)'
- en: Figure 2.5 Pairwise relationships between the selected features and the target
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.5 选定特征与目标之间的成对关系
- en: Using Pearson’s correlation coefficient to select features is easy, but it may
    not always be effective in practice. It ignores nonlinear relationships between
    the features and the target, as well as the correlations among the features. Moreover,
    the correlation between feature and target may not be meaningful for categorical
    features, whose values are not ordinal. As more and more feature engineering techniques
    have been proposed, deciding how to select the best one has become a pain point.
    That brings up an important topic in AutoML—automated feature selection and transformation—but
    we’ll leave discussion of that to the second part of the book, and for now continue
    solving the problem at hand.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用皮尔逊相关系数来选择特征很容易，但在实践中可能并不总是有效。它忽略了特征与目标之间的非线性关系，以及特征之间的相关性。此外，对于值不是序数的分类特征，特征与目标之间的相关性可能没有意义。随着越来越多的特征工程技术的提出，决定如何选择最佳方法已经成为一个痛点。这引出了AutoML中的一个重要话题——自动特征选择和转换，但我们将在本书的第二部分讨论这个问题，现在我们继续解决当前的问题。
- en: Now that we have prepared the training data and selected our features, we’re
    ready to choose the algorithms to use to train an ML model with the preprocessed
    data. (In practice, you can also select the ML algorithms before the data-preprocessing
    and feature-engineering steps to pursue a more tailored data-preparation process.)
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好了训练数据并选择了特征，我们就可以选择用于使用预处理数据训练机器学习模型的算法了。（在实践中，你还可以在数据预处理和特征工程步骤之前选择机器学习算法，以追求更定制化的数据准备过程。）
- en: 2.5 ML algorithm selection
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 机器学习算法选择
- en: 'Remember that for each ML algorithm, we have four core components to select:
    an ML model to be trained, a metric to measure the model’s efficacy, an optimization
    method to update the model’s parameters based on that metric, and a stop criterion
    to terminate the updating process. Because our main focus here is not optimization,
    we will talk only briefly about the optimization method and stop criterion for
    each selected model.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，对于每个机器学习算法，我们都有四个核心组件需要选择：要训练的机器学习模型、衡量模型效力的指标、基于该指标更新模型参数的优化方法，以及终止更新过程的停止标准。因为我们的主要焦点不是优化，所以我们只会简要地讨论每个选定模型的优化方法和停止标准。
- en: For this example we will use two simple, classic models. The first one is the
    *linear regression model*, and the second is the *decision tree model*. We’ll
    start by recapping the core idea behind the linear regression model and the process
    of creating, training, and evaluating it. We will train the model using the whole
    training set and evaluate it on the test set, without further splitting the training
    data into training and validation sets for hyperparameter tuning. We will discuss
    the hyperparameter tuning step after the introduction of the decision tree model.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个例子，我们将使用两个简单、经典的模型。第一个是*线性回归模型*，第二个是*决策树模型*。我们将首先回顾线性回归模型背后的核心思想以及创建、训练和评估它的过程。我们将使用整个训练集来训练模型，并在测试集上评估它，而不会进一步将训练数据分成训练集和验证集进行超参数调整。我们将在介绍决策树模型后讨论超参数调整步骤。
- en: 2.5.1 Building the linear regression model
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.1 构建线性回归模型
- en: 'Linear regression is one of the simplest models in supervised ML, and probably
    the first ML model you learned about. It tries to predict the target value of
    a data point by computing the weighted sum of its features: ![02-05-EQ01](../Images/02-05-EQ01.png) ,
    where the *m* is the number of features. In the current example *m* is 2 because
    we selected only two features: “MedInc” and “AveRooms.” *w[i]* are the parameters
    (or *weights*) to be learned from the data, where *w*[0] is called the *intercept*
    and ![02-05-EQ02](../Images/02-05-EQ02.png) is called a *coefficient* for feature
    *x[i]*. The parameters are learned based on the training data to capture the linear
    relationship between the features and the target. The code to build a linear regression
    model with scikit-learn follows:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归是监督机器学习中最简单的模型之一，可能是你首先了解的机器学习模型。它试图通过计算数据点的特征加权和来预测目标值：![02-05-EQ01](../Images/02-05-EQ01.png)，其中*m*是特征的数量。在当前示例中*m*为2，因为我们只选择了两个特征：“MedInc”和“AveRooms”。*w[i]*是从数据中学习到的参数（或*权重*），其中*w*[0]被称为*截距*，![02-05-EQ02](../Images/02-05-EQ02.png)被称为特征*x[i]*的*系数*。参数基于训练数据学习，以捕捉特征和目标之间的线性关系。使用scikit-learn构建线性回归模型的代码如下：
- en: '[PRE13]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: To learn the weights, we need to select an optimization method and a metric
    to measure their performance. *Mean squared error* (MSE) is a widely used loss
    function and evaluation metric for regression problems—it measures the average
    squared difference between the model’s predictions and the targets. We’ll use
    MSE as the loss function in the training stage to learn the model, and we’ll use
    the evaluation metric in the testing phase to measure the model’s predictive power
    on the test set. To help you understand how it’s calculated, a code illustration
    is provided in listing 2.8\. In the training stage, true_target_values is a list
    of all the target values (actual prices of the houses) in the training dataset,
    and predictions is all the housing prices predicted by the model.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 为了学习权重，我们需要选择一个优化方法和一个指标来衡量它们的性能。*均方误差*（MSE）是回归问题中广泛使用的损失函数和评估指标——它衡量模型预测与目标之间的平均平方差异。在训练阶段，我们将使用MSE作为损失函数来学习模型，在测试阶段，我们将使用评估指标来衡量模型在测试集上的预测能力。为了帮助您理解其计算方式，提供了一个代码示例在列表2.8中。在训练阶段，true_target_values是训练数据集中所有目标值（房屋的实际价格）的列表，predictions是模型预测的所有房价。
- en: Listing 2.8 Computing the MSE
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.8 计算MSE
- en: '[PRE14]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Initializes the sum to zero
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将总和初始化为零
- en: ❷ Sums up the squared errors
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 求平方误差之和
- en: ❸ Averages the sum of squared errors
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算平方误差之和
- en: Figure 2.6 is a simple illustration of a linear regression model with a single
    variable (or feature). The learning process aims to find the best slope and intercept
    to minimize the mean of the squared errors, indicated by the dashed lines between
    the data points and the regression line.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6是线性回归模型单变量（或特征）的简单示意图。学习过程旨在找到最佳斜率和截距，以最小化平方误差的平均值，这由数据点和回归线之间的虚线表示。
- en: '![02-06](../Images/02-06.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![02-06](../Images/02-06.png)'
- en: Figure 2.6 Illustration of linear regression with only one feature
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.6 线性回归单特征示意图
- en: 'With the help of scikit-learn, we can easily optimize the weights by calling
    the fit function and feeding in the training data. MSE is used as the loss function
    by default, as shown here:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn的帮助下，我们可以通过调用fit函数并输入训练数据来轻松优化权重。默认情况下，MSE（均方误差）用作损失函数，如下所示：
- en: '[PRE15]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can print the learned weights with the following code.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码打印出学习到的权重。
- en: Listing 2.9 Displaying the learned parameters
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.9 显示学习到的参数
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Converts the coefficient values to a DataFrame
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将系数值转换为DataFrame
- en: ❷ Prints the intercept value
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 打印截距值
- en: ❸ Prints the coefficients
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 打印系数
- en: 'The learned coefficients show that the “MedInc” feature and the target do have
    a positive linear correlation. The “AveRooms” feature has a negative correlation,
    contrary to our expectations. This could be due to the following two possible
    factors:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的系数表明，“MedInc”特征和目标确实存在正线性相关性。而“AveRooms”特征具有负相关性，这与我们的预期相反。这可能是以下两个可能因素之一：
- en: The outliers in the training data (some high-priced housing blocks have fewer
    rooms) are affecting the training process.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据中的异常值（一些高价住宅区房间较少）正在影响训练过程。
- en: The two features we have selected are positively linearly correlated. This is
    because they share some common information that is useful for predicting the target.
    Because “MedInc” already covers some of the information provided by “AveRooms,”
    the effect of “AveRooms” is reduced, resulting in a slight negative correlation.
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们选择的两个特征是正线性相关的。这是因为它们共享一些对预测目标有用的共同信息。由于“MedInc”已经覆盖了“AveRooms”提供的一些信息，因此“AveRooms”的影响减小，导致轻微的负相关性。
- en: Ideally, a good set of features for linear regression should be only weakly
    correlated with one another but highly correlated with the target. At this stage,
    we could iteratively proceed with feature selection and model training to try
    different sets of features and select a good combination. I’ll leave that process
    for you to try as an exercise, and we’ll go directly to the test phase. The MSE
    of the learned model on the test set can be calculated and printed with the following
    code.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，线性回归的良好特征集应彼此之间只有弱相关性，但与目标变量高度相关。在这个阶段，我们可以通过迭代进行特征选择和模型训练，尝试不同的特征组合，并选择一个良好的组合。我将把这个过程留给你作为练习尝试，我们将直接进入测试阶段。测试集上学习模型的均方误差（MSE）可以通过以下代码计算和打印。
- en: Listing 2.10 Testing the linear regression model
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.10 测试线性回归模型
- en: '[PRE17]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Imports the evaluation metric
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入评估指标
- en: ❷ Predicts the targets of the test data
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预测测试数据的目标值
- en: The test MSE is 0.70, which means on average, the squared difference between
    the model’s predictions and the ground-truth targets of the test data is 0.70\.
    A lower value for the MSE is better; ideally, you want this to be as close to
    0 as possible. Next we’ll try out a decision tree model and compare the performance
    of the two.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 测试MSE为0.70，这意味着平均而言，模型预测与测试数据真实目标之间的平方差为0.70。MSE的值越低越好；理想情况下，你希望这个值尽可能接近0。接下来，我们将尝试决策树模型，并比较两种模型的性能。
- en: 2.5.2 Building the decision tree model
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.5.2 构建决策树模型
- en: The key idea of a decision tree is to split the data into different groups based
    on a series of (usually binary) conditions, as shown in figure 2.7\. Each of the
    non-leaf nodes in a decision tree is a condition that causes each data sample
    to be placed in one of the child nodes. Each leaf node has a specific value as
    the prediction. Each data sample will navigate from the root (top) of the tree
    to one of the leaf nodes, giving us the prediction for that sample. For example,
    suppose we have a house with MedInc=5 and AveRooms=3\. We’ll start from the root
    node and go through the No path and the Yes path until we reach a leaf node with
    the value $260,000, which is the predicted price for this house.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的关键思想是根据一系列（通常是二元的）条件将数据分割成不同的组，如图2.7所示。决策树中的每个非叶节点都是一个条件，它将每个数据样本放置在子节点之一中。每个叶节点有一个特定的值作为预测。每个数据样本将从树的根（顶部）导航到叶节点之一，为我们提供该样本的预测。例如，假设我们有一所房子，其MedInc=5，AveRooms=3。我们将从根节点开始，通过No路径和Yes路径，直到我们达到一个值为$260,000的叶节点，这是这所房子的预测价格。
- en: '![02-07](../Images/02-07.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![02-07](../Images/02-07.png)'
- en: Figure 2.7 Predicting values with a decision tree model
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.7 使用决策树模型预测值
- en: Both the splits of the tree and the predictions in each leaf are learned based
    on the training data. A typical process for constructing a decision tree is shown
    in listing 2.11\. The tree is constructed recursively. In each recursion, we find
    the optimal split for the dataset in the current node. In each split node, the
    prediction given by the node equals the mean value of all the training samples
    that fall into this node. The optimal split is defined as the one that minimizes
    the MSE between the predicted value and the target values of all the samples in
    the two child nodes. The recursion will stop when an exit condition is met. We
    have multiple ways to define this condition. For example, we can predefine the
    maximum depth of the tree, so that when that depth is reached, the recursion stops.
    We can also base it on the stop criterion of the algorithm. For example, we can
    define the exit condition of the recursion as “stop if the number of training
    samples that fall into the current node is less than five.”
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 树的分割和每个叶子节点中的预测都是基于训练数据学习的。构建决策树的典型过程如列表 2.11 所示。树是递归构建的。在每次递归中，我们找到当前节点数据集的最佳分割。在每个分割节点中，节点给出的预测值等于落入此节点的所有训练样本的平均值。最佳分割定义为最小化两个子节点中所有样本预测值与目标值之间均方误差的分割。当满足退出条件时，递归将停止。我们可以有多种方式来定义这个条件。例如，我们可以预先定义树的最大深度，当达到这个深度时，递归停止。我们也可以基于算法的停止标准。例如，我们可以将递归的退出条件定义为“如果落入当前节点的训练样本数量少于五个，则停止。”
- en: Listing 2.11 Constructing a decision tree model
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.11 构建决策树模型
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ If the exit condition is met, computes the predicted value
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果满足退出条件，则计算预测值
- en: ❷ Gets the optimal split condition from the data
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从数据中获取最佳分割条件
- en: ❸ Makes a new node with the condition
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个新的节点，包含条件
- en: ❹ Splits the data into two parts with the condition
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据条件将数据分成两部分
- en: ❺ Constructs the left subtree recursively
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 递归构建左子树
- en: ❻ Constructs the right subtree recursively
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 递归构建右子树
- en: It’s easy to build a decision tree with scikit-learn. Code for training and
    testing is shown in listing 2.12\. The max_depth argument is a hyperparameter
    constraining the maximum depth of the tree model during training. It will stop
    growing either when a node of this max_depth is achieved or when the current node
    contains less than two samples (a default stop criterion).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 scikit-learn 构建决策树很容易。训练和测试的代码如列表 2.12 所示。max_depth 参数是一个超参数，在训练过程中约束树模型的深度。它将在达到此最大深度或当前节点包含少于两个样本（默认停止标准）时停止增长。
- en: Listing 2.12 Building a decision tree model with scikit-learn
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12 使用 scikit-learn 构建决策树模型
- en: '[PRE19]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Creates a decision tree regressor
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建决策树回归器
- en: 'Let’s print the MSE results for both the training and test sets, as shown next.
    The difference between them indicates a small amount of overfitting:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们打印出训练集和测试集的 MSE 结果，如下所示。它们之间的差异表明存在少量过拟合：
- en: '[PRE20]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Compared to the linear regression model, the current decision tree model performs
    slightly worse on the test set. But it’s worth pointing out that we should not
    select our model based only on these test results. The right way of doing model
    selection and hyperparameter tuning is to try out different models on a separate
    validation set through the validation process. The reason we directly evaluate
    the test set here is to get you familiar with the training and testing procedure.
    We can also visualize the learned tree model to gain a more intuitive understanding,
    using the following code.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 与线性回归模型相比，当前的决策树模型在测试集上的表现略差。但值得注意的是，我们不应仅基于这些测试结果选择模型。正确的模型选择和超参数调整方法是通过验证过程在单独的验证集上尝试不同的模型。我们在这里直接评估测试集的原因是为了让您熟悉训练和测试过程。我们还可以使用以下代码可视化学习到的树模型，以获得更直观的理解。
- en: Listing 2.13 Visualizing the decision tree
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.13 可视化决策树
- en: '[PRE21]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ The target’s name
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 目标名称
- en: ❷ Whether to fill in the boxes with colors
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 是否用颜色填充方框
- en: ❸ Whether to round the corners of the boxes
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 是否圆化方框的角
- en: 'The learned tree is a balanced binary tree with a depth of three, as shown
    in figure 2.8\. Except for the leaf nodes, which do not have a split condition,
    each node conveys four messages: the split condition, which decides which child
    node a sample should fall into based on a feature; the number of training samples
    that fall into the current node; the mean value of their targets; and the MSE
    of all the samples that fall into the current node. The MSE of each node is calculated
    based on the ground-truth targets and the prediction given by the tree, which
    is the mean value of the targets.'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 学习到的树是一个深度为三的平衡二叉树，如图2.8所示。除了叶节点外，它们没有分裂条件，每个节点传达四个信息：分裂条件，它决定了样本应该根据哪个特征落入哪个子节点；落入当前节点的训练样本数量；它们的目标值的平均值；以及落入当前节点的所有样本的均方误差（MSE）。每个节点的MSE是基于真实目标和树给出的预测计算的，即目标的平均值。
- en: '![02-08](../Images/02-08.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![02-08](../Images/02-08.png)'
- en: Figure 2.8 Visualization of the learned decision tree
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.8 学习到的决策树可视化
- en: 'We have now created two ML models and know that the decision tree model performs
    a bit worse than the regression model on our test dataset. The question now is
    whether, without touching the test set, we will be able to improve the decision
    tree model to make it perform better than our linear regression model in final
    testing. This introduces an important step in the ML pipeline: hyperparameter
    tuning and model selection.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经创建了两个机器学习模型，并且知道在测试数据集上，决策树模型的表现略逊于回归模型。现在的问题是，在不接触测试集的情况下，我们是否能够改进决策树模型，使其在最终测试中比我们的线性回归模型表现更好。这引入了机器学习流程中的一个重要步骤：超参数调整和模型选择。
- en: '2.6 Fine-tuning the ML model: Introduction to grid search'
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.6 精调机器学习模型：网格搜索简介
- en: Knowing the optimal hyperparameters for an ML algorithm, such as the max_depth
    of the decision tree model, before you get started is usually impossible. So,
    hyperparameter tuning is a very important step—it allows you to select the best
    components to form your ML algorithm and improve the performance of the algorithm
    you create. Tuning is generally a trial-and-error process. You predefine a set
    of candidate combinations of hyperparameters and select the best combination by
    applying them to the training data with the validation process and evaluating
    the results. To help you better understand the tuning process, let’s try to improve
    the decision tree model by tuning its max_depth hyperparameter.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前就知道机器学习算法的最佳超参数，例如决策树模型的max_depth，通常是不可能的。因此，超参数调整是一个非常关键的步骤——它允许你选择最佳组件来构建你的机器学习算法，并提高你创建的算法的性能。调整通常是一个试错的过程。你预先定义一组候选的超参数组合，并通过将它们应用于训练数据，使用验证过程并评估结果来选择最佳组合。为了帮助你更好地理解调整过程，让我们尝试通过调整其max_depth超参数来改进决策树模型。
- en: 'The first thing we should do is construct a validation set so we can compare
    our different models based on their validation performance. Always remember that
    you should not touch the test set during the hyperparameter-tuning process. The
    model should be exposed to this data only once tuning is complete. There are many
    ways to partition the training data and conduct model validation. We’ll use *cross-validation*
    here; it’s a technique that is widely used for model validation, especially when
    the dataset size is small. Cross-validation averages the results of multiple rounds
    of model training and evaluation. In each round, the dataset is randomly partitioned
    into two complementary subsets (the training and validation sets). Every data
    point has an equal chance of being in either the training set or the validation
    set across different rounds. The two main groups of cross-validation methods follow:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该做的是构建一个验证集，这样我们就可以根据不同模型的验证性能进行比较。始终记住，在超参数调整过程中，你不应该接触测试集。模型只应在调整完成后一次接触这些数据。有许多方法可以将训练数据分割并进行模型验证。在这里我们将使用*交叉验证*；这是一种广泛用于模型验证的技术，尤其是在数据集大小较小时。交叉验证平均了多轮模型训练和评估的结果。在每一轮中，数据集被随机分割成两个互补的子集（训练集和验证集）。在不同的轮次中，每个数据点都有平等的机会被分配到训练集或验证集中。交叉验证的主要方法分为以下两组：
- en: With *exhaustive* cross-validation methods, you train and evaluate a model on
    all the possible ways to make up the two sets into which you partition the data.
    For example, suppose you decide to use 80% of the dataset for training and 20%
    for validation. In that case, you’ll need to exhaust every possible combination
    of data points in those two sets and average the training and testing results
    of the model on all these partitions. A representative example of exhaustive cross-validation
    is *leave-one-out cross-validation*, in which each example is an individual test
    set whereas all the rest form the corresponding training set. Given *N* samples,
    you’ll have *N* partitions for *N*-time training and evaluation of your candidate
    models.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*穷尽性*交叉验证方法，你将在所有可能的方式上训练和评估模型，以组成你将数据分成的两个集合。例如，假设你决定使用数据集的80%进行训练，20%进行验证。在这种情况下，你需要穷尽这两组数据点中所有可能的组合，并平均所有这些分区上模型的训练和测试结果。穷尽性交叉验证的一个代表性例子是*留一法交叉验证*，其中每个例子都是一个单独的测试集，而所有其余的则形成相应的训练集。给定*N*个样本，你将会有*N*个分区，用于*N*次训练和评估你的候选模型。
- en: With *nonexhaustive* cross-validation methods, as the name implies, you don’t
    have to exhaust all the possibilities for each partition. Two representative examples
    are the *holdout method* and *k-fold cross-validation*. The holdout method simply
    involves randomly partitioning the original training data into two sets. One is
    the new training set, and the other is the validation set. People often treat
    this method as simple validation rather than cross-validation because it typically
    involves a single run and the individual data points are not used for both training
    and validation. *k*-fold cross-validation divides the original training data into
    *k* equally partitioned subsets. Each subset in turn is used as the validation
    set, whereas the rest are the corresponding training set at that time (see figure
    2.9). Given *N* samples, *N*-fold cross-validation is equivalent to leave-one-out
    cross-validation.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用*非穷尽性*交叉验证方法，正如其名所示，你不必穷尽每个分区所有可能的情况。两个代表性的例子是*留出法*和*k折交叉验证*。留出法简单地将原始训练数据随机分为两组。一组是新训练集，另一组是验证集。人们通常将这种方法视为简单的验证而不是交叉验证，因为它通常只涉及一次运行，并且个体数据点既不用于训练也不用于验证。*k折交叉验证*将原始训练数据分为*k*个等分的子集。每个子集依次用作验证集，其余的则是当时相应的训练集（见图2.9）。给定*N*个样本，*N*折交叉验证等同于留一法交叉验证。
- en: '![02-09](../Images/02-09.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![02-09](../Images/02-09.png)'
- en: Figure 2.9 Three-fold cross-validation
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.9 三折交叉验证
- en: Let’s try using five-fold cross-validation to tune the max_depth hyperparameter.
    In listing 2.14, we use the KFold cross-validator in the scikit-learn library
    to generate the cross-validation sets and loop through all the candidate values
    of the max_depth hyperparameter to generate the tree model and conduct cross-validation.
    This search strategy is called *grid search*, and it’s one of the most naive AutoML
    approaches for searching for the best hyperparameters. The main idea is to traverse
    all the combinations of the values in the candidate hyperparameter sets and select
    the best combination based on the evaluation results. Because we have only one
    candidate hyperparameter to tune, it becomes a simple loop over all the possible
    values.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用五折交叉验证来调整max_depth超参数。在列表2.14中，我们使用scikit-learn库中的KFold交叉验证器生成交叉验证集，并遍历max_depth超参数的所有候选值来生成树模型并进行交叉验证。这种搜索策略被称为*网格搜索*，它是寻找最佳超参数的最简单的AutoML方法之一。主要思想是遍历候选超参数集中值的所有组合，并根据评估结果选择最佳组合。因为我们只有一个候选超参数需要调整，所以它变成了对所有可能值进行简单循环的过程。
- en: Listing 2.14 Generating cross-validation sets, tuning max_depth
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.14 生成交叉验证集，调整max_depth
- en: '[PRE22]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Creates a five-fold cross-validation object for data partitioning
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个五折交叉验证对象用于数据分区
- en: ❷ Constructs the candidate value list for the max_depth hyperparameter
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为max_depth超参数构建候选值列表
- en: ❸ Loops through all the cross-validation sets and averages the validation results
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历所有交叉验证集并平均验证结果
- en: 'From the evaluation results that follow, we can observe that max_depth=6 gives
    the lowest MSE, as shown next:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下评估结果中，我们可以观察到max_depth=6给出了最低的均方误差（MSE），如下所示：
- en: '[PRE23]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can use the same technique to select the values for other hyperparameters,
    and even the model type. For example, we can do cross-validation for both the
    linear regression and decision tree models with the same validation set and select
    the one with the better cross-validation results.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用相同的技巧来选择其他超参数的值，甚至模型类型。例如，我们可以使用相同的验证集对线性回归和决策树模型进行交叉验证，并选择交叉验证结果更好的一个。
- en: Sometimes you may have more hyperparameters to tune, which makes it hard to
    use a simple for loop for this task. scikit-learn provides a built-in class called
    GridSearchCV that makes this task more convenient. You provide it the search space
    of the hyperparameters as a dictionary, the model you want to tune, and a scoring
    function to measure the performance of the model. For example, in this problem,
    the search space is a dictionary with only one key, max_depth, whose value is
    a list containing its candidate values. The scoring function can be transformed
    from the performance metric using the make_scorer function. For example, we can
    transform the MSE into a scoring function as shown in listing 2.15\. It should
    be noted that GridSearchCV in scikit-learn assumes by default that highter scores
    are better. Because we want to find the model with the smallest MSE, we should
    set greater_is_better to False in the make_scorer function when defining the scoring
    function for the grid search.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候你可能需要调整更多的超参数，这使得使用简单的 for 循环来完成这项任务变得困难。scikit-learn 提供了一个内置的类，称为 GridSearchCV，这使得这项任务更加方便。你提供它超参数的搜索空间作为一个字典，你想要调整的模型，以及一个评分函数来衡量模型的性能。例如，在这个问题中，搜索空间是一个只有一个键的字典，即
    max_depth，其值是一个包含其候选值的列表。评分函数可以通过 make_scorer 函数从性能指标转换而来。例如，我们可以将 MSE 转换为如列表
    2.15 所示的评分函数。需要注意的是，scikit-learn 中的 GridSearchCV 默认假设更高的分数更好。因为我们想要找到具有最小 MSE
    的模型，所以在定义网格搜索的评分函数时，我们应该在 make_scorer 函数中将 greater_is_better 设置为 False。
- en: Listing 2.15 Grid search for the max_depth hyperparameter
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.15 对 max_depth 超参数的网格搜索
- en: '[PRE24]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Builds the decision tree regressor
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建决策树回归器
- en: ❷ Creates a dictionary as the search space for the hyperparameter max_depth
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个字典作为超参数 max_depth 的搜索空间
- en: ❸ Defines a scoring function
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义评分函数
- en: ❹ Creates the grid search cross-validation object with five-fold cross-validation
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 创建具有五折交叉验证的网格搜索交叉验证对象
- en: ❺ Fits the grid search object to the training data to find the optimal model
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将网格搜索对象拟合到训练数据以找到最佳模型
- en: We can retrieve the cross-validation results and plot the MSE against max_depth
    using the following code.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码检索交叉验证结果，并绘制 MSE 与 max_depth 的关系。
- en: Listing 2.16 Plotting the grid search cross-validation results
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.16 绘制网格搜索交叉验证结果
- en: '[PRE25]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Retrieves the cross-validation results
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取交叉验证结果
- en: ❷ Plots the MSE curve by increasing max_depth
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过增加 max_depth 绘制 MSE 曲线
- en: In figure 2.10, we can see that the MSE first decreases and then increases as
    max_depth increases. This is because as the depth of the tree grows, the model
    gains more flexibility and better capacity to refine the partitions. This will
    help the model better fit the training data, but eventually it will start to overfit.
    In our case this happens when max_depth>6. The model achieves the best performance
    when max_depth=6.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 2.10 中，我们可以看到 MSE 随着max_depth的增加先下降后上升。这是因为随着树深度的增加，模型获得了更多的灵活性，更好地能够细化分区。这将有助于模型更好地拟合训练数据，但最终它将开始过拟合。在我们的例子中，这发生在
    max_depth>6 时。模型在 max_depth=6 时达到最佳性能。
- en: '![02-10](../Images/02-10.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![02-10](../Images/02-10.png)'
- en: Figure 2.10 Variation of five-fold cross-validation results with increasing
    max_depth
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.10 随 max_depth 增加的五折交叉验证结果的变异
- en: 'You may be wondering whether cross-validation will accurately reflect the models’
    generalization ability on unseen examples. We can verify this by plotting both
    the cross-validation MSE curve and the test MSE curve of the 10 models with different
    values of max_depth, as shown here:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道交叉验证是否能够准确地反映模型在未见示例上的泛化能力。我们可以通过绘制具有不同 max_depth 值的 10 个模型的交叉验证 MSE
    曲线和测试 MSE 曲线来验证这一点，如下所示：
- en: '[PRE26]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Based on the test MSE values and figure 2.11, we can observe that the validation
    results perfectly select the max_depth corresponding to the best test results,
    and the two curves roughly align. Note that this is done only to illustrate the
    effectiveness of cross-validation—you should never use the test curve to select
    the model in practice!
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 根据测试均方误差（MSE）值和图2.11，我们可以观察到验证结果完美地选择了对应最佳测试结果的max_depth，并且两条曲线大致对齐。请注意，这样做只是为了说明交叉验证的有效性——在实际应用中，你绝不应该使用测试曲线来选择模型！
- en: '![02-11](../Images/02-11.png)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![02-11](../Images/02-11.png)'
- en: Figure 2.11 Comparison of the cross-validation result curve and the test result
    curve with increasing max_depth
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.11 随着max_depth增加的交叉验证结果曲线和测试结果曲线的比较
- en: Up to now, we’ve been using the California housing price-prediction problem
    to demonstrate the general steps in an ML pipeline before deploying the ML solution.
    The data we’ve used is structured in a table format, with rows representing the
    instances and columns indicating their features. This type of data is usually
    called *tabular data* or *structured data*. Besides tabular data, you might encounter
    many other types of data in different ML applications that require different treatment
    by selecting tailored components in the ML pipeline. Appendix B provides three
    more examples showing how to deal with image, text, and tabular data for classification
    tasks. All of them use classic data preparation methods and ML models. I recommend
    that you take a look at them before going on to the next chapter if you’re not
    familiar with these problems. These examples also show how to tune multiple hyperparameters
    jointly in an ML pipeline using the grid search method in scikit-learn. More advanced
    options for hyperparameter tuning will be discussed in the second part of the
    book.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直使用加利福尼亚房价预测问题来展示在部署机器学习解决方案之前机器学习管道中的通用步骤。我们使用的数据是以表格格式结构化的，行代表实例，列表示它们的特征。这类数据通常被称为*表格数据*或*结构化数据*。除了表格数据外，你可能在不同的机器学习应用中遇到许多其他类型的数据，这些数据需要通过在机器学习管道中选择定制组件来处理。附录B提供了三个更多示例，展示了如何处理图像、文本和表格数据以进行分类任务。所有这些都使用了经典的数据准备方法和机器学习模型。如果你不熟悉这些问题，我建议你在继续下一章之前先看看它们。这些示例还展示了如何使用scikit-learn中的网格搜索方法在机器学习管道中联合调整多个超参数。本书的第二部分将讨论更高级的超参数调整选项。
- en: Summary
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In an ML project, the first task is to frame the problem as an ML problem and
    assemble the dataset to be used.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在机器学习项目中，首要任务是将问题表述为机器学习问题，并组装用于的数据库集。
- en: Exploring and preparing the dataset is important. Extracting useful patterns
    from the data can improve the performance of the final ML solution.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索和准备数据集非常重要。从数据中提取有用的模式可以提高最终机器学习解决方案的性能。
- en: During the process of selecting an ML model, one should try out different models
    and evaluate their relative performance.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在选择机器学习模型的过程中，应该尝试不同的模型并评估它们的相对性能。
- en: Using the right hyperparameters for your model is critical to the final performance
    of the ML solution. Grid search is a naive AutoML approach that can be used for
    hyperparameter tuning and model selection.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用适合你模型的正确超参数对于机器学习解决方案的最终性能至关重要。网格搜索是一种简单的自动化机器学习（AutoML）方法，可用于超参数调整和模型选择。
