- en: 3 Using Fluentd to capture log events
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 使用 Fluentd 捕获日志事件
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Configuring Fluentd for the input of log files
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为日志文件的输入配置 Fluentd
- en: Examining the impact of stopping and starting during file reading by Fluentd
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过 Fluentd 检查文件读取过程中停止和启动的影响
- en: Using parsers to extract more meaning from log events
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用解析器从日志事件中提取更多意义
- en: Self-monitoring and external monitoring of Fluentd using APIs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 API 进行 Fluentd 的自监控和外部监控
- en: With the conceptual and architectural foundations set up, and having run a simple
    configuration, we’re ready to start looking at the capture of log events in more
    detail. In this chapter, we’re going to focus on capturing log events. But before
    we do, let’s look at how we can check that our Fluentd configuration is correct.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在建立了概念和架构基础，并运行了一个简单的配置之后，我们准备开始更详细地查看日志事件的捕获。在本章中，我们将重点关注捕获日志事件。但在我们这样做之前，让我们看看我们如何检查我们的
    Fluentd 配置是否正确。
- en: Setting up to follow and try the configurations
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 设置以跟踪和尝试配置
- en: A quick note about how we’re presenting code in the book. To avoid the book
    becoming bloated with code and Fluentd configuration files, we’ve included only
    the configuration and code parts relevant to the subject being discussed. But
    the files referenced in the downloads and GitHub repository are the complete configurations
    ([https://github.com/mp3monster/LoggingInActionWithFluentd](https://github.com/mp3monster/LoggingInActionWithFluentd)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 关于我们在书中如何呈现代码的简要说明。为了避免书籍因代码和 Fluentd 配置文件而变得臃肿，我们只包含了与讨论主题相关的配置和代码部分。但下载和 GitHub
    仓库中引用的文件是完整的配置（[https://github.com/mp3monster/LoggingInActionWithFluentd](https://github.com/mp3monster/LoggingInActionWithFluentd)）。
- en: The repository includes both complete configurations and partial configuration
    files so you can implement configuration yourself. Along with this are scenarios
    and solutions to the scenarios that will allow you to try out your understanding
    of ideas in the book.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中包含了完整的配置和部分配置文件，这样您可以自行实现配置。此外，还包括了一些场景及其解决方案，这将使您能够尝试检验自己对书中观点的理解。
- en: If you have skipped the initial chapters, you need to ensure you have the LogSimulator
    installed and configured (details in Chapter 2) or have the basic setup as documented
    at [https://github.com/mp3monster/LogGenerator](https://github.com/mp3monster/LogGenerator)
    and including any troubleshooting tips.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您跳过了前面的章节，您需要确保您已安装并配置了 LogSimulator（详情见第 2 章）或已按照[https://github.com/mp3monster/LogGenerator](https://github.com/mp3monster/LogGenerator)中记录的基本设置进行操作，包括任何故障排除技巧。
- en: 3.1 Dry running to check a configuration
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 干运行以检查配置
- en: When developing Fluentd configurations, we don’t want to set up a test to discover
    that the configuration is incorrect. Just as we do when developing code, we use
    a means to check code before we try running the solution. This becomes more important
    as the configuration or code becomes more complex.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在开发 Fluentd 配置时，我们不希望设置一个测试来发现配置错误。正如我们在开发代码时所做的，我们在尝试运行解决方案之前，会使用一种方法来检查代码。当配置或代码变得更加复杂时，这一点变得更加重要。
- en: 'Dry running a configuration file gets Fluentd to load the configuration and
    confirm that it can execute it, based on syntactical correctness and whether the
    attributes are recognized and the values provided are valid. The dry-run option
    is part of the Fluentd command line. To use the dry-run capability, we just add
    `–-dry-run` to the command-line parameters. Any configuration errors are reported
    in the console output; for example:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 干运行配置文件使 Fluentd 加载配置并确认它可以根据语法正确性以及属性是否被识别以及提供的值是否有效来执行它。干运行选项是 Fluentd 命令行的一部分。要使用干运行功能，我们只需将
    `–-dry-run` 添加到命令行参数中。任何配置错误都会在控制台输出中报告；例如：
- en: '[PRE0]'
  id: totrans-14
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This dry run shows that a plugin is missing a mandatory attribute; in this case,
    the failed plugin configuration needs a path attribute.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这次干运行显示了一个插件缺少一个必需的属性；在这种情况下，失败的插件配置需要一个路径属性。
- en: Solving structural errors
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解决结构错误
- en: If the error is more “structural” in nature, such as omitting a start or end
    to a declarative block—for example, `</parse>` being missed when there is a `<parse>`
    declaration—then we’re most likely to see a `backtrace` (stack trace) error. Fluentd
    will complete the backtrace error with what it thinks is missing. The suggestion
    may be incorrect in these circumstances, and a missing syntactical element elsewhere
    is the cause. The easiest way to sort out these kinds of issues is to ensure you
    have applied good indentation and start matching up start and end blocks.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如果错误更“结构化”，例如省略了声明性块的开始或结束——例如，当存在`<parse>`声明时却遗漏了`</parse>`——那么我们最有可能看到`backtrace`（堆栈跟踪）错误。Fluentd将完成对它认为缺失内容的回溯错误。在这些情况下，建议可能是不正确的，而其他地方缺少的语法元素是原因。解决这类问题的最简单方法是确保你已经应用了良好的缩进并开始匹配开始和结束块。
- en: 'Successful execution of the dry run will result in Fluentd stopping gracefully.
    If you are running with the default logging level, then the following kind of
    message is displayed:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 干运行的成功执行将导致Fluentd优雅地停止。如果你使用默认的日志级别运行，那么将显示以下类型的消息：
- en: '[PRE1]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As this is classed as *info*, if you have configured Fluentd to be quieter,
    you will not see a message, just Fluentd coming to a stop without any errors.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这被归类为*info*，如果你已经将Fluentd配置为更安静，你可能不会看到消息，只会看到Fluentd在没有错误的情况下停止。
- en: Fluent Bit currently does not have a comparable feature, partly because the
    expectation is for simpler configurations. The ability to provide the Fluent Bit
    configuration entirely via the command line communicating an error will be more
    challenging. If your goal is to supply Fluent Bit with the configuration using
    the command line, we recommend that you start working with a file until the configuration
    is complete and valid. Then strip the new line and redundant whitespace—something
    that can be done with tools online (e.g., [browserling.com](https://www.browserling.com/))
    or just awk and sed on a Linux host. This should allow you to reduce the configuration
    to a single line, ready for use.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Fluent Bit目前没有类似的功能，部分原因是期望配置更简单。通过命令行通信来提供Fluent Bit配置的能力将更具挑战性。如果你的目标是使用命令行向Fluent
    Bit提供配置，我们建议你开始使用文件，直到配置完成且有效。然后删除新行和多余的空白字符——这可以通过在线工具（例如[browserling.com](https://www.browserling.com/))或仅使用Linux主机上的awk和sed来完成。这应该允许你将配置简化为单行，以便使用。
- en: 3.1.1 Putting validating Fluentd configuration into action
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 将验证Fluentd配置付诸实践
- en: 'As the nominated team Fluentd expert, you have been asked to check several
    configurations. This is an opportunity to try the dry-run feature to evaluate
    whether the configuration is valid and fix it if necessary (if you need to fix
    a configuration, it might be worth making a copy of the original configuration).
    The configuration files to validate are the following:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 作为指定的团队Fluentd专家，你被要求检查几个配置。这是一个尝试使用dry-run功能来评估配置是否有效并必要时修复它的机会（如果你需要修复配置，可能值得制作原始配置的副本）。要验证的配置文件如下：
- en: '`Chapter3/Fluentd/basic-file-read.conf`'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Chapter3/Fluentd/basic-file-read.conf`'
- en: '`Chapter3/Fluentd/dry-run.conf`'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Chapter3/Fluentd/dry-run.conf`'
- en: 'No one wants to become the person who fixes everyone’s Fluentd configurations,
    so you might consider how to share the answers to the following questions with
    your colleagues:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 没有人想成为那个为每个人修复Fluentd配置的人，所以你可能需要考虑如何与你的同事分享以下问题的答案：
- en: How do you know that the configuration file is valid?
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何知道配置文件是有效的？
- en: How do you know when a configuration is faulty?
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你如何知道配置有误？
- en: Answers
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: 'You should have found that `Chapter3/Fluentd/basic-file-read.conf` is valid
    already. We know that this configuration file is fine, as when the dry-run mode
    is used, the console output will not report any error messages and should terminate
    cleanly. The process will terminate with the following message (assuming the log
    levels haven’t been set higher than info):'
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你应该已经发现`Chapter3/Fluentd/basic-file-read.conf`已经是有效的。我们知道这个配置文件是好的，因为当使用dry-run模式时，控制台输出不会报告任何错误消息并且应该干净地终止。进程将以以下消息终止（假设日志级别没有设置高于info）：
- en: '[PRE2]'
  id: totrans-31
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'The console logging output will report an error reflecting the configuration
    issue identified for `Chapter3/Fluentd/dry-run.conf`. We should see the following
    message with the details:'
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 控制台日志输出将报告一个错误，反映为`Chapter3/Fluentd/dry-run.conf`识别出的配置问题。我们应该看到以下带有详细信息的消息：
- en: '[PRE3]'
  id: totrans-33
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 3.2 Reading log files
  id: totrans-34
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 读取日志文件
- en: Log files are the most common source of log events when it comes to applications.
    While it is inefficient, file creation and consumption are the oldest ways for
    data to be shared between processes, including events. As a result, the *File*
    plugin is part of the set of core plugins.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到应用程序时，日志文件是日志事件最常见的来源。虽然效率不高，但文件创建和消耗是数据在进程之间共享（包括事件）的最古老方式。因此，*File* 插件是核心插件集的一部分。
- en: 'The first step is constructing a file source in a Fluentd configuration file.
    This can be done by adding the following fragment into a copy of the `Chapter3/Fluentd/no-source-config.conf`
    (or using `Chapter3/Fluentd/basic-file-read.conf`):'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在 Fluentd 配置文件中构建一个文件源。这可以通过将以下片段添加到 `Chapter3/Fluentd/no-source-config.conf`
    的副本中（或使用 `Chapter3/Fluentd/basic-file-read.conf`）来完成：
- en: Listing 3.1 Chapter3/Fluentd/basic-file-read.conf illustrating a file tail
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.1 `Chapter3/Fluentd/basic-file-read.conf` 展示文件尾部
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The file source plugin is called tail, as it behaves a bit like the Linux
    command of the same name.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 文件源插件被称为 tail，因为它在行为上有点像同名的 Linux 命令。
- en: ❷ Defines the file(s) to be captured
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 定义要捕获的文件
- en: ❸ Maximum number of lines that should be read before the events are read before
    starting to process them
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在开始处理事件之前应读取的最大行数
- en: ❹ Every file processor needs to know how to convert the text line input into
    a log event. There are several standard parsers provided by Fluentd, from predefined
    formats to expression processors.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 每个文件处理器都需要知道如何将文本行输入转换为日志事件。Fluentd 提供了几个标准解析器，从预定义格式到表达式处理器。
- en: As the configuration extract shows, we have told Fluentd to use any file it
    finds in the chapter 3 folder with a name starting with `basic-file` and read
    its contents without any form of parsing. Before starting Fluentd, we run the
    simulator so we can see what is produced. This can be done using the command
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如配置摘录所示，我们已告诉 Fluentd 使用第 3 章文件夹中任何以 `basic-file` 开头的文件，并读取其内容而不进行任何形式的解析。在启动
    Fluentd 之前，我们运行模拟器以查看输出内容。这可以通过以下命令完成
- en: '[PRE5]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: What we should see is the creation of a file called `basic-log.txt` in the chapter
    3 folder. The folder will contain only the message part of our source file `(TestData/small-source.txt`).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到在第三章文件夹中创建了一个名为 `basic-log.txt` 的文件。该文件夹将只包含我们源文件的消息部分 `(TestData/small-source.txt`）。
- en: With the log file generated, we can now start Fluentd to see what happens. This
    is done with the command (remembering that we are using relative paths as explained
    in chapter 2)
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 生成日志文件后，我们现在可以启动 Fluentd 来查看会发生什么。这是通过以下命令完成的（记住我们正在使用第 2 章中解释的相对路径）
- en: '[PRE6]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When Fluentd starts up, we’ll see the console output showing the configuration
    file. Soon after, it will detect the file and start sending it to the console
    as log events.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Fluentd 启动时，我们将看到控制台输出显示配置文件。不久之后，它将检测到文件并将其作为日志事件发送到控制台。
- en: 3.2.1 Putting the adaption of a Fluentd configuration to Fluent Bit into action
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 将 Fluentd 配置的适配应用到 Fluent Bit 中
- en: Your team has decided that the current configuration requirements are simple
    enough to use Fluent Bit rather than Fluentd. As part of preparing to deploy your
    solution in a container, you need to copy the `Chapter3/FluentBit/no-source-config.conf`
    file. Then apply the appropriate configuration changes and run Fluent Bit to test
    the configuration.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 您的团队已决定当前的配置要求足够简单，可以使用 Fluent Bit 而不是 Fluentd。作为在容器中部署解决方案的准备的一部分，您需要复制 `Chapter3/FluentBit/no-source-config.conf`
    文件。然后应用适当的配置更改并运行 Fluent Bit 以测试配置。
- en: Answer
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: Fluent Bit is started using the command `fluent-bit -c <configuration file>`.
    The configuration you will have produced should look like the configuration in
    `Chapter3/ExerciseResults/basic-file-read-FluentBit-Answer.conf`.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 使用命令 `fluent-bit -c <配置文件>` 启动 Fluent Bit。您将生成的配置应该看起来像 `Chapter3/ExerciseResults/basic-file-read-FluentBit-Answer.conf`
    中的配置。
- en: The simulator can be started with the command
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下命令启动模拟器
- en: '[PRE7]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The log events should be displayed on the console as a result.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 日志事件应作为结果显示在控制台上。
- en: 3.2.2 Rereading and resuming reading of log files
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 重新读取和继续读取日志文件
- en: 'If Fluentd was to stop (or needed to be stopped), but the application continued
    to write log events, then when Fluentd restarts, it will collect all log events
    it finds rather than only those written after Fluentd stopped. This behavior may
    be acceptable in a microservice where Fluentd is within the same container as
    the application logic, and as a result of Fluentd stopping, the Kubernetes pod
    is shut down. A fresh instance of the container is then started. But in many cases,
    this isn’t enough. Fortunately, this has been considered, and Fluentd has the
    means to track its progress through the log file(s). If the configuration does
    not track its position to resume where it left off, then a warning is displayed
    like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Fluentd需要停止（或需要被停止），但应用程序继续写入日志事件，那么当Fluentd重新启动时，它将收集它找到的所有日志事件，而不仅仅是Fluentd停止后写入的事件。在Fluentd与应用程序逻辑位于同一容器内的微服务中，这种行为可能是可接受的，并且由于Fluentd停止，Kubernetes
    pod被关闭。然后启动一个新的容器实例。但在许多情况下，这还不够。幸运的是，这已经被考虑到了，Fluentd有方法通过日志文件（们）跟踪其进度。如果配置没有跟踪其位置以从上次停止的地方继续，则会显示如下警告：
- en: '[PRE8]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'If you want to eliminate this warning, because you don’t need to continue where
    you left off, then an additional attribute in the configuration needs to be added
    to the tail statement:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想消除这个警告，因为你不需要从上次停止的地方继续，那么需要在配置中添加一个额外的属性到尾部语句中：
- en: '[PRE9]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In many cases, particularly with more traditional deployments, we will definitely
    want to resume where the last log event was read. We should introduce an attribute
    that tells Fluentd to record its progress through log files with
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，尤其是在更传统的部署中，我们肯定会希望从上次读取的最后一个日志事件继续。我们应该引入一个属性，告诉Fluentd通过日志文件记录其进度
- en: '[PRE10]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The `pos_file` attributed is used by the tail plugin to specify a file where
    the plugin can record its progress through the log file(s). When Fluentd restarts,
    the `pos_file` is examined as part of the startup to determine where to pick up
    from. But the `pos_ file` alone will not ensure that existing log entries are
    picked up the first time Fluentd is started. To ensure that all log events are
    collected from the start, we need to use the `read_from_head` attribute and set
    it to be `true`.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`pos_file`属性由尾部插件用于指定一个插件可以记录其通过日志文件（们）进度的文件。当Fluentd重新启动时，`pos_file`作为启动过程的一部分被检查，以确定从哪里开始。但仅`pos_file`本身并不能确保在Fluentd第一次启动时收集现有的日志条目。为了确保从开始收集所有日志事件，我们需要使用`read_from_head`属性并将其设置为`true`。'
- en: 3.2.3 Configuration considerations for tracking position
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 跟踪位置配置考虑因素
- en: 'Some design issues need to be considered when using position files, such as
    the one used by the tail plugin when the `pos_file` is defined. These considerations
    include the following:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用位置文件时，需要考虑一些设计问题，例如当定义了`pos_file`时，尾部插件所使用的位置文件。这些考虑因素包括以下内容：
- en: '*Avoid sharing the files across different tail configurations.* Sharing a file
    like this runs the risk of an I/O collision, as two different threads try to write
    their position information at the same time, resulting in file corruption. This
    also applies to setting up multiple worker threads (we’ll examine this in detail
    when looking at Fluentd scaling).'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*避免在不同尾部配置之间共享文件。* 共享此类文件存在I/O冲突的风险，因为两个不同的线程试图同时写入它们的位置信息，从而导致文件损坏。这也适用于设置多个工作线程（我们将在查看Fluentd扩展时详细研究这一点）。'
- en: '*You want the* `pos_file` *entry to exist only for as long as the log files
    exist.* If the log files are deleted, then the tracker file needs to be deleted.
    Otherwise, on a restart, the plugin can’t work out where to resume file processing
    correctly. This can be tricky in a containerized environment, as the file system
    may be entirely local and therefore as transient as the container. This can be
    overcome if the log files have their part of the file system mapped to durable
    storage outside the container.'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*你希望`pos_file`条目仅存在于日志文件存在期间。* 如果删除了日志文件，则需要删除跟踪文件。否则，在重启时，插件无法正确确定文件处理从哪里继续。在容器化环境中，这可能很棘手，因为文件系统可能完全是本地的，因此与容器一样短暂。如果日志文件的部分文件系统映射到容器外的持久存储，则可以克服这一点。'
- en: Recommendation If possible, when using position tracker files, hold them in
    the same folder as the log file(s) that they are used to track. This raises the
    chance that the tracker and log files will be handled consistently (i.e., retained
    or treated transiently). When the log files are purged, the chance of the `pos_file`
    being purged at the same time is better.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐：如果可能的话，在使用位置跟踪文件时，请将它们保存在与它们所跟踪的日志文件相同的文件夹中。这样做可以提高跟踪器和日志文件被一致处理（即保留或临时处理）的机会。当日志文件被清除时，`pos_file`
    同时被清除的机会会更大。
- en: Note There is some divergence between Fluentd and Fluent Bit when it comes to
    the use of `pos_file`. Fluent Bit doesn’t have a `pos_file` attribute; instead,
    it used `DB` as the attribute for this task.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在`pos_file`的使用方面，Fluentd和Fluent Bit之间存在一些差异。Fluent Bit没有`pos_file`属性；相反，它使用`DB`作为此任务的属性。
- en: Try to rerun the Fluentd configuration with the changes just described applied
    to the current configuration file (or run Fluentd with `Chapter3/Fluentd/basic-file
    -read2.conf`, which includes the position file in the configuration).
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试重新运行Fluentd配置，将上述更改应用于当前配置文件（或者运行带有`Chapter3/Fluentd/basic-file -read2.conf`的Fluentd，其中包含位置文件在配置中）。
- en: 3.2.4 Wildcards in the path attribute
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 路径属性中的通配符
- en: You may have noticed that the configuration has a wildcard (i.e., asterisk,
    “`*`”) instead of a file extension in the path declaration. Fluentd will accept
    the use of wildcards in the same way the operating system will. As a result, it
    is possible to read multiple files through a single *source* directive (or configuration
    if you prefer).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在路径声明中配置使用了通配符（即星号，“`*`”），而不是文件扩展名。Fluentd将以操作系统相同的方式接受通配符的使用。因此，可以通过单个*source*指令（或者如果你更喜欢，配置）读取多个文件。
- en: This can be simply illustrated if you have run the previous case configuration.
    By running that configuration, you should have a file called `basic-file.txt`
    and `basic -file-read2.pos_file` in the Chapter 3 folder. Delete the `pos_file`
    and then copy `basic-file.txt` to `basic-file.log`. Rerun Fluentd with the same
    configuration file as last time. In the log output, you will have two entries
    saying `#0 following tail of ./Chapter3/basic-file.txt` and `#0 following the
    tail of ./Chapter3/basic-file.log`. If you open the `pos_file`, you will see that
    each file has a line.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经运行了之前的案例配置，这可以简单地说明。通过运行该配置，你应该在第三章文件夹中有一个名为`basic-file.txt`和`basic -file-read2.pos_file`的文件。删除`pos_file`，然后将`basic-file.txt`复制到`basic-file.log`。使用与上次相同的配置文件重新运行Fluentd。在日志输出中，你会看到两条记录说`#0
    following tail of ./Chapter3/basic-file.txt`和`#0 following the tail of ./Chapter3/basic-file.log`。如果你打开`pos_file`，你会看到每个文件都有一行。
- en: NOTE If you try to delete the pos file while Fluentd is still running, Fluentd
    will still be holding a handle to the file, which prevents deletion. Therefore,
    always shut down the Fluentd process first.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你在Fluentd仍在运行时尝试删除pos文件，Fluentd仍然会持有该文件的句柄，这会阻止删除。因此，始终先关闭Fluentd进程。
- en: This means that wildcards must be used with care; it can be advantageous if
    an application creates multiple logs in the same place that need to be captured.
    The other use case where this can be of enormous help is if you elastically scale
    solutions such as web servers with all the servers configured to log to a high-performance
    network storage device. We can set the Fluentd configuration to target a single
    folder location rather than having a Fluentd configuration for each server’s log
    files.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着在使用通配符时必须谨慎；如果应用程序需要在同一位置创建多个需要捕获的日志，这可能会带来优势。另一个可以极大帮助的使用案例是，如果你弹性扩展解决方案，例如配置为将日志记录到高性能网络存储设备的Web服务器。我们可以将Fluentd配置设置为针对单个文件夹位置，而不是为每个服务器的日志文件设置Fluentd配置。
- en: In this latter case, if each web server has its log file and new web servers
    are started after Fluentd, we need to detect the new log files. By default, Fluentd
    checks every 60 seconds for files matching the `path` attribute. This can be tuned
    using the attribute called `refresh_interval` using a time expression; for example,
    `refresh_interval 5s` means checking every 5 seconds`.` As a result, those new
    web server log files will be picked up on the subsequent scan. This provides a
    simple way to accommodate autoscaling.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在后一种情况下，如果每个Web服务器都有自己的日志文件，并且在Fluentd启动后启动了新的Web服务器，我们需要检测新的日志文件。默认情况下，Fluentd每60秒检查一次匹配`path`属性的文件。这可以通过使用名为`refresh_interval`的属性并使用时间表达式来调整；例如，`refresh_interval
    5s`表示每5秒检查一次。因此，那些新的Web服务器日志文件将在后续扫描中被捕获。这提供了一种简单的方法来适应自动扩展。
- en: 3.2.5 Expressing time
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.5 表达时间
- en: When defining time as an interval, such as when you need to specify a task frequency,
    Fluentd has a time data type with an associated notation for setting these attribute’s
    values. For time data type attributes, we can represent the time values as shown
    in table 3.1.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 当将时间定义为间隔，例如需要指定任务频率时，Fluentd有一个时间数据类型，以及用于设置这些属性值的关联符号。对于时间数据类型属性，我们可以像表3.1所示那样表示时间值。
- en: Table 3.1 Notations for expressing values of time type in Fluentd configurations
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 Fluentd配置中表达时间类型值的符号
- en: '| Interval | Character | Examples |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 时间间隔 | 字符 | 示例 |'
- en: '| Seconds | s | 10s → 10 seconds0.1s → 100ms |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 秒 | s | 10s → 10秒0.1s → 100毫秒 |'
- en: '| Minutes | m | 1m → 1 minute0.25m → 15 seconds |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 分钟 | m | 1m → 1分钟0.25m → 15秒 |'
- en: '| Hours | h | 24h → 24 hours0.25h → 15 minutes |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 小时 | h | 24h → 24小时0.25h → 15分钟 |'
- en: '| Days | d | 1d → 1 day0.5d → 12 hours |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| 天 | d | 1d → 1天0.5d → 12小时 |'
- en: The number provided will be treated as an integer. If the value is not an integer,
    it will be processed as a float representing a fraction of the time period. This
    notation applies to nearly all standard Fluentd and Fluent Bit plugin properties
    used for expressing time intervals.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 提供的数字将被视为整数。如果值不是整数，它将被处理为一个表示时间周期分数的浮点数。这种表示法适用于几乎所有用于表达时间间隔的标准Fluentd和Fluent
    Bit插件属性。
- en: 3.2.6 Controlling the impact of wildcards in filenames
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.6 控制文件名中通配符的影响
- en: As we have just discussed, the use of wildcards can be powerful, but it also
    comes with risks, such as picking up unwanted files. There are several strategies
    that we can apply to control the risks of wildcards.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚才讨论的，通配符的使用可能很强大，但也伴随着风险，例如选择不需要的文件。我们可以应用几种策略来控制通配符的风险。
- en: Explicit listing
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 明确列出
- en: 'If all the filenames are known in advance, the `path` attribute can be populated
    as a comma-separated list of files. The files will be processed in the same way
    as the wildcard path matching multiple files—each file will be read and a pos
    entry recorded if we are recording read progress. The `path` attribute could look
    something like this:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果所有文件名都提前知道，`path` 属性可以填充为一个以逗号分隔的文件列表。文件将以与通配符路径匹配多个文件相同的方式进行处理——如果我们在记录读取进度，每个文件将被读取并记录一个
    pos 条目。`path` 属性可能看起来像这样：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: As with many other plugins, it is possible to change the delimiter of each entry
    in the path. This is done by setting the attribute `path_delimiter` (e.g., `path_delimiter
    = ';'`), allowing us to get around any strange file-naming issues.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与许多其他插件一样，可以更改路径中每个条目的分隔符。这是通过设置属性 `path_delimiter`（例如，`path_delimiter = ';'`）来完成的，这使得我们可以绕过任何奇怪的文件命名问题。
- en: Log sequencing by dated folders
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 按日期文件夹进行日志排序
- en: Some solutions allow you to configure logging so that folders contain all the
    logs for a particular period (all logs for a day, month, etc.). This makes it
    easier to manage multiple logs covering long periods. Fluentd is, therefore, able
    to process the structure of such file paths using date elements (e.g., `Chapter3/2020/08/30/app.log`).
    To achieve this effect, we need to modify the configuration to look like
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 一些解决方案允许您配置日志记录，以便文件夹包含特定时间段的所有日志（一天、一个月等的所有日志）。这使得管理覆盖长期的多日志更容易。因此，Fluentd能够使用日期元素（例如，`Chapter3/2020/08/30/app.log`）处理此类文件路径的结构。为了实现这种效果，我们需要修改配置，使其看起来像
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: In the section “Expressing dates and times” in appendix B, there is a table
    that describes the different escape sequences, such as `%Y`, `%m,` and `%d`, shown
    in the preceding example.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录B的“表达日期和时间”部分，有一个表格描述了不同的转义序列，例如 `%Y`、`%m,` 和 `%d`，如前例所示。
- en: Configuration errors
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 配置错误
- en: Fluentd is good at indicating errors in its configuration. Typically they are
    published as warnings in the Fluentd output with an explanation. For example,
    adding `%B` into the filename would yield `./Chapter3/structured-rolling-logMay.0.log
    not found. Continuing without tailing it`. This makes sense; the file does not
    exist. Another example of error handling is using escape sequences for incorrect
    date attributes, resulting in Fluentd treating the values as normal text and reporting
    errors.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd擅长指示其配置中的错误。通常它们作为警告发布在Fluentd输出中，并附有解释。例如，将 `%B` 添加到文件名中会产生 `./Chapter3/structured-rolling-logMay.0.log
    not found. Continuing without tailing it`。这是有道理的；文件不存在。另一个错误处理的例子是使用转义序列处理不正确的日期属性，导致Fluentd将这些值视为普通文本并报告错误。
- en: Using file exclusions
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用文件排除
- en: 'Another approach is to exclude files by providing a list of files that should
    never be considered using the `exclude_path` attribute. This attribute works just
    like the `path` attribute, so it can use wildcards or comma-separated lists. For
    example:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用`exclude_path`属性提供不应考虑的文件列表来排除文件。此属性的工作方式与`path`属性类似，因此它可以使用通配符或逗号分隔列表。例如：
- en: '[PRE13]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This declaration would prevent any files with .zip or thread dump files from
    being collected from the folder. This is a good way to mitigate the risk of accidentally
    picking up files that shouldn’t be captured--for example, if an application’s
    logging may also generate stack dump files or documents intended to be sent to
    a vendor for analysis in the same file location as normal logs.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此声明将防止从文件夹中收集任何具有.zip或线程转储文件的文件。这是一种减轻意外选择不应捕获的文件的风险的好方法——例如，如果应用程序的日志也可能生成堆栈转储文件或打算发送给供应商进行分析的文档，并且这些文件位于与正常日志相同的文件位置。
- en: Only consider recently changed files
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 只考虑最近更改的文件
- en: 'We can tell the plugin to only consider files that have changed within a certain
    time frame. So, we could assume that any files generated/changed soon after installation
    or a restart contain log events worth collecting; this is done by setting the
    attribute `limit_recently_modified` to a time-interval value. For example:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以告诉插件只考虑在一定时间范围内发生变化的文件。因此，我们可以假设在安装或重启后不久生成的/更改的任何文件都包含值得收集的日志事件；这是通过将属性`limit_recently_modified`设置为时间间隔值来完成的。例如：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `limit_recently_modified` attribute is another instance of a time data type,
    so it can be configured in the way we just described.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`limit_recently_modified`属性是时间数据类型的一个实例，因此它可以按照我们刚才描述的方式进行配置。'
- en: 'Controlling which log files are examined by using the change duration can help
    in several ways:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用更改持续时间来控制要检查哪些日志文件，可以在几个方面有所帮助：
- en: If a lot of log files are retained in the same location (as would be the case
    for log rotation--more on this shortly), then we can control how far back we go
    in processing files.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果在相同位置保留了大量的日志文件（如日志轮转的情况——稍后将有更多介绍），那么我们可以控制在处理文件时可以回溯多远。
- en: In real-time use cases, such as log events from the equipment on a manufacturing
    line, if the log event is not captured within a specific time frame, then the
    log event becomes redundant, as nothing can be done. So why waste time processing
    log events that are effectively out of date?
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在实时用例中，例如来自制造线设备的日志事件，如果日志事件没有在特定时间范围内被捕获，那么该日志事件就变得冗余，因为什么也不能做。那么为什么还要浪费时间处理实际上已经过时的日志事件呢？
- en: If the available compute capacity is small, and there is a large backlog of
    logs to be caught up on, you can create a condition where you never catch up to
    the currently generated events. Limiting how far backlog file processing can go
    reduces the risk of this scenario.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可用的计算能力较小，并且有大量的日志需要追回，你可以创建一个条件，即你永远不会追赶上当前生成的事件。限制回溯文件处理可以回溯多远，可以降低这种场景的风险。
- en: The use of this configuration needs to be carefully considered, along with the
    frequency of checking for new files. If the refresh interval is longer than the
    `limit_ recently_modified` attribute, then by the time new files are identified,
    they may well have fallen out of the time frame of the limit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 需要仔细考虑这种配置的使用，以及检查新文件的频率。如果刷新间隔长于`limit_recently_modified`属性，那么当新文件被识别时，它们可能已经超出了限制的时间范围。
- en: 3.2.7 Replacing wildcards with delimited lists in action
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.7 在操作中用分隔列表替换通配符
- en: Back up the configuration file `Chapter3/Fluentd/basic-file-read2.conf`. Copy
    `basic-file.txt` to a file called `basic-file.log`, and then copy the file again
    so that this copy is called `basic-file.out`. Modify the path in the Fluentd configuration
    so that the wildcard is not in the path, and, using the comma notation, add the
    .txt and .out files.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 备份配置文件`Chapter3/Fluentd/basic-file-read2.conf`。将`basic-file.txt`复制到名为`basic-file.log`的文件中，然后再次复制该文件，以便这个副本被称为`basic-file.out`。修改Fluentd配置中的路径，以便通配符不在路径中，并使用逗号表示法添加.txt和.out文件。
- en: Run Fluentd and the simulator as we did earlier using the commands
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们之前使用的命令运行Fluentd和模拟器
- en: '`groovy LogSimulator.groovy ./Chapter3/SimulatorConfig/basic-log-file.properties
    ./TestData/source.txt`'
  id: totrans-114
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`groovy LogSimulator.groovy ./Chapter3/SimulatorConfig/basic-log-file.properties
    ./TestData/source.txt`'
- en: '`fluentd -c Chapter3/Fluentd/basic-file-read2.conf`'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`fluentd -c Chapter3/Fluentd/basic-file-read2.conf`'
- en: Replacing wildcards with delimited lists solution
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 用分隔列表替换通配符的解决方案
- en: The change to the configuration file should result in the attribute looking
    like `Chapter3/Fluentd/basic-file-read2-Answer.conf.` As with the wildcard run,
    the output will contain two log files (you can see this from the `pos_file` and
    by reviewing the console output). But the additional third file was not processed.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件的更改应导致属性看起来像`Chapter3/Fluentd/basic-file-read2-Answer.conf.`。与通配符运行一样，输出将包含两个日志文件（你可以从`pos_file`中看到，也可以通过查看控制台输出）。但额外的第三个文件没有被处理。
- en: 3.2.8 Handling log rotation
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.8 处理日志轮转
- en: Log rotation is a common solution to allowing a substantial level of logging
    to be collected without logs files becoming so large that they are too difficult
    to work with or endlessly consume space. Log rotation also simplifies the process
    of purging older content rather than trimming a file; you simply delete the oldest
    log.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 日志轮转是一种常见的解决方案，允许收集大量日志，同时避免日志文件变得过大，难以处理或无限消耗空间。日志轮转还简化了清除旧内容的过程，而不是修剪文件；你只需删除最旧的日志文件。
- en: The *tail* input plugin can handle log rotation. The out-of-the-box approach
    is to define the configuration so that the path explicitly identifies the lead
    file in the rotation (e.g., `path ./Chapter3/structured-rolling-log.0.log`). By
    excluding wildcards from the folder that contains the log, rotation means that
    as files are rotated, they will not get picked up by the folder rescan. With this,
    we add the attribute `rotate_wait`. This attribute stipulates a period during
    which the current file, which will have been rotated to `./Chapter3/structured-rolling-log.1
    .log`, continues to be read. This is necessary, as the log writer may not have
    finished flushing content to the first file before creating the new file, possibly
    resulting in the final content of the older file being missed. So, awareness of
    how long it may take the writer to complete is important; if this isn’t known,
    you need to allow enough time so that logs aren’t being generated so fast that
    you can never catch up. We have created a setup to illustrate the behavior. Listing
    3.2 shows the input configuration.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '*tail* 输入插件可以处理日志轮转。默认的方法是定义配置，以便路径明确标识轮转中的主文件（例如，`path ./Chapter3/structured-rolling-log.0.log`）。通过排除包含日志的文件夹中的通配符，轮转意味着当文件轮转时，它们不会被文件夹重新扫描选中。因此，我们添加了`rotate_wait`属性。此属性规定了一个时间段，在此期间，当前文件（将轮转至`./Chapter3/structured-rolling-log.1.log`）继续被读取。这是必要的，因为日志写入者可能在新文件创建之前还没有完成将内容刷新到第一个文件，这可能导致错过旧文件的最终内容。因此，了解写入者可能需要多长时间完成是很重要的；如果不知道，你需要留出足够的时间，以便日志不会被生成得太快，以至于你永远无法赶上。我们已经创建了一个设置来展示这种行为。列表3.2显示了输入配置。'
- en: 'Rotate_wait: Suggested configuration'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 'Rotate_wait: 建议的配置'
- en: Every case is different and is dependent upon how the log writer mechanism works.
    For backend servers, we have typically looked at 30 seconds as a reasonable tradeoff.
    This is based on not wanting to get too far behind with the logs, as catchup can
    create spikes in workload (as we catch up with the log events on an active server),
    and if we experience a node issue, there is a fair chance we’ll have caught the
    events leading up to the catastrophic event.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 每个案例都是不同的，并且取决于日志写入机制的工作方式。对于后端服务器，我们通常将30秒视为合理的权衡。这是基于不希望日志落后太多，因为追赶可能会在工作负载中产生峰值（因为我们正在追赶活动服务器上的日志事件），并且如果我们遇到节点问题，有很大机会我们会捕捉到灾难性事件之前的事件。
- en: Listing 3.2 Chapter3/Fluentd/rotating-file-read.conf
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2 Chapter3/Fluentd/rotating-file-read.conf
- en: '[PRE15]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Note the absence of a wildcard, although we could also use ./*/structured-rolling-log.0.log
    as long as the other chapter folders are still clean.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注意没有通配符，尽管我们也可以使用 ./*/structured-rolling-log.0.log，只要其他章节文件夹仍然干净。
- en: ❷ Our rotation control with a decent amount of time expressed as an elapsed
    time using the notation described in section 3.2.5
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们使用3.2.5节中描述的符号表示法，以合理的时间间隔表示我们的轮转控制。
- en: ❸ Ensures we read from the beginning of the file. However, this will only be
    the beginning of the current rotation.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 确保我们从文件的开头读取。然而，这将是当前轮转的开始。
- en: We can run this configuration with the console command from the folder where
    all the book’s chapter folders have been downloaded to
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以从所有章节文件夹下载到的文件夹中运行此配置的命令行。
- en: '[PRE16]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'In a second console, run the following command:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二个控制台中，运行以下命令：
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The LogSimulator runs with a configuration that leverages the *standard Java
    utility logging* framework (part of the core of Java) to provide the log rotation
    behavior. Java’s logging works like a wide range of logging frameworks (logging
    frameworks are explored further in part 4). The simulator has been configured
    to loop over a data set several times. To make it easy to observe this behavior,
    the simulator adds a line counter for each line in the file. The iteration counter
    is added to the front of each message. So, if you track the output, you will see
    that all the lines are in the correct sequence as they come out of Fluentd. If
    you are quick enough, you will also observe log messages like this on the console:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: LogSimulator使用配置利用了*标准Java实用工具日志*框架（Java核心的一部分）来提供日志轮换行为。Java的日志工作方式类似于广泛的日志框架（日志框架在第四部分中进一步探讨）。模拟器已被配置为多次遍历数据集。为了便于观察这种行为，模拟器为文件中的每一行添加了一个行计数器。迭代计数器被添加到每条消息的前面。因此，如果您跟踪输出，您将看到所有行都按照从Fluentd输出的正确顺序排列。如果您足够快，您还会在控制台上观察到这样的日志消息：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Note the value `(36-1)` in the first of these lines; this is reflecting the
    line number and iteration number, respectively, from the LogSimulator configuration,
    with the line number being the line from the source log and the iteration being
    the count of how many times we’ve fed the log entries through for Fluentd to pick
    up. The rest of the first line reflects the log message. The following two lines
    tell us Fluentd is aware of the rotation file changes, but before ensuring that
    it is reading from the new lead file, it will continue to monitor the current
    file for any final content to be flushed.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这些行中的第一个值`(36-1)`；这分别反映了从LogSimulator配置中得到的行号和迭代号，行号是从源日志的行开始的，迭代号是我们将日志条目传递给Fluentd以供其捕获的次数。第一行剩余的部分反映了日志消息。接下来的两行告诉我们Fluentd已经意识到轮换文件的变化，但在确保它从新的领先文件中读取之前，它将继续监控当前文件以查找任何需要刷新的最终内容。
- en: Warning If Fluentd must restart but the application continues to run, and the
    logs rotate before Fluentd recovers, then the use of `read_from_head true` will
    take only log events from the start of the current rotation. A result of any logs
    between Fluentd stopping and the latest rotation will not get captured. This impact
    can be mitigated by ensuring that Fluentd will automatically restart if it fails
    and making the time to typically fill a log more than the time for Fluentd to
    fail and recover.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果Fluentd必须重新启动，但应用程序继续运行，并且日志在Fluentd恢复之前轮换，那么使用`read_from_head true`将只会读取当前轮换开始的日志事件。Fluentd停止和最新轮换之间的任何日志都不会被捕获。可以通过确保Fluentd在失败时自动重启，并使日志通常填充的时间超过Fluentd失败和恢复的时间来减轻这种影响。
- en: 'There is an alternative approach if it is necessary to use wildcards within
    a single folder using several attributes:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要在单个文件夹内使用多个属性使用通配符，则存在另一种方法：
- en: '`refresh_interval`—Controls the frequency at which the list of files that should
    be detected by the wildcard can be renewed.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`refresh_interval`——控制应该由通配符检测的文件列表更新的频率。'
- en: '`limit_recently_modified`—This prevents older log files that may be picked
    up because of the wildcard from being used as long as they haven’t changed in
    the period defined.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`limit_recently_modified`——这防止了由于通配符而被捕获的较旧日志文件在它们在定义的期间内未更改的情况下被使用。'
- en: '`pos_file_compaction_interval`—This is an interval between each visit to the
    position tracker file to have its entries tidied up. Depending upon the configuration,
    the pos file can accumulate entries that then become redundant. As the position
    file is regularly read and updated, the fewer the entries in the file, the more
    efficient handling the position file will be. Given this, it is best to periodically
    do some housekeeping.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`pos_file_compaction_interval`——这是每次访问位置跟踪器文件以整理其条目之间的间隔。根据配置，位置文件可能会积累条目，然后变得冗余。由于位置文件定期读取和更新，文件中的条目越少，对位置文件的处理就越高效。因此，最好定期进行一些清理工作。'
- en: This approach depends on the log file being regularly written to; otherwise,
    the logging will “stutter.” As a rule, a log file is out of scope for capture
    if the last update was older than the `limit_recently_modified` value. As before,
    we have a config file containing the input as shown in the next listing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法依赖于日志文件定期写入；否则，日志将“中断”。一般来说，如果最后更新时间早于`limit_recently_modified`值，则日志文件将超出捕获范围。与之前一样，我们有一个包含输入的配置文件，如下所示。
- en: Listing 3.3 Chapter3/Fluentd/rotating-file-read-alternate.conf
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 Chapter3/Fluentd/rotating-file-read-alternate.conf
- en: '[PRE19]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The path has wildcards now.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 路径现在有通配符。
- en: ❷ The time interval to check to sweep for new log files if logs need to be set,
    as explained in section 3.2.5
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果需要设置日志，检查新日志文件的时间间隔，如 3.2.5 节中所述
- en: ❸ To avoid accidentally reading the older logs, we identify a period in which
    the file must change.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为了避免意外读取旧日志，我们确定了一个必须更改文件的时间段。
- en: ❹ The cleanup interface
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 清理界面
- en: As with the previous illustration, the Fluentd instance can be fired up with
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 与前面的示例一样，Fluentd 实例可以通过以下方式启动
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'For the log events to track, you can use the same configuration as the last
    time:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 对于要跟踪的日志事件，你可以使用上次相同的配置：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Warning As with the main way to handle log rotation, it is possible to lose
    logs. In addition, if the `limit_recently_modified` attribute is set to be too
    short, the new log file is picked up as the current file has a final flush or
    filehandle closed, which may include any final log entries to be written to storage.
    These final file operations can impact the file change stamp, triggering Fluentd’s
    file scan to detect the older log file as being in scope. This will potentially
    result in overlapping log entries as older log events are collected after more
    recent ones.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：与处理日志轮换的主要方式一样，可能会丢失日志。此外，如果将 `limit_recently_modified` 属性设置得太短，新日志文件在当前文件进行最终刷新或文件句柄关闭时被选中，这可能会包括任何要写入存储的最终日志条目。这些最终文件操作可能会影响文件更改时间戳，触发
    Fluentd 的文件扫描以检测旧日志文件是否在范围内。这可能会导致日志条目重叠，因为旧日志事件在较新的事件之后被收集。
- en: 3.3 Self-monitoring
  id: totrans-152
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 自我监控
- en: In the previous section, there are scenarios in rotating logs where there may
    be an outside chance of losing log events. But Fluentd, like any good application,
    logs events about its activity. This means it is possible to use Fluentd to monitor
    its well-being by tracking its log events. In addition to Fluentd’s logs, there
    are other ways to obtain health information, as we will see.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，在日志轮换的场景中，可能会有丢失日志事件的外部机会。但是，Fluentd，就像任何好的应用程序一样，会记录其活动的事件。这意味着可以通过跟踪其日志事件来使用
    Fluentd 监控其健康状况。除了 Fluentd 的日志之外，还有其他方法可以获得健康信息，我们将在后面看到。
- en: 3.3.1 HTTP interface check
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 HTTP 接口检查
- en: Fluentd provides an HTTP endpoint that will provide information about how the
    instance is set up, as the following listing shows.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 提供了一个 HTTP 端点，它将提供有关实例设置的详细信息，如下面的列表所示。
- en: Listing 3.4 Chapter3/Fluentd/rotating-file-self-check.conf
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.4 Chapter3/Fluentd/rotating-file-self-check.conf
- en: '[PRE22]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ The address to bind to (i.e., the local server)
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 绑定的地址（即本地服务器）
- en: ❷ The port to be used for this service
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于此服务要使用的端口
- en: With the Fluentd running with the provided configuration (`fluentd -c Chapter3/Fluentd/
    rotating-file-self-check.conf`), start up Postman as we did in chapter 2\. Then
    configure the address to be `0.0.0.0:24220/api/plugins.json`. As you can see in
    the `bind` attribute, as with other plugins, this relates to the DNS or IP of
    the host, and the `port` attribute matches the port part of the URL. The interface
    could be described as `{bind}:{port}/api/plugins.json`. Unlike in chapter 2, where
    the operation was a POST, we need the operation set to be GET. Once done, click
    the send button, and we will see an HTTP representation of the running configuration
    returned, as highlighted in figure 3.1.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 使用提供的配置（`fluentd -c Chapter3/Fluentd/ rotating-file-self-check.conf`）运行 Fluentd，然后像第
    2 章中那样启动 Postman。然后将地址配置为 `0.0.0.0:24220/api/plugins.json`。正如你在 `bind` 属性中看到的，与其他插件一样，这关系到主机的
    DNS 或 IP，而 `port` 属性与 URL 的端口号相匹配。接口可以描述为 `{bind}:{port}/api/plugins.json`。与第
    2 章中的操作不同，那里的操作是 POST，我们需要将操作设置为 GET。完成设置后，点击发送按钮，我们将看到返回的运行配置的 HTTP 表示，如图 3.1
    所示。
- en: '![](../Images/CH03_F01_Wilkins.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_F01_Wilkins.png)'
- en: Figure 3.1 Postman illustrating the outcome of invoking the Fluentd API made
    available by including the monitor_agent plugin
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 使用包含 monitor_agent 插件的 Fluentd API 返回结果的 Postman 示例
- en: As you can see in figure 3.1, the URL and the result are highlighted. If you
    prefer the results to be represented using label tab-separated values (*ltsv*),
    just omit the `.json` from the URL. The URL can also handle several additional
    parameters when the output is set to be JSON. For example, adding to the URL `?debug=1`
    will yield a range of additional state information (note the value for `debug`
    does not matter; it is the presence of a parameter that is significant). The full
    set of parameters available to use as part of the monitoring agent URL is described
    in table 3.2.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 正如图 3.1 所示，URL 和结果被突出显示。如果你希望结果使用标签分隔值（*ltsv*）来表示，只需从 URL 中省略 `.json`。当输出设置为
    JSON 时，URL 也可以处理几个额外的参数。例如，向 URL 中添加 `?debug=1` 将会提供一系列额外的状态信息（注意 `debug` 的值并不重要；参数的存在才是重要的）。可用于作为监控代理
    URL 部分的所有参数的完整集合在表 3.2 中描述。
- en: Table 3.2 URI parameters available for calling the monitor_agent API
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 表 3.2 可用于调用 monitor_agent API 的 URI 参数
- en: '| URI Parameter | Description | Example |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| URI 参数 | 描述 | 示例 |'
- en: '| *debug* | Will get additional plugin state information to be included in
    the response. The value set for the parameter does not matter. | `?debug=0` |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| *debug* | 将获取额外的插件状态信息以包含在响应中。参数设置的值并不重要。 | `?debug=0` |'
- en: '| *with_ivars* | The use of this parameter is sufficient for the `instance_variables`
    attribute to be included in the response. We will discuss the instance variables
    when we develop our plugin later in the book. | `?with_ivars=false` |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| *with_ivars* | 使用此参数足以使 `instance_variables` 属性包含在响应中。我们将在本书后面的插件开发部分讨论实例变量。
    | `?with_ivars=false` |'
- en: '| *with_config* | Overrides the default or explicit setting `include_config`.
    The value provided must be true in lowercase; all other values are treated as
    false. | `?with_config=true` |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| *with_config* | 覆盖默认或显式设置的 `include_config`。提供的值必须是小写形式的 true；其他所有值都被视为 false。
    | `?with_config=true` |'
- en: '| *with_retry* | Overrides the default or explicit setting `include_config`.
    The value provided must be true or false in lowercase; all other values are treated
    as false. | `?with_retry=true` |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| *with_retry* | 覆盖默认或显式设置的 `include_config`。提供的值必须是小写形式的 true 或 false；其他所有值都被视为
    false。 | `?with_retry=true` |'
- en: '| *tag* | This filters the returned configuration to return only the directives
    linked to the tag name provided. | `?tag=simpleFile` |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| *tag* | 这将返回的配置过滤，只返回与提供的标签名称链接的指令。 | `?tag=simpleFile` |'
- en: '| *@id* | This filters the response down to a specific directive. If the configuration
    doesn’t have an explicit ID, then the value will be arbitrary. | `?id= in_monitor_agent`
    |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| *@id* | 这将响应过滤到特定的指令。如果配置没有显式的 ID，则值将是任意的。 | `?id= in_monitor_agent` |'
- en: '| *@type* | Allows the results to be filtered by plugin type. | `?id=tail`
    |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| *@type* | 允许通过插件类型过滤结果。 | `?id=tail` |'
- en: 'We can also get the basic state of the plugins periodically reported within
    Fluentd by adding the `tag` and `emit_interval` attributes to the source directive.
    We can see the impact if we run the configuration with these values set using
    `fluentd -c Chapter3/Fluentd/ rotating-file-self-check2.conf` (or you can try
    editing and adding the attributes to the previous configuration yourself). With
    Fluentd up and running, we will start to see some status information every 10
    seconds, like the following fragment published:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过在源指令中添加 `tag` 和 `emit_interval` 属性来获取 Fluentd 定期报告的插件的基本状态。如果我们使用 `fluentd
    -c Chapter3/Fluentd/rotating-file-self-check2.conf`（或者你可以尝试编辑并添加这些属性到之前的配置中）来运行配置，我们可以看到这些设置的影响。当
    Fluentd 运行时，我们每 10 秒钟将开始看到一些状态信息，如下面的片段所示：
- en: '[PRE23]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: As the information is tagged, we can direct this traffic to a central monitoring
    point, all of which saves on needing to script the HTTP polling. By incorporating
    the source in the following listing into the configuration file (before the `match`),
    the information gets fed to the same output.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 由于信息被标记，我们可以将此流量引导到中央监控点，从而节省了需要编写 HTTP 轮询脚本的需求。通过将以下列表中的源包含到配置文件中（在 `match`
    之前），信息会被发送到相同的输出。
- en: Listing 3.5 Chapter3/Fluentd/rotating-file-self-check2.conf
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 Chapter3/Fluentd/rotating-file-self-check2.conf
- en: '[PRE24]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Tells the monitor_agent to include configuration information in the output
    of the agent
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 告诉监控代理在代理的输出中包含配置信息
- en: ❷ How frequently the monitoring_agent should run its self-checking and output
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 监控代理应多久运行一次自我检查和输出
- en: 3.4 Imposing structure on log events
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 在日志事件上施加结构
- en: 'So far, we have only looked at the most simplistic log files; however, very
    few log files are like this. The more structured they are, the better we can apply
    more meaning to log events, and it becomes easier to make the events actionable
    within Fluentd or downstream solutions. This means changing the type of parser
    from none to using one of the provided plugins. It is worth noting that we can
    group parsers into two general categories:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只看了最简单的日志文件；然而，很少有日志文件是这样的。它们越是有结构，我们就能更好地为日志事件赋予更多意义，并且使事件在 Fluentd
    或下游解决方案中变得更容易操作。这意味着将解析器的类型从 none 更改为使用提供的插件之一。值得注意的是，我们可以将解析器分为两大类：
- en: '*Product-specific parsers* --Some products are so heavily used that parsers
    have been produced specifically for them, rather than using a generic parser with
    detailed configuration. Apache and Nginx are examples of this. The benefit of
    these is a typically more straightforward configuration, and performance is higher
    as code is optimized to process just that specific log structure.'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*产品特定解析器* --有些产品被广泛使用，以至于为它们专门开发了解析器，而不是使用具有详细配置的通用解析器。Apache 和 Nginx 就是这样的例子。这些解析器的优点是通常配置更简单，性能更高，因为代码被优化以处理特定的日志结构。'
- en: '*Generic parsers*—These can be grouped by either'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通用解析器*——这些可以根据以下任一进行分组'
- en: Supporting a specific type of file notation (e.g., CSV, LTSV, discussed shortly)
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支持特定类型的文件表示法（例如，CSV，LTSV，将在稍后讨论）
- en: Using highly configurable parser technology (e.g., Grok, Regex)
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高度可配置的解析器技术（例如，Grok，正则表达式）
- en: These parsers are highly configurable, but at the same time, they are a lot
    more complex to configure. The more configurable and flexible the parser is, the
    less efficient the parsing of each event.
  id: totrans-186
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这些解析器高度可配置，但同时也更加复杂。解析器的可配置性和灵活性越高，对每个事件的解析效率就越低。
- en: Out of the box, there is a range of parsers coving these categories. In addition
    to the pass-through parser (referred to as *none*), other parsers include CSV
    and JSON, along with specific parsers for web server monitor files. These are
    complemented with the other community (open source) provided parsers. Section
    3.4.1 describes the core parsers, how they work, and when they can help. Section
    3.4.2 continues with a couple of community-provided parsers also worth knowing
    about; these do not reflect the sum totality of all the possible parsers but are
    the ones we believe are worth knowing about.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，有一系列解析器覆盖这些类别。除了透传解析器（称为 *none*）之外，其他解析器包括 CSV 和 JSON，以及针对网络服务器监控文件的特定解析器。这些与社区（开源）提供的其他解析器相辅相成。第
    3.4.1 节描述了核心解析器，它们的工作原理以及它们何时可以提供帮助。第 3.4.2 节继续介绍一些值得了解的社区提供的解析器；这些并不反映所有可能的解析器的总和，但我们认为这些是值得了解的。
- en: Having reviewed the different options, we will apply one of the most commonly
    used parsers--Regex--to log events.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在审查了不同的选项后，我们将应用最常用的解析器之一——正则表达式——来处理日志事件。
- en: 3.4.1 Standard parsers
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 标准解析器
- en: apache2
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: apache2
- en: '*Apache* and *Nginx* are the two most dominant web servers in production today,
    with Apache having been available since the mid-1990s. In the process of applying
    monitoring within an enterprise, there is a good chance you’ll encounter an Apache
    server, even if it is wrapped up as part of a larger product. This parser and
    the Nginx parser are designed to take standard web server logs that record the
    requests and responses that are processed. Recording details the following:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: '*Apache* 和 *Nginx* 是目前生产中最主要的两个网络服务器，Apache 自1990年代中期以来一直可用。在企业内部应用监控的过程中，你很可能遇到
    Apache 服务器，即使它被包含在更大的产品中。这个解析器和 Nginx 解析器被设计用来处理记录请求和响应的标准网络服务器日志。记录的细节包括以下内容：'
- en: Host
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主机
- en: User
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户
- en: Method (HTTP POST, GET, etc.)
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方法（HTTP POST，GET 等）
- en: URI
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URI
- en: HTTP request and response codes
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: HTTP 请求和响应代码
- en: Payload size
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载大小
- en: Referrer
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 引用者
- en: Agent
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 代理
- en: apache_error
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: apache_error
- en: In addition to the core Apache log files, we also need to capture separate errors
    and associated diagnostic and debug information for CGI scripts, and so on. The
    information captured covers
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心的 Apache 日志文件外，我们还需要捕获与 CGI 脚本等相关的单独错误以及诊断和调试信息。捕获的信息包括
- en: Level (e.g., warning, error)
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 级别（例如，警告，错误）
- en: 'PID: process identifier'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PID：进程标识符
- en: Client associated with the error (e.g., browser, application)
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与错误关联的客户端（例如，浏览器，应用程序）
- en: Error message
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误信息
- en: Nginx
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Nginx
- en: This parser handles standard Nginx access logs that capture the HTTP calls received.
    At the heart of the plugin, it applies a regular expression to capture the message
    elements. This means that modifying the Nginx configuration would necessitate
    changes to this parser for a Regex and adapt the standard Regex accordingly. The
    attributes captured are
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 此解析器处理捕获HTTP调用的标准Nginx访问日志。在插件的核心中，它通过应用正则表达式来捕获消息元素。这意味着修改Nginx配置将需要更改此解析器以进行正则表达式处理，并相应地调整标准正则表达式。捕获的属性包括
- en: '*Remote*—Remote address'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*远程地址*—远程地址'
- en: '*User*—Remote user'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户*—远程用户'
- en: '*Method*—HTTP verb post, get, etc.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*方法*—HTTP动词post、get等。'
- en: '*Path*—URL being used that Nginx is handling'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*路径*—Nginx正在处理的URL'
- en: '*Code*—HTTP code'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代码*—HTTP状态码'
- en: '*Size*—Buffer size'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*大小*—缓冲区大小'
- en: '*Referrer*—Provided identity of the referrer when a call is referred'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*引用者*—在调用被转接时提供的引用者身份'
- en: '*Agent*—Usually the browser type'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*代理*—通常是浏览器类型'
- en: '*Http_x_forwarded_for*—If forwarding has occurred, the HTTP header information
    recording the forwarding steps is held by this element.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Http_x_forwarded_for*—如果发生了转发，记录转发步骤的HTTP头信息由该元素持有。'
- en: NOTE More information about the Nginx logging can be obtained from [http://mng.bz/1joX](http://mng.bz/1joX)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：有关Nginx日志的更多信息，可以从[http://mng.bz/1joX](http://mng.bz/1joX)获取
- en: CSV
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: CSV
- en: The CSV parser is a rapid parser using, by default, a comma to delimit each
    field. The parser by default works by using Ruby’s own CSV processor (see [http://mng.bz/J12o](http://mng.bz/J12o)).
    Alternatively, an optimized quick parser (which is restricted to recognizing the
    use of quotes to allow the delimiter to be used as usual and multiple quotes as
    an escape pattern [e.g., """]) can be used by setting the attribute `parser_type`
    to `fast`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: CSV解析器是一个快速解析器，默认使用逗号作为字段分隔符。解析器默认通过使用Ruby自己的CSV处理器（见[http://mng.bz/J12o](http://mng.bz/J12o)）来工作。或者，可以通过设置`parser_type`属性为`fast`来使用一个优化的快速解析器（该解析器限制为识别引号的使用，以便分隔符可以像平常一样使用，并且多个引号作为转义模式[例如，"""]）。
- en: An optimized string parser to apply meaning to a CSV string will always be better
    than trying to apply meaning by performing our string processing or using a Regex
    parser.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 一个优化的字符串解析器，用于将意义应用于CSV字符串，将始终优于通过执行我们的字符串处理或使用正则表达式解析器来应用意义。
- en: The `keys` attribute then takes a list of field names. The `time_key` attribute
    identifies which of the keys to use as the timestamp for the log event. The delimiter
    can be changed from a comma to something else using the delimiter attribute.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，`keys`属性接受一个字段名称列表。`time_key`属性标识了哪些键用作日志事件的时间戳。分隔符可以通过分隔符属性从逗号更改为其他内容。
- en: 'The following illustrates a CSV parser using the optimized option, rather than
    the default:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了使用优化选项的CSV解析器，而不是默认选项：
- en: '[PRE25]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: When applied to a log entry, `"my quoted message, to you", 124, 2020/04/15 16:59:04,
    192.168.0.1` would produce an internal representation of
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 当应用于日志条目时，`"my quoted message, to you", 124, 2020/04/15 16:59:04, 192.168.0.1`将产生一个内部表示为
- en: '[PRE26]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: JSON
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: JSON
- en: This treats the received log event as a JSON payload. There is a trend of treating
    log events as JSON structures to aid in meaning without creating considerable
    overheads in the log file size. This trend is even reflected in Fluentd; as you
    may remember, Fluentd treats log events as JSON objects.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这将接收到的日志事件作为JSON有效载荷处理。将日志事件作为JSON结构处理以帮助理解，而不在日志文件大小上产生大量开销的趋势正在出现。这种趋势甚至在Fluentd中也有所体现；如您所记得，Fluentd将日志事件视为JSON对象。
- en: 'It will look for a root element called `time` to apply as a log event timestamp.
    The tag associated with the log event can be found in the root element as the
    `tag`. Nested JSON structures are not, by default, processed. The parser could
    easily process the following structure, although additional attention would be
    needed to enable `nested1` to be processed as JSON in the following JSON fragment:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 它将寻找一个名为`time`的根元素，将其作为日志事件的时间戳应用。与日志事件关联的标签可以在根元素中找到，作为`tag`。默认情况下，嵌套的JSON结构不会被处理。解析器可以轻松处理以下结构，尽管需要额外的注意来使`nested1`在以下JSON片段中作为JSON处理：
- en: '[PRE27]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: If desired, it is possible to change the JSON parser implementation for an alternate
    Ruby implementation. However, the default parser has shown up across multiple
    benchmarks as being the most performant.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，可以更改JSON解析器的实现以使用替代的Ruby实现。然而，默认解析器在多个基准测试中显示出是最高性能的。
- en: TSV
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: TSV
- en: This is very similar to the CSV parser. The key difference is there is no support
    for escaping and quoting values. It assumes a default delimiter as a tab character
    (or the escape sequence `\t`). As with the CSV parser, the TSV parser can have
    the delimiter changed (*delimiter* attribute). It also expects attributes of `keys`
    and `time_key` to define the JSON mapping and timestamp. It does offer one additional
    optional attribute called `null_value_pattern,` which, if set, will result in
    any value found to contain that value to replace the value in the JSON with an
    empty string. For example, `null_value_pattern` '-' would mean that a line like
    `afield\t123\t-\totherField` would result in
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 这与 CSV 解析器非常相似。关键区别在于不支持转义和引号值。它假设默认分隔符为制表符字符（或转义序列 `\t`）。与 CSV 解析器一样，TSV 解析器可以更改分隔符（*分隔符*
    属性）。它还期望 `keys` 和 `time_key` 属性定义 JSON 映射和时间戳。它还提供了一个额外的可选属性，称为 `null_value_pattern`，如果设置，则将找到的任何包含该值的值替换为
    JSON 中的空字符串。例如，`null_value_pattern` '-' 意味着类似 `afield\t123\t-\totherField` 的行将导致
- en: '[PRE28]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: LTSV
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: LTSV
- en: Label tab separated value (LTSV) is a variation on the tab-separated value.
    The key difference is that each tab-separated value is prefixed by a label and
    a label-delimiting character in the form of a colon. This means the values have
    semantic meaning, and value order is not important. As a result, there is no need
    for masses of comma separators for empty values, as you can see in CSV files.
    It can be argued that this is more efficient and tolerant than JSON in terms of
    logging data (e.g., no additional quotes, characters, braces), and only three
    characters are reserved--tab, label delimiter, and new line. Additional characters
    are not legal in JSON. For example, `hostname:localhost/tip:127.0.0.1` (note tab
    is represented by `/t` in the middle of the example) has two labels--`hostname`
    and `ip`. LTSV is documented in detail at [http://ltsv.org/](http://ltsv.org/)
    and includes links to helpful tools.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 标签制表符分隔值（LTSV）是制表符分隔值的一种变体。关键区别在于，每个制表符分隔值都由一个标签和一个冒号形式的标签分隔符前缀。这意味着值具有语义意义，值顺序并不重要。因此，不需要大量逗号分隔符来表示空值，正如你在
    CSV 文件中看到的那样。可以说，在日志数据方面（例如，没有额外的引号、字符、花括号），这比 JSON 更高效和宽容，并且只预留了三个字符——制表符、标签分隔符和换行符。JSON
    中不允许使用其他字符。例如，`hostname:localhost/tip:127.0.0.1`（注意示例中间的制表符用 `/t` 表示）有两个标签——`hostname`
    和 `ip`。LTSV 的详细文档可以在 [http://ltsv.org/](http://ltsv.org/) 找到，并包括指向有用工具的链接。
- en: As with the TSV parser, we can modify the delimiter so characters other than
    the tab can be used. In addition, the attribute `label_delimiter` can be used
    to change the default (colon) label delimiter.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 与 TSV 解析器一样，我们可以修改分隔符，以便可以使用除了制表符之外的字符。此外，可以使用 `label_delimiter` 属性来更改默认（冒号）标签分隔符。
- en: MSGPACK
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: MSGPACK
- en: MessagePack is an open source standard and library that describes inline the
    payload allowing content to be removed or shortened. The format is supported in
    several parts of Fluentd (which is unsurprising, considering it was developed
    by the same team responsible for Fluentd).
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: MessagePack 是一个开源标准和库，它内联描述了有效载荷，允许内容被移除或缩短。该格式在 Fluentd 的多个部分中得到支持（考虑到它是由负责
    Fluentd 的同一团队开发的，这并不令人惊讶）。
- en: It works by providing a short field and value descriptors. As a result, compression
    can be achieved by stripping out redundant characters such as quotes, whitespace,
    and so on.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 它通过提供简短的字段和值描述符来实现。因此，可以通过删除冗余字符（如引号、空白等）来实现压缩。
- en: 'This format can be used for communication between Fluentd and Fluent Bit nodes
    and is worth using when crossing networks, particularly with additional dynamic
    HTTP-based compression (more at [https://msgpack.org/](https://msgpack.org/) and
    [www.websiteoptimization.com/speed/tweak/compress/](https://www.websiteoptimization.com/speed/tweak/compress/)).
    For example, the JSON fragment `{"Fluentd": 1, "msgPackSupport":true}` would be
    reduced to the hex'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '此格式可用于 Fluentd 和 Fluent Bit 节点之间的通信，在跨越网络时尤其值得使用，尤其是在使用基于动态 HTTP 的压缩时（更多信息请参阅
    [https://msgpack.org/](https://msgpack.org/) 和 [www.websiteoptimization.com/speed/tweak/compress/](https://www.websiteoptimization.com/speed/tweak/compress/))。例如，JSON
    片段 `{"Fluentd": 1, "msgPackSupport":true}` 将被缩减为十六进制'
- en: '[PRE29]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: which is 26 bytes and a compression of 68%.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 这占用了 26 字节，压缩率为 68%。
- en: Given this efficiency, it is worth considering using msgpack when you’re sharing
    log events that cross wide area networks, cloud providers, and so on, as those
    networks will cost more based on data volume and can be subject to bandwidth and
    latency issues.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到这种效率，当你在跨广域网、云提供商等共享日志事件时，考虑使用msgpack是值得的，因为这些网络将根据数据量收费，并且可能受到带宽和延迟问题的影响。
- en: Multiline
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 多行
- en: Unfortunately, not all logs are elegant such that a single line represents a
    single event, as is the case when handling stack traces and stack dumps. The *multiline*
    plugin addresses this by defining multiple regular expressions (Regex). The regex
    expressions allow the log line to be parsed and tease out the log elements wanted.
    For this to work, the plugin requires a Regex to identify the first line of a
    multiline log event (and, by implication, the end of this log entry). This Regex
    is specified with an attribute-name of `format_firstline`. After this attribute,
    up to 20 additional regex formats can be defined in numerical sequence.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，并非所有日志都像处理堆栈跟踪和堆栈转储时那样优雅，即单行代表一个事件。*多行*插件通过定义多个正则表达式（Regex）来解决这个问题。正则表达式允许解析日志行并提取所需的日志元素。为了使插件正常工作，它需要一个正则表达式来识别多行日志事件的起始行（以及由此推断的日志条目的结束）。这个正则表达式使用属性名`format_firstline`指定。在此属性之后，可以定义多达20个额外的正则表达式格式，按数字顺序排列。
- en: See the following for how the Regex works, but the configuration follows the
    pattern
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 有关正则表达式的工作方式，请参阅以下内容，但配置遵循模式
- en: '[PRE30]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The multiline plugin as a parser is currently only available with the *tail*
    input plugin because of the unique interactions between the two plugins.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个插件之间的独特交互，多行插件作为解析器目前仅与*tail*输入插件一起提供。
- en: If the application’s logging framework can be configured to not produce multiline
    output (e.g., Log4J 2 can support this in its pattern configuration), then it
    is worth at least considering. This is because the multiline parser isn’t as efficient
    as most single-line parsers. Another strategy to avoid the multiline parser is
    for log events like stack traces to be written to separate files. Separate files
    mean that we can use the multiline parser on just a subset of all log events.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用程序的日志框架可以被配置为不产生多行输出（例如，Log4J 2可以在其模式配置中支持这一点），那么至少值得考虑。这是因为多行解析器并不像大多数单行解析器那样高效。避免使用多行解析器的另一种策略是将像堆栈跟踪这样的日志事件写入单独的文件。单独的文件意味着我们可以在所有日志事件的一个子集上使用多行解析器。
- en: None
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 无
- en: This can be used where a parser must be defined (e.g., in the tail plugin).
    But no parsing is applied, and the entire log line forms the Fluentd record. The
    time of reading is set to be the time value for the log event.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在必须定义解析器的地方使用（例如，在tail插件中）。但是不应用任何解析，整个日志行形成Fluentd记录。读取时间被设置为日志事件的值。
- en: Regex
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式
- en: Regex parser is probably the most powerful parser option available, but as a
    result, it is also the most complex. In section 3.4.3, we will drill into the
    use of Regex in more depth.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式解析器可能是可用的最强大的解析器选项，但作为结果，它也是最复杂的。在第3.4.3节中，我们将更深入地探讨正则表达式的使用。
- en: Syslog
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 系统日志
- en: Syslogs are most commonly generated by OS and infrastructure processes, but
    nothing prevents applications from using the format. The original unofficial structure
    of a syslog entry was formalized by the IETF as *RFC 3164* ([https://tools.ietf.org/html/rfc3164](https://tools.ietf.org/html/rfc3164)).
    This was then superseded by *RFC 5424* ([https://tools.ietf.org/html/rfc5424](https://tools.ietf.org/html/rfc5424))
    in 2009\. As some hardware can take many years before being replaced, Fluentd
    can handle both standards. By default, Fluentd will assume the original standard,
    but you can tell Fluentd to use the later standard, or use the payload to work
    it by setting the `message_format` attribute with one (*rfc3164*, *rfc5424*, *auto*).
    If you know which format will be handled, it is better to explicitly define it
    in the configuration. Doing so removes the overhead of having to evaluate each
    event before parsing. If a single endpoint is consuming both event types, you
    may have to accept the overhead.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 系统日志通常由操作系统和基础设施进程生成，但没有任何东西阻止应用程序使用该格式。syslog条目的原始非官方结构被IETF正式化为*RFC 3164*
    ([https://tools.ietf.org/html/rfc3164](https://tools.ietf.org/html/rfc3164))。然后，在2009年被*RFC
    5424* ([https://tools.ietf.org/html/rfc5424](https://tools.ietf.org/html/rfc5424))所取代。由于一些硬件可能需要多年才能更换，Fluentd可以处理这两个标准。默认情况下，Fluentd将假设原始标准，但您可以告诉Fluentd使用较新的标准，或者通过设置`message_format`属性为之一（*rfc3164*，*rfc5424*，*auto*）来使用负载来处理它。如果您知道将处理哪种格式，最好在配置中明确定义它。这样做可以消除在解析之前评估每个事件的负担。如果单个端点正在消耗这两种事件类型，您可能必须接受这种开销。
- en: The plugin has two different algorithms for processing the events--string processing
    logic and a regular expression (`regexp`). Currently, the default is *regex*,
    but in the future, this will be changed to default to the string option, which
    is faster than the `regexp` algorithm. If you wish to force the algorithm, the
    attribute `parser_type` needs to be set with either *regexp* or string.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 该插件有两个不同的算法用于处理事件--字符串处理逻辑和正则表达式（`regexp`）。目前，默认是*regex*，但将来这将被更改为默认使用字符串选项，这比`regexp`算法更快。如果您想强制使用算法，需要使用`parser_type`属性设置为*regexp*或字符串。
- en: 3.4.2 Third-party parsers
  id: totrans-257
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 第三方解析器
- en: In addition to the core parsers, there are some third-party parsers. The following
    ones are only a subset of the possible options. However, these are either certified
    or have been heavily downloaded, so they are likely to have benefited from extensive
    use and, given that the code is open source, benefited from Linus’s Law. Given
    enough eyeballs, all bugs are shallow--Eric Raymond).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 除了核心解析器之外，还有一些第三方解析器。以下只是可能选项的一部分。然而，这些要么经过认证，要么下载量很大，因此它们很可能已经从广泛的使用中受益，并且由于代码是开源的，因此也受益于林纳斯定律。只要有足够的眼睛，所有错误都是浅显的--埃里克·雷蒙德）。
- en: Multiformat parser plugin for Fluentd
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd的多格式解析器插件
- en: This attempts to use different format patterns in the defined order to get a
    match. This is available from [http://mng.bz/wnoO](http://mng.bz/wnoO).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 这尝试按照定义的顺序使用不同的格式模式以获得匹配。这可以从[http://mng.bz/wnoO](http://mng.bz/wnoO)获取。
- en: Grok parser for Fluentd
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd的Grok解析器
- en: This uses a Grok-based approach to pull details from a log entry. It includes
    multiline support. This is available from [https://github.com/fluent/fluent-plugin-grok-parser](https://github.com/fluent/fluent-plugin-grok-parser).
    The benefit of using the Grok parser is that Grok is used by Logstash as a filtering
    and parsing mechanism; therefore, it is a relatively small step to leverage Grok
    predefined patterns and switch between using Logstash and Fluentd.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这使用基于Grok的方法从日志条目中提取详细信息。它包括多行支持。这可以从[https://github.com/fluent/fluent-plugin-grok-parser](https://github.com/fluent/fluent-plugin-grok-parser)获取。使用Grok解析器的优点是Grok被Logstash用作过滤和解析机制；因此，利用Grok预定义的模式并在Logstash和Fluentd之间切换相对容易。
- en: 3.4.3 Applying a Regex parser to a complex log
  id: totrans-263
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 将正则表达式解析器应用于复杂日志
- en: As previously noted, the Regex or regular expression parser is probably the
    most powerful and the hardest to use. In most applications of Regex, the outcome
    is normally a single result, a substring, or a count of occurrences of a string.
    However, when it comes to Fluentd, we need the regex to produce multiple values
    back, such as setting the log event time, breaking down the payload to the first
    level of elements in the JSON event body. Let’s take a Regex expression and break
    it down to highlight the basics. But before we can do that, we need to work with
    a realistic log entry. Let’s run up the log simulator configuration for this,
    using the command
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，正则表达式或常规表达式解析器可能是最强大但最难使用的。在大多数正则表达式的应用中，结果通常是单个结果、子字符串或字符串出现次数的计数。然而，当涉及到Fluentd时，我们需要正则表达式产生多个值返回，例如设置日志事件时间，将有效负载分解为JSON事件体中元素的第一个级别。让我们取一个正则表达式并将其分解以突出基本要点。但在我们能够这样做之前，我们需要与一个实际的日志条目一起工作。让我们运行日志模拟器配置，使用以下命令
- en: '[PRE31]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Note that this is a slightly different configuration from the last example,
    so we can see a couple of possibilities within Fluentd. Looking at the output
    file generated (still `structuredrolling-log.0.log`), the payload that is being
    sent now appears as
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这与上一个例子略有不同，因此我们可以在Fluentd中看到几个可能性。查看生成的输出文件（仍然是`structuredrolling-log.0.log`），现在发送的有效负载看起来如下
- en: '[PRE32]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In this output line, we can see the applied timestamp, with to-the-second accuracy,
    then a space followed by the log level, more space, and then a package name, followed
    by the numbering scheme, as previously explained. Finally, the core log entry
    is wrapped as a JSON structure. When we parse the message, we need to strip away
    that JSON notation, as it should not be in the JSON structure we want.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一输出行中，我们可以看到应用的精确到秒的时间戳，然后是一个空格，接着是日志级别，更多空格，然后是包名，后面跟着编号方案，如前所述。最后，核心日志条目被包装为一个JSON结构。当我们解析消息时，我们需要移除该JSON符号，因为它不应该出现在我们想要的JSON结构中。
- en: 'The goal is to end up with a structure as follows in Fluentd, so we can then
    use the values for future manipulation:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是在Fluentd中结束结构如下，这样我们就可以使用这些值进行未来的操作：
- en: '[PRE33]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Here is the regular expression, with character positions added below it, to
    make it easy to precisely reference each piece:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是正则表达式，下面添加了字符位置，以便精确引用每个部分：
- en: '[PRE34]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As part of the expression, we need to use Regex’s ability to define groups of
    text. The scope of a group is defined by open and closing brackets (e.g., characters
    1 and 12). To assign some source text to a JSON element, we need to use `?<name>`,
    where the name is the element name to appear in the JSON. It should be the values
    `level`, `class`, `line`, `iteration`, and `msg` in our case. In addition to this,
    we also need to capture the log event time with the default `time` value. This
    can be seen between characters 2 and 8, for example, and between 16 and 23\. Immediately
    after this, we can use the Regex notation to describe the text to be captured.
    For this, we use `\S` (characters 9 and 10), which means a non-whitespace character;
    by adding a `+` (character 11), we are declaring that this should happen one or
    more times. We will need to provide the parser with additional configuration,
    as we will need to declare how to break this part of the message into a specific
    time.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 作为表达式的一部分，我们需要使用正则表达式定义文本组的能力。组的范围由开括号和闭括号定义（例如，字符1和12）。为了将一些源文本分配给JSON元素，我们需要使用`?<name>`，其中name是将在JSON中出现的元素名称。在我们的情况下，这些值应该是`level`、`class`、`line`、`iteration`和`msg`。此外，我们还需要使用默认的`time`值捕获日志事件时间。例如，它可以在字符2和8之间看到，也可以在16和23之间看到。紧接着，我们可以使用正则表达式符号来描述要捕获的文本。为此，我们使用`\S`（字符9和10），这意味着非空白字符；通过添加一个`+`（字符11），我们声明这应该发生一次或多次。我们需要向解析器提供额外的配置，因为我们需要声明如何将消息的这一部分分解为特定的时间。
- en: The first character not to match the pattern is the space between the time and
    the log level--so we use the Regex representation for a single space (characters
    13 and 14) and then start the group for the log level.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 不符合模式的第一字符是时间和日志级别之间的空格--因此我们使用正则表达式表示单个空格（字符13和14），然后开始日志级别的组。
- en: The expression defines the log level, which we know will be formed by one or
    more alphabetic uppercase characters. The use of the square brackets denotes a
    choice of values (characters 24 and 28). We could list all the possible characters
    within the brackets, but for readability, we’ve opted to indicate between capital
    `A` and capital `Z`; the hyphen between `A` and `Z` (character 27) denotes that
    this is a range. As the log level word will be multiple characters, we use the
    asterisk to note multiple. That completes the log level. So, outside the group,
    we need to denote the multiple whitespace characters--that is, `\s*` (starting
    at 31).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 表达式定义了日志级别，我们知道它将由一个或多个大写字母组成。方括号的使用表示值的选项（字符24和28）。我们可以列出方括号内所有可能的字符，但为了可读性，我们选择在`A`和`Z`大写字母之间表示；`A`和`Z`之间的连字符（字符27）表示这是一个范围。由于日志级别单词将是多个字符，我们使用星号来表示多个。这样就完成了日志级别。因此，在组外部，我们需要表示多个空白字符--即`\s*`（从31开始）。
- en: We can follow the same basic pattern used for the date and time for a path or
    class string. This can be seen between characters 34 and 46\. To pass along the
    line to the following meaningful characters, we have defined a range with the
    Regex expression `\d` (characters 49 and 50), which means a numeric digit. However,
    by adding the circumflex (`^`), we negate the following value--in this case, any
    non-numeric character. This means that we will skip over the whitespace and the
    opening bracket.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以遵循用于日期和时间的相同基本模式来处理路径或类字符串。这可以在字符34和46之间看到。为了将行传递到后续的有意义字符，我们定义了一个范围，使用正则表达式`\d`（字符49和50），这意味着一个数字。然而，通过添加圆点(`^`)，我们否定后续的值--在这种情况下，任何非数字字符。这意味着我们将跳过空白和开括号。
- en: The groups for the line and iteration are the same--multiple digits required
    with a hyphen between the two groups. The hyphen is escaped (character 68) because
    it has meaning to Regex. We can see the same character escape for the curly bracket
    at character 95.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 线和迭代的组是相同的--需要多个数字，并且两个组之间用连字符分隔。连字符被转义（字符68），因为它在正则表达式中有特殊意义。我们可以看到在字符95处的花括号也有相同的转义字符。
- en: After the curly bracket starts the log detail, we know what the text is, so
    we can put it into the expression (starting at character 97). This underlines
    the importance of being exact in the expression, as non-escaped code characters
    will get treated as literals and therefore will be expected to be found where
    they occur in the Regex. If the literal character is not found as expected by
    the Regex, then the string being processed will be rejected.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在花括号开始日志细节之后，我们知道文本是什么，因此我们可以将其放入表达式（从字符97开始）。这强调了在表达式中精确性的重要性，因为未转义的代码字符将被视为字面量，因此将期望在正则表达式中找到它们出现的位置。如果正则表达式没有按预期找到字面量字符，那么正在处理的字符串将被拒绝。
- en: The next new Regex trick is the use of the dot (character 112). This denotes
    any character; when combined with a following asterisk, then the expression becomes
    multiple occurrences of any character. This makes for an interesting challenge--how
    do we stop the closing quotes and bracket from being consumed into the msg group?
    This is by using a subgroup definition `containing ?=` (characters 115 and 116).
    This describes a look ahead for the following sequence when you find that sequence
    and then stop allocating text to the current group. As a result, the match expression
    is `"}`, but as a curly bracket has Regex meaning, we have to escape it with another
    slash. This does mean that any characters after this will be ignored.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个新正则表达式技巧是使用点（字符112）。这表示任何字符；当与后面的星号结合时，表达式就变成了任何字符的多次出现。这构成了一个有趣的挑战--我们如何阻止关闭引号和括号被消费到`msg`组中？这是通过使用子组定义`containing
    ?=`（字符115和116）来实现的。这描述了当你找到该序列时，向前查看后续序列，然后停止分配文本到当前组。因此，匹配表达式是`"}",但由于花括号在正则表达式中具有特殊意义，我们必须使用另一个反斜杠来转义它。这意味着此之后的任何字符都将被忽略。
- en: In appendix B, we have included details of the Regex expressions, so you have
    a quick reference for building your expressions. Please note that if you research
    Regex elsewhere, while there is a high level of commonality in implementations,
    you will find subtle differences. Keep in mind that the Regex here is implemented
    using the Ruby language.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 在附录B中，我们包含了正则表达式的详细信息，以便您快速参考构建您的表达式。请注意，如果您在其他地方研究正则表达式，虽然实现上高度相似，但您会发现一些细微的差异。请记住，这里的正则表达式是使用Ruby语言实现的。
- en: To complete the parser configuration, we need to tell the parser which named
    grouping represents the date and time (often shortened to *date-time* or *date-time-group*
    [DTG]) and how that date-time is represented. In our case, the date and time can
    be expressed using the pattern of `%Y-%m-%d--%T`. As the time element is standard,
    we can use one of the short circuit formats (`%T`) described in appendix B. Finally,
    let’s piece all this information together and define the parser attributes (listing
    3.6).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成解析器配置，我们需要告诉解析器哪个命名的分组代表日期和时间（通常简称为 *date-time* 或 *date-time-group* [DTG]）以及该日期和时间是如何表示的。在我们的例子中，日期和时间可以使用
    `%Y-%m-%d--%T` 的模式来表示。由于时间元素是标准的，我们可以使用附录 B 中描述的短路格式（`%T`）。最后，让我们将这些信息拼凑在一起，并定义解析器属性（列表
    3.6）。
- en: Regular expression processing is a rich and complex capability
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式处理是一个丰富且复杂的特性
- en: 'There are entire books devoted to the subject and many more with dedicated
    chapters. Here we have only scratched the surface, giving just enough to help
    you understand how it works within the Fluentd context. It might be worthwhile
    investing in a book to help. This link may also help: [www.rubyguides.com/2015/06/ruby-regex/](https://www.rubyguides.com/2015/06/ruby-regex/).'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 有关这个主题的书籍有很多，还有更多有专门章节的书籍。在这里，我们只是触及了表面，只提供了足够的信息来帮助你理解它在 Fluentd 上下文中的工作方式。投资一本书来帮助可能会有所帮助。此链接也可能有所帮助：[www.rubyguides.com/2015/06/ruby-regex/](https://www.rubyguides.com/2015/06/ruby-regex/)。
- en: NOTE When the parser expression fails during processing, Fluentd will generate
    a warning log entry.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当解析器表达式在处理过程中失败时，Fluentd 将生成一个警告日志条目。
- en: Listing 3.6 Chapter3/Fluentd/rotating-file-read-regex.conf--parse extract
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 Chapter3/Fluentd/rotating-file-read-regex.conf--parse extract
- en: '[PRE35]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ We have changed the parser type here to regexp for regular expressions.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在这里将解析器类型更改为正则表达式。
- en: ❷ The expression needs to be provided between forward slashes. It is possible
    to add extra controls after the trailing slash, as we will see.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 表达式需要在正斜杠之间提供。我们将在后面看到，可以在尾随斜杠之后添加额外的控制。
- en: ❸ time_format allows us to define the date-time format more concisely.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ time_format 允许我们更简洁地定义日期时间格式。
- en: ❹ time_key is used to tell Fluentd which extracted value to use for the timestamp.
    By default, it will use a value called time, so technically this is redundant.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ time_key 用于告诉 Fluentd 使用哪个提取值作为时间戳。默认情况下，它将使用名为 time 的值，所以从技术上讲这是多余的。
- en: This configuration can be run by restarting the simulator, as we did to get
    an example value, and then starting Fluentd with the configuration
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这个配置可以通过重新启动模拟器来运行，就像我们为了获取示例值所做的那样，然后使用配置启动 Fluentd
- en: '[PRE36]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As Fluentd outputs the processed stream of events to console, you will see
    entries like this:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Fluentd 将处理后的事件流输出到控制台，你会看到如下条目：
- en: '[PRE37]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Each line printed by Fluentd to the console will take the date and time of the
    log event, the time zone offset, the event tag, and the payload as a correctly
    structured JSON, with the time omitted. Notice how the nanoseconds are all 0\.
    This is because we have given Fluentd a log time that does not have nanosecond
    precision; therefore, that part of the timestamp is left at 0.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Fluentd 打印到控制台的每一行都将包含日志事件的日期和时间、时区偏移、事件标签和有效载荷，以正确结构的 JSON 格式，省略了时间。注意，所有的纳秒值都是
    0。这是因为我们给 Fluentd 提供了一个没有纳秒精度的日志时间；因此，时间戳的这一部分被留在了 0。
- en: More importantly, you will note that all the JSON values are quoted, so they
    will be treated as strings. This may not be an issue. But having come this far,
    it would be a shame not to define the data types correctly. It may well enable
    downstream activities, such as extrapolating additional meaning, to be more effective.
    The defining of nonstring data types is straightforward. We need to add the attribute
    types within the parser construct, which takes a comma-separated list with each
    defined value described in the format *name:type*. In our use case, we would want
    to add `types line:integer,iteration:integer`. The complete list of types supported
    are
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 更重要的是，你会注意到所有的 JSON 值都被引号包围，因此它们将被视为字符串。这可能不是问题。但既然已经走到这一步，不正确地定义数据类型就太遗憾了。这可能会使下游活动，如推断额外含义，更加有效。定义非字符串数据类型很简单。我们需要在解析器结构中添加属性类型，它包含一个以逗号分隔的列表，每个定义的值都按照
    *name:type* 的格式描述。在我们的用例中，我们希望添加 `types line:integer,iteration:integer`。支持的类型列表如下
- en: '*string*—Can be defined explicitly, but is the default type'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*string*—可以显式定义，但默认类型'
- en: '*bool*—Boolean'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*bool*—布尔值'
- en: '*integer*—Representation for any whole number (i.e., no decimal places)'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*integer*—表示任何整数（即，没有小数位）'
- en: '*float*—Represents any decimal number'
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*浮点数*—表示任何十进制数'
- en: '*time*—Converts the value into the way that Fluentd represents time internally.
    We can extend this to describe how the time should translate. For example:'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*时间*—将值转换为 Fluentd 内部表示时间的方式。我们可以扩展此描述时间应该如何转换。例如：'
- en: '*date:time:%d/%b/%Y:%H:%M*—*Defining the formatting of the representation*'
  id: totrans-302
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期：时间：%d/%b/%Y:%H:%M*—*定义表示的格式*'
- en: '*date:time:unixtime*—*Timer from 1 Jan 1970 in integer format*'
  id: totrans-303
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期：时间：Unix时间*—*从 1970 年 1 月 1 日起的整数格式时间*'
- en: '*date:time:float*—*The same epoch point, but the number is as float*'
  id: totrans-304
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*日期：时间：浮点数*—*相同的纪元点，但数字是浮点数*'
- en: '*array*—A sequence of values of the same type (e.g., all strings, all integers)'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数组*—相同类型值的序列（例如，所有字符串，所有整数）'
- en: The handling of the array requires the values to have a delimiter to separate
    each value. The delimiter by default is a comma, but it can be changed by adding
    a colon and the delimiter character. For example, a comma-delimited array could
    be defined as `myList:array`. But if I wanted to replace the delimiter with a
    hash, then the expression would be `myList:array:#`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数组需要值之间有一个分隔符来分隔每个值。默认的分隔符是逗号，但可以通过添加一个冒号和分隔符字符来更改。例如，逗号分隔的数组可以定义为 `myList:array`。但如果我想用哈希替换分隔符，那么表达式将是
    `myList:array:#`。
- en: The last manipulation of the JSON involves whether we would like the date timestamp
    to be included in the JSON; after all, it was in the body of the log event. This
    can easily be done by adding `keep_time_key true` to the parser attributes.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 最后对 JSON 的操作涉及我们是否希望将日期时间戳包含在 JSON 中；毕竟，它已经在日志事件的主体中了。这可以通过向解析器属性中添加 `keep_time_key
    true` 来轻松完成。
- en: 'We can add the changes described (although the provided configuration has these
    values ready but commented out, so you could just uncomment them and rerun the
    simulator and Fluentd as before). As a result of these changes, the log entries
    will appear like this:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以添加所描述的更改（尽管提供的配置已经准备好了这些值但被注释掉了，所以您只需取消注释它们并像以前一样重新运行模拟器和 Fluentd）。这些更改的结果是日志条目将如下所示：
- en: '[PRE38]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: If you look at the JSON body now, our numeric elements are no longer in quotes,
    and the timestamp appears in the JSON payload.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如果现在查看 JSON 主体，我们的数值元素不再带引号，时间戳出现在 JSON 负载中。
- en: Evaluating/checking Regex expressions
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 评估/检查正则表达式
- en: Regex expressions can be challenging; the last thing we want to have to do is
    run logs throughout the Fluentd environment to determine whether the expression
    is complete or not. To this end, Fluentd UI configuration for tail supports Regex
    validation.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式表达式可能具有挑战性；我们最不希望做的就是在整个 Fluentd 环境中运行日志来确定表达式是否完整。为此，Fluentd UI 配置支持 Regex
    验证。
- en: In addition, there is a free web tool called Fluentular ([https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/))
    that will allow you to develop and test expressions.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还有一个名为 Fluentular 的免费网络工具（[https://fluentular.herokuapp.com/](https://fluentular.herokuapp.com/))，它允许您开发和测试表达式。
- en: Some IDEs, such as Microsoft’s Visual Studio Code, have Regex tools to help
    visualize the Regex being built--for example, Regexp Explain ([http://mng.bz/q2YA](http://mng.bz/q2YA)).
    The completed Regex can be seen in the following figure.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 一些 IDE，例如微软的 Visual Studio Code，具有正则表达式工具来帮助可视化正在构建的正则表达式--例如，Regexp Explain
    ([http://mng.bz/q2YA](http://mng.bz/q2YA))。完成的正则表达式可以在以下图中看到。
- en: '![](../Images/CH03_UN01_Wilkins2.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH03_UN01_Wilkins2.png)'
- en: Visualization of a regular expression (Regex) using Regexp Explain in visual
    code to help you understand how a parser should process a log event. This can
    also be done with [https://regexper.com/](https://regexper.com/).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Regexp Explain 在 Visual Code 中可视化正则表达式（Regex），以帮助您了解解析器应该如何处理日志事件。这也可以通过
    [https://regexper.com/](https://regexper.com/) 完成。
- en: If you look carefully, you will note that the `?<element name>` is missing;
    however, it is easy to see where these pieces need to be added, as the core parts
    have been grouped. If the grouping is used, it becomes easy to port the expression
    into Fluentd and add the elements.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您仔细观察，您会注意到 `?<元素名称>` 缺失；然而，由于核心部分已经被分组，因此很容易看到这些部分需要添加的位置。如果使用分组，则将表达式导入
    Fluentd 并添加元素变得容易。
- en: 3.4.4 Putting parser configuration into action
  id: totrans-318
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.4 将解析器配置投入实际应用
- en: This exercise is designed to allow you to work with the parser. The simulator
    configuration `Chapter3/SimulatorConfig/jul-log-file2-exercise.properties` has
    some differences to the previous worked example. Copy the Fluentd configuration
    file used to illustrate the parser (`Chapter3/Fluentd/rotating-file-read-regex.conf`).
    Then, modify the parser expression so that all the input values are properly represented
    as JSON elements of the log event, rather than just the payload as defaulted by
    Fluentd. A variation of the log simulator configuration can be run using the command
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 本练习旨在让您与解析器一起工作。模拟器配置 `Chapter3/SimulatorConfig/jul-log-file2-exercise.properties`
    与之前的示例有所不同。复制用于说明解析器的 Fluentd 配置文件（`Chapter3/Fluentd/rotating-file-read-regex.conf`）。然后，修改解析器表达式，以确保所有输入值都作为日志事件的
    JSON 元素正确表示，而不是像 Fluentd 默认那样仅作为有效载荷。可以使用命令运行日志模拟器配置的变体
- en: '[PRE39]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Run the revised Fluentd configuration and determine whether your changes have
    been effective.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 运行修订后的 Fluentd 配置，并确定您的更改是否有效。
- en: Answer
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 答案
- en: The parser configuration should appear like the code shown in the following
    listing.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 解析器配置应类似于以下列表中所示的代码。
- en: Listing 3.7 Chapter3/ExerciseResults/rotating-file-read-regex-Answer.conf
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.7 Chapter3/ExerciseResults/rotating-file-read-regex-Answer.conf
- en: '[PRE40]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: A complete configuration file is provided at `Chapter3/ExerciseResults/rotating-file-read-regex-Answer.conf`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 完整的配置文件在 `Chapter3/ExerciseResults/rotating-file-read-regex-Answer.conf` 中提供。
- en: Summary
  id: totrans-327
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Fluentd’s configuration can be validated using the `dry_run` option without
    needing to start a proper deployment.
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用 `dry_run` 选项验证 Fluentd 的配置，而无需启动实际的部署。
- en: Log events held in log files can be wide-ranging in format, from unstructured
    to fully structured. Fluentd’s ability to consume log events from files allows
    it to accommodate this level of diversity.
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储在日志文件中的日志事件格式可能范围很广，从非结构化到完全结构化。Fluentd 消费来自文件的日志事件的能力使其能够适应这种多样性水平。
- en: Fluentd can handle log file complexity--the processes applied to log files can
    be complex, such as handling log rotation and tracking where to resume if Fluentd
    is stopped and started.
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fluentd 可以处理日志文件复杂性——应用于日志文件的过程可能很复杂，例如处理日志轮转和跟踪 Fluentd 停止和启动后如何恢复的位置。
- en: A wide range of logging event sources can be handled as a result of Fluentd’s
    plugin model and broad community and vendor support.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于 Fluentd 的插件模型以及广泛的社区和供应商支持，可以处理广泛的日志事件源。
- en: Input plugins can use a range of out-of-the-box parsers (e.g., CSV, Regex, LTSV,
    web server standard files) to apply structure and meaning to the log event.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入插件可以使用一系列现成的解析器（例如，CSV、Regex、LTSV、Web 服务器标准文件）来为日志事件应用结构和意义。
- en: Developing configurations for regular expressions (regex) can be challenging,
    but tools exist to ease this challenge (e.g., Fluentular, designed specifically
    for Fluentd; Regexp Explain; and others).
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 开发正则表达式（regex）配置可能具有挑战性，但存在工具可以简化这一挑战（例如，专为 Fluentd 设计的 Fluentular；Regexp Explain；以及其他工具）。
- en: In addition to monitoring other systems using plugins, Fluentd can be configured
    to provide the means to be monitored via methods such as enabling and using an
    HTTP endpoint to check Fluentd.
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了使用插件监控其他系统外，Fluentd 还可以配置为通过启用和使用 HTTP 端点来检查 Fluentd 的方法提供被监控的手段。
