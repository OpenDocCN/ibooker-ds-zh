- en: 9 Measuring your platforms
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9衡量你的平台
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Learning the importance of measuring platform performance
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习衡量平台性能的重要性
- en: Implementing DORA metrics and learning the secret continuous improvement
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施DORA指标和了解持续改进的秘密
- en: Using tools and standards to collect and calculate metrics
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工具和标准来收集和计算指标
- en: In chapter 8, we covered the principles of how to build a platform that helps
    you deliver software and enables your teams to have the tools they need when needed.
    This last chapter is all about making sure that the platform is working, not only
    for application development teams, but for the entire organization. To understand
    how the platform is performing, we need to be able to measure it. There are different
    ways of taking measurements on the software we run. Still, in this chapter, we
    will focus on the DORA (DevOps Research and Assessment) metrics, which provide
    a good foundation for understanding our organization’s software delivery speed
    and how good we are at recovering from failures when they happen.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8章中，我们介绍了如何构建一个帮助您交付软件并使团队在需要时拥有所需工具的平台的原则。这一章的全部内容都是确保平台不仅对应用开发团队，而且对整个组织都是有效的。为了了解平台的表现，我们需要能够衡量它。我们对运行的软件有不同方式进行测量。然而，在本章中，我们将重点关注DORA（DevOps研究和评估）指标，这些指标为我们理解组织的软件交付速度以及我们在发生故障时恢复能力提供了良好的基础。
- en: 'This chapter is divided into two main sections:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为两个主要部分：
- en: 'What to measure: DORA metrics and high-performant teams'
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要衡量什么：DORA指标和高性能团队
- en: 'How to measure our platform initiatives:'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何衡量我们的平台举措：
- en: CloudEvents and CDEvents to the rescue
  id: totrans-9
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudEvents和CDEvents来拯救
- en: Keptn Lifecycle Toolkit
  id: totrans-10
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Keptn生命周期工具包
- en: Let’s get started by understanding what we should be measuring, and for that,
    we will need to look at the DORA metrics.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从了解我们应该衡量什么开始，为此，我们需要查看DORA指标。
- en: '9.1 What to measure: DORA metrics and high-performant teams'
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 要衡量什么：DORA指标和高性能团队
- en: 'After performing thorough research in the industry, the DevOps Research and
    Assessment (DORA) team has identified five key metrics that highlight the performance
    of software development teams delivering software. Initially, in 2020, only four
    keys were defined so that you might find references to the “DORA four keys”’ metrics.
    After surveying hundreds of teams, DORA discovered which indicators and metrics
    separated high-performant/elite teams from the rest, and the numbers were quite
    shocking. DORA used the following four keys to rank teams and their practices:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在对行业进行彻底研究后，DevOps研究和评估（DORA）团队确定了五个关键指标，这些指标突出了交付软件的软件开发团队的性能。最初，在2020年，只定义了四个关键指标，因此您可能会找到对“DORA四个关键”指标的引用。在调查了数百个团队后，DORA发现了哪些指标和指标将高性能/精英团队与其他团队区分开来，数字相当令人震惊。DORA使用以下四个关键来对团队及其实践进行排名：
- en: '*Deployment frequency:* How often an organization successfully releases software
    in front of their customers'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*部署频率:* 组织成功向客户发布软件的频率'
- en: '*Lead time for change:* The time that it takes a change produced by an application
    team to reach live customers'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变更领先时间:* 应用团队产生的变更达到实时客户所需的时间'
- en: '*Change failure rate:* The number of problems that are created by new changes
    being introduced to our production environments'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变更失败率:* 新变化引入到我们的生产环境中导致的问题数量'
- en: '*Time to restore service:* How long it takes to recover from a problem in our
    production environments'
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*恢复服务时间:* 从我们的生产环境中解决问题的恢复所需时间'
- en: Figure 9.1 shows the DORA metrics by category, where the first two are associated
    with teams’ velocity. The second two, change failure rate and time to restore
    service, indicate how likely we are as an organization to recover from failure.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1显示了按类别划分的DORA指标，其中前两个与团队的速率相关。后两个，变更失败率和恢复服务时间，表明我们作为一个组织从故障中恢复的可能性。
- en: '![](../../OEBPS/Images/09-01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-01.png)'
- en: Figure 9.1 DORA metrics by category
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1按类别划分的DORA指标
- en: In 2022, a fifth key metric focused on reliability was added to cover operational
    performance. We will only discuss on the four software delivery metrics, because
    this book focuses on application development teams and not operation teams.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在2022年，一个关注可靠性的第五个关键指标被添加，以涵盖运营性能。我们只讨论四个软件交付指标，因为这本书的重点是应用开发团队，而不是运营团队。
- en: These five key metrics, as shown in the reports, establish a clear correlation
    between high-performing teams and their velocity expressed by these metrics. If
    you manage your teams to reduce their deployment frequency (that is, how often
    they deploy new versions in front of your users) and reduce the time caused by
    incidents, your software delivery performance will increase.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如报告中所示，这五个关键指标建立了高性能团队与其通过这些指标表达的速率之间的明确关联。如果您管理团队以减少他们的部署频率（即他们向用户展示新版本的速度）并减少事件引起的时间，您的软件交付性能将会提高。
- en: In this chapter, we will look at how to calculate these metrics for the platforms
    we are building to ensure that these platforms are improving our continuous delivery
    practices. To collect data and calculate these metrics, you will need to tap into
    different systems that your teams are using to deliver software. For example,
    if you want to calculate *deployment frequency*, you will need access to data
    from the production environment every time a new release is deployed (see figure
    9.2). Another option would be to use data from the environment pipelines performing
    the releases to our production environment. Figure 9.2 shows how we can observe
    our CI/CD pipelines and the production environment to calculate a metric like
    deployment frequency.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何计算我们正在构建的平台上的这些指标，以确保这些平台正在提高我们的持续交付实践。为了收集数据和计算这些指标，您需要利用您的团队用于交付软件的不同系统。例如，如果您想计算**部署频率**，您将需要每次新版本部署时访问生产环境的数据（见图9.2）。另一个选择是使用执行向生产环境发布的环境管道的数据。图9.2显示了我们可以如何观察我们的CI/CD管道和生产环境，以计算部署频率等指标。
- en: '![](../../OEBPS/Images/09-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-02.png)'
- en: Figure 9.2 Deployment frequency data sources.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 部署频率数据来源。
- en: If you want to calculate *lead time for change,* you will need to aggregate
    data coming from your source code version control system like GitHub/GitLab/BitBucket
    and have a way to correlate this information with the artifacts that are being
    deployed into production (figure 9.3).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想计算**变更的领先时间**，您将需要汇总来自您的源代码版本控制系统（如GitHub/GitLab/BitBucket）的数据，并有一种方法将此信息与部署到生产环境的工件关联起来（见图9.3）。
- en: '![](../../OEBPS/Images/09-03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-03.png)'
- en: Figure 9.3 Lead time for change data sources
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 变更的领先时间数据来源
- en: Suppose you have a straightforward way to correlate commits to artifacts and
    later to deployments. In that case, you can rely on a few sources, but if you
    want to have a more detailed understanding of where the bottlenecks are, you might
    choose to aggregate more data to be able to see where time is being spent.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您有一种直接将提交与工件关联，然后再与部署关联的方法。在这种情况下，您可以依赖几个来源，但如果您想更详细地了解瓶颈在哪里，您可能选择汇总更多数据，以便能够看到时间是如何被花费的。
- en: You might need to tap into incident management and monitoring tools to calculate
    change failure rate and time to restore service, as in figure 9.4.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能需要利用事件管理和监控工具来计算变更失败率和恢复服务时间，如图9.4所示。
- en: '![](../../OEBPS/Images/09-04.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-04.png)'
- en: Figure 9.4 Recovery metrics data sources
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 恢复指标数据来源
- en: For recovery metrics (change failure rate and time to restore service), data
    collection can be more challenging, because we need to find a way to measure the
    time when the application performance is degraded or there is downtime. This might
    involve reports from actual users experiencing problems with our applications.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于恢复指标（变更失败率和恢复服务时间），数据收集可能更具挑战性，因为我们需要找到一种方法来衡量应用性能下降或出现停机的时间。这可能需要来自实际用户的问题报告。
- en: 9.1.1 The integration problem
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 集成问题
- en: This quickly becomes a system integration challenge. In general terms, we need
    to observe the systems involved in our software delivery process, capture relevant
    data, and then have the mechanisms to aggregate this information. Once this information
    is available, we can use these metrics to optimize our delivery processes and
    find and solve bottlenecks.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 这迅速变成一个系统集成挑战。一般来说，我们需要观察我们软件交付过程中涉及的系统，捕获相关数据，然后有机制来汇总这些信息。一旦这些信息可用，我们可以使用这些指标来优化我们的交付流程，找到并解决瓶颈。
- en: While some projects already provide DORA metrics out of the box, you must evaluate
    if they are flexible enough to plug your systems into them. The Four Keys project
    by Google provides an out-of-the-box experience to calculate these metrics based
    on external outputs. You can read more about it at [https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance](https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然一些项目已经提供了开箱即用的 DORA 指标，但你必须评估它们是否足够灵活，以便将你的系统连接到它们。谷歌的“四要素”项目提供了一个开箱即用的体验，用于根据外部输出计算这些指标。你可以在[https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance](https://cloud.google.com/blog/products/devops-sre/using-the-four-keys-to-measure-your-devops-performance)了解更多相关信息。
- en: Unfortunately, the Four Keys project requires you to run on the Google Cloud
    Platform because it uses BigData and Google Cloud run to do the calculations.
    Following the principles of this book, we need a solution that works across cloud
    providers and uses Kubernetes as the baseline. Other tools like LinearB ([https://linearb.io/](https://linearb.io/))
    offer a SaaS solution to track different tools. I also recommend a blog post by
    Codefresh ([https://codefresh.io/learn/software-deployment/dora-metrics-4-key-metrics-for-improving-devops-performance/](https://codefresh.io/learn/software-deployment/dora-metrics-4-key-metrics-for-improving-devops-performance/))
    that explains the challenges of calculating these metrics and the data points
    that you will need to do so.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，“四要素”项目要求你在 Google Cloud Platform 上运行，因为它使用 BigData 和 Google Cloud run
    进行计算。遵循本书的原则，我们需要一个可以在不同云服务提供商之间工作且以 Kubernetes 为基础解决方案。其他工具，如 LinearB ([https://linearb.io/](https://linearb.io/))，提供了一种
    SaaS 解决方案来跟踪不同的工具。我还推荐 Codefresh ([https://codefresh.io/learn/software-deployment/dora-metrics-4-key-metrics-for-improving-devops-performance/](https://codefresh.io/learn/software-deployment/dora-metrics-4-key-metrics-for-improving-devops-performance/))
    的一篇博客文章，该文章解释了计算这些指标所面临的挑战以及你需要的数据点。
- en: 'To have a Kubernetes-native way to calculate these metrics, we need to standardize
    the way we consume information from different systems, transform this information
    into a model that we can use to calculate these metrics, and make sure that different
    organizations can extend this model with their metrics and their very diverse
    sources of information. In the next section, we will look at two standards that
    can help us with this mission: CloudEvents ([https://cloudevents.io/](https://cloudevents.io/))
    and CDEvents ([https://cdevents.dev/](https://cdevents.dev/)).'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 要有一种 Kubernetes 原生的方法来计算这些指标，我们需要标准化我们从不同系统获取信息的方式，将此信息转换成我们可以用来计算这些指标的模型，并确保不同的组织能够通过它们的指标和非常多样化的信息来源扩展此模型。在下一节中，我们将探讨两个可以帮助我们完成这项任务的标准：CloudEvents
    ([https://cloudevents.io/](https://cloudevents.io/)) 和 CDEvents ([https://cdevents.dev/](https://cdevents.dev/))。
- en: '9.2 How to measure our platform: CloudEvents and CDEvents'
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 如何衡量我们的平台：CloudEvents 和 CDEvents
- en: More and more tools and service providers are adopting CloudEvents ([https://cloudevents.io](https://cloudevents.io))
    as a standard way to wrap event data. In this book, we have covered Tekton ([https://tekton.dev](https://tekton.dev))
    and Dapr PubSub ([https://dapr.io](https://dapr.io)), but if you look on the official
    CloudEvents website (go to [https://cloudevents.io](https://cloudevents.io) and
    scroll to the CloudEvents Adopters section), you can find all the projects that
    already support the standard. In that list, you will find Argo Events ([https://argoproj.github.io/argo-events/](https://argoproj.github.io/argo-events/))
    and Knative Eventing ([https://knative.dev/docs/eventing/](https://knative.dev/docs/eventing/)),
    projects that we haven’t covered but that work very well with the tools described
    in previous chapters. I find it interesting to see cloud provider services such
    as GoogleCloud Eventarc ([https://cloud.google.com/eventarc/docs](https://cloud.google.com/eventarc/docs))
    and Alibaba Cloud EventBridge ([https://www.alibabacloud.com/help/en/eventbridge](https://www.alibabacloud.com/help/en/eventbridge))
    in the list, which indicates that CloudEvents are here to stay.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的工具和服务提供商正在采用 CloudEvents ([https://cloudevents.io](https://cloudevents.io))
    作为封装事件数据的标准方式。在这本书中，我们已经介绍了 Tekton ([https://tekton.dev](https://tekton.dev))
    和 Dapr PubSub ([https://dapr.io](https://dapr.io))，但如果您查看官方 CloudEvents 网站（访问
    [https://cloudevents.io](https://cloudevents.io) 并滚动到 CloudEvents Adopters 部分），您将找到所有已经支持该标准的项目。在该列表中，您会发现
    Argo Events ([https://argoproj.github.io/argo-events/](https://argoproj.github.io/argo-events/))
    和 Knative Eventing ([https://knative.dev/docs/eventing/](https://knative.dev/docs/eventing/))，这些是我们没有介绍但与前面章节中描述的工具配合得非常好的项目。我发现云服务提供商的服务，如
    GoogleCloud Eventarc ([https://cloud.google.com/eventarc/docs](https://cloud.google.com/eventarc/docs))
    和阿里巴巴云 EventBridge ([https://www.alibabacloud.com/help/en/eventbridge](https://www.alibabacloud.com/help/en/eventbridge))
    出现在列表中，这表明 CloudEvents 将会持续存在。
- en: While seeing more adoption is an excellent indicator, much work remains when
    you receive or want to emit a CloudEvent. CloudEvents are simple and thin envelopes
    for our events data. Figure 9.5 shows the very simple structure of a CloudEvent.
    The specification defines the CloudEvent required metadata and verifies that the
    CloudEvent will have a Payload that contains the event data that we want to send
    to other systems.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然看到更多的采用是一个很好的指标，但在接收或想要发射 CloudEvent 时，还有很多工作要做。CloudEvents 是我们事件数据的简单且轻薄的信封。图
    9.5 展示了 CloudEvent 的非常简单的结构。规范定义了 CloudEvent 所需的元数据，并验证 CloudEvent 将包含一个包含我们想要发送到其他系统的数据的事件有效载荷。
- en: '![](../../OEBPS/Images/09-05.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-05.png)'
- en: Figure 9.5 CloudEvents, a simple envelope to wrap our events data
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 CloudEvents，一个简单的信封来封装我们的事件数据
- en: Using CloudEvents, developers emit and consume events by relying on the CloudEvents
    specification to know at least what the events are about. Because the CloudEvents
    specification is not transport-specific, we can use different transports to move
    CloudEvents around. The specification includes the definition of bindings for
    protocols such as AMQP, HTTP, AVRO, KAFKA, NATS, MQQT, JSON, XML, websockets,
    and webhooks. You can find the full list at [https://github.com/cloudevents/spec/tree/main#cloudevents-documents](https://github.com/cloudevents/spec/tree/main#cloudevents-documents).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 CloudEvents，开发者通过依赖 CloudEvents 规范来了解事件至少包含什么内容，从而发射和消费事件。由于 CloudEvents
    规范不是传输特定的，我们可以使用不同的传输方式来移动 CloudEvents。该规范包括 AMQP、HTTP、AVRO、KAFKA、NATS、MQQT、JSON、XML、websockets
    和 webhooks 等协议的绑定定义。您可以在[https://github.com/cloudevents/spec/tree/main#cloudevents-documents](https://github.com/cloudevents/spec/tree/main#cloudevents-documents)找到完整的列表。
- en: When we used Dapr PubSub in chapter 7, we used the CloudEvents SDK to verify
    the type of the event and get the CloudEvent payload ([https://github.com/salaboy/platforms-on-k8s/blob/v2.0.0/conference-application/frontend-go/frontend.go#L118](https://github.com/salaboy/platforms-on-k8s/blob/v2.0.0/conference-application/frontend-go/frontend.go#L118)).
    Projects like Tekton, Knative Eventing, and Argo Events already produce and provide
    CloudEvents sources that we can consume. For example, Knative Eventing provides
    sources for GitHub, GitLab, the Kubernetes API Server, Kafka, RabbitMQ, etc. ([https://knative.dev/docs/eventing/sources/#knative-sources](https://knative.dev/docs/eventing/sources/#knative-sources)).
    Argo Events adds to the list Slack and Stripe, but it gives us 20+ out-of-the-box
    event sources ([https://argoproj.github.io/argo-events/concepts/event_source/](https://argoproj.github.io/argo-events/concepts/event_source/)).
    While projects like Tekton provide us with internal events for their own managed
    resources such as pipelines, tasks, pipelineRuns and taskRuns, it would be great
    to collect events about other tools in a unified way.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在第 7 章中使用 Dapr PubSub 时，我们使用了 CloudEvents SDK 来验证事件类型并获取 CloudEvent 有效负载
    ([https://github.com/salaboy/platforms-on-k8s/blob/v2.0.0/conference-application/frontend-go/frontend.go#L118](https://github.com/salaboy/platforms-on-k8s/blob/v2.0.0/conference-application/frontend-go/frontend.go#L118))。像
    Tekton、Knative Eventing 和 Argo Events 这样的项目已经产生并提供了我们可以消费的 CloudEvents 源。例如，Knative
    Eventing 提供了 GitHub、GitLab、Kubernetes API 服务器、Kafka、RabbitMQ 等源 ([https://knative.dev/docs/eventing/sources/#knative-sources](https://knative.dev/docs/eventing/sources/#knative-sources))。Argo
    Events 增加了 Slack 和 Stripe 到列表中，但它提供了 20 多个开箱即用的事件源 ([https://argoproj.github.io/argo-events/concepts/event_source/](https://argoproj.github.io/argo-events/concepts/event_source/))。虽然像
    Tekton 这样的项目为我们提供了它们自己管理的资源（如管道、任务、pipelineRuns 和 taskRuns）的内部事件，但以统一的方式收集关于其他工具的事件会更好。
- en: If we want to measure how the tools we include in our platform are helping our
    teams to release more software, we need to tap into these event sources to collect
    data, aggregate data, and extract meaningful metrics. Figure 9.6 shows different
    events sources that we can tap into to measure how tools are helping teams to
    deliver more software, but if we want to calculate metrics, we will need to store
    these events somewhere for further processing.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想衡量我们平台中包含的工具如何帮助我们的团队发布更多软件，我们需要利用这些事件源来收集数据、聚合数据并提取有意义的指标。图 9.6 展示了我们可以利用的不同事件源来衡量工具如何帮助团队交付更多软件，但如果我们想计算指标，我们需要将这些事件存储在某处以进行进一步处理。
- en: '![](../../OEBPS/Images/09-06.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-06.png)'
- en: Figure 9.6 Event sources and event store
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 事件源和事件存储
- en: If we want to use these events to calculate metrics, we will need to open the
    envelope, read the data, and based on that, aggregate and correlate these events
    together.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想使用这些事件来计算指标，我们需要打开信封，读取数据，并根据这些数据聚合和关联这些事件。
- en: This has proven challenging, as each tool that generates CloudEvents can define
    its schemas for the CloudEvent payload. We would need to understand how each system
    is encoding the payload to extract the data we need to calculate our metrics.
    Wouldn’t it be great to have some standard model to quickly filter and consume
    these events based on what they mean for our software delivery needs? Welcome
    CDEvents ([https://cdevents.dev](https://cdevents.dev)).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这已经证明是一个挑战，因为每个生成 CloudEvents 的工具都可以定义其 CloudEvent 有效负载的架构。我们需要了解每个系统是如何编码有效负载的，以便提取我们用于计算指标所需的数据。如果有一个标准模型，可以快速根据它们对我们软件交付需求的意义来过滤和消费这些事件，那岂不是很好？欢迎
    CDEvents ([https://cdevents.dev](https://cdevents.dev))。
- en: '9.2.1 CloudEvents for continuous delivery: CDEvents'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 持续交付的 CloudEvents：CDEvents
- en: 'CDEvents is just CloudEvents but with a more specific purpose. They map to
    different phases of our continuous delivery practices. CDEvents is an initiative
    that the Continuous Delivery Foundation ([https://cd.foundation](https://cd.foundation))
    drives, and as its website defines, they focus on enabling interoperability across
    different tools that are related to continuous delivery: “CDEvents is a common
    specification for Continuous Delivery events, enabling interoperability in the
    complete software production ecosystem” ([https://cdevents.dev](https://cdevents.dev)).'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: CDEvents 只是 CloudEvents，但具有更具体的目的。它们映射到我们持续交付实践的不同阶段。CDEvents 是由持续交付基金会 ([https://cd.foundation](https://cd.foundation))
    推动的倡议，正如其网站定义的，它们专注于在不同与持续交付相关的工具之间实现互操作性：“CDEvents 是持续交付事件的通用规范，使整个软件生产生态系统中的互操作性成为可能”
    ([https://cdevents.dev](https://cdevents.dev))。
- en: 'To provide interoperability, the CDEvents specification defines four stages
    ([https://github.com/cdevents/spec/blob/v0.3.0/spec.md#vocabulary-stages](https://github.com/cdevents/spec/blob/v0.3.0/spec.md#vocabulary-stages)).
    These stages are used to group events that are conceptually related to different
    phases and tools in our software delivery ecosystem:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供互操作性，CDEvents规范定义了四个阶段([https://github.com/cdevents/spec/blob/v0.3.0/spec.md#vocabulary-stages](https://github.com/cdevents/spec/blob/v0.3.0/spec.md#vocabulary-stages))。这些阶段用于将概念上与我们的软件交付生态系统中的不同阶段和工具相关的事件进行分组：
- en: '*Core:* Events related to the orchestration of tasks usually come from pipeline
    engines. Here you will find the specification of the events around the subjects
    “taskRun” and “pipelineRun.” Events like “PipelineRun started” or “TaskRun queued”
    can be found at this stage.'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核心*：与任务编排相关的事件通常来自管道引擎。在这里，您可以找到关于“taskRun”和“pipelineRun”主题的事件规范。在此阶段可以找到像“PipelineRun
    started”或“TaskRun queued”这样的事件。'
- en: '*Source code version control:* Events related to changes associated with your
    source code. The specification focuses on covering the subjects: “repository,”
    “branch,” and “change.” Events like “Change created” or “Change Merged” can be
    found at this stage.'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*源代码版本控制*：与源代码变更相关的事件。规范侧重于涵盖“repository”、“branch”和“change”等主题。在此阶段可以找到像“Change
    created”或“Change Merged”这样的事件。'
- en: '*Continuous integration:* Events related to building software, producing artifacts,
    and running tests. This stage covers the subjects “artifact,” “build,” “testCase,”
    and “testSuite.” Events like “Artifact published” or “Build finished” can be found
    at this stage.'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续集成*：与构建软件、生成工件和运行测试相关的事件。此阶段涵盖“工件”、“构建”、“testCase”和“testSuite”等主题。在此阶段可以找到像“Artifact
    published”或“Build finished”这样的事件。'
- en: '*Continuous deployment:* Events related to deploying software in different
    environments. The subjects covered in this stage are “services” and “environments.”
    Events like “service deployed” or “environment modified” can be found at this
    stage.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续部署*：与在不同环境中部署软件相关的事件。此阶段涵盖的主题是“services”和“environments”。在此阶段可以找到像“service
    deployed”或“environment modified”这样的事件。'
- en: '*Continuous operations:* Events related to the incidents related to our running
    services.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续运营*：与我们的运行服务相关的事件。'
- en: Figure 9.7 shows these categories and some example events for each.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7显示了这些类别以及每个类别的示例事件。
- en: '![](../../OEBPS/Images/09-07.png)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-07.png)'
- en: Figure 9.7 The four stages defined by the CDEvents specification
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 CDEvents规范定义的四个阶段
- en: We can easily use CDEvents to calculate our software delivery metrics, because
    they already cover the subjects that these metrics are interested in. For example,
    we can use events from the continuous deployment stage to calculate the *deployment
    frequency* metric. We can combine continuous deployment events and source code
    version control events to calculate *lead time for change*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以轻松地使用CDEvents来计算我们的软件交付指标，因为它们已经涵盖了这些指标感兴趣的主题。例如，我们可以使用持续部署阶段的事件来计算*部署频率*指标。我们可以结合持续部署事件和源代码版本控制事件来计算*变更领先时间*。
- en: The question then becomes, where do we get CDEvents from? CDEvents is a much
    newer specification that is currently being incubated at the CDFoundation, and
    it is my firm belief that as part of the interoperability story, this specification
    can serve as a hook mechanism for different tools and implementations to map their
    tools to a standard model that we can use to calculate all these metrics while
    allowing legacy systems (and tools that are not emitting cloud events) to benefit
    from them too.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，问题随之而来，我们从哪里获取CDEvents？CDEvents是一个相对较新的规范，目前正在CDFoundation进行孵化，我坚信作为互操作性故事的一部分，这个规范可以作为不同工具和实现的钩子机制，将它们的工具映射到我们可以用来计算所有这些指标的标准模型，同时允许旧系统（以及不发射云事件的工具）也能从中受益。
- en: This chapter will use the CDEvents specification to define our standardized
    data model. We will collect information from various systems using CloudEvents
    and rely on CDEvents to map the incoming events into the different stages of our
    software delivery practice. Figure 9.8 shows the most common sources of events
    that can be related to software delivery.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将使用CDEvents规范来定义我们的标准化数据模型。我们将使用CloudEvents从各种系统中收集信息，并依靠CDEvents将传入的事件映射到我们软件交付实践的不同阶段。图9.8显示了与软件交付相关的最常见事件来源。
- en: '![](../../OEBPS/Images/09-08.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-08.png)'
- en: Figure 9.8 CDEvents are more specialized CloudEvents for continuous delivery.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.8 CDEvents 是为持续交付而设计的更专业的 CloudEvents。
- en: Tools like Tekton are already providing experimental support for CDEvents ([https://www.youtube.com/watch?v=GAm6JzTW4nc](https://www.youtube.com/watch?v=GAm6JzTW4nc)),
    and as we will see in the next section, we can transform CloudEvents into CDEvents
    using functions. More importantly, the CDEvents Working Group is also focused
    on providing software development kits (SDKs) in different languages so you can
    build your applications that consume and emit CDEvents no matter the programming
    language you are using.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 工具如 Tekton 已经为 CDEvents 提供了实验性支持 ([https://www.youtube.com/watch?v=GAm6JzTW4nc](https://www.youtube.com/watch?v=GAm6JzTW4nc))，正如我们在下一节中将要看到的，我们可以使用函数将
    CloudEvents 转换为 CDEvents。更重要的是，CDEvents 工作组还专注于提供不同语言的软件开发工具包 (SDKs)，这样无论你使用哪种编程语言，都可以构建消费和发射
    CDEvents 的应用程序。
- en: The next section will examine how a Kubernetes-based solution for calculating
    DORA metrics can be built and extended to support different metrics and event
    sources. This is important to ensure that different platforms using different
    tools can use their performance and detect early bottlenecks and improvement points.
    Notice that this is just an example of how different tools can be wired together
    in the context of projects related to Kubernetes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 下一节将探讨如何构建一个基于 Kubernetes 的解决方案来计算 DORA 指标，并将其扩展以支持不同的指标和事件源。这很重要，以确保使用不同工具的不同平台可以使用它们的性能，并检测早期瓶颈和改进点。请注意，这只是一个示例，说明在不同与
    Kubernetes 相关的项目背景下，不同的工具如何相互连接。
- en: 9.2.2 Building a CloudEvents-based metrics collection pipeline
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 构建 CloudEvents 基础的指标收集管道
- en: To calculate the metrics proposed by the DORA team (deployment frequency, lead
    time for change, change failure rate, and time to restore service), we need to
    collect data. Once we have the data coming from different systems, we need to
    transform the data into a standardized model that we can use to calculate the
    metrics. Then we need to process the data to calculate the values for each metric.
    We need to store the results of these calculations, and then we need to make them
    available to everyone interested, probably using a graphical dashboard that summarizes
    the data collected and the calculated metrics.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算 DORA 团队提出的指标（部署频率、变更的领先时间、变更失败率和恢复服务的时间），我们需要收集数据。一旦我们有了来自不同系统的数据，我们需要将这些数据转换成标准化的模型，以便我们可以用它来计算指标。然后我们需要处理这些数据以计算每个指标的价值。我们需要存储这些计算的结果，然后我们需要使它们对所有人可用，可能使用一个图形仪表板来总结收集的数据和计算出的指标。
- en: 'Different tools can be used to build this data collection, transformation,
    and aggregation pipeline. Still, to build a simple yet extensible solution, we
    will use some of the tools covered in the previous chapters, such as Knative Serving
    to build our aggregation and transformation functions, CloudEvents, and CDEvents.
    We will also use Knative Eventing event sources, but this demo can be easily extended
    to support any other CloudEvent source, such as Argo Events. This section is divided
    into three subsections:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用不同的工具来构建此数据收集、转换和聚合管道。然而，为了构建一个简单且可扩展的解决方案，我们将使用上一章中介绍的一些工具，例如 Knative Serving
    来构建我们的聚合和转换函数，CloudEvents 和 CDEvents。我们还将使用 Knative Eventing 事件源，但这个演示可以很容易地扩展以支持任何其他
    CloudEvent 源，例如 Argo Events。本节分为三个小节：
- en: Data collection from event sources
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从事件源收集数据
- en: Data transformation to CDEvents
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据转换为 CDEvents
- en: Metrics calculations
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标计算
- en: These sections map one to one with the proposed architecture that, from a high
    level, looks like figure 9.9.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 这些部分与提出的架构一一对应，从高层次来看，类似于图 9.9。
- en: '![](../../OEBPS/Images/09-09.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-09.png)'
- en: Figure 9.9 Collecting and transforming data to calculate DORA metrics
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.9 收集和转换数据以计算 DORA 指标
- en: From a high-level perspective, we need to architect our data collection and
    transformation pipeline to support any number of event sources, because different
    companies and implementations will collect data from systems that we cannot anticipate.
    We are imposing the data to be in the form of CloudEvents before it enters our
    system. If you have event sources not following the CloudEvents specification,
    you must adapt their data to follow the specification. This can be easily achieved
    using the CloudEvents SDKs ([https://cloudevents.io/](https://cloudevents.io/)
    > SDKs section) to wrap your existing events to follow the specification.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 从一个高层次的角度来看，我们需要设计我们的数据收集和转换管道以支持任意数量的事件源，因为不同的公司和实现将收集我们无法预见系统的数据。我们在数据进入我们的系统之前就要求数据以CloudEvents的形式。如果您有不符合CloudEvents规范的事件源，您必须调整它们的数据以符合规范。这可以通过使用CloudEvents
    SDKs（[https://cloudevents.io/](https://cloudevents.io/) > SDKs部分）轻松实现，以包装您现有的事件以符合规范。
- en: Once the data enters our system, we will store it in persistent storage. In
    this case, we have used a PostgreSQL database to store all the incoming data and
    calculations. Components don’t directly call the next stage (data transformation).
    Instead, each component periodically fetches data from the database and processes
    all the data that hasn’t been processed yet. This stage (data transformation)
    transforms incoming CloudEvents already stored in the database into CDEvents structures
    that will be used to calculate the metrics. Once the transformation to the CDEvents
    structure happens, the result is stored in a separate table in our PostgreSQL
    database. Finally, the “metrics calculation” stage periodically reads from the
    database all new CDEvents that haven’t been processed and calculates the metrics
    we have defined.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据进入我们的系统，我们将将其存储在持久存储中。在这种情况下，我们使用PostgreSQL数据库来存储所有传入的数据和计算。组件不会直接调用下一阶段（数据转换）。相反，每个组件定期从数据库中获取数据，并处理尚未处理的所有数据。这个阶段（数据转换）将已存储在数据库中的传入CloudEvents转换为用于计算指标的CDEvents结构。一旦转换到CDEvents结构，结果将存储在我们PostgreSQL数据库的单独表中。最后，“指标计算”阶段定期从数据库中读取所有未处理的新CDEvents并计算我们定义的指标。
- en: This simple architecture allows us to plug in new data sources, new transformation
    logic depending on the data we receive, and finally, new metrics calculation logic
    for your domain-specific metrics (not only DORA metrics). It is also important
    to notice that as soon as we guarantee that the incoming data is correctly stored,
    all the transformations and calculations can be recalculated if the metrics data
    is lost. Let’s look deeper at the stages required to calculate the simplest DORA
    four key metrics, “deployment frequency.”
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的架构使我们能够根据接收到的数据插入新的数据源、新的转换逻辑，以及最终为您的特定领域指标（不仅限于DORA指标）的新指标计算逻辑。同样重要的是要注意，一旦我们保证传入的数据被正确存储，如果指标数据丢失，所有转换和计算都可以重新计算。让我们更深入地看看计算最简单的DORA四个关键指标“部署频率”所需的阶段。
- en: 9.2.3 Data collection from event sources
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 从事件源收集数据
- en: As shown in figure 9.9, we want to consume data from multiple sources, but we
    have set CloudEvents as the standard input format. While CloudEvents has been
    widely adopted, many systems still don’t support the standard. This section will
    look into Knative Sources as a mechanism that can declaratively define our event
    sources and transform non-CloudEvent data into CloudEvents.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.9所示，我们希望从多个来源消费数据，但我们已将CloudEvents设置为标准输入格式。虽然CloudEvents已被广泛采用，但许多系统仍然不支持该标准。本节将探讨Knative
    Sources作为一种机制，可以声明性地定义我们的事件源，并将非CloudEvent数据转换为CloudEvents。
- en: The proposed solution then exposes a REST Endpoint to receive incoming CloudEvents.
    Once we have CloudEvents, we will validate the data and store it in a PostgreSQL
    table called `cloudevents_raw`. Let’s look at Knative Eventing event sources,
    because we can just install and configure these event sources to produce events
    for us automatically.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 提出的解决方案随后暴露了一个REST端点以接收传入的CloudEvents。一旦我们有了CloudEvents，我们将验证数据并将其存储在一个名为`cloudevents_raw`的PostgreSQL表中。让我们看看Knative
    Eventing的事件源，因为我们只需安装和配置这些事件源，它们就可以自动为我们生成事件。
- en: 9.2.4 Knative Eventing event sources
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 Knative Eventing事件源
- en: With Knative Eventing event sources, you can install existing or create new
    event sources. Figure 9.10 shows some of the event sources provided out of the
    box and how these events will be routed to the data collection step of our data
    transformation pipeline.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Knative Eventing 事件源，您可以安装现有的事件源或创建新的事件源。图 9.10 显示了一些开箱即用的事件源以及这些事件将如何路由到我们的数据转换管道的数据收集步骤。
- en: '![](../../OEBPS/Images/09-10.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-10.png)'
- en: Figure 9.10 Knative Sources and data collection
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 Knative Sources 和数据收集
- en: 'Several Knative Eventing event sources are provided out of the box by the Knative
    community and different software vendors. The following list is not exhaustive,
    but it covers some of the sources that you might want to use to calculate your
    metrics:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Knative Eventing 提供了几个 Knative Eventing 事件源，由 Knative 社区和不同的软件供应商提供。以下列表并不详尽，但它涵盖了您可能想要用于计算您的指标的一些源：
- en: APIServerSource
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: APIServerSource
- en: PingSource
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PingSource
- en: GitHubSource
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitHubSource
- en: GitLabSource
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitLabSource
- en: RabbitMQSource
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RabbitMQSource
- en: KafkaSource
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KafkaSource
- en: Check the complete list of third-party sources at [https://knative.dev/docs/eventing/sources/#third-party-sources](https://knative.dev/docs/eventing/sources/#third-party-sources).
    These sources transform events, for example, from the Kubernetes API Server, GitHub,
    or RabbitMQ AMQP messages into CloudEvents.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在[https://knative.dev/docs/eventing/sources/#third-party-sources](https://knative.dev/docs/eventing/sources/#third-party-sources)检查第三方源的完整列表。这些源将事件转换，例如，从
    Kubernetes API 服务器、GitHub 或 RabbitMQ AMQP 消息转换为 CloudEvents。
- en: If you want to use one of the available Knative Sources, for example, the `APIServerSource`,
    you just need to ensure that the source is installed in your cluster and then
    configure the source according to your needs(see listing 9.1). For calculating
    the deployment frequency metric, we will tap into Kubernetes Events related to
    deployments. You can declaratively configure the source and where the events will
    be sent by defining an `APIServerSource` resource.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想使用可用的 Knative Sources 之一，例如 `APIServerSource`，您只需确保源已安装到您的集群中，然后根据您的需求配置源（参见列表
    9.1）。为了计算部署频率指标，我们将利用与部署相关的 Kubernetes 事件。您可以通过定义一个 `APIServerSource` 资源来声明性地配置源以及事件将被发送到何处。
- en: Listing 9.1 Knative Source APIServerSource definition
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.1 Knative Source APIServerSource 定义
- en: '[PRE0]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① ApiServerSource is the resource type that we use to configure the Knative
    ApiServerSource component that reads from the Kubernetes Event stream ([https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/](https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/)),
    transforms these events to CloudEvents, and sends them to a sink (target destination).
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ① ApiServerSource 是我们用来配置从 Kubernetes 事件流中读取的 Knative ApiServerSource 组件的资源类型（[https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/](https://www.cncf.io/blog/2021/12/21/extracting-value-from-the-kubernetes-events-feed/)），将这些事件转换为
    CloudEvents，并将它们发送到接收器（目标目的地）。
- en: ② As with every Kubernetes resource, we need to define a name for this resource.
    We can configure as many ApiServerSources as we want.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ② 与每个 Kubernetes 资源一样，我们需要为此资源定义一个名称。我们可以配置任意数量的 ApiServerSource。
- en: ③ Because we are reading events from the Kubernetes API server, we need to have
    access. Hence, a ServiceAccount needs to exist to enable the ApiServerSource components
    to read from the internal event stream. You can check the ServiceAccount, Role,
    and RoleBinding resources that are needed for this ApiServerSource resource to
    work at [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/api-serversource-deployments.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/api-serversource-deployments.yaml).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 由于我们从 Kubernetes API 服务器读取事件，我们需要有访问权限。因此，需要存在一个 ServiceAccount 以启用 ApiServerSource
    组件从内部事件流中读取。您可以检查为使此 ApiServerSource 资源正常工作所需的 ServiceAccount、Role 和 RoleBinding
    资源，请参阅[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/api-serversource-deployments.yaml](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/api-serversource-deployments.yaml)。
- en: ④ As defined before, this source is interested in resources of type Event.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 如前所述，此源对类型为 Event 的资源感兴趣。
- en: ⑤ In the sink section, we define where we want to send the CloudEvents generated
    from this source. In this case, we use a service reference to a Kubernetes Service
    named cloudevents-raw-service, which lives in the four-keys namespace. Knative
    Sources, when referencing other Kubernetes resources, will check that these resources
    exist and only be ready when the target service is found. Alternatively, we can
    point to a URI if the service doesn’t live in the Kubernetes API context, but
    we lose this valuable check that can help us to troubleshoot scenarios where we
    are sending events to a non-existing endpoint.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 在接收器部分，我们定义了我们希望将从这个源生成的CloudEvents发送到何处。在这种情况下，我们使用了一个指向名为cloudevents-raw-service的Kubernetes服务的服务引用，该服务位于four-keys命名空间中。Knative源在引用其他Kubernetes资源时，会检查这些资源是否存在，并且只有当目标服务被找到时才会准备就绪。或者，如果服务不在Kubernetes
    API上下文中，我们也可以指向一个URI，但我们会失去这个有价值的检查，这可以帮助我们解决将事件发送到不存在端点的情况。
- en: As you can imagine, the `ApiServerSource` will generate tons of events, which
    are sent to `cloudevents-raw-service` and stored in the PostgreSQL database. More
    complex routing and filtering can be configured only to forward events that interest
    us, but we can also apply filtering in the next stages, allowing for an approach
    that can enable us to add more metrics as we evolve our data collection process.
    With this source, we will receive one or more CloudEvents and store them in the
    database whenever a new deployment resource is created, modified, or deleted.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所想，`ApiServerSource`将生成大量的事件，这些事件被发送到`cloudevents-raw-service`并存储在PostgreSQL数据库中。我们只能配置更复杂的路由和过滤来转发我们感兴趣的事件，但我们也可以在下一阶段应用过滤，从而实现一种方法，使我们能够在数据收集过程演变时添加更多指标。使用这个源，每当创建、修改或删除新的部署资源时，我们都会接收到一个或多个CloudEvents并将它们存储在数据库中。
- en: 'If you have a system already producing events but need CloudEvents, you can
    create your own Custom Knative Eventing event source. Look at the following tutorial
    for more information on how to do this: [https://knative.dev/docs/eventing/custom-event-source/custom-event-source/](https://knative.dev/docs/eventing/custom-event-source/custom-event-source/).'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已经有了一个正在生成事件的系统但需要CloudEvents，您可以创建自己的自定义Knative Eventing事件源。查看以下教程以获取有关如何执行此操作的更多信息：[https://knative.dev/docs/eventing/custom-event-source/custom-event-source/](https://knative.dev/docs/eventing/custom-event-source/custom-event-source/)。
- en: The big advantage of declaring and managing your event sources using Knative
    Eventing event sources is that you can query your sources as any other Kubernetes
    resource, monitor and manage their state, and troubleshoot when problems arise
    using all the tools available in the Kubernetes ecosystem. Once CloudEvents are
    stored in our database, we can analyze them and map them into CDEvents for further
    calculations.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 声明和管理您的事件源使用Knative Eventing事件源的一个大优点是，您可以像查询任何其他Kubernetes资源一样查询您的源，监控和管理它们的状态，并在出现问题时使用Kubernetes生态系统中的所有工具进行故障排除。一旦CloudEvents存储在我们的数据库中，我们就可以分析它们并将它们映射到CDEvents以进行进一步计算。
- en: 9.2.5 Data transformation to CDEvents
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.5 数据转换为CDEvents
- en: Now that we have CloudEvents in our PostgreSQL database, we have validated that
    they are valid CloudEvents. We want to transform some of these very generic CloudEvents
    into CDEvents, which we will use to calculate our metrics.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在我们的PostgreSQL数据库中有CloudEvents，我们已经验证了它们是有效的CloudEvents。我们希望将其中一些非常通用的CloudEvents转换为CDEvents，我们将使用它们来计算我们的指标。
- en: As explained in the introduction, these transformations will depend on what
    kind of metrics you are trying to calculate. For this example, we will look into
    internal Kubernetes events related to deployment resources to calculate the deployment
    frequency metric, but completely different approaches can be used. For example,
    instead of looking into Kubernetes internal events, you can look into ArgoCD events
    or Tekton Pipeline events to monitor when deployments are triggered but from outside
    the cluster. Figure 9.11 shows the mapping and transformation process that needs
    to happen to map CloudEvent to CDEvents.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍中所述，这些转换将取决于您试图计算哪种类型的指标。对于这个例子，我们将查看与部署资源相关的内部Kubernetes事件来计算部署频率指标，但可以使用完全不同的方法。例如，您不必查看Kubernetes内部事件，也可以查看ArgoCD事件或Tekton
    Pipeline事件来监控何时触发部署，但来自集群外部。图9.11显示了将CloudEvent映射到CDEvents所需的映射和转换过程。
- en: '![](../../OEBPS/Images/09-11.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-11.png)'
- en: Figure 9.11 Mapping and transforming from CloudEvents to CDEvents
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.11 从CloudEvents到CDEvents的映射和转换
- en: We need a way to map a very generic CloudEvent to a concrete CDEvent that indicates
    that a service deployment has happened or has been updated. This mapping and transformation
    logic can be written in any programming language as we only deal with CloudEvents
    and CDEvents. Because of the volume of events we might be receiving, it is essential
    not to block and process all the events as they arrive. For this reason, a more
    asynchronous approach has been chosen here. The data transformation logic is scheduled
    at fixed periods, which can be configured depending on how often we want/can process
    the incoming events.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一种方法将一个非常通用的CloudEvent映射到具体的CDEvent，以指示服务部署已发生或已更新。这种映射和转换逻辑可以用任何编程语言编写，因为我们只处理CloudEvents和CDEvents。由于我们可能接收到的事件量很大，因此不阻塞并处理所有到达的事件至关重要。因此，这里选择了更异步的方法。数据转换逻辑被安排在固定的时间间隔，这可以根据我们希望/能够处理传入事件的多寡进行配置。
- en: For this example, we will map and translate incoming events with `type` equal
    to `dev.knative.apiserver.resource.add` and `data.InvolvedObject.Kind` equal to
    `Deployment` to CDEvents of the type `dev.cdevents.service.deployed.0.1.0`. This
    transformation is particular to our needs because it correlates events from the
    Knative APIServerSource to those defined in the CDEvents specification, as shown
    in figure 9.12.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们将映射和转换具有`type`等于`dev.knative.apiserver.resource.add`和`data.InvolvedObject.Kind`等于`Deployment`的传入事件到类型为`dev.cdevents.service.deployed.0.1.0`的CDEvent。这种转换特别符合我们的需求，因为它将来自Knative
    APIServerSource的事件与CDEvents规范中定义的事件相关联，如图9.12所示。
- en: '![](../../OEBPS/Images/09-12.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-12.png)'
- en: Figure 9.12 Concrete mapping and CDEvent creation for deployments
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.12 部署的实体映射和CDEvent创建
- en: To calculate different metrics, we will need more of these transformations.
    One option would be to add all the transformation logic into a single container.
    This approach would allow us to version all the transformations together as a
    single unit, but at the same time, it can complicate or limit teams writing new
    transformations, because they have a single place to change code. An alternative
    that we can take is to use a function-based approach, we can promote the creation
    of single-purpose functions to do these transformations. By using functions, only
    functions that are currently transforming events will be running. All the ones
    that are not being used can be downscaled. If we have too many events to process,
    functions can be upscaled on demand based on traffic.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算不同的指标，我们需要更多的这些转换。一个选择是将所有转换逻辑添加到一个单独的容器中。这种方法将允许我们将所有转换作为一个单一单元进行版本控制，但同时也可能使编写新转换的团队复杂化或受限，因为他们只有一个地方可以更改代码。我们可以采取的另一种方法是使用基于函数的方法，我们可以提升创建单一用途函数来执行这些转换。通过使用函数，只有当前正在转换事件的函数才会运行。所有未被使用的函数都可以进行降级。如果我们有太多事件需要处理，可以根据流量需求进行函数的升级。
- en: '![](../../OEBPS/Images/09-13.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-13.png)'
- en: Figure 9.13 Using functions to map CloudEvents to CDEvents
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../../OEBPS/Images/09-13.png)'
- en: As shown in figure 9.13, a new component is needed to route the CloudEvents
    being read from the database to concrete functions. Each transformation function
    can transform the incoming CloudEvent by inspecting its payload, enriching the
    content with an external data source, or simply wrapping the entire CloudEvent
    into a CDEvent.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.13所示，需要一个新组件来路由从数据库中读取的CloudEvents到具体的函数。每个转换函数可以通过检查其有效载荷、使用外部数据源丰富内容或简单地将整个CloudEvent包装成CDEvent来转换传入的CloudEvent。
- en: The data transformation router component must be flexible enough to allow new
    transformation functions to be plugged into the system and multiple functions
    to process the same event (the same CloudEvent being sent to one or more transformation
    functions).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换路由组件必须足够灵活，以便允许将新的转换函数插入到系统中，并允许多个函数处理同一事件（同一个CloudEvent被发送到一个或多个转换函数）。
- en: Transformation and mapping functions don’t need to care about how the CDEvents
    will be persisted. This allows us to keep these functions simple and focused on
    transformations only. Once the transformation is done and a new CDEvent is produced,
    the function will send the event to the CDEvents endpoint component, which stores
    the CDEvent in our database.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 转换和映射函数不需要关心CDEvents如何持久化。这使我们能够保持这些函数简单且仅关注转换。一旦转换完成并生成新的CDEvent，函数将事件发送到CDEvents端点组件，该组件将CDEvent存储在我们的数据库中。
- en: By the end of the transformations, we will have zero or more CDEvents stored
    in our database. These CDEvents can be used by the metric calculation functions
    that we will look at in the following section.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换完成后，我们将在数据库中存储零个或多个CDEvents。这些CDEvents可以被我们在下一节中将要查看的指标计算函数使用。
- en: 9.2.6 Metrics calculation
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.6 指标计算
- en: To calculate our metrics (DORA or custom metrics), we will use the same function-based
    approach we used for the CDEvents transformation and mapping. In this case, we
    will write functions to calculate different metrics. Because each metric requires
    aggregating data from different events and maybe systems, each metric calculation
    function can implement a different logic, see figure 9.14\. The mechanisms used
    to calculate a metric are up to the developers who write the code to perform the
    calculation.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算我们的指标（DORA或自定义指标），我们将使用与CDEvents转换和映射相同的基于函数的方法。在这种情况下，我们将编写用于计算不同指标的函数。因为每个指标都需要从不同的事件和可能系统中聚合数据，所以每个指标计算函数可以实现不同的逻辑，见图9.14。用于计算指标的机制取决于编写计算代码的开发者。
- en: '![](../../OEBPS/Images/09-14.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-14.png)'
- en: Figure 9.14 Using functions to calculate DORA metrics
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 使用函数计算DORA指标
- en: To calculate metrics, each function can be configured to fetch very specific
    CDEvents from the database and with different periods depending on how often we
    need to get updates for a particular metric. The metric result can be stored in
    the database or sent to an external system, depending on what you want to do with
    the calculated data.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算指标，每个函数可以被配置为从数据库中获取非常具体的CDEvents，并且根据我们需要为特定指标获取更新的频率不同，有不同的时间段。指标结果可以存储在数据库中或发送到外部系统，具体取决于你想要如何处理计算出的数据。
- en: If we look at calculating the deployment frequency metric for a more concrete
    example, we need to implement a couple of custom mechanisms and data structures
    to keep track of the metric, as in figure 9.15.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们以计算更具体的部署频率指标为例，我们需要实现一些自定义机制和数据结构来跟踪该指标，如图9.15所示。
- en: '![](../../OEBPS/Images/09-15.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-15.png)'
- en: Figure 9.15 Deployment frequency calculation flow
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.15 部署频率计算流程
- en: 'A simplified flow for calculating the deployment frequency metric is shown
    in figure 9.15 where step #1 is to get CDEvents related to deployments from the
    `cdevents_raw` table. The `Create Deployments structure function` is in charge
    of reading CDEvents with type `dev.cdevents.service.deployed.0.1.0`, inspecting
    the payload and metadata, and creating a new structure that can be later queried.
    Step #2 is responsible for persisting this new structure in our database. The
    main reason for this structure is to make our data easier and more performant
    to query for the metric we are implementing. In this case, a new `deployment`
    structure (and table) is created to record data we want to use to calculate our
    deployment frequency metric. For this simple example, the deployment structure
    contains the service’s name, the timestamp, and the deployment’s name. In step
    #3 we can use this data to get our deployment frequency by service and display
    this information per day, week, or month. These functions need to be idempotent,
    meaning that we can retrigger the calculation of the metrics again using the same
    CDEvents as input, and we should obtain the same results.'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 计算部署频率指标的简化流程如图9.15所示，其中步骤#1是从`cdevents_raw`表中获取与部署相关的CDEvents。`Create Deployments
    structure function`负责读取类型为`dev.cdevents.service.deployed.0.1.0`的CDEvents，检查有效载荷和元数据，并创建一个可以稍后查询的新结构。步骤#2负责将这个新结构持久化到我们的数据库中。这个结构的主要目的是使我们的数据更容易和更高效地查询我们正在实施的指标。在这种情况下，创建了一个新的`deployment`结构（和表）来记录我们想要用于计算部署频率指标的数据。在这个简单的例子中，部署结构包含服务的名称、时间戳和部署的名称。在步骤#3中，我们可以使用这些数据通过服务获取我们的部署频率，并按日、周或月显示这些信息。这些函数需要是无状态的，这意味着我们可以使用相同的CDEvents作为输入重新触发指标的计算，并且应该获得相同的结果。
- en: Optimizations can be added to this flow; for example, a custom mechanism can
    be created to avoid reprocessing CDEvents that have already been processed. These
    customizations can be treated as internal mechanisms for each metric, and developers
    should be able to add integration with other systems and tools as needed. For
    the sake of the example, the `Get Deployment Frequency Function` can fetch the
    metrics from the database. Still, in a more realistic scenario, you can have a
    dashboard directly querying the database where the simplified structures are stored,
    because many dashboard solutions provide an SQL connector out of the box.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 可以向此流程添加优化；例如，可以创建一个自定义机制来避免重新处理已经处理过的CDEvents。这些定制可以被视为每个指标的内部机制，开发者应该能够根据需要添加与其他系统和工具的集成。为了示例的目的，`获取部署频率函数`可以从数据库中检索指标。然而，在更现实的场景中，你可以有一个仪表板直接查询存储简化结构的数据库，因为许多仪表板解决方案都提供了开箱即用的SQL连接器。
- en: Now that we have covered the flow to calculate the deployment frequency metric
    let’s look at a working example where we will install all the components required
    for data collection, data transformation, and metrics calculation.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经覆盖了计算部署频率指标的流程，让我们看看一个工作示例，其中我们将安装所有用于数据收集、数据转换和指标计算所需的组件。
- en: 9.2.7 Working example
  id: totrans-134
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.7 工作示例
- en: This section will look at a working example, showing how we can combine data
    collection, data transformation to CDEvents, and metrics calculation for our Kubernetes-based
    platforms. It covers a very basic example and a step-by-step tutorial on how to
    install and how to run the components needed to calculate the deployment frequency
    metric of our deployments ([https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/README.md)).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将探讨一个工作示例，展示我们如何结合数据收集、将数据转换为CDEvents以及为我们基于Kubernetes的平台进行指标计算。它涵盖了一个非常基础的示例以及如何安装和运行计算部署频率指标所需组件的逐步教程（[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/dora-cloudevents/README.md)）。
- en: 'The architecture implemented in this example puts together the stages defined
    in the previous sections: data collection, data transformation, and metrics calculation.
    One of the main aspects covered by this architecture is the extensibility and
    pluggability of components for data transformation and metrics calculation. This
    architecture assumes that we will collect data as CloudEvents, so the user is
    responsible for transforming their event sources to CloudEvents to use this architecture.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中实现的架构将前几节中定义的阶段组合在一起：数据收集、数据转换和指标计算。该架构涵盖的主要方面之一是数据转换和指标计算组件的可扩展性和可插拔性。该架构假设我们将以CloudEvents的形式收集数据，因此用户负责将他们的事件源转换为CloudEvents以使用此架构。
- en: Figure 9.16 shows how all the components are tied together to provide the functionality
    of deciding which events we want to collect and how to transform them into CDEvents
    so we can calculate DORA metrics with them.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16展示了所有组件如何相互关联，以提供决定我们想要收集哪些事件以及如何将它们转换为CDEvents以计算DORA指标的功能。
- en: '![](../../OEBPS/Images/09-16.png)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-16.png)'
- en: Figure 9.16 Example architecture for capturing and calculating DORA metrics
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.16 捕获和计算DORA指标的示例架构
- en: While the architecture might look complicated initially, it was designed to
    allow custom extensions and mappings necessary to collect and process events from
    various sources.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然该架构最初可能看起来很复杂，但它被设计成允许收集和处理来自各种来源的事件所需的自定义扩展和映射。
- en: Following the step-by-step tutorial, you will create a new Kubernetes cluster
    to install all the components needed to collect CloudEvents and calculate the
    metrics. Still, the architecture is in no way limited by a single cluster. After
    you create and connect to a cluster, you will install tools such as Knative Serving
    for our function’s runtime and Knative Eventing only for our event sources. Once
    the cluster is ready, you will create a new `namespace` to host all the components
    actively processing the data collected and an instance of PostgreSQL to store
    our events.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 按照逐步教程，你将创建一个新的Kubernetes集群来安装收集CloudEvent和计算指标所需的所有组件。然而，架构并不局限于单个集群。在你创建并连接到集群后，你将安装如Knative
    Serving这样的工具用于我们的函数运行时，以及仅用于我们的事件源的Knative Eventing。一旦集群准备就绪，你将创建一个新的`namespace`来托管所有积极处理收集数据的组件，以及一个PostgreSQL实例来存储我们的事件。
- en: Storing events and metrics
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 存储事件和指标
- en: 'Once we have our database to store events and metrics information, we need
    to create the tables for our components to store and read events. For this example,
    we will create the following tables: `cloudevents_raw`, `cdevents_raw`, and `deployments`,
    as shown in figure 9.17.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了数据库来存储事件和指标信息，我们需要为我们的组件创建存储和读取事件的表。对于这个例子，我们将创建以下表：`cloudevents_raw`、`cdevents_raw`和`deployments`，如图9.17所示。
- en: '![](../../OEBPS/Images/09-17.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图9.17](../../OEBPS/Images/09-17.png)'
- en: Figure 9.17 Tables, CloudEvents, CDEvents, and metrics calculations
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 表格、CloudEvents、CDEvents和指标计算
- en: 'Let’s look at what information we are going to be storing in these three tables.
    The `cloudevents_raw` table stores all the incoming CloudEvents from different
    sources. The main purpose of this table is data collection:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们将要存储在这三个表中的信息。`cloudevents_raw`表存储了来自不同来源的所有传入CloudEvent。这个表的主要目的是数据收集：
- en: 'The schema of this table is very simple and only has three columns:'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表的架构非常简单，只有三个列：
- en: '`event_id`: This value is generated by the database.'
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event_id`: 这个值由数据库生成。'
- en: '`event_timestamp`: Stores the timestamp of when the event is received. This
    can be later used to order events for reprocessing.'
  id: totrans-149
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`event_timestamp`: 存储事件接收的时间戳。这可以用于稍后对事件进行重新处理排序。'
- en: '`content`: Stores the serialized JSON version of the CloudEvent in a JSON column.'
  id: totrans-150
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content`: 存储CloudEvent序列化的JSON版本在一个JSON列中。'
- en: This table is kept as simple as possible because we don’t know what kind of
    cloud events we are getting, and at this point, we don’t want to unmarshal and
    read the payload, because this can be done in the data transformation stage.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个表尽可能保持简单，因为我们不知道我们会得到什么类型的云事件，在这个阶段，我们不想反序列化和读取有效负载，因为这可以在数据转换阶段完成。
- en: 'The `cdevents_raw` table stores all the CDEvents we are interested in storing
    after filtering and transforming all the incoming CloudEvents. Because CDEvents
    are more specific, and we have more metadata about these events, this table has
    more columns:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`cdevents_raw`表存储了我们过滤和转换所有传入的CloudEvent后感兴趣存储的所有CDEvent。由于CDEvent更具体，并且我们关于这些事件的元数据更多，因此这个表有更多的列：'
- en: '`cd_id`: Stores the CloudEvent ID from the original CloudEvent.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_id`: 存储原始CloudEvent的ID。'
- en: '`cd_timestamp`: Stores the timestamp of when the original CloudEvent was received.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_timestamp`: 存储原始CloudEvent接收的时间戳。'
- en: '`cd_source`: Stores the source where the original CloudEvent was generated.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_source`: 存储原始CloudEvent生成的来源。'
- en: '`cd_type`: Stores and allows us to filter by different CDEvents types. The
    types of CDEvents stored in this table are defined by the transformation functions
    running in our setup.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_type`: 存储并允许我们根据不同的CDEvent类型进行过滤。存储在这个表中的CDEvent类型由我们设置中运行的转换函数定义。'
- en: '`cd_subject_id`: Stores the ID of the entity associated with this CDEvent.
    This information is obtained when our transformation functions analyze the content
    of the original CloudEvent.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_subject_id`: 存储与该CDEvent关联的实体的ID。这个信息是在我们的转换函数分析原始CloudEvent的内容时获得的。'
- en: '`cd_subject_source`: Stores the source of the entity associated with this CDEvent.'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`cd_subject_source`: 存储与该CDEvent关联的实体的来源。'
- en: '`content`: The JSON serialized version of our CDEvent, which includes the original
    CloudEvent as a payload.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`content`: 我们CDEvent的JSON序列化版本，其中包含原始CloudEvent作为有效负载。'
- en: 'The `deployments` table is custom to calculate the deployment frequency metric.
    There are no rules to what you store in these custom tables that are used to calculate
    different metrics. For the sake of simplicity, this table only has three columns:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`deployments` 表是自定义的，用于计算部署频率指标。用于计算不同指标的自定义表中存储的内容没有规则。为了简化，此表只有三个列：'
- en: '`deploy_id`: The id used to identify a service deployment.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deploy_id`: 用于识别服务部署的 ID。'
- en: '`time_created`: When the deployment was created or updated.'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`time_created`: 部署创建或更新的时间。'
- en: '`deploy_name`: The deployment name used to calculate the metrics.'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`deploy_name`: 用于计算指标的部署名称。'
- en: Once we have the tables ready to store our events and metrics data, we need
    to have events flowing into our components, and for that, we will need to configure
    event sources.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了存储事件和指标数据的表，我们需要让事件流进入我们的组件，为此，我们需要配置事件源。
- en: Configuring event sources
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 配置事件源
- en: Finally, before installing the data transformation or metrics calculation functions,
    we will configure the Kubernetes API Server event source from Knative Eventing
    to detect when new deployments are being created. See figure 9.18.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在安装数据转换或指标计算函数之前，我们将从 Knative Eventing 配置 Kubernetes API 服务器事件源以检测新部署的创建。见图
    9.18。
- en: '![](../../OEBPS/Images/09-18.png)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-18.png)'
- en: Figure 9.18 Example using the Knative Eventing API server source. We can tap
    into Kubernetes Event Stream by using the Knative Eventing API Server Source,
    which transforms internal events into CloudEvents that can be routed to different
    systems for filtering and processing.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.18 使用 Knative Eventing API 服务器源的示例。我们可以通过使用 Knative Eventing API 服务器源来访问
    Kubernetes 事件流，该源将内部事件转换为 CloudEvents，这些事件可以被路由到不同的系统进行过滤和处理。
- en: Here, you can use any CloudEvent-enabled data source. The Knative API server
    source is an example of how easy it is to consume and route events for further
    processing.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，您可以使用任何 CloudEvent 兼容的数据源。Knative API 服务器源是展示如何轻松消费和路由事件进行进一步处理的示例。
- en: Check projects like Argo Events ([https://argoproj.github.io/argo-events/](https://argoproj.github.io/argo-events/))
    and other Knative Eventing sources ([https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/))
    to familiarize yourself with what is available out of the box. Also, check the
    CloudEvents specification adopters list ([https://cloudevents.io/](https://cloudevents.io/)),
    because all these tools are already generating CloudEvents that you can consume
    and map to calculate metrics.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 检查类似 Argo Events ([https://argoproj.github.io/argo-events/](https://argoproj.github.io/argo-events/))
    和其他 Knative Eventing 源 ([https://knative.dev/docs/eventing/sources/](https://knative.dev/docs/eventing/sources/))
    的项目，以熟悉开箱即用的功能。还要检查 CloudEvents 规范采用者列表 ([https://cloudevents.io/](https://cloudevents.io/))，因为所有这些工具都已经生成
    CloudEvents，您可以使用它们进行消费并将它们映射到计算指标。
- en: Deploying data transformation and metrics calculation components
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 部署数据转换和指标计算组件
- en: 'Now that we have a place to store our events and metrics data, and event sources
    are configured and ready to emit events when users interact with our cluster,
    we can deploy the components that will take these events, filter them, and transform
    them to calculate our deployment frequency metric. The step-by-step tutorial deploys
    the following components:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了存储事件和指标数据的地方，事件源已配置并准备好在用户与我们的集群交互时发出事件，我们可以部署将接收这些事件、过滤它们并将它们转换为计算部署频率指标的组件。逐步教程部署以下组件：
- en: '*CloudEvents endpoint:* Exposes an HTTP endpoint to receive CloudEvents and
    connects to the database to store them.'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CloudEvents 端点:* 提供一个 HTTP 端点以接收 CloudEvents 并将它们连接到数据库进行存储。'
- en: '*CDEvents endpoint:* Exposes an HTTP endpoint to receive CDEvents and connects
    to the database to store them.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CDEvents 端点:* 提供一个 HTTP 端点以接收 CDEvents 并将它们连接到数据库进行存储。'
- en: '*CloudEvents router:* Reads CloudEvents from the database and routes them to
    the configured transformation functions. This component allows users to plug their
    transformation functions to transform a CloudEvent into a CDEvent for further
    processing. The CloudEvents router runs periodically by fetching unprocessed events
    from the database.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*CloudEvents 路由器:* 从数据库中读取 CloudEvents 并将它们路由到配置的转换函数。此组件允许用户将他们的转换函数插入以将 CloudEvent
    转换为 CDEvent 以进行进一步处理。CloudEvents 路由器通过从数据库中检索未处理的事件定期运行。'
- en: '*(CDEvents) transformation function:* Users can define transformation functions
    and map CloudEvents to CDEvents. The idea here is to enable users to add as many
    functions as needed to calculate DORA and other metrics.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(CDEvents)转换函数:* 用户可以定义转换函数并将CloudEvents映射到CDEvents。这里的想法是使用户能够添加所需的所有函数来计算DORA和其他指标。'
- en: '*(Deployment frequency) calculation function:* Metrics calculation functions
    provide a way to calculate different metrics by reading CDEvents from the database.
    These functions can store the calculated metrics in custom database tables if
    needed.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(部署频率)计算函数:* 指标计算函数提供了一种通过从数据库中读取CDEvents来计算不同指标的方法。如果需要，这些函数可以将计算出的指标存储在自定义数据库表中。'
- en: '*(Deployment frequency)* *metric endpoint:* These metric endpoints can be optionally
    exposed for applications to consume the calculated metrics. Alternatively, dashboards
    can query the data directly from the database.'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*(部署频率)* *指标端点:* 这些指标端点可以可选地暴露给应用程序以消费计算出的指标。或者，仪表板可以直接从数据库查询数据。'
- en: Figure 9.19 shows how CloudEvents flows throughout the different components
    that we have installed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19展示了CloudEvents如何流经我们已安装的不同组件。
- en: '![](../../OEBPS/Images/09-19.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图9.19](../../OEBPS/Images/09-19.png)'
- en: Figure 9.19 Data flows from Data Sources producing CloudEvents to the CloudEvents
    endpoint whose only mission is to store these events into the Event Store. From
    there, the CloudEvents Router have the logic to decide where to route events to
    transformation functions, which allows us to map CloudEvents to CDEvents for further
    processing. Once we have CDEvents, the Calculation Functions can read these events
    to aggregate data and produce metrics. Metrics consumers can get the metrics by
    interacting with the Metric Endpoint, which will fetch the calculated metrics
    from the metrics database.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.19 数据从数据源流向产生CloudEvents的CloudEvents端点，其唯一任务是将这些事件存储到事件存储中。从那里，CloudEvents
    Router拥有逻辑来决定将事件路由到转换函数，这使我们能够将CloudEvents映射到CDEvents以进行进一步处理。一旦我们有了CDEvents，计算函数就可以读取这些事件来聚合数据并产生指标。指标消费者可以通过与指标端点交互来获取指标，该端点将从指标数据库中检索计算出的指标。
- en: As soon as we have our components up and running, we can start using our cluster
    to generate events filtered and processed by these components to produce the deployment
    frequency metric.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们的组件启动并运行，我们就可以开始使用我们的集群来生成由这些组件过滤和处理的事件，以产生部署频率指标。
- en: Deployment frequency metric for your deployments
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 部署频率指标针对您的部署
- en: We need to deploy new workloads to our cluster to calculate the deployment frequency
    metric. The tutorial includes all the transformation and metric calculation functions
    to monitor events coming from deployment resources.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将新的工作负载部署到我们的集群中，以计算部署频率指标。教程包括所有转换和指标计算函数，以监控来自部署资源的事件。
- en: While development teams can create and update their existing deployments, the
    platform team can transparently monitor how efficient the platform is to enable
    teams to perform their work. Figure 9.20 shows the teams involved and how the
    metrics are calculated for this example.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当开发团队能够创建和更新他们现有的部署时，平台团队能够透明地监控平台效率，以便团队能够执行他们的工作。图9.20显示了涉及的团队以及本例中如何计算指标。
- en: '![](../../OEBPS/Images/09-20.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图9.20](../../OEBPS/Images/09-20.png)'
- en: Figure 9.20 Components and data flow to measure performance metrics
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.20 组件和数据流以测量性能指标
- en: 'Finally, you can `curl` the following endpoint if you are running the example
    on KinD:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果您在KinD上运行示例，您可以`curl`以下端点。
- en: '[PRE1]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You should see something like the following listing.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到以下类似列表。
- en: Listing 9.2 Getting deployment frequency metrics
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 获取部署频率指标
- en: '[PRE2]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Transformation and metrics calculation functions are scheduled to run every
    minute. Hence, these metrics will be only returned after the functions have been
    executed. Alternatively, you can use a dashboard solution like Grafana to connect
    to our PostgreSQL database and configure the metrics. Dashboard tools can be focused
    on the tables that store data about particular metrics. For our deployment frequency
    example, the `deployments` table is the only one relevant for displaying the metrics.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 转换和指标计算函数每分钟调度运行一次。因此，这些指标只有在函数执行完毕后才会返回。或者，您可以使用Grafana这样的仪表板解决方案来连接到我们的PostgreSQL数据库并配置指标。仪表板工具可以专注于存储特定指标数据的表。对于我们的部署频率示例，`deployments`表是唯一与显示指标相关的表。
- en: I strongly recommend you check the example and try to run it locally, follow
    the step-by-step tutorial, and get in touch if you have questions or want to help
    improve it. Modifying the example to calculate the metrics differently or adding
    your custom metrics will give you a good overview of how complex these metrics
    calculations are but, at the same time, how important it is to have this information
    available to our application development and operations teams so they can understand
    how things are going almost in real-time.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您检查示例并尝试在本地运行它，遵循逐步教程，如果您有问题或想帮助改进它，请与我联系。修改示例以不同方式计算指标或添加您自定义的指标将为您提供这些指标计算复杂性的良好概述，同时，这也说明了为什么让我们的应用开发和运维团队能够几乎实时地了解情况是多么重要。
- en: In the next section, we will look at the Keptn Lifecycle Toolkit ([https://keptn.sh](https://keptn.sh)),
    an open-source and CNCF project that built different mechanisms not only to monitor,
    observe, and calculate metrics about our cloud-native applications, but also to
    take actions when things are not going as expected or with integrations with other
    systems are needed.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨 Keptn 生命周期工具包 ([https://keptn.sh](https://keptn.sh))，这是一个开源的 CNCF
    项目，它构建了不同的机制，不仅用于监控、观察和计算我们云原生应用的指标，而且在预期之外或需要与其他系统集成时也能采取行动。
- en: 9.3 Keptn Lifecycle Toolkit
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 Keptn 生命周期工具包
- en: The Keptn Lifecycle Toolkit (KLT) is a cloud-native lifecycle orchestration
    toolkit. KLT focuses on deployment observability, deployment data access, and
    deployment check orchestration. Keptn is not only all about monitoring and observing
    what is going on with our workloads, but it also provides the mechanisms to check
    and act when things go wrong.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: Keptn 生命周期工具包 (KLT) 是一个云原生生命周期编排工具包。KLT 专注于部署可观察性、部署数据访问和部署检查编排。Keptn 不仅关注监控和观察我们的工作负载的状态，而且还提供了在出现问题时进行检查和采取行动的机制。
- en: As we saw in the previous section, getting basic metrics such as deployment
    frequency can be very useful in measuring teams’ performance. While deployment
    frequency is just one metric, we can use that to start measuring our early platform
    initiatives. In this short section, I wanted to show how KLT can help you with
    this task by following a different but complementary approach to the one discussed
    in section 9.2.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在上一节中看到的，获取基本指标，如部署频率，可以非常有助于衡量团队的表现。虽然部署频率只是一个指标，但我们可以用它来开始衡量我们的早期平台倡议。在本节中，我想展示
    KLT 如何通过采用与第 9.2 节中讨论的不同但互补的方法来帮助您完成这项任务。
- en: Keptn extends the Kubernetes Scheduler component (which decides where our workloads
    will run on our clusters) to monitor and extract information about our workloads,
    as in figure 9.21\. This mechanism enables teams to set up custom pre/post-deployment
    tasks by providing Keptn Task Definitions resources. Keptn is planning to use
    Kubernetes built-in scheduling gates, a feature that, at the time of writing,
    is being proposed to the Kubernetes community ([http://mng.bz/PRW2](http://mng.bz/PRW2)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Keptn 扩展了 Kubernetes 调度器组件（它决定我们的工作负载将在我们的集群上运行的位置），以监控和提取关于我们工作负载的信息，如图 9.21
    所示。这种机制使得团队可以通过提供 Keptn 任务定义资源来设置自定义的预/后部署任务。Keptn 正在计划使用 Kubernetes 内置的调度门功能，这是一个在撰写本文时被提议给
    Kubernetes 社区的功能 ([http://mng.bz/PRW2](http://mng.bz/PRW2))。
- en: 'Note You can follow a step-by-step tutorial to see Keptn in action by following
    this link: [https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/keptn/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/keptn/README.md).'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可以通过以下链接跟随一个逐步教程，以了解 Keptn 的实际操作：[https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/keptn/README.md](https://github.com/salaboy/platforms-on-k8s/blob/main/chapter-9/keptn/README.md)。
- en: '![](../../OEBPS/Images/09-21.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-21.png)'
- en: Figure 9.21 Keptn architecture providing out-of-the-box observability and application
    lifecycle hooks.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.21 Keptn 架构提供即插即用的可观察性和应用程序生命周期钩子。
- en: Keptn uses standard Kubernetes annotations to identify which applications are
    interested in being monitored and managed. I have included the following annotations
    for the Conference application to make Keptn aware of our services. The Agenda
    service deployment resource includes the following annotations, as shown in listing
    9.3 ([https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/helm/conference-app/templates/agenda-service.yaml#L14](https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/helm/conference-app/templates/agenda-service.yaml#L14)).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Keptn 使用标准的 Kubernetes 注解来识别哪些应用程序希望被监控和管理。我为 Conference 应用程序包含了以下注解，以便 Keptn
    了解我们的服务。Agenda 服务部署资源包括以下注解，如列表 9.3 所示 ([https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/helm/conference-app/templates/agenda-service.yaml#L14](https://github.com/salaboy/platforms-on-k8s/blob/main/conference-application/helm/conference-app/templates/agenda-service.yaml#L14))。
- en: Listing 9.3 Kubernetes standard application annotations
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.3 Kubernetes 标准应用程序注解
- en: '[PRE3]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Keptn is now aware of the Agenda service and can monitor and execute actions
    related to this service lifecycle. Notice the `part-of` annotation, which allows
    us to monitor single services and group a set of services under the same logical
    application. This grouping allows Keptn to execute pre/post-deployment actions
    for each service and the logical application (a group of services sharing the
    same value for the `app.kubernetes.io/part-of` annotation. This example doesn’t
    use that feature because I want to keep things simple and focused on single services.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Keptn 现在已经了解 Agenda 服务，并且可以监控和执行与此服务生命周期相关的操作。注意 `part-of` 注解，它允许我们监控单个服务并将一组服务分组在同一个逻辑应用下。这种分组允许
    Keptn 为每个服务以及逻辑应用（共享相同 `app.kubernetes.io/part-of` 注解值的多个服务组）执行部署前后的操作。本例中没有使用该功能，因为我希望保持内容简单并专注于单个服务。
- en: 'The step-by-step tutorial installs Keptn, Prometheus, Grafana, and Jaeger so
    we can understand what Keptn is doing. Once Keptn is installed in your cluster,
    you need to let Keptn know which namespaces should be monitored, by annotating
    the namespace resources with a Keptn annotation. You can do that by running the
    following command to enable Keptn in the default namespace:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 逐步教程安装了 Keptn、Prometheus、Grafana 和 Jaeger，以便我们了解 Keptn 的功能。一旦 Keptn 在您的集群中安装完成，您需要让
    Keptn 知道哪些命名空间需要被监控，通过在命名空间资源上添加 Keptn 注解来实现。您可以通过运行以下命令在默认命名空间中启用 Keptn：
- en: '[PRE4]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Once Keptn starts monitoring a specific namespace, it will look for annotated
    deployments to start getting metrics that the Keptn Applications Grafana dashboards
    can consume, as shown in figure 9.22.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦 Keptn 开始监控特定的命名空间，它将寻找带有注解的部署以开始获取 Keptn 应用程序 Grafana 仪表板可以消费的指标，如图 9.22
    所示。
- en: '![](../../OEBPS/Images/09-22.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-22.png)'
- en: Figure 9.22 Keptn Application Grafana dashboard for the notifications service
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.22 通知服务的 Keptn 应用程序 Grafana 仪表板
- en: This dashboard shows us our deployment frequency for the annotated deployments
    (all the Conference application’s services) running in the default namespace.
    In the step-by-step tutorial, we make changes to the notification service deployment
    so Keptn can detect the change and show the new version in the dashboard. As shown
    in figure 9.22, the average time between deployments is 5.83 minutes. On the side,
    you can see exactly how long it took to deploy v1.0.0 and v1.10\. Having these
    dashboards available to the teams responsible for each service can help provide
    visibility on the whole process of releasing new versions. Having this information
    available from day one can show progress on how teams improve their workflows
    or find bottlenecks and recurring problems that can be easily fixed.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 此仪表板显示了我们在默认命名空间中运行带有注解的部署（所有 Conference 应用程序的服务）的部署频率。在逐步教程中，我们对通知服务部署进行更改，以便
    Keptn 可以检测到更改并在仪表板中显示新版本。如图 9.22 所示，部署的平均时间为 5.83 分钟。在旁边，您可以确切地看到部署 v1.0.0 和 v1.10
    所花费的时间。这些仪表板可供每个服务的责任团队使用，有助于提供整个发布新版本过程的可见性。从第一天起就有这些信息可以帮助展示团队改进工作流程或找到可以轻松解决的瓶颈和重复性问题。
- en: Besides gaining all this information and out-of-the-box metrics, as mentioned
    before, KLT goes one step further by providing hook points to execute pre-/post-deployment
    tasks. We can use these tasks to validate the environment’s state before performing
    a release, send notifications to the teams on call, or just audit the process.
    After the deployment, we can use post-deployment hooks to run validation tests,
    send automated notifications to customers about the update, or just congratulate
    the team for their amazing work.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 除了获得所有这些信息和前面提到的开箱即用的指标外，KLT 还更进一步，通过提供执行预/部署任务的钩子点。我们可以使用这些任务在发布前验证环境状态，向值班团队发送通知，或者只是审计流程。部署后，我们可以使用部署后钩子运行验证测试，向客户发送关于更新的自动化通知，或者只是祝贺团队出色的表现。
- en: Keptn introduces the KeptnTaskDefinitions resource, which supports Deno ([https://deno.land/](https://deno.land/)),
    Python3, or any container image reference ([https://lifecycle.keptn.sh/docs/yaml-crd-ref/taskdefinition/](https://lifecycle.keptn.sh/docs/yaml-crd-ref/taskdefinition/))
    to define what the task behavior. The KeptnTaskDefinition resource used by the
    step-by-step tutorial is quite simple, and it looks like listing 9.4.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: Keptn 引入了 KeptnTaskDefinitions 资源，它支持 Deno ([https://deno.land/](https://deno.land/))、Python3
    或任何容器镜像引用 ([https://lifecycle.keptn.sh/docs/yaml-crd-ref/taskdefinition/](https://lifecycle.keptn.sh/docs/yaml-crd-ref/taskdefinition/))
    来定义任务行为。用于逐步教程的 KeptnTaskDefinition 资源相当简单，看起来像列表 9.4。
- en: Listing 9.4 Keptn TaskDefinition using Deno
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 使用 Deno 的 Keptn TaskDefinition
- en: '[PRE5]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① The team will use this resource name to define where this task will be executed.
    This is a reusable task definition, so this can be called from different services’
    lifecycle hooks.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ① 团队将使用此资源名称来定义此任务将在哪里执行。这是一个可重用的任务定义，因此可以从不同服务的生命周期钩子中调用它。
- en: ② We can access the context of the task that is being executed by calling Deno.env.get("CONTEXT").
    This provides us with all the details used to create the task, such as which workload
    requests this task to be executed.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们可以通过调用 Deno.env.get("CONTEXT") 来访问正在执行的任务的上下文。这为我们提供了创建任务时使用的所有详细信息，例如哪个工作负载请求执行此任务。
- en: 'To bind a task definition with one of our services, we use a Keptn-specific
    annotation in our deployments:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 要将任务定义与我们的某个服务绑定，我们在部署中使用 Keptn 特定的注释：
- en: '[PRE6]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This annotation will configure Keptn to execute this task after the notification
    service deployment is changed and the new version is deployed. Keptn will create
    a new Kubernetes Job to run the KeptnTaskDefinition. This means you can query
    all the pre-/post-deployment task definition executions by looking at the job
    executions in the default namespace.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 此注释将配置 Keptn 在更改通知服务部署并部署新版本后执行此任务。Keptn 将创建一个新的 Kubernetes Job 来运行 KeptnTaskDefinition。这意味着您可以通过查看默认命名空间中的作业执行来查询所有预/部署任务定义的执行。
- en: By using annotations and KeptnTaskDefinitions, the platform engineering team
    can create a library of shared tasks that teams can reuse in their workloads,
    or even better, they can use mutation webhooks or a policy engine like OPA to
    automatically mutate the deployment resources to add the Keptn annotation.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用注释和 KeptnTaskDefinitions，平台工程团队可以创建一个共享任务库，团队可以在他们的工作负载中重用这些任务，或者更好的是，他们可以使用突变
    Webhook 或 OPA 这样的策略引擎自动突变部署资源以添加 Keptn 注释。
- en: If you change the notification service deployment and then tail the logs, you
    should see the following (listing 9.5).
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更改了通知服务部署并跟踪日志，你应该会看到以下内容（列表 9.5）。
- en: Listing 9.5 Expected output from the TaskDefinition execution
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.5 TaskDefinition 执行的预期输出
- en: '[PRE7]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you look at Jaeger in figure 9.23, you can see all the steps involved in
    deploying a new version of our notification service by looking at the Keptn Lifecycle
    Operator traces.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看图 9.23 中的 Jaeger，你可以通过查看 Keptn 生命周期操作符跟踪来了解部署我们通知服务新版本所涉及的所有步骤。
- en: '![](../../OEBPS/Images/09-23.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/09-23.png)'
- en: Figure 9.23 Keptn Lifecycle Operator traces for service updates
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.23 Keptn 生命周期操作符跟踪服务更新
- en: If you run the step-by-step tutorial on your environment, you can see that the
    post-deployment hook is being scheduled after the new version of the service is
    up and running.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在你自己的环境中运行逐步教程，你可以看到在服务的新版本启动并运行后，部署后钩子正在被安排。
- en: In this short section, we have learned the basics of what the Keptn Lifecycle
    Toolkit can do for us, how we can benefit from having these metrics from day one,
    and how we can have more control over the lifecycle of our services by adding
    pre-/post-deployment tasks using a declarative way.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个简短的章节中，我们学习了Keptn生命周期工具包能为我们做什么的基础知识，以及我们如何从第一天开始就受益于这些指标，以及我们如何通过使用声明性方式添加预/部署任务来增加对我们服务生命周期的控制。
- en: I strongly recommend you check the Keptn website and other more advanced mechanisms
    that they provide, such as Evaluations ([https://lifecycle.keptn.sh/docs-klt-v0.8.1/concepts/evaluations/](https://lifecycle.keptn.sh/docs-klt-v0.8.1/concepts/evaluations/)),
    which allows us to make decisions and even gate deployments that are not meeting
    certain requirements, such as increased memory consumption or too much CPU usage.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您查看Keptn网站以及他们提供的其他更高级的机制，例如评估（[https://lifecycle.keptn.sh/docs-klt-v0.8.1/concepts/evaluations/](https://lifecycle.keptn.sh/docs-klt-v0.8.1/concepts/evaluations/))，它允许我们做出决策，甚至可以阻止不符合某些要求（如内存消耗增加或CPU使用过多）的部署。
- en: While Keptn uses a completely different approach from the one described in section
    9.2, I strongly believe these approaches are complementary. I hope to see further
    integrations between Keptn and CloudEvents. If this topic interests you, I encourage
    you to join the conversation at [https://github.com/keptn/lifecycle-toolkit/issues/1841](https://github.com/keptn/lifecycle-toolkit/issues/1841).
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然Keptn使用的方法与第9.2节中描述的方法完全不同，但我坚信这些方法是互补的。我希望看到Keptn和CloudEvents之间的进一步集成。如果您对这个话题感兴趣，我鼓励您加入[https://github.com/keptn/lifecycle-toolkit/issues/1841](https://github.com/keptn/lifecycle-toolkit/issues/1841)的讨论。
- en: 9.4 What’s next on the platform engineering journey?
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 平台工程之旅的下一步是什么？
- en: The examples covered in this chapter highlighted the importance of measuring
    our technical decisions. In a good or bad way, each decision will affect all the
    teams involved in delivering software.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖的示例强调了衡量我们的技术决策的重要性。无论好坏，每个决策都会影响所有参与软件交付的团队。
- en: These metrics built into our platforms can help us measure improvement and justify
    investing in tools that facilitate our software delivery practices. If we want
    to include a new tool in our platform, you can test your assumptions and measure
    the effect of each tool or adopted methodology. It is quite a common practice
    to have these metrics accessible and visible for all your teams, so when things
    go wrong or a tool is not working as expected, you will have hard evidence to
    back up your claims.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们平台内建这些指标可以帮助我们衡量改进并证明投资于促进我们的软件交付实践的工具有价值。如果我们想在平台中包含一个新工具，你可以测试你的假设并衡量每个工具或采用的方法的影响。让这些指标对所有团队都易于访问和可见是一种相当常见的做法，这样当事情出错或工具不符合预期时，你将会有确凿的证据来支持你的主张。
- en: From a platform engineering perspective, I strongly recommend not leaving this
    topic until the end (as I did with this chapter in the book). Using tools like
    KLT, you can gain insights with a small investment and use standard monitoring
    techniques that are well-understood in the industry. Looking into CloudEvents
    and CDEvents is worth it, not only from a monitoring and metrics calculation perspective,
    but also for event-driven integrations with other tools and systems. Figure 9.24
    shows that by tapping into event sources from the tools that we are using in our
    golden paths, we can keep our teams informed about their decisions affect the
    entire software delivery chain.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 从平台工程的角度来看，我强烈建议不要将这个话题留到最后一刻（就像我在书中这一章所做的那样）。使用KLT等工具，你可以以较小的投入获得洞察力，并使用在业界被广泛理解的标准化监控技术。研究CloudEvents和CDEvents是值得的，不仅从监控和指标计算的角度来看，而且对于与其他工具和系统的基于事件的集成来说也是如此。图9.24显示，通过利用我们在黄金路径中使用的工具的事件源，我们可以让我们的团队能够了解他们的决策如何影响整个软件交付链。
- en: '![](../../OEBPS/Images/09-24.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图9.24](../../OEBPS/Images/09-24.png)'
- en: Figure 9.24 Golden paths and workflows provided by our platform are the best
    source of raw information for calculating the team’s performance metrics.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.24 我们平台提供的黄金路径和工作流程是计算团队性能指标的最佳原始信息来源。
- en: Ensuring that the basic metrics for your platform can be calculated will help
    your teams to think about end-to-end flows for each release—where the bottlenecks
    are and where they spend or waste most of their time. If the DORA metrics are
    too hard for your organization to implement, you can focus on measuring your platform’s
    golden paths or main workflows. For example, based on the examples provided in
    chapter 6, you can measure how much time it takes to provision a development environment,
    which capabilities are provided, and how often the team requests new instances,
    as shown in figure 9.25.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 确保您的平台的基本指标可以计算，这将帮助您的团队思考每个发布版本的端到端流程——瓶颈在哪里，他们花了或浪费了大部分时间在哪里。如果DORA指标对您的组织来说太难实施，您可以专注于衡量您平台上的黄金路径或主要工作流程。例如，根据第6章提供的示例，您可以衡量配置开发环境所需的时间，提供哪些功能，以及团队多久请求一次新实例，如图9.25所示。
- en: '![](../../OEBPS/Images/09-25.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-25.png)'
- en: Figure 9.25 Platform and application walking skeleton metrics
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.25 平台和应用行走骨架指标
- en: By collecting metrics, not only from customer applications but also from platform-specific
    workflows, like creating development environments, your teams will have full visibility
    of the tools they are using and how the changes in the tool affect and unlock
    the velocity of software delivery. Figure 9.26 shows a recap of our platform journey
    and how important these metrics are for our platform teams. Remember, if you are
    measuring your platform initiatives, your platforms will get better.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 通过收集指标，不仅来自客户应用程序，还来自平台特定的流程，如创建开发环境，您的团队将能够全面了解他们使用的工具以及工具的变化如何影响和释放软件交付的速度。图9.26展示了我们平台之旅的回顾以及这些指标对我们平台团队的重要性。记住，如果您正在衡量您的平台项目，您的平台将会变得更好。
- en: '![](../../OEBPS/Images/09-26.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/09-26.png)'
- en: Figure 9.26 Tapping into platform components to collect data and calculate metrics
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.26 利用平台组件收集数据和计算指标
- en: 9.5 Final thoughts
  id: totrans-245
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 最后的想法
- en: I hope that going through the examples of this book has given you enough hands-on
    experience to tackle real-life challenges. While the examples covered here are
    not exhaustive or deep in detail, the intention is to show a wide range of topics
    that platform engineering teams must deal with. The cloud-native space is constantly
    evolving, and the tools I evaluated when I started writing this book have completely
    changed in two years, pushing teams worldwide to be very flexible about their
    decisions. Making mistakes and reviewing decisions is part of the day-to-day work
    that platform engineers must do for small and large organizations.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望阅读这本书的示例已经给您提供了足够的实践经验来应对现实生活中的挑战。虽然这里涵盖的示例并不全面或深入，但目的是展示平台工程团队必须处理的各种主题。云原生空间正在不断演变，我在写这本书时评估的工具在两年内已经完全改变，推动全球团队在决策上非常灵活。犯错误和审查决策是平台工程师必须为小型和大型组织做的日常工作的一部分。
- en: Going back to the beginning of this book, platform engineers must encapsulate
    all these decisions behind platform APIs that they can maintain and evolve, so
    understanding the capabilities needed by different teams is key to having a successful
    platform engineering journey. Providing self-service capabilities and focusing
    on what your teams need should be heavy influencers on the platform engineers’
    priority lists.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 回到这本书的开头，平台工程师必须将这些决策封装在可以维护和演化的平台API之后，因此了解不同团队所需的能力对于成功进行平台工程之旅至关重要。提供自助服务功能和关注团队的需求应该是平台工程师优先事项列表上的重要影响因素。
- en: Unfortunately, I don’t have an unlimited number of pages or unlimited time to
    keep adding content to this book, but I did my best to include the topics and
    challenges I’ve seen organizations and communities facing while working in the
    cloud-native space. We have reached a point in the Kubernetes ecosystem where
    tools are maturing, and more projects are graduating, indicating that more and
    more companies are reusing tools instead of building their own.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，我没有无限的页面或无限的时间来不断添加内容到这本书中，但我尽力包括了我在云原生空间工作期间看到组织和社会面临的话题和挑战。我们已经到达了一个点，在Kubernetes生态系统中，工具正在成熟，更多项目正在毕业，这表明越来越多的公司正在重用工具而不是自己构建。
- en: I’ve intentionally omitted topics such as extending Kubernetes with custom controllers,
    because balancing what is built in-house for your platforms needs to be carefully
    defined by platform engineering teams. Creating and maintaining your extensions
    should be left to very special cases where no tools exist to solve a problem you
    are trying to solve. For the most common cases, as we have seen in this book,
    CI/CD, GitOps, infrastructure provisioning in the cloud, developer tools, platform-building
    tools, and other tools are mature enough for you to use and extend if necessary.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我故意省略了诸如使用自定义控制器扩展 Kubernetes 等主题，因为平衡为您的平台构建的内部构建内容需要由平台工程团队仔细定义。创建和维护您的扩展应留给非常特殊的情况，在这些情况下，没有工具可以解决您试图解决的问题。对于最常见的情况，正如我们在本书中所看到的，CI/CD、GitOps、云中的基础设施配置、开发者工具、平台构建工具和其他工具已经足够成熟，您可以使用并在必要时扩展它们。
- en: It was quite hard to leave topics such as service meshes, policy engines, observability,
    incident management, operations tools, and cloud development environments out
    of this book. There are wonderful projects that would require entire chapters
    to cover. But as a platform engineer, you must keep researching and keeping an
    eye on the cloud-native communities to see where new developments and projects
    can help your organization’s teams.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 将诸如服务网格、策略引擎、可观察性、事件管理、操作工具和云开发环境等主题排除在本书之外是非常困难的。有一些非常棒的项目需要整章来介绍。但作为一个平台工程师，您必须继续研究并关注云原生社区，以了解新的发展和项目如何帮助您的组织团队。
- en: I strongly recommend you engage with your local Kubernetes communities and be
    active in the open-source ecosystem. This not only gives you a great playground
    to learn, but it also helps you to make the right informed decisions about which
    technologies to adopt. Understanding how strong the communities behind these projects
    are is key to validating that they are solving a problem that first needs a solution
    and is common enough to be solved in a generic (non-organization) specific way.
    Tools like OSS Insight ([https://ossinsight.io/](https://ossinsight.io/)) provide
    enormous value for decision-making and ensure that if you invest time and resources
    in an open-source project, an active community will maintain your changes and
    improvements.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议您参与您当地的 Kubernetes 社区，并在开源生态系统中保持活跃。这不仅为您提供了一个极佳的学习游乐场，而且有助于您就采用哪些技术做出正确的知情决策。了解这些项目背后的社区有多强大，对于验证它们是否在解决首先需要解决方案且足够普遍以至于可以用通用（非组织）特定方式解决的问题至关重要。像
    OSS Insight ([https://ossinsight.io/](https://ossinsight.io/)) 这样的工具为决策提供了巨大的价值，并确保如果您在开源项目中投入时间和资源，一个活跃的社区将维护您的更改和改进。
- en: Finally, keep an eye on my blog ([https://salaboy.com](https://salaboy.com)),
    because further articles related to the book will be published to explore other
    topics that I consider important for platform engineering teams. If you are interested
    in contributing to open-source, expanding or fixing the examples provided in the
    book is a great way to get hands-on experience with all the tools most open-source
    projects use.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，请关注我的博客 ([https://salaboy.com](https://salaboy.com))，因为将会有更多与本书相关的文章发表，以探讨我认为对平台工程团队重要的其他主题。如果您对开源感兴趣，扩展或修复本书中提供的示例是一个很好的方式，可以亲身体验大多数开源项目使用的所有工具。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Using DORA metrics gives you a clear picture of how the organization delivers
    software in front of your customers. This can be used to understand bottlenecks
    resulting in improvements on the platforms we are building. Using the team’s performance
    metrics based on our software delivery practices will help you understand how
    your platform initiatives affect how teams’ work and the benefits to the overall
    organization.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 DORA 指标可以清楚地了解组织如何向客户交付软件。这可以用来理解导致我们在构建的平台上的改进的瓶颈。使用基于我们软件交付实践的团队绩效指标将帮助您了解您的平台倡议如何影响团队的工作以及整体组织的益处。
- en: CloudEvents standardize how we consume and emit events. Over the last couple
    of years, we have seen a rise in the adoption of CloudEvents by different projects
    in the CNCF landscape. This adoption allows us to rely on CloudEvents to get information
    about components and other systems that we can aggregate and collect helpful information
    that can be used for decision-making.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CloudEvents 标准化了我们消费和发射事件的方式。在过去的几年里，我们看到了 CNCF 生态系统中不同项目对 CloudEvents 的采用率上升。这种采用使我们能够依赖
    CloudEvents 来获取有关组件和其他系统的信息，我们可以聚合和收集有助于决策的有用信息。
- en: CDEvents provides a CloudEvents extension, a set of more specific CloudEvents
    related to continuous delivery software practices. While I expect the adoption
    of CDEvents to grow over time, we have seen how to map CloudEvents to CDEvents
    to calculate the DORA metrics. By using CDEvents as the base model to calculate
    these metrics, we can map any event source to contribute to the calculations of
    these metrics.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CDEvents 提供了一个 CloudEvents 扩展，这是一组与持续交付软件实践相关的更具体的 CloudEvents。虽然我预计 CDEvents
    的采用率会随着时间的推移而增长，但我们已经看到了如何将 CloudEvents 映射到 CDEvents 来计算 DORA 指标。通过使用 CDEvents
    作为计算这些指标的基础模型，我们可以将任何事件源映射以贡献这些指标的计算。
- en: If we can measure our platform, we will know what needs improvement and where
    the organization struggles with its delivery practices. This feedback loop provided
    by the metrics gives valuable information to platform teams in charge of continuously
    improving the tools and processes our teams use daily.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们能衡量我们的平台，我们将知道需要改进什么以及组织在交付实践中遇到哪些困难。这些指标提供的反馈循环为负责持续改进我们团队日常使用的工具和流程的平台团队提供了宝贵的信息。
- en: If you followed this chapter’s step-by-step tutorials, you gained hands-on experience
    with setting CloudEvent sources, monitoring deployments, and how CDEvents can
    help standardize information about our software delivery lifecycle. You also installed
    Keptn as a different approach to monitor your workloads and execute pre-/post-deployment
    tasks to validate that newer versions are working as expected.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你遵循了本章的逐步教程，你将获得实际操作经验，包括设置 CloudEvent 源、监控部署以及 CDEvents 如何帮助我们标准化关于软件交付生命周期的信息。你还安装了
    Keptn 作为另一种监控工作负载和执行预/后部署任务以验证新版本是否按预期工作的方法。
