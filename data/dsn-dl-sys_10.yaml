- en: 10 Path to production
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 生产之路
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Preliminary work and tasks before productionizing deep learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产化深度学习模型之前进行的初步工作和任务
- en: Productionizing deep learning models with a deep learning system
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用深度学习系统生产化深度学习模型
- en: Model deployment strategies for experimentation in production
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型部署策略在生产中的实验
- en: For the concluding chapter of the book, we think it makes sense to return to
    a high-level view and connect all the dots from previous chapters. We have now
    discussed in detail each service in a deep learning system. In this chapter, we
    will talk about how the services work together to support the deep learning *product
    development cycle* we introduced in chapter 1\. That cycle, if you remember, brings
    the efforts of research and data science all the way through productionization
    to the end products that customers use.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本书的最后一章，我们认为回到一个高层次的观点并连接之前章节中的所有要点是有意义的。我们目前已经详细讨论了深度学习系统中每个服务。在这一章中，我们将讨论这些服务如何协同工作以支持我们在第一章中引入的深度学习*产品开发周期*。如果你还记得，这个周期将研究工作和数据科学的所有努力从生产化一直带到客户使用的最终产品。
- en: 'As a reminder, figure 10.1, borrowed from chapter 1, shows the product development
    cycle. Our focus in this chapter will be on three phases that occur toward the
    end of the process: deep learning research, prototyping, and productionization.
    This focus means we’ll ignore the cycles of experimentation, testing, training,
    and exploration and look at how to take a final product from the research phase
    to the end product, making it ready to be released to the public.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 作为提醒，图10.1，来自第一章，展示了产品开发周期。在本章中，我们的重点将放在过程末尾发生的三个阶段：深度学习研究、原型设计和生产化。这种关注意味着我们将忽略实验、测试、训练和探索的周期，并探讨如何将最终产品从研究阶段带到最终产品，使其准备好公开发布。
- en: '![](../Images/10-01.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1](../Images/10-01.png)'
- en: Figure 10.1 This deep learning development cycle is a typical scenario for bringing
    deep learning from research to the finished product. Boxes (3), (4), and (5) are
    the focus of this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 这个深度学习开发周期是将深度学习从研究带到成品的一个典型场景。框(3)、(4)和(5)是本章的重点。
- en: Definition Productionization is the process of producing a product that is worthwhile
    and ready to be consumed by its users. Production worthiness is commonly defined
    as being able to serve customer requests, withstand a certain level of request
    load, and gracefully handle adverse situations such as malformed input and request
    overload.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 定义 生产化是将产品生产出来，使其值得用户消费的过程。生产性通常定义为能够满足客户请求、承受一定程度的请求负载，并优雅地处理不良情况，如输入格式错误和请求过载。
- en: As we’ve said, this chapter focuses on the path of the production cycle from
    research, through prototyping, to productionization. Let’s lift those three phases
    out of the typical development cycle shown in figure 10.1, so we can see them
    in greater detail. We’ve placed those phases in the next diagram, figure 10.2,
    and zoomed in on them to reveal the steps within each phase, as well as the ways
    the three phases connect to each other. Don’t let the complexity of this diagram
    alarm you! We will walk you through each phase, and each step, in this chapter.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所言，本章重点介绍了从研究到原型设计再到生产化的生产周期路径。让我们将这三个阶段从图10.1所示的典型开发周期中提取出来，以便我们可以更详细地观察它们。我们将这些阶段放在下一张图中，即图10.2，并放大它们以揭示每个阶段内的步骤以及三个阶段如何相互连接。不要让这张图的复杂性让你感到不安！在本章中，我们将逐步引导你了解每个阶段和每个步骤。
- en: '![](../Images/10-02.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图10.2](../Images/10-02.png)'
- en: Figure 10.2 Three major stages in a sample path to production. Research and
    prototyping go through many iterations before productionization.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 样本生产路径中的三个主要阶段。研究和原型设计在生产化之前会经历许多迭代。
- en: Let’s briefly review this diagram, as it will provide a preview of the chapter.
    The first two phases in figure 10.2 are research and prototyping. Both of these
    efforts require rapid iteration and turnaround from model training and experimentation.
    The primary interaction point in these phases (steps 1–8) is a notebooking environment.
    Using notebooks, researchers and data scientists invoke the dataset management
    service for tracking training datasets (during steps 2 and 6) and may use the
    training service and hyperparameter optimization library/service for model training
    and experimentation (during steps 4 and 8). We go over these phases in section
    10.1, ending at the point where training data shape and code become fairly stable
    and are ready to be productionized. In other words, the team has come up with
    the more-or-less final version, and it is ready to go through the final steps
    to release it to the public.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们简要回顾一下这个图表，因为它将提供本章的预览。图10.2中的前两个阶段是研究和原型设计。这两个阶段的工作都需要从模型训练和实验中进行快速迭代和调整。这些阶段的主要交互点是笔记环境。使用笔记，研究人员和数据科学家可以调用数据集管理服务来跟踪训练数据集（在第2步和第6步），并且可以使用训练服务和超参数优化库/服务来进行模型训练和实验（在第4步和第8步）。我们将在第10.1节中回顾这些阶段，直到训练数据形状和代码变得相当稳定，并准备好进行产业化。换句话说，团队已经提出了大致的最终版本，并准备进入最后一步，将其发布给公众。
- en: In section 10.2, we will pick up from the previous section and walk through
    the productionization of models, up to the point where models are served to production
    inference request traffic.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10.2节中，我们将从上一节的内容继续，探讨模型的产业化过程，直到模型被用于生产推理请求流量。
- en: Definition *Inference requests* are inputs generated by a user or an application
    against a trained model to produce inferences. Take visual recognition as an example.
    An inference request can be a picture of a cat. Using a trained visual recognition
    model, or an inference, a label in the form of the word *cat* can be generated.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 定义：**推理请求**是由用户或应用程序针对训练模型生成的输入，以产生推理。以视觉识别为例，一个推理请求可以是一张猫的图片。使用训练好的视觉识别模型或推理，可以生成一个标签，形式为单词**猫**。
- en: This section corresponds to the third and final phase in figure 10.2\. In productionization,
    pretty much every service in our system will come into play. The dataset management
    service manages training data; the workflow management service launches and tracks
    training workflows; the training service executes and manages model training jobs;
    the metadata and artifacts store contains and tracks code artifacts, trained models,
    and their metadata; and the model service serves trained models to inference request
    traffic.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分对应图10.2中的第三和最后一个阶段。在产业化过程中，我们系统中的几乎所有服务都将发挥作用。数据集管理服务管理训练数据；工作流程管理服务启动和跟踪训练工作流程；训练服务执行和管理模型训练作业；元数据和工件存储包含并跟踪代码工件、训练模型及其元数据；模型服务为推理请求流量提供服务。
- en: From productionization, we move to deployment. In section 10.3, we look at a
    number of model deployment strategies that support updating models to new versions
    in production. These strategies also support experimentation in production. The
    main focus here will be on the model service because this is where all inference
    requests are served.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从产业化转向部署。在第10.3节中，我们将探讨多种模型部署策略，这些策略支持在生产中更新模型到新版本，同时也支持生产中的实验。这里的重点将主要放在模型服务上，因为所有推理请求都是在这里被处理的。
- en: By walking through the full journey to production, we hope that you will be
    able to see how the first principles that we discussed in previous chapters affect
    the work of different parties that use the system to deliver deep learning features.
    The understanding that you gain from this chapter should help you adapt your own
    design to different situations. We will use the development of an image recognition
    product as an example to illustrate all the steps in action.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 通过完整的生产过程，我们希望您能够看到我们在前几章中讨论的第一性原理如何影响使用该系统提供深度学习功能的各个方面的相关工作。您从本章中获得的理解将帮助您根据不同情况调整自己的设计。我们将以图像识别产品的开发为例，来说明所有步骤的实际操作。
- en: 10.1 Preparing for productionization
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 准备产业化
- en: In this section, we will look at the journey of a deep learning model from before
    its birth to when it is ready to be productionized. In figure 10.3, we highlight
    the phases of deep learning research and prototyping from the larger deep learning
    development cycle (shown in figure 10.1). We will start from the deep learning
    research step, where model training algorithms are born. Not every organization
    performs deep learning research—some use out-of-the-box training algorithms—so
    feel free to skip this step if it does not apply to your situation.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨深度学习模型从诞生前到准备投入生产的过程。在图10.3中，我们突出了从更大的深度学习开发周期（如图10.1所示）中提炼出的深度学习研究和原型设计阶段。我们将从深度学习研究步骤开始，这是模型训练算法诞生的阶段。并非每个组织都进行深度学习研究——有些组织使用现成的训练算法——所以如果这一步不适用于您的情境，请随意跳过。
- en: '![](../Images/10-03.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-03.png)'
- en: Figure 10.3 Excerpt of the path to production in the research and prototyping
    phases
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.3 研究和原型设计阶段通往生产阶段的路径摘录
- en: After deep learning research, we proceed to prototyping. At this stage, we assume
    algorithms are ready to use for training models. A rapid and iterative process
    of data exploration and experimental model training forms the central part of
    this step. The goal of this step is to find the appropriate training data shape
    and develop a stable codebase for model training.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习研究之后，我们进入原型设计阶段。在这个阶段，我们假设算法已经准备好用于训练模型。数据探索和实验模型训练的快速迭代过程构成了这一步骤的核心。这一步骤的目标是找到合适的训练数据形状，并为模型训练开发一个稳定的代码库。
- en: 10.1.1 Research
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 研究
- en: New deep learning algorithms are invented, and existing ones are improved through
    research. Because peer-reviewed research requires reproducible results, model
    training data needs to be publicly accessible. Many public datasets, such as ImageNet
    for example, are available for research teams to use.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 新的深度学习算法通过研究被发明，现有的算法通过研究得到改进。由于同行评审的研究需要可重复的结果，模型训练数据需要公开可访问。许多公共数据集，例如ImageNet，可供研究团队使用。
- en: 'The notebooking environment, such as JupyterLab, is a popular choice among
    researchers for prototyping model training due to its interactivity and flexibility.
    Let’s go through some sample steps that a researcher may take during model training
    prototyping:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 笔记本环境，如JupyterLab，因其交互性和灵活性，是研究人员进行模型训练原型设计的流行选择。让我们回顾一下研究人员在模型训练原型设计过程中可能采取的一些示例步骤：
- en: Alice, a deep learning researcher, is working on improving a visual recognition
    algorithm. After working on her theories, she is ready to start prototyping.
  id: totrans-27
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝，一位深度学习研究员，正在努力改进一个视觉识别算法。在完成她的理论研究后，她准备开始原型设计。
- en: Alice creates a new notebook in JupyterLab.
  id: totrans-28
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝在JupyterLab中创建了一个新的笔记本。
- en: Alice wants to use the ImageNet dataset for training and benchmarking her algorithm.
    She may
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝想使用ImageNet数据集来训练和基准测试她的算法。她可能
- en: Write code to download the dataset to her notebook and store it in the dataset
    management service (chapter 2) for reuse.
  id: totrans-30
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 编写代码将数据集下载到她的笔记本中，并将其存储在数据集管理服务（第2章）中以供重用。
- en: Find that the dataset is already stored in the dataset management service and
    write code to use it as is.
  id: totrans-31
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 发现数据集已经存储在数据集管理服务中，并编写代码直接使用它。
- en: Alice starts coding up improvements on an existing visual recognition algorithm
    until it can produce experimental models locally within the notebook.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝开始编写代码，对现有的视觉识别算法进行改进，直到它能够在笔记本中本地生成实验模型。
- en: Alice tries to change some hyperparameters, train and test a few experimental
    models, and compare the metrics that they generate.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝尝试调整一些超参数，训练和测试几个实验模型，并比较它们生成的指标。
- en: Alice may further use hyperparameter optimization techniques (chapter 5) to
    run more experiments automatically to confirm that she has indeed made improvements
    to the existing algorithm.
  id: totrans-34
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝可能进一步使用超参数优化技术（第5章）来自动运行更多实验，以确认她确实改进了现有的算法。
- en: Alice publishes her results and packages her training code improvement as a
    library for others to use.
  id: totrans-35
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 爱丽丝发布她的结果，并将她的训练代码改进打包成一个库供他人使用。
- en: By using a versioned dataset for training, Alice is sure that the input training
    data of all her experimental model training runs are the same. She also uses a
    source-control management system, such as Git, to keep track of her code so that
    all experimental models can be traced back to a version of her code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用版本化的数据集进行训练，爱丽丝确保她所有实验模型训练运行的输入训练数据都是相同的。她还使用源代码管理工具，如Git，来跟踪她的代码，以便所有实验模型都可以追溯到她的代码版本。
- en: Notice that model training at this stage usually takes place locally at the
    computing node where the notebooking environment is hosted, so it is a good idea
    to allocate sufficient resources to these nodes. If training data is stored over
    the network, make sure the read speed does not become a bottleneck for model training.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个阶段，模型训练通常在托管笔记环境的计算节点上本地进行，因此为这些节点分配足够的资源是个好主意。如果训练数据存储在网络上，确保读取速度不会成为模型训练的瓶颈。
- en: 10.1.2 Prototyping
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 原型设计
- en: Prototyping is where research is bridged with real-world use cases. It is a
    practice of looking for the right combination of training data, algorithm, hyperparameter,
    and inference support to provide the right deep learning feature that meets product
    requirements.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 原型设计是将研究与实际应用案例相结合的地方。这是一种寻找合适的训练数据、算法、超参数和推理支持的组合，以提供满足产品要求的正确深度学习特征的实践。
- en: 'At this stage, it is still very common to find notebooking environments to
    be the top choice for data scientists and engineers due to the rapid iterative
    nature of prototyping. A fast turnaround is expected. Let’s walk through one possible
    scenario of prototyping:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，由于原型设计的快速迭代特性，笔记环境仍然是数据科学家和工程师的首选。期望快速周转。让我们通过一个可能的原型设计场景来了解一下：
- en: The model development team receives product requirements to improve the motion
    detection of a security camera product.
  id: totrans-41
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型开发团队接收了改进安全摄像头产品运动检测功能的产品需求。
- en: Based on the requirements, the team finds that Alice’s new vision recognition
    training algorithm may help improve motion detection.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据需求，团队发现爱丽丝的新视觉识别训练算法可能有助于改善运动检测。
- en: 'The team creates a new notebook and begins exploring data that is relevant
    for model training, given the set of algorithms that they picked:'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 团队创建一个新的笔记环境，并开始探索与所选算法相关的数据：
- en: The team may be able to use existing, collected data for model training if they
    happen to fit the problem that is being solved.
  id: totrans-44
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果他们收集到的现有数据恰好适合正在解决的问题，团队可能能够使用这些数据来训练模型。
- en: In some cases, the team may need to collect new data for training.
  id: totrans-45
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在某些情况下，团队可能需要收集新的数据用于训练。
- en: In most cases, transfer learning is applied at this stage, and the team picks
    one or more existing models as source models.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在大多数情况下，这个阶段会应用迁移学习，团队会选择一个或多个现有模型作为源模型。
- en: The team develops modeling code with algorithms and trains experimental models
    with collected data and source models.
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 团队使用算法开发建模代码，并使用收集到的数据和源模型训练实验模型。
- en: Experimental models are evaluated to see whether they yield satisfactory results.
    Steps 3 to 6 are repeated until the training data shape and code become stable.
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实验模型将被评估以查看它们是否产生令人满意的结果。步骤3到6会重复进行，直到训练数据的形状和代码变得稳定。
- en: We call steps 3 to 6 the exploratory loop. This loop corresponds to the iterate
    circles in the blow-up section of prototyping in figure 10.3\. When prototyping
    begins, the loop is iterated rapidly. The focus at this stage is to narrow down
    the training data shape and code.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将步骤3到6称为探索循环。这个循环对应于图10.3中原型设计放大部分的迭代循环。当原型设计开始时，循环会快速迭代。在这个阶段，重点是缩小训练数据的形状和代码。
- en: Once training data shape and code become stable, they will be ready for further
    tuning and optimization. The goal in this phase is to converge to a state where
    model training and inference code are ready to be packaged and deployed to production.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练数据的形状和代码变得稳定，它们将准备好进行进一步的调整和优化。这个阶段的目标是收敛到一个状态，其中模型训练和推理代码可以被打包并部署到生产环境中。
- en: 10.1.3 Key takeaways
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 主要收获
- en: 'We have walked through both the research and prototyping phases of our reference
    deep learning development cycle in figure 10.1\. Even though they serve different
    purposes, we see quite a bit of overlap in how they work with the deep learning
    system:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在图10.1中概述了我们的参考深度学习开发周期的研究和原型设计阶段。尽管它们服务于不同的目的，但我们看到它们在如何与深度学习系统交互方面有很多重叠：
- en: The notebooking environment is a common choice for both research and preproduction
    prototyping due to its high degree of interactivity and verbosity.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于笔记环境具有高度交互性和详尽性，它通常是研究和预生产原型设计的常见选择。
- en: Access to training data should be as wide and flexible as possible (to the limit
    of legality and compliance), which helps accelerate the data exploration process.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据的访问应尽可能广泛和灵活（在法律和合规性的极限内），这有助于加速数据探索过程。
- en: Sufficient computing resources should be allocated for model training so that
    turnaround time is short.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应为模型训练分配足够的计算资源，以便周转时间短。
- en: At a minimum, use a dataset management service and a source-control management
    system to keep track of the provenance of experimental models. In addition, use
    a metadata store to contain metrics and associate them with a training dataset
    and code for complete lineage tracking.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 至少，使用数据集管理服务和源代码管理系统来跟踪实验模型的来源。此外，使用元数据存储来包含指标，并将它们与训练数据集和代码关联起来，以实现完整的溯源跟踪。
- en: 10.2 Model productionization
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 模型生产化
- en: 'Before deep learning models can be integrated into an end product, they need
    to go through the process of productionization. There are certainly many interpretations
    of this term, but fundamentally:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在深度学习模型可以集成到最终产品之前，它们需要经历生产化的过程。当然，对这个术语有许多不同的解释，但本质上：
- en: Models need to serve production inference requests either from end products
    or end users.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型需要从最终产品或最终用户那里处理生产推理请求。
- en: Model serving should meet a predefined service level agreement, such as responding
    within 50 milliseconds or being available 99.999% of the time.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务应满足预定义的服务级别协议，例如在50毫秒内响应或99.999%的时间内可用。
- en: Production problems related to models should be easy to troubleshoot.
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与模型相关的生产问题应该易于调试。
- en: In this section, we will look at how deep learning models transition from living
    in a rather dynamic environment, such as a notebook, to a production environment
    where they are hit by various harsh conditions. Figure 10.4 shows the productionization
    phase relative to the rest of the development cycle. Let’s review the steps in
    this phase.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨深度学习模型如何从相对动态的环境，如笔记本，过渡到生产环境，在那里它们会受到各种恶劣条件的影响。图10.4显示了生产化阶段相对于整个开发周期的相对位置。让我们回顾这一阶段中的步骤。
- en: '![](../Images/10-04.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-04.png)'
- en: Figure 10.4 Excerpt of the path to production in the productionization phase
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.4 生产化阶段中通往生产的路径摘录
- en: 10.2.1 Code componentization
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 代码组件化
- en: As shown in the previous section, it is common during prototyping for training
    data preparation, model training, and inference code to exist in a single notebook.
    To productionize them into a deep learning system, we need to split them apart
    as separate components. One approach to splitting the components, or *code componentization*,
    is shown in figure 10.5.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在原型设计期间，训练数据准备、模型训练和推理代码通常存在于单个笔记本中。要将它们生产化为深度学习系统，我们需要将它们拆分为独立的组件。一种拆分组件的方法，或称为*代码组件化*，如图10.5所示。
- en: '![](../Images/10-05.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-05.png)'
- en: Figure 10.5 Componentizing code from a single notebook into multiple pieces
    that can be packaged separately. The first split happens where a trained model
    is the output. An optional second split happens where training data is the output.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 将单个笔记本中的代码拆分为多个可以单独打包的部分。第一次拆分发生在训练模型是输出的地方。可选的第二次拆分发生在训练数据是输出的地方。
- en: 'Let’s put the process in the figure into action. The first place to draw a
    line for separation in the code is where the model is the output. This should
    result in two pieces of code as follows:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们把图中的过程付诸实践。在代码中首先划分分离线的地方是模型作为输出的地方。这应该导致以下两段代码：
- en: Model training code that outputs a model
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出模型的模型训练代码
- en: Model inference code that takes a model and an inference request as input to
    produce an inference
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型推理代码，它接受模型和推理请求作为输入以产生推理
- en: 'Optionally, the model training code can be split as follows:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，模型训练代码可以按以下方式拆分：
- en: Training data transformation code, which takes raw data as input and output
    training data that can be consumed by model training code
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据转换代码，它接受原始数据作为输入并输出可以被模型训练代码消费的训练数据
- en: Model training code, which takes training data and trains a model as its output
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练代码，它接受训练数据并训练模型作为其输出
- en: If you have other model training code that will benefit from the same kind of
    prepared data, it is a good idea to perform this separation. Separation is also
    a good idea if your data preparation step needs to be executed on a different
    cadence from model training.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还有其他将从相同类型的数据准备中受益的模型训练代码，进行这种分离是个好主意。如果你的数据准备步骤需要以与模型训练不同的节奏执行，分离也是一个好主意。
- en: 10.2.2 Code packaging
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 代码打包
- en: Once code components are separated cleanly, they are ready to be packaged for
    deployment. To be able to run them on a training service (chapter 3), model service
    (chapter 6), and workflow service (chapter 9), we first need to make sure they
    follow the conventions set by these services.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦代码组件被干净地分离，它们就准备好打包以供部署。为了能够在训练服务（第3章）、模型服务（第6章）和工作流服务（第9章）上运行，我们首先需要确保它们遵循这些服务设定的规范。
- en: Model training code should be modified to fetch training data from the location
    indicated by an environment variable set by the training service. A similar convention
    should be followed in other components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练代码应修改为从由训练服务设置的环境变量指示的位置获取训练数据。其他组件也应遵循类似的规范。
- en: 'Model inference code should follow the convention of the model serving strategy
    of your choice:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 模型推理代码应遵循你选择的模型服务策略的规范：
- en: If you use direct model embedding, work with the team that embeds your model
    to make sure your inference code works.
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用直接模型嵌入，与嵌入你的模型的团队合作，确保你的推理代码能够正常工作。
- en: If you plan to serve your model with model service, make sure your inference
    code provides an interface with which the model service can communicate.
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你计划使用模型服务提供模型服务，确保你的推理代码提供了一个模型服务可以与之通信的接口。
- en: If you use a model server, you may not need model inference code as long as
    the model server can serve the model properly.
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你使用模型服务器，只要模型服务器能够正确地提供模型，你可能不需要模型推理代码。
- en: We package these code components as Docker containers so that they can be launched,
    accessed, and tracked by their respective host services. An example of how this
    is done can be found in appendix A. If special data transformation is needed,
    we can integrate data transformation code into the data management service.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些代码组件打包成Docker容器，以便它们可以被相应的宿主服务启动、访问和跟踪。这种做法的例子可以在附录A中找到。如果需要特殊的数据转换，我们可以将数据转换代码集成到数据管理服务中。
- en: 10.2.3 Code registration
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 代码注册
- en: Before training code and inference code can be used by the system, their packages
    must be registered and stored with the metadata and artifacts service. This provides
    the necessary link between the training code and the inference code. Let’s look
    at how they are related (figure 10.6).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练代码和推理代码可以被系统使用之前，它们的包必须注册并存储在元数据和工件服务中。这为训练代码和推理代码之间提供了必要的链接。让我们看看它们是如何相关的（图10.6）。
- en: '![](../Images/10-06.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-06.png)'
- en: Figure 10.6 A simple training and inference execution flow in a production deep
    learning system
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 生产级深度学习系统中简单的训练和推理执行流程
- en: Once the training and inference codes are packaged as containers (the training
    container and inference container in the diagram), they can be registered with
    the metadata and artifacts store using a common handle, such as `visual_recognition`
    like in the example shown in figure 10.6\. This helps system services find and
    use correct code containers when they receive requests that provide the same handle
    name. We will continue walking through the figure in the next few sections.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练和推理代码被打包成容器（图中的训练容器和推理容器），它们可以使用一个公共句柄（如图10.6中所示示例中的`visual_recognition`）注册到元数据和工件存储中。这有助于系统服务在接收到提供相同句柄名称的请求时找到并使用正确的代码容器。我们将在接下来的几节中继续讲解这个图。
- en: 10.2.4 Training workflow setup
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.4 训练工作流程设置
- en: We recommend setting up a training workflow even if you do not regularly train
    your model. The main reason is to provide reproducibility of the same model training
    flow in production. This is helpful when someone other than you needs to train
    a model and can use the flow that you set up. In some cases, the production environment
    is isolated, and going through a workflow that is set up in the production environment
    may be the only way to produce a model there. In figure 10.7, we’ve enlarged the
    model training portion of the previous diagram so you can see the details.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议即使您不经常训练模型，也设置一个训练工作流程。主要原因是为了在生产环境中提供相同模型训练流程的可重复性。当其他人需要训练模型并可以使用您设置的工作流程时，这非常有帮助。在某些情况下，生产环境是隔离的，通过在生产环境中设置的工作流程可能是唯一的生产模型的方法。在图10.7中，我们放大了之前图表中的模型训练部分，以便您可以看到细节。
- en: '![](../Images/10-07.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-07.png)'
- en: Figure 10.7 A typical production model training setup. The workflow service
    manages what, when, and how training flows are run. The training service runs
    model training jobs. The metadata and artifacts store provides training code,
    stores trained models, and associates them with metadata.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.7 典型的生产模型训练设置。工作流程服务管理训练流程的运行内容、时间和方式。训练服务运行模型训练作业。元数据和工件存储提供训练代码，存储训练好的模型，并将它们与元数据关联。
- en: Referring to figure 10.7, once a training workflow for `visual_recognition`
    is set up, training can be triggered to the training service. The training service
    uses the handle to look up the training code container to execute from the metadata
    and artifacts store. Once a model is trained, it saves the model to the metadata
    and artifacts store with the handle name.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 参考图10.7，一旦为`视觉识别`设置了训练工作流程，就可以触发训练服务。训练服务使用句柄从元数据和工件存储中查找训练代码容器来执行。一旦模型训练完成，它将模型以句柄名称保存到元数据和工件存储中。
- en: At this stage, it is also common to find hyperparameter optimization techniques
    being used to find optimal training hyperparameters during model training. If
    an HPO service is used, the workflow will talk to the HPO service instead of to
    the training service directly. If you need a reminder of how the HPO service works,
    refer to chapter 5.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，也常见使用超参数优化技术来在模型训练期间找到最优的训练超参数。如果使用HPO服务，工作流程将直接与HPO服务通信，而不是与训练服务通信。如果您需要HPO服务的工作原理的提醒，请参考第5章。
- en: 10.2.5 Model inferences
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.5 模型推理
- en: Once a model is trained and registered in the production environment, the next
    step is to make sure it can handle inference requests coming into our system at
    a certain rate and producing inferences within a certain latency. We can do so
    by sending inference requests to the model service. When the model service receives
    a request, it finds the handle name `visual_recognition` in the request and queries
    the metadata and artifacts store for the matching model inference container and
    model file. The model service can then use these artifacts together to produce
    an inference response. You can see this process in figure 10.8, which, again,
    is an enlarged version of the model serving portion of figure 10.6.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型在生产环境中训练并注册，下一步是确保它能够以一定的速率处理进入我们系统的推理请求，并在一定的延迟内产生推理结果。我们可以通过向模型服务发送推理请求来实现这一点。当模型服务收到请求时，它会查找请求中的句柄名称`visual_recognition`，并查询元数据和工件存储以获取匹配的模型推理容器和模型文件。然后，模型服务可以使用这些工件一起产生推理响应。您可以在图10.8中看到这个过程，这同样是对图10.6中模型服务部分的一个放大版本。
- en: '![](../Images/10-08.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-08.png)'
- en: Figure 10.8 A typical production model serving setup. When inference requests
    arrive at the model service, the service looks for the inference container and
    model using the metadata and artifacts store to produce inference responses.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.8 典型的生产模型服务设置。当推理请求到达模型服务时，服务使用元数据和工件存储查找推理容器和模型，以产生推理响应。
- en: If you use a model server, you may need a thin layer in front of it so that
    it knows where to obtain the model file. Some model server implementations support
    custom model manager implementation, which can also be used to make queries against
    the metadata and artifacts store to load the correct model.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用模型服务器，您可能需要在它前面添加一个薄层，以便它知道从哪里获取模型文件。一些模型服务器实现支持自定义模型管理器实现，这也可以用来对元数据和工件存储进行查询，以加载正确的模型。
- en: 10.2.6 Product integration
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.6 产品集成
- en: 'Once you get a proper inference response back from the model service, it is
    time to integrate a model service client into the product that will use these
    inferences. This is the final step in productionization, and we should make sure
    to check a few things before launching it to the end customer. Because we are
    improving the motion detection of our security camera product, we must integrate
    a model service client in the security camera video-processing backend that will
    request inferences from the newly improved model:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦从模型服务获得适当的推理响应，就是时候将模型服务客户端集成到将使用这些推理的产品中。这是生产化的最后一步，在将其推向最终客户之前，我们应该确保检查一些事项。因为我们正在改进我们的安全摄像头产品的运动检测，我们必须在安全摄像头视频处理后端集成一个模型服务客户端，该客户端将从新改进的模型请求推理：
- en: Make sure the inference response is consumable by the product using it.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保推理响应可以被使用它的产品消费。
- en: Stress test inferencing by sending inference requests at a rate that approximates
    the production traffic.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过以近似生产流量的速率发送推理请求来对推理进行压力测试。
- en: Test inferencing with malformed inference requests to make sure they do not
    break the model inference code or the model service.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用格式错误的推理请求进行推理测试，以确保它们不会破坏模型推理代码或模型服务。
- en: This is a very basic list of items to look for. Your organization may define
    more production readiness criteria that you need to fulfill before launching the
    integration. Besides system metrics that can tell us whether our model is serving
    inference requests properly, we should also set up business metrics that will
    tell us whether the model helps the business use case.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个非常基本的查找项目列表。您的组织可能定义了更多生产就绪标准，您需要在发布集成之前满足这些标准。除了可以告诉我们模型是否正确处理推理请求的系统指标之外，我们还应该设置业务指标，这些指标将告诉我们模型是否有助于业务用例。
- en: 10.3 Model deployment strategies
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 模型部署策略
- en: 'In the previous section, we went through a sample path from prototyping to
    production. That process assumed that the model was deployed for the first time,
    without an existing version of the model to replace. Once a model is being used
    in production, unless there is a maintenance window allowance, we usually need
    to use a model deployment strategy to ensure production inference request traffic
    is not disrupted. In fact, these model deployment strategies can also double as
    performing experimentation in production by using business metrics that we set
    up in the previous section. We will look at three strategies: canary, blue-green,
    and multi-armed bandit.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们经历了一个从原型到生产的示例路径。这个过程假设模型是首次部署，没有现有版本可以替换。一旦模型在生产中使用，除非有维护窗口的允许，我们通常需要使用模型部署策略来确保生产推理请求流量不被中断。实际上，这些模型部署策略也可以通过使用我们在上一节中设置的业务指标来在生产中进行实验。我们将探讨三种策略：金丝雀、蓝绿和多臂老虎机。
- en: 10.3.1 Canary deployment
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 金丝雀部署
- en: A canary deployment, similar to A/B testing, means deploying a new model to
    serve a small portion of production inference requests while keeping the old one
    to serve the remaining majority of requests. An example is shown in figure 10.9\.
    This requires the model service to support segmenting and routing a small portion
    of inference request traffic to the new model.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 金丝雀部署，类似于A/B测试，意味着部署一个新模型来服务一小部分生产推理请求，同时保留旧模型来服务剩余的大多数请求。一个例子在图10.9中展示。这要求模型服务支持将一小部分推理请求流量分割和路由到新模型。
- en: '![](../Images/10-09.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图10-9](../Images/10-09.png)'
- en: Figure 10.9 Canary deployment showing the redirection of a small portion of
    traffic to a new version of the model
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.9 金丝雀部署展示了将一小部分流量重定向到模型新版本的过程
- en: With this strategy, any potential adverse effect from deploying a new model
    is contained within a small portion of end users. Rolling back is rather straightforward
    by routing all inference request traffic back to the old model.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种策略，任何新模型部署可能带来的潜在负面影响都被限制在少量最终用户中。回滚操作相对简单，只需将所有推理请求流量路由回旧模型即可。
- en: A drawback to this approach is that you only get to know the performance of
    the model to a small portion of end users. Releasing the new model to serve all
    inference request traffic may have a different effect from what you observe serving
    only a small portion of the traffic.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的缺点是，您只能了解模型对一小部分最终用户的性能。将新模型发布以服务所有推理请求流量可能产生与仅服务一小部分流量观察到的不同效果。
- en: 10.3.2 Blue-green deployment
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 蓝绿部署
- en: In our context, a blue-green deployment means deploying a new model, routing
    all inference request traffic to the new model, and keeping the old model online
    until we have confidence that the new model’s performance meets expectations.
    Implementation-wise it is the simplest among all three strategies because there
    is no traffic splitting at all. All the service needs to do is point to the new
    model internally to serve all inference requests. Blue-green deployment is depicted
    in figure 10.10.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的上下文中，蓝绿部署意味着部署一个新模型，将所有推理请求流量路由到新模型，并保持旧模型在线，直到我们有信心新模型的性能符合预期。从实施的角度来看，这是三种策略中最简单的，因为它根本不需要流量分割。所有服务需要做的只是将内部服务指向新模型以处理所有推理请求。蓝绿部署在图10.10中展示。
- en: '![](../Images/10-10.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-10.png)'
- en: Figure 10.10 Blue–green deployment showing the direction of all traffic to either
    the old (blue) or the new (green) model
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.10展示了所有流量要么流向旧模型（蓝色），要么流向新模型（绿色）的蓝绿部署方向
- en: Not only is this strategy simple, but it also gives you a full picture of how
    the model performs when serving all end users. Rolling back is also straightforward.
    Simply point the model service back to the old model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 不仅这种策略简单，而且它还为你提供了一个全面了解模型在为所有最终用户提供服务时的表现的视角。回滚操作也很直接。只需将模型服务指向旧模型即可。
- en: The obvious downside to this approach is if something goes wrong with the new
    model, it affects all end users. This strategy may make sense when you are developing
    a new product feature based on a new model. As you iterate on training a better
    model over time, you may want to move away from this strategy, as end users would
    have built their expectations on having a stable experience.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的明显缺点是，如果新模型出现问题，它会影响所有最终用户。当你基于新模型开发新产品功能时，这种策略可能是有意义的。随着你随着时间的推移迭代训练更好的模型，你可能希望放弃这种策略，因为最终用户已经对稳定体验有了期望。
- en: 10.3.3 Multi-armed bandit deployment
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.3 多臂老虎机部署
- en: Multi-armed bandit (MAB) is the most complex deployment strategy among the three.
    MAB refers to a technique that continuously monitors multiple models’ performance
    and redirects more and more inference request traffic to the winning model over
    time. This uses the most elaborate implementation of the model service because
    it requires the service to understand model performance, which can be complicated
    depending on how your model performance metrics are defined. MAB deployment is
    illustrated in figure 10.11.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机（MAB）是三种部署策略中最复杂的。MAB指的是一种技术，它持续监控多个模型的性能，并随着时间的推移将越来越多的推理请求流量重定向到获胜的模型。这需要模型服务的最复杂实现，因为它要求服务理解模型性能，这可能会很复杂，具体取决于你的模型性能指标是如何定义的。MAB部署在图10.11中展示。
- en: '![](../Images/10-11.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-11.png)'
- en: Figure 10.11 Multi-armed bandit deployment showing traffic patterns on day 0
    and day 1\. Notice that model v2.0a is leading in terms of model performance on
    day 1 as it is receiving the most traffic.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.11展示了多臂老虎机部署在第一天和第二天上的流量模式。请注意，在第一天，由于模型v2.0a接收了最多的流量，它在模型性能方面处于领先地位。
- en: This strategy does come with an advantage, though, because it maximizes the
    benefits of the best-performing models within a set time frame, whereas with a
    canary deployment, you may only gain minimal benefits if the new model outperforms
    the old one. Notice that you should make sure the model service reports how the
    traffic split changes over time. This helps correlate with the models’ performance.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略确实有其优势，因为它在特定时间内最大化了最佳性能模型的效益，而与金丝雀部署相比，如果新模型优于旧模型，你可能只能获得微小的收益。请注意，你应该确保模型服务报告流量分割随时间的变化情况。这有助于与模型的性能相关联。
- en: Summary
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Deep learning research teams invent and improve deep learning algorithms that
    are used to train models.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习研究团队发明和改进深度学习算法，这些算法用于训练模型。
- en: Model development teams make use of existing algorithms and available data to
    train models that help solve a deep learning use case.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型开发团队利用现有的算法和可用数据来训练帮助解决深度学习用例的模型。
- en: Both research and prototyping require a high degree of interactivity with code
    development, data exploration, and visualization. Notebooking environments are
    popular choices for these teams.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 研究和原型设计都需要与代码开发、数据探索和可视化高度互动。对于这些团队来说，笔记簿环境是流行的选择。
- en: Dataset management service can be used during research and prototyping to help
    track training data used to train experimental models.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在研究和原型设计期间，可以使用数据集管理服务来帮助跟踪用于训练实验模型的训练数据。
- en: Once training data and code are stable enough, the first step to productionization
    is to package model training code, model inference code, and any source models.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦训练数据和代码足够稳定，将模型训练代码、模型推理代码以及任何源模型打包是生产化的第一步。
- en: These packages can be used by all services of the deep learning system to train,
    track, and serve models.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些包可以被深度学习系统的所有服务用于训练、跟踪和提供模型。
- en: Once a model training workflow is up and running and a satisfactory inference
    response is obtained, integration with the end-user product can begin.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一旦模型训练工作流程运行起来，并且获得了令人满意的推理响应，就可以开始与最终用户产品的集成。
- en: A model deployment strategy is required if serving inference requests cannot
    be interrupted.
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果无法中断推理请求的提供，则需要一个模型部署策略。
- en: Multiple model deployment strategies are available, and they can double as performing
    experimentation in production.
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可用多种模型部署策略，它们还可以在生产环境中进行实验。
