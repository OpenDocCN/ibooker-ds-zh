- en: 'Chapter 5\. Data Ingestion: Loading Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第5章 数据摄取：加载数据
- en: In [Chapter 4](ch04.xhtml#ch04), you extracted data from your desired source
    system. Now it’s time to complete the data ingestion by loading the data into
    your Redshift data warehouse. How you load depends on what the output of your
    data extraction looks like. In this section I will describe how to load data extracted
    into CSV files with the values corresponding to each column in a table, as well
    as extraction output containing CDC-formatted data.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#ch04)中，您从所需的源系统提取了数据。现在是通过将数据加载到Redshift数据仓库来完成数据摄取的时候了。如何加载取决于数据提取的输出。在本节中，我将描述如何将提取出的数据加载到CSV文件中，其中值对应于表中的每列，以及包含CDC格式数据的提取输出。
- en: Configuring an Amazon Redshift Warehouse as a Destination
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 配置Amazon Redshift数据仓库作为目的地
- en: If you’re using Amazon Redshift for your data warehouse, integration with S3
    for loading data after it has been extracted is quite simple. The first step is
    to create an IAM role for loading data if you don’t already have one.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您正在使用Amazon Redshift作为数据仓库，那么在提取数据后使用S3加载数据的集成就非常简单。第一步是为加载数据创建IAM角色（如果还没有）。
- en: Note
  id: totrans-4
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For instructions on setting up an Amazon Redshift cluster, check the latest
    [documentation and pricing, including free trials](https://oreil.ly/YSaxa).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何设置Amazon Redshift集群，请查看最新的[文档和定价信息，包括免费试用](https://oreil.ly/YSaxa)。
- en: 'To create the role, follow these instructions or check the [AWS documentation](https://oreil.ly/QEJzH)
    for the latest details:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建角色，请按照以下说明操作，或查看[AWS文档](https://oreil.ly/QEJzH)获取最新详情：
- en: Under the Services menu in the AWS console (or top navigation bar), navigate
    to IAM.
  id: totrans-7
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台的服务菜单（或顶部导航栏）下，导航到IAM。
- en: On the left navigation menu, select Roles, and then click the “Create role”
    button.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在左侧导航菜单中，选择“角色”，然后点击“创建角色”按钮。
- en: You’ll be presented with a list of AWS services to select from. Find and select
    Redshift.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将显示一个AWS服务列表供您选择。找到并选择Redshift。
- en: Under “Select your use case,” choose Redshift – Customizable.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“选择您的用例”下，选择Redshift – Customizable。
- en: On the next page (Attach permission policies), search for and select AmazonS3ReadOnlyAccess,
    and click Next.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在下一页（附加权限策略）上，搜索并选择AmazonS3ReadOnlyAccess，然后点击“下一步”。
- en: Give your role a name (for example, “RedshiftLoadRole”) and click “Create role.”
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给您的角色命名（例如，“RedshiftLoadRole”），然后点击“创建角色”。
- en: 'Click the name of the new role, and copy the *role Amazon resource name* (ARN)
    so you can use it in later in this chapter. You can find this later in the IAM
    console under the role properties as well. The ARN looks like this: `arn:aws:iam::*<aws-account-id>*:role/*<role-name>*`.'
  id: totrans-13
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击新角色的名称，并复制*角色Amazon资源名称*（ARN），以便您可以在本章后面使用。您还可以在IAM控制台的角色属性下找到它。ARN的格式如下：`arn:aws:iam::*<aws-account-id>*:role/*<role-name>*`。
- en: Now you can associate the IAM role you just created with your Redshift cluster.
    To do so, follow these steps or check the [Redshift documentation](https://oreil.ly/uHLEk)
    for more details.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以将刚创建的IAM角色与您的Redshift集群关联起来。要执行此操作，请按照以下步骤操作，或查看[Redshift文档](https://oreil.ly/uHLEk)获取更多详细信息。
- en: Note
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Your cluster will take a minute or two to apply the changes, but it will still
    be accessible during this time.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 您的集群将花费一到两分钟来应用这些更改，但在此期间仍然可以访问它。
- en: Go back to the AWS Services menu and go to Amazon Redshift.
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回AWS服务菜单，然后转到Amazon Redshift。
- en: In the navigation menu, select Clusters and select the cluster you want to load
    data into.
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导航菜单中，选择“集群”，然后选择要加载数据的集群。
- en: Under Actions, click “Manage IAM roles.”
  id: totrans-19
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在操作下，点击“管理IAM角色”。
- en: On the “Manage IAM roles” page that loads, you will be able to select your role
    in the “Available roles” drop-down. Then click “Add IAM role.”
  id: totrans-20
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载后会显示“管理IAM角色”页面，您可以在“可用角色”下拉菜单中选择您的角色。然后点击“添加IAM角色”。
- en: Click Done.
  id: totrans-21
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击完成。
- en: 'Finally, add another section to the *pipeline.conf* file that you created in
    [“Setting Up Cloud File Storage”](ch04.xhtml#setup-cloud-storage) with your Redshift
    credentials and the name of the IAM role you just created. You can find your Redshift
    cluster connection information on the AWS Redshift Console page:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在您创建的[pipeline.conf](ch04.xhtml#setup-cloud-storage)文件中添加另一部分，包括您的Redshift凭据和刚创建的IAM角色名称。您可以在AWS
    Redshift控制台页面上找到Redshift集群连接信息：
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Loading Data into a Redshift Warehouse
  id: totrans-24
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据加载到Redshift数据仓库
- en: Loading data into Redshift that’s been extracted and stored as values corresponding
    to each column in a table in your S3 bucket as a CSV file is relatively straightforward.
    Data in this format is most common and is the result of extracting data from a
    source such as a MySQL or MongoDB database. Each row in the CSV file to be loaded
    corresponds to a record to be loaded into the destination Redshift table, and
    each column in the CSV corresponds to the column in the destination table. If
    you extracted events from a MySQL binlog or other CDC log, see the following section
    for instructions on loading.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 将从S3存储为CSV文件中每列对应于Redshift表中每列的值提取和存储的数据加载到Redshift相对简单。 这种格式的数据最常见，是从诸如MySQL或MongoDB数据库之类的源中提取数据的结果。
    要加载到目标Redshift表中的每个CSV文件中的行对应于要加载到目标表中的记录，CSV中的每列对应于目标表中的列。 如果您从MySQL binlog或其他CDC日志中提取了事件，请参阅下一节有关加载说明。
- en: The most efficient way to load data from S3 into Redshift is to use the `COPY`
    command. `COPY` can be executed as a SQL statement in whatever SQL client you
    use to query your Redshift cluster or in a Python script using the Boto3 library.
    `COPY` appends the data you’re loading to the existing rows in the destination
    table.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据从S3加载到Redshift的最有效方法是使用`COPY`命令。 `COPY`可以作为SQL语句在查询Redshift集群的SQL客户端中执行，或者在使用Boto3库的Python脚本中执行。
    `COPY`将加载的数据追加到目标表的现有行中。
- en: 'The `COPY` command’s syntax is as follows. All bracketed ([]) items are optional:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: '`COPY`命令的语法如下。 所有方括号([])项都是可选的：'
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Note
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: You can learn more about additional options, and the COPY command in general,
    in the [AWS documentation](https://oreil.ly/0uWo8).
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[AWS文档](https://oreil.ly/0uWo8)中了解更多有关附加选项和`COPY`命令的一般信息。
- en: 'In its simplest form, using IAM role authorization as specified in [Chapter 4](ch04.xhtml#ch04)
    and a file in your S3 bucket looks something like this when run from a SQL client:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的形式中，使用IAM角色授权如[第4章](ch04.xhtml#ch04)中指定的，并且从SQL客户端运行时，S3存储桶中的文件看起来像这样：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'As you’ll recall from [“Configuring an Amazon Redshift Warehouse as a Destination”](#configure-amazon-redshift),
    the ARN is formatted like this:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您从[“配置Amazon Redshift Warehouse作为目标”](#configure-amazon-redshift)中回想起的那样，ARN的格式如下：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If you named the role `RedshiftLoadRole`, then the `COPY` command syntax looks
    like the following. Note that the numeric value in the ARN is specific to your
    AWS account:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将角色命名为`RedshiftLoadRole`，则`COPY`命令语法如下所示。 请注意，ARN中的数字值特定于您的AWS帐户：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: When executed, the contents of *file.csv* are appended to a table called `my_table`
    in the `my_schema` schema of your Redshift cluster.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，*file.csv*的内容将追加到Redshift集群中`my_schema`模式下名为`my_table`的表中。
- en: 'By default, the `COPY` command inserts data into the columns of the destination
    table in the same order as the fields in the input file. In other words, unless
    you specify otherwise, the order of the fields in the CSV you’re loading in this
    example should match the order of the columns in the destination table in Redshift.
    If you’d like to specify the column order, you can do so by adding the names of
    the destination columns in an order that matches your input file, as shown here:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，`COPY`命令将数据插入到目标表的列中，顺序与输入文件中字段的顺序相同。 换句话说，除非另有指定，否则您在此示例中加载的CSV中的字段顺序应与Redshift目标表中列的顺序匹配。
    如果您想指定列顺序，可以通过按与输入文件匹配的顺序添加目标列的名称来执行此操作，如下所示：
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: It’s also possible to use the Boto3 library to implement the `COPY` command
    in a Python script. In fact, following the template of the data extraction examples
    in [Chapter 4](ch04.xhtml#ch04), loading data via Python makes for a more standardized
    data pipeline.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用Boto3库在Python脚本中实现`COPY`命令。 实际上，按照[第4章](ch04.xhtml#ch04)中数据提取示例的模板，通过Python加载数据可以创建更标准化的数据流水线。
- en: 'To interact with the Redshift cluster you configured earlier in this chapter,
    you’ll need to install the `psycopg2` library:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 要与本章早期配置的Redshift集群进行交互，需要安装`psycopg2`库：
- en: '[PRE6]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Now you can start writing your Python script. Create a new file called *copy_to_redshift.py*
    and add the following three code blocks.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以开始编写Python脚本。 创建一个名为*copy_to_redshift.py*的新文件，并添加以下三个代码块。
- en: 'The first step is to import `boto3` to interact with the S3 bucket, `psycopg2`
    to run the `COPY` command on the Redshift cluster, and the `configparser` library
    to read the *pipeline.conf* file:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是导入`boto3`以与S3存储桶交互，`psycopg2`以在Redshift集群上运行`COPY`命令，以及`configparser`库以读取*pipeline.conf*文件：
- en: '[PRE7]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, connect to the Redshift cluster using the `psycopg2.connect` function
    and credentials stored in the *pipeline.conf* file:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`psycopg2.connect`函数和存储在*pipeline.conf*文件中的凭据连接到Redshift集群：
- en: '[PRE8]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Now you can execute the `COPY` command using a `psycopg2` `Cursor` object.
    Run the same `COPY` command that you ran manually earlier in the section, but
    instead of hard-coding the AWS account ID and IAM role name, load those values
    from the *pipeline.conf* file:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用`psycopg2`的`Cursor`对象执行`COPY`命令。运行与本节中手动运行的`COPY`命令相同的`COPY`命令，但是不要直接编码AWS账户ID和IAM角色名称，而是从*pipeline.conf*文件中加载这些值：
- en: '[PRE9]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Before you can run the script, you’ll need to create the destination table
    if it does not already exist. In this example, I’m loading data that was extracted
    into the *order_extract.csv* file in [“Full or Incremental MySQL Table Extraction”](ch04.xhtml#full-increment-msql).
    You can of course load whatever data you’d like. Just make sure the destination
    table has the structure to match. To create the destination table on your cluster,
    run the following SQL via the Redshift Query Editor or other application connected
    to your cluster:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行脚本之前，如果目标表还不存在，你需要先创建它。在本例中，我正在加载从[“完整或增量 MySQL 表抽取”](ch04.xhtml#full-increment-msql)中提取出的数据，该数据保存在*order_extract.csv*文件中。当然，你可以加载任何你想要的数据。只需确保目标表的结构匹配即可。要在你的集群上创建目标表，请通过Redshift查询编辑器或其他连接到你的集群的应用程序运行以下SQL：
- en: '[PRE10]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Finally, run the script as follows:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，按以下步骤运行脚本：
- en: '[PRE11]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Incremental Versus Full Loads
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 增量加载与完整加载对比
- en: In the previous code sample, the `COPY` command loaded the data from the extracted
    CSV file directly into a table in the Redshift cluster. If the data in the CSV
    file came from an incremental extract of an immutable source (as is the case with
    something like immutable event data or other “insert-only” dataset), then there’s
    nothing more to do. However, if the data in the CSV file contains updated records
    as well as inserts or the entire contents of the source table, then you have a
    bit more work to do, or at least considerations to take into account.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码示例中，`COPY`命令从提取的CSV文件直接加载数据到Redshift集群中的表中。如果CSV文件中的数据来自于不可变源的增量抽取（例如不可变事件数据或其他“仅插入”数据集），那么无需进行其他操作。然而，如果CSV文件中的数据包含更新的记录以及插入或源表的全部内容，则需要做更多工作，或者至少需要考虑一些因素。
- en: Take the case of the `Orders` table from [“Full or Incremental MySQL Table Extraction”](ch04.xhtml#full-increment-msql).
    That means the data you’re loading from the CSV file was extracted either in full
    or incrementally from the source MySQL table.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 以[“完整或增量 MySQL 表抽取”](ch04.xhtml#full-increment-msql)中的`Orders`表为例。这意味着你从CSV文件中加载的数据可能是从源MySQL表中完整或增量提取出来的。
- en: 'If the data was extracted in full, then you have one small addition to make
    to the loading script. Truncate the destination table in Redshift (using TRUNCATE)
    before you run the `COPY` operation. The updated code snippet looks like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是完整提取的，那么在运行`COPY`操作之前，需要对Redshift中的目标表进行截断（使用TRUNCATE）。更新后的代码片段如下所示：
- en: '[PRE12]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If the data was incrementally extracted, you don’t want to truncate the destination
    table. If you did, all you’d have left are the updated records from the last run
    of the extraction job. There are a few ways you can handle data extracted in this
    way, but the best is to keep things simple.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据是增量提取的，则不应该对目标表进行截断。如果截断了，那么最后一次运行抽取作业后剩下的只是更新的记录。有几种方法可以处理以这种方式提取的数据，但最好的方法是保持简单。
- en: In this case, you can simply load the data using the `COPY` command (no `TRUNCATE`!)
    and rely on the timestamp stating when the record was last updated to later identify
    which record is the latest or to look back at an historical record. For example,
    let’s say that a record in the source table was modified and thus present in the
    CSV file being loaded. After loading, you’d see something like [Table 5-1](#the_orders_table_in_redshift)
    in the Redshift destination table.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，你可以简单地使用`COPY`命令加载数据（不使用`TRUNCATE`！），并依靠时间戳来确定记录的最新状态或查看历史记录。例如，假设源表中的记录已修改并因此存在于正在加载的CSV文件中。加载完成后，你将在Redshift目标表中看到类似于[表5-1](#the_orders_table_in_redshift)的内容。
- en: Table 5-1\. The Orders table in Redshift
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-1\. Redshift中的订单表
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| OrderId | 订单状态 | 最后更新时间 |'
- en: '| --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Backordered | 2020-06-01 12:00:00 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已备货 | 2020-06-01 12:00:00 |'
- en: '| 1 | Shipped | 2020-06-09 12:00:25 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 12:00:25 |'
- en: As you can see in [Table 5-1](#the_orders_table_in_redshift), the order with
    an ID value of 1 is in the table twice. The first record existed prior to the
    latest load, and the second was just loaded from the CSV file. The first record
    came in due to an update to the record on 2020-06-01, when the order was in a
    `Backordered` state. It was updated again on 2020-06-09, when it `Shipped` and
    included in the last CSV file you loaded.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[表 5-1](#the_orders_table_in_redshift) 中所看到的，ID 值为 1 的订单在表中出现了两次。第一条记录存在于最新加载之前，并且第二条刚从
    CSV 文件中加载。第一条记录是由于在 2020-06-01 更新了记录时订单处于`Backordered`状态时创建的。它在 2020-06-09 再次更新时`Shipped`，并包含在你最后加载的
    CSV 文件中。
- en: From the standpoint of historical record keeping, it’s ideal to have both of
    these records in the destination table. Later in the transform phase of the pipeline,
    an analyst can choose to use either or both of the records, depending on the needs
    of a particular analysis. Perhaps they want to know how long the order was in
    a backordered state. They need both records to do that. If they want to know the
    current status of the order, they have that as well.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 从历史记录保存的角度来看，在目标表中拥有这两条记录是理想的。在流水线的转换阶段后，分析师可以根据特定分析的需求选择使用其中一条或两条记录。也许他们想知道订单在缺货状态下的持续时间。他们需要这两条记录。如果他们想知道订单的当前状态，他们也可以得到这些信息。
- en: Though it may feel uncomfortable to have multiple records for the same `OrderId`
    in the destination table, in this case it’s the right thing to do! The goal of
    data ingestion is to focus on extracting and loading data. What to do with the
    data is a job for the transform phase of a pipeline, explored in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在目标表中为相同的`OrderId`拥有多条记录可能会让人感到不舒服，但在这种情况下，这样做是正确的选择！数据摄取的目标是专注于提取和加载数据。如何处理数据是流水线转换阶段的任务，在[第
    6 章](ch06.xhtml#ch06) 中有所探讨。
- en: Loading Data Extracted from a CDC Log
  id: totrans-69
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从 CDC 日志中提取加载数据
- en: If your data was extracted via a CDC method, then there is one other consideration.
    Though it’s a similar process to loading data that was extracted incrementally,
    you’ll have access to not only inserted and updated records, but also deleted
    records.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的数据是通过 CDC 方法提取的，那么还有另一个考虑因素。尽管这与增量加载的数据类似，你不仅可以访问插入和更新的记录，还可以访问删除的记录。
- en: 'Take the example of the MySQL binary log extraction from [Chapter 4](ch04.xhtml#ch04).
    Recall that the output of the code sample was a CSV file named *orders_extract.csv*
    that was uploaded to the S3 bucket. Its contents looked like the following:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 以从[第 4 章](ch04.xhtml#ch04) 中提取的 MySQL 二进制日志为例。回想一下代码示例的输出是一个名为*orders_extract.csv*的
    CSV 文件，上传到了 S3 存储桶。其内容如下所示：
- en: '[PRE13]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Just like the incremental load example earlier in this section, there are two
    records for `OrderId` 1\. When loaded into the data warehouse, the data looks
    like it did back in [Table 5-1](#the_orders_table_in_redshift). However, unlike
    the previous example, *orders_extract.csv* contains a column for the event responsible
    for the record in the file. In this example, that’s either `insert` or `update`.
    If those were the only two event types, you could ignore the event field and end
    up with a table in Redshift that looks like [Table 5-1](#the_orders_table_in_redshift).
    From there, analysts would have access to both records when they build data models
    later in the pipeline. However, consider another version of *orders_extract.csv*
    with one more line included:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本节前面的增量加载示例一样，`OrderId` 1 有两条记录。当加载到数据仓库时，数据看起来就像[表 5-1](#the_orders_table_in_redshift)
    中的样子。然而，与之前的示例不同，*orders_extract.csv* 包含了文件中记录事件的列。在这个例子中，可以是`insert`或`update`。如果这些是唯一的两种事件类型，你可以忽略事件字段，并最终得到在
    Redshift 中看起来像[表 5-1](#the_orders_table_in_redshift) 的表格。从那里开始，分析师在后续流水线中构建数据模型时将可以访问这两条记录。然而，请考虑*orders_extract.csv*
    的另一个版本，其中包含了一行额外的内容：
- en: '[PRE14]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The third line shows that the order record was deleted the day after it was
    updated. In a full extraction, the record would have disappeared completely, and
    an incremental extraction would not have picked up the delete (see [“Extracting
    Data from a MySQL Database”](ch04.xhtml#extract-data-mysql) for a more detailed
    explanation). With CDC, however, the delete event was picked up and included in
    the CSV file.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第三行显示订单记录在更新后的第二天被删除了。在完全提取中，该记录将完全消失，并且增量提取不会捕获到删除操作（请参阅[“从 MySQL 数据库提取数据”](ch04.xhtml#extract-data-mysql)以获取更详细的解释）。然而，使用
    CDC，删除事件被捕获并包含在 CSV 文件中。
- en: To accommodate deleted records, it’s necessary to add a column to the destination
    table in the Redshift warehouse to store the event type. [Table 5-2](#the_orders_table_with_eventype_in_redshift)
    shows what the extended version of the `Orders` looks like.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了容纳已删除的记录，需要在 Redshift 仓库的目标表中添加一个列来存储事件类型。[表 5-2](#the_orders_table_with_eventype_in_redshift)
    展示了 `Orders` 的扩展版本的外观。
- en: Table 5-2\. The Orders table with EventType in Redshift
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 表 5-2\. Redshift 中具有 EventType 的 Orders 表
- en: '| EventType | OrderId | OrderStatus | LastUpdated |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| EventType | OrderId | OrderStatus | LastUpdated |'
- en: '| --- | --- | --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| insert | 1 | Backordered | 2020-06-01 12:00:00 |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| insert | 1 | 已备货 | 2020-06-01 12:00:00 |'
- en: '| update | 1 | Shipped | 2020-06-09 12:00:25 |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| update | 1 | 已发货 | 2020-06-09 12:00:25 |'
- en: '| delete | 1 | Shipped | 2020-06-10 9:05:12 |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| delete | 1 | 已发货 | 2020-06-10 9:05:12 |'
- en: Once again, the goal of data ingestion in a data pipeline is to efficiently
    extract data from a source and load it into a destination. The transform step
    in a pipeline is where the logic to model the data for a specific use case resides.
    [Chapter 6](ch06.xhtml#ch06) discusses how to model data loaded via a CDC ingestion,
    such as this example.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 数据管道中数据摄取的目标是有效地从源提取数据并加载到目标中。管道中的转换步骤是针对特定用例对数据建模的逻辑所在。[第六章](ch06.xhtml#ch06)讨论了如何对通过
    CDC 摄取加载的数据进行建模，例如本例。
- en: Configuring a Snowflake Warehouse as a Destination
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将 Snowflake 仓库配置为目标
- en: 'If you’re using Snowflake as your data warehouse, you have three options for
    configuring access to the S3 bucket from your Snowflake instance:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您将 Snowflake 作为数据仓库，有三种选项可配置从 Snowflake 实例访问 S3 存储桶的访问权限：
- en: Configure a Snowflake storage integration
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Snowflake 存储集成
- en: Configure an AWS IAM role
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 AWS IAM 角色
- en: Configure an AWS IAM user
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 AWS IAM 用户
- en: Of the three, the first is recommended because of how seamless using a Snowflake
    storage integration is when later interacting with the S3 bucket from Snowflake.
    Because the specifics of the configuration include a number of steps, it’s best
    to refer to the [latest Snowflake documentation](https://oreil.ly/RCoMT) on the
    topic.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，推荐第一种，因为在稍后从 Snowflake 中与 S3 存储桶交互时，使用 Snowflake 存储集成是多么无缝。由于配置的具体步骤包括多个步骤，建议参考有关该主题的[最新
    Snowflake 文档](https://oreil.ly/RCoMT)。
- en: In the final step of the configuration you’ll create an *external stage*. An
    external stage is an object that points to an external storage location so Snowflake
    can access it. The S3 bucket you created earlier will serve as that location.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在配置的最后一步中，您将创建一个 *外部 stage*。外部 stage 是指向外部存储位置的对象，以便 Snowflake 可以访问它。您之前创建的
    S3 存储桶将作为该位置。
- en: 'Before you create the stage, it’s handy to define a `FILE FORMAT` in Snowflake
    that you can both refer to for the stage and later use for similar file formats.
    Because the examples in this chapter create pipe-delimited CSV files, create the
    following `FILE FORMAT`:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建 stage 之前，最好在 Snowflake 中定义一个 `FILE FORMAT`，您既可以为 stage 引用，也可以稍后用于类似的文件格式。由于本章的示例创建了管道分隔的
    CSV 文件，因此创建以下 `FILE FORMAT`：
- en: '[PRE15]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When you create the stage for the bucket per the final step of the Snowflake
    documentation, the syntax will look something like this:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 当根据 Snowflake 文档的最后一步创建桶的 stage 时，语法将类似于：
- en: '[PRE16]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: In [“Loading Data into a Snowflake Data Warehouse”](#load-data-snowflake), you’ll
    be using the stage to load data that’s been extracted and stored in the S3 bucket
    into Snowflake.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [“将数据加载到Snowflake数据仓库”](#load-data-snowflake) 中，您将使用 stage 将从 S3 存储桶中提取并存储的数据加载到
    Snowflake 中。
- en: Finally, you’ll need to add a section to the *pipeline.conf* file with Snowflake
    login credentials. Note that the user you specify must have `USAGE` permission
    on the stage you just created. Also, the `account_name` value must be formatted
    based on your cloud provider and the region where the account is located. For
    example, if your account is named `snowflake_acct1` and hosted in the US East
    (Ohio) region of AWS, the `account_name` value will be `snowflake_acct1.us-east-2.aws`.
    Because this value will be used to connect to Snowflake via Python using the `snowflake-connector-python`
    library, you can refer to the [library documentation](https://oreil.ly/ijcHw)
    for help determining the proper value for your `account_name`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您需要向*pipeline.conf*文件中添加一个部分，其中包含Snowflake登录凭据。请注意，您指定的用户必须在您刚创建的阶段上具有`USAGE`权限。此外，`account_name`值必须根据您的云提供商和帐户所在地区进行格式化。例如，如果您的帐户名为`snowflake_acct1`，托管在AWS的美国东部（俄亥俄州）区域，则`account_name`值将是`snowflake_acct1.us-east-2.aws`。因为此值将用于使用Python通过`snowflake-connector-python`库连接到Snowflake，您可以参考[库文档](https://oreil.ly/ijcHw)以获取有关确定`account_name`正确值的帮助。
- en: 'Here is the section to add to *pipeline.conf*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是添加到*pipeline.conf*的部分：
- en: '[PRE17]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Loading Data into a Snowflake Data Warehouse
  id: totrans-99
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 将数据加载到Snowflake数据仓库
- en: Loading data into Snowflake follows a nearly identical pattern to the previous
    sections on loading data into Redshift. As such, I will not discuss the specifics
    of handing full, incremental, or CDC data extracts. Rather, I will describe the
    syntax of loading data from a file that has been extracted.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到Snowflake与之前加载数据到Redshift的模式几乎相同。因此，我不会讨论处理全量、增量或CDC数据提取的具体细节。相反，我将描述从已提取文件加载数据的语法。
- en: The mechanism for loading data into Snowflake is the `COPY INTO` command. `COPY
    INTO` loads the contents of a file or multiple files into a table in the Snowflake
    warehouse. You can read more about the advanced usage and options of the command
    in the [Snowflake documentation](https://oreil.ly/E3KG5).
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据加载到Snowflake的机制是`COPY INTO`命令。`COPY INTO`将一个或多个文件的内容加载到Snowflake仓库中的表中。您可以在[Snowflake文档](https://oreil.ly/E3KG5)中了解有关该命令的高级用法和选项。
- en: Note
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Snowflake also has a data integration service called *Snowpipe* that enables
    loading data from files as soon as they’re available in a Snowflake stage like
    the one used in the example in this section. You can use Snowpipe to continuously
    load data rather than scheduling a bulk load via the `COPY INTO` command.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: Snowflake还有一个名为*Snowpipe*的数据集成服务，允许从文件加载数据，这些文件一旦在Snowflake阶段（如本节示例中使用的阶段）中可用即可。您可以使用Snowpipe连续加载数据，而不是通过`COPY
    INTO`命令安排批量加载。
- en: 'Each extraction example in [Chapter 4](ch04.xhtml#ch04) wrote a CSV file to
    an S3 bucket. In [“Configuring a Snowflake Warehouse as a Destination”](#configure-snowflake-warehouse),
    you created a Snowflake stage called `my_s3_stage` that is linked to that bucket.
    Now, using the `COPY INTO` command, you can load the file into a Snowflake table
    as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](ch04.xhtml#ch04)中的每个提取示例都将CSV文件写入了一个S3存储桶。在[“将Snowflake仓库配置为目的地”](#configure-snowflake-warehouse)中，您创建了一个名为`my_s3_stage`的Snowflake阶段，该阶段链接到该存储桶。现在，使用`COPY
    INTO`命令，您可以将文件加载到Snowflake表中，如下所示：
- en: '[PRE18]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'It’s also possible to load multiple files into the table at once. In some cases,
    data is extracted into more than one file due to volume or as a result of multiple
    extraction job runs since the last load. If the files have a consistent naming
    pattern (and they should!), you can load them all using the `pattern` parameter:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以一次加载多个文件到表中。在某些情况下，由于数据量或自上次加载以来的多个提取作业运行的结果，数据被提取为多个文件。如果文件具有一致的命名模式（而且应该有！），您可以使用`pattern`参数加载它们所有：
- en: '[PRE19]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Note
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The format of the file to be loaded was set when you created the Snowflake stage
    (a pipe-delimited CSV); thus, you do not need to state it in the `COPY INTO` command
    syntax.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 文件的格式在创建Snowflake阶段时已设置（管道分隔的CSV文件），因此不需要在`COPY INTO`命令语法中指定它。
- en: Now that you know how the `COPY INTO` command works, it’s time to write a short
    Python script that can be scheduled and executed to automate the load in a pipeline.
    See [Chapter 7](ch07.xhtml#ch07) for more details on this and other pipeline orchestration
    techniques.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了`COPY INTO`命令的工作原理，现在是时候编写一个简短的Python脚本，以便安排并执行它，从而自动化管道中的加载。详情请参阅[第7章](ch07.xhtml#ch07)，了解有关此及其他管道编排技术的更多详细信息。
- en: 'First, you’ll need to install a Python library to connect to your Snowflake
    instance. You can do so using `pip`:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要安装一个Python库来连接到您的Snowflake实例。您可以使用`pip`完成此操作：
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Now, you can write a simple Python script to connect to your Snowflake instance
    and use `COPY INTO` to load the contents of the CSV file into a destination table:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以编写一个简单的Python脚本连接到您的Snowflake实例，并使用`COPY INTO`将CSV文件的内容加载到目标表中：
- en: '[PRE21]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Using Your File Storage as a Data Lake
  id: totrans-115
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用文件存储作为数据湖
- en: There are times when it makes sense to extract data from an S3 bucket (or other
    cloud storage) and not load into a data warehouse. Data stored in a structured
    or semistructured form in this way is often referred to as a *data lake*.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，从S3存储桶（或其他云存储）中提取数据而不加载到数据仓库中是有意义的。以这种方式以结构化或半结构化形式存储的数据通常称为*数据湖*。
- en: Unlike a data warehouse, a data lake stores data in many formats in a raw and
    sometimes unstructured form. It’s cheaper to store, but is not optimized for querying
    in the same way that structured data in a warehouse is.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据仓库不同，数据湖以原始且有时非结构化的形式存储数据，可以存储多种格式的数据。它的存储成本较低，但并不像数据仓库中的结构化数据那样优化用于查询。
- en: However, in recent years, tools have come along to make querying data in a data
    lake far more accessible and often transparent to a user comfortable with SQL.
    For example, Amazon Athena is an AWS service that allows a user to query data
    stored in S3 using SQL. Amazon Redshift Spectrum is a service that allows Redshift
    to access data in S3 as an *external table* and reference it in queries alongside
    tables in the Redshift warehouse. Other cloud providers and products have similar
    functionality.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，近年来出现了一些工具，使得对数据湖中的数据进行查询变得更加可访问，通常对于熟悉SQL的用户来说也更加透明。例如，Amazon Athena是一个AWS服务，允许用户使用SQL查询存储在S3中的数据。Amazon
    Redshift Spectrum是一种服务，允许Redshift访问S3中的数据作为*外部表*，并在与Redshift仓库中的表一起查询时引用它。其他云提供商和产品也具有类似的功能。
- en: When should you consider using such an approach rather than structuring and
    loading the data into your warehouse? There are a few situations that stand out.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在何时应考虑使用这种方法而不是结构化和加载数据到您的仓库中？有几种情况显得特别突出。
- en: Storing large amounts of data in a cloud storage–based data lake is less expensive
    than storing it in a warehouse (this is not true for Snowflake data lakes that
    use the same storage as Snowflake data warehouses). In addition, because it’s
    unstructured or semistructured data (no predefined schema), making changes to
    the types or properties of data stored is far easier than modifying a warehouse
    schema. JSON documents are an example of the type of semistructured data that
    you might encounter in a data lake. If a data structure is frequently changing,
    you may consider storing it in a data lake, at least for the time being.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在基于云存储的数据湖中存储大量数据比在仓库中存储便宜（对于使用与Snowflake数据仓库相同存储的Snowflake数据湖不适用）。此外，由于它是非结构化或半结构化数据（没有预定义的模式），因此更改存储的数据类型或属性要比修改仓库模式容易得多。JSON文档是您可能在数据湖中遇到的半结构化数据类型的示例。如果数据结构经常变化，您可能会考虑将其暂时存储在数据湖中。
- en: During the exploration phase of a data science or machine learning project,
    the data scientist or machine learning engineer might not know yet exactly what
    “shape” they need their data in. By granting them access to data in a lake in
    its raw form, they can explore the data and determine what attributes of the data
    they need to make use of. Once they know, you can determine whether it makes sense
    to load the data into a table in the warehouse and gain the query optimization
    that comes with doing so.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据科学或机器学习项目的探索阶段，数据科学家或机器学习工程师可能尚不清楚他们需要数据呈现的确切“形状”。通过以原始形式访问湖中的数据，他们可以探索数据，并确定需要利用数据的哪些属性。一旦确定，您可以确定是否有意义将数据加载到仓库中的表中，并获得随之而来的查询优化。
- en: In reality, many organization have both data lakes and data warehouses in their
    data infrastructure. Over time, the two have become complementary, rather than
    competing, solutions.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，许多组织在其数据基础设施中既有数据湖又有数据仓库。随着时间的推移，这两者已经成为互补而非竞争的解决方案。
- en: Open Source Frameworks
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 开源框架
- en: As you’ve noticed by now, there are repetitive steps in each data ingestion
    (both in the extract and load steps). As such, numerous frameworks that provide
    the core functionally and connections to common data sources and destinations
    have sprung up in recent years. Some are open source, as discussed in this section,
    while the next section provides an overview of some popular commercial products
    for data ingestions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您现在已经注意到的，每个数据摄取中（提取和加载步骤都有）都存在重复的步骤。因此，近年来出现了许多框架，提供核心功能和与常见数据源和目标的连接。正如本节所讨论的，其中一些是开源的，而下一节则概述了一些流行的商业数据摄取产品。
- en: One popular open source framework is called [Singer](https://www.singer.io).
    Written in Python, Singer uses *taps* to extract data from a source and streams
    it in JSON to a *target*. For example, if you want to extract data from a MySQL
    database and load it into a Google BigQuery data warehouse, you’d use the MySQL
    tap and the BigQuery target.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的开源框架称为[Singer](https://www.singer.io)。Singer用Python编写，使用*taps*从源提取数据，并以JSON流方式传输到*target*。例如，如果您想从MySQL数据库提取数据并将其加载到Google
    BigQuery数据仓库中，您将使用MySQL tap和BigQuery target。
- en: As with the code samples in this chapter, with Singer you’ll still need to use
    a separate orchestration framework to schedule and coordinate data ingestions
    (see [Chapter 7](ch07.xhtml#ch07) for more). However, whether you use Singer or
    another framework, you have a lot to gain from a well-built foundation to get
    you up and running quickly.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 就像本章中的代码示例一样，使用Singer仍然需要使用单独的编排框架来调度和协调数据摄取（有关更多信息，请参见[第7章](ch07.xhtml#ch07)）。然而，无论您使用Singer还是其他框架，通过一个良好构建的基础，可以快速启动和运行。
- en: Being an open source project, there are a wide number of taps and targets available
    (see some of the most popular in [Table 5-3](#popular_singer_taps_and_targets)),
    and you can contribute your own back to the project as well. Singer is well documented
    and has active [Slack](https://oreil.ly/tBQs0) and [GitHub](https://oreil.ly/nLJgF)
    communities.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一个开源项目，有大量可用的taps和targets（请参阅[表5-3](#popular_singer_taps_and_targets)中一些最受欢迎的）。您还可以向项目贡献自己的内容。Singer有着良好的文档和活跃的[Slack](https://oreil.ly/tBQs0)和[GitHub](https://oreil.ly/nLJgF)社区。
- en: Table 5-3\. Popular singer taps and targets
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 表5-3\. 流行的singer taps和targets
- en: '| Taps | Targets |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| Taps | Targets |'
- en: '| --- | --- |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| Google Analytics | CSV |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| Google Analytics | CSV |'
- en: '| Jira | Google BigQuery |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| Jira | Google BigQuery |'
- en: '| MySQL | PostgreSQL |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| MySQL | PostgreSQL |'
- en: '| PostgreSQL | Amazon Redshift |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '| PostgreSQL | Amazon Redshift |'
- en: '| Salesforce | Snowflake |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| Salesforce | Snowflake |'
- en: Commercial Alternatives
  id: totrans-136
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 商业替代方案
- en: There are several commercial cloud-hosted products that make many common data
    ingestions possible without writing a single line of code. They also have built-in
    scheduling and job orchestration. Of course, this all comes at a cost.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 有几种商业云托管产品可以实现许多常见的数据摄取，而无需编写一行代码。它们还具有内置的调度和作业编排功能。当然，这一切都是有代价的。
- en: Two of the most popular commercial tools for data ingestion are [Stitch](https://www.stitchdata.com)
    and [Fivetran](https://fivetran.com). Both are fully web-based and accessible
    to data engineers as well as other data professionals on a data team. They provide
    hundreds of prebuilt “connectors” to common data sources, such as Salesforce,
    HubSpot, Google Analytics, GitHub, and more. You can also ingest data from MySQL,
    Postgres, and other databases. Support for Amazon Redshift, Snowflake, and other
    data warehouses is built in as well.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 两个最受欢迎的商业数据摄取工具是[Stitch](https://www.stitchdata.com)和[Fivetran](https://fivetran.com)。两者都是完全基于Web的，数据工程师以及数据团队中的其他数据专业人员都可以访问。它们提供数百个预构建的“连接器”，用于常见数据源，如Salesforce、HubSpot、Google
    Analytics、GitHub等。您还可以从MySQL、Postgres和其他数据库中获取数据。还内置了对Amazon Redshift、Snowflake等数据仓库的支持。
- en: If you ingest data from sources that are supported, you’ll save a great deal
    of time in building a new data ingestion. In addition, as [Chapter 7](ch07.xhtml#ch07)
    outlines in detail, scheduling and orchestrating data ingestions aren’t trivial
    tasks. With Stitch and Fivetran, you’ll be able to build, schedule, and alert
    on broken ingestion pipelines right in your browser.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从支持的源中提取数据，将节省大量构建新数据摄取的时间。此外，正如[第7章](ch07.xhtml#ch07)中详细概述的那样，调度和编排数据摄取并不是微不足道的任务。使用Stitch和Fivetran，您可以在浏览器中构建、调度和对中断的摄取管道进行警报。
- en: Selected connectors on both platforms also support things like job execution
    timeouts, duplicate data handling, source system schema changes, and more. If
    you’re building ingestions on your own, you’ll need to take all that into account
    yourself.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 两个平台上选择的连接器还支持作业执行超时、重复数据处理、源系统架构更改等功能。如果你自行构建摄取，你需要自己考虑所有这些因素。
- en: 'Of course, there are some trade-offs:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这其中也存在一些权衡：
- en: Cost
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: Both Stitch and Fivetran have volume-based pricing models. Though they differ
    in how they measure volume and what other features they include in each pricing
    tier, at the end of the day what you pay is based on how much data you ingest.
    If you have a number of high-volume data sources to ingest from, it will cost
    you.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: Stitch 和 Fivetran 都采用基于数据量的定价模型。虽然它们在量化数据和每个定价层中包含的其他功能方面有所不同，但归根结底，你支付的费用取决于你摄取的数据量。如果你有多个高容量数据源需要摄取，费用将会增加。
- en: Vendor lock-in
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 锁定供应商
- en: Once you invest in a vendor, you’ll be facing a nontrivial amount of work to
    migrate to another tool or product, should you decide to move on in the future.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你投资了某个供应商，未来要迁移到另一个工具或产品将需要大量工作。
- en: Customization requires coding
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 定制化需要编码
- en: If the source system you want to ingest from doesn’t have a prebuilt connector,
    you’ll have to write a little code on your own. For Stitch, that means writing
    a custom Singer tap (see the previous section), and with Fivetran, you’ll need
    to write cloud functions using AWS Lambda, Azure Function, or Google Cloud Functions.
    If you have many custom data sources, such as custom-built REST APIs, you’ll end
    up having to write custom code and then pay for Stitch or Fivetran to run it.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想从的源系统没有预构建的连接器，你就得自己写一些代码。对于 Stitch 来说，这意味着编写自定义的 Singer tap（参见前一节），而对于
    Fivetran，则需要使用 AWS Lambda、Azure Function 或 Google Cloud Functions 编写云函数。如果你有许多自定义数据源，比如自定义构建的
    REST API，你最终会不得不编写自定义代码，然后支付 Stitch 或 Fivetran 运行它。
- en: Security and privacy
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 安全和隐私
- en: Though both products serve as passthroughs for your data and don’t store it
    for long periods of time, they still technically have access to both your source
    systems as well as destinations (usually data warehouses or data lakes). Both
    Fivetran and Stitch meet high standards for security; however, some organizations
    are reluctant to utilize them due to risk tolerance, regulatory requirements,
    potential liability, and the overhead of reviewing and approving a new data processor.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两款产品作为数据的透传器使用，并不会长时间存储数据，但它们仍然技术上可以访问你的源系统以及目标地点（通常是数据仓库或数据湖）。虽然 Fivetran
    和 Stitch 都符合高安全标准，但一些组织由于风险容忍度、监管要求、潜在责任以及审查和批准新数据处理器的额外开销而不愿使用它们。
- en: The choice to build or buy is complex and unique to each organization and use
    case. It’s also worth keeping in mind that some organizations use a mix of custom
    code and a product like Fivetran or Stitch for data ingestions. For example, it
    might be most cost effective to write custom code to handle some high-volume ingestions
    that would be costly to run in a commercial platform but also worth the cost of
    using Stitch or Fivetran for ingestions with prebuilt, vendor-supported connectors.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 选择构建或购买对每个组织和使用案例来说都是复杂且独特的。值得注意的是，一些组织同时使用自定义代码和像 Fivetran 或 Stitch 这样的产品进行数据摄取。例如，编写自定义代码来处理一些在商业平台上运行成本高昂的高容量摄取可能更具成本效益，但使用
    Stitch 或 Fivetran 进行具有预构建、供应商支持连接器的摄取也是值得的。
- en: If you do choose a mix of custom and commercial tools, keep in mind you’ll need
    to consider how you standardize things such as logging, alerting, and dependency
    management. Later chapters of this book discuss those subjects and touch on the
    challenges of managing pipelines that span multiple platforms.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择了自定义和商业工具的混合使用，记住你需要考虑如何标准化日志记录、警报和依赖管理等事项。本书的后续章节将讨论这些主题，并触及跨多个平台管理流水线的挑战。
