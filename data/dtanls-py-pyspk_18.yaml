- en: Appendix A. Solutions to the exercises
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录 A. 练习解答
- en: This appendix contains the solutions to the exercises presented in the book.
    If you have not solved them, I encourage you to do so. Reading the API doc and
    searching in other chapters of the book is fair game, but merely reading the answers
    won’t do any good!
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本附录包含书中提出的练习的解决方案。如果你还没有解决它们，我鼓励你去做。阅读 API 文档和在其他章节中搜索是公平的游戏，但仅仅阅读答案不会有任何好处！
- en: 'Unless specified, each code block assumes the following:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 除非指定，否则每个代码块都假设以下内容：
- en: '[PRE0]'
  id: totrans-3
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Chapter 2
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章
- en: Exercise 2.1
  id: totrans-5
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.1
- en: 'Eleven records. `explode()` generates one record for each element of each array
    of the exploded column. The `numbers` column contains two arrays, one with five
    elements, one with six: 5 + 6 = 11.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 十一条记录。`explode()` 为每个展开列的每个数组元素生成一个记录。`numbers` 列包含两个数组，一个有五个元素，一个有六个：5 + 6
    = 11。
- en: '[PRE1]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Exercise 2.2
  id: totrans-8
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.2
- en: Using a list comprehension (see appendix C), we can iterate over each `dtypes`
    of our data frame. Because `dtypes` is a list of tuple, we can destructure to
    `x,` `y`, where `x` maps to the name of the columns and `y` to the type. We only
    need to keep the ones where `y` `!=` `"string"`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 使用列表推导（参见附录 C），我们可以遍历数据框中的每个 `dtypes`。因为 `dtypes` 是一个元组列表，我们可以解构为 `x,` `y`，其中
    `x` 映射到列名，`y` 映射到类型。我们只需要保留 `y` `!=` `"string"` 的那些。
- en: '[PRE2]'
  id: totrans-10
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Exercise 2.3
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.3
- en: Rather than creating a column through function application and then renaming
    it, we can use `alias()` directly on the resulting column.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在结果列上使用 `alias()`，而不是通过函数应用创建列然后重命名它。
- en: '[PRE3]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Exercise 2.4
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.4
- en: 'The data frame has one column after the first `select()` statement: `maximum_value`.
    We then try to select `key` and `max_value`, which fails.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一个 `select()` 语句之后，数据框只剩下一列：`maximum_value`。然后我们尝试选择 `key` 和 `max_value`，这失败了。
- en: Exercise 2.5
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.5
- en: a)
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: a)
- en: We just have to filter our column (using `filter()` or `where()`) to keep the
    `word` that is not equal (`!=`) to `"is"`.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需使用 `filter()` 或 `where()` 过滤我们的列，以保留不等于 `"is"` 的 `word`。
- en: '[PRE4]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: b)
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: b)
- en: '[PRE5]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Exercise 2.6
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.6
- en: Remember that the negation sign in PySpark is `~`.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 记住 PySpark 中的否定符号是 `~`。
- en: '[PRE6]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Exercise 2.7
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.7
- en: 'By assigning `book.printSchema()` to our `book` variable, we lose the data
    frame: `printSchema()` returns `None`, which we assign to `book`. `NoneType` does
    not have a `select()` method.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `book.printSchema()` 赋值给我们的 `book` 变量，我们失去了数据框：`printSchema()` 返回 `None`，我们将其赋值给
    `book`。`NoneType` 没有提供 `select()` 方法。
- en: Chapter 3
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第三章
- en: Exercise 3.1
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.1
- en: 'Answer: b'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 答案：b
- en: (a) is missing the `"length"` alias, so the `groupby()` clause won’t work. (c)
    groups by a column that does not exist.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: (a) 缺少 `"length"` 别名，因此 `groupby()` 子句将不起作用。(c) 按一个不存在的列进行分组。
- en: '[PRE7]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Exercise 3.2
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.2
- en: 'PySpark does not necessarily preserve order during operations. In this specific
    case, we do the following:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 在操作过程中不一定保留顺序。在这种情况下，我们做以下操作：
- en: Order by the column `count`.
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按列 `count` 排序。
- en: Group by the length of each (unique) word.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 按每个（唯一）单词的长度分组。
- en: '`count` again, generating a new `count` column (different from the one in 1).'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次使用 `count`，生成一个新的 `count` 列（与 1 中的不同）。
- en: The `count` column we ordered by in 1 does not exist anymore.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在 1 中按列 `count` 排序的 `count` 列不再存在。
- en: Exercise 3.3
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.3
- en: '**1)**'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '**1)**'
- en: '[PRE8]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ The data frame had only one record per word through the groupby()/count().
    Counting the number of records again will give the number of records, or the number
    of distinct words.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过 `groupby()/count()`，数据框每个单词只有一条记录。再次计数将给出记录数，或不同单词的数量。
- en: Alternatively, we can remove the `groupby()/count()` and replace it with a `distinct()`
    that will keep only distinct records.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以移除 `groupby()/count()` 并用 `distinct()` 替换它，这将只保留不同的记录。
- en: '[PRE9]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ distinct() removes the need for a groupby()/count().
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `distinct()` 去除了对 `groupby()/count()` 的需求。
- en: '**2)**'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '**2)**'
- en: '[PRE10]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Exercise 3.4
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.4
- en: After the `groupby()/count()`, we can use the `count` column like any other
    column. In this case, we filter the `count` values to keep only the `1`s.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `groupby()/count()` 之后，我们可以像使用任何其他列一样使用 `count` 列。在这种情况下，我们过滤 `count` 值，只保留
    `1`。
- en: '[PRE11]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ We only keep the records with a count value of 1.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们只保留计数值为 1 的记录。
- en: Exercise 3.5
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.5
- en: 'Assuming that `results` is available (from `words_count_submit.py`):'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 假设 `results` 可用（来自 `words_count_submit.py`）：
- en: '[PRE12]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 1)
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 1)
- en: '[PRE13]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 2)
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 2)
- en: '[PRE14]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Exercise 3.6
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 3.6
- en: After using `groupby()/count()`, we get a data frame. The `DataFrame` object
    has no `sum()` method (see chapter 5 for a broader introduction to the `GroupedData`
    object).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 `groupby()/count()` 之后，我们得到一个数据框。`DataFrame` 对象没有 `sum()` 方法（有关 `GroupedData`
    对象的更广泛介绍，请参阅第 5 章）。
- en: Chapter 4
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第四章
- en: Exercise 4.1
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.1
- en: '[PRE15]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '**Explanation:**'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释：**'
- en: '`sample.csv` is the name of the file we want to ingest.'
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`sample.csv` 是我们想要摄取的文件名。'
- en: The record delimiter is the comma. Since we are asked to provide a value there,
    I pass the comma character `,` explicitly, knowing it is the default.
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录分隔符是逗号。由于我们被要求提供一个值，我明确传递逗号字符 `,`，知道它是默认的。
- en: The file has a header row, so I input `header=True`.
  id: totrans-66
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该文件有一个标题行，所以我输入 `header=True`。
- en: The quoting character is the dollar sign character, `$`, so I pass it as an
    argument to `quote`.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 引用字符是美元符号 `$`，所以我将其作为参数传递给 `quote`。
- en: Finally, since inferring the schema is nice, I pass `True` to `inferSchema`.
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，由于推断模式很有用，我传递 `True` 给 `inferSchema`。
- en: Exercise 4.2
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.2
- en: c
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: c
- en: '**Explanation:** Both `item` and `UPC` match as columns, while `prices` doesn’t.
    PySpark will ignore the nonexistent columns passed to `drop()`.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释**：`item` 和 `UPC` 作为列匹配，而 `prices` 不匹配。PySpark 将忽略传递给 `drop()` 的不存在列。'
- en: Exercise 4.3
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.3
- en: '[PRE16]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Two major differences:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 两个主要区别：
- en: PySpark put everything into a single string column, since it did not encounter
    the default delimiter (`,`) consistently in the records.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 将所有内容放入单个字符串列中，因为它在记录中一致地没有遇到默认分隔符 (`,`).
- en: It names the record `_c0`, the default convention when it has no information
    about column names.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将记录命名为 `_c0`，这是当没有关于列名称信息时的默认约定。
- en: Exercise 4.4
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 4.4
- en: '[PRE17]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '**Explanation:** I use the list comprehension trick on the data frame’s columns,
    using the filtering clause `if` `not` `x.endswith("ID")` to keep only the columns
    that do not end with “ID.”'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释**：我在数据帧的列上使用列表推导技巧，使用过滤子句 `if not x.endswith("ID")` 来保留不以“ID”结尾的列。'
- en: Chapter 5
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第五章
- en: Exercise 5.1
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.1
- en: This is a left join between one and two.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个一与二之间的左连接。
- en: '**Explanation:** A `left_semi` join only keeps the records on the left, where
    the `my_column` value is also present in the `my_column` column on the `right.`
    A `left_anti` join is the opposite: it will keep the records not present. Unioning
    these results in the original data frame, `left`.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释**：`left_semi` 连接只保留左边的记录，其中 `my_column` 的值也在右边的 `my_column` 列中。`left_anti`
    连接是相反的：它将保留不存在的记录。将这些结果与原始数据帧 `left` 联合。'
- en: Exercise 5.2
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.2
- en: 'c: inner'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 'c: inner'
- en: Exercise 5.3
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.3
- en: 'b: right'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'b: right'
- en: Exercise 5.4
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.4
- en: '[PRE18]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '**Explanation:** When performing an inner join, all the records from the left
    data frame are kept in the joined data frame. If the predicate is unsuccessful,
    then the column values from the right table are all set to `null` for the affected
    records. We just have to filter to keep only the unmatched records and then select
    the `left["my_` `column"]` column.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '**解释**：在进行内部连接时，左数据帧的所有记录都保留在连接的数据帧中。如果谓词失败，则右表中的列值对于受影响的记录都设置为 `null`。我们只需过滤以保留不匹配的记录，然后选择
    `left["my_"]` 列。'
- en: Exercise 5.5
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.5
- en: First, we need to read the `Call_Signs.csv` file. Since the delimiter is the
    comma, we can keep the default parameterization for the reader, with the exception
    of `header=True`. Then we see that both tables share `LogIdentifierID`, which
    we can equi-join over.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要读取 `Call_Signs.csv` 文件。由于分隔符是逗号，我们可以保留读取器的默认参数化，除了 `header=True`。然后我们看到两个表共享
    `LogIdentifierID`，我们可以通过它进行等值连接。
- en: '[PRE19]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ We can do an equi-join on those columns.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们可以在这些列上执行等值连接。
- en: Exercise 5.6
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.6
- en: We can reuse the same plumbing for generating our final answer, slightly changing
    the `when()` chause to remove `"PRC"` from the “pure” (1.0) commercials. Then
    we chain an additional `when()` to account for the different treatment of `"PRC"`.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重用相同的管道来生成我们的最终答案，稍微改变 `when()` 子句以从“纯” (1.0) 广告中移除 `"PRC"`。然后我们再链式添加一个
    `when()` 来处理 `"PRC"` 的不同处理方式。
- en: '[PRE20]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ We separate the when() clauses into separate variable for neatness (optional).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将 `when()` 子句分开到单独的变量中以提高清晰度（可选）。
- en: ❷ Here is the second when() clause for "PRC".
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这是 "PRC" 的第二个 `when()` 子句。
- en: Exercise 5.7
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 5.7
- en: We can create our `round()` predicate directly in the `groupby()` clause, making
    sure to `alias` our new columns.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接在 `groupby()` 子句中创建我们的 `round()` 谓词，并确保为我们的新列 `alias`。
- en: '[PRE21]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Chapter 6
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第六章
- en: Exercise 6.1
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 6.1
- en: For this solution, I create a dictionary copy of the JSON document that I then
    dump using the `json.dump` function. Because `spark.read.json` can only read files,
    we use a neat trick where we create an RDD (see chapter 8) that can be read via
    our `spark.read.json` (see [http://mng.bz/g41E](http://mng.bz/g41E) for more information).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个解决方案，我创建了一个JSON文档的字典副本，然后使用`json.dump`函数将其导出。因为`spark.read.json`只能读取文件，所以我们使用一个巧妙的方法，创建一个可以由我们的`spark.read.json`读取的RDD（有关更多信息，请参阅第8章[http://mng.bz/g41E](http://mng.bz/g41E)）。
- en: '[PRE22]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Exercise 6.2
  id: totrans-107
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.2
- en: Although we have a number in our list/array of `keywords`, PySpark will default
    to the lowest common denominator and create an array of `strings`. The answer
    is the same as exercise 6.1.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们在列表/数组中的`keywords`有多个，但PySpark会默认选择最低公倍数并创建一个`strings`数组。答案与练习6.1相同。
- en: '[PRE23]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Exercise 6.3
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.3
- en: A `StructType()` will take a list of `StructField()`, not the types directly.
    We need to wrap `T.StringType()`, `T.LongType()`, and `T.LongType()` into a `StructField()`,
    giving them an appropriate name.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`StructType()`将接受一个`StructField()`列表，而不是直接接受类型。我们需要将`T.StringType()`、`T.LongType()`和`T.LongType()`包装在一个`StructField()`中，并给它们一个合适的名称。'
- en: Exercise 6.4
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.4
- en: To illustrate the problem, let’s create a column, `info.status`, in a data frame
    that already has an `info` struct, containing a `status` field. By creating an
    `info.status`, the column becomes unreachable because Spark defaults to picking
    `status` from the `info` struct column.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明问题，让我们在一个已经包含`info`结构体，其中包含`status`字段的`DataFrame`中创建一个名为`info.status`的列。通过创建`info.status`，该列变得不可访问，因为Spark默认从`info`结构体列中选取`status`。
- en: '[PRE24]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Ended, not Wrong
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 结束，非错误
- en: Exercise 6.5
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.5
- en: '[PRE25]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Exercise 6.6
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.6
- en: Depending on your Spark version, the interval might be displayed differently.
    When selecting an element of a column of type `array` of `struct`, you get an
    array of elements without the need to `explode`. We can then use `array_min()`
    and `array_max()` to compute the first and last `airdate`.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你的Spark版本，间隔可能显示不同。当选择类型为`array`的`struct`类型的列的一个元素时，你将得到一个不需要`explode`的元素数组。然后我们可以使用`array_min()`和`array_max()`来计算第一个和最后一个`airdate`。
- en: '[PRE26]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Exercise 6.7
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.7
- en: '[PRE27]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Exercise 6.8
  id: totrans-123
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习6.8
- en: '[PRE28]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Chapter 7
  id: totrans-125
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章
- en: Exercise 7.1
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.1
- en: b
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: b
- en: Note that d could work as well, but it returns an integer value and not a data
    frame like the example.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，d也可以工作，但它返回一个整数值，而不是像示例中那样的数据帧。
- en: '[PRE29]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Exercise 7.2
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.2
- en: Looking at the `failures` table, we can see that we `count` the records where
    `failure` `=` `1`. A useful trick when working with Booleans (`True`/`False`)
    as integers (`1`/`0`) is that we can combine the filtering and counting clauses
    into a `sum` operation (the count of all records equaling 1 is the same as the
    sum of all records). Using the `sum` operation also removes the need for using
    `fillna` for the values from the left join because we don’t filter any records.
    Because of this, the code is greatly simplified.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 观察到`failures`表，我们可以看到我们`count`了`failure`等于`1`的记录。当使用布尔值（`True`/`False`）作为整数（`1`/`0`）工作时，一个有用的技巧是我们可以将过滤和计数子句组合成一个`sum`操作（所有记录等于1的计数与所有记录的总和相同）。使用`sum`操作也消除了使用`fillna`对左连接值的需求，因为我们没有过滤任何记录。正因为如此，代码得到了极大的简化。
- en: '[PRE30]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Exercise 7.3
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.3
- en: 'This problem warrants a little more thought. When looking at the reliability
    of each drive model, we can use drive days as a unit and count the failures versus
    drive days. Now we need to compute the age of each drive. We can break down this
    function into a few components:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这个问题需要更多的思考。当我们查看每个驱动器模型的可靠性时，我们可以使用驱动天数作为单位，并计算失败与驱动天数之间的比较。现在我们需要计算每个驱动器的年龄。我们可以将这个函数分解成几个组件：
- en: Create a date of death for each drive.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个驱动器创建一个死亡日期。
- en: Compute the age of each drive.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算每个驱动器的年龄。
- en: Group by model; get the average age.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按车型分组；获取平均年龄。
- en: Return the drives per average age.
  id: totrans-138
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回平均年龄的驱动器数量。
- en: I provide the code from the raw data frame `data`.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我提供了从原始数据帧`data`中的代码。
- en: '[PRE31]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Exercise 7.4
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.4
- en: In SQL, you can use the `extract(day` `from` `COLUMN)` to get the day out of
    a date. This is equivalent to the `dayofmonth()` function.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在SQL中，你可以使用`extract(day from COLUMN)`来从一个日期中获取天。这相当于`dayofmonth()`函数。
- en: '[PRE32]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Exercise 7.5
  id: totrans-144
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习7.5
- en: To solve this problem, we need to extract the most common capacity in bytes
    and then keep only the top record for each capacity (if there is more than one,
    we keep both). We do this by counting all the capacities for a given drive model
    and keeping only the one that occurrs most often (see `most_common_capacity` and
    `capacity_count`). Following this, we join the most common capacity to our original
    data.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，我们需要提取字节中最常见的容量，然后只保留每个容量（如果有多个，则保留两个）的最高记录。我们通过计算给定驱动模型的全部容量并只保留出现次数最多的一个来实现这一点（参见`most_common_capacity`和`capacity_count`）。随后，我们将最常见的容量与我们的原始数据合并。
- en: '[PRE33]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Chapter 8
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章
- en: Exercise 8.1
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.1
- en: Let’s create a simple RDD to solve the exercise.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的RDD来解决这个练习。
- en: '[PRE34]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '**Explanation:**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**说明：**'
- en: I start by mapping each element to the value 1, regardless of the input. The
    `_` in the lambda function doesn’t bind the elements because we don’t process
    the element; we just care that it exists. After the `map` operation, we have an
    RDD containing only the value `1`. We can `reduce(sum)` to get the sum of all
    the `1`s, which yields the number of elements in the RDD.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我首先将每个元素映射到值1，无论输入如何。lambda函数中的`_`不绑定元素，因为我们不处理元素；我们只关心它是否存在。在`map`操作之后，我们有一个只包含值`1`的RDD。我们可以使用`reduce(sum)`来获取所有`1`的总和，这给出了RDD中元素的数量。
- en: Exercise 8.2
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.2
- en: a
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: a
- en: Filter will drop any values when the predicate (the function passed as an argument)
    returns a falsey value. In Python, `0`, `None`, and empty collections are falsey.
    Since the predicate returns the value unchanged, `0`, `None`, `[]` and `0.0` are
    falsey and filtered out, leaving only `[1]` as the answer.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当谓词（作为参数传递的函数）返回一个假值时，过滤器将丢弃任何值。在Python中，`0`、`None`和空集合是假值。由于谓词返回值不变，`0`、`None`、`[]`和`0.0`是假值并被过滤掉，只留下`[1]`作为答案。
- en: Exercise 8.3
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.3
- en: Because C and K are the same (minus a constant), and F and R are the same (minus
    another constant), we can reduce the decision tree of our function. If we pass
    a string value that isn’t `F`, `C`, `K`, or `R` to the `from_temp` and/or `to_temp`,
    we return `None`.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 由于C和K相同（减去一个常数），F和R相同（减去另一个常数），我们可以减少我们函数的决策树。如果我们向`from_temp`和/或`to_temp`传递一个不是`F`、`C`、`K`或`R`的字符串值，我们返回`None`。
- en: '[PRE35]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Exercise 8.4
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.4
- en: 'There are three things to fix:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 有三件事需要修复：
- en: 'Variable usage: we use `value` consistently instead of `t` and `answer`.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变量使用：我们始终使用`value`而不是`t`和`answer`。
- en: Because we multiply our value by `3.14159`, our function needs to be annotated
    `float` → `float` rather than `str` → `str`.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们乘以`3.14159`，我们的函数需要被注解为`float` → `float`而不是`str` → `str`。
- en: We change the return type of the UDF to `DoubleType()`.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将UDF的返回类型更改为`DoubleType()`。
- en: '[PRE36]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Exercise 8.5
  id: totrans-165
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.5
- en: '[PRE37]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Exercise 8.6
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习8.6
- en: '[PRE38]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'We do not need to change the return type from `Optional[Frac]`: the return
    value of the updated `py_reduce_fraction` is still either a `Frac` or `None`.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要更改返回类型从`Optional[Frac]`：更新后的`py_reduce_fraction`的返回值仍然是`Frac`或`None`。
- en: Chapter 9
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章
- en: Exercise 9.1
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9.1
- en: '[PRE39]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Exercise 9.2
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9.2
- en: Compared to the identical exercise in chapter 8, we need to return a `pd.Series`
    instead of a scalar value. The `null` value here (if we pass an unacceptable unit)
    is a Series of `None`.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 与第8章中相同的练习相比，我们需要返回一个`pd.Series`而不是标量值。这里的`null`值（如果我们传递一个不可接受的单位）是一个`None`的Series。
- en: '[PRE40]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Exercise 9.3
  id: totrans-176
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9.3
- en: The output is the same. The normalization process does not change based on the
    units of temperature.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 输出相同。归一化过程不会根据温度的单位而改变。
- en: '[PRE41]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Exercise 9.4
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9.4
- en: Because of the way we defined our function, our data frame returns a six-columned
    data frame, where we expect only four. The faulty line is `answer` `=` `temp_by_day[["stn",`
    `"year",` `"mo",` `"da",` `"temp"]]`, where we hardcode the columns.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们定义函数的方式，我们的数据帧返回一个六列的数据帧，而我们期望只有四列。错误行是`answer` `=` `temp_by_day[["stn",`
    `"year",` `"mo",` `"da",` `"temp"]`]，其中我们硬编码了列。
- en: '[PRE42]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Exercise 9.5
  id: totrans-182
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习9.5
- en: '[PRE43]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: Chapter 10
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章
- en: Exercise 10.1
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.1
- en: c
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: c
- en: '[PRE44]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Exercise 10.2
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.2
- en: Let’s create a data frame with 1,000 records (250 distinct `index` values and
    the `value` column), all equal to 2.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个包含1,000条记录（250个不同的`index`值和`value`列），所有值都等于2的数据帧。
- en: '[PRE45]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The result might appear counterintuitive, but following our definition (illustrated
    in figure A.1), we can see that PySpark does the right thing by trying to split
    every partition window into three (as even as it can) buckets.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可能看起来反直觉，但根据我们的定义（如图A.1所示），我们可以看到PySpark通过尝试将每个分区窗口分成三个（尽可能均匀）的桶来正确地做了这件事。
- en: '![](../Images/A-01.png)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/A-01.png)'
- en: 'Figure A.1 Three-tiling with all the same `values`. We follow the same behavior:
    splitting across records.'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 图A.1使用所有相同`values`的三分区。我们遵循相同的行为：跨记录分割。
- en: Exercise 10.3
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.3
- en: The `rowsBetween()` window partitions contain five records. Because the first
    and second records of the data don’t have two preceding records, we see `3` and
    `4` for the first and second records, respectively.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '`rowsBetween()`窗口分区包含五个记录。因为数据的前两个记录没有两个前置记录，所以我们看到第一个和第二个记录分别是`3`和`4`。'
- en: The `rangeBetween()` window partitions uses the `10` value (always the same)
    to compute the window frame boundaries. The result is 1,000,001 everywhere.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: '`rangeBetween()`窗口分区使用`10`值（始终相同）来计算窗口框架边界。结果是到处都是1,000,001。'
- en: '[PRE46]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: Exercise 10.4
  id: totrans-198
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.4
- en: 'We have multiple records with the highest temperature: PySpark will show all
    of them.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多个最高温度的记录：PySpark将显示所有这些记录。
- en: '[PRE47]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Exercise 10.5
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.5
- en: While we can tackle this multiple ways, the simplest (in my opinion) is to create
    a record number (which will always be increasing) to break the ties.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们可以用多种方式解决这个问题，但在我看来最简单的方法是创建一个记录号（它将始终递增）来打破这种联系。
- en: '[PRE48]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ These records are 1 and 2.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这些记录是1和2。
- en: Exercise 10.6
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.6
- en: We can convert the date into a `unix_timestamp` (the number of seconds since
    the UNIX epoch; see [http://mng.bz/enPv](http://mng.bz/enPv)) and then use a 7-day
    window (or 7 days × 24 hours × 60 minutes × 60 seconds).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将日期转换为`unix_timestamp`（自UNIX纪元以来的秒数；见[http://mng.bz/enPv](http://mng.bz/enPv)）然后使用一个7天窗口（或7天×24小时×60分钟×60秒）。
- en: '[PRE49]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Exercise 10.7
  id: totrans-208
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.7
- en: Assuming there are always 12 months in a year, we can create a pseudo index,
    `num_mo`, with `year` `*` `12` `+` `mo`. With this, we can use an exact range
    of ± 1 month.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一年中总是有12个月，我们可以创建一个伪索引，`num_mo`，通过`year` `*` `12` `+` `mo`。有了这个，我们可以使用± 1个月的精确范围。
- en: '[PRE50]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Chapter 11
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章
- en: Exercise 11.1
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习11.1
- en: No. We would still have one job (the program, triggered by `showString()`) and
    two stages.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 不，我们仍然只有一个任务（由`showString()`触发的程序）和两个阶段。
- en: Exercise 11.2
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习11.2
- en: The first plan has no action attached to it (`show()`), so we do not have a
    last action.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个计划没有附加任何动作（`show()`），因此我们没有最后一个动作。
- en: Exercise 11.3
  id: totrans-216
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习11.3
- en: a and b are operations on a single record at a time; they are narrow operations.
    The other ones need to shuffle the data to collocate the relevant records and
    therefore are wide operations. c, d, and e need the matching keys to be on the
    same node at one point.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: a和b是对单个记录的操作；它们是窄操作。其他操作需要洗牌数据以协调相关记录，因此是宽操作。c、d和e需要在某一点上位于同一节点上的匹配键。
- en: Chapter 13
  id: totrans-218
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第13章
- en: Exercise 13.1
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习13.1
- en: When `E_max` `==` `E_min`, every value becomes `0.5` `*` (max + min)
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 当`E_max` `==` `E_min`时，每个值都变为`0.5` `*` (max + min)
