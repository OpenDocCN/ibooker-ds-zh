- en: 'Chapter 11\. Neural networks that understand language: king – man + woman ==
    ?'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第11章。理解语言的网络：王 - 男 + 女 == ?
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容**'
- en: Natural language processing (NLP)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: Supervised NLP
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监督NLP
- en: Capturing word correlation in input data
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 捕获输入数据中的词相关性
- en: Intro to an embedding layer
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入层的介绍
- en: Neural architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经架构
- en: Comparing word embeddings
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较词嵌入
- en: Filling in the blank
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 填空
- en: Meaning is derived from loss
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 意义来源于损失
- en: Word analogies
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 词类比
- en: “Man is a slow, sloppy, and brilliant thinker; computers are fast, accurate,
    and stupid.”
  id: totrans-11
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “人是一个缓慢、粗心大意但聪明的人；计算机是快速、准确但愚蠢的。”
- en: ''
  id: totrans-12
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*John Pfeiffer, in *Fortune*, 1961*'
  id: totrans-13
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*约翰·费菲尔，在《财富》杂志，1961年*'
- en: What does it mean to understand language?
  id: totrans-14
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 理解语言意味着什么？
- en: What kinds of predictions do people make about language?
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 人们关于语言会做出哪些预测？
- en: Up until now, we’ve been using neural networks to model image data. But neural
    networks can be used to understand a much wider variety of datasets. Exploring
    new datasets also teaches us a lot about neural networks in general, because different
    datasets often justify different styles of neural network training according to
    the challenges hidden in the data.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在使用神经网络来建模图像数据。但神经网络可以用来理解更广泛的数据集。探索新的数据集也让我们对神经网络的一般知识有了很多了解，因为不同的数据集通常根据数据中隐藏的挑战来证明不同的神经网络训练风格。
- en: '![](Images/f0188-01.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0188-01.jpg)'
- en: 'We’ll begin this chapter by exploring a much older field that overlaps deep
    learning: *natural language processing* (NLP). This field is dedicated exclusively
    to the automated understanding of human language (previously not using deep learning).
    We’ll discuss the basics of deep learning’s approach to this field.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从这个章节开始，探索一个与深度学习重叠的古老领域：*自然语言处理*（NLP）。这个领域专门致力于人类语言的自动化理解（之前没有使用深度学习）。我们将讨论深度学习对这个领域的处理方法的基本原理。
- en: Natural language processing (NLP)
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）
- en: NLP is divided into a collection of tasks or challenges
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NLP被划分为一系列任务或挑战
- en: 'Perhaps the best way to quickly get to know NLP is to consider a few of the
    many challenges the NLP community seeks to solve. Here are a few types of classification
    problem that are common to NLP:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 可能最快了解自然语言处理（NLP）的方法之一是考虑NLP社区试图解决的许多挑战中的一些。以下是NLP中常见的几种分类问题类型：
- en: Using the *characters* of a document to predict *where words start and end*.
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档的*字符*来预测*单词的开始和结束位置*。
- en: Using the *words* of a document to predict *where sentences start and end*.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档的*单词*来预测*句子开始和结束的位置*。
- en: Using the *words in a sentence* to predict *the part of speech for each word*.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用句子中的*单词*来预测*每个单词的词性*。
- en: Using *words in a sentence* to predict *where phrases start and end*.
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用句子中的*单词*来预测*短语开始和结束的位置*。
- en: Using *words in a sentence* to predict *where named entity (person, place, thing)
    references start and end*.
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用句子中的*单词*来预测*命名实体（人、地点、事物）引用的开始和结束位置*。
- en: Using *sentences in a document* to predict *which pronouns refer to the same
    person / place / thing*.
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用文档中的*句子*来预测*哪些代词指的是同一个人/地点/事物*。
- en: Using *words in a sentence* to predict the *sentiment* of a sentence.
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用句子中的*单词*来预测句子的*情感*。
- en: 'Generally speaking, NLP tasks seek to do one of three things: label a region
    of text (such as part-of-speech tagging, sentiment classification, or named-entity
    recognition); link two or more regions of text (such as coreference, which tries
    to answer whether two mentions of a real-world thing are in fact referencing the
    same real-world thing, where the real-world thing is generally a person, place,
    or some other named entity); or try to fill in missing information (missing words)
    based on context.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，NLP任务试图做以下三件事情之一：为文本区域（如词性标注、情感分类或命名实体识别）贴标签；将两个或更多文本区域（如指代消解，试图回答两个对现实世界事物的提及是否实际上指的是同一个现实世界事物，其中现实世界事物通常是人、地点或其他命名实体）链接起来；或者根据上下文尝试填补缺失的信息（缺失的单词）。
- en: 'Perhaps it’s also apparent how machine learning and NLP are deeply intertwined.
    Until recently, most state-of-the-art NLP algorithms were advanced, probabilistic,
    non-parametric models (not deep learning). But the recent development and popularization
    of two major neural algorithms have swept the field of NLP: neural word embeddings
    and recurrent neural networks (RNNs).'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 可能也明显地看出机器学习和NLP是如何紧密相连的。直到最近，大多数最先进的NLP算法都是高级、概率性、非参数模型（不是深度学习）。但最近两个主要神经网络算法的发展和普及已经席卷了NLP领域：神经词嵌入和循环神经网络（RNNs）。
- en: In this chapter, we’ll build a word-embedding algorithm and demonstrate why
    it increases the accuracy of NLP algorithms. In the next chapter, we’ll create
    a recurrent neural network and demonstrate why it’s so effective at predicting
    across sequences.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将构建一个词嵌入算法，并展示它如何提高自然语言处理算法的准确性。在下一章中，我们将创建一个循环神经网络，并展示它在预测序列时的有效性。
- en: 'It’s also worth mentioning the key role that NLP (perhaps using deep learning)
    plays in the advancement of artificial intelligence. AI seeks to create machines
    that can think and engage with the world as humans do (and beyond). NLP plays
    a very special role in this endeavor, because language is the bedrock of conscious
    logic and communication in humans. As such, methods by which machines can use
    and understand language form the foundation of human-like logic in machines: the
    foundation of thought.'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 还值得一提的是，自然语言处理（可能使用深度学习）在人工智能进步中扮演的关键角色。人工智能寻求创造能够像人类一样（甚至超越人类）思考和与世界互动的机器。自然语言处理在这个努力中扮演着非常特殊的角色，因为语言是人类意识逻辑和沟通的基础。因此，机器使用和理解语言的方法构成了机器中类似人类逻辑的基础：思想的基础。
- en: Supervised NLP
  id: totrans-33
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督式自然语言处理
- en: Words go in, and predictions come out
  id: totrans-34
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 输入单词，输出预测
- en: Perhaps you’ll remember the following figure from [chapter 2](kindle_split_010.xhtml#ch02).
    Supervised learning is all about taking “what you know” and transforming it into
    “what you want to know.” Up until now, “what you know” has always consisted of
    numbers in one way or another. But NLP uses text as input. How do you process
    it?
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得[第2章](kindle_split_010.xhtml#ch02)中的以下图示。监督学习就是将“你所知道的”转化为“你想要知道的”。到目前为止，“你所知道的”总是以某种方式由数字组成。但自然语言处理使用文本作为输入。你该如何处理它？
- en: '![](Images/f0190-01.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0190-01.jpg)'
- en: Because neural networks only map input numbers to output numbers, the first
    step is to convert the text into numerical form. Much as we converted the streetlight
    dataset, we need to convert the real-world data (in this case, text) into a *matrix*
    the neural network can consume. As it turns out, how we do this is extremely important!
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 因为神经网络只将输入数字映射到输出数字，所以第一步是将文本转换为数值形式。就像我们转换街灯数据集一样，我们需要将现实世界的数据（在这种情况下，文本）转换为神经网络可以消费的*矩阵*。实际上，我们如何做这一点非常重要！
- en: '![](Images/f0190-02_alt.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0190-02_alt.jpg)'
- en: How should we convert text to numbers? Answering that question requires some
    thought regarding the problem. Remember, neural networks look for correlation
    between their input and output layers. Thus, we want to convert text into numbers
    in such a way that the correlation between input and output is *most obvious*
    to the network. This will make for faster training and better generalization.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何将文本转换为数字？回答这个问题需要对问题进行一些思考。记住，神经网络在其输入层和输出层之间寻找相关性。因此，我们希望以使输入和输出之间的相关性对网络来说*最明显*的方式将文本转换为数字。这将使训练更快，泛化能力更好。
- en: In order to know what input format makes input/output correlation the most obvious
    to the network, we need to know what the input/output dataset looks like. To explore
    this topic, let’s take on the challenge of *topic classification*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了知道哪种输入格式能让网络最明显地感知输入/输出相关性，我们需要了解输入/输出数据集的样子。为了探讨这个话题，让我们接受挑战，进行*主题分类*。
- en: IMDB movie reviews dataset
  id: totrans-41
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: IMDB电影评论数据集
- en: You can predict whether people post positive or negative reviews
  id: totrans-42
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你可以预测人们发布的评论是正面还是负面
- en: 'The IMDB movie reviews dataset is a collection of review -> rating pairs that
    often look like the following (this is an imitation, not pulled from IMDB):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: IMDB电影评论数据集是一组评论->评分对，通常看起来像以下这样（这是一个模仿，并非来自IMDB）：
- en: “This movie was terrible! The plot was dry, the acting unconvincing, and I spilled
    popcorn on my shirt.”
  id: totrans-44
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “这部电影太糟糕了！剧情枯燥，表演不可信，我还把爆米花洒在了衬衫上。”
- en: ''
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Rating: 1 (stars)*'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*评分：1（星星）*'
- en: The entire dataset consists of around 50,000 of these pairs, where the input
    reviews are usually a few sentences and the output ratings are between 1 and 5
    stars. People consider it a *sentiment dataset* because the stars are indicative
    of the overall sentiment of the movie review. But it should be obvious that this
    sentiment dataset might be very different from other sentiment datasets, such
    as product reviews or hospital patient reviews.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 整个数据集大约由50,000个这样的对组成，其中输入评论通常是几句话，输出评分在1到5星之间。人们认为这是一个*情感数据集*，因为星星表示电影评论的整体情感。但很明显，这个情感数据集可能与其他情感数据集（如产品评论或医院患者评论）有很大不同。
- en: You want to train a neural network that can use the input text to make accurate
    predictions of the output score. To accomplish this, you must first decide how
    to turn the input and output datasets into matrices. Interestingly, the output
    dataset is a number, which perhaps makes it an easier place to start. You’ll adjust
    the range of stars to be between 0 and 1 instead of 1 and 5, so that you can use
    binary `softmax`. That’s all you need to do to the output. I’ll show an example
    on the next page.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要训练一个神经网络，它可以使用输入文本来准确预测输出分数。为了实现这一点，你必须首先决定如何将输入和输出数据集转换为矩阵。有趣的是，输出数据集是一个数字，这可能使得它更容易开始。你将调整星级范围在0到1之间，而不是1到5，这样你就可以使用二进制的`softmax`。这就是你需要对输出所做的所有事情。我将在下一页上展示一个例子。
- en: 'The input data, however, is a bit trickier. To begin, let’s consider the raw
    data. It’s a list of characters. This presents a few problems: not only is the
    input data text instead of numbers, but it’s *variable-length* text. So far, neural
    networks always take an input of a fixed size. You’ll need to overcome this.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，输入数据却有些棘手。首先，让我们考虑原始数据。它是一系列字符。这带来了一些问题：不仅输入数据是文本而不是数字，而且它是*可变长度*的文本。到目前为止，神经网络总是需要一个固定大小的输入。你需要克服这个问题。
- en: So, the raw input won’t work. The next question to ask is, “What about this
    data will have correlation with the output?” Representing that property might
    work well. For starters, I wouldn’t expect any characters (in the list of characters)
    to have any correlation with the sentiment. You need to think about it differently.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，原始输入将不起作用。接下来要问的问题是，“这些数据中的哪些将与输出有相关性？”表示这个属性可能效果很好。首先，我不期望任何字符（在字符列表中）与情感有任何相关性。你需要以不同的方式思考。
- en: What about the words? Several words in this dataset would have a bit of correlation.
    I’d bet that *terrible* and *unconvincing* have significant negative correlation
    with the rating. By *negative*, I mean that as they increase in frequency in any
    input datapoint (any review), the rating tends to decrease.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，关于单词呢？这个数据集中的一些单词会有一些相关性。我敢打赌*糟糕*和*不可信*与评分有显著的负相关性。这里的*负相关性*是指，随着它们在任何输入数据点（任何评论）中的频率增加，评分往往会下降。
- en: Perhaps this property is more general! Perhaps words by themselves (even out
    of context) would have significant correlation with sentiment. Let’s explore this
    further.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 也许这个属性更普遍！也许单词本身（即使在没有上下文的情况下）会与情感有显著的相关性。让我们进一步探讨这个问题。
- en: Capturing word correlation in input data
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在输入数据中捕捉单词相关性
- en: 'Bag of words: Given a review’s vocabulary, predict the sentiment'
  id: totrans-54
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 词袋模型：给定一个评论的词汇，预测情感
- en: 'If you observe correlation between the vocabulary of an IMDB review and its
    rating, then you can proceed to the next step: creating an input matrix that represents
    the vocabulary of a movie review.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察到IMDB评论的词汇与其评分之间的相关性，那么你可以进行下一步：创建一个表示电影评论词汇的输入矩阵。
- en: What’s commonly done in this case is to create a matrix where each row (vector)
    corresponds to each movie review, and each column represents whether a review
    contains a particular word in the vocabulary. To create the vector for a review,
    you calculate the vocabulary of the review and then put 1 in each corresponding
    column for that review and 0s everywhere else. How big are these vectors? Well,
    if there are 2,000 words, and you need a place in each vector for each word, each
    vector will have 2,000 dimensions.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下通常的做法是创建一个矩阵，其中每一行（向量）对应于每一部电影评论，每一列表示评论是否包含词汇表中的特定单词。为了创建评论的向量，你计算评论的词汇表，然后在对应的列中为该评论放置1，其他所有地方放置0。这些向量有多大？嗯，如果有2000个单词，并且你需要在每个向量中为每个单词留出空间，那么每个向量将有2000个维度。
- en: 'This form of storage, called *one-hot encoding*, is the most common format
    for encoding binary data (the binary presence or absence of an input datapoint
    among a vocabulary of possible input datapoints). If the vocabulary was only four
    words, the one-hot encoding might look like this:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 这种存储形式被称为*独热编码*，是编码二进制数据（在可能的输入数据点词汇表中，输入数据点的二进制存在或不存在）最常见的形式。如果词汇表只有四个单词，独热编码可能看起来像这样：
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](Images/f0192-01.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0192-01.jpg)'
- en: As you can see, we create a vector for each term in the vocabulary, and this
    allows you to use simple vector addition to create a vector representing a subset
    of the total vocabulary (such as a subset corresponding to the words in a sentence).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们为词汇表中的每个术语创建一个向量，这允许你使用简单的向量加法来创建表示总词汇子集的向量（例如，与句子中的单词相对应的子集）。
- en: '[PRE1]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![](Images/f0192-02.jpg)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0192-02.jpg)'
- en: Note that when you create an embedding for several terms (such as “the cat sat”),
    you have multiple options if words occur multiple times. If the phrase was “cat
    cat cat,” you could either sum the vector for “cat” three times (resulting in
    `[3,0,0,0]`) or just take the unique “cat” a single time (resulting in `[1,0,0,0]`).
    The latter typically works better for language.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当你为几个术语（如“the cat sat”）创建嵌入时，如果单词重复出现多次，你有多种选择。如果短语是“cat cat cat”，你可以选择将“cat”的向量相加三次（结果为`[3,0,0,0]`），或者只取唯一的“cat”一次（结果为`[1,0,0,0]`）。后者通常对语言来说效果更好。
- en: Predicting movie reviews
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 预测电影评论
- en: With the encoding strategy and the previous network, you can predict sentiment
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 使用编码策略和之前的网络，你可以预测情感
- en: Using the strategy we just identified, you can build a vector for each word
    in the sentiment dataset and use the previous two-layer network to predict sentiment.
    I’ll show you the code, but I strongly recommend attempting this from memory.
    Open a new Jupyter notebook, load in the dataset, build your one-hot vectors,
    and then build a neural network to predict the rating of each movie review (positive
    or negative).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们刚才确定的战略，你可以为情感数据集中的每个单词构建一个向量，并使用之前的两层网络来预测情感。我会展示代码，但我强烈建议你尝试从记忆中完成这个任务。打开一个新的Jupyter笔记本，加载数据集，构建你的独热向量，然后构建一个神经网络来预测每篇电影评论的评分（正面或负面）。
- en: 'Here’s how I would do the preprocessing step:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我会这样进行预处理步骤：
- en: '[PRE2]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Intro to an embedding layer
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 嵌入层的简介
- en: Here’s one more trick to make the network faster
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这还有一个使网络更快的小技巧
- en: At right is the diagram from the previous neural network, which you’ll now use
    to predict sentiment. But before that, I want to describe the layer names. The
    first layer is the dataset (`layer_0`). This is followed by what’s called a *linear
    layer* (`weights_0_1`). This is followed by a `relu` layer (`layer_1`), another
    linear layer (`weights_1_2`), and then the output, which is the prediction layer.
    As it turns out, you can take a bit of a shortcut to `layer_1` by replacing the
    first linear layer (`weights_0_1`) with an embedding layer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 右边是之前神经网络的图，你现在将使用它来预测情感。但在那之前，我想描述一下层名称。第一层是数据集（`layer_0`）。接下来是所谓的*线性层*（`weights_0_1`）。接下来是一个`relu`层（`layer_1`），另一个线性层（`weights_1_2`），然后是输出，也就是预测层。实际上，你可以通过用嵌入层替换第一个线性层（`weights_0_1`）来稍微缩短到`layer_1`的路径。
- en: '![](Images/f0194-01.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0194-01.jpg)'
- en: Taking a vector of 1s and 0s is mathematically equivalent to summing several
    rows of a matrix. Thus, it’s much more efficient to select the relevant rows of
    `weights_0_1` and sum them as opposed to doing a big vector-matrix multiplication.
    Because the sentiment vocabulary is on the order of 70,000 words, most of the
    vector-matrix multiplication is spent multiplying 0s in the input vector by different
    rows of the matrix before summing them. Selecting the rows corresponding to each
    word in a matrix and summing them is much more efficient.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 使用1s和0s的向量在数学上等同于对矩阵的几行求和。因此，选择`weights_0_1`的相关行并求和要比进行大规模的向量-矩阵乘法更高效。因为情感词汇量大约有70,000个单词，大部分的向量-矩阵乘法都是在输入向量中的0与矩阵的不同行相乘后再求和。选择矩阵中对应每个单词的行并求和要高效得多。
- en: '![](Images/f0194-02.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0194-02.jpg)'
- en: Using this process of selecting rows and performing a sum (or average) means
    treating the first linear layer (`weights_0_1`) as an embedding layer. Structurally,
    they’re identical (`layer_1` is exactly the same using either method for forward
    propagation). The only difference is that summing a small number of rows is much
    faster.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用选择行并执行求和（或平均）的过程，意味着将第一线性层（`weights_0_1`）作为嵌入层。从结构上看，它们是相同的（`layer_1`使用两种方法进行前向传播时都是一样的）。唯一的区别是求和少量行要快得多。
- en: '![](Images/f0194-03.jpg)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0194-03.jpg)'
- en: After running the previous code, run this code
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在运行之前的代码后，运行此代码
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Trains on the first 24,000 reviews**'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在前24,000条评论上进行训练**'
- en: '***2* embed + sigmoid**'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 嵌入 + sigmoid**'
- en: '***3* linear + softmax**'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 线性 + softmax**'
- en: '***4* Compares the prediction with the truth**'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将预测与真实值进行比较**'
- en: '***5* Backpropagation**'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 反向传播**'
- en: Interpreting the output
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解释输出
- en: What did the neural network learn along the way?
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络在过程中学到了什么？
- en: 'Here’s the output of the movie reviews neural network. From one perspective,
    this is the same correlation summarization we’ve already discussed:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 这是电影评论神经网络的结果。从一个角度来看，这与我们之前讨论过的相同的相关性总结：
- en: '[PRE4]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: The neural network was looking for correlation between the input datapoints
    and the output datapoints. But those datapoints have characteristics we’re familiar
    with (notably those of language). Furthermore, it’s extremely beneficial to consider
    what patterns of language would be detected by the correlation summarization,
    and more importantly, which ones wouldn’t. After all, just because the network
    is able to find correlation between the input and output datasets doesn’t mean
    it understands every useful pattern of language.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络正在寻找输入数据点和输出数据点之间的相关性。但这些数据点具有我们熟悉的特征（尤其是语言的那些特征）。此外，考虑由相关性总结检测到的语言模式，以及更重要的是，哪些模式不会被检测到，这一点极为有益。毕竟，仅仅因为网络能够在输入和输出数据集之间找到相关性，并不意味着它理解了语言中的每一个有用模式。
- en: '![](Images/f0196-01.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0196-01.jpg)'
- en: Furthermore, understanding the difference between what the network (in its current
    configuration) is capable of learning relative to what it needs to know to properly
    understand language is an incredibly fruitful line of thinking. This is what researchers
    on the front lines of state-of-the-art research consider, and it’s what we’re
    going to consider here.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，理解网络（在其当前配置下）能够学习的内容与它为了正确理解语言所需知道的内容之间的区别，是一条极其富有成效的思考路线。这正是处于最前沿的研究人员所考虑的，也是我们在这里将要考虑的。
- en: What about language did the movie reviews network learn? Let’s start by considering
    what was presented to the network. As displayed in the diagram at top right, you
    presented each review’s vocabulary as input and asked the network to predict one
    of two labels (`positive` or `negative`). Given that the correlation summarization
    says the network will look for correlation between the input and output datasets,
    at a minimum, you’d expect the network to identify words that have either a positive
    or negative correlation (by themselves).
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 电影评论网络学习了哪些关于语言的知识？让我们首先考虑呈现给网络的内容。如右上角的图所示，你将每篇评论的词汇作为输入，并要求网络预测两个标签中的一个（`正面`或`负面`）。鉴于相关性总结表明网络将在输入和输出数据集之间寻找相关性，至少你可以期望网络识别出具有正或负相关性的单词（单独来看）。
- en: This follows naturally from the correlation summarization. You present the presence
    or absence of a word. As such, the correlation summarization will find direct
    correlation between this presence/absence and each of the two labels. But this
    isn’t the whole story.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这自然地导致了相关性总结。你呈现一个词的存在或不存在。因此，相关性总结将找到这种存在/不存在与两个标签中的每一个之间的直接相关性。但这并不是全部的故事。
- en: '![](Images/f0196-02.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0196-02.jpg)'
- en: Neural architecture
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经架构
- en: How did the choice of architecture affect what the network learned?
  id: totrans-95
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 架构的选择是如何影响网络学习的内容的？
- en: 'We just discussed the first, most trivial type of information the neural network
    learned: direct correlation between the input and target datasets. This observation
    is largely the clean slate of neural intelligence. (If a network can’t discover
    direct correlation between input and output data, something is probably broken.)
    The development of more-sophisticated architectures is based on the need to find
    more-complex patterns than direct correlation, and this network is no exception.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚讨论了神经网络学习的第一种、最简单类型的信息：输入和目标数据集之间的直接相关性。这一观察结果在很大程度上是神经网络智能的起点。（如果一个网络无法在输入和输出数据之间发现直接相关性，那么可能出了问题。）更复杂架构的发展基于寻找比直接相关性更复杂模式的需要，而这个网络也不例外。
- en: The minimal architecture needed to identify direct correlation is a two-layer
    network, where the network has a single weight matrix that connects directly from
    the input layer to the output layer. But we used a network that has a hidden layer.
    This begs the question, what does this hidden layer do?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 识别直接相关性的最小架构是一个两层网络，其中网络有一个直接从输入层连接到输出层的单权重矩阵。但我们的网络有一个隐藏层。这引发了一个问题，这个隐藏层有什么作用？
- en: Fundamentally, hidden layers are about grouping datapoints from a previous layer
    into *n* groups (where *n* is the number of neurons in the hidden layer). Each
    hidden neuron takes in a datapoint and answers the question, “Is this datapoint
    in my group?” As the hidden layer learns, it searches for useful groupings of
    its input. What are useful groupings?
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，隐藏层是将前一层的数据点分组为 *n* 组（其中 *n* 是隐藏层中神经元的数量）。每个隐藏神经元接收一个数据点并回答问题：“这个数据点是否在我的组中？”随着隐藏层的不断学习，它会寻找其输入的有用分组。什么是有用的分组？
- en: An input datapoint grouping is useful if it does two things. First, the grouping
    must be useful to the prediction of an output label. If it’s not useful to the
    output prediction, *the correlation summarization will never lead the network
    to find the group*. This is a hugely valuable realization. Much of neural network
    research is about finding training data (or some other manufactured signal for
    the network to artificially predict) so it finds groupings that are useful for
    a task (such as predicting movie review stars). We’ll discuss this more in a moment.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个输入数据点分组能够做两件事，那么它是有用的。首先，分组必须对预测输出标签有用。如果它对输出预测没有用，*相关性总结永远不会引导网络找到该组*。这是一个非常有价值的认识。神经网络研究的大部分内容都是关于找到训练数据（或为网络制造的其他信号，以便它能够人为地预测）以找到对任务有用的分组（例如预测电影评论星级）。我们将在稍后讨论这一点。
- en: Second, a grouping is useful if it’s an actual phenomenon in the data that you
    care about. Bad groupings just memorize the data. Good groupings pick up on phenomena
    that are useful linguistically.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 其次，如果分组是数据中你关心的实际现象，那么它是有用的。不良的分组只是记住数据。好的分组能够捕捉到有用的语言现象。
- en: For example, when predicting whether a movie review is positive or negative,
    understanding the difference between “terrible” and “not terrible” is a powerful
    grouping. It would be great to have a neuron that turned *off* when it saw “awful”
    and turned *on* when it saw “not awful.” This would be a powerful grouping for
    the next layer to use to make the final prediction. But because the input to the
    neural network is the vocabulary of a review, “it was great, not terrible” creates
    exactly the same `layer_1` value as “it was terrible, not great.” For this reason,
    the network is very unlikely to create a hidden neuron that understands negation.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在预测电影评论是正面还是负面时，理解“糟糕”和“不糟糕”之间的差异是一种强大的分组。如果有一个神经元在看到“糟糕”时关闭，在看到“不糟糕”时打开，这将是一个强大的分组，供下一层使用以做出最终预测。但由于神经网络输入是评论的词汇，“它很棒，不糟糕”与“它很糟糕，不棒”创造了完全相同的`layer_1`值。因此，网络非常不可能创建一个理解否定意义的隐藏神经元。
- en: Testing whether a layer is the same or different based on a certain language
    pattern is a great first step for knowing whether an architecture is likely to
    find that pattern using the correlation summarization. If you can construct two
    examples with an identical hidden layer, one with the pattern you find interesting
    and one without, the network is unlikely to find that pattern.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 基于某种语言模式测试层是否相同或不同，是了解架构是否可能使用相关性总结找到该模式的一个很好的第一步。如果你可以构建两个具有相同隐藏层的例子，一个有你感兴趣的模式，另一个没有，那么网络不太可能找到该模式。
- en: As you just learned, a hidden layer fundamentally groups the previous layer’s
    data. At a granular level, each neuron classifies a datapoint as either subscribing
    or not subscribing to its group. At a higher level, two datapoints (movie reviews)
    are similar if they subscribe to many of the same groups. Finally, two inputs
    (words) are similar if the weights linking them to various hidden neurons (a measure
    of each word’s group affinity) are similar. Given this knowledge, in the previous
    neural network, what should you observe in the weights going into the hidden neurons
    from the words?
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你刚刚学到的，隐藏层在本质上将前一层的数据分组。在细粒度层面，每个神经元将数据点分类为属于或不属于其组。在更高层面，如果两个数据点（电影评论）属于许多相同的组，则它们是相似的。最后，如果连接到各种隐藏神经元的权重（每个单词的组亲和力度量）相似，则两个输入（单词）是相似的。有了这些知识，在前一个神经网络中，你应该在连接到隐藏神经元的单词权重中观察到什么？
- en: What should you see in the weights connecting words and hidden neurons?
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你应该在连接单词和隐藏神经元的权重中看到什么？
- en: 'Here’s a hint: words that have a similar predictive power should subscribe
    to similar groups (hidden neuron configurations). What does this mean for the
    weights connecting each word to each hidden neuron?'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个提示：具有相似预测能力的单词应该属于相似的组（隐藏神经元配置）。这对连接每个单词到每个隐藏神经元的权重意味着什么？
- en: Here’s the answer. Words that correlate with similar labels (positive or negative)
    will have similar weights connecting them to various hidden neurons. This is because
    the neural network learns to bucket them into similar hidden neurons so that the
    final layer (`weights_1_2`) can make the correct positive or negative predictions.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是答案。与相似标签（正面或负面）相关的单词将会有类似的权重将它们连接到各种隐藏神经元。这是因为神经网络学会将它们归入相似的隐藏神经元，以便最终层（`weights_1_2`）可以做出正确的正面或负面预测。
- en: You can see this phenomenon by taking a particularly positive or negative word
    and searching for the other words with the most similar weight values. In other
    words, you can take each word and see which other words have the most similar
    weight values connecting them to each hidden neuron (to each group).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过选择一个特别积极或消极的词语，并搜索具有最相似权值的其他词语来观察这一现象。换句话说，您可以取每个词语，并查看哪些其他词语与每个隐藏神经元（每个组）具有最相似的权值连接。
- en: '![](Images/f0198-01.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0198-01.jpg)'
- en: The three bold weights for “good” form the embedding for “good.” They reflect
    how much the term “good” is a member of each group (hidden neuron). Words with
    similar predictive power have similar word embeddings (weight values).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: “好”的三个粗体权值形成了“好”的嵌入。它们反映了“好”这个术语是每个组（隐藏神经元）的成员程度。具有相似预测能力的词语具有相似的词嵌入（权值）。
- en: Words that subscribe to similar groups will have similar predictive power for
    positive or negative labels. As such, words that subscribe to similar groups,
    having similar weight values, will also have similar meaning. Abstractly, in terms
    of neural networks, a neuron has similar meaning to other neurons in the same
    layer if and only if it has similar weights connecting it to the next and/or previous
    layers.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 属于相似组的词语将具有相似的正负标签预测能力。因此，属于相似组且具有相似权值（权重）的词语也将具有相似的含义。从抽象的角度来看，在神经网络中，一个神经元与同一层中其他神经元的含义相似，当且仅当它与下一层和/或前一层的连接权重相似。
- en: Comparing word embeddings
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 比较词嵌入
- en: How can you visualize weight similarity?
  id: totrans-112
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 如何可视化权重相似性？
- en: 'For each input word, you can select the list of weights proceeding out of it
    to the various hidden neurons by selecting the corresponding row of `weights_0_1`.
    Each entry in the row represents each weight proceeding from that row’s word to
    each hidden neuron. Thus, to figure out which words are most similar to a target
    term, you compare each word’s vector (row of the matrix) to that of the target
    term. The comparison of choice is called *Euclidian distance*, as shown in the
    following code:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个输入词语，您可以通过选择`weights_0_1`中对应的行来选择从该词语到各个隐藏神经元的权值列表。行中的每个条目代表从该行词语到每个隐藏神经元的权值。因此，为了确定哪些词语与目标术语最相似，您需要比较每个词语的向量（矩阵的行）与目标术语的向量。选择的比较方法是称为*欧几里得距离*，如下面的代码所示：
- en: '[PRE5]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This allows you to easily query for the most similar word (neuron) according
    to the network:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得您可以根据网络轻松查询最相似的词语（神经元）：
- en: '[PRE6]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you might expect, the most similar term to every word is itself, followed
    by words with similar usefulness as the target term. Again, as you might expect,
    because the network has only two labels (`positive` and `negative`), the input
    terms are grouped according to which label they tend to predict.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所预期的那样，每个词语最相似的术语是它自己，其次是具有与目标术语相似有用性的词语。再次，正如您所预期的那样，因为网络只有两个标签（`正面`和`负面`），输入术语根据它们倾向于预测的标签分组。
- en: This is a standard phenomenon of the correlation summarization. It seeks to
    create similar representations (`layer_1` values) within the network based on
    the label being predicted, so that it can predict the right label. In this case,
    the side effect is that the weights feeding into `layer_1` get grouped according
    to output label.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种标准的关联总结现象。它试图根据预测的标签在网络上创建相似的表现（`layer_1`值），以便能够预测正确的标签。在这种情况下，副作用是输入到`layer_1`的权重根据输出标签分组。
- en: The key takeaway is a gut instinct about this phenomenon of the correlation
    summarization. It consistently attempts to convince the hidden layers to be similar
    based on which label should be predicted.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 关联总结现象的关键启示是对这一现象的直观理解。它始终试图说服隐藏层基于应该预测的标签保持相似。
- en: What is the meaning of a neuron?
  id: totrans-120
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 神经元的含义是什么？
- en: Meaning is entirely based on the target labels being predicted
  id: totrans-121
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 意义完全基于预测的目标标签
- en: Note that the meanings of different words didn’t totally reflect how you might
    group them. The term most similar to “beautiful” is “atmosphere.” This is a valuable
    lesson. For the purposes of predicting whether a movie review is positive or negative,
    these words have nearly identical meaning. But in the real world, their meaning
    is quite different (one is an adjective and another a noun, for example).
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，不同词语的含义并没有完全反映您可能如何将它们分组。与“美丽”最相似的术语是“气氛”。这是一个宝贵的教训。为了预测电影评论是正面还是负面，这些词语具有几乎相同的意义。但在现实世界中，它们的含义却截然不同（例如，一个是形容词，另一个是名词）。
- en: '[PRE7]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This realization is incredibly important. The meaning (of a neuron) in the network
    is defined based on the target labels. Everything in the neural network is contexualized
    based on the correlation summarization trying to correctly make predictions. Thus,
    even though you and I know a great deal about these words, the neural network
    is entirely ignorant of all information outside the task at hand.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 这一认识非常重要。网络中（神经元）的含义是基于目标标签定义的。神经网络中的所有内容都是基于试图正确预测的相关性总结进行语境化的。因此，尽管你和我对这些单词非常了解，但神经网络对任务之外的所有信息一无所知。
- en: How can you convince the network to learn more-nuanced information about neurons
    (in this case, word neurons)? Well, if you give it input and target data that
    requires a more nuanced understanding of language, it will have reason to learn
    more-nuanced interpretations of various terms.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 你如何让网络学习关于神经元（在这种情况下，是单词神经元）的更细微的信息？好吧，如果你给它输入和目标数据，这些数据需要更细微的语言理解，它就会有理由学习各种术语的更细微的解释。
- en: What should you use the neural network to predict so that it learns more-interesting
    weight values for the word neurons? The task you’ll use to learn more-interesting
    weight values for the word neurons is a glorified fill-in-the blank task. Why
    use this? First, there’s nearly infinite training data (the internet), which means
    nearly infinite signal for the neural network to use to learn more-nuanced information
    about words. Furthermore, being able to accurately fill in the blank requires
    at least some notion of context about the real world.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该使用神经网络来预测什么，以便它为单词神经元学习更有趣的权重值？你将用于学习单词神经元更有趣的权重值的学习任务是一个美化的填空任务。为什么使用这个？首先，有几乎无限的训练数据（互联网），这意味着神经网络有几乎无限的信号来学习关于单词的更细微的信息。此外，能够准确地填充空白至少需要一些关于现实世界的上下文概念。
- en: For instance, in the following example, is it more likely that the blank is
    correctly filled by the word “anvil” or “wool”? Let’s see if the neural network
    can figure it out.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在以下示例中，空白处更可能是被“anvil”（砧）还是“wool”（羊毛）正确填充？让我们看看神经网络能否解决这个问题。
- en: '![](Images/f0200-01.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0200-01.jpg)'
- en: Filling in the blank
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 填空
- en: Learn richer meanings for words by having a richer signal to learn
  id: totrans-130
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 通过拥有更丰富的学习信号来学习单词的更丰富含义
- en: This example uses almost exactly the same neural network as the previous one,
    with only a few modifications. First, instead of predicting a single label given
    a movie review, you’ll take each (five-word) phrase, remove one word (a focus
    term), and attempt to train a network to figure out the identity of the word you
    removed using the rest of the phrase. Second, you’ll use a trick called *negative
    sampling* to make the network train a bit faster.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子几乎与上一个例子使用完全相同的神经网络，只有一些修改。首先，你不会预测针对电影评论的单个标签，而是将每个（五个单词）短语中的每个单词（焦点词）移除，并尝试训练一个网络来根据剩余的短语确定被移除的单词的身份。其次，你将使用一种称为*负采样*的技巧来使网络训练更快。
- en: 'Consider that in order to predict which term is missing, you need one label
    for each possible word. This would require several thousand labels, which would
    cause the network to train slowly. To overcome this, let’s randomly ignore most
    of the labels for each forward propagation step (as in, pretend they don’t exist).
    Although this may seem like a crude approximation, it’s a technique that works
    well in practice. Here’s the preprocessing code for this example:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到为了预测哪个术语缺失，你需要为每个可能的单词提供一个标签。这将需要数千个标签，这将导致网络训练缓慢。为了克服这一点，让我们在每次前向传播步骤中随机忽略每个标签的大部分（即，假装它们不存在）。虽然这看起来可能是一种粗略的近似，但在实践中这是一种效果很好的技术。以下是本例的预处理代码：
- en: '[PRE8]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Predicts only a random subset, because it’s really expensive to predict
    every vocabulary**'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 只预测随机子集，因为预测每个词汇都非常昂贵**'
- en: Meaning is derived from loss
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 意义来源于损失
- en: With this new neural network, you can subjectively see that the word embeddings
    cluster in a rather different way. Where before words were clustered according
    to their likelihood to predict a `positive` or `negative` label, now they’re clustered
    based on their likelihood to occur within the same phrase (sometimes regardless
    of sentiment).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的神经网络，你可以主观地看到单词嵌入的聚类方式有所不同。以前单词是根据预测“积极”或“消极”标签的可能性进行聚类的，而现在它们是根据在同一短语中出现的可能性进行聚类的（有时不考虑情感）。
- en: '| Predicting POS/NEG | Fill in the blank |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| Predicting POS/NEG | Fill in the blank |'
- en: '| --- | --- |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE9]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '|'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE10]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '|'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The key takeaway is that, even though the network trained over the same dataset
    with a very similar architecture (three layers, cross entropy, `sigmoid` nonlinear),
    you can influence what the network learns within its weights by changing what
    you tell the network to predict. Even though it’s looking at the same statistical
    information, you can target what it learns based on what you select as the input
    and target values. For the moment, let’s call this process of choosing what you
    want the network to learn *intelligence targeting*.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 关键的收获是，尽管网络在具有非常相似架构（三层，交叉熵，`sigmoid`非线性）的相同数据集上进行了训练，但你可以通过改变你告诉网络预测的内容来影响网络在其权重中学习的内容。尽管它正在查看相同的统计信息，但你可以根据你选择的输入和目标值来定位它学习的内容。暂时，让我们称这个过程为选择你希望网络学习的内容为*智能目标化*。
- en: Controlling the input/target values isn’t the only way to perform intelligence
    targeting. You can also adjust how the network measures error, the size and types
    of layers it has, and the types of regularization to apply. In deep learning research,
    all of these techniques fall under the umbrella of constructing what’s called
    a *loss function*.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 控制输入/目标值并不是执行智能目标化的唯一方法。您还可以调整网络如何衡量错误，它所具有的层的大小和类型，以及要应用的正则化类型。在深度学习研究中，所有这些技术都属于构建所谓的*损失函数*的范畴。
- en: Neural networks don’t really learn data; they minimize the loss function
  id: totrans-146
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络实际上并不是学习数据；它们最小化损失函数
- en: In [chapter 4](kindle_split_012.xhtml#ch04), you learned that learning is about
    adjusting each weight in the neural network to bring the error down to 0\. In
    this section, I’ll explain the same phenomena from a different perspective, choosing
    the error so the neural network learns the patterns we’re interested in. Remember
    these lessons?
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第4章](kindle_split_012.xhtml#ch04)中，你学习了学习是关于调整神经网络中的每个权重，将错误降低到0。在本节中，我将从不同角度解释相同的现象，选择错误，以便神经网络学习我们感兴趣的模式。你还记得这些教训吗？
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The golden method for learning**'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '**学习的黄金法则**'
- en: Adjust each weight in the correct direction and by the correct amount so `error`
    reduces to 0.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 调整每个权重到正确的方向和正确的量，以便将`错误`减少到0。
- en: '**The secret**'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '**秘密**'
- en: For any `input` and `goal_pred`, an exact relationship is defined between `error`
    and `weight`, found by combining the `prediction` and `error` formulas.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何`输入`和`goal_pred`，定义了`错误`和`权重`之间的精确关系，这是通过结合`预测`和`错误`公式找到的。
- en: '[PRE11]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '|  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Perhaps you remember this formula from the one-weight neural network. In that
    network, you could evaluate the error by first forward propagating (`0.5 * weight`)
    and then comparing to the target (0.8). I encourage you not to think about this
    from the perspective of two different steps (forward propagation, then error evaluation),
    but instead to consider the entire formula (including forward prop) to be the
    evaluation of an error value. This context will reveal the true cause of the different
    word-embedding clusterings. Even though the network and datasets were similar,
    the error function was fundamentally different, leading to different word clusterings
    within each network.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 也许你还记得这个公式来自单权重神经网络。在那个网络中，你可以通过首先正向传播（`0.5 * 权重`）然后与目标（0.8）进行比较来评估错误。我鼓励你不要从两个不同步骤（正向传播，然后错误评估）的角度来思考，而应该将整个公式（包括正向传播）视为错误值的评估。这个上下文将揭示不同词嵌入聚类背后的真正原因。尽管网络和数据集相似，但错误函数在本质上不同，导致每个网络内部的不同词聚类。
- en: '| Predicting POS/NEG | Fill in the blank |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 预测POS/NEG | 填空 |'
- en: '| --- | --- |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '|'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE12]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '|'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '|'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The choice of loss function determines the neural network’s knowledge
  id: totrans-163
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 损失函数的选择决定了神经网络的认知
- en: The more formal term for an *error function* is a *loss function* or *objective
    function* (all three phrases are interchangeable). Considering learning to be
    all about minimizing a loss function (which includes forward propagation) gives
    a far broader perspective on how neural networks learn. Two neural networks can
    have identical starting weights, be trained over identical datasets, and ultimately
    learn very different patterns because you choose a different loss function. In
    the case of the two movie review neural networks, the loss function was different
    because you chose two different target values (positive or negative versus fill
    in the blank).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *误差函数* 的更正式的术语是 *损失函数* 或 *目标函数*（这三个短语可以互换使用）。将学习视为最小化损失函数（这包括前向传播）的过程，可以提供一个更广泛的视角来理解神经网络是如何学习的。两个神经网络可能具有相同的起始权重，在相同的数据集上进行训练，但最终学习到非常不同的模式，因为您选择了不同的损失函数。在两个电影评论神经网络的情况下，损失函数之所以不同，是因为您选择了两个不同的目标值（正面或负面与填空）。
- en: Different kinds of architectures, layers, regularization techniques, datasets,
    and nonlinearities aren’t really that different. These are the ways you can choose
    to construct a loss function. If the network isn’t learning properly, the solution
    can often come from any of these possible categories.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的架构、层、正则化技术、数据集和非线性并不是真的那么不同。这些都是您可以用来构建损失函数的方式。如果网络没有正确学习，解决方案通常可以来自这些可能的任何一种。
- en: For example, if a network is overfitting, you can augment the loss function
    by choosing simpler nonlinearities, smaller layer sizes, shallower architectures,
    larger datasets, or more-aggressive regularization techniques. All of these choices
    will have a fundamentally similar effect on the loss function and a similar consequence
    on the behavior of the network. They all interplay together, and over time you’ll
    learn how changing one can affect the performance of another; but for now, the
    important takeaway is that learning is about constructing a loss function and
    then minimizing it.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个网络过度拟合，您可以通过选择更简单的非线性、较小的层大小、较浅的架构、更大的数据集或更激进的正则化技术来增强损失函数。所有这些选择将对损失函数产生根本上的相似影响，并对网络的行为产生相似的影响。它们相互作用，随着时间的推移，您将学会如何改变一个影响另一个的性能；但就目前而言，重要的启示是学习是关于构建损失函数然后最小化它。
- en: 'Whenever you want a neural network to learn a pattern, everything you need
    to know to do so will be contained in the loss function. When you had only a single
    weight, this allowed the loss function to be simple, as you’ll recall:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 无论何时您想让神经网络学习一个模式，您需要知道的一切都会包含在损失函数中。当您只有一个权重时，这使得损失函数变得简单，正如您所回忆的那样：
- en: '|'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE14]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '|'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: But as you chain large numbers of complex layers together, the loss function
    will become more complicated (and that’s OK). Just remember, if something is going
    wrong, the solution is in the loss function, which includes both the forward prediction
    and the raw error evaluation (such as mean squared error or cross entropy).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 但随着您将大量复杂的层连接起来，损失函数将变得更加复杂（这是可以的）。但请记住，如果出现问题，解决方案就在损失函数中，这包括前向预测和原始误差评估（如均方误差或交叉熵）。
- en: King – Man + Woman ~= Queen
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 国王 – 男人 + 女人 ~= 女王
- en: Word analogies are an interesting consequence of the previously built network
  id: totrans-173
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 单词类比是之前构建的网络的一个有趣的结果
- en: Before closing out this chapter, let’s discuss what is, at the time of writing,
    still one of the most famous properties of neural word embeddings (word vectors
    like those we just created). The task of filling in the blank creates word embeddings
    with interesting phenomena known as *word analogies*, wherein you can take the
    vectors for different words and perform basic algebraic operations on them.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在结束这一章之前，让我们讨论一下，在撰写本文时，仍然是神经网络词嵌入（如我们刚刚创建的词向量）最著名的属性之一。填空的任务创建了具有有趣现象的词嵌入，这种现象被称为
    *单词类比*，您可以对不同单词的向量执行基本的代数运算。
- en: For example, if you train the previous network on a large enough corpus, you’ll
    be able to take the vector for `king`, subtract from it the vector for `man`,
    add in the vector for `woman`, and then search for the most similar vector (other
    than those in the query). As it turns out, the most similar vector is often the
    word “queen.” There are even similar phenomena in the fill-in-the-blank network
    trained over movie reviews.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你在一个足够大的语料库上训练前面的网络，你将能够从“king”的向量中减去“man”的向量，加上“woman”的向量，然后搜索最相似的向量（除了查询中的那些）。结果证明，最相似的向量通常是“queen”。在电影评论上训练的填空网络中甚至有类似的现象。
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Word analogies
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 单词类比
- en: Linear compression of an existing property in the data
  id: totrans-178
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 数据中现有特性的线性压缩
- en: When this property was first discovered, it created a flurry of excitement as
    people extrapolated many possible applications of such a technology. It’s an amazing
    property in its own right, and it did create a veritable cottage industry around
    generating word embeddings of one variety or another. But the word analogy property
    in and of itself hasn’t grown that much since then, and most of the current work
    in language focuses instead on recurrent architectures (which we’ll get to in
    [chapter 12](kindle_split_020.xhtml#ch12)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当这个特性首次被发现时，它引起了人们的极大兴奋，因为人们推测了许多这种技术的可能应用。这本身就是一个惊人的特性，它确实在生成各种类型的单词嵌入方面创造了一个真正的
    cottage industry。但自从那时起，单词类比特性本身并没有增长多少，而目前大部分的语言研究工作都集中在循环架构上（我们将在第12章中讨论）。
- en: That being said, getting a good intuition for what’s going on with word embeddings
    as a result of a chosen loss function is extremely valuable. You’ve already learned
    that the choice of loss function can affect how words are grouped, but this word
    analogy phenomenon is something different. What about the new loss function causes
    it to happen?
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，由于选择损失函数的结果，对单词嵌入中发生的事情有一个良好的直观理解是非常宝贵的。你已经了解到损失函数的选择可以影响单词的分组方式，但这个单词类比现象是另一回事。是什么新的损失函数导致了这种现象的发生？
- en: If you consider a word embedding having two dimensions, it’s perhaps easier
    to envision exactly what it means for these word analogies to work.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑一个单词嵌入有两个维度，那么也许更容易想象这些单词类比是如何工作的。
- en: '[PRE16]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](Images/f0207-01.jpg)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0207-01.jpg)'
- en: The relative usefulness to the final prediction between “king”/“man” and “queen”/“woman”
    is similar. Why? The difference between “king” and “man” leaves a vector of `royalty`.
    There are a bunch of male- and female-related words in one grouping, and then
    there’s another grouping in the royal direction.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: “king”（国王）/“man”（男人）与“queen”（王后）/“woman”（女人）之间的相对有用性是相似的。为什么？ “king”和“man”之间的差异留下了一个“royalty”（皇室）向量。有一组与男性、女性相关的单词，然后还有另一组在皇室方向的分组。
- en: This can be traced back to the chosen loss. When the word “king” shows up in
    a phrase, it changes the probability of other words showing up in a certain way.
    It increases the probability of words related to “man” and the probability of
    words related to royalty. The word “queen” appearing in a phrase increases the
    probability of words related to “woman” and the probability of words related to
    royalty (as a group). Thus, because the words have this sort of Venn diagram impact
    on the output probability, they end up subscribing to similar combinations of
    groupings.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以追溯到选择的损失函数。当单词“king”出现在一个短语中时，它会改变其他单词以某种方式出现的概率。它增加了与“man”相关的单词的概率以及与皇室相关的单词的概率。短语中出现的“queen”增加了与“woman”相关的单词的概率以及与皇室（作为一个群体）相关的概率。因此，由于单词对输出概率有这种类似维恩图的影响，它们最终订阅了类似的组合分组。
- en: Oversimplified, “king” subscribes to the male and the royal dimensions of the
    hidden layer, whereas “queen” subscribes to the female and royal dimensions of
    the hidden layer. Taking the vector for “king” and subtracting out some approximation
    of the male dimensions and adding in the female ones yields something close to
    “queen.” The most important takeaway is that this is more about the properties
    of language than deep learning. Any linear compression of these co-occurrence
    statistics will behave similarly.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，“king”订阅了隐藏层的男性维度和皇室维度，而“queen”订阅了隐藏层的女性维度和皇室维度。从“king”的向量中减去一些男性维度的近似值并添加女性维度，可以得到接近“queen”的结果。最重要的收获是，这更多地关于语言的特性，而不是深度学习。这些共现统计的任何线性压缩都会表现出类似的行为。
- en: Summary
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: You’ve learned a lot about neural word embeddings and the impact - of loss on
    learning
  id: totrans-188
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 你已经学到了很多关于神经词嵌入以及损失对学习的影响的知识。
- en: 'In this chapter, we’ve unpacked the fundamental principles of using neural
    networks to study language. We started with an overview of the primary problems
    in natural language processing and then explored how neural networks model language
    at the word level using word embeddings. You also learned how the choice of loss
    function can change the kinds of properties that are captured by word embeddings.
    We finished with a discussion of perhaps the most magical of neural phenomena
    in this space: word analogies.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了使用神经网络研究语言的基本原理。我们从自然语言处理的主要问题概述开始，然后探讨了神经网络如何使用词嵌入在词级别模拟语言。你还学习了损失函数的选择如何改变词嵌入捕获的性质。我们最后讨论了在这个领域可能最神奇的一种神经现象：词类比。
- en: As with the other chapters, I encourage you to build the examples in this chapter
    from scratch. Although it may seem as though this chapter stands on its own, the
    lessons in loss-function creation and tuning are invaluable and will be extremely
    important as you tackle increasingly more complicated strategies in future chapters.
    Good luck!
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他章节一样，我鼓励您从头开始构建本章的示例。尽管这个章节看起来似乎是独立的，但关于损失函数创建和调整的教训是无价的，并且在你未来章节中处理越来越复杂的策略时将极其重要。祝你好运！
