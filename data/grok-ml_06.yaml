- en: '6 A continuous approach to splitting points: Logistic classifiers'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 一种连续的分割点方法：逻辑分类器
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: the difference between hard assignments and soft assignments in classification
    models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类模型中硬任务和软任务之间的区别
- en: the sigmoid function, a continuous activation function
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid函数，一种连续的激活函数
- en: discrete perceptrons vs. continuous perceptrons, also called logistic classifiers
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 离散感知器与连续感知器，也称为逻辑分类器
- en: the logistic regression algorithm for classifying data
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于分类数据的逻辑回归算法
- en: coding the logistic regression algorithm in Python
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Python中编码逻辑回归算法
- en: using the logistic classifier in Turi Create to analyze the sentiment of movie
    reviews
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Turi Create中使用逻辑分类器分析电影评论的情感
- en: using the softmax function to build classifiers for more than two classes
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用softmax函数构建超过两个类别的分类器
- en: '![](../Images/6-unnumb.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-unnumb.png)'
- en: In the previous chapter, we built a classifier that determined if a sentence
    was happy or sad. But as we can imagine, some sentences are happier than others.
    For example, the sentence “I’m good” and the sentence “Today was the most wonderful
    day in my life!” are both happy, yet the second is much happier than the first.
    Wouldn’t it be nice to have a classifier that not only predicts if sentences are
    happy or sad but that gives a rating for how happy sentences are—say, a classifier
    that tells us that the first sentence is 60% happy and the second one is 95% happy?
    In this chapter, we define the *logistic classifier*, which does precisely that.
    This classifier assigns a score from 0 to 1 to each sentence, in a way that the
    happier a sentence is, the higher the score it receives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们构建了一个分类器，用于判断一个句子是快乐还是悲伤。但正如我们可以想象的那样，有些句子比其他句子更快乐。例如，“我感觉很好”和“今天是我一生中最美好的一天！”这两个句子都是快乐的，但第二个句子比第一个句子快乐得多。不是很好吗？有一个分类器不仅能预测句子是快乐还是悲伤，还能给出句子快乐的评分——比如说，一个告诉我们第一个句子有60%快乐的分类器，第二个句子有95%快乐的分类器？在这一章中，我们定义了*逻辑分类器*，它正是这样做的。这个分类器给每个句子分配一个从0到1的分数，这样句子越快乐，它收到的分数就越高。
- en: In a nutshell, a logistic classifier is a type of model that works just like
    a perceptron classifier, except instead of returning a yes or no answer, it returns
    a number between 0 and 1\. In this case, the goal is to assign scores close to
    0 to the saddest sentences, scores close to 1 to the happiest sentences, and scores
    close to 0.5 to neutral sentences. This threshold of 0.5 is common in practice,
    though arbitrary. In chapter 7, we’ll see how to adjust it to optimize our model,
    but for this chapter we use 0.5.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 简而言之，逻辑分类器是一种模型，它的工作方式与感知器分类器非常相似，除了它返回的是0到1之间的一个数字。在这种情况下，目标是给最悲伤的句子分配接近0的分数，给最快乐的句子分配接近1的分数，给中性的句子分配接近0.5的分数。这个0.5的阈值在实践中很常见，尽管是任意的。在第7章中，我们将看到如何调整它以优化我们的模型，但本章我们使用0.5。
- en: 'This chapter relies on chapter 5, because the algorithms we develop here are
    similar, aside from some technical differences. Making sure you understand chapter
    5 well will help you understand the material in this chapter. In chapter 5, we
    described the perceptron algorithm using an error function that tells us how good
    a perceptron classifier is and an iterative step that turns a classifier into
    a slightly better classifier. In this chapter, we learn the logistic regression
    algorithm, which works in a similar way. The main differences follow:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章依赖于第5章，因为我们在这里开发的算法在技术差异之外是相似的。确保你很好地理解第5章将有助于你理解本章的内容。在第5章中，我们使用一个错误函数描述了感知器算法，该函数告诉我们感知器分类器有多好，以及一个迭代步骤，该步骤将分类器转变为一个稍微好一点的分类器。在这一章中，我们学习逻辑回归算法，它以类似的方式工作。主要区别如下：
- en: The step function is replaced by a new activation function, which returns values
    between 0 and 1.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步函数被一个新的激活函数所取代，该函数返回介于0和1之间的值。
- en: The perceptron error function is replaced by a new error function, which is
    based on a probability calculation.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器错误函数被一个新的基于概率计算的错误函数所取代。
- en: The perceptron trick is replaced by a new trick, which improves the classifier
    based on this new error function.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器技巧被一个新的技巧所取代，该技巧基于这个新的错误函数改进了分类器。
- en: aside In this chapter we carry out a lot of numerical computations. If you follow
    the equations, you might find that your calculations differ from those in the
    book by a small amount. The book rounds the numbers at the very end of the equation,
    not in between steps. This, however, should have very little effect on the final
    results.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白：在本章中，我们进行了大量的数值计算。如果你跟随方程式，可能会发现你的计算与书中所述的数值略有不同。书中在方程式的最后一位数进行四舍五入，而不是在步骤之间。然而，这应该对最终结果的影响非常小。
- en: At the end of the chapter, we apply our knowledge to a real-life dataset of
    movie reviews on the popular site IMDB ([www.imdb.com](https://www.imdb.com)).
    We use a logistic classifier to predict whether movie reviews are positive or
    negative.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章末尾，我们将所学知识应用于IMDB（[www.imdb.com](https://www.imdb.com)）网站上真实的电影评论数据集。我们使用逻辑分类器来预测电影评论是正面还是负面。
- en: 'The code for this chapter is available in the following GitHub repository:
    [https://github.com/luisguiserrano/manning/tree/master/Chapter_6_Logistic_Regression](https://github.com/luisguiserrano/manning/tree/master/Chapter_6_Logistic_Regression).'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在以下GitHub仓库中找到：[https://github.com/luisguiserrano/manning/tree/master/Chapter_6_Logistic_Regression](https://github.com/luisguiserrano/manning/tree/master/Chapter_6_Logistic_Regression).
- en: 'Logistic classifiers: A continuous version of perceptron classifiers'
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑分类器：感知器分类器的连续版本
- en: In chapter 5, we covered the perceptron, which is a type of classifier that
    uses the features of our data to make a prediction. The prediction can be 1 or
    0\. This is called a *discrete perceptron*, because it returns an answer from
    a discrete set (the set containing 0 and 1). In this chapter, we learn *continuous
    perceptrons*, which return an answer that can be any number in the interval between
    0 and 1\. A more common name for continuous perceptrons is *logistic classifiers*.
    The output of a logistic classifier can be interpreted as a score, and the goal
    of the logistic classifier is to assign scores as close as possible to the label
    of the points—points with label 0 should get scores close to 0, and points with
    label 1 should get scores close to 1.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们介绍了感知器，这是一种使用数据特征进行预测的分类器。预测可以是1或0。这被称为*离散感知器*，因为它从离散集合（包含0和1的集合）中返回一个答案。在本章中，我们学习*连续感知器*，它返回一个介于0和1之间的任何数值。连续感知器的一个更常见的名称是*逻辑分类器*。逻辑分类器的输出可以解释为分数，逻辑分类器的目标是分配尽可能接近点的标签——标签为0的点应得到接近0的分数，标签为1的点应得到接近1的分数。
- en: 'We can visualize continuous perceptrons similar to discrete perceptrons: with
    a line (or high-dimensional plane) that separates two classes of data. The only
    difference is that the discrete perceptron predicts that everything to one side
    of the line has label 1 and to the other side has label 0, whereas the continuous
    perceptron assigns a value from 0 to 1 to all the points based on their position
    with respect to the line. Every point on the line gets a value of 0.5\. This value
    means the model can’t decide if the sentence is happy or sad. For example, in
    the ongoing sentiment analysis example, the sentence “Today is Tuesday” is neither
    happy nor sad, so the model would assign it a score close to 0.5\. Points in the
    positive zone get scores larger than 0.5, where the points even farther away from
    the 0.5 line in the positive direction get values closer to 1\. Points in the
    negative zone get scores smaller than 0.5, where, again, the points farther from
    the line get values closer to 0\. No point gets a value of 1 or 0 (unless we consider
    points at infinity), as shown in figure 6.1.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像离散感知器一样可视化连续感知器：用一条线（或高维平面）分隔两个数据类别。唯一的区别是，离散感知器预测线的一侧的所有内容都有标签1，另一侧有标签0，而连续感知器根据点相对于线的位置分配一个从0到1的值。线上的每个点都得到0.5的值。这个值意味着模型无法决定句子是快乐还是悲伤。例如，在正在进行的情感分析示例中，句子“今天是星期二”既不快乐也不悲伤，因此模型会分配一个接近0.5的分数。正区间的点得到大于0.5的分数，其中正方向上离0.5线更远的点得到更接近1的值。负区间的点得到小于0.5的分数，其中离线更远的点得到更接近0的值。没有点得到1或0的值（除非我们考虑无穷远的点），如图6.1所示。
- en: '![](../Images/6-1.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图6.1](../Images/6-1.png)'
- en: 'Figure 6.1 Left: The perceptron algorithm trains a discrete perceptron, where
    the predictions are 0 (happy) and 1 (sad). Right: The logistic regression algorithm
    trains a continuous perceptron, where the predictions are numbers between 0 and
    1 which indicate the predicted level of happiness.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.1 左：感知器算法训练一个离散感知器，其中预测值为0（快乐）和1（悲伤）。右：逻辑回归算法训练一个连续感知器，其中预测值是介于0和1之间的数字，表示预测的幸福程度。
- en: Why do we call this *classification* instead of *regression*, given that the
    logistic classifier is not outputting a state per se but a number? The reason
    is, after scoring the points, we can classify them into two classes, namely, those
    points with a score of 0.5 or higher and those with a score lower than 0.5\. Graphically,
    the two classes are separated by the boundary line, just like with the perceptron
    classifier. However, the algorithm we use to train logistic classifiers is called
    the *logistic regression algorithm*. This notation is a bit peculiar, but we’ll
    keep it as it is to match the literature.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们称之为*分类*而不是*回归*，鉴于逻辑分类器并不是输出一个状态本身，而是一个数字？原因是，在评分之后，我们可以将它们分为两类，即得分在0.5或更高的点和得分低于0.5的点。从图形上看，这两类点被边界线分开，就像感知器分类器一样。然而，我们用来训练逻辑分类器的算法被称为*逻辑回归算法*。这个符号有点奇特，但我们将保持原样以匹配文献。
- en: 'A probability approach to classification: The sigmoid function'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 分类的一种概率方法：sigmoid函数
- en: How do we slightly modify the perceptron models from the previous section to
    get a score for each sentence, as opposed to a simple “happy” or “sad”? Recall
    how we made the predictions in the perceptron models. We scored each sentence
    by separately scoring each word and adding the scores, plus the bias. If the score
    was positive, we predicted that the sentence was happy, and if it was negative,
    we predicted that the sentence was sad. In other words, what we did was apply
    the step function to the score. The step function returns a 1 if the score was
    nonnegative and a 0 if it was negative.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何稍微修改上一节中的感知器模型，为每个句子得到一个分数，而不是简单的“快乐”或“悲伤”？回想一下我们在感知器模型中是如何进行预测的。我们通过分别评分每个单词并将评分相加，再加上偏差来评分每个句子。如果评分是正的，我们预测句子是快乐的；如果评分是负的，我们预测句子是悲伤的。换句话说，我们所做的是将阶跃函数应用于评分。阶跃函数在非负评分时返回1，在负评分时返回0。
- en: Now we do something similar. We take a function that receives the score as the
    input and outputs a number between 0 and 1\. The number is close to 1 if the score
    is positive and close to zero if the score is negative. If the score is zero,
    then the output is 0.5\. Imagine if you could take the entire number line and
    crunch it into the interval between 0 and 1\. It would look like the function
    in figure 6.2.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们做类似的事情。我们取一个接收分数作为输入并输出0到1之间数字的函数。如果分数是正的，则数字接近1；如果分数是负的，则数字接近0。如果分数是零，则输出是0.5。想象一下，如果你能将整个数轴压缩到0和1之间的区间，它看起来就像图6.2中的函数。
- en: '![](../Images/6-2.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-2.png)'
- en: Figure 6.2 The sigmoid function sends the entire number line to the interval
    (0,1).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 Sigmoid函数将整个数轴映射到区间(0,1)。
- en: 'Many functions can help us here, and in this case, we use one called the *sigmoid*,
    denoted with the Greek letter *sigma* (*σ*). The formula for the sigmoid follows:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 许多函数可以在这里帮助我们，在这种情况下，我们使用一个称为*sigmoid*的函数，用希腊字母*sigma*（*σ*）表示。sigmoid的公式如下：
- en: '![](../Images/6-11.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-11.png)'
- en: What really matters here is not the formula but what the function does, which
    is crunching the real number line into the interval (0,1). In figure 6.3, we can
    see a comparison of the graphs of the step and the sigmoid functions.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这里真正重要的是公式本身，而是函数所做的工作，即将实数线压缩到区间(0,1)。在图6.3中，我们可以看到阶跃函数和sigmoid函数的图形比较。
- en: '![](../Images/6-3.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-3.png)'
- en: 'Figure 6.3 Left: The step function used to build discrete perceptrons. It outputs
    a value of 0 for any negative input and a value of 1 for any input that is positive
    or zero. It has a discontinuity at zero. Right: The sigmoid function used to build
    continuous perceptrons. It outputs values less than 0.5 for negative inputs and
    values greater than 0.5 for positive inputs. At zero, it outputs 0.5\. It is continuous
    and differentiable everywhere.'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 左：用于构建离散感知器的阶跃函数。对于任何负输入，它输出0；对于任何正或零输入，它输出1。它在零处有间断。右：用于构建连续感知器的sigmoid函数。对于负输入，它输出小于0.5的值；对于正输入，它输出大于0.5的值。在零处，它输出0.5。它在任何地方都是连续且可微的。
- en: The sigmoid function is, in general, better than the step function for several
    reasons. Having continuous predictions gives us more information than discrete
    predictions. In addition, when we get into the calculus, the sigmoid function
    has a much nicer derivative than the step function. The step function has a derivative
    of zero, with the exception of the origin, where it is undefined. In table 6.1,
    we calculate some values of the sigmoid function to make sure the function does
    what we want it to.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: S形函数在一般情况下，由于以下几个原因，通常比阶梯函数更好。连续预测比离散预测提供了更多信息。此外，当我们进入微积分时，S形函数的导数比阶梯函数要好得多。阶梯函数的导数为零，除了原点，在那里它是未定义的。在表6.1中，我们计算了S形函数的一些值，以确保函数按我们的预期工作。
- en: Table 6.1 Some inputs and their outputs under the sigmoid function. Notice that
    for large negative inputs, the output is close to 0, whereas for large positive
    inputs, the output is close to 1\. For the input 0, the output is 0.5.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 在S形函数下的某些输入及其输出。注意，对于大的负输入，输出接近0，而对于大的正输入，输出接近1。对于输入0，输出是0.5。
- en: '| *x*   | σ(*x*) |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| *x*   | σ(*x*) |'
- en: '| –5 | 0.007 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| –5 | 0.007 |'
- en: '| –1 | 0.269 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| –1 | 0.269 |'
- en: '| 0 | 0.5 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0.5 |'
- en: '| 1 | 0.731 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.731 |'
- en: '| 5 | 0.993 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.993 |'
- en: The prediction of a logistic classifier is obtained by applying the sigmoid
    function to the score, and it returns a number between 0 and 1, which, as was
    mentioned earlier, can be interpreted in our example as the probability that the
    sentence is happy.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑分类器的预测是通过将S形函数应用于得分得到的，它返回一个介于0和1之间的数字，正如之前提到的，在我们的例子中，这可以解释为句子快乐的概率。
- en: In chapter 5, we defined an error function for a perceptron, called the perceptron
    error. We used this perceptron error to iteratively build a perceptron classifier.
    In this chapter, we follow the same procedure. The error of a continuous perceptron
    is slightly different from the one of a discrete predictor, but they still have
    similarities.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第5章中，我们为感知器定义了一个误差函数，称为感知器误差。我们使用这个感知器误差来迭代构建感知器分类器。在本章中，我们遵循相同的程序。连续感知器的误差与离散预测器的误差略有不同，但它们仍然有相似之处。
- en: The dataset and the predictions
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集和预测
- en: In this chapter, we use the same use case as in chapter 5, in which we have
    a dataset of sentences in alien language with the labels “happy” and “sad,” denoted
    by 1 and 0, respectively. The dataset for this chapter is slightly different than
    that in chapter 5, and it is shown in table 6.2.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用与第5章相同的用例，其中我们有一个外星语言的句子数据集，标签为“快乐”和“悲伤”，分别用1和0表示。本章的数据集与第5章略有不同，如表6.2所示。
- en: Table 6.2 The dataset of sentences with their happy/sad labels. The coordinates
    are the number of appearances of the words aack and beep in the sentence.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.2 句子的数据集及其快乐/悲伤标签。坐标是句子中单词aack和beep的出现次数。
- en: '|  | Words | Coordinates(#aack, #beep) | Label |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  | 单词 | 坐标(#aack, #beep) | 标签 |'
- en: '| Sentence 1 | Aack beep beep aack aack. | (3,2) | Sad (0) |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 句子1 | Aack beep beep aack aack. | (3,2) | 悲伤 (0) |'
- en: '| Sentence 2 | Beep aack beep. | (1,2) | Happy (1) |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 句子2 | Beep aack beep. | (1,2) | 快乐 (1) |'
- en: '| Sentence 3 | Beep! | (0,1) | Happy (1) |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 句子3 | Beep! | (0,1) | 快乐 (1) |'
- en: '| Sentence 4 | Aack aack. | (2,0) | Sad (0) |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 句子4 | Aack aack. | (2,0) | 悲伤 (0) |'
- en: 'The model we use has the following weights and bias:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的模型具有以下权重和偏差：
- en: Logistic Classifier 1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑分类器1
- en: 'Weight of *Aack*: *a* = 1'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack*的权重：*a* = 1'
- en: 'Weight of *Beep*: *b* = 2'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Beep*的权重：*b* = 2'
- en: 'Bias: *c* = –4'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差：*c* = –4
- en: 'We use the same notation as in chapter 5, where the variables *x*[aack] and
    *x*[beep] keep track of the appearances of *aack* and *beep*, respectively. A
    perceptron classifier would predict according to the formula *ŷ* = *step*(*ax*[aack]
    + *bx*[beep] + *c*), but because this is a logistic classifier, it uses the sigmoid
    function instead of the step function. Thus, its prediction is *ŷ* = *σ*(*ax*[aack]
    + *bx*[beep] + *c*). In this case, the prediction follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用与第5章相同的符号，其中变量 *x*[aack] 和 *x*[beep] 分别跟踪 *aack* 和 *beep* 的出现次数。感知器分类器会根据公式
    *ŷ* = *step*(*ax*[aack] + *bx*[beep] + *c*) 进行预测，但由于这是一个逻辑分类器，它使用S形函数而不是阶梯函数。因此，其预测为
    *ŷ* = *σ*(*ax*[aack] + *bx*[beep] + *c*)。在这种情况下，预测如下：
- en: '**Prediction**: *ŷ* = *σ*(1 · *x*[aack] + 2 · *x*[beep] – 4)'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**：*ŷ* = *σ*(1 · *x*[aack] + 2 · *x*[beep] – 4)'
- en: 'Therefore, the classifier makes the following predictions on our dataset:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，分类器对我们的数据集做出了以下预测：
- en: '**Sentence 1**: *ŷ* = *σ*(3 + 2 · 2 – 4) = *σ*(3) = 0.953.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子1**：*ŷ* = *σ*(3 + 2 · 2 – 4) = *σ*(3) = 0.953。'
- en: '**Sentence 2**: *ŷ* = *σ*(1 + 2 · 2 – 4) = *σ*(1) = 0.731.'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子2**：*ŷ* = *σ*(1 + 2 · 2 – 4) = *σ*(1) = 0.731。'
- en: '**Sentence 3**: *ŷ* = *σ*(0 + 2 · 1 – 4) = *σ*(–2) = 0.119.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子3**：*ŷ* = *σ*(0 + 2 · 1 – 4) = *σ*(–2) = 0.119。'
- en: '**Sentence 4**: *ŷ* = *σ*(2 + 2 · 0 – 4) = *σ*(–2) = 0.119.'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**句子4**：*ŷ* = *σ*(2 + 2 · 0 – 4) = *σ*(–2) = 0.119。'
- en: The boundary between the “happy” and “sad” classes is the line with equation
    *x*[aack] + 2*x*[beep] – 4 = 0, depicted in figure 6.4.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: “快乐”和“悲伤”类别的边界是方程*x*[aack] + 2*x*[beep] – 4 = 0的线，如图6.4所示。
- en: '![](../Images/6-4.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-4.png)'
- en: Figure 6.4 The plot of the dataset in table 6.2 with predictions. Notice that
    points 2 and 4 are correctly classified, but points 1 and 3 are misclassified.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 表6.2中数据集的预测图。注意点2和4被正确分类，但点1和3被错误分类。
- en: This line splits the plane into positive (happy) and negative (sad) zones. The
    positive zone is formed by the points with prediction higher than or equal to
    0.5, and the negative zone is formed by those with prediction less than 0.5.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这条线将平面分为正（快乐）和负（悲伤）区域。正区域由预测值高于或等于0.5的点组成，负区域由预测值小于0.5的点组成。
- en: 'The error functions: Absolute, square, and log loss'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数：绝对误差、平方误差和对数损失
- en: 'In this section, we build three error functions for a logistic classifier.
    What properties would you like a good error function to have? Some examples follow:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们为逻辑分类器构建了三个误差函数。一个好的误差函数应该具备哪些特性？以下是一些例子：
- en: If a point is correctly classified, the error is a small number.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个点被正确分类，误差将是一个小数。
- en: If a point is incorrectly classified, the error is a large number.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果一个点被错误分类，误差将是一个大数。
- en: The error of a classifier for a set of points is the sum (or average) of the
    errors at all the points.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于一组点的分类器的误差是所有点误差的总和（或平均值）。
- en: 'Many functions satisfy these properties, and we will see three of them; the
    absolute error, the square error, and the log loss. In table 6.3, we have the
    labels and predictions for the four points corresponding to the sentences in our
    dataset with the following characteristics:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 许多函数满足这些属性，我们将看到其中的三个；绝对误差、平方误差和对数损失。在表6.3中，我们有与我们的数据集中句子对应的四个点的标签和预测，以下是其特征：
- en: The points on the line are given a prediction of 0.5.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线上的点被预测为0.5。
- en: Points that are in the positive zone are given predictions higher than 0.5,
    and the farther a point is from the line in that direction, the closer its prediction
    is to 1.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位于正区域的点被预测为高于0.5，并且一个点在这个方向上离线越远，其预测值就越接近1。
- en: Points that are in the negative zone are given predictions lower than 0.5, and
    the farther a point is from the line in that direction, the closer its prediction
    is to 0.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 位于负区域的点被预测为低于0.5，并且一个点在这个方向上离线越远，其预测值就越接近0。
- en: Table 6.3 Four points—two happy and two sad with their predictions—as illustrated
    in figure 6.4\. Notice that points 1 and 4 are correctly classified, but points
    2 and 3 are not. A good error function should assign small errors to the correctly
    classified points and large errors to the poorly classified points.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.3 四个点——两个快乐和两个悲伤及其预测，如图6.4所示。注意点1和4被正确分类，但点2和3没有被分类。一个好的误差函数应该将小的误差分配给正确分类的点，将大的误差分配给分类不良的点。
- en: '| Point | Label | Prediction | Error? |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 标签 | 预测 | 误差？ |'
- en: '| 1 | 0 (Sad) | 0.953 | Should be large |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 (悲伤) | 0.953 | 应该很大 |'
- en: '| 2 | 1 (Happy) | 0.731 | Should be small |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 (快乐) | 0.731 | 应该很小 |'
- en: '| 3 | 1 (Happy) | 0.119 | Should be large |'
  id: totrans-82
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 (快乐) | 0.119 | 应该很大 |'
- en: '| 4 | 0 (Sad) | 0.119 | Should be small |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 (悲伤) | 0.119 | 应该很小 |'
- en: 'Notice that in table 6.3, points 2 and 4 get a prediction that is close to
    the label, so they should have small errors. In contrast, points 1 and 3 get a
    prediction that is far from the label, so they should have large errors. Three
    error functions that have this particular property follow:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在表6.3中，点2和4得到了一个接近标签的预测，因此它们应该有小的误差。相比之下，点1和3得到了一个远离标签的预测，因此它们应该有大的误差。具有这种特定属性的三个误差函数如下：
- en: 'Error function 1: Absolute error'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数1：绝对误差
- en: The absolute error is similar to the absolute error we defined for linear regression
    in chapter 3\. It is the absolute value of the difference between the prediction
    and the label. As we can see, it is large when the prediction is far from the
    label and small when they are close.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差与我们在第3章中为线性回归定义的绝对误差相似。它是预测值与标签之间差异的绝对值。正如我们所见，当预测值远离标签时，误差很大；而当它们接近时，误差很小。
- en: 'Error function 2: Square error'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数 2：平方误差
- en: Again, just like in linear regression, we also have the square error. This is
    the square of the difference between the prediction and the label, and it works
    for the same reason that the absolute error works.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，就像在线性回归中一样，我们也有平方误差。这是预测值和标签之间的差的平方，它以与绝对误差相同的原因工作。
- en: Before we proceed, let’s calculate the absolute and square error for the points
    in table 6.4\. Notice that points 2 and 4 (correctly classified) have small errors,
    and points 1 and 3 (incorrectly classified) have larger errors.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续之前，让我们计算表 6.4 中点的绝对误差和平方误差。请注意，点 2 和 4（正确分类）有小的误差，而点 1 和 3（错误分类）有较大的误差。
- en: Table 6.4 We have attached the absolute error and the square error for the points
    in table 6.3\. Notice that as we desired, points 2 and 4 have small errors, and
    points 1 and 3 have larger errors.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.4 我们附上了表 6.3 中点的绝对误差和平方误差。请注意，正如我们所希望的，点 2 和 4 有小的误差，而点 1 和 3 有较大的误差。
- en: '| Point | Label | Predicted label | Absolute Error | Square Error |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 标签 | 预测标签 | 绝对误差 | 平方误差 |'
- en: '| 1 | 0 (Sad) | 0.953 | 0.953 | 0.908 |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 (悲伤) | 0.953 | 0.953 | 0.908 |'
- en: '| 2 | 1 (Happy) | 0.731 | 0.269 | 0.072 |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 (快乐) | 0.731 | 0.269 | 0.072 |'
- en: '| 3 | 1 (Happy) | 0.119 | 0.881 | 0.776 |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 (快乐) | 0.119 | 0.881 | 0.776 |'
- en: '| 4 | 0 (Sad) | 0.119 | 0.119 | 0.014 |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 (悲伤) | 0.119 | 0.119 | 0.014 |'
- en: The absolute and the square errors may remind you of the error functions used
    in regression. However, in classification, they are not so widely used. The most
    popular is the next one we see. Why is it more popular? The math (derivatives)
    works much nicer with the next function. Also, these errors are all pretty small.
    In fact, they are all smaller than 1, no matter how poorly classified the point
    is. The reason is that the difference (or the square of the difference) between
    two numbers that are between 0 and 1 is at most 1\. To properly train models,
    we need error functions that take larger values than that. Thankfully, a third
    error function can do that for us.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差和平方误差可能会让你想起回归中使用的误差函数。然而，在分类中，它们并不那么广泛使用。最流行的是我们接下来要看到的下一个函数。为什么它更受欢迎？与下一个函数相关的数学（导数）工作得更好。此外，这些误差都非常小。事实上，无论分类有多差，它们都小于
    1。原因是介于 0 和 1 之间的两个数字之间的差（或差的平方）最多为 1。为了正确训练模型，我们需要误差函数能够取比这更大的值。幸运的是，第三个误差函数可以为我们做到这一点。
- en: 'Error function 3: log loss'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 误差函数 3：log loss
- en: The *log loss* is the most widely used error function for continuous perceptrons.
    Most of the error functions in this book have the word *error* in their name,
    but this one instead has the word *loss* in its name. The *log* part in the name
    comes from a natural logarithm that we use in the formula. However, the real soul
    of the log loss is probability.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*log loss* 是连续感知器最广泛使用的误差函数。本书中的大多数误差函数名称中都包含“误差”一词，但这个函数的名称中却包含“loss”（损失）。名称中的
    *log* 部分来源于公式中使用的自然对数。然而，log loss 的真正灵魂是概率。'
- en: The outputs of a continuous perceptron are numbers between 0 and 1, so they
    can be considered probabilities. The model assigns a probability to every data
    point, and that is the probability that the point is happy. From this, we can
    infer the probability that the point is sad, which is 1 minus the probability
    of being happy. For example, if the prediction is 0.75, that means the model believes
    the point is happy with a probability of 0.75 and sad with a probability of 0.25.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 连续感知器的输出是介于 0 和 1 之间的数字，因此它们可以被认为是概率。模型为每个数据点分配一个概率，即该点快乐的概率。从这个结果中，我们可以推断出该点悲伤的概率，即快乐概率的
    1 减去。例如，如果预测值为 0.75，这意味着模型认为该点有 0.75 的概率是快乐的，有 0.25 的概率是悲伤的。
- en: 'Now, here is the main observation. The goal of the model is to assign high
    probabilities to the happy points (those with label 1) and low probabilities to
    the sad points (those with label 0). Notice that the probability that a point
    is sad is 1 minus the probability that the point is happy. Thus, for each point,
    let’s calculate the probability that the model gives to its label. For the points
    in our dataset, the corresponding probabilities follow:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这里有一个主要的观察结果。模型的目标是为快乐点（标签为 1 的点）分配高概率，为悲伤点（标签为 0 的点）分配低概率。请注意，一个点悲伤的概率是它快乐概率的
    1 减去。因此，对于每个点，让我们计算模型为其标签分配的概率。对于我们的数据集中的点，相应的概率如下：
- en: '**Point 1**:'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点 1**：'
- en: Label = 0 (sad)
  id: totrans-102
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签 = 0 (悲伤)
- en: Prediction (probability of being happy) = 0.953
  id: totrans-103
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测（快乐概率）= 0.953
- en: 'Probability of being its label: 1 – 0.953 = **0.047**'
  id: totrans-104
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为该标签的概率：1 – 0.953 = **0.047**
- en: '**Point 2**:'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第2点**:'
- en: Label = 1 (happy)
  id: totrans-106
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签 = 1（快乐）
- en: Prediction (probability of being happy) = 0.731
  id: totrans-107
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测（快乐的概率）= 0.731
- en: 'Probability of being its label: **0.731**'
  id: totrans-108
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为该标签的概率：**0.731**
- en: '**Point 3**:'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第3点**:'
- en: Label = 1 (happy)
  id: totrans-110
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签 = 1（快乐）
- en: Prediction (probability of being happy) = 0.119
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测（快乐的概率）= 0.119
- en: 'Probability of being its label: **0.119**'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为该标签的概率：**0.119**
- en: '**Point 4**:'
  id: totrans-113
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**第4点**:'
- en: Label = 0 (sad)
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签 = 0（悲伤）
- en: Prediction (probability of being happy) = 0.119
  id: totrans-115
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测（快乐的概率）= 0.119
- en: 'Probability of being its label: 1 – 0.119 = **0.881**'
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 成为该标签的概率：1 – 0.119 = **0.881**
- en: Notice that points 2 and 4 are the points that are well classified, and the
    model assigns a high probability that they are their own label. In contrast, points
    1 and 3 are poorly classified, and the model assigns a low probability that they
    are their own label.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第2点和第4点是分类良好的点，模型分配了很高的概率认为它们是自己的标签。相比之下，第1点和第3点是分类不良的点，模型分配了很低的概率认为它们是自己的标签。
- en: The logistic classifier, in contrast with the perceptron classifier, doesn’t
    give definite answers. The perceptron classifier would say, “I am 100% sure that
    this point is happy,” whereas the logistic classifier says, “Your point has a
    73% probability of being happy and 27% of being sad.” Although the goal of the
    perceptron classifier is to be correct as many times as possible, the goal of
    the logistic classifier is to assign to each point the highest possible probability
    of having the correct label. This classifier assigns the probabilities 0.047,
    0.731, 0.119, and 0.881 to the four labels. Ideally, we’d like these numbers to
    be higher. How do we measure these four numbers? One way would be to add them
    or average them. But because they are probabilities, the natural approach is to
    multiply them. When events are independent, the probability of them occurring
    simultaneously is the product of their probabilities. If we assume that the four
    predictions are independent, then the probability that this model assigns to the
    labels “sad, happy, happy, sad” is the product of the four numbers, which is 0.047
    · 0.731 · 0.119 · 0.881 = 0.004\. This is a very small probability. Our hope would
    be that a model that fits this dataset better would result in a higher probability.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 与感知器分类器相比，逻辑回归分类器不给出明确的答案。感知器分类器会说，“我100%确信这个点是快乐的”，而逻辑回归分类器会说，“你的点有73%的概率是快乐的，27%的概率是悲伤的。”虽然感知器分类器的目标是尽可能多地正确，但逻辑回归分类器的目标是给每个点分配具有最高可能性的正确标签。这个分类器将概率0.047、0.731、0.119和0.881分配给四个标签。理想情况下，我们希望这些数字更高。我们如何衡量这四个数字呢？一种方法是将它们相加或取平均值。但由于它们是概率，自然的方法是相乘。当事件是独立时，它们同时发生的概率是它们概率的乘积。如果我们假设这四个预测是独立的，那么这个模型分配给标签“悲伤、快乐、快乐、悲伤”的概率是这四个数字的乘积，即0.047
    · 0.731 · 0.119 · 0.881 = 0.004。这是一个非常小的概率。我们希望一个更适合这个数据集的模型会产生更高的概率。
- en: That probability we just calculated seems like a good measure for our model,
    but it has some problems. For instance, it is a product of many small numbers.
    Products of many small numbers tend to be tiny. Imagine if our dataset had one
    million points. The probability would be a product of one million numbers, all
    between 0 and 1\. This number may be so small that a computer may not be able
    to represent it. Also, manipulating a product of one million numbers is extremely
    difficult. Is there any way that we could perhaps turn it into something easier
    to manipulate, like a sum?
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚才计算的概率似乎是我们模型的良好度量，但它有一些问题。例如，它是由许多小数相乘得到的。许多小数的乘积往往非常小。想象一下，如果我们的数据集有一百万个点，概率将是这些在0到1之间的一个百万个数的乘积。这个数字可能非常小，以至于计算机可能无法表示它。此外，操作一个百万个数的乘积极其困难。我们是否有办法将它转换成更容易操作的东西，比如和？
- en: 'Luckily for us, we have a convenient way to turn products into sums—using the
    logarithms. For this entire book, all we need to know about the logarithm is that
    it turns products into sums. More specifically, the logarithm of a product of
    two numbers is the sum of the logarithms of the numbers, as shown next:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们有一个方便的方法将产品转换为和——使用对数。对于整本书，我们只需要了解对数的一个特性，那就是它可以将产品转换为和。更具体地说，两个数的乘积的对数是这些数的对数之和，如下所示：
- en: '*ln*(*a · b*) = *ln*(*a*) + *ln*(*b*)'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '*ln*(*a · b*) = *ln*(*a*) + *ln*(*b*)'
- en: We can use logarithms in base 2, 10, or e. In this chapter, we use the natural
    logarithm, which is on base e. However, the same results can be obtained if we
    were to use the logarithm in any other base.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以2、10或e为底的对数。在本章中，我们使用自然对数，其底数为e。然而，如果我们使用任何其他底数的对数，也可以得到相同的结果。
- en: If we apply the natural logarithm to our product of probabilities, we obtain
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将自然对数应用于我们的概率乘积，我们得到
- en: '*ln*(0.047 *·* 0.731 *·* 0.119 *·* 0.881) = *ln*(0.047) + *ln*(0.731) + *ln*(0.119)
    + *ln*(0.881) = –5.616.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '*ln*(0.047 *·* 0.731 *·* 0.119 *·* 0.881) = *ln*(0.047) + *ln*(0.731) + *ln*(0.119)
    + *ln*(0.881) = –5.616.'
- en: One small detail. Notice that the result is a negative number. In fact, this
    will always be the case, because the logarithm of a number between 0 and 1 is
    always negative. Thus, if we take the negative logarithm of the product of probabilities,
    it is always a positive number.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一个小细节。注意结果是一个负数。实际上，这总是会发生，因为0到1之间的数的对数总是负数。因此，如果我们取概率乘积的负对数，它总是正数。
- en: The log loss is defined as the negative logarithm of the product of probabilities,
    which is also the sum of the negative logarithms of the probabilities. Furthermore,
    each of the summands is the log loss at that point. In table 6.5, you can see
    the calculation of the log loss for each of the points. By adding the log losses
    of all the points, we obtain a total log loss of 5.616.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失定义为概率乘积的负对数，这也就是概率的负对数之和。此外，每个加数都是该点的对数损失。在表6.5中，你可以看到每个点的对数损失的计算。通过将所有点的对数损失相加，我们得到总对数损失为5.616。
- en: Table 6.5 Calculation of the log loss for the points in our dataset. Notice
    that points that are well classified (2 and 4) have a small log loss, whereas
    points that are poorly classified (1 and 3) have a large log loss.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.5 计算数据集中点的对数损失。注意，分类良好的点（2和4）具有较小的对数损失，而分类不良的点（1和3）具有较大的对数损失。
- en: '| Point | Label | Predicted label | Probability of being its label | Log loss
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 标签 | 预测标签 | 是其标签的概率 | 对数损失 |'
- en: '| 1 | 0 (Sad) | 0.953 | 0.047 | –ln(0.047) = 3.049 |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 (悲伤) | 0.953 | 0.047 | –ln(0.047) = 3.049 |'
- en: '| 2 | 1 (Happy) | 0.731 | 0.731 | –ln(0.731) = 0.313 |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 (快乐) | 0.731 | 0.731 | –ln(0.731) = 0.313 |'
- en: '| 3 | 1 (Happy) | 0.119 | 0.119 | –ln(0.119) = 2.127 |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 (快乐) | 0.119 | 0.119 | –ln(0.119) = 2.127 |'
- en: '| 4 | 0 (Sad) | 0.119 | 0.881 | –ln(0.881) = 0.127 |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 (悲伤) | 0.119 | 0.881 | –ln(0.881) = 0.127 |'
- en: Notice that, indeed, the well-classified points (2 and 4) have a small log loss,
    and the poorly classified points have a large log loss. The reason is that if
    a number *x* is close to 0, –*ln*(*x*) is a large number, but if *x* is close
    to 1, then –*ln*(*x*) is a small number.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，确实，分类良好的点（2和4）具有较小的对数损失，而分类不良的点具有较大的对数损失。原因是如果数字*x*接近0，则–*ln*(*x*)是一个大数，但如果*x*接近1，则–*ln*(*x*)是一个小数。
- en: 'To summarize, the steps for calculating the log loss follow:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，计算对数损失的步骤如下：
- en: For each point, we calculate the probability that the classifier gives its label.
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个点，我们计算分类器给出其标签的概率。
- en: For the happy points, this probability is the score.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于快乐点，这个概率就是得分。
- en: For the sad points, this probability is 1 minus the score.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于悲伤点，这个概率是得分减去1。
- en: We multiply all these probabilities to obtain the total probability that the
    classifier has given to these labels.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将这些概率相乘，以获得分类器对这些标签给出的总概率。
- en: We apply the natural logarithm to that total probability.
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将自然对数应用于该总概率。
- en: The logarithm of a product is the sum of the logarithms of the factors, so we
    obtain a sum of logarithms, one for each point.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘积的对数是各个因子的对数之和，因此我们得到一个对数之和，每个点一个。
- en: We notice that all the terms are negative, because the logarithm of a number
    less than 1 is a negative number. Thus, we multiply everything by –1 to get a
    sum of positive numbers.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们注意到所有项都是负数，因为小于1的数的对数是负数。因此，我们将所有项乘以–1以得到正数的和。
- en: This sum is our log loss.
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这个和就是我们的对数损失。
- en: The log loss is closely related to the concept of *cross-entropy*, which is
    a way to measure similarity between two probability distributions. More details
    about cross-entropy are available in the references in appendix C.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失与*交叉熵*的概念密切相关，交叉熵是衡量两个概率分布之间相似度的一种方法。关于交叉熵的更多细节可以在附录C的参考文献中找到。
- en: Formula for the log loss
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 对数损失的公式
- en: 'The log loss for a point can be condensed into a nice formula. Recall that
    the log loss is the negative logarithm of the probability that the point is its
    label (happy or sad). The prediction the model gives to each point is *ŷ*, and
    that is the probability that the point is happy. Thus, the probability that the
    point is sad, according to the model, is 1 – *ŷ*. Therefore, we can write the
    log loss as follows:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 一个点的对数损失可以简化为一个漂亮的公式。回想一下，对数损失是点是其标签（快乐或悲伤）的概率的负对数。模型对每个点的预测是 *ŷ*，即点是快乐的概率。因此，根据模型，点是悲伤的概率是
    1 – *ŷ*。因此，我们可以将对数损失写成以下形式：
- en: 'If the label is 0: *log loss* = –*ln*(1 – *ŷ*)'
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标签是 0：*对数损失* = –*ln*(1 – *ŷ*)
- en: 'If the label is 1: *log loss* = –*ln*(*ŷ*)'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果标签是 1：*对数损失* = –*ln*(*ŷ*)
- en: 'Because the label is y, the previous `if` statement can be condensed into the
    following formula:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 因为标签是 y，之前的 `if` 语句可以简化为以下公式：
- en: '*log loss* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '*对数损失* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*)'
- en: The previous formula works because if the label is 0, the first summand is 0,
    and if the label is 1, the second summand is 0\. We use the term *log loss* when
    we refer to the log loss of a point or of a whole dataset. The log loss of a dataset
    is the sum of the log losses at every point.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的公式之所以有效，是因为如果标签是 0，第一个加数是 0，如果标签是 1，第二个加数是 0。当我们提到点的对数损失或整个数据集的对数损失时，我们使用术语
    *对数损失*。数据集的对数损失是每个点的对数损失的总和。
- en: Comparing classifiers using the log loss
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用对数损失比较分类器
- en: 'Now that we have settled on an error function for logistic classifiers, the
    log loss, we can use it to compare two classifiers. Recall that the classifier
    we’ve been using in this chapter is defined by the following weights and bias:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经确定了对逻辑分类器的误差函数，即对数损失，我们可以用它来比较两个分类器。回想一下，我们本章中使用的分类器由以下权重和偏差定义：
- en: Logistic Classifier 1
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑分类器 1
- en: 'Weight of *Aack*: *a* = 1'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack* 权重：*a* = 1'
- en: 'Weight of *Beep*: *b* = 2'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Beep* 权重：*b* = 2'
- en: 'Bias: *c* = –4'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差：*c* = –4
- en: 'In this section, we compare it with the following logistic classifier:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将其与以下逻辑分类器进行比较：
- en: Logistic Classifier 2
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑分类器 2
- en: 'Weight of *Aack*: *a* = –1'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Aack* 权重：*a* = –1'
- en: 'Weight of *Beep*: *b* = 1'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Beep* 权重：*b* = 1'
- en: 'Bias: *c* = 0'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差：*c* = 0
- en: 'The predictions that each classifier makes follow:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 每个分类器做出的预测如下：
- en: '**Classifier 1**: *ŷ* = *σ*(*x*[aack] + 2*x*[beep] – 4)'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器 1**：*ŷ* = *σ*(*x*[aack] + 2*x*[beep] – 4)'
- en: '**Classifier 2**: *ŷ* = *σ*(–*x*[aack] + *x*[beep])'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**分类器 2**：*ŷ* = *σ*(–*x*[aack] + *x*[beep])'
- en: The predictions of both classifiers are recorded in table 6.6, and the plot
    of the dataset and the two boundary lines are shown in figure 6.5.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 两个分类器的预测记录在表 6.6 中，数据集和两个边界线的图示在图 6.5 中。
- en: Table 6.6 Calculation of the log loss for the points in our dataset. Notice
    that the predictions made by classifier 2 are much closer to the labels of the
    points than the predictions made by classifier 1\. Thus, classifier 2 is a better
    classifier.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.6 计算数据集中点的对数损失。注意，分类器 2 的预测比分类器 1 的预测更接近点的标签。因此，分类器 2 是一个更好的分类器。
- en: '| Point | Label | Classifier 1 prediction | Classifier 2 prediction |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 标签 | 分类器 1 预测 | 分类器 2 预测 |'
- en: '| 1 | 0 (Sad) | 0.953 | 0.269 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 (悲伤) | 0.953 | 0.269 |'
- en: '| 2 | 1 (Happy) | 0.731 | 0.731 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 (快乐) | 0.731 | 0.731 |'
- en: '| 3 | 1 (Happy) | 0.119 | 0.731 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 (快乐) | 0.119 | 0.731 |'
- en: '| 4 | 0 (Sad) | 0.881 | 0.119 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 (悲伤) | 0.881 | 0.119 |'
- en: '![](../Images/6-5.png)'
  id: totrans-172
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-5.png)'
- en: 'Figure 6.5 Left: A bad classifier that makes two mistakes. Right: A good classifier
    that classifies all four points correctly.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.5 左：一个犯两个错误的糟糕分类器。右：一个正确分类所有四个点的良好分类器。
- en: From the results in table 6.6 and figure 6.5, it is clear that classifier 2
    is much better than classifier 1\. For instance, in figure 6.5, we can see that
    classifier 2 correctly located the two happy sentences in the positive zone and
    the two sad sentences in the negative zone. Next, we compare the log losses. Recall
    that the log loss for classifier 1 was 5.616\. We should obtain a smaller log
    loss for classifier 2, because this is the better classifier.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 从表 6.6 和图 6.5 的结果来看，很明显分类器 2 比分类器 1 好得多。例如，在图 6.5 中，我们可以看到分类器 2 正确地将两个快乐句子定位在正区域，将两个悲伤句子定位在负区域。接下来，我们比较对数损失。回想一下，分类器
    1 的对数损失是 5.616。我们应该得到一个更小的对数损失，因为这是一个更好的分类器。
- en: 'According to the formula *log loss* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*),
    the log loss for classifier 2 at each of the points in our dataset follows:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 根据公式 *对数损失* = –*y* *ln*(*ŷ*) – (1 – *y*) *ln*(1 – *ŷ*)，我们数据集中每个点的对数损失如下：
- en: '**Point 1**: *y* = 0, *ŷ* = 0.269:'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点1**：*y* = 0, *ŷ* = 0.269：'
- en: '*log loss* = *ln*(1 – 0.269) = 0.313'
  id: totrans-177
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数损失* = *ln*(1 – 0.269) = 0.313'
- en: '**Point 2**: *y* = 1, *ŷ* = 0.73:'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点2**：*y* = 1, *ŷ* = 0.73：'
- en: '*log loss* = *ln*(0.721) = 0.313'
  id: totrans-179
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数损失* = *ln*(0.721) = 0.313'
- en: '**Point 3**: *y* = 1, *ŷ* = 0.73:'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点3**：*y* = 1, *ŷ* = 0.73：'
- en: '*log loss* = *ln*(731) = 0.313'
  id: totrans-181
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数损失* = *ln*(731) = 0.313'
- en: '**Point 4**: *y* = 0, *ŷ* = 0.119:'
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**点4**：*y* = 0, *ŷ* = 0.119：'
- en: '*log loss* = *ln*(1 – 0.119) = 0.127'
  id: totrans-183
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数损失* = *ln*(1 – 0.119) = 0.127'
- en: The total log loss for the dataset is the sum of these four, which is 1.067\.
    Notice that this is much smaller than 5.616, confirming that classifier 2 is indeed
    much better than classifier 1.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的总对数损失是这四个值的总和，为1.067。请注意，这个值远小于5.616，这证实了分类器2确实比分类器1好得多。
- en: How to find a good logistic classifier? The logistic regression algorithm
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何找到一个好的逻辑分类器？逻辑回归算法
- en: 'In this section, we learn how to train a logistic classifier. The process is
    similar to the process of training a linear regression model or a perceptron classifier
    and consists of the following steps:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何训练逻辑分类器。这个过程与训练线性回归模型或感知器分类器的过程类似，包括以下步骤：
- en: Start with a random logistic classifier.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从一个随机的逻辑分类器开始。
- en: 'Repeat many times:'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Slightly improve the classifier.
  id: totrans-189
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 略微改进分类器。
- en: Measure the log loss to decide when to stop running the loop.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测量对数损失以决定何时停止循环。
- en: The key to the algorithm is the step inside the loop, which consists of slightly
    improving a logistic classifier. This step uses a trick called the *logistic trick*.
    The logistic trick is similar to the perceptron trick, as we see in the next section.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 算法的关键是循环内部的步骤，它包括略微改进逻辑分类器。这个步骤使用了一个称为*逻辑技巧*的技巧。逻辑技巧与感知器技巧类似，如下一节所示。
- en: 'The logistic trick: A way to slightly improve the continuous perceptron'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑技巧：略微改进连续感知器的方法
- en: 'Recall from chapter 5 that the perceptron trick consists of starting with a
    random classifier, successively picking a random point, and applying the perceptron
    trick. It had the following two cases:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第5章中提到的感知器技巧，它包括从一个随机的分类器开始，依次选择一个随机点，并应用感知器技巧。它有以下两种情况：
- en: '**Case 1**: If the point is correctly classified, leave the line as it is.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例1**：如果点被正确分类，保持线不变。'
- en: '**Case 2**: If the point is incorrectly classified, move the line a little
    closer to the point.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例2**：如果点被错误分类，将线稍微移近该点。'
- en: 'The logistic trick (illustrated in figure 6.6) is similar to the perceptron
    trick. The only thing that changes is that when the point is well classified,
    we move the line *away* from the point. It has the following two cases:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑技巧（如图6.6所示）与感知器技巧类似。唯一不同的是，当点被正确分类时，我们将线*远离*点。它有以下两种情况：
- en: '**Case 1**: If the point is correctly classified, slightly move the line away
    from the point.'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例1**：如果点被正确分类，稍微将线移离该点。'
- en: '**Case 2**: If the point is incorrectly classified, slightly move the line
    closer to the point.'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**案例2**：如果点被错误分类，稍微将线移近该点。'
- en: '![](../Images/6-6.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/6-6.png)'
- en: Figure 6.6 In the logistic regression algorithm, every point has a say. Points
    that are correctly classified tell the line to move farther away, to be deeper
    in the correct zone. Points that are incorrectly classified tell the line to come
    closer, in hopes of one day being on the correct side of the line.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 在逻辑回归算法中，每个点都有发言权。被正确分类的点告诉线远离，进入正确的区域更深。被错误分类的点告诉线靠近，希望有一天能位于线的正确一侧。
- en: Why do we move the line away from a correctly classified point? If the point
    is well classified, it means it is in the correct zone with respect to the line.
    If we move the line farther away, we move the point even deeper into the correct
    zone. Because the prediction is based on how far the point is from the boundary
    line, for points in the positive (happy) zone, the prediction increases if the
    point is farther from the line. Similarly, for points in the negative (sad) zone,
    the prediction decreases if the point is farther from the line. Thus, if the label
    of the point is 1, we are increasing the prediction (making it even closer to
    1), and if the label of the point is 0, we are decreasing the prediction (making
    it even closer to 0).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们要将线移动到正确分类的点之外？如果点被正确分类，这意味着它在相对于线的正确区域内。如果我们把线移得更远，我们就把点推向正确的区域更深。因为预测是基于点与边界线的距离，对于位于正（快乐）区域的点，如果点离线更远，预测会增加。同样，对于位于负（悲伤）区域的点，如果点离线更远，预测会降低。因此，如果点的标签是1，我们是在增加预测（使其更接近1），如果点的标签是0，我们是在减少预测（使其更接近0）。
- en: 'For example, look at classifier 1 and the first sentence in our dataset. Recall
    that the classifier has weights *a* = 1, *b* = 2, and bias *c* = –4\. The sentence
    corresponds to a point of coordinates (*x*[aack], *x*[beep]) = (3,2), and label
    *y* = 0\. The prediction we obtained for this point was *ŷ* = *σ*(3 + 2 · 2 –
    4) = *σ*(3) = 0.953\. The prediction is quite far from the label, so the error
    is high: in fact, in table 6.5, we calculated it to be 3.049\. The error that
    this classifier made was to think that this sentence is happier than it is. Thus,
    to tune the weights to ensure that the classifier reduces the prediction for this
    sentence, we should drastically decrease the weights *a*, *b*, and the bias *c*.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，看看分类器1和我们的数据集中的第一句话。回想一下，这个分类器的权重 *a* = 1，*b* = 2，以及偏置 *c* = –4。这句话对应于坐标点
    (*x*[aack], *x*[beep]) = (3,2)，标签 *y* = 0。我们对这个点的预测是 *ŷ* = *σ*(3 + 2 · 2 – 4)
    = *σ*(3) = 0.953。预测值与标签相差甚远，因此误差很高：实际上，在表6.5中，我们计算出的误差是3.049。这个分类器犯的错误是认为这句话比实际情况更快乐。因此，为了调整权重以确保分类器降低对这个句子的预测，我们应该大幅降低权重
    *a*，*b* 和偏置 *c*。
- en: Using the same logic, we can analyze how to tune the weights to improve the
    classification for the other points. For the second sentence in the dataset, the
    label is *y* = 1 and the prediction is 0.731\. This is a good prediction, but
    if we want to improve it, we should slightly increase the weights and the bias.
    For the third sentence, because the label is *y* = 1 and the prediction is *ŷ*
    = 0.119, we should drastically increase the weights and the bias. Finally, for
    the fourth sentence, the label is *y* = 0 and the prediction is *ŷ* = 0.119, so
    we should slightly decrease the weights and the bias. These are summarized in
    table 6.7.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 使用相同的逻辑，我们可以分析如何调整权重以改进其他点的分类。对于数据集中的第二句话，标签是 *y* = 1，预测值是 0.731。这是一个好的预测，但如果我们想改进它，我们应该稍微增加权重和偏置。对于第三句话，因为标签是
    *y* = 1，预测值是 *ŷ* = 0.119，我们应该大幅增加权重和偏置。最后，对于第四句话，标签是 *y* = 0，预测值是 *ŷ* = 0.119，因此我们应该稍微降低权重和偏置。这些总结在表6.7中。
- en: Table 6.7 Calculation of the log loss for the points in our dataset. Notice
    that points that are well classified (2 and 4) have a small log loss, whereas
    points that are poorly classified (1 and 3) have a large log loss.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.7 计算数据集中点的对数损失。注意，被正确分类的点（2和4）对数损失很小，而分类不良的点（1和3）对数损失很大。
- en: '| Point | Label *y* | Classifier 1 prediction *y* | How to tune the weights
    *a*, *b*, and the bias *c* | *y* – *ŷ* |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 标签 *y* | 分类器1预测 *y* | 如何调整权重 *a*，*b* 和偏置 *c* | *y* – *ŷ* |'
- en: '| 1 | 0 | 0.953 | Decrease by a large amount | –0.953 |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0.953 | 大幅降低 | –0.953 |'
- en: '| 2 | 1 | 0.731 | Increase by a small amount | 0.269 |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 0.731 | 稍微增加 | 0.269 |'
- en: '| 3 | 1 | 0.119 | Increase by a large amount | 0.881 |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 0.119 | 大幅增加 | 0.881 |'
- en: '| 4 | 0 | 0.119 | Decrease by a small amount | –0.119 |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 0.119 | 稍微降低 | –0.119 |'
- en: 'The following observations can help us figure out the perfect amount that we
    want to add to the weights and bias to improve the predictions:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 以下观察结果可以帮助我们确定我们想要添加到权重和偏置中的完美数量，以改进预测：
- en: '**Observation 1**: the last column of table 6.7 has the value of the label
    minus the prediction. Notice the similarities between the two rightmost columns
    in this table. This hints that the amount we should update the weights and the
    bias should be a multiple of *y* – *ŷ*.'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察1**：表6.7的最后一列是标签减去预测的值。注意这个表中最后两列之间的相似性。这表明我们应该更新权重和偏置的量应该是*y* – *ŷ*的倍数。'
- en: '**Observation 2**: imagine a sentence in which the word *aack* appears 10 times
    and *beep* appears once. If we are to add (or subtract) a value to the weights
    of these two words, it makes sense to think that the weight of *aack* should be
    updated by a larger amount, because this word is more crucial to the overall score
    of the sentence. Thus, the amount we should update the weight of *aack* should
    be multiplied by *x*[aack] , and the amount we should update the weight of *beep*
    should be multiplied by *x*[beep] *.*'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察2**：想象一个句子，其中单词*aack*出现了10次，而*beep*出现了1次。如果我们要向这两个单词的权重中添加（或减去）一个值，那么认为*aack*的权重应该更新得更多是有意义的，因为这个单词对句子的整体得分更为关键。因此，我们应该更新*aack*权重的量应该乘以*x*[aack]，而我们应该更新*beep*权重的量应该乘以*x*[beep]
    *.*'
- en: '**Observation 3**: the amount that we update the weights and biases should
    also be multiplied by the learning rate *η* because we want to make sure that
    this number is small.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察3**：我们更新权重和偏置的量也应该乘以学习率*η*，因为我们想确保这个数字很小。'
- en: 'Putting the three observations together, we conclude that the following is
    a good set of updated weights:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 将三个观察结果结合起来，我们得出以下更新权重是一个很好的集合：
- en: '*a*'' = *a* + *η*(*y* – *ŷ*)*x*[1]'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* = *a* + *η*(*y* – *ŷ*)*x*[1]'
- en: '*b*'' = *b* + *η*(*y* – *ŷ*)*x*[2]'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* = *b* + *η*(*y* – *ŷ*)*x*[2]'
- en: '*c*'' = *c* + *η*(*y* – *ŷ*)'
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* = *c* + *η*(*y* – *ŷ*)'
- en: Thus, the pseudocode for the logistic trick follows. Notice how similar it is
    to the pseudocode for the perceptron trick we learned at the end of the section
    “The perceptron trick” in chapter 5.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，逻辑技巧的伪代码如下。注意它与我们第5章“感知器技巧”部分末尾学习的感知器技巧的伪代码是多么相似。
- en: Pseudocode for the logistic trick
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑技巧的伪代码
- en: 'Inputs:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A logistic classifier with weights *a, b,* and bias *c*
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有权重*a, b,*和偏置*c*的逻辑分类器
- en: A point with coordinates (*x*[1], *x*[2]) and label *y*
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为(*x*[1], *x*[2])和标签*y*的点
- en: A small value *η* (the learning rate)
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的值*η*（学习率）
- en: 'Output:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A perceptron with new weights *a', b',* and bias *c'* which is at least as good
    as the input
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有新权重*a', b',*和偏置*c'*的感知器，至少与输入一样好
- en: 'Procedure:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: The prediction the perceptron makes at the point is *ŷ* = *σ*(*ax*[1] + *bx*[2]
    + *c*).
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 感知器在点上的预测是*ŷ* = *σ*(*ax*[1] + *bx*[2] + *c*).
- en: 'Return:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: 'The perceptron with the following weights and bias:'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有以下权重和偏置的感知器：
- en: '*a*'' = *a* + *η*(*y* - *ŷ*)*x*[1]'
  id: totrans-230
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* = *a* + *η*(*y* - *ŷ*)*x*[1]'
- en: '*b*'' = *b* + *η*(*y* - *ŷ*)*x*[2]'
  id: totrans-231
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* = *b* + *η*(*y* - *ŷ*)*x*[2]'
- en: '*c*'' = *c* + *η*(*y* - *ŷ*)'
  id: totrans-232
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* = *c* + *η*(*y* - *ŷ*)'
- en: The way we updated the weights and bias in the logistic trick is no coincidence.
    It comes from applying the gradient descent algorithm to reduce the log loss.
    The mathematical details are described in appendix B, section “Using gradient
    descent to train classification models.”
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在逻辑技巧中更新权重和偏置的方式并非巧合。它来自于应用梯度下降算法来减少对数损失。数学细节在附录B的第“使用梯度下降训练分类模型”部分中描述。
- en: To verify that the logistic trick works in our case, let’s apply it to the current
    dataset. In fact, we’ll apply the trick to each of the four points separately,
    to see how much each one of them would modify the weights and bias of the model.
    Finally, we’ll compare the log loss at that point before and after the update
    and verify that it has indeed been reduced. For the following calculations, we
    use a learning rate of *η* = 0.05.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证逻辑技巧在我们的案例中是否有效，让我们将其应用于当前数据集。实际上，我们将分别对四个点应用这个技巧，看看每个点会如何修改模型的权重和偏置。最后，我们将比较更新前后的对数损失，以验证它确实已经减少。对于以下计算，我们使用学习率*η*
    = 0.05。
- en: Updating the classifier using each of the sentences
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 使用每个句子更新分类器
- en: 'Using the first sentence:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 使用第一句话：
- en: 'Initial weights and bias: *a* = 1, *b* = 2, *c* = –4'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始权重和偏置：*a* = 1, *b* = 2, *c* = –4
- en: 'Label: *y* = 0'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：*y* = 0
- en: 'Prediction: 0.953'
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：0.953
- en: 'Initial log loss: –0 · *ln*(0.953) – 1 *ln*(1 – 0.953) = 3.049'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 初始对数损失：–0 · *ln*(0.953) – 1 *ln*(1 – 0.953) = 3.049
- en: 'Coordinates of the point: *x*[aack] = 3, *x*[beep] = 2'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 点的坐标：*x*[aack] = 3, *x*[beep] = 2
- en: 'Learning rate: *η* = 0.01'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率：*η* = 0.01
- en: 'Updated weights and bias:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新后的权重和偏置：
- en: a' = 1 + 0.05 · (0 – 0.953) · 3 = 0.857
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: a' = 1 + 0.05 · (0 – 0.953) · 3 = 0.857
- en: b' = 2 + 0.05 · (0 – 0.953) · 2 = 1.905
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: b' = 2 + 0.05 · (0 – 0.953) · 2 = 1.905
- en: c' = –4 + 0.05 · (0 – 0.953) = –4.048
  id: totrans-246
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: c' = –4 + 0.05 · (0 – 0.953) = –4.048
- en: 'Updated prediction: *ŷ* = *σ*(0.857 · 3 + 1.905 · 2 – 4.048 = 0.912\. (Notice
    that the prediction decreased, so it is now closer to the label 0).'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更新预测：*ŷ* = *σ*(0.857 · 3 + 1.905 · 2 – 4.048 = 0.912\. (注意，预测降低，因此现在更接近标签0)。
- en: 'Final log loss: –0 · *ln*(0.912) – 1 *ln*(1 – 0.912) = 2.426\. (Note that the
    error decreased from 3.049 to 2.426).'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终对数损失：–0 · *ln*(0.912) – 1 *ln*(1 – 0.912) = 2.426\. (注意，错误从3.049减少到2.426)。
- en: The calculations for the other three points are shown in table 6.8\. Notice
    that in the table, the updated prediction is always closer to the label than the
    initial prediction, and the final log loss is always smaller than the initial
    log loss. This means that no matter which point we use for the logistic trick,
    we’ll be improving the model for that point and decreasing the final log loss.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 其他三个点的计算显示在表6.8中。注意，在表中，更新后的预测始终比初始预测更接近标签，最终对数损失始终小于初始对数损失。这意味着无论我们使用哪个点进行逻辑回归技巧，我们都会改进该点的模型并减少最终对数损失。
- en: Table 6.8 Calculations of the predictions, log loss, updated weights, and updated
    predictions for all the points.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.8所有点的预测计算、对数损失、更新权重和更新预测。
- en: '| Point | Coordinates | Label | Initial prediction | Initial log loss | Updated
    weights: | Updated prediction | Final log loss |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| 点 | 坐标 | 标签 | 初始预测 | 初始对数损失 | 更新权重： | 更新预测 | 最终对数损失 |'
- en: '| 1 | (3,2) | 0 | 0.953 | 3.049 | *a’* = 0.857*b’* = 1.905*c’* = –4.048 | 0.912
    | 2.426 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| 1 | (3,2) | 0 | 0.953 | 3.049 | *a’* = 0.857*b’* = 1.905*c’* = –4.048 | 0.912
    | 2.426 |'
- en: '| 2 | (1,2) | 1 | 0.731 | 0.313 | *a’* = 1.013*b*’ = 2.027*c*’ = –3.987 | 0.747
    | 0.292 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '| 2 | (1,2) | 1 | 0.731 | 0.313 | *a’* = 1.013*b*’ = 2.027*c*’ = –3.987 | 0.747
    | 0.292 |'
- en: '| 3 | (0,1) | 1 | 0.119 | 2.127 | *a’* = 1*b’* = 2.044*c’* = –3.956 | 0.129
    | 2.050 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| 3 | (0,1) | 1 | 0.119 | 2.127 | *a’* = 1*b’* = 2.044*c’* = –3.956 | 0.129
    | 2.050 |'
- en: '| 4 | (2,0) | 0 | 0.119 | 0.127 | *a’* = 0.988*b’* = 2*c’* = –4.006 | 0.127
    | 0.123 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| 4 | (2,0) | 0 | 0.119 | 0.127 | *a’* = 0.988*b’* = 2*c’* = –4.006 | 0.127
    | 0.123 |'
- en: At the beginning of this section, we discussed that the logistic trick can also
    be visualized geometrically as moving the boundary line with respect to the point.
    More specifically, the line is moved closer to the point if the point is misclassified
    and farther from the point if the point is correctly classified. We can verify
    this by plotting the original classifier and the modified classifier in the four
    cases in table 6.8\. In figure 6.7, you can see the four plots. In each of them,
    the solid line is the original classifier, and the dotted line is the classifier
    obtained by applying the logistic trick, using the highlighted point. Notice that
    points 2 and 4, which are correctly classified, push the line away, whereas points
    1 and 3, which are misclassified, move the line closer to them.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的开头，我们讨论了逻辑回归技巧也可以通过几何方式可视化，即相对于点移动边界线。更具体地说，如果点被错误分类，则将线移近点；如果点被正确分类，则将线移远点。我们可以通过在表6.8中的四种情况下绘制原始分类器和修改后的分类器来验证这一点。在图6.7中，你可以看到这四个图。在每一个图中，实线是原始分类器，虚线是通过应用逻辑回归技巧得到的分类器，使用突出显示的点。注意，正确分类的点2和4将线推开，而错误分类的点1和3将线移近它们。
- en: '![](../Images/6-7.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-7.png)'
- en: Figure 6.7 The logistic trick applied to each of the four data points. Notice
    that for correctly classified points, the line moves away from the point, whereas
    for misclassified points, the line moves closer to the point.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7将逻辑回归技巧应用于每个四个数据点。注意，对于正确分类的点，线远离点，而对于错误分类的点，线移近点。
- en: 'Repeating the logistic trick many times: The logistic regression algorithm'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 重复多次逻辑回归技巧：逻辑回归算法
- en: 'The logistic regression algorithm is what we use to train a logistic classifier.
    In the same way that the perceptron algorithm consists of repeating the perceptron
    trick many times, the logistic regression algorithm consists of repeating the
    logistic trick many times. The pseudocode follows:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法是我们用来训练逻辑分类器的算法。与感知器算法通过多次重复感知器技巧一样，逻辑回归算法通过多次重复逻辑回归技巧来实现。伪代码如下：
- en: Pseudocode for the logistic regression algorithm
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法的伪代码
- en: 'Inputs:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A dataset of points, labeled 1 and 0
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标记为1和0的点数据集
- en: A number of epochs, *n*
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 许多个时代，*n*
- en: A learning rate *η*
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习率 *η*
- en: 'Output:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A logistic classifier consisting of a set of weights and a bias, which fits
    the dataset
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由一组权重和偏差组成的逻辑分类器，该分类器适合数据集
- en: 'Procedure:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Start with random values for the weights and bias of the logistic classifier.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从随机值开始，为逻辑分类器的权重和偏差设置初始值。
- en: 'Repeat many times:'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Update the weights and the bias using the logistic trick.
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用逻辑技巧更新权重和偏差。
- en: 'Return:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 返回：
- en: The perceptron classifier with the updated weights and bias
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用更新后的权重和偏差的感知器分类器
- en: As we saw previously, each iteration of the logistic trick either moves the
    line closer to a misclassified point or farther away from a correctly classified
    point.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们之前看到的，逻辑技巧的每一次迭代要么将线移动到错误分类点的附近，要么将其移动到正确分类点的附近。
- en: Stochastic, mini-batch, and batch gradient descent
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降、小批量梯度下降和批量梯度下降
- en: The logistic regression algorithm, together with linear regression and the perceptron,
    is another algorithm that is based on gradient descent. If we use gradient descent
    to reduce the log loss, the gradient descent step becomes the logistic trick.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法，与线性回归和感知器一样，是基于梯度下降的另一种算法。如果我们使用梯度下降来减少对数损失，那么梯度下降步骤就变成了逻辑技巧。
- en: The general logistic regression algorithm works not only for datasets with two
    features but for datasets with as many features as we want. In this case, just
    like the perceptron algorithm, the boundary won’t look like a line, but it would
    look like a higher-dimensional hyperplane splitting points in a higher dimensional
    space. However, we don’t need to visualize this higher-dimensional space; we only
    need to build a logistic regression classifier with as many weights as features
    in our data. The logistic trick and the logistic algorithm update the weights
    in a similar way to what we did in the previous sections.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 通用逻辑回归算法不仅适用于具有两个特征的数据集，也适用于具有我们想要的任何数量特征的数据集。在这种情况下，就像感知器算法一样，边界不会看起来像一条线，而会像更高维空间中分割点的更高维超平面。然而，我们不需要可视化这个更高维的空间；我们只需要构建一个具有与我们的数据中特征数量一样多的权重的逻辑回归分类器。逻辑技巧和逻辑算法以与我们之前章节中相同的方式更新权重。
- en: Just like with the previous algorithms we learned, in practice, we don’t update
    the model by picking one point at a time. Instead, we use mini-batch gradient
    descent—we take a batch of points and update the model to fit those points better.
    For the fully general logistic regression algorithm and a thorough mathematical
    derivation of the logistic trick using gradient descent, please refer to appendix
    B, section “Using gradient descent to train classification models.”
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前学过的算法一样，在实践中，我们不会一次只选择一个点来更新模型。相反，我们使用小批量梯度下降——我们取一批点并更新模型以更好地适应这些点。对于完全通用的逻辑回归算法和利用梯度下降对逻辑技巧的彻底数学推导，请参阅附录B，第“使用梯度下降训练分类模型”节。
- en: Coding the logistic regression algorithm
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 编写逻辑回归算法
- en: 'In this section, we see how to code the logistic regression algorithm by hand.
    The code for this section follows:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到如何手动编写逻辑回归算法。本节的代码如下：
- en: '**Notebook**: Coding_logistic_regression.ipynb'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**：Coding_logistic_regression.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Coding_logistic_regression.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Coding_logistic_regression.ipynb)'
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Coding_logistic_regression.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Coding_logistic_regression.ipynb)'
- en: We’ll test our code in the same dataset that we used in chapter 5\. The dataset
    is shown in table 6.9.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第5章中使用的数据集上测试我们的代码。数据集如表6.9所示。
- en: Table 6.9 The dataset that we will fit with a logistic classifier
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.9 我们将用逻辑分类器拟合的数据集
- en: '| *Aack* *x*[1] | *Beep* *x*[2] | Label *y* |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '| *Aack* *x*[1] | *Beep* *x*[2] | 标签 *y* |'
- en: '| 1 | 0 | 0 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 |'
- en: '| 0 | 2 | 0 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 2 | 0 |'
- en: '| 1 | 1 | 0 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 |'
- en: '| 1 | 2 | 0 |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 | 0 |'
- en: '| 1 | 3 | 1 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 3 | 1 |'
- en: '| 2 | 2 | 1 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2 | 1 |'
- en: '| 2 | 3 | 1 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 3 | 1 |'
- en: '| 3 | 2 | 1 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 2 | 1 |'
- en: 'The code for loading our small dataset follows, and the plot of the dataset
    is shown in figure 6.8:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 加载我们小型数据集的代码如下，数据集的图表如图6.8所示：
- en: '[PRE0]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '![](../Images/6-8.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-8.png)'
- en: Figure 6.8 The plot of our dataset, where the happy sentences are represented
    by triangles and the sad sentences by squares.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 我们数据集的图表，其中快乐的句子由三角形表示，悲伤的句子由正方形表示。
- en: Coding the logistic regression algorithm by hand
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 手动编写逻辑回归算法
- en: 'In this section, we see how to code the logistic trick and the logistic regression
    algorithm by hand. More generally, we’ll code the logistic regression algorithm
    for a dataset with *n* weights. The notation we use follows:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示了如何手动编写逻辑技巧和逻辑回归算法。更一般地，我们将为具有*n*个权重的数据集编写逻辑回归算法。我们使用的符号如下：
- en: 'Features: *x*[1], *x*[2], … , *x*[n]'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征：*x*[1], *x*[2], … , *x*[n]
- en: 'Label: *y*'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签：*y*
- en: 'Weights: *w*[1], *w*[2], … , *w*[n]'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重：*w*[1], *w*[2], … , *w*[n]
- en: 'Bias: *b*'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏置：*b*
- en: The score for a particular sentence is the sigmoid of the sum of the weight
    of each word (*w*[i]) times the number of times that appears (*x*[i]), plus the
    bias (*b*). Notice that we use the summation notation for
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 特定句子的分数是该句子中每个单词的权重（*w*[i]）乘以该单词出现的次数（*x*[i]）的总和的sigmoid函数，再加上偏置（*b*）。注意，我们使用求和符号表示
- en: '![](../Images/06_08_E01.png).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/06_08_E01.png).'
- en: 'Prediction: *ŷ* = *σ*(*w*[1]*x*[1] + *w*[2]*x*[2] + … + *w*[n]*x*[n] + *b*)
    = *σ*(Σ[i]^n[=1]*w*[i] *x*[i] + *b*).'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测：*ŷ* = *σ*(*w*[1]*x*[1] + *w*[2]*x*[2] + … + *w*[n]*x*[n] + *b*) = *σ*(Σ[i]^n[=1]*w*[i]
    *x*[i] + *b*).
- en: For our current problem, we’ll refer to *x*[aack] and *x*[beep] as *x*[1] and
    *x*[2], respectively. Their corresponding weights are *w*[1] and *w*[1], and the
    bias is *b.*
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们当前的问题，我们将*x*[aack]和*x*[beep]分别称为*x*[1]和*x*[2]。它们对应的权重是*w*[1]和*w*[1]，偏置是*b*。
- en: We start by coding the sigmoid function, the score, and the prediction. Recall
    that the formula for the sigmoid function is
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先编写sigmoid函数、分数和预测的代码。回忆一下sigmoid函数的公式
- en: '![](../Images/06_08_E02.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06_08_E02.png)'
- en: '[PRE1]'
  id: totrans-311
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: For the score function, we use the dot product between the features and the
    weights. Recall that the dot product between vectors (*x*[1], *x*[2], … , *x*[n])
    and (*w*[1], *w*[2], … , *w*[n]) is *w*[1] *x*[1] + *w*[2] *x*[2] + … + *w*[n]
    *x*[n].
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分数函数，我们使用特征和权重之间的点积。回忆一下，向量(*x*[1], *x*[2], … , *x*[n])和(*w*[1], *w*[2], …
    , *w*[n])之间的点积是*w*[1] *x*[1] + *w*[2] *x*[2] + … + *w*[n] *x*[n]。
- en: '[PRE2]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Finally, recall that the prediction is the sigmoid activation function applied
    to the score.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，回忆一下，预测是sigmoid激活函数应用于分数。
- en: '[PRE3]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now that we have the prediction, we can proceed to the log loss. Recall that
    the formula for the log loss is
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了预测，我们可以继续计算log loss。回忆一下，log loss的公式是
- en: '*log loss* = –*y ln*(*ŷ*) – (1 – *y*) *ln*(1 – *y*).'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '*log loss* = –*y ln*(*ŷ*) – (1 – *y*) *ln*(1 – *y*).'
- en: 'Let’s code that formula as follows:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们按照以下方式编写该公式：
- en: '[PRE4]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We need the log loss over the whole dataset, so we can add over all the data
    points as shown here:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要整个数据集上的log loss，因此我们可以将所有数据点相加，如下所示：
- en: '[PRE5]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we are ready to code the logistic regression trick, and the logistic regression
    algorithm. In more than two variables, recall that the logistic regression step
    for the *i*-th weight is the following formula, where *η* is the learning rate:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们准备编写逻辑回归技巧和逻辑回归算法的代码。在超过两个变量的情况下，回忆一下，第*i*个权重的逻辑回归步骤是以下公式，其中*η*是学习率：
- en: '*w*[i] → *w*[i] + *η*(*y* – *ŷ*)*x*[i] for *i* = 1, 2, … , *n*'
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[i] → *w*[i] + *η*(*y* – *ŷ*)*x*[i] for *i* = 1, 2, … , *n*'
- en: '*b* → *b* + *η*(*y* – *ŷ*) for *i* = 1, 2, … , *n*.'
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* → *b* + *η*(*y* – *ŷ*) for *i* = 1, 2, … , *n*.'
- en: '[PRE6]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Now we can run the logistic regression algorithm to build a logistic classifier
    that fits our dataset as follows:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以运行逻辑回归算法来构建一个适合我们数据集的逻辑分类器，如下所示：
- en: '[PRE7]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'The classifier we obtain has the following weights and biases:'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们获得分类器具有以下权重和偏置：
- en: '*w*[1] = 0.47'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[1] = 0.47'
- en: '*w*[2] = 0.10'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[2] = 0.10'
- en: '*b* = –0.68'
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* = –0.68'
- en: The plot of the classifier (together with a plot of the previous classifiers
    at each of the epochs) is depicted in figure 6.9.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的图（以及每个epoch的先前分类器的图）如图6.9所示。
- en: '![](../Images/6-9.png)'
  id: totrans-333
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-9.png)'
- en: Figure 6.9 The boundary of the resulting logistic classifier
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 结果逻辑分类器的边界
- en: In figure 6.10, we can see the plot of the classifiers corresponding to all
    the epochs (left) and the plot of the log loss (right). On the plot of the intermediate
    classifiers, the final one corresponds to the dark line. Notice from the log loss
    plot that, as we run the algorithm for more epochs, the log loss decreases drastically,
    which is exactly what we want. Furthermore, the log loss is never zero, even though
    all the points are correctly classified. This is because for any point, no matter
    how well classified, the log loss is never zero. Contrast this to figure 5.26
    in chapter 5, where the perceptron loss indeed reaches a value of zero when every
    point is correctly classified.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.10中，我们可以看到所有epoch对应的分类器图表（左）和log损失图表（右）。在中间分类器图表中，最后一个对应的是深色线。从log损失图表中可以看出，随着我们运行算法的epoch数增加，log损失急剧下降，这正是我们想要的。此外，log损失永远不会为零，即使所有点都被正确分类。这是因为对于任何点，无论分类得多好，log损失永远不会为零。这与第5章中的图5.26形成对比，在那里感知器损失在所有点都被正确分类时确实达到了零值。
- en: '![](../Images/6-10.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/6-10.png)'
- en: 'Figure 6.10 Left: A plot of all the intermediate steps of the logistic regression
    algorithm. Notice that we start with a bad classifier and slowly move toward a
    good one (the thick line). Right: The error plot. Notice that the more epochs
    we run the logistic regression algorithm, the lower the error gets.'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 左：逻辑回归算法所有中间步骤的图表。注意我们从一个糟糕的分类器开始，逐渐向一个好的分类器（粗线）移动。右：错误图。注意随着我们运行逻辑回归算法的epoch数增加，错误率降低。
- en: 'Real-life application: Classifying IMDB reviews with Turi Create'
  id: totrans-338
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 实际应用：使用Turi Create对IMDB评论进行分类
- en: 'In this section, we see a real-life application of the logistic classifier
    in sentiment analysis. We use Turi Create to build a model that analyzes movie
    reviews on the popular IMDB site. The code for this section follows:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了逻辑分类器在情感分析中的实际应用。我们使用Turi Create构建一个分析IMDB网站上电影评论的模型。本节的代码如下：
- en: '**Notebook**: Sentiment_analysis_IMDB.ipynb'
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**: Sentiment_analysis_IMDB.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Sentiment_analysis_IMDB.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Sentiment_analysis_IMDB.ipynb)'
  id: totrans-341
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Sentiment_analysis_IMDB.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_6_Logistic_Regression/Sentiment_analysis_IMDB.ipynb)'
- en: '**Dataset**: IMDB_Dataset.csv'
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**: IMDB_Dataset.csv'
- en: 'First, we import Turi Create, download the dataset, and convert it into an
    SFrame, which we call `movies`, as follows:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们导入Turi Create，下载数据集，并将其转换为SFrame，我们称之为`movies`，如下所示：
- en: '[PRE8]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The first five rows of the dataset appear in table 6.10.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的前五行显示在表6.10中。
- en: Table 6.10 The first five rows of the IMDB dataset. The Review column has the
    text of the review, and the Sentiment column has the sentiment of the review,
    which can be positive or negative.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.10 IMDB数据集的前五行。评论列包含评论文本，情感列包含评论的情感，可以是积极或消极。
- en: '| Review | Sentiment |'
  id: totrans-347
  prefs: []
  type: TYPE_TB
  zh: '| 评论 | 情感 |'
- en: '| One of the other reviewers has mentioned... | Positive |'
  id: totrans-348
  prefs: []
  type: TYPE_TB
  zh: '| 另一位评论者提到... | 积极 |'
- en: '| A wonderful little production... | Positive |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '| 一部精彩的小制作... | 积极 |'
- en: '| I thought this was a wonderful day to spend... | Positive |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '| 我觉得这是一个度过美好一天的好日子... | 积极 |'
- en: '| Basically, there’s a family where a little... | Negative |'
  id: totrans-351
  prefs: []
  type: TYPE_TB
  zh: '| 基本上，有一个家庭里有一个小... | 消极 |'
- en: '| Petter Mattei’s “Love in the time of money” is a... | Positive |'
  id: totrans-352
  prefs: []
  type: TYPE_TB
  zh: '| Petter Mattei的《金钱时代里的爱情》是一部... | 积极 |'
- en: 'The dataset has two columns, one with the review, as a string, and one with
    the sentiment, as positive or negative. First, we need to process the string,
    because each of the words needs to be a different feature. The Turi Create built-in
    function `count_words` in the `text_analytics` package is useful for this task,
    because it turns a sentence into a dictionary with the word counts. For example,
    the sentence “to be or not to be” is turned into the dictionary {‘to’:2, ‘be’:2,
    ‘or’:1, ‘not’:1}. We add a new column called `words` containing this dictionary
    as follows:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集有两列，一列是评论，作为字符串，另一列是情感，作为积极或消极。首先，我们需要处理字符串，因为每个单词都需要成为一个不同的特征。Turi Create内置的`text_analytics`包中的`count_words`函数对于这个任务很有用，因为它将一个句子转换成一个包含单词计数的字典。例如，句子“to
    be or not to be”被转换成字典{‘to’:2, ‘be’:2, ‘or’:1, ‘not’:1}。我们添加一个名为`words`的新列，包含这个字典，如下所示：
- en: '[PRE9]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The first few rows of our dataset with the new column are shown in table 6.11.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的新列的前几行显示在表6.11中。
- en: Table 6.11 The Words column is a dictionary where each word in the review is
    recorded together with its number of appearances. This is the column of features
    for our logistic classifier.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6.11 词语列是一个字典，其中记录了评论中的每个词语及其出现次数。这是我们的逻辑分类器的特征列。
- en: '| Review | Sentiment | Words |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '| 评论 | 情感 | 词语 |'
- en: '| One of the other reviewers has mentioned... | Positive | {''if'': 1.0, ''viewing'':
    1.0, ''comfortable'': 1.0, ... |'
  id: totrans-358
  prefs: []
  type: TYPE_TB
  zh: '| 另一位评论者提到... | 积极 | {''if'': 1.0, ''viewing'': 1.0, ''comfortable'': 1.0,
    ... |'
- en: '| A wonderful little production... | Positive | {''done'': 1.0, ''every'':
    1.0, ''decorating'': 1.0, ... |'
  id: totrans-359
  prefs: []
  type: TYPE_TB
  zh: '| 一部美好的小制作... | 积极 | {''done'': 1.0, ''every'': 1.0, ''decorating'': 1.0, ...
    |'
- en: '| I thought this was a wonderful day to spend... | Positive | {''see'': 1.0,
    ''go'': 1.0, ''great'': 1.0, ''superm ... |'
  id: totrans-360
  prefs: []
  type: TYPE_TB
  zh: '| 我觉得这是一个度过美好的一天... | 积极 | {''see'': 1.0, ''go'': 1.0, ''great'': 1.0, ''superm
    ... |'
- en: '| Basically, there’s a family where a little... | Negative | {''them'': 1.0,
    ''ignore'': 1.0, ''dialogs'': 1.0, ... |'
  id: totrans-361
  prefs: []
  type: TYPE_TB
  zh: '| 基本上，有一个家庭，一个小孩... | 消极 | {''them'': 1.0, ''ignore'': 1.0, ''dialogs'': 1.0,
    ... |'
- en: '| Peter Mattei’s *Love in the Time of Money* is a... | Positive | {''work'':
    1.0, ''his'': 1.0, ''for'': 1.0, ''anxiously'': ... |'
  id: totrans-362
  prefs: []
  type: TYPE_TB
  zh: '| 彼得·马泰利的《金钱时代里的爱情》是... | 积极 | {''work'': 1.0, ''his'': 1.0, ''for'': 1.0,
    ''anxiously'': ... |'
- en: 'We are ready to train our model! For this, we use the function `create` in
    the `logistic_classifier` package, in which we specify the target (label) to be
    the `sentiment` column and the features to be the `words` column. Note that the
    target is expressed as a string with the name of the column containing the label,
    but the features are expressed as an array of strings with the names of the columns
    containing each of the features (in case we need to specify several columns),
    as shown here:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经准备好训练我们的模型了！为此，我们使用 `logistic_classifier` 包中的 `create` 函数，在其中我们指定目标（标签）为
    `sentiment` 列，特征为 `words` 列。请注意，目标以包含标签的列的名称命名的字符串形式表示，但特征以包含每个特征的列名称的字符串数组形式表示（以防我们需要指定多个列），如下所示：
- en: '[PRE10]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that we’ve trained our model, we can look at the weights of the words,
    with the `coefficients` command. The table we obtain has several columns, but
    the ones we care about are `index` and `value`, which show the words and their
    weights. The top five follow:'
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了我们的模型，我们可以查看词语的权重，使用 `coefficients` 命令。我们得到的表格有几列，但我们关心的是 `index` 和
    `value` 列，它们显示了词语及其权重。前五项如下：
- en: '(intercept): 0.065'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: (截距)：0.065
- en: 'if: –0.018'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'if: –0.018'
- en: 'viewing: 0.089'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'viewing: 0.089'
- en: 'comfortable: 0.517'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'comfortable: 0.517'
- en: 'become: 0.106'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'become: 0.106'
- en: 'The first one, called intercept, is the bias. Because the bias of the model
    is positive, the empty review is positive, as we learned in chapter 5, in the
    section “The bias, the *y*-intercept, and the inherent mood of a quiet alien.”
    This makes sense, because users who rate movies negatively tend to leave a review,
    whereas many users who rate movies positively don’t leave any review. The other
    words are neutral, so their weights don’t mean very much, but let’s explore the
    weights of some words, such as *wonderful*, *horrible*, and *the*, as shown next:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个，称为截距，是偏差。因为模型的偏差是正的，所以空评论是积极的，正如我们在第 5 章的“偏差、*y*-截距和安静外星人的固有情绪”一节中学到的。这很有道理，因为给电影打负分的用户倾向于留下评论，而许多给电影打正分的用户则不留任何评论。其他词语是中性的，所以它们的权重意义不大，但让我们探索一些词语的权重，例如
    *wonderful*、*horrible* 和 *the*，如下所示：
- en: 'wonderful: 1.043'
  id: totrans-372
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'wonderful: 1.043'
- en: 'horrible: –1.075'
  id: totrans-373
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'horrible: –1.075'
- en: 'the: 0.0005'
  id: totrans-374
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'the: 0.0005'
- en: 'As we see, the weight of the word *wonderful* is positive, the weight of the
    word *horrible* is negative, and the weight of the word *the* is small. This makes
    sense: *wonderful* is a positive word, *horrible* is a negative word, and *the*
    is a neutral word.'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 'As we see, the weight of the word *wonderful* is positive, the weight of the
    word *horrible* is negative, and the weight of the word *the* is small. This makes
    sense: *wonderful* is a positive word, *horrible* is a negative word, and *the*
    is a neutral word.'
- en: 'As a last step, let’s find the most positive and negative reviews. For this,
    we use the model to make predictions for all the movies. These predictions will
    be stored in a new column called `predictions`, using the following command:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后一步，让我们找出最积极和消极的评论。为此，我们使用模型对所有电影进行预测。这些预测将存储在一个名为 `predictions` 的新列中，使用以下命令：
- en: '[PRE11]'
  id: totrans-377
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Let’s find the most positive and most negative movies, according to the model.
    We do this by sorting the array, as follows:'
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 根据模型，让我们找出最积极和最消极的电影。我们通过以下方式对数组进行排序：
- en: 'Most positive review:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 最积极评论：
- en: '[PRE12]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '**Output**: “It seems to me that a lot of people don’t know that *Blade* is
    actually a superhero movie on par with *X-Men*…”'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**： “在我看来，很多人不知道实际上 *Blade* 是一部与 *X-Men* 相当的超级英雄电影……”'
- en: 'Most negative review:'
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 最多负面评论：
- en: '[PRE13]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '**Output**: “Even duller, if possible, than the original…”'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：“甚至比原来的更无趣……”'
- en: We could do a lot more to improve this model. For example, some text manipulation
    techniques, such as removing punctuation and capitalization, or removing stop
    words (such as *the*, *and*, *of*, *it*), tend to give us better results. But
    it’s great to see that with a few lines of code, we can build our own sentiment
    analysis classifier!
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以做很多事情来改进这个模型。例如，一些文本处理技术，如删除标点符号和大小写，或删除停用词（如 *the*，*and*，*of*，*it*），通常会给我们更好的结果。但是，看到我们只需要几行代码就能构建自己的情感分析分类器真是太棒了！
- en: 'Classifying into multiple classes: The softmax function'
  id: totrans-386
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将多个类别分类：softmax函数
- en: So far we have seen continuous perceptrons classify two classes, happy and sad.
    But what if we have more classes? At the end of chapter 5, we discussed that classifying
    between more than two classes is hard for a discrete perceptron. However, this
    is easy to do with a logistic classifier.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经看到连续感知器将两个类别分类为快乐和悲伤。但是，如果我们有更多类别呢？在第5章的结尾，我们讨论了对于离散感知器来说，在两个以上类别之间进行分类是困难的。然而，使用逻辑分类器来做这件事很容易。
- en: 'Imagine an image dataset with three labels: “dog”, “cat”, and “bird”. The way
    to build a classifier that predicts one of these three labels for every image
    is to build three classifiers, one for each one of the labels. When a new image
    comes in, we evaluate it with each of the three classifiers. The classifier corresponding
    to each animal returns a probability that the image is the corresponding animal.
    We then classify the image as the animal from the classifier that returned the
    highest probability.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一个包含三个标签的图像数据集：“狗”、“猫”和“鸟”。要构建一个分类器，预测每个图像的这三个标签之一，就需要为每个标签构建三个分类器，每个标签一个。当有新的图像进来时，我们用这三个分类器中的每一个来评估它。对应于每个动物的分类器返回一个概率，表示图像是相应的动物。然后，我们将图像分类为返回最高概率的分类器对应的动物。
- en: This, however, is not the ideal way to do it, because this classifier returns
    a discrete answer, such as “dog,” “cat,” or “bird.” What if we wanted a classifier
    that returns probabilities for the three animals? Say, an answer could be of the
    form “10% dog, 85% cat, and 5% bird.” The way we do this is using the softmax
    function.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是做这件事的理想方式，因为这个分类器返回的是一个离散的答案，例如“狗”、“猫”或“鸟”。如果我们想要一个返回三种动物概率的分类器呢？比如说，一个答案可能是这样的形式：“10%狗，85%猫，5%鸟。”我们这样做是使用softmax函数。
- en: 'The softmax function works as follows: recall that a logistic classifier makes
    a prediction using a two-step process—first it calculates a score, and then it
    applies the sigmoid function to this score. Let’s forget about the sigmoid function
    and output the score instead. Now imagine that the three classifiers returned
    the following scores:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: softmax函数的工作原理如下：回想一下，逻辑分类器使用两步过程进行预测——首先它计算一个分数，然后它将sigmoid函数应用于这个分数。现在让我们忘记sigmoid函数，输出分数。现在想象三个分类器返回以下分数：
- en: 'Dog classifier: 3'
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗分类器：3
- en: 'Cat classifier: 2'
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫分类器：2
- en: 'Bird classifier: –1'
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟分类器：-1
- en: 'How do we turn these scores into probabilities? Well, here’s an idea: we can
    normalize. This means dividing all these numbers by their sum, which is five,
    to get them to add to one. When we do this, we get the probabilities 3/5 for dog,
    2/5 for cat, and –1/5 for bird. This works, but it’s not ideal, because the probability
    of the image being a bird is a negative number. Probabilities must always be positive,
    so we need to try something different.'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将这些分数转换为概率呢？好吧，这里有一个想法：我们可以进行归一化。这意味着将这些数字除以它们的总和，即五，使它们加起来等于一。当我们这样做时，我们得到狗的概率为3/5，猫的概率为2/5，鸟的概率为-1/5。这可以工作，但并不理想，因为图像是鸟的概率是一个负数。概率必须始终为正，所以我们需要尝试不同的方法。
- en: 'What we need is a function that is always positive and that is also increasing.
    Exponential functions work great for this. Any exponential function, such as 2^x,
    3^x, or 10^x, would do the job. By default, we use the function *e*^x, which has
    wonderful mathematical properties (e.g., the derivative of *e*^x is also *e*^x
    ). We apply this function to the scores, to get the following values:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要的是一个始终为正且递增的函数。指数函数非常适合这个。任何指数函数，如2^x，3^x或10^x，都可以完成这项工作。默认情况下，我们使用函数 *e*^x，它具有美好的数学特性（例如，*e*^x的导数也是
    *e*^x）。我们将此函数应用于分数，得到以下值：
- en: 'Dog classifier: *e*³ = 20.085'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗分类器：*e*³ = 20.085
- en: 'Cat classifier: *e*² = 7.389'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫分类器：*e*² = 7.389
- en: 'Bird classifier: *e*^(–1) = 0.368'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟分类器：*e*^(–1) = 0.368
- en: 'Now, we do what we did before—we normalize, or divide by the sum of these numbers
    for them to add to one. The sum is 20.085 + 7.389 + 0.368 = 27.842, so we get
    the following:'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们做之前做过的事情——归一化，或者除以这些数字的总和，使它们加起来为1。总和是20.085 + 7.389 + 0.368 = 27.842，所以我们得到以下结果：
- en: 'Probability of dog: 20.085/27.842 = 0.721'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 狗的概率：20.085/27.842 = 0.721
- en: 'Probability of cat: 7.389/27.842 = 0.265'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫的概率：7.389/27.842 = 0.265
- en: 'Probability of bird: 0.368/27.842 = 0.013'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 鸟的概率：0.368/27.842 = 0.013
- en: 'These are the three probabilities given by our three classifiers. The function
    we used was the softmax, and the general version follows: if we have *n* classifiers
    that output the *n* scores *a*[1], *a*[2], … , *a*[n], the probabilities obtained
    are *p*[1], *p*[2], … , *p*[n], where'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们三个分类器给出的三个概率。我们使用的函数是softmax，其通用版本如下：如果我们有 *n* 个分类器，它们输出 *n* 个分数 *a*[1]，*a*[2]，…
    ，*a*[n]，得到的概率是 *p*[1]，*p*[2]，… ，*p*[n]，其中
- en: '![](../Images/06_10_E01.png)'
  id: totrans-404
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06_10_E01.png)'
- en: This formula is known as the softmax function.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这个公式被称为softmax函数。
- en: What would happen if we use the softmax function for only two classes? We obtain
    the sigmoid function. Why not convince yourself of this as an exercise?
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只对两个类别使用softmax函数会发生什么？我们会得到sigmoid函数。为什么不作为练习来验证这一点呢？
- en: Summary
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Continuous perceptrons, or logistic classifiers, are similar to perceptron classifiers,
    except instead of making a discrete prediction such as 0 or 1, they predict any
    number between 0 and 1.
  id: totrans-408
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续感知器，或逻辑分类器，与感知器分类器类似，除了它们不会做出0或1这样的离散预测，而是预测0到1之间的任何数字。
- en: Logistic classifiers are more useful than discrete perceptrons, because they
    give us more information. Aside from telling us which class the classifier predicts,
    they also give us a probability. A good logistic classifier would assign low probabilities
    to points with label 0 and high probabilities to points with label 1.
  id: totrans-409
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑分类器比离散感知器更有用，因为它们提供了更多信息。除了告诉我们分类器预测了哪个类别，它们还提供了一个概率。一个好的逻辑分类器会给标签为0的点分配低概率，给标签为1的点分配高概率。
- en: The log loss is an error function for logistic classifiers. It is calculated
    separately for every point as the negative of the natural logarithm of the probability
    that the classifier assigns to its label.
  id: totrans-410
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数损失是逻辑分类器的误差函数。它对每个点分别计算，作为分类器分配给其标签的概率的自然对数的负数。
- en: The total log loss of a classifier on a dataset is the sum of the log loss at
    every point.
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类器在数据集上的总对数损失是每个点的对数损失的加和。
- en: The logistic trick takes a labeled data point and a boundary line. If the point
    is incorrectly classified, the line is moved closer to the point, and if it is
    correctly classified, the line is moved farther from the point. This is more useful
    than the perceptron trick, because the perceptron trick doesn’t move the line
    if the point is correctly classified.
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑技巧接受一个标记数据点和边界线。如果点被错误分类，则将线移近点；如果点被正确分类，则将线移远点。这比感知器技巧更有用，因为感知器技巧在点被正确分类时不会移动线。
- en: The logistic regression algorithm is used to fit a logistic classifier to a
    labeled dataset. It consists of starting with a logistic classifier with random
    weights and continuously picking a random point and applying the logistic trick
    to obtain a slightly better classifier.
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归算法用于将逻辑分类器拟合到标记数据集。它包括从一个具有随机权重的逻辑分类器开始，并持续随机选择一个点，应用逻辑技巧以获得一个略微更好的分类器。
- en: When we have several classes to predict, we can build several linear classifiers
    and combine them using the softmax function.
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们有多个类别要预测时，我们可以构建多个线性分类器，并使用softmax函数将它们组合起来。
- en: Exercises
  id: totrans-415
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 6.1
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.1
- en: A dentist has trained a logistic classifier on a dataset of patients to predict
    if they have a decayed tooth. The model has determined that the probability that
    a patient has a decayed tooth is
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 一位牙医在患者数据集上训练了一个逻辑分类器，以预测他们是否有蛀牙。模型确定患者有蛀牙的概率是
- en: '*σ*(*d* + 0.5*c* – 0.8),'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: '*σ*(*d* + 0.5*c* – 0.8),'
- en: where
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*d* is a variable that indicates whether the patient has had another decayed
    tooth in the past, and'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d* 是一个变量，表示患者过去是否有过其他蛀牙，并且'
- en: '*c* is a variable that indicates whether the patient eats candy.'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*c* 是一个变量，表示患者是否吃糖果。'
- en: For example, if a patient eats candy, then *c* = 1, and if they don’t, then
    *c* = 0\. What is the probability that a patient that eats candy and was treated
    for a decayed tooth last year has a decayed tooth today?
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个病人吃了糖果，那么 *c* = 1，如果没有吃，那么 *c* = 0。那么一个去年治疗过蛀牙且现在吃糖果的病人今天有蛀牙的概率是多少？
- en: Exercise 6.2
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.2
- en: Consider the logistic classifier that assigns to the point (*x*[1], *x*[2])
    the prediction *ŷ* = *σ*(2*x*[1] + 3*x*[2] – 4), and the point *p* = (1, 1) with
    label 0.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑将点 (*x*[1], *x*[2]) 分配给预测 *ŷ* = *σ*(2*x*[1] + 3*x*[2] – 4) 的逻辑分类器，以及点 *p*
    = (1, 1) 带有标签 0。
- en: Calculate the prediction *ŷ* that the model gives to the point *p*.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型对点 *p* 的预测 *ŷ*。
- en: Calculate the log loss that the model produces at the point *p*.
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算模型在点 *p* 处产生的对数损失。
- en: Use the logistic trick to obtain a new model that produces a smaller log loss.
    You can use *η* = 0.1 as the learning rate.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用逻辑技巧获得一个产生更小对数损失的新的模型。你可以使用 *η* = 0.1 作为学习率。
- en: Find the prediction given by the new model at the point *p*, and verify that
    the log loss obtained is smaller than the original.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到新模型在点 *p* 处的预测，并验证获得的对数损失是否小于原始值。
- en: Exercise 6.3
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.3
- en: Using the model in the statement of exercise 6.2, find a point for which the
    prediction is 0.8.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 使用练习 6.2 中的模型，找到一个预测值为 0.8 的点。
- en: hint First find the score that will give a prediction of 0.8, and recall that
    the prediction is *ŷ* = *σ*(score).
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：首先找到给出预测值为 0.8 的分数，并记住预测是 *ŷ* = *σ*(score)。
