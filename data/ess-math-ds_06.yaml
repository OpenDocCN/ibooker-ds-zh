- en: Chapter 6\. Logistic Regression and Classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章 逻辑回归和分类
- en: In this chapter we are going to cover *logistic regression*, a type of regression
    that predicts a probability of an outcome given one or more independent variables.
    This in turn can be used for *classification*, which is predicting categories
    rather than real numbers as we did with linear regression.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍*逻辑回归*，一种根据一个或多个自变量预测结果概率的回归类型。这反过来可以用于*分类*，即预测类别而不是像线性回归那样预测实数。
- en: We are not always interested in representing variables as *continuous*, where
    they can represent an infinite number of real decimal values. There are situations
    where we would rather variables be *discrete*, or representative of whole numbers,
    integers, or booleans (1/0, true/false). Logistic regression is trained on an
    output variable that is discrete (a binary 1 or 0) or a categorical number (which
    is a whole number). It does output a continuous variable in the form of probability,
    but that can be converted into a discrete value with a threshold.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们并不总是对将变量表示为*连续*感兴趣，其中它们可以表示无限数量的实数十进制值。有些情况下，我们更希望变量是*离散*的，或者代表整数、布尔值（1/0，真/假）。逻辑回归是在一个离散的输出变量上进行训练的（二进制1或0）或一个分类数字（整数）。它输出一个概率的连续变量，但可以通过阈值转换为离散值。
- en: Logistic regression is easy to implement and fairly resilient against outliers
    and other data challenges. Many machine learning problems can best be solved with
    logistic regression, offering more practicality and performance than other types
    of supervised machine learning.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归易于实现，并且相对抗干扰和其他数据挑战。许多机器学习问题最好通过逻辑回归来解决，提供比其他类型的监督式机器学习更实用和更高性能的解决方案。
- en: Just like we did in [Chapter 5](ch05.xhtml#ch05) when we covered linear regression,
    we will attempt to walk the line between statistics and machine learning, using
    tools and analysis from both disciplines. Logistic regression will integrate many
    concepts we have learned from this book, from probability to linear regression.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们在[第5章](ch05.xhtml#ch05)中讨论线性回归时所做的那样，我们将尝试在统计学和机器学习之间找到平衡，使用两个学科的工具和分析。逻辑回归将整合我们从本书中学到的许多概念，从概率到线性回归。
- en: Understanding Logistic Regression
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解逻辑回归
- en: Imagine there was a small industrial accident and you are trying to understand
    the impact of chemical exposure. You have 11 patients who were exposed for differing
    numbers of hours to this chemical (please note this is fabricated data). Some
    have shown symptoms (value of 1) and others have not shown symptoms (value of
    0). Let’s plot them in [Figure 6-1](#ipsJABMLqW), where the x-axis is hours of
    exposure and the y-axis is whether or not (1 or 0) they have showed symptoms.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，发生了一起小型工业事故，你正在尝试了解化学物质暴露的影响。有11名患者暴露于不同小时数的化学物质中（请注意这是虚构数据）。一些患者出现了症状（值为1），而另一些没有出现症状（值为0）。让我们在[图 6-1](#ipsJABMLqW)中绘制它们，其中x轴是暴露的小时数，y轴是他们是否出现了症状（1或0）。
- en: '![emds 0601](Images/emds_0601.png)'
  id: totrans-7
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0601](Images/emds_0601.png)'
- en: Figure 6-1\. Plotting whether patients showed symptoms (1) or not (0) over *x*
    hours of exposure
  id: totrans-8
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1。绘制患者在* x *小时暴露后是否出现症状（1）或未出现症状（0）
- en: At what length of time do patients start showing symptoms? Well it is easy to
    see at almost four hours, we immediately transition from patients not showing
    symptoms (0) to showing symptoms (1). In [Figure 6-2](#LTFTchtBPw), we see the
    same data with a predictive curve.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 患者在多长时间后开始出现症状？很容易看到，几乎在四小时后，我们立即从患者不出现症状（0）转变为出现症状（1）。在[图 6-2](#LTFTchtBPw)中，我们看到相同的数据带有一个预测曲线。
- en: '![emds 0602](Images/emds_0602.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0602](Images/emds_0602.png)'
- en: Figure 6-2\. After four hours, we see a clear jump where patients start showing
    symptoms
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-2。四小时后，我们看到患者开始出现症状的明显跳跃
- en: Doing a cursory analysis on this sample, we can say that there is nearly 0%
    probability a patient exposed for fewer than four hours will show symptoms, but
    there is 100% probability for greater than four hours. Between these two groups,
    there is an immediate jump to showing symptoms at approximately four hours.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 对这个样本进行粗略分析，我们可以说，暴露时间少于四小时的患者几乎不可能出现症状，但暴露时间超过四小时的患者出现症状的概率为100%。在这两组之间，大约在四小时左右立即跳跃到出现症状。
- en: Of course, nothing is ever this clear-cut in the real world. Let’s say you gathered
    more data, where the middle of the range has a mix of patients showing symptoms
    versus not showing symptoms as shown in [Figure 6-3](#nlRTQIvpKn).
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在现实世界中，没有什么是如此清晰明了的。假设你收集了更多数据，在范围的中间有一些患者表现出症状与不表现症状的混合，如[图 6-3](#nlRTQIvpKn)所示。
- en: '![emds 0603](Images/emds_0603.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0603](Images/emds_0603.png)'
- en: Figure 6-3\. A mix of patients who show symptoms (1) and do not show symptoms
    (0) exists in the middle
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 中间存在一些表现出症状（1）和不表现症状（0）的患者混合
- en: The way to interpret this is the probability of patients showing symptoms gradually
    increases with each hour of exposure. Let’s visualize this with a *logistic function*,
    or an S-shaped curve where the output variable is squeezed between 0 and 1, as
    shown in [Figure 6-4](#nDHvgvHkOC).
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 解释这一点的方式是，随着每小时的暴露，患者表现出症状的概率逐渐增加。让我们用一个*逻辑函数*或一个S形曲线来可视化这一点，其中输出变量被挤压在0和1之间，如[图 6-4](#nDHvgvHkOC)所示。
- en: '![emds 0604](Images/emds_0604.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0604](Images/emds_0604.png)'
- en: Figure 6-4\. Fitting a logistic function to the data
  id: totrans-18
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-4\. 将逻辑函数拟合到数据上
- en: Because of this overlap of points in the middle, there is no distinct cutoff
    when patients show symptoms but rather a gradual transition from 0% probability
    to 100% probability (0 and 1). This example demonstrates how a *logistic regression*
    results in a curve indicating a probability of belonging to the true category
    (a patient showed symptoms) across an independent variable (hours of exposure).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于中间点的重叠，当患者表现出症状时没有明显的分界线，而是从0%概率逐渐过渡到100%概率（0和1）。这个例子展示了*逻辑回归*如何产生一个曲线，表示属于真实类别（患者表现出症状）的概率在一个独立变量（暴露小时数）上。
- en: We can repurpose a logistic regression to not just predict a probability for
    given input variables but also add a threshold to predict whether it belongs to
    that category. For example, if I get a new patient and find they have been exposed
    for six hours, I predict a 71.1% chance they will show symptoms as traced in [Figure 6-5](#WgtUGPOfFr).
    If my threshold is at least 50% probability to show symptoms, I will simply classify
    that the patient will show symptoms.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以重新利用逻辑回归，不仅预测给定输入变量的概率，还可以添加一个阈值来预测它是否属于该类别。例如，如果我得到一个新患者，并发现他们暴露了六个小时，我预测他们有71.1%的机会表现出症状，如[图 6-5](#WgtUGPOfFr)所示。如果我的阈值至少为50%的概率表现出症状，我将简单地分类为患者将表现出症状。
- en: '![emds 0605](Images/emds_0605.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0605](Images/emds_0605.png)'
- en: Figure 6-5\. We can expect a patient exposed for six hours to be 71.1% likely
    to have symptoms, and because that’s greater than a threshold of 50% we predict
    they will show symptoms
  id: totrans-22
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-5\. 我们可以预期一个暴露了六个小时的患者有71.1%的可能表现出症状，因为这大于50%的阈值，我们预测他们将表现出症状
- en: Performing a Logistic Regression
  id: totrans-23
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进行逻辑回归
- en: So how do we perform a logistic regression? Let’s first take a look at the logistic
    function and explore the math behind it.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么我们如何进行逻辑回归呢？让我们首先看看逻辑函数，并探索其背后的数学。
- en: Logistic Function
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑函数
- en: The *logistic function* is an S-shaped curve (also known as a *sigmoid curve*)
    that, for a given set of input variables, produces an output variable between
    0 and 1\. Because the output variable is between 0 and 1 it can be used to represent
    a probability.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '*逻辑函数*是一个S形曲线（也称为*sigmoid曲线*），对于给定的一组输入变量，产生一个在0和1之间的输出变量。因为输出变量在0和1之间，它可以用来表示概率。'
- en: 'Here is the logistic function that outputs a probability *y* for one input
    variable *x*:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个输出一个输入变量*x*的概率*y*的逻辑函数：
- en: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: Note this formula uses Euler’s number <math alttext="e"><mi>e</mi></math> ,
    which we covered in [Chapter 1](ch01.xhtml#ch01). The *x* variable is the independent/input
    variable. <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and
    <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> are the coefficients
    we need to solve for.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，这个公式使用了欧拉数<math alttext="e"><mi>e</mi></math>，我们在[第1章](ch01.xhtml#ch01)中讨论过。*x*变量是独立/输入变量。<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>是我们需要解决的系数。
- en: <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math> are packaged inside an exponent resembling
    a linear function, which you may recall looks identical to <math alttext="y equals
    m x plus b"><mrow><mi>y</mi> <mo>=</mo> <mi>m</mi> <mi>x</mi> <mo>+</mo> <mi>b</mi></mrow></math>
    or <math alttext="y equals beta 0 plus beta 1 x"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>
    . This is not a coincidence; logistic regression actually has a close relationship
    to linear regression, which we will discuss later in this chapter. <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> indeed is the intercept (which we
    call <math alttext="b"><mi>b</mi></math> in a simple linear regression) and <math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> is the slope for *x*
    (which we call <math alttext="m"><mi>m</mi></math> in a simple linear regression).
    This linear function in the exponent is known as the log-odds function, but for
    now just know this whole logistic function produces this S-shaped curve we need
    to output a shifting probability across an x-value.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> 和 <math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math> 被打包在一个类似于线性函数的指数中，你可能会记得它看起来与 <math
    alttext="y equals m x plus b"><mrow><mi>y</mi> <mo>=</mo> <mi>m</mi> <mi>x</mi>
    <mo>+</mo> <mi>b</mi></mrow></math> 或 <math alttext="y equals beta 0 plus beta
    1 x"><mrow><mi>y</mi> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math> 相同。这并非巧合；逻辑回归实际上与线性回归有着密切的关系，我们将在本章后面讨论。实际上，<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> 是截距（在简单线性回归中我们称之为<math
    alttext="b"><mi>b</mi></math>），<math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    是*x*的斜率（在简单线性回归中我们称之为<math alttext="m"><mi>m</mi></math>）。指数中的这个线性函数被称为对数几率函数，但现在只需知道整个逻辑函数产生了我们需要在x值上输出移动概率的S形曲线。
- en: To declare the logistic function in Python, use the `exp()` function from the
    `math` package to declare the *e* exponent as shown in [Example 6-1](#ikVeOTGjpQ).
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中声明逻辑函数，使用`math`包中的`exp()`函数声明*e*指数，如[示例 6-1](#ikVeOTGjpQ)所示。
- en: Example 6-1\. The logistic function in Python for one independent variable
  id: totrans-32
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. Python中用于一个自变量的逻辑函数
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Let’s plot to see what it looks like, and assume *Β*[0] = –2.823 and *Β*[1]
    = 0.62\. We will use SymPy in [Example 6-2](#WsJKESbiCj) and the output graph
    is shown in [Figure 6-6](#DllsJpEMCJ).
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制一下看看它是什么样子，并假设*Β*[0] = –2.823 和 *Β*[1] = 0.62。我们将在[示例 6-2](#WsJKESbiCj)中使用SymPy，输出图形显示在[图
    6-6](#DllsJpEMCJ)中。
- en: Example 6-2\. Using SymPy to plot a logistic function
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. 使用SymPy绘制逻辑函数
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![emds 0606](Images/emds_0606.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0606](Images/emds_0606.png)'
- en: Figure 6-6\. A logistic function
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 一个逻辑函数
- en: 'In some textbooks, you may alternatively see the logistic function declared
    like this:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在一些教科书中，你可能会看到逻辑函数被这样声明：
- en: <math alttext="p equals StartFraction e Superscript beta 0 plus beta 1 x Baseline
    Over 1 plus e Superscript beta 0 plus beta Baseline 1 x Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></msup>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo><mi>β</mi><mn>1</mn><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p equals StartFraction e Superscript beta 0 plus beta 1 x Baseline
    Over 1 plus e Superscript beta 0 plus beta Baseline 1 x Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><msup><mi>e</mi> <mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></msup>
    <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi> <mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo><mi>β</mi><mn>1</mn><mi>x</mi></mrow></msup></mrow></mfrac></mrow></math>
- en: 'Do not fret about it, because it is the same function, just algebraically expressed
    differently. Note like linear regression we can also extend logistic regression
    to more than one input variable ( <math alttext="x 1 comma x 2 comma period period
    period x Subscript n Baseline"><mrow><msub><mi>x</mi> <mn>1</mn></msub> <mo>,</mo>
    <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo> <mo>.</mo> <mo>.</mo>
    <msub><mi>x</mi> <mi>n</mi></msub></mrow></math> ), as shown in this formula.
    We just add more <math alttext="beta Subscript x"><msub><mi>β</mi> <mi>x</mi></msub></math>
    coefficients:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 不要为此担心，因为这是相同的函数，只是代数上表达不同。注意，像线性回归一样，我们也可以将逻辑回归扩展到多于一个输入变量（<math alttext="x
    1 comma x 2 comma period period period x Subscript n Baseline"><mrow><msub><mi>x</mi>
    <mn>1</mn></msub> <mo>,</mo> <msub><mi>x</mi> <mn>2</mn></msub> <mo>,</mo> <mo>.</mo>
    <mo>.</mo> <mo>.</mo> <msub><mi>x</mi> <mi>n</mi></msub></mrow></math>），如此公式所示。我们只需添加更多的<math
    alttext="beta Subscript x"><msub><mi>β</mi> <mi>x</mi></msub></math> 系数：
- en: <math alttext="p equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: Fitting the Logistic Curve
  id: totrans-43
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 拟合逻辑曲线
- en: How do you fit the logistic curve to a given training dataset? First, the data
    can have any mix of decimal, integer, and binary variables, but the output variable
    must be binary (0 or 1). When we actually do prediction, the output variable will
    be between 0 and 1, resembling a probability.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如何将逻辑曲线拟合到给定的训练数据集？首先，数据可以包含任意混合的十进制、整数和二进制变量，但输出变量必须是二进制（0或1）。当我们实际进行预测时，输出变量将在0和1之间，类似于概率。
- en: The data provides our input and output variable values, but we need to solve
    for the <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> coefficients to fit
    our logistic function. Recall how we used least squares in [Chapter 5](ch05.xhtml#ch05).
    However, this does not apply here. Instead we use *maximum likelihood estimation*,
    which, as the name suggests, maximizes the likelihood a given logistic curve would
    output the observed data.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 数据提供了我们的输入和输出变量值，但我们需要解出<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>系数以拟合我们的逻辑函数。回想一下我们在[Chapter 5](ch05.xhtml#ch05)中如何使用最小二乘法。然而，在这里不适用这种方法。相反，我们使用*最大似然估计*，顾名思义，最大化给定逻辑曲线输出观测数据的可能性。
- en: To calculate the maximum likelihood estimation, there really is no closed form
    equation like in linear regression. We can still use gradient descent, or have
    a library do it for us. Let’s cover both of these approaches starting with the
    library SciPy.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算最大似然估计，实际上没有像线性回归那样的封闭形式方程。我们仍然可以使用梯度下降，或者让一个库来为我们做这件事。让我们从库SciPy开始涵盖这两种方法。
- en: Using SciPy
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用SciPy
- en: The nice thing about SciPy is the models often have a standardized set of functions
    and APIs, meaning in many cases you can copy/paste your code and can then reuse
    it between models. In [Example 6-3](#rOMnaFDjgv) you will see a logistic regression
    performed on our patient data. If you compare it to our linear regression code
    in [Chapter 5](ch05.xhtml#ch05), you will see it has nearly identical code in
    importing, separating, and fitting our data. The main difference is I use a `LogisticRegression()`
    for my model instead of a `LinearRegression()`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy的好处在于，模型通常具有一套标准化的函数和API，这意味着在许多情况下，您可以复制/粘贴您的代码，然后在模型之间重复使用它。在[Example 6-3](#rOMnaFDjgv)中，您将看到我们的患者数据上执行的逻辑回归。如果您将其与我们在[Chapter 5](ch05.xhtml#ch05)中的线性回归代码进行比较，您将看到在导入、分离和拟合数据方面几乎完全相同的代码。主要区别在于我使用`LogisticRegression()`作为我的模型，而不是`LinearRegression()`。
- en: Example 6-3\. Using a plain logistic regression in SciPy
  id: totrans-49
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-3。在SciPy中使用普通逻辑回归
- en: '[PRE2]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After running the model in SciPy, I get a logistic regression where *β*[0] =
    –3.17576395 and *β*[1] = 0.69267212\. When I plot this, it should look pretty
    good as shown in [Figure 6-7](#VEMwqkqrSk).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在SciPy中运行模型后，我得到一个逻辑回归，其中*β*[0] = –3.17576395，*β*[1] = 0.69267212。当我绘制这个图时，应该看起来很好，就像在[Figure 6-7](#VEMwqkqrSk)中显示的那样。
- en: '![emds 0607](Images/emds_0607.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0607](Images/emds_0607.png)'
- en: Figure 6-7\. Plotting the logistic regression
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7。绘制逻辑回归
- en: There are a couple of things to note here. When I created the `LogisticRegression()`
    model, I specified no `penalty` argument, which chooses a regularization technique
    like `l1` or `l2`. While this is beyond the scope of this book, I have included
    brief insights in the following note “Learning About SciPy Parameters” so that
    you have helpful references on hand.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有几件事情需要注意。当我创建`LogisticRegression()`模型时，我没有指定`penalty`参数，这会选择像`l1`或`l2`这样的正则化技术。虽然这超出了本书的范围，但我在以下注释“学习关于SciPy参数”中包含了简要见解，以便您手边有有用的参考资料。
- en: Finally, I am going to `flatten()` the coefficient and intercept, which come
    out as multidimensional matrices but with one element. *Flattening* means collapsing
    a matrix of numbers into lesser dimensions, particularly when there are fewer
    elements than there are dimensions. For example, I use `flatten()` here to take
    a single number nested into a two-dimensional matrix and pull it out as a single
    value. I then have my <math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>
    and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math> coefficients.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我将`flatten()`系数和截距，这些系数和截距出来时是多维矩阵，但只有一个元素。*Flattening*意味着将一组数字的矩阵折叠成较小的维度，特别是当元素少于维度时。例如，我在这里使用`flatten()`来将嵌套在二维矩阵中的单个数字提取出来作为单个值。然后我有了我的<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>系数。
- en: Learning About SciPy Parameters
  id: totrans-56
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 学习关于SciPy参数
- en: SciPy offers a lot of options in its regression and classification models. Unfortunately,
    there is not enough bandwidth or pages to cover them as this is not a book focusing
    exclusively on machine learning.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: SciPy在其回归和分类模型中提供了许多选项。不幸的是，由于这不是一本专门关注机器学习的书籍，没有足够的带宽或页面来覆盖它们。
- en: However, the SciPy docs are well-written and the page on logistic regression
    is found [here](https://oreil.ly/eL8hZ).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，SciPy文档写得很好，逻辑回归页面在[这里](https://oreil.ly/eL8hZ)找到。
- en: If a lot of terms are unfamiliar, such as regularization and `l1` and `l2` penalties,
    there are other great O’Reilly books exploring these topics. One of the more helpful
    texts I have found is *Hands-On Machine Learning with Scikit-Learn, Keras, and
    TensorFlow* by Aurélien Géron.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 如果很多术语都很陌生，比如正则化和`l1`和`l2`惩罚，还有其他很棒的 O’Reilly 书籍探讨这些主题。我发现其中一本更有帮助的书是由Aurélien
    Géron撰写的*Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow*。
- en: Using Maximum Likelihood and Gradient Descent
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用最大似然和梯度下降
- en: As I have done throughout this book, I aim to provide insights on building techniques
    from scratch even if libraries can do it for us. There are several ways to fit
    a logistic regression ourselves, but all methods typically turn to maximum likelihood
    estimation (MLE). MLE maximizes the likelihood a given logistic curve would output
    the observed data. It is different than sum of squares, but we can still apply
    gradient descent or stochastic gradient descent to solve it.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在整本书中所做的，我旨在提供关于从头开始构建技术的见解，即使库可以为我们完成。有几种方法可以自己拟合逻辑回归，但所有方法通常都转向最大似然估计（MLE）。MLE
    最大化了给定逻辑曲线输出观测数据的可能性。这与平方和不同，但我们仍然可以应用梯度下降或随机梯度下降来解决它。
- en: I’ll try to streamline the mathematical jargon and minimize the linear algebra
    here. Essentially, the idea is to find the <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    coefficients that bring our logistic curve to those points as closely as possible,
    indicating it is most likely to have produced those points. If you recall from
    [Chapter 2](ch02.xhtml#ch02) when we studied probability, we combine probabilities
    (or likelihoods) of multiple events by multiplying them together. In this application,
    we are calculating the likelihood we would see all these points for a given logistic
    regression curve.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我会尽量简化数学术语，并尽量减少线性代数的内容。基本上，这个想法是找到使我们的逻辑曲线尽可能接近这些点的<math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math>和<math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>系数，表明它最有可能产生这些点。如果你还记得[第2章](ch02.xhtml#ch02)中我们学习概率时，我们通过将多个事件的概率（或可能性）相乘来组合它们。在这个应用中，我们正在计算我们会看到所有这些点的可能性，对于给定的逻辑回归曲线。
- en: Applying the idea of joint probabilities, each patient has a likelihood they
    would show symptoms *based on the fitted logistic function* as shown in [Figure 6-8](#qVWERlmlnt).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 应用联合概率的概念，每个患者都有一个基于拟合的逻辑函数的可能性，如[图6-8](#qVWERlmlnt)所示。
- en: '![emds 0608](Images/emds_0608.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0608](Images/emds_0608.png)'
- en: Figure 6-8\. Every input value has a corresponding likelihood on the logistic
    curve
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-8\. 每个输入值在逻辑曲线上都有相应的可能性
- en: We fetch each likelihood off the logistic regression curve above or below each
    point. If the point is below the logistic regression curve, we need to subtract
    the resulting probability from 1.0 because we want to maximize the false cases
    too.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从逻辑回归曲线上方或下方获取每个点的可能性。如果点在逻辑回归曲线下方，我们需要从1.0中减去结果概率，因为我们也想最大化假阳性。
- en: Given coefficients *β*[0] = –3.17576395 and *β*[1] = 0.69267212, [Example 6-4](#aScjqhrufa)
    shows how we calculate the joint likelihood for this data in Python.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 给定系数*β*[0] = –3.17576395和*β*[1] = 0.69267212，[示例6-4](#aScjqhrufa)展示了我们如何在 Python
    中计算这些数据的联合概率。
- en: Example 6-4\. Calculating the joint likelihood of observing all the points for
    a given logistic regression
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. 计算给定逻辑回归观察到所有点的联合概率
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Here’s a mathematical trick we can do to compress that `if` expression. As
    we covered in [Chapter 1](ch01.xhtml#ch01), when you set any number to the power
    of 0 it will always be 1\. Take a look at this formula and note the handling of
    true (1) and false (0) cases in the exponents:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个数学技巧，我们可以用来压缩那个`if`表达式。正如我们在[第1章](ch01.xhtml#ch01)中讨论的，当你将任何数的幂设为0时，结果总是1。看看这个公式，并注意指数中对真（1）和假（0）情况的处理：
- en: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <msub><mi>y</mi>
    <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∏</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <msub><mi>y</mi>
    <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup></mrow></math>
- en: To do this in Python, compress everything inside that `for` loop into [Example 6-5](#hwJLCkrAPp).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Python 中实现这一点，将`for`循环中的所有内容压缩到[示例6-5](#hwJLCkrAPp)中。
- en: Example 6-5\. Compressing the joint likelihood calculation without an `if` expression
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-5\. 在不使用`if`表达式的情况下压缩联合概率计算
- en: '[PRE4]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: What exactly did I do? Notice that there are two halves to this expression,
    one for when <math alttext="y equals 1"><mrow><mi>y</mi> <mo>=</mo> <mn>1</mn></mrow></math>
    and the other where <math alttext="y equals 0"><mrow><mi>y</mi> <mo>=</mo> <mn>0</mn></mrow></math>
    . When any number is raised to exponent 0, it will result in 1\. Therefore, whether
    *y* is 1 or 0, it will cause the opposite condition on the other side to evaluate
    to 1 and have no effect in multiplication. We get to express our `if` expression
    but do it completely in a mathematical expression. We cannot do derivatives on
    expressions that use `if`, so this will be helpful.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 我到底做了什么？注意这个表达式有两个部分，一个是当<math alttext="y 等于 1"><mrow><mi>y</mi> <mo>=</mo>
    <mn>1</mn></mrow></math>时，另一个是当<math alttext="y 等于 0"><mrow><mi>y</mi> <mo>=</mo>
    <mn>0</mn></mrow></math>时。当任何数被提升到指数 0 时，结果将为 1。因此，无论*y*是 1 还是 0，它都会导致另一侧的条件评估为
    1 并且在乘法中没有影响。我们可以用数学表达式完全表达我们的`if`表达式。我们无法对使用`if`的表达式进行导数，所以这将很有帮助。
- en: Note that computers can get overwhelmed multiplying several small decimals together,
    known as *floating point underflow*. This means that as decimals get smaller and
    smaller, which can happen in multiplication, the computer runs into limitations
    keeping track of that many decimal places. There is a clever mathematical hack
    to get around this. You can take the `log()` of each decimal you are multiplying
    and instead add them together. This is thanks to the additive properties of logarithms
    we covered in [Chapter 1](ch01.xhtml#ch01). This is more numerically stable, and
    you can then call the `exp()` function to convert the total sum back to get the
    product.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，计算机可能会因为乘以多个小小数而不堪重负，这被称为*浮点下溢*。这意味着随着小数变得越来越小，可能会在乘法中发生，计算机在跟踪那么多小数位数时会遇到限制。有一个巧妙的数学技巧可以解决这个问题。你可以对要相乘的每个小数取`log()`，然后将它们相加。这要归功于我们在[第
    1 章](ch01.xhtml#ch01)中介绍的对数的加法性质。这样更加数值稳定，然后你可以调用`exp()`函数将总和转换回来得到乘积。
- en: Let’s revise our code to use logarithmic addition instead of multiplication
    (see [Example 6-6](#dNMlqaFFPW)). Note that the `log()` function will default
    to base *e* and while any base technically works, this is preferable because <math
    alttext="e Superscript x"><msup><mi>e</mi> <mi>x</mi></msup></math> is the derivative
    of itself and will computationally be more efficient.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改我们的代码，使用对数加法而不是乘法（参见[示例 6-6](#dNMlqaFFPW)）。请注意，`log()`函数默认为基数*e*，虽然任何基数在技术上都可以工作，但这是首选，因为<math
    alttext="e 上标 x"><msup><mi>e</mi> <mi>x</mi></msup></math>是其自身的导数，计算上更有效率。
- en: Example 6-6\. Using logarithmic addition
  id: totrans-78
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-6\. 使用对数加法
- en: '[PRE5]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'To express the preceding Python code in mathematical notation:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 要用数学符号表示前面的 Python 代码：
- en: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mi mathvariant="italic">log</mi>
    <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mn>1.0</mn><mo>-</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup> <mo>)</mo></mrow></mrow></math>
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>joint likelihood</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mi mathvariant="italic">log</mi>
    <mrow><mo>(</mo> <msup><mrow><mo>(</mo><mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>)</mo></mrow> <msub><mi>y</mi> <mi>i</mi></msub></msup> <mo>×</mo> <msup><mrow><mo>(</mo><mn>1.0</mn><mo>-</mo><mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <msub><mi>x</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac> <mo>)</mo></mrow> <mrow><mn>1.0</mn><mo>-</mo><msub><mi>y</mi>
    <mi>i</mi></msub></mrow></msup> <mo>)</mo></mrow></mrow></math>
- en: Would you like to calculate the partial derivatives for <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi>
    <mn>1</mn></msub></math> in the preceding expression? I didn’t think so. It’s
    a beast. Goodness, expressing that function in SymPy alone is a mouthful! Look
    at this in [Example 6-7](#QsBpdpBWda).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要计算前述表达式中的<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>的偏导数吗？我觉得不会。这太复杂了。天哪，在
    SymPy 中表达那个函数本身就是一大口水！看看[示例 6-7](#QsBpdpBWda)中的内容。
- en: Example 6-7\. Expressing a joint likelihood for logistic regression in SymPy
  id: totrans-83
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-7\. 在 SymPy 中表达逻辑回归的联合似然
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: So let’s just allow SymPy to do the partial derivatives for us, for <math alttext="beta
    0"><msub><mi>β</mi> <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi>
    <mn>1</mn></msub></math> respectively. We will then immediately compile and use
    them for gradient descent, as shown in [Example 6-8](#aVIANjbpTT).
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 所以让我们让 SymPy 为我们做<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math
    alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>的偏导数。然后我们将立即编译并使用它们进行梯度下降，如[示例
    6-8](#aVIANjbpTT)所示。
- en: Example 6-8\. Using gradient descent on logistic regression
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-8\. 在逻辑回归上使用梯度下降
- en: '[PRE7]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: After calculating the partial derivatives for <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    , we substitute the x- and y-values as well as the number of data points *n*.
    Then we use `lambdify()` to compile the derivative function for efficiency (it
    uses NumPy behind the scenes). After that, we perform gradient descent like we
    did in [Chapter 5](ch05.xhtml#ch05), but since we are trying to maximize rather
    than minimize, we add each adjustment to <math alttext="beta 0"><msub><mi>β</mi>
    <mn>0</mn></msub></math> and <math alttext="beta 1"><msub><mi>β</mi> <mn>1</mn></msub></math>
    rather than subtract like in least squares.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在计算<math alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>的偏导数后，我们将x值和y值以及数据点数*n*代入。然后我们使用`lambdify()`来编译导数函数以提高效率（它在幕后使用NumPy）。之后，我们执行梯度下降，就像我们在[第5章](ch05.xhtml#ch05)中所做的那样，但由于我们试图最大化而不是最小化，我们将每次调整添加到<math
    alttext="beta 0"><msub><mi>β</mi> <mn>0</mn></msub></math>和<math alttext="beta
    1"><msub><mi>β</mi> <mn>1</mn></msub></math>中，而不是像最小二乘法中那样减去。
- en: As you can see in [Example 6-8](#aVIANjbpTT), we got *β*[0] = –3.17575 and *β*[1]
    = 0.692667\. This is highly comparable to the coefficient values we got in SciPy
    earlier.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[示例6-8](#aVIANjbpTT)中所看到的，我们得到了*β*[0] = –3.17575和*β*[1] = 0.692667。这与我们之前在SciPy中得到的系数值非常相似。
- en: As we learned to do in [Chapter 5](ch05.xhtml#ch05), we can also use stochastic
    gradient descent and only sample one or a handful of records on each iteration.
    This would extend the benefits of increasing computational speed and performance
    as well as prevent overfitting. It would be redundant to cover it again here,
    so we will keep moving on.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第5章](ch05.xhtml#ch05)中学到的那样，我们也可以使用随机梯度下降，每次迭代只对一个或少数几个记录进行采样。这将延伸增加计算速度和性能的好处，同时防止过拟合。在这里再次覆盖将是多余的，所以我们将继续前进。
- en: Multivariable Logistic Regression
  id: totrans-91
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 多变量逻辑回归
- en: Let’s try an example that uses logistic regression on multiple input variables.
    [Table 6-1](#ijGQdDBEmd) shows a sample of a few records from a fictitious dataset
    containing some employment-retention data (full dataset is [here](https://bit.ly/3aqsOMO)).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一个使用多个输入变量进行逻辑回归的示例。[表6-1](#ijGQdDBEmd)展示了一个虚构数据集中一些就业保留数据的样本（完整数据集在[这里](https://bit.ly/3aqsOMO)）。
- en: Table 6-1\. Sample of employment-retention data
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-1。就业保留数据样本
- en: '| SEX | AGE | PROMOTIONS | YEARS_EMPLOYED | DID_QUIT |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 性别 | 年龄 | 晋升次数 | 工龄 | 是否离职 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | 32 | 3 | 7 | 0 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 32 | 3 | 7 | 0 |'
- en: '| 1 | 34 | 2 | 5 | 0 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 34 | 2 | 5 | 0 |'
- en: '| 1 | 29 | 2 | 5 | 1 |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 29 | 2 | 5 | 1 |'
- en: '| 0 | 42 | 4 | 10 | 0 |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 42 | 4 | 10 | 0 |'
- en: '| 1 | 43 | 4 | 10 | 0 |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 43 | 4 | 10 | 0 |'
- en: 'There are 54 records in this dataset. Let’s say we want to use it to predict
    whether other employees are going to quit and logistic regression can be utilized
    here (although none of this is a good idea, and I will elaborate why later). Recall
    we can support more than one input variable as shown in this formula:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集中有54条记录。假设我们想要用它来预测其他员工是否会离职，这里可以使用逻辑回归（尽管这不是一个好主意，稍后我会详细说明原因）。回想一下，我们可以支持多个输入变量，如下公式所示：
- en: <math alttext="y equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="y equals StartFraction 1 Over 1 plus e Superscript minus left-parenthesis
    beta 0 plus beta 1 x 1 plus beta 2 x 2 plus period period period beta Super Subscript
    n Superscript x Super Subscript n Superscript right-parenthesis Baseline EndFraction"
    display="block"><mrow><mi>y</mi> <mo>=</mo> <mfrac><mn>1</mn> <mrow><mn>1</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>1</mn></msub> <msub><mi>x</mi> <mn>1</mn></msub> <mo>+</mo><msub><mi>β</mi>
    <mn>2</mn></msub> <msub><mi>x</mi> <mn>2</mn></msub> <mo>+</mo><mo>.</mo><mo>.</mo><mo>.</mo><msub><mi>β</mi>
    <mi>n</mi></msub> <msub><mi>x</mi> <mi>n</mi></msub> <mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: I will create <math alttext="beta"><mi>β</mi></math> coefficients for each of
    the variables `sex`, `age`, `promotions`, and `years_employed`. The output variable
    `did_quit` is binary, and that is going to drive the logistic regression outcome
    we are predicting. Because we are dealing with multiple dimensions, it is going
    to be hard to visualize the curvy hyperplane that is our logistic curve. So we
    will steer clear from visualization.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 我将为每个变量`sex`、`age`、`promotions`和`years_employed`创建<math alttext="beta"><mi>β</mi></math>系数。输出变量`did_quit`是二进制的，这将驱动我们正在预测的逻辑回归结果。因为我们处理多个维度，所以很难可视化我们的逻辑曲线所代表的曲线超平面。因此，我们将避免可视化。
- en: Let’s make it interesting. We will use scikit-learn but make an interactive
    shell we can test employees with. [Example 6-9](#GOoIdgKATe) shows the code, and
    when we run it, a logistic regression will be performed, and then we can type
    in new employees to predict whether they quit or not. What can go wrong? Nothing,
    I’m sure. We are only making predictions on people’s personal attributes and making
    decisions accordingly. I’m sure it will be fine.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来点有趣的。我们将使用scikit-learn，但创建一个交互式shell，我们可以用来测试员工。[示例6-9](#GOoIdgKATe)展示了代码，当我们运行它时，将执行逻辑回归，然后我们可以输入新员工以预测他们是否会离职。会出什么问题呢？我相信没有。我们只是根据人们的个人属性进行预测并做出相应决策。我相信一切都会好的。
- en: (If it was not clear, I’m being very tongue in cheek).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: （如果不清楚，我是在开玩笑）。
- en: Example 6-9\. Doing a multivariable logistic regression on employee data
  id: totrans-106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-9。对员工数据进行多变量逻辑回归
- en: '[PRE8]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '[Figure 6-9](#PtmqbJijiV) shows the result whether an employee is predicted
    to quit. The employee is a sex “1,” age is 34, had 1 promotion, and has been at
    the company for 5 years. Sure enough, the prediction is “WILL LEAVE.”'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-9](#PtmqbJijiV)显示了员工是否被预测会离职的结果。员工的性别为“1”，年龄为34岁，晋升1次，公司工作了5年。果然，预测是“将离开”。'
- en: '![emds 0609](Images/emds_0609.png)'
  id: totrans-109
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0609](Images/emds_0609.png)'
- en: Figure 6-9\. Making a prediction whether a 34-year-old employee with 1 promotion
    and 5 years, employment will quit
  id: totrans-110
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-9。预测34岁员工，1次晋升和5年工作经验是否会离职
- en: Note that the `predict_proba()` function will output two values, the first being
    the probability of 0 (false) and the second being 1 (true).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，`predict_proba()`函数将输出两个值，第一个是0（假）的概率，第二个是1（真）的概率。
- en: 'You will notice that the coefficients for `sex`, `age`, `promotions`, and `years_employed`
    are displayed in that order. By the weight of the coefficients, you can see that
    `sex` and `age` play very little role in the prediction (they both have a weight
    near 0). However, `promotions` and `years_employed` have significant weights of
    `–2.504` and `0.97`. Here’s a secret with this toy dataset: I fabricated it so
    that an employee quits if they do not get a promotion roughly every two years.
    Sure enough, my logistic regression picked up this pattern and you can try it
    out with other employees as well. However, if you venture outside the ranges of
    data it was trained on, the predictions will likely start falling apart (e.g.,
    if put in a 70-year-old employee who hasn’t been promoted in three years, it’s
    hard to say what this model will do since it has no data around that age).'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到`sex`、`age`、`promotions`和`years_employed`的系数按照这个顺序显示。通过系数的权重，您可以看到`sex`和`age`在预测中起到很小的作用（它们的权重接近0）。然而，`promotions`和`years_employed`具有显著的权重分别为`-2.504`和`0.97`。这个玩具数据集的一个秘密是，如果员工每两年没有晋升就会离职，我制造了这个模式，我的逻辑回归确实捕捉到了这个模式，您也可以尝试对其他员工进行测试。然而，如果您超出了它训练的数据范围，预测可能会开始失效（例如，如果放入一个70岁的员工，三年没有晋升，很难说这个模型会做出什么，因为它没有关于那个年龄的数据）。
- en: Of course, real life is not always this clean. An employee who has been at a
    company for eight years and has never gotten a promotion is likely comfortable
    with their role and not leaving anytime soon. If that is the case, variables like
    age then might play a role and get weighted. Then of course we can get concerned
    about other relevant variables that are not being captured. See the following
    warning to learn more.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，现实生活并不总是如此干净。一个在公司工作了八年，从未晋升过的员工很可能对自己的角色感到满意，不会很快离开。如果是这种情况，年龄等变量可能会发挥作用并被赋予权重。然后当然我们可能会担心其他未被捕捉到的相关变量。查看以下警告以了解更多信息。
- en: Be Careful Making Classifications on People!
  id: totrans-114
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 谨慎对人进行分类！
- en: A quick and surefire way to shoot yourself in the foot is to collect data on
    people and use it to make predictions haphazardly. Not only can data privacy concerns
    come about, but legal and PR issues can emerge if the model is found to be discriminatory.
    Input variables like race and gender can become weighted from machine learning
    training. After that, undesirable outcomes are inflicted on those demographics
    like not being hired or being denied loans. More extreme applications include
    being falsely flagged by surveillance systems or being denied criminal parole.
    Note too that seemingly benign variables like commute time have been found to
    correlate with discriminatory variables.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 一个快速而肯定地让自己陷入困境的方法是收集关于人们的数据，并随意地用它来做预测。不仅可能引发数据隐私问题，还可能出现法律和公关问题，如果模型被发现具有歧视性。像种族和性别这样的输入变量可能在机器学习训练中被赋予权重。之后，这些人口统计学数据可能会导致不良结果，比如不被录用或被拒绝贷款。更极端的应用包括被监视系统错误标记或被拒绝刑事假释。还要注意，看似无害的变量，比如通勤时间，已被发现与歧视性变量相关联。
- en: 'At the time of writing, a number of articles have been citing machine learning
    discrimination as an issue:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，已有多篇文章引用机器学习歧视作为一个问题：
- en: Katyanna Quach, [“Teen turned away from roller rink after AI wrongly identifies
    her as banned troublemaker”](https://oreil.ly/boUcW), *The Register*, July 16,
    2021.
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katyanna Quach，[“AI错误识别为被禁止捣乱者而拒绝青少年进入滑冰场”](https://oreil.ly/boUcW)，*The Register*，2021年7月16日。
- en: Kashmir Hill, [“Wrongfully Accused by an Algorithm”](https://oreil.ly/dOJyI),
    *New York Times*, June 24, 2020.
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kashmir Hill, [“算法错误地指控”](https://oreil.ly/dOJyI), *纽约时报*, 2020年6月24日。
- en: As data privacy laws continue to evolve, it is advisable to err on the side
    of caution and engineer personal data carefully. Think about what automated decisions
    will be propagated and how that can cause harm. Sometimes it is better to just
    leave a “problem” alone and keep doing it manually.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据隐私法律的不断发展，谨慎处理个人数据是明智的。考虑自动决策将如何传播以及如何造成伤害。有时最好的做法是让“问题”保持原样，继续手动处理。
- en: Finally, on this employee-retention example, think about where this data came
    from. Yes, I made up this dataset but in the real world you always want to question
    what process created the data. Over what period of time did this sample come from?
    How far back do we go looking for employees who quit? What constitutes an employee
    who stayed? Are they current employees at this point in time? How do we know they
    are not about to quit, making them a false negative? Data scientists easily fall
    into traps analyzing only what data says, but not questioning where it came from
    and what assumptions are built into it.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在这个员工留存的例子中，想想这些数据是从哪里来的。是的，我虚构了这个数据集，但在现实世界中，你总是要质疑数据是如何生成的。这个样本是从多长时间内得出的？我们要回溯多久来寻找已经离职的员工？什么构成了留下的员工？他们现在是当前员工吗？我们怎么知道他们不会马上离职，从而成为一个假阴性？数据科学家很容易陷入分析数据所说的内容，但不质疑数据来源和内置的假设。
- en: The best way to get answers to these questions is to understand what the predictions
    are being used for. Is it to decide when to give people promotions to retain them?
    Can this create a circular bias promoting people with a set of attributes? Will
    that bias be reaffirmed when those promotions start becoming the new training
    data?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 获取这些问题的答案的最佳方法是了解预测用途。是用来决定何时提升人员以留住他们吗？这会产生一种循环偏见，促使具有一组属性的人员晋升吗？当这些晋升开始成为新的训练数据时，这种偏见会得到确认吗？
- en: These are all important questions, and perhaps even inconvenient ones that cause
    unwanted scope to creep into the project. If this scrutiny is not welcomed by
    your team or leadership on a project, consider empowering yourself with a different
    role where curiosity becomes a strength.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是重要的问题，甚至可能是令人不快的问题，会导致不必要的范围渗入项目中。如果你的团队或领导不欢迎对项目进行这种审查，考虑让自己担任一个不同的角色，让好奇心成为一种优势。
- en: Understanding the Log-Odds
  id: totrans-123
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 理解对数几率
- en: At this point, it is time to discuss the logistic regression and what it is
    mathematically made of. This can be a bit dizzying so take your time here. If
    you get overwhelmed, you can always revisit this section later.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 此时，是时候讨论逻辑回归及其数学构成了。这可能有点令人眩晕，所以在这里要花点时间。如果你感到不知所措，随时可以稍后回顾这一部分。
- en: Starting in the 1900s, it has always been of interest to mathematicians to take
    a linear function and scale its output to fall between 0 and 1, and therefore
    be useful for predicting probability. The log-odds, also called the logit function,
    lends itself to logistic regression for this purpose.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 从20世纪开始，数学家一直对将线性函数的输出缩放到0和1之间感兴趣，因此对于预测概率是有用的。对数几率，也称为对数函数，适用于逻辑回归的这一目的。
- en: 'Remember earlier I pointed out the exponent value <math alttext="beta 0 plus
    beta 1 x"><mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math> is a linear function? Look at our logistic
    function again:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 记得之前我指出指数值 <math alttext="beta 0 plus beta 1 x"><mrow><msub><mi>β</mi> <mn>0</mn></msub>
    <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math> 是一个线性函数吗？再看看我们的逻辑函数：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math>
- en: This linear function being raised to *e* is known as the *log-odds* function,
    which takes the logarithm of the odds for the event of interest. Your response
    might be, “Wait, I don’t see any `log()` or odds. I just see a linear function!”
    Bear with me, I will show the hidden math.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 这个被提升到 *e* 的线性函数被称为 *对数几率* 函数，它取得了感兴趣事件的对数几率。你可能会说，“等等，我看不到 `log()` 或几率。我只看到一个线性函数！”
    请耐心等待，我会展示隐藏的数学。
- en: 'As an example, let’s use our logistic regression from earlier where *Β*[0]
    = -3.17576395 and *Β*[1] = 0.69267212\. What is the probability of showing symptoms
    after six hours, where <math alttext="x equals 6"><mrow><mi>x</mi> <mo>=</mo>
    <mn>6</mn></mrow></math> ? We already know how to do this: plug these values into
    our logistic function:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 举个例子，让我们使用之前的逻辑回归，其中*Β*[0] = -3.17576395，*Β*[1] = 0.69267212。在六小时后出现症状的概率是多少，其中<math
    alttext="x equals 6"><mrow><mi>x</mi> <mo>=</mo> <mn>6</mn></mrow></math>？我们已经知道如何做到这一点：将这些值代入我们的逻辑函数：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math>
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math>
- en: 'We plug in these values and output a probability of 0.72716\. But let’s look
    at this from an odds perspective. Recall in [Chapter 2](ch02.xhtml#ch02) we learned
    how to calculate odds from a probability:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些值代入并输出概率为0.72716。但让我们从赔率的角度来看这个问题。回想一下，在[第2章](ch02.xhtml#ch02)中我们学习了如何从概率计算赔率：
- en: <math display="block"><mrow><mtext>odds</mtext> <mo>=</mo> <mfrac><mi>p</mi>
    <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math display="block"><mrow><mtext>odds</mtext>
    <mo>=</mo> <mfrac><mrow><mn>.72716</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac>
    <mo>=</mo> <mn>2.66517246407876</mn></mrow></math>
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>赔率</mtext> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math
    display="block"><mrow><mtext>赔率</mtext> <mo>=</mo> <mfrac><mrow><mn>.72716</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac> <mo>=</mo> <mn>2.66517246407876</mn></mrow></math>
- en: So at six hours, a patient is 2.66517 times more likely to show symptoms than
    not show symptoms.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在六小时时，患者出现症状的可能性是不出现症状的2.66517倍。
- en: 'When we wrap the odds function in a natural logarithm (a logarithm with base
    *e*), we call this the *logit function*. The output of this formula is what we
    call the *log-odds*, named…shockingly…because we take the logarithm of the odds:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将赔率函数包装在一个自然对数（以*e*为底的对数）中时，我们称之为*对数几率函数*。这个公式的输出就是我们所说的*对数几率*，之所以这样命名...令人震惊...是因为我们取了赔率的对数：
- en: <math display="block"><mrow><mtext>logit</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac> <mo>)</mo></mrow></math><math
    display="block"><mrow><mtext>logit</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo>
    <mfrac><mrow><mn>.72716</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac>
    <mo>)</mo> <mo>=</mo> <mn>0.98026877</mn></mrow></math>
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>对数几率</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo>
    <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac> <mo>)</mo></mrow></math><math
    display="block"><mrow><mtext>对数几率</mtext> <mo>=</mo> <mi>log</mi> <mo>(</mo> <mfrac><mrow><mn>.72716</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>.72716</mn></mrow></mfrac> <mo>)</mo> <mo>=</mo>
    <mn>0.98026877</mn></mrow></math>
- en: Our log-odds at six hours is 0.9802687\. What does this mean and why do we care?
    When we are in “log-odds land” it is easier to compare one set of odds against
    another. We treat anything greater than 0 as favoring odds an event will happen,
    whereas anything less than 0 is against an event. A log-odds of –1.05 is linearly
    the same distance from 0 as 1.05\. In plain odds, though, the equivalents are
    0.3499 and 2.857, respectively, which is not as interpretable. That is the convenience
    of log-odds.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在六小时时的对数几率为0.9802687。这意味着什么，为什么我们要关心呢？当我们处于“对数几率领域”时，比较一组赔率相对容易。我们将大于0的任何值视为支持事件发生的赔率，而小于0的任何值则反对事件发生。对数几率为-1.05与0的距离与1.05相同。然而，在普通赔率中，这些等价值分别为0.3499和2.857，这并不容易解释。这就是对数几率的便利之处。
- en: Odds and Logs
  id: totrans-137
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 赔率和对数
- en: Logarithms and odds have an interesting relationship. Odds are against an event
    when it is between 0.0 and 1.0, but anything greater than 1.0 favors the event
    and extends into positive infinity. This lack of symmetry is awkward. However,
    logarithms rescale an odds so that it is completely linear, where a log-odds of
    0.0 means fair odds. A log-odds of –1.05 is linearly the same distance from 0
    as 1.05, thus make comparing odds much easier.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对数和赔率有着有趣的关系。当赔率在0.0和1.0之间时，事件是不利的，但大于1.0的任何值都支持事件，并延伸到正无穷。这种缺乏对称性很尴尬。然而，对数重新调整了赔率，使其完全线性，其中对数几率为0.0表示公平的赔率。对数几率为-1.05与0的距离与1.05相同，因此比较赔率更容易。
- en: Josh Starmer has a [great video](https://oreil.ly/V0H8w) talking about this
    relationship between odds and logs.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: Josh Starmer有一个[很棒的视频](https://oreil.ly/V0H8w)讲述了赔率和对数之间的关系。
- en: 'Recall I said the linear function in our logistic regression formula <math
    alttext="beta 0 plus beta 1 x"><mrow><msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo>
    <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math> is our log-odds function.
    Check this out:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我说过我们逻辑回归公式中的线性函数<math alttext="beta 0 plus beta 1 x"><mrow><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>是我们的对数几率函数。看看这个：
- en: <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mo>-</mo> <mn>3.17576395</mn>
    <mo>+</mo> <mn>0.69267212</mn> <mo>(</mo> <mn>6</mn> <mo>)</mo></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mn>0.98026877</mn></mrow></math>
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mo>-</mo> <mn>3.17576395</mn>
    <mo>+</mo> <mn>0.69267212</mn> <mo>(</mo> <mn>6</mn> <mo>)</mo></mrow></math>
    <math display="block"><mrow><mtext>log-odds</mtext> <mo>=</mo> <mn>0.98026877</mn></mrow></math>
- en: 'It’s the same value 0.98026877 as our previous calculation, the odds of our
    logistic regression at *x* = 6 and then taking the `log()` of it! So what is the
    link? What ties all this together? Given a probability from a logistic regression
    *p* and input variable *x*, it is this:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们先前计算的值0.98026877相同，即在*x* = 6时逻辑回归的赔率，然后取其`log()`！那么这之间有什么联系？是什么将所有这些联系在一起？给定逻辑回归*p*的概率和输入变量*x*，就是这样：
- en: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis equals beta 0 plus beta 1 x" display="block"><mrow><mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math>
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="l o g left-parenthesis StartFraction p Over 1 minus p EndFraction
    right-parenthesis equals beta 0 plus beta 1 x" display="block"><mrow><mi>l</mi>
    <mi>o</mi> <mi>g</mi> <mrow><mo>(</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac>
    <mo>)</mo></mrow> <mo>=</mo> <msub><mi>β</mi> <mn>0</mn></msub> <mo>+</mo> <msub><mi>β</mi>
    <mn>1</mn></msub> <mi>x</mi></mrow></math>
- en: Let’s plot the log-odds line alongside the logistic regression, as shown in
    [Figure 6-10](#PnVjoMFBlR).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将对数几率线与逻辑回归一起绘制，如[图6-10](#PnVjoMFBlR)所示。
- en: '![emds 0610](Images/emds_0610.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0610](Images/emds_0610.png)'
- en: Figure 6-10\. The log-odds line is converted into a logistic function that outputs
    a probability
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10。对数几率线转换为输出概率的逻辑函数
- en: Every logistic regression is actually backed by a linear function, and that
    linear function is a log-odds function. Note in [Figure 6-10](#PnVjoMFBlR) that
    when the log-odds is 0.0 on the line, then the probability of the logistic curve
    is at 0.5\. This makes sense because when our odds are fair at 1.0, the probability
    is going to be 0.50 as shown in the logistic regression, and the log-odds are
    going to be 0 as shown by the line.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 每个逻辑回归实际上都由一个线性函数支持，而该线性函数是一个对数几率函数。请注意，在[图6-10](#PnVjoMFBlR)中，当对数几率在线上为0.0时，逻辑曲线的概率为0.5。这是有道理的，因为当我们的赔率为1.0时，概率将为0.50，如逻辑回归所示，而对数几率将为0，如线所示。
- en: Another benefit we get looking at the logistic regression from an odds perspective
    is we can compare the effect between one x-value and another. Let’s say I want
    to understand how much my odds change between six hours and eight hours of exposure
    to the chemical. I can take the odds at six hours and then eight hours, and then
    ratio the two odds against each other in an *odds ratio*. This is not to be confused
    with a plain odds which, yes, is a ratio, but it is not an odds ratio.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 从赔率的角度看逻辑回归的另一个好处是我们可以比较一个x值和另一个x值之间的效果。假设我想了解暴露于化学物质六小时和八小时之间我的赔率变化有多大。我可以取六小时和八小时的赔率，然后将两个赔率相对于彼此进行比较，得到一个*赔率比*。这不应与普通赔率混淆，是的，它是一个比率，但不是赔率比。
- en: 'Let’s first find the probabilities of symptoms for six hours and eight hours,
    respectively:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先找出分别为六小时和八小时的症状概率：
- en: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>p</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math><math display="block"><mrow><msub><mi>p</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>8</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.914167258137741</mn></mrow></math>
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>p</mi> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><msub><mi>β</mi>
    <mn>0</mn></msub> <mo>+</mo><msub><mi>β</mi> <mn>1</mn></msub> <mi>x</mi><mo>)</mo></mrow></msup></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>p</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow>
    <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi> <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>6</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.727161542928554</mn></mrow></math><math display="block"><mrow><msub><mi>p</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>1.0</mn></mrow> <mrow><mn>1.0</mn><mo>+</mo><msup><mi>e</mi>
    <mrow><mo>-</mo><mo>(</mo><mo>-</mo><mn>3.17576395</mn><mo>+</mo><mn>0.69267212</mn><mo>(</mo><mn>8</mn><mo>)</mo><mo>)</mo></mrow></msup></mrow></mfrac>
    <mo>=</mo> <mn>0.914167258137741</mn></mrow></math>
- en: 'Now let’s convert those into odds, which we will declare as <math alttext="o
    Subscript x"><msub><mi>o</mi> <mi>x</mi></msub></math> :'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将这些转换为几率，我们将其声明为<math alttext="o Subscript x"><msub><mi>o</mi> <mi>x</mi></msub></math>：
- en: <math display="block"><mrow><mi>o</mi> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>o</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.727161542928554</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>0.727161542928554</mn></mrow></mfrac> <mo>=</mo>
    <mn>2.66517246407876</mn></mrow></math><math display="block"><mrow><msub><mi>o</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.914167258137741</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>0.914167258137741</mn></mrow></mfrac>
    <mo>=</mo> <mn>10.6505657200694</mn></mrow></math>
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>o</mi> <mo>=</mo> <mfrac><mi>p</mi> <mrow><mn>1</mn><mo>-</mo><mi>p</mi></mrow></mfrac></mrow></math><math
    display="block"><mrow><msub><mi>o</mi> <mn>6</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.727161542928554</mn></mrow>
    <mrow><mn>1</mn><mo>-</mo><mn>0.727161542928554</mn></mrow></mfrac> <mo>=</mo>
    <mn>2.66517246407876</mn></mrow></math><math display="block"><mrow><msub><mi>o</mi>
    <mn>8</mn></msub> <mo>=</mo> <mfrac><mrow><mn>0.914167258137741</mn></mrow> <mrow><mn>1</mn><mo>-</mo><mn>0.914167258137741</mn></mrow></mfrac>
    <mo>=</mo> <mn>10.6505657200694</mn></mrow></math>
- en: 'Finally, set the two odds against each other as an odds ratio, where the odds
    for eight hours is the numerator and the odds for six hours is in the denominator.
    We get a value of approximately 3.996, meaning that our odds of showing symptoms
    increases by nearly a factor of four with an extra two hours of exposure:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，将两个几率相互对比作为一个几率比值，其中8小时的几率为分子，6小时的几率为分母。我们得到一个约为3.996的数值，这意味着我们的症状出现的几率随着额外两小时的暴露增加了近四倍：
- en: <math display="block"><mrow><mtext>odds ratio</mtext> <mo>=</mo> <mfrac><mrow><mn>10.6505657200694</mn></mrow>
    <mrow><mn>2.66517246407876</mn></mrow></mfrac> <mo>=</mo> <mn>3.99620132040906</mn></mrow></math>
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>odds ratio</mtext> <mo>=</mo> <mfrac><mrow><mn>10.6505657200694</mn></mrow>
    <mrow><mn>2.66517246407876</mn></mrow></mfrac> <mo>=</mo> <mn>3.99620132040906</mn></mrow></math>
- en: You will find this odds ratio value of 3.996 holds across any two-hour range,
    like 2 hours to 4 hours, 4 hours to 6 hours, 8 hours to 10 hours, and so forth.
    As long as it’s a two-hour gap, you will find that odds ratio stays consistent.
    It will differ for other range lengths.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现这个几率比值为3.996的数值在任何两小时范围内都成立，比如2小时到4小时，4小时到6小时，8小时到10小时等等。只要是两小时的间隔，你会发现几率比值保持一致。对于其他范围长度，它会有所不同。
- en: R-Squared
  id: totrans-156
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: R-平方
- en: We covered quite a few statistical metrics for linear regression in [Chapter 5](ch05.xhtml#ch05),
    and we will try to do the same for logistic regression. We still worry about many
    of the same problems as in linear regression, including overfitting and variance.
    As a matter of fact, we can borrow and adapt several metrics from linear regression
    and apply them to logistic regression. Let’s start with <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> .
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在[第5章](ch05.xhtml#ch05)中已经涵盖了线性回归的许多统计指标，我们将尝试对 logistic 回归做同样的事情。我们仍然担心许多与线性回归相同的问题，包括过拟合和方差。事实上，我们可以借鉴并调整几个线性回归的指标，并将它们应用于
    logistic 回归。让我们从<math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>开始。
- en: Just like linear regression, there is an <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> for a given logistic regression. If you recall from [Chapter 5](ch05.xhtml#ch05),
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    indicates how well a given independent variable explains a dependent variable.
    Applying this to our chemical exposure problem, it makes sense we want to measure
    how much chemical exposure hours explains showing symptoms.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，给定 logistic 回归也有一个<math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math>。如果你回忆一下[第5章](ch05.xhtml#ch05)，<math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math>表示一个给定自变量解释因变量的程度。将这应用于我们的化学暴露问题，我们想要衡量化学暴露小时数解释症状出现的程度是有意义的。
- en: 'There is not really a consensus on the best way to calculate the <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> on a logistic regression,
    but a popular technique known as McFadden’s Pseudo <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> closely mimics the <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> used in linear regression. We will use this technique
    in the following examples and here is the formula:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上并没有关于如何计算 logistic 回归的<math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    的最佳方法的共识，但一种被称为麦克法登伪<math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    的流行技术紧密模仿了线性回归中使用的<math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>。我们将在以下示例中使用这种技术，以下是公式：
- en: <math alttext="upper R squared equals StartFraction left-parenthesis log likelihood
    right-parenthesis minus left-parenthesis log likelihood fit right-parenthesis
    Over left-parenthesis log likelihood right-parenthesis EndFraction" display="block"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo><mo>-</mo><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mtext>fit</mtext><mo>)</mo></mrow>
    <mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo></mrow></mfrac></mrow></math>
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="upper R squared equals StartFraction left-parenthesis log likelihood
    right-parenthesis minus left-parenthesis log likelihood fit right-parenthesis
    Over left-parenthesis log likelihood right-parenthesis EndFraction" display="block"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo><mo>-</mo><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mtext>fit</mtext><mo>)</mo></mrow>
    <mrow><mo>(</mo><mtext>log</mtext><mtext>likelihood</mtext><mo>)</mo></mrow></mfrac></mrow></math>
- en: We will learn how to calculate the “log likelihood fit” and “log likelihood”
    so we can calculate the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    .
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将学习如何计算“对数似然拟合”和“对数似然”，以便计算<math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math>。
- en: We cannot use residuals here like in linear regression, but we can project the
    outcomes back onto the logistic curve as shown in [Figure 6-11](#nTgHGJUoVu),
    and look up their corresponding likelihoods between 0.0 and 1.0.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不能像线性回归那样在这里使用残差，但我们可以将结果投影回逻辑曲线，如[图6-11](#nTgHGJUoVu)所示，并查找它们在0.0和1.0之间的相应可能性。
- en: '![emds 0611](Images/emds_0611.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0611](Images/emds_0611.png)'
- en: Figure 6-11\. Projecting the output values back onto the logistic curve
  id: totrans-164
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 将输出值投影回逻辑曲线
- en: We can then take the `log()` of each of those likelihoods and sum them together.
    This will be the log likelihood of the fit ([Example 6-10](#oFlUCSCfPU)). Just
    like we did calculating maximum likelihood, we will convert the “false” likelihoods
    by subtracting from 1.0.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们可以取这些可能性的`log()`并将它们相加。这将是拟合的对数似然（[示例6-10](#oFlUCSCfPU)）。就像我们计算最大似然一样，我们通过从1.0中减去“假”可能性来转换“假”可能性。
- en: Example 6-10\. Calculating the log likelihood of the fit
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-10\. 计算拟合的对数似然
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Using some clever binary multiplication and Python comprehensions, we can consolidate
    that `for` loop and `if` expression into one line that returns the `log_likelihood_fit`.
    Similar to what we did in the maximum likelihood formula, we can use some binary
    subtraction between the true and false cases to mathematically eliminate one or
    the other. In this case, we multiply by 0 and therefore apply either the true
    or the false case, but not both, to the sum accordingly ([Example 6-11](#etUMOEwqlo)).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 使用一些巧妙的二进制乘法和Python推导，我们可以将那个`for`循环和`if`表达式整合成一行，返回`log_likelihood_fit`。类似于我们在最大似然公式中所做的，我们可以使用一些真假病例之间的二进制减法来在数学上消除其中一个。在这种情况下，我们乘以0，因此相应地将真或假情况应用于总和（[示例6-11](#etUMOEwqlo)）。
- en: Example 6-11\. Consolidating our log likelihood logic into a single line
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-11\. 将我们的对数似然逻辑整合成一行
- en: '[PRE10]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If we were to express the likelihood of the fit in mathematic notation, this
    is what it would look like. Note that <math alttext="f left-parenthesis x Subscript
    i Baseline right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math> is the logistic function for a given input variable <math
    alttext="x Subscript i"><msub><mi>x</mi> <mi>i</mi></msub></math> :'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们要用数学符号来表达拟合的可能性，它会是这个样子。注意<math alttext="f left-parenthesis x Subscript
    i Baseline right-parenthesis"><mrow><mi>f</mi> <mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow></math>是给定输入变量<math alttext="x Subscript i"><msub><mi>x</mi>
    <mi>i</mi></msub></math>的逻辑函数：
- en: <math display="block"><mrow><mtext>log likelihood fit</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>×</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mn>1.0</mn> <mo>-</mo> <mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>log likelihood fit</mtext> <mo>=</mo> <munderover><mo>∑</mo>
    <mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow> <mi>n</mi></munderover> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mi>f</mi> <mrow><mo>(</mo>
    <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow> <mo>×</mo>
    <msub><mi>y</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>+</mo> <mrow><mo>(</mo>
    <mi mathvariant="italic">log</mi> <mrow><mo>(</mo> <mn>1.0</mn> <mo>-</mo> <mi>f</mi>
    <mrow><mo>(</mo> <msub><mi>x</mi> <mi>i</mi></msub> <mo>)</mo></mrow> <mo>)</mo></mrow>
    <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi> <mi>i</mi></msub>
    <mo>)</mo></mrow> <mo>)</mo></mrow></mrow></math>
- en: 'As calculated in Examples [6-10](#oFlUCSCfPU) and [6-11](#etUMOEwqlo), we have
    -9.9461 as our log likelihood of the fit. We need one more datapoint to calculate
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    : the log likelihood that estimates without using any input variables and simply
    uses the number of true cases divided by all cases (effectively leaving only the
    intercept). Note we can count the number of symptomatic cases by summing all the
    y-values together <math alttext="sigma-summation y Subscript i"><mrow><mo>∑</mo>
    <msub><mi>y</mi> <mi>i</mi></msub></mrow></math> , because only the 1s and not
    the 0s will count into the sum. Here is the formula:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如在示例[6-10](#oFlUCSCfPU)和[6-11](#etUMOEwqlo)中计算的，我们的拟合对数似然为-9.9461。我们需要另一个数据点来计算<math
    alttext="上限 R 平方"><msup><mi>R</mi> <mn>2</mn></msup></math>：即估计不使用任何输入变量，只使用真实病例数除以所有病例数（实际上只留下截距）。请注意，我们可以通过将所有y值相加<math
    alttext="sigma-summation y Subscript i"><mrow><mo>∑</mo> <msub><mi>y</mi> <mi>i</mi></msub></mrow></math>来计算症状病例的数量，因为只有1会计入总和，而0不会。这是公式：
- en: <math alttext="log likelihood equals StartFraction sigma-summation y Subscript
    i Baseline Over n EndFraction times y Subscript i Baseline plus left-parenthesis
    1 minus StartFraction sigma-summation y Subscript i Baseline Over n EndFraction
    right-parenthesis times left-parenthesis 1 minus y Subscript i Baseline right-parenthesis"
    display="block"><mrow><mtext>log</mtext> <mtext>likelihood</mtext> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>×</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="log likelihood equals StartFraction sigma-summation y Subscript
    i Baseline Over n EndFraction times y Subscript i Baseline plus left-parenthesis
    1 minus StartFraction sigma-summation y Subscript i Baseline Over n EndFraction
    right-parenthesis times left-parenthesis 1 minus y Subscript i Baseline right-parenthesis"
    display="block"><mrow><mtext>log</mtext> <mtext>likelihood</mtext> <mo>=</mo>
    <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>×</mo> <msub><mi>y</mi> <mi>i</mi></msub> <mo>+</mo> <mrow><mo>(</mo> <mn>1</mn>
    <mo>-</mo> <mfrac><mrow><mo>∑</mo><msub><mi>y</mi> <mi>i</mi></msub></mrow> <mi>n</mi></mfrac>
    <mo>)</mo></mrow> <mo>×</mo> <mrow><mo>(</mo> <mn>1</mn> <mo>-</mo> <msub><mi>y</mi>
    <mi>i</mi></msub> <mo>)</mo></mrow></mrow></math>
- en: Here is the expanded Python equivalent of this formula applied in [Example 6-12](#pBKtDOrDKa).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这是应用于[示例6-12](#pBKtDOrDKa)中的公式的Python等效展开。
- en: Example 6-12\. Log likelihood of patients
  id: totrans-176
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-12\. 患者的对数似然
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: To consolidate this logic and reflect the formula, we can compress that `for`
    loop and `if` expression into a single line, using some binary multiplication
    logic to handle both true and false cases ([Example 6-13](#pvdQHolacp)).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了整合这个逻辑并反映公式，我们可以将那个`for`循环和`if`表达式压缩成一行，使用一些二进制乘法逻辑来处理真假病例（[示例6-13](#pvdQHolacp)）。
- en: Example 6-13\. Consolidating the log likelihood into a single line
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-13\. 将对数似然整合成一行
- en: '[PRE12]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, just plug these values in and get your <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> :'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，只需将这些值代入并获得你的<math alttext="上限 R 平方"><msup><mi>R</mi> <mn>2</mn></msup></math>：
- en: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mtext>(log
    likelihood)</mtext><mo>-</mo><mtext>(log likelihood fit)</mtext></mrow> <mrow><mo>(</mo><mtext>log
    likelihood</mtext><mo>)</mo></mrow></mfrac></mrow></math><math display="block"><mrow><msup><mi>R</mi>
    <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>-</mo><mn>0.5596</mn><mo>-</mo><mo>(</mo><mo>-</mo><mn>9.9461</mn><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>0.5596</mn></mrow></mfrac> <msup><mi>R</mi> <mn>2</mn></msup>
    <mo>=</mo> <mn>0.306456</mn></mrow></math>
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mtext>(对数似然)</mtext><mo>-</mo><mtext>(拟合对数似然)</mtext></mrow>
    <mrow><mo>(</mo><mtext>对数似然</mtext><mo>)</mo></mrow></mfrac></mrow></math><math
    display="block"><mrow><msup><mi>R</mi> <mn>2</mn></msup> <mo>=</mo> <mfrac><mrow><mo>-</mo><mn>0.5596</mn><mo>-</mo><mo>(</mo><mo>-</mo><mn>9.9461</mn><mo>)</mo></mrow>
    <mrow><mo>-</mo><mn>0.5596</mn></mrow></mfrac> <msup><mi>R</mi> <mn>2</mn></msup>
    <mo>=</mo> <mn>0.306456</mn></mrow></math>
- en: And here is the Python code shown in [Example 6-14](#gbVRCTIbOV), calculating
    the <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    in its entirety.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是Python代码，显示在[示例 6-14](#gbVRCTIbOV)中，完整计算<math alttext="上R平方"><msup><mi>R</mi>
    <mn>2</mn></msup></math>。
- en: Example 6-14\. Calculating the <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> for a logistic regression
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-14\. 计算逻辑回归的<math alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>
- en: '[PRE13]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: OK, so we got an <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    = 0.306456, so do hours of chemical exposure explain whether someone shows symptoms?
    As we learned in [Chapter 5](ch05.xhtml#ch05) on linear regression, a poor fit
    will be closer to an <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    of 0.0 and a greater fit will be closer to 1.0\. Therefore, we can conclude that
    hours of exposure is mediocre for predicting symptoms, as the <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> is 0.30645\. There must be
    variables other than time exposure that better predict if someone will show symptoms.
    This makes sense because we have a large mix of patients showing symptoms versus
    not showing symptoms for most of our observed data, as shown in [Figure 6-12](#IPKLkNGRGb).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们得到一个<math alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math> =
    0.306456，那么化学暴露时间是否能解释某人是否出现症状？正如我们在[第5章](ch05.xhtml#ch05)中学到的线性回归一样，拟合不好的情况下<math
    alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>会接近0.0，而拟合较好的情况下会接近1.0。因此，我们可以得出结论，暴露时间对于预测症状是一般般的，因为<math
    alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>为0.30645。除了时间暴露之外，肯定还有其他变量更好地预测某人是否会出现症状。这是有道理的，因为我们观察到的大多数数据中，有很多患者出现症状和没有出现症状的混合，如[图6-12](#IPKLkNGRGb)所示。
- en: '![emds 0612](Images/emds_0612.png)'
  id: totrans-187
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0612](Images/emds_0612.png)'
- en: Figure 6-12\. Our data has a mediocre <math alttext="upper R squared"><msup><mi>R</mi>
    <mn>2</mn></msup></math> of 0.30645 because there is a lot of variance in the
    middle of our curve
  id: totrans-188
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-12\. 我们的数据中间有很多方差，因此我们的数据有一个中等的<math alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>为0.30645
- en: But if we did have a clean divide in our data, where 1 and 0 outcomes are cleanly
    separated as shown in [Figure 6-13](#JPVUIsSBQP), we would have a perfect <math
    alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> of 1.0.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们的数据中有一个明确的分界线，其中1和0的结果被清晰地分开，如[图6-13](#JPVUIsSBQP)所示，我们将有一个完美的<math alttext="上R平方"><msup><mi>R</mi>
    <mn>2</mn></msup></math>为1.0。
- en: '![emds 0613](Images/emds_0613.png)'
  id: totrans-190
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0613](Images/emds_0613.png)'
- en: Figure 6-13\. This logistic regression has a perfect <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> of 1.0 because there is a clean
    divide in outcomes predicted by hours of exposure
  id: totrans-191
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-13\. 这个逻辑回归有一个完美的<math alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>为1.0，因为根据暴露时间预测的结果有一个清晰的分界线
- en: P-Values
  id: totrans-192
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: P-值
- en: Just like linear regression, we are not done just because we have an <math alttext="upper
    R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> . We need to investigate
    how likely we would have seen this data by chance rather than because of an actual
    relationship. This means we need a p-value.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 就像线性回归一样，我们并不因为有一个<math alttext="上R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>就结束了。我们需要调查我们看到这些数据是因为偶然还是因为实际关系的可能性。这意味着我们需要一个p值。
- en: To do this, we will need to learn a new probability distribution called the
    *chi-square distribution*, annotated as <math alttext="chi squared"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> distribution. It is continuous and used in several areas
    of statistics, including this one!
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们需要学习一个称为*卡方分布*的新概率分布，标记为<math alttext="卡方"><msup><mi>χ</mi> <mn>2</mn></msup></math>分布。它是连续的，在统计学的几个领域中使用，包括这个！
- en: If we take each value in a standard normal distribution (mean of 0 and standard
    deviation of 1) and square it, that will give us the <math alttext="chi squared"><msup><mi>χ</mi>
    <mn>2</mn></msup></math> distribution with one degree of freedom. For our purposes,
    the degrees of freedom will depend on how many parameters <math alttext="n"><mi>n</mi></math>
    are in our logistic regression, which will be <math alttext="n minus 1"><mrow><mi>n</mi>
    <mo>-</mo> <mn>1</mn></mrow></math> . You can see examples of different degrees
    of freedom in [Figure 6-14](#pmQLIIsNqF).
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们取标准正态分布中的每个值（均值为0，标准差为1）并平方，那将给我们一个自由度为1的<math alttext="卡方"><msup><mi>χ</mi>
    <mn>2</mn></msup></math>分布。对于我们的目的，自由度将取决于我们逻辑回归中有多少参数<math alttext="n"><mi>n</mi></math>，这将是<math
    alttext="n减1"><mrow><mi>n</mi> <mo>-</mo> <mn>1</mn></mrow></math>。你可以在[图6-14](#pmQLIIsNqF)中看到不同自由度的例子。
- en: '![emds 0614](Images/emds_0614.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0614](Images/emds_0614.png)'
- en: Figure 6-14\. A <math alttext="chi squared"><msup><mi>χ</mi> <mn>2</mn></msup></math>
    distribution with differing degrees of freedom
  id: totrans-197
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14。不同自由度的<math alttext="卡方"><msup><mi>χ</mi> <mn>2</mn></msup></math>分布
- en: Since we have two parameters (hours of exposure and whether symptoms were shown),
    our degree of freedom will be 1 because <math alttext="2 minus 1 equals 1"><mrow><mn>2</mn>
    <mo>-</mo> <mn>1</mn> <mo>=</mo> <mn>1</mn></mrow></math> .
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有两个参数（暴露时间和是否出现症状），我们的自由度将为1，因为<math alttext="2减1等于1"><mrow><mn>2</mn> <mo>-</mo>
    <mn>1</mn> <mo>=</mo> <mn>1</mn></mrow></math>。
- en: 'We will need the log likelihood fit and log likelihood as calculated in the
    previous subsection on R². Here is the formula that will produce the <math alttext="chi
    squared"><msup><mi>χ</mi> <mn>2</mn></msup></math> value we need to look up:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在前一小节关于R²中计算的对数似然拟合和对数似然。这是产生我们需要查找的<math alttext="卡方"><msup><mi>χ</mi>
    <mn>2</mn></msup></math>值的公式：
- en: <math alttext="chi squared equals 2 left-parenthesis log likelihood fit right-parenthesis
    minus left-parenthesis log likelihood right-parenthesis" display="block"><mrow><msup><mi>χ</mi>
    <mn>2</mn></msup> <mo>=</mo> <mn>2</mn> <mrow><mo>(</mo> <mtext>log</mtext> <mtext>likelihood</mtext>
    <mtext>fit</mtext> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mo>)</mo></mrow></mrow></math>
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="chi squared equals 2 left-parenthesis log likelihood fit right-parenthesis
    minus left-parenthesis log likelihood right-parenthesis" display="block"><mrow><msup><mi>χ</mi>
    <mn>2</mn></msup> <mo>=</mo> <mn>2</mn> <mrow><mo>(</mo> <mtext>log</mtext> <mtext>likelihood</mtext>
    <mtext>fit</mtext> <mo>)</mo></mrow> <mo>-</mo> <mrow><mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mo>)</mo></mrow></mrow></math>
- en: 'We then take that value and look up the probability from the <math alttext="chi
    squared"><msup><mi>χ</mi> <mn>2</mn></msup></math> distribution. That will give
    us our p-value:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们取该值并从<math alttext="卡方"><msup><mi>χ</mi> <mn>2</mn></msup></math>分布中查找概率。这将给我们我们的p值：
- en: <math alttext="p hyphen value equals chi left-parenthesis 2 left-parenthesis
    left-parenthesis log likelihood fit right-parenthesis minus left-parenthesis log
    likelihood right-parenthesis right-parenthesis" display="block"><mrow><mtext>p-value</mtext>
    <mo>=</mo> <mtext>chi</mtext> <mo>(</mo> <mn>2</mn> <mo>(</mo> <mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mtext>fit</mtext> <mo>)</mo> <mo>-</mo> <mo>(</mo>
    <mtext>log</mtext> <mtext>likelihood</mtext> <mo>)</mo> <mo>)</mo></mrow></math>
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="p hyphen value equals chi left-parenthesis 2 left-parenthesis
    left-parenthesis log likelihood fit right-parenthesis minus left-parenthesis log
    likelihood right-parenthesis right-parenthesis" display="block"><mrow><mtext>p-value</mtext>
    <mo>=</mo> <mtext>chi</mtext> <mo>(</mo> <mn>2</mn> <mo>(</mo> <mo>(</mo> <mtext>log</mtext>
    <mtext>likelihood</mtext> <mtext>fit</mtext> <mo>)</mo> <mo>-</mo> <mo>(</mo>
    <mtext>log</mtext> <mtext>likelihood</mtext> <mo>)</mo> <mo>)</mo></mrow></math>
- en: '[Example 6-15](#PPMuoBavDD) shows our p-value for a given fitted logistic regression.
    We use SciPy’s `chi2` module to use the chi-square distribution.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例6-15](#PPMuoBavDD)展示了我们拟合的逻辑回归的p值。我们使用SciPy的`chi2`模块使用卡方分布。'
- en: Example 6-15\. Calculating a p-value for a given logistic regression
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-15。计算给定逻辑回归的p值
- en: '[PRE14]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: So we have a p-value of 0.00166, and if our threshold for signifiance is .05,
    we say this data is statistically significant and was not by random chance.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 所以我们有一个p值为0.00166，如果我们的显著性阈值为0.05，我们说这些数据在统计上是显著的，不是随机事件。
- en: Train/Test Splits
  id: totrans-207
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 训练/测试拆分
- en: As covered in [Chapter 5](ch05.xhtml#ch05) on linear regression, we can use
    train/test splits as a way to validate machine learning algorithms. This is the
    more machine learning approach to assessing the performance of a logistic regression.
    While it is a good idea to rely on traditional statistical metrics like <math
    alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math> and p-values,
    when you are dealing with more variables, this becomes less practical. This is
    where train/test splits come in handy once again. To review, [Figure 6-15](#HgfhjgkCrp2)
    visualizes a three-fold cross-validation alternating a testing dataset.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如第5章中所述，我们可以使用训练/测试拆分来验证机器学习算法。这是评估逻辑回归性能的更机器学习方法。虽然依赖传统统计指标如<math alttext="上限R平方"><msup><mi>R</mi>
    <mn>2</mn></msup></math>和p值是个好主意，但当你处理更多变量时，这变得不太实际。这时再次用到了训练/测试拆分。回顾一下，[图6-15](#HgfhjgkCrp2)展示了一个三折交叉验证交替测试数据集。
- en: '![emds 0615](Images/emds_0615.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0615](Images/emds_0615.png)'
- en: Figure 6-15\. A three-fold cross-validation alternating each third of the dataset
    as a testing dataset
  id: totrans-210
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。一个三折交叉验证，交替将数据集的每个第三部分作为测试数据集
- en: In [Example 6-16](#RHQQqqghLN) we perform a logistic regression on the employee-retention
    dataset, but we split the data into thirds. We then alternate each third as the
    testing data. Finally, we summarize the three accuracies with an average and standard
    deviation.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在[示例6-16](#RHQQqqghLN)中，我们对员工留存数据集执行逻辑回归，但将数据分成三部分。然后我们交替将每个部分作为测试数据。最后，我们用平均值和标准差总结三个准确性。
- en: Example 6-16\. Performing a logistic regression with three-fold cross-validation
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-16。执行带有三折交叉验证的逻辑回归
- en: '[PRE15]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: We can also use random-fold validation, leave-one-out cross-validation, and
    all the other folding variants we performed in [Chapter 5](ch05.xhtml#ch05). With
    that out of the way, let’s talk about why accuracy is a bad measure for classification.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以使用随机折叠验证、留一交叉验证以及我们在[第5章](ch05.xhtml#ch05)中执行的所有其他折叠变体。说完这些，让我们谈谈为什么准确率是分类的一个糟糕的度量。
- en: Confusion Matrices
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 混淆矩阵
- en: Suppose a model observed people with the name “Michael” quit their job. The
    reason why first and last names are captured as input variables is indeed questionable,
    as it is doubtful someone’s name has any impact on whether they quit. However,
    to simplify the example, let’s go with it. The model then predicts that any person
    named “Michael” will quit their job.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个模型观察到名为“迈克尔”的人离职。捕获名字作为输入变量的原因确实值得怀疑，因为一个人的名字是否会影响他们是否离职是可疑的。然而，为了简化例子，让我们继续。该模型随后预测任何名为“迈克尔”的人都会离职。
- en: Now this is where accuracy falls apart. I have one hundred employees, including
    one named “Michael” and another named “Sam.” Michael is wrongly predicted to quit,
    and it is Sam that ends up quitting. What’s the accuracy of my model? It is 98%
    because there were only two wrong predictions out of one hundred employees as
    visualized in [Figure 6-16](#SouBMsENVQ).
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这就是准确率失效的地方。我有一百名员工，其中一个名叫“迈克尔”，另一个名叫“山姆”。迈克尔被错误地预测会离职，而实际上是山姆离职了。我的模型准确率是多少？是98%，因为在一百名员工中只有两次错误预测，如[图6-16](#SouBMsENVQ)所示。
- en: '![emds 0616](Images/emds_0616.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0616](Images/emds_0616.png)'
- en: Figure 6-16\. The employee named “Michael” is predicted to quit, but it’s actually
    another employee that does, giving us 98% accuracy
  id: totrans-219
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-16\. 预测名为“迈克尔”的员工会离职，但实际上是另一名员工离职，给我们带来了98%的准确率
- en: Especially for imbalanced data where the event of interest (e.g., a quitting
    employee) is rare, the accuracy metric is horrendously misleading for classification
    problems. If a vendor, consultant, or data scientist ever tries to sell you a
    classification system on claims of accuracy, ask for a confusion matrix.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其对于数据不平衡的情况，其中感兴趣的事件（例如，员工离职）很少见，准确率指标对于分类问题是极其误导的。如果供应商、顾问或数据科学家试图通过准确性来销售分类系统，请要求一个混淆矩阵。
- en: A *confusion matrix* is a grid that breaks out the predictions against the actual
    outcomes showing the true positives, true negatives, false positives (type I error),
    and false negatives (type II error). Here is a confusion matrix presented in [Figure 6-17](#sVsaaKoGnR).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '*混淆矩阵*是一个网格，将预测与实际结果进行对比，显示出真正的正例、真正的负例、假正例（I型错误）和假负例（II型错误）。这里是一个在[图6-17](#sVsaaKoGnR)中呈现的混淆矩阵。'
- en: '![emds 0617](Images/emds_0617.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0617](Images/emds_0617.png)'
- en: Figure 6-17\. A simple confusion matrix
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-17\. 一个简单的混淆矩阵
- en: Generally, we want the diagonal values (top-left to bottom-right) to be higher
    because these reflect correct classifications. We want to evaluate how many employees
    who were predicted to quit actually did quit (true positives). Conversely, we
    also want to evaluate how many employees who were predicted to stay actually did
    stay (true negatives).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们希望对角线值（从左上到右下）更高，因为这些反映了正确的分类。我们想评估有多少被预测会离职的员工实际上确实离职（真正的正例）。相反，我们也想评估有多少被预测会留下的员工实际上确实留下（真正的负例）。
- en: The other cells reflect wrong predictions, where an employee predicted to quit
    ended up staying (false positive), and where an employee predicted to stay ends
    up quitting (false negative).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 其他单元格反映了错误的预测，其中一个被预测会离职的员工最终留下（假正例），以及一个被预测会留下的员工最终离职（假负例）。
- en: What we need to do is dice up that accuracy metric into more specific accuracy
    metrics targeting different parts of the confusion matrix. Let’s look at [Figure 6-18](#mbgQPeoPAQ),
    which adds some useful measures.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将准确率指标分解为针对混淆矩阵不同部分的更具体的准确率指标。让我们看看[图6-18](#mbgQPeoPAQ)，它添加了一些有用的指标。
- en: From the confusion matrix, we can derive all sorts of useful metrics beyond
    just accuracy. We can easily see that precision (how accurate positive predictions
    were) and sensitivity (rate of identified positives) are 0, meaning this machine
    learning model fails entirely at positive predictions.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 从混淆矩阵中，我们可以得出除准确率之外的各种有用的指标。我们可以清楚地看到精确度（正面预测的准确性）和灵敏度（识别出的正面率）都为0，这意味着这个机器学习模型在正面预测上完全失败。
- en: '![emds 0618](Images/emds_0618.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0618](Images/emds_0618.png)'
- en: Figure 6-18\. Adding useful metrics to the confusion matrix
  id: totrans-229
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-18\. 在混淆矩阵中添加有用的指标
- en: '[Example 6-17](#jsciVCfjkQ) shows how to use the confusion matrix API in SciPy
    on a logistic regression with a train/test split. Note that the confusion matrix
    is only applied to the testing dataset.'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 6-17](#jsciVCfjkQ)展示了如何在SciPy中使用混淆矩阵API对具有训练/测试拆分的逻辑回归进行操作。请注意，混淆矩阵仅应用于测试数据集。'
- en: Example 6-17\. Creating a confusion matrix for a testing dataset in SciPy
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例6-17。在SciPy中为测试数据集创建混淆矩阵
- en: '[PRE16]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Bayes’ Theorem and Classification
  id: totrans-233
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 贝叶斯定理和分类
- en: Do you recall Bayes’ Theorem in [Chapter 2](ch02.xhtml#ch02)? You can use Bayes’
    Theorem to bring in outside information to further validate findings on a confusion
    matrix. [Figure 6-19](#AKsSBkvTTJ) shows a confusion matrix of one thousand patients
    tested for a disease.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 你还记得[第2章](ch02.xhtml#ch02)中的贝叶斯定理吗？你可以使用贝叶斯定理引入外部信息来进一步验证混淆矩阵的发现。[图6-19](#AKsSBkvTTJ)展示了对一千名患者进行疾病测试的混淆矩阵。
- en: '![emds 0619](Images/emds_0619.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0619](Images/emds_0619.png)'
- en: Figure 6-19\. A confusion matrix for a medical test identifying a disease
  id: totrans-236
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-19。用于识别疾病的医学测试的混淆矩阵
- en: 'We are told that for patients that have a health risk, 99% will be identified
    successfully (sensitivity). Using the confusion matrix, we can see this mathematically
    checks out:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 我们被告知对于存在健康风险的患者，将成功识别99%（敏感性）。使用混淆矩阵，我们可以数学上验证这一点：
- en: <math display="block"><mrow><mtext>sensitivity</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>2</mn></mrow></mfrac> <mo>=</mo> <mn>.99</mn></mrow></math>
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>sensitivity</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>2</mn></mrow></mfrac> <mo>=</mo> <mn>.99</mn></mrow></math>
- en: 'But what if we flip the condition? What percentage of those who tested positive
    have the health risk (precision)? While we are flipping a conditional probability,
    we do not have to use Bayes’ Theorem here because the confusion matrix gives us
    all the numbers we need:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果我们颠倒条件呢？那么测试结果为阳性的人中有多少百分比存在健康风险（精确度）？虽然我们在颠倒条件概率，但在这里我们不必使用贝叶斯定理，因为混淆矩阵为我们提供了所有我们需要的数字：
- en: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>50</mn></mrow></mfrac> <mo>=</mo> <mn>.798</mn></mrow></math>
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mtext>precision</mtext> <mo>=</mo> <mfrac><mn>198</mn>
    <mrow><mn>198</mn><mo>+</mo><mn>50</mn></mrow></mfrac> <mo>=</mo> <mn>.798</mn></mrow></math>
- en: OK, so 79.8% is not terrible, and that’s the percentage of people who tested
    positive that actually have the disease. But ask yourself this…what are we assuming
    about our data? Is it representative of the population?
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，79.8%并不算糟糕，这是测试为阳性的人中实际患有疾病的百分比。但请问自己...我们对数据做了什么假设？它是否代表人口？
- en: Some quick research found 1% of the population actually has the disease. There
    is an opportunity to use Bayes’ Theorem here. We can account for the proportion
    of the population that actually has the disease and incorporate that into our
    confusion matrix findings. We then discover something significant.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 一些快速研究发现1%的人口实际上患有这种疾病。在这里有机会使用贝叶斯定理。我们可以考虑实际患有疾病的人口比例，并将其纳入我们的混淆矩阵结果中。然后我们发现了一些重要的东西。
- en: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>At Risk if Positive</mtext>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>Positive
    if At Risk</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mtext>(At Risk)</mtext></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>Positive</mtext><mo>)</mo></mrow></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>At Risk if Positive</mtext>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mn>.99</mn><mo>×</mo><mn>.01</mn></mrow>
    <mrow><mn>.248</mn></mrow></mfrac></mrow></math><math display="block"><mrow><mi>P</mi>
    <mo>(</mo> <mtext>At Risk if Positive</mtext> <mo>)</mo> <mo>=</mo> <mn>.0339</mn></mrow></math>
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: <math display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>如果阳性则存在风险</mtext>
    <mo>)</mo></mrow> <mo>=</mo> <mfrac><mrow><mi>P</mi><mo>(</mo><mtext>如果存在风险则阳性</mtext><mo>)</mo><mo>×</mo><mi>P</mi><mtext>(存在风险)</mtext></mrow>
    <mrow><mi>P</mi><mo>(</mo><mtext>阳性</mtext><mo>)</mo></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mrow><mo>(</mo> <mtext>如果阳性则存在风险</mtext> <mo>)</mo></mrow>
    <mo>=</mo> <mfrac><mrow><mn>.99</mn><mo>×</mo><mn>.01</mn></mrow> <mrow><mn>.248</mn></mrow></mfrac></mrow></math><math
    display="block"><mrow><mi>P</mi> <mo>(</mo> <mtext>如果阳性则存在风险</mtext> <mo>)</mo>
    <mo>=</mo> <mn>.0339</mn></mrow></math>
- en: When we account for the fact that only 1% of the population is at risk, and
    20% of our test patients are at risk, the probability of being at risk given a
    positive test is 3.39%! How did it drop from 99%? This just shows how easily we
    can get duped by probabilities that are high only in a specific sample like the
    vendor’s one thousand test patients. So if this test has only a 3.39% probability
    of successfully identifying a true positive, we probably should not use it.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们考虑到只有1%的人口存在风险，并且我们测试的患者中有20%存在风险时，接受阳性测试的人存在风险的概率为3.39%！为什么从99%下降到了这个数字？这只是展示了我们如何容易被高概率欺骗，这种概率只在特定样本中高，比如供应商的一千名测试患者。因此，如果这个测试只有3.39%的概率成功识别真正的阳性，我们可能不应该使用它。
- en: Receiver Operator Characteristics/Area Under Curve
  id: totrans-245
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 接收器操作特性/曲线下面积
- en: When we are evaluating different machine learning configurations, we may end
    up with dozens, hundreds, or thousands of confusion matrices. These can be tedious
    to review, so we can summarize all of them with a *receiver operator characteristic
    (ROC) curve* as shown in [Figure 6-20](#QCgoMOWuNR). This allows us to see each
    testing instance (each represented by a black dot) and find an agreeable balance
    between true positives and false positives.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们评估不同的机器学习配置时，可能会得到数十、数百或数千个混淆矩阵。这些可能很繁琐，因此我们可以用一个*接收器操作特性（ROC）曲线*来总结所有这些，如[图
    6-20](#QCgoMOWuNR)所示。这使我们能够看到每个测试实例（每个由一个黑点表示）并找到真正例和假正例之间的一个令人满意的平衡。
- en: We can also compare different machine learning models by creating separate ROC
    curves for each. For example, if in [Figure 6-21](#VNjNDUrmFa) our top curve represents
    a logistic regression and the bottom curve represents a decision tree (a machine
    learning technique we did not cover in this book), we can see the performance
    of them side by side. The *area under the curve (AUC)* is a good metric for choosing
    which model to use. Since the top curve (logistic regression) has a greater area,
    this suggests it is a superior model.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过为每个模型创建单独的 ROC 曲线来比较不同的机器学习模型。例如，如果在[图 6-21](#VNjNDUrmFa)中，我们的顶部曲线代表逻辑回归，底部曲线代表决策树（这是本书中没有涵盖的一种机器学习技术），我们可以并排看到它们的性能。*曲线下面积（AUC）*是选择使用哪个模型的良好指标。由于顶部曲线（逻辑回归）的面积更大，这表明它是一个更优秀的模型。
- en: '![emds 0620](Images/emds_0620.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0620](Images/emds_0620.png)'
- en: Figure 6-20\. A receiver operator characteristic curve
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-20\. 接收器操作特性曲线
- en: '![emds 0621](Images/emds_0621.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![emds 0621](Images/emds_0621.png)'
- en: Figure 6-21\. Comparing two models by their area under the curve (AUC) with
    their respective ROC curves
  id: totrans-251
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-21\. 通过它们各自的 ROC 曲线比较两个模型的曲线下面积（AUC）
- en: To use the AUC as a scoring metric, change the `scoring` parameter in the scikit-learn
    API to use `roc_auc` as shown for a cross-validation in [Example 6-18](#OIHKJooDId).
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要将 AUC 作为评分指标使用，请在 scikit-learn API 中将`scoring`参数更改为使用`roc_auc`，如在[示例 6-18](#OIHKJooDId)中所示进行交叉验证。
- en: Example 6-18\. Using the AUC as the scikit-learn parameter
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-18\. 使用 AUC 作为 scikit-learn 参数
- en: '[PRE17]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Class Imbalance
  id: totrans-255
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 类别不平衡
- en: There is one last thing to cover before we close this chapter. As we saw earlier
    when discussing confusion matrices, *class imbalance*, which happens when data
    is not equally represented across every outcome class, is a problem in machine
    learning. Unfortunately, many problems of interest are imbalanced, such as disease
    prediction, security breaches, fraud detection, and so on. Class imbalance is
    still an open problem with no great solution. However, there are a few techniques
    you can try.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们结束本章之前，还有一件事情需要讨论。正如我们之前在讨论混淆矩阵时看到的那样，*类别不平衡*，即当数据在每个结果类别中没有得到平等表示时，是机器学习中的一个问题。不幸的是，许多感兴趣的问题都存在不平衡，比如疾病预测、安全漏洞、欺诈检测等等。类别不平衡仍然是一个没有很好解决方案的问题。然而，你可以尝试一些技术。
- en: First, you can do obvious things like collect more data or try different models
    as well as use confusion matrices and ROC/AUC curves. All of this will help track
    poor predictions and proactively catch errors.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，你可以做一些明显的事情，比如收集更多数据或尝试不同的模型，以及使用混淆矩阵和 ROC/AUC 曲线。所有这些都将有助于跟踪糟糕的预测并主动捕捉错误。
- en: Another common technique is to duplicate samples in the minority class until
    it is equally represented in the dataset. You can do this in scikit-learn as shown
    in [Example 6-19](#NUGnKQArce) when doing your train-test splits. Pass the `stratify`
    option with the column containing the class values, and it will attempt to equally
    represent the data of each class.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的技术是复制少数类别中的样本，直到它在数据集中得到平等的表示。你可以在 scikit-learn 中这样做，如[示例 6-19](#NUGnKQArce)所示，在进行训练-测试拆分时传递包含类别值的列的`stratify`选项，它将尝试平等地表示每个类别的数据。
- en: Example 6-19\. Using the `stratify` option in scikit-learn to balance classes
    in the data
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-19\. 在 scikit-learn 中使用`stratify`选项来平衡数据中的类别
- en: '[PRE18]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: There is also a family of algorithms called SMOTE, which generate synthetic
    samples of the minority class. What would be most ideal though is to tackle the
    problem in a way that uses anomaly-detection models, which are deliberately designed
    for seeking out a rare event. These seek outliers, however, and are not necessarily
    a classification since they are unsupervised algorithms. All these techniques
    are beyond the scope of this book but are worth mentioning as they *might* provide
    better solutions to a given problem.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一类算法称为SMOTE，它会生成少数类的合成样本。然而，最理想的方法是以一种利用异常检测模型的方式解决问题，这些模型专门设计用于寻找罕见事件。它们寻找异常值，但不一定是分类，因为它们是无监督算法。所有这些技术超出了本书的范围，但值得一提，因为它们*可能*为给定问题提供更好的解决方案。
- en: Conclusion
  id: totrans-262
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Logistic regression is the workhorse model for predicting probabilities and
    classifications on data. Logistic regressions can predict more than one category
    rather than just a true/false. You just build separate logistic regressions modeling
    whether or not it belongs to that category, and the model that produces the highest
    probability is the one that wins. You may discover that scikit-learn, for the
    most part, will do this for you and detect when your data has more than two classes.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是预测数据概率和分类的主力模型。逻辑回归可以预测不止一个类别，而不仅仅是真/假。你只需构建单独的逻辑回归模型，模拟它是否属于该类别，产生最高概率的模型即为胜者。你可能会发现，大部分情况下，scikit-learn会为你完成这一点，并在数据具有两个以上类别时进行检测。
- en: In this chapter, we covered not just how to fit a logistic regression using
    gradient descent and scikit-learn but also statistical and machine learning approaches
    to validation. On the statistical front we covered the <math alttext="upper R
    squared"><msup><mi>R</mi> <mn>2</mn></msup></math> and p-value, and in machine
    learning we explored train/test splits, confusion matrices, and ROC/AUC.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们不仅介绍了如何使用梯度下降和scikit-learn拟合逻辑回归，还涉及了统计学和机器学习方法的验证。在统计学方面，我们涵盖了<math
    alttext="上限R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>和p值，而在机器学习方面，我们探讨了训练/测试拆分、混淆矩阵和ROC/AUC。
- en: If you want to learn more about logistic regression, probably the best resource
    to jump-start further is Josh Starmer’s StatQuest playlist on Logistic Regression.
    I have to credit Josh’s work in assisting some portions of this chapter, particularly
    in how to calculate <math alttext="upper R squared"><msup><mi>R</mi> <mn>2</mn></msup></math>
    and p-values for logistic regression. If nothing else, watch his videos for the
    [fantastic opening jingles](https://oreil.ly/tueJJ)!
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想了解更多关于逻辑回归的知识，可能最好的资源是Josh Starmer的StatQuest播放列表关于逻辑回归的视频。我必须感谢Josh在协助本章某些部分方面的工作，特别是如何计算逻辑回归的<math
    alttext="上限R平方"><msup><mi>R</mi> <mn>2</mn></msup></math>和p值。如果没有其他，观看他的视频也是为了那些[出色的开场曲](https://oreil.ly/tueJJ)！
- en: As always, you will find yourself walking between the two worlds of statistics
    and machine learning. Many books and resources right now cover logistic regression
    from a machine learning perspective, but try to seek out statistics resources
    too. There are advantages and disadvantages to both schools of thought, and you
    can win only by being adaptable to both!
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一如既往，你将发现自己在统计学和机器学习两个世界之间徘徊。目前许多书籍和资源都从机器学习的角度讨论逻辑回归，但也尝试寻找统计学资源。两种思维方式各有优劣，只有适应两者才能取得胜利！
- en: Exercises
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 练习
- en: A dataset of three input variables `RED`, `GREEN`, and `BLUE` as well as an
    output variable `LIGHT_OR_DARK_FONT_IND` is provided [here](https://bit.ly/3imidqa).
    It will be used to predict whether a light/dark font (0/1 respectively) will work
    for a given background color (specified by RGB values).
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 提供了一个包含三个输入变量`RED`、`GREEN`和`BLUE`以及一个输出变量`LIGHT_OR_DARK_FONT_IND`的数据集，链接在[这里](https://bit.ly/3imidqa)。这将用于预测给定背景颜色（由RGB值指定）是否适合使用浅色/深色字体（分别为0/1）。
- en: Perform a logistic regression on the preceding data, using three-fold cross-validation
    and accuracy as your metric.
  id: totrans-269
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对上述数据执行逻辑回归，使用三折交叉验证和准确率作为评估指标。
- en: Produce a confusion matrix comparing the predictions and actual data.
  id: totrans-270
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 生成一个混淆矩阵，比较预测和实际数据。
- en: Pick a few different background colors (you can use an RGB tool like [this one](https://bit.ly/3FHywrZ))
    and see if the logistic regression sensibly chooses a light (0) or dark (1) font
    for each one.
  id: totrans-271
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择几种不同的背景颜色（你可以使用像[这个](https://bit.ly/3FHywrZ)这样的RGB工具），看看逻辑回归是否明智地为每种颜色选择浅色（0）或深色（1）字体。
- en: Based on the preceding exercises, do you think logistic regression is effective
    for predicting a light or dark font for a given background color?
  id: totrans-272
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据前面的练习，你认为逻辑回归对于预测给定背景颜色的浅色或深色字体有效吗？
- en: Answers are in [Appendix B](app02.xhtml#exercise_answers).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 答案在[附录 B](app02.xhtml#exercise_answers)中。
