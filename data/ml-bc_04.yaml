- en: 4 Evaluation metrics for classification
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 分类评估指标
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Accuracy as a way of evaluating binary classification models and its limitations
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率作为评估二元分类模型及其局限性的方式
- en: Determining where our model makes mistakes using a confusion table
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用混淆矩阵确定模型出错的地方
- en: Deriving other metrics like precision and recall from the confusion table
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从混淆矩阵推导其他指标，如精确率和召回率
- en: Using ROC (receiver operating characteristics) and AUC (area under the ROC curve)
    to further understand the performance of a binary classification model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用ROC（接收者操作特征）和AUC（ROC曲线下的面积）来进一步了解二元分类模型的表现
- en: Cross-validating a model to make sure it behaves optimally
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型进行交叉验证以确保其表现最优
- en: Tuning the parameters of a model to achieve the best predictive performance
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整模型参数以实现最佳的预测性能
- en: 'In this chapter, we continue with the project we started in the previous chapter:
    churn prediction. We have already downloaded the dataset, performed the initial
    preprocessing and exploratory data analysis, and trained the model that predicts
    whether customers will churn. We have also evaluated this model on the validation
    dataset and concluded that it has 80% accuracy.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续上一章开始的项目：客户流失预测。我们已经下载了数据集，完成了初步的预处理和探索性数据分析，并训练了预测客户是否会流失的模型。我们还在验证数据集上评估了该模型，并得出结论，其准确率为80%。
- en: 'The question we postponed until now was whether 80% accuracy is good and what
    it actually means in terms of the quality of our model. We answer this question
    in this chapter and discuss other ways of evaluating a binary classification model:
    the confusion table, precision and recall, the ROC curve, and AUC.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直推迟到现在的疑问是80%的准确率是否良好，以及它在模型质量方面的实际意义。我们将在本章回答这个问题，并讨论评估二元分类模型的其它方法：混淆矩阵、精确率、召回率、ROC曲线和AUC。
- en: 'This chapter provides a lot of complex information, but the evaluation metrics
    we cover here are essential for doing practical machine learning. Don’t worry
    if you don’t immediately understand all the details of the different evaluation
    metrics: it requires time and practice. Feel free to come back to this chapter
    to revisit the finer points.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章提供了大量复杂信息，但我们在这里讨论的评估指标对于实际机器学习至关重要。如果你不立即理解不同评估指标的细节，请不要担心：这需要时间和实践。随时可以回到本章，重新审视细节。
- en: 4.1 Evaluation metrics
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 评估指标
- en: We have already built a binary classification model for predicting churning
    customers. Now we need to be able to determine how good it is.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经为预测流失客户构建了一个二元分类模型。现在我们需要能够确定它的好坏。
- en: 'For this, we use a *metric*—a function that looks at the predictions the model
    makes and compares them with the actual values. Then, based on the comparison,
    it calculates how good the model is. This is quite useful: we can use it to compare
    different models and select the one with the best metric value.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们使用了一个*指标*——一个查看模型做出的预测并将其与实际值进行比较的函数。然后，基于比较结果，它计算出模型的好坏。这非常有用：我们可以用它来比较不同的模型，并选择具有最佳指标值的模型。
- en: There are different kinds of metrics. In chapter 2, we used RMSE (root mean
    squared error) to evaluate regression models. However, this metric can be used
    only for regression models and doesn’t work for classification.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 存在着不同类型的指标。在第2章中，我们使用了RMSE（均方根误差）来评估回归模型。然而，这个指标只能用于回归模型，不适用于分类。
- en: For evaluating classification models, we have other more suitable metrics. In
    this section, we cover the most common evaluation metrics for binary classification,
    starting with accuracy, which we already saw in chapter 3.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 对于评估分类模型，我们有其他更合适的指标。在本节中，我们将介绍二元分类最常见的评估指标，从我们在第3章中看到的准确率开始。
- en: 4.1.1 Classification accuracy
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 分类准确率
- en: As you probably remember, the accuracy of a binary classification model is the
    percentage of correct predictions it makes (figure 4.1).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所记得的，二元分类模型的准确率是指它做出正确预测的百分比（图4.1）。
- en: '![](../Images/04-01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-01.png)'
- en: Figure 4.1 The accuracy of a model is the fraction of predictions that turned
    out to be correct.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 模型的准确率是正确预测的预测比例。
- en: 'This accuracy is the simplest way to evaluate a classifier: by counting the
    number of cases in which our model turned out to be right, we can learn a lot
    about the model’s behavior and quality.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这种准确率是评估分类器的最简单方法：通过计算我们的模型正确的情况数量，我们可以了解很多关于模型行为和质量的信息。
- en: 'Computing accuracy on the validation dataset is easy—we simply calculate the
    fraction of correct predictions:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在验证数据集上计算准确率很简单——我们只需计算正确预测的比例：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Gets the predictions from the model
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从模型获取预测值
- en: ❷ Makes "hard" predictions
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 进行“硬”预测
- en: ❸ Computes the accuracy
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算准确率
- en: We first apply the model to the validation set to get the predictions in ❶.
    These predictions are probabilities, so we cut them at 0.5 in ❷. Finally, we calculate
    the fraction of predictions that matched reality in ❷.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将模型应用于验证集以获取预测值 ❶。这些预测值是概率，所以我们将其在 ❷ 处截断为 0.5。最后，我们计算在 ❷ 中与实际情况匹配的预测比例。
- en: The result is 0.8016, which means that our model is 80% accurate.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是 0.8016，这意味着我们的模型准确率为 80%。
- en: 'The first thing we should ask ourselves is why we chose 0.5 as the threshold
    and not any other number. That was an arbitrary choice, but it’s actually not
    difficult to check other thresholds as well: we can just loop over all possible
    threshold candidates and compute the accuracy for each. Then we can choose the
    one with the best accuracy score.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先应该问自己为什么选择 0.5 作为阈值而不是其他数字。这是一个任意的选择，但实际上检查其他阈值也不难：我们只需遍历所有可能的阈值候选者并计算每个的准确率。然后我们可以选择具有最佳准确率分数的那个。
- en: Even though it’s easy to implement accuracy ourselves, we can use existing implementations
    as well. The Scikit-learn library offers a variety of metrics, including accuracy
    and many others that we will use later. You can find these metrics in the metrics
    package.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们可以轻松实现自己的准确率，但我们也可以使用现有的实现。Scikit-learn 库提供了各种指标，包括准确率以及我们稍后将要使用的许多其他指标。你可以在
    metrics 包中找到这些指标。
- en: 'We’ll continue working on the same notebook that we started in chapter 3\.
    Let’s open it and add the `import` statement to import accuracy from Scikit-learn’s
    metrics package:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续在第三章开始的工作笔记本上工作。让我们打开它，并添加 `import` 语句以从 Scikit-learn 的 metrics 包中导入准确率：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now we can loop over different thresholds and check which one gives the best
    accuracy:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以遍历不同的阈值并检查哪个阈值给出了最佳的准确率：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '❶ Creates an array with different thresholds: 0.0, 0.1, 0.2, and so on'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个包含不同阈值的数组：0.0、0.1、0.2 等
- en: ❷ Loops over each threshold value
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历每个阈值值
- en: ❸ Uses the accuracy_score function from Scikit-learn for computing accuracy
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用 Scikit-learn 的 accuracy_score 函数计算准确率
- en: ❹ Prints the thresholds and the accuracy values to standard output
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将阈值和准确率值打印到标准输出
- en: 'In this code, we first create an array with thresholds. We use the `linspace`
    function from NumPy for that: it takes two numbers (0 and 1, in our case) and
    the number of elements the array should have (11). As a result, we get an array
    with the numbers 0.0, 0.1, 0.2, ..., 1.0\. You can learn more about `linspace`
    and other NumPy functions in appendix C.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们首先创建一个包含阈值的数组。我们使用 NumPy 的 `linspace` 函数来做到这一点：它接受两个数字（在我们的情况下是 0 和
    1）和数组应具有的元素数量（11）。结果，我们得到一个包含数字 0.0、0.1、0.2、...、1.0 的数组。你可以在附录 C 中了解更多关于 `linspace`
    和其他 NumPy 函数的信息。
- en: 'We use these numbers as thresholds: we loop over them, and for each one, we
    calculate the accuracy. Finally, we print the thresholds and the accuracy scores
    so we can decide which threshold is the best.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些数字用作阈值：我们遍历它们，并对每个值计算准确率。最后，我们打印出阈值和准确率分数，以便我们可以决定哪个阈值是最好的。
- en: 'When we execute the code, it prints the following:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行代码时，它将打印以下内容：
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As we see, using the threshold of 0.5 gives us the best accuracy. Typically,
    0.5 is a good threshold value to start with, but we should always try other threshold
    values to make sure 0.5 is the best choice.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，使用 0.5 的阈值给出了最佳的准确率。通常，0.5 是一个很好的起始阈值，但我们应该尝试其他阈值以确保 0.5 是最佳选择。
- en: 'To make it more visual, we can use Matplotlib to create a plot that shows how
    accuracy changes depending on the threshold. We repeat the same process as previously,
    but instead of just printing the accuracy scores, we first put the values to a
    list:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更直观，我们可以使用 Matplotlib 创建一个图表，显示准确率如何根据阈值变化。我们重复之前的过程，但这次不是只打印准确率分数，我们首先将这些值放入一个列表中：
- en: '[PRE4]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Creates different threshold values (this time 21 instead of 11)
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建不同的阈值值（这次是 21 而不是 11）
- en: ❷ Creates an empty list to hold the accuracy values
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个空列表来存储准确率值
- en: ❸ Calculates the accuracy for a given threshold
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算给定阈值下的准确率
- en: ❹ Records the accuracy for this threshold
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 记录此阈值的准确率
- en: 'And then we plot these values using Matplotlib:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用 Matplotlib 绘制这些值：
- en: '[PRE5]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After executing this line, we should see a plot that shows the relationship
    between the threshold and the accuracy (figure 4.2). As we already know, the 0.5
    threshold is the best in terms of accuracy.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 执行这一行后，我们应该看到一个显示阈值与准确率之间关系的图表（图4.2）。正如我们已经知道的，0.5的阈值在准确率方面是最好的。
- en: '![](../Images/04-02.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-02.png)'
- en: 'Figure 4.2 Accuracy of our model evaluated at different thresholds. The best
    accuracy is achieved when cutting the predictions at the 0.5 threshold: if a prediction
    is higher than 0.5, we predict “churn,” and otherwise, we predict “no churn.”'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 在不同阈值下评估的模型准确率。在将预测切分在0.5阈值时，我们达到了最佳准确率：如果预测高于0.5，我们预测“流失”，否则我们预测“无流失”。
- en: So, the best threshold is 0.5, and the best accuracy for this model that we
    can achieve is 80%.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最佳阈值是0.5，我们可以达到的该模型的最佳准确率是80%。
- en: 'In the previous chapter, we trained a simpler model: we called it model_small.
    It was based on only three variables: `contract`, `tenure`, and `totalcharges`.'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们训练了一个更简单的模型：我们称之为model_small。它基于仅三个变量：`contract`、`tenure`和`totalcharges`。
- en: 'Let’s also check its accuracy. For that, we first make predictions on the validation
    dataset and then compute the accuracy score:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也检查一下它的准确率。为此，我们首先在验证数据集上做出预测，然后计算准确率：
- en: '[PRE6]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Applies one-hot encoding to the validation data
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对验证数据进行one-hot编码
- en: ❷ Predicts churn using the small model
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用小型模型预测流失
- en: ❸ Calculates the accuracy of the predictions
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算预测的准确率
- en: When we run this code, we see that the accuracy of the small model is 76%. So,
    the large model is actually 4% more accurate than the small model.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，我们看到小型模型的准确率是76%。因此，大型模型实际上比小型模型准确4%。
- en: However, this still doesn’t tell us whether 80% (or 76%) is a good accuracy
    score.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仍然没有告诉我们80%（或76%）是否是一个好的准确率。
- en: 4.1.2 Dummy baseline
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 虚拟基线
- en: Although it seems like a decent number, to understand whether 80% is actually
    good, we need to relate it to something—for example, a simple baseline that’s
    easy to understand. One such baseline could be a dummy model that always predicts
    the same value.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这个数字看起来还不错，但要了解80%是否真的很好，我们需要将其与某些东西联系起来——例如，一个简单易懂的基线。这样一个基线可以是总是预测相同值的虚拟模型。
- en: In our example, the dataset is imbalanced, and we don’t have many churned users.
    So, the dummy model can always predict the majority class—“no churn.” In other
    words, this model will always output False, regardless of the features. This is
    not a super useful model, but we can use it as a baseline and compare it with
    the other two models.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，数据集是不平衡的，我们没有很多流失用户。因此，虚拟模型可以总是预测大多数类——“无流失”。换句话说，这个模型将始终输出False，无论特征如何。这不是一个特别有用的模型，但我们可以将其用作基线，并与其他两个模型进行比较。
- en: 'Let’s create this baseline prediction:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建这个基线预测：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Gets the number of customers in the validation set
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取验证集中的客户数量
- en: ❷ Creates an array with only False elements
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个只包含False元素的数组
- en: To create an array with the baseline predictions, we first need to determine
    how many elements are in the validation set.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个包含基线预测的数组，我们首先需要确定验证集中有多少个元素。
- en: 'Next, we create an array of dummy predictions—all the elements of this array
    are False values. We do this using the `repeat` function from NumPy: it takes
    in an element and repeats it as many times as we ask. For more details about the
    `repeat` function and other NumPy functions, please refer to appendix C.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个包含虚拟预测的数组——这个数组的所有元素都是False值。我们使用NumPy的`repeat`函数来完成这个操作：它接受一个元素，并按照我们要求重复它多次。有关`repeat`函数和其他NumPy函数的更多详细信息，请参阅附录C。
- en: 'Now we can check the accuracy of this baseline prediction using the same code
    as we used previously:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用之前相同的代码来检查这个基线预测的准确率：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When we run this code, it shows 0.738\. This means that the accuracy of the
    baseline model is around 74% (figure 4.3).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行这段代码时，它显示0.738。这意味着基线模型的准确率大约是74%（图4.3）。
- en: '![](../Images/04-03.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-03.png)'
- en: Figure 4.3 The baseline is a “model” that always predicts the same value for
    all the customers. The accuracy of this baseline is 74%.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 基线是一个总是为所有客户预测相同值的“模型”。这个基线的准确率是74%。
- en: As we see, the small model is only 2% better than the naive baseline, and the
    large one is 6% better. If we think about all the trouble we have gone through
    to train this large model, 6% doesn’t seem like a significant improvement over
    the dummy baseline.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，小型模型仅比朴素基线好2%，而大型模型好6%。如果我们考虑我们为训练这个大型模型所付出的所有努力，6%似乎并不比虚拟基线有显著改进。
- en: Churn prediction is a complex problem, and maybe this improvement is great.
    However, that’s not evident from the accuracy score alone. According to accuracy,
    our model is only slightly better than a dummy model that treats all the customers
    as non-churning and doesn’t attempt to keep any of them.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 流失预测是一个复杂的问题，也许这个改进是很大的。然而，仅从准确率分数来看，这一点并不明显。根据准确率，我们的模型仅略优于一个将所有客户视为非流失且不尝试保留任何客户的虚拟模型。
- en: Thus, we need other metrics—other ways of measuring the quality of our model.
    These metrics are based on the confusion table, the concept that we cover in the
    next section.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们需要其他指标——其他衡量我们模型质量的方法。这些指标基于混淆矩阵，我们将在下一节中介绍的概念。
- en: 4.2 Confusion table
  id: totrans-80
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 混淆矩阵
- en: 'Even though accuracy is easy to understand, it’s not always the best metric.
    In fact, it sometimes can be misleading. We’ve already seen this occur: the accuracy
    of our model is 80%, and although that seems like a good number, it’s just 6%
    better than the accuracy of a dummy model that always outputs the same prediction
    of “no churn.”'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管准确率容易理解，但它并不总是最好的指标。事实上，有时它可能会误导。我们已经看到这种情况发生了：我们模型的准确率是80%，尽管这个数字看起来不错，但它仅比总是输出相同预测“无流失”的虚拟模型的准确率高6%。
- en: 'This situation typically happens when we have a class imbalance (more instances
    of one class than another). We know that this is definitely the case for our problem:
    74% of customers did not churn, and only 26% did churn.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况通常发生在我们有一个类别不平衡（一个类别的实例比另一个类别多）的情况下。我们知道，对于我们的问题，这绝对是一个案例：74%的客户没有流失，而只有26%的客户流失了。
- en: 'For such cases, we need a different way of measuring the quality of our models.
    We have a few options, and most of them are based on the confusion table: a table
    that concisely represents every possible outcome for our model’s predictions.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这类情况，我们需要一种不同的方法来衡量我们模型的质量。我们有几种选择，其中大多数都是基于混淆矩阵：一个简洁地表示我们模型预测所有可能结果的表格。
- en: 4.2.1 Introduction to the confusion table
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 混淆矩阵简介
- en: 'We know that for a binary classification model, we can have only two possible
    predictions: True and False. In our case, we can predict that a customer is either
    going to churn (True) or not (False).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道，对于二元分类模型，我们只能有两种可能的预测：真和假。在我们的情况下，我们可以预测一个客户是否会流失（真）或不会流失（假）。
- en: 'When we apply the model to the entire validation dataset with customers, we
    split it into two parts (figure 4.4):'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将模型应用于包含客户的整个验证数据集时，我们将它分为两部分（图4.4）：
- en: Customers for whom the model predicts “churn”
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测为“流失”的客户
- en: Customers for whom the model predicts “no churn”
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型预测为“无流失”的客户
- en: '![](../Images/04-04.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-04.png)'
- en: 'Figure 4.4 Our model splits all the customers in the validation dataset into
    two groups: customers who we think will churn and customers who will not.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 我们的模型将验证数据集中的所有客户分为两组：我们认为会流失的客户和不会流失的客户。
- en: 'Only two possible correct outcomes can occur: again, True or False. A customer
    has either actually churned (True) or not (False).'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 只有两种可能的正确结果：再次，真或假。客户要么实际上流失了（真），要么没有（假）。
- en: 'This means that by using the ground truth information—the information about
    the target variable—we can again split the dataset into two parts (figure 4.5):'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着，通过使用真实信息——关于目标变量的信息——我们再次可以将数据集分为两部分（图4.5）：
- en: The customers who churned
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流失的客户
- en: The customers who didn’t churn
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有流失的客户
- en: '![](../Images/04-05.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-05.png)'
- en: 'Figure 4.5 Using the ground truth data, we can split the validation dataset
    into two groups: customers who actually churned and customers who didn’t.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 使用真实数据，我们可以将验证数据集分为两组：实际流失的客户和没有流失的客户。
- en: 'When we make a prediction, it will either turn out to be correct or not:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们做出预测时，它要么是正确的，要么是错误的：
- en: If we predict “churn,” the customer may indeed churn, or they may not.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们预测“流失”，客户确实可能会流失，也可能不会。
- en: If we predict “no churn,” it’s possible that the customer indeed doesn’t churn,
    but it’s also possible that they do churn.
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们预测“无流失”，那么客户确实可能没有流失，但也可能流失。
- en: 'This gives us four possible outcomes (figure 4.6):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了四种可能的结果（图4.6）：
- en: We predict False, and the answer is False.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测为假，答案是假。
- en: We predict False, and the answer is True.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测为假，但答案是真。
- en: We predict True, and the answer is False.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测为真，但答案是假。
- en: We predict True, and the answer is True.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们预测为真，答案是真。
- en: '![](../Images/04-06.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-06.png)'
- en: 'Figure 4.6 There are four possible outcomes: we predict “churn,” and the customers
    either churn or do not, and we predict “no churn,” and the customers again either
    churn or do not.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 有四种可能的结果：我们预测“churn”，客户要么流失要么没有，我们预测“no churn”，客户再次要么流失要么没有。
- en: 'Two of these situations—the first and last ones—are good: the prediction matched
    the actual value. The two remaining ones are bad: we didn’t make a correct prediction.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种情况——第一种和最后一种——是好的：预测与实际值匹配。剩下的两种情况是坏的：我们没有做出正确的预测。
- en: 'Each of these four situations has its own name (figure 4.7):'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 每种这四种情况都有其自己的名称（图4.7）：
- en: 'True negative (TN): we predict False (“no churn”), and the actual label is
    also False (“no churn”).'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阴性（TN）：我们预测为假（“no churn”），而实际标签也是假（“no churn”）。
- en: 'True positive (TP): we predict True (“churn”), and the actual label is True
    (“churn”).'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性（TP）：我们预测为真（“churn”），而实际标签也是真（“churn”）。
- en: 'False negative (FN): we predict False (“no churn”), but it’s actually True
    (the customer churned).'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阴性（FN）：我们预测为假（“no churn”），但实际上是真（客户流失了）。
- en: 'False positive (FP): we predict True (“churn”), but it’s actually False (the
    customer stayed with us).'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性（FP）：我们预测为真（“churn”），但实际上是假（客户留在了我们这里）。
- en: '![](../Images/04-07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-07.png)'
- en: 'Figure 4.7 Each of the four possible outcomes has its own name: true negative,
    false negative, false positive, and true positive.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 每种可能的四种结果都有其自己的名称：真阴性、假阴性、假阳性和真阳性。
- en: It’s visually helpful to arrange these outcomes in a table. We can put the predicted
    classes (False and True) in the columns and the actual classes (False and True)
    in the rows (figure 4.8).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些结果以表格形式排列是直观的。我们可以将预测类别（假和真）放在列中，将实际类别（假和真）放在行中（图4.8）。
- en: '![](../Images/04-08.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-08.png)'
- en: 'Figure 4.8 We can organize the outcomes in a table—the predicted values as
    columns and the actual values as rows. This way, we break down all prediction
    scenarios into four distinct groups: TN (true negative), TP (true positive), FN
    (false negative), and FP (false positive).'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 我们可以将结果组织成表格——预测值作为列，实际值作为行。这样，我们将所有预测场景分解为四个不同的组：TN（真阴性）、TP（真阳性）、FN（假阴性）和FP（假阳性）。
- en: When we substitute the number of times each outcome happens, we get the confusion
    table for our model (figure 4.9).
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们替换每种结果发生的次数时，我们得到我们模型的混淆矩阵（图4.9）。
- en: '![](../Images/04-09.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-09.png)'
- en: Figure 4.9 In the confusion table, each cell contains the number of times each
    outcome happens.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 在混淆矩阵中，每个单元格包含每种结果发生的次数。
- en: Calculating the values in the cells of the confusion matrix is quite easy with
    NumPy. Next, we see how to do it.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 使用NumPy计算混淆矩阵单元格中的值相当简单。接下来，我们将看到如何做到这一点。
- en: 4.2.2 Calculating the confusion table with NumPy
  id: totrans-122
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 使用NumPy计算混淆矩阵
- en: To help us understand our confusion table better, we can visually depict what
    it does to the validation dataset (figure 4.10).
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解我们的混淆矩阵，我们可以直观地展示它对验证数据集的影响（图4.10）。
- en: '![](../Images/04-10.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-10.png)'
- en: Figure 4.10 When we apply the model to the validation dataset, we get four different
    outcomes (TN, FP, TP, and FN).
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 当我们将模型应用于验证数据集时，我们得到四种不同的结果（TN、FP、TP和FN）。
- en: 'To calculate the confusion table, we need to do these steps:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算混淆矩阵，我们需要执行以下步骤：
- en: 'First, the predictions split the dataset into two parts: the part for which
    we predict True (“churn”) and the part for which we predict False (“no churn”).'
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，预测将数据集分为两部分：我们预测为真（“churn”）的部分和预测为假（“no churn”）的部分。
- en: 'At the same time, the target variable splits this dataset into two different
    parts: the customers who actually churned (“1” in `y_val`) and the customers who
    didn’t (“0” in `y_val`).'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同时，目标变量将这个数据集分为两个不同的部分：实际流失的客户（`y_val`中的“1”）和没有流失的客户（`y_val`中的“0”）。
- en: When we combine these splits, we get four groups of customers, which are exactly
    the four different outcomes from the confusion table.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们将这些分割组合起来时，我们得到四组客户，这正好是混淆矩阵中的四种不同结果。
- en: 'Translating these steps to NumPy is straightforward:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些步骤转换为NumPy是直接的：
- en: '[PRE9]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Makes predictions at threshold 0.5
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在阈值0.5处进行预测
- en: ❷ Gets the actual target values
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取实际的目标值
- en: ❸ Calculates true positives (cases when we predicted churn correctly)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算真正结果（我们正确预测了流失的情况）
- en: ❹ Calculates false positives (cases when we predicted churn, but the customers
    didn't churn)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算假阳性（我们预测了流失，但客户没有流失的情况）
- en: ❺ Calculates true negatives (cases when we predicted no churn correctly)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算真阴性（我们正确预测了没有流失的情况）
- en: ❻ Calculates false negatives (cases when we predicted no churn, but the customers
    churned)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 计算假阴性（我们预测了没有流失，但客户流失了的情况）
- en: We begin by making predictions at the threshold of 0.5.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从在0.5的阈值上进行预测开始。
- en: 'The results are two NumPy arrays:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是两个NumPy数组：
- en: In the first array (`predict_churn`), an element is True if the model thinks
    the respective customer is going to churn and False otherwise.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第一个数组（`predict_churn`）中，如果一个元素为True，表示模型认为相应的客户将要流失，否则为False。
- en: Likewise, in the second array (`predict_no_churn`), True means that the model
    thinks the customer isn’t going to churn.
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 同样，在第二个数组（`predict_no_churn`）中，True表示模型认为客户不会流失。
- en: 'The second array, `predict_no_churn`, is the exact opposite of `predict_churn`:
    if an element is True in `predict_churn`, it’s False in `predict_no_churn` and
    vice versa (figure 4.11). This is the first split of the validation dataset into
    two parts—the one that’s based on the predictions.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个数组`predict_no_churn`是`predict_churn`的完全相反：如果`predict_churn`中的元素为True，则`predict_no_churn`中的元素为False，反之亦然（图4.11）。这是验证数据集第一次分割成两部分——基于预测的分割。
- en: '![](../Images/04-11.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-11.png)'
- en: 'Figure 4.11 Splitting the predictions into two Boolean NumPy arrays: `predict_churn`
    if the probability is higher than 0.5, and `predict_no_churn` if it’s lower'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 将预测分割成两个布尔NumPy数组：`predict_churn`如果概率高于0.5，`predict_no_churn`如果概率低于0.5
- en: 'Next, we record the actual values of the target variable in ❷. The results
    are two NumPy arrays as well (figure 4.12):'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在❷中记录目标变量的实际值。结果也是两个NumPy数组（图4.12）：
- en: If the customer churned (value “1”), then the respective element of `actual_
    churn` is True, and it’s False otherwise.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果客户流失了（值为“1”），那么`actual_churn`中的相应元素为True，否则为False。
- en: 'For `actual_no_churn` it’s exactly the opposite: it’s True when the customer
    didn’t churn.'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`actual_no_churn`，情况正好相反：当客户没有流失时为True。
- en: '![](../Images/04-12.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-12.png)'
- en: 'Figure 4.12 Splitting the array with actual values into two Boolean NumPy arrays:
    `actual_no_churn` if the customer didn’t churn (`y_val` `==` `0`) and `actual_churn`
    if the customer churned (`y_val` `==` `1`)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 将实际值数组分割成两个布尔NumPy数组：`actual_no_churn`如果客户没有流失（`y_val` `==` `0`）和`actual_churn`如果客户流失了（`y_val`
    `==` `1`）
- en: That’s the second split of the dataset—the one that’s based on the target variable.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是数据集的第二次分割——基于目标变量的分割。
- en: Now we combine these two splits—or, to be exact, these four NumPy arrays.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将这两个分割合并——或者更准确地说，这四个NumPy数组。
- en: 'To calculate the number of true positive outcomes in ❸, we use the logical
    “and” operator of NumPy (`&`) and the `sum` method:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算在❸中的真正结果数量，我们使用NumPy的逻辑“与”运算符（`&`）和`sum`方法：
- en: '[PRE10]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The logical “and” operator evaluates to True only if both values are True. If
    at least one is False or both are False, it’s False. In case of `true_positive`,
    it will be True only if we predict “churn” and the customer actually churned (figure 4.13).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑“与”运算符仅在两个值都为True时才返回True。如果至少有一个是False或者两个都是False，则返回False。在`true_positive`的情况下，它只有在预测“流失”并且客户实际上流失时才会是True（图4.13）。
- en: '![](../Images/04-13.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-13.png)'
- en: Figure 4.13 Applying the element-wise and operator (`&`) to two NumPy arrays,
    `predict_churn` and `actual_churn`; this creates another array with True in any
    position where both arrays contained True and False in all others.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 将元素级和运算符（`&`）应用于两个NumPy数组`predict_churn`和`actual_churn`；这创建了一个新的数组，其中在两个数组都包含True的位置为True，在其他所有位置为False。
- en: Then we use the `sum` method from NumPy, which simply counts how many `True`
    values are in the array. It does that by first casting the Boolean array to integers
    and then summing it (figure 4.14). We already saw similar behavior in the previous
    chapter when we used the `mean` method.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用NumPy的`sum`方法，它简单地计算数组中有多少个`True`值。它是通过首先将布尔数组转换为整数，然后求和来做到这一点的（图4.14）。我们在上一章使用`mean`方法时已经看到了类似的行为。
- en: '![](../Images/04-14.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-14.png)'
- en: 'Figure 4.14 Invoking the `sum` method on a Boolean array: we get the number
    of elements in this array that are True.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 在布尔数组上调用`sum`方法：我们得到数组中为True的元素数量。
- en: As a result, we have the number of true positive cases. The other values are
    computed similarly in lines ❹, ❺, and ❻.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了真正正例的数量。其他值在行❹、❺和❻中类似计算。
- en: 'Now we just need to put all these values together in a NumPy array:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们只需要将这些值组合成一个NumPy数组：
- en: '[PRE11]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'When we print it, we get the following numbers:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印出来时，我们得到以下数字：
- en: '[PRE12]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The absolute numbers may be difficult to understand, so we can turn them into
    fractions by dividing each value by the total number of items:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对数字可能难以理解，因此我们可以通过将每个值除以项目总数来将它们转换为分数：
- en: '[PRE13]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This prints the following numbers:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打印出以下数字：
- en: '[PRE14]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'We can summarize the results in a table (table 4.1). We see that the model
    predicts negative values quite well: 65% of the predictions are true negatives.
    However, it makes quite a few mistakes of both types: the number of false positives
    and false negatives is roughly equal (9% and 11%, respectively).'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将结果总结在一张表中（表4.1）。我们看到模型在预测负值方面做得相当好：65%的预测是真正的负例。然而，它犯了很多两种类型的错误：错误正例和错误负例的数量大致相等（分别为9%和11%）。
- en: Table 4.1 The confusion table for the churn classifier at the threshold of 0.5\.
    We see that it’s easy for the model to correctly predict non-churning users, but
    it’s more difficult for it to identify churning users.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 在阈值为0.5的流失分类器的混淆矩阵。我们看到模型很容易正确预测非流失用户，但更难识别流失用户。
- en: '| Full model with all features |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 包含所有特征的完整模型 |'
- en: '|  | Predicted |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '|  | 预测 |'
- en: '|  | False | True |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '|  | 错误 | 正确 |'
- en: '| Actual | False | 1202 (65%) | 172 (9%) |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 实际 | 错误 | 1202 (65%) | 172 (9%) |'
- en: '| True | 197 (11%) | 289 (15%) |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 真实 | 错误 | 197 (11%) | 289 (15%) |'
- en: 'This table gives us a better understanding of the performance of the model—it’s
    now possible to break down the performance into different components and understand
    where the model makes mistakes. We actually see that the performance of the model
    is not great: it makes quite a few errors when trying to identify users that will
    churn. This is something we couldn’t see with the accuracy score alone.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 这个表格让我们更好地理解了模型的性能——现在我们可以将性能分解为不同的组成部分，并了解模型在哪些地方犯了错误。实际上，我们看到模型的性能并不理想：它在尝试识别将要流失的用户时犯了很多错误。这是仅凭准确率分数无法看到的。
- en: We can repeat the same process for the small model using exactly the same code
    (table 4.2).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用完全相同的代码对小型模型重复相同的流程（表4.2）。
- en: Table 4.2 The confusion table for the small model
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.2 小型模型的混淆矩阵
- en: '| Small model with three features |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 包含三个特征的小型模型 |'
- en: '|  | Predicted |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  | 预测 |'
- en: '|  | False | True |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '|  | 错误 | 正确 |'
- en: '| Actual | False | 1189 (63%) | 185 (10%) |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 实际 | 错误 | 1189 (63%) | 185 (10%) |'
- en: '| True | 248 (12%) | 238 (13%) |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 真实 | 错误 | 248 (12%) | 238 (13%) |'
- en: When we compare the smaller model with the full model, we see that it’s 2% worse
    at correctly identifying non-churning users (63% versus 65% for true negatives)
    and 2% worse at correctly identifying churning users (13% versus 15% for true
    positives), which together accounts for the 4% difference between the accuracies
    of these two models (76% versus 80%).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将小型模型与完整模型进行比较时，我们发现它在正确识别非流失用户方面差了2%（真正负例从63%变为65%），在正确识别流失用户方面也差了2%（真正正例从13%变为15%），这两个差异加起来解释了这两个模型准确率之间的4%差异（76%对80%）。
- en: 'The values from the confusion table serve as the basis for many other evaluation
    metrics. For example, we can calculate accuracy by taking all the correct predictions—TN
    and TP together—and dividing that number by the total number of observations in
    all four cells of the table:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵中的值是许多其他评估指标的基础。例如，我们可以通过将所有正确的预测——真正的负例和真正的正例相加——然后除以表中四个单元格中所有观察值的总数来计算准确率：
- en: accuracy = (TN + TP) / (TN + TP + FN + FP)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 = (真正的负例 + 真正的正例) / (真正的负例 + 真正的正例 + 错误的正例 + 错误的负例)
- en: Apart from accuracy, we can calculate other metrics based on the values from
    the confusion table. The most useful ones are precision and recall, which we will
    cover next.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 除了准确率之外，我们还可以根据混淆矩阵中的值计算其他指标。最有用的是精确率和召回率，我们将在下一节中介绍。
- en: Exercise 4.1
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.1
- en: What is a false positive?
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是错误正例？
- en: a) A customer for whom we predicted “not churn,” but they stopped using our
    services
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: a) 我们预测客户“不会流失”，但他们停止使用我们的服务
- en: b) A customer for whom we predicted “churn,” but they didn’t churn
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: b) 我们预测客户会“流失”，但他们没有流失
- en: c) A customer for whom we predicted “churn,” and they churned
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: c) 我们预测客户会“流失”，并且他们确实流失了
- en: 4.2.3 Precision and recall
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 精确率和召回率
- en: 'As already mentioned, accuracy can be misleading when dealing with imbalanced
    datasets such as ours. Other metrics are helpful to use for such cases: precision
    and recall.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在处理像我们这样的不平衡数据集时，准确率可能会误导。在这种情况下，其他指标是有帮助的：精确率和召回率。
- en: Both precision and recall are calculated from the values of the confusion table.
    They both help us understand the quality of the model in cases of class imbalance.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率和召回率都是从混淆矩阵的值计算得出的。它们两者都有助于我们理解模型在类别不平衡情况下的质量。
- en: 'Let’s start with precision. The precision of a model tells us how many of the
    positive predictions turned out to be correct. It’s the fraction of correctly
    predicted positive examples. In our case, it’s the number of customers who actually
    churned (TP) out of all the customers we thought would churn (TP + FP) (figure 4.15):'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从精确率开始。模型的精确率告诉我们有多少正预测是正确的。它是正确预测的正例的比例。在我们的案例中，它是实际流失（TP）的客户数除以我们认为会流失的所有客户数（TP
    + FP）（图4.15）：
- en: P = TP / (TP + FP)
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: P = TP / (TP + FP)
- en: 'For our model, the precision is 62%:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，精确率为62%：
- en: P = 289 / (289 + 172) = 172 / 461 = 0.62
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: P = 289 / (289 + 172) = 172 / 461 = 0.62
- en: '![](../Images/04-15.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-15.png)'
- en: Figure 4.15 The precision of a model is the fraction of correct predictions
    (TP) among all positive predictions (TP + FP).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 模型的精确率是指所有正预测（TP + FP）中正确预测（TP）的比例。
- en: Recall is the fraction of correctly classified positive examples among all positive
    examples. In our case, to calculate recall we first look at all the customers
    who churned and see how many of them we managed to identify correctly.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率是指所有正例中正确分类的正例的比例。在我们的案例中，为了计算召回率，我们首先查看所有流失的客户，看看我们正确识别了多少。
- en: The formula for calculating recall is
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 计算召回率的公式是
- en: R = TP / (TP + FN)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: R = TP / (TP + FN)
- en: 'Like in the formula for precision, the numerator is the number of true positives,
    but the denominator is different: it’s the number of all positive examples (`y_val`
    `==` `1`) in our validation dataset (figure 4.16).'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 就像精确率的公式一样，分子是真正的正例数，但分母不同：它是验证数据集中所有正例（`y_val` `==` `1`）的数量（图4.16）。
- en: '![](../Images/04-16.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-16.png)'
- en: Figure 4.16 The recall of a model is the fraction of correctly predicted churning
    customers (TP) among all customers who churned (TP + FN).
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 模型的召回率是指所有客户中正确预测为流失（TP）的客户占所有流失客户（TP + FN）的比例。
- en: 'For our model, the recall is 59%:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的模型，召回率为59%：
- en: R = 286 / (289 + 197) = 289 / 486 = 0.59
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: R = 286 / (289 + 197) = 289 / 486 = 0.59
- en: 'The difference between precision and recall may seem subtle at first. In both
    cases, we look at the number of correct predictions, but the difference is in
    the denominators (figure 4.17):'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率和召回率之间的差异可能一开始看起来很微妙。在两种情况下，我们都关注正确预测的数量，但差异在于分母（图4.17）：
- en: 'Precision: what’s the percent of correct predictions (TP) among customers predicted
    as churning (TP + FP)?'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确率：在预测为流失的客户中，正确预测（TP）的百分比是多少？
- en: 'Recall: what’s the percentage correctly predicted as churning (TP) among all
    churned customers (TP + FN)?'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率：在所有流失客户（TP + FN）中，正确预测为流失（TP）的百分比是多少？
- en: '![](../Images/04-17.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-17.png)'
- en: Figure 4.17 Both precision and recall look at the correct predictions (TP),
    but the denominators are different. For precision, it’s the number of customers
    predicted as churning, whereas for recall, it’s the number of customers who churned.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 精确率和召回率都关注正确的预测（TP），但分母不同。对于精确率，它是预测为流失的客户数，而对于召回率，它是流失的客户数。
- en: We can also see that both precision and recall don’t take true negatives into
    account (figure 4.17). This is exactly why they are good evaluation metrics for
    imbalanced datasets. For situations with class imbalance, true negatives typically
    outnumber everything else—but at the same time, they are also often not really
    interesting for us. Let’s see why.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以看到，精确率和召回率都没有考虑真正的负例（图4.17）。这正是为什么它们是评估不平衡数据集的良好指标。对于类别不平衡的情况，真正的负例通常比其他所有东西都多——但与此同时，它们通常对我们来说也不是特别有趣。让我们看看原因。
- en: The goal of our project is to identify customers who are likely to churn. Once
    we do, we can send them promotional messages in the hopes that they’ll change
    their mind.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的目标是识别可能流失的客户。一旦我们做到了这一点，我们就可以向他们发送促销信息，希望他们改变主意。
- en: 'When doing this, we make two types of mistakes:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行这项工作时，我们会犯两种类型的错误：
- en: We accidentally send messages to people who weren’t going to churn—these people
    are the false positives of the model.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们意外地向那些本不会流失的人发送了消息——这些人就是模型的假阳性。
- en: We also sometimes fail to identify people who are actually going to churn. We
    don’t send messages to these people—they are our false negatives.
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们有时也未能识别那些实际上会流失的人。我们没有向这些人发送消息——他们是我们的假阴性。
- en: Precision and recall help us quantify these errors.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率帮助我们量化这些错误。
- en: Precision helps us understand how many people received a promotional message
    by mistake. The better the precision, the fewer false positives we have. The precision
    of 62% means that 62% of the reached customers indeed were going to churn (our
    true positives), whereas the remaining 38% were not (false positives).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 精度帮助我们了解有多少人错误地收到了促销信息。精度越好，假阳性就越少。62%的精度意味着62%的触及客户确实会流失（我们的真阳性），而剩余的38%则不会（假阳性）。
- en: Recall helps us understand how many of the churning customers we failed to find.
    The better the recall, the fewer false negatives we have. The recall of 59% means
    that we reach only 59% of all churning users (true positives) and fail to identify
    the remaining 41% (false negatives).
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率帮助我们了解我们未能找到多少流失客户。召回率越好，假阴性就越少。59%的召回率意味着我们只触及了59%的所有流失用户（真阳性），而未能识别剩余的41%（假阴性）。
- en: 'As we can see, in both cases, we don’t really need to know the number of true
    negatives: even though we can correctly identify them as not churning, we aren’t
    going to do anything with them.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，在这两种情况下，我们实际上并不需要知道真阴性的数量：尽管我们可以正确地将它们识别为未流失，但我们不会对它们做任何事情。
- en: 'Although the accuracy of 80% might suggest that the model is great, looking
    at its precision and recall tells us that it actually makes quite a few errors.
    This is typically not a deal-breaker: with machine learning it’s inevitable that
    models make mistakes, and at least now we have a better and more realistic understanding
    of the performance of our churn-prediction model.'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管80%的准确率可能表明模型很棒，但查看其精度和召回率告诉我们，它实际上犯了很多错误。这通常不是决定性的：在机器学习中，模型犯错误是不可避免的，而现在我们至少对我们的流失预测模型性能有了更好的、更现实的了解。
- en: Precision and recall are useful metrics, but they describe the performance of
    a classifier only at a certain threshold. Often it’s useful to have a metric that
    summarizes the performance of a classifier for all possible threshold choices.
    We look at such metrics in the next section.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 精度和召回率是很有用的指标，但它们仅描述了分类器在某个特定阈值下的性能。通常，有一个能够总结分类器在所有可能阈值选择下性能的指标是有用的。我们将在下一节中查看这些指标。
- en: Exercise 4.2
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.2
- en: What is precision?
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是精度？
- en: a) The percent of correctly identified churned customers in the validation dataset
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: a) 验证数据集中正确识别的流失客户百分比
- en: b) The percent of customers who actually churned among the customers who we
    predicted as churning
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在我们预测为流失的客户中，实际流失的客户百分比
- en: '* * *'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Exercise 4.3
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.3
- en: What is recall?
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是召回率？
- en: a) The percent of correctly identified churned customers among all churned customers
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: a) 在所有流失客户中正确识别的流失客户百分比
- en: b) The percent of correctly classified customers among customers we predicted
    as churning
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: b) 在我们预测为流失的客户中，正确分类的客户百分比
- en: 4.3 ROC curve and AUC score
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 ROC曲线和AUC分数
- en: The metrics we have covered so far work only with binary predictions—when we
    have only True and False values in the output. However, we do have ways to evaluate
    the performance of a model across all possible threshold choices. ROC curves is
    one of these options.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前提到的指标仅适用于二元预测——当我们输出中只有真和假值时。然而，我们确实有方法来评估模型在所有可能的阈值选择下的性能。ROC曲线就是这些选项之一。
- en: 'ROC stands for “receiver operating characteristic,” and it was initially designed
    for evaluating the strength of radar detectors during World War II. It was used
    to assess how well a detector could separate two signals: whether an airplane
    was there or not. Nowadays it’s used for a similar purpose: it shows how well
    a model can separate two classes, positive and negative. In our case, these classes
    are “churn” and “no churn.”'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ROC代表“接收者操作特征”，它最初是为评估二战期间雷达探测器的强度而设计的。它被用来评估探测器如何区分两个信号：是否有飞机在那里。如今，它被用于类似的目的：它显示了模型如何区分两个类别，正类和负类。在我们的案例中，这些类别是“流失”和“未流失”。
- en: 'We need two metrics for ROC curves: TPR and FPR, or true positive rate and
    false positive rate. Let’s take a look at these metrics.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要两个指标来绘制 ROC 曲线：TPR 和 FPR，即真阳性率和假阳性率。让我们来看看这些指标。
- en: 4.3.1 True positive rate and false positive rate
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 真阳性率和假阳性率
- en: 'The ROC curve is based on two quantities, FPR and TPR:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ROC 曲线基于两个量，FPR 和 TPR：
- en: 'False positive rate (FPR): the fraction of false positives among all negative
    examples'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假阳性率 (FPR)：所有负例中假阳性的比例
- en: 'True positive rate (TPR): the fraction of true positives among all positive
    examples'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 真阳性率 (TPR)：所有正例中真阳性的比例
- en: 'Like precision and recall, these values are based on the confusion matrix.
    We can calculate them using the following formulas:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 与精确率和召回率一样，这些值基于混淆矩阵。我们可以使用以下公式来计算它们：
- en: FPR = FP / (FP + TN)
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: FPR = FP / (FP + TN)
- en: TPR = TP / (TP + FN)
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: TPR = TP / (TP + FN)
- en: 'FPR and TPR involve two separate parts of the confusion table (figure 4.18):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: FPR 和 TPR 涉及混淆矩阵（图4.18）的两个独立部分：
- en: 'For FPR, we look at the first row of the table: the fraction of false positives
    among all negatives.'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 FPR，我们查看表格的第一行：所有负例中假阳性的比例。
- en: 'For TPR, we look at the second row of the table: the fraction of true positives
    among all positives.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 TPR，我们查看表格的第二行：所有正例中真阳性的比例。
- en: '![](../Images/04-18.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-18.png)'
- en: Figure 4.18 For calculating FPR, we look at the first row of the confusion table,
    and for calculating TPR, we look at the second row.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 在计算 FPR 时，我们查看混淆矩阵的第一行，而在计算 TPR 时，我们查看第二行。
- en: 'Let’s calculate these values for our model (figure 4.19):'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为我们的模型计算这些值（图4.19）：
- en: FPR = 172 / 1374 = 12.5%
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: FPR = 172 / 1374 = 12.5%
- en: 'FPR is the fraction of users we predicted as churning among everybody who didn’t
    churn. A small value for FPR tells us that a model is good—it has few false positives:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: FPR 是我们预测为流失的用户占未流失用户总数的比例。FPR 的值越小，说明模型越好——它有很少的假阳性：
- en: TPR = 289 / 486 = 59%
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: TPR = 289 / 486 = 59%
- en: TPR is the fraction of users who we predicted as churning among everybody who
    actually did churn. Note that TPR is the same as recall, so the higher the TPR
    is, the better.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: TPR 是我们预测为流失的用户占实际流失用户总数的比例。请注意，TPR 与召回率相同，因此 TPR 越高越好。
- en: '![](../Images/04-19.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-19.png)'
- en: 'Figure 4.19 FPR is the fraction of false positives among all non-churning customers:
    the smaller the FPR, the better. TPR is the fraction of true positives among all
    churning customers: the larger the TPR, the better.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19 FPR 是所有非流失客户中假阳性的比例：FPR 越小越好。TPR 是所有流失客户中真阳性的比例：TPR 越大越好。
- en: However, we still consider FPR and TPR metrics at only one threshold value—in
    our case, 0.5\. To be able to use them for ROC curves, we need to calculate these
    metrics for many different threshold values.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们仍然只考虑 FPR 和 TPR 指标在单个阈值值上的情况——在我们的例子中，是 0.5。为了能够使用它们来绘制 ROC 曲线，我们需要为许多不同的阈值值计算这些指标。
- en: 4.3.2 Evaluating a model at multiple thresholds
  id: totrans-259
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 在多个阈值下评估模型
- en: Binary classification models, such as logistic regression, typically output
    a probability—a score between zero and one. To make actual predictions, we binarize
    the output by setting some threshold to get only True and False values.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 二元分类模型，如逻辑回归，通常输出一个概率——介于零和一之间的分数。为了做出实际预测，我们通过设置某个阈值将输出二值化，以获得仅包含真和假值的分数。
- en: Instead of evaluating the model at one particular threshold, we can do it for
    a range of them—in the same way we did it for accuracy earlier in this chapter.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以像本章前面评估准确率时那样，对一系列阈值进行模型评估，而不是评估一个特定的阈值。
- en: For that, we first iterate over different threshold values and compute the values
    of the confusion table for each.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们首先遍历不同的阈值值，并计算每个阈值的混淆矩阵值。
- en: Listing 4.1 Computing the confusion table for different thresholds
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.1 计算不同阈值下的混淆矩阵
- en: '[PRE15]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Creates a list where we'll keep the results
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个列表，我们将在这里保存结果
- en: ❷ Creates an array with different threshold values, and loops over them
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个包含不同阈值值的数组，并遍历它们
- en: ❸ Computes the confusion table for predictions at each threshold
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算每个阈值下的预测混淆矩阵
- en: ❹ Appends the results to the scores list
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将结果追加到分数列表中
- en: The idea is similar to what we previously did with accuracy, but instead of
    recording just one value, we record all the four outcomes for the confusion table.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法与我们之前用准确率所做的是类似的，但不同的是，我们记录的不是单个值，而是混淆矩阵的所有四个结果。
- en: 'It’s not easy to deal with a list of tuples, so let’s convert it to a Pandas
    dataframe:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 处理一个元组列表并不容易，所以让我们将其转换为 Pandas 数据框：
- en: '[PRE16]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Turns the list into a Pandas dataframe
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将列表转换为Pandas dataframe
- en: ❷ Assigns names to the columns of the dataframe
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为dataframe的列分配名称
- en: This gives us a dataframe with five columns (figure 4.20).
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们一个包含五个列的dataframe（图4.20）。
- en: '![](../Images/04-20.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-20.png)'
- en: Figure 4.20 The dataframe with the elements of the confusion matrix evaluated
    at different threshold levels. The `[::10]` expression selects every 10th record
    of the dataframe.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 在不同阈值水平下评估的混淆矩阵元素的dataframe。`[::10]`表达式选择dataframe中的每10条记录。
- en: 'Now we can compute the TPR and FPR scores. Because the data is now in a dataframe,
    we can do it for all the values at once:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以计算TPR和FPR得分。因为数据现在在dataframe中，我们可以一次性计算所有值：
- en: '[PRE17]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'After running this code, we have two new columns in the dataframe: tpr and
    fpr (figure 4.21).'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此代码后，dataframe中新增了两列：tpr和fpr（图4.21）。
- en: '![](../Images/04-21.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-21.png)'
- en: Figure 4.21 The dataframe with the values of the confusion matrix as well as
    TPR and FPR evaluated at different thresholds
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21 包含混淆矩阵值以及在不同阈值下评估的TPR和FPR的dataframe
- en: 'Let’s plot them (figure 4.22):'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制它们（图4.22）：
- en: '[PRE18]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '![](../Images/04-22.png)'
  id: totrans-284
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-22.png)'
- en: Figure 4.22 The TPR and FPR for our model, evaluated at different thresholds
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.22 模型在不同阈值下的TPR和FPR
- en: 'Both TPR and FPR start at 100%—at the threshold of 0.0, we predict “churn”
    for everyone:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: TPR和FPR都从100%开始——在阈值为0.0时，我们预测每个人都“流失”：
- en: 'FPR is 100% because we have only false positives in the prediction. There are
    no true negatives: nobody is predicted as non-churning.'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPR（假正率）为100%，因为我们预测中只有假阳性。没有真正的阴性：没有人被预测为非流失。
- en: TPR is 100% because we have only true positives and no false negatives.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR为100%，因为我们只有真正的阳性，没有假阴性。
- en: As the threshold grows, both metrics decline but at different rates.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 随着阈值的增加，这两个指标都会下降，但下降速度不同。
- en: Ideally, FPR should go down very quickly. A small FPR indicates that the model
    makes very few mistakes predicting negative examples (false positives).
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，FPR应该迅速下降。小的FPR表明模型在预测负例（假阳性）时犯的错误很少。
- en: 'On the other hand, TPR should go down slowly, ideally staying near 100% all
    the time: that will mean that the model predicts true positives well.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，TPR应该缓慢下降，理想情况下始终接近100%：这意味着模型很好地预测了真正的阳性。
- en: 'To better understand what these TPR and FPR mean, let’s compare it with two
    baseline models: a random model and the ideal model. We will start with a random
    model.'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地理解这些TPR和FPR的含义，让我们将其与两个基线模型进行比较：一个随机模型和理想模型。我们将从一个随机模型开始。
- en: 4.3.3 Random baseline model
  id: totrans-293
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 随机基线模型
- en: 'A random model outputs a random score between 0 and 1, regardless of the input.
    It’s easy to implement—we simply generate an array with uniform random numbers:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 随机模型输出0到1之间的随机分数，无论输入如何。它很容易实现——我们只需生成一个包含均匀随机数的数组：
- en: '[PRE19]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Fixes the random seed for reproducibility
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置随机种子以确保可重复性
- en: ❷ Generates an array with random numbers between 0 and 1
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成0到1之间的随机数数组
- en: Now we can simply pretend that `y_rand` contains the predictions of our “model.”
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以简单地假设`y_rand`包含我们“模型”的预测。
- en: Let’s calculate FPR and TPR for our random model. To make it simpler, we’ll
    reuse the code we wrote previously and put it into a function.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算随机模型的FPR和TPR。为了简化，我们将重用之前编写的代码并将其放入函数中。
- en: Listing 4.2 Function for calculating TPR and FPR at different thresholds
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.2 在不同阈值下计算TPR和FPR的函数
- en: '[PRE20]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Defines a function that takes in actual and predicted values
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个函数，该函数接受实际值和预测值
- en: ❷ Calculates the confusion table for different thresholds
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算不同阈值下的混淆矩阵
- en: ❸ Converts the confusion table numbers to a dataframe
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将混淆矩阵中的数字转换为dataframe
- en: ❹ Calculates TPR and FPR using the confusion table numbers
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用混淆矩阵中的数字计算TPR和FPR
- en: ❺ Returns the resulting dataframe
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回结果dataframe
- en: 'Now let’s use this function to calculate the TPR and FPR for the random model:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用此函数来计算随机模型的TPR和FPR：
- en: '[PRE21]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This creates a dataframe with TPR and FPR values at different thresholds (figure 4.23).
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 这创建了一个包含不同阈值下TPR和FPR值的dataframe（图4.23）。
- en: '![](../Images/04-23.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-23.png)'
- en: Figure 4.23 The TPR and FPR values of a random model
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.23 随机模型的TPR和FPR值
- en: 'Let’s plot them:'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制它们：
- en: '[PRE22]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: We see that both TPR and FPR curves go from 100% to 0%, almost following the
    straight line (figure 4.24).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，TPR和FPR曲线几乎沿着直线从100%下降到0%，几乎遵循直线（图4.24）。
- en: '![](../Images/04-24.png)'
  id: totrans-315
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-24.png)'
- en: Figure 4.24 Both TPR and FPR of a random classifier decrease from 100% to 0%
    as a straight line.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.24 随机分类器的 TPR 和 FPR 以直线从 100% 降至 0%。
- en: 'At the threshold of 0.0, we treat everybody as churning. Both TPR and FPR are
    100%:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 在阈值为 0.0 时，我们将所有人视为流失。TPR 和 FPR 都是 100%：
- en: 'FPR is 100% because we have only false positives: all non-churning customers
    are identified as churning.'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPR 为 100%，因为我们只有假正例：所有非流失客户都被识别为流失。
- en: 'TPR is 100% because we have only true positives: we can correctly classify
    all churning customers as churning.'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR 为 100%，因为我们只有真正的正例：我们可以正确地将所有流失客户分类为流失。
- en: As we increase the threshold, both TPR and FPR decrease.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 随着阈值的增加，TPR 和 FPR 都会降低。
- en: 'At the threshold of 0.4, the model with a probability of 40% predicts “non-churn,”
    and with a probability of 60% predicts “churn.” Both TPR and FPR are 60%:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在阈值为 0.4 时，概率为 40% 的模型预测“非流失”，概率为 60% 的模型预测“流失”。TPR 和 FPR 都是 60%：
- en: FPR is 60% because we incorrectly classify 60% of non-churning customers as
    churning.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPR 为 60%，因为我们错误地将 60% 的非流失客户分类为流失。
- en: TPR is 60% because we correctly classify 60% of churning customers as churning.
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR 为 60%，因为我们正确地将 60% 的流失客户分类为流失。
- en: 'Finally, at 1.0, both TPR and FPR are 0%. At this threshold, we predict everybody
    as non-churning:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 1.0 的阈值下，TPR 和 FPR 都是 0%。在这个阈值下，我们预测所有人都是非流失：
- en: 'FPR is 0% because we have no false positives: we can correctly classify all
    non-churning customers as non-churning.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: FPR 为 0%，因为我们没有假正例：我们可以正确地将所有非流失客户分类为非流失。
- en: 'TPR is 0% because we have no true positives: all churning customers are identified
    as non-churning.'
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR 为 0%，因为我们没有真正的正例：所有流失客户都被识别为非流失。
- en: Let’s now move on to the next baseline and see how TPR and FPR look for the
    ideal model.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们继续到下一个基线，看看理想模型下的 TPR 和 FPR 看起来如何。
- en: 4.3.4 The ideal model
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 理想模型
- en: The ideal model always makes correct decisions. We’ll take it a step further
    and consider the ideal ranking model. This model outputs scores in such a way
    that churning customers always have higher scores than non-churning ones. In other
    words, the predicted probability for all churned ones should be higher than the
    predicted probability for non-churned ones.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 理想模型总是做出正确的决策。我们将更进一步，考虑理想排名模型。该模型以这种方式输出分数，使得流失客户的分数总是高于非流失客户。换句话说，所有流失的预测概率应该高于非流失的预测概率。
- en: So, if we apply the model to all the customers in our validation set and then
    sort them by the predicted probability, we first will have all the non-churning
    customers, followed by the churning ones (figure 4.25).
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果我们将模型应用于我们的验证集中的所有客户，然后按预测概率排序，我们首先将得到所有非流失客户，然后是流失客户（图 4.25）。
- en: '![](../Images/04-25.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-25.png)'
- en: Figure 4.25 The ideal model orders customers such that first we have non-churning
    customers and then churning ones.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.25 理想模型按顺序排列客户，首先是非流失客户，然后是流失客户。
- en: 'Of course, we cannot have such a model in real life. It’s still useful, however:
    we can use it for comparing our TPR and FPR to the TPR and FPR of the ideal model.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，在现实生活中我们不可能有这样的模型。然而，它仍然很有用：我们可以用它来比较我们的 TPR 和 FPR 与理想模型的 TPR 和 FPR。
- en: 'Let’s generate the ideal predictions. To make it easier, we generate an array
    with fake target variables that are already ordered: first it contains only 0s
    and then only 1s (figure 4.25). As for “predictions,” we simply can create an
    array with numbers that grow from 0 in the first cell to 1 in the last cell using
    the `np.linspace` function.'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们生成理想的预测。为了简化，我们生成一个带有假目标变量的数组，这些变量已经排序：首先只包含 0s，然后只包含 1s（图 4.25）。至于“预测”，我们可以使用
    `np.linspace` 函数创建一个数组，其中的数字从第一个单元格的 0 增长到最后一个单元格的 1。
- en: 'Let’s do it:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来做这件事：
- en: '[PRE23]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Calculates the number of negative and positive examples in the dataset
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算数据集中负例和正例的数量
- en: ❷ Generates an array that first repeats 0s num_neg number of times, followed
    by 1s repeated num_pos number of times
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 生成一个数组，首先重复 0s num_neg 次数，然后重复 1s num_pos 次数
- en: '❸ Generates the predictions of the "model": numbers that grow from 0 in the
    first cell to 1 in the last'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 生成“模型”的预测：从第一个单元格的 0 增长到最后一个单元格的 1 的数字
- en: ❹ Computes the TPR and FPR curves for the classifier
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算分类器的 TPR 和 FPR 曲线
- en: As a result, we get a dataframe with the TPR and FPR values of the ideal model
    (figure 4.26). You can read more about `np.linspace` and `np.repeat` functions
    in appendix C.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到一个包含理想模型 TPR 和 FPR 值的数据框（图 4.26）。您可以在附录 C 中了解更多关于 `np.linspace` 和 `np.repeat`
    函数的信息。
- en: '![](../Images/04-26.png)'
  id: totrans-342
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-26.png)'
- en: Figure 4.26 The TPR and FPR values for the ideal model
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.26 理想模型的TPR和FPR值
- en: 'Now we can plot it (figure 4.27):'
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以绘制它（图4.27）：
- en: '[PRE24]'
  id: totrans-345
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '![](../Images/04-27.png)'
  id: totrans-346
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-27.png)'
- en: Figure 4.27 The TPR and FPR curves for the ideal model
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.27 理想模型的TPR和FPR曲线
- en: From the plot, we can see that
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中，我们可以看到
- en: Both TPR and FPR start at 100% and end at 0%.
  id: totrans-349
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TPR和FPR都从100%开始，结束于0%。
- en: For thresholds lower than 0.74, we always correctly classify all churning customers
    as churning; that’s why TRP stays at 100%. On the other hand, we incorrectly classify
    some non-churning ones as churning—those are our false positives. As we increase
    the threshold, fewer and fewer non-churning customers are classified as churning,
    so FPR goes down. At 0.6, we misclassify 258 non-churning customers as churning
    (figure 4.28, A).
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于低于0.74的阈值，我们总是正确地将所有流失客户分类为流失；这就是为什么TRP保持在100%。另一方面，我们将一些非流失客户错误地分类为流失——这些是我们的假阳性。随着阈值的提高，越来越少地非流失客户被分类为流失，因此FPR下降。在0.6时，我们错误地将258名非流失客户分类为流失（图4.28，A）。
- en: 'The threshold of 0.74 is the ideal situation: all churning customers are classified
    as churning, and all non-churning are classified as non-churning; that’s why TPR
    is 100% and FPR is 0% (figure 4.28, B).'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 阈值为0.74是理想情况：所有流失客户都被分类为流失，所有非流失客户都被分类为非流失；这就是为什么TPR是100%，FPR是0%（图4.28，B）。
- en: Between 0.74 and 1.0, we always correctly classify all non-churning customers,
    so FPR stays at 0%. However, as we increase the threshold, we start incorrectly
    classifying more and more churning customers as non-churning, so TPR goes down.
    At 0.8, 114 out of 446 churning customers are incorrectly classified as non-churning.
    Only 372 predictictions are correct, so TPR is 76% (figure 4.28, C).
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在0.74到1.0之间，我们总是正确地将所有非流失客户分类，因此FPR保持在0%。然而，当我们提高阈值时，我们开始错误地将越来越多的流失客户分类为非流失客户，因此TPR下降。在0.8时，446名流失客户中有114名被错误地分类为非流失客户。只有372个预测是正确的，因此TPR是76%（图4.28，C）。
- en: '![](../Images/04-28.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-28.png)'
- en: Figure 4.28 TPR and FPR of the ideal ranking model evaluated at different thresholds
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.28 在不同阈值下评估的理想排名模型的TPR和FPR
- en: Now we’re ready to build the ROC curve.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好构建ROC曲线。
- en: Exercise 4.4
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.4
- en: What does the ideal ranking model do?
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 理想排名模型做什么？
- en: a) When applied to the validation data, it scores the customers such that for
    non-churning customers, the score is always lower than for churning ones.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: a) 当应用于验证数据时，它对客户的评分使得对于非流失客户，评分总是低于流失客户。
- en: b) It scores non-churning customers higher than churning ones.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它将非流失客户的评分高于流失客户。
- en: 4.3.5 ROC Curve
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.5 ROC曲线
- en: 'To create an ROC curve, instead of plotting FPR and TPR against different threshold
    values, we plot them against each other. For comparison, we also add the ideal
    and random models to the plot:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建ROC曲线，我们不是将FPR和TPR与不同的阈值值进行比较，而是将它们相互比较。为了比较，我们还添加了理想和随机模型到图中：
- en: '[PRE25]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Makes the plot square
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使图表成为正方形
- en: ❷ Plots the ROC curve for the model and baselines
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 绘制模型和基线模型的ROC曲线
- en: As a result, we get an ROC curve (figure 4.29). When we plot it, we can see
    that the ROC curve of the random classifier is an approximately straight line
    from bottom left to top right. For the ideal model, however, the curve first goes
    up until it reaches 100% TPR, and from there it goes right until it reaches 100%
    FPR.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到了一个ROC曲线（图4.29）。当我们绘制它时，我们可以看到随机分类器的ROC曲线是从左下角到右上角的近似直线。然而，对于理想模型，曲线首先上升直到达到100%
    TPR，然后向右直到达到100% FPR。
- en: '![](../Images/04-29.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-29.png)'
- en: Figure 4.29 The ROC curve shows the relationship between the FPR and TPR of
    a model.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.29 ROC曲线显示了模型FPR和TPR之间的关系。
- en: Our models should always be somewhere between these two curves. We want our
    model to be as close to the ideal curve as possible and as far as possible from
    the random curve.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标模型应该位于这两条曲线之间。我们希望我们的模型尽可能接近理想曲线，尽可能远离随机曲线。
- en: The ROC curve of a random model serves as a good visual baseline—when we add
    it to the plot, it helps us to judge how far our model is from this baseline—so
    it’s a good idea to always include this line in the plot.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 随机模型的ROC曲线作为一个良好的视觉基线——当我们将其添加到图中时，它有助于我们判断我们的模型与这个基线有多远——所以总是在图中包含这条线是个好主意。
- en: 'However, we don’t really need to generate a random model each time we want
    to have an ROC curve: we know what it looks like, so we can simply include a straight
    line from (0, 0) to (1, 1) in the plot.'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们并不真的需要在每次想要有一个ROC曲线时都生成一个随机模型：我们知道它看起来像什么，所以我们可以简单地在一个从(0, 0)到(1, 1)的直线上绘制一条直线。
- en: 'As for the ideal model, we know that it always goes up to (0, 1) and then goes
    right to (1, 1). The top-left corner is called the “ideal spot”: it’s the point
    when the ideal model gets 100% TPR and 0% FPR. We want our models to get as close
    to the ideal spot as possible.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 对于理想模型，我们知道它总是上升到(0, 1)，然后向右到(1, 1)。左上角被称为“理想点”：这是理想模型获得100% TPR和0% FPR的点。我们希望我们的模型尽可能地接近理想点。
- en: 'With this information, we can reduce the code for plotting the curve to the
    following:'
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 有这个信息，我们可以将绘制曲线的代码简化为以下内容：
- en: '[PRE26]'
  id: totrans-373
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: This produces the result in figure 4.30.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了图4.30中的结果。
- en: '![](../Images/04-30.png)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-30.png)'
- en: 'Figure 4.30 The ROC curve. The baseline makes it easier to see how far the
    ROC curve of our model is from that of a random model. The top-left corner (0,
    1) is the “ideal spot”: the closer our models get to it, the better.'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.30 ROC曲线。基线使得我们更容易看到我们的模型ROC曲线与随机模型ROC曲线的距离。左上角(0, 1)是“理想点”：我们的模型越接近这一点，越好。
- en: 'Although computing all the FPR and TPR values across many thresholds is a good
    exercise, we don’t need to do it ourselves every time we want to plot an ROC curve.
    We simply can use the `roc_curve` function from the `metrics` package of Scikit-learn:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然计算许多阈值下的所有FPR和TPR值是一个很好的练习，但每次我们想要绘制ROC曲线时，我们不需要自己来做。我们只需使用Scikit-learn的`metrics`包中的`roc_curve`函数即可：
- en: '[PRE27]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As a result, we get a plot identical to the previous one (figure 4.30).
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们得到了与前一个图相同的图（图4.30）。
- en: Now let’s try to make more sense of the curve and understand what it can actually
    tell us. To do this, we visually map the TPR and FPR values to their thresholds
    on the ROC curve (figure 4.31).
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来尝试更深入地理解曲线，并了解它实际上能告诉我们什么。为此，我们将TPR和FPR值在ROC曲线上可视化为它们的阈值（图4.31）。
- en: 'In the ROC plot, we start from the (0, 0) point—this is the point at the bottom
    left. It corresponds to 0% FPR and 0% TPR, which happens at high thresholds like
    1.0, when no customers are above that score. For these cases we simply end up
    predicting “no churn” for everyone. That’s why our TPR is 0%: we are never correctly
    predicting churned customers. FPR, on the other hand, is 0% because this dummy
    model can correctly predict all non-churning customers as non-churning, so there
    are no false positives.'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 在ROC图中，我们从(0, 0)点开始——这是左下角的位置。它对应于0%的FPR和0%的TPR，这发生在高阈值如1.0时，此时没有客户得分高于该值。对于这些情况，我们只是简单地预测“无流失”给所有人。这就是为什么我们的TPR是0%：我们从未正确预测到流失客户。另一方面，FPR是0%，因为这个虚拟模型可以正确预测所有非流失客户为非流失，因此没有假阳性。
- en: As we go up the curve, we consider FPR and TPR values evaluated at smaller thresholds.
    At 0.7, FPR changes only slightly, from 0% to 2%, but the TPR increases from 0%
    to 20% (figure 4.31, B and C).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 随着曲线上升，我们考虑在更小阈值下评估的FPR和TPR值。在0.7时，FPR变化很小，从0%变为2%，但TPR从0%增加到20%（图4.31，B和C）。
- en: '![](../Images/04-31a.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-31a.png)'
- en: (A) TPR and FPR at different thresholds
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: (A) 不同阈值下的TPR和FPR
- en: '![](../Images/04-31b.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-31b.png)'
- en: (B) FPR and TPR values of the model for different thresholds
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: (B) 模型在不同阈值下的FPR和TPR值
- en: '![](../Images/04-31c.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-31c.png)'
- en: (C) FPR and TPR values for selected thresholds
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: (C) 选择阈值下的FPR和TPR值
- en: Figure 4.31 Translation of the TPR and FPR plots against different threshold
    values (A and B) to the ROC curve (C). In the ROC plot, we start from the bottom
    left with high threshold values, where most of the customers are predicted as
    non-churning, and gradually go to the top right with low thresholds, where most
    of the customers are predicted as churning.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.31 将不同阈值下的TPR和FPR图（A和B）转换为ROC曲线（C）。在ROC图中，我们从左下角的高阈值值开始，此时大多数客户被预测为非流失，并逐渐过渡到右上角，此时大多数客户被预测为流失。
- en: As we follow the line, we keep decreasing the threshold and evaluating the model
    at smaller values, predicting more and more customers as churning. At some point,
    we cover most of the positives (churning customers). For example, at the threshold
    of 0.2, we predict most of the users as churning, which means that many of these
    predictions are false positives. FPR then starts to grow faster than TPR; at the
    threshold of 0.2, it’s already at almost 40%.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们沿着线走，我们不断降低阈值，并在更小的值上评估模型，预测越来越多的客户为流失。在某个点上，我们覆盖了大部分的正面（流失客户）。例如，在0.2的阈值下，我们预测大多数用户为流失，这意味着许多这些预测是假阳性。FPR随后开始比TPR增长得更快；在0.2的阈值下，它已经接近40%。
- en: Eventually, we reach the 0.0 threshold and predict that everyone is churning,
    thus reaching the top-right corner of the ROC plot.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，我们达到0.0阈值，预测每个人都将流失，从而达到ROC图的最右上角。
- en: 'When we start at high threshold values, all models are equal: any model at
    high threshold values degrades to the constant “model” that predicts False all
    the time. As we decrease the threshold, we start predicting some of the customers
    as churning. The better the model, the more customers are correctly classified
    as churning, resulting in a better TPR. Likewise, good models have a smaller FPR
    because they have fewer false positives.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们从高阈值值开始时，所有模型都是相同的：任何在高阈值值下的模型都会退化到始终预测False的恒定“模型”。随着我们降低阈值，我们开始预测一些客户会流失。模型越好，正确分类为流失的客户就越多，从而产生更好的TPR。同样，好的模型FPR较小，因为它们有更少的误报。
- en: Thus, the ROC curve of a good model first goes up as high as it can and only
    then starts turning right. Poor models, on the other hand, from the start have
    higher FPRs and lower TPRs, so their curves tend to go to the right earlier (figure 4.32).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，一个好的模型的ROC曲线首先尽可能地上升，然后才开始向右转。另一方面，表现差的模型从一开始就具有更高的FPR（假正率）和更低的TPR（真正率），因此它们的曲线倾向于更早地向右移动（图4.32）。
- en: '![](../Images/04-32.png)'
  id: totrans-394
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-32.png)'
- en: Figure 4.32 ROC curves of good models go up as much as they can before turning
    right. Poor models, on the other hand, tend to have more false positives from
    the beginning, so they tend to go right earlier.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.32 良好模型的ROC曲线在转向右之前尽可能地上升。另一方面，表现差的模型一开始就倾向于有更多的误报，因此它们倾向于更早地向右移动。
- en: 'We can use this for comparing multiple models: we can simply plot them on the
    same graph and see which of them is closer to the ideal point of (0, 1). For example,
    let’s take a look at the ROC curves of the large and small models and plot them
    on the same graph:'
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用这个方法来比较多个模型：我们只需将它们绘制在同一张图上，并查看哪个更接近（0, 1）的理想点。例如，让我们看一下大模型和小模型的ROC曲线，并将它们绘制在同一张图上：
- en: '[PRE28]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'This way we can get two ROC curves on the same plot (figure 4.33). We can see
    that the large model is better than the small model: it’s closer to the ideal
    point for all the thresholds.'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 这样我们可以在同一张图上得到两个ROC曲线（图4.33）。我们可以看到大模型比小模型好：它在所有阈值下都更接近理想点。
- en: '![](../Images/04-33.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-33.png)'
- en: Figure 4.33 Plotting multiple ROC curves on the same graph helps us visually
    identify which model performs better.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.33 在同一张图上绘制多个ROC曲线有助于我们直观地识别哪个模型表现更好。
- en: 'ROC curves are quite useful on their own, but we also have another metric that’s
    based on it: AUC, or the area under the ROC curve.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线本身非常有用，但我们还有一个基于它的另一个指标：AUC，即ROC曲线下的面积。
- en: 4.3.6 Area under the ROC curve (AUC)
  id: totrans-402
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.6 ROC曲线下的面积（AUC）
- en: When evaluating our models using the ROC curve, we want them to be as close
    to the ideal spot and as far from the random baseline as possible.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们使用ROC曲线来评估我们的模型时，希望它们尽可能地接近理想位置，并且尽可能地远离随机基线。
- en: We can quantify this “closeness” by measuring the area under the ROC curve.
    We can use this metric—abbreviated as AU ROC, or often simply AUC—as a metric
    for evaluating the performance of a binary classification model.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过测量ROC曲线下的面积来量化这种“接近度”。我们可以使用这个指标——简称为AUROC，或通常简单地称为AUC——作为评估二元分类模型性能的指标。
- en: The ideal model forms a 1x1 square, so the area under its ROC curve is 1, or
    100%. The random model takes only half of that, so its AUC is 0.5, or 50%. The
    AUCs of our two models—the large one and the small one—will be somewhere between
    the random baseline of 50% and the ideal curve of 100%.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 理想模型形成一个1x1的正方形，因此其ROC曲线下的面积是1，或100%。随机模型只占一半，因此其AUC是0.5，或50%。我们两个模型的AUC——大模型和小模型——将在50%的随机基线和100%的理想曲线之间。
- en: Important An AUC of 0.9 is indicative of a reasonably good model; 0.8 is okay,
    0.7 is not very performant, and 0.6 indicates quite poor performance.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 重要：AUC（曲线下面积）为0.9表明模型表现相当好；0.8是可接受的，0.7表现不佳，而0.6则表示表现相当差。
- en: 'To calculate the AUC for our models we can use `auc`, a function from the `metrics`
    package of Scikit-learn:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算我们模型的AUC，我们可以使用来自Scikit-learn的`metrics`包中的`auc`函数：
- en: '[PRE29]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: For the large model, the result is 0.84; for the small model, it’s 0.81 (figure 4.34).
    Churn prediction is a complex problem, so an AUC of 80% is quite good.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 对于大模型，结果是0.84；对于小模型，结果是0.81（图4.34）。客户流失预测是一个复杂的问题，因此AUC为80%已经相当不错。
- en: '![](../Images/04-34.png)'
  id: totrans-410
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04-34.png)'
- en: 'Figure 4.34 The AUC for our models: 84% for the large model and 81% for the
    small model'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.34 我们模型的AUC：大模型为84%，小模型为81%
- en: 'If all we need is the AUC, we don’t need to compute the ROC curve first. We
    can take a shortcut and use the `roc_auc_score` function from Scikit-learn, which
    takes care of everything and simply returns the AUC of our model:'
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们只需要AUC，我们不需要先计算ROC曲线。我们可以走捷径，使用Scikit-learn的`roc_auc_score`函数，它负责一切并直接返回我们模型的AUC：
- en: '[PRE30]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: We get approximately the same results as previously (figure 4.35).
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到的结果与之前的大致相同（图4.35）。
- en: '![](../Images/04-35.png)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-35.png)'
- en: Figure 4.35 Calculating AUC using Scikit-learn’s `roc_auc_score` function.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.35 使用Scikit-learn的`roc_auc_score`函数计算AUC。
- en: 'Note The values from `roc_auc_score` may be slightly different from AUC computed
    from the dataframes where we calculated TPR and FPR ourselves: Scikit-learn internally
    uses a more precise method for creating ROC curves.'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`roc_auc_score`的值可能与我们自己计算TPR和FPR的数据框中的AUC值略有不同：Scikit-learn内部使用更精确的方法来创建ROC曲线。
- en: 'ROC curves and AUC scores tell us how well the model separates positive and
    negative examples. What is more, AUC has a nice probabilistic interpretation:
    it tells us what the probability is that a randomly selected positive example
    will have a score higher than a randomly selected negative example.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ROC曲线和AUC得分告诉我们模型如何将正例和负例分开。更重要的是，AUC有一个很好的概率解释：它告诉我们随机选择的正例得分高于随机选择的负例得分的概率是多少。
- en: 'Suppose we randomly pick a customer that we know churned and a customer who
    didn’t and then apply the model to these customers and see what the score is for
    each. We want the model to score the churning customer higher than the non-churning
    one. AUC tells us the probability of that happening: it’s the probability that
    the score of a randomly selected churning customer is higher than the score of
    a randomly selected non-churning one.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们随机选择一个已知已经流失的客户和一个没有流失的客户，然后对这些客户应用模型并查看每个客户的得分。我们希望模型对流失客户的评分高于非流失客户。AUC告诉我们这种情况发生的概率：这是随机选择的流失客户得分高于随机选择的非流失客户得分的概率。
- en: 'We can verify this. If we do this experiment 10,000 times and then count how
    many times the score of the positive example was higher than the score of the
    negative one, the percentage of cases when it’s true should roughly correspond
    to the AUC:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以验证这一点。如果我们进行10,000次这样的实验，然后统计正例得分高于负例得分的情况有多少次，那么这个比例应该大致对应于AUC：
- en: '[PRE31]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Selects the score for all non-churning customers
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 选择所有非流失客户的得分
- en: ❷ Selects the score for all churning customers
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 选择所有流失客户的得分
- en: ❸ Fixes the seed to make sure the results are reproducible
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将种子值固定以确保结果可重复
- en: ❹ Randomly selects 10,000 scores of negative examples (non-churning customers)
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 随机选择10,000个负例得分（非流失客户）
- en: ❺ Randomly selects 10,000 scores of positive examples (churning customers)
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 随机选择10,000个正例得分（流失客户）
- en: ❻ For each of the positive examples, checks if the score is higher than the
    respective negative example
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 对于每个正例，检查其得分是否高于相应的负例
- en: This prints 0.8356, which is indeed pretty close to the AUC value of our classifier.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 这会输出0.8356，这确实非常接近我们分类器的AUC值。
- en: 'This interpretation of AUC gives us additional insight into the quality of
    our models. The ideal model orders all the customers such that we first have non-churning
    customers and then churning customers. With this order, the AUC is always 1.0:
    the score of a randomly chosen churning customer is always higher than the score
    of a non-churning customer. On the other hand, the random model just shuffles
    the customers, so the score of a churning customer has only a 50% chance of being
    higher than the score of a non-churning one.'
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 这种对AUC的解释使我们能够对模型的质量有更深入的了解。理想的模型将所有客户按顺序排列，首先是未流失客户，然后是流失客户。在这种情况下，AUC总是1.0：随机选择的流失客户得分总是高于非流失客户。另一方面，随机模型只是重新排列客户，因此流失客户的得分只有50%的机会高于非流失客户。
- en: 'AUC thus not only gives us a way of evaluating the models at all possible thresholds
    but also describes how well the model separates two classes: in our case, churning
    and non-churning. If the separation is good, then we can order the customers such
    that most of the churning users come first. Such a model will have a good AUC
    score.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，AUC不仅为我们提供了一种评估所有可能阈值下模型的方法，而且还描述了模型分离两个类别的好坏：在我们的例子中，是流失和非流失。如果分离得好，那么我们可以按顺序排列客户，使得大多数流失用户排在前面。这样的模型将有一个好的AUC得分。
- en: 'Note You should keep this interpretation in mind: it provides an easy way to
    explain the meaning behind AUC to people without a machine learning background,
    such as managers and other decision makers.'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你应该记住这个解释：它为那些没有机器学习背景的人提供了一个简单的方式来解释AUC的含义，比如经理和其他决策者。
- en: This makes AUC the default classification metric in most situations, and it’s
    often the metric we use when finding the best parameter set for our models.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 这使得AUC成为大多数情况下的默认分类指标，并且当我们寻找模型的最佳参数集时，它通常是使用的指标。
- en: The process of finding the best parameters is called “parameter tuning,” and
    in the next section we will see how to do this.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳参数的过程称为“参数调整”，在下一节中我们将看到如何进行这一过程。
- en: 4.4 Parameter tuning
  id: totrans-434
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 参数调整
- en: In the previous chapter, we used a simple hold-out validation scheme for testing
    our models. In this scheme, we take part of the data out and keep it for validation
    purposes only. This practice is good but doesn’t always give us the whole picture.
    It tells us how well the model will perform on these specific data points. However,
    it doesn’t necessarily mean the model will perform equally well on other data
    points. So, how do we check if the model indeed works well in a consistent and
    predictable manner?
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用了一个简单的保留验证方案来测试我们的模型。在这个方案中，我们取出部分数据并仅保留它用于验证目的。这种做法很好，但并不总是能给出完整的画面。它告诉我们模型在这些特定的数据点上表现如何。然而，这并不一定意味着模型在其他数据点上会有相同的表现。那么，我们如何检查模型是否确实以一致和可预测的方式工作呢？
- en: 4.4.1 K-fold cross-validation
  id: totrans-436
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 K折交叉验证
- en: It’s possible to use all the available data to assess the quality of models
    and get more reliable validation results. We can simply perform validation multiple
    times.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能使用所有可用数据来评估模型的质量并获得更可靠的验证结果。我们可以简单地多次进行验证。
- en: First, we split the entire dataset into a certain number of parts (say, three).
    Then we train a model on two parts and validate on the remaining one. We repeat
    this process three times and at the end get three different scores. This is exactly
    the idea behind K-fold cross-validation (figure 4.36).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将整个数据集分成若干部分（比如说，三部分）。然后我们在其中两部分上训练一个模型，在剩下的那部分上进行验证。我们重复这个过程三次，最后得到三个不同的分数。这正是K折交叉验证（图4.36）背后的理念。
- en: '![](../Images/04-36.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04-36.png)'
- en: Figure 4.36 K-fold cross-validation (K=3). We split the entire dataset into
    three equal parts, or folds. Then, for each fold, we take it as the validation
    dataset and use the remaining K – 1 folds as the training data. After training
    the model, we evaluate it on the validation fold, and at the end we get k metric
    values.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.36 K折交叉验证（K=3）。我们将整个数据集分成三个相等的部分，或者称为折。然后，对于每一折，我们将它作为验证数据集，并使用剩下的K-1折作为训练数据。在训练模型后，我们在验证折上评估它，最后我们得到k个指标值。
- en: Before we implement it, we need to make the training process simpler, so it’s
    easy to run this process multiple times. For that, we’ll put all the code for
    training into a `train` function, which first converts the data into a one-hot
    encoding representation and then trains the model.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们实现它之前，我们需要使训练过程更简单，这样就可以轻松地多次运行这个过程。为此，我们将所有训练代码放入一个`train`函数中，该函数首先将数据转换为one-hot编码表示，然后训练模型。
- en: Listing 4.3 Training the model
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.3 训练模型
- en: '[PRE32]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Applies one-hot encoding
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用one-hot编码
- en: ❷ Trains the model
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练模型
- en: Likewise, we also put the prediction logic into a `predict` function. This function
    takes in a dataframe with customers, the vectorizer we “trained” previously—for
    doing one-hot encoding—and the model. Then we apply the vectorizer to the dataframe,
    get a matrix, and finally apply the model to the matrix to get predictions.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们也把预测逻辑放入一个`predict`函数中。这个函数接受一个包含客户的数据框，我们之前“训练”过的向量器——用于进行one-hot编码——以及模型。然后我们应用向量器到数据框上，得到一个矩阵，最后将模型应用到矩阵上以得到预测。
- en: Listing 4.4 Applying the model to new data
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.4 将模型应用于新数据
- en: '[PRE33]'
  id: totrans-448
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Applies the same one-hot encoding scheme as in training
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 应用与训练相同的one-hot编码方案
- en: ❷ Uses the model to make predictions
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用模型进行预测
- en: Now we can use these functions for implementing K-fold cross-validation.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以使用这些函数来实现K折交叉验证。
- en: 'We don’t need to implement cross-validation ourselves: in Scikit-learn there’s
    a class for doing that. It’s called `KFold`, and it lives in the `model_selection`
    package.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不需要自己实现交叉验证：在Scikit-learn中有一个用于此目的的类。它被称为`KFold`，位于`model_selection`包中。
- en: Listing 4.5 K-fold cross-validation
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 列表4.5 K折交叉验证
- en: '[PRE34]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Imports the KFold class
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入KFold类
- en: ❷ Uses it to split the data into 10 parts
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用它将数据分割成 10 部分
- en: ❸ Creates a list for storing the results
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个用于存储结果的列表
- en: ❹ Iterates over the 10 different splits of the data
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 对数据的 10 个不同分割进行迭代
- en: ❺ Splits the data into train and validation sets
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将数据分割成训练集和验证集
- en: ❻ Trains the model and makes predictions
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 训练模型并进行预测
- en: ❼ Evaluates the quality of the train model on the validation data using AUC
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用 AUC 评估训练模型在验证数据上的质量
- en: ❽ Saves the AUC to the list with the results
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 将 AUC 保存到结果列表中
- en: 'Note that when defining the splitting in the `KFold` class in ❷, we set three
    parameters:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，当在 `KFold` 类中定义分割时（❷），我们设置了三个参数：
- en: '`n_splits` `=` `10`: That’s K, which specifies the number of splits.'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n_splits` `=` `10`：这是 K，它指定了分割的数量。'
- en: '`shuffle` `=` `True`: We ask it to shuffle the data before splitting it.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shuffle` `=` `True`：我们要求它在分割数据之前先打乱数据。'
- en: '`random_state` `=` `1`: Because there’s randomization in the process (shuffling
    data), we want the results to be reproducible, so we fix the seed for the random-number
    generator.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`random_state` `=` `1`：因为在这个过程中有随机化（打乱数据），我们希望结果是可以重现的，因此我们固定随机数生成器的种子。'
- en: 'Here we used K-fold cross-validation with K =10\. Thus, when we run it, at
    the end we get 10 different numbers—10 AUC scores evaluated on 10 different validation
    folds:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用了 K 折交叉验证，K = 10。因此，当我们运行它时，最后我们得到 10 个不同的数字——10 个 AUC 分数，这些分数是在 10
    个不同的验证折叠上评估的：
- en: '[PRE35]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'It’s not a single number anymore, and we can think of it as a distribution
    of AUC scores for our model. We can get some statistics from this distribution,
    such as the mean and standard deviation:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 它不再是单个数字，我们可以将其视为模型 AUC 分数的分布。我们可以从这个分布中获得一些统计数据，例如均值和标准差：
- en: '[PRE36]'
  id: totrans-470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This prints “0.842 ± 0.012”.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 这会打印“0.842 ± 0.012”。
- en: Now, not only do we know the average performance, but we also have an idea of
    how volatile that performance is, or how far it may deviate from the average.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们不仅知道了平均性能，而且还有关于该性能波动性的一个概念，或者它可能偏离平均值的程度。
- en: 'A good model should be quite stable across different folds: this way, we make
    sure we don’t get a lot of surprises when the model goes live. The standard deviation
    tells us about that: the smaller it is, the more stable the model is.'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好的模型应该在不同的折叠中相当稳定：这样我们才能确保当模型上线时不会出现很多意外。标准差告诉我们这一点：它越小，模型就越稳定。
- en: 'Now we can use K-fold cross-validation for parameter tuning: selecting the
    best parameters.'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 K 折交叉验证来进行参数调整：选择最佳参数。
- en: 4.4.2 Finding best parameters
  id: totrans-475
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 寻找最佳参数
- en: We learned how we can use K-fold cross-validation for evaluating the performance
    of our model. The model we trained previously was using the default value for
    the parameter `C`, which controls the amount of regularization.
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 我们学习了如何使用 K 折交叉验证来评估我们模型的性能。我们之前训练的模型使用的是参数 `C` 的默认值，该参数控制正则化的数量。
- en: Let’s select our cross-validation procedure for selecting the best parameter
    `C`. For that, we first adjust the `train` function to take in an additional parameter.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们选择我们的交叉验证过程来选择最佳参数 `C`。为此，我们首先调整 `train` 函数以接受一个额外的参数。
- en: Listing 4.6 Function for training the model with parameter C for controlling
    regularization
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.6 用于训练具有参数 C 的模型的函数，以控制正则化
- en: '[PRE37]'
  id: totrans-479
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: ❶ Adds an extra parameter to the train function
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在训练函数中添加一个额外的参数
- en: ❷ Uses this parameter during training
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练期间使用此参数
- en: 'Now let’s find the best parameter `C`. The idea is simple:'
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们找到最佳的参数 `C`。这个想法很简单：
- en: Loop over different values of `C`.
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 遍历不同的 `C` 值。
- en: For each `C`, run cross-validation and record the mean AUC across all folds
    as well as the standard deviation.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个 `C`，运行交叉验证，并记录所有折叠的平均 AUC 以及标准差。
- en: 'Listing 4.7 Tuning the model: selecting the best parameter C using cross-validation'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 4.7 调整模型：使用交叉验证选择最佳参数 C
- en: '[PRE38]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: When we run it, it prints
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行它时，它会打印
- en: '[PRE39]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: What we see is that after `C` = 0.1, the average AUC is the same and doesn’t
    grow anymore.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，当 `C` = 0.1 后，平均 AUC 保持不变，不再增长。
- en: 'However, the standard deviation is smaller for `C` = 0.5 than for `C` = 0.1,
    so we should use that. The reason we prefer `C` = 0.5 to `C` = 1 and `C` = 10
    is simple: when the `C` parameter is small, the model is more regularized. The
    weights of this model are more restricted, so in general, they are smaller. Small
    weights in the model give us additional assurance that the model will behave well
    when we use it on real data. So we select `C` = 0.5.'
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当`C` = 0.5时，标准差比`C` = 0.1时小，因此我们应该使用那个。我们更喜欢`C` = 0.5而不是`C` = 1和`C` = 10的原因很简单：当`C`参数较小时，模型更正则化。这个模型的权重更受限制，所以一般来说，它们更小。模型中的小权重给我们额外的保证，即当我们使用真实数据时，模型会表现良好。因此，我们选择`C`
    = 0.5。
- en: 'Now we need to do the last step: train the model on the entire train and validation
    datasets and apply it to the test dataset to verify it indeed works well.'
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要做最后一步：在完整的训练和验证数据集上训练模型，并将其应用于测试数据集以验证它确实工作得很好。
- en: 'Let’s use our `train` and `predict` functions for that:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用我们的`train`和`predict`函数来做这件事：
- en: '[PRE40]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Trains the model on the full training dataset
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在完整训练数据集上训练模型
- en: ❷ Applies it to the test dataset
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将其应用于测试数据集
- en: ❸ Evaluates the predictions on the test data
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在测试数据上评估预测
- en: When we execute the code, we see that the performance of the model (AUC) on
    the held-out test set is 0.858.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行代码时，我们看到模型在保留的测试集上的性能（AUC）为0.858。
- en: That’s a little higher than what we had on the validation set, but that’s not
    an issue; it could happen just by chance. What’s important is that the score is
    not significantly different from the validation score.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 这比我们在验证集上得到的分数略高，但这不是问题；这可能是偶然发生的。重要的是，分数与验证分数没有显著差异。
- en: Now we can use this model for scoring real customers and think about our marketing
    campaign for preventing churn. In the next chapter, we will see how to deploy
    this model in a production environment.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个模型对真实客户进行评分，并考虑我们的防止客户流失的市场营销活动。在下一章中，我们将看到如何在生产环境中部署这个模型。
- en: 4.5 Next steps
  id: totrans-500
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 下一步
- en: 4.5.1 Exercises
  id: totrans-501
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.1 练习
- en: 'Try the following exercises to further explore the topics of model evaluation
    and model selection:'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以下练习以进一步探索模型评估和模型选择的话题：
- en: In this chapter, we plotted TPR and FPR for different threshold values, and
    it helped us understand what these metrics mean and also how the performance of
    our model changes when we choose a different threshold. It’s helpful to do a similar
    exercise for precision and recall, so try to repeat this experiment, this time
    using precision and recall instead of TPR and FPR.
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们绘制了不同阈值值下的TPR和FPR，这帮助我们理解了这些指标的含义，以及当我们选择不同的阈值时，模型性能如何变化。对于精度和召回率进行类似的练习是有帮助的，所以尝试重复这个实验，这次使用精度和召回率而不是TPR和FPR。
- en: 'When plotting precision and recall for different threshold values, we can see
    that a conflict exists between precision and recall: when one goes up, the other
    goes down, and the other way around. This is called the “precision-recall trade-off”:
    we cannot select a threshold that makes both precision and recall good. However,
    we do have strategies for selecting the threshold, even though precision and recall
    are conflicting. One of them is plotting precision and recall curves and seeing
    where they intersect, and using this threshold for binarizing the predictions.
    Try implementing this idea.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当绘制不同阈值值下的精度和召回率时，我们可以看到精度和召回率之间存在冲突：当一个上升时，另一个下降，反之亦然。这被称为“精度-召回率权衡”：我们无法选择一个使精度和召回率都好的阈值。然而，我们确实有选择阈值的策略，尽管精度和召回率是冲突的。其中之一是绘制精度和召回率曲线，并查看它们的交点，然后使用这个阈值对预测进行二值化。尝试实现这个想法。
- en: Another idea for working around the precision-recall trade-off is the F1 score—a
    score that combines both precision and recall into one value. Then, to select
    the best threshold, we can simply choose the one that maximizes the F1 score.
    The formula for computing the F1 score is F1 = 2 · P · R / (P + R), where P is
    precision and R is recall. Implement this idea, and select the best threshold
    based on the F1 metric.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 另一个解决精度-召回率权衡问题的方法是F1分数——一个将精度和召回率结合成一个值的分数。然后，为了选择最佳阈值，我们可以简单地选择最大化F1分数的那个。计算F1分数的公式是F1
    = 2 · P · R / (P + R)，其中P是精度，R是召回率。实施这个想法，并根据F1指标选择最佳阈值。
- en: 'We’ve seen that precision and recall are better metrics for evaluating classification
    models than accuracy because they don’t rely on false positives, the amount of
    which could be high in imbalanced datasets. Yet, we saw later that AUC does actually
    use false positives in FPR. For very highly imbalanced cases (say, 1,000 negatives
    to 1 positive), AUC may become problematic as well. Another metric works better
    in such cases: area under the precision-recall curve, or AU PR. The precision-recall
    curve is similar to ROC, but instead of plotting FPR versus TPR, we plot recall
    on the x-axis and precision on the y-axis. Like for the ROC curve, we can also
    calculate the area under the PR curve and use it as a metric for evaluating different
    models. Try plotting the PR curves for our models, calculating the AU PR scores,
    and comparing them with those of the random model as well as the ideal model.'
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经看到，精确度和召回率是比准确率更好的分类模型评估指标，因为它们不依赖于假阳性，而在不平衡数据集中假阳性的数量可能很高。然而，我们后来发现，AUC实际上确实使用了FPR中的假阳性。对于非常不平衡的情况（例如，1,000个负例对1个正例），AUC也可能成为问题。在这种情况下，另一个指标表现得更好：精确度-召回率曲线下的面积，或称AU
    PR。精确度-召回率曲线类似于ROC曲线，但不是绘制FPR与TPR的对比，而是我们在x轴上绘制召回率，在y轴上绘制精确率。就像ROC曲线一样，我们也可以计算PR曲线下的面积，并将其用作评估不同模型的指标。尝试绘制我们模型的PR曲线，计算AU
    PR分数，并将它们与随机模型以及理想模型的分数进行比较。
- en: 'We covered K-fold cross-validation, and we used it to understand what the distribution
    of AUC scores could look like on a test dataset. When K = 10, we get 10 observations,
    which under some circumstances might not be enough. However, the idea can be extended
    to repeated K-fold cross-validation steps. The process is simple: we repeat the
    K-fold cross-validation process multiple times, each time shuffling the dataset
    differently by selecting a different random seed at each iteration. Implement
    repeated cross-validation and perform 10-fold cross-validation 10 times to see
    what the distribution of scores looks like.'
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了K折交叉验证，并使用它来了解测试数据集上AUC分数的分布可能是什么样子。当K = 10时，我们得到10个观察值，在某些情况下可能不够。然而，这个想法可以扩展到重复的K折交叉验证步骤。这个过程很简单：我们多次重复K折交叉验证过程，每次迭代通过选择不同的随机种子来以不同的方式对数据集进行洗牌。实现重复交叉验证，并进行10折交叉验证10次，以查看分数的分布情况。
- en: 4.5.2 Other projects
  id: totrans-508
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5.2 其他项目
- en: 'You can also continue with the other self-study projects from the previous
    chapter: the lead scoring project and the default prediction project. Try the
    following:'
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以继续学习前一章中的其他自学项目：线索评分项目和违约预测项目。尝试以下操作：
- en: 'Calculate all the metrics that we covered in this chapter: the confusion table,
    precision and recall, and AUC. Also try to calculate the scores from the exercises:
    the F1 score as well as AU PR (the area under the precision-recall curve).'
  id: totrans-510
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算本章中涵盖的所有指标：混淆矩阵、精确度和召回率，以及AUC。还尝试计算练习中的分数：F1分数以及AU PR（精确度-召回率曲线下的面积）。
- en: Use K-fold cross-validation to select the best parameter `C` for the model.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用K折交叉验证来选择模型的最佳参数`C`。
- en: Summary
  id: totrans-512
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A metric is a single number that can be used for evaluating the performance
    of a machine learning model. Once we choose a metric, we can use it to compare
    multiple machine learning models with each other and select the best one.
  id: totrans-513
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 指标是用于评估机器学习模型性能的单个数字。一旦我们选择了一个指标，我们就可以用它来比较多个机器学习模型，并选择最佳模型。
- en: 'Accuracy is the simplest binary classification metric: it tells us the percentage
    of correctly classified observations in the validation set. It’s easy to understand
    and compute, but it can be misleading when a dataset is imbalanced.'
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 准确率是最简单的二元分类指标：它告诉我们验证集中正确分类的观察值的百分比。它易于理解和计算，但当数据集不平衡时可能会产生误导。
- en: 'When a binary classification model makes a prediction, we have only four possible
    outcomes: true positive and true negative (correct answers) and false positive
    and false negative (incorrect answers). The confusion table arranges these outcomes
    visually so it’s easy to understand them. It gives us the foundation for many
    other binary classification metrics.'
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一个二元分类模型做出预测时，我们只有四种可能的结果：真正例和真负例（正确答案）以及假阳性和假阴性（错误答案）。混淆矩阵以视觉方式排列这些结果，使其易于理解。它为我们提供了许多其他二元分类指标的基础。
- en: Precision is the fraction of correct answers among observations for which our
    prediction is True. If we use the churn model to send promotional messages, precision
    tells us the percentage of customers who really were going to churn among everybody
    who received the message. The higher the precision, the fewer non-churning users
    we incorrectly classify as churning.
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 精确度是我们预测为真的观测值中正确答案的比例。如果我们使用流失模型发送促销信息，精确度告诉我们收到信息的所有客户中真正打算流失的客户百分比。精确度越高，我们错误地将非流失用户分类为流失的用户就越少。
- en: Recall is the fraction of correct answers among all positive observations. It
    tells us the percentage of churning customers who we correctly identified as churning.
    The higher the recall, the fewer churning customers we fail to identify.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 召回率是所有正观测值中正确答案的比例。它告诉我们我们正确识别为流失的流失客户百分比。召回率越高，我们未能识别的流失客户就越少。
- en: The ROC curve analyzes binary classification models at all the thresholds at
    once. The area under the ROC curve (AUC) tells us how well a model separates positive
    observations from negative ones. Because of its interpretability and wide applicability,
    AUC has become the default metric for evaluating binary classification models.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ROC曲线同时分析所有阈值下的二分类模型。ROC曲线下的面积（AUC）告诉我们模型将正观测值与负观测值区分得有多好。由于其可解释性和广泛适用性，AUC已成为评估二分类模型的默认指标。
- en: 'K-fold cross-validation gives us a way to use all the training data for model
    validation: we split the data into K folds and use each fold in turn as a validation
    set, and the remaining K – 1 folds are used for training. As a result, instead
    of a single number, we have K values, one for each fold. We can use these numbers
    to understand the performance of a model on average as well as to estimate how
    volatile it is across different folds.'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K折交叉验证为我们提供了一种使用所有训练数据进行模型验证的方法：我们将数据分成K个折叠，依次使用每个折叠作为验证集，其余的K-1个折叠用于训练。结果，我们得到的不是单个数字，而是K个值，每个折叠一个。我们可以使用这些数字来了解模型平均性能，以及估计它在不同折叠之间的波动性。
- en: 'K-fold cross-validation is the best way of tuning parameters and selecting
    the best model: it gives us a reliable estimate of the metric across multiple
    folds.'
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K折交叉验证是调整参数和选择最佳模型的最佳方法：它为我们提供了跨多个折叠的指标可靠估计。
- en: In the next chapter we look into deploying our model into a production environment.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨将我们的模型部署到生产环境中的方法。
- en: Answers to exercises
  id: totrans-522
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习题答案
- en: Exercise 4.1 B) A customer for whom we predicted “churn,” but they didn’t churn.
  id: totrans-523
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习4.1 B) 我们预测会“流失”的客户，但他们并没有流失。
- en: Exercise 4.2 B) The percent of customers who actually churned among the customers
    who we predicted as churning.
  id: totrans-524
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习4.2 B) 我们预测为流失的客户中实际流失的客户百分比。
- en: Exercise 4.3 A) The percent of correctly identified churned customers among
    all churned customers.
  id: totrans-525
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习4.3 A) 在所有流失客户中正确识别的流失客户百分比。
- en: Exercise 4.4 A) The ideal ranking model always scores churning customers higher
    than non-churning ones.
  id: totrans-526
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习4.4 A) 理想排名模型总是给流失客户比非流失客户更高的分数。
