- en: 4 Model serving patterns
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 模型服务模式
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Using model serving to generate predictions or make inferences on new data with
    previously trained machine learning models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用模型服务生成对新数据的预测或推理，这些数据是使用之前训练好的机器学习模型
- en: Handling model serving requests and achieving horizontal scaling with replicated
    model serving services
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理模型服务请求并使用复制的模型服务服务实现水平扩展
- en: Processing large model serving requests using the sharded services pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分片服务模式处理大量的模型服务请求
- en: Assessing model serving systems and event-driven design
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 评估模型服务系统和事件驱动设计
- en: In the previous chapter, we explored some of the challenges involved in the
    distributed training component, and I introduced a couple of practical patterns
    that can be incorporated into this component. Distributed training is the most
    critical part of a distributed machine learning system. For example, we’ve seen
    challenges when training very large machine learning models that tag main themes
    in new YouTube videos but cannot fit in a single machine. We looked at how we
    can overcome the difficulty of using the parameter server pattern. We also learned
    how to use the collective communication pattern to speed up distributed training
    for smaller models and avoid unnecessary communication overhead between parameter
    servers and workers. Last but not least, we talked about some of the vulnerabilities
    often seen in distributed machine learning systems due to corrupted datasets,
    unstable networks, and preempted worker machines and how we can address those
    problems.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们探讨了分布式训练组件中涉及的一些挑战，并介绍了几种可以集成到该组件中的实用模式。分布式训练是分布式机器学习系统中最关键的部分。例如，我们看到了在训练非常大的机器学习模型时遇到的挑战，这些模型用于标记新YouTube视频中的主要主题，但无法在一个单一机器上运行。我们探讨了如何克服使用参数服务器模式的困难。我们还学习了如何使用集体通信模式来加速较小模型的分布式训练，并避免参数服务器和工作节点之间不必要的通信开销。最后但同样重要的是，我们讨论了由于数据集损坏、网络不稳定和抢占工作机器等原因，在分布式机器学习系统中经常看到的一些漏洞，以及我们如何解决这些问题。
- en: Model serving is the next step after we have successfully trained a machine
    learning model. It is one of the essential steps in a distributed machine learning
    system. The model serving component needs to be scalable and reliable to handle
    the growing number of user requests and the growing size of individual requests.
    It’s also essential to know what tradeoffs we may see when making different design
    decisions to build a distributed model serving system.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 模型服务是在我们成功训练了一个机器学习模型之后的下一步。它是分布式机器学习系统中的关键步骤之一。模型服务组件需要具备可扩展性和可靠性，以处理不断增长的用户请求数量和单个请求的大小。了解在构建分布式模型服务系统时可能遇到的不同设计决策的权衡也是至关重要的。
- en: In this chapter, we’ll explore some of the challenges involved in distributed
    model serving systems, and I’ll introduce a few established patterns adopted heavily
    in industry. For example, we’ll see challenges when handling the increasing number
    of model serving requests and how we can overcome these challenges to achieve
    horizontal scaling with the help of replicated services. We’ll also discuss how
    the sharded services pattern can help the system process large model serving requests.
    In addition, we’ll learn how to assess model serving systems and determine whether
    event-driven design would be beneficial in real-world scenarios.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨分布式模型服务系统中涉及的一些挑战，并介绍一些在工业界广泛采用的成熟模式。例如，我们将看到在处理越来越多的模型服务请求时遇到的挑战，以及我们如何通过复制的服务来实现水平扩展来克服这些挑战。我们还将讨论分片服务模式如何帮助系统处理大量的模型服务请求。此外，我们还将学习如何评估模型服务系统，并确定在现实场景中事件驱动设计是否具有益处。
- en: 4.1 What is model serving?
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 什么是模型服务？
- en: '*Model serving* is the process of loading a previously trained machine learning
    model to generate predictions or make inferences on new input data. It’s the step
    after we’ve successfully trained a machine learning model. Figure 4.1 shows where
    model serving fits in the machine learning pipeline.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '*模型服务* 是将之前训练好的机器学习模型加载到系统中，以生成对新输入数据的预测或推理的过程。这是在我们成功训练了一个机器学习模型之后的步骤。图4.1显示了模型服务在机器学习流程中的位置。'
- en: '![04-01](../../OEBPS/Images/04-01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![04-01](../../OEBPS/Images/04-01.png)'
- en: Figure 4.1 A diagram showing where model serving fits in the machine learning
    pipeline
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 展示了模型服务在机器学习流程中的位置
- en: Note that model serving is a general concept that appears in distributed and
    traditional machine learning applications. In traditional machine learning applications,
    model serving is usually a single program that runs on a local desktop or machine
    and generates predictions on new datasets that are not used for model training.
    Both the dataset and the machine learning model used should be small enough to
    fit on a single machine for traditional model serving, and they are stored in
    the local disk of a single machine.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型服务是一个通用概念，它出现在分布式和传统机器学习应用中。在传统机器学习应用中，模型服务通常是一个在本地桌面或机器上运行的单一程序，并为未用于模型训练的新数据集生成预测。用于模型服务的数据集和机器学习模型应该足够小，可以适应单个机器，并且存储在单个机器的本地磁盘上。
- en: In contrast, distributed model serving usually happens in a cluster of machines.
    Both the dataset and the trained machine learning model used for model serving
    can be very large and must be stored in a remote distributed database or partitioned
    on disks of multiple machines. The differences between traditional model serving
    and distributed model serving systems is summarized in table 4.1.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，分布式模型服务通常发生在机器集群中。用于模型服务的数据集和训练好的机器学习模型可以非常大，必须存储在远程分布式数据库中或在多台机器的磁盘上分区。传统模型服务和分布式模型服务系统之间的差异总结在表4.1中。
- en: Table 4.1 Comparison between traditional model serving and distributed model
    serving systems
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.1 传统模型服务和分布式模型服务系统之间的比较
- en: '|  | Traditional model serving | Distributed model serving |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | 传统模型服务 | 分布式模型服务 |'
- en: '| Computational resources | Personal laptop or single remote server | Cluster
    of machines |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| 计算资源 | 个人笔记本电脑或单个远程服务器 | 机器集群 |'
- en: '| Dataset location | Local disk on a single laptop or machine | Remote distributed
    database or partitioned on disks of multiple machines |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| 数据集位置 | 单个笔记本电脑或机器的本地磁盘 | 远程分布式数据库或多个机器的磁盘分区 |'
- en: '| Size of model and dataset | Small enough to fit on a single machine | Medium
    to large |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 模型与数据集大小 | 足够小，可以适应单个机器 | 中等到大型 |'
- en: It’s nontrivial to build and manage a distributed model serving system that’s
    scalable, reliable, and efficient for different use cases. We will examine a couple
    of use cases as well as some established patterns that may address different challenges.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 构建和管理一个可扩展、可靠且高效的分布式模型服务系统，适用于不同的用例，并非易事。我们将检查几个用例以及一些可能解决不同挑战的既定模式。
- en: '4.2 Replicated services pattern: Handling the growing number of serving requests'
  id: totrans-21
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 复制服务模式：处理不断增长的服务请求
- en: As you may recall, in the previous chapter, we built a machine learning model
    to tag the main themes of new videos that the model hasn’t seen before using the
    YouTube-8M dataset ([http://research.google.com/youtube8m/](http://research.google.com/youtube8m/)),
    which consists of millions of YouTube video IDs, with high-quality machine-generated
    annotations from a diverse vocabulary of 3,800+ visual entities such as Food,
    Car, Music, etc. A screenshot of what the videos in the YouTube-8M dataset look
    like is shown in Figure 4.2.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所忆，在前一章中，我们使用YouTube-8M数据集（[http://research.google.com/youtube8m/](http://research.google.com/youtube8m/)）构建了一个机器学习模型，以标记模型之前未见过的视频的主要主题。YouTube-8M数据集包含数百万个YouTube视频ID，以及来自3,800多个视觉实体（如食物、汽车、音乐等）的优质机器生成注释。YouTube-8M数据集中视频的截图如图4.2所示。
- en: '![04-02](../../OEBPS/Images/04-02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![04-02](../../OEBPS/Images/04-02.png)'
- en: 'Figure 4.2 A screenshot of what the videos in the YouTube-8M dataset look like.
    (Source: Sudheendra Vijayanarasimhan et al. Licensed under Nonexclusive License
    1.0)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 YouTube-8M数据集中视频的截图。（来源：Sudheendra Vijayanarasimhan等人。许可协议：非独占许可1.0）
- en: Now we would like to build a model serving system that allows users to upload
    new videos. Then, the system would load the previously trained machine learning
    model to tag entities/themes that appear in the uploaded videos. Note that the
    model serving system is stateless, so users’ requests won’t affect the model serving
    results.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们想构建一个模型服务系统，允许用户上传新视频。然后，系统将加载之前训练好的机器学习模型，以标记上传视频中出现的实体/主题。请注意，模型服务系统是无状态的，因此用户的请求不会影响模型服务结果。
- en: The system basically takes the videos uploaded by users and sends requests to
    the model server. The model server then retrieves the previously trained entity-tagging
    machine learning model from the model storage to process the videos and eventually
    generate possible entities that appear in the videos. A high-level overview of
    the system is shown in figure 4.3.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 系统基本上是接收用户上传的视频，并向模型服务器发送请求。模型服务器随后从模型存储中检索先前训练好的实体标注机器学习模型来处理视频，并最终生成视频中可能出现的实体。系统的高级概述如图4.3所示。
- en: '![04-03](../../OEBPS/Images/04-03.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![04-03](../../OEBPS/Images/04-03.png)'
- en: Figure 4.3 A high-level architecture diagram of the single-node model serving
    system
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 单节点模型服务系统的总体架构图
- en: Note that this initial version of the model server only runs on a single machine
    and responds to model serving requests from users on a first-come, first-served
    basis, as shown in figure 4.4\. This approach may work well if only very few users
    are testing the system. However, as the number of users or model serving requests
    increases, users will experience huge delays while waiting for the system to finish
    processing any previous requests. In the real world, this bad user experience
    would immediately lose our users’ interest in engaging with this system.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这个模型服务器的初始版本仅在单台机器上运行，并按先到先得的原则响应用户的模型服务请求，如图4.4所示。如果只有极少数用户在测试系统，这种方法可能效果很好。然而，随着用户数量或模型服务请求的增加，用户在等待系统完成处理任何先前请求时将经历巨大的延迟。在现实世界中，这种糟糕的用户体验会立即失去用户对参与该系统的兴趣。
- en: '![04-04](../../OEBPS/Images/04-04.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![04-04](../../OEBPS/Images/04-04.png)'
- en: Figure 4.4 The model server only runs on a single machine and responds to model
    serving requests from users on a first-come, first-served basis.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 模型服务器仅在单台机器上运行，并按先到先得的原则响应用户的模型服务请求。
- en: 4.2.1 The problem
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 问题
- en: The system takes the videos uploaded by users and then sends the requests to
    the model server. These model serving requests are queued and must wait to be
    processed by the model server.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 系统接收用户上传的视频，然后向模型服务器发送请求。这些模型服务请求被排队，必须等待由模型服务器处理。
- en: Unfortunately, due to the nature of the single-node model server, it can only
    effectively serve a limited number of model serving requests on a first-come,
    first-served basis. As the number of requests grows in the real world, the user
    experience worsens when users must wait a long time to receive the model serving
    result. All requests are waiting to be processed by the model serving system,
    but the computational resources are bound to this single node. Is there a better
    way to handle model serving requests than sequentially?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，由于单节点模型服务器的特性，它只能基于先到先得的原则有效地处理有限数量的模型服务请求。随着实际应用中请求数量的增长，当用户必须等待很长时间才能收到模型服务结果时，用户体验会变差。所有请求都在等待由模型服务系统处理，但计算资源都绑定在这个单节点上。是否有比顺序处理更好的处理模型服务请求的方法？
- en: 4.2.2 The solution
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 解决方案
- en: One fact we’ve neglected is that the existing model server is stateless, meaning
    that the model serving results for each request aren’t affected by other requests,
    and the machine learning model can only process a single request. In other words,
    the model server doesn’t require a saved state to operate correctly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们忽略的一个事实是，现有的模型服务器是无状态的，这意味着每个请求的模型服务结果不受其他请求的影响，机器学习模型只能处理单个请求。换句话说，模型服务器不需要保存状态来正确运行。
- en: Since the model server is stateless, we can add more server instances to help
    handle additional user requests without the requests interfering with each other,
    as shown in figure 4.5\. These additional model server instances are exact copies
    of the original model server but with different server addresses, and each handles
    different model serving requests. In other words, they are *replicated services*
    for model serving or, in short, *model server replicas*.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于模型服务器是无状态的，我们可以添加更多服务器实例来帮助处理额外的用户请求，而不会相互干扰，如图4.5所示。这些额外的模型服务器实例是原始模型服务器的精确副本，但具有不同的服务器地址，并且每个处理不同的模型服务请求。换句话说，它们是模型服务的*复制服务*，或者简称为*模型服务器副本*。
- en: '![04-05](../../OEBPS/Images/04-05.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![04-05](../../OEBPS/Images/04-05.png)'
- en: Figure 4.5 Additional server instances help handle additional user requests
    without the requests interfering with each other.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 额外的服务器实例帮助处理额外的用户请求，而不会相互干扰。
- en: Adding additional resources into our system with more machines is called *horizontal
    scaling*. Horizontal scaling systems handle more and more users or traffic by
    adding more replicas. The opposite of horizontal scaling is *vertical scaling*,
    which is usually implemented by adding computational resources to existing machines.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 将更多机器添加到我们的系统中称为*水平扩展*。水平扩展系统通过添加更多副本来处理越来越多的用户或流量。与水平扩展相反的是*垂直扩展*，这通常是通过向现有机器添加计算资源来实现的。
- en: 'An analogy: Horizontal scaling vs. vertical scaling'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 类比：水平扩展与垂直扩展
- en: You can think of vertical scaling like retiring your sports car and buying a
    race car when you need more horsepower. While a race car is fast and looks amazing,
    it’s also expensive and not very practical, and at the end of the day, they can
    only take you so far before running out of gas. In addition, there’s only one
    seat, and the car must be driven on a flat surface. It is really only suitable
    for racing.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将垂直扩展想象为当你需要更多动力时，退役你的跑车并购买一辆赛车。虽然赛车速度快，外观惊人，但它也很昂贵，并不实用，最终，它们在耗尽燃料之前只能带你走这么远。此外，只有一个座位，汽车必须在平坦的路面上驾驶。它实际上只适合赛车。
- en: Horizontal scaling gets you that added horsepower--not by favoring sports cars
    over race cars, but by adding another vehicle to the mix. In fact, you can think
    of horizontal scaling like several vehicles that could fit a lot of passengers
    at once. Maybe none of these machines is a race car, but none of them need to
    be--across the fleet, you have all the horsepower you need.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 水平扩展为你提供了额外的动力——不是通过偏爱跑车而不是赛车，而是通过添加另一种车辆到混合中。实际上，你可以将水平扩展想象为几辆车，可以一次容纳很多乘客。也许这些机器中没有一辆是赛车，但它们都不需要是——在整个车队中，你拥有你需要的所有动力。
- en: Let’s return to our original model serving system, which takes the videos uploaded
    by users and sends requests to the model server. Unlike our previous design of
    the model serving system, the system now has multiple model server replicas to
    process the model serving requests asynchronously. Each model server replica takes
    a single request, retrieves the previously trained entity-tagging machine learning
    model from model storage, and then processes the videos in the request to tag
    possible entities in the videos.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的原始模型服务系统，该系统处理用户上传的视频并向模型服务器发送请求。与我们的先前模型服务系统设计不同，现在的系统具有多个模型服务器副本，用于异步处理模型服务请求。每个模型服务器副本接收单个请求，从模型存储中检索先前训练的实体标注机器学习模型，然后处理请求中的视频以标记视频中的可能实体。
- en: As a result, we’ve successfully scaled up our model server by adding model server
    replicas to the existing model serving system. The new architecture is shown in
    figure 4.6\. The model server replicas are capable of handling many requests at
    a time since each replica can process individual model serving requests independently.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们通过向现有的模型服务系统中添加模型服务器副本，成功地扩展了我们的模型服务器。新的架构如图 4.6 所示。模型服务器副本能够同时处理许多请求，因为每个副本可以独立处理单个模型服务请求。
- en: '![04-06](../../OEBPS/Images/04-06.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![04-06](../../OEBPS/Images/04-06.png)'
- en: Figure 4.6 The system architecture after we’ve scaled up our model server by
    adding model server replicas to the system
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.6 在我们通过向系统中添加模型服务器副本来扩展我们的模型服务器后，的系统架构
- en: In the new architecture, multiple model serving requests from users are sent
    to the model server replicas at the same time. However, we haven’t discussed how
    they are being distributed and processed. For example, which request is being
    processed by which model server replica? In other words, we haven’t yet defined
    a clear mapping relationship between the requests and the model server replicas.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在新的架构中，来自用户的多个模型服务请求同时发送到模型服务器副本。然而，我们还没有讨论它们是如何被分配和处理的。例如，哪个请求正在由哪个模型服务器副本处理？换句话说，我们还没有定义请求和模型服务器副本之间的明确映射关系。
- en: To do that, we can add another layer--namely, a *load balancer*, which handles
    the distribution of model serving requests among the replicas. For example, the
    load balancer takes multiple model serving requests from our users and then distributes
    the requests evenly to each of the model server replicas, which then are responsible
    for processing individual requests, including model retrieval and inference on
    the new data in the request. Figure 4.7 illustrates this process.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 为了做到这一点，我们可以添加另一层——即负载均衡器，它负责在副本之间分配模型服务请求。例如，负载均衡器从我们的用户那里接收多个模型服务请求，然后将请求均匀地分配给每个模型服务器副本，这些副本随后负责处理单个请求，包括检索模型和在请求中的新数据上进行推理。图4.7说明了这个过程。
- en: '![04-07](../../OEBPS/Images/04-07.png)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![04-07](../../OEBPS/Images/04-07.png)'
- en: Figure 4.7 A diagram showing how a loader balancer is used to distribute the
    requests evenly across model server replicas
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 展示了负载均衡器如何用于在模型服务器副本之间均匀分配请求
- en: The load balancer uses different algorithms to decide which request goes to
    which model server replica. Example algorithms for load balancing include round
    robin, the least connection method, hashing, etc.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器使用不同的算法来决定哪个请求发送到哪个模型服务器副本。负载均衡的示例算法包括轮询、最少连接方法、哈希等。
- en: Round robin for load balancing
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询负载均衡
- en: Round robin is a simple technique in which the load balancer forwards each request
    to a different server replica based on a rotating list.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 轮询是一种简单的技术，其中负载均衡器根据旋转列表将每个请求转发到不同的服务器副本。
- en: Even though it’s easy to implement a load balancer with the round-robin algorithm,
    the load is already on a load balancer server, and it might be dangerous if the
    load balancer server itself receives a lot of requests that require expensive
    processing. It may become overloaded past the point it can effectively do its
    job.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然使用轮询算法实现负载均衡器很简单，但负载已经位于负载均衡服务器上，如果负载均衡服务器本身接收到大量需要昂贵处理请求，可能会变得危险。它可能会超出其有效工作的能力而超载。
- en: The replicated services pattern provides a great way to scale our model serving
    system horizontally. It can also be generalized for any systems that serve a large
    amount of traffic. Whenever a single instance cannot handle the traffic, introducing
    this pattern ensures that all traffic can be handled equivalently and efficiently.
    We’ll apply this pattern in section 9.3.2.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 复制服务模式为我们提供了横向扩展模型服务系统的绝佳方式。它也可以推广到任何需要处理大量流量的系统。每当单个实例无法处理流量时，引入这种模式可以确保所有流量都能等效且高效地处理。我们将在第9.3.2节中应用此模式。
- en: 4.2.3 Discussion
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.3 讨论
- en: Now that we have load-balanced model server replicas in place, we should be
    able to support the growing number of user requests, and the entire model serving
    system achieves horizontal scaling. Not only can we handle model serving requests
    in a scalable way, but the overall model serving system also becomes *highly available*
    ([https://mng.bz/EQBd](https://mng.bz/EQBd)). High availability is a characteristic
    of a system that maintains an agreed-on level of operational performance, usually
    uptime, for a longer-than-normal period. It’s often expressed as a percentage
    of uptime in a given year.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经设置了负载均衡的模型服务器副本，我们应该能够支持不断增长的用户请求，整个模型服务系统实现横向扩展。我们不仅能够以可扩展的方式处理模型服务请求，而且整个模型服务系统也变得*高度可用*
    ([https://mng.bz/EQBd](https://mng.bz/EQBd))。高可用性是系统在超过正常时间保持协议的操作性能（通常是正常运行时间）的特征。它通常以一年内正常运行时间的百分比来表示。
- en: For example, some organizations may require services to reach a highly available
    service-level agreement, which means the service is up and running 99.9% of the
    time (known as three-nines availability). In other words, the service can only
    get 1.4 minutes of downtime per day (24 hours × 60 minutes × 0.1%). With the help
    of replicated model services, if any of the model server replicas crashes or gets
    preempted on a spot instance, the remaining model server replicas are still available
    and ready to process any incoming model serving requests from users, which provides
    a good user experience and makes the system reliable.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一些组织可能需要达到高度可用的服务级别协议，这意味着服务99.9%的时间都在运行（称为三九可用性）。换句话说，服务每天只能有1.4分钟的停机时间（24小时×60分钟×0.1%）。在复制模型服务的帮助下，如果任何模型服务器副本崩溃或在即时实例上被抢占，剩余的模型服务器副本仍然可用并准备好处理来自用户的任何模型服务请求，这提供了良好的用户体验并使系统可靠。
- en: In addition, since our model server replicas will need to retrieve previously
    trained machine learning models from a remote model storage, they need to be *ready*
    in addition to being *alive*. It’s important to build and deploy *readiness probes*
    to inform the load balancer that the replicas are all successfully established
    connections to the remote model storage and are ready to serve model serving requests
    from users. A readiness probe helps the system determine whether a particular
    replica is ready to serve. With readiness probes, users do not experience unexpected
    behaviors when the system is not ready due to internal system problems.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于我们的模型服务器副本需要从远程模型存储检索先前训练的机器学习模型，它们除了需要处于“活跃”状态外，还需要处于“就绪”状态。构建和部署“就绪探测”对于通知负载均衡器副本已成功建立与远程模型存储的连接并准备好为用户提供模型服务请求非常重要。就绪探测有助于系统确定特定副本是否就绪。有了就绪探测，当系统因内部系统问题而未就绪时，用户不会遇到意外的行为。
- en: The replicated services pattern addresses our horizontal scalability problem
    that prevents our model serving system from supporting a large number of model
    serving requests. However, in real-world model serving systems, not only the number
    of serving requests increases but also the size of each request, which can get
    extremely large if the data or the payload is large. In that case, replicated
    services may not be able to handle the large requests. We will talk about that
    scenario and introduce a pattern that would alleviate the problem in the next
    section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 复制服务模式解决了我们的水平扩展问题，防止我们的模型服务系统支持大量的模型服务请求。然而，在实际的模型服务系统中，不仅服务请求的数量增加，每个请求的大小也在增加，如果数据或有效负载很大，这个大小可能会变得非常大。在这种情况下，复制服务可能无法处理大请求。我们将在下一节中讨论这种情况，并介绍一种可以缓解问题的模式。
- en: 4.2.4 Exercises
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.4 练习
- en: Are replicated model servers stateless or stateful?
  id: totrans-63
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制的模型服务器是无状态的还是有状态的？
- en: What happens when we don’t have a load balancer as part of the model serving
    system?
  id: totrans-64
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当模型服务系统中没有负载均衡器时会发生什么？
- en: Can we achieve three-nines service-level agreements with only one model server
    instance?
  id: totrans-65
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们能否仅使用一个模型服务器实例就实现三个九的服务级别协议？
- en: 4.3 Sharded services pattern
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 分片服务模式
- en: The replicated services pattern efficiently resolves our horizontal scalability
    problem so that our model serving system can support a growing number of user
    requests. We achieve the additional benefit of high availability with the help
    of model server replicas and a load balancer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 复制服务模式有效地解决了我们的水平扩展问题，从而使我们的模型服务系统可以支持越来越多的用户请求。借助模型服务器副本和负载均衡器的帮助，我们还获得了高可用性的额外好处。
- en: Note Each model server replica has a limited and pre-allocated amount of computational
    resources. More important, the amount of computational resources for each replica
    must be identical for the load balancer to distribute requests correctly and evenly.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：每个模型服务器副本都有有限的、预先分配的计算资源。更重要的是，每个副本的计算资源量必须相同，以便负载均衡器可以正确且均匀地分配请求。
- en: Next, let’s imagine that a user wants to upload a high-resolution YouTube video
    that needs to be tagged with an entity using the model server application. Even
    though the high-resolution video is too large, it may be uploaded successfully
    to the model server replica if it has sufficient disk storage. However, we could
    not process the request in any of the individual model server replicas themselves
    since processing this single large request would require a larger memory allocated
    in the model server replica. This need for a large amount of memory is often due
    to the complexity of the trained machine learning model, as it may contain a lot
    of expensive matrix computations or mathematical operations, as we’ve seen in
    the previous chapter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们想象一个用户想要上传一个需要使用模型服务器应用程序标记实体的高分辨率YouTube视频。尽管高分辨率视频太大，但如果模型服务器副本有足够的磁盘存储，它可能仍然可以成功上传到模型服务器副本。然而，我们无法在任何单个模型服务器副本中处理请求，因为处理这个单个大请求需要在模型服务器副本中分配更多的内存。这种对大量内存的需求通常是由于训练的机器学习模型的复杂性，因为它可能包含大量的昂贵矩阵计算或数学运算，正如我们在上一章中看到的。
- en: For instance, a user uploads a high-resolution video to the model serving system
    through a large request. One of the model server replicas takes this request and
    successfully retrieves the previously trained machine learning model. Unfortunately,
    the model then fails to process the large data in the request since the model
    server replica that’s responsible for processing this request does not have sufficient
    memory. Eventually, we may notify the user of this failure after they have waited
    a long time, which results in a bad user experience. A diagram for this situation
    is shown in figure 4.8.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个用户通过大请求将高分辨率视频上传到模型提供系统。其中一个模型服务器副本接收这个请求并成功检索先前训练的机器学习模型。不幸的是，由于负责处理此请求的模型服务器副本没有足够的内存，该模型随后无法处理请求中的大量数据。最终，我们可能在用户等待很长时间后通知他们此失败，这导致糟糕的用户体验。这种情况的示意图如图4.8所示。
- en: '![04-08](../../OEBPS/Images/04-08.png)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![04-08](../../OEBPS/Images/04-08.png)'
- en: Figure 4.8 A diagram showing that model fails to process the large data in the
    request since the model server replica responsible for processing this request
    does not have sufficient memory
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 一个示意图，显示由于负责处理此请求的模型服务器副本没有足够的内存，模型无法处理请求中的大量数据
- en: '4.3.1 The problem: Processing large model serving requests with high-resolution
    videos'
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 问题：处理高分辨率视频的大模型提供请求
- en: The requests the system is serving are large since the videos users upload are
    high resolution. In cases where the previously trained machine learning model
    may contain expensive mathematical operations, these large video requests cannot
    be successfully processed and served by individual model server replicas with
    a limited amount of memory. How do we design the model serving system to handle
    large requests of high-resolution videos successfully?
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 系统正在处理的大请求是因为用户上传的视频具有高分辨率。在先前训练的机器学习模型可能包含昂贵的数学运算的情况下，这些大视频请求无法由具有有限内存的个别模型服务器副本成功处理和提供。我们如何设计模型提供系统以成功处理高分辨率视频的大请求？
- en: 4.3.2 The solution
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 解决方案
- en: Given our requirement for the computational resources on each model server replica,
    can we scale vertically by increasing each replica’s computational resources so
    it can handle large requests like high-resolution videos? Since we are vertically
    scaling all the replicas by the same amount, we will not affect our load balancer’s
    work.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到我们对每个模型服务器副本的计算资源需求，我们能否通过增加每个副本的计算资源来垂直扩展，以便它可以处理像高分辨率视频这样的大请求？由于我们通过相同的数量垂直扩展所有副本，因此我们不会影响负载均衡器的工作。
- en: Unfortunately, we cannot simply scale the model server replicas vertically since
    we don’t know how many such large requests there are. Imagine only a couple of
    users have high-resolution videos needing to be processed (e.g., professional
    photographers who have high-end cameras that capture high-resolution videos),
    and the remaining vast majority of the users only upload videos from their smartphones
    with much smaller resolutions. As a result, most of the added computational resources
    on the model server replicas are idling, which results in very low resource utilization.
    We will examine the resource utilization perspective in the next section, but
    for now, we know that this approach is not practical.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们不知道有多少这样的请求，所以我们不能简单地垂直扩展模型服务器副本。想象一下，只有少数用户需要处理高分辨率视频（例如，使用高端相机捕获高分辨率视频的专业摄影师），而剩余的绝大多数用户仅上传来自智能手机的视频，分辨率要小得多。因此，大多数添加到模型服务器副本上的计算资源都是闲置的，这导致资源利用率非常低。我们将在下一节检查资源利用率，但到目前为止，我们知道这种方法是不切实际的。
- en: Remember we introduced the parameter server pattern in chapter 3, which allows
    us to partition a very large model? Figure 4.9 is the diagram we discussed in
    chapter 3 that shows distributed model training with multiple parameter servers;
    the large model has been partitioned, and each partition is located on different
    parameter servers. Each worker node takes a subset of the dataset, performs calculations
    required in each neural network layer, and then sends the calculated gradients
    to update one model partition stored in one of the parameter servers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 记得我们在第3章中介绍了参数服务器模式，它允许我们将一个非常大的模型分区？图4.9是我们在第3章中讨论的示意图，展示了具有多个参数服务器的分布式模型训练；大模型已经被分区，每个分区位于不同的参数服务器上。每个工作节点获取数据集的一个子集，执行每个神经网络层所需的计算，然后将计算出的梯度发送到更新存储在参数服务器中的一个模型分区。
- en: '![04-09](../../OEBPS/Images/04-09.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![04-09](../../OEBPS/Images/04-09.png)'
- en: Figure 4.9 Distributed model training with multiple parameter servers where
    the large model has been sharded and each partition is located on different parameter
    servers
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了具有多个参数服务器的分布式模型训练，其中大模型已经被分割，每个分区位于不同的参数服务器上。
- en: To deal with our problem of large model serving requests, we can borrow the
    same idea and apply it to our particular scenario.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理我们的大模型服务请求问题，我们可以借用同样的想法并将其应用于我们的特定场景。
- en: We first divide the original high-resolution video into multiple separate videos,
    and then each video is processed by multiple *model server shards* independently.
    The model server shards are partitions from a single model server instance, and
    each is responsible for processing a subset of a large request.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将原始的高分辨率视频分割成多个单独的视频，然后每个视频由多个独立的*模型服务器碎片*分别处理。模型服务器碎片是从单个模型服务器实例中划分出来的，每个碎片负责处理大量请求的一个子集。
- en: The diagram in figure 4.10 is an example architecture of the *sharded services
    pattern*. In the diagram, a high-resolution video that contains a dog and a kid
    gets divided into two separate videos where each of the videos represents a subset
    of the original large request. One of the separated videos contains the part where
    the dog appears, and the other video contains the part where the kid appears.
    These two separated videos become two separate requests and are processed by different
    model server shards independently.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10中的图示是*分割服务模式*的一个示例架构。在该图中，包含狗和小孩的高分辨率视频被分割成两个单独的视频，每个视频代表原始大请求的一个子集。其中一个分割的视频包含狗出现的那部分，另一个视频包含小孩出现的那部分。这两个分割的视频成为两个单独的请求，并由不同的模型服务器碎片独立处理。
- en: '![04-10](../../OEBPS/Images/04-10.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![04-10](../../OEBPS/Images/04-10.png)'
- en: Figure 4.10 An example architecture of the sharded services pattern where a
    high-resolution video gets divided into two separate videos. Each video represents
    a subset of the original large request and is processed by different model server
    shard independently.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10展示了分割服务模式的一个示例架构，其中高分辨率视频被分割成两个单独的视频。每个视频代表原始大请求的一个子集，并由不同的模型服务器碎片独立处理。
- en: After the model server shards receive the sub-requests where each contains part
    of the original large model serving request, each model server shard then retrieves
    the previously trained entity-tagging machine learning model from model storage
    and then processes the videos in the request to tag possible entities that appear
    in the videos, similar to the previous model serving system we’ve designed. Once
    all the sub-requests have been processed by each of the model server shards, we
    merge the model inference result from two sub-requests--namely, the two entities,
    dog and kid--to obtain a result for the original large model serving request with
    the high-resolution video.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型服务器碎片接收到包含原始大模型服务请求一部分的子请求后，每个模型服务器碎片随后从模型存储中检索之前训练好的实体标注机器学习模型，然后处理请求中的视频以标注视频中可能出现的实体，类似于我们之前设计的模型服务系统。一旦每个模型服务器碎片都处理了所有子请求，我们将两个子请求（即两个实体，狗和小孩）的模型推理结果合并，以获得原始大模型服务请求的高分辨率视频的结果。
- en: How do we distribute the two sub-requests to different model server shards?
    Similar to the algorithms we use to implement the load balancer, we can use a
    *sharding function*, which is very similar to a hashing function, to determine
    which shard in the list of model server shards should be responsible for processing
    each sub-request.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何将两个子请求分配给不同的模型服务器碎片？类似于我们用来实现负载均衡器的算法，我们可以使用一个*分割函数*，它与哈希函数非常相似，以确定模型服务器碎片列表中的哪个碎片应该负责处理每个子请求。
- en: Usually, the sharding function is defined using a hashing function and the modulo
    (%) operator. For example, hash(request) % 10 would return 10 shards even when
    the outputs of the hash function are significantly larger than the number of shards
    in a sharded service.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分割函数使用哈希函数和取模（%）运算符定义。例如，hash(request) % 10会在哈希函数的输出显著大于分割服务中的碎片数量时，返回10个碎片。
- en: Characteristics of hashing functions for sharding
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 分割的哈希函数特性
- en: 'The hashing function that defines the sharding function transforms an arbitrary
    object into an integer representing a particular shard index. It has two important
    characteristics:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 定义分割函数的哈希函数将任意对象转换为一个表示特定碎片索引的整数。它有两个重要的特性：
- en: The output from hashing is always the same for a given input.
  id: totrans-91
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哈希的输出对于给定的输入始终相同。
- en: The distribution of outputs is always uniform within the output space.
  id: totrans-92
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出的分布总是在输出空间内均匀。
- en: These characteristics are important and can ensure that a particular request
    will always be processed by the same shard server and that the requests are evenly
    distributed among the shards.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 这些特性很重要，可以确保特定的请求始终由同一个分片服务器处理，并且请求在分片之间均匀分布。
- en: 'The sharded services pattern solves the problem we encounter when building
    model serving systems at scale and provides a great way to handle large model
    serving requests. It’s similar to the data-sharding pattern we introduced in chapter
    2: instead of applying sharding to datasets, we apply sharding to model serving
    requests. When a distributed system has limited computational resources for a
    single machine, we can apply this pattern to offload the computational burden
    to multiple machines.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 分片服务模式解决了我们在构建大规模模型服务系统时遇到的问题，并提供了一种处理大型模型服务请求的极好方式。它与我们在第2章中介绍的数据分片模式类似：我们不是将分片应用于数据集，而是将分片应用于模型服务请求。当一个分布式系统为单个机器有限的计算资源时，我们可以应用此模式将计算负担卸载到多台机器上。
- en: 4.3.3 Discussion
  id: totrans-95
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.3 讨论
- en: The sharded services pattern helps handle large requests and efficiently distributes
    the workload of processing large model serving requests to multiple model server
    shards. It’s generally useful when considering any sort of service where the data
    exceeds what can fit on a single machine.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 分片服务模式有助于处理大量请求，并有效地将处理大型模型服务请求的工作负载分配到多个模型服务器分片中。在考虑任何数据量超过单台机器可容纳的数据的服务时，通常很有用。
- en: However, unlike the replicated services pattern we discussed in the previous
    section, which is useful when building stateless services, the sharded services
    pattern is generally used for building stateful services. In our case, we need
    to maintain the state or the results from serving the sub-requests from the original
    large request using sharded services and then merge the results into the final
    response so it includes all entities from the original high-resolution video.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，与我们在上一节中讨论的复制的服务模式不同，后者在构建无状态服务时很有用，分片服务模式通常用于构建有状态的服务。在我们的情况下，我们需要维护状态或从原始大请求使用分片服务处理子请求的结果，然后将结果合并到最终响应中，以便它包含原始高分辨率视频中的所有实体。
- en: In some cases, this approach may not work well because it depends on how we
    divide the original large request into smaller requests. For example, if the original
    video has been divided into more than two sub-requests, some may not be meaningful
    since they don’t contain any complete entities that are recognizable by the machine
    learning model we’ve trained. For situations like that, we need additional handling
    and cleaning of the merged result to remove meaningless entities that are not
    useful to our application.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这种方法可能不起作用，因为它取决于我们如何将原始的大请求分割成更小的请求。例如，如果原始视频已经被分割成超过两个子请求，其中一些可能没有意义，因为它们不包含任何机器学习模型可以识别的完整实体。对于这种情况，我们需要额外的处理和清理合并后的结果，以移除对应用程序无用的无意义实体。
- en: Both the replicated services pattern and sharded services pattern are valuable
    when building a model serving system at scale to handle a great number of large
    model serving requests. However, to incorporate them into the model serving system,
    we need to know the required computational resources at hand, which may not be
    available if the traffic is rather dynamic. In the next section, I will introduce
    another pattern focusing on model serving systems that can handle dynamic traffic.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在大规模构建模型服务系统以处理大量大型模型服务请求时，复制的服务模式和分片服务模式都很有价值。然而，要将它们纳入模型服务系统，我们需要了解手头可用的计算资源，如果流量相对动态，这些资源可能不可用。在下一节中，我将介绍另一种模式，该模式专注于可以处理动态流量的模型服务系统。
- en: 4.3.4 Exercises
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.4 练习
- en: Would vertical scaling be helpful when handling large requests?
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在处理大量请求时，垂直扩展会有帮助吗？
- en: Are the model server shards stateful or stateless?
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型服务器分片是有状态的还是无状态的？
- en: 4.4 The event-driven processing pattern
  id: totrans-103
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 事件驱动处理模式
- en: The replicated services pattern we examined in section 4.2 helps handle a large
    number of model serving requests, and the sharded services pattern in section
    4.3 can be used to process very large requests that may not fit in a single model
    server instance. While these patterns address the challenges of building model
    serving systems at scale, they are more suitable when the system knows how much
    computational resources, model server replicas, or model server shards to allocate
    before the system starts taking user requests. However, for cases in which we
    do not know how much model serving traffic the system will be receiving, it’s
    hard to allocate and use resources efficiently.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在4.2节中考察的复制服务模式有助于处理大量的模型服务请求，而4.3节中的分片服务模式可以用来处理可能不适合单个模型服务器实例的非常大的请求。尽管这些模式解决了在规模上构建模型服务系统的挑战，但它们更适合在系统启动接收用户请求之前就知道需要分配多少计算资源、模型服务器副本或模型服务器分片时使用。
- en: Now imagine that we work for a company that provides holiday and event planning
    services to subscribed customers. We’d like to provide a new service that will
    use a trained machine learning model to predict hotel prices per night for the
    hotels located in resort areas, given a range of dates and a specific location
    where our customers would like to spend their holidays.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们为一家为订阅客户提供假日和活动规划服务的公司工作。我们希望提供一个新服务，该服务将使用训练好的机器学习模型来预测位于度假区的酒店每晚的价格，前提是给定日期范围和客户希望度假的具体地点。
- en: To provide that service, we can design a machine learning model serving system.
    This model serving system provides a user interface where users can enter the
    range of dates and locations they are interested in staying for holidays. Once
    the requests are sent to the model server, the previously trained machine learning
    model will be retrieved from the distributed database and process the data in
    the requests (dates and locations). Eventually, the model server will return the
    predicted hotel prices for each location within the given date range. The complete
    process is shown in figure 4.11.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提供这项服务，我们可以设计一个机器学习模型服务系统。这个模型服务系统提供了一个用户界面，用户可以在其中输入他们感兴趣的度假日期和地点范围。一旦请求发送到模型服务器，之前训练好的机器学习模型将从分布式数据库中检索出来，并处理请求中的数据（日期和地点）。最终，模型服务器将返回给定日期范围内每个地点的预测酒店价格。整个过程如图4.11所示。
- en: '![04-11](../../OEBPS/Images/04-11.png)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![04-11](../../OEBPS/Images/04-11.png)'
- en: Figure 4.11 A diagram of the model serving system to predict hotel prices
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11 预测酒店价格的模型服务系统图示
- en: After we test this model serving system for one year on selected customers,
    we will have collected sufficient data to plot the model serving traffic over
    time. As it turns out, people prefer to book their holidays at the last moment,
    so traffic increases abruptly shortly before holidays and then decreases again
    after the holiday periods. The problem with this traffic pattern is that it introduces
    a very low resource utilization rate.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们对选定客户测试这个模型服务系统一年后，我们将收集足够的数据来绘制模型服务流量随时间的变化图。结果证明，人们倾向于在假期最后一刻预订假期，因此在假期前交通量突然增加，然后在假期结束后再次减少。这种流量模式的问题在于它引入了非常低的资源利用率。
- en: 'In our current architecture of model serving system, the underlying computational
    resources allocated to the model remain unchanged at all times. This strategy
    seems far from optimal: during periods of low traffic, most of our resources are
    idling and thus wasted, whereas during periods of high traffic, our system struggles
    to respond in a timely fashion, and more resources than normal are required to
    operate. In other words, the system has to deal with either high or low traffic
    with the same amount of computational resources (e.g., 10 CPUs and 100 GB of memory),
    as shown in figure 4.12.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的模式服务系统架构中，分配给模型的底层计算资源始终保持不变。这种策略似乎远非最佳：在流量低峰期，我们的大部分资源都在闲置，因此被浪费了，而在流量高峰期，我们的系统难以及时响应，需要比正常情况下更多的资源来运行。换句话说，系统必须用相同的计算资源（例如，10个CPU和100GB的内存）来应对高流量或低流量，如图4.12所示。
- en: '![04-12](../../OEBPS/Images/04-12.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![04-12](../../OEBPS/Images/04-12.png)'
- en: Figure 4.12 The traffic changes of the model serving system over time with an
    equal amount of computational resources allocated all the time.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12 模型服务系统在分配等量计算资源的情况下随时间变化的流量变化。
- en: Since we know, more or less, when those holiday periods are, why don’t we plan
    accordingly? Unfortunately, some events make it hard to predict surges in traffic.
    For example, a huge international conference may be planned near one of the resorts,
    as shown in figure 4.13\. This unexpected event, which happens before Christmas,
    has suddenly added traffic at that particular time window (solid line). Not knowing
    about the conferences, we would miss a window that should be taken into account
    when allocating computational resources. Specifically, in our scenario, two CPUs
    and 20 GB of memory, although optimized for our use case, no longer is sufficient
    to handle all resources within this time window. The user experience would be
    very bad. Imagine all the conference attendants sitting in front of their laptops,
    waiting a long time to book a hotel room.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们多少知道那些节假日时期，为什么我们不相应地计划呢？不幸的是，一些事件使得预测流量激增变得困难。例如，一个大型国际会议可能计划在图4.13所示的某个度假胜地附近举行。这个意外事件，发生在圣诞节前，突然在该特定时间窗口增加了流量（实线）。如果我们不知道这些会议，我们就会错过在分配计算资源时应考虑的窗口。具体来说，在我们的场景中，尽管两个CPU和20GB的内存针对我们的用例进行了优化，但已不足以处理这个时间窗口内的所有资源。用户体验会很差。想象一下，所有会议参加者坐在笔记本电脑前，等待很长时间才能预订酒店房间。
- en: '![04-13](../../OEBPS/Images/04-13.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![04-13](../../OEBPS/Images/04-13.png)'
- en: Figure 4.13 The traffic of our model serving system over time with an optimal
    amount of computational resources allocated for different time windows. In addition,
    an unexpected event happened before Christmas that suddenly added traffic during
    that particular time window (solid line).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13：我们的模型服务系统随时间变化的流量，为不同时间窗口分配了最佳数量的计算资源。此外，在圣诞节前发生了一个意外事件，在该特定时间窗口突然增加了流量（实线）。
- en: In other words, this naive solution is still not very practical and effective
    since it’s nontrivial to figure out the time windows to allocate different amounts
    of resources and how much additional resources are needed for each time window.
    Can we come up with any better approach?
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，这种简单解决方案仍然不太实用和有效，因为确定分配不同数量资源的时间窗口以及每个时间窗口需要多少额外资源并不简单。我们能否想出更好的方法？
- en: In our scenario, we are dealing with a dynamic number of model serving requests
    that varies over time and is highly correlated to times around holidays. What
    if we can guarantee we have enough resources and forget about our goal of increasing
    the resource utilization rate for now? If the computational resources are guaranteed
    to be more than sufficient at all times, we can make sure that the model serving
    system can handle heavy traffic during holiday seasons.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，我们处理的是一个随时间变化的动态模型服务请求数量，它与节假日时间高度相关。如果我们能保证始终有足够的资源，现在暂时忘记提高资源利用率的目标会怎样？如果计算资源始终保证是充足的，我们可以确保模型服务系统在节假日季节能够处理大量流量。
- en: '4.4.1 The problem: Responding to model serving requests based on events'
  id: totrans-118
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 问题：基于事件响应模型服务请求
- en: The naive approach, which is to estimate and allocate computational resources
    accordingly before identifying any possible time windows in which the system might
    experience a high volume of traffic, is not feasible. It’s not easy to determine
    the exact dates of the high-traffic time windows and the exact amount of computational
    resources needed during each.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一种简单的方法是在确定系统可能经历高流量时段的任何可能的时间窗口之前，相应地估计和分配计算资源，但这并不可行。确定高流量时段的确切日期以及每个时段所需的计算资源的确切数量并不容易。
- en: Simply increasing the computational resources to an amount sufficient at all
    times also is not practical, as the resource utilization rate we were concerned
    about earlier remains low. For example, if nearly no user requests are made during
    a particular time period, the computational resources we have allocated are, unfortunately,
    mostly idling and thus wasted. Is there another approach that allocates and uses
    computational resources more wisely?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地增加计算资源到始终足够多的程度也不切实际，因为我们之前关注的资源利用率仍然很低。例如，如果在某个特定时间段内几乎没有用户请求，那么我们分配的计算资源，不幸的是，大部分时间都在闲置，从而造成浪费。是否有另一种方法可以更明智地分配和使用计算资源？
- en: 4.4.2 The solution
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 解决方案
- en: The solution to our problem is maintaining a pool of computational resources
    (e.g., CPUs, memory, disk, etc.) allocated not only to this particular model serving
    system but also to model serving of other applications or other components of
    the distributed machine learning pipeline.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们问题的解决方案是维护一个计算资源池（例如，CPU、内存、磁盘等），不仅分配给这个特定的模型服务系统，还分配给其他应用程序或分布式机器学习管道的其他组件的模型服务。
- en: Figure 4.14 is an example architecture diagram where a shared resource pool
    is used by different systems--for example, data ingestion, model training, model
    selection, model deployment, and model serving--at the same time. This shared
    resource pool gives us enough resources to handle peak traffic for the model serving
    system by pre-allocating resources required during historical peak traffic and
    autoscaling when the limit is reached. Therefore, we only use resources when needed
    and only the specific amount of resources required for each particular model serving
    request.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14是一个示例架构图，其中不同的系统（例如，数据摄取、模型训练、模型选择、模型部署和模型服务）同时使用共享资源池。这个共享资源池为我们提供了足够的资源，通过预先分配历史高峰流量期间所需的资源，并在达到限制时自动扩展，来处理模型服务系统的峰值流量。因此，我们只在需要时使用资源，并且只为每个特定的模型服务请求使用所需的特定数量的资源。
- en: '![04-14](../../OEBPS/Images/04-14.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![04-14](../../OEBPS/Images/04-14.png)'
- en: Figure 4.14 An architecture diagram in which a shared resource pool is being
    used by different components--for example, data ingestion, model training, model
    selection, and model deployment--and two different model serving systems at the
    same time. The arrows with solid lines indicate resources, and the arrows with
    dashed lines indicate requests.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14是一个架构图，其中不同的组件（例如，数据摄取、模型训练、模型选择和模型部署）以及两个不同的模型服务系统同时使用共享资源池。实线箭头表示资源，虚线箭头表示请求。
- en: For our discussions, I only focus on the model serving system in the diagram,
    and details for other systems are neglected here. In addition, here I assume that
    the model training component only utilizes similar types of resources, such as
    CPUs. If the model training component requires GPUs or a mix of CPUs/GPUs, it
    may be better to use a separate resource pool, depending on specific use cases.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的讨论中，我只关注图中的模型服务系统，其他系统的细节在这里被忽略。此外，这里我假设模型训练组件仅利用类似类型的资源，例如CPU。如果模型训练组件需要GPU或CPU/GPU的混合，根据具体用例，可能最好使用单独的资源池。
- en: When the users of our hotel price prediction application enter into the UI the
    range of dates and locations that they are interested in staying for holidays,
    the model serving requests are sent to the model serving system. Upon receiving
    each request, the system notifies the shared resource pool that certain amounts
    of computational resources are being used by the system.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的酒店价格预测应用的用户输入他们感兴趣的度假日期和地点范围时，请求模型的服务请求被发送到模型服务系统。在收到每个请求后，系统通知共享资源池，系统正在使用一定量的计算资源。
- en: For example, figure 4.15 shows the traffic of our model serving system over
    time with an unexpected bump. The unexpected bump is due to a new very large international
    conference that happens before Christmas. This event suddenly adds traffic, but
    the model serving system successfully handles the surge in traffic by borrowing
    a necessary amount of resources from the shared resource pool. With the help of
    the shared resource pool, the resource utilization rate remains high during this
    unexpected event. The shared resource pool monitors the current amount of available
    resources and autoscales when needed.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，图4.15显示了我们的模型服务系统随时间变化的流量，并出现了一个意外的峰值。这个意外的峰值是由于在圣诞节前举行的一个新的非常大型国际会议。这一事件突然增加了流量，但模型服务系统通过从共享资源池借用必要的资源量成功处理了流量的激增。在共享资源池的帮助下，在此意外事件期间资源利用率保持较高。共享资源池监控当前可用资源的数量，并在需要时自动扩展。
- en: '![04-15](../../OEBPS/Images/04-15.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![04-15](../../OEBPS/Images/04-15.png)'
- en: Figure 4.15 The traffic of our model serving system over time. An unexpected
    bump happened before Christmas that suddenly added traffic. The jump in requests
    is handled successfully by the model serving system by borrowing the necessary
    amount of resources from the shared resource pool. The resource utilization rate
    remains high during this unexpected event.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 我们模型服务系统随时间变化的流量。在圣诞节前出现了一个意外的峰值，突然增加了流量。模型服务系统通过从共享资源池中借用必要的资源量成功处理了请求的增加。在此意外事件期间，资源利用率保持较高。
- en: This approach, in which the system listens to the user requests and only responds
    and utilizes the computational resources when the user request is being made,
    is called *event-driven processing*.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法，即系统监听用户请求，仅在用户请求被提出时才响应和利用计算资源，被称为*事件驱动处理*。
- en: Event-driven processing vs. long-running serving systems
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动处理与长时间运行的服务系统
- en: Event-driven processing is different from the model serving systems that we’ve
    looked at in previous sections (e.g., systems using replicated services [section
    4.2] and sharded services patterns [section 4.3]), where the servers that handle
    user requests are always up and running. Those long-running serving systems work
    well for many applications that are under heavy load, keep a large amount of data
    in memory, or require some sort of background processing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动处理与我们在前几节中查看的模型服务系统（例如，使用复制服务的系统[第4.2节]和分片服务模式[第4.3节]）不同，在这些系统中，处理用户请求的服务器始终处于运行状态。这些长时间运行的服务系统对于许多在重负载下运行、在内存中保持大量数据或需要某种类型后台处理的应用程序来说效果很好。
- en: However, for applications that handle very few requests during nonpeak periods
    or respond to specific events, such as our hotel price prediction system, the
    event-driven processing pattern is more suitable. This event-driven processing
    pattern has flourished in recent years as cloud providers have developed *function-as-a-service*
    products.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于在非高峰期间处理请求很少或响应特定事件的应用程序，例如我们的酒店价格预测系统，事件驱动处理模式更为合适。这种事件驱动处理模式近年来随着云服务提供商开发*函数即服务*产品而蓬勃发展。
- en: In our scenario, each model serving request made from our hotel price prediction
    system represents an *event*. Our serving system listens for this type of event,
    utilizes necessary resources from the shared resource pool, and retrieves and
    loads the trained machine learning model from the distributed database to estimate
    the hotel prices for the specified time/location query. Figure 4.16 is a diagram
    of this event-driven model serving system.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，从我们的酒店价格预测系统发出的每个模型服务请求都代表一个*事件*。我们的服务系统监听此类事件，从共享资源池中利用必要的资源，并从分布式数据库中检索和加载训练好的机器学习模型，以估计指定时间/位置的酒店价格。图4.16是此事件驱动模型服务系统的示意图。
- en: '![04-16](../../OEBPS/Images/04-16.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![04-16](../../OEBPS/Images/04-16.png)'
- en: Figure 4.16 A diagram of the event-driven model serving system to predict hotel
    prices
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 预测酒店价格的事件驱动模型服务系统示意图
- en: Using this event-driven processing pattern for our serving system, we can make
    sure that our system is using only the resources necessary to process every request
    without concerning ourselves with resource utilization and idling. As a result,
    the system has sufficient resources to deal with peak traffic and return the predicted
    prices without users experiencing noticeable delays or lags when using the system.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这种事件驱动处理模式为我们提供服务系统，我们可以确保我们的系统仅使用处理每个请求所必需的资源，而不必担心资源利用和闲置。因此，系统有足够的资源来应对高峰流量，并在用户使用系统时，不会出现明显的延迟或滞后，返回预测价格。
- en: Even though we now have a shared pool of sufficient computational resources
    where we can borrow computational resources from the shared resource pool to handle
    user requests on demand, we should also build a mechanism in our model serving
    system to defend *denial-of-service attacks*. Denial-of-service attacks interrupt
    an authorized user’s access to a computer network, typically caused with malicious
    intent and often seen in model serving systems. These attacks can cause unexpected
    use of computational resources from the shared resource pool, which may eventually
    lead to resource scarcity for other services that rely on the shared resource
    pool.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们现在有一个足够的计算资源池，我们可以从中借用计算资源来按需处理用户请求，但我们也应该在我们的模型服务系统中建立一个防御*服务拒绝攻击*的机制。服务拒绝攻击中断了授权用户对计算机网络访问，通常由恶意意图引起，在模型服务系统中经常看到。这些攻击可能导致从共享资源池中意外使用计算资源，这最终可能导致依赖于共享资源池的其他服务资源稀缺。
- en: Denial-of-service attacks may happen in various cases. For example, they may
    come from users who accidentally send a huge amount of model serving requests
    in a very short period of time. Developers may have misconfigured a client that
    uses our model serving APIs, so it sends requests constantly or accidentally kicks
    off an unexpected load/stress test in a production environment.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 服务拒绝攻击可能发生在各种情况下。例如，它们可能来自那些在极短的时间内意外发送大量模型服务请求的用户。开发者可能错误配置了使用我们模型服务API的客户端，因此它不断发送请求或在生产环境中意外启动了意外的负载/压力测试。
- en: To deal with these situations, which often happen in real-world applications,
    it makes sense to introduce a defense mechanism for denial-of-service attacks.
    One approach to avoid these attacks is via *rate limiting*, which adds the model
    serving requests to a queue and limits the rate the system is processing the requests
    in the queue.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对这些在现实应用中经常发生的情况，引入针对服务拒绝攻击的防御机制是有意义的。避免这些攻击的一种方法是通过*速率限制*，它将模型服务请求添加到队列中，并限制系统处理队列中请求的速率。
- en: Figure 4.17 is a flowchart showing four model serving requests sent to the model
    serving system. However, only two are under the current rate limit, which allows
    a maximum of two concurrent model serving requests. In this case, the rate-limiting
    queue for model serving requests first checks whether the requests received are
    under the current rate limit. Once the system has finished processing those two
    requests, it will proceed to the remaining two requests in the queue.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17是一个流程图，显示了发送到模型服务系统的四个模型服务请求。然而，只有两个处于当前的速率限制之下，这允许最多两个并发模型服务请求。在这种情况下，模型服务请求的速率限制队列首先检查接收到的请求是否在当前的速率限制之下。一旦系统处理完这两个请求，它将继续处理队列中的剩余两个请求。
- en: '![04-17](../../OEBPS/Images/04-17.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![04-17](../../OEBPS/Images/04-17.png)'
- en: Figure 4.17 A flowchart of four model serving requests sent to the model serving
    system. However, only two are under the current rate limit, which allows a maximum
    of two concurrent model serving requests. Once the system has finished processing
    those two requests, it will proceed to the remaining two requests in the queue.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 模型服务系统接收到的四个模型服务请求的流程图。然而，只有两个处于当前的速率限制之下，这允许最多两个并发模型服务请求。一旦系统处理完这两个请求，它将继续处理队列中的剩余两个请求。
- en: If we are deploying and exposing an API for a model serving service to our users,
    it’s also generally a best practice to have a relatively small rate limit (e.g.,
    only one request is allowed within 1 hour) for users with anonymous access and
    then ask users to log in to obtain a higher rate limit. This system would allow
    the model serving system to better control and monitor the users’ behavior and
    traffic so that we can take necessary actions to address any potential problems
    or denial-of-service attacks. For example, requiring a login provides auditing
    to find out which users/events are responsible for the unexpectedly large number
    of model serving requests.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们正在向用户部署和公开模型服务API的API，对于匿名访问的用户，通常也是最佳实践设置一个相对较小的速率限制（例如，每小时只允许一个请求），然后要求用户登录以获得更高的速率限制。这个系统将允许模型服务系统更好地控制和监控用户的行为和流量，以便我们可以采取必要的措施来解决任何潜在的问题或服务拒绝攻击。例如，要求登录可以提供审计，以找出哪些用户/事件对意外大量的模型服务请求负有责任。
- en: Figure 4.18 demonstrates the previously described strategy. In the diagram,
    the flowchart on the left side is the same as figure 4.17 where four total model
    serving requests from unauthenticated users are sent to the model serving system.
    However, only two can be served by the system due to the current rate limit, which
    allows a maximum of two concurrent model serving requests for unauthenticated
    users. Conversely, the model serving requests in the flowchart on the right side
    all come from authenticated users. Thus, three requests can be processed by the
    model serving system since the limit of maximum concurrent requests for authenticated
    users is three.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18展示了之前描述的策略。在图中，左侧的流程图与图4.17相同，其中未经身份验证的用户向模型服务系统发送了四个总模型服务请求。然而，由于当前的速率限制，只有两个可以被系统服务，因为未经身份验证用户的最大并发模型服务请求限制为两个。相反，右侧流程图中的模型服务请求全部来自经过身份验证的用户。因此，由于经过身份验证用户的最大并发请求限制为三个，模型服务系统可以处理三个请求。
- en: '![04-18](../../OEBPS/Images/04-18.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![04-18](../../OEBPS/Images/04-18.png)'
- en: Figure 4.18 A comparison of behaviors from different rate limits applied to
    authenticated and unauthenticated users
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 对应用于经过身份验证和未经身份验证用户的速率限制行为的比较
- en: Rate limits differ depending on whether the user is authenticated. Rate limits
    thus effectively control the traffic of the model serving system and prevent malicious
    denial-of-service attacks, which could cause unexpected use of computational resources
    from the shared resource pool and eventually lead to resource scarcity of other
    services that rely on it.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 速率限制根据用户是否经过身份验证而有所不同。因此，速率限制有效地控制了模型服务系统的流量，并防止了恶意拒绝服务攻击，这可能导致从共享资源池中意外使用计算资源，最终导致依赖它的其他服务的资源稀缺。
- en: 4.4.3 Discussion
  id: totrans-150
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 讨论
- en: Even though we’ve seen how the event-driven processing pattern benefits our
    particular serving system, we should not attempt to use this pattern as a universal
    solution. The use of many tools and patterns can help you develop a distributed
    system to meet unique real-world requirements.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们已经看到事件驱动处理模式如何使我们的特定服务系统受益，但我们不应试图将此模式作为通用的解决方案。使用许多工具和模式可以帮助您开发一个分布式系统以满足独特的现实世界需求。
- en: For machine learning applications with consistent traffic--for example, model
    predictions calculated regularly based on a schedule--an event-driven processing
    approach is unnecessary as the system already knows when to process the requests,
    and there will be too much overhead trying to monitor this regular traffic. In
    addition, applications that can tolerate less-accurate predictions can work well
    without being driven by events; they can also recalculate and provide good-enough
    predictions to a particular granularity level, such as per day or per week.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有一致流量的机器学习应用（例如，根据预定计划定期计算模型预测）来说，事件驱动处理方法是不必要的，因为系统已经知道何时处理请求，尝试监控这种定期流量将产生过多的开销。此外，可以容忍不太准确预测的应用可以在不受到事件驱动的情况下良好运行；它们也可以重新计算并提供足够好的预测到特定的粒度级别，例如每天或每周。
- en: Event-driven processing is more suitable for applications with different traffic
    patterns that are complicated for the system to prepare beforehand necessary computational
    resources. With event-driven processing, the model serving system only requests
    a necessary amount of computational resources on demand. The applications can
    also provide more accurate and real-time predictions since they obtain the predictions
    right after the users send requests instead of relying on precalculated prediction
    results based on a schedule.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 事件驱动处理更适合那些系统在事先准备必要的计算资源时复杂的、具有不同流量模式的应用。使用事件驱动处理，模型服务系统仅在需要时请求必要的计算资源。由于它们在用户发送请求后立即获得预测，而不是依赖于基于预定计划的预先计算的预测结果，因此应用可以提供更准确和实时的预测。
- en: From developers’ perspective, one benefit of the event-driven processing pattern
    is that it’s very intuitive. For example, it greatly simplifies the process of
    deploying code to running services since there is no end artifact to create or
    push beyond the source code itself. The event-driven processing pattern makes
    it simple to deploy code from our laptops or web browser to run code in the cloud.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 从开发者的角度来看，事件驱动处理模式的一个好处是它非常直观。例如，它极大地简化了将代码部署到运行中的服务的过程，因为除了源代码本身之外，没有需要创建或推送到源代码之外的最终产物。事件驱动处理模式使得从我们的笔记本电脑或网络浏览器部署代码到云中运行变得简单。
- en: In our scenario, we only need to deploy the trained machine learning model that
    may be used as a *function* to be triggered based on user requests. Once deployed,
    this model serving function is then managed and scaled automatically without the
    need to allocate resources manually by developers. In other words, as more traffic
    is loaded onto the service, more instances of the model serving function are created
    to handle the increase in traffic using the shared resource pool. If the model
    serving function fails due to machine failures, it will be restarted automatically
    on other machines in the shared resource pool.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的场景中，我们只需要部署可能根据用户请求触发的训练好的机器学习模型，作为*函数*。一旦部署，这个模型服务函数就会自动管理和扩展，无需开发者手动分配资源。换句话说，随着服务上承载的流量增加，模型服务函数的实例也会增加，以使用共享资源池来处理流量的增加。如果模型服务函数由于机器故障而失败，它将在共享资源池中的其他机器上自动重启。
- en: Given the nature of the event-driven processing pattern, each function that’s
    used to process the model serving requests needs to be *stateless* and independent
    from other model serving requests. Each function instance cannot have local memory,
    which requires all states to be stored in a storage service. For example, if our
    machine learning models depend heavily on the results from previous predictions
    (e.g., a time-series model), in this case, the event-driven processing pattern
    may not be suitable.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到事件驱动处理模式的特点，用于处理模型服务请求的每个函数都需要是**无状态的**并且独立于其他模型服务请求。每个函数实例不能有本地内存，这意味着所有状态都需要存储在存储服务中。例如，如果我们的机器学习模型高度依赖于先前预测的结果（例如，时间序列模型），在这种情况下，事件驱动处理模式可能不适合。
- en: 4.4.4 Exercises
  id: totrans-157
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.4 练习
- en: Suppose we allocate the same amount of computational resources over the lifetime
    of the model serving system for hotel price prediction. What would the resource
    utilization rate look like over time?
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们在模型服务系统的整个生命周期内为酒店价格预测分配相同数量的计算资源。随着时间的推移，资源利用率率会是什么样的？
- en: Are the replicated services or sharded services long-running systems?
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 复制的服务或分片的服务是长运行系统吗？
- en: Is event-driven processing stateless or stateful?
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 事件驱动处理是有状态的还是无状态的？
- en: 4.5 Answers to exercises
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 练习答案
- en: Section 4.2
  id: totrans-162
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4.2节
- en: Stateless
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无状态的
- en: The model server replicas would not know which requests from users to process,
    and there will be potential conflicts or duplicate work when multiple model server
    replicas try to process the same requests.
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型服务器副本不知道应该处理哪些用户请求，当多个模型服务器副本尝试处理相同的请求时，可能会出现潜在的冲突或重复工作。
- en: Yes, only if the single server has no more than 1.4 minutes of downtime per
    day
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，只有当单个服务器每天的停机时间不超过1.4分钟时
- en: Section 4.3
  id: totrans-166
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4.3节
- en: Yes, it helps, but it would decrease the overall resource utilization.
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的，它有帮助，但会降低整体资源利用率。
- en: Stateful
  id: totrans-168
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有状态的
- en: Section 4.4
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第4.4节
- en: It varies over time depending on the traffic.
  id: totrans-170
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 它随时间变化，取决于流量。
- en: Yes. Servers are required to keep them running to accept user requests, and
    computational resources need to be allocated and occupied all the time.
  id: totrans-171
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 是的。服务器需要保持运行以接受用户请求，并且计算资源需要始终分配和占用。
- en: Stateless
  id: totrans-172
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 无状态的
- en: Summary
  id: totrans-173
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Model serving is the process of loading a previously trained machine learning
    model, generating predictions, or making inferences on new input data.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型服务是将先前训练好的机器学习模型加载到内存中，生成预测或对新输入数据进行推理的过程。
- en: Replicated services help handle the growing number of model serving requests
    and achieve horizontal scaling with the help of replicated services.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 复制的服务有助于处理不断增长的模型服务请求数量，并在复制的服务帮助下实现水平扩展。
- en: The sharded services pattern allows the system to handle large requests and
    efficiently distributes the workload of processing large model serving requests
    to multiple model server shards.
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片服务模式允许系统处理大型请求，并有效地将处理大型模型服务请求的工作负载分配给多个模型服务器分片。
- en: With the event-driven processing pattern, we can ensure that our system only
    uses the resources necessary to process every request without worrying about resource
    utilization and idling.
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用事件驱动处理模式，我们可以确保我们的系统只使用处理每个请求所必需的资源，无需担心资源利用率和空闲。
