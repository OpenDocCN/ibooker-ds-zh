- en: '10 Your data under a different lens: Window functions'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 10 以不同的视角看待你的数据：窗口函数
- en: 'This chapter covers:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖：
- en: Window functions and the kind of data transformation they enable
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口函数和它们所允许的数据转换类型
- en: Summarizing, ranking, and analyzing data using the different classes of window
    functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的窗口函数类别总结、排名和分析数据
- en: Building static, growing, and unbounded windows to your functions
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为你的函数构建静态、增长和无界的窗口
- en: Apply UDF to windows as custom window functions
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 UDF 应用到窗口作为自定义窗口函数
- en: When performing data analysis or feature engineering (which is my favorite part
    of machine learning; see chapter 13), nothing makes me quite as happy as window
    functions. On first glance, they look like a watered-down version of the split-apply-combine
    pattern introduced in chapter 9\. Then you open the blinds and bam—powerful manipulations
    in a short, expressive body of code.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行数据分析或特征工程（这是我机器学习中最喜欢的部分；见第 13 章）时，没有什么能让我比窗口函数更高兴。乍一看，它们看起来像是第 9 章中引入的拆分-应用-组合模式的一个稀释版本。然后你拉开窗帘——哇——在简短而富有表现力的代码体中进行强大的操作。
- en: Those who don’t know window functions are bound to reimplement its functionality
    poorly. This has been my experience coaching data analysts, scientists, and engineers.
    If you find yourself struggling to
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 不了解窗口函数的人很可能会糟糕地重新实现其功能。这是我辅导数据分析师、科学家和工程师时的经验。如果你发现自己难以
- en: Rank records
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 排名记录
- en: Identify the top/bottom record according to a set of conditions
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据一组条件识别顶部/底部记录
- en: Get a value from a previous observation in a table (e.g., using our temperature
    data frame from chapter 9 and asking “What was the temperature yesterday?”)
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从表中的先前观察中获取一个值（例如，使用第 9 章中的温度数据框并询问“昨天的温度是多少？”）
- en: Build trended features (i.e., features that summarize past observations, such
    as the average of the observations for the previous week)
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建趋势特征（即总结过去观察的特征，如上周观察的平均值）
- en: you will find that window functions will multiply your productivity and simplify
    your code.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现窗口函数将提高你的生产力并简化你的代码。
- en: 'Window functions fill a niche between group aggregate (`groupBy().agg()`) and
    group map UDF (`groupBy().apply()`) transformations, both seen in chapter 9\.
    Both rely on *partitioning* to split the data frame based on a predicate. A group
    aggregate transformation will yield one record per grouping, while a group map
    UDF allows for any shape of a resulting data frame; a window function always keeps
    the dimensions of the data frame intact. Window functions have a secret weapon
    in the *window frame* that we define within a partition: it determines which records
    are included in the application of the function.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数填补了分组聚合（`groupBy().agg()`）和分组映射 UDF（`groupBy().apply()`）转换之间的空白，这两种转换在第
    9 章中都有介绍。两者都依赖于*分区*来根据谓词拆分数据帧。分组聚合转换将为每个分组生成一条记录，而分组映射 UDF 允许结果数据帧具有任何形状；窗口函数始终保持数据帧的维度不变。窗口函数有一个秘密武器，即我们在分区中定义的*窗口框架*：它决定了哪些记录包含在函数的应用中。
- en: 'Window functions are mostly used for creating new columns, so they leverage
    some familiar methods, such as `select()` and `withColumn()`. Because we already
    are familiar with the syntax for adding columns, I approach this chapter differently.
    First, we look at how we can emulate a simple window function by relying on concepts
    we already know, such as the `groupby` and `join` methods. Then we get familiar
    with the two components of a window function: the window spec and the function
    itself. I then apply and dissect the three main types of window functions (summarizing,
    ranking, and analytical). Once you are equipped with these building blocks of
    window function application, we break open the window spec by introducing ordered
    and bounded windows and window frames. Finally, we go full circle and introduce
    UDF as window functions.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数主要用于创建新列，因此它们利用了一些熟悉的方法，例如 `select()` 和 `withColumn()`。因为我们已经熟悉了添加列的语法，所以我以不同的方式处理本章。首先，我们看看如何通过依赖我们已知的概念来模拟一个简单的窗口函数，例如
    `groupby` 和 `join` 方法。然后我们熟悉窗口函数的两个组成部分：窗口规范和函数本身。然后我应用并剖析三种主要的窗口函数类型（总结、排名和分析）。一旦你装备了窗口函数应用的这些构建块，我们就通过引入有序和有界窗口以及窗口框架来打开窗口规范。最后，我们回到起点，引入
    UDF 作为窗口函数。
- en: This chapter builds heavily on the content from chapters 5 and 9\. Likewise,
    each section builds heavily on the one that precedes it. Window functions are
    themselves not complicated, but there is a lot of new terminology, and the behavior
    of the functions may not be intuitive at first. If you get lost, make sure you
    work carefully through the examples. Because the best way to learn is by doing,
    try the exercises throughout and at the end of the chapter. As always, answers
    are available in appendix A.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本章大量借鉴了第5章和第9章的内容。同样，每个部分都大量借鉴了前一个部分的内容。窗口函数本身并不复杂，但有很多新的术语，而且函数的行为可能一开始并不直观。如果你感到困惑，请确保你仔细地研究例子。因为最好的学习方式是通过实践，尝试本章中的练习和结尾处的练习。一如既往，答案可在附录A中找到。
- en: 10.1 Growing and using a simple window function
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 增长和使用简单的窗口函数
- en: 'When learning a new concept, I find that I have a much easier time when I am
    able to build it from basic principles, using what I already know. This is exactly
    what happened when I learned about window functions: I started by reproducing
    their behavior using a mess of SQL instructions. After I was done, it was obvious
    why window functions are so useful. Imagine my joy when I found them woven into
    PySpark, with a beautiful Python API to boot.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 当学习一个新概念时，我发现当我能够从基本原理出发，利用我所知道的知识来构建它时，我会更容易。这正是我在学习窗口函数时发生的情况：我开始通过使用一串SQL指令来重现它们的行为。完成之后，很明显窗口函数为什么如此有用。当我发现它们被编织进PySpark中，并且还附带一个漂亮的Python
    API时，我感到无比的喜悦。
- en: 'In this section, we follow the same path: I start by reproducing a simple window
    function using techniques from past chapters. Then I introduce the syntax for
    window functions and how they simplify your data transformation logic. My hope
    is that you’ll get as excited as I did when window functions finally “clicked”
    for me.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们遵循相同的路径：我首先通过使用之前章节中的技术来重现一个简单的窗口函数。然后，我介绍了窗口函数的语法以及它们如何简化你的数据转换逻辑。我希望你能像我第一次对窗口函数“恍然大悟”时那样兴奋。
- en: For this section, we reuse the temperature data set from chapter 8; the data
    set contains weather observations for a series of stations, summarized by day.
    Window functions especially shine when working with time series-like data (e.g.,
    daily observations of temperature) because you can slice the data by day, month,
    or year and get useful statistics. If you want to use the data from BigQuery,
    use the code from chapter 9, keeping as many (or as little) years of data you
    want. For those who prefer a local-first approach, the book’s repository contains
    three years of data in Parquet format (see listing 10.1).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我们重用了第8章中的温度数据集；该数据集包含一系列气象站的天气观测数据，按日汇总。窗口函数在处理类似时间序列的数据（例如，温度的每日观测）时特别出色，因为你可以按日、月或年切片数据，并获取有用的统计数据。如果你想使用BigQuery中的数据，请使用第9章中的代码，保留你想要的（或尽可能少的）年份数据。对于那些更喜欢本地优先的方法，本书的存储库包含三年的数据，以Parquet格式存储（参见列表10.1）。
- en: 'Listing 10.1 Reading the data necessary: GSOD NOAA weather data'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.1 读取必要的数据：GSOD NOAA天气数据
- en: '[PRE0]'
  id: totrans-21
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now that we’re equipped with the data, let’s start asking questions! The next
    sections illustrate the thought process behind a window function, before diving
    right into the terminology and the syntax.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经具备了数据，让我们开始提问吧！接下来的几节将展示窗口函数背后的思考过程，然后再深入到术语和语法。
- en: 10.1.1 Identifying the coldest day of each year, the long way
  id: totrans-23
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 通过长方法确定每年最冷的一天
- en: 'In this section, we emulate a simple window function through functionality
    we learned in previous chapters—most noticeably in chapter 5 using the `join()`
    method. The idea is to provide an intuitive sense for window functions and remove
    some of the magic surrounding them. To illustrate this, we start with simple questions
    to ask our data frame: *when* and *where* were the lowest temperature recorded
    each year? In other words, we want a data frame containing three records, one
    for each year and showing the station, the date (year, month, day), and the temperature
    of the coldest day recorded for that year.'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过之前章节中学到的功能来模拟一个简单的窗口函数——最明显的是在第5章中使用`join()`方法。目的是提供一个对窗口函数直观的感觉，并消除围绕它们的一些神秘感。为了说明这一点，我们从一个简单的问题开始，询问我们的数据框：*何时*和*何地*记录了每年最低的温度？换句话说，我们想要一个包含三个记录的数据框，每个记录对应一年，显示该年的气象站、日期（年、月、日）以及当年记录的最低温度。
- en: Let’s map the thought process. First, we will get a data frame containing the
    coldest temperature for each year. This will give us two of the columns (`year`,
    and `temp`) and their values. In listing 10.2, we create the `coldest_temp` data
    frame, which takes our historical data and groups by the `year` column, and we
    extract the minimum `temp` through the `min()` aggregate function applied through
    the `agg()`. If the syntax is a little cloudy, head to chapter 5 for a refresher
    on grouped data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们映射一下思维过程。首先，我们将获取一个包含每年最低温度的数据帧。这将给我们两个列（`year`和`temp`）及其值。在列表10.2中，我们创建了`coldest_temp`数据帧，它使用我们的历史数据并按`year`列分组，然后通过`agg()`应用`min()`聚合函数提取最小`temp`。如果语法有点模糊，请前往第5章复习分组数据。
- en: Listing 10.2 Computing the lowest temperature for each year using `groupBy()`
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.2 使用`groupBy()`计算每年最低温度
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: People, Earth is cold!
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 人们，地球很冷！
- en: This provides the year and the temperature, which are about 40% of the original
    ask. To get the other three columns (`mo`, `da`, `stn`), we can use a left-semi
    join on the original table, using the results of `coldest_temp` to resolve the
    join. In listing 10.3, we join `gsod` to `coldest_temp` using a left-semi equi-join
    on the `year` and `temp` columns (see chapter 5 for more information on left-semi
    and equi-joins!). Because `coldest_temp` only contains the coldest temperature
    for each year, the left semi-join keeps only the records from `gsod` that correspond
    to that year-temperature pair; this is equivalent to keeping only the records
    where the temperature was coldest for each year.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这提供了年份和温度，这大约是原始要求的40%。为了获取其他三个列（`mo`、`da`、`stn`），我们可以使用原始表上的左半半连接，使用`coldest_temp`的结果来解决连接。在列表10.3中，我们使用`year`和`temp`列上的左半等值连接将`gsod`与`coldest_temp`连接起来（有关左半连接和等值连接的更多信息，请参阅第5章）！因为`coldest_temp`只包含每年最冷的温度，所以左半连接只保留与该年份-温度对对应的`gsod`记录；这相当于只保留每年温度最低的记录。
- en: Listing 10.3 Using a left semi-join for computing the coldest station/day for
    each year
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.3 使用左半连接计算每年最冷的站点/日期
- en: '[PRE2]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In listing 10.2 and listing 10.3 we are performing a join between the `gsod`
    table and, well, something coming from the `gsod` table. A *self-join*, which
    is when you join a table with itself, is often considered an anti-pattern for
    data manipulation. While it’s not technically wrong, it can be slow and make the
    code look more complex than it needs to be. It also looks a little odd. Joining
    tables make sense when you want to link data contained into two or more tables.
    Joining a table with itself feels redundant, as we can see in figure 10.1: the
    data is already in the (one) table!'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表10.2和列表10.3中，我们正在执行`gsod`表与来自`gsod`表中的某些内容的连接。这是一种*自连接*，即当你将一个表与自身连接时，通常被认为是一种数据操作的反模式。虽然从技术上讲并没有错误，但它可能很慢，并且使代码看起来比实际需要更复杂。它看起来也有些奇怪。当你想要将包含在两个或多个表中的数据链接起来时，连接表是有意义的。将一个表与自身连接感觉是多余的，正如我们在图10.1中看到的那样：数据已经在（一个）表中了！
- en: '![](../Images/10-01.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![图10.1](../Images/10-01.png)'
- en: Figure 10.1 A self-join happens when a table is joined with itself. You can
    replace most self-joins by window functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.1 当一个表与自身连接时发生自连接。你可以用窗口函数替换大多数自连接。
- en: Fortunately, a window function gives you the same result faster, and with less
    code clutter. In the next section, we’ll reproduce the same data transformation
    using a window function, and simplify and speed up our data transformation code.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，窗口函数可以更快地给出相同的结果，并且代码更简洁。在下一节中，我们将使用窗口函数重现相同的数据转换，并简化并加快我们的数据转换代码。
- en: 10.1.2 Creating and using a simple window function to get the coldest days
  id: totrans-36
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 创建和使用简单的窗口函数以获取最冷的日子
- en: This section introduces window functions by replacing the self-join example
    of the previous section. I introduce the `Window` object and parameterize it to
    split a data frame over column values. We then apply the window over a data frame,
    using the traditional selector approach.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 本节通过替换前节中的自连接示例来介绍窗口函数。我介绍了`Window`对象，并将其参数化以根据列值拆分数据帧。然后，我们使用传统的选择方法在数据帧上应用窗口函数。
- en: 'At the beginning of the chapter, I drew a parallel between window functions
    and the split-apply-combine pattern I covered when introducing pandas group map
    UDF (chapter 8). To stay consistent with the `Window` function terminology, which
    comes from SQL (see chapter 7), I use different vocabulary for the three stages
    of the split-apply-combine pattern:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的开头，我将窗口函数与我在介绍 pandas group map UDF（第 8 章）时提到的分而治之、合并模式进行了类比。为了与来自 SQL 的
    `Window` 函数术语保持一致（见第 7 章），我在分而治之、合并模式的三个阶段使用了不同的词汇：
- en: Instead of splitting, we’ll *partition* the data frame.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不是进行拆分，而是将数据框进行*分区*。
- en: Instead of applying, we’ll *select* values *over the window*.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不是应用，而是在窗口*上选择*值。
- en: The `combine`/`union` operation is implicit (i.e., not explicitly coded) in
    a window function.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在窗口函数中，`combine`/`union` 操作是隐式的（即，不是显式编码的）。
- en: Note Why use a different vocabulary here? Window functions are concepts from
    the SQL world, where the split-apply-combine pattern comes from the data analysis
    world. Different worlds, different vocabulary!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 注意为什么这里使用不同的词汇？窗口函数是来自 SQL 世界的一个概念，而分而治之、合并的模式则来自数据分析领域。不同的领域，不同的词汇！
- en: 'Window functions apply over a window of data split according to the values
    on a column. Each split, called a partition, gets the window function applied
    to each of its records as if they were independent data frames. The result then
    gets unioned back into a single data frame. In listing 10.4, I create the window,
    partitioning according to the values in column `year`. The `Window` class is a
    builder class, just like with `SparkSession.builder`: we chain the parameterization
    by appending methods after the `Window` class identifier. The result is a `WindowSpec`
    object that contains the information about the parameterization.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数在根据列上的值拆分的数据集上应用。每个拆分，称为分区，将窗口函数应用于其记录，就像它们是独立的数据框一样。然后结果被合并回单个数据框。在列表 10.4
    中，我创建了一个窗口，根据 `year` 列的值进行分区。`Window` 类是一个构建器类，就像 `SparkSession.builder` 一样：我们通过在
    `Window` 类标识符后附加方法来链式参数化。结果是包含参数化信息的 `WindowSpec` 对象。
- en: Listing 10.4 Creating a `WindowSpec` object by using the `Window` builder class
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.4 通过使用 `Window` 构建器类创建 `WindowSpec` 对象
- en: '[PRE3]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ We import Window from pyspark.sql.window. Since it’s the only object we’ll
    use for window functions, there’s no need import the whole module.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们从 pyspark.sql.window 导入 Window。由于我们只会使用这个对象进行窗口函数，因此没有必要导入整个模块。
- en: ❷ To partition according to the values of one or more columns, we pass the column
    name (or a Column object) to the partitionBy() method.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 要根据一列或多列的值进行分区，我们将列名（或列对象）传递给 partitionBy() 方法。
- en: A `WindowSpec` object is nothing more than a blueprint for an eventual window
    function. In our case, in listing 10.4, we created a window specification called
    `each_year` that instructs the window application to split the data frame according
    to the values in the `year` column. The real magic happens when you apply the
    window function to your data frame. For our first window function application,
    I print the whole code, reproducing the self-join approach in section 10.1.1 before
    going through it line by line. See the difference between the window application
    (listing 10.6) and the left semi-join.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`WindowSpec` 对象不过是一个最终窗口函数的蓝图。在我们的案例中，在列表 10.4 中，我们创建了一个名为 `each_year` 的窗口规范，指示窗口应用根据
    `year` 列的值拆分数据框。真正的魔法发生在你将窗口函数应用于数据框的时候。对于我们的第一个窗口函数应用，我打印了整个代码，重复了第 10.1.1 节中提到的自连接方法，然后逐行分析。看看窗口应用（列表
    10.6）和左外连接之间的区别。'
- en: Listing 10.5 Using a left semi-join for computing the coldest station/day for
    each year
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.5 使用左外连接计算每年最冷的站点/天
- en: '[PRE4]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Listing 10.6 Selecting the minimum temperature for each year using a window
    function
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.6 使用窗口函数选择每年最低温度
- en: '[PRE5]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ We select the minimum temperature over the defined window (for each year).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们在定义的窗口内选择最低温度（对于每年）。
- en: 'It’s time for some code unpacking. Through the `withColumn()` method we define
    a column, `min_temp`, that collects the minimum of the `temp` column. Now, rather
    than picking the minimum temperature of the whole data frame, the `min()` is applied
    *over* the window specification we defined, using the `over()` method. For each
    window partition, Spark computes the minimum and then broadcasts the value over
    each record. This is an important distinction compared to aggregating functions
    or UDF: in the case of a window function, *the number of records in the data frame
    does not change*. Although `min()` is an aggregate function, since it’s applied
    with the `over()` method, every record in the window has the minimum value appended.
    The same would apply for any other aggregate function from `pyspark.sql.functions`,
    such as `sum()`, `avg()`, `min()`, `max()`, and `count()`.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候对代码进行解包了。通过`withColumn()`方法，我们定义了一个名为`min_temp`的列，该列收集`temp`列的最小值。现在，我们不再选择整个数据框的最小温度，而是使用`over()`方法在定义的窗口规范上应用`min()`。对于每个窗口分区，Spark计算最小值，然后将该值广播到每个记录。与聚合函数或UDF相比，这是一个重要的区别：在窗口函数的情况下，**数据框中的记录数不会改变**。虽然`min()`是一个聚合函数，但由于它是通过`over()`方法应用的，所以窗口中的每个记录都会附加最小值。对于`pyspark.sql.functions`中的任何其他聚合函数也是如此，例如`sum()`、`avg()`、`min()`、`max()`和`count()`。
- en: Window functions are just methods on columns (almost)
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 窗口函数仅仅是列上的方法（几乎）
- en: Since a window function is applied though a method on a `Column` object, you
    can also apply them in a `select()`. You can also apply more than one window (or
    different ones) within the same `select()`. Spark won’t allow you to use a window
    directly in a `groupby()` or `where()` method, where it’ll spit an `AnalysisException`.
    If you want to group by or filter according to the result of a window function,
    “materialize” the column using `select()` or `withColumn()` before using the desired
    operation.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 由于窗口函数是通过`Column`对象上的方法应用的，因此你还可以在`select()`中应用它们。你还可以在同一个`select()`中应用多个窗口（或不同的窗口）。Spark不允许你在`groupby()`或`where()`方法中使用窗口，否则会抛出`AnalysisException`。如果你想根据窗口函数的结果进行分组或过滤，请在使用所需操作之前使用`select()`或`withColumn()`“物化”该列。
- en: As an example, listing 10.6 could be rewritten, with the window definition put
    into the select. Because the window applies on a column-by-column basis, you can
    have multiple window applications within a select statement.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，列表10.6可以重写，将窗口定义放入`select`中。因为窗口是按列逐列应用的，所以你可以在一个选择语句中有多处窗口应用。
- en: Listing 10.7 Using a window function within a `select()` method
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.7 在`select()`方法中使用窗口函数
- en: '[PRE6]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ We drop min_temp as it served its purpose during the where clause and is no
    longer needed (it’ll always be equal to temp in the resulting data frame).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们删除`min_temp`，因为它在`where`子句中已经完成了它的作用，不再需要（它将始终等于结果数据框中的`temp`）。
- en: Review the end-of-chapter exercises to experiment with multiple windows applications.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 检查章节末尾的练习，以实验多个窗口应用。
- en: 'Under the hood, PySpark realizes the window spec when applied to a column.
    I defined a rather simple window spec here: partition the data frame according
    to the values of the `year` column. Just like with the split-apply-combine pattern,
    we partition the data frame according to the `year` column values.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 在底层，PySpark在应用于列时实现窗口规范。我在这里定义了一个相当简单的窗口规范：根据`year`列的值对数据框进行分区。就像split-apply-combine模式一样，我们根据`year`列的值对数据框进行分区。
- en: Tip You can `partitionBy()` by more than one column! Just add more column names
    to the `partitionBy()` method.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士 你可以对多个列使用`partitionBy()`！只需将更多列名添加到`partitionBy()`方法中。
- en: For each window partition (see the *But data frames already have partitions!*
    sidebar at the end of the section), we compute the aggregate function (here, `min()`),
    before broadcasting the result of each record. In plain English, we compute the
    minimum temperature for each year and append it as a column for each record of
    this year. I creatively name the new column `min_temp`.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个窗口分区（参见本节末尾的“但是数据框已经有了分区！”侧边栏），我们在广播每个记录的结果之前计算聚合函数（这里为`min()`）。用简单的话来说，我们计算每年的最低温度，并将其作为该年每个记录的列附加。我创造性地将新列命名为`min_temp`。
- en: Next, we need to keep only the records where the temperature is actually the
    minimum for the year. For this, we simply need to `filter()` (or `where()`) to
    keep only the records where `temp` `=` `min_temp`. Because the window function
    application gives each record a `min_temp` field corresponding to the minimum
    temperature for that year, we are back to our regular arsenal of data manipulation
    tricks.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要只保留那些温度实际上是该年最低的记录。为此，我们只需简单地使用`filter()`（或`where()`）来保留那些`temp`等于`min_temp`的记录。因为窗口函数应用给每条记录提供了一个对应于该年最低温度的`min_temp`字段，所以我们又回到了常规的数据操作技巧。
- en: That’s all, folks! We have our very first window function. This was a purposefully
    simple example to teach the concept of a window spec, window function, and window
    partition. In the next section, I compare the application and speed of both approaches
    and explain why window functions are easier, friendlier, and faster.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 好了，朋友们！我们有了我们非常第一个窗口函数。这是一个故意简单的例子，用来教授窗口规范、窗口函数和窗口分区概念。在下一节中，我将比较两种方法的应用和速度，并解释为什么窗口函数更容易、更友好、更快。
- en: But data frames already have partitions!
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 但数据框已经有了分区！
- en: Once again, we’re having a vocabulary problem. Since the beginning of the book,
    *partition* has referred to the physical splits of the data on each executor node.
    Now we are also using partitions with window functions to mean logical splits
    of the data, which may or may not be equal to the Spark physical ones.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们遇到了一个词汇问题。从本书开始，*分区*一词指的是每个执行节点上数据的物理分割。现在我们也在使用窗口函数的分区来表示数据的逻辑分割，这些分割可能等于也可能不等于Spark的物理分割。
- en: Unfortunately, most of the literature online will not tell you which partition
    they refer to. But once you’ve internalized both Spark and window function concepts,
    it’ll be easy to know which is which. For this chapter, I’ll use *window partitions*
    when talking about the logical partitions made by the application of a window
    spec.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，网上的大部分文献都不会告诉你它们指的是哪个分区。但一旦你内化了Spark和窗口函数的概念，就会很容易知道哪个是哪个。对于本章，当谈到由窗口规范应用产生的逻辑分区时，我将使用*窗口分区*。
- en: '![](../Images/10-02.png)'
  id: totrans-70
  prefs: []
  type: TYPE_IMG
  zh: '![图10-02](../Images/10-02.png)'
- en: Figure 10.2 We partition the `gsod` data frame according to the `year` column
    and compute the minimum temperature for each partition. Each record belonging
    to the partition gets the minimum temperature appended. The resulting data frame
    contains the same number of records, but with a new column, `min_temp`, that contains
    the coldest temperature for the year.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.2 我们根据`year`列将`gsod`数据框进行分区，并计算每个分区的最低温度。属于每个分区的每条记录都会附加最低温度。结果数据框包含相同数量的记录，但新增了一个名为`min_temp`的列，该列包含该年的最低温度。
- en: 10.1.3 Comparing both approaches
  id: totrans-72
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.3 比较两种方法
- en: In this section, I compare both the self-join and window function approaches
    from a code readability perspective. We also touch on performance implications
    of windows versus joins. When performing data transformation and analysis, code
    clarity and performance are the two most important considerations for a working
    body of code; since we have two approaches that perform the same work, it makes
    sense to compare them from a clarity and performance perspective.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我从代码可读性的角度比较了自连接和窗口函数方法。我们还简要讨论了窗口与连接的性能影响。在进行数据转换和分析时，代码清晰度和性能是工作代码体最重要的两个考虑因素；由于我们有两种执行相同工作的方法，因此从清晰度和性能的角度进行比较是有意义的。
- en: 'Compared to the self-join approach, using a window function makes your intention
    more clear. With a window named `each_year`, the code snippet `F.min("temp")`
    `.over(each_year)` almost reads like an English sentence. The self-join approach
    accomplishes the same thing, but at the expense of a slightly more cryptic code:
    *why am I joining this table to itself?*'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 与自连接方法相比，使用窗口函数可以使你的意图更加清晰。使用名为`each_year`的窗口，代码片段`F.min("temp") .over(each_year)`几乎就像一个英语句子。自连接方法可以完成相同的工作，但代价是代码稍微有点晦涩：*我为什么要将这个表与自身连接？*
- en: Performance wise, window functions avoid potentially costly self-joins. When
    working with large data sets, the data frame only has to be split into window
    partitions before performing a function over the (smaller) partitions. When you
    consider that Spark’s operating model is splitting large data sets across multiple
    nodes, it makes a lot of sense.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在性能方面，窗口函数避免了可能代价高昂的自连接。当处理大型数据集时，数据框在执行函数之前只需将数据分割成窗口分区。考虑到Spark的操作模型是在多个节点上拆分大型数据集，这很有意义。
- en: Finding which approach works fastest will depend on the size of the data, the
    memory available (see chapter 11 for an overview of how memory is used in Spark),
    and how complex the join/window operation is. I tend to overwhelmingly prefer
    window functions, as they are more clear and express my intent more clearly as
    well. As I repeat to myself when I code, *make it work, make it clear, then make
    it fast*!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 找出哪种方法最快将取决于数据的大小、可用的内存（参见第11章，了解Spark中内存使用的概述），以及连接/窗口操作有多复杂。我倾向于压倒性地偏好窗口函数，因为它们更清晰，而且能更清楚地表达我的意图。正如我在编码时反复对自己说，*先让它工作，再让它清晰，最后让它快速*！
- en: Finally, and this will be the content of the next sections, window functions
    are much more flexible than merely computing aggregated measurements over a given
    window. Next, I introduce ranking and analytical functions, which provide a new
    window (get it?) over your data. The *summarize-and-join* approach will quickly
    fall short!
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这也是下一节的内容，窗口函数比仅仅在给定窗口上计算聚合度量更灵活。接下来，我将介绍排名和分析函数，它们为你的数据提供一个新的窗口（明白了吗？）。*汇总和连接*方法将很快变得不足。
- en: Exercise 10.1
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.1
- en: Using the `gsod` data frame, which window spec that, once applied, could generate
    the hottest station for each day?
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`gsod`数据框，哪个窗口规范一旦应用，就能生成每天的气温最高的站点？
- en: a) `Window.partitionBy("da")`
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: a) `Window.partitionBy("da")`
- en: b) `Window.partitionBy("stn",` `"da")`
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: b) `Window.partitionBy("stn", "da")`
- en: c) `Window.partitionBy("year",` `"mo",` `"da")`
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: c) `Window.partitionBy("year", "mo", "da")`
- en: d) `Window.partitionBy("stn",` `"year",` `"mo",` `"da")`
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: d) `Window.partitionBy("stn", "year", "mo", "da")`
- en: e) None of the above
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: e) 以上皆非
- en: '10.2 Beyond summarizing: Using ranking and analytical functions'
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 超越汇总：使用排名和分析函数
- en: 'In this section, I cover the two other families of functions that can be applied
    over a window. Both families provide additional functionality to the humble window.
    Together, those families of functions allow performance of a wider range of operations
    versus aggregate functions such as `count()`, `sum()`, or `min()`:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍可以在窗口上应用的其他两种函数族。这两个函数族都为谦逊的窗口提供了额外的功能。这些函数族共同允许执行比聚合函数（如`count()`、`sum()`或`min()`）更广泛的操作：
- en: The *ranking* family, which provides information about rank (first, second,
    all the way to last), n-tiles, and the ever so useful row number
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*排名*族，它提供了关于排名（第一、第二，一直到最后）、n-tiles和非常有用的行号的信息。'
- en: The *analytical* family, which, despite its namesake, covers a variety of behaviors
    not related to summary or ranking.
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分析*族，尽管其名称暗示了与汇总或排名相关的行为，但实际上涵盖了各种与这些行为无关的行为。'
- en: Both provide information over a window that isn’t easy to obtain via other SQL-esque
    functionality (trying to torture the SQL language to reproduce window function
    behavior using only basic SQL functionality is left as an exercise if you’re really
    into useless coding puzzles). Because they add new functionality to the window,
    I also cover how to order values in a data frame (which is very useful when you
    want to rank records).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 两个族都提供了其他SQL式功能难以获得的信息窗口（如果你真的喜欢无用的编码谜题，尝试仅使用基本SQL功能来折磨SQL语言以重现窗口函数的行为，这被留作练习）。因为它们为窗口添加了新功能，我还将介绍如何在数据框中排序值（当你想要对记录进行排名时非常有用）。
- en: For this section, I use a much smaller data frame—keeping 10 records and only
    the `stn`, `year`, `mo`, `da`, `temp`, and `count_temp` columns—so that we can
    see it in its entirety when `show()`ing it. I find that this helps tremendously
    in understanding what’s happening. This new data frame is called `gsod_light`
    and is available (in Parquet format, a data format optimized for rapid retrieval
    of column data; see [https://databricks.com/glossary/what-is-parquet](https://databricks.com/glossary/what-is-parquet))
    in the book’s repository. All the examples can also be run using the original
    `gsod` data frame or even over more years, should you have a more powerful cluster
    readily available.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我使用了一个更小的数据帧——保留 10 条记录和 `stn`、`year`、`mo`、`da`、`temp` 和 `count_temp` 列，这样我们就可以在
    `show()` 时看到它的全部。我发现这极大地帮助了理解正在发生的事情。这个新的数据帧称为 `gsod_light`，可在书籍的仓库中找到（以 Parquet
    格式提供，这是一种针对快速检索列数据优化的数据格式；参见 [https://databricks.com/glossary/what-is-parquet](https://databricks.com/glossary/what-is-parquet)）。所有示例也可以使用原始的
    `gsod` 数据帧运行，或者如果您有一个更强大的集群，甚至可以运行更多年份。
- en: Listing 10.8 Reading `gsod_light` from the book’s code repository
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.8 从书籍的代码仓库中读取 `gsod_light`
- en: '[PRE7]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a small but easy-to-reason-about data frame, let’s explore
    ranking functions.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个小但易于推理的数据帧，让我们来探索排名函数。
- en: '10.2.1 Ranking functions: Quick, who’s first?'
  id: totrans-94
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 排名函数：快速，谁是第一？
- en: 'This section covers ranking functions: nonconsecutive ranks with `rank()`,
    consecutive ranks with `dense_rank()`, percentile ranks with `percent_rank()`,
    tiles with `ntile()`, and finally a bare row number with `row_number()`. Ranking
    functions are used for getting the top (or bottom) record for each window partition,
    or, more generally, for getting an order according to some column’s value. For
    example, if you wanted to get the top three hottest days for each station/month,
    a ranking function would make this a walk in the park. Because ranking functions
    behave quite similarly to one another, they are better introduced in one fell
    swoop. Have no fear; I promise it won’t read like a technical manual.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了排名函数：使用 `rank()` 进行非连续排名，使用 `dense_rank()` 进行连续排名，使用 `percent_rank()` 进行百分位排名，使用
    `ntile()` 进行分块，最后使用 `row_number()` 获取裸行号。排名函数用于获取每个窗口分区的顶部（或底部）记录，或者更普遍地，根据某些列的值进行排序。例如，如果您想获取每个站点/月份的前三个最热的日子，排名函数会让这变成一件轻而易举的事情。由于排名函数的行为相当相似，因此最好一次性介绍。别担心，我保证它不会读起来像技术手册。
- en: 'Ranking functions have one sole purpose in life: to rank records based on the
    value of a field. Because of this, we need to order the values within a window.
    Enter the `orderBy()` method for windows. In listing 10.9, I create a new window,
    `temp_per_month_asc`, which partitions the data frame according to the `mo` column,
    ordering each record in the partition according to the `count_temp` column. Just
    like when ordering a data frame, `orderBy()` will sort the values in ascending
    order.'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 排名函数在一生中只有一个目的：根据字段的值对记录进行排名。正因为如此，我们需要对窗口内的值进行排序。这就是窗口的 `orderBy()` 方法。在列表
    10.9 中，我创建了一个新的窗口，`temp_per_month_asc`，它根据 `mo` 列对数据帧进行分区，并按 `count_temp` 列对分区中的每条记录进行排序。就像对数据帧进行排序一样，`orderBy()`
    将按升序排序值。
- en: Tip When naming my windows, I like to give them names, so they read well when
    reading the code. In this case, I can read the code and know that my column will
    be over each month, ordering by the count of temperature recorded. No need to
    add a `_window` suffix.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在命名我的窗口时，我喜欢给它们起名字，这样在阅读代码时读起来会更好。在这种情况下，我可以阅读代码并知道我的列将覆盖每个月，按记录的温度计数进行排序。不需要添加
    `_window` 后缀。
- en: Listing 10.9 An ordered version of the month-partitioned window
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 按月分区的有序窗口
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Our window partitions are ordered by the values in the mo column.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们按 `mo` 列的值对窗口分区进行排序。
- en: ❷ Within each window, the records will be ordered according to the value in
    the count_temp column.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在每个窗口内，记录将根据 `count_temp` 列的值进行排序。
- en: 'Gold, silver, bronze: Simple ranking using rank()'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 金、银、铜：使用 `rank()` 进行简单排名
- en: This section covers the simplest and most intuitive form of ranking, using the
    `rank()` function. With `rank()`, each record gets a position based on the value
    contained in one (or more) columns. Identical values have identical ranks—just
    like medalists in the Olympics, where the same score/time yields the same rank
    (unless you cheat!).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍最简单、最直观的排名形式，使用 `rank()` 函数。使用 `rank()`，每条记录根据一个（或多个）列中的值获得一个位置。相同的值具有相同的排名——就像奥运会的获奖者一样，相同的分数/时间会产生相同的排名（除非你作弊！）。
- en: '`rank()` takes no parameters since it ranks according to the `orderBy()` method
    from the window spec; it would not make sense to order according to one column
    but rank according to another.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`rank()` 不需要参数，因为它根据窗口规范的 `orderBy()` 方法进行排名；按照一列排序但按另一列排名是没有意义的。'
- en: Listing 10.10 The `rank()` according to the value of the `count_temp` column
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.10 根据 `count_temp` 列的值进行 `rank()`
- en: '[PRE9]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ rank() does not need a column name; everything has been defined as part of
    the window spec.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `rank()` 函数不需要列名；所有内容都已定义为窗口规范的一部分。
- en: ❷ When a window contains a single record, we get a rank of 1.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当窗口中只有一个记录时，我们得到排名为 1。
- en: ❸ When a window has more than one record with the same value for the orderBy()
    column (here, count_temp = 12), rank() gives the same rank to both records.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 当窗口中有一个或多个记录的 `orderBy()` 列（此处为 `count_temp = 12`）值相同时，`rank()` 会给这两个记录相同的排名。
- en: ❹ Now, because we have two rank = 1 records within the window, the third record
    will have a rank of 3.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在，因为我们窗口中有两个排名为 1 的记录，第三个记录的排名将是 3。
- en: 'The function `rank()` provides nonconsecutive ranks for each record, based
    on the value of the ordered value, or the column(s) provided in the `orderBy()`
    method of the window spec we call. In listing 10.10, for each window, the lower
    the `count_temp`, the lower the rank. When two records have the same ordered value,
    their ranks are the same. We say that the rank is nonconsecutive because, when
    you have multiple records that tie for a rank, the next one will be offset by
    the number of ties. For instance, for `mo` `=` `03`, we have two records with
    `count_temp` `=` `12`: both are rank 1\. The next record (`count_temp` `=` `24`)
    has a position of 3 rather than 2, because two records tied for the first position.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `rank()` 根据排序值或窗口规范中我们调用的 `orderBy()` 方法提供的列（s）的值，为每个记录提供非连续排名。在列表 10.10
    中，对于每个窗口，`count_temp` 越低，排名越低。当两个记录具有相同的排序值时，它们的排名相同。我们说排名是非连续的，因为当你有多个记录平局时，下一个记录将偏移平局的记录数。例如，对于
    `mo` `=` `03`，我们有 `count_temp` `=` `12` 的两个记录：两者都是排名 1。下一个记录（`count_temp` `=`
    `24`）的位置是 3 而不是 2，因为有两个记录平局第一位置。
- en: 'No ties when ranking: Using dense_rank()'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 排名时无平局：使用 `dense_rank()`
- en: 'What if we want, say, a denser ranking that would allocate consecutive ranks
    for records? Enter `dense_rank()`. The same principle as `rank()` applies, where
    ties share the same rank, but there won’t be any gap between the ranks: 1, 2,
    3, and so on. This is practical when you want the second (or third, or any ordinal
    position) value over a window, rather than the record.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要一个更密集的排名，为记录分配连续的排名呢？请使用 `dense_rank()`。与 `rank()` 的原理相同，平局记录共享相同的排名，但排名之间不会有任何间隔：1，2，3，以此类推。当你想要窗口中的第二个（或第三个，或任何序数位置）值，而不是记录本身时，这很有用。
- en: Listing 10.11 Avoiding gaps in ranking using `dense_rank()`
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.11 使用 `dense_rank()` 避免排名中的间隔
- en: '[PRE10]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ dense_rank() is applied instead of rank().
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 `dense_rank()` 而不是 `rank()`。
- en: ❷ When a dense rank ties, both records have the same rank.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当密集排名出现平局时，两个记录具有相同的排名。
- en: ❸ Unlike with rank(), dense ranks are consecutive, regardless of how many ties
    the previous rank has.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 与 `rank()` 不同，密集排名是连续的，不管前一个排名有多少平局。
- en: The three remaining ranking functions, `percent_rank()`, `ntile()`, and `row_number()`,
    are more niche but still useful. I find them better explained visually.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的三个排名函数 `percent_rank()`、`ntile()` 和 `row_number()` 虽然较为特殊，但仍然很有用。我发现它们用视觉方式解释得更好。
- en: Ranking? Scoring? percent_rank() gives you both!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 排名？评分？`percent_rank()` 给你两者！
- en: 'Ranking is usually thought of as an ordinal operation: first, second, third,
    and so on. What if you want something closer to a scope, perhaps even a percentage
    that would reflect where a record stands compared to its peers in the same window
    partition? Enter `percent_rank()`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 排名通常被视为序数操作：第一，第二，第三，等等。如果你想要更接近范围的概念，也许是一个百分比，可以反映记录相对于同一窗口分区中其他记录的位置，请使用 `percent_rank()`。
- en: For every window, `percent_rank()` will compute the percentage rank (between
    zero and one) based on the ordered value. For those who are mathematically inclined,
    the formula is as follows.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个窗口，`percent_rank()` 将根据排序值计算百分比排名（介于零和一之间）。对于数学倾向的人，公式如下。
- en: '![](../Images/10-02-unnumb-01.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-02-unnumb-01.png)'
- en: Listing 10.12 Computing percentage rank for every recorded temperature per year
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.12 计算每年每个记录的温度的百分比排名
- en: '[PRE11]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ You can create a window spec from another window spec by chaining additional
    methods on it. Here, I create an ordered version of each_year, which partitions
    the records according to year.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你可以通过在它上面链式调用额外的方法来从另一个窗口规范创建一个窗口规范。在这里，我创建了一个按年排序的版本，它根据年份对记录进行分区。
- en: '❷ For example, this record has two records in 2019 with a value of less than
    44.7 and a total of four records in the window: 2 ÷(4 - 1) = 0.666.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 例如，这条记录在 2019 年有两个记录的值小于 44.7，在窗口中总共有四个记录：2 ÷(4 - 1) = 0.666。
- en: Creating buckets based on ranks, using ntile()
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 ntile() 根据排名创建桶
- en: This section covers a handy function that allows you to create an arbitrary
    number of buckets (called *tiles*) based on the rank of your data. You might have
    heard of quartiles (4 tiles), quintiles (5), deciles (10), or even percentiles
    (100). The `ntile()` computes n-tile for a given parameter `n`. The code in the
    next listing is visually depicted in figure 10.3.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一个实用的函数，它允许你根据数据的排名创建任意数量的桶（称为 *瓦片*）。你可能听说过四分位数（4 个瓦片）、五分位数（5 个）、十分位数（10
    个），甚至百分位数（100 个）。`ntile()` 函数用于计算给定参数 `n` 的 n-瓦片。下一列表中的代码在图 10.3 中进行了视觉描述。
- en: '![](../Images/10-03.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-03.png)'
- en: Figure 10.3 Two-tile for the three window partitions in `gsod_light`. If we
    consider each window to be a rectangle, each value takes the same space within
    that rectangle. With two tiles, the values under the 50% mark (including those
    overlapping) are in the first tile, whereas the ones fully over are in the second.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.3 `gsod_light` 中三个窗口分区的两瓦片。如果我们把每个窗口看作一个矩形，每个值在该矩形内占据相同的空间。有两个瓦片，低于 50%
    标记的值（包括重叠的）在第一个瓦片中，而完全超过的值在第二个瓦片中。
- en: Listing 10.13 Computing the two-tile value over the window
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.13 计算窗口中的两瓦片值
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Plain row numbers using row_number()
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 row_number() 的普通行号
- en: 'This section covers `row_number()`, which does exactly that: given an ordered
    window, it’ll give an increasing rank (1, 2, 3, . . .) regardless of the ties
    (the row number of tied records is nondeterministic, so if you need to have reproducible
    results, make sure you order each window so that there are no ties). This is identical
    to indexing each window.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 `row_number()`，它正是这样做的：给定一个有序窗口，它将为每个记录提供一个递增的排名（1，2，3，……），无论是否存在平局（平局记录的行号是不确定的，所以如果你需要可重复的结果，请确保对每个窗口进行排序，以便没有平局）。这与对每个窗口进行索引是相同的。
- en: Listing 10.14 Numbering records within each window partition using `row_number()`
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.14 使用 `row_number()` 对每个窗口分区内的记录进行编号
- en: '[PRE13]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ row_number() will give you strictly increasing ranks for every record in your
    window.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ row_number() 将为你的窗口中的每条记录提供严格递增的排名。
- en: 'Losers first: Ordering your WindowSpec using orderBy()'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 输家优先：使用 orderBy() 对 WindowSpec 进行排序
- en: Finally, what if we want to reverse the order of our window? Unlike the `orderBy()`
    method on the data frame, the `orderBy()` method on a window does not have an
    `ascending` parameter we can use. We need to resort to the `desc()`method on the
    `Column` object directly. It’s a minor annoyance that’s easily solved for.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，如果我们想反转窗口的顺序怎么办？与数据框上的 `orderBy()` 方法不同，窗口上的 `orderBy()` 方法没有 `ascending`
    参数可以使用。我们需要直接在 `Column` 对象上使用 `desc()` 方法。这是一个小麻烦，但很容易解决。
- en: Listing 10.15 Creating a window with a descending-ordered column
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.15 创建一个按降序排序的列的窗口
- en: '[PRE14]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ By default, a column will be ordered with ascending values. Passing the desc()
    method will reverse that order for that column.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 默认情况下，列将按升序值排序。传递 desc() 方法将反转该列的顺序。
- en: 'This section introduced the different types of ranking that PySpark provides
    in its window function API. With nonconsecutive/olympic, consecutive/dense, percent,
    tiles, and strict/row_number, you have a lot of options on the table when it comes
    to rank records. In the next section, I introduce analytic functions, which contain
    some of the coolest functionality of window functions: the ability to look back
    and ahead.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 PySpark 在其窗口函数 API 中提供的不同类型的排名。在非连续/奥运、连续/密集、百分比、瓦片和严格/行号中，当涉及到排名记录时，你有很多选项可供选择。在下一节中，我将介绍分析函数，它包含窗口函数中最酷的功能之一：回顾和前瞻的能力。
- en: '10.2.2 Analytic functions: Looking back and ahead'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 分析函数：回顾与前瞻
- en: This section covers a very useful group of functions that makes it possible
    for you to look at the records around the record at hand. Being able to look at
    a previous or following record unlocks a lot of functionality when building a
    time series feature. For instance, when doing modeling on time series data, one
    of the most important features are the observations in the past. Analytic window
    functions are by far the easiest way to do this.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一组非常有用的函数，它使你能够查看手头记录周围的记录。能够查看前一个或后一个记录，在构建时间序列特征时解锁了许多功能。例如，当对时间序列数据进行建模时，最重要的特征之一就是过去的观察结果。分析窗口函数无疑是做到这一点最简单的方法。
- en: Access the records before or after using lag() and lead()
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 lag() 和 lead() 之前或之后访问记录
- en: The two most important functions in the analytic functions family are `lag(col,`
    `n=1,` `default=None)` and `lead(col,` `n=1,` `default=None)`, which will give
    you the value of the `col` column of the `n`-th record before and after the record
    you’re over, respectively. If the record, offset by the lag/lead, falls beyond
    the boundaries of the window, Spark will default to `default`. To avoid `null`
    values, pass a value to the optional parameter `default`. In the next listing,
    I create two columns, one with a lag of one record, and one with a lag of two
    records. If we go beyond the window, we get `null` values since I did not provide
    a `default` parameter.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析函数家族中，最重要的两个函数是 `lag(col, n=1, default=None)` 和 `lead(col, n=1, default=None)`，它们分别会给出你正在查看的记录之前和之后的第
    `n` 条记录的 `col` 列的值。如果通过 lag/lead 偏移的记录超出了窗口的边界，Spark 将默认使用 `default`。为了避免 `null`
    值，请向可选参数 `default` 传递一个值。在下一个列表中，我创建了两个列，一个滞后一个记录，另一个滞后两个记录。如果我们超出窗口，我们会得到 `null`
    值，因为我没有提供 `default` 参数。
- en: Listing 10.16 Getting the temperature of the previous two observations using
    `lag()`
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.16 使用 `lag()` 获取前两个观察的温度
- en: '[PRE15]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ The previous observation of the second record is the twice-previous observation
    of the third record, and so on.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第二条记录的前一个观察结果是第三条记录的两次前一个观察结果，依此类推。
- en: Cumulative distribution of the records using cume_dist()
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 cume_dist() 计算记录的累积分布
- en: The last analytical function we cover is `cume_dist()`, and it is similar to
    `percent_rank()`. `cume_dist()`, as its name indicates, provides a cumulative
    distribution (in the statistical sense of the term) rather than a ranking (where
    `percent_rank()` shines).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要介绍的最后一个分析函数是 `cume_dist()`，它与 `percent_rank()` 类似。`cume_dist()`，正如其名称所示，提供的是累积分布（在统计意义上的术语），而不是排名（在
    `percent_rank()` 中表现突出）。
- en: 'Just like with `percent_rank()`, I find it easier to explain via a formula:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `percent_rank()` 一样，我发现通过公式解释它更容易：
- en: '![](../Images/10-03-unnumb-02.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-03-unnumb-02.png)'
- en: In practice, I use it when doing an EDA (exploratory data analysis) of the cumulative
    distribution of certain variables.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，我在进行某些变量的累积分布的 EDA（探索性数据分析）时使用它。
- en: Listing 10.17 `percent_rank()` and `cume_dist()` over a window
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.17 在窗口上使用 `percent_rank()` 和 `cume_dist()`
- en: '[PRE16]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '`cume_dist()` is an analytic function, not a ranking function, as it does not
    provide a rank. Instead, it provides the cumulative density function `F(x)` (for
    those statistically inclined) for the records in the data frame.'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`cume_dist()` 是一个分析函数，而不是排名函数，因为它不提供排名。相反，它提供了数据框中记录的累积密度函数 `F(x)`（对于那些对统计感兴趣的人来说）。'
- en: This section introduced the cornucopia of window functions. While it reads a
    little like a buffet, window functions are nothing more than functions that operate
    over a window, just like the functions that we saw in chapters 4 and 5 apply over
    the whole data frame at once. Once you see them applied in the wild, it gets easy
    to recognize good use cases for them and reach for your new tool. In the next
    section, I introduce window frames, which are a powerful tool for changing which
    records are used in the computation of a window function.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了窗口函数的丰富多样性。虽然它读起来有点像自助餐，但窗口函数不过是作用在窗口上的函数，就像我们在第 4 章和第 5 章中看到的函数一次作用于整个数据框一样。一旦你在野外看到它们的应用，就很容易识别它们的好用例，并伸手去拿你的新工具。在下一节中，我将介绍窗口框架，这是一个强大的工具，用于改变在窗口函数的计算中使用的记录。
- en: Exercise 10.2
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 10.2
- en: If you have a window where all the ordered values are the same, what is the
    result of applying `ntile()` to the window?
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个所有有序值都相同的窗口，应用 `ntile()` 到窗口的结果是什么？
- en: 10.3 Flex those windows! Using row and range boundaries
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 拉伸这些窗口！使用行和范围边界
- en: This section goes beyond the uniform window definition for each record. I introduce
    how we can build static, growing, and unbounded windows based on rows and ranges.
    Being able to fine-tune the boundaries of a window augments the capabilities of
    your code by flexing the concept of static window partitions. By the end of this
    section, you will be fully operational with windows in PySpark.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 本节不仅超越了每个记录的统一窗口定义。我介绍了如何根据行和范围构建静态、增长和无界窗口。能够微调窗口的边界通过灵活运用静态窗口分区概念，增强了代码的能力。在本节的结尾，你将能够完全掌握在PySpark中使用窗口。
- en: 'I start this section with a seemingly harmless operation: applying an average
    computation over two windows identically partitioned. The only difference is that
    the first one is not ordered while the second one is. Surely the order of a window
    would have no impact on the computation of the average, right?'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我从这个看似无害的操作开始本节：对两个相同分区窗口的平均值进行计算。唯一的区别是第一个窗口没有排序，而第二个窗口有排序。当然，窗口的顺序对平均值的计算应该没有影响，对吧？
- en: Check out the following listing—same window function, almost the same window
    (besides the ordering), different results.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下列表——相同的窗口函数，几乎相同的窗口（除了排序），不同的结果。
- en: Listing 10.18 Ordering a window and the computation of the average
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.18：对窗口进行排序和计算平均值
- en: '[PRE17]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '❶ All good: the average is consistent across each window, and the results are
    logical.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 所有都很好：每个窗口的平均值是一致的，结果是合理的。
- en: ❷ Some odd stuff is happening. It looks like each window grows, record by record,
    so the average changes every time.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 有些奇怪的事情发生了。看起来每个窗口都是按记录逐个增长的，所以平均值每次都会变化。
- en: This is fun stuff. Something with the ordering of a window messes up the computation.
    The official Spark API documentation informs us that when ordering is not defined,
    an unbounded window frame `(rowFrame,` `unboundedPreceding,` `unboundedFollowing)`
    is used by default. When ordering is defined, a growing window frame `(rangeFrame,`
    `unboundedPreceding,` `currentRow)` is used by default.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这很有趣。当窗口的顺序出错时，计算会受到影响。官方Spark API文档告诉我们，当未定义排序时，默认使用无界窗口框架`(rowFrame, unboundedPreceding,
    unboundedFollowing)`。当定义了排序时，默认使用增长窗口框架`(rangeFrame, unboundedPreceding, currentRow)`。
- en: The secret to deciphering the new, mysterious behavior is to understand the
    types of window frames we can build and how they are used. I start by introducing
    the different *frame sizes* (static versus growing versus unbounded) and how to
    reason about them before adding the second dimension, the *frame type* (range
    versus rows). At the end of this section, the explanation for the previous code
    will make perfect sense, and you will be able to flex your window skills regardless
    of the situation.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 解密这种新、神秘行为的关键是理解我们可以构建的窗口框架类型以及它们的使用方式。我首先介绍不同的*框架大小*（静态、增长、无界）以及如何推理它们，然后再添加第二个维度，即*框架类型*（范围与行）。在本节的结尾，对之前代码的解释将变得完全合理，你将能够根据具体情况灵活运用窗口技能。
- en: '10.3.1 Counting, window style: Static, growing, unbounded'
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.1 计数，窗口风格：静态、增长、无界
- en: This section covers the boundaries of a window, something we call a *window
    frame*. I break the traditional catch-all window (where a window is equal to the
    whole partition) to introduce record-based boundaries. This will provide an incredible
    new layer of flexibility when using window functions, as it controls the scope
    of visibility of a record within the window. You’ll be able to create window functions
    that only look in the past and avoid feature leakage when working with time series.
    This is just one of the many use cases for flexible window frames!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了窗口的边界，我们称之为*窗口框架*。我将传统的包含所有窗口（窗口等于整个分区）的做法打破，引入基于记录的边界。这将在使用窗口函数时提供令人难以置信的新灵活性，因为它控制了记录在窗口中的可见范围。你将能够创建只查看过去并避免在处理时间序列时特征泄露的窗口函数。这仅仅是灵活窗口框架的许多用例之一！
- en: Tip Feature leakage happens when you use future information when building a
    predictive model. An example of this would be to use tomorrow’s rainfall to predict
    the total rainfall for the upcoming week. See chapters 12 and 13 for more information
    on features and feature leakage.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：特征泄露发生在构建预测模型时使用未来信息。一个例子就是使用明天的降雨量来预测即将到来的周的总降雨量。有关特征和特征泄露的更多信息，请参阅第12章和第13章。
- en: 'Before getting started, let’s take a visual of a window: when a function is
    applied to it, a window spec partitions a data frame based on one or more column
    values and then (potentially) orders them. Spark also provides the `rowsBetween()`
    and `rangeBetween()` methods to create window frame boundaries. For this section,
    I focus on row boundaries since they are closer to what we would expect. Section
    10.3.2 explains the difference between ranges versus rows.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在开始之前，让我们先想象一下窗口的视觉概念：当一个函数作用于它时，窗口规范会根据一个或多个列值将数据帧分区，然后（可能）对它们进行排序。Spark 还提供了
    `rowsBetween()` 和 `rangeBetween()` 方法来创建窗口框架边界。在本节中，我专注于行边界，因为它们更接近我们预期的结果。第 10.3.2
    节解释了范围与行之间的区别。
- en: '![](../Images/10-04.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/10-04.png)'
- en: Figure 10.4 The different possible boundaries within a window. Some are numerical;
    some have reserved keywords. We count forward (up to `Window.unboundedFollowing`)
    when looking at records ahead, and backward (down to `Window.unboundedPreceding`)
    when looking at records behind.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 10.4 窗口内的不同可能的边界。有些是数值的；有些有保留关键字。当我们查看记录的前面时，我们向前计数（直到 `Window.unboundedFollowing`），当我们查看记录的后面时，我们向后计数（直到
    `Window.unboundedPreceding`）。
- en: When using an unbounded/unordered window, we do not care about which record
    is which. This changes when we are using ranking or analytical functions. For
    a rank, a lag, or a lead, for instance, Spark will call the record being processed
    the `Window` `.currentRow`. (I keep the name of the class. Using the `currentRow`
    keyword makes it obvious you are working with window functions.) The record before
    takes a value of `-1`, and so on, until the first record, named `Window.unboundedPreceding`.
    The record following the current row takes a value of `1`, and so on, until the
    last record, named `Window.unboundedFollowing`.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用无界/无序窗口时，我们不在乎哪个记录是哪个。当我们使用排名或分析函数时，情况就改变了。例如，对于一个排名、滞后或领先，Spark 会将正在处理的记录称为
    `Window` 的 `.currentRow`。 （我保留了类的名称。使用 `currentRow` 关键字使得你正在使用窗口函数显而易见。）之前的记录取值为
    `-1`，以此类推，直到第一个记录，命名为 `Window.unboundedPreceding`。当前行之后的记录取值为 `1`，以此类推，直到最后一条记录，命名为
    `Window.unboundedFollowing`。
- en: Warning *Do not* use a numerical value to represent the first or last record
    in a window. It makes your code harder to reason about, and you never know when
    the window will grow beyond that size. Internally, Spark will translate `Window.unboundedPreceding`
    and `Window.unboundedFollowing` to appropriate numerical values, so you don’t
    have to.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 *不要* 使用数值来表示窗口中的第一条或最后一条记录。这会使你的代码更难推理，而且你永远不知道窗口何时会超过那个大小。内部，Spark 会将 `Window.unboundedPreceding`
    和 `Window.unboundedFollowing` 转换为适当的数值，所以你不需要这样做。
- en: Let’s go back to listing 10.18; we can “add” boundaries to our window spec.
    In listing 10.19, I explicitly add the boundaries that Spark assumes when none
    are provided. This means that `not_ordered` and `ordered` will provide the same
    results whether we define the boundaries (listing 10.19) or not (listing 10.18).
    If I want to be very accurate, the `ordered` window spec is bounded by range,
    not rows, but for our data frame, it works just the same. I’ll trade accuracy
    for ease of understanding for now, but if you apply it to the `gsod` data frame,
    results will differ slightly (see section 10.3.2).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到列表 10.18；我们可以在我们的窗口规范中“添加”边界。在列表 10.19 中，我明确添加了 Spark 在没有提供任何边界时假设的边界。这意味着无论我们是否定义边界（列表
    10.19），“not_ordered”和“ordered”都会提供相同的结果。如果我想非常准确，`ordered` 窗口规范是由范围而不是行界定的，但对我们这个数据帧来说，效果是一样的。我现在会为了易于理解而牺牲准确性，但如果你将其应用于
    `gsod` 数据帧，结果会有所不同（见第 10.3.2 节）。
- en: Listing 10.19 Rewriting the window spec with explicit window boundaries
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.19 使用显式窗口边界重写窗口规范
- en: '[PRE18]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '❶ This window is unbounded: every record, from the first to the last, is in
    the window.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个窗口是无界的：从第一条记录到最后一条记录，每条记录都在窗口内。
- en: '❷ This window is growing to the left: every record up to the current row value
    is included in a window.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 这个窗口正在向左扩展：直到当前行值的所有记录都包含在窗口内。
- en: Because the window used in the computation of `avg_NO` is *unbounded*, meaning
    that it spans from the first to the last record of the window, the average is
    consistent across the whole window. The one used in the computation of `avg_O`
    is *growing* on the left, meaning that the right record is bounded to the `currentRow`,
    where the left record is set at the first value of the window. As you move from
    one record to the next, the average is over more and more values. The average
    of the last record of the window contains all the values (because `currentRow`
    is the last record of the window). A *static* window frame is nothing more than
    a window where both records are bounded relative to the current row; for example,
    `rowsBetween(-1,` `1)` for a window that contains the current row, the record
    immediately preceding, and the record immediately following.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 因为在计算`avg_NO`时使用的窗口是**无界的**，意味着它跨越了窗口中的第一条记录到最后一条记录，所以平均数在整个窗口内是一致的。而在计算`avg_O`时，窗口在左侧是**增长的**，意味着右侧的记录被限制在`currentRow`上，而左侧的记录被设置为窗口的第一个值。当你从一个记录移动到下一个记录时，平均数会覆盖越来越多的值。窗口的最后一条记录的平均数包含了所有值（因为`currentRow`是窗口的最后一条记录）。一个**静态**的窗口框架不过是一个窗口，其中两个记录相对于当前行都是有限制的；例如，对于包含当前行、紧邻的前一条记录和紧邻的后一条记录的窗口，使用`rowsBetween(-1,
    1)`。
- en: Warning If your window spec is not ordered, using a boundary is a nondeterministic
    operation. Spark will not guarantee that your window will contain the same values
    as we are not ordering within a window before picking the boundary. This also
    applies if you order the data frame in a previous operation. If you use a boundary,
    provide an explicit ordering clause.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：如果你的窗口规范没有排序，使用边界是一个非确定性的操作。Spark无法保证你的窗口将包含与我们在选择边界之前在窗口内未排序相同的值。这也适用于你在之前的操作中对数据帧进行排序的情况。如果你使用边界，请提供一个明确的排序子句。
- en: In practice, it is easy to know what kind of window you need. Ranking and analytical
    functions rely on ordered windows, since order matters in their application. Aggregate
    functions don’t care about the ordering of values, so you *shouldn’t* use them
    with an ordered window spec unless you want partial aggregation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，很容易知道你需要哪种类型的窗口。排名和分析函数依赖于有序窗口，因为它们的适用性中顺序很重要。聚合函数不关心值的排序，所以你**不应该**使用有序窗口规范，除非你想要部分聚合。
- en: 'This section covered the different types of window boundaries, and partially
    explained the behavior of the growing average when using ordered window specs.
    In the next section, I introduce the last core window concept: range versus rows.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了不同类型的窗口边界，并部分解释了在有序窗口规范中使用时增长平均数的行为。在下一节中，我将介绍最后一个核心窗口概念：范围与行。
- en: '10.3.2 What you are vs. where you are: Range vs. rows'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3.2 你所是 vs. 你所在：范围 vs. 行
- en: This section covers the subtle yet incredibly important difference between row
    windows versus range windows. This notion unlocks the option to build windows
    that care about the content of the ordered column, not just its position. Working
    with ranges is useful when working with dates and time, as you may want to gather
    windows based on time intervals that are different than the primary measure. As
    an example, the `gsod` data frame collects daily temperature information. What
    happens if we want to compare this temperature to the average of the previous
    month? Months have 28, 29, 30, or 31 days. This is where ranges get useful.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了行窗口与范围窗口之间微妙但极其重要的区别。这个概念解锁了构建关注有序列内容而不是其位置的窗口的选项。当处理日期和时间时，与主要度量标准不同的时间间隔时，使用范围很有用。例如，`gsod`数据帧收集每日温度信息。如果我们想将这个温度与上个月的平均值进行比较会发生什么？月份有28天、29天、30天或31天。这就是范围变得有用的地方。
- en: To start, I transform the `gsod_light` data frame slightly in listing 10.20\.
    First, I convert all the dates in 2019 using `F.lit(2019)` as a column value so
    that we have a single window when breaking by year; this will give us more data
    to play with when using ranges. I also create a `dt` column that contains the
    date of the observation before transforming it into an integer value in the `dt_num`
    column. Range windows in PySpark work only on numerical columns; `unix_timestamp()`
    transforms the date in a number of seconds since 1970-01-01 00:00:00 UTC (the
    UNIX epoch). This gives us a serviceable date-like number we can use for our range
    windows.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我在列表10.20中对`gsod_light`数据框进行了一些微小的转换。我使用`F.lit(2019)`作为列值将2019年的所有日期转换为整数，以便在按年分割时只有一个窗口；这将给我们更多的数据来使用范围。我还创建了一个包含观测日期的`dt`列，在将其转换为`dt_num`列的整数值之前。PySpark中的范围窗口仅在数值列上工作；`unix_timestamp()`将日期转换为自1970-01-01
    00:00:00 UTC（UNIX纪元）以来的秒数。这为我们提供了一个可用的类似日期的数字，我们可以用它来创建范围窗口。
- en: Listing 10.20 Creating a date column to apply range window on
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.20 创建日期列以应用范围窗口
- en: '[PRE19]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ The new column is of type DateType(), which can be treated (window wise) as
    a number.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 新列的类型是DateType()，它可以被（按窗口方式）视为一个数字。
- en: ❷ When using PySpark, windows must be over numerical values. Using unix_timestamp()
    is the easiest way to convert a date/timestamp to a number.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当使用PySpark时，窗口必须覆盖数值。使用unix_timestamp()是将日期/时间戳转换为数字的最简单方法。
- en: 'For a simple range window, let’s compute the average of the temperatures recorded
    one month before and after a given day. Because our numerical date is in seconds,
    I’ll keep things simple and say that 1 month = 30 days = 720 hours = 43,200 minutes
    = 2,592,000 seconds.[¹](#pgfId-1035237) Visually, the window for a record would
    look like figure 10.5: for each record, Spark computes the left and right (or
    lower and upper) window boundaries, and those boundaries are used to determine
    if a record is in the window.'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个简单的范围窗口，让我们计算给定日期一个月前和一个月后的温度平均值。因为我们的数值日期是以秒为单位的，所以我会保持简单，并说1个月=30天=720小时=43,200分钟=2,592,000秒。[¹](#pgfId-1035237)
    从视觉上看，记录的窗口将类似于图10.5：对于每条记录，Spark计算左和右（或下和上）窗口边界，并使用这些边界来确定记录是否在窗口内。
- en: '![](../Images/10-05.png)'
  id: totrans-198
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-05.png)'
- en: Figure 10.5 Displaying a window with a range of `(-2_592_000,` `2_592_000)`
    (or ±30 days, in seconds)
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.5 显示范围在`(-2_592_000,` `2_592_000)`（或±30天，以秒为单位）的窗口
- en: In the next listing, we create a range window over 60 days (30 before, 30 after),
    partitioning by `year`; our window frame is ordered by `dt_num`, so we can use
    the `rangeBetween` over the number of seconds.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个列表中，我们创建了一个60天（30天前，30天后）的范围窗口，按`year`分区；我们的窗口框架按`dt_num`排序，因此我们可以使用`rangeBetween`来处理秒数。
- en: Listing 10.21 Computing the average temperature for a 60-day sliding window
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.21 计算一个60天滑动窗口的平均温度
- en: '[PRE20]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ The range becomes (current_row_value – ONE_MONTH_ISH, current_row_value +
    ONE_MONTH_ISH).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 范围变为（当前行值 – ONE_MONTH_ISH，当前行值 + ONE_MONTH_ISH）。
- en: 'For each record in the window, Spark computes the range boundaries based on
    the current row value (from the field `dt_num`) and determines the actual window
    that it will aggregate over. This makes it easy to compute sliding or growing
    time/date windows: when working with row ranges, you can say only that “*X* records
    before and after.” When using `Window.currentRow/unboundedFollowing/unboundedPreceding`
    with a range window, Spark will use the value of the record as the range boundary.
    If you have multiple observations for a given time, your row-based window frame
    will not work. Using ranges and looking at the actual values makes your window
    respect the context you’re applying it over.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于窗口中的每条记录，Spark根据当前行的值（来自字段`dt_num`）计算范围边界，并确定它将聚合的实际窗口。这使得计算滑动或增长的时间/日期窗口变得容易：当处理行范围时，你只需说“*X*条记录之前和之后。”当使用`Window.currentRow/unboundedFollowing/unboundedPreceding`与范围窗口一起使用时，Spark将使用记录的值作为范围边界。如果你对于给定时间有多个观测值，基于行的窗口框架将不会工作。使用范围并查看实际值可以使你的窗口尊重你应用它的上下文。
- en: '![](../Images/10-06.png)'
  id: totrans-205
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/10-06.png)'
- en: Figure 10.6 A matrix for the window types available in Spark
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 图10.6 Spark中可用的窗口类型矩阵
- en: This section explained the difference between row- and range-based windows and
    where one is best applied versus the other. This concludes the “standard” portion
    of window functions. With this chapter under your belt, you should be comfortable
    applying window functions as part of data analyses or feature engineering operations.
    Before concluding this chapter, I’ve added an optional, bonus section that covers
    how we can apply UDFs over windows. With this, you’ll be able to break the mold
    and compose your own window functions.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节解释了基于行和基于范围的窗口之间的区别，以及何时最好应用其中一个而不是另一个。这完成了窗口函数的“标准”部分。带着这一章的知识，你应该能够舒适地将窗口函数作为数据分析或特征工程操作的一部分来应用。在结束这一章之前，我增加了一个可选的额外部分，介绍了我们如何可以在窗口上应用UDF。这样，你将能够打破常规，创建自己的窗口函数。
- en: Exercise 10.3
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 练习10.3
- en: If you have a data frame with 1,000,001 rows, where the ordered column `ord`
    is defined by `F.lit(10)`, what is the result of the following window functions?
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个包含1,000,001行的数据框架，其中有序列`ord`由`F.lit(10)`定义，以下窗口函数的结果是什么？
- en: '`F.count("ord").over(Window.partitionBy().orderBy("ord").rowsBetween(-2,` `2))`'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`F.count("ord").over(Window.partitionBy().orderBy("ord").rowsBetween(-2, 2))`'
- en: '`F.count("ord").over(Window.partitionBy().orderBy("ord").rangeBetween(-2,`
    `2))`'
  id: totrans-211
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`F.count("ord").over(Window.partitionBy().orderBy("ord").rangeBetween(-2, 2))`'
- en: '10.4 Going full circle: Using UDFs within windows'
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.4 完美循环：在窗口中使用UDF
- en: 'This section teaches what I consider the PySpark-iest thing PySpark can do:
    using UDFs within windows. It uses two very Sparky things: UDFs and the split-apply-combine
    paradigm we learned in chapter 9\. It’s also something that is Python-specific,
    as it relies on pandas UDFs. PySpark all the way down! pandas UDFs in a window
    definition are useful when you need the flexibility of pandas UDFs. For instance,
    when you need functionality not defined in the set of window functions available
    in PySpark, just define UDFs to implement the functionality!'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节介绍了我认为PySpark能做的最PySpark的事情：在窗口中使用UDF。它使用了两个非常Sparky的东西：UDF和我们在第9章学到的拆分-应用-组合范式。这同样也是Python特有的，因为它依赖于pandas
    UDF。PySpark到底了！在窗口定义中使用pandas UDF在需要pandas UDF的灵活性时很有用。例如，当你需要PySpark中可用的窗口函数集中未定义的功能时，只需定义UDF来实现该功能！
- en: 'This is not a very long section, as we build on existing knowledge. For a complete
    refresher on pandas UDFs, refer to chapter 9\. The recipe for applying a pandas
    UDF is very simple:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节并不长，因为我们是在现有知识的基础上构建的。要全面复习pandas UDF，请参阅第9章。应用pandas UDF的配方非常简单：
- en: We need to use a *Series to Scalar* UDF (or a group aggregate UDF). PySpark
    will apply the UDF to every window (once per record) and put the (scalar) value
    as a result.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用一个*Series to Scalar* UDF（或一个分组聚合UDF）。PySpark将UDF应用于每个窗口（每条记录一次），并将（标量）值作为结果。
- en: A UDF over *unbounded window frames* is only supported by Spark 2.4 and above.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅支持Spark 2.4及以上版本的*无界窗口框架*上的UDF。
- en: A UDF over *bounded window frames* is only supported by Spark 3.0 and above.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 仅支持Spark 3.0及以上版本的*有界窗口框架*上的UDF。
- en: The rest? Business as usual. In listing 10.22, I create a `median` UDF using
    the Spark 3-type hint notation. If you use Spark 2.4, change the decorator to
    `@F.pandas_udf ("double",` `PandasUDFType.GROUPED_AGG)` and remove the type hints.
    This simple `median` function computes the median of a pandas `Series`. I then
    apply it twice to the `gsod_light` data frame. There is nothing remarkable to
    see here; it just works.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的呢？一切照旧。在列表10.22中，我使用Spark 3类型的提示符号创建了一个`median` UDF。如果你使用Spark 2.4，将装饰器改为`@F.pandas_udf
    ("double", PandasUDFType.GROUPED_AGG)`并移除类型提示。这个简单的`median`函数计算pandas `Series`的中位数。然后我将它两次应用于`gsod_light`数据框架。这里没有什么特别之处可以看；它只是正常工作。
- en: Warning Do not modify the `Series` you are passing as input. Doing so will introduce
    hard-to-find bugs in your code, and we saw in chapter 9 how nasty UDF bugs are.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 不要修改你作为输入传递的`Series`。这样做会在你的代码中引入难以发现的错误，我们在第9章中看到了UDF错误的恶劣性。
- en: Listing 10.22 Using a pandas UDF over window intervals
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.22 在窗口区间使用pandas UDF
- en: '[PRE21]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ The UDF is applied over an unbounded/unordered window frame.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用户定义函数（UDF）被应用于一个无界/无序的窗口框架中。
- en: ❷ The same UDF is now applied over a bounded/ordered window frame.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 现在相同的用户定义函数（UDF）被应用于一个有界/有序的窗口框架中。
- en: ❸ Since the window is unbounded, every record within a window has the same median.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于窗口是无界的，窗口内的每个记录都具有相同的中位数。
- en: ❹ Since the window is bounded to the right, the median changes as we add more
    records to the window.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 由于窗口是有界到右边的，随着我们向窗口中添加更多记录，中位数会发生变化。
- en: '10.5 Look in the window: The main steps to a successful window function'
  id: totrans-226
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.5 在窗口中寻找：成功窗口函数的主要步骤
- en: 'This concludes the chapter about window functions. I encourage you to expand
    your data manipulation, analysis, and feature engineering arsenal to incorporate
    window function-based transformation. If you are stumped on how to perform a certain
    transformation, always remember the basic parameters of using a window function:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了关于窗口函数的章节。我鼓励你扩展你的数据处理、分析和特征工程工具集，以包含基于窗口函数的转换。如果你在执行某种转换时遇到难题，请始终记住使用窗口函数的基本参数：
- en: What kind of operation do I want to perform? Summarize, rank, or look ahead/behind.
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我想要执行哪种操作？总结、排名，还是向前/向后查看。
- en: How do I need to construct my window? Should it be bounded or unbounded? Do
    I need every record to have the same window value (unbounded), or should the answer
    depend on where the record fits within the window (bounded)? When bounding a window
    frame, you most often want to order it as well.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我应该如何构建我的窗口？它应该是有界还是无界？我是否需要每个记录都有相同的窗口值（无界），或者答案应该取决于记录在窗口中的位置（有界）？在界定窗口框架时，你通常还希望对其进行排序。
- en: For bounded windows, do you want the window frame to be set according to the
    position of the record (row based) or the value of the record (range based)?
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于有界窗口，你希望窗口框架根据记录的位置（基于行）还是记录的值（基于范围）来设置？
- en: Finally, remember that a window function does not make your data frame special.
    After your function is applied, you can filter, group by, and even apply another,
    completely different, window.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，请记住，窗口函数并不会使你的数据框变得特殊。在你应用函数之后，你可以进行过滤、按组分组，甚至应用另一个完全不同的窗口。
- en: As a parting gift, it seems like window functions are a favorite of data analysts
    and scientists’ interviews. Applying window functions in PySpark will become second
    nature!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 作为临别礼物，窗口函数似乎成了数据分析师和科学家面试中的宠儿。在PySpark中应用窗口函数将变得习以为常！
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Window functions are functions that are applied over a portion of a data frame
    called a window frame. They can perform aggregation, ranking, or analytical operations.
    A window function will return the data frame with the same number of records,
    unlike its siblings the `groupby-aggregate` operation and the group map UDF.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口函数是在数据框的一部分上应用的功能，这部分称为窗口框架。它们可以执行聚合、排名或分析操作。窗口函数将返回具有相同记录数量的数据框，与它的兄弟`groupby-aggregate`操作和group
    map UDF不同。
- en: A window frame is defined through a window spec. A window spec mandates how
    the data frame is split (`partitionBy()`), how it’s ordered (`orderBy()`), and
    how it’s portioned (`rowsBetween()/rangeBetween()`).
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口框架是通过窗口规范定义的。窗口规范规定了数据框是如何分割的（`partitionBy()`），如何排序（`orderBy()`），以及如何分割（`rowsBetween()/rangeBetween()`）。
- en: By default, an unordered window frame will be unbounded, meaning that the window
    frame will be equal to the window partition for every record. An ordered window
    frame will grow to the left, meaning that each record will have a window frame
    ranging from the first record in the window partition to the current record.
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 默认情况下，无序窗口框架将是无界的，这意味着窗口框架将等于每个记录的窗口分区。有序窗口框架将向左扩展，这意味着每个记录都将有一个从窗口分区中的第一条记录到当前记录的窗口框架。
- en: A window can be bounded by row, meaning that the records included in the window
    frame are tied to the row boundaries passed as parameters (with the range boundaries
    added to the row number of the current row), or by range, meaning that the records
    included in the window frame depend on the value of the current row (with the
    range boundaries added to the value).
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 窗口可以通过行进行界定，这意味着窗口框架中包含的记录与作为参数传递的行边界相关联（范围边界添加到当前行的行号），或者通过范围进行界定，这意味着窗口框架中包含的记录取决于当前行的值（范围边界添加到值）。
- en: Additional Exercises
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 10.4
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.4
- en: Using the following code, first identify the day with the warmest temperature
    for each year, and then compute the average temperature. What happens when there
    are more than two occurrences?
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下代码，首先确定每年最热的天气，然后计算平均温度。当有超过两个发生时会发生什么？
- en: '[PRE22]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Exercise 10.5
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.5
- en: How would you create a rank that is full, meaning that each record within a
    the `temp_per_month_asc` has a unique rank, using the `gsod_light` data frame?
    For records with an identical `orderBy()` value, the order of rank does not matter.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 你将如何创建一个完整的排名，这意味着`temp_per_month_asc`中的每个记录都有一个唯一的排名，使用`gsod_light`数据框？对于具有相同`orderBy()`值的记录，排名的顺序并不重要。
- en: '[PRE23]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ These records should be 1 and 2.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这些记录应该是1和2。
- en: Exercise 10.6
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.6
- en: Take the `gsod` data frame (not the `gsod_light`) and create a new column that
    is `True` if the temperature at a given station is maximum for that station and
    a time window of seven days (before and after), and `False` otherwise.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `gsod` 数据框（而不是 `gsod_light`），创建一个新列，如果给定站点的温度是该站点在七天时间窗口（前后）内的最高温度，则为 `True`，否则为
    `False`。
- en: Exercise 10.7
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习10.7
- en: How would you create a window like the code that follows, but taking into account
    that months have different number of days? For instance, March has 31 days, but
    April has 30 days, so you can’t do a window spec over a set number of days.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 你会如何创建一个像以下代码一样的窗口，但考虑到月份有不同的天数？例如，三月有31天，但四月有30天，所以你不能在固定天数上做窗口指定。
- en: '(Hint: My solution doesn’t use `dt_num`.)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: (提示：我的解决方案不使用 `dt_num`。)
- en: '[PRE24]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '* * *'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ¹ If we want to be very strict on “a month is a month,” check the exercises.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ¹ 如果我们要对“一个月是一个月”非常严格，请检查练习。
