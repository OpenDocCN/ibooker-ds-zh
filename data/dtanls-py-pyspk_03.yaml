- en: 2 Your first data program in PySpark
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 在PySpark中的第一个数据程序
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Launching and using the `pyspark` shell for interactive development
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 启动和使用 `pyspark` shell 进行交互式开发
- en: Reading and ingesting data into a data frame
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据读取和摄取到数据框中
- en: Exploring data using the `DataFrame` structure
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `DataFrame` 结构探索数据
- en: Selecting columns using the `select()` method
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `select()` 方法选择列
- en: Reshaping single-nested data into distinct records using `explode()`
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `explode()` 将单层嵌套数据重塑为不同的记录
- en: Applying simple functions to your columns to modify the data they contain
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将简单函数应用于您的列以修改它们包含的数据
- en: Filtering columns using the `where()` method
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 `where()` 方法过滤列
- en: 'Data-driven applications, no matter how complex, all boil down to what we can
    think of as three meta steps, which are easy to distinguish in a program:'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 任何复杂的数据驱动应用程序，无论多么复杂，都可以归结为我们认为的三个元步骤，这些步骤在程序中很容易区分：
- en: We start by *loading* or reading the data we wish to work with.
  id: totrans-10
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们首先*加载*或读取我们希望处理的数据。
- en: We *transform* the data, either via a few simple instructions or a very complex
    machine learning model.
  id: totrans-11
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们通过一些简单的指令或一个非常复杂的机器学习模型来*转换*数据。
- en: We then *export* (or *sink*) the resulting data, either into a file or by summarizing
    our findings into a visualization.
  id: totrans-12
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们将结果数据*导出*（或*存储*），无论是放入文件中还是将我们的发现总结成可视化。
- en: The next two chapters will introduce a basic workflow with PySpark via the creation
    of a simple ETL (*extract, transform, and load*, which is a more business-speak
    way of saying *ingest, transform, and export*). You will find these three simple
    steps repeated in every program we build in this book, from a simple summary to
    the most complex ML model. We will spend most of our time in the `pyspark` shell,
    interactively building our program one step at a time. Just like normal Python
    development, using the shell or REPL (I’ll use the terms interchangeably) provides
    rapid feedback and quick progression. Once we are comfortable with the results,
    we will wrap our program so we can submit it in batch mode.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的两章将通过创建简单的ETL（*提取、转换和加载*，这是一种更商业化的说法，即*摄取、转换和导出*）来介绍PySpark的基本工作流程。您将在本书构建的每个程序中找到这三个简单的步骤，从简单的摘要到最复杂的ML模型。我们将大部分时间花在
    `pyspark` shell 中，逐个步骤交互式地构建我们的程序。就像正常的Python开发一样，使用shell或REPL（我将交替使用这两个术语）提供快速反馈和快速进展。一旦我们对结果感到满意，我们将包装我们的程序，以便我们可以以批量模式提交它。
- en: Note REPL stands for *read, evaluate, print, and loop*. In the case of Python,
    it represents the interactive prompt in which we input commands and read results.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，REPL代表*读取、评估、打印和循环*。在Python的情况下，它代表我们输入命令并读取结果的交互式提示符。
- en: Data manipulation is the most basic and important aspect of any data-driven
    program, and PySpark puts a lot of focus on this. It serves as the foundation
    of any reporting, machine learning, or data science exercise we wish to perform.
    This section gives you the tools to not only use PySpark to manipulate data at
    scale but also to think in terms of data transformation. We obviously can’t cover
    every function provided in PySpark, but I provide a good explanation of the ones
    we use. I also introduce how to use the shell as a friendly reminder for those
    cases when you forget how something works.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 数据操作是任何数据驱动程序最基本且最重要的方面，PySpark非常注重这一点。它是我们希望执行的任何报告、机器学习或数据科学练习的基础。本节为您提供工具，不仅可以使用PySpark在规模上操作数据，还可以从数据转换的角度思考。显然，我们无法涵盖PySpark提供的每个函数，但我提供了对我们使用的函数的良好解释。我还介绍了如何使用shell作为友好提醒，以防您忘记某些功能的工作方式。
- en: 'Since this is your first end-to-end program in PySpark, we get our feet wet
    with a simple problem to solve: What are the most popular words used in the English
    language? Since collecting all the material ever produced in the English language
    would be a massive undertaking, we start with a very small sample: *Pride and
    Prejudice*, by Jane Austen. We first make our program work with this small sample
    and then scale it to ingest a larger corpus of text. I use this principle—starting
    with a sample of the data locally to get the structure and concepts right—when
    building a new program; when working in a cloud environment, this means less cost
    when exploring. Once I am confident about the flow of my program, I go all nodes
    blazing on the full data set.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是您在 PySpark 中的第一个端到端程序，我们将从一个简单的问题开始解决：英语中最常用的单词是什么？由于收集英语语言中所有产生的材料将是一项庞大的工作，我们从一个非常小的样本开始：简·奥斯汀的《傲慢与偏见》。我们首先让程序与这个小的样本一起工作，然后将其扩展以处理更大的文本语料库。我在构建新程序时使用这个原则——从本地数据样本开始，以正确地获取结构和概念——当在云环境中工作时，这意味着探索时成本更低。一旦我对程序的流程有信心，我就会在完整的数据集上全速运行所有节点。
- en: Since this is our first program, and I need to introduce many new concepts,
    this chapter will focus on the data manipulation part of the program. Chapter
    3 will cover the final computation, as well as wrapping our program and then scaling
    it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是我们第一个程序，我需要介绍许多新概念，因此本章将专注于程序的数据操作部分。第 3 章将涵盖最终的计算，以及封装我们的程序并对其进行扩展。
- en: Tip The book repository contains the code and data used for the examples and
    exercises. It is available online at [http://mng.bz/6ZOR](http://mng.bz/6ZOR).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：本书的代码库包含了示例和练习中使用的代码和数据。您可以在 [http://mng.bz/6ZOR](http://mng.bz/6ZOR) 上在线获取。
- en: 2.1 Setting up the PySpark shell
  id: totrans-19
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.1 设置 PySpark shell
- en: Python provides a REPL for interactive development. Since PySpark is a Python
    library, it also uses the same environment. It speeds up your development process
    by giving instantaneous feedback the moment you submit an instruction instead
    of forcing you to compile your program and submit it as one big monolithic block.
    I’ll even say that using a REPL is even more useful in PySpark, since every operation
    can take a fair amount of time. Having a program crash midway is always frustrating,
    but it’s even worse when you’ve been running a data-intensive job for a few hours.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Python 提供了一个交互式开发环境（REPL）。由于 PySpark 是一个 Python 库，它也使用相同的环境。它通过在您提交指令时立即提供反馈来加速您的开发过程，而不是强迫您编译整个程序并作为一个大型的单体块提交。我甚至可以说，在
    PySpark 中使用 REPL 更加有用，因为每个操作都可能需要相当长的时间。程序在中间崩溃总是令人沮丧的，但当你已经运行了一个数据密集型作业几个小时后，这会更糟。
- en: For this chapter (and the rest of the book), I assume that you have access to
    a working installation of Spark, either locally or in the cloud. If you want to
    perform the installation yourself, appendix B contains step-by-step instructions
    for Linux, macOS, and Windows. If you can’t install it on your computer, or prefer
    not to, the same appendix also provides a few cloud-powered options.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章（以及本书的其余部分），我假设您可以访问一个工作状态的 Spark 安装，无论是本地还是云上。如果您想自己进行安装，附录 B 包含了针对 Linux、macOS
    和 Windows 的逐步安装说明。如果您无法在自己的计算机上安装它，或者您更愿意不安装，同样的附录还提供了一些基于云的选项。
- en: Once everything is set up, the easiest way to ensure that everything is running
    is by launching the `PySpark` shell by inputting `pyspark` into your terminal.
    You should see an ASCII-art version of the Spark logo, as well as some useful
    information. Listing 2.1 shows what happens on my local machine. In section 2.1.1,
    you’ll find a less magical alternative to running `pyspark` as a command that
    will help you with integrating PySpark into an existing Python REPL.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 一切设置完成后，确保一切运行的最简单方法是通过在终端中输入 `pyspark` 来启动 `PySpark` shell。您应该会看到一个 Spark 标志的
    ASCII 艺术版本以及一些有用的信息。列表 2.1 展示了我本地机器上发生的情况。在第 2.1.1 节中，您将找到一个运行 `pyspark` 作为命令的更不神奇的选择，这将帮助您将
    PySpark 集成到现有的 Python REPL 中。
- en: Listing 2.1 Launching `pyspark` on a local machine
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.1 在本地机器上启动 `pyspark`
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ When using PySpark locally, you most often won’t have a full Hadoop cluster
    preconfigured. For learning purposes, this is perfectly fine.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当在本地使用 PySpark 时，您通常不会有一个预先配置好的完整 Hadoop 集群。出于学习目的，这是完全可以接受的。
- en: ❷ Spark is indicating the level of details it’ll provide to you. We will see
    how to configure this in section 2.1.2.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Spark 正在指示它将为您提供多少细节。我们将在第 2.1.2 节中看到如何配置这一点。
- en: ❸ We are using Spark version 3.2.0.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们使用 Spark 版本 3.2.0。
- en: ❹ PySpark is using the Python available on your path. This will display the
    Python version on the master node. Since we are working locally, this is the Python
    installed on my machine.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ PySpark正在使用你路径上的Python。这将显示主节点上的Python版本。由于我们是在本地工作，这是安装在我机器上的Python。
- en: ❺ The Spark UI is available at this address (check chapter 11 on how to use
    it efficiently).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ Spark UI可在该地址访问（请参阅第11章了解如何高效使用它）。
- en: ❻ The pyspark shell provides an entry point for you through the variables spark
    and sc. More on this insection 2.1.1.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ pyspark shell通过变量spark和sc为您提供了一个入口点。更多内容将在2.1.1节中介绍。
- en: ❼ The REPL is now ready for your input!
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ REPL现在已准备好接受您的输入！
- en: No IPython? No problem!
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 没有IPython？没问题！
- en: I highly recommend you use IPython when using PySpark in interactive mode. IPython
    is a better frontend to the Python shell, with many useful functionalities, such
    as friendlier copy-and-paste and syntax highlighting. The installation instructions
    in appendix B include configuring PySpark to use the IPython shell.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我强烈建议你在使用PySpark的交互模式下使用IPython。IPython是Python shell的一个更好的前端，具有许多有用的功能，例如更友好的复制粘贴和语法高亮。附录B中的安装说明包括配置PySpark以使用IPython
    shell。
- en: 'If you don’t use the IPython REPL, you will see something like this:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用IPython REPL，你将看到类似以下内容：
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Appendix B also provides instructions for how to use PySpark with a Jupyter
    notebook interface if you prefer this user experience. In the cloud—for instance,
    when using Databricks—you’ll most often be provided with the option to use a notebook
    by default.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 附录B还提供了如何使用Jupyter笔记本界面来使用PySpark的说明，如果您更喜欢这种用户体验。在云中——例如，当使用Databricks时——您通常会默认提供使用笔记本的选项。
- en: 'The `pyspark` program provides quick and easy access to a Python REPL with
    PySpark preconfigured: in the last two lines of listing 2.1, we see that the variables
    `spark` and `sc` are preconfigured. When using my favorite code editor, I usually
    prefer to start with a regular python/IPython shell and add a Spark instance from
    said shell, like in appendix B. In the next section, we explore `spark` and `sc`
    as the entry points of a PySpark program by defining and instantiating them.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: '`pyspark`程序提供了快速便捷的访问Python REPL的方法，其中PySpark已预先配置：在列表2.1的最后两行中，我们看到变量`spark`和`sc`已预先配置。当使用我最喜欢的代码编辑器时，我通常更喜欢从一个常规的python/IPython
    shell开始，并从该shell添加Spark实例，就像附录B中那样。在下一节中，我们将通过定义和实例化它们来探索`spark`和`sc`作为PySpark程序的入口点。'
- en: 2.1.1 The SparkSession entry point
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.1 SparkSession入口点
- en: This section covers the `SparkSession` object and its role as the entry point
    to PySpark’s functionality within a program. Knowing how it gets created and used
    removes some of the magic of getting PySpark set up. I also explain how to connect
    PySpark within an existing REPL, simplifying integration with Python IDEs and
    tooling.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了`SparkSession`对象及其在程序中作为PySpark功能入口点的角色。了解它是如何创建和使用的，可以消除一些设置PySpark时的神秘感。我还解释了如何在现有的REPL中连接PySpark，从而简化了与Python
    IDE和工具的集成。
- en: If you have a `pyspark` shell already launched, `exit()` (or Ctrl-D) will get
    you back to your regular terminal. Launch a `python` (or better yet, an `ipython`)
    shell and input the code in listing 2.2; we create the `spark` object by hand.
    This makes it very explicit that PySpark is used as a Python library and not as
    a separate tool. It becomes easy to mix and blend Python libraries with PySpark
    when you start with a Python REPL. Chapter 8 and 9 are focused on integrating
    Python and pandas code within PySpark’s data frame.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经启动了`pyspark` shell，使用`exit()`（或Ctrl-D）可以让你回到常规终端。启动一个`python`（或者更好的是`ipython`）shell，并在其中输入列表2.2中的代码；我们手动创建`spark`对象。这非常明确地表明PySpark被用作Python库，而不是作为单独的工具。当你从Python
    REPL开始时，很容易将Python库与PySpark混合和融合。第8章和第9章专注于在PySpark的数据框中集成Python和pandas代码。
- en: PySpark uses a builder pattern through the `SparkSession.builder` object. For
    those familiar with object-oriented programming, a builder pattern provides a
    set of methods to create a highly configurable object without having multiple
    constructors. In this chapter, we will only look at the happiest case, but the
    `SparkSession` builder pattern will become increasingly useful in parts 2 and
    3 as we look into cluster configuration and adding dependencies to our jobs.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark通过`SparkSession.builder`对象使用构建器模式。对于那些熟悉面向对象编程的人来说，构建器模式提供了一系列方法来创建一个高度可配置的对象，而无需多个构造函数。在本章中，我们只关注最理想的情况，但随着我们研究集群配置和向我们的作业添加依赖项，`SparkSession`构建器模式将在第2部分和第3部分中变得越来越有用。
- en: In listing 2.2, we start the builder pattern and then chain a configuration
    parameter that defined the application name. This isn’t necessary, but when monitoring
    your jobs (see chapter 11), having a unique and well-thought-out job name will
    make it easier to know what’s what. We finish the builder pattern with the `.getOrCreate()`
    method to materialize and instantiate our SparkSession.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表2.2中，我们开始使用构建器模式，然后链式一个配置参数来定义应用程序名称。这不是必需的，但在监控你的作业时（见第11章），拥有一个独特且经过深思熟虑的作业名称将有助于了解各个作业。我们通过`.getOrCreate()`方法完成构建器模式，以具体化和实例化我们的SparkSession。
- en: Listing 2.2 Creating a `SparkSession` entry point from scratch
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.2：从头开始创建`SparkSession`入口点
- en: '[PRE2]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The SparkSession entry point is located in the pyspark.sql package, providing
    the functionality for data transformation.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ SparkSession的入口点位于pyspark.sql包中，提供了数据转换的功能。
- en: ❷ PySpark provides a builder pattern abstraction for constructing a SparkSession,
    where we chain the methods to configure the entry point.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ PySpark提供了一个构建器模式抽象，用于构建SparkSession，其中我们链式调用方法来配置入口点。
- en: ❸ Providing a relevant appName helps in identifying which programs run on your
    Spark cluster (see chapter 11).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提供一个相关的appName有助于识别在Spark集群上运行的程序（见第11章）。
- en: Note By using the `getOrCreate()` method, your program will work in both interactive
    and batch mode by avoiding the creation of a new `SparkSession` if one already
    exists. Note that if a session already exists, you won’t be able to change certain
    configuration settings (mostly related to JVM options). If you need to change
    the configuration of your `SparkSession`, kill everything and start from scratch
    to avoid any confusion.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通过使用`getOrCreate()`方法，你的程序将在交互式和批处理模式下工作，避免在已存在一个`SparkSession`的情况下创建新的`SparkSession`。注意，如果会话已存在，你将无法更改某些配置设置（大多数与JVM选项相关）。如果你需要更改`SparkSession`的配置，请杀死所有进程并从头开始，以避免任何混淆。
- en: 'In chapter 1, we spoke briefly about the Spark entry point called `SparkContext`,
    which is the liaison between your Python REPL and the Spark cluster. `SparkSession`
    is a superset of that. It wraps the `SparkContext` and provides functionality
    for interacting with the Spark SQL API, which includes the data frame structure
    we’ll use in most of our programs. Just to prove our point, see how easy it is
    to get to the `SparkContext` from our `SparkSession` object—just call the `sparkContext`
    attribute from `spark`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在第1章中，我们简要介绍了名为`SparkContext`的Spark入口点，它是你的Python REPL和Spark集群之间的联络员。`SparkSession`是它的超集。它封装了`SparkContext`，并为与Spark
    SQL API交互提供了功能，这包括我们在大多数程序中使用的DataFrame结构。为了证明我们的观点，看看从我们的`SparkSession`对象获取`SparkContext`有多容易——只需从`spark`调用`sparkContext`属性：
- en: '[PRE3]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The `SparkSession` object is a more recent addition to the PySpark API, making
    its way in version 2.0\. This is due to the API evolving in a way that makes more
    room for the faster, more versatile data frame as the main data structure over
    the lower-level RDD. Before that time, you had to use another object (called the
    `SQLContext`) to use the data frame. It’s much easier to have everything under
    a single umbrella.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession`对象是PySpark API中较新的添加，它从版本2.0开始出现。这是由于API以使DataFrame（作为主要数据结构）比低级别的RDD有更多空间的方式发展。在此之前，你必须使用另一个对象（称为`SQLContext`）来使用DataFrame。将所有内容放在一个伞下要容易得多。'
- en: This book will focus mostly on the data frame as our main data structure. I’ll
    discuss the RDD in chapter 8 when we discuss lower-level PySpark programming and
    how to embed our Python functions in our programs. In the next section, I explain
    how we can use Spark to provide more (or less!) information about its underpinning
    via the log level.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 本书将主要关注DataFrame作为我们的主要数据结构。我将在第8章讨论RDD，当时我们将讨论低级PySpark编程以及如何在程序中嵌入我们的Python函数。在下一节中，我将解释我们如何通过日志级别使用Spark来提供更多（或更少！）关于其基础的信息。
- en: Reading older PySpark code
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 阅读旧的PySpark代码
- en: 'While this book shows modern PySpark programming, we are not living in a vacuum.
    Online you might face older PySpark code that uses the former `SparkContext`/`sqlContext`
    combo. You’ll also see the `sc` variable mapped to the `SparkContext` entry point.
    With what we know about `SparkSession` and `SparkContext`, we can reason about
    old PySpark code by using the following variable assignments:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书展示了现代PySpark编程，但我们并不生活在真空中。在网上，你可能会遇到使用旧版`SparkContext`/`sqlContext`组合的旧PySpark代码。你也会看到`sc`变量映射到`SparkContext`入口点。根据我们对`SparkSession`和`SparkContext`的了解，我们可以通过以下变量赋值来推理旧PySpark代码：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You’ll see traces of `SQLContext` in the API documentation for backward compatibility.
    I recommend avoiding using this, as the new `SparkSession` approach is cleaner,
    simpler, and more future-proof.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在API文档中看到`SQLContext`的痕迹，以保持向后兼容性。我建议避免使用它，因为新的`SparkSession`方法更干净、更简单，并且更具未来性。
- en: If you are running `pyspark` from the command line, all of this is defined for
    you, as seen in listing 2.1\.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您从命令行运行`pyspark`，所有这些都会为您定义好，如列表2.1所示。
- en: '2.1.2 Configuring how chatty spark is: The log level'
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.1.2 配置Spark的“话多”程度：日志级别
- en: This section covers the log level, probably the most overlooked (and annoying)
    element of a PySpark program. Monitoring your PySpark jobs is an important part
    of developing a robust program. PySpark provides many levels of logging, from
    nothing at all to a full description of everything happening on the cluster. The
    `pyspark` shell defaults on `WARN`, which can be a little chatty when we’re learning.
    More importantly, a non-interactive PySpark program (which is how you’ll run your
    scripts for the most part) defaults to the oversharing `INFO` level. Fortunately,
    we can change the settings for your session by using the code in the next listing.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了日志级别，可能是PySpark程序中最被忽视（且令人烦恼）的元素。监控您的PySpark作业是开发健壮程序的重要部分。PySpark提供了许多日志级别，从什么也不显示到对集群上发生的所有事情的完整描述。`pyspark`
    shell默认为`WARN`，当我们学习时可能会有些话太多。更重要的是，非交互式PySpark程序（您将大部分时间用于运行脚本）默认为过度分享的`INFO`级别。幸运的是，我们可以通过使用下一列表中的代码来更改会话的设置。
- en: Listing 2.3 Deciding how chatty you want PySpark to be
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.3 决定您希望PySpark有多“话多”
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Table 2.1 lists the available keywords you can pass to `setLogLevel` (as strings).
    Each subsequent keyword contains all the previous ones, with the obvious exception
    of `OFF`, which doesn’t show anything.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 列出了您可以传递给`setLogLevel`（作为字符串）的可用关键词。每个后续关键词都包含所有前面的关键词，唯一的例外是`OFF`，它不会显示任何内容。
- en: Table 2.1 Log-level keywords
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 表2.1 日志级别关键词
- en: '| Keyword | Signification |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 关键词 | 含义 |'
- en: '| `OFF` | No logging at all (not recommended). |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| `OFF` | 完全不记录日志（不推荐）。|'
- en: '| `FATAL` | Only fatal errors. A fatal error will crash your Spark cluster.
    |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| `FATAL` | 只显示致命错误。致命错误将使您的Spark集群崩溃。|'
- en: '| `ERROR` | Will show `FATAL`, as well as other recoverable errors. |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| `ERROR` | 将显示`FATAL`以及其他可恢复的错误。|'
- en: '| `WARN` | Add warnings (and there are quite a lot of them). |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| `WARN` | 添加警告（而且有很多）。|'
- en: '| `INFO` | Will give you runtime information, such as repartitioning and data
    recovery (see chapter 1). |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| `INFO` | 将提供运行时信息，例如分区和数据恢复（见第1章）。|'
- en: '| `DEBUG` | Will provide debug information on your jobs. |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| `DEBUG` | 将提供关于您作业的调试信息。|'
- en: '| `TRACE` | Will trace your jobs (more verbose debug logs). Can be quite informative
    but very annoying. |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| `TRACE` | 将跟踪您的作业（更详细的调试日志）。可能非常有信息量，但非常令人烦恼。|'
- en: '| `ALL` | Everything that PySpark can spit, it will spit. As useful as `OFF`.
    |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| `ALL` | PySpark能输出的所有内容，它都会输出。与`OFF`一样有用。|'
- en: Note When using the `pyspark` shell, anything chattier than `WARN` might appear
    when you’re typing a command, which makes it quite hard to input commands into
    the shell. You’re welcome to play with the log levels as you please, but we won’t
    show any output unless it’s valuable for the task at hand. Setting the log level
    to `ALL` is a *very* good way to annoy oblivious coworkers if they don’t lock
    their computers. You didn’t hear this from me.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当使用`pyspark` shell时，当您输入命令时，任何比`WARN`更话多的内容都可能显示出来，这使得向shell中输入命令变得非常困难。您可以根据自己的喜好调整日志级别，但除非对当前任务有价值，否则我们不会显示任何输出。将日志级别设置为`ALL`是让未锁电脑的同事感到非常烦恼的*一个非常*好的方法。这不是我告诉您的。
- en: You now have the REPL fired up and ready for your input. This is enough housekeeping
    for now. Let’s start planning our program and get coding!
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经启动了REPL并准备好接收输入。现在就足够进行这些日常维护工作了。让我们开始规划我们的程序并开始编码！
- en: 2.2 Mapping our program
  id: totrans-75
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.2 映射我们的程序
- en: 'This section maps the blueprint of our simple program. Taking the time to design
    our data analysis beforehand pays dividends since we can construct our code knowing
    what’s coming. This will eventually speed up our coding and improve the reliability
    and modularity of our code. Think of it like reading the recipe when cooking:
    you never want to realize you’re missing a cup of flour when mixing the dough!'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 本节映射了我们简单程序的大纲。在数据分析之前花时间设计可以带来回报，因为我们可以在知道将要发生什么的情况下构建代码。这将最终加快我们的编码速度，并提高代码的可靠性和模块化。把它想象成烹饪时阅读食谱：您永远不希望在揉面团时意识到您少了一杯面粉！
- en: 'In this chapter’s introduction, we introduced our problem statement: “What
    are the most popular words used in the English language?” Before we can even hammer
    out code in the REPL, we have to start by mapping the major steps our program
    will need to perform:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的引言中，我们介绍了我们的问题陈述：“在英语中，最常用的单词是什么？”在我们甚至能在REPL中编写代码之前，我们必须首先确定程序需要执行的主要步骤：
- en: '*Read*—Read the input data (we’re assuming a plain text file).'
  id: totrans-78
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*读取*—读取输入数据（我们假设是一个纯文本文件）。'
- en: '*Token*—Tokenize each word.'
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*标记*—将每个单词标记化。'
- en: '*Clean*—Remove any punctuation and/or tokens that aren’t words. Lowercase each
    word.'
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*清理*—删除任何标点符号和/或非单词标记。将每个单词转换为小写。'
- en: '*Count*—Count the frequency of each word present in the text.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*—计算文本中每个单词的出现频率。'
- en: '*Answer*—Return the top 10 (or 20, 50, 100).'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*—返回前10（或20、50、100）个。'
- en: Visually, a simplified flow of our program would look like figure 2.1.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 从视觉上看，我们程序的简化流程将类似于图2.1。
- en: '![](../Images/02-01.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-01.png)'
- en: Figure 2.1 A simplified flow of our program, illustrating the five steps
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.1 我们程序的简化流程，展示了五个步骤
- en: 'Our goal is quite lofty: the English language has produced, throughout history,
    an unfathomable amount of written material. Since we are learning, we’ll start
    with a relatively small source, get our program working, and then scale it to
    accommodate a larger body of text. For this, I chose to use Jane Austen’s *Pride
    and Prejudice*, since it’s already in plain text and freely available. In the
    next section, we ingest and explore our data to start building our program.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标相当宏伟：英语语言在历史上产生了难以估量的书面材料。由于我们正在学习，我们将从一个相对较小的来源开始，让程序运行起来，然后扩展它以适应更大的文本库。为此，我选择了简·奥斯汀的《傲慢与偏见》，因为它已经是纯文本格式并且可以免费获取。在下一节中，我们将摄取和探索我们的数据，以开始构建我们的程序。
- en: Data analysis and Pareto’s principle
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 数据分析和帕累托原理
- en: Pareto’s principle, also known as the 80/20 rule, is often summarized as “20%
    of the efforts will yield 80% of the results.” In data analysis, we can consider
    that 20% to be analysis, visualization, or machine learning models, anything that
    provides tangible value to the recipient.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 帕累托原理，也称为80/20法则，通常总结为“20%的努力将产生80%的结果。”在数据分析中，我们可以将这20%视为分析、可视化或机器学习模型，任何为接收者提供实质性价值的东西。
- en: 'The remainder is what I call *invisible work*: ingesting the data, cleaning
    it, figuring out its meaning, and shaping it into a usable form. If you look at
    your simple steps, steps 1 to 3 can be considered invisible work: we’re ingesting
    data and getting it ready for the counting process. Steps 4 and 5 are the visible
    ones that are answering our question (one could argue that only step 5 is performing
    visible work, but let’s not split hairs here). Steps 1 to 3 are there because
    the data requires processing for it to be usable for our problem. The steps aren’t
    core to our problem, but we can’t do without them.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 剩余的部分是我所说的*无形工作*：摄取数据、清理数据、理解其含义并将其塑造成可用的形式。如果你看看你的简单步骤，步骤1到3可以被认为是无形工作：我们正在摄取数据并使其为计数过程做好准备。步骤4和5是可见的，它们在回答我们的问题（有人可能会争论只有步骤5在进行可见工作，但让我们不要过于纠结于此）。步骤1到3之所以存在，是因为数据需要处理才能为我们的问题所用。这些步骤不是问题的核心，但我们不能没有它们。
- en: When building your project, this will be the part that will be the most time-consuming,
    and you might be tempted (or pressured!) to skimp on it. Always keep in mind that
    the data you ingest and process is the raw material of your programs, and that
    feeding it garbage will yield, well, garbage.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在构建项目时，这部分将是耗时最长的部分，你可能会倾向于（或者被迫！）在这方面节省时间。始终记住，你摄取和处理的这些数据是程序的原始材料，而给它垃圾将产生，嗯，垃圾。
- en: '2.3 Ingest and explore: Setting the stage for data transformation'
  id: totrans-91
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.3 吞吐和探索：为数据转换做准备
- en: 'This section covers the three operations every PySpark program will encounter,
    regardless of the nature of your program: ingesting data into a structure, printing
    the structure (or *schema*) to see how the data is organized, and finally showing
    a sample of the data for review. Those operations are fundamental to any data
    analysis, whether it is text (this chapter and chapter 3), tabular (most chapters,
    but especially chapter 4 and 5), or even binary or hierarchical data (chapter
    6); the general blueprint and methods will apply everywhere in your PySpark journey.'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了每个PySpark程序都会遇到的三种操作，无论您的程序性质如何：将数据摄取到结构中，打印结构（或*模式*）以查看数据是如何组织的，最后展示数据样本以供审查。这些操作对于任何数据分析都是基本的，无论是文本（本章和第3章），表格（大多数章节，尤其是第4章和第5章），甚至是二进制或分层数据（第6章）；通用的蓝图和方法将在您的PySpark之旅的每个地方都适用。
- en: 2.3.1 Reading data into a data frame with spark.read
  id: totrans-93
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.1 使用spark.read将数据读取到数据框中
- en: The first step of our program is to ingest the data in a structure we can perform
    work in. This section introduces the basic functionality PySpark provides for
    reading data and how it is specialized for plain text.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我们程序的第一步是将数据以我们可以进行工作的结构中摄取。本节介绍了PySpark提供的基本数据读取功能以及它是如何针对纯文本进行优化的。
- en: 'Before ingesting any data, we need to choose where it’s going to go. PySpark
    provides two main structures for storing data when performing manipulations:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在摄取任何数据之前，我们需要选择它将去往何处。PySpark提供了两种主要结构来存储在执行操作时的数据：
- en: The RDD
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD
- en: The data frame
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框
- en: The RDD was the only structure for a long time. It looks like a distributed
    collection of objects (or rows). I visualize this as a bag that you give orders
    to. You pass orders to the RDD through regular Python functions over the items
    in the bag.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: RDD在很长一段时间内是唯一的结构。它看起来像是一个分布式的对象（或行）集合。我将它可视化为一个你可以下命令的袋子。你通过在袋子中的项目上使用常规Python函数将命令传递给RDD。
- en: The data frame is a stricter version of the RDD. Conceptually, you can think
    of it like a table, where each cell can contain one value. The data frame makes
    heavy usage of the concept of columns, where you operate on columns instead of
    on records, like in the RDD. Figure 2.2 provides a visual summary of the two structures.
    The data frame is now the dominant data structure, and we will almost exclusively
    use it in this book; chapter 8 covers the RDD (a more general and flexible structure,
    from which the data frame inherits) for cases that need record-by-record flexibility.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 数据框是RDD的更严格版本。从概念上讲，您可以将其视为一个表格，其中每个单元格可以包含一个值。数据框大量使用列的概念，您在列上操作，而不是在RDD中那样在记录上操作。图2.2提供了这两个结构的视觉总结。数据框现在是主导的数据结构，我们将几乎在本书中独家使用它；第8章涵盖了RDD（一个更通用和灵活的结构，数据框从中继承），用于需要逐记录灵活性的情况。
- en: '![](../Images/02-02.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02-02.png)'
- en: Figure 2.2 An RDD versus a data frame. In the RDD, we think of each record as
    an independent entity. With the data frame, we mostly interact with columns, performing
    functions on them. We still can access the rows of a data frame via RDD if necessary.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.2 RDD与数据框的比较。在RDD中，我们将每条记录视为一个独立的实体。使用数据框时，我们主要与列进行交互，对它们执行函数。如果需要，我们仍然可以通过RDD访问数据框的行。
- en: If you’ve used SQL in the past, you’ll find that the data frame implementation
    takes a lot of inspiration from SQL. The module name for data organization and
    manipulation is even named `pyspark.sql`! Furthermore, chapter 7 teaches how to
    mix PySpark and SQL code within the same program.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您以前使用过SQL，您会发现数据框实现借鉴了SQL的很多灵感。数据组织和操作模块的名称甚至被命名为`pyspark.sql`！此外，第7章介绍了如何在同一程序中混合PySpark和SQL代码。
- en: 'Reading data into a data frame is done through the `DataFrameReader` object,
    which we can access through `spark.read`. The code in listing 2.4 displays the
    object, as well as the methods it exposes. We recognize a few file formats: CSV
    stands for comma-separated values (which we’ll use as early as chapter 4), JSON
    for JavaScript Object Notation (a popular data exchange format), and text is,
    well, just plain text.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`DataFrameReader`对象将数据读取到数据框中，我们可以通过`spark.read`访问该对象。列表2.4中的代码显示了该对象及其公开的方法。我们识别了几种文件格式：CSV代表逗号分隔值（我们将在第4章早期使用它），JSON代表JavaScript对象表示法（一种流行的数据交换格式），而文本则是，嗯，就是纯文本。
- en: Listing 2.4 The `DataFrameReader` object
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.4 `DataFrameReader`对象
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: PySpark reads your data
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark读取您的数据
- en: PySpark can accommodate the different ways you can process data. Under the hood,
    `spark.read.csv()` will map to `spark.read.format('csv').load()`, and you may
    encounter this form in the wild. I usually prefer using the direct `csv` method
    as it provides a handy reminder of the different parameters the reader can take.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark可以适应您处理数据的不同方式。在底层，`spark.read.csv()`将映射到`spark.read.format('csv').load()`，您可能会在野外遇到这种形式。我通常更喜欢使用直接的`csv`方法，因为它提供了一个方便的提醒，说明了读者可以接受的不同参数。
- en: '`orc` and `parquet` are also data formats that are especially well suited for
    big data processing. ORC (which stands for “optimized row columnar”) and Parquet
    are competing data formats that pretty much serve the same purpose. Both are open
    sourced and now part of the Apache project, just like Spark.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '`orc`和`parquet`也是特别适合大数据处理的数据格式。ORC（代表“优化行列”）和Parquet是竞争性的数据格式，基本上服务于相同的目的。两者都是开源的，现在都是Apache项目的一部分，就像Spark一样。'
- en: PySpark defaults to using Parquet when reading and writing files, and we’ll
    use this format to store our results throughout the book. I’ll provide a longer
    discussion about the usage, advantages, and trade-offs of using Parquet or ORC
    as a data format in chapter 6.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark在读取和写入文件时默认使用Parquet格式，我们将在整本书中使用这种格式来存储我们的结果。我将在第6章提供关于使用Parquet或ORC作为数据格式的使用、优势和权衡的更详细讨论。
- en: Let’s read our data file in listing 2.5\. I am assuming you launched PySpark
    at the root of this book’s repository. Depending on your case, you might need
    to change the path where the file is located. The code is all available on the
    book’s companion repository on GitHub ([http://mng.bz/6ZOR](http://mng.bz/6ZOR)).
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们读取列表2.5中的数据文件。我假设您在这个书的仓库根目录启动了PySpark。根据您的具体情况，您可能需要更改文件所在的位置。代码全部可在GitHub上找到，该书的配套仓库（[http://mng.bz/6ZOR](http://mng.bz/6ZOR)）。
- en: Listing 2.5 “Reading” our Jane Austen novel in record time
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.5 “以记录时间读取”我们的简·奥斯汀小说
- en: '[PRE7]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We get a data frame, as expected! If you input your data frame, conveniently
    named `book`, into the shell, you see that PySpark doesn’t output any data to
    the screen. Instead, it prints the schema, which is the name of the columns and
    their type. In PySpark’s world, each column has a type: it represents how the
    value is represented by Spark’s engine. By having the type attached to each column,
    you can instantly know what operations you can do on the data. With this information,
    you won’t inadvertently try to add an integer to a string: PySpark won’t let you
    add 1 to “blue.” Here, we have one column, named `value`, composed of a `string`.
    A quick graphical representation of our data frame would look like figure 2.3:
    each line of text (separated by a newline character) is a record. Besides being
    a helpful reminder of the content of the data frame, types are integral to how
    Spark processes data quickly and accurately. We will explore the subject extensively
    in chapter 6.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了一个数据框，正如预期的那样！如果您将名为`book`的数据框输入到shell中，您会发现PySpark不会将任何数据输出到屏幕上。相反，它打印出模式，即列名及其类型。在PySpark的世界里，每一列都有一个类型：它代表了Spark的引擎如何表示值。通过将类型附加到每一列，您可以立即知道可以对数据进行哪些操作。有了这些信息，您就不会无意中尝试将整数添加到字符串中：PySpark不会让您将1加到“blue”上。在这里，我们有一个名为`value`的列，由一个`string`组成。我们数据框的快速图形表示将类似于图2.3：每一行文本（由换行符分隔）是一个记录。除了作为数据框内容的提醒外，类型对于Spark快速且准确地处理数据至关重要。我们将在第6章对此进行深入探讨。
- en: '![](../Images/02-03.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图2.3](../Images/02-03.png)'
- en: Figure 2.3 A high-level logical schema of our `book` data frame, containing
    a `value` string column. We can see the name of the column, its type, and a small
    snippet of the data.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图2.3展示了我们`book`数据框的高级逻辑模式，其中包含一个`value`字符串列。我们可以看到列名、其类型以及数据的一个小片段。
- en: 'When working with data frames, we will most often worry about the logical schema,
    which is the organization of the data as if the data were on a single node. We
    use schemas to understand the data and its type (integer, string, date, etc.)
    for a given data frame. Spark displays the logical schema when we input the variable
    in the REPL: columns and types. In practice, your data frame will be distributed
    across multiple nodes, each one having a segment of the records. When performing
    data transformation and analysis, it is more convenient to work with the logical
    schema. Chapter 11 provides a deeper look into the logical versus physical world
    through *query planning*, which gives us insight into how Spark moves from high-level
    instruction to optimized machine instructions.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据框时，我们最常担心的是逻辑模式，即数据作为单个节点上的组织形式。我们使用模式来理解数据及其类型（整数、字符串、日期等），对于给定的数据框。当我们在REPL中输入变量时，Spark会显示逻辑模式：列和类型。在实践中，你的数据框将分布在多个节点上，每个节点都有记录的一部分。在执行数据转换和分析时，使用逻辑模式更方便。第11章通过*查询计划*更深入地探讨了逻辑与物理世界，这让我们了解到Spark是如何从高级指令到优化机器指令的。
- en: When working with a larger data frame (think hundreds or even thousands of columns),
    you may want to see the schema displayed more clearly. PySpark provides `printSchema()`
    to display the schema in a tree form. I use this method probably more than any
    other one as it gives you direct information on the structure of the data frame.
    Since `printSchema()` directly prints to the REPL with no other option, should
    you want to filter the schema, you can use the `dtypes` attributes of the data
    frame, which gives you a list of tuples `(column_name,` `column_type)`. You can
    also access the schema programmatically (as a data structure) using the `schema`
    attribute (see chapter 6 for more information).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理较大的数据框（想想有成百上千列）时，你可能希望更清晰地显示模式。PySpark提供了`printSchema()`来以树形显示模式。我可能比其他任何方法都更常用这个方法，因为它直接提供了关于数据框结构的信息。由于`printSchema()`直接打印到REPL而没有其他选项，如果你想过滤模式，可以使用数据框的`dtypes`属性，它给你一个`(column_name,
    column_type)`元组的列表。你还可以使用`schema`属性以编程方式（作为一个数据结构）访问模式（更多信息请参见第6章）。
- en: Listing 2.6 Printing the schema of our data frame
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.6 打印我们的数据框的模式
- en: '[PRE8]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Each data frame tree starts with a root, which the columns are attached to.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个数据框树都以一个根节点开始，列都附加到这个根节点上。
- en: ❷ We have one column value, containing strings that can be null (or None in
    Python terms).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们有一个包含字符串的列值，这些字符串可以是null（或Python中的None）。
- en: ❸ The same information is stored as a list of tuples under the data frame’s
    dtypes attribute.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 同样的信息存储在数据框的dtypes属性下的元组列表中。
- en: In this section, we ingested our textual data into a data frame. This data frame
    inferred a simple columnar structure that we can explore through the variable
    name in the REPL, the `printSchema()` method, or the `dtypes` attribute. In the
    next section, we go beyond the structure to peek at the data inside.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将我们的文本数据导入到数据框中。这个数据框推断出了一种简单的列式结构，我们可以通过REPL中的变量名、`printSchema()`方法或`dtypes`属性来探索它。在下一节中，我们将超越结构，窥视数据内部。
- en: Speeding up your learning by using the shell
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用shell加速学习
- en: Using the shell doesn’t just apply to PySpark, but using its functionality can
    often save a lot of searching in the documentation. I am a big fan of using `dir()`
    on an object when I don’t remember the exact method I want to apply, as I did
    in listing 2.4.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 使用shell不仅适用于PySpark，而且利用其功能通常可以节省在文档中大量搜索的时间。当我记不起要应用的确切方法时，我非常喜欢在对象上使用`dir()`，就像我在2.4列表中做的那样。
- en: PySpark’s source code is very well documented. If you’re unsure about the proper
    usage of a function, class, or method, you can print the `__doc__` attribute or,
    for those using IPython, use a trailing question mark (or two, if you want more
    details).
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的源代码非常详细地进行了文档说明。如果你不确定函数、类或方法的正确用法，可以打印`__doc__`属性，或者对于使用IPython的用户，可以使用尾随问号（如果你想获取更多详细信息，可以加两个问号）。
- en: Listing 2.7 Using PySpark’s documentation directly in the REPL
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.7 在REPL中直接使用PySpark的文档
- en: '[PRE9]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '2.3.2 From structure to content: Exploring our data frame with show()'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.3.2 从结构到内容：使用show()探索我们的数据框
- en: One of the key advantages of using the REPL for interactive development is that
    you can peek at your work as you’re performing it. Now that our data is loaded
    into a data frame, we can start looking at how PySpark structured our text. This
    section covers the most important method for looking at the data contained in
    a data frame, `show()`.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 REPL 进行交互式开发的一个关键优势是您可以在执行过程中查看您的作品。现在我们的数据已加载到数据框中，我们可以开始查看 PySpark 如何结构化我们的文本。本节涵盖了查看数据框中数据的最重要的方法，即
    `show()`。
- en: In section 2.3.1, we saw that the default behavior of inputting a data frame
    in the shell is to provide the schema or column information of the object. While
    very useful, sometimes we want to take a peek at the data.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 在 2.3.1 节中，我们看到了在 shell 中输入数据框的默认行为是提供对象的模式或列信息。虽然非常有用，但有时我们想窥视一下数据。
- en: Enter the `show()` method, which displays a few rows of the data back to you—nothing
    more, nothing less. With `printSchema()`, this method will become one of your
    best friends when performing data exploration and validation. By default, it will
    show 20 rows and truncate long values. The code in listing 2.8 shows the default
    behavior of the method applied to our `book` data frame. For text data, the length
    limitation is limiting (pun intended). Fortunately, `show()` provides some options
    to display just what you need.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 `show()` 方法，它将显示一些数据行——不多也不少。与 `printSchema()` 一起，此方法将成为您在执行数据探索和验证时的最佳朋友。默认情况下，它将显示
    20 行并截断长值。列表 2.8 中的代码显示了将此方法的默认行为应用于我们的 `book` 数据框。对于文本数据，长度限制是有限的（有意为之）。幸运的是，`show()`
    提供了一些选项来显示您所需的内容。
- en: Listing 2.8 Showing a little data using the `.show()` method
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.8 使用 `.show()` 方法显示少量数据
- en: '[PRE10]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Spark displays the data from the data frame in an ASCII art-like table, limiting
    the length of each cell to 20 characters. If the contents spill over the limit,
    an ellipsis is added at the end.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Spark 以类似 ASCII 艺术的表格形式显示数据框中的数据，限制每个单元格的长度为 20 个字符。如果内容超出限制，将在末尾添加省略号。
- en: 'The `show()` method takes three optional parameters:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '`show()` 方法有三个可选参数：'
- en: '`n` can be set to any positive integer and will display that number of rows.'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`n` 可以设置为任何正整数，并将显示该数量的行。'
- en: '`truncate`, if set to true, will truncate the columns to display only 20 characters.
    Set to `False`, it will display the whole length, or any positive integer to truncate
    to a specific number of characters.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`truncate`，如果设置为 true，将截断列以仅显示 20 个字符。设置为 `False`，将显示整个长度，或任何正整数以截断到特定数量的字符。'
- en: '`vertical` takes a Boolean value and, when set to `True`, will display each
    record as a small table. If you need to check records in detail, this is a very
    useful option.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical` 接受布尔值，当设置为 `True` 时，将每个记录显示为一个小表格。如果您需要详细检查记录，这是一个非常有用的选项。'
- en: The code in the next listing shows a more useful view of the `book` data frame,
    showing only 10 records but truncating them at 50 characters. We can see more
    of the text now!
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表中的代码展示了 `book` 数据框的一个更有用的视图，仅显示 10 条记录，但将它们截断到 50 个字符。现在我们可以看到更多的文本了！
- en: Listing 2.9 Showing less length, more width with the `show()` method
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.9 使用 `show()` 方法显示更短的长度，更宽的宽度
- en: '[PRE11]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Together, `show()` and `printSchema()` give you a complete overview of the structure
    and the content of the data frame. It’s no surprise that those will be the methods
    you will reach for most often when building a data analysis at the REPL.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '`show()` 和 `printSchema()` 一起为您提供了数据框结构和内容的完整概述。毫不奇怪，当在 REPL 中构建数据分析时，您将最频繁地使用这些方法。'
- en: 'We can now start the real work: performing transformations on the data frame
    to accomplish our goal. Let’s take some time to review the five steps we outlined
    at the beginning of the chapter:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以开始真正的工作了：对数据框进行转换以实现我们的目标。让我们花些时间回顾一下本章开头概述的五步：
- en: '**[DONE]***Read*—Read the input data (we’re assuming a plain text file).'
  id: totrans-145
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取*—读取输入数据（我们假设是纯文本文件）。'
- en: '*Token*—Tokenize each word.'
  id: totrans-146
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Token*—对每个单词进行标记。'
- en: '*Clean*—Remove any punctuation and/or tokens that aren’t words. Lowercase each
    word.'
  id: totrans-147
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*Clean*—移除任何标点符号和/或非单词标记。将每个单词转换为小写。'
- en: '*Count*—Count the frequency of each word present in the text.'
  id: totrans-148
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*—计算文本中每个单词的频率。'
- en: '*Answer*—Return the top 10 (or 20, 50, 100).'
  id: totrans-149
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*回答*—返回前 10（或 20、50、100）条。'
- en: In the next section, we start performing some simple column transformations
    to tokenize and clean the data. Our data frame will start changing right before
    our eyes!
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们开始执行一些简单的列转换，以标记和清理数据。我们的数据框将直接在我们的眼前发生变化！
- en: 'Optional topic: Nonlazy Spark?'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 可选主题：非惰性 Spark？
- en: If you are coming from another data frame implementation, such as pandas or
    R `data.frame`, you might find it odd to see the structure of the data frame instead
    of a summary of the data when calling the variable. The `show()` method might
    be a nuisance to you.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你来自其他数据框实现，例如 pandas 或 R `data.frame`，你可能会发现当调用变量时看到数据框的结构而不是数据摘要很奇怪。`show()`
    方法可能对你来说是个麻烦。
- en: If we take a step back and think about PySpark’s use cases, it makes a lot of
    sense. `show()` is an action, since it performs the visible work of printing data
    on the screen. As savvy PySpark programmers, we want to avoid accidentally triggering
    the chain of computations, so the Spark developers made `show()` explicit. When
    building a complicated chain of transformations, triggering its execution is a
    lot more annoying and time-consuming than having to type the `show()` method when
    you’re ready. This transformation-versus-action distinction also leads to more
    opportunities for the Spark optimizer to generate a more efficient program (see
    chapter 11).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们退一步思考 PySpark 的用例，这很有道理。`show()` 是一个动作，因为它执行了在屏幕上打印数据的可见工作。作为熟练的 PySpark
    程序员，我们希望避免意外触发计算链，因此 Spark 开发者将 `show()` 明确化。在构建复杂的转换链时，触发其执行比在准备好时输入 `show()`
    方法要麻烦得多。这种转换与动作的区别也使得 Spark 优化器有更多机会生成更高效的程序（参见第 11 章）。
- en: 'That being said, there are some moments, especially when learning, when you
    want your data frames to be evaluated after each transformation (which we call
    *eager evaluation*). Since Spark 2.4.0, you can configure the `SparkSession` object
    to support printing to screen. We will cover how to create a `SparkSession` object
    in greater detail in chapter 3, but if you want to use eager evaluation in the
    shell, you can paste the following code in your shell:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，在学习过程中，有时你希望你的数据框在每次转换后都能被评估（我们称之为 *急切评估*）。从 Spark 2.4.0 版本开始，你可以配置 `SparkSession`
    对象以支持屏幕打印。我们将在第 3 章中更详细地介绍如何创建 `SparkSession` 对象，但如果你想在 shell 中使用急切评估，你可以在你的 shell
    中粘贴以下代码：
- en: '[PRE12]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: All the examples in the book assume that the data frames are evaluated lazily,
    but this option can be useful if you’re demonstrating Spark. Use it as you see
    fit, but remember that Spark owes a lot of its performance to its lazy evaluation.
    You’ll be leaving some extra horsepower on the table!
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 书中的所有示例都假设数据框是惰性评估的，但如果你在演示 Spark，这个选项可能很有用。根据需要使用它，但请记住，Spark 的许多性能都归功于其惰性评估。你将留下一些额外的马力！
- en: '2.4 Simple column transformations: Moving from a sentence to a list of words'
  id: totrans-157
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.4 简单列转换：从句子到单词列表的转换
- en: When ingesting our selected text into a data frame, PySpark created one record
    for each line of text and provided a `value` column of type `String`. To tokenize
    each word, we need to split each string into a list of distinct words. This section
    covers simple transformations using `select()`. We will split our lines of text
    into words so we can count them.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将选定的文本导入数据框时，PySpark 为每一行文本创建了一个记录，并提供了类型为 `String` 的 `value` 列。为了对每个单词进行分词，我们需要将每个字符串拆分成一个不同的单词列表。本节将介绍使用
    `select()` 的简单转换。我们将把我们的文本行拆分成单词，以便我们能够计数。
- en: Because PySpark’s code can be pretty self-explanatory, I start by providing
    the code in one fell swoop, and then we’ll break down each step one at a time.
    You can see it in all its glory in the next listing.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 因为 PySpark 的代码可以相当直观，所以我一开始就一次性提供代码，然后我们会一步一步地分解每个步骤。你可以在下一部分中看到它的全部风采。
- en: Listing 2.10 Splitting our lines of text into arrays or words
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.10 将我们的文本行拆分成数组或单词
- en: '[PRE13]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In a single line of code (I don’t count the import or the `show()`, which is
    only being used to display the result), we’ve done quite a lot. The remainder
    of this section will introduce basic column operations and explain how we can
    build our tokenization step as a one-liner. More specifically, we learn about
    the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在一行代码中（我不计算导入或 `show()`，因为 `show()` 只用于显示结果），我们已经做了很多事情。本节的剩余部分将介绍基本的列操作，并解释我们如何将分词步骤作为一个单行代码来实现。具体来说，我们将学习以下内容：
- en: The `select()` method and its canonical usage, which is selecting data
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`select()` 方法及其标准用法，即选择数据'
- en: The `alias()` method to rename transformed columns
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`alias()` 方法用于重命名转换后的列'
- en: Importing column functions from `pyspark.sql.functions` and using them
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 `pyspark.sql.functions` 导入列函数并使用它们
- en: 'Although our example looks very specific (moving from a string to a list of
    words), the blueprint for using PySpark’s transformation functions is very consistent:
    you’ll see and use this pattern very frequently when transforming data frames.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们的示例看起来非常具体（从字符串转换为单词列表），但使用 PySpark 转换函数的蓝图非常一致：当转换数据框时，您会非常频繁地看到并使用这种模式。
- en: 2.4.1 Selecting specific columns using select()
  id: totrans-167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.1 使用 select() 选择特定列
- en: This section will introduce the most basic functionality of `select()`, which
    is to select one or more columns from your data frame. It’s a conceptually very
    simple method but provides the foundation for many additional operations on your
    data.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍 `select()` 函数最基本的功能，即从您的数据框中选择一个或多个列。这是一个概念上非常简单的方法，但为在您的数据上执行许多附加操作提供了基础。
- en: 'In PySpark’s world, a data frame is made out of `Column` objects, and you perform
    transformations on them. The most basic transformation is the identity, where
    you return exactly what was provided to you. If you’ve used SQL in the past, you
    might think that this sounds like a `SELECT` statement, and you’d be right! You
    also get a free pass: the method name is also conveniently named `select()`.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 的世界中，数据框由 `Column` 对象组成，您对它们执行转换。最基本的转换是恒等转换，即您返回所提供的内容。如果您以前使用过 SQL，您可能会认为这听起来像是一个
    `SELECT` 语句，您是对的！您还获得了一个免费通行证：方法名也方便地命名为 `select()`。
- en: 'We’ll go over a quick example: selecting the only column of our `book` data
    frame. Since we already know the expected output, we can focus on the gymnastics
    for the `select()` method. The next listing provides the code for that very useful
    task.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将快速通过一个示例：选择 `book` 数据框的唯一列。由于我们已经知道了预期的输出，我们可以专注于 `select()` 方法的技巧。下面的列表提供了执行这个非常有用的任务的代码。
- en: Listing 2.11 The simplest select statement ever
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.11：最简单的选择语句
- en: '[PRE14]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'PySpark provides for each column in its data frame a dot notation that refers
    to the column. This is the simplest way to select a column, as long as the name
    doesn’t contain any funny characters: PySpark will accept `$!@#` as a column name,
    but you won’t be able to use the dot notation for this column.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 为其数据框中的每一列提供了一个点表示法，它指向该列。只要列名不包含任何特殊字符，这就是选择列的最简单方法：PySpark 会接受 `$!@#`
    作为列名，但您将无法使用点表示法来表示此列。
- en: PySpark provides more than one way to select columns. I display the four most
    common in the next listing.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 提供了多种选择列的方法。我在下面的列表中展示了四种最常见的方法。
- en: Listing 2.12 Selecting the `value` column from the `book` data frame
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.12：从 `book` 数据框中选择 `value` 列
- en: '[PRE15]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The first way to select a column is the trusty dot notation we got acquainted
    with a few paragraphs ago. The second one uses brackets instead of the dot to
    name the column. It addresses the `$!@#` problem since you pass the name of the
    column as a string.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 选择列的第一种方法是我们在几段之前熟悉的信任的点表示法。第二种方法使用方括号而不是点来命名列。它解决了 `$!@#` 问题，因为您将列名作为字符串传递。
- en: The third one uses the `col` function from the `pyspark.sql.functions` module.
    The main difference here is that you don’t specify that the column comes from
    the `book` data frame. This will become very useful when working with more complex
    data pipelines in part 2 of the book. I’ll use the `col` object as much as I can
    since I consider its usage more idiomatic and it’ll prepare us for more complex
    use cases, such as performing column transformation (see chapter 4 and 5).
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种方法使用来自 `pyspark.sql.functions` 模块的 `col` 函数。这里的主要区别是您没有指定该列来自 `book` 数据框。这在本书的第二部分处理更复杂的数据管道时将非常有用。我会尽可能多地使用
    `col` 对象，因为我认为它的使用更符合习惯，这将为我们准备更复杂的使用案例，例如执行列转换（参见第 4 章和第 5 章）。
- en: Finally, the fourth one only uses the name of the column as a string. PySpark
    is smart enough to infer that we mean a column here. For simple select statements
    (and other methods that I’ll cover later), using the name of the column directly
    can be a viable option. That being said, it’s not as flexible as the other options,
    and the moment your code requires column transformations, like in section 2.4.2,
    you’ll have to use another option.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，第四种方法仅使用列名作为字符串。PySpark 足够智能，可以推断出我们在这里指的是列。对于简单的选择语句（以及我稍后将要介绍的其他方法），直接使用列名可能是一个可行的选项。但话虽如此，它不如其他选项灵活，一旦您的代码需要列转换，例如在
    2.4.2 节中，您就必须使用另一种选项。
- en: Now that we’ve selected our column, let’s start working PySpark out. Up next
    is splitting the lines of text.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经选择了列，让我们开始使用 PySpark。接下来是拆分文本行。
- en: '2.4.2 Transforming columns: Splitting a string into a list of words'
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.2 转换列：将字符串拆分为单词列表
- en: We just saw a very simple way to select a column in PySpark. In this section,
    we build on this foundation by selecting a transformation of a column instead.
    This provides a powerful and flexible way to express our transformations, and
    as you’ll see, this pattern will be frequently used when manipulating data.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了在 PySpark 中选择列的一个非常简单的方法。在本节中，我们在此基础上构建，通过选择列的转换来选择。这提供了一种强大且灵活的方式来表达我们的转换，正如你将看到的，这种模式在处理数据时将被频繁使用。
- en: 'PySpark provides a `split()` function in the `pyspark.sql.functions` module
    for splitting a longer string into a list of shorter strings. The most popular
    use case for this function is to split a sentence into words. The `split()` function
    takes two or three parameters:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 在 `pyspark.sql.functions` 模块中提供了一个 `split()` 函数，用于将较长的字符串拆分为较短的字符串列表。此函数最常用的用例是将句子拆分为单词。`split()`
    函数接受两个或三个参数：
- en: A column object containing `strings`
  id: totrans-184
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含 `字符串` 的列对象
- en: A Java regular expression delimiter to split the strings against
  id: totrans-185
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 Java 正则表达式分隔符，用于拆分字符串
- en: An optional integer about how many times we apply the delimiter (not used here)
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个可选的整数，表示我们应用分隔符的次数（此处未使用）
- en: Since we want to split words, we won’t overcomplicate our regular expression
    and will use the space character to split. The next listing shows the results
    of our code.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们想要拆分单词，我们不会使我们的正则表达式过于复杂，并使用空格字符进行拆分。下一个列表显示了我们的代码结果。
- en: Listing 2.13 Splitting our lines of text into lists of words
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.13 将我们的文本行拆分为单词列表
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'The `split` functions transformed our `string` column into an `array` column,
    containing one or more `string` elements. This is what we were expecting: even
    before looking at the data, seeing that the structure behaves according to plan
    is a good way to sanity-check our code.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '`split` 函数将我们的 `字符串` 列转换成了包含一个或多个 `字符串` 元素的 `数组` 列。这正是我们所期待的：甚至在查看数据之前，看到结构按计划运行是一种很好的方法来验证我们的代码。'
- en: Looking at the five rows we’ve printed, we can see that our values are now separated
    by a comma and wrapped in square brackets, which is how PySpark visually represents
    an array. The second record is empty, so we just see `[]`, an empty array.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们打印的五行，我们可以看到我们的值现在由逗号分隔，并用方括号括起来，这是 PySpark 视觉表示数组的方式。第二条记录为空，所以我们只看到 `[]`，一个空数组。
- en: PySpark’s built-in functions for data manipulations are extremely useful, and
    you should spend a little bit of time going over the API documentation ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/))
    to see what’s available at core functionality. If you don’t find exactly what
    you’re after, chapter 6 covers how you can create your function over `Column`
    objects, and gives a deeper look into PySpark’s complex data types like the array.
    Built-in PySpark functions are as performant as plain Spark (in Java and Scala),
    as they map directly to a JVM function. (See the following sidebar for more information.)
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 的内置数据操作函数非常实用，你应该花点时间浏览 API 文档 ([http://spark.apache.org/docs/latest/api/python/](http://spark.apache.org/docs/latest/api/python/))，看看核心功能提供了什么。如果你找不到你想要的，第
    6 章介绍了如何创建在 `Column` 对象上运行的函数，并深入探讨了 PySpark 的复杂数据类型，如数组。内置的 PySpark 函数与纯 Spark（在
    Java 和 Scala 中）一样高效，因为它们直接映射到 JVM 函数。（有关更多信息，请参阅以下侧边栏。）
- en: 'Advanced topic: PySpark’s architecture and the JVM heritage'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 高级主题：PySpark 的架构和 JVM 遗产
- en: 'If you’re like me, you might be interested to see how PySpark builds its core
    `pyspark` `.sql.functions` functions. If you look at the source code for `split()`
    (from the API documentation; see [http://mng.bz/oa4D](http://mng.bz/oa4D)), you
    might be in for a disappointment:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样，你可能对查看 PySpark 如何构建其核心 `pyspark` `.sql.functions` 函数感兴趣。如果你查看 `split()`
    函数的源代码（来自 API 文档；见 [http://mng.bz/oa4D](http://mng.bz/oa4D)），你可能会感到失望：
- en: '[PRE17]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'It effectively refers to the `split` function of the `sc._jvm.functions` object.
    This has to do with how the data frame was built. PySpark uses a translation layer
    to call JVM functions for its core functions. This makes PySpark faster since
    you’re not transforming your Python code into a JVM one all the time; it’s already
    done for you. It also makes porting PySpark to another platform a little easier:
    if you can call the JVM functions directly, you don’t have to re-implement everything.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 它实际上指的是 `sc._jvm.functions` 对象的 `split` 函数。这与数据框的构建方式有关。PySpark 使用一个转换层来调用 JVM
    函数来实现其核心功能。这使得 PySpark 更快，因为你不必总是将你的 Python 代码转换成 JVM 代码；这已经为你完成了。它还使得将 PySpark
    移植到另一个平台变得更容易：如果你可以直接调用 JVM 函数，你就不必重新实现一切。
- en: This is one of the trade-offs of standing on the shoulders of the Spark giant.
    This also explains why PySpark uses JVM-base regular expressions instead of the
    Python ones in its built-in functions. Part 3 will expand on this subject greatly,
    but in the meantime, explore PySpark’s source code!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在巨人的肩膀上站立的一个权衡。这也解释了为什么 PySpark 在其内置函数中使用 JVM 基础的正则表达式而不是 Python 的正则表达式。第三部分将大大扩展这个主题，但在此期间，探索
    PySpark 的源代码！
- en: 'With our lines of text now tokenized into words, there is a little annoyance
    present: Spark gave a very unintuitive name (`split(value,` `,` `-1)`) to our
    column. The next section addresses how we can rename transformed columns to our
    liking so we can explicitly control our columns’ naming schema.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将文本行分解成单词后，出现了一点小麻烦：Spark 给我们的列起了一个非常不直观的名字（`split(value,`, `,`, `-1)`）。下一节将介绍我们如何将转换后的列重命名为我们喜欢的名字，以便我们可以显式地控制列的命名模式。
- en: '2.4.3 Renaming columns: alias and withColumnRenamed'
  id: totrans-199
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.3 重命名列：alias 和 withColumnRenamed
- en: When performing a transformation on your columns, PySpark will give a default
    name to the resulting column. In our case, we were blessed by the `split(value,`
    `,` `-1)` name after splitting our value column, using a space as the delimiter.
    While accurate, it’s not programmer-friendly. This section provides a blueprint
    to rename columns, both newly created and existing, using `alias()` and `withColumnRenamed()`.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 当对列进行转换时，PySpark 会给结果列提供一个默认名称。在我们的例子中，我们在使用空格作为分隔符拆分值列后，得到了 `split(value,`,
    `,`, `-1)` 这个名字。虽然准确，但并不适合程序员。本节提供了一个蓝图，使用 `alias()` 和 `withColumnRenamed()` 来重命名新创建的和现有的列。
- en: 'There is an implicit assumption that you’ll want to rename the resulting column
    yourself, using the `alias()` method. Its usage isn’t very complicated: when applied
    to a column, it takes a single parameter and returns the column it was applied
    to, with the new name. A simple demonstration is provided in the next listing.'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 隐含的假设是你将使用 `alias()` 方法自己重命名结果列。它的使用并不复杂：当应用于列时，它接受一个参数并返回应用了该参数的列，并带有新名称。下一个列表提供了一个简单的演示。
- en: Listing 2.14 Our data frame before and after the aliasing
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.14 重命名前后的数据框
- en: '[PRE18]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Our new column is called split(value, , -1), which isn’t really pretty.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们的新列名为 split(value, , -1)，这并不太美观。
- en: ❷ We aliased our column to the name line. Much better!
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们将列重命名为 name。好多了！
- en: '`alias()` provides a clean and explicit way to name your columns after you’ve
    performed work on it. On the other hand, it’s not the only renaming player in
    town. Another equally valid way to do so is by using the `.withColumnRenamed()`
    method on the data frame. It takes two parameters: the current name of the column
    and the wanted name of the column. Since we’re already performing work on the
    column with `split`, chaining `alias` makes a lot more sense than using another
    method. Listing 2.15 shows you the two different approaches.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`alias()` 在对列进行操作后提供了一个干净且明确的方式来命名你的列。另一方面，它并不是唯一的重命名选项。另一种同样有效的方法是在数据框上使用
    `.withColumnRenamed()` 方法。它接受两个参数：列的当前名称和想要的名称。由于我们已经在列上使用 `split` 进行了操作，因此链式
    `alias` 比使用其他方法更有意义。列表2.15 展示了两种不同的方法。'
- en: 'When writing your code, choosing between those two options is pretty easy:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在编写代码时，选择这两种选项相当容易：
- en: When you’re using a method where you’re specifying which columns you want to
    appear, like the `select()` method, use `alias()`.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你使用指定要出现的列的方法时，如 `select()` 方法，使用 `alias()`。
- en: If you just want to rename a column without changing the rest of the data frame,
    use `.withColumnRenamed`. Note that, should the column not exist, PySpark will
    treat this method as a no-op and not perform anything.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你只想重命名列而不更改数据框的其余部分，请使用 `.withColumnRenamed`。请注意，如果该列不存在，PySpark 将将此方法视为无操作，不会执行任何操作。
- en: Listing 2.15 Renaming a column, two ways
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.15 两种重命名列的方法
- en: '[PRE19]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This section introduced a new set of PySpark fundamentals: we learned how to
    select not only plain columns but also column transformations. We also learned
    how to explicitly name the resulting columns, avoiding PySpark’s predictable but
    jarring naming convention. Now we can move forward with the remainder of the operations.
    If we look at our five steps, we’re halfway done with step 2\. We have a list
    of words, but we need each token or word to be its own record:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了一套新的 PySpark 基础知识：我们学习了如何选择不仅包括普通列还包括列转换。我们还学习了如何显式命名结果列，避免 PySpark 预测但令人不快的命名约定。现在我们可以继续进行剩余的操作。如果我们看看我们的五个步骤，我们已经完成了第二步的一半。我们有一个单词列表，但我们需要每个标记或单词成为它自己的记录：
- en: '**[DONE]***Read*—Read the input data (we’re assuming a plain text file).'
  id: totrans-213
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取*—读取输入数据（我们假设是一个纯文本文件）。'
- en: '**[IN PROGRESS]***Token*—Tokenize each word.'
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[进行中]***标记*—对每个单词进行标记。'
- en: '*Clean*—Remove any punctuation and/or tokens that aren’t words. Lowercase each
    word.'
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*清理*—删除任何标点符号和/或不是单词的标记。将每个单词转换为小写。'
- en: '*Count*—Count the frequency of each word present in the text.'
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*—计算文本中每个单词的出现频率。'
- en: '*Answer*—Return the top 10 (or 20, 50, 100).'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*—返回前 10（或 20、50、100）个。'
- en: '2.4.4 Reshaping your data: Exploding a list into rows'
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.4 重新塑形你的数据：将列表分解为行
- en: When working with data, a key element in data preparation is making sure that
    it “fits the mold”; this means making sure that the structure containing the data
    is logical and appropriate for the work at hand. At the moment, each record of
    our data frame contains multiple words into an array of strings. It would be better
    to have one record for each word.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当处理数据时，数据准备中的一个关键要素是确保它“符合模式”；这意味着要确保包含数据的结构是逻辑上合理且适合当前工作的。目前，我们数据框中的每一行都包含多个单词到一个字符串数组中。最好每个单词对应一个记录。
- en: Enter the `explode()` function. When applied to a column containing a container-like
    data structure (such as an array), it’ll take each element and give it its own
    row. This is much easier explained visually rather than using words, and figure
    2.4 explains the process.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 进入 `explode()` 函数。当应用于包含容器类型数据结构（如数组）的列时，它将每个元素转换为它自己的行。这比用文字解释更容易用视觉方式说明，图
    2.4 解释了这个过程。
- en: '![](../Images/02-04.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02-04.png)'
- en: Figure 2.4 Exploding a data frame of `array[String]` into a data frame of `String`.
    Each element of each array becomes its own record.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图 2.4 将 `array[String]` 数据框分解为 `String` 数据框。每个数组的每个元素都成为其自己的记录。
- en: The code follows the same structure as `split()`, and you can see the results
    in the next listing. We now have a data frame containing, at most, one word per
    row. We are almost there!
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 代码结构与 `split()` 相同，你可以在下一个列表中看到结果。我们现在有一个数据框，最多每行只有一个单词。我们几乎完成了！
- en: Listing 2.16 Exploding a column of arrays into rows of elements
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.16 将数组列分解为元素行
- en: '[PRE20]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Before continuing our data-processing journey, we can take a step back and look
    at a sample of the data. Just by looking at the 15 rows returned, we can see that
    `Prejudice`, has a comma and that the cell between `Austen` and `This` contains
    the empty string. That gives us a good blueprint of the next steps that need to
    be performed before we start analyzing word frequency.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续我们的数据处理之旅之前，我们可以退一步看看数据的一个样本。仅通过查看返回的 15 行，我们就可以看到“偏见”有一个逗号，以及“奥斯汀”和“这”之间的单元格包含空字符串。这为我们提供了在开始分析单词频率之前需要执行的下一步的良好蓝图。
- en: 'Looking back at our five steps, we can now conclude step 2, and our words are
    tokenized. Let’s attack the third one, where we’ll clean our words to simplify
    the counting:'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾我们的五个步骤，我们现在可以得出第二步，我们的单词已经被分词。让我们来处理第三步，我们将清理我们的单词以简化计数：
- en: '**[DONE]***Read*—Read the input data (we’re assuming a plain text file).'
  id: totrans-228
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取*—读取输入数据（我们假设是一个纯文本文件）。'
- en: '**[DONE]***Token*—Tokenize each word.'
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***标记*—对每个单词进行标记。'
- en: '*Clean*—Remove any punctuation and/or tokens that aren’t words. Lowercase each
    word.'
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*清理*—删除任何标点符号和/或不是单词的标记。将每个单词转换为小写。'
- en: '*Count*—Count the frequency of each word present in the text.'
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*—计算文本中每个单词的出现频率。'
- en: '*Answer*—Return the top 10 (or 20, 50, 100).'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*—返回前 10（或 20、50、100）个。'
- en: '2.4.5 Working with words: Changing case and removing punctuation'
  id: totrans-233
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2.4.5 处理单词：更改大小写和删除标点符号
- en: 'So far, with `split()` and `explode()` our pattern has been the following:
    find the relevant function in `pyspark.sql.functions`, apply it, profit! This
    section will use the same winning formula to normalize the case of our words and
    remove punctuation, so I’ll focus on the functions’ behavior rather than on how
    to apply them. This section takes care of lowering the case (using the `lower()`
    function) and removing punctuation through the usage of a regular expression.'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，使用`split`和`explode`，我们的模式如下：在`pyspark.sql.functions`中找到相关函数，应用它，然后获利！本节将使用相同的成功公式来规范化我们的单词的大小写并删除标点符号，因此我将专注于函数的行为而不是如何应用它们。本节负责将大小写转换为小写（使用`lower`函数）并通过使用正则表达式删除标点符号。
- en: 'Let’s get right to it. Listing 2.17 contains the source code to lower the case
    of all the words in the data frame. The code should look very familiar: we select
    a column transformed by `lower`, a PySpark function lowering the case of the data
    inside the column passed as a parameter. We then alias the resulting column to
    `word_lower` to avoid PySpark’s default nomenclature.'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们直接进入正题。列表2.17包含了将数据框中所有单词转换为小写的源代码。代码看起来非常熟悉：我们选择由`lower`转换的列，这是一个降低传递给参数的列内数据的字面的PySpark函数。然后我们将结果列别名为`word_lower`以避免PySpark的默认命名约定。
- en: Listing 2.17 Lower the case of the words in the data frame
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.17 降低数据框中单词的大小写
- en: '[PRE21]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Next, we want to clean our words of any punctuation and other non-useful characters;
    in this case, we’ll keep only the letters using a regular expression (see the
    end of the section for a reference on regular expressions [or *regex*]). This
    can be a little trickier: we won’t improvise a full NLP (Natural Language Processing)
    library here, and instead rely on the functionality PySpark provides in its data
    manipulation toolbox. In the spirit of keeping this exercise simple, we’ll keep
    the first contiguous group of letters as the word, and remove the rest. It will
    effectively remove punctuation, quotation marks, and other symbols, at the expense
    of being less robust with more exotic word construction. The next listing shows
    the code in all its splendor.'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们想要清除我们的单词中的任何标点符号和其他非有用字符；在这种情况下，我们将仅使用正则表达式保留字母（参见本节末尾的正则表达式参考[或*regex*]）。这可能有点棘手：我们不会在这里即兴创作一个完整的NLP（自然语言处理）库，而是依赖PySpark在其数据处理工具箱中提供的功能。本着保持这个练习简单化的精神，我们将保留第一个连续的字母组作为单词，并删除其余部分。这将有效地删除标点符号、引号和其他符号，但会牺牲对更奇特单词结构的鲁棒性。下一个列表显示了所有代码的精彩之处。
- en: Listing 2.18 Using `regexp_extract` to keep what looks like a word
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.18 使用`regexp_extract`保留看起来像单词的内容
- en: '[PRE22]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ We only match for multiple lowercase characters (between a and z). The plus
    sign (+) will match for one or more occurrences.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们只匹配多个小写字母（介于a和z之间）。加号（+）将匹配一个或多个出现。
- en: Our data frame of words looks pretty regular by now, except for the empty cell
    between `austen` and `this`. In the next section, we cover the filtering operation
    by removing any empty records.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们的单词数据框看起来相当规范，除了`austen`和`this`之间的空单元格。在下一节中，我们将介绍通过删除任何空记录的过滤操作。
- en: Regular expressions for the rest of us
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 我们其余人的正则表达式
- en: 'PySpark uses regular expressions in two functions we have used so far: `regexp_`
    `extract()` and `split()`. You do not have to be a regexp expert to work with
    PySpark (I certainly am not). Throughout the book, each time I use a nontrivial
    regular expression, I’ll provide a plain English definition so you can follow
    along.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark在我们迄今为止使用的两个函数中使用了正则表达式：`regexp_extract`和`split`。你不必成为正则表达式专家就能使用PySpark（我当然不是）。在整个书中，每当我使用一个非平凡的正则表达式时，我都会提供一个简单的英文定义，以便你能够跟上。
- en: If you are interested in building your own, the RegExr ([https://regexr.com/](https://regexr.com/))
    website is really useful, as well as the *Regular Expression Cookbook* by Steven
    Levithan and Jan Goyvaerts (O’Reilly, 2012).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你感兴趣自己构建，RegExr（[https://regexr.com/](https://regexr.com/））网站非常有用，以及Steven
    Levithan和Jan Goyvaerts的《正则表达式烹饪书》（O’Reilly，2012年）。
- en: Exercise 2.1
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 练习2.1
- en: 'Given the following `exo_2_1_df` data frame, how many records will the `solution_`
    `2_1_df` data frame contain? (Note: No need to write code to solve this problem.)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 给定以下`exo_2_1_df`数据框，`solution_2_1_df`数据框将包含多少条记录？（注意：无需编写代码来解决这个问题。）
- en: '[PRE23]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 2.5 Filtering rows
  id: totrans-249
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 2.5 过滤行
- en: An important data manipulation operation is filtering records according to a
    certain predicate. In our case, blank cells shouldn’t be considered—they’re not
    words! This section covers how to filter records from a data frame. After `select()`-ing
    records, filtering is probably the most frequent and easiest operation to perform
    on your data; PySpark provides a simple process to do so.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的数据处理操作是根据某个谓词过滤记录。在我们的例子中，空白单元格不应被视为单词！本节介绍了如何从数据帧中过滤记录。在 `select()` 记录之后，过滤可能是对您的数据执行的最频繁且最简单的操作；PySpark提供了一个简单的流程来完成此操作。
- en: Conceptually, we should be able to provide a test to perform on each record.
    If it returns true, we keep the record. False? You’re out! PySpark provides not
    one, but two identical methods to perform this task. You can use either `.filter()`
    or its alias `.where()`. This duplication is to ease the transition for users
    coming from other data-processing engines or libraries; some use one, some the
    other. PySpark provides both, so no arguments are possible! I prefer `filter()`,
    because `w` maps to more data frame methods (`withColumn()` in chapter 4 or `withColumnRenamed()`
    in chapter 3). If we look at the next listing, we can see that columns can be
    compared to values using the usual Python comparison operators. In this case,
    we’re using “not equal,” or `!=`.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 从概念上讲，我们应该能够为每条记录提供一个测试。如果它返回true，我们保留该记录。如果是false？您就被淘汰了！PySpark提供了不止一个，而是两个相同的方法来执行此任务。您可以使用
    `.filter()` 或其别名 `.where()`。这种重复是为了方便来自其他数据处理引擎或库的用户过渡；有些人使用一个，有些人使用另一个。PySpark提供了这两个，所以不可能有争议！我更喜欢
    `filter()`，因为 `w` 映射到更多的数据帧方法（第4章中的 `withColumn()` 或第3章中的 `withColumnRenamed()`）。如果我们查看下一个列表，我们可以看到列可以使用常用的Python比较运算符与值进行比较。在这种情况下，我们使用“不等于”，或
    `!=`。
- en: Listing 2.19 Filtering rows in your data frame using `where` or `filter`
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 列表2.19 使用 `where` 或 `filter` 过滤数据帧中的行
- en: '[PRE24]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ The blank cell is gone!
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 空白单元格已消失！
- en: Tip If you want to negate a whole expression in a `filter()` method, PySpark
    provides the `~` operator. We could theoretically use `filter(~(col("word")` `==`
    `""))`. Look at the exercises at the end of the chapter to see them in an application.
    You can also use SQL-style expression; check out chapter 7 for an alternative
    syntax.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您想在 `filter()` 方法中否定整个表达式，PySpark提供了 `~` 操作符。理论上，我们可以使用 `filter(~(col("word")
    == ""))`。查看本章末尾的练习，以了解它们在实际应用中的使用。您还可以使用SQL风格的表达式；请参阅第7章了解替代语法。
- en: 'We could have tried to filter earlier in our program. It’s a trade-off to consider:
    if we filtered too early, our filtering clause would have been comically complex
    for no good reason. Since PySpark caches all the transformations until an action
    is triggered, we can focus on the readability of our code and let Spark optimize
    our intent, like we saw in chapter 1\. We’ll see in chapter 3 how you can transform
    PySpark code so it almost reads like a series of written instructions and take
    advantage of the lazy evaluation.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们本可以在程序中更早地进行过滤。这是一个需要考虑的权衡：如果我们过滤得太早，我们的过滤条件将因为没有任何原因而变得滑稽复杂。由于PySpark会在触发操作之前缓存所有转换，因此我们可以专注于代码的可读性，并让Spark优化我们的意图，就像我们在第1章中看到的那样。我们将在第3章中看到如何将PySpark代码转换成几乎像一系列书面指令一样阅读，并利用延迟评估。
- en: 'This seems like a good time to take a break and reflect on what we have accomplished
    so far. If we look at our five steps, we’re 60% of the way there. Our cleaning
    step took care of nonletter characters and filtered the empty records. We’re ready
    for counting and displaying the results of our analysis:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 现在似乎是休息和反思我们所取得的成果的好时机。如果我们看看我们的五个步骤，我们已经完成了60%。我们的清理步骤处理了非字母字符并过滤了空记录。我们准备好对分析结果进行计数和显示：
- en: '**[DONE]***Read*—Read the input data (we’re assuming a plain text file).'
  id: totrans-258
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***读取*—读取输入数据（我们假设是一个纯文本文件）。'
- en: '**[DONE]***Token*—Tokenize each word.'
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***标记*—将每个单词标记化。'
- en: '**[DONE]***Clean*—Remove any punctuation and/or tokens that aren’t words. Lowercase
    each word.'
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**[完成]***清理*—删除任何标点符号和/或不是单词的标记。将每个单词转换为小写。'
- en: '*Count*—Count the frequency of each word present in the text.'
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*计数*—计算文本中每个单词的出现频率。'
- en: '*Answer*—Return the top 10 (or 20, 50, 100).'
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*答案*—返回前10（或20、50、100）条。'
- en: In terms of PySpark operations, we covered a huge amount of ground in the data
    manipulation space. You can now select not only columns but transformations of
    columns, renaming them as you please after the fact. We learned how to break nested
    structures, such as arrays, into single records. Finally, we learned how to filter
    records using simple conditions.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 操作方面，我们在数据处理空间中覆盖了大量的内容。您现在不仅可以选择列，还可以选择列的转换，并在事后按需重命名它们。我们学习了如何将嵌套结构，如数组，分解成单个记录。最后，我们学习了如何使用简单条件过滤记录。
- en: We can now rest. The next chapter will cover the end of our program. We will
    also be looking at bringing our code into one single file, moving away from the
    REPL into batch mode. We’ll explore options to simplify and increase the readability
    of our program and then finish by scaling it to a larger corpus of texts.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以休息一下了。下一章将涵盖我们程序的结束。我们还将探讨将我们的代码合并到一个单独的文件中，从交互式解释器（REPL）模式转向批处理模式。我们将探索简化并提高程序可读性的选项，然后通过将其扩展到更大的文本语料库来完成。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Almost all PySpark programs will revolve around three major steps: reading,
    transforming, and exporting data.'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几乎所有的 PySpark 程序都将围绕三个主要步骤展开：读取、转换和导出数据。
- en: PySpark provides a REPL (read, evaluate, print, loop) via the `pyspark` shell
    where you can experiment interactively with data.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 通过 `pyspark` shell 提供了一个交互式解释器（REPL，读取、评估、打印、循环），您可以在其中与数据进行交互式实验。
- en: PySpark data frames are a collection of columns. You operate on the structure
    using chained transformations. PySpark will optimize the transformations and perform
    the work only when you submit an action, such as `show()`. This is one of the
    pillars of PySpark’s performance.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 数据帧是一组列的集合。您使用链式转换在结构上操作。PySpark 将优化转换，仅在您提交操作（如 `show()`）时执行工作。这是 PySpark
    性能的支柱之一。
- en: PySpark’s repertoire of functions that operate on columns is located in `pyspark
    .sql.functions`.
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 的列操作函数集合位于 `pyspark.sql.functions`。
- en: You can select columns or transformed columns via the `select()` method.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以通过 `select()` 方法选择列或转换后的列。
- en: You can filter columns using the `where()` or `filter()` methods and by providing
    a test that will return `True` or `False`; only the records returning `True` will
    be kept.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `where()` 或 `filter()` 方法以及提供返回 `True` 或 `False` 的测试来过滤列；只有返回 `True` 的记录将被保留。
- en: PySpark can have columns of nested values, like arrays of elements. In order
    to extract the elements into distinct records, you need to use the `explode()`
    method.
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PySpark 可以有嵌套值的列，如元素数组。为了将元素提取到不同的记录中，您需要使用 `explode()` 方法。
- en: Additional exercises
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: 'For all exercises, assume the following:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 对于所有练习，假设以下：
- en: '[PRE25]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: Exercise 2.2
  id: totrans-276
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.2
- en: Given the following data frame, programmatically count the number of columns
    that aren’t strings (answer = only one column isn’t a string).
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 给定以下数据帧，以编程方式计算不是字符串的列数（答案 = 只有一列不是字符串）。
- en: '`createDataFrame()` allows you to create a data frame from a variety of sources,
    such as a pandas data frame or (in this case) a list of lists.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '`createDataFrame()` 允许您从各种来源创建数据帧，例如 pandas 数据帧或（在这种情况下）列表的列表。'
- en: '[PRE26]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Exercise 2.3
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.3
- en: Rewrite the following code snippet, removing the `withColumnRenamed` method.
    Which version is clearer and easier to read?
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 重写以下代码片段，删除 `withColumnRenamed` 方法。哪个版本更清晰、更容易阅读？
- en: '[PRE27]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Exercise 2.4
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.4
- en: Assume a data frame `exo2_4_df`. The following code block gives an error. What
    is the problem, and how can you solve it?
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 假设一个数据帧 `exo2_4_df`。以下代码块会产生错误。问题是什么，如何解决它？
- en: '[PRE28]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Exercise 2.5
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.5
- en: Let’s take our `words_nonull` data frame, available in the next listing. You
    can use the code from the repository (`code/Ch02/end_of_chapter.py`) in your REPL
    to get the data frame loaded.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看一下下一列表中的 `words_nonull` 数据帧。您可以使用存储库中的代码（`code/Ch02/end_of_chapter.py`）在您的
    REPL 中获取加载数据帧。
- en: Listing 2.20 The `words_nonull` for the exercise
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 2.20 练习的 `words_nonull`
- en: '[PRE29]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: a) Remove all of the occurrences of the word *is*.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: a) 删除所有单词 *is* 的出现。
- en: b) (Challenge) Using the `length` function, keep only the words with more than
    three characters.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: b)（挑战）使用 `length` 函数，仅保留具有三个以上字符的单词。
- en: Exercise 2.6
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 2.6
- en: The `where` clause takes a Boolean expression over one or many columns to filter
    the data frame. Beyond the usual Boolean operators (`>`, `<`, `==`, `<=`, `>=`,
    `!=`), PySpark provides other functions that return Boolean columns in the `pyspark.sql.functions`
    module.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`where`子句接受一个布尔表达式，该表达式涉及一个或多个列以过滤数据框。除了常用的布尔运算符（`>`、`<`、`==`、`<=`、`>=`、`!=`）之外，PySpark在`pyspark.sql.functions`模块中提供了其他函数，这些函数返回布尔列。'
- en: A good example is the `isin()` method (applied on a `Column` object, like `col(...).isin(...)`),
    which takes a list of values as a parameter, and will return only the records
    where the value in the column equals a member of the list.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 一个很好的例子是`isin()`方法（应用于`Column`对象，如`col(...).isin(...)`），该方法接受一个值列表作为参数，并且只返回列中的值等于列表中成员的记录。
- en: Let’s say you want to *remove* the words `is`, `not`, `the` and `if` from your
    list of words, using a single `where()` method on the `words_nonull` data frame.
    Write the code to do so.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你想要使用单个`where()`方法从`words_nonull`数据框中移除单词`is`、`not`、`the`和`if`，编写相应的代码。
- en: Exercise 2.7
  id: totrans-296
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习2.7
- en: One of your friends comes to you with the following code. They have no idea
    why it doesn’t work. Can you diagnose the problem in the `try` block, explain
    why it is an error, and provide a fix?
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 你的一个朋友向你展示了以下代码。他们不知道为什么它不起作用。你能诊断`try`块中的问题，解释为什么是错误，并提供一个修复方案？
- en: '[PRE30]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
