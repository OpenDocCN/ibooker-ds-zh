- en: 6 Decision trees and ensemble learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 决策树和集成学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Decision trees and the decision tree learning algorithm
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树和决策树学习算法
- en: 'Random forests: putting multiple trees together into one model'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林：将多个树组合成一个模型
- en: Gradient boosting as an alternative way of combining decision trees
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升作为组合决策树的另一种方法
- en: In chapter 3, we described the binary classification problem and used the logistic
    regression model to predict if a customer is going to churn.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我们描述了二元分类问题，并使用逻辑回归模型来预测客户是否会流失。
- en: 'In this chapter, we also solve a binary classification problem, but we use
    a different family of machine learning models: tree-based models. Decision trees,
    the simplest tree-based model, are nothing but a sequence of if-then-else rules
    put together. We can combine multiple decision trees into an ensemble to achieve
    better performance. We cover two tree-based ensemble models: random forest and
    gradient boosting.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们同样解决一个二元分类问题，但我们使用的是不同系列的机器学习模型：基于树的模型。决策树，最简单的基于树的模型，不过是一系列的条件-结果规则组合。我们可以将多个决策树组合成一个集成，以实现更好的性能。我们介绍了两种基于树的集成模型：随机森林和梯度提升。
- en: 'The project we prepared for this chapter is default prediction: we predict
    whether or not a customer will fail to pay back a loan. We learn how to train
    decision trees and random forest models with Scikit-learn and explore XGBoost—a
    library for implementing gradient boosting models.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们准备的项目是违约预测：我们预测客户是否会无法偿还贷款。我们学习如何使用Scikit-learn训练决策树和随机森林模型，并探索XGBoost——一个实现梯度提升模型的库。
- en: 6.1 Credit risk scoring project
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 信用风险评分项目
- en: Imagine that we work at a bank. When we receive a loan application, we need
    to make sure that if we give the money, the customer will be able to pay it back.
    Every application carries a risk of *default*—the failure to return the money.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 想象我们是一家银行的员工。当我们收到一个贷款申请时，我们需要确保如果我们提供资金，客户将能够偿还。每个申请都存在一种违约风险——即无法偿还资金。
- en: 'We’d like to minimize this risk: before agreeing to give a loan, we want to
    score the customer and assess the chances of default. If it’s too high, we reject
    the application. This process is called “credit risk scoring.”'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望最小化这种风险：在同意提供贷款之前，我们想要对客户进行评分并评估违约的可能性。如果风险太高，我们拒绝申请。这个过程被称为“信用风险评分”。
- en: Machine learning can be used for calculating the risk. For that, we need a dataset
    with loans, where for each application, we know whether or not it was paid back
    successfully. Using this data, we can build a model for predicting the probability
    of default, and we can use this model to assess the risk of future borrowers not
    repaying the money.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习可以用于计算风险。为此，我们需要一个包含贷款的数据库，其中对于每个申请，我们知道它是否成功偿还。使用这些数据，我们可以构建一个预测违约概率的模型，并可以使用这个模型来评估未来借款人无法偿还资金的风险。
- en: 'This is what we do in this chapter: use machine learning to calculate the risk
    of default. The plan for the project is the following:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 这是本章我们要做的事情：使用机器学习来计算违约风险。项目的计划如下：
- en: First, we get the data and do some initial preprocessing.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们获取数据并进行一些初步预处理。
- en: Next, we train a decision tree model from Scikit-learn for predicting the probability
    of default.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 接下来，我们使用Scikit-learn训练一个决策树模型来预测违约概率。
- en: After that, we explain how decision trees work and which parameters the model
    has and show how to adjust these parameters to get the best performance.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们解释了决策树是如何工作的，以及模型有哪些参数，并展示了如何调整这些参数以获得最佳性能。
- en: Then we combine multiple decision trees into one model—a random forest. We look
    at its parameters and tune them to achieve the best predictive performance.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后我们将多个决策树组合成一个模型——随机森林。我们查看其参数并调整它们以实现最佳的预测性能。
- en: Finally, we explore a different way of combining decision trees—gradient boosting.
    We use XGBoost, a highly efficient library that implements gradient boosting.
    We’ll train a model and tune its parameters.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们探索将决策树组合的另一种方式——梯度提升。我们使用XGBoost，这是一个高效的库，实现了梯度提升。我们将训练一个模型并调整其参数。
- en: 'Credit risk scoring is a binary classification problem: the target is positive
    (“1”) if the customer defaults and negative (“0”) otherwise. For evaluating our
    solution, we’ll use AUC (area under the ROC curve), which we covered in chapter
    4\. AUC describes how well our model can separate the cases into positive and
    negative ones.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 信用风险评分是一个二元分类问题：如果客户违约，目标为正（“1”），否则为负（“0”）。为了评估我们的解决方案，我们将使用 AUC（ROC 曲线下面积），这在第
    4 章中已介绍。AUC 描述了我们的模型将案例区分成正例和负例的能力。
- en: The code for this project is available in the book’s GitHub repository at [https://github.com/alexeygrigorev/mlbookcamp-code](https://github.com/alexeygrigorev/mlbookcamp-code)
    (in the chapter-06-trees folder).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本项目的代码可在本书的 GitHub 仓库中找到（[https://github.com/alexeygrigorev/mlbookcamp-code](https://github.com/alexeygrigorev/mlbookcamp-code)
    中的 chapter-06-trees 文件夹）。
- en: 6.1.1 Credit scoring dataset
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 信用评分数据集
- en: For this project, we use a dataset from a data mining course at the Polytechnic
    University of Catalonia ([https://www.cs.upc.edu/~belanche/Docencia/mineria/mineria.html](https://www.cs.upc.edu/~belanche/Docencia/mineria/mineria.html)).
    The dataset describes the customers (seniority, age, marital status, income, and
    other characteristics), the loan (the requested amount, the price of the item),
    and its status (paid back or not).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本项目，我们使用加泰罗尼亚理工大学数据挖掘课程的数据集（[https://www.cs.upc.edu/~belanche/Docencia/mineria/mineria.html](https://www.cs.upc.edu/~belanche/Docencia/mineria/mineria.html)）。该数据集描述了客户（资历、年龄、婚姻状况、收入和其他特征）、贷款（请求的金额、物品的价格）及其状态（已偿还或未偿还）。
- en: We use a copy of this dataset available on GitHub at [https://github.com/gastonstat/
    CreditScoring/](https://github.com/gastonstat/CreditScoring/). Let’s download
    it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用可在 GitHub 上找到的此数据集的副本（[https://github.com/gastonstat/CreditScoring/](https://github.com/gastonstat/CreditScoring/)）。让我们下载它。
- en: 'First, create a folder for our project (e.g., chapter-06-credit-risk), and
    then use `wget` to get it:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为我们的项目创建一个文件夹（例如，chapter-06-credit-risk），然后使用 `wget` 获取它：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Alternatively, you can enter the link to your browser and save it to the project
    folder.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以将链接输入浏览器并保存到项目文件夹中。
- en: 'Next, start a Jupyter Notebook server if it’s not started yet:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果尚未启动，请启动 Jupyter Notebook 服务器：
- en: '[PRE1]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Go to the project folder, and create a new notebook (e.g., chapter-06-credit-risk).
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 进入项目文件夹，创建一个新的笔记本（例如，chapter-06-credit-risk）。
- en: 'As usual, we begin by importing Pandas, NumPy, Seaborn, and Matplotlib:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，我们首先导入 Pandas、NumPy、Seaborn 和 Matplotlib：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After we press Ctrl-Enter, the libraries are imported and we’re ready to read
    the data with Pandas:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们按下 Ctrl-Enter 后，库被导入，我们就可以使用 Pandas 读取数据了：
- en: '[PRE3]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now the data is loaded, so let’s take an initial look at it and see if we need
    to do any preprocessing before we can use it.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在数据已加载，因此让我们先初步查看它，看看在使用之前是否需要进行任何预处理。
- en: 6.1.2 Data cleaning
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 数据清理
- en: To use a dataset for our task, we need to look for any issues in the data and
    fix them.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用数据集完成任务，我们需要查找数据中的任何问题并修复它们。
- en: Let’s start by looking at the first rows of the DataFrame, generated by the
    `df.head()` function (figure 6.1).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看由 `df.head()` 函数生成的 DataFrame 的前几行开始（图 6.1）。
- en: '![](../Images/06-01.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-01.png)'
- en: Figure 6.1 The first five rows of the credit scoring dataset
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 信用评分数据集的前五行
- en: 'First, we can see that all the column names start with a capital letter. Before
    doing anything else, let’s lowercase all the column names and make it consistent
    with other projects (figure 6.2):'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以看到所有列名都以大写字母开头。在执行其他任何操作之前，让我们将所有列名转换为小写，并使其与其他项目保持一致（图 6.2）：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/06-02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-02.png)'
- en: Figure 6.2 The DataFrame with lowercase column names
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.2 列名为小写的 DataFrame
- en: 'We can see that the DataFrame has the following columns:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到 DataFrame 有以下列：
- en: 'status: whether the customer managed to pay back the loan (1) or not (2)'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'status: 客户是否成功偿还贷款（1）或未偿还（2）'
- en: 'seniority: job experience in years'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'seniority: 工作经验（年）'
- en: 'home: type of homeownership: renting (1), a homeowner (2), and others'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'home: 房屋所有权类型：租房（1），房主（2），以及其他'
- en: 'time: period planned for the loan (in months)'
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'time: 贷款计划期（月）'
- en: 'age: age of the client'
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'age: 客户的年龄'
- en: 'marital [status]: single (1), married (2), and others'
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'marital [status]: 单身（1），已婚（2），以及其他'
- en: 'records: whether the client has any previous records: no (1), yes (2) (It’s
    not clear from the dataset description what kind of records we have in this column.
    For the purposes of this project, we may assume that it’s about records in the
    bank’s database.)'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 'records: 客户是否有任何先前记录：没有（1），有（2）（从数据集描述中不清楚这一列中有什么样的记录。为了本项目的目的，我们可能假设它是指银行数据库中的记录。）'
- en: 'job: type of job: full-time (1), part-time (2), and others'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 职业：工作类型：全职（1），兼职（2）和其他
- en: 'expenses: how much the client spends per month'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 支出：客户每月的花费
- en: 'income: how much the client earns per month'
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收入：客户每月的收入
- en: 'assets: total worth of all the assets of the client'
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 资产：客户所有资产的总价值
- en: 'debt: amount of credit debt'
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信贷债务：信用债务金额
- en: 'amount: requested amount of the loan'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 金额：贷款请求的金额
- en: 'price: price of an item the client wants to buy'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 价格：客户想要购买的商品的价格
- en: 'Although most of the columns are numerical, some are categorical: status, home,
    marital [status], records, and job. The values we see in the DataFrame, however,
    are numbers, not strings. This means that we need to translate them to their actual
    names. In the GitHub repository with the dataset is a script that decodes the
    numbers to categories ([https://github.com/gastonstat/CreditScoring/blob/master/Part1_CredScoring_
    Processing.R](https://github.com/gastonstat/CreditScoring/blob/master/Part1_CredScoring_Processing.R)).
    Originally, this script was written in R, so we need to translate it to Pandas.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管大多数列都是数值型，但也有一些是分类型：状态、住所、婚姻[状态]、记录和职业。然而，我们在DataFrame中看到的是数字，而不是字符串。这意味着我们需要将它们转换为它们的实际名称。在包含数据集的GitHub仓库中有一个脚本，可以将数字解码为类别（[https://github.com/gastonstat/CreditScoring/blob/master/Part1_CredScoring_
    Processing.R](https://github.com/gastonstat/CreditScoring/blob/master/Part1_CredScoring_Processing.R)）。最初，这个脚本是用R编写的，因此我们需要将其转换为Pandas。
- en: We start with the status column. The value “1” means “OK,” the value “2” means
    “default,” and “0” means that the value is missing—let’s replace it with “unk”
    (short for “unknown”).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从状态列开始。值“1”表示“OK”，值“2”表示“default”，而“0”表示该值缺失——让我们用“unk”（代表“unknown”）来替换它。
- en: 'In Pandas, we can use `map` for converting the numbers to strings. For that,
    we first define the dictionary with mapping from the current value (number) to
    the desired value (string):'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在Pandas中，我们可以使用`map`将数字转换为字符串。为此，我们首先定义一个字典，将当前值（数字）映射到所需值（字符串）：
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Now we can use this dictionary to do the mapping:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用这个字典来进行映射：
- en: '[PRE6]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: It creates a new series, which we immediately write back to the DataFrame. As
    a result, the values in the status column are overwritten and look more meaningful
    (figure 6.3).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这会创建一个新的序列，我们立即将其写回DataFrame。结果，状态列中的值被覆盖，看起来更有意义（图6.3）。
- en: '![](../Images/06-03.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-03.png)'
- en: Figure 6.3 To translate the original values in the status column (numbers) to
    a more meaningful representation (strings), we use the `map` method.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 为了将状态列中的原始值（数字）转换为更有意义的表示（字符串），我们使用`map`方法。
- en: 'We repeat the same procedure for all the other columns. First, we’ll do it
    for the home column:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对其他所有列重复相同的步骤。首先，我们将对住所列进行操作：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Next, let’s do it for the marital, records, and job columns:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们为婚姻、记录和职业列进行操作：
- en: '[PRE8]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: After these transformations, the columns with categorical variables contain
    the actual values, not numbers (figure 6.4).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些转换后，包含分类变量的列包含的是实际值，而不是数字（图6.4）。
- en: '![](../Images/06-04.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-04.png)'
- en: Figure 6.4 The values of categorical variables are translated from integers
    to strings.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 将分类变量的值从整数转换为字符串。
- en: 'As the next step, let’s take a look at numerical columns. First, let’s check
    the summary statistics for each of the columns: min, mean, max, and others. To
    do so, we can use the `describe` method of the DataFrame:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 作为下一步，让我们看一下数值列。首先，让我们检查每个列的摘要统计信息：最小值、平均值、最大值等。要做到这一点，我们可以使用DataFrame的`describe`方法：
- en: '[PRE9]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Note The output of `describe` may be confusing. In our case, there are values
    in scientific notation like 1.000000e+08 or 8.703625e+06\. To force Pandas to
    use a different notation, we use `round`: it removes the fractional part of a
    number and rounds it to the closest integer.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：`describe`命令的输出可能会让人困惑。在我们的例子中，存在像1.000000e+08或8.703625e+06这样的科学记数法值。为了强制Pandas使用不同的表示法，我们使用`round`：它会移除数字的小数部分并将其四舍五入到最接近的整数。
- en: It gives us an idea of how the distribution of the values in each column looks
    (figure 6.5).
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这让我们对每个列中值的分布情况有了大致的了解（图6.5）。
- en: '![](../Images/06-05.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-05.png)'
- en: Figure 6.5 The summary of all numerical columns of the dataframe. We notice
    that some of them have 99999999 as the max value.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 数据框中所有数值列的摘要。我们注意到其中一些列的最大值是99999999。
- en: One thing we notice immediately is that the max value is `99999999` in some
    cases. This is quite suspicious. As it turns out, it’s an artificial value—this
    is how missing values are encoded in this dataset.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们立即注意到，在某些情况下最大值是`99999999`。这相当可疑。实际上，这是一个人工值——这就是在这个数据集中缺失值是如何编码的。
- en: 'Three columns have this problem: income, assets, and debt. Let’s replace this
    big number with NaN for these columns:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 有三个列存在这个问题：收入、资产和债务。让我们将这些大数字替换为NaN：
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We use the `replace` method, which takes two values:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`replace`方法，它接受两个值：
- en: '`to_replace:` the original value (“99999999,” in our case)'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`to_replace:` 原始值（在我们的案例中是“99999999”）'
- en: '`value:` the target value (“NaN,” in our case)'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`value:` 目标值（在我们的案例中是“NaN”）'
- en: After this transformation, no more suspicious numbers appear in the summary
    (figure 6.6).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这种转换后，汇总统计中不再出现可疑的数字（图6.6）。
- en: '![](../Images/06-06.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-06.png)'
- en: Figure 6.6 The summary statistics after replacing large values with NaN
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 替换大值后的汇总统计
- en: 'Before we finish with the dataset preparation, let’s look at our target variable
    `status`:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在完成数据集准备之前，让我们看看我们的目标变量`status`：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The output of `value_counts` shows the count of each value:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '`value_counts`的输出显示了每个值的计数：'
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Notice that there’s one row with “unknown” status: we don’t know whether or
    not this client managed to pay back the loan. For our project, this row is not
    useful, so let’s remove it from the dataset:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到有一行状态为“未知”：我们不知道这位客户是否成功偿还了贷款。对于我们的项目来说，这一行没有用，所以让我们从数据集中移除它：
- en: '[PRE13]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In this case, we don’t really “remove” it: we create a new DataFrame where
    we don’t have records with “unknown” status.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们实际上并没有“移除”它：我们创建了一个新的DataFrame，其中不包含状态为“未知”的记录。
- en: By looking at the data, we have identified a few important issues in the data
    and addressed them.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看数据，我们已经识别出数据中的一些重要问题，并解决了它们。
- en: For this project, we skip a more detailed exploratory data analysis like we
    did for chapter 2 (the car-price prediction project) and chapter 3 (churn prediction
    project), but you’re free to repeat the steps we covered there for this project
    as well.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们跳过像第2章（汽车价格预测项目）和第3章（客户流失预测项目）中那样更详细的数据探索，但你也可以自由地重复我们在那里覆盖的步骤。
- en: 6.1.3 Dataset preparation
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 数据集准备
- en: 'Now our dataset is cleaned, and we’re almost ready to use it for model training.
    Before we can do that, we need to do a few more steps:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们的数据集已经清理完毕，我们几乎准备好用它进行模型训练了。在我们能够这样做之前，我们还需要做几步：
- en: Split the dataset into train, validation, and test.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据集分割为训练集、验证集和测试集。
- en: Handle missing values.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理缺失值。
- en: Use one-hot encoding to encode categorical variables.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用独热编码对分类变量进行编码。
- en: Create the feature matrix *X* and the target variable *y* .
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建特征矩阵 *X* 和目标变量 *y* 。
- en: 'Let’s start by splitting the data. We will split the data into three parts:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先开始分割数据。我们将数据分成三个部分：
- en: Training data (60% of the original dataset)
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练数据（原始数据集的60%）
- en: Validation data (20%)
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证数据（20%）
- en: Test data (20%)
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试数据（20%）
- en: 'Like previously, we’ll use `train_test_split` from Scikit-learn for that. Because
    we cannot split it into three datasets at once, we’ll need to split two times
    (figure 6.7). First we’ll hold out 20% of data for testing, and then split the
    remaining 80% into training and validation:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将使用Scikit-learn中的`train_test_split`。因为我们不能一次将其分成三个数据集，所以我们需要分两次（图6.7）。首先，我们将20%的数据保留用于测试，然后剩下的80%将分为训练集和验证集：
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '![](../Images/06-07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-07.png)'
- en: Figure 6.7 Because `train_test_split` can split a dataset into only two parts,
    but we need three, we perform the split two times.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7 因为`train_test_split`只能将数据集分成两部分，但我们需要三部分，所以我们进行了两次分割。
- en: When splitting for the second time, we put aside 25% of data instead of 20%
    (`test_` `size=0.25`). Because `df_train_full` contains 80% of records, one-quarter
    (i.e., 25%) of 80% corresponds to 20% of the original dataset.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次分割时，我们将25%的数据放在一边，而不是20%（`test_size=0.25`）。因为`df_train_full`包含80%的记录，四分之一（即25%）的80%对应于原始数据集的20%。
- en: 'To check the size of our datasets, we can use the `len` function:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查数据集的大小，我们可以使用`len`函数：
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'When running it, we get the following output:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 运行时，我们得到以下输出：
- en: '[PRE16]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: So, for training, we will use approximately 2,700 examples and almost 900 for
    validation and testing.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于训练，我们将使用大约2,700个示例，几乎900个用于验证和测试。
- en: 'The outcome we want to predict is `status`. We will use it to train a model,
    so it’s our *y*—the target variable. Because our objective is to determine if
    somebody fails to pay back their loan, the positive class is `default`. This means
    that *y* is “1” if the client defaulted and “0” otherwise. It’s quite simple to
    implement:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要预测的结果是`status`。我们将用它来训练模型，所以它是我们的*y*——目标变量。因为我们的目标是确定某人是否未能偿还贷款，所以正类是`default`。这意味着如果客户违约，*y*是“1”，否则是“0”。这很简单就可以实现：
- en: '[PRE17]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now we need to remove `status` from the DataFrames. If we don’t do it, we may
    accidentally use this variable for training. For that, we use the `del` operator:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要从DataFrames中移除`status`。如果我们不这样做，我们可能会意外地使用这个变量进行训练。为此，我们使用`del`运算符：
- en: '[PRE18]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Next, we’ll take care of *X*—the feature matrix.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将处理*X*——特征矩阵。
- en: 'From the initial analysis, we know our data contains missing values—we added
    these NaNs ourselves. We can replace the missing values with zero:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 从初步分析中，我们知道我们的数据包含缺失值——是我们自己添加的NaN。我们可以用零替换缺失值：
- en: '[PRE19]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: To use categorical variables, we need to encode them. In chapter 3, we applied
    the one-hot encoding technique for that. In one-hot encoding, each value is encoded
    as “1” if it’s present (“hot”) or “0” if it’s absent (“cold”). To implement it,
    we used `DictVectorizer` from Scikit-learn.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用分类变量，我们需要对它们进行编码。在第3章中，我们应用了独热编码技术。在独热编码中，每个值如果存在（“hot”）则编码为“1”，如果不存在（“cold”）则编码为“0”。为了实现它，我们使用了Scikit-learn的`DictVectorizer`。
- en: '`DictVectorizer` needs a list of dictionaries, so we first need to convert
    the DataFrames into this format:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '`DictVectorizer`需要一个字典列表，所以我们需要首先将DataFrames转换为这种格式：'
- en: '[PRE20]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Each dictionary in the result represents a row from the DataFrame. For example,
    the first record in `dict_train` looks like this:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 结果中的每个字典代表DataFrame中的一行。例如，`dict_train`中的第一个记录看起来是这样的：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'This list of dictionaries now can be used as input to `DictVectorizer`:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 这个字典列表现在可以用作`DictVectorizer`的输入：
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: As a result, we have feature matrices for both train and validation datasets.
    Please refer to chapter 3 for more details on doing one-hot encoding with Scikit-learn.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们有了训练集和验证集的特征矩阵。请参阅第3章，了解更多关于使用Scikit-learn进行独热编码的细节。
- en: 'Now we''re ready to train a model! In the next section, we cover the simplest
    tree model: decision tree.'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练一个模型了！在下一节中，我们将介绍最简单的树模型：决策树。
- en: 6.2 Decision trees
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 决策树
- en: A *decision tree* is a data structure that encodes a series of if-then-else
    rules. Each node in a tree contains a condition. If the condition is satisfied,
    we go to the right side of the tree; otherwise, we go to the left. In the end
    we arrive at the final decision (figure 6.8).
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '*决策树*是一种编码一系列if-then-else规则的数据结构。树中的每个节点包含一个条件。如果条件得到满足，我们就走向树的右侧；否则，我们走向左侧。最终我们到达最终决策（图6.8）。'
- en: '![](../Images/06-08.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-08.png)'
- en: Figure 6.8 A decision tree consists of nodes with conditions. If the condition
    in a node is satisfied, we go right; otherwise, we go left.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 决策树由具有条件的节点组成。如果一个节点的条件得到满足，我们就走向右侧；否则，我们走向左侧。
- en: 'It’s quite easy to represent a decision tree as a set of `if-else` statements
    in Python. For example:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 在Python中将决策树表示为一系列`if-else`语句非常简单。例如：
- en: '[PRE23]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: With machine learning, we can extract these rules from data automatically. Let’s
    see how we can do it.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 通过机器学习，我们可以自动从数据中提取这些规则。让我们看看我们如何做到这一点。
- en: 6.2.1 Decision tree classifier
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.1 决策树分类器
- en: 'We’ll use Scikit-learn for training a decision tree. Because we’re solving
    a classification problem, we need to use `DecisionTreeClassifier` from the `tree`
    package. Let’s import it:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Scikit-learn来训练决策树。因为我们正在解决一个分类问题，所以我们需要使用`tree`包中的`DecisionTreeClassifier`。让我们导入它：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Training the model is as simple as invoking the `fit` method:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 训练模型就像调用`fit`方法一样简单：
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To check if the result is good, we need to evaluate the predictive performance
    of the model on the validation set. Let’s use AUC (area under the ROC curve) for
    that.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查结果是否良好，我们需要评估模型在验证集上的预测性能。让我们使用AUC（ROC曲线下的面积）来做这件事。
- en: 'Credit risk scoring is a binary classification problem, and for cases like
    that, AUC is one of the best evaluation metrics. As you may recall from our discussion
    in chapter 4, AUC shows how well a model separates positive examples from negative
    examples. It has a nice interpretation: it describes the probability that a randomly
    chosen positive example (“default”) has a higher score than a randomly chosen
    negative example (“OK”). This is a relevant metric for the project: we want risky
    clients to have higher scores than nonrisky ones. For more details on AUC, refer
    to chapter 4.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 信用风险评估是一个二元分类问题，对于这种情况，AUC是最佳评估指标之一。如您在第四章中回忆的那样，AUC显示了模型将正例与负例分开的好坏程度。它有一个很好的解释：它描述了随机选择的一个正例（“违约”）的分数高于随机选择的一个负例（“正常”）的概率。这是一个与项目相关的指标：我们希望风险客户比非风险客户有更高的分数。有关AUC的更多详细信息，请参阅第四章。
- en: 'Like previously, we’ll use an implementation from Scikit-learn, so let’s import
    it:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们将使用Scikit-learn的实现，所以让我们导入它：
- en: '[PRE26]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'First, we evaluate the performance on the training set. Because we chose AUC
    as the evaluation metric, we need scores, not hard predictions. As we know from
    chapter 3, we need to use the `predict_proba` method for that:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们在训练集上评估性能。因为我们选择了AUC作为评估指标，我们需要分数而不是硬预测。正如我们在第三章中了解的那样，我们需要使用`predict_proba`方法：
- en: '[PRE27]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'When we execute it, we see that the score is 100%—the perfect score. Does it
    mean that we can predict default without errors? Let’s check the score on validation
    before jumping to conclusions:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行它时，我们看到分数是100%——完美的分数。这意味着我们可以无误差地预测违约吗？在得出结论之前，让我们先检查一下验证集上的分数：
- en: '[PRE28]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: After running, we see that AUC on validation is only 65%.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，我们看到验证集上的AUC只有65%。
- en: We just observed a case of *overfitting*. The tree learned the training data
    so well that it simply memorized the outcome for each customer. However, when
    we applied it to the validation set, the model failed. The rules it extracted
    from the data turned out to be too specific to the training set, so it worked
    poorly for customers it didn’t see during training. In such cases, we say that
    the model cannot *generalize*.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚观察到一个过拟合的例子。这个树学得非常好，以至于它简单地记住了每个客户的结局。然而，当我们将其应用于验证集时，模型失败了。它从数据中提取的规则对训练集来说过于具体，因此在训练期间没有看到的客户上表现不佳。在这种情况下，我们说模型不能*泛化*。
- en: Overfitting happens when we have a complex model with enough power to remember
    all the training data. If we force the model to be simpler, we can make it less
    powerful and improve the model’s ability to generalize.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个复杂且足够强大的模型来记住所有训练数据时，就会发生过拟合。如果我们迫使模型更简单，我们可以使其更弱，并提高模型泛化的能力。
- en: 'We have multiple ways to control the complexity of a tree. One option is to
    restrict its size: we can specify the `max_depth` parameter, which controls the
    maximum number of levels. The more levels a tree has, the more complex rules it
    can learn (figure 6.9).'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有多种方法可以控制树的复杂性。一个选项是限制其大小：我们可以指定`max_depth`参数，它控制最大层数。树具有的层次越多，它能够学习的规则就越复杂（见图6.9）。
- en: '![](../Images/06-09.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-09.png)'
- en: Figure 6.9 A tree with more levels can learn more complex rules. A tree with
    two levels is less complex than a tree with three levels and, thus, less prone
    to overfitting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.9 一个具有更多层次的树可以学习更复杂的规则。一个具有两个层次的树比具有三个层次的树更简单，因此更不容易过拟合。
- en: The default value for the `max_depth` parameter is `None`, which means that
    the tree can grow as large as possible. We can try a smaller value and compare
    the results.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: '`max_depth`参数的默认值是`None`，这意味着树可以尽可能大。我们可以尝试一个更小的值，并比较结果。'
- en: 'For example, we can change it to 2:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以将其更改为2：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'To visualize the tree we just learned, we can use the `export_text` function
    from the `tree` package:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化我们刚刚学习的树，我们可以使用`tree`包中的`export_text`函数：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'We only need to specify the names of features using the `feature_names` parameter.
    We can get it from the `DictVectorizer`. When we print it, we get the following:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要使用`feature_names`参数指定特征的名称。我们可以从`DictVectorizer`中获取它。当我们打印出来时，我们得到以下内容：
- en: '[PRE31]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Each line in the output corresponds to a node with a condition. If the condition
    is true, we go inside and repeat the process until we arrive at the final decision.
    At the end, if class is `True`, then the decision is “default,” and otherwise
    it’s “OK.”
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 输出中的每一行都对应一个具有条件的节点。如果条件为真，我们就进入并重复这个过程，直到我们到达最终决策。最后，如果类别是`True`，则决策是“违约”，否则是“正常”。
- en: 'The condition `records=no` `>` `0.50` means that a customer has no records.
    Recall that we use one-hot encoding to represent `records` with two features:
    `records=yes` and `records=no`. For a customer with no records, `records=no` is
    set to “1” and `records=yes` to “0.” Thus, “`records=no` `>` `0.50` is true when
    the value for `records` is `no` (figure 6.10).'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 `records=no` `>` `0.50` 意味着客户没有记录。回想一下，我们使用 one-hot 编码用两个特征来表示 `records`：`records=yes`
    和 `records=no`。对于一个没有记录的客户，`records=no` 被设置为“1”，而 `records=yes` 被设置为“0”。因此，“`records=no`
    `>` `0.50`”在 `records` 的值为 `no` 时为真（图6.10）。
- en: '![](../Images/06-10.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-10.png)'
- en: Figure 6.10 The tree we learned with `max_depth` set to 2
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.10 将 `max_depth` 设置为 2 时我们学习的树
- en: 'Let’s check the score:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查分数：
- en: '[PRE32]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We see that the score on train dropped:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到训练分数下降了：
- en: '[PRE33]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Previously, the performance on the training set was 100%, but now it’s only
    70.5%. It means that the model can no longer memorize all the outcomes from the
    training set.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，训练集上的性能是 100%，但现在只有 70.5%。这意味着模型不能再记住训练集中的所有结果。
- en: 'However, the score on the validation set is better: it’s 66.9%, which is an
    improvement over the previous result (65%). By making it less complex, we improved
    the ability of our model to generalize. Now it’s better at predicting the outcomes
    for customers it hasn’t seen previously.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，验证集上的分数更好：66.9%，比之前的结果（65%）有所提高。通过使其更简单，我们提高了模型泛化的能力。现在它更好地预测它之前未见过的客户的结局。
- en: 'However, this tree has another problem—it’s too simple. To make it better,
    we need to tune the model: try different parameters, and see which ones lead to
    the best AUC. In addition to `max_depth`, we can control other parameters. To
    understand what these parameters mean and how they influence the model, let’s
    take a step back and look at how decision trees learn rules from data.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这棵树还有一个问题——它太简单了。为了使其更好，我们需要调整模型：尝试不同的参数，看看哪些参数能导致最佳的 AUC。除了 `max_depth`，我们还可以控制其他参数。为了了解这些参数的含义以及它们如何影响模型，让我们退一步，看看决策树是如何从数据中学习规则的。
- en: 6.2.2 Decision tree learning algorithm
  id: totrans-178
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.2 决策树学习算法
- en: 'To understand how a decision tree learns from data, let’s simplify the problem.
    First, we’ll use a much smaller dataset with just one feature: `assets` (figure 6.11).'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解决策树如何从数据中学习，让我们简化这个问题。首先，我们将使用一个包含仅一个特征：`assets` 的更小的数据集（图6.11）。
- en: '![](../Images/06-11.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-11.png)'
- en: 'Figure 6.11 A smaller dataset with one feature: `assets`. The target variable
    is `status`.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.11 一个包含一个特征：`assets` 的较小数据集。目标变量是 `status`。
- en: Second, we’ll grow a very small tree, with a single node.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，我们将生长一个非常小的树，只有一个节点。
- en: The only feature we have in the dataset is `assets`. This is why the condition
    in the node will be `assets` `>` `T`, where *T* is a threshold value that we need
    to determine. If the condition is true, we’ll predict “OK,” and if it’s false,
    our prediction will be “default” (figure 6.12).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集中我们只有一个特征：`assets`。这就是为什么节点中的条件将是 `assets` `>` `T`，其中 *T* 是我们需要确定的阈值。如果条件为真，我们将预测“OK”，如果为假，我们的预测将是“default”（图6.12）。
- en: '![](../Images/06-12.png)'
  id: totrans-184
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-12.png)'
- en: Figure 6.12 A simple decision tree with only one node. The node contains a condition
    `assets` `>` `T`. We need to find the best value for *T*.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.12 一个只有一个节点的简单决策树。该节点包含一个条件 `assets` `>` `T`。我们需要找到 *T* 的最佳值。
- en: 'The condition `assets` `>` `T` is called a *split*. It splits the dataset into
    two groups: the data points that satisfy the condition and the data points that
    do not.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 条件 `assets` `>` `T` 被称为 *split*。它将数据集分为两组：满足条件的数据点和不满足条件的数据点。
- en: If `T` is 4000, then we have customers with more than $4,000 in assets (on the
    right) and the customers with less than $4,000 in assets (on the left) (figure 6.13).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `T` 是 4000，那么我们就有资产超过 $4,000 的客户（在右侧）和资产少于 $4,000 的客户（在左侧）（图6.13）。
- en: '![](../Images/06-13.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-13.png)'
- en: 'Figure 6.13 The condition in a node splits the dataset into two parts: data
    points that satisfy the condition (on the right) and data points that don’t (on
    the left).'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 节点中的条件将数据集分为两部分：满足条件的数据点（在右侧）和不满足条件的数据点（在左侧）。
- en: Now we turn these groups into *leaves*—the decision nodes—by taking the most
    frequent status in each group and using it as the final decision. In our example,
    “default” is the most frequent outcome in the left group and “OK” in the right
    (figure 6.14).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将这些组转换为 *leaves*——决策节点——通过在每个组中取最频繁的状态并将其用作最终决策。在我们的例子中，“default”是左侧组中最频繁的结果，“OK”在右侧（图6.14）。
- en: '![](../Images/06-14.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-14.png)'
- en: Figure 6.14 The most frequent outcome on the left is “default.” For the group
    on the right, it’s “OK.”
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 左侧最频繁的结果是“default”。对于右侧的组，结果是“OK”。
- en: Thus, if a customer has more than $4,000 in assets, our decision is “OK,” and,
    otherwise, it’s “default” `assets` `>` `4000` (figure 6.15).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果一个客户的资产超过$4,000，我们的决策是“OK”，否则是“default” `assets` `>` `4000`（图6.15）。
- en: '![](../Images/06-15.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-15.png)'
- en: Figure 6.15 By taking the most frequent outcome in each group and assigning
    it to leaves, we get the final decision tree
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 通过将每个组的最大频率结果分配给叶子节点，我们得到最终的决策树
- en: Impurity
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 杂质度
- en: These groups should be as homogeneous as possible. Ideally, each group should
    contain only observations of one class. In this case, we call these groups *pure*.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这些组应尽可能同质。理想情况下，每个组应只包含一个类的观测值。在这种情况下，我们称这些组为*纯*。
- en: 'For example, if we have a group of four customers with outcomes [“default,”
    “default,” “default,” “default”], it’s pure: it contains only customers who defaulted.
    But a group [“default,” “default,” “default,” “OK”] is impure: there’s one customer
    who didn’t default.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们有一个包含四个客户的结果组[“default”，“default”，“default”，“default”]，它是纯的：它只包含违约的客户。但一个组[“default”，“default”，“default”，“OK”]是不纯的：有一个客户没有违约。
- en: When training a decision tree model, we want to find such *T* that the *impurity*
    of both groups is minimal.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练决策树模型时，我们希望找到这样的 *T*，使得两组的 *杂质度* 最小。
- en: 'So, the algorithm for finding *T* is quite simple:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，寻找 *T* 的算法相当简单：
- en: Try all possible values of *T.*
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 尝试所有可能的 *T* 值。
- en: For each *T*, split the dataset into left and right groups and measure their
    impurity.
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个 *T*，将数据集分为左右两组并测量它们的杂质度。
- en: Select *T* that has the lowest degree of impurity.
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有最低杂质度的 *T*。
- en: We can use different criteria for measuring impurity. The easiest one to understand
    is the *misclassification rate*, which says how many observations in a group don’t
    belong to the majority class.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的标准来衡量杂质度。最容易理解的是 *误分类率*，它表示一个组中有多少观测值不属于多数类。
- en: 'Note Scikit-learn uses more advanced split criteria such as entropy and the
    Gini impurity. We do not cover them in this book, but the idea is the same: they
    measure the degree of impurity of the split.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：Scikit-learn使用更先进的分割标准，如熵和基尼杂质度。我们在这本书中没有涵盖它们，但思想是相同的：它们测量分割的杂质度程度。
- en: '![](../Images/06-16.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-16.png)'
- en: Figure 6.16 For `assets` `>` `4000`, the misclassification rate for both groups
    is one-quarter.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 对于 `assets` `>` `4000`，两组的误分类率都是四分之一。
- en: 'Let’s calculate the misclassification rate for the split *T* = 4000 (figure 6.16):'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算分割 *T* = 4000 的误分类率（图6.16）：
- en: For the left group, the majority class is “default.” There are four data points
    in total, and one doesn’t belong to “default.” The misclassification rate is 25%
    (1/4).
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于左侧组，多数类是“default”。总共有四个数据点，其中有一个不属于“default”。误分类率是25%（1/4）。
- en: For the right group, “OK” is the majority class, and there’s one “default.”
    Thus, the misclassification rate is also 25% (1/4).
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于右侧组，“OK”是多数类，有一个“default”。因此，误分类率也是25%（1/4）。
- en: To calculate the overall impurity of the split, we can take the average across
    both groups. In this case, the average is 25%.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了计算分割的整体杂质度，我们可以对两组取平均值。在这种情况下，平均值为25%。
- en: Note In reality, instead of taking the simple average across both groups, we
    take a weighted average—we weight each group proportionally to its size. To simplify
    calculations, we use the simple average in this chapter.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在实际中，我们不是简单地跨两组取平均值，而是按比例对每个组进行加权平均——我们按其大小成比例地加权每个组。为了简化计算，我们在这章中使用简单平均。
- en: '*T* = 4000 is not the only possible split for `assets`. Let’s try other values
    for *T* such as 2000, 3000, and 5000 (figure 6.17):'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*T* = 4000 不是 `assets` 的唯一可能分割。让我们尝试其他 *T* 值，例如2000、3000和5000（图6.17）：'
- en: For *T* = 2000, we have 0% impurity on the left (0/2, all are “default”) and
    33.3% impurity on the right (2/6, 2 out of 6 are “default,” the rest are “OK”).
    The average is 16.6%.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *T* = 2000，左侧的杂质度为0%（0/2，所有都是“default”），右侧的杂质度为33.3%（2/6，6个中的2个是“default”，其余是“OK”）。平均值为16.6%。
- en: For *T* = 3000, 0% on the left and 20% (1/5) on the right. The average is 10%.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *T* = 3000，左侧为0%，右侧为20%（1/5）。平均值为10%。
- en: For *T* = 5000, 50% (3/6) on the left and 50% (1/2) on the right. The average
    is 50%.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *T* = 5000，左侧的50%（3/6）和右侧的50%（1/2）。平均值为50%。
- en: '![](../Images/06-17.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-17.png)'
- en: Figure 6.17 In addition to `assets` `>` `4000`, we can try other values of *T*,
    such as 2000, 3000, and 5000.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 除了 `assets` `>` `4000`，我们还可以尝试其他 *T* 的值，例如2000、3000和5000。
- en: 'The best average impurity is 10% for *T* = 3000: we got zero mistakes for the
    left tree and only one (out of five rows) for the right. So, we should select
    3000 as the threshold for our final model (figure 6.18).'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当 *T* = 3000 时，最佳平均不纯度为10%：左边的树没有错误，而右边的树只有一个（五行中的一行）。因此，我们应该选择3000作为我们最终模型的阈值（图6.18）。
- en: '![](../Images/06-18.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-18.png)'
- en: Figure 6.18 The best split for this dataset is `assets` `>` `3000`.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 对于这个数据集，最好的分割是 `assets` `>` `3000`。
- en: Selecting the best feature for splitting
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 选择最佳的分割特征
- en: 'Now let’s make the problem a bit more complex and add another feature to the
    dataset: `debt` (figure 6.19).'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们使问题变得更加复杂，并向数据集添加另一个特征：`debt`（图6.19）。
- en: '![](../Images/06-19.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-19.png)'
- en: 'Figure 6.19 A dataset with two features: `assets` and `debt`. The target variable
    is `status`.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 包含两个特征的数据集：`assets` 和 `debt`。目标变量是 `status`。
- en: 'Previously we had only one feature: `assets`. We knew for sure that it would
    be used for splitting the data. Now we have two features, so in addition to selecting
    the best threshold for splitting, we need to figure out which feature to use.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们只有一个特征：`assets`。我们确信它将被用于分割数据。现在我们有两个特征，因此除了选择最佳的分割阈值外，我们还需要弄清楚使用哪个特征。
- en: 'The solution is simple: we try all the features, and for each feature select
    the best threshold.'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 解决方案很简单：我们尝试所有特征，并为每个特征选择最佳阈值。
- en: 'Let’s modify the training algorithm to include this change:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们修改训练算法以包括这个变化：
- en: For each feature, try all possible thresholds.
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个特征，尝试所有可能的阈值。
- en: For each threshold value *T*, measure the impurity of the split.
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个阈值值 *T*，测量分割的不纯度。
- en: Select the feature and the threshold with the lowest impurity possible.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择具有可能最低不纯度的特征和阈值。
- en: 'Let’s apply this algorithm to our dataset:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将这个算法应用于我们的数据集：
- en: We already identified that for `assets`, the best *T* is 3000\. The average
    impurity of this split is 10%.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们已经确定对于 `assets`，最好的 *T* 是 3000。这个分割的平均不纯度为10%。
- en: For `debt`, the best *T* is 1000\. In this case, the average impurity is 17%.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 `debt`，最好的 *T* 是 1000。在这种情况下，平均不纯度为17%。
- en: So, the best split is `asset` `>` `3000` (figure 6.20).
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，最好的分割是 `asset` `>` `3000`（图6.20）。
- en: '![](../Images/06-20.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-20.png)'
- en: Figure 6.20 The best split is `assets` `>` `3000`, which has the average impurity
    of 10%.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 最好的分割是 `assets` `>` `3000`，其平均不纯度为10%。
- en: 'The group on the left is already pure, but the group on the right is not. We
    can make it less impure by repeating the process: split it again!'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的群组已经纯净，但右边的群组不是。我们可以通过重复这个过程来减少其不纯度：再次分割它！
- en: When we apply the same algorithm to the dataset on the right, we find that the
    best split condition is `debt` `>` `1000`. We have two levels in the tree now—or
    we can say that the depth of this tree is 2 (figure 6.21).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将相同的算法应用于右侧的数据集时，我们发现最好的分割条件是 `debt` `>` `1000`。现在树中有两个层级——或者说，我们可以这样说，这个树的深度是2（图6.21）。
- en: '![](../Images/06-21.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-21.png)'
- en: Figure 6.21 By repeating the algorithm recursively to the group on the right,
    we get a tree with two levels.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 通过递归地重复算法到右边的群组，我们得到一个有两个层级的树。
- en: 'Before the decision tree is ready, we need to do the last step: convert the
    groups into decision nodes. For that, we take the most frequent status in each
    group. This way, we get a decision tree (figure 6.22).'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在决策树准备好之前，我们需要做最后一步：将群组转换为决策节点。为此，我们取每个群组中最频繁的状态。这样，我们得到一个决策树（图6.22）。
- en: '![](../Images/06-22.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-22.png)'
- en: Figure 6.22 The groups are already pure, so the most frequent status is the
    only status each group has. We take this status as the final decision in each
    leaf.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 群组已经纯净，所以最频繁的状态就是每个群组唯一的状态。我们将这个状态作为每个叶节点的最终决策。
- en: Stopping criteria
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 停止标准
- en: When training a decision tree, we can keep splitting the data until all the
    groups are pure. This is exactly what happens when we don’t put any restrictions
    on the trees in Scikit-learn. As we’ve seen, the resulting model becomes too complex,
    which leads to overfitting.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当训练决策树时，我们可以继续分割数据，直到所有群组都是纯净的。这正是当我们不对Scikit-learn中的树进行任何限制时发生的情况。正如我们所看到的，得到的模型变得过于复杂，这导致了过拟合。
- en: We solved this problem by using the `max_depth` parameter—we restricted the
    tree size and didn’t let it grow too big.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过使用 `max_depth` 参数解决了这个问题——我们限制了树的大小，不让它长得太大。
- en: To decide if we want to continue splitting the data, we use *stopping criteria*—criteria
    that describe if we should add another split in the tree or stop.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 要决定是否继续分割数据，我们使用*停止标准*——描述是否应在树中添加另一个分割或停止的标准。
- en: The most common stopping criteria are
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的停止标准是
- en: The group is already pure.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组已经纯净。
- en: The tree reached the depth limit (controlled by the `max_depth` parameter).
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树达到了深度限制（由`max_depth`参数控制）。
- en: The group is too small to continue splitting (controlled by the `min_samples_
    leaf` parameter).
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组太小，无法继续分割（由`min_samples_leaf`参数控制）。
- en: By using these criteria to stop earlier, we force our model to be less complex
    and, therefore, reduce the risk of overfitting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用这些标准提前停止，我们迫使我们的模型更简单，从而降低过拟合的风险。
- en: 'Let’s use this information to adjust the training algorithm:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们利用这些信息来调整训练算法：
- en: 'Find the best split:'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到最佳分割点：
- en: For each feature try all possible threshold values.
  id: totrans-256
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对每个特征尝试所有可能的阈值值。
- en: Use the one with the lowest impurity.
  id: totrans-257
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用具有最低不纯度的算法。
- en: If the maximum allowed depth is reached, stop.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果达到最大允许深度，则停止。
- en: If the group on the left is sufficiently large and it’s not pure yet, repeat
    on the left.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果左侧的组足够大且尚未纯净，则在左侧重复。
- en: If the group on the right is sufficiently large and it’s not pure yet, repeat
    on the right.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果右侧的组足够大且尚未纯净，则在右侧重复。
- en: Even though this is a simplified version of the decision tree learning algorithm,
    it should provide you enough intuition about the internals of the learning process.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这是一个简化的决策树学习算法版本，但它应该足以让你对学习过程的内部有足够的直觉。
- en: Most important, we know two parameters control the complexity of the model.
    By changing these parameters, we can improve the performance of the model.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们知道两个参数控制着模型的复杂性。通过改变这些参数，我们可以提高模型的表现。
- en: Exercise 6.1
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.1
- en: We have a dataset with 10 features and need to add another feature to this dataset.
    What happens with the speed of training?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含10个特征的数据集，需要向该数据集添加另一个特征。训练速度会发生什么变化？
- en: a) With one more feature, training takes longer.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: a) 添加一个特征后，训练时间会更长。
- en: b) The number of features does not affect the speed of training.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: b) 特征数量不会影响训练速度。
- en: 6.2.3 Parameter tuning for decision tree
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2.3 决策树参数调整
- en: The process of finding the best set of parameters is called *parameter tuning*.
    We usually do it by changing the model and checking its score on the validation
    dataset. In the end, we use the model with the best validation score.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 寻找最佳参数集的过程称为*参数调整*。我们通常通过改变模型并检查其在验证数据集上的分数来完成此操作。最后，我们使用具有最佳验证分数的模型。
- en: 'As we have just learned, we can tune two parameters:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们刚刚学到的，我们可以调整两个参数：
- en: '`max_depth`'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: '`min_leaf_size`'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_leaf_size`'
- en: These two are the most important ones, so we will adjust only them. You can
    check the other parameters in the official documentation ([https://scikit-learn.org/stable/
    modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个是最重要的，所以我们只调整它们。您可以在官方文档中检查其他参数（[https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html)）。
- en: When we trained our model previously, we restricted the depth of the tree to
    2, but we didn’t touch `min_leaf_size`. With this, we got an AUC of 66% on the
    validation set.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们之前训练模型时，我们将树的深度限制为2，但没有触及`min_leaf_size`。这样，我们在验证集上得到了66%的AUC。
- en: Let’s find the best parameters.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们找到最佳参数。
- en: 'We start by tuning max_depth. For that, we iterate over a few reasonable values
    and see what works best:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先调整`max_depth`。为此，我们迭代几个合理的值，看看哪个效果最好：
- en: '[PRE34]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The value `None` means that there’s no restriction on depth, so the tree will
    grow as large as it can.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 值`None`表示没有对深度的限制，因此树将尽可能生长。
- en: When we run this code, we see that `max_depth` of 5 gives the best AUC (76.6%),
    followed by 4 and 6 (figure 6.23).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们运行此代码时，我们看到`max_depth`为5时给出了最佳的AUC（76.6%），其次是4和6（图6.23）。
- en: '![](../Images/06-23.png)'
  id: totrans-279
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-23.png)'
- en: Figure 6.23 The optimal value for depth is 5 (76.6%) followed by 4 (76.1%) and
    6 (75.4%).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 深度的最佳值为5（76.6%），其次是4（76.1%）和6（75.4%）。
- en: 'Next, we tune `min_leaf_size`. For that, we iterate over the three best parameters
    of `max_depth`, and for each, go over different values of `min_leaf_size`:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们调整`min_leaf_size`。为此，我们对`max_depth`的三个最佳参数进行迭代，并对每个参数，遍历不同的`min_leaf_size`值：
- en: '[PRE35]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: After running it, we see that the best AUC is 78.5% with parameters `min_sample_
    leaf=15` and `max_depth=6` (table 6.1).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 运行后，我们看到最佳的AUC为78.5%，参数为`min_sample_leaf=15`和`max_depth=6`（表6.1）。
- en: Note As we see, the value we use for `min_leaf_size` influences the best value
    of `max_depth`. You can experiment with a wider range of values for `max_depth`
    to tweak the performance further.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：正如我们所见，我们用于`min_leaf_size`的值会影响`max_depth`的最佳值。你可以尝试更广泛的`max_depth`值范围来进一步调整性能。
- en: Table 6.1 AUC on validation set for different values of `min_leaf_size` (rows)
    and `max_depth` (columns)
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 表6.1 对于不同`min_leaf_size`（行）和`max_depth`（列）值的验证集上的AUC
- en: '|  | depth=4 | depth=5 | depth=6 |'
  id: totrans-286
  prefs: []
  type: TYPE_TB
  zh: '|  | depth=4 | depth=5 | depth=6 |'
- en: '| 1 | 0.761 | 0.766 | 0.754 |'
  id: totrans-287
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.761 | 0.766 | 0.754 |'
- en: '| 5 | 0.761 | 0.768 | 0.760 |'
  id: totrans-288
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0.761 | 0.768 | 0.760 |'
- en: '| 10 | 0.761 | 0.762 | 0.778 |'
  id: totrans-289
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0.761 | 0.762 | 0.778 |'
- en: '| 15 | 0.764 | 0.772 | **0.785** |'
  id: totrans-290
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 0.764 | 0.772 | **0.785** |'
- en: '| 20 | 0.761 | 0.774 | 0.774 |'
  id: totrans-291
  prefs: []
  type: TYPE_TB
  zh: '| 20 | 0.761 | 0.774 | 0.774 |'
- en: '| 50 | 0.753 | 0.768 | 0.770 |'
  id: totrans-292
  prefs: []
  type: TYPE_TB
  zh: '| 50 | 0.753 | 0.768 | 0.770 |'
- en: '| 100 | 0.756 | 0.763 | 0.776 |'
  id: totrans-293
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 0.756 | 0.763 | 0.776 |'
- en: '| 200 | 0.747 | 0.759 | 0.768 |'
  id: totrans-294
  prefs: []
  type: TYPE_TB
  zh: '| 200 | 0.747 | 0.759 | 0.768 |'
- en: 'We have found the best parameters, so let’s use them to train the final model:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经找到了最佳参数，所以让我们使用它们来训练最终的模型：
- en: '[PRE36]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Decision trees are simple and effective models, but they become even more powerful
    when we combine many trees together. Next, we’ll see how we can do it to achieve
    even better predictive performance.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树是简单而有效的模型，但当我们组合多个树时，它们变得更加强大。接下来，我们将看到如何实现它以获得更好的预测性能。
- en: 6.3 Random forest
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 随机森林
- en: For a moment, let’s suppose that we don’t have a machine learning algorithm
    to help us with credit risk scoring. Instead, we have a group of experts.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们没有机器学习算法来帮助我们进行信用风险评估。相反，我们有一组专家。
- en: Each expert can independently decide if we should approve a loan application
    or reject it. An individual expert may make a mistake. However, it’s less likely
    that all the experts together decide to accept the application, but the customer
    fails to pay the money back.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 每个专家可以独立决定是否批准贷款申请或拒绝它。单个专家可能会犯错误。然而，所有专家一起决定接受申请，但客户未能偿还金钱的可能性较小。
- en: Thus, we can ask all the experts independently and then combine their verdicts
    into the final decision, for example, by using the majority vote (figure 6.24).
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以独立地询问所有专家，然后将他们的意见合并成最终的决策，例如，通过使用多数投票（图6.24）。
- en: '![](../Images/06-24.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-24.png)'
- en: Figure 6.24 A group of experts can make a decision better than a single expert
    individually.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 一组专家的决策比单个专家的决策更好。
- en: This idea also applies to machine learning. One model individually may be wrong,
    but if we combine the output of multiple models into one, the chance of an incorrect
    answer is smaller. This concept is called *ensemble learning*, and a combination
    of models is called an *ensemble*.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 这个想法也适用于机器学习。单个模型可能错误，但如果我们将多个模型的输出组合成一个，错误答案的可能性更小。这个概念被称为*集成学习*，模型的组合被称为*集成*。
- en: For this to work, the models need to be different. If we train the same decision
    tree model 10 times, they will all predict the same output, so it’s not useful
    at all.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这可行，模型需要不同。如果我们训练同一个决策树模型10次，它们都会预测相同的输出，所以这完全没有用。
- en: 'The easiest way to have different models is to train each tree on a different
    subset of features. For example, suppose we have three features: `assets`, `debts`,
    and `price`. We can train three models:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 要有不同的模型，最简单的方法是训练每棵树在不同的特征子集上。例如，假设我们有三个特征：“资产”、“负债”和“价格”。我们可以训练三个模型：
- en: The first will use `assets` and `debts`.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个将使用“资产”和“负债”。
- en: The second will use `debts` and `price`.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个将使用“负债”和“价格”。
- en: The last one will use `assets` and `price`.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后一个将使用“资产”和“价格”。
- en: With this approach, we’ll have different trees, each making its own decisions
    (figure 6.25). But when we put their predictions together, their mistakes average
    out, and combined, they have more predictive power.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 采用这种方法，我们将有不同的树，每棵树都会做出自己的决策（图6.25）。但是，当我们把他们的预测放在一起时，他们的错误会平均化，结合起来，它们具有更强的预测能力。
- en: '![](../Images/06-25.png)'
  id: totrans-311
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-25.png)'
- en: Figure 6.25 Models we want to combine in an ensemble should not be the same.
    We can make sure they are different by training each tree on a different subset
    of features.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25 我们想要组合的集成模型不应该相同。我们可以通过在每个树的不同特征子集上训练来确保它们不同。
- en: 'This way of putting together multiple decision trees into an ensemble is called
    a *random forest*. To train a random forest, we can do this (figure 6.26):'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 将多个决策树组合成一个集成的方式称为*随机森林*。为了训练随机森林，我们可以这样做（图6.26）：
- en: Train *N* independent decision tree models.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练*N*个独立的决策树模型。
- en: For each model, select a random subset of features, and use only them for training.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个模型，选择一个随机特征子集，并仅使用这些特征进行训练。
- en: When predicting, combine the output of *N* models into one.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测时，将*N*个模型的输出组合在一起。
- en: Note This is a very simplified version of the algorithm. It’s enough to illustrate
    the main idea, but in reality, it’s more complex.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这是一个非常简化的算法版本。它足以说明主要思想，但在现实中，它更复杂。
- en: '![](../Images/06-26.png)'
  id: totrans-318
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-26.png)'
- en: 'Figure 6.26 Training a random forest model: for training each tree, randomly
    select a subset of features. When making the final prediction, combine all the
    predictions into one.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 训练随机森林模型：对于每个树的训练，随机选择一个特征子集。在做出最终预测时，将所有预测组合在一起。
- en: Scikit-learn contains an implementation of a random forest, so we can use it
    for solving our problem. Let’s do it.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn包含随机森林的实现，因此我们可以用它来解决我们的问题。让我们试试。
- en: 6.3.1 Training a random forest
  id: totrans-321
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 训练随机森林
- en: 'To use random forest in Scikit-learn, we need to import `RandomForestClassifier`
    from the `ensemble` package:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-learn中使用随机森林，我们需要从`ensemble`包中导入`RandomForestClassifier`：
- en: '[PRE37]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'When training a model, the first thing we need to specify is the number of
    trees we want to have in the ensemble. We do it with the `n_estimators` parameter:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，我们首先需要指定我们想要在集成中拥有的树的数量。我们通过`n_estimators`参数来完成：
- en: '[PRE38]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'After training finishes, we can evaluate the performance of the result:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们可以评估结果：
- en: '[PRE39]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It shows 77.9%. However, the number you see may be different. Every time we
    retrain the model, the score changes: it varies from 77% to 80%.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 它显示77.9%。然而，你看到的数字可能不同。每次我们重新训练模型，分数都会变化：它在77%到80%之间变化。
- en: 'The reason for this is randomization: to train a tree, we randomly select a
    subset of features. To make the results consistent, we need to fix the seed for
    the random-number generator by assigning some value to the `random_state` parameter:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 原因是随机化：为了训练一棵树，我们随机选择一个特征子集。为了使结果一致，我们需要通过将某个值分配给`random_state`参数来固定随机数生成器的种子：
- en: '[PRE40]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Now we can evaluate it:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以评估它：
- en: '[PRE41]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: This time, we get an AUC of 78%. This score doesn’t change, no matter how many
    times we retrain the model.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们得到了78%的AUC。无论我们重新训练模型多少次，这个分数都不会改变。
- en: The number of trees in the ensemble is an important parameter, and it influences
    the performance of the model. Usually, a model with more trees is better than
    a model with fewer trees. On the other hand, adding too many trees is not always
    helpful.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 集成中的树的数量是一个重要的参数，它影响模型的性能。通常，具有更多树的模型比具有较少树的模型更好。另一方面，添加过多的树并不总是有帮助。
- en: 'To see how many trees we need, we can iterate over different values for `n_estimators`
    and see its effect on AUC:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 为了看到我们需要多少棵树，我们可以遍历不同的`n_estimators`值，并查看其对AUC的影响：
- en: '[PRE42]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Creates a list with AUC results
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个包含AUC结果的列表
- en: ❷ Trains progressively more trees in each iteration
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在每次迭代中逐步训练更多的树
- en: ❸ Evaluates the score
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 评估分数
- en: ❹ Adds the score to the list with other scores
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将分数添加到其他分数的列表中
- en: 'In this code, we try different numbers of trees: from 10 to 200, going by steps
    of 10 (10, 20, 30, ...). Each time we train a model, we calculate its AUC on the
    validation set and record it.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 在这段代码中，我们尝试不同的树的数量：从10到200，以10为步长（10，20，30，...）。每次我们训练一个模型，我们都在验证集上计算其AUC，并记录下来。
- en: 'After we finish, we can plot the results:'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 完成后，我们可以绘制结果：
- en: '[PRE43]'
  id: totrans-343
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: In figure 6.27, we can see the results.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.27中，我们可以看到结果。
- en: '![](../Images/06-27.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-27.png)'
- en: Figure 6.27 The performance of the random forest model with different values
    for the `n_estimators` parameter
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 不同`n_estimators`参数值的随机森林模型性能
- en: 'The performance grows rapidly for the first 25–30 trees; then the growth slows
    down. After 130, adding more trees is not helpful anymore: the performance stays
    approximately at the level of 82%.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 性能在前25-30棵树时迅速增长；然后增长放缓。在130棵树之后，添加更多的树不再有帮助：性能大约保持在82%的水平。
- en: The number of trees is not the only parameter we can change to get better performance.
    Next, we see which other parameters we should also tune to improve the model.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 树的数量并不是我们为了获得更好的性能可以更改的唯一参数。接下来，我们看看还有哪些其他参数我们应该调整以改进模型。
- en: 6.3.2 Parameter tuning for random forest
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 随机森林的参数调整
- en: 'A random forest ensemble consists of multiple decision trees, so the most important
    parameters we need to tune for random forest are the same:'
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林集成由多个决策树组成，因此我们需要调整随机森林的最重要参数是相同的：
- en: '`max_depth`'
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`'
- en: '`min_leaf_size`'
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_leaf_size`'
- en: We can change other parameters, but we won’t cover them in detail in this chapter.
    Refer to the official documentation for more information ([https://scikit-learn.org/
    stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以更改其他参数，但在此章节中不会详细讨论。有关更多信息，请参阅官方文档（[https://scikit-learn.org/ stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html](https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)）。
- en: 'Let’s start with `max_depth`. We already know that this parameter significantly
    affects the performance of a decision tree. This is also the case for random forest:
    larger trees tend to overfit more than smaller trees.'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`max_depth`开始。我们已经知道这个参数显著影响决策树的表现。对于随机森林也是如此：较大的树比较小的树更容易过拟合。
- en: 'Let’s test a few values for `max_depth` and see how AUC evolves as the number
    of trees grows:'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们测试几个`max_depth`的值，看看随着树的数量增长，AUC如何演变：
- en: '[PRE44]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Creates a dictionary with AUC results
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个包含AUC结果的字典
- en: ❷ Iterates over different depth values
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历不同的深度值
- en: ❸ Creates a list with AUC results for the current depth level
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为当前深度级别创建一个包含AUC结果的列表
- en: ❹ Iterates over different n_estimator values
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 遍历不同的n_estimator值
- en: ❺ Evaluates the model
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 评估模型
- en: ❻ Save the AUCs for the current depth level in the dictionary
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将当前深度级别的AUC保存到字典中
- en: 'Now for each value of `max_depth`, we have a series of AUC scores. We can plot
    them now:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 现在对于每个`max_depth`的值，我们有一系列AUC分数。我们可以现在绘制它们：
- en: '[PRE45]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In figure 6.28 we see the result.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.28中我们看到结果。
- en: '![](../Images/06-28.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-28.png)'
- en: Figure 6.28 The performance of random forest with different values of the `max_depth`
    parameter
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28 不同`max_depth`参数值的随机森林性能
- en: With `max_depth=10`, AUC goes over 82%, whereas for other values it performs
    worse.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 当`max_depth=10`时，AUC超过82%，而对于其他值，表现较差。
- en: 'Now let’s tune `min_samples_leaf`. We set the value for the `max_depth` parameter
    from the previous step and then follow the same approach as previously for determining
    the best value for `min_samples_leaf`:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调整`min_samples_leaf`。我们将上一步中`max_depth`参数的值设置好，然后按照之前的方法确定`min_samples_leaf`的最佳值：
- en: '[PRE46]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Let’s plot it:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制它：
- en: '[PRE47]'
  id: totrans-372
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Then review the results (figure 6.29).
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 然后回顾结果（图6.29）。
- en: '![](../Images/06-29.png)'
  id: totrans-374
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-29.png)'
- en: Figure 6.29 The performance of random forest with different values of `min_samples_leaf`
    (with `max_depth=10`)
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29 不同`min_samples_leaf`值（`max_depth=10`）的随机森林性能
- en: We see that AUC is slightly better for small values of `min_samples_leaf` and
    the best value is 5.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到对于小的`min_samples_leaf`值，AUC略好，最佳值为5。
- en: Thus, the best parameters for random forest for our problem are
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，对于我们的问题，随机森林的最佳参数是
- en: '`max_depth=10`'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth=10`'
- en: '`min_samples_leaf=5`'
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_samples_leaf=5`'
- en: We achieved the best AUC with 200 trees, so we should set the `n_estimators`
    parameter to 200.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用200棵树实现了最佳的AUC，因此我们应该将`n_estimators`参数设置为200。
- en: 'Let’s train the final model:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练最终的模型：
- en: '[PRE48]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'Random forest is not the only way to combine multiple decision trees. There’s
    a different approach: gradient boosting. We cover that next.'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林并不是结合多个决策树的唯一方法。还有另一种方法：梯度提升。我们将在下一节中介绍。
- en: Exercise 6.2
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.2
- en: To make an ensemble useful, trees in a random forest should be different from
    each other. This is done by
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使集成有用，随机森林中的树应该彼此不同。这是通过以下方式实现的：
- en: a) Selecting different parameters for each individual tree
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: a) 为每棵单独的树选择不同的参数
- en: b) Randomly selecting a different subset of features for each tree
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: b) 为每棵树随机选择不同的特征子集
- en: c) Randomly selecting values for splitting
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: c) 随机选择分割值
- en: 6.4 Gradient boosting
  id: totrans-389
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 梯度提升
- en: 'In a random forest, each tree is independent: it’s trained on a different set
    of features. After individual trees are trained, we combine all their decisions
    together to get the final decision.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机森林中，每棵树都是独立的：它是在不同的特征集上训练的。在单独的树训练完成后，我们将它们的决策组合起来以获得最终的决策。
- en: 'It’s not the only way to combine multiple models together in one ensemble,
    however. Alternatively, we can train models sequentially—each next model tries
    to fix errors from the previous one:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这并不是将多个模型组合在一起的一种唯一方法。另一种方法是按顺序训练模型——每个后续模型都试图纠正前一个模型的错误：
- en: Train the first model.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练第一个模型。
- en: Look at the errors it makes.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看它所犯的错误。
- en: Train another model that fixes these errors.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练另一个模型来纠正这些错误。
- en: Look at the errors again; repeat sequentially.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 再次查看错误；按顺序重复。
- en: This way of combining models is called *boosting*. *Gradient boosting* is a
    particular variation of this approach that works especially well with trees (figure 6.30).
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 这种组合模型的方式被称为*提升*。*梯度提升*是这种方法的特定变体，它与树特别有效（见图6.30）。
- en: '![](../Images/06-30.png)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-30.png)'
- en: Figure 6.30 In gradient boosting, we train the models sequentially, and each
    next tree fixes the errors of the previous one.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.30 在梯度提升中，我们按顺序训练模型，每个后续的树都纠正前一个树的错误。
- en: Let’s have a look at how we can use it for solving our problem.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何用它来解决我们的问题。
- en: '6.4.1 XGBoost: Extreme gradient boosting'
  id: totrans-400
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 XGBoost：极端梯度提升
- en: 'We have many good implementations of the gradient boosting model: `GradientBoostingClassifier`
    from Scikit-learn, XGBoost, LightGBM and CatBoost. In this chapter, we use XGBoost
    (short for “Extreme Gradient Boosting”), which is the most popular implementation.'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有许多优秀的梯度提升模型实现：Scikit-learn的`GradientBoostingClassifier`，XGBoost，LightGBM和CatBoost。在本章中，我们使用XGBoost（代表“Extreme
    Gradient Boosting”），这是最受欢迎的实现。
- en: 'XGBoost doesn’t come with Anaconda, so to use it, we need to install it. The
    easiest way is to install it with `pip`:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: XGBoost不包含在Anaconda中，因此要使用它，我们需要安装它。最简单的方法是使用`pip`安装：
- en: '[PRE49]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, open the notebook with our project and import it:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开我们的项目笔记本并导入它：
- en: '[PRE50]'
  id: totrans-405
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note In some cases, importing XGBoost may give you a warning like `YMLLoadWarning`.
    You shouldn’t worry about it; the library will work without problems.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在某些情况下，导入XGBoost可能会给出类似于`YMLLoadWarning`的警告。你不必担心这个问题；库将无问题地工作。
- en: Using the alias `xgb` when importing XGBoost is a convention, just like with
    other popular machine learning packages in Python.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 在导入XGBoost时使用别名`xgb`是一种约定，就像在其他流行的Python机器学习包中一样。
- en: 'Before we can train an XGBoost model, we need to wrap our data into `DMatrix`—a
    special data structure for finding splits efficiently. Let’s do it:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们能够训练XGBoost模型之前，我们需要将我们的数据包装成`DMatrix`——一种用于高效查找分割的特殊数据结构。让我们来做这件事：
- en: '[PRE51]'
  id: totrans-409
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'When creating an instance of `DMatrix`, we pass three parameters:'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个`DMatrix`实例时，我们传递三个参数：
- en: '`X_train`: the feature matrix'
  id: totrans-411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`X_train`：特征矩阵'
- en: '`y_train`: the target variable'
  id: totrans-412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`y_train`：目标变量'
- en: '`feature_names`: the names of features in `X_train`'
  id: totrans-413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`feature_names`：`X_train`中特征的名字'
- en: 'Let’s do the same for the validation dataset:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对验证数据集也做同样的事情：
- en: '[PRE52]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The next step is specifying the parameters for training. We’re using only a
    small subset of the default parameters of XGBoost (check the official documentation
    for the entire list of parameter: [https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)):'
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是指定训练参数。我们只使用XGBoost默认参数的一小部分（查看官方文档以获取参数完整列表：[https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)）：
- en: '[PRE53]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'For us, the most important parameter now is `objective`: it specifies the learning
    task. We’re solving a binary classification problem—that’s why we need to choose
    `binary :logistic`. We cover the rest of these parameters later in this section.'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们来说，现在最重要的参数是`objective`：它指定了学习任务。我们正在解决一个二元分类问题——这就是为什么我们需要选择`binary :logistic`。我们将在本节的后面部分介绍这些参数的其余部分。
- en: 'For training an XGBoost model, we use the `train` function. Let’s start with
    10 trees:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个XGBoost模型，我们使用`train`函数。让我们从10棵树开始：
- en: '[PRE54]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We provide three arguments to `train`:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们向`train`函数提供了三个参数：
- en: '`xgb_params`: the parameters for training'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`xgb_params`：训练参数'
- en: '`dtrain`: the dataset for training (an instance of `DMatrix`)'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`dtrain`：训练数据集（`DMatrix`的一个实例）'
- en: '`num_boost_round=10`: the number of trees to train'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`num_boost_round=10`：要训练的树的数量'
- en: 'After a few seconds, we get a model. To evaluate it, we need to make a prediction
    on the validation dataset. For that, use the `predict` method with the validation
    data wrapped in `DMatrix`:'
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 几秒钟后，我们得到一个模型。为了评估它，我们需要在验证数据集上进行预测。为此，使用`predict`方法，并将验证数据包装在`DMatrix`中：
- en: '[PRE55]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'The result, `y_pred`, is a one-dimensional NumPy array with predictions: the
    risk score for each customer in the validation dataset (figure 6.31).'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 结果`y_pred`是一个一维NumPy数组，包含预测：验证数据集中每个客户的预测风险分数（图6.31）。
- en: '![](../Images/06-31.png)'
  id: totrans-428
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-31.png)'
- en: Figure 6.31 The predictions of XGBoost
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.31 XGBoost的预测
- en: 'Next, we calculate AUC using the same approach as previously:'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用与之前相同的方法计算AUC：
- en: '[PRE56]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: After executing it, we get 81.5%. This is quite a good result, but it’s still
    slightly worse than our best random forest model (82.5%).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，我们得到81.5%。这是一个相当好的结果，但仍然略逊于我们最好的随机森林模型（82.5%）。
- en: Training an XGBoost model is simpler when we can see how its performance changes
    when the number of trees grows. We see how to do it next.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们可以看到随着树的数量增长，模型性能如何变化时，训练XGBoost模型会更简单。我们将在下一部分看到如何做到这一点。
- en: 6.4.2 Model performance monitoring
  id: totrans-434
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 模型性能监控
- en: To get an idea of how AUC changes as the number of trees grows, we can use a
    watchlist—a built-in feature in XGBoost for monitoring model performance.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解随着树的数量增长，AUC如何变化，我们可以使用watchlist——XGBoost内置的用于监控模型性能的功能。
- en: 'A watchlist is a Python list with tuples. Each tuple contains a DMatrix and
    its name. This is how we typically do it:'
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: watchlist是一个包含元组的Python列表。每个元组包含一个DMatrix及其名称。我们通常这样做：
- en: '[PRE57]'
  id: totrans-437
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Additionally, we modify the list of parameters for training: we need to specify
    the metric we use for evaluation. In our case, it’s the AUC:'
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们修改了训练参数列表：我们需要指定用于评估的指标。在我们的例子中，它是AUC：
- en: '[PRE58]'
  id: totrans-439
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: ❶ Sets the evaluation metric to the AUC
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将评估指标设置为AUC
- en: 'To use the watchlist during training, we need to specify two extra arguments
    for the `train` function:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练期间使用watchlist，我们需要为`train`函数指定两个额外的参数：
- en: '`evals`: the watchlist.'
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`evals`: watchlist。'
- en: '`verbose_eval`: how often we print the metric. If we set it to “10,” we see
    the result after each 10th step.'
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`verbose_eval`: 打印指标的频率。如果我们将其设置为“10”，则每10步后我们会看到结果。'
- en: 'Let’s train it:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来训练它：
- en: '[PRE59]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'While training, XGBoost prints the scores to the output:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，XGBoost将分数打印到输出：
- en: '[PRE60]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: As the number of trees grows, the score on the training set goes up (figure 6.32).
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 随着树的数量增长，训练集上的分数上升（图6.32）。
- en: '![](../Images/06-32.png)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-32.png)'
- en: Figure 6.32 The effect of the number of trees on the AUC from train and validation
    sets. To see how to plot these values, check the notebook in the book’s GitHub
    repository.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.32 树的数量对训练集和验证集AUC的影响。要了解如何绘制这些值，请查看书中GitHub仓库中的笔记本。
- en: 'This behavior is expected: in boosting, every next model tries to fix the mistakes
    from the previous step, so the score is always improving.'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 这种行为是预期的：在提升中，每个后续模型都试图纠正前一个步骤中的错误，因此分数总是提高的。
- en: 'For the validation score, however, this is not the case. It goes up initially
    but then starts to decrease. This is the effect of overfitting: our model becomes
    more and more complex until it simply memorizes the entire training set. It’s
    not helpful for predicting the outcome for the customers outside of the training
    set, and the validation score reflects that.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，对于验证分数来说，情况并非如此。它最初会上升，但随后开始下降。这是过拟合的影响：我们的模型变得越来越复杂，直到它简单地记住整个训练集。这对预测训练集之外的客户结果没有帮助，验证分数反映了这一点。
- en: We get the best AUC on the 30th iteration (81.7%), but it’s not so different
    from the score we got on the 10th iteration (81.5%).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在第30次迭代时获得了最佳的AUC（81.7%），但这与第10次迭代获得的分数（81.5%）并没有太大区别。
- en: Next, we’ll see how to get the best out of XGBoost by tuning its parameters.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看到如何通过调整参数来获得XGBoost的最佳性能。
- en: 6.4.3 Parameter tuning for XGBoost
  id: totrans-455
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.3 XGBoost的参数调整
- en: 'Previously, we used a subset of default parameters for training a model:'
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 此前，我们使用默认参数的子集来训练模型：
- en: '[PRE61]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'We’re mostly interested in the first three parameters. These parameters control
    the training process:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们主要对前三个参数感兴趣。这些参数控制训练过程：
- en: '`eta`: Learning rate. Decision trees and random forest don’t have this parameter.
    We cover it later in this section when we tune it.'
  id: totrans-459
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eta`: 学习率。决策树和随机森林没有这个参数。我们将在本节稍后调整它时介绍。'
- en: '`max_depth`: The maximum allowed depth of each tree; the same as `max_depth`
    in `DecisionTreeClassifier` from Scikit-learn.'
  id: totrans-460
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`max_depth`: 每棵树允许的最大深度；与Scikit-learn中的`DecisionTreeClassifier`的`max_depth`相同。'
- en: '`min_child_weight`: The minimal number of observations in each group; the same
    as `min_leaf_size` in `DecisionTreeClassifier` from Scikit-learn.'
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`min_child_weight`: 每个组中最小观察数；与Scikit-learn中的`DecisionTreeClassifier`的`min_leaf_size`相同。'
- en: 'Other parameters:'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 其他参数：
- en: '`objective`: The type of task we want to solve. For classification, it should
    be `binary:logistic`.'
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`objective`：我们想要解决的问题的类型。对于分类，它应该是`binary:logistic`。'
- en: '`eval_metric`: The metric we use for evaluation. For this project, it’s “AUC.”'
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eval_metric`：我们用于评估的指标。对于这个项目，它是“AUC”。'
- en: '`nthread`: The number of threads we use for training the model. XGBoost is
    very good at parallelizing training, so set it to the number of cores your computer
    has.'
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`nthread`：我们用于训练模型的线程数。XGBoost在并行化训练方面非常出色，所以将其设置为计算机的核心数。'
- en: '`seed`: The seed for the random-number generator; we need to set it to make
    sure the results are reproducible.'
  id: totrans-466
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`seed`：随机数生成器的种子；我们需要设置它以确保结果可重复。'
- en: '`silent`: The verbosity of the output. When we set it to “1,” it outputs only
    warnings.'
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`silent`：输出的详细程度。当我们将其设置为“1”时，它只输出警告。'
- en: This is not the full list of parameters, only the basic ones. You can learn
    more about all the parameters in the official documentation ([https://xgboost.readthedocs.io/en/
    latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 这不是参数的完整列表，只是基本参数。你可以在官方文档中了解更多关于所有参数的信息（[https://xgboost.readthedocs.io/en/latest/parameter.html](https://xgboost.readthedocs.io/en/latest/parameter.html)）。
- en: We already know `max_depth` and `min_child_weight` (`min_leaf_size`), but we
    haven't previously come across `eta`—the learning rate parameter. Let’s talk about
    it and see how we can optimize it.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道`max_depth`和`min_child_weight`（`min_leaf_size`），但我们之前没有遇到过`eta`——学习率参数。让我们来谈谈它，看看我们如何可以优化它。
- en: Learning rate
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率
- en: In boosting, each tree tries to correct the mistakes from the previous iterations.
    Learning rate determines the weight of this correction. If we have a large value
    for `eta`, the correction overweights the previous predictions significantly.
    On the other hand, if the value is small, only a small fraction of this correction
    is used.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在提升中，每一棵树都试图纠正前一次迭代的错误。学习率决定了这种纠正的权重。如果我们有一个大的`eta`值，纠正会显著地超过之前的预测。另一方面，如果值较小，只有一小部分这种纠正被使用。
- en: In practice it means
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，这意味着
- en: If `eta` is too large, the model starts to overfit quite early without realizing
    its full potential.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果`eta`太大，模型会过早地开始过拟合，而没有意识到其全部潜力。
- en: If it’s too small, we need to train too many trees before it can produce good
    results.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果它太小，我们需要训练很多树才能产生好的结果。
- en: The default value of 0.3 is reasonably good for large datasets, but for smaller
    datasets like ours, we should try smaller values like 0.1 or even 0.05.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 默认值0.3对于大数据集来说相当合理，但对我们这样的小数据集，我们应该尝试更小的值，如0.1甚至0.05。
- en: 'Let’s do it and see if it helps to improve the performance:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们试试看是否有助于提高性能：
- en: '[PRE62]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ❶ Changes eta from 0.3 to 0.1
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将`eta`从0.3改为0.1
- en: 'Because now we can use a watchlist to monitor the performance of our model,
    we can train for as many iterations as we want. Previously we used 100 iterations,
    but this may be not enough for smaller `eta`. So let’s use 500 rounds for training:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 因为现在我们可以使用watchlist来监控我们模型的性能，我们可以训练尽可能多的迭代次数。之前我们使用了100次迭代，但这可能对于较小的`eta`来说不够。所以让我们用500轮进行训练：
- en: '[PRE63]'
  id: totrans-480
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'When running it, we see that the best validation score is 82.4%:'
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行时，我们看到最佳的验证分数是82.4%：
- en: '[PRE64]'
  id: totrans-482
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Previously, we could achieve AUC of 81.7% when `eta` was set to the default
    value of 0.3\. Let’s compare these two models (figure 6.33).
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，当`eta`设置为默认值0.3时，我们能够达到81.7%的AUC。让我们比较这两个模型（图6.33）。
- en: '![](../Images/06-33.png)'
  id: totrans-484
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-33.png)'
- en: Figure 6.33 The effect of the `eta` parameter on the validation score
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.33 `eta`参数对验证分数的影响
- en: When `eta` is 0.3, we get the best AUC pretty quickly, but then it starts to
    overfit. After the 30th iteration, the performance on the validation set goes
    down.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 当`eta`为0.3时，我们很快就能得到最佳的AUC，但随后开始过拟合。在第30次迭代后，验证集上的性能下降。
- en: When `eta` is 0.1, AUC grows more slowly but peaks at a higher value. For a
    smaller learning rate, it takes more trees to reach the peak, but we could achieve
    better performance.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 当`eta`为0.1时，AUC增长较慢，但峰值更高。对于较小的学习率，需要更多的树才能达到峰值，但我们可以实现更好的性能。
- en: 'For comparison, we can also try other values of `eta` (figure 6.34):'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 为了比较，我们还可以尝试其他`eta`的值（图6.34）：
- en: For 0.05, the best AUC is 82.2% (after 120 iterations).
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于0.05，最佳的AUC为82.2%（经过120次迭代）。
- en: For 0.01, the best AUC is 82.1% (after 500 iterations).
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于0.01，最佳的AUC是82.1%（经过500次迭代）。
- en: '![](../Images/06-34.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/06-34.png)'
- en: Figure 6.34 The model requires more trees when `eta` is small.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.34 当`eta`较小时，模型需要更多的树。
- en: When `eta` is 0.05, the performance is similar to 0.1, but it takes 60 more
    iterations to reach the peak.
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 当`eta`为0.05时，性能与0.1相似，但需要多60次迭代才能达到峰值。
- en: 'For `eta` of 0.01, it grows too slowly, and even after 500 iterations, it hasn’t
    reached the peak. If we tried it for more iterations, it could potentially get
    to the same level of AUC as other values. Even if it was the case, it’s not practical:
    it becomes computationally expensive to evaluate all these trees during prediction
    time.'
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 对于`eta`为0.01，它增长得太慢，即使经过500次迭代，也没有达到峰值。如果我们尝试更多的迭代，它可能达到与其他值相同的AUC水平。即使如此，这也不实用：在预测时间评估所有这些树变得计算成本高昂。
- en: Thus, we use the value of 0.1 for `eta`. Next, let’s tune other parameters.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用`eta`的值为0.1。接下来，让我们调整其他参数。
- en: Exercise 6.3
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 练习6.3
- en: We have a gradient boosting model with `eta=0.1`. It needs 60 trees to get the
    peak performance. If we increase `eta` to 0.5, what will happen?
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个`eta=0.1`的梯度提升模型。它需要60棵树来达到最佳性能。如果我们把`eta`增加到0.5，会发生什么？
- en: a) The number of trees will not change.
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: a) 树的数量不会改变。
- en: b) The model will need more trees to reach its peak performance.
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: b) 模型需要更多的树来达到其最佳性能。
- en: c) The model will need fewer trees to reach its peak performance.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: c) 模型需要更少的树来达到其最佳性能。
- en: Tuning other parameters
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 调整其他参数
- en: The next parameter we tune is `max_depth`. The default value is 6, so we can
    try
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个调整的参数是`max_depth`。默认值是6，所以我们可以尝试
- en: A lower value; for example, 3
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较低的值；例如，3
- en: A higher value; for example, 10
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 较高的值；例如，10
- en: The outcome should give us an idea if the best value for `max_depth` is between
    3 and 6 or between 6 and 10.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 这个结果应该能让我们知道最佳`max_depth`值是在3到6之间还是6到10之间。
- en: 'First, check 3:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，检查3：
- en: '[PRE65]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: ❶ Changes max_depth from 6 to 3
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将max_depth从6改为3
- en: The best AUC we get with it is 83.6%.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 使用它我们能得到的最佳AUC是83.6%。
- en: Next, try 10\. In this case, the best value is 81.1%.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，尝试10。在这种情况下，最佳值是81.1%。
- en: This means that the optimal parameter of `max_depth` should be between 3 and
    6\. When we try 4, however, we see that the best AUC is 83%, which is slightly
    worse than the AUC we got with the depth of 3 (figure 6.35).
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着`max_depth`的最佳参数值应该在3到6之间。然而，当我们尝试4时，我们发现最佳AUC是83%，这略低于3深度的AUC（图6.35）。
- en: '![](../Images/06-35.png)'
  id: totrans-512
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-35.png)'
- en: 'Figure 6.35 The optimal value for `max_depth` is 4: with it, we can achieve
    an AUC of 83.6%.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.35 `max_depth`的最佳值是4：使用它，我们可以达到83.6%的AUC。
- en: 'The next parameter we tune is `min_child_weight`. It’s the same as `min_leaf_size`
    in decision trees from Scikit-learn: it controls the minimal number of observations
    a tree can have in a leaf.'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个调整的参数是`min_child_weight`。它与Scikit-learn中的决策树中的`min_leaf_size`相同：它控制树在叶子节点中的最小观测数。
- en: Let’s try a range of values and see which one works best. In addition to the
    default value (1), we can try 10 and 30 (figure 6.36).
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试一系列值，看看哪个效果最好。除了默认值（1）外，我们还可以尝试10和30（图6.36）。
- en: '![](../Images/06-36.png)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-36.png)'
- en: Figure 6.36 The optimal value for `min_child_weight` is 1, but it’s not drastically
    different from other values for this parameter.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.36 `min_child_weight`的最佳值是1，但与其他参数值相比并没有显著不同。
- en: From figure 6.36 we see that
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 从图6.36中我们可以看到
- en: For `min_child_weight=1`, AUC is 83.6%.
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`min_child_weight=1`，AUC为83.6%。
- en: For `min_child_weight=10`, AUC is 83.3%.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`min_child_weight=10`，AUC为83.3%。
- en: For `min_child_weight=30`, AUC is 83.5%.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于`min_child_weight=30`，AUC为83.5%。
- en: The difference between these options is not significant, so we’ll leave the
    default value.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项之间的差异并不显著，所以我们将保留默认值。
- en: The parameters for our final model are
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最终模型的参数是
- en: '[PRE66]'
  id: totrans-524
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'We need to do one last step before we can finish the model: we need to select
    the optimal number of trees. It’s quite simple: look at the iteration when the
    validation score peaked and use this number.'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以完成模型之前，我们需要做最后一步：我们需要选择最佳树的数量。这很简单：查看验证分数达到峰值时的迭代次数，并使用这个数字。
- en: 'In our case, we need to train 180 trees for the final model (figure 6.37):'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，我们需要为最终模型训练180棵树（图6.37）：
- en: '[PRE67]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '![](../Images/06-37.png)'
  id: totrans-528
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/06-37.png)'
- en: Figure 6.37 The optimal number of trees for the final model is 180.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.37 最终模型的最佳树数量是180。
- en: The best the random forest model was able to get 82.5% AUC, whereas the best
    the gradient boosting model could get was 1% more (83.6%).
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林模型能得到的最佳AUC是82.5%，而梯度提升模型能得到的最佳AUC是1%更多（83.6%）。
- en: This is the best model, so let’s use it as our final model—and we should use
    it for scoring loan applications.
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 这是最好的模型，所以让我们将其作为我们的最终模型——并且我们应该用它来评分贷款申请。
- en: 6.4.4 Testing the final model
  id: totrans-532
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.4 测试最终模型
- en: 'We’re almost ready to use it for risk scoring. We still need to do two things
    before we can use it:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 我们几乎准备好使用它来进行风险评估。在我们能够使用它之前，我们还需要做两件事：
- en: Retrain the final model on both train and validation datasets combined. We no
    longer need the validation dataset, so we can use more data for training, which
    will make the model slightly better.
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在结合了训练集和验证集的数据集上重新训练最终模型。我们不再需要验证集，因此我们可以使用更多的数据进行训练，这将使模型略微更好。
- en: Test the model on the test set. This is the part of data we kept aside from
    the beginning. Now we use it to make sure the model didn’t overfit and performs
    well on completely unseen data.
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在测试集上测试模型。这是我们一开始就保留的数据的一部分。现在我们使用它来确保模型没有过拟合，并且能够在完全未见过的数据上表现良好。
- en: 'The next steps are:'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是：
- en: Apply the same preprocessing to `df_full_train` and `df_test` as we did to `df_train`
    and `df_val`. As a result, we get the feature matrices `X_train` and `X_test`
    as well as our target variables `y_train` and `y_test`.
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将与`df_train`和`df_val`相同的预处理应用于`df_full_train`和`df_test`。结果，我们得到了特征矩阵`X_train`和`X_test`以及我们的目标变量`y_train`和`y_test`。
- en: Train a model on the combined dataset with the parameters we selected previously.
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用我们之前选择的参数在组合数据集上训练模型。
- en: Apply the model to the test data to get the test predictions.
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型应用于测试数据以获取测试预测。
- en: Verify that the model performs well and doesn’t overfit.
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确认模型表现良好且没有过拟合。
- en: 'Let’s do it. First, create the target variable:'
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧。首先，创建目标变量：
- en: '[PRE68]'
  id: totrans-542
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Because we use the entire DataFrame for creating the feature matrix, we need
    to remote the target variable:'
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们使用整个DataFrame来创建特征矩阵，所以我们需要移除目标变量：
- en: '[PRE69]'
  id: totrans-544
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Next, we convert DataFrames into lists of dictionaries and then use one-hot
    encoding to get the feature matrices:'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将DataFrame转换为字典列表，然后使用独热编码来获取特征矩阵：
- en: '[PRE70]'
  id: totrans-546
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'Finally, we train the XGBoost model using this data and the optimal parameters
    we determined previously:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用之前确定的最优参数和这些数据来训练XGBoost模型：
- en: '[PRE71]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Then evaluate its performance on the test set:'
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 然后在测试集上评估其性能：
- en: '[PRE72]'
  id: totrans-550
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The output is 83.2%, which is comparable to 83.6%—the performance on the validation
    set. It means that our model doesn’t overfit and can work well with customers
    it hasn’t seen.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 输出是83.2%，这与验证集上的83.6%的性能相当。这意味着我们的模型没有过拟合，并且可以很好地与它未见过的客户一起工作。
- en: Exercise 6.4
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 6.4
- en: The main difference between random forest and gradient boosting is
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和梯度提升之间的主要区别是
- en: a) Trees in gradient boosting are trained sequentially, and each next tree improves
    the previous one. In a random forest, all trees are trained independently.
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: a) 在梯度提升中，树是按顺序训练的，并且每一棵后续的树都会改进前一棵树。在随机森林中，所有树都是独立训练的。
- en: b) Gradient boosting is a lot faster than using a random forest.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: b) 梯度提升比使用随机森林要快得多。
- en: c) Trees in a random forest are trained sequentially, and each next tree improves
    the previous one. In gradient boosting, all trees are trained independently.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: c) 在随机森林中，树是按顺序训练的，并且每一棵后续的树都会改进前一棵树。在梯度提升中，所有树都是独立训练的。
- en: 6.5 Next steps
  id: totrans-557
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5 下一步
- en: We’ve learned the basics about decision trees, random forest, and gradient boosting.
    We’ve learned a lot, but there’s much more than we could fit in this chapter.
    You can explore this topic further by doing the exercises.
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了关于决策树、随机森林和梯度提升的基础知识。我们学到了很多，但在这个章节中我们所能涵盖的只是冰山一角。你可以通过做练习来进一步探索这个主题。
- en: 6.5.1 Exercises
  id: totrans-559
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 练习
- en: Feature engineering is the process of creating new features out of existing
    ones. For this project, we haven’t created any features; we simply used the ones
    provided in the dataset. Adding more features should help improve the performance
    of our model. For example, we can add the ratio of requested money to the total
    price of the item. Experiment with engineering more features.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程是从现有特征中创建新特征的过程。对于这个项目，我们没有创建任何特征；我们只是使用了数据集中提供的那些。添加更多特征应该有助于提高我们模型的性能。例如，我们可以添加请求金额与物品总价的比率。尝试通过工程更多特征进行实验。
- en: When training a random forest, we get different models by selecting a random
    subset of features for each tree. To control the size of the subset, we use the
    `max_features` parameter. Try adjusting this parameter, and see if it changes
    the AUC on validation.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练随机森林时，我们通过为每棵树选择特征的一个随机子集来得到不同的模型。为了控制子集的大小，我们使用`max_features`参数。尝试调整此参数，看看它是否会影响验证集上的AUC。
- en: 'Extreme randomized trees (or extra trees, for short) is a variation of a random
    forest where the idea of randomization is taken to the extreme. Instead of finding
    the best possible split, it picks a splitting condition randomly. This approach
    has a few advantages: extra trees are faster to train, and they are less prone
    to overfitting. On the other hand, they require more trees to have adequate performance.
    In Scikit-learn, `ExtraTreesClassifier` from the `ensemble` package implements
    it. Experiment with it for this project.'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 极端随机树（或简称extra trees）是随机森林的一种变体，其中随机化的想法被推向了极致。它不是寻找最佳可能的分割点，而是随机选择分割条件。这种方法有几个优点：extra
    trees训练更快，并且它们不太容易过拟合。另一方面，它们需要更多的树才能达到足够的性能。在Scikit-learn中，`ExtraTreesClassifier`来自`ensemble`包实现了它。在这个项目中尝试它。
- en: 'In XGBoost, the `colsample_bytree` parameter controls the number of features
    we select for each tree—it’s similar to `max_features` from the random forest.
    Experiment with this parameter, and see if it improves the performance: try values
    from 0.1 to 1.0 with a step of 0.1\. Usually the optimal values are between 0.6
    and 0.8, but sometimes 1.0 gives the best result.'
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在XGBoost中，`colsample_bytree`参数控制我们为每棵树选择的特征数量——它与随机森林中的`max_features`类似。尝试调整此参数，看看是否可以提高性能：尝试从0.1到1.0的值，步长为0.1。通常最佳值在0.6到0.8之间，但有时1.0会给出最佳结果。
- en: In addition to randomly selecting columns (features), we can also select a subset
    of rows (customers). This is called *subsampling*, and it helps to prevent overfitting.
    In XGBoost, the `subsample` parameter controls the fraction of examples we select
    for training each tree in the ensemble. Try values from 0.4 to 1.0 with a step
    of 0.1\. Usually the optimal values are between 0.6 and 0.8.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了随机选择列（特征）之外，我们还可以选择行的一个子集（客户）。这被称为*子采样*，它有助于防止过拟合。在XGBoost中，`subsample`参数控制我们为每个集成树选择的示例比例。尝试从0.4到1.0的值，步长为0.1。通常最佳值在0.6到0.8之间。
- en: 6.5.2 Other projects
  id: totrans-565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 其他项目
- en: All tree-based models can solve the regression problem—predict a number. In
    Scikit-learn, DecisionTreeRegressor, and RandomForestRegressor, implement the
    regression variation of the models. In XGBoost, we need to change the objective
    to `reg:squarederror`. Use these models for predicting the price of the car, and
    try to solve other regression problems as well.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有基于树的模型都可以解决回归问题——预测一个数字。在Scikit-learn中，DecisionTreeRegressor和RandomForestRegressor实现了模型的回归变体。在XGBoost中，我们需要将目标更改为`reg:squarederror`。使用这些模型来预测汽车价格，并尝试解决其他回归问题。
- en: Summary
  id: totrans-567
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Decision tree is a model that represents a sequence of if-then-else decisions.
    It’s easy to understand, and it also performs quite well in practice.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树是一个表示一系列if-then-else决策的模型。它很容易理解，并且在实践中表现也相当好。
- en: We train decision trees by selecting the best split using impurity measures.
    The main parameters that we control are the depth of the tree and the maximum
    number of samples in each leaf.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们通过选择最佳分割点来训练决策树，使用不纯度度量。我们控制的主要参数是树的深度和每个叶子的最大样本数。
- en: A random forest is a way to combine many decision trees into one model. Like
    a team of experts, individual trees can make mistakes, but together, they are
    less likely to reach an incorrect decision.
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林是将许多决策树组合成一个模型的方法。就像一个专家团队一样，单个树可能会犯错，但在一起，它们不太可能做出错误的决策。
- en: A random forest should have a diverse set of models to make good predictions.
    That’s why each tree in the model uses a different set of features for training.
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林应该有一系列多样化的模型来进行良好的预测。这就是为什么模型中的每棵树都使用不同的特征集进行训练。
- en: 'The main parameters we need to change for random forest are the same as for
    decision trees: the depth and the maximum number of samples in each leaf. Additionally,
    we need to select the number of trees we want to have in the ensemble.'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要为随机森林更改的主要参数与决策树相同：树的深度和每个叶子的最大样本数。此外，我们还需要选择我们想要在集成中拥有的树的数量。
- en: While in a random forest the trees are independent, in gradient boosting, the
    trees are sequential, and each next model corrects the mistakes of the previous
    one. In some cases, this leads to better predictive performance.
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然在随机森林中树是独立的，但在梯度提升中，树是顺序的，每个后续模型都纠正前一个模型的错误。在某些情况下，这会导致更好的预测性能。
- en: 'The parameters we need to tune for gradient boosting are similar for a random
    forest: the depth, the maximum number of observations in the leaf, and the number
    of trees. In addition to that, we have `eta`—the learning rate. It specifies the
    contribution of each individual tree to the ensemble.'
  id: totrans-574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们需要调整的梯度提升参数与随机森林类似：树的深度、叶子节点中最大观测数以及树的数量。除此之外，我们还有`eta`——学习率。它指定了每棵树对集成模型的贡献。
- en: Tree-based models are easy to interpret and understand, and often they perform
    quite well. Gradient boosting is great and often achieves the best possible performance
    on structured data (data in tabular format).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的模型易于解释和理解，并且通常表现相当出色。梯度提升非常出色，通常在结构化数据（表格格式的数据）上实现最佳性能。
- en: 'In the next chapter, we look at neural nets: a different type of model, which,
    in contrast, achieves best performance on unstructured data, such as images.'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨神经网络：一种不同类型的模型，与之前相比，它在非结构化数据（如图像）上实现最佳性能。
- en: Answers to exercises
  id: totrans-577
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习题答案
- en: Exercise 6.1 A) With one more feature, training takes longer.
  id: totrans-578
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习题6.1 A) 添加一个更多特征后，训练时间会更长。
- en: Exercise 6.3 C) The model will need fewer trees to reach its peak performance.
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习题6.3 C) 该模型需要更少的树来达到其最佳性能。
- en: Exercise 6.2 B) Randomly selecting a different subset of features for each tree.
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习题6.2 B) 为每棵树随机选择不同的特征子集。
- en: Exercise 6.4 A) Trees in gradient boosting are trained sequentially. In a random
    forest, trees are trained independently.
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习题6.4 A) 梯度提升中的树是顺序训练的。在随机森林中，树是独立训练的。
