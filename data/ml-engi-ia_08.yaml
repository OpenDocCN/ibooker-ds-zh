- en: '8 Experimentation in action: Finalizing an MVP with MLflow and runtime optimization'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 实验行动：使用MLflow和运行时优化完成最小可行产品（MVP）
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Approaches, tools, and methods to version-control ML code, models, and experiment
    results
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 版本控制ML代码、模型和实验结果的方法、工具和技巧
- en: Scalable solutions for model training and inference
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型训练和推理的可扩展解决方案
- en: 'In the preceding chapter, we arrived at a solution to one of the most time-consuming
    and monotonous tasks that we face as ML practitioners: fine-tuning models. By
    having techniques to solve the tedious act of tuning, we can greatly reduce the
    risk of producing ML-backed solutions that are inaccurate to the point of being
    worthless. In the process of applying those techniques, however, we quietly welcomed
    an enormous elephant into the room of our project: tracking.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在前一章中，我们找到了解决作为机器学习从业者面临的最耗时和单调的任务之一的方法：微调模型。通过拥有解决繁琐调优的技术，我们可以大大降低产生不准确到毫无价值的机器学习解决方案的风险。然而，在应用这些技术的过程中，我们默默地欢迎了一个巨大的大象进入我们的项目房间：跟踪。
- en: Throughout the last several chapters, we have been required to retrain our time-series
    models each time that we do inference. For the vast majority of other supervised
    learning tasks, this won’t be the case. Those other applications of modeling,
    both supervised and unsupervised, will have periodic retraining events, between
    which each model will be called for inference (prediction) many times.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，我们每次进行推理时都需要重新训练我们的时间序列模型。对于绝大多数其他监督学习任务，情况并非如此。那些其他建模应用，无论是监督学习还是无监督学习，都将定期进行重新训练事件，在这些事件之间，每个模型将被多次调用进行推理（预测）。
- en: Regardless of whether we’ll have to retrain daily, weekly, or monthly (you really
    shouldn’t be letting a model go stale for longer than that), we will have versions
    of not only the final production model that will generate scoring metrics, but
    also the optimization history of automated tuning. Add to this volume of modeling
    information a wealth of statistical validation tests, metadata, artifacts, and
    run-specific data that is valuable for historical reference, and you have yourself
    a veritable mountain of critical data that needs to be recorded.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 无论我们是否需要每天、每周或每月重新训练（你真的不应该让模型闲置超过那么长时间），我们都会有最终生成评分指标的最终生产模型的版本，以及自动化调优的优化历史。将这一大量建模信息与丰富的统计验证测试、元数据、工件和特定运行数据相结合，这些数据对于历史参考非常有价值，你就拥有了一座需要记录的关键数据山。
- en: In this chapter, we’ll go through logging our tuning run data to MLflow’s tracking
    server, enabling us to have historical references to everything that we deem important
    to store about our project’s solution. Having this data available is valuable
    not merely for tuning and experimentation; it’s also critical for monitoring the
    long-term health of your solution. Having referenceable metrics and parameter
    search history over time helps inform ways to potentially make the solution better,
    and also gives insight into when the performance degrades to the point that you
    need to rebuild the solution.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍将我们的调优运行数据记录到MLflow的跟踪服务器中，使我们能够对存储关于项目解决方案的重要信息有历史参考。拥有这些数据不仅对于调优和实验有价值；对于监控解决方案的长期健康状况也至关重要。随着时间的推移，可参考的指标和参数搜索历史有助于了解如何可能使解决方案变得更好，同时也提供了洞察，当性能下降到需要重建解决方案的程度时。
- en: Note A companion Spark notebook provides examples of the points discussed in
    this chapter. See the accompanying GitHub repository for further details, if interested.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章节提供了一个配套的Spark笔记本，其中包含了本章讨论的要点示例。如需进一步详情，请参阅随附的GitHub仓库。
- en: '8.1 Logging: Code, metrics, and results'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 记录：代码、指标和结果
- en: Chapters 2 and 3 covered the critical importance of communication about modeling
    activities, both to the business and among a team of fellow data scientists. Being
    able to not only show our project solutions, but also have a provenance history
    for reference, is just as important to the project’s success, if not more so,
    than the algorithms used to solve it.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 第2章和第3章讨论了关于建模活动的沟通对于企业和数据科学家团队的重要性。不仅能够展示我们的项目解决方案，而且拥有一个可追溯的历史记录以供参考，这对于项目的成功至关重要，甚至比解决该问题的算法更为重要。
- en: For the forecasting project that we’ve been covering through the last few chapters,
    the ML aspect of the solution isn’t particularly complex, but the magnitude of
    the problem is. With thousands of airports to model (which, in turn, means thousands
    of models to tune and keep track of), handling communication and having a reference
    for historical data for each execution of the project code is a daunting task.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们在过去几章中一直在讨论的预测项目，解决方案的机器学习方面并不特别复杂，但问题的规模却是巨大的。要为成千上万的机场建模（这反过来意味着需要调整和跟踪成千上万的模型），处理通信并为项目代码的每次执行提供历史数据的参考是一项艰巨的任务。
- en: What happens when, after running our forecasting project in production, a member
    of the business unit team wants an explanation as to why a particular forecast
    was so far off from the eventual reality of the data that is collected? This is
    a common question from many companies that rely on ML predictions to inform the
    business about actions that should be taken in running the business. The very
    last thing that you would want to have to deal with if a black swan event occurs
    and the business is asking questions about why the modeled forecast solution didn’t
    foresee it, is having to try to regenerate what the model might have forecasted
    at a certain point in time in order to fully explain how unpredictable events
    cannot be modeled.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在生产环境中运行我们的预测项目后，如果业务单元团队中的成员想要解释为什么某个预测与收集到的数据的最终现实相差如此之远，会发生什么？这是许多依赖机器学习预测来告知业务应采取哪些行动的公司常见的疑问。如果发生黑天鹅事件，业务询问为什么模型预测没有预见它，而你又不得不尝试重新生成模型在某个时间点的预测以全面解释不可预测事件无法被建模的情况，这将是你最不愿意面对的事情。
- en: 'Note A black swan event is an unforeseeable and many times catastrophic event
    that changes the nature of acquired data. While rare, they can have disastrous
    effects on models, businesses, and entire industries. Some recent black swan events
    include the September 11th terrorist attacks, the financial collapse of 2008,
    and the Covid-19 pandemic. Due to the far-reaching and entirely unpredictable
    nature of these events, the impact to models can be absolutely devastating. The
    term “black swan” was coined and popularized in reference to data and business
    in the book *The Black Swan: The Impact of the Highly Improbable* by Nassim Nicholas
    Taleb (Random House, 2007).'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：黑天鹅事件是一种不可预见且许多时候具有灾难性的事件，它改变了获取数据的性质。虽然罕见，但它们可以对模型、企业和整个行业产生灾难性的影响。一些最近的黑天鹅事件包括2001年9月11日的恐怖袭击、2008年的金融危机和Covid-19大流行。由于这些事件的影响范围广泛且完全不可预测，对模型的影响可能是绝对毁灭性的。术语“黑天鹅”是由纳西姆·尼古拉斯·塔勒布（Nassim
    Nicholas Taleb）在他的书中《黑天鹅：几乎不可能发生的事物的影响》（Random House，2007年）中提出并普及的。
- en: To solve these intractable issues that ML practitioners have had to deal with
    historically, MLflow was created. The aspect of MLflow that we’re going to look
    at in this section is the Tracking API, giving us a place to record all of our
    tuning iterations, our metrics from each model’s tuning runs, and pre-generated
    visualizations that can be easily retrieved and referenced from a unified graphical
    user interface (GUI).
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决机器学习从业者历史上必须处理的这些棘手问题，MLflow被创建出来。在本节中，我们将探讨MLflow的跟踪API，它为我们提供了一个记录所有调整迭代、每个模型调整运行中的指标以及可以轻松检索和引用的预生成可视化（GUI）的地方。
- en: 8.1.1 MLflow tracking
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 MLflow跟踪
- en: Let’s look at what is going on with the two Spark-based implementations from
    chapter 7 (section 7.2) as they pertain to MLflow logging. In the code examples
    shown in that chapter, the initialization of the context for MLflow was instantiated
    in two distinct places.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看第7章（7.2节）中基于Spark的两个实现与MLflow日志记录相关的情况。在该章节中展示的代码示例中，MLflow上下文的初始化在两个不同的地方实例化。
- en: In the first approach, using `SparkTrials` as the state-management object (running
    on the driver), the MLflow context was placed as a wrapper around the entire tuning
    run within the function `run_tuning()`. This is the preferred method of orchestrating
    the tracking of runs when using `SparkTrials` so that a parent run’s individual
    children runs can be associated easily for querying from within the tracking server’s
    GUI as well as from REST API requests to the tracking server that involve filter
    predicates.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，使用`SparkTrials`作为状态管理对象（在驱动程序上运行），MLflow上下文被放置在`run_tuning()`函数中整个调整运行的外部包装器。这是使用`SparkTrials`进行运行跟踪的首选方法，以便可以轻松地将父运行的个人子运行关联起来，以便在跟踪服务器的GUI中查询，以及涉及过滤谓词的跟踪服务器REST
    API请求。
- en: Figure 8.1 shows a graphical representation of this code when interacting with
    MLflow’s tracking server. The code records not only the metadata of the parent
    encapsulating run, but the per iteration logging that occurs from the workers
    as each hyperparameter evaluation happens.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1展示了与MLflow跟踪服务器交互时此代码的图形表示。该代码不仅记录了封装运行的父级元数据，还记录了每个超参数评估发生时工作者进行的迭代日志。
- en: '![08-01](../Images/08-01.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![08-01](../Images/08-01.png)'
- en: Figure 8.1 MLflow tracking server logging using distributed hyperparameter optimization
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 使用分布式超参数优化进行MLflow跟踪服务器日志记录。
- en: When looking at the actual code manifestation within the MLflow tracking server’s
    GUI, we can see the results of this parent-child relationship, shown in figure
    8.2.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看MLflow跟踪服务器GUI中的实际代码表现时，我们可以看到这种父子关系的结果，如图8.2所示。
- en: '![08-02](../Images/08-02.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![08-02](../Images/08-02.png)'
- en: Figure 8.2 Example of the MLflow tracking UI
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 MLflow跟踪UI示例
- en: Conversely, the approach used for the `pandas_udf` implementation is slightly
    different. In chapter 7’s listing 7.10, each individual iteration that Hyperopt
    executes requires the creation of a new experiment. Since there is no child-parent
    relationship to group the data together, the application of custom naming and
    tagging is required to allow for searchability within the GUI and—more important
    for production-capable code—the REST API. The overview of the logging mechanics
    for this alternative (and more scalable implementation for this use case of thousands
    of models) is shown in figure 8.3.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，用于`pandas_udf`实现的这种方法略有不同。在第7章的列表7.10中，Hyperopt执行的每个单独迭代都需要创建一个新的实验。由于没有父子关系来分组数据，因此需要应用自定义命名和标记，以便在GUI中进行搜索，并且对于具有生产能力的代码来说更为重要——REST
    API。此替代方案（以及此用例数千个模型的更可扩展实现）的日志机制概述如图8.3所示。
- en: '![08-03](../Images/08-03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![08-03](../Images/08-03.png)'
- en: Figure 8.3 MLflow logging logical execution for the pandas_udf distributed model
    approach.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 MLflow为pandas_udf分布式模型方法提供的日志逻辑执行。
- en: Regardless of which methodology is chosen, the important aspect of all of this
    discussion is that we’ve solved a large problem that frequently causes projects
    to fail. (Each methodology has its own merits for different approaches; for a
    single-model project, `SparkTrails` is by far the better option, while for the
    scenario of forecasting that we’ve shown here, with thousands of models, the `pandas_udf`
    approach is far superior.) We’ve solved the historical tracking and organization
    woes that have hamstrung ML project work for a long time. Having the ability to
    readily access the results of not only our testing, but also the state of a model
    currently running in production as of the point of its training and scoring, is
    simply an essential aspect of creating successful ML projects.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 无论选择哪种方法，所有这些讨论的重要方面是，我们已经解决了一个经常导致项目失败的大问题。（每种方法都有其针对不同方法的优点；对于单一模型项目，`SparkTrails`无疑是更好的选择，而对于我们在此处展示的具有数千个模型的预测场景，`pandas_udf`方法则远胜一筹。）我们解决了长期困扰机器学习项目工作的历史跟踪和组织难题。能够轻松访问测试结果以及训练和评分时正在生产的模型的状态，是创建成功的机器学习项目的一个基本要素。
- en: 8.1.2 Please stop printing and log your information
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 请停止打印并记录您的信息
- en: 'Now that we’ve seen a tool that we can use to keep track of our experiments,
    tuning runs, and pre-production training for each prediction job that is run,
    let’s take a moment to discuss another best-practice aspect of using a tracking
    service when building ML-backed projects: logging.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了一个可以用来跟踪我们的实验、调整运行和每个预测作业的预生产训练的工具，让我们花点时间讨论使用跟踪服务构建机器学习项目时的另一个最佳实践方面：日志记录。
- en: The number of times that I’ve seen `print` statements in production ML code
    is truly astonishing. Most of the time, it’s due to forgotten (or intentionally
    left-in for future debugging) lines of debugging script to let the developer know
    that code is being executed (and whether it’s safe to go get a coffee while it
    runs). At no point outside of coffee breaks during solution development will these
    `print` statements ever be seen by human eyes again. The top of figure 8.4 shows
    the irrelevance of these `print` statements within a code base.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我在生产ML代码中看到的`print`语句的次数确实令人震惊。大多数情况下，这是由于忘记（或故意留下以供未来调试）的调试脚本行，让开发者知道代码正在执行（并且是否在运行时可以安全地去喝咖啡）。在解决方案开发期间的咖啡休息时间之外，这些`print`语句永远不会再次被人类眼睛看到。图8.4的顶部显示了这些`print`语句在代码库中的无关性。
- en: Figure 8.4 compares methodologies that are frequent patterns in ML project code,
    particularly in the top two areas. While the top portion (printing to stdout in
    notebooks that get executed on some periodicity) is definitely not recommended,
    it is, unfortunately, the most frequent habit seen in industry. For more sophisticated
    teams that are writing packaged code for their ML projects (or using languages
    that can be compiled, like Java, Scala, or a C-based language), the historical
    recourse has been to log information about the run to a logging daemon. While
    this does maintain a historical reference for the data record, it also involves
    a great deal of either ETL or, more commonly, ELT in order to extract information
    in the event that something goes wrong. The final block in figure 8.4 demonstrates
    how utilizing MLflow solves these accessibility concerns, as well as the historical
    provenance needs for any ML solution.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4比较了在ML项目代码中常见的模式，尤其是在前两个领域。虽然顶部部分（在笔记本中打印到stdout，这些笔记本以某种周期性执行）绝对不推荐，但遗憾的是，这是在行业中看到的最常见的习惯。对于编写ML项目打包代码的更复杂的团队（或使用可以编译的语言，如Java、Scala或基于C的语言），历史上的做法是将运行信息记录到日志守护程序中。虽然这确实为数据记录维护了历史参考，但它也涉及到大量的ETL或更常见的是ELT，以便在出现问题时提取信息。图8.4中的最后一个块展示了如何利用MLflow解决这些可访问性问题，以及任何ML解决方案的历史溯源需求。
- en: '![08-04](../Images/08-04.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![08-04](../Images/08-04.png)'
- en: Figure 8.4 Comparison of information storage paradigms for ML experimentation
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4比较了ML实验的信息存储范式
- en: To be explicit, I’m not saying to never use `print` or `log` statements. They
    have a remarkable utility when debugging particularly complex code bases, and
    are incredibly useful while developing solutions. This utility begins to fade
    as you transition to production development. The `print` statements are no longer
    looked at, and the desire to parse logs to retrieve status information becomes
    far less palatable when you’re busy with other projects.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了明确起见，我并不是说永远不要使用`print`或`log`语句。在调试特别复杂的代码库时，它们具有非凡的效用，并且在开发解决方案时非常有用。这种效用在你过渡到生产开发时开始减弱。`print`语句不再被查看，当你忙于其他项目时，解析日志以检索状态信息的需求变得远不如以前那样令人愉快。
- en: If critical information needs to be recorded for a project’s code execution,
    it *should be logged and recorded for future reference at all times*. Before tools
    like MLflow solved this problem, many DS teams would record this critical information
    for production purposes to a table in an RDBMS. Larger-scale groups with dozens
    of solutions in production may have utilized a NoSQL solution to handle scalability.
    The truly masochistic would write ELT jobs to parse system logs to retrieve their
    critical data about their models. MLflow simplifies all of these situations by
    creating a cogent unified framework for metric, attribute, and artifact logging
    to eliminate the time-consuming work of ML logging.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要记录一个项目代码执行的关键信息，它*应该始终被记录并保存以供将来参考*。在像MLflow这样的工具解决这个问题之前，许多数据科学团队会将这些关键信息记录到关系型数据库管理系统（RDBMS）中的一个表中，以用于生产目的。更大规模的团队，拥有数十个在生产中的解决方案，可能会利用NoSQL解决方案来处理可扩展性。真正自虐的人会编写ELT作业来解析系统日志以检索他们关于模型的关键数据。MLflow通过创建一个连贯的统一框架来简化所有这些情况，该框架用于指标、属性和工件记录，以消除ML日志记录中耗时的工作。
- en: As we saw in the earlier examples running on Spark, we were recording additional
    information to these runs outside of the typical information that would be associated
    with a tuning execution. We logged the per airport metrics and parameters for
    historical searchability, as well as charts of our forecasts. If we had additional
    data to record, we could simply add a tag through the API in the form of `mlflow.set_tag(<key>,``<value>)`
    for run information logging, or, for more complex information (visualizations,
    data, models, or highly structured data), we can log that information as an artifact
    with the API `mlflow.log_artifact(<location``and``name``of``data``on``local``filesystem>)`.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在Spark早期示例中看到的那样，我们在这些运行之外记录了额外的信息，这些信息通常与调优执行相关联。我们记录了每个机场的指标和参数，以便于历史搜索，以及我们的预测图表。如果我们有额外的数据要记录，我们可以通过API简单地添加一个标签，形式为`mlflow.set_tag(<key>,
    <value>)`用于运行信息记录，或者，对于更复杂的信息（可视化、数据、模型或高度结构化数据），我们可以使用API `mlflow.log_artifact(<location
    and name of data on local filesystem>)`将其记录为工件。
- en: Keeping a history of all information surrounding a particular model tuning and
    training event in a single place, external to the system used to execute the run,
    can save countless hours of frustrating work when trying to re-create the exact
    conditions that the model may have seen when it was trained and you are asked
    to explain what happened to a particular build. Being able to quickly answer questions
    about the business’s faith in your model’s performance can dramatically reduce
    the chances of project abandonment, as well as save a great deal of time in improving
    an underperforming model.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 将特定模型调优和训练事件的所有相关信息保存在一个单独的地方，且在执行运行的系统之外，可以在尝试重新创建模型在训练时可能遇到的确切条件时节省无数令人沮丧的工作时间。能够快速回答有关业务对模型性能的信心的问题可以显著降低项目放弃的可能性，以及节省大量时间来改进表现不佳的模型。
- en: 8.1.3 Version control, branch strategies, and working with others
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 版本控制、分支策略和与他人协作
- en: One of the biggest aspects of development work that can affect a timely and
    organized delivery of a project to the MVP phase is in the way a team (or an individual)
    interacts with a repository. In our example scenario, with a relatively sizeable
    ML team working on individual components of the forecasting model, the ability
    for everyone to contribute to pieces of the code base in a structured and controlled
    manner is absolutely critical for eliminating frustrating rework, broken code,
    and large-scale refactoring. While we haven’t been delving into what the production
    version of this code would look like (it wouldn’t be developed in a notebook,
    that’s for certain), the general design would look something like the module layout
    in figure 8.5.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 影响项目及时且有序地交付到MVP阶段的最大开发工作之一是团队（或个人）与存储库的交互方式。在我们的示例场景中，一个相对规模较大的ML团队正在处理预测模型的各个组件，每个人都能够以结构化和受控的方式为代码库的各个部分做出贡献，这对于消除令人沮丧的重做、损坏的代码和大规模重构至关重要。虽然我们还没有深入研究该代码的生产版本会是什么样子（它不会在笔记本中开发，这是肯定的），但总体设计看起来可能类似于图8.5中的模块布局。
- en: '![08-05](../Images/08-05.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![08-05](../Images/08-05.png)'
- en: Figure 8.5 An initial repository structure for the forecasting project
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 预测项目的初始存储库结构
- en: As the project progresses, different team members of the project will be contributing
    to different modules within the code base at any given time. Some, within the
    sprint, may be tackling tasks and stories surrounding the visualizations. Others
    on that sprint may be working on the core modeling classes, while the common utility
    functions will be added to and refined by nearly everyone on the team.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 随着项目的进展，项目不同的团队成员将在任何给定时间贡献代码库中的不同模块。有些人可能在冲刺期间处理与可视化相关的任务和故事。其他人可能在该冲刺期间从事核心建模类的工作，而常见的实用函数将由团队中的几乎每个人添加和改进。
- en: Without the use of not only a strong version-control system but also a foundational
    process surrounding the committing of code to that repository, the chances of
    the code base being significantly degraded or broken is high. While most aspects
    of ML development are significantly different from traditional software engineering
    development, the one aspect that is completely identical between the two fields
    is in version-control and branched development practices.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 没有使用强大的版本控制系统以及围绕将代码提交到该仓库的基础流程，代码库被显著降级或损坏的可能性很高。虽然机器学习（ML）开发的大部分方面与传统软件开发开发有显著不同，但两个领域完全相同的一个方面是版本控制和分支开发实践。
- en: To prevent issues arising from incompatible changes being merged to a master
    branch, each story or task that is taken from a sprint for a DS to work on should
    have its own branch cut from the current build of the master branch of the repo.
    It is within this branch that the new features should be built, updates to common
    functionality made, and the addition of new unit tests to assure the team that
    the modifications are not going to break anything should all be done. When it
    comes time to close out the story (or task), the DS who developed the code for
    that story will need to ensure that the entire project’s code passes both unit
    tests (especially for modules and functionality that they did not modify) and
    a full-run integration test before submitting their peer review request to merge
    their code into the master.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止不兼容的更改合并到主分支导致的问题，从冲刺中选取的每个故事或任务，DS需要对其工作的当前主分支的构建进行分支。在这个分支中，应该构建新功能，更新通用功能，并添加新的单元测试，以确保团队相信这些修改不会破坏任何东西。当关闭故事（或任务）的时间到来时，为该故事（或任务）开发代码的DS需要确保整个项目的代码通过单元测试（特别是对于他们没有修改的模块和功能）以及完整的运行集成测试，然后才能提交他们的同行评审请求以合并代码到主分支。
- en: Figure 8.6 shows the standard approach for ML project work when dealing with
    a repository, regardless of the repository technology or service used. Each has
    its own nuances, functionality, and commands, which we won’t get into here; what’s
    important is the way the repository is used, rather than how to use a particular
    one.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6显示了处理仓库时机器学习项目工作的标准方法，无论使用的仓库技术或服务如何。每个都有自己的细微差别、功能和命令，我们在这里不会深入探讨；重要的是仓库的使用方式，而不是如何使用特定的一个。
- en: '![08-06](../Images/08-06.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![08-06](../Images/08-06.png)'
- en: Figure 8.6 Repository management process during feature development for an ML
    team
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 机器学习团队在功能开发期间的仓库管理流程
- en: By following a paradigm for code merging like this one, a great deal of frustration
    and wasted time can be completely avoided. It will simply leave more time for
    the DS team members to solve the actual problem of the project, rather than solving
    merge-hell problems and fixing broken code resulting from a bad merge. Effective
    testing of code-merge candidates brings a higher level of project velocity that
    can dramatically reduce the chances of project abandonment by creating a more
    reliable, stable, and bug-free code base for a project.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 通过遵循这样的代码合并范例，可以完全避免大量的挫败感和浪费时间。这将简单地为数据科学（DS）团队成员留下更多时间来解决项目的实际问题，而不是解决合并地狱问题以及修复由不良合并导致的损坏代码。对代码合并候选者的有效测试可以带来更高的项目速度，这可以显著降低项目放弃的可能性，为项目创建一个更可靠、更稳定且无错误的代码库。
- en: 8.2 Scalability and concurrency
  id: totrans-49
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 可扩展性和并发性
- en: Throughout this project that we’ve been working on, the weightiest and most
    complex aspect of the solution has been in scalability. When we talk about scalability
    here, we’re actually referring to *cost*. The longer that VMs are running and
    executing our project code, the more the silent ticker of our bill is going up.
    Anything that we can do to maximize resource utilization of that hardware as a
    function of time is going to keep that bill in a manageable state, reducing the
    concern that the business will have about the total cost of the solution.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们一直在工作的这个项目中，解决方案最关键和最复杂的方面是可扩展性。当我们谈论可扩展性时，我们实际上是在指**成本**。虚拟机（VM）运行和执行我们的项目代码的时间越长，我们的账单上的无声计时器就越高。我们可以做的任何事，都是为了最大化硬件随时间变化的资源利用率，这将保持账单在可管理状态，减少企业对解决方案总成本的担忧。
- en: Throughout the second half of chapter 7, we evaluated two strategies for scaling
    our problem to support modeling many airports. The first, parallelizing the hyperparameter
    evaluation over a cluster, scaled down the per-model training time significantly
    as compared to the serial approach. The second, parallelizing the actual per-model
    training across a cluster, scaled the solution in a slightly different way (which
    is more in favor of the many models/reasonable training iterations approach),
    reducing our cost footprint for the solution in a much larger manner.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章的后半部分，我们评估了两种将我们的问题扩展以支持建模多个机场的策略。第一种，在集群上并行化超参数评估，与串行方法相比，显著缩短了每个模型的训练时间。第二种，在集群上并行化实际的每个模型的训练，以略不同的方式扩展了解决方案（这更有利于许多模型/合理的训练迭代方法），以更大的方式减少了我们的解决方案的成本足迹。
- en: As mentioned in chapter 7, these are but two ways of scaling this problem, both
    involving parallel implementations that distribute portions of the modeling process
    across multiple machines. However, we can add a layer of additional processing
    to speed these operations up even more. Figure 8.7 shows an overview of our options
    for increasing the throughput for ML tasks to reduce the wall-clock time involved
    in building a solution.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如第7章所述，这些只是扩展此问题的两种方法，两者都涉及并行实现，将建模过程的各个部分分散到多台机器上。然而，我们可以添加一个额外的处理层来进一步加快这些操作的速度。图8.7展示了我们增加机器学习任务吞吐量以减少构建解决方案所需墙钟时间的选项概述。
- en: '![08-07](../Images/08-07.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![08-07](../Images/08-07.png)'
- en: Figure 8.7 Comparison of execution paradigms
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 执行范例比较
- en: Moving down the scale in figure 8.7 brings a trade-off between simplicity and
    performance. For problems that require a scale that distributed computing can
    offer, it is important to understand the level of complexity that will be introduced
    into the code base. The challenges with these implementations are no longer relegated
    to the DS part of the solution and instead require increasingly sophisticated
    engineering skills in order to build.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.7中向下缩放会带来简单性和性能之间的权衡。对于需要分布式计算可以提供的规模的问题，了解将引入代码库的复杂性水平是很重要的。这些实现的挑战不再局限于解决方案的DS部分，而是需要越来越复杂的工程技能来构建。
- en: Gaining the knowledge and ability to build large-scale ML projects that leverage
    systems capable of handling distributed computation (for example, Spark, Kubernetes,
    or Dask) will help ensure that you are capable of implementing solutions requiring
    scale. In my own experience, my time has been well spent learning how to leverage
    concurrency and the use of distributed systems to accelerate the performance and
    reduce the cost of projects by monopolizing available hardware resources as much
    as I can.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 获得构建利用能够处理分布式计算的系统（例如，Spark、Kubernetes或Dask）的大规模机器学习项目的知识和能力，将有助于确保你能够实施需要扩展的解决方案。根据我自己的经验，我花费的时间很好地用于学习如何利用并发性和分布式系统的使用来通过尽可能多地垄断可用硬件资源来加速项目性能和降低项目成本。
- en: For the purposes of brevity, we won’t go into examples of implementing the last
    two sections of figure 8.7 within this chapter. However, we will touch on examples
    of concurrent operations later in this book.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，我们不会在本章中详细介绍图8.7最后两个部分的实现示例。然而，我们将在本书的后面部分讨论并发操作的一些示例。
- en: 8.2.1 What is concurrency?
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 什么是并发性？
- en: In figure 8.7, you can see the term *concurrency*listed in the bottom two solutions.
    For most data scientists who don’t come from a software engineering background,
    this term may easily be misconstrued as *parallelism*. It is, after all, effectively
    doing a bunch of things at the same time.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.7中，你可以看到在底部两个解决方案中列出了*并发性*这个术语。对于大多数没有软件工程背景的数据科学家来说，这个术语很容易被误解为*并行性*。毕竟，它实际上是在同时做很多事情。
- en: '*Concurrency*, by definition, is the act of executing many tasks at the same
    time. It doesn’t imply ordering or sequential processing of tasks simultaneously.
    It merely requires that a system and the code instructions being sent to it be
    capable of running more than one task at the same time.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '*并发性*，按定义，是指同时执行许多任务的行为。它并不暗示任务的排序或同时处理的顺序。它仅仅要求系统和发送给它的代码指令能够同时运行多个任务。'
- en: '*Parallelism*, on the other hand, works by dividing tasks into subtasks that
    can be executed in parallel, simultaneously, on discrete threads and cores of
    a CPU or GPU. Spark, for instance, executes tasks in parallel on a distributed
    system of discrete cores in executors.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，*并行性*通过将任务划分为可以在CPU或GPU的离散线程和核心上并行执行的子任务来实现。例如，Spark在执行器上的离散核心分布式系统中并行执行任务。
- en: These two concepts can be combined in a system that can support them, one of
    multiple machines, each of which has multiple cores available to it. This system
    architecture is shown in the final bottom section of figure 8.7\. Figure 8.8 illustrates
    the differences between parallel execution, concurrent execution, and the hybrid
    parallel-concurrent system.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个概念可以在支持它们的系统中结合，一个由多台机器组成，每台机器都有多个可用的核心。这种系统架构在图8.7的底部最后部分显示。图8.8说明了并行执行、并发执行以及混合并行-并发系统之间的差异。
- en: '![08-08](../Images/08-08.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![08-08](../Images/08-08.png)'
- en: Figure 8.8 Comparison of execution strategies
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 执行策略比较
- en: Leveraging these execution strategies for the appropriate type of problem being
    solved can dramatically improve the cost of a project. While it may seem tempting
    to utilize the most complex approach for every problem (parallel concurrent processing
    in a distributed system), it simply isn’t worth it. If the problem that you’re
    trying to solve can be implemented on a single machine, it’s always best to reduce
    the infrastructure complexity by going with that approach. It’s advisable to move
    down the path of greater infrastructure complexity only when you need to. This
    is particularly true when the data, the algorithm, or the scale of tasks is so
    large that a simpler approach is not possible.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 利用这些执行策略来解决适当类型的问题可以显著提高项目的成本。虽然使用最复杂的方法（在分布式系统中进行并行并发处理）来处理每个问题可能看起来很有吸引力，但这并不值得。如果你试图解决的问题可以在单台机器上实现，那么通过采用这种方法来降低基础设施复杂性总是最好的选择。只有在你需要的时候，才建议你走更复杂的路径。这尤其适用于数据、算法或任务规模如此之大，以至于简单的方案不可行的情况。
- en: 8.2.2 What you can (and can’t) run asynchronously
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 你可以（和不可以）异步运行的内容
- en: For a final note on improving runtime performance, it is important to mention
    that not every problem in ML can be solved through the use of parallel execution
    or on a distributed system. Many algorithms require maintaining state to function
    correctly, and as such, cannot be split into subtasks to execute on a pool of
    cores.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 关于提高运行时性能的最后一句话，重要的是要提到，在机器学习中并非每个问题都可以通过并行执行或在分布式系统中解决。许多算法需要维护状态才能正确运行，因此不能分割成子任务在核心池上执行。
- en: The scenario that we’ve gone through in the past few chapters with univariate
    time series could certainly benefit from parallelizing. We can parallelize both
    the Hyperopt tuning and the model training. The isolation that we can achieve
    within the data itself (each airport’s data is self-contained and has no dependency
    on any other’s) and the tuning actions means that we can dramatically reduce the
    total runtime of our job by appropriately leveraging both distributed processing
    and asynchronous concurrency.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在前几章中讨论的单变量时间序列的情景确实可以从并行化中受益。我们可以并行化Hyperopt调优和模型训练。我们可以在数据本身内实现隔离（每个机场的数据是自包含的，不依赖于任何其他数据）以及调优操作，这意味着我们可以通过适当利用分布式处理和异步并发来显著减少我们工作的总运行时间。
- en: When selecting opportunities for improving performance of a modeling solution,
    you should be thinking about the dependencies within the tasks being executed.
    If there is an opportunity to isolate tasks from one another, such as separating
    model evaluation, training, or inference based on filters that can be applied
    to a dataset, it could be worthwhile to leverage a framework that can handle this
    processing for you.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当选择提高建模解决方案性能的机会时，你应该考虑正在执行的任务之间的依赖关系。如果有机会将任务彼此隔离，例如根据可以应用于数据集的过滤器来分离模型评估、训练或推理，那么利用可以为你处理此处理的框架可能是值得的。
- en: However, many tasks in ML cannot be distributed (or, at least, cannot be distributed
    easily). Models that require access to the entirety of a feature training set
    are poor candidates for distributed training. Other models may have the capability
    to be distributed but simply have not been because of either demand or the technological
    complexity involved in building a distributed solution. The best bet, when wondering
    whether an algorithm or approach can leverage concurrency or parallelism through
    distributed processing, is to read the library documentation for popular frameworks.
    If an algorithm hasn’t been implemented on a distributed processing framework,
    there’s likely a good reason. Either simpler approaches are available that fulfill
    the same requirements of the model you’re looking into (highly likely), or the
    development and runtime costs for building a distributed solution for the algorithm
    are astronomically high.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在机器学习中，许多任务是无法分布的（或者至少难以分布）。需要访问整个特征训练集的模型不适合分布式训练。其他模型可能具有分布的能力，但由于需求或构建分布式解决方案的技术复杂性，尚未实现。当考虑一个算法或方法是否可以通过分布式处理利用并发或并行性时，最好的办法是阅读流行框架的库文档。如果一个算法尚未在分布式处理框架上实现，那么很可能有很好的理由。要么有更简单的方法可以满足你正在研究的模型（高度可能），要么构建分布式解决方案的开发和运行成本非常高。
- en: Summary
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Utilizing an experimentation tracking service such as MLflow throughout a solution’s
    life cycle can dramatically increase auditability and historical monitoring for
    projects. Additionally, utilizing version control and logging will enhance production
    code bases with the ability to reduce troubleshooting time and allow for diagnostic
    reporting of the project’s health when in production.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在整个解决方案的生命周期中利用实验跟踪服务，例如 MLflow，可以显著提高项目的可审计性和历史监控。此外，利用版本控制和日志记录将增强生产代码库，能够减少故障排除时间，并在生产中允许对项目健康状况的诊断报告。
- en: Learning to use and implement solutions in a scalable infrastructure is incredibly
    important for many large-scale ML projects. While not appropriate for all implementations,
    understanding distributed systems, concurrency, and the frameworks that enable
    these paradigms is crucial for an ML engineer.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习在可扩展的基础设施中使用和实现解决方案对于许多大规模机器学习项目至关重要。虽然这不适用于所有实现，但理解分布式系统、并发性和使这些范式成为可能的框架对于机器学习工程师来说是至关重要的。
