- en: 13 Putting it all in practice: A real-life example of data engineering and machine
    learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 将一切付诸实践：数据工程和机器学习的真实案例
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: cleaning up and preprocessing data to make it readable by our model
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清洗和预处理数据，使其可由我们的模型读取
- en: using Scikit-Learn to train and evaluate several models
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Scikit-Learn训练和评估多个模型
- en: using grid search to select good hyperparameters for our model
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用网格搜索为我们的模型选择良好的超参数
- en: using k-fold cross-validation to be able to use our data for training and validation
    simultaneously
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用k折交叉验证以便能够同时使用我们的数据进行训练和验证
- en: '![](../Images/13-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/13-unnumb.png)'
- en: Throughout this book, we’ve learned some of the most important algorithms in
    supervised learning, and we’ve had the chance to code them and use them to make
    predictions on several datasets. However, the process of training a model on real
    data requires several more steps, and this is what we discuss in this chapter.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在整本书中，我们学习了一些监督学习中最重要的算法，并有机会编码它们，并使用它们在多个数据集上进行预测。然而，在真实数据上训练模型的过程需要更多步骤，这正是本章所讨论的内容。
- en: One of the most fundamental jobs of a data scientist is cleaning and preprocessing
    the data. This is crucial because the computer can’t fully do it. To clean data
    properly, it’s necessary to have good knowledge of the data and of the problem
    being solved. In this chapter, we see some of the most important techniques for
    cleaning up and preprocessing data. Then we look more carefully into the features
    and apply some feature engineering that will get them ready for the model. As
    a next step, we split the model into training, validation, and testing sets, train
    several models on our dataset, and evaluate them. This way, we’ll be able to pick
    the best-performing model for this dataset. Finally, we learn important methods
    such as grid search to find the best hyperparameters for our model.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家最基本的工作之一是清洗和预处理数据。这一点至关重要，因为计算机无法完全完成这项工作。要正确地清洗数据，需要具备良好的数据知识和解决问题的能力。在本章中，我们将探讨一些用于清理和预处理数据的重要技术。然后，我们将更仔细地研究特征，并应用一些特征工程，使它们为模型做好准备。作为下一步，我们将模型分为训练集、验证集和测试集，在数据集上训练多个模型，并评估它们。这样，我们就能为这个数据集选择表现最佳的模型。最后，我们学习了一些重要的方法，例如网格搜索，以找到我们模型的最佳超参数。
- en: 'We apply all these steps on a popular dataset for learning and practicing machine
    learning techniques: the Titanic dataset. We cover this dataset in depth in the
    following section. This chapter includes lots of coding. The two Python packages
    that we use are Pandas and Scikit-Learn, which we’ve used extensively throughout
    this book. The Pandas package is good for handling data, including opening files,
    loading data, and organizing it as tables, called DataFrames. The Scikit-Learn
    package is good for training and evaluating models, and it contains solid implementations
    of most of the algorithms we learn in this book.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将这些步骤应用于学习机器学习技术的流行数据集：泰坦尼克号数据集。在下一节中，我们将深入探讨这个数据集。本章包含大量的编码。我们使用的两个Python包是Pandas和Scikit-Learn，我们在整本书中广泛使用了它们。Pandas包非常适合处理数据，包括打开文件、加载数据以及将其组织成表格，称为DataFrame。Scikit-Learn包非常适合训练和评估模型，它包含了我们在这本书中学到的大多数算法的稳健实现。
- en: 'The code and the dataset that we’ll be working with throughout this whole chapter
    are the following:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中使用的代码和数据集如下：
- en: '**Notebook**: End_to_end_example.ipynb'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：End_to_end_example.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_13_End_to_end_example/End_to_end_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_13_End_to_end_example/End_to_end_example.ipynb)'
  id: totrans-12
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_13_End_to_end_example/End_to_end_example.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_13_End_to_end_example/End_to_end_example.ipynb)'
- en: '**Dataset**: titanic.csv'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：titanic.csv'
- en: The Titanic dataset
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 泰坦尼克号数据集
- en: In this section, we load and study the dataset. Loading and handling data is
    a crucial skill for a data scientist, because the success of the model depends
    highly on how the data that feeds into it is preprocessed. We use the Pandas package
    to do this.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们加载数据并对其进行研究。加载数据和处理数据是数据科学家的一项关键技能，因为模型的成功高度依赖于输入数据的预处理方式。我们使用Pandas包来完成这项工作。
- en: 'Throughout this chapter, we work with an example that is popular for learning
    machine learning: the Titanic dataset. At a high level, the dataset contains information
    about many of the passengers in the *Titanic*, including their name, age, marital
    status, port of embarkment, and class. Most importantly, it also contains information
    about the passenger’s survival. This dataset can be found in Kaggle ([www.kaggle.com](https://www.kaggle.com)),
    a popular online community with great datasets and contests, which I highly recommend
    you check out.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了一个流行的机器学习学习示例：泰坦尼克号数据集。从高层次来看，这个数据集包含了关于泰坦尼克号上许多乘客的信息，包括他们的姓名、年龄、婚姻状况、登船港口和舱位等级。最重要的是，它还包含了乘客的生还信息。这个数据集可以在Kaggle([www.kaggle.com](https://www.kaggle.com))上找到，这是一个拥有大量数据集和竞赛的流行在线社区，我强烈推荐你查看。
- en: note The dataset we use is a historic dataset, which, as you may imagine, contains
    many societal biases from 1912\. Historical datasets do not present the opportunity
    for revision or additional sampling to reflect current societal norms and understanding
    of the world. Some examples found here are the lack of inclusion of nonbinary
    genders, different treatment for passengers with respect to gender and social
    class, and many others. We’ll evaluate this dataset as if it were a number table,
    because we believe that it is a very rich and commonly used dataset for building
    models and making predictions. However, as data scientists, it is our duty to
    always be mindful of biases in our data, such as those concerning race, gender
    identity, sexual orientation, social status, ability, nationality, beliefs, and
    many others, and to do everything in our power to ensure that the models we build
    will not perpetuate historical biases.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 备注：我们使用的这个数据集是一个历史数据集，正如你可能想象的那样，它包含了1912年的许多社会偏见。历史数据集不提供修订或额外抽样的机会，以反映当前的社会规范和对世界的理解。这里的一些例子包括非二元性别的缺失、对性别和社会阶级的乘客的不同待遇，以及许多其他情况。我们将像对待数字表一样评估这个数据集，因为我们相信它是一个非常丰富且常用的数据集，用于构建模型和进行预测。然而，作为数据科学家，我们有责任始终警惕数据中的偏见，例如关于种族、性别认同、性取向、社会地位、能力、国籍、信仰等方面的偏见，并尽我们所能确保我们构建的模型不会延续历史偏见。
- en: The features of our dataset
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据集的特征
- en: 'The Titanic dataset we are using contains the names and information of 891
    passengers on the *Titanic*, including whether they survived. Here are the columns
    of the dataset:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用的泰坦尼克号数据集包含了891名乘客的姓名和信息，包括他们是否生还。以下是数据集的列：
- en: '**PassengerID**: a number that identifies each passenger, from 1 to 891'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**乘客ID**：一个标识每个乘客的数字，从1到891'
- en: '**Name**: the full name of the passenger'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**姓名**：乘客的全名'
- en: '**Sex**: the gender of the passenger (male or female)'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**性别**：乘客的性别（男性或女性）'
- en: '**Age**: the age of the passenger as an integer'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**年龄**：乘客的年龄，以整数表示'
- en: '**Pclass**: the class in which the passenger was traveling: first, second,
    or third'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**舱位等级**：乘客所乘坐的舱位等级：头等舱、二等舱或三等舱'
- en: '**SibSP**: the number of siblings and spouse of the passenger (0 if the passenger
    is traveling alone)'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**兄弟姐妹和配偶数量**：乘客的兄弟姐妹和配偶的数量（如果乘客独自旅行，则为0）'
- en: '**Parch**: the number of parents and children of the passenger (0 if the passenger
    is traveling alone)'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**父母和子女数量**：乘客的父母和子女的数量（如果乘客独自旅行，则为0）'
- en: '**Ticket**: the ticket number'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**船票号**：乘客的船票号码'
- en: '**Fare**: the fare the passenger paid in British pounds'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**票价**：乘客支付的英镑票价'
- en: '**Cabin**: the cabin in which the passenger was traveling'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**船舱**：乘客所乘坐的船舱'
- en: '**Embarked**: the port in which the passenger embarked: “C” for Cherbourg,
    “Q” for Queenstown, and “S” for Southampton'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**启航港**：乘客登船的港口：“C”代表瑟堡，“Q”代表昆士敦，“S”代表南安普顿'
- en: '**Survived**: information whether the passenger survived (1) or not (0)'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**生还**：乘客是否生还的信息（1代表生还，0代表未生还）'
- en: Using Pandas to load the dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas加载数据集
- en: 'In this section, we learn how to open a dataset using Pandas and load it into
    a DataFrame, which is the object Pandas uses to store tables of data. I have downloaded
    the data from [www.kaggle.com](https://www.kaggle.com) and stored it as a CSV
    (comma-separated values) file named titanic.csv. Before we do anything on Pandas,
    we must import Pandas with the following command:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用Pandas打开数据集并将其加载到DataFrame中，这是Pandas用来存储数据表的对象。我已经从[www.kaggle.com](https://www.kaggle.com)下载了数据，并将其存储为名为titanic.csv的CSV（逗号分隔值）文件。在我们对Pandas进行任何操作之前，我们必须使用以下命令导入Pandas：
- en: '[PRE0]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that we have loaded Pandas, we need to load the dataset. For storing datasets,
    Pandas uses two objects: the *DataFrame* and the *Series*. They are essentially
    the same thing, except that the Series is used for datasets of only one column,
    and the DataFrame is used for datasets of more than one column.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经加载了 Pandas，我们需要加载数据集。对于存储数据集，Pandas 使用两个对象：*DataFrame* 和 *Series*。它们本质上是一样的，区别在于
    Series 用于只有一列的数据集，而 DataFrame 用于多列数据集。
- en: 'We can load the dataset as a DataFrame using the following command:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令将数据集作为 DataFrame 加载：
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: This command stores the dataset into a Pandas DataFrame called `raw_data`. We
    call it raw data because our goal is to clean it and preprocess it later. Once
    we load it, we can see that the first rows look like table 13.1\. In general,
    Pandas adds an extra column numbering all the elements in the dataset. Because
    the dataset already comes with this numbering, we can set this index to be that
    column by specifying `index_col="PassengerId"`. For this reason, we may see that
    in this dataset, the rows are indexed starting from 1 instead of starting from
    0 as is more common in practice.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 这个命令将数据集存储到名为 `raw_data` 的 Pandas DataFrame 中。我们称之为原始数据，因为我们的目标是清理和预处理它。一旦我们加载它，我们就可以看到前几行看起来像表
    13.1。一般来说，Pandas 会添加一个额外的列，编号数据集中的所有元素。因为数据集已经包含了这种编号，我们可以通过指定 `index_col="PassengerId"`
    来将这个索引设置为该列。因此，我们可能会看到在这个数据集中，行是从 1 开始编号，而不是像实践中更常见的从 0 开始。
- en: Table 13.1 The Titanic dataset contains information on the passengers on the
    Titanic, including whether they survived. Here we use Pandas to open the dataset
    and print out its rows and columns. Notice that it has 891 rows and 12 columns.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 表 13.1 泰坦尼克号数据集包含关于泰坦尼克号乘客的信息，包括他们是否幸存。这里我们使用 Pandas 打开数据集并打印其行和列。注意它有 891 行和
    12 列。
- en: '| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket
    | Fare | Cabin | Embarked |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket
    | Fare | Cabin | Embarked |'
- en: '| 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500
    | NaN | S |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 3 | 布劳恩，欧文·哈里斯先生 | 男 | 22.0 | 1 | 0 | A/5 21171 | 7.2500 | NaN |
    S |'
- en: '| 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female |
    38.0 | 1 | 0 | PC 17599 | 71.2833 | C85 | C |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | 库明斯，约翰·布拉德利夫人（佛罗伦萨·布里格斯·... | 女 | 38.0 | 1 | 0 | PC 17599 | 71.2833
    | C85 | C |'
- en: '| 3 | 1 | 3 | Heikkinen, Miss Laina | female | 26.0 | 0 | 0 | STON/O2\. 3101282
    | 7.9250 | NaN | S |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 3 | 海金恩，莱娜小姐 | 女 | 26.0 | 0 | 0 | STON/O2\. 3101282 | 7.9250 | NaN
    | S |'
- en: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... | ... |'
- en: '| 890 | 1 | 1 | Behr, Mr. Karl Howell | male | 26.0 | 0 | 0 | 111369 | 30.0000
    | C148 | C |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| 890 | 1 | 1 | 贝尔，卡尔·豪厄尔先生 | 男 | 26.0 | 0 | 0 | 111369 | 30.0000 | C148 |
    C |'
- en: '| 891 | 0 | 3 | Dooley, Mr. Patrick | male | 32.0 | 0 | 0 | 370376 | 7.7500
    | NaN | Q |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 891 | 0 | 3 | 杜利，帕特里克先生 | 男 | 32.0 | 0 | 0 | 370376 | 7.7500 | NaN | Q |'
- en: Saving and loading the dataset
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 保存和加载数据集
- en: Before we embark on studying our dataset, here’s a small step that will help
    us. At the end of each section, we’ll save the dataset in a CSV file, and we’ll
    load it again at the beginning of the next section. This is so we can put down
    the book or quit the Jupyter Notebook and come back to work on it later at any
    checkpoint, without having to rerun all the commands from the beginning. With
    a small dataset like this one, it is not a big deal to rerun the commands, but
    imagine if we were processing large volumes of data. Serializing and saving data
    is important there, because it saves time and processing power.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始研究我们的数据集之前，这里有一个小步骤将帮助我们。在每个部分的末尾，我们将数据集保存为 CSV 文件，并在下一部分的开头再次加载它。这样我们就可以放下书本或退出
    Jupyter Notebook，在任何检查点稍后回来继续工作，而无需重新运行所有命令。对于如此小的数据集，重新运行命令并不是什么大问题，但想象一下如果我们正在处理大量数据。序列化和保存数据在那里很重要，因为它节省了时间和处理能力。
- en: 'Here are the names of the datasets saved at the end of each section:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个部分末尾保存的数据集名称：
- en: '“The Titanic dataset”: raw_data'
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “泰坦尼克号数据集”：raw_data
- en: '“Cleaning up our dataset”: clean_data'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “清理我们的数据集”：clean_data
- en: '“Feature engineering”: preprocessed_data'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: “特征工程”：preprocessed_data
- en: 'The commands for saving and loading follow:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 保存和加载的命令如下：
- en: '[PRE2]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: When Pandas loads a dataset, it adds an index column that numbers each of the
    elements. We can ignore this column, but when we save the dataset, we must set
    the parameter `index=None` to avoid saving unnecessary index columns.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Pandas 加载数据集时，它会添加一个索引列，为每个元素编号。我们可以忽略这个列，但在保存数据集时，我们必须设置参数 `index=None` 以避免保存不必要的索引列。
- en: The dataset already has an index column called PassengerId. If we wanted to
    instead use this one as the default index column in Pandas, we could specify `index_col='PassengerId'`
    when we load the dataset (but we won’t do this).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 该数据集已经有一个名为 PassengerId 的索引列。如果我们想将其作为 Pandas 中的默认索引列，我们可以在加载数据集时指定 `index_col='PassengerId'`（但我们不会这样做）。
- en: Using Pandas to study our dataset
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Pandas 研究我们的数据集
- en: 'In this section, I teach you some useful methods for studying our dataset.
    The first one is the length function, or `len`. This function returns the number
    of rows in the dataset as follows:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你一些研究我们数据集的有用方法。第一个是长度函数，或 `len`。此函数返回数据集中的行数，如下所示：
- en: '[PRE3]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'This means our dataset has 891 rows. To output the names of the columns, we
    use the `columns` property of a DataFrame, as follows:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们的数据集有 891 行。要输出列名，我们使用 DataFrame 的 `columns` 属性，如下所示：
- en: '[PRE4]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Now let’s explore one of the columns. With the following command, we can explore
    the Survived column:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们探索一个列。使用以下命令，我们可以探索幸存列：
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'The first column is the index of the passenger (1 to 891). The second one is
    a 0 if the passenger didn’t survive, and a 1 if the passenger survived. However,
    if we wanted two columns—for example Name and Age—we can use the `next` command:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列是乘客的索引（1 到 891）。第二列如果是乘客未幸存，则为 0，如果是乘客幸存，则为 1。然而，如果我们想添加两列——例如姓名和年龄——我们可以使用
    `next` 命令：
- en: '[PRE6]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: and this would return a DataFrame with only those two columns.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 这样就会返回只包含这两列的 DataFrame。
- en: 'Now let’s say we want to find out how many passengers survived. We can sum
    up the values in the Survived column using the `sum` function, as follows:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在假设我们想找出有多少乘客幸存。我们可以使用 `sum` 函数对幸存列的值进行求和，如下所示：
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This indicates that out of the 891 passengers in our dataset, only 342 survived.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在我们的数据集中，891 名乘客中只有 342 名幸存。
- en: This is only the tip of the iceberg in terms of all the functionality that Pandas
    offers for handling datasets. Visit the documentation page at [https://pandas.pydata.org](https://pandas.pydata.org)
    to learn more about it.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是 Pandas 为处理数据集提供的所有功能的一小部分。请访问[https://pandas.pydata.org](https://pandas.pydata.org)的文档页面以了解更多信息。
- en: 'Cleaning up our dataset: Missing values and how to deal with them'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 清理我们的数据集：缺失值及其处理方法
- en: Now that we know how to handle DataFrames, we discuss some techniques to clean
    up our dataset. Why is this important? In real life, data can be messy, and feeding
    messy data into a model normally results in a bad model. It is important that
    before training models, the data scientist explores the dataset well and performs
    some cleanup to get the data ready for the models.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经知道如何处理 DataFrame，我们将讨论一些清理数据集的技术。为什么这很重要？在现实生活中，数据可能很混乱，将混乱的数据输入模型通常会导致模型效果不佳。在训练模型之前，数据科学家充分探索数据集并进行一些清理，以便为模型准备数据是非常重要的。
- en: 'The first problem we encounter is datasets with missing values. Due to human
    or computer errors, or simply due to problems with data collection, datasets don’t
    always come with all the values in them. Trying to fit a model to a dataset with
    missing values will probably result in an error. The Titanic dataset is not an
    exception when it comes to missing data. For example, let’s look at the Cabin
    column of our dataset, shown here:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到的第一问题是带有缺失值的数据集。由于人为或计算机错误，或者简单地由于数据收集问题，数据集并不总是包含所有值。试图将模型拟合到带有缺失值的数据集可能会产生错误。在缺失数据方面，泰坦尼克号数据集也不例外。例如，让我们看看我们数据集中的舱位列，如下所示：
- en: '[PRE8]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Some cabin names are present, such as C123 or C148, but the majority of the
    values are NaN. NaN, or “not a number,” means the entry is either missing, unreadable,
    or simply another type that can’t be converted into a number. This could have
    happened because of clerical errors; one could imagine that the records of the
    *Titanic* are old and some information has been lost, or they simply didn’t record
    the cabin number for every passenger to begin with. Either way, we don’t want
    to have NaN values in our dataset. We are at a decision point: should we deal
    with these NaN values or remove the column completely? First let’s check how many
    NaN values are in each column of our dataset. Our decision will depend on the
    answer to that question.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 一些船舱名称是存在的，例如C123或C148，但大多数值都是NaN。NaN，或“不是一个数字”，意味着条目可能缺失、不可读，或者简单地是另一种无法转换为数字的类型。这可能是由于文书错误造成的；可以想象，“泰坦尼克号”的记录很旧，一些信息已经丢失，或者他们一开始就没有为每位乘客记录船舱号码。无论如何，我们都不希望数据集中有NaN值。我们现在面临一个决策点：我们应该处理这些NaN值还是完全删除该列？首先，让我们检查数据集中每一列有多少NaN值。我们的决策将取决于对这个问题的回答。
- en: 'To find out how many values in each column are NaN, we use the `is_na` (or
    `is_null`) function. The `is_na` function returns a 1 if the entry is NaN, and
    a 0 otherwise. Therefore, if we sum over these values, we get the number of entries
    that are NaN in every column, as shown here:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 为了找出每一列中有多少值是NaN，我们使用`is_na`（或`is_null`）函数。如果条目是NaN，`is_na`函数返回1，否则返回0。因此，如果我们对这些值求和，我们就可以得到每一列中NaN条目的数量，如下所示：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: This tells us that the only columns with missing data are Age, which is missing
    177 values; Cabin, which is missing 687 values; and Embarked, which is missing
    2 values. We can deal with missing data using a few methods, and we’ll apply different
    ones to different columns for this dataset.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉我们，唯一有缺失数据的列是年龄列，缺失177个值；船舱列，缺失687个值；以及登船列，缺失2个值。我们可以使用几种方法处理缺失数据，并且我们将为这个数据集的不同列应用不同的方法。
- en: Dropping columns with missing data
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 删除有缺失数据的列
- en: 'When a column is missing too many values, the corresponding feature may not
    be useful to our model. In this case, Cabin does not look like a good feature.
    Out of 891 rows, 687 don’t have a value. This feature should be removed. We can
    do it with the `drop` function in Pandas as follows. We’ll make a new DataFrame
    called `clean_data` to store the data we’re about to clean up:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当一列缺失太多值时，对应特征可能对我们的模型没有用处。在这种情况下，船舱似乎不是一个好的特征。在891行中，有687行没有值。这个特征应该被删除。我们可以使用Pandas中的`drop`函数来完成这个操作。我们将创建一个新的DataFrame，名为`clean_data`，以存储我们即将清理的数据：
- en: '[PRE10]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The arguments to the `drop` function follow:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: '`drop`函数的参数如下：'
- en: The name of the column we want to drop
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要删除的列的名称
- en: The `axis` parameter, which is 1 when we want to drop a column and 0 when we
    want to drop a row
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们想要删除列时，`axis`参数为1，当我们想要删除行时，`axis`参数为0
- en: We then assign the output of this function to the variable `clean_data`, indicating
    that we want to replace the old DataFrame called `data` by the new one with the
    deleted column.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将这个函数的输出分配给变量`clean_data`，表示我们想要用新的DataFrame替换旧的DataFrame，即名为`data`的DataFrame，并删除该列。
- en: 'How to not lose the entire column: Filling in missing data'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如何不丢失整个列：填充缺失数据
- en: 'We don’t always want to delete columns with missing data, because we might
    lose important information. We can also fill in the data with values that would
    make sense. For example, let’s take a look at the Age column, shown next:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '我们并不总是想删除有缺失数据的列，因为我们可能会丢失重要的信息。我们也可以用有意义的值填充数据。例如，让我们看一下下面的年龄列： '
- en: '[PRE11]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As we calculated previously, the Age column is missing only 177 values out
    of 891, which is not that many. This column is useful, so let’s not delete it.
    What can we do with these missing values, then? There are many things we can do,
    but the most common are filling them in with the average or the median of the
    other values. Let’s do the latter. First, we calculate the median, using the median
    function, and we obtain 28\. Next, we use the `fillna` function, which fills in
    the missing values with the value we give it, as shown in the next code snippet:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前所计算的，年龄列中只有177个值缺失，而总共有891个值，这并不多。这个列是有用的，所以我们不要删除它。那么我们可以对这些缺失值做些什么呢？我们可以做很多事情，但最常见的是用其他值的平均值或中位数来填充它们。让我们做后者。首先，我们使用中值函数计算中值，得到28。接下来，我们使用`fillna`函数，用我们给出的值填充缺失值，如下一代码片段所示：
- en: '[PRE12]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'The third column that is missing values is Embarked, which is missing two values.
    What can we do here? There is no average we can use, because these are letters,
    not numbers. Luckily, only two rows are missing this number among 891 of them,
    so we are not losing too much information here. My suggestion is to lump all the
    passengers with no value in the Embarked column into the same class. We can call
    this class U, for “Unknown.” The following line of code will do it:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 第三列缺失值的是登船港口（Embarked），缺失了两个值。我们在这里能做什么呢？由于这些是字母而不是数字，我们无法使用平均值。幸运的是，在891个乘客中只有两行缺失这个数字，所以我们在这里并没有丢失太多信息。我的建议是将所有在登船港口列中没有值的乘客归入同一个类别。我们可以将这个类别称为U，代表“未知”。以下代码行将完成这项操作：
- en: '[PRE13]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Finally, we can save this DataFrame in a CSV file called clean_titanic_data
    to use in the next section:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以将这个DataFrame保存为名为clean_titanic_data的CSV文件，以便在下一节中使用：
- en: '[PRE14]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Feature engineering: Transforming the features in our dataset before training
    the models'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征工程：在训练模型之前转换数据集中的特征
- en: Now that we’ve cleaned our dataset, we are much closer to being able to train
    a model. However, we still need to do some important data manipulations, which
    we see in this section. The first is transforming the type of data from numerical
    to categorical, and vice versa. The second is feature selection, in which we manually
    decide which features to remove to improve the training of our model.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经清洗了数据集，我们离能够训练模型更近了。然而，我们仍然需要进行一些重要的数据处理，这些内容在本节中可以看到。首先是数据类型从数值到分类的转换，反之亦然。其次是特征选择，我们手动决定哪些特征需要移除以提高我们模型的训练效果。
- en: 'Recall from chapter 2 that there are two types of features, numerical and categorical.
    A numerical feature is one that is stored as numbers. In this dataset, features
    such as the age, fare, and class are numbers. A categorical feature is one that
    contains several categories, or classes. For example, the gender feature contains
    two classes: female and male. The embarked feature contains three classes, C for
    Cherbourg, Q for Queenstown, and S for Southampton.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 回想第2章，特征有两种类型：数值型和分类型。数值型特征是存储为数字的特征。在本数据集中，年龄、票价和等级等特征都是数字。分类型特征包含几个类别或等级。例如，性别特征包含两个类别：女性和男性。登船港口特征包含三个类别，C代表瑟堡，Q代表皇后镇，S代表南安普顿。
- en: As we have seen throughout this book, machine learning models take numbers as
    input. If that is the case, how do we input the word “female,” or the letter “Q”?
    We need to have a way to turn categorical features into numerical features. Also,
    believe it or not, sometimes we may be interested in treating numerical features
    as categorical to aid us in our training, such as putting them in buckets, for
    example, age 1–10, 11–20, and so on. We cover this more in the section “Turning
    numerical data into categorical data.”
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在整本书中看到的，机器学习模型以数字作为输入。如果是这样，我们如何输入“女性”这个词或“Q”这个字母呢？我们需要有一种方法将分类特征转换为数值特征。而且，信不信由你，有时我们可能对将数值特征作为分类特征感兴趣，以帮助我们进行训练，例如将它们放入桶中，例如，年龄1-10，11-20等等。我们将在“将数值数据转换为分类数据”这一节中更详细地介绍这一点。
- en: 'Even more, when we think of a feature such as the passenger class (called Pclass),
    is this truly a numerical feature, or is it categorical? Should we think of class
    as a number between one and three, or as three classes: first, second, and third?
    We answer all those questions in this section.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 更进一步，当我们想到一个特征，比如乘客等级（称为Pclass），这究竟是一个数值特征，还是一个分类特征？我们应该将等级视为介于一和三之间的数字，还是将其视为三个等级：头等舱、二等舱和三等舱？我们将在本节中回答所有这些问题。
- en: In this section, we call the DataFrame `preprocessed_data`. The first few rows
    of this dataset are shown in table 13.2
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将DataFrame称为`preprocessed_data`。该数据集的前几行如表13.2所示
- en: Table 13.2 The first five rows of the cleaned-up dataset. We will proceed to
    preprocess this data for training.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.2 清洗后的数据集的前五行。我们将继续预处理这些数据以进行训练。
- en: '| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket
    | Fare | Embarked |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| PassengerId | Survived | Pclass | Name | Sex | Age | SibSp | Parch | Ticket
    | Fare | Embarked |'
- en: '| 1 | 0 | 3 | Braund, Mr. Owen Harris | male | 22.0 | 1 | 0 | A/5 21171 | 7.2500
    | S |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 3 | Braund, Mr. Owen Harris | 男性 | 22.0 | 1 | 0 | A/5 21171 | 7.2500
    | S |'
- en: '| 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | female |
    38.0 | 1 | 0 | PC 17599 | 71.2833 | C |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 1 | 1 | Cumings, Mrs. John Bradley (Florence Briggs Th... | 女性 | 38.0
    | 1 | 0 | PC 17599 | 71.2833 | C |'
- en: '| 3 | 1 | 3 | Heikkinen, Miss. Laina | female | 26.0 | 0 | 0 | STON/O2\. 3101282
    | 7.9250 | S |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 1 | 3 | Heikkinen, Miss. Laina | 女性 | 26.0 | 0 | 0 | STON/O2\. 3101282
    | 7.9250 | S |'
- en: '| 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0
    | 1 | 0 | 113803 | 53.1000 | S |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1 | 1 | Futrelle, Mrs. Jacques Heath (Lily May Peel) | female | 35.0
    | 1 | 0 | 113803 | 53.1000 | S |'
- en: '| 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500
    | S |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0 | 3 | Allen, Mr. William Henry | male | 35.0 | 0 | 0 | 373450 | 8.0500
    | S |'
- en: 'Turning categorical data into numerical data: One-hot encoding'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 将分类数据转换为数值数据：One-hot encoding
- en: As was mentioned previously, machine learning models perform a lot of mathematical
    operations, and to perform mathematical operations in our data, we must make sure
    all the data is numerical. If we have any columns with categorical data, we must
    turn them into numbers. In this section, we learn a way to do this effectively
    using a technique called *one-hot encoding*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: As was mentioned previously, machine learning models perform a lot of mathematical
    operations, and to perform mathematical operations in our data, we must make sure
    all the data is numerical. If we have any columns with categorical data, we must
    turn them into numbers. In this section, we learn a way to do this effectively
    using a technique called *one-hot encoding*.
- en: 'But before we delve into one-hot encoding, here’s a question: why not simply
    attach a different number to each one of the classes? For example, if our feature
    has 10 classes, why not number them 0, 1, 2,..., 9? The reason is that this forces
    an order in the features that we may not want. For example, if the Embarked column
    has the three classes C, Q, and S, corresponding to Cherbourg, Queenstown, and
    Southampton, assigning the numbers 0, 1, and 2 to these would implicitly tell
    the model that the value of Queenstown is between the values of Cherbourg and
    Southampton, which is not necessarily true. A complex model may be able to deal
    with this implicit ordering, but simpler models (such as linear models, for example)
    will suffer. We’d like to make these values more independent of each other, and
    this is where one-hot encoding comes in.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 'But before we delve into one-hot encoding, here’s a question: why not simply
    attach a different number to each one of the classes? For example, if our feature
    has 10 classes, why not number them 0, 1, 2,..., 9? The reason is that this forces
    an order in the features that we may not want. For example, if the Embarked column
    has the three classes C, Q, and S, corresponding to Cherbourg, Queenstown, and
    Southampton, assigning the numbers 0, 1, and 2 to these would implicitly tell
    the model that the value of Queenstown is between the values of Cherbourg and
    Southampton, which is not necessarily true. A complex model may be able to deal
    with this implicit ordering, but simpler models (such as linear models, for example)
    will suffer. We’d like to make these values more independent of each other, and
    this is where one-hot encoding comes in.'
- en: 'One-hot encoding works in the following way: First, we look at how many classes
    the feature has and build as many new columns. For example, a column with two
    categories, female and male, would turn it into two columns, one for female and
    one for male. We can call these columns gender_male and gender_female for clarity.
    Then, we look at each passenger. If the passenger is female, then the gender_female
    column will have a 1, and the gender_male column will have a 0\. If the passenger
    is male, then we do the opposite.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'One-hot encoding works in the following way: First, we look at how many classes
    the feature has and build as many new columns. For example, a column with two
    categories, female and male, would turn it into two columns, one for female and
    one for male. We can call these columns gender_male and gender_female for clarity.
    Then, we look at each passenger. If the passenger is female, then the gender_female
    column will have a 1, and the gender_male column will have a 0\. If the passenger
    is male, then we do the opposite.'
- en: What if we have a column with more classes, such as the embarked column? Because
    that column has three classes (C for Cherbourg, Q for Queenstown, and S for Southampton),
    we simply make three columns called embarked_c, embarked_q, and embarked_s. In
    that way, if a passenger embarked in, say, Southampton, the third column will
    have a 1 and the other two a 0\. This process is illustrated in figure 13.1.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: What if we have a column with more classes, such as the embarked column? Because
    that column has three classes (C for Cherbourg, Q for Queenstown, and S for Southampton),
    we simply make three columns called embarked_c, embarked_q, and embarked_s. In
    that way, if a passenger embarked in, say, Southampton, the third column will
    have a 1 and the other two a 0\. This process is illustrated in figure 13.1.
- en: '![](../Images/13-1.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-1.png)'
- en: Figure 13.1 One-hot encoding our data to turn it all into numbers for the machine
    learning model to read. On the left, we have columns with categorical features
    such as gender or port of embarkment. On the right, we have turned these categorical
    features into numerical features.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: Figure 13.1 One-hot encoding our data to turn it all into numbers for the machine
    learning model to read. On the left, we have columns with categorical features
    such as gender or port of embarkment. On the right, we have turned these categorical
    features into numerical features.
- en: 'The Pandas function `get_dummies` helps us with one-hot encoding. We use it
    to create some new columns, then we attach these columns to the dataset, and we
    must not forget to remove the original column, because that information is redundant.
    Next is the code to do one-hot encoding in the gender and the embarked columns:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas函数`get_dummies`帮助我们进行独热编码。我们用它来创建一些新列，然后将这些列附加到数据集上，我们绝不能忘记删除原始列，因为那个信息是冗余的。接下来是进行性别和登船列独热编码的代码：
- en: '[PRE15]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Creates columns with the one-hot encoded columns
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建包含独热编码列的列
- en: ❷ Concatenates the dataset with the newly created columns
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数据集与新创建的列连接
- en: ❸ Deletes the old columns from the dataset
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从数据集中删除旧列
- en: Sometimes this process can be expensive. Imagine having a column with 500 classes.
    That will add 500 new columns to our table! Not only that, but the rows will be
    very sparse, namely, they will contain mostly zeroes. Now imagine if we had many
    columns with hundreds of classes each—our table would become too big to handle.
    In this case, as a data scientist, use your criteria to make a decision. If there
    is enough computing power and storage space to handle thousands or perhaps millions
    of columns, then one-hot encoding is no problem. If these resources are limited,
    perhaps we can broaden our classes to produce fewer columns. For example, if we
    had a column with 100 animal types, we can lump them into six columns formed by
    mammals, birds, fish, amphibians, invertebrates, and reptiles.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有时这个过程可能会很昂贵。想象一下有一个有500个类别的列。这将向我们的表格中添加500个新列！不仅如此，行将非常稀疏，即它们将包含大部分零。现在想象如果我们有很多每个有数百个类别的列——我们的表格将变得太大而无法处理。在这种情况下，作为数据科学家，使用你的标准来做出决定。如果有足够的计算能力和存储空间来处理数千甚至数百万个列，那么独热编码就没有问题。如果这些资源有限，也许我们可以将类别合并以产生更少的列。例如，如果我们有一个有100种动物类型的列，我们可以将它们合并成六个列，分别由哺乳动物、鸟类、鱼类、两栖动物、无脊椎动物和爬行动物组成。
- en: Can we one-hot encode numerical features? If so, why would we want to?
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否对数值特征进行独热编码？如果是的话，我们为什么要这样做？
- en: 'Clearly, if a feature has categories such as male or female, our best strategy
    is to one-hot encode it. However, there are some numerical features for which
    we still may want to consider one-hot encoding. Let’s look, for example, at the
    Pclass column. This column has the classes 0, 1, and 2, for first, second, and
    third class. Should we keep it as a numerical feature, or should we one-hot encode
    it as three features, Pclass1, Pclass2, and Pclass3? This is certainly debatable,
    and we can make good arguments on both sides. One may argue that we don’t want
    to unnecessarily enlarge a dataset if it doesn’t give the model a potential improvement
    in performance. There is a rule of thumb that we can use to decide whether to
    split a column into several columns. We can ask ourselves: is this feature directly
    correlated to the outcome? In other words, does increasing the value of the feature
    make it more likely (or less likely) for a passenger to survive? One would imagine
    that perhaps the higher the class, the more likely a passenger survived. Let’s
    see if this is the case by doing some counting (see code in the notebook), shown
    next:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，如果一个特征有男性或女性等类别，我们最好的策略是对其进行独热编码。然而，还有一些数值特征，我们仍然可能想要考虑进行独热编码。让我们以Pclass列为例。这个列有0、1、2三个类别，分别对应头等舱、二等舱和三等舱。我们应该将其保留为数值特征，还是将其独热编码为三个特征，Pclass1、Pclass2和Pclass3？这当然是有争议的，我们可以在两个方面都提出很好的论据。有人可能会争论，如果我们不希望无谓地扩大数据集，如果这不会给模型带来性能上的潜在提升，我们就不想这样做。我们可以使用一个经验法则来决定是否将一个列拆分成几个列。我们可以问自己：这个特征是否与结果直接相关？换句话说，增加这个特征的价值是否会增加乘客生存的可能性？人们可能会想象，等级越高，乘客生存的可能性就越大。让我们通过一些计数来验证这一点（参见笔记本中的代码），如下所示：
- en: In first class, 62.96% of the passengers survived.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在头等舱中，62.96%的乘客幸存。
- en: In second class, 40.38% of the passengers survived.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在二等舱中，40.38%的乘客幸存。
- en: In third class, 55% of the passengers survived.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在三等舱中，55%的乘客幸存。
- en: 'Notice that the lowest possibility of survival is for the passengers in second
    class. Therefore, it is not true that increasing (or decreasing) the class automatically
    improves the chances of survival. For this reason, I suggest one-hot encoding
    this feature as follows:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，第二等舱乘客的生存可能性最低。因此，增加（或减少）舱位并不一定自动提高生存机会。出于这个原因，我建议将这个特征独热编码如下：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Turning numerical data into categorical data (and why would we want to do this?):
    Binning'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 将数值数据转换为分类数据（以及我们为什么要这样做）：分箱
- en: In the previous section, we learned to turn categorical data into numerical
    data. In this section, we see how to go in the other direction. Why would we ever
    want this? Let’s look at an example.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们学习了如何将分类数据转换为数值数据。在本节中，我们将看到如何进行相反的操作。我们为什么要这样做呢？让我们看看一个例子。
- en: 'Let’s look at the Age column. It’s nice and numerical. A machine learning model
    answers the following question: “How much does age determine survival in the *Titanic*?”
    Imagine that we have a linear model for survival. Such a model would end up with
    one of the following two conclusions:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看年龄列。它很棒，是数值型的。机器学习模型回答以下问题：“年龄在多大程度上决定了在*泰坦尼克号*上的生存？”想象一下，我们有一个用于生存的线性模型。这样的模型最终会得出以下两个结论之一：
- en: The older the passenger is, the more likely they are to survive.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘客越老，他们生存的可能性就越大。
- en: The older the passenger is, the less likely they are to survive.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 乘客越老，他们生存的可能性就越小。
- en: However, is this always the case? What if the relationship between age and survival
    is not as straightforward? What if the highest possibility of survival is when
    the passenger is between 20 and 30, and it’s low for all other age groups. What
    if the lowest possibility of survival is between 20 and 30? We need to give the
    model all the freedom to determine which ages determine whether a passenger is
    more or less likely to survive. What can we do?
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这总是如此吗？如果年龄和生存之间的关系并不那么简单直接呢？如果乘客在20到30岁之间生存的可能性最高，而其他所有年龄组的可能性都很低呢？如果最低的生存可能性是在20到30岁之间呢？我们需要给模型所有的自由来决定哪些年龄决定了乘客更有可能或不太可能生存。我们能做什么？
- en: 'Many nonlinear models can deal with this, but we should still modify the Age
    column to something that gives the model more freedom to explore the data. A useful
    technique we can do is to bin the ages, namely, split them into several different
    buckets. For example, we can turn the age column into the following:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 许多非线性模型可以处理这个问题，但我们应该仍然修改年龄列，使其给模型更多的自由来探索数据。我们可以采取的一个有用技术是将年龄进行分组，即，将它们分成几个不同的桶。例如，我们可以将年龄列转换为以下形式：
- en: From 0 to 10 years old
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从0岁到10岁
- en: From 11 to 20 years old
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从11岁到20岁
- en: From 21 to 30 years old
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从21岁到30岁
- en: From 31 to 40 years old
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从31岁到40岁
- en: From 41 to 50 years old
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从41岁到50岁
- en: From 51 to 60 years old
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从51岁到60岁
- en: From 61 to 70 years old
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从61岁到70岁
- en: From 71 to 80 years old
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从71岁到80岁
- en: 81 years old or older
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 81岁或以上
- en: 'This is similar to one-hot encoding, in the sense that it will turn the Age
    column into nine new columns. The code to do this follows:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这与独热编码类似，因为它会将年龄列转换为九个新的列。执行此操作的代码如下：
- en: '[PRE17]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Feature selection: Getting rid of unnecessary features'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择：去除不必要的特征
- en: 'In the subsection “Dropping columns with missing data,” we dropped some columns
    in our table, because they had too many missing values. However, we should also
    drop some other columns, because they are not necessary for our model, or even
    worse, they may completely ruin our model! In this section, we discuss which features
    should be dropped. But before that, take a look at the features and think of which
    of them would be bad for our model. Here they are again:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 在“删除缺失数据的列”的子节中，我们在我们的表中删除了一些列，因为它们有太多的缺失值。然而，我们也应该删除一些其他列，因为它们对于我们的模型不是必要的，甚至更糟，它们可能会完全破坏我们的模型！在本节中，我们将讨论哪些特征应该被删除。但在那之前，先看看这些特征，想想哪一个可能会对我们的模型有害。它们如下所示：
- en: '**PassengerID**: a unique number corresponding to every passenger'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**PassengerID**：对应每个乘客的唯一编号'
- en: '**Name**: the full name of the passenger'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Name**：乘客的全名'
- en: '**Sex (two categories)**: the gender of the passenger as male or female'
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Sex（两个类别）**：乘客的性别，男性或女性'
- en: '**Age (several categories)**: the age of the passenger as an integer'
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Age（几个类别）**：乘客的年龄，以整数表示'
- en: '**Pclass (several categories)**: the class in which the passenger was traveling:
    first, second, or third'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Pclass（几个类别）**：乘客所乘坐的等级：头等、二等或三等'
- en: '**SibSP**: the number of siblings and spouse of the passenger (0 if the passenger
    is traveling alone)'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**SibSP**：乘客的兄弟姐妹和配偶的数量（如果乘客独自旅行，则为0）'
- en: '**Parch**: the number of parents and children of the passenger (0 if the passenger
    is traveling alone)'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Parch**：乘客的父母和孩子的数量（如果乘客独自旅行，则为0）'
- en: '**Ticket**: the ticket number'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Ticket**：票号'
- en: '**Fare**: the fare the passenger paid in British pounds'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Fare**：乘客支付的票价（以英镑为单位）'
- en: '**Cabin**: the cabin in which the passenger was traveling'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Cabin**：乘客所乘坐的客舱'
- en: '**Embarked**: the port in which the passenger embarked: C for Cherbourg, Q
    for Queenstown, and S for Southampton'
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Embarked**：乘客登船的港口：C代表瑟堡，Q代表皇后镇，S代表南安普顿'
- en: '**Survived**: information whether the passenger survived (1) or not (0)'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**Survived**：乘客是否幸存的信息（1表示幸存，0表示未幸存）'
- en: 'First, let’s look at the name feature. Should we consider it in our model?
    Absolutely not, and here is the reason: every passenger has a different name (perhaps
    with some very few exceptions, which are not significant). Therefore, the model
    would be trained to simply learn the names of the passengers who survived, and
    it wouldn’t be able to tell us anything about new passengers whose names it hasn’t
    seen. This model is memorizing the data—it is not learning anything meaningful
    about its features. This means it heavily overfits, and, therefore, we should
    completely get rid of the Name column.'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看名字特征。我们应该在我们的模型中考虑它吗？绝对不应该，原因如下：每位乘客都有一个不同的名字（可能有一些非常少的例外，但它们并不重要）。因此，模型将训练成仅仅学习幸存乘客的名字，而无法告诉我们关于它尚未见过的新的乘客的名字。这个模型只是在记忆数据——它并没有从其特征中学习到任何有意义的东西。这意味着它过度拟合了，因此，我们应该完全去掉“名字”这一列。
- en: 'The ticket and the PassengerID features have the same problem as the name feature,
    because there is a unique one for each passenger. We’ll remove those two columns
    as well. The `drop` function will help us do this, as shown next:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 票据和乘客ID特征与名字特征有同样的问题，因为每个乘客都有一个独特的值。我们也将去掉这两个列。`drop`函数将帮助我们完成这项工作，如下所示：
- en: '[PRE18]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: What about the Survived feature—shouldn’t we get rid of that one, too? Definitely!
    Keeping the Survived column in our dataset while training will overfit, because
    the model will simply use this feature to determine whether the passenger survived.
    This is like cheating on a test by looking at the solution. We won’t remove it
    yet from the dataset, because we will remove it when we split the dataset into
    features and labels later for training.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 关于幸存特征——我们不应该也去掉它吗？当然！在训练数据集时保留幸存列会导致过度拟合，因为模型将简单地使用这个特征来确定乘客是否幸存。这就像在考试中作弊一样，看答案。我们不会现在从数据集中去掉它，因为我们在稍后分割数据集为特征和标签用于训练时将会去掉它。
- en: As usual, we can save this dataset in the csv file preprocessed_titanic_data.csv
    for use in the next section.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们可以将这个数据集保存为csv文件preprocessed_titanic_data.csv，以便在下一节中使用。
- en: Training our models
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 训练我们的模型
- en: 'Now that our data has been preprocessed, we can start training different models
    on the data. What models should we choose from the ones we’ve learned in this
    book: decision trees, support vector machines, logistic classifiers? The answer
    to this lies in evaluating our models. In this section, we see how to train several
    different models, evaluate them on a validation dataset, and pick the best model
    that fits our dataset.'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对数据进行了预处理，我们可以开始在不同模型上训练数据。我们应该从这本书中学到的哪些模型中选择：决策树、支持向量机、逻辑分类器？这个答案在于评估我们的模型。在本节中，我们将看到如何训练几个不同的模型，在验证数据集上评估它们，并选择最适合我们数据集的最佳模型。
- en: As usual, we load our data from the file where we saved it in the previous section,
    as shown next. We’ll call it `data`.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如往常一样，我们从上一节保存数据所在的文件中加载数据，如下所示。我们将它称为`data`。
- en: '[PRE19]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Table 13.3 contains the first few rows of the preprocessed data. Notice that
    not all the columns are displayed because there are 27 of them.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.3包含了预处理数据的前几行。请注意，并不是所有列都显示出来，因为总共有27列。
- en: Table 13.3 The first five rows of our preprocessed data, ready to be fed into
    the model. Note that it has 21 columns, many more than before. These extra columns
    were created when we one-hot encoded and binned several of the existing features.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.3 我们预处理数据的前五行，准备输入到模型中。请注意，它有21列，比之前多得多。这些额外的列是在我们对一些现有特征进行独热编码和分箱时创建的。
- en: '| Survived | SibSp | Parch | Fare | Sex_female | Sex_male | Pclass_C | Pclass_Q
    | Pclass_S | Pclass_U | ... | Categorized_age_(10, 20] |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| Survived | SibSp | Parch | Fare | Sex_female | Sex_male | Pclass_C | Pclass_Q
    | Pclass_S | Pclass_U | ... | Categorized_age_(10, 20] |'
- en: '| 0 | 1 | 0 | 7.25000 | 0 | 1 | 0 | 0 | 1 | 0 | ... | 0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 1 | 0 | 7.25000 | 0 | 1 | 0 | 0 | 1 | 0 | ... | 0 |'
- en: '| 1 | 1 | 0 | 71.2833 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 71.2833 | 1 | 0 | 1 | 0 | 0 | 0 | ... | 0 |'
- en: '| 1 | 0 | 0 | 7.9250 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 7.9250 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 |'
- en: '| 1 | 1 | 0 | 53.1000 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 1 | 0 | 53.1000 | 1 | 0 | 0 | 0 | 1 | 0 | ... | 0 |'
- en: '| 0 | 0 | 0 | 8.0500 | 0 | 1 | 0 | 0 | 1 | 0 | ... | 0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 8.0500 | 0 | 1 | 0 | 0 | 1 | 0 | ... | 0 |'
- en: aside If you run the code from the notebook, you may get different numbers.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白 如果你运行笔记本中的代码，你可能会得到不同的数字。
- en: Splitting the data into features and labels, and training and validation
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 将数据分为特征和标签，以及训练和验证
- en: Our dataset is a table with the features and labels together. We need to perform
    two splits. First, we need to separate the features from the labels to feed this
    to the model. Next, we need to form a training and a testing set. This is what
    we cover in this subsection.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据库是一个包含特征和标签的表格。我们需要执行两个分割。首先，我们需要将特征从标签中分离出来，以便将其输入到模型中。接下来，我们需要形成一个训练集和一个测试集。这就是本小节所涵盖的内容。
- en: 'To split the dataset into two tables called `features` and `labels`, we use
    the `drop` function as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将数据集分割成两个称为`features`和`labels`的表格，我们使用`drop`函数如下：
- en: '[PRE20]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Next, we split the data into training and validation sets. We’ll use 60% of
    our data for training, 20% for validation, and 20% for testing. For splitting
    the data, we use the Scikit-Learn function `train_test_split`. In this function,
    we specify the percentage of data we want for validation with the `test_size`
    parameter. The output is the four tables called `features_train, features_test,
    labels_train, labels_test`.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分为训练集和验证集。我们将使用60%的数据进行训练，20%的数据进行验证，20%的数据进行测试。对于数据分割，我们使用Scikit-Learn函数`train_test_split`。在这个函数中，我们使用`test_size`参数指定我们想要的验证数据百分比。输出是四个称为`features_train,
    features_test, labels_train, labels_test`的表格。
- en: 'If we wanted to split our data into 80% training and 20% testing, we would
    use the following code:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要将数据分成80%的训练数据和20%的测试数据，我们将使用以下代码：
- en: '[PRE21]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'However, because we want 60% training, 20% validation, and 20% testing, we
    need to use the `train_test_split` function twice: once for separating the training
    data, and once for splitting the validation and testing sets, as shown here:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，因为我们想要60%的训练数据，20%的验证数据和20%的测试数据，我们需要两次使用`train_test_split`函数：一次用于分离训练数据，另一次用于分割验证集和测试集，如下所示：
- en: '[PRE22]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: aside You may see that in the notebook we have specified a fixed `random_state`
    in this function. The reason is that `train_test_split` shuffles the data when
    it splits it. We fix the random state to make sure we always get the same split.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白 你可能会发现在笔记本中我们在这个函数中指定了一个固定的`random_state`。原因是`train_test_split`在分割数据时会打乱数据。我们固定随机状态以确保我们总是得到相同的分割。
- en: We can check the lengths of these DataFrames, and notice that the length of
    the training set is 534, of the validation set is 178, and of the testing set
    is 179\. Now, recall from chapter 4 that the Golden Rule is to never use our testing
    data for training or for making decisions on our models. Therefore, we’ll save
    the test set for the very end, when we’ve decided what model to use. We’ll use
    the training set for training the models and the validation set for taking any
    decisions on what model to choose.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以检查这些数据框的长度，并注意到训练集的长度是534，验证集的长度是178，测试集的长度是179。现在，回想一下第4章中提到的黄金法则，那就是永远不要使用我们的测试数据来训练或对我们的模型做出决策。因此，我们将测试集留到最后，当我们决定使用什么模型时再使用。我们将使用训练集来训练模型，并使用验证集来决定选择哪个模型。
- en: Training several models on our dataset
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集上训练几个模型
- en: 'We’re finally getting to the fun part: training the models! In this section,
    we see how to train several different models in Scikit-Learn in just a few lines
    of code.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于到了有趣的部分：训练模型！在本节中，我们将看到如何在Scikit-Learn中仅用几行代码就训练几个不同的模型。
- en: 'First, we start by training a logistic regression model. We can do this by
    creating an instance of `LogisticRegression` and using the `fit` method, as follows:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们开始训练一个逻辑回归模型。我们可以通过创建一个`LogisticRegression`实例并使用`fit`方法来实现，如下所示：
- en: '[PRE23]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Let’s also train a decision tree, a naive Bayes model, a support vector machine,
    a random forest, a gradient boosted tree, and an AdaBoost model, as shown in the
    next code:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再训练一个决策树、一个朴素贝叶斯模型、一个支持向量机、一个随机森林、一个梯度提升树和一个AdaBoost模型，如下面的代码所示：
- en: '[PRE24]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Which model is better? Evaluating the models
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 哪个模型更好？评估模型
- en: 'Now that we’ve trained some models, we need to select the best one. In this
    section, we use different metrics to evaluate them using the validation sets.
    Recall that in chapter 4 we learned accuracy, recall, precision, and *F*[1]-score.
    To refresh your memory, the definitions follow:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经训练了一些模型，我们需要选择最好的一个。在本节中，我们使用不同的指标来评估它们，使用验证集进行评估。回想一下，在第4章中我们学习了准确率、召回率、精确率和*F*
    [1] 分数。为了刷新你的记忆，定义如下：
- en: accuracy The ratio between the number of correctly labeled points and the total
    number of points.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率 正确标记的点数与总点数的比率。
- en: recall Among the points with positive labels, the proportion of them that are
    correctly classified. In other words, Recall = TP / (TP + FN), where TP is the
    number of true positives and FN the number of false negatives.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 回收率 在具有正标签的点中，正确分类的点的比例。换句话说，回收率 = 真阳性 / (真阳性 + 假阴性)，其中真阳性是真实阳性的数量，假阴性是假阴性的数量。
- en: precision Among the points that have been classified as positive, the proportion
    of them that are correctly classified. In other words, Precision = TP / (TP +
    FP), where FP is the number of false positives.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 精确率 在被分类为正类的点中，正确分类的点的比例。换句话说，精确率 = 真阳性 / (真阳性 + 假阳性)，其中假阳性是假阳性的数量。
- en: F[1]-score The harmonic mean of precision and recall. This is a number between
    precision and recall, but it is closer to the smaller of the two.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: F[1]-分数 精确率和回收率的调和平均数。这是一个介于精确率和回收率之间的数字，但它更接近两者中的较小者。
- en: Testing each model’s accuracy
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 测试每个模型的准确率
- en: 'Let’s start by evaluating the accuracy of these models. The `score` function
    in Scikit-Learn will do this, as shown next:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先评估这些模型的准确率。Scikit-Learn中的`score`函数将执行此操作，如下所示：
- en: '[PRE25]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'We calculate it for all the other models and get the following results, which
    I’ve rounded to two figures (see the notebook for the whole procedure):'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为所有其他模型计算准确率，并得到以下结果，我已经将其四舍五入到两位数（有关整个过程的详细信息，请参阅笔记本）：
- en: Accuracy
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率
- en: '**Logistic regression**: 0.77'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**: 0.77'
- en: '**Decision tree**: 0.78'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**: 0.78'
- en: '**Naive Bayes**: 0.72'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**: 0.72'
- en: '**SVM**: 0.68'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**: 0.68'
- en: '**Random forest**: 0.7875'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**: 0.7875'
- en: '**Gradient boosting**: 0.81'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升**: 0.81'
- en: '**AdaBoost**: 0.76'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaBoost**: 0.76'
- en: This hints that the best model in this dataset is a gradient boosted tree, because
    it gives us the highest accuracy on the validation set (81%, which is good for
    the Titanic dataset). This is not surprising, because this algorithm normally
    performs very well.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这表明在这个数据集中，最佳模型是梯度提升树，因为它在验证集上给出了最高的准确率（81%，这对于泰坦尼克号数据集来说是个好成绩）。这并不令人惊讶，因为这个算法通常表现非常好。
- en: You can follow a similar procedure to calculate the recall, precision, and *F*[1]
    score. I will let you do it for recall and precision, and we will do it together
    for *F*[1] score.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以遵循类似的程序来计算回收率、精确率和F[1]分数。我将让您计算回收率和精确率，我们将一起计算F[1]分数。
- en: Testing each model’s F[1]-score
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 测试每个模型的F[1]-分数
- en: 'Here is how we check the *F*[1] score. First, we have to output the predictions
    of the model, using the `predict` function, and then we use the `f1_score` function,
    as follows:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 这是检查F[1]分数的方法。首先，我们必须使用`predict`函数输出模型的预测，然后我们使用`f1_score`函数，如下所示：
- en: '[PRE26]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Uses the model to make the predictions
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用模型进行预测
- en: ❷ Calculates the F[1] score
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算F[1]分数
- en: 'As before, we can do this for all the models, and get the following:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们可以为所有模型执行此操作，并得到以下结果：
- en: '**F**[1]**-Score**'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '**F**[1]**-分数**'
- en: '**Logistic regression**: 0.69'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**逻辑回归**: 0.69'
- en: '**Decision tree**: 0.71'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**决策树**: 0.71'
- en: '**Naive Bayes**: 0.63'
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**朴素贝叶斯**: 0.63'
- en: '**Support vector machine**: 0.42'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**支持向量机**: 0.42'
- en: '**Random forest**: 0.68'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**随机森林**: 0.68'
- en: '**Gradient boosting**: 0.74'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**梯度提升**: 0.74'
- en: '**AdaBoost**: 0.69'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**AdaBoost**: 0.69'
- en: Again, the gradient boosted tree won with an *F*[1]-score of 0.74\. Given the
    fact that its numbers were much higher than the other models, we can safely conclude
    that among these eight models, gradient boosted trees is the best one. Notice
    that the tree-based models did well in general, which is not surprising, given
    the high nonlinearity of the dataset. It would be interesting to train a neural
    network and an XGBoost model on this dataset, too, and I encourage you to do so!
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，梯度提升树以0.74的F[1]-分数获胜。鉴于其数值远高于其他模型，我们可以安全地得出结论，在这八个模型中，梯度提升树是最佳选择。请注意，基于树的模型总体表现良好，这在数据集高度非线性这一点上并不令人惊讶。训练一个神经网络和一个XGBoost模型在这个数据集上也会很有趣，我鼓励你这样做！
- en: Testing the model
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 测试模型
- en: 'After comparing the models using the validation set, we have finally made up
    our minds, and we have chosen the gradient boosted tree as the best model for
    this dataset. Don’t be surprised; gradient boosted trees (and their close cousin,
    XGBoost) win most competitions. But to see if we really did well or if we accidentally
    overfit, we need to give this model its final test: we need to test the model
    in the test set that we haven’t touched yet.'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用验证集比较模型后，我们最终做出了决定，并选择了梯度提升树作为这个数据集的最佳模型。不要感到惊讶；梯度提升树（及其近亲XGBoost）在大多数比赛中获胜。但为了看看我们是否真的做得很好，或者我们是否意外地过拟合，我们需要对这个模型进行最后的测试：我们需要在尚未接触过的测试集上测试这个模型。
- en: 'First, let’s evaluate the accuracy, as follows:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们评估准确率，如下所示：
- en: '[PRE27]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And now let’s look at the *F*[1]-score, shown next:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看*F*[1]分数，如下所示：
- en: '[PRE28]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: These scores are quite good for the Titanic dataset. Thus, we can comfortably
    say that our model is good.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 这些分数对于泰坦尼克号数据集来说相当不错。因此，我们可以放心地说我们的模型是好的。
- en: However, we trained these models without touching their hyperparameters, which
    means that Scikit-Learn picked some standard hyperparameters for them. Is there
    a way we can find the best hyperparameters for a model? In the next section, we
    learn that there is.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们训练这些模型时没有触及它们的超参数，这意味着Scikit-Learn为它们选择了某些标准超参数。我们有没有办法找到模型的最佳超参数？在下一节中，我们将了解到这一点。
- en: 'Tuning the hyperparameters to find the best model: Grid search'
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调整超参数以找到最佳模型：网格搜索
- en: In the previous section, we trained several models and found that the gradient
    boosting tree performed best among them. However, we didn’t explore many different
    combinations of hyperparameters, so we have room to improve in our training. In
    this section, we see a useful technique to search among many combinations of hyperparameters
    to find a good model for our data.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们训练了几个模型，并发现梯度提升树在它们中表现最好。然而，我们没有探索许多不同的超参数组合，所以我们还有改进训练的空间。在本节中，我们看到了一种有用的技术，可以在许多超参数组合中搜索，以找到适合我们数据的好模型。
- en: The performance of the gradient boosted tree was about as high as one can obtain
    for the Titanic dataset, so let’s leave that one alone. The poor SVM, however,
    performed last, with an accuracy of 69% and an *F*[1]-score of 0.42\. We believe
    in SVMs, however, because they are a powerful machine learning model. Perhaps
    the bad performance of this SVM is due to the hyperparameters it is using. There
    may be a better combination of them that works.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升树在泰坦尼克号数据集上的性能几乎达到了可能达到的最高水平，所以我们不妨让它保持原样。然而，性能较差的SVM表现最差，准确率为69%，*F*[1]分数为0.42。然而，我们仍然相信SVMs，因为它们是一个强大的机器学习模型。也许这个SVM表现不佳是由于它使用的超参数。可能存在更好的超参数组合，它们可以工作得更好。
- en: aside Throughout this section, we make some choices of parameters. Some are
    based on experience, some on standard practices, and some are arbitrary. I encourage
    you to try following a similar procedure with any choices you decide to make and
    to try to beat the current scores of the models!
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 旁白：在本节中，我们做出了一些参数选择。有些是基于经验，有些是基于标准实践，还有些是任意的。我鼓励你尝试使用你决定的任何选择遵循类似的程序，并尝试超越当前模型的分数！
- en: To improve the performance of our SVM, we use a method called *grid search*,
    which consists of training our model several times over different combinations
    of the hyperparameters and selecting the one that performs best on our validation
    set.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 为了提高我们SVM的性能，我们使用了一种称为*网格搜索*的方法，它包括在不同超参数组合上多次训练我们的模型，并选择在验证集上表现最好的那个。
- en: Let’s begin by picking a kernel. In practice, I have found that the RBF (radial
    basis functions) kernel tends to perform well, so let’s select that one. Recall
    from chapter 9 that the hyperparameter that goes with the RBF kernel is gamma,
    which is a real number. Let’s try to train an SVM with two values for gamma, namely,
    1 and 10\. Why 1 and 10? Normally when we search for hyperparameters, we tend
    to do an exponential search, so we would try values such as 0.1, 1, 10, 100, 1000,
    and so on, as opposed to 1, 2, 3, 4, 5\. This exponential search covers a larger
    space and gives us better chances of finding good hyperparameters, and doing this
    type of search is standard practice among data scientists.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从选择一个核函数开始。在实践中，我发现径向基函数（RBF）核通常表现良好，所以我们选择这个。回顾第9章，与RBF核相关的超参数是gamma，它是一个实数。让我们尝试用两个gamma值来训练SVM，即1和10。为什么是1和10？通常当我们搜索超参数时，我们倾向于进行指数搜索，所以我们可能会尝试0.1、1、10、100、1000等值，而不是1、2、3、4、5。这种指数搜索覆盖了更大的空间，给我们找到良好超参数的机会更大，而且这种搜索在数据科学家中是一种标准实践。
- en: 'Recall again from chapter 9 that another hyperparameter associated to SVMs
    is the C parameter. Let’s also try to train models with C = 1 and C = 10\. This
    gives us the following four possible models to train:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 再次回顾第9章，SVMs关联的另一个超参数是C参数。让我们也尝试用C = 1和C = 10来训练模型。这给我们提供了以下四个可能的模型来训练：
- en: '**Model 1**: kernel = RBF, gamma = 1, C = 1'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型1**：核函数 = RBF，gamma = 1，C = 1'
- en: '**Model 2**: kernel = RBF, gamma = 1, C = 10'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型2**：核函数 = RBF，gamma = 1，C = 10'
- en: '**Model 3**: kernel = RBF, gamma = 10, C = 1'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型3**：核函数 = RBF，gamma = 10，C = 1'
- en: '**Model 4**: kernel = RBF, gamma = 10, C = 10'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型4**：核函数 = RBF，gamma = 10，C = 10'
- en: 'We can easily train all of those in our training set with the following eight
    lines of code:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用以下八行代码轻松地在我们的训练集中训练所有这些：
- en: '[PRE29]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Now we evaluate them, using accuracy (another arbitrary choice—we could also
    use *F*[1]-score, precision, or recall). The scores are recorded in table 13.4.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们使用准确率（另一个任意选择——我们也可以使用*F*[1]-分数、精确率或召回率）来评估它们。这些分数记录在表13.4中。
- en: Table 13.4 The grid search method is useful to search over many combinations
    of hyperparameters and pick the best model. Here we use a grid search to pick
    the best combination of parameters C and gamma in an SVM. We used accuracy to
    compare the models in the validation set. Notice that the best model among these
    is the one with gamma = 0.1 and C = 10, with an accuracy of 0.72.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 表13.4 网格搜索方法对于搜索许多超参数组合并选择最佳模型非常有用。在这里，我们使用网格搜索来选择SVM中参数C和gamma的最佳组合。我们使用准确率来比较验证集中的模型。请注意，在这些模型中，最佳模型是gamma
    = 0.1且C = 10的模型，准确率为0.72。
- en: '|  | C = 1 | C = 10 |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  | C = 1 | C = 10 |'
- en: '| gamma = 0.1 | 0.69 | 0.72 |'
  id: totrans-254
  prefs: []
  type: TYPE_TB
  zh: '| gamma = 0.1 | 0.69 | 0.72 |'
- en: '| gamma = 1 | 0.70 | 0.70 |'
  id: totrans-255
  prefs: []
  type: TYPE_TB
  zh: '| gamma = 1 | 0.70 | 0.70 |'
- en: '| gamma = 10 | 0.67 | 0.65 |'
  id: totrans-256
  prefs: []
  type: TYPE_TB
  zh: '| gamma = 10 | 0.67 | 0.65 |'
- en: Notice from table 13.4 that the best accuracy is 0.72, given by the model with
    gamma = 0.1 and C = 1\. This is an improvement from the 0.68 we obtained previously
    when we didn’t specify any hyperparameters.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 从表13.4中可以看出，最佳准确率为0.72，由gamma = 0.1和C = 1的模型给出。这比我们之前未指定任何超参数时获得的0.68有所改进。
- en: If we had many more parameters, we simply make a grid with them and train all
    the possible models. Notice that as we explore more choices, the number of models
    rapidly increases. For example, if we wanted to explore five values for gamma
    and four values for C, we would have to train 20 models (five times four). We
    can also add more hyperparameters—for example, if there was a third hyperparameter
    that we wanted to try, and there were seven values for this one, we would have
    to train a total of 140 models (five times four times seven). As the number of
    models grows so quickly, it is important to pick the choices in such a way that
    it explores the hyperparameter space well, without training a huge number of models.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有很多更多的参数，我们只需用它们制作一个网格，并训练所有可能的模型。请注意，随着我们探索更多的选择，模型的数量会迅速增加。例如，如果我们想要探索gamma的五个值和C的四个值，我们就必须训练20个模型（五个乘以四个）。我们还可以添加更多的超参数——例如，如果有一个我们想要尝试的第三个超参数，并且这个超参数有七个值，我们就必须总共训练140个模型（五个乘以四个乘以七个）。由于模型的数量增长如此之快，因此选择选择的方式非常重要，这样它就能很好地探索超参数空间，而无需训练大量模型。
- en: 'Scikit-Learn offers a simple way to do this: using the `GridSearchCV` object.
    First, we define the hyperparameters as a dictionary, where the key of the dictionary
    is the name of the parameter and the value corresponding to this key is the list
    of values we want to try for our hyperparameter. In this case, let’s explore the
    following combinations of hyperparameters:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn提供了一个简单的方法来做这件事：使用`GridSearchCV`对象。首先，我们将超参数定义为一个字典，其中字典的键是参数的名称，与该键对应的值是我们想要尝试的超参数值的列表。在这种情况下，让我们探索以下超参数组合：
- en: '**Kernel**: RBF'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '**核函数**：RBF'
- en: '**C**: 0.01, 0.1, 1, 10, 100'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: '**C**：0.01, 0.1, 1, 10, 100'
- en: '**gamma**: 0.01, 0.1, 1, 10, 100'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: '**gamma**：0.01, 0.1, 1, 10, 100'
- en: 'The following code will do it:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码将完成这项工作：
- en: '[PRE30]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ A dictionary with the hyperparameters and the values we want to try
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个包含我们想要尝试的超参数及其值的字典
- en: ❷ A regular SVM with no hyperparameters
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个没有超参数的常规SVM
- en: ❸ A GridSearchCV object where we pass the SVM and the hyperparameter dictionary
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个包含SVM和超参数字典的GridSearchCV对象
- en: ❹ We fit the GridSearchCV model in the same way that we fit a regular model
    in Scikit-Learn.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们以与在Scikit-Learn中拟合常规模型相同的方式拟合GridSearchCV模型。
- en: 'This trains 25 models with all the combinations of hyperparameters given in
    the hyperparameter dictionary. Now, we pick the best of these models and call
    it `svm_winner`. Let’s calculate the accuracy of this model on the validation
    set as follows:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 这将训练25个模型，这些模型包含超参数字典中给出的所有超参数组合。现在，我们从中选择最佳模型，并将其称为`svm_winner`。让我们按照以下方式计算这个模型在验证集上的准确率：
- en: '[PRE31]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Our winning model achieved an accuracy of 0.73, which is better than the original
    0.68\. We could still improve this model by running a larger hyperparameter search,
    and I encourage you to try it yourself. For now, let’s explore which hyperparameters
    were used by the last winning SVM model, as shown here:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的最佳模型达到了0.73的准确率，这比原始的0.68要好。我们仍然可以通过运行更大的超参数搜索来改进这个模型，我鼓励你自己尝试。现在，让我们探索最后获胜的SVM模型使用了哪些超参数，如下所示：
- en: '[PRE32]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The winning model used an RBF kernel with gamma = 0.01 and C = 10.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 获胜模型使用了gamma = 0.01和C = 10的RBF核。
- en: 'challenge I encourage you to try using grid search on other models, and see
    how much you can improve the accuracy and *F*[1]-score of the winning model! If
    you have a good score, run it on the Kaggle dataset and submit your predictions
    using this link: [https://www.kaggle.com/c/titanic/submit](https://www.kaggle.com/c/titanic/submit).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 挑战我鼓励你尝试在其他模型上使用网格搜索，看看你能提高多少获胜模型的准确性和*F*[1]分数！如果你有一个好分数，请在Kaggle数据集上运行它，并使用此链接提交你的预测：[https://www.kaggle.com/c/titanic/submit](https://www.kaggle.com/c/titanic/submit)。
- en: 'One more thing: what is that *CV* at the end of `GridSearchCV`? It stands for
    *cross-validation*, which we learn in the next section.'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一件事：`GridSearchCV`结尾的*CV*是什么意思？它代表*交叉验证*，我们将在下一节中学习。
- en: Using K-fold cross-validation to reuse our data as training and validation
  id: totrans-276
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用K折交叉验证来重复使用我们的数据作为训练和验证
- en: In this section, we learn an alternative to the traditional training-validation-testing
    method we’ve been using in this chapter. It is called *k-fold cross validation,*
    and it is useful in many situations, especially when our dataset is small.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了一种替代本章中使用的传统训练-验证-测试方法的替代方法。它被称为*k*-折交叉验证，它在许多情况下都很有用，尤其是在我们的数据集较小的情况下。
- en: 'Throughout this example, we used 60% of our data for training, 20% for validation,
    and a final 20% for testing. That works in practice, but it seems that we are
    losing some data, right? We end up training the model with only 60% of our data,
    which may hurt our model, especially when the dataset is small. *K*-fold cross-validation
    is a way to use all the data for training and testing, by recycling it several
    times. It works as follows:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在整个这个例子中，我们使用了60%的数据进行训练，20%进行验证，最后的20%用于测试。这在实践中是可行的，但似乎我们丢失了一些数据，对吧？我们最终只使用60%的数据来训练模型，这可能会损害我们的模型，尤其是在数据集较小的情况下。*K*-折交叉验证是一种通过多次回收数据来使用所有数据用于训练和测试的方法。它的工作原理如下：
- en: Split the data into *k* equal (or almost equal) portions.
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分成*k*个相等（或几乎相等）的部分。
- en: Train the model *k* times, using the union of *k* – 1 of the portions as the
    training set and the remaining one as a validation set.
  id: totrans-280
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用*k*次训练模型，使用*k* - 1个部分作为训练集，剩下的一个作为验证集。
- en: The final score of that model is the average of the validation scores from the
    *k* steps.
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 该模型的最终分数是*k*个步骤中验证分数的平均值。
- en: Figure 13.2 shows a picture of fourfold cross-validation.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2展示了四折交叉验证的图片。
- en: '![](../Images/13-2.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/13-2.png)'
- en: Figure 13.2 *K*-fold cross validation is a useful method that recycles our data
    to use it both as training and as validation. On the top, we see the classical
    training-validation split. On the bottom, we see an illustration of fourfold cross-validation
    in which we split the data into four equal (or almost equal) portions. We then
    train our model four times. Each time we pick three of the portions as the training
    set, and the remaining one as the validation set. The score of this model is the
    average of the four scores obtained on each of the validation sets.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 *K*-折交叉验证是一种有用的方法，它回收我们的数据，使其既用于训练又用于验证。在顶部，我们看到经典的训练-验证分割。在底部，我们看到一个四折交叉验证的示意图，我们将数据分成四个相等（或几乎相等）的部分。然后我们训练我们的模型四次。每次我们选择三个部分作为训练集，剩下的一个作为验证集。这个模型的分数是四个验证集上获得的四个分数的平均值。
- en: This method is what is used in `GridSearchCV`, and the results of this process
    can be examined by typing `svm_gs.cv_results_`. We won’t show the results here,
    because they are very long, but you can take a look at them in the notebook.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是在`GridSearchCV`中使用的，可以通过输入`svm_gs.cv_results_`来检查这个过程的结果。我们在这里不会展示结果，因为它们非常长，但你可以查看笔记本中的结果。
- en: Summary
  id: totrans-286
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Pandas is a useful Python package to open, manipulate, and save datasets.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pandas是一个有用的Python包，用于打开、操作和保存数据集。
- en: Cleaning up our data is necessary, because it may come with problems such as
    missing values.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 清理我们的数据是必要的，因为它可能存在一些问题，如缺失值。
- en: Features can be numerical or categorical. Numerical features are numbers, such
    as age. Categorical features are categories, or types, such as dog/cat/bird.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征可以是数值的或分类的。数值特征是数字，例如年龄。分类特征是类别，或类型，例如狗/猫/鸟。
- en: Machine learning models take only numbers, so to feed categorical data to a
    machine learning model, we must turn it into numerical data. One way to do this
    is by one-hot encoding.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习模型只接受数字，因此为了将分类数据输入到机器学习模型中，我们必须将其转换为数值数据。一种方法是通过独热编码。
- en: In some situations, we may want to treat our numerical features as categorical
    features as well, which we can do by binning the data.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在某些情况下，我们可能希望将我们的数值特征也视为分类特征，我们可以通过数据分箱来实现这一点。
- en: It is important to use feature selection to remove unnecessary features in our
    data.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用特征选择来去除我们数据中的不必要特征是很重要的。
- en: Scikit-Learn is a useful package to train, test, and evaluate machine learning
    models.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Scikit-Learn是一个用于训练、测试和评估机器学习模型的实用包。
- en: Before training the model, we must split the data into training, validation,
    and testing. We have Pandas functions for doing this.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练模型之前，我们必须将数据分成训练、验证和测试。我们有Pandas函数来完成这项工作。
- en: Grid search is a method used to find the best hyperparameters for a model. It
    consists of training several models over a (sometimes large) set of hyperparameters.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网格搜索是一种用于寻找模型最佳超参数的方法。它包括在（有时是大量）超参数集上训练多个模型。
- en: '*K*-fold cross-validation is a method used to recycle the data and use it as
    training and validation as well. It consists of training and testing several models
    on different portions of the data.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*K*-fold交叉验证是一种用于回收数据并将其作为训练和验证使用的方法。它包括在不同数据部分上训练和测试多个模型。'
- en: Exercises
  id: totrans-297
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 13.1
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 练习13.1
- en: The repository contains a file called test.csv. This is a file with more passengers
    on the *Titanic*, except it doesn’t have the Survived column.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 仓库中包含一个名为test.csv的文件。这是一个包含更多乘客的*泰坦尼克号*的文件，但它没有“Survived”列。
- en: Preprocess the data in this file as we did in this chapter.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照本章所述，预处理这个文件中的数据。
- en: Use any of the models to predict labels in this dataset. According to your model,
    how many passengers survived?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用任何模型来预测这个数据集的标签。根据你的模型，你认为有多少乘客幸存了？
- en: Comparing the performance of all the models in this chapter, how many passengers
    from the test set would you think actually survived?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较本章中所有模型的性能，你认为测试集中有多少乘客实际上幸存了？
