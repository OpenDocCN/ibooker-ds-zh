- en: '18 Deploying Kubernetes: Multinode and multiarchitecture clusters'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 18 部署 Kubernetes：多节点和多架构集群
- en: You can do an awful lot with Kubernetes without understanding the architecture
    of the cluster and how all the pieces fit together—you already have in the previous
    17 chapters. But that additional knowledge will help you understand what high
    availability looks like in Kubernetes and what you need to think about if you
    want to run your own cluster. The best way to learn about all of the Kubernetes
    components is to install a cluster from scratch, and that’s what you’ll do in
    this chapter. The exercises start with plain virtual machines and walk you through
    a basic Kubernetes setup for a multinode cluster, which you can use to run some
    of the sample apps you’re familiar with from the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在不了解集群架构以及所有组件如何协同工作的情况下做很多事情——您在前 17 章中已经做到了这一点。但额外的知识将帮助您了解 Kubernetes
    中的高可用性是什么样的，以及如果您想运行自己的集群，您需要考虑什么。了解所有 Kubernetes 组件的最好方式是从头开始安装集群，这正是本章要做的。练习从普通的虚拟机开始，引导您完成多节点集群的基本
    Kubernetes 设置，您可以使用它来运行书中熟悉的一些示例应用程序。
- en: Every app we’ve run so far has used Linux containers built for Intel 64-bit
    processors, but Kubernetes is a multiarchitecture platform. A single cluster can
    have nodes with different operating systems and different types of CPU, so you
    can run a variety of workloads. In this chapter, you’ll also add a Windows Server
    node to your cluster and run some Windows applications. That part is optional,
    but if you’re not a Windows user, it’s worth following through those exercises
    to see how Kubernetes uses the same modeling language for different architectures,
    with just a few tweaks to the manifests.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们运行的所有应用程序都使用了为 Intel 64 位处理器构建的 Linux 容器，但 Kubernetes 是一个多架构平台。单个集群可以包含具有不同操作系统和不同类型
    CPU 的节点，因此您可以运行各种工作负载。在本章中，您还将向您的集群添加一个 Windows Server 节点并运行一些 Windows 应用程序。这部分是可选的，但如果您不是
    Windows 用户，那么跟随这些练习了解 Kubernetes 如何使用相同的建模语言来处理不同的架构，只需对清单进行一些调整，这也是值得的。
- en: 18.1 What’s inside a Kubernetes cluster?
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.1 Kubernetes 集群内部有什么？
- en: You’ll need some different tools to follow along in this chapter and some virtual
    machine images, which will take a while to download. You can start the installation
    now, and by the time we’ve finished looking at the architecture of Kubernetes,
    you should be ready to go.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要一些不同的工具来跟随本章的内容，以及一些虚拟机镜像，下载这些镜像需要一些时间。您现在可以开始安装，等到我们完成对 Kubernetes 架构的查看时，您应该已经准备好开始使用了。
- en: Try it now You’ll use Vagrant, a free open source tool for VM management, to
    run the virtual machines. You also need to have a VM runtime; you can use VirtualBox,
    Hyper-V (on Windows), or Parallels (on macOS) for these machines. Install Vagrant,
    and then download the base VM images you’ll use for your cluster.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧！您将使用 Vagrant，这是一个用于虚拟机管理的免费开源工具，来运行虚拟机。您还需要一个虚拟机运行时；您可以使用 VirtualBox、Hyper-V（在
    Windows 上）或 Parallels（在 macOS 上）来运行这些机器。安装 Vagrant，然后下载您将用于集群的基础虚拟机镜像。
- en: '[PRE0]'
  id: totrans-6
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Okay, while that’s happening, it’s time to learn about the architecture of Kubernetes.
    You know that a cluster is composed of one or more servers called nodes, but those
    nodes can play different roles. There’s the Kubernetes *control plane*, which
    is the management side of the cluster (previously called master nodes), and then
    *nodes*, which run your workloads (these were once called minions). At a high
    level, the control plane is the thing that receives your kubectl deployment requests,
    and it actions them by scheduling Pods on the nodes. Figure 18.1 shows the cluster
    at the user-experience level.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，当这一切在进行时，是时候学习 Kubernetes 的架构了。您知道集群由一个或多个称为节点的服务器组成，但这些节点可以扮演不同的角色。有 Kubernetes
    的 *控制平面*，这是集群的管理方面（之前称为主节点），然后是 *节点*，它们运行您的工作负载（这些曾经被称为从节点）。在较高层次上，控制平面是接收您的 kubectl
    部署请求的东西，并通过在节点上调度 Pods 来执行这些操作。图 18.1 展示了从用户体验级别的集群。
- en: '![](../Images/18-1.jpg)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![图 18-1](../Images/18-1.jpg)'
- en: Figure 18.1 From the user perspective, this is one cluster with management and
    application endpoints.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.1 从用户的角度来看，这是一个包含管理和应用端点的集群。
- en: 'In a managed Kubernetes platform in the cloud, the control plane is taken care
    of for you, so you only need to worry about your own nodes (in AKS, the control
    plane is completely abstracted so you see—and pay for—only worker nodes). That’s
    one reason why a managed platform is so attractive—the control plane contains
    multiple components, and you need to manage those if you run your own environment.
    The following components are all critical for the cluster to function:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中的托管Kubernetes平台上，控制平面由您负责，因此您只需关注自己的节点（在AKS中，控制平面完全抽象，您只能看到并支付工作节点）。这就是托管平台如此吸引人的一个原因——控制平面包含多个组件，如果您运行自己的环境，则需要管理这些组件。以下组件对于集群的正常运行至关重要：
- en: The API *server* is the management interface. It’s a REST API on an HTTPS endpoint,
    which you connect to with kubectl and which Pods can use internally. It runs in
    the kube-apiserver Pod, and it can be scaled up for high availability.
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: API *服务器*是管理接口。它是一个HTTPS端点的REST API，您可以通过kubectl连接到它，Pod也可以内部使用。它运行在kube-apiserver
    Pod中，并且可以扩展以实现高可用性。
- en: The *scheduler* watches for new Pod requests and selects a node to run them.
    It runs in the kube-scheduler Pod, but it’s a pluggable component—you can deploy
    your own custom scheduler.
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度器*监视新的Pod请求并选择一个节点来运行它们。它运行在kube-scheduler Pod中，但它是一个可插拔组件——您可以部署自己的自定义调度器。'
- en: The *controller manager* runs core controllers, which are internal components,
    not visible controllers like Deployments. The kube-controller-manager Pod runs
    controllers that observe node availability and manage Service endpoints.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*控制器管理器*运行核心控制器，这些是内部组件，不是像Deployments这样的可见控制器。kube-controller-manager Pod运行观察节点可用性和管理服务端点的控制器。'
- en: '*etcd* is the Kubernetes data store where all cluster data is stored. It’s
    a distributed key-value database, which replicates data across many instances.'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*etcd*是Kubernetes数据存储，所有集群数据都存储在这里。它是一个分布式键值数据库，数据在多个实例之间进行复制。'
- en: You need to run multiple control plane nodes for high availability. Use an odd
    number to support failures, so if the manager node goes down, the remaining nodes
    can vote for a new manager. Each control plane node runs an instance of all of
    these components, as shown in figure 18.2\. The API server is load-balanced, and
    the backend data is replicated, so any control plane node can work on a request,
    and it will be processed in the same way.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要运行多个控制平面节点以实现高可用性。使用奇数以支持故障，因此如果管理节点宕机，剩余的节点可以投票选举一个新的管理节点。每个控制平面节点运行所有这些组件的实例，如图18.2所示。API服务器是负载均衡的，后端数据是复制的，因此任何控制平面节点都可以处理请求，并且将以相同的方式进行处理。
- en: '![](../Images/18-2.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-2.jpg)'
- en: Figure 18.2 Production clusters need multiple control plane nodes for high availability.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.2 生产集群需要多个控制平面节点以实现高可用性。
- en: Your downloads should be nearly done now, so we’ll look at just one more level
    of detail—the components running on each node. Nodes are responsible for creating
    Pods and ensuring their containers keep running and for connecting Pods to the
    Kubernetes network.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 您的下载应该现在几乎完成了，所以我们将查看每个节点上运行的组件的更详细的一级——节点负责创建Pod并确保其容器持续运行，以及将Pod连接到Kubernetes网络。
- en: The *kubelet* is a background agent that runs on the server—not in a Pod or
    in a container. It receives requests to create Pods and manages the Pod life cycle
    and also sends heartbeats to the API server confirming the node is healthy.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*kubelet*是一个在服务器上运行的背景代理——不在Pod或容器中。它接收创建Pod的请求，管理Pod的生命周期，并向API服务器发送心跳以确认节点健康。'
- en: The *kube-proxy* is the network component that routes traffic between Pods or
    from Pods to the outside world. It runs as a DaemonSet, with a Pod on each node
    managing that node’s traffic.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*kube-proxy*是网络组件，负责在Pod之间或从Pod到外部世界路由流量。它作为一个DaemonSet运行，每个节点上的Pod管理该节点的流量。'
- en: The *container runtime*, used by the kubelet to manage Pod containers. Typically
    Docker, containerd, or CRI-O, this is pluggable with any runtime that supports
    CRI (the Container Runtime Interface).
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由kubelet用于管理Pod容器的*容器运行时*。通常是Docker、containerd或CRI-O，这是可插拔的，可以与支持CRI（容器运行时接口）的任何运行时一起使用。
- en: Figure 18.3 shows the internal components on each node, and these all run on
    the control plane nodes, too.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3显示了每个节点上的内部组件，这些组件也运行在控制平面节点上。
- en: '![](../Images/18-3.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-3.jpg)'
- en: Figure 18.3 The next level of detail for the nodes—the kubelet, kube-proxy,
    and container runtime
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.3 节点的下一级细节——kubelet、kube-proxy和容器运行时
- en: You can see that Kubernetes is a complex platform with lots of moving pieces.
    These are just the core components; there’s also Pod networking, DNS, and, in
    the cloud, a separate cloud controller manager, which integrates with cloud services.
    You can deploy and manage your own Kubernetes cluster with 100% open source components,
    but you need to be aware of the complexity you’re taking on. And that’s more than
    enough theory—let’s go build a cluster.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以看到，Kubernetes是一个由许多组件组成的复杂平台。这些只是核心组件；还有Pod网络、DNS，以及在云中，一个独立的云控制器管理器，它集成了云服务。你可以使用100%的开源组件来部署和管理自己的Kubernetes集群，但你需要意识到你将要承担的复杂性。而且理论已经足够多了——让我们去构建一个集群。
- en: 18.2 Initializing the control plane
  id: totrans-26
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.2 初始化控制平面
- en: Most of the work in deploying Kubernetes is taken care of by a tool called kubeadm.
    It’s an administration command line that can initialize a new control plane, join
    nodes to the cluster, and upgrade the Kubernetes version. Before you get to kubeadm,
    you need to install several dependencies. In a production environment, you’d have
    these already installed in your VM image, but to show you how it works, we’ll
    start from scratch.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署Kubernetes的大部分工作都由一个名为kubeadm的工具来完成。它是一个管理命令行工具，可以初始化新的控制平面，将节点加入集群，并升级Kubernetes版本。在你使用kubeadm之前，你需要安装几个依赖项。在生产环境中，你会在虚拟机镜像中已经安装了这些依赖项，但为了展示它是如何工作的，我们将从头开始。
- en: Try it now Run a Linux VM, which will be the control plane node and install
    all of the Kubernetes dependencies. If you’re using Hyper-V on Windows, you’ll
    need to run your shell as administrator.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 运行一个Linux虚拟机，它将成为控制平面节点，并安装所有的Kubernetes依赖项。如果你在Windows上使用Hyper-V，你需要以管理员身份运行你的shell。
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Figure 18.4 gives you the highlights—creating the VM, running the setup script,
    and verifying all the tools are there. If you inspect the `linux-setup.sh` script
    in this chapter’s source, you’ll see that it installs Docker and the Kubernetes
    tools and sets a couple of memory and network configurations for the server.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4展示了重点——创建虚拟机、运行设置脚本，并验证所有工具都已安装。如果你检查本章源代码中的`linux-setup.sh`脚本，你会看到它安装了Docker和Kubernetes工具，并为服务器设置了一些内存和网络配置。
- en: '![](../Images/18-4.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/18-4.jpg)'
- en: Figure 18.4 Kubeadm is the tool that sets up the cluster, but it needs the container
    runtime and kubelet.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.4 Kubeadm是设置集群的工具，但它需要容器运行时和kubelet。
- en: 'Now this machine is ready to become a Kubernetes control plane node. After
    all the build-up to get here, the next exercise will be an anticlimax: you need
    to run only a single command to initialize the cluster and get all the control
    plane components running. Keep an eye on the output, and you’ll see all the pieces
    you learned about in section 18.1 starting up.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在这台机器已经准备好成为Kubernetes控制平面节点了。经过所有这些准备工作，接下来的练习将会很平淡：你只需要运行一个命令来初始化集群并启动所有控制平面组件。注意输出，你将看到你在18.1节中学到的所有组件开始启动。
- en: Try it now Use kubeadm to initialize a new cluster with a fixed set of network
    addresses for Pods and Services.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 使用kubeadm初始化一个新的集群，并为Pod和Service分配一组固定的网络地址。
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output from initializing the cluster tells you what to do next, including
    the command you’ll run on other nodes to join the cluster (you’ll need that for
    later exercises, so be sure to copy it into a text file somewhere). The command
    also generates a config file, which you can use to manage the cluster with kubectl.
    You can see in figure 18.5 that the cluster exists with a single control plane
    node, but the node isn’t in the Ready status.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化集群的输出会告诉你下一步该做什么，包括你将在其他节点上运行的命令以加入集群（你将在后面的练习中需要它，所以请确保将其复制到某个文本文件中）。该命令还会生成一个配置文件，你可以使用它通过kubectl管理集群。你可以在图18.5中看到，集群存在，但只有一个控制平面节点，而这个节点不在Ready状态。
- en: '![](../Images/18-5.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/18-5.jpg)'
- en: 'Figure 18.5 Initializing the cluster is simple: kubeadm starts all the control
    plane components.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.5初始化集群很简单：kubeadm启动所有控制平面组件。
- en: 'The cluster isn’t ready because it doesn’t have a Pod network installed. You
    know from chapter 16 that Kubernetes has a network plugin model, and different
    plugins have different capabilities. We used Calico in that chapter to demonstrate
    network policy enforcement, and in this chapter, we’ll use flannel (another open
    source option), because it has the most mature support for clusters with mixed
    architectures. You install flannel in the same way as Calico: with a Kubernetes
    manifest you apply on the control plane node.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 集群尚未准备好，因为它没有安装Pod网络。你从第16章知道Kubernetes有一个网络插件模型，不同的插件有不同的功能。在第16章中，我们使用Calico来演示网络策略执行，而在本章中，我们将使用flannel（另一个开源选项），因为它对混合架构集群的支持最为成熟。你将以与Calico相同的方式安装flannel：在控制平面节点上应用Kubernetes清单。
- en: Try it now Add a network plugin to your new cluster, using a ready-made flannel
    manifest.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 向您的新的集群添加一个网络插件，使用现成的flannel清单。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Kubeadm deploys the DNS server for the cluster as Pods in the `kube-system`
    namespace, but those Pods can’t start until the network plugin is running. As
    soon as flannel is deployed, the DNS Pods start and the node is ready. My output
    in figure 18.6 shows the multiarchitecture support in flannel. If you want to
    add an IBM mainframe to your Kubernetes cluster, you can do that.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeadm在`kube-system`命名空间中以Pod的形式部署集群的DNS服务器，但那些Pod在网络插件运行之前无法启动。一旦flannel部署完成，DNS
    Pods就会启动，节点就准备好了。我在图18.6中的输出显示了flannel的多架构支持。如果你想将IBM大型机添加到你的Kubernetes集群中，你可以做到。
- en: '![](../Images/18-6.jpg)'
  id: totrans-43
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-6.jpg)'
- en: Figure 18.6 Kubeadm doesn’t deploy a Pod network; flannel is a good multiarchitecture
    option.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.6 Kubeadm不部署Pod网络；flannel是一个好的多架构选项。
- en: And that’s all you need for the control plane. I’ve glossed over the network
    setup—the IP address ranges in the kubeadm command use the configuration flannel
    expects—but you’ll need to plan that out so it works with your own network. And
    I skipped over the other 25 options for kubeadm, but you’ll want to research them
    if you’re serious about managing your own cluster. Right now, you have a simple
    single-node cluster. You can’t use it for application workloads yet because the
    default setup restricts the control plane nodes, so they run only system workloads.
    Next we’ll add another node and run some apps.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 对于控制平面，你需要的就这些。我略过了网络设置——kubeadm命令中使用的IP地址范围是flannel期望的配置——但你需要规划好，以便与你的网络兼容。而且我跳过了kubeadm的其他25个选项，但如果你认真管理自己的集群，你将需要研究它们。目前，你有一个简单的单节点集群。你还不能用它来运行应用程序工作负载，因为默认设置限制了控制平面节点，它们只能运行系统工作负载。接下来，我们将添加另一个节点并运行一些应用程序。
- en: 18.3 Adding nodes and running Linux workloads
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.3 添加节点和运行Linux工作负载
- en: The output of the kubeadm initialization gives you the command you need to run
    on other servers to join them to the cluster. New Linux nodes require the same
    setup as the control plane node, with a container runtime and all the Kubernetes
    tools. In the next exercise, you’ll create another VM and install the prerequisites
    using the same setup script you used for the control plane.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: kubeadm初始化的输出会给你运行在其他服务器上的命令，以便将它们加入集群。新的Linux节点需要与控制平面节点相同的设置，包括容器运行时和所有Kubernetes工具。在下一个练习中，你将创建另一个虚拟机，并使用与控制平面相同的设置脚本安装先决条件。
- en: Try it now Create a second VM with Vagrant, and install the prerequisites for
    it to join the Kubernetes cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 使用Vagrant创建第二个虚拟机，并安装它加入Kubernetes集群所需的先决条件。
- en: '[PRE4]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Figure 18.7 is pretty much the same as figure 18.5 except for the machine name,
    so you can see that the setup script should really be part of the VM provisioning.
    That way, when you spin up a new machine with Vagrant (or Terraform or whatever
    tool works for your infrastructure), it will have all the prerequisites and will
    be ready to join the cluster.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7几乎与图18.5相同，只是机器名称不同，因此你可以看到设置脚本确实应该是虚拟机配置的一部分。这样，当你使用Vagrant（或Terraform或其他适用于您基础设施的工具）启动新机器时，它将具备所有先决条件，并准备好加入集群。
- en: '![](../Images/18-7.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-7.jpg)'
- en: Figure 18.7 Installing required dependencies—nodes need the same initial setup
    as the control plane.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.7 安装所需依赖项——节点需要与控制平面相同的初始设置。
- en: Now you can double the size of your Kubernetes cluster by joining a new node.
    The output of the kubeadm `init` command contains everything you need—a CA certificate
    hash so the new node can trust the control plane and a `join` token so the control
    plane allows the new server to join. The `join` token is sensitive, and you need
    to distribute it securely to stop rogue nodes from joining your cluster. Any machine
    with network access to the control plane and the token could potentially join.
    Your new VM is in the same virtual network as the control plane VM, so all you
    need to do is run the `join` command.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以通过加入一个新节点来将你的Kubernetes集群的大小加倍。kubeadm `init`命令的输出包含你需要的一切——一个CA证书哈希，以便新节点可以信任控制平面，以及一个`join`令牌，以便控制平面允许新服务器加入。`join`令牌是敏感的，你需要安全地分发它以防止恶意节点加入你的集群。任何可以访问控制平面和令牌的网络机器都可能加入。你的新虚拟机与控制平面虚拟机位于同一虚拟网络中，所以你只需要运行`join`命令。
- en: Try it now Join the cluster using kubeadm and the `join` command from the control
    plane.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 使用kubeadm和从控制平面执行的`join`命令加入集群。
- en: '[PRE5]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: In this exercise, you’ll see logs from the kubelet about TLS bootstrapping.
    The control plane generates TLS certificates for new nodes so the kubelet can
    authenticate with the API server. You can customize the kubeadm install to provide
    your own certificate authority, but that’s another layer of detail (including
    certificate renewal and external CAs) that we won’t cover here. Figure 18.8 shows
    my new node successfully joining the cluster with the simple default setup.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会看到kubelet关于TLS引导的日志。控制平面为新节点生成TLS证书，以便kubelet可以与API服务器进行身份验证。你可以自定义kubeadm安装以提供自己的证书颁发机构，但这又是一个额外的细节层（包括证书续订和外部CA），我们在这里不会讨论。图18.8显示我的新节点成功使用简单的默认设置加入集群。
- en: '![](../Images/18-8.jpg)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图18-8](../Images/18-8.jpg)'
- en: Figure 18.8 Joining a node to the cluster sets up secure communication to the
    control plane.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.8 将节点加入集群设置了与控制平面的安全通信。
- en: 'The new node runs its own subset of the components already running on the control
    plane. The kubelet runs as a background process outside of Kubernetes, communicating
    with Docker, which also runs as a background process. It runs two more components
    in Pods: the network proxy and the network plugin. All the other components—DNS,
    the controller manager, and the API server—are specific to control plane nodes
    and won’t run on standard nodes. Switch back to the control plane node, and you
    can see the Pods.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 新节点运行控制平面已运行的组件的子集。kubelet作为Kubernetes之外的背景进程运行，与Docker通信，Docker也作为背景进程运行。它在Pod中运行另外两个组件：网络代理和网络插件。所有其他组件——DNS、控制器管理器和API服务器——都是针对控制平面节点特定的，不会在标准节点上运行。切换回控制平面节点，你就可以看到Pods。
- en: Try it now Kubectl is set up only on the control plane, although you can share
    the config file to connect from another machine. Switch back to that control plane
    node to see all the cluster resources.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 Kubectl仅在控制平面设置，尽管你可以共享配置文件从另一台机器连接。切换回那个控制平面节点以查看所有集群资源。
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: My output is shown in figure 18.9\. You should see the same, unless the flannel
    Pod is still starting up, in which case, the new node won’t be ready yet.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出显示在图18.9中。除非法兰绒豆还在启动中，否则你应该看到相同的结果，在这种情况下，新节点可能还没有准备好。
- en: '![](../Images/18-9.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图18-9](../Images/18-9.jpg)'
- en: Figure 18.9 DaemonSets run a Pod on every node; the new Pod runs system components
    when it joins.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.9中的DaemonSets在每个节点上运行一个Pod；新Pod在加入时运行系统组件。
- en: You’re good to deploy an app now, but you need to be aware of the limitations
    in this cluster. There is no default storage class set up and no volume provisioners,
    so you won’t be able to deploy dynamic PersistentVolumeClaims (which we covered
    in chapter 6), and you’re stuck using `HostPath` volumes. There’s also no load
    balancer integration so you can’t use LoadBalancer services. In the datacenter,
    you could use network file system (NFS) shares for distributed storage, and a
    project called MetalLB for load balancer support. That’s all too much for this
    chapter, so we’ll stick to simple apps with no storage requirements and use NodePort
    Services to get traffic into the cluster.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以部署应用程序了，但你需要意识到这个集群的限制。没有设置默认的存储类，也没有卷提供程序，所以你无法部署动态持久卷声明（我们在第6章中讨论过），你将只能使用`HostPath`卷。也没有负载均衡器集成，所以你不能使用负载均衡器服务。在数据中心，你可以使用网络文件系统（NFS）共享来实现分布式存储，以及一个名为MetalLB的项目来支持负载均衡器。这一切都超出了本章的范围，所以我们将继续使用没有存储要求的简单应用程序，并使用NodePort服务将流量引入集群。
- en: 'NodePorts are a much simpler type of Service: they work in the same way as
    other Services to distribute traffic to Pods, but they listen for incoming traffic
    on a specific port on the node. Every node listens on the same port, so any server
    can receive a request and route it to the correct Pod, even if that Pod is running
    on a different node. You can use NodePorts in on-premises clusters if you have
    an existing load balancer, but NodePorts are restricted to certain port ranges,
    so your load balancer will need to do some port mapping. Listing 18.1 shows a
    NodePort Service spec for the Astronomy Picture of the Day web application.'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: NodePorts 是一种更简单的服务类型：它们与其他服务一样工作，将流量分发到 Pods，但它们在节点上的特定端口上监听传入的流量。每个节点都监听相同的端口，因此任何服务器都可以接收请求并将其路由到正确的
    Pod，即使该 Pod 在不同的节点上运行。如果您在本地集群中有一个现有的负载均衡器，则可以使用 NodePorts，但 NodePorts 限制在特定的端口范围内，因此您的负载均衡器需要进行一些端口映射。列表
    18.1 展示了用于每日天文图片 web 应用的 NodePort 服务规范。
- en: Listing 18.1 web.yaml, a Service exposed as a NodePort
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.1 web.yaml，一个以 NodePort 暴露的服务
- en: '[PRE7]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The APOD application contains three components, and the Service type is the
    only difference between this spec and the others we’ve deployed. When you run
    the app, you might expect Kubernetes to distribute Pods all around the cluster,
    but remember that control plane nodes are isolated from user workloads by default.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: APOD 应用包含三个组件，与我们已经部署的其他规范相比，服务类型是唯一的区别。当您运行应用程序时，您可能会期望 Kubernetes 将 Pods 分布在集群的各个地方，但请记住，默认情况下控制平面节点与用户工作负载是隔离的。
- en: Try it now Deploy the app to your new cluster, and see where the Pods are scheduled
    to run.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 将应用程序部署到您的新集群，并查看 Pods 被调度到哪个节点运行。
- en: '[PRE8]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You can see in figure 18.10 that every Pod is scheduled to run on the same node.
    That’s a new VM with none of the container images for the book available, so it
    will download them from Docker Hub. The Pods will be in the ContainerCreating
    status while that happens. The largest image for this app is just over 200 MB,
    so it shouldn’t take too long to start up.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图 18.10 中看到，每个 Pod 都被调度在相同的节点上。这是一个没有任何本书容器镜像的新 VM，因此它将从 Docker Hub 下载它们。在发生这种情况时，Pod
    将处于 ContainerCreating 状态。此应用程序的最大镜像只有 200 多 MB，因此启动 shouldn’t take too long.
- en: '![](../Images/18-10.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/18-10.jpg)'
- en: Figure 18.10 The user experience is mostly the same for all Kubernetes clusters.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.10 对于所有 Kubernetes 集群，用户体验基本上是相同的。
- en: If this is the first time you’ve used a different Kubernetes cluster from your
    normal lab environment, then it’s now that you see how powerful Kubernetes is.
    This is a completely different setup, probably using a different version of Kubernetes,
    maybe a different container runtime, and a different host operating system. With
    one change to the spec, you can deploy and manage the APOD app in exactly the
    same way you previously did. You could take any of the exercises from this book
    and deploy them here, but you’d have to make changes to Services and volumes so
    they use valid types for the new cluster.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这是您第一次使用与您正常的实验室环境不同的 Kubernetes 集群，那么现在您就可以看到 Kubernetes 的强大之处。这是一个完全不同的设置，可能使用的是
    Kubernetes 的不同版本，可能使用的是不同的容器运行时，以及不同的主机操作系统。通过更改规范的一次，您可以以与之前完全相同的方式部署和管理 APOD
    应用程序。您可以从这本书的任何练习中选择并在这里部署，但您必须更改服务和卷，以便它们使用新集群的有效类型。
- en: Kubernetes stops control plane nodes from running application workloads to make
    sure they aren’t starved of compute resources. If your control plane node is maxing
    CPU while calculating Pi to one million decimal places, there’s nothing left for
    the API server and DNS Pods, and your cluster becomes unusable. You should leave
    that safety guard in place for real clusters, but in a lab setup, you can relax
    it to get the most out of your servers.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 停止控制平面节点运行应用程序工作负载，以确保它们不会因计算资源不足而受饿。如果您的控制平面节点在计算 Pi 到一百万位小数时达到最大
    CPU，那么 API 服务器和 DNS Pods 就没有剩余的资源了，您的集群将变得不可用。您应该在真实集群中保留这个安全防护，但在实验室设置中，您可以放松它以充分利用您的服务器。
- en: Try it now Kubernetes uses taints to classify nodes, and the `master` taint
    prevents application workloads running. Remove that taint, and scale up the app
    to see new Pods being scheduled on the control plane node.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 Kubernetes 使用污点来分类节点，而 `master` 污点阻止应用程序工作负载运行。移除该污点，并扩展应用程序以查看新的 Pods
    被调度到控制平面节点。
- en: '[PRE9]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You’ll learn all about taints and scheduling in chapter 19; for now, it’s enough
    to know that taints are a way of marking particular nodes to prevent Pods running
    on them. Removing the taint makes the control plane node eligible to run application
    workloads, so when you scale up the API deployment, the new Pods are scheduled
    on the control plane node. You can see in figure 18.11 that those Pods are in
    the ContainerCreating status because each node has its own image store, and the
    control plane node needs to download the API image. The size of your container
    images directly affects the speed at which you can scale up in this scenario,
    which is why you need to invest in optimizing your Dockerfiles.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 您将在第 19 章中了解有关污点和调度的所有内容；目前，只需知道污点是一种标记特定节点以防止 Pod 在其上运行的方法。移除污点使控制平面节点有资格运行应用程序工作负载，因此当您扩展
    API 部署时，新的 Pods 将在控制平面节点上调度。您可以在图 18.11 中看到这些 Pods 处于 ContainerCreating 状态，因为每个节点都有自己的镜像存储，控制平面节点需要下载
    API 镜像。您的容器镜像的大小直接影响到您在这个场景中扩展的速度，这也是为什么您需要投资优化您的 Dockerfile 的原因。
- en: '![](../Images/18-11.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-11.jpg)'
- en: Figure 18.11 You can run application Pods on the control plane nodes, but you
    shouldn’t do so in production.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.11 您可以在控制平面节点上运行应用程序 Pod，但在生产环境中不应这样做。
- en: The app is running and the NodePort Service means all nodes are listening on
    port 30000, including the control plane node. If you browse to any node’s IP address,
    you’ll see the APOD app. Your request is directed to the web Pod on the standard
    node, and it makes an API call, which could be directed to a Pod on either node.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序正在运行，NodePort 服务意味着所有节点都在监听端口 30000，包括控制平面节点。如果您浏览到任何节点的 IP 地址，您将看到 APOD
    应用程序。您的请求将被导向标准节点上的 Web Pod，并执行 API 调用，这可能被导向任意节点的 Pod。
- en: Try it now Your Virtual Machine runtime sets up your network so you can access
    the VMs by their IP address. Get the address of either node, and then browse to
    it on your host machine.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 您的虚拟机运行时设置您的网络，以便您可以通过 IP 地址访问虚拟机。获取任意节点的地址，然后在您的宿主机上浏览到它。
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: My output appears in figure 18.12\. The picture I saw when I was working through
    the exercises was much more striking, but I didn’t grab a screenshot, so we’ve
    got this comet instead.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出显示在图 18.12 中。我在做练习时看到的图片要生动得多，但我没有截图，所以我们只有这张彗星图片。
- en: '![](../Images/18-12.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/18-12.jpg)'
- en: Figure 18.12 NodePort and ClusterIP Services span the Pod network so traffic
    can be routed to any node.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.12 NodePort 和 ClusterIP 服务跨越 Pod 网络，以便流量可以路由到任意节点。
- en: Building your own Kubernetes cluster isn’t all that complicated if you’re happy
    to keep it simple, with NodePorts, HostPaths, and maybe NFS volumes. If you want
    to, you can extend this cluster and add more nodes; the Vagrant setup includes
    machine definitions for `kiamol-node2` and `kiamol-node3`, so you can repeat the
    first two exercises in this section with those VM names to build a four-node cluster.
    But that just gives you a boring all-Linux cluster. One of the great benefits
    of Kubernetes is that it can run all sorts of apps. Next, we’ll see how to add
    a different architecture to the cluster—a Windows server—so we can run all-Windows
    or hybrid Linux-Windows apps.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您愿意保持简单，使用 NodePorts、HostPaths 和可能还有 NFS 卷，构建自己的 Kubernetes 集群并不复杂。如果您想扩展这个集群并添加更多节点；Vagrant
    设置包括 `kiamol-node2` 和 `kiamol-node3` 的机器定义，因此您可以使用这些虚拟机名称重复本节的前两个练习来构建一个四节点集群。但这仅仅给您一个无聊的全
    Linux 集群。Kubernetes 的一个巨大好处是它可以运行各种应用程序。接下来，我们将看到如何向集群添加不同的架构——一个 Windows 服务器——这样我们就可以运行全
    Windows 或混合 Linux-Windows 应用程序。
- en: 18.4 Adding Windows nodes and running hybrid workloads
  id: totrans-89
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.4 添加 Windows 节点并运行混合工作负载
- en: The Kubernetes website itself says *Windows applications constitute a large
    portion of the services and applications that run in many organizations*, so bear
    that in mind if you’re thinking of skipping this section. I won’t go into a lot
    of detail about Windows containers and the differences from Linux (for that, you
    can read my book *Docker on Windows*; Packt Publishing, 2019)—just the basics
    so you can see how Windows apps fit in Kubernetes.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 网站本身就说 *Windows 应用程序构成了许多组织中运行的服务和应用程序的大部分*，如果您考虑跳过这一节，请记住这一点。我不会过多地详细介绍
    Windows 容器和与 Linux 的区别（关于这一点，您可以阅读我的书 *Windows 上的 Docker*；Packt 出版，2019）——只是基础知识，以便您了解
    Windows 应用程序如何在 Kubernetes 中适配。
- en: 'Container images are built for a specific architecture: a combination of operating
    system and CPU. Containers use the kernel of the machine they’re running on, so
    that has to match the architecture of the image. You can build a Docker image
    on a Raspberry Pi, but you can’t run it on your laptop because the Pi uses an
    Arm CPU and your laptop uses Intel. It’s the same with the operating system—you
    can build an image on a Windows machine to run a Windows app, but you can’t run
    that in a container on a Linux server. Kubernetes supports different types of
    workload by having nodes with different architectures in the same cluster.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 容器镜像是为特定架构构建的：操作系统和CPU的组合。容器使用它们运行所在机器的内核，因此必须与镜像的架构相匹配。您可以在树莓派上构建Docker镜像，但不能在您的笔记本电脑上运行，因为Pi使用Arm
    CPU而您的笔记本电脑使用Intel。操作系统也是如此——您可以在Windows机器上构建用于运行Windows应用程序的镜像，但不能在Linux服务器上的容器中运行该镜像。Kubernetes通过在同一个集群中拥有不同架构的节点来支持不同类型的工作负载。
- en: You have some restrictions as to how diverse your cluster can be, but the diagram
    in figure 18.13 is something you can genuinely build. The control plane is Linux
    only, but the kubelet and proxy are cross platform. AWS has Arm-powered servers,
    which are almost half the price of Intel equivalents, and you can use them as
    nodes in EKS. If you have a large application suite with some apps that work in
    Linux on Arm, some that need Linux on Intel, and some that are Windows, you can
    run and manage them all in one cluster.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 您对集群的多样性有一些限制，但图18.13中的内容是您可以真正构建的。控制平面仅支持Linux，但kubelet和代理是跨平台的。AWS有基于Arm的服务器，其价格几乎相当于Intel服务器的半价，您可以将它们用作EKS中的节点。如果您有一个大型应用程序套件，其中一些应用程序在Arm上的Linux中运行，一些需要Intel上的Linux，还有一些是Windows系统，您可以在一个集群中运行和管理它们所有。
- en: '![](../Images/18-13.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图18-13](../Images/18-13.jpg)'
- en: Figure 18.13 Who hasn’t got a spare Raspberry Pi and IBM Z mainframe gathering
    dust?
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.13 谁没有闲置的树莓派和积满灰尘的IBM Z大型机？
- en: Let’s get to it and add a Windows node to your cluster. The approach is the
    same as adding a Linux node—spin up a new VM, add a container runtime and the
    Kubernetes tools, and join it to the cluster. Windows Server 2019 is the minimum
    version supported in Kubernetes, and Docker is the only container runtime available
    at the time of writing—containerd support for Windows is in progress.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始操作，将一个Windows节点添加到您的集群中。方法与添加Linux节点相同——启动一个新的虚拟机，添加容器运行时和Kubernetes工具，并将其加入集群。Windows
    Server 2019是Kubernetes支持的最小版本，Docker是当时可用的唯一容器运行时——Windows的containerd支持正在进行中。
- en: Try it now Create a Windows VM, and install the Kubernetes prerequisites. Folder-sharing
    in Vagrant doesn’t always work for Windows, so you’ll download the setup script
    from the book’s source on GitHub.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 创建一个Windows虚拟机，并安装Kubernetes的先决条件。Vagrant中的文件夹共享对于Windows并不总是有效，因此您需要从GitHub上的书籍源下载设置脚本。
- en: '[PRE11]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The Windows setup needs to enable an operating system feature and install Docker;
    the script reboots the VM when that’s done. You can see in figure 18.14 that my
    session is back to the normal command line.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Windows设置需要启用操作系统功能并安装Docker；完成这些操作后，脚本会重启虚拟机。您可以在图18.14中看到，我的会话已经回到了正常的命令行。
- en: '![](../Images/18-14.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图18-14](../Images/18-14.jpg)'
- en: Figure 18.14 The first stage in adding a Windows node is installing the container
    runtime.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 图18.14 添加Windows节点第一阶段是安装容器运行时。
- en: This is only the first part of the setup, because the control plane needs to
    be configured for Windows support. The standard deployment of flannel and the
    kube proxy doesn’t create a DaemonSet for Windows nodes, so we need to set that
    up as an additional step. The new DaemonSets use Windows container images in the
    spec, and the Pods are configured to work with the Windows server to set up networking.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是设置的第一部分，因为控制平面需要配置以支持Windows。标准的flannel和kube proxy部署不会为Windows节点创建DaemonSet，因此我们需要将其作为额外步骤来设置。新的DaemonSets在规范中使用Windows容器镜像，并且Pod被配置为与Windows服务器一起设置网络。
- en: Try it now Deploy the new system components for the Windows node.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 部署Windows节点的新的系统组件。
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Again, this is something you would add to the initial cluster setup if you’re
    serious about running your own hybrid cluster. I’ve kept it as a separate step
    so you can see what needs to change to add Windows support—which you can do with
    an existing cluster, provided it’s running Kubernetes 1.14 or greater. My output
    in figure 18.15 shows the new Windows-specific DaemonSets, with a desired count
    of zero because the Windows node hasn’t joined yet.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，如果你认真考虑运行自己的混合集群，这将是你需要添加到初始集群设置中的内容。我将其作为一个单独的步骤，以便你可以看到需要更改什么以添加 Windows
    支持——只要你运行的集群是 Kubernetes 1.14 或更高版本，你就可以使用现有的集群来实现这一点。图 18.15 中的输出显示了新的特定于 Windows
    的 DaemonSets，期望计数为零，因为 Windows 节点尚未加入。
- en: '![](../Images/18-15.jpg)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.15](../Images/18-15.jpg)'
- en: Figure 18.15 Updating the control plane to schedule system components on Windows
    nodes
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.15 更新控制平面以在 Windows 节点上调度系统组件
- en: Join the Windows node. It will have Pods scheduled for the proxy and the network
    components, so it will download the images and start containers. One difference
    is that Windows containers are more restricted than Linux containers, so the flannel
    setup is slightly different. The Kubernetes Special Interest Group (SIG) for Windows
    publishes a helper script to set up flannel and the kubelet. I have a snapshot
    of that script in the source folder for this chapter, which matches the Kubernetes
    version we’re running; after this second setup script, the node is ready to join.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 加入 Windows 节点。它将为代理和网络组件安排 Pod，因此它将下载镜像并启动容器。一个不同之处在于，Windows 容器比 Linux 容器更受限制，因此
    flannel 设置略有不同。Kubernetes 的 Windows 特别兴趣小组（SIG）发布了一个辅助脚本，用于设置 flannel 和 kubelet。我在本章的源文件夹中有一个该脚本的快照，它与我们在运行的
    Kubernetes 版本相匹配；在第二个设置脚本之后，节点就准备好加入集群了。
- en: Try it now Add the remaining dependencies to the Windows node, and join it to
    the cluster.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看，将剩余的依赖项添加到 Windows 节点，并将其加入集群。
- en: '[PRE13]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: The happy message “this node has joined the cluster” that you see in figure
    18.16 is a bit ahead of itself. The new node needs to download the proxy and network
    images. The flannel image is 5 GB, so it will take a few minutes before the Windows
    node is ready.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 你在图 18.16 中看到的“此节点已加入集群”的愉快信息有点过于乐观。新节点需要下载代理和网络镜像。flannel 镜像为 5 GB，因此 Windows
    节点准备就绪可能需要几分钟。
- en: '![](../Images/18-16.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![图 18.16](../Images/18-16.jpg)'
- en: Figure 18.16 Joining the Windows node uses the same kubeadm command as the Linux
    node.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.16 显示加入 Windows 节点使用与 Linux 节点相同的 kubeadm 命令。
- en: While that’s downloading, we’ll look at modeling applications to run on different
    architectures. Kubernetes doesn’t automatically work out which nodes are suitable
    for which Pods—this isn’t easy to do from the image name alone. Instead, you add
    a selector in your Pod spec to specify the architecture the Pod needs. Listing
    18.2 shows a selector that sets the Pod to run on a Windows node.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 当下载进行时，我们将探讨如何对运行在不同架构上的应用程序进行建模。Kubernetes 不会自动确定哪些节点适合哪些 Pod——仅从镜像名称本身很难做到这一点。相反，你需要在
    Pod 规范中添加一个选择器来指定 Pod 需要的架构。列表 18.2 显示了一个将 Pod 设置为在 Windows 节点上运行的选择器。
- en: Listing 18.2 api.yaml, using a node selector to request a specific operating
    system
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 18.2 api.yaml，使用节点选择器请求特定的操作系统
- en: '[PRE14]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: That’s it. Remember that Pods run on a single node, so if you have multiple
    containers in your Pod spec, they all need to use the same architecture. It’s
    a good practice to include a node selector for every Pod if you’re running a multiarchitecture
    cluster to ensure Pods always end up where they should. You can include an operating
    system, CPU architecture, or both.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 就这些了。记住，Pod 在单个节点上运行，所以如果你的 Pod 规范中有多个容器，它们都需要使用相同的架构。如果你正在运行一个多架构集群，为每个 Pod
    包含一个节点选择器是一个好习惯，以确保 Pod 总是出现在它们应该出现的地方。你可以包括操作系统、CPU 架构或两者都包括。
- en: The listing in 18.2 is for a Windows version of the random-number API, which
    also has a Linux version of the website to go with it. The web spec includes a
    node selector for the Linux OS. You can deploy the app, and the Pods will run
    on different nodes, but the website still accesses the API Pod in the usual way
    through a ClusterIP Service, even though it’s running on Windows.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 18.2 节中的列表是随机数 API 的 Windows 版本，它还附带了一个 Linux 版本的网站。网站规范包括 Linux 操作系统的节点选择器。你可以部署应用程序，Pod
    将在不同的节点上运行，但网站仍然通过 ClusterIP 服务以通常的方式访问 API Pod，即使它运行在 Windows 上。
- en: Try it now This is a hybrid application, with one Windows and one Linux component.
    Both use the same YAML format, and in the specs, only the node selectors show
    they need to run on different architectures.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个 这是一个混合应用程序，包含一个 Windows 组件和一个 Linux 组件。两者都使用相同的 YAML 格式，在规格说明中，只有节点选择器显示它们需要在不同的架构上运行。
- en: '[PRE15]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: It’s a shame the demo app in figure 18.17 is so basic, because this capability
    has taken many years and a ton of effort from the Kubernetes community and the
    engineering teams at Microsoft and Docker.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 真遗憾，图 18.17 中的演示应用程序如此基础，因为这个功能已经花费了 Kubernetes 社区和微软、Docker 的工程团队多年的时间和大量努力。
- en: '![](../Images/18-17.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/18-17.jpg)'
- en: Figure 18.17 A world-class container orchestrator running a hybrid app to generate
    one random number
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.17 一个世界级的容器编排器运行混合应用程序以生成一个随机数
- en: By day, I work as a consultant helping companies adopt container technologies,
    and the pattern here is exactly what many organizations want to do with their
    Windows apps—move them to Kubernetes without any changes, and then gradually break
    up the monolithic architecture with new components running in lightweight Linux
    containers. It’s a pragmatic and low-risk approach to modernizing applications
    that takes full advantage of all the features of Kubernetes, and it gives you
    an easy migration to the cloud.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 白天，我作为顾问帮助公司采用容器技术，这里的模式正是许多组织希望对其 Windows 应用程序做的事情——在不做任何更改的情况下将它们迁移到 Kubernetes，然后逐步通过在轻量级
    Linux 容器中运行的新组件来分解单体架构。这是一种实用且风险低的现代化应用程序的方法，充分利用了 Kubernetes 的所有功能，并为你提供了轻松迁移到云的途径。
- en: 'Way back in chapter 1, I said that Kubernetes runs your applications, but it
    doesn’t really care what those applications are. We’ll prove that with one last
    deployment to the cluster: the Windows Pet Shop app. Microsoft built this demo
    app in 2008 to showcase the latest features of .NET. It uses technologies and
    approaches that have long since been replaced, but the source code is still out
    there, and I’ve packaged it up to run in Windows containers and published the
    images on Docker Hub. This exercise shows you that you really can run decade-old
    applications in Kubernetes without any changes to the code.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一章中，我说 Kubernetes 运行你的应用程序，但它实际上并不关心那些应用程序是什么。我们将通过最后一次部署到集群：Windows 宠物商店应用程序来证明这一点。微软在
    2008 年构建了这个演示应用程序来展示 .NET 的最新功能。它使用的技术和方法早已被取代，但源代码仍然存在，我已经打包好以在 Windows 容器中运行，并在
    Docker Hub 上发布了镜像。这个练习表明，你真的可以在 Kubernetes 中运行十年前的应用程序，而无需对代码进行任何更改。
- en: Try it now Deploy a legacy Windows app. This one downloads more large container
    images, so it will take a while to start.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个 部署一个遗留的 Windows 应用程序。这个应用程序会下载更多的容器镜像，所以启动需要一段时间。
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: There you have it. Figure 18.18 could be faked—but it’s not—and you can run
    this all yourself to prove it. (I will admit it took me two attempts—my Windows
    VM lost network connectivity the first time around, which is most likely a Hyper-V
    issue.) The Pet Shop is an app that last had a code change 12 years ago, now running
    in the latest version of Kubernetes.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样了。图 18.18 可能是伪造的——但不是——你可以自己运行这个来证明它。（我必须承认我试了两次——我的 Windows 虚拟机第一次尝试时失去了网络连接，这很可能是
    Hyper-V 的问题。）宠物商店是一个最后一次代码更改是在 12 年前的应用程序，现在运行在 Kubernetes 的最新版本中。
- en: '![](../Images/18-18.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/18-18.jpg)'
- en: Figure 18.18 I bet there hasn’t been a screenshot of the Pet Shop in a book
    for a long time.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 18.18 我打赌这本书很久没有宠物商店的截图了。
- en: 'That’s as far as we’ll go with this cluster. If you want to add more Windows
    nodes, you can repeat the setup and join exercises from this section for two more
    machines defined in Vagrant: `kiamol-node-win2` and `kiamol-node-win3`. You’ll
    just about squeeze the control plane node, three Linux nodes, and three Windows
    nodes onto your machine if you have a least 16 GB of memory. We’ll finish up with
    a look at the considerations for multinode Kubernetes clusters and the future
    of multiarchitecture.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们就到这里为止这个集群。如果你想添加更多的 Windows 节点，你可以重复本节的设置和加入练习，用于 Vagrant 中定义的两个更多机器：`kiamol-node-win2`
    和 `kiamol-node-win3`。如果你至少有 16 GB 的内存，你几乎可以将控制平面节点、三个 Linux 节点和三个 Windows 节点挤在你的机器上。我们将以查看多节点
    Kubernetes 集群的考虑因素和多架构的未来结束。
- en: 18.5 Understanding Kubernetes at scale
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.5 规模化理解 Kubernetes
- en: Whether you diligently followed the exercises in this chapter or just skimmed
    through, you now have a good idea of how complex it is to set up and manage a
    Kubernetes cluster, and you’ll understand why I recommend Docker Desktop or K3s
    for your lab environment. Deploying a multinode cluster is a great learning exercise
    to see how all the pieces fit together, but it’s not something I’d recommend for
    production.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您是否认真遵循本章中的练习，还是只是浏览了一下，您现在对设置和管理Kubernetes集群的复杂性有了很好的了解，您也会理解为什么我建议在实验室环境中使用Docker
    Desktop或K3s。部署多节点集群是一个很好的学习练习，可以了解所有部件是如何组合在一起的，但这不是我为生产推荐的事情。
- en: Kubernetes is all about high availability and scale, and the more nodes you
    have, the more complex it becomes to manage. You need multiple control plane nodes
    for high availability; if you lose the control plane, your apps keep running on
    the nodes, but you can’t manage them with kubectl, and they’re no longer self-healing.
    The control plane stores all its data in etcd. For better redundancy you can run
    etcd outside of the cluster. For a performance improvement, you can run an additional
    etcd database just to store the events Kubernetes records for objects. It’s all
    starting to look complicated, and we’re still dealing with only a single cluster,
    not high availability across multiple clusters.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的一切都是关于高可用性和可扩展性，您拥有的节点越多，管理起来就越复杂。您需要多个控制平面节点来实现高可用性；如果您丢失了控制平面，您的应用程序将继续在节点上运行，但您无法使用kubectl来管理它们，它们也不再是自我修复的。控制平面将所有数据存储在etcd中。为了更好的冗余，您可以在集群外部运行etcd。为了提高性能，您可以运行一个额外的etcd数据库，专门用于存储Kubernetes为对象记录的事件。事情开始看起来很复杂，但我们仍在处理单个集群，而不是多个集群的高可用性。
- en: 'You can build Kubernetes clusters that run at huge scale: the latest release
    supports up to 5,000 nodes and 150,000 Pods in a single cluster. In practice,
    you’re likely to hit performance issues with etcd or your network plugin when
    you get to around 500 nodes, and then you’ll need to look at scaling parts of
    the control plane independently. The good news is that you can run a cluster with
    hundreds of worker nodes managed by a control plane with just three nodes, provided
    those nodes are fairly powerful. The bad news is you need to manage all that,
    and you’ll need to decide if you’re better off with one large cluster or multiple
    smaller clusters.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以构建运行在巨大规模的Kubernetes集群：最新版本支持单个集群中最多5,000个节点和150,000个Pod。在实践中，当您达到大约500个节点时，您可能会遇到etcd或您的网络插件的性能问题，然后您需要独立扩展控制平面的部分。好消息是，如果您那些节点相当强大，您可以使用仅由三个节点管理的控制平面运行一个拥有数百个工作节点的集群。坏消息是您需要管理所有这些，并且您需要决定您是更倾向于一个大型集群还是多个较小的集群。
- en: The other side of scale is being able to run as much of your application catalog
    as possible on a single platform. You saw in this chapter that you can run a multiarchitecture
    cluster by adding Windows nodes—Arm and IBM nodes work in the same way—which means
    you can run pretty much anything in Kubernetes. Older applications bring their
    own challenges, but one of the big advantages of Kubernetes is that you don’t
    need to rewrite those apps. Breaking down monolithic apps into a more cloud-native
    architecture offers benefits, but that can be part of a longer-term program, which
    starts with moving apps to Kubernetes as is.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 尺度的另一面是能够在单个平台上尽可能多地运行你的应用程序目录。在本章中，您可以看到通过添加Windows节点可以运行多架构集群——Arm和IBM节点以相同的方式工作——这意味着您几乎可以在Kubernetes中运行任何东西。旧应用程序会带来自己的挑战，但Kubernetes的一个重大优势是您不需要重写这些应用程序。将单体应用程序分解为更云原生架构可以带来好处，但这可以是长期计划的一部分，该计划从将应用程序迁移到Kubernetes开始。
- en: We’re out of room to continue the discussion. You should leave your cluster
    running for the lab, but when you’re done, come back to this final exercise to
    clear it down.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有空间继续讨论了。您应该让您的集群在实验室中继续运行，但完成之后，请回到这个最后的练习来清理它。
- en: Try it now You have a few options for closing down your cluster—choose one after
    you’ve had a go at the lab.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 您有几种关闭集群的方法——在尝试实验室之后选择一个。
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 18.6 Lab
  id: totrans-139
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 18.6 实验室
- en: 'Here’s an easy lab for this chapter, but it will need some research. Time passes
    after you deploy your cluster, and at some point, the nodes will need maintenance
    work. Kubernetes lets you safely take a node out of the cluster—moving its Pods
    to another node—and then bring it back online when you’re done. Let’s do that
    for the Linux node in the cluster. Just one hint: you can do it all with kubectl.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是本章的一个简单实验，但需要一些研究。在你部署你的集群之后，时间会流逝，在某个时刻，节点将需要进行维护工作。Kubernetes 允许你安全地将一个节点从集群中移除——将其
    Pods 移动到另一个节点——完成维护后再将其重新上线。让我们为集群中的 Linux 节点做这个操作。只有一个提示：你可以用 kubectl 完成所有操作。
- en: 'This is a useful lab to work through because temporarily removing a node from
    service is something you’ll want to do whichever platform you’re using. My solution
    is on GitHub for you in the usual place: [https://github.com/sixeyed/kiamol/blob/master/ch18/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch18/lab/README.md).'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个有用的实验，值得尝试，因为无论你使用哪个平台，你都会希望暂时从服务中移除一个节点。我的解决方案在 GitHub 的常规位置供你参考：[https://github.com/sixeyed/kiamol/blob/master/ch18/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch18/lab/README.md)。
