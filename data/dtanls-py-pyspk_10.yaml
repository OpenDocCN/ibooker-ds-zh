- en: '8 Extending PySpark with Python: RDD and UDFs'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 使用 Python 扩展 PySpark：RDD 和 UDFs
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Using the RDD as a low-level, flexible data container
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 RDD 作为低级、灵活的数据容器
- en: Manipulating data in the RDD using higher-order functions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用高阶函数在 RDD 中操作数据
- en: How to promote regular Python functions to UDFs to run in a distributed fashion
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何将常规 Python 函数提升为 UDFs 以分布式方式运行
- en: How to apply UDFs on local data to ease debugging
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在本地数据上应用 UDFs 以简化调试
- en: Our journey with PySpark thus far has proven that it is a powerful and versatile
    data-processing tool. So far, we’ve explored many out-of-the-box functions and
    methods to manipulate data in a data frame. Recall from chapter 1 that PySpark’s
    data frame manipulation functionality takes our Python code and applies an optimized
    query plan. This makes our data jobs efficient, consistent, and predictable, just
    like coloring within the lines. What if we need to go off-script and manipulate
    our data according to our own rules?
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们与 PySpark 的旅程证明它是一个强大且多用途的数据处理工具。到目前为止，我们已经探索了许多现成的函数和方法来操作数据框中的数据。回想一下第
    1 章，PySpark 的数据框操作功能将我们的 Python 代码应用于优化的查询计划。这使得我们的数据作业高效、一致且可预测，就像在框内上色一样。如果我们需要偏离脚本并根据我们自己的规则操作数据怎么办？
- en: In this chapter, I cover the two mechanisms PySpark provides for distributing
    Python code. In other words, we move away from the set of functions and methods
    provided by `pyspark.sql`; instead, we build our own set of transformations in
    pure Python, using PySpark as a convenient distributing engine. For this, we start
    with the *resilient distributed dataset* (or RDD); the RDD is akin to the data
    frame, but distributes unordered objects rather than records and columns. This
    object-first approach provides more flexibility compared to the more rigid schema
    of the data frame. Second, I introduce UDFs, a simple way to promote regular Python
    functions to be used on a data frame.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我介绍了 PySpark 提供的两种用于分发 Python 代码的机制。换句话说，我们离开了 `pyspark.sql` 提供的函数和方法集；相反，我们使用
    PySpark 作为方便的分发引擎，在纯 Python 中构建我们自己的转换集。为此，我们从 *弹性分布式数据集*（或 RDD）开始；RDD 类似于数据框，但分布的是无序对象而不是记录和列。这种以对象为先的方法与数据框的更刚性模式相比提供了更多的灵活性。其次，我介绍了
    UDFs，这是一种将常规 Python 函数提升为可用于数据框的简单方法。
- en: The RDD, too old-school?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: RDD，过时了吗？
- en: With the advent of the data frame, which boasts better performance and a streamlined
    API for common data operations (`select`, `filter`, `groupby`, `join`), the RDD
    fell behind in terms of popularity. Is there room for the RDD in a modern PySpark
    program?
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据框的出现，它拥有更好的性能和针对常见数据操作（`select`、`filter`、`groupby`、`join`）的简化 API，RDD 在流行度方面落后了。在现代
    PySpark 程序中还有 RDD 的空间吗？
- en: 'While the data frame is becoming more and more flexible as the Spark versions
    are released, the RDD still reigns in terms of flexibility. The RDD especially
    shines in two use cases:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然随着 Spark 版本的发布，数据框变得越来越灵活，但 RDD 在灵活性方面仍然占据主导地位。RDD 特别在两个用例中表现出色：
- en: When you have an unordered collection of Python objects that can be pickled
    (which is how Python calls object serialization; see [http://mng.bz/M2X7](http://mng.bz/M2X7))
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你有一个可以序列化的无序 Python 对象集合（这是 Python 调用对象序列化的方式；参见 [http://mng.bz/M2X7](http://mng.bz/M2X7)）
- en: When you have unordered `key`, `value` pairs, like in a Python dictionary
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你有无序的 `key`、`value` 对，就像 Python 字典中一样
- en: Both use cases are covered in this chapter. The data frame should be your structure
    of choice by default, but know that if you find it restrictive, the RDD is waiting
    for you.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了这两个用例。默认情况下，数据框应该是你的首选结构，但要知道，如果你觉得它限制性太强，RDD 总是为你准备的。
- en: '8.1 PySpark, freestyle: The RDD'
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 PySpark，自由风格：RDD
- en: This section covers the RDD. More specifically, I introduce how to reason about
    the data structure and the API to manipulate data it contains.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖了 RDD。更具体地说，我介绍了如何推理数据结构以及操作其中数据的 API。
- en: '![](../Images/08-01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-01.png)'
- en: Figure 8.1 The `collection_rdd`. Each object is independent from the others
    in the container—no column, no structure, no schema.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 `collection_rdd`。容器中的每个对象都是独立的——没有列，没有结构，没有模式。
- en: 'Unlike the data frame, where most of our data manipulation tool kit revolved
    around columns, RDD revolves around objects: I think of an RDD as a bag of elements
    with no order or relationship to one another. Each element is independent of the
    other. The easiest way to experiment with a RDD is to create one from a Python
    list. In listing 8.1, I create a list containing multiple objects of different
    types, and then promote it to an RDD via the `parallelize` method. The resulting
    RDD is depicted in figure 8.1\. The creation of the RDD takes the objects on the
    list, serializes (or *pickles*) them, and then distributes them between the worker
    nodes.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据框不同，我们的数据操作工具套件大多围绕列展开，而RDD围绕对象：我认为RDD是一个没有顺序或相互关系的元素集合。每个元素都是独立的。实验RDD的最简单方法是从Python列表创建一个RDD。在列表8.1中，我创建了一个包含多种类型对象的列表，然后通过`parallelize`方法将其提升为RDD。结果RDD如图8.1所示。RDD的创建过程将列表上的对象序列化（或*快照*），然后在工作节点之间分配它们。
- en: Listing 8.1 Promoting a Python list to an RDD
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.1 将Python列表提升为RDD
- en: '[PRE0]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ My collection is a list of an integer, a string, a float, a tuple, and a dict.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我的集合是一个包含整数、字符串、浮点数、元组和字典的列表。
- en: ❷ The RDD functions and methods are under the SparkContext object, accessible
    as an attribute of our SparkSession. I alias it to sc for convenience.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ RDD函数和方法位于SparkContext对象下，可以通过我们的SparkSession的属性访问。为了方便，我将其别名为sc。
- en: ❸ The list gets promoted to an RDD using the parallelize method of the SparkContext.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用SparkContext的parallelize方法将列表提升为RDD。
- en: ❹ Our collection_rdd object is effectively an RDD. PySpark returns the type
    of the collection when we print the object.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们集合_rdd对象实际上是一个RDD。PySpark在打印对象时返回集合的类型。
- en: Compared to a data frame, the RDD is much more *freestyle* (pardon the 90s reference)
    in terms of what it accepts. If we were trying to store an integer, a string,
    a floating point number, a tuple, and a dictionary in a single column, the data
    frame would have (and fail) to find a common denominator to fit those different
    types of data.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 与数据框相比，RDD在它接受的类型方面更加*自由式*（请原谅90年代的引用）。如果我们试图将整数、字符串、浮点数、元组和字典存储在单个列中，数据框将（并失败）找不到一个共同的基础来适应这些不同类型的数据。
- en: In this section, we checked the general allure of an RDD and witnessed the flexibility
    it provides in ingesting many types of data into a single container abstraction.
    The next section covers manipulating data in the RDD; with great flexibility comes
    great responsibility!
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们检查了RDD的一般吸引力，并见证了它在将多种类型的数据摄入单一容器抽象中的灵活性。下一节将介绍在RDD中操作数据；巨大的灵活性伴随着巨大的责任！
- en: '8.1.1 Manipulating data the RDD way: map(), filter(), and reduce()'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 以RDD方式操作数据：map(), filter(), 和 reduce()
- en: This section explains the building blocks of data manipulation using an RDD.
    I discuss the concept of higher-order functions and use them to transform data.
    I finish with a quick overview of MapReduce, a fundamental concept in large-scale
    data processing, and place it in the context of Spark and the RDD.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 本节解释了使用RDD进行数据操作的基本构建块。我讨论了高阶函数的概念，并使用它们来转换数据。我以对MapReduce的快速概述结束，MapReduce是大规模数据处理的一个基本概念，并将其置于Spark和RDD的上下文中。
- en: 'Manipulating data with an RDD feels like giving orders to an army as a general:
    you have full obedience from your privates/divisions in the field/cluster, but
    if you give an incomplete or wrong order, you’ll cause havoc within your troops/RDD.
    Furthermore, each division has its own specific type of order it can perform,
    and you don’t have a reminder of what’s what (unlike with a data frame schema).
    Sounds like a fun job.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 使用RDD操作数据就像作为一名将军下达军队的命令：你的士兵/师/集群现场完全服从你的指挥，但如果你下达了不完整或错误的命令，你会在你的部队/RDD中造成混乱。此外，每个师都有自己的特定类型命令可以执行，而你没有一个提醒来区分这些命令（与数据框模式不同）。听起来像是一项有趣的工作。
- en: 'An RDD provides many methods (which you can find in the API documentation for
    the `pyspark.RDD` object), but we put our focus on three specific methods: `map()`,
    `filter()`, and `reduce()`. Together, they capture the ethos of data manipulation
    with an RDD; knowing how these three work will give you the necessary foundation
    to understand the others.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: RDD提供了许多方法（你可以在`pyspark.RDD`对象的API文档中找到），但我们专注于三个特定的方法：`map()`、`filter()`和`reduce()`。这三个方法共同捕捉了使用RDD进行数据操作的精髓；了解这三个方法的工作原理将为你理解其他方法提供必要的基石。
- en: '`map()`, `filter()`, and `reduce()` all take a function (that we will call
    `f`) as their only parameter and return a copy of the RDD with the desired modifications.
    We call functions that take other functions as parameters *higher-order functions*.
    They can be a little difficult to understand the first time you encounter them;
    fear not, after seeing them in action, you’ll be very comfortable using them in
    PySpark (and in Python, if you look at appendix C).'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`、`filter()` 和 `reduce()` 都接受一个函数（我们将称之为 `f`）作为它们的唯一参数，并返回一个具有所需修改的 RDD
    的副本。我们将接受其他函数作为参数的函数称为 *高阶函数*。第一次遇到它们时可能有点难以理解；不要担心，在看到它们实际应用后，你将非常舒适地在 PySpark（以及如果你查看附录
    C，Python）中使用它们。'
- en: 'Apply one function to every object: map'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 将一个函数应用到每个对象：map
- en: 'We start with the most basic and common operation: applying a Python function
    to every element of the RDD. For this, PySpark provides `map()`. This directly
    echoes the functionality of the `map()` function in Python.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从最基本和最常见操作开始：将 Python 函数应用到 RDD 的每个元素。为此，PySpark 提供了 `map()`。这直接反映了 Python
    中 `map()` 函数的功能。
- en: The best way to illustrate `map()` is through an example. In listing 8.2, I
    apply a function that adds 1 to its argument in every object in the RDD. After
    the application of the function, via `map()`, I `collect()` each of the elements
    of the RDD in a Python list to print them back—or so it seems; we get an error
    with its companion stack trace. What’s happening?
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的说明 `map()` 的方式是通过一个例子。在列表 8.2 中，我应用了一个将 1 添加到其参数的函数，在 RDD 的每个对象中。通过 `map()`
    应用函数后，我 `collect()` RDD 的每个元素到一个 Python 列表中以打印它们——或者看起来是这样；我们得到一个与其伴随的堆栈跟踪的错误。发生了什么？
- en: Listing 8.2 Mapping a simple function, `add_one()`, to each element
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 将简单函数 `add_one()` 映射到 RDD 的每个元素
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ A seemingly inoffensive function, add_one() adds 1 to the value passed as
    an argument.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 一个看似无害的函数，`add_one()` 将 1 添加到作为参数传递的值。
- en: ❷ I apply my function to every element in the RDD via the map() method.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我通过 `map()` 方法将我的函数应用到 RDD 中的每个元素。
- en: ❸ collect() materializes an RDD into a Python list on the master node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `collect()` 方法在主节点上将 RDD 转换为 Python 列表。
- en: To understand why our code is failing, we’ll break down the mapping process,
    illustrated in figure 8.2\. I apply the `add_one()` function to each element in
    the RDD by passing it as an argument to the `map()` method. `add_one()` is a regular
    Python function, applied to regular Python objects. Since we have incompatible
    types (e.g., `"two" + 1` is not a legal operation in Python), three of our elements
    are `TypeError`. When I `collect()` the RDD to peek at the values, it explodes
    into a stack trace right in my REPL.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么我们的代码失败，我们将分解映射过程，如图 8.2 所示。我通过将 `add_one()` 函数作为参数传递给 `map()` 方法，将其应用到
    RDD 中的每个元素。`add_one()` 是一个常规的 Python 函数，应用于常规的 Python 对象。由于我们有不兼容的类型（例如，在 Python
    中 `"two" + 1` 不是一个合法的操作），我们的三个元素是 `TypeError`。当我 `collect()` RDD 来查看值时，它在我的 REPL
    中爆炸成堆栈跟踪。
- en: '![](../Images/08-02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/08-02.png)'
- en: Figure 8.2 Applying the `add_one()` function to each element of the RDD via
    `map()`. If the function cannot be applied, an error will be raised during action
    time.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 通过 `map()` 应用 `add_one()` 函数到 RDD 的每个元素。如果函数无法应用，则在动作时间将引发错误。
- en: Note The RDD is a lazy collection. If you have an error in your function application,
    it will not be visible until you perform an action (e.g., `collect()`), just like
    with the data frame.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：RDD 是一个惰性集合。如果你在函数应用中有一个错误，它将不会在你执行动作（例如，`collect()`）之前可见，就像数据框一样。
- en: Fortunately, since we are working with Python, we can use a `try/except` block
    to prevent errors. I provide an improved `safer_add_one()` function in the next
    listing, which returns the original element if the function runs into a type error.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，由于我们使用 Python，我们可以使用 `try/except` 块来防止错误。我在下一个列表中提供了一个改进的 `safer_add_one()`
    函数，如果函数遇到类型错误，则返回原始元素。
- en: Listing 8.3 Mapping `safer_add_one()` to each element in an RDD
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 将 `safer_add_one()` 函数映射到 RDD 中的每个元素
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ I recreate my RDD from scratch to remove the erroneous operation in the thunk
    (see chapter 1 for a description of a computation thunk).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我从头开始重新创建我的 RDD，以移除 thunk 中的错误操作（参见第 1 章中对计算 thunk 的描述）。
- en: ❷ Our function returns the original value untouched if it encounters a TypeError.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果函数遇到 `TypeError`，则返回原始值不变。
- en: ❸ The relevant elements of the RDD have been incremented by 1.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ RDD 的相关元素已增加 1。
- en: In summary, you use `map()` to apply a function to every element of the RDD.
    Because of the flexibility of the RDD, PySpark does not give you any safeguards
    regarding the content of the RDD. You are responsible, as the developer, for making
    your function robust regardless of the input. In the next section, we get to filter
    some elements of our RDD.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，你使用 `map()` 将函数应用于 RDD 的每个元素。由于 RDD 的灵活性，PySpark 不会给你关于 RDD 内容的任何保障。作为开发者，你需要确保你的函数无论输入如何都足够健壮。在下一节中，我们将过滤
    RDD 的一些元素。
- en: Only keep what you want using filter
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 仅保留你想要的元素使用 filter
- en: 'Just like `where()/filter()` was one of the first methods I went over when
    I introduced the data frame, we have the same for the RDD. `filter()` is used
    to keep only the element that satisfies a predicate. The RDD version of `filter()`
    is a little different than the data frame version: it takes a function `f`, which
    applies to each object (or element) and keeps only those that return a truthful
    value. In listing 8.4, I filter my RDD to keep only the integer and float elements,
    using a lambda function. The `isinstance()` function returns `True` if the first
    argument’s type is present in the second argument; in our case, it’ll test if
    each element is either a `float` or an `int`.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我在介绍数据框时首先介绍的方法之一是 `where()` 或 `filter()` 一样，对于 RDD 我们也有同样的操作。`filter()` 用于保留满足谓词的元素。RDD
    版本的 `filter()` 与数据框版本略有不同：它接受一个函数 `f`，该函数应用于每个对象（或元素），并仅保留返回真实值的那些元素。在列表 8.4 中，我使用
    lambda 函数过滤我的 RDD，仅保留整数和浮点元素。`isinstance()` 函数在第一个参数的类型存在于第二个参数中时返回 `True`；在我们的情况下，它将测试每个元素是否为
    `float` 或 `int`。
- en: Listing 8.4 Filtering our RDD with a lambda function
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 使用 lambda 函数过滤我们的 RDD
- en: '[PRE3]'
  id: totrans-54
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Using lambda functions like a pro
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 像专业人士一样使用 lambda 函数
- en: 'Lambda functions are a great way to reduce boilerplate when you need a simple
    function for only one use. At its core, a lambda function allows you to create
    a function without assigning it a name. In the following figure, I show the correspondence
    between a *named* function (using the `def` keyword) and a lambda (or *anonymous*)
    function using the `lambda` keyword. Both statements return a function object
    that can then be used: where the named function can be referred to by its name
    (`is_a_number`), the lambda function is usually defined where it needs to be used.
    In the case of the `filter()` application in listing 8.4, we could have replaced
    our lambda by the `is_a_number` function, but directly using a lambda function
    as the argument to `filter()` saves a few keystrokes.'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数是当你需要一个仅用于一次的简单函数时减少样板代码的好方法。本质上，lambda 函数允许你创建一个没有名称的函数。在下面的图中，我展示了使用
    `def` 关键字定义的 *命名* 函数与使用 `lambda` 关键字定义的 lambda（或 *匿名*）函数之间的对应关系。两个语句都返回一个函数对象，然后可以用来使用：命名函数可以通过其名称（`is_a_number`）引用，而
    lambda 函数通常定义在需要使用它的地方。在列表 8.4 中的 `filter()` 应用中，我们可以用 `is_a_number` 函数替换我们的 lambda，但直接使用
    lambda 函数作为 `filter()` 的参数可以节省一些按键。
- en: '![](../Images/08-02-unnumb.png)'
  id: totrans-57
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.2-unnumb](../Images/08-02-unnumb.png)'
- en: Converting a simple function to a lambda function using the lambda keyword
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 lambda 关键字将简单函数转换为 lambda 函数
- en: Lambda functions are very useful in conjunction with higher-order functions,
    such as `map()` and `filter()`, for the RDD. Often, the functions applied to the
    elements will only be used once, so there is no point in assigning them to a variable.
    If you don’t feel comfortable using lambda functions, don’t worry; it is still
    okay to create a small function and then apply it to your RDD.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda 函数与 RDD 中的高阶函数（如 `map()` 和 `filter()`）结合使用非常有用。通常，应用于元素的函数只会使用一次，所以没有必要将它们分配给变量。如果你不习惯使用
    lambda 函数，不用担心；仍然可以创建一个小函数并将其应用于你的 RDD。
- en: Just like `map()`, the function passed as a parameter to `filter()` is applied
    to every element in the RDD. This time, though, instead of returning the result
    in a new RDD, we keep the original value if the result of the function is truthy.
    If the result is falsy, we drop the element. I show the breakdown of the `filter()`
    operation in figure 8.3.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 就像 `map()` 一样，传递给 `filter()` 的函数应用于 RDD 中的每个元素。不过，这一次，我们不是在新的 RDD 中返回结果，而是在函数的结果为真值时保留原始值。如果结果为假值，则丢弃该元素。我在图
    8.3 中展示了 `filter()` 操作的分解。
- en: '![](../Images/08-03.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.3](../Images/08-03.png)'
- en: Figure 8.3 Filtering our RDD to keep only `int` and `float`. Our predicate function
    is applied element-wise, and only the values leading to truthy predicates are
    kept.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.3 过滤我们的 RDD 以保留 `int` 和 `float`。我们的谓词函数逐元素应用，并且仅保留导致真值谓词的值。
- en: Truthy/falsy in Python
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Python中的真/假值
- en: 'When working with the data frame, the argument to `filter()` (the *data frame*
    method) needs to return a Boolean explicitly: either `True` or `False`. In this
    chapter, with the RDD, we follow Python’s convention for Boolean testing; because
    of this, I avoid using absolute `True`/`False` when talking about filtering in
    the RDD since Python has its own way of determining if a value is “truthy” (it
    will be considered `True` by Python) or “falsy.” As a reminder, `False`, `0` (the
    number zero in any Python numerical type), and empty sequences and collections
    (list, tuple, dict, set, range) are falsy. For more explanation on how Python
    imputes Boolean values on non-Boolean types, refer to the Python documentation
    ([http://mng.bz/aDoz](http://mng.bz/aDoz)).'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当与数据框一起工作时，`filter()`（数据框方法）的参数需要显式返回布尔值：要么是 `True`，要么是 `False`。在本章中，使用 RDD
    时，我们遵循 Python 的布尔测试约定；因此，在讨论 RDD 中的过滤时，我避免使用绝对的 `True`/`False`，因为 Python 有自己的方式来确定值是否是“真值”（它将被
    Python 考虑为 `True`）。作为提醒，`False`、`0`（任何 Python 数值类型中的数字零）、空序列和集合（列表、元组、字典、集合、范围）都是假值。有关
    Python 在非布尔类型上如何推断布尔值的更多解释，请参阅 Python 文档 ([http://mng.bz/aDoz](http://mng.bz/aDoz))。
- en: We can now map over and filter the elements in our RDD. How would we aggregate
    them into a summary value? If you think that the RDD provides yet again a very
    general way of summarizing the elements it contains, you are right! Ready to reduce?
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以映射并过滤我们的 RDD 中的元素。我们如何将它们聚合到汇总值中？如果你认为 RDD 再次提供了一个非常通用的方法来汇总它包含的元素，你是正确的！准备好进行归约了吗？
- en: 'Two elements come in, one comes out: reduce()'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 两个元素进来，一个元素出来：reduce()
- en: This section covers the last important operation of the RDD, which enables the
    summarization of data (similar to `groupby()`/`agg()`) using the data frame. `reduce()`,
    as its name implies, is used to reduce elements in an RDD. By *reducing*, I mean
    taking two elements and applying a function that will return only one element.
    PySpark will apply the function to the first two elements, then apply it again
    to the result and the third element, and so on, until there are no elements left.
    I find the concept easier when explained visually, so figure 8.4 shows the process
    of summing the value of the elements in the RDD using `reduce()`. The following
    listing presents how to use the `reduce()` method on a data frame.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了 RDD 的最后一个重要操作，该操作允许使用数据框对数据进行汇总（类似于 `groupby()`/`agg()`）。`reduce()`，正如其名称所暗示的，用于减少
    RDD 中的元素。通过“减少”，我的意思是取两个元素并应用一个函数，该函数将只返回一个元素。PySpark 将函数应用于前两个元素，然后再次将其应用于结果和第三个元素，依此类推，直到没有元素为止。我发现当视觉解释时，这个概念更容易理解，所以图
    8.4 展示了使用 `reduce()` 求和 RDD 中元素值的过程。下面的列表展示了如何在数据框上使用 `reduce()` 方法。
- en: '![](../Images/08-04.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/08-04.png)'
- en: Figure 8.4 Reducing our RDD by summing the values of the elements
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 通过求和 RDD 元素的值来减少我们的 RDD
- en: Listing 8.5 Applying the `add()` function via `reduce()`
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.5 通过 `reduce()` 应用 `add()` 函数
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '❶ The operator module contains the function version or common operators, such
    as + (add()), so we do not have to pass a lambda a, b: a + b.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '❶ `operator` 模块包含函数版本或常用运算符，例如 + (add())，因此我们不需要传递 lambda a, b: a + b。'
- en: '`map()`, `filter()`, and `reduce()` appear at first glance like simple concepts:
    they take a function and apply it to all the elements inside the collection. The
    result is treated differently depending on the method chosen, and `reduce()` requires
    a function of two arguments returning a single value. In 2004, Google used this
    humble concept and caused a revolution in the large-scale data processing world
    by publishing its MapReduce framework ([https://research.google/pubs/pub62/](https://research.google/pubs/pub62/)).
    You can’t argue about naming here: the name is a combination of `map()` and `reduce()`.
    This framework was a direct inspiration for big data frameworks, such as Hadoop
    and Spark. Although modern abstractions, such as the data frame, aren’t as close
    to the original MapReduce, the ideas remain, and gaining a high-level understanding
    of the building blocks will make it easier to understand some higher-level design
    choices.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`, `filter()`, 和 `reduce()` 初看起来像是简单的概念：它们接受一个函数并将其应用于集合内的所有元素。结果根据选择的方法不同而有所不同，`reduce()`
    需要一个返回单个值的两个参数的函数。在 2004 年，Google 通过发布其 MapReduce 框架 ([https://research.google/pubs/pub62/](https://research.google/pubs/pub62/))，利用了这个朴素的概念，在大型数据处理领域引发了一场革命。在这里，关于命名无可争议：这个名字是
    `map()` 和 `reduce()` 的组合。这个框架直接启发了大数据框架，如 Hadoop 和 Spark。尽管现代抽象，如数据框，并不像原始的 MapReduce
    那么接近，但思想仍然存在，对构建块的高层次理解将使理解一些高级设计选择变得更容易。'
- en: reduce() in a distributed world
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式世界中的 `reduce()`
- en: Because of PySpark’s distributed nature, the data of an RDD can be distributed
    across multiple partitions. The `reduce()` function will be applied independently
    on each partition, and then each intermediate value will be sent to the master
    node for the final reduction. Because of this, you need to provide a commutative
    and associative function to `reduce()`.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 PySpark 的分布式特性，RDD 的数据可以分布在多个分区上。`reduce()` 函数将在每个分区上独立应用，然后每个中间值将被发送到主节点进行最终减少。因此，你需要为
    `reduce()` 提供一个交换律和结合律的函数。
- en: 'A *commutative function* is a function where the order in which the arguments
    are applied is not important. For example, `add()` is commutative, since `a +
    b = b + a`. Oh the flip side, `subtract()` is not: `a - b != b - a`.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *交换律* 函数是一个其中参数应用的顺序并不重要的函数。例如，`add()` 是交换律的，因为 `a + b = b + a`。另一方面，`subtract()`
    不是：`a - b != b - a`。
- en: 'An *associative* function is a function where how the values are grouped is
    not important. `add()` is associative, since `(a + b) + c = a + (b + c)`. `subtract()`
    is not: `(a - b) - c != a - (b` `-` `c)`.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 一个 *结合律* 函数是一个其中值的分组方式并不重要的函数。`add()` 是结合律的，因为 `(a + b) + c = a + (b + c)`。`subtract()`
    不是：`(a - b) - c != a - (b - c)`。
- en: '`add()`, `multiply()`, `min()`, and `max()` are both associative and commutative.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`add()`, `multiply()`, `min()`, 和 `max()` 既是结合律的也是交换律的。'
- en: This concludes our whirlwind tour of the PySpark RDD API. We covered how the
    RDD applies transformations to its elements through higher-order functions such
    as `map()`, `filter()`, and `reduce()`. Those higher-order functions apply the
    functions passed as parameters to each element, making the RDD *element-major*
    (or *row-major*). If you are curious about the other applications of the RDD,
    I recommend looking at the PySpark online API documentation. Most of the methods
    in the RDD have a direct equivalent to the data frame or grow directly from the
    usage of `map()`, `filter()`, and `reduce()`. The next sections build on the concept
    of applying Python functions directly, but this time on a data frame. The fun
    begins!
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这就结束了我们对 PySpark RDD API 的快速浏览。我们介绍了 RDD 如何通过 `map()`, `filter()`, 和 `reduce()`
    等高阶函数对其元素应用转换。这些高阶函数将作为参数传递的函数应用于每个元素，使 RDD 成为 *元素主要*（或 *行主要*）。如果你对 RDD 的其他应用感兴趣，我建议查看
    PySpark 在线 API 文档。RDD 中的大多数方法在数据框中都有直接对应的方法，或者直接从 `map()`, `filter()`, 和 `reduce()`
    的使用中发展而来。接下来的几节将建立在直接应用 Python 函数的概念之上，但这次是在数据框上。乐趣从这里开始！
- en: Exercise 8.1
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.1
- en: The PySpark RDD API provides a `count()` method that returns the number of elements
    in the RDD as an integer. Reproduce the behavior of this method using `map()`,
    `filter()`, and/or `reduce()`.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark RDD API 提供了一个 `count()` 方法，该方法返回 RDD 中的元素数量作为一个整数。使用 `map()`, `filter()`,
    和/或 `reduce()` 重新生成此方法的行为。
- en: Exercise 8.2
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.2
- en: What is the return value of the following code block?
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码块的返回值是什么？
- en: '[PRE5]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: a) `[1]`
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: a) `[1]`
- en: b) `[0,` `1]`
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: b) `[0, 1]`
- en: c) `[0,` `1,` `0.0]`
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: c) `[0, 1, 0.0]`
- en: d) `[]`
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: d) `[]`
- en: e) `[1,` `[]]`
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: e) `[1, []]`
- en: 'Optional topic: Going full circle, a data frame is an RDD!'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 可选主题：回到起点，数据框是一个 RDD！
- en: 'To show the ultimate flexibility of the RDD, look at this: you can access an
    implicit RDD within a data frame via the `rdd` attribute of a data frame.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 要展示 RDD 的最终灵活性，看看这个：你可以通过数据帧的 `rdd` 属性访问数据帧内的隐式 RDD。
- en: Listing 8.6 Uncovering the RDD from within a data frame using the `rdd` attribute
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.6 使用 `rdd` 属性在数据帧内部发现 RDD
- en: '[PRE6]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: From a PySpark perspective, a data frame is also an `RDD[Row]` (from `pyspark.sql
    .Row`), where each row can be thought of as a dictionary; the key is the column
    name and the value is the value contained in the record. To do the opposite, you
    can pass the RDD to `spark.createDataFrame` with an optional schema. Remember
    that, when moving from a data frame to an RDD, you give up the schema safety of
    the data frame!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 从 PySpark 的角度来看，数据帧也是一个 `RDD[Row]`（来自 `pyspark.sql.Row`），其中每一行可以被视为一个字典；键是列名，值是记录中包含的值。要执行相反的操作，你可以将
    RDD 传递给 `spark.createDataFrame` 并可选地指定模式。记住，当你从数据帧移动到 RDD 时，你放弃了数据帧的模式安全性！
- en: It can be tempting to move back and forth between a data frame and an RDD depending
    on the operation you wish to perform. Bear in mind that this will come at a performance
    cost (because you convert the data from column-major to row-major), but will also
    make your code harder to follow. You will also have to make sure all your `Row`s
    follow the same schema before putting your RDD into a data frame. Usually, if
    you need the occasional power of the RDD but with the comfort of the columnar
    aspect of the data frame, UDFs are the way to go. Luckily, this is the topic of
    the next section.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 根据你希望执行的操作，在数据帧和 RDD 之间来回移动可能会很有吸引力。请记住，这将会带来性能成本（因为你要将数据从列主序转换为行主序），但也会使你的代码更难理解。你还需要确保在将你的
    RDD 放入数据帧之前，所有的 `Row` 都遵循相同的模式。通常，如果你偶尔需要 RDD 的强大功能，但又希望享受数据帧的列属性，UDF 是一个不错的选择。幸运的是，这正是下一节的主题。
- en: 8.2 Using Python to extend PySpark via UDFs
  id: totrans-96
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 使用 Python 通过 UDFs 扩展 PySpark
- en: 'In section 8.1, we got a taste of the flexibility of the RDD approach to data
    manipulation. This section takes the same question—how can we run Python code
    on our data?—and applies it to the data frame. More specifically, we focus on
    the `map()` transformation: for each record that comes in, one record comes out.
    Map-type transformations are by far the most frequent and the easiest to implement.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在 8.1 节中，我们体验了 RDD 方法在数据处理方面的灵活性。本节将同样的一个问题——我们如何在我们数据上运行 Python 代码？——应用到数据帧上。更具体地说，我们关注
    `map()` 转换：对于每个输入的记录，输出一个记录。Map 类型的转换到目前为止是最常见且最容易实现的。
- en: Unlike the RDD, the data frame has a structure enforced by columns. To address
    this constraint, PySpark provides the possibility of creating UDFs via the `pyspark.sql.functions.udf()`
    function. What comes in is a regular Python function, and what goes out is a function
    promoted to work on PySpark columns.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 与 RDD 不同，数据帧由列强制执行的结构。为了解决这个限制，PySpark 提供了通过 `pyspark.sql.functions.udf()` 函数创建
    UDF 的可能性。输入的是一个常规的 Python 函数，输出的是一个提升到可以在 PySpark 列上工作的函数。
- en: 'To illustrate this, we will mock up a data type not present in PySpark: the
    `Fraction`. Fractions are made of a numerator and a denominator. In PySpark, we’ll
    represent this as an array of two integers. In the next listing, I create a data
    frame containing two columns that stand for the numerator and the denominator.
    I fuse the two columns in an array column via the `array()` function.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，我们将模拟一个 PySpark 中不存在的数据类型：`Fraction`。分数由分子和分母组成。在 PySpark 中，我们将它表示为两个整数的数组。在下一个列表中，我创建了一个包含两个列的数据帧，这两个列分别代表分子和分母。我通过
    `array()` 函数将这两个列融合到一个数组列中。
- en: Listing 8.7 Creating a data frame containing a single-array column
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.7 创建包含单个数组列的数据帧
- en: '[PRE7]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ I start the range for the denominator at 1, since a fraction with 0 for the
    denominator is undefined.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我将分母的范围从 1 开始，因为分母为 0 的分数是未定义的。
- en: ❷ The array() function takes two or more columns of the same type and creates
    a single column containing an array of the columns passed as a parameter.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `array()` 函数接受两个或更多相同类型的列，并创建一个包含作为参数传递的列的数组的单列。
- en: To support our new makeshift fraction type, we create a few functions that provide
    basic functionality. This is a perfect job for Python UDFs, and I take the opportunity
    to introduce the two ways PySpark enables its creation.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持我们新的临时分数类型，我们创建了一些提供基本功能的功能。这对于 Python UDF 是一个完美的任务，我借此机会介绍 PySpark 使其创建的两种方式。
- en: '8.2.1 It all starts with plain Python: Using typed Python functions'
  id: totrans-105
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 一切从纯 Python 开始：使用类型化 Python 函数
- en: This section covers creating a Python function that will work seamlessly with
    a PySpark data frame. While Python and Spark usually work seamlessly together,
    creating and using UDFs requires a few precautions. I introduce how you can use
    Python-type hints to make sure your code will work seamlessly with PySpark types.
    At the end of this section, we will have a function to reduce a fraction and one
    to transform a fraction into a floating-point number.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍创建一个与PySpark数据帧无缝工作的Python函数。虽然Python和Spark通常可以无缝协作，但创建和使用UDF需要一些预防措施。我介绍如何使用Python类型提示来确保你的代码将与PySpark类型无缝工作。在本节的末尾，我们将有一个用于化简分数的函数和一个将分数转换为浮点数的函数。
- en: 'My blueprint when creating a function destined to become a Python UDF is as
    follows:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当创建一个注定要成为Python UDF（用户定义函数）的函数时，我的蓝图如下：
- en: Create and document the function.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建并记录函数。
- en: Make sure the input and output types are compatible.
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保输入和输出类型兼容。
- en: Test the function.
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试函数。
- en: For this section, I provide a couple of assertions to make sure the function
    is behaving as expected.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我提供了一些断言以确保函数按预期行为。
- en: Behind every UDF is a Python function, so our two functions are in listing 8.8\.
    I introduce Python-type annotations in this code block; the rest of the section
    covers how they are used in this context and why they are a powerful tool when
    combined with Python UDFs.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 每个UDF背后都有一个Python函数，因此我们的两个函数在列表8.8中。我在这个代码块中介绍了Python类型注解；本节的其余部分涵盖了它们在这个上下文中的应用以及为什么它们与Python
    UDF结合时是一个强大的工具。
- en: Listing 8.8 Creating our three Python functions
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表8.8 创建我们的三个Python函数
- en: '[PRE8]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ We rely on the Fraction data type from the fractions module to avoid reinventing
    the wheel.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们依赖`fractions`模块中的`Fraction`数据类型来避免重新发明轮子。
- en: '❷ Some specific types need to be imported to be used: the standard library
    contains the types for scalar values, but containers like Option and Tuple need
    to be explicitly imported.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一些特定的类型需要导入才能使用：标准库包含标量值的类型，但像Option和Tuple这样的容器需要显式导入。
- en: '❸ We create a type synonym: Frac. This is equivalent to telling Python/mypy,
    “When you see Frac, assume it’s a Tuple[int, int]” (a tuple containing two integers).
    This makes the type annotations easier to read.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们创建了一个类型同义词：Frac。这相当于告诉Python/mypy，“当你看到Frac时，假设它是一个Tuple[int, int]”（一个包含两个整数的元组）。这使得类型注解更容易阅读。
- en: ❹ Our function takes a Frac as an argument and returns a Optional[Frac], which
    translates to “either a Frac or None.”
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 我们的函数接受一个Frac作为参数，并返回一个`Optional[Frac]`，这相当于“一个Frac或None。”
- en: ❺ I create a few assertions to sanity check my code and make sure I get the
    expected behavior.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我创建了一些断言来验证我的代码并确保我得到预期的行为。
- en: Both functions are similar, so I’ll take `py_reduce_fraction` and go through
    it line by line.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 两个函数都很相似，所以我将以`py_reduce_fraction`为例，逐行分析。
- en: My function definition has a few new elements. The `frac` parameter has a `:`
    `Frac`, and we have a `->` `Optional[Frac]` before the colon. Those additions
    are *type annotations* and are an amazing tool to make sure the function accepts
    and returns what we expect. Python is a dynamic language; this means that the
    type of an object is known at runtime. When working with PySpark’s data frame,
    where each column has one and only one type, we need to make sure that our UDF
    will return consistent types. We can use type hints to ensure this.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 我的函数定义有几个新元素。`frac`参数有一个`:` `Frac`，在冒号之前有一个`->` `Optional[Frac]`。这些添加是*类型注解*，是确保函数接受和返回我们期望内容的惊人工具。Python是一种动态语言；这意味着对象的类型是在运行时知道的。当与PySpark的数据帧一起工作时，其中每个列只有一个类型，我们需要确保我们的UDF将返回一致的类型。我们可以使用类型提示来确保这一点。
- en: Python’s type checking can be enabled through multiple libraries; `mypy` is
    one of them. You install it via `pip` `install` `mypy`. Once installed, you can
    run `mypy` on your file with `mypy` `MY_FILE.py`. Appendix C contains a deeper
    introduction to the `typing` module and `mypy` and how it applies (and why it
    should apply) beyond a UDF. I’ll add type annotation when relevant, as it can
    be useful documentation, in addition to making code more robust. (What does my
    function expect? What does it return?)
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Python的类型检查可以通过多个库启用；`mypy`就是其中之一。你可以通过`pip install mypy`来安装它。一旦安装，你就可以使用`mypy
    MY_FILE.py`在你的文件上运行`mypy`。附录C包含了对`typing`模块和`mypy`的深入介绍，以及它们如何应用于（以及为什么应该应用于）UDF之外。我会在相关的地方添加类型注解，因为它们可以作为有用的文档，同时使代码更加健壮。（我的函数期望什么？它返回什么？）
- en: 'In my function definition, I announce that the `frac` function parameter is
    of type `Frac`, which is equivalent to a `Tuple[int,` `int]` or a two-element
    tuple containing two integers. If I get to share my code with others, this type
    annotation sends a signal about the input type of my function. Furthermore, `mypy`
    will complain if I try to pass an incompatible argument to my function. If I try
    to do `py_reduce_fraction("one` `half")`, `mypy` will tell me the following:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的函数定义中，我宣布 `frac` 函数参数的类型为 `Frac`，它等同于 `Tuple[int, int]` 或包含两个整数的两个元素元组。如果我与他人分享我的代码，这种类型注解会向他人发出关于我的函数输入类型的信号。此外，如果我将不兼容的参数传递给我的函数，`mypy`
    将会抱怨。如果我尝试执行 `py_reduce_fraction("one half")`，`mypy` 将告诉我以下内容：
- en: '[PRE9]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: I can already see the type errors vanishing.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经可以看到类型错误正在消失。
- en: The second type annotation, located after the function arguments and prefixed
    with an arrow, is for the return type of the function. We recognize the `Frac`,
    but this time, I wrapped it into an `Optional` type.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个类型注解位于函数参数之后，并以前缀箭头开头，是关于函数的返回类型。我们识别 `Frac`，但这次，我将它包装在 `Optional` 类型中。
- en: 'In section 8.1, when creating functions to be distributed over the RDD, I needed
    to make sure that they would not trigger an error and return `None` instead. I
    apply the same concept here. I test for `denom` being a truthy value: if it is
    equal to zero, I return `None`. This is such a frequent use case that Python provides
    the `Optional[...]` type, which means either the type between the brackets or
    `None`. PySpark will accept `None` values as `null`.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在 8.1 节中，当创建要在 RDD 上分布的函数时，我需要确保它们不会触发错误并返回 `None`。我在这里应用了相同的概念。我检查 `denom`
    是否是一个真实值：如果它等于零，我返回 `None`。这是一个如此常见的用例，以至于 Python 提供了 `Optional[...]` 类型，这意味着括号内的类型或
    `None`。PySpark 会接受 `None` 值作为 `null`。
- en: 'Type annotations: Extra keystrokes, fewer errors'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 类型注解：额外的按键，更少的错误
- en: Type annotations are incredibly useful out of the box, but they are especially
    nifty when used with Python UDFs. Since PySpark’s execution model is lazy, you’ll
    often get your error stack trace at action time. UDF stack traces are not any
    harder to read than any other stack trace in PySpark—which is not saying much—but
    a vast majority of the bugs are because of a bad input or return value. Type annotations
    are not a silver bullet, but they are a great tool for avoiding and diagnosing
    type errors.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 类型注解开箱即用就非常有用，但与 Python UDF 一起使用时尤其巧妙。由于 PySpark 的执行模型是懒加载的，你通常会在动作执行时得到错误堆栈跟踪。UDF
    堆栈跟踪并不比 PySpark 中的任何其他堆栈跟踪更难阅读——这并不是说很多——但绝大多数错误都是由于不良的输入或返回值引起的。类型注解不是万能的银弹，但它是避免和诊断类型错误的一个很好的工具。
- en: 'The rest of the function is relatively straightforward: I ingest the numerator
    and denominator in a `Fraction` object, which reduces the fraction. I then extract
    the numerator and denominator from the `Fraction` and return them as a tuple of
    two integers, as I promised in my return type annotation.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 函数的其余部分相对简单：我将分子和分母摄入一个 `Fraction` 对象中，该对象会化简分数。然后我从 `Fraction` 中提取分子和分母，并将它们作为两个整数的元组返回，正如我在返回类型注解中所承诺的那样。
- en: We have our two functions with well-defined input and output types. In the next
    section, I show how you promote regular Python functions to UDFs and apply them
    to your data frame.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有两个具有明确定义输入和输出类型的函数。在下一节中，我将展示如何将常规 Python 函数提升为 UDF 并将其应用于你的数据框。
- en: 8.2.2 From Python function to UDFs using udf()
  id: totrans-132
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 使用 udf() 从 Python 函数到 UDFs
- en: Once you have your Python function created, PySpark provides a simple mechanism
    to promote to a UDF. This section covers the `udf()` function and how to use it
    directly to create a UDF, as well as using the decorator to simplify the creation
    of a UDF.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你创建了你的 Python 函数，PySpark 提供了一种简单的机制来将其提升为 UDF。本节涵盖了 `udf()` 函数及其如何直接用于创建 UDF，以及使用装饰器简化
    UDF 的创建。
- en: 'PySpark provides a `udf()` function in the `pyspark.sql.functions` module to
    promote Python functions to their UDF equivalents. The function takes two parameters:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 在 `pyspark.sql.functions` 模块中提供了一个 `udf()` 函数，用于将 Python 函数提升为其 UDF
    等效物。该函数接受两个参数：
- en: The function you want to promote
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你想要提升的函数
- en: The return type of the generated UDF
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成的 UDF 的返回类型
- en: In table 8.1, I summarize the type equivalences between Python and PySpark.
    If you provide a return type, it must be compatible with the return value of your
    UDF.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在表 8.1 中，我总结了 Python 和 PySpark 之间的类型等效性。如果你提供了返回类型，它必须与你的 UDF 的返回值兼容。
- en: Table 8.1 A summary of the types in PySpark. A star next to the `Python` `equivalent`
    column means the Python type is more precise or can contain larger values, so
    you need to be careful with the values you return.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.1 PySpark 中类型总结。在 `Python` `等效` 列旁边有一个星号表示 Python 类型更精确或可以包含更大的值，因此在使用返回的值时需要小心。
- en: '| Type Constructor | String representation | Python equivalent |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| 类型构造函数 | 字符串表示 | Python 等价类型 |'
- en: '| `NullType()` | `null` | `None` |'
  id: totrans-140
  prefs: []
  type: TYPE_TB
  zh: '| `NullType()` | `null` | `None` |'
- en: '| `StringType()` | `string` | Python’s regular strings |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| `StringType()` | `string` | Python 的常规字符串 |'
- en: '| `BinaryType()` | `binary` | bytearray |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| `BinaryType()` | `binary` | bytearray |'
- en: '| `BooleanType()` | `boolean` | bool |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| `BooleanType()` | `boolean` | bool |'
- en: '| `DateType()` | `date` | `datetime.date` (from the `datetime` library) |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| `DateType()` | `date` | `datetime.date`（来自 `datetime` 库）|'
- en: '| `TimestampType()` | `timestamp` | `datetime.datetime` (from the `datetime`
    library) |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| `TimestampType()` | `timestamp` | `datetime.datetime`（来自 `datetime` 库）|'
- en: '| `DecimalType(p,s)` | `decimal` | `decimal.Decimal` (from the `decimal` library)*
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| `DecimalType(p,s)` | `decimal` | `decimal.Decimal`（来自 `decimal` 库）* |'
- en: '| `DoubleType()` | `double` | float |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| `DoubleType()` | `double` | float |'
- en: '| `FloatType()` | `float` | float* |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '| `FloatType()` | `float` | float* |'
- en: '| `ByteType()` | `byte` or `tinyint` | int* |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| `ByteType()` | `byte` 或 `tinyint` | int* |'
- en: '| `IntegerType()` | `int` | int* |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| `IntegerType()` | `int` | int* |'
- en: '| `LongType()` | `long` or `bigint` | int* |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| `LongType()` | `long` 或 `bigint` | int* |'
- en: '| `ShortType()` | `short` or `smallint` | int* |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| `ShortType()` | `short` 或 `smallint` | int* |'
- en: '| `ArrayType(T)` | N/A | list, tuple, or Numpy array (from the `numpy` library)
    |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `ArrayType(T)` | N/A | list, tuple, 或 Numpy 数组（来自 `numpy` 库）|'
- en: '| `MapType(K, V)` | N/A | dict |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `MapType(K, V)` | N/A | dict |'
- en: '| `StructType([...])` | N/A | list or tuple |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `StructType([...])` | N/A | list 或 tuple |'
- en: In listing 8.9, I promote the `py_reduce_fraction()` function to a UDF via the
    `udf()` function. Just like I did with the Python equivalent, I provide a return
    type to the UDF (this time, an `Array` of `Long`, since `Array` is the companion
    type of the tuple and `Long` is the one for Python integers). Once the UDF is
    created, we can apply it just like any other PySpark function on columns. I chose
    to create a new column to showcase the before and after; in the sample shown,
    the fraction appears properly reduced.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 8.9 中，我通过 `udf()` 函数将 `py_reduce_fraction()` 函数提升为 UDF。就像我处理 Python 等效函数一样，我为
    UDF 提供了一个返回类型（这次是一个 `Long` 的 `Array`，因为 `Array` 是元组的伴随类型，而 `Long` 是 Python 整数的类型）。一旦创建了
    UDF，我们就可以像对任何其他 PySpark 函数一样在列上应用它。我选择创建一个新列来展示前后变化；在示例中，分数被正确约简。
- en: Listing 8.9 Creating a UDF explicitly with the `udf()` function
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.9 使用 `udf()` 函数显式创建 UDF
- en: '[PRE10]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ I alias the “array” of long PySpark type to the SparkFrac variable.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我将 PySpark 的长类型数组别名为 SparkFrac 变量。
- en: ❷ I promote my Python function using the udf() function, passing my SparkFrac-type
    alias as the return type.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我使用 `udf()` 函数提升我的 Python 函数，并将我的 SparkFrac 类型别名作为返回类型。
- en: ❸ A UDF can be used like any other PySpark column function.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ UDF 可以像任何其他 PySpark 列函数一样使用。
- en: You also have the option of creating your Python function and promoting it as
    a UDF using the `udf` function as a decorator. *Decorators* are functions applied
    to other functions through the `@` sign above the function definition (we call
    that function *decorated*). This allows for changing the behavior of a function—here,
    we create a UDF from a regular Python function definition—with minimal boilerplate
    (see appendix C for more information). In listing 8.10, I define `py_fraction_to_float()`
    (now simply called `fraction_to_float()`) directly as a UDF by preceding my function
    definition with `@F.udf([return_type])`. In both cases, you can access the underlying
    function from the UDF by calling the attribute `frac`.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以创建自己的 Python 函数，并使用 `udf` 函数作为装饰器将其提升为 UDF。*装饰器* 是通过函数定义上方 `@` 符号应用于其他函数的函数（我们称该函数为
    *装饰*）。这允许改变函数的行为——在这里，我们从一个常规的 Python 函数定义中创建一个 UDF——而无需编写大量样板代码（更多信息请参阅附录 C）。在列表
    8.10 中，我通过在函数定义前加上 `@F.udf([return_type])` 直接将 `py_fraction_to_float()`（现在简称为
    `fraction_to_float()`）定义为 UDF。在两种情况下，您都可以通过调用属性 `frac` 从 UDF 访问底层函数。
- en: Listing 8.10 Creating a UDF directly using the `udf()` decorator
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.10 使用 `udf()` 装饰器直接创建 UDF
- en: '[PRE11]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ The decorator performs the same function as the udf() function, but returns
    a UDF bearing the name of the function defined under it.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 装饰器执行与 `udf()` 函数相同的功能，但返回一个带有其下定义的函数名称的 UDF。
- en: ❷ In order to perform my assertion, I use the func attribute of the UDF, which
    returns the function ready to be called.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为了执行我的断言，我使用 UDF 的 func 属性，该属性返回一个准备就绪的可以调用的函数。
- en: 'In this chapter, we effectively tied Python and Spark in the tightest way possible.
    With the RDD, you have full control over the data inside the container, but you
    also have the responsibility to create functions and use higher-order functions
    such as `map()`, `filter()`, and `reduce()` to process the data objects inside.
    For the data frame, UDFs are your best tool: you can convert a Python function
    to a UDF that will use columns as inputs and outputs, at the expense of a performance
    hit to marry the data between Spark and Python.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们以可能的最紧密方式将 Python 和 Spark 结合起来。使用 RDD，你对容器内的数据拥有完全的控制权，但你也有责任创建函数并使用
    `map()`、`filter()` 和 `reduce()` 等高阶函数来处理容器内的数据对象。对于数据框，UDF 是你的最佳工具：你可以将 Python
    函数转换为 UDF，它将使用列作为输入和输出，但会牺牲一些性能以将 Spark 和 Python 之间的数据结合在一起。
- en: 'The next chapter takes this to the next level: we will delve into the interaction
    between PySpark and pandas though special types of UDFs. PySpark and pandas make
    an attractive combo, and having the ability to scale Pandas through the Spark
    framework elevates both libraries. Fun and power awaits!'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 下一章将进一步提升这一概念：我们将深入研究 PySpark 和 pandas 之间的交互，通过特殊类型的 UDF。PySpark 和 pandas 是一个吸引人的组合，并且能够通过
    Spark 框架扩展 Pandas，这将提升这两个库。乐趣与力量在等待！
- en: Summary
  id: totrans-169
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The resilient distributed dataset allows for better flexibility compared to
    the records and columns approach of the data frame.
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 弹性的分布式数据集（resilient distributed dataset）与数据框的记录和列方法相比，提供了更好的灵活性。
- en: The most low-level and flexible way of running Python code within the distributed
    Spark environment is to use the RDD. With an RDD, you have no structure imposed
    on your data and need to manage type information in your program and defensively
    code against potential exceptions.
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分布式 Spark 环境中运行 Python 代码的最底层和最灵活的方式是使用 RDD。使用 RDD，你的数据没有结构上的限制，需要在程序中管理类型信息，并防御性地编写代码以应对潜在的异常。
- en: The API for data processing on the RDD is heavily inspired by the MapReduce
    framework. You use higher-order functions such as `map()`, `filter()`, and `reduce()`
    on the objects of the RDD.
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RDD 上的数据处理 API 严重受到 MapReduce 框架的启发。你在 RDD 的对象上使用高阶函数，如 `map()`、`filter()` 和
    `reduce()`。
- en: The data frame’s most basic Python code promotion functionality, called the
    (PySpark) UDF, emulates the “map” part of the RDD. You use it as a scalar function,
    taking `Column` objects as parameters and returning a single `Column`.
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据框最基本的 Python 代码提升功能，称为（PySpark）UDF，模拟了 RDD 的“map”部分。你将其用作标量函数，以 `Column` 对象作为参数，并返回一个单一的
    `Column`。
- en: Additional exercises
  id: totrans-174
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 额外练习
- en: Exercise 8.3
  id: totrans-175
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8.3
- en: Using the following definitions, create a `temp_to_temp(value,` `from,` `to)`
    that takes a numerical `value` in `from` degrees and converts it `to` degrees.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下定义，创建一个 `temp_to_temp(value, from, to)` 函数，它接受 `from` 度的数值 `value` 并将其转换为
    `to` 度。
- en: '`C` `=` `(F` `-` `32)` `*` `5` `/` `9` (Celsius)'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`C` `=` `(F` `-` `32)` `*` `5` `/` `9`（摄氏度）'
- en: '`K` `=` `C` `+` `273.15` (Kelvin)'
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`K` `=` `C` `+` `273.15`（开尔文）'
- en: '`R` `=` `F` `+` `459.67` (Rankine)'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`R` `=` `F` `+` `459.67`（兰金）'
- en: Exercise 8.4
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8.4
- en: Correct the following UDF, so it doesn’t generate an error.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 修正以下 UDF，使其不会产生错误。
- en: '[PRE12]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Exercise 8.5
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8.5
- en: Create a UDF that adds two fractions together, and test it by adding the `reduced_
    fraction` to itself in the `test_frac` data frame.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 UDF（用户定义函数），用于将两个分数相加，并通过在 `test_frac` 数据框中将 `reduced_fraction` 加到自身来测试它。
- en: Exercise 8.6
  id: totrans-185
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习 8.6
- en: Because of the `LongType()`, the `py_reduce_fraction` (see the previous exercise)
    will not work if the numerator or denominator exceeds `pow(2,` `63)-1` or is lower
    than `-pow(2,` `63)`. Modify the `py_reduce_fraction` to return `None` if this
    is the case.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 `LongType()`，如果分子或分母超过 `pow(2, 63)-1` 或低于 `-pow(2, 63)`，则 `py_reduce_fraction`（见前一个练习）将无法工作。修改
    `py_reduce_fraction`，使其在这种情况下返回 `None`。
- en: '**Bonus**: Does this change the type annotation provided? Why?'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: '**附加题**：这会改变提供的类型注解吗？为什么？'
