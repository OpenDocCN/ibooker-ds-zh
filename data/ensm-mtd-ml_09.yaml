- en: 'Part 3 Ensembles in the wild: Adapting ensemble methods to your data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第3部分：野外的集成：将集成方法适应你的数据
- en: 'The world of data is a wild and dangerous place for a data scientist. We must
    contend with different types of data, such as counts, categories, and strings,
    strewn with missing values and noise. We are asked to build predictive models
    for different types of tasks: binary classification, multiclass classification,
    regression, and ranking.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家的世界是一个充满狂野和危险的领域。我们必须应对不同类型的数据，例如计数、类别和字符串，这些数据中充满了缺失值和噪声。我们被要求为不同类型的任务构建预测模型：二元分类、多类分类、回归和排名。
- en: We have to build our machine-learning pipelines and preprocess our data with
    care to avoid data leakage. They have to be accurate, fast, robust, and meme-worthy
    (ok, that last one is probably optional). After all this, we end up with models
    that may well do the job they were trained for but are ultimately black boxes
    that no one understands.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 我们必须谨慎构建我们的机器学习管道并预处理我们的数据，以避免数据泄露。它们必须准确、快速、健壮，并且值得传播（好吧，最后一个可能不是必需的）。经过这一切，我们最终得到的模型可能确实能完成它们被训练的任务，但最终是没有人理解的黑盒。
- en: In this final part of the book, you’ll learn how to tackle these challenges,
    armed with the arsenal of ensemble methods from the previous part of the book,
    as well as a few new ensemble methods. This is your last stop from ensembler-in-training
    to seasoned ensembler-explorer of the wild world of data.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的最后一部分，你将学习如何应对这些挑战，凭借上一部分书中提供的集成方法武器库，以及一些新的集成方法。这是你从正在训练的集成者到经验丰富的野外数据世界集成者的最后一站。
- en: Chapter 7 covers ensemble learning for regression tasks, where you’ll learn
    how to adapt different ensemble methods to handle continuous and count-valued
    labels.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第7章涵盖了回归任务的集成学习方法，你将学习如何适应不同的集成方法来处理连续和计数标签。
- en: Chapter 8 covers ensemble learning with nonnumeric features, where you’ll learn
    how to encode categorical and string-valued features before or during ensembling.
    You’ll also learn about two pervasive issues that arise during such preprocessing
    (and sometimes in other ways)—data leakage and prediction shift—and how they often
    mess with our ability to accurately evaluate model performance. In addition, chapter
    8 introduces another variant of gradient boosting called ordered boosting and
    another powerful gradient boosting package called CatBoost, which is similar to
    LightGBM and XGBoost.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章涵盖了具有非数值特征的集成学习方法，你将学习在集成之前或期间如何编码类别和字符串值特征。你还将了解在预处理过程中（有时以其他方式）出现的两个普遍问题——数据泄露和预测偏移——以及它们如何经常干扰我们准确评估模型性能的能力。此外，第8章还介绍了一种名为有序提升的梯度提升变体，以及一个名为CatBoost的强大梯度提升包，它与LightGBM和XGBoost类似。
- en: Chapter 9 covers the exciting new area of explainable AI, which seeks to create
    models that humans can understand and trust. While this chapter is presented from
    the perspective of ensemble methods, many explainability methods (e.g., surrogate
    models, LIME, and SHAP) covered in this chapter can be applied to any machine-learning
    model. Chapter 9 also introduces explainable boosting machines, a type of ensemble
    method that is explicitly designed to be directly explainable.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章涵盖了令人兴奋的新领域——可解释人工智能，它寻求创建人类可以理解和信任的模型。虽然本章从集成方法的角度进行介绍，但本章涵盖的许多可解释方法（例如，代理模型、LIME和SHAP）可以应用于任何机器学习模型。第9章还介绍了可解释提升机，这是一种专门设计为直接可解释的集成方法类型。
- en: This part of the book covers advanced topics in ensemble methods and builds
    on some key concepts from part 2, especially gradient boosting. Don’t hesitate
    to jump back to part 2 as a refresher or reference, as needed.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本书这一部分涵盖了集成方法的进阶主题，并基于第2部分的一些关键概念，特别是梯度提升。如有需要，不要犹豫，随时回到第2部分进行复习或参考。
