- en: 4 Training fundamentals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 训练基础
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Forward feeding and backward propagation
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 前向传播和反向传播
- en: Splitting datasets and preprocessing data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割数据集和预处理数据
- en: Using validation data to monitor overfitting
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用验证数据监控过拟合
- en: Using checkpointing and early stopping for more-economical training
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用检查点和提前停止以实现更经济的训练
- en: Using hyperparameters versus model parameters
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用超参数与模型参数
- en: Training for invariance to location and scale
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练对位置和尺度的不变性
- en: Assembling and accessing on-disk datasets
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组装和访问磁盘上的数据集
- en: Saving and then restoring a trained model
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保存并恢复训练好的模型
- en: This chapter covers the fundamentals of training a model. Prior to 2019, the
    majority of models were trained according to this set of fundamental steps. Consider
    this chapter as a foundation.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了训练模型的基础知识。在2019年之前，大多数模型都是根据这一套基本步骤进行训练的。可以将本章视为一个基础。
- en: In this chapter, we cover methods, techniques, and best practices developed
    over time by experimentation and trial and error. We will start by reviewing forward
    feeding and backward propagation. While these concepts and practices pre-existed
    deep learning, numerous refinements over the years made model training practical—specifically,
    in the way we split the data, feed it, and then update weights using gradient
    descent during backward propagation. These technique refinements provided the
    means to train models to convergence, the point where the accuracy of the model
    to predict would plateau.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍随着时间的推移通过实验和试错开发的方法、技术和最佳实践。我们将从回顾前向传播和反向传播开始。虽然这些概念和实践在深度学习之前就已经存在，但多年的改进使模型训练变得实用——特别是，在数据分割、喂入以及使用梯度下降在反向传播期间更新权重的方式。这些技术改进提供了训练模型到收敛的手段，即模型预测准确率达到平台期的点。
- en: Other training techniques in data preprocessing and augmentation were developed
    to push convergence to higher plateaus, and aid models into better generalizing
    to data that the model was not trained on. Further refinements continued to make
    training more economical through hyperparameter search and tuning, along with
    checkpointing and early stopping, and more-efficient formats and methods of drawing
    data from disk storage during training. All these techniques combined led to making
    deep learning for real-world applications practical both computationally and economically.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据预处理和增强中开发的其他训练技术旨在推动收敛到更高的平台，并帮助模型更好地泛化到模型未训练过的数据。进一步的改进继续通过超参数搜索和调整、检查点和提前停止，以及更高效地从磁盘存储中抽取数据格式和方法，使训练更加经济。所有这些技术相结合，使得深度学习在现实世界中的应用在计算和经济上都具有可行性。
- en: 4.1 Forward feeding and backward propagation
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.1 前向传播和反向传播
- en: Let’s start with an overview of supervised training. When training a model,
    you feed data forward through the model, and compute how incorrect the predicted
    results are—the *loss*. Then the loss is backward-propagated to make updates to
    the model’s parameters, which is what the model is learning—the values for the
    parameters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从监督训练的概述开始。在训练模型时，你将数据前向通过模型，并计算预测结果的错误程度——*损失*。然后，将损失反向传播以更新模型的参数，这就是模型正在学习的内容——参数的值。
- en: 'When training a model, you start with training data that’s representative of
    the target environment where the model will be deployed. That data, in other words,
    is a sampling distribution of a population distribution. The training data consists
    of examples. Each example has two parts: the features, also referred to as *independent
    variables*; and corresponding labels, also referred to as the *dependent variable*.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练模型时，你从代表模型将部署的目标环境的训练数据开始。换句话说，这些数据是人群分布的采样分布。训练数据由示例组成。每个示例有两个部分：特征，也称为*独立变量*；以及相应的标签，也称为*因变量*。
- en: The labels are also known as the *ground truths* (the “correct answers”). Our
    goal is to train a model that, once deployed and given examples without labels
    from the population (examples the model has never seen before), the model is generalized
    so that it can accurately predict the label (the “correct answer”)—supervised
    learning. This step is known as *inference*.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 标签也被称为*真实值*（“正确答案”）。我们的目标是训练一个模型，一旦部署并给出来自人群（模型之前从未见过的例子）的无标签示例，模型就能泛化到足以准确预测标签（“正确答案”）——监督学习。这一步被称为*推理*。
- en: During training, we feed *batches* (also called *samples*) of the training data
    to the model through the input layer (also referred to as the *bottom* of the
    model). The training data is then transformed by the parameters (weights and biases)
    in the layers of the model as it moves forward toward the output nodes (also referred
    to as the *top* of the model). At the output nodes, we measure how far away we
    are from the “correct” answers, which, again, is called the *loss.* We then backward-propagate
    the loss through the layers of the models and update the parameters to be closer
    to getting the correct answer on the next batch.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练过程中，我们通过输入层（也称为模型的**底部**）将训练数据的**批次**（也称为**样本**）输入到模型中。随着数据向前移动，经过模型各层的参数（权重和偏置）的转换，训练数据被转化为输出节点（也称为模型的**顶部**）。在输出节点，我们测量我们与“正确”答案的距离，这被称为**损失**。然后，我们将损失反向传播通过模型的各层，并更新参数以更接近下一个批次得到正确答案。
- en: We continue to repeat this process until we reach *convergence*, which could
    be described as “this is as accurate as we can get on this training run.”
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续重复这个过程，直到达到**收敛**，这可以描述为“在这个训练运行中，我们已经达到了尽可能高的准确性。”
- en: 4.1.1 Feeding
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.1 输入
- en: '*Feeding* is the process of sampling batches from the training data and forward-feeding
    the batches through the model, and then calculating the loss at the output. A
    batch can be one or more examples from the training data chosen at random.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**是从训练数据中采样批次并通过模型前馈批次的过程，然后在输出处计算损失。一个批次可以是随机选择的一个或多个训练数据示例。'
- en: The size of the batch is typically constant, which is referred to as the (mini)
    *batch size*. All the training data is split into batches, and typically each
    example will appear in only one batch.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 批次的大小通常是恒定的，这被称为（迷你）**批次大小**。所有训练数据被分成批次，通常每个示例只会出现在一个批次中。
- en: All of the training data is fed multiple times to the model. Each time we feed
    the entire training data, it is called an *epoch*. Each epoch is a different random
    permutation into batches—that is, no two epochs have the same ordering of examples—as
    depicted in figure 4.1.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练数据被多次输入到模型中。每次我们输入整个训练数据，都称为一个**epoch**。每个epoch是批次的不同随机排列——也就是说，没有两个epoch具有相同的示例顺序——如图4.1所示。
- en: '![](Images/CH04_F01_Ferlitsch.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F01_Ferlitsch.png)'
- en: Figure 4.1 Mini-batches from the training data are fed forward through the neural
    network during training.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1 在训练过程中，训练数据的迷你批次通过神经网络前馈。
- en: 4.1.2 Backward propagation
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1.2 反向传播
- en: In this section, we explore the importance of the discovery of backward propagation
    and how it is used today.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨反向传播发现的的重要性以及它今天的应用。
- en: Background
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 背景
- en: Let’s take a step back in history to understand the importance of how backward
    propagation contributed to the success of deep learning. In early neural networks,
    like the perceptron and single-layer neurons, academic researchers experimented
    with ways to update weights to get the correct answer.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾历史，了解反向传播对深度学习成功的重要性。在早期的神经网络中，如感知器和单层神经元，学术研究人员尝试了各种更新权重的方法以获得正确答案。
- en: When they were working with just a few neurons and simple problems, the logical
    first tries were to just make random updates. Eventually, lo and behold, a random
    guess would work. Well, this was not scalable to large numbers of neurons (say,
    thousands) and real-world applications; making the correct random guess could
    take millions of years.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当他们只使用少量神经元和简单问题时，逻辑上的第一次尝试只是随机更新。最终，令人惊讶的是，随机猜测竟然有效。然而，这种方法并不适用于大量神经元（比如数千个）和实际应用；正确的随机猜测可能需要数百万年。
- en: The next logical step tried was to make the random value proportional to how
    far off the prediction is. In other words, the further off, the larger the range
    of random values; and the closer, the smaller the range. Not bad—now we’re down
    to maybe a thousand years on a real-world application to guess the right random
    values.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个逻辑步骤是使随机值与预测的偏差成正比。换句话说，偏差越大，随机值的范围就越大；偏差越小，随机值的范围就越小。不错——现在我们可能只需要数千年就能在实际应用中猜测正确的随机值。
- en: Eventually, academic researchers experimented with multilayer perceptrons (MLPs),
    and the technique of making the random values proportional to how far off they
    were from the correct answer—the loss—just didn’t work. They found that when you
    have multiple layers, this technique has the effect of the left hand—one layer—undoing
    the work of the right hand—another layer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，学术研究人员尝试了多层感知器（MLPs），但将随机值与它们偏离正确答案的程度（损失）成比例的技术并没有奏效。他们发现，当你有多个层次时，这种技术会产生左手（一层）抵消右手（另一层）工作的效果。
- en: These researchers discovered that while the updates to the weights of the output
    layer are relative to the loss in the prediction, updates to weights in earlier
    layers are relative to the updates in the next layer. Thus, the concept of backward
    propagation was formed. At this point, academic researchers went beyond using
    random distributions to calculate an update. Many things were tried, with no improvements,
    until a technique was developed to update weights not to the amount of change
    in the next layer, but relative to the rate of change—hence the discovery and
    development of gradient descent techniques.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 这些研究人员发现，虽然输出层权重的更新与预测中的损失相关，但早期层中权重的更新与下一层的更新相关。因此，形成了反向传播的概念。在此阶段，学术研究人员超越了仅使用随机分布来计算更新的方法。尝试了许多方法，但都没有改进，直到开发出一种更新权重的方法，不是基于下一层的变化量，而是基于变化率——因此发现了梯度下降技术并进行了发展。
- en: Batch-based backward propagation
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 基于批次的反向传播
- en: After each batch of training data is forward-fed through the model and the loss
    is calculated, the loss is backward-propagated through the model. We go layer
    by layer updating the model’s parameters (weights and parameters), starting at
    the top layer (output) and moving toward the bottom layer (input). How the parameters
    are updated is a combination of the loss, the values of the current parameters,
    and the updates made to the proceeding layer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在将每个训练数据批次正向通过模型并计算损失后，损失将通过模型进行反向传播。我们逐层更新模型的参数（权重和参数），从顶层（输出）开始，移动到底层（输入）。参数如何更新是损失、当前参数的值以及前一层所做的更新的组合。
- en: The general method for doing this is based on *gradient descent*. The optimizer
    is an implementation of gradient descent whose job is to update the parameters
    to minimize the loss (maximize getting closer to the correct answer) on subsequent
    batches. Figure 4.2 illustrates this process.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这一般方法的通用方法是基于*梯度下降*。优化器是梯度下降的一种实现，其任务是更新参数以最小化后续批次上的损失（最大化接近正确答案的程度）。图4.2展示了这一过程。
- en: '![](Images/CH04_F02_Ferlitsch.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH04_F02_Ferlitsch.png)'
- en: Figure 4.2 The calculated loss from a mini-batch is backward-propagated; the
    optimizer updates weights to minimize the loss on the next batch.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 从迷你批次计算出的损失通过反向传播；优化器更新权重以最小化下一批次的损失。
- en: 4.2 Dataset splitting
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.2 数据集划分
- en: A *dataset* is a collection of examples that are large and diverse enough to
    be representative of the population being modeled (the sampling distribution).
    When a dataset meets this definition and is cleaned (not noisy), and in a format
    that’s ready for machine learning training, we refer to it as a *curated dataset*.
    This book does not cover details of dataset cleaning, as it is a large and diverse
    topic that would be a book in its own right. We do touch on aspects of data cleaning
    throughout the book, where relevant.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据集*是一组足够大且多样化的示例，足以代表所建模的总体（采样分布）。当一个数据集符合此定义，并且经过清理（无噪声），以及以适合机器学习训练的格式准备时，我们称之为*精心制作的数据集*。本书不涉及数据集清理的细节，因为它是一个庞大且多样化的主题，可以成为一本单独的书籍。我们在本书中涉及数据清理的各个方面，当相关时。'
- en: A wide variety of curated datasets are available for academic and research purposes.
    Some of the well-known ones for image classification are MNIST (introduced in
    chapter 2), CIFAR-10/100, SVHN, Flowers, and Cats vs. Dogs. MNIST and CIFAR-10/100
    (Canadian Institute for Advanced Research) are built into the TF.Keras framework.
    SVHN (Street View Home Numbers), Flowers, and Cats vs. Dogs are available with
    TensorFlow Datasets (TFDS). Throughout this section, we will be using these datasets
    for tutorial purposes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对于学术和研究目的，有许多精心制作的数据库可供使用。其中一些用于图像分类的知名数据库包括MNIST（在第2章中介绍）、CIFAR-10/100、SVHN、Flowers和Cats
    vs. Dogs。MNIST和CIFAR-10/100（加拿大高级研究研究所）已内置到TF.Keras框架中。SVHN（街景房屋号码）、Flowers和Cats
    vs. Dogs可通过TensorFlow Datasets（TFDS）获得。在本节中，我们将使用这些数据集进行教程演示。
- en: Once you have a curated dataset, the next step is to split it into examples
    that will be used for training and those that will be used for testing (also called
    *evaluation* or *holdout*). We train the model with the portion of the dataset
    that is the training data. If we assume the training data is a good sampling distribution
    (representative of the population distribution), the accuracy of the training
    data should reflect the accuracy when deployed to the real-world predictions on
    examples from the population not seen by the model during training.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你拥有了一个精心挑选的数据集，下一步就是将其分割成用于训练的示例和用于测试（也称为*评估*或*保留*）的示例。我们使用数据集中作为训练数据的部分来训练模型。如果我们假设训练数据是一个好的采样分布（代表总体分布），那么训练数据的准确性应该反映在将模型部署到现实世界中对模型在训练期间未见过的总体中的示例进行预测时的准确性。
- en: But how do we know whether this is true before we deploy the model? Hence the
    purpose for the test (holdout) data. We set aside a portion of the dataset that
    we will test against after the model is done training and see if we get comparable
    accuracy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们部署模型之前，我们如何知道这是否正确呢？因此，测试（保留）数据的目的。我们在模型训练完成后，留出一部分数据集来测试，看看我们是否可以得到可比的准确性。
- en: 'For example, let’s say that when we are done training, we have 99% accuracy
    on the training data, but only 70% accuracy on the test data. Something went wrong
    (for example, overfitting). So how much do we set aside for training and testing?
    Historically, the rule of thumb has been 80/20: 80% for training and 20% for testing.
    That has changed, but we will start with this rule of thumb and in later chapters
    discuss the modern updates.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设我们完成训练后，在训练数据上达到了99%的准确性，但在测试数据上只有70%的准确性。出了些问题（例如，过拟合）。那么我们为训练和测试预留多少呢？从历史上看，经验法则一直是80/20：80%用于训练，20%用于测试。但这已经改变了，但我们将从这一经验法则开始，并在后面的章节中讨论现代更新。
- en: 4.2.1 Training and test sets
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.1 训练和测试集
- en: What is important is that we are able to assume our dataset is sufficiently
    large enough that if we split it into 80% and 20%, and the examples are randomly
    chosen so that both datasets will be good sampling distributions representative
    of the population distribution, the model will make predictions (inference) after
    it’s deployed. Figure 4.3 illustrates this process.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，我们能够假设我们的数据集足够大，以至于如果我们将其分成80%和20%，并且示例是随机选择的，以便两个数据集都将成为代表总体分布的好的采样分布，那么模型在部署后将会做出预测（推理）。图4.3说明了这个过程。
- en: '![](Images/CH04_F03_Ferlitsch.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F03_Ferlitsch.png)'
- en: Figure 4.3 Training data is first randomly shuffled before being split into
    training and test data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 在分割成训练和测试数据之前，训练数据首先被随机打乱。
- en: 'Let’s start with the step-by-step process of training with a curated dataset.
    For our first step, we import the curated TF.Keras built-in MNIST dataset, as
    demonstrated in the following code. The TF.Keras built-in datasets have a `load_data()`
    method. This method loads into memory the dataset, already randomly shuffled and
    presplit into training and test data. Both training and test data are further
    separated into the features (the image data, in this case) and the corresponding
    labels (the numerical values 0 to 9, representing each digit). It is a common
    convention to refer to the features and labels for training and testing as `(x_train,`
    `y_train)` and `(x_test,` `y_test)`, respectively:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用精心挑选的数据集进行训练的逐步过程开始。作为第一步，我们导入精心挑选的TF.Keras内置MNIST数据集，如下面的代码所示。TF.Keras内置数据集有一个`load_data()`方法。此方法将数据集加载到内存中，该数据集已经随机打乱并预先分割成训练和测试数据。训练和测试数据进一步被分成特征（在这种情况下是图像数据）和相应的标签（代表每个数字的数值0到9）。将训练和测试的特征和标签分别称为`(x_train,
    y_train)`和`(x_test, y_test)`是一种常见的约定：
- en: '[PRE0]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ MNIST is a built-in dataset in the framework.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ MNIST是该框架中的一个内置数据集。
- en: ❷ Built-in dataset is automatically randomly shuffled and presplit into training
    and testing data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 内置数据集会自动随机打乱并预先分割成训练和测试数据。
- en: 'The MNIST dataset consists of 60,000 training and 10,000 test examples, with
    an even (balanced) distribution across the ten digits 0 to 9\. Each example consists
    of a 28-×-28-pixel grayscale image (single channel). From the following output,
    you can see that the training data `(x_train, y_train)` consists of 60,000 examples
    of size 28 × 28 images and corresponding 60,000 labels, while the test data `(x_test,
    y_test)` consists of 10,000 examples and labels:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: MNIST数据集包含60,000个训练样本和10,000个测试样本，十个数字0到9的分布均匀（平衡）。每个示例由一个28×28像素的灰度图像（单通道）组成。从以下输出中，你可以看到训练数据`(x_train,
    y_train)`由60,000个大小为28×28的图像和相应的60,000个标签组成，而测试数据`(x_test, y_test)`由10,000个示例和标签组成：
- en: '[PRE1]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 4.2.2 One-hot encoding
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2.2 独热编码
- en: Let’s build a simple DNN to train our curated dataset. In the next code example,
    we start by flattening the 28-×-28-image input into a 1D vector by using the `Flatten`
    layer, which is then followed by two hidden `Dense()` layers of 512 nodes each,
    each using the convention of a `relu` activation function. Finally, the output
    layer is a `Dense` layer with 10 nodes, one for each digit. Since this is a multiclass
    classifier, the activation function for the output layer is a `softmax`.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建一个简单的DNN来训练我们的精选数据集。在下一个代码示例中，我们首先通过使用`Flatten`层将28×28图像输入展平为1D向量，然后是两个各有512个节点的隐藏`Dense()`层，每个层使用`relu`激活函数的约定。最后，输出层是一个有10个节点的`Dense`层，每个数字一个节点。由于这是一个多类分类器，输出层的激活函数是`softmax`。
- en: 'Next, we compile the model for the convention for multiclass classifiers by
    using `categorical_crossentropy` for the loss and `adam` for the optimizer:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过使用`categorical_crossentropy`作为损失函数和`adam`作为优化器来编译模型，以符合多类分类器的约定：
- en: '[PRE2]'
  id: totrans-57
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Flattens the 2D grayscale image into 1D vector for a DNN
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将2D灰度图像展平为1D向量以供DNN使用
- en: ❷ The actual input layer of the DNN, once the image is flattened
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ DNN的实际输入层，一旦图像被展平
- en: ❸ A hidden layer
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个隐藏层
- en: ❹ The output layer of the DNN
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ DNN的输出层
- en: 'The most basic way to train this model with this dataset is to use the `fit()`
    method. We will pass as parameters the training data `(x_train,` `y_train)`. We
    will keep the remaining keyword parameters set to their defaults:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此数据集训练此模型的最基本方法是使用`fit()`方法。我们将传递训练数据`(x_train, y_train)`作为参数。我们将剩余的关键字参数设置为它们的默认值：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'When you run the preceding code, you will see an error message:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 当你运行前面的代码时，你会看到一个错误信息：
- en: '[PRE4]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: What went wrong? This is an issue with the loss function we choose. It will
    compare the difference between each output node and corresponding output expectation.
    For example, if the answer is the digit 3, we need a 10-element vector (one element
    per digit) with a 1 (100% probability) in the 3 index and 0s (0% probability)
    in the remaining indexes. In this case, we need to convert the scalar-value labels
    into 10-element vectors with a 1 in the corresponding index. This is known as
    *one-hot encoding*, depicted in figure 4.4.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 发生了什么问题？这是与我们选择的损失函数有关的问题。它将比较每个输出节点与相应的输出期望之间的差异。例如，如果答案是数字3，我们需要一个10个元素的向量（每个数字一个元素），在3的索引处有一个1（100%的概率），在其余索引处有0（0%的概率）。在这种情况下，我们需要将标量值标签转换为在相应索引处有1的10个元素的向量。这被称为*独热编码*，如图4.4所示。
- en: '![](Images/CH04_F04_Ferlitsch.png)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F04_Ferlitsch.png)'
- en: Figure 4.4 The size of a one-hot-encoded label is the same as the number of
    output classes.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 独热编码标签的大小与输出类别的数量相同。
- en: 'Let’s fix our example by first importing the `to_categorical()` function from
    TF.Keras and then using it to convert the scalar-value labels to one-hot-encoded
    labels. Note that we pass the value 10 to `to_categorical()` to indicate the size
    of the one-hot-encoded labels (number of classes):'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过首先从TF.Keras导入`to_categorical()`函数，然后使用它将标量值标签转换为独热编码标签来修复我们的示例。注意，我们将值10传递给`to_categorical()`，以指示独热编码标签的大小（类别数量）：
- en: '[PRE5]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Method is used for one-hot encoding
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用方法进行独热编码
- en: ❷ One-hot-encodes the training and testing labels
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对训练和测试标签进行独热编码
- en: 'Now when you run this, your output will look something like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你运行这个，你的输出将看起来像这样：
- en: '[PRE6]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The accuracy on the training data is just over 90%.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据上的准确率刚好超过90%。
- en: That works, and we got 90% accuracy on the training data—but we can simplify
    this step. The `compile()` method has one-hot encoding built into it. To enable
    it, we just change the loss function from `categorical_crossentropy` to `sparse_categorical_
    crossentry`. In this mode, the loss function will receive the labels as scalar
    values and dynamically convert them to one-hot-encoded labels before performing
    the cross-entropy loss calculation.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做是有效的，我们在训练数据上达到了90%的准确率——但我们还可以简化这一步骤。`compile()`方法内置了one-hot编码。要启用它，我们只需将损失函数从`categorical_crossentropy`更改为`sparse_categorical_crossentropy`。在此模式下，损失函数将接收标签作为标量值，并在执行交叉熵损失计算之前动态地将它们转换为one-hot编码的标签。
- en: 'We do this in the following example and additionally set the keyword parameter
    `epoch` to 10 to feed the entire training data to the model 10 times:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在以下示例中这样做，并且还设置关键字参数`epoch`为10，以便将整个训练数据输入模型10次：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Loads MNIST dataset into memory
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将MNIST数据集加载到内存中
- en: ❷ Trains MNIST model for 10 epochs
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 对MNIST模型进行10个epoch的训练
- en: 'After the 10th epoch, you should see the accuracy on the training data at around
    97%:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在第10个epoch后，你应该看到训练数据上的准确率大约为97%：
- en: '[PRE8]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 4.3 Data normalization
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.3 数据正则化
- en: We can further improve on this. The image data loaded by the `mnist()` module
    is in raw format; each image is a 28 × 28 matrix of integer values from 0 to 255\.
    If you were to inspect the parameters (weights and biases) within a trained model,
    they are very small numbers, typically from –1 to 1\. Generally, when data feeds
    forward through the layer and the parameters of one layer are matrix-multiplied
    against parameters at the next layer, the result is a very small number.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进一步改进这一点。通过`mnist()`模块加载的图像数据是原始格式；每个图像是一个28 × 28的整数值矩阵，范围从0到255。如果你检查训练模型内的参数（权重和偏差），它们是非常小的数字，通常从-1到1。通常，当数据通过层前馈并通过一层参数矩阵乘以下一层的参数时，结果是一个非常小的数字。
- en: The problem with our preceding example is that the input values are substantially
    larger (up to 255), which will produce large numbers initially as they are multiplied
    through the layers. This will result in taking longer for the parameters to learn
    their optimal values—if they learn them at all.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们前面示例的问题在于输入值相当大（高达255），这将在通过层乘法时产生很大的初始数值。这将导致参数学习它们的最佳值需要更长的时间——如果它们能学习的话。
- en: 4.3.1 Normalization
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.1 正则化
- en: We can increase the speed at which the parameters learn the optimal values and
    increase our chances of convergence (discussed subsequently) by squashing the
    input values into a smaller range. One simple way to do this is to squash them
    proportionally into a range from 0 to 1\. We can do this by dividing each value
    by 255.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将输入值压缩到更小的范围来增加参数学习最优值的速度，并提高我们的收敛概率（随后讨论）。一种简单的方法是将它们按比例压缩到0到1的范围。我们可以通过将每个值除以255来实现这一点。
- en: In the following code, we add the step of normalizing the input data by dividing
    each pixel value by 255\. The `load_data()` function loads the dataset into memory
    in a NumPy format. *NumPy*, a high-performance array-handling module written in
    C with a Python wrapper (CPython), is highly efficient for feeding data during
    training of a model, when the entire training dataset is in memory. (Chapter 13
    covers methods and formats when the training dataset is too large to fit into
    memory.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们添加了通过将每个像素值除以255来规范化输入数据的步骤。`load_data()`函数以NumPy格式将数据集加载到内存中。*NumPy*是一个用C语言编写并带有Python包装器（CPython）的高性能数组处理模块，在模型训练期间，当整个训练数据集都在内存中时，它非常高效。第13章涵盖了当训练数据集太大而无法放入内存时的方法和格式。
- en: A *NumPy array* is a class object that implements polymorphism on arithmetic
    operators. In our example, we show a single division operation `(x_train` `/`
    `255.0)`. The division operator is overridden for NumPy arrays and implements
    a broadcast operation—which means that every element in the array will be divided
    by 255.0.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*NumPy数组*是一个实现算术运算符多态性的类对象。在我们的例子中，我们展示了单个除法操作`(x_train / 255.0)`。除法运算符被NumPy数组覆盖，并实现了广播操作——这意味着数组中的每个元素都将除以255.0。
- en: 'By default, NumPy does floating-point operations as double precision (64 bits).
    By default, the parameters in a TF.Keras model are single-precision floating-point
    (32 bits). For efficiency, as a last step, we convert the result of the broadcasted
    division to 32 bits by using the NumPy `astype()` method. If we did not do the
    conversion, the initial matrix multiplication from the input-to-input layer would
    take double the number of machine cycles (64 × 32 instead of 32 × 32):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，NumPy使用双精度（64位）进行浮点运算。默认情况下，TF.Keras模型中的参数是单精度浮点数（32位）。为了提高效率，作为最后一步，我们使用NumPy的`astype()`方法将广播除法的结果转换为32位。如果我们没有进行转换，从输入层到输入层的初始矩阵乘法将需要双倍的机器周期（64×32而不是32×32）：
- en: '[PRE9]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Normalizes the pixel data from 0 to 1
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将像素数据从0标准化到1
- en: 'The following is the output from running the preceding code. Let’s compare
    the output with a normalized input to the prior non-normalized input. In the prior
    input, we reached 97% accuracy after the 10th epoch. In our normalized input,
    we reach the same accuracy after just the second epoch, and almost 99.5% accuracy
    after the tenth. Thus, we learned faster and more accurately when we normalized
    the input data:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的输出是运行前面代码的结果。让我们将输出与先前非标准化输入的标准化输入进行比较。在先前的输入中，我们在第10个epoch后达到了97%的准确率。在我们的标准化输入中，我们只需第二个epoch就达到了相同的准确率，在第10个epoch后几乎达到了99.5%的准确率。因此，当我们标准化输入数据时，我们学得更快，更准确：
- en: '[PRE10]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Let’s now evaluate our model by using the `evaluate()` method on the test (holdout)
    data to see how well the model will perform on data it has never seen during training.
    The `evaluate()` method operates in inference mode: the test data is forward-fed
    through the model to make predictions, but there is no backward propagation. The
    model’s parameters are not updated. Finally, `evaluate()` will output the loss
    and overall accuracy:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过在测试（保留）数据上使用`evaluate()`方法来评估我们的模型，看看模型在训练期间从未见过的数据上的表现如何。`evaluate()`方法在推理模式下操作：测试数据被正向传递通过模型进行预测，但没有反向传播。模型的参数不会被更新。最后，`evaluate()`将输出损失和整体准确率：
- en: '[PRE11]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'In the following output, we see that the accuracy is about 98%, compared to
    the training accuracy of 99.5%. This is expected. Some overfitting always occurs
    during training. What we are looking for is a very small difference between the
    training and testing, and in this case it’s about 1.5%:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，我们看到准确率约为98%，与训练准确率99.5%相比。这是预期的。在训练过程中总会发生一些过拟合。我们寻找的是训练和测试之间非常小的差异，在这种情况下大约是1.5%：
- en: '[PRE12]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 4.3.2 Standardization
  id: totrans-99
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3.2 标准化
- en: 'There are a variety of ways to squash the input data beyond the normalization
    used in the preceding example. For example, some ML practitioners prefer to squash
    the input values between –1 and 1 (instead of 0 and 1), so that the values are
    centered at 0\. The following code is an example implementation that divides each
    element by one-half the maximum value (in this example, 127.5) and then subtracts
    1 from the result:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前面示例中使用的归一化之外，还有许多方法可以压缩输入数据。例如，一些机器学习从业者更喜欢将输入值压缩在-1和1之间（而不是0和1），这样值就集中在0。以下代码是一个示例实现，它将每个元素除以最大值的一半（在这个例子中是127.5），然后从结果中减去1：
- en: '[PRE13]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Does squashing the values between –1 and 1 produce better results than between
    0 and 1? I haven’t seen anything in the research literature, or my own experience,
    that indicates a difference.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 将值压缩在-1和1之间是否比压缩在0和1之间产生更好的结果？我在研究文献或我的个人经验中都没有看到任何表明有差异的内容。
- en: 'This and the previous method don’t require any pre-analysis of the input data,
    other than knowing the maximum value. Another technique, called *standardization*,
    is considered to give a better result. However, it requires a pre-analysis (scan)
    over the entire input data to find its mean and standard deviation. You then center
    the data at the mean of the full distribution of the input data and squash the
    values between +/– one standard deviation. The following code, which implements
    standardization when the input data is in memory as a NumPy multidimensional array,
    uses the NumPy methods `np.mean``()` and `np.std``()`:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法和之前的方法不需要对输入数据进行任何预分析，除了知道最大值。另一种称为*标准化*的技术被认为可以产生更好的结果。然而，它需要对整个输入数据进行预分析（扫描）以找到其平均值和标准差。然后，你将数据中心化在输入数据全分布的平均值处，并将值压缩在+/-一个标准差之间。以下代码实现了当输入数据作为NumPy多维数组存储在内存中时的标准化，使用了NumPy方法`np.mean()`和`np.std()`：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Calculates the mean value for the pixel data
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算像素数据的平均值
- en: ❷ Calculates the standard deviation for the pixel data
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 计算像素数据的标准差
- en: ❸ Standardization of the pixel data using the mean and standard deviation
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用均值和标准差对像素数据进行标准化
- en: 4.4 Validation and overfitting
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.4 验证和过拟合
- en: This section demonstrates a case of overfitting and then shows how to detect
    overfitting during training and how we might tackle the problem. Let’s revisit
    what *overfitting* means. Typically, to get higher accuracy, we build larger and
    larger models. One consequence is that the model can rote-memorize some or all
    of the examples. The model learns the examples instead of learning to generalize
    from the examples to accurately predict examples it never saw during training.
    In an extreme case, a model could achieve 100% training accuracy yet have random
    accuracy on the testing (for 10 classes, that would be 10% accuracy).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 本节演示了一个过拟合的案例，然后展示了如何在训练过程中检测过拟合以及我们可能如何解决这个问题。让我们重新回顾一下*过拟合*的含义。通常，为了获得更高的准确率，我们会构建更大和更大的模型。一个后果是模型可以死记硬背一些或所有示例。模型学习的是示例，而不是从示例中学习泛化，以准确预测训练过程中从未见过的示例。在极端情况下，一个模型可以达到100%的训练准确率，但在测试（对于10个类别，那就是10%的准确率）时具有随机准确率。
- en: 4.4.1 Validation
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.1 验证
- en: Let’s say training the model takes several hours. Do you really want to wait
    until the end of training and then test on the test data to learn whether the
    model overfitted? Of course not. Instead, we set aside a small portion of the
    training data, which we call *validation data*.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 假设训练模型需要几个小时。你真的想等到训练结束再在测试数据上测试，以了解模型是否过拟合吗？当然不想。相反，我们留出一小部分训练数据，我们称之为*验证数据*。
- en: We don’t train the model with the validation data. Instead, after each epoch,
    we use the validation data to estimate the likely result on the test data. Like
    the test data, the validation data is forward-fed through the model without updating
    the model’s parameters (inference mode), and we measure the loss and accuracy.
    Figure 4.5 depicts this process.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不使用验证数据来训练模型。相反，在每个epoch之后，我们使用验证数据来估计测试数据的可能结果。像测试数据一样，验证数据通过模型前馈（推理模式）而不更新模型的参数，我们测量损失和准确率。图4.5描述了此过程。
- en: '![](Images/CH04_F05_Ferlitsch.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F05_Ferlitsch.png)'
- en: Figure 4.5 On each epoch, the validation data is used to estimate the likely
    accuracy on the test data.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 在每个epoch，使用验证数据来估计测试数据的可能准确率。
- en: If a dataset is very small, and using even less data for training has a negative
    impact, we can use *cross-validation*. Instead of setting aside at the outset
    a portion of the training data that the model will never be trained on, a random
    split is done for each epoch. At the beginning of each epoch, the examples for
    validation are randomly selected and not used for training for that epoch, and
    instead used for the validation test. But since the selection is random, some
    or all of the examples will appear in the training data for other epochs. Today’s
    datasets are large, so you seldom see the need for this technique. Figure 4.6
    illustrates cross-validation splitting of a dataset.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据集非常小，并且使用更少的数据进行训练会产生负面影响，我们可以使用*交叉验证*。不是一开始就留出一部分模型永远不会训练的训练数据，而是在每个epoch进行随机分割。在每个epoch的开始，随机选择验证示例，并在此epoch中不用于训练，而是用于验证测试。但由于选择是随机的，一些或所有示例将出现在其他epoch的训练数据中。今天的
    datasets 都很大，所以很少需要这种技术。图4.6说明了数据集的交叉验证分割。
- en: '![](Images/CH04_F06_Ferlitsch.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F06_Ferlitsch.png)'
- en: Figure 4.6 On each epoch, a randomly selected fold is selected for validation
    data.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 在每个epoch，随机选择一个折作为验证数据。
- en: 'Next, we will train a simple CNN to classify images from the CIFAR-10 dataset.
    Our dataset is a subset of this dataset of tiny images, of size 32 × 32 × 3\.
    It consists of 60,000 training and 10,000 test images covering 10 classes: airplane,
    automobile, bird, cat, deer, dog, frog, horse, ship, and truck.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将训练一个简单的CNN来对CIFAR-10数据集中的图像进行分类。我们的数据集是此小型图像数据集的一个子集，大小为32 × 32 × 3。它包含60,000个训练图像和10,000个测试图像，涵盖了10个类别：飞机、汽车、鸟、猫、鹿、狗、青蛙、马、船和卡车。
- en: In our simple CNN, we have one convolutional layer of 32 filters with kernel
    size 3 × 3, followed by a strided max pooling layer. The output is then flattened
    and passed to the final outputting dense layer. Figure 4.7 illustrates this process.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的简单CNN中，我们有一个32个滤波器的3 × 3核大小的卷积层，后面跟着一个步长最大池化层。然后输出被展平并传递到最后一个输出密集层。图4.7说明了这个过程。
- en: '![](Images/CH04_F07_Ferlitsch.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F07_Ferlitsch.png)'
- en: Figure 4.7 A simple ConvNet for classifying CIFAR-10 images
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 用于分类CIFAR-10图像的简单卷积神经网络
- en: 'Here’s the code to train our simple CNN:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这是训练我们的简单CNN的代码：
- en: '[PRE15]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Uses 10% of training data for validation—not trained on
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用10%的训练数据用于验证——未进行训练
- en: Here, we’ve added the keyword parameter `validation_split=0.1` to the `fit()`
    method to set aside 10% of the training data for validation testing after each
    epoch.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们已将关键字参数 `validation_split=0.1` 添加到 `fit()` 方法中，以便在每个epoch之后为验证测试保留10%的训练数据。
- en: 'The following is the output after running 15 epochs. You can see that after
    the fourth epoch, the training and evaluation accuracy are essentially the same.
    But after the fifth epoch, we start to see them spread apart (65% versus 61%).
    By the 15th epoch, the spread is very large (74% versus 63%). Our model clearly
    started overfitting around the fifth epoch:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是在运行15个epochs后的输出。你可以看到，在第4个epoch之后，训练和评估准确率基本上是相同的。但在第5个epoch之后，我们开始看到它们开始分散（65%对61%）。到第15个epoch时，分散非常大（74%对63%）。我们的模型显然在第5个epoch开始过拟合：
- en: '[PRE16]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ After 4th epoch, accuracy on training data and validation data is about the
    same.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在第4个epoch之后，训练数据和验证数据的准确率大致相同。
- en: ❷ After 5th epoch, accuracy between train and validation data start to spread
    apart.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在第5个epoch之后，训练数据和验证数据之间的准确率开始分散。
- en: ❸ After 15th epoch, accuracy between train and validation data are far apart.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在第15个epoch之后，训练数据和验证数据之间的准确率相差甚远。
- en: 'Let’s now work on getting the model to not overfit to the examples and instead
    generalize from them. As discussed in earlier chapters, we want to add some regularization—some
    noise—during training so the model cannot rote-memorize the training examples.
    In this code example, we modify our model by adding 50% dropout before the final
    dense layer. Because dropout will slow our learning (because of forgetting), we
    increase the number of epochs to 20:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来让模型不要过度拟合示例，而是从它们中泛化。正如前面章节所讨论的，我们希望在训练期间添加一些正则化——一些噪声——这样模型就不能死记硬背训练示例。在这个代码示例中，我们通过在最终密集层之前添加50%的dropout来修改我们的模型。由于dropout会减慢我们的学习（因为遗忘），我们将epochs的数量增加到20：
- en: '[PRE17]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Adds noise to training to prevent overfitting
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向训练中添加噪声以防止过拟合
- en: 'We can see from the following output that while achieving comparable training
    accuracy requires more epochs, the training and test accuracy are comparable.
    Thus, the model is learning to generalize instead of rote-memorizing the training
    examples:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 从以下输出中我们可以看到，虽然达到可比的训练准确率需要更多的epochs，但训练和测试准确率是可比的。因此，模型正在学习泛化而不是死记硬背训练示例：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Adding noise by using dropout keeps the training and validation accuracy from
    drifting apart.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通过使用dropout添加噪声，可以保持训练和验证准确率不偏离。
- en: 4.4.2 Loss monitoring
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.2 损失监控
- en: Up to now, we’ve been focusing on accuracy. The other metric you see outputted
    is the average loss across batches for both training and valuation data. Ideally,
    we would like to see a consistent increase in accuracy per epoch. But we might
    also see sequences of epochs for which the accuracy plateaus or even fluctuates
    +/– a small amount.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于准确率。你看到的另一个输出指标是训练和验证数据批次的平均损失。理想情况下，我们希望看到每个epoch准确率的持续增加。但我们也可能看到一系列的epochs，其中准确率持平或甚至波动±一小部分。
- en: What is important is that we see a steady decrease in the loss. The plateau
    or fluctuations in this case occur because we are near or hovering over lines
    of linear separation or haven’t fully pushed over a line, but are getting closer
    as indicated by the decrease in loss.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是我们看到损失持续下降。在这种情况下，平台期或波动发生是因为我们接近或悬浮在线性分离的线上，或者还没有完全越过一条线，但随着损失的下降，我们正在接近。
- en: 'Let’s look at this another way. Assume you’re building a classifier for dogs
    versus cats. You have two output nodes on the classifier layer: one for cats and
    one for dogs. Assume that on a specific batch, when the model incorrectly classifies
    a dog as a cat, the output values (confidence level) are 0.6 for cat and 0.4 for
    dog. In a subsequent batch, when the model again misclassifies a dog as a cat,
    the output values are 0.55 (cat) and 0.45 (dog). The values are now closer to
    the ground truths, and thus the loss is diminishing, but they still have not passed
    the 0.5 threshold, so the accuracy has not changed yet. Then assume in another
    subsequent batch, the output values for the dog image are 0.49 (cat) and 0.51
    (dog); the loss has further diminished, and because we crossed the 0.5 threshold,
    the accuracy has gone up.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从另一个角度来观察。假设你正在构建一个用于区分狗和猫的分类器。在分类层上，你有两个输出节点：一个用于猫，一个用于狗。假设在某个特定的批次中，当模型错误地将狗分类为猫时，输出值（置信度）为猫0.6，狗0.4。在随后的批次中，当模型再次将狗错误分类为猫时，输出值变为0.55（猫）和0.45（狗）。这些值现在更接近真实值，因此损失正在减少，但它们仍未通过0.5的阈值，因此精度尚未改变。然后假设在另一个随后的批次中，狗图像的输出值为0.49（猫）和0.51（狗）；损失进一步减少，因为我们越过了0.5的阈值，精度有所上升。
- en: 4.4.3 Going deeper with layers
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4.3 使用层深入探索
- en: As mentioned in earlier chapters, simply going deeper with layers can lead to
    instability in the model, without addressing the issues with techniques such as
    identity links and batch normalization. For example, many of the values we are
    matrix-multiplying are small numbers less than 1\. Multiply two numbers less than
    1, and you get an even smaller number. At some point, numbers get so small that
    the hardware can’t represent the value anymore, which is referred to as a *vanishing
    gradient*. In other cases, the parameters may be too close to distinguish from
    each other—or the opposite, spread too far apart, which is referred to as an exploding
    gradient .
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，仅仅通过增加层数来深入探索，可能会导致模型不稳定，而没有解决像身份链接和批量归一化等技术问题。例如，我们进行矩阵乘法的许多值都是小于1的小数。乘以两个小于1的数，你会得到一个更小的数。在某个点上，数值变得如此之小，以至于硬件无法表示该值，这被称为*梯度消失*。在其他情况下，参数可能过于接近以至于无法区分——或者相反，分布得太远，这被称为*梯度爆炸*。
- en: 'The following code example demonstrates this by using a 40-layer DNN absent
    of methods to protect from numerical instability as we go deeper in layers, such
    as batch normalization after each dense layer:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码示例通过使用一个没有采用防止数值不稳定性方法（如每个密集层后的批量归一化）的40层深度神经网络（DNN）来演示这一点：
- en: '[PRE19]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Constructs a model with 40 hidden layers
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建一个包含40个隐藏层的模型
- en: 'In the following output, you can see in the first three epochs we have a consistent
    increase in accuracy in training and evaluation data, as well as a consistent
    decrease in corresponding loss. But afterward, the accuracy becomes erratic; the
    model is numerically unstable:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下输出中，你可以看到在前三个epoch中，训练和评估数据上的精度持续增加，相应的损失持续减少。但之后，精度变得不稳定；模型数值不稳定：
- en: '[PRE20]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Model accuracy is stable in improvement to training and evaluation data.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模型精度在训练和评估数据上稳定提升。
- en: ❷ Model accuracy becomes unstable for training and evaluation data.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 模型精度在训练和评估数据上变得不稳定。
- en: 4.5 Convergence
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.5 收敛
- en: 'Early presumptions on training were that the more times you feed the training
    data into the model, the better the accuracy. What we’ve found, particularly on
    larger and more complex networks, is that at a certain point, accuracy will degrade.
    Today, we now look for convergence on an acceptable local optimum based on how
    the model will be used in an application. If we overtrain the neural network,
    the following can happen:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练阶段早期的假设是，你将训练数据喂给模型的次数越多，精度就越好。但我们发现，尤其是在更大、更复杂的网络中，在某个点上，精度会下降。今天，我们根据模型在应用中的使用方式，寻找在可接受的局部最优解上的收敛。如果我们过度训练神经网络，以下情况可能会发生：
- en: The neural network becomes overfitted to the training data, showing increasing
    accuracy on the training data, but degrading accuracy on the testing data.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络对训练数据过度拟合，显示在训练数据上的精度增加，但在测试数据上的精度下降。
- en: In deeper neural networks, the layers will learn in a nonuniform manner and
    have different convergence rates. Thus, as some layers are working toward convergence,
    others may have convergence and thus start diverging.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在更深的神经网络中，层将以非均匀的方式学习，并具有不同的收敛速率。因此，当一些层正在向收敛迈进时，其他层可能已经收敛并开始发散。
- en: Continued training may cause the neural network to pop out of one local optimum
    and start converging on another that is less accurate.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续训练可能会导致神经网络跳出局部最优，并开始收敛到另一个更不准确的局部最优。
- en: Figure 4.8 shows what we ideally want to see in convergence when training a
    model. You start with a fairly fast reduction in loss across the early epochs,
    and as training homes in on a (near) optimal optimum, the rate of reduction slows,
    and then finally plateaus—at which point, you have convergence.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8显示了在训练模型时理想情况下我们希望看到的收敛情况。你开始时在早期epoch中损失减少得相当快，随着训练逐渐接近（近）最优解，减少的速度减慢，然后最终停滞——这时，你达到了收敛。
- en: '![](Images/CH04_F08_Ferlitsch.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F08_Ferlitsch.png)'
- en: Figure 4.8 Convergence happens when the loss plateaus out.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.8 当损失停滞时发生收敛。
- en: 'Let’s start with a simple ConvNet model in TF.Keras using the CIFAR-10 dataset
    to demonstrate the concept of convergence and then diverging. In this code, I
    have intentionally left out methods that prevent overfitting, like dropout or
    batch normalization:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从使用CIFAR-10数据集在TF.Keras中构建一个简单的ConvNet模型开始，以演示收敛和发散的概念。在这段代码中，我故意省略了防止过拟合的方法，如dropout或批量归一化：
- en: '[PRE21]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Calculates the height and width of the images in the dataset
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算数据集中图像的高度和宽度
- en: ❷ Normalizes the input data
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 标准化输入数据
- en: ❸ Sets the input shape to the model to the height and width of the images in
    the dataset
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将模型的输入形状设置为数据集中图像的高度和宽度
- en: 'The stats for the first six epochs follow. You can see a steady reduction in
    loss with each pass, which means the neural network is getting closer to fitting
    the data. Additionally, the accuracy on the training data is going up from 52.35%
    to 87.4%, and on the validation data it’s increasing from 63.46% to 67.14%:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 前六个epoch的统计数据如下。你可以看到每次通过时损失都在稳步减少，这意味着神经网络正在接近拟合数据。此外，训练数据的准确率从52.35%上升到87.4%，验证数据的准确率从63.46%上升到67.14%：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Initial loss on training data
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练数据的初始损失
- en: ❷ Steady decline on training data loss, but signs of fitting to the data on
    validation loss
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练数据损失稳步下降，但在验证数据损失上有拟合数据的迹象
- en: 'Let’s now look at epochs 11 through 20\. You can see that we’ve hit 98.46%
    on the training data, which means we are tightly fitted to it. On the other hand,
    our accuracy on the validation data plateaued at 66.58%. Thus, after six epochs,
    continued training provided no improvement, and we can conclude that by epoch
    7, the model was overfitted to the training data:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看第11到20个epoch。你可以看到我们在训练数据上达到了98.46%，这意味着我们非常紧密地拟合了它。另一方面，我们的验证数据准确率在66.58%处停滞。因此，经过六个epoch后，继续训练没有提供任何改进，我们可以得出结论，在第7个epoch时，模型已经过拟合到训练数据：
- en: '[PRE23]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Validation loss continues to climb while the model becomes very overfitted
    to training data.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 验证损失持续上升，而模型变得非常过拟合训练数据。
- en: ❷ Validation loss is very high, and the model is highly fitted to the training
    data.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 验证损失非常高，模型高度拟合训练数据。
- en: The values of the loss function for the training and validation data also indicate
    that the model is overfitting. The loss function between epochs 11 and 20 for
    the training data continues to get smaller, but for the corresponding validation
    data, it plateaus and then gets worse (diverges).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据和验证数据的损失函数值也表明模型正在过拟合。训练数据在第11到20个epoch之间的损失函数值持续减小，但对于相应的验证数据，它停滞并变得更差（发散）。
- en: 4.6 Checkpointing and early stopping
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.6 检查点和提前停止
- en: 'This section covers two techniques for making training more economical: checkpointing
    and early stopping. Checkpointing is useful when a model overtrains and diverges,
    and we want to recover the model weights at the point of convergence without the
    additional cost of retraining. You can think of early stopping as an extension
    to checkpointing. We have a monitoring system detect divergence at the earliest
    moment that it occurs, and then we stop training, saving additional cost when
    recovering a checkpoint at the point of divergence.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 本节介绍了两种使训练更经济的技巧：检查点和提前停止。当模型过拟合并发散时，检查点非常有用，我们希望在收敛点恢复模型权重，而不需要额外的重新训练成本。你可以将提前停止视为检查点的扩展。我们有一个监控系统在最早的时刻检测到发散，然后停止训练，在发散点恢复检查点时节省额外的成本。
- en: 4.6.1 Checkpointing
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.1 检查点
- en: '*Checkpointing* is periodically saving the learned model parameters and current
    hyperparameter values during training. There are two reasons for doing this:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '*检查点*是指在训练过程中定期保存学习到的模型参数和当前超参数值。这样做有两个原因：'
- en: To be able to resume training of a model where you left off, instead of restarting
    the training from the beginning
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了能够从上次停止的地方恢复模型的训练，而不是从头开始重新训练
- en: To identify a past point in training where the model gave the best results
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了识别训练中模型给出最佳结果的过去某个点
- en: In the first case, we might want to split training across sessions as a way
    of managing resources. For example, we might reserve (or be authorized) one hour
    a day for training. At the end of the one-hour training each day, the training
    is checkpointed. The following day, training is resumed by restoring from the
    checkpoint. For example, you might be working in a research organization that
    has a fixed budget for computing expenses, and your team is experimenting with
    training a model with substantial computing costs. To manage the budget, your
    team might be allocated a limit of daily computing expenses.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，我们可能希望将训练分散到不同的会话中，作为管理资源的一种方式。例如，我们可能每天预留（或被授权）一个小时用于训练。每天一小时的训练结束后，训练将进行检查点保存。第二天，训练将通过从检查点恢复来继续。例如，你可能在一家有固定计算费用预算的研究机构工作，你的团队正在尝试训练一个计算成本较高的模型。为了管理预算，你的团队可能被分配了每日计算费用的限额。
- en: Why wouldn’t saving the model’s weights and biases be enough? In neural networks,
    some hyperparameter values will dynamically change, such as the learning rate
    and decay. We would want to resume at the same hyperparameter values at the time
    the training was paused.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么仅仅保存模型的权重和偏差就不够呢？在神经网络中，一些超参数值会动态变化，例如学习率和衰减。我们希望在训练暂停时的相同超参数值下继续。
- en: In another scenario, we might implement continuous learning as a part of a continuous
    integration and delivery (CI/CD) process. In this scenario, new labeled images
    are continuously added to the training data, and we want to only incrementally
    retrain the model instead of retraining from scratch on each integration cycle.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一种场景中，我们可能将连续学习作为持续集成和持续交付（CI/CD）过程的一部分来实现。在这种情况下，新的标记图像会持续添加到训练数据中，我们只想增量地重新训练模型，而不是在每个集成周期从头开始重新训练。
- en: In the second case, we might want to find the best result after a model has
    trained past the best optimum, and started to diverge and/or overfit. We would
    not want to start retraining from scratch with fewer epochs (or other hyperparameter
    changes), but instead identify the epoch that achieved the best results, and restore
    (set) the learned model parameters to those that were checkpointed at the end
    of that epoch.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种情况下，我们可能希望在模型训练超过最佳最优值并开始发散和/或过拟合后找到最佳结果。我们不想从更少的epoch（或其他超参数变化）开始重新训练，而是识别出达到最佳结果的epoch，并将学习到的模型参数恢复（设置）为该epoch结束时检查点的参数。
- en: Checkpointing occurs at the end of an epoch, but should we checkpoint after
    each epoch? Probably not. That can be expensive in terms of space. Let’s presume
    that the model has 25 million parameters (for example, ResNet50), and each parameter
    is a 32-bit floating-point value (4 bytes). Each checkpoint would then require
    100 MB to save. After 10 epochs, that would already be 1 GB of disk space.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 检查点在每个epoch结束时发生，但我们是否应该在每个epoch后都进行检查点保存？可能不是。这可能会在空间上变得昂贵。让我们假设模型有2500万个参数（例如，ResNet50），每个参数是一个32位的浮点值（4字节）。那么每个检查点就需要100
    MB来保存。经过10个epoch后，这已经需要1 GB的磁盘空间了。
- en: We generally checkpoint after each epoch only if the number of model parameters
    is small and/or the number of epochs is small. In the following code example,
    a checkpoint is instantiated with the `ModelCheckpoint` class. The parameter `filepath`
    indicates the file path for the checkpoint. The file path can be either a complete
    file path or a formatted file path. In the former case, the checkpoint file would
    be overwritten each time.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只在模型参数数量较小和/或epoch数量较小的情况下在每个epoch后进行检查点保存。在下面的代码示例中，使用`ModelCheckpoint`类实例化了一个检查点。参数`filepath`表示检查点的文件路径。文件路径可以是完整的文件路径或格式化的文件路径。在前者的情况下，检查点文件每次都会被覆盖。
- en: 'In the following code, we use the format syntax `epoch:02d` to generate a unique
    file for each checkpoint, based on the epoch number. For example, if it’s the
    third epoch, the file would be mymodel-03.ckpt:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在下面的代码中，我们使用格式语法`epoch:02d`为每个检查点生成一个唯一的文件，基于epoch编号。例如，如果是第三个epoch，文件将是mymodel-03.ckpt：
- en: '[PRE24]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Imports the ModelCheckpoint class
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入ModelCheckpoint类
- en: ❷ Sets the file pathname to be unique per epoch
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为每个epoch设置唯一的文件路径名
- en: ❸ Creates a ModelCheckpoint object
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 创建一个ModelCheckpoint对象
- en: ❹ Trains the model and uses the callbacks parameter to enable the checkpoint
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练模型并使用回调参数启用检查点
- en: 'A model can then be subsequently restored from a checkpoint by using the `load_
    model()` method:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，可以使用 `load_model()` 方法从检查点恢复模型：
- en: '[PRE25]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Imports the load_model method
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 load_model 方法
- en: ❷ Restores a model from a saved checkpoint
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从保存的检查点恢复模型
- en: 'For models with a larger number of parameters and/or number of epochs, we can
    choose to save a checkpoint on every *n*th epoch with the parameter `period`.
    In this example, a checkpoint is saved on every fourth epoch:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有更多参数和/或epoch数的模型，我们可以选择使用参数 `period` 在每 *n* 个epoch上保存一个检查点。在这个例子中，每四个epoch保存一个检查点：
- en: '[PRE26]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Creates a checkpoint for every fourth epoch
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每四个epoch创建一个检查点
- en: 'Alternatively, we can save the current best checkpoint with the parameter `save_best_
    only=True` and the parameter `monitor` to the measurement to base the decision
    on. For example, if the parameter `monitor` is set to `val_acc`, it will write
    a checkpoint only if the validation accuracy is higher than the last saved checkpoint.
    If the parameter is set to `val_loss`, it will write a checkpoint only if the
    validation loss is lower than the last saved checkpoint:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以使用参数 `save_best_only=True` 和参数 `monitor` 来保存当前最佳检查点，以基于测量结果做出决策。例如，如果参数
    `monitor` 设置为 `val_acc`，则只有在验证准确率高于上次保存的检查点时才会写入检查点。如果参数设置为 `val_loss`，则只有在验证损失低于上次保存的检查点时才会写入检查点：
- en: '[PRE27]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ File path for saving the best checkpoint
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 保存最佳检查点的文件路径
- en: ❷ Saves the checkpoint only if validation loss is smaller than the last checkpoint
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅当验证损失小于上次检查点时保存检查点
- en: 4.6.2 Early stopping
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6.2 提前停止
- en: An *early stop* is setting a condition upon which training is terminated earlier
    than the set limits (for example, number of epochs). This is generally set to
    conserve resources and/or prevent overtraining when a goal objective is reached,
    such as a level of accuracy or convergence on evaluation loss. For example, we
    might set a training for 20 epochs, which average 30 minutes each, for a total
    of 10 hours. But if the objective is met after 8 epochs, it would be ideal to
    terminate the training, saving 6 hours of resources.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '*提前停止* 是设置一个条件，使得训练在设定的限制（例如，epoch数）之前提前终止。这通常是为了在达到目标目标时（例如，准确率水平或评估损失的收敛）节省资源或防止过拟合而设置的。例如，我们可能会设置20个epoch的训练，每个epoch平均30分钟，总共10小时。但如果目标在8个epoch后达成，提前终止训练将节省6小时资源。'
- en: 'An early stop is specified in a manner similar to a checkpoint. An `EarlyStopping`
    object is instantiated and configured with a target goal, and passed to the `callbacks`
    parameter of the `fit()` method. In this example, training will be stopped early
    only if the validation loss stops, reducing from the previous epoch:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 提前停止的指定方式与检查点类似。实例化一个 `EarlyStopping` 对象，并配置一个目标目标，然后将其传递给 `fit()` 方法的 `callbacks`
    参数。在这个例子中，只有在验证损失停止减少时，训练才会提前停止：
- en: '[PRE28]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Imports the EarlyStopping class
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入 EarlyStopping 类
- en: ❷ Sets an early stop when the validation loss has stopped reducing
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当验证损失停止减少时设置提前停止
- en: ❸ Trains the model and uses early stop to stop training early if the validation
    loss stops decreasing
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练模型并使用提前停止，如果验证损失停止减少则提前停止训练
- en: 'In addition to monitoring the validation loss for an early stop, we can monitor
    the validation accuracy with the parameter setting `monitor="val_acc"`. Additional
    parameters exist for fine-tuning to prevent an inadvertent early stop; for example,
    where more training will overcome being stuck on a saddle point—a plateau region
    in the loss curve. The parameter `patience` specifies a minimum number of epochs
    without improvement before early stopping, and `min_delta` specifies a minimum
    threshold to determine whether the model improved. In this example, the training
    will stop early if no improvement occurs in the validation loss after three epochs:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 除了监控验证损失以实现提前停止外，我们还可以通过参数设置 `monitor="val_acc"` 监控验证准确率。存在一些额外的参数用于微调，以防止意外提前停止；例如，在损失曲线上陷入鞍点（一个损失曲线的平坦区域）时，更多的训练将克服这种情况。参数
    `patience` 指定了在提前停止之前没有改进的最小epoch数，而 `min_delta` 指定了确定模型是否改进的最小阈值。在这个例子中，如果在三个epoch后验证损失没有改进，训练将提前停止：
- en: '[PRE29]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Sets an early stop when the validation loss has stopped reducing for three
    epochs
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 当验证损失停止减少三个epoch时设置提前停止
- en: 4.7 Hyperparameters
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.7 超参数
- en: Let’s start by explaining the difference between learned parameters and hyperparameters.
    *Learned parameters*, weights and biases, are learned during training. For neural
    networks, these typically are the weights on each neural network connection, and
    the biases on each node. For CNNs, learned parameters are the filters in each
    convolutional layer. These learned parameters stay as part of the model when the
    model is done training.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先解释一下学习参数和超参数之间的区别。*学习参数*，权重和偏差，是在训练过程中学习的。对于神经网络来说，这些通常是每个神经网络连接上的权重和每个节点的偏差。对于卷积神经网络（CNNs），学习参数是每个卷积层中的过滤器。这些学习参数在模型完成训练后仍作为模型的一部分。
- en: '*Hyperparameters* are parameters used to train the model, but are not part
    of the trained model itself. After training, the hyperparameters no longer exist.
    Hyperparameters are used to improve the training of the model, by answering questions
    such as these:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*超参数*是用于训练模型的参数，但它们本身不是训练模型的一部分。训练后，超参数不再存在。超参数通过回答诸如这些问题来提高模型的训练效果：'
- en: How long does it take to train the model?
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练模型需要多长时间？
- en: How fast does the model converge?
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型收敛有多快？
- en: Does it find the global optimum?
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是否找到了全局最优解？
- en: How accurate is the model?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的准确度如何？
- en: How overfitted is the model?
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型过拟合的程度如何？
- en: Another perspective of hyperparameters is that they are a means to measure the
    cost and quality of developing the model. We will delve into these questions and
    others as we explore hyperparameters further in chapter 10.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 超参数的另一个视角是，它们是衡量模型开发成本和质量的一种手段。随着我们在第10章进一步探讨超参数，我们将深入研究这些问题和其他问题。
- en: 4.7.1 Epochs
  id: totrans-220
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.1 Epochs
- en: The most basic hyperparameter is the number of epochs, though this is now being
    more commonly replaced with steps. The *epochs hyperparameter* is the number of
    times you will pass the entire training data through the neural network during
    training.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 最基本的超参数是epoch的数量，尽管现在这更常见地被步骤所取代。*epoch超参数*是在训练过程中你将整个训练数据通过神经网络的次数。
- en: Training is very expensive in terms of computing time. It includes both the
    forward feeding to pass the training data through and the backward propagation
    to update (train) the model’s parameters. For example, if a full pass of the data
    (epoch) takes 15 minutes, and we run 100 epochs, the training time will take 25
    hours.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 训练在计算时间上非常昂贵。它包括正向传播将训练数据传递过去和反向传播来更新（训练）模型的参数。例如，如果一次完整的数据传递（epoch）需要15分钟，而我们运行100个epoch，训练时间将需要25小时。
- en: 4.7.2 Steps
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.2 步骤
- en: Another way to improve accuracy and reduce training time is by changing the
    sampling distribution of the training dataset. For epochs, we think of a sequential
    draw of batches from our training data. Even though we randomly shuffle the training
    data at the start of each epoch, the sampling distribution is still the same.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 提高准确度和减少训练时间的另一种方法是改变训练数据集的抽样分布。对于epoch，我们考虑从我们的训练数据中按顺序抽取批次。尽管我们在每个epoch的开始时随机打乱训练数据，但抽样分布仍然是相同的。
- en: Let’s now think of the entire population of the subject we want to recognize.
    In statistics, we call this the *population distribution* (figure 4.9).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在考虑我们想要识别的主题的整个群体。在统计学中，我们称这为*总体分布*（图4.9）。
- en: '![](Images/CH04_F09_Ferlitsch.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F09_Ferlitsch.png)'
- en: Figure 4.9 Difference between a population distribution and a random sample
    from within a population
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9 总体分布与人群中的随机样本之间的区别
- en: But we will never have a dataset that is the actual entire population distribution.
    Instead, we have samples, which we refer to as a *sampling distribution of the
    population distribution* (figure 4.10).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们永远不会有一个数据集是实际的整个总体分布。相反，我们有样本，我们称之为*总体分布的抽样分布*（图4.10）。
- en: '![](Images/CH04_F10_Ferlitsch.png)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F10_Ferlitsch.png)'
- en: Figure 4.10 A sampling distribution is made up of random samples from the population.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.10 由人群中的随机样本组成的抽样分布。
- en: 'Another way to improve our model is to additionally learn the best sampling
    distribution for training the model. Although our dataset may be fixed, we can
    use several techniques to alter the distribution, and thus learn the sampling
    distribution that best fits training the model. These methods include the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 提高我们的模型的另一种方法是学习训练模型的最佳抽样分布。尽管我们的数据集可能是固定的，但我们可以使用几种技术来改变分布，从而学习最适合训练模型的抽样分布。这些方法包括以下内容：
- en: Regularization/dropout
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化/丢弃
- en: Batch normalization
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 批标准化
- en: Data augmentation
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强
- en: From this perspective, we no longer see feeding the neural network as sequential
    passes over the training data, but as making random draws from a sampling distribution.
    In this context, *steps* refers to the number of batches (draws) we will make
    from the sampling distribution of our training data.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个角度来看，我们不再将神经网络视为对训练数据的顺序遍历，而是将其视为从训练数据的采样分布中进行随机抽取。在这种情况下，*步骤*指的是我们将从训练数据的采样分布中抽取的批次（抽取）的数量。
- en: When we add dropout layers to the neural networks, we are randomly dropping
    activations on a per-sample basis. In addition to reducing overfitting of a neural
    network, we are also changing the distribution.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在神经网络中添加dropout层时，我们是在每个样本的基础上随机丢弃激活。除了减少神经网络的过拟合，我们还改变了分布。
- en: With batch normalization, we are minimizing covariance shift between our batches
    of training data (samples). Just as we use standardization on our input, the activations
    are rescaled using standardization (we subtract the batch mean and divide by the
    batch standard deviation). This normalization reduces the fluctuations in updates
    to parameters in the model; this process is referred to as *adding more stability*
    to the training. In addition, this normalization mimics drawing from a sampling
    distribution that is more representative of the population distribution.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量归一化中，我们最小化训练数据批次（样本）之间的协方差偏移。正如我们在输入上使用标准化一样，激活也被使用标准化重新缩放（我们减去批次均值并除以批次标准差）。这种归一化减少了模型参数更新中的波动；这个过程被称为*增加更多稳定性*到训练中。此外，这种归一化模仿从更具有代表性的总体分布的采样分布中抽取的过程。
- en: With data augmentation (discussed in chapter 13), we create new examples by
    modifying existing examples within a set of parameters. We then randomly select
    the modification, which also contributes to changing the distribution.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 通过数据增强（在第13章中讨论），我们在一组参数内修改现有示例来创建新的示例。然后我们随机选择修改，这也有助于改变分布。
- en: With batch normalization, regularization/dropout, and data augmentation, no
    two epochs will have the same sampling distribution. In this case, the practice
    now is to limit the number of random draws (steps) from each new sampling distribution,
    further changing the distribution. For example, if steps are set to 1000, then
    per epoch, only 1000 random batches will be selected and fed into the neural network
    for training.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在批量归一化、正则化/dropout和数据增强的情况下，没有两个epoch会有相同的采样分布。在这种情况下，现在的做法是限制从每个新的采样分布中随机抽取（步骤）的数量，进一步改变分布。例如，如果步骤设置为1000，那么每个epoch中，只有1000个随机批次将被选中并输入到神经网络中进行训练。
- en: 'In TF.Keras, we can specify both the number of epochs and steps as parameters
    to the `fit()` method, as the parameters `epochs` and `steps_per_epoch`:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF.Keras中，我们可以将`epochs`和`steps_per_epoch`参数指定为`fit()`方法的参数，作为参数：
- en: '[PRE30]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 4.7.3 Batch size
  id: totrans-242
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.3 批量大小
- en: 'To understand how to set batch size, you should have a basic understanding
    of the three types of gradient descent algorithms: stochastic gradient descent,
    batch gradient descent, and mini-batch gradient descent. The algorithm is the
    means by which the model parameters are updated (learned) during training.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何设置批量大小，你应该对三种梯度下降算法有基本的了解：随机梯度下降、批量梯度下降和迷你批量梯度下降。算法是模型参数在训练期间更新的（学习）手段。
- en: Stochastic gradient descent
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 随机梯度下降
- en: In *stochastic gradient descent* (*SGD*), the model is updated after each example
    is fed through during training. Since each example is randomly selected, the variance
    between examples can result in large swings in the gradient.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在*随机梯度下降*（*SGD*）中，模型在训练过程中每个示例被输入后更新。由于每个示例都是随机选择的，因此示例之间的方差可能导致梯度的大幅波动。
- en: A benefit is that during training, we are less likely to converge on a local
    (that is, lessor) optimum, and more likely to find the global optimum to converge
    on. Another benefit is that the rate of change in loss can be monitored in real
    time, which may aid in algorithms that do automatic hyperparameter tuning. The
    downside is that this is more computationally expensive per epoch.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 一个好处是，在训练过程中，我们不太可能收敛到局部（即较差的）最优解，而更有可能找到全局最优解并收敛。另一个好处是，损失变化的速率可以实时监控，这可能有助于自动超参数调整的算法。缺点是这每轮计算成本更高。
- en: Batch gradient descent
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 批量梯度下降
- en: In *batch gradient descent*, the error loss per example is calculated as each
    example is fed through during training, but the updating of the model is done
    at the end of each epoch (after the entire training data is passed through). As
    a result, the gradient is smoothed out because it’s calculated across the loss
    of all the examples, instead of a single example.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在*批量梯度下降*中，每个示例的错误损失是在训练过程中每个示例被输入时计算的，但模型的更新是在每个epoch结束时（在所有训练数据通过之后）进行的。因此，由于它是基于所有示例的损失来计算的，而不是单个示例，所以梯度被平滑了。
- en: The benefits are that this is less computationally expensive per epoch, and
    the training more reliably converges. The downsides are that the model may converge
    on a less accurate local optimum, and an entire epoch needs to be run to monitor
    performance data.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的优点是每个epoch的计算成本更低，训练过程更可靠地收敛。缺点是模型可能收敛到一个不太准确的局部最优解，并且需要运行整个epoch来监控性能数据。
- en: Mini-batch gradient descent
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 小批量梯度下降
- en: The *mini-batch gradient descent* method is a tradeoff between stochastic and
    batch gradient descent. Instead of one example or all examples, the neural network
    is fed in mini-batches that are a subset of the entire training data. The smaller
    the mini-batch side, the more the training will resemble stochastic gradient descent,
    while larger batch sizes will resemble batch gradient descent.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '*小批量梯度下降*方法是在随机梯度下降和批量梯度下降之间的折中。不是单个示例或所有示例，神经网络被输入到小批量中，这些小批量是整个训练数据的一个子集。小批量的大小越小，训练越类似于随机梯度下降，而较大的批量大小则更类似于批量梯度下降。'
- en: 'For certain models and datasets, SGD works best. In general, it’s a common
    practice to use the tradeoff of mini-batch gradient descent. The hyperparameter
    `batch_ size` indicates the size of the mini-batch. Because of hardware architectures,
    the most time/space-efficient batch sizes are multiples of 8, such as 8, 16, 32,
    and 64\. The batch size that is most commonly tried first is 32, and then 128\.
    For extremely large datasets on higher-end hardware (HW) accelerators (such as
    GPUs and TPUs), it is common to see batch sizes of 256 and 512\. In TF.Keras,
    you can specify `batch_size` in the model `fit``()` method:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 对于某些模型和数据集，随机梯度下降（SGD）效果最好。通常，使用小批量梯度下降的折中方案是一种常见做法。超参数`batch_size`表示小批量的大小。由于硬件架构，最节省时间/空间的批量大小是8的倍数，例如8、16、32和64。首先尝试的批量大小通常是32，然后是128。对于在高端硬件（HW）加速器（如GPU和TPU）上的极大数据集，常见的批量大小是256和512。在TF.Keras中，你可以在模型的`fit()`方法中指定`batch_size`：
- en: '[PRE31]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 4.7.4 Learning rate
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.7.4 学习率
- en: The *learning rate* is generally the most influential of the hyperparameters.
    It can have a significant impact on the length of time to train a neural network
    as well as on whether the neural network converges on a local (lessor) optimum,
    and whether it converges on the global (best) optimum.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习率*通常是超参数中最有影响力的。它可以对训练神经网络所需的时间长度以及神经网络是否收敛到局部（较差）最优解，以及是否收敛到全局（最佳）最优解产生重大影响。'
- en: When updating the model parameters during the backward propagation pass, the
    gradient descent algorithm is used to derive a value to add/subtract to the parameters
    in the model from the loss function for that pass. These additions and subtractions
    could result in large swings in parameter values. If a model has and continues
    to have large swings in parameter values, the model’s parameters will be “all
    over the map” and never converge.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播过程中更新模型参数时，梯度下降算法被用来从损失函数中导出一个值，以添加/减去到模型参数中。这些添加和减去可能会导致参数值的大幅波动。如果一个模型有并且继续有参数值的大幅波动，那么模型的参数将会“四处乱飞”并且永远不会收敛。
- en: If you observe big swings in the amount of loss and/or accuracy, the training
    of your model is not converging. If the training is not converging, it won’t matter
    how many epochs you run; the model will never finish training.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你观察到损失和/或准确性的波动很大，那么你的模型训练并没有收敛。如果训练没有收敛，那么运行多少个epoch都没有关系；模型永远不会完成训练。
- en: The learning rate provides us with a means to control the degree that the model
    parameters are updated. In the basic method, the learning rate is a fixed coefficient
    between 0 and 1 that is multiplied against the value to add/subtract, to reduce
    the amount being added or subtracted. These smaller increments add more stability
    during the training and increase the likelihood of convergence.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率为我们提供了一种控制模型参数更新程度的方法。在基本方法中，学习率是0到1之间的一个固定系数，它乘以要添加/减去的值，以减少添加或减去的量。这些较小的增量在训练期间增加了稳定性，并增加了收敛的可能性。
- en: Small vs. large learning rate
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 小学习率与大脑学习率
- en: If we use a very small learning rate, like 0.001, we will eliminate large swings
    in the model parameters during updates. This will generally guarantee that the
    training will converge on a local optimum. But there is a drawback. First, the
    smaller we make the increments, the more passes of the training data (epochs)
    will be needed to minimize the loss. That means more time to train. Second, the
    smaller the increments, the less likely the training will explore other local
    optima, which might be more accurate than the one that the training is converging
    on; instead, it may converge on poor local optimum or get stuck on a saddle point.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用一个非常小的学习率，如0.001，我们将在更新模型参数时消除大的摆动。这通常可以保证训练将收敛到一个局部最优解。但有一个缺点。首先，我们使增量越小，需要的训练数据（周期）遍历次数越多，以最小化损失。这意味着需要更多的时间来训练。其次，增量越小，训练探索其他局部最优解的可能性就越小，这些局部最优解可能比训练收敛到的更准确；相反，它可能收敛到一个较差的局部最优解或卡在鞍点上。
- en: A large learning rate, like 0.1, likely will cause big jumps in the model parameters
    during updates. In some cases, it might initially lead to faster convergence (fewer
    epochs). The drawback is that even if you are initially converging fast, the jumps
    may overshoot and start causing the convergence to swing back and forth, or hop
    across different local optima. At very high learning rates, the training may start
    to diverge (increasing loss).
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 一个大的学习率，如0.1，在更新模型参数时很可能会引起大的跳跃。在某些情况下，它可能最初导致更快的收敛（更少的周期）。缺点是，即使你最初收敛得很快，跳跃可能会超过并开始导致收敛来回摆动，或者跳到不同的局部最优解。在非常高的学习率下，训练可能开始发散（损失增加）。
- en: Many factors help determine what the best learning rate will be at various times
    during the training. In best practices, the rate will range from 10e-5 to 0.1.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 许多因素有助于确定在训练过程中的不同时间点最佳学习率是什么。在最佳实践中，该速率将在10e-5到0.1之间。
- en: 'Here is a basic formula that adjusts weight by multiplying the learning rate
    by the amount calculated to add/subtract (the gradient):'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个基本的公式，通过将学习率乘以计算出的添加/减去的量（梯度）来调整权重：
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Decay
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 衰减
- en: One common practice has been to start with a slightly larger learning rate,
    and then gradually decrease it, also referred to as *learning rate decay*. The
    larger learning rate would at first explore different local optima to converge
    on and make initial deep swings into the respective local optima. The rate of
    convergence and minimizing the loss function on the initial updates can be used
    to home in on the best (good) local optimum.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的做法是开始时使用稍微大一点的学习率，然后逐渐减小它，这也被称为*学习率衰减*。较大的学习率最初会探索不同的局部最优解，以收敛到并使初始深度摆动到相应的局部最优解。收敛速率和最小化损失函数的初始更新可以用来聚焦于最佳（好的）局部最优解。
- en: From that point, the learning rate is gradually decayed. As the learning rate
    decays, it is less likely for swings out of the good local optimum to occur, and
    the steadily decreasing learning rate will tune the convergence to approach the
    minimal point (albeit, the smaller and smaller learning rate will increase training
    time). So the decay becomes a tradeoff between small increases in final accuracy
    and the overall training time.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 从那个点开始，学习率逐渐衰减。随着学习率的衰减，出现偏离良好局部最优解的摆动可能性降低，稳定下降的学习率将调整收敛，使其接近最小点（尽管，越来越小的学习率会增加训练时间）。因此，衰减成为在最终精度的小幅度提升和整体训练时间之间的权衡。
- en: 'The following is a basic formula adding decay to the calculation of updating
    the weights. On each update, the learning rate is reduced by the decay amount
    (called the *fixed decay*):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个基本公式，在更新权重计算中添加衰减：在每次更新中，学习率通过衰减量（称为*固定衰减*）减少：
- en: '[PRE33]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'In practice, the decay formulas are generally time-based, step-based, or cosine-based
    decays. These formulas can be expressed in simplified terms, and an iteration
    may be a batch or epoch. By default, TF.Keras optimizers use time-based decay.
    The formulas are as follows:'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，衰减公式通常是基于时间、步长或余弦衰减。这些公式可以用简化的术语表达，迭代可以是批量或周期。默认情况下，TF.Keras优化器使用基于时间的衰减。公式如下：
- en: Time-based decay
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于时间的衰减
- en: '[PRE34]'
  id: totrans-272
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Step-based decay
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步长衰减
- en: '[PRE35]'
  id: totrans-274
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Cosine decay
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 余弦衰减
- en: '[PRE36]'
  id: totrans-276
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Momentum
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 动量
- en: 'Another common practice is to accelerate or decelerate the rate of change based
    on prior changes. If we have large jumps in convergence, we risk jumping out of
    the local optimum, so we may want to decelerate the learning rate. If we have
    small to no changes in convergence, we may want to accelerate the learning rate
    to hop over a saddle point. Typically, values for momentum range from 0.5 to 0.99:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的做法是根据先前变化来加速或减速变化率。如果我们有大的收敛跳跃，我们可能会跳出局部最优，所以我们可能希望减速学习率。如果我们有小的或没有收敛变化，我们可能希望加速学习率以跳过一个鞍点。通常，动量的值在0.5到0.99之间：
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Adaptive learning rate
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 自适应学习率
- en: 'Many popular algorithms dynamically adapt the learning rate:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 许多流行的算法会动态调整学习率：
- en: Adadelta
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adadelta
- en: Adagrad
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adagrad
- en: Adam
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Adam
- en: AdaMax
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AdaMax
- en: AMSGrad
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AMSGrad
- en: Momentum
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动量
- en: Nadam
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nadam
- en: Nesterov
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Nesterov
- en: RMSprop
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: RMSprop
- en: 'The explanation of these algorithms is beyond the scope of this section. For
    more information on these and other optimizers, see documentation for `tf.keras.optimizers`
    ([http://mng.bz/Par9](http://mng.bz/Par9)). For TF.Keras, these learning rate
    algorithms are specified when the optimizer is defined for minimizing the loss
    function:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 这些算法的解释超出了本节的范围。有关这些和其他优化器的更多信息，请参阅`tf.keras.optimizers`的文档（[http://mng.bz/Par9](http://mng.bz/Par9)）。对于TF.Keras，这些学习率算法是在定义优化器以最小化损失函数时指定的：
- en: '[PRE38]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Specifies learning rate and decay for the optimizer
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定优化器的学习率和衰减
- en: ❷ Compiles the model, specifying the loss function and optimizer
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 编译模型，指定损失函数和优化器
- en: 4.8 Invariance
  id: totrans-295
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.8 不变性
- en: So what’s *invariance*? In the context of neural networks, it means that the
    outcome (the prediction) is unchanged when the input is transformed. In the context
    of training an image classifier, image augmentation can be used to train a model
    to recognize an object regardless of the object’s size and location in the image,
    without the need for additional training data.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，*不变性*是什么意思？在神经网络的情况下，这意味着当输入被转换时，结果（预测）保持不变。在训练图像分类器的情况下，可以使用图像增强来训练模型，使其能够识别图像中无论对象的大小和位置如何的对象，而无需额外的训练数据。
- en: Let’s consider a CNN that is an image classifier (this analogy can also be applied
    to object detection). We want the object being classified to be correctly recognized
    regardless of its location in the image. If we transform the input so that the
    object is shifted to a new location in the image, we want the outcome (the prediction)
    to remain unchanged.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑一个图像分类器CNN（这个类比也可以应用于目标检测）。我们希望被分类的对象无论其在图像中的位置如何都能被正确识别。如果我们变换输入，使得对象在图像中移动到新的位置，我们希望结果（预测）保持不变。
- en: For CNNs and imaging in general, the primary types of invariance we want the
    model to support are *translational* and *scale* invariance. Prior to 2019, translational
    and scale invariance were handled by image augmentation preprocessing upstream
    from the model training, using preprocessing of the image data on a CPU while
    the data was fed during training on a GPU. We will discuss these traditional techniques
    in this section.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 对于CNN和一般成像，我们希望模型支持的主要不变性类型是*平移*和*尺度*不变性。在2019年之前，平移和尺度不变性是通过在模型训练之前使用图像增强预处理来处理的，即在CPU上对图像数据进行预处理，同时在GPU上训练时提供数据。我们将在本节中讨论这些传统技术。
- en: One approach to training for translational/scale invariance is simply to have
    enough images per class (per object), so that the object is in different locations
    in the image, different rotations, different scales, and different view angles.
    Well, this may not be practical to collect.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 训练平移/尺度不变性的一个方法是为每个类别（每个对象）提供足够的图像，使得对象在图像中的位置、旋转、尺度以及视角都不同。嗯，这可能不太实际收集。
- en: It turns out there is a straightforward method of autogenerating translational/
    scale invariant images using image augmentation preprocessing, which is performed
    efficiently using matrix operations. Matrix-based transforms can be done by a
    variety of Python packages, such as the TF.Keras `ImageDataGenerator` class, TensorFlow
    `tf.image` module, or OpenCV.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，使用图像增强预处理自动生成平移/尺度不变图像的方法非常直接，这种预处理通过矩阵操作高效执行。基于矩阵的变换可以通过各种Python包执行，例如TF.Keras
    `ImageDataGenerator`类、TensorFlow `tf.image`模块或OpenCV。
- en: Figure 4.11 depicts a typical image augmentation pipeline when feeding training
    data to a model. For each batch drawn, a random subset of the images in the batch
    are selected for augmentation (for example, 50%). Then, this randomly selected
    subset of images is randomly transformed according to certain constraints, such
    as a randomly selected rotation value from –30 to 30 degrees. The modified batch
    (originals plus augmented) is then fed to the model for training.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11描述了在向模型提供训练数据时的典型图像增强流程。对于每个抽取的批次，从批次中的图像中选择一个随机子集进行增强（例如，50%）。然后，根据某些约束（例如，从-30到30度的随机旋转值）随机变换这个随机选择的图像子集。然后，将修改后的批次（原始图像加上增强图像）输入模型进行训练。
- en: '![](Images/CH04_F11_Ferlitsch.png)'
  id: totrans-302
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH04_F11_Ferlitsch.png)'
- en: Figure 4.11 During image augmentation, a randomly selected subset of images
    in the batch is augmented.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.11在图像增强过程中，随机选择批次中的图像子集进行增强。
- en: 4.8.1 Translational invariance
  id: totrans-304
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.1 平移不变性
- en: This subsection covers how to manually augment images in a training dataset
    such that the model learns to recognize the object in the image regardless of
    its location in the image. For example, we want the model to recognize a horse
    regardless of which direction the horse faces in the image, or an apple regardless
    of where in the background the apple is located.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节将介绍如何在训练数据集中手动增强图像，以便模型学会识别图像中的对象，而不管其在图像中的位置如何。例如，我们希望模型能够识别出无论马在图像中朝哪个方向，都能识别出马，或者无论苹果在背景中的位置如何，都能识别出苹果。
- en: '*Translational invariance* in the context of image input includes the following:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: '*平移不变性*在图像输入的上下文中包括以下内容：'
- en: Vertical/horizontal location (object can be anywhere in the picture)
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直/水平位置（对象可以在图片的任何位置）
- en: Rotation (object can be at any rotation)
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转（对象可以处于任何旋转角度）
- en: A vertical/horizontal transformation is typically performed either as a matrix
    roll operation or a crop. An orientation (for example, mirror) is typically performed
    as a matrix flip. A rotation is typically handled as a matrix transpose.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直/水平变换通常是通过矩阵滚动操作或裁剪来执行的。一个方向（例如，镜像）通常是通过矩阵翻转来实现的。旋转通常是通过矩阵转置来处理的。
- en: Flip
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 翻转
- en: A *matrix flip* transforms an image by flipping it either on the vertical or
    horizontal axis. Since the image data is represented as a stack of 2D matrices
    (one per channel), a flip can be done efficiently as a matrix transpose function
    without changes (such as interpolation) of the pixel data. Figure 4.12 compares
    an original and flipped versions of an image.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '*矩阵翻转*通过在垂直或水平轴上翻转图像来变换图像。由于图像数据表示为2D矩阵的堆叠（每个通道一个），翻转可以通过矩阵转置函数高效执行，而无需更改像素数据（例如插值）。图4.12比较了图像的原始版本和翻转版本。'
- en: '![](Images/CH04_F12_Ferlitsch.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH04_F12_Ferlitsch.png)'
- en: 'Figure 4.12 Comparison of an apple: original, vertical axis, and horizontal
    axis flips (image source: malerapaso, iStock)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.12比较了一个苹果：原始图像、垂直轴翻转和水平轴翻转（图片来源：malerapaso，iStock）
- en: 'Let’s start by showing how to flip an image by using the popular imaging libraries
    in Python. The following code demonstrates how to flip an image vertically (mirror)
    and horizontally by using a matrix transpose method in Python’s PIL imaging library:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从展示如何使用Python中流行的图像库来翻转图像开始。以下代码演示了如何使用Python的PIL图像库中的矩阵转置方法来垂直（镜像）和水平翻转图像：
- en: '[PRE39]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Reads the image into memory
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像读入内存
- en: ❷ Displays the image in its original perspective
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 显示图像的原始视角
- en: ❸ Flips the image on the vertical axis (mirror)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在垂直轴上翻转图像（镜像）
- en: ❹ Flips the image on the horizontal axis (upside down)
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在水平轴上翻转图像（上下颠倒）
- en: 'Alternately, the flips can be done using the PIL class `ImageOps` module, as
    demonstrated here:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，可以使用PIL类`ImageOps`模块来执行翻转，如下所示：
- en: '[PRE40]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Reads in the image
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取图像
- en: ❷ Flips the image on the vertical axis (mirror)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在垂直轴上翻转图像（镜像）
- en: ❸ Flips the image on the horizontal axis (upside down)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在水平轴上翻转图像（上下颠倒）
- en: 'The following code demonstrates how to flip an image vertically (mirror) and
    horizontally by using a matrix transpose method in OpenCV:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用OpenCV中的矩阵转置方法垂直（镜像）和水平翻转图像：
- en: '[PRE41]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Displays the image in its original perspective
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以原始视角显示图像
- en: ❷ Flips the image on the vertical axis (mirror)
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在垂直轴上翻转图像（镜像）
- en: ❸ Flips the image on the horizontal axis (upside down)
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在水平轴上翻转图像（上下颠倒）
- en: 'This code demonstrates how to flip an image vertically (mirror) and horizontally
    by using a matrix transpose method in NumPy:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用NumPy中的矩阵转置方法翻转图像垂直（镜像）和水平翻转：
- en: '[PRE42]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Flips the image on the vertical axis (mirror)
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在垂直轴上翻转图像（镜像）
- en: ❷ Flips the image on the horizontal axis (upside down)
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在水平轴上翻转图像（上下颠倒）
- en: Rotate 90/180/270
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转90/180/270
- en: In addition to flips, a *matrix transpose* operation can be used to rotate an
    image 90 degrees (left), 180 degrees, and 270 degrees (right). Like a flip, the
    operation is efficient, does not require interpolation of pixels, and does not
    have a side effect of clipping. Figure 4.13 compares the original and 90-degree
    rotation versions.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 除了翻转之外，可以使用*矩阵转置*操作旋转图像90度（左转）、180度和270度（右转）。与翻转一样，该操作效率高，不需要插值像素，并且没有裁剪的副作用。图4.13比较了原始图像和90度旋转版本。
- en: '![](Images/CH04_F13_Ferlitsch.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F13_Ferlitsch.png)'
- en: 'Figure 4.13 Comparison of an apple: 90-, 180-, and 270-degree rotations'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.13 苹果的90度、180度和270度旋转比较
- en: 'This code demonstrates how to rotate an image 90, 180, and 270 degrees by using
    a matrix transpose method in Python’s PIL imaging library:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了如何使用Python的PIL图像库中的矩阵转置方法旋转图像90度、180度和270度：
- en: '[PRE43]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: ❶ Rotates the image 90 degrees
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 旋转图像90度
- en: ❷ Rotates the image 180 degrees
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 旋转图像180度
- en: ❸ Rotates the image 270 degrees
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 旋转图像270度
- en: OpenCV does not have a transpose method for 90 or 270 degrees; you can do a
    180 by using the flip method with a value of –1\. (All other rotations using OpenCV
    are demonstrated in the following subsection, using the `imutils` module.)
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV没有90度或270度的转置方法；您可以使用带有-1值的翻转方法来执行180度翻转。（使用`imutils`模块演示了OpenCV中其他所有旋转方法，见下文小节。）
- en: '[PRE44]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: ❶ Rotates the image 180 degrees
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 旋转图像180度
- en: 'The next example demonstrates how to rotate an image 90, 180, and 270 degrees
    by using the NumPy method `rot90``()`, whose first parameter is the image to rotate
    90 degrees, and the second parameter (`k`) is the number of times to perform the
    rotation:'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例演示了如何使用NumPy的`rot90()`方法旋转图像90度、180度和270度，其中第一个参数是要旋转90度的图像，第二个参数（`k`）是旋转的次数：
- en: '[PRE45]'
  id: totrans-347
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Rotates the image 90 degrees
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 旋转图像90度
- en: ❷ Rotates the image 180 degrees
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 旋转图像180度
- en: ❸ Rotates the image 270 degrees
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 旋转图像270度
- en: When flipping the image 90 or 270 degrees, you are changing the orientation
    of the image, which is not a problem if the height and width of the image are
    the same. If not, the height and width will be transposed in the rotated image
    and will not match the input vector of the neural network. In this case, you should
    use the `imutils` module or other means to resize the image.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 当翻转图像90度或270度时，您正在改变图像的方向，如果图像的高度和宽度相同，则这不是问题。如果不相同，旋转后的图像高度和宽度将互换，并且不会与神经网络的输入向量匹配。在这种情况下，您应使用`imutils`模块或其他方法调整图像大小。
- en: Rotation
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转
- en: 'A *rotation* transforms an image by rotating it within –180 and 180 degrees.
    Generally, the degree of rotation is randomly selected. You may also want to limit
    the range of rotation to match the environment the model will be deployed in.
    Here are some common practices:'
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: '*旋转*通过在-180度和180度之间旋转图像来变换图像。通常，旋转的角度是随机选择的。您可能还想限制旋转的范围以匹配模型部署的环境。以下是一些常见的做法：'
- en: If the images will be dead-on, use a range of –15 to 15 degrees.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果图像将直接对齐，请使用-15到15度的范围。
- en: If the images may be on an incline, use a range of –30 to 30 degrees.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果图像可能处于倾斜状态，请使用-30度到30度的范围。
- en: For small objects, like packages or money, use the full range of –180 to 180
    degrees.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于小型物体，如包裹或货币，请使用-180度到180度的完整范围。
- en: Another issue with rotation is that if you rotate an image within the same-size
    boundaries, other than 90, 180, or 270, a portion of the image’s edge will end
    up outside the boundary (clipped).
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转的另一个问题是，如果您在相同大小的边界内旋转图像，除了90度、180度或270度之外，图像的边缘部分将最终超出边界（裁剪）。
- en: Figure 4.14 is an example of using the PIL method `rotate``()` to rotate the
    image of the apple 45 degrees. You can see that part of the bottom of the apple
    and the leaf are clipped.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14展示了使用PIL方法`rotate()`将苹果图像旋转45度的例子。你可以看到苹果底部的一部分和叶子被裁剪掉了。
- en: '![](Images/CH04_F14_Ferlitsch.png)'
  id: totrans-359
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F14_Ferlitsch.png)'
- en: Figure 4.14 Example of image being clipped when rotated a non-multiple of 90
    degrees
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.14 旋转非90度倍数时的图像裁剪示例
- en: 'The correct way to handle a rotation is to rotate it within a larger bounding
    area, so that none of the image is clipped, and then resize the rotated image
    back to its original size. For this purpose, I recommend using the `imutils` module
    (created by Adrian Rosebrock, [http://mng.bz/JvR0](https://shortener.manning.com/JvR0)),
    which consists of a collection of convenience methods for OpenCV:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 正确处理旋转的方法是在更大的边界区域内旋转，这样图像的任何部分都不会被裁剪，然后将旋转后的图像调整回原始大小。为此，我推荐使用`imutils`模块（由Adrian
    Rosebrock创建，[http://mng.bz/JvR0](https://shortener.manning.com/JvR0)），它包含了一组针对OpenCV的便利方法：
- en: '[PRE46]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Remembers the original height and width
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记录原始高度和宽度
- en: ❷ Rotates the image
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 旋转图像
- en: ❸ Resizes the image back to its original shape
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将图像调整回原始形状
- en: Shift
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 移动
- en: A *shift* will shift the pixel data in the image +/– in the vertical (height)
    or horizontal (width) axis. This will change the location in the image of the
    object being classified. Figure 4.15 shows the apple image shifted down 10% and
    up 10%.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 移动会将图像中的像素数据在垂直（高度）或水平（宽度）轴上移动+/-。这将改变被分类对象在图像中的位置。图4.15显示了苹果图像向下移动10%和向上移动10%的情况。
- en: '![](Images/CH04_F15_Ferlitsch.png)'
  id: totrans-368
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F15_Ferlitsch.png)'
- en: 'Figure 4.15 Comparison of an apple: original, 10% shift down, 10% shift up'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.15 苹果的比较：原始图像，向下移动10%，向上移动10%
- en: 'The following code demonstrates shifting the image +/– 10% vertically and horizontally
    by using the NumPy `np.roll()` method:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码演示了使用NumPy的`np.roll()`方法垂直和水平移动图像+/- 10%：
- en: '[PRE47]'
  id: totrans-371
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ Gets the height and width of the image
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取图像的高度和宽度
- en: ❷ Shifts the image down by 10%
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向下移动图像10%
- en: ❸ Shifts the image up by 10%
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 向上移动图像10%
- en: ❹ Shifts the image right by 10%
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 向右移动图像10%
- en: ❺ Shifts the image left by 10%
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 向左移动图像10%
- en: A shift is efficient in that it is implemented as a roll operation of the matrix;
    the rows (height) or columns (width) are shifted. As such, the pixels that are
    shifted off the end are added to the beginning.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 移动是高效的，因为它作为矩阵的滚动操作实现；行（高度）或列（宽度）被移动。因此，移出末尾的像素被添加到开始处。
- en: If the shift is too large, the image can become fractured into two pieces, with
    each piece opposing the other. Figure 4.16 shows the apple shifted by 50% vertically,
    leaving it fractured.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 如果移动太大，图像可以分裂成两块，每块都与另一块相对。图4.16显示了苹果垂直移动了50%，导致其破碎。
- en: '![](Images/CH04_F16_Ferlitsch.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F16_Ferlitsch.png)'
- en: Figure 4.16 When an image is shifted too much, it becomes fractured.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.16 当图像移动过多时，它变得破碎。
- en: 'To avoid fracture, limiting the shift of the image to no more than 20% is a
    general practice. Alternatively, we could crop the image and fill the cut-off
    space with a black pad, as demonstrated here using OpenCV:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 为了避免破碎，通常将图像的移动限制在不超过20%。或者，我们可以裁剪图像，并用黑色填充裁剪的空间，如下所示使用OpenCV：
- en: '[PRE48]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: ❶ Gets the height of the image
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取图像的高度
- en: ❷ Drops the bottom (50%) of the image
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 删除图像底部（50%）
- en: ❸ Makes black border to refit the image back to its original size
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 添加黑色边框以重新调整图像大小
- en: This code produces the output in figure 4.17.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码生成了图4.17的输出。
- en: '![](Images/CH04_F17_Ferlitsch.png)'
  id: totrans-387
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F17_Ferlitsch.png)'
- en: Figure 4.17 Using a crop and fill to avoid fractures in an image
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.17 使用裁剪和填充避免图像破碎
- en: 4.8.2 Scale invariance
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.2 尺度不变性
- en: This subsection covers how to manually augment images in a training dataset
    such that the model learns to recognize the object in the image regardless of
    the object’s size. For example, we want the model to recognize an apple regardless
    whether it takes up most of the image or is a small fraction of the image overlaid
    on a background.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 本小节介绍了如何在训练数据集中手动增强图像，以便模型学会识别图像中的对象，无论对象的大小如何。例如，我们希望模型能够识别出苹果，无论它占据图像的大部分还是作为图像背景上的小部分。
- en: 'Scale invariance in the context of an image input includes the following:'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 在图像输入的上下文中，尺度不变性包括以下内容：
- en: Zoom (object can be any size in the image)
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 缩放（对象可以是图像中的任何大小）
- en: Affine (object can be viewed from any perspective)
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 放射性（对象可以从任何角度观看）
- en: Zoom
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放
- en: '*Zoom* transforms an image by zooming in from the center of the image, which
    is done with a resize-and-crop operation. You find the center of the image, calculate
    the crop-bounding box around the center, and then crop the image. Figure 4.18
    is the apple image zoomed by a factor of 2.'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: '*缩放*通过从图像中心放大图像来转换图像，这是通过调整大小和裁剪操作完成的。你找到图像的中心，计算围绕中心的裁剪边界框，然后裁剪图像。图4.18是放大2倍的苹果图像。'
- en: '![](Images/CH04_F18_Ferlitsch.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH04_F18_Ferlitsch.png)'
- en: Figure 4.18 The image is cropped after zooming in to maintain the same image
    size.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.18 在放大后裁剪图像以保持相同的图像大小。
- en: 'When enlarging an image by using `Image.resize()` the `Image.BICUBIC` interpolation
    generally provides the best results. This code demonstrates how to zoom into an
    image by using Python’s PIL imaging library:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`Image.resize()`放大图像时，`Image.BICUBIC`插值通常提供最佳结果。此代码演示了如何使用Python的PIL图像库放大图像：
- en: '[PRE49]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Remembers the original height, width of the image
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记录图像的原始高度和宽度
- en: ❷ Resizes (scale) the image proportional to the zoom
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过缩放（按比例）调整图像大小
- en: ❸ Finds the center of the scaled image
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 找到缩放图像的中心
- en: ❹ Calculates the crop upper-left corner
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算裁剪的左上角
- en: ❺ Calculates the crop bounding box
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 计算裁剪边界框
- en: ❻ Crops the image
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 裁剪图像
- en: The next code example demonstrates how to zoom into an image by using the OpenCV
    imaging library. When enlarging an image by using `cv2.resize()` interpolation,
    `cv2.INTER_CUBIC` generally provides the best results. The interpolation `cv2.INTER_LINEAR`
    is faster and provides nearly comparable results. The interpolation `cv2.INTER_AREA`
    is generally used when reducing an image.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码示例演示了如何使用OpenCV图像库放大图像。当使用`cv2.resize()`插值放大图像时，`cv2.INTER_CUBIC`通常提供最佳结果。插值`cv2.INTER_LINEAR`更快，并提供几乎相当的结果。插值`cv2.INTER_AREA`通常用于减小图像。
- en: '[PRE50]'
  id: totrans-407
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: ❶ Remembers the original height, width of the image
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记录图像的原始高度和宽度
- en: ❷ Finds the center of the scaled image
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 找到缩放图像的中心
- en: ❸ Slices (cuts out) the zoomed image by forming a crop bounding box
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过形成裁剪边界框来切割缩放图像
- en: ❹ Resizes (enlarges) the cropped image back to the original size
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将裁剪的图像重新调整回原始大小
- en: 4.8.3 TF.Keras ImageDataGenerator
  id: totrans-412
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.8.3 TF.Keras ImageDataGenerator
- en: 'The TF.Keras image-preprocessing module supports a wide variety of image augmentation
    with the class `ImageDataGenerator`. This class creates a generator for generating
    batches of augmented images. The class initializer takes as input zero or more
    parameters for specifying the type of augmentation. Here are a few of the parameters,
    which we will cover in this section:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: TF.Keras图像预处理模块支持使用`ImageDataGenerator`类进行多种图像增强。这个类创建一个生成器，用于生成增强图像的批次。类的初始化器接受零个或多个参数，用于指定增强的类型。以下是一些参数，我们将在本节中介绍：
- en: '`horizontal_flip=True|False`'
  id: totrans-414
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip=True|False`'
- en: '`vertical_flip=True|False`'
  id: totrans-415
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical_flip=True|False`'
- en: '`rotation_range=degrees`'
  id: totrans-416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotation_range=degrees`'
- en: '`zoom_range=(lower, upper)`'
  id: totrans-417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range=(lower, upper)`'
- en: '`width_shift_range=percent`'
  id: totrans-418
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width_shift_range=percent`'
- en: '`height_shift_range=percent`'
  id: totrans-419
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height_shift_range=percent`'
- en: '`brightness_range=(lower, upper)`'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`brightness_range=(lower, upper)`'
- en: Flip
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 翻转
- en: 'In the following code example, we do the following:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码示例中，我们执行以下操作：
- en: Read in a single image of an apple.
  id: totrans-423
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 读取一个苹果的单张图像。
- en: Create a batch of one image (the apple).
  id: totrans-424
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个包含一个图像（苹果）的批次。
- en: Instantiate an `ImageDataGenerator` object.
  id: totrans-425
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 实例化一个`ImageDataGenerator`对象。
- en: Initialize the `ImageDataGenerator` with our augmentation options (in this case,
    horizontal and vertical flips).
  id: totrans-426
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用我们的增强选项（在这种情况下，水平和垂直翻转）初始化`ImageDataGenerator`。
- en: Use the `flow()` method of `ImageDataGenerator` to create a batch generator.
  id: totrans-427
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`ImageDataGenerator`的`flow()`方法创建一个批次生成器。
- en: Iterate through the generator six times, each time returning a batch of one
    image in `x`.
  id: totrans-428
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过生成器迭代六次，每次返回一个包含一个图像的`x`批次。
- en: The generator will randomly select an augmentation (including no augmentation)
    per iteration.
  id: totrans-429
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 生成器将每次迭代随机选择一个增强（包括无增强）。
- en: After transformation (augmentation), the pixel value type will be 32-bit float.
  id: totrans-430
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 变换（增强）后，像素值类型将是32位浮点数。
- en: Change the data type of the pixels back to 8-bit integer, for displaying using
    Matplotlib.
  id: totrans-431
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将像素的数据类型改回8位整数，以便使用Matplotlib显示。
- en: 'Here is the code:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是代码：
- en: '[PRE51]'
  id: totrans-433
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ❶ Makes a batch of one image (apple)
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 制作一个包含一个图像（苹果）的批次
- en: ❷ Creates a data generator for augmenting the data
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建一个用于增强数据的生成器
- en: ❸ Runs the generator, where every image is a random augmentation
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行生成器，其中每个图像都是随机增强
- en: ❹ The augmentation operation changes the pixel data to float, then changes it
    back to uint8 for displaying the image.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 增强操作将像素数据转换为浮点数，然后将其转换回uint8以显示图像。
- en: Rotation
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转
- en: 'In the following code, we use the `rotation_range` parameter to set random
    rotations between –60 and 60 degrees. Note that rotate operation does not perform
    a bounds check and resize (like `imutils.rotate_bound()`), so part of the image
    may end up being clipped:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用`rotation_range`参数设置介于-60度和60度之间的随机旋转。请注意，旋转操作不执行边界检查和调整大小（如`imutils.rotate_bound()`），因此图像的一部分可能最终被裁剪：
- en: '[PRE52]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Zoom
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放
- en: 'In this code, we use the `zoom_range` parameter to set random values from 0.5
    (zoom out) to 2 (zoom in). The value can be specified either as a tuple or list
    of two elements:'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们使用`zoom_range`参数设置从0.5（缩小）到2（放大）的随机值。该值可以是两个元素的元组或列表：
- en: '[PRE53]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Shift
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 平移
- en: 'In this code, we use `width_shift_range` and `height_shift_range` to set random
    values from 0 to 20% to shift horizontally or vertically:'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 在此代码中，我们使用`width_shift_range`和`height_shift_range`设置从0到20%的随机值以水平或垂直移动：
- en: '[PRE54]'
  id: totrans-446
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Brightness
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 亮度
- en: 'In the following code, we use the `brightness_range` parameter to set random
    values from 0.5 (darker) to 2 (brighter). The value can be specified either as
    a tuple or list of two elements:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下代码中，我们使用`brightness_range`参数设置从0.5（较暗）到2（较亮）的随机值。该值可以是两个元素的元组或列表：
- en: '[PRE55]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: As a final note, transformations like brightness that add a fixed amount to
    the pixel value are done after normalization or standardization. If done before,
    normalization and standardization would squash the values into the same original
    range, undoing the transformation.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的注意事项，像亮度这样的变换，在像素值上添加一个固定量，是在归一化或标准化之后完成的。如果在此之前完成，归一化和标准化会将值压缩到相同的原始范围，从而撤销变换。
- en: 4.9 Raw (disk) datasets
  id: totrans-451
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.9 原始（磁盘）数据集
- en: So far, we have discussed training techniques for images that are stored and
    accessed directly from memory. This works for small datasets, such as those with
    tiny images, or for larger images in datasets that contain fewer than 50,000 images.
    But once we start training with larger-size images and large numbers of images,
    such as several hundred thousand images, your dataset will likely be stored on
    disk. This subsection covers common conventions for storing images on disk and
    accessing them for training.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了直接从内存中存储和访问的图像的训练技术。这对于小型数据集有效，例如那些包含微小图像的数据集，或者对于包含少于50,000个图像的大型图像数据集。但是，一旦我们开始使用较大尺寸的图像和大量图像进行训练，例如几十万个图像，您的数据集很可能存储在磁盘上。本小节涵盖了在磁盘上存储图像和访问它们进行训练的常见约定。
- en: 'Beyond the curated datasets used for academic/research purposes, the datasets
    we use in production are likely stored on disk (or a database, if structured data).
    In the case of image data, we need to do the following:'
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 除了用于学术/研究目的的精选数据集之外，我们在生产中使用的数据集很可能存储在磁盘上（如果是结构化数据，则为数据库）。在图像数据的情况下，我们需要执行以下操作：
- en: Read images and corresponding labels from disk into memory (assuming image data
    fits into memory).
  id: totrans-454
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从磁盘读取图像及其对应的标签到内存中（假设图像数据适合内存）。
- en: Resize the images to match the input vector of the CNN.
  id: totrans-455
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将图像调整大小以匹配CNN的输入向量。
- en: Next we’ll cover several common methods used to lay out image datasets on disk.
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将介绍几种在磁盘上布局图像数据集的常用方法。
- en: 4.9.1 Directory structure
  id: totrans-457
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.1 目录结构
- en: Placing images into a directory folder structure on a local disk is one of the
    most common layouts. In this layout, shown in figure 4.19, the root (parent) folder
    is a container for the dataset. Below the root level are one or more subdirectories.
    Each subdirectory corresponds to a class (label) and contains the images that
    correspond to that class.
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像放置在本地磁盘上的目录文件夹结构中是最常见的布局之一。在此布局中，如图4.19所示，根（父）文件夹是数据集的容器。在根级别以下有一个或多个子目录。每个子目录对应一个类别（标签）并包含对应类别的图像。
- en: Using our cats and dogs example, we would have a parent directory that might
    be named cats_n_dogs, with two subdirectories, one named cats and the other dogs.
    Within each subdirectory would be the corresponding class of images.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们的猫和狗示例，我们可能有一个名为cats_n_dogs的父目录，其中包含两个子目录，一个名为cats，另一个名为dogs。在每个子目录中会有对应类别的图像。
- en: '![](Images/CH04_F19_Ferlitsch.png)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH04_F19_Ferlitsch.png)'
- en: Figure 4.19 Directory folder layout by class
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.19按类别目录文件夹布局
- en: Alternatively, if the dataset has been previously split into training and test
    data, we’d first group the data by train/test, and then group the data by the
    two classes for cats and dogs, as depicted in figure 4.20.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果数据集已经被分割成训练和测试数据，我们首先按训练/测试分组数据，然后按猫和狗的两个类别分组数据，如图4.20所示。
- en: '![](Images/CH04_F20_Ferlitsch.png)'
  id: totrans-463
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH04_F20_Ferlitsch.png)'
- en: Figure 4.20 Directory folder layout for splitting by training and test data
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.20 按训练和测试数据分割的目录文件夹布局
- en: When the dataset is hierarchically labeled, each top-level class (label) subfolder
    is further partitioned into child subfolders according to the class (label) hierarchy.
    Using our cats and dogs example, each image is hierarchically labeled by whether
    it’s a cat or dog (species) and then by breed. See figure 4.21.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集是分层标记时，每个顶级类别（标签）子文件夹根据类别（标签）层次进一步划分为子文件夹。以我们的猫和狗为例，每个图像根据是否是猫或狗（物种）进行分层标记，然后按品种划分。见图4.21。
- en: '![](Images/CH04_F21_Ferlitsch.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH04_F21_Ferlitsch.png)'
- en: Figure 4.21 Hierarchical directory folder layout for hierarchical labeling
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.21 分层目录文件夹布局用于分层标记
- en: 4.9.2 CSV file
  id: totrans-468
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.2 CSV文件
- en: 'Another common layout is to use a comma-separated values (CSV) file to identify
    the location and class (label) of each image. In this case, each row in the CSV
    file is a separate image, and the CSV file contains at least two columns, one
    for the location of the image, and the other for the class (label) of the image.
    The location might be a local path, a remote location, or the pixel data that’s
    embedded as the value of the location:'
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的布局是使用逗号分隔值（CSV）文件来标识每个图像的位置和类别（标签）。在这种情况下，CSV文件中的每一行都是一个单独的图像，CSV文件至少包含两列，一列用于图像的位置，另一列用于图像的类别（标签）。位置可能是一个本地路径、远程位置，或者作为位置值的嵌入像素数据：
- en: 'Local path example:'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地路径示例：
- en: '[PRE56]'
  id: totrans-471
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '...'
  id: totrans-472
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'Remote path example:'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 远程路径示例：
- en: '[PRE57]'
  id: totrans-474
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '...'
  id: totrans-475
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '...'
- en: 'Embedded data example:'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 嵌入数据示例：
- en: '[PRE58]'
  id: totrans-477
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 4.9.3 JSON file
  id: totrans-478
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.3 JSON文件
- en: Another common layout is to use a JavaScript Object Notation (JSON) file to
    identify the location and class (label) of each image. In this case, the JSON
    file is an array of objects; each object is a separate image, and each object
    has at least two keys, one for the location of the image, and the other for the
    class (label) of the image.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的布局是使用JavaScript对象表示法（JSON）文件来标识每个图像的位置和类别（标签）。在这种情况下，JSON文件是一个对象数组；每个对象是一个单独的图像，每个对象至少有两个键，一个用于图像的位置，另一个用于图像的类别（标签）。
- en: 'The location might be either a local path, a remote location, or pixel data
    embedded as the value of the location. Here is a local path example:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 位置可能是一个本地路径、远程位置，或者作为位置值的嵌入像素数据。以下是一个本地路径示例：
- en: '[PRE59]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 4.9.4 Reading images
  id: totrans-482
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.4 读取图像
- en: 'When training on an on-disk dataset, the first step is to read an image from
    disk into memory. The image on disk will be in an image format such as JPG, PNG,
    or TIF. These formats define how the image is encoded and compressed for storage.
    An image can be read into memory by using the PIL `Image.open()` method:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 在磁盘数据集上训练时，第一步是从磁盘读取图像到内存。磁盘上的图像将以JPG、PNG或TIF等图像格式存在。这些格式定义了图像的编码和压缩方式以便存储。可以通过使用PIL的`Image.open()`方法将图像读取到内存中：
- en: '[PRE60]'
  id: totrans-484
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'In practice, you will have many images that need to be read in. Let’s assume
    you want to read in all the images under a subdirectory (for example, cats). In
    the following code, we scan (get a list of) all the files in the subdirectory,
    read each one in as an image, and maintain a list of the read-in images as a list:'
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，你可能需要读取很多图像。假设你想要读取子目录（例如，猫）下的所有图像。在下面的代码中，我们扫描（获取子目录中所有文件的列表），将每个文件作为图像读取，并将读取的图像列表作为列表维护：
- en: '[PRE61]'
  id: totrans-486
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: ❶ Procedure to read all images in a subfolder for a single class label
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取单个类别标签下子文件夹中所有图像的步骤
- en: ❷ Gets list of all files in the subdirectory cats
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取子目录cats中所有文件的列表
- en: ❸ Reads each image in and appends the in-memory image to a list
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 读取每个图像并将其内存中的图像追加到列表中
- en: ❹ Reads all images in the subfolder cats
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 读取子目录cats中的所有图像
- en: Note that `os.scandir()` was added in Python 3.5\. If you are using Python 2.7
    or an earlier version of Python 3, you can obtain a compatible version with `pip
    install scandir`.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`os.scandir()`是在Python 3.5中添加的。如果你使用Python 2.7或更早版本的Python 3，你可以使用`pip install
    scandir`来获取兼容版本。
- en: 'Let’s expand on the preceding example and assume that the image dataset is
    laid out as a directory structure; each subdirectory is a class (label). In this
    case, we would want to scan each subdirectory separately and keep a record of
    the subdirectory names for the classes:'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们扩展前面的例子并假设图像数据集是按目录结构布局的；每个子目录是一个类别（标签）。在这种情况下，我们希望分别扫描每个子目录并记录类别的子目录名称：
- en: '[PRE62]'
  id: totrans-493
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: ❶ Procedure for reading all images of a dataset by class
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 读取数据集所有图像的类别的步骤
- en: ❷ Gets list of all subdirectories under the parent (root) directory of the dataset
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取数据集父目录（根目录）下的所有子目录列表
- en: ❸ Ignores any entry that is not a subdirectory (e.g., license file)
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 忽略任何非子目录的条目（例如，许可证文件）
- en: ❹ Maintains mapping of class (subdirectory name) to label (index)
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 维护类别（子目录名称）到标签（索引）的映射
- en: ❺ Returns the dataset images and class mappings
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回数据集图像和类别映射
- en: ❻ Reads all images by class for the dataset cats_n_dogs
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 读取数据集 cats_n_dogs 中所有图像的类别
- en: 'Let’s now try an example in which the location of the image is remote (not
    local) and specified by a URL. In this case, we will need to make an HTTP request
    for the contents of the resource (image) specified by the URL and then decode
    the response into a binary byte stream:'
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们尝试一个例子，其中图像的位置是远程的（非本地）并且由 URL 指定。在这种情况下，我们需要对 URL 指定的资源（图像）的内容发出 HTTP 请求，然后将响应解码成二进制字节流：
- en: '[PRE63]'
  id: totrans-501
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: ❶ Python package for HTTP requests
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Python 的 HTTP 请求包
- en: ❷ Python package for deserializing I/O into a byte stream
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Python 的反序列化 I/O 到字节流的包
- en: ❸ Requests the image content at the specified URL
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 请求指定 URL 的图像内容
- en: ❹ Reads the deserialized content into memory as an image
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将反序列化的内容读取到内存中作为图像
- en: After you’ve read in the images for training, you need to set the number of
    channels to match the input shape of your convolutional neural network, such as
    a single channel for grayscale or three channels for RGB images.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在读取训练图像后，您需要设置通道数以匹配卷积神经网络的输入形状，例如灰度图像的单通道或 RGB 图像的三个通道。
- en: The number of channels is the number of color planes in your image. For example,
    a grayscale image will have one color channel. An RGB color image will have three
    color channels, one for each of red, green and blue. In most cases, this is either
    going to be a single channel (grayscale) or three channels (RGB), as depicted
    in figure 4.22.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 通道数是您图像中的颜色平面的数量。例如，灰度图像将有一个颜色通道。RGB 颜色图像将有三个颜色通道，每个通道分别对应红色、绿色和蓝色。在大多数情况下，这将是一个单通道（灰度）或三个通道（RGB），如图
    4.22 所示。
- en: '![](Images/CH04_F22_Ferlitsch.png)'
  id: totrans-508
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH04_F22_Ferlitsch.png)'
- en: Figure 4.22 A grayscale image has one channel, and an RGB image has three channels.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.22 灰度图像有一个通道，RGB 图像有三个通道。
- en: The `Image.open()` method will read in the image according to the number of
    channels in the image stored on the disk. So if it’s a grayscale image, the method
    will read it in as a single channel; if it’s RGB, it will read it in as three
    channels; and if it’s RGBA (+alpha channel), it will read it in as four channels.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: '`Image.open()` 方法将根据磁盘上存储的图像的通道数读取图像。因此，如果是一个灰度图像，该方法将作为一个通道读取它；如果是 RGB 图像，它将作为三个通道读取；如果是
    RGBA（+alpha 通道），它将作为四个通道读取。'
- en: In general, when working with RGBA images, the alpha channel can be dropped.
    It is a mask for setting the transparency of each pixel in the image, and therefore
    does not contain information that would otherwise contribute to the recognition
    of the image.
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当处理 RGBA 图像时，可以丢弃 alpha 通道。它是设置图像中每个像素透明度的掩码，因此不包含有助于图像识别的其他信息。
- en: 'Once the image is read into memory, the next step is to convert the image to
    the number of channels that match the input shape of your neural network. So if
    the neural network takes grayscale images (single channel), we want to convert
    to grayscale; or if the neural network takes RGB images (three channels), we want
    to convert to RGB. The `convert()` method performs channel conversion. A parameter
    value of `L` converts to a single channel (grayscale), and RGB converts to three
    channels (RGB color). Here we’ve updated the `loadImages()` function to include
    channel conversion:'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被读入内存，下一步是将图像转换为与您的神经网络输入形状匹配的通道数。因此，如果神经网络接受灰度图像（单通道），我们希望将其转换为灰度；或者如果神经网络接受
    RGB 图像（三个通道），我们希望将其转换为 RGB。`convert()` 方法执行通道转换。参数值 `L` 转换为单通道（灰度），RGB 转换为三个通道（RGB
    颜色）。在这里，我们已经更新了 `loadImages()` 函数以包括通道转换：
- en: '[PRE64]'
  id: totrans-513
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ❶ Converts to grayscale
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 转换为灰度图
- en: ❷ Converts to RGB
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 转换为 RGB
- en: ❸ Specifies conversion to RGB
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定转换为 RGB
- en: 4.9.5 Resizing
  id: totrans-517
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.9.5 调整大小
- en: So far, you’ve seen how to read in the image from the disk, get the label, and
    then set the number of channels to match the number of channels in the input shape
    of the CNN. Next, we need to resize the height and width of the image to finalize
    matching the input shape for feeding images during training.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经看到了如何从磁盘读取图像，获取标签，然后设置通道数以匹配CNN输入形状中的通道数。接下来，我们需要调整图像的高度和宽度以最终匹配训练期间输入图像的形状。
- en: For example, a 2D convolutional neural network will take the shape of the form
    (height, width, channels). We dealt with the channel portion already, so next
    we need to resize the pixel height and width of each image to match the input
    shape. For example, if the input shape is (128, 128, 3), we want to resize the
    height and width of each image to (128, 128). The `resize()` method will do the
    resizing.
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个二维卷积神经网络将具有形式为（高度，宽度，通道）的形状。我们已经处理了通道部分，所以接下来我们需要调整每个图像的像素高度和宽度以匹配输入形状。例如，如果输入形状是（128，128，3），我们希望将每个图像的高度和宽度调整到（128，128）。`resize()`
    方法将执行调整大小。
- en: 'In most cases, you will be downsizing (downsampling) each image. For example,
    an 1024 × 768 image will be 3 MB in size. This is far more resolution than a neural
    network needs (see chapter 3 for more details). When the image is downsized, some
    resolution (details) will be lost. To minimize the effect when downsizing, it
    is a common practice to use the anti-aliasing algorithm in PIL. Finally, we will
    then want to convert our list of PIL images into a multidimensional array:'
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，您将减小每个图像的大小（下采样）。例如，一个 1024 × 768 的图像将大小为 3 MB。这比神经网络所需的分辨率要高得多（更多细节请见第
    3 章）。当图像被下采样时，一些分辨率（细节）将会丢失。为了最小化下采样时的影响，一个常见的做法是在 PIL 中使用反走样算法。最后，我们将然后将我们的 PIL
    图像列表转换为多维数组：
- en: '[PRE65]'
  id: totrans-521
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: ❶ Resizes the image to the target input shape
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像调整为目标输入形状
- en: ❷ Converts all the PIL images to NumPy arrays in a single invocation
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在单次调用中将所有 PIL 图像转换为 NumPy 数组
- en: ❸ Specifies target input size of 128 × 128.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定目标输入大小为 128 × 128。
- en: 'Let’s now repeat the preceding steps by using OpenCV. An image is read into
    memory by using the `cv2.imread()` method. One of the first advantages I find
    with this method is that the output is already in a multidimensional NumPy data
    type:'
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用 OpenCV 重复前面的步骤。通过使用 `cv2.imread()` 方法将图像读入内存。我发现这个方法的一个优点是输出已经是一个多维
    NumPy 数据类型：
- en: '[PRE66]'
  id: totrans-526
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'Another advantage of OpenCV over PIL is that you can do the channel conversion
    at the time of reading in the image, instead of a second step. By default, `cv2.imread()`
    will convert the image to a three-channel RGB image. You can specify a second
    parameter that indicates which channel conversion to use. In the following example,
    we are doing the channel conversion at the time the image is read in:'
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: OpenCV 相对于 PIL 的另一个优点是您可以在读取图像时进行通道转换，而不是第二步。默认情况下，`cv2.imread()` 将图像转换为三通道
    RGB 图像。您可以指定一个第二个参数，以指示要使用的通道转换。在以下示例中，我们在读取图像时进行通道转换：
- en: '[PRE67]'
  id: totrans-528
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ❶ Reads in image as a single-channel (grayscale) image
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 以单通道（灰度）图像读取图像
- en: ❷ Reads in image as a three-channel (color) image
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 以三通道（彩色）图像读取图像
- en: 'In the next example, we read in the image from a remote location (`url`) and
    do the channel conversion at the same time. In this case, we use the method `cv2.imdecode``()`:'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: '在下一个示例中，我们从远程位置（`url`）读取图像，并在同一时间进行通道转换。在这种情况下，我们使用 `cv2.imdecode()` 方法： '
- en: '[PRE68]'
  id: totrans-532
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'Images are resized by using the `cv2.resize()` method. The second parameter
    is a tuple of the height and width for the resized image. The optional (keyword)
    third parameter is the interpolation algorithm to use when resizing. Since in
    most cases you will be downsampling, a common practice is to use the `cv2.INTER_AREA`
    algorithm for best results in preserving information and minimizing artifacts
    when downsampling an image:'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `cv2.resize()` 方法调整图像大小。第二个参数是一个包含调整后图像高度和宽度的元组。可选的（关键字）第三个参数是在调整大小时使用的插值算法。由于在大多数情况下您将进行下采样，一个常见的做法是使用
    `cv2.INTER_AREA` 算法以在保留信息和最小化下采样图像时的伪影方面获得最佳结果：
- en: '[PRE69]'
  id: totrans-534
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'Let’s now rewrite the `loadImages()` function by using OpenCV:'
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在使用 OpenCV 重写 `loadImages()` 函数：
- en: '[PRE70]'
  id: totrans-536
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: ❶ Resizes the image to the target input shape
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将图像调整为目标输入形状
- en: ❷ Specifies target input shape as 128 × 128
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定目标输入形状为 128 × 128
- en: 4.10 Model save/restore
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 4.10 模型保存/恢复
- en: 'In this subsection, we cover post-training: now that you’ve trained a model,
    what do you do next? Well, you would likely want to save the model architecture
    and corresponding learned weights and biases (parameters) and then subsequently
    restore the model for deployment.'
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 在本小节中，我们将介绍训练后的内容：现在你已经训练了一个模型，接下来你该做什么？嗯，你可能会想要保存模型架构和相应的学习权重和偏差（参数），然后随后恢复模型以进行部署。
- en: 4.10.1 Save
  id: totrans-541
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10.1 保存
- en: 'In TF.Keras, we can save both the model and the trained parameters (weights
    and biases). The model and weights can be saved separately or together. The `save()`
    method saves both the weights/biases and the model to a specified folder in TensorFlow
    SavedModel format. Here is an example:'
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF.Keras中，我们可以保存模型和训练好的参数（权重和偏差）。模型和权重可以分别保存或一起保存。`save()`方法将权重/偏差和模型保存到TensorFlow
    SavedModel格式的指定文件夹中。以下是一个示例：
- en: '[PRE71]'
  id: totrans-543
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: ❶ Trains the model
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练模型
- en: ❷ Saves the model and trained weights and biases
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 保存模型和训练好的权重和偏差
- en: 'The trained weights/biases and the model can be saved separately. The `save_
    weights()` method will save the model’s parameters only to the specified folder
    in TensorFlow Checkpoint format. Here is an example:'
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 训练好的权重/偏差和模型可以分别保存。`save_weights()`方法将模型的参数仅保存到TensorFlow Checkpoint格式的指定文件夹中。以下是一个示例：
- en: '[PRE72]'
  id: totrans-547
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: ❶ Saves the trained weights and biases only
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅保存训练好的权重和偏差
- en: 4.10.2 Restore
  id: totrans-549
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.10.2 恢复
- en: In TF.Keras, we can restore a model architecture and/or the model parameters
    (weights and biases). Restoring a model architecture is generally done for loading
    a prebuilt model, while loading both the model architecture and model parameters
    is generally done for transfer learning (discussed in chapter 11).
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 在TF.Keras中，我们可以恢复模型架构和/或模型参数（权重和偏差）。通常，恢复模型架构是为了加载预构建模型，而同时加载模型架构和模型参数通常用于迁移学习（在第11章中讨论）。
- en: 'Note that loading the model and model parameters is not the same as checkpointing,
    in that we are not restoring the current state of hyperparameters. Therefore,
    this approach should not be used for continuous learning:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，加载模型和模型参数与检查点不同，因为我们不是在恢复超参数的当前状态。因此，这种方法不应用于持续学习：
- en: '[PRE73]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: ❶ Loads a pretrained model
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载预训练模型
- en: 'In the next code example, the trained weights/biases for a model are loaded
    into the corresponding prebuilt model, using the `load_weights()` method:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个代码示例中，使用`load_weights()`方法将模型的训练好的权重/偏差加载到相应的预构建模型中：
- en: '[PRE74]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: ❶ Loads a prebuilt model
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载预构建模型
- en: ❷ Loads pretrained weights for the model
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 加载模型的预训练权重
- en: Summary
  id: totrans-558
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: When a batch of images is fed forward, the difference between the predicted
    value and the ground truths is the loss. The loss is used by the optimizer to
    determine how to update the weights on backward propagation.
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当一批图像被前向传递时，预测值与真实值之间的差异是损失。优化器使用损失来确定在反向传播中如何更新权重。
- en: A small amount of the dataset is held out, as test data, and not trained on.
    After training, the test data is used to observe how well the model generalized
    versus memorizing the data examples.
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将一小部分数据集保留为测试数据，不进行训练。训练完成后，使用测试数据来观察模型泛化能力与记忆数据示例之间的差异。
- en: Validation data is used after each epoch to detect model overfitting.
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在每个epoch之后使用验证数据来检测模型过拟合。
- en: Standardization of the pixel data is preferred over normalization because it
    contributes to slightly better speed of convergence.
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与归一化相比，像素数据的标准化更受欢迎，因为它有助于略微提高收敛速度。
- en: Convergence occurs when the loss plateaus during training.
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练过程中损失值达到平台期时，会发生收敛。
- en: Hyperparameters are used to improve training the model, but are not part of
    the model.
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数用于改进模型的训练，但不是模型的一部分。
- en: Augmentation allows training for invariance with fewer original images.
  id: totrans-565
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强允许使用更少的原始图像进行训练以实现不变性。
- en: Checkpointing is used to recover a good epoch without restarting training after
    training has diverged.
  id: totrans-566
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查点用于在训练发散后恢复一个好的epoch，而无需重新启动训练。
- en: Early stopping saves training time and cost by detecting that the model will
    not improve with further training.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提前停止通过检测模型不会随着进一步训练而改进来节省训练时间和成本。
- en: Small datasets can be trained from in-memory storage and access, but large datasets
    are trained from on-disk storage and access.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 小数据集可以从内存存储和访问中进行训练，但大数据集是从磁盘存储和访问中进行训练的。
- en: After training, you save the model architecture and learned parameters and then
    subsequently restore the model for deployment.
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练完成后，保存模型架构和学习的参数，然后随后恢复模型以进行部署。
