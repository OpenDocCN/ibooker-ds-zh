- en: 9 Managing app releases with rollouts and rollbacks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 使用滚动更新和回滚管理应用发布
- en: You’ll update existing apps far more often than you’ll deploy something new.
    Containerized apps inherit multiple release cadences from the base images they
    use; official images on Docker Hub for operating systems, platform SDKs, and runtimes
    typically have a new release every month. You should have a process to rebuild
    your images and release updates whenever those dependencies get updated, because
    they could contain critical security patches. Key to that process is being able
    to roll out an update safely and give yourself options to pause and roll back
    the update if it goes wrong. Kubernetes has those scenarios covered for Deployments,
    DaemonSets, and StatefulSets.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 你将更频繁地更新现有应用，而不是部署新的东西。容器化应用从它们使用的基镜像中继承了多个发布周期；Docker Hub上的操作系统、平台SDK和运行时官方镜像通常每月都有一个新版本。你应该有一个流程来重建你的镜像并在依赖项更新时发布更新，因为这些依赖项可能包含关键的安全补丁。这个过程的关键是能够安全地推出更新，并在更新出错时提供暂停和回滚更新的选项。Kubernetes为Deployments、DaemonSets和StatefulSets提供了这些场景的解决方案。
- en: A single update approach doesn’t work for every type of application, so Kubernetes
    provides different update strategies for the controllers and options to tune how
    the strategies work. We’ll explore all those options in this chapter. If you’re
    thinking of skipping this one because you’re not excited by the thought of 6,000
    words on application updates, I’d recommend sticking with it. Updates are the
    biggest cause of application downtime, but you can reduce the risk significantly
    if you understand the tools Kubernetes gives you. And I’ll try to inject a little
    excitement along the way.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 单一更新方法并不适用于所有类型的应用，因此Kubernetes为控制器提供了不同的更新策略，并提供了调整策略工作方式的选项。我们将在本章中探讨所有这些选项。如果你因为对6000字关于应用更新的内容不感兴趣而考虑跳过这一部分，我建议你坚持下来。更新是导致应用停机时间最长的原因，但如果你理解Kubernetes提供的工具，你可以显著降低风险。而且，我会尽力在这个过程中加入一些兴奋的元素。
- en: 9.1 How Kubernetes manages rollouts
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 Kubernetes如何管理滚动更新
- en: We’ll start with Deployments—actually, you’ve already done plenty of Deployment
    updates. Every time we’ve applied a change to an existing Deployment (something
    we do 10 times a chapter), Kubernetes has implemented that with a *rollout*. In
    a rollout, the Deployment creates a new ReplicaSet and scales it up to the desired
    number of replicas, while scaling down the previous ReplicaSet to zero replicas.
    Figure 9.1 shows an update in progress.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从部署（Deployments）开始——实际上，你已经进行了很多部署更新。每次我们对现有的部署进行更改（我们每章都会做10次这样的操作），Kubernetes都会通过一个*滚动更新*来实现。在滚动更新中，部署会创建一个新的副本集（ReplicaSet），并将其扩展到所需的副本数量，同时将之前的副本集缩小到零副本。图9.1显示了正在进行的更新。
- en: '![](../Images/9-1.jpg)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-1.jpg)'
- en: Figure 9.1 Deployments control multiple ReplicaSets so they can manage rolling
    updates.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 部署控制多个副本集，以便它们可以管理滚动更新。
- en: Rollouts aren’t triggered from every change to a Deployment, only from a change
    to the Pod spec. If you make a change that the Deployment can manage with the
    current ReplicaSet, like updating the number of replicas, that’s done without
    a rollout.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新不是由对部署的每一次更改触发的，而只是由对Pod规范的更改触发的。如果你做出的是部署可以用当前副本集管理的更改，比如更新副本数量，那么这个更改将不会通过滚动更新来完成。
- en: Try it now Deploy a simple app with two replicas, then update it to increase
    scale, and see how the ReplicaSet is managed.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：部署一个具有两个副本的简单应用，然后更新它以增加规模，看看副本集是如何被管理的。
- en: '[PRE0]'
  id: totrans-9
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The kubectl `rollout` command has options to view and manage rollouts. You can
    see from my output in figure 9.2 that there’s only one rollout in this exercise,
    which was the initial deployment that created the ReplicaSet. The scale update
    changed only the existing ReplicaSet, so there was no second rollout.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubectl rollout`命令有选项可以查看和管理滚动更新。你可以从图9.2中的我的输出中看到，在这个练习中只有一个滚动更新，那就是创建副本集的初始部署。规模更新只更改了现有的副本集，所以没有第二次滚动更新。'
- en: '![](../Images/9-2.jpg)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-2.jpg)'
- en: Figure 9.2 Deployments manage changes through rollouts but only if the Pod spec
    changes.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.2 部署通过滚动更新管理更改，但仅当Pod规范更改时。
- en: Your ongoing application updates will center on deploying new Pods running an
    updated version of your container image. You should manage that with an update
    to your YAML specs, but kubectl provides a quick alternative with the `set` command.
    Using this command is an imperative way to update an existing Deployment, and
    you should view it the same as the `scale` command—it’s a useful hack to get out
    of a sticky situation, but it needs to be followed up with an update to the YAML
    files.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 您的应用程序持续更新将集中在部署运行更新版本容器镜像的新Pods上。您应该通过更新YAML规范来管理这一点，但kubectl提供了使用`set`命令的快速替代方案。使用此命令是更新现有Deployment的强制方式，您应该将其视为与`scale`命令相同——这是一个有用的技巧，可以帮助您摆脱困境，但需要随后更新YAML文件。
- en: Try it now Use kubectl to update the image version for the Deployment. This
    is a change to the Pod spec, so it will trigger a new rollout.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：使用kubectl更新Deployment的镜像版本。这是一个对Pod规范的更改，因此将触发一个新的滚动发布。
- en: '[PRE1]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The kubectl `set` command changes the spec of an existing object. You can use
    it to change the image or environment variables for a Pod or the selector for
    a Service. It’s a shortcut to applying a new YAML spec, but it is implemented
    in the same way. In this exercise, the change caused a rollout, with a new ReplicaSet
    created to run the new Pod spec and the old ReplicaSet scaled down to zero. You
    can see this in figure 9.3.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl的`set`命令更改现有对象的规范。您可以使用它来更改Pod的镜像或环境变量或服务的选择器。它是应用新YAML规范的快捷方式，但它是以相同的方式实现的。在这个练习中，更改导致了一个滚动，创建了一个新的ReplicaSet来运行新的Pod规范，而旧的ReplicaSet被缩放到零。您可以在图9.3中看到这一点。
- en: '![](../Images/9-3.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图9.3](../Images/9-3.jpg)'
- en: Figure 9.3 Imperative updates go through the same rollout process, but now your
    YAML is out of sync.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 强制更新会经过相同的滚动过程，但现在您的YAML已经不同步。
- en: Kubernetes uses the same concept of rollouts for the other Pod controllers,
    DaemonSets and StatefulSets. They’re an odd part of the API because they don’t
    map directly to an object (you don’t create a resource with the kind “rollout”),
    but they’re an important management tool to work with your releases. You can use
    rollouts to track release history and to revert back to previous releases.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes使用与其他Pod控制器（如DaemonSets和StatefulSets）相同的滚动概念。它们是API的一个奇怪部分，因为它们不直接映射到对象（您不使用“rollout”类型创建资源），但它们是与您的发布一起工作的重要管理工具。您可以使用滚动来跟踪发布历史并回滚到以前的发布。
- en: 9.2 Updating Deployments with rollouts and rollbacks
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 使用滚动和回滚更新Deployment
- en: If you look again at figure 9.3, you’ll see the rollout history is pretty unhelpful.
    There’s a revision number recorded for each rollout but nothing else. It’s not
    clear what caused the change or which ReplicaSet relates to which revision. It’s
    good to include a version number (or a Git commit ID) as a label for the Pods,
    and then the Deployment adds that label to the ReplicaSet, too, which makes it
    easier to trace updates.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您再次查看图9.3，您会看到滚动历史记录非常不实用。每个滚动都记录了一个修订号，但没有其他信息。不清楚是什么导致了更改或哪个ReplicaSet与哪个修订号相关。为Pod添加版本号（或Git提交ID）作为标签是个好主意，然后Deployment也会将此标签添加到ReplicaSet中，这使得跟踪更新更容易。
- en: Try it now Apply an update to the Deployment, which uses the same Docker image
    but changes the version label for the Pod. That’s a change to the Pod spec, so
    it will create a new rollout.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：应用对Deployment的更新，它使用相同的Docker镜像，但更改了Pod的版本标签。这是一个对Pod规范的更改，因此将创建一个新的滚动发布。
- en: '[PRE2]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: My output appears in figure 9.4\. Adding the `record` flag saves the kubectl
    command as a detail to the rollout, which can be helpful if your YAML files have
    identifying names. Often they won’t because you’ll be deploying a whole folder,
    so the version number label in the Pod spec is a useful addition. Then, however,
    you need some awkward JSONPath to find the link between a rollout revision and
    a ReplicaSet.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出出现在图9.4中。添加`record`标志将kubectl命令作为细节保存到滚动中，如果您的YAML文件有标识名称，这可能很有帮助。通常它们不会有，因为您将部署整个文件夹，所以Pod规范中的版本号标签是一个有用的补充。然而，然后您需要一些尴尬的JSONPath来找到滚动修订版和ReplicaSet之间的链接。
- en: '![](../Images/9-4.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图9.4](../Images/9-4.jpg)'
- en: Figure 9.4 Kubernetes uses labels for key information, and extra detail is stored
    in annotations.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 Kubernetes使用标签来存储关键信息，额外的详细信息存储在注释中。
- en: As your Kubernetes maturity increases, you’ll want to have a standard set of
    labels that you include in all your object specs. Labels and selectors are a core
    feature, and you’ll use them all the time to find and manage objects. Application
    name, component name, and version are good labels to start with, but it’s important
    to distinguish between the labels you include for your convenience and the labels
    that Kubernetes uses to map object relationships.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你的Kubernetes成熟度提高，你将希望有一个标准的标签集，你将包括在所有对象规范中。标签和选择器是核心功能，你将经常使用它们来查找和管理对象。应用程序名称、组件名称和版本是良好的起始标签，但区分你为方便而包含的标签和Kubernetes用于映射对象关系的标签很重要。
- en: Listing 9.1 shows the Pod labels and the selector for the Deployment in the
    previous exercise. The `app` label is used in the selector, which the Deployment
    uses to find its Pods. The Pod also contains a `version` label for our convenience,
    but that’s not part of the selector. If it were, then the Deployment would be
    linked to one version, because you can’t change the selector once a Deployment
    is created.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1显示了在之前练习中的Pod标签和Deployment的选择器。`app`标签用于选择器，Deployment使用它来找到其Pod。Pod还包含一个`version`标签以方便我们使用，但它不是选择器的一部分。如果它是，那么Deployment就会与一个版本相关联，因为你一旦创建了Deployment就不能更改选择器。
- en: Listing 9.1 vweb-v11.yaml, a Deployment with additional labels in the Pod spec
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 vweb-v11.yaml，一个在Pod规范中包含额外标签的Deployment
- en: '[PRE3]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You need to plan your selectors carefully up front, but you should add whatever
    labels you need to your Pod spec to make your updates manageable. Deployments
    retain multiple ReplicaSets (10 is the default), and the Pod template hash in
    the name makes them hard to work with directly, even after just a few updates.
    Let’s see what the app we’ve deployed actually does and then look at the ReplicaSets
    in another rollout.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要提前仔细规划选择器，但你应该将你需要的所有标签添加到Pod规范中，以便使更新可管理。Deployments保留多个ReplicaSets（默认为10个），名称中的Pod模板哈希值使得它们在经过几次更新后仍然难以直接操作。让我们看看我们部署的应用实际上做了什么，然后看看另一个rollout中的ReplicaSets。
- en: Try it now Make an HTTP call to the web Service to see the response, then start
    another update and check the response again.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看。向web服务发起HTTP调用以查看响应，然后开始另一个更新并再次检查响应。
- en: '[PRE4]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You’ll see in this exercise that ReplicaSets aren’t easy objects to manage,
    which is where standardized labels come in. It’s easy to see which version of
    the app is active by checking the labels for the ReplicaSet which has all the
    desired replicas—as you see in figure 9.5—but labels are just text fields, so
    you need process safeguards to make sure they’re reliable.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会发现ReplicaSets不是容易管理的对象，这就是标准化标签发挥作用的地方。通过检查具有所有所需副本的ReplicaSet的标签，你可以轻松地看到哪个版本的app是活动的——如图9.5所示——但标签只是文本字段，所以你需要处理保障来确保它们的可靠性。
- en: '![](../Images/9-5.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图9-5](../Images/9-5.jpg)'
- en: Figure 9.5 Kubernetes manages rollouts for you, but it helps if you add labels
    to see what’s what.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 Kubernetes为你管理rollouts，但如果你添加标签以便了解情况，那就更有帮助了。
- en: Rollouts do help to abstract away the details of the ReplicaSets, but their
    main use is to manage releases. We’ve seen the rollout history from kubectl, and
    you can also run commands to pause an ongoing rollout or roll back a deployment
    to an earlier revision. A simple command will roll back to the previous deployment,
    but if you want to roll back to a specific version, you need some more JSONPath
    trickery to find the revision you want. We’ll see that now and use a very handy
    feature of kubectl that tells you what will happen when you run a command, without
    actually executing it.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: Rollouts确实有助于抽象出ReplicaSets的细节，但它们的主要用途是管理发布。我们已经从kubectl中看到了rollout历史，你也可以运行命令来暂停正在进行的rollout或将部署回滚到早期版本。一个简单的命令会回滚到上一个部署，但如果你想要回滚到特定版本，你需要使用一些JSONPath技巧来找到你想要的修订版本。我们现在将展示这一点，并使用kubectl的一个非常实用的功能，它会在实际执行命令之前告诉你命令执行的结果。
- en: Try it now Check the rollout history and try rolling back to v1 of the app.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看。检查rollout历史并尝试将应用回滚到v1版本。
- en: '[PRE5]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Hands up if you ran that exercise and got confused when you saw the final output
    shown in figure 9.6 (this is the exciting part of the chapter). My hand is up,
    and I already knew what was going to happen. This is why you need a consistent
    release process, preferably one that is fully automated, because as soon as you
    start mixing approaches, you get confusing results. I rolled back to revision
    2, and that should have reverted back to v1 of the app, judging by the labels
    on the ReplicaSets. But revision 2 was actually from the `kubectl` `set` `image`
    exercise in section 9.1, so the container image is v2, but the ReplicaSet label
    is v1.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在看到图9.6中显示的最终输出时感到困惑，请举手（这是本章的精彩部分）。我的手举起来了，我早就知道会发生什么。这就是为什么你需要一个一致的发布流程，最好是完全自动化的，因为一旦你开始混合方法，你就会得到令人困惑的结果。我回滚到了修订版2，根据ReplicaSets上的标签，这应该会回滚到应用程序的v1版本。但修订版2实际上是来自第9.1节中的`kubectl`
    `set` `image`练习，所以容器镜像版本是v2，但ReplicaSet标签是v1。
- en: '![](../Images/9-6.jpg)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图9.6](../Images/9-6.jpg)'
- en: Figure 9.6 Labels are a key management feature, but they’re set by humans so
    they’re fallible.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 标签是关键管理功能，但它们是由人类设置的，因此是易出错的。
- en: 'You see that the moving parts of the release process are fairly simple: Deployments
    create and reuse ReplicaSets, scaling them up and down as required, and changes
    to ReplicaSets are recorded as rollouts. Kubernetes gives you control of the key
    factors in the rollout strategy, but before we move on to that, we’re going to
    look at releases which also involve a configuration change, because that adds
    another complicating factor.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 你会发现发布流程的移动部分相当简单：部署创建和重用ReplicaSets，根据需要对其进行扩展和缩减，而ReplicaSets的更改则记录为滚动更新。Kubernetes让你能够控制滚动策略中的关键因素，但在我们继续之前，我们将看看也涉及配置更改的发布，因为这增加了另一个复杂因素。
- en: In chapter 4, I talked about different approaches to updating the content of
    ConfigMaps and Secrets, and the choice you make impacts your ability to roll back
    cleanly. The first approach is to say that configuration is mutable, so a release
    might include a ConfigMap change, which is an update to an existing ConfigMap
    object. But if your release is *only* a configuration change, then you have no
    record of that as a rollout and no option to roll back.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第4章中，我谈到了更新ConfigMaps和Secrets内容的不同方法，你做出的选择会影响你干净回滚的能力。第一种方法是说配置是可变的，因此发布可能包括ConfigMap更改，这是对现有ConfigMap对象的更新。但如果你发布的**仅**是配置更改，那么你将没有滚动更新的记录，也没有回滚的选项。
- en: Try it now Remove the existing Deployment so we have a clean history, then deploy
    a new version that uses a ConfigMap, and see what happens when you update the
    same ConfigMap.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个：移除现有的Deployment，以便我们有干净的历史记录，然后部署一个使用ConfigMap的新版本，看看当你更新相同的ConfigMap时会发生什么。
- en: '[PRE6]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: As you see in figure 9.7, the update to the ConfigMap changes the behavior of
    the app, but it’s not a change to the Deployment, so there is no revision to roll
    back to if the configuration change causes an issue.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图9.7中看到的，ConfigMap的更新改变了应用程序的行为，但这不是对Deployment的更改，因此如果配置更改导致问题，就没有可回滚的修订版。
- en: '![](../Images/9-7.jpg)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7](../Images/9-7.jpg)'
- en: Figure 9.7 Configuration updates might change app behavior but without recording
    a rollout.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 配置更新可能会改变应用程序的行为，但不会记录滚动更新。
- en: This is the hot reload approach, which works nicely if your apps support it,
    precisely because a configuration-only change doesn’t require a rollout. The existing
    Pods and containers keep running, so there’s no risk of service interruption.
    The cost is the loss of the rollback option, and you’ll have to decide whether
    that’s more important than a hot reload.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是热重载方法，如果你的应用程序支持它，这种方法工作得很好，因为仅配置更改不需要滚动更新。现有的Pods和容器继续运行，因此没有服务中断的风险。代价是失去了回滚选项，你必须决定这是否比热重载更重要。
- en: Your alternative is to consider all ConfigMaps and Secrets as immutable, so
    you include some versioning scheme in the object name and never update a config
    object once it’s created. Instead you create a new config object with a new name
    and release it along with an update to your Deployment, which references the new
    config object.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 你的另一种选择是将所有ConfigMaps和Secrets视为不可变的，因此在对象名称中包含一些版本控制方案，一旦创建配置对象就不再更新。相反，你创建一个新的配置对象并使用新名称发布，同时与更新Deployment一起发布，该Deployment引用新的配置对象。
- en: Try it now Deploy a new version of the app with an immutable config, so you
    can compare the release process.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个：部署一个具有不可变配置的应用程序新版本，这样你可以比较发布流程。
- en: '[PRE7]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Figure 9.8 shows my output, where the config update is accompanied by a Deployment
    update, which preserves the rollout history and enables the rollback.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 显示了我的输出，其中配置更新伴随着部署更新，这保留了滚动发布的历史并启用了回滚。
- en: '![](../Images/9-8.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-8.jpg)'
- en: Figure 9.8 An immutable config preserves rollout history, but it means a rollout
    for every configuration change.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.8 不可变配置保留了滚动发布历史，但这意味着每次配置更改都需要进行滚动发布。
- en: 'Kubernetes doesn’t really care which approach you take, and your choice will
    partly depend on who owns the configuration in your organization. If the project
    team also owns deployment and configuration, then you might prefer mutable config
    objects to simplify the release process and the number of objects to manage. If
    a separate team owns the configuration, then the immutable approach will be better
    because they can deploy new config objects ahead of the release. The scale of
    your apps will affect the decision, too: at a high scale, you may prefer to reduce
    the number of app deployments and rely on mutable configuration.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes实际上并不关心你采取哪种方法，你的选择将部分取决于你组织中谁拥有配置。如果项目团队也拥有部署和配置，那么你可能更喜欢可变配置对象以简化发布过程和管理对象的数量。如果有一个独立的团队负责配置，那么不可变的方法会更好，因为他们可以在发布之前部署新的配置对象。你的应用程序规模也会影响这个决定：在高规模下，你可能更喜欢减少应用程序部署的数量并依赖于可变配置。
- en: There’s a cultural impact to this decision, because it frames how application
    releases are perceived—as everyday events that are no big deal, or as something
    slightly scary that is to be avoided as much as possible. In the container world,
    releases should be trivial events that you’re happy to do with minimal ceremony
    as soon as they’re needed. Testing and tweaking your release strategy will go
    a long way to giving you that confidence.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决策会产生文化影响，因为它决定了人们如何看待应用程序的发布——是日常事件，没什么大不了的，还是稍微有些可怕的事情，应尽可能避免。在容器世界中，发布应该是简单的事件，一旦需要，你就可以以最少的仪式进行。测试和调整你的发布策略将大大增强你的信心。
- en: 9.3 Configuring rolling updates for Deployments
  id: totrans-59
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 配置Deployments的滚动更新
- en: 'Deployments support two update strategies: RollingUpdate is the default and
    the one we’ve used so far, and the other is Recreate. You know how rolling updates
    work—by scaling down the old ReplicaSet while scaling up the new ReplicaSet, which
    provides service continuity and the ability to stagger the update over a longer
    period. The Recreate strategy gives you neither of those. It still uses ReplicaSets
    to implement changes, but it scales down the previous set to zero before scaling
    up the replacement. Listing 9.2 shows the Recreate strategy in a Deployment spec.
    It’s just one setting, but it has a significant impact.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 部署支持两种更新策略：RollingUpdate是默认的，也是我们迄今为止使用的策略，另一种是Recreate。你知道滚动更新的工作原理——通过缩小旧的ReplicaSet同时扩大新的ReplicaSet，这提供了服务连续性和在更长时期内分阶段更新能力。重构策略则不提供这些。它仍然使用ReplicaSets来实现更改，但在扩大替代ReplicaSet之前，它会将前一个集合缩小到零。列表9.2显示了在部署规范中使用重构策略。这只是一个设置，但它有重大影响。
- en: Listing 9.2 vweb-recreate-v2.yaml, a Deployment using the recreate update strategy
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 vweb-recreate-v2.yaml，使用重构更新策略的部署
- en: '[PRE8]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When you deploy this, you’ll see it’s just a normal app with a Deployment, a
    ReplicaSet, and some Pods. If you look at the details of the Deployment, you’ll
    see it uses the Recreate update strategy, but that has an effect only when the
    Deployment is updated.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当你部署它时，你会看到它只是一个普通的带有Deployment、ReplicaSet和一些Pod的应用程序。如果你查看Deployment的详细信息，你会看到它使用重构更新策略，但只有在部署更新时才会有影响。
- en: Try it now Deploy the app from listing 9.2, and explore the objects. This is
    just like a normal Deployment.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 部署列表9.2中的应用程序，并探索对象。这就像一个普通的Deployment。
- en: '[PRE9]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As shown in Figure 9.9, this is a new deployment of the same old web app, using
    version 2 of the container image. There are three Pods, they’re all running, and
    the app works as expected—so far so good.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如图9.9所示，这是一个相同的老式Web应用程序的新部署，使用容器镜像的版本2。有三个Pod，它们都在运行，应用程序按预期工作——到目前为止一切顺利。
- en: '![](../Images/9-9.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-9.jpg)'
- en: Figure 9.9 The Recreate update strategy doesn’t affect behavior until you release
    an update.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.9 重构更新策略在发布更新之前不会影响行为。
- en: This configuration is dangerous, though, and one you should use only if different
    versions of your app can’t coexist—some thing like a database schema update, where
    you need to be sure that only one version of your app connects to the database.
    Even in that case, you have better options, but if you have a scenario that definitely
    needs this approach, then you’d better be sure you test all your updates before
    you go live. If you deploy an update where the new Pods fail, you won’t know that
    until your old Pods have all been terminated, and your app will be completely
    unavailable.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种配置是危险的，你应该只在你的应用程序的不同版本无法共存时使用——例如数据库模式更新，你需要确保只有应用程序的一个版本连接到数据库。即使在那种情况下，你也有更好的选择，但如果你的场景确实需要这种方法，那么在上线之前确保测试所有更新会更好。如果你部署了一个新的
    Pod 失败的更新，你直到所有的旧 Pod 都被终止了才会知道，你的应用程序将完全不可用。
- en: Try it now Version 3 of the web app is ready to deploy. It’s broken, as you’ll
    see when the app goes offline because no Pods are running.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下，Web 应用程序的版本 3 已经准备好部署。它已经损坏，正如你将在应用程序离线时看到的那样，因为没有任何 Pod 在运行。
- en: '[PRE10]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You’ll see in this exercise that Kubernetes happily takes your app offline,
    because that’s what you’ve requested. The Recreate strategy creates a new ReplicaSet
    with the updated Pod template, then scales down the previous ReplicaSet to zero
    and scales up the new ReplicaSet to three. The new image is broken, so the new
    Pods fail, and there’s nothing to respond to requests, as you see in figure 9.10.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 你会在这个练习中看到，Kubernetes 会愉快地将你的应用程序离线，因为这就是你请求的。重新创建策略使用更新的 Pod 模板创建一个新的 ReplicaSet，然后将之前的
    ReplicaSet 缩小到零，并将新的 ReplicaSet 扩展到三个。新的镜像已损坏，所以新的 Pod 失败，没有东西可以响应请求，正如你在图 9.10
    中看到的那样。
- en: '![](../Images/9-10.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片 9-10](../Images/9-10.jpg)'
- en: Figure 9.10 The Recreate strategy happily takes down your app if the new Pod
    spec is broken.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.10 重新创建策略如果新的 Pod 规范损坏，会愉快地关闭你的应用程序。
- en: Now that you’ve seen it, you should probably try to forget about the Recreate
    strategy. In some scenarios, it might seem attractive, but when it does, you should
    still consider alternative options, even if it means looking again at your architecture.
    The wholesale takedown of your application is going to cause downtime, and probably
    more downtime than you plan for.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了，你可能应该尝试忘记重新创建策略。在某些情况下，它可能看起来很有吸引力，但当你这样做时，你仍然应该考虑其他选项，即使这意味着再次审视你的架构。你应用程序的大规模关闭将导致停机时间，而且可能比你计划的停机时间更长。
- en: 'Rolling updates are the default because they guard against downtime, but even
    then, the default behavior is quite aggressive. For a production release, you’ll
    want to tune a few settings that set the speed of the release and how it gets
    monitored. As part of the rolling update spec, you can add options that control
    how quickly the new ReplicaSet is scaled up and how quickly the old ReplicaSet
    is scaled down, using the following two values:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是默认的，因为它们可以防止停机时间，但即使在这种情况下，默认行为也非常激进。对于生产发布，你可能需要调整一些设置，以设置发布的速度和监控方式。作为滚动更新规范的一部分，你可以添加选项来控制新的
    ReplicaSet 的扩展速度和旧的 ReplicaSet 的缩减速度，使用以下两个值：
- en: '`maxUnavailable` is the accelerator for scaling down the old ReplicaSet. It
    defines how many Pods can be unavailable during the update, relative to the desired
    Pod count. You can think of it as the batch size for terminating Pods in the old
    ReplicaSet. In a Deployment of 10 Pods, setting this to 30% means three Pods will
    be terminated immediately.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxUnavailable` 是缩小旧 ReplicaSet 的加速器。它定义了在更新期间相对于期望的 Pod 数量，可以有多少个 Pod 不可用。你可以将其视为在旧
    ReplicaSet 中终止 Pod 的批量大小。在一个包含 10 个 Pod 的 Deployment 中，将此设置为 30% 意味着将有三个 Pod 立即终止。'
- en: '`maxSurge` is the accelerator for scaling up the new ReplicaSet. It defines
    how many extra Pods can exist, over the desired replica count, like the batch
    size for creating Pods in the new ReplicaSet. In a Deployment of 10, setting this
    to 40% will create four new Pods.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`maxSurge` 是扩展新 ReplicaSet 的加速器。它定义了在期望的副本数量之上可以存在多少额外的 Pod，就像在新的 ReplicaSet
    中创建 Pod 的批量大小。在一个包含 10 个 Pod 的 Deployment 中，将此设置为 40% 将创建四个新的 Pod。'
- en: Nice and simple, except both settings are used during a rollout, so you have
    a seesaw effect. The new ReplicaSet is scaled up until the Pod count is the desired
    replica count plus the `maxSurge` value, and then the Deployment waits for old
    Pods to be removed. The old ReplicaSet is scaled down to the desired count minus
    the `maxUnavailable` count, then the Deployment waits for new Pods to reach the
    ready state. You can’t set both values to zero because that means nothing will
    change. Figure 9.11 shows how you can combine the settings to prefer a create-then-remove,
    or a remove-then-create, or a remove-and-create approach to new releases.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 简单明了，但是这两个设置在滚动发布过程中都会使用，因此会产生一种摇摆效应。新的 ReplicaSet 会扩展，直到 Pod 数量达到所需的副本数量加上
    `maxSurge` 值，然后 Deployment 会等待旧 Pods 被移除。旧的 ReplicaSet 会缩小到所需的数量减去 `maxUnavailable`
    数量，然后 Deployment 会等待新 Pods 达到就绪状态。您不能将这两个值都设置为 0，因为这意味着没有任何变化。图 9.11 展示了如何组合这些设置，以优先选择创建后删除、删除后创建或删除并创建的新发布方法。
- en: '![](../Images/9-11.jpg)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.11](../Images/9-11.jpg)'
- en: Figure 9.11 Deployment updates in progress, using different rollout options
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.11 展示了使用不同的发布选项进行中的部署更新。
- en: 'You can tweak these settings for a faster rollout if you have spare compute
    power in your cluster. You can also create additional Pods over your scale setting,
    but that’s riskier if you have a problem with the new release. A slower rollout
    is more conservative: it uses less compute and gives you more opportunity to discover
    any issues, but it reduces the overall capacity of your app during the release.
    Let’s see how these look, first by fixing our broken app with a conservative rollout.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在集群中有额外的计算能力，您可以调整这些设置以加快发布速度。您还可以在您的缩放设置之上创建额外的 Pods，但如果新版本有问题，这会更危险。较慢的发布更保守：它使用的计算资源更少，给您更多机会发现任何问题，但会减少发布期间应用程序的整体容量。让我们先看看这些设置如何，首先通过保守的发布方式修复我们损坏的应用程序。
- en: Try it now Revert back to the working version 2 image, using `maxSurge=1` and
    `maxUnavailable=0` in the RollingUpdate strategy.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下，将镜像回滚到工作版本 2，在滚动更新策略中使用 `maxSurge=1` 和 `maxUnavailable=0`。
- en: '[PRE11]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: In this exercise, the new Deployment spec changed the Pod image back to version
    2, and it also changed the update strategy to a rolling update. You can see in
    figure 9.12 that the strategy change is made first, and then the Pod update is
    made in line with the new strategy, creating one new Pod at a time.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，新的部署规范将 Pod 镜像版本改回 2，并且也将更新策略更改为滚动更新。您可以在图 9.12 中看到，策略更改首先进行，然后按照新的策略进行
    Pod 更新，每次创建一个新 Pod。
- en: '![](../Images/9-12.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图 9.12](../Images/9-12.jpg)'
- en: Figure 9.12 Deployment updates will use an existing ReplicaSet if the Pod template
    matches the new spec.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.12 部署更新将使用与新的规范匹配的现有 ReplicaSet。
- en: 'You’ll need to work fast to see the rollout in progress in the previous exercise,
    because this simple app starts quickly, and as soon as one new Pod is running,
    the rollout continues with another new Pod. You can control the pace of the rollout
    with the following two fields in the Deployment spec:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要快速工作才能在前一个练习中看到发布过程，因为这个简单的应用程序启动很快，一旦一个新 Pod 运行，发布就会继续，另一个新 Pod 开始运行。您可以通过
    Deployment 规范中的以下两个字段来控制发布的速度：
- en: '`minReadySeconds` adds a delay where the Deployment waits to make sure new
    Pods are stable. It specifies the number of seconds the Pod should be up with
    no containers crashing before it’s considered to be successful. The default is
    zero, which is why new Pods are created quickly during rollouts.'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`minReadySeconds` 添加了一个延迟，让 Deployment 等待以确保新 Pods 稳定。它指定了 Pod 在没有容器崩溃的情况下应该运行多少秒才被认为是成功的。默认值为零，这就是为什么在滚动发布期间新
    Pods 会快速创建。'
- en: '`progressDeadlineSeconds` specifies the amount of time a Deployment update
    can run before it’s considered as failing to progress. The default is 600 seconds,
    so if an update is not completed within 10 minutes, it’s flagged as not progressing.'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`progressDeadlineSeconds` 指定了 Deployment 更新在被视为失败之前可以运行的时间。默认值为 600 秒，所以如果一个更新在
    10 分钟内没有完成，它会被标记为没有进展。'
- en: Monitoring how long the release takes sounds useful, but as of Kubernetes 1.19,
    exceeding the deadline doesn’t actually affect the rollout—it just sets a flag
    on the Deployment. Kubernetes doesn’t have an automatic rollback feature for failed
    rollouts, but when that feature does come, it will be triggered by this flag.
    Waiting and checking a Pod for failed containers is a fairly blunt tool, but it’s
    better than having no checks at all, and you should consider having `minReadySeconds`
    specified in all your Deployments.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 监控发布所需的时间听起来很有用，但截至 Kubernetes 1.19，超过截止时间实际上并不会影响发布——它只是在 Deployment 上设置一个标志。Kubernetes
    没有自动回滚失败发布的特性，但当这个特性出现时，它将由这个标志触发。等待并检查 Pod 的失败容器是一个相当直接的工具，但比完全没有检查要好，您应该考虑在所有
    Deployment 中指定 `minReadySeconds`。
- en: These safety measures are useful to add to your Deployment, but they don’t really
    help with our web app because the new Pods always fail. We can make this Deployment
    safe and keep the app online using a rolling update. The next version 3 update
    sets both `maxUnavailable` and `maxSurge` to 1\. Doing so has the same effect
    as the default values (each 25%), but it’s clearer to use exact values in the
    spec, and Pod counts are easier to work with than percentages in small deployments.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些安全措施对于添加到您的部署中很有用，但它们实际上对我们的 Web 应用并没有太大的帮助，因为新的 Pod 总是会失败。我们可以通过滚动更新使这个部署安全并保持应用在线。下一个版本
    3 的更新将 `maxUnavailable` 和 `maxSurge` 都设置为 1。这样做与默认值（每个 25%）的效果相同，但在规范中使用确切值更清晰，并且在小规模部署中，Pod
    数量比百分比更容易处理。
- en: Try it now Deploy the version 3 update again. It will still fail, but by using
    a RollingUpdate strategy, it doesn’t take the app offline.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试部署版本 3 的更新再次。它仍然会失败，但通过使用滚动更新策略，它不会使应用离线。
- en: '[PRE12]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: When you run this exercise, you’ll see the update never completes, and the Deployment
    is stuck with two ReplicaSets having a desired Pod count of two, as shown in figure
    9.13\. The old ReplicaSet won’t scale down any further because the Deployment
    has `maxUnavailable` set to 1; it has already been scaled down by 1 and no new
    Pods will become ready to continue the rollout. The new ReplicaSet won’t scale
    up anymore because `maxSurge` is set to 1, and the total Pod count for the Deployment
    has been reached.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行这个练习时，您会看到更新永远不会完成，Deployment 会卡在两个具有两个所需 Pod 数量的 ReplicaSet 上，如图 9.13 所示。旧的
    ReplicaSet 不会进一步缩放，因为 Deployment 已经将 `maxUnavailable` 设置为 1；它已经缩放了 1，并且没有新的 Pods
    准备就绪以继续发布。新的 ReplicaSet 不会再缩放，因为 `maxSurge` 设置为 1，并且部署的 Pod 总数已经达到。
- en: '![](../Images/9-13.jpg)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/9-13.jpg)'
- en: Figure 9.13 Failed updates don’t automatically roll back or pause; they just
    keep trying.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.13 失败的更新不会自动回滚或暂停；它们只是继续尝试。
- en: If you check back on the new Pods in a few minutes, you’ll see they’re in the
    state `CrashLoopBackoff`. Kubernetes keeps restarting failed Pods by creating
    replacement containers, but it adds a pause between each restart so it doesn’t
    choke the CPU on the node. That pause is the backoff time, and it increases exponentially-10
    seconds for the first restart, then 20 seconds, and then 40 seconds, up to a maximum
    of 5 minutes. These version 3 Pods will never restart successfully, but Kubernetes
    will keep trying.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您过几分钟再检查新的 Pods，您会看到它们处于 `CrashLoopBackoff` 状态。Kubernetes 通过创建替换容器来不断重启失败的
    Pods，但在每次重启之间添加一个暂停，这样就不会使节点的 CPU 过载。这个暂停就是回退时间，它会呈指数增长——第一次重启为 10 秒，然后是 20 秒，然后是
    40 秒，最多可达 5 分钟。这些版本 3 的 Pods 永远不会成功重启，但 Kubernetes 会继续尝试。
- en: Deployments are the controllers you use the most, and it’s worth spending time
    working through the update strategy and timing settings to be sure you understand
    the impact for your apps. DaemonSets and StatefulSets also have rolling update
    functionality, and because they have different ways of managing their Pods, they
    have different approaches to rollouts, too.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Deployments 是您最常用的控制器，花时间研究更新策略和定时设置以确保您理解对您的应用的影响是值得的。DaemonSets 和 StatefulSets
    也具有滚动更新功能，由于它们管理 Pods 的方式不同，因此它们在发布方面也有不同的方法。
- en: 9.4 Rolling updates in DaemonSets and StatefulSets
  id: totrans-100
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 在 DaemonSets 和 StatefulSets 中的滚动更新
- en: DaemonSets and StatefulSets have two update strategies available. The default
    is RollingUpdate, which we’ll explore in this section. The alternative is OnDelete,
    which is for situations when you need close control over when each Pod is updated.
    You deploy the update, and the controller watches Pods, but it doesn’t terminate
    any existing Pods. It waits until they are deleted by another process, and then
    it replaces them with Pods from the new spec.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: DaemonSet和StatefulSet有两种更新策略可供选择。默认是滚动更新，我们将在本节中探讨。另一种是OnDelete，用于需要严格控制每个Pod更新时间的情况。你部署更新，控制器监视Pod，但它不会终止任何现有的Pod。它等待其他进程删除它们，然后使用新规范中的Pod替换它们。
- en: This isn’t quite as pointless as it sounds, when you think about the use cases
    for these controllers. You may have a StatefulSet where each Pod needs to have
    flushed data to disk before it’s removed, and you can have an automated process
    to do that. You may have a DaemonSet where each Pod needs to be disconnected from
    a hardware component, so it’s free for the next Pod to use. These are rare cases,
    but the OnDelete strategy lets you take ownership of when Pods are deleted and
    still have Kubernetes automatically create replacements.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当你考虑这些控制器的用例时，这并不像听起来那么毫无意义。你可能有一个StatefulSet，其中每个Pod在删除之前都需要将数据刷新到磁盘上，你可以有一个自动化的过程来完成这个任务。你可能有一个DaemonSet，其中每个Pod都需要从硬件组件断开连接，以便下一个Pod可以使用。这些是罕见的情况，但OnDelete策略让你可以控制Pod何时被删除，同时仍然让Kubernetes自动创建替换。
- en: We’ll focus on rolling updates in this section, and for that we’ll deploy a
    version of the to-do list app, which runs the database in a StatefulSet, the web
    app in a Deployment, and a reverse proxy for the web app in a DaemonSet.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重点关注滚动更新，为此我们将部署待办事项列表应用的一个版本，该版本在StatefulSet中运行数据库，在Deployment中运行Web应用，并在DaemonSet中运行Web应用的反向代理。
- en: Try it now The to-do app runs across six Pods, so start by clearing the existing
    apps to make room. Then deploy the app, and test that it works correctly.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 待办事项应用运行在六个Pod上，所以首先清除现有的应用以腾出空间。然后部署应用，并测试它是否正确运行。
- en: '[PRE13]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This is just setting us up for the updates. You should now have a working app
    where you can add items and see the list. My output is shown in figure 9.14.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是为了设置更新。现在你应该有一个可以添加项目并查看列表的工作应用。我的输出显示在图9.14中。
- en: '![](../Images/9-14.jpg)'
  id: totrans-107
  prefs: []
  type: TYPE_IMG
  zh: '![图9.14](../Images/9-14.jpg)'
- en: Figure 9.14 Running the to-do app with a gratuitous variety of controllers
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.14 使用各种控制器运行待办事项应用
- en: The first update is for the DaemonSet, where we’ll be rolling out a new version
    of the Nginx proxy image. DaemonSets run a single Pod on all (or some) of the
    nodes in the cluster, and with a rolling update, you have no surge option. During
    the update, nodes will never run two Pods, so this is always a delete-then-remove
    strategy. You can add the `maxUnavailable` setting to control how many nodes are
    updated in parallel, but if you take down multiple Pods, you’ll be running at
    reduced capacity until the replacements are ready.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次更新是针对DaemonSet的，我们将推出Nginx代理镜像的新版本。DaemonSet在集群的所有（或某些）节点上运行单个Pod，并且在使用滚动更新时，没有激增选项。在更新期间，节点永远不会同时运行两个Pod，因此这始终是先删除后删除的策略。你可以添加`maxUnavailable`设置来控制并行更新的节点数量，但如果你关闭多个Pod，你将运行在降低的容量下，直到替换的Pod就绪。
- en: We’ll update the proxy using a `maxUnavailable` setting of 1, and a `minReadySeconds`
    setting of 90\. On a single-node lab cluster, the delay won’t have any effect-there’s
    only one Pod on one node to replace. On a larger cluster, it would mean replacing
    one Pod at a time and waiting 90 seconds for the Pod to prove it’s stable before
    moving on to the next.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用`maxUnavailable`设置为1和`minReadySeconds`设置为90来更新代理。在单节点实验室集群中，延迟不会有任何影响——只有一个Pod在一个节点上需要替换。在更大的集群中，这意味着一次替换一个Pod，并在Pod证明其稳定后等待90秒，然后再进行下一个。
- en: Try it now Start the rolling update of the DaemonSet. On a single-node cluster,
    a short outage will occur while the replacement Pod starts.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 开始DaemonSet的滚动更新。在单节点集群中，在替换Pod启动期间将发生短暂的停机。
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The `watch` flag in kubectl is useful for monitoring changes—it keeps looking
    at an object and prints an update line whenever the state changes. In this exercise
    you’ll see that the old Pod is terminated before the new one is created, which
    means the app has downtime while the new Pod starts up. Figure 9.15 shows I had
    one second of downtime in my release.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl 中的 `watch` 标志用于监控更改——它持续查看一个对象，并在状态改变时打印更新行。在这个练习中，你会看到在创建新 Pod 之前终止了旧
    Pod，这意味着在新的 Pod 启动期间应用程序会有停机时间。图 9.15 显示我在发布过程中有一个秒的停机时间。
- en: '![](../Images/9-15.jpg)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-15.jpg)'
- en: Figure 9.15 DaemonSets update by removing the existing Pod before creating a
    replacement.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.15：在创建替换 Pod 之前删除现有 Pod 以更新 DaemonSets。
- en: A multinode cluster wouldn’t have any downtime because the Service sends traffic
    only to Pods that are ready, and only one Pod at a time gets updated, so the other
    Pods are always available. You will have reduced capacity, though, and if you
    tune a faster rollout with a higher `maxUnavailable` setting, that means a greater
    reduction in capacity as more Pods are updated in parallel. That’s the only setting
    you have for DaemonSets, so it’s a simple choice between manually controlling
    the update by deleting Pods or having Kubernetes roll out the update by a specified
    number of Pods in parallel.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 多节点集群不会有任何停机时间，因为 Service 只将流量发送到已准备好的 Pods，并且每次只更新一个 Pod，所以其他 Pods 总是可用的。不过，你的容量会减少，如果你通过更高的
    `maxUnavailable` 设置加快滚动更新，这意味着随着更多 Pods 并行更新，容量减少得更多。这是你为 DaemonSets 唯一可用的设置，所以这是一个简单的选择：手动通过删除
    Pods 控制更新，或者让 Kubernetes 通过并行指定数量的 Pods 来滚动更新。
- en: StatefulSets are more interesting, although they have only one option to configure
    the rollout. Pods are managed in order by the StatefulSet, which also applies
    to updates—the rollout proceeds backward from the last Pod in the set down to
    the first. That’s especially useful for clustered applications where Pod 0 is
    the primary, because it validates the update on the secondaries first.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 更有趣，尽管它们只有一个选项来配置滚动更新。Pods 由 StatefulSet 按顺序管理，这也适用于更新——滚动更新从集合中的最后一个
    Pod 向前进行到第一个。这对于 Pod 0 是主节点的集群应用程序特别有用，因为它首先在辅助节点上验证更新。
- en: There is no `maxSurge` or `maxUnavailable` setting for StatefulSets. The update
    is always by one Pod at a time. Your configuration option is to define how many
    Pods should be updated in total, using the `partition` setting. This setting defines
    the cut-off point where the rollout stops, and it’s useful for performing a staged
    rollout of a stateful app. If you have five replicas in your set and your spec
    includes partition=3, then only Pod 4 and Pod 3 will be updated; Pods 0, 1, and
    2 are left running the previous spec.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: StatefulSets 没有设置 `maxSurge` 或 `maxUnavailable`。更新总是逐个 Pod 进行。你的配置选项是使用 `partition`
    设置来定义总共应该更新多少个 Pods。此设置定义了滚动更新停止的截止点，这对于执行有阶段性的状态应用程序的滚动更新很有用。如果你在集合中有五个副本，并且你的规范包括
    partition=3，那么只有 Pod 4 和 Pod 3 将被更新；Pod 0、1 和 2 将继续运行之前的规范。
- en: Try it now Deploy a partitioned update to the database image in the StatefulSet,
    which stops after Pod 1, so Pod 0 doesn’t get updated.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试这个：将分区更新部署到 StatefulSet 中的数据库镜像，更新在 Pod 1 后停止，这样 Pod 0 就不会更新。
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: This exercise is a partitioned update that rolls out a new version of the Postgres
    container image, but only to the secondary Pods, which is a single Pod in this
    case, as shown in figure 9.16\. When you use the app in read-only mode, you’ll
    see that it connects to the updated secondary, which still contains the replicated
    data from the previous Pod.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习是一个分区更新，它滚动推出 Postgres 容器镜像的新版本，但只更新到辅助 Pods，在这个例子中是一个 Pod，如图 9.16 所示。当你以只读模式使用应用程序时，你会看到它连接到更新的辅助节点，该节点仍然包含来自上一个
    Pod 的复制数据。
- en: '![](../Images/9-16.jpg)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-16.jpg)'
- en: Figure 9.16 Partitioned updates to StatefulSets let you update secondaries and
    leave the primary unchanged.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.16：对 StatefulSets 进行分区更新，允许更新辅助节点并保持主节点不变。
- en: This rollout is complete, even though the Pods in the set are running from different
    specs. For a data-heavy application in a StatefulSet, you may have a suite of
    verification jobs that you need to run on each updated Pod before you’re happy
    to continue the rollout, and a partitioned update lets you do that. You can manually
    control the pace of the release by running successive updates with decreasing
    partition values, until you remove the partition altogether in the final update
    to finish the set.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 即使该集中的Pod运行的是不同的规范，这次滚动更新也已完成。对于在StatefulSet中的数据密集型应用程序，你可能有一套验证作业需要在每个更新的Pod上运行，你才会满意地继续滚动更新，分区更新允许你这样做。你可以通过运行具有递减分区值的连续更新来手动控制发布的节奏，直到在最终的更新中完全移除分区，以完成集合。
- en: Try it now Deploy the update to the database primary. This spec is the same
    as the previous exercise but with the partition setting removed.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 部署更新到数据库主节点。这个规范与之前的练习相同，但去除了分区设置。
- en: '[PRE16]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: You can see my output in figure 9.17, where the full update has completed and
    the primary is using the same updated version of Postgres as the secondary. If
    you’ve done updates to replicated databases before, you’ll know that this is about
    as simple as it gets—unless you’re using a managed database service, of course.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图9.17中看到我的输出，其中完整更新已完成，主节点正在使用与辅助节点相同的更新版Postgres。如果你之前进行过复制数据库的更新，你会知道这已经非常简单了——当然，除非你使用的是托管数据库服务。
- en: '![](../Images/9-17.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![图9-17](../Images/9-17.jpg)'
- en: Figure 9.17 Completing the StatefulSet rollout, with an update that is not partitioned
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.17 完成有状态集的滚动更新，更新未分区
- en: 'Rolling updates are the default for Deployments, DaemonSets, and StatefulSets,
    and they all work in broadly the same way: gradually replacing Pods running the
    previous application spec with Pods running the new spec. The actual details differ
    because the controllers work in different ways and have different goals, but they
    impose the same requirement on your app: it needs to work correctly when multiple
    versions are live. That’s not always possible, and there are alternative ways
    to deploy app updates in Kubernetes.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动更新是Deployments、DaemonSets和StatefulSets的默认设置，并且它们都以大致相同的方式工作：逐渐用运行新规范的新Pod替换运行旧规范的老Pod。实际的细节不同，因为控制器以不同的方式工作，有不同的目标，但它们对你的应用程序提出了相同的要求：当多个版本同时运行时，应用程序需要正确工作。这并不总是可能的，并且有几种不同的方法可以在Kubernetes中部署应用程序更新。
- en: 9.5 Understanding release strategies
  id: totrans-131
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 理解发布策略
- en: Take the example of a web application. A rolling update is great because it
    lets each Pod close gracefully when all its client requests are dealt with, and
    the rollout can be as fast or as conservative as you like. The practical side
    of the rollout is simple, but you have to consider the user experience (UX) side,
    too.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 以Web应用程序为例。滚动更新很棒，因为它允许每个Pod在处理完所有客户端请求后优雅地关闭，并且滚动更新可以快或保守，取决于你的喜好。滚动更新的实际方面很简单，但你也要考虑用户体验（UX）方面。
- en: Application updates might well change the UX-with a different design, new features,
    or an updated workflow. Any changes like that will be pretty strange for the user
    if they see the new version during a rollout, then refresh and find themselves
    with the old version, because the requests have been served by Pods running different
    versions of the app.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序更新可能会改变UX——不同的设计、新功能或更新的工作流程。如果用户在滚动更新期间看到新版本，然后刷新并发现自己回到了旧版本，因为请求是由运行应用程序不同版本的Pod服务的，那么这样的变化对用户来说会很奇怪。
- en: The strategies to deal with that go beyond the RollingUpdate spec in your controllers.
    You can set cookies in your web app to link a client to a particular UX, and then
    use a more advanced traffic routing system to ensure users keep seeing the new
    version. When we cover that in chapter 15, you’ll see it introduces several more
    moving parts. For cases where that method is too complex or doesn’t solve the
    problem of dual running multiple versions, you can manage the release yourself
    with a blue-green deployment.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这些问题的策略超出了你控制器中的滚动更新规范。你可以在你的Web应用程序中设置cookie，将客户端与特定的UX关联起来，然后使用更高级的流量路由系统来确保用户始终看到新版本。当我们第15章介绍这一点时，你会看到它引入了更多的动态部分。对于这种方法过于复杂或无法解决同时运行多个版本的问题的情况，你可以通过蓝绿部署自行管理发布。
- en: 'Blue-green deployments are a simple concept: you have both the old and new
    versions of your app deployed at the same time, but only one version is active.
    You can flip a switch to choose which version is the active one. In Kubernetes,
    you can do that by updating the label selector in a Service to send traffic to
    the Pods in a different Deployment, as shown in figure 9.18.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署是一个简单的概念：你同时部署了应用程序的旧版本和新版本，但只有一个版本是活跃的。你可以切换一个开关来选择哪个版本是活跃的。在Kubernetes中，你可以通过更新服务的标签选择器来实现，如图9.18所示，将流量发送到不同部署中的Pods。
- en: '![](../Images/9-18.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/9-18.jpg)'
- en: Figure 9.18 You run multiple versions of the app in a blue-green deployment,
    but only one is live.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.18 你在蓝绿部署中运行应用程序的多个版本，但只有一个版本是活跃的。
- en: You need to have the capacity in your cluster to run two complete copies of
    your app. If it’s a web or API component, then the new version should be using
    minimal memory and CPU because it’s not receiving any traffic. You switch between
    versions by updating the label selector for the Service, so the update is practically
    instant because all the Pods are running and ready to receive traffic. You can
    flip back and forth easily, so you can roll back a problem release without waiting
    for ReplicaSets to scale up and down.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 你需要在你的集群中拥有运行应用程序两个完整副本的能力。如果它是Web或API组件，那么新版本应该使用最少的内存和CPU，因为它没有收到任何流量。你通过更新服务的标签选择器在版本之间切换，因此更新几乎是瞬时的，因为所有Pods都在运行并准备好接收流量。你可以轻松地来回切换，因此你可以回滚有问题的发布版本，而无需等待ReplicaSets进行扩展和缩减。
- en: Blue-green deployments are less sophisticated than rolling updates, but they’re
    simpler because of that. They can be a better fit for organizations moving to
    Kubernetes who have a history of big-bang deployments, but they’re a compute-intensive
    approach that requires multiple steps and doesn’t preserve the rollout history
    of your app. You should look to rolling updates as your preferred deployment strategy,
    but blue-green deployments are a good stepping-stone to use while you gain confidence.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 蓝绿部署比滚动更新更复杂，但正因为如此，它们更简单。对于有大规模部署历史记录的组织来说，蓝绿部署可能更适合迁移到Kubernetes，但它们是一种计算密集型方法，需要多个步骤，并且不会保留应用程序的部署历史。你应该将滚动更新作为首选的部署策略，但蓝绿部署在你获得信心时是一个很好的过渡步骤。
- en: That’s all on rolling updates for now, but we will return to the concepts when
    we cover topics in production readiness, network ingress, and monitoring. We just
    need to tidy up the cluster now before going on to the lab.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 目前关于滚动更新的内容就到这里，但当我们讨论到生产准备、网络入口和监控等主题时，我们还会回到这些概念。在我们进入实验室之前，现在我们需要整理一下集群。
- en: Try it now Remove all the objects created for this chapter.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 移除本章创建的所有对象。
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 9.6 Lab
  id: totrans-143
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 实验室
- en: We learned the theory of blue-green deployments in the previous section, and
    now in the lab, you’re going to make it happen. Working through this lab will
    help make it clear how selectors relate Pods to other objects and give you experience
    working with the alternative to rolling updates.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节学习了蓝绿部署的理论，现在在实验室中，你将让它成为现实。完成这个实验室将帮助你清楚地了解选择器如何将Pods与其他对象相关联，并为你提供使用滚动更新的替代方案的经验。
- en: The starting point is version 1 of the web app, which you can deploy from the
    `lab/v1` folder.
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 起始点是Web应用程序的版本1，你可以从`lab/v1`文件夹中部署它。
- en: You need to create a blue-green deployment for version 2 of the app. The spec
    will be similar to the version 1 spec but using the `:v2` container image.
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要为应用程序的版本2创建一个蓝绿部署。规范将与版本1规范类似，但使用`:v2`容器镜像。
- en: When you deploy your update, you should be able to flip between the version
    1 and version 2 release just by changing the Service and without any updates to
    Pods.
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你部署更新时，你应该能够通过更改服务来在版本1和版本2之间切换，而不需要对Pods进行任何更新。
- en: 'This is good practice in copying YAML files and trying to work out which fields
    you need to change. You can find my solution on GitHub: [https://github.com/sixeyed/kiamol/blob/master/ch09/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch09/lab/README.md).'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这是在复制YAML文件并尝试确定你需要更改哪些字段时的良好实践。你可以在GitHub上找到我的解决方案：[https://github.com/sixeyed/kiamol/blob/master/ch09/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch09/lab/README.md)。
