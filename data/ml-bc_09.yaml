- en: 9 Serving models with Kubernetes and Kubeflow
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 使用 Kubernetes 和 Kubeflow 提供模型服务
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding different methods of deploying and serving models in the cloud
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解在云中部署和提供模型的不同方法
- en: Serving Keras and TensorFlow models with TensorFlowServing
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 TensorFlowServing 提供 Keras 和 TensorFlow 模型服务
- en: Deploying TensorFlow Serving to Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 TensorFlow Serving 部署到 Kubernetes
- en: Using Kubeflow and KFServing for simplifying the deployment process
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Kubeflow 和 KFServing 简化部署过程
- en: In the previous chapter, we talked about model deployment with AWS Lambda and
    TensorFlow Lite.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了使用 AWS Lambda 和 TensorFlow Lite 进行模型部署。
- en: 'In this chapter, we discuss the “serverful” approach to model deployment: we
    serve the clothing classification model with TensorFlow Serving on Kubernetes.
    Also, we talk about Kubeflow, an extension for Kubernetes that makes model deployment
    easier.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论了模型部署的“服务器端”方法：我们在 Kubernetes 上使用 TensorFlow Serving 提供服装分类模型。我们还讨论了
    Kubeflow，它是 Kubernetes 的一个扩展，使得模型部署更加容易。
- en: We’re going to cover a lot of material in this chapter, but Kubernetes is so
    complex that it’s simply not possible to go deep into detail. Because of that,
    we often refer to external resources for a more in-depth coverage of some topics.
    But don’t worry; you will learn enough to feel comfortable deploying your own
    models with it.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中涵盖大量内容，但由于 Kubernetes 的复杂性，深入探讨细节是不可能的。因此，我们经常引用外部资源，以更深入地介绍一些主题。但请放心；你将学习到足够多的知识，以便能够舒适地使用它部署自己的模型。
- en: 9.1 Kubernetes and Kubeflow
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 Kubernetes 和 Kubeflow
- en: Kubernetes is a container orchestration platform. It sounds complex, but it’s
    nothing other than a place where we can deploy Docker containers. It takes care
    of exposing these containers as web services and scales these services up and
    down as the amount of requests we receive changes.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 是一个容器编排平台。听起来很复杂，但实际上它只是一个我们可以部署 Docker 容器的场所。它负责将这些容器作为 Web 服务暴露出来，并根据我们接收到的请求数量上下调整这些服务。
- en: Kubernetes is not the easiest tool to learn, but it’s very powerful. It’s likely
    that you will need to use it at some point. That’s why we decided to cover it
    in this book.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 不是最容易学习的工具，但它非常强大。你可能会在某些时候需要使用它。这就是我们决定在本书中介绍它的原因。
- en: Kubeflow is another popular tool built on top of Kubernetes. It makes it easier
    to use Kubernetes to deploy machine learning models. In this chapter, we cover
    both Kubernetes and Kubeflow.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 是建立在 Kubernetes 之上的另一个流行工具。它使得使用 Kubernetes 部署机器学习模型变得更加容易。在本章中，我们将介绍
    Kubernetes 和 Kubeflow。
- en: In the first part, we talk about TensorFlow Serving and plain Kubernetes. We
    discuss how we can use these technologies for model deployment. The plan for the
    first part is
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一部分，我们讨论 TensorFlow Serving 和纯 Kubernetes。我们讨论了如何使用这些技术进行模型部署。第一部分计划如下
- en: First, we convert the Keras model into the special format used by TensorFlow
    Serving.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们将 Keras 模型转换为 TensorFlow Serving 所使用的特殊格式。
- en: Then we use TensorFlow Serving to run the model locally.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们使用 TensorFlow Serving 在本地运行模型。
- en: After that, we create a service for preprocessing images and communicating with
    TensorFlow Serving.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们创建一个服务来预处理图像并与 TensorFlow Serving 通信。
- en: Finally, we deploy both the model and the preprocessing service with Kubernetes.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们使用 Kubernetes 同时部署模型和预处理服务。
- en: Note This chapter doesn’t attempt to cover Kubernetes in depth. We show only
    how to use Kubernetes for deploying models and often refer to more specialized
    sources that cover it in more detail.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章不试图深入探讨 Kubernetes。我们仅展示如何使用 Kubernetes 来部署模型，并经常引用更专业的资源，这些资源更详细地介绍了 Kubernetes。
- en: 'In the second part, we use Kubeflow, a tool on top of Kubernetes that makes
    deployment easier:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二部分，我们使用 Kubeflow，这是 Kubernetes 上的一个工具，使得部署更加容易：
- en: We use the same model we’ve prepared for TensorFlow Serving and deploy it with
    KFServing—the part of Kubeflow that takes care of serving.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用为 TensorFlow Serving 准备的相同模型，并使用 KFServing（Kubeflow 负责提供服务的部分）进行部署。
- en: Then we create a transformer for preprocessing the images and postprocessing
    the predictions.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们创建一个转换器来预处理图像和后处理预测。
- en: The code for this chapter is available in the book’s GitHub repository ([https://github.com/alexeygrigorev/mlbookcamp-code/](https://github.com/alexeygrigorev/mlbookcamp-code/))
    in the chapter-09-kubernetes and chapter-09-kubeflow folders.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可在本书的 GitHub 仓库中找到（[https://github.com/alexeygrigorev/mlbookcamp-code/](https://github.com/alexeygrigorev/mlbookcamp-code/)），位于
    chapter-09-kubernetes 和 chapter-09-kubeflow 文件夹中。
- en: Let’s get started!
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 9.2 Serving models with TensorFlow Serving
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 使用 TensorFlow Serving 提供模型服务
- en: In chapter 7, we used Keras for predicting the classes of images. In chapter
    8, we converted the model to TF Lite and used it for making predictions from AWS
    Lambda. In this chapter, we do this with TensorFlow Serving.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在第7章中，我们使用Keras预测图像的类别。在第8章中，我们将模型转换为TF Lite，并使用AWS Lambda进行预测。在本章中，我们将使用TensorFlow
    Serving来完成这项工作。
- en: TensorFlow Serving, usually abbreviated as “TF Serving,” is a system designed
    for serving TensorFlow models. Unlike TF Lite, which is made for mobile devices,
    TF Serving focuses on servers. Often, the servers have GPUs, and TF Serving knows
    how to make use of them.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving，通常简称为“TF Serving”，是一个专为提供TensorFlow模型而设计的系统。与专为移动设备制作的TF
    Lite不同，TF Serving专注于服务器。通常，服务器具有GPU，TF Serving知道如何利用它们。
- en: AWS Lambda is great for experimenting and for dealing with small amounts of
    images—fewer than one million per day. But when we grow past that amount and get
    more images, AWS Lambda becomes expensive. Then deploying models with Kubernetes
    and TF Serving is a better option.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Lambda非常适合实验和应对少量图像——每天少于一百万张。但是，当我们超过这个数量并获得更多图像时，AWS Lambda变得昂贵。然后，使用Kubernetes和TF
    Serving部署模型是一个更好的选择。
- en: Just using TF Serving for deploying models is not sufficient, however. We also
    need another service for preparing the images. Next, we’ll discuss the architecture
    of the system that we will build.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 仅使用TF Serving部署模型是不够的。我们还需要另一个服务来准备图像。接下来，我们将讨论我们将构建的系统架构。
- en: 9.2.1 Overview of the serving architecture
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 服务架构概述
- en: 'TF Serving focuses on only one thing—serving the model. It expects that the
    data it receives is already prepared: images are resized, preprocessed, and sent
    in the right format.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: TF Serving专注于唯一的一件事——提供模型服务。它期望接收到的数据已经准备好：图像已调整大小、预处理并发送为正确的格式。
- en: This is why simply putting the model into TF Serving is not enough. We need
    an additional service that takes care of preprocessing the data.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么仅仅将模型放入TF Serving还不够。我们需要一个额外的服务来处理数据预处理。
- en: 'We need two components for a system for serving a deep learning model (figure 9.1):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个系统来提供深度学习模型的服务（如图9.1所示）：
- en: 'Gateway: The preprocessing part. It gets the URL for which we need to make
    the prediction, prepares it, and sends it further to the model. We will use Flask
    for creating this service.'
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网关：预处理部分。它获取我们需要进行预测的URL，准备它，并将其发送到模型。我们将使用Flask来创建这个服务。
- en: 'Model: The part with the actual model. We will use TF Serving for this.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型：实际模型的那个部分。我们将使用TF Serving来完成这项工作。
- en: '![](../Images/09-01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09-01.png)'
- en: Figure 9.1 The two-tier architecture of our system. The gateway gets the user
    requests and prepares the data, and TF Serving uses the data to make the predictions.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 我们系统的双层架构。网关获取用户请求并准备数据，TF Serving使用这些数据来进行预测。
- en: Creating a system with two components instead of one may seem like an unnecessary
    complication. In the previous chapter, we didn’t need to do it. We had only one
    part—the lambda function.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 与单一组件相比，使用两个组件的系统可能看起来是一个不必要的复杂性。在前一章中，我们不需要这样做。我们只有一个部分——lambda函数。
- en: In principle, we could take the code from that lambda function, put it in Flask,
    and use it for serving the model. This approach will indeed work, but it won’t
    be the most effective one. If we need to process millions of images, being able
    to properly utilize the resources is important.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在原则上，我们可以从那个lambda函数中提取代码，将其放入Flask中，并用于提供模型服务。这种方法确实可行，但可能不是最有效的方法。如果我们需要处理数百万张图像，合理利用资源是很重要的。
- en: 'Having two separate components instead of a single one makes it easier to select
    the right resource for each part:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 与单一组件相比，有两个独立的组件使得为每个部分选择正确的资源变得更容易：
- en: The gateway spends a lot of time downloading the images in addition to doing
    preprocessing. It doesn’t need a powerful computer for that.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了预处理之外，网关还花费大量时间下载图像。它不需要强大的计算机来完成这项工作。
- en: The TF Serving component requires a more powerful machine, often with a GPU.
    It would be wasteful to use this powerful machine for downloading images.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF Serving组件需要更强大的机器，通常带有GPU。使用这样强大的机器来下载图像将是浪费。
- en: We might require many gateway instances and only a few TF Serving instances.
    By separating them into different components, we can scale each independently.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能需要许多网关实例，而只需要少数几个TF Serving实例。通过将它们分离成不同的组件，我们可以独立地扩展每个组件。
- en: We will start with the second component—TF Serving.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从第二个组件——TF Serving开始。
- en: In chapter 7, we trained a Keras model. To use it with TF Serving, we need to
    convert it to the special format used by TF Serving, which is called *saved_model*.
    We do this next.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 7 章，我们训练了一个 Keras 模型。为了使用 TF Serving，我们需要将其转换为 TF Serving 使用的特殊格式，这被称为 *saved_model*。我们将在下一步进行此操作。
- en: 9.2.2 The saved_model format
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 保存模型格式
- en: 'The Keras model we trained previously was saved in the h5 format. TF Serving
    cannot read h5: it expects models to be in the saved_model format. In this section,
    we convert the h5 model to a saved_model file.'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前训练的 Keras 模型以 h5 格式保存。TF Serving 无法读取 h5 格式：它期望模型以 saved_model 格式存在。在本节中，我们将
    h5 模型转换为 saved_model 文件。
- en: 'In case you don’t have the model from chapter 7, you can download it with `wget`:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您没有第 7 章中的模型，可以使用 `wget` 下载它：
- en: '[PRE0]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Now let’s convert it. We can do this from a Jupyter Notebook or from a Python
    script.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们进行转换。我们可以从 Jupyter Notebook 或 Python 脚本中执行此操作。
- en: 'In either case, we start with imports:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何情况下，我们首先进行导入：
- en: '[PRE1]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Then load the model:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 然后加载模型：
- en: '[PRE2]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'And, finally, save it in the saved_model format:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以 saved_model 格式保存它：
- en: '[PRE3]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: That’s it. After running this code, we have the model saved in the clothing-model
    folder.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样。运行此代码后，我们在 clothing-model 文件夹中保存了模型。
- en: 'To be able to use this model later, we need to know a few things:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够稍后使用此模型，我们需要了解一些信息：
- en: 'The name of the model signature. The model signature describes the model’s
    inputs and outputs. You can read more about model signatures here: [https://www.tensorflow.org/tfx/serving/signature_defs.](https://www.tensorflow.org/tfx/serving/signature_defs)'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型签名名称。模型签名描述了模型的输入和输出。您可以在此处了解更多关于模型签名的信息：[https://www.tensorflow.org/tfx/serving/signature_defs.](https://www.tensorflow.org/tfx/serving/signature_defs)
- en: The name of the input layer.
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入层的名称。
- en: The name of the output layer.
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出层的名称。
- en: When using Keras, we didn’t need to worry about it, but TF Serving requires
    us to have this information.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用 Keras 时，我们不需要担心它，但 TF Serving 要求我们必须有这些信息。
- en: 'TensorFlow comes with a special utility for analyzing the models in the saved_
    model format—`saved_model_cli`. We don’t need to install anything extra. We will
    use the `show` command from this utility:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 内置了一个用于分析 saved_model 格式模型的特殊实用工具——`saved_model_cli`。我们不需要安装任何额外的软件。我们将使用此实用工具的
    `show` 命令：
- en: '[PRE4]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Let’s take a look at the output:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看输出：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The signature definition—serving_default
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 签名定义—serving_default
- en: ❷ The input name—input_8
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输入名称—input_8
- en: ❸ The output name—dense_7
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 输出名称—dense_7
- en: 'In this output, we’re interested in three things:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出中，我们关注三件事：
- en: The signature definition (signature_def) of the model. In this case, it’s `serving_
    default`.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的签名定义（signature_def）。在这种情况下，它是 `serving_default`。
- en: The input (`input_8`). The name of the input of the model.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入 (`input_8`)。这是模型的输入名称。
- en: The output (`dense_7`). tThe name of the output layer of the model.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出 (`dense_7`)。这是模型输出层的名称。
- en: Note Take note of these names—we’ll need them later when invoking this model.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：请注意这些名称——我们稍后调用此模型时需要用到它们。
- en: The model is converted, and now we’re ready to serve it with TF Serving.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已转换，现在我们准备好使用 TF Serving 提供服务。
- en: 9.2.3 Running TensorFlow Serving locally
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 在本地运行 TensorFlow Serving
- en: 'One of the easiest ways of running TF Serving locally is to use Docker. You
    can read more about it in the official documentation: [https://www.tensorflow.org/tfx/serving/docker.](https://www.tensorflow.org/tfx/serving/docker)
    Refer to chapter 5 for more information about Docker.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地运行 TF Serving 最简单的方法之一是使用 Docker。您可以在官方文档中了解更多信息：[https://www.tensorflow.org/tfx/serving/docker.](https://www.tensorflow.org/tfx/serving/docker)
    更多关于 Docker 的信息请参阅第 5 章。
- en: 'All we need to do is invoke the `docker` `run` command specifying the path
    to the model and its name:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的只是调用 `docker` `run` 命令，指定模型路径及其名称：
- en: '[PRE6]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Opens port 8500 for serving
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 打开端口 8500 以提供服务
- en: ❷ Mounts our model
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 挂载我们的模型
- en: ❸ Specifies the name of the model
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定模型名称
- en: ❹ Uses the TensorFlow Serving image, version 2.3.0
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用 TensorFlow Serving 镜像，版本 2.3.0
- en: 'When running it, we use three parameters:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行时，我们使用三个参数：
- en: '`-p`: To map port 8500 on the host machine (the computer where we run Docker)
    to port 8500 inside the container ❶.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-p`: 将主机机器（我们运行 Docker 的计算机）上的端口 8500 映射到容器内部的端口 8500 ❶。'
- en: '`-v`: To put the model files inside the Docker image ❷. The model is put in
    /models/clothing-model/1, where clothing-model is the name of the model and 1
    is the version.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-v`: 将模型文件放入 Docker 镜像内部 ❷。模型被放置在 /models/clothing-model/1，其中 clothing-model
    是模型的名称，1 是版本号。'
- en: '`-e`: To set the `MODEL_NAME` variable to c`lothing-model` ❸, which is the
    directory name from ❷.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`-e`: 将 `MODEL_NAME` 变量设置为 `clothing-model` ❸，这是 ❷ 中的目录名称。'
- en: To learn more about the `docker` `run` command, refer to the official Docker
    documentation ([https://docs.docker.com/engine/reference/run/](https://docs.docker.com/engine/reference/run/)).
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于 `docker` `run` 命令的信息，请参阅官方 Docker 文档 ([https://docs.docker.com/engine/reference/run/](https://docs.docker.com/engine/reference/run/))。
- en: 'After running this command, we should see logs in the terminal:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令后，我们应该在终端中看到日志：
- en: '[PRE7]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The `Entering` `the` `event` `loop` message tells us that TF Serving has started
    successfully and is ready to receive requests.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`Entering` `the` `event` `loop` 消息告诉我们 TF Serving 已成功启动并准备好接收请求。'
- en: But we cannot use it yet. To prepare a request, we need to load an image, preprocess
    it, and convert it to a special binary format. Next, we’ll see how we can do it.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们目前还不能使用它。为了准备一个请求，我们需要加载一个图像，对其进行预处理，并将其转换为特殊的二进制格式。接下来，我们将看到我们如何做到这一点。
- en: 9.2.4 Invoking the TF Serving model from Jupyter
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 从 Jupyter 调用 TF Serving 模型
- en: For communication, TF Serving uses gRPC—a special protocol designed for high-performance
    communication. This protocol relies on protobuf, a format for effective data transfer.
    Unlike JSON, it’s binary, which makes the requests significantly more compact.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于通信，TF Serving 使用 gRPC——一种专为高性能通信设计的特殊协议。此协议依赖于 protobuf，这是一种有效的数据传输格式。与 JSON
    不同，它是二进制的，这使得请求显著更紧凑。
- en: To understand how to use it, let’s first experiment with these technologies
    from a Jupyter Notebook. We connect to our model deployed with TF Serving using
    gRPC and protobuf. After that, we can put this code into a Flask application in
    the next section.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解如何使用它，让我们首先从 Jupyter Notebook 中对这些技术进行实验。我们使用 gRPC 和 protobuf 连接到使用 TF Serving
    部署的模型。之后，我们可以在下一节将此代码放入 Flask 应用程序中。
- en: 'Let’s start. We need to install a couple of libraries:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始。我们需要安装几个库：
- en: 'grpcio: For gRPC support in Python'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grpcio：用于 Python 中的 gRPC 支持
- en: 'tensorflow-serving-api: For using TF Serving from Python'
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: tensorflow-serving-api：用于从 Python 使用 TF Serving
- en: 'Install them with `pip`:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `pip` 安装它们：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We also need the keras_image_helper library for preprocessing the images. We
    already used this library in chapter 8\. If you haven’t installed it yet, use
    `pip` for that:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要 keras_image_helper 库来预处理图像。我们已经在第 8 章中使用了这个库。如果您还没有安装它，请使用 `pip` 进行安装：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Next, create a Jupyter Notebook. We can call it chapter-09-image-preparation.
    As usual, we start with imports:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建一个 Jupyter Notebook。我们可以称它为 chapter-09-image-preparation。像往常一样，我们从导入开始：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'We imported three things:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了三样东西：
- en: 'gRPC: for communicating with TF Serving'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gRPC：用于与 TF Serving 通信
- en: 'TensorFlow: for protobuf definitions (We’ll see later how it’s used.)'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow：用于 protobuf 定义（我们稍后会看到它是如何使用的。）
- en: A couple of functions from TensorFlow Serving
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 来自 TensorFlow Serving 的几个函数
- en: 'Now we need to define the connection to our service:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要定义到我们服务的连接：
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note We use an insecure channel—a channel that requires no authentication. All
    the communication between the services in this chapter happens inside the same
    network. This network is closed to the outside world, so using an insecure channel
    does not cause any security vulnerabilities. Setting a secure channel is possible
    but outside the scope of this book.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我们使用了一个不安全的通道——一个不需要认证的通道。本章中所有服务之间的通信都在同一网络内部进行。这个网络对外界是关闭的，因此使用不安全的通道不会造成任何安全漏洞。设置安全通道是可能的，但超出了本书的范围。
- en: 'For preprocessing the images, we use the keras_image_helper library, like previously:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像预处理，我们使用 keras_image_helper 库，就像之前一样：
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Let’s use the same image of pants we used in chapter 8 (figure 9.2).
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用与第 8 章中（图 9.2）相同的裤子图片。
- en: '![](../Images/09-02.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/09-02.png)'
- en: Figure 9.2 The picture of pants that we use for testing
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 我们用于测试的裤子图片
- en: 'Let’s convert it to a NumPy array:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其转换为 NumPy 数组：
- en: '[PRE13]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'We have a NumPy array in `X`, but we can’t use it as is. For gRPC, we need
    to convert it to protobuf. TensorFlow has a special function for that: `tf.make_tensor_proto`.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在 `X` 中有一个 NumPy 数组，但我们不能直接使用它。对于 gRPC，我们需要将其转换为 protobuf。TensorFlow 有一个专门用于此的功能：`tf.make_tensor_proto`。
- en: 'This is how we use it:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是我们的使用方法：
- en: '[PRE14]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'This function takes two arguments:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数接受两个参数：
- en: 'A NumPy array: `data`,'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个 NumPy 数组：`data`，
- en: 'The dimensions of this array: `data.shape`'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此数组的维度：`data.shape`
- en: 'Note In this example, we use TensorFlow for converting a NumPy array to protobuf.
    TensorFlow is a large library, so depending on it for just one small function
    is unreasonable. In this chapter, we do it for simplicity, but you shouldn’t do
    it in production, because using Docker with big images can create problems: it
    takes more time to download the image and they occupy more space. Check this repository
    to see what you can do instead: [https://github.com/alexeygrigorev/tensorflow-protobuf](https://github.com/alexeygrigorev/tensorflow-protobuf).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这个例子中，我们使用 TensorFlow 将 NumPy 数组转换为 protobuf。TensorFlow 是一个庞大的库，所以仅仅为了一个小的函数而依赖它是不可取的。在本章中，我们这样做是为了简单起见，但在生产环境中你不应该这样做，因为使用带有大镜像的
    Docker 可能会引发问题：下载镜像需要更多时间，并且它们会占用更多空间。查看此存储库以了解你可以做什么：[https://github.com/alexeygrigorev/tensorflow-protobuf](https://github.com/alexeygrigorev/tensorflow-protobuf)。
- en: 'Now we can use the `np_to_protobuf` function to prepare a gRPC request:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用 `np_to_protobuf` 函数来准备一个 gRPC 请求：
- en: '[PRE15]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ If there is one item, bringing in the one item
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果有一个项目，引入该项目
- en: ❷ Sets the model name to clothing-model
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置模型名称为 clothing-model
- en: '❸ Specifies the signature name: serving_default'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 指定签名名称：serving_default
- en: ❹ Converts X to protobuf, and assigns it to input_8
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将 X 转换为 protobuf，并将其分配给 input_8
- en: Let’s take a look at each line. First, in ❶, we create a request object. TF
    Serving uses the information from this object to determine how to process the
    request.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐行查看。首先，在 ❶ 中，我们创建了一个请求对象。TF Serving 使用这个对象中的信息来确定如何处理请求。
- en: In ❷, we specify the name of the model. Recall that when running TF Serving
    in Docker, we specified the `MODEL_NAME` parameter—we set it to `clothing-model`.
    Here we say that we want to send the request to that model.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❷ 中，我们指定了模型的名称。回想一下，当在 Docker 中运行 TF Serving 时，我们指定了 `MODEL_NAME` 参数——我们将其设置为
    `clothing-model`。在这里，我们表示我们想要将请求发送到该模型。
- en: In ❸, we specify which signature we want to query. When we analyzed the saved_
    model file, the signature name was `serving_default`, so this is what we use here.
    You can read more about signatures in the official TF Serving documentation ([https://www
    .tensorflow.org/tfx/serving/signature_defs](https://www.tensorflow.org/tfx/serving/signature_defs)).
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❸ 中，我们指定了想要查询的签名。当我们分析 saved_model 文件时，签名名称是 `serving_default`，所以我们在这里使用它。你可以在官方
    TF Serving 文档中了解更多关于签名的信息（[https://www.tensorflow.org/tfx/serving/signature_defs](https://www.tensorflow.org/tfx/serving/signature_defs)）。
- en: In ❹, we do two things. First, we convert `X` to protobuf. Then, we set the
    results to the input named `input_8`. This name also comes from our analysis of
    the saved_model file.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❹ 中，我们做了两件事。首先，我们将 `X` 转换为 protobuf。然后，我们将结果设置到名为 `input_8` 的输入中。这个名称也来源于我们对
    saved_model 文件的分析。
- en: 'Let’s execute it:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们执行它：
- en: '[PRE16]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This sends a request to the TF Serving instance. Then TF Serving applies the
    model to the request and sends back the results. The results are saved to the
    `pb_result` variable. To get the predictions from there, we need to access one
    of the outputs:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这向 TF Serving 实例发送了一个请求。然后 TF Serving 将模型应用于请求，并将结果发送回来。结果被保存到 `pb_result` 变量中。要从那里获取预测，我们需要访问其中一个输出：
- en: '[PRE17]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Note that we need to refer to a specific output by name—`dense_7`. When analyzing
    the signatures of the saved_model file, we also took a note of it—and now used
    it to get the predictions.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们需要通过名称引用特定的输出——`dense_7`。在分析 saved_model 文件的签名时，我们也注意到了它——现在我们用它来获取预测结果。
- en: 'The `pred` variable is a list of floats—the predictions:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`pred` 变量是一个浮点数列表——预测结果：'
- en: '[PRE18]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'We need to turn this list of numbers into something that we can understand—we
    need to connect it to the labels. We use the same approach as in the previous
    chapters:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要将这些数字列表转换为我们可以理解的东西——我们需要将其与标签连接起来。我们使用与之前章节相同的方法：
- en: '[PRE19]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'This gives us the final result:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 这给出了最终结果：
- en: '[PRE20]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: We see that the `pants` label has the highest score.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到 `pants` 标签具有最高的分数。
- en: We successfully managed to connect to the TF Serving instance from a Jupyter
    Notebook, and we used gRPC and protobuf for that. Now let’s put this code into
    a web service.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们成功从 Jupyter Notebook 连接到 TF Serving 实例，并使用了 gRPC 和 protobuf 来实现这一点。现在让我们把这个代码放入一个网络服务中。
- en: 9.2.5 Creating the Gateway service
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.5 创建网关服务
- en: We already have all the code we need for communicating with our model deployed
    with TF Serving.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了与使用 TF Serving 部署的模型通信所需的所有代码。
- en: This code, however, is not convenient to use. The users of our model shouldn’t
    need to worry about downloading the image, doing the preprocessing, converting
    it to protobuf, and all other things we did. They should be able to send an URL
    of an image and get back the predictions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这段代码并不方便使用。我们模型的用户不需要担心下载图像、进行预处理、将其转换为protobuf以及我们做的所有其他事情。他们应该能够发送图像的URL并获取预测结果。
- en: To make it easier for our users, we’ll put all this code into a web service.
    The users will interact with the service, and the service will talk to TF Serving.
    So, the service will act as a gateway to our model. This is why we can simply
    call it “Gateway” (figure 9.3).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让我们的用户更容易使用，我们将所有这些代码放入一个网络服务中。用户将与服务交互，服务将与TF Serving通信。因此，服务将作为我们模型的网关。这就是为什么我们可以简单地称之为“网关”（图9.3）。
- en: '![](../Images/09-03.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-03.png)'
- en: Figure 9.3 The Gateway service is a Flask app that gets an URL to the image
    and prepares it. Then it uses gRPC and protobuf to communicate with TF Serving.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 网关服务是一个Flask应用，它获取图像的URL并准备它。然后它使用gRPC和protobuf与TF Serving进行通信。
- en: We use Flask to create this service. We already used Flask previously; you can
    refer to chapter 5 for more details.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Flask来创建这个服务。我们之前已经使用过Flask；你可以参考第5章以获取更多详细信息。
- en: 'The Gateway service needs to do these things:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 网关服务需要做这些事情：
- en: Take the URL of an image in the request.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从请求中获取图像的URL。
- en: Download the image, preprocess it, and convert it to a NumPy array.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 下载图像，预处理它，并将其转换为NumPy数组。
- en: Convert the NumPy array to protobuf, and use gRPC to communicate with TF Serving.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将NumPy数组转换为protobuf，并使用gRPC与TF Serving通信。
- en: Postprocess the results—convert the raw list with numbers to human-understandable
    form.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 后处理结果——将包含数字的原始列表转换为人类可理解的形式。
- en: So, let’s create it! Start by creating a file model_server.py—we’ll put all
    this logic there.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们创建它！首先创建一个名为model_server.py的文件——我们将所有这些逻辑放在那里。
- en: 'First, we get the same imports as we have in the notebook:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们获取与笔记本中相同的导入：
- en: '[PRE21]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Now we need to add Flask imports:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要添加Flask导入：
- en: '[PRE22]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Next, create the connection gRPC stub:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建gRPC stub连接：
- en: '[PRE23]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Makes the TF Serving URL configurable
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使TF Serving URL可配置
- en: Instead of simply hardcoding the URL of the TF Serving instance, we make it
    configurable via the environment variable `TF_SERVING_HOST`. If the variable is
    not set, we use the default value `'localhost:8500'`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是简单地硬编码TF Serving实例的URL，我们通过环境变量`TF_SERVING_HOST`使其可配置。如果没有设置变量，我们使用默认值`'localhost:8500'`。
- en: 'Now let’s create the preprocessor:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建预处理程序：
- en: '[PRE24]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Also, we need to define the names of our classes:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们还需要定义我们类的名称：
- en: '[PRE25]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Instead of simply copying and pasting the code from the notebook, we can make
    the code more organized and put it into two functions:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是简单地从笔记本中复制粘贴代码，我们可以使代码更有组织，并将其放入两个函数中：
- en: '`make_request`: For creating a gRPC request from a NumPy array'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`make_request`：用于从NumPy数组创建gRPC请求'
- en: '`process_response`: For attaching the class labels to the predictions'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`process_response`：用于将类标签附加到预测中'
- en: 'Let’s start with `make_request`:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`make_request`开始：
- en: '[PRE26]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Next, create process_response:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，创建process_response：
- en: '[PRE27]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'And finally, let’s put everything together:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们将所有这些放在一起：
- en: '[PRE28]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Preprocesses an image from the provided URL
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从提供的URL预处理图像
- en: ❷ Converts the NumPy array into a gRPC request
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将NumPy数组转换为gRPC请求
- en: ❸ Executes the request
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行请求
- en: ❹ Processes the response, and attaches the labels to predictions
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 处理响应，并将标签附加到预测中
- en: 'All the code is ready. We only need to do one last thing: create a Flask app
    and the `predict` function. Let’s do it:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 所有代码都已准备就绪。我们只需要做最后一件事：创建一个Flask应用和`predict`函数。让我们来做：
- en: '[PRE29]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Now we’re ready to run the service. Execute this command in the terminal:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好运行服务。在终端中执行此命令：
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'Wait until it''s ready. We should see the following in the terminal:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 等待它准备好。我们应该在终端中看到以下内容：
- en: '[PRE31]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Let’s test it! Like in chapter 5, we use the requests library for that. You
    can open any Jupyter Notebook. For example, you can continue in the same notebook
    where we experimented with connecting to TF Serving with gRPC.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来测试一下！就像在第5章中一样，我们使用requests库来做这个。你可以打开任何Jupyter Notebook。例如，你可以在我们用gRPC连接到TF
    Serving的同一个笔记本中继续。
- en: 'We need to send a request with a URL and show the response. This is how we
    do it with requests:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要发送一个带有URL的请求并显示响应。这是使用requests库来完成的方式：
- en: '[PRE32]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Here, we send a POST request to our service and display the results. The response
    is the same as previously:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们向我们的服务发送一个POST请求并显示结果。响应与之前相同：
- en: '[PRE33]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The service is ready and it works locally. Let’s deploy it with Kubernetes!
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 服务已就绪并且本地工作正常。让我们使用Kubernetes来部署它！
- en: 9.3 Model deployment with Kubernetes
  id: totrans-198
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 使用Kubernetes进行模型部署
- en: Kubernetes is an orchestration system for automating container deployment. We
    can use it to host any Docker containers. In this section, we’ll see how we can
    use Kubernetes to deploy our application.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes是一个用于自动化容器部署的编排系统。我们可以用它来托管任何Docker容器。在本节中，我们将看到如何使用Kubernetes来部署我们的应用程序。
- en: First, we’ll start by going over some Kubernetes basics.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将从介绍一些Kubernetes基础知识开始。
- en: 9.3.1 Introduction to Kubernetes
  id: totrans-201
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 Kubernetes简介
- en: The main unit of abstraction in Kubernetes is a *pod*. A pod contains a single
    Docker image, and when we want to serve something, pods do the actual job.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes中的主要抽象单元是*Pod*。Pod包含一个单一的Docker镜像，当我们想要提供服务时，Pod执行实际的工作。
- en: Pods live on a *node* —this is an actual machine. A node usually contains one
    or more pods.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Pods存在于一个*节点*上——这是一个实际的机器。一个节点通常包含一个或多个Pod。
- en: To deploy an application, we define a *deployment*. We specify how many pods
    the application should have and which image should be used. When our application
    starts to get more requests, sometimes we want to add more pods to our deployment
    to handle the increase in traffic. This can also happen automatically—this process
    is called *horizontal autoscaling*.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 部署一个应用程序，我们定义一个*部署*。我们指定应用程序应该有多少个Pod以及应该使用哪个镜像。当我们的应用程序开始接收到更多请求时，有时我们想要向我们的部署中添加更多Pod来处理流量的增加。这也可以自动发生——这个过程被称为*水平自动扩展*。
- en: A *service* is the entry point to the pods in a deployment. The clients interact
    with the service, not individual pods. When a service gets a request, it routes
    it to one of the pods in the deployment.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '*服务*是部署中Pod的入口点。客户端与服务交互，而不是与单个Pod交互。当服务收到请求时，它会将其路由到部署中的一个Pod。'
- en: Clients outside of the Kubernetes cluster interact with the services inside
    the cluster through *ingress*.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes集群外部的客户端通过*入口*与集群内部的服务交互。
- en: Suppose we have a service—Gateway. For this service, we have a deployment (Gateway
    Deployment) with three pods—pod A, pod B on Node 1, and pod D on Node 2 (figure 9.4).
    When a client wants to send a request to the service, it’s first processed by
    the ingress, and then the service routes the request to one of the pods. In this
    example, it’s pod A deployed on node 1\. The service on pod A processes the request,
    and the client receives the response.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个名为Gateway的服务。对于这个服务，我们有一个包含三个Pod的部署（Gateway Deployment）——Pod A、Node 1上的Pod
    B和Node 2上的Pod D（图9.4）。当客户端想要向服务发送请求时，它首先由入口处理，然后服务将请求路由到部署中的一个Pod。在这个例子中，是部署在节点1上的Pod
    A。Pod A上的服务处理请求，客户端接收响应。
- en: '![](../Images/09-04.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-04.png)'
- en: Figure 9.4 The anatomy of a Kubernetes cluster. Pods are instances of our application.
    They live on nodes—the actual machines. Pods that belong to the same application
    are grouped in a deployment. The client communicates to services, and the services
    route the request to one of the pods in a deployment.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4 Kubernetes集群的解剖结构。Pod是我们应用程序的实例。它们存在于节点上——实际的机器。属于同一应用程序的Pod被分组在一个部署中。客户端与服务通信，服务将请求路由到部署中的一个Pod。
- en: This is a very short introduction to the key vocabulary of Kubernetes, but it
    should be sufficient to get started. To learn more about Kubernetes, refer to
    the official documentation ([https://kubernetes.io/](https://kubernetes.io/)).
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 这是对Kubernetes关键词汇的非常简短的介绍，但应该足以开始。要了解更多关于Kubernetes的信息，请参阅官方文档([https://kubernetes.io/](https://kubernetes.io/))。
- en: In the next section, we’ll see how we can create our own Kubernetes cluster
    on AWS.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何在AWS上创建我们自己的Kubernetes集群。
- en: 9.3.2 Creating a Kubernetes cluster on AWS
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 在AWS上创建Kubernetes集群
- en: 'To be able to deploy our services to a Kubernetes cluster, we need to have
    one. We have multiple options:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 要能够将我们的服务部署到Kubernetes集群，我们需要有一个。我们有多个选项：
- en: You can create a cluster in the cloud. All the major cloud providers make it
    possible to set up a Kubernetes cluster in the cloud.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在云中创建一个集群。所有主要的云服务提供商都使得在云中设置Kubernetes集群成为可能。
- en: 'You can set it up locally using Minikube or MicroK8S. You can read more about
    it here: [https://mlbookcamp.com/article/local-k8s.html](https://mlbookcamp.com/article/local-k8s.html).'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以使用Minikube或MicroK8S在本地设置它。你可以在这里了解更多信息：[https://mlbookcamp.com/article/local-k8s.html](https://mlbookcamp.com/article/local-k8s.html)。
- en: In this section, we use EKS from AWS. EKS, which stands for Elastic Kubernetes
    Service, is a service from AWS that lets us create a Kubernetes cluster with a
    minimal amount of effort. Alternatives are GKE (Google Kubernetes Engine) from
    Google Cloud and AKS (Azure Kubernetes Service) from Azure.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们使用 AWS 的 EKS。EKS 代表 Elastic Kubernetes Service，是 AWS 提供的一项服务，允许我们以最小的努力创建一个
    Kubernetes 集群。其他选择包括 Google Cloud 的 GKE（Google Kubernetes Engine）和 Azure 的 AKS（Azure
    Kubernetes Service）。
- en: 'For this section, you need to use three command-line tools:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，你需要使用三个命令行工具：
- en: 'AWS CLI: Manages AWS resources. Refer to appendix A for more information.'
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: AWS CLI：管理 AWS 资源。更多信息请参阅附录 A。
- en: '`eksctl`: Manages EKS clusters ([https://docs.aws.amazon.com/eks/latest/userguide
    eksctl.html](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html)).'
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`eksctl`：管理 EKS 集群 ([https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html](https://docs.aws.amazon.com/eks/latest/userguide/eksctl.html))。'
- en: '`kubectl`: Manages resources in a Kubernetes cluster ([https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/)).
    It works for any cluster, not just EKS.'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`kubectl`：管理 Kubernetes 集群中的资源 ([https://kubernetes.io/docs/tasks/tools/install-kubectl/](https://kubernetes.io/docs/tasks/tools/install-kubectl/))。它适用于任何集群，而不仅仅是
    EKS。'
- en: The official documentation is sufficient for installing these tools, but you
    can also refer to the book’s website for more information ([https://mlbookcamp.com/article/eks](https://mlbookcamp.com/article/eks)).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 官方文档足以安装这些工具，但你也可以参考本书的网站获取更多信息 ([https://mlbookcamp.com/article/eks](https://mlbookcamp.com/article/eks))。
- en: If you don’t use AWS but do use a different cloud provider, you need to use
    their tools for setting up a Kubernetes cluster. Because Kubernetes is not tied
    to any particular vendor, most of the instructions in this chapter will work,
    regardless of where you have the cluster.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有使用 AWS 但使用的是其他云服务提供商，你需要使用他们的工具来设置 Kubernetes 集群。因为 Kubernetes 并未绑定到任何特定的供应商，所以本章中的大多数说明将适用于你拥有集群的任何位置。
- en: Once you have installed `eksctl` and AWS CLI, we can create an EKS cluster.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦安装了 `eksctl` 和 AWS CLI，我们就可以创建一个 EKS 集群。
- en: 'First, prepare a file with the cluster configuration. Create a file in your
    project directory and call it cluster.yaml:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，准备一个包含集群配置的文件。在你的项目目录中创建一个名为 cluster.yaml 的文件：
- en: '[PRE34]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'After creating the config file, we can use `eksctl` for spinning up a cluster:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 创建配置文件后，我们可以使用 `eksctl` 启动一个集群：
- en: '[PRE35]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Note Creating a cluster takes 15–20 minutes, so be patient.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：创建集群需要 15-20 分钟，所以请耐心等待。
- en: 'With this configuration, we create a cluster with Kuberbetes version 1.18 deployed
    in the eu-west-1 region. The name of the cluster is ml-bookcamp-eks. If you want
    to deploy it to a different region, you can change it. This cluster will use two
    m5.xlarge machines. You can read more about this type of instance here: [https://aws.amazon.com/ec2/instance-types/m5/.](https://aws.amazon.com/ec2/instance-types/m5/)
    This is sufficient for the experiments we need to do in this chapter for both
    Kubernetes and Kubeflow.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，我们在 eu-west-1 区域创建了一个集群，集群中部署了 Kubernetes 版本 1.18。集群的名称是 ml-bookcamp-eks。如果你想将其部署到其他区域，你可以更改它。此集群将使用两台
    m5.xlarge 机器。你可以在此处了解更多关于此类实例的信息：[https://aws.amazon.com/ec2/instance-types/m5/](https://aws.amazon.com/ec2/instance-types/m5/)。这对于我们在本章中需要进行的
    Kubernetes 和 Kubeflow 的实验来说是足够的。
- en: Note EKS is not covered by the AWS free tier. You can learn more about the costs
    in the official documentation of AWS ([https://aws.amazon.com/eks/pr-icing/](https://aws.amazon.com/eks/pricing/)).
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：EKS 不在 AWS 免费层中。你可以在 AWS 的官方文档中了解更多关于费用的信息 ([https://aws.amazon.com/eks/pr-icing/](https://aws.amazon.com/eks/pricing/))。
- en: 'Once it’s created, we need to configure `kubectl` to be able to access it.
    For AWS, we do this with the AWS CLI:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建，我们需要配置 `kubectl` 以便能够访问它。对于 AWS，我们使用 AWS CLI 来完成此操作：
- en: '[PRE36]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This command should generate a `kubectl` config file in the default location.
    On Linux and MacOS, this location is ~/.kube/config.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令应在默认位置生成一个 `kubectl` 配置文件。在 Linux 和 MacOS 上，此位置是 ~/.kube/config。
- en: 'Now let’s check that everything works, and that we can connect to our cluster
    using `kubectl`:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来检查一切是否正常工作，并确认我们可以使用 `kubectl` 连接到我们的集群：
- en: '[PRE37]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'This command returns the list of currently running services. We haven’t deployed
    anything, so we expect to see only one service—Kubernetes itself. This is the
    result you should see:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令返回当前正在运行的服务列表。我们还没有部署任何内容，所以我们预计只会看到一个服务——Kubernetes 本身。你应该看到以下结果：
- en: '[PRE38]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The connection works, and now we can deploy a service. To do that, we first
    need to prepare a Docker image with the actual service. Let’s do that next.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 连接成功，现在我们可以部署一个服务。为此，我们首先需要准备一个包含实际服务的Docker镜像。让我们接下来做这件事。
- en: 9.3.3 Preparing the Docker images
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 准备Docker镜像
- en: 'In the previous sections, we created two components of the serving system:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们创建了服务系统的两个组件：
- en: 'TF-Serving: The component with the actual model'
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TF-Serving：包含实际模型的组件
- en: 'Gateway: The component for image preprocessing that communicates with TF Serving'
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gateway：与TF Serving通信的图像预处理组件
- en: Now we deploy them. We start first with deploying the TF Serving image.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们部署它们。我们首先部署TF Serving镜像。
- en: The TensorFlow Serving image
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow Serving镜像
- en: 'As in chapter 8, we first need to publish our image to ECR—the Docker registry
    of AWS. Let’s create a registry called model-serving:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 如第8章所述，我们首先需要将我们的镜像发布到ECR——AWS的Docker注册库。让我们创建一个名为model-serving的注册库：
- en: '[PRE39]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'It should return a path like this:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 应该返回一个类似这样的路径：
- en: '[PRE40]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Important Take note—we’ll need this path later.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示——我们稍后会需要这个路径。
- en: 'When running a Docker image of TF Serving locally, we used this command (you
    don’t need to run it now):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地运行TF Serving的Docker镜像时，我们使用了这个命令（你现在不需要运行它）：
- en: '[PRE41]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: We used the `-v` parameter to mount the model from the `clothing-model` to the
    /models/clothing-model/1 directory within the image.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用了`-v`参数将模型从`clothing-model`挂载到镜像内的/models/clothing-model/1目录。
- en: It’s also possible to do it with Kubernetes, but in this chapter, we follow
    a simpler approach and include the model into the image itself, similar to what
    we did in chapter 8.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用Kubernetes来完成这个操作，但在这个章节中，我们遵循一个更简单的方法，将模型直接包含在镜像中，类似于我们在第8章中所做的。
- en: 'Let’s create a Dockerfile for that. We can name it tf-serving.dockerfile:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为它创建一个Dockerfile。我们可以将其命名为tf-serving.dockerfile：
- en: '[PRE42]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Uses the Tensorflow Serving image as its base
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用Tensorflow Serving镜像作为其基础
- en: ❷ Sets the MODEL_NAME variable to clothing-model
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置MODEL_NAME变量为clothing-model
- en: ❸ Copies the model to /models/clothing-model/1
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将模型复制到/models/clothing-model/1
- en: We base our image on the TensorFlow Serving image in ❶. Next, in ❷, we set the
    environment variable `MODEL_NAME` to `clothing-model`, which is the equivalent
    of the `-e` parameter. Next, we copy the model to /models/clothing-model/1 in
    ❸, which is the equivalent of using the `-v` parameter.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们基于❶中的TensorFlow Serving镜像。接下来，在❷中，我们将环境变量`MODEL_NAME`设置为`clothing-model`，这相当于`-e`参数。接下来，在❸中，我们将模型复制到/models/clothing-model/1，这相当于使用`-v`参数。
- en: Note If you want to use a computer with a GPU, use the tensorflow/serving :2.3.0-gpu
    image (commented as ❶ in the Dockerfile).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你想要使用带有GPU的计算机，请使用tensorflow/serving :2.3.0-gpu镜像（在Dockerfile中注释为❶）。
- en: 'Let’s build it:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们构建它：
- en: '[PRE43]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, we need to publish this image to ECR. First, we need to authenticate
    with ECR using AWS CLI:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要将此镜像发布到ECR。首先，我们需要使用AWS CLI对ECR进行认证：
- en: '[PRE44]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Note You need to include the “`$`” when typing the command. The command inside
    the parentheses returns another command. Using the “`$()`”, we execute this command.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在输入命令时需要包含“`$`”。括号内的命令返回另一个命令。使用“`$()`”，我们执行这个命令。
- en: 'Next, tag the image with the remote URI:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用远程URI标记镜像：
- en: '[PRE45]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Be sure to change the `ACCOUNT` and `REGION` variables.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 确保更改ACCOUNT和REGION变量。
- en: 'Now we’re ready to push the image to ECR:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好将图像推送到ECR：
- en: '[PRE46]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: It’s pushed! Now we need to do the same with the Gateway component.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 已推送！现在我们需要对Gateway组件做同样的操作。
- en: The Gateway image
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Gateway镜像
- en: 'Now let’s prepare the image for the Gateway component. Gateway is a web service,
    and it relies on a number of Python libraries:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们为Gateway组件准备图像。Gateway是一个网络服务，它依赖于多个Python库：
- en: Flask and Gunicorn
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Flask和Gunicorn
- en: keras_image_helper
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: keras_image_helper
- en: grpcio
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: grpcio
- en: TensorFlow
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow
- en: TensorFlow-Serving-API
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow-Serving-API
- en: 'Remember that in chapter 5, we used Pipenv for managing dependencies. Let’s
    use it here as well:'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在第5章中，我们使用了Pipenv来管理依赖项。让我们在这里也使用它：
- en: '[PRE47]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Running this command creates two files: Pipfile and Pipfile.lock.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 运行此命令会创建两个文件：Pipfile和Pipfile.lock。
- en: 'Warning Even though we already mentioned it, it’s important enough to repeat
    it. Here, we rely on TensorFlow for only one function. In a production environment,
    it’s better not to install TensorFlow. In this chapter, we do it for simplicity.
    Instead of depending on TensorFlow, we can take only the protobuf files we need
    and reduce the size of our Docker image significantly. Refer to this repository
    for instructions: [https://github.com/alexeygrigorev/tensorflow-protobuf.](https://github.com/alexeygrigorev/tensorflow-protobuf)'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：尽管我们已经提到了这一点，但这是非常重要的，所以需要重复。在这里，我们只依赖于 TensorFlow 的一个功能。在生产环境中，最好不要安装 TensorFlow。在本章中，我们这样做是为了简单起见。而不是依赖于
    TensorFlow，我们可以只取我们需要的 protobuf 文件，并显著减小我们的 Docker 镜像大小。有关说明，请参阅此存储库：[https://github.com/alexeygrigorev/tensorflow-protobuf.](https://github.com/alexeygrigorev/tensorflow-protobuf)
- en: 'Now let’s create a Docker image. Start with creating a Dockerfile named gateway.dockerfile
    with the following content:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个 Docker 镜像。首先创建一个名为 gateway.dockerfile 的 Dockerfile，内容如下：
- en: '[PRE48]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: This Dockerfile is very similar to the file we had previously. Refer to chapter
    5 for more information about it.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这个 Dockerfile 与我们之前拥有的文件非常相似。有关更多信息，请参阅第 5 章。
- en: 'Let’s build this image now:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在构建这个镜像：
- en: '[PRE49]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'And push it to ECR:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将其推送到 ECR：
- en: '[PRE50]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Note For verifying that these images work well together locally, you need to
    use Docker Compose ([https://docs.docker.com/compose/](https://docs.docker.com/compose/)).
    This is a very useful tool, and we recommend that you spend time learning it,
    but we will not cover it here.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了验证这些镜像在本地很好地协同工作，您需要使用 Docker Compose ([https://docs.docker.com/compose/](https://docs.docker.com/compose/))。这是一个非常有用的工具，我们建议您花时间学习它，但在这里我们不会对其进行介绍。
- en: We have published both images to ECR, and now we’re ready to deploy the services
    to Kubernetes! We’ll do that next.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经将这两个镜像发布到 ECR，现在我们准备将服务部署到 Kubernetes！我们将在下一部分进行操作。
- en: 9.3.4 Deploying to Kubernetes
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 部署到 Kubernetes
- en: 'Before we deploy, let’s revisit the basics of Kubernetes. We have the following
    objects living inside a cluster:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署之前，让我们回顾一下 Kubernetes 的基础知识。在集群内部存在以下对象：
- en: 'Pod: The smallest unit in Kubernetes. It’s a single process, and we have one
    Docker container in one pod.'
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Pod：Kubernetes 中的最小单元。它是一个单独的进程，我们有一个 Docker 容器在一个 pod 中。
- en: 'Deployment: A group of multiple related pods.'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署：多个相关 pods 的组。
- en: 'Service: What sits in front of a deployment and routes the requests to individual
    pods.'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务：位于部署前面并路由请求到各个 pods 的组件。
- en: 'To deploy an application to Kubernetes, we need to configure two things:'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 要将应用程序部署到 Kubernetes，我们需要配置两件事：
- en: 'A deployment: It specifies how the pods of this deployment will look.'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署：它指定了此部署的 pods 将如何看起来。
- en: 'A service: Specifies how to access the service and how the service connects
    to the pods.'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务：指定如何访问服务以及服务如何连接到 pods。
- en: Let’s start with configuring the deployment for TF Serving.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从配置 TF Serving 的部署开始。
- en: Deployment for TF Serving
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 部署 TF Serving
- en: 'In Kubernetes, we usually configure everything with YAML files. For configuring
    a deployment, we create a file named tf-serving-clothing-model-deployment.yaml
    in our project directory with the following content:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中，我们通常使用 YAML 文件来配置一切。为了配置部署，我们在项目目录中创建一个名为 tf-serving-clothing-model-deployment.yaml
    的文件，内容如下：
- en: '[PRE51]'
  id: totrans-303
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: ❶ Configures a deployment
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置部署
- en: ❷ Specifies the name of the deployment
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定部署的名称
- en: ❸ Creates only one instance of the service—one pod
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 仅创建一个服务实例——一个 pod
- en: ❹ Defines the specification for the pods
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义 pods 的规范
- en: ❺ Uses the image we created previously
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用我们之前创建的镜像
- en: ❻ Exposes port 8500 This config is rather long, so let’s take a look at all
    the important lines.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 公开端口 8500 这个配置相当长，所以让我们看看所有重要的行。
- en: In ❶, we specify the type of the Kubernetes object we want to configure in this
    YAML file—it’s a deployment.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❶ 中，我们指定了在这个 YAML 文件中我们想要配置的 Kubernetes 对象类型——它是一个部署。
- en: 'In ❷, we define the name of the deployment as well as set some metadata information.
    We need to repeat it multiple times: one time for setting the name of the deployment
    (“name”) and a few more times (“labels: app”) for the service that we’ll configure
    later.'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '在 ❷ 中，我们定义了部署的名称并设置了一些元数据信息。我们需要重复多次：一次用于设置部署的名称（“name”），以及几次（“labels: app”）用于我们稍后要配置的服务。'
- en: In ❸, we set the number of instances—pods—we want to have in the deployment.
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❸ 中，我们设置了部署中我们想要的实例数——pods。
- en: In ❹, we specify the configuration for the pods—we set the parameters that all
    of the pods will have.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ❹ 中，我们指定了 pods 的配置——我们设置了所有 pods 都将拥有的参数。
- en: In ❺, we set the URI for the Docker image. The pod will use this image. Don’t
    forget to put your account ID there as well as the correct region.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在❺中，我们设置Docker镜像的URI。Pod将使用这个镜像。别忘了在那里也放入您的账户ID以及正确的区域。
- en: Finally, in ❻, we open port 8500 on the pods of this deployment. This is the
    port that TF Serving uses.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在❻中，我们打开此部署的pods上的8500端口。这是TF Serving使用的端口。
- en: To learn more about configuring deployments in Kubernetes, check the official
    documentation ([https://kubernetes.io/docs/concepts/workloads/controllers/deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解更多关于在Kubernetes中配置部署的信息，请查看官方文档（[https://kubernetes.io/docs/concepts/workloads/controllers/deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/))。
- en: 'We have a config. Now we need to use it to create a Kubernetes object—a deployment
    in our case. We do it by using the `apply` command from `kubectl`:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个配置。现在我们需要使用它来创建一个Kubernetes对象——在我们的情况下是一个部署。我们通过使用`kubectl`的`apply`命令来完成：
- en: '[PRE52]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: The `-f` parameter tells `kubectl` that it needs to read the configuration from
    the config file.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '`-f`参数告诉`kubectl`它需要从配置文件中读取配置。'
- en: 'To verify that it’s working, we need to check if a new deployment appeared.
    This is how we can get the list of all active deployments:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 为了验证它是否正在工作，我们需要检查是否出现了新的部署。这是我们可以获取所有活动部署列表的方法：
- en: '[PRE53]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'The output should look similar to this:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该看起来像这样：
- en: '[PRE54]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'We see that our deployment is there. Also, we can get the list of pods. It’s
    quite similar to getting the list of all deployments:'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到我们的部署在那里。此外，我们还可以获取pods的列表。它与获取所有部署的列表非常相似：
- en: '[PRE55]'
  id: totrans-325
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'We should see something like that in the output:'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于以下内容：
- en: '[PRE56]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: Now we need to create a service on top of this deployment.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要在部署之上创建一个服务。
- en: Service for TF Serving
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: TF Serving服务
- en: We want to invoke TF Serving from Gateway. For that, we need to create a service
    in front of the TF Serving deployment.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望从网关调用TF Serving。为此，我们需要在TF Serving部署前面创建一个服务。
- en: 'Like with a deployment, we start by creating a configuration file for the service.
    It’s also a YAML file. Create a file called tf-serving-clothing-model-service.yaml
    with the following content:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 就像部署一样，我们首先为服务创建一个配置文件。它也是一个YAML文件。创建一个名为`tf-serving-clothing-model-service.yaml`的文件，内容如下：
- en: '[PRE57]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: ❶ Configures the name of the service
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置服务的名称
- en: ❷ Specification of the service—the port that will be used
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 服务的规范——将要使用的端口
- en: ❸ Connects the service to the deployment by specifying the label of the deployment
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过指定部署的标签将服务连接到部署
- en: 'We apply it in the same way—by using the `apply` command:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以相同的方式应用它——通过使用`apply`命令：
- en: '[PRE58]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'To check that it works, we can get the list of all services and see if our
    service is there:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 为了检查它是否工作，我们可以获取所有服务的列表，看看我们的服务是否在那里：
- en: '[PRE59]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: We should see something like
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似以下的内容
- en: '[PRE60]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: In addition to the default Kubernetes service, we also have tf-serving-clothing-model,
    which is the service we just created.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 除了默认的Kubernetes服务之外，我们还有一个tf-serving-clothing-model，这是我们刚刚创建的服务。
- en: 'To access this service, we need to get its URL. The internal URLs typically
    follow this pattern:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问此服务，我们需要获取其URL。内部URL通常遵循以下模式：
- en: '[PRE61]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: The `<service-name>` part is tf-serving-clothing-model.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: '`<service-name>`部分是tf-serving-clothing-model。'
- en: We haven’t used any specific namespace for this service, so Kubernetes automatically
    put the service in the “default” namespace. We won’t cover namespaces here, but
    you can read more about them in the official documentation ([https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/)).
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有为这个服务使用任何特定的命名空间，因此Kubernetes自动将服务放入“default”命名空间。我们在这里不会介绍命名空间，但你可以在官方文档（[https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/))中了解更多。
- en: 'This is the URL for the service we just created:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这是刚刚创建的服务的URL：
- en: '[PRE62]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: We’ll need this URL later, when configuring Gateway.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会需要这个URL，当配置网关时。
- en: We’ve created a deployment for TF Serving as well as a service. Now let's create
    a deployment for Gateway.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经创建了TF Serving的部署以及服务。现在让我们创建一个网关的部署。
- en: Deployment for Gateway
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 网关部署
- en: 'Like previously, we start by creating a YAML file with configuration. Create
    a file named serving-gateway-deployment.yaml:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们首先创建一个包含配置的YAML文件。创建一个名为`serving-gateway-deployment.yaml`的文件：
- en: '[PRE63]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: ❶ Sets the value for the TF_SERVING_HOST environment variable
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 设置TF_SERVING_HOST环境变量的值
- en: Replace `<ACCOUNT>` and `<REGION>` in the image URL with your values.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 将图像URL中的`<ACCOUNT>`和`<REGION>`替换为您的值。
- en: 'The configuration of this deployment is very similar to the deployment of TF
    Serving, with one important difference: we specify the value of the `TF_SERVING_HOST`
    variable by setting it to the URL of the service with our model (shown in bold
    in the listing).'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署的配置与 TF Serving 的部署非常相似，只有一个重要区别：我们通过将其设置为包含我们模型的服务（在列表中用粗体显示）的 URL 来指定 `TF_SERVING_HOST`
    变量的值。
- en: 'Let’s apply this configuration:'
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个配置：
- en: '[PRE64]'
  id: totrans-358
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'This should create a new pod and a new deployment. Let’s take a look at the
    list of pods:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 这应该会创建一个新的 Pod 和一个新的部署。让我们看看 Pod 列表：
- en: '[PRE65]'
  id: totrans-360
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Indeed, a new pod is there:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 的确，有一个新的 Pod 在那里：
- en: '[PRE66]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: Warning Gateway uses gRPC to communicate with TF Serving. When deploying multiple
    instances of TF Serving, you may run into a problem with distributing the load
    between these instances ([https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/)).
    To solve it, you will need to install a service mesh tool like Linkerd, Istio,
    or something similar. Talk to your operations team to see how you can do it at
    your company.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 Gateway 使用 gRPC 与 TF Serving 通信。当部署多个 TF Serving 实例时，您可能会遇到在这些实例之间分配负载的问题（[https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/](https://kubernetes.io/blog/2018/11/07/grpc-load-balancing-on-kubernetes-without-tears/)）。为了解决这个问题，您需要安装一个服务网格工具，如
    Linkerd、Istio 或类似工具。与您的运维团队交谈，了解您在公司如何实现。
- en: We’ve created a deployment for Gateway. Now we need to configure the service
    for. We do it next.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 Gateway 创建了一个部署。现在我们需要为它配置服务。我们将在下一步进行。
- en: Service for Gateway
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: Gateway 服务配置
- en: We’ve created a deployment for Gateway, and now we need to create a service.
    This service is different from the service we created for TF Serving—it needs
    to be publicly accessible, so services outside of our Kubernetes cluster can use
    it. For that, we need to use a special type of service—LoadBalancer. It creates
    an external load balancer, which is available outside of the Kubernetes cluster.
    In the case of AWS, it uses ELB, the Elastic Load Balancing service.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为 Gateway 创建了一个部署，现在我们需要创建一个服务。这个服务与我们为 TF Serving 创建的服务不同——它需要公开访问，因此 Kubernetes
    集群之外的服务可以使用它。为此，我们需要使用一种特殊类型的服务——LoadBalancer。它创建了一个外部负载均衡器，可在 Kubernetes 集群外部使用。在
    AWS 的情况下，它使用 ELB，即弹性负载均衡服务。
- en: 'Let’s create a config file named serving-gateway-service.yaml:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个名为 serving-gateway-service.yaml 的配置文件：
- en: '[PRE67]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: ❶ Uses the LoadBalancer type
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 LoadBalancer 类型
- en: ❷ Maps port 9696 in the pod to port 80 n the service
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 Pod 中的端口 9696 映射到服务的端口 80
- en: In ❶, we specify the type of the service—LoadBalancer.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 在❶中，我们指定了服务的类型——LoadBalancer。
- en: In ❷, we connect port 80 in the service to port 9696 in pods. This way, we don’t
    need to specify the port when connecting to the service—it will use the default
    HTTP port, which is 80.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 在❷中，我们将服务的端口 80 连接到 Pod 的端口 9696。这样，我们就不需要在连接服务时指定端口——它将使用默认的 HTTP 端口，即 80。
- en: 'Let’s apply this config:'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个配置：
- en: '[PRE68]'
  id: totrans-374
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To see the external URL of the service, use the `describe` command:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 要查看服务的外部 URL，请使用 `describe` 命令：
- en: '[PRE69]'
  id: totrans-376
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'It will output some information about the service:'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 它将输出有关服务的一些信息：
- en: '[PRE70]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: 'We’re interested in the line with `LoadBalancer` `Ingress`. This is the URL
    we need to use to access the Gateway service. In our case, this is the URL:'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 我们对带有 `LoadBalancer` `Ingress` 的行感兴趣。这是我们访问 Gateway 服务的 URL。在我们的例子中，这是 URL：
- en: '[PRE71]'
  id: totrans-380
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: The Gateway service is ready to use. Let’s do it!
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: Gateway 服务已准备好使用。让我们开始吧！
- en: 9.3.5 Testing the service
  id: totrans-382
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 测试服务
- en: 'When running TF Serving and Gateway locally, we prepared a simple snippet of
    Python code for testing our service. Let’s reuse it. Go to the same notebook,
    and replace the local IP address by the URL we got from the previous section:'
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地运行 TF Serving 和 Gateway 时，我们准备了一段简单的 Python 代码片段来测试我们的服务。让我们重用它。转到相同的笔记本，并将本地
    IP 地址替换为上一节中获得的 URL：
- en: '[PRE72]'
  id: totrans-384
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Run it. As a result, we get the same predictions as previously:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 运行它。结果，我们得到与之前相同的预测：
- en: '[PRE73]'
  id: totrans-386
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: It’s working—and it means we just successfully deployed our deep learning model
    with TF Serving and Kubernetes!
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 它正在工作——这意味着我们刚刚成功使用 TF Serving 和 Kubernetes 部署了我们的深度学习模型！
- en: Important If you finished experimenting with EKS, don’t forget to shut down
    the cluster. If you don’t turn it off, you’ll need to pay for it, even if it’s
    idle and you don’t use it. You will find the instructions for that at the end
    of this chapter.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 重要提示：如果您完成了 EKS 的实验，别忘了关闭集群。如果您不关闭它，即使它是空闲的，您也需要为此付费。您可以在本章末尾找到有关如何操作的说明。
- en: In this example, we covered Kubernetes from the user’s perspective only, not
    from an operational standpoint. We haven’t talked about autoscaling, monitoring,
    alerting, and other important topics required for productionizing machine learning
    models. For more details about these topics, consult a Kubernetes book or the
    official documentation of Kubernetes.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 在本例中，我们仅从用户的角度介绍了Kubernetes，而不是从运维的角度。我们还没有讨论自动扩展、监控、警报以及其他对于将机器学习模型投入生产所必需的重要主题。有关这些主题的更多详细信息，请参阅Kubernetes书籍或Kubernetes的官方文档。
- en: 'You probably noticed that we needed to do quite a lot of things for deploying
    a single model: create a Docker image, push it to ECR, create a deployment, create
    a service. Doing this for a couple of models is not a problem, but if you need
    to do it for tens or hundreds of models, it becomes problematic and repetitive.'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，为了部署单个模型，我们需要做很多事情：创建Docker镜像，推送到ECR，创建部署，创建服务。对于几个模型来说，这没问题，但如果需要为数十个或数百个模型这样做，就会变得有问题且重复。
- en: There’s a solution—Kubeflow. It makes deployment easier. In the next section,
    we’ll see how we can use it for serving Keras models.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个解决方案——Kubeflow。它使部署变得更容易。在下一节中，我们将看到如何使用它来托管Keras模型。
- en: 9.4 Model deployment with Kubeflow
  id: totrans-392
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 使用Kubeflow进行模型部署
- en: Kubeflow is a project that aims to simplify the deployment of machine learning
    services on Kubernetes.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow是一个旨在简化在Kubernetes上部署机器学习服务的项目。
- en: 'It consists of a set of tools, each of which aims at solving a particular problem.
    For example:'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 它由一系列工具组成，每个工具都旨在解决特定问题。例如：
- en: 'Kubeflow Notebooks Server: Makes it easier to centrally host Jupyter Notebooks'
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow Notebooks Server：使集中托管Jupyter Notebooks变得更容易
- en: 'Kubeflow Pipelines: Automates the training process'
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow Pipelines：自动化训练过程
- en: 'Katib: Selects the best parameters for the model'
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katib：为模型选择最佳参数
- en: 'Kubeflow Serving (abbreviated as “KFServing”): Deploys machine learning models'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow Serving（简称“KFServing”）：部署机器学习模型
- en: 'And many others. You can read more about its components here: [https://www.kubeflow.org/docs/components/.](https://www.kubeflow.org/docs/components/)'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 以及许多其他内容。你可以在这里了解更多关于其组件的信息：[https://www.kubeflow.org/docs/components/](https://www.kubeflow.org/docs/components/)。
- en: In this chapter, we focus on model deployment, so we’ll need to use only one
    component of Kubeflow—KFServing.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于模型部署，因此我们只需要使用Kubeflow的一个组件——KFServing。
- en: If you want to install the entire Kubeflow project, refer to the official documentation.
    It has the installation instructions for the major cloud providers such as Google
    Cloud Platform, Microsoft Azure, and AWS ([https://www.kubeflow.org/docs/aws/aws-e2e/](https://www.kubeflow.org/docs/aws/aws-e2e/)).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想安装整个Kubeflow项目，请参阅官方文档。它包含了针对主要云提供商（如Google Cloud Platform、Microsoft Azure和AWS）的安装说明。[https://www.kubeflow.org/docs/aws/aws-e2e/](https://www.kubeflow.org/docs/aws/aws-e2e/)。
- en: 'For the instructions about installing only KFServing without the rest of Kubeflow
    on AWS, refer to the book’s website: [https://mlbookcamp.com/article/kfserving-eks-install](https://mlbookcamp.com/article/kfserving-eks-install).
    We used this article for setting up the environment for the rest of this chapter,
    but the code here should work with any Kubeflow installation with minor changes.'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 关于在AWS上仅安装KFServing而不安装Kubeflow其余部分的说明，请参阅本书网站：[https://mlbookcamp.com/article/kfserving-eks-install](https://mlbookcamp.com/article/kfserving-eks-install)。我们使用这篇文章来设置本章其余部分的环境，但这里的代码应该可以通过少量修改与任何Kubeflow安装兼容。
- en: Note The installation may be nontrivial for you, especially if you haven’t done
    anything similar in the past. If you are not sure about some things, ask somebody
    from the operations team to help you set it up.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：安装可能对你来说并不简单，尤其是如果你之前没有做过类似的事情。如果你对某些事情不确定，请向运维团队的人寻求帮助以设置它。
- en: '9.4.1 the model: Uploading it to S3'
  id: totrans-404
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 模型：将其上传到S3
- en: To deploy a Keras model with KFServing, we first need to convert it to the saved_
    model format. We already did this previously, so we can just use the converted
    files.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用KFServing部署Keras模型，我们首先需要将其转换为saved_model格式。我们之前已经完成了这个步骤，所以我们可以直接使用转换后的文件。
- en: Next, we need to create a bucket in S3, where we will put our models. Let’s
    call it mlbookcamp-models-<NAME>, where <NAME> could be anything—for example,
    your name. Bucket names must be unique across the entire AWS. That’s why we need
    to add some suffix to the name of the bucket. It should be in the same region
    as our EKS cluster. In our case, it’s eu-west-1.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要在S3中创建一个存储桶，我们将把我们的模型放在那里。让我们称它为mlbookcamp-models-<NAME>，其中<NAME>可以是任何东西——例如，你的名字。存储桶名称必须在整个AWS中是唯一的。这就是为什么我们需要在存储桶的名称中添加一些后缀。它应该与我们的EKS集群在同一个区域。在我们的例子中，它是eu-west-1。
- en: 'We can create it with the AWS CLI:'
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用AWS CLI创建它：
- en: '[PRE74]'
  id: totrans-408
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'After creating a bucket, we need to upload the model there. Use the AWS CLI
    for that:'
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建存储桶之后，我们需要将模型上传到那里。为此，请使用AWS CLI：
- en: '[PRE75]'
  id: totrans-410
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Note that there’s “0001” at the end. This is important—KFServing, like TF Serving,
    needs a version of the model. We don’t have any previous versions of this model,
    so we add “0001” at the end.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，结尾有“0001”。这很重要——KFServing，就像TF Serving一样，需要一个模型的版本。我们没有这个模型的任何先前版本，所以我们把“0001”加在结尾。
- en: Now we’re ready to deploy this model.
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好部署这个模型了。
- en: 9.4.2 Deploying TensorFlow models with KFServing
  id: totrans-413
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 使用KFServing部署TensorFlow模型
- en: Previously, when deploying our model with plain Kubernetes, we needed to configure
    a deployment and then a service. Instead of doing it, KFServing defines a special
    kind of Kubernetes object—InferenceService. We need to configure it only once,
    and it will take care of creating all other Kubernetes objects—including a service
    and a deployment—automatically.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 在之前，当我们使用纯Kubernetes部署我们的模型时，我们需要配置一个部署然后一个服务。而不是这样做，KFServing定义了一种特殊的Kubernetes对象——InferenceService。我们只需要配置一次，它将自动创建所有其他Kubernetes对象——包括服务和部署。
- en: 'First, create another YAML file (tf-clothes.yaml) with the following content:'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建另一个YAML文件（tf-clothes.yaml），内容如下：
- en: '[PRE76]'
  id: totrans-416
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: ❶ Uses serviceAccountName to access S3
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用serviceAccountName访问S3
- en: ❷ Specifies the location in S3 with the model
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在S3中指定包含模型的存储位置
- en: When accessing a model from S3, we need to specify the service account name
    to be able to get the model. This tells KFServing how to access the S3 bucket—and
    we specify it in ❶. The article about installing KFServing on EKS covers this
    as well ([https://mlbookcamp.com/article/kfserving-eks-install](https://mlbookcamp.com/article/kfserving-eks-install)).
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 当从S3访问模型时，我们需要指定服务账户名称以便能够获取模型。这告诉KFServing如何访问S3存储桶——我们在❶中指定它。关于在EKS上安装KFServing的文章也涵盖了这一点([https://mlbookcamp.com/article/kfserving-eks-install](https://mlbookcamp.com/article/kfserving-eks-install))。
- en: 'Like with usual Kubernetes, we use `kubectl` to apply this config:'
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 就像通常的Kubernetes一样，我们使用`kubectl`应用此配置：
- en: '[PRE77]'
  id: totrans-421
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Because it creates an InferenceService object, we need to get the list of such
    objects using the `get` command from `kubectl`:'
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 因为它创建了一个InferenceService对象，我们需要使用`kubectl`的`get`命令获取此类对象的列表：
- en: '[PRE78]'
  id: totrans-423
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: 'We should see something like this:'
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该看到类似以下的内容：
- en: '[PRE79]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: If our service `READY` is not yet `True`, we need to wait a bit before it becomes
    ready. It may take 1–2 minutes.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们的服务`READY`尚未为`True`，我们需要等待一会儿，直到它准备好。这可能需要1-2分钟。
- en: 'Now take note of the URL and the name of the model:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 现在请注意URL和模型的名称：
- en: 'The URL: [https://clothing-model.default.kubeflow.mlbookcamp.com/v1/models/clothing-model](https://clothing-model.default.kubeflow.mlbookcamp.com/v1/models/clothing-model).
    In your configuration, the host will be different, so the entire URL will also
    be different.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL：[https://clothing-model.default.kubeflow.mlbookcamp.com/v1/models/clothing-model](https://clothing-model.default.kubeflow.mlbookcamp.com/v1/models/clothing-model)。在你的配置中，主机将不同，因此整个URL也将不同。
- en: 'The model name: clothing-model.'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型名称：clothing-model。
- en: Note It may take some time for the URL to become reachable from our laptop.
    Changes in DNS may need some time to propagate.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，URL可能需要一些时间才能从我们的笔记本电脑上访问。DNS的变化可能需要一些时间来传播。
- en: 9.4.3 Accessing the model
  id: totrans-431
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 访问模型
- en: The model is deployed. Let’s use it! For that, we can start a Jupyter Notebook
    or create a Python script file.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 模型已部署。让我们使用它！为此，我们可以启动Jupyter Notebook或创建一个Python脚本文件。
- en: 'KFServing uses HTTP and JSON, so we use the requests library for communicating
    with it. So let’s start by importing it:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: KFServing使用HTTP和JSON，因此我们使用requests库与其通信。所以，让我们首先导入它：
- en: '[PRE80]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: 'Next, we need to use the image preprocessor for preparing the images. It’s
    the same one we used previously:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要使用图像预处理器来准备图像。它与之前使用的是同一个：
- en: '[PRE81]'
  id: totrans-436
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: 'Now, we need an image for testing. We use the same image of pants as in the
    previous section and use the same code for getting it and preprocessing it:'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们需要一个用于测试的图像。我们使用与上一节中相同的裤子图像，并使用相同的代码来获取它和预处理它：
- en: '[PRE82]'
  id: totrans-438
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'The `X` variable contains a NumPy array. We need to convert it to a list before
    we can send the data to KFServing:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: '`X` 变量包含一个 NumPy 数组。在我们将数据发送到 KFServing 之前，我们需要将其转换为列表：'
- en: '[PRE83]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: 'We have the request. As the next step, we need to define the URL where we will
    send this request. We already have it from the previous section, but we need to
    modify it slightly:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了请求。作为下一步，我们需要定义我们将发送请求的 URL。我们已经在上一节中有了它，但我们需要稍作修改：
- en: Use HTTPS instead of HTTP.
  id: totrans-442
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 HTTPS 而不是 HTTP。
- en: Add “:predict” at the end of the URL.
  id: totrans-443
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 URL 的末尾添加 “:predict”。
- en: 'With these changes, this is how the URL appears:'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 经过这些更改，URL 如下所示：
- en: '[PRE84]'
  id: totrans-445
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'We’re ready to post the request:'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备发送请求：
- en: '[PRE85]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'Let’s take a look at the results:'
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结果：
- en: '[PRE86]'
  id: totrans-449
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: 'Like we did previously, we need to translate the predictions into human-readable
    form. We do it by assigning a label to each element of the result:'
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们之前做的那样，我们需要将预测转换为人类可读的格式。我们通过将标签分配给结果的每个元素来实现：
- en: '[PRE87]'
  id: totrans-451
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Here’s the result:'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是结果：
- en: '[PRE88]'
  id: totrans-453
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: We have deployed our model, and it can be used.
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经部署了我们的模型，并且它可以被使用。
- en: But we cannot expect that the people who use our model will be happy about having
    to prepare the images themselves. In the next section, we’ll talk about transformers—they
    can take away the burden of preprocessing the images.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 但我们不能期望使用我们模型的人会高兴于不得不自己准备图像。在下一节中，我们将讨论转换器——它们可以减轻预处理图像的负担。
- en: 9.4.4 KFServing transformers
  id: totrans-456
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.4 KFServing 转换器
- en: In the previous section, we introduced the Gateway service. It was sitting between
    the client and the model, and it took care of transforming the requests from the
    clients to the format the model expects (figure 9.5).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们介绍了网关服务。它位于客户端和模型之间，负责将客户端的请求转换为模型期望的格式（图 9.5）。
- en: '![](../Images/09-05.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-05.png)'
- en: Figure 9.5 The Gateway service takes care of preprocessing the image, so the
    clients of our applications don’t need to do it.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.5 网关服务负责预处理图像，因此我们的应用程序的客户端不需要这样做。
- en: Fortunately for us, we don’t have to introduce another Gateway service for KFServing.
    Instead, we can use a *transformer*.
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，对于我们来说，我们不需要为 KFServing 引入另一个网关服务。相反，我们可以使用一个 *转换器*。
- en: Transformers take care of
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 转换器负责
- en: Preprocessing the request coming from the client and converting it to the format
    our model expects
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理来自客户端的请求并将其转换为我们的模型期望的格式
- en: Postprocessing the output of the model—converting it to the format the client
    needs
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对模型输出的后处理——将其转换为客户端需要的格式
- en: We can put all the preprocessing code from the previous section into a transformer
    (figure 9.6).
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将上一节中的所有预处理代码放入一个转换器中（图 9.6）。
- en: '![](../Images/09-06.png)'
  id: totrans-465
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/09-06.png)'
- en: Figure 9.6 The KFServing transformer can download the image and prepare it in
    the preprocessing step, as well as attach labels to the output of the model in
    the postprocessing step.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.6 KFServing 转换器可以在预处理步骤中下载图像并准备它，以及在后处理步骤中为模型的输出附加标签。
- en: 'Like the Gateway service we created manually, transformers in KFServing are
    deployed separately from the model. This means that they can scale up and down
    independently. It’s a good thing—they perform a different kind of work:'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 就像我们手动创建的网关服务一样，KFServing 中的转换器是独立于模型部署的。这意味着它们可以独立地进行扩展和缩减。这是一件好事——它们执行不同类型的工作：
- en: Transformers are doing I/O work (downloading the image).
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 转换器正在执行 I/O 工作（下载图像）。
- en: Models are doing CPU-intensive work (applying the neural network to make predictions).
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型正在执行 CPU 密集型工作（应用神经网络进行预测）。
- en: To create a transformer, we need to install the KFServing library for Python
    and create a class that extends the `KFModel` class.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个转换器，我们需要安装 Python 的 KFServing 库并创建一个扩展 `KFModel` 类的类。
- en: 'It looks like this:'
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来像这样：
- en: '[PRE89]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: 'We will not go into details about building your own transformer, but if you’d
    like to know how to do it, check out this article: [https://mlbookcamp.com/article/kfserving-transformers](https://mlbookcamp.com/article/kfserving-transformers).
    Instead, for this book, we’ve prepared a transformer that uses the keras_image_helper
    library. You can check its source code here: [https://github.com/alexeygrigorev/kfserving-keras-transformer](https://github.com/alexeygrigorev/kfserving-keras-transformer).'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入介绍如何构建自己的转换器，但如果你想了解如何做，请查看这篇文章：[https://mlbookcamp.com/article/kfserving-transformers](https://mlbookcamp.com/article/kfserving-transformers)。相反，对于这本书，我们准备了一个使用
    keras_image_helper 库的转换器。你可以在这里查看其源代码：[https://github.com/alexeygrigorev/kfserving-keras-transformer](https://github.com/alexeygrigorev/kfserving-keras-transformer)。
- en: 'Let’s use it. First, we need to delete the old inference service:'
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用它。首先，我们需要删除旧的推理服务：
- en: '[PRE90]'
  id: totrans-475
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Then, update the config file (tf-clothes.yaml), and include the transformer
    section (in bold) there:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，更新配置文件（tf-clothes.yaml），并在其中包含变换器部分（加粗）：
- en: '[PRE91]'
  id: totrans-477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: ❶ Defines the model in the predictor section
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在“预测器”部分定义了模型
- en: ❷ Defines the transformer in the transformer section
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在“变换器”部分定义了变换器
- en: ❸ Sets the image for the transformer
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为变换器设置镜像
- en: ❹ Configures it—specifies input size, model name, and labels
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 配置它——指定输入大小、模型名称和标签
- en: In addition to the “predictor” section, which we had previously, we add another
    one—“transformer.” The transformer we use is a publicly available image at agrigorev/
    kfserving-keras-transformer:0.0.1.
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 除了我们之前提到的“预测器”部分之外，我们添加了另一个部分——“变换器”。我们使用的变换器是一个公开可用的镜像，位于agrigorev/kfserving-keras-transformer:0.0.1。
- en: 'It relies on the keras_image_helper library to do the transformation. For that,
    we need to set three parameters:'
  id: totrans-483
  prefs: []
  type: TYPE_NORMAL
  zh: 它依赖于keras_image_helper库来进行变换。为此，我们需要设置三个参数：
- en: '`MODEL_INPUT_SIZE`: The size of the input that the model expects: 299 x 299'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_INPUT_SIZE`: 模型期望的输入大小：299 x 299'
- en: '`KERAS_MODEL_NAME`: The name of the architecture from Keras applications ([https://keras.io/api/applications/](https://keras.io/api/applications/))
    that we used for training the model'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`KERAS_MODEL_NAME`: 用于训练模型的Keras应用程序（[https://keras.io/api/applications/](https://keras.io/api/applications/)）中架构的名称'
- en: '`MODEL_LABELS`: The classes that we want to predict'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MODEL_LABELS`: 我们想要预测的类别'
- en: 'Let’s apply this config:'
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用这个配置：
- en: '[PRE92]'
  id: totrans-488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Wait a couple of minutes before it becomes ready—use `kubectl` `get` `inferenceservice`
    to check the status.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 等待几分钟，直到它准备好——使用`kubectl get inferenceservice`来检查状态。
- en: After it’s deployed (`READY` is `True`), we can test it. We’ll do that next.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后（`READY`为`True`），我们可以对其进行测试。我们将在下一节进行测试。
- en: 9.4.5 Testing the transformer
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.5 测试变换器
- en: 'With a transformer, we don’t need to worry about preparing the image: it’s
    enough to just send the URL of an image. The code becomes much simpler.'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 使用变换器，我们不需要担心准备镜像：只需发送镜像的URL即可。代码变得更加简单。
- en: 'This is how it looks:'
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 它看起来是这样的：
- en: '[PRE93]'
  id: totrans-494
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: 'The URL of the service stays the same. The result contains the predictions:'
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 服务的URL保持不变。结果包含预测：
- en: '[PRE94]'
  id: totrans-496
  prefs: []
  type: TYPE_PRE
  zh: '[PRE94]'
- en: And that’s all! Now we can use the model.
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！现在我们可以使用这个模型了。
- en: 9.4.6 Deleting the EKS cluster
  id: totrans-498
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.6 删除EKS集群
- en: 'After experimenting with EKS, don’t forget to shut down the cluster. Use `eksctl`
    for that:'
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 在尝试了EKS之后，别忘了关闭集群。使用`eksctl`来完成这个操作：
- en: '[PRE95]'
  id: totrans-500
  prefs: []
  type: TYPE_PRE
  zh: '[PRE95]'
- en: To verify that the cluster was removed, you can check the EKS service page in
    AWS Console.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 要验证集群已被移除，你可以在AWS控制台中检查EKS服务页面。
- en: 9.5 Next steps
  id: totrans-502
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 下一步
- en: You’ve learned the basics you need for training a classification model for predicting
    the type of clothes. We’ve covered a lot of material, but there is a lot more
    to learn than we could fit in this chapter. You can explore this topic more by
    doing the exercises.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学到了训练用于预测衣服类型的分类模型所需的基本知识。我们已经涵盖了大量的内容，但还有更多内容需要学习，而本章无法全部涵盖。你可以通过做练习来更深入地探索这个主题。
- en: 9.5.1 Exercises
  id: totrans-504
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 练习
- en: Docker Compose is a tool for running applications with multiple containers.
    In our example, Gateway needs to communicate with the TF Serving model; that is
    why we need to be able to link them. Docker Compose can help with that. Experiment
    with it for running TF Serving and Gateway locally.
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker Compose是一个用于运行具有多个容器的应用程序的工具。在我们的示例中，网关需要与TF Serving模型通信；这就是为什么我们需要能够将它们链接起来。Docker
    Compose可以帮助我们做到这一点。尝试在本地运行TF Serving和网关。
- en: In this chapter, we used EKS from AWS. For learning Kubernetes, it’s beneficial
    to experiment with Kubernetes locally. Use Minikube or Microk8s to reproduce the
    example with TF Serving and Gateway locally.
  id: totrans-506
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们使用了AWS的EKS。为了学习Kubernetes，在本地进行实验是有益的。使用Minikube或Microk8s在本地重现TF Serving和Gateway的示例。
- en: For all experiments in this chapter, we used the default Kubernetes namespace.
    In practice, we typically use different namespaces for different groups of applications.
    Learn more about namespaces in Kubernetes and then deploy our services in a different
    namespace. For example, you can call it “models.”
  id: totrans-507
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章的所有实验中，我们使用了默认的Kubernetes命名空间。在实际应用中，我们通常为不同的应用程序组使用不同的命名空间。在了解了Kubernetes中的命名空间之后，然后在不同的命名空间中部署我们的服务。例如，你可以称其为“模型”。
- en: KFServing transformers are a powerful tool for preprocessing the data. We haven’t
    discussed how we can implement them ourselves and instead used an already-implemented
    transformer. To learn more about them, implement this transformer yourself.
  id: totrans-508
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KFServing转换器是预处理数据的有力工具。我们尚未讨论如何自行实现它们，而是使用了已经实现的转换器。要了解更多关于它们的信息，请自行实现这个转换器。
- en: 9.5.2 Other projects
  id: totrans-509
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 其他项目
- en: 'There are many projects that you can do to learn Kubernetes and Kubeflow better:'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多项目你可以做来更好地学习Kubernetes和Kubeflow：
- en: In this chapter, we covered a deep learning model. It’s quite complex, and we
    ended up creating two services. Other models we developed before chapter 7 are
    less complex and only require a simple Flask app for hosting them. You can deploy
    the models from chapters 2, 3, and 6 using Flask and Kubernetes.
  id: totrans-511
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了一个深度学习模型。它相当复杂，我们最终创建了两个服务。在第七章之前我们开发的其它模型较为简单，只需要一个简单的Flask应用来托管它们。你可以使用Flask和Kubernetes部署第2章、第3章和第6章中的模型。
- en: KFServing can be used for deploying other types of models, not just TensorFlow.
    Use it for deploying the Scikit-learn models from chapters 3 and 6.
  id: totrans-512
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KFServing可以用于部署其他类型的模型，而不仅仅是TensorFlow。用它来部署第3章和第6章中的Scikit-learn模型。
- en: Summary
  id: totrans-513
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow-Serving is a system for deploying Keras and TensorFlow models. It
    uses gRPC and protobuf for communication, and it’s highly optimized for serving.
  id: totrans-514
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow-Serving是一个用于部署Keras和TensorFlow模型的系统。它使用gRPC和protobuf进行通信，并且高度优化以提供服务。
- en: When using TensorFlow Serving, we typically need a component for preparing the
    user request into the format the model expects. This component hides the complexity
    of interacting with TensorFlow Serving and makes it easier for the clients to
    use the model.
  id: totrans-515
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当使用TensorFlow Serving时，我们通常需要一个组件来将用户请求准备成模型期望的格式。这个组件隐藏了与TensorFlow Serving交互的复杂性，使得客户端更容易使用模型。
- en: 'To deploy something on Kubernetes, we need to create a deployment and a service.
    The deployment describes what should be deployed: the Docker image and its configuration.
    The service sits in front of a deployment and routes requests to individual containers.'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在Kubernetes上部署某些内容，我们需要创建一个部署和一个服务。部署描述了应该部署的内容：Docker镜像及其配置。服务位于部署之前，并将请求路由到各个容器。
- en: 'Kubeflow and KFServing make the deployment process simpler: we need to specify
    only the location to the model, and they take care of creating a deployment, a
    service, and other important things automatically.'
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow和KFServing简化了部署过程：我们只需要指定模型的存储位置，它们会自动处理创建部署、服务和其他重要事项。
- en: KFServing transformers make it easier to preprocess data coming to the model
    and postprocess the results. With transformers, we don’t need to create a special
    Gateway service for preprocessing.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KFServing转换器使得预处理模型接收到的数据和后处理结果变得更加容易。有了转换器，我们不需要为预处理创建一个特殊的网关服务。
