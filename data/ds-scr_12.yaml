- en: Chapter 11\. Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第11章 机器学习
- en: I am always ready to learn although I do not always like being taught.
  id: totrans-1
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 我总是乐意学习，尽管我并不总是喜欢被教导。
- en: ''
  id: totrans-2
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Winston Churchill
  id: totrans-3
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 温斯顿·丘吉尔
- en: Many people imagine that data science is mostly machine learning and that data
    scientists mostly build and train and tweak machine learning models all day long.
    (Then again, many of those people don’t actually know what machine learning *is*.)
    In fact, data science is mostly turning business problems into data problems and
    collecting data and understanding data and cleaning data and formatting data,
    after which machine learning is almost an afterthought. Even so, it’s an interesting
    and essential afterthought that you pretty much have to know about in order to
    do data science.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人想象数据科学主要是机器学习，认为数据科学家整天都在构建、训练和调整机器学习模型。（不过，很多这样想的人其实并不知道机器学习是什么。）事实上，数据科学主要是将业务问题转化为数据问题，收集数据、理解数据、清理数据和格式化数据，而机器学习几乎成了事后的事情。尽管如此，它是一个有趣且必不可少的事后步骤，你基本上必须了解它才能从事数据科学工作。
- en: Modeling
  id: totrans-5
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 建模
- en: Before we can talk about machine learning, we need to talk about *models*.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论机器学习之前，我们需要谈谈*模型*。
- en: What is a model? It’s simply a specification of a mathematical (or probabilistic)
    relationship that exists between different variables.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是模型？简单来说，它是描述不同变量之间数学（或概率）关系的规范。
- en: For instance, if you’re trying to raise money for your social networking site,
    you might build a *business model* (likely in a spreadsheet) that takes inputs
    like “number of users,” “ad revenue per user,” and “number of employees” and outputs
    your annual profit for the next several years. A cookbook recipe entails a model
    that relates inputs like “number of eaters” and “hungriness” to quantities of
    ingredients needed. And if you’ve ever watched poker on television, you know that
    each player’s “win probability” is estimated in real time based on a model that
    takes into account the cards that have been revealed so far and the distribution
    of cards in the deck.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在为你的社交网络站点筹集资金，你可能会建立一个*商业模型*（通常在电子表格中），该模型接受“用户数量”、“每用户广告收入”和“员工数量”等输入，并输出未来几年的年度利润。烹饪食谱涉及一个模型，将“用餐者数量”和“饥饿程度”等输入与所需的食材量联系起来。如果你曾经在电视上观看扑克比赛，你会知道每位玩家的“获胜概率”是根据模型实时估算的，该模型考虑了到目前为止已经公开的牌和牌堆中牌的分布。
- en: 'The business model is probably based on simple mathematical relationships:
    profit is revenue minus expenses, revenue is units sold times average price, and
    so on. The recipe model is probably based on trial and error—someone went in a
    kitchen and tried different combinations of ingredients until they found one they
    liked. And the poker model is based on probability theory, the rules of poker,
    and some reasonably innocuous assumptions about the random process by which cards
    are dealt.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 商业模型可能基于简单的数学关系：利润等于收入减去支出，收入等于销售单位数乘以平均价格，等等。食谱模型可能基于试错法——有人在厨房尝试不同的配料组合，直到找到自己喜欢的那一种。而扑克模型则基于概率论、扑克规则以及关于发牌随机过程的一些合理假设。
- en: What Is Machine Learning?
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 什么是机器学习？
- en: 'Everyone has her own exact definition, but we’ll use *machine learning* to
    refer to creating and using models that are *learned from data*. In other contexts
    this might be called *predictive modeling* or *data mining*, but we will stick
    with machine learning. Typically, our goal will be to use existing data to develop
    models that we can use to *predict* various outcomes for new data, such as:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人都有自己的确切定义，但我们将使用*机器学习*来指代从数据中创建和使用模型的过程。在其他情境下，这可能被称为*预测建模*或*数据挖掘*，但我们将坚持使用机器学习。通常，我们的目标是利用现有数据开发模型，用于预测新数据的各种结果，比如：
- en: Whether an email message is spam or not
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否是垃圾邮件
- en: Whether a credit card transaction is fraudulent
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信用卡交易是否属于欺诈
- en: Which advertisement a shopper is most likely to click on
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪个广告最有可能被购物者点击
- en: Which football team is going to win the Super Bowl
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 哪支橄榄球队会赢得超级碗
- en: We’ll look at both *supervised* models (in which there is a set of data labeled
    with the correct answers to learn from) and *unsupervised* models (in which there
    are no such labels). There are various other types, like *semisupervised* (in
    which only some of the data are labeled), *online* (in which the model needs to
    continuously adjust to newly arriving data), and *reinforcement* (in which, after
    making a series of predictions, the model gets a signal indicating how well it
    did) that we won’t cover in this book.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论*监督*模型（其中有一组带有正确答案标签的数据可供学习）和*无监督*模型（其中没有这些标签）两种模型。还有其他各种类型，比如*半监督*（其中只有部分数据被标记）、*在线*（模型需要持续调整以适应新到达的数据）和*强化*（在做出一系列预测后，模型会收到一个指示其表现如何的信号），这些我们在本书中不会涉及。
- en: Now, in even the simplest situation there are entire universes of models that
    might describe the relationship we’re interested in. In most cases we will ourselves
    choose a *parameterized* family of models and then use data to learn parameters
    that are in some way optimal.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使在最简单的情况下，也有可能有整个宇宙的模型可以描述我们感兴趣的关系。在大多数情况下，我们会自己选择一个*参数化*的模型家族，然后使用数据来学习某种方式上最优的参数。
- en: For instance, we might assume that a person’s height is (roughly) a linear function
    of his weight and then use data to learn what that linear function is. Or we might
    assume that a decision tree is a good way to diagnose what diseases our patients
    have and then use data to learn the “optimal” such tree. Throughout the rest of
    the book, we’ll be investigating different families of models that we can learn.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可能假设一个人的身高（大致上）是他的体重的线性函数，然后使用数据来学习这个线性函数是什么。或者我们可能认为决策树是诊断我们的患者患有哪些疾病的好方法，然后使用数据来学习“最优”这样的树。在本书的其余部分，我们将研究我们可以学习的不同模型家族。
- en: But before we can do that, we need to better understand the fundamentals of
    machine learning. For the rest of the chapter, we’ll discuss some of those basic
    concepts, before we move on to the models themselves.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 但在此之前，我们需要更好地理解机器学习的基本原理。在本章的其余部分，我们将讨论一些基本概念，然后再讨论模型本身。
- en: Overfitting and Underfitting
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 过拟合和欠拟合
- en: A common danger in machine learning is *overfitting*—producing a model that
    performs well on the data you train it on but generalizes poorly to any new data.
    This could involve learning *noise* in the data. Or it could involve learning
    to identify specific inputs rather than whatever factors are actually predictive
    for the desired output.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中一个常见的危险是*过拟合*——生成一个在您训练它的数据上表现良好但泛化性能差的模型。这可能涉及学习数据中的*噪音*。或者可能涉及学习识别特定输入，而不是实际上对所需输出有预测能力的因素。
- en: The other side of this is *underfitting*—producing a model that doesn’t perform
    well even on the training data, although typically when this happens you decide
    your model isn’t good enough and keep looking for a better one.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况的另一面是*拟合不足*——产生一个即使在训练数据上表现也不好的模型，尽管通常在这种情况下，您会认为您的模型还不够好，继续寻找更好的模型。
- en: In [Figure 11-1](#overfitting_and_underfitting), I’ve fit three polynomials
    to a sample of data. (Don’t worry about how; we’ll get to that in later chapters.)
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图11-1](#overfitting_and_underfitting)中，我拟合了三个多项式到一组数据样本中。（不用担心具体方法；我们会在后面的章节中介绍。）
- en: '![Overfitting and Underfitting.](assets/dsf2_1101.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![过拟合和欠拟合。](assets/dsf2_1101.png)'
- en: Figure 11-1\. Overfitting and underfitting
  id: totrans-25
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-1\. 过拟合和欠拟合
- en: The horizontal line shows the best fit degree 0 (i.e., constant) polynomial.
    It severely *underfits* the training data. The best fit degree 9 (i.e., 10-parameter)
    polynomial goes through every training data point exactly, but it very severely
    *overfits*; if we were to pick a few more data points, it would quite likely miss
    them by a lot. And the degree 1 line strikes a nice balance; it’s pretty close
    to every point, and—if these data are representative—the line will likely be close
    to new data points as well.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 水平线显示了最佳拟合度为0（即常数）的多项式。它严重*拟合不足*训练数据。最佳拟合度为9（即10参数）的多项式恰好通过每个训练数据点，但它非常严重*过拟合*；如果我们再选几个数据点，它很可能会严重偏离。而一次拟合度的线条达到了很好的平衡；它非常接近每个点，如果这些数据是代表性的，那么这条线也很可能接近新数据点。
- en: Clearly, models that are too complex lead to overfitting and don’t generalize
    well beyond the data they were trained on. So how do we make sure our models aren’t
    too complex? The most fundamental approach involves using different data to train
    the model and to test the model.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，过于复杂的模型会导致过拟合，并且在训练数据之外不能很好地泛化。那么，我们如何确保我们的模型不会太复杂呢？最基本的方法涉及使用不同的数据来训练模型和测试模型。
- en: 'The simplest way to do this is to split the dataset, so that (for example)
    two-thirds of it is used to train the model, after which we measure the model’s
    performance on the remaining third:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这样做的最简单方法是将数据集分割，例如，将其的三分之二用于训练模型，之后我们可以在剩余的三分之一上测量模型的性能：
- en: '[PRE0]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Often, we’ll have paired input variables and output variables. In that case,
    we need to make sure to put corresponding values together in either the training
    data or the test data:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，我们会有成对的输入变量和输出变量。在这种情况下，我们需要确保将对应的值放在训练数据或测试数据中：
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'As always, we want to make sure our code works right:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 如常，我们要确保我们的代码能够正常工作：
- en: '[PRE2]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'After which you can do something like:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 之后，您可以做一些像这样的事情：
- en: '[PRE3]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: If the model was overfit to the training data, then it will hopefully perform
    really poorly on the (completely separate) test data. Said differently, if it
    performs well on the test data, then you can be more confident that it’s *fitting*
    rather than *overfitting*.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如果模型对训练数据过拟合，那么它在（完全分开的）测试数据上的表现希望会非常差。换句话说，如果它在测试数据上表现良好，那么您可以更有信心它是在*适应*而不是*过拟合*。
- en: However, there are a couple of ways this can go wrong.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，有几种情况可能会出错。
- en: The first is if there are common patterns in the test and training data that
    wouldn’t generalize to a larger dataset.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种情况是测试数据和训练数据中存在的常见模式不会推广到更大的数据集中。
- en: For example, imagine that your dataset consists of user activity, with one row
    per user per week. In such a case, most users will appear in both the training
    data and the test data, and certain models might learn to *identify* users rather
    than discover relationships involving *attributes*. This isn’t a huge worry, although
    it did happen to me once.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，您的数据集包含用户活动，每个用户每周一行。在这种情况下，大多数用户会出现在训练数据和测试数据中，并且某些模型可能会学习*识别*用户而不是发现涉及*属性*的关系。这并不是一个很大的担忧，尽管我曾经遇到过一次。
- en: A bigger problem is if you use the test/train split not just to judge a model
    but also to *choose* from among many models. In that case, although each individual
    model may not be overfit, “choosing a model that performs best on the test set”
    is a meta-training that makes the test set function as a second training set.
    (Of course the model that performed best on the test set is going to perform well
    on the test set.)
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 更大的问题是，如果您不仅用于评估模型而且用于*选择*多个模型。在这种情况下，尽管每个单独的模型可能不会过拟合，“选择在测试集上表现最佳的模型”是一个元训练，使得测试集充当第二个训练集。（当然，在测试集上表现最佳的模型在测试集上表现良好。）
- en: 'In such a situation, you should split the data into three parts: a training
    set for building models, a *validation* set for choosing among trained models,
    and a test set for judging the final model.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您应该将数据分为三部分：用于构建模型的训练集，用于在训练后的模型中进行选择的*验证*集，以及用于评估最终模型的测试集。
- en: Correctness
  id: totrans-42
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 正确性
- en: 'When I’m not doing data science, I dabble in medicine. And in my spare time
    I’ve come up with a cheap, noninvasive test that can be given to a newborn baby
    that predicts—with greater than 98% accuracy—whether the newborn will ever develop
    leukemia. My lawyer has convinced me the test is unpatentable, so I’ll share with
    you the details here: predict leukemia if and only if the baby is named Luke (which
    sounds sort of like “leukemia”).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 当我不从事数据科学时，我涉足医学。在业余时间里，我想出了一种廉价的、无创的测试方法，可以给新生儿做，预测——准确率超过98%——新生儿是否会患白血病。我的律师说服我这个测试方法无法申请专利，所以我会在这里和大家分享详细信息：只有当宝宝被命名为卢克（听起来有点像“白血病”）时，预测白血病。
- en: As we’ll see, this test is indeed more than 98% accurate. Nonetheless, it’s
    an incredibly stupid test, and a good illustration of why we don’t typically use
    “accuracy” to measure how good a (binary classification) model is.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个测试确实有超过98%的准确率。然而，这是一个非常愚蠢的测试，很好地说明了为什么我们通常不使用“准确性”来衡量（二元分类）模型的好坏。
- en: Imagine building a model to make a *binary* judgment. Is this email spam? Should
    we hire this candidate? Is this air traveler secretly a terrorist?
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 想象构建一个用于进行*二进制*判断的模型。这封邮件是垃圾邮件吗？我们应该雇佣这位候选人吗？这位空中旅客是不是秘密的恐怖分子？
- en: 'Given a set of labeled data and such a predictive model, every data point lies
    in one of four categories:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 针对一组标记数据和这样一个预测模型，每个数据点都属于四个类别之一：
- en: True positive
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 真阳性
- en: “This message is spam, and we correctly predicted spam.”
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: “此消息是垃圾邮件，我们正确预测了垃圾邮件。”
- en: False positive (Type 1 error)
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 假阳性（第一类错误）
- en: “This message is not spam, but we predicted spam.”
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: “此消息不是垃圾邮件，但我们预测了垃圾邮件。”
- en: False negative (Type 2 error)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 假阴性（第二类错误）
- en: “This message is spam, but we predicted not spam.”
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: “此消息是垃圾邮件，但我们预测了非垃圾邮件。”
- en: True negative
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 真阴性
- en: “This message is not spam, and we correctly predicted not spam.”
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: “此消息不是垃圾邮件，我们正确预测了非垃圾邮件。”
- en: 'We often represent these as counts in a *confusion matrix*:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常将这些表示为*混淆矩阵*中的计数：
- en: '|  | Spam | Not spam |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|  | 垃圾邮件 | 非垃圾邮件 |'
- en: '| --- | --- | --- |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Predict “spam” | True positive | False positive |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 预测“垃圾邮件” | 真阳性 | 假阳性 |'
- en: '| Predict “not spam” | False negative | True negative |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: 预测“非垃圾邮件” | 假阴性 | 真阴性 |
- en: Let’s see how my leukemia test fits into this framework. These days approximately
    [5 babies out of 1,000 are named Luke](https://www.babycenter.com/baby-names-luke-2918.htm).
    And the lifetime prevalence of leukemia is about 1.4%, or [14 out of every 1,000
    people](https://seer.cancer.gov/statfacts/html/leuks.html).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我的白血病测试如何符合这个框架。 近年来，大约[每 1,000 名婴儿中有 5 名被命名为卢克](https://www.babycenter.com/baby-names-luke-2918.htm)。
    白血病的终身患病率约为 1.4%，或[每 1,000 人中有 14 人](https://seer.cancer.gov/statfacts/html/leuks.html)。
- en: 'If we believe these two factors are independent and apply my “Luke is for leukemia”
    test to 1 million people, we’d expect to see a confusion matrix like:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们相信这两个因素是独立的，并将我的“卢克是用于白血病检测”的测试应用于 1 百万人，我们预计会看到一个混淆矩阵，如下所示：
- en: '|  | Leukemia | No leukemia | Total |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  | 白血病 | 无白血病 | 总计 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| “Luke” | 70 | 4,930 | 5,000 |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| “卢克” | 70 | 4,930 | 5,000 |'
- en: '| Not “Luke” | 13,930 | 981,070 | 995,000 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 非“卢克” | 13,930 | 981,070 | 995,000 |'
- en: '| Total | 14,000 | 986,000 | 1,000,000 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 总计 | 14,000 | 986,000 | 1,000,000 |'
- en: 'We can then use these to compute various statistics about model performance.
    For example, *accuracy* is defined as the fraction of correct predictions:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这些来计算有关模型性能的各种统计信息。 例如，*准确度* 定义为正确预测的分数的比例：
- en: '[PRE4]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: That seems like a pretty impressive number. But clearly this is not a good test,
    which means that we probably shouldn’t put a lot of credence in raw accuracy.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这似乎是一个相当令人印象深刻的数字。 但显然这不是一个好的测试，这意味着我们可能不应该对原始准确性赋予很高的信任。
- en: 'It’s common to look at the combination of *precision* and *recall*. Precision
    measures how accurate our *positive* predictions were:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常会查看*精确度*和*召回率*的组合。 精确度衡量我们的*阳性*预测的准确性：
- en: '[PRE5]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'And recall measures what fraction of the positives our model identified:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 召回率衡量了我们的模型识别出的阳性的分数：
- en: '[PRE6]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: These are both terrible numbers, reflecting that this is a terrible model.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个数字都很糟糕，反映出这是一个糟糕的模型。
- en: 'Sometimes precision and recall are combined into the *F1 score*, which is defined
    as:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 有时精确度和召回率会结合成*F1 分数*，其定义为：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This is the [*harmonic mean*](http://en.wikipedia.org/wiki/Harmonic_mean) of
    precision and recall and necessarily lies between them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 这是[*调和平均*](http://en.wikipedia.org/wiki/Harmonic_mean) 精度和召回率，必然位于它们之间。
- en: Usually the choice of a model involves a tradeoff between precision and recall.
    A model that predicts “yes” when it’s even a little bit confident will probably
    have a high recall but a low precision; a model that predicts “yes” only when
    it’s extremely confident is likely to have a low recall and a high precision.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，模型的选择涉及精确度和召回率之间的权衡。 当模型在稍微有信心时预测“是”可能会具有很高的召回率但较低的精确度； 仅当模型极度自信时才预测“是”可能会具有较低的召回率和较高的精确度。
- en: Alternatively, you can think of this as a tradeoff between false positives and
    false negatives. Saying “yes” too often will give you lots of false positives;
    saying “no” too often will give you lots of false negatives.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以将其视为假阳性和假阴性之间的权衡。 说“是”的次数太多会产生大量的假阳性； 说“不”太多会产生大量的假阴性。
- en: 'Imagine that there were 10 risk factors for leukemia, and that the more of
    them you had the more likely you were to develop leukemia. In that case you can
    imagine a continuum of tests: “predict leukemia if at least one risk factor,”
    “predict leukemia if at least two risk factors,” and so on. As you increase the
    threshold, you increase the test’s precision (since people with more risk factors
    are more likely to develop the disease), and you decrease the test’s recall (since
    fewer and fewer of the eventual disease-sufferers will meet the threshold). In
    cases like this, choosing the right threshold is a matter of finding the right
    tradeoff.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，白血病有 10 个风险因素，而且你拥有的风险因素越多，患白血病的可能性就越大。在这种情况下，你可以想象一系列测试：“如果至少有一个风险因素则预测患白血病”，“如果至少有两个风险因素则预测患白血病”，依此类推。随着阈值的提高，测试的准确性增加（因为拥有更多风险因素的人更有可能患病），而召回率降低（因为越来越少最终患病者将满足阈值）。在这种情况下，选择正确的阈值是找到正确权衡的问题。
- en: The Bias-Variance Tradeoff
  id: totrans-81
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 偏差-方差权衡
- en: Another way of thinking about the overfitting problem is as a tradeoff between
    bias and variance.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考过拟合问题的方式是将其视为偏差和方差之间的权衡。
- en: Both are measures of what would happen if you were to retrain your model many
    times on different sets of training data (from the same larger population).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这两者都是在假设你会在来自同一较大总体的不同训练数据集上多次重新训练模型时会发生的情况的度量。
- en: For example, the degree 0 model in [“Overfitting and Underfitting”](#overfitting)
    will make a lot of mistakes for pretty much any training set (drawn from the same
    population), which means that it has a high *bias*. However, any two randomly
    chosen training sets should give pretty similar models (since any two randomly
    chosen training sets should have pretty similar average values). So we say that
    it has a low *variance*. High bias and low variance typically correspond to underfitting.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[“过拟合和欠拟合”](#overfitting)中的零阶模型在几乎任何训练集上都会犯很多错误（从同一总体中抽取），这意味着它有很高的*偏差*。然而，任意选择的两个训练集应该产生相似的模型（因为任意选择的两个训练集应该具有相似的平均值）。所以我们说它的*方差*很低。高偏差和低方差通常对应欠拟合。
- en: On the other hand, the degree 9 model fit the training set perfectly. It has
    very low bias but very high variance (since any two training sets would likely
    give rise to very different models). This corresponds to overfitting.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，九阶模型完美地适应了训练集。它的偏差非常低，但方差非常高（因为任意两个训练集可能会产生非常不同的模型）。这对应于过拟合。
- en: Thinking about model problems this way can help you figure out what to do when
    your model doesn’t work so well.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式思考模型问题可以帮助你弄清楚当你的模型效果不佳时该怎么做。
- en: If your model has high bias (which means it performs poorly even on your training
    data), one thing to try is adding more features. Going from the degree 0 model
    in [“Overfitting and Underfitting”](#overfitting) to the degree 1 model was a
    big improvement.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型存在高偏差（即使在训练数据上表现也很差），可以尝试的一种方法是添加更多特征。从[“过拟合和欠拟合”](#overfitting)中的零阶模型转换为一阶模型是一个很大的改进。
- en: If your model has high variance, you can similarly *remove* features. But another
    solution is to obtain more data (if you can).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的模型方差很高，你可以类似地*删除*特征。但另一个解决方案是获取更多数据（如果可能的话）。
- en: In [Figure 11-2](#overfit_underfit_more_data), we fit a degree 9 polynomial
    to different size samples. The model fit based on 10 data points is all over the
    place, as we saw before. If we instead train on 100 data points, there’s much
    less overfitting. And the model trained from 1,000 data points looks very similar
    to the degree 1 model. Holding model complexity constant, the more data you have,
    the harder it is to overfit. On the other hand, more data won’t help with bias.
    If your model doesn’t use enough features to capture regularities in the data,
    throwing more data at it won’t help.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 11-2](#overfit_underfit_more_data)中，我们将一个九阶多项式拟合到不同大小的样本上。基于 10 个数据点进行的模型拟合到处都是，正如我们之前看到的。如果我们改为在
    100 个数据点上训练，过拟合就会减少很多。而在 1,000 个数据点上训练的模型看起来与一阶模型非常相似。保持模型复杂性恒定，拥有的数据越多，过拟合就越困难。另一方面，更多的数据对偏差没有帮助。如果你的模型没有使用足够的特征来捕获数据的规律，那么扔更多数据进去是没有帮助的。
- en: '![Reducing Variance With More Data.](assets/dsf2_1102.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![通过增加数据减少方差。](assets/dsf2_1102.png)'
- en: Figure 11-2\. Reducing variance with more data
  id: totrans-91
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图11-2\. 通过增加数据减少方差
- en: Feature Extraction and Selection
  id: totrans-92
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 特征提取和选择
- en: As has been mentioned, when your data doesn’t have enough features, your model
    is likely to underfit. And when your data has too many features, it’s easy to
    overfit. But what are features, and where do they come from?
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，当你的数据没有足够的特征时，你的模型很可能会欠拟合。而当你的数据有太多特征时，很容易过拟合。但特征是什么，它们从哪里来呢？
- en: '*Features* are whatever inputs we provide to our model.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*特征*就是我们向模型提供的任何输入。'
- en: In the simplest case, features are simply given to you. If you want to predict
    someone’s salary based on her years of experience, then years of experience is
    the only feature you have. (Although, as we saw in [“Overfitting and Underfitting”](#overfitting),
    you might also consider adding years of experience squared, cubed, and so on if
    that helps you build a better model.)
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在最简单的情况下，特征只是简单地给出。如果你想根据某人的工作经验预测她的薪水，那么工作经验就是你唯一拥有的特征。（尽管正如我们在[“过拟合和欠拟合”](#overfitting)中看到的那样，如果这有助于构建更好的模型，你可能还会考虑添加工作经验的平方、立方等。）
- en: 'Things become more interesting as your data becomes more complicated. Imagine
    trying to build a spam filter to predict whether an email is junk or not. Most
    models won’t know what to do with a raw email, which is just a collection of text.
    You’ll have to extract features. For example:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 随着数据变得更加复杂，事情变得更加有趣。想象一下试图构建一个垃圾邮件过滤器来预测邮件是否是垃圾的情况。大多数模型不知道如何处理原始邮件，因为它只是一堆文本。你需要提取特征。例如：
- en: Does the email contain the word *Viagra*?
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 邮件中是否包含*Viagra*一词？
- en: How many times does the letter *d* appear?
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 字母*d*出现了多少次？
- en: What was the domain of the sender?
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 发件人的域名是什么？
- en: The answer to a question like the first question here is simply a yes or no,
    which we typically encode as a 1 or 0. The second is a number. And the third is
    a choice from a discrete set of options.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 对于像这里第一个问题的答案，答案很简单，是一个是或否的问题，我们通常将其编码为1或0。第二个问题是一个数字。第三个问题是从一组离散选项中选择的一个选项。
- en: Pretty much always, we’ll extract features from our data that fall into one
    of these three categories. What’s more, the types of features we have constrain
    the types of models we can use.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎总是，我们会从数据中提取属于这三类之一的特征。此外，我们拥有的特征类型限制了我们可以使用的模型类型。
- en: The Naive Bayes classifier we’ll build in [Chapter 13](ch13.html#naive_bayes)
    is suited to yes-or-no features, like the first one in the preceding list.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在第[13章](ch13.html#naive_bayes)中构建的朴素贝叶斯分类器适用于像前面列表中的第一个这样的是或否特征。
- en: Regression models, which we’ll study in Chapters [14](ch14.html#simple_linear_regression)
    and [16](ch16.html#logistic_regression), require numeric features (which could
    include dummy variables that are 0s and 1s).
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在第[14](ch14.html#simple_linear_regression)和第[16](ch16.html#logistic_regression)章中学习的回归模型需要数值特征（可能包括虚拟变量，即0和1）。
- en: And decision trees, which we’ll look at in [Chapter 17](ch17.html#decision_trees),
    can deal with numeric or categorical data.
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将在[第17章](ch17.html#decision_trees)中探讨的决策树可以处理数值或分类数据。
- en: Although in the spam filter example we looked for ways to create features, sometimes
    we’ll instead look for ways to remove features.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在垃圾邮件过滤器示例中我们寻找创建特征的方法，但有时我们会寻找删除特征的方法。
- en: For example, your inputs might be vectors of several hundred numbers. Depending
    on the situation, it might be appropriate to distill these down to a handful of
    important dimensions (as in [“Dimensionality Reduction”](ch10.html#principal_component_analysis))
    and use only that small number of features. Or it might be appropriate to use
    a technique (like regularization, which we’ll look at in [“Regularization”](ch15.html#regularization))
    that penalizes models the more features they use.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，你的输入可能是几百个数字的向量。根据情况，将这些特征简化为几个重要的维度可能是合适的（正如在[“降维”](ch10.html#principal_component_analysis)中所示），然后仅使用这少量的特征。或者可能适合使用一种技术（如我们将在[“正则化”](ch15.html#regularization)中看到的那样），该技术惩罚使用更多特征的模型。
- en: How do we choose features? That’s where a combination of *experience* and *domain
    expertise* comes into play. If you’ve received lots of emails, then you probably
    have a sense that the presence of certain words might be a good indicator of spamminess.
    And you might also get the sense that the number of *d*s is likely not a good
    indicator of spamminess. But in general you’ll have to try different things, which
    is part of the fun.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择特征？这就是经验和领域专业知识结合起来发挥作用的地方。如果你收到了大量的邮件，那么你可能会意识到某些词语的出现可能是垃圾邮件的良好指标。而你可能还会觉得字母*d*的数量可能不是衡量邮件是否是垃圾的好指标。但总的来说，你必须尝试不同的方法，这也是乐趣的一部分。
- en: For Further Exploration
  id: totrans-108
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 进一步探索
- en: Keep reading! The next several chapters are about different families of machine
    learning models.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 继续阅读！接下来的几章讲述不同类型的机器学习模型。
- en: The Coursera [Machine Learning](https://www.coursera.org/course/ml) course is
    the original MOOC and is a good place to get a deeper understanding of the basics
    of machine learning.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Coursera的[机器学习课程](https://www.coursera.org/course/ml)是最早的大规模在线开放课程（MOOC），是深入了解机器学习基础知识的好地方。
- en: '*The Elements of Statistical Learning*, by Jerome H. Friedman, Robert Tibshirani,
    and Trevor Hastie (Springer), is a somewhat canonical textbook that can be [downloaded
    online for free](http://stanford.io/1ycOXbo). But be warned: it’s *very* mathy.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*统计学习的要素*，作者是Jerome H. Friedman、Robert Tibshirani和Trevor Hastie（Springer），是一本可以[免费在线下载](http://stanford.io/1ycOXbo)的经典教材。但请注意：它非常数学化。'
