- en: 'Chapter 4\. Data Ingestion: Extracting Data'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第4章。数据摄取：提取数据
- en: As discussed in [Chapter 3](ch03.xhtml#ch03), the ELT pattern is the ideal design
    for data pipelines built for data analysis, data science, and data products. The
    first two steps in the ELT pattern, extract and load, are collectively referred
    to as *data ingestion*. This chapter discusses getting your development environment
    and infrastructure set up for both, and it goes through the specifics of extracting
    data from various source systems. [Chapter 5](ch05.xhtml#ch05) discusses loading
    the resulting datasets into a data warehouse.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第3章](ch03.xhtml#ch03)中讨论的那样，ELT模式是为数据分析、数据科学和数据产品构建的数据流水线的理想设计。ELT模式的前两个步骤，提取和加载，通常被称为*数据摄取*。本章讨论了为这两个步骤设置开发环境和基础设施的具体步骤，并详细介绍了从各种源系统提取数据的细节。[第5章](ch05.xhtml#ch05)讨论了将结果数据集加载到数据仓库中。
- en: Note
  id: totrans-2
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The extract and load code samples in this chapter are fully decoupled from each
    other. Coordinating the two steps to complete a data ingestion is a topic that’s
    discussed in [Chapter 7](ch07.xhtml#ch07).
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的提取和加载代码示例完全解耦。协调这两个步骤以完成数据摄取是[第7章](ch07.xhtml#ch07)讨论的主题。
- en: As discussed in [Chapter 2](ch02.xhtml#ch02), there are numerous types of source
    systems to extract from, as well as numerous destinations to load into. In addition,
    data comes in many forms, all of which present different challenges for ingesting
    it.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如[第2章](ch02.xhtml#ch02)中讨论的那样，有许多类型的源系统可以提取数据，以及许多目的地可以加载数据。此外，数据以多种形式呈现，每种形式都对其摄取提出了不同的挑战。
- en: This chapter and the next include code samples for exporting and ingesting data
    from and to common systems. The code is highly simplified and contains only minimal
    error handing. Each example is intended as an easy-to-understand starting point
    for data ingestions, but is fully functional and extendable to more scalable solutions.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 本章和下一章包括了从常见系统中导出和摄取数据的代码示例。这些代码简化了高度，仅包含了最小的错误处理。每个示例都旨在作为数据摄取的易于理解的起点，但完全功能且可扩展到更可伸缩的解决方案。
- en: Note
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The code samples in this chapter write extracted data to CSV files to be loaded
    into the destination data warehouse. There are times when it makes more sense
    to store extracted data in another format, such as JSON, prior to loading. Where
    applicable, I note where you might want to consider making such an adjustment.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码示例将提取的数据写入CSV文件，以加载到目标数据仓库中。在某些情况下，将提取的数据存储为其他格式（如JSON）可能更合理。在适用的情况下，我会指出您可能考虑进行这样的调整。
- en: '[Chapter 5](ch05.xhtml#ch05) also discusses some open source frameworks you
    can build off of, and commercial alternatives that give data engineers and analysts
    “low code” options for ingesting data.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '[第5章](ch05.xhtml#ch05)还讨论了一些您可以构建的开源框架，以及为数据工程师和分析师提供“低代码”选项的商业替代方案，用于摄取数据。'
- en: Setting Up Your Python Environment
  id: totrans-9
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置您的Python环境
- en: All code samples that follow are written in Python and SQL and use open source
    frameworks that are common in the data engineering field today. For simplicity,
    the number of sources and destinations is limited. However, where applicable,
    I provide notes on how to modify for similar systems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有后续的代码示例都是用Python和SQL编写的，并使用今天数据工程领域常见的开源框架。为简单起见，源和目的地的数量有限。但在适用的情况下，我提供了如何修改为类似系统的说明。
- en: To run the sample code, you’ll need a physical or virtual machine running Python
    3.x. You’ll also need to install and import a few libraries.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行示例代码，您需要运行Python 3.x的物理或虚拟机。您还需要安装和导入一些库。
- en: If you don’t have Python installed on your machine, you can get the distribution
    and installer for your OS [directly from them](https://oreil.ly/ytH4s).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的计算机上没有安装Python，可以直接从[官网](https://oreil.ly/ytH4s)获取适合您操作系统的分发版和安装程序。
- en: Note
  id: totrans-13
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following commands are written for a Linux or Macintosh command line. On
    Windows, you may need to add the Python 3 executable to your PATH.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 以下命令适用于Linux或Macintosh命令行。在Windows上，您可能需要将Python 3可执行文件添加到您的PATH中。
- en: Before you install the libraries used in this chapter, it’s best to create a
    *virtual environment* to install them into. To do so, you can use a tool called
    `virtualenv`. `virtualenv` is helpful in managing Python libraries for different
    projects and applications. It allows you to install Python libraries within a
    scope specific to your project rather than globally. First, create a virtual environment
    named *env*.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在安装本章使用的库之前，最好创建一个 *虚拟环境* 来安装它们。为此，你可以使用一个名为 `virtualenv` 的工具。`virtualenv` 有助于管理不同项目和应用程序的
    Python 库。它允许你在特定于你的项目的范围内安装 Python 库，而不是全局安装。首先，创建一个名为 *env* 的虚拟环境。
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Now that your virtual environment is created, activate it with the following
    command:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经创建了虚拟环境，请使用以下命令激活它：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'You can verify that your virtual environment is activated in two ways. First,
    you’ll notice that your command prompt is now prefixed by the environment name:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过两种方式验证你的虚拟环境是否已激活。首先，你会注意到你的命令提示符现在前缀为环境名称：
- en: '[PRE2]'
  id: totrans-20
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You can also use the `which python` command to verify where Python is looking
    for libraries. You should see something like this, which shows the path of the
    virtual environment directory:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 你还可以使用 `which python` 命令验证 Python 查找库的位置。你应该会看到类似于这样的输出，显示了虚拟环境目录的路径：
- en: '[PRE3]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now it’s safe to install the libraries you need for the code samples that follow.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 现在安全地安装接下来代码示例需要的库。
- en: Note
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: On some operating systems (OS), you must use `python3` instead of `python` to
    run the Python 3.x executable. Older OS versions may default to Python 2.x. You
    can find out which version of Python your OS uses by typing `python --version`.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些操作系统（OS）上，你必须使用`python3`而不是`python`来运行 Python 3.x 可执行文件。较旧的 OS 版本可能默认使用 Python
    2.x。你可以通过输入`python --version`来查看你的操作系统使用的 Python 版本。
- en: Throughout this chapter, you’ll use `pip` to install the libraries used in the
    code samples. [`pip`](https://pypi.org/project/pip) is a tool that ships with
    most Python distributions.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将使用 `pip` 来安装代码示例中使用的库。[`pip`](https://pypi.org/project/pip) 是大多数 Python
    发行版附带的工具。
- en: The first library you’ll install using `pip` is `configparser`, which will be
    used to read configuration information you’ll add to a file later.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个你将使用 `pip` 安装的库是 `configparser`，它将用于读取稍后添加到文件中的配置信息。
- en: '[PRE4]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Next, create a file named *pipeline.conf* in the same directory as the Python
    scripts you’ll create in the following sections. Leave the file empty for now.
    The code samples in this chapter will call for adding to it. In Linux and Mac
    operating systems, you can create the empty file on the command line with the
    following command:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在接下来的几节中创建的 Python 脚本所在的同一目录中创建一个名为 *pipeline.conf* 的文件。现在先保持文件为空。本章的代码示例将需要向其中添加内容。在
    Linux 和 Mac 操作系统中，你可以使用以下命令在命令行中创建空文件：
- en: '[PRE5]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Setting Up Cloud File Storage
  id: totrans-31
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 设置云文件存储
- en: For each example in this chapter, you’ll be using an Amazon Simple Storage Service
    (Amazon S3 or simply S3) bucket for file storage. S3 is hosted on AWS, and as
    the name implies, S3 is a simple way to store and access files. It’s also very
    cost effective. As of this writing, AWS offers 5 GB of free S3 storage for 12
    months with a new AWS account and charges less than 3 cents USD per month per
    gigabyte for the standard S3 class of storage after that. Given the simplicity
    of the samples in this chapter, you’ll be able to store the necessary data in
    S3 for free if you are still in the first 12 months of creating an AWS account,
    or for less than a $1 a month after that.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的每个示例中，你将使用一个名为亚马逊简单存储服务（Amazon Simple Storage Service，简称 S3）的存储桶来进行文件存储。S3
    托管在 AWS 上，正如其名称所示，S3 是一种简单的存储和访问文件的方式。它还非常经济高效。截至本文撰写时，AWS 提供新 AWS 账户的首个 12 个月免费
    5 GB 的 S3 存储，并在此后的标准 S3 存储类型中每个千兆字节不到 3 美分的费用。考虑到本章示例的简单性，如果你在创建 AWS 账户的前 12 个月内，你可以免费在
    S3 中存储所需数据；或者在此后每月不到 1 美元的费用下进行存储。
- en: To run the samples in this chapter, you’ll need an S3 bucket. Thankfully, creating
    an S3 bucket is simple, and the latest instructions can be found in the [AWS documentation](https://oreil.ly/7W9ZD).
    Setting up the proper access control to the S3 bucket is dependent upon which
    data warehouse you are using. In general, it’s best to use AWS Identity and Access
    Management (IAM) roles for access management policies. Detailed instructions for
    setting up such access for both an Amazon Redshift and Snowflake data warehouse
    are in the sections that follow, but for now, follow the instruction to create
    a new bucket. Name it whatever you’d like; I suggest using the default settings,
    including keeping the bucket private.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 要运行本章中的示例，您将需要一个S3存储桶。幸运的是，创建S3存储桶很简单，最新的说明可以在[AWS文档](https://oreil.ly/7W9ZD)中找到。设置适当的访问控制以访问S3存储桶取决于您使用的数据仓库。通常情况下，最好使用AWS身份和访问管理（IAM）角色进行访问管理策略。有关为Amazon
    Redshift和Snowflake数据仓库设置此类访问的详细说明将在接下来的章节中提供，但现在请按照指示创建一个新的存储桶。可以使用默认设置来命名它，包括保持存储桶私有。
- en: Each extraction example extracts data from the given source system and stores
    the output in the S3 bucket. Each loading example in [Chapter 5](ch05.xhtml#ch05)
    loads that data from the S3 bucket into the destination. This is a common pattern
    in data pipelines. Every major public cloud provider has a service similar to
    S3\. Equivalents on other public clouds are Azure Storage in Microsoft Azure and
    Google Cloud Storage (GCS) in GCP.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 每个抽取示例从给定的源系统中提取数据，并将输出存储在S3存储桶中。[第5章](ch05.xhtml#ch05)中的每个加载示例将数据从S3存储桶加载到目标中。这是数据管道中的常见模式。每个主要的公共云提供商都有类似S3的服务。其他公共云上的等价服务包括Microsoft
    Azure中的Azure存储和GCP中的Google Cloud Storage（GCS）。
- en: It’s also possible to modify each example to use local or on-premises storage.
    However, there is additional work required to load data into your data warehouse
    from storage outside of its specific cloud provider. Regardless, the patterns
    described in this chapter are valid no matter which cloud provider you use, or
    if you choose to host your data infrastructure on-premises.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以修改每个示例以使用本地或本地存储。但是，需要额外的工作来从特定云提供商以外的存储加载数据到您的数据仓库中。无论您使用哪个云提供商，或者选择在本地托管您的数据基础设施，本章描述的模式都是有效的。
- en: 'Before I move on to each example, there’s one more Python library that you’ll
    need to install so that your scripts for extracting and loading can interact with
    your S3 bucket. Boto3 is the AWS SDK for Python. Make sure the virtual environment
    you set up in the previous section is active and use `pip` to install it:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在我继续每个示例之前，还有一个Python库，您需要安装它，以便您的抽取和加载脚本可以与您的S3存储桶进行交互。Boto3是AWS的Python SDK。确保您在前一节中设置的虚拟环境处于活动状态，并使用`pip`安装它：
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'In the examples that follow, you’ll be asked to import `boto3` into your Python
    scripts like this:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，您将被要求像这样将`boto3`导入到您的Python脚本中：
- en: '[PRE7]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Because you’ll be using the `boto3` Python library to interact with your S3
    bucket, you’ll also need to create an IAM user, generate access keys for that
    user, and store the keys in a configuration file that your Python scripts can
    read from. This is all necessary so that your scripts have permissions to read
    and write files in your S3 bucket.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您将使用`boto3` Python库与您的S3存储桶进行交互，所以您还需要创建一个IAM用户，为该用户生成访问密钥，并将密钥存储在配置文件中，以便您的Python脚本可以读取。这些都是必需的，以便您的脚本具有权限在您的S3存储桶中读取和写入文件。
- en: 'First, create the IAM user:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建IAM用户：
- en: Under the Services menu in the AWS console (or top nav bar), navigate to IAM.
  id: totrans-42
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在AWS控制台的服务菜单（或顶部导航栏）下，导航至IAM。
- en: In the navigation pane, click Users and then click “Add user.” Type the username
    for the new user. In this example, name the user *data_pipeline_readwrite*.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在导航窗格中，点击“用户”，然后点击“添加用户”。键入新用户的用户名。在本例中，将用户命名为*data_pipeline_readwrite*。
- en: Click the type of access for this IAM user. Click “programmatic access” since
    this user won’t need to log into the AWS Console, but rather access AWS resources
    programmatically via Python scripts.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为此IAM用户选择访问类型。选择“程序化访问”，因为此用户不需要登录到AWS控制台，而是通过Python脚本以编程方式访问AWS资源。
- en: 'Click Next: Permissions.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“下一步：权限”。
- en: On the “Set permissions” page, click the “Attach existing policies to user directly”
    option. Add the AmazonS3FullAccess policy.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在“设置权限”页面上，点击“直接为用户附加现有策略”选项。添加AmazonS3FullAccess策略。
- en: 'Click Next: Tags. It’s a best practice in AWS to add tags to various objects
    and services so you can find them later. This is optional, however.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“下一步：标签”。在 AWS 中，向各种对象和服务添加标签以便稍后查找是最佳实践。但这是可选的。
- en: 'Click Next: Review to verify your settings. If everything looks good, click
    “Create user.”'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 点击“下一步：审阅”以验证您的设置。如果一切正常，请点击“创建用户”。
- en: You’ll want to save the access key ID and secret access key for the new IAM
    user. To do so, click Download.csv and then save the file to a safe location so
    you can use it in just a moment.
  id: totrans-49
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 您需要保存新 IAM 用户的访问密钥 ID 和秘密访问密钥。为此，请单击“下载 .csv” 然后将文件保存到安全位置，以便稍后使用。
- en: 'Finally, add a section to the *pipeline.conf* file called `[aws_boto_credentials]`
    to store the credentials for the IAM user and the S3 bucket information. You can
    find your AWS account ID by clicking your account name at the top right of any
    page when logged into the AWS site. Use the name of the S3 bucket you created
    earlier for the `bucket_name` value. The new section in *pipline.conf* will look
    like this:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在 *pipeline.conf* 文件中添加一个名为 `[aws_boto_credentials]` 的部分，以存储 IAM 用户和 S3 存储桶信息的凭证。您可以在登录
    AWS 网站后点击右上角的帐户名找到您的 AWS 帐户 ID。将您之前创建的 S3 存储桶的名称用作 `bucket_name` 的值。*pipeline.conf*
    文件中的新部分将如下所示：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Extracting Data from a MySQL Database
  id: totrans-52
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 MySQL 数据库提取数据
- en: 'Extracting data from a MySQL database can be done in two ways:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 从 MySQL 数据库提取数据可以通过两种方式完成：
- en: Full or incremental extraction using SQL
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 SQL 进行全量或增量提取
- en: Binary Log (binlog) replication
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制日志（binlog）复制
- en: Full or incremental extraction using SQL is far simpler to implement, but also
    less scalable for large datasets with frequent changes. There are also trade-offs
    between full and incremental extractions that I discuss in the following section.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 SQL 进行全量或增量提取较为简单，但对于数据变化频繁的大型数据集来说，可扩展性较差。全量和增量提取之间也存在一些权衡，我将在下一节中讨论。
- en: Binary Log replication, though more complex to implement, is better suited to
    cases where the data volume of changes in source tables is high, or there is a
    need for more frequent data ingestions from the MySQL source.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然二进制日志复制实现起来更复杂，但更适合源表数据变更量大或需要更频繁地从 MySQL 源中摄取数据的情况。
- en: Note
  id: totrans-58
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Binlog replication is also a path to creating a *streaming data ingestion*.
    See the “Batch Versus Stream Ingestion” section of this chapter for more on the
    distinction between the two approaches as well as implementation patterns.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 二进制日志复制也是创建*流式数据摄入*的一种方式。有关这两种方法之间以及实现模式的区别，请参阅本章的“批处理与流摄入”部分。
- en: This section is relevant to those readers who have a MySQL data source they
    need to extract data from. However, if you’d like to set up a simple database
    so you can try the code samples, you have two options. First, you can install
    MySQL on your local machine or virtual machine for free. You can find an installer
    for your OS on the [MySQL downloads page](https://oreil.ly/p2-L1).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 本节内容适用于那些需要从 MySQL 数据源中提取数据的读者。但是，如果您想设置一个简单的数据库以便尝试代码示例，您有两个选择。首先，您可以免费在本地机器或虚拟机上安装
    MySQL。您可以在[MySQL 下载页面](https://oreil.ly/p2-L1)找到适用于您操作系统的安装程序。
- en: Alternatively, you can create a fully managed Amazon RDS for MySQL instance
    in [AWS](https://oreil.ly/XahtN). I find this method more straightforward, and
    it’s nice not to create unnecessary clutter on my local machine!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以在[AWS](https://oreil.ly/XahtN)中创建一个完全托管的 Amazon RDS for MySQL 实例。我发现这种方法更加直观，而且不会在本地机器上创建不必要的混乱！
- en: Warning
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: When you follow the [linked instructions](https://oreil.ly/CiVgC) to set up
    an MySQL RDS database instance, you’ll be prompted to set your database as publicly
    accessible. That’s just fine for learning and working with sample data. In fact,
    it makes it much easier to connect from whatever machine you’re running the samples
    in this section. However, for more robust security in a production setting, I
    suggest following the [Amazon RDS security best practices](https://oreil.ly/8DYsz).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当您按照[链接指南](https://oreil.ly/CiVgC)设置 MySQL RDS 数据库实例时，您将被提示将数据库设置为公共访问。这对于学习和处理示例数据非常合适。事实上，这样做可以更轻松地从您运行本节示例的任何计算机连接。但是，在生产环境中为了更强大的安全性，建议遵循[Amazon
    RDS 安全最佳实践](https://oreil.ly/8DYsz)。
- en: Note that just like the S3 pricing noted earlier, if you are no longer eligible
    for the free tier of AWS, there is a cost associated with doing so. Otherwise,
    it’s free to set up and run! Just remember to delete your RDS instance when you’re
    done so you don’t forget and incur charges when your free tier expires.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，就像前面提到的S3定价一样，如果您不再符合AWS的免费套餐条件，这将产生费用。否则，设置和运行是免费的！只需记得在完成时删除您的RDS实例，以免忘记并在免费套餐过期时产生费用。
- en: 'The code samples in this section are quite simple and refer to a table named
    `Orders` in a MySQL database. Once you have a MySQL instance to work with, you
    can create the table and insert some sample rows by running the following SQL
    commands:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码示例非常简单，并参考了MySQL数据库中名为`Orders`的表。一旦有了MySQL实例可以使用，你可以通过运行以下SQL命令创建表并插入一些示例行：
- en: '[PRE9]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Full or Incremental MySQL Table Extraction
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MySQL表的全量或增量抽取
- en: When you need to ingest either all or a subset of columns from a MySQL table
    into a data warehouse or data lake, you can do so using either full extraction
    or incremental extraction.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 当你需要将MySQL表中的所有或部分列导入到数据仓库或数据湖中时，可以使用全量抽取或增量抽取。
- en: 'In a *full extraction*, every record in the table is extracted on each run
    of the extraction job. This is the least complex approach, but for high-volume
    tables it can take a long time to run. For example, if you want to run a full
    extraction on a table called `Orders`, the SQL executed on the source MySQL database
    will look like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在*全量抽取*中，每次运行抽取作业时都会提取表中的每条记录。这是最简单的方法之一，但对于高容量的表来说，运行时间可能会很长。例如，如果你想对名为`Orders`的表进行全量抽取，在源MySQL数据库上执行的SQL如下所示：
- en: '[PRE10]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'In an *incremental extraction*, only records from the source table that have
    changed or been added since the last run of the job are extracted. The timestamp
    of the last extraction can either be stored in an extraction job log table in
    the data warehouse or retrieved by querying the maximum timestamp in a `LastUpdated`
    column in the destination table in the warehouse. Using the fictional `Orders`
    table as an example, the SQL query executed on the source MySQL database will
    look like this:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在*增量抽取*中，只提取源表中自上次作业运行以来发生更改或新增的记录。上次抽取的时间戳可以存储在数据仓库中的抽取作业日志表中，或者通过查询仓库中目标表中`LastUpdated`列的最大时间戳来检索。以虚构的`Orders`表为例，执行在源MySQL数据库上执行的SQL查询如下所示：
- en: '[PRE11]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Note
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: For tables containing *immutable data* (meaning records can be inserted, but
    not updated), you can make use of the timestamp for when the record was created
    instead of a `LastUpdated` column.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于包含*不可变数据*（即可以插入但不能更新记录的数据），可以使用记录创建时间戳而不是`LastUpdated`列。
- en: 'The `{{ last_extraction_run }}` variable is a timestamp representing the most
    recent run of the extraction job. Most commonly it’s queried from the destination
    table in the data warehouse. In that case, the following SQL would be executed
    in the data warehouse, with the resulting value used for `{{ last_extraction_run
    }}`:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '`{{ last_extraction_run }}`变量是表示最近一次抽取作业运行的时间戳。通常情况下，它是从数据仓库中的目标表中查询的。在这种情况下，将在数据仓库中执行以下SQL，并使用结果值作为`{{
    last_extraction_run }}`使用：'
- en: '[PRE12]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Though incremental extraction is ideal for optimal performance, there are some
    downsides and reasons why it may not be possible for a given table.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管增量抽取对于性能优化是理想的，但也存在一些缺点和可能使得某个表无法使用的原因。
- en: First, with this method deleted, rows are not captured. If a row is deleted
    from the source MySQL table, you won’t know, and it will remain in the destination
    table as if nothing changed.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用此方法删除时，不会捕获行。如果从源MySQL表中删除行，则不会知道，并且它将保留在目标表中，就像没有发生任何更改一样。
- en: Second, the source table must have a reliable timestamp for when it was last
    updated (the `LastUpdated` column in the previous example). It’s not uncommon
    for source system tables to be missing such a column or have one that is not updated
    reliably. There’s nothing stopping developers from updating records in the source
    table and forgetting to update the `LastUpdated` timestamp.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，源表必须具有可靠的时间戳来标记其上次更新的时间（如前面示例中的`LastUpdated`列）。源系统表中缺少这样的列或者列的更新不可靠并不少见。开发人员可以随意更新源表中的记录，而忘记更新`LastUpdated`时间戳。
- en: However, incremental extraction does make it easier to capture updated rows.
    In the upcoming code samples, if a particular row in the `Orders` table is updated,
    both the full and incremental extractions will bring back the latest version of
    the row. In the full extract, that’s true for all rows in the table as the extraction
    retrieves a full copy of the table. In the incremental extraction, only rows that
    have changed are retrieved.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，增量抽取确实使捕捉更新行变得更容易。在即将出现的代码示例中，如果`Orders`表中的特定行被更新，那么无论是全量还是增量抽取都将带回行的最新版本。在全量抽取中，这对表中的所有行都适用，因为抽取会检索表的完整副本。在增量抽取中，只检索已更改的行。
- en: When it comes time for the load step, full extracts are usually loaded by first
    truncating the destination table and loading in the newly extracted data. In that
    case, you’re left with only the latest version of the row in the data warehouse.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 当进行加载步骤时，通常通过首先截断目标表并加载新抽取的数据来加载全量抽取数据。在这种情况下，您将仅在数据仓库中留下行的最新版本。
- en: When loading data from an incremental extraction, the resulting data is appended
    to the data in the destination table. In that case, you have both the original
    record as well as the updated version. Having both can be valuable when it comes
    time to transform and analyze data, as I discuss in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 从增量抽取加载数据时，结果数据将追加到目标表中的数据中。在这种情况下，你将同时拥有原始记录和更新后的版本。当进行数据转换和分析时，同时拥有这两者可能非常有价值，正如我在[第
    6 章](ch06.xhtml#ch06)中讨论的那样。
- en: For example, [Table 4-1](#original_state_of_orderid_1) shows the original record
    for `OrderId` 1 in the MySQL database. When the order was placed by the customer,
    it was on back order. [Table 4-2](#original_state_of_orderid_2) shows the updated
    record in the MySQL database. As you can see, the order was updated because it
    shipped on 2020-06-09.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，[表 4-1](#original_state_of_orderid_1)显示了MySQL数据库中订单ID为1的原始记录。当客户下订单时，订单处于缺货后补状态。[表
    4-2](#original_state_of_orderid_2)显示了MySQL数据库中的更新记录。如您所见，该订单已更新，因为在2020-06-09发货。
- en: Table 4-1\. Original state of OrderId 1
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-1\. 订单ID 1的原始状态
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 订单ID | 订单状态 | 最后更新时间 |'
- en: '| --- | --- | --- |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Backordered | 2020-06-01 12:00:00 |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 缺货后补 | 2020-06-01 12:00:00 |'
- en: Table 4-2\. Updated state of OrderId 1
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-2\. 订单ID 1的更新状态
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 订单ID | 订单状态 | 最后更新时间 |'
- en: '| --- | --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Shipped | 2020-06-09 12:00:25 |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 12:00:25 |'
- en: When a full extraction is run, the destination table in the data warehouse is
    first truncated and then loaded with the output of the extraction. The result
    for `OrderId` 1 is the single record shown in [Table 4-2](#original_state_of_orderid_2).
    In an incremental extraction, however, the output of the extract is simply appended
    to the destination table in the data warehouse. The result is both the original
    and updated records for `OrderId` 1 being in the data warehouse, as illustrated
    in [Table 4-3](#all_versions_of_orderid_1_in_the_data_warehouse).
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行全量抽取时，数据仓库中的目标表首先被截断，然后加载抽取的输出数据。`订单ID`为1的结果是只显示在[表 4-2](#original_state_of_orderid_2)中的单个记录。然而，在增量抽取中，抽取的输出数据简单地追加到数据仓库的目标表中。结果是，订单ID为1的原始和更新记录都在数据仓库中，如[表
    4-3](#all_versions_of_orderid_1_in_the_data_warehouse)所示。
- en: Table 4-3\. All versions of OrderId 1 in the data warehouse
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4-3\. 数据仓库中订单ID 1的所有版本
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| 订单ID | 订单状态 | 最后更新时间 |'
- en: '| --- | --- | --- |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Backordered | 2020-06-01 12:00:00 |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 缺货后补 | 2020-06-01 12:00:00 |'
- en: '| 1 | Shipped | 2020-06-09 12:00:25 |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 12:00:25 |'
- en: You can learn more about loading full and incremental extractions in sections
    of [Chapter 5](ch05.xhtml#ch05) including [“Loading Data into a Redshift Warehouse”](ch05.xhtml#load-data-redshift).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在[第 5 章](ch05.xhtml#ch05)的相关部分了解更多关于加载全量和增量抽取的信息，包括[“将数据加载到Redshift数据仓库”](ch05.xhtml#load-data-redshift)。
- en: Warning
  id: totrans-99
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Never assume a `LastUpdated` column in a source system is reliably updated.
    Check with the owner of the source system and confirm before relying on it for
    an incremental extraction.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 永远不要假设源系统中的`LastUpdated`列会被可靠地更新。在依赖它进行增量抽取之前，请与源系统的所有者确认。
- en: 'Both full and incremental extractions from a MySQL database can be implemented
    using SQL queries executed on the database but triggered by Python scripts. In
    addition to the Python libraries installed in previous sections, you’ll need to
    install the `PyMySQL` library using `pip`:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用在数据库上执行的SQL查询实现MySQL数据库的完整和增量提取，但由Python脚本触发。除了在前几节安装的Python库外，您还需要使用`pip`安装`PyMySQL`库：
- en: '[PRE13]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'You’ll also need to add a new section to the *pipeline.conf* file to store
    the connection information for the MySQL database:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 您还需要向*pipeline.conf*文件添加新的部分，以存储MySQL数据库的连接信息：
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now create a new Python script named *extract_mysql_full.py*. You’ll need to
    import several libraries, such as `pymysql`, which connects to the MySQL database,
    and the `csv` library so that you can structure and write out the extracted data
    in a flat file that’s easy to import into a data warehouse in the load step of
    ingestion. Also, import `boto3` so that you can upload the resulting CSV file
    to your S3 bucket for later loading into the data warehouse:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个名为*extract_mysql_full.py*的新Python脚本。您需要导入几个库，例如连接到MySQL数据库的`pymysql`，以及`csv`库，这样您可以在摄入过程的加载步骤中结构化并写出提取的数据为易于导入数据仓库的平面文件。还导入`boto3`，这样您可以将生成的CSV文件上传到您的S3桶，以便稍后加载到数据仓库：
- en: '[PRE15]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Now you can initialize a connection to the MySQL database:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你可以初始化到MySQL数据库的连接：
- en: '[PRE16]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Run a full extraction of the `Orders` table from the earlier example. The following
    code will extract the entire contents of the table and write it to a pipe-delimited
    CSV file. To perform the extraction, it uses a `cursor` object from the `pymysql`
    library to execute the SELECT query:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 从之前的示例中完全提取`Orders`表。以下代码将提取整个表的内容并将其写入管道分隔的CSV文件中。为执行提取操作，它使用`pymysql`库中的`cursor`对象执行SELECT查询：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Now that the CSV file is written locally, it needs to be uploaded to the S3
    bucket for later loading into the data warehouse or other destination. Recall
    from [“Setting Up Cloud File Storage”](#setup-cloud-storage) that you set up an
    IAM user for the Boto3 library to use for authentication to the S3 bucket. You
    also stored the credentials in the `aws_boto_credentials` section of the *pipeline.conf*
    file. Here is the code to upload the CSV file to your S3 bucket:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在CSV文件已在本地编写，需要将其上传到S3桶中，以便稍后加载到数据仓库或其他目的地。从[“设置云文件存储”](#setup-cloud-storage)中回忆，您已设置了IAM用户供Boto3库使用，以对S3桶进行身份验证。您还将凭证存储在*pipeline.conf*文件的`aws_boto_credentials`部分。以下是将CSV文件上传到您的S3桶的代码：
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'You can execute the script as follows:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以按照以下步骤执行脚本：
- en: '[PRE19]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: When the script is executed, the entire contents of the `Orders` table is now
    contained in a CSV file sitting in the S3 bucket waiting to be loaded into the
    data warehouse or other data store. See [Chapter 5](ch05.xhtml#ch05) for more
    on loading into the data store of your choice.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行脚本时，`Orders`表的整个内容现在包含在等待加载到数据仓库或其他数据存储中的S3桶中的CSV文件中。有关加载到所选数据存储的更多信息，请参见[第5章](ch05.xhtml#ch05)。
- en: If you want to extract data incrementally, you’ll need to make a few changes
    to the script. I suggest creating a copy of *extract_mysql_full.py* named *extract_mysql_incremental.py*
    as a starting point.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如果要增量提取数据，您需要对脚本进行一些更改。建议将*extract_mysql_full.py*的副本命名为*extract_mysql_incremental.py*作为起点。
- en: First, find the timestamp of the last record that was extracted from the source
    `Orders` table. To do that, query the `MAX(LastUpdated)` value from the `Orders`
    table in the data warehouse. In this example, I’ll use a Redshift data warehouse
    (see [“Configuring an Amazon Redshift Warehouse as a Destination”](ch05.xhtml#configure-amazon-redshift)),
    but you can use the same logic with the warehouse of your choice.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，查找从源`Orders`表中提取的最后记录的时间戳。为此，请查询数据仓库中`Orders`表的`MAX(LastUpdated)`值。在本示例中，我将使用Redshift数据仓库（参见[“配置亚马逊Redshift仓库作为目标”](ch05.xhtml#configure-amazon-redshift)），但您也可以使用您选择的仓库。
- en: To interact with your Redshift cluster, install the `psycopg2` library, if you
    haven’t already.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 要与您的Redshift集群交互，请安装`psycopg2`库（如果尚未安装）。
- en: '[PRE20]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Here is the code to connect to and query the Redshift cluster to get the `MAX(LastUpdated)`
    value from the `Orders` table:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是连接并从Redshift集群查询`Orders`表中的`MAX(LastUpdated)`值的代码：
- en: '[PRE21]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Using the value stored in `last_updated_warehouse`, modify the extraction query
    run on the MySQL database to pull only those records from the `Orders` table that
    have been updated since the prior run of the extraction job. The new query contains
    a placeholder, represented by `%s` for the `last_updated_warehouse` value. The
    value is then passed into the cursor’s `.execute()` function as a tuple (a data
    type used to store collections of data). This is the proper and secure way to
    add parameters to a SQL query to avoid possible SQL injection. Here is the updated
    code block for running the SQL query on the MySQL database:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 使用存储在 `last_updated_warehouse` 中的值，修改在 MySQL 数据库上运行的抽取查询，仅拉取自上次运行抽取作业以来已更新的
    `Orders` 表记录。新查询包含一个占位符 `%s`，用于 `last_updated_warehouse` 值。然后将该值作为元组传递给游标的 `.execute()`
    函数（一种用于存储数据集合的数据类型）。这是向 SQL 查询添加参数的正确和安全方式，以避免可能的 SQL 注入。以下是在 MySQL 数据库上运行 SQL
    查询的更新代码块：
- en: '[PRE22]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'The entire *extract_mysql_incremental.py* script for the incremental extraction
    (using a Redshift cluster for the `last_updated` value) looks like this:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 用于增量抽取的整个 *extract_mysql_incremental.py* 脚本（使用 Redshift 集群作为 `last_updated`
    值）如下所示：
- en: '[PRE23]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Warning
  id: totrans-126
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Beware of large extraction jobs—whether full or incremental—putting strain on
    the source MySQL database, and even blocking production queries from executing.
    Consult with the owner of the database and consider setting up a replica to extract
    from, rather than extracting from the primary source database.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 要注意大型的抽取作业，无论是完整的还是增量的，可能会给源 MySQL 数据库造成压力，甚至会阻塞生产查询的执行。请与数据库所有者咨询，并考虑设置一个副本用于抽取，而不是直接从主源数据库抽取。
- en: Binary Log Replication of MySQL Data
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MySQL 数据的二进制日志复制
- en: Though more complex to implement, ingesting data from a MySQL database using
    the contents of the MySQL binlog to replicate changes is efficient in cases of
    high-volume ingestion needs.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然实施起来更复杂，但使用 MySQL 数据库的 MySQL binlog 内容复制变更在高容量摄入需求的情况下效率高。
- en: Note
  id: totrans-130
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Binlog replication is a form of change data capture (CDC). Many source data
    stores have some form of CDC that you can use.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Binlog 复制是一种变更数据捕获（CDC）形式。许多源数据存储都有一些 CDC 形式，可以用来使用。
- en: The MySQL binlog is a log that keeps a record of every operation performed in
    the database. For example, depending on how it’s configured, it will log the specifics
    of every table creation or modification, as well as every `INSERT`, `UPDATE`,
    and `DELETE` operation. Though originally intended to replicate data to other
    MySQL instances, it’s not hard to see why the contents of the binlog are so appealing
    to data engineers who want to ingest data into a data warehouse.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: MySQL binlog 是一个日志，记录数据库中执行的每个操作。例如，根据其配置方式，它将记录每个表的创建或修改的具体信息，以及每个 `INSERT`、`UPDATE`
    和 `DELETE` 操作。尽管最初是用于将数据复制到其他 MySQL 实例，但很容易理解为什么数据工程师对 binlog 的内容如此感兴趣，希望将数据摄入数据仓库中。
- en: 'Because your data warehouse is likely not a MySQL database, it’s not possible
    to simply use the built-in MySQL replication features. To make use of the binlog
    for data ingestion to a non-MySQL source, there are a number of steps to take:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 因为您的数据仓库可能不是 MySQL 数据库，所以不能简单地使用内置的 MySQL 复制功能。为了利用 binlog 将数据摄入到非 MySQL 源中，需要采取一些步骤：
- en: Enable and configure the binlog on the MySQL server.
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 启用并配置 MySQL 服务器上的 binlog。
- en: Run an initial full table extraction and load.
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行初始的完整表抽取和加载。
- en: Extract from the binlog on a continuous basis.
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续从二进制日志中进行抽取。
- en: Translate and load binlog extracts into the data warehouse.
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将二进制日志的抽取内容翻译并加载到数据仓库中。
- en: Note
  id: totrans-138
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Step 3 is not discussed in detail, but to use the binlog for ingestion, you
    must first populate the tables in the data warehouse with the current state of
    the MySQL database and then use the binlog to ingest subsequent changes. Doing
    so often involves putting a `LOCK` on the tables you want to extract, running
    a `mysqldump` of those tables, and then loading the result of the `mysqldump`
    into the warehouse before turning on the binlog ingestion.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 第三步没有详细讨论，但要使用 binlog 进行摄入，您必须首先使用当前 MySQL 数据库的内容填充数据仓库中的表，然后使用 binlog 摄入后续的变更。通常需要对要抽取的表进行
    `LOCK`，运行这些表的 `mysqldump`，然后将 `mysqldump` 的结果加载到数据仓库中，然后再打开 binlog 摄入。
- en: Though it’s best to refer to the latest MySQL [binlog documentation](https://oreil.ly/2Vdyf)
    for instructions in enabling and configuring binary logging, I will walk through
    the key configuration values.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然最好参考最新的 MySQL [binlog 文档](https://oreil.ly/2Vdyf) 获取有关启用和配置二进制日志的指导，我将介绍关键的配置值。
- en: There are two key settings to ensure on the MySQL database in regard to binlog
    configuration.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 MySQL 数据库中，关于 binlog 配置，有两个关键设置需要确保。
- en: 'First, ensure that binary logging is enabled. Typically it is enabled by default,
    but you can check by running the following SQL query on the database (exact syntax
    may vary by MySQL distribution):'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保启用了二进制日志。通常情况下，默认情况下启用了它，但是您可以通过在数据库上运行以下 SQL 查询来检查（确切的语法可能因 MySQL 分发而异）：
- en: '[PRE24]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If the binary logging is enabled, you’ll see the following. If the status returned
    is `OFF`, then you’ll need to consult the MySQL documentation for the relevant
    version to enable it.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果启用了二进制日志，您将看到以下内容。如果返回的状态是`OFF`，则需要查阅相关版本的 MySQL 文档以启用它。
- en: '[PRE25]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Next, ensure that the binary logging format is set appropriately. There are
    three formats supported in the recent version of MySQL:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，请确保二进制日志格式设置正确。在最近版本的 MySQL 中支持三种格式：
- en: '`STATEMENT`'
  id: totrans-147
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`STATEMENT`'
- en: '`ROW`'
  id: totrans-148
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`ROW`'
- en: '`MIXED`'
  id: totrans-149
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MIXED`'
- en: The `STATEMENT` format logs every SQL statement that inserts or modifies a row
    in the binlog. If you wanted to replicate data from one MySQL database to another,
    this format is useful. To replicate the data, you could just run all statements
    to reproduce the state of the database. However, because the extracted data is
    likely bound for a data warehouse running on a different platform, the SQL statements
    produced in the MySQL database may not be compatible with your data warehouse.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '`STATEMENT` 格式记录了将 SQL 语句插入或修改行的每个 SQL 语句到 binlog 中。如果您想要从一个 MySQL 数据库复制数据到另一个，这种格式是有用的。为了复制数据，您可以运行所有语句以重现数据库的状态。但是，由于提取的数据可能用于运行在不同平台上的数据仓库，MySQL
    数据库中生成的 SQL 语句可能与您的数据仓库不兼容。'
- en: With the `ROW` format, every change to a row in a table is represented on a
    line of the binlog not as a SQL statement but rather the data in the row itself.
    This is the preferred format to use.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`ROW`格式，表中每行的每个更改都表示为 binlog 的一行，而不是 SQL 语句本身。这是首选的格式。
- en: The `MIXED` format logs both `STATEMENT`- and `ROW`-formatted records in the
    binlog. Though it’s possible to sift out just the `ROW` data later, unless the
    binlog is being used for another purpose, it’s not necessary to enable `MIXED`,
    given the additional disk space that it takes up.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`MIXED` 格式在 binlog 中记录了`STATEMENT`和`ROW`格式的记录。尽管以后可以筛选出只有`ROW`数据，但除非 binlog
    用于其他目的，否则没有必要启用`MIXED`，因为它会占用额外的磁盘空间。'
- en: 'You can verify the current binlog format by running the following SQL query:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行以下 SQL 查询来验证当前的 binlog 格式：
- en: '[PRE26]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'The statement will return the format that’s currently active:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 该语句将返回当前活动的格式：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The binlog format as well as other configuration settings are typically set
    in the *my.cnf* file specific to the MySQL database instance. If you open the
    file, you’ll see a row like the following included:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: binlog 格式以及其他配置设置通常在特定于 MySQL 数据库实例的 *my.cnf* 文件中设置。如果打开文件，您会看到包括以下内容的行：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Again, it’s best to consult with the owner of the MySQL database or the latest
    MySQL documentation before modifying any configurations.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，在修改任何配置之前，最好先与 MySQL 数据库的所有者或最新的 MySQL 文档进行咨询。
- en: Now that binary logging is enabled in a `ROW` format, you can build a process
    to extract the relevant information from it and store it in a file to be loaded
    into your data warehouse.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在二进制日志以`ROW`格式启用，您可以构建一个流程来从中提取相关信息，并将其存储在一个文件中，以便加载到您的数据仓库中。
- en: 'There are three different types of `ROW`-formatted events that you’ll want
    to pull from the binlog. For the sake of this ingestion example, you can ignore
    other events you find in the log, but in more advanced replication strategies,
    extracting events that modify the structure of a table is also of value. The events
    that you’ll work with are as follows:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同类型的`ROW`格式事件，您希望从 binlog 中提取。在此摄取示例中，您可以忽略日志中找到的其他事件，但在更高级的复制策略中，提取修改表结构的事件也是有价值的。您将处理的事件如下：
- en: '`WRITE_ROWS_EVENT`'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`WRITE_ROWS_EVENT`'
- en: '`UPDATE_ROWS_EVENT`'
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`UPDATE_ROWS_EVENT`'
- en: '`DELETE_ROWS_EVENT`'
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`DELETE_ROWS_EVENT`'
- en: 'Next, it’s time to get the events from the binlog. Thankfully, there are some
    open source Python libraries available to get you started. One of the most popular
    is the `python-mysql-replication` project, which can be found on [GitHub](https://oreil.ly/QqBSu).
    To get started, install it using `pip`:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 紧接着，是时候从binlog获取事件了。幸运的是，有一些开源的Python库可以帮助你入门。其中最流行的之一是`python-mysql-replication`项目，可以在[GitHub](https://oreil.ly/QqBSu)上找到。要开始使用，可以使用`pip`安装它：
- en: '[PRE29]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: To get an idea of what the output from the binlog looks like, you can connect
    to the database and read from the binlog. In this example, I’ll use the MySQL
    connection information added to the *pipeline.conf* file for the full and incremental
    ingestion example earlier in this section.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解binlog输出的样子，可以连接到数据库并从binlog中读取。在这个例子中，我将使用早期在本节中添加到*pipeline.conf*文件中的MySQL连接信息，作为完整和增量摄取示例。
- en: Note
  id: totrans-168
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The following example reads from the MySQL server’s default binlog file. The
    default binlog filename and path are set in the `log_bin` variable, which is stored
    in the *my.cnf* file for the MySQL database. In some cases, binlogs are rotated
    over time (perhaps daily or hourly). If so, you will need to determine the file
    path based on the method of log rotation and file naming scheme chosen by the
    MySQL administrator and pass it as a value to the `log_file` parameter when creating
    the `BinLogStreamReader` instance. See [the documentation for the `BinLogStreamReader`
    class](https://oreil.ly/uzn0B) for more.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例从MySQL服务器的默认binlog文件中读取。默认的binlog文件名和路径在MySQL数据库的*my.cnf*文件中通过`log_bin`变量设置。在某些情况下，binlog会随时间旋转（可能是每天或每小时）。如果是这样，您将需要根据MySQL管理员选择的日志旋转方法和文件命名方案确定文件路径，并在创建`BinLogStreamReader`实例时将其作为`log_file`参数的值传递。有关更多信息，请参阅[BinLogStreamReader类的文档](https://oreil.ly/uzn0B)。
- en: '[PRE30]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: There are a few things to note about the `BinLogStreamReader` object that’s
    instantiated in the code sample. First, it connects to the MySQL database specified
    in the *pipeline.conf* file and reads from a specific binlog file. Next, the combination
    of the `resume_stream=True` setting and the `log_pos` value tells it to start
    reading the binlog at a specified point. In this case, that’s position 1400\.
    Finally, I tell `BinLogStreamReader` to only read the `DeleteRowsEvent`, `WriteRowsEvent`,
    and `UpdateRowsEvent`, events using the `only_events` parameter.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在代码示例中实例化的`BinLogStreamReader`对象有几个需要注意的地方。首先，它连接到*pipeline.conf*文件中指定的MySQL数据库，并从特定的binlog文件中读取。接下来，通过`resume_stream=True`设置和`log_pos`值的组合告诉它从指定点开始读取binlog。在这种情况下，位置是1400。最后，我告诉`BinLogStreamReader`只读取`DeleteRowsEvent`、`WriteRowsEvent`和`UpdateRowsEvent`事件，使用`only_events`参数。
- en: 'Next, the script iterates through all of the events and prints them in a human-readable
    format. For your database with the `Orders` table in it, you’ll see something
    like this as output:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，脚本会迭代所有事件，并以人类可读的格式打印它们。对于你的包含`Orders`表的数据库，输出将类似于这样：
- en: '[PRE31]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: As you can see, there are two events that represent the `INSERT` and `UPDATE`
    of `OrderId` 1, which was shown in [Table 4-3](#all_versions_of_orderid_1_in_the_data_warehouse).
    In this fictional example, the two sequential binlog events are days apart, but
    in reality there would be numerous events between them, representing all changes
    made in the database.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，有两个事件代表了`OrderId`为1的`INSERT`和`UPDATE`，这在[Table 4-3](#all_versions_of_orderid_1_in_the_data_warehouse)中有显示。在这个虚构的例子中，这两个连续的binlog事件相隔几天，但实际上之间会有许多事件，代表数据库中的所有更改。
- en: Note
  id: totrans-175
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The value of `log_pos`, which tells `BinLogStreamReader` where to start, is
    a value that you’ll need to store somewhere in a table of your own to keep track
    of where to pick up when the next extract runs. I find it best to store the value
    in a log table in the data warehouse from which it can be read when the extraction
    starts and to which it can be written, with the position value of the final event
    when it finishes.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 告诉`BinLogStreamReader`从何处开始的`log_pos`值是一个你需要在自己的表中存储的值，以便在下一次提取时从该位置开始。我发现最好将该值存储在数据仓库中的日志表中，在提取开始时读取该值，并在提取完成时写入最终事件的位置值。
- en: 'Though the code sample shows what the events look like in a human-readable
    format, to make the output easy to load into the data warehouse, it’s necessary
    to do a couple more things:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管代码示例显示了事件在人类可读的格式中的样子，为了方便加载到数据仓库中，还需要做一些额外的工作：
- en: Parse and write the data in a different format. To simplify loading, the next
    code sample will write each event to a row in a CSV file.
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解析并以不同格式写入数据。为了简化加载，下一个代码示例将每个事件写入CSV文件的一行。
- en: Write one file per table that you want to extract and load. Though the example
    binlog only contains events related to the `Orders` table, it’s highly likely
    that in a real binlog, events related to other tables are included as well.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个想要提取和加载的表都要写一个文件。尽管示例binlog仅包含与`Orders`表相关的事件，但在实际的binlog中，很可能也包含与其他表相关的事件。
- en: 'To address the first change, instead of using the `.dump()` function I will
    instead parse out the event attributes and write them to a CSV file. For the second,
    instead of writing a file for each table, for simplicity I will only write events
    related to the `Orders` table to a file called *orders_extract.csv*. In a fully
    implemented extraction, modify this code sample to group events by table and write
    multiple files, one for each table you want to ingest changes for. The last step
    in the final code sample uploads the CSV file to the S3 bucket so it can be loaded
    into the data warehouse, as described in detail in [Chapter 5](ch05.xhtml#ch05):'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决第一个变更，我将不再使用`.dump()`函数，而是解析事件属性并将其写入CSV文件。至于第二个变更，为了简化起见，我只会将与`Orders`表相关的事件写入名为*orders_extract.csv*的文件中。在完全实施的提取过程中，您可以修改此代码示例，将事件按表分组，并写入多个文件，每个表一个文件。最后一个代码示例的最后一步将CSV文件上传到S3存储桶中，以便加载到数据仓库中，详细描述请参阅[第5章](ch05.xhtml#ch05)。
- en: '[PRE32]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'After execution, *orders_extract.csv* will look like this:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 执行后，*orders_extract.csv*将如下所示：
- en: '[PRE33]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: As I discuss in [Chapter 5](ch05.xhtml#ch05), the format of the resulting CSV
    file is optimized for fast loading. Making sense of the data that’s been extracted
    is a job for the transform step in a pipeline, reviewed in detail in [Chapter 6](ch06.xhtml#ch06).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在[第5章](ch05.xhtml#ch05)中讨论的那样，生成的CSV文件格式经过优化，以便快速加载。解析提取出的数据是管道中的转换步骤的任务，在[第6章](ch06.xhtml#ch06)中详细审查。
- en: Extracting Data from a PostgreSQL Database
  id: totrans-185
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从PostgreSQL数据库中提取数据
- en: 'Just like MySQL, ingesting data from a PostgreSQL (commonly known as Postgres)
    database can be done in one of two ways: either with full or incremental extractions
    using SQL or by leveraging features of the database meant to support replication
    to other nodes. In the case of Postgres, there are a few ways to do this, but
    this chapter will focus on one method: turning the Postgres *write-ahead log*
    (WAL) into a data stream.'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 就像MySQL一样，从PostgreSQL（通常称为Postgres）数据库中提取数据可以通过两种方式之一完成：使用SQL进行全量或增量抽取，或者利用数据库支持的复制功能将数据复制到其他节点。在Postgres的情况下，有几种方法可以实现这一点，但本章节将重点介绍一种方法：将Postgres的*预写式日志*（WAL）转换为数据流。
- en: Like the previous section, this one is intended for those who need to ingest
    data from an existing Postgres database. However, if you’d like to just try the
    code samples, you can set up Postgres either by [installing on your local machine](https://oreil.ly/3KId7),
    or in AWS by using an a [RDS instance](https://oreil.ly/SWj3g), which I recommend.
    See the previous section for notes on pricing and security-related best practices
    for RDS MySQL, as they apply to RDS Postgres as well.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 与前一节类似，本节适用于需要从现有的Postgres数据库中提取数据的人群。但是，如果您只想尝试代码示例，可以通过[在本地安装](https://oreil.ly/3KId7)Postgres或使用[RDS实例](https://oreil.ly/SWj3g)在AWS上安装Postgres，我推荐使用。有关RDS
    MySQL的定价和与安全相关的最佳实践的注意事项，请参阅前一节，因为它们同样适用于RDS Postgres。
- en: 'The code samples in this section are quite simple and refer to a table named
    `Orders` in a Postgres database. Once you have a Postgres instance to work with,
    you can create the table and insert some sample rows by running the following
    SQL commands:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 本节中的代码示例非常简单，涉及到Postgres数据库中名为`Orders`的表。一旦您有一个可用的Postgres实例，可以通过运行以下SQL命令创建表并插入一些示例行：
- en: '[PRE34]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Full or Incremental Postgres Table Extraction
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 全量或增量Postgres表抽取
- en: This method is similar to full and incremental and full extractions demonstrated
    in [“Extracting Data from a MySQL Database”](#extract-data-mysql). It’s so similar
    that I won’t go into detail here beyond one difference in the code. Like the example
    in that section, this one will extract data from a table called `Orders` in a
    source database, write it to a CSV file, and then upload it to an S3 bucket.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法与从MySQL数据库中提取数据的全量和增量抽取方法相似，详见[“从MySQL数据库中提取数据”](#extract-data-mysql)。它们如此相似，我在这里不会详细介绍，只提到代码中的一个差异。与前述示例类似，此示例将从源数据库中名为`Orders`的表中提取数据，将其写入CSV文件，然后将其上传到S3存储桶中。
- en: 'The only difference in this section is the Python library I’ll use to extract
    the data. Instead of `PyMySQL`, I’ll be using `pyscopg2` to connect to a Postgres
    database. If you have not already installed it, you can do so using `pip`:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这一部分唯一的区别在于我将使用的Python库来提取数据。与`PyMySQL`不同，我将使用`pyscopg2`连接到Postgres数据库。如果你尚未安装它，可以使用`pip`安装：
- en: '[PRE35]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You’ll also need to add a new section to the *pipeline.conf* file with the
    connection information for the Postgres database:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 你还需要向*pipeline.conf*文件添加一个新的部分，包含Postgres数据库的连接信息：
- en: '[PRE36]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The code to run the full extraction of the `Orders` table is nearly identical
    to the sample from the MySQL section, but as you can see, it uses `pyscopg2` to
    connect to the source database and to run the query. Here it is in its entirety:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 运行完整提取`Orders`表的代码与MySQL部分的示例几乎完全相同，但是你可以看到，它使用`pyscopg2`连接到源数据库并运行查询。以下是完整的代码：
- en: '[PRE37]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Modifying the incremental version shown in the MySQL section is just as simple.
    All you need to do is make use of `psycopg2` instead of `PyMySQL`.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 修改MySQL部分中显示的增量版本同样简单。你只需使用`psycopg2`而不是`PyMySQL`即可。
- en: Replicating Data Using the Write-Ahead Log
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用预写式日志复制数据
- en: Like the MySQL binlog (as discussed in the previous section), the Postgres WAL
    can be used as a method of CDC for extraction. Also like the MySQL binlog, using
    the WAL for data ingestion in a pipeline is quite complex.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 与MySQL二进制日志（如前一节所讨论的）类似，Postgres WAL可用作CDC的一种方法。同样地，使用WAL在管道中进行数据摄取是非常复杂的。
- en: Though you can take a similar, simplified approach to the one used as an example
    with the MySQL binlog, I suggest using an open source distributed platform called
    Debezium to stream the contents of the Postgres WAL to an S3 bucket or data warehouse.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管你可以采用类似简化的方法来处理MySQL二进制日志示例，但我建议使用一个名为Debezium的开源分布式平台来流式传输Postgres WAL的内容到S3存储桶或数据仓库。
- en: Though the specifics of configuring and running Debezium services are a topic
    worth dedicating an entire book to, I give an overview of Debezium and how it
    can be used for data ingestions in [“Streaming Data Ingestions with Kafka and
    Debezium”](#stream-w-kafka-debezium). You can learn more about how it can be used
    for Postgres CDC there.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管配置和运行Debezium服务的具体细节值得专门一本书来讨论，我在[“使用Kafka和Debezium进行流数据摄取”](#stream-w-kafka-debezium)中概述了Debezium的使用及其在Postgres
    CDC中的应用。你可以在那里了解更多关于如何在Postgres CDC中使用它的信息。
- en: Extracting Data from MongoDB
  id: totrans-203
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从MongoDB提取数据
- en: This example illustrates how to extract a subset of MongoDB documents from a
    collection. In this sample MongoDB collection, documents represent events logged
    from some system such as a web server. Each document has a timestamp of when it
    was created, as well as a number of properties that the sample code extracts a
    subset of. After the extraction is complete, the data is written to a CSV file
    and stored in an S3 bucket so that it can be loaded into a data warehouse in a
    future step (see [Chapter 5](ch05.xhtml#ch05)).
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 本示例说明了如何从集合中提取MongoDB文档的子集。在这个示例的MongoDB集合中，文档代表从某些系统（如Web服务器）记录的事件。每个文档都有一个创建时的时间戳，以及一些属性，示例代码提取了其中的一个子集。提取完成后，数据将被写入CSV文件并存储在S3存储桶中，以便在后续步骤中加载到数据仓库中（参见[第5章](ch05.xhtml#ch05)）。
- en: 'To connect to the MongoDB database, you’ll need to first install the PyMongo
    library. As with other Python libraries, you can install it using `pip`:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 要连接到MongoDB数据库，你首先需要安装PyMongo库。与其他Python库一样，你可以使用`pip`来安装它：
- en: '[PRE38]'
  id: totrans-206
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: You can of course modify the following sample code to connect to your own MongoDB
    instance and extract data from your documents. However, if you’d like to run the
    sample as is, you can do so by creating a MongoDB cluster for free with MongoDB
    Atlas. Atlas is a fully managed MongoDB service and includes a free-for-life tier
    with plenty of storage and computing power for learning and running samples like
    the one I provide. You can upgrade to a paid plan for production deployments.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，你可以修改以下示例代码以连接到自己的MongoDB实例并从文档中提取数据。但是，如果你想按原样运行示例，可以通过MongoDB Atlas免费创建一个MongoDB集群。Atlas是一个完全托管的MongoDB服务，包括一个免费的终身套件，提供了足够的存储空间和计算能力，适用于学习和运行我提供的示例。你也可以升级到付费计划用于生产部署。
- en: You can learn how to create a free MongoDB cluster in Atlas, create a database,
    and configure it so that you can connect via a Python script running on your local
    machine by following [these instructions](https://oreil.ly/DIPdo).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以按照[这些说明](https://oreil.ly/DIPdo)学习如何在Atlas中创建一个免费的MongoDB集群，创建一个数据库，并进行配置，以便您可以通过在本地机器上运行的Python脚本进行连接。
- en: 'You’ll need to install one more Python library named `dnspython` to support
    `pymongo` in connecting to your cluster hosted in MongoDB Atlas. You can install
    it using `pip`:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要安装一个名为`dnspython`的Python库，以支持`pymongo`连接到MongoDB Atlas中托管的集群。您可以使用`pip`安装它：
- en: '[PRE39]'
  id: totrans-210
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: Next, add a new section to the *pipeline.conf* file with connection information
    for the MongoDB instance you’ll be extracting data from. Fill in each line with
    your own connection details. If you’re using MongoDB Atlas and can’t recall these
    values from when you set up your cluster, you can learn how to find them by reading
    the [Atlas docs](https://oreil.ly/Zdynu).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，在*pipeline.conf*文件中添加一个新的部分，包含用于从MongoDB实例中提取数据的连接信息。请填写每一行中的自己的连接细节。如果您使用的是MongoDB
    Atlas，并且无法从设置集群时记起这些值，您可以阅读[Atlas文档](https://oreil.ly/Zdynu)来了解如何找到它们。
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'Before creating and running the extraction script, you can insert some sample
    data to work with. Create a file called *sample_mongodb.py* with the following
    code:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建和运行提取脚本之前，您可以插入一些样本数据以进行工作。创建一个名为*sample_mongodb.py*的文件，并添加以下代码：
- en: '[PRE41]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'When you execute it, the three documents will be inserted into your MongoDB
    collection:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行时，这三个文档将被插入到您的MongoDB集合中：
- en: '[PRE42]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Now create a new Python script called *mongo_extract.py* so you can add the
    following code blocks to it.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 现在创建一个名为*mongo_extract.py*的新Python脚本，以便您可以在其中添加以下代码块。
- en: 'First, import PyMongo and Boto3 so that you can extract data from the MongoDB
    database and store the results in an S3 bucket. Also import the `csv` library
    so that you can structure and write out the extracted data in a flat file that’s
    easy to import into a data warehouse in the load step of ingestion. Finally, you’ll
    need some `datetime` functions for this example so that you can iterate through
    the sample event data in the MongoDB collection:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，导入PyMongo和Boto3，以便您可以从MongoDB数据库中提取数据，并将结果存储在S3存储桶中。还导入`csv`库，以便您可以在摄取过程的加载步骤中将提取的数据结构化并写入一个易于导入到数据仓库的扁平文件中。最后，您还需要一些`datetime`函数，以便您可以在MongoDB集合中的示例事件数据中进行迭代：
- en: '[PRE43]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Next, connect to the MongoDB instance specified in the `pipelines.conf` file,
    and create a `collection` object where the documents you want to extract are stored:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，连接到`pipelines.conf`文件中指定的MongoDB实例，并创建一个`collection`对象，其中存储了您想要提取的文档：
- en: '[PRE44]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Now it’s time to query the documents to extract. You can do this by calling
    the `.find()` function on `mongo_collection` to query the documents you’re looking
    for. In the following example, you’ll grab all documents with a `event_timestamp`
    field value between two dates defined in the script.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是查询要提取的文档的时候了。您可以通过在`mongo_collection`上调用`.find()`函数来执行此操作，以查询您正在寻找的文档。在下面的示例中，您将获取所有具有脚本中定义的两个日期之间的`event_timestamp`字段值的文档。
- en: Note
  id: totrans-223
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Extracting immutable data such as log records or generic “event” records from
    a data store by date range is a common use case. Although the sample code uses
    a datetime range defined in the script, it’s more likely you’ll pass in a datetime
    range to the script, or have the script query your data warehouse to get the datetime
    of the last event loaded, and extract subsequent records from the source data
    store. See [“Extracting Data from a MySQL Database”](#extract-data-mysql) for
    an example of doing so.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据存储中按日期范围提取不可变数据，例如日志记录或通用的“事件”记录，是一个常见的用例。尽管示例代码使用脚本中定义的日期时间范围，但更有可能的是，您将在脚本中传递日期时间范围，或者让脚本查询数据仓库以获取最后加载事件的日期时间，并从源数据存储中提取后续记录。请参阅[“从MySQL数据库中提取数据”](#extract-data-mysql)来了解如何执行此操作的示例。
- en: '[PRE45]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: Note
  id: totrans-226
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The `batch_size` parameter in this example is set to `3000`. PyMongo makes a
    round-trip to the MongoDB host for each batch. For example, if the `result_docs
    Cursor` has 6,000 results, it will take two trips to the MongoDB host to pull
    all the documents down to the machine where your Python script is running. What
    you set as the batch size value is up to you and will depend on the trade-off
    of storing more documents in memory on the system running the extract versus making
    lots of round trips to the MongoDB instance.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`batch_size` 参数设置为 `3000`。PyMongo 每个批次都会与 MongoDB 主机进行一次往返。例如，如果 `result_docs
    游标`有 6,000 个结果，将需要两次与 MongoDB 主机的往返，以将所有文档拉取到运行 Python 脚本的机器上。将批次大小值设为多少取决于您，在提取过程中在内存中存储更多文档与进行多次与
    MongoDB 实例的往返之间的权衡。
- en: 'The result of the preceding code is a `Cursor` named `event_docs` that I’ll
    use to iterate through the resulting documents. Recall that in this simplified
    example, each document represents an event that was generated from a system such
    as a web server. An event might represent something like a user logging in, viewing
    a page, or submitting a feedback form. Though the documents might have dozens
    of fields to represent things like the browser the user logged in with, I take
    just a few fields for this example:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码的结果是名为 `event_docs 游标`，我将用它来迭代结果文档。请记住，在这个简化的例子中，每个文档表示从诸如 Web 服务器之类的系统生成的事件。一个事件可能代表用户登录、查看页面或提交反馈表单等活动。尽管文档可能有几十个字段来表示诸如用户登录时使用的浏览器之类的信息，但在这个例子中，我只选择了几个字段：
- en: '[PRE46]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: I’m including a default value in the `doc.get()` function call (–1 or None).
    Why? The nature of unstructured document data means that it’s possible for fields
    to go missing from a document altogether. In other words, you can’t assume that
    each of the documents you’re iterating through has an “event_name” or any other
    field. In those cases, tell `doc.get()` to return a `None` value instead of throwing
    an error.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `doc.get()` 函数调用中包含了一个默认值（-1 或 None）。为什么这样做？非结构化文档数据的特性意味着文档中的字段可能会完全丢失。换句话说，您不能假设您迭代的每个文档都有“event_name”或任何其他字段。在这些情况下，告诉
    `doc.get()` 返回一个 `None` 值，而不是抛出错误。
- en: 'After iterating through all the events in `event_docs`, the `all_events` list
    is ready to be written to a CSV file. To do so, you’ll make use of the `csv` module,
    which is included in the standard Python distribution and was imported earlier
    in this example:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `event_docs` 中迭代所有事件后，`all_events` 列表已准备好写入 CSV 文件。为此，您将使用标准 Python 分发中包含的
    `csv` 模块，此模块已在此示例中导入：
- en: '[PRE47]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: 'Now, upload the CSV file to the S3 bucket that you configured in [“Setting
    Up Cloud File Storage”](#setup-cloud-storage). To do so, use the Boto3 library:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，上传 CSV 文件到您在[“设置云文件存储”](#setup-cloud-storage)中配置的 S3 存储桶。为此，使用 Boto3 库：
- en: '[PRE48]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'That’s it! The data you extracted from the MongoDB collection is now sitting
    in the S3 bucket waiting to be loaded into the data warehouse or other data store.
    If you used the sample data provided, the contents of *export_file.csv* will look
    something like this:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！从 MongoDB 集合中提取的数据现在已经位于 S3 存储桶中，等待加载到数据仓库或其他数据存储中。如果您使用了提供的示例数据，*export_file.csv*
    文件的内容将类似于以下内容：
- en: '[PRE49]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: See [Chapter 5](ch05.xhtml#ch05) for more on loading the data into the data
    store of your choice.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 参见[第 5 章](ch05.xhtml#ch05)，了解更多将数据加载到您选择的数据存储中的信息。
- en: Extracting Data from a REST API
  id: totrans-238
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 从 REST API 中提取数据
- en: 'REST APIs are a common source to extract data from. You may need to ingest
    data from an API that your organization created and maintains, or from an API
    from an external service or vendor that your organization uses, such as Salesforce,
    HubSpot, or Twitter. No matter the API, there’s a common pattern for data extraction
    that I’ll use in the simple example that follows:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: REST API 是常见的数据提取来源。您可能需要从您的组织创建和维护的 API，或者从您的组织使用的外部服务或供应商（如 Salesforce、HubSpot
    或 Twitter）的 API 中摄取数据。无论是哪种 API，数据提取的常见模式如下所示，我将在接下来的简单示例中使用：
- en: Send an HTTP GET request to the API endpoint.
  id: totrans-240
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 向 API 端点发送 HTTP GET 请求。
- en: Accept the response, which is most likely formatted in JSON.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受响应，这些响应大多数情况下是以 JSON 格式进行格式化的。
- en: Parse the response and “flatten” it into a CSV file that you can later load
    into the data warehouse.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解析响应并将其“展平”成一个后续可以加载到数据仓库的 CSV 文件。
- en: Note
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Though I am parsing the JSON response and storing it in a flat file (CSV), you
    can also save the data in JSON format for loading into your data warehouse. For
    the sake of simplicity, I’m sticking to the pattern of this chapter and using
    CSV files. Please consult [Chapter 5](ch05.xhtml#ch05) or your data warehouse
    documentation for more on loading data in a format other than CSV.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我正在解析JSON响应并将其存储在平面文件（CSV）中，但您也可以将数据保存为JSON格式以加载到数据仓库中。为简单起见，我坚持使用CSV文件的模式。请参阅[第5章](ch05.xhtml#ch05)或您的数据仓库文档，了解如何加载其他格式的数据更多信息。
- en: In this example, I’ll connect to an API called Open Notify. The API has several
    endpoints, each returning data from NASA about things happening in space. I’ll
    query the endpoint that returns the next five times that the International Space
    Station (ISS) will pass over the given location on Earth.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个示例中，我将连接到一个名为Open Notify的API。该API有几个端点，每个端点返回有关太空活动的NASA数据。我将查询返回给定地球位置上国际空间站（ISS）将经过的下五次时间的端点。
- en: 'Before I share the Python code for querying the endpoint, you can see what
    the output of a simple query looks like by typing the following URL into your
    browser:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 在分享用于查询端点的Python代码之前，您可以通过将以下URL键入浏览器来查看简单查询输出的样子：
- en: '[PRE50]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'The resulting JSON looks like this:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果的JSON如下所示：
- en: '[PRE51]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'The goal of this extraction is to retrieve the data in the response and format
    it in a CSV file with one line for each time and duration of each pass that the
    ISS will make over the lat/long pair. For example, the first two lines of the
    CSV file will be as follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 此提取的目标是从响应中检索数据，并将其格式化为CSV文件，每行描述ISS经过的时间和持续时间。例如，CSV文件的前两行将如下所示：
- en: '[PRE52]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'To query the API and handle the response in Python, you’ll need to install
    the `requests` library. `requests` makes HTTP requests and responses easy to work
    with in Python. You can install it with `pip`:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要在Python中查询API并处理响应，您需要安装`requests`库。`requests`使Python中的HTTP请求和响应操作变得简单。您可以使用`pip`安装它：
- en: '[PRE53]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'Now, you can use `requests` to query the API endpoint, get back the response,
    and print out the resulting JSON, which will look like what you saw in your browser:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以使用`requests`查询API端点，获取响应并打印出结果JSON的样子，它看起来与您在浏览器中看到的类似：
- en: '[PRE54]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: Instead of printing out the JSON, I’ll iterate through the response, parse out
    the values for duration and risetime, write the results to a CSV file, and upload
    the file to the S3 bucket.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 不再打印JSON，而是迭代响应，解析出持续时间和经过时间的值，将结果写入CSV文件，并将文件上传到S3存储桶。
- en: 'To parse the JSON response, I’ll import the Python `json` library. There’s
    no need to install it as it comes with the standard Python installation. Next,
    I’ll import the `csv` library, which is also included in the standard Python distribution
    for writing the CSV file. Finally, I’ll use the `configparser` library to get
    the credentials required by the Boto3 library to upload the CSV file to the S3
    bucket:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 要解析JSON响应，我将导入Python的`json`库。无需安装它，因为它随标准Python安装而来。接下来，我将导入`csv`库，这也包含在标准Python发行版中，用于编写CSV文件。最后，我将使用`configparser`库获取Boto3库上传CSV文件到S3存储桶所需的凭据：
- en: '[PRE55]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, query the API just as you did before:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，像以前一样查询API：
- en: '[PRE56]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'Now, it’s time to iterate through the response, store the results in a Python
    `list` called `all_passes`, and save the results to a CSV file. Note that I also
    store the lat and long from the request even though they are not included in the
    response. They are needed on each line of the CSV file so that the pass times
    are associated with the correct lat and long when loaded into the data warehouse:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，是时候迭代响应，将结果存储在名为`all_passes`的Python列表中，并将结果保存到CSV文件中。请注意，尽管它们未包含在响应中，但我也会存储来自请求的纬度和经度。在加载到数据仓库时，这些信息在每行CSV文件中都是必需的，以便将经过时间与正确的纬度和经度关联起来：
- en: '[PRE57]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'Finally, upload the CSV file to the S3 bucket using the Boto3 library:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用Boto3库将CSV文件上传到S3存储桶：
- en: '[PRE58]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Streaming Data Ingestions with Kafka and Debezium
  id: totrans-265
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用Kafka和Debezium进行流式数据接入
- en: When it comes to ingesting data from a CDC system such as MySQL binlogs or Postgres
    WALs, there’s no simple solution without some help from a great framework.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及从CDC系统（如MySQL binlog或Postgres WAL）摄取数据时，没有简单的解决方案，除非借助一个很棒的框架。
- en: 'Debezium is a distributed system made up of several open source services that
    capture row-level changes from common CDC systems and then streams them as events
    that are consumable by other systems. There are three primary components of a
    Debezium installation:'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: Debezium 是一个分布式系统，由几个开源服务组成，捕获常见 CDC 系统中的行级变更，然后将它们作为可被其他系统消费的事件流。Debezium 安装的三个主要组件包括：
- en: '*Apache Zookeeper* manages the distributed environment and handles configuration
    across each service.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Zookeeper* 管理分布式环境并处理每个服务的配置。'
- en: '*Apache Kafka* is a distributed streaming platform that is commonly used to
    build highly scalable data pipelines.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Kafka* 是一个分布式流处理平台，通常用于构建高度可扩展的数据流水线。'
- en: '*Apache Kafka Connect* is a tool to connect Kafka with other systems so that
    the data can be easily streamed via Kafka. *Connectors* are built for systems
    like MySQL and Postgres and turn data from their CDC systems (binlogs and WAL)
    into *Kakfa topics*.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Apache Kafka Connect* 是一个工具，用于连接 Kafka 与其他系统，以便通过 Kafka 轻松地流式传输数据。连接器针对诸如
    MySQL 和 Postgres 等系统构建，将来自它们的 CDC 系统（binlog 和 WAL）的数据转换为 *Kafka topics*。'
- en: Kafka exchanges messages that are organized by *topic*. One system might publish
    to a topic, while one or more might consume, or subscribe to, the topic.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: Kafka 通过 *topic* 交换组织的消息。一个系统可能发布到一个主题，而一个或多个系统可能订阅该主题。
- en: Debezium ties these systems together and includes connectors for common CDC
    implementations. For example, I discussed the challenges for CDC in [“Extracting
    Data from a MySQL Database”](#extract-data-mysql) and [“Extracting Data from a
    PostgreSQL Database”](#extract-data-postgressql). Thankfully, there are connectors
    already built to “listen” to the MySQL binlog and Postgres WAL. The data is then
    routed through Kakfa as records in a topic and consumed into a destination such
    as an S3 bucket, Snowflake, or Redshift data warehouse using another connector.
    [Figure 4-1](#fig_0401) illustrates an example of using Debezium, and its individual
    components, to send the events created by a MySQL binlog into a Snowflake data
    warehouse.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: Debezium 将这些系统连接在一起，并包括常见 CDC 实现的连接器。例如，我讨论了在 [“从 MySQL 数据库中提取数据”](#extract-data-mysql)
    和 [“从 PostgreSQL 数据库中提取数据”](#extract-data-postgressql) 中 CDC 的挑战。幸运的是，已经建立了可以“监听”MySQL
    binlog 和 Postgres WAL 的连接器。然后，数据作为主题中的记录通过 Kafka 路由，并通过另一个连接器消耗到目的地，如 S3 存储桶、Snowflake
    或 Redshift 数据仓库。[图 4-1](#fig_0401) 演示了使用 Debezium 及其各个组件将 MySQL binlog 生成的事件发送到
    Snowflake 数据仓库的示例。
- en: '![dppr 0401](Images/dppr_0401.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 0401](Images/dppr_0401.png)'
- en: Figure 4-1\. Using components of Debezium for CDC from MySQL to Snowflake.
  id: totrans-274
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 4-1\. 使用 Debezium 组件从 MySQL 向 Snowflake 进行 CDC。
- en: 'As of this writing, there are a number of Debezium connectors already built
    for source systems that you may find yourself needing to ingest from:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 据我所知，已经为您可能需要从中摄取数据的源系统建立了多个 Debezium 连接器：
- en: MongoDB
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MongoDB
- en: MySQL
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MySQL
- en: PostgreSQL
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PostgreSQL
- en: Microsoft SQL Server
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Microsoft SQL Server
- en: Oracle
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Oracle
- en: Db2
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Db2
- en: Cassandra
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cassandra
- en: There are also Kafka Connect connectors for the most common data warehouses
    and storage systems, such as S3 and Snowflake.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 还有 Kafka Connect 连接器适用于最常见的数据仓库和存储系统，如 S3 和 Snowflake。
- en: Though Debezium, and Kafka itself, is a subject that justifies its own book,
    I do want to point out its value if you decide that CDC is a method you want to
    use for data ingestion. The simple example I used in the MySQL extraction section
    of this chapter is functional; however, if you want to use CDC at scale, I highly
    suggest using something like Debezium rather than building an existing platform
    like Debezium on your own!
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管 Debezium 本身及 Kafka 本身是一个值得撰写专著的主题，但我确实希望指出，如果您决定使用 CDC 作为数据摄入的方法，它们的价值是显而易见的。本章
    MySQL 提取部分中使用的简单示例是功能性的；然而，如果您希望大规模使用 CDC，我强烈建议使用像 Debezium 这样的现有平台，而不是自行构建一个现有平台！
- en: Tip
  id: totrans-285
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The [Debezium documentation](https://oreil.ly/9igMR) is excellent and a great
    starting point for learning about the system.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '[Debezium 文档](https://oreil.ly/9igMR) 极好，是学习该系统的绝佳起点。'
