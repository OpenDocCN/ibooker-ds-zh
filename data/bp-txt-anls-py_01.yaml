- en: Chapter 1\. Gaining Early Insights from Textual Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章。从文本数据中获得早期见解
- en: One of the first tasks in every data analytics and machine learning project
    is to become familiar with the data. In fact, it is always essential to have a
    basic understanding of the data to achieve robust results. Descriptive statistics
    provide reliable and robust insights and help to assess data quality and distribution.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个数据分析和机器学习项目中的第一个任务是熟悉数据。事实上，对数据有基本了解始终是获得稳健结果的关键。描述性统计提供可靠且稳健的见解，并有助于评估数据质量和分布。
- en: When considering texts, frequency analysis of words and phrases is one of the
    main methods for data exploration. Though absolute word frequencies usually are
    not very interesting, relative or weighted frequencies are. When analyzing text
    about politics, for example, the most common words will probably contain many
    obvious and unsurprising terms such as *people*, *country*, *government*, etc.
    But if you compare relative word frequencies in text from different political
    parties or even from politicians in the same party, you can learn a lot from the
    differences.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑文本时，词语和短语的频率分析是数据探索的主要方法之一。虽然绝对词频通常不太有趣，但相对或加权频率却是如此。例如，当分析政治文本时，最常见的词可能包含许多明显和不足为奇的术语，如*人民*、*国家*、*政府*等。但是，如果比较不同政治党派甚至同一党派政客文本中的相对词频，你可以从中学到很多不同之处。
- en: What You’ll Learn and What We’ll Build
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 你将学到什么，我们将构建什么
- en: This chapter presents blueprints for the statistical analysis of text. It gets
    you started quickly and introduces basic concepts that you will need to know in
    subsequent chapters. We will start by analyzing categorical metadata and then
    focus on word frequency analysis and visualization.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章介绍了文本统计分析的蓝图。它可以让你快速入门，并介绍了后续章节中需要了解的基本概念。我们将从分析分类元数据开始，然后专注于词频分析和可视化。
- en: After studying this chapter, you will have basic knowledge about text processing
    and analysis. You will know how to tokenize text, filter stop words, and analyze
    textual content with frequency diagrams and word clouds. We will also introduce
    TF-IDF weighting as an important concept that will be picked up later in the book
    for text vectorization.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 学习完本章后，你将具备关于文本处理和分析的基础知识。你将知道如何对文本进行标记化、过滤停用词，并使用频率图和词云分析文本内容。我们还将介绍TF-IDF加权作为一个重要概念，该概念将在本书后面用于文本向量化时再次提到。
- en: 'The blueprints in this chapter focus on quick results and follow the *KISS*
    principle: “Keep it simple, stupid!” Thus, we primarily use Pandas as our library
    of choice for data analysis in combination with regular expressions and Python
    core functionality. [Chapter 4](ch04.xhtml#ch-preparation) will discuss advanced
    linguistic methods for data preparation.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的蓝图侧重于快速结果，并遵循“保持简单，傻瓜！”的原则。因此，我们主要使用Pandas作为数据分析的首选库，结合正则表达式和Python核心功能。[第4章](ch04.xhtml#ch-preparation)将讨论用于数据准备的高级语言学方法。
- en: Exploratory Data Analysis
  id: totrans-7
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 探索性数据分析
- en: Exploratory data analysis is the process of systematically examining data on
    an aggregated level. Typical methods include summary statistics for numerical
    features as well as frequency counts for categorical features. Histograms and
    box plots will illustrate the distribution of values, and time-series plots will
    show their evolution.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 探索性数据分析是系统地检查聚合级别数据的过程。典型方法包括数值特征的摘要统计以及分类特征的频率计数。直方图和箱线图将说明值的分布，时间序列图将展示其演变。
- en: A dataset consisting of text documents such as news, tweets, emails, or service
    calls is called a *corpus* in natural language processing. The statistical exploration
    of such a corpus has different facets. Some analyses focus on metadata attributes,
    while others deal with the textual content. [Figure 1-1](#fig-statistical-features)
    shows typical attributes of a text corpus, some of which are included in the data
    source, while others could be calculated or derived. The document metadata comprise
    multiple descriptive attributes, which are useful for aggregation and filtering.
    Time-like attributes are essential to understanding the evolution of the corpus.
    If available, author-related attributes allow you to analyze groups of authors
    and to benchmark these groups against one another.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理中，包含新闻、推文、电子邮件或服务呼叫等文本文档的数据集被称为*语料库*。对这样一个语料库的统计探索具有不同的方面。一些分析侧重于元数据属性，而其他分析则处理文本内容。[图 1-1](#fig-statistical-features)展示了文本语料库的典型属性，其中一些包含在数据源中，而另一些可以计算或推导得出。文档元数据包括多个描述性属性，这些属性对聚合和筛选非常有用。类似时间的属性对理解语料库的演变至关重要。如果有的话，与作者相关的属性允许您分析作者群体，并将这些群体相互比较。
- en: '![](Images/btap_0101.jpg)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0101.jpg)'
- en: Figure 1-1\. Statistical features for text data exploration.
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 文本数据探索的统计特征。
- en: Statistical analysis of the content is based on the frequencies of words and
    phrases. With the linguistic data preprocessing methods described in [Chapter 4](ch04.xhtml#ch-preparation),
    we will extend the space of analysis to certain word types and named entities.
    Besides that, descriptive scores for the documents could be included in the dataset
    or derived by some kind of feature modeling. For example, the number of replies
    to a user’s post could be taken as a measure of popularity. Finally, interesting
    soft facts such as sentiment or emotionality scores can be determined by one of
    the methods described later in this book.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 内容的统计分析基于词语和短语的频率。通过[第四章](ch04.xhtml#ch-preparation)中描述的语言数据预处理方法，我们将扩展分析的范围到特定的词类和命名实体。此外，文档的描述性分数可以包含在数据集中或通过某种特征建模推导得出。例如，回复用户帖子的数量可以作为受欢迎程度的一种衡量标准。最后，通过本书后面描述的某种方法，可以确定有趣的软事实，如情感或情绪分数。
- en: Note that absolute figures are generally not very interesting when working with
    text. The mere fact that the word *problem* appears a hundred times does not contain
    any relevant information. But the fact that the relative frequency of *problem*
    has doubled within a week can be remarkable.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的是，在处理文本时，绝对数字通常并不是非常有趣的。仅仅因为单词*问题*出现了一百次，并不包含任何相关信息。但是，*问题*的相对频率在一周内翻了一番可能是引人注目的。
- en: Introducing the Dataset
  id: totrans-14
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 引入数据集
- en: Analyzing political text, be it news or programs of political parties or parliamentary
    debates, can give interesting insights on national and international topics. Often,
    text from many years is publicly available so that an insight into the zeitgeist
    can be gained. Let’s jump into the role of a political analyst who wants to get
    a feeling for the analytical potential of such a dataset.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 分析政治文本，无论是新闻还是政党纲领或议会辩论，都可以为国家和国际议题提供有趣的见解。通常，多年来的文本是公开可用的，因此可以获取对时代精神的洞察。让我们来看看作为政治分析师的角色，他想要了解这样一个数据集的分析潜力。
- en: For that, we will work with the [UN General Debate dataset](https://oreil.ly/lHHUm).
    The corpus consists of 7,507 speeches held at the annual sessions of the United
    Nations General Assembly from 1970 to 2016\. It was created in 2017 by Mikhaylov,
    Baturo, and Dasandi at Harvard “for understanding and measuring state preferences
    in world politics.” Each of the almost 200 countries in the United Nations has
    the opportunity to present its views on global topics such international conflicts,
    terrorism, or climate change at the annual General Debate.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 为此，我们将使用[联合国大会辩论数据集](https://oreil.ly/lHHUm)。该语料库由哈佛大学的米哈伊洛夫、巴图罗和达桑迪于2017年创建，“用于理解和衡量世界政治中的国家偏好”。联合国几乎所有的200个国家在年度大会上都有机会就全球议题如国际冲突、恐怖主义或气候变化发表意见。
- en: The original dataset on Kaggle is provided in the form of two CSV files, a big
    one containing the speeches and a smaller one with information about the speakers.
    To simplify matters, we prepared a single zipped CSV file containing all the information.
    You can find the code for the preparation as well as the resulting file in our
    [GitHub repository](https://oreil.ly/btap-code).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kaggle 上的原始数据集以两个 CSV 文件的形式提供，一个大文件包含演讲内容，一个小文件包含演讲者信息。为简化事务，我们准备了一个单独的压缩 CSV
    文件包含所有信息。您可以在我们的 [GitHub 代码库](https://oreil.ly/btap-code) 中找到准备代码及其结果文件。
- en: 'In Pandas, a CSV file can be loaded with `pd.read_csv()`. Let’s load the file
    and display two random records of the `DataFrame`:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Pandas 中，可以使用 `pd.read_csv()` 加载 CSV 文件。让我们加载文件并显示`DataFrame`的两条随机记录：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '`Out:`'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '|   | session | year | country | country_name | speaker | position | text |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '|   | 会话 | 年份 | 国家 | 国家名称 | 演讲者 | 职位 | 文本 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 3871 | 51 | 1996 | PER | Peru | Francisco Tudela Van Breughel Douglas | Minister
    for Foreign Affairs | At the outset, allow me,\nSir, to convey to you and to this
    Assembly the greetings\nand congratulations of the Peruvian people, as well as\ntheir...
    |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 3871 | 51 | 1996 | PER | 秘鲁 | 弗朗西斯科·图德拉·范·布鲁赫尔·道格拉斯 | 外交部长 | 在此，我首先要向您和本届大会转达秘鲁人民的问候和祝贺……
    |'
- en: '| 4697 | 56 | 2001 | GBR | United Kingdom | Jack Straw | Minister for Foreign
    Affairs | Please allow me\nwarmly to congratulate you, Sir, on your assumption
    of\nthe presidency of the fifty-sixth session of the General\nAssembly.\nThi...
    |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 4697 | 56 | 2001 | GBR | 英国 | 杰克·斯特劳 | 外交部长 | 请允许我热情地祝贺您，先生，您担任第五十六届大会主席一职。\n这...
    |'
- en: The first column contains the index of the records. The combination of session
    number and year can be considered as the logical primary key of the table. The
    `country` column contains a standardized three-letter country ISO code and is
    followed by the textual description. Then we have two columns about the speaker
    and their position. The last column contains the actual text of the speech.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 第一列包含记录的索引。会话号和年份的组合可以视为表的逻辑主键。`country` 列包含标准化的三位字母国家 ISO 代码，接着是关于演讲者及其职位的两列。最后一列包含演讲文本。
- en: Our dataset is small; it contains only a few thousand records. It is a great
    dataset to use because we will not run into performance problems. If your dataset
    is larger, check out [“Working with Large Datasets”](#sb-working-with-large-datasets)
    for options.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集很小，仅包含几千条记录。这是一个很好的数据集，因为我们不会遇到性能问题。如果您的数据集较大，请参考[“处理大型数据集”](#sb-working-with-large-datasets)
    了解更多选项。
- en: 'Blueprint: Getting an Overview of the Data with Pandas'
  id: totrans-27
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：使用 Pandas 获取数据概览
- en: 'In our first blueprint, we use only metadata and record counts to explore data
    distribution and quality; we will not yet look at the textual content. We will
    work through the following steps:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的第一个蓝图中，我们仅使用元数据和记录计数来探索数据分布和质量；我们还没有查看文本内容。我们将按以下步骤进行操作：
- en: Calculate summary statistics.
  id: totrans-29
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算汇总统计信息。
- en: Check for missing values.
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检查缺失值。
- en: Plot distributions of interesting attributes.
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制有趣属性的分布图。
- en: Compare distributions across categories.
  id: totrans-32
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较不同类别之间的分布。
- en: Visualize developments over time.
  id: totrans-33
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 可视化时间发展。
- en: Before we can start analyzing the data, we need at least some information about
    the structure of the `DataFrame`. [Table 1-1](#tab-pandas-info) shows some important
    descriptive properties or functions.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析数据之前，我们至少需要了解一些关于`DataFrame`结构的信息。[表 1-1](#tab-pandas-info)显示了一些重要的描述性属性或函数。
- en: Table 1-1\. Pandas commands to get information about dataframes
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-1\. Pandas 数据框信息获取命令
- en: '| `df.columns` | List of column names |   |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| `df.columns` | 列名列表 |   |'
- en: '| `df.dtypes` | Tuples (column name, data type) | Strings are represented as
    object in versions before Pandas 1.0. |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| `df.dtypes` | 元组（列名，数据类型） | 在 Pandas 1.0 版本之前，字符串被表示为对象。 |'
- en: '| `df.info()` | Dtypes plus memory consumption | Use with `memory_usage=''deep''`
    for good estimates on text. |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| `df.info()` | 数据类型及内存消耗 | 使用 `memory_usage=''deep''` 可以获得文本的良好内存消耗估算。 |'
- en: '| `df.describe()` | Summary statistics | Use with `include=''O''` for categorical
    data. |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| `df.describe()` | 汇总统计信息 | 对于分类数据，请使用 `include=''O''` 参数。 |'
- en: Calculating Summary Statistics for Columns
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算列的汇总统计信息
- en: 'Pandas’s `describe` function computes statistical summaries for the columns
    of the `DataFrame`. It works on a single series as well as on the complete `DataFrame`.
    The default output in the latter case is restricted to numerical columns. Currently,
    our `DataFrame` contains only the session number and the year as numerical data.
    Let’s add a new numerical column to the `DataFrame` containing the text length
    to get some additional information about the distribution of the lengths of the
    speeches. We recommend transposing the result with `describe().T` to switch rows
    and columns in the representation:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas的`describe`函数为`DataFrame`的列计算统计摘要。它可以在单个系列上工作，也可以在整个`DataFrame`上工作。在后一种情况下，默认输出限于数值列。当前，我们的`DataFrame`只包含会话号和年份作为数值数据。让我们添加一个新的数值列到`DataFrame`中，该列包含文本长度，以获取关于演讲长度分布的额外信息。我们建议使用`describe().T`来转置结果，以在表示中交换行和列：
- en: '[PRE1]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '`Out:`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | count | mean | std | min | 25% | 50% | 75% | max |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|   | 计数 | 平均值 | 标准差 | 最小值 | 25% | 50% | 75% | 最大值 |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-45
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| session | 7507.00 | 49.61 | 12.89 | 25.00 | 39.00 | 51.00 | 61.00 | 70.00
    |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 会话 | 7507.00 | 49.61 | 12.89 | 25.00 | 39.00 | 51.00 | 61.00 | 70.00 |'
- en: '| year | 7507.00 | 1994.61 | 12.89 | 1970.00 | 1984.00 | 1996.00 | 2006.00
    | 2015.00 |'
  id: totrans-47
  prefs: []
  type: TYPE_TB
  zh: '| 年份 | 7507.00 | 1994.61 | 12.89 | 1970.00 | 1984.00 | 1996.00 | 2006.00 |
    2015.00 |'
- en: '| length | 7507.00 | 17967.28 | 7860.04 | 2362.00 | 12077.00 | 16424.00 | 22479.50
    | 72041.00 |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| 长度 | 7507.00 | 17967.28 | 7860.04 | 2362.00 | 12077.00 | 16424.00 | 22479.50
    | 72041.00 |'
- en: '`describe()`, without additional parameters, computes the total count of values,
    their mean and standard deviation, and a [five-number summary](https://oreil.ly/h2nrN)
    of only the numerical columns. The `DataFrame` contains 7,507 entries for `session`,
    `year`, and `length`. Mean and standard deviation do not make much sense for `year`
    and `session`, but minimum and maximum are still interesting. Obviously, our dataset
    contains speeches from the 25th to the 70th UN General Debate sessions, spanning
    the years 1970 to 2015.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '`describe()`，没有额外的参数，计算值的总数、均值和标准差，以及只有数值列的[五数总结](https://oreil.ly/h2nrN)。`DataFrame`包含`session`、`year`和`length`的7,507个条目。对于`year`和`session`来说，均值和标准差没有太多意义，但最小值和最大值仍然很有趣。显然，我们的数据集包含了从1970年到2015年的第25届至第70届联合国大会的演讲。'
- en: 'A summary for nonnumerical columns can be produced by specifying `include=''O''`
    (the alias for `np.object`). In this case, we also get the count, the number of
    unique values, the top-most element (or one of them if there are many with the
    same number of occurrences), and its frequency. As the number of unique values
    is not useful for textual data, let’s just analyze the `country` and `speaker`
    columns:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 对非数字列的摘要可以通过指定`include='O'`（`np.object`的别名）来生成。在这种情况下，我们还会得到计数、唯一值的数量、最顶部的元素（如果有很多具有相同出现次数的话，则获取其一个）及其频率。由于唯一值的数量对文本数据来说没有用，所以让我们只分析`country`和`speaker`列：
- en: '[PRE2]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '`Out:`'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '|   | count | unique | top | freq |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|   | 计数 | 唯一 | 最顶部 | 频率 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| country | 7507 | 199 | ITA | 46 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 7507 | 199 | ITA | 46 |'
- en: '| speaker | 7480 | 5428 | Seyoum Mesfin | 12 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 发言者 | 7480 | 5428 | 谢悠姆·梅斯芬 | 12 |'
- en: The dataset contains data from 199 unique countries and apparently 5,428 speakers.
    The number of countries is valid, as this column contains standardized ISO codes.
    But counting the unique values of text columns like `speaker` usually does not
    give valid results, as we will show in the next section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含来自199个独特国家和显然5,428位发言者的数据。国家数量是有效的，因为此列包含标准化的ISO代码。但计算像`speaker`这样的文本列的唯一值通常不会得到有效结果，如下一节将展示的。
- en: Checking for Missing Data
  id: totrans-58
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 检查缺失数据
- en: 'By looking at the counts in the previous table, we can see that the `speaker`
    column has missing values. So, let’s check all columns for null values by using
    `df.isna()` (the alias to `df.isnull()`) and compute a summary of the result:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 通过查看前表中的计数，我们可以看到`speaker`列存在缺失值。因此，让我们使用`df.isna()`（`df.isnull()`的别名）来检查所有列的空值，并计算结果的摘要：
- en: '[PRE3]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`Out:`'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: We need to be careful using the `speaker` and `position` columns, as the output
    tells us that this information is not always available! To prevent any problems,
    we could substitute the missing values with some generic value such as `unknown
    speaker` or `unknown position` or just the empty string.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要谨慎使用`speaker`和`position`列，因为输出告诉我们这些信息并不总是可用的！为了避免任何问题，我们可以用一些通用值来替换缺失值，比如`unknown
    speaker`或`unknown position`，或者只是空字符串。
- en: 'Pandas supplies the function `df.fillna()` for that purpose:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas 提供了 `df.fillna()` 函数来实现这一目的：
- en: '[PRE5]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'But even the existing values can be problematic because the same speaker’s
    name is sometimes spelled differently or even ambiguously. The following statement
    computes the number of records per speaker for all documents containing `Bush`
    in the speaker column:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 但即使是现有的值可能也存在问题，因为同一演讲者的姓名有时拼写不同甚至含糊不清。以下语句计算包含演讲者列中 `Bush` 的所有文档的每位演讲者的记录数量：
- en: '[PRE6]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '`Out:`'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE7]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Any analysis on speaker names would produce the wrong results unless we resolve
    these ambiguities. So, we had better check the distinct values of categorical
    attributes. Knowing this, we will ignore the speaker information here.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 除非我们解决这些歧义，否则对发言者姓名的任何分析都会产生错误结果。因此，最好检查分类属性的不同值。了解到这一点后，我们将忽略演讲者信息。
- en: Plotting Value Distributions
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 绘制数值分布图
- en: 'One way to visualize the five-number summary of a numerical distribution is
    a [box plot](https://oreil.ly/7xZJ_). It can be easily produced by Pandas’s built-in
    plot functionality. Let’s take a look at the box plot for the `length` column:'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 用于可视化数值分布的一种方式是使用 [箱线图](https://oreil.ly/7xZJ_) 来展示数值分布的五数概括。Pandas内置的绘图功能能够轻松生成这种图表。让我们看一下
    `length` 列的箱线图：
- en: '[PRE8]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '`Out:`'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_01in01.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_01in01.jpg)'
- en: 'As illustrated by this plot, 50% percent of the speeches (the box in the middle)
    have a length between roughly 12,000 and 22,000 characters, with the median at
    about 16,000 and a long tail with many outliers to the right. The distribution
    is obviously left-skewed. We can get some more details by plotting a histogram:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 正如这个图表所示，50% 的演讲（中间的箱子）长度大约在12,000到22,000个字符之间，中位数约为16,000，并且右侧有很多异常值的长尾。该分布显然是左偏的。通过绘制直方图，我们可以获取更多细节：
- en: '[PRE9]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '`Out:`'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '![](Images/btap_01in02.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_01in02.jpg)'
- en: For the histogram, the value range of the `length` column is divided into 30
    intervals of equal width, the *bins*. The y-axis shows the number of documents
    falling into each of these bins.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 对于直方图，`length` 列的值范围被划分为30个等宽的间隔，即*柱状*。y轴显示每个柱中的文档数量。
- en: Comparing Value Distributions Across Categories
  id: totrans-81
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 比较不同类别的数值分布
- en: Peculiarities in the data often become visible when different subsets of the
    data are examined. A nice visualization to compare distributions across different
    categories is Seaborn’s [`catplot`](https://oreil.ly/jhlEE).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当不同数据子集被检查时，数据的特殊性通常会变得明显。用于比较不同类别分布的一种优秀可视化方式是 Seaborn 的 [`catplot`](https://oreil.ly/jhlEE)。
- en: 'We show box and violin plots to compare the distributions of the speech length
    of the five permanent members of the UN security council ([Figure 1-2](#fig-box-violin)).
    Thus, the category for the x-axis of `sns.catplot` is `country`:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示箱线图和小提琴图，以比较联合国安全理事会五个常任理事国演讲长度的分布（[图 1-2](#fig-box-violin)）。因此，`sns.catplot`
    的 x 轴类别是 `country`：
- en: '[PRE10]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](Images/btap_0102.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0102.jpg)'
- en: Figure 1-2\. Box plots (left) and violin plots (right) visualizing the distribution
    of speech lengths for selected countries.
  id: totrans-86
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 箱线图（左）和小提琴图（右），展示了选定国家演讲长度的分布情况。
- en: The violin plot is the “smoothed” version of a box plot. Frequencies are visualized
    by the width of the violin body, while the box is still visible inside the violin.
    Both plots reveal that the dispersion of values, in this case the lengths of the
    speeches, for Russia is much larger than for Great Britain. But the existence
    of multiple peaks, as in Russia, only becomes apparent in the violin plot.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 小提琴图是箱线图的“平滑”版本。通过小提琴体的宽度来可视化频率，同时箱线仍然可见于小提琴内部。这两种图表显示，对于俄罗斯而言，演讲长度的值分布范围要比英国大得多。但是，如俄罗斯的多个峰值存在只有在小提琴图中才能明显看出。
- en: Visualizing Developments Over Time
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 时间序列的可视化发展
- en: 'If your data contains date or time attributes, it is always interesting to
    visualize some developments within the data over time. A first time series can
    be created by analyzing the number of speeches per year. We can use the Pandas
    grouping function `size()` to return the number of rows per group. By simply appending
    `plot()`, we can visualize the resulting `DataFrame` ([Figure 1-3](#fig-timeline),
    left):'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的数据包含日期或时间属性，将数据随时间的发展进行可视化通常会很有趣。首先，可以通过分析每年演讲次数来创建时间序列。我们可以使用Pandas的分组函数
    `size()` 来返回每个组的行数。通过简单地附加 `plot()`，我们可以可视化生成的 `DataFrame`（[图 1-3](#fig-timeline)，左侧）：
- en: '[PRE11]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The timeline reflects the development of the number of countries in the UN,
    as each country is eligible for only one speech per year. Actually, the UN has
    193 members today. Interestingly, the speech length needed to decrease with more
    countries entering the debates, as the following analysis reveals ([Figure 1-3](#fig-timeline),
    right):'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 时间轴反映了联合国成员国数量的发展，因为每个国家每年只有一次发言机会。事实上，联合国今天有193个成员国。有趣的是，随着更多国家参与辩论，所需的演讲长度也在减少，如下面的分析所显示（见[图1-3](#fig-timeline)，右图）：
- en: '[PRE12]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '![](Images/btap_0103.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0103.jpg)'
- en: Figure 1-3\. Number of countries and average speech length over time.
  id: totrans-94
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-3\. 随时间变化的国家数量和平均演讲长度。
- en: Note
  id: totrans-95
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Pandas dataframes not only can be easily visualized in Jupyter notebooks but
    also can be exported to Excel (*.xlsx*), HTML, CSV, LaTeX, and many other formats
    by built-in functions. There is even a `to_clipboard()` function. Check the [documentation](https://oreil.ly/HZDVN)
    for details.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas数据框不仅可以在Jupyter笔记本中轻松可视化，还可以通过内置函数导出到Excel (*.xlsx*)、HTML、CSV、LaTeX和许多其他格式。甚至还有一个`to_clipboard()`函数。查看[文档](https://oreil.ly/HZDVN)获取详情。
- en: 'Blueprint: Building a Simple Text Preprocessing Pipeline'
  id: totrans-97
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：构建一个简单的文本预处理流水线
- en: The analysis of metadata such as categories, time, authors, and other attributes
    gives some first insights on the corpus. But it’s much more interesting to dig
    deeper into the actual content and explore frequent words in different subsets
    or time periods. In this section, we will develop a basic blueprint to prepare
    text for a quick first analysis consisting of a simple sequence of steps ([Figure 1-4](#fig-simple-pipeline)).
    As the output of one operation forms the input of the next one, such a sequence
    is also called a *processing pipeline* that transforms the original text into
    a number of tokens.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据分析，如类别、时间、作者和其他属性，可以为语料库提供一些初步见解。但更有趣的是深入挖掘实际内容，探索不同子集或时间段中的常见词语。在本节中，我们将开发一个基本的蓝图，准备文本进行快速的初步分析，由一系列步骤组成（见[图1-4](#fig-simple-pipeline)）。由于每个操作的输出形成下一个操作的输入，这样的顺序也称为*处理流水线*，将原始文本转换为一系列标记。
- en: '![](Images/btap_0104.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0104.jpg)'
- en: Figure 1-4\. Simple preprocessing pipeline.
  id: totrans-100
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 简单的预处理流水线。
- en: 'The pipeline presented here consists of three steps: case-folding into lowercase,
    tokenization, and stop word removal. These steps will be discussed in depth and
    extended in [Chapter 4](ch04.xhtml#ch-preparation), where we make use of spaCy.
    To keep it fast and simple here, we build our own tokenizer based on regular expressions
    and show how to use an arbitrary stop word list.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这里呈现的流水线包括三个步骤：大小写转换为小写、分词和停用词去除。这些步骤将在[第四章](ch04.xhtml#ch-preparation)中深入讨论和扩展，我们将使用spaCy。为了保持快速和简单，我们在这里基于正则表达式构建自己的分词器，并展示如何使用任意的停用词列表。
- en: Performing Tokenization with Regular Expressions
  id: totrans-102
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用正则表达式进行分词
- en: '*Tokenization* is the process of extracting words from a sequence of characters.
    In Western languages, words are often separated by whitespaces and punctuation
    characters. Thus, the simplest and fastest tokenizer is Python’s native `str.split()`
    method, which splits on whitespace. A more flexible way is to use regular expressions.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: '*分词*是从字符序列中提取单词的过程。在西方语言中，单词通常由空格和标点符号分隔。因此，最简单和最快的分词器是Python的本地`str.split()`方法，它以空格分割。更灵活的方式是使用正则表达式。'
- en: Regular expressions and the Python libraries `re` and `regex` will be introduced
    in more detail in [Chapter 4](ch04.xhtml#ch-preparation). Here, we want to apply
    a simple pattern that matches words. Words in our definition consist of at least
    one letter as well as digits and hyphens. Pure numbers are skipped because they
    almost exclusively represent dates or speech or session identifiers in this corpus.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式和Python库`re`和`regex`将在[第四章](ch04.xhtml#ch-preparation)中详细介绍。在这里，我们希望应用一个简单的模式来匹配单词。在我们的定义中，单词至少包含一个字母以及数字和连字符。纯数字被跳过，因为它们几乎只代表这个语料库中的日期、讲话或会话标识符。
- en: 'The frequently used expression `[A-Za-z]` is not a good option for matching
    letters because it misses accented letters like *ä* or *â*. Much better is the
    POSIX character class `\p{L}`, which selects all Unicode letters. Note that we
    need the [`regex` library](https://oreil.ly/hJ6M2) instead of `re` to work with
    POSIX character classes. The following expression matches tokens consisting of
    at least one letter (`\p{L}`), preceded and followed by an arbitrary sequence
    of alphanumeric characters (`\w` includes digits, letters, and underscore) and
    hyphens (`-`):'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁使用的表达式`[A-Za-z]`不适合匹配字母，因为它会忽略像*ä*或*â*这样的重音字母。更好的选择是POSIX字符类`\p{L}`，它选择所有Unicode字母。请注意，我们需要使用[`regex`库](https://oreil.ly/hJ6M2)而不是`re`来处理POSIX字符类。以下表达式匹配由至少一个字母组成的标记（`\p{L}`），前后是任意的字母数字字符（`\w`包括数字、字母和下划线）和连字符（`-`）的序列：
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Let’s try it with a sample sentence from the corpus:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试使用语料库中的一个示例句子：
- en: '[PRE14]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '`Out:`'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE15]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Treating Stop Words
  id: totrans-111
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理停用词
- en: The most frequent words in text are common words such as determiners, auxiliary
    verbs, pronouns, adverbs, and so on. These words are called *stop words*. Stop
    words usually don’t carry much information but hide interesting content because
    of their high frequencies. Therefore, stop words are often removed before data
    analysis or model training.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 文本中最常见的词是诸如限定词、助动词、代词、副词等常见词汇。这些词称为*停用词*。停用词通常不携带太多信息，但由于其高频率而隐藏了有趣的内容。因此，在数据分析或模型训练之前通常会删除停用词。
- en: 'In this section, we show how to discard stop words contained in a predefined
    list. Common stop word lists are available for many languages and are integrated
    in almost any NLP library. We will work with NLTK’s list of stop words here, but
    you could use any list of words as a filter.^([2](ch01.xhtml#idm45634218281240))
    For fast lookup, you should always convert a list to a set. Sets are hash-based
    structures like dictionaries with nearly constant lookup time:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们展示如何丢弃预定义列表中包含的停用词。许多语言都有通用的停用词列表，并且几乎所有的自然语言处理库都集成了这些列表。我们将在这里使用NLTK的停用词列表，但你可以使用任何单词列表作为过滤器。^([2](ch01.xhtml#idm45634218281240))
    为了快速查找，你应该总是将列表转换为集合。集合是基于哈希的数据结构，类似于字典，具有几乎恒定的查找时间：
- en: '[PRE16]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Our approach to remove stop words from a given list, wrapped into the small
    function shown here, consists of a simple list comprehension. For the check, tokens
    are converted to lowercase as NLTK’s list contains only lowercase words:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从给定列表中移除停用词的方法，封装成下面展示的小函数，通过简单的列表推导来实现检查。作为NLTK的列表只包含小写词汇，因此将标记转换为小写：
- en: '[PRE17]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Often you’ll need to add domain-specific stop words to the predefined list.
    For example, if you are analyzing emails, the terms *dear* and *regards* will
    probably appear in almost any document. On the other hand, you might want to treat
    some of the words in the predefined list not as stop words. We can add additional
    stop words and exclude others from the list using two of Python’s set operators,
    `|` (union/or) and `-` (difference):'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，您需要将领域特定的停用词添加到预定义的列表中。例如，如果您正在分析电子邮件，术语*dear*和*regards*可能会出现在几乎所有文档中。另一方面，您可能希望将预定义列表中的某些词视为非停用词。我们可以使用Python的两个集合运算符`|`（并集/或）和`-`（差集）添加额外的停用词并排除列表中的其他词：
- en: '[PRE18]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'The stop word list from NLTK is conservative and contains only 179 words. Surprisingly,
    *would* is not considered a stop word, while *wouldn’t* is. This illustrates a
    common problem with predefined stop word lists: inconsistency. Be aware that removing
    stop words can significantly affect the performance of semantically targeted analyses,
    as explained in [“Why Removing Stop Words Can Be Dangerous”](#why-rm-stop-words-can-be-danger).'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: NLTK的停用词列表保守，仅包含179个词。令人惊讶的是，*would*不被视为停用词，而*wouldn’t*却是。这说明了预定义停用词列表常见的问题：不一致性。请注意，删除停用词可能会显著影响语义目标分析的性能，详细说明请参见[“为什么删除停用词可能是危险的”](#why-rm-stop-words-can-be-danger)。
- en: In addition to or instead of a fixed list of stop words, it can be helpful to
    treat every word that appears in more than, say, 80% of the documents as a stop
    word. Such common words make it difficult to distinguish content. The parameter
    `max_df` of the scikit-learn vectorizers, as covered in [Chapter 5](ch05.xhtml#ch-vectorization),
    does exactly this. Another method is to filter words based on the word type (part
    of speech). This concept will be explained in [Chapter 4](ch04.xhtml#ch-preparation).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 除了或替代固定的停用词列表外，将每个在文档中出现频率超过80%的单词视为停用词也可能很有帮助。这些常见词汇会使内容难以区分。scikit-learn向量化器的参数`max_df`，如[第5章](ch05.xhtml#ch-vectorization)中所述，正是为此而设计。另一种方法是根据词类别（词性）过滤单词。这个概念将在[第4章](ch04.xhtml#ch-preparation)中解释。
- en: Processing a Pipeline with One Line of Code
  id: totrans-121
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用一行代码处理流水线
- en: 'Let’s get back to the `DataFrame` containing the documents of our corpus. We
    want to create a new column called `tokens` containing the lowercased, tokenized
    text without stop words for each document. For that, we use an extensible pattern
    for a processing pipeline. In our case, we will change all text to lowercase,
    tokenize it, and remove stop words. Other operations can be added by simply extending
    the pipeline:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到包含语料库文档的`DataFrame`。我们想要创建一个名为`tokens`的新列，其中包含每个文档的小写化、标记化文本，而且没有停用词。为此，我们使用一个可扩展的处理流程模式。在我们的案例中，我们将所有文本转换为小写，进行标记化，并去除停用词。通过简单扩展这个流程，可以添加其他操作：
- en: '[PRE19]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: If we put all this into a function, it becomes a perfect use case for Pandas’s
    `map` or `apply` operation. Functions such as `map` and `apply`, which take other
    functions as parameters, are called *higher-order functions* in mathematics and
    computer science.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有这些放入一个函数中，它就成为了Pandas的`map`或`apply`操作的完美用例。在数学和计算机科学中，接受其他函数作为参数的函数（如`map`和`apply`）称为*高阶函数*。
- en: Table 1-2\. Pandas higher-order functions
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 表 1-2\. Pandas高阶函数
- en: '| Function | Description |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 函数 | 描述 |'
- en: '| --- | --- |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| `Series.map` | Works element by element on a Pandas `Series` |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| `Series.map` | 逐个元素作用于Pandas的`Series` |'
- en: '| `Series.apply` | Same as `map` but allows additional parameters |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| `Series.apply` | 与`map`相同，但允许额外参数 |'
- en: '| `DataFrame.applymap` | Element by element on a Pandas `DataFrame` (same as
    `map` on `Series`) |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '| `DataFrame.applymap` | 逐个元素作用于Pandas的`DataFrame`（与`Series`上的`map`相同） |'
- en: '| `DataFrame.apply` | Works on rows or columns of a `DataFrame` and supports
    aggregation |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| `DataFrame.apply` | 作用于`DataFrame`的行或列，并支持聚合 |'
- en: Pandas supports the different higher-order functions on series and dataframes
    ([Table 1-2](#tab-map-apply)). These functions not only allow you to specify a
    series of functional data transformations in a comprehensible way, but they can
    also be easily parallelized. The Python package [`pandarallel`](https://oreil.ly/qwPB4),
    for example, provides parallel versions of `map` and `apply`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Pandas支持在系列和数据框上的不同高阶函数（[表 1-2](#tab-map-apply)）。这些函数不仅可以让您以一种易于理解的方式指定一系列的功能数据转换，而且可以轻松并行化。例如，Python包[`pandarallel`](https://oreil.ly/qwPB4)提供了`map`和`apply`的并行版本。
- en: Scalable frameworks like [Apache Spark](https://spark.apache.org) support similar
    operations on dataframes even more elegantly. In fact, the map and reduce operations
    in distributed programming are based on the same principle of functional programming.
    In addition, many programming languages, including Python and JavaScript, have
    a native map operation for lists or arrays.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 像[Apache Spark](https://spark.apache.org)这样的可扩展框架支持更加优雅的数据框操作。事实上，在分布式编程中，`map`和`reduce`操作基于函数式编程的同一原理。此外，许多编程语言，包括Python和JavaScript，都有针对列表或数组的本地`map`操作。
- en: 'Using one of Pandas’s higher-order operations, applying a functional transformation
    becomes a one-liner:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Pandas的一个高阶操作，应用功能转换变成了一行代码。
- en: '[PRE20]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'The `tokens` column now consists of Python lists containing the extracted tokens
    for each document. Of course, this additional column basically doubles memory
    consumption of the `DataFrame`, but it allows you to quickly access the tokens
    directly for further analysis. Nevertheless, the following blueprints are designed
    in such a way that the tokenization can also be performed on the fly during analysis.
    In this way, performance can be traded for memory consumption: either tokenize
    once before analysis and consume memory or tokenize on the fly and wait.'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 现在`tokens`列包含每个文档中提取的令牌的 Python 列表。当然，这个额外的列基本上会将`DataFrame`的内存消耗翻倍，但它允许您直接快速访问令牌以进行进一步分析。尽管如此，以下蓝图设计的方式使得令牌化也可以在分析过程中即时执行。通过这种方式，性能可以用内存消耗来交换：要么在分析前进行一次令牌化并消耗内存，要么在分析过程中动态令牌化并等待。
- en: 'We also add another column containing the length of the token list for summarizations
    later:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还添加了另一列，包含令牌列表的长度，以便后续摘要使用：
- en: '[PRE21]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-139
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: '`tqdm` (pronounced *taqadum* for “progress” in Arabic) is a great library for
    progress bars in Python. It supports conventional loops, e.g., by using `tqdm_range`
    instead of `range`, and it supports Pandas by providing `progress_map` and `progress_apply`
    operations on dataframes.^([3](ch01.xhtml#idm45634217921624)) Our accompanying
    notebooks on GitHub use these operations, but we stick to plain Pandas here in
    the book.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`tqdm`（阿拉伯语中“进展”的发音是*taqadum*）是 Python 中优秀的进度条库。它支持传统的循环，例如使用`tqdm_range`替代`range`，并且通过提供在数据框上的`progress_map`和`progress_apply`操作支持
    Pandas。^([3](ch01.xhtml#idm45634217921624)) 我们在 GitHub 上的相关笔记本使用这些操作，但在本书中我们仅使用纯粹的
    Pandas。'
- en: Blueprints for Word Frequency Analysis
  id: totrans-141
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 词频分析的蓝图
- en: 'Frequently used words and phrases can give us some basic understanding of the
    discussed topics. However, word frequency analysis ignores the order and the context
    of the words. This is the idea of the famous bag-of-words model (see also [Chapter 5](ch05.xhtml#ch-vectorization)):
    all the words are thrown into a bag where they tumble into a jumble. The original
    arrangement in the text is lost; only the frequency of the terms is taken into
    account. This model does not work well for complex tasks such as sentiment analysis
    or question answering, but it works surprisingly well for classification and topic
    modeling. In addition, it’s a good starting point for understanding what the texts
    are all about.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 频繁使用的单词和短语可以帮助我们基本了解讨论的主题。然而，词频分析忽略了单词的顺序和上下文。这就是著名的词袋模型的理念（参见[第 5 章](ch05.xhtml#ch-vectorization)）：所有单词都被扔进一个袋子里，它们在里面翻滚成一团乱麻。原始文本中的排列被丢失，只有词项的频率被考虑进来。这个模型对于情感分析或问题回答等复杂任务效果不佳，但对于分类和主题建模却表现出色。此外，它是理解文本内容起点良好的一种方式。
- en: In this section, we will develop a number of blueprints to calculate and visualize
    word frequencies. As raw frequencies overweigh unimportant but frequent words,
    we will also introduce TF-IDF at the end of the process. We will implement the
    frequency calculation by using a `Counter` because it is simple and extremely
    fast.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开发多个蓝图来计算和可视化词频。由于原始频率过高导致不重要但频繁出现的单词占主导地位，因此我们在过程末尾还将引入 TF-IDF。我们将使用`Counter`来实现频率计算，因为它简单且速度极快。
- en: 'Blueprint: Counting Words with a Counter'
  id: totrans-144
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：使用计数器计数单词
- en: 'Python’s standard library has a built-in class `Counter`, which does exactly
    what you might think: it counts things.^([4](ch01.xhtml#idm45634217882104)) The
    easiest way to work with a counter is to create it from a list of items, in our
    case strings representing the words or tokens. The resulting counter is basically
    a dictionary object containing those items as keys and their frequencies as values.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Python 的标准库中内置了一个名为`Counter`的类，它正如其名字所示：用来计数。^([4](ch01.xhtml#idm45634217882104))
    使用计数器的最简单方式是从一个项目列表创建它，本例中是代表单词或标记的字符串。生成的计数器基本上是一个包含这些项目作为键和它们频率作为值的字典对象。
- en: 'Let’s illustrate its functionality with a simple example:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来说明它的功能：
- en: '[PRE22]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '`Out:`'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE23]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The counter requires a list as input, so any text needs to be tokenized in
    advance. What’s nice about the counter is that it can be incrementally updated
    with a list of tokens of a second document:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 计数器需要一个列表作为输入，因此任何文本都需要预先进行令牌化。计数器的好处在于它可以通过第二个文档的令牌列表进行增量更新。
- en: '[PRE24]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '`Out:`'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: '`Out:`'
- en: '[PRE25]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: To find the most frequent words within a corpus, we need to create a counter
    from the list of all words in all documents. A naive approach would be to concatenate
    all documents into a single, giant list of tokens, but that does not scale for
    larger datasets. It is much more efficient to call the `update` function of the
    counter object for each single document.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找语料库中最频繁出现的单词，我们需要从所有文档中的单词列表创建一个计数器。一个简单的方法是将所有文档连接成一个巨大的标记列表，但对于较大的数据集来说这不可扩展。对于每个单个文档，调用计数器对象的`update`函数要高效得多。
- en: '[PRE26]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We do a little trick here and put `counter.update` in the `map` function. The
    magic happens inside the `update` function under the hood. The whole `map` call
    runs extremely fast; it takes only about three seconds for the 7,500 UN speeches
    and scales linearly with the total number of tokens. The reason is that dictionaries
    in general and counters in particular are implemented as hash tables. A single
    counter is pretty compact compared to the whole corpus: it contains each word
    only once, along with its frequency.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做了一个小技巧，将 `counter.update` 放入 `map` 函数中。奇迹发生在 `update` 函数内部。整个 `map` 调用运行得非常快；对于7500篇联合国演讲，仅需约三秒，并且与标记总数成线性关系。原因是一般而言字典和特别是计数器都实现为哈希表。单个计数器相对于整个语料库来说非常紧凑：它只包含每个单词一次以及其频率。
- en: 'Now we can retrieve the most common words in the text with the respective counter
    function:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用相应的计数器函数检索文本中最常见的单词：
- en: '[PRE27]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '`Out:`'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE28]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For further processing and analysis, it is much more convenient to transform
    the counter into a Pandas `DataFrame`, and this is what the following blueprint
    function finally does. The tokens make up the index of the `DataFrame`, while
    the frequency values are stored in a column named `freq`. The rows are sorted
    so that the most frequent words appear at the head:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进一步处理和分析，将计数器转换为 Pandas `DataFrame` 要方便得多，这正是以下蓝图函数最终要做的。标记构成 `DataFrame`
    的索引，而频率值存储在名为 `freq` 的列中。行已排序，使得最常见的单词出现在前面：
- en: '[PRE29]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The function takes, as a first parameter, a Pandas `DataFrame` and takes the
    column name containing the tokens or the text as a second parameter. As we already
    stored the prepared tokens in the column `tokens` of the `DataFrame` containing
    the speeches, we can use the following two lines of code to compute the `DataFrame`
    with word frequencies and display the top five tokens:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的第一个参数是一个 Pandas `DataFrame`，第二个参数是包含标记或文本的列名。由于我们已经将准备好的标记存储在包含演讲内容的 `DataFrame`
    的列 `tokens` 中，我们可以使用以下两行代码计算包含单词频率并显示前五个标记的 `DataFrame`：
- en: '[PRE30]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: '`Out:`'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| token | freq |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 频率 |'
- en: '| --- | --- |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| nations | 124508 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 124508 |'
- en: '| united | 120763 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 联合 | 120763 |'
- en: '| international | 117223 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 国际 | 117223 |'
- en: '| world | 89421 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 世界 | 89421 |'
- en: '| countries | 85734 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 国家 | 85734 |'
- en: 'If we don’t want to use precomputed tokens for some special analysis, we could
    tokenize the text on the fly with a custom preprocessing function as the third
    parameter. For example, we could generate and count all words with 10 or more
    characters with this on-the-fly tokenization of the text:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不想对一些特殊分析使用预先计算的标记，我们可以使用自定义预处理函数作为第三个参数来动态标记文本。例如，我们可以通过文本的即时标记化生成并计数所有具有10个或更多字符的单词：
- en: '[PRE31]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The last parameter of `count_words` defines a minimum frequency of tokens to
    be included in the result. Its default is set to 2 to cut down the long tail of
    hapaxes, i.e., tokens occurring only once.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: '`count_words` 的最后一个参数定义了要包含在结果中的最小标记频率。其默认值设置为2，以削减出现仅一次的偶发标记的长尾部分。'
- en: 'Blueprint: Creating a Frequency Diagram'
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：创建频率图
- en: 'There are dozens of ways to produce tables and diagrams in Python. We prefer
    Pandas with its built-in plot functionality because it is easier to use than plain
    Matplotlib. We assume a `DataFrame` `freq_df` generated by the previous blueprint
    for visualization. Creating a frequency diagram based on such a `DataFrame` now
    becomes basically a one-liner. We add two more lines for formatting:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Python 中有几十种生成表格和图表的方法。我们喜欢 Pandas 和其内置的绘图功能，因为它比纯 Matplotlib 更容易使用。我们假设由前述蓝图生成的
    `DataFrame` `freq_df` 用于可视化。基于这样一个 `DataFrame` 创建频率图现在基本上变成了一行代码。我们再添加两行格式化代码：
- en: '[PRE32]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '`Out:`'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '![](Images/btap_01in03.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_01in03.jpg)'
- en: Using horizontal bars (`barh`) for word frequencies greatly improves readability
    because the words appear horizontally on the y-axis in a readable form. The y-axis
    is inverted to place the top words at the top of the chart. The axis labels and
    title can optionally be modified.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 使用水平条 (`barh`) 来显示词频极大地提高了可读性，因为单词在 y 轴上以可读的形式水平显示。 y 轴被反转以将顶部的单词放置在图表的顶部。 可以选择修改坐标轴标签和标题。
- en: 'Blueprint: Creating Word Clouds'
  id: totrans-182
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：创建词云
- en: Plots of frequency distributions like the ones shown previously give detailed
    information about the token frequencies. But it is quite difficult to compare
    frequency diagrams for different time periods, categories, authors, and so on.
    Word clouds, in contrast, visualize the frequencies by different font sizes. They
    are much easier to comprehend and to compare, but they lack the precision of tables
    and bar charts. You should keep in mind that long words or words with capital
    letters get unproportionally high attraction.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前显示的频率分布图一样，详细显示了标记频率的信息。 但是，对于不同的时间段、类别、作者等进行频率图的比较是相当困难的。 相比之下，词云通过不同字体大小来可视化频率。
    它们更容易理解和比较，但缺乏表格和条形图的精确性。 您应该记住，长单词或带有大写字母的单词会吸引不成比例的高关注度。
- en: 'The Python module [`wordcloud`](https://oreil.ly/RV0r5) generates nice word
    clouds from texts or counters. The simplest way to use it is to instantiate a
    word cloud object with some options, such as the maximum number of words and a
    stop word list, and then let the `wordcloud` module handle the tokenization and
    stop word removal. The following code shows how to generate a word cloud for the
    text of the 2015 US speech and display the resulting image with Matplotlib:'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: Python 模块 [`wordcloud`](https://oreil.ly/RV0r5) 从文本或计数器生成漂亮的词云。 使用它的最简单方法是实例化一个词云对象，带有一些选项，例如最大单词数和停用词列表，然后让
    `wordcloud` 模块处理标记化和停用词移除。 以下代码显示了如何为 2015 年美国演讲的文本生成词云，并显示生成的图像与 Matplotlib：
- en: '[PRE33]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: However, this works only for a single text and not a (potentially large) set
    of documents. For the latter use case, it is much faster to create a frequency
    counter first and then use the function `generate_from_frequencies()`.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这仅适用于单个文本，而不是（可能很大的）文档集。 对于后一种用例，首先创建一个频率计数器，然后使用函数 `generate_from_frequencies()`
    要快得多。
- en: 'Our blueprint is a little wrapper around this function to also support a Pandas
    `Series` containing frequency values as created by `count_words`. The `WordCloud`
    class already has a magnitude of options to fine-tune the result. We use some
    of them in the following function to demonstrate possible adjustments, but you
    should check the documentation for details:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的蓝图是在此函数周围做了一点小包装，以支持由 `count_words` 创建的 Pandas `Series` 包含的频率值。 `WordCloud`
    类已经有许多选项可以微调结果。 我们在以下函数中使用了其中一些来演示可能的调整，但您应该查看详细文档：
- en: '[PRE34]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The function has two convenience parameters to filter words. `skip_n` skips
    the top *n* words of the list. Obviously, in a UN corpus words like *united*,
    *nations*, or *international* are heading the list. It may be more interesting
    to visualize what comes next. The second filter is an (additional) list of stop
    words. Sometimes it is helpful to filter out specific frequent but uninteresting
    words for the visualization only.^([5](ch01.xhtml#idm45634217138552))
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数有两个方便的参数来过滤单词。 `skip_n` 跳过列表中前 *n* 个单词。 显然，在联合国语料库中，像 *united*、*nations*
    或 *international* 这样的单词位于列表的前列。 可视化之后，过滤掉特定但无趣的频繁单词可能更有帮助。 第二个过滤器是一个（额外的）停用词列表。
    有时，仅在可视化时过滤掉特定频繁但无趣的单词是有帮助的。 ^([5](ch01.xhtml#idm45634217138552))
- en: 'So, let’s take a look at the 2015 speeches ([Figure 1-5](#fig-2015-speeches)).
    The left word cloud visualizes the most frequent words unfiltered. The right word
    cloud instead treats the 50 most frequent words of the complete corpus as stop
    words:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们来看看 2015 年的演讲（[图示 1-5](#fig-2015-speeches)）。 左侧的词云可视化了最常见的单词，未经过滤。 而右侧的词云则将整个语料库中最频繁的
    50 个单词视为停用词：
- en: '[PRE35]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: '![](Images/btap_0105.jpg)'
  id: totrans-192
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0105.jpg)'
- en: Figure 1-5\. Word clouds for the 2015 speeches including all words (left) and
    without the 50 most frequent words (right).
  id: totrans-193
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 2015 年演讲的词云，包含所有单词（左）和不包含 50 个最频繁单词（右）。
- en: Clearly, the right word cloud without the most frequent words of the corpus
    gives a much better idea of the 2015 topics, but there are still frequent and
    unspecific words like *today* or *challenges*. We need a way to give less weight
    to those words, as shown in the next section.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，正确的词云在去除了语料库中最频繁出现的词后，更好地展示了 2015 年的主题，但仍然存在像 *today* 或 *challenges* 这样频繁且不具体的词语。我们需要一种方法来减少这些词语的权重，如下一节所示。
- en: 'Blueprint: Ranking with TF-IDF'
  id: totrans-195
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 蓝图：TF-IDF 排名
- en: As illustrated in [Figure 1-5](#fig-2015-speeches), visualizing the most frequent
    words usually does not reveal much insight. Even if stop words are removed, the
    most common words are usually obvious domain-specific terms that are quite similar
    in any subset (slice) of the data. But we would like to give more importance to
    those words that appear more frequently in a given slice of the data than “usual.”
    Such a slice can be any subset of the corpus, e.g., a single speech, the speeches
    of a certain decade, or the speeches from one country.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 [1-5](#fig-2015-speeches) 所示，可视化最频繁出现的词通常并不会带来深刻的洞见。即使去除停用词，最常见的词通常是显而易见的特定领域术语，在数据的任何子集（切片）中都相似。但我们希望更加重视那些在给定数据切片中比“通常”更频繁出现的词语。这样的切片可以是语料库的任何子集，例如单篇演讲、某个十年的演讲，或者来自某个国家的演讲。
- en: We want to highlight words whose actual word frequency in a slice is higher
    than their total probability would suggest. There is a number of algorithms to
    measure the “surprise” factor of a word. One of the simplest but best working
    approaches is to complement the term frequency with the inverse document frequency
    (see sidebar).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望突出显示那些在切片中实际词频高于其总概率所表明的词语。有多种算法可以衡量词语的“惊讶”因素。其中一种最简单但效果最好的方法是将词频与逆文档频率结合（见侧边栏）。
- en: 'Let’s define a function to compute the IDF for all terms in the corpus. It
    is almost identical to `count_words`, except that each token is counted only once
    per document (`counter.update(set(tokens))`), and the IDF values are computed
    after counting. The parameter `min_df` serves as a filter for the long tail of
    infrequent words. The result of this function is again a `DataFrame`:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个函数来计算语料库中所有术语的 IDF。它几乎与 `count_words` 相同，不同之处在于每个标记仅在每个文档中计算一次（`counter.update(set(tokens))`），并且在计数后计算
    IDF 值。参数 `min_df` 用作罕见词的长尾过滤器。该函数的结果再次是一个 `DataFrame`：
- en: '[PRE36]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'The IDF values need to be computed once for the entire corpus (do not use a
    subset here!) and can then be used in all kinds of analyses. We create a `DataFrame`
    containing the IDF values for each token (`idf_df`) with this function:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: IDF 值需要一次性计算整个语料库（不要在此处使用子集！），然后可以在各种分析中使用。我们使用此函数创建一个包含每个标记 IDF 值的 `DataFrame`
    (`idf_df`)：
- en: '[PRE37]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'As both the IDF and the frequency `DataFrame` have an index consisting of the
    tokens, we can simply multiply the columns of both `DataFrame`s to calculate the
    TF-IDF score for the terms:'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 IDF 和词频 `DataFrame` 都有一个由标记组成的索引，我们可以简单地将两个 `DataFrame` 的列相乘，以计算术语的 TF-IDF
    分数：
- en: '[PRE38]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Let’s compare the word clouds based on word counts (term frequencies) alone
    and TF-IDF scores for the speeches of the first and last years in the corpus.
    We remove some more stop words that stand for the numbers of the respective debate
    sessions.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较基于词频的词云和语料库中第一年和最后一年演讲的 TF-IDF 分数。我们去除了一些代表各自辩论会话次数的停用词。
- en: '[PRE39]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: The word clouds in [Figure 1-6](#fig-tf-idf) impressively demonstrate the power
    of TF-IDF weighting. While the most common words are almost identical in 1970
    and 2015, the TF-IDF weighted visualizations emphasize the differences of political
    topics.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 TF-IDF 加权的词云在 [图 1-6](#fig-tf-idf) 中生动展示了其威力。尽管 1970 年和 2015 年最常见的词几乎相同，但
    TF-IDF 加权的可视化强调了政治主题的差异。
- en: '![](Images/btap_0106.jpg)'
  id: totrans-207
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0106.jpg)'
- en: Figure 1-6\. Words weighted by plain counts (upper) and TF-IDF (lower) for speeches
    in two selected years.
  id: totrans-208
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. 两个选定年份演讲中，按纯计数（上）和 TF-IDF（下）加权的词语。
- en: The experienced reader might wonder why we implemented functions to count words
    and compute IDF values ourselves instead of using the classes `CountVectorizer`
    and `TfidfVectorizer` of scikit-learn. Actually, there two reasons. First, the
    vectorizers produce a vector with weighted term frequencies for each single document
    instead of arbitrary subsets of the dataset. Second, the results are matrices
    (good for machine learning) and not dataframes (good for slicing, aggregation,
    and visualization). We would have to write about the same number of code lines
    in the end to produce the results in [Figure 1-6](#fig-tf-idf) but miss the opportunity
    to introduce this important concept from scratch. The scikit-learn vectorizers
    will be discussed in detail in [Chapter 5](ch05.xhtml#ch-vectorization).
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 有经验的读者可能会想知道，为什么我们要自己实现计算单词数和计算 IDF 值的函数，而不是使用 scikit-learn 的 `CountVectorizer`
    和 `TfidfVectorizer` 类。实际上，有两个原因。首先，向量化器为每个单个文档生成加权词频向量，而不是数据集的任意子集。其次，结果是矩阵（适合机器学习），而不是数据框架（适合切片、聚合和可视化）。最终，为了生成
    [图1-6](#fig-tf-idf) 中的结果，我们将不得不编写大致相同数量的代码行，但错过了从头介绍这一重要概念的机会。scikit-learn 的向量化器将在
    [第5章](ch05.xhtml#ch-vectorization) 中详细讨论。
- en: 'Blueprint: Finding a Keyword-in-Context'
  id: totrans-210
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：寻找关键词上下文
- en: 'Word clouds and frequency diagrams are great tools to visually summarize textual
    data. However, they also often raise questions about why a certain term appears
    so prominently. For example, the *2015 TF-IDF* word cloud discussed earlier shows
    the terms *pv*, *sdgs*, or *sids*, and you probably do not know their meaning.
    To find that out, we need a way to inspect the actual occurrences of those words
    in the original, unprepared text. A simple yet clever way to do such an inspection
    is the keyword-in-context (KWIC) analysis. It produces a list of text fragments
    of equal length showing the left and right context of a keyword. Here is a sample
    of the KWIC list for *sdgs*, which gives us an explanation of that term:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 词云和频率图是视觉总结文本数据的强大工具。然而，它们通常也会引发关于为什么某个术语如此突出的问题。例如，前面讨论的 *2015 TF-IDF* 词云显示了术语
    *pv*、*sdgs* 或 *sids*，您可能不知道它们的含义。为了弄清楚这一点，我们需要一种检查这些词在原始未准备文本中实际出现情况的方法。一种简单而聪明的方法是关键词上下文分析（KWIC
    分析）。它生成显示关键词左右上下文的等长文本片段列表。以下是 *sdgs* 的 KWIC 列表示例，它为我们解释了这个术语：
- en: '[PRE40]'
  id: totrans-212
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Obviously, *sdgs* is the lowercased version of SDGs, which stands for “sustainable
    development goals.” With the same analysis we can learn that *sids* stands for
    “small island developing states.” That is important information to interpret the
    topics of 2015! *pv*, however, is a tokenization artifact. It is actually the
    remainder of citation references like *(A/70/PV.28)*, which stands for “Assembly
    70, Process Verbal 28,” i.e., speech 28 of the 70th assembly.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，*sdgs* 是 SDGs 的小写版本，SDGs 代表“可持续发展目标”。通过相同的分析，我们可以了解 *sids* 代表“小岛屿发展中国家”。这是解释
    2015 年主题的重要信息！*pv* 则是一个标记化的人为产物。实际上，它是引用参考文献的剩余部分，例如 *(A/70/PV.28)*，表示“第70届大会，28号议事录”，即第70届大会的第28次发言。
- en: Note
  id: totrans-214
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Always look into the details when you encounter tokens that you do not know
    or that do not make sense to you! Often they carry important information (like
    *sdgs*) that you as an analyst should be able to interpret. But you’ll also often
    find artifacts like *pv*. Those should be discarded if irrelevant or treated correctly.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 当您遇到不认识或不理解的令牌时，请务必深入了解细节！通常它们携带重要信息（如 *sdgs*），您作为分析师应能够解释。但您也经常会发现 *pv* 等人为产物。如果与您的分析无关，则应将其丢弃或正确处理。
- en: 'KWIC analysis is implemented in NLTK and textacy. We will use textacy’s [`KWIC`
    function](https://oreil.ly/-dSrA) because it is fast and works on the untokenized
    text. Thus, we can search for strings spanning multiple tokens like “climate change,”
    while NLTK cannot. Both NLTK and textacy’s KWIC functions work on a single document
    only. To extend the analysis to a number of documents in a `DataFrame`, we provide
    the following function:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: KWIC 分析已在 NLTK 和 textacy 中实现。我们将使用 textacy 的 [`KWIC` 函数](https://oreil.ly/-dSrA)，因为它快速且适用于未标记化的文本。因此，我们可以搜索跨越多个标记的字符串，如“气候变化”，而
    NLTK 无法做到。NLTK 和 textacy 的 KWIC 函数仅适用于单个文档。要将分析扩展到 `DataFrame` 中的若干文档，我们提供以下函数：
- en: '[PRE41]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: The function iteratively collects the keyword contexts by applying the `add_kwic`
    function to each document with `map`. This trick, which we already used in the
    word count blueprints, is very efficient and enables KWIC analysis also for larger
    corpora. By default, the function returns a list of tuples of the form `(left
    context, keyword, right context)`. If `print_samples` is greater than 0, a random
    sample of the results is printed.^([8](ch01.xhtml#idm45634216293848)) Sampling
    is especially useful when you work with lots of documents because the first entries
    of the list would otherwise stem from a single or a very small number of documents.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数通过将`map`应用于每个文档来迭代收集关键字上下文的关键字上下文，这是我们已经在单词计数蓝图中使用过的技巧，非常有效，并且还可以对更大的语料库进行KWIC分析。
    默认情况下，该函数返回形式为`(left context, keyword, right context)`的元组列表。 如果`print_samples`大于0，则会打印结果的随机样本。^([8](ch01.xhtml#idm45634216293848))
    当您处理大量文档时，采样尤其有用，因为列表的前几个条目否则将来自单个或非常少量的文档。
- en: 'The KWIC list for *sdgs* from earlier was generated by this call:'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的*sdgs*的KWIC列表是通过以下调用生成的：
- en: '[PRE42]'
  id: totrans-220
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Blueprint: Analyzing N-Grams'
  id: totrans-221
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：分析N-Grams
- en: Just knowing that climate is a frequent word does not tell us too much about
    the topic of discussion because, for example, *climate change* and *political
    climate* have completely different meanings. Even *change climate* is not the
    same as *climate change*. It can therefore be helpful to extend frequency analyses
    from single words to short sequences of two or three words.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 仅仅知道气候是一个常见的词并不能告诉我们太多关于讨论主题的信息，因为，例如，*climate change*和*political climate*有完全不同的含义。
    即使是*change climate*也不同于*climate change*。 因此，将频率分析从单个词扩展到两个或三个词的短序列可能会有所帮助。
- en: 'Basically, we are looking for two types of word sequences: compounds and collocations.
    A *compound* is a combination of two or more words with a specific meaning. In
    English, we find compounds in closed form, like *earthquake*; hyphenated form
    like *self-confident*; and open form like *climate change*. Thus, we may have
    to consider two tokens as a single semantic unit. *Collocations*, in contrast,
    are words that are frequently used together. Often, they consist of an adjective
    or verb and a noun, like *red carpet* or *united nations*.'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 基本上，我们正在寻找两种类型的词序列：化合物和搭配词。 *化合物*是具有特定含义的两个或更多个词的组合。 在英语中，我们发现以封闭形式出现的化合物，例如*earthquake*；以连字符形式出现的化合物，例如*self-confident*；以及以开放形式出现的化合物，例如*climate
    change*。 因此，我们可能需要将两个标记视为单个语义单位。 相反，*搭配词*是经常一起使用的词。 通常，它们由形容词或动词和名词组成，例如*red carpet*或*united
    nations*。
- en: In text processing, we usually work with bigrams (sequences of length 2), sometimes
    even trigrams (length 3). *n*-grams of size 1 are single words, also called *unigrams*.
    The reason to stick to <math alttext="n less-than-or-equal-to 3"><mrow><mi>n</mi>
    <mo>≤</mo> <mn>3</mn></mrow></math> is that the number of different n-grams increases
    exponentially with respect to *n*, while their frequencies decrease in the same
    way. By far the most trigrams appear only once in a corpus.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本处理中，我们通常处理bigrams（长度为2的序列），有时甚至是trigrams（长度为3）。 大小为1的n-grams是单个单词，也称为*unigrams*。
    坚持保持<math alttext="n less-than-or-equal-to 3"><mrow><mi>n</mi> <mo>≤</mo> <mn>3</mn></mrow></math>的原因是，不同的n-grams数量随着*n*的增加呈指数增长，而它们的频率以相同的方式减少。
    到目前为止，大多数trigrams在语料库中只出现一次。
- en: The following function produces elegantly the set of n-grams for a sequence
    of tokens:^([9](ch01.xhtml#idm45634216272552))
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 以下函数优雅地生成了一组标记序列的n-gram：^([9](ch01.xhtml#idm45634216272552))
- en: '[PRE43]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '`Out:`'
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE44]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'As you can see, most of the bigrams contain stop words like prepositions and
    determiners. Thus, it is advisable to build bigrams without stop words. But we
    need to be careful: if we remove the stop words first and then build the bigrams,
    we generate bigrams that don’t exist in the original text as a “manifestation
    global” in the example. Thus, we create the bigrams on all tokens but keep only
    those that do not contain any stop words with this modified `ngrams` function:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，大多数bigrams包含了像介词和冠词之类的停止词。 因此，建议构建不含停用词的bigrams。 但是我们需要小心：如果首先删除停止词然后构建bigrams，则会生成原始文本中不存在的bigrams，例如示例中的“manifestation
    global”。 因此，我们在所有标记上创建bigrams，但仅保留不包含任何停止词的bigrams，使用此修改后的`ngrams`函数：
- en: '[PRE45]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '`Out:`'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE46]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'Using this `ngrams` function, we can add a column containing all bigrams to
    our `DataFrame` and apply the word count blueprint to determine the top five bigrams:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此`ngrams`函数，我们可以向我们的`DataFrame`添加一个包含所有bigrams的列，并应用单词计数蓝图以确定前五个bigrams：
- en: '[PRE47]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '`Out:`'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| token | freq |'
  id: totrans-236
  prefs: []
  type: TYPE_TB
  zh: '| 标记 | 频率 |'
- en: '| --- | --- |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| united nations | 103236 |'
  id: totrans-238
  prefs: []
  type: TYPE_TB
  zh: '| 联合国 | 103236 |'
- en: '| international community | 27786 |'
  id: totrans-239
  prefs: []
  type: TYPE_TB
  zh: '| 国际社会 | 27786 |'
- en: '| general assembly | 27096 |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '| 大会 | 27096 |'
- en: '| security council | 20961 |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '| 安全理事会 | 20961 |'
- en: '| human rights | 19856 |'
  id: totrans-242
  prefs: []
  type: TYPE_TB
  zh: '| 人权 | 19856 |'
- en: You may have noticed that we ignored sentence boundaries during tokenization.
    Thus, we will generate nonsense bigrams with the last word of one sentence and
    the first word of the next. Those bigrams will not be very frequent, so they don’t
    really matter for data exploration. If we wanted to prevent this, we would need
    to identify sentence boundaries, which is much more complicated than word tokenization
    and not worth the effort here.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到我们在标记化过程中忽略了句子边界。因此，我们将生成最后一个句子的最后一个词和下一个句子的第一个词的无意义双字词。这些双字词不会很频繁，所以它们对数据探索并不重要。如果我们想要避免这种情况，我们需要识别句子边界，这比词标记化要复杂得多，在这里并不值得努力。
- en: 'Now let’s extend our TF-IDF-based unigram analysis from the previous section
    and include bigrams. We add the bigram IDF values, compute the TF-IDF-weighted
    bigram frequencies for all speeches from 2015, and generate a word cloud from
    the resulting `DataFrame`:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们扩展我们基于 TF-IDF 的单字词分析，包括双字词。我们添加了双字词的 IDF 值，计算了所有2015年演讲的 TF-IDF 加权双字词频率，并从结果的
    `DataFrame` 生成了一个词云：
- en: '[PRE48]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'As we can see in the word cloud on the left of [Figure 1-7](#fig-bigrams),
    *climate change* was a frequent bigram in 2015\. But to understand the different
    contexts of *climate*, it may be interesting to take a look at the bigrams containing
    *climate* only. We can use a text filter on *climate* to achieve this and plot
    the result again as a word cloud ([Figure 1-7](#fig-bigrams), right):'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[图1-7](#fig-bigrams)左侧的词云中看到的那样，*气候变化*是2015年的一个常见双字词。但是，为了理解*气候*的不同上下文，了解仅包含*气候*的双字词可能会很有趣。我们可以在*气候*上使用文本过滤器来实现这一点，并再次将结果绘制为词云（[图1-7](#fig-bigrams)，右侧）：
- en: '[PRE49]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '![](Images/btap_0107.jpg)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0107.jpg)'
- en: Figure 1-7\. Word clouds for all bigrams and bigrams containing the word *climate*.
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-7. 所有双字词和包含单词*climate*的双字词的词云。
- en: The approach presented here creates and weights all n-grams that do not contain
    stop words. For a first analysis, the results look quite good. We just don’t care
    about the long tail of infrequent bigrams. More sophisticated but also computationally
    expensive algorithms to identify collocations are available, for example, in [NLTK’s
    collocation finder](https://oreil.ly/uW-2A). We will show alternatives to identify
    meaningful phrases in Chapters [4](ch04.xhtml#ch-preparation) and [10](ch10.xhtml#ch-embeddings).
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 这里介绍的方法创建并加权所有不包含停用词的 n-gram。初步分析的结果看起来相当不错。我们只关心不频繁出现的双字词的长尾部分。还有更复杂但计算成本更高的算法可用于识别搭配词，例如在[NLTK的搭配词查找器](https://oreil.ly/uW-2A)中。我们将在[第4章](ch04.xhtml#ch-preparation)和[第10章](ch10.xhtml#ch-embeddings)展示识别有意义短语的替代方法。
- en: 'Blueprint: Comparing Frequencies Across Time Intervals and Categories'
  id: totrans-251
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 蓝图：比较时间间隔和类别之间的频率
- en: You surely know [Google Trends](http://trends.google.com), where you can track
    the development of a number of search terms over time. This kind of trend analysis
    computes frequencies by day and visualizes them with a line chart. We want to
    track the development of certain keywords over the course of the years in our
    UN Debates dataset to get an idea about the growing or shrinking importance of
    topics such as climate change, terrorism, or migration.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 你肯定知道[Google 趋势](http://trends.google.com)，你可以跟踪一些搜索词随时间的发展。这种趋势分析按日计算频率，并用线状图可视化。我们想要跟踪我们的
    UN 辩论数据集中某些关键词随着年份的变化情况，以了解诸如气候变化、恐怖主义或移民等主题的重要性增长或减少的情况。
- en: Creating Frequency Timelines
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建频率时间线
- en: 'Our approach is to calculate the frequencies of given keywords per document
    and then aggregate those frequencies using Pandas’s `groupby` function. The following
    function is for the first task. It extracts the counts of given keywords from
    a list of tokens:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的方法是计算每个文档中给定关键词的频率，然后使用 Pandas 的 `groupby` 函数汇总这些频率。以下函数是第一个任务的。它从标记列表中提取给定关键词的计数：
- en: '[PRE50]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: 'Let’s demonstrate the functionality with a small example:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个小例子来演示功能：
- en: '[PRE51]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '`Out:`'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '[PRE52]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: As you can see, the function returns a list or vector of word counts. In fact,
    it’s a very simple count-vectorizer for keywords. If we apply this function to
    each document in our `DataFrame`, we get a matrix of counts. The blueprint function
    `count_keywords_by`, shown next, does exactly this as a first step. The matrix
    is then again converted into a `DataFrame` that is finally aggregated and sorted
    by the supplied grouping column.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，该函数返回一个单词计数的列表或向量。事实上，它是一个非常简单的关键词计数向量化器。如果我们将此函数应用于我们`DataFrame`中的每个文档，我们将得到一个计数的矩阵。接下来显示的蓝图函数`count_keywords_by`正是这样的第一步。然后，该矩阵再次转换为一个`DataFrame`，最终按提供的分组列进行聚合和排序。
- en: '[PRE53]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: 'This function is very fast because it has to take care of the keywords only.
    Counting the four keywords from earlier in the UN corpus takes just two seconds
    on a laptop. Let’s take a look at the result:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数非常快，因为它只需处理关键词。在笔记本电脑上，对早期的四个关键词进行统计只需两秒钟。让我们来看看结果：
- en: '[PRE54]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '`Out:`'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`输出：`'
- en: '| nuclear | terrorism | climate | freedom | year |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| nuclear | terrorism | climate | freedom | year |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1970 | 192 | 7 | 18 | 128 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| 1970 | 192 | 7 | 18 | 128 |'
- en: '| 1971 | 275 | 9 | 35 | 205 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 1971 | 275 | 9 | 35 | 205 |'
- en: '| ... | ... | ... | ... | ... |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... |'
- en: '| 2014 | 144 | 404 | 654 | 129 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 2014 | 144 | 404 | 654 | 129 |'
- en: '| 2015 | 246 | 378 | 662 | 148 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 2015 | 246 | 378 | 662 | 148 |'
- en: Note
  id: totrans-272
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Even though we use only the attribute `year` as a grouping criterion in our
    examples, the blueprint function allows you to compare word frequencies across
    any discrete attribute, e.g., country, category, author—you name it. In fact,
    you could even specify a list of grouping attributes to compute, for example,
    counts per country and year.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 即使在我们的例子中只使用了属性`year`作为分组标准，但蓝图函数允许您跨任何离散属性比较单词频率，例如国家、类别、作者等。事实上，您甚至可以指定一个分组属性列表，以计算例如按国家和年份计数。
- en: 'The resulting `DataFrame` is already perfectly prepared for plotting as we
    have one data series per keyword. Using Pandas’s `plot` function, we get a nice
    line chart similar to Google Trends ([Figure 1-8](#fig-timeline_2)):'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的`DataFrame`已经完全准备好用于绘图，因为每个关键词都有一个数据系列。使用Pandas的`plot`函数，我们得到了一个类似于Google趋势的漂亮折线图（参见[图1-8](#fig-timeline_2)）：
- en: '[PRE55]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '![](Images/btap_0108.jpg)'
  id: totrans-276
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0108.jpg)'
- en: Figure 1-8\. Frequencies of selected words per year.
  id: totrans-277
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-8。每年选定词汇的频率。
- en: 'Note the peak of *nuclear* in the 1980s indicating the arms race and the high
    peak of terrorism in 2001\. It is somehow remarkable that the topic *climate*
    already got some attention in the 1970s and 1980s. Has it really? Well, if you
    check with a KWIC analysis ([“Blueprint: Finding a Keyword-in-Context”](#ch1-kwic)),
    you’d find out that the word *climate* in those decades was almost exclusively
    used in a figurative sense.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 注意1980年代“核”词的高峰，表明了军备竞赛，以及2001年恐怖主义的高峰。引人注目的是，“气候”主题在1970年代和1980年代已经引起了一些关注。真的吗？好吧，如果你用KWIC分析（“蓝图：寻找上下文关键词”）检查一下，你会发现在那些年代，“气候”一词几乎完全是以比喻意义使用的。
- en: Creating Frequency Heatmaps
  id: totrans-279
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 创建频率热图
- en: Say we want to analyze the historic developments of global crises like the cold
    war, terrorism, and climate change. We could pick a selection of significant words
    and visualize their timelines by line charts as in the previous example. But line
    charts become confusing if you have more than four or five lines. An alternative
    visualization without that limitation is a heatmap, as provided by the Seaborn
    library. So, let’s add a few more keywords to our filter and display the result
    as a heatmap ([Figure 1-9](#fig-heatmap)).
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想分析全球危机的历史发展，比如冷战、恐怖主义和气候变化。我们可以选择一些显著词汇，并像前面的例子中那样通过线图来可视化它们的时间线。但是，如果线图超过四五条线，它们会变得令人困惑。一个没有这种限制的替代可视化方法是热图，如Seaborn库所提供的。因此，让我们为我们的过滤器添加更多关键词，并将结果显示为热图（参见[图1-9](#fig-heatmap)）。
- en: '[PRE56]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '![](Images/btap_0109.jpg)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/btap_0109.jpg)'
- en: Figure 1-9\. Word frequencies over time as heatmap.
  id: totrans-283
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-9。随时间变化的词频热图。
- en: 'There are a few things to consider for this kind of analysis:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这种分析，有几点需要考虑：
- en: Prefer relative frequencies for any kind of comparison.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 任何类型的比较都应优先使用相对频率。
- en: Absolute term frequencies are problematic if the total number of tokens per
    year or category is not stable. For example, absolute frequencies naturally go
    up if more countries are speaking year after year in our example.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果每年或每个类别的令牌总数不稳定，绝对术语频率可能存在问题。例如，在我们的例子中，如果越来越多的国家每年都在发言，绝对频率自然会上升。
- en: Be careful with the interpretation of frequency diagrams based on keyword lists.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 谨慎解释基于关键词列表的频率图表。
- en: Although the chart looks like a distribution of topics, it is not! There may
    be other words representing the same topic but not included in the list. Keywords
    may also have different meanings (e.g., “climate of the discussion”). Advanced
    techniques such as topic modeling ([Chapter 8](ch08.xhtml#ch-topicmodels)) and
    word embeddings ([Chapter 10](ch10.xhtml#ch-embeddings)) can help here.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然图表看起来像是主题的分布，但事实并非如此！可能还有其他代表相同主题的词语，但未包含在列表中。关键词也可能有不同的含义（例如，“讨论的气候”）。高级技术如主题建模（[第8章](ch08.xhtml#ch-topicmodels)）和词嵌入（[第10章](ch10.xhtml#ch-embeddings)）在这里可以提供帮助。
- en: Use sublinear scaling.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 使用亚线性缩放。
- en: As the frequency values differ greatly, it may be hard to see any change for
    less-frequent tokens. Therefore, you should scale the frequencies sublinearly
    (we applied the square root `np.sqrt`). The visual effect is similar to lowering
    contrast.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 由于频率值差异很大，对于频率较低的令牌可能很难看到任何变化。因此，你应该对频率进行亚线性缩放（我们应用了平方根 `np.sqrt`）。视觉效果类似于降低对比度。
- en: Closing Remarks
  id: totrans-291
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结语
- en: We demonstrated how to get started analyzing textual data. The process for text
    preparation and tokenization was kept simple to get quick results. In [Chapter 4](ch04.xhtml#ch-preparation),
    we will introduce more sophisticated methods and discuss the advantages and disadvantages
    of different approaches.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们展示了如何开始分析文本数据。文本准备和标记化的过程被保持简单以获得快速结果。在[第4章](ch04.xhtml#ch-preparation)中，我们将介绍更复杂的方法，并讨论不同方法的优缺点。
- en: Data exploration should not only provide initial insights but actually help
    to develop confidence in your data. One thing you should keep in mind is that
    you should always identify the root cause for any strange tokens popping up. The
    KWIC analysis is a good tool to search for such tokens.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索不仅应该提供初步的见解，而且实际上应该帮助您对数据产生信心。你应该记住的一件事是，你应该总是确定任何奇怪令牌出现的根本原因。KWIC分析是搜索这类令牌的一个好工具。
- en: For a first analysis of the content, we introduced several blueprints for word
    frequency analysis. The weighting of terms is based either on term frequency alone
    or on the combination of term frequency and inverse document frequency (TF-IDF).
    These concepts will be picked up later in [Chapter 5](ch05.xhtml#ch-vectorization)
    because TF-IDF weighting is a standard method to vectorize documents for machine
    learning.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 对于内容的初步分析，我们介绍了几种词频分析的蓝图。术语的加权基于术语频率或术语频率和逆文档频率（TF-IDF）的组合。这些概念稍后将在[第5章](ch05.xhtml#ch-vectorization)中继续讨论，因为TF-IDF加权是机器学习中标准的文档向量化方法之一。
- en: 'There are many aspects of textual analysis that we did not cover in this chapter:'
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 文本分析有很多方面在本章中我们没有涉及：
- en: Author-related information can help to identify influential writers, if that
    is one of your project goals. Authors can be distinguished by activity, social
    scores, writing style, etc.
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 作者相关的信息可以帮助识别有影响力的作家，如果这是你的项目目标之一的话。作者可以通过活动、社交分数、写作风格等来区分。
- en: Sometimes it is interesting to compare authors or different corpora on the same
    topic by their readability. The [`textacy` library](https://oreil.ly/FRZJb) has
    a function called `textstats` that computes different readability scores and other
    statistics in a single pass over the text.
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有时候比较不同作者或不同语料库在相同主题上的可读性是很有趣的。[`textacy` 库](https://oreil.ly/FRZJb)有一个名为 `textstats`
    的函数，可以在一次遍历文本中计算不同的可读性分数和其他统计数据。
- en: An interesting tool to identify and visualize distinguishing terms between categories
    (e.g., political parties) is Jason Kessler’s [`Scattertext`](https://oreil.ly/R6Aw8)
    library.
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个有趣的工具，用于识别和可视化不同类别之间的特征术语（例如政党）是Jason Kessler的[`Scattertext`](https://oreil.ly/R6Aw8)库。
- en: Besides plain Python, you can also use interactive visual tools for data analysis.
    Microsoft’s PowerBI has a nice word cloud add-on and lots of other options to
    produce interactive charts. We mention it because it is free to use in the desktop
    version and supports Python and R for data preparation and visualization.
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了纯Python之外，你还可以使用交互式的视觉工具进行数据分析。Microsoft的PowerBI有一个不错的词云插件和许多其他选项来生成交互式图表。我们提到它是因为在桌面版中免费使用，并支持Python和R用于数据准备和可视化。
- en: For larger projects, we recommend setting up a search engine like [Apache SOLR](https://oreil.ly/LqPvG),
    [Elasticsearch](https://elastic.co), or [Tantivy](https://oreil.ly/NCz1g). Those
    platforms create specialized indexes (also using TF-IDF weighting) for fast full-text
    search. Python APIs are available for all of them.
  id: totrans-300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于较大的项目，我们建议设置搜索引擎，如[Apache SOLR](https://oreil.ly/LqPvG)，[Elasticsearch](https://elastic.co)，或[Tantivy](https://oreil.ly/NCz1g)。这些平台创建了专门的索引（还使用TF-IDF加权），以便进行快速全文搜索。Python
    API适用于所有这些平台。
- en: ^([1](ch01.xhtml#idm45634218510984-marker)) See the [Pandas documentation](https://oreil.ly/XjAKa)
    for a complete list.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch01.xhtml#idm45634218510984-marker)) 查看[Pandas文档](https://oreil.ly/XjAKa)获取完整列表。
- en: ^([2](ch01.xhtml#idm45634218281240-marker)) You can address spaCy’s list similarly
    with `spacy.lang.en.STOP_WORDS`.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch01.xhtml#idm45634218281240-marker)) 您可以类似地处理spaCy的列表，使用`spacy.lang.en.STOP_WORDS`。
- en: ^([3](ch01.xhtml#idm45634217921624-marker)) Check out the [documentation](https://oreil.ly/gO_VN)
    for further details.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch01.xhtml#idm45634217921624-marker)) 查看[文档](https://oreil.ly/gO_VN)获取更多细节。
- en: ^([4](ch01.xhtml#idm45634217882104-marker)) The NLTK class [`FreqDist`](https://oreil.ly/xQXUu)
    is derived from `Counter` and adds some convenience functions.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch01.xhtml#idm45634217882104-marker)) NLTK类[`FreqDist`](https://oreil.ly/xQXUu)派生自`Counter`，并添加了一些便利功能。
- en: ^([5](ch01.xhtml#idm45634217138552-marker)) Note that the `wordcloud` module
    ignores the stop word list if `generate_from_frequencies` is called. Therefore,
    we apply an extra filter.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: ^([5](ch01.xhtml#idm45634217138552-marker)) 注意，如果调用`generate_from_frequencies`，`wordcloud`模块会忽略停用词列表。因此，我们需要额外进行过滤。
- en: ^([6](ch01.xhtml#idm45634216924552-marker)) For example, scikit-learn’s `TfIdfVectorizer`
    adds `+1`.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ^([6](ch01.xhtml#idm45634216924552-marker)) 例如，scikit-learn的`TfIdfVectorizer`会添加`+1`。
- en: ^([7](ch01.xhtml#idm45634216916280-marker)) Another option is to add +1 in the
    denominator to prevent a division by zero for unseen terms with *df*(*t*) = 0.
    This technique is called *smoothing*.
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ^([7](ch01.xhtml#idm45634216916280-marker)) 另一种选择是在分母中添加+1，以避免未见术语导致的除零。这种技术称为*平滑*。
- en: ^([8](ch01.xhtml#idm45634216293848-marker)) The parameter `print_only` in textacy’s
    `KWIC` function works similarly but does not sample.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ^([8](ch01.xhtml#idm45634216293848-marker)) textacy的`KWIC`函数中的参数`print_only`类似工作，但不进行抽样。
- en: ^([9](ch01.xhtml#idm45634216272552-marker)) See Scott Triglia’s [blog post](https://oreil.ly/7WwTe)
    for an explanation.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ^([9](ch01.xhtml#idm45634216272552-marker)) 查看斯科特·特里格利亚的[博文](https://oreil.ly/7WwTe)了解解释。
