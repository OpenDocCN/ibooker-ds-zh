- en: Part 3
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三部分
- en: Extending and deploying Dask
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 扩展和部署 Dask
- en: 'In part 3, we round out our exploration of Dask by covering some advanced topics:
    unstructured data, machine learning, and deploying Dask to the cloud. These are
    good topics to end on, because you should be fairly comfortable with the Dask
    paradigm by now. Once again, all the chapters are anchored on real-world datasets
    and common tasks you may encounter in any data science project.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三部分，我们通过涵盖一些高级主题来完善对 Dask 的探索：非结构化数据、机器学习和将 Dask 部署到云。这些是很好的结束话题，因为现在你应该对
    Dask 模式相当熟悉了。再次强调，所有章节都基于现实世界的数据集和你在任何数据科学项目中可能遇到的标准任务。
- en: Chapter 9 discusses how to use Dask Bags—a parallelized implementation of standard
    Python Lists—and Dask Arrays—a parallelized implementation of NumPy Arrays—to
    work with more complicated, unstructured datasets. We’ll cover some advanced collections
    topics such as mapping, folding, and reducing by parsing text data stored in JSON
    format.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 第9章讨论了如何使用 Dask Bags——标准 Python Lists 的并行化实现——和 Dask Arrays——NumPy Arrays 的并行化实现——来处理更复杂、非结构化的数据集。我们将涵盖一些高级集合主题，如通过解析存储在
    JSON 格式的文本数据来映射、折叠和归约。
- en: Chapter 10 demonstrates how to use the Dask ML API to build parallelized scikit-learn
    models. This is extremely useful for building models from huge datasets where
    training time may be prohibitive and scaling the work out to many different machines
    effectively speeds up the training process.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 第10章演示了如何使用 Dask ML API 来构建并行化的 scikit-learn 模型。这对于从大型数据集中构建模型非常有用，其中训练时间可能很长，将工作扩展到多台不同的机器上可以有效地加快训练过程。
- en: 'Last but not least, chapter 11 covers two things: how to run Dask in the cloud
    using Docker and AWS, and how to run Dask in cluster mode. The chapter walks through
    a step-by-step configuration of an AWS environment, and then demonstrates how
    easy it is to execute and monitor code introduced in previous chapters in the
    cluster.'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 最后但同样重要的是，第11章涵盖了两个主题：如何使用 Docker 和 AWS 在云中运行 Dask，以及如何在集群模式下运行 Dask。本章逐步介绍了
    AWS 环境的配置，然后演示了在集群中执行和监控前几章中引入的代码是多么简单。
- en: '9'
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '9'
- en: Working with Bags and Arrays
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Bags 和 Arrays 进行操作
- en: '**This chapter covers**'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章内容涵盖**'
- en: Reading, transforming, and analyzing unstructured data using Bags
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Bags 读取、转换和分析非结构化数据
- en: Creating Arrays and DataFrames from Bags
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Bags 创建 Arrays 和 DataFrames
- en: Extracting and filtering data from Bags
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Bags 中提取和过滤数据
- en: Combining and grouping elements of Bags using fold and reduce functions
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 fold 和 reduce 函数组合和分组 Bags 的元素
- en: Using NLTK (Natural Language Toolkit) with Bags for text mining on large text
    datasets
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 NLTK（自然语言工具包）与 Bags 结合进行大型文本数据集的文本挖掘
- en: 'The majority of this book focuses on using DataFrames for analyzing structured
    data, but our exploration of Dask would not be complete without mentioning the
    two other high-level Dask APIs: Bags and Arrays. When your data doesn’t fit neatly
    in a tabular model, Bags and Arrays offer additional flexibility. DataFrames are
    limited to only two dimensions (rows and columns), but Arrays can have many more.
    The Array API also offers additional functionality for certain linear algebra,
    advanced mathematics, and statistics operations. However, much of what’s been
    covered already through working with DataFrames also applies to working with Arrays—just
    as Pandas and NumPy have many similarities. In fact, you might recall from chapter
    1 that Dask DataFrames are parallelized Pandas DataFrames and Dask Arrays are
    parallelized NumPy arrays.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书的大部分内容集中在使用 DataFrames 分析结构化数据上，但如果不提及 Dask 的另外两个高级 API：Bags 和 Arrays，我们的探索就不会完整。当你的数据不适合整齐地放入表格模型中时，Bags
    和 Arrays 提供了额外的灵活性。DataFrames 仅限于两个维度（行和列），但 Arrays 可以有更多维度。Array API 还为某些线性代数、高级数学和统计操作提供了额外的功能。然而，通过使用
    DataFrames 已经涵盖的大部分内容也适用于使用 Arrays——正如 Pandas 和 NumPy 有很多相似之处。实际上，你可能还记得第一章中提到的
    Dask DataFrames 是并行化的 Pandas DataFrames，而 Dask Arrays 是并行化的 NumPy 数组。
- en: Bags, on the other hand, are unlike any of the other Dask data structures. Bags
    are very powerful and flexible because they are parallelized general collections,
    most like Python’s built-in List object. Unlike Arrays and DataFrames, which have
    predetermined shapes and datatypes, Bags can hold any Python objects, whether
    they are custom classes or built-in types. This makes it possible to contain very
    complicated data structures, like raw text or nested JSON data, and navigate them
    with ease.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他Dask数据结构不同，Bags非常强大且灵活，因为它们是并行化的通用集合，最像Python内置的List对象。与具有预定形状和数据类型的Arrays和DataFrames不同，Bags可以持有任何Python对象，无论是自定义类还是内置类型。这使得包含非常复杂的数据结构，如原始文本或嵌套JSON数据，并且可以轻松地导航它们。
- en: Working with unstructured data is becoming more commonplace for data scientists,
    especially data scientists who are working independently or in small teams without
    a dedicated data engineer. Take the following, for example.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 与非结构化数据一起工作正在成为数据科学家越来越常见的做法，尤其是那些独立工作或在小型团队中没有专门数据工程师的数据科学家。以下是一个例子。
- en: '![c09_01.eps](Images/c09_01.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![c09_01.eps](Images/c09_01.png)'
- en: '[Figure 9.1](#figureanchor9.1) An example comparison of structured and unstructured
    data'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.1](#figureanchor9.1) 结构化和非结构化数据的一个比较示例'
- en: 'In [figure 9.1](#figure9.1), the same data is presented two different ways:
    the upper half shows some examples of product reviews as structured data with
    rows and columns, and the lower half shows the raw review text as unstructured
    data. If the only information we care about is the customer’s name, the product
    they purchased, and whether or not they were satisfied, the structured data gives
    us this information at a glance without any ambiguity. Every value in the Customer
    Name column is always the customer’s name. Conversely, the varying length, writing
    style, and free-form nature of the raw text makes it unclear what data is relevant
    for analysis and requires some sort of parsing and interpretation to extract the
    relevant data. In the first review, the reviewer’s name (Mary) is the fourth word
    of the review. However, the second reviewer put his name (Bob) at the very end
    of his review. These inconsistencies make it difficult to use a rigid data structure
    like a DataFrame or an Array to organize the information. Instead, the flexibility
    of Bags really shines here: whereas a DataFrame or Array always has a fixed number
    of columns, a Bag can contain strings, lists, or any other element of varying
    length.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图9.1](#figure9.1)中，数据以两种不同的方式呈现：上半部分展示了以行和列形式组织的结构化数据的产品评论示例，下半部分展示了原始的评论文本作为非结构化数据。如果我们只关心客户的名字、他们购买的产品以及他们是否满意，结构化数据可以让我们一目了然地获得这些信息，没有任何歧义。客户名字列中的每个值始终是客户的名字。相反，原始文本的长度、写作风格和自由形式特性使得难以确定哪些数据与分析相关，需要某种形式的解析和解释来提取相关数据。在第一篇评论中，评论者的名字（玛丽）是评论的第四个单词。然而，第二位评论者将他的名字（鲍勃）放在了他的评论的末尾。这些不一致性使得使用像DataFrame或Array这样的刚性数据结构来组织信息变得困难。相反，Bags的灵活性在这里得到了很好的体现：与DataFrame或Array总是具有固定数量的列不同，Bag可以包含字符串、列表或任何其他可变长度的元素。
- en: In fact, the typical use case that involves working with unstructured data comes
    from analyzing text data scraped from web APIs, such as product reviews, tweets
    from Twitter, or ratings from services like Yelp and Google Reviews. Therefore,
    we’ll walk through an example of using Bags to parse and prepare unstructured
    text data; then we’ll look at how to map and derive structured data from Bags
    to Arrays.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，涉及处理非结构化数据的典型用例来自分析从网络API抓取的文本数据，例如产品评论、Twitter上的推文或Yelp和Google评论等服务提供的评分。因此，我们将通过一个使用Bags解析和准备非结构化文本数据的示例；然后我们将探讨如何从Bags映射和推导出结构化数据到Arrays。
- en: '![c09_02.eps](Images/c09_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![c09_02.eps](Images/c09_02.png)'
- en: '[Figure 9.2](#figureanchor9.2) The *Data Science with Python and Dask* workflow'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.2](#figureanchor9.2) 使用Python和Dask进行数据科学的流程'
- en: '[Figure 9.2](#figure9.2) is our familiar workflow diagram, but it might be
    a bit surprising because we’ve stepped back to the first three tasks! Since we’re
    starting with a new problem and dataset, rather than progressing onward from chapter
    8, we’ll be revisiting the first three elements of our workflow, this time with
    the focus on unstructured data. Many of the concepts that were covered in chapters
    4 and 5 are the same, but we will look at how to technically achieve the same
    results when the data doesn’t come in a tabular format such as CSV.'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.2](#figure9.2) 是我们熟悉的流程图，但它可能有点令人惊讶，因为我们已经退回到前三个任务！由于我们是从一个新的问题和数据集开始，而不是从第8章继续前进，我们将重新审视工作流程的前三个要素，这次的重点是非结构化数据。在第4章和第5章中介绍的概念很多是相同的，但我们将探讨当数据不是以CSV等表格格式出现时，如何技术上实现相同的结果。'
- en: 'As a motivating example for this chapter, we’ll look at a set of product reviews
    from Amazon.com sourced by Stanford University’s Network Analysis Project. You
    can download the data from here: [https://goo.gl/yDQgfH](https://goo.gl/yDQgfH).
    To learn more about how the dataset was created, see McAuley and Leskovec’s paper
    “From Amateurs to Connoisseurs: Modeling the Evolution of User Expertise through
    Online Reviews” (Stanford, 2013).'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 作为本章的激励示例，我们将查看由斯坦福大学网络分析项目收集的来自Amazon.com的一组产品评论。您可以从这里下载数据：[https://goo.gl/yDQgfH](https://goo.gl/yDQgfH)。要了解更多关于数据集是如何创建的信息，请参阅McAuley和Leskovec的论文“从业余爱好者到鉴赏家：通过在线评论建模用户专业知识的演变”（斯坦福，2013年）。
- en: 9.1 Reading and parsing unstructured data with Bags
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 使用Bags读取和解析非结构化数据
- en: After you’ve downloaded the data, the first thing you need to do is properly
    read and parse the data so you can easily manipulate it. The first scenario we’ll
    walk through is
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在您下载数据后，您需要做的第一件事是正确读取和解析数据，这样您就可以轻松地操作它。我们将首先介绍的场景是
- en: Using the Amazon Fine Foods Revie ws dataset, determine its format and parse
    the data into a Bag of dictionaries.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Amazon Fine Foods Reviews数据集，确定其格式并将数据解析为字典的Bag。
- en: This particular dataset is a plain text file. You can open it with any text
    editor and start to make sense of the layout of the file. The Bag API offers a
    few convenience methods for reading text files. In addition to plain text, the
    Bag API is also equipped to read files in the Apache Avro format, which is a popular
    binary format for JSON data and is usually denoted by a file ending in .avro.
    The function used to read plain text files is `read_text`, and has only a few
    parameters. In its simplest form, all it needs is a filename. If you want to read
    multiple files into one Bag, you can alternatively pass a list of filenames or
    a string with a wildcard (such as *.txt). In this instance, all the files in the
    list of filenames should have the same kind of information; for example, log data
    collected over time where one file represents a single day of logged events. The
    `read_text` function also natively supports most compression schemes (such as
    GZip and BZip), so you can leave the data compressed on disk. Leaving your data
    compressed can, in some cases, offer significant performance gains by reducing
    the load on your machine’s input/output subsystem, so it’s generally a good idea.
    Let’s take a look at what the `read_text` function will produce.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这个特定的数据集是一个纯文本文件。您可以用任何文本编辑器打开它，并开始理解文件的布局。Bag API提供了一些方便的方法来读取文本文件。除了纯文本，Bag
    API还配备了读取Apache Avro格式文件的能力，这是一种流行的JSON数据的二进制格式，通常以文件扩展名.avro表示。用于读取纯文本文件的函数是`read_text`，它只有几个参数。在其最简单形式中，它只需要一个文件名。如果您想将多个文件读入一个Bag中，您可以传递一个文件名列表或一个包含通配符（如*.txt）的字符串。在这种情况下，文件名列表中的所有文件应该具有相同类型的信息；例如，随着时间的推移收集的日志数据，其中每个文件代表一天的事件记录。`read_text`函数还原生支持大多数压缩方案（如GZip和BZip），因此您可以将数据压缩在磁盘上。在某些情况下，将数据压缩在磁盘上可以通过减少机器的输入/输出子系统负载来提供显著的性能提升，所以这通常是一个好主意。让我们看看`read_text`函数将产生什么。
- en: Listing 9.1 Reading text data into a Bag
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 将文本数据读入Bag
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: As you’ve probably come to expect by now, the `read_text` operation produces
    a lazy object that won’t get evaluated until we actually perform a compute-type
    operation on it. The Bag’s metadata indicates that it will read the entire data
    in as one partition. Since the size of this file is rather small, that’s probably
    OK. However, if we wanted to manually increase parallelism, `read_text` also takes
    an optional `blocksize` parameter that allows you to specify how large each partition
    should be in bytes. For instance, to split the roughly 400 MB file into four partitions,
    we could specify a blocksize of 100,000,000 bytes, which equates to 100 MB. This
    will cause Dask to divide the file into four partitions.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所期望的，`read_text`操作会产生一个惰性对象，它只有在实际上对其执行计算类型操作时才会被评估。Bag的元数据表明它将整个数据读作一个分区。由于这个文件的大小相当小，这可能没问题。然而，如果我们想手动增加并行性，`read_text`还接受一个可选的`blocksize`参数，允许你指定每个分区的大小（以字节为单位）。例如，要将大约400
    MB的文件分成四个分区，我们可以指定一个100,000,000字节的blocksize，这相当于100 MB。这将导致Dask将文件分成四个分区。
- en: 9.1.1 Selecting and viewing data from a Bag
  id: totrans-32
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 从Bag中选择和查看数据
- en: Now that we’ve created a Bag from the data, let’s see how the data looks. The
    `take` method allows us to look at a small subset of the items in the Bag, just
    as the `head` method allows us to do the same with DataFrames. Simply specify
    the number of items you want to view, and Dask prints the result.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从数据中创建了一个Bag，让我们看看数据看起来是什么样子。`take`方法允许我们查看Bag中的小部分项目，就像`head`方法允许我们对DataFrames做同样的操作。只需指定你想要查看的项目数量，Dask就会打印出结果。
- en: Listing 9.2 Viewing items in a Bag
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 查看Bag中的项目
- en: '[PRE1]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: As you can see from the result of [listing 9.2](#listing9.2), each element of
    the Bag currently represents a line in the file. However, this structure will
    prove to be problematic for our analysis. There is an obvious relationship between
    some of the elements. For example, the `review/score` element being displayed
    is the review score for the product ID preceding it (`B001E4KFG0`). But since
    there is nothing structurally relating these elements, it would be difficult to
    do something like calculate the mean review score for item `B001E4KFG0`. Therefore,
    we should add a bit of structure to this data by grouping the elements that are
    associated together into a single object.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 如从[列表9.2](#listing9.2)的结果中可以看出，Bag中的每个元素当前代表文件中的一行。然而，这种结构将证明对我们的分析是有问题的。一些元素之间存在明显的关联。例如，显示的`review/score`元素是前一个产品ID（`B001E4KFG0`）的评论评分。但由于这些元素在结构上没有关联，很难进行像计算项目`B001E4KFG0`的平均评论评分这样的操作。因此，我们应该通过将关联的元素组合成一个单一对象来给这些数据添加一些结构。
- en: 9.1.2 Common parsing issues and how to overcome them
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 常见解析问题和解决方法
- en: A common issue that arises when working with text data is making sure the data
    is being read using the same character encoding as it was written in. Character
    encoding is used to map raw data stored as binary into symbols that we humans
    can identify as letters. For example, the capital letter `J` is represented in
    binary as `01001010` using the UTF-8 encoding. If you open a text file in a text
    editor using UTF-8 to decode the file, everywhere `01001010` is encountered in
    the file, it will be translated to a `J` before it is displayed on the screen.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理文本数据时，一个常见的问题是要确保数据是以与写入时相同的字符编码进行读取的。字符编码用于将存储为二进制的原始数据映射成我们人类可以识别的符号。例如，大写字母`J`使用UTF-8编码表示为`01001010`。如果你使用UTF-8编码在文本编辑器中打开一个文本文件进行解码，文件中遇到的每个`01001010`都会在显示到屏幕上之前被翻译成`J`。
- en: Using the correct character encoding makes sure the data will be read correctly
    and you won’t see any garbled text. By default, the `read_text` function assumes
    that the data is encoded using UTF-8\. Since Bags are inherently lazy, the validity
    of this assumption isn’t checked ahead of time, meaning you’ll be tipped off that
    there’s a problem only when you perform a function over the entire dataset. For
    example, if we want to count the number of items in the Bag, we could use the
    `count` function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 使用正确的字符编码确保数据将被正确读取，你不会看到任何乱码文本。默认情况下，`read_text`函数假定数据使用UTF-8编码。由于Bag本质上是惰性的，这个假设的验证不是提前进行的，这意味着只有在你对整个数据集执行函数时才会发现问题。例如，如果我们想计算Bag中的项目数量，我们可以使用`count`函数。
- en: Listing 9.3 Exposing an encoding error while counting the items in the Bag
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 在计数Bag中的项目时暴露编码错误
- en: '[PRE2]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The `count` function, which looks exactly like the `count` function from the
    DataFrame API, fails with a UnicodeDecodeError exception. This tells us that the
    file is probably not encoded in UTF-8 since it can’t be parsed. These issues typically
    arise if the text uses any kind of characters that aren’t used in the English
    alphabet (such as accent marks, Hanzi, Hiragana, and Abjads). If you are able
    to ask the producer of the file which encoding was used, you can simply add the
    encoding to the `read_text` function using the `encoding` parameter. If you’re
    not able to find out the encoding that the file was saved in, a bit of trial and
    error is necessary to determine which encoding to use. A good place to start is
    trying the `cp1252` encoding, which is the standard encoding used by Windows.
    In fact, this example dataset was encoded using `cp1252`, so we can modify the
    `read_text` function to use `cp1252` and try our `count` operation again.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 与 DataFrame API 中的 `count` 函数看起来完全相同的 `count` 函数会引发 UnicodeDecodeError 异常。这告诉我们，文件可能不是以
    UTF-8 编码的，因为它无法被解析。这些问题通常会在文本使用任何在英语字母表中未使用的字符（如重音符号、汉字、平假名和阿拉伯字母）时出现。如果你能够询问文件的制作者使用了哪种编码，你可以简单地使用
    `encoding` 参数将编码添加到 `read_text` 函数中。如果你无法找出文件保存的编码，就需要进行一些试错来确定使用哪种编码。一个好的起点是尝试
    `cp1252` 编码，这是 Windows 使用的标准编码。实际上，这个示例数据集就是使用 `cp1252` 编码的，因此我们可以修改 `read_text`
    函数以使用 `cp1252` 并再次尝试我们的 `count` 操作。
- en: Listing 9.4 Changing the encoding of the `read_text` function
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.4 修改 `read_text` 函数的编码
- en: '[PRE3]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This time, the file is able to be parsed completely and we’re shown that the
    file contains 5.11 million lines.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，文件能够被完全解析，并且我们得知文件包含 5.11 百万行。
- en: 9.1.3 Working with delimiters
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 使用分隔符进行工作
- en: With the encoding problem solved, let’s look at how we can add the structure
    we need to group the attributes of each review together. Since the file we’re
    working with is just one long string of text data, we can look for patterns in
    the text that might be useful for dividing the text into logical chunks. [Figure
    9.3](#figure9.3) shows a few hints as to where some useful patterns in the text
    are.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 解决了编码问题后，让我们看看我们如何添加所需的结构来将每条评论的属性分组在一起。由于我们正在处理的文件只是一个长字符串的文本数据，我们可以寻找文本中的模式，这些模式可能有助于将文本划分为逻辑块。图
    9.3 [#figure9.3](#figure9.3) 展示了一些关于文本中一些有用模式的提示。
- en: '![c09_03.eps](Images/c09_03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![c09_03.eps](Images/c09_03.png)'
- en: '[Figure 9.3](#figureanchor9.3) A pattern enables us to split the text into
    individual reviews.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.3](#figureanchor9.3) 一个模式使我们能够将文本分割成单个评论。'
- en: In this particular example, the author of the dataset has put two newline characters
    (which show up as `\n`) between each review. We can use this pattern as a delimiter
    to split the text into chunks, where each chunk of text contains all attributes
    of the review, such as the product ID, the rating, the review text, and so on.
    We will need to manually parse the text file using some functions from the Python
    standard library. What we want to avoid doing, however, is reading the entire
    file into memory in order to do this. Although this file could comfortably fit
    into memory, an example that reads the entire file into memory would not work
    once you start working with datasets that exceed the limits of your machine (and
    it would defeat the whole purpose of parallelism to boot!). Therefore, we will
    use Python’s file iterator to stream the file a small chunk at a time, search
    the text in the buffer for our desired delimiter, mark the position in the file
    where a review starts and ends, and then advance the buffer to find the position
    of the next review. We’ll end up with a list of Delayed objects that have pointers
    to the start and end of each review, which can further be parsed into a dictionary
    of key-value pairs. The full process from start to finish is outlined in the flowchart
    in [figure 9.4](#figure9.4).
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的例子中，数据集的作者在每个审查之间放置了两个换行符（显示为`\n`）。我们可以使用这个模式作为分隔符来分割文本，其中每个文本块包含审查的所有属性，例如产品ID、评分、审查文本等。我们将需要手动使用Python标准库中的某些函数来解析文本文件。然而，我们想要避免的是为了完成这项工作而将整个文件读入内存。尽管这个文件可以轻松地放入内存，但一旦开始处理超出你机器限制的数据集，这种读取整个文件到内存的方法将无法工作（而且这还会违背并行化的整个目的！）。因此，我们将使用Python的文件迭代器一次流式传输文件的一小部分，在缓冲区中搜索我们想要的分隔符，标记文件中审查开始和结束的位置，然后移动缓冲区以找到下一个审查的位置。最终，我们将得到一个包含指向每个审查开始和结束位置的指针的延迟对象列表，这些对象可以进一步解析为键值对字典。从开始到结束的完整过程在[图9.4](#figure9.4)中的流程图中概述。
- en: '![c09_04.eps](Images/c09_04.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![c09_04.eps](Images/c09_04.png)'
- en: '[Figure 9.4](#figureanchor9.4) Our custom text-parsing algorithm to implement
    using Dask Delayed'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.4](#figureanchor9.4) 使用Dask Delayed实现的定制文本解析算法'
- en: First, we’ll define a function that searches a part of a file for our specified
    delimiter. With Python’s file handle system, it’s possible to stream data in from
    a file starting at a specific byte number and stopping at a specific byte number.
    For instance, the beginning of the file is byte 0\. The next character is byte
    1, and so forth. Rather than load the whole file into memory, we can load chunks
    in at a time. For instance, we could load 1000 bytes of data starting at byte
    5000\. The space in memory we’re loading the 1000 bytes of data into is called
    a *buffer*. We can *decode* the buffer from raw bytes to a string object, and
    then use all the string manipulation functions available to us in Python, such
    as `find`, `strip`, `split`, and so on. And, since the buffer space is only 1000
    bytes in this example, that’s approximately all the memory we will use.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将定义一个函数，用于在文件的一部分中搜索指定的分隔符。使用Python的文件句柄系统，可以从文件的特定字节位置开始流式传输数据，并停止在特定的字节位置。例如，文件的开始是字节0。下一个字符是字节1，以此类推。我们不必将整个文件加载到内存中，可以一次加载一块。例如，我们可以从字节5000开始加载1000字节的数据。我们将1000字节的数据加载到内存中的空间称为*缓冲区*。我们可以*解码*缓冲区，从原始字节到字符串对象，然后使用Python中所有可用的字符串操作函数，例如`find`、`strip`、`split`等。而且，由于在这个例子中缓冲区空间只有1000字节，这大约就是我们将使用的所有内存。
- en: We need a function that will
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要一个函数来
- en: Accept a file handle, a starting position (such as byte 5000), and a buffer
    size.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接受一个文件句柄、起始位置（例如字节5000）和缓冲区大小。
- en: Then read the data into a buffer and search the buffer for the delimiter.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后将数据读入缓冲区并搜索缓冲区中的分隔符。
- en: If it’s found, it should return the position of the delimiter relative to the
    starting position.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果找到了，它应该返回分隔符相对于起始位置的位置。
- en: However, we also need to cope with the possibility that a review is longer than
    our buffer size, which will result in the delimiter not being found.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然而，我们还需要应对可能出现的审查内容比我们的缓冲区大小还要长的情况，这会导致无法找到分隔符。
- en: If this happens, the code should keep expanding the search space of the buffer
    by reading the next 1000 bytes again and again until the delimiter is found.
  id: totrans-59
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果发生这种情况，代码应该通过一次又一次地读取下一个1000字节来不断扩展缓冲区的搜索空间，直到找到分隔符。
- en: Here’s a function that will do that.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个将执行此操作的函数。
- en: Listing 9.5 A function to find the next occurrence of a delimiter in a file
    handle
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 查找文件句柄中分隔符下一个出现位置的函数
- en: '[PRE4]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Given a file handle and a starting position, this function will find the next
    occurrence of the delimiter. The recursive function call that occurs if the delimiter
    isn’t found in the current buffer adds the current buffer size to the `span_index`
    parameter. This is how the window continues to expand if the delimiter search
    fails. The first time the function is called, the `span_index` will be 0\. With
    a default `blocksize` of 1000, this means the function will read the next 1000
    bytes after the starting position (1000 `blocksize` + 0 `span_index`). If the
    find fails, the function is called again after incrementing the `span_index` by
    1000\. Then it will then try again by searching the next 2000 bytes after the
    starting position (1000 `blocksize` + 1000 `span_index`). If the find continues
    to fail, the search window will keep expanding by 1000 bytes until a delimiter
    is finally found or the end of the file is reached. A visual example of this process
    can be seen in [figure 9.5](#figure9.5).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个文件句柄和起始位置，此函数将找到分隔符的下一个出现位置。如果在当前缓冲区中没有找到分隔符，将发生递归函数调用。这会将当前缓冲区大小添加到`span_index`参数中。这就是如果分隔符搜索失败，窗口如何继续扩展的原因。第一次调用该函数时，`span_index`将为0。默认`blocksize`为1000，这意味着函数将读取起始位置之后的下一个1000个字节（1000
    `blocksize` + 0 `span_index`）。如果查找失败，函数将在将`span_index`增加1000之后再次调用。然后它将尝试在起始位置之后的下一个2000个字节中再次搜索（1000
    `blocksize` + 1000 `span_index`）。如果查找继续失败，搜索窗口将以1000字节为单位继续扩展，直到最终找到分隔符或达到文件末尾。这个过程的一个视觉示例可以在[图9.5](#figure9.5)中看到。
- en: '![c09_05.eps](Images/c09_05.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![c09_05.eps](Images/c09_05.png)'
- en: '[Figure 9.5](#figureanchor9.5) A visual representation of the recursive delimiter
    search function'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.5](#figureanchor9.5) 递归分隔符搜索函数的视觉表示'
- en: To find all instances of the delimiter in the file, we can call this function
    inside a loop that will iterate chunk by chunk until the end of the file is reached.
    To do this, we’ll use a `while` loop.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 要在文件中找到所有分隔符的实例，我们可以在一个循环中调用此函数，该循环将分块迭代，直到达到文件末尾。为此，我们将使用`while`循环。
- en: Listing 9.6 Finding all instances of the delimiter
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.6 查找分隔符的所有实例
- en: '[PRE5]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Essentially this code accomplishes four things:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这段代码完成了四件事情：
- en: Find the start position and bytes to delimiter for each review.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 找到每个评论的起始位置和分隔符的字节数。
- en: Save all these positions to a list.
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将所有这些位置保存到列表中。
- en: Distribute the byte positions of reviews to the workers.
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将评论的字节位置分配给工作进程。
- en: Workers process the data for the reviews at the byte positions they receive.
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作人员在他们接收的字节位置处理评论数据。
- en: 'After initializing a few variables, we enter the loop starting at byte 0\.
    Every time a delimiter is found, the current position is advanced to just after
    the position of the delimiter. For instance, if the first delimiter starts at
    byte 627, the first review is made up of bytes 0 through 626\. Bytes 0 and 626
    would be appended to the output list, and the current position would be advanced
    to 628\. We add two to the `next_position` variable because the delimiter is two
    bytes (each `‘\n’` is one byte). Therefore, since we don’t really care about keeping
    the delimiters as part of the final review objects, we’ll skip over them. The
    search for the next delimiter will pick up at byte 629, which should be the first
    character of the next review. This continues until the end of the file is reached.
    By then we have a list of tuples. The first element in each tuple represents the
    starting byte, and the second element in each tuple represents the number of bytes
    to read after the starting byte. The list of tuples looks like this:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在初始化几个变量后，我们从字节0开始进入循环。每次找到分隔符时，当前位置将向前推进到分隔符之后的位置。例如，如果第一个分隔符从字节627开始，第一个评论由字节0到626组成。字节0和626将被追加到输出列表中，当前位置将推进到628。我们将`next_position`变量增加2，因为分隔符是两个字节（每个`‘\n’`是一个字节）。因此，由于我们并不真正关心将分隔符作为最终评论对象的一部分保留，我们将跳过它们。下一次分隔符的搜索将从字节629开始，这应该是下一个评论的第一个字符。这个过程一直持续到文件末尾。到那时，我们有一个元组的列表。每个元组的第一个元素代表起始字节，第二个元素代表在起始字节之后要读取的字节数。元组列表看起来像这样：
- en: '[PRE6]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Before moving on, check the length of the `output` list using the `len` function.
    The list should contain 568,454 elements.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，使用`len`函数检查`output`列表的长度。列表应包含568,454个元素。
- en: Now that we have a list containing all the byte positions of the reviews, we
    need to create some instructions to transform the list of addresses into a list
    of actual reviews. To do that, we’ll need to create a function that takes a starting
    position and a number of bytes as input, reads the file at the specified byte
    location, and returns a parsed review object. Because there are thousands of reviews
    to be parsed, we can speed up this process using Dask. [Figure 9.6](#figure9.6)
    demonstrates how we can divide up the work across multiple workers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个包含所有审查字节位置的列表，我们需要创建一些指令将地址列表转换为实际审查的列表。为此，我们需要创建一个函数，该函数接受起始位置和字节数作为输入，读取指定字节位置的文件，并返回一个解析的审查对象。由于有数千条审查需要解析，我们可以使用
    Dask 来加速此过程。[图 9.6](#figure9.6) 展示了如何将工作分配给多个工作者。
- en: '![c09_06.eps](Images/c09_06.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![c09_06.eps](Images/c09_06.png)'
- en: '[Figure 9.6](#figureanchor9.6) Mapping the parsing code to the review data'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.6](#figureanchor9.6) 将解析代码映射到审查数据'
- en: 'Effectively, the list of addresses will be divided up among the workers; each
    worker will open the file and parse the reviews at the byte positions it receives.
    Since the reviews are stored as JSON, we will create a dictionary object for each
    review to store its attributes. Each attribute of the review looks something like
    this: `''review/userId: A3SGXH7AUHU8GW\n''`, so we can exploit the pattern of
    each key ending in '': '' to split the data into key-value pairs for the dictionaries.
    The next listing shows a function that will do that.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '实际上，地址列表将被分配给工作者；每个工作者将打开文件并在接收的字节位置解析审查。由于审查以 JSON 格式存储，我们将为每个审查创建一个字典对象来存储其属性。每个审查的属性看起来像这样：`''review/userId:
    A3SGXH7AUHU8GW\n''`，因此我们可以利用每个键以 '': '' 结尾的模式将数据分割成字典的键值对。下一个列表将展示一个执行此操作的函数。'
- en: Listing 9.7 Parsing each byte stream into a dictionary of key-value pairs
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.7 将每个字节流解析成键值对字典
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now that we have a function that will parse a specified part of the file, we
    need to actually send those instructions out to the workers so they can apply
    the parsing code to the data. We’ll now bring everything together and create a
    Bag that contains the parsed reviews.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个可以解析文件指定部分的函数，我们需要将这些指令发送给工作者，以便他们可以将解析代码应用于数据。现在我们将所有内容整合在一起，创建一个包含解析审查的
    Bag。
- en: Listing 9.8 Producing the Bag of reviews
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.8 生成审查的 Bag
- en: '[PRE8]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The code in [listing 9.8](#listing9.8) does two things: first, we turn the
    list of byte addresses into a Bag using the `from_sequence` function of the Bag
    array. This creates a Bag that holds the same list of byte addresses as our original
    list, but now allows Dask to distribute the contents of the Bag to the workers.
    Next, the `map` function is called to transform each byte address tuple into its
    respective review object. `Map` effectively hands out the Bag of byte addresses
    and the instructions contained in the `get_item` function to the workers (remember
    that when Dask is running in local mode, the workers are independent threads on
    your machine). A new Bag called `reviews` is created, and when it is computed,
    it will output the parsed reviews. In [listing 9.8](#listing9.8), we pass in the
    `get_item` function inside of a `lambda` expression so we can keep the filename
    parameter fixed while dynamically inputting the start and end byte address from
    each item in the Bag. As before, this entire process is lazy. The result of [listing
    9.8](#listing9.8) will show that a Bag has been created with 101 partitions. However,
    taking elements from the Bag will now result in a very different output!'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.8](#listing9.8) 中的代码做了两件事：首先，我们使用 Bag 数组的 `from_sequence` 函数将字节地址列表转换为
    Bag。这创建了一个包含与原始列表相同字节地址列表的 Bag，但现在允许 Dask 将 Bag 的内容分发到工作者。接下来，调用 `map` 函数将每个字节地址元组转换为相应的审查对象。`Map`
    实际上向工作者分配了字节地址的 Bag 和 `get_item` 函数中包含的指令（记住，当 Dask 以本地模式运行时，工作者是机器上的独立线程）。创建了一个新的名为
    `reviews` 的 Bag，当它被计算时，将输出解析的审查。在 [列表 9.8](#listing9.8) 中，我们通过一个 `lambda` 表达式传递
    `get_item` 函数，这样我们就可以在动态输入 Bag 中每个项目的起始和结束字节地址的同时保持文件名参数固定。与之前一样，整个过程是惰性的。[列表
    9.8](#listing9.8) 的结果将显示创建了一个包含 101 个分区的 Bag。然而，从 Bag 中提取元素现在将产生非常不同的输出！'
- en: Listing 9.9 Taking elements from the transformed Bag
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.9 从转换后的 Bag 中提取元素
- en: '[PRE9]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Each element in the transformed Bag is now a dictionary that contains all attributes
    of the review! This will make analysis a lot easier for us. Additionally, if we
    count the items in the transformed Bag, we also get a far different result.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 现在转换后的 Bag 中的每个元素都是一个包含所有评论属性的字典！这将使我们的分析变得容易得多。此外，如果我们计算转换后的 Bag 中的项目数量，我们也会得到一个截然不同的结果。
- en: Listing 9.10 Counting items in the transformed Bag
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.10 计算转换后的 Bag 中的项目数量
- en: '[PRE10]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The number of elements in the Bag has been greatly reduced because we’ve assembled
    the raw text into logical reviews. This count also matches the number of reviews
    stated by Stanford on the dataset’s webpage, so we can be sure that we’ve correctly
    parsed the data without running into any more encoding issues! Now that our data
    is a bit easier to work with, we’ll look at some other ways we can manipulate
    data using Bags.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将原始文本组装成逻辑评论，Bag 中的元素数量已经大大减少。这个计数也符合斯坦福在数据集网页上声明的评论数量，因此我们可以确信我们已经正确解析了数据，没有遇到任何更多的编码问题！现在我们的数据更容易处理了，我们将探讨一些其他使用
    Bags 操作数据的方法。
- en: 9.2 Transforming, filtering, and folding elements
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 转换、过滤和折叠元素
- en: Unlike lists and other generic collections in Python, Bags are not subscript-able,
    meaning it’s not possible to access a specific element of a Bag in a straightforward
    way. This can make data manipulation slightly more challenging until you become
    comfortable with thinking about data manipulation in terms of transformations.
    If you are familiar with functional programming or MapReduce style programming,
    this line of thinking comes naturally. However, it can seem a bit counterintuitive
    at first if you have a background of SQL, spreadsheets, and Pandas. If this is
    the case, don’t worry. With a bit of practice, you too will be able to start thinking
    of data manipulation in terms of transformations!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Python 中的列表和其他通用集合不同，Bags 不可索引，这意味着无法以直接的方式访问 Bag 中的特定元素。这可能会使数据处理稍微有些挑战，直到你习惯于从转换的角度思考数据处理。如果你熟悉函数式编程或
    MapReduce 风格的编程，这种思维方式会自然而然地出现。然而，如果你有 SQL、电子表格和 Pandas 的背景，一开始可能会觉得有点反直觉。如果情况是这样，请不要担心。经过一些练习，你也能开始从转换的角度思考数据处理！
- en: 'The next scenario we’ll use for motivation is the following:'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 我们接下来要使用的动机场景如下：
- en: Using the Amazon Fine Foods Reviews dataset, tag the review as being positive
    or negative by using the review score as a threshold.
  id: totrans-96
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用亚马逊精选食品评论数据集，通过使用评论分数作为阈值来标记评论为正面或负面。
- en: 9.2.1 Transforming elements with the map method
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.1 使用 map 方法转换元素
- en: Let’s start easy—first, we’ll simply get all the review scores for the entire
    dataset. To do this, we’ll use the `map` function again. Rather than thinking
    about what we’re trying to do as getting the review scores, think about what we’re
    trying to do as transforming our Bag of reviews to a Bag of review scores. We
    need some kind of function that will take a review (dictionary) object in as an
    input and spit out the review score. A function that does that looks like this.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从简单开始——首先，我们将简单地获取整个数据集的所有评论分数。为此，我们将再次使用 `map` 函数。与其将我们试图做的事情视为获取评论分数，不如将其视为将我们的评论
    Bag 转换为评论分数 Bag 的转换。我们需要某种函数，该函数将接受一个评论（字典）对象作为输入，并输出评论分数。一个执行此操作的函数看起来像这样。
- en: Listing 9.11 Extracting a value from a dictionary
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.11 从字典中提取值
- en: '[PRE11]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This is just plain old Python. We could pass any dictionary into this function,
    and if it contained a key of `review/score`, this function would cast the value
    to a float and return the value. If we map over our Bag of dictionaries using
    this function, it will transform each dictionary into a float containing the relevant
    review score. This is quite simple.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是普通的 Python。我们可以将任何字典传递给这个函数，如果它包含 `review/score` 键，这个函数会将值转换为浮点数并返回该值。如果我们使用这个函数映射我们的字典
    Bag，它将把每个字典转换成一个包含相关评论分数的浮点数。这很简单。
- en: Listing 9.12 Getting the review scores
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.12 获取评论分数
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The `review_scores` Bag now contains all the raw review scores. The transformations
    you create can be any valid Python function. For instance, if we wanted to tag
    the reviews as being positive or negative based on the review score, we could
    use a function like this.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: '`review_scores` Bag 现在包含所有原始评论分数。你创建的转换可以是任何有效的 Python 函数。例如，如果我们想根据评论分数将评论标记为正面或负面，我们可以使用这样的函数。'
- en: Listing 9.13 Tagging reviews as positive or negative
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.13 将评论标记为正面或负面
- en: '[PRE13]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'In [listing 9.13](#listing9.13), we mark a review as being positive if its
    score is greater than three stars; otherwise, we mark it as negative. You can
    see the new `review/sentiment` elements are displayed when we take some elements
    from the transformed Bag. However, be careful: while it may look like we’ve modified
    the original data since we’re assigning new key-value pairs to each dictionary,
    the original data actually remains the same. Bags, like DataFrames and Arrays,
    are immutable objects. What happens behind the scenes is each old dictionary being
    transformed to a copy of itself with the additional key-value pairs, leaving the
    original data intact. We can confirm this by looking at the original `reviews`
    Bag.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 9.13](#listing9.13) 中，我们如果评论的评分大于三星，则将其标记为正面；否则，将其标记为负面。您可以看到，当我们从转换后的包中取出一些元素时，新的
    `review/sentiment` 元素会被显示出来。但是请注意：虽然看起来我们修改了原始数据，因为我们正在为每个字典分配新的键值对，但原始数据实际上保持不变。包，就像
    DataFrames 和 Arrays 一样，是不可变对象。幕后发生的事情是每个旧字典被转换为其自身的副本，并添加了额外的键值对，从而保持原始数据不变。我们可以通过查看原始的
    `reviews` 包来确认这一点。
- en: Listing 9.14 Demonstrating the immutability of Bags
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.14 展示包的不可变性
- en: '[PRE14]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see, the `review/sentiment` key is nowhere to be found. Just as when
    working with DataFrames, be aware of immutability to ensure you don’t run into
    any issues with disappearing data.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，`review/sentiment` 键无处可寻。就像在处理 DataFrames 时一样，请注意不可变性，以确保您不会遇到数据消失的问题。
- en: 9.2.2 Filtering Bags with the filter method
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.2 使用 filter 方法过滤包
- en: The second important data manipulation operation with Bags is filtering. Although
    Bags don’t offer a way to easily access a specific element, say the 45th element
    in the Bag, they do offer an easy way to search for specific data. Filter expressions
    are Python functions that return `True` or `False`. The `filter` method maps the
    filter expression over the Bag, and any element that returns `True` when the filter
    expression is evaluated is retained. Conversely, any element that returns `False`
    when the filter expression is evaluated is discarded. For example, if we want
    to find all reviews of product `B001E4KFG0`, we can create a filter expression
    to return that data.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 包的第二项重要数据操作是过滤。尽管包没有提供轻松访问特定元素的方法，比如包中的第 45 个元素，但它们确实提供了一个轻松搜索特定数据的方法。过滤表达式是返回
    `True` 或 `False` 的 Python 函数。`filter` 方法将过滤表达式映射到包上，任何在评估过滤表达式时返回 `True` 的元素都会被保留。相反，任何在评估过滤表达式时返回
    `False` 的元素都会被丢弃。例如，如果我们想找到所有关于产品 `B001E4KFG0` 的评论，我们可以创建一个过滤表达式来返回这些数据。
- en: Listing 9.15 Searching for a specific product
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.15 搜索特定产品
- en: '[PRE15]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '[Listing 9.15](#listing9.15) returns the data we requested, as well as a warning
    letting us know that there were fewer elements in the Bag than we asked for indicating
    there was only one review for the product we specified. We can also easily do
    fuzzy-matching searches. For example, we could find all reviews that mention “dog”
    in the review text.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.15](#listing9.15) 返回我们请求的数据，同时还有一个警告告诉我们包中的元素比我们请求的少，表明我们指定的产品只有一个评论。我们还可以轻松地进行模糊匹配搜索。例如，我们可以找到所有提到“狗”的评论。'
- en: Listing 9.16 Looking for all reviews that mention “dog”
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.16 查找所有提到“狗”的评论
- en: '[PRE16]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'And, just as with mapping operations, it’s possible to make filtering expressions
    more complex as well. To demonstrate, let’s use the following scenario for motivation:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 而且，就像映射操作一样，过滤表达式也可以变得更加复杂。为了演示，让我们使用以下场景作为动机：
- en: Using the Amazon Fine Foods Reviews dataset, write a filter function that removes
    reviews that were not deemed “helpful” by other Amazon customers.
  id: totrans-119
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Amazon Fine Foods Reviews 数据集，编写一个 filter 函数，移除其他亚马逊客户认为“有用”的评论。
- en: Amazon gives users the ability to rate reviews for their helpfulness. The `review/helpfulness`
    attribute represents the number of times a user said the review was helpful over
    the number of times users voted for the review. A helpfulness of 1/3 indicates
    that three users evaluated the review and only one found the review helpful (meaning
    the other two did not find the review helpful). Reviews that are unhelpful are
    likely to either be reviews where the reviewer unfairly gave a very low score
    or a very high score without justifying it in the review. It might be a good idea
    to eliminate unhelpful reviews from the dataset because they may not fairly represent
    the quality or value of a product. Let’s have a look at how unhelpful reviews
    are influencing the data by comparing the mean review score with and without unhelpful
    reviews. First, we’ll create a filter expression that will return `True` if more
    than 75% of users who voted found the review to be helpful, thereby removing any
    reviews below that threshold.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 亚马逊允许用户对评论的有用性进行评分。`review/helpfulness` 属性表示用户说评论有帮助的次数与用户为评论投票的次数之比。有用性为 1/3
    表示有三个用户评估了评论，但只有一个人认为评论有帮助（这意味着其他两个人没有认为评论有帮助）。无用的评论可能要么是评论者不公平地给出了非常低的分数或非常高的分数而没有在评论中加以说明。可能最好从数据集中消除无用的评论，因为它们可能无法公平地代表产品的质量或价值。让我们通过比较有无无用评论的平均评论评分来查看无用的评论是如何影响数据的。首先，我们将创建一个过滤表达式，如果超过
    75% 的投票用户认为评论有帮助，则该表达式将返回 `True`，从而删除低于该阈值的任何评论。
- en: Listing 9.17 A filter expression to filter out unhelpful reviews
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.17 一个过滤表达式，用于过滤掉无用的评论
- en: '[PRE17]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Unlike the simple filter expressions defined inline using `lambda` expressions
    in listings 9.15 and 9.16, we’ll define a function for this filter expression.
    Before we can evaluate the percentage of users that found the review helpful,
    we first have to calculate the percentage by parsing and transforming the raw
    helpfulness score. Again, we can do this in plain old Python using local scoped
    variables. We add some guards around the calculation to catch any potential divide-by-zero
    errors in the event no users voted on the review (note: practically, this means
    we assume reviews that haven’t been voted on are deemed unhelpful). If it’s had
    at least one vote, we return a Boolean expression that will evaluate to True if
    more than 75% of users found the review helpful. Now we can apply it to the data
    to see what happens.'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 与在列表 9.15 和 9.16 中使用 `lambda` 表达式定义的简单过滤表达式不同，我们将为此过滤表达式定义一个函数。在我们能够评估发现评论有帮助的用户百分比之前，我们首先必须通过解析和转换原始的有用性评分来计算这个百分比。同样，我们可以使用普通的
    Python 和局部作用域变量来完成这项工作。我们在计算周围添加了一些保护措施，以捕获任何潜在的除以零错误（注意：实际上，这意味着我们假设尚未投票的评论被认为是无用的）。如果至少有一个投票，我们返回一个布尔表达式，如果超过
    75% 的用户认为评论有帮助，则该表达式将评估为 True。现在我们可以将其应用于数据以查看会发生什么。
- en: Listing 9.18 Viewing the filtered data
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.18 查看过滤后的数据
- en: '[PRE18]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 9.2.3 Calculating descriptive statistics on Bags
  id: totrans-126
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.3 在“包”上计算描述性统计
- en: As expected, all of the reviews in the filtered Bag are “helpful.” Now let’s
    take a look at how that affects the review scores.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，过滤后的“包”中的所有评论都是“有帮助的”。现在让我们看看这如何影响评论评分。
- en: Listing 9.19 Comparing mean review scores
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.19 比较平均评论评分
- en: '[PRE19]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: In [listing 9.19](#listing9.19), we first extract the scores from the filtered
    Bag by mapping the `get_score` function over it. Then, we can call the `mean`
    method on each of the Bags that contain the review scores. After the means are
    computed, the output will display. Comparing the mean scores allows us to see
    if there’s any relationship between the helpfulness of reviews and the sentiment
    of the reviews. Are negative reviews typically seen as helpful? Unhelpful? Comparing
    the means allows us to answer this question. As can be seen, if we filter out
    the unhelpful reviews, the mean review score is actually a bit higher than the
    mean score for all reviews. This is most likely explained by the tendency of negative
    reviews to get downvoted if the reviewers don’t do a good job of justifying the
    negative score. We can confirm our suspicions by looking at the mean length of
    reviews that are helpful or unhelpful.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 9.19](#listing9.19) 中，我们首先通过映射 `get_score` 函数从过滤后的评论袋子中提取分数。然后，我们可以对包含评论分数的每个袋子调用
    `mean` 方法。计算平均值后，输出将显示。比较平均值可以让我们看到评论的有用性与评论的情感之间是否存在任何关系。负面评论通常被视为有帮助的吗？无帮助的吗？比较平均值可以帮助我们回答这个问题。如所见，如果我们过滤掉无帮助评论，平均评论分数实际上略高于所有评论的平均分数。这很可能是由于负面评论在评论者没有很好地为负面分数辩护的情况下，往往会得到负面投票。我们可以通过查看有帮助或无帮助评论的平均长度来证实我们的怀疑。
- en: Listing 9.20 Comparing mean review lengths based on helpfulness
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.20 比较基于有用性的平均评论长度
- en: '[PRE20]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'In [listing 9.20](#listing9.20), we’ve chained both map and filter operations
    together to produce our result. Since we already filtered out the helpful reviews,
    we can simply map the `get_length` function over the Bag of helpful reviews to
    extract the length of each review. However, we hadn’t isolated the unhelpful reviews
    before, so we did the following:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 9.20](#listing9.20) 中，我们将 map 和 filter 操作链式连接在一起以生成我们的结果。由于我们已经过滤掉了有帮助的评论，我们可以简单地映射
    `get_length` 函数到有帮助评论的袋子上以提取每条评论的长度。然而，我们之前并没有隔离无帮助评论，所以我们做了以下操作：
- en: Filtered the Bag of reviews by using the `remove_unhelpful_reviews` filter expression
  id: totrans-134
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过使用 `remove_unhelpful_reviews` 过滤表达式对评论袋进行过滤
- en: Used the `not` operator to invert the behavior of the filter expression (unhelpful
    reviews are retained, helpful reviews are discarded)
  id: totrans-135
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `not` 操作符反转过滤表达式的行为（保留无帮助评论，丢弃有帮助评论）
- en: Used `map` with the `get_length` function to count the length of each unhelpful
    review
  id: totrans-136
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `map` 与 `get_length` 函数一起计算每条无帮助评论的长度
- en: Finally, calculated the mean of all review lengths
  id: totrans-137
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，计算了所有评论长度的平均值
- en: It looks like unhelpful reviews are indeed shorter than helpful reviews on average.
    This means that the longer a review is, the more likely it will be voted by the
    community to be helpful.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来无帮助评论的平均长度确实比有帮助评论短。这意味着评论越长，越有可能被社区投票为有帮助。
- en: 9.2.4 Creating aggregate functions using the foldby method
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2.4 使用 foldby 方法创建聚合函数
- en: 'The last important data manipulation operation with Bags is *folding*. Folding
    is a special kind of reduce operation. While reduce operations have not been explicitly
    called out in this chapter, we’ve already seen a number of reduce operations throughout
    the book, as well as even in the previous code listing. Reduce operations, as
    you may guess by the name, reduce a collection of items in a Bag to a single value.
    For example, the `mean` method in the previous code listing reduces the Bag of
    raw review scores to a single value: the mean. Reduce operations typically involve
    some sort of aggregation over the Bag of values, such as summing, counting, and
    so on. Regardless of what the reduce operation does, it always results in a single
    value. Folding, on the other hand, allows us to add a grouping to the aggregation.
    A good example is counting the number of reviews by review score. Rather than
    count all the items in the Bag using a reduce operation, using a fold operation
    will allow us to count the number of items in each group. This means a fold operation
    reduces the number of elements in a Bag to the number of distinct groups that
    exist within the specified grouping. In the example of counting reviews by review
    score, this would result in reducing our original Bag down to five elements as
    there are five distinct review scores possible. [Figure 9.7](#figure9.7) shows
    an example of folding.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Bags的最后一个重要数据操作是 *折叠*。折叠是一种特殊的reduce操作。虽然reduce操作在本章中没有明确提及，但我们已经在整本书中看到了许多reduce操作，甚至在之前的代码列表中也是如此。正如你可能从名字猜到的，reduce操作会将Bag中的项目集合减少到单个值。例如，之前代码列表中的`mean`方法将原始评论分数的Bag减少到单个值：平均值。reduce操作通常涉及对Bag中的值进行某种聚合，例如求和、计数等。无论reduce操作做什么，它总是导致一个单一值。另一方面，折叠允许我们在聚合中添加一个分组。一个很好的例子是按评论分数计数评论的数量。与其使用reduce操作计数Bag中的所有项目，不如使用折叠操作可以让我们计数每个组中的项目数量。这意味着折叠操作将Bag中的元素数量减少到指定分组中存在的不同组数。在按评论分数计数评论的例子中，这将导致将我们的原始Bag减少到五个元素，因为有五种可能的不同的评论分数。[图9.7](#figure9.7)
    展示了折叠的一个示例。
- en: First, we need to define two functions to feed to the `foldby` method. These
    are called the `binop` and `combine` functions.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义两个函数来传递给`foldby`方法。这些函数被称为`binop`和`combine`函数。
- en: '![c09_07.png](Images/c09_07.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![c09_07.png](Images/c09_07.png)'
- en: '[Figure 9.7](#figureanchor9.7) An example of a fold operation'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '[图9.7](#figureanchor9.7) 折叠操作的示例'
- en: 'The `binop` function defines what should be done with the elements of each
    group, and always has two parameters: one for the accumulator and one for the
    element. The accumulator is used to hold the intermediate result across calls
    to the `binop` function. In this example, since our `binop` function is a counting
    function, it simply adds one to the accumulator each time the `binop` function
    is called. Since the `binop` function is called for every element in a group,
    this results in a count of items in each group. If the value of each element needs
    to be accessed, for instance if we wanted to sum the review scores, it can be
    accessed through the `element` parameter of the `binop` function. A `sum` function
    would simply add the element to the accumulator.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: '`binop`函数定义了每个组中的元素应该做什么，并且总是有两个参数：一个用于累加器，一个用于元素。累加器用于在`binop`函数的调用之间保持中间结果。在这个例子中，由于我们的`binop`函数是一个计数函数，它每次调用`binop`函数时都会将累加器加一。由于`binop`函数为组中的每个元素调用，这会导致每个组中的项目计数。如果需要访问每个元素的值，例如如果我们想求和评论分数，它可以通过`binop`函数的`element`参数访问。一个`sum`函数将简单地把元素加到累加器上。'
- en: The `combine` function defines what should be done with the results of the `binop`
    function across the Bag’s partitions. For example, we might have reviews with
    a score of three in several partitions. We want to count the total number of three-star
    reviews across the entire Bag, so intermediate results from each partition should
    be summed together. Just like the `binop` function, the first argument of the
    `combine` function is an accumulator, and the second argument is an element. Constructing
    these two functions can be challenging, but you can effectively think of it as
    a “group by” operation. The `binop` function specifies what should be done to
    the grouped data, and the `combine` function defines what should be done with
    groups that exist across partitions.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: '`combine` 函数定义了在Bag的分区上`binop`函数的结果应该如何处理。例如，我们可能在几个分区中有评分是三颗星的评论。我们希望计算整个Bag中三颗星评论的总数，因此每个分区的中间结果应该相加。就像`binop`函数一样，`combine`函数的第一个参数是一个累加器，第二个参数是一个元素。构建这两个函数可能具有挑战性，但你可以有效地将其视为一个“按组”操作。`binop`函数指定了对分组数据的操作，而`combine`函数定义了跨分区的组应该如何处理。'
- en: Now let’s take a look at what this looks like in code.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看代码中的样子。
- en: Listing 9.21 Using `foldby` to count the reviews by review score
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.21 使用`foldby`按评论评分计数
- en: '[PRE21]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: The five required arguments of the `foldby` method, in order from left to right,
    are the `key` function, the `binop` function, an initial value for the `binop`
    accumulator, the `combine` function, and an initial value for the `combine` accumulator.
    The `key` function defines what the values should be grouped by. Generally, the
    `key` function will just return a value that’s used as a grouping key. In the
    previous example, it simply returns the value of the review score using the `get_score`
    function defined earlier in the chapter. The output of [listing 9.21](#listing9.21)
    looks like this.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '`foldby`方法的五个必需参数，从左到右依次是`key`函数、`binop`函数、`binop`累加器的初始值、`combine`函数以及`combine`累加器的初始值。`key`函数定义了应该按什么值进行分组。通常，`key`函数将只返回一个用作分组键的值。在前面的例子中，它简单地使用本章前面定义的`get_score`函数返回评论评分的值。列表9.21的输出如下。'
- en: Listing 9.22 The output of the `foldby` operation
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.22 `foldby`操作的输出
- en: '[PRE22]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: What we’re left with after the code runs is a list of tuples, where the first
    element is the `key` and the second element is the result of the `binop` function.
    For example, there were 363,122 reviews that were given a five-star rating. Given
    the high mean review score, it shouldn’t come as any surprise that most of the
    reviews gave a five-star rating. It’s also interesting that there were more one-star
    reviews than there were two-star or three-star reviews. Nearly 75% of all reviews
    in this dataset were either five stars or one star—it seems most of our reviewers
    either absolutely loved their purchase or absolutely hated it. To get a better
    feel for the data, let’s dig a little bit deeper into the statistics of both the
    review scores and the helpfulness of reviews.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 代码运行后留下的是一个元组列表，其中第一个元素是`key`，第二个元素是`binop`函数的结果。例如，有363,122条评论被评为五星级。鉴于平均评论评分较高，大多数评论给出了五星级评分并不令人惊讶。有趣的是，一星级评论的数量比两星级或三星级评论多。在这个数据集中，近75%的所有评论都是五星级或一星级——似乎大多数评论者要么非常喜欢他们的购买，要么非常讨厌它。为了更好地了解数据，让我们深入了解一下评论评分和评论有用性的统计数据。
- en: 9.3 Building Arrays and DataFrames from Bags
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 从Bags构建数组和DataFrame
- en: Because the tabular format lends itself so well to numerical analysis, it’s
    likely that even if you begin a project by working with an unstructured dataset,
    as you clean and massage the data, you might have a need to put some of your transformed
    data into a more structured format. Therefore, it’s good to know how to build
    other kinds of data structures using data that begins in a Bag. In the Amazon
    Fine Foods dataset we’ve been looking at in this chapter, we have some numeric
    data, such as the review scores and the helpfulness percentage that was calculated
    earlier. To get a better understanding of what information these values tell us
    about the reviews, it would be helpful to produce descriptive statistics for these
    values. As we touched on in chapter 6, Dask provides a wide range of statistical
    functions in the `stats` module of the Dask Array API. We’ll now look at how to
    convert the Bag data we want to analyze into a Dask Array so we can use some of
    those statistics functions. First, we’ll start by creating a function that will
    isolate the review score and calculate the helpfulness percentage for each review.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 由于表格格式非常适合数值分析，因此即使你开始一个项目时使用的是非结构化数据集，随着你清理和整理数据，你可能需要将一些转换后的数据放入更结构化的格式中。因此，了解如何使用开始于
    Bag 的数据构建其他类型的数据结构是很有用的。在本章中我们一直在查看的 Amazon Fine Foods 数据集中，我们有一些数值数据，例如之前计算出的评论分数和有用性百分比。为了更好地理解这些值告诉我们关于评论的信息，生成这些值的描述性统计信息将是有帮助的。正如我们在第
    6 章中提到的，Dask 在 Dask Array API 的 `stats` 模块中提供了广泛的统计函数。现在，我们将看看如何将我们想要分析的 Bag 数据转换为
    Dask Array，以便我们可以使用这些统计函数。首先，我们将创建一个函数，该函数将隔离评论分数并计算每个评论的有用性百分比。
- en: Listing 9.23 A function to get the review score and helpfulness rating of each
    review
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.23 获取每个评论的评论分数和有用性评分的函数
- en: '[PRE23]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: The code in [listing 9.23](#listing9.23) should look familiar. It essentially
    combines the `get_score` function and the calculation of the helpfulness score
    from the filter function used to remove unhelpful reviews. Since this function
    returns a tuple of the two values, mapping over the Bag of reviews using this
    function will result in a Bag of tuples. This effectively mimics the row-column
    format of tabular data, since each tuple in the Bag will be the same length, and
    each tuple’s values have the same meaning.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.23](#listing9.23) 中的代码应该看起来很熟悉。它本质上结合了 `get_score` 函数和用于移除无用评论的过滤函数中的有用性分数计算。由于此函数返回两个值的元组，使用此函数映射评论的
    Bag 将导致一个包含元组的 Bag。这有效地模仿了表格数据的行-列格式，因为 Bag 中的每个元组长度相同，并且每个元组的值具有相同的意义。'
- en: To easily convert a Bag with the proper structure to a DataFrame, Dask Bags
    have a `to_dataframe` method. Now we’ll create a DataFrame holding the review
    score and helpfulness values.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 为了轻松地将具有适当结构的 Bag 转换为 DataFrame，Dask Bags 有一个 `to_dataframe` 方法。现在我们将创建一个包含评论分数和有用性值的
    DataFrame。
- en: Listing 9.24 Creating a DataFrame from a Bag
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.24 从 Bag 创建 DataFrame
- en: '[PRE24]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The `to_dataframe` method takes a single argument that specifies the name and
    datatype for each column. This is essentially the same `meta` argument that we
    saw many times with the drop-assign-rename pattern introduced in chapter 5\. The
    argument accepts a dictionary where the key is the column name and the value is
    the datatype for the column. With the data in a DataFrame, all the previous things
    you’ve learned about DataFrames can now be used to analyze and visualize the data!
    For example, calculating the descriptive statistics is the same as before.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: '`to_dataframe` 方法接受一个参数，指定每列的名称和数据类型。这本质上与我们在第 5 章中介绍的 drop-assign-rename 模式多次看到的
    `meta` 参数相同。该参数接受一个字典，其中键是列名，值是列的数据类型。在 DataFrame 中，你现在可以使用之前学到的所有关于 DataFrame
    的知识来分析和可视化数据！例如，计算描述性统计与之前相同。'
- en: Listing 9.25 Calculating descriptive statistics
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.25 计算描述性统计
- en: '[PRE25]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '[Listing 9.25](#listing9.25) produces the output shown in [figure 9.8](#figure9.8).'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.25](#listing9.25) 生成如图 9.8 所示的输出。'
- en: '![c09_08.png](Images/c09_08.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![c09_08.png](Images/c09_08.png)'
- en: '[Figure 9.8](#figureanchor9.8) The descriptive statistics of the Review Scores
    and Helpfulness Percent'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.8](#figureanchor9.8) 评论分数和有用性百分比的描述性统计'
- en: 'The descriptive statistics of the review scores give us a little more insight,
    but generally tell us what we already knew: reviews are overwhelmingly positive.
    The helpfulness percentage, however, is a bit more interesting. The mean helpfulness
    score is only about 41%, indicating that more often than not, reviewers didn’t
    find reviews to be helpful. However, this is likely influenced by the high number
    of reviews that didn’t have any votes. This might indicate that either Amazon
    shoppers are generally apathetic to reviews of food products and therefore don’t
    go out of their way to say something when a review was helpful—which may be the
    case since tastes are so variable—or that the typical Amazon shopper truly didn’t
    find these reviews very helpful. It might be interesting to compare these findings
    with reviews of other types of items that aren’t food to see if that makes a difference
    in engagement with reviews.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 回顾评分的描述性统计为我们提供了一些额外的洞察，但总体上只是告诉我们我们已经知道的事情：评论几乎都是正面的。然而，有用率的百分比却有点更有趣。平均有用率评分仅为约41%，这表明在大多数情况下，评论者并没有发现评论是有帮助的。然而，这很可能是由于大量没有投票的评论数量较多所影响。这可能表明，亚马逊购物者通常对食品产品的评论持冷漠态度，因此当评论有帮助时不会特意去说些什么——这可能是因为口味差异很大——或者典型的亚马逊购物者确实没有发现这些评论很有帮助。也许将这些建议与对其他类型（非食品）商品的评论进行比较，看看这会不会在评论的参与度上有所差异，会很有趣。
- en: 9.4 Using Bags for parallel text analysis with NLTK
  id: totrans-168
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 使用Bags进行NLTK的并行文本分析
- en: 'As we’ve looked at how to transform and filter elements in Bags, something
    may have become apparent to you: if all the transformation functions are just
    plain old Python, we should be able to use any Python library that works with
    generic collections—and that’s precisely what makes Bags so powerful and versatile!
    In this section, we’ll walk through some typical tasks for preparing and analyzing
    text data using the popular text analysis library NLTK (Natural Language Toolkit).
    As motivation for this example, we’ll use the following scenario:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们研究了如何在Bags中转换和过滤元素，可能已经对你有所启示：如果所有的转换函数都是普通的Python，我们应该能够使用任何与通用集合一起工作的Python库——这正是Bags如此强大和灵活的原因！在本节中，我们将通过一些典型任务来介绍如何使用流行的文本分析库NLTK（自然语言工具包）准备和分析文本数据。为了激发这个示例，我们将使用以下场景：
- en: Using NLTK and Dask Bags, find the most commonly mentioned phrases in the text
    of both positive and negative reviews for Amazon products to see what reviewers
    frequently discuss in their reviews.
  id: totrans-170
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用NLTK和Dask Bags，找出亚马逊产品正负评论文本中最常提到的短语，以了解评论者在评论中经常讨论的内容。
- en: 9.4.1 The basics of bigram analysis
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 双语分析的基本原理
- en: To find out more of what the reviewers in this dataset are writing about, we
    will perform a *bigram analysis* of the review text. Bigrams are pairs of adjacent
    words in text. The reason bigrams tend to be more useful than simply counting
    the frequency of individual words is they typically add more context. For example,
    we might expect positive reviews to contain the word “good” very frequently, but
    that doesn’t really help us understand *what* is good. The bigram “good flavor”
    or “good packaging” tells us a lot more about what the reviewers find positive
    about the products. Another thing that we need to do in order to better understand
    the true subject or sentiment of the reviews is to remove words that don’t help
    convey that information. Many words in the English language add structure to a
    sentence but don’t convey information. For example, articles like “the,” “a,”
    and “an” do not provide any context or information. Because these words are so
    common (and necessary for proper sentence formation), we’re just as likely to
    find these words in positive reviews as we are negative reviews. Since they don’t
    add any information, we will remove them. These are known as *stopwords*, and
    one of the most important data preparation tasks when doing text analysis is detecting
    and removing stopwords. [Figure 9.9](#figure9.9) shows examples of a few common
    stopwords.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解这个数据集中的评论者都在写些什么，我们将对评论文本进行*二元分析*。二元是文本中相邻单词的成对。二元通常比简单地计数单个单词的频率更有用，因为它们通常提供了更多的上下文。例如，我们可能会预期正面评论中“好”这个词会非常频繁地出现，但这并不能真正帮助我们理解*什么是好*。二元“好味道”或“好包装”告诉我们评论者对产品持正面看法的很多信息。为了更好地理解评论的真实主题或情感，我们还需要做的一件事是移除那些不帮助传达该信息的单词。英语中许多单词为句子增添了结构，但并不传达信息。例如，像“the”、“a”和“an”这样的冠词并不提供任何上下文或信息。因为这些单词非常常见（并且对于正确的句子结构是必要的），我们在正面评论和负面评论中都同样可能找到这些单词。由于它们不提供任何信息，我们将移除它们。这些被称为*停用词*，在进行文本分析时，检测和移除停用词是最重要的数据准备任务之一。[图
    9.9](#figure9.9)展示了几个常见停用词的示例。
- en: '![c09_09.eps](Images/c09_09.png)'
  id: totrans-173
  prefs: []
  type: TYPE_IMG
  zh: '![c09_09.eps](Images/c09_09.png)'
- en: '[Figure 9.9](#figureanchor9.9) Example of stopwords'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.9](#figureanchor9.9) 停用词示例'
- en: 'The process we’ll follow for performing the bigram analysis consists of the
    following steps:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将遵循以下步骤进行二元分析：
- en: Extract the text data.
  id: totrans-176
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 提取文本数据。
- en: Remove stopwords.
  id: totrans-177
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 移除停用词。
- en: Create bigrams.
  id: totrans-178
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建二元。
- en: Count the frequency of the bigrams.
  id: totrans-179
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算二元频率。
- en: Find the top 10 bigrams.
  id: totrans-180
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到前10个二元。
- en: 9.4.2 Extracting tokens and filtering stopwords
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 提取标记和过滤停用词
- en: Before we jump in, make sure you have NLTK set up properly in your Python environment.
    See the appendix for instructions on installing and configuring NLTK. With NLTK
    installed, we need to import the relevant modules into our current workspace;
    then we’ll create a few functions to help us along with the data prep.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，请确保您已经正确地在Python环境中设置了NLTK。有关安装和配置NLTK的说明，请参阅附录。安装了NLTK之后，我们需要将相关模块导入到当前工作区；然后我们将创建一些函数来帮助我们进行数据准备。
- en: Listing 9.26 Extract and filter functions
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.26 提取和过滤函数
- en: '[PRE26]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: In [listing 9.26](#listing9.26), we’re defining a few functions to help grab
    the review text from the original Bag and filter out the stopwords. One thing
    to point out is the use of the `partial` function inside the `filter_stopwords`
    function. Using `partial` allows us to freeze the value of the `stopwords` argument
    while keeping the value of the `word` argument dynamic. Since we want to compare
    every word to the same list of stopwords, the value of the `stopwords` argument
    should remain static. With our data preparation functions defined, we’ll now map
    over the Bag of reviews to extract and clean the review text.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 9.26](#listing9.26)中，我们定义了一些函数来帮助从原始的Bag中抓取评论文本并过滤掉停用词。需要指出的一点是`filter_stopwords`函数内部使用了`partial`函数。使用`partial`函数允许我们在保持`word`参数动态的同时冻结`stopwords`参数的值。由于我们想要将每个单词与相同的停用词列表进行比较，因此`stopwords`参数的值应该保持静态。在定义了我们的数据准备函数之后，我们现在将映射到评论Bag上以提取和清理评论文本。
- en: Listing 9.27 Extracting, tokenizing, and cleaning the review text
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.27 提取、标记化和清理评论文本
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The code in [listing 9.27](#listing9.27) should be pretty straightforward.
    We simply use the `map` function to apply the extracting, tokenizing, and filtering
    functions to the Bag of reviews. As you can see, we’re left with a Bag of lists,
    and each list contains all the unique nonstopwords found in the text of each review.
    If we take one element from this new Bag, we’re returned a list of all words in
    the first review (except for stopwords, that is). This is important to note: currently
    our Bag is a nested collection. We’ll come back to that momentarily. However,
    now that we have the cleaned list of words for each review, we’ll transform our
    Bag of lists of tokens into a Bag of lists of bigrams.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.27](#listing9.27) 中的代码应该相当直接。我们只是简单地使用 `map` 函数将提取、分词和过滤函数应用于评论的词袋。正如你所见，我们得到了一个包含列表的词袋，每个列表包含每个评论文本中找到的所有独特非停用词。如果我们从这个新词袋中取一个元素，我们会得到第一个评论中所有单词的列表（除了停用词）。这一点很重要：目前我们的词袋是一个嵌套集合。我们稍后会回到这一点。然而，现在我们有了每个评论的清洁单词列表，我们将把我们的标记列表词袋转换成大词袋列表。'
- en: Listing 9.28 Creating bigrams
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.28 创建大词袋
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: In [listing 9.28](#listing9.28), we simply have another function to map over
    the previously created Bag. Again, this is pretty exciting because this process
    is completely parallelized using Dask. This means we could use the exact same
    code to analyze billions or trillions of reviews! As you can see, we now have
    a list of bigrams. However, we still have the nested data structure. Taking two
    elements results in two lists of bigrams. We’re going to want to find the most
    frequent bigrams across the entire Bag, so we need to get rid of the nested structure.
    This is called *flattening* a Bag. Flattening removes one level of nesting; for
    example, a list of two lists containing 5 elements each becomes a single list
    containing all 10 elements.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 9.28](#listing9.28) 中，我们只是有一个映射到之前创建的词袋的另一个函数。这同样非常令人兴奋，因为这个过程完全使用 Dask
    并行化。这意味着我们可以使用完全相同的代码来分析数十亿或数万亿条评论！正如你所见，我们现在有一个大词袋的列表。然而，我们仍然有嵌套的数据结构。取两个元素会产生两个大词袋的列表。我们想要找到整个词袋中最频繁的大词袋，因此我们需要消除嵌套结构。这被称为
    *平滑* 词袋。平滑移除一个嵌套层级；例如，包含 5 个元素的列表的列表变成一个包含所有 10 个元素的单一列表。
- en: Listing 9.29 Flattening the Bag of bigrams
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.29 平滑大词袋
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: After flattening the Bag in [listing 9.29](#listing9.29), we’re now left with
    a Bag that contains all bigrams without any nesting by review. It’s now no longer
    possible to figure out which bigram came from which review, but that’s OK because
    that’s not important for our analysis. What we want to do is fold this Bag using
    the bigram as the key, and counting the number of times each bigram appears in
    the dataset. We can reuse the `count` and `compute` functions we defined earlier
    in the chapter.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [列表 9.29](#listing9.29) 中平滑词袋之后，我们现在剩下的是一个包含所有大词袋且没有嵌套的词袋。现在不再可能确定哪个大词袋来自哪个评论，但这没关系，因为这对我们的分析并不重要。我们想要做的是使用大词袋作为键来折叠这个词袋，并计算每个大词袋在数据集中出现的次数。我们可以重用本章中定义的
    `count` 和 `compute` 函数。
- en: Listing 9.30 Counting the bigrams and finding the top 10 most common bigrams
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.30 计数大词袋并找出最常见的 10 个大词袋
- en: '[PRE30]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: The `foldby` function in [listing 9.30](#listing9.30) looks exactly like the
    `foldby` function you saw earlier in the chapter. However, we’ve chained a new
    method to it, `topk`, which gets the top *k* number of elements when the Bag is
    sorted in descending order. In the previous example, we get the top 10 elements
    as denoted by the first parameter of the method. The second parameter, the `key`
    parameter, defines what the Bag should be sorted by. The folding function returns
    a Bag of tuples where the first element is the key and the second element is the
    frequency. We want to find the top 10 most frequent bigrams, so the Bag should
    be sorted by the second element of each tuple. Therefore, the `key` function simply
    returns the frequency element of each tuple. This has been shortened by using
    a `lambda` expression since the `key` function is so simple. Taking a look at
    the most common bigrams, it looks like we have some unhelpful entries. For example,
    “amazon com” is the second most frequent bigram. This makes sense, since the reviews
    are from Amazon. It looks like some HTML may have also leaked into the reviews,
    because “br br” is the most common bigram. This is in reference to the HTML tag,
    `<br>`, which denotes whitespace. These words aren’t helpful or descriptive at
    all, so we should add them to our list of stopwords and rerun the bigram analysis.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 9.30](#listing9.30) 中的 `foldby` 函数看起来与本章前面看到的 `foldby` 函数完全一样。然而，我们给它链接着一个新的方法，`topk`，当Bag按降序排序时，它会获取前
    *k* 个元素。在先前的例子中，我们通过方法的第一个参数得到了前10个元素。第二个参数，`key` 参数，定义了Bag应该按什么排序。折叠函数返回一个包含元组的Bag，其中第一个元素是键，第二个元素是频率。我们想要找到最频繁的10个二元组，所以Bag应该按每个元组的第二个元素排序。因此，`key`
    函数简单地返回每个元组的频率元素。由于`key` 函数非常简单，所以已经通过使用 `lambda` 表达式进行了简化。看一下最常见的二元组，看起来我们有一些无用的条目。例如，“amazon
    com”是第二频繁的二元组。这是有道理的，因为评论来自亚马逊。看起来一些HTML也可能泄漏到了评论中，因为“br br”是最常见的二元组。这是指HTML标签`<br>`，表示空白。这些词完全没有帮助或描述性，所以我们应该将它们添加到我们的停用词列表中，并重新运行二元组分析。'
- en: Listing 9.31 Adding more stopwords and rerunning the analysis
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.31 添加更多停用词并重新运行分析
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 9.4.3 Analyzing the bigrams
  id: totrans-200
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.3 分析二元组
- en: Now that we’ve removed the additional stopwords, we can see some clear topics.
    For example, “k cups” and “coffee” were mentioned a large number of times. This
    is probably because many of the reviews are for coffee pods for Keurig coffee
    machines. The most common bigram is “highly recommend,” which also makes sense
    because a lot of the reviews were positive. We could continue iterating over our
    list of stopwords to see what new patterns emerge (perhaps we could remove the
    words such as “like” and “store” because they don’t add much information), but
    it would also be interesting to see how the list of bigrams look for reviews that
    are negative. To close out the chapter, we’ll filter our original set of reviews
    to those that got only one or two stars, and then see what bigrams are the most
    common.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经移除了额外的停用词，我们可以看到一些清晰的主题。例如，“k 杯”和“咖啡”被提及很多次。这可能是由于许多评论是针对Keurig咖啡机的咖啡胶囊。最常见的二元组是“强烈推荐”，这也很有道理，因为许多评论都是积极的。我们可以继续迭代我们的停用词列表，看看是否有新的模式出现（也许我们可以移除像“喜欢”和“商店”这样的词，因为它们没有提供太多信息），但也很想看看负面评论的二元组列表看起来如何。为了结束本章，我们将过滤原始评论集，只保留得到一星或两星的评论，然后看看哪些二元组是最常见的。
- en: Listing 9.32 Finding the most common bigrams for negative reviews
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.32 寻找负面评论中最常见的二元组
- en: '[PRE32]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: The list of bigrams we get from [listing 9.32](#listing9.32) shares some similarities
    with the bigrams from all reviews, but also has some distinct bigrams that show
    frustration or disappointment with the product (“thought would,” “waste money,”
    and so forth). Interestingly, “taste good” is a bigram for the negative reviews.
    This might be because reviewers would say something like “I thought it would taste
    good” or “It didn’t taste good.” This shows that the dataset needs a bit more
    work—perhaps more stopwords—but now you have all the tools you need to do it!
    We’ll come back to this dataset in the next chapter, when we’ll use Dask’s machine
    learning pipelines to build a sentiment classifier that will try to predict whether
    a review is positive or negative based on its text. In the meantime, hopefully
    you’ve come to appreciate how powerful and flexible Dask Bags are for unstructured
    data analysis.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 从 [列表 9.32](#listing9.32) 获得的大词组与所有评论中的大词组有一些相似之处，但也包含一些独特的大词组，这些大词组显示了用户对产品的挫败感或失望（例如，“thought
    would”、“waste money”等等）。有趣的是，“taste good”是负面评论的大词组。这可能是因为评论者会说类似于“我想它会尝起来很好”或“它不好吃。”这表明数据集需要做更多的工作——可能需要更多的停用词——但现在你有了所有需要的工具来做这件事！我们将在下一章回到这个数据集，届时我们将使用
    Dask 的机器学习管道构建一个情感分类器，该分类器将尝试根据文本预测评论是正面还是负面。同时，希望你已经开始欣赏 Dask Bags 在非结构化数据分析中的强大和灵活性。
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Unstructured data, such as text, doesn’t lend itself well to being analyzed
    using DataFrames. Dask Bags are a more flexible solution and are useful for manipulating
    unstructured data.
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 非结构化数据，如文本，不适合使用 DataFrame 进行分析。Dask Bags 是一个更灵活的解决方案，并且对于操作非结构化数据很有用。
- en: Bags are unordered and do not have any concept of an index (unlike DataFrames).
    To access elements of a Bag, the `take` method can be used.
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bags 是无序的，并且没有索引的概念（与 DataFrame 不同）。要访问 Bag 的元素，可以使用 `take` 方法。
- en: The `map` method is used to transform each element of a Bag using a user-defined
    function.
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`map` 方法用于使用用户定义的函数将 Bag 的每个元素进行转换。'
- en: The `foldby` function makes it possible to aggregate elements of a Bag before
    mapping a function over them. This can be used for all sorts of aggregate-type
    functions.
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`foldby` 函数使得在映射函数之前对 Bag 的元素进行聚合成为可能。这可以用于所有类型的聚合函数。'
- en: When analyzing text data, tokenizing the text and removing stopwords helps extract
    the underlying meaning of the text.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在分析文本数据时，对文本进行分词和去除停用词有助于提取文本的潜在含义。
- en: Bigrams are used to extract phrases from text that may have more meaning than
    their constituent words (for example, “not good” versus “not” and “good” in isolation).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大词组用于从文本中提取可能比其组成单词更有意义的短语（例如，“不好”与“不”和“好”单独使用时的区别）。
- en: '10'
  id: totrans-212
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '10'
- en: Machine learning with Dask-ML
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Dask-ML 进行机器学习
- en: '**This chapter covers**'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Building machine learning models using the Dask-ML API
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask-ML API 构建机器学习模型
- en: Using the Dask-ML API to extend scikit-learn
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Dask-ML API 扩展 scikit-learn
- en: Validating models and tuning hyperparameters using cross-validated gridsearch
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证网格搜索验证模型和调整超参数
- en: Using serialization to save and publish trained models
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用序列化保存和发布训练好的模型
- en: 'A common admission by data scientists is that the 80/20 rule definitely applies
    to data science: that is, 80% of time spent on data science projects is preparing
    data for machine learning and the other 20% is actually building and testing the
    machine learning models. This book is no exception! By now, we’ve been through
    the gathering, cleaning, and exploration process for two different datasets in
    two different “flavors”—using DataFrames and using Bags. It’s now time to move
    on and build some machine learning models of our own! For a point of reference,
    [figure 10.1](#figure10.1) shows how we’re progressing through our workflow. We’ve
    almost arrived at the end!'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家普遍认为，80/20 规则确实适用于数据科学：也就是说，80% 的时间用于为机器学习项目准备数据，其余 20% 的时间是实际构建和测试机器学习模型。这本书也不例外！到目前为止，我们已经完成了两个不同数据集的收集、清洗和探索过程，使用了两种不同的“风味”——使用
    DataFrame 和使用 Bag。现在是时候继续前进，构建我们自己的机器学习模型了！为了参考，[图 10.1](#figure10.1) 展示了我们在工作流程中的进展。我们几乎到达了终点！
- en: '![c10_01.eps](Images/c10_01.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![c10_01.eps](Images/c10_01.png)'
- en: '[Figure 10.1](#figureanchor10.1) Having thoroughly covered data preparation,
    it’s time to move on to model building.'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.1](#figureanchor10.1) 在彻底完成数据准备之后，现在是时候转向模型构建了。'
- en: 'In this chapter, we’ll have a look at the last major API of Dask: Dask-ML.
    Just as we’ve seen how Dask DataFrames parallelize Pandas and Dask Arrays parallelize
    NumPy, Dask-ML is a parallel implementation of scikit-learn. [Figure 10.2](#figure10.2)
    shows the relationship between the Dask APIs and the underlying functionality
    they provide.'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将查看 Dask 的最后一个主要 API：Dask-ML。正如我们看到的，Dask DataFrames 并行化 Pandas，Dask
    Arrays 并行化 NumPy，Dask-ML 是 scikit-learn 的并行实现。[图 10.2](#figure10.2) 展示了 Dask API
    与其提供的底层功能之间的关系。
- en: '![c10_02.eps](Images/c10_02.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![c10_02.eps](Images/c10_02.png)'
- en: '[Figure 10.2](#figureanchor10.2) A review of the API components of Dask'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.2](#figureanchor10.2) Dask API 组件回顾'
- en: 'If you have prior experience with scikit-learn, you will find the API very
    familiar; if not, what you learn here should give you enough of an introduction
    to scikit-learn for you to be able to continue exploring it on your own! Dask-ML
    is a relatively recent addition to Dask and therefore has not had as much time
    to mature compared to Dask’s other APIs. However, it still offers a wide variety
    of functionality and was designed with flexibility that allows it to solve most
    problems where scikit-learn would normally be used. We’ll pick up where we left
    off in the previous chapter with the Amazon Fine Foods reviews and use the following
    scenario as a backdrop to exploring Dask-ML:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前有使用 scikit-learn 的经验，你会发现 API 非常熟悉；如果没有，这里学到的知识应该足以让你能够自己继续探索 scikit-learn！Dask-ML
    是 Dask 中相对较新的功能，因此与 Dask 的其他 API 相比，它没有太多的时间来成熟。然而，它仍然提供了广泛的功能，并且设计时考虑了灵活性，使其能够解决大多数通常使用
    scikit-learn 解决的问题。我们将从上一章的 Amazon Fine Foods 评论继续，并使用以下场景作为探索 Dask-ML 的背景：
- en: Using the Amazon Fine Foods Reviews dataset, train a sentiment classifier model
    with Dask-ML that can interpret whether a review is positive or negative without
    knowing the review score.
  id: totrans-226
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Amazon Fine Foods 评论数据集，使用 Dask-ML 训练一个情感分类器模型，该模型可以解释评论是正面还是负面，而无需知道评论分数。
- en: 10.1 Building linear models with Dask-ML
  id: totrans-227
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.1 使用 Dask-ML 构建线性模型
- en: 'Before we jump in to model building, we’ll need to take care of a few things:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始构建模型之前，我们需要处理一些事情：
- en: We’ll need to tag the reviews as positive or negative using the code from chapter
    9.
  id: totrans-229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们需要使用第 9 章中的代码将评论标记为正面或负面。
- en: Then we’ll need to convert the data into a format that our machine learning
    model can understand.
  id: totrans-230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后，我们需要将数据转换成机器学习模型可以理解的形式。
- en: Finally, we’ll need to set aside a small chunk of the data to use for testing
    and validating the accuracy of our model.
  id: totrans-231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要留出一小部分数据用于测试和验证我们模型的准确性。
- en: First, the reviews need to be tagged as either positive or negative. We went
    through some code in chapter 9 to do that based on the review score that the reviewer
    provided. If the review received three stars or more, we tagged the review as
    positive. If the review received two stars or fewer, we tagged the review as negative.
    To recap, here’s the code we used to do that.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，评论需要被标记为正面或负面。我们在第 9 章中通过评论者提供的评论分数进行了一些代码操作来实现这一点。如果评论得到三个星或以上，我们将评论标记为正面。如果评论得到两个星或以下，我们将评论标记为负面。为了回顾，以下是用来完成这一操作的代码。
- en: Listing 10.1 Tagging the review data based on the review score
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.1 根据评论分数标记评论数据
- en: '[PRE33]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 10.1.1 Preparing the data with binary vectorization
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.1 使用二进制向量化准备数据
- en: 'Now that we’ve tagged the reviews again, we need to turn the review text into
    a format that the machine learning algorithm can understand. We humans can intuitively
    understand that if someone says a product is “great,” that person likely has a
    positive sentiment toward the product. Computers, on the other hand, don’t generally
    share the same grasp of language that humans do—a computer doesn’t intrinsically
    understand what “great” means or how it translates to sentiments about a product.
    However, think about what was just said: if a person says a product is “great,”
    they probably feel positively toward the product. This is a pattern we can search
    for in our data. Were reviews that used the word “great” more likely to be positive
    than reviews that didn’t? If so, we could state that the presence of the word
    “great” in a review makes it some amount more likely to be positive. This is the
    whole idea behind one common way to transform text data to a machine-understandable
    format called *binary vectorization*. Using binary vectorization, we take a *corpus*,
    or a unique list of all words that show up in our review data, and generate a
    vector of 1s and 0s, where a 1 indicates the presence of a word and a 0 indicates
    the absence of a word.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经再次标记了评论，我们需要将评论文本转换为机器学习算法可以理解的形式。我们人类可以直观地理解，如果有人说一个产品是“很好”，那么这个人可能对这个产品有积极的情感。然而，计算机通常并不具备与人类相同的语言理解能力——计算机本身并不理解“很好”的含义或它如何转化为对产品的情感。然而，想想刚才说过的话：如果一个人说一个产品是“很好”，他们可能对这个产品有积极的情感。这是我们可以在数据中寻找的模式。使用“很好”这个词的评论是否比没有使用这个词的评论更有可能为正面？如果是这样，我们就可以说，评论中出现“很好”这个词会使它有更大的可能性是正面的。这正是将文本数据转换为机器可理解格式的一种常见方法，称为
    *二元向量化*。使用二元向量化，我们取一个 *语料库*，即我们评论数据中出现的所有独特单词的唯一列表，并生成一个由 1 和 0 组成的向量，其中 1 表示单词的存在，而
    0 表示单词的缺失。
- en: '![c10_03.eps](Images/c10_03.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![c10_03.eps](Images/c10_03.png)'
- en: '[Figure 10.3](#figureanchor10.3) An example of binary vectorization'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.3](#figureanchor10.3) 二元向量化的示例'
- en: In [figure 10.3](#figure10.3), you can see that the words that appear in the
    raw text such as “lots” and “fun” are assigned a 1 in the binary vector, whereas
    words that do not appear in the raw text (but appear in other text samples) are
    marked with a 0\. Once the text has been transformed with binary vectorization,
    we can use any of the standard classification algorithms, such as logistic regression,
    to find correlations between the presence of words and the sentiment. This in
    turn will help us build a model to classify reviews as positive or negative where
    we don’t have the actual review score. Let’s take a look at how to transform our
    raw reviews into an array of binary vectors.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 10.3](#figure10.3) 中，您可以看到在原始文本中出现的单词，如“很多”和“有趣”，在二元向量中被分配为 1，而那些在原始文本中没有出现（但在其他文本样本中出现）的单词则标记为
    0。一旦文本经过二元向量化转换，我们就可以使用任何标准分类算法，例如逻辑回归，来找出单词出现与情感之间的相关性。这反过来又可以帮助我们构建一个模型，将评论分类为正面或负面，即使我们没有实际的评论评分。让我们看看如何将我们的原始评论转换为二元向量数组。
- en: First, we’ll apply some of the transformations we applied in chapter 9 to tokenize
    the text and remove stopwords (if this is your first time running this code, make
    sure you’ve followed the instructions in the appendix to set up NLTK correctly).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们将应用在第九章中应用的一些转换来分词文本并去除停用词（如果您是第一次运行此代码，请确保您已遵循附录中的说明正确设置 NLTK）。
- en: Listing 10.2 Tokenizing text and Removing stopwords
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.2 分词文本和去除停用词
- en: '[PRE34]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: As you’ve already seen the code from [listing 10.2](#listing10.2) in chapter
    9, we’ll move on. With the cleaned and tokenized review data, let’s get a quick
    count of the number of unique words that show up in the reviews. To do this, we’ll
    revisit a few built-in functions from the Bag API.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在第九章中已经看到的 [列表 10.2](#listing10.2) 的代码，我们将继续前进。使用清洗和分词后的评论数据，让我们快速统计一下在评论中出现的独特单词的数量。为此，我们将重新访问
    Bag API 中的几个内置函数。
- en: Listing 10.3 Counting the unique words in the Amazon Fine Foods review set
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.3 计算亚马逊美食评论集中的独特单词数量
- en: '[PRE35]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: This code should look mostly familiar. The only thing noteworthy is that the
    extracted tokens must be flattened to get a distinct list of all words. Because
    the `extract_tokens` function returns a list of lists of strings, we need to use
    `flatten` to concatenate all the inner lists before applying `distinct`. According
    to our code, 114,290 unique words appear in our 568,454 reviews. This means the
    array we would produce with binary vectorization would be 568,454 rows by 114,290
    columns or 64.9 billion ones and zeros. At one byte per Boolean value, by way
    of NumPy’s data sizes, this is ~64 GB of data. While Dask is certainly up to the
    task of dealing with such large arrays, we’ll scale down the exercise a bit to
    make it easier to run this solution quickly. Instead of using the entire corpus
    of 114,290 unique words, we’ll use a corpus of the top 100 most frequently used
    words in the review dataset. If you’d like to use a larger or smaller corpus,
    you can easily modify the code to use the top 1,000 or top 10 words instead. You
    can also modify the code to use the entire corpus if you’d like. All the code
    will work regardless of the size of the corpus you select. Of course, in practice,
    it would be best to start with the entire corpus—by selecting only the top 100
    words, we may be leaving out some important patterns that occur infrequently but
    are strong predictors of the target variable. Again, I’m only suggesting scaling
    down here for the sake of a fast-running example. Let’s take a look at how to
    get the top 100 most common words in our corpus.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码看起来应该很熟悉。唯一值得注意的是，提取的标记必须被展平以获得所有单词的唯一列表。因为`extract_tokens`函数返回一个字符串列表的列表，我们需要使用`flatten`在应用`distinct`之前连接所有内部列表。根据我们的代码，我们的568,454条评论中出现了114,290个独特的单词。这意味着我们用二进制向量化产生的数组将是568,454行乘以114,290列，或者649亿个一和零。以每个布尔值一个字节计算，通过NumPy的数据大小，这大约是64GB的数据。虽然Dask当然能够处理这样的大型数组，但为了使这个解决方案更容易快速运行，我们将稍微缩小一下练习的范围。我们不会使用114,290个独特单词的整个语料库，而是使用评论数据集中使用频率最高的前100个单词的语料库。如果您想使用更大的或更小的语料库，可以轻松修改代码以使用前1,000个或前10个单词。如果您愿意，也可以修改代码以使用整个语料库。无论您选择的语料库大小如何，所有代码都将正常工作。当然，在实践中，最好从整个语料库开始——通过只选择前100个单词，我们可能会错过一些不常见但却是目标变量强预测器的模式。再次强调，我只是在为了快速运行的示例而建议缩小规模。让我们看看如何获取语料库中最常见的100个单词。
- en: Listing 10.4 Finding the top 100 most common words in the reviews dataset
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.4 在评论数据集中查找最常见的100个单词
- en: '[PRE36]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Again, this code should look familiar since we’ve looked at a few examples of
    folding in the previous chapter. As before, we use the `count` and `combine` functions
    to count the occurrences of each word in the corpus. The result of the fold gives
    us a list of tuples where element 0 of each tuple is the word and element 1 of
    each tuple is the count of occurrences. Using Python’s built-in `sorted` method,
    we sort along element 1 of each tuple (the frequency counts) to return a list
    of tuples sorted in descending order. Finally, we use the `map` function to peel
    the words out of the sorted tuples to return a list of the top 100 most commonly
    used words. Now that we have our final corpus, we can apply binary vectorization
    across the review tokens. We’ll do this by searching each review to see if it
    contains the words in the corpus.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，这段代码应该看起来很熟悉，因为我们已经在上一章中查看了一些折叠的例子。和之前一样，我们使用`count`和`combine`函数来计算语料库中每个单词的出现次数。折叠的结果给我们一个元组的列表，其中每个元组的第0个元素是单词，第1个元素是该单词出现次数的计数。使用Python内置的`sorted`方法，我们根据每个元组的第1个元素（频率计数）进行排序，以返回一个按降序排序的元组列表。最后，我们使用`map`函数从排序后的元组中提取单词，以返回使用频率最高的前100个单词的列表。现在我们有了最终的语料库，我们可以对评论标记应用二进制向量化。我们将通过搜索每个评论是否包含语料库中的单词来实现这一点。
- en: Listing 10.5 Generating training data by applying binary vectorization
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 列表10.5 通过应用二进制向量化生成训练数据
- en: '[PRE37]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'The code in [listing 10.5](#listing10.5) shows another good example of how
    we can mix other libraries like NumPy into Dask. Here, we use the `where` function
    in NumPy to compare the list of words from the corpus to the list of tokens for
    each review. This results in a vector of 100 ones and zeros for each review, as
    you can see in the sample output. We also apply binary vectorization to the sentiment
    tag, which is what we want to predict—also known as our *target*. The result of
    the code returns a Bag of dictionaries, where each dictionary object represents
    a review and contains its respective binarized values. We’re getting very close
    to building our model, but one important thing stands in the way: our data is
    still in a Bag, and it needs to be in an Array for Dask-ML to read it. Previously,
    we converted data from a Bag to an Array by first converting it to a DataFrame
    and then using the `values` attribute of the DataFrame to directly access the
    underlying Array. We could do that here, but DataFrames tend to not perform very
    well with a large number of columns. Instead, we’ll take the existing NumPy arrays
    that we produced in the binary vectorization step and concatenate them into one
    large Dask Array. Put another way, we’ll *reduce* a list of arrays to a single
    array using concatenation. [Figure 10.4](#figure10.4) shows a visual representation
    of what we want to accomplish.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 10.5](#listing10.5) 中的代码展示了另一个很好的例子，说明了我们如何将其他库如 NumPy 混合到 Dask 中。在这里，我们使用
    NumPy 的 `where` 函数来比较语料库中的单词列表与每个评论的标记列表。这为每个评论生成一个包含 100 个一和零的向量，正如你在示例输出中所看到的。我们还对情感标签应用了二进制向量化，这是我们想要预测的目标——也称为我们的
    *目标*。代码的结果返回一个字典包，其中每个字典对象代表一个评论，并包含其各自的二进制值。我们正在接近构建我们的模型，但有一个重要的事情阻碍了我们：我们的数据仍然在包中，它需要以数组的形式存在于
    Dask-ML 中。之前，我们通过首先将其转换为 DataFrame，然后使用 DataFrame 的 `values` 属性直接访问底层数组来将数据从包转换为数组。我们也可以这样做，但
    DataFrame 在处理大量列时通常表现不佳。相反，我们将使用在二进制向量化步骤中产生的现有 NumPy 数组，并将它们连接成一个大的 Dask 数组。换句话说，我们将通过连接将一系列数组
    *减少* 到单个数组。[图 10.4](#figure10.4) 展示了我们想要实现的可视化表示。'
- en: '![c10_04.eps](Images/c10_04.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![c10_04.eps](Images/c10_04.png)'
- en: '[Figure 10.4](#figureanchor10.4) Vectorizing the raw data into a Bag of arrays,
    then concatenating to a single array'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.4](#figureanchor10.4) 将原始数据向量化为数组包，然后连接到单个数组'
- en: Effectively, we’re building a Dask Array from scratch one row at a time. This
    is actually fairly quick and efficient, because Dask’s lazy evaluation means we’re
    largely dealing with metadata until we actually try to materialize the data in
    the final array. Let’s take a look at how to do this in code.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们是一行一行地从零开始构建 Dask 数组。这实际上相当快且高效，因为 Dask 的懒加载评估意味着我们主要处理元数据，直到我们真正尝试将数据在最终数组中具体化。让我们看看如何在代码中实现这一点。
- en: Listing 10.6 Creating the feature array
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[代码列表 10.6](#listing10.6) 创建特征数组'
- en: '[PRE38]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[Listing 10.6](#listing10.6) contains several new methods that we’ll unpack.
    First is the `concatenate` function from the Dask Array API. It will concatenate,
    or combine, a list of Dask Arrays into a single Dask Array. Since we ultimately
    want to combine each of the 568,454 vectors into one large array, this is exactly
    the function we want to use. Since the data is spread out across roughly 100 partitions,
    we’ll need to reduce each partition’s list of arrays into a single array, and
    then combine the 100 partition-level arrays into one final large array. This can
    be done with the `reduction` method of Dask Array. This function works slightly
    differently from `map` in that the function passed to it should receive an entire
    partition instead of a single element. After mapping the `from_array` function
    to each element, each partition is essentially a lazy list of Dask Array objects.
    This is exactly what input `dask_array.concatenate` wants. However, the partition
    object passed into our `stacker` function happens to be a generator object, which
    `dask_array.concatenate` cannot cope with. Therefore, we have to materialize it
    into a list by using a list comprehension. You may think, at first, that this
    would be counterproductive, because materializing the partition into a list would
    bring the data with it. However, the partition happens to be a list of lazy Dask
    Array objects, so the only data that actually gets shuttled around is some metadata
    and the DAG tracking the computation that’s occurred so far. We can see that we
    get the result we want because the new Array shape states it’s 568,454 rows by
    100 columns. The shape of the feature array can be seen in [figure 10.5](#figure10.5).'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.6](#listing10.6) 包含了几个我们将要展开的新方法。首先是来自 Dask 数组 API 的 `concatenate` 函数。它将连接或组合一系列
    Dask 数组成为一个单一的 Dask 数组。由于我们最终想要将 568,454 个向量组合成一个大的数组，这正是我们想要的函数。由于数据分布在大约 100
    个分区中，我们需要将每个分区的数组列表缩减成一个单一的数组，然后将 100 个分区级别的数组组合成一个最终的大的数组。这可以通过 Dask 数组的 `reduction`
    方法来完成。这个函数与 `map` 函数的工作方式略有不同，因为它传递给它的函数应该接收整个分区而不是单个元素。在将 `from_array` 函数映射到每个元素之后，每个分区本质上是一个懒加载的
    Dask 数组对象列表。这正是 `dask_array.concatenate` 输入所想要的。然而，传递给我们的 `stacker` 函数的分区对象恰好是一个生成器对象，而
    `dask_array.concatenate` 无法处理。因此，我们必须通过列表推导式将其实体化成一个列表。你可能会想，一开始，这可能是适得其反的，因为将分区实体化成列表会带来数据。然而，分区恰好是一个懒加载的
    Dask 数组对象列表，所以实际上被传递的数据只有一些元数据和跟踪到目前为止所发生计算的有向无环图（DAG）。我们可以看到，我们得到了想要的结果，因为新的数组形状表明它有
    568,454 行和 100 列。特征数组的形状可以在 [图 10.5](#figure10.5) 中看到。'
- en: '![c10_05.eps](Images/c10_05.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![c10_05.eps](Images/c10_05.png)'
- en: '[Figure 10.5](#figureanchor10.5) The shape of the feature array'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.5](#figureanchor10.5) 特征数组的形状'
- en: Since we’ve done so much to the data already, now would be an opportune time
    to save our progress. Writing out the data before we train the model will also
    speed things up since the data will already be in the shape needed to build the
    model. The Array API contains a method to write Dask Arrays to disk using the
    ZARR format, which is a column-store format similar to Parquet. The specifics
    of the file format are irrelevant here—we’re just using ZARR because the Array
    API makes it easy to read and write to that format. We’ll quickly dump the prepared
    data to disk and read it back in for fast access.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经对数据做了很多处理，现在保存我们的进度将是一个合适的时间点。在训练模型之前将数据写入也会加快速度，因为数据已经处于构建模型所需的形状。数组
    API 包含一个方法，可以使用 ZARR 格式将 Dask 数组写入磁盘，ZARR 是类似于 Parquet 的列存储格式。文件格式的具体细节在这里并不重要——我们只是使用
    ZARR，因为数组 API 使读写该格式变得容易。我们将快速将准备好的数据写入磁盘，并读取回来以实现快速访问。
- en: Listing 10.7 Writing the data to ZARR and reading it back in
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.7 将数据写入 ZARR 并读取
- en: '[PRE39]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '[Listing 10.7](#listing10.7) is straightforward—since we’ve already gotten
    the feature array into the shape we want through the concatenating we did in [listing
    10.6](#listing10.6), we just need to save it. We reuse the concatenating code
    on the target array data to follow the same process for the target data. The only
    new item worth pointing out is our decision to rechunk the data. You might have
    noticed after the concatenation, the array had a chunk size of (1,100). This means
    that each chunk contains one row and 100 columns. The ZARR format writes one file
    per chunk, meaning we would produce 568,454 individual files if we didn’t rechunk
    the data. This would be extremely inefficient because of the overhead involved
    with getting data off a disk—this is the case regardless of if we’re running Dask
    in local mode or on a large cluster. Typically, we’d want each chunk to be somewhere
    between 10 MB and 1 GB to minimize the IO overhead. I’ve selected a chunk size
    of 5,000 rows per chunk in this example, so we end up with around 100 files, similar
    to the 100 partitions that the raw data was broken into. We also follow the same
    process of converting the target variable to an array and writing it to disk.
    Now we’re finally ready to build our model!'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.7](#listing10.7) 非常直接——因为我们已经通过在 [列表 10.6](#listing10.6) 中所做的连接操作，将特征数组转换成了我们想要的形状，所以我们只需要保存它。我们重复使用连接代码在目标数组数据上，以对目标数据进行相同的处理过程。唯一值得指出的是我们决定重新分块数据。你可能已经注意到在连接之后，数组有一个块大小为
    (1,100)。这意味着每个块包含一行和 100 列。ZARR 格式为每个块写入一个文件，这意味着如果我们不重新分块数据，我们将产生 568,454 个单独的文件。这会非常低效，因为从磁盘获取数据涉及到的开销很大——无论我们是在本地模式还是在大集群上运行
    Dask，情况都是如此。通常，我们希望每个块的大小在 10 MB 到 1 GB 之间，以最小化 I/O 开销。在这个例子中，我选择了每个块 5,000 行的大小，因此我们最终得到大约
    100 个文件，这与原始数据被分割成的 100 个分区相似。我们还遵循了将目标变量转换为数组并将其写入磁盘的相同过程。现在我们终于准备好构建我们的模型了！'
- en: 10.1.2 Building a logistic regression model with Dask-ML
  id: totrans-265
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1.2 使用 Dask-ML 构建逻辑回归模型
- en: 'We’ll start by using an algorithm built in to Dask-ML’s API: logistic regression.
    Logistic regression is an algorithm that can be used to predict binary (yes or
    no, good or bad, and so forth) outcomes. This perfectly fits our desire to build
    a model to predict the sentiment of a review, because sentiment is discrete: it’s
    either positive or negative. But how can we know how good our model is at predicting
    sentiment? Or, put another way, how can we be sure that our model actually learned
    some useful patterns in the data? To do that, we’ll want to set aside some reviews
    that the algorithm isn’t allowed to look at and learn from. This is called a *holdout
    set* or a *test set*. If the model does a good job predicting the outcomes of
    the holdout set, we can be reasonably confident that the model has actually learned
    useful patterns that generalize to our problem well. Otherwise, if the model does
    not perform well on the holdout set, it’s likely due to the algorithm picking
    up on strong patterns that are unique to the data that it was trained on. This
    is called *overfitting* to the training set and should be avoided. Dask-ML, like
    scikit-learn, has some tools to help randomly select a holdout set that we can
    use for validation. Let’s take a look at how to split the data and build a logistic
    regression model.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先使用 Dask-ML API 内置的算法：逻辑回归。逻辑回归是一种可以用来预测二元（是或否、好或坏等）结果的算法。这完美符合我们构建一个预测评论情感的模型的愿望，因为情感是离散的：它是积极的或消极的。但我们如何知道我们的模型在预测情感方面有多好？或者，换一种说法，我们如何确保我们的模型实际上在数据中学习了有用的模式？为了做到这一点，我们想要留出一部分评论，算法不允许其查看和学习。这被称为
    *保留集* 或 *测试集*。如果模型在预测保留集的结果方面做得很好，我们可以合理地相信模型实际上已经学习了有用的模式，这些模式可以很好地推广到我们的问题上。否则，如果模型在保留集上表现不佳，这很可能是由于算法捕捉到了训练数据中独特的强烈模式。这被称为对训练集的
    *过度拟合*，应该避免。Dask-ML，就像 scikit-learn 一样，有一些工具可以帮助我们随机选择一个保留集，我们可以用它来进行验证。让我们看看如何分割数据并构建一个逻辑回归模型。
- en: Listing 10.8 Building the logistic regression
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.8 构建逻辑回归
- en: '[PRE40]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: In [listing 10.8](#listing10.8), now that we’ve done all the hard work for data
    prep, building the model itself is relatively easy. The `train_test_split` function
    will randomly split off a holdout set for us; then it’s as simple as feeding the
    features (`X`) and targets (`y`) to the `fit` method of the `LogisticRegression`
    object. It’s worth mentioning that we set the `random_state` parameter of the
    `train_test_split` function to 42, and you may be wondering why. The value of
    this parameter doesn’t really matter—what’s most important is that you set it.
    This ensures the data is split the same way every time the `train_test_split`
    function is called on the dataset. This is important when you’re running and comparing
    many models against each other. Because of inherent variability in the data, you
    could, by random chance, test on a very easy or very hard-to-predict holdout set.
    In this case, the improvement (or worsening) of the model you’d witness wouldn’t
    be because you did anything to affect the model. Therefore, we want to make sure
    the data is “randomly” split the same way every time the model is built. After
    a few minutes, the model will be trained and ready to make predictions. Then,
    it’s time to score the model to see how good a job it does predicting reviews
    it hasn’t seen before.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 10.8](#listing10.8)中，现在我们已经完成了所有艰难的数据准备工作，构建模型本身相对简单。`train_test_split`函数会随机为我们分割出一个保留集；然后，只需将特征(`X`)和目标(`y`)输入到`LogisticRegression`对象的`fit`方法即可。值得一提的是，我们将`train_test_split`函数的`random_state`参数设置为42，你可能想知道为什么。这个参数的值并不重要——最重要的是你设置了它。这确保了每次在数据集上调用`train_test_split`函数时，数据都是相同方式分割的。当你运行并比较许多模型时，这一点很重要。由于数据固有的可变性，你可能会随机测试一个非常容易或非常难以预测的保留集。在这种情况下，你所见证的模型改进（或恶化）并不是因为你做了什么来影响模型。因此，我们想确保每次构建模型时数据都是“随机”以相同方式分割的。经过几分钟的训练，模型将准备好进行预测。然后，是时候评分模型，看看它在预测之前未见过的评论方面做得如何。
- en: 10.2 Evaluating and tuning Dask-ML models
  id: totrans-270
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.2 评估和调整 Dask-ML 模型
- en: While it may seem easy to build a model compared to all the hard work we’ve
    done preparing the data, we’re hardly finished. The goal is always to produce
    the most accurate model possible, and you must make a number of considerations
    to achieve that goal. First, there’s the sheer number of algorithms out there.
    For classification alone, there’s logistic regression, support vector machines,
    decision trees, random forests, Bayesian models, and so on. And each of these
    models has several different *hyperparameters* that define things like how sensitive
    the algorithm is to outliers and highly influential points. With many combinations
    of models and parameters, how can we be sure we have the best model we can make?
    The answer is through methodical experimentation. If we have a way to score the
    accuracy of an arbitrary model, finding the best model can be done objectively
    and we can use automation to make the task easier. The highest scoring model is
    the *champion* model until a new *challenger* model comes along and beats it.
    Then, the challenger becomes the new champion and the cycle repeats. This champion-challenger
    model works quite well in practice, but we have to start somewhere. By definition,
    it doesn’t really matter how good or bad the first champion model is—it simply
    serves as a baseline to compare with potential challengers. Therefore, it’s perfectly
    fine to start with a simple model, such as logistic regression, and use all the
    default values.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们为准备数据所付出的艰辛努力相比，构建模型可能看起来很简单，但我们远未完成。目标始终是产生尽可能精确的模型，为此你必须考虑许多因素。首先，算法的数量是巨大的。仅就分类而言，就有逻辑回归、支持向量机、决策树、随机森林、贝叶斯模型等等。而且，这些模型中的每一个都有几个不同的*超参数*，它们定义了算法对异常值和高度影响点的敏感度。在许多模型和参数的组合中，我们如何确保我们拥有最好的模型呢？答案是通过对任意模型的准确性进行系统性的实验。如果我们有一种方法来评分任意模型的准确性，那么找到最佳模型就可以客观地进行，并且我们可以使用自动化来简化任务。得分最高的模型是*冠军*模型，直到一个新的*挑战者*模型出现并击败它。然后，挑战者成为新的冠军，循环重复。这种冠军-挑战者模型在实践中效果很好，但我们必须从某个地方开始。根据定义，第一个冠军模型的优劣并不重要——它仅仅作为一个基准，用于与潜在的挑战者进行比较。因此，从简单的模型开始，例如逻辑回归，并使用所有默认值是完全可行的。
- en: 10.2.1 Evaluating Dask-ML models with the score method
  id: totrans-272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.1 使用评分方法评估 Dask-ML 模型
- en: Once we’ve established a baseline, we can pit it against more sophisticated
    models that use different algorithms or different sets of hyperparameters. Fortunately,
    every scikit-learn algorithm, and by extension every Dask-ML algorithm, comes
    with a `score` method. The `score` method calculates a widely accepted scoring
    metric based on the type of algorithm. For example, classification algorithms
    calculate the classification accuracy score when the `score` method is called.
    This score represents the percent of correct classification predictions, and ranges
    between 0 and 1 with a higher score being more accurate. Some data scientists
    prefer to use other scores such as the F1 accuracy score, but the pros and cons
    of each scoring method are irrelevant for this exercise. You should always choose
    the scoring metric that best aligns to the needs of your solution, and it’s a
    very good idea to learn about the different scoring methods out there. Since we’ve
    already trained a baseline logistic regression, let’s take a look at how well
    it performs.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们建立了一个基线，我们就可以将其与使用不同算法或不同超参数集的更复杂的模型进行对比。幸运的是，每个 scikit-learn 算法，以及通过扩展每个
    Dask-ML 算法，都附带了一个 `score` 方法。`score` 方法根据算法类型计算一个广泛接受的评分指标。例如，当调用 `score` 方法时，分类算法计算分类准确度评分。这个评分表示正确分类预测的百分比，范围在
    0 到 1 之间，分数越高表示越准确。一些数据科学家更喜欢使用其他评分，如 F1 准确度评分，但每种评分方法的优缺点对于这个练习来说并不重要。你应该始终选择与你的解决方案需求最匹配的评分指标，并且了解不同的评分方法是非常好的主意。既然我们已经训练了一个基线逻辑回归模型，让我们看看它的表现如何。
- en: Listing 10.9 Scoring the logistic regression model
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.9 评分逻辑回归模型
- en: '[PRE41]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: As you can see the code to score a model is very simple. Where we passed the
    train versions of `X` and `y` to the `fit` method, we pass the test versions of
    `X` and `y` to the `score` method. In one line, this will generate predictions
    using the features contained in `X_test` and compare the predictions to the actual
    values contained in `y_test`. Our baseline model correctly classified 79.6% of
    the reviews in the test set. Not a bad start! And now that we have a baseline,
    we can set off to try to beat it with a challenger model. As you work, keep in
    mind that a perfect classification score is not likely to be achievable. Our goal
    here isn’t to find a model that’s 100% perfect, but to make deliberate, measurable
    progress and use objective criteria to find the best model we can within the constraints
    of time, data quality, and so on.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，评分模型的代码非常简单。在我们将 `X` 和 `y` 的训练版本传递给 `fit` 方法时，我们将 `X` 和 `y` 的测试版本传递给 `score`
    方法。一行代码就可以生成使用 `X_test` 中包含的特征的预测，并将预测与 `y_test` 中包含的实际值进行比较。我们的基线模型正确分类了测试集中
    79.6% 的评论。这是一个不错的开始！现在，既然我们已经有了基线，我们可以尝试使用挑战者模型来超越它。在您的工作过程中，请记住，完美的分类评分不太可能实现。我们在这里的目标不是找到一个
    100% 完美的模型，而是要做出有意识、可衡量的进步，并使用客观标准在时间、数据质量等约束条件下找到最好的模型。
- en: 10.2.2 Building a naïve Bayes classifier with Dask-ML
  id: totrans-277
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.2 使用 Dask-ML 构建朴素贝叶斯分类器
- en: 'Let’s see how our logistic regression model fares against a naïve Bayes classifier.
    Naïve Bayes is a commonly used algorithm in text classification because it’s a
    simple algorithm and has reasonably good predictive power even with small datasets.
    There’s just one problem here: there is no naïve Bayes classifier class in the
    Dask-ML API. However, we can still use Dask to train a naïve Bayes classifier!
    No, we’re not going to build the algorithm from scratch. Instead, we can use one
    of Dask-ML’s interfaces to scikit-learn called the Incremental wrapper. The Incremental
    wrapper allows us to use any scikit-learn algorithm with Dask so long as the algorithm
    implements the `partial_fit` interface.'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们的逻辑回归模型与朴素贝叶斯分类器的表现如何。朴素贝叶斯是文本分类中常用的算法，因为它是一个简单的算法，即使在小型数据集上也有相当好的预测能力。这里只有一个问题：在
    Dask-ML API 中没有朴素贝叶斯分类器类。然而，我们仍然可以使用 Dask 来训练一个朴素贝叶斯分类器！不，我们不是从头开始构建算法。相反，我们可以使用
    Dask-ML 的一个接口，即 scikit-learn 的增量包装器。增量包装器允许我们只要算法实现了 `partial_fit` 接口，就可以使用 Dask
    来使用任何 scikit-learn 算法。
- en: A growing number of scikit-learn algorithms are supporting this interface because
    there’s been a growing interest in batch learning for large datasets. The naïve
    Bayes algorithms fall into the group of algorithms that support batch learning,
    so they can easily be used with Dask to parallelize training. Let’s see what this
    looks like.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 越来越多的 scikit-learn 算法支持这个接口，因为对大型数据集的批量学习越来越感兴趣。朴素贝叶斯算法属于支持批量学习的算法组，因此它们可以很容易地与
    Dask 一起使用以并行化训练。让我们看看这会是什么样子。
- en: Listing 10.10 Training a naïve Bayes classifier with the Incremental wrapper
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.10 使用 Incremental 包装器训练朴素贝叶斯分类器
- en: '[PRE42]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'The code in [listing 10.10](#listing10.10) largely looks the same as the code
    in [listing 10.8](#listing10.8), except this time we’ve imported an algorithm
    from scikit-learn rather than Dask-ML. To use the algorithm with Dask, all we
    have to do is create the estimator object as normal, then wrap it in the Incremental
    wrapper. The Incremental wrapper is essentially a helper function that tells Dask
    about the estimator object so it can pass it off to the workers for training.
    Fitting the model continues as normal, with one key exception: we have to specify
    the valid target classes up front. Since we have only two potential outcomes in
    our dataset, positive and negative, which we’ve encoded as 1 and 0, respectively,
    we simply pass them in a list here. The code should take only a few seconds to
    run; then we can score the model to see if it beats the logistic regression.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 10.10](#listing10.10) 中的代码在很大程度上与 [列表 10.8](#listing10.8) 中的代码相同，除了这次我们导入了一个来自
    scikit-learn 的算法而不是 Dask-ML。要使用 Dask 的算法，我们只需像平常一样创建估算器对象，然后将其包装在 Incremental
    包装器中。Incremental 包装器本质上是一个辅助函数，它告诉 Dask 估算器对象，以便它可以将其传递给工作节点进行训练。模型的拟合过程与平常一样，只有一个关键例外：我们必须提前指定有效的目标类别。由于我们的数据集中只有两种可能的输出，即正例和负例，分别编码为
    1 和 0，我们只需将它们作为一个列表传递。代码应该只需几秒钟就能运行；然后我们可以评分模型，看看它是否优于逻辑回归。'
- en: Listing 10.11 Scoring the Incremental wrapped model
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.11 评分 Incremental 包装的模型
- en: '[PRE43]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'Curiously, unlike the `score` method for the Dask-ML algorithm, the `score`
    method for Incremental is not lazy, so we don’t need a call to `compute`. It looks
    like the naïve Bayes model performs similar to logistic regression model, but
    its score is about 1% worse than the logistic regression—meaning the logistic
    regression is still our champion for now. We should continue trying other algorithms
    as potential challengers, but we will leave that for now to talk about the other
    element we should experiment with: hyperparameter tuning.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 奇怪的是，与 Dask-ML 算法的 `score` 方法不同，Incremental 的 `score` 方法不是惰性的，所以我们不需要调用 `compute`。看起来朴素贝叶斯模型的表现与逻辑回归模型相似，但其评分大约比逻辑回归差
    1%——这意味着逻辑回归目前仍然是我们的冠军。我们应该继续尝试其他算法作为潜在的挑战者，但我们将暂时留出时间来讨论我们应该实验的其他元素：超参数调整。
- en: 10.2.3 Automatically tuning hyperparameters
  id: totrans-286
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2.3 自动调整超参数
- en: As stated previously, most algorithms have a few hyperparameters that control
    how the algorithm behaves. While the default values are typically selected by
    the algorithm’s author to provide the best general performance, it’s usually possible
    to eke out some additional accuracy gains by tuning the hyperparameters to fit
    the training data better. Tuning hyperparameters manually can be a highly repetitive
    and monotonous process, but thanks to the scikit-learn and Dask-ML APIs, we can
    automate a lot of that work. For example, we might want to evaluate the effect
    that changing a couple of the hyperparameters for logistic regression will have
    on the results. Using a “meta-estimator” called `GridSearchCV`, we can instruct
    Dask-ML to try many different combinations of hyperparameters and automatically
    pit the models against each other in champion-challenger fashion. It’s quite easy
    to use, as you’ll see in the next code listing.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，大多数算法都有一些超参数，这些参数控制着算法的行为。虽然默认值通常由算法的作者选择以提供最佳的一般性能，但通常可以通过调整超参数以更好地适应训练数据来获得一些额外的准确度提升。手动调整超参数可能是一个高度重复和单调的过程，但多亏了
    scikit-learn 和 Dask-ML API，我们可以自动化很多这样的工作。例如，我们可能想评估改变逻辑回归中几个超参数对结果的影响。使用名为 `GridSearchCV`
    的“元估算器”，我们可以指示 Dask-ML 尝试许多不同的超参数组合，并以冠军挑战者的方式自动将模型相互对抗。使用起来非常简单，您将在下一代码示例中看到。
- en: Listing 10.12 Using `GridSearchCV` to tune hyperparameters
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.12 使用 `GridSearchCV` 调整超参数
- en: '[PRE44]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The `GridSearchCV` object behaves like the Incremental wrapper. Just as before,
    we can take any algorithm, such as Dask-ML’s logistic regression, and wrap it
    in `GridSearchCV`. The other element we need is a dictionary containing the parameters
    we want the grid search to try and the list of possible values to try. As can
    be seen in [listing 10.12](#listing10.12), we’re having the grid search change
    two parameters: the `penalty` parameter and the C coefficient. The value associated
    with each parameter name is a list of the values to try.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '`GridSearchCV` 对象的行为类似于增量包装器。就像之前一样，我们可以取任何算法，例如 Dask-ML 的逻辑回归，并将其包装在 `GridSearchCV`
    中。我们还需要另一个元素，即包含我们想要网格搜索尝试的参数及其可能值的字典。正如[列表 10.12](#listing10.12)所示，我们正在让网格搜索改变两个参数：`penalty`
    参数和 C 系数。与每个参数名称关联的值是要尝试的值的列表。'
- en: You can include any parameter in the grid search by including it in the dictionary.
    scikit-learn’s API documentation lists all the parameters and example values for
    each algorithm, so you can use that as a reference for choosing parameters to
    tune. It’s important to note that `GridSearchCV` is a “brute force” type algorithm—meaning
    it will try all combinations of parameters you pass into it. In [listing 10.12](#listing10.12),
    we provided two choices for the penalty parameter and three choices for the C
    coefficient. This means six models in total will be built, each representing a
    different combination of parameters. Be careful not to choose a search space that’s
    too large, or the time it will take the grid search to complete may become prohibitively
    long. However, `GridSearchCV` scales quite well with Dask. Each model can be built
    on a separate worker, meaning it’s not at all hard to cut down on grid search
    time by deploying to a cluster and/or scaling up the number of workers. Once the
    grid search is complete, you can see a report of what happened for each model,
    including its test scores and the time it took to train it. To see the results,
    we’ll run the following code.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在字典中包含任何参数来将其包括在网格搜索中。scikit-learn 的 API 文档列出了每个算法的所有参数及其示例值，因此您可以使用它作为选择要调整的参数的参考。需要注意的是，`GridSearchCV`
    是一种“暴力”类型的算法——这意味着它将尝试您传递给它的所有参数组合。在[列表 10.12](#listing10.12)中，我们提供了两个惩罚参数的选择和三个
    C 系数的选项。这意味着总共将构建六个模型，每个模型代表不同的参数组合。请小心不要选择太大的搜索空间，否则网格搜索完成所需的时间可能会变得过长。然而，`GridSearchCV`
    与 Dask 的扩展性相当好。每个模型都可以在单独的工人上构建，这意味着通过部署到集群和/或增加工人的数量来减少网格搜索时间并不困难。一旦网格搜索完成，您可以看到每个模型的报告，包括其测试分数和训练所需的时间。要查看结果，我们将运行以下代码。
- en: Listing 10.13 Seeing the results of the `GridSearchCV`
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.13 查看 `GridSearchCV` 的结果
- en: '[PRE45]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: The completed `GridSearchCV` object has an attribute called `cv_results_`, which
    is a dictionary of test metrics. It’s much easier to read when displayed in a
    Pandas DataFrame, so we’ll put it in a DataFrame and print the results. It should
    look like [figure 10.6](#figure10.6).
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 完成的 `GridSearchCV` 对象有一个名为 `cv_results_` 的属性，它是一个测试指标的字典。当以 Pandas DataFrame
    的形式显示时，它更容易阅读，因此我们将它放入 DataFrame 并打印结果。它应该看起来像[图 10.6](#figure10.6)。
- en: '![c10_06.png](Images/c10_06.png)'
  id: totrans-295
  prefs: []
  type: TYPE_IMG
  zh: '![c10_06.png](Images/c10_06.png)'
- en: '[Figure 10.6](#figureanchor10.6) The `GridSearchCV` results'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 10.6](#figureanchor10.6) `GridSearchCV` 的结果'
- en: In [figure 10.6](#figure10.6) we get a whole bunch of metrics about what happened
    during the `GridSearchCV` process. The four columns of greatest interest are the
    test score columns. This shows how well each model performed on different splits
    of the data. Interestingly, the model with a C coefficient of `1` and a penalty
    of `L2` performed the best (tied with several other C coefficient combinations).
    These happen to be the default values for logistic regression, so in this case
    hyperparameter tuning didn’t find a modification of the model that performed better
    than the baseline. If you’re curious about the default values of each algorithm,
    they can be found in the scikit-learn documentation. The defaults generally do
    a good job of fitting in most cases, but it never hurts to check if hyperparameter
    tuning can eke out any improvement. To make our search exhaustive, we’d want to
    run hyperparameter tuning over each different algorithm we try as well (such as
    naïve Bayes). With the techniques and code snippets we covered in this section,
    you should be able to build an automated pipeline that will try many combinations
    of algorithms and hyperparameters! Another thing to keep in mind is that we can
    use the champion-challenger model to evaluate the value of new data and features.
    For example, if we increased our corpus from 100 words to 150 words, how much
    more accurate does the model get? With objective experimentation, we can answer
    all those questions and end up with the best model we can produce. However, at
    the moment, our original logistic regression does the best job of predicting the
    sentiment of a review. We should expect to be able to feed our model new reviews
    it hasn’t seen before and, on average, correctly predict if they were positive
    or negative experiences with about 80% accuracy.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 10.6](#figure10.6)中，我们可以看到`GridSearchCV`过程中发生的一系列指标。其中最感兴趣的四个列是测试分数列。这显示了每个模型在不同数据分割上的表现情况。有趣的是，具有`1`的C系数和`L2`惩罚的模型表现最佳（与几个其他C系数组合并列）。这些恰好是逻辑回归的默认值，因此在这种情况下，超参数调整并没有找到比基线模型表现更好的模型修改。如果你对每个算法的默认值感兴趣，可以在scikit-learn文档中找到。默认值通常在大多数情况下都能很好地拟合，但检查超参数调整是否能挤出任何改进也从未有过害处。为了使我们的搜索全面，我们希望对每个尝试的不同算法（如朴素贝叶斯）也进行超参数调整。通过本节中介绍的技术和代码片段，你应该能够构建一个自动化的管道，尝试许多算法和超参数的组合！另外，需要注意的是，我们可以使用冠军挑战者模型来评估新数据和特征的价值。例如，如果我们把我们的语料库从100个单词增加到150个单词，模型会提高多少准确性？通过目标实验，我们可以回答所有这些问题，并最终得到我们能够生产的最佳模型。然而，目前，我们的原始逻辑回归在预测评论情感方面做得最好。我们应该期待能够向我们的模型提供之前未见过的新的评论，并且平均来说，能够以大约80%的准确性正确预测它们是正面还是负面体验。
- en: 10.3 Persisting Dask-ML models
  id: totrans-298
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 10.3 持久化 Dask-ML 模型
- en: The last thing we’ll briefly touch on in this chapter is persisting a trained
    Dask-ML model so it can be published or deployed somewhere else. For many data
    science projects, the resulting model, such as our classification model, is intended
    to be used somewhere in an application to make predictions or recommendations.
    Although it can take an immense amount of computing power to produce a model,
    it generally takes much less power to produce predictions that can be surfaced
    in a user-facing application.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们将简要讨论的最后一件事是持久化训练好的Dask-ML模型，以便它可以被发布或部署到其他地方。对于许多数据科学项目来说，产生的模型，例如我们的分类模型，旨在在应用程序的某个地方使用，以进行预测或推荐。尽管产生模型可能需要巨大的计算能力，但产生可以在用户界面应用程序中展示的预测通常需要较少的计算能力。
- en: Many data science workflows consist of spinning up a large and powerful cluster
    to churn through data and produce a model, publish the model to a repository such
    as Amazon S3, shut down the cluster to save costs, and expose the model via inexpensive,
    less-powerful machines such as web servers. This makes sense, because the size
    of a predictive model tends to be only a few kilobytes to a few megabytes at most,
    compared to the terabytes or petabytes of training data that might be used to
    train the model. We can use a binary serialization library to help us take what
    we’ve learned from all the data—which is the resulting predictive model—and persist
    it to disk so it can be reused without having to rebuild from scratch the next
    time we want to use it.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 许多数据科学工作流程包括启动一个大型且强大的集群来处理数据并生成模型，将模型发布到如 Amazon S3 这样的存储库，关闭集群以节省成本，并通过成本较低、性能较弱的机器（如
    Web 服务器）公开模型。这很有道理，因为预测模型的大小通常只有几千字节到几兆字节，而用于训练模型的训练数据可能达到千兆字节或更多。我们可以使用二进制序列化库来帮助我们保存从所有数据中学到的知识——即生成的预测模型——并将其持久化到磁盘，以便下次使用时无需从头开始重建。
- en: Python has a built-in binary serialization library, called `pickle`, which allows
    us to take any Python object in memory and save it to disk. Deserializing that
    object later by reading it off disk and loading it back into memory can recreate
    the object faithfully to the state it was in when it was saved to disk. This also
    means that a Python object can be created on one machine, serialized, transmitted
    over the network, and deserialized and consumed by a different machine. In fact,
    that’s how Dask sends data and tasks to different worker nodes in a cluster!
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Python 内置了一个名为 `pickle` 的二进制序列化库，它允许我们将内存中的任何 Python 对象保存到磁盘。稍后通过从磁盘读取并重新加载到内存中反序列化该对象，可以忠实地重建对象，使其状态与保存到磁盘时的状态一致。这也意味着一个
    Python 对象可以在一台机器上创建，序列化，通过网络传输，然后由另一台机器反序列化和使用。实际上，这正是 Dask 在集群中向不同的工作节点发送数据和任务的方式！
- en: Fortunately, this process is also very easy. The only requirement is that the
    machine that loads the serialized object must have all the Python libraries used
    by that object. For example, if we serialized a Dask DataFrame, we couldn’t load
    it on a machine that didn’t have Dask installed; we would get an ImportError on
    trying to load the file. Other than that, it’s very simple. For this example,
    we’ll use a library called `dill`, which is a wrapper around the `pickle` library.
    Dill has better support for complex data structures such as JSON and nested dictionaries,
    whereas Python’s built-in `pickle` library sometimes has issues. To write one
    of our models to disk is very simple. For example, here’s how to write the naïve
    Bayes classifier to disk.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个过程也非常简单。唯一的要求是加载序列化对象的机器必须拥有该对象使用的所有 Python 库。例如，如果我们序列化了一个 Dask DataFrame，我们就不能在未安装
    Dask 的机器上加载它；尝试加载文件时会遇到 ImportError。除此之外，它非常简单。对于这个例子，我们将使用一个名为 `dill` 的库，它是 `pickle`
    库的包装器。Dill 对复杂数据结构（如 JSON 和嵌套字典）的支持更好，而 Python 内置的 `pickle` 库有时会有问题。将我们的模型写入磁盘非常简单。例如，下面是如何将朴素贝叶斯分类器写入磁盘的方法。
- en: Listing 10.14 Writing the naïve Bayes classifier to disk
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.14 将朴素贝叶斯分类器写入磁盘
- en: '[PRE46]'
  id: totrans-304
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: That’s all there is to it. The `dump` function serializes the object you pass
    to it and writes it to the file handle you specify. Here we’ve opened a handle
    to a new file called naïve_bayes_model.pkl, so the data will be written into that
    file. Because pickle files are binary files, we need to always read and write
    with the `b` flag in the file handle to denote that the file should be opened
    in binary mode. To read in a model file is also very simple.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是全部内容。`dump` 函数将你传递给它的对象序列化，并将其写入你指定的文件句柄。在这里，我们打开了一个名为 naïve_bayes_model.pkl
    的新文件的句柄，因此数据将被写入该文件。由于 pickle 文件是二进制文件，我们需要始终在文件句柄中使用 `b` 标志来表示文件应以二进制模式打开。读取模型文件也非常简单。
- en: Listing 10.15 Reading the naïve Bayes classifier from disk
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 10.15 从磁盘读取朴素贝叶斯分类器
- en: '[PRE47]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: As can be seen in [listing 10.15](#listing10.15), we simply read the file in
    using the `load` function. We don’t have to have any of the data we used to train
    the model around—the model object is completely self-contained. To demonstrate
    its ability to produce predictions, we’ve fed in some dummy data by generating
    some random binary vectors. As expected, we get an array of predictions.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [列表 10.15](#listing10.15) 所示，我们只需使用 `load` 函数读取文件。我们不需要使用训练模型时使用的任何数据——模型对象是完全自包含的。为了展示其生成预测的能力，我们通过生成一些随机的二进制向量输入了一些虚拟数据。不出所料，我们得到了一个预测数组。
- en: I hope you now have an appreciation for how easy it is to use Dask-ML once you’ve
    put in the hard work to prepare the data! In the next chapter, we’ll finish our
    journey by exploring how to use Dask in cluster mode and how to deploy Dask on
    the cloud using AWS.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望您现在已经对使用 Dask-ML 的便捷性有了认识，一旦您投入了艰苦的工作来准备数据！在下一章中，我们将通过探索如何在集群模式下使用 Dask 以及如何使用
    AWS 部署 Dask 来结束我们的旅程。
- en: Summary
  id: totrans-310
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Binary vectorization is used to relate the existence of a word in a chunk of
    text to some predictor (for example, sentiment).
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二进制向量化用于将文本块中单词的存在与某些预测器（例如，情感）相关联。
- en: Machine learning uses statistical and mathematical methods to find patterns
    that relate features (inputs) to predictors (outputs).
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习使用统计和数学方法来寻找与特征（输入）相关的预测器（输出）的模式。
- en: Data should be split into training and testing sets to avoid overfitting.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应该将数据分成训练集和测试集，以避免过拟合。
- en: When trying to decide which model to use, select some error metrics and use
    the champion-challenger approach to objectively find the best model based on your
    selected metrics.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当试图决定使用哪种模型时，选择一些错误度量，并使用冠军挑战者方法根据您选择的度量客观地找到最佳模型。
- en: You can use `GridSearchCV` to automate the selection and tuning processes of
    your machine learning models.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以使用 `GridSearchCV` 自动化选择和调整您的机器学习模型的流程。
- en: Trained machine learning models can be saved using the dill library to reuse
    later to generate predictions.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练好的机器学习模型可以使用 dill 库保存，以便稍后用于生成预测。
- en: '11'
  id: totrans-317
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '11'
- en: Scaling and deploying Dask
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 缩放和部署 Dask
- en: '**This chapter covers**'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Creating a Dask Distributed cluster on Amazon AWS using Docker and Elastic Container
    Service
  id: totrans-320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Docker 和弹性容器服务在 Amazon AWS 上创建 Dask 分布式集群
- en: Using a Jupyter Notebook server and Elastic File System to store and access
    data science notebooks and shared datasets in Amazon AWS
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jupyter Notebook 服务器和弹性文件系统在 Amazon AWS 中存储和访问数据科学笔记本和共享数据集
- en: Using the Distributed client object to submit jobs to a Dask cluster
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布式客户端对象将作业提交到 Dask 集群
- en: Monitoring execution of jobs on the cluster using the Distributed monitoring
    dashboard
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布式监控仪表板监控集群上作业的执行
- en: Up to this point, we’ve been working with Dask in *local mode*. This means that
    everything we’ve asked Dask to do has all been executed on a single computer.
    Running Dask in local mode is very useful for prototyping, development, and ad-hoc
    exploration, but we can still quickly reach the performance limits of a single
    computer. Just as our hypothetical chef in chapter 1 needed to call in reinforcements
    to get her kitchen prepped in time for dinner service, we too can configure Dask
    to spread the work out across many computers to process large jobs more quickly.
    This becomes especially important in production systems when time constraints
    apply. Therefore, it’s typical to scale out and run Dask in *cluster mode* in
    production.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在本地模式下使用 Dask。这意味着我们要求 Dask 执行的所有操作都是在单个计算机上完成的。在本地模式下运行 Dask 对于原型设计、开发和临时探索非常有用，但我们仍然可以迅速达到单台计算机的性能限制。正如第一章中我们假设的厨师需要召集援兵以便在晚餐服务前准备好厨房一样，我们也可以配置
    Dask 将工作分散到多台计算机上以更快地处理大型作业。这在时间受限的生产系统中尤为重要。因此，在生产中通常会将 Dask 扩展到集群模式运行。
- en: '![c11_01.eps](Images/c11_01.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![c11_01.eps](Images/c11_01.png)'
- en: '[Figure 11.1](#figureanchor11.1) This chapter will cover the last elements
    of the workflow: deployment and monitoring.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.1](#figureanchor11.1) 本章将涵盖工作流程的最后几个元素：部署和监控。'
- en: '[Figure 11.1](#figure11.1) shows that we’ve arrived at the final part of our
    workflow: deployment and monitoring. Although it’s always good to be planning
    ahead when designing a solution, it’s quite rare for the final version to closely
    represent how it was originally envisioned. This is why deployment and monitoring
    have been put last in our workflow. Once you have a good idea of what data is
    necessary to solve a problem, the volume of that data, and how quickly you or
    your applications must provide an answer to the problem, you can begin to plan
    out what resources will be needed to host your final solution. These considerations
    are typically settled during prototyping of the solution. Fortunately, Dask is
    intended to make the transition from a prototype on a laptop to a full-scale application
    on a cluster as seamless as possible. In fact, whether the scheduler is running
    in local mode or cluster mode is transparent to everything else. This means that
    any Dask code you write—and by association all the code we’ve covered in the past
    10 chapters—will run both on your laptop and on any size cluster without modifications.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.1](#figure11.1)显示我们已经到达了工作流程的最后一部分：部署和监控。虽然在设计解决方案时提前规划总是好的，但最终版本与最初设想非常接近的情况相当罕见。这就是为什么部署和监控在我们的工作流程中排在最后。一旦你对解决一个问题的必要数据有了很好的了解，数据的量，以及你或你的应用程序必须多快提供问题的答案，你就可以开始规划你需要哪些资源来托管你的最终解决方案。这些考虑通常在解决方案的原型设计阶段确定。幸运的是，Dask旨在使从笔记本电脑上的原型到集群上的完整应用程序的过渡尽可能无缝。事实上，无论调度器是在本地模式还是集群模式下运行，对其他所有内容都是透明的。这意味着你写的任何Dask代码——以及由此关联的过去10章中我们涵盖的所有代码——都可以在笔记本电脑和任何大小的集群上运行，无需修改。'
- en: That said, it’s still worth seeing firsthand how to set up, maintain, and monitor
    a Dask cluster. In this chapter, we’ll walk through how to set up a cluster using
    Amazon AWS that you can use as a private sandbox. AWS was chosen for this exercise
    because it is a very popular cloud computing platform with a supportive community,
    lots of learning resources, and a generous account tier that allows you to experiment
    with AWS free of charge. Dask is just as suited to run on other cloud computing
    platforms such as Microsoft Azure or Google Cloud Platform, and, of course, can
    also be run on private server farms. While this exercise will also provide some
    good hands-on experience with elements of AWS and Docker, the focus will primarily
    be on Dask. We’ll cover only the essentials of AWS and Docker that you need to
    know to get the cluster up and running. We’ll also look at a few general troubleshooting
    steps you can take if you run into issues with AWS and Docker. However, they are
    both vast subjects in their own right, and it wouldn’t be useful to cover them
    in depth here.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，亲眼看到如何设置、维护和监控一个Dask集群仍然很有价值。在本章中，我们将指导您如何使用Amazon AWS设置一个集群，您可以用它作为私有沙盒。选择AWS进行这项练习是因为它是一个非常受欢迎的云计算平台，拥有支持性的社区、大量的学习资源和慷慨的账户层级，允许您免费实验AWS。Dask同样适合在Microsoft
    Azure或Google Cloud Platform等其他云计算平台上运行，当然，也可以在私有服务器农场上运行。虽然这项练习也将提供一些关于AWS和Docker的好手头经验，但重点将主要放在Dask上。我们只会涵盖您需要知道的基本AWS和Docker知识，以便将集群启动并运行。我们还将查看一些通用的故障排除步骤，如果您在AWS和Docker遇到问题，可以采取这些步骤。然而，它们各自都是庞大的主题，在这里深入探讨它们并不实用。
- en: 11.1 Building a Dask cluster on Amazon AWS with Docker
  id: totrans-329
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 在Amazon AWS上使用Docker构建Dask集群
- en: Before we get started, we’ll cover some basic terminology and have a look at
    the architecture we’re going to be creating in AWS.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始之前，我们将介绍一些基本术语，并查看我们将在AWS中创建的架构。
- en: '![c11_02.eps](Images/c11_02.png)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![c11_02.eps](Images/c11_02.png)'
- en: '[Figure 11.2](#figureanchor11.2) An architecture diagram of our Dask Distributed
    cluster'
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.2](#figureanchor11.2) 我们Dask分布式集群的架构图'
- en: 'As you can see in [figure 11.2](#figure11.2), the system has four distinct
    elements: the client, the notebook server, the scheduler, and the workers. The
    four elements each serve different roles:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图11.2](#figure11.2)中看到的，该系统有四个不同的元素：客户端、笔记本服务器、调度器和工作者。这四个元素各自扮演不同的角色：
- en: '*Scheduler*—Receives jobs from the client by way of the notebook server, divides
    up the work to be done, and coordinates the workers to complete the job'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*调度器*——通过笔记本服务器从客户端接收作业，将待做的工作分割，并协调工作者完成作业'
- en: '*Worker*—Receives tasks from the scheduler and computes them'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*工作者*——从调度器接收任务并计算它们'
- en: '*Jupyter Notebook server*—Provides a frontend to allow the user to run code
    and submit jobs to the cluster'
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Jupyter Notebook服务器*——提供一个前端，允许用户运行代码并将作业提交到集群'
- en: '*Client*—Displays the results to the user'
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*客户端*——向用户显示结果'
- en: 11.1.1 Getting started
  id: totrans-338
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.1 开始
- en: 'In local mode, all elements run on the same computer, and the number of workers
    corresponds with the number of CPU cores by default. In cluster mode, we’ll configure
    each of the elements to run on a separate computer. We’ll also gain the freedom
    to increase or decrease the number of workers on the fly, which gives us the flexibility
    to scale the cluster as needed. But, since all these elements will reside on different
    computers, we must consider a few new things: the data must be in a shared place
    that can be accessed by all the workers, and all the workers must have the right
    Python packages installed. The first consideration is straightforward—we’ll set
    up a shared filesystem that each of the workers can access, and then put the data
    there. The second consideration has, historically, been a bit trickier to handle.
    For instance, if we want to run the code from chapter 9 that filters out all of
    the stopwords using NLTK (Natural Language Toolkit) on a cluster, we have to make
    sure that every worker in the cluster has NLTK installed and also has the stopword
    data downloaded. This might not be a problem to do manually if we have a handful
    of workers in the cluster, but if we needed to scale the cluster to 10,000 workers,
    it would take a very long time to configure the workers one at a time. This is
    where Docker becomes incredibly useful. Docker essentially allows us to create
    a blueprint, called an *image*, that contains data and instructions to build an
    identical copy of a system. The image can be launched inside a *container* and
    becomes a fully functional self-contained system, much like a virtual machine.
    This image can be deployed to Amazon Elastic Container Service (ECS) to launch
    many hundreds or thousands of workers, all with the same identical configuration
    and software, at the press of a button. Later in the chapter, we’ll build a Docker
    image that contains all the software we need to run the Dask worker alongside
    all the necessary Python packages. We’ll also do the same for the scheduler and
    the notebook server. Our overall objective for this section is encompassed in
    this scenario:'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地模式下，所有元素都在同一台计算机上运行，默认情况下工作进程的数量与CPU核心数相对应。在集群模式下，我们将配置每个元素在单独的计算机上运行。我们还将获得动态增加或减少工作进程数量的自由，这使我们能够根据需要扩展集群。但是，由于所有这些元素都将驻留在不同的计算机上，我们必须考虑一些新事物：数据必须位于一个所有工作进程都可以访问的共享位置，并且所有工作进程都必须安装正确的Python包。第一个考虑因素很简单——我们将设置一个共享文件系统，每个工作进程都可以访问，然后将数据放在那里。第二个考虑因素在历史上处理起来有点棘手。例如，如果我们想在集群上运行第9章中的代码，该代码使用NLTK（自然语言工具包）过滤掉所有停用词，我们必须确保集群中的每个工作进程都已安装NLTK，并且已下载停用词数据。如果我们集群中的工作进程数量不多，手动操作可能不成问题，但如果我们需要将集群扩展到10,000个工作进程，逐个配置工作进程将花费非常长的时间。这正是Docker变得极其有用的地方。Docker本质上允许我们创建一个蓝图，称为*镜像*，它包含数据和构建一个系统相同副本的指令。这个镜像可以在*容器*内部启动，成为一个完全功能化的自包含系统，就像虚拟机一样。这个镜像可以部署到Amazon
    Elastic Container Service (ECS)以启动数百或数千个工作进程，只需按一下按钮，所有工作进程都具有相同的配置和软件。在本章的后面部分，我们将构建一个包含运行Dask工作进程所需的所有软件以及所有必要的Python包的Docker镜像。我们还将为调度器和笔记本服务器做同样的事情。本节的整体目标包含在这个场景中：
- en: Set up an Amazon AWS environment with eight Elastic Container Service instances
    and deploy a Dask cluster using the prebuilt Dask Docker images.
  id: totrans-340
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 设置一个包含八个弹性容器服务实例的Amazon AWS环境，并使用预构建的Dask Docker镜像部署Dask集群。
- en: 'To follow along with the examples, you will need to download and install the
    latest copy of Docker on your machine from [www.docker.com/get-started](http://www.docker.com/get-started).
    This will allow you to build the images that we will deploy to Amazon ECS. You
    will also need to create an AWS account by following the instructions at [https://aws.amazon.com/free](https://aws.amazon.com/free).
    Please note that this exercise was designed to stay within the limits of the AWS
    free tier; however, Amazon requires you to provide payment information up front
    to activate your account, and you must follow the cleanup instructions at the
    end of the exercise to avoid account charges. In the event you do exceed the limits
    of AWS Free Tier, the resources we’re using are very inexpensive, so the costs
    you would incur are minimal. You’ll also need an SSH client. If you’re using macOS
    or a Unix/Linux-based OS, an SSH client should be preinstalled on your system.
    However, if you’re running Windows, you will need to download an SSH client such
    as PuTTY ([https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html)).
    Finally, install the AWS Command Line Interface (CLI) tools by following the directions
    at [https://aws.amazon.com/cli](https://aws.amazon.com/cli). Once you’re finished
    getting set up, we’ll follow a seven-step process to set up the cluster:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随示例进行操作，您需要在您的机器上下载并安装Docker的最新版本，请访问[www.docker.com/get-started](http://www.docker.com/get-started)。这将允许您构建我们将部署到Amazon
    ECS的镜像。您还需要根据[https://aws.amazon.com/free](https://aws.amazon.com/free)上的说明创建一个AWS账户。请注意，这项练习旨在保持在AWS免费层级的限制内；然而，亚马逊要求您在激活账户之前提供支付信息，并且您必须遵循练习结束时的清理说明，以避免账户费用。如果您超出AWS免费层级的限制，我们使用的资源非常便宜，因此您将承担的费用非常低。您还需要一个SSH客户端。如果您使用的是macOS或基于Unix/Linux的操作系统，SSH客户端应该已经预安装在您的系统上。但是，如果您使用的是Windows，您将需要下载一个SSH客户端，例如PuTTY([https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html))。最后，按照[https://aws.amazon.com/cli](https://aws.amazon.com/cli)上的说明安装AWS命令行界面（CLI）工具。一旦您完成设置，我们将遵循七个步骤来设置集群：
- en: Create a security key.
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个安全密钥。
- en: Create the ECS cluster.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建ECS集群。
- en: Configure the cluster’s networking.
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 配置集群的网络。
- en: Create a shared data drive in Elastic File System (EFS).
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹性文件系统（EFS）中创建一个共享数据驱动器。
- en: Allocate space for Docker images in Elastic Container Repository (ECR).
  id: totrans-346
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在弹性容器仓库（ECR）中为Docker镜像分配空间。
- en: Build and deploy the images for the scheduler, worker, and notebook server.
  id: totrans-347
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建和部署调度器、工作节点和笔记本服务器的镜像。
- en: Connect to the cluster.
  id: totrans-348
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 连接到集群。
- en: 11.1.2 Creating a security key
  id: totrans-349
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.2 创建安全密钥
- en: To start, log in to the AWS console. You should be greeted by a screen that
    looks similar to [figure 11.3](#figure11.3).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，登录到AWS控制台。您应该会看到一个类似于[图11.3](#figureanchor11.3)的屏幕。
- en: '![c11_03.png](Images/c11_03.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![c11_03.png](Images/c11_03.png)'
- en: '[Figure 11.3](#figureanchor11.3) The AWS Console home screen'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.3](#figureanchor11.3) AWS控制台主页'
- en: The first thing we need to do is create a security key that we will use later
    to authenticate with AWS while deploying the Docker images. To do this, hover
    over your account name in the top-right corner next to the bell icon, as shown
    in [figure 11.4](#figure11.4).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是创建一个安全密钥，稍后我们将使用它来在部署Docker镜像时验证AWS。为此，将鼠标悬停在右上角靠近铃铛图标处的账户名称上，如图[图11.4](#figure11.4)所示。
- en: Click My Security Credentials, and you will be brought to the Your Security
    Credentials page. If you receive a warning message popup, such as in [figure 11.5](#figure11.5),
    choose Continue to Security Credentials.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“我的安全凭证”，您将被带到“您的安全凭证”页面。如果您收到一个警告消息弹出窗口，如图[图11.5](#figure11.5)所示，请选择继续到安全凭证。
- en: '![c11_04.png](Images/c11_04.png)'
  id: totrans-355
  prefs: []
  type: TYPE_IMG
  zh: '![c11_04.png](Images/c11_04.png)'
- en: '[Figure 11.4](#figureanchor11.4) Account control menu'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.4](#figureanchor11.4) 账户控制菜单'
- en: '![c11_05.png](Images/c11_05.png)'
  id: totrans-357
  prefs: []
  type: TYPE_IMG
  zh: '![c11_05.png](Images/c11_05.png)'
- en: '[Figure 11.5](#figureanchor11.5) Security warning message'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.5](#figureanchor11.5) 安全警告消息'
- en: Click the Access Keys drop-down area. If you have any existing keys, you can
    select Delete to remove them. Then, click Create New Access Key. You should see
    a dialog box similar to [figure 11.6](#figure11.6) with your new access key and
    secret access key (the keys in [figure 11.6](#figure11.6) have been masked for
    security—you will see your actual generated keys in the dialog box on your screen).
    Click Download Key File to download a CSV file containing these two values. You
    can take a screenshot if you’d like, because we will use these keys later. It’s
    important that you keep track of your secret access key and also keep it safe.
    It can’t be recovered if you lose it (you’d have to create a new key), and it
    could be used to compromise your AWS account if it got into the wrong hands. Treat
    it like a password or a credit card number.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 点击访问密钥下拉区域。如果您有任何现有的密钥，可以选择删除来移除它们。然后，点击创建新访问密钥。您应该会看到一个类似于[图11.6](#figure11.6)的对话框，其中包含您的新访问密钥和秘密访问密钥（[图11.6](#figure11.6)中的密钥已被屏蔽以保障安全——您将在屏幕上的对话框中看到您实际生成的密钥）。点击下载密钥文件以下载包含这两个值的CSV文件。如果您愿意，可以截图，因为我们稍后会使用这些密钥。跟踪您的秘密访问密钥并保持其安全非常重要。如果您丢失了它（您将不得不创建一个新的密钥），并且如果它落入错误的手中，它可能会被用来危害您的AWS账户。把它当作密码或信用卡号码一样对待。
- en: '![c11_06.png](Images/c11_06.png)'
  id: totrans-360
  prefs: []
  type: TYPE_IMG
  zh: '![c11_06.png](Images/c11_06.png)'
- en: '[Figure 11.6](#figureanchor11.6) Create Access Key dialog box'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.6](#figureanchor11.6) 创建访问密钥对话框'
- en: Now that you’ve created a security key, the next step in the process is to create
    the ECS cluster.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经创建了一个安全密钥，下一步是创建ECS集群。
- en: 11.1.3 Creating the ECS cluster
  id: totrans-363
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.3 创建ECS集群
- en: When working in the cloud, it’s typical to talk about “compute resources” rather
    than physical computers. This is because when servers are requisitioned in the
    cloud, they are rarely dedicated physical machines solely for your personal use.
    Instead, they are virtual machines, called *instances*, that run on huge clusters
    of servers shared by many other cloud customers. For all intents and purposes,
    though, they appear to be separate physical machines. Each instance gets its own
    separate IP address, filesystem space, and so on. In AWS, compute resources are
    requisitioned through Elastic Compute Cloud (EC2)—this service allows you to create
    and tear down virtual servers that can be used to host anything you want. ECS
    allows you to run Docker images in containers on EC2 instances. Again, this is
    beneficial for us because we won’t have to log in to each EC2 instance and configure
    it manually. We can simply use the EC2 instances to run copies of the preconfigured
    Docker images we’ll build later in the chapter.
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中工作的时候，通常谈论的是“计算资源”而不是物理计算机。这是因为当在云中请求服务器时，它们很少是仅用于您个人使用的专用物理机器。相反，它们是运行在由许多其他云客户共享的庞大服务器集群上的虚拟机，称为*实例*。然而，从所有目的和意义上讲，它们看起来像是独立的物理机器。每个实例都有自己的独立IP地址、文件系统空间等等。在AWS中，通过弹性计算云（EC2）请求计算资源——这项服务允许您创建和拆除虚拟服务器，可以用来托管任何您想要的东西。ECS允许您在EC2实例上运行容器中的Docker镜像。对我们来说，这很有益处，因为我们不需要登录到每个EC2实例并手动配置它。我们可以简单地使用EC2实例来运行我们将在本章稍后构建的预配置Docker镜像的副本。
- en: Since so many users have embraced the ease of using Docker for cloud deployments,
    Amazon has streamlined the process to requisition EC2 instances and configure
    them for Docker into a setup wizard. We’ll step through that wizard in a moment.
    First, we need to create an SSH key that we will associate to the EC2 instances.
    This will allow you to log in to the EC2 instances using SSH, which we will need
    to do later in the process. To start, roll over the Services menu in the AWS Console,
    and under the Compute section, choose EC2\. [Figure 11.7](#figure11.7) shows the
    area of the menu where you can find the link to EC2.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多用户已经接受了使用Docker进行云部署的便捷性，亚马逊简化了请求EC2实例并为其配置Docker的过程，将其整合到一个设置向导中。我们稍后将逐步通过这个向导。首先，我们需要创建一个SSH密钥，我们将将其关联到EC2实例上。这将允许您使用SSH登录到EC2实例，我们在后续过程中需要这样做。首先，在AWS控制台中滚动到服务菜单，然后在计算部分选择EC2。[图11.7](#figure11.7)显示了菜单中可以找到EC2链接的区域。
- en: '![c11_07.png](Images/c11_07.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![c11_07.png](Images/c11_07.png)'
- en: '[Figure 11.7](#figureanchor11.7) Navigate to the EC2 dashboard.'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.7](#figureanchor11.7) 导航到EC2仪表板。'
- en: Once you arrive at the EC2 dashboard, click the area similar to [figure 11.8](#figure11.8)
    where it says 0 Key Pairs.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦到达EC2仪表板，点击类似于[图11.8](#figure11.8)的区域，上面写着“0密钥对”。
- en: '![c11_08.png](Images/c11_08.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![c11_08.png](Images/c11_08.png)'
- en: '[Figure 11.8](#figureanchor11.8) Navigating to the Key Pair view'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.8](#figureanchor11.8) 导航到密钥对视图'
- en: Next, click the Create Key Pair button, as shown in [figure 11.9](#figure11.9).
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，点击如图[图11.9](#figure11.9)所示的创建密钥对按钮。
- en: '![c11_09.png](Images/c11_09.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![c11_09.png](Images/c11_09.png)'
- en: '[Figure 11.9](#figureanchor11.9) Select Create Key Pair.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.9](#figureanchor11.9) 选择创建密钥对。'
- en: When prompted to give the key pair a name, type `dask-cluster-key` and click
    Create. This will create the key pair and download a copy of it to your Downloads
    folder. It should be named dask-cluster-key.pem.txt. Rename it to dask-cluster-key.pem
    and put it somewhere safe. This is also a private key file; you should protect
    it because it can be used to access your EC2 instances.
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示为密钥对命名时，键入`dask-cluster-key`并点击创建。这将创建密钥对并将其副本下载到您的下载文件夹中。它应该命名为dask-cluster-key.pem.txt。将其重命名为dask-cluster-key.pem并将其放在安全的地方。这也是一个私钥文件；您应该保护它，因为它可以用来访问您的EC2实例。
- en: Now that we’ve created the key pair, we can create the ECS cluster. Go back
    to the Services menu in the top-left of the AWS Console and select ECS under the
    Compute menu. Once greeted by the welcome screen for ECS, click the Clusters menu
    option under the Amazon ECS menu on the left edge of the screen. You should now
    see a screen that looks similar to [figure 11.10](#figure11.10).
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经创建了密钥对，我们可以创建ECS集群。返回AWS控制台顶部的左侧服务菜单，并在计算菜单下选择ECS。一旦看到ECS的欢迎屏幕，点击屏幕左侧边缘Amazon
    ECS菜单下的集群菜单选项。现在您应该看到一个类似于[图11.10](#figure11.10)的屏幕。
- en: '![c11_10.png](Images/c11_10.png)'
  id: totrans-376
  prefs: []
  type: TYPE_IMG
  zh: '![c11_10.png](Images/c11_10.png)'
- en: '[Figure 11.10](#figureanchor11.10) The Amazon ECS cluster management screen'
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.10](#figureanchor11.10) Amazon ECS集群管理屏幕'
- en: Click the blue Create Cluster button in the upper-left area of the screen. This
    will start the ECS Create Cluster wizard. When prompted to select a cluster template,
    select EC2 Linux + Networking, as shown in [figure 11.11](#figure11.11).
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 在屏幕左上角点击蓝色创建集群按钮。这将启动ECS创建集群向导。当提示选择集群模板时，选择EC2 Linux + Networking，如图[图11.11](#figure11.11)所示。
- en: '![c11_11.png](Images/c11_11.png)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![c11_11.png](Images/c11_11.png)'
- en: '[Figure 11.11](#figureanchor11.11) ECS Create Cluster wizard step 1'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.11](#figureanchor11.11) ECS创建集群向导步骤1'
- en: Click Next Step to advance to the Configure Cluster screen. Enter a name for
    your cluster in the Cluster Name box, such as `dask-cluster`. The name cannot
    have any spaces, capital letters, or special characters other than hyphens. Select
    the t2.micro instance type in the EC2 Instance Type drop-down box. This is the
    Instance type that’s eligible for the AWS Free Tier. Finally, enter `8` in the
    Number of Instances box and select the key pair you created earlier in the Key
    pair drop-down box. The rest of the options can be left to their default values.
    Verify your configuration looks similar to [figure 11.12](#figure11.12) before
    proceeding.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一步以进入配置集群屏幕。在集群名称框中为您的集群输入一个名称，例如`dask-cluster`。名称不能包含任何空格、大写字母或除破折号以外的特殊字符。在EC2实例类型下拉框中选择t2.micro实例类型。这是符合AWS免费层的实例类型。最后，在实例数量框中输入`8`，并在密钥对下拉框中选择您之前创建的密钥对。其余选项可以保留为默认值。在继续之前，请验证您的配置看起来类似于[图11.12](#figure11.12)。
- en: Once you’ve verified the configuration, click the Finish button at the bottom
    of the screen to create the cluster. If everything was entered successfully, you
    will see the Launch Status screen. This screen displays the progress while the
    cluster is requisitioned and built. Once the blue View Cluster button lights up,
    the cluster is finished being built. Click Clusters on the menu on the left side
    of the screen. This will bring you back to the Clusters screen, where you should
    see your newly built cluster. Your screen should look similar to [figure 11.13](#figure11.13).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦验证了配置，点击屏幕底部的完成按钮以创建集群。如果一切输入成功，您将看到启动状态屏幕。此屏幕显示集群请求和构建的进度。一旦蓝色查看集群按钮亮起，集群构建完成。点击屏幕左侧菜单中的集群。这将带您回到集群屏幕，您应该会看到您新构建的集群。您的屏幕应该类似于[图11.13](#figure11.13)。
- en: '![c11_12.png](Images/c11_12.png)'
  id: totrans-383
  prefs: []
  type: TYPE_IMG
  zh: '![c11_12.png](Images/c11_12.png)'
- en: '[Figure 11.12](#figureanchor11.12) Cluster configuration settings'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.12](#figureanchor11.12) 集群配置设置'
- en: '![c11_13.png](Images/c11_13.png)'
  id: totrans-385
  prefs: []
  type: TYPE_IMG
  zh: '![c11_13.png](Images/c11_13.png)'
- en: '[Figure 11.13](#figureanchor11.13) Cluster status window showing the newly
    built cluster'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.13](#figureanchor11.13) 显示新构建集群的集群状态窗口'
- en: The most important thing to note on this screen is the number of Container Instances
    on the far right-hand side of the screen. This shows you how many EC2 instances
    are currently running and joined to the cluster. Since we requisitioned eight
    EC2 instances, you should see eight container instances available. If you don’t
    see eight container instances, wait a few minutes and refresh the page. Occasionally
    it will take several minutes for the EC2 instances to completely boot up and connect
    to the cluster.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在此屏幕上需要注意的最重要的事情是屏幕右侧远端显示的容器实例数量。这显示了当前运行并加入集群的 EC2 实例数量。由于我们预定了八个 EC2 实例，您应该看到八个容器实例可用。如果您看不到八个容器实例，请等待几分钟并刷新页面。偶尔需要几分钟的时间才能让
    EC2 实例完全启动并连接到集群。
- en: 11.1.4 Configuring the cluster’s networking
  id: totrans-388
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.4 配置集群的网络
- en: Now that the cluster is up and running, we need to configure the cluster’s firewall
    rules to allow us to connect to it. To do this, we’ll need to go back to the EC2
    Dashboard. Click the Services menu and select EC2 under the Compute section. Once
    on the EC2 Dashboard, select Security Groups from the menu on the left edge of
    the screen under the Network & Security heading, as shown in [figure 11.14](#figure11.14).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在集群已经启动并运行，我们需要配置集群的防火墙规则以允许我们连接到它。为此，我们需要回到 EC2 仪表板。点击“服务”菜单，在“计算”部分下选择 EC2。一旦进入
    EC2 仪表板，在“网络与安全”标题下左侧菜单中选择“安全组”，如图 [11.14](#figure11.14) 所示。
- en: '![c11_14.png](Images/c11_14.png)'
  id: totrans-390
  prefs: []
  type: TYPE_IMG
  zh: '![c11_14.png](Images/c11_14.png)'
- en: '[Figure 11.14](#figureanchor11.14) EC2 Dashboard menu showing Security Groups
    configuration'
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.14](#figureanchor11.14) 显示安全组配置的 EC2 仪表板菜单'
- en: On the Security Groups page, locate the security group that corresponds with
    the cluster you just created. The group name should be something similar to EC2ContainerService-<cluster
    name>-EcsSecurityGroup-xxxxxxxxx. Click the check box to the left of the security
    group to select it. [Figure 11.15](#figure11.15) shows an example of the security
    groups.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在“安全组”页面，找到与您刚刚创建的集群相对应的安全组。组名应该是类似于 EC2ContainerService-<集群名称>-EcsSecurityGroup-xxxxxxxxx
    的样子。点击安全组左侧的复选框以选择它。[图 11.15](#figure11.15) 展示了安全组的示例。
- en: '![c11_15.png](Images/c11_15.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![c11_15.png](Images/c11_15.png)'
- en: '[Figure 11.15](#figureanchor11.15) An example security group for the ECS cluster'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.15](#figureanchor11.15) ECS 集群的示例安全组'
- en: On the lower half of the screen, select the Inbound tab and click the Edit button.
    An example of this is shown in [figure 11.16](#figure11.16).
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在屏幕的下半部分，选择“入站”选项卡并点击“编辑”按钮。图 [11.16](#figure11.16) 展示了此操作的示例。
- en: '![c11_16.png](Images/c11_16.png)'
  id: totrans-396
  prefs: []
  type: TYPE_IMG
  zh: '![c11_16.png](Images/c11_16.png)'
- en: '[Figure 11.16](#figureanchor11.16) Inbound firewall rules'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.16](#figureanchor11.16) 入站防火墙规则'
- en: 'First, create a rule to allow inbound SSH connections from your IP address.
    This will allow you to log in to the EC2 instances that are part of the cluster.
    Under the Type column, choose SSH, and under the Source column, choose My IP.
    You can also type in an optional description for the firewall rule. [Figure 11.17](#figure11.17)
    shows an example of this configuration (note: your IP address will be different
    from the IP address listed in the figures).'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，创建一条规则以允许从您的 IP 地址进行入站 SSH 连接。这将允许您登录到集群中的 EC2 实例。在“类型”列下选择 SSH，在“来源”列下选择“我的
    IP”。您还可以为防火墙规则输入可选的描述。[图 11.17](#figure11.17) 展示了此配置的示例（注意：您的 IP 地址将与图中列出的 IP
    地址不同）。
- en: '![c11_17.png](Images/c11_17.png)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![c11_17.png](Images/c11_17.png)'
- en: '[Figure 11.17](#figureanchor11.17) Example SSH firewall rule'
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.17](#figureanchor11.17) 示例 SSH 防火墙规则'
- en: The next rule to configure is to allow all the EC2 instances to talk to each
    other. For example, the Dask scheduler needs to be able to talk to the workers
    to hand out instructions. Click Add Rule again, select All TCP from the Type column,
    and choose Custom from the Source column. Next to the drop-down box where you
    selected Custom, start typing EC2 (with capital letters). A drop-down will appear
    with the security groups listed, as shown in [figure 11.18](#figure11.18).
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 需要配置的下一条规则是允许所有 EC2 实例之间进行通信。例如，Dask 调度器需要能够与工作节点通信以分发指令。再次点击“添加规则”，在“类型”列中选择“所有
    TCP”，在“来源”列中选择“自定义”。在您选择“自定义”的下拉框旁边，开始键入 EC2（使用大写字母）。将出现一个下拉列表，其中列出了安全组，如图 [11.18](#figure11.18)
    所示。
- en: '![c11_18.png](Images/c11_18.png)'
  id: totrans-402
  prefs: []
  type: TYPE_IMG
  zh: '![c11_18.png](Images/c11_18.png)'
- en: '[Figure 11.18](#figureanchor11.18) Creating an inbound rule from a security
    group'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.18](#figure11.18) 从安全组创建入站规则'
- en: From the drop-down list, choose the ECS cluster security group. Lastly, we need
    to open ports for the Jupyter Notebook server as well as the Dask diagnostics
    pages. [Figure 11.19](#figure11.19) shows the relevant configurations for the
    two additional rules to be created.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 从下拉列表中选择 ECS 集群安全组。最后，我们还需要为 Jupyter Notebook 服务器以及 Dask 诊断页面打开端口。[图 11.19](#figure11.19)
    显示了创建两个附加规则的相关配置。
- en: '![c11_19.png](Images/c11_19.png)'
  id: totrans-405
  prefs: []
  type: TYPE_IMG
  zh: '![c11_19.png](Images/c11_19.png)'
- en: '[Figure 11.19](#figureanchor11.19) Firewall rules for Jupyter and Dask diagnostics'
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.19](#figure11.19) Jupyter 和 Dask 诊断的防火墙规则'
- en: To create the inbound rule for the Jupyter Notebook server, click Add Rule,
    select Custom TCP Rule from the Type column, type `8888` into the Port Range column,
    and select My IP from the Source column. Then, create an identical rule for the
    Dask diagnostic ports. Instead of port 8888, type `8787 – 8789` into the Port
    Range column. Once all rules have been created, click the Save button.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建 Jupyter Notebook 服务器的入站规则，请点击“添加规则”，从“类型”列中选择“自定义 TCP 规则”，在“端口范围”列中输入 `8888`，并从“源”列中选择“我的
    IP”。然后，为 Dask 诊断端口创建一个相同的规则。在“端口范围”列中，输入 `8787 – 8789` 而不是 `8888`。一旦创建了所有规则，请点击“保存”按钮。
- en: Now that the rules have been saved, it’s a good idea to test them to make sure
    everything is working as expected. To do this, we first need to look up the IP
    address or hostname of one of your running EC2 instances. From the EC2 Dashboard,
    click Instances on the left menu under the Instances heading. This will bring
    you to the EC2 Instances Manager. On the screen, you should see a list of all
    currently running EC2 instances. This should look similar to [figure 11.20](#figure11.20).
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 现在规则已经保存，最好测试一下以确保一切按预期工作。为此，我们首先需要查找您运行中的 EC2 实例之一的 IP 地址或主机名。从 EC2 仪表板中，点击左侧菜单下“实例”标题下的“实例”。这将带您到
    EC2 实例管理器。在屏幕上，您应该会看到一个当前运行的所有 EC2 实例的列表。这应该类似于 [图 11.20](#figure11.20)。
- en: '![c11_20.png](Images/c11_20.png)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![c11_20.png](Images/c11_20.png)'
- en: '[Figure 11.20](#figureanchor11.20) A list of all running EC2 instances'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.20](#figureanchor11.20) 所有运行中的 EC2 实例列表'
- en: In the Public DNS (IPv4) column, copy one of the hostnames. It doesn’t matter
    which one you select.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在“公共 DNS (IPv4)”列中，复制一个主机名。您选择哪个都无关紧要。
- en: Connecting using SSH on MacOS/Linux/Unix
  id: totrans-412
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 MacOS/Linux/Unix 上使用 SSH 进行连接
- en: 'To connect to the EC2 instance, use the following directions based on the operating
    system you’re using:'
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 根据您使用的操作系统，使用以下说明连接到 EC2 实例：
- en: Open a Terminal window and navigate to the folder where you stored the dask-cluster-key.pem
    file.
  id: totrans-414
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 打开一个终端窗口，导航到您存储 dask-cluster-key.pem 文件的文件夹。
- en: If this is your first time connecting, make the PEM file read-only by typing
    `chmod 400 dask-cluster-key.pem`; otherwise, your SSH client may not allow you
    to use the key file to connect.
  id: totrans-415
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果这是您第一次连接，请通过输入 `chmod 400 dask-cluster-key.pem` 将 PEM 文件设置为只读；否则，您的 SSH 客户端可能不允许您使用密钥文件进行连接。
- en: To connect, type `ssh -i dask-cluster-key.pem ec2-user@<hostname>`; fill the
    hostname you copied from the EC2 Instance Manager into <hostname> space.
  id: totrans-416
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要连接，请输入 `ssh -i dask-cluster-key.pem ec2-user@<hostname>`；将您从 EC2 实例管理器中复制的
    hostname 填入 <hostname> 空间。
- en: If you are prompted to add a key fingerprint, type `yes`.
  id: totrans-417
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果提示您添加密钥指纹，请输入 `yes`。
- en: If your connection is successful, you should see a login screen similar to [figure
    11.21](#figure11.21).
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果连接成功，您应该看到一个类似于 [图 11.21](#figure11.21) 的登录屏幕。
- en: If your connection was unsuccessful, double-check that the SSH command was typed
    correctly. If connection problems persist, double-check the firewall rules to
    ensure the correct ports are open.
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果连接失败，请再次检查 SSH 命令是否输入正确。如果连接问题持续存在，请检查防火墙规则以确保正确的端口已打开。
- en: '![c11_21.png](Images/c11_21.png)'
  id: totrans-420
  prefs: []
  type: TYPE_IMG
  zh: '![c11_21.png](Images/c11_21.png)'
- en: '[Figure 11.21](#figureanchor11.21) Successful connection to an EC2 instance'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.21](#figureanchor11.21) 成功连接到 EC2 实例'
- en: Connecting using SSH on Windows
  id: totrans-422
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在 Windows 上使用 SSH 进行连接
- en: Windows does not have a built-in SSH client, unlike MacOS/Linux/Unix systems.
    Amazon recommends using a free SSH client called PuTTY to connect to EC2\. You
    can find instructions for downloading and installing PuTTY and using it to connect
    to your EC2 instance at [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html).
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 与 MacOS/Linux/Unix 系统不同，Windows 没有内置的 SSH 客户端。Amazon 建议使用名为 PuTTY 的免费 SSH 客户端来连接到
    EC2。您可以在 [https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html](https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/putty.html)
    找到有关下载和安装 PuTTY 以及使用它连接到您的 EC2 实例的说明。
- en: After you’ve connected successfully to your EC2 instance, you can go ahead and
    disconnect for now. You’ll reconnect at the end of the next section to upload
    some data to the shared file system we’re about to create. On that note, don’t
    bother keeping the host name or IP address of your EC2 instance copied down anywhere.
    EC2 instances are *ephemeral*, meaning characteristics like their IP address and
    filesystem contents are only around for the life of the instance. When EC2 instances
    are terminated, they release their IP address allocations, and it’s unlikely the
    instance will receive the same IP address when it’s started back up. Generally,
    any time you need to connect to an EC2 instance, you should use the EC2 Instance
    Manager to look up its current IP address. Similarly, don’t store any data you
    want long-term access to on an EC2 instance. Instead, put data that you want to
    be persistent on a persistent filesystem, such as the Elastic File System share
    that we will create in the next section.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 成功连接到您的 EC2 实例后，您可以暂时断开连接。您将在下一节的末尾重新连接，以便将数据上传到我们即将创建的共享文件系统。关于这一点，请不要费心将您的
    EC2 实例的主机名或 IP 地址记录下来。EC2 实例是*短暂的*，这意味着它们的 IP 地址和文件系统内容仅存在于实例的生命周期内。当 EC2 实例被终止时，它们会释放它们的
    IP 地址分配，并且实例在重新启动时不太可能收到相同的 IP 地址。通常，每次您需要连接到 EC2 实例时，您都应该使用 EC2 实例管理器来查找其当前的
    IP 地址。同样，不要在 EC2 实例上存储您希望长期访问的数据。相反，将您希望持久化的数据放在持久文件系统上，例如我们将在下一节中创建的弹性文件系统共享。
- en: 11.1.5 Creating a shared data drive in Elastic File System
  id: totrans-425
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.5 在弹性文件系统中创建共享数据驱动器
- en: Before leaving the EC2 Instance Manager, you’ll need to get the VPC ID from
    one of the EC2 instances. The VPC ID is used by EC2 to identify cloud resources
    that are part of your account. To get this value, select one of the instances
    in the EC2 Instance Manager and look at the lower half of the window for the VPC
    ID value. You should find this value below the Private IP value, as shown in [figure
    11.22](#figure11.22).
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在离开 EC2 实例管理器之前，您需要从 EC2 实例中获取 VPC ID。VPC ID 由 EC2 用于识别您账户中的云资源。要获取此值，请选择 EC2
    实例管理器中的一个实例，并查看窗口的下半部分以找到 VPC ID 值。您应该在这个值在私有 IP 值下方找到，如图 [图 11.22](#figure11.22)
    所示。
- en: '![c11_22.png](Images/c11_22.png)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![c11_22.png](Images/c11_22.png)'
- en: '[Figure 11.22](#figureanchor11.22) VPC ID in EC2 Instance Manager'
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.22](#figureanchor11.22) EC2 实例管理器中的 VPC ID'
- en: Copy this value, then click the Services menu on the top-left corner of the
    screen. Choose EFS under the Storage heading. You’ll be taken to a welcome screen
    for Amazon EFS. Click the blue Create File System button to start the EFS creation
    wizard. In the first step, select the VPC ID from the VPC drop-down box that matches
    the VPC ID you copied down from the EC2 instance. Under the Create Mount Targets
    section, leave the default values for the Subnet column. However, clear the Security
    Groups boxes. Then, start typing `EC2` (with uppercase letters) and select the
    security group ID for the ECS cluster (it should look similar to EC2ContainerService-<cluster
    name>-EcsSecurityGroup-xxxxxxxxx). Your screen should look similar to [figure
    11.23](#figure11.23).
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 复制此值，然后点击屏幕左上角的“服务”菜单。在“存储”标题下选择 EFS。您将被带到 Amazon EFS 的欢迎屏幕。点击蓝色的“创建文件系统”按钮以启动
    EFS 创建向导。在第一步中，从 VPC 下拉框中选择与您从 EC2 实例中复制的 VPC ID 匹配的 VPC ID。在“创建挂载目标”部分，保留子网列的默认值。但是，清除安全组框。然后，开始键入
    `EC2`（使用大写字母）并选择 ECS 集群的安全组 ID（它应该类似于 EC2ContainerService-<cluster name>-EcsSecurityGroup-xxxxxxxxx）。您的屏幕应该类似于
    [图 11.23](#figure11.23)。
- en: '![c11_23.png](Images/c11_23.png)'
  id: totrans-430
  prefs: []
  type: TYPE_IMG
  zh: '![c11_23.png](Images/c11_23.png)'
- en: '[Figure 11.23](#figureanchor11.23) File system access configuration in EFS'
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.23](#figureanchor11.23) EFS 文件系统访问配置'
- en: Click the Next Step button. Accept the default values for step 2 and click the
    Next Step button again. Finally, on the review screen, click Create File System
    to finish. In a few minutes, you should see that the filesystem has been successfully
    created. Before moving away from the page, copy the DNS name of the filesystem
    we just created. This value is displayed under the File System Access header,
    as shown in [figure 11.24](#figure11.24).
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“下一步”按钮。接受步骤 2 的默认值，然后再次点击“下一步”按钮。最后，在审查屏幕上，点击“创建文件系统”以完成。几分钟后，您应该会看到文件系统已成功创建。在离开页面之前，复制我们刚刚创建的文件系统的
    DNS 名称。此值显示在“文件系统访问”标题下，如图 [图 11.24](#figure11.24) 所示。
- en: '![c11_24.png](Images/c11_24.png)'
  id: totrans-433
  prefs: []
  type: TYPE_IMG
  zh: '![c11_24.png](Images/c11_24.png)'
- en: '[Figure 11.24](#figureanchor11.24) DNS name of EFS'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.24](#figureanchor11.24) EFS 的 DNS 名称'
- en: Now that the filesystem is created, we need to tell the EC2 instances to mount
    the filesystem at boot time so you can use it for storage. To do this, navigate
    back to the EC2 Dashboard. On the left side menu, click Launch Configurations
    under the Auto Scaling heading. The Launch Configurations Manager should display
    and look similar to [figure 11.25](#figure11.25).
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 现在文件系统已创建，我们需要告诉 EC2 实例在启动时挂载该文件系统，以便您可以使用它进行存储。为此，导航回 EC2 仪表板。在左侧菜单中，点击“自动扩展”下的“启动配置”。启动配置管理器应显示并看起来类似于
    [图 11.25](#figure11.25)。
- en: '![c11_25.png](Images/c11_25.png)'
  id: totrans-436
  prefs: []
  type: TYPE_IMG
  zh: '![c11_25.png](Images/c11_25.png)'
- en: '[Figure 11.25](#figureanchor11.25) The Launch Configurations Manager'
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.25](#figureanchor11.25) 启动配置管理器'
- en: Select the launch configuration (there should only be one), click the Actions
    button, and select Copy Launch Configuration. The Copy Launch Configuration wizard
    will appear. At the top edge of the screen, click 3\. Configure Details, and expand
    the Advanced Details section. Your screen should look similar to [figure 11.26](#figure11.26).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 选择启动配置（应该只有一个），点击“操作”按钮，然后选择“复制启动配置”。将出现复制启动配置向导。在屏幕顶部边缘点击 3. 配置详细信息，并展开高级详细信息部分。您的屏幕应该看起来类似于
    [图 11.26](#figure11.26)。
- en: Give your new launch configuration a unique name in the Name field. Also, be
    sure ecsInstanceRole is selected in the IAM Role drop-down box. Otherwise, the
    EC2 instances will not be able to communicate with ECS after rebooting. In the
    User Data field, copy the contents of [listing 11.1](#listing11.1) into the text
    box.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 在“名称”字段中为您的新的启动配置提供一个独特的名称。同时，确保在“ IAM 角色”下拉框中选中了ecsInstanceRole。否则，EC2 实例在重启后无法与
    ECS 通信。在“用户数据”字段中，将 [列表 11.1](#listing11.1) 的内容复制到文本框中。
- en: '![c11_26.png](Images/c11_26.png)'
  id: totrans-440
  prefs: []
  type: TYPE_IMG
  zh: '![c11_26.png](Images/c11_26.png)'
- en: '[Figure 11.26](#figureanchor11.26) Launch configuration details'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.26](#figureanchor11.26) 启动配置详细信息'
- en: Listing 11.1 User data for launch configuration
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.1 启动配置的用户数据
- en: '[PRE48]'
  id: totrans-443
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Fill in the filesystem DNS name you copied from the EFS confirmation screen
    where <your filesystem DNS name> appears, and fill in your ECS cluster name where
    <your ecs cluster name> appears (it should be dask-cluster unless you selected
    a different name). This data essentially tells the EC2 instances to configure
    themselves to mount the EFS filesystem you created earlier in the section at boot
    time. Once you’ve finished adding the User Data to the configuration, click Skip
    to Review, and then click Create Launch Configuration. When prompted to select
    a key pair, select the dask-cluster-key you previously generated. Check the check
    box and click Create Launch Configuration. [Figure 11.27](#figure11.27) displays
    this dialog box.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 在 EFS 确认屏幕上出现 <your filesystem DNS name> 的位置，填写您复制的文件系统 DNS 名称，并在出现 <your ecs
    cluster name> 的位置填写您的 ECS 集群名称（除非您选择了不同的名称，否则应该是 dask-cluster）。这些数据本质上告诉 EC2 实例在启动时配置自己以挂载您在前面章节中创建的
    EFS 文件系统。完成添加用户数据到配置后，点击“跳过预览”，然后点击“创建启动配置”。当提示选择密钥对时，选择您之前生成的 dask-cluster-key。勾选复选框并点击“创建启动配置”。[图
    11.27](#figure11.27) 显示了此对话框。
- en: '![c11_27.png](Images/c11_27.png)'
  id: totrans-445
  prefs: []
  type: TYPE_IMG
  zh: '![c11_27.png](Images/c11_27.png)'
- en: '[Figure 11.27](#figureanchor11.27) Confirmation of key pairs'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.27](#figureanchor11.27) 密钥对确认'
- en: After the launch configuration has been created, click the Close button. Then,
    on the left-side menu, click Auto Scaling Groups under the Auto Scaling heading.
    We’ll now need to configure the EC2 instances to use the new launch configuration.
    Select the auto scaling group that was created automatically when the ECS cluster
    was created (there should only be one), and click the Edit button. Your Auto Scaling
    Group Manager screen should look similar to [figure 11.28](#figure11.28).
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 启动配置创建完成后，点击“关闭”按钮。然后，在左侧菜单中，点击“自动扩展”下的“自动扩展组”。现在我们需要配置 EC2 实例使用新的启动配置。选择在创建
    ECS 集群时自动创建的自动扩展组（应该只有一个），然后点击“编辑”按钮。您的自动扩展组管理器屏幕应该看起来类似于 [图 11.28](#figure11.28)。
- en: '![c11_28.png](Images/c11_28.png)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![c11_28.png](Images/c11_28.png)'
- en: '[Figure 11.28](#figureanchor11.28) Auto Scaling Group Manager'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.28](#figureanchor11.28) 自动扩展组管理器'
- en: In the Edit Details dialog box, change the Launch Configuration drop-down to
    the launch configuration you just created. Your screen should look similar to
    [figure 11.29](#figure11.29). This configuration is very important—it controls
    how many EC2 instances are part of the ECS cluster. We should have a number of
    instances equal to the number of workers we want in the Dask cluster, plus an
    additional instance to host the scheduler and one more instance to host the notebook
    server. With eight instances in the cluster, we’ll be able to have six workers,
    a scheduler, and a notebook server. If you wanted 100 workers instead, we’d need
    102 instances. Once you’ve completed configuring the Auto Scaling Group, click
    the Save button.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 在编辑详细信息对话框中，将启动配置下拉菜单更改为您刚刚创建的启动配置。您的屏幕应该类似于[图11.29](#figure11.29)。此配置非常重要——它控制有多少个EC2实例是ECS集群的一部分。我们应该有与Dask集群中我们想要的工人数量相等的实例数量，再加上一个用于托管调度器的一个实例，以及一个用于托管笔记本服务器的另一个实例。在集群中有八个实例的情况下，我们将能够拥有六个工人、一个调度器和一台笔记本服务器。如果您想要100个工人，我们需要102个实例。完成自动扩展组的配置后，点击保存按钮。
- en: '![c11_29-R.png](Images/c11_29-R.png)'
  id: totrans-451
  prefs: []
  type: TYPE_IMG
  zh: '![c11_29-R.png](Images/c11_29-R.png)'
- en: '[Figure 11.29](#figureanchor11.29) Auto Scaling Group configuration'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.29](#figureanchor11.29) 自动扩展组配置'
- en: Because launch configurations are only run when an EC2 instance starts up, we’ll
    need to terminate and relaunch the currently running EC2 instances for our configuration
    changes to take effect. To do this, navigate back to the EC2 Instances Manager.
    Then, select all the instances that are currently in the Running state, click
    the Actions button, select Instance State, then select Terminate. Your screen
    should look similar to [figure 11.30](#figure11.30).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 由于启动配置仅在EC2实例启动时运行，我们需要终止并重新启动当前运行的EC2实例，以便我们的配置更改生效。为此，返回到EC2实例管理器。然后，选择所有当前处于运行状态的实例，点击操作按钮，选择实例状态，然后选择终止。您的屏幕应该类似于[图11.30](#figure11.30)。
- en: '![c11_30.png](Images/c11_30.png)'
  id: totrans-454
  prefs: []
  type: TYPE_IMG
  zh: '![c11_30.png](Images/c11_30.png)'
- en: '[Figure 11.30](#figureanchor11.30) Cycling out EC2 instances to apply new launch
    configurations'
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.30](#figureanchor11.30) 将EC2实例轮换以应用新的启动配置'
- en: When prompted if you’re sure you want to terminate the instances, click Yes,
    Terminate. In a few seconds, you should see the instances change from the green
    Running to the amber Shutting Down state, and eventually the red Terminated state.
    After about 5–10 minutes, you should see eight new EC2 instances start up and
    change to the green Running state. After the instances have restarted, navigate
    back to the ECS Dashboard and ensure your ECS cluster now shows eight connected
    ECS container instances. If you see zero connected ECS container instances and
    have waited at least 15 minutes, double-check your launch configuration for an
    incorrect configuration. Be especially mindful that the IAM role must be set to
    ecsContainerInstance to avoid permissions issues that prevent the instances from
    associating with the cluster!
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 当提示您是否确定要终止实例时，点击是，终止。几秒钟后，您应该看到实例从绿色运行状态变为琥珀色关闭状态，最终变为红色已终止状态。大约5-10分钟后，您应该看到八个新的EC2实例启动并变为绿色运行状态。实例重启后，返回到ECS仪表板，并确保您的ECS集群现在显示八个连接的ECS容器实例。如果您看到零个连接的ECS容器实例并且已经等待至少15分钟，请仔细检查您的启动配置是否存在配置错误。特别注意IAM角色必须设置为ecsContainerInstance，以避免权限问题，防止实例与集群关联！
- en: Once the EC2 instances have successfully rebooted, we now need to test the connection
    between the EC2 instances and EFS by uploading some data. To do this, navigate
    back to the EC2 Instances Manager, and copy the hostname or IP address of a running
    EC2 instance. Then, locate the arrays.tar file in the chapter 11 files. Open a
    Terminal window and type `scp -i dask-cluster-key.pem arrays.tar ec2-user@``<hostname>``:/home/ec2-user`.
    Fill in the name of your EC2 instance where <hostname> appears. This uses the
    SCP application to upload the arrays.tar file to the home directory on your EC2
    instance. Your screen should look similar to [figure 11.31](#figure11.31).
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦EC2实例成功重启，我们需要通过上传一些数据来测试EC2实例与EFS之间的连接。为此，返回到EC2实例管理器，并复制正在运行的EC2实例的主机名或IP地址。然后，在第11章文件中找到arrays.tar文件。打开一个终端窗口，并输入`scp
    -i dask-cluster-key.pem arrays.tar ec2-user@``<hostname>``:/home/ec2-user`。在<hostname>处填写您EC2实例的名称。这使用SCP应用程序将arrays.tar文件上传到您的EC2实例上的家目录。您的屏幕应该类似于[图11.31](#figure11.31)。
- en: '![c11_31.png](Images/c11_31.png)'
  id: totrans-458
  prefs: []
  type: TYPE_IMG
  zh: '![c11_31.png](Images/c11_31.png)'
- en: '[Figure 11.31](#figureanchor11.31) Using SCP to upload the data'
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.31](#figureanchor11.31) 使用 SCP 上传数据'
- en: After the data upload is complete, use SSH to connect to the EC2 instance. Once
    you’ve logged in to the EC2 instance, type `tar -xvf arrays.tar` to extract the
    data from the TAR file. Your screen should look similar to [figure 11.32](#figure11.32).
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 数据上传完成后，使用 SSH 连接到 EC2 实例。一旦你登录到 EC2 实例，键入 `tar -xvf arrays.tar` 从 TAR 文件中提取数据。你的屏幕应该类似于
    [图 11.32](#figure11.32)。
- en: '![c11_32.png](Images/c11_32.png)'
  id: totrans-461
  prefs: []
  type: TYPE_IMG
  zh: '![c11_32.png](Images/c11_32.png)'
- en: '[Figure 11.32](#figureanchor11.32) Extracting the uploaded data'
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.32](#figureanchor11.32) 提取上传的数据'
- en: Next, type `rm arrays.tar` to delete the TAR file, then `sudo mv * /efs` to
    move the extracted data to the EFS volume you created. Verify the data has moved
    by typing `cd /efs`, then *`ls`*. You should see the two ZARR files displayed.
    Finally, verify that the data is accessible to all the other EC2 instances. To
    do this, go back to the EC2 Instance Manager, copy the hostname of a different
    running EC2 instance, connect to it using SSH, type `cd /efs`, then `ls`, and
    verify you can still see the two ZARR files. Whenever you need to store additional
    data that can be accessed by all your EC2 instances, you can follow the same pattern
    of uploading the data to one instance and moving it to the /efs folder.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，键入 `rm arrays.tar` 以删除 TAR 文件，然后 `sudo mv * /efs` 将提取的数据移动到你创建的 EFS 卷中。通过键入
    `cd /efs`，然后 `ls` 验证数据是否已移动。你应该能看到两个 ZARR 文件。最后，验证数据是否对所有其他 EC2 实例都可用。为此，返回到 EC2
    实例管理器，复制一个不同运行中的 EC2 实例的主机名，使用 SSH 连接到它，键入 `cd /efs`，然后 `ls`，并验证你是否仍然可以看到两个 ZARR
    文件。无论何时你需要存储可以被所有你的 EC2 实例访问的附加数据，你都可以遵循将数据上传到一个实例并将它移动到 /efs 文件夹的相同模式。
- en: 11.1.6 Allocating space for Docker images in Elastic Container Repository
  id: totrans-464
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.6 在弹性容器仓库中为 Docker 镜像分配空间
- en: By this point, we’re almost finished with building out the infrastructure for
    our cluster. The very last thing we need to do before we can deploy and launch
    our Dask cluster is allocate some space for the Docker images we’re going to build
    in the next section. This will let us upload the complete images and launch them
    inside ECS containers. First, go back to the ECS Dashboard and click Repositories
    on the left menu. This will bring you to the Elastic Container Repository (ECR)
    Manager page. Click the Create Repository button in the upper-right corner of
    the screen to start the Create Repository wizard. We’ll create a repository for
    the scheduler first. In the empty Name field, type `dask-scheduler`. Your screen
    should look like [figure 11.33](#figure11.33).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们几乎完成了集群基础设施的构建。在我们能够部署和启动 Dask 集群之前，我们需要做的最后一件事是为我们在下一节中构建的 Docker 镜像分配一些空间。这将使我们能够上传完整的镜像并在
    ECS 容器内启动它们。首先，返回到 ECS 仪表板，点击左侧菜单中的仓库。这将带你到弹性容器仓库（ECR）管理员页面。点击屏幕右上角的创建仓库按钮以启动创建仓库向导。我们首先为调度器创建一个仓库。在空白的名称字段中，键入
    `dask-scheduler`。你的屏幕应该看起来像 [图 11.33](#figure11.33)。
- en: '![c11_33.png](Images/c11_33.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![c11_33.png](Images/c11_33.png)'
- en: '[Figure 11.33](#figureanchor11.33) Creating an ECR repository'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.33](#figureanchor11.33) 创建 ECR 仓库'
- en: Click Create Repository. Once you’ve been returned to the ECR Manager page,
    repeat the process twice more. Create two repositories called dask-worker and
    dask-notebook. Once you’ve completed creating the repositories, we’ll now build
    and deploy the images to the cluster.
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 点击创建仓库。一旦你返回到 ECR 管理员页面，重复此过程两次。创建两个名为 dask-worker 和 dask-notebook 的仓库。一旦完成创建仓库，我们现在将构建和部署镜像到集群中。
- en: 11.1.7 Building and deploying images for scheduler, worker, and notebook
  id: totrans-469
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.7 为调度器、工作节点和笔记本构建和部署镜像
- en: We’ll start with building and deploying the scheduler image because the worker
    and notebook images need to be configured to point to the IP address of the scheduler
    for the cluster to work. Before we begin, ensure Docker is installed and currently
    running on your local machine. Also, ensure your AWS CLI is configured correctly
    using the security key you created in section 11.1.1\. You can find instructions
    on configuring AWS CLI at [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html).
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构建和部署调度器镜像开始，因为工作节点和笔记本镜像需要配置为指向集群的 IP 地址，以便集群能够工作。在我们开始之前，请确保 Docker 已安装并正在你的本地机器上运行。同时，请确保你的
    AWS CLI 已正确配置，使用你在 11.1.1 节中创建的安全密钥。你可以在 [https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html](https://docs.aws.amazon.com/cli/latest/userguide/cli-chap-configure.html)
    找到配置 AWS CLI 的说明。
- en: Once you’ve verified everything has been configured, locate the scheduler folder
    in the chapter 11 files, and navigate to it in a Terminal window (or PowerShell
    if you’re running Windows). In the ECR Manager page, select the repository for
    dask-scheduler and click the View Push Commands button. A pop-up will display
    similar to [figure 11.34](#figure11.34).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你验证了所有配置都已设置正确，在章节11的文件中找到调度器文件夹，并在终端窗口中导航到它（如果你在运行Windows，则使用PowerShell）。在ECR管理器页面中，选择dask-scheduler存储库，然后点击查看推送命令（View
    Push Commands）按钮。将显示一个类似于[图11.34](#figureanchor11.34)的弹出窗口。
- en: '![c11_34.png](Images/c11_34.png)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![c11_34.png](Images/c11_34.png)'
- en: '[Figure 11.34](#figureanchor11.34) Docker push commands for dask-scheduler'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.34](#figureanchor11.34) dask-scheduler的Docker推送命令'
- en: Copy and run the commands from this dialog box, one after the other, in your
    Terminal or PowerShell window. You should see something similar to [figure 11.35](#figure11.35)
    while the build process runs. It may take several minutes to complete depending
    on the speed of your computer and internet connection.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 将此对话框中的命令逐个复制并运行到你的终端或PowerShell窗口中。在构建过程中，你应该会看到类似于[图11.35](#figureanchor11.35)的内容。根据你的计算机和互联网连接速度，完成可能需要几分钟。
- en: '![c11_35.png](Images/c11_35.png)'
  id: totrans-475
  prefs: []
  type: TYPE_IMG
  zh: '![c11_35.png](Images/c11_35.png)'
- en: '[Figure 11.35](#figureanchor11.35) Building the dask-scheduler image'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.35](#figureanchor11.35) 构建 dask-scheduler 图像'
- en: After the build completes, you can verify that the image was uploaded by clicking
    on the dask-scheduler repository. You should see a page similar to [figure 11.36](#figure11.36).
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 构建完成后，你可以通过点击dask-scheduler存储库来验证图像是否已上传。你应该看到一个类似于[图11.36](#figureanchor11.36)的页面。
- en: Now that the image has been uploaded to ECR, we need to tell ECS how to launch
    it in a container. This is done by creating an ECS task definition. Before we
    leave the ECR Manager page, copy the value in the Image URI column for the dask-scheduler
    image. We’ll need this value in just a moment.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 现在图像已上传到ECR，我们需要告诉ECS如何在容器中启动它。这是通过创建一个ECS任务定义来完成的。在我们离开ECR管理器页面之前，复制dask-scheduler图像的Image
    URI列中的值。我们很快就需要这个值。
- en: '![c11_36-R.png](Images/c11_36-R.png)'
  id: totrans-479
  prefs: []
  type: TYPE_IMG
  zh: '![c11_36-R.png](Images/c11_36-R.png)'
- en: '[Figure 11.36](#figureanchor11.36) Verifying the image exists in ECR'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.36](#figureanchor11.36) 验证图像存在于ECR中'
- en: To create a task definition for the scheduler image, click Task Definitions
    on the left menu of the ECR Manager page. You will be brought to the ECS Task
    Definitions Manager page. Click the blue Create a New Task Definition button to
    start the Create Task Definition wizard. On the first page, select EC2 as the
    launch type, as shown in [figure 11.37](#figure11.37).
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 要为调度器图像创建任务定义，点击ECR管理器页面左侧菜单中的任务定义（Task Definitions）。你将被带到ECS任务定义管理器页面。点击蓝色的创建新任务定义（Create
    a New Task Definition）按钮以启动创建任务定义向导。在第一页，选择EC2作为启动类型，如图[图11.37](#figureanchor11.37)所示。
- en: Click Next Step. On the next page, type `dask-scheduler` in the Task Definition
    Name field. Change the Network Mode drop-down to Host. Your screen should look
    similar to [figure 11.38](#figure11.38).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 点击下一步（Next Step）。在下一页，在任务定义名称（Task Definition Name）字段中输入`dask-scheduler`。将网络模式（Network
    Mode）下拉菜单更改为主机（Host）。你的屏幕应该类似于[图11.38](#figureanchor11.38)。
- en: '![c11_37.png](Images/c11_37.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![c11_37.png](Images/c11_37.png)'
- en: '[Figure 11.37](#figureanchor11.37) Selecting a launch type compatibility for
    the scheduler image'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.37](#figureanchor11.37) 选择调度器图像的启动类型兼容性'
- en: '![c11_38.png](Images/c11_38.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![c11_38.png](Images/c11_38.png)'
- en: '[Figure 11.38](#figureanchor11.38) Task definition configuration'
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.38](#figureanchor11.38) 任务定义配置'
- en: Leave the other settings as their defaults. Scroll down until you see the Volumes
    heading, and click Add Volume. You’ll see a dialog box appear to add a volume.
    Type `efs-data` in the Name field and `/efs` in the Source Path field. Your screen
    should look like [figure 11.39](#figure11.39).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 将其他设置保留为默认值。向下滚动，直到看到卷（Volumes）标题，然后点击添加卷（Add Volume）。你会看到一个对话框出现以添加卷。在名称（Name）字段中输入`efs-data`，在源路径（Source
    Path）字段中输入`/efs`。你的屏幕应该看起来像[图11.39](#figure11.39)。
- en: '![c11_39.png](Images/c11_39.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![c11_39.png](Images/c11_39.png)'
- en: '[Figure 11.39](#figureanchor11.39) Volume configuration'
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.39](#figureanchor11.39) 卷配置'
- en: Click the Add button to return to the task definition configuration page. Next,
    click the Add Container button under the Container Definitions heading. In the
    Container Name field, type `dask-scheduler`. Then, paste the image URI you copied
    from the ECR Manager page into the Image text box. Next, change the Memory Limits
    to a Soft limit of 700 MiB. Finally, add host and container port mappings for
    TCP port 8786 and 8787\. Your screen should look similar to [figure 11.40](#figure11.40).
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加”按钮返回任务定义配置页面。接下来，在“容器定义”标题下点击“添加容器”按钮。在“容器名称”字段中，输入`dask-scheduler`。然后，将您从ECR管理器页面复制的镜像URI粘贴到“镜像”文本框中。接下来，将内存限制更改为软限制700
    MiB。最后，添加TCP端口8786和8787的主机和容器端口映射。您的屏幕应类似于[图11.40](#figure11.40)。
- en: '![c11_40.png](Images/c11_40.png)'
  id: totrans-491
  prefs: []
  type: TYPE_IMG
  zh: '![c11_40.png](Images/c11_40.png)'
- en: '[Figure 11.40](#figureanchor11.40) Container configurations for dask-scheduler'
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.40](#figureanchor11.40) dask-scheduler的容器配置'
- en: Scroll down to the Storage and Logging section and configure the Mount Points
    to match [figure 11.41](#figure11.41).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 滚动到“存储和日志”部分，并配置挂载点以匹配[图11.41](#figure11.41)。
- en: Check the box labeled Auto-configure CloudWatch Logs and leave the default settings.
    Your screen should look similar to [figure 11.42](#figure11.42).
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 勾选标有“自动配置CloudWatch日志”的复选框，并保留默认设置。您的屏幕应类似于[图11.42](#figure11.42)。
- en: '![c11_41.png](Images/c11_41.png)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![c11_41.png](Images/c11_41.png)'
- en: '[Figure 11.41](#figureanchor11.41) Mount point configuration'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.41](#figureanchor11.41) 挂载点配置'
- en: '![c11_42.png](Images/c11_42.png)'
  id: totrans-497
  prefs: []
  type: TYPE_IMG
  zh: '![c11_42.png](Images/c11_42.png)'
- en: '[Figure 11.42](#figureanchor11.42) Logging settings'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.42](#figureanchor11.42) 日志设置'
- en: Finally, click Add to finish adding the container to the task definition. Your
    screen should now look similar to [figure 11.43](#figure11.43).
  id: totrans-499
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，点击“添加”以完成将容器添加到任务定义中。现在，您的屏幕应类似于[图11.43](#figure11.43)。
- en: Click Create to finish creating the task definition and observe that the task
    definition was created successfully. We now have a template that we can use to
    launch copies of our dask-scheduler image! Now all we have to do is launch it.
    To do this, we have to create an ECS *service* that binds to the task definition.
    First, navigate back to the ECS Task Definitions Manager page. Then, select the
    check box next to the dask-scheduler task definition, click the Actions button,
    and select Create Service. This will launch the Create Service wizard.
  id: totrans-500
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“创建”以完成创建任务定义，并观察任务定义是否创建成功。我们现在有一个模板，我们可以用它来启动dask-scheduler镜像的副本！现在我们只需要启动它。为此，我们必须创建一个与任务定义绑定的ECS
    *服务*。首先，导航回ECS任务定义管理器页面。然后，选中dask-scheduler任务定义旁边的复选框，点击“操作”按钮，并选择“创建服务”。这将启动创建服务向导。
- en: First, select EC2 for the Launch type. Next, type `dask-scheduler` into the
    Service Name field. Then, type `1` into the Number of Tasks field and select One
    Task Per Host from the Placement Templates drop-down. Your screen should look
    like [figure 11.44](#figure11.44).
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，选择“EC2”作为启动类型。接下来，在“服务名称”字段中输入`dask-scheduler`。然后，在“任务数量”字段中输入`1`，并从放置模板下拉菜单中选择“每主机一个任务”。您的屏幕应类似于[图11.44](#figure11.44)。
- en: Click the Next step button. On the next page, uncheck the check box next to
    Enable Service Discovery Integration. Leave the other settings as default. Click
    Next Step. Leave the settings as default and click Next Step. Finally, at the
    review screen, click Create Service. You will be taken to the Launch Status screen.
    Click the View Service button to be taken to the cluster status screen. Your screen
    should now look similar to [figure 11.45](#figure11.45).
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“下一步”按钮。在下一页，取消勾选“启用服务发现集成”旁边的复选框。保留其他设置作为默认设置。点击“下一步”。保留设置并点击“下一步”。最后，在审查屏幕上，点击“创建服务”。您将被带到启动状态屏幕。点击“查看服务”按钮，将被带到集群状态屏幕。现在，您的屏幕应类似于[图11.45](#figure11.45)。
- en: '![c11_43.png](Images/c11_43.png)'
  id: totrans-503
  prefs: []
  type: TYPE_IMG
  zh: '![c11_43.png](Images/c11_43.png)'
- en: '[Figure 11.43](#figureanchor11.43) Complete container configuration'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.43](#figureanchor11.43) 完整容器配置'
- en: '![c11_44.png](Images/c11_44.png)'
  id: totrans-505
  prefs: []
  type: TYPE_IMG
  zh: '![c11_44.png](Images/c11_44.png)'
- en: '[Figure 11.44](#figureanchor11.44) Service configuration for dask-scheduler'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.44](#figureanchor11.44) dask-scheduler的服务配置'
- en: '![c11_45.png](Images/c11_45.png)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![c11_45.png](Images/c11_45.png)'
- en: '[Figure 11.45](#figureanchor11.45) Status of dask-cluster'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.45](#figureanchor11.45) dask-cluster的状态'
- en: To get more information about the dask-scheduler service, click on the blue
    dask-scheduler link. This will bring you to the service status page. After a few
    minutes, you should notice that you have one running task, similar to [figure
    11.46](#figure11.46).
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取有关dask-scheduler服务的更多信息，请单击蓝色的dask-scheduler链接。这将带您到服务状态页面。几分钟之后，您应该会注意到有一个正在运行的任务，类似于[图11.46](#figure11.46)。
- en: '![c11_46.png](Images/c11_46.png)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![c11_46.png](Images/c11_46.png)'
- en: '[Figure 11.46](#figureanchor11.46) Example of a running task'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.46](#figureanchor11.46) 运行任务的示例'
- en: If the service is sitting in a Pending state, wait a few more minutes and refresh
    the page. Sometimes it can take a few minutes for ECS to get set up with the image.
    Once the task is in a Running state, click the link under the Task column to pull
    up the task details. On the task detail page, you will see a link next to a field
    called EC2 Instance ID. Click the link to be taken to the EC2 Instance Manager.
    This is the EC2 instance where the dask-scheduler container is running! The public
    DNS name and public IP can be used to connect to the scheduler from outside AWS,
    such as when you want to log in to the monitoring dashboard. Copy down the public
    DNS name because we’ll open the diagnostic dashboard after we’ve connected the
    workers to the cluster. An example of this information is shown in [figure 11.47](#figure11.47).
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 如果服务处于挂起状态，请等待几分钟并刷新页面。有时ECS设置镜像可能需要几分钟时间。一旦任务处于运行状态，点击任务列下的链接以查看任务详情。在任务详情页面上，您将看到一个名为EC2实例ID的字段旁边的链接。点击该链接将被带到EC2实例管理器。这就是dask-scheduler容器正在运行的EC2实例！公共DNS名称和公共IP可以用来从AWS外部连接到调度器，例如您想登录到监控仪表板时。请记下公共DNS名称，因为我们将在将工作节点连接到集群后打开诊断仪表板。此信息的示例显示在[图11.47](#figure11.47)中。
- en: '![c11_47.png](Images/c11_47.png)'
  id: totrans-513
  prefs: []
  type: TYPE_IMG
  zh: '![c11_47.png](Images/c11_47.png)'
- en: '[Figure 11.47](#figureanchor11.47) Example IP and DNS information of an EC2
    instance'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.47](#figureanchor11.47) EC2实例的示例IP和DNS信息'
- en: 'Now we’re ready to deploy the images for dask-worker and dask-notebook! To
    do this, follow the exact same steps you followed to deploy and launch the dask-scheduler
    image, using the Dockerfiles in the worker and notebook folders, respectively.
    However, when creating the task definition and services for each, note a few exceptions
    in the configuration:'
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已准备好部署dask-worker和dask-notebook的镜像！为此，按照您部署和启动dask-scheduler镜像时遵循的相同步骤，分别使用工作节点和笔记本文件夹中的Dockerfile。然而，在创建每个任务的定义和服务时，请注意配置中的几个例外：
- en: '**For the dask-worker image**'
  id: totrans-516
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于dask-worker镜像**'
- en: The Port mappings on the Add Container screen should be set to 8000 tcp instead
    of 8786 tcp and 8787 tcp.
  id: totrans-517
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在添加容器屏幕上的端口映射应设置为8000 tcp，而不是8786 tcp和8787 tcp。
- en: The Number of Tasks field on the Step 1 screen of the Create Service wizard
    should be set to **6** instead of 1.
  id: totrans-518
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建服务向导的第1步屏幕上的任务数量字段应设置为**6**，而不是1。
- en: '**For the dask-notebook image**'
  id: totrans-519
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**对于dask-notebook镜像**'
- en: The Network Mode drop-down box on the Step 2 screen of the Create Task Definition
    wizard should be set to Default instead of Host.
  id: totrans-520
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在创建任务定义向导的第2步屏幕上的网络模式下拉框应设置为默认值，而不是主机模式。
- en: The Port mappings on the Add Container screen should be set to 8888 tcp instead
    of 8786 tcp and 8787 tcp.
  id: totrans-521
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在添加容器屏幕上的端口映射应设置为8888 tcp，而不是8786 tcp和8787 tcp。
- en: Once you’ve verified that you have one running instance of the dask-scheduler
    service, one running instance of the dask-notebook service, and six running instances
    of the dask-worker service, we can connect to the cluster and begin running some
    jobs.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您已验证您有一个正在运行的dask-scheduler服务实例、一个正在运行的dask-notebook服务实例以及六个正在运行的dask-worker服务实例，我们就可以连接到集群并开始运行一些作业。
- en: 11.1.8 Connecting to the cluster
  id: totrans-523
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1.8 连接到集群
- en: The dask-notebook image contains a Jupyter Notebook server, which we’ll use
    to interact with our Dask cluster. When Dask is running in cluster mode, it also
    offers some additional diagnostics tools that show how workloads are being distributed
    over the cluster. This is useful to see if a worker has gotten stuck, or to generally
    track the progress of a job. Before connecting to the notebook server, we’ll take
    a look at the diagnostics page. To access the diagnostics page on your Dask cluster,
    open a web browser and type `http://<your scheduler hostname>:8787`, replacing
    <your scheduler hostname> with the public DNS value you copied down two pages
    ago. When the page loads, click Workers on the top menu. You should see a screen
    similar to [figure 11.48](#figure11.48).
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: dask-notebook 映像包含一个 Jupyter Notebook 服务器，我们将使用它来与我们的 Dask 集群交互。当 Dask 以集群模式运行时，它还提供一些额外的诊断工具，这些工具显示了工作负载如何在集群中分布。这有助于查看是否有工作节点卡住，或一般跟踪作业的进度。在连接到笔记本服务器之前，我们将查看诊断页面。要访问
    Dask 集群上的诊断页面，请打开网络浏览器并在地址栏中键入 `http://<your scheduler hostname>:8787`，将 <your
    scheduler hostname> 替换为您之前在两页前记录的公网 DNS 值。当页面加载时，点击顶部菜单中的“工作节点”。您应该看到一个类似于 [图
    11.48](#figure11.48) 的屏幕。
- en: '![c11_48.png](Images/c11_48.png)'
  id: totrans-525
  prefs: []
  type: TYPE_IMG
  zh: '![c11_48.png](Images/c11_48.png)'
- en: '[Figure 11.48](#figureanchor11.48) Dask cluster worker status'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.48](#figureanchor11.48) Dask 集群工作节点状态'
- en: You should see six rows in the table. These correspond with each worker in the
    cluster. You can see each worker’s current CPU usage, memory usage, and network
    activity. This page is useful to keep an eye on the overall health of the workers.
    Keep this window open because we’ll come back to it later in the chapter.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该在表中看到六行。这些对应于集群中的每个工作节点。您可以看到每个工作节点的当前 CPU 使用率、内存使用率和网络活动。此页面有助于监控工作节点的整体健康状况。请保持此窗口打开，因为我们将在本章后面再次回到它。
- en: Finally, we’ll connect to the notebook server. To do this, look up the public
    DNS name of the dask-notebook container by following the steps you took to find
    the public DNS name for the dask-scheduler container. Once you’ve copied the public
    DNS name for the notebook server, open a web browser and type `http://<your notebook
    hostname>:8888` in the address bar, replacing <your notebook hostname> with the
    public DNS name you copied. Your screen should look similar to [figure 11.49](#figure11.49).
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将连接到笔记本服务器。为此，按照您查找 dask-scheduler 容器公网 DNS 名称的步骤查找 dask-notebook 容器的公网
    DNS 名称。一旦您复制了笔记本服务器的公网 DNS 名称，请打开网络浏览器并在地址栏中键入 `http://<your notebook hostname>:8888`，将
    <your notebook hostname> 替换为您复制的公网 DNS 名称。您的屏幕应类似于 [图 11.49](#figure11.49)。
- en: '![c11_49.png](Images/c11_49.png)'
  id: totrans-529
  prefs: []
  type: TYPE_IMG
  zh: '![c11_49.png](Images/c11_49.png)'
- en: '[Figure 11.49](#figureanchor11.49) Jupyter login screen'
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.49](#figureanchor11.49) Jupyter 登录界面'
- en: To look up the login token, go back to your web browser window where you’re
    logged in to the AWS Console and go to the ECS Dashboard. Navigate to the task
    details for the currently running task in the dask-notebook service, then click
    the Logs tab. Your screen should look similar to [figure 11.50](#figure11.50).
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 要查找登录令牌，请返回您的网络浏览器窗口，其中您已登录到 AWS 控制台，并转到 ECS 仪表板。导航到 dask-notebook 服务中当前运行的任务的任务详情，然后点击日志选项卡。您的屏幕应类似于
    [图 11.50](#figure11.50)。
- en: '![c11_50.png](Images/c11_50.png)'
  id: totrans-532
  prefs: []
  type: TYPE_IMG
  zh: '![c11_50.png](Images/c11_50.png)'
- en: '[Figure 11.50](#figureanchor11.50) Task logging screen'
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.50](#figureanchor11.50) 任务日志界面'
- en: We configured every task definition to send logs to AWS CloudWatch, so we’re
    able to access the raw logs for any running container through the ECS Dashboard.
    If you ever need more insight into what’s happening behind the scenes for a particular
    service, take a look at the logs. By default, Jupyter prints the login token in
    the logs when it starts up. Scroll down until you find the log entry lines that
    contain the login token. Copy this value and paste it into the Jupyter login window,
    then click Log In. Your screen should look similar to [figure 11.51](#figure11.51).
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已将每个任务定义配置为将日志发送到 AWS CloudWatch，因此我们可以通过 ECS 仪表板访问任何运行容器的原始日志。如果您需要深入了解特定服务的幕后情况，请查看日志。默认情况下，Jupyter
    启动时会将登录令牌打印到日志中。向下滚动，直到找到包含登录令牌的日志条目行。复制此值并将其粘贴到 Jupyter 登录窗口中，然后点击登录。您的屏幕应类似于
    [图 11.51](#figure11.51)。
- en: '![c11_51-R.png](Images/c11_51-R.png)'
  id: totrans-535
  prefs: []
  type: TYPE_IMG
  zh: '![c11_51-R.png](Images/c11_51-R.png)'
- en: '[Figure 11.51](#figureanchor11.51) The Jupyter Lab window'
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.51](#figureanchor11.51) Jupyter Lab 窗口'
- en: From here, we’re ready to start running code on the cluster! In the next section,
    we’ll have a look at how to upload notebooks to the notebook server and monitor
    the execution of jobs on the cluster.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们就可以在集群上运行代码了！在下一节中，我们将探讨如何将笔记本上传到笔记本服务器以及如何在集群上监控作业的执行。
- en: 11.2 Running and monitoring Dask jobs on a cluster
  id: totrans-538
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 在集群上运行和监控 Dask 作业
- en: 'For this section, we’ll return to the sentiment classifier problem we looked
    at in chapter 10 through the following scenario:'
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本节，我们将通过以下场景回到我们在第 10 章中探讨的情感分类器问题：
- en: Using the Amazon Fine Foods dataset, build a sentiment classifier model using
    the Dask cluster in AWS and monitor the execution of the jobs.
  id: totrans-540
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 使用 Amazon Fine Foods 数据集，在 AWS 中的 Dask 集群上构建情感分类器模型，并监控作业的执行。
- en: For the sake of brevity, the chapter 11 notebook is an abbreviated excerpt of
    the chapter 10 notebook. Rather than running the full process from raw data to
    a complete sentiment classifier model, the data you uploaded to EFS in the previous
    section contains the ZARR files that were generated in chapter 10\. Therefore,
    the chapter 11 notebook is just a short example of building the classifier model
    from the preprocessed data, highlighting the minor differences necessary to run
    the code on the cluster. After walking through the chapter 11 notebook, you will
    be able to modify any of the notebooks from previous chapters to run on your cluster
    if you so desire. To begin, upload the chapter 11 notebook to your Jupyter Notebook
    server.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简洁起见，第 11 章笔记本是第 10 章笔记本的简略摘录。它不是从原始数据到完整的情感分类器模型的完整流程，而是包含了上一节上传到 EFS 的数据中生成的
    ZARR 文件。因此，第 11 章笔记本只是构建分类器模型的一个简短示例，突出了在集群上运行代码所需的微小差异。在浏览完第 11 章笔记本后，如果你愿意，你可以修改之前章节中的任何笔记本以在你的集群上运行。首先，将第
    11 章笔记本上传到你的 Jupyter Notebook 服务器。
- en: On the Jupyter Notebook server home screen, click the up-arrow icon in the file
    explorer pane, right under the Settings menu. The location of this button is shown
    in [figure 11.52](#figure11.52).
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Jupyter Notebook 服务器的首页上，点击文件资源管理器窗格中设置菜单下方向上的箭头图标。此按钮的位置在[图 11.52](#figure11.52)中显示。
- en: '![c11_52-R.png](Images/c11_52-R.png)'
  id: totrans-543
  prefs: []
  type: TYPE_IMG
  zh: '![c11_52-R.png](Images/c11_52-R.png)'
- en: '[Figure 11.52](#figureanchor11.52) Uploading a notebook to the Jupyter Notebook
    server'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.52](#figureanchor11.52) 将笔记本上传到 Jupyter Notebook 服务器'
- en: 'Navigate to the location of the chapter 11 notebook and click Upload. After
    a few seconds, you should see the notebook appear in the file explorer pane beneath
    the work folder. Double-click the notebook to open it in a new tab. As mentioned
    before, the code in this notebook is exactly the same as chapter 10\. Only two
    differences allow the code to run on the cluster: using the Distributed Client
    interface, and a small change to the filesystem path where the ZARR files are
    stored.'
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 导航到第 11 章笔记本的位置，然后点击上传。几秒钟后，你应该会在工作文件夹下方的文件资源管理器窗格中看到笔记本出现。双击笔记本以在新标签页中打开它。如前所述，这个笔记本中的代码与第
    10 章完全相同。只有两个差异使得代码能够在集群上运行：使用分布式客户端接口，以及将 ZARR 文件存储的文件系统路径进行的小幅修改。
- en: The Distributed Client interface is the key to making the code run on the cluster
    rather than running locally on the notebook server.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式客户端接口是使代码在集群上运行而不是在笔记本服务器上本地运行的关键。
- en: Listing 11.2 Initializing the Distributed Client
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 初始化分布式客户端
- en: '[PRE49]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: All we need to do in this case is simply initialize the Client. After this code
    executes, any compute-type Dask methods (such as `compute`, `head`, and so on)
    will be sent to the cluster rather than executed locally. We can, optionally,
    pass an IP address and port for the scheduler to the Client object, but in this
    case it’s unnecessary. This is because the Dockerfile change that you made for
    the notebook server image in the previous section was adding an environment variable
    that holds the URI for the scheduler. In the absence of an explicitly passed scheduler
    URI in the `Client` constructor, it will read the value of the `DASK_SCHEDULER_ADDRESS`
    environment variable. After executing this code, you should see a result similar
    to [figure 11.53](#figure11.53).
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个情况下，我们只需要简单地初始化客户端。在这段代码执行之后，任何计算类型的 Dask 方法（例如 `compute`、`head` 等）都将发送到集群而不是在本地执行。我们可以选择性地将调度器的
    IP 地址和端口传递给客户端对象，但在这个情况下是不必要的。这是因为你在上一节中对笔记本服务器镜像所做的 Dockerfile 修改是添加了一个包含调度器
    URI 的环境变量。如果没有在 `Client` 构造函数中显式传递调度器 URI，它将读取 `DASK_SCHEDULER_ADDRESS` 环境变量的值。执行此代码后，你应该会看到一个类似于
    [图 11.53](#figure11.53) 的结果。
- en: '![c11_53.eps](Images/c11_53.png)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![c11_53.eps](Images/c11_53.png)'
- en: '[Figure 11.53](#figureanchor11.53) Client statistics for the cluster'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.53](#figureanchor11.53) 集群的客户端统计信息'
- en: This information shows that six workers are in the Dask cluster, just as we
    should expect! Now we can run any Dask code and it will execute on the cluster.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 这个信息显示，有六个工作节点在 Dask 集群中，正如我们预期的那样！现在我们可以运行任何 Dask 代码，并且它将在集群上执行。
- en: The second change made to the notebook for the cluster is in the second cell.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 对集群笔记本所做的第二个更改是在第二个单元格中。
- en: Listing 11.3 Changing the file paths for the data
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.3 更改数据文件路径
- en: '[PRE50]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see, the data files being referenced are in the /data folder. This
    is because the mountpoint we set up in the task definitions in the last section
    exposes the /efs folder from the EC2 instances to the /data folder inside the
    containers. This means any data you copy to the /efs folder on one of the EC2
    instances will instantly be available to your notebook server and workers in their
    /data folder. If you want to analyze other datasets on your cluster, use the steps
    you followed to upload the *arrays.tar* file to EFS using SCP in the previous
    section.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，被引用的数据文件位于 /data 文件夹中。这是因为我们在上一节的任务定义中设置的挂载点将 EC2 实例中的 /efs 文件夹暴露给容器内的
    /data 文件夹。这意味着你将任何数据复制到 EC2 实例上的 /efs 文件夹，这些数据将立即在笔记本服务器和其 /data 文件夹中的工作节点上可用。如果你想分析集群上的其他数据集，使用你之前在上一节中使用的步骤，通过
    SCP 将 *arrays.tar* 文件上传到 EFS。
- en: Lastly, before you execute the remaining cells in the notebook, go back to the
    Dask diagnostics window and click the Status link on the top menu. The Status
    page provides detailed information about the execution of Dask jobs. Once you
    have that page visible, execute the remaining cells in the notebook. This will
    kick off the process to build the sentiment classifier model on the cluster, and
    you will be able to see the details of that process on the diagnostics page. An
    example of the diagnostics page is shown in [figure 11.54](#figure11.54).
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在你执行笔记本中剩余的单元格之前，回到 Dask 诊断窗口，点击顶部菜单中的状态链接。状态页面提供了关于 Dask 作业执行的详细信息。一旦你看到该页面，就可以执行笔记本中剩余的单元格。这将启动在集群上构建情感分类器模型的过程，你将能够在诊断页面上看到该过程的详细信息。诊断页面的一个示例显示在
    [图 11.54](#figure11.54) 中。
- en: '![c11_54.eps](Images/c11_54.png)'
  id: totrans-558
  prefs: []
  type: TYPE_IMG
  zh: '![c11_54.eps](Images/c11_54.png)'
- en: '[Figure 11.54](#figureanchor11.54) An example of the Dask diagnostics page'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 11.54](#figureanchor11.54) Dask 诊断页面的示例'
- en: 'Four sections of the Status page provide different information about the progression
    of the job through the cluster:'
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 状态页面的四个部分提供了关于作业在集群中进度的不同信息：
- en: Memory pressure
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存压力
- en: Worker-level task queue
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点级别的任务队列
- en: Task stream
  id: totrans-563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务流
- en: Progress
  id: totrans-564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进度
- en: Starting from the top-left, under Bytes stored, is the memory pressure information.
    The number of bytes stored lets us know how much data is being held in memory
    globally across the cluster. This number will generally fluctuate up and down
    as data processes. The graph shows the memory pressure for each worker. A blue
    bar signifies the worker has plenty of memory to spare, whereas a yellow bar signifies
    the worker is running out of memory and may have to spill data to disk. If you
    have jobs that run slowly or crash randomly, keep an eye on the memory pressure
    to ensure workers aren’t running out of memory. If workers are consistently under
    a lot of memory pressure, it may be a good idea to repartition your dataset and
    increase the number of partitions. Smaller chunks of data will fit in memory easier
    and lowers the need to spill to disk.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 从左上角开始，在“存储的字节数”下面是内存压力信息。存储的字节数让我们知道整个集群中内存中保留的数据量。这个数字通常会随着数据处理上下波动。图表显示了每个工作者的内存压力。蓝色条表示工作者有足够的内存空间，而黄色条表示工作者可能快要用完内存，可能需要将数据溢出到磁盘。如果你有运行缓慢或随机崩溃的作业，请密切关注内存压力，以确保工作者不会用完内存。如果工作者持续承受大量内存压力，那么重新分区你的数据集并增加分区数量可能是个好主意。较小的数据块更容易放入内存，并降低需要溢出到磁盘的需求。
- en: Below the memory pressure graph is the worker-level task queue section. This
    shows a simple count of the number of tasks that are currently queued up to execute
    on each worker based on the scheduler’s current execution plan. A blue bar indicates
    an acceptable number of tasks in queue, whereas a red bar indicates the worker
    is being starved for work. This typically happens when one worker has a dependency
    on some data that another worker is working on. Generally, tasks should be spread
    out evenly across workers. If one worker has a much higher number of tasks in
    queue than other workers, there may be something wrong with that worker that’s
    causing it to process more slowly than other workers. It would be a good idea
    to check and see if another process outside of Dask is running on that worker
    or if it is having other performance issues.
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 在内存压力图表下方是工作者级任务队列部分。这部分显示了根据调度器的当前执行计划，每个工作者当前排队等待执行的任务数量。蓝色条表示队列中可接受的任务数量，而红色条表示工作者正在等待工作。这通常发生在某个工作者依赖于其他工作者正在处理的一些数据时。通常，任务应该在工作者之间均匀分布。如果一个工作者的队列中任务数量远高于其他工作者，那么可能存在某个问题导致该工作者处理速度比其他工作者慢。检查并查看该工作者上是否运行了Dask之外的其他进程或是否存在其他性能问题是个好主意。
- en: To the right of the worker-level task queue section is the job progress section.
    This shows how many tasks are pending, how many have completed, and if any errors
    occurred. The progress bars will fill up over time as the job nears completion.
    Any errored tasks will be retried on another worker.
  id: totrans-567
  prefs: []
  type: TYPE_NORMAL
  zh: 在工作者级任务队列部分右侧是作业进度部分。这部分显示了待处理任务的数量、已完成任务的数量以及是否发生错误。随着作业接近完成，进度条会逐渐填满。任何发生错误的任务将在另一个工作者上重试。
- en: Finally, above the job progress section is the task stream section. This shows
    how long each task is taking to complete on each worker. The colors of the bars
    correlate to the color of the progress bars. For example, if a `pandas_read` task
    has a green progress bar, the duration of `pandas_read` tasks will show up in
    the task stream as green bars. This can be used to spot inefficiencies in slow
    jobs. If a certain type of operation is taking a long time, it may be possible
    to refactor your code to be more efficient. It can also be used to help spot performance
    issues with individual workers similar to the worker-level task queue. For example,
    if a worker is regularly taking 100 ms to complete a `pandas_read` task and other
    workers are only taking 50 ms per task, there may be something wrong with that
    particular worker causing it to slow down.
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在作业进度部分上方是任务流部分。这部分显示了每个工作者完成每个任务所需的时间。条形的颜色与进度条的颜色相对应。例如，如果一个`pandas_read`任务的进度条是绿色的，那么`pandas_read`任务的持续时间将在任务流中以绿色条显示。这可以用来发现慢作业中的低效之处。如果某种类型的操作耗时较长，可能可以通过重构代码使其更高效。它还可以用来帮助发现与工作者级任务队列类似的工作者的性能问题。例如，如果一个工作者通常需要100毫秒来完成一个`pandas_read`任务，而其他工作者每个任务只需要50毫秒，那么可能存在某个特定工作者的问题，导致其运行速度变慢。
- en: An alternative way to view the task stream is by viewing the underlying DAG
    (directed acyclic graphs). The diagnostic dashboard actually lets you view the
    underlying DAG in real-time as the job processes. To view the DAG, click the Graph
    link on the top menu. An example of the Graph page is shown in [figure 11.55](#figure11.55).
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种查看任务流的方法是查看底层的DAG（有向无环图）。诊断仪表板实际上允许你在作业处理时实时查看底层的DAG。要查看DAG，请点击顶部菜单中的“图形”链接。图形页面的一个示例显示在[图11.55](#figure11.55)中。
- en: '![c11_55.eps](Images/c11_55.png)'
  id: totrans-570
  prefs: []
  type: TYPE_IMG
  zh: '![c11_55.eps](Images/c11_55.png)'
- en: '[Figure 11.55](#figureanchor11.55) Real-time DAG view'
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.55](#figureanchor11.55) 实时DAG视图'
- en: The DAGs on this page are always read from left to right. Green blocks indicate
    a task that’s currently processing. Gray blocks indicate tasks that are waiting
    for upstream dependencies, such as the blocks to the right side of the graph.
    Red blocks indicate tasks that have completed, and their result is being held
    in memory. All upstream dependencies of a downstream task will be held in memory
    until the downstream task has finished processing. At that point, the data from
    the upstream tasks will be released from memory, and the blocks will turn blue.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 此页上的DAG总是从左到右读取。绿色块表示当前正在处理的任务。灰色块表示等待上游依赖的任务，例如图右侧的块。红色块表示已完成任务的块，其结果正在内存中保留。下游任务的全部上游依赖项将在下游任务完成处理之前保留在内存中。在此之后，上游任务的数据将从内存中释放，块将变为蓝色。
- en: 11.3 Cleaning up the Dask cluster on AWS
  id: totrans-573
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 清理AWS上的Dask集群
- en: The last thing we’ll cover is how to clean up the services in AWS. As mentioned
    earlier in the chapter, AWS uses usage-based billing. This means, for example,
    that whenever an EC2 instance is put into a Running state, AWS starts timing how
    long the EC2 instance has been up for and charges per minute of usage. The AWS
    Free Tier includes 750 hours of EC2 usage per month. With eight EC2 instances
    up in our cluster configuration, this uses eight hours of time per clock hour
    that the instances are up. This means that the cluster can be kept online for
    93 hours, or just under four days, without incurring charges. Fortunately, you
    can easily turn the cluster on and off using the Auto Scaling Group we configured
    earlier in the chapter.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要讨论的最后一件事是如何清理AWS中的服务。如本章前面所述，AWS使用基于使用量的计费。这意味着，例如，每当一个EC2实例被置于运行状态时，AWS就开始计时该EC2实例已运行的时间，并按分钟计费。AWS免费层每月包括750小时的EC2使用量。在我们的集群配置中，有八个EC2实例运行，这意味着每个实例运行的小时数使用了八个小时。这意味着集群可以在线保持93小时，即不到四天，而不会产生费用。幸运的是，你可以通过本章前面配置的自动扩展组轻松地打开和关闭集群。
- en: To shut down the EC2 instances, simply return to the EC2 Dashboard in AWS Console,
    click Auto Scaling Groups under the Auto Scaling heading in the left side menu.
    Then, select the auto scaling group for the ECS cluster (there should still only
    be one), and click Edit. In the Desired Capacity field, change the 8 to a 0 and
    click Save. Your screen should look similar to [figure 11.56](#figure11.56).
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 要关闭EC2实例，只需返回AWS控制台的EC2仪表板，在左侧菜单下的自动扩展部分点击“自动扩展组”。然后，选择ECS集群的自动扩展组（应该只有一个），点击编辑。在期望容量字段中，将8改为0并点击保存。你的屏幕应该看起来类似于[图11.56](#figure11.56)。
- en: '![c11_56.eps](Images/c11_56.png)'
  id: totrans-576
  prefs: []
  type: TYPE_IMG
  zh: '![c11_56.eps](Images/c11_56.png)'
- en: '[Figure 11.56](#figureanchor11.56) Shutting down the EC2 auto scaling group'
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: '[图11.56](#figureanchor11.56) 关闭EC2自动扩展组'
- en: After a few minutes, the EC2 instances will begin shutting down. You can verify
    this by checking the EC2 Instances Manager and observing that the running EC2
    instances are now either in a Shutting Down or Terminated state. To start the
    cluster back up later, you can simply change the Desired Capacity from 0 back
    to 8 (or whatever number of instances you’d like to bring up).
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 几分钟后，EC2实例将开始关闭。你可以通过检查EC2实例管理器并观察现在运行的EC2实例现在处于关闭或终止状态来验证这一点。要稍后重新启动集群，你只需将期望容量从0改回8（或你希望启动的实例数量）。
- en: The other services to consider are EFS and ECR. Since these are both storage
    services, they are billed based on the size of storage being consumed. As long
    as you don’t upload more than 5 GB total of data to the /efs folder, you will
    stay within the Free Tier limits for EFS.
  id: totrans-579
  prefs: []
  type: TYPE_NORMAL
  zh: 需要考虑的其他服务是EFS和ECR。由于这两个都是存储服务，因此它们的计费基于所消耗的存储大小。只要你不将超过5 GB的总数据上传到/efs文件夹，你就可以保持在EFS的免费层限制内。
- en: Unfortunately, ECR is a bit more restrictive. The Free Tier limit for ECR is
    500 MB per month. Storage on a per-month basis means an average of the storage
    used over a month is calculated, and if that average exceeds 500 MB, then there
    will be billable charges for usage. The total space consumed by the three Dask
    cluster images is about 2 GB, so if these images are left in ECR for more than
    about a week, you will exceed the ECR Free Tier limits. Based on ECR pricing of
    $0.10-per-GB/month for storage, it would cost $0.20 to store the images in ECR
    for the entire month. If you want to avoid all billable charges, you must delete
    the ECR repositories when you are finished with the exercise. You can do this
    from the ECR Repository Manager screen. However, this unfortunately means that
    if you wish to resume working with the cluster at a later time, you will have
    to re-deploy the images and re-create the ECS services.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，ECR（Elastic Container Registry）的限制稍微严格一些。ECR的免费层每月限制为500 MB。按月存储意味着计算一个月内使用的存储平均量，如果这个平均值超过500
    MB，那么将产生可计费的使用费用。三个Dask集群映像的总空间大约为2 GB，所以如果这些映像在ECR中保留超过大约一周，你将超出ECR免费层的限制。根据ECR的存储定价，每GB/月0.10美元，存储这些映像整个月将花费0.20美元。如果你想避免所有可计费的费用，你必须在你完成练习后删除ECR仓库。你可以从ECR仓库管理器屏幕中这样做。然而，这不幸地意味着，如果你希望在以后的时间重新开始与集群一起工作，你必须重新部署映像并重新创建ECS服务。
- en: Once you’ve shut down the EC2 instances and removed the ECR repositories, all
    usage-based billing will be stopped.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你关闭了EC2实例并删除了ECR仓库，所有基于使用的计费都将停止。
- en: Summary
  id: totrans-582
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A Dask cluster can be built in the cloud using Amazon AWS, Docker, and ECS,
    allowing you to easily scale the size of the cluster based on your workload’s
    needs.
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用Amazon AWS、Docker和ECS在云中构建Dask集群，这让你可以根据工作负载的需求轻松调整集群的大小。
- en: Jobs are submitted to a Dask cluster using the distributed task scheduler client.
    The distributed task scheduler divides up and organizes the work across the entire
    cluster and ships the results back to the end user.
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用分布式任务调度器客户端将作业提交到Dask集群。分布式任务调度器将工作分割和组织到整个集群中，并将结果发送回最终用户。
- en: The distributed task scheduler has a diagnostics page that runs on port 8787,
    allowing you to monitor the execution of jobs and identify problems with the cluster.
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式任务调度器有一个运行在端口8787的诊断页面，允许你监控作业的执行并识别集群的问题。
- en: EC2 Auto Scaling groups can be used to quickly boot up and shut down the cluster,
    allowing you to control resource costs and easily clean up when you’re finished.
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用EC2自动扩展组快速启动和关闭集群，这让你能够控制资源成本，并在完成工作后轻松清理。
