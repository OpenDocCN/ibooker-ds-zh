- en: 7 Fairly simple neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7. 相对简单的神经网络
- en: When we hear about advances in artificial intelligence these days, they generally
    concern a particular subdiscipline known as *machine learning* (computers learning
    some new information without being explicitly told it). More often than not, those
    advances are being driven by a particular machine learning technique known as
    *neural networks*. Although they were invented decades ago, neural networks have
    been going through a kind of renaissance as improved hardware and newly discovered
    research-driven software techniques enable a new paradigm known as *deep learning*.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们今天听到人工智能的进步时，它们通常涉及一个被称为**机器学习**（计算机在没有明确告知的情况下学习一些新信息）的特定子领域。更常见的是，这些进步是由一种称为**神经网络**的特定机器学习技术驱动的。尽管它们几十年前就被发明了，但神经网络正经历一种复兴，因为改进的硬件和新的研究驱动的软件技术使得一种称为**深度学习**的新范式成为可能。
- en: Deep learning has turned out to be a broadly applicable technique. It has been
    found useful in everything from hedge-fund algorithms to bioinformatics. Two deep
    learning applications that consumers have become familiar with are image recognition
    and speech recognition. If you have ever asked your digital assistant what the
    weather is or had a photo program recognize your face, there was probably some
    deep learning going on.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习已经证明是一种广泛适用的技术。它被发现对从对冲基金算法到生物信息学等各个方面都很有用。消费者已经熟悉的两个深度学习应用是图像识别和语音识别。如果你曾经询问你的数字助手天气如何，或者有一个照片程序识别你的脸，那么可能就有一些深度学习在发挥作用。
- en: Deep learning techniques utilize the same building blocks as simpler neural
    networks. In this chapter, we will explore those blocks by building a simple neural
    network. It will not be state of the art, but it will give you a basis for understanding
    deep learning (which is based on more complex neural networks than we will build).
    Most practitioners of machine learning do not build neural networks from scratch.
    Instead, they use popular, highly optimized, off-the-shelf frameworks that do
    the heavy lifting. Although this chapter will not help you learn how to use any
    specific framework, and the network we will build will not be useful for a real-world
    application, it will help you understand how those frameworks work at a low level.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习技术利用与简单神经网络相同的构建块。在本章中，我们将通过构建一个简单的神经网络来探索这些块。它可能不是最先进的，但它将为你理解深度学习（它基于比我们构建的更复杂的神经网络）打下基础。大多数机器学习实践者不会从头开始构建神经网络。相反，他们使用流行的、高度优化的现成框架，这些框架承担了繁重的工作。尽管本章不会帮助你学习如何使用任何特定的框架，而且我们将构建的网络对于实际应用来说可能没有用，但它将帮助你理解这些框架在底层是如何工作的。
- en: 7.1 Biological basis?
  id: totrans-4
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 生物基础？
- en: The human brain is the most incredible computational device in existence. It
    cannot crunch numbers as fast as a microprocessor, but its ability to adapt to
    new situations, learn new skills, and be creative is unsurpassed by any known
    machine. Since the dawn of computers, scientists have been interested in modeling
    the brain’s machinery. Each nerve cell in the brain is known as a *neuron*. Neurons
    in the brain are networked to one another via connections known as *synapses*.
    Electricity passes through synapses to power these networks of neurons--also known
    as *neural networks*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 人类大脑是现存最令人难以置信的计算设备。它不能像微处理器那样快速处理数字，但它的适应新情况、学习新技能和创造力的能力是任何已知机器所无法比拟的。自从计算机诞生以来，科学家们就对模拟大脑的机制感兴趣。大脑中的每个神经细胞都被称为**神经元**。大脑中的神经元通过称为**突触**的连接相互连接。电信号通过突触传递，为这些神经元网络（也称为**神经网络**）提供动力。
- en: Note The preceding description of biological neurons is a gross oversimplification
    for analogy’s sake. In fact, biological neurons have parts like axons, dendrites,
    and nuclei that you may remember from high school biology. And synapses are actually
    gaps between neurons where neurotransmitters are secreted to enable those electrical
    signals to pass.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了类比，前面关于生物神经元的描述是一种粗略的简化。实际上，生物神经元具有像轴突、树突和核这样的部分，这些你可能从高中生物学中记得。而突触实际上是神经元之间的间隙，神经递质在这里被分泌出来，以使这些电信号得以传递。
- en: Although scientists have identified the parts and functions of neurons, the
    details of how biological neural networks form complex thought patterns are still
    not well understood. How do they process information? How do they form original
    thoughts? Most of our knowledge of how the brain works comes from looking at it
    on a macro level. Functional magnetic resonance imaging (fMRI) scans of the brain
    show where blood flows when a human is doing a particular activity or thinking
    a particular thought (illustrated in figure 7.1). This and other macro techniques
    can lead to inferences about how the various parts are connected, but they do
    not explain the mysteries of how individual neurons aid in the development of
    new thoughts.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管科学家们已经确定了神经元的部分和功能，但生物神经网络如何形成复杂思维模式的细节仍然了解不多。它们是如何处理信息的？它们是如何形成原始思维的？我们对大脑如何工作的了解大部分来自于宏观层面的观察。功能性磁共振成像（fMRI）扫描显示，当人类进行特定活动或思考特定思维时，血液流动到大脑的哪些区域（如图7.1所示）。这些和其他宏观技术可以导致对各个部分如何连接的推断，但它们并不能解释单个神经元如何帮助形成新思维的奥秘。
- en: '![7-1](../Images/7-1.png)'
  id: totrans-8
  prefs: []
  type: TYPE_IMG
  zh: '![7-1](../Images/7-1.png)'
- en: 'Figure 7.1 A researcher studies fMRI images of the brain. fMRI images do not
    tell us much about how individual neurons function or how neural networks are
    organized. (Source: National Institute of Mental Health)'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 研究人员正在研究大脑的fMRI图像。fMRI图像并没有告诉我们太多关于单个神经元的功能或神经网络是如何组织的。（来源：美国国立精神健康研究所）
- en: 'Teams of scientists around the globe are racing to unlock the brain’s secrets,
    but consider this: The human brain has approximately 100,000,000,000 neurons,
    and each of them may have connections with as many as tens of thousands of other
    neurons. Even for a computer with billions of logic gates and terabytes of memory,
    a single human brain would be impossible to model using today’s technology. Humans
    will still likely be the most advanced general-purpose learning entities for the
    foreseeable future.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 全球各地的科学家团队正在竞相解开大脑的秘密，但请考虑这一点：人脑大约有1000亿个神经元，每个神经元可能与其他多达数万个神经元相连。即使对于拥有数十亿个逻辑门和数太字节内存的计算机来说，使用今天的技术也无法对人脑进行建模。在可预见的未来，人类仍然可能是最先进的多用途学习实体。
- en: 'Note A general-purpose learning machine that is equivalent to human beings
    in abilities is the goal of so-called *strong AI* (also known as *artificial general
    intelligence*). At this point in history, it is still the stuff of science fiction.
    *Weak AI* is the type of AI you see every day: computers intelligently solving
    specific tasks they were preconfigured to accomplish.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与人类在能力上相当的多用途学习机器是所谓的*强人工智能*（也称为*人工通用智能*）的目标。在当前的历史时刻，这仍然是科幻小说的内容。"弱人工智能"是你每天都能看到的类型：计算机智能地解决它们预先配置好的特定任务。
- en: If biological neural networks are not fully understood, then how has modeling
    them been an effective computational technique? Although digital neural networks,
    known as *artificial neural networks*, are inspired by biological neural networks,
    inspiration is where the similarities end. Modern artificial neural networks do
    not claim to work like their biological counterparts. In fact, that would be impossible,
    because we do not completely understand how biological neural networks work to
    begin with.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如果生物神经网络没有被完全理解，那么建模它们是如何成为一种有效的计算技术的呢？尽管被称为*人工神经网络*的数字神经网络受到了生物神经网络的启发，但相似之处到此为止。现代人工神经网络并不声称它们的工作方式与生物对应物相同。实际上，这是不可能的，因为我们一开始就没有完全理解生物神经网络是如何工作的。
- en: 7.2 Artificial neural networks
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 人工神经网络
- en: In this section, we will look at what is arguably the most common type of artificial
    neural network, a *feed-forward* network with *backpropagation*--the same type
    we will later be developing. *Feed-forward* means the signal is generally moving
    in one direction through the network. *Backpropagation* means we will determine
    errors at the end of each signal’s traversal through the network and try to distribute
    fixes for those errors back through the network, especially affecting the neurons
    that were most responsible for them. There are many other types of artificial
    neural networks, and perhaps this chapter will pique your interest in exploring
    further.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探讨可能是最常见的人工神经网络类型，即具有**反向传播**的**前馈**网络——这是我们稍后将要开发的同一种类型。**前馈**意味着信号通常在网络中单向移动。**反向传播**意味着我们将确定每个信号通过网络传输结束时产生的错误，并尝试将这些错误的修复分布回网络中，特别是影响这些错误的神经元。还有许多其他类型的人工神经网络，也许这一章会激发你对进一步探索的兴趣。
- en: 7.2.1 Neurons
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 神经元
- en: The smallest unit in an artificial neural network is a neuron. It holds a vector
    of weights, which are just floating-point numbers. A vector of inputs (also just
    floating-point numbers) is passed to the neuron. It combines those inputs with
    its weights using a dot product. It then runs an *activation function* on that
    product and spits the result out as its output. This action can be thought of
    as analogous to a real neuron firing.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络中最小的单元是神经元。它包含一个权重向量，这些权重只是浮点数。一个输入向量（也是浮点数）被传递给神经元。它使用点积将那些输入与其权重结合起来。然后，它对那个乘积运行一个**激活函数**，并将结果作为其输出输出。这个动作可以被认为与真实神经元的放电相似。
- en: An activation function is a transformer of the neuron’s output. The activation
    function is almost always nonlinear, which allows neural networks to represent
    solutions to nonlinear problems. If there were no activation functions, the entire
    neural network would just be a linear transformation. Figure 7.2 shows a single
    neuron and its operation.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数是神经元输出的转换器。激活函数几乎总是非线性的，这使得神经网络能够表示非线性问题的解决方案。如果没有激活函数，整个神经网络将只是一个线性变换。图7.2显示了单个神经元及其操作。
- en: '![7-2](../Images/7-2.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![7-2](../Images/7-2.png)'
- en: Figure 7.2 A single neuron combines its weights with input signals to produce
    an output signal that is modified by an activation function.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 一个神经元将其权重与输入信号结合以产生一个由激活函数修改的输出信号。
- en: Note There are some math terms in this section that you may not have seen since
    a precalculus or linear algebra class. Explaining what vectors or dot products
    are is beyond the scope of this chapter, but you will likely get an intuition
    of what a neural network does by following along in this chapter, even if you
    do not understand all of the math. Later in the chapter, there will be some calculus,
    including the use of derivatives and partial derivatives, but even if you do not
    understand all of the math, you should be able to follow the code. In fact, this
    chapter will not explain how to derive the formulas using calculus. Instead, it
    will focus on using the derivations.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本节中有些数学术语你可能自预微积分或线性代数课程以来就没有见过。解释向量或点积是什么超出了本章的范围，但通过跟随本章的内容，即使你不理解所有的数学，你也很可能会对神经网络的作用有一个直观的理解。在本章的后面部分，将会有一些微积分的内容，包括导数和偏导数的应用，但即使你不理解所有的数学，你也应该能够理解代码。实际上，本章不会解释如何使用微积分推导公式。相反，它将专注于使用推导。
- en: 7.2.2 Layers
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 层
- en: In a typical feed-forward artificial neural network, neurons are organized in
    layers. Each layer consists of a certain number of neurons lined up in a row or
    column (depending on the diagram; the two are equivalent). In a feed-forward network,
    which is what we will be building, signals always pass in a single direction from
    one layer to the next. The neurons in each layer send their output signal to be
    used as input to the neurons in the next layer. Every neuron in each layer is
    connected to every neuron in the next layer.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在典型的前馈人工神经网络中，神经元被组织成层。每一层由一行或一列（取决于图示；两者是等价的）中排列的一定数量的神经元组成。在前馈网络中，这是我们将要构建的，信号总是从一层单向传递到下一层。每一层的神经元将它们的输出信号发送给下一层的神经元作为输入。每一层的每个神经元都与下一层的每个神经元相连。
- en: The first layer is known as the *input layer*, and it receives its signals from
    some external entity. The last layer is known as the *output layer*, and its output
    typically must be interpreted by an external actor to get an intelligent result.
    The layers between the input and output layers are known as *hidden layers*. In
    simple neural networks like the one we will be building in this chapter, there
    is just one hidden layer, but deep learning networks have many. Figure 7.3 shows
    the layers working together in a simple network. Note how the outputs from one
    layer are used as the inputs to every neuron in the next layer.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 第一层被称为**输入层**，它从某个外部实体接收信号。最后一层被称为**输出层**，其输出通常必须由外部行为者解释才能得到智能结果。输入层和输出层之间的层被称为**隐藏层**。在我们将在本章中构建的简单神经网络中，只有一个隐藏层，但深度学习网络有很多。图7.3显示了简单网络中层的协同工作。注意，一个层的输出被用作下一层每个神经元的输入。
- en: '![7-3](../Images/7-3.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![7-3](../Images/7-3.png)'
- en: Figure 7.3 A simple neural network with one input layer of two neurons, one
    hidden layer of four neurons, and one output layer of three neurons. The number
    of neurons in each layer in this figure is arbitrary.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 一个简单的神经网络，包含一个输入层两个神经元，一个隐藏层四个神经元，以及一个输出层三个神经元。图中每一层的神经元数量是任意的。
- en: These layers just manipulate floating-point numbers. The inputs to the input
    layer are floating-point numbers, and the outputs from the output layer are floating-point
    numbers.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 这些层只是操作浮点数。输入层的输入是浮点数，输出层的输出也是浮点数。
- en: Obviously, these numbers must represent something meaningful. Imagine that the
    network was designed to classify small black-and-white images of animals. Perhaps
    the input layer has 100 neurons representing the grayscale intensity of each pixel
    in a 10 × 10 pixel animal image, and the output layer has 5 neurons representing
    the likelihood that the image is of a mammal, reptile, amphibian, fish, or bird.
    The final classification could be determined by the output neuron with the highest
    floating-point output. If the output numbers were 0.24, 0.65, 0.70, 0.12, and
    0.21, respectively, the image would be determined to be an amphibian.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，这些数字必须代表一些有意义的含义。想象一下，这个网络被设计用来分类小型的黑白动物图像。也许输入层有100个神经元，代表10×10像素动物图像中每个像素的灰度强度，而输出层有5个神经元，代表图像是哺乳动物、爬行动物、两栖动物、鱼类或鸟类的可能性。最终的分类可以通过具有最高浮点输出的输出神经元来确定。如果输出数字分别是0.24、0.65、0.70、0.12和0.21，那么图像将被判定为两栖动物。
- en: 7.2.3 Backpropagation
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.3 反向传播
- en: The last piece of the puzzle, and the inherently most complex part, is backpropagation.
    Backpropagation finds the error in a neural network’s output and uses it to modify
    the weights of neurons in an effort to reduce the error in subsequent runs. The
    neurons most responsible for the error are most heavily modified. But where does
    the error come from? How can we know the error? The error comes from a phase in
    the use of a neural network known as *training*.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个拼图，也是本质上最复杂的一部分，是反向传播。反向传播找到神经网络输出的错误，并使用它来修改神经元的权重，以减少后续运行中的错误。最负责错误的神经元被最严重地修改。但错误从何而来？我们如何知道错误？错误来自神经网络使用过程中的一个阶段，称为**训练**。
- en: Tip There are steps written out (in English) for several mathematical formulas
    in this section. Pseudo formulas (not using proper notation) are in the accompanying
    figures. This approach will make the formulas readable for those uninitiated in
    (or out of practice with) mathematical notation. If the more formal notation (and
    the derivation of the formulas) interests you, check out chapter 18 of Norvig
    and Russell’s *Artificial Intelligence*.[1](#pgfId-1161201)
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：本节中（用英语）详细说明了几个数学公式的步骤。伪公式（不使用正确的符号）见附图。这种方法将使那些不熟悉（或对数学符号不熟练）的人也能读懂公式。如果你对更正式的符号（以及公式的推导）感兴趣，请参阅诺维格和拉塞尔的《人工智能》第18章。[1](#pgfId-1161201)
- en: Before they can be used, most neural networks must be trained. We must know
    the right outputs for some inputs so that we can use the difference between expected
    outputs and actual outputs to find errors and modify weights. In other words,
    neural networks know nothing until they are told the right answers for a certain
    set of inputs, so that they can prepare themselves for other inputs. Backpropagation
    only occurs during training.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 在可以使用之前，大多数神经网络必须经过训练。我们必须知道某些输入的正确输出，以便我们可以使用预期输出和实际输出之间的差异来找到误差并修改权重。换句话说，神经网络在被告知一组特定输入的正确答案之前，一无所知，这样它们就可以为其他输入做好准备。反向传播仅在训练期间发生。
- en: Note Because most neural networks must be trained, they are considered a type
    of *supervised* machine learning. Recall from chapter 6 that the k-means algorithm
    and other cluster algorithms are considered a form of *unsupervised* machine learning
    because once they are started, no outside intervention is required. There are
    other types of neural networks than the one described in this chapter that do
    not require pretraining and are considered a form of unsupervised learning.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由于大多数神经网络都需要训练，因此它们被视为一种*监督学习*。回想第6章，k-means算法和其他聚类算法被视为一种*无监督学习*，因为一旦启动，就不需要外部干预。除了本章描述的神经网络之外，还有其他类型的神经网络不需要预训练，被视为一种无监督学习。
- en: The first step in backpropagation is to calculate the error between the neural
    network’s output for some input and the expected output. This error is spread
    across all of the neurons in the output layer. (Each neuron has an expected output
    and its actual output.) The derivative of the output neuron’s activation function
    is then applied to what was output by the neuron before its activation function
    was applied. (We cache its preactivation function output.) This result is multiplied
    by the neuron’s error to find its *delta*. This formula for finding the delta
    uses a partial derivative, and its calculus derivation is beyond the scope of
    this book, but we are basically figuring out how much of the error each output
    neuron was responsible for. See figure 7.4 for a diagram of this calculation.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播的第一步是计算神经网络对于某些输入的输出与预期输出之间的误差。这个误差会分布到输出层中的所有神经元上。（每个神经元都有一个预期输出和实际输出。）然后，输出神经元激活函数的导数被应用于激活函数应用之前的神经元输出。（我们缓存了其预激活函数输出。）这个结果乘以神经元的误差以找到其*delta*。这个用于找到delta的公式使用的是偏导数，其微积分推导超出了本书的范围，但我们的基本思路是确定每个输出神经元对误差的贡献有多大。参见图7.4了解这个计算的示意图。
- en: '![7-4](../Images/7-4.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![7-4](../Images/7-4.png)'
- en: Figure 7.4 The mechanism by which an output neuron’s delta is calculated during
    the backpropagation phase of training
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4展示了在训练的反向传播阶段计算输出神经元delta的机制。
- en: Deltas must then be calculated for every neuron in the hidden layer(s) in the
    network. We must determine how much each neuron was responsible for the incorrect
    output in the output layer. The deltas in the output layer are used to calculate
    the deltas in the preceding hidden layer. For each previous layer, the deltas
    are calculated by taking the dot product of the next layer’s weights with respect
    to the particular neuron in question and the deltas already calculated in the
    next layer. This dot product is multiplied by the derivative of the activation
    function applied to a neuron’s last output (cached before the activation function
    was applied) to get the neuron’s delta. Again, this formula is derived using a
    partial derivative, which you can read about in more mathematically focused texts.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，必须计算网络中隐藏层（s）中每个神经元的delta。我们必须确定每个神经元对输出层中不正确输出的责任有多大。输出层的delta用于计算前一个隐藏层的delta。对于每个前一层，通过取下一层的权重与特定神经元以及下一层已计算的delta的点积来计算delta。这个点积乘以应用于神经元最后输出的激活函数的导数（在激活函数应用之前缓存）以得到神经元的delta。再次强调，这个公式是使用偏导数推导出来的，你可以在更数学化的文本中了解更多。
- en: Figure 7.5 shows the actual calculation of deltas for neurons in hidden layers.
    In a network with multiple hidden layers, neurons O1, O2, and O3 could be neurons
    in the next hidden layer instead of in the output layer.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5显示了隐藏层中神经元的实际delta计算。在一个具有多个隐藏层的网络中，O1、O2和O3可以是下一隐藏层的神经元，而不是输出层的神经元。
- en: '![7-5](../Images/7-5.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![7-5](../Images/7-5.png)'
- en: Figure 7.5 The weights of every hidden layer and output layer neuron are updated
    using the deltas calculated in the previous steps, the prior weights, the prior
    inputs, and a user-determined learning rate.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 每个隐藏层和输出层神经元的权重都使用前一步计算出的delta、先前权重、先前输入以及用户确定的学习率进行更新。
- en: Last, but most important, all of the weights for every neuron in the network
    must be updated. They can be updated by multiplying each individual weight’s last
    input with the delta of the neuron and something called a *learning rate*, and
    adding that to the existing weight. This method of modifying the weight of a neuron
    is known as *gradient descent*. It is like climbing down a hill representing the
    error function of the neuron toward a point of minimal error. The delta represents
    the direction we want to climb, and the learning rate affects how fast we climb.
    It is hard to determine a good learning rate for an unknown problem without trial
    and error. Figure 7.6 shows how every weight in the hidden layer and output layer
    is updated.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，但同样重要的是，网络中每个神经元的所有权重都必须更新。它们可以通过将每个单独权重的最后输入与神经元的delta以及一个称为*学习率*的值相乘，并将这个值加到现有权重上来更新。这种修改神经元权重的方
    法被称为*梯度下降*。它就像沿着表示神经元误差函数的斜坡向下爬，朝着最小误差点前进。delta代表我们想要爬的方向，学习率影响我们爬的速度。在没有试错的情况下，很难确定一个未知问题的良好学习率。图7.6显示了隐藏层和输出层中每个权重的更新方式。
- en: '![7-6](../Images/7-6.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![7-6](../Images/7-6.png)'
- en: Figure 7.6 How a delta is calculated for a neuron in a hidden layer
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 如何计算隐藏层中神经元的delta
- en: Once the weights are updated, the neural network is ready to be trained again
    with another input and expected output. This process repeats until the network
    is deemed well trained by the neural network’s user. This can be determined by
    testing it against inputs with known correct outputs.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦权重被更新，神经网络就准备好再次使用另一个输入和预期输出进行训练。这个过程会重复进行，直到神经网络的用户认为网络已经很好地训练好了。这可以通过测试它对具有已知正确输出的输入来确定。
- en: 'Backpropagation is complicated. Do not worry if you do not yet grasp all of
    the details. The explanation in this section may not be enough. Ideally, implementing
    backpropagation will take your understanding to the next level. As we implement
    our neural network and backpropagation, keep in mind this overarching theme: backpropagation
    is a way of adjusting each individual weight in the network according to its responsibility
    for an incorrect output.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 反向传播是复杂的。如果你还没有完全掌握所有细节，请不要担心。本节中的解释可能不足以说明问题。理想情况下，实现反向传播将使你的理解达到新的水平。当我们实现神经网络和反向传播时，请记住这个总体主题：反向传播是一种根据其对错误输出的责任来调整网络中每个单独权重的方
    法。
- en: 7.2.4 The big picture
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.4 整体图景
- en: 'We covered a lot of ground in this section. Even if the details are still a
    bit fuzzy, it is important to keep the main themes in mind for a feed-forward
    network with backpropagation:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们涵盖了大量的内容。即使细节仍然有些模糊，但对于具有反向传播的前馈网络，保持主要主题在心中是很重要的：
- en: Signals (floating-point numbers) move through neurons organized in layers in
    one direction. Every neuron in each layer is connected to every neuron in the
    next layer.
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 信号（浮点数）在一个方向上通过分层组织的神经元移动。每个层的每个神经元都与下一层的每个神经元相连。
- en: Each neuron (except in the input layer) processes the signals it receives by
    combining them with weights (also floating-point numbers) and applying an activation
    function.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个神经元（除了输入层）通过将接收到的信号与权重（也是浮点数）结合并应用激活函数来处理这些信号。
- en: During a process called training, network outputs are compared with expected
    outputs to calculate errors.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在一个称为训练的过程中，网络输出与预期输出进行比较，以计算误差。
- en: Errors are backpropagated through the network (back toward where they came from)
    to modify weights so that they are more likely to create correct outputs.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 误差通过网络反向传播（返回到它们来源的地方）以修改权重，使它们更有可能产生正确的输出。
- en: There are more methods for training neural networks than the one explained here.
    There are also many other ways for signals to move within neural networks. The
    method explained here, and that we will be implementing, is just a particularly
    common form that serves as a decent introduction. Appendix B lists further resources
    for learning more about neural networks (including other types) and the math.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络的方 法比这里解释的要多。还有许多其他信号在神经网络中移动的方式。这里解释的，以及我们将要实现的，只是特别常见的一种形式，它作为相当不错的入门介绍。附录B列出了更多关于学习神经网络（包括其他类型）和数学的资源。
- en: 7.3 Preliminaries
  id: totrans-52
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 前置知识
- en: Neural networks utilize mathematical mechanisms that require a lot of floating-point
    operations. Before we develop the actual structures of our simple neural network,
    we will need some mathematical primitives. These simple primitives are used extensively
    in the code that follows, so if you can find ways to accelerate them, it will
    really improve the performance of your neural network.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络利用需要大量浮点运算的数学机制。在我们开发简单的神经网络的实际结构之前，我们需要一些数学原语。这些简单的原语在接下来的代码中得到了广泛的应用，所以如果你能找到加速它们的方法，这将真正提高你神经网络的性能。
- en: Warning The complexity of the code in this chapter is arguably greater than
    any other in the book. There is a lot of buildup, with actual results seen only
    at the very end. There are many resources about neural networks that help you
    build one in very few lines of code, but this example is aimed at exploring the
    machinery and how the different components work together in a readable and extensible
    fashion. That is our goal, even if the code is a little longer and more expressive.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 本章中的代码复杂度可能比书中任何其他章节都要高。有很多准备工作，实际结果只有在最后才能看到。有很多关于神经网络资源可以帮助你在极少的代码行中构建一个神经网络，但这个例子旨在探索机制以及不同组件如何以可读和可扩展的方式协同工作。这就是我们的目标，即使代码稍微长一些，表达也更丰富。
- en: 7.3.1 Dot product
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 点积
- en: As you will recall, dot products are required both for the feed-forward phase
    and for the backpropagation phase. We will keep our static utility functions in
    a Util class. Like all of the code in this chapter, which is written for illustrative
    purposes, this is a very naive implementation, with no performance considerations.
    In a production library, vector instructions would be used, as discussed in section
    7.6.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所回忆，点积在正向传播阶段和反向传播阶段都是必需的。我们将保持我们的静态实用函数在 Util 类中。像本章中所有为了说明目的而编写的代码一样，这是一个非常简单的实现，没有考虑性能。在生产库中，将使用在第
    7.6 节中讨论的向量指令。
- en: Listing 7.1 Util.java
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.1 Util.java
- en: '[PRE0]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 7.3.2 The activation function
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 激活函数
- en: 'Recall that the activation function transforms the output of a neuron before
    the signal passes to the next layer (see figure 7.2). The activation function
    has two purposes: it allows the neural network to represent solutions that are
    not just linear transformations (as long as the activation function itself is
    not just a linear transformation), and it can keep the output of each neuron within
    a certain range. An activation function should have a computable derivative so
    that it can be used for backpropagation.'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，激活函数在信号传递到下一层之前会转换神经元的输出（见图 7.2）。激活函数有两个目的：它允许神经网络表示不仅仅是线性变换的解（只要激活函数本身不是仅仅是线性变换），并且它可以保持每个神经元的输出在某个范围内。激活函数应该有一个可计算的导数，以便它可以用于反向传播。
- en: '*Sigmoid* functions are a popular set of activation functions. One particularly
    popular sigmoid function (often just referred to as “the sigmoid function”) is
    illustrated in figure 7.7 (referred to in the figure as S(*x* )), along with its
    equation and derivative (S''(*x* )). The result of the sigmoid function will always
    be a value between 0 and 1, which is useful for the network, as you will see.
    You will shortly see the formulas from the figure written out in code.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: '*Sigmoid* 函数是一组流行的激活函数。一个特别受欢迎的 sigmoid 函数（通常简称为“sigmoid 函数”）如图 7.7 所示（图中称为
    S(*x* )），以及其方程和导数（S''(*x* )）。sigmoid 函数的结果始终是一个介于 0 和 1 之间的值，这对网络来说很有用，你很快就会看到。你很快就会看到图中的公式在代码中的实现。'
- en: There are other activation functions, but we will use the sigmoid function.
    It and its derivative are easy to implement. Here is a straightforward conversion
    of the formulas in figure 7.7 into code.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他激活函数，但我们将使用 sigmoid 函数。它及其导数很容易实现。以下是将图 7.7 中的公式直接转换为代码的简单方法。
- en: Listing 7.2 Util.java continued
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 Util.java 续
- en: '[PRE1]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![7-7](../Images/7-7.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![7-7](../Images/7-7.png)'
- en: Figure 7.7 The sigmoid activation function (S(x)) will always return a value
    between 0 and 1\. Note that its derivative is easy to compute as well (S' (x)).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.7 sigmoid 激活函数（S(x)）将始终返回一个介于 0 和 1 之间的值。请注意，其导数也容易计算（S' (x)）。
- en: 7.4 Building the network
  id: totrans-67
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 构建网络
- en: 'We will create classes to model all three organizational units in the network:
    neurons, layers, and the network itself. For the sake of simplicity, we will start
    from the smallest (neurons), move to the central organizing component (layers),
    and build up to the largest (the whole network). As we go from smallest component
    to largest component, we will encapsulate the previous level. Neurons only know
    about themselves. Layers know about the neurons they contain and other layers.
    And the network knows about all of the layers.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将创建类来模拟网络中的所有三个组织单元：神经元、层和网络本身。为了简化，我们将从最小的（神经元）开始，过渡到中央组织组件（层），然后构建到最大的（整个网络）。当我们从最小组件过渡到最大组件时，我们将封装前一个级别。神经元只知道自身。层知道它们包含的神经元和其他层。网络知道所有层。
- en: 'NOTE There are many long lines of code in this chapter that do not neatly fit
    in the column limits of a printed book. I strongly recommend downloading the source
    code for this chapter from the book’s source code repository and following along
    on your computer screen as you read: [https://github.com/ davecom/ClassicComputerScienceProblemsInJava](https://github.com/davecom/ClassicComputerScienceProblemsInJava).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章有许多长代码行，它们无法整齐地适应印刷书的列宽限制。我强烈建议您从本书的源代码仓库下载本章的源代码，并在阅读时在您的计算机屏幕上跟随：[https://github.com/davecom/ClassicComputerScienceProblemsInJava](https://github.com/davecom/ClassicComputerScienceProblemsInJava)。
- en: 7.4.1 Implementing neurons
  id: totrans-70
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 实现神经元
- en: Let’s start with a neuron. An individual neuron will store many pieces of state,
    including its weights, its delta, its learning rate, a cache of its last output,
    and its activation function, along with the derivative of that activation function.
    Some of these elements could be more efficiently stored up a level (in the future
    Layer class), but they are included in the following Neuron class for illustrative
    purposes.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从神经元开始。一个单独的神经元将存储许多状态信息，包括其权重、delta、学习率、其最后输出的缓存以及其激活函数，以及该激活函数的导数。其中一些元素可能更有效地存储在更高一层（未来
    Layer 类），但为了说明目的，它们包含在下面的 Neuron 类中。
- en: Listing 7.3 Neuron.java
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.3 Neuron.java
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Most of these parameters are initialized in the constructor. Because delta and
    outputCache are not known when a Neuron is first created, they are just initialized
    to 0.0. Several of these variables (learningRate, activationFunction, derivativeActivationFunction)
    look preset, so why are we making them configurable at the neuron level? If this
    Neuron class were to be used with other types of neural networks, it is possible
    that some of these values might differ from one neuron to another, so they are
    configurable for maximum flexibility. There are even neural networks that change
    the learning rate as the solution approaches, and that automatically try different
    activation functions. Since our variables are final, they could not change mid-stream,
    but making them non-final would be an easy code change.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数这些参数都在构造函数中初始化。因为当 Neuron 首次创建时，delta 和 outputCache 是未知的，所以它们只是初始化为 0.0。这些变量中的几个（学习率、激活函数、导数激活函数）看起来是预设的，那么为什么我们在神经元级别使它们可配置呢？如果这个
    Neuron 类要与其他类型的神经网络一起使用，那么这些值可能因神经元而异，因此它们是可配置的，以实现最大的灵活性。甚至有神经网络在解决方案接近时改变学习率，并自动尝试不同的激活函数。由于我们的变量是
    final 的，它们在流过程中不能改变，但将它们改为非 final 是一个简单的代码更改。
- en: The only other method, other than the constructor, is output(). output() takes
    the input signals (inputs) coming to the neuron and applies the formula discussed
    earlier in the chapter (see figure 7.2). The input signals are combined with the
    weights via a dot product, and this is cached in outputCache. Recall from the
    section on backpropagation that this value, obtained before the activation function
    is applied, is used to calculate delta. Finally, before the signal is sent on
    to the next layer (by being returned from output()), the activation function is
    applied to it.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 除了构造函数之外，还有一个方法是 output()。output() 方法接收传入神经元的输入信号（输入），并应用本章前面讨论过的公式（见图 7.2）。输入信号通过点积与权重结合，并将结果缓存到
    outputCache 中。回想一下关于反向传播的部分，这个在应用激活函数之前获得的价值用于计算 delta。最后，在信号被发送到下一层（通过从 output()
    返回）之前，对该信号应用激活函数。
- en: That is it! An individual neuron in this network is fairly simple. It cannot
    do much beyond taking an input signal, transforming it, and sending it off to
    be processed further. It maintains several elements of state that are used by
    the other classes.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 就这样！这个网络中的单个神经元相当简单。它不能做很多，除了接收输入信号，对其进行转换，并将其发送出去进一步处理。它维护了其他类使用的几个状态元素。
- en: 7.4.2 Implementing layers
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 实现层
- en: 'A layer in our network will need to maintain three pieces of state: its neurons,
    the layer that preceded it, and an output cache. The output cache is similar to
    that of a neuron, but up one level. It caches the outputs (after activation functions
    are applied) of every neuron in the layer.'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的网络中的每一层都需要维护三块状态：其神经元、它之前的一层，以及一个输出缓存。输出缓存类似于神经元的缓存，但高一个层级。它缓存了该层中每个神经元的输出（在应用激活函数之后）。
- en: At creation time, a layer’s main responsibility is to initialize its neurons.
    Our Layer class’s constructor therefore needs to know how many neurons it should
    be initializing, what their activation functions should be, and what their learning
    rates should be. In this simple network, every neuron in a layer has the same
    activation function and learning rate.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建时，层的主要责任是初始化其神经元。因此，我们的 Layer 类的构造函数需要知道它应该初始化多少个神经元，它们的激活函数是什么，以及它们的学习率是什么。在这个简单的网络中，层的每个神经元都有相同的激活函数和学习率。
- en: Listing 7.4 Layer.java
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.4 Layer.java
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: As signals are fed forward through the network, the Layer must process them
    through every neuron. (Remember that every neuron in a layer receives the signals
    from every neuron in the previous layer.) outputs() does just that. outputs()
    also returns the result of processing them (to be passed by the network to the
    next layer) and caches the output. If there is no previous layer, that indicates
    the layer is an input layer, and it just passes the signals forward to the next
    layer.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当信号通过网络前向传递时，层必须通过每个神经元处理它们。（记住，层中的每个神经元都接收来自前一层的每个神经元的信号。）outputs() 正是做这件事。outputs()
    还返回处理后的结果（将被网络传递给下一层）并缓存输出。如果没有前一层，这表明该层是输入层，它只需将信号传递给下一层。
- en: Listing 7.5 Layer.java continued
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.5 Layer.java 继续内容
- en: '[PRE4]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'There are two distinct types of deltas to calculate in backpropagation: deltas
    for neurons in the output layer and deltas for neurons in hidden layers. The formulas
    are described in figures 7.4 and 7.5, and the following two methods are rote translations
    of those formulas. These methods will later be called by the network during backpropagation.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在反向传播中需要计算两种不同类型的 delta：输出层神经元的 delta 和隐藏层神经元的 delta。公式在图 7.4 和 7.5 中描述，以下两个方法是这些公式的直接翻译。这些方法将在反向传播期间由网络调用。
- en: Listing 7.6 Layer.java continued
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.6 Layer.java 继续内容
- en: '[PRE5]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 7.4.3 Implementing the network
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 实现网络
- en: 'The network itself has only one piece of state: the layers that it manages.
    The Network class is responsible for initializing its constituent layers.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 网络本身只有一块状态：它管理的层。Network 类负责初始化其组成部分层。
- en: The constructor takes an int array describing the structure of the network.
    For example, the array {2, 4, 3} describes a network with 2 neurons in its input
    layer, 4 neurons in its hidden layer, and 3 neurons in its output layer. In this
    simple network, we will assume that all layers in the network will make use of
    the same activation function for their neurons and the same learning rate.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 构造函数接收一个 int 数组，描述网络的架构。例如，数组 {2, 4, 3} 描述了一个输入层有 2 个神经元、隐藏层有 4 个神经元、输出层有 3
    个神经元的网络。在这个简单的网络中，我们将假设网络中的所有层都将使用相同的激活函数和相同的学习率来为其神经元服务。
- en: Listing 7.7 Network.java
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.7 Network.java
- en: '[PRE6]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: NOTE The generic type, T, links the network to the type of the final classification
    categories from the data set. It is only used in the final method of the class,
    validate().
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：泛型类型 T 将网络与数据集最终分类类别类型链接起来。它仅在类的最终方法 validate() 中使用。
- en: The outputs of the neural network are the result of signals running through
    all of its layers, one after another.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的输出是信号依次通过所有层的结果。
- en: Listing 7.8 Network.java continued
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.8 Network.java 继续内容
- en: '[PRE7]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The backpropagate() method is responsible for computing deltas for every neuron
    in the network. It uses the Layer methods calculateDeltasForOutputLayer() and
    calculateDeltasForHiddenLayer() in sequence. (Recall that in backpropagation,
    deltas are calculated backward.) It passes the expected values of output for a
    given set of inputs to calculateDeltasForOutputLayer(). That method uses the expected
    values to find the error used for delta calculation.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: backpropagate() 方法负责计算网络中每个神经元的 delta 值。它按顺序使用 Layer 方法的 calculateDeltasForOutputLayer()
    和 calculateDeltasForHiddenLayer()。 (回想一下，在反向传播中，delta 是向后计算的。)它传递给定输入集的输出期望值以计算
    calculateDeltasForOutputLayer()。该方法使用期望值来找到用于 delta 计算的错误。
- en: Listing 7.9 Network.java continued
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.9 Network.java 继续部分
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: backpropagate() is responsible for calculating all deltas, but it does not actually
    modify any of the network’s weights. updateWeights() must be called after backpropagate()
    because weight modification depends on deltas. This method follows directly from
    the formula in figure 7.6.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: backpropagate() 负责计算所有 delta 值，但它实际上并不修改网络中的任何权重。必须在 backpropagate() 之后调用 updateWeights()，因为权重修改依赖于
    delta。此方法直接来源于图 7.6 中的公式。
- en: Listing 7.10 Network.java continued
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.10 Network.java 继续部分
- en: '[PRE9]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Neuron weights are modified at the end of each round of training. Training sets
    (inputs coupled with expected outputs) must be provided to the network. The train()
    method takes a list of arrays of doubles of inputs and a list of arrays of doubles
    of expected outputs. It runs each input through the network and then updates its
    weights by calling backpropagate() with the expected output (and updateWeights()
    after that). Try adding code in the following training function to print out the
    error rate as the network goes through a training set to see how the network gradually
    decreases its error rate as it rolls down the hill in gradient descent.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 神经元的权重在每个训练回合结束时进行修改。必须向网络提供训练集（输入与期望输出的组合）。train() 方法接收输入的双精度浮点数数组列表和期望输出的双精度浮点数数组列表。它将每个输入通过网络运行，然后通过调用带有期望输出的
    backpropagate()（之后调用 updateWeights()）来更新其权重。尝试在以下训练函数中添加代码以打印出错误率，以查看网络如何在梯度下降过程中逐渐降低其错误率，就像它沿着山丘滚动一样。
- en: Listing 7.11 Network.java continued
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.11 Network.java 继续部分
- en: '[PRE10]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Finally, after a network is trained, we need to test it. validate() takes inputs
    and expected outputs (not unlike train()), but uses them to calculate an accuracy
    percentage rather than perform training. It is assumed that the network is already
    trained. validate() also takes a function, interpretOutput(), that is used for
    interpreting the output of the neural network to compare it to the expected output.
    Perhaps the expected output is a string like "Amphibian" instead of a set of floating-point
    numbers. interpretOutput() must take the floating-point numbers it gets as output
    from the network and convert them into something comparable to the expected outputs.
    It is a custom function specific to a data set. validate() returns the number
    of correct classifications, the total number of samples tested, and the percentage
    of correct classifications. Those three values are wrapped inside the inner Results
    type.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，在训练好一个网络之后，我们需要对其进行测试。validate() 函数接收输入和期望输出（与 train() 函数类似），但它使用它们来计算准确率百分比而不是执行训练。假设网络已经训练好。validate()
    还接收一个函数，interpretOutput()，用于解释神经网络的输出并将其与期望输出进行比较。也许期望输出是一个像 "Amphibian" 这样的字符串，而不是一组浮点数。interpretOutput()
    必须接收网络输出的浮点数并将其转换为与期望输出可比较的东西。这是一个针对特定数据集的定制函数。validate() 返回正确分类的数量、测试的总样本数和正确分类的百分比。这三个值被封装在内部
    Results 类型中。
- en: Listing 7.12 Network.java continued
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.12 Network.java 继续部分
- en: '[PRE11]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The neural network is done! It is ready to be tested with some actual problems.
    Although the architecture we built is general-purpose enough to be used for a
    variety of problems, we will concentrate on a popular kind of problem: classification.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络已完成！它准备好用一些实际问题进行测试。尽管我们构建的架构足够通用，可以用于各种问题，但我们将专注于一种流行的问题类型：分类。
- en: 7.5 Classification problems
  id: totrans-110
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 分类问题
- en: In chapter 6 we categorized a data set with k-means clustering, using no preconceived
    notions about where each individual piece of data belonged. In clustering, we
    know we want to find categories of data, but we do not know ahead of time what
    those categories are. In a classification problem, we are also trying to categorize
    a data set, but there are preset categories. For example, if we were trying to
    classify a set of pictures of animals, we might decide ahead of time on categories
    like mammal, reptile, amphibian, fish, and bird.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在第6章中，我们使用k-means聚类对数据集进行了分类，没有预先设定关于每个数据点所属位置的想法。在聚类中，我们知道我们想要找到数据类别，但我们事先不知道这些类别是什么。在分类问题中，我们也在尝试对数据集进行分类，但存在预设的类别。例如，如果我们试图对一组动物图片进行分类，我们可能会提前决定类别，如哺乳动物、爬行动物、两栖动物、鱼类和鸟类。
- en: There are many machine learning techniques that can be used for classification
    problems. Perhaps you have heard of support vector machines, decision trees, or
    naive Bayes classifiers. Recently, neural networks have become widely deployed
    in the classification space. They are more computationally intensive than some
    of the other classification algorithms, but their ability to classify seemingly
    arbitrary kinds of data makes them a powerful technique. Neural network classifiers
    are behind much of the interesting image classification that powers modern photo
    software.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多机器学习技术可以用于分类问题。你可能听说过支持向量机、决策树或朴素贝叶斯分类器。最近，神经网络在分类领域得到了广泛的应用。与一些其他分类算法相比，它们在计算上更为密集，但它们对看似任意类型数据的分类能力使它们成为一种强大的技术。神经网络分类器是许多现代照片软件中推动有趣图像分类的幕后力量。
- en: Why is there renewed interest in using neural networks for classification problems?
    Hardware has become fast enough that the extra computation involved, compared
    to other algorithms, makes the benefits worthwhile.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么对使用神经网络进行分类问题再次产生了兴趣？硬件已经足够快，与其他算法相比，额外的计算量使得这些好处值得。
- en: 7.5.1 Normalizing data
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 数据归一化
- en: The data sets that we want to work with generally require some “cleaning” before
    they are input into our algorithms. Cleaning may involve removing extraneous characters,
    deleting duplicates, fixing errors, and other menial tasks. The aspect of cleaning
    we will need to perform for the two data sets we are working with is normalization.
    In chapter 6 we did this via the zScoreNormalize() method in the KMeans class.
    Normalization is about taking attributes recorded on different scales and converting
    them to a common scale.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要处理的数据集通常在输入我们的算法之前需要一些“清理”。清理可能包括删除多余的字符、删除重复项、修复错误和其他琐事。对于我们将要处理的两个数据集，我们需要执行的清理方面是归一化。在第6章中，我们通过KMeans类中的zScoreNormalize()方法来完成这项工作。归一化是将记录在不同刻度上的属性转换为公共刻度的过程。
- en: Every neuron in our network outputs values between 0 and 1 due to the sigmoid
    activation function. It sounds logical that a scale between 0 and 1 would make
    sense for the attributes in our input data set as well. Converting a scale from
    some range to a range between 0 and 1 is not challenging. For any value, V, in
    a particular attribute range with maximum (max) and minimum (min), the formula
    is just newV = (oldV - min) / (max - min). This operation is known as *feature
    scaling*. Here is a Java implementation to add to the Util class, as well as two
    utility methods for loading data from CSV files and finding the maximum number
    in an array that will be convenient for the rest of the chapter.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 由于使用了Sigmoid激活函数，我们网络中的每个神经元输出的值都在0到1之间。对于输入数据集中的属性，一个介于0到1之间的刻度似乎是有意义的。将某个范围转换为0到1的范围并不具有挑战性。对于特定属性范围内的任何值V，其最大值（max）和最小值（min），公式只是
    newV = (oldV - min) / (max - min)。这种操作被称为*特征缩放*。以下是一个添加到Util类的Java实现，以及两个用于从CSV文件加载数据和查找数组中最大数的实用方法，这将使本章的其余部分更加方便。
- en: Listing 7.13 Util.java continued
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.13 Util.java继续
- en: '[PRE12]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Look at the dataset parameter in normalizeByFeatureScaling(). It is a reference
    to a list of double arrays that will be modified in place. In other words, normalizeByFeatureScaling()
    does not receive a copy of the data set. It receives a reference to the original
    data set. This is a situation where we want to make changes to a value rather
    than receive back a transformed copy. Java is pass by value, but in this instance
    we are passing by value a reference, therefore getting a copy of a reference to
    the same list.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下normalizeByFeatureScaling()函数中的数据集参数。它是对一个双精度数组列表的引用，该列表将在原地被修改。换句话说，normalizeByFeatureScaling()函数不接收数据集的副本。它接收原始数据集的引用。这是一种我们想要修改一个值而不是接收转换后的副本的情况。Java是按值传递的，但在这个实例中，我们按值传递了一个引用，因此得到了对同一列表的引用的副本。
- en: Note also that our program assumes that data sets are effectively two-dimensional
    lists of floating-point numbers arranged as lists of double arrays.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，我们的程序假设数据集是二维浮点数列表，排列为双精度数组列表。
- en: 7.5.2 The classic iris data set
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 经典的鸢尾花数据集
- en: 'Just as there are classic computer science problems, there are classic data
    sets in machine learning. These data sets are used to validate new techniques
    and compare them to existing ones. They also serve as good starting points for
    people learning machine learning for the first time. Perhaps the most famous is
    the iris data set. Originally collected in the 1930s, the data set consists of
    150 samples of iris plants (pretty flowers), split among three different species
    (50 of each). Each plant is measured on four different attributes: sepal length,
    sepal width, petal length, and petal width.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 正如存在经典的计算机科学问题一样，机器学习中也有经典的数据集。这些数据集用于验证新技术并将其与现有技术进行比较。它们还作为学习机器学习的新手的良好起点。最著名的是鸢尾花数据集。最初在20世纪30年代收集，该数据集包含150个鸢尾花植物（漂亮的花）样本，分为三个不同的物种（每种50个）。每株植物在四个不同的属性上进行了测量：花萼长度、花萼宽度、花瓣长度和花瓣宽度。
- en: It is worth noting that a neural network does not care what the various attributes
    represent. Its model for training makes no distinction between sepal length and
    petal length in terms of importance. If such a distinction should be made, it
    is up to the user of the neural network to make appropriate adjustments.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，神经网络并不关心各种属性代表什么。其训练模型在重要性方面对花萼长度和花瓣长度没有区别。如果应该做出这样的区分，那么神经网络的使用者需要做出适当的调整。
- en: The source code repository that accompanies this book contains a comma-separated
    values (CSV) file that features the iris data set.[2](#pgfId-1161646) The iris
    data set is from the University of California’s UCI Machine Learning Repository.[3](#pgfId-1164143)
    A CSV file is just a text file with values separated by commas. It is a common
    interchange format for tabular data, including spreadsheets.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 伴随本书的源代码库包含一个逗号分隔值（CSV）文件，其中包含鸢尾花数据集。[2](#pgfId-1161646) 鸢尾花数据集来自加州大学伯克利分校的UCI机器学习库。[3](#pgfId-1164143)
    CSV文件只是一个以逗号分隔值的文本文件。它是表格数据的常见交换格式，包括电子表格。
- en: 'Here are a few lines from iris.csv:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是来自iris.csv的几行内容：
- en: '[PRE13]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Each line represents one data point. The four numbers represent the four attributes
    (sepal length, sepal width, petal length, and petal width), which, again, are
    arbitrary to us in terms of what they actually represent. The name at the end
    of each line represents the particular iris species. All five lines are for the
    same species because this sample was taken from the top of the file, and the three
    species are clumped together, with fifty lines each.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行代表一个数据点。这四个数字代表四个属性（花萼长度、花萼宽度、花瓣长度和花瓣宽度），就它们实际代表的内容而言，对我们来说是任意的。每行末尾的名称代表特定的鸢尾花种类。这五行都是同一物种，因为样本是从文件顶部取出的，而且三种物种聚集在一起，每种有五十行。
- en: To read the CSV file from disk, we will use a few functions from the Java standard
    library. These are wrapped up in the loadCSV() method we earlier defined in the
    Util class. Beyond those few lines, the rest of the constructor for IrisTest,
    our class for actually running the classification, just rearranges the data from
    the CSV file to prepare it to be consumed by our network for training and validation.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 要从磁盘读取CSV文件，我们将使用Java标准库中的几个函数。这些函数被封装在我们之前在Util类中定义的loadCSV()方法中。除了这几行之外，IrisTest类的构造函数的其余部分，即我们实际运行分类的类，只是将CSV文件中的数据重新排列，以便准备由我们的网络进行训练和验证。
- en: Listing 7.14 IrisTest.java
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 列表7.14 IrisTest.java
- en: '[PRE14]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: irisParameters represents the collection of four attributes per sample that
    we are using to classify each iris. irisClassifications is the actual classification
    of each sample. Our neural network will have three output neurons, with each representing
    one possible species. For instance, a final set of outputs of {0.9, 0.3, 0.1}
    will represent a classification of iris-setosa, because the first neuron represents
    that species, and it is the largest number.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: irisParameters 代表每个样本所使用的四个属性的集合，我们用这些属性来对每个鸢尾花进行分类。irisClassifications 是每个样本的实际分类。我们的神经网络将有三个人工神经元，每个神经元代表一种可能的物种。例如，最终输出集合
    {0.9, 0.3, 0.1} 将代表鸢尾花-setosa 的分类，因为第一个神经元代表这种物种，并且它是最大的数值。
- en: For training, we already know the right answers, so each iris has a premarked
    answer. For a flower that should be iris-setosa, the entry in irisClassifications
    will be {1.0, 0.0, 0.0}. These values will be used to calculate the error after
    each training step. IrisSpecies corresponds directly to what each flower should
    be classified as in English. An iris-setosa will be marked as "Iris-setosa" in
    the data set.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 对于训练，我们已经知道正确的答案，因此每个鸢尾花都有一个预先标记的答案。对于一个应该是 iris-setosa 的花，irisClassifications
    中的条目将是 {1.0, 0.0, 0.0}。这些值将在每次训练步骤后用于计算误差。IrisSpecies 直接对应于每朵花在英语中应该被分类为什么。一个
    iris-setosa 将在数据集中标记为 "Iris-setosa"。
- en: Warning The lack of error-checking makes this code fairly dangerous. It is not
    suitable as is for production, but it is fine for testing.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 缺乏错误检查使此代码相当危险。它不适合直接用于生产，但适用于测试。
- en: Listing 7.15 IrisTest.java continued
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.15 IrisTest.java 继续部分
- en: '[PRE15]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: irisInterpretOutput() is a utility function that will be passed to the network’s
    validate() method to help identify correct classifications.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: irisInterpretOutput() 是一个实用函数，它将被传递给网络的 validate() 方法，以帮助识别正确的分类。
- en: The network is finally ready to be created. Let’s define a classify() method
    that will set up the network, train it, and run it.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 网络最终准备就绪，可以创建。让我们定义一个 classify() 方法，该方法将设置网络、训练它并运行它。
- en: Listing 7.16 IrisTest.java continued
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.16 IrisTest.java 继续部分
- en: '[PRE16]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The layerStructure argument of the Network constructor specifies a network with
    three layers (one input layer, one hidden layer, and one output layer) with {4,
    6, 3}. The input layer has four neurons, the hidden layer has six neurons, and
    the output layer has three neurons. The four neurons in the input layer map directly
    to the four parameters that are used to classify each specimen. The three neurons
    in the output layer map directly to the three different species that we are trying
    to classify each input within. The hidden layer’s six neurons are more the result
    of trial and error than some formula. The same is true of learningRate. These
    two values (the number of neurons in the hidden layer and the learning rate) can
    be experimented with if the accuracy of the network is suboptimal.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 网络构造函数的 layerStructure 参数指定了一个具有三个层（一个输入层、一个隐藏层和一个输出层）的网络，其配置为 {4, 6, 3}。输入层有四个神经元，隐藏层有六个神经元，输出层有三个神经元。输入层的四个神经元直接映射到用于对每个标本进行分类的四个参数。输出层的三个神经元直接映射到我们试图对每个输入进行分类的三个不同物种。隐藏层的六个神经元更多的是基于试错的结果，而不是某个公式。学习率也是这样。这两个值（隐藏层中的神经元数量和学习率）如果网络的准确性不佳，可以进行实验。
- en: Listing 7.17 IrisTest.java continued
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.17 IrisTest.java 继续部分
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: We train on the first 140 irises out of the 150 in the data set. Recall that
    the lines read from the CSV file were shuffled. This ensures that every time we
    run the program, we will be training on a different subset of the data set. Note
    that we train over the 140 irises 50 times. Modifying this value will have a large
    effect on how long it takes your neural network to train. Generally, the more
    training, the more accurately the neural network will perform, although there
    is a risk of what is known as *overfitting*. The final test will be to verify
    the correct classification of the final 10 irises from the data set. We do this
    at the end of classify(), and we run the network from main().
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在数据集中的前 140 个鸢尾花上进行训练，共有 150 个鸢尾花。回想一下，从 CSV 文件中读取的行是随机排序的。这确保了每次运行程序时，我们都会在不同的数据集子集上进行训练。请注意，我们对
    140 个鸢尾花进行了 50 次训练。修改这个值将对您的神经网络训练所需的时间产生重大影响。一般来说，训练越多，神经网络的性能越准确，尽管存在所谓的 *过拟合*
    的风险。最终的测试将是验证数据集中最后 10 个鸢尾花的正确分类。我们在 classify() 的末尾做这件事，并在 main() 中运行网络。
- en: Listing 7.18 IrisTest.java continued
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.18 IrisTest.java 继续部分
- en: '[PRE18]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'All of the work leads up to this final question: out of 10 randomly chosen
    irises from the data set, how many can our neural network correctly classify?
    Because there is randomness in the starting weights of each neuron, different
    runs may give you different results. You can try tweaking the learning rate, the
    number of hidden neurons, and the number of training iterations to make your network
    more accurate.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些工作都指向最终的问题：从数据集中随机选择的 10 个鸢尾花中，我们的神经网络能正确分类多少个？由于每个神经元的起始权重中存在随机性，不同的运行可能会给出不同的结果。你可以尝试调整学习率、隐藏神经元的数量和训练迭代次数，以提高网络的准确性。
- en: 'Ultimately, you should see a result that is close to this:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，你应该看到接近以下结果：
- en: '[PRE19]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 7.5.3 Classifying wine
  id: totrans-149
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 分类葡萄酒
- en: 'We are going to test our neural network with another data set, one based on
    the chemical analysis of wine cultivars from Italy.[4](#pgfId-1161772) There are
    178 samples in the data set. The machinery of working with it will be much the
    same as with the iris data set, but the layout of the CSV file is slightly different.
    Here is a sample:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用另一个数据集来测试我们的神经网络，这个数据集基于意大利葡萄酒品种的化学分析。[4](#pgfId-1161772) 数据集中有 178 个样本。处理它的机制将与鸢尾花数据集类似，但
    CSV 文件的布局略有不同。以下是一个示例：
- en: '[PRE20]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: The first value on each line will always be an integer from 1 to 3 representing
    one of three cultivars that the sample may be a kind of. But notice how many more
    parameters there are for classification. In the iris data set, there were just
    4\. In this wine data set, there are 13.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 每行的第一个值将始终是一个整数，从 1 到 3，代表样本可能属于的三种品种之一。但请注意，用于分类的参数数量要多得多。在鸢尾花数据集中，只有 4 个参数。在这个葡萄酒数据集中，有
    13 个。
- en: Our neural network model will scale just fine. We simply need to increase the
    number of input neurons. WineTest.java is analogous to IrisTest.java, but there
    are some minor changes to account for the different layouts of the respective
    files.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的神经网络模型将能够很好地扩展。我们只需要增加输入神经元的数量。WineTest.java与IrisTest.java类似，但对相应文件的布局进行了一些小的修改。
- en: Listing 7.19 WineTest.java
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.19 WineTest.java
- en: '[PRE21]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: wineInterpretOutput() is analogous to irisInterpretOutput(). Because we do not
    have names for the wine cultivars, we are just working with the integer assignment
    in the original data set.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: wineInterpretOutput()与irisInterpretOutput()类似。因为我们没有葡萄酒品种的名称，所以我们只是在原始数据集中使用整数分配。
- en: Listing 7.20 WineTest.java continued
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.20 WineTest.java 继续显示
- en: '[PRE22]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The layer configuration for the wine classification network needs 13 input neurons,
    as was already mentioned (one for each parameter). It also needs 3 output neurons.
    There are three cultivars of wine, just as there were three species of iris. Interestingly,
    the network works well with fewer neurons in the hidden layer than in the input
    layer. One possible intuitive explanation for this is that some of the input parameters
    are not actually helpful for classification, and it is useful to cut them out
    during processing. This is not, in fact, exactly how having fewer neurons in the
    hidden layer works, but it is an interesting intuitive idea.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于葡萄酒分类网络，层配置需要 13 个输入神经元，正如之前提到的（每个参数一个）。它还需要 3 个输出神经元。正如有三种鸢尾花品种一样，也有三种葡萄酒品种。有趣的是，网络在隐藏层中的神经元数量少于输入层时也能很好地工作。一个可能的直观解释是，一些输入参数实际上对分类没有帮助，并且在处理过程中删除它们是有用的。实际上，隐藏层中神经元数量较少的工作方式并不完全是这样，但这是一个有趣的直观想法。
- en: Listing 7.21 WineTest.java continued
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.21 WineTest.java 继续显示
- en: '[PRE23]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Once again, it can be interesting to experiment with a different number of hidden-layer
    neurons or a different learning rate.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，尝试不同的隐藏层神经元数量或不同的学习率是有趣的。
- en: Listing 7.22 WineTest.java continued
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.22 WineTest.java 继续显示
- en: '[PRE24]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We will train over the first 150 samples in the data set, leaving the last 28
    for validation. We will train 10 times over the samples, significantly less than
    the 50 for the iris data set. For whatever reason (perhaps innate qualities of
    the data set or tuning of parameters like the learning rate and number of hidden
    neurons), this data set requires less training to achieve significant accuracy
    than the iris data set.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在数据集的前 150 个样本上进行训练，留下最后的 28 个样本进行验证。我们将对样本进行 10 次训练，这比鸢尾花数据集的 50 次要少得多。无论什么原因（可能是数据集的内在特性或学习率、隐藏神经元数量等参数的调整），这个数据集比鸢尾花数据集需要更少的训练就能达到显著的准确性。
- en: Listing 7.23 WineTest.java continued
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.23 WineTest.java 继续显示
- en: '[PRE25]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'With a little luck, your neural network should be able to classify the 28 samples
    quite accurately:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 如果有点运气，你的神经网络应该能够非常准确地分类这28个样本：
- en: '[PRE26]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 7.6 Speeding up neural networks
  id: totrans-170
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 加速神经网络
- en: Neural networks require a lot of vector/matrix math. Essentially, this means
    taking a list of numbers and doing an operation on all of them at once. Libraries
    for optimized, performant vector/matrix math are increasingly important as machine
    learning continues to permeate our society. Many of these libraries take advantage
    of GPUs, because GPUs are optimized for this role. (Vectors/matrices are at the
    heart of computer graphics.) An older library specification you may have heard
    of is BLAS (Basic Linear Algebra Subprograms). A BLAS implementation underlies
    many numerical libraries, including the Java library ND4J.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络需要大量的向量/矩阵数学。本质上，这意味着对一系列数字进行一次操作。随着机器学习继续渗透到我们的社会中，优化、高性能的向量/矩阵数学库变得越来越重要。许多这些库利用GPU，因为GPU为此角色进行了优化。（向量/矩阵是计算机图形学的核心。）你可能听说过的较老的库规范是BLAS（基本线性代数子程序）。许多数值库，包括Java库ND4J，都基于BLAS实现。
- en: Beyond the GPU, CPUs have extensions that can speed up vector/matrix processing.
    BLAS implementations often include functions that make use of *single instruction*,
    *multiple data* (SIMD) instructions. SIMD instructions are special microprocessor
    instructions that allow multiple pieces of data to be processed at once. They
    are sometimes known as *vector instructions*.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 除了GPU之外，CPU也有可以加速向量/矩阵处理的扩展。BLAS实现通常包括使用*单指令，多数据*（SIMD）指令的函数。SIMD指令是特殊的微处理器指令，允许同时处理多个数据。它们有时也被称为*向量指令*。
- en: Different microprocessors include different SIMD instructions. For example,
    the SIMD extension to the G4 (a PowerPC architecture processor found in early
    ’00s Macs) was known as AltiVec. ARM microprocessors, like those found in iPhones,
    have an extension known as NEON. And modern Intel microprocessors include SIMD
    extensions known as MMX, SSE, SSE2, and SSE3\. Luckily, you do not need to know
    the differences. A well-turned numerical library will automatically choose the
    right instructions for computing efficiently on the underlying architecture that
    your program is running on.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的微处理器包括不同的SIMD指令。例如，G4（一种在2000年代初期Mac中发现的PowerPC架构处理器）的SIMD扩展被称为AltiVec。iPhone中发现的ARM微处理器有一个名为NEON的扩展。现代英特尔微处理器包括名为MMX、SSE、SSE2和SSE3的SIMD扩展。幸运的是，你不需要知道这些差异。一个精心设计的数值库会自动选择适合在程序运行的底层架构上高效计算的指令。
- en: It is no surprise, then, that real-world neural network libraries (unlike our
    toy library in this chapter) use specialized types as their base data structure
    instead of Java standard library lists or arrays. But they go even further. Popular
    neural network libraries like TensorFlow and PyTorch not only make use of SIMD
    instructions, but also make extensive use of GPU computing. Because GPUs are explicitly
    designed for fast vector computations, this accelerates neural networks by an
    order of magnitude compared with running on a CPU alone.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，现实世界的神经网络库（与本章中的玩具库不同）使用专用类型作为其基本数据结构，而不是Java标准库列表或数组，这并不令人惊讶。但它们做得更远。像TensorFlow和PyTorch这样的流行神经网络库不仅使用SIMD指令，而且还大量使用GPU计算。因为GPU是专门为快速向量计算设计的，所以与仅使用CPU相比，这可以加速神经网络一个数量级。
- en: 'Let’s be clear: *You would never want to naively implement a neural network
    for production using just the Java standard library as we did in this chapter*.
    Instead, you should use a well-optimized, SIMD- and GPU-enabled library like TensorFlow.
    The only exceptions would be a neural network library designed for education or
    one that had to run on an embedded device without SIMD instructions or a GPU.'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们明确一点：*你绝对不希望像本章中那样，仅仅使用Java标准库天真地实现用于生产的神经网络*。相反，你应该使用像TensorFlow这样的经过良好优化、支持SIMD和GPU的库。唯一的例外可能是为教育设计的神经网络库，或者必须在没有SIMD指令或GPU的嵌入式设备上运行的神经网络库。
- en: 7.7 Neural network problems and extensions
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.7 神经网络问题与扩展
- en: Neural networks are all the rage right now, thanks to advances in deep learning,
    but they have some significant shortcomings. The biggest problem is that a neural
    network solution to a problem is something of a black box. Even when neural networks
    work well, they do not give the user much insight into how they solve the problem.
    For instance, the iris data set classifier we worked on in this chapter does not
    clearly show how much each of the four parameters in the input affects the output.
    Was sepal length more important than sepal width for classifying each sample?
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络目前非常流行，这得益于深度学习的进步，但它们存在一些显著的缺点。最大的问题是，神经网络对问题的解决方案有点像黑箱。即使神经网络工作得很好，它们也不会给用户太多关于如何解决问题的洞察。例如，我们在本章中工作的鸢尾花数据集分类器并没有清楚地显示输入中的四个参数中的每一个对输出的影响程度。是花瓣长度比花瓣宽度对分类每个样本更重要吗？
- en: It is possible that careful analysis of the final weights for the trained network
    could provide some insight, but such analysis is nontrivial and does not provide
    the kind of insight that, say, linear regression does in terms of the meaning
    of each variable in the function being modeled. In other words, a neural network
    may solve a problem, but it does not explain how the problem is solved.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 可能对训练网络的最终权重进行仔细分析可以提供一些洞察，但这种分析并不简单，并且不会提供类似于线性回归在模型中每个变量的意义方面的洞察。换句话说，神经网络可以解决问题，但它并不解释问题是如何解决的。
- en: Another problem with neural networks is that to become accurate, they often
    require very large data sets. Imagine an image classifier for outdoor landscapes.
    It may need to classify thousands of different types of images (forests, valleys,
    mountains, streams, steppes, and so on). It will potentially need millions of
    training images. Not only are such large data sets hard to come by, but also,
    for some applications they may be completely nonexistent. It tends to be large
    corporations and governments that have the data-warehousing and technical facilities
    for collecting and storing such massive data sets.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的另一个问题是，为了变得准确，它们通常需要非常大的数据集。想象一下用于户外景观的图像分类器。它可能需要分类成千上万种不同的图像类型（森林、山谷、山脉、溪流、草原等等）。它可能需要数百万个训练图像。不仅这样的大型数据集很难获得，而且对于某些应用，它们可能根本不存在。通常是大公司和政府拥有收集和存储这样庞大数据集的数据仓库和技术设施。
- en: Finally, neural networks are computationally expensive. Just training on a moderately
    sized data set can bring your computer to its knees. And it’s not just naive neural
    network implementations--with any computational platform where neural networks
    are used, it is the sheer number of calculations that have to be performed in
    training the network, more than anything else, that takes so much time. Many tricks
    abound to make neural networks more performant (like using SIMD instructions or
    GPUs), but ultimately, training a neural network requires a lot of floating-point
    operations.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，神经网络在计算上非常昂贵。仅仅在中等规模的数据集上进行训练就可以让你的电脑不堪重负。这不仅仅是简单的神经网络实现——在任何使用神经网络的计算平台上，训练网络时必须执行的计算数量，而不是其他任何因素，才是花费如此多的时间的主要原因。有许多技巧可以使神经网络更高效（比如使用SIMD指令或GPU），但最终，训练神经网络需要大量的浮点运算。
- en: One nice realization is that training is much more computationally expensive
    than actually using the network. Some applications do not require ongoing training.
    In those instances, a trained network can just be dropped into an application
    to solve a problem. For example, the first version of Apple’s Core ML framework
    did not even support training. It only supported helping app developers run pretrained
    neural network models in their apps. An app developer creating a photo app could
    download a freely licensed image-classification model, drop it into Core ML, and
    start using performant machine learning in an app instantly.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一个令人欣慰的认识是，训练的计算成本远高于实际使用网络。有些应用不需要持续的训练。在这些情况下，一个训练好的网络可以直接应用到应用中解决问题。例如，苹果公司Core
    ML框架的第一个版本甚至不支持训练。它只支持帮助应用开发者在其应用中运行预训练的神经网络模型。一个创建照片应用的应用开发者可以下载一个免费许可的图像分类模型，将其放入Core
    ML中，并立即在应用中使用高效的机器学习。
- en: 'In this chapter, we only worked with a single type of neural network: a feed-forward
    network with backpropagation. As has been mentioned, many other kinds of neural
    networks exist. Convolutional neural networks are also feed-forward, but they
    have multiple different types of hidden layers, different mechanisms for distributing
    weights, and other interesting properties that make them especially well designed
    for image classification. In recurrent neural networks, signals do not just travel
    in one direction. They allow feedback loops and have proven useful for continuous
    input applications like handwriting recognition and voice recognition.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们只使用了一种类型的神经网络：具有反向传播的前馈网络。正如已经提到的，存在许多其他类型的神经网络。卷积神经网络也是前馈的，但它们有多个不同类型的隐藏层，不同的权重分配机制，以及其他有趣的特性，使它们特别适合图像分类。在循环神经网络中，信号不仅在一个方向上传播。它们允许反馈循环，并且已被证明对于连续输入应用，如手写识别和语音识别，非常有用。
- en: A simple extension to our neural network that would make it more performant
    would be the inclusion of bias neurons. A bias neuron is like a dummy neuron in
    a layer that allows the next layer’s output to represent more functions by providing
    a constant input (still modified by a weight) into it. Even simple neural networks
    used for real-world problems usually contain bias neurons. If you add bias neurons
    to our existing network, you will likely find that it requires less training to
    achieve a similar level of accuracy.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 对我们的神经网络的一个简单扩展，使其性能更佳，就是包含偏差神经元。偏差神经元就像一层中的虚拟神经元，它允许下一层的输出通过提供一个常数输入（仍然由权重修改）来表示更多函数。即使是用于现实世界问题的简单神经网络通常也包含偏差神经元。如果你向我们的现有网络添加偏差神经元，你可能会发现它需要更少的训练就能达到相似的准确度水平。
- en: 7.8 Real-world applications
  id: totrans-184
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.8 真实世界的应用
- en: Although they were first imagined in the middle of the 20th century, artificial
    neural networks did not become commonplace until the last decade. Their widespread
    application was held back by a lack of sufficiently performant hardware. Today,
    artificial neural networks have become the most explosive growth area in machine
    learning because they work!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们是在20世纪中叶被想象出来的，但人工神经网络直到最近十年才变得普遍。它们广泛的应用受到了缺乏足够性能的硬件的限制。今天，由于它们有效，人工神经网络已经成为机器学习中最爆炸性的增长领域。
- en: Artificial neural networks have enabled some of the most exciting user-facing
    computing applications in decades. These include practical voice recognition (practical
    in terms of sufficient accuracy), image recognition, and handwriting recognition.
    Voice recognition is present in typing aids like Dragon NaturallySpeaking and
    digital assistants like Siri, Alexa, and Cortana. A specific example of image
    recognition is Facebook’s automatic tagging of people in a photo using facial
    recognition. In recent versions of iOS, you can search words within your notes,
    even if they are handwritten, by employing handwriting recognition.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 人工神经网络使得几十年来最激动人心的面向用户的计算应用成为可能。这包括实用的语音识别（在足够准确性的意义上是实用的）、图像识别和手写识别。语音识别存在于像Dragon
    NaturallySpeaking这样的打字辅助工具和像Siri、Alexa和Cortana这样的数字助手中。图像识别的一个具体例子是Facebook使用面部识别自动标记照片中的人。在iOS的最新版本中，你可以通过使用手写识别来搜索你的笔记中的单词，即使它们是手写的。
- en: An older recognition technology that can be powered by neural networks is OCR
    (optical character recognition). OCR is used every time you scan a document and
    it comes back as selectable text instead of an image. OCR enables toll booths
    to read license plates and envelopes to be quickly sorted by the postal service.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 一种可以由神经网络驱动的较老识别技术是OCR（光学字符识别）。每次你扫描文档并返回为可选择的文本而不是图像时，都会使用OCR。OCR使收费站能够读取车牌，并使邮局能够快速对信封进行分类。
- en: In this chapter you have seen neural networks used successfully for classification
    problems. Similar applications that neural networks work well in are recommendation
    systems. Think of Netflix suggesting a movie you might like to watch or Amazon
    suggesting a book you might want to read. There are other machine learning techniques
    that work well for recommendation systems too (Amazon and Netflix do not necessarily
    use neural networks for these purposes; the details of their systems are likely
    proprietary), so neural networks should only be selected after all options have
    been explored.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，您已经看到神经网络在分类问题上的成功应用。神经网络在推荐系统中也表现出良好的应用效果。想想Netflix推荐您可能想看的电影或Amazon推荐您可能想读的书。还有其他机器学习技术也适用于推荐系统（Amazon和Netflix不一定使用神经网络来完成这些任务；他们系统的细节可能是专有的），因此只有在探索了所有选项之后，才应选择神经网络。
- en: Neural networks can be used in any situation where an unknown function needs
    to be approximated. This makes them useful for prediction. Neural networks can
    be employed to predict the outcome of a sporting event, election, or the stock
    market (and they are). Of course, their accuracy is a product of how well they
    are trained, and that has to do with how large a data set relevant to the unknown-outcome
    event is available, how well the parameters of the neural network are tuned, and
    how many iterations of training are run. With prediction, like most neural network
    applications, one of the hardest parts is deciding on the structure of the network
    itself, which is often ultimately determined by trial and error.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络可以用于任何需要近似未知函数的情况。这使得它们在预测方面非常有用。神经网络可以用来预测体育赛事、选举或股市的结果（它们确实可以）。当然，它们的准确性是它们训练得有多好的产物，这涉及到与未知结果事件相关的数据集有多大，神经网络参数调整得有多好，以及运行了多少次训练迭代。在预测方面，像大多数神经网络应用一样，最难的部分之一是决定网络的自身结构，这通常最终是通过试错来确定的。
- en: 7.9 Exercises
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.9 练习
- en: Use the neural network framework developed in this chapter to classify items
    in another data set.
  id: totrans-191
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用本章开发的神经网络框架对另一个数据集中的项目进行分类。
- en: Try running the examples with a different activation function. (Remember to
    also find its derivative.) How does the change in activation function affect the
    accuracy of the network? Does it require more or less training?
  id: totrans-192
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试使用不同的激活函数运行示例。（记得也要找到它的导数。）激活函数的变化如何影响网络的准确性？是否需要更多或更少的训练？
- en: Take the problems in this chapter and re-create their solutions using a popular
    neural network framework like TensorFlow.
  id: totrans-193
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将本章中的问题重新创建解决方案，使用流行的神经网络框架，如TensorFlow。
- en: Rewrite the Network, Layer, and Neuron classes using a third-party Java numerical
    library to accelerate the execution of the neural network developed in this chapter.
  id: totrans-194
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用第三方Java数值库重写网络、层和神经元类，以加速本章开发的神经网络执行。
- en: '* * *'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '1. Stuart Russell and Peter Norvig, Artificial Intelligence: A Modern Approach,
    3rd ed. (Pearson, 2010).'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Stuart Russell和Peter Norvig，《人工智能：现代方法》，第3版（培生，2010年）。
- en: 2. The repository is available from GitHub at [https://github.com/davecom/ClassicComputerScienceProblemsInJava](https://github.com/davecom/ClassicComputerScienceProblemsInJava).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 仓库可在GitHub上获取，[https://github.com/davecom/ClassicComputerScienceProblemsInJava](https://github.com/davecom/ClassicComputerScienceProblemsInJava)。
- en: '3. M. Lichman, UCI Machine Learning Repository (Irvine, CA: University of California,
    School of Information and Computer Science, 2013), [http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml).'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 3. M. Lichman，UCI机器学习仓库（加州欧文，加州大学信息与计算机科学学院，2013年），[http://archive.ics.uci.edu/ml](http://archive.ics.uci.edu/ml)。
- en: 4. See footnote 3.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 参见脚注3。
