- en: Part 3\. Next steps
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 3 部分\. 下一步
- en: In these final chapters, you’ll learn about common pitfalls that many Redis
    users encounter (reducing memory use, scaling performance, and scripting with
    Lua), as well as how to solve those issues with standard techniques.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些最后一章中，你将了解许多 Redis 用户遇到的一些常见陷阱（降低内存使用、扩展性能和 Lua 脚本），以及如何使用标准技术解决这些问题。
- en: Chapter 9\. Reducing memory use
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 9 章\. 降低内存使用
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Short structures
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 短结构
- en: Sharded structures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分片结构
- en: Packing bits and bytes
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比特和字节打包
- en: In this chapter, we’ll cover three important methods to help reduce your memory
    use in Redis. By reducing the amount of memory you use in Redis, you can reduce
    the time it takes to create or load a snapshot, rewrite or load an append-only
    file, reduce slave synchronization time,^([[1](#ch09fn01)]) and store more data
    in Redis without additional hardware.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍三种重要的方法来帮助降低你在 Redis 中的内存使用。通过减少你在 Redis 中使用的内存量，你可以减少创建或加载快照、重写或加载只写文件、减少从服务器同步时间，以及在不增加额外硬件的情况下在
    Redis 中存储更多数据。
- en: ¹ Snapshots, append-only file rewriting, and slave synchronization are all discussed
    in [chapter 4](kindle_split_015.html#ch04).
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 快照、只写文件重写和从服务器同步都在第 4 章（[kindle_split_015.html#ch04](https://kindle_split_015.html#ch04)）中讨论。
- en: We’ll begin this chapter by discussing how the use of short data structures
    in Redis can result in a more efficient representation of the data. We’ll then
    discuss how to apply a concept called *sharding* to help make some larger structures
    small.^([[2](#ch09fn02)]) Finally, we’ll talk about packing fixed-length data
    into `STRING`s for even greater memory savings.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先讨论在 Redis 中使用短数据结构如何导致数据表示更加高效。然后，我们将讨论如何应用称为 *分片* 的概念来帮助使一些较大的结构变小。[^2](#ch09fn02)
    最后，我们将讨论将固定长度数据打包到 `STRING` 中以实现更大的内存节省。
- en: ² Our use of sharding here is primarily driven to reduce memory use on a single
    server. In [chapter 10](kindle_split_022.html#ch10), we’ll apply similar techniques
    to allow for increased read throughput, write throughput, and memory partitioning
    across multiple Redis servers.
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ² 我们在这里使用分片主要是为了减少单个服务器上的内存使用。在第 10 章（[kindle_split_022.html#ch10](https://kindle_split_022.html#ch10)）中，我们将应用类似的技术，以允许增加多个
    Redis 服务器上的读取吞吐量、写入吞吐量和内存分区。
- en: 'When used together, these methods helped me to reduce memory use from more
    than 70 gigabytes, split across three machines, down to under 3 gigabytes on a
    single machine. As we work through these methods, remember that some of our earlier
    problems would lend themselves well to these optimizations, which I’ll point out
    when applicable. Let’s get started with one of the first and easiest methods to
    reduce memory use: short structures.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当这些方法结合使用时，帮助我将内存使用从超过 70 个 GB，分布在三台机器上，降低到单台机器上的不到 3 个 GB。在我们通过这些方法时，请记住，我们的一些早期问题非常适合这些优化，我将在适用时指出。让我们从第一个也是最简单的方法开始，即减少内存使用的短结构。
- en: 9.1\. Short structures
  id: totrans-12
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1\. 短结构
- en: 'The first method of reducing memory use in Redis is simple: use short structures.
    For `LIST`s, `SET`s, `HASH`es, and `ZSET`s, Redis offers a group of configuration
    options that allows for Redis to store short structures in a more space-efficient
    manner. In this section, we’ll discuss those configuration options, show how to
    verify that we’re getting those optimizations, and discuss some drawbacks to using
    short structures.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 降低 Redis 中内存使用的第一种方法很简单：使用短结构。对于 `LIST`、`SET`、`HASH` 和 `ZSET`，Redis 提供了一组配置选项，允许
    Redis 以更节省空间的方式存储短结构。在本节中，我们将讨论这些配置选项，展示如何验证我们是否获得了这些优化，并讨论使用短结构的一些缺点。
- en: When using short `LIST`s, `HASH`es, and `ZSET`s, Redis can optionally store
    them using a more compact storage method known as a *ziplist*. A ziplist is an
    unstructured representation of one of the three types of objects. Rather than
    storing the doubly linked list, the hash table, or the hash table plus the skiplist
    as would normally be the case for each of these structures, Redis stores a serialized
    version of the data, which must be decoded for every read, partially re-encoded
    for every write, and may require moving data around in memory.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用短的 `LIST`、`HASH` 和 `ZSET` 时，Redis 可以选择使用一种称为 *ziplist* 的更紧凑的存储方法。ziplist
    是三种类型对象之一的无结构表示。与通常情况下存储双链表、哈希表或哈希表加跳表不同，Redis 存储数据的序列化版本，每次读取时都需要解码，每次写入时部分重新编码，并且可能需要在内存中移动数据。
- en: 9.1.1\. The ziplist representation
  id: totrans-15
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1\. 链表表示法
- en: 'To understand why ziplists may be more efficient, we only need to look at the
    simplest of our structures, the `LIST`. In a typical doubly linked list, we have
    structures called *nodes*, which represent each value in the list. Each of these
    nodes has pointers to the previous and next nodes in the list, as well as a pointer
    to the string in the node. Each string value is actually stored as three parts:
    an integer representing the length, an integer representing the number of remaining
    free bytes, and the string itself followed by a null character. An example of
    this in [figure 9.1](#ch09fig01) shows the three string values `"one"`, `"two"`,
    and `"ten"` as part of a larger linked list.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解为什么 ziplist 可能更高效，我们只需看看我们结构中最简单的，即 `LIST`。在一个典型的双向链表中，我们有称为 *nodes* 的结构，它们代表列表中的每个值。这些节点中的每一个都指向列表中前一个和下一个节点，以及节点中的字符串指针。每个字符串值实际上存储为三个部分：一个表示长度的整数，一个表示剩余空闲字节数的整数，以及字符串本身后跟一个空字符。[图
    9.1](#ch09fig01) 中的例子显示了三个字符串值 `"one"`、`"two"` 和 `"ten"` 作为更大链表的一部分。
- en: Figure 9.1\. How long `LIST`s are stored in Redis
  id: totrans-17
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.1\. Redis 中 `LIST` 存储的长度
- en: '![](09fig01_alt.jpg)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片 09fig01_alt](09fig01_alt.jpg)'
- en: Ignoring a few details (which only make linked lists look worse), each of these
    three strings that are each three characters long will actually take up space
    for three pointers, two integers (the length and remaining bytes in the value),
    plus the string and an extra byte. On a 32-bit platform, that’s 21 bytes of overhead
    to store 3 actual bytes of data (remember, this is an underestimate of what’s
    actually stored).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 忽略一些细节（这些细节只会让链表看起来更糟糕），这三个各为三个字符长的字符串实际上将占用三个指针的空间，两个整数（长度和值中的剩余字节），加上字符串和额外的字节。在
    32 位平台上，这需要 21 字节的开销来存储 3 个实际字节的数据（记住，这是一个低估了实际存储量的估计）。
- en: On the other hand, the ziplist representation will store a sequence of length,
    length, string elements. The first length is the size of the previous entry (for
    easy scanning in both directions), the second length is the size of the current
    entry, and the string is the stored data itself. There are some other details
    about what these lengths really mean in practice, but for these three example
    strings, the lengths will be 1 byte long, for 2 bytes of overhead per entry in
    this example. By not storing additional pointers and metadata, the ziplist can
    cut down overhead from 21 bytes each to roughly 2 bytes (in this example).
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，ziplist 表示将存储一系列长度、长度、字符串元素。第一个长度是前一个条目的大小（便于双向扫描），第二个长度是当前条目的大小，字符串是存储的数据本身。关于这些长度在实际中真正意味着什么还有一些其他细节，但就这三个示例字符串而言，长度将是
    1 字节长，在这个例子中每个条目有 2 字节的开销。通过不存储额外的指针和元数据，ziplist 可以将开销从每个 21 字节减少到大约 2 字节（在这个例子中）。
- en: Let’s see how we can ensure that we’re using the compact ziplist encoding.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们如何确保使用紧凑的 ziplist 编码。
- en: Using the ziplist encoding
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用 ziplist 编码
- en: In order to ensure that these structures are only used when necessary to reduce
    memory, Redis includes six configuration options, shown in the following listing,
    for determining when the ziplist representation will be used for `LIST`s, `HASH`es,
    and `ZSET`s.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保这些结构仅在必要时使用以减少内存，Redis 包含了六个配置选项，如下所示，用于确定何时将 ziplist 表示用于 `LIST`、`HASH`
    和 `ZSET`。
- en: Listing 9.1\. Configuration options for the ziplist representation of different
    structures
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.1\. 不同结构的 ziplist 表示的配置选项
- en: '![](211fig01_alt.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片 211fig01_alt](211fig01_alt.jpg)'
- en: The basic configuration options for `LIST`s, `HASH`es, and `ZSET`s are all similar,
    composed of `-max-ziplist-entries` settings and `-max-ziplist-value` settings.
    Their semantics are essentially identical in all three cases. The `entries` settings
    tell us the maximum number of items that are allowed in the `LIST`, `HASH`, or
    `ZSET` for them to be encoded as a ziplist. The `value` settings tell us how large
    in bytes each individual entry can be. If either of these limits are exceeded,
    Redis will convert the `LIST`, `HASH`, or `ZSET` into the nonziplist structure
    (thus increasing memory).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: '`LIST`、`HASH` 和 `ZSET` 的基本配置选项都是相似的，由 `-max-ziplist-entries` 设置和 `-max-ziplist-value`
    设置组成。它们的语义在这三种情况下本质上都是相同的。`entries` 设置告诉我们，当 `LIST`、`HASH` 或 `ZSET` 中的项目数达到最大值时，它们将被编码为
    ziplist。`value` 设置告诉我们每个单独条目可以有多大。如果这些限制中的任何一个被超过，Redis 将将 `LIST`、`HASH` 或 `ZSET`
    转换为非 ziplist 结构（从而增加内存）。'
- en: If we’re using an installation of Redis 2.6 with the default configuration,
    Redis should have default settings that are the same as what was provided in [listing
    9.1](#ch09ex01). Let’s play around with ziplist representations of a simple `LIST`
    object by adding some items and checking its representation, as shown in the next
    listing.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用的是默认配置的 Redis 2.6 版本，Redis 应该具有默认设置，这些设置与 [列表 9.1](#ch09ex01) 中提供的内容相同。让我们通过添加一些项目并检查其表示来玩转简单
    `LIST` 对象的 ziplist 表示，如下一个列表所示。
- en: Listing 9.2\. How to determine whether a structure is stored as a ziplist
  id: totrans-28
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.2。如何确定一个结构是否存储为 ziplist
- en: '![](212fig01_alt.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图 2-13](212fig01_alt.jpg)'
- en: With the new `DEBUG OBJECT` command at our disposal, discovering whether an
    object is stored as a ziplist can be helpful to reduce memory use.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 利用我们可用的新 `DEBUG OBJECT` 命令，发现一个对象是否存储为 ziplist 可以帮助减少内存使用。
- en: You’ll notice that one structure is obviously missing from the special ziplist
    encoding, the `SET`. `SET`s also have a compact representation, but different
    semantics and limits, which we’ll cover next.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，特殊 ziplist 编码中显然缺少一个结构，那就是 `SET`。`SET` 也有紧凑的表示，但具有不同的语义和限制，我们将在下一部分介绍。
- en: 9.1.2\. The intset encoding for SETs
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.2. `SET` 的 intset 编码
- en: Like the ziplist for `LIST`s, `HASH`es, and `ZSET`s, there’s also a compact
    representation for short `SET`s. If our `SET` members can all be interpreted as
    base-10 integers within the range of our platform’s signed long integer, and our
    `SET` is short enough (we’ll get to that in a moment), Redis will store our `SET`
    as a sorted array of integers, or *intset*.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 与 `LIST`、`HASH` 和 `ZSET` 的 ziplist 一样，对于短 `SET`，也存在紧凑的表示。如果我们的 `SET` 成员都可以解释为我们平台有符号长整数的范围之内的十进制整数，并且我们的
    `SET` 足够短（我们稍后会提到），Redis 将将我们的 `SET` 存储为整数排序数组，或称为 *intset*。
- en: By storing a `SET` as a sorted array, not only do we have low overhead, but
    all of the standard `SET` operations can be performed quickly. But how big is
    too big? The next listing shows the configuration option for defining an intset’s
    maximum size.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将 `SET` 存储为排序数组，我们不仅具有低开销，而且所有标准的 `SET` 操作都可以快速执行。但是，多大才算太大？下一个列表显示了定义 intset
    最大大小的配置选项。
- en: Listing 9.3\. Configuring the maximum size of the intset encoding for `SET`s
  id: totrans-35
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.3。配置 `SET` 的 intset 编码的最大大小
- en: '![](212fig02.jpg)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![图 2-12](212fig02.jpg)'
- en: As long as we keep our `SET`s of integers smaller than our configured size,
    Redis will use the intset representation to reduce data size. The following listing
    shows what happens when an intset grows to the point of being too large.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 只要我们的整数 `SET` 小于我们配置的大小，Redis 就会使用 intset 表示来减少数据大小。以下列表显示了 intset 增长到太大时的发生情况。
- en: Listing 9.4\. When an intset grows to be too large, it’s represented as a hash
    table.
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4。当一个 intset 增长到太大时，它将以哈希表的形式表示。
- en: '![](213fig01_alt.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![图 2-13](213fig01_alt.jpg)'
- en: Earlier, in the introduction to [section 9.1](#ch09lev1sec1), I mentioned that
    to read or update part of an object that uses the compact ziplist representation,
    we may need to decode the entire ziplist, and may need to move in-memory data
    around. For these reasons, reading and writing large ziplist-encoded structures
    can reduce performance. Intset-encoded `SET`s also have similar issues, not so
    much due to encoding and decoding the data, but again because we need to move
    data around when performing insertions and deletions. Next, we’ll examine some
    performance issues when operating with long ziplists.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面 [第 9.1 节](#ch09lev1sec1) 的介绍中，我提到，为了读取或更新使用紧凑 ziplist 表示的对象的一部分，我们可能需要解码整个
    ziplist，并且可能需要移动内存中的数据。由于这些原因，读取和写入大型 ziplist 编码的结构可能会降低性能。intset 编码的 `SET` 也存在类似的问题，这不仅仅是因为编码和解码数据，而是因为我们需要在执行插入和删除操作时移动数据。接下来，我们将检查在操作长
    ziplist 时的一些性能问题。
- en: 9.1.3\. Performance issues for long ziplists and intsets
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.3. 长ziplist和intset的性能问题
- en: As our structures grow beyond the ziplist and intset limits, they’re automatically
    converted into their more typical underlying structure types. This is done primarily
    because manipulating the compact versions of these structures can become slow
    as they grow longer.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的结构超过 ziplist 和 intset 的限制时，它们会自动转换为更典型的底层结构类型。这主要是因为随着这些结构的增长，操作它们的紧凑版本可能会变得缓慢。
- en: For a firsthand look at how this happens, let’s start by updating our setting
    for `list-max-ziplist-entries` to 110,000\. This setting is a lot larger than
    we’d ever use in practice, but it does help to highlight the issue. After that
    setting is updated and Redis is restarted, we’ll benchmark Redis to discover performance
    issues that can happen when long ziplist-encoded `LIST`s are used.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 为了亲自看看这是如何发生的，让我们首先将 `list-max-ziplist-entries` 设置更新为 110,000。这个设置比我们在实践中会用到的要大得多，但它确实有助于突出这个问题。在更新设置并重启
    Redis 之后，我们将基准测试 Redis 以发现使用长 ziplist 编码 `LIST` 时可能出现的性能问题。
- en: To benchmark the behavior of long ziplist-encoded `LIST`s, we’ll write a function
    that creates a `LIST` with a specified number of elements. After the `LIST` is
    created, we’ll repeatedly call the `RPOPLPUSH` command to move an item from the
    right end of the `LIST` to the left end. This will give us a lower bound on how
    expensive commands can be on very long ziplist-encoded `LIST`s. This benchmarking
    function is shown in the next listing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 为了基准测试长 ziplist 编码 `LIST` 的行为，我们将编写一个函数来创建一个具有指定元素数量的 `LIST`。在 `LIST` 创建后，我们将反复调用
    `RPOPLPUSH` 命令，将项目从 `LIST` 的右端移动到左端。这将给我们一个关于非常长的 ziplist 编码 `LIST` 上命令成本的下限。这个基准测试函数在下一个列表中展示。
- en: Listing 9.5\. Our code to benchmark varying sizes of ziplist-encoded `LIST`s
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.5\. 我们用于基准测试不同大小的 ziplist 编码 `LIST` 的代码
- en: '![](214fig01_alt.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](214fig01_alt.jpg)'
- en: As mentioned before, this code creates a `LIST` of a given size and then executes
    a number of `RPOPLPUSH` commands in pipelines. By computing the number of calls
    to `RPOPLPUSH` divided by the amount of time it took, we can calculate a number
    of operations per second that can be executed on ziplist-encoded `LIST`s of a
    given size. Let’s run this with steadily increasing list sizes to see how long
    ziplists can reduce performance.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，此代码创建了一个给定大小的 `LIST`，然后执行一系列 `RPOPLPUSH` 命令。通过计算 `RPOPLPUSH` 调用次数除以所需时间，我们可以计算出在给定大小的
    ziplist 编码 `LIST` 上可以执行的操作数。让我们用不断增加的列表大小运行这个基准测试，看看 ziplist 可以降低性能到什么程度。
- en: Listing 9.6\. As ziplist-encoded `LIST`s grow, we can see performance drop
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.6\. 随着 ziplist 编码 `LIST` 的增长，我们可以看到性能下降
- en: '![](214fig02_alt.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](214fig02_alt.jpg)'
- en: At first glance, you may be thinking that this isn’t so bad even when you let
    a ziplist grow to a few thousand elements. But this shows only a single example
    operation, where all we’re doing is taking items off of the right end and pushing
    them to the left end. The ziplist encoding can find the right or left end of a
    sequence quickly (though shifting all of the items over for the insert is what
    slows us down), and for this small example we can exploit our CPU caches. But
    when scanning through a list for a particular value, like our autocomplete example
    from [section 6.1](kindle_split_017.html#ch06lev1sec1), or fetching/updating individual
    fields of a `HASH`, Redis will have to decode many individual entries, and CPU
    caches won’t be as effective. As a point of data, replacing our `RPOPLPUSH` command
    with a call to `LINDEX` that gets an element in the middle of the `LIST` shows
    performance at roughly half the number of operations per second as our `RPOPLPUSH`
    call when `LIST`s are at least 5,000 items long. Feel free to try it for yourself.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 初看之下，你可能认为即使让 ziplist 增长到几千个元素，这也不是什么大问题。但这只展示了单个操作示例，我们只是从右端取出项目并将它们推到左端。ziplist
    编码可以快速找到序列的右端或左端（尽管插入时移动所有项目会减慢我们），在这个小示例中，我们可以利用我们的 CPU 缓存。但是，当扫描列表以查找特定值时，比如我们的来自
    [第 6.1 节](kindle_split_017.html#ch06lev1sec1) 的自动完成示例，或者获取/更新 `HASH` 的单个字段时，Redis
    将不得不解码许多单个条目，CPU 缓存将不再那么有效。作为一个数据点，用 `LINDEX` 调用来替换我们的 `RPOPLPUSH` 命令，该命令从 `LIST`
    的中间获取一个元素，当 `LIST` 至少有 5,000 个项目长时，其性能大约是 `RPOPLPUSH` 调用性能的一半。欢迎你自己尝试一下。
- en: If you keep your max ziplist sizes in the 500–2,000 item range, and you keep
    the max item size under 128 bytes or so, you should get reasonable performance.
    I personally try to keep max ziplist sizes to 1,024 elements with item sizes at
    64 bytes or smaller. For many uses of `HASH`es that we’ve used so far, these limits
    should let you keep memory use down, and performance high.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你将最大 ziplist 大小保持在 500–2,000 项范围内，并且将最大项目大小保持在 128 字节左右，你应该会得到合理的性能。我个人尝试将最大
    ziplist 大小保持在 1,024 个元素，项目大小在 64 字节或更小。对于到目前为止我们使用的许多 `HASH` 用例，这些限制应该让你能够将内存使用量保持在较低水平，并将性能保持在较高水平。
- en: As you develop solutions to problems outside of our examples, remember that
    if you can keep your `LIST`, `SET`, `HASH`, and `ZSET` sizes small, you can help
    keep your memory use low, which can let you use Redis for a wider variety of problems.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 当你为我们的示例之外的问题开发解决方案时，请记住，如果你能保持你的`LIST`、`SET`、`HASH`和`ZSET`的大小较小，你可以帮助降低内存使用，这可以使你能够使用Redis解决更广泛的问题。
- en: '|  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Keeping key names short
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保持键名短
- en: One thing that I haven’t mentioned before is the use of minimizing the length
    of keys, both in the key portion of all values, as well as keys in `HASH`es, members
    of `SET`s and `ZSET`s, and entries in `LIST`s. The longer all of your strings
    are, the more data needs to be stored. Generally speaking, whenever it’s possible
    to store a relatively abbreviated piece of information like `user:joe` as a key
    or member, that’s preferable to storing `username:joe`, or even `joe` if `user`
    or `username` is implied. Though in some cases it may not make a huge difference,
    if you’re storing millions or billions of entries, those extra few megabytes or
    gigabytes may be useful later.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前没有提到的一点是，最小化键的长度，不仅在所有值的键部分，而且在`HASH`中的键、`SET`的成员和`ZSET`的条目以及`LIST`中的条目中。你的所有字符串越长，需要存储的数据就越多。一般来说，每当可能将相对简短的信息，如`user:joe`作为键或成员存储时，那比存储`username:joe`，甚至如果`user`或`username`是隐含的，存储`joe`更好。尽管在某些情况下可能没有太大差别，但如果你存储了数百万或数十亿条条目，那些额外的几兆字节或吉字节可能以后会有用。
- en: '|  |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that you’ve seen that short structures in Redis can be used to reduce memory
    use, in the next section we’ll talk about sharding large structures to let us
    gain the benefits of the ziplist and intset optimizations for more problems.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到Redis中的短结构可以用来减少内存使用，在下一节中，我们将讨论分片大型结构，以便我们能够从zip列表和intset优化中获得更多问题的好处。
- en: 9.2\. Sharded structures
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2. 分片结构
- en: '*Sharding* is a well-known technique that has been used to help many different
    databases scale to larger data storage and processing loads. Basically, sharding
    takes your data, partitions it into smaller pieces based on some simple rules,
    and then sends the data to different locations depending on which partition the
    data had been assigned to.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: '*分片*是一种众所周知的技术，它已被用于帮助许多不同的数据库扩展到更大的数据存储和处理负载。基本上，分片将你的数据分成更小的部分，基于一些简单的规则，然后根据数据分配到的分区将数据发送到不同的位置。'
- en: In this section, we’ll talk about applying the concept of sharding to `HASH`es,
    `SET`s, and `ZSET`s to support a subset of their standard functionality, while
    still letting us use the small structures from [section 9.1](#ch09lev1sec1) to
    reduce memory use. Generally, instead of storing value X in key Y, we’ll store
    X in key `Y:<shardid>`.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论将分片的概念应用于`HASH`、`SET`和`ZSET`以支持它们标准功能的一个子集，同时仍然让我们能够使用[第9.1节](#ch09lev1sec1)中的小结构来减少内存使用。通常，我们不会将值X存储在键Y中，而是将X存储在键`Y:<shardid>`中。
- en: '|  |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Sharding `LIST`s
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分片`LIST`
- en: Sharding `LIST`s without the use of Lua scripting is difficult, which is why
    we omit it here. When we introduce scripting with Lua in [chapter 11](kindle_split_023.html#ch11),
    we’ll build a sharded `LIST` implementation that supports blocking and nonblocking
    pushes and pops from both ends.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 不使用Lua脚本进行`LIST`的分片是困难的，这就是为什么我们在这里省略了它。当我们[第11章](kindle_split_023.html#ch11)中引入Lua脚本时，我们将构建一个支持从两端进行阻塞和非阻塞推送和弹出的分片`LIST`实现。
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Sharding `ZSET`s
  id: totrans-66
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分片`ZSET`
- en: Unlike sharded `HASH`es and `SET`s, where essentially all operations can be
    supported with a moderate amount of work (or even `LIST`s with Lua scripting),
    commands like `ZRANGE`, `ZRANGEBYSCORE`, `ZRANK`, `ZCOUNT`, `ZREMRANGE`, `ZREMRANGEBYSCORE`,
    and more require operating on all of the shards of a `ZSET` to calculate their
    final result. Because these operations on sharded `ZSET`s violate almost all of
    the expectations about how quickly a `ZSET` should perform with those operations,
    sharding a `ZSET` isn’t necessarily that useful, which is why we essentially omit
    it here.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 与基本上所有操作都可以通过适度的工作（甚至使用Lua脚本的`LIST`）来支持的分片`HASH`和`SET`不同，命令如`ZRANGE`、`ZRANGEBYSCORE`、`ZRANK`、`ZCOUNT`、`ZREMRANGE`、`ZREMRANGEBYSCORE`等需要操作`ZSET`的所有分片来计算它们的最终结果。因为这些操作违反了几乎关于`ZSET`应该以多快的速度执行这些操作的几乎所有预期，所以分片`ZSET`并不一定非常有用，这就是为什么我们在这里基本上省略了它。
- en: 'If you need to keep full information for a large `ZSET`, but you only really
    perform queries against the top- or bottom-scoring X, you can shard your `ZSET`
    in the same way we shard `HASH`es in [section 9.2.1](#ch09lev2sec4): keeping auxiliary
    top/bottom scoring `ZSET`s, which you can update with `ZADD`/`ZREMRANGEBYRANK`
    to keep limited (as we’ve done previously in [chapters 2](kindle_split_012.html#ch02)
    and [4](kindle_split_015.html#ch04)–[8](kindle_split_019.html#ch08)).'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要为大型`ZSET`保留完整信息，但你实际上只对最高或最低得分的X进行查询，你可以像我们在[9.2.1节](#ch09lev2sec4)中分片`HASH`一样分片你的`ZSET`：保留辅助的最高/最低得分`ZSET`s，你可以使用`ZADD`/`ZREMRANGEBYRANK`来更新它们，以保持有限（就像我们在[第2章](kindle_split_012.html#ch02)、[第4章](kindle_split_015.html#ch04)至[第8章](kindle_split_019.html#ch08)中之前所做的那样）。
- en: You could also use sharded `ZSET`s as a way of reducing single-command latencies
    if you have large search indexes, though discovering the final highestand lowest-scoring
    items would take a potentially long series of `ZUNIONSTORE`/`ZREMRANGEBYRANK`
    pairs.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有大型搜索索引，你也可以使用分片`ZSET`来减少单命令延迟，尽管发现最终得分最高和最低的项目可能需要一系列潜在的`ZUNIONSTORE`/`ZREMRANGEBYRANK`对。
- en: '|  |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When sharding structures, we can make a decision to either support all of the
    functionality of a single structure or only a subset of the standard functionality.
    For the sake of simplicity, when we shard structures in this book, we’ll only
    implement a subset of the functionality offered by the standard structures, because
    to implement the full functionality can be overwhelming (from both computational
    and code-volume perspectives). Even though we only implement a subset of the functionality,
    we’ll use these sharded structures to offer memory reductions to existing problems,
    or to solve new problems more efficiently than would otherwise be possible.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 当分片结构时，我们可以决定支持单个结构的全部功能或仅支持标准功能的子集。为了简化起见，当我们在本书中分片结构时，我们只会实现标准结构提供的功能子集，因为实现完整功能可能会令人不知所措（从计算和代码量两个方面来看）。尽管我们只实现了功能子集，但我们将使用这些分片结构来为现有问题提供内存减少，或者以比其他方式更有效地解决新问题。
- en: The first structure we’ll talk about sharding is the `HASH`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将讨论的第一个可分片结构是`HASH`。
- en: 9.2.1\. HASHes
  id: totrans-73
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.1\. HASHes
- en: One of the primary uses for `HASH`es is storing simple key/value pairs in a
    grouped fashion. Back in [section 5.3](kindle_split_016.html#ch05lev1sec3), we
    developed a method of mapping IP addresses to locations around the world. In addition
    to a `ZSET` that mapped from IP addresses to city IDs, we used a single `HASH`
    that mapped from city IDs to information about the city itself. That `HASH` had
    more than 370,000 entries using the August 2012 version of the database, which
    we’ll now shard.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '`HASH`的主要用途之一是以分组的方式存储简单的键/值对。在[第5.3节](kindle_split_016.html#ch05lev1sec3)中，我们开发了一种将IP地址映射到世界各地位置的方法。除了将IP地址映射到城市ID的`ZSET`之外，我们还使用了一个将城市ID映射到城市本身信息的单个`HASH`。该`HASH`在2012年8月版本的数据库中拥有超过370,000个条目，我们现在将对其进行分片。'
- en: To shard a `HASH` table, we need to choose a method of partitioning our data.
    Because `HASH`es themselves have keys, we can use those keys as a source of information
    to partition the keys. For partitioning our keys, we’ll generally calculate a
    hash function on the key itself that will produce a number. Depending on how many
    keys we want to fit in a single shard and how many total keys we need to store,
    we’ll calculate the number of shards we need, and use that along with our hash
    value to determine the shard ID that the data will be stored in.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 要分片`HASH`表，我们需要选择一种数据分区方法。因为`HASH`本身有键，我们可以使用这些键作为信息源来分区键。为了分区我们的键，我们通常会在键本身上计算一个哈希函数，这将产生一个数字。根据我们想要在一个分片中放入多少键以及我们需要存储的总键数，我们将计算所需的分片数，并使用该数字以及我们的哈希值来确定数据将存储在哪个分片ID中。
- en: For numeric keys, we’ll assume that the keys will be more or less sequential
    and tightly packed, and will assign them to a shard ID based on their numeric
    key value (keeping numerically similar keys in the same shard). The next listing
    shows our function for calculating a new key for a sharded `HASH`, given the base
    key and the `HASH` key `HASH`.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 对于数值键，我们假设键将大致上是顺序的并且紧密排列，并将它们分配给基于它们的数值键值的分片ID（将数值相似的键保持在同一个分片中）。下一个列表显示了我们的函数，用于计算分片`HASH`的新键，给定基本键和`HASH`键`HASH`。
- en: Listing 9.7\. A function to calculate a shard key from a base key and a secondary
    entry key
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.7\. 从基本键和次要条目键计算分片键的函数
- en: '![](217fig01_alt.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![图片](217fig01_alt.jpg)'
- en: In our function, you’ll notice that for non-numeric keys we calculate a CRC32
    checksum. We’re using CRC32 in this case because it returns a simple integer without
    additional work, is fast to calculate (much faster than MD5 or SHA1 hashes), and
    because it’ll work well enough for most situations.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的函数中，你会注意到对于非数值键，我们计算 CRC32 校验和。在这种情况下，我们使用 CRC32 是因为它返回一个简单的整数，无需额外工作，计算速度快（比
    MD5 或 SHA1 哈希快得多），并且因为它在大多数情况下都足够好。
- en: '|  |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Being consistent about `total_elements` and `shard_size`
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 保持 `total_elements` 和 `shard_size` 的一致性
- en: When using non-numeric keys to shard on, you’ll notice that we use the `total_elements`
    value to calculate the total number of shards necessary, in addition to the `shard_size`
    that’s used for both numeric and non-numeric keys. These two pieces of information
    are necessary to keep the total number of shards down. If you were to change either
    of these numbers, then the number of shards (and thus the shard that any piece
    of data goes to) will change. Whenever possible, you shouldn’t change either of
    these values, or when you do change them, you should have a process for moving
    your data from the old data shards to the new data shards (this is generally known
    as *resharding*).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用非数值键进行分片时，你会注意到我们使用 `total_elements` 值来计算所需的分片总数，除了用于数值和非数值键的 `shard_size`。这两条信息对于保持分片总数是必要的。如果你要更改这两个数字中的任何一个，那么分片数量（以及任何数据片段所属的分片）将发生变化。在可能的情况下，你不应该更改这两个值；或者当你更改它们时，你应该有一个将数据从旧数据分片移动到新数据分片的过程（这通常被称为
    *resharding*）。
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’ll now use our `shard_key()` to pick shards as part of two functions that
    will work like `HSET` and `HGET` on sharded hashes in the following listing.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在将使用我们的 `shard_key()` 来选择分片，作为以下列表中两个将像分片 `HASH` 上的 `HSET` 和 `HGET` 一样工作的函数的一部分。
- en: Listing 9.8\. Sharded `HSET` and `HGET` functions
  id: totrans-85
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.8\. 分片 `HSET` 和 `HGET` 函数
- en: '![](217fig02_alt.jpg)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](217fig02_alt.jpg)'
- en: Nothing too complicated there; we’re finding the proper location for the data
    to be stored or fetched from the `HASH` and either setting or getting the values.
    To update our earlier IP-address-to-city lookup calls, we only need to replace
    one call in each of two functions. The next listing shows just those parts of
    our earlier functions that need to be updated.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有太多复杂的内容；我们正在寻找存储或从 `HASH` 中检索数据的位置，并设置或获取值。为了更新我们之前的 IP 地址到城市查找调用，我们只需要在每个函数中替换一个调用。下一个列表显示了需要更新的我们之前函数的相应部分。
- en: Listing 9.9\. Sharded IP lookup functions
  id: totrans-88
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.9\. 分片 IP 查找函数
- en: '![](218fig01_alt.jpg)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](218fig01_alt.jpg)'
- en: On a 64-bit machine, storing the single `HASH` of all of our cities takes up
    roughly 44 megabytes. But with these few small changes to shard our data, setting
    `hash-max-ziplist-entries` to 1024 and `hash-max-ziplist-value` to 256 (the longest
    city/country name in the list is a bit over 150 characters), the sharded `HASH`es
    together take up roughly 12 megabytes. That’s a 70% reduction in data size, which
    would allow us to store 3.5 times as much data as before. For shorter keys and
    values, you can potentially see even greater percentage savings (overhead is larger
    relative to actual data stored).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在 64 位机器上，存储我们所有城市的单个 `HASH` 大约占用 44 兆字节。但是，通过这些对分片数据的少量更改，将 `hash-max-ziplist-entries`
    设置为 1024，将 `hash-max-ziplist-value` 设置为 256（列表中最长的城市/国家名称略超过 150 个字符），分片 `HASH`
    一起大约占用 12 兆字节。这比数据大小减少了 70%，这将使我们能够存储比之前多 3.5 倍的数据。对于更短的键和值，你可能会看到更大的百分比节省（相对于实际存储的数据开销更大）。
- en: '|  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Storing `STRING`s in `HASH`es
  id: totrans-92
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 `HASH` 中存储 `STRING`s
- en: If you find yourself storing a lot of relatively short strings or numbers as
    plain `STRING` values with consistently named keys like `namespace:id`, you can
    store those values in sharded `HASH`es for significant memory reduction in some
    cases.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你发现自己正在将大量相对较短的字符串或数字作为具有一致命名键（如 `namespace:id`）的普通 `STRING` 值存储，你可以在某些情况下将这些值存储在分片
    `HASH` 中，以显著减少内存使用。
- en: '|  |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Adding other operations**'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：添加其他操作**'
- en: As you saw, getting and setting values in a sharded `HASH` is easy. Can you
    add support for sharded `HDEL`, `HINCRBY`, and `HINCRBYFLOAT` operations?
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，在分片 `HASH` 中获取和设置值是很容易的。你能添加对分片 `HDEL`、`HINCRBY` 和 `HINCRBYFLOAT` 操作的支持吗？
- en: '|  |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We’ve just finished sharding large hashes to reduce their memory use. Next,
    you’ll learn how to shard `SET`s.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成了对大型 `HASH` 的分片以减少其内存使用。接下来，你将学习如何分片 `SET`s。
- en: 9.2.2\. SETs
  id: totrans-100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.2.2\. SETs
- en: One common use of an operation known as *map-reduce* (which I mentioned in [chapters
    1](kindle_split_011.html#ch01) and [6](kindle_split_017.html#ch06)) is calculating
    unique visitors to a website. Rather than waiting until the end of the day to
    perform that calculation, we could instead keep a live updated count of unique
    visitors as the day goes on. One method to calculate unique visitors in Redis
    would use a `SET`, but a single `SET` storing many unique visitors would be very
    large. In this section, we’ll shard `SET`s as a way of building a method to count
    unique visitors to a website.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 一个名为*map-reduce*的操作（我在[第1章](kindle_split_011.html#ch01)和[第6章](kindle_split_017.html#ch06)中提到过）的常见用途是计算网站的唯一访问者数量。而不是等到一天结束时才进行这个计算，我们可以在一天中实时更新唯一访问者的计数。在Redis中计算唯一访问者的一个方法使用`SET`，但存储许多唯一访问者的单个`SET`会非常大。在本节中，我们将通过分片`SET`s来构建一种方法，以计算网站的唯一访问者数量。
- en: 'To start, we’ll assume that every visitor already has a unique identifier similar
    to the UUIDs that we generated in [chapter 2](kindle_split_012.html#ch02) for
    our login session cookies. Though we could use these UUIDs directly in our `SET`
    as members and as keys to shard using our sharding function from [section 9.2.1](#ch09lev2sec4),
    we’d lose the benefit of the intset encoding. Assuming that we generated our UUIDs
    randomly (as we’ve done in previous chapters), we could instead use the first
    15 hexadecimal digits from the UUID as a full key. This should bring up two questions:
    First, why would we want to do this? And second, why is this enough?'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们假设每个访问者都已经有一个唯一的标识符，类似于我们在[第2章](kindle_split_012.html#ch02)中为我们的登录会话cookie生成的UUIDs。尽管我们可以直接将这些UUIDs用作`SET`的成员和作为使用[第9.2.1节](#ch09lev2sec4)中的分片函数进行分片的键，但我们将会失去intset编码的好处。假设我们随机生成了我们的UUIDs（就像我们在前面的章节中做的那样），我们可以使用UUID的前15个十六进制数字作为完整的键。这会引发两个问题：首先，我们为什么要这样做？其次，这为什么足够？
- en: For the first question (why we’d want to do this), UUIDs are basically 128-bit
    numbers that have been formatted in an easy-to-read way. If we were to store them,
    we’d be storing roughly 16 bytes (or 36 if we stored them as-is) per unique visitor.
    But by only storing the first 15 hexadecimal digits^([[3](#ch09fn03)]) turned
    into a number, we’d only be storing 8 bytes per unique visitor. So we save space
    up front, which we may be able to use later for other problems. This also lets
    us use the intset optimization for keeping memory use down.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第一个问题（我们为什么要这样做），UUIDs基本上是128位的数字，它们已经被格式化为易于阅读的形式。如果我们存储它们，我们将为每个唯一的访问者存储大约16字节（如果我们直接存储，则为36字节）。但是，通过只存储转换为数字的前15个十六进制数字^([[3](#ch09fn03)))，我们只为每个唯一的访问者存储8字节。因此，我们在一开始就节省了空间，这我们可能以后可以用在其他问题上。这也让我们可以使用intset优化来降低内存使用。
- en: ³ Another good question is why 56 and not 64 bits? That’s because Redis will
    only use intsets for up to 64-bit signed integers, and the extra work of turning
    our 64-bit unsigned integer into a signed integer isn’t worth it in most situations.
    If you need the extra precision, check out the Python `struct` module and look
    at the `Q` and `q` format codes.
  id: totrans-104
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 另一个很好的问题是为什么是56位而不是64位？这是因为Redis只会使用intsets来处理最多64位的有符号整数，将我们的64位无符号整数转换为有符号整数在大多数情况下并不值得。如果你需要额外的精度，可以查看Python的`struct`模块，并查看`Q`和`q`格式代码。
- en: 'For the second question (why this is enough), it boils down to what are called
    *birthday collisions*. Put simply: What are the chances of two 128-bit random
    identifiers matching in the first 56 bits? Mathematically, we can calculate the
    chances exactly, and as long as we have fewer than 250 million unique visitors
    in a given time period (a day in our case), we’ll have at most a 1% chance of
    a single match (so if every day we have 250 million visitors, about once every
    100 days we’ll have about 1 person not counted). If we have fewer than 25 million
    unique visitors, then the chance of not counting a user falls to the point where
    we’d need to run the site for roughly 2,739 years before we’d miss counting a
    single user.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 对于第二个问题（这为什么足够），它归结为所谓的*生日碰撞*。简单来说：两个128位随机标识符在前56位匹配的概率是多少？从数学上讲，我们可以精确地计算出这个概率，只要在给定的时间段（在我们的例子中是一天）内，我们有少于2.5亿的唯一访问者，我们最多有1%的匹配概率（所以如果我们每天有2.5亿访问者，大约每100天会有大约1个人未被计算）。如果我们有少于2500万唯一访问者，那么不计算用户的概率会降低到我们需要运行网站大约2739年才可能错过计算一个用户。
- en: Now that we’ve decided to use the first 56 bits from the UUID, we’ll build a
    sharded `SADD` function, which we’ll use as part of a larger bit of code to actually
    count unique visitors. This sharded `SADD` function in [listing 9.10](#ch09ex10)
    will use the same shard key calculation that we used in [section 9.2.1](#ch09lev2sec4),
    modified to prefix our numeric ID with a non-numeric character for shard ID calculation,
    since our 56-bit IDs aren’t densely packed (as is the assumption for numeric IDs).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经决定使用 UUID 的前 56 位，我们将构建一个分区 `SADD` 函数，我们将将其用作更大块代码的一部分，以实际计数独立访客。在 [列表
    9.10](#ch09ex10) 中的这个分区 `SADD` 函数将使用我们在 [第 9.2.1 节](#ch09lev2sec4) 中使用的相同的分区键计算方法，修改为在数字
    ID 前添加一个非数字字符以进行分区 ID 计算，因为我们的 56 位 ID 并不是紧密排列的（这是对数字 ID 的假设）。
- en: Listing 9.10\. A sharded `SADD` function we’ll use as part of a unique visitor
    counter
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.10\. 我们将用作独立访客计数器一部分的分区 `SADD` 函数
- en: '![](220fig01_alt.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![220fig01_alt.jpg](220fig01_alt.jpg)'
- en: With a sharded `SADD` function, we can now keep unique visitor counts. When
    we want to count a visitor, we’ll first calculate their shorter ID based on the
    first 56 bits of their session UUID. We’ll then determine today’s date and add
    the ID to the sharded unique visitor `SET` for today. If the ID wasn’t already
    in the `SET`, we’ll increment today’s unique visitor count. Our code for keeping
    track of the unique visitor count can be seen in the following listing.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 使用分区 `SADD` 函数后，我们现在可以保持独立访客计数。当我们想要计数一个访客时，我们首先根据会话 UUID 的前 56 位计算他们的较短 ID。然后我们确定今天的日期并将
    ID 添加到今天的分区独立访客 `SET` 中。如果该 ID 之前不在 `SET` 中，我们将增加今天的独立访客计数。我们跟踪独立访客计数的代码将在下面的列表中展示。
- en: Listing 9.11\. A function to keep track of the unique visitor count on a daily
    basis
  id: totrans-110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.11\. 一个用于按日跟踪独立访客数量的函数
- en: '![](220fig02_alt.jpg)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![220fig02_alt.jpg](220fig02_alt.jpg)'
- en: That function works exactly as described, though you’ll notice that we make
    a call to `get_expected()` to determine the number of expected daily visitors.
    We do this because web page visits will tend to change over time, and keeping
    the same number of shards every day wouldn’t grow as we grow (or shrink if we
    have significantly fewer than a million unique visitors daily).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的工作方式与描述的完全一致，尽管你会注意到我们调用了 `get_expected()` 来确定预期的每日访客数量。我们这样做是因为网页访问量可能会随时间变化，每天保持相同数量的分片不会随着我们的增长（或如果我们每天有显著少于一百万独立访客，则会缩小）而增长。
- en: To address the daily change in expected viewers, we’ll write a function that
    calculates a new expected number of unique visitors for each day, based on yesterday’s
    count. We’ll calculate this once for any given day, estimating that today will
    see at least 50% more visitors than yesterday, rounded up to the next power of
    2\. Our code for calculating this can be seen next.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了应对预期观众数量的每日变化，我们将编写一个函数，根据昨日的数量计算每一天的新预期独立访客数量。对于任何给定的一天，我们预计今天的访客数量至少比昨天多
    50%，并向上取到下一个 2 的幂。我们计算这一值的代码将在下面展示。
- en: Listing 9.12\. Calculate today’s expected unique visitor count based on yesterday’s
    count
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.12\. 基于昨日的数量计算今天的预期独立访客数量
- en: '![](220fig03_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![220fig03_alt.jpg](220fig03_alt.jpg)'
- en: Most of that function is reading and massaging data in one way or another, but
    the overall result is that we calculate an expected number of unique views for
    today by taking yesterday’s view count, increasing it by 50%, and rounding up
    to the next power of 2\. If the expected number of views for today has already
    been calculated, we’ll use that.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 该函数的大部分操作是读取和以某种方式处理数据，但总体结果是，我们通过将昨日的观看次数增加 50%，并向上取到下一个 2 的幂，来计算今天的预期独立观看次数。如果今天预期观看次数已经计算过，我们将使用那个值。
- en: Taking this exact code and adding 1 million unique visitors, Redis will use
    approximately 9.5 megabytes to store the unique visitor count. Without sharding,
    Redis would use 56 megabytes to store the same data (56-bit integer IDs in a single
    `SET`). That’s an 83% reduction in storage with sharding, which would let us store
    5.75 times as much data with the same hardware.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 将这段代码与 100 万独立访客一起使用，Redis 将使用大约 9.5 兆字节来存储独立访客计数。如果没有分区，Redis 将使用 56 兆字节来存储相同的数据（单个
    `SET` 中的 56 位整数 ID）。这通过分区减少了 83% 的存储空间，这将使我们能够在相同的硬件上存储 5.75 倍的数据。
- en: '|  |'
  id: totrans-118
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Filling out the sharded `SET` API**'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：填写分区 `SET` API**'
- en: 'For this example, we only needed a single `SET` command to determine the unique
    visitor count for a given day. Can you add sharded `SREM` and `SISMEMBER` calls?
    Bonus points: Assuming that you have two sharded `SET`s with the same expected
    total number of items, as well as the same shard size, you’ll have the same number
    of shards, and identical IDs will be in the same shard IDs. Can you add sharded
    versions of `SINTERSTORE`, `SUNIONSTORE`, and `SDIFFSTORE`?'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个示例，我们只需要一个`SET`命令就可以确定给定日期的唯一访客数。你能添加分片`SREM`和`SISMEMBER`调用吗？加分项：假设你有两个具有相同预期项目总数和相同分片大小的分片`SET`s，你将具有相同数量的分片，并且相同的ID将在相同的分片ID中。你能添加`SINTERSTORE`、`SUNIONSTORE`和`SDIFFSTORE`的分片版本吗？
- en: '|  |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Other methods to calculate unique visitor counts
  id: totrans-123
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算唯一访客数的其他方法
- en: If you have numeric visitor IDs (instead of UUIDs), and the visitor IDs have
    relatively low maximum value, rather than storing your visitor information as
    sharded `SET`s, you can store them as bitmaps using techniques similar to what
    we describe in the next section. A Python library for calculating unique visitor
    counts and other interesting analytics based on bitmaps can be found at [https://github.com/Doist/bitmapist](https://github.com/Doist/bitmapist).
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你拥有数字访客ID（而不是UUIDs），并且访客ID的最大值相对较低，那么你不必将访客信息存储为分片`SET`s，可以使用类似我们在下一节中描述的技术将它们存储为位图。一个用于计算基于位图的唯一访客数和其他有趣分析的Python库可以在[https://github.com/Doist/bitmapist](https://github.com/Doist/bitmapist)找到。
- en: '|  |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: After sharding large `SET`s of integers to reduce storage, it’s now time to
    learn how to pack bits and bytes into `STRING`s.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 在将大量整数`SET`s分片以减少存储后，现在是时候学习如何将位和字节打包到`STRING`s中。
- en: 9.3\. Packing bits and bytes
  id: totrans-127
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3. 打包位和字节
- en: When we discussed sharding `HASH`es, I briefly mentioned that if we’re storing
    short strings or counters as `STRING` values with keys like `namespace:id`, we
    could use sharded `HASH`es as a way of reducing memory use. But let’s say that
    we wanted to store a short fixed amount of information for sequential IDs. Can
    we use even less memory than sharded `HASH`es?
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论分片`HASH`时，我简要提到，如果我们将短字符串或计数器作为`STRING`值存储，键如`namespace:id`，我们可以使用分片`HASH`作为减少内存使用的一种方式。但假设我们想要为顺序ID存储一个短固定量的信息。我们能否比分片`HASH`使用更少的内存？
- en: In this section, we’ll use sharded Redis `STRING`s to store location information
    for large numbers of users with sequential IDs, and discuss how to perform aggregate
    counts over this stored data. This example shows how we can use sharded Redis
    `STRING`s to store, for example, location information for users on Twitter.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将使用分片Redis `STRING`s来存储具有顺序ID的大量用户的位置信息，并讨论如何对存储的数据执行聚合计数。此示例展示了我们如何使用分片Redis
    `STRING`s来存储，例如，Twitter上用户的位置信息。
- en: 'Before we start storing our data, we need to revisit four commands that’ll
    let us efficiently pack and update `STRING`s in Redis: `GETRANGE`, `SETRANGE`,
    `GETBIT`, and `SETBIT`. The `GETRANGE` command lets us read a substring from a
    stored `STRING`. `SETRANGE` will let us set the data stored at a substring of
    the larger `STRING`. Similarly, `GETBIT` will fetch the value of a single bit
    in a `STRING`, and `SETBIT` will set an individual bit. With these four commands,
    we can use Redis `STRING`s to store counters, fixed-length strings, Booleans,
    and more in as compact a format as is possible without compression. With our brief
    review out of the way, let’s talk about what information we’ll store.'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始存储数据之前，我们需要回顾四个命令，这些命令将使我们能够高效地打包和更新Redis中的`STRING`s：`GETRANGE`、`SETRANGE`、`GETBIT`和`SETBIT`。`GETRANGE`命令允许我们从存储的`STRING`中读取子字符串。`SETRANGE`将允许我们在较大的`STRING`的子字符串中设置数据。同样，`GETBIT`将获取`STRING`中单个位的值，而`SETBIT`将设置单个位。有了这四个命令，我们可以使用Redis
    `STRING`s以尽可能紧凑的格式存储计数器、固定长度字符串、布尔值等，而不需要压缩。简要回顾完毕后，让我们谈谈我们将存储哪些信息。
- en: 9.3.1\. What location information should we store?
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.1. 我们应该存储哪些位置信息？
- en: When I mentioned locations, you were probably wondering what I meant. Well,
    we could store a variety of different types of locations. With 1 byte, we could
    store country-level information for the world. With 2 bytes, we could store region/state-level
    information. With 3 bytes, we could store regional postal codes for almost every
    country. And with 4 bytes, we could store latitude/longitude information down
    to within about 2 meters or 6 feet.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 当我提到位置时，你可能想知道我指的是什么。好吧，我们可以存储各种不同类型的位置。用 1 个字节，我们可以存储全球的国家级信息。用 2 个字节，我们可以存储地区/州级信息。用
    3 个字节，我们可以存储几乎所有国家的地区邮政编码。而用 4 个字节，我们可以存储纬度/经度信息，精确到大约 2 米或 6 英尺。
- en: Which level of precision to use will depend on our given use case. For the sake
    of simplicity, we’ll start with just 2 bytes for region/state-level information
    for countries around the world. As a starter, [listing 9.13](#ch09ex13) shows
    some base data for ISO3 country codes around the world, as well as state/province
    information for the United States and Canada.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 应该使用哪种精度的级别将取决于我们的特定用例。为了简化，我们将从全球各国地区/州级信息的 2 个字节开始。作为一个起点，[列表 9.13](#ch09ex13)
    展示了全球 ISO3 国家代码的一些基本数据，以及美国和加拿大的州/省信息。
- en: Listing 9.13\. Base location tables we can expand as necessary
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.13\. 我们可以根据需要扩展的基本位置表
- en: '![](222fig01_alt.jpg)'
  id: totrans-135
  prefs: []
  type: TYPE_IMG
  zh: '![222fig01_alt.jpg](222fig01_alt.jpg)'
- en: I introduce these tables of data initially so that if/when we’d like to add
    additional state, region, territory, or province information for countries we’re
    interested in, the format and method for doing so should be obvious. Looking at
    the data tables, we initially define them as strings. But these strings are converted
    into lists by being split on whitespace by our call to the `split()` method on
    strings without any arguments. Now that we have some initial data, how are we
    going to store this information on a per-user basis?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 我最初引入这些数据表是为了如果/当我们要为我们感兴趣的国家的州、地区、领地或省添加额外的信息时，进行格式和方法应该是显而易见的。查看数据表，我们最初将它们定义为字符串。但通过在没有任何参数的情况下调用字符串的
    `split()` 方法，这些字符串被分割成列表。现在我们有一些初始数据，我们将如何按用户存储这些信息？
- en: Let’s say that we’ve determined that user 139960061 lives in California, U.S.,
    and we want to store this information for that user. To store the information,
    we first need to pack the data into 2 bytes by first discovering the code for
    the United States, which we can calculate by finding the index of the United States’
    ISO3 country code in our `COUNTRIES` list. Similarly, if we have state information
    for a user, and we also have state information in our tables, we can calculate
    the code for the state by finding its index in the table. The next listing shows
    the function for turning country/state information into a 2-byte code.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经确定用户 139960061 居住在美国加利福尼亚州，并且我们想要为该用户存储此信息。为了存储信息，我们首先需要将数据打包成 2 个字节，这需要首先发现美国的代码，我们可以通过在我们的
    `COUNTRIES` 列表中找到美国 ISO3 国家代码的索引来计算它。同样，如果我们有一个用户的州信息，并且我们也在我们的表中有了州信息，我们可以通过在表中找到它的索引来计算该州的代码。下一个列表显示了将国家/州信息转换为
    2 个字节代码的函数。
- en: Listing 9.14\. ISO3 country codes
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.14\. ISO3 国家代码
- en: '![](223fig01_alt.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![223fig01_alt.jpg](223fig01_alt.jpg)'
- en: Location code calculation isn’t that interesting or difficult; it’s primarily
    a matter of finding offsets in lookup tables, and then dealing with “not found”
    data. Next, let’s talk about actually storing our packed location data.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 位置代码计算并不那么有趣或困难；这主要是找到查找表中的偏移量，然后处理“未找到”的数据。接下来，让我们谈谈实际存储我们的打包位置数据。
- en: 9.3.2\. Storing packed data
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.2\. 存储打包数据
- en: After we have our packed location codes, we only need to store them in `STRING`s
    with `SETRANGE`. But before we do so, we have to think for a moment about how
    many users we’re going to be storing information about. For example, suppose that
    Twitter has 750 million users today (based on the observation that recently created
    users have IDs greater than 750 million); we’d need over 1.5 gigabytes of space
    to store location information for all Twitter users. Though most operating systems
    are able to reasonably allocate large regions of memory, Redis limits us to 512
    megabyte `STRING`s, and due to Redis’s clearing out of data when setting a value
    beyond the end of an existing `STRING`, setting the first value at the end of
    a long `STRING` will take more time than would be expected for a simple `SETBIT`
    call. Instead, we can use a technique similar to what we used in [section 9.2.1](#ch09lev2sec4),
    and shard our data across a collection of `STRING`s.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们有了打包的位置代码后，我们只需要使用 `SETRANGE` 将它们存储在 `STRING`s 中。但在这样做之前，我们必须稍微思考一下我们将要存储多少用户的信息。例如，假设
    Twitter 今天有 7.5 亿用户（基于观察，最近创建的用户 ID 大于 7.5 亿）；我们需要超过 1.5 GB 的空间来存储所有 Twitter 用户的地理位置信息。尽管大多数操作系统都能够合理地分配大块内存，但
    Redis 限制我们只能使用 512 MB 的 `STRING`s，而且由于 Redis 在设置超出现有 `STRING` 结尾的值时会清除数据，因此在长
    `STRING` 的末尾设置第一个值将比预期的简单 `SETBIT` 调用花费更多时间。相反，我们可以使用与我们在 [第 9.2.1 节](#ch09lev2sec4)
    中使用的技术类似的方法，并将我们的数据分片存储在一系列 `STRING`s 中。
- en: Unlike when we were sharding `HASH`es and `SET`s, we don’t have to worry about
    being efficient by keeping our shards smaller than a few thousand elements, since
    we can access an element directly without decoding any others. Similarly, we can
    write to a given offset efficiently. Instead, our concern is more along the lines
    of being efficient at a larger scale—specifically what will balance potential
    memory fragmentation, as well as minimize the number of keys that are necessary.
    For this example, we’ll store location information for 2^(20) users (just over
    1 million entries) per `STRING`, which will use about 2 megabytes per `STRING`.
    In the next listing, we see the code for updating location information for a user.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 与我们分片 `HASH` 和 `SET` 不同，我们不需要担心通过保持我们的分片小于几千个元素来提高效率，因为我们可以直接访问一个元素而无需解码其他任何元素。同样，我们可以高效地向给定的偏移量写入。相反，我们的关注点更多地在于在大规模上提高效率——具体来说，是什么将潜在的内存碎片化与最小化所需键的数量平衡。在这个例子中，我们将为每个
    `STRING` 存储关于 2^(20) 个用户（略超过 100 万条记录）的地理位置信息，这将占用每个 `STRING` 大约 2 MB 的空间。在下一个列表中，我们可以看到更新用户地理位置信息的代码。
- en: Listing 9.15\. A function for storing location data in sharded `STRING`s
  id: totrans-144
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.15\. 在分片 `STRING`s 中存储位置数据的函数
- en: '![](224fig01_alt.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](224fig01_alt.jpg)'
- en: For the most part, there shouldn’t be anything surprising there. We calculate
    the location code to store for a user, calculate the shard and the individual
    shard offset for the user, and then store the location code in the proper location
    for the user. The only thing that’s strange and may not seem necessary is that
    we also update a `ZSET` that stores the highest-numbered user ID that has been
    seen. This is primarily important when calculating aggregates over everyone we
    have information about (so we know when to stop).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在大多数情况下，这里不应该有任何令人惊讶的内容。我们计算存储用户的位置代码，计算用户的分片和分片偏移量，然后将位置代码存储在用户正确的位置。唯一奇怪且可能看起来不必要的事情是我们还更新了一个
    `ZSET`，该 `ZSET` 存储已看到的最高编号用户 ID。这在计算关于我们拥有的所有信息的聚合时非常重要（这样我们知道何时停止）。
- en: 9.3.3\. Calculating aggregates over sharded STRINGs
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.3.3\. 在分片 STRING 上计算聚合
- en: To calculate aggregates, we have two use cases. Either we’ll calculate aggregates
    over all of the information we know about, or we’ll calculate over a subset. We’ll
    start by calculating aggregates over the entire population, and then we’ll write
    code that calculates aggregates over a smaller group.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 为了计算聚合，我们有两种用例。要么我们将计算关于我们所知道的所有信息的聚合，要么我们将计算一个子集的聚合。我们将从计算整个群体的聚合开始，然后我们将编写计算较小群体聚合的代码。
- en: To calculate aggregates over everyone we have information for, we’ll recycle
    some code that we wrote back in [section 6.6.4](kindle_split_017.html#ch06lev2sec19),
    specifically the `readblocks()` function, which reads blocks of data from a given
    key. Using this function, we can perform a single command and round trip with
    Redis to fetch information about thousands of users at one time. Our function
    to calculate aggregates with this block-reading function is shown next.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 要计算我们拥有的所有信息的聚合，我们将回收我们在[第6.6.4节](kindle_split_017.html#ch06lev2sec19)中编写的部分代码，特别是`readblocks()`函数，该函数从给定键读取数据块。使用此函数，我们可以执行单个命令并与Redis进行往返，一次获取成千上万用户的有关信息。使用此块读取函数计算聚合的函数如下所示。
- en: Listing 9.16\. A function to aggregate location information for everyone
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.16\. 一个用于聚合每个人位置信息的函数
- en: '![](225fig01_alt.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![225fig01_alt.jpg](225fig01_alt.jpg)'
- en: This function to calculate aggregates over country- and state-level information
    for everyone uses a structure called a `defaultdict`, which we also first used
    in [chapter 6](kindle_split_017.html#ch06) to calculate aggregates about location
    information before writing back to Redis. Inside this function, we refer to a
    helper function that actually updates the aggregates and decodes location codes
    back into their original ISO3 country codes and local state abbreviations, which
    can be seen in this next listing.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这个用于计算每个人国家/州级别信息的聚合函数使用了一个名为`defaultdict`的结构，我们也在[第6章](kindle_split_017.html#ch06)中首次使用它来计算在将信息写回Redis之前关于位置信息的聚合。在这个函数内部，我们引用了一个辅助函数，该函数实际上更新了聚合并将位置代码解码回原始的ISO3国家代码和本地州缩写，这些可以在下面的列表中看到。
- en: Listing 9.17\. Convert location codes back to country/state information
  id: totrans-153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.17\. 将位置代码转换回国家/州信息
- en: '![](225fig02_alt.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![225fig02_alt.jpg](225fig02_alt.jpg)'
- en: With a function to convert location codes back into useful location information
    and update aggregate information, we have the building blocks to perform aggregates
    over a subset of users. As an example, say that we have location information for
    many Twitter users. And also say that we have follower information for each user.
    To discover information about where the followers of a given user are located,
    we’d only need to fetch location information for those users and compute aggregates
    similar to our global aggregates. The next listing shows a function that will
    aggregate location information over a provided list of user IDs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 通过一个将位置代码转换回有用位置信息并更新聚合信息的函数，我们拥有了在用户子集上执行聚合的构建块。例如，假设我们有许多Twitter用户的地理位置信息。还假设我们每个用户都有关注者信息。要发现特定用户关注者的地理位置信息，我们只需获取这些用户的地理位置信息并计算类似于我们的全局聚合的聚合。下面的列表显示了一个将聚合给定用户ID列表上的位置信息的函数。
- en: Listing 9.18\. A function to aggregate location information over provided user
    IDs
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.18\. 一个用于聚合给定用户ID位置信息的函数
- en: '![](226fig01_alt.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![226fig01_alt.jpg](226fig01_alt.jpg)'
- en: This technique of storing fixed-length data in sharded `STRING`s can be useful.
    Though we stored multiple bytes of data per user, we can use `GETBIT` and `SETBIT`
    identically to store individual bits, or even groups of bits.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这种在分片`STRING`s中存储固定长度数据的技术可能很有用。尽管我们为每个用户存储了多个字节数据，但我们可以使用`GETBIT`和`SETBIT`相同地存储单个位，甚至位组。
- en: 9.4\. Summary
  id: totrans-159
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4\. 摘要
- en: In this chapter, we’ve explored a number of ways to reduce memory use in Redis
    using short data structures, sharding large structures to make them small again,
    and by packing data directly into `STRING`s.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了多种方法来减少Redis的内存使用，包括使用短数据结构、将大型结构分片以使其再次变小，以及直接将数据打包到`STRING`s中。
- en: If there’s one thing that you should take away from this chapter, it’s that
    by being careful about how you store your data, you can significantly reduce the
    amount of memory that Redis needs to support your applications.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这个章节中带走了一件事，那就是通过仔细考虑如何存储你的数据，你可以显著减少Redis支持你的应用程序所需的内存量。
- en: In the next chapter, we’ll revisit a variety of topics to help Redis scale to
    larger groups of machines, including read slaves, sharding data across multiple
    masters, and techniques for scaling a variety of different types of queries.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将重新探讨各种主题，以帮助Redis扩展到更大的机器组，包括读取从属服务器、跨多个主服务器分片数据以及扩展各种不同类型查询的技术。
- en: Chapter 10\. Scaling Redis
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第10章\. 缩放Redis
- en: '*This chapter covers*'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Scaling reads
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展读取
- en: Scaling writes and memory capacity
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展写入和内存容量
- en: Scaling complex queries
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展复杂查询
- en: As your use of Redis grows, there may come a time when you’re no longer able
    to fit all of your data into a single Redis server, or when you need to perform
    more reads and/or writes than Redis can sustain. When this happens, you have a
    few options to help you scale Redis to your needs.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 随着你对Redis的使用增长，可能会有这样的时候，你不再能够将所有数据放入单个Redis服务器中，或者当你需要执行比Redis能够承受的更多读和/或写操作时。当这种情况发生时，你有几个选项可以帮助你将Redis扩展到你的需求。
- en: In this chapter, we’ll cover techniques to help you to scale your read queries,
    write queries, total memory available, and techniques for scaling a selection
    of more complicated queries.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍帮助您扩展读查询、写查询、可用总内存以及扩展更复杂查询选择的技术。
- en: Our first task is addressing those problems where we can store all of the data
    we need, and we can handle writes without issue, but where we need to perform
    more read queries in a second than a single Redis server can handle.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是解决那些我们可以存储所有所需数据，并且可以无问题处理写操作，但需要在一个Redis服务器上执行比单个Redis服务器能处理更多的读查询的问题。
- en: 10.1\. Scaling reads
  id: totrans-171
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.1. 扩展读操作
- en: In [chapter 8](kindle_split_019.html#ch08) we built a social network that offered
    many of the same features and functionalities of Twitter. One of these features
    was the ability for users to view their *home timeline* as well as their *profile
    timeline*. When viewing these timelines, we’ll be fetching 30 posts at a time.
    For a small social network, this wouldn’t be a serious issue, since we could still
    support anywhere from 3,000–10,000 users fetching timelines every second (if that
    was all that Redis was doing). But for a larger social network, it wouldn’t be
    unexpected to need to serve many times that number of timeline fetches every second.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](kindle_split_019.html#ch08)中，我们构建了一个提供许多与Twitter相同功能和特性的社交网络。其中一项功能是用户可以查看他们的*主页时间线*以及他们的*个人资料时间线*。在查看这些时间线时，我们将一次获取30个帖子。对于一个小型社交网络，这不会是一个严重的问题，因为我们仍然可以支持每秒从3,000到10,000个用户获取时间线（如果Redis只是做这件事的话）。但对于一个大型社交网络，每秒需要处理的时间线获取次数可能是这个数字的许多倍，这并不令人意外。
- en: In this section, we’ll discuss the use of read slaves to support scaling read
    queries beyond what a single Redis server can handle.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论使用读从属服务器来支持扩展读查询，使其超出单个Redis服务器处理能力的做法。
- en: 'Before we get into scaling reads, let’s first review a few opportunities for
    improving performance before we must resort to using additional servers with slaves
    to scale our queries:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们讨论扩展读操作之前，让我们首先回顾一下在必须求助于使用从属服务器和额外服务器来扩展我们的查询之前，可以改进性能的一些机会：
- en: If we’re using small structures (as we discussed in [chapter 9](kindle_split_021.html#ch09)),
    first make sure that our max ziplist size isn’t too large to cause performance
    penalties.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用的是小结构（如我们在[第9章](kindle_split_021.html#ch09)中讨论的），首先确保我们的最大ziplist大小不是太大，以免造成性能惩罚。
- en: Remember to use structures that offer good performance for the types of queries
    we want to perform (don’t treat `LIST`s like `SET`s; don’t fetch an entire `HASH`
    just to sort on the client—use a `ZSET`; and so on).
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住要使用为我们想要执行的查询类型提供良好性能的结构（不要将`LIST`当作`SET`来处理；不要为了在客户端排序而获取整个`HASH`，使用`ZSET`；等等）。
- en: If we’re sending large objects to Redis for caching, consider compressing the
    data to reduce network bandwidth for reads and writes (compare lz4, gzip, and
    bzip2 to determine which offers the best trade-offs for size/performance for our
    uses).
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们要将大对象发送到Redis进行缓存，考虑压缩数据以减少读写时的网络带宽（比较lz4、gzip和bzip2，以确定哪种为我们使用提供了最佳的大小/性能权衡）。
- en: Remember to use pipelining (with or without transactions, depending on our requirements)
    and connection pooling, as we discussed in [chapter 4](kindle_split_015.html#ch04).
  id: totrans-178
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 记住要使用管道（带或不带事务，取决于我们的需求）和连接池，正如我们在[第4章](kindle_split_015.html#ch04)中讨论的那样。
- en: When we’re doing everything that we can to ensure that reads and writes are
    fast, it’s time to address the need to perform more read queries. The simplest
    method to increase total read throughput available to Redis is to add read-only
    slave servers. If you remember from [chapter 4](kindle_split_015.html#ch04), we
    can run additional servers that connect to a master, receive a replica of the
    master’s data, and be kept up to date in real time (more or less, depending on
    network bandwidth). By running our read queries against one of several slaves,
    we can gain additional read query capacity with every new slave.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们正在尽一切努力确保读写速度快时，是时候解决执行更多读查询的需求了。增加 Redis 可用总读吞吐量的最简单方法就是添加只读从服务器。如果你还记得
    [第4章](kindle_split_015.html#ch04)，我们可以运行额外的服务器，这些服务器连接到主服务器，接收主服务器数据的副本，并实时保持更新（或多或少取决于网络带宽）。通过在我们的读查询中使用多个从服务器之一，我们可以通过每个新的从服务器获得额外的读查询能力。
- en: '|  |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Remember: Write to the master'
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 记住：向大师学习
- en: When using read slaves, and generally when using slaves at all, you must remember
    to write to the master Redis server only. By default, attempting to write to a
    Redis server configured as a slave (even if it’s also a master) will cause that
    server to reply with an error. We’ll talk about using a configuration option to
    allow for writes to slave servers in [section 10.3.1](#ch10lev2sec3), but generally
    you should run with slave writes disabled; writing to slaves is usually an error.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用读从服务器，以及通常使用任何从服务器时，你必须记住只向主 Redis 服务器写入。默认情况下，尝试向配置为从服务器的 Redis 服务器（即使它也是主服务器）写入将导致该服务器返回错误。我们将在
    [第10.3.1节](#ch10lev2sec3) 中讨论使用配置选项允许向从服务器写入，但通常你应该禁用从服务器写入；向从服务器写入通常是一个错误。
- en: '|  |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[Chapter 4](kindle_split_015.html#ch04) has all the details on configuring
    Redis for replication to slave servers, how it works, and some ideas for scaling
    to many read slaves. Briefly, we can update the Redis configuration file with
    a line that reads `slaveof host port`, replacing `host` and `port` with the host
    and port of the master server. We can also configure a slave by running the `SLAVEOF
    host port` command against an existing server. Remember: When a slave connects
    to a master, any data that previously existed on the slave will be discarded.
    To disconnect a slave from a master to stop it from slaving, we can run `SLAVEOF
    no one`.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: '[第4章](kindle_split_015.html#ch04) 包含了配置 Redis 以复制到从服务器、其工作原理以及扩展到多个读从服务器的一些想法的所有详细信息。简而言之，我们可以通过添加一行读取
    `slaveof host port` 的命令来更新 Redis 配置文件，用主服务器的地址和端口替换 `host` 和 `port`。我们还可以通过在现有服务器上运行
    `SLAVEOF host port` 命令来配置一个从服务器。记住：当从服务器连接到主服务器时，从服务器上之前存在的任何数据都将被丢弃。要断开从服务器与主服务器的连接以停止其复制，我们可以运行
    `SLAVEOF no one`。'
- en: One of the biggest issues that arises when using multiple Redis slaves to serve
    data is what happens when a master temporarily or permanently goes down. Remember
    that when a slave connects, the Redis master initiates a snapshot. If multiple
    slaves connect before the snapshot completes, they’ll all receive the same snapshot.
    Though this is great from an efficiency perspective (no need to create multiple
    snapshots), sending multiple copies of a snapshot at the same time to multiple
    slaves can use up the majority of the network bandwidth available to the server.
    This could cause high latency to/from the master, and could cause previously connected
    slaves to become disconnected.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用多个 Redis 从服务器来服务数据时，出现的一个最大问题是当主服务器暂时或永久性地宕机时会发生什么。记住，当从服务器连接时，Redis 主服务器会启动一个快照。如果在快照完成之前多个从服务器连接，它们都将接收到相同的快照。虽然这在效率方面很好（无需创建多个快照），但同时在多个从服务器上发送多个快照副本可能会消耗服务器大部分的网络带宽。这可能会导致主服务器的高延迟，并可能导致之前连接的从服务器断开连接。
- en: One method of addressing the slave resync issue is to reduce the total data
    volume that’ll be sent between the master and its slaves. This can be done by
    setting up intermediate replication servers to form a type of tree, as can be
    seen in [figure 10.1](#ch10fig01), which we borrow from [chapter 4](kindle_split_015.html#ch04).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 解决从服务器重新同步问题的方法之一是减少主服务器和其从服务器之间发送的总数据量。这可以通过设置中间复制服务器来形成一个树形结构来实现，如图 10.1 所示，我们借鉴自
    [第4章](kindle_split_015.html#ch04)。
- en: Figure 10.1\. An example Redis master/slave replica tree with nine lowest-level
    slaves and three intermediate replication helper servers
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 10.1\. 一个示例 Redis 主/从副本树，包含九个最低级别的从服务器和三个中间复制辅助服务器
- en: '![](10fig01_alt.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](10fig01_alt.jpg)'
- en: These slave trees will work, and can be necessary if we’re looking to replicate
    to a different data center (resyncing across a slower WAN link will take resources,
    which should be pushed off to an intermediate slave instead of running against
    the root master). But slave trees suffer from having a complex network topology
    that makes manually or automatically handling failover situations difficult.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 这些从属树将会工作，如果我们想要复制到不同的数据中心（在较慢的WAN链路上进行重新同步将消耗资源，这些资源应该推送到一个中间从属而不是运行在根主节点上），那么它们可能是必要的。但是从属树由于具有复杂的网络拓扑，使得手动或自动处理故障转移情况变得困难。
- en: An alternative to building slave trees is to use compression across our network
    links to reduce the volume of data that needs to be transferred. Some users have
    found that using SSH to tunnel a connection with compression dropped bandwidth
    use significantly. One company went from using 21 megabits of network bandwidth
    for replicating to a single slave to about 1.8 megabits ([http://mng.bz/2ivv](http://mng.bz/2ivv)).
    If you use this method, you’ll want to use a mechanism that automatically reconnects
    a disconnected SSH connection, of which there are several options to choose from.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 建立从属树的另一种选择是使用网络链路上的压缩来减少需要传输的数据量。一些用户发现使用SSH进行压缩隧道连接可以显著降低带宽使用。有一家公司从使用21兆比特的网络带宽复制到单个从属节点降低到大约1.8兆比特（[http://mng.bz/2ivv](http://mng.bz/2ivv)）。如果你使用这种方法，你将想要使用一种机制来自动重新连接断开的SSH连接，这里有几个选项可供选择。
- en: '|  |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Encryption and compression overhead
  id: totrans-192
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 加密和压缩开销
- en: Generally, encryption overhead for SSH tunnels shouldn’t be a huge burden on
    your server, since AES-128 can encrypt around 180 megabytes per second on a single
    core of a 2.6 GHz Intel Core 2 processor, and RC4 can encrypt about 350 megabytes
    per second on the same machine. Assuming that you have a gigabit network link,
    roughly one moderately powerful core can max out the connection with encryption.
    Compression is where you may run into issues, because SSH compression defaults
    to gzip. At compression level 1 (you can configure SSH to use a specific compression
    level; check the man pages for SSH), our trusty 2.6 GHz processor can compress
    roughly 24–52 megabytes per second of a few different types of Redis dumps (the
    initial sync), and 60–80 megabytes per second of a few different types of append-only
    files (streaming replication). Remember that, though higher compression levels
    may compress more, they’ll also use more processor, which may be an issue for
    high-throughput low-processor machines. Generally, I’d recommend using compression
    levels below 5 if possible, since 5 still provides a 10–20% reduction in total
    data size over level 1, for roughly 2–3 times as much processing time. Compression
    level 9 typically takes 5–10 times the time for level 1, for compression only
    1–5% better than level 5 (I stick to level 1 for any reasonably fast network connection).
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，SSH隧道加密的开销不应给你的服务器带来巨大负担，因为AES-128可以在2.6 GHz Intel Core 2处理器的单个核心上每秒加密大约180兆字节，而RC4可以在同一台机器上每秒加密大约350兆字节。假设你有一个千兆网络链路，大约一个中等强大的核心可以加密到连接的最大值。压缩可能会遇到问题，因为SSH压缩默认使用gzip。在压缩级别1（你可以配置SSH使用特定的压缩级别；请检查SSH的手册页），我们可靠的2.6
    GHz处理器可以每秒压缩大约24-52兆字节的几种不同类型的Redis转储（初始同步），以及60-80兆字节的几种不同类型的只追加文件（流式复制）。记住，尽管更高的压缩级别可以压缩更多数据，但它们也会使用更多的处理器，这可能会成为高吞吐量低处理器机器的问题。通常，如果可能的话，我会建议使用低于5的压缩级别，因为5仍然提供了相对于级别1大约10-20%的总数据量减少，大约是2-3倍的处理时间。压缩级别9通常需要比级别1多5-10倍的时间，而压缩效果仅比级别5好1-5%（对于任何合理的快速网络连接，我坚持使用级别1）。
- en: '|  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Using compression with OpenVPN
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用OpenVPN进行压缩
- en: At first glance, OpenVPN’s support for AES encryption and compression using
    lzo may seem like a great turnkey solution to offering transparent reconnections
    with compression and encryption (as opposed to using one of the third-party SSH
    reconnecting scripts). Unfortunately, most of the information that I’ve been able
    to find has suggested that performance improvements when enabling lzo compression
    in OpenVPN are limited to roughly 25–30% on 10 megabit connections, and effectively
    zero improvement on faster connections.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 初看起来，OpenVPN对AES加密和lzo压缩的支持似乎是一个提供透明重新连接、压缩和加密（与使用第三方SSH重新连接脚本相比）的很好的即插即用解决方案。不幸的是，我所找到的大部分信息都表明，在OpenVPN中启用lzo压缩时的性能提升仅限于10兆比特连接的大约25-30%，在更快连接上几乎没有提升。
- en: '|  |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: One recent addition to the list of Redis tools that can be used to help with
    replication and failover is known as *Redis Sentinel*. Redis Sentinel is a mode
    of the Redis server binary where it doesn’t act as a typical Redis server. Instead,
    Sentinel watches the behavior and health of a number of masters and their slaves.
    By using `PUBLISH`/`SUBSCRIBE` against the masters combined with `PING` calls
    to slaves and masters, a collection of Sentinel processes independently discover
    information about available slaves and other Sentinels. Upon master failure, a
    single Sentinel will be chosen based on information that all Sentinels have and
    will choose a new master server from the existing slaves. After that slave is
    turned into a master, the Sentinel will move the slaves over to the new master
    (by default one at a time, but this can be configured to a higher number).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 最近添加到 Redis 工具列表中，可以帮助处理复制和故障转移的工具被称为 *Redis Sentinel*。Redis Sentinel 是 Redis
    服务器二进制的一个模式，其中它不作为典型的 Redis 服务器运行。相反，Sentinel 监视多个主节点及其从节点的行为和健康状态。通过针对主节点的 `PUBLISH`/`SUBSCRIBE`
    操作以及针对从节点和主节点的 `PING` 调用，一组 Sentinel 进程独立地发现有关可用从节点和其他 Sentinel 的信息。在主节点故障时，将根据所有
    Sentinel 拥有的信息选择一个 Sentinel，并从现有的从节点中选择一个新的主服务器。在此从节点变为主节点后，Sentinel 将将从节点移动到新的主节点（默认情况下一次一个，但可以配置为更高的数量）。
- en: Generally, the Redis Sentinel service is intended to offer automated failover
    from a master to one of its slaves. It offers options for notification of failover,
    calling user-provided scripts to update configurations, and more.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，Redis Sentinel 服务旨在提供从主节点到其从节点之一的自动故障转移。它提供了故障转移通知、调用用户提供的脚本来更新配置等选项。
- en: Now that we’ve made an attempt to scale our read capacity, it’s time to look
    at how we may be able to scale our write capacity as well.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经尝试扩展我们的读取容量，现在是时候看看我们如何可能能够扩展我们的写入容量了。
- en: 10.2\. Scaling writes and memory capacity
  id: totrans-202
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.2. 扩展写入和内存容量
- en: Back in [chapter 2](kindle_split_012.html#ch02), we built a system that could
    automatically cache rendered web pages inside Redis. Fortunately for us, it helped
    reduce page load time and web page processing overhead. Unfortunately, we’ve come
    to a point where we’ve scaled our cache up to the largest single machine we can
    afford, and must now split our data among a group of smaller machines.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第 2 章](kindle_split_012.html#ch02)中，我们构建了一个系统，该系统能够自动在 Redis 中缓存渲染的网页。幸运的是，这有助于减少页面加载时间和网页处理开销。不幸的是，我们已经达到了我们能够负担的最大单台机器的缓存规模，现在必须将数据分散到一组较小的机器上。
- en: '|  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scaling write volume
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 扩展写入量
- en: Though we discuss sharding in the context of increasing our total available
    memory, these methods also work to increase write throughput if we’ve reached
    the limit of performance that a single machine can sustain.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们在增加总可用内存的上下文中讨论了分片，但这些方法也可以在达到单台机器可以承受的性能极限时增加写入吞吐量。
- en: '|  |'
  id: totrans-207
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: In this section, we’ll discuss a method to scale memory and write throughput
    with sharding, using techniques similar to those we used in [chapter 9](kindle_split_021.html#ch09).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论一种使用与我们在[第 9 章](kindle_split_021.html#ch09)中使用的技术类似的方法来扩展内存和写入吞吐量的方法。
- en: 'To ensure that we really need to scale our write capacity, we should first
    make sure we’re doing what we can to reduce memory and how much data we’re writing:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保我们确实需要扩展我们的写入容量，我们首先应该确保我们已经做了我们能做的一切来减少内存使用和写入的数据量：
- en: Make sure that we’ve checked all of our methods to reduce read data volume first.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们已经检查了所有减少读取数据量的方法。
- en: Make sure that we’ve moved larger pieces of unrelated functionality to different
    servers (if we’re using our connection decorators from [chapter 5](kindle_split_016.html#ch05)
    already, this should be easy).
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确保我们已经将较大的无关功能块移动到不同的服务器上（如果我们已经从[第 5 章](kindle_split_016.html#ch05)中使用了我们的连接装饰器，这应该很容易）。
- en: If possible, try to aggregate writes in local memory before writing to Redis,
    as we discussed in [chapter 6](kindle_split_017.html#ch06) (which applies to almost
    all analytics and statistics calculation methods).
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果可能，在写入 Redis 之前，尽量在本地内存中聚合写入，正如我们在[第 6 章](kindle_split_017.html#ch06)中讨论的那样（这适用于几乎所有分析和统计计算方法）。
- en: If we’re running into limitations with `WATCH`/`MULTI`/`EXEC`, consider using
    locks as we discussed in [chapter 6](kindle_split_017.html#ch06) (or consider
    using Lua, as we’ll talk about in [chapter 11](kindle_split_023.html#ch11)).
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们在 `WATCH`/`MULTI`/`EXEC` 方面遇到限制，可以考虑使用我们在[第 6 章](kindle_split_017.html#ch06)中讨论的锁（或者考虑使用
    Lua，我们将在[第 11 章](kindle_split_023.html#ch11)中讨论）。
- en: If we’re using AOF persistence, remember that our disk needs to keep up with
    the volume of data we’re writing (400,000 small commands may only be a few megabytes
    per second, but 100,000 × 1 KB writes is 100 megabytes per second).
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用AOF持久化，请记住我们的磁盘需要跟上我们写入的数据量（400,000个小命令可能每秒只有几兆字节，但100,000 × 1 KB的写入是每秒100兆字节）。
- en: Now that we’ve done everything we can to reduce memory use, maximize performance,
    and understand the limitations of what a single machine can do, it’s time to actually
    shard our data to multiple machines. The methods that we use to shard our data
    to multiple machines rely on the number of Redis servers used being more or less
    fixed. If we can estimate that our write volume will, for example, increase 4
    times every 6 months, we can preshard our data into 256 shards. By presharding
    into 256 shards, we’d have a plan that should be sufficient for the next 2 years
    of expected growth (how far out to plan ahead for is up to you).
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经尽可能减少内存使用、最大化性能并了解单台机器的限制，是时候实际上将我们的数据分片到多台机器上了。我们将数据分片到多台机器的方法依赖于使用的Redis服务器数量大致固定。如果我们能估计到我们的写入量，例如，每6个月增加4倍，我们可以在256个分片中预先分片我们的数据。通过预先分片到256个分片，我们将有一个计划，应该能够满足未来2年的预期增长（提前计划多远取决于你）。
- en: '|  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Presharding for growth
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为了增长而预先分片
- en: When presharding your system in order to prepare for growth, you may be in a
    situation where you have too little data to make it worth running as many machines
    as you could need later. To still be able to separate your data, you can run multiple
    Redis servers on a single machine for each of your shards, or you can use multiple
    Redis databases inside a single Redis server. From this starting point, you can
    move to multiple machines through the use of replication and configuration management
    (see [section 10.2.1](#ch10lev2sec1)). If you’re running multiple Redis servers
    on a single machine, remember to have them listen on different ports, and make
    sure that all servers write to different snapshot files and/or append-only files.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 当为了准备增长而预先分片你的系统时，你可能处于数据太少以至于不值得运行你将来可能需要的那么多机器的情况。为了仍然能够分离你的数据，你可以为每个分片在单个机器上运行多个Redis服务器，或者你可以在单个Redis服务器内部使用多个Redis数据库。从这个起点，你可以通过使用复制和配置管理（参见[10.2.1节](#ch10lev2sec1)）过渡到多台机器。如果你在单个机器上运行多个Redis服务器，请记住让它们监听不同的端口，并确保所有服务器都写入不同的快照文件和/或追加文件。
- en: '|  |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The first thing that we need to do is to talk about how we’ll define our shard
    configuration.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是讨论我们将如何定义我们的分片配置。
- en: 10.2.1\. Handling shard configuration
  id: totrans-221
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.1\. 处理分片配置
- en: As you may remember from [chapter 5](kindle_split_016.html#ch05), we wrote a
    method to create and use named Redis configurations automatically. This method
    used a Python decorator to fetch configuration information, compare it with preexisting
    configuration information, and create or reuse an existing connection. We’ll extend
    this idea to add support for sharded connections. With these changes, we can use
    much of our code developed in [chapter 9](kindle_split_021.html#ch09) with minor
    changes.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从[第5章](kindle_split_016.html#ch05)中可能记得的，我们编写了一个创建和使用命名Redis配置的方法。这种方法使用Python装饰器来获取配置信息，将其与现有的配置信息进行比较，并创建或重用现有连接。我们将扩展这个想法以添加对分片连接的支持。通过这些更改，我们可以使用我们在[第9章](kindle_split_021.html#ch09)中开发的代码的大部分，只需进行一些小的修改。
- en: To get started, first let’s make a simple function that uses the same configuration
    layout that we used in our connection decorator from [chapter 5](kindle_split_016.html#ch05).
    If you remember, we use JSON-encoded dictionaries to store connection information
    for Redis in keys of the format `config:redis:<component>`. Pulling out the connection
    management part of the decorator, we end up with a simple function to create or
    reuse a Redis connection, based on a named configuration, shown here.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 要开始，首先让我们创建一个简单的函数，该函数使用我们在第5章（[chapter 5](kindle_split_016.html#ch05)）中连接装饰器中使用的相同配置布局。如果你还记得，我们使用JSON编码的字典来存储Redis的连接信息，键的格式为`config:redis:<component>`。提取装饰器的连接管理部分，我们最终得到一个简单的函数，用于根据命名配置创建或重用Redis连接，如下所示。
- en: Listing 10.1\. A function to get a Redis connection based on a named configuration
  id: totrans-224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.1\. 基于命名配置获取Redis连接的函数
- en: '![](233fig01_alt.jpg)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![预先分片图](233fig01_alt.jpg)'
- en: This simple function fetches the previously known as well as the current configuration.
    If they’re different, it updates the known configuration, creates a new connection,
    and then stores and returns that new connection. If the configuration hasn’t changed,
    it returns the previous connection.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 这个简单的函数获取先前已知以及当前的配置。如果它们不同，它将更新已知配置，创建一个新的连接，然后存储并返回这个新连接。如果配置没有改变，它将返回之前的连接。
- en: When we have the ability to fetch connections easily, we should also add support
    for the creation of sharded Redis connections, so even if our later decorators
    aren’t useful in every situation, we can still easily create and use sharded connections.
    To connect to a new sharded connection, we’ll use the same configuration methods,
    though sharded configuration will be a little different. For example, shard 7
    of component `logs` will be stored at a key named `config:redis:logs:7`. This
    naming scheme will let us reuse the existing connection and configuration code
    we already have. Our function to get a sharded connection is in the following
    listing.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们能够轻松获取连接时，我们还应该添加对创建分片Redis连接的支持，这样即使我们的后续装饰器在每种情况下都不太有用，我们仍然可以轻松地创建和使用分片连接。要连接到新的分片连接，我们将使用相同的配置方法，尽管分片配置会有所不同。例如，组件`logs`的第7个分片将存储在名为`config:redis:logs:7`的键中。这种命名方案将使我们能够重用我们已有的现有连接和配置代码。我们的获取分片连接的函数如下所示。
- en: Listing 10.2\. Fetch a connection based on shard information
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.2\. 基于分片信息获取连接
- en: '![](234fig01_alt.jpg)'
  id: totrans-229
  prefs: []
  type: TYPE_IMG
  zh: '![234fig01_alt.jpg](234fig01_alt.jpg)'
- en: Now that we have a simple method of fetching a connection to a Redis server
    that’s sharded, we can create a decorator like we saw in [chapter 5](kindle_split_016.html#ch05)
    that creates a sharded connection automatically.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了获取分片Redis服务器连接的简单方法，我们可以创建一个装饰器，就像我们在[第5章](kindle_split_016.html#ch05)中看到的那样，它会自动创建分片连接。
- en: 10.2.2\. Creating a server-sharded connection decorator
  id: totrans-231
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.2.2\. 创建服务器分片连接装饰器
- en: Now that we have a method to easily fetch a sharded connection, let’s use it
    to build a decorator to automatically pass a sharded connection to underlying
    functions.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了轻松获取分片连接的方法，让我们用它来构建一个装饰器，自动将分片连接传递给底层函数。
- en: We’ll perform the same three-level function decoration we used in [chapter 5](kindle_split_016.html#ch05),
    which will let us use the same kind of “component” passing we used there. In addition
    to component information, we’ll also pass the number of Redis servers we’re going
    to shard to. The following listing shows the details of our shard-aware connection
    decorator.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将执行与[第5章](kindle_split_016.html#ch05)中使用的相同的三级函数装饰，这将使我们能够使用那里使用的相同类型的“组件”传递。除了组件信息外，我们还将传递我们将要分片到其上的Redis服务器数量。以下列表显示了我们的分片感知连接装饰器的详细信息。
- en: Listing 10.3\. A shard-aware connection decorator
  id: totrans-234
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.3\. 一个分片感知连接装饰器
- en: '![](234fig02_alt.jpg)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![234fig02_alt.jpg](234fig02_alt.jpg)'
- en: Because of the way we constructed our connection decorator, we can decorate
    our `count_visit()` function from [chapter 9](kindle_split_021.html#ch09) almost
    completely unchanged. We need to be careful because we’re keeping aggregate count
    information, which is fetched and/or updated by our `get_expected()` function.
    Because the information stored will be used and reused on different days for different
    users, we need to use a nonsharded connection for it. The updated and decorated
    `count_visit()` function as well as the decorated and slightly updated `get_expected()`
    function are shown next.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们构建了连接装饰器的方式，我们可以几乎完全不变地装饰我们的`count_visit()`函数，该函数来自[第9章](kindle_split_021.html#ch09)。我们需要小心，因为我们正在保持聚合计数信息，这些信息由我们的`get_expected()`函数获取和/或更新。由于存储的信息将在不同用户的不同日子上被使用和重复使用，我们需要为它使用一个非分片连接。下面显示了更新并装饰过的`count_visit()`函数以及装饰并略有更新的`get_expected()`函数。
- en: Listing 10.4\. A machine and key-sharded `count_visit()` function
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.4\. 一个机器和键分片的`count_visit()`函数
- en: '![](235fig01_alt.jpg)'
  id: totrans-238
  prefs: []
  type: TYPE_IMG
  zh: '![235fig01_alt.jpg](235fig01_alt.jpg)'
- en: In our example, we’re sharding our data out to 16 different machines for the
    unique visit `SET`s, whose configurations are stored as JSON-encoded strings at
    keys named `config:redis:unique:0` to `config:redis:unique:15`. For our daily
    count information, we’re storing them in a nonsharded Redis server, whose configuration
    information is stored at key `config:redis:unique`.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将数据分片到16台不同的机器上，用于唯一的访问`SET`，其配置存储在名为`config:redis:unique:0`到`config:redis:unique:15`的键中。对于我们的每日计数信息，我们将其存储在一个非分片Redis服务器上，其配置信息存储在键`config:redis:unique`中。
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Multiple Redis servers on a single machine
  id: totrans-241
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 单机上的多个 Redis 服务器
- en: This section discusses sharding writes to multiple machines in order to increase
    total memory available and total write capacity. But if you’re feeling limited
    by Redis’s single-threaded processing limit (maybe because you’re performing expensive
    searches, sorts, or other queries), and you have more cores available for processing,
    more network available for communication, and more available disk I/O for snapshots/AOF,
    you can run multiple Redis servers on a single machine. You only need to configure
    them to listen on different ports and ensure that they have different snapshot/AOF
    configurations.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 本节讨论将写入扩展到多台机器以增加总内存和总写入容量。但如果你觉得受限于 Redis 的单线程处理限制（可能是因为你正在执行昂贵的搜索、排序或其他查询），并且你有更多的核心用于处理，更多的网络用于通信，以及更多的可用磁盘
    I/O 用于快照/AOF，你可以在单机上运行多个 Redis 服务器。你只需要配置它们在不同的端口上监听，并确保它们有不同的快照/AOF 配置。
- en: '|  |'
  id: totrans-243
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Alternate methods of handling unique visit counts over time
  id: totrans-245
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 随时间处理唯一访问次数的替代方法
- en: With the use of `SETBIT`, `BITCOUNT`, and `BITOP`, you can actually scale unique
    visitor counts without sharding by using an indexed lookup of bits, similar to
    what we did with locations in [chapter 9](kindle_split_021.html#ch09). A library
    that implements this in Python can be found at [https://github.com/Doist/bitmapist](https://github.com/Doist/bitmapist).
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用 `SETBIT`、`BITCOUNT` 和 `BITOP`，你可以通过使用类似我们在第 9 章 [chapter 9](kindle_split_021.html#ch09)
    中对位置所做的索引查找位，在不分片的情况下扩展唯一访问计数。一个在 Python 中实现此功能的库可以在 [https://github.com/Doist/bitmapist](https://github.com/Doist/bitmapist)
    找到。
- en: '|  |'
  id: totrans-247
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now that we have functions to get regular and sharded connections, as well as
    decorators to automatically pass regular and sharded connections, using Redis
    connections of multiple types is significantly easier. Unfortunately, not all
    operations that we need to perform on sharded datasets are as easy as a unique
    visitor count. In the next section, we’ll talk about scaling search in two different
    ways, as well as how to scale our social network example.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了获取常规和分片连接的函数，以及自动传递常规和分片连接的装饰器，使用多种类型的 Redis 连接变得显著更容易。不幸的是，我们需要的所有操作在分片数据集中并不像唯一访问计数那样简单。在下一节中，我们将讨论以两种不同的方式扩展搜索，以及如何扩展我们的社交网络示例。
- en: 10.3\. Scaling complex queries
  id: totrans-249
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.3. 扩展复杂查询
- en: As we scale out a variety of services with Redis, it’s not uncommon to run into
    situations where sharding our data isn’t enough, where the types of queries we
    need to execute require more work than just setting or fetching a value. In this
    section, we’ll discuss one problem that’s trivial to scale out, and two problems
    that take more work.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们使用 Redis 扩展各种服务，遇到数据分片不足以解决问题的情况并不少见，我们需要执行的类型查询比仅仅设置或获取值需要更多的工作。在本节中，我们将讨论一个简单可扩展的问题，以及两个需要更多工作的难题。
- en: The first problem that we’ll scale out is our search engine from [chapter 7](kindle_split_018.html#ch07),
    where we have machines with enough memory to hold our index, but we need to execute
    more queries than our server can currently handle.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先扩展的问题是我们从第 7 章 [chapter 7](kindle_split_018.html#ch07) 中的搜索引擎，我们有一些机器有足够的内存来存储我们的索引，但我们需要执行的查询比我们的服务器目前能处理的要多。
- en: 10.3.1\. Scaling search query volume
  id: totrans-252
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.1. 扩展搜索查询量
- en: As we expand our search engine from [chapter 7](kindle_split_018.html#ch07)
    with `SORT`, using the `ZSET`-based scored search, our ad-targeting search engine
    (or even the job-search system), at some point we may come to a point where a
    single server isn’t capable of handling the number of queries per second required.
    In this section, we’ll talk about how to add query slaves to further increase
    our capability to serve more search requests.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们的搜索引擎从第 7 章 [chapter 7](kindle_split_018.html#ch07) 使用 `SORT` 和基于 `ZSET`
    的评分搜索进行扩展，我们的广告定位搜索引擎（甚至求职搜索系统），在某个时候我们可能会达到一个点，即单个服务器无法处理每秒所需的查询数量。在本节中，我们将讨论如何添加查询从属服务器以进一步提高我们服务更多搜索请求的能力。
- en: In [section 10.1](#ch10lev1sec1), you saw how to scale read queries against
    Redis by adding read slaves. If you haven’t already read [section 10.1](#ch10lev1sec1),
    you should do so before continuing. After you have a collection of read slaves
    to perform queries against, if you’re running Redis 2.6 or later, you’ll immediately
    notice that performing search queries will fail. This is because performing a
    search as discussed in [chapter 7](kindle_split_018.html#ch07) requires performing
    `SUNIONSTORE`, `SINTERSTORE`, `SDIFFSTORE`, `ZINTERSTORE`, and/or `ZUNIONSTORE`
    queries, all of which write to Redis.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第10.1节](#ch10lev1sec1)中，你看到了如何通过添加从节点来扩展对Redis的只读查询。如果你还没有阅读[第10.1节](#ch10lev1sec1)，你应该在继续之前阅读。在你有一组从节点来执行查询之后，如果你正在运行Redis
    2.6或更高版本，你将立即注意到执行搜索查询会失败。这是因为执行第7章中讨论的搜索需要执行`SUNIONSTORE`、`SINTERSTORE`、`SDIFFSTORE`、`ZINTERSTORE`和/或`ZUNIONSTORE`查询，所有这些都会写入Redis。
- en: In order to perform writes against Redis 2.6 and later, we’ll need to update
    our Redis slave configuration. In the Redis configuration file, there’s an option
    to disable/enable writing to slaves. This option is called `slave-read-only`,
    and it defaults to `yes`. By changing `slave-read-only` to `no` and restarting
    our slaves, we should now be able to perform standard search queries against slave
    Redis servers. Remember that we cache the results of our queries, and these cached
    results are only available on the slave that the queries were run on. So if we
    intend to reuse cached results, we’ll probably want to perform some level of session
    persistence (where repeated requests from a client go to the same web server,
    and that web server always makes requests against the same Redis server).
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对Redis 2.6及更高版本执行写入操作，我们需要更新我们的Redis从节点配置。在Redis配置文件中，有一个选项可以禁用/启用对从节点的写入。这个选项被称为`slave-read-only`，默认值为`yes`。通过将`slave-read-only`更改为`no`并重新启动我们的从节点，我们现在应该能够对从Redis服务器执行标准搜索查询。记住，我们缓存了查询的结果，并且这些缓存的结果仅在运行查询的从节点上可用。所以如果我们打算重用缓存的结果，我们可能需要执行一定级别的会话持久性（即来自客户端的重复请求都发送到同一个Web服务器，并且该Web服务器始终向同一个Redis服务器发送请求）。
- en: In the past, I’ve used this method to scale an ad-targeting engine quickly and
    easily. If you decide to go this route to scale search queries, remember to pay
    attention to the resync issues discussed in [section 10.1](#ch10lev1sec1).
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去，我使用这种方法快速且容易地扩展了一个广告定位引擎。如果你决定走这条路来扩展搜索查询，请记住注意[第10.1节](#ch10lev1sec1)中讨论的重新同步问题。
- en: When we have enough memory in one machine and our operations are read-only (or
    at least don’t really change the underlying data to be used by other queries),
    adding slaves can help us to scale out. But sometimes data volumes can exceed
    memory capacity, and we still need to perform complex queries. How can we scale
    search when we have more data than available memory?
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们在一台机器上拥有足够的内存，并且我们的操作是只读的（或者至少不会真正更改其他查询使用的底层数据）时，添加从节点可以帮助我们扩展。但有时数据量可能会超过内存容量，我们仍然需要执行复杂的查询。当我们拥有的数据比可用内存多时，我们如何扩展搜索？
- en: 10.3.2\. Scaling search index size
  id: totrans-258
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.2. 扩展搜索索引大小
- en: If there’s one thing we can expect of a search engine, it’s that the search
    index will grow over time. As search indexes grow, the memory used by those search
    indexes also grows. Depending on the speed of the growth, we may or may not be
    able to keep buying/renting larger computers to run our index on. But for many
    of us, getting bigger and bigger computers is just not possible.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们能够期待搜索引擎的一点，那就是搜索索引会随着时间的推移而增长。随着搜索索引的增长，这些索引使用的内存也会增长。根据增长的速度，我们可能或可能无法继续购买/租赁更大的计算机来运行我们的索引。但对于我们中的许多人来说，获取更大和更大的计算机只是不可能的。
- en: In this section, we’ll talk about how to structure our data to support sharded
    search queries, and will include code to execute sharded search queries against
    a collection of sharded Redis masters (or slaves of sharded masters, if you followed
    the instructions in [section 10.3.1](#ch10lev2sec3)).
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论如何构建我们的数据结构以支持分片搜索查询，并将包括执行针对一组分片Redis主节点（或遵循[第10.3.1节](#ch10lev2sec3)中的说明的分片主节点的从节点）的分片搜索查询的代码。
- en: In order to shard our search queries, we must first shard our indexes so that
    for each document that we index, all of the data about that document is on the
    same shard. It turns out that our `index_document()` function from [chapter 7](kindle_split_018.html#ch07)
    takes a connection object, which we can shard by hand with the `docid` that’s
    passed. Or, because `index_document()` takes a connection followed by the `docid`,
    we can use our automatic sharding decorator from [listing 10.3](#ch10ex03) to
    handle sharding for us.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 为了对搜索查询进行分片，我们必须首先对索引进行分片，以便对于每个索引的文档，该文档的所有数据都在同一个分片上。结果是我们第7章中的`index_document()`函数接受一个连接对象，我们可以通过传递的`docid`手动进行分片。或者，因为`index_document()`接受一个连接后跟`docid`，我们可以使用我们的自动分片装饰器（来自列表10.3）来为我们处理分片。
- en: When we have our documents indexed across shards, we only need to perform queries
    against the shards to get the results. The details of what we need to do will
    depend on our type of index—whether it’s `SORT`-based or `ZSET`-based. Let’s first
    update our `SORT`-based index for sharded searches.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们的文档在分片之间索引时，我们只需要对分片执行查询以获取结果。我们需要做什么的细节将取决于我们的索引类型——是`SORT`基于的还是`ZSET`基于的。让我们首先更新我们的基于`SORT`的索引以进行分片搜索。
- en: Sharding SORT-based search
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 基于分片的排序搜索
- en: 'As is the case with all sharded searches, we need a way to combine the results
    of the sharded searches. In our implementation of `search_and_sort()` from [chapter
    7](kindle_split_018.html#ch07), we received a total count of results and the document
    IDs that were the result of the required query. This is a great building block
    to start from, but overall we’ll need to write functions to perform the following
    steps:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有分片搜索一样，我们需要一种方法来组合分片搜索的结果。在我们第7章中`search_and_sort()`的实现中，我们收到了结果的总量和所需查询的文档ID。这是一个很好的起点，但总体上我们需要编写函数来执行以下步骤：
- en: '**1**.  Perform the search and fetch the values to sort on for a query against
    a single shard.'
  id: totrans-265
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 对单个分片执行搜索并获取排序查询的值。'
- en: '**2**.  Execute the search on all shards.'
  id: totrans-266
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 在所有分片上执行搜索。'
- en: '**3**.  Merge the results of the queries, and choose the subset desired.'
  id: totrans-267
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 合并查询结果，并选择所需的子集。'
- en: First, let’s look at what it takes to perform the search and fetch the values
    from a single shard.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看执行搜索和从单个分片获取值需要什么。
- en: Because we already have `search_and_sort()` from [chapter 7](kindle_split_018.html#ch07),
    we can start by using that to fetch the result of a search. After we have the
    results, we can then fetch the data associated with each search result. But we
    have to be careful about pagination, because we don’t know which shard each result
    from a previous search came from. So, in order to always return the correct search
    results for items 91–100, we need to fetch the first 100 search results from every
    shard. Our code for fetching all of the necessary results and data can be seen
    in the next listing.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们已经有了来自第7章的`search_and_sort()`函数，所以我们可以先使用它来获取搜索结果。在得到结果后，我们可以获取与每个搜索结果相关的数据。但我们必须注意分页，因为我们不知道之前搜索中的每个结果来自哪个分片。因此，为了始终返回正确的搜索结果（第91-100项），我们需要从每个分片获取前100个搜索结果。获取所有必要结果和数据的代码可以在下一列表中看到。
- en: Listing 10.5\. `SORT`-based search that fetches the values that were sorted
  id: totrans-270
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.5\. 基于排序搜索并获取排序值的函数
- en: '![](237fig01_alt.jpg)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![图片](237fig01_alt.jpg)'
- en: This function fetches all of the information necessary from a single shard in
    preparation for the final merge. Our next step is to execute the query on all
    of the shards.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数从单个分片中获取所有必要的准备最终合并的信息。我们的下一步是在所有分片上执行查询。
- en: To execute a query on all of our shards, we have two options. We can either
    run each query on each shard one by one, or we can execute our queries across
    all of our shards simultaneously. To keep it simple, we’ll execute our queries
    one by one on each shard, collecting the results together in the next listing.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 要在我们的所有分片上执行查询，我们有两种选择。我们可以逐个在每个分片上运行每个查询，或者我们可以同时在我们的所有分片上执行我们的查询。为了简单起见，我们将在每个分片上逐个执行我们的查询，并将结果收集在下一列表中。
- en: Listing 10.6\. A function to perform queries against all shards
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.6\. 对所有分片执行查询的函数
- en: '![](238fig01_alt.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![图片](238fig01_alt.jpg)'
- en: 'This function works as explained: we execute queries against each shard one
    at a time until we have results from all shards. Remember that in order to perform
    queries against all shards, we must pass the proper shard count to the `get_shard_results()`
    function.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数按如下方式工作：我们逐个对每个分片执行查询，直到我们从所有分片获得结果。记住，为了对所有分片执行查询，我们必须将正确的分片数传递给 `get_shard_results()`
    函数。
- en: '|  |'
  id: totrans-277
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Run queries in parallel**'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：并行运行查询**'
- en: Python includes a variety of methods to run calls against Redis servers in parallel.
    Because most of the work with performing a query is actually just waiting for
    Redis to respond, we can easily use Python’s built-in threading and queue libraries
    to send requests to the sharded Redis servers and wait for a response. Can you
    write a version of `get_shard_results()` that uses threads to fetch results from
    all shards in parallel?
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: Python 包含了多种方法来并行运行对 Redis 服务器的调用。因为执行查询的大部分工作实际上只是等待 Redis 响应，我们可以轻松地使用 Python
    内置的线程和队列库向分片 Redis 服务器发送请求并等待响应。你能编写一个 `get_shard_results()` 的版本，它使用线程并行从所有分片获取结果吗？
- en: '|  |'
  id: totrans-280
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now that we have all of the results from all of the queries, we only need to
    re-sort our results so that we can get an ordering on all of the results that
    we fetched. This isn’t terribly complicated, but we have to be careful about numeric
    and non-numeric sorts, handling missing values, and handling non-numeric values
    during numeric sorts. Our function for merging results and returning only the
    requested results is shown in the next listing.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经从所有查询中获取了所有结果，我们只需要重新排序我们的结果，以便我们可以对所有获取的结果进行排序。这并不特别复杂，但我们必须小心处理数字和非数字排序，处理缺失值，以及处理数字排序中的非数字值。我们用于合并结果并仅返回请求结果的函数在下一个列表中显示。
- en: Listing 10.7\. A function to merge sharded search results
  id: totrans-282
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.7\. 一个合并分片搜索结果的函数
- en: '![](239fig01_alt.jpg)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![](239fig01_alt.jpg)'
- en: 'In order to handle sorting properly, we needed to write two function to convert
    data returned by Redis into values that could be consistently sorted against each
    other. You’ll notice that we chose to use Python `Decimal` values for sorting
    numerically. This is because we get the same sorted results with less code, and
    transparent support for handling infinity correctly. From there, all of our code
    does exactly what’s expected: we fetch the results, prepare to sort the results,
    sort the results, and then return only those document IDs from the search that
    are in the requested range.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了正确处理排序，我们需要编写两个函数将 Redis 返回的数据转换为可以一致排序的值。你会注意到我们选择使用 Python `Decimal` 值进行数字排序。这是因为我们用更少的代码得到相同的排序结果，并且透明地支持正确处理无穷大。从那里，我们所有的代码都完全符合预期：我们获取结果，准备对结果进行排序，对结果进行排序，然后仅返回搜索中请求范围内的文档
    ID。
- en: Now that we have a version of our `SORT`-based search that works across sharded
    Redis servers, it only remains to shard searching on `ZSET`-based sharded indexes.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有了跨分片 Redis 服务器工作的 `SORT` 基于搜索版本，剩下的工作就是分片基于 `ZSET` 的分片索引的搜索。
- en: Sharding ZSET-based search
  id: totrans-286
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分片基于 `ZSET` 的搜索
- en: 'Like a `SORT`-based search, handling searching for `ZSET`-based search requires
    running our queries against all shards, fetching the scores to sort by, and merging
    the results properly. We’ll go through the same steps that we did for `SORT`-based
    search in this section: search on one shard, search on all shards, and then merge
    the results.'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于 `SORT` 的搜索一样，处理基于 `ZSET` 的搜索需要运行我们的查询对所有分片，获取用于排序的分数，并正确合并结果。我们将在本节中执行与基于
    `SORT` 的搜索相同的步骤：在一个分片上搜索，在所有分片上搜索，然后合并结果。
- en: To search on one shard, we’ll wrap the [chapter 7](kindle_split_018.html#ch07)
    `search_and_zsort()` function on `ZSET`s, fetching the results and scores from
    the cached `ZSET` in the next listing.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 要在一个分片上搜索，我们将 `ZSET`s 上的 [第 7 章](kindle_split_018.html#ch07) `search_and_zsort()`
    函数包装起来，从下一个列表中的缓存 `ZSET` 中获取结果和分数。
- en: Listing 10.8\. `ZSET`-based search that returns scores for each result
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.8\. 基于 `ZSET` 的搜索，返回每个结果的分数
- en: '![](240fig01_alt.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](240fig01_alt.jpg)'
- en: Compared to the `SORT`-based search that does similar work, this function tries
    to keep things simple by ignoring the returned results without scores, and just
    fetches the results with scores directly from the cached `ZSET`. Because we have
    our scores already as floating-point numbers for easy sorting, we’ll combine the
    function to search on all shards with the function that merges and sorts the results.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 与基于`SORT`的执行类似工作的搜索相比，这个函数通过忽略没有得分的返回结果，直接从缓存的`ZSET`中获取带得分的直接结果来尝试保持简单。因为我们已经将得分作为浮点数存储，便于排序，所以我们将搜索所有分片的函数与合并和排序结果的函数结合起来。
- en: As before, we’ll perform searches for each shard one at a time, combining the
    results. When we have the results, we’ll sort them based on the scores that were
    returned. After the sort, we’ll return the results to the caller. The function
    that implements this is shown next.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将逐个对每个分片进行搜索，合并结果。当我们得到结果时，我们将根据返回的得分进行排序。排序后，我们将结果返回给调用者。实现此功能的函数如下所示。
- en: Listing 10.9\. Sharded search query over `ZSET`s that returns paginated results
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.9\. 在`ZSET`s上进行的分片搜索查询，返回分页结果
- en: '![](240fig02_alt.jpg)'
  id: totrans-294
  prefs: []
  type: TYPE_IMG
  zh: '![240fig02_alt.jpg](240fig02_alt.jpg)'
- en: With this code, you should have a good idea of the kinds of things necessary
    for handling sharded search queries. Generally, when confronted with a situation
    like this, I find myself questioning whether it’s worth attempting to scale these
    queries in this way. Given that we now have at least working sharded search code,
    the question is easier to answer. Note that as our number of shards increase,
    we’ll need to fetch more and more data in order to satisfy our queries. At some
    point, we may even need to delegate fetching and merging to other processes, or
    even merging in a tree-like structure. At that point, we may want to consider
    other solutions that were purpose-built for search (like Lucene, Solr, Elastic
    Search, or even Amazon’s Cloud Search).
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这段代码，你应该对处理分片搜索查询所需的各种事物有一个很好的了解。通常，面对这种情况，我会质疑是否值得尝试以这种方式扩展这些查询。鉴于我们现在至少有可工作的分片搜索代码，这个问题更容易回答。请注意，随着我们分片数量的增加，我们需要获取越来越多的数据以满足我们的查询。在某个时候，我们甚至可能需要将获取和合并的任务委托给其他进程，或者甚至以树状结构进行合并。到那时，我们可能需要考虑其他专门为搜索构建的解决方案（如Lucene、Solr、Elastic
    Search，甚至是亚马逊的云搜索）。
- en: Now that you know how to scale a second type of search, we really have only
    covered one other problem in other sections that might reach the point of needing
    to be scaled. Let’s take a look at what it would take to scale our social network
    from [chapter 8](kindle_split_019.html#ch08).
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了如何扩展第二种搜索类型，我们实际上在其他章节中只覆盖了可能需要扩展的一个其他问题。让我们看看从第8章[构建社交网络](kindle_split_019.html#ch08)扩展我们的社交网络需要什么。
- en: 10.3.3\. Scaling a social network
  id: totrans-297
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 10.3.3\. 缩放社交网络
- en: As we built our social network in [chapter 8](kindle_split_019.html#ch08), I
    pointed out that it wasn’t designed to scale to the size of a social network like
    Twitter, but that it was primarily meant to help you understand what structures
    and methods it takes to build a social network. In this section, I’ll describe
    a few methods that can let us scale a social networking site with sharding, almost
    without bounds (mostly limited by our budget, which is always the case with large
    projects).
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8章[构建社交网络](kindle_split_019.html#ch08)中构建社交网络时所指出的，它并不是为了扩展到像Twitter这样的社交网络规模而设计的，而是主要为了帮助你了解构建社交网络所需的结构和方法。在本节中，我将描述一些方法，这些方法可以让我们通过分片几乎无限制地扩展社交网站（主要受我们的预算限制，这在大型项目中总是如此）。
- en: 'One of the first steps necessary to helping a social network scale is figuring
    out what data is read often, what data is written often, and whether it’s possible
    to separate often-used data from rarely used data. To start, say that we’ve already
    pulled out our posted message data into a separate Redis server, which has read
    slaves to handle the moderately high volume of reads that occurs on that data.
    That really leaves two major types of data left to scale: timelines and follower/following
    lists.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 帮助社交网络进行扩展的第一个必要步骤之一是确定哪些数据经常被读取，哪些数据经常被写入，以及是否有可能将常用数据与很少使用的数据分开。首先，假设我们已经将发布消息数据提取到一个单独的Redis服务器中，该服务器有读取从属服务器来处理该数据上发生的中等高读取量。这实际上留下了两种主要类型的数据需要扩展：时间线和关注者/被关注者列表。
- en: '|  |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Scaling posted message database size
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 缩放已发布消息数据库的大小
- en: If you actually built this system out, and you had any sort of success, at some
    point you’d need to further scale the posted message database beyond just read
    slaves. Because each message is completely contained within a single `HASH`, these
    can be easily sharded onto a cluster of Redis servers based on the key where the
    `HASH` is stored. Because this data is easily sharded, and because we’ve already
    worked through how to fetch data from multiple shards as part of our search scaling
    in [section 10.3.2](#ch10lev2sec4), you shouldn’t have any difficulty here. Alternatively,
    you can also use Redis as a cache, storing recently posted messages in Redis,
    and older (rarely read) messages in a primarily on-disk storage server (like PostgreSQL,
    MySQL, Riak, MongoDB, and so on). If you’re finding yourself challenged, please
    feel free to post on the message board or on the Redis mailing list.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你实际上构建了这个系统，并且取得了一些成功，那么在某个时候，你需要进一步扩展发布消息数据库，而不仅仅是读副本。因为每条消息完全包含在一个单一的`HASH`中，这些可以基于存储`HASH`的键轻松地分片到Redis服务器集群中。由于这些数据可以轻松分片，并且因为我们已经在[第10.3.2节](#ch10lev2sec4)中解决了如何从多个分片中获取数据作为我们搜索扩展的一部分，所以你在这里不应该有任何困难。或者，你也可以使用Redis作为缓存，将最近发布的消息存储在Redis中，而将较旧（很少读取）的消息存储在主要基于磁盘的存储服务器（如PostgreSQL、MySQL、Riak、MongoDB等）中。如果你发现自己遇到了挑战，请随时在消息板上或在Redis邮件列表上发帖。
- en: '|  |'
  id: totrans-303
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'As you may remember, we had three primary types of timelines: home timelines,
    profile timelines, and list timelines. Timelines themselves are all similar, though
    both list timelines and home timelines are limited to 1,000 items. Similarly,
    followers, following, list followers, and list following are also essentially
    the same, so we’ll also handle them the same. First, let’s look at how we can
    scale timelines with sharding.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所记，我们主要有三种时间线类型：主页时间线、个人资料时间线和列表时间线。时间线本身都是相似的，尽管列表时间线和主页时间线都限制在1,000个项目以内。同样，关注者、被关注者、列表关注者和列表被关注者本质上也是相同的，因此我们也将以相同的方式处理它们。首先，让我们看看我们如何通过分片来扩展时间线。
- en: Sharding timelines
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分片时间线
- en: When we say that we’re sharding timelines, it’s a bit of a bait-and-switch.
    Because home and list timelines are short (1,000 entries max, which we may want
    to use to inform how large to set `zset-max-ziplist-size`),^([[1](#ch10fn01)])
    there’s really no need to shard the contents of the `ZSET`s; we merely need to
    place those timelines on different shards based on their key names.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说我们在分片时间线时，这有点像是一种诱饵和开关。因为主页和列表时间线较短（最多1,000条条目，我们可能希望用它来告知如何设置`zset-max-ziplist-size`的大小），^([[1](#ch10fn01)])实际上没有必要分片`ZSET`的内容；我们只需要根据它们的键名将这些时间线放置在不同的分片上。
- en: ¹ Because of the way we add items to home and list timelines, they can actually
    grow to roughly 2,000 entries for a short time. And because Redis doesn’t turn
    structures back into ziplist-encoded versions of themselves when they’ve gotten
    too large, setting `zset-max-ziplist-size` to be a little over 2,000 entries can
    keep these two timelines encoded efficiently.
  id: totrans-307
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 由于我们向主页和列表时间线添加项目的方式，它们实际上可以在短时间内增长到大约2,000条条目。而且由于Redis不会在结构变得太大时将其转换回自己的ziplist编码版本，将`zset-max-ziplist-size`设置为略超过2,000条条目可以保持这两个时间线编码效率。
- en: On the other hand, the size that profile timelines can grow to is currently
    unlimited. Though the vast majority of users will probably only be posting a few
    times a day at most, there can be situations where someone is posting significantly
    more often. As an example of this, the top 1,000 posters on Twitter have all posted
    more than 150,000 status messages, with the top 15 all posting more than a million
    messages.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，个人资料时间线可以增长到的大小目前是无限的。尽管绝大多数用户可能每天最多只发几次帖，但仍然可能存在某人发帖频率显著更高的情况。例如，Twitter上排名前1,000的帖主都发布了超过150,000条状态消息，其中前15位都发布了超过一百万条消息。
- en: On a practical level, it wouldn’t be unreasonable to cap the number of messages
    that are kept in the timeline for an individual user to 20,000 or so (the oldest
    being hidden or deleted), which would handle 99.999% of Twitter users generally.
    We’ll assume that this is our plan for scaling profile timelines. If not, we can
    use the technique we cover for scaling follower/following lists later in this
    section for scaling profile timelines instead.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: '在实际层面上，将单个用户的时间线中保留的消息数量限制在约20,000条（最旧的被隐藏或删除）是不合理的，这将处理99.999%的Twitter用户。我们将假设这是我们扩展个人资料时间线的计划。如果不是，我们可以使用本节后面覆盖的用于扩展关注者/被关注者列表的技术来扩展个人资料时间线。 '
- en: In order to shard our timelines based on key name, we could write a set of functions
    that handle sharded versions of `ZADD`, `ZREM`, and `ZRANGE`, along with others,
    all of which would be short three-line functions that would quickly get boring.
    Instead, let’s write a class that uses Python dictionary lookups to automatically
    create connections to shards.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了根据键名分片我们的时间线，我们可以编写一组处理`ZADD`、`ZREM`和`ZRANGE`等分片版本的函数，以及其他一些函数，所有这些都将是非常简短的、三行的函数，很快就会变得无聊。相反，让我们编写一个使用
    Python 字典查找来自动创建连接到分片的类。
- en: First, let’s start with what we want our API to look like by updating our `follow_user()`
    function from [chapter 8](kindle_split_019.html#ch08). We’ll create a generic
    sharded connection object that will let us create a connection to a given shard,
    based on a key that we want to access in that shard. After we have that connection,
    we can call all of the standard Redis methods to do whatever we want on that shard.
    We can see what we want our API to look like, and how we need to update our function,
    in the next listing.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们从更新我们的`follow_user()`函数开始，该函数在[第8章](kindle_split_019.html#ch08)中进行了更新。我们将创建一个通用的分片连接对象，它将允许我们根据我们想要访问的分片中的键创建一个连接。一旦我们有了这个连接，我们就可以调用所有标准的
    Redis 方法来在该分片上执行我们想要的任何操作。我们可以在下一个列表中看到我们想要的 API 看起来是什么样子，以及我们需要如何更新我们的函数。
- en: Listing 10.10\. An example of how we want our API for accessing shards to work
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.10\. 我们想要我们的分片访问 API 工作方式的示例
- en: '![](ch10ex10-0.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex10-0.jpg)'
- en: '![](ch10ex10-1.jpg)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex10-1.jpg)'
- en: Now that we have an idea of what we want our API to look like, let’s build it.
    We first need an object that takes the component and number of shards. When a
    key is referenced via dictionary lookup on the object, we need to return a connection
    to the shard that the provided key should be stored on. The class that implements
    this follows.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对想要我们的 API 看起来是什么样子有了想法，让我们来构建它。我们首先需要一个对象，该对象接受组件和分片数量。当一个键通过对象上的字典查找被引用时，我们需要返回一个连接到应该存储提供键的分片。实现这一点的类如下所示。
- en: Listing 10.11\. A class that implements sharded connection resolution based
    on key
  id: totrans-316
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 10.11\. 基于键名实现分片连接解析的类
- en: '![](243fig01_alt.jpg)'
  id: totrans-317
  prefs: []
  type: TYPE_IMG
  zh: '![图片](243fig01_alt.jpg)'
- en: For simple key-based sharding, this is all that’s necessary to support almost
    every call that we’d perform in Redis. All that remains is to update the remainder
    of `unfollow_user()`, `refill_timeline()`, and the rest of the functions that
    access home timelines and list timelines. If you intend to scale this social network,
    go ahead and update those functions yourself. For those of us who aren’t scaling
    the social network, we’ll continue on.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于键的简单分片，这几乎是我们将在 Redis 中执行的所有调用所需的一切。剩下的只是更新`unfollow_user()`、`refill_timeline()`以及访问家庭和列表时间线的其他函数的其余部分。如果您打算扩展这个社交网络，请自行更新这些函数。对于我们这些不扩展社交网络的人来说，我们将继续前进。
- en: '|  |'
  id: totrans-319
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Syndicating posts to home and list timelines**'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：将帖子同步到家庭和列表时间线**'
- en: 'With the update to where data is stored for both home and list timelines, can
    you update your list timeline supporting syndication task from [chapter 8](kindle_split_019.html#ch08)
    to support sharded profiles? Can you keep it almost as fast as the original version?
    Hint: If you’re stuck, we include a fully updated version that supports sharded
    follower lists in [listing 10.15](#ch10ex15).'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 随着对家庭和列表时间线数据存储位置的更新，您能否更新您的列表时间线以支持从[第8章](kindle_split_019.html#ch08)中的[章节
    8](kindle_split_019.html#ch08)支持的分片配置文件？您能否保持其几乎与原始版本一样快？提示：如果您遇到困难，我们包括了一个完全更新的版本，该版本支持分片关注者列表，请参阅[列表
    10.15](#ch10ex15)。
- en: '|  |'
  id: totrans-322
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Up next is scaling follower and following lists.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是扩展关注者和关注列表。
- en: Scaling follower and following lists with sharding
  id: totrans-324
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用分片扩展关注者和关注列表
- en: Though our scaling of timelines is pretty straightforward, scaling followers,
    following, and the equivalent “list” `ZSET`s is more difficult. The vast majority
    of these `ZSET`s will be short (99.99% of users on Twitter have fewer than 1,000
    followers), but there may be a few users who are following a large number of users,
    or who have a large number of followers. As a practical matter, it wouldn’t be
    unreasonable to limit the number of users that a given user or list can follow
    to be somewhat small (perhaps up to 1,000, to match the limits on home and list
    timelines), forcing them to create lists if they really want to follow more people.
    But we still run into issues when the number of followers of a given user grows
    substantially.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然我们对时间线的扩展相当直接，但扩展关注者、被关注者和等效的“列表”`ZSET`s则更为困难。这些`ZSET`s中的绝大多数将是短的（99.99%的Twitter用户关注者少于1,000人），但可能会有一些用户关注了大量用户，或者拥有大量关注者。作为一个实际问题，限制一个用户或列表可以关注的用户数量相对较小（可能最多1,000人，以匹配主页和列表时间线的限制）是合理的，迫使他们创建列表以真正地关注更多人。但是，当某个用户的关注者数量显著增加时，我们仍然会遇到问题。
- en: To handle the situation where follower/following lists can grow to be very large,
    we’ll shard these `ZSET`s across multiple shards. To be more specific, a user’s
    followers will be broken up into as many pieces as we have shards. For reasons
    we’ll get into in a moment, we only need to implement specific sharded versions
    of `ZADD`, `ZREM`, and `ZRANGEBYSCORE`.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 为了处理关注者/被关注者列表可能变得非常大的情况，我们将这些`ZSET`s分片到多个分片上。更具体地说，一个用户的关注者将被分成与我们拥有的分片数量一样多的部分。由于我们将要讨论的原因，我们只需要实现特定的分片版本的`ZADD`、`ZREM`和`ZRANGEBYSCORE`。
- en: 'I know what you’re thinking: since we just built a method to handle sharding
    automatically, we could use that. We will (to some extent), but because we’re
    sharding data and not just keys, we can’t just use our earlier class directly.
    Also, in order to reduce the number of connections we need to create and call,
    it makes a lot of sense to have data for both sides of the follower/following
    link on the same shard, so we can’t just shard by data like we did in [chapter
    9](kindle_split_021.html#ch09) and in [section 10.2](#ch10lev1sec2).'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 我知道你在想什么：既然我们刚刚构建了一个自动处理分片的方法，我们可以使用它。我们将（在一定程度上）这样做，但由于我们是在分片数据而不是仅仅键，我们无法直接使用我们之前的类。此外，为了减少我们需要创建和调用的连接数量，让关注者/被关注者链接的双方数据都在同一个分片上是非常有意义的，所以我们不能像在第9章（kindle_split_021.html#ch09）和第10.2节（#ch10lev1sec2）中那样仅通过数据来分片。
- en: In order to shard our follower/following data such that both sides of the follower/following
    relationship are on the same shard, we’ll use both IDs as part of the key to look
    up a shard. Like we did for sharding timelines, let’s update `follow_user()` to
    show the API that we’d like to use, and then we’ll create the class that’s necessary
    to implement the functionality. The updated `follow_user()` with our desired API
    is next.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使我们的关注者/被关注者数据分片，使得关注者/被关注者关系的双方都在同一个分片上，我们将使用这两个ID作为查找分片的关键部分。就像我们对分片时间线所做的那样，让我们更新`follow_user()`以显示我们希望使用的API，然后我们将创建实现该功能所需的类。带有我们期望的API的更新后的`follow_user()`将在下面。
- en: Listing 10.12\. Access follower/following `ZSET` shards
  id: totrans-329
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.12\. 访问关注者/被关注者`ZSET`分片
- en: '![](ch10ex12-0.jpg)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex12-0.jpg)'
- en: '![](ch10ex12-1.jpg)'
  id: totrans-331
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch10ex12-1.jpg)'
- en: Aside from a bit of rearrangement and code updating, the only difference between
    this change and the change we made earlier for sharding timelines is that instead
    of passing a specific key to look up the shard, we pass a pair of IDs. From these
    two IDs, we’ll calculate the proper shard that data involving both IDs should
    be stored on. The class that implements this API appears next.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 除了稍微调整和代码更新之外，这次更改与我们之前为分片时间线所做的更改唯一的不同之处在于，我们不是传递一个特定的键来查找分片，而是传递一对ID。从这两个ID中，我们将计算出涉及这两个ID的数据应该存储在哪个合适分片上。实现此API的类将在下面出现。
- en: Listing 10.13\. Sharded connection resolution based on ID pairs
  id: totrans-333
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.13\. 基于ID对的分片连接解析
- en: '![](245fig01_alt.jpg)'
  id: totrans-334
  prefs: []
  type: TYPE_IMG
  zh: '![图片](245fig01_alt.jpg)'
- en: The only thing different for this sharded connection generator, compared to
    [listing 10.11](#ch10ex11), is that this sharded connection generator takes a
    pair of IDs instead of a key. From those two IDs, we generate a key where the
    lower of the two IDs is first, and the higher is second. By constructing the key
    in this way, we ensure that whenever we reference the same two IDs, regardless
    of initial order, we always end up on the same shard.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 与[列表10.11](#ch10ex11)相比，这个分片连接生成器唯一的不同之处在于，它接受一对ID而不是一个键。从这两个ID中，我们生成一个键，其中较低的ID排在前面，较高的ID排在后面。通过这种方式构建键，我们确保无论初始顺序如何，只要我们引用相同的两个ID，我们总是会落在同一个分片上。
- en: With this sharded connection generator, we can update almost all of the remaining
    follower/following `ZSET` operations. The one remaining operation that’s left
    is to properly handle `ZRANGEBYSCORE`, which we use in a few places to fetch a
    “page” of followers. Usually this is done to syndicate messages out to home and
    list timelines when an update is posted. When syndicating to timelines, we could
    scan through all of one shard’s `ZSET`, and then move to the next. But with a
    little extra work, we could instead pass through all `ZSET`s simultaneously, which
    would give us a useful sharded `ZRANGEBYSCORE` operation that can be used in other
    situations.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个分片连接生成器，我们可以更新几乎所有剩余的跟随者/被跟随者`ZSET`操作。剩下的一个操作是正确处理`ZRANGEBYSCORE`，我们在几个地方使用它来获取“页面”的跟随者。通常这是在更新发布时将消息分发给主页和列表时间线时进行的。在分发给时间线时，我们可以扫描一个分片的全部`ZSET`，然后转到下一个。但通过一点额外的工作，我们可以同时通过所有`ZSET`s，这将给我们一个有用的分片`ZRANGEBYSCORE`操作，可以在其他情况下使用。
- en: As we saw in [section 10.3.2](#ch10lev2sec4), in order to fetch items 100–109
    from sharded `ZSET`s, we needed to fetch items 0–109 from all `ZSET`s and merge
    them together. This is because we only knew the index that we wanted to start
    at. Because we have the opportunity to scan based on score instead, when we want
    to fetch the next 10 items with scores greater than X, we only need to fetch the
    next 10 items with scores greater than X from all shards, followed by a merge.
    A function that implements `ZRANGEBYSCORE` across multiple shards is shown in
    the following listing.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们在[第10.3.2节](#ch10lev2sec4)中看到的，为了从分片的`ZSET`s中获取项目100-109，我们需要从所有`ZSET`s中获取项目0-109并将它们合并在一起。这是因为我们只知道我们想要开始的索引。由于我们有基于分数扫描的机会，当我们想要获取分数大于X的下一个10个项目时，我们只需要从所有分片中获取分数大于X的下一个10个项目，然后进行合并。以下列表显示了实现跨多个分片`ZRANGEBYSCORE`的函数。
- en: Listing 10.14\. A function that implements a sharded `ZRANGEBYSCORE`
  id: totrans-338
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.14。实现分片`ZRANGEBYSCORE`的函数
- en: '![](246fig01_alt.jpg)'
  id: totrans-339
  prefs: []
  type: TYPE_IMG
  zh: '![](246fig01_alt.jpg)'
- en: This function works a lot like the query/merge that we did in [section 10.3.2](#ch10lev2sec4),
    only we can start in the middle of the `ZSET` because we have scores (and not
    indexes).
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数的工作方式与我们在[第10.3.2节](#ch10lev2sec4)中做的查询/合并非常相似，只是我们可以在`ZSET`的中间开始，因为我们有分数（而不是索引）。
- en: '|  |'
  id: totrans-341
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Using this method for sharding profile timelines
  id: totrans-342
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用此方法进行分片个人资料时间线
- en: You’ll notice that we use timestamps for follower/following lists, which avoided
    some of the drawbacks to paginate over sharded `ZSET`s that we covered in [section
    10.3.2](#ch10lev2sec4). If you’d planned on using this method for sharding profile
    timelines, you’ll need to go back and update your code to use timestamps instead
    of offsets, and you’ll need to implement a `ZREVRANGEBYSCORE` version of [listing
    10.14](#ch10ex14), which should be straightforward.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到我们使用时间戳来处理跟随者/被跟随者列表，这避免了我们在[第10.3.2节](#ch10lev2sec4)中讨论的跨分片`ZSET`s分页的一些缺点。如果你计划使用这种方法来分片个人资料时间线，你需要回过头来更新你的代码，使用时间戳而不是偏移量，并且你需要实现一个`ZREVRANGEBYSCORE`版本的[列表10.14](#ch10ex14)，这应该是直截了当的。
- en: '|  |'
  id: totrans-344
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: With this new sharded `ZRANGEBYSCORE` function, let’s update our function that
    syndicates posts to home and list timelines in the next listing. While we’re at
    it, we may as well add support for sharded home timelines.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个新的分片`ZRANGEBYSCORE`函数，让我们更新下一个列表中用于将帖子分发给主页和列表时间线的函数。当我们这样做的时候，我们不妨添加对分片主页时间线的支持。
- en: Listing 10.15\. Updated syndicate status function
  id: totrans-346
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表10.15。更新后的联盟状态函数
- en: '![](ch10ex15-0.jpg)'
  id: totrans-347
  prefs: []
  type: TYPE_IMG
  zh: '![](ch10ex15-0.jpg)'
- en: '![](ch10ex15-1.jpg)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![](ch10ex15-1.jpg)'
- en: As you can see from the code, we use the sharded `ZRANGEBYSCORE` function to
    fetch those users who are interested in this user’s posts. Also, in order to keep
    the syndication process fast, we group requests that are to be sent to each home
    or list timeline shard server together. Later, after we’ve grouped all of the
    writes together, we add the post to all of the timelines on a given shard server
    with a pipeline. Though this may be slower than the nonsharded version, this does
    allow us to scale our social network much larger than before.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 如代码所示，我们使用分片的 `ZRANGEBYSCORE` 函数来获取对这个用户帖子感兴趣的用户。此外，为了保持内容同步过程快速，我们将要发送到每个主页或列表时间线分片服务器的请求分组在一起。稍后，在我们将所有写入操作分组在一起后，我们使用管道将帖子添加到给定分片服务器上的所有时间线上。虽然这可能比非分片版本慢，但这确实允许我们将社交网络扩展得比以前更大。
- en: All that remains is to finish updating the rest of the functions to support
    all of the sharding that we’ve done in the rest of [section 10.3.3](#ch10lev2sec5).
    Again, if you’re going to scale this social network, feel free to do so. But if
    you have some nonsharded code that you want to shard, you can compare the earlier
    version of `syndicate_status()` from [section 8.4](kindle_split_019.html#ch08lev1sec4)
    with our new version to get an idea of how to update your code.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 剩下的工作就是完成更新其余的函数，以支持我们在 [第 10.3.3 节](#ch10lev2sec5) 中所做的所有分片。再次提醒，如果你打算扩展这个社交网络，请随意这样做。但如果你有一些未分片的代码想要分片，你可以将
    [第 8.4 节](kindle_split_019.html#ch08lev1sec4) 中的 `syndicate_status()` 的早期版本与我们的新版本进行比较，以了解如何更新你的代码。
- en: 10.4\. Summary
  id: totrans-351
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 10.4\. 概述
- en: In this chapter, we revisited a variety of problems to look at what it’d take
    to scale them to higher read volume, higher write volume, and more memory. We’ve
    used read-only slaves, writable query slaves, and sharding combined with shard-aware
    classes and functions. Though these methods may not cover all potential use cases
    for scaling your particular problem, each of these examples was chosen to offer
    a set of techniques that can be used generally in other situations.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们回顾了各种问题，以了解将它们扩展到更高的读取量、更高的写入量和更多的内存需要什么。我们使用了只读从属服务器、可写查询从属服务器以及与分片感知类和函数结合的分片。尽管这些方法可能不会涵盖你特定问题所有潜在的扩展用例，但每个示例都是选择提供一套可以在其他情况下普遍使用的技术的。
- en: If there’s one concept that you should take away from this chapter, it’s that
    scaling any system can be a challenge. But with Redis, you can use a variety of
    methods to scale your platform (hopefully as far as you need it to scale).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这个章节中带走一个概念，那就是扩展任何系统都可能是一个挑战。但有了 Redis，你可以使用各种方法来扩展你的平台（希望可以扩展到你需要的程度）。
- en: Coming up in the next and final chapter, we’ll cover Redis scripting with Lua.
    We’ll revisit a few past problems to show how our solutions can be simplified
    and performance improved with features available in Redis 2.6 and later.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章和最后一章中，我们将介绍使用 Lua 脚本进行 Redis 脚本编程。我们将回顾一些过去的问题，以展示如何通过 Redis 2.6 及更高版本中提供的功能简化我们的解决方案并提高性能。
- en: Chapter 11\. Scripting Redis with Lua
  id: totrans-355
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 11 章\. 使用 Lua 脚本脚本化 Redis
- en: '*This chapter covers*'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Adding functionality without writing C
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不用编写 C 代码添加功能
- en: Rewriting locks and semaphores with Lua
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lua 重新编写锁和信号量
- en: Doing away with WATCH/MULTI/EXEC
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 摆脱 WATCH/MULTI/EXEC
- en: Sharding LISTs with Lua
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Lua 分片 LIST
- en: Over the last several chapters, you’ve built up a collection of tools that you
    can use in existing applications, while also encountering techniques you can use
    to solve a variety of problems. This chapter does much of the same, but will turn
    some of your expectations on their heads. As of Redis 2.6, Redis includes *server-side
    scripting* with the Lua programming language. This lets you perform a variety
    of operations inside Redis, which can both simplify your code and increase performance.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的几章中，你已经建立了一个工具集，你可以在现有应用程序中使用这些工具，同时也会遇到可以用来解决各种问题的技术。本章做了很多同样的事情，但会颠覆一些你的预期。自
    Redis 2.6 版本起，Redis 包含了 Lua 编程语言的 *服务器端脚本* 功能。这让你可以在 Redis 内部执行各种操作，既可以简化你的代码，也可以提高性能。
- en: In this chapter, we’ll start by discussing some of the advantages of Lua over
    performing operations on the client, showing an example from the social network
    in [chapter 8](kindle_split_019.html#ch08). We’ll then go through two problems
    from [chapters 4](kindle_split_015.html#ch04) and [6](kindle_split_017.html#ch06)
    to show examples where using Lua can remove the need for `WATCH`/`MULTI`/`EXEC`
    transactions. Later, we’ll revisit our locks and semaphores from [chapter 6](kindle_split_017.html#ch06)
    to show how they can be implemented using Lua for fair multiple client access
    and higher performance. Finally, we’ll build a sharded `LIST` using Lua that supports
    many (but not all) standard `LIST` command equivalents.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将首先讨论Lua相对于在客户端执行操作的一些优势，并通过[第8章](kindle_split_019.html#ch08)中的社交网络示例来展示。然后，我们将通过[第4章](kindle_split_015.html#ch04)和[第6章](kindle_split_017.html#ch06)中的两个问题来展示使用Lua可以消除`WATCH`/`MULTI`/`EXEC`事务需求的示例。稍后，我们将重新审视[第6章](kindle_split_017.html#ch06)中的锁和信号量，以展示它们如何使用Lua实现公平的多客户端访问和更高的性能。最后，我们将使用Lua构建一个分片`LIST`，它支持许多（但不是所有）标准的`LIST`命令等效功能。
- en: Let’s get started by learning about some of the things that we can do with Lua
    scripting.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始学习我们可以使用Lua脚本做一些什么。
- en: 11.1\. Adding functionality without writing C
  id: totrans-364
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.1. 添加功能而不编写C代码
- en: Prior to Redis 2.6 (and the unsupported scripting branch of Redis 2.4), if we
    wanted higher-level functionality that didn’t already exist in Redis, we’d either
    have to write client-side code (like we have through the last 10 chapters), or
    we’d have to edit the C source code of Redis itself. Though editing Redis’s source
    code isn’t too difficult, supporting such code in a business environment, or trying
    to convince management that running our own version of the Redis server is a good
    idea, could be challenging.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis 2.6版本之前（以及Redis 2.4版本不受支持的脚本分支），如果我们想要Redis中尚未存在的高级功能，我们要么必须编写客户端代码（就像我们在过去10章中做的那样），要么必须编辑Redis本身的C源代码。尽管编辑Redis的源代码并不太难，但在商业环境中支持此类代码，或者试图说服管理层运行我们自己的Redis服务器版本是一个好主意，可能会很具挑战性。
- en: In this section, we’ll introduce methods by which we can execute Lua inside
    the Redis server. By scripting Redis with Lua, we can avoid some common pitfalls
    that slow down development or reduce performance.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将介绍一些方法，通过这些方法我们可以在Redis服务器内部执行Lua代码。通过用Lua脚本编写Redis，我们可以避免一些常见的陷阱，这些陷阱会减慢开发速度或降低性能。
- en: The first step in executing Lua code in Redis is loading the code into Redis.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 在Redis中执行Lua代码的第一步是将代码加载到Redis中。
- en: 11.1.1\. Loading Lua scripts into Redis
  id: totrans-368
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.1. 将Lua脚本加载到Redis中
- en: Some older (and still used) Python Redis libraries for Redis 2.6 don’t yet offer
    the capability to load or execute Lua scripts directly, so we’ll spend a few moments
    to create a loader for the scripts. To load scripts into Redis, there’s a two-part
    command called `SCRIPT LOAD` that, when provided with a string that’s a Lua script,
    will store the script for later execution and return the SHA1 hash of the script.
    Later, when we want to execute that script, we run the Redis command `EVALSHA`
    with the hash that was returned by Redis, along with any arguments that the script
    needs.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 一些较旧的（仍在使用中）的Redis 2.6版本的Python Redis库还没有提供直接加载或执行Lua脚本的能力，因此我们将花几分钟时间创建一个用于脚本加载的程序。要将脚本加载到Redis中，有一个两部分的命令称为`SCRIPT
    LOAD`，当提供一个Lua脚本字符串时，它将存储脚本以供以后执行，并返回脚本的SHA1哈希值。稍后，当我们想要执行该脚本时，我们运行Redis命令`EVALSHA`，并提供Redis返回的哈希值以及脚本需要的任何参数。
- en: Our code for doing these operations will be inspired by the current Python Redis
    code. (We use our method primarily because it allows for using any connection
    we want without having to explicitly create new scripting objects, which can be
    useful when dealing with server sharding.) When we pass a string to our `script_load()`
    function, it’ll create a function that can later be called to execute the script
    in Redis. When calling the object to execute the script, we must provide a Redis
    connection, which will then call `SCRIPT LOAD` on its first call, followed by
    `EVALSHA` for all future calls. The `script_load()` function is shown in the following
    listing.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进行这些操作的代码将受到当前Python Redis代码的启发。（我们主要使用这种方法，因为它允许我们使用任何我们想要的连接，而无需显式创建新的脚本对象，这在处理服务器分片时可能很有用。）当我们向`script_load()`函数传递一个字符串时，它将创建一个可以稍后调用来在Redis中执行脚本的函数。在调用对象以执行脚本时，我们必须提供一个Redis连接，该连接将在第一次调用时调用`SCRIPT
    LOAD`，然后是所有后续调用中的`EVALSHA`。`script_load()`函数如下所示。
- en: Listing 11.1\. A function that loads scripts to be called later
  id: totrans-371
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.1\. 一个用于加载稍后调用的脚本的函数
- en: '![](250fig01_alt.jpg)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![](250fig01_alt.jpg)'
- en: You’ll notice that in addition to our `SCRIPT LOAD` and `EVALSHA` calls, we
    captured an exception that can happen if we’ve cached a script’s SHA1 hash locally,
    but the server doesn’t know about it. This can happen if the server were to be
    restarted, if someone had executed the `SCRIPT FLUSH` command to clean out the
    script cache, or if we provided connections to two different Redis servers to
    our function at different times. If we discover that the script is missing, we
    execute the script directly with `EVAL`, which caches the script in addition to
    executing it. Also, we allow clients to directly execute the script, which can
    be useful when executing a Lua script as part of a transaction or other pipelined
    sequence.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，除了我们的 `SCRIPT LOAD` 和 `EVALSHA` 调用之外，我们还捕获了一个异常，这个异常可能发生在我们本地缓存了脚本的 SHA1
    哈希值，但服务器不知道它。如果服务器重启，如果有人执行了 `SCRIPT FLUSH` 命令来清理脚本缓存，或者如果我们函数在一段时间内提供了连接到两个不同的
    Redis 服务器，这种情况就会发生。如果我们发现脚本缺失，我们将直接使用 `EVAL` 执行脚本，这将在执行脚本的同时将其缓存。此外，我们还允许客户端直接执行脚本，这在将
    Lua 脚本作为事务或其他管道序列的一部分执行时可能很有用。
- en: '|  |'
  id: totrans-374
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Keys and arguments to Lua scripts
  id: totrans-375
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Lua 脚本的键和参数
- en: Buried inside our script loader, you may have noticed that calling a script
    in Lua takes three arguments. The first is a Redis connection, which should be
    standard by now. The second argument is a list of keys. The third is a list of
    arguments to the function.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的脚本加载器内部，你可能已经注意到调用 Lua 脚本需要三个参数。第一个是一个 Redis 连接，到现在应该已经是标准的了。第二个参数是一个键列表。第三个是传递给函数的参数列表。
- en: The difference between keys and arguments is that you’re supposed to pass all
    of the keys that the script will be reading or writing as part of the `keys` argument.
    This is to potentially let other layers verify that all of the keys are on the
    same shard if you were using multiserver sharding techniques like those described
    in [chapter 10](kindle_split_022.html#ch10).
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 键和参数之间的区别在于，你应该将脚本将读取或写入的所有键作为 `keys` 参数传递。这是为了在您使用类似第 10 章中描述的多服务器分片技术时，让其他层验证所有键是否位于同一分片上。
- en: When Redis cluster is released, which will offer automatic multiserver sharding,
    keys will be checked before a script is run, and will return an error if any keys
    that aren’t on the same server are accessed.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Redis 集群发布时，它将提供自动的多服务器分片，脚本运行之前会检查键，如果访问了不在同一服务器上的任何键，则会返回错误。
- en: The second list of arguments has no such limitation and is meant to hold data
    to be used inside the Lua call.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个参数列表没有这样的限制，其目的是在 Lua 调用内部使用数据。
- en: '|  |'
  id: totrans-380
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s try it out in the console for a simple example to get started.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在控制台中尝试一个简单的例子来开始。
- en: '![](251fig01_alt.jpg)'
  id: totrans-382
  prefs: []
  type: TYPE_IMG
  zh: '![](251fig01_alt.jpg)'
- en: As you can see in this example, we created a simple script whose only purpose
    is to return a value of 1\. When we call it with the connection object, the script
    is loaded and then executed, resulting in the value `1` being returned.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 如此例所示，我们创建了一个简单的脚本，其唯一目的是返回值 1。当我们用连接对象调用它时，脚本被加载并执行，结果返回值 `1`。
- en: Returning non-string and non-integer values from Lua
  id: totrans-384
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从 Lua 返回非字符串和非整数值
- en: Due to limitations in how Lua allows data to be passed in and out of it, some
    data types that are available in Lua aren’t allowed to be passed out, or are altered
    before being returned. [Table 11.1](#ch11table01) shows how this data is altered
    upon return.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Lua 允许数据进出时的限制，一些在 Lua 中可用的数据类型不允许被传出，或者在被返回之前会被修改。[表 11.1](#ch11table01)
    展示了这些数据在被返回时的修改情况。
- en: Table 11.1\. Values returned from Lua and what they’re translated into
  id: totrans-386
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.1\. Lua 返回的值及其转换结果
- en: '| Lua value | What happens during conversion to Python |'
  id: totrans-387
  prefs: []
  type: TYPE_TB
  zh: '| Lua 值 | 转换为 Python 时的行为 |'
- en: '| --- | --- |'
  id: totrans-388
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| true | Turns into 1 |'
  id: totrans-389
  prefs: []
  type: TYPE_TB
  zh: '| true | 转换为 1 |'
- en: '| false | Turns into None |'
  id: totrans-390
  prefs: []
  type: TYPE_TB
  zh: '| false | 转换为 None |'
- en: '| nil | Doesn’t turn into anything, and stops remaining values in a table from
    being returned |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '| nil | 不会转换为任何东西，并阻止表中的剩余值被返回 |'
- en: '| 1.5 (or any other float) | Fractional part is discarded, turning it into
    an integer |'
  id: totrans-392
  prefs: []
  type: TYPE_TB
  zh: '| 1.5（或任何其他浮点数） | 舍弃小数部分，转换为整数 |'
- en: '| 1e30 (or any other large float) | Is turned into the minimum integer for
    your version of Python |'
  id: totrans-393
  prefs: []
  type: TYPE_TB
  zh: '| 1e30（或任何其他大浮点数） | 转换为 Python 版本的最小整数 |'
- en: '| "strings" | Unchanged |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '| "strings" | 不变 |'
- en: '| 1 (or any other integer +/-2^(53)-1) | Integer is returned unchanged |'
  id: totrans-395
  prefs: []
  type: TYPE_TB
  zh: '| 1（或任何其他整数 +/-2^(53)-1）| 整数返回不变 |'
- en: Because of the ambiguity that results when returning a variety of data types,
    you should do your best to explicitly return strings whenever possible, and perform
    any parsing manually. We’ll only be returning Booleans, strings, integers, and
    Lua tables (which are turned into Python lists) for our examples.
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 由于返回多种数据类型时产生的歧义，您应尽可能明确地返回字符串，并手动进行任何解析。在我们的示例中，我们只返回布尔值、字符串、整数和Lua表（这些表将被转换为Python列表）。
- en: Now that we can load and execute scripts, let’s get started with a simple example
    from [chapter 8](kindle_split_019.html#ch08), creating a status message.
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们可以加载和执行脚本，让我们从第8章的一个简单示例开始，创建一个状态消息。
- en: 11.1.2\. Creating a new status message
  id: totrans-398
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.1.2\. 创建新的状态消息
- en: As we build Lua scripts to perform a set of operations, it’s good to start with
    a short example that isn’t terribly complicated or structure-intensive. In this
    case, we’ll start by writing a Lua script combined with some wrapper code to post
    a status message.
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们构建用于执行一系列操作的Lua脚本时，从简短且不复杂的示例开始是很好的。在这种情况下，我们将首先编写一个Lua脚本，并结合一些包装代码来发布状态消息。
- en: '|  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Lua scripts—as atomic as single commands or `MULTI/EXEC`
  id: totrans-401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Lua脚本——与单个命令或`MULTI/EXEC`一样原子
- en: As you already know, individual commands in Redis are *atomic* in that they’re
    run one at a time. With `MULTI`/`EXEC`, you can prevent other commands from running
    while you’re executing multiple commands. But to Redis, `EVAL` and `EVALSHA` are
    each considered to be a (very complicated) command, so they’re executed without
    letting any other structure operations occur.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，Redis中的单个命令是*原子性*的，因为它们一次执行一个。使用`MULTI`/`EXEC`，您可以在执行多个命令时防止其他命令运行。但是，对Redis来说，`EVAL`和`EVALSHA`被视为一个（非常复杂）的命令，因此它们在执行时不会允许任何其他结构操作发生。
- en: '|  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-404
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Lua scripts—can’t be interrupted if they have modified structures
  id: totrans-405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Lua脚本——如果它们修改了结构，则无法中断
- en: When executing a Lua script with `EVAL` or `EVALSHA`, Redis doesn’t allow any
    other read/write commands to run. This can be convenient. But because Lua is a
    general-purpose programming language, you can write scripts that never return,
    which could stop other clients from executing commands. To address this, Redis
    offers two options for stopping a script in Redis, depending on whether you’ve
    performed a Redis call that writes.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用`EVAL`或`EVALSHA`执行Lua脚本时，Redis不允许运行任何其他读写命令。这可能是方便的。但是，因为Lua是一种通用编程语言，您可以编写永远不会返回的脚本，这可能会阻止其他客户端执行命令。为了解决这个问题，Redis提供了两种停止Redis中脚本的方法，具体取决于您是否执行了Redis的写入调用。
- en: If your script hasn’t performed any calls that write (only reads), you can execute
    `SCRIPT KILL` if the script has been executing for longer than the configured
    `lua-time-limit` (check your Redis configuration file).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的脚本没有执行任何写入操作（只有读取），当脚本执行时间超过配置的`lua-time-limit`（检查您的Redis配置文件）时，您可以执行`SCRIPT
    KILL`。
- en: If your script has written to Redis, then killing the script could cause Redis
    to be in an inconsistent state. In that situation, the only way you can recover
    is to kill Redis with the `SHUTDOWN NOSAVE` command, which will cause Redis to
    lose any changes that have been made since the last snapshot, or since the last
    group of commands was written to the AOF.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的脚本已写入Redis，则终止脚本可能会导致Redis处于不一致的状态。在这种情况下，您唯一可以恢复的方法是使用`SHUTDOWN NOSAVE`命令终止Redis，这将导致Redis丢失自上次快照以来或自上次命令组写入AOF以来所做的任何更改。
- en: Because of these limitations, you should always test your Lua scripts before
    running them in production.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这些限制，您应该在将Lua脚本投入生产运行之前始终对其进行测试。
- en: '|  |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As you may remember from [chapter 8](kindle_split_019.html#ch08), [listing 8.2](kindle_split_019.html#ch08ex02)
    showed the creation of a status message. A copy of the original code we used for
    posting a status message appears next.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如您从[第8章](kindle_split_019.html#ch08)中可能记得的，[列表8.2](kindle_split_019.html#ch08ex02)展示了创建状态消息的过程。接下来将展示我们用于发布状态消息的原始代码的一个副本。
- en: Listing 11.2\. Our function from [listing 8.2](kindle_split_019.html#ch08ex02)
    that creates a status message `HASH`
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.2\. 来自[列表8.2](kindle_split_019.html#ch08ex02)的函数，用于创建状态消息`HASH`
- en: '![](253fig01_alt.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![](253fig01_alt.jpg)'
- en: Generally speaking, the performance penalty for making two round trips to Redis
    in order to post a status message isn’t very much—twice the latency of one round
    trip. But if we can reduce these two round trips to one, we may also be able to
    reduce the number of round trips for other examples where we make many round trips.
    Fewer round trips means lower latency for a given group of commands. Lower latency
    means less waiting, fewer required web servers, and higher performance for the
    entire system overall.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 一般而言，为了发布状态消息而进行两次往返Redis的性能惩罚并不大——是单次往返延迟的两倍。但如果我们能将这两次往返减少到一次，我们也许还能减少其他许多例子中的往返次数。往返次数越少，给定命令组的延迟就越低。较低的延迟意味着等待时间更少，所需的Web服务器更少，整个系统的整体性能更高。
- en: 'To review what happens when we post a new status message: we look up the user’s
    name in a `HASH`, increment a counter (to get a new ID), add data to a Redis `HASH`,
    and increment a counter in the user’s `HASH`. That doesn’t sound so bad in Lua;
    let’s give it a shot in the next listing, which shows the Lua script, with a Python
    wrapper that implements the same API as before.'
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾当我们发布一条新的状态消息时会发生什么：我们在`HASH`中查找用户的姓名，增加一个计数器（以获取一个新的ID），将数据添加到Redis的`HASH`中，并在用户的`HASH`中增加一个计数器。在Lua中这听起来并不糟糕；让我们在下一个列表中尝试一下，其中显示了Lua脚本，以及一个实现与之前相同的API的Python包装器。
- en: Listing 11.3\. Creating a status message with Lua
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.3\. 使用Lua创建状态消息
- en: '![](254fig01_alt.jpg)'
  id: totrans-417
  prefs: []
  type: TYPE_IMG
  zh: '![](254fig01_alt.jpg)'
- en: This function performs all of the same operations that the previous all-Python
    version performed, only instead of needing two round trips to Redis with every
    call, it should only need one (the first call will load the script into Redis
    and then call the loaded script, but subsequent calls will only require one).
    This isn’t a big issue for posting status messages, but for many other problems
    that we’ve gone through in previous chapters, making multiple round trips can
    take more time than is necessary, or lend to `WATCH`/`MULTI`/`EXEC` contention.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数执行了之前所有-Python版本执行的所有相同操作，只是每次调用不需要两次往返Redis，而只需要一次（第一次调用将脚本加载到Redis中，然后调用加载的脚本，但后续调用只需要一次）。这对发布状态消息来说不是一个大问题，但对于我们在前几章中遇到的大多数其他问题，多次往返可能会比必要的花费更多时间，或者导致`WATCH`/`MULTI`/`EXEC`竞争。
- en: '|  |'
  id: totrans-419
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Writing keys that aren’t a part of the `KEYS` argument to the script
  id: totrans-420
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将不是脚本`KEYS`参数一部分的键写入
- en: In the note in [section 11.1.1](#ch11lev2sec1), I mentioned that we should pass
    all keys to be modified as part of the keys argument of the script, yet here we’re
    writing a `HASH` based on a key that wasn’t passed. Doing this makes this Lua
    script incompatible with the future Redis cluster. Whether this operation is still
    correct in a noncluster sharded server scenario will depend on your sharding methods.
    I did this to highlight that you may need to do this kind of thing, but doing
    so prevents you from being able to use Redis cluster.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第11.1.1节](#ch11lev2sec1)的注释中，我提到我们应该将所有要修改的键作为脚本的关键字参数传递，但在这里我们正在根据未传递的键编写一个`HASH`。这样做使得这个Lua脚本与未来的Redis集群不兼容。这个操作在非集群分片服务器场景中是否仍然正确将取决于你的分片方法。我这样做是为了强调你可能需要这样做，但这样做会阻止你使用Redis集群。
- en: '|  |'
  id: totrans-422
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-423
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Script loaders and helpers
  id: totrans-424
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 脚本加载器和辅助工具
- en: You’ll notice in this first example that we have two major pieces. We have a
    Python function that handles the translation of the earlier API into the Lua script
    call, and we have the Lua script that’s loaded from our earlier `script_load()`
    function. We’ll continue this pattern for the remainder of the chapter, since
    the native API for Lua scripting (`KEYS` and `ARGV`) can be difficult to call
    in multiple contexts.
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到在这个第一个例子中，我们有两个主要部分。我们有一个Python函数，它处理将早期的API翻译成Lua脚本调用的转换，还有一个从我们之前的`script_load()`函数加载的Lua脚本。我们将继续这种模式，因为Lua脚本的本地API（`KEYS`和`ARGV`）在多个上下文中调用可能很困难。
- en: '|  |'
  id: totrans-426
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Since Redis 2.6 has been completed and released, libraries that support Redis
    scripting with Lua in the major languages should get better and more complete.
    On the Python side of things, a script loader similar to what we’ve written is
    already available in the source code repository for the redis-py project, and
    is currently available from the Python Package Index. We use our script loader
    due to its flexibility and ease of use when confronted with sharded network connections.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 自从 Redis 2.6 完成并发布以来，支持 Lua 脚本的主要语言库应该会变得更好、更完整。在 Python 方面，一个类似于我们编写的脚本加载器已经在
    redis-py 项目的源代码库中可用，并且目前可以从 Python 包索引中获取。我们使用我们的脚本加载器，因为它在面对分片网络连接时的灵活性和易用性。
- en: As our volume of interactions with Redis increased over time, we switched to
    using locks and semaphores to help reduce contention issues surrounding `WATCH`/`MULTI`/`EXEC`
    transactions. Let’s take a look at rewriting locks and semaphores to see if we
    might be able to further improve performance.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 随着我们与 Redis 交互量的增加，我们转向使用锁和信号量来帮助减少 `WATCH`/`MULTI`/`EXEC` 事务周围的竞争问题。让我们来看看重写锁和信号量，看看我们是否能够进一步提高性能。
- en: 11.2\. Rewriting locks and semaphores with Lua
  id: totrans-429
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.2\. 使用 Lua 重写锁和信号量
- en: When I introduced locks and semaphores in [chapter 6](kindle_split_017.html#ch06),
    I showed how locks can reduce contention compared to `WATCH`/`MULTI`/`EXEC` transactions
    by being pessimistic in heavy traffic scenarios. But locks themselves require
    two to three round trips to acquire or release a lock in the best case, and can
    suffer from contention in some situations.
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在[第 6 章](kindle_split_017.html#ch06)中介绍锁和信号量时，我展示了锁如何通过在流量高峰场景中采取悲观策略来减少与
    `WATCH`/`MULTI`/`EXEC` 事务的竞争。但是，锁本身在最佳情况下需要两到三个往返来获取或释放锁，并且在某些情况下可能会遇到竞争。
- en: In this section, we’ll revisit our lock from [section 6.2](kindle_split_017.html#ch06lev1sec2)
    and rewrite it in Lua in order to further improve performance. We’ll then revisit
    our semaphore example from [section 6.3](kindle_split_017.html#ch06lev1sec3) to
    implement a completely fair lock while also improving performance there.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视[第 6.2 节](kindle_split_017.html#ch06lev1sec2)中的锁，并使用 Lua 重新编写它，以进一步提高性能。然后，我们将重新审视[第
    6.3 节](kindle_split_017.html#ch06lev1sec3)中的信号量示例，以实现一个完全公平的锁，同时在那里提高性能。
- en: Let’s first take a look at locks with Lua, and why we’d want to continue using
    locks at all.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先看看 Lua 中的锁，以及为什么我们还想继续使用锁。
- en: 11.2.1\. Why locks in Lua?
  id: totrans-433
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.1\. 为什么在 Lua 中使用锁？
- en: Let’s first deal with the question of *why* we would decide to build a lock
    with Lua. There are two major reasons.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先来讨论一下我们决定使用 Lua 构建锁的*原因*。主要有两个原因。
- en: Technically speaking, when executing a Lua script with `EVAL` or `EVALSHA`,
    the first group of arguments after the script or hash is the keys that will be
    read or written within Lua (I mentioned this in two notes in [sections 11.1.1](#ch11lev2sec1)
    and [11.1.2](#ch11lev2sec2)). This is primarily to allow for later Redis cluster
    servers to reject scripts that read or write keys that aren’t available on a particular
    shard. If we don’t know what keys will be read/written in advance, we shouldn’t
    be using Lua (we should instead use `WATCH`/`MULTI`/`EXEC` or locks). As such,
    any time we’re reading or writing keys that weren’t provided as part of the `KEYS`
    argument to the script, we risk potential incompatibility or breakage if we transition
    to a Redis cluster later.
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来说，当使用 `EVAL` 或 `EVALSHA` 执行 Lua 脚本时，脚本或哈希之后的第一组参数是将在 Lua 中读取或写入的键（我在[章节
    11.1.1](#ch11lev2sec1)和[11.1.2](#ch11lev2sec2)中的两个笔记中提到过）。这主要是为了允许后续的 Redis 集群服务器拒绝读取或写入特定分片上不可用的键的脚本。如果我们事先不知道将要读取/写入哪些键，那么我们不应该使用
    Lua（我们应该改用 `WATCH`/`MULTI`/`EXEC` 或锁）。因此，每次我们读取或写入作为脚本 `KEYS` 参数一部分之外的键时，如果我们后来过渡到
    Redis 集群，我们都会面临潜在的不兼容性或损坏的风险。
- en: The second reason is because there are situations where manipulating data in
    Redis requires data that’s not available at the time of the initial call. One
    example would be fetching some `HASH` values from Redis, and then using those
    values to access information from a relational database, which then results in
    a write back to Redis. We saw this first when we were scheduling the caching of
    rows in Redis back in [section 2.4](kindle_split_012.html#ch02lev1sec4). We didn’t
    bother locking in that situation because writing two copies of the same row twice
    wouldn’t have been a serious issue. But in other caching scenarios, reading the
    data to be cached multiple times can be more overhead than is acceptable, or could
    even cause newer data to be overwritten by older data.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个原因是，在某些情况下，在Redis中操作数据需要的数据在初始调用时不可用。一个例子是从Redis获取一些`HASH`值，然后使用这些值从关系型数据库中获取信息，这然后导致将数据写回Redis。我们第一次看到这种情况是在[第2.4节](kindle_split_012.html#ch02lev1sec4)中安排在Redis中缓存行时。在那个情况下，我们没有麻烦去锁定，因为两次写入相同的行两次不会是一个严重的问题。但在其他缓存场景中，多次读取要缓存的数据可能会比可接受的开销更大，甚至可能导致较新的数据被较旧的数据覆盖。
- en: Given these two reasons, let’s rewrite our lock to use Lua.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这两个原因，让我们重写我们的锁以使用Lua。
- en: 11.2.2\. Rewriting our lock
  id: totrans-438
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.2. 重写我们的锁
- en: As you may remember from [section 6.2](kindle_split_017.html#ch06lev1sec2),
    locking involved generating an ID, conditionally setting a key with `SETNX`, and
    upon success setting the expiration time of the key. Though conceptually simple,
    we had to deal with failures and retries, which resulted in the original code
    shown in the next listing.
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 如你从[第6.2节](kindle_split_017.html#ch06lev1sec2)中可能记得的那样，锁定涉及生成一个ID，条件性地使用`SETNX`设置一个键，并在成功后设置键的过期时间。尽管在概念上很简单，但我们不得不处理失败和重试，这导致了下一列表中显示的原始代码。
- en: Listing 11.4\. Our final `acquire_lock_with_timeout()` function from [section
    6.2.5](kindle_split_017.html#ch06lev2sec7)
  id: totrans-440
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.4. 来自[第6.2.5节](kindle_split_017.html#ch06lev2sec7)的我们的最终`acquire_lock_with_timeout()`函数
- en: '![](256fig01_alt.jpg)'
  id: totrans-441
  prefs: []
  type: TYPE_IMG
  zh: '![图片](256fig01_alt.jpg)'
- en: There’s nothing too surprising here if you remember how we built up to this
    lock in [section 6.2](kindle_split_017.html#ch06lev1sec2). Let’s go ahead and
    offer the same functionality, but move the core locking into Lua.
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你记得我们在[第6.2节](kindle_split_017.html#ch06lev1sec2)中是如何构建这个锁的，那么这里就没有什么太令人惊讶的内容。让我们继续提供相同的功能，但将核心锁定移动到Lua中。
- en: Listing 11.5\. A rewritten `acquire_lock_with_timeout()` that uses Lua
  id: totrans-443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.5. 使用Lua重写的`acquire_lock_with_timeout()`
- en: '![](256fig02_alt.jpg)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![图片](256fig02_alt.jpg)'
- en: There aren’t any significant changes in the code, except that we change the
    commands we use so that if a lock is acquired, it always has a timeout. Let’s
    also go ahead and rewrite the release lock code to use Lua.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 代码中没有任何显著的变化，除了我们更改了使用的命令，使得如果获取了锁，它总是有一个超时时间。我们还可以继续重写释放锁的代码以使用Lua。
- en: Previously, we watched the lock key, and then verified that the lock still had
    the same value. If it had the same value, we removed the lock; otherwise we’d
    say that the lock was lost. Our Lua version of `release_lock()` is shown next.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们监视锁键，然后验证锁是否仍然具有相同的值。如果它具有相同的值，我们就移除锁；否则，我们会说锁已经丢失。我们下面展示了`release_lock()`的Lua版本。
- en: Listing 11.6\. A rewritten `release_lock()` that uses Lua
  id: totrans-447
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.6. 使用Lua重写的`release_lock()`
- en: '![](257fig01_alt.jpg)'
  id: totrans-448
  prefs: []
  type: TYPE_IMG
  zh: '![图片](257fig01_alt.jpg)'
- en: Unlike acquiring the lock, releasing the lock became shorter as we no longer
    needed to perform all of the typical `WATCH`/`MULTI`/`EXEC` steps.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 与获取锁不同，由于我们不再需要执行所有典型的`WATCH`/`MULTI`/`EXEC`步骤，释放锁变得简短了。
- en: Reducing the code is great, but we haven’t gotten far if we haven’t actually
    improved the performance of the lock itself. We’ve added some instrumentation
    to the locking code along with some benchmarking code that executes 1, 2, 5, and
    10 parallel processes to acquire and release locks repeatedly. We count the number
    of attempts to acquire the lock and how many times the lock was acquired over
    10 seconds, with both our original and Lua-based acquire and release lock functions.
    [Table 11.2](#ch11table02) shows the number of calls that were performed and succeeded.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 减少代码很好，但如果我们没有真正提高锁本身的性能，那么我们就没有走得太远。我们在锁定代码中添加了一些仪表，以及一些基准测试代码，该代码执行1、2、5和10个并行进程来重复获取和释放锁。我们计算了获取锁的尝试次数以及10秒内锁被获取的次数，包括我们的原始和基于Lua的获取和释放锁函数。[表11.2](#ch11table02)显示了执行并成功的调用次数。
- en: Table 11.2\. Performance of our original lock against a Lua-based lock over
    10 seconds
  id: totrans-451
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表11.2. 在10秒内，我们的原始锁与基于Lua的锁的性能比较
- en: '| Benchmark configuration | Tries in 10 seconds | Acquires in 10 seconds |'
  id: totrans-452
  prefs: []
  type: TYPE_TB
  zh: '| 基准配置 | 10秒内尝试次数 | 10秒内获取次数 |'
- en: '| --- | --- | --- |'
  id: totrans-453
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Original lock, 1 client | 31,359 | 31,359 |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '| 原始锁，1个客户端 | 31,359 | 31,359 |'
- en: '| Original lock, 2 clients | 30,085 | 22,507 |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 原始锁，2个客户端 | 30,085 | 22,507 |'
- en: '| Original lock, 5 clients | 47,694 | 19,695 |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| 原始锁，5个客户端 | 47,694 | 19,695 |'
- en: '| Original lock, 10 clients | 71,917 | 14,361 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| 原始锁，10个客户端 | 71,917 | 14,361 |'
- en: '| Lua lock, 1 client | 44,494 | 44,494 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| Lua 锁，1个客户端 | 44,494 | 44,494 |'
- en: '| Lua lock, 2 clients | 50,404 | 42,199 |'
  id: totrans-459
  prefs: []
  type: TYPE_TB
  zh: '| Lua 锁，2个客户端 | 50,404 | 42,199 |'
- en: '| Lua lock, 5 clients | 70,807 | 40,826 |'
  id: totrans-460
  prefs: []
  type: TYPE_TB
  zh: '| Lua 锁，5个客户端 | 70,807 | 40,826 |'
- en: '| Lua lock, 10 clients | 96,871 | 33,990 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| Lua 锁，10个客户端 | 96,871 | 33,990 |'
- en: Looking at the data from our benchmark (pay attention to the right column),
    one thing to note is that Lua-based locks succeed in acquiring and releasing the
    lock in cycles significantly more often than our previous lock—by more than 40%
    with a single client, 87% with 2 clients, and over 100% with 5 or 10 clients attempting
    to acquire and release the same locks. Comparing the middle and right columns,
    we can also see how much faster attempts at locking are made with Lua, primarily
    due to the reduced number of round trips.
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们的基准测试数据（请注意右列），有一点需要注意，那就是基于 Lua 的锁在获取和释放锁的循环中比我们之前的锁成功得更多——单个客户端超过 40%，2
    个客户端达到 87%，5 或 10 个客户端尝试获取和释放相同锁时超过 100%。比较中间和右列，我们还可以看到 Lua 的锁定尝试速度有多快，这主要归因于往返次数的减少。
- en: But even better than performance improvements, our code to acquire and release
    the locks is significantly easier to understand and verify as correct.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 但比性能改进更好的是，我们获取和释放锁的代码更容易理解并验证其正确性。
- en: Another example where we built a synchronization primitive is with semaphores;
    let’s take a look at building them next.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们构建同步原语（例如信号量）的例子；让我们看看如何构建它们。
- en: 11.2.3\. Counting semaphores in Lua
  id: totrans-465
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.2.3\. Lua 中的计数信号量
- en: As we worked through our counting semaphores in [chapter 6](kindle_split_017.html#ch06),
    we spent a lot of time trying to ensure that our semaphores had some level of
    fairness. In the process, we used a counter to create a sort of numeric identifier
    for the client, which was then used to determine whether the client had been successful.
    But because we still had race conditions when acquiring the semaphore, we ultimately
    still needed to use a lock for the semaphore to function correctly.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们处理第 6 章中的计数信号量时 [kindle_split_017.html#ch06](kindle_split_017.html#ch06)，我们花费了大量时间来确保我们的信号量具有一定的公平性。在这个过程中，我们使用计数器为客户端创建了一种数字标识符，然后使用它来确定客户端是否成功。但由于我们在获取信号量时仍然存在竞态条件，我们最终仍然需要使用锁来使信号量正确地工作。
- en: Let’s look at our earlier implementation of the counting semaphore and think
    about what it may take to improve it using Lua.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看我们之前实现的计数信号量，并思考如何使用 Lua 来改进它。
- en: Listing 11.7\. The `acquire_semaphore()` function from [section 6.3.2](kindle_split_017.html#ch06lev2sec9)
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.7\. 来自 [第 6.3.2 节](kindle_split_017.html#ch06lev2sec9) 的 `acquire_semaphore()`
    函数
- en: '![](258fig01_alt.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![图片](258fig01_alt.jpg)'
- en: In the process of translating this function into Lua, after cleaning out timed-out
    semaphores, it becomes possible to know whether a semaphore is available to acquire,
    so we can simplify our code in the case where a semaphore isn’t available. Also,
    because everything is occurring inside Redis, we don’t need the counter or the
    owner `ZSET`, since the first client to execute their Lua function should be the
    one to get the semaphore. The Lua version of `acquire_semaphore()` can be seen
    in the next listing.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 在将此函数翻译成 Lua 的过程中，在清理超时的信号量后，我们可以知道信号量是否可用以获取，因此我们可以简化在信号量不可用的情况下的代码。此外，由于所有操作都在
    Redis 内部进行，我们不需要计数器或所有者 `ZSET`，因为第一个执行 Lua 函数的客户端应该获得信号量。`acquire_semaphore()`
    的 Lua 版本可以在下一个列表中看到。
- en: Listing 11.8\. The `acquire_semaphore()` function rewritten with Lua
  id: totrans-471
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.8\. 使用 Lua 重写的 `acquire_semaphore()` 函数
- en: '![](259fig01_alt.jpg)'
  id: totrans-472
  prefs: []
  type: TYPE_IMG
  zh: '![图片](259fig01_alt.jpg)'
- en: This updated semaphore offers the same capabilities of the lock-wrapped `acquire_fair_semaphore_with_lock()`,
    including being completely fair. Further, because of the simplifications we’ve
    performed (no locks, no `ZINTERSTORE`, and no `ZREMRANGEBYRANK`), our new semaphore
    will operate significantly faster than the previous semaphore implementation,
    while at the same time reducing the complexity of the semaphore itself.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 这个更新的信号量提供了与`acquire_fair_semaphore_with_lock()`加锁版本相同的特性，包括完全公平。此外，由于我们进行的简化（没有锁，没有`ZINTERSTORE`，没有`ZREMRANGEBYRANK`），我们新的信号量将比之前的信号量实现运行得更快，同时降低信号量的复杂性。
- en: Due to our simplification, releasing the semaphore can be done using the original
    `release_semaphore()` code from [section 6.3.1](kindle_split_017.html#ch06lev2sec8).
    We only need to create a Lua-based refresh semaphore function to replace the fair
    semaphore version from [section 6.3.3](kindle_split_017.html#ch06lev2sec10), shown
    next.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们的简化，释放信号量可以使用来自[第6.3.1节](kindle_split_017.html#ch06lev2sec8)的原始`release_semaphore()`代码。我们只需要创建一个基于Lua的刷新信号量函数来替换来自[第6.3.3节](kindle_split_017.html#ch06lev2sec10)的公平信号量版本，如下所示。
- en: Listing 11.9\. A `refresh_semaphore()` function written with Lua
  id: totrans-475
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.9. 使用Lua编写的`refresh_semaphore()`函数
- en: '![](259fig02_alt.jpg)'
  id: totrans-476
  prefs: []
  type: TYPE_IMG
  zh: '![](259fig02_alt.jpg)'
- en: With acquire and refresh semaphore rewritten with Lua, we now have a completely
    fair semaphore that’s faster and easier to understand.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 通过用Lua重写获取和刷新信号量，我们现在有一个完全公平的信号量，它更快，更容易理解。
- en: Now that we’ve rewritten locks and semaphores in Lua and have seen a bit of
    what they can do to improve performance, let’s try to remove `WATCH`/`MULTI`/`EXEC`
    transactions and locks from two of our previous examples to see how well we can
    make them perform.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用Lua重写了锁和信号量，并看到了它们如何提高性能，让我们尝试从我们之前的两个示例中去除`WATCH`/`MULTI`/`EXEC`事务和锁，看看我们如何使它们表现良好。
- en: 11.3\. Doing away with WATCH/MULTI/EXEC
  id: totrans-479
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.3. 去除WATCH/MULTI/EXEC
- en: In previous chapters, we used a combination of `WATCH`, `MULTI`, and `EXEC`
    in several cases to implement a form of transaction in Redis. Generally, when
    there are few writers modifying `WATCH`ed data, these transactions complete without
    significant contention or retries. But if operations can take several round trips
    to execute, if contention is high, or if network latency is high, clients may
    need to perform many retries in order to complete operations.
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们使用`WATCH`、`MULTI`和`EXEC`的组合在几个案例中实现Redis中的事务形式。一般来说，当有少量写操作修改被`WATCH`的数据时，这些事务在没有显著争用或重试的情况下完成。但如果操作需要多次往返才能执行，如果争用很高，或者网络延迟很高，客户端可能需要多次重试才能完成操作。
- en: In this section, we’ll revisit our autocomplete example from [chapter 6](kindle_split_017.html#ch06)
    along with our marketplace example originally covered in [chapter 4](kindle_split_015.html#ch04)
    to show how we can simplify our code and improve performance at the same time.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将重新审视[第6章](kindle_split_017.html#ch06)中的自动完成示例，以及最初在第4章中涵盖的市场示例，以展示我们如何简化代码并提高性能。
- en: First up, let’s look at one of our autocomplete examples from [chapter 6](kindle_split_017.html#ch06).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看[第6章](kindle_split_017.html#ch06)中的一个自动完成示例。
- en: 11.3.1\. Revisiting group autocomplete
  id: totrans-483
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.1. 重新审视组自动完成
- en: Back in [chapter 6](kindle_split_017.html#ch06), we introduced an autocomplete
    procedure that used a `ZSET` to store user names to be autocompleted on.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_017.html#ch06)中，我们介绍了一个使用`ZSET`存储要自动完成的用户名的自动完成过程。
- en: As you may remember, we calculated a pair of strings that would surround all
    of the values that we wanted to autocomplete on. When we had those values, we’d
    insert our data into the `ZSET`, and then `WATCH` the `ZSET` for anyone else making
    similar changes. We’d then fetch 10 items between the two endpoints and remove
    them between a `MULTI`/`EXEC` pair. Let’s take a quick look at the code that we
    used.
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所记，我们计算了一对字符串，将围绕我们想要自动完成的全部值。当我们有了这些值，我们将数据插入到`ZSET`中，然后`WATCH``ZSET`以观察其他人是否进行类似更改。然后我们在两个端点之间获取10个项目，并在`MULTI`/`EXEC`对之间删除它们。让我们快速看一下我们使用的代码。
- en: Listing 11.10\. Our autocomplete code from [section 6.1.2](kindle_split_017.html#ch06lev2sec2)
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.10. [第6.1.2节](kindle_split_017.html#ch06lev2sec2)中的我们的自动完成代码
- en: '![](260fig01_alt.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![](260fig01_alt.jpg)'
- en: If few autocomplete operations are being performed at one time, this shouldn’t
    cause many retries. But regardless of retries, we still have a lot of code related
    to handling hopefully rare retries—roughly 40%, depending on how we count lines.
    Let’s get rid of all of that retry code, and move the core functionality of this
    function into a Lua script. The result of this change is shown in the next listing.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一次只执行少量自动完成操作，这不应该导致很多重试。但无论重试与否，我们仍然有很多与处理可能很少发生的重试相关的代码——大约40%，具体取决于我们如何计算行数。让我们删除所有这些重试代码，并将此函数的核心功能移动到
    Lua 脚本中。这种变化的成果在下一列表中展示。
- en: Listing 11.11\. Autocomplete on prefix using Redis scripting
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.11\. 使用 Redis 脚本在词缀上进行自动完成
- en: '![](261fig01_alt.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![](261fig01_alt.jpg)'
- en: The body of the Lua script should be somewhat familiar; it’s a direct translation
    of the [chapter 6](kindle_split_017.html#ch06) code. Not only is the resulting
    code significantly shorter, it also executes much faster. Using a similar benchmarking
    method to what we used in [chapter 6](kindle_split_017.html#ch06), we ran 1, 2,
    5, and 10 concurrent processes that performed autocomplete requests against the
    same guild as fast as possible. To keep our chart simple, we only calculated attempts
    to autocomplete and successful autocompletes, over the course of 10 seconds. [Table
    11.3](#ch11table03) shows the results of this test.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: Lua 脚本的主体应该相当熟悉；它是 [第 6 章](kindle_split_017.html#ch06) 代码的直接翻译。不仅结果代码显著更短，而且执行速度也更快。使用与
    [第 6 章](kindle_split_017.html#ch06) 中使用的类似基准测试方法，我们运行了1、2、5和10个并发进程，尽可能快地对相同的公会执行自动完成请求。为了使我们的图表简单，我们只计算了10秒内的自动完成尝试和成功的自动完成次数。[表
    11.3](#ch11table03) 显示了这次测试的结果。
- en: Table 11.3\. Performance of our original autocomplete versus our Lua-based autocomplete
    over 10 seconds
  id: totrans-492
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.3\. 10秒内我们的原始自动完成与基于 Lua 的自动完成的性能比较
- en: '| Benchmark configuration | Tries in 10 seconds | Autocompletes in 10 seconds
    |'
  id: totrans-493
  prefs: []
  type: TYPE_TB
  zh: '| 基准配置 | 10秒内的尝试次数 | 10秒内的自动完成次数 |'
- en: '| --- | --- | --- |'
  id: totrans-494
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Original autocomplete, 1 client | 26,339 | 26,339 |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '| 原始自动完成，1 个客户端 | 26,339 | 26,339 |'
- en: '| Original autocomplete, 2 clients | 35,188 | 17,551 |'
  id: totrans-496
  prefs: []
  type: TYPE_TB
  zh: '| 原始自动完成，2 个客户端 | 35,188 | 17,551 |'
- en: '| Original autocomplete, 5 clients | 59,544 | 10,989 |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '| 原始自动完成，5 个客户端 | 59,544 | 10,989 |'
- en: '| Original autocomplete, 10 clients | 57,305 | 6,141 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 原始自动完成，10 个客户端 | 57,305 | 6,141 |'
- en: '| Lua autocomplete, 1 client | 64,440 | 64,440 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| Lua 自动完成，1 个客户端 | 64,440 | 64,440 |'
- en: '| Lua autocomplete, 2 clients | 89,140 | 89,140 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| Lua 自动完成，2 个客户端 | 89,140 | 89,140 |'
- en: '| Lua autocomplete, 5 clients | 125,971 | 125,971 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| Lua 自动完成，5 个客户端 | 125,971 | 125,971 |'
- en: '| Lua autocomplete, 10 clients | 128,217 | 128,217 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| Lua 自动完成，10 个客户端 | 128,217 | 128,217 |'
- en: Looking at our table, when executing the older autocomplete function that uses
    `WATCH`/`MULTI`/`EXEC` transactions, the probability of finishing a transaction
    is reduced as we add more clients, and the total attempts over 10 seconds hit
    a peak limit. On the other hand, our Lua autocomplete can attempt and finish far
    more times every second, primarily due to the reduced overhead of fewer network
    round trips, as well as not running into any `WATCH` errors due to contention.
    Looking at just the 10-client version of both, the 10-client Lua autocomplete
    is able to complete more than 20 times as many autocomplete operations as the
    original autocomplete.
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 观察我们的表格，当我们执行使用 `WATCH`/`MULTI`/`EXEC` 事务的较老自动完成函数时，随着客户端数量的增加，完成事务的概率降低，并且10秒内的总尝试次数达到峰值限制。另一方面，我们的
    Lua 自动完成可以每秒尝试和完成更多次，这主要归功于更少的网络往返带来的开销减少，以及没有遇到任何由于竞争导致的 `WATCH` 错误。仅观察两者的10客户端版本，10客户端的
    Lua 自动完成能够完成比原始自动完成超过20倍的自动完成操作。
- en: Now that we’ve seen how well we can do on one of our simpler examples, let’s
    look at how we can improve our marketplace.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经看到了在更简单的例子中我们能做得有多好，让我们看看我们如何可以改进我们的市场。
- en: 11.3.2\. Improving the marketplace, again
  id: totrans-505
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.3.2\. 再次改进市场
- en: In [section 6.2](kindle_split_017.html#ch06lev1sec2), we revisited the marketplace
    example we introduced in [section 4.4](kindle_split_015.html#ch04lev1sec4), replacing
    our use of `WATCH`, `MULTI`, and `EXEC` with locks, and showed how using coarse-grained
    and fine-grained locks in Redis can help reduce contention and improve performance.
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 6.2 节](kindle_split_017.html#ch06lev1sec2) 中，我们回顾了我们在 [第 4.4 节](kindle_split_015.html#ch04lev1sec4)
    中引入的市场示例，用锁代替了我们的 `WATCH`、`MULTI` 和 `EXEC` 的使用，并展示了如何在 Redis 中使用粗粒度和细粒度锁来减少竞争并提高性能。
- en: In this section, we’ll again work on the marketplace, further improving performance
    by removing the locks completely and moving our code into a Lua script.
  id: totrans-507
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次处理市场，通过完全移除锁并将我们的代码移入Lua脚本中，进一步提高性能。
- en: Let’s first look at our marketplace code with a lock. As a review of what goes
    on, first the lock is acquired, and then we watch the buyer’s user information
    `HASH` and let the buyer purchase the item if they have enough money. The original
    function from [section 6.2](kindle_split_017.html#ch06lev1sec2) appears next.
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看带有锁的市场代码。作为一个回顾，首先获取锁，然后我们观察买家的用户信息`HASH`，如果买家有足够的钱，就允许买家购买商品。原始函数在[第6.2节](kindle_split_017.html#ch06lev1sec2)中展示。
- en: Listing 11.12\. The purchase item with lock function from [section 6.2](kindle_split_017.html#ch06lev1sec2)
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.12.来自[第6.2节](kindle_split_017.html#ch06lev1sec2)的商品购买带锁函数
- en: '![](ch11ex12-0.jpg)'
  id: totrans-510
  prefs: []
  type: TYPE_IMG
  zh: '![](ch11ex12-0.jpg)'
- en: '![](ch11ex12-1.jpg)'
  id: totrans-511
  prefs: []
  type: TYPE_IMG
  zh: '![](ch11ex12-1.jpg)'
- en: 'Despite using a lock, we still needed to watch the buyer’s user information
    `HASH` to ensure that enough money was available at the time of purchase. And
    because of that, we have the worst of both worlds: chunks of code to handle locking,
    and other chunks of code to handle the potential `WATCH` errors. Of all of the
    solutions we’ve seen so far, this one is a prime candidate for rewriting in Lua.'
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管使用了锁，我们仍然需要监视买家的用户信息`HASH`以确保购买时资金充足。正因为如此，我们有了两方面的最坏情况：处理锁定的代码块，以及处理潜在的`WATCH`错误的代码块。在我们迄今为止看到的所有解决方案中，这个方案是重写为Lua的理想候选。
- en: 'When rewriting this function in Lua, we can do away with the locking, the `WATCH`/`MULTI`/`EXEC`
    transactions, and even timeouts. Our code becomes straightforward and simple:
    make sure the item is available, make sure the buyer has enough money, transfer
    the item to the buyer, and transfer the buyer’s money to the seller. The following
    listing shows the rewritten item-purchasing function.'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 当用Lua重写这个函数时，我们可以去掉锁，`WATCH`/`MULTI`/`EXEC`事务，甚至超时。我们的代码变得简单直接：确保商品可用，确保买家有足够的钱，将商品转给买家，并将买家的钱转给卖家。下面的列表展示了重写的商品购买函数。
- en: Listing 11.13\. The purchase item function rewritten with Lua
  id: totrans-514
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表11.13.用Lua重写的购买商品函数
- en: '![](263fig01_alt.jpg)'
  id: totrans-515
  prefs: []
  type: TYPE_IMG
  zh: '![](263fig01_alt.jpg)'
- en: Just comparing the two code listings, the Lua-based item-purchase code is far
    easier to understand. And without the multiple round trips to complete a single
    purchase (locking, watching, fetching price and available money, then the purchase,
    and then the unlocking), it’s obvious that purchasing an item will be even faster
    than the fine-grained locking that we used in [chapter 6](kindle_split_017.html#ch06).
    But how much faster?
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 仅比较两个代码列表，基于Lua的商品购买代码更容易理解。而且没有多次往返才能完成一次购买（锁定、监视、获取价格和可用资金，然后购买，然后解锁），很明显，购买商品将比我们在[第6章](kindle_split_017.html#ch06)中使用的细粒度锁定更快。但快多少呢？
- en: '|  |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise: Rewrite item listing in Lua**'
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：用Lua重写商品列表**'
- en: 'We rewrote the purchase-item function in Lua for our benchmarks, but can you
    rewrite the original item-listing function from [section 4.4.2](kindle_split_015.html#ch04lev2sec11)
    into Lua? Hint: The source code for this chapter includes the answer to this exercise,
    just as the source code for each of the other chapters includes the solutions
    to almost all of the exercises.'
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为了基准测试重写了Lua中的购买商品函数，但你能否将[第4.4.2节](kindle_split_015.html#ch04lev2sec11)中的原始商品列表函数重写为Lua？提示：本章的源代码包含了这个练习的答案，就像其他章节的源代码包含了几乎所有练习的答案一样。
- en: '|  |'
  id: totrans-520
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: We now have an item-purchase function rewritten in Lua, and if you perform the
    exercise, you’ll also have an item-listing function written in Lua. If you read
    the hint to our exercise, you’ll notice that we also rewrote the item-listing
    function in Lua. You may remember that at the end of [section 6.2.4](kindle_split_017.html#ch06lev2sec6),
    we ran some benchmarks to compare the performance of `WATCH`/`MULTI`/`EXEC` transactions
    against coarse-grained and fine-grained locks. We reran the benchmark for five
    listing and five buying processes using our newly rewritten Lua versions, to produce
    the last row in [table 11.4](#ch11table04).
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经用Lua重写了商品购买函数，如果你做了练习，你也会有一个用Lua编写的商品列表函数。如果你阅读了我们的练习提示，你会注意到我们也用Lua重写了商品列表函数。你可能记得，在[第6.2.4节](kindle_split_017.html#ch062sec6)的结尾，我们进行了一些基准测试，比较了`WATCH`/`MULTI`/`EXEC`事务与粗粒度和细粒度锁的性能。我们重新运行了基准测试，使用我们新重写的Lua版本，对五个列表和五个购买过程进行了测试，以产生[表11.4](#ch11table04)中的最后一行。
- en: Table 11.4\. Performance of Lua compared with no locking, coarse-grained locks,
    and fine-grained locks over 60 seconds
  id: totrans-522
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 11.4\. 与无锁、粗粒度锁和细粒度锁相比，Lua 在 60 秒内的性能
- en: '|   | Listed items | Bought items | Purchase retries | Average wait per purchase
    |'
  id: totrans-523
  prefs: []
  type: TYPE_TB
  zh: '|   | 列出项目 | 购买项目 | 购买重试 | 平均每次购买等待时间 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-524
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 5 listers, 5 buyers, no lock | 206,000 | <600 | 161,000 | 498ms |'
  id: totrans-525
  prefs: []
  type: TYPE_TB
  zh: '| 5 个列表器，5 个买家，无锁 | 206,000 | <600 | 161,000 | 498ms |'
- en: '| 5 listers, 5 buyers, with lock | 21,000 | 20,500 | 0 | 14ms |'
  id: totrans-526
  prefs: []
  type: TYPE_TB
  zh: '| 5 个列表器，5 个买家，带有锁 | 21,000 | 20,500 | 0 | 14ms |'
- en: '| 5 listers, 5 buyers, with fine-grained lock | 116,000 | 111,000 | 0 | <3ms
    |'
  id: totrans-527
  prefs: []
  type: TYPE_TB
  zh: '| 5 个列表器，5 个买家，带有细粒度锁 | 116,000 | 111,000 | 0 | <3ms |'
- en: '| 5 listers, 5 buyers, using Lua | 505,000 | 480,000 | 0 | <1ms |'
  id: totrans-528
  prefs: []
  type: TYPE_TB
  zh: '| 5 个列表器，5 个买家，使用 Lua | 505,000 | 480,000 | 0 | <1ms |'
- en: As in other cases where we’ve moved functionality into Lua, we see a substantial
    performance increase. Numerically, we see an improvement in listing and buying
    performance of more than 4.25 times compared with fine-grained locking, and see
    latencies of under 1 millisecond to execute a purchase (actual latencies were
    consistently around .61 milliseconds). From this table, we can see the performance
    advantages of coarse-grained locks over `WATCH`/`MULTI`/`EXEC`, fine-grained locks
    over coarse-grained locks, and Lua over fine-grained locks. That said, try to
    remember that while Lua can offer extraordinary performance advantages (and substantial
    code simplification in some cases), Lua scripting in Redis is limited to data
    we can access from within Lua and Redis, whereas there are no such limits when
    using locks or `WATCH`/`MULTI`/`EXEC` transactions.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在将功能移入 Lua 的其他情况下所看到的那样，我们看到了显著的性能提升。从数字上看，与细粒度锁相比，我们看到了列表和购买性能提高了 4.25
    倍以上，并且执行购买时的延迟低于 1 毫秒（实际延迟始终在 .61 毫秒左右）。从这张表中，我们可以看到粗粒度锁相对于 `WATCH`/`MULTI`/`EXEC`
    的性能优势，细粒度锁相对于粗粒度锁的优势，以及 Lua 相对于细粒度锁的优势。尽管如此，请记住，虽然 Lua 可以提供非凡的性能优势（在某些情况下还可以大幅简化代码），但
    Redis 中的 Lua 脚本仅限于我们可以从 Lua 和 Redis 内部访问的数据，而使用锁或 `WATCH`/`MULTI`/`EXEC` 事务时则没有这样的限制。
- en: Now that you’ve seen some of the amazing performance advantages that are available
    with Lua, let’s look at an example where we can save memory with Lua.
  id: totrans-530
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经看到了 Lua 可提供的惊人性能优势，让我们看看一个示例，我们可以通过 Lua 节省内存。
- en: 11.4\. Sharding LISTs with Lua
  id: totrans-531
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.4\. 使用 Lua 分片 `LIST`
- en: Back in [sections 9.2](kindle_split_021.html#ch09lev1sec2) and [9.3](kindle_split_021.html#ch09lev1sec3),
    we sharded `HASH`es, `SET`s, and even `STRING`s as a way of reducing memory. In
    [section 10.3](kindle_split_022.html#ch10lev1sec3), we sharded `ZSET`s to allow
    for search indexes to grow beyond one machine’s memory and to improve performance.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [第 9.2 节](kindle_split_021.html#ch09lev1sec2) 和 [第 9.3 节](kindle_split_021.html#ch09lev1sec3)
    中，我们将 `HASH`、`SET` 和甚至 `STRING` 分片，作为减少内存的一种方式。在 [第 10.3 节](kindle_split_022.html#ch10lev1sec3)
    中，我们将 `ZSET` 分片，以便允许搜索索引超过一台机器的内存，并提高性能。
- en: As promised in [section 9.2](kindle_split_021.html#ch09lev1sec2), in this section
    we’ll create a sharded `LIST` in order to reduce memory use for long `LIST`s.
    We’ll support pushing to both ends of the `LIST`, and we’ll support blocking and
    nonblocking pops from both ends of the list.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 如 [第 9.2 节](kindle_split_021.html#ch09lev1sec2) 所承诺，在本节中，我们将创建一个分片 `LIST` 以减少长
    `LIST` 的内存使用。我们将支持向 `LIST` 的两端推送，并支持从列表两端进行阻塞和非阻塞弹出。
- en: Before we get started on actually implementing these features, we’ll look at
    how to structure the data itself.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际实现这些功能之前，我们将看看如何结构化数据本身。
- en: 11.4.1\. Structuring a sharded LIST
  id: totrans-535
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.1\. 结构化分片 `LIST`
- en: In order to store a sharded `LIST` in a way that allows for pushing and popping
    from both ends, we need the IDs for the first and last shard, as well as the `LIST`
    shards themselves.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 为了以允许从两端推送和弹出数据的方式存储分片 `LIST`，我们需要第一个和最后一个分片的 ID，以及 `LIST` 分片本身。
- en: To store information about the first and last shards, we’ll keep two numbers
    stored as standard Redis strings. These keys will be named `<listname>:first`
    and `<listname>:last`. Any time the sharded `LIST` is empty, both of these numbers
    will be the same. [Figure 11.1](#ch11fig01) shows the first and last shard IDs.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 为了存储关于第一个和最后一个分片的信息，我们将保留两个数字，作为标准 Redis 字符串存储。这些键将命名为 `<listname>:first` 和
    `<listname>:last`。每当分片 `LIST` 为空时，这两个数字都将相同。[图 11.1](#ch11fig01) 显示了第一个和最后一个分片的
    ID。
- en: Figure 11.1\. First and last shard IDs for sharded `LIST`s
  id: totrans-538
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.1\. 分片 `LIST` 的第一个和最后一个分片 ID
- en: '![](11fig01_alt.jpg)'
  id: totrans-539
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig01_alt.jpg)'
- en: Additionally, each shard will be named `<listname>:<shardid>`, and shards will
    be assigned sequentially. More specifically, if items are popped from the left,
    then as items are pushed onto the right, the last shard index will increase, and
    more shards with higher shard IDs will be used. Similarly, if items are popped
    from the right, then as items are pushed onto the left, the first shard index
    will decrease, and more shards with lower shard IDs will be used. [Figure 11.2](#ch11fig02)
    shows some example shards as part of the same sharded `LIST`.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，每个分片将被命名为 `<listname>:<shardid>`，并且分片将按顺序分配。更具体地说，如果从左侧弹出项目，那么当项目被推到右侧时，最后一个分片索引将增加，并将使用更多具有更高分片
    ID 的分片。同样，如果从右侧弹出项目，那么当项目被推到左侧时，第一个分片索引将减少，并将使用更多具有较低分片 ID 的分片。[图 11.2](#ch11fig02)
    展示了作为同一分片 `LIST` 部分的一些示例分片。
- en: Figure 11.2\. `LIST` shards with data
  id: totrans-541
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 11.2\. `LIST` 分片中的数据
- en: '![](11fig02.jpg)'
  id: totrans-542
  prefs: []
  type: TYPE_IMG
  zh: '![图片](11fig02.jpg)'
- en: The structures that we’ll use for sharded `LIST`s shouldn’t seem strange. The
    only interesting thing that we’re doing is splitting a single `LIST` into multiple
    pieces and keeping track of the IDs of the first and last shards. But actually
    implementing our operations? That’s where things get interesting.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将用于分片 `LIST` 的结构不应显得奇怪。我们唯一有趣的事情是将单个 `LIST` 分割成多个部分并跟踪第一个和最后一个分片的 ID。但实际上实现我们的操作？这就是事情变得有趣的地方。
- en: 11.4.2\. Pushing items onto the sharded LIST
  id: totrans-544
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.2\. 将项目推入分片 `LIST`
- en: It turns out that one of the simplest operations that we’ll perform will be
    pushing items onto either end of the sharded `LIST`. Because of some small semantic
    changes to the way that blocking pop operations work in Redis 2.6, we have to
    do some work to ensure that we don’t accidentally overflow a shard. I’ll explain
    when we talk more about the code.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，我们将执行的最简单的操作之一将是将项目推入分片 `LIST` 的两端。由于 Redis 2.6 中阻塞弹出操作方式的一些小的语义变化，我们必须做一些工作以确保我们不会意外地溢出一个分片。当我们更详细地讨论代码时，我会解释。
- en: In order to push items onto either end of a sharded `LIST`, we must first prepare
    the data for sending by breaking it up into chunks. This is because if we’re sending
    to a sharded `LIST`, we may know the total capacity, but we won’t know if any
    clients are waiting on a blocking pop from that LIST,^([[1](#ch00fn01)]) so we
    may need to take multiple passes for large `LIST` pushes.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将项目推入分片 `LIST` 的两端，我们必须首先通过将其分成块来准备发送的数据。这是因为如果我们向分片 `LIST` 发送，我们可能知道总容量，但我们不知道是否有任何客户端正在等待该
    `LIST` 的阻塞弹出，^([[1](#ch00fn01)]) 因此我们可能需要为大型 `LIST` 推送进行多次遍历。
- en: ¹ In earlier versions of Redis, pushing to a `LIST` with blocking clients waiting
    on them would cause the item to be pushed immediately, and subsequent calls to
    `LLEN` would tell the length of the `LIST` after those items had been sent to
    the blocking clients. In Redis 2.6, this is no longer the case—the blocking pops
    are handled after the current command has completed. In this context, that means
    that blocking pops are handled after the current Lua call has finished.
  id: totrans-547
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹ 在 Redis 的早期版本中，向带有阻塞客户端等待的 `LIST` 推送项目会导致项目立即被推送，并且随后的 `LLEN` 调用会在将项目发送到阻塞客户端之后报告
    `LIST` 的长度。在 Redis 2.6 中，这种情况不再存在——阻塞弹出在当前命令完成后处理。在这种情况下，这意味着阻塞弹出在当前 Lua 调用完成后处理。
- en: After we’ve prepared our data, we pass it on to the underlying Lua script. And
    in Lua, we only need to find the first/last shards, and then push the item(s)
    onto that `LIST` until it’s full, returning the number of items that were pushed.
    The Python and Lua code to push items onto either end of a sharded `LIST` is shown
    in the following listing.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们准备我们的数据之后，我们将其传递给底层的 Lua 脚本。在 Lua 中，我们只需要找到第一个/最后一个分片，然后将项目推入该 `LIST` 直到它满，返回推入的项目数量。以下列表显示了将项目推入分片
    `LIST` 两端的 Python 和 Lua 代码。
- en: Listing 11.14\. Functions for pushing items onto a sharded `LIST`
  id: totrans-549
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.14\. 将项目推入分片 `LIST` 的函数
- en: '![](ch11ex14-0.jpg)'
  id: totrans-550
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch11ex14-0.jpg)'
- en: '![](ch11ex14-1.jpg)'
  id: totrans-551
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch11ex14-1.jpg)'
- en: As I mentioned before, because we don’t know about whether clients are blocking
    on pops, we can’t push all items in a single call, so we choose a modestly sized
    block of 64 at a time, though you should feel free to adjust it up or down depending
    on the size of your maximum ziplist-encoded `LIST` configuration.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 如我之前所述，因为我们不知道客户端是否正在阻塞弹出，所以我们不能在一次调用中推送所有项目，所以我们选择每次推送 64 个项目，尽管你可以根据你最大的 ziplist
    编码 `LIST` 配置的大小自由调整它。
- en: '|  |'
  id: totrans-553
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Limitations to this sharded `LIST`
  id: totrans-554
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 分片 `LIST` 的限制
- en: Earlier in this chapter, I mentioned that in order to properly check keys for
    sharded databases (like in the future Redis cluster), we’re supposed to pass all
    of the keys that will be modified as part of the `KEYS` argument to the Redis
    script. But since the shards we’re supposed to write to aren’t necessarily known
    in advance, we can’t do that here. As a result, this sharded `LIST` is only able
    to be contained on a single actual Redis server, not sharded onto multiple servers.
  id: totrans-555
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我提到，为了正确检查分片数据库（如未来的 Redis 集群）中的键，我们应该将所有将要修改的键作为 `KEYS` 参数传递给 Redis 脚本。但由于我们打算写入的分片不一定事先知道，所以我们不能这样做。因此，这个分片
    `LIST` 只能包含在一个实际的 Redis 服务器上，不能分片到多个服务器。
- en: '|  |'
  id: totrans-556
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'You’ll notice that for our sharded push, we may loop in order to go to another
    shard when the first is full. Because no other commands can execute while the
    script is executing, the loop should take at most two passes: one to notice that
    the initial shard is full, the second to push items onto the empty shard.'
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 你会注意到，对于我们的分片推送，当第一个分片满了之后，我们可能需要循环以访问另一个分片。因为脚本执行期间没有其他命令可以执行，循环最多需要两次：一次是为了注意到初始分片已满，另一次是为了将项目推送到空分片。
- en: '|  |'
  id: totrans-558
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise: Finding the length of a sharded `LIST`**'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习：查找分片 `LIST` 的长度**'
- en: Now that you can create a sharded `LIST`, knowing how long your `LIST` has grown
    can be useful, especially if you’re using sharded `LIST`s for very long task queues.
    Can you write a function (with or without Lua) that can return the size of a sharded
    `LIST`?
  id: totrans-560
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经可以创建分片 `LIST`，知道你的 `LIST` 增长了多少可能很有用，特别是如果你正在使用分片 `LIST` 作为非常长的任务队列。你能编写一个函数（使用或不用
    Lua）来返回分片 `LIST` 的大小吗？
- en: '|  |'
  id: totrans-561
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s work on popping items from the `LIST` next.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们来处理从 `LIST` 中弹出项目。
- en: 11.4.3\. Popping items from the sharded LIST
  id: totrans-563
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.3\. 从分片 `LIST` 弹出项目
- en: 'When popping items from a sharded `LIST`, we technically don’t need to use
    Lua. Redis already has everything we need to pop items: `WATCH`, `MULTI`, and
    `EXEC`. But like we’ve seen in other situations, when there’s high contention
    (which could definitely be the case for a `LIST` that grows long enough to need
    sharding), `WATCH`/`MULTI`/`EXEC` transactions may be slow.'
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 当从分片 `LIST` 弹出项目时，技术上我们不需要使用 Lua。Redis 已经有了我们弹出项目所需的一切：`WATCH`、`MULTI` 和 `EXEC`。但就像我们在其他情况下看到的那样，当存在高竞争（对于足够长的
    `LIST`，这绝对可能是需要分片的情况），`WATCH`/`MULTI`/`EXEC` 事务可能会很慢。
- en: To pop items in a nonblocking manner from a sharded `LIST` in Lua, we only need
    to find the endmost shard, pop an item (if one is available), and if the resulting
    `LIST` shard is empty, adjust the end shard information, as demonstrated in the
    next listing.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 要以非阻塞方式从 Lua 中的分片 `LIST` 弹出项目，我们只需要找到最末尾的分片，弹出项目（如果有的话），如果结果 `LIST` 分片为空，则调整末尾分片信息，如下一列表所示。
- en: Listing 11.15\. The Lua script for pushing items onto a sharded `LIST`
  id: totrans-566
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.15\. 将项目推送到分片 `LIST` 的 Lua 脚本
- en: '![](ch11ex15-0.jpg)'
  id: totrans-567
  prefs: []
  type: TYPE_IMG
  zh: '![ch11ex15-0.jpg](ch11ex15-0.jpg)'
- en: '![](ch11ex15-1.jpg)'
  id: totrans-568
  prefs: []
  type: TYPE_IMG
  zh: '![ch11ex15-1.jpg](ch11ex15-1.jpg)'
- en: When popping items from a sharded `LIST`, we need to remember that if we pop
    from an empty shard, we don’t know if it’s because the whole sharded `LIST` is
    empty or just the shard itself. In this situation, we need to verify that we still
    have space between the endpoints, in order to know if we can adjust one end or
    the other. In the situation where just this one shard is empty, we have an opportunity
    to pop an item from the proper shard, which we do.
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 当从分片 `LIST` 弹出项目时，我们需要记住，如果我们从一个空分片弹出，我们不知道是因为整个分片 `LIST` 为空还是只是这个分片本身。在这种情况下，我们需要验证我们是否在端点之间还有空间，以便知道我们是否可以调整一个端点或另一个端点。在这种情况下，只有一个分片为空，我们有从正确分片弹出项目的机会，我们就是这样做的。
- en: The only remaining piece for our promised API is blocking pop operations.
  id: totrans-570
  prefs: []
  type: TYPE_NORMAL
  zh: 我们承诺的 API 中唯一剩下的部分是阻塞弹出操作。
- en: 11.4.4\. Performing blocking pops from the sharded LIST
  id: totrans-571
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 11.4.4\. 从分片 `LIST` 执行阻塞弹出
- en: We’ve stepped through pushing items onto both ends of a long `LIST`, popping
    items off both ends, and even written a function to get the total length of a
    sharded `LIST`. In this section, we’ll build a method to perform blocking pops
    from both ends of the sharded `LIST`. In previous chapters, we’ve used blocking
    pops to implement messaging and task queues, though other uses are also possible.
  id: totrans-572
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经逐步完成了将项目推送到长 `LIST` 的两端，从两端弹出项目，甚至编写了一个函数来获取分片 `LIST` 的总长度。在本节中，我们将构建一个方法来从分片
    `LIST` 的两端执行阻塞弹出。在前几章中，我们使用阻塞弹出实现了消息传递和任务队列，尽管还有其他可能的用途。
- en: Whenever possible, if we don’t need to actually block and wait on a request,
    we should use the nonblocking versions of the sharded `LIST` pop methods. This
    is because, with the current semantics and commands available to Lua scripting
    and `WATCH`/`MULTI`/`EXEC` transactions, there are still some situations where
    we may receive incorrect data. These situations are rare, and we’ll go through
    a few steps to try to prevent them from happening, but every system has limitations.
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 在可能的情况下，如果我们不需要实际阻塞并等待请求，我们应该使用分片`LIST`弹出方法的非阻塞版本。这是因为，在当前Lua脚本和`WATCH`/`MULTI`/`EXEC`事务可用的语义和命令中，仍然存在一些我们可能会收到错误数据的情况。这些情况很少见，我们将通过几个步骤来尝试防止它们发生，但每个系统都有局限性。
- en: In order to perform a blocking pop, we’ll cheat somewhat. First, we’ll try to
    perform a nonblocking pop in a loop until we run out of time, or until we get
    an item. If that works, then we’re done. If that doesn’t get an item, then we’ll
    loop over a few steps until we get an item, or until our timeout is up.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 为了执行阻塞式弹出，我们将采取一些欺骗手段。首先，我们将尝试在循环中执行非阻塞式弹出，直到耗尽时间或获取到项目。如果这成功了，那么我们就完成了。如果这没有获取到项目，那么我们将重复几个步骤，直到获取到项目或直到超时。
- en: The specific sequence of operations we’ll perform is to start by trying the
    nonblocking pop. If that fails, then we fetch information about the first and
    last shard IDs. If the IDs are the same, we then perform a blocking pop on that
    shard ID. Well, sort of.
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要执行的具体操作顺序是首先尝试非阻塞式弹出。如果失败了，然后我们获取第一个和最后一个分片ID的信息。如果ID相同，我们就在那个分片ID上执行阻塞式弹出。嗯，有点像。
- en: Because the shard ID of the end we want to pop from could’ve changed since we
    fetched the endpoints (due to round-trip latencies), we insert a pipelined Lua
    script `EVAL` call just before the blocking pop. This script verifies whether
    we’re trying to pop from the correct `LIST`. If we are, then it does nothing,
    and our blocking pop operation occurs without issue. But if it’s the wrong `LIST`,
    then the script will push an extra “dummy” item onto the `LIST`, which will then
    be popped with the blocking pop operation immediately following.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们获取端点以来，我们想要弹出的端片的分片ID可能已经改变（由于往返延迟），我们在阻塞式弹出之前插入了一个管道化的Lua脚本`EVAL`调用。这个脚本验证我们是否正在尝试从正确的`LIST`弹出。如果我们正在尝试，那么它将不执行任何操作，我们的阻塞式弹出操作将无问题发生。但如果它不是正确的`LIST`，那么脚本将向`LIST`推送一个额外的“虚拟”项目，然后这个项目将立即在随后的阻塞式弹出操作中被弹出。
- en: There’s a potential race between when the Lua script is executed and when the
    blocking pop operation is executed. If someone attempts to pop or push an item
    from that same shard between when the Lua script is executed and when the blocking
    pop operation is executed, then we could get bad data (the other popping client
    getting our dummy item), or we could end up blocking on the wrong shard.
  id: totrans-577
  prefs: []
  type: TYPE_NORMAL
  zh: 当Lua脚本执行和阻塞式弹出操作执行之间可能存在潜在的竞争条件。如果在Lua脚本执行和阻塞式弹出操作执行之间有人尝试从同一分片弹出或推送项目，那么我们可能会得到错误的数据（其他弹出客户端获取我们的虚拟项目），或者我们可能会阻塞在错误分片上。
- en: '|  |'
  id: totrans-578
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Why not use a `MULTI/EXEC` transaction?
  id: totrans-579
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 为什么不使用`MULTI/EXEC`事务？
- en: We’ve talked a lot about `MULTI`/`EXEC` transactions as a way of preventing
    race conditions through the other chapters. So why don’t we use `WATCH`/`MULTI`/`EXEC`
    to prepare information, and then use a `BLPOP`/`BRPOP` operation as the last command
    before `EXEC`? This is because if a `BLPOP`/`BRPOP` operation occurs on an empty
    `LIST` as part of a `MULTI`/`EXEC` transaction, it’d block forever because no
    other commands can be run in that time. To prevent such an error, `BLPOP`/`BRPOP`
    operations within a `MULTI`/`EXEC` block will execute as their nonblocking `LPOP`/`RPOP`
    versions (except allowing the client to pass multiple lists to attempt to pop
    from).
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在其他章节中已经大量讨论了`MULTI`/`EXEC`事务作为防止竞争条件的一种方法。那么为什么我们不使用`WATCH`/`MULTI`/`EXEC`来准备信息，然后在`EXEC`之前使用`BLPOP`/`BRPOP`操作作为最后一个命令呢？这是因为如果`BLPOP`/`BRPOP`操作在空`LIST`上作为`MULTI`/`EXEC`事务的一部分发生，它将永远阻塞，因为在这段时间内无法运行其他命令。为了防止这种错误，`MULTI`/`EXEC`块内的`BLPOP`/`BRPOP`操作将作为它们的非阻塞`LPOP`/`RPOP`版本执行（除了允许客户端传递多个列表以尝试弹出）。
- en: '|  |'
  id: totrans-581
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: To address the issue with blocking on the wrong shard, we’ll only ever block
    for one second at a time (even if we’re supposed to block forever). And to address
    the issue with our blocking pop operations getting data that wasn’t actually on
    the end shard, we’ll operate under the assumption that if data came in between
    two non-transactional pipelined calls, it’s close enough to being correct. Our
    functions for handling blocking pops can be seen in the next listing.
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决在错误分片上阻塞的问题，我们每次只会阻塞一秒钟（即使我们本应永远阻塞）。为了解决我们的阻塞弹出操作获取到实际上不在末端分片上的数据的问题，我们假设如果数据在两个非事务性管道调用之间到来，它就足够接近正确。我们处理阻塞弹出的函数可以在下一个列表中看到。
- en: Listing 11.16\. Our code to perform a blocking pop from a sharded `LIST`
  id: totrans-583
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 11.16\. 我们用于从分片 `LIST` 中执行阻塞弹出操作的代码
- en: '![](ch11ex16-0.jpg)'
  id: totrans-584
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch11ex16-0.jpg)'
- en: '![](ch11ex16-1.jpg)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![图片](ch11ex16-1.jpg)'
- en: There are a lot of pieces that come together to make this actually work, but
    remember that there are three basic pieces. The first piece is a helper that handles
    the loop to actually fetch the item. Inside this loop, we call the second piece,
    which is the helper/blocking pop pair of functions, which handles the blocking
    portion of the calls. The third piece is the API that users will actually call,
    which handles passing all of the proper arguments to the helper.
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 要使这一切真正工作，需要很多部分协同作用，但请记住，这里有三个基本部分。第一个部分是一个辅助程序，它处理循环以实际获取项目。在这个循环内部，我们调用第二个部分，即辅助程序/阻塞弹出函数对，它处理调用中的阻塞部分。第三个部分是用户实际调用的
    API，它处理将所有适当的参数传递给辅助程序。
- en: For each of the commands operating on sharded `LIST`s, we could implement them
    with `WATCH`/`MULTI`/`EXEC` transactions. But a practical issue comes up when
    there’s a modest amount of contention, because each of these operations manipulates
    multiple structures simultaneously, and will manipulate structures that are calculated
    as part of the transaction itself. Using a lock over the entire structure can
    help somewhat, but using Lua improves performance significantly.
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 对于在分片 `LIST` 上操作的每个命令，我们都可以使用 `WATCH`/`MULTI`/`EXEC` 事务来实现它们。但是，当存在适度的竞争时，会出现一个实际问题，因为每个这些操作都会同时操作多个结构，并且会操作作为事务本身一部分计算的结构。在整个结构上使用锁可以在一定程度上有所帮助，但使用
    Lua 可以显著提高性能。
- en: 11.5\. Summary
  id: totrans-588
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 11.5\. 摘要
- en: If there’s one idea that you should take away from this chapter, it’s that scripting
    with Lua can greatly improve performance and can simplify the operations that
    you need to perform. Though there are some limitations with Redis scripting across
    shards that aren’t limitations when using locks or `WATCH`/`MULTI`/`EXEC` transactions
    in some scenarios, Lua scripting is a significant win in the majority of situations.
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你从这个章节中带走一个想法，那就是使用 Lua 脚本可以极大地提高性能，并且可以简化你需要执行的操作。尽管在某些情况下，Redis 脚本在分片上存在一些限制，而这些限制在使用锁或
    `WATCH`/`MULTI`/`EXEC` 事务时并不存在，但在大多数情况下，Lua 脚本都是一个重大的胜利。
- en: Sadly, we’ve come to the end of our chapters. Up next you’ll find appendixes
    that offer installation instructions for three major platforms; references to
    potentially useful software, libraries, and documentation; and an index to help
    you find relevant topics in this or other chapters.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 悲剧的是，我们的章节已经结束。接下来，你将找到三个主要平台的安装说明附录；对可能有用的软件、库和文档的引用；以及一个索引，帮助你在这个章节或其他章节中找到相关主题。
