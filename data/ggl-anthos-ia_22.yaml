- en: Appendix E. An end-to-end example of ML application
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录E.机器学习应用的端到端示例
- en: Amita Kapoor
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Amita Kapoor
- en: This Chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Why do ML in the cloud?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为什么要在云中做机器学习？
- en: When to do ML in the cloud?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 何时在云中做机器学习？
- en: How to build an ML pipeline on Anthos using Kubeflow?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如何在Anthos上使用Kubeflow构建机器学习管道？
- en: Understand TensorFlow Extended
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解TensorFlow Extended
- en: Learn the features of Vertex AI
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习Vertex AI的功能
- en: In the preceding sections, you were introduced to Anthos and how to migrate
    your existing applications to the Anthos platform. This chapter will demonstrate
    how to run end-to-end machine learning workloads on multiple cloud providers and
    on-prem. A fully working and production-ready project will be discussed in deep
    detail. We will be using Kubeflow on the Anthos platform.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，你已了解到Anthos以及如何将现有应用程序迁移到Anthos平台。本章将演示如何在多个云提供商和本地环境中运行端到端的机器学习工作负载。将深入讨论一个完全工作且生产就绪的项目。我们将使用Anthos平台上的Kubeflow。
- en: Specifically, the chapter will introduce you to the need for automation in the
    ML pipeline, the concept of MLOps, TensorFlow extended, and Kubeflow. We will
    learn how Kubeflow can be used on-prem and cloud to automate the ML pipeline,
    with a specific example of hand digit recognition. Finally, we will explore Vertex
    AI- a one-stop shop for complete MLOPs.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，本章将介绍机器学习管道中自动化的必要性、MLOps的概念、TensorFlow扩展和Kubeflow。我们将学习如何在本地和云上使用Kubeflow来自动化机器学习管道，并以手写数字识别为例。最后，我们将探索Vertex
    AI——一个完整的MLOps一站式商店。
- en: E.1 The need for MLOps
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.1 MLOps的必要性
- en: Cloud Computing has democratized the machine learning world. Computational resources
    like GPU/TPUs are no longer limited to big institutes or organizations; the cloud
    has made them accessible to the masses. The Google Maps on your mobile or Google
    Translate you use on the go; both use machine learning algorithms running on the
    cloud. Whether you are a big tech company or a small business setup, shifting
    your ML tasks to the cloud allows you to leverage the elasticity and scalability
    offered by the cloud. Your system resources no longer constrain you; furthermore,
    you benefit from the proprietary tools provided by the cloud service providers.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 云计算已经使机器学习世界民主化。计算资源如GPU/TPUs不再仅限于大型机构或组织；云使得它们对大众可及。你手机上的谷歌地图或你路上使用的谷歌翻译；两者都使用了在云上运行的机器学习算法。无论你是大型科技公司还是小型企业，将你的机器学习任务转移到云上可以让你利用云提供的弹性和可扩展性。你的系统资源不再限制你；此外，你还能从云服务提供商提供的专有工具中受益。
- en: As an AI/ML scientist/engineer, I can hear you saying that the cloud, etc. is
    good, but shifting to the cloud is cumbersome. Your views are not lopsided either;
    many of us have struggled to deploy our AI/ML model to the web. The journey from
    AI research to production is long and lengthy, and full of many hurdles. The complete
    AI/ML workload starting from model building to model deployment to allocate web
    resources is cumbrous - as any change in one step leads to changes in another.
    As shown in Figure E.1, only a small fraction of the real-world ML system is concerned
    with learning and prediction; however, it requires the support of a vast and complex
    infrastructure. The problem is aggravated by the fact that changing anything changes
    everything (CACE), a minor tweak in hyperparameters, changing learning settings,
    modifying data selection methods - the whole system needs change.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一名AI/ML科学家/工程师，我能听到你说云等是好的，但转移到云上很麻烦。你的观点也不是片面的；我们中许多人都在努力将我们的AI/ML模型部署到网络上。从AI研究到生产的旅程漫长且冗长，充满了许多障碍。从模型构建到模型部署再到分配网络资源，整个AI/ML工作负载都很繁琐——因为任何一步的改变都会导致其他步骤的改变。如图E.1所示，只有一小部分现实世界的机器学习系统与学习和预测相关；然而，它需要一个庞大而复杂的支持基础设施。问题因“改变任何东西都会改变一切”（CACE）而加剧，对超参数的微小调整、改变学习设置、修改数据选择方法——整个系统都需要改变。
- en: '![E_01](../../OEBPS/Images/E_01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![E_01](../../OEBPS/Images/E_01.png)'
- en: Figure E.1 Different Components of an ML system[[1]](#ftn1)
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.1 机器学习系统的不同组件[[1]](#ftn1)
- en: In the IT sector, speed, reliability, and access to information are critical
    components for success. No matter which sector your company is working in, it
    requires IT agility. This becomes even more important when we talk about AI/ML-based
    solutions and products. Today most industries perform the ML task manually, resulting
    in enormous time gaps between building an ML model and its deployment (Figure
    E.2). The data collected is prepared and processed (normalization, feature engineering,
    etc.) so that it can be fed to the model. The model is trained and then evaluated
    over various metrics and techniques; once the model satisfies the requirements,
    it is sent to the model registry, where it is containerized for serving.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在 IT 行业，速度、可靠性和信息获取是成功的关键组成部分。无论你的公司从事哪个行业，都需要 IT 灵活性。当我们谈论基于 AI/ML 的解决方案和产品时，这一点变得更加重要。今天，大多数行业都是手动执行
    ML 任务，导致在构建 ML 模型和其部署之间出现巨大的时间差距（图 E.2）。收集的数据被准备和处理（归一化、特征工程等），以便可以输入到模型中。模型被训练，然后根据各种指标和技术进行评估；一旦模型满足要求，它就会被发送到模型注册表，在那里它被容器化以供服务。
- en: Each step from data analysis to model serving is performed manually, and the
    transition from one step to another is also manual. The data scientist works separately
    from the Ops team; they hand over a trained model to the development team, who
    then deploy the model in their API infrastructure. This can result in training-serving
    skew[^([2])](#ftn2) - the difference between the model performance during training
    and performance during serving.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 从数据分析到模型服务的每一步都是手动执行的，从一个步骤到另一个步骤的过渡也是手动的。数据科学家与运维团队分开工作；他们将训练好的模型交给开发团队，然后开发团队将其部署到他们的
    API 基础设施中。这可能导致训练-服务偏差[^([2])](#ftn2)——即模型在训练期间和部署期间的性能差异。
- en: '![E_02](../../OEBPS/Images/E_02.png)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![E_02](../../OEBPS/Images/E_02.png)'
- en: Figure E.2 Machine Learning Workflow[[3]](#ftn3)
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.2 机器学习工作流程[[3]](#ftn3)
- en: Further, since the model development is separate from its final deployment,
    there are infrequent release iterations. Furthermore, the greatest setback is
    the lack of active performance monitoring. The prediction service does not track
    or maintain a log of the model predictions necessary to detect any model performance
    degradation or drift in its behavior. Theoretically, this manual process might
    be sufficient if the model is rarely changed or trained. However, in practice,
    models often fail when they are deployed in the real world[^([4])](#ftn4).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，由于模型开发与其最终部署是分开的，因此发布迭代频率较低。此外，最大的障碍是缺乏主动性能监控。预测服务不跟踪或维护模型预测的日志，这些预测对于检测模型性能下降或行为漂移是必要的。理论上，如果模型很少改变或训练，这个手动过程可能是足够的。然而，在实践中，模型在现实世界部署时往往失败[^([4])](#ftn4)。
- en: 'The reasons for the failure are multifold:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 失败的原因是多方面的：
- en: 'Models get outdated: With time, the accuracy of the model drops, in the classical
    ML pipeline, there is no continuous monitoring to detect the fall in model performance
    and rectify it. The end-user, however, bears the pain. Imagine you are providing
    services to a fashion house suggesting new apparel designs based on customers''
    past purchases and fashion trends. Fashion changes dramatically with time; the
    colors that were ‘in’ in Autumn are no longer working in winters. If your model
    is not ingesting the recent fashion data and using it to give customers recommendations-
    the customers will complain- users to the site will drop, after some delay, the
    business team will notice and then on identifying the problem, you will be asked
    to update the model to latest data. This situation can be avoided if there is
    an option of continuous monitoring of the model performance and there are systems
    in place to implement continuous training (figure 3) on newly acquired data.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型过时：随着时间的推移，模型的准确性下降，在经典的机器学习流程中，没有持续监控来检测模型性能的下降并纠正它。然而，最终用户却承受着痛苦。想象一下，你正在为一个时尚屋提供服务，根据客户的过去购买和时尚趋势建议新的服装设计。时尚随着时间的推移而急剧变化；在秋季流行的颜色在冬天就不再适用了。如果你的模型没有摄取最新的时尚数据并据此向客户提供推荐，客户会抱怨——访问网站的用户会减少，经过一段时间延迟后，业务团队会注意到，然后在确定问题后，你会被要求更新模型到最新数据。如果有一种持续监控模型性能的选项，并且有系统来对新获得的数据实施持续训练（图
    3），这种情况是可以避免的。
- en: '![E_03](../../OEBPS/Images/E_03.png)'
  id: totrans-22
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![E_03](../../OEBPS/Images/E_03.png)'
- en: Figure E.3 Continuous Training
  id: totrans-23
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 E.3 持续训练
- en: 'Data Drift: The difference between the joint distribution of input features
    and output in the training dataset and test dataset can cause dataset drift [2].
    When the model was deployed, the real-world data had the same distribution as
    the training dataset, but with time the distribution changed. Consider you build
    a model to detect network intrusion based on the data available at that time.
    Six months have passed, do you think it will work as efficiently as it did at
    the time of deployment? It may, but chances are it would be fast drifting away,
    in the internet world - six months is almost six generations! The problem can
    be resolved if there are options to get metrics sliced on recent data.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据漂移：训练数据集和测试数据集中输入特征与输出的联合分布之间的差异可能导致数据集漂移[2]。当模型部署时，现实世界的数据分布与训练数据集相同，但随着时间的推移，分布发生了变化。假设你基于当时可用的数据构建了一个用于检测网络入侵的模型。六个月过去了，你认为它还会像部署时那样高效吗？可能会，但可能性不大，因为随着时间的推移，模型可能会迅速偏离，在互联网世界中——六个月几乎相当于六代！如果有机会获取最近数据的指标切片，这个问题是可以解决的。
- en: 'Feedback loops: There may exist un-intentional feedback, where the predictions
    by the model end up affecting its own training data. For example, considering
    you are working for a music streaming company, the company uses a recommendation
    system that recommends users new music albums based on their past listening history
    and profile. The system recommends the albums with, let us say, more than 70%
    confidence level. The company decided to add a feature for users to like or dislike
    music albums. Initially, you will be jubilant, as the recommended albums get more
    and more likes, but as time goes by, the viewing history will affect that model
    prediction, and unknowingly the system will be recommending more and more music
    similar to the ones they had heard before, and leave out the new music to which
    the users might have enjoyed listening. To mitigate this problem, continuous monitoring
    of the system metrics will be helpful.'
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 反馈循环：可能存在无意识的反馈，其中模型的预测最终影响了其自身的训练数据。例如，假设你为一家音乐流媒体公司工作，该公司使用一个基于用户过去收听历史和资料推荐新音乐专辑的系统。系统以超过70%的置信度推荐专辑。公司决定为用户提供喜欢或不喜欢音乐专辑的功能。最初，你会感到非常高兴，因为推荐的专辑越来越受欢迎，但随着时间的推移，查看历史将影响那个模型预测，并且不知不觉中，系统会越来越多地推荐用户之前听过的类似音乐，而忽略了用户可能喜欢的新音乐。为了减轻这个问题，持续监控系统指标将是有帮助的。
- en: 'To know more about the technical debt incurred by machine learning models,
    I would suggest readers go through the paper titled "Machine learning: The high
    interest credit card of technical debt” by Sculley et al. In the paper, they talk
    in detail about the technical debt in machine learning, they talk about the maintenance
    cost associated with the systems using AI/ML solutions.'
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要了解更多关于机器学习模型产生的技术债务，我建议读者阅读Sculley等人撰写的论文《机器学习：技术债务的高息信用卡》。在论文中，他们详细讨论了机器学习中的技术债务，以及使用AI/ML解决方案的系统相关的维护成本。
- en: 'Though it is impossible and even unnecessary to obliterate the technical debt,
    a holistic approach can reduce the debt. What is needed is a system that allows
    one to integrate the standard DevOps pipelines with our machine learning workflows-
    the ML pipeline automation: “the MLOps”. Let us see how Anthos can facilitate
    MLOps.'
  id: totrans-27
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 虽然消除技术债务是不可能的，甚至是不必要的，但整体方法可以减少债务。所需的是一个允许将标准DevOps管道与我们的机器学习工作流程集成的系统——即ML管道自动化：“MLOps”。让我们看看Anthos如何促进MLOps。
- en: 'Run AI/ML applications across hybrid and multi-cloud environments: Anthos,
    a managed application platform, allows you to conveniently and efficiently manage
    your entire AI/ML product lifecycle by managing the on-prem and on-cloud infrastructure,
    and security of data and model. Traditionally an AI engineer develops machine
    learning code in different environments, with different clusters, dependencies,
    and even infrastructure needs (for example, training is compute-intensive and
    typically requires GPUs). Once the model is fully trained, the development team
    takes it to the next stage- the infrastructure at the deployment and production
    stage are very different (deployment can take place on CPUs). The infrastructure
    abstraction offered by Anthos provides much-needed portability; it allows one
    to build and run AI/Ml applications efficiently and securely. Anthos''s truly
    hybrid cloud architecture lets you build and deploy your code anywhere without
    making any changes. With Anthos hybrid architecture, you can develop and run some
    code blocks on-prem while others on the cloud. Anthos gives you the flexibility
    to build, test, and run your AI/ML applications across hybrid and multi-cloud
    environments.'
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混合和多云环境中运行 AI/ML 应用程序：Anthos，一个托管的应用程序平台，允许您通过管理本地和云端的架构以及数据和安全模型来方便且高效地管理整个
    AI/ML 产品生命周期。传统上，AI 工程师在不同的环境中开发机器学习代码，这些环境有不同的集群、依赖关系，甚至基础设施需求（例如，训练是计算密集型的，通常需要
    GPU）。一旦模型完全训练完成，开发团队将其带入下一个阶段——部署和生产阶段的基础设施非常不同（部署可以在 CPU 上进行）。Anthos 提供的基础设施抽象提供了急需的可移植性；它允许用户高效且安全地构建和运行
    AI/ML 应用程序。Anthos 的真正混合云架构让您可以在任何地方构建和部署代码，而无需进行任何更改。使用 Anthos 混合架构，您可以在本地和云端开发并运行一些代码块。Anthos
    让您能够在混合和多云环境中构建、测试和运行您的 AI/ML 应用程序。
- en: 'Use Anthos GKE to manage CPU/GPU clusters: Another advantage of using Anthos
    for your AI/ML workflow is the GPU support provided by Anthos. In collaboration
    with NVIDIA[^([5])](#ftn5), the world’s number one GPU manufacturer, Google’s
    Anthos uses NVIDIA GPU operators to deploy GPU drivers required to enable GPUs
    on Kubernetes. This provides users with a broad choice of GPUs like V100, T4,
    and P4 GPUs. With Anthos, you can thus manage your existing GPUs on-prem and even
    support any future GPU investment you make. It is also possible to shift your
    workloads into the cluster, in case you require more compute resources. Thus,
    using Anthos GKE you can with ease manage GPU/CPU clusters both in-house and on-cloud.'
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Anthos GKE 管理CPU/GPU集群：使用 Anthos 进行您的 AI/ML 工作流程的另一个优点是 Anthos 提供的 GPU 支持。与全球最大的
    GPU 制造商 NVIDIA 合作，Google 的 Anthos 使用 NVIDIA GPU 运营商来部署启用 Kubernetes 上 GPU 所需的
    GPU 驱动程序。这为用户提供了一个广泛的 GPU 选择，如 V100、T4 和 P4 GPU。使用 Anthos，您因此可以轻松管理本地现有的 GPU，甚至支持您未来做出的任何
    GPU 投资。如果您需要更多的计算资源，还可以将工作负载转移到集群中。因此，使用 Anthos GKE，您可以轻松管理本地和云端内的 GPU/CPU 集群。
- en: 'Secure data and model with ASM: Security of both data and model is paramount.
    Anthos ASM allows you to configure security access for your entire working environment.
    The chapter on Anthos Service mesh covers in detail how it can be used to provide
    a resilient, scalable, secure and manageable service.'
  id: totrans-30
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 ASM 保护数据和模型：数据和模型的安全性至关重要。Anthos ASM 允许您配置整个工作环境的安全访问权限。关于 Anthos 服务网格的章节详细介绍了如何使用它来提供弹性、可扩展、安全和可管理的服务。
- en: 'Deploy AI/ML using Cloud Run: Lastly one can directly deploy the trained dockerized
    model on cloud run, Google''s serverless container as a service platform.'
  id: totrans-31
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Cloud Run 在云端部署 AI/ML：最后，可以直接在云运行上部署训练好的容器化模型，这是 Google 的无服务器容器即服务平台。
- en: In the coming sections, we will see how with Anthos and leveraging GCP tools
    like CloudRun, TensorFlow Extend, and operation orchestration tools like Kubeflow,
    and vertex AI, we can solve the core MLOps issues like portability, reproducibility,
    composability, agility, versioning, and build production-ready AI/ML solutions.
    Let us first start with understanding what we exactly mean by full ML pipeline
    automation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将看到如何利用 Anthos 和 GCP 工具（如 CloudRun、TensorFlow Extend）以及操作编排工具（如 Kubeflow
    和 Vertex AI），我们可以解决核心 MLOps 问题，如可移植性、可重复性、可组合性、敏捷性、版本控制和构建生产就绪的 AI/ML 解决方案。让我们首先从理解我们所说的完整机器学习管道自动化究竟是什么开始。
- en: E.2 ML pipeline Automation
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.2 机器学习管道自动化
- en: 'In the previous section (Figure E.2), we elaborated on the steps involved in
    delivering an ML project from inception to production. Each of these steps can
    be completed manually or via an automatic pipeline. In this section, we will see
    how each of these steps can be automated. The level of automation of these steps
    decides the time gap between training new models and their deployment and can
    help fix the challenges we discussed in the previous section. The automated ML
    pipeline should be able to:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节（图 E.2）中，我们详细介绍了从构思到生产交付机器学习（ML）项目的各个步骤。这些步骤中的每一个都可以手动完成或通过自动管道完成。在本节中，我们将探讨如何自动化这些步骤。这些步骤的自动化程度决定了训练新模型与部署之间的时间间隔，并有助于解决上一节中讨论的挑战。自动化的机器学习（ML）管道应该能够：
- en: Allow different teams involved in the product development to work independently.
    Ideally, many teams are involved in an AI/Ml workflow, starting from data collection,
    data ingestion, model development to model deployment. As discussed in the introduction
    section, any change by one of the teams affects all the rest (CACE). An ideal
    ML pipeline automation should allow the teams to work independently on various
    components without any interference from others.
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许参与产品开发的各个团队独立工作。理想情况下，许多团队参与人工智能/机器学习（AI/ML）工作流程，从数据收集、数据摄入、模型开发到模型部署。如引言部分所述，任何一个团队的任何变化都会影响其他所有团队（CACE）。理想的机器学习（ML）管道自动化应该允许团队在各个组件上独立工作，而不受其他团队的干扰。
- en: Actively monitor the model in production. Building the model is not the real
    challenge. The real challenge resides in maintaining the model’s accuracy in production.
    This is possible if the model in production is actively monitored- logs are maintained
    and triggers generated if model performance goes below a threshold. This will
    allow you to detect any degradation in the performance. This can be done by performing
    an online model validation step.
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主动监控生产中的模型。构建模型并不是真正的挑战。真正的挑战在于保持模型在生产中的准确性。如果生产中的模型得到积极监控——维护日志并在模型性能低于阈值时生成触发器，这是可能的。这将允许你检测到性能的任何下降。这可以通过执行在线模型验证步骤来完成。
- en: Accommodating data drift, it should evolve with new data patterns that emerge
    when new data comes in. This can be accomplished by adding an automated data validation
    step in the production pipeline. Any skew in data schema (missing features or
    unexpected values for the features) should trigger the data science team to investigate,
    while any substantial change in the statistical properties of data should set
    a trigger for retraining the model.
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适应数据漂移，它应该随着新数据的到来而演进新的数据模式。这可以通过在生产管道中添加自动数据验证步骤来实现。任何数据模式（缺失特征或特征的不期望值）的偏差都应触发数据科学团队进行调查，而任何数据统计特性的重大变化都应触发重新训练模型的触发器。
- en: In the field of AI/ML, new model architectures come every week, and you may
    be interested in experimenting with the latest model or tweaking your hyperparameters.
    The automated pipeline should allow for continuous training (CT). CT also becomes
    necessary when the production model falls below its performance threshold or a
    substantial data drift is observed.
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在人工智能/机器学习领域，每周都会有新的模型架构出现，你可能对尝试最新的模型或调整超参数感兴趣。自动化的管道应该允许进行持续训练（CT）。当生产模型低于其性能阈值或观察到数据漂移时，CT
    也变得必要。
- en: Additionally, reproducibility is a big problem in AI, so much so that NeurIPS,
    the premiere AI conference, has established a reproducibility chair[^([6])](#ftn6).
    The aim is for researchers to submit a reproducibility checklist to enable others
    to reproduce the results. Using modularized components not only allows teams to
    work independently but also makes changes without impacting other teams. It allows
    the teams to narrow down issues to a given component and thus helps in reproducibility.
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，可重复性是人工智能领域的一个大问题，以至于顶级人工智能会议 NeurIPS 已经设立了可重复性主席[^([6])](#ftn6)。目标是研究人员提交可重复性清单，以便其他人能够重现结果。使用模块化组件不仅允许团队独立工作，而且在不影响其他团队的情况下进行更改。它允许团队将问题缩小到特定的组件，从而有助于可重复性。
- en: And finally, for an expeditious and dependable update at the production level,
    there should be a robust CI/CD system. Delivering AI/ML solutions rapidly, reliably,
    and securely can help enhance your organization's performance.
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，为了在生产级别实现快速且可靠的更新，应该有一个强大的持续集成/持续部署（CI/CD）系统。快速、可靠和安全地交付人工智能/机器学习（AI/ML）解决方案可以帮助提升您组织的性能。
- en: Before you serve your model to the live traffic, you may also want to do A/B
    testing; you can do so by configuring such that the new model serves 10-20% of
    the live traffic. If the new model performs better than the old model, you can
    serve all the traffic to it; otherwise, roll back to the old model.
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你将模型服务于实时流量之前，你可能还想进行A/B测试；你可以通过配置新模型服务于10-20%的实时流量来实现。如果新模型的表现优于旧模型，你可以将所有流量都导向它；否则，回滚到旧模型。
- en: 'In essence, we need MLOps - Machine learning (ML) with DevOps (Ops) - An integrated
    engineering solution that unifies ML system development and ML system operation.
    This will allow the data scientists to explore various model architectures, experiment
    with feature engineering techniques, and hyperparameters and push the changes
    automatically to the deployment stage. Figure E.4 below shows different stages
    of the ML CI/CD automation pipeline. We can see the complete automation pipeline
    contains six stages:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从本质上讲，我们需要MLOps - 将机器学习（ML）与DevOps（Ops）相结合 - 一种统一的工程解决方案，它将机器学习系统开发和机器学习系统操作统一起来。这将允许数据科学家探索各种模型架构，实验特征工程技术和超参数，并将更改自动推送到部署阶段。下面的图E.4展示了机器学习CI/CD自动化管道的不同阶段。我们可以看到完整的自动化管道包含六个阶段：
- en: 'Development/Experimentation: In this stage, the data scientist iteratively
    tries various ML algorithms and architectures. Once satisfied, s/he pushes a source
    code of the model to the source code repository.'
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发/实验：在这个阶段，数据科学家迭代地尝试各种机器学习算法和架构。一旦满意，他们就会将模型的源代码推送到源代码仓库。
- en: 'Pipeline continuous integration: This stage involves building the source code,
    identifying and outputting the packages, executables, and artifacts needed to
    be deployed in a later stage.'
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道持续集成：这个阶段涉及构建源代码，识别并输出需要在后续阶段部署的包、可执行文件和工件。
- en: 'Pipeline continuous delivery: The artifacts produced in stage 2 are deployed
    to the target environment.'
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管道持续交付：阶段2产生的工件被部署到目标环境。
- en: 'Continuous training: Depending upon the triggers set, a trained model is pushed
    to the model registry at this stage.'
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 持续训练：根据设置的触发器，训练好的模型在这个阶段被推送到模型注册库。
- en: 'Model continuous delivery: At this stage, we get a deployed model prediction
    service.'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 模型持续交付：在这个阶段，我们得到了一个已部署的模型预测服务。
- en: 'Monitoring: In this stage, the model performance statistics are collected and
    used to set triggers to execute the pipeline or execute a new experiment cycle.'
  id: totrans-48
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 监控：在这个阶段，收集模型性能统计数据并用于设置触发器以执行管道或执行新的实验周期。
- en: '![E_04](../../OEBPS/Images/E_04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![E_04](../../OEBPS/Images/E_04.png)'
- en: Figure E.4 Stages of the automated ML pipeline with CI/CD
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.4 自动化机器学习管道的阶段
- en: In the coming sections, we will cover some GCP tools that you can use to implement
    MLOps. We will talk about Cloud Run, TensorFlow Extend, and Kubeflow. The focus
    of the chapter will be Kubeflow and Vertex AI.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将介绍一些可以用于实现MLOps的GCP工具。我们将讨论Cloud Run、TensorFlow Extend和Kubeflow。本章的重点将是Kubeflow和Vertex
    AI。
- en: 'Before we delve into the chapter, we should refer to a few important concepts:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入本章之前，我们应该参考一些重要概念：
- en: Cloud Run, which has been covered in another chapter. As you already know, Cloud
    Run is Google's serverless container as a service platform. Cloud Run allows one
    to run an entire application in a container. Cloud Run can be used to deploy any
    stateless HTTP container. You just need to specify a Docker file with all the
    dependencies and your ML prediction code that you want to run, package them up
    as a container, and boom, the service is deployed on the cloud. Recently[^([7])](#ftn7)
    Google extended Cloud run capabilities to include end-to-end HTTP/2 connections,
    WebSockets compatibility, and bidirectional gRPC streaming. Thus, now you can
    deploy and run a wide variety of web services using Cloud Run. While Cloud Run
    is scalable, resilient, and offers straightforward AI/ML apps deployment, it has
    some constraints. For example, the maximum number of vCPUs[^([8])](#ftn8) that
    can be requested is limited to 4 (The option to increase upto 8vCPUs was available
    as preview at the time of writing this book).
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Cloud Run，已在另一章节中介绍。如您所知，Cloud Run 是 Google 的无服务器容器即服务平台。Cloud Run 允许您在容器中运行整个应用程序。Cloud
    Run 可以用来部署任何无状态的 HTTP 容器。您只需指定一个包含所有依赖项以及您想要运行的机器学习预测代码的 Docker 文件，将它们打包成容器，然后，服务就部署在云端了。最近[^([7])](#ftn7)，Google
    扩展了 Cloud Run 的功能，包括端到端 HTTP/2 连接、WebSocket 兼容性和双向 gRPC 流。因此，现在您可以使用 Cloud Run
    部署和运行各种网络服务。虽然 Cloud Run 可扩展、弹性好，并且提供简单的 AI/ML 应用部署，但它也有一些限制。例如，可以请求的最大 vCPU 数量[^([8])](#ftn8)限制为
    4（在撰写本书时，可以选择预览增加到 8vCPU 的选项）。
- en: Integrating Cloud Run with Cloud Build you can automate the whole process and
    quickly implement CI/CD for your AI/ML workflow. The concepts related to Cloud
    Build- the GCP native CI/CD platform- are covered in another chapter. Cloud build
    works on container technology. Details about container registry and building container
    images are covered in another chapter.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Cloud Run 与 Cloud Build 集成，您可以自动化整个流程，并快速为您的 AI/ML 工作流程实现 CI/CD。与 Cloud Build
    相关的概念——GCP 原生的 CI/CD 平台——将在另一章节中介绍。Cloud Build 基于容器技术。有关容器注册和构建容器镜像的详细信息将在另一章节中介绍。
- en: E.3 TensorFlow Extended
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.3 TensorFlow 扩展
- en: 'TensorFlow Extended (TFX) is a scalable end-to-end platform for the development
    and deployment of AI/ML workflows in TensorFlow. TFX includes libraries for data
    validation, data preprocessing, feature engineering, building, and training AI/ML
    models, evaluating model performance, and finally serving models as REST and gRPC
    APIs. You can judge the value of TFX by knowing that many Google products[^([9])](#ftn9),
    like Chrome, Google search, Mail, etc., are powered by TFX. Google uses TFX extensively,
    and so does Airbnb, PayPal, and Twitter. TFX as a platform uses various libraries
    to make an end-to-end- ML workflow. Let us see these libraries and what they can
    do:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow 扩展 (TFX) 是一个可扩展的端到端平台，用于在 TensorFlow 中开发和部署 AI/ML 工作流程。TFX 包括用于数据验证、数据预处理、特征工程、构建和训练
    AI/ML 模型、评估模型性能以及最终作为 REST 和 gRPC API 提供模型的库。您可以通过了解许多 Google 产品[^([9])](#ftn9)，如
    Chrome、Google 搜索、邮件等，都是通过 TFX 驱动的来判断 TFX 的价值。Google 广泛使用 TFX，Airbnb、PayPal 和 Twitter
    也同样如此。作为平台，TFX 使用各种库来实现端到端 ML 工作流程。让我们看看这些库以及它们能做什么：
- en: 'TensorFlow Data Validation (TFDV): This library has modules that allow you
    to explore and validate your data. It allows you to visualize the data on which
    the model was trained and/or tested. The statistical summary that it provides
    can be used to detect any anomaly present in the data. It has an automatic schema
    generation feature which allows you to get a description of the expected range
    of data. Additionally, when comparing different experiments and runs, you can
    also use it to identify data drift.'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 数据验证 (TFDV)：这个库包含模块，允许您探索和验证您的数据。它允许您可视化模型训练和/或测试所使用的数据。它提供的统计摘要可以用来检测数据中存在的任何异常。它具有自动模式生成功能，允许您获取数据预期范围的描述。此外，当比较不同的实验和运行时，您还可以用它来识别数据漂移。
- en: 'TensorFlow Transform (TFT): With the help of TFT, you can pre-process your
    data at scale. The functions provided by the TFT library can be used to analyze
    data, transform data and perform advanced feature engineering tasks. The advantage
    of using TFT is that the pre-processing step is modularized. A hybrid of Apache
    Beam and Tensorflow allows you to process the entire dataset, like getting maximum
    and minimum values or all possible categories, and manipulate the data batch as
    Tensors. It uses the Google DataFlow cloud service.'
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Transform (TFT)：借助 TFT，您可以在规模上预处理您的数据。TFT 库提供的函数可用于分析数据、转换数据并执行高级特征工程任务。使用
    TFT 的优势在于预处理步骤是模块化的。Apache Beam 和 Tensorflow 的混合体允许您处理整个数据集，例如获取最大值和最小值或所有可能的类别，并将数据批量作为张量进行操作。它使用
    Google DataFlow 云服务。
- en: 'TensorFlow Estimator and Keras: This is the standard TensorFlow framework you
    can use to build your model and train them. It also provides you access to a good
    range of pre-trained models.'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Estimator 和 Keras：这是您可以使用来构建和训练模型的标准 TensorFlow 框架。它还为您提供了访问一系列预训练模型的机会。
- en: 'TensorFlow Model Analysis (TFMA): It allows you to evaluate your trained model
    on large amounts of data in a distributed manner on the same model evaluation
    metrics that you defined while training. It helps analyze and understand the trained
    models.'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow 模型分析 (TFMA)：它允许您在分布式方式下，使用您在训练时定义的相同模型评估指标，在大量数据上评估您的训练模型。它有助于分析和理解训练模型。
- en: 'TensorFlow Serving (TFServing): Finally, if you are satisfied with your trained
    model, you can serve your model as REST and gRPC APIs for online production.'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow Serving (TFServing)：最后，如果您对您的训练模型感到满意，您可以将模型作为 REST 和 gRPC API 在线提供服务。
- en: The figure below shows how the different libraries are integrated to form a
    TFX based AI/ML pipeline.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的图示显示了不同的库如何集成形成一个基于 TFX 的 AI/ML 流程。
- en: '![E_05](../../OEBPS/Images/E_05.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![E_05](../../OEBPS/Images/E_05.png)'
- en: Figure E.5 TFX based AI/ML pipeline[[10]](#ftn10)
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.5 基于 TFX 的 AI/ML 流程[[10]](#ftn10)
- en: It is possible to run each of the above steps manually, however, as we discussed
    in the preceding section for MLOps we would like the steps to run automatically.
    To do this we need an orchestration tool, a tool which connects these various
    blocks (components) of the ML workflow together- this is where KubeFlow comes
    into picture, which will be the topic of the next section.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然可以手动运行上述每个步骤，但正如我们在上一节中讨论的，对于 MLOps，我们希望这些步骤能够自动运行。为此，我们需要一个编排工具，一个将 ML 工作流程的这些不同块（组件）连接在一起的工具——这就是
    KubeFlow 发挥作用的地方，它将是下一节的主题。
- en: 'E.4 Kubeflow: an introduction'
  id: totrans-66
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.4 Kubeflow：简介
- en: Kubeflow allows you to manage the entire AI/ML lifecycle. It is a Kubernetes
    native OSS (Operations Support System) platform to develop, deploy, and manage
    scalable and end-to-end machine learning workloads on hybrid and multi-cloud environments.
    Kubeflow pipelines, a Kubeflow service, helps you to automate the entire AI/ML
    lifecycle - in other words lets you compose, orchestrate and automate your AI/ML
    workloads.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 允许您管理整个 AI/ML 生命周期。它是一个 Kubernetes 原生的 OSS（运营支持系统）平台，用于在混合和多云环境中开发、部署和管理可扩展的端到端机器学习工作负载。Kubeflow
    流程，作为 Kubeflow 服务的一部分，帮助您自动化整个 AI/ML 生命周期——换句话说，让您能够组合、编排和自动化您的 AI/ML 工作负载。
- en: It is an open-source project, and as you can see from the image of commits below
    - it is an active and growing project. One of the primary goals, with which Kubeflow
    is built, is to make it easy for everyone to develop, deploy, and manage portable,
    scalable machine learning.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个开源项目，正如您从下面的提交图像中可以看到——它是一个活跃且不断发展的项目。Kubeflow 的主要目标之一，即构建 Kubeflow，是使每个人都能轻松地开发、部署和管理可移植、可扩展的机器学习。
- en: '![E_06](../../OEBPS/Images/E_06.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![E_06](../../OEBPS/Images/E_06.png)'
- en: Figure E.6 Commits on Kubeflow project as shown in its Github ([https://github.com/kubeflow/kubeflow/graphs/contributors](https://github.com/kubeflow/kubeflow/graphs/contributors))
    repo on 4th Feb 2021.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.6 如其在 Github ([https://github.com/kubeflow/kubeflow/graphs/contributors](https://github.com/kubeflow/kubeflow/graphs/contributors))
    仓库中所示，2021 年 2 月 4 日对 Kubeflow 项目的提交[[10]](#ftn10)。
- en: 'The best part is that even if you do not know much about Kubernetes you can
    use the Kubeflow API to build your AI/ML workflow. It is possible to use Kubeflow
    on your local machine, and on any cloud (GCP, Azure AWS), you can choose a single
    node or cluster, it is built to run consistently across various environments.
    In November 2020, Google released Kubeflow 1.2, which allows organizations to
    run their ML workflow on Anthos across environments. Kubeflow is built around
    three key principles:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 最好的部分是，即使您对Kubernetes不太了解，也可以使用Kubeflow API构建您的AI/ML工作流程。您可以在本地机器上使用Kubeflow，在任何云（GCP、Azure
    AWS）上，您可以选择单个节点或集群，它被构建成能够在各种环境中一致运行。2020年11月，Google发布了Kubeflow 1.2版本，允许组织在Anthos跨环境中运行他们的机器学习工作流程。Kubeflow围绕以下三个关键原则构建：
- en: 'Composability: Kubeflow extends Kubernetes ability to run independent and configurable
    steps using machine learning specific frameworks (like TensorFlow, PyTorch, etc)
    and libraries (Scikit-Learn, Pandas etc). This allows you to have various libraries
    for different tasks involved in AI/ML workflow, for instance while doing Data
    Processing steps- you may require a different version of TensorFlow, while during
    training you may be using a different version. Each task in AI/ML workflow thus
    can be independently containerized and worked upon.'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可组合性：Kubeflow扩展了Kubernetes的能力，使用机器学习特定框架（如TensorFlow、PyTorch等）和库（Scikit-Learn、Pandas等）运行独立和可配置的步骤。这使得您可以为AI/ML工作流程中涉及的不同任务使用各种库，例如，在进行数据处理步骤时，您可能需要TensorFlow的不同版本，而在训练期间，您可能使用的是不同版本。因此，AI/ML工作流程中的每个任务都可以独立容器化并处理。
- en: 'Portability: You can run all the pieces of your AI/ML workflow anywhere you
    want- on cloud, on-prem, or on your laptop while on vacation - the only condition-
    they all are running Kubeflow. Kubeflow creates an abstraction layer between your
    AI/ML project and your system, thus making it possible to run the ML project anywhere
    Kubeflow is installed.'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可移植性：您可以在任何您想要的地方运行AI/ML工作流程的所有部分——在云上、在本地，或者在度假时在您的笔记本电脑上——唯一条件是它们都在运行Kubeflow。Kubeflow在您的AI/ML项目和系统之间创建了一个抽象层，从而使得在Kubeflow安装的地方运行ML项目成为可能。
- en: 'Scalability: You can have more resources when you want, and release them when
    not needed. Kubeflow extends the Kubernetes ability to maximize available resources
    and scale them with as little manual effort as possible.'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可伸缩性：当您需要更多资源时，可以拥有它们，当不需要时，可以释放它们。Kubeflow扩展了Kubernetes的能力，以最大化可用资源，并以尽可能少的手动努力进行扩展。
- en: 'In this section, we will learn to use Kubeflow on the cloud-native ecosystem
    provided by Anthos running on Kubernetes. Some of the advantages of using Kubeflow:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何在Anthos提供的云原生生态系统中使用Kubeflow，该系统在Kubernetes上运行。使用Kubeflow的一些优势：
- en: Standardize on a common infrastructure
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标准化通用基础设施
- en: 'Leverage open-source cloud-native ecosystems for the entire AI/ML lifecycle:
    developing, orchestrating, deploying, and running scalable and portable AI/ML
    workloads.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 利用开源云原生生态系统实现整个AI/ML生命周期的开发、编排、部署和运行可伸缩和可移植的AI/ML工作负载。
- en: Run AI/ML workflows in hybrid and multi-cloud environments.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在混合和多云环境中运行AI/ML工作流程。
- en: Additionally, when running on GKE, you can take advantage of GKE’s enterprise-grade
    security, logging, autoscaling, and identify features.
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 此外，当在GKE上运行时，您可以利用GKE的企业级安全、日志记录、自动扩展和标识功能。
- en: 'Kubeflow adds CRDs (Custom resource definitions) to the clusters. Kubeflow
    leverages containers and Kubernetes and thus can be used anywhere Kubernetes is
    already running, especially on premises with Anthos with GKE. Below we list various
    Kubeflow applications and components that can be used to arrange your ML workflow
    on top of Kubernetes:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow向集群中添加了CRDs（自定义资源定义）。Kubeflow利用容器和Kubernetes，因此可以在Kubernetes已经运行的地方使用，尤其是在使用Anthos和GKE的本地环境中。以下列出了一些Kubeflow应用程序和组件，可用于在Kubernetes之上安排您的机器学习工作流程：
- en: 'Jupyter Notebook: For AI/ML practitioners Jupyter notebooks[^([11])](#ftn11)
    is the de facto tool for rapid data analysis. Most data science projects start
    with a Jupyter notebook. It is the starting point of the modern cloud-native machine
    learning pipeline. The Kubeflow notebooks allow you to run your experiments locally,
    or if you want you can take the data, train the model and even serve it- all through
    the notebook. Notebooks integrate well with the rest of the infrastructure for
    things like accessing other services in the Kubeflow cluster using the cluster
    IP addresses. It also integrates with access control and authentication. Kubeflow
    allows one to set up multiple notebook servers, with the possibility to run multiple
    notebooks per server. Each notebook server belongs to a single namespace, depending
    upon the project or team for that server. Kubeflow provides multi-user support
    using namespaces, this makes it easier to collaborate and manage access. Using
    a notebook on Kubeflow allows you to dynamically scale resources. And the best
    part, it comes with all the plugins/dependencies you might need to train a model
    in Jupyter, including TensorBoard visualizations and customize compute resources
    that you might need to train the model. The Kubeflow notebooks provide the same
    experience as Jupyter Notebooks locally, with an added benefit of scalability,
    access control, collaboration and submitting jobs directly to the Kubernetes cluster.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebook：对于人工智能/机器学习从业者来说，Jupyter笔记本[^([11])](#ftn11)是快速数据分析的事实上工具。大多数数据科学项目都是从Jupyter笔记本开始的。它是现代云原生机器学习管道的起点。Kubeflow笔记本允许您在本地运行实验，或者如果您想的话，可以取用数据、训练模型，甚至通过笔记本部署它。笔记本与基础设施的其他部分集成良好，例如使用集群IP地址访问Kubeflow集群中的其他服务。它还与访问控制和身份验证集成。Kubeflow允许用户设置多个笔记本服务器，每个服务器可以运行多个笔记本。每个笔记本服务器属于一个单独的命名空间，具体取决于该服务器的项目或团队。Kubeflow通过命名空间提供多用户支持，这使得协作和管理访问权限变得更容易。在Kubeflow上使用笔记本允许您动态扩展资源。最好的部分是，它包含了您在Jupyter中训练模型可能需要的所有插件/依赖项，包括TensorBoard可视化以及您可能需要的自定义计算资源。Kubeflow笔记本在本地提供与Jupyter
    Notebook相同的体验，并增加了可扩展性、访问控制、协作以及直接提交作业到Kubernetes集群的额外好处。
- en: 'Kubeflow UI: A user interface that is used to run pipelines, create and start
    experiments, explore the graph, configuration, and the output of your pipeline,
    and even schedule runs.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow UI：一个用于运行管道、创建和启动实验、探索图、配置和管道输出的用户界面，甚至可以安排运行。
- en: 'Katib: Hyperparameter tuning is a pivotal step in AI/ML workflow. Finding the
    right hyperparameter space can take a lot of effort. Katib supports hyperparameter
    tuning, early stopping, and neural network architecture search. It helps one to
    find optimum configuration for production around the metrics of choice.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katib：超参数调优是人工智能/机器学习工作流程中的关键步骤。找到合适的超参数空间可能需要大量努力。Katib支持超参数调优、早期停止和神经网络架构搜索。它帮助用户在选择的指标周围找到最佳的生产配置。
- en: 'KubeFlow Pipelines: Kubeflow pipelines let you build a set of steps to do everything,
    from collecting data to serving the trained model. It is built upon containers,
    so each step is portable and scalable. You can use Kubeflow pipelines to orchestrate
    end-to-end ML workflow.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KubeFlow Pipelines：Kubeflow管道允许您构建一系列步骤，从收集数据到部署训练好的模型。它是基于容器的，因此每个步骤都是可移植和可扩展的。您可以使用Kubeflow管道编排端到端的机器学习工作流程。
- en: 'Metadata: It helps in tracking and managing the metadata that AI/ML workflows
    produce. This metadata logging can be used to evaluate models in real time. It
    can help in identifying data drift, or training-serving skew. It can also be used
    for audit and compliance - you can know which models are in production and how
    they are behaving. Metadata component is installed with Kubeflow by default. Many
    Kubeflow components write to the metadata server, additionally you can write to
    the metadata server using your code. You can use Kubeflow UI to see the metadata-
    through the artifact store.'
  id: totrans-85
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元数据：它有助于跟踪和管理人工智能/机器学习工作流程产生的元数据。这种元数据记录可用于实时评估模型。它可以帮助识别数据漂移或训练-服务偏差。它还可以用于审计和合规性——您可以了解哪些模型在生产中以及它们的运行情况。元数据组件默认安装于Kubeflow中。许多Kubeflow组件会写入元数据服务器，此外，您还可以使用代码向元数据服务器写入。您可以使用Kubeflow
    UI通过工件存储查看元数据。
- en: 'KFserving: It allows one to serve AI/ML models on arbitrary frameworks. It
    includes features like auto scaling, networking and canary rollouts. It provides
    an easy to use interface for serving models in production. Using a YAML file you
    can provision the resources for serving and computing. The canary rollout, allows
    you to test and update your models without impacting the user experience.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: KFserving：它允许在任意框架上提供AI/ML模型。它包括自动缩放、网络和金丝雀发布等功能。它提供了一个易于使用的界面来在生产中提供模型。使用YAML文件，您可以配置用于提供和计算的资源。金丝雀发布允许您在不影响用户体验的情况下测试和更新您的模型。
- en: 'Fairing: A python package which allows you to build , train and deploy your
    AI/ML models in hybrid cloud environments.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Fairing：一个Python包，允许您在混合云环境中构建、训练和部署您的AI/ML模型。
- en: To summarize, Kubeflow provides a curated set of compatible tools and artifacts
    that lie at the heart of running production-enabled AI/ML apps. It allows businesses
    to standardize on a common modeling infrastructure across the entire machine learning
    lifecycle. Let us take a deep dive into the core set of applications and components
    included in Kubeflow next.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，Kubeflow提供了一套经过精心挑选的兼容工具和工件，这些工具和工件是运行生产级AI/ML应用的核心。它允许企业在整个机器学习生命周期中采用通用的建模基础设施。让我们深入了解Kubeflow包含的核心应用和组件集。
- en: E.4.1 Kubeflow deep dive
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4.1 Kubeflow深入探讨
- en: By now you know how to deploy your Anthos environment with clusters, applications,
    Anthos Service Mesh, and Anthos Config Management. You have the project selected
    and Service Management APIs enabled. Also, verify that Istio ingress gateway service
    is enabled for traffic. This will be used by Anthos service mesh meshes to add
    more complex traffic routing to their inbound traffic. To continue the deep dive,
    you will need to install Kubeflow[^([12])](#ftn12). Since Kubeflow uses Kustomize[^([13])](#ftn13)
    to manage deployments across different environments. So the first task to be able
    to use Kubeflow in your cluster is to install Kustomize[^([14])](#ftn14).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，您已经知道如何使用集群、应用程序、Anthos Service Mesh和Anthos Config Management部署您的Anthos环境。您已经选择了项目并启用了服务管理API。此外，请确保启用了Istio
    ingress网关服务以处理流量。这将用于Anthos服务网格向其入站流量添加更复杂的路由。要继续深入探讨，您需要安装Kubeflow[^([12])](#ftn12)。由于Kubeflow使用Kustomize[^([13])](#ftn13)来管理不同环境中的部署。因此，要在您的集群中使用Kubeflow，首先要安装Kustomize[^([14])](#ftn14)。
- en: 'You can verify all the Kubeflow resources deployed on the cluster using kubectl
    get all, below you can see an excerpt of the output of get all command:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用kubectl get all命令验证集群上部署的所有Kubeflow资源，下面您可以查看get all命令的输出摘要：
- en: '![E_07](../../OEBPS/Images/E_07.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![E_07](../../OEBPS/Images/E_07.png)'
- en: Figure E.7 Output of kubectl get all command
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.7 kubectl get all命令的输出
- en: Let us now look deeply into different components and features of Kubeflow. You
    can also use Anthos Config Management to install and manage Kubeflow[^([15])](#ftn15).
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们深入探讨Kubeflow的不同组件和功能。您还可以使用Anthos Config Management来安装和管理Kubeflow[^([15])](#ftn15)。
- en: The easiest way is to try solutions from the GCP Marketplace like MiniKF or
    Kubeflow pipelines. The AI platform on GCP offers an easy graphical interface
    to create a cluster and install kubeflow pipelines for your ML/AI workflow. With
    just three clicks you can have a kubeflow cluster ready for use.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是尝试来自GCP Marketplace的解决方案，如MiniKF或Kubeflow pipelines。GCP上的AI平台提供了一个简单的图形界面，用于创建集群并为您的ML/AI工作流程安装kubeflow
    pipelines。只需点击三次，您就可以拥有一个可供使用的kubeflow集群。
- en: E.4.2 Kubeflow central dashboard
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4.2 Kubeflow中央仪表板
- en: Just like all other GCP services, Kubeflow has a central dashboard that provides
    a quick overview of the components installed in your cluster. The dashboard provides
    a cool graphical user interface that you can use to run pipelines, create and
    start experiments, explore the graph, configuration, and the output of your pipeline,
    and even schedule runs.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 就像所有其他GCP服务一样，Kubeflow有一个中央仪表板，它提供了对您集群中安装的组件的快速概述。仪表板提供了一个酷炫的图形用户界面，您可以使用它来运行管道、创建和启动实验、探索图、配置以及管道的输出，甚至安排运行。
- en: 'The Kubeflow central dashboard is accessible through the URL with the pattern:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow中央仪表板可以通过以下模式的URL访问：
- en: '[PRE0]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'It is also possible to access the UI using Kubeflow command line kubectl. You
    will first need to set up port forwarding[^([16])](#ftn16) to the Istio gateway
    using the command:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用Kubeflow命令行kubectl访问UI。您首先需要使用以下命令设置到Istio网关的端口转发[^([16])](#ftn16)：
- en: '[PRE1]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'and then access the central dashboard using:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后使用以下命令访问中央仪表板：
- en: '[PRE2]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Remember you should have an IAP-secured web app[^([17])](#ftn17) user role
    to be able to access Kubeflow UI. Also, you should have Istio Ingress configured
    to accept HTTPS traffic. The central dashboard includes the following:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，你应该有一个受 IAP 保护的 web 应用程序用户角色才能访问 Kubeflow UI。此外，你应该已配置 Istio Ingress 以接受
    HTTPS 流量。中心仪表板包括以下内容：
- en: 'Home: The central dashboard that can be used for navigation between various
    Kubeflow components.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主页：用于在各个 Kubeflow 组件之间导航的中心仪表板。
- en: Pipelines
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管道
- en: Notebook Servers
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 笔记本服务器
- en: Katib
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Katib
- en: Artifact Store
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 艺术品存储库
- en: Manage Contributors
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理贡献者
- en: '![E_08](../../OEBPS/Images/E_08.png)'
  id: totrans-111
  prefs: []
  type: TYPE_IMG
  zh: '![E_08](../../OEBPS/Images/E_08.png)'
- en: Figure E.8 Kubeflow central dashboard using MiniKF deployment
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.8 使用 MiniKF 部署的 Kubeflow 中心仪表板
- en: We will explore each of these components next.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一节中探讨这些组件。
- en: E.4.3 Kubeflow Pipelines
  id: totrans-114
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4.3 Kubeflow 管道
- en: Kubeflow pipelines, a service of Kubeflow, allows you to orchestrate your AI/ML
    workloads. It can be installed with Kubeflow or as a standalone. GCP marketplace
    offers an easy installation of Kubeflow Pipelines with a single click. As we have
    discussed, in the preceding sections - building an AI/ML solution is an iterative
    process, and hence it is important to track changes in an orderly, organized manner
    - keeping a track of changes- monitoring and versioning can be challenging. Kubeflow
    pipelines ease the process, by providing you with easily composable, shareable,
    and reproducible AI/ML workflows. Kubeflow pipelines allow one to completely automate
    the process of training and tuning the model. To do this, Kubeflow pipelines make
    use of the fact that machine learning processes can be broken down into a sequence
    of standard steps, and these steps can be arranged in a form of a directed graph
    (Figure E.8). While the process appears straightforward, what complicates the
    matter is, it is an iterative process, the AI scientist needs to experiment with
    multiple types of pre-processing, feature-extraction, type of model, etc. Each
    such experiment results in a different trained model, these different trained
    models are compared in terms of their performances on the chosen metrics. The
    best model is then saved and deployed to production. To be able to continuously
    perform this operation, DevOps requires that the infrastructure is flexible enough
    so that the multiple experiments can coexist in the same environment. Kubeflow
    enables this via Kubeflow pipelines. Each of the boxes in Figure E.9 is self-contained
    codes conceptualized as docker containers. Since containers are portable, each
    of these tasks inherits the same portability.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 管道，Kubeflow 的一项服务，允许你编排你的 AI/ML 工作负载。它可以与 Kubeflow 一起安装或作为独立服务安装。GCP
    市场提供了一个单点点击即可轻松安装 Kubeflow 管道的选项。正如我们前面所讨论的，构建 AI/ML 解决方案是一个迭代过程，因此以有序、组织的方式跟踪更改非常重要——跟踪更改、监控和版本控制可能具有挑战性。Kubeflow
    管道通过为你提供易于组合、共享和可重复的 AI/ML 工作流程来简化此过程。Kubeflow 管道允许完全自动化模型的训练和调整过程。为此，Kubeflow
    管道利用机器学习过程可以被分解成一系列标准步骤的事实，这些步骤可以以有向图的形式排列（图 E.8）。虽然过程看起来很简单，但使问题复杂化的是，它是一个迭代过程，AI
    科学家需要尝试多种预处理、特征提取、模型类型等。每次这样的实验都会产生一个不同的训练模型，这些不同的训练模型在所选指标上的性能进行比较。然后，最佳模型被保存并部署到生产环境中。为了能够持续执行此操作，DevOps
    需要基础设施足够灵活，以便多个实验可以在同一环境中共存。Kubeflow 通过 Kubeflow 管道实现了这一点。图 E.9 中的每个框都是一个作为 Docker
    容器概念化的自包含代码。由于容器是可移植的，因此每个任务都继承了相同的可移植性。
- en: '![E_09](../../OEBPS/Images/E_09.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![E_09](../../OEBPS/Images/E_09.png)'
- en: Figure E.9 The machine learning process as a directed graph
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.9 将机器学习过程作为有向图
- en: The containerization of the tasks provides portability, repeatability, and encapsulation.
    Each of these containerized tasks can invoke other GCP services like Dataflow,
    Dataproc, etc. Each task in the Kubeflow pipeline is a self-contained code packaged
    as a Docker image, with its inputs (arguments) and outputs. This containerization
    of tasks - allows portability- since they are self-contained code- you can run
    them anywhere. Moreover, you can use the same task in another AI/ML pipeline -
    the tasks can be reused.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 任务容器化提供了可移植性、可重复性和封装性。每个容器化任务都可以调用其他 GCP 服务，如 Dataflow、Dataproc 等。Kubeflow 管道中的每个任务都是一个自包含的代码，打包成
    Docker 镜像，包含其输入（参数）和输出。这种任务的容器化——允许可移植性——因为它们是自包含的代码——你可以在任何地方运行它们。此外，你可以在另一个
    AI/ML 管道中使用相同的任务——任务可以被重用。
- en: '![E_10](../../OEBPS/Images/E_10.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![E_10](../../OEBPS/Images/E_10.png)'
- en: Figure E.10 Sample Graph from Kubeflow Pipeline
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.10 Kubeflow 管道的示例图
- en: 'The Kubeflow pipeline platform consists of five main elements:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 管道平台由五个主要元素组成：
- en: A user interface for creating, managing, and tracking experiments, jobs, and
    runs.
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于创建、管理和跟踪实验、作业和运行的用户界面。
- en: It uses in background Kubernetes resources and makes use of Argo[^([18])](#ftn18),
    to orchestrate portable and scalable ML jobs on Kubernetes.
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在后台使用 Kubernetes 资源，并利用 Argo[^([18])](#ftn18)，在 Kubernetes 上编排可移植和可扩展的 ML 作业。
- en: It has a Python SDK, which is used to define and manipulate pipelines and components.
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它提供了一个 Python SDK，用于定义和操作管道和组件。
- en: Jupyter Notebooks which you can use to interact with the system using Python
    SDK.
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Jupyter Notebooks，您可以使用 Python SDK 与系统交互。
- en: And ML Metadata which stores information about different executions, models,
    datasets used, and other artifacts, in essence, metadata logging. This metadata
    logging allows you to visualize the metrics output and compare between different
    runs.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以及 ML 元数据，它存储有关不同执行、模型、数据集使用和其他工件的信息，本质上是一种元数据日志。这种元数据日志允许您可视化度量输出并比较不同运行的结果。
- en: The Python SDK allows one to describe the pipeline in code, one can also use
    the Kubeflow UI to visualize the pipeline and view different tasks. Configuring
    the ML pipeline as a containerized task arranged as a directed acyclic graph (DAG),
    enables one to run multiple experiments parallelly. Additionally, Kubeflow allows
    one to reuse pre-built codes, this saves a lot of time as there is no need to
    reinvent the wheel. GCP also offers AI Hub, which has a variety of plug-and-play
    reusable pipeline components. Figure E.9 shows a sample graph of Kubeflow Pipeline.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: Python SDK 允许用户通过代码描述管道，也可以使用 Kubeflow UI 可视化管道并查看不同的任务。将 ML 管道配置为容器化任务，以有向无环图（DAG）的形式排列，可以并行运行多个实验。此外，Kubeflow
    允许用户重用预构建的代码，这节省了大量时间，因为无需重新发明轮子。GCP 还提供了 AI Hub，其中包含各种即插即用的可重用管道组件。图 E.9 展示了
    Kubeflow 管道的示例图。
- en: Kubeflow uses domain-specific language (DSL) to describe your pipeline and components
    (Figure E.10). A Kubeflow component can be specified using kfp.dsl.pipeline decorator.
    It contains metadata fields where you can specify its name and purpose. The arguments
    to the component describe what this component will take as inputs. The body of
    the function describes the actual Kubeflow ops to be executed in the component.
    These Ops are the Docker containers that are executed when the task is run.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 使用领域特定语言（DSL）来描述您的管道和组件（图 E.10）。可以使用 kfp.dsl.pipeline 装饰器指定 Kubeflow
    组件。它包含元数据字段，您可以在此处指定其名称和用途。组件的参数描述了该组件将接受哪些输入。函数的主体描述了组件中实际要执行的 Kubeflow 操作。这些操作是在任务运行时执行的
    Docker 容器。
- en: '![E_11](../../OEBPS/Images/E_11.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![E_11](../../OEBPS/Images/E_11.png)'
- en: Figure E.11 Basic structure of Kubeflow Component
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.11 Kubeflow 组件的基本结构
- en: 'Kubeflow pipeline allows three types of components:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 管道支持三种类型的组件：
- en: 'Pre-built components: These are prebuilt components available at the GitHub
    repo: [https://github.com/kubeflow/pipelines/tree/master/components](https://github.com/kubeflow/pipelines/tree/master/components).
    There are a wide range of components available here for different platforms, from
    preprocessing, training to deploying a machine learning model. To use them you
    just require the URI to the component.yaml, which is the description of the component.
    It contains the URI of the container image and the component run parameters. These
    arguments are passed to the corresponding Kubeflow ops into pipeline code. One
    of the pre-built components can represent all the operations like training, tuning,
    and deploying a model.'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预构建组件：这些是在 GitHub 仓库中可用的预构建组件：[https://github.com/kubeflow/pipelines/tree/master/components](https://github.com/kubeflow/pipelines/tree/master/components)。这里提供了广泛的可用于不同平台的组件，从预处理、训练到部署机器学习模型。要使用它们，您只需要组件.yaml
    的 URI，这是组件的描述。它包含容器镜像的 URI 和组件运行参数。这些参数传递给管道代码中的相应 Kubeflow 操作。其中一个预构建组件可以代表所有操作，如训练、调优和部署模型。
- en: 'Lightweight Python Components: If you have small python functions, it doesn''t
    make sense to write full Dockerfiles for each, the Kubeflow SDK allows one to
    wrap these lightweight functions as Kubeflow components with the help of func_to_container_op
    helper function, defined in kfp.components. We pass the function as input to func_to_container_op
    and a base image.'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级Python组件：如果你有小的Python函数，为每个函数编写完整的Dockerfile是没有意义的，Kubeflow SDK允许你使用func_to_container_op辅助函数（在kfp.components中定义）将这些轻量级函数包装成Kubeflow组件。我们将函数作为输入传递给func_to_container_op和一个基础镜像。
- en: 'Custom components: In case we have functions written in other languages for
    example go, one can use Custom build components. In this case, you need to write
    the code that describes the behaviour of the component, the code that creates
    the container, and all the dependencies of the code.'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自定义组件：如果我们用其他语言（例如Go）编写了函数，可以使用自定义构建组件。在这种情况下，你需要编写描述组件行为的代码，创建容器的代码，以及代码的所有依赖项。
- en: 'Let us take a simple example to demonstrate how Kubeflow components can be
    created. As the first step you will need to define your component code as a standalone
    python function, for example, we define a function to multiply two numbers:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过一个简单的例子来演示如何创建Kubeflow组件。作为第一步，你需要将你的组件代码定义为独立的Python函数，例如，我们定义一个函数来乘以两个数字：
- en: '[PRE3]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'Next generate component specification YAML using kfp.components.create_from_func:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用kfp.components.create_from_func生成组件规范YAML。
- en: '[PRE4]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The YAML file is reusable, that is, you can share it with others, or reuse
    it in another AI/ML pipeline. And now you can create your pipeline:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: YAML文件是可重用的，也就是说，你可以与他人共享它，或者在其他AI/ML管道中重用它。现在你可以创建你的管道：
- en: '[PRE5]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Finally, you can create a pipeline run using create_run_from_pipeline_func().
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你可以使用create_run_from_pipeline_func()创建一个管道运行。
- en: E.4.4 Hyperparameter tuning using Katib
  id: totrans-142
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.4.4 使用Katib进行超参数调整
- en: Finding the right hyperparameters is very important in AI/ML workflow. AI scientists
    spend hours and sometimes days to find the right hyperparameters. The work involves
    frequent experimentation. It is often challenging and time-consuming. We can automate
    the process of hyperparameter tuning using one of the pre-built components as
    described in the previous section, alternatively, we can use Katib over Kubeflow
    to do the same.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在AI/ML工作流程中找到合适的超参数非常重要。AI科学家花费数小时甚至数天来寻找合适的超参数。这项工作涉及频繁的实验。这通常具有挑战性和耗时。我们可以使用前一个章节中描述的预构建组件之一来自动化超参数调整的过程，或者我们可以使用Kubeflow上的Katib来完成同样的工作。
- en: 'Katib is a scalable Kubernetes native AutoML platform, it allows both hyperparameter
    tuning and neural network architecture search. Figure E.12 shows the design of
    Katib. To learn more about how it works, readers should refer to the paper Katib:
    A distributed general automl platform on Kubernetes.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Katib是一个可扩展的Kubernetes原生AutoML平台，它允许进行超参数调整和神经网络架构搜索。图E.12显示了Katib的设计。要了解更多关于它的工作原理，读者应参考论文《Katib：基于Kubernetes的分布式通用AutoML平台》。
- en: '![E_12](../../OEBPS/Images/E_12.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![E_12](../../OEBPS/Images/E_12.png)'
- en: Figure E.12 Design of Katib as general AutoML system[[19]](#ftn19)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.12 Katib作为通用AutoML系统的设计[[19]](#ftn19)
- en: Katib allows you to define hyperparameter tuning using both command-line via
    a YAML file specification, or using Jupyter Notebook and the Python SDK. It also
    has a graphical user interface, which you can use to specify hyperparameters to
    be tuned and visualize the results.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: Katib允许你通过命令行（通过YAML文件规范）或使用Jupyter Notebook和Python SDK来定义超参数调整。它还提供了一个图形用户界面，你可以使用它来指定要调整的超参数并可视化结果。
- en: Figure E.13 shows the graphical interface of Katib.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.13显示了Katib的图形界面。
- en: '![E_13](../../OEBPS/Images/E_13.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![E_13](../../OEBPS/Images/E_13.png)'
- en: Figure E.13 Katib graphical interface
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.13 Katib图形界面
- en: Katib allows you to choose the metric and whether you want to minimize it or
    maximize it. You can specify the hyperparameters you want to tune. It allows you
    to visualize the results of the entire experiment as well as the results of the
    individual runs. Figure E.14 shows the result from a Katib run for validation
    accuracy as the metrics and learning rate, the number of layers, and optimizer
    as the hyperparameters to be tuned.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: Katib允许你选择指标以及你是否想要最小化它或最大化它。你可以指定你想要调整的超参数。它允许你可视化整个实验的结果以及单个运行的结果。图E.14显示了以验证准确率作为指标，学习率、层数和优化器作为要调整的超参数的Katib运行结果。
- en: '![E_14](../../OEBPS/Images/E_14.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![E_14](../../OEBPS/Images/E_14.png)'
- en: Figure E.14 Results of hyperparameter tuning generated by Katib
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 图E.14 Katib生成的超参数调整结果
- en: E.5 End to End ML on Kubeflow
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.5 在 Kubeflow 上进行端到端机器学习
- en: 'Now that we have learned about Kubeflow, it is time to put it into practice.
    We will be building a complete pipeline, from data ingestion to serving using
    Kubeflow. Since the aim of this chapter is to talk about Kubeflow, not the AI/ML
    models, we will work with the MNIST example and will train a basic model to classify
    the handwritten numerals of MNIST, the trained model will be deployed on a web
    interface. The complete code is available on the repo: [https://github.com/EnggSols/KubeFlow](https://github.com/EnggSols/KubeFlow).
    To keep it simple and cost-effective we will be using CPU-only training, we will
    use the command line Kubeflow.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经了解了 Kubeflow，是时候将其付诸实践了。我们将构建一个完整的管道，从数据摄取到使用 Kubeflow 的服务。由于本章的目的是讨论
    Kubeflow，而不是 AI/ML 模型，我们将使用 MNIST 示例，并训练一个基本的模型来分类 MNIST 的手写数字，训练好的模型将部署在 Web
    界面上。完整的代码可在仓库中找到：[https://github.com/EnggSols/KubeFlow](https://github.com/EnggSols/KubeFlow)。为了保持简单和成本效益，我们将使用仅
    CPU 的训练，我们将使用命令行 Kubeflow。
- en: 'Ensure all the environment variables are properly set, and GKE API is enabled.
    The trained model will be stored in a storage bucket. If you do not have it already
    create one using gsutil:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 确保所有环境变量都已正确设置，并且已启用 GKE API。训练好的模型将存储在存储桶中。如果您还没有，可以使用 gsutil 创建一个：
- en: '[PRE6]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Here BUCKET_NAME is the unique name across your entire GCS. We use the model.py
    file to train; it is fairly straightforward code. With very little variation for
    the Kubeflow platform, the program uploads the trained model to the specified
    path after training.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这里 BUCKET_NAME 是整个 GCS 中的唯一名称。我们使用 model.py 文件进行训练；这是一段相当简单的代码。对于 Kubeflow 平台，程序在训练后将训练好的模型上传到指定的路径，几乎没有变化。
- en: 'Now to perform the training on Kubeflow we will need first to build a container
    image, below is the Dockerfile we will use from the GitHub repo:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在要在 Kubeflow 上执行训练，我们首先需要构建一个容器镜像，以下是我们将从 GitHub 仓库中使用的 Dockerfile：
- en: '[PRE7]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'We use docker build command to build the container image. Once it is built,
    push the image to the Google container registry so that you can run it on your
    cluster. Before actually pushing the image you can check locally if the model
    is indeed running, using docker run:'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 docker build 命令来构建容器镜像。一旦构建完成，就将镜像推送到 Google 容器注册库，以便您可以在集群上运行它。在实际上传镜像之前，您可以使用
    docker run 命令在本地检查模型是否确实正在运行：
- en: '[PRE8]'
  id: totrans-162
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You should see training logs like shown in figure 14, this implies that training
    is working.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到如图 14 所示的训练日志，这表明训练正在工作。
- en: '![E_15](../../OEBPS/Images/E_15.png)'
  id: totrans-164
  prefs: []
  type: TYPE_IMG
  zh: '![E_15](../../OEBPS/Images/E_15.png)'
- en: Figure E.15 Training logs
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.15 训练日志
- en: If you see this log, it means you can safely push the image to the GCS registry.
    Now that the image is pushed we build a Kustomize YAML file by setting the required
    training parameters.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您看到这个日志，这意味着您可以安全地将镜像推送到 GCS 注册库。现在镜像已推送，我们通过设置所需的训练参数来构建一个 Kustomize YAML
    文件。
- en: '[PRE9]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Figure E.16 shows the screenshot of the output of the Kustomize build command.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.16 显示了 Kustomize 构建命令的输出截图。
- en: '![E_16](../../OEBPS/Images/E_16.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![E_16](../../OEBPS/Images/E_16.png)'
- en: Figure E.16 Kustomize build output
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.16 Kustomize 构建输出
- en: And finally we pipe this YAML manifest to kubectl, this deploys the training
    job to the cluster.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将这个 YAML 清单通过 kubectl 管道传递，这将在集群中部署训练作业。
- en: '[PRE10]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The training might take a few minutes, while the training is going on you can
    check the bucket to verify the trained model is uploaded there.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要几分钟，在训练进行时，您可以检查存储桶以验证训练好的模型是否已上传。
- en: '![E_17](../../OEBPS/Images/E_17.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![E_17](../../OEBPS/Images/E_17.png)'
- en: Figure E.17 Cloud Bucket, you can see the saved models listed here
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.17 云存储桶，您可以看到保存的模型在此列出
- en: 'Like earlier, create the Kustomize manifest YAML for serving the model, and
    deploy the model to the server. Now the only step left is to run the web UI, which
    you can do by using the web front manifest. Establish the port forwarding, so
    that you can directly access the cluster and see the web UI:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，创建用于服务的 Kustomize 清单 YAML，并将模型部署到服务器。现在剩下的唯一步骤是运行 Web UI，您可以通过使用 Web 前端清单来完成此操作。建立端口转发，以便您可以直接访问集群并查看
    Web UI：
- en: '![E_18](../../OEBPS/Images/E_18.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![E_18](../../OEBPS/Images/E_18.png)'
- en: Figure E.18 Deployed Model
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.18 已部署模型
- en: You can test it with random images, the end to end MNIST classifier is deployed.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用随机图像进行测试，端到端 MNIST 分类器已部署。
- en: E.6 Vertex AI
  id: totrans-180
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.6 Vertex AI
- en: 'Kubeflow allows you to orchestrate the MLOPs workflow, but one still needs
    to manage the Kubernetes cluster. An even better solution will be if we need not
    worry about the management of clusters at all: presenting Vertex AI Pipelines.
    Vertex AI provides tools for every step of the machine learning workflow: from
    managing datasets to different ways of training the model, evaluating, deploying,
    and making predictions. Vertex AI in short is a one-stop shop for AI needs. You
    can use the Kubeflow pipelines or TensorFlow extended pipelines in Vertex AI.
    Whether you are a beginner, with no code experience but have a great idea to use
    A I or a seasoned AI engineer, Vertex AI has something to offer to you. As a beginner
    you can use the AutoML feature offered by Vertex AI, you just load your data,
    use the data exploration tools provided by Vertex AI and train a model using AutoML.
    An experienced AI engineer can build their own training loops, train the model
    on the cloud and deploy it using endpoints. Additionally, one can train the model
    locally and use Vertex AI for just deployment and monitoring. In essence, Vertex
    AI provides a unified interface for the entire AI/ML workflow (Figure E.19).'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: Kubeflow 允许您编排 MLOps 工作流程，但仍然需要管理 Kubernetes 集群。更好的解决方案将是如果我们根本不需要担心集群的管理：介绍
    Vertex AI Pipelines。Vertex AI 为机器学习工作流程的每一步提供工具：从管理数据集到不同方式的模型训练、评估、部署和预测。简而言之，Vertex
    AI 是 AI 需求的一站式商店。您可以在 Vertex AI 中使用 Kubeflow 管道或 TensorFlow 扩展管道。无论您是初学者，没有代码经验但有很好的想法使用
    AI，还是经验丰富的 AI 工程师，Vertex AI 都有一些东西可以为您提供。作为一名初学者，您可以使用 Vertex AI 提供的 AutoML 功能，只需加载您的数据，使用
    Vertex AI 提供的数据探索工具，然后使用 AutoML 训练模型。经验丰富的 AI 工程师可以构建自己的训练循环，在云上训练模型并使用端点进行部署。此外，您可以在本地训练模型，仅使用
    Vertex AI 进行部署和监控。本质上，Vertex AI 为整个 AI/ML 工作流程提供了一个统一的接口（图 E.19）。
- en: '![E_19](../../OEBPS/Images/E_19.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![E_19](../../OEBPS/Images/E_19.png)'
- en: Figure E.19 Vertex AI, a unified interface for complete AI/ML workflow
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.19 Vertex AI，完整的 AI/ML 工作流程统一接口
- en: Figure E.20 shows the vertex AI dashboard, in the following subsections we will
    explore some of the important elements available in the dashboard.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.20 展示了 vertex AI 仪表板，在以下小节中，我们将探讨仪表板中的一些重要元素。
- en: '![E_20](../../OEBPS/Images/E_20.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![E_20](../../OEBPS/Images/E_20.png)'
- en: Figure E.20 Vertex AI Dashboard
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.20 Vertex AI 仪表板
- en: E.6.1 Datasets
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6.1 数据集
- en: 'Vertex AI supports four types of managed data types: image, video, text, and
    tabular data. The table below lists the AI/ML tasks supported by vertex-managed
    datasets.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 支持四种类型的管理数据类型：图像、视频、文本和表格数据。下表列出了 vertex-managed 数据集支持的 AI/ML 任务。
- en: '| Type of Data | Tasks Supported |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 数据类型 | 支持的任务 |'
- en: '| Image | Image classification (single label)Image classification (multi-label)Image
    object detectionImage segmentation |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| 图像 | 图像分类（单标签）图像分类（多标签）图像目标检测图像分割 |'
- en: '| Video | Video action recognitionVideo classificationVideo object tracking
    |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 视频 | 视频动作识别视频分类视频目标跟踪 |'
- en: '| Text | Text classification (single label)Text classification (multi-label)Text
    entity extractionText sentiment analysis |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| 文本 | 文本分类（单标签）文本分类（多标签）文本实体提取文本情感分析 |'
- en: '| Tabular | RegressionClassificationForecasting |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| 表格 | 回归分类预测 |'
- en: For image, video, and text data set, in case you do not have labels- you can
    upload the files directly from your computer. In case there is a file that contains
    image URI and its labels- you can import the files from your computer. Additionally,
    you can import data from Google cloud storage. Please remember that uploaded data
    will make use of Google Cloud Storage to store the files you upload from your
    computer. For tabular data, Vertex AI supports only csv files; you can upload
    a csv file from your computer, from cloud storage, or import a table or view from
    BigQuery.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像、视频和文本数据集，如果您没有标签，您可以直接从您的计算机上传文件。如果有包含图像 URI 及其标签的文件，您可以从您的计算机导入文件。此外，您还可以从
    Google 云存储导入数据。请记住，上传的数据将使用 Google Cloud Storage 存储您从计算机上传的文件。对于表格数据，Vertex AI
    仅支持 csv 文件；您可以从您的计算机、云存储上传 csv 文件，或从 BigQuery 导入表或视图。
- en: Once the data is specified, the Vertex AI allows you to browse and analyze the
    data. If the data is not labeled, you can browse the data on the browser itself
    and assign labels. Additionally, the Vertex AI allows you to either do the test-training
    validation split manually or automatically. Figure E.21 shows the analysis of
    the Titanic survival dataset using vertex AI-managed datasets service.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦指定了数据，Vertex AI 允许您浏览和分析数据。如果数据未标记，您可以在浏览器本身上浏览数据并分配标签。此外，Vertex AI 允许您手动或自动进行测试-训练验证分割。图
    E.21 显示了使用 vertex AI 管理数据集服务对泰坦尼克号生存数据集进行分析。
- en: Vertex AI also provides you an option of Feature Store, which you can use to
    analyze the features of your data, it can help in mitigating the training serving
    skew- by ensuring that the same feature data distribution is used for training
    and serving. Feature Store can also help in detecting model/data drift. And if
    one requires a data annotation service- that is also available via Vertex AI.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 还为您提供了一个功能存储选项，您可以使用它来分析数据的特征，它可以帮助缓解训练/服务偏差 - 确保训练和服务的特征数据分布相同。功能存储还可以帮助检测模型/数据漂移。如果需要数据标注服务
    - 这也通过 Vertex AI 提供。
- en: E.6.2 Training and Experiments
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6.2 训练和实验
- en: Training tab lists all the training jobs you are doing and have done in the
    Vertex AI platform. You can also use it to initiate a new training pipeline. The
    whole process is straightforward; just click on create and follow the instructions
    on the screen. If you choose AutoML, then you get the option of choosing which
    features to use for training and which ones to ignore; you can also mention the
    transformations on the tabular data. One also has the option of selecting the
    objective function. After making all the selections, just decide the maximum budget
    you want to allocate for training (minimum being 1 hour) and start training. It
    is always better to use the Early stopping option so that if there is no improvement
    in the model’s performance, training stops (Figure E.22).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 训练选项卡列出了您在 Vertex AI 平台上正在进行的和已经完成的全部训练作业。您还可以使用它来启动一个新的训练流程。整个过程非常直接；只需点击创建并遵循屏幕上的说明。如果您选择
    AutoML，那么您可以选择用于训练的特征以及要忽略的特征；您还可以在表格数据选项卡上提及转换。还有一个选项可以选择目标函数。在做出所有选择后，只需决定您想要分配给训练的最大预算（最小为
    1 小时）并开始训练。始终使用早期停止选项会更好，这样如果模型性能没有改进，训练就会停止（图 E.22）。
- en: Experiment lets you track, visualize, and compare machine learning experiments
    and share them with others.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 实验（Experiment）让您跟踪、可视化和比较机器学习实验，并与他人分享。
- en: '![E_21](../../OEBPS/Images/E_21.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![E_21](../../OEBPS/Images/E_21.png)'
- en: Figure E.21 Choosing training parameters and transformations
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.21 选择训练参数和转换
- en: '![E_22](../../OEBPS/Images/E_22.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![E_22](../../OEBPS/Images/E_22.png)'
- en: Figure E.22 Selecting budget and early stopping
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.22 选择预算和早期停止
- en: For the purpose of demonstration we used the HR analytics data[^([20])](#ftn20)
    to predict whether a data scientist will go for a job change or not. We use all
    data except the enrolled id for training the model. The data files contain a target
    column that tells if a data scientist is looking for a job or not.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 为了演示目的，我们使用了 HR 分析数据[^([20])](#ftn20) 来预测数据科学家是否会进行工作变动。我们使用除了注册 ID 以外的所有数据进行模型训练。数据文件包含一个目标列，说明数据科学家是否在寻找工作。
- en: E.6.3 Models and Endpoint
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6.3 模型和端点
- en: All the trained model details are provided in the Models tab. The models include
    the evaluation of the model on the test dataset (when trained using managed datasets).
    Not only this, but in the case of tabular data, one can also see the feature importance
    of all the input features. The information is available directly on the dashboard
    in both visual and text form.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 所有训练模型的详细信息都提供在“模型”选项卡中。这些模型包括在测试数据集上对模型的评估（当使用托管数据集进行训练时）。不仅如此，在表格数据的情况下，还可以看到所有输入特征的特征重要性。这些信息以视觉和文本形式直接在仪表板上提供。
- en: We had set a 1 node hour budget for model training. It took about 1 hour 35
    minutes for the training to complete. Figure E.23 shows the model evaluation of
    the model trained by AutoML on the test dataset and Figure E.24 shows the associated
    confusion matrix.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为模型训练设置了 1 个节点小时的预算。训练完成大约需要 1 小时 35 分钟。图 E.23 显示了 AutoML 在测试数据集上训练的模型评估，图
    E.24 显示了相关的混淆矩阵。
- en: '![E_23](../../OEBPS/Images/E_23.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![E_23](../../OEBPS/Images/E_23.png)'
- en: Figure E.23 Model Evaluation on the test dataset
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.23 在测试数据集上对模型的评估
- en: '![E_24](../../OEBPS/Images/E_24.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![E_24](../../OEBPS/Images/E_24.png)'
- en: Figure E.24 Confusion matrix and feature importance
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.24 混淆矩阵和特征重要性
- en: 'One can check the prediction of the model directly from the dashboard. To test
    the model, the model needs to be deployed to the endpoint. Vertex AI also gives
    the option to save the model (TensorFlow SavedModel format) in a container, which
    you can use to launch your model in any other service on-prem or cloud. Let us
    choose to deploy the model, click on the Deploy to Endpoint button. To deploy
    you will need to select the following options:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 可以直接从仪表板检查模型的预测。要测试模型，需要将模型部署到端点。Vertex AI 还提供了将模型（TensorFlow SavedModel 格式）保存到容器中的选项，您可以使用它来在其他本地或云服务中启动模型。让我们选择部署模型，点击“部署到端点”按钮。部署时，您需要选择以下选项：
- en: Give a name to the endpoint.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为端点命名。
- en: Choose traffic split, for a single model it is 100%, but if you have more than
    one model you can split the traffic.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择流量分配，对于单个模型是 100%，但如果您有多个模型，则可以分割流量。
- en: Choose the minimum number of compute nodes.
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择最小计算节点数。
- en: Select the machine type.
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择机器类型。
- en: Select if you require a model explainability option, for the tabular data Vertex
    AI offers sampled Shapley explainability method.
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择是否需要模型可解释性选项，对于表格数据，Vertex AI 提供了样本 Shapley 可解释性方法。
- en: Choose if you want to monitor the model for feature drift, training-serving
    skew, and set alert thresholds.
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择是否要监控模型的特征漂移、训练-服务偏差，并设置警报阈值。
- en: Once done it takes a few minutes to deploy. Now we are ready to test the prediction
    batch predictions are also supported). In Figure E.25, you can see that for the
    inputs selected, the data scientist is not looking for a job with a confidence
    level of 0.67\. The sample request to the model can be made using REST API or
    through a Python client. Vertex AI endpoint includes the necessary code for both
    sample requests.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 部署完成后需要几分钟时间。现在我们已准备好测试预测（批量预测也受支持）。在图 E.25 中，您可以看到对于所选输入，数据科学家不寻求置信度为 0.67
    的职位。可以使用 REST API 或通过 Python 客户端向模型发出样本请求。Vertex AI 端点包括用于样本请求的必要代码。
- en: '![E_25](../../OEBPS/Images/E_25.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![E_25](../../OEBPS/Images/E_25.png)'
- en: Figure E.25 Model Prediction
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.25 模型预测
- en: All the models in your project and endpoint deployed in the project are listed
    in the Model and Endpoint tabs dashboards respectively.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 您项目中的所有模型和端点分别列在“模型”和“端点”选项卡的仪表板中。
- en: E.6.4 Workbench
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6.4 工作台
- en: Vertex AI provides JupyterLab and Notebook support via the workbench. The user
    has the option of Managed Notebooks or custom Notebooks. The managed notebooks
    contain all the popular deep learning frameworks and modules, you can also add
    your own Jupyter Kernels using the docker images. The User managed notebooks offer
    a wide range of base environments. Users have an option to choose vCPUs and GPU
    while setting up the notebook. The Managed Notebooks are a good place if you want
    to start using Vertex AI, the User-Managed Notebooks are good if you want better
    control over the environment. Once the notebook is created, click on the Open
    JupyterLab link to access your Jupyter Lab environment.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 通过工作台提供 JupyterLab 和笔记本支持。用户可以选择托管笔记本或自定义笔记本。托管笔记本包含所有流行的深度学习框架和模块，您还可以使用
    Docker 镜像添加自己的 Jupyter Kernels。用户管理的笔记本提供广泛的基环境。用户在设置笔记本时可以选择 vCPUs 和 GPU。如果您想开始使用
    Vertex AI，托管笔记本是个不错的选择；如果您想更好地控制环境，用户管理的笔记本是个好选择。一旦创建笔记本，点击“打开 JupyterLab”链接即可访问您的
    Jupyter Lab 环境。
- en: '![E_26](../../OEBPS/Images/E_26.png)'
  id: totrans-225
  prefs: []
  type: TYPE_IMG
  zh: '![E_26](../../OEBPS/Images/E_26.png)'
- en: Figure E.26 Managed Notebooks in Google Cloud Console
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 图 E.26 Google Cloud Console 中的托管笔记本
- en: The Vertex AI workbench can be used to further explore the data, build and train
    a model, and run the code as a part of the TFx or Kubeflow pipeline.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 工作台可用于进一步探索数据、构建和训练模型，以及作为 TFx 或 Kubeflow 管道的一部分运行代码。
- en: E.6.5 Vertex AI- Final words
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.6.5 Vertex AI- 最后的话
- en: Vetex AI provides a single interface with all the components of AI/ML workflow.
    You can set up pipelines to train the model and run many experiments. The interface
    provides an easy interface for hyperparameter tuning. Users can opt for custom
    training, where the user can select from containers and directly load their code
    for training code on their selected machine. To expedite the ML workflow, VertexAI
    also has AutoML integration. For managed datasets, you can use the AutoML feature
    to get an efficient ML model with the least ML expertise. Vertex AI also offers
    explainability to the models using feature attribution. Lastly, with your model
    available, you can set endpoints for batch or single prediction and deploy your
    model. The most important feature which I find while deploying is that you can
    even deploy on Edge devices- take your model where the data is.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: Vertex AI 提供了一个包含 AI/ML 工作流程所有组件的单个接口。您可以设置管道来训练模型并运行多个实验。该接口提供了一个简单的界面用于超参数调整。用户可以选择自定义训练，用户可以从容器中选择并直接在所选机器上加载他们的训练代码。为了加速机器学习工作流程，VertexAI
    还集成了 AutoML。对于托管数据集，您可以使用 AutoML 功能获得一个高效的机器学习模型，而无需具备最低的机器学习专业知识。Vertex AI 还提供了使用特征归因对模型的可解释性。最后，当您的模型可用时，您可以设置批处理或单次预测的端点并部署您的模型。我在部署时发现的最重要功能是，您甚至可以在边缘设备上部署——将您的模型带到数据所在的地方。
- en: E.7 Summary
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: E.7 摘要
- en: Present AI/ML workflows introduce technical debt making it necessary to employ
    MLOPs tools.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现今的 AI/ML 工作流程引入了技术债务，这使得采用 MLOps 工具变得必要。
- en: 'GCP provides a variety of solutions for MLOps, namely: Cloud Run, TensorFlow
    Extend, and Kubeflow. The chapter dives deep into Kubeflow, a cloud-native solution
    to orchestrate your ML workflow on Kubernetes.'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GCP 提供了各种 MLOps 解决方案，包括：Cloud Run、TensorFlow Extend 和 Kubeflow。本章深入探讨了 Kubeflow，这是在
    Kubernetes 上编排您的机器学习工作流程的云原生解决方案。
- en: Kubeflow provides a curated set of compatible tools and artifacts that lie at
    the heart of running production-enabled AI/ML apps. It allows businesses to standardize
    on a common modeling infrastructure across the entire machine learning lifecycle.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubeflow 提供了一套经过精心挑选的兼容工具和工件，这些工具和工件是运行生产级 AI/ML 应用程序的核心。它允许企业在整个机器学习生命周期中标准化通用的建模基础设施。
- en: Vertex AI provides an integrated solution for the entire AI/ML workflow. The
    features of Vertex AI are demonstrated by training a model using AutoML on the
    HR analytics dataset.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vertex AI 为整个 AI/ML 工作流程提供了一体化解决方案。Vertex AI 的功能通过在 HR 分析数据集上使用 AutoML 训练模型来展示。
- en: E.7.1 References
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: E.7.1 参考文献
- en: Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar
    Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, and Dan Dennison.
    "Hidden technical debt in machine learning systems." In Advances in neural information
    processing systems, pp. 2503-2511\. 2015.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar
    Ebner, Vinay Chaudhary, Michael Young, Jean-Francois Crespo, 和 Dan Dennison. "机器学习系统中的隐藏技术债务。"
    收录于《神经信息处理系统进展》，第 2503-2511 页。2015。
- en: Quionero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, and Neil D.
    Lawrence. Dataset shift in machine learning. The MIT Press, 2009.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Quionero-Candela, Joaquin, Masashi Sugiyama, Anton Schwaighofer, 和 Neil D. Lawrence.
    《机器学习中的数据集偏移》。麻省理工学院出版社，2009。
- en: 'Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar
    Ebner, Vinay Chaudhary, and Michael Young. "Machine learning: The high interest
    credit card of technical debt." (2014).'
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: Sculley, David, Gary Holt, Daniel Golovin, Eugene Davydov, Todd Phillips, Dietmar
    Ebner, Vinay Chaudhary, 和 Michael Young. "机器学习：技术债务的高利率信用卡。" (2014).
- en: 'Zhou, Jinan, et al. "Katib: A distributed general automl platform on kubernetes."
    2019 {USENIX} Conference on Operational Machine Learning (OpML 19). 2019.'
  id: totrans-239
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 周金南，等人。 "Katib：基于 Kubernetes 的分布式通用 AutoML 平台。" 2019 {USENIX} 操作机器学习会议 (OpML
    19)。2019。
- en: '* * *'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[^([1])](#ftnref1) Adapted from [1]'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([1])](#ftnref1) 改编自 [1]'
- en: '[^([2])](#ftnref2) [https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew)'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([2])](#ftnref2) [https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew](https://developers.google.com/machine-learning/guides/rules-of-ml/#training-serving_skew)'
- en: '[^([3])](#ftnref3) Image source: [https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([3])](#ftnref3) 图片来源：[https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning](https://cloud.google.com/solutions/machine-learning/mlops-continuous-delivery-and-automation-pipelines-in-machine-learning)'
- en: '[^([4])](#ftnref4) [https://www.forbes.com/sites/forbestechcouncil/2019/04/03/why-machine-learning-models-crash-and-burn-in-production/#64ca83e92f43](https://www.forbes.com/sites/forbestechcouncil/2019/04/03/why-machine-learning-models-crash-and-burn-in-production/#64ca83e92f43)'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([4])](#ftnref4) [https://www.forbes.com/sites/forbestechcouncil/2019/04/03/why-machine-learning-models-crash-and-burn-in-production/#64ca83e92f43](https://www.forbes.com/sites/forbestechcouncil/2019/04/03/why-machine-learning-models-crash-and-burn-in-production/#64ca83e92f43)'
- en: '[^([5])](#ftnref5) [https://thepoweroftwo.solutions/overview/](https://thepoweroftwo.solutions/overview/)'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([5])](#ftnref5) [https://thepoweroftwo.solutions/overview/](https://thepoweroftwo.solutions/overview/)'
- en: '[^([6])](#ftnref6) [https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([6])](#ftnref6) [https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/](https://www.wired.com/story/artificial-intelligence-confronts-reproducibility-crisis/)'
- en: '[^([7])](#ftnref7) [https://cloud.google.com/blog/products/serverless/cloud-run-gets-websockets-http-2-and-grpc-bidirectional-streams](https://cloud.google.com/blog/products/serverless/cloud-run-gets-websockets-http-2-and-grpc-bidirectional-streams)'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([7])](#ftnref7) [https://cloud.google.com/blog/products/serverless/cloud-run-gets-websockets-http-2-and-grpc-bidirectional-streams](https://cloud.google.com/blog/products/serverless/cloud-run-gets-websockets-http-2-and-grpc-bidirectional-streams)'
- en: '[^([8])](#ftnref8) [https://cloud.google.com/run/quotas](https://cloud.google.com/run/quotas)
    (The option to increase up to 8vCPUs was available as preview at the time of writing
    this book'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([8])](#ftnref8) [https://cloud.google.com/run/quotas](https://cloud.google.com/run/quotas)
    (在撰写本书时，增加至8vCPUs的选项作为预览版可用'
- en: '[^([9])](#ftnref9) TF Dev Summit 2019: TensorFlow Extended Overview and Pre-Training
    Workflow'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([9])](#ftnref9) TF Dev Summit 2019: TensorFlow Extended Overview and Pre-Training
    Workflow'
- en: '[^([10])](#ftnref10) Image source: [https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build](https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build)'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([10])](#ftnref10) 图片来源：[https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build](https://cloud.google.com/solutions/machine-learning/architecture-for-mlops-using-tfx-kubeflow-pipelines-and-cloud-build)'
- en: '[^([11])](#ftnref11) [https://www.altexsoft.com/blog/datascience/the-best-machine-learning-tools-experts-top-picks/](https://www.altexsoft.com/blog/datascience/the-best-machine-learning-tools-experts-top-picks/)'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([11])](#ftnref11) [https://www.altexsoft.com/blog/datascience/the-best-machine-learning-tools-experts-top-picks/](https://www.altexsoft.com/blog/datascience/the-best-machine-learning-tools-experts-top-picks/)'
- en: '[^([12])](#ftnref12) Kindly refer to the Kubeflow documentation for latest
    installation instructions: [https://www.kubeflow.org/docs/started/installing-kubeflow/](https://www.kubeflow.org/docs/started/installing-kubeflow/)'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([12])](#ftnref12) 请参考Kubeflow文档以获取最新的安装说明：[https://www.kubeflow.org/docs/started/installing-kubeflow/](https://www.kubeflow.org/docs/started/installing-kubeflow/)'
- en: '[^([13])](#ftnref13) [https://github.com/kubernetes-sigs/kustomize](https://github.com/kubernetes-sigs/kustomize)'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([13])](#ftnref13) [https://github.com/kubernetes-sigs/kustomize](https://github.com/kubernetes-sigs/kustomize)'
- en: '[^([14])](#ftnref14) Take note of the Kustomize version, Kubeflow is not compatible
    with later versions of Kustomize, to know the latest status refer to this GitHub
    issue: [https://github.com/kubeflow/manifests/issues/538](https://github.com/kubeflow/manifests/issues/538)'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([14])](#ftnref14) 请注意Kustomize的版本，Kubeflow与Kustomize的后续版本不兼容，要了解最新状态，请参阅此GitHub问题：[https://github.com/kubeflow/manifests/issues/538](https://github.com/kubeflow/manifests/issues/538)'
- en: '[^([15])](#ftnref15) [https://github.com/kubeflow/gcp-blueprints/blob/master/kubeflow/README.md#gitopswork-in-progress-using-anthos-config-managment-to-install-and-manage-kubeflow](https://github.com/kubeflow/gcp-blueprints/blob/master/kubeflow/README.md#gitopswork-in-progress-using-anthos-config-managment-to-install-and-manage-kubeflow)'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([15])](#ftnref15) [https://github.com/kubeflow/gcp-blueprints/blob/master/kubeflow/README.md#gitopswork-in-progress-using-anthos-config-managment-to-install-and-manage-kubeflow](https://github.com/kubeflow/gcp-blueprints/blob/master/kubeflow/README.md#gitopswork-in-progress-using-anthos-config-managment-to-install-and-manage-kubeflow)'
- en: '[^([16])](#ftnref16) Please remember not all UIs work behind port-forwarding
    to the reverse proxy, it depends on how you have configured Kubeflow.'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([16])](#ftnref16) 请记住，并非所有UI都支持通过反向代理进行端口转发，这取决于您如何配置Kubeflow。'
- en: '[^([17])](#ftnref17) This will grant access to the app and other HTTPS resources
    that use IAP.'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([17])](#ftnref17) 这将授予访问应用程序和其他使用IAP的HTTPS资源的权限。'
- en: '[^([18])](#ftnref18)[https://argoproj.github.io](https://argoproj.github.io/projects/argo/)'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([18])](#ftnref18)[https://argoproj.github.io](https://argoproj.github.io/projects/argo/)'
- en: '[^([19])](#ftnref19) From the paper Zhou, Jinan, et al. "Katib: A distributed
    general automl platform on kubernetes." *2019 {USENIX} Conference on Operational
    Machine Learning (OpML 19).* 2019.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([19])](#ftnref19) 来自论文周金南等人撰写的 "Katib: A distributed general automl platform
    on kubernetes." *2019 {USENIX} Conference on Operational Machine Learning (OpML
    19).* 2019.'
- en: '[^([20])](#ftnref20) [https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv)'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([20])](#ftnref20) [https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv](https://www.kaggle.com/arashnic/hr-analytics-job-change-of-data-scientists?select=aug_train.csv)'
