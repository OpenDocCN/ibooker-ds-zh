- en: 17 Compute environment running on bare metal
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 17 在裸金属上运行的计算环境
- en: Giovanni Galloro
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 乔治·加洛罗
- en: This chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: An introduction to Anthos on bare metal
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裸金属 Anthos 简介
- en: Deployment options
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署选项
- en: Networking architecture
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络架构
- en: Storage architecture
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储架构
- en: Installing and configuring Anthos on bare metal
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在裸金属上安装和配置 Anthos
- en: The original release of Anthos required you to deploy your clusters on a vSphere
    infrastructure and didn’t offer the option to deploy on a different hypervisor
    or to physical servers. For the initial release, this made sense because vSphere
    is used by numerous enterprises, and it allowed businesses to use their existing
    infrastructure and skill sets. As the use cases for containers and Kubernetes
    grew, however, it became clear that organizations wanted, and needed, more flexible
    deployment options.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 的原始发布版本要求您在 vSphere 基础设施上部署您的集群，并且不提供在不同的虚拟机管理程序或物理服务器上部署的选项。对于初始发布版本来说，这样做是有道理的，因为
    vSphere 被众多企业使用，这允许企业使用他们现有的基础设施和技能集。然而，随着容器和 Kubernetes 用例的增长，组织越来越希望并且需要更灵活的部署选项。
- en: To address these additional use cases, Google expanded Anthos to include a bare
    metal deployment model. One point to highlight is that you do not have to deploy
    Anthos on bare metal to actual physical servers. The bare metal model allows you
    to deploy to any supported operating system, whether a physical server or virtual
    machine, or even VMs running on Hyper-V or KVM.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这些额外的用例，Google 将 Anthos 扩展到包括裸金属部署模型。需要强调的一点是，您不必在裸金属上实际部署 Anthos 到物理服务器。裸金属模型允许您将部署到任何支持的操作系统，无论是物理服务器还是虚拟机，甚至是运行在
    Hyper-V 或 KVM 上的虚拟机。
- en: You can think of the bare metal option as a “bring your own Linux” deployment
    model. Rather than having an appliance to deploy your nodes, like the vSphere
    deployment model, you need to provide ready-to-use servers before you can deploy
    Anthos on bare metal. Now let’s introduce you to Anthos on bare metal.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将裸金属选项视为“自带 Linux”的部署模型。与 vSphere 部署模型不同，您需要提供可用的服务器，然后才能在裸金属上部署 Anthos。现在让我们向您介绍裸金属
    Anthos。
- en: 17.1 Introduction to Anthos on bare metal
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.1 裸金属 Anthos 简介
- en: As described in previous chapters, Anthos is a platform designed for multiple
    deployment environments, as summarized in figure 17.1.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 如前几章所述，Anthos 是一个为多个部署环境设计的平台，如图 17.1 所总结。
- en: '![17-01](../../OEBPS/Images/17-01.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![17-01](../../OEBPS/Images/17-01.png)'
- en: Figure 17.1 Anthos deployment environments
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.1 Anthos 部署环境
- en: Anthos on bare metal is a deployment option to run Anthos on physical servers,
    deployed on an operating system provided by the customer. It ships with built-in
    networking, life cycle management, diagnostics, health checks, logging, and monitoring.
    Additionally, it supports CentOS, Red Hat Enterprise Linux (RHEL), and Ubuntu,
    all validated by Google. With Anthos on bare metal, you can use an organization’s
    standard hardware and operating system images, taking advantage of existing investments,
    which are automatically checked and validated against Anthos infrastructure requirements.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 裸金属上的 Anthos 是一种部署选项，可以在客户提供的操作系统上运行的物理服务器上运行 Anthos。它内置了网络、生命周期管理、诊断、健康检查、日志记录和监控功能。此外，它支持
    CentOS、Red Hat Enterprise Linux (RHEL) 和 Ubuntu，所有这些都经过 Google 验证。使用裸金属上的 Anthos，您可以使用组织的标准硬件和操作系统镜像，利用现有投资，这些投资将自动检查并验证是否符合
    Anthos 基础设施要求。
- en: 17.1.1 Comparing Anthos on-prem deployment options
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.1.1 比较本地 Anthos 部署选项
- en: Now that you have options to deploy Anthos on-prem, how do you decide which
    is the best for your organization? Both options have their own advantages and
    disadvantages, and to decide which deployment is best for you, you’ll need to
    consider your personal requirements. In table 17.1, we have provided an overview
    of some advantages and disadvantages of each option.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有选择在本地部署 Anthos 的选项，那么您如何决定哪种最适合您的组织？这两种选项都有其自身的优缺点，为了决定哪种部署最适合您，您需要考虑您个人的需求。在表
    17.1 中，我们概述了每个选项的一些优缺点。
- en: Table 17.1 Advantages and disadvantages of Anthos on VMware and Anthos on bare
    metal
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.1 Anthos 在 VMware 和裸金属上的优缺点
- en: '| Anthos on VMware | Anthos on bare metal |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| VMware 上的 Anthos | 裸金属上的 Anthos |'
- en: '| Runs on VMwareBest for organizations who want'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在 VMware 上运行最佳适用于'
- en: vSphere as a corporate standard
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere 作为企业标准
- en: Hardware shared across multiple teams or clusters (Dev/Test)
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 涉及多个团队或集群（开发/测试）共享硬件
- en: Integrated OS life cycle management
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成操作系统生命周期管理
- en: Self-healing/autoscaling for clusters
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群的自愈/自动扩展
- en: '| Runs on bare-metal or on-prem IaaSBest for organizations who want'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: '| 在裸金属或本地 IaaS 上运行，最适合希望'
- en: Reduced cost and complexity (due to elimination of the vSphere license)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低成本和复杂性（由于消除了 vSphere 许可证）
- en: Low-latency workloads (telco and high-performance computing)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低延迟工作负载（电信和高性能计算）
- en: To unlock new use cases for edge computing with simplified software stack
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了通过简化的软件堆栈解锁边缘计算的新的用例
- en: To run closer to the hardware for better performance
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了更接近硬件以获得更好的性能
- en: '|'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deployment advantages |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 部署优点 |'
- en: '|'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Easier to deploy multiple clusters
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署多个集群更容易
- en: Provided node appliance requires little maintenance
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提供的节点设备需要很少的维护
- en: Includes two vSphere storage providers for persistent disks
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包括两个用于持久磁盘的 vSphere 存储提供程序
- en: Node autohealing
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 节点自动修复
- en: Easy to scale cluster nodes up or out
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 容易扩展集群节点
- en: '|'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Can be deployed to any supported Linux node, on-prem, or in a CSP
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以部署到任何支持的 Linux 节点，本地或云服务提供商
- en: No workload scheduling conflicts
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有工作负载调度冲突
- en: Expanded GPU compatibility
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩展 GPU 兼容性
- en: Allows node customizations to meet an organization’s requirements
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 允许节点定制以满足组织的需求
- en: Better node performance
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 更好的节点性能
- en: Uses existing corporate standards (e.g., logging and monitoring standards)
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用现有的企业标准（例如，日志记录和监控标准）
- en: '|'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '| Deployment disadvantages |'
  id: totrans-46
  prefs: []
  type: TYPE_TB
  zh: '| 部署缺点 |'
- en: '|'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Requires additional VMware licensing.
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要额外的 VMware 许可证。
- en: Customization of node appliance not supported.
  id: totrans-49
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不支持节点设备的定制。
- en: Limited GPU supported through pass-through.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过直通支持有限的 GPU。
- en: Requires additional training for either the VM teams or the Kubernetes support
    teams.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要对虚拟机团队或 Kubernetes 支持团队进行额外的培训。
- en: The vSphere scheduler and Kubernetes scheduler are not aware of each other.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere 调度程序和 Kubernetes 调度程序互不认识。
- en: Storage DRS can break your cluster.
  id: totrans-53
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storage DRS 可能会破坏您的集群。
- en: '|'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: Requires planning to right-size nodes to avoid wasted resources
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要规划以正确调整节点大小，以避免资源浪费
- en: Does not include a storage provisioner other than local host storage
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了本地主机存储外，不包括其他存储分配器
- en: Difficult to scale in most enterprises
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在大多数企业中难以扩展
- en: No node autohealing
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有节点自动修复
- en: Managing and updating nodes underlying the OS
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理和更新操作系统底层的节点
- en: '|'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: 17.2 Anthos bare metal architecture
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.2 Anthos 裸金属架构
- en: Whereas Anthos on VMware cluster nodes are deployed from preconfigured VM images
    provided by Google, Anthos on bare metal relies on customers to provide a supported
    operating system version that they manage and patch themselves. As you can see
    in figure 17.2, the operating system can be installed directly on a physical server
    or on a VM running on any virtualization platform (KVM, OpenStack) that supports
    one of the Linux distributions compatible with Anthos on bare metal.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Anthos on VMware 集群节点从 Google 提供的预配置 VM 映像部署不同，Anthos on bare metal 依赖于客户提供他们自己管理和打补丁的受支持的操作系统版本。如图
    17.2 所示，操作系统可以直接安装在物理服务器上，或者安装在支持与 Anthos on bare metal 兼容的 Linux 分发的任何虚拟化平台（KVM、OpenStack）上运行的
    VM 上。
- en: '![17-02](../../OEBPS/Images/17-02.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![17-02](../../OEBPS/Images/17-02.png)'
- en: Figure 17.2 Anthos on-prem deployment options, shared responsibility model
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.2 Anthos 本地部署选项，共享责任模型
- en: 17.2.1 Cluster architecture
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.2.1 集群架构
- en: In this section, we will discuss the architecture of a bare metal cluster. Much
    of the architecture you know from the Anthos on VMware deployment is the same
    for bare metal; however, the bare metal option includes a few architecture differences
    from the VMware model.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论裸金属集群的架构。您从 Anthos on VMware 部署中了解到的许多架构对于裸金属也是相同的；然而，裸金属选项包括一些与 VMware
    模型不同的架构差异。
- en: Cluster roles
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 集群角色
- en: 'An Anthos bare metal installation has the following two kinds of clusters:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 裸金属安装有以下两种类型的集群：
- en: '*User cluster*—Where applications are deployed, it includes control plane nodes
    and worker nodes where containerized application instances run.'
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*用户集群*—应用程序部署的地方，它包括控制平面节点和运行容器化应用程序实例的工作节点。'
- en: '*Admin cluster*—A cluster that manages one or more user clusters. It is used
    to install, update, upgrade, and delete user clusters through an Anthos on bare
    metal-specific operator configured through two custom resources: Cluster and NodePool.'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*管理集群*—管理一个或多个用户集群的集群。它通过配置在两个自定义资源（集群和NodePool）中的 Anthos 在裸金属上特定的操作员来安装、更新、升级和删除用户集群。'
- en: An admin cluster includes only control plane nodes, where the components used
    to manage the installation run. It also hosts some security-sensitive data, including
    SSH keys to access nodes’ OS and GCP service account keys. Unlike Anthos on VMware,
    user cluster control plane nodes are decoupled from the admin cluster.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员集群仅包括控制平面节点，其中运行用于管理安装的组件。它还托管一些安全敏感的数据，包括访问节点OS的SSH密钥和GCP服务账户密钥。与在VMware上运行的Anthos不同，用户集群控制平面节点与管理员集群解耦。
- en: High availability
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '**高可用性**'
- en: You can run the user or admin cluster control plane in high-availability (HA)
    mode, so a control plane node failure does not affect cluster operations. This
    mode requires three or more control plane nodes. If high availability is not required,
    you can run a single control plane node in each cluster, but this method should
    be used only for nonproduction workloads.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在高可用性（HA）模式下运行用户或管理员集群控制平面，这样控制平面节点故障就不会影响集群操作。此模式需要三个或更多控制平面节点。如果不需要高可用性，你可以在每个集群中运行单个控制平面节点，但这种方法仅适用于非生产工作负载。
- en: Along with the control plane, you need to consider high availability for the
    worker nodes as well. For applications with high availability, you’ll need a minimum
    of two worker nodes. As with the control plane, you should never run production
    workloads without HA for the worker nodes.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 除了控制平面外，你还需要考虑工作节点的可用性。对于需要高可用性的应用程序，你需要至少两个工作节点。与控制平面一样，你不应该在没有为工作节点启用高可用性的情况下运行生产工作负载。
- en: Deployment models
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**部署模型**'
- en: This is where you will start to see the differences between how Anthos deploys
    between VMware and bare metal. Anthos on bare metal provides a few different deployment
    models, offering flexibility to meet different organization requirements.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 这是你开始看到Anthos在VMware和裸金属之间部署差异的地方。在裸金属上运行的Anthos提供几种不同的部署模型，以提供灵活性以满足不同的组织需求。
- en: Standalone cluster deployment
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '**独立集群部署**'
- en: In a standalone cluster deployment model, shown in figure 17.3, a single cluster
    serves both as the admin cluster and the user cluster. Because this model doesn’t
    require a separate admin cluster, you save three nodes in an HA setup. This situation
    can be helpful in scenarios where each cluster is managed independently or for
    a single cluster where each deployment location is required, for example, an edge
    use case or for an isolated network.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 在独立集群部署模型中，如图17.3所示，单个集群同时作为管理员集群和用户集群。由于此模型不需要单独的管理员集群，你可以在高可用性设置中节省三个节点。这种情况在需要独立管理每个集群或对单个集群每个部署位置都有要求的情况下可能很有用，例如边缘用例或隔离网络。
- en: '![17-03](../../OEBPS/Images/17-03.png)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![17-03](../../OEBPS/Images/17-03.png)'
- en: Figure 17.3 Standalone cluster deployment
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.3 独立集群部署
- en: From a security perspective, you need to consider that user workloads will run
    on the same cluster as the control plane. You will need to carefully consider
    securing your cluster to protect information like the node SSH keys and GCP service
    account keys. Implementing RBAC policies, OPA policies, network policies, and
    proper auditing will help to secure the cluster.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 从安全角度来看，你需要考虑用户工作负载将在与控制平面相同的集群上运行。你需要仔细考虑保护你的集群，以保护诸如节点SSH密钥和GCP服务账户密钥等信息。实施RBAC策略、OPA策略、网络策略和适当的审计将有助于保护集群。
- en: To provide more flexibility for these types of deployments, starting with Anthos
    version 1.8, Google reduced the minimum number of supported nodes per cluster
    from two to one and introduced, for standalone clusters, the new edge profile,
    which further reduces the hardware requirements.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 为了为这些类型的部署提供更多灵活性，从Anthos版本1.8开始，Google将每个集群支持的最小节点数从两个减少到一个，并为独立集群引入了新的边缘配置文件，这进一步降低了硬件要求。
- en: Multicluster deployment
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '**多集群部署**'
- en: This is the same deployment model that Anthos on VMware uses. In a multicluster
    deployment model, shown in figure 17.4, you have a single admin cluster managing
    multiple user clusters. This model is useful if you need to centrally manage a
    fleet of clusters deployed in the same data center.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Anthos在VMware上使用的相同部署模型。在多集群部署模型中，如图17.4所示，你有一个单个管理员集群管理多个用户集群。如果需要集中管理在同一数据中心部署的集群系列，则此模型很有用。
- en: '![17-04](../../OEBPS/Images/17-04.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![17-04](../../OEBPS/Images/17-04.png)'
- en: Figure 17.4 Multicluster deployment
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.4 多集群部署
- en: Hybrid cluster deployment
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '**混合集群部署**'
- en: 'The hybrid cluster deployment model, shown in figure 17.5, is similar to the
    multicluster deployment with one difference: you can use the admin cluster to
    run user workloads along with the standard worker nodes.'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 混合集群部署模型，如图17.5所示，与多集群部署类似，只有一个区别：你可以使用管理集群来运行用户工作负载以及标准工作节点。
- en: '![17-05](../../OEBPS/Images/17-05.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![17-05](../../OEBPS/Images/17-05.png)'
- en: Figure 17.5 Hybrid cluster deployment
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.5 混合集群部署
- en: As you can see, Anthos on bare metal added greater deployment flexibility compared
    to the VMware deployment model. The added flexibility doesn’t stop there, though.
    In the next subsection, we will discuss the updates to the networking architecture.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，与VMware部署模型相比，Anthos在裸金属上增加了更大的部署灵活性。然而，增加的灵活性并不止于此。在下一小节中，我们将讨论网络架构的更新。
- en: Networking architecture
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 网络架构
- en: It makes sense that the networking architecture for the bare metal option would
    differ from the VMware model’s. In the VMware deployment, you can use an external
    load balancer, or you can use the bundled load balancer, Seesaw. If you use the
    Seesaw option, Anthos deploys a preconfigured virtual machine, or machines in
    HA mode, to your VMware environment.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 对于裸金属选项的网络架构与VMware模型的不同是有道理的。在VMware部署中，你可以使用外部负载均衡器，或者使用捆绑的负载均衡器，Seesaw。如果你选择Seesaw选项，Anthos将在你的VMware环境中部署一个预配置的虚拟机，或者以HA模式运行的机器。
- en: In the bare metal deployment, Google doesn’t supply any appliances or VM images
    for any component. Don’t worry, though—Google has this covered using other components
    like HAProxy and MetalLB.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属部署中，Google不提供任何组件的设备或虚拟机镜像。不过，不用担心——Google通过其他组件如HAProxy和MetalLB来解决这个问题。
- en: Load balancing
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡
- en: Anthos on bare metal requires layer-4 (L4) load balancing to expose the control
    plane endpoint, ingress endpoint, and applications, using the LoadBalancer type
    Service. The load balancer’s responsibility is to route and balance the traffic
    to the appropriate nodes.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos在裸金属上需要使用层4（L4）负载均衡来暴露控制平面端点、入口端点和应用程序，使用LoadBalancer类型的服务。负载均衡器的责任是路由和平衡流量到适当的节点。
- en: Whichever cluster deployment model you choose, Anthos on bare metal can provide
    the needed load-balancing capabilities through a bundled L4 load balancer (bundled
    load balancer mode) or, alternatively, you can use an external load-balancing
    solution provided and configured by the customer (manual load balancer mode).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种集群部署模型，Anthos在裸金属上可以通过捆绑的L4负载均衡器（捆绑负载均衡器模式）提供所需的负载均衡功能，或者，你也可以使用客户提供的和配置的外部负载均衡解决方案（手动负载均衡器模式）。
- en: Whichever option you choose for L4 load balancing, during the installation an
    Envoy-based Istio Ingress gateway is deployed, and it’s exposed through a LoadBalancer
    type Service using the L4 load balancer. This Envoy deployment is used to provide
    application proxy capabilities to applications, exposing them through standard
    Kubernetes ingress objects.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择哪种L4负载均衡选项，在安装过程中都会部署基于Envoy的Istio Ingress网关，并通过L4负载均衡器以LoadBalancer类型的服务公开。这个Envoy部署用于为应用程序提供代理功能，通过标准的Kubernetes入口对象暴露它们。
- en: Bundled load balancer mode
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 捆绑式负载均衡器模式
- en: To begin with, let’s discuss load balancing for the control plane. If you choose
    bundled load balancing, Anthos on bare metal deploys L4 load balancers during
    cluster installation, removing the requirement of providing an external load balancer.
    The load balancers can run on a dedicated pool of worker nodes, or they can be
    located on the same nodes as the control plane. In either case, the load balancer
    nodes must be in the same subnet.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们讨论控制平面的负载均衡。如果你选择捆绑的负载均衡，Anthos在裸金属上部署L4负载均衡器，在集群安装期间，无需提供外部负载均衡器。负载均衡器可以运行在专用的工作节点池上，或者位于与控制平面相同的节点上。在任一情况下，负载均衡器节点必须在同一子网中。
- en: Starting with Anthos 1.9, Google changed how L4 load balancers are deployed.
    Previously, the HAProxy container image was deployed to the node(s) as a standard
    Docker (i.e., not Kubernetes controlled) container as a systemd Service. Starting
    with version 1.9, the Keepalived and HAProxy containers have been updated to run
    as static Kubernetes Pods on the load balancer nodes.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 从Anthos 1.9版本开始，Google改变了L4负载均衡器的部署方式。之前，HAProxy容器镜像作为标准Docker容器（即非Kubernetes控制）以systemd服务的形式部署到节点（s）。从版本1.9开始，Keepalived和HAProxy容器已更新为在负载均衡器节点上作为静态Kubernetes
    Pod运行。
- en: HAProxy is used only for load-balancing the control plane. To provide L4 to
    the data plane, Anthos deploys a popular, open source solution called MetalLB,
    which services requests in the cluster for any service that is deployed using
    the LoadBalancer type.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: HAProxy仅用于控制平面的负载均衡。为了向数据平面提供L4服务，Anthos部署了一个流行的开源解决方案，称为MetalLB，该解决方案为集群中任何使用LoadBalancer类型部署的服务提供服务。
- en: 'To recap the bundled load balancer components:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 为了回顾捆绑的负载均衡器组件：
- en: '*Control plane load balancing*—The control plane virtual IP address (VIP),
    routing traffic to the Kubernetes API server running on control plane nodes, is
    exposed through an HAProxy load balancer running as a Kubernetes Pod on the load
    balancer nodes, together with a containerized Keepalived service that manages
    HAProxy high availability.'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*控制平面负载均衡*——控制平面虚拟IP地址（VIP），将流量路由到运行在控制平面节点上的Kubernetes API服务器，通过在负载均衡器节点上作为Kubernetes
    Pod运行的HAProxy负载均衡器公开，同时还有一个容器化的Keepalived服务来管理HAProxy的高可用性。'
- en: '*Data plane load balancing*—The LoadBalancer type Service objects created for
    the applications and the Istio Ingress gateway deployed with Anthos on bare metal
    are exposed through an Anthos-managed MetalLB deployment running on the load balancer
    nodes. IP addresses for LoadBalancer type Services can be automatically assigned
    from a predefined pool and are part of the same subnet where load balancer nodes
    are deployed.'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据平面负载均衡*——为应用程序创建的LoadBalancer类型服务对象以及与裸金属上的Anthos一起部署的Istio Ingress网关，通过在负载均衡器节点上运行的Anthos管理的MetalLB部署公开，IP地址可以从预定义的池中自动分配，并且是负载均衡器节点部署的同一子网的一部分。'
- en: Both control plane load-balancing components (HAProxy and Keepalived) and data
    plane load-balancing components (MetalLB) run together on designated nodes (cluster
    control plane nodes or dedicated load balancer worker nodes).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面负载均衡组件（HAProxy和Keepalived）以及数据平面负载均衡组件（MetalLB）都在指定的节点上一起运行（集群控制平面节点或专用负载均衡器工作节点）。
- en: Figure 17.6 shows an architecture example of a user cluster deployed in a single
    subnet with bundled load balancers running on control plane nodes. Figure 17.7
    shows an architecture example of an user cluster deployed in a single subnet with
    bundled load balancers running on dedicated worker nodes.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6显示了在单个子网中部署的用户集群的架构示例，其中捆绑的负载均衡器在控制平面节点上运行。图17.7显示了在单个子网中部署的用户集群的架构示例，其中捆绑的负载均衡器在专用工作节点上运行。
- en: '![17-06](../../OEBPS/Images/17-06.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![17-06](../../OEBPS/Images/17-06.png)'
- en: Figure 17.6 Load balancers running on control plane nodes
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.6 在控制平面节点上运行的负载均衡器
- en: '![17-07](../../OEBPS/Images/17-07.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![17-07](../../OEBPS/Images/17-07.png)'
- en: Figure 17.7 Load balancers running on dedicated worker nodes
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.7 在专用工作节点上运行的负载均衡器
- en: Manual load balancer mode
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 手动负载均衡器模式
- en: If you choose manual load balancer mode, the Anthos installation doesn’t deploy
    the bundled load balancers, and you are responsible for deploying an external
    load-balancing solution.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择手动负载均衡器模式，Anthos安装不会部署捆绑的负载均衡器，你需要负责部署外部负载均衡解决方案。
- en: Figure 17.8 shows an example of a user cluster deployed in a single subnet with
    an external load balancer configured in manual load balancer mode.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8显示了在单个子网中部署的用户集群的示例，其中配置了手动负载均衡模式的外部负载均衡器。
- en: '![17-08](../../OEBPS/Images/17-08.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![17-08](../../OEBPS/Images/17-08.png)'
- en: Figure 17.8 Manual load-balancing architecture
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 图17.8 手动负载均衡架构
- en: Internal cluster networking
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 内部集群网络
- en: Anthos on bare metal deploys an overlay network based on GENEVE tunnels that
    requires layer-3 (L3) connectivity between the nodes in the cluster, except for
    load balancer nodes that are required to be in the same layer-2 (L2) domain.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上部署的Anthos使用基于GENEVE隧道的覆盖网络，这要求集群中的节点之间具有第3层（L3）连接性，除了需要位于同一第2层（L2）域中的负载均衡器节点。
- en: Similar to how Anthos on VMware works, Pod IP addressing works in island mode,
    which means IP addresses assigned to Pods are accessible only from the same cluster,
    and Pod CIDR ranges can be reused across clusters.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 类似于Anthos在VMware上的工作方式，Pod IP寻址在孤岛模式下工作，这意味着分配给Pod的IP地址只能从同一集群访问，Pod CIDR范围可以在集群之间重复使用。
- en: Storage architecture
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 存储架构
- en: The main approach to provide persistent storage for workloads running on Anthos
    on bare metal is through a Container Storage Interface driver from an Anthos-ready
    storage partner. You can find a list of partners and validated storage solutions
    at [http://mng.bz/1MdX](http://mng.bz/1MdX).
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 为在裸金属上运行的 Anthos 工作负载提供持久存储的主要方法是通过来自 Anthos 准备存储合作伙伴的 Container Storage Interface
    驱动程序。您可以在 [http://mng.bz/1MdX](http://mng.bz/1MdX) 找到合作伙伴列表和经过验证的存储解决方案。
- en: Anthos on bare metal also bundles the sig-storage-local-static-provisioner,
    which provides mount points on each node and creates a local persistent volume
    (PV) for each mount point. Because of its limitations, you should use local PVs
    only for nonproduction environments or specific advanced use cases.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 裸金属上的 Anthos 还捆绑了 sig-storage-local-static-provisioner，它为每个节点提供挂载点，并为每个挂载点创建一个本地持久卷
    (PV)。由于其局限性，您应仅将本地 PV 用于非生产环境或特定的先进用例。
- en: Observability
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性
- en: You can use Google Cloud operations to collect logs and monitoring metrics for
    Anthos on bare metal, just like in other Anthos deployment environments. By default,
    system components logs are sent to Cloud Logging and system components metrics
    are sent to Cloud Monitoring. Cloud operations can also collect application logs
    and metrics (using Google Managed Service for Prometheus) by enabling it in the
    cluster configuration.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 Google Cloud 操作收集裸金属上 Anthos 的日志和监控指标，就像在其他 Anthos 部署环境中一样。默认情况下，系统组件日志被发送到
    Cloud Logging，系统组件指标被发送到 Cloud Monitoring。通过在集群配置中启用它，Cloud 操作还可以收集应用程序日志和指标（使用
    Google Managed Service for Prometheus）。
- en: As an alternative to Cloud operations, you can use other solutions if preferred,
    such as Prometheus/Grafana, Elastic Stack, Sysdig, Datadog, or Splunk.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 作为云操作的一种替代方案，如果您更倾向于使用其他解决方案，可以选择 Prometheus/Grafana、Elastic Stack、Sysdig、Datadog
    或 Splunk。
- en: Identity integration
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 身份集成
- en: Anthos on bare metal can integrate, through the Anthos Identity Service authentication
    proxy, with any identity provider that supports OpenID Connect (OIDC) or LDAP
    to manage user and group authentication to clusters using existing user identities
    and credentials. If you already use or want to use Google IDs to log in to your
    Anthos clusters instead of an OIDC or LDAP provider, is it recommended you use
    the Connect gateway for authentication.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 通过 Anthos Identity Service 认证代理，裸金属上的 Anthos 可以与支持 OpenID Connect (OIDC) 或 LDAP
    的任何身份提供者集成，以使用现有的用户身份和凭据管理对集群的用户和组身份验证。如果您已经使用或希望使用 Google IDs 登录到您的 Anthos 集群而不是
    OIDC 或 LDAP 提供商，建议您使用 Connect 网关进行身份验证。
- en: With this integration, you can manage access to an Anthos on bare metal cluster
    by using standard procedures in your organization for creating, enabling, and
    disabling accounts. You can employ Kubernetes RBAC to bind specific roles to users
    and groups defined in the identity provider to authorize them to perform specific
    actions on specific resources.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这种集成，您可以使用组织内部创建、启用和禁用账户的标准程序来管理对裸金属上 Anthos 集群的访问。您可以使用 Kubernetes RBAC 将特定角色绑定到身份提供者中定义的用户和组，以授权他们在特定资源上执行特定操作。
- en: 17.3 Installation and configuration overview
  id: totrans-129
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.3 安装和配置概述
- en: In this section, we will provide an overview of the requirements to deploy a
    cluster. Understanding the requirements is an important step before attempting
    to deploy a cluster, which we will discuss in the next section.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将概述部署集群所需的要求。在尝试部署集群之前，了解这些要求是一个重要的步骤，我们将在下一节中进行讨论。
- en: Google has made deploying Anthos on bare metal an easy process. Like the VMware
    deployment, you configure the cluster options in a configuration file and perform
    the deployment using a single binary executable called bmctl.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Google 已经使在裸金属上部署 Anthos 变得简单。与 VMware 部署类似，您在配置文件中配置集群选项，并使用名为 bmctl 的单个二进制可执行文件执行部署。
- en: Anthos requires you to meet requirements for both software and hardware. Most
    organizations easily meet the requirements, but you should always verify that
    your infrastructure meets all requirements before deploying.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 要求您满足软件和硬件的要求。大多数组织很容易满足这些要求，但在部署之前，您应始终验证您的基础设施是否满足所有要求。
- en: 17.3.1 Operating systems and software requirements
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.1 操作系统和软件要求
- en: 'As described in the introduction and architecture sections, Anthos on bare
    metal is installed on servers provided and configured by the customer. Servers
    can be physical or virtual, as long as they have one of the supported operating
    systems, configured to meet the Anthos requirements. Servers to be used as Anthos
    on bare metal nodes need to have one of the following operating systems:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 如介绍和架构部分所述，Anthos on bare metal 是安装在客户提供的并由客户配置的服务器上。服务器可以是物理的或虚拟的，只要它们具有支持的操作系统之一，并配置以满足
    Anthos 的要求。用作 Anthos on bare metal 节点的服务器需要以下操作系统之一：
- en: CentOS 8.2/8.3/8.4/8.5
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CentOS 8.2/8.3/8.4/8.5
- en: Red Hat Enterprise Linux (RHEL) 8.2/8.3/8.4/8.5/8.6
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Red Hat Enterprise Linux (RHEL) 8.2/8.3/8.4/8.5/8.6
- en: Ubuntu 18.04 and 20.04
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ubuntu 18.04 和 20.04
- en: Each supported version requires a slightly different configuration. If you are
    using RHEL or CentOS, the firewalld service must be configured to allow traffic
    to TCP and UDP ports—these will be covered in the internal connectivity requirements
    in section 17.3.4\. On these operating systems, if SELinux is enabled in enforcing
    mode, a policy for container isolation and security is configured during the Anthos
    on bare metal setup. If you are running the nodes on Ubuntu, you must disable
    the Uncomplicated Firewall service.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 每个支持的版本都需要稍有不同的配置。如果您使用 RHEL 或 CentOS，必须配置 firewalld 服务以允许流量通过 TCP 和 UDP 端口——这些将在
    17.3.4 节中的内部连接性要求中介绍。在这些操作系统上，如果 SELinux 以强制模式启用，则在 Anthos on bare metal 设置期间将配置容器隔离和安全策略。如果您在
    Ubuntu 上运行节点，则必须禁用 Uncomplicated Firewall 服务。
- en: Time is very important for a cluster. To ensure that all nodes have their clocks
    in sync, all servers need to have an NTP service configured and enabled. Finally,
    because the installation establishes an SSH connection to the nodes, you’ll need
    an SSH key pair to access each node with root privileges.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 时间对于集群来说非常重要。为确保所有节点的时钟同步，所有服务器都需要配置并启用 NTP 服务。最后，因为安装会在节点上建立 SSH 连接，所以您需要一对
    SSH 密钥来以 root 权限访问每个节点。
- en: 17.3.2 Hardware capacity requirements
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.2 硬件容量要求
- en: Anthos on bare metal will work on any hardware compatible with one of the supported
    operating systems. The number of nodes required for an installation depends on
    the chosen deployment and load-balancing model, as described earlier. The number
    of worker nodes required will depend on the capacity requirements of the applications
    that the cluster(s) will host.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on bare metal 可以在任何与支持的操作系统兼容的硬件上运行。安装所需的节点数量取决于选择的部署和负载均衡模型，如前所述。所需的
    worker 节点数量将取决于集群（们）将要托管的应用程序的能力要求。
- en: Table 17.2 describes the minimum and recommended hardware requirements for each
    node, whatever its role, using the default profile, excluding the applications
    capacity requirements that should be added.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.2 描述了每个节点的最小和推荐硬件要求，无论其角色如何，使用默认配置文件，不包括应添加的应用程序容量要求。
- en: Table 17.2 Hardware requirements for Anthos on bare metal
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.2 Anthos on bare metal 的硬件要求
- en: '| Resource | Minimum | Recommended |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 最小 | 推荐 |'
- en: '| CPUs/vCPUs | 4 core | 8 core |'
  id: totrans-145
  prefs: []
  type: TYPE_TB
  zh: '| CPU/vCPU | 4 核 | 8 核 |'
- en: '| RAM | 16 GiB | 32 GiB |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | 16 GiB | 32 GiB |'
- en: '| Storage | 128 GiB | 256 GiB |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| 存储 | 128 GiB | 256 GiB |'
- en: Table 17.3 describes the hardware requirements for the edge profile introduced
    in Anthos on bare metal version 1.8.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.3 描述了 Anthos on bare metal 版本 1.8 中引入的边缘配置文件的硬件要求。
- en: Table 17.3 Hardware requirements for the edge profile
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.3 边缘配置文件的硬件要求
- en: '| Resource | Minimum | Recommended |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 资源 | 最小 | 推荐 |'
- en: '| CPUs/vCPUs | 2 core | 4 core |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| CPU/vCPU | 2 核 | 4 核 |'
- en: '| RAM | Ubuntu: 4 GBCentOS/RHEL: 6 GiB | Ubuntu: 8 GBCentOS/RHEL: 12 GiB |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 内存 | Ubuntu: 4 GBCentOS/RHEL: 6 GiB | Ubuntu: 8 GBCentOS/RHEL: 12 GiB |'
- en: '| Storage | 128 GiB | 256 GiB |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 存储 | 128 GiB | 256 GiB |'
- en: 17.3.3 Admin workstation
  id: totrans-154
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.3 管理工作站
- en: 'Besides the nodes, it’s suggested to have an additional workstation to run
    the installation tool. This workstation must have the same Linux operating system
    running on cluster nodes with Docker 19.03 or higher configured to be managed
    by nonroot users. In addition to Docker, the machine must have the following:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 除了节点之外，建议再有一个工作站来运行安装工具。此工作站必须运行与集群节点相同的 Linux 操作系统，并配置 Docker 19.03 或更高版本，以便由非
    root 用户管理。除了 Docker 之外，该机器还必须具备以下条件：
- en: gcloud with anthos-auth and kubectl installed.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 已安装带有 anthos-auth 和 kubectl 的 gcloud。
- en: More than 50 GB of free disk space.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 需要 50 GB 以上的空闲磁盘空间。
- en: L3 connectivity to all cluster node machines.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 到所有集群节点机器的 L3 连接性。
- en: Access to all cluster node machines through SSH via private keys with passwordless
    root access. Access can be either direct or through sudo.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过私钥和密码无根访问权限通过 SSH 访问所有集群节点机器。访问可以是直接或通过 sudo。
- en: Access to the control plane VIP.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 访问控制平面 VIP。
- en: 17.3.4 Networking requirements
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.4 网络需求
- en: Anthos has different requirements for external versus internal network connectivity.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 对外部和内部网络连接有不同的需求。
- en: External connectivity requirements
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 外部连接需求
- en: 'All Anthos on bare metal nodes will need outbound HTTPS connectivity to the
    internet to do the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 所有在裸金属节点上的 Anthos 都需要出站 HTTPS 连接到互联网以执行以下操作：
- en: Register to the GCP console and be managed from there through GKE Connect
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 GCP 控制台中注册并从那里通过 GKE Connect 进行管理
- en: Send metrics and logs to Cloud operation endpoints
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将指标和日志发送到云操作端点
- en: Pull images from the Google Container Registry
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 Google 容器注册库拉取镜像
- en: This connectivity can use the public internet, an HTTP proxy, or a private connection
    like Google Cloud VPN or Dedicated Interconnect.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这种连接可以使用公共互联网、HTTP 代理或类似 Google Cloud VPN 或专用互连的私有连接。
- en: Internal connectivity requirements
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 内部连接需求
- en: This section will detail the internal networking requirements for your cluster.
    Each component of the cluster has different requirements, and tables 17.4-17.7
    list the specific connectivity ports used for cluster node traffic.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将详细说明您集群的内部网络需求。集群的每个组件都有不同的需求，表格 17.4-17.7 列出了用于集群节点流量的特定连接端口。
- en: Table 17.4 Control plane nodes
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.4 控制平面节点
- en: '| Protocol | Direction | Port range | Purpose | Used by |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 协议 | 方向 | 端口范围 | 目的 | 使用者 |'
- en: '| UDP | Inbound | 6081 | GENEVE encapsulation | Self |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| UDP | Inbound | 6081 | GENEVE 封装 | Self |'
- en: '| TCP | Inbound | 22 | Provisioning and updates of admin cluster nodes | Admin
    workstation |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 22 | 管理集群节点的配置和更新 | 管理工作站 |'
- en: '| TCP | Inbound | 6444 | Kubernetes API server | All |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 6444 | Kubernetes API 服务器 | All |'
- en: '| TCP | Inbound | 2379-2380 | etcd server client API | kube-apiserver and etcd
    |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 2379-2380 | etcd 服务器客户端 API | kube-apiserver 和 etcd |'
- en: '| TCP | Inbound | 10250 | kubelet API | Self and control plane |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10250 | kubelet API | Self 和控制平面 |'
- en: '| TCP | Inbound | 10251 | kube-scheduler | Self |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10251 | kube-scheduler | Self |'
- en: '| TCP | Inbound | 10252 | kube-controller-manager | Self |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10252 | kube-controller-manager | Self |'
- en: '| TCP | Inbound | 10256 | Node health check | All |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10256 | 节点健康检查 | All |'
- en: '| TCP | Both | 4240 | CNI health check | All |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Both | 4240 | CNI 健康检查 | All |'
- en: Table 17.5 Worker nodes
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.5 工作节点
- en: '| Protocol | Direction | Port range | Purpose | Used by |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 协议 | 方向 | 端口范围 | 目的 | 使用者 |'
- en: '| TCP | Inbound | 22 | Provisioning and updates of user cluster nodes | Admin
    cluster nodes |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 22 | 用户集群节点的配置和更新 | 管理集群节点 |'
- en: '| UDP | Inbound | 6081 | GENEVE encapsulation | Self |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| UDP | Inbound | 6081 | GENEVE 封装 | Self |'
- en: '| TCP | Inbound | 10250 | kubelet API | Self and control plane |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10250 | kubelet API | Self 和控制平面 |'
- en: '| TCP | Inbound | 10256 | Node health check | All |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10256 | 节点健康检查 | All |'
- en: '| TCP | Inbound | 30000-32767 | NodePort services | Self |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 30000-32767 | NodePort服务 | Self |'
- en: '| TCP | Both | 4240 | CNI health check | All |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Both | 4240 | CNI 健康检查 | All |'
- en: Table 17.6 Load balancer nodes
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.6 负载均衡节点
- en: '| Protocol | Direction | Port range | Purpose | Used by |'
  id: totrans-191
  prefs: []
  type: TYPE_TB
  zh: '| 协议 | 方向 | 端口范围 | 目的 | 使用者 |'
- en: '| TCP | Inbound | 22 | Provisioning and updates of user cluster nodes | Admin
    cluster nodes |'
  id: totrans-192
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 22 | 用户集群节点的配置和更新 | 管理集群节点 |'
- en: '| UDP | Inbound | 6081 | GENEVE encapsulation | Self |'
  id: totrans-193
  prefs: []
  type: TYPE_TB
  zh: '| UDP | Inbound | 6081 | GENEVE 封装 | Self |'
- en: '| TCP | Inbound | 443 | Cluster management | All |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 443 | 集群管理 | All |'
- en: '| TCP | Both | 4240 | CNI health check | All |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Both | 4240 | CNI health check | All |'
- en: '| TCP | Inbound | 7946 | Metal LB health check | Load balancer nodes |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 7946 | Metal LB 健康检查 | 负载均衡节点 |'
- en: '| TCP | Inbound | 10256 | Node health check | All |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 10256 | 节点健康检查 | All |'
- en: '| UDP | Inbound | 7946 | Metal LB health check | Load balancer nodes |'
  id: totrans-198
  prefs: []
  type: TYPE_TB
  zh: '| UDP | Inbound | 7946 | Metal LB 健康检查 | 负载均衡节点 |'
- en: Table 17.7 Multicluster port requirements
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 表 17.7 多集群端口需求
- en: '| Protocol | Direction | Port range | Purpose | Used by |'
  id: totrans-200
  prefs: []
  type: TYPE_TB
  zh: '| 协议 | 方向 | 端口范围 | 目的 | 使用者 |'
- en: '| TCP | Inbound | 22 | Provisioning and updates of cluster nodes | All nodes
    |'
  id: totrans-201
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 22 | 集群节点的配置和更新 | 所有节点 |'
- en: '| TCP | Inbound | 443 | Kubernetes API server for added cluster | Control plane
    and load balancer nodes |'
  id: totrans-202
  prefs: []
  type: TYPE_TB
  zh: '| TCP | Inbound | 443 | 添加集群的 Kubernetes API 服务器 | 控制平面和负载均衡节点 |'
- en: With the networking requirements covered, let’s move on to the additional requirements
    for configuring the cluster load balancer.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络要求得到解决后，让我们继续讨论配置集群负载均衡器的附加要求。
- en: Load-balancing requirements
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡要求
- en: 'Before installing Anthos on bare metal, you need to choose an architecture
    for load balancing (manual versus bundled) and, in the case of bundled, decide
    whether your load balancers will be installed on control plane nodes or dedicated
    worker nodes. Whatever solution you choose, the following VIP addresses must be
    reserved:'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上安装 Anthos 之前，您需要选择一个负载均衡的架构（手动或捆绑）以及在捆绑的情况下，决定您的负载均衡器是否将安装在控制平面节点或专用工作节点上。无论您选择哪种解决方案，以下
    VIP 地址都必须预留：
- en: '*One VIP for the control plane for each cluster*—If you’re using the bundled
    load balancer, this will be automatically created based on the configuration you
    defined during installation. If you’re using a manual load balancer, this needs
    to be manually associated with a backend server group containing all the IP addresses
    of the cluster’s control plane nodes. The backend port the control plane listens
    on is 6444.'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个集群的控制平面一个 VIP*——如果您使用捆绑的负载均衡器，这将根据您在安装过程中定义的配置自动创建。如果您使用手动负载均衡器，则需要手动将其关联到一个包含集群控制平面节点所有
    IP 地址的后端服务器组。控制平面监听的后端端口是 6444。'
- en: '*One VIP for the Ingress service for each user cluster*—If you’re using the
    bundled load balancer, this will be automatically created based on the configuration
    defined during installation. If you’re using a manual load balancer, this needs
    to be manually configured with the same IP address assigned to the istio-ingress
    Service created in the gke-system namespace in the cluster and associated with
    a backend server group containing the IP addresses of the cluster nodes. The backend
    port would be the NodePort of the istio-ingress Service. If you want to use the
    Ingress gateway both for HTTP and HTTPS traffic, it’s possible that you have to
    configure one VIP (and backend pool) for each port.'
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*每个用户集群的 Ingress 服务一个 VIP*——如果您使用捆绑的负载均衡器，这将根据您在安装过程中定义的配置自动创建。如果您使用手动负载均衡器，则需要手动配置，使用分配给在集群
    gke-system 命名空间中创建的 istio-ingress Service 的相同 IP 地址，并关联到一个包含集群节点 IP 地址的后端服务器组。后端端口将是
    istio-ingress Service 的 NodePort。如果您想同时使用 Ingress 网关处理 HTTP 和 HTTPS 流量，可能需要为每个端口配置一个
    VIP（和后端池）。'
- en: '*One VIP for each* LoadBalancer *type Service created in the cluster*—If you’re
    using the bundled load balancer, these will be automatically assigned based on
    the pool defined during installation. If you’re using a manual load balancer,
    it needs to be manually configured with the same IP address assigned to the Service
    object and associated with a backend server group containing all the IP addresses
    of the cluster worker nodes. The backend port would be the NodePort of the Service
    object.'
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*集群中每个 LoadBalancer 类型 Service 创建一个 VIP*——如果您使用捆绑的负载均衡器，这些将根据您在安装过程中定义的池自动分配。如果您使用手动负载均衡器，则需要手动配置，使用分配给
    Service 对象的相同 IP 地址，并关联到一个包含集群工作节点所有 IP 地址的后端服务器组。后端端口将是 Service 对象的 NodePort。'
- en: 'If the cluster deployment will use the bundled load balancer, the following
    items must be configured:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 如果集群部署将使用捆绑的负载均衡器，以下项目必须进行配置：
- en: The load-balancing nodes need to be in the same L2 network, whereas other connections,
    including worker nodes, require only L3 connectivity.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡节点需要位于同一 L2 网络中，而其他连接，包括工作节点，只需要 L3 连接性。
- en: All VIPs must be in the load balancer machine subnet and fully routable.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有 VIP 必须位于负载均衡器机器子网中，并且完全可路由。
- en: The gateway of the load balancer subnet must listen to gratuitous ARP messages
    and forward ARP packets to the load balancer nodes.
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器子网的网关必须监听免费 ARP 消息并将 ARP 数据包转发到负载均衡器节点。
- en: Moving on, the next section will cover the Google Cloud Platform requirements.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下一节将介绍 Google Cloud Platform 的要求。
- en: 17.3.5 Google Cloud Platform requirements
  id: totrans-214
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.3.5 Google Cloud Platform 要求
- en: The Anthos on bare metal installation has a few GCP project requirements, including
    required APIs, service accounts, and required roles.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 在裸金属上的安装有几个 GCP 项目要求，包括必需的 API、服务帐户和必需的角色。
- en: Required GCP APIs
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 必需的 GCP API
- en: 'For a successful deployment, the project to which the cluster will be connected
    must have several APIs enabled. You can do this manually, or you can enable them
    automatically as an option when you execute the deployment using bmctl. The following
    APIs must be enabled in the GCP project used for installation:'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 为了成功部署，将连接到集群的项目必须启用几个API。您可以手动完成此操作，或者您可以在使用bmctl执行部署时自动启用它们作为选项。在用于安装的GCP项目中必须启用以下API：
- en: anthos.googleapis.com
  id: totrans-218
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: anthos.googleapis.com
- en: anthosaudit.googleapis.com
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: anthosaudit.googleapis.com
- en: anthosgke.googleapis.com
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: anthosgke.googleapis.com
- en: cloudresourcemanager.googleapis.com
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: cloudresourcemanager.googleapis.com
- en: container.googleapis.com
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: container.googleapis.com
- en: gkeconnect.googleapis.com
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkeconnect.googleapis.com
- en: gkehub.googleapis.com
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gkehub.googleapis.com
- en: iam.googleapis.com
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: iam.googleapis.com
- en: serviceusage.googleapis.com
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: serviceusage.googleapis.com
- en: stackdriver.googleapis.com
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: stackdriver.googleapis.com
- en: monitoring.googleapis.com
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: monitoring.googleapis.com
- en: logging.googleapis.com
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: logging.googleapis.com
- en: opsconfigmonitoring.googleapis.com
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: opsconfigmonitoring.googleapis.com
- en: If any of the APIs is not enabled before you run the deployment, the preflight
    check will catch the missing API and stop the deployment from continuing.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在运行部署之前未启用任何API，预检检查将捕获缺失的API并停止部署继续进行。
- en: Required service accounts and roles
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 必需的服务帐户和角色
- en: Another requirement before deploying Anthos on bare metal is to create the required
    service account(s) and required roles. Although you can use a single account with
    all the roles, it is considered a bad security practice. Your organization will
    have its own security requirements, but it is advised that you create all the
    accounts as distinct service accounts.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上部署Anthos之前的一个要求是创建所需的服务帐户和所需的角色。尽管您可以使用具有所有角色的单个帐户，但这被认为是一种不良的安全实践。您的组织将有其自己的安全要求，但建议您创建所有帐户作为不同的服务帐户。
- en: 'You can elect to create the service accounts manually, or you can create them
    during the installation, using a parameter of the bmctl installation tool. Anthos
    on bare metal needs the following Google Cloud service accounts with the roles
    specified:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以选择手动创建服务帐户，或者您可以在安装过程中创建它们，使用bmctl安装工具的参数。在裸金属上运行的Anthos需要以下具有指定角色的Google
    Cloud服务帐户：
- en: A service account Container Registry (gcr.io) with no special role
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有特殊角色的服务帐户Container Registry (gcr.io)
- en: A service account used to register the cluster to the GCP console with the GKE
    hub admin IAM role
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将集群注册到GCP控制台并具有GKE hub管理员IAM角色的服务帐户
- en: A service account used to maintain a connection between the cluster and Google
    Cloud with the GKE Connect Agent IAM role
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于维护集群与Google Cloud之间连接并具有GKE Connect代理IAM角色的服务帐户
- en: 'A service account used to send logs and metrics to Google Cloud’s operations
    suite with the following IAM roles:'
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于将日志和指标发送到Google Cloud操作套件并具有以下IAM角色的服务帐户：
- en: Logs writer
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录器
- en: Monitoring metric writer
  id: totrans-240
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控指标写入器
- en: Stackdriver resource metadata writer
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stackdriver资源元数据写入器
- en: Monitoring dashboard configuration editor
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 监控仪表板配置编辑器
- en: Ops config monitoring resource metadata writer
  id: totrans-243
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Ops配置监控资源元数据写入器
- en: 'If you want to enable these APIs and create the needed GCP service accounts
    during installation using the bmctl tool, the account used for installation must
    have either the project owner/editor role or, at least, the following roles assigned:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想通过bmctl工具启用这些API并在安装过程中创建所需的GCP服务帐户，用于安装的帐户必须具有项目所有者/编辑角色，或者至少分配以下角色：
- en: Service account admin
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务帐户管理员
- en: Service account key admin
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务帐户密钥管理员
- en: Project IAM admin
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 项目IAM管理员
- en: Compute viewer
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 计算机查看器
- en: Service usage admin
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 服务使用管理员
- en: Finally, we will cover one more requirement that Anthos will use for cluster
    metrics.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将涵盖一个Anthos将用于集群指标的要求。
- en: Cloud metric requirements
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 云指标要求
- en: To send metrics to Google Cloud’s operations suite, in addition to the service
    accounts listed in the previous section, you must create a Cloud Monitoring workspace
    within the GCP project.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 要将指标发送到Google Cloud的操作套件，除了上一节中列出的服务帐户外，您必须在GCP项目中创建一个云监控工作区。
- en: 17.4 Creating clusters
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.4 创建集群
- en: After all the requirements have been satisfied, you can proceed with the cluster
    creation. The following sections assume that all the installation tasks are performed
    from a machine that satisfies the requirements stated in section 17.3.3.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 满足所有要求后，您可以继续创建集群。以下章节假设所有安装任务都是在满足第17.3.3节所述要求的机器上执行的。
- en: 17.4.1 Creating an admin, hybrid, or standalone cluster
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.4.1 创建管理员、混合或独立集群
- en: As we have detailed, you can deploy Anthos on bare metal using a few different
    cluster models, including separate admin/user clusters, hybrid clusters, or standalone
    clusters. In this section, we will discuss the process of deploying each model.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们详细说明的，您可以使用几种不同的集群模型在裸金属上部署 Anthos，包括独立的管理员/用户集群、混合集群或独立集群。在本节中，我们将讨论部署每种模型的过程。
- en: Summary of installation flow
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 安装流程摘要
- en: 'You install the first cluster in a specific deployment environment, regardless
    of the selected model, using the bmctl tool. Additional user clusters in the same
    environment can be created by applying an Anthos on bare metal user cluster configuration
    file, which is similar to the first cluster configuration, with a few minor changes.
    The high-level steps to create the first cluster follow:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用 bmctl 工具在特定的部署环境中安装第一个集群，无论选择哪种模型。在相同环境中创建附加用户集群可以通过应用 Anthos on bare
    metal 用户集群配置文件实现，该文件与第一个集群配置类似，只有一些细微的更改。创建第一个集群的高级步骤如下：
- en: Download the bmctl tool.
  id: totrans-259
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载 bmctl 工具。
- en: Use bmctl to create a cluster config template file.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 bmctl 创建集群配置模板文件。
- en: Modify the config file with desired settings.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所需的设置修改配置文件。
- en: Run bmctl to create the cluster.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 运行 bmctl 创建集群。
- en: Figure 17.9 shows the flow for the first cluster creation with bmctl.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9 展示了使用 bmctl 创建第一个集群的流程。
- en: '![](../../OEBPS/Images/17-09.png)'
  id: totrans-264
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/17-09.png)'
- en: Figure 17.9 Cluster creation flow using bmctl
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 图 17.9 使用 bmctl 创建集群的流程
- en: 'The initial four steps are initiated by the user, and when you perform step
    4, the bmctl create cluster command, the following steps are executed:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 前四个步骤由用户启动，当您执行第 4 步，即 bmctl create cluster 命令时，将执行以下步骤：
- en: '*Validate Config*—The cluster configuration file is checked to verify that
    specs are well formed, no IP address overlap occurs, service account keys are
    available, and the cluster has not been already registered in GCP console.'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*验证配置*—检查集群配置文件，以验证规范是否格式正确，没有 IP 地址冲突发生，服务账户密钥可用，并且集群尚未在 GCP 控制台中注册。'
- en: '*Create kind cluster*—As part of the setup, bmctl initially creates a temporary
    kind (Kubernetes in Docker) cluster on the admin workstation in which some of
    the resources needed for the admin cluster, such as cluster and NodePool objects
    or Secrets containing static site generator and service account keys, are created
    from the configuration file specs.'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*创建 kind 集群*—作为设置的一部分，bmctl 首先在管理员工作站上创建一个临时的 kind（Docker 中的 Kubernetes）集群，其中一些用于管理员集群的资源，如集群和
    NodePool 对象或包含静态站点生成器和服务账户密钥的 Secrets，是从配置文件规范创建的。'
- en: '*Preflight check*—Checks are performed on cluster machines and network requirements,
    such as OS version and configuration, filesystem available space, and reachability
    of GCP API endpoints.'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预检查*—对集群机器和网络要求进行检查，例如操作系统版本和配置、文件系统可用空间以及 GCP API 端点的可达性。'
- en: '*Provision bare metal cluster*—Binaries are copied to target nodes, and installation
    is executed, including node initialization and join.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*配置裸金属集群*—二进制文件被复制到目标节点，并执行安装，包括节点初始化和加入。'
- en: '*Install add-ons*—Add-on components like GKE Connect Agent, logging and monitoring
    components, bare metal operator, and MetalLB are installed.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*安装附加组件*—安装附加组件，如 GKE Connect Agent、日志和监控组件、裸金属操作员和 MetalLB。'
- en: '*Pivot*—The process of moving bare metal resources from the kind cluster to
    the provisioned cluster. Kubernetes resources will be deleted from the kind cluster
    afterward.'
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*转换*—将裸金属资源从 kind 集群移动到已配置集群的过程。之后将从 kind 集群中删除 Kubernetes 资源。'
- en: Finally, let’s dive into creating a cluster!
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们深入探讨如何创建一个集群！
- en: Logging in to GCP and downloading the bmctl tool
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 登录 GCP 并下载 bmctl 工具
- en: 'Remember that Google supplies a tool to deploy Anthos on bare metal called
    bmctl. On the workstation that you will use to deploy the cluster, download the
    bmctl tool by following the steps here (for our example, we will assume the working
    directory is ~/baremetal):'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，Google 提供了一个名为 bmctl 的工具，用于在裸金属上部署 Anthos。在您将用于部署集群的工作站上，按照以下步骤下载 bmctl 工具（在我们的示例中，我们将假设工作目录是
    ~/baremetal）：
- en: Log in with gcloud, using gcloud auth application-default login, as a user that
    has the roles described in the “*Installing account role requirements” sub*section
    of the Installation requirements section.
  id: totrans-276
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用具有“安装账户角色要求”部分中描述的角色之一的用户，通过 gcloud auth application-default login 登录 gcloud。
- en: 'Download the bmctl tool from the URL or storage bucket you will find in the
    documentation, shown here:'
  id: totrans-277
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从文档中找到的URL或存储桶下载bmctl工具，如下所示：
- en: '[PRE0]'
  id: totrans-278
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Make bmctl executable:'
  id: totrans-279
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使bmctl可执行：
- en: '[PRE1]'
  id: totrans-280
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that we have the bmctl executable, we can move on to creating the cluster
    configuration.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了bmctl可执行文件，我们可以继续创建集群配置。
- en: Creating the cluster configuration
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集群配置
- en: 'To deploy a cluster, we need to have a cluster configuration file that contains
    all the parameters and options for the deployment. The bmctl tool can create a
    new configuration file for us by using the create config option:'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 要部署集群，我们需要有一个包含部署所需所有参数和选项的集群配置文件。bmctl工具可以通过使用create config选项为我们创建一个新的配置文件：
- en: '[PRE2]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: where CLUSTER_NAME is the name you want to give to the cluster and CLOUD_PROJECT
    _ID is the project ID of the project you want to use with Anthos on bare metal.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 其中CLUSTER_NAME是你想要给集群的名字，CLOUD_PROJECT_ID是你想要与裸金属上的Anthos一起使用的项目的项目ID。
- en: If you haven’t already enabled the required APIs, you can add the --enable-apis
    option to the previous command to enable them, and if you haven’t created the
    required service accounts yet, you can add the option --create-service-accounts
    to have them created along with the needed roles.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你还没有启用所需的API，你可以在之前的命令中添加--enable-apis选项来启用它们，如果你还没有创建所需的服务帐户，你可以添加选项--create-service-accounts以在创建所需角色时同时创建它们。
- en: Populating the cluster configuration file
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 填充集群配置文件
- en: Before creating the cluster, we need to properly prepare the configuration file
    created by bmctl. The file is saved by default in a folder named with the cluster
    name inside a folder named *bmctl-workspace.* In this section, we will explain
    the options in the configuration file.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建集群之前，我们需要正确准备由bmctl创建的配置文件。该文件默认保存在名为*bmctl-workspace*的文件夹内，其名称与集群名称相同。在本节中，我们将解释配置文件中的选项。
- en: SSH private key
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: SSH私钥
- en: 'This is the SSH private key that will be used to connect to the nodes during
    the cluster deployment. Add to the sshPrivateKeyPath: spec the full path to access
    an SSH private key authorized to access all the target nodes as root, for example:'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将在集群部署期间用于连接到节点的SSH私钥。将sshPrivateKeyPath:规范添加到完整的路径，以便访问授权访问所有目标节点作为root的SSH私钥，例如：
- en: '[PRE3]'
  id: totrans-291
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: GCP service account keys
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: GCP服务帐户密钥
- en: 'If you manually created the GCP service accounts before running bmctl, you
    need to populate the related fields with the full paths to service account keys
    as in the next example. If you used the bmctl --create-service-accounts parameter,
    they will be already populated:'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经在运行bmctl之前手动创建了GCP服务帐户，你需要使用以下示例中的完整路径填充相关字段，以服务帐户密钥的完整路径。如果你使用了bmctl --create-service-accounts参数，它们将已经填充：
- en: '[PRE4]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Cluster type
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 集群类型
- en: 'Depending on the chosen cluster deployment model, set the type spec value in
    the Cluster custom resource accordingly, choosing between admin, hybrid*,* or
    standalone as in the following example:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所选的集群部署模型，在Cluster自定义资源中设置type spec的值，选择admin、hybrid*或standalone，如下例所示：
- en: '[PRE5]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Control plane configuration
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面配置
- en: 'Depending on the chosen control plane architecture, add the IP address of the
    target control plane node in the nodePoolSpec: specification in the controlPlane:
    section. An example follows with an HA architecture based on three control plane
    nodes. Remember, if you want to enable a highly available control plane, you need
    to supply at least three IP addresses:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 根据所选的控制平面架构，在controlPlane:部分的nodePoolSpec:规范中添加目标控制平面节点的IP地址。以下是一个基于三个控制平面节点的HA架构的示例。记住，如果你想启用高度可用的控制平面，你需要提供至少三个IP地址：
- en: '[PRE6]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Pod and Services CIDR blocks
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: Pod和服务的CIDR块
- en: 'The clusterNetwork: section includes the CIDR ranges assigned to Pods and Kubernetes
    Service objects inside the cluster; these ranges are visible only inside the cluster
    and are never used externally. Change the defaults only if there is any overlap
    with existing services on your network that any running Pod in the cluster could
    need to contact.'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: clusterNetwork:部分包括分配给集群内Pods和Kubernetes Service对象的CIDR范围；这些范围仅在集群内部可见，永远不会用于外部。只有当与你的网络中现有服务的任何重叠时，才更改默认值，这些服务是集群中运行的Pod可能需要联系的服务。
- en: Load balancer configuration
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器配置
- en: 'You need to populate the loadBalancer: section based on the chosen load balancer
    mode (bundled or manual) and the desired configuration options for that mode.
    A description of the various specifications follows:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '您需要根据所选的负载均衡器模式（捆绑式或手动）以及该模式的期望配置选项来填写 loadBalancer: 部分。以下是对各种规范的描述：'
- en: mode—The load balancer mode; you need to choose between bundled or manual.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mode—负载均衡器模式；您需要在捆绑式或手动之间进行选择。
- en: ports.controlPlaneLBPort—The port on which the load balancer serves the Kubernetes
    API server.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ports.controlPlaneLBPort—负载均衡器服务 Kubernetes API 服务器所使用的端口。
- en: vips.controlPlaneVIP—The VIP assigned to the Kubernetes API server on the cluster.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vips.controlPlaneVIP—分配给集群中 Kubernetes API 服务器上的 VIP。
- en: vips.ingressVIP—The VIP assigned to the layer-7 (L7) Istio Ingress gateway on
    the cluster; this must be part of the address pool defined later. This VIP is
    needed only if the cluster is hybrid, standalone, or user; it’s not needed on
    the admin cluster and can stay commented out.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vips.ingressVIP—分配给集群中第 7 层（L7）Istio Ingress 网关的 VIP；这必须是稍后定义的地址池的一部分。如果集群是混合型、独立型或用户型，则需要此
    VIP；在管理集群中不需要，可以保持注释状态。
- en: addressPools—The pool used by the data plane load balancer to assign VIPs to
    the Ingress gateway and LoadBalancer type Kubernetes Service objects; it must
    include the Ingress VIP defined earlier, but it’s not needed on the admin cluster
    and can stay commented out.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: addressPools—数据平面负载均衡器使用的池，用于将 VIP 分配给 Ingress 网关和 LoadBalancer 类型的 Kubernetes
    服务对象；它必须包括之前定义的 Ingress VIP，但在管理集群中不需要，可以保持注释状态。
- en: nodePoolSpec—Lists the address of the nodes in which you want to deploy the
    bundled load balancers. It needs to be used only if you want to specify dedicated
    worker nodes for bundled load balancers. If left commented out, the load balancers
    will be deployed on control plane nodes.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: nodePoolSpec—列出您想要部署捆绑式负载均衡器的节点地址。只有在您想为捆绑式负载均衡器指定专用工作节点时才需要使用它。如果未注释，负载均衡器将部署在控制平面节点上。
- en: 'Remember that if the bundled load balancer is being deployed, all the VIPs
    (control plane and address pools, including the Ingress gateway) must be in the
    same subnet of the load balancer nodes. The following code shows a configuration
    for a hybrid cluster with bundled load balancers deployed on two dedicated worker
    nodes with the IP addresses 172.16.0.7 and 172.16.0.7:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，如果正在部署捆绑式负载均衡器，所有 VIP（包括控制平面和地址池以及 Ingress 网关）都必须位于负载均衡器节点的同一子网中。以下代码显示了一个配置示例，用于在两个专用工作节点（IP
    地址为 172.16.0.7 和 172.16.0.7）上部署捆绑式负载均衡器的混合集群：
- en: '[PRE7]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Proxy configuration
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 代理配置
- en: 'If nodes need to pass through an HTTP proxy to connect to the internet, populate
    the proxy: section with the needed information:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '如果节点需要通过 HTTP 代理连接到互联网，请在 proxy: 部分填写所需信息：'
- en: url—The URL that the proxy server is accessible on in the format http://username:password@fqdn:port
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: url—代理服务器可访问的 URL，格式为 http://username:password@fqdn:port
- en: noProxy—A list of IPs, hostnames, or domains that should not be proxied
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: noProxy—不应代理的 IP 地址、主机名或域名列表
- en: 'The following example configures an entry for a proxy server accessible at
    http:// 172.16.0.101:3128 with no authentication needed and a noProxy entry for
    the 172.16.0.0/16 range, which tells the system to exclude sending IPs in that
    range to the proxy server:'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例配置了一个可访问于 http://172.16.0.101:3128 的代理服务器条目，无需身份验证，并为 172.16.0.0/16 范围添加了
    noProxy 条目，该条目告诉系统不要将此范围内的 IP 地址发送到代理服务器：
- en: '[PRE8]'
  id: totrans-318
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Cloud operations for logging and monitoring
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 日志和监控的云操作
- en: 'To configure the options for logging and monitoring, you need to add the projectID
    and location, described next, in the clusterOperations: section:'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '要配置日志和监控的选项，您需要在 clusterOperations: 部分中添加 projectID 和 location，如下所述：'
- en: projectID—The project ID of the project in which you want to host metrics and
    logs.
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: projectID—您想要托管指标和日志的项目 ID。
- en: location—A Google Cloud region where you want to store logs and metrics. It’s
    a good idea to choose a region that is near your on-prem data center.
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: location—您想要存储日志和指标的 Google Cloud 区域。选择靠近您的本地数据中心所在的区域是个好主意。
- en: By default, Cloud operations collect only logs and metrics from workloads in
    the admin cluster and for user clusters and workloads in system namespaces such
    as kube-system, gke-system, gke-connect, istio-system, and config-management-system.
    System components logs and metrics are used also by Google support to troubleshoot
    problems in case of support cases. In addition to the metrics for system namespaces,
    Cloud operations also collect metrics on resource usage on nodes from all the
    Pods.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，云操作仅收集管理集群和工作负载以及系统命名空间（如kube-system、gke-system、gke-connect、istio-system和config-management-system）中的工作负载的日志和指标。系统组件的日志和指标也由Google支持用于解决支持案例中的问题。除了系统命名空间的指标外，云操作还收集来自所有Pod的节点资源使用情况的指标。
- en: You can also configure Cloud operations to collect application logs and use
    Managed Service for Prometheus to collect application metrics. You can enable
    both capabilities after installation by modifying the stackdriver custom resources,
    as in other Anthos deployment options.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以配置云操作收集应用程序日志并使用Prometheus托管服务收集应用程序指标。您可以通过修改stackdriver自定义资源来在安装后启用这两种功能，就像在其他Anthos部署选项中一样。
- en: Storage configuration
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 存储配置
- en: 'The storage: section includes the configuration for the local volume provisioner
    (LVP) that you can use to provide persistent volumes using mount points on local
    nodes. Using local persistent volumes binds the Pod to a specific disk and node.
    If that disk or node becomes unavailable, then the Pod also becomes unavailable.
    Due to this, workloads using local PVs need to be resilient to this kind of failure.
    Therefore, using local persistent volumes generally fits proof of concept or advanced
    use cases where data persistence is not critical or data is replicated to other
    volumes and is recoverable in case of node or disk unavailability.'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: storage：部分包括用于使用本地节点上的挂载点提供持久卷的本地卷提供程序（LVP）的配置。使用本地持久卷将Pod绑定到特定的磁盘和节点。如果该磁盘或节点变得不可用，则Pod也变得不可用。因此，使用本地PV的工作负载需要能够抵御此类故障。因此，通常使用本地持久卷适用于数据持久性不是关键或数据已复制到其他卷且在节点或磁盘不可用的情况下可恢复的证明概念或高级用例。
- en: 'The three types of storage classes for local PVs in an Anthos on bare metal
    cluster follow:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos裸金属集群中本地PV的存储类有三种类型如下：
- en: '*LVP node mounts*—This storage class creates a local PV for each mounted disk
    in a specified directory. Each PV maps to a disk with a capacity equal to the
    underlying disk capacity. The total number of local PVs created in the cluster
    is the number of disks mounted under the path across all nodes.'
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LVP节点挂载*—此存储类为指定目录中每个挂载的磁盘创建一个本地PV。每个PV映射到与底层磁盘容量相等的磁盘。集群中创建的本地PV总数是所有节点下路径挂载的磁盘数量。'
- en: '*LVP share*—This storage class creates a local PV backed by subdirectories
    in a local, shared filesystem on every node in the cluster. These subdirectories
    are automatically created during cluster creation. Workloads using this storage
    class will share capacity and input/output operations per second because the PVs
    are backed by the same shared filesystem.'
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*LVP共享*—此存储类为集群中每个节点的本地共享文件系统上的子目录创建一个由本地PV支持的存储类。这些子目录在集群创建期间自动创建。使用此存储类的工作负载将共享容量和每秒输入/输出操作，因为PV由相同的共享文件系统支持。'
- en: '*Anthos system*—This storage class creates preconfigured local PVs during cluster
    creation that are used by Anthos system Pods. Do not change or delete this storage
    class, and do not use this storage class for stateful apps.'
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Anthos系统*—此存储类在集群创建期间创建预配置的本地PV，供Anthos系统Pod使用。不要更改或删除此存储类，也不要为此存储类使用有状态应用。'
- en: 'The lvpNodeMounts: section contains the parameters described here to configure
    the LVP node mounts:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: lvpNodeMounts：部分包含以下参数，用于配置LVP节点挂载：
- en: path—Local node directory path under which the disk to be used as local persistent
    volumes are mounted.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: path—用作本地持久卷的磁盘挂载的本地节点目录路径。
- en: storageClassName—The StorageClass with which PVs will be created. The StorageClass
    is created during cluster creation.
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: storageClassName—将创建PV的StorageClass。StorageClass是在集群创建期间创建的。
- en: 'The lvpShare: section contains the following parameters to configure the LVP
    share:'
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: lvpShare：部分包含以下参数以配置LVP共享：
- en: path*—The local* node directory path under which subdirectories will be created
    on each host. A local PV will be created for each subdirectory.
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: path*—在每个主机上创建子目录的本地节点目录路径。将为每个子目录创建一个本地PV。
- en: storageClassName—The StorageClass with which PVs will be created. The StorageClass
    is created during cluster creation.
  id: totrans-336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: storageClassName—将创建 PV 的 StorageClass。StorageClass 在集群创建期间创建。
- en: numPVUnderSharedPath—The number of subdirectories to create under path. The
    total number of LVPs that share persistent volumes in the cluster would be this
    number multiplied by the number of nodes.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: numPVUnderSharedPath—在路径下创建的子目录数量。在集群中共享持久卷的 LVP 总数将是此数量乘以节点数。
- en: 'The example configuration that follows uses the default parameters for a hybrid
    cluster:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例配置使用混合集群的默认参数：
- en: '[PRE9]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Authentication
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 认证
- en: As mentioned earlier, Anthos on bare metal uses the Anthos Identity Service
    authentication proxy to integrate with existing identity providers through OpenID
    Connect (OIDC) or LDAP. Anthos Identity Service allows users to authenticate using
    existing corporate credentials both through the GCP console and kubectl (in that
    case, the gcloud CLI is used to authenticate and create a kubeconfig file containing
    the ID token to be used by kubectl).
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，Anthos 在裸金属上使用 Anthos Identity Service 认证代理通过 OpenID Connect (OIDC) 或 LDAP
    与现有的身份提供程序集成。Anthos Identity Service 允许用户通过 GCP 控制台和 kubectl（在这种情况下，使用 gcloud
    CLI 进行身份验证并创建一个包含用于 kubectl 的 ID 令牌的 kubeconfig 文件）使用现有的企业凭据进行身份验证。
- en: 'The authentication: section in the cluster configuration file can be used to
    configure authentication during cluster creation. It is also possible to configure
    authentication after cluster creation using the ClientConfig object or fleet-level
    Anthos Identity Service.'
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 集群配置文件中的 authentication：部分可用于在创建集群时配置身份验证。在集群创建后，也可以使用 ClientConfig 对象或集群级别的
    Anthos Identity Service 配置身份验证。
- en: 'The following example configuration sets the parameters for OIDC authentication
    during cluster creation:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例配置设置创建集群时 OIDC 认证的参数：
- en: '[PRE10]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The example shown next uses LDAP:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个示例使用 LDAP：
- en: '[PRE11]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Node pools for worker nodes
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点节点池
- en: 'If the first cluster in an installation is a hybrid or standalone cluster intended
    to host user workloads, you will need to configure the worker nodes NodePool resource
    in the cluster config file, providing the IP addresses of the target worker nodes.
    An example using three worker nodes in a node pool follows:'
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 如果安装中的第一个集群是混合或独立集群，旨在托管用户工作负载，您需要在集群配置文件中配置工作节点 NodePool 资源，提供目标工作节点的 IP 地址。以下是一个使用节点池中的三个工作节点的示例：
- en: '[PRE12]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Note If the cluster is an admin cluster, this section is not needed.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果集群是管理集群，本节内容不需要。
- en: Creating the cluster
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'Now that we have a fully populated configuration file, we can deploy the cluster
    using the bmctl create cluster option as follows:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经有一个完整的配置文件，我们可以使用 bmctl create cluster 选项部署集群，如下所示：
- en: '[PRE13]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Replace CLUSTER_NAME with the name of the cluster you defined when you created
    the cluster configuration file. This process will take some time, and once the
    cluster has been successfully created, you will be able to connect to it by using
    the generated kubeconfig file.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 将 CLUSTER_NAME 替换为您在创建集群配置文件时定义的集群名称。这个过程需要一些时间，一旦集群成功创建，您就可以通过使用生成的 kubeconfig
    文件来连接到它。
- en: Connecting to the cluster
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到集群
- en: After cluster creation is completed, you can use the kubeconfig file created
    by the installation tool inside the bmctl-workspace/CLUSTER_NAME folder to connect
    to it using kubectl.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 集群创建完成后，您可以使用安装工具在 bmctl-workspace/CLUSTER_NAME 文件夹中创建的 kubeconfig 文件来使用 kubectl
    连接到它。
- en: You can also connect from the GCP console using a bearer token. Many kinds of
    bearer tokens are supported. The easiest method is to create a Kubernetes Service
    Account in the cluster and use its bearer token to log in.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用承载令牌从 GCP 控制台连接。支持多种类型的承载令牌。最简单的方法是在集群中创建一个 Kubernetes 服务帐户，并使用其承载令牌进行登录。
- en: If you configured the cluster for identity integration with an identity provider,
    you can authorize existing users and groups to perform specific actions on specific
    resources creating RoleBindings or ClusterRoleBindings to assign them to roles
    that have the desired permissions. After you have created the needed bindings,
    you can log in to the cluster from the GCP console by selecting the option Authenticate
    with Identity Provider Configured for the Cluster.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您已将集群配置为与身份提供程序进行身份集成，您可以通过创建 RoleBindings 或 ClusterRoleBindings 来授权现有用户和组对特定资源执行特定操作，将它们分配到具有所需权限的角色。在创建所需的绑定后，您可以通过选择
    GCP 控制台中的“使用为集群配置的身份提供程序进行身份验证”选项从 GCP 控制台登录到集群。
- en: 'To authenticate to the cluster to perform actions through kubectl, you need
    to perform the following steps after you created the needed RoleBindings and/or
    ClusterRoleBindings:'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 要通过kubectl对集群进行认证以执行操作，您需要在创建了所需的RoleBindings和/或ClusterRoleBindings之后执行以下步骤：
- en: '*Create and distribute the authentication configuration file.* You need to
    create an authentication configuration file that will be distributed to the clients
    that would need to access the cluster with kubectl. This file contains the OIDC
    configuration needed from the gcloud CLI to initiate the authentication and token
    request from the client.'
  id: totrans-360
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*创建和分发认证配置文件。* 您需要创建一个认证配置文件，该文件将被分发给需要使用kubectl访问集群的客户端。此文件包含从gcloud CLI获取的OIDC配置，以从客户端启动认证和令牌请求。'
- en: 'Execute the following command from the admin workstation or any machine that
    has access to the kubeconfig file created by the installation:'
  id: totrans-361
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 从管理员工作站或任何可以访问安装过程中创建的kubeconfig文件的机器执行以下命令：
- en: '[PRE14]'
  id: totrans-362
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Replace CLUSTER_KUBECONFIG with the kubeconfig file created by the installation.
    If the command completes successfully, the authentication configuration file,
    named kubectl-anthos-config.yaml, is created in the current directory. This kubeconfig
    file provides admin access to the cluster and should be provided only to people
    who need to access the cluster with kubectl for administrative tasks. Most organizations
    should secure this file using existing security standards that are part of a “break-glass”
    process.
  id: totrans-363
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 将CLUSTER_KUBECONFIG替换为安装过程中创建的kubeconfig文件。如果命令成功完成，将在当前目录中创建名为kubectl-anthos-config.yaml的认证配置文件。此kubeconfig文件提供了对集群的admin访问权限，并且仅应提供给需要使用kubectl进行管理任务的人员。大多数组织应使用现有的安全标准来保护此文件，这些标准是“破窗”过程的一部分。
- en: '*Authenticate with the cluster.* The client machine used to access the cluster
    needs to have kubectl and the gcloud CLI, including the anthos-auth component.
    From the client machine, execute the following command to obtain an ID token from
    the OIDC provider and configure the local kubeconfig accordingly to successfully
    authenticate with the cluster:'
  id: totrans-364
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用集群进行认证。* 用于访问集群的客户端机器需要安装kubectl和gcloud CLI，包括anthos-auth组件。从客户端机器执行以下命令以从OIDC提供者获取ID令牌，并相应地配置本地kubeconfig以成功认证集群：'
- en: '[PRE15]'
  id: totrans-365
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The login options are described here:'
  id: totrans-366
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 登录选项在此处描述：
- en: CLUSTER_NAME—Optional. This is the name of the cluster as you want it to be
    defined in the target kubeconfig file. If this flag is omitted, you are prompted
    to choose from the clusters that are specified in your authentication configuration
    file.
  id: totrans-367
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLUSTER_NAME—可选。这是您希望在目标kubeconfig文件中定义的集群名称。如果省略此标志，系统将提示您从您的认证配置文件中指定的集群中选择。
- en: USER_NAME—Optional. This is the username to use in the kubeconfig file; if omitted,
    it defaults to CLUSTER_NAME-anthos-default-user.
  id: totrans-368
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: USER_NAME—可选。这是在kubeconfig文件中使用的用户名；如果省略，则默认为CLUSTER_NAME-anthos-default-user。
- en: AUTH_CONFIG_FILE_PATH—Specifies the path of the authentication configuration
    file.
  id: totrans-369
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: AUTH_CONFIG_FILE_PATH—指定认证配置文件的路径。
- en: CA_CERT_PEM_FILE—Specifies the path to a PEM certificate file from your CA,
    which is needed if the authentication configuration file is stored on an HTTPS
    server.
  id: totrans-370
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CA_CERT_PEM_FILE—指定从您的CA获取的PEM证书文件的路径，如果认证配置文件存储在HTTPS服务器上，则需要此证书。
- en: CLUSTER_KUBECONFIG—The target kubeconfig file where the OIDC ID token is written;
    if omitted, it defaults to the kubectl default location.
  id: totrans-371
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: CLUSTER_KUBECONFIG—写入OIDC ID令牌的目标kubeconfig文件；如果省略，则默认为kubectl默认位置。
- en: The command will open the browser on the OIDC provider consent login page where
    you need to insert credentials. Your kubeconfig file now contains an ID token
    that your kubectl commands will use to authenticate with the Kubernetes API server
    on your cluster.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 命令将在OIDC提供者的同意登录页面打开浏览器，您需要在此处输入凭据。您的kubeconfig文件现在包含一个ID令牌，您的kubectl命令将使用此令牌与集群上的Kubernetes
    API服务器进行认证。
- en: 17.4.2 Creating a user cluster
  id: totrans-373
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.4.2 创建用户集群
- en: 'Once you’ve created an admin or hybrid cluster, you can add user clusters to
    it. You do this by applying a new config file that contains only the Cluster and
    NodePool custom resource manifests for the new cluster. The high-level steps to
    create the first cluster follow:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您创建了一个管理员或混合集群，您就可以向其中添加用户集群。您可以通过应用一个只包含新集群的Cluster和NodePool自定义资源清单的新配置文件来完成此操作。创建第一个集群的高级步骤如下：
- en: Use bmctl to create a cluster config template file.
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用bmctl创建集群配置模板文件。
- en: Modify the config file with the desired settings.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用所需的设置修改配置文件。
- en: Apply the config file with bmctl.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 bmctl 应用配置文件。
- en: These tasks are a subset of what you already did to create the first cluster.
    In the next section, we will detail the configuration file to deploy the user
    cluster.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 这些任务是你已经为创建第一个集群所做任务的一个子集。在下一节中，我们将详细说明部署用户集群的配置文件。
- en: Creating the cluster configuration
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集群配置
- en: As you did for first cluster creation, launch the following command to create
    the cluster config file
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 与创建第一个集群时相同，使用以下命令启动集群配置文件：
- en: '[PRE16]'
  id: totrans-381
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: where CLUSTER_NAME is the name you want to give to the user cluster.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 CLUSTER_NAME 是您希望赋予用户集群的名称。
- en: Populating the cluster configuration file
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 填充集群配置文件
- en: As you did for the first cluster creation, you need to prepare the configuration
    file created by the create config command. The file is saved by default in a folder
    named with the cluster name inside a folder named bmctl-workspace.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 与创建第一个集群时相同，您需要准备由 create config 命令创建的配置文件。该文件默认保存在名为 bmctl-workspace 的文件夹内，以集群名称命名的文件夹中。
- en: For many of the sections of the config file, the same instructions already given
    for first cluster creation apply to the user cluster, too, so follow the instructions
    given in the “Populating the cluster configuration file” in section 17.4.1.
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 对于配置文件中的许多部分，已经为第一个集群创建提供的相同说明也适用于用户集群，因此请遵循第 17.4.1 节中“填充集群配置文件”给出的说明。
- en: Note It’s important to ensure the IP addresses used in the control plane and
    load balancer sections and NodePool resources don’t overlap with the ones you
    already used for the first cluster.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：确保控制平面和负载均衡器部分以及 NodePool 资源中使用的 IP 地址与您为第一个集群已使用的地址不冲突。
- en: In the next section, you will find the tasks that are specific for a user cluster.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将找到针对用户集群特定的任务。
- en: Removing the credentials section
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 删除凭据部分
- en: 'The user cluster will use the credentials provided during admin/hybrid cluster
    creation, so we do not need to supply the credentials for GCP. Because these are
    not required, we need to delete the lines from the file, such as the section containing
    pointers to keys:'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群将使用在管理员/混合集群创建期间提供的凭据，因此我们不需要为 GCP 提供凭据。因为这些不是必需的，我们需要从文件中删除，例如包含指向密钥的节：
- en: '[PRE17]'
  id: totrans-390
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Cluster type
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 集群类型
- en: 'Set the type spec value in the Cluster to user:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群中将类型规范值设置为用户：
- en: '[PRE18]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Load balancer config
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器配置
- en: 'Next, you need to supply the configuration for the user cluster load balancer
    as follows. The IP addresses used here cannot overlap with those assigned to the
    first cluster’s load balancing:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您需要提供用户集群负载均衡器的配置，如下所示。这里使用的 IP 地址不能与分配给第一个集群的负载均衡器重叠：
- en: '[PRE19]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: Creating the cluster
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 创建集群
- en: 'After you complete the cluster configuration file, you can create the first
    cluster with the following command:'
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 完成集群配置文件后，您可以使用以下命令创建第一个集群：
- en: '[PRE20]'
  id: totrans-399
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Once the user cluster deployment is completed, you can use the kubeconfig that
    is generated to connect to the new cluster.
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群部署完成后，您可以使用生成的 kubeconfig 连接到新集群。
- en: Connecting to the cluster
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到集群
- en: 'After cluster creation, you can get the kubeconfig to connect to it using kubectl
    from the Secret created in the user cluster namespace by the installation process.
    An example command to extract the kubeconfig follows:'
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 集群创建后，您可以使用 kubectl 从用户集群命名空间中安装过程创建的 Secret 中提取 kubeconfig 来连接到它。以下是一个提取 kubeconfig
    的示例命令：
- en: '[PRE21]'
  id: totrans-403
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: You can also connect from the GCP console using a bearer token, or, if you configured
    the cluster for identity integration, users can authenticate with the cluster
    from the GCP console and gcloud CLI following the steps described in “Connecting
    to the cluster” in section 17.4.1.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用承载令牌从 GCP 控制台连接，或者如果您已为集群配置了身份集成，用户可以按照第 17.4.1 节中“连接到集群”所述的步骤从 GCP 控制台和
    gcloud CLI 验证集群身份。
- en: 17.5 Upgrading clusters
  id: totrans-405
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 17.5 升级集群
- en: When a new version of Anthos on bare metal is released, you can upgrade your
    clusters. In nonstandalone cluster installations, you need to upgrade the admin/hybrid
    cluster first, and then the user clusters.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 当 Anthos on bare metal 发布新版本时，您可以升级您的集群。在非独立集群安装中，您需要首先升级管理员/混合集群，然后升级用户集群。
- en: 17.5.1 Upgrading an admin, standalone, or hybrid cluster
  id: totrans-407
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.5.1 升级管理员、独立或混合集群
- en: 'The steps to perform an upgrade to an admin, standalone, or hybrid cluster
    follow:'
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 执行升级到管理员、独立或混合集群的步骤如下：
- en: 'Modify the cluster config file used during cluster creation to change the Anthos
    on bare metal cluster version from the existing one to the one you want to upgrade
    to. See the following example configuration for an upgrade to version 1.13:'
  id: totrans-409
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 修改在集群创建过程中使用的集群配置文件，以将 Anthos 在裸金属集群的版本从现有版本更改为要升级到的版本。以下是一个升级到版本 1.13 的示例配置：
- en: '[PRE22]'
  id: totrans-410
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Download the desired version of the bmctl tool (the version to which you want
    to upgrade the cluster):'
  id: totrans-411
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 下载所需的 bmctl 工具版本（您要将集群升级到的版本）：
- en: '[PRE23]'
  id: totrans-412
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE23]'
- en: Execute the following command to upgrade the cluster
  id: totrans-413
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以升级集群
- en: '[PRE24]'
  id: totrans-414
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE24]'
- en: where CLUSTER_NAME is the name of the cluster and ADMIN_KUBECONFIG is the kubeconfig
    file created by the installation.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，CLUSTER_NAME 是集群名称，ADMIN_KUBECONFIG 是由安装创建的 kubeconfig 文件。
- en: 17.5.2 Upgrading a user cluster
  id: totrans-416
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 17.5.2 升级用户集群
- en: 'After you have upgraded the admin or hybrid cluster, you can upgrade the user
    cluster(s) with the following steps:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 在您升级了管理员或混合集群之后，可以按照以下步骤升级用户集群：
- en: As done for the admin/hybrid cluster config file, modify the user cluster config
    file to change the Anthos on bare metal cluster version from the existing one
    to the one you want to upgrade to.
  id: totrans-418
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如同管理员/混合集群配置文件的做法，修改用户集群配置文件，以将 Anthos 在裸金属集群的版本从现有版本更改为要升级到的版本。
- en: Execute the following command to upgrade the cluster version
  id: totrans-419
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 执行以下命令以升级集群版本
- en: '[PRE25]'
  id: totrans-420
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE25]'
- en: where CLUSTER_NAME is the user cluster name to be upgraded and ADMIN_ KUBECONFIG
    is the kubeconfig file created by the installation of the first admin/ hybrid
    cluster.
  id: totrans-421
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 其中，CLUSTER_NAME 是要升级的用户集群名称，而 ADMIN_KUBECONFIG 是由第一个管理员/混合集群安装创建的 kubeconfig
    文件。
- en: Summary
  id: totrans-422
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Anthos on bare metal allows an organization to deploy Anthos on non-VMware platforms,
    including bare metal or alternate hypervisors.
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos 在裸金属上允许组织在非 VMware 平台上部署 Anthos，包括裸金属或替代虚拟机管理程序。
- en: Different deployment options are provided when using the bare metal installation,
    including admin/user, hybrid, and standalone clusters.
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在使用裸金属安装时，提供了不同的部署选项，包括管理员/用户、混合和独立集群。
- en: Anthos on bare metal includes multiple choices for load balancing, including
    using an external load balancer, known as manual mode, or the included option,
    known as bundled mode. Bundled mode will deploy an HAProxy solution for the control
    plane and MetalLB for workloads.
  id: totrans-425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos 在裸金属上提供了多种负载均衡选择，包括使用外部负载均衡器（称为手动模式）或包含的选项（称为捆绑模式）。捆绑模式将为控制平面部署 HAProxy
    解决方案，并为工作负载部署 MetalLB。
- en: The default storage option provided by the bare metal installation is limited
    to local host storage and should be used only for development clusters.
  id: totrans-426
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 裸金属安装提供的默认存储选项仅限于本地主机存储，并且应仅用于开发集群。
- en: Installing Anthos on bare metal provides an easy deployment and upgrade process,
    using a few self-documented configuration files that are deployed using a single
    executable, bmctl.
  id: totrans-427
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在裸金属上安装 Anthos 提供了一个简单的部署和升级过程，使用几个自文档化的配置文件，这些文件通过单个可执行文件 bmctl 部署。
