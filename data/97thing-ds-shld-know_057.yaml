- en: Chapter 53\. Causality and Fairness—Awareness in Machine Learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第53章 因果性和机器学习中的公平意识
- en: Scott Radcliffe
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 斯科特·拉德克利夫
- en: '![](Images/Scott_Radcliffe.png)'
  id: totrans-2
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Scott_Radcliffe.png)'
- en: Managing Director, MS in Business Analytics Program,
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 商业分析硕士项目董事总经理
- en: Emory University
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 埃默里大学
- en: It has become axiomatic that addressing fairness and bias in machine learning
    models is not optional. However, the race to deploy learning models has outpaced
    the development of standards and methods for detecting and systematically avoiding
    bias. This situation is due in some part to the fact that machine learning practice
    is typically not concerned with causality but rather is based on observational
    criteria. The focus is on prediction, classification, and identification. Observational
    criteria are fundamentally unable to determine whether a predictor exhibits unresolved
    discrimination.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 处理机器学习模型中的公平性和偏见已经成为不可选的命题。然而，部署学习模型的竞争已经超过了检测和系统避免偏见的标准和方法的发展。这种情况在某种程度上是因为机器学习实践通常不关注因果关系，而是基于观察标准。焦点是预测、分类和识别。观察标准基本上无法确定预测因子是否存在未解决的歧视。
- en: A long history of data analysis in the social science and medical fields has
    shown that fairness should be studied from the causal perspective. In order to
    be fairness-aware, special emphasis is placed on the assumptions that underlie
    all causal inferences, the languages used in formulating those assumptions, the
    conditional nature of all causal and counterfactual claims, and the methods that
    have been developed for the assessment of such claims.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在社会科学和医学领域的长期数据分析历史表明，公平性应该从因果关系的视角进行研究。为了关注公平性，特别强调了支持所有因果推断的假设、用于制定这些假设的语言、所有因果和反事实主张的条件性质以及已开发用于评估此类主张的方法。
- en: What is a “causal model”? Wikipedia provides a useful definition. A causal model
    (or structural causal model) is a *conceptual* model that describes the causal
    mechanisms of a system. Causal models can improve study designs by providing clear
    rules for deciding which independent variables need to be included/controlled.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是“因果模型”？维基百科提供了一个有用的定义。因果模型（或结构性因果模型）是描述系统因果机制的*概念*模型。因果模型可以通过提供清晰的规则来改进研究设计，以决定哪些独立变量需要被包含/控制。
- en: They can allow some questions to be answered from existing observational data
    without the need for an interventional study such as a randomized controlled trial.
    Some interventional studies are inappropriate for ethical or practical reasons,
    meaning that without a causal model, some hypotheses cannot be tested.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 它们可以通过现有的观察数据回答一些问题，而无需进行类似随机对照试验的干预研究。某些干预研究由于伦理或实际原因而不适当，这意味着在没有因果模型的情况下，某些假设无法得到验证。
- en: Causal models are falsifiable—meaning that if they do not match data, they must
    be rejected as invalid. They must also be credible to those close to the phenomena
    the model intends to explain.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 因果模型是可证伪的——这意味着如果它们与数据不符合，则必须被拒绝为无效。它们还必须对打算解释的现象非常可信。
- en: 'It is imperative that data science and machine learning practice include understanding
    and training in causal reasoning. Judea Pearl, a professor of computer science
    and the director of the Cognitive Systems Laboratory at UCLA, is a pioneer in
    establishing cause-and-effect relationships as a statistical and mathematical
    concept. Pearl is the author of the 2018 book *The Book of Why: The New Science
    of Cause and Effect* (Basic Books).'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学和机器学习实践必须包括对因果推理的理解和培训是至关重要的。Judea Pearl是加州大学洛杉矶分校计算机科学教授和认知系统实验室主任，他是将因果关系建立为统计和数学概念的先驱。Pearl是2018年著作《*为什么：因果关系的新科学*》（基础书籍出版）的作者。
- en: The central metaphor driving the narrative of *The Book of Why* is three ascending
    rungs of what the author calls the “ladder of causation.” The lowest rung deals
    simply with observation—basically looking for regularities in past behavior. Pearl
    places “present-day learning machines squarely on rung one.” While it is true
    that the explosion of computing power and accessible deep datasets have yielded
    many surprising and important results, the mechanics still operate “in much the
    same way that a statistician tries to fit a line to a collection of points.”
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: '*《为什么之书》*的核心隐喻是作者称之为“因果推理梯子”的三个升级阶段。最低的阶段仅仅涉及观察，基本上是寻找过去行为的规律性。珍珠将“当今学习机器”明确置于第一阶段。“尽管计算能力的爆炸和可接触的深度数据集产生了许多令人惊讶和重要的结果，但机制仍然“基本上是统计学家试图将一条线拟合到一组点的方式。”'
- en: The second rung of the ladder of causation moves from seeing to doing. That
    is, it goes from asking what happened to asking what would happen based on possible
    interventions. Pearl notes that “many scientists have been traumatized to learn
    that none of the methods they learned in statistics is sufficient to articulate,
    let alone answer, a simple question like ‘What happens if we double the price?’”
    *The Book of Why* provides a detailed explanation and history of how and when
    a model alone can answer such questions in the absence of live experiments.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 因果推理梯子的第二阶段从看到行动转变。也就是说，它从询问发生了什么转变为基于可能干预的结果会发生什么。珍珠指出，“许多科学家对于了解统计学所学方法无法足以表达，更不用说回答‘如果我们将价格翻倍会发生什么’这样简单的问题感到心碎。”
    *《为什么之书》*详细解释了在实验缺失的情况下，模型单独如何回答此类问题的历史和详细说明。
- en: 'The third and top rung of the ladder involves counterfactual questions, such
    as: what would the world be like if a different path had been taken? Such questions
    are “the building blocks of moral behavior as well as scientific thought.” The
    ability to look backward and imagine what could have been governs our judgments
    on success and failure, right and wrong.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 梯子的第三个和顶部阶段涉及反事实问题，例如：如果采取了不同的路径，世界会是什么样子？这些问题“是道德行为以及科学思想的基石。”回顾过去并想象可能会发生的事情，决定了我们对成功和失败，对与错的判断。
- en: Where is machine learning on this ladder? Achievements from state-of-the-art
    diagnosis in chest radiography to beyond-human-level skill in games such as Go
    and Dota 2 demonstrate the power and real-world utility of deep learning. Nonetheless,
    these methods are sometimes condescendingly described as mere “curve-fitting.”
    Suffice it to say that these methods amount to learning highly complex functions
    defined by the neural network architecture for connecting input X to output Y.
    For a game-playing agent, X is an observed state of the game (board positions,
    players’ health, etc.), and Y is the subsequent action or plan. As Pearl says,
    “As long as our system optimizes some property of the observed data, however noble
    or sophisticated, while making no reference to the world outside the data, we
    are back to level-1 of the hierarchy with all the limitations that this level
    entails.” Thus we find AI/ML at the first rung of Pearl’s causal inference ladder.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习在这个梯子上处于何位置？从胸部放射学的最先进诊断成就到超越人类水平的游戏技能，如围棋和Dota 2，展示了深度学习的力量和现实世界的实用性。尽管如此，有时候这些方法被傲慢地描述为简单的“曲线拟合”。可以说，这些方法涉及学习由神经网络架构定义的高度复杂函数，用于将输入X连接到输出Y。对于一个游戏代理，X是游戏的观察状态（棋盘位置，玩家健康等），而Y是随后的动作或计划。如珍珠所言，“只要我们的系统优化观察数据的某种特性，无论多么高尚或复杂，同时不引用数据之外的世界，我们回到了层次结构的第一级别，具有这个级别所带来的所有限制。”因此，我们发现AI/ML处于珍珠因果推理梯子的第一阶段。
- en: So why isn’t AI/ML practice moving up the ladder more quickly? One of the challenges
    facing data scientists and machine learning engineers interested in learning about
    causality is that most resources on the topic are geared toward the needs of statisticians
    or economists, versus those of data scientists and machine learning engineers.
    Closing this gap represents a major opportunity to advance fairness-awareness
    in tandem with the rapid advances in AI/ML technology.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 那么为什么AI/ML实践不能更快地向上移动梯子？面对数据科学家和机器学习工程师对因果关系感兴趣的挑战之一是，大多数关于此主题的资源都是针对统计学家或经济学家的需求，而不是数据科学家和机器学习工程师的需求。弥合这一差距代表了一个重大机会，可以在AI/ML技术的快速进步中推动公平意识的提升。
