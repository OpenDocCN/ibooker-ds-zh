- en: 11 App development-Developer workflows and CI/CD
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 11 应用程序开发-开发者工作流程和CI/CD
- en: This is the final chapter on Kubernetes in the real world, and the focus here
    is the practicality of developing and delivering software to run on Kubernetes.
    Whether you identify as a developer or you’re on the ops side working with developers,
    the move to containers impacts the way you work, the tools you use, and the amount
    of time and effort from making a code change to seeing it running in development
    and test environments. In this chapter, we’ll examine how Kubernetes affects both
    the *inner loop*—the developer workflow on the local machine—and the *outer loop*—the
    CI/CD workflow that pushes changes to test and production.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关于现实世界中Kubernetes的最后一章，重点是开发和交付在Kubernetes上运行的软件的实用性。无论你自认为是开发者还是你在运维方面与开发者合作，转向容器化都会影响你的工作方式、使用的工具，以及从代码更改到在开发和测试环境中看到运行所需的时间和精力。在本章中，我们将探讨Kubernetes如何影响**内部循环**——本地机器上的开发者工作流程——以及**外部循环**——将更改推送到测试和生产环境的CI/CD工作流程。
- en: How you use Kubernetes in your organization will be quite different from how
    you’ve used it so far in this book, because you’ll use shared resources like clusters
    and image registries. As we explore delivery workflows in this chapter, we’ll
    also cover lots of small details that can trip you up as you make the change to
    the real world—things like using private registries and maintaining isolation
    on a shared cluster. The main focus of the chapter is to help you understand the
    choice between a Docker-centric workflow and something more like a Platform-as-a-Service
    (PaaS) running on Kubernetes.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 你在组织中使用Kubernetes的方式将与你在这本书中迄今为止使用的方式大不相同，因为你将使用共享资源，如集群和镜像注册库。在本章中，我们将探讨交付工作流程时，我们还将涵盖许多小细节，这些细节可能会在你向现实世界转变时让你感到困惑——比如使用私有注册库和在共享集群上保持隔离。本章的主要重点是帮助你理解在以Docker为中心的工作流程和类似于在Kubernetes上运行的Platform-as-a-Service（PaaS）之间进行选择。
- en: 11.1 The Docker developer workflow
  id: totrans-3
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.1 Docker开发者工作流程
- en: 'Developers love Docker. It was voted the number one most-wanted platform and
    the number two “most loved” in Stack Overflow’s annual survey two years in a row.
    Docker makes some parts of the developer workflow incredibly easy but at a cost:
    the Docker artifacts become central to the project, and that has an impact on
    the inner loop. You can run the app in a local environment using the same technologies
    as production but only if you accept a different way of working. If you’re not
    familiar with building apps using containers, appendix A in the ebook covers that
    in detail; it’s the chapter “Packaging Applications from Source Code into Docker
    Images” from *Learn Docker in a Month of Lunches* (Manning, 2020).'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者热爱Docker。它连续两年在Stack Overflow的年度调查中被选为最受欢迎的平台和第二受欢迎的平台。Docker使开发者工作流程的一些部分变得极其简单，但代价是：Docker工件成为项目的核心，这对内部循环有影响。你可以在本地环境中使用与生产环境相同的技术运行应用程序，但前提是你接受不同的工作方式。如果你不熟悉使用容器构建应用程序，电子书的附录A详细介绍了这一点；它是来自《Learn
    Docker in a Month of Lunches》（Manning，2020）的章节“从源代码打包应用程序到Docker镜像”。
- en: In this section, we’ll walk through the developer workflow where Docker and
    Kubernetes are used in every environment, and where developers have their own
    dedicated cluster. You’ll need to have Docker running if you want to follow along
    with the exercises, so if your lab environment is Docker Desktop or K3s, then
    you’re good to go. The first thing we’ll look at is developer onboarding—joining
    a new project and getting up to speed as quickly as possible.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将逐步介绍开发工作流程，其中在每个环境中都使用Docker和Kubernetes，并且开发者拥有自己的专用集群。如果你想跟随练习，需要确保Docker正在运行。如果你的实验室环境是Docker
    Desktop或K3s，那么你已经准备好了。我们将首先关注开发者入职——加入新项目并尽可能快速地熟悉情况。
- en: Try it now This chapter offers a whole new demo app—a simple bulletin board
    where you can post details of upcoming events. It’s written in Node.js, but you
    don’t need to have Node.js installed to get up and running with the Docker workflow.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 现在就试试吧！本章提供了一个全新的演示应用程序——一个简单的公告板，你可以在这里发布即将发生的事件的详细信息。它是用Node.js编写的，但你不需要安装Node.js就可以使用Docker工作流程启动运行。
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: This is just about the simplest way you can get started as a developer on a
    new project. The only software you need installed is Docker, and then you grab
    a copy of the code, and off you go. You can see my output in figure 11.1\. I don’t
    have Node.js installed on my machine, and it doesn’t matter whether you do or
    what version you have; your results will be the same.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是作为开发者开始新项目的一种最简单的方式。你唯一需要安装的软件是 Docker，然后你获取代码的副本，就可以开始了。你可以看到图 11.1 中的我的输出。我没有在我的机器上安装
    Node.js，而且你是否有 Node.js 以及它的版本无关紧要；你的结果将会相同。
- en: '![](../Images/11-1.jpg)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-1.jpg)'
- en: Figure 11.1 Developer onboarding is a breeze with Docker and Compose—if there
    are no problems.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.1 使用 Docker 和 Compose 进行开发者入职非常简单——如果没有问题的话。
- en: 'Behind the magic are two things: a Dockerfile, which has all the steps to build
    and package the Node.js component, and a Docker Compose file, which specifies
    all the components and the path to their Dockerfiles. There’s only one component
    in this app, but there could be a dozen-all using different technologies—and the
    workflow would be the same. But this isn’t how we’re going to run the app in production,
    so if we want to use the same technology stack, we can switch to running the app
    in Kubernetes locally, just using Docker for the build.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在魔法背后是两件事：一个 Dockerfile，其中包含构建和打包 Node.js 组件的所有步骤，以及一个 Docker Compose 文件，它指定了所有组件及其
    Dockerfile 的路径。在这个应用程序中只有一个组件，但可能有十几个——使用不同的技术——工作流程将是相同的。但这种方式并不是我们在生产环境中运行应用程序的方式，因此如果我们想使用相同的技术堆栈，我们可以切换到在本地运行应用程序在
    Kubernetes 中，只需使用 Docker 进行构建。
- en: Try it now Simple Kubernetes manifests for running the app using the local image
    are in the source folder. Remove the Compose version of the app, and deploy it
    to Kubernetes instead.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 简单的 Kubernetes 清单，用于使用本地镜像运行应用程序，位于源文件夹中。删除应用程序的 Compose 版本，并将其部署到 Kubernetes
    中。
- en: '[PRE1]'
  id: totrans-13
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'This workflow is still pretty simple, although we now have three container
    artifacts to work with: the Dockerfile, the Compose file, and the Kubernetes manifest.
    I have my own Kubernetes cluster, and with that I can run the app exactly as it
    will run in production. My output in figure 11.2 shows it’s the same app, using
    the same local image, built with Docker Compose in the previous exercise.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管我们现在有三个容器工件需要处理：Dockerfile、Compose 文件和 Kubernetes 清单，但此工作流程仍然相当简单。我拥有自己的 Kubernetes
    集群，因此我可以以生产环境中的方式运行应用程序。图 11.2 中的输出显示，这是同一个应用程序，使用相同的本地镜像，在前一个练习中使用 Docker Compose
    构建。
- en: '![](../Images/11-2.jpg)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-2.jpg)'
- en: Figure 11.2 You can mix Docker and Kubernetes using Compose to build images
    to run in Pods.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.2 你可以使用 Compose 混合 Docker 和 Kubernetes，以在 Pods 中运行构建的镜像。
- en: Kubernetes is happy to use a local image that you’ve created or pulled with
    Docker, but you must follow some rules about whether it uses the local image or
    pulls it from a registry. If the image doesn’t have an explicit tag in the name
    (and uses the default `:latest` tag), then Kubernetes will always try to pull
    the image first. Otherwise, Kubernetes will use the local image if it exists in
    the image cache on the node. You can override the rules by specifying an image
    pull policy. Listing 11.1 shows the Pod spec for the bulletin board app, which
    includes an explicit policy.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 愿意使用你使用 Docker 创建或拉取的本地镜像，但你必须遵循一些规则，关于它是否使用本地镜像或从仓库中拉取。如果镜像名称中没有显式的标签（并使用默认的
    `:latest` 标签），那么 Kubernetes 将始终尝试首先拉取镜像。否则，如果节点上的镜像缓存中存在本地镜像，Kubernetes 将使用本地镜像。你可以通过指定镜像拉取策略来覆盖这些规则。列表
    11.1 显示了公告板应用程序的 Pod 规范，其中包含一个显式的策略。
- en: Listing 11.1 bb-deployment.yaml, specifying image pull policies
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.1 bb-deployment.yaml，指定镜像拉取策略
- en: '[PRE2]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: That’s the sort of detail that can be a nasty stumbling block in the developer
    workflow. The Pod spec might be configured so the registry image is preferred,
    and then you can rebuild your own local image as much as you like and never see
    any changes, because Kubernetes will always use the remote image. Similar complications
    exist around image versions, because an image can be replaced with another version
    using the same name and tag. That doesn’t play well with the Kubernetes desired
    state approach, because if you deploy an update with an unchanged Pod spec, nothing
    happens, even if the image contents have changed.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这类细节可能会成为开发者工作流程中的一个大障碍。Pod 规范可能被配置为优先使用仓库镜像，然后你可以尽可能多地重建你自己的本地镜像，但永远不会看到任何变化，因为
    Kubernetes 总是使用远程镜像。在镜像版本方面也存在类似的复杂性，因为可以使用具有相同名称和标签的另一个版本替换镜像。这并不符合 Kubernetes
    所需状态的方法，因为如果你部署了一个更新，Pod 规范没有变化，即使镜像内容已经改变，也不会发生任何事情。
- en: Back to our demo app. Your first task on the project is to add some more detail
    to the events list, which is an easy code change for you. Testing your change
    is more challenging, because you can repeat the Docker Compose command to rebuild
    the image, but if you repeat the kubectl command to deploy the changes, you’ll
    see that nothing happens. If you’re into containers, you can do some investigation
    to understand the problem and delete the Pod to force a replacement, but if you’re
    not, then your workflow is already broken.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 回到我们的演示应用程序。您在项目中的第一个任务是向事件列表添加更多细节，这对您来说是一个简单的代码更改。测试您的更改更具挑战性，因为您可以通过重复Docker
    Compose命令来重新构建镜像，但如果您重复kubectl命令来部署更改，您会发现没有任何事情发生。如果您对容器感兴趣，您可以做一些调查来了解问题并删除Pod以强制替换，但如果您不感兴趣，那么您的工作流程已经中断了。
- en: Try it now You don’t really need to make a code change—a new file has the changes
    in it. Just replace the code file and rebuild the image, then delete the Pod to
    see the new app version running in the replacement Pod.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看您实际上不需要进行代码更改——新文件中包含了更改。只需替换代码文件并重新构建镜像，然后删除Pod以查看在替换Pod中运行的新应用程序版本。
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can see my output in figure 11.3\. The updated application is running in
    the screenshot, but only after the Pod was manually deleted and then recreated
    by the Deployment controller, using the latest image version.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图11.3中看到我的输出。更新的应用程序在屏幕截图上运行，但只有在Pod被手动删除并由Deployment控制器重新创建后，使用最新的镜像版本，才运行。
- en: '![](../Images/11-3.jpg)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-3.jpg)'
- en: Figure 11.3 Docker images are mutable, but renaming images doesn’t trigger an
    update in Kubernetes.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.3 Docker镜像是可以更改的，但重命名镜像不会在Kubernetes中触发更新。
- en: If you chose a Docker-centric workflow, then this is just one of the complications
    that will slow down and frustrate the development teams (debugging and making
    live app updates are the next ones they’ll hit). Container technologies are not
    easy topics to learn as you go—they really need some dedicated time to understand
    the principles, and not everyone on every team will want to make that investment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您选择以Docker为中心的工作流程，那么这只是开发团队将遇到并减缓、挫败感的工作流程中的复杂问题之一（调试和实时应用程序更新将是他们接下来会遇到的问题）。容器技术不是容易学习的话题，您需要投入一些专门的时间来理解其原理，并且并非每个团队中的每个人都会愿意进行这种投资。
- en: The alternative is to centralize all the container technologies in a single
    team that provides a CI/CD pipeline that development teams can plug into to deploy
    their apps. The pipeline takes care of packaging container images and deploying
    to the cluster, so the development teams don’t need to bring Docker and Kubernetes
    into their own work.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种选择是将所有容器技术集中在一个团队中，该团队提供一个CI/CD管道，开发团队可以将其连接到以部署他们的应用程序。该管道负责打包容器镜像并将其部署到集群中，因此开发团队不需要将Docker和Kubernetes引入自己的工作。
- en: 11.2 The Kubernetes-as-a-Service developer workflow
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.2 Kubernetes-as-a-Service开发工作流程
- en: A Platform-as-a-Service experience running on top of Kubernetes is an attractive
    option for a lot of organizations. You can run a single cluster for all your test
    environments that also hosts the CI/CD service to take care of the messy details
    about running in containers. All of the Docker artifacts are removed from the
    developer workflow so developers work on components directly, running Node.js
    and everything else they need on their machines, and they don’t use containers
    locally.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在Kubernetes之上运行的平台即服务（PaaS）体验对于许多组织来说是一个有吸引力的选择。您可以运行一个集群来处理所有测试环境，该集群还托管CI/CD服务以处理容器运行中的繁琐细节。所有Docker工件都从开发工作流程中移除，因此开发者可以直接在组件上工作，在他们的机器上运行Node.js和其他所有他们需要的软件，并且他们不使用本地容器。
- en: This method moves containers to the outer loop—when developers push changes
    to source control, that triggers a build, which creates the container images,
    pushes them to a registry, and deploys the new version to a test environment in
    the cluster. You get all the benefits of running in a container platform, without
    the friction containers bring to development. Figure 11.4 shows how that looks
    with one set of technology options.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法将容器移动到外层循环——当开发人员向源代码控制推送更改时，这会触发构建，创建容器镜像，将它们推送到注册表，并将新版本部署到集群中的测试环境中。您将获得在容器平台运行的所有好处，而无需承受容器给开发带来的摩擦。图11.4显示了使用一组技术选项时的样子。
- en: '![](../Images/11-4.jpg)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-4.jpg)'
- en: Figure 11.4 Using containers in the outer loop lets developers focus on code.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.4在外层循环中使用容器让开发者专注于代码。
- en: The promise of this approach is that you get to run your app on Kubernetes without
    affecting the developer workflow or requiring every team member to skill up on
    Docker and Compose. It can work well in organizations where development teams
    work on small components and a separate team assembles all the pieces into a working
    system, because only the assembly team needs the container skills. You can also
    remove Docker entirely, which is useful if your cluster uses a different container
    runtime. If you want to build container images without Docker, however, you need
    to replace it with a lot of other moving pieces. You’ll end up with more complexity
    overall, but it will be centralized in the delivery pipelines and not the projects.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法的承诺是，您可以在不影响开发工作流程或不需要每个团队成员都掌握Docker和Compose技能的情况下在Kubernetes上运行您的应用程序。它可以在开发团队专注于小型组件而另一个团队将这些组件组装成工作系统的组织中很好地工作，因为只有组装团队需要容器技能。您还可以完全删除Docker，如果您的集群使用不同的容器运行时，这很有用。但是，如果您想在没有Docker的情况下构建容器镜像，则需要用许多其他组件来替换它。您最终将拥有更多的复杂性，但这些复杂性将集中在交付管道而不是项目中。
- en: We’ll walk through an example of that in this chapter, but to manage the complexity,
    we’ll do it in stages, starting with the view from inside the build service. To
    keep it simple, we’ll run our own Git server so we can push changes and trigger
    builds all from our lab cluster.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章中通过一个示例来展示这一点，但为了管理复杂性，我们将分阶段进行，首先从构建服务的内部视角开始。为了简化，我们将运行自己的Git服务器，这样我们就可以从我们的实验室集群中推送更改并触发构建。
- en: Try it now Gogs is a simple but powerful Git server that is published as an
    image on Docker Hub. It’s a great way to run a private Git server in your organization
    or to quickly spin up a backup if your online service goes offline. Run Gogs in
    your cluster to push a local copy of the book’s source code.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 Gogs是一个简单但强大的Git服务器，它作为Docker Hub上的镜像发布。这是在您的组织中运行私有Git服务器或快速启动备份（如果您的在线服务离线）的好方法。在您的集群中运行Gogs以推送书籍源代码的本地副本。
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Figure 11.5 shows my output. You don’t need to run your own Git server for this
    workflow; it works in the same way using GitHub or any other source control system,
    but doing this makes for an easily reproducible environment—the Gogs setup for
    the chapter is preconfigured with a user account, so you can get up and running
    quickly.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5显示了我的输出。您不需要运行自己的Git服务器来完成此工作流程；使用GitHub或任何其他源代码控制系统也可以以相同的方式工作，但这样做可以创建一个易于复制的环境——本章的Gogs设置已预配置了用户账户，因此您可以快速启动并运行。
- en: '![](../Images/11-5.jpg)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-5.jpg)'
- en: Figure 11.5 Running your own Git server in Kubernetes is easy with Gogs.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.5 使用Gogs在Kubernetes中运行自己的Git服务器非常简单。
- en: Now we have a local source control server into which we can plug the other components.
    Next is a system that can build container images. To make this portable so it
    runs on any cluster, we need something that doesn’t require Docker, because the
    cluster might use a different container runtime. We have a few options, but one
    of the best is BuildKit, an open source project from the Docker team. BuildKit
    started as a replacement for the image-building component inside the Docker Engine,
    and it has a pluggable architecture, so you can build images with or without Dockerfiles.
    You can run BuildKit as a server, so other components in the toolchain can use
    it to build images.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个本地源代码服务器，我们可以将其与其他组件连接起来。接下来是一个可以构建容器镜像的系统。为了使其可移植，以便在任何集群上运行，我们需要一个不需要Docker的东西，因为集群可能使用不同的容器运行时。我们有几种选择，但其中之一是BuildKit，这是Docker团队的一个开源项目。BuildKit最初是作为Docker
    Engine内部镜像构建组件的替代品，它具有可插拔的架构，因此您可以使用或不需要Dockerfile来构建镜像。您可以将BuildKit作为服务器运行，这样工具链中的其他组件就可以使用它来构建镜像。
- en: Try it now Run BuildKit as a server inside the cluster, and confirm it has all
    the tools it needs to build container images without Docker.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 在集群内部运行BuildKit作为服务器，并确认它拥有构建容器镜像所需的所有工具。
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You can see my output in figure 11.6, where the BuildKit Pod is running from
    an image with BuildKit and the Git client installed but not Docker. It’s important
    to realize that BuildKit is completely standalone—it doesn’t connect to the container
    runtime in Kubernetes to build images; that’s all going to happen within the Pod.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图11.6中看到我的输出，其中BuildKit Pod从一个安装了BuildKit和Git客户端但未安装Docker的镜像中运行。重要的是要认识到BuildKit是完全独立的——它不会连接到Kubernetes中的容器运行时来构建镜像；所有这些都将发生在Pod内部。
- en: '![](../Images/11-6.jpg)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-6.jpg)'
- en: Figure 11.6 BuildKit running as a container image—building service, without
    requiring Docker
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.6 BuildKit 作为容器镜像运行——构建服务，无需 Docker
- en: We need to set up a few more pieces before we can see the full PaaS workflow,
    but we have enough in place now to see how the build part of it works. We’re targeting
    a Docker-free approach here, so we’re going to ignore the Dockerfile we used in
    the last section and build the app into a container image directly from source
    code. How so? By using a CNCF project called Buildpacks, a technology pioneered
    by Heroku to power their PaaS product.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以看到完整的 PaaS 工作流程之前，我们还需要设置一些其他组件，但现在我们已经有了足够的组件来了解构建部分是如何工作的。我们在这里的目标是采用无
    Docker 的方法，因此我们将忽略上一节中使用的 Dockerfile，并直接从源代码构建应用程序到容器镜像中。如何做到这一点？通过使用一个名为 Buildpacks
    的 CNCF 项目，这是一个 Heroku 领先的技术，用于推动他们的 PaaS 产品。
- en: 'Buildpacks use the same concept as multistage Dockerfiles: running the build
    tools inside a container to compile the app and then packaging the compiled app
    on top of another container image that has the application runtime. You can do
    that with a tool called Pack, which you run over the source code for your app.
    Pack works out what language you’re using, matches it to a Buildpack, and then
    packages your app into an image—no Dockerfile required. Right now Pack runs only
    with Docker, but we’re not using Docker, so we can use an alternative to integrate
    Buildpacks with BuildKit.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: Buildpacks 使用与多阶段 Dockerfile 相同的概念：在容器内运行构建工具来编译应用程序，然后在具有应用程序运行时的另一个容器镜像上打包编译后的应用程序。你可以使用一个名为
    Pack 的工具来完成这项工作，你需要在应用程序的源代码上运行它。Pack 会确定你使用的语言，将其与 Buildpack 匹配，然后将你的应用程序打包成一个镜像——无需
    Dockerfile。目前 Pack 只能在 Docker 上运行，但我们没有使用 Docker，因此我们可以使用一个替代方案来将 Buildpacks 与
    BuildKit 集成。
- en: Try it now We’re going to step inside the build process to manually run a build
    that we’ll go on to automate later in the chapter. Connect to the BuildKit Pod,
    pull the book’s code from your local Git server, and build it using Buildpacks
    instead of the Dockerfile.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 我们将进入构建过程，手动运行一个我们将在本章后面自动化的构建。连接到 BuildKit Pod，从你的本地 Git 服务器拉取书籍的代码，并使用
    Buildpacks 而不是 Dockerfile 来构建它。
- en: '[PRE6]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: This exercise takes a while to run, but keep an eye on the output from BuildKit,
    and you’ll see what’s happening—first, it downloads the component that provides
    the Buildpacks integration, and then that runs and finds this is a Node.js app;
    it packages the app into a compressed archive and then exports the archive into
    a container image that has the Node.js runtime installed. My output is shown in
    figure 11.7.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 这个练习需要一段时间才能运行，但请密切关注 BuildKit 的输出，你将看到正在发生的事情——首先，它下载提供 Buildpacks 集成的组件，然后运行并发现这是一个
    Node.js 应用程序；它将应用程序打包成一个压缩归档，然后将归档导出到一个已安装 Node.js 运行时的容器镜像中。我的输出如图 11.7 所示。
- en: '![](../Images/11-7.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-7.jpg)'
- en: Figure 11.7 Building container images without Docker and Dockerfiles adds a
    lot of complexity.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.7 无 Docker 和 Dockerfile 构建容器镜像增加了许多复杂性。
- en: You can’t run a container from that image on the BuildKit Pod because it doesn’t
    have a container runtime configured, but BuildKit is able to push images to a
    registry after building, and that’s what we’ll do in the complete workflow. So
    far, we’ve seen that you can build and package your apps to run in containers
    without Dockerfiles or Docker, which is pretty impressive, but it comes at a cost.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 你不能在 BuildKit Pod 上从这个镜像运行容器，因为它没有配置容器运行时，但 BuildKit 在构建后能够将镜像推送到注册表，这就是我们在完整工作流程中要做的。到目前为止，我们已经看到可以在没有
    Dockerfile 或 Docker 的情况下构建和打包应用程序以在容器中运行，这相当令人印象深刻，但这也带来了一定的代价。
- en: The biggest issue is the complexity of the build process and the maturity of
    all the pieces. BuildKit is a stable tool, but it isn’t anywhere near as well
    used as the standard Docker build engine. Buildpacks are a promising approach,
    but the dependency on Docker means they don’t work well in a Docker-free environment
    like a managed Kubernetes cluster in the cloud. The component we’re using to bridge
    them is a tool written by Tõnis Tiigi, a maintainer on the BuildKit project. It’s
    really just a proof of concept to plug Buildpacks into BuildKit; it works well
    enough to demonstrate the workflow, but it’s not something you would want to rely
    on to build apps for production.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的问题是构建过程的复杂性和所有组件的成熟度。BuildKit 是一个稳定的工具，但它并没有像标准的 Docker 构建引擎那样被广泛使用。Buildpacks
    是一种有希望的方法，但由于对 Docker 的依赖，它们在云中管理的 Kubernetes 集群等无 Docker 环境中工作得并不好。我们用来连接它们的组件是由
    BuildKit 项目维护者 Tõnis Tiigi 编写的工具。这实际上只是一个将 Buildpacks 插入 BuildKit 的概念证明；它足够好，可以演示工作流程，但它不是你想要用于生产应用程序构建的东西。
- en: There are alternatives. GitLab is a product that combines a Git server with
    a build pipeline that uses Buildpacks, and Jenkins X is a native build server
    for Kubernetes. They are complex products themselves, and you need to be aware
    that if you want to remove Docker from your developer workflow, you’ll be trading
    it for more complexity in the build process. You’ll be able to decide whether
    the result is worth it by the end of this chapter. Next we’ll look at how you
    can isolate workloads in Kubernetes, so a single cluster can run your delivery
    pipelines and all your test environments.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有其他选择。GitLab是一个将Git服务器与使用Buildpacks的构建管道结合在一起的产品，而Jenkins X是Kubernetes的原生构建服务器。它们本身是复杂的产品，你需要意识到，如果你想从你的开发者工作流程中移除Docker，你将在构建过程中以更多的复杂性为代价。你将在本章结束时能够决定结果是否值得。接下来，我们将看看如何在Kubernetes中隔离工作负载，以便单个集群可以运行你的交付管道和所有测试环境。
- en: 11.3 Isolating workloads with contexts and namespaces
  id: totrans-57
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.3 使用上下文和命名空间隔离工作负载
- en: Way back in chapter 3, I introduced Kubernetes namespaces—and very quickly moved
    on. You need to be aware of them to make sense of the fully qualified DNS names
    Kubernetes uses for Services, but you don’t need to use them until you start dividing
    up your cluster. Namespaces are a grouping mechanism—every Kubernetes object belongs
    to a namespace—and you can use multiple namespaces to create virtual clusters
    from one real cluster.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，我介绍了Kubernetes的命名空间——并且很快转到了其他内容。你需要了解它们才能理解Kubernetes为服务使用的完全限定DNS名称，但你不需要使用它们，直到你开始划分你的集群。命名空间是一种分组机制——每个Kubernetes对象都属于一个命名空间——你可以使用多个命名空间从一个真实集群中创建虚拟集群。
- en: Namespaces are very flexible, and organizations use them in different ways.
    You might use them in a production cluster to divide it up for different products
    or to divide up a nonproduction cluster for different environments—integration
    test, system test, and user testing. You might even have a development cluster
    where each developer has their own namespace, so they don’t need to run their
    own cluster. Namespaces are a boundary where you can apply security and resource
    restrictions, so they support all these scenarios. We’ll use a dedicated namespace
    in our CI/CD deployment, but we’ll start with a simple walkthrough.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间非常灵活，组织以不同的方式使用它们。你可能在生产集群中使用它们来划分不同的产品，或者划分非生产集群以适应不同的环境——集成测试、系统测试和用户测试。你甚至可能有一个开发集群，其中每个开发者都有自己的命名空间，这样他们就不需要运行自己的集群。命名空间是一个边界，你可以在这里应用安全和资源限制，因此它们支持所有这些场景。在我们的CI/CD部署中，我们将使用一个专用的命名空间，但我们将从简单的流程开始。
- en: Try it now Kubectl is namespace aware. You can explicitly create a namespace,
    and then deploy and query resources using the `namespace` flag—this creates a
    simple sleep Deployment.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 Kubectl是命名空间感知的。你可以显式创建一个命名空间，然后使用`namespace`标志部署和查询资源——这将创建一个简单的sleep
    Deployment。
- en: '[PRE7]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: My output is shown in figure 11.8, where you can see that namespaces are an
    essential part of resource metadata. You need to explicitly specify the namespace
    to work with an object in kubectl. The only reason we’ve avoided this for the
    first 10 chapters is that every cluster has a namespace called `default`, which
    is used if you don’t specify a namespace, and that’s where we’ve created and used
    everything so far.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出显示在图11.8中，你可以看到命名空间是资源元数据的一个基本组成部分。你需要明确指定命名空间才能在kubectl中使用对象。我们之所以在前10章中避免这样做，唯一的原因是每个集群都有一个名为`default`的命名空间，如果你没有指定命名空间，就会使用这个命名空间，而且我们到目前为止一直在那里创建和使用一切。
- en: '![](../Images/11-8.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图11.8](../Images/11-8.jpg)'
- en: Figure 11.8 Namespaces isolate workloads—you can use them to represent different
    environments.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.8 命名空间隔离工作负载——你可以使用它们来表示不同的环境。
- en: Objects within a namespace are isolated, so you can deploy the same apps with
    the same object names in different namespaces. Resources can’t see resources in
    other namespaces. Kubernetes networking is flat, so Pods in different namespaces
    can communicate through Services, but a controller looks for Pods only in its
    own namespace. Namespaces are ordinary Kubernetes resources, too. Listing 11.2
    shows a namespace spec in YAML, along with the metadata for another sleep Deployment
    that uses the new namespace.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间内的对象是隔离的，因此你可以在不同的命名空间中部署具有相同对象名称的相同应用程序。资源不能看到其他命名空间中的资源。Kubernetes的网络是扁平的，所以不同命名空间中的Pod可以通过服务进行通信，但控制器只在其自己的命名空间中查找Pod。命名空间也是普通的Kubernetes资源。列表11.2显示了YAML中的命名空间规范，以及使用新命名空间的其他sleep
    Deployment的元数据。
- en: Listing 11.2 sleep-uat.yaml, a manifest that creates and targets a namespace
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 11.2 sleep-uat.yaml，一个创建并针对命名空间的清单
- en: '[PRE8]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The Deployment and Pod specs in that YAML file use the same names as the objects
    you deployed in the previous exercise, but because the controller is set to use
    a different namespace, all the objects it creates will be in that namespace, too.
    When you deploy this manifest, you’ll see the new objects created without any
    naming collisions.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 该 YAML 文件中的 Deployment 和 Pod 规范使用与你在上一个练习中部署的对象相同的名称，但由于控制器设置为使用不同的命名空间，它创建的所有对象也将位于该命名空间中。当你部署此清单时，你会看到创建的新对象而不会出现任何命名冲突。
- en: Try it now Create a new `UAT` namespace and Deployment from the YAML in listing
    11.2\. The controller uses the same name, and you can see objects across namespaces
    using kubectl. Deleting a namespace deletes all its resources.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：从列表 11.2 中的 YAML 创建一个新的 `UAT` 命名空间和部署。控制器使用相同的名称，并且你可以使用 kubectl 在命名空间之间查看对象。删除命名空间将删除其所有资源。
- en: '[PRE9]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: You can see my output in figure 11.9\. The original sleep Deployment didn’t
    specify a namespace in the YAML file, and we created it in the `kiamol-ch11-test`
    namespace by specifying that in the kubectl command. The second sleep Deployment
    specified the `kiamol-ch11-uat` namespace in the YAML, so it was created there
    without needing a kubectl namespace flag.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 11.9 中看到我的输出。原始的 sleep 部署在 YAML 文件中没有指定命名空间，我们通过在 kubectl 命令中指定它，在 `kiamol-ch11-test`
    命名空间中创建了它。第二个 sleep 部署在 YAML 中指定了 `kiamol-ch11-uat` 命名空间，因此它在那里创建，无需使用 kubectl
    命名空间标志。
- en: '![](../Images/11-9.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-9.jpg)'
- en: Figure 11.9 Namespaces are a useful abstraction for managing groups of objects.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.9 命名空间是管理对象组的有用抽象
- en: In a shared cluster environment, you might regularly use different namespaces—deploying
    apps in your own development namespace and then looking at logs in the test namespace.
    Switching between them using kubectl flags is time consuming and error prone,
    and kubectl provides an easier way with *contexts*. A context defines the connection
    details for a Kubernetes cluster and sets the default namespace to use in kubectl
    commands. Your lab environment will already have a context set up, and you can
    modify that to switch namespaces.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 在共享集群环境中，你可能经常使用不同的命名空间——在自己的开发命名空间中部署应用程序，然后在测试命名空间中查看日志。使用 kubectl 标志在它们之间切换既耗时又容易出错，而
    kubectl 提供了一种更简单的方法，即 *上下文*。上下文定义了 Kubernetes 集群的连接细节，并设置在 kubectl 命令中使用的默认命名空间。你的实验环境已经设置了一个上下文，你可以修改它以切换命名空间。
- en: Try it now Show your configured contexts, and update the current one to set
    the default namespace to the test namespace.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下：显示你的配置上下文，并将当前上下文更新为将默认命名空间设置为测试命名空间。
- en: '[PRE10]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: You can see in figure 11.10 that setting the namespace for the context sets
    the default namespace for all kubectl commands. Any queries that don’t specify
    a namespace and any `create` commands where the YAML doesn’t specify a namespace
    will now all use the test namespace. You can create multiple contexts, all using
    the same cluster but different namespaces, and switch between them with the kubectl
    `use-context` command.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在图 11.10 中看到，为上下文设置命名空间将设置所有 kubectl 命令的默认命名空间。任何未指定命名空间的查询以及任何 YAML 中未指定命名空间的
    `create` 命令现在都将使用测试命名空间。你可以创建多个上下文，所有这些上下文都使用相同的集群但不同的命名空间，并且可以使用 kubectl 的 `use-context`
    命令在它们之间切换。
- en: '![](../Images/11-10.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/11-10.jpg)'
- en: Figure 11.10 Contexts are an easy way to switch between namespaces and clusters.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.10 上下文是切换命名空间和集群的简单方法。
- en: The other important use for contexts is to switch between clusters. When you
    set up Docker Desktop or K3s, they create a context for your local cluster—the
    details all live in a configuration file, which is stored in the `.kube` directory
    in your home folder. Managed Kubernetes services usually have a feature to add
    a cluster to your config file, so you can work with remote clusters from your
    local machine. The remote API server will be secured using TLS, and your kubectl
    configuration will use a client certificate to identify you as the user. You can
    see those security details by viewing the configuration.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文的另一个重要用途是切换集群。当你设置 Docker Desktop 或 K3s 时，它们会为你的本地集群创建一个上下文——所有细节都存储在配置文件中，该文件存储在你家目录中的
    `.kube` 目录中。托管 Kubernetes 服务通常具有将集群添加到配置文件的功能，因此你可以从本地机器上与远程集群一起工作。远程 API 服务器将使用
    TLS 加密，你的 kubectl 配置将使用客户端证书来识别你作为用户。你可以通过查看配置来查看这些安全细节。
- en: Try it now Reset your context to use the default namespace, and then print out
    the details of the client configuration.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 将上下文重置为使用默认命名空间，然后打印客户端配置的详细信息。
- en: '[PRE11]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Figure 11.11 shows my output, with a local connection to my Docker Desktop cluster
    using TLS certificates—which aren’t shown by kubectl—to authenticate the connection.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '图 11.11 展示了我的输出，使用 TLS 证书通过本地连接到我的 Docker Desktop 集群进行验证——这些证书在 kubectl 中没有显示。 '
- en: '![](../Images/11-11.jpg)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![图 11-11](../Images/11-11.jpg)'
- en: Figure 11.11 Contexts contain the connection details for the cluster, which
    could be local or remote.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.11 上下文包含集群的连接细节，这些细节可能是本地或远程的。
- en: 'Kubectl can also use a token to authenticate with the Kubernetes API server,
    and Pods are provided with a token they can use as a Secret, so apps running in
    Kubernetes can connect to the Kubernetes API to query or deploy objects. That’s
    a long way to getting where we want to go next: we’ll run a build server in a
    Pod that triggers a build when the source code changes in Git, builds the image
    using BuildKit, and deploys it to Kubernetes in the test namespace.'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: Kubectl 还可以使用令牌与 Kubernetes API 服务器进行身份验证，Pod 被提供了一个令牌，它们可以使用这个令牌作为 Secret，因此运行在
    Kubernetes 中的应用程序可以连接到 Kubernetes API 来查询或部署对象。这是我们想要达到的下一个目标：我们将在 Pod 中运行一个构建服务器，当
    Git 中的源代码发生变化时触发构建，使用 BuildKit 构建镜像，并将其部署到测试命名空间中的 Kubernetes。
- en: 11.4 Continuous delivery in Kubernetes without Docker
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.4 在 Kubernetes 中不使用 Docker 的持续交付
- en: Actually, we’re not quite there yet, because the build process needs to push
    the image to a registry, so Kubernetes can pull it to run Pod containers. Real
    clusters have multiple nodes, and each of them needs to be able to access the
    image registry. That’s been easy so far because we’ve used public images on Docker
    Hub, but in your own builds, you’ll push to a private repository first. Kubernetes
    supports pulling private images by storing registry credentials in a special type
    of Secret object.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们还没有完全到达那里，因为构建过程需要将镜像推送到注册库，以便 Kubernetes 可以将其拉取以运行 Pod 容器。真实集群有多个节点，每个节点都需要能够访问镜像注册库。到目前为止这很容易，因为我们使用了
    Docker Hub 上的公共镜像，但在您自己的构建中，您首先需要将镜像推送到私有仓库。Kubernetes 通过在特殊类型的 Secret 对象中存储注册库凭证来支持拉取私有镜像。
- en: You’ll need to have an account set up on an image registry to follow along with
    this section—Docker Hub is fine, or you can create a private registry on the cloud
    using Azure Container Registry (ACR) or Amazon Elastic Container Registry (ECR).
    If you’re running your cluster in the cloud, it makes sense to use that cloud’s
    registry to reduce download times, but all registries use the same API as Docker
    Hub, so they’re interchangeable.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 您需要在一个镜像仓库上设置一个账户，以便跟随本节内容——Docker Hub 是可以的，或者您可以在云上使用 Azure 容器注册库 (ACR) 或 Amazon
    弹性容器注册库 (ECR) 创建一个私有注册库。如果您在云中运行集群，使用该云的注册库来减少下载时间是有意义的，但所有注册库都使用与 Docker Hub
    相同的 API，因此它们可以互换。
- en: Try it now Create a Secret to store registry credentials. To make it easier
    to follow along, there’s a script to collect the credentials into local variables.
    Don’t worry—the scripts don’t email your credentials to me. . .
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下 创建一个密钥来存储注册库凭证。为了便于跟随，有一个脚本来收集凭证到本地变量中。不用担心——脚本不会将您的凭证发送给我。...
- en: '[PRE12]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: My output appears in figure 11.12\. I’m using Docker Hub, which lets you create
    temporary access tokens that you can use in the same way as a password for your
    account. When I’m done with this chapter, I’ll revoke the access token—that’s
    a nice security feature in Hub.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 我的输出如图 11.12 所示。我使用的是 Docker Hub，它允许您创建临时访问令牌，您可以使用它与账户密码相同的方式使用。当我完成这一章时，我会撤销访问令牌——这是
    Hub 中一个很好的安全特性。
- en: '![](../Images/11-12.jpg)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![图 11-12](../Images/11-12.jpg)'
- en: Figure 11.12 Your organization may use a private image registry—you need a Secret
    to authenticate.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 11.12 您的组织可能使用一个私有镜像仓库——您需要一个密钥来验证。
- en: Okay, now we’re ready. We have a Docker-less build server running in the BuildKit
    Pod, a local Git server we can use to quickly iterate over the build process,
    and a registry Secret stored in the cluster. We can use all of those pieces with
    an automation server to run the build pipeline, and we’ll be using Jenkins for
    that. Jenkins has a long legacy as a build server, and it’s very popular, but
    you don’t need to be a Jenkins guru to set up this build, because I have it already
    configured in a custom Docker Hub image.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在我们准备好了。我们有一个无Docker的构建服务器在BuildKit Pod中运行，一个本地的Git服务器，我们可以用它快速迭代构建过程，还有一个存储在集群中的注册表Secret。我们可以使用所有这些组件与自动化服务器一起运行构建管道，我们将使用Jenkins来完成这项工作。Jenkins作为构建服务器有着悠久的传统，并且非常受欢迎，但你不需要成为Jenkins大师就能设置这个构建，因为我已经在一个自定义的Docker
    Hub镜像中配置好了它。
- en: The Jenkins image for this chapter has the BuildKit and kubectl command lines
    installed, and the Pod is set up to surface credentials in the right places. The
    registry Secret you created in the previous exercise is mounted in the Pod container,
    so BuildKit can use it to authenticate to the registry when it pushes the image.
    Kubectl is configured to connect to the local API server in the cluster using
    the token Kubernetes provides in another Secret. Deploy the Jenkins server, and
    check that everything is correctly configured.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的Jenkins镜像已安装了BuildKit和kubectl命令行，Pod已设置好以在正确位置暴露凭证。你在之前的练习中创建的注册表Secret已挂载到Pod容器中，因此BuildKit可以使用它来在推送镜像时认证到注册表。Kubectl配置为使用Kubernetes在另一个Secret中提供的令牌连接到集群中的本地API服务器。部署Jenkins服务器，并检查一切是否配置正确。
- en: Try it now Jenkins gets everything it needs from Kubernetes Secrets, using a
    startup script in the container image. Start by deploying Jenkins and confirming
    it can connect to Kubernetes.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试吧，Jenkins可以从Kubernetes Secrets中获取所有需要的资源，使用容器镜像中的启动脚本。首先部署Jenkins并确认它能够连接到Kubernetes。
- en: '[PRE13]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: In this exercise, you’ll see kubectl report the version of your own Kubernetes
    lab cluster—that confirms the Jenkins Pod container is set up correctly to authenticate
    to Kubernetes, so it can deploy applications to the same cluster where it is running.
    My output is shown in figure 11.13.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，你会看到kubectl报告你自己的Kubernetes实验室集群的版本——这确认了Jenkins Pod容器已正确设置以认证到Kubernetes，因此它可以在运行它的同一集群中部署应用程序。我的输出显示在图11.13中。
- en: '![](../Images/11-13.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图11.13](../Images/11-13.jpg)'
- en: Figure 11.13 Jenkins runs the pipeline, so it needs authentication details for
    Kubernetes and the registry.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.13 Jenkins运行管道，因此它需要Kubernetes和注册表的认证详情。
- en: Everything is in place now for Jenkins to fetch application code from the Gogs
    Git server, connect to the BuildKit server to build the container image using
    Buildpacks and push it to the registry, and deploy the latest application version
    to the test namespace. That work is already set up using a Jenkins pipeline, but
    the pipeline steps just use simple build scripts in the application folder. Listing
    11.3 shows the build stage, which packages and pushes the image.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 现在一切准备就绪，Jenkins可以从Gogs Git服务器获取应用程序代码，连接到BuildKit服务器使用Buildpacks构建容器镜像并将其推送到注册表，并将最新应用程序版本部署到测试命名空间。这项工作已经通过Jenkins管道设置好了，但管道步骤只是使用应用程序文件夹中的简单构建脚本。列表11.3显示了构建阶段，它打包并推送镜像。
- en: Listing 11.3 build.sh, the build script using BuildKit
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.3 build.sh，使用BuildKit的构建脚本
- en: '[PRE14]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The script is an extension of the simpler BuildKit command you ran in section
    11.2, when you were pretending to be the build server. The `buildctl` command
    uses the same integration component for Buildpacks, so there’s no Dockerfile in
    here. This command runs inside the Jenkins Pod, so it specifies an address for
    the BuildKit server, which is running in a separate Pod behind the Service called
    `buildkitd`. No Docker here, either. The variables in the image name are all set
    by Jenkins, but they’re standard environment variables, so there’s no dependency
    on Jenkins in the build scripts.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 该脚本是对11.2节中当你假装自己是构建服务器时运行的更简单的BuildKit命令的扩展。`buildctl`命令使用与Buildpacks相同的集成组件，因此这里没有Dockerfile。这个命令在Jenkins
    Pod内部运行，因此指定了BuildKit服务器的地址，该服务运行在名为`buildkitd`的单独Pod后面。这里也没有Docker。镜像名称中的变量都是由Jenkins设置的，但它们都是标准环境变量，因此在构建脚本中没有对Jenkins的依赖。
- en: When this stage of the pipeline completes, the image will have been built and
    pushed to the registry. The next stage is to deploy the updated application, which
    is in a separate script, shown in listing 11.4\. You don’t need to run this yourself—it’s
    all in the Jenkins pipeline.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 当管道的这一阶段完成时，镜像将被构建并推送到注册表。下一阶段是部署更新后的应用程序，这在一个单独的脚本中，如列表11.4所示。您不需要自己运行它——所有这些都在Jenkins管道中。
- en: Listing 11.4 run.sh, the deployment script using Helm
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 列表11.4 run.sh，使用Helm的部署脚本
- en: '[PRE15]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The deployment uses Helm with a chart that has values for the parts of the image
    name. They’re set from the same variables used in the build stage, which are compiled
    from the Docker registry Secret and the build number in Jenkins. In my case, the
    first build pushes an image to Docker Hub named `sixeyed/bulletin-board:1-kiamol`
    and installs a Helm release using that image. To run the build in your cluster
    and push to your registry, you just need to log in to Jenkins and enable the build—the
    pipeline itself is already set up.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 部署使用Helm和一个具有镜像名称部分值的图表。它们来自构建阶段使用的相同变量，这些变量是从Docker注册表Secret和Jenkins中的构建号编译而来的。在我的情况下，第一次构建将镜像推送到Docker
    Hub，命名为`sixeyed/bulletin-board:1-kiamol`，并使用该镜像安装Helm发布。要在您的集群中运行构建并将其推送到您的注册表，您只需登录到Jenkins并启用构建——管道本身已经设置好了。
- en: Try it now Jenkins is running and configured, but the pipeline job isn’t enabled.
    Log in to enable the job, and you will see the pipeline execute and the app deployed
    to the cluster.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下，Jenkins正在运行并已配置，但管道作业尚未启用。登录以启用作业，您将看到管道执行并将应用程序部署到集群中。
- en: '[PRE16]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: The build should be fast because it’s using the same BuildKit server that has
    already cached the images for the Buildpack build from section 11.2\. When the
    build has completed, you can browse to the application deployed by Helm in the
    test namespace and see the app running—mine is shown in figure 11.14.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 构建应该很快，因为它使用的是已经为第11.2节中的Buildpack构建缓存的相同BuildKit服务器。当构建完成后，您可以通过测试命名空间中Helm部署的应用程序进行浏览，并看到应用程序正在运行——我的应用程序如图11.14所示。
- en: '![](../Images/11-14.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-14.jpg)'
- en: Figure 11.14 The pipeline in action, built and deployed to Kubernetes without
    Docker or Dockerfiles
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.14 管道执行中的情况，构建并部署到Kubernetes，无需Docker或Dockerfile
- en: So far so good. We’re playing the ops role, so we understand all the moving
    parts in the delivery of this app—we would own the pipeline in the Jenkinsfile
    and the application specs in the Helm chart. Lots of small fiddly details are
    in there, like the templated image name and the image pull Secret in the Deployment
    YAML, but from the developer’s point of view, that’s all hidden.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止一切顺利。我们扮演运维角色，因此我们理解这个应用程序交付过程中的所有动态部分——我们将拥有Jenkinsfile中的管道和Helm图表中的应用程序规范。其中有很多小的繁琐细节，比如模板化的镜像名称和在Deployment
    YAML中的镜像拉取Secret，但从开发者的角度来看，这些都已隐藏。
- en: The developer’s view is that you can work on the app using your local environment,
    push changes, and see them running at the test URL, without worrying what happens
    in between. We can see that workflow now. You made an application change earlier
    to add event descriptions to the site, and to deploy that, all you need to do
    is push the changes to your local Git server and wait for the Jenkins build to
    complete.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 开发者的观点是，您可以使用本地环境对应用程序进行工作，推送更改，并在测试URL上看到它们正在运行，无需担心中间发生的事情。我们现在可以看到这个工作流程。您之前对应用程序进行了更改，以添加事件描述到网站，要部署该更改，您只需将更改推送到您的本地Git服务器并等待Jenkins构建完成。
- en: Try it now Push your code change to your Gogs server; Jenkins will see the change
    within one minute and start a new build. That will push a new image version to
    your registry and update the Helm release to use that version.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 现在尝试一下，将您的代码更改推送到您的Gogs服务器；Jenkins将在一分钟内看到更改并启动新的构建。这将向您的注册表推送新的镜像版本，并更新Helm发布以使用该版本。
- en: '[PRE17]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This is the `git` `push` PaaS workflow applied to Kubernetes. We’re dealing
    with a simple app here, but the approach is the same for a large system with many
    components: a shared namespace could be the deployment target for all the latest
    versions, pushed by many different teams. Figure 11.15 shows an application update
    in Kubernetes triggered from a push of code, with no requirement for developers
    to use Docker, Kubernetes, or Helm.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 这是将`git` `push` PaaS工作流程应用于Kubernetes的示例。我们在这里处理的是一个简单的应用程序，但对于具有许多组件的大型系统，方法是一样的：共享命名空间可以是多个不同团队推送的所有最新版本的部署目标。图11.15显示了从代码推送触发的Kubernetes中的应用程序更新，无需开发者使用Docker、Kubernetes或Helm。
- en: '![](../Images/11-15.jpg)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/11-15.jpg)'
- en: Figure 11.15 It’s PaaS on your own Kubernetes cluster—a lot of complexity is
    hidden from the developer.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 图11.15：这是你自己的Kubernetes集群上的PaaS——很多复杂性对开发者来说是隐藏的。
- en: Of course, the PaaS approach and the Docker approach are not mutually exclusive.
    If your cluster is running on Docker, you can take advantage of a simpler build
    process for Docker-based apps but still support a Docker-free PaaS approach for
    other apps, all in the same cluster. Each approach offers benefits and drawbacks,
    and we’ll end by looking at how you should choose between them.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，PaaS方法和Docker方法并不是相互排斥的。如果你的集群运行在Docker上，你可以利用基于Docker的应用的更简单的构建过程，但仍然支持在同一集群中为其他应用提供无Docker的PaaS方法。每种方法都有其优点和缺点，我们将在最后探讨如何在它们之间做出选择。
- en: 11.5 Evaluating developer workflows on Kubernetes
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.5 评估Kubernetes上的开发者工作流程
- en: In this chapter, we’ve looked at developer workflows at extreme ends of the
    spectrum, from teams who fully embrace containers and want to make them front
    and center in every environment, to teams who don’t want to add any ceremony to
    their development process, want to keep working natively, and leave all the container
    bits to the CI/CD pipeline. There are plenty of places in between, and the likelihood
    is that you’ll build an approach to suit your organization, your application architectures,
    and your Kubernetes platform.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们探讨了光谱两端的开发者工作流程，从完全拥抱容器并希望在每个环境中将其置于核心位置的团队，到那些不想在开发过程中增加任何仪式、希望保持本地工作并让所有容器部分都由CI/CD管道处理的团队。中间还有很多地方，可能性很大，你将构建一个适合你组织、你的应用程序架构和你的Kubernetes平台的方法。
- en: 'The decision is as much about culture as about technology. Do you want every
    team to level up on container knowledge, or do you want to centralize that knowledge
    in a service team and leave the developer teams to focus on delivering software?
    Although I’d love to see copies of *Learn Docker in a Month of Lunches* and *Learn
    Kubernetes in a Month of Lunches* on every desk, skilling up on containers does
    require a pretty big commitment. Here are the major advantages I see in keeping
    Docker and Kubernetes visible in your projects:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这个决定与文化的关联程度不亚于与技术。你希望每个团队都提升容器知识水平，还是希望将这种知识集中在服务团队中，让开发团队专注于交付软件？虽然我很希望看到每个桌子上都有一本《一个月午餐学会Docker》和《一个月午餐学会Kubernetes》，但提升容器技能确实需要相当大的承诺。以下是我在保持Docker和Kubernetes在你的项目中可见性时看到的主要优势：
- en: The PaaS approach is complicated and bespoke—you’ll be plugging together lots
    of different technologies with different maturity levels and support structures.
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaaS方法复杂且定制化——你将连接许多不同成熟度和支持结构的技术。
- en: The Docker approach is flexible—you can add any dependencies and setup you need
    in a Dockerfile, whereas PaaS approaches are more prescriptive, so they won’t
    fit every app.
  id: totrans-127
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Docker方法很灵活——你可以在Dockerfile中添加任何所需的依赖和设置，而PaaS方法则更为具体，因此它们可能不适合每个应用。
- en: PaaS technologies don’t have the optimizations you can get when you fine-tune
    your Docker images; the bulletin board image from the Docker workflow is 95 MB
    compared to 1 GB for the Buildpacks version—that’s a much smaller surface area
    to secure.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PaaS技术没有你在微调Docker镜像时可以获得的优化；Docker工作流程中的公告板镜像为95 MB，而Buildpacks版本为1 GB——这为安全提供了更小的表面区域。
- en: The commitment to learning Docker and Kubernetes pays off because they’re portable
    skills—developers can easily move between projects using a standard toolset.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对学习Docker和Kubernetes的承诺是有回报的，因为它们是可移植的技能——开发者可以轻松地使用标准工具集在不同项目之间移动。
- en: Teams don’t have to use the full container stack; they can opt out at different
    stages—some developers might just use Docker to run containers, whereas others
    might use Docker Compose and others Kubernetes.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 团队不必使用完整的容器栈；他们可以在不同阶段选择退出——一些开发者可能只使用Docker来运行容器，而其他人可能使用Docker Compose和Kubernetes。
- en: Distributed knowledge makes for a better collaborative culture—centralized service
    teams might be resented for being the only ones who get to play with all the fun
    technology.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式知识有助于形成更好的协作文化——集中的服务团队可能会因为只有他们能够玩所有有趣的技术而受到怨恨。
- en: Ultimately, it’s a decision for your organization and teams, and the pain of
    migrating from the current workflow to the desired workflow needs to be considered.
    In my own consulting work, I’m often balancing development and operations roles,
    and I tend to be pragmatic. When I’m actively developing, I use native tooling
    (I typically work on .NET projects using Visual Studio), but before I push any
    changes, I run the CI process locally to build container images with Docker Compose
    and then spin everything up in my local Kubernetes cluster. That won’t fit every
    scenario, but I find it a good balance between development speed and confidence
    that my changes will work the same way in the next environment.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，这是一个由你的组织和团队做出的决定，并且从当前工作流程迁移到期望工作流程的痛苦需要被考虑。在我的咨询工作中，我经常在开发和运维角色之间保持平衡，我倾向于务实。当我积极开发时，我使用原生工具（我通常使用Visual
    Studio在.NET项目中工作），但在推送任何更改之前，我会在本地运行CI流程，使用Docker Compose构建容器镜像，然后在本地Kubernetes集群中启动一切。这不会适合每个场景，但我发现这是开发速度和信心之间的良好平衡，即我的更改将在下一个环境中以相同的方式工作。
- en: That’s all for the developer workflow, so we can tidy up the cluster before
    we move on. Leave your build components running (Gogs, BuildKit, and Jenkins)—you’ll
    need them for the lab.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是开发工作流程的全部内容，因此在我们继续之前，我们可以整理一下集群。留下你的构建组件运行（Gogs、BuildKit和Jenkins）——你将在实验室中需要它们。
- en: Try it now Remove the bulletin board deployments.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 现在试试看 移除公告板部署。
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 11.6 Lab
  id: totrans-136
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 11.6 实验室
- en: 'This lab is a bit nasty, so I’ll apologize in advance—but I want you to see
    that going down the PaaS path with a custom set of tools has danger in store.
    The bulletin board app for this chapter used a very old version of the Node runtime,
    version 10.5.0, and in the lab, that needs updating to a more recent version.
    There’s a new source code folder for the lab that uses Node 10.6.0, and your job
    is to set up a pipeline to build that version, and then find out why it fails
    and fix it. There are a few hints that follow because the goal isn’t for you to
    learn Jenkins but to see how to debug failing pipelines:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 这个实验室有点棘手，所以我提前道歉——但我希望你能看到，使用自定义工具集走PaaS路径是有风险的。本章的公告板应用使用了非常旧的Node运行时版本，版本号为10.5.0，在实验室中，需要将其更新到更近的版本。实验室有一个新的源代码文件夹，使用Node
    10.6.0，你的任务是设置一个管道来构建这个版本，然后找出它失败的原因并修复它。以下是一些提示，因为目标不是让你学习Jenkins，而是看看如何调试失败的管道：
- en: 'Start by creating a new item from the Jenkins home page: choose the option
    to copy an existing job, and copy the `kiamol` job; call the new job anything
    you like.'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，从Jenkins主页创建一个新项目：选择复制现有作业的选项，并复制`kiamol`作业；你可以将新作业命名为任何你喜欢的。
- en: 'In the new job configuration in the Pipeline tab, change the path to the pipeline
    file to the new source code folder: `ch11/lab/bulletin-board/Jenkinsfile`.'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在“管道”选项卡中的新作业配置中，将管道文件的路径更改为新的源代码文件夹：`ch11/lab/bulletin-board/Jenkinsfile`。
- en: Build the job, and look through the logs to find out why it failed.
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建作业，并查看日志以找出它失败的原因。
- en: You’ll need to make a change in the lab source folder and push it to Gogs to
    fix the build
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你需要在实验室源文件夹中做出更改，并将其推送到Gogs以修复构建。
- en: 'My sample solution is on GitHub with some screenshots for the Jenkins setup
    to help you: [https://github.com/sixeyed/kiamol/blob/master/ch11/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch11/lab/README.md).'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我的示例解决方案在GitHub上，有一些Jenkins设置的截图来帮助你：[https://github.com/sixeyed/kiamol/blob/master/ch11/lab/README.md](https://github.com/sixeyed/kiamol/blob/master/ch11/lab/README.md)。
