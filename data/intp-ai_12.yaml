- en: 8 Fairness and mitigating bias
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 公平性与减轻偏差
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Identifying sources of bias in datasets
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别数据集中偏差的来源
- en: Validating whether machine learning models are fair using various fairness notions
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种公平性概念验证机器学习模型的公平性
- en: Applying interpretability techniques to identify the source of discrimination
    in machine learning models
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将可解释性技术应用于识别机器学习模型中歧视的来源
- en: Mitigating bias using preprocessing techniques
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预处理技术减轻偏差
- en: Documenting datasets using datasheets to improve transparency and accountability
    and to ensure compliance with regulation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用数据表记录数据集以改善透明度和问责制，并确保符合法规
- en: You have learned a lot so far and have added a lot of interpretability techniques
    to your toolkit, ranging from those that you can use to interpret model processing
    (chapters 2 to 5) to those for interpreting representations learned by a machine
    learning model (chapters 6 and 7). We will now employ some of these techniques
    to address an important problem when building systems driven by machine learning
    models, which is tackling the problem of bias. This problem is important for multiple
    reasons. We must build systems that do not discriminate against individuals or
    users of the system. If businesses use AI for decision-making, such as providing
    opportunities to users or for some quality of service or information, biased decisions
    can incur a huge cost to the business by damaging the business’s reputation or
    by having a negative impact on their customers’ trust. Certain regions, like the
    United States and Europe, have laws that prohibit discriminating against individuals
    based on protected attributes, such as gender, race, ethnicity, sexual orientation,
    and so on. Some regulated industries, such as financial services, education, housing,
    employment, credit, and health care, prohibit or restrict the use of protected
    attributes in decision-making, and AI systems need to provide certain fairness
    guarantees.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你已经学到了很多，并将许多可解释性技术添加到你的工具箱中，从你可以用来解释模型处理的技术（第2章至第5章）到解释机器学习模型学习到的表示的技术（第6章和第7章）。现在，我们将使用其中一些技术来解决构建由机器学习模型驱动的系统时遇到的一个重要问题，即解决偏差问题。这个问题有多个原因很重要。我们必须构建不歧视个人或系统用户的系统。如果企业使用AI进行决策，例如为用户提供机会或某些服务质量或信息，那么有偏见的决策可能会通过损害企业的声誉或对客户信任产生负面影响而给企业带来巨大的成本。某些地区，如美国和欧洲，有法律禁止基于受保护属性（如性别、种族、民族、性取向等）歧视个人。一些受监管的行业，如金融服务、教育、住房、就业、信贷和医疗保健，禁止或限制在决策中使用受保护属性，并且AI系统需要提供某些公平性保证。
- en: Before we jump into the problem of bias and fairness, let’s recap the process
    that we used to build a robust AI system that addresses common issues, such as
    data leakage, bias, regulatory noncompliance, and concept drift, as shown in figure
    8.1\. The learning, testing, and understanding phases are done offline and are
    all about training the model based on historical labeled data, evaluating it,
    and understanding it using various interpretability techniques. Once the model
    is deployed, it goes online and starts to make predictions on live data. This
    model is also monitored to ensure there is no concept drift, which happens when
    the distribution of the data in the production environment deviates from that
    in the development and testing environments. There is also a feedback loop where
    new data is added back to the historical training dataset for continuous training,
    evaluation, and deployment.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入探讨偏差和公平性问题之前，让我们回顾一下我们用来构建一个解决常见问题（如图8.1所示的数据泄露、偏差、法规不合规和概念漂移）的健壮AI系统的过程。学习和测试阶段是在线完成的，都是基于历史标记数据训练模型、评估它以及使用各种可解释性技术来理解它。一旦模型部署，它就会上线并开始对实时数据进行预测。该模型也会被监控以确保没有概念漂移，这发生在生产环境中的数据分布与开发和测试环境中的数据分布不同时。还有一个反馈循环，其中新数据被添加到历史训练数据集中，以进行持续的训练、评估和部署。
- en: '![](../Images/CH08_F01_Thampi.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F01_Thampi.png)'
- en: Figure 8.1 Recap of how to build a robust AI system
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.1 回顾如何构建一个健壮的AI系统
- en: What may be the sources of bias in this system? One source, as shown in figure
    8.2, is the historical training dataset where there may be bias in the labeling
    process or bias in the sampling or data-collection process. Another source is
    the model itself, where the algorithm may be favoring certain individuals or groups
    of individuals over others. If the model is training on a dataset that is itself
    biased, then the bias is further amplified. Another source of bias is the feedback
    loop from the production environment back to the development and test environments.
    If the initial dataset or model is biased, then the deployed model in production
    will continue to make biased predictions. If the data based on these predictions
    is fed back as training data, then these biases are further reinforced and amplified.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 这个系统中可能存在哪些偏差来源？一个来源，如图8.2所示，是历史训练数据集，其中可能在标签过程中或采样或数据收集过程中存在偏差。另一个来源是模型本身，算法可能更倾向于某些个人或群体而不是其他人。如果模型在本身存在偏差的数据集上训练，那么偏差会进一步放大。偏差的另一个来源是生产环境反馈到开发和测试环境中的反馈循环。如果初始数据集或模型存在偏差，那么在生产环境中部署的模型将继续做出有偏差的预测。如果基于这些预测的数据被反馈作为训练数据，那么这些偏差将进一步得到加强和放大。
- en: '![](../Images/CH08_F02_Thampi.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F02_Thampi.png)'
- en: Figure 8.2 Sources of bias in the AI system
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.2 AI系统中的偏差来源
- en: Where does interpretability fit into the problem of bias and fairness? As seen
    in figure 8.2, we can use interpretability techniques during training and testing
    to expose issues with the historical dataset or the model. We have already seen
    this in action in chapter 3 where biases in ethnicity were exposed in the high
    school student grade-prediction problem using partial dependency plots (PDPs).
    Once the model has been deployed, we can use interpretability techniques to ensure
    that the model predictions continue to be fair.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 可解释性在偏差和公平性问题中如何定位？如图8.2所示，我们可以在训练和测试过程中使用可解释性技术来揭露历史数据集或模型的问题。我们已经在第3章中看到了这一点，在高中学生成绩预测问题中，使用部分依赖图（PDPs）揭露了种族偏差。一旦模型部署，我们可以使用可解释性技术来确保模型预测继续保持公平。
- en: In this chapter, we will delve deeper into the topic of bias and fairness using
    another concrete example of predicting the income of adults. We will then come
    up with formal definitions for various notions of fairness and use them to determine
    whether the model is biased. We will then use interpretability techniques to measure
    and expose fairness issues. We will also discuss techniques to mitigate bias.
    Lastly, we will look at a standardizing approach of documenting datasets using
    datasheets that will help improve transparency and accountability with the stakeholders
    and users of the AI system.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将通过预测成人收入的另一个具体例子，更深入地探讨偏差和公平性的问题。我们将为各种公平性概念给出正式的定义，并使用它们来判断模型是否存在偏差。然后，我们将使用可解释性技术来衡量和揭露公平性问题。我们还将讨论减轻偏差的技术。最后，我们将探讨使用数据表来标准化记录数据集的方法，这将有助于提高AI系统利益相关者和用户的透明度和问责制。
- en: 8.1 Adult income prediction
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 成人收入预测
- en: To contextualize the problem of fairness, let’s look at a concrete example.
    You are tasked by the Census Bureau to build a model to predict the income of
    adults in the United States. The prediction problem is shown in figure 8.3.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 为了具体说明公平性问题，让我们来看一个具体的例子。你被人口普查局委托构建一个模型来预测美国成人的收入。预测问题如图8.3所示。
- en: '![](../Images/CH08_F03_Thampi.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F03_Thampi.png)'
- en: Figure 8.3 Income predictor for the Census Bureau
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 人口普查局的收入预测器
- en: 'As we can see in figure 8.3, we are given various inputs for income prediction,
    such as level of education, occupation, age, gender, race, capital gains earned,
    and so on. We are tasked with building the income predictor shown as the rectangular
    box that takes these inputs and outputs a yes or no answer to the question, “Does
    the adult earn more than $50,000 per year?” This problem can, therefore, be formulated
    as a binary classification problem because we are interested in a binary answer:
    yes or no. We will treat the answer “yes” as the positive label and the answer
    “no” as the negative label.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.3所示，我们得到了用于收入预测的各种输入，例如教育水平、职业、年龄、性别、种族、资本收益等。我们的任务是构建一个收入预测器，它以矩形框的形式接受这些输入，并输出一个“是”或“否”的答案来回答“成人每年收入是否超过50,000美元？”这个问题。因此，这个问题可以表述为一个二元分类问题，因为我们感兴趣的是二元答案：是或否。我们将把答案“是”视为正标签，将答案“否”视为负标签。
- en: We are given a historical dataset from the Census Bureau consisting of 30,940
    adults. The input features are summarized in table 8.1\. From the table, we can
    see a mixture of continuous and categorical variables. Most of the datasets that
    we have dealt with in this book consisted of continuous features where the feature
    values are real numbers. We have seen how to deal with categorical features in
    chapter 3\. To recap, categorical features are features whose values are discrete
    and finite. We need to encode them into numerical values, and we have also seen
    how to do that in chapter 3 using label encoders.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了来自人口普查局的历史数据集，包含30,940名成年人。输入特征总结在表8.1中。从表中，我们可以看到连续和分类变量的混合。我们在这本书中处理的大多数数据集都由连续特征组成，其特征值是实数。我们已经在第3章中看到了如何处理分类特征。为了回顾，分类特征是值离散且有限的特征。我们需要将它们编码成数值，我们也在第3章中看到了如何使用标签编码器来完成这一操作。
- en: Table 8.1 Input features for income prediction
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 收入预测输入特征
- en: '| Feature name | Description | Type | Is protected attribute? |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 特征名称 | 描述 | 类型 | 是否为受保护属性？ |'
- en: '| age | Age of the adult | Continuous | Yes |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| age | 成年人年龄 | 连续 | 是 |'
- en: '| workclass | Class of worker | Categorical | No |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '| workclass | 工作类别 | 分类 | 否 |'
- en: '| fnlwgt | Final weight assigned by the Census Bureau | Continuous | No |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| fnlwgt | 人口普查局分配的最终权重 | 连续 | 否 |'
- en: '| education | Level of education | Categorical | No |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '| education | 教育水平 | 分类 | 否 |'
- en: '| marital-status | Marital status | Categorical | No |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| marital-status | 婚姻状况 | 分类 | 否 |'
- en: '| occupation | Occupation | Categorical | No |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| occupation | 职业 | 分类 | 否 |'
- en: '| gender | Male or female | Categorical | Yes |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| gender | 男性或女性 | 分类 | 是 |'
- en: '| race | White or Black | Categorical | Yes |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| race | 白人或黑人 | 分类 | 是 |'
- en: '| capital-gain | Capital gains | Continuous | No |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| capital-gain | 资本收益 | 连续 | 否 |'
- en: '| capital-loss | Capital losses | Continuous | No |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| capital-loss | 资本损失 | 连续 | 否 |'
- en: '| hours-per-week | Number of working hours per week | Continuous | No |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| hours-per-week | 每周工作小时数 | 连续 | 否 |'
- en: '| native-country | Country of origin | Categorical | No |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| native-country | 原籍国 | 分类 | 否 |'
- en: In addition, table 8.1 also shows whether a given feature is a protected attribute.
    Protected attributes are attributes that cannot be used to discriminate against
    an individual according to legislation largely shared by many countries. In the
    United States, for instance, the Civil Rights Act of 1964 protects individuals
    from discrimination on the basis of attributes like gender, race, age, color,
    creed, national origin, sexual orientation, and religion. In the United Kingdom,
    individuals with the same attributes are protected from discrimination according
    to the Equality Act of 2010.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，表8.1还显示了给定特征是否为受保护属性。受保护属性是指根据许多国家广泛共享的立法，不能用于歧视个人的属性。例如，在美国，1964年的民权法保护个人免受基于性别、种族、年龄、肤色、信仰、国籍、性取向和宗教等属性的歧视。在英国，具有相同属性的个人根据2010年的平等法受到保护免受歧视。
- en: 'In this dataset, we are dealing with three protected attributes: age, gender,
    and race. Age is a continuous feature, and gender and race are categorical. We
    will primarily focus on gender and race in this chapter, but we will learn how
    to extend the fairness notions and techniques to a continuous protected attribute
    like age as well. As for gender and race, we are dealing with two genders, male
    and female, and two races, white and Black, in this dataset. We unfortunately
    cannot include more gender or race groups because they are not properly represented
    in this dataset.'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个数据集中，我们处理三个受保护属性：年龄、性别和种族。年龄是一个连续特征，而性别和种族是分类的。在本章中，我们将主要关注性别和种族，但我们将学习如何将公平概念和技术扩展到像年龄这样的连续受保护属性。至于性别和种族，在这个数据集中，我们处理两种性别，男性和女性，以及两种种族，白人和黑人。遗憾的是，我们无法包括更多性别或种族群体，因为它们在这个数据集中没有得到适当的代表。
- en: 'Finally, the target variable in this dataset is binary, where 1 is used to
    indicate that the adult earns more than $50,000 per year and 0 is used to indicate
    that the salary is less than or equal to $50,000 per year. Let’s now explore this
    dataset, specifically focusing on the distribution of salary overall and for the
    two protected attributes of interest: gender and race.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，这个数据集的目标变量是二元的，其中1表示成年人每年收入超过50,000美元，0表示年薪低于或等于50,000美元。现在让我们探索这个数据集，特别是关注整体薪资分布以及两个感兴趣的受保护属性：性别和种族的分布。
- en: 8.1.1 Exploratory data analysis
  id: totrans-39
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 探索性数据分析
- en: Figure 8.4 shows the overall split of salary, gender, and race in the 30,940
    adults in the dataset provided by the Census Bureau. We can see that the dataset
    is indeed skewed or biased. Around 75% of the population earns a salary that is
    less than or equal to $50K, and the rest earn a salary greater than $50K. In terms
    of gender, male adults are more represented in this dataset than female adults,
    where around 65% of the population is male. Similarly for race, we do see a bias
    toward white adults, in that around 90% of the adults in the dataset are white.
    Note that you can find the source code used for exploratory data analysis in the
    GitHub repository associated with this book.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4显示了美国人口普查局提供的数据集中30,940名成年人的薪资、性别和种族的整体分布。我们可以看到，数据集确实存在倾斜或偏见。大约75%的人口收入低于或等于50K美元，其余人收入超过50K美元。在性别方面，男性成年人在这个数据集中比女性成年人更为代表，大约65%的人口是男性。同样，在种族方面，我们也看到对白人成年人的偏见，大约90%的成年人在数据集中是白人。请注意，您可以在与本书相关的GitHub存储库中找到用于探索性数据分析的源代码。
- en: '![](../Images/CH08_F04_Thampi.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F04_Thampi.png)'
- en: Figure 8.4 Salary, gender, and race distributions
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.4 薪资、性别和种族分布
- en: Let’s now look at the distribution of salary for various protected gender and
    race groups to determine whether there is any bias there. This is shown in figure
    8.5\. If we look at gender, we can see that a higher proportion of male adults
    than female adults earn more than $50K. We can also make the same observation
    for race, where a higher proportion of white adults than Black adults earns more
    than $50K.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们来看看不同受保护性别和种族群体的薪资分布，以确定是否存在任何偏见。这如图8.5所示。如果我们看性别，我们可以看到，比女性成年人，男性成年人的比例更高，他们的收入超过50K美元。对于种族，我们也可以做出相同的观察，其中白人成年人的比例高于黑人成年人，他们的收入超过50K美元。
- en: '![](../Images/CH08_F05_Thampi.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F05_Thampi.png)'
- en: Figure 8.5 Distributions of salary vs. gender and salary vs. race
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.5 薪资与性别、薪资与种族的分布
- en: Finally, let’s look at the representation of gender for the two races in this
    dataset, shown in figure 8.6\. We can see that among Black adults, the split between
    male and female is pretty even, at around 50%. For white adults, on the other
    hand, more white male adults are represented than white females. This analysis
    is useful to determine the main cause of the bias in terms of salary. Because
    70% of the white adults are male, the bias in salary for white adults may be better
    explained by the gender protected group where male users may be earning more than
    female users in the dataset. For Black adults, on the other hand, because the
    split between male and female is pretty even, the main source of bias for Black
    adults may be the race itself.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们来看看这个数据集中两种种族的性别代表性，如图8.6所示。我们可以看到，在黑人成年人中，男性和女性的比例相当，大约为50%。另一方面，对于白人成年人来说，白人男性成年人的代表性比白人女性更高。这种分析有助于确定薪资偏见的主要原因。因为70%的白人成年人是男性，所以白人成年人的薪资偏见可能更好地解释为性别保护群体，其中数据集中的男性用户可能比女性用户赚得更多。另一方面，对于黑人成年人来说，因为男性和女性的比例相当，所以黑人成年人的主要偏见来源可能是种族本身。
- en: '![](../Images/CH08_F06_Thampi.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F06_Thampi.png)'
- en: Figure 8.6 Gender vs. race distribution
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 性别与种族分布
- en: 'It is important to understand the root cause of these biases in the dataset
    before we proceed to build the model. We are not sure how the dataset was collected
    and, hence, cannot be certain of the root cause. We can, however, hypothesize
    that the sources of bias could be the following:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续构建模型之前，了解数据集中这些偏差的根本原因非常重要。我们不确定数据集是如何收集的，因此无法确定根本原因。然而，我们可以假设偏见来源可能是以下几种：
- en: Sampling bias, where the dataset does not properly represent the true population.
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 样本偏差，其中数据集未能正确代表真实人口。
- en: Labeling bias, where biases may exist in the way that salary information is
    recorded for various groups in the population.
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签偏见，其中可能在记录人口中各个群体的薪资信息的方式中存在偏见。
- en: Systemic bias in society. If there is systemic bias, then that bias will be
    reflected in the dataset.
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 社会系统性偏见。如果存在系统性偏见，那么这种偏见将在数据集中得到反映。
- en: As we already discussed in chapter 3, the first problem can be solved by collecting
    more data that is representative of the population. In this chapter, we will also
    learn about properly documenting the data-collection process using datasheets
    for improved transparency and accountability. These datasheets can also be used
    to determine the root cause of biases in the dataset. Labeling bias can be fixed
    by improving the data-collection process. We will also learn about another technique
    to correct for label bias in this chapter. The last problem is much harder to
    solve, requiring better policies and laws, and this is beyond the scope of this
    book.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们已在第3章中讨论过的，第一个问题可以通过收集更多代表人群的数据来解决。在本章中，我们还将学习如何使用数据表来正确记录数据收集过程，以提高透明度和问责制。这些数据表还可以用来确定数据集中偏差的根本原因。通过改进数据收集过程可以修复标签偏差。在本章中，我们还将学习另一种纠正标签偏差的技术。最后一个问题更难解决，需要更好的政策和法律，这超出了本书的范围。
- en: 8.1.2 Prediction model
  id: totrans-54
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 预测模型
- en: From our exploratory analysis, we found some biases in our dataset, the root
    causes of which are unfortunately unknown. In the interest of measuring model
    *f*airness, we will now build a model for predicting the income of adults. We
    will use a random forest model for this purpose. As you learned in chapter 3,
    a random forest is a way of combining decision trees, specifically using the bagging
    technique. An illustration of this model is shown in figure 8.7\. The training
    data is fed in tabular or matrix form into the random forest model. Note that
    the categorical features are encoded into numerical values. Using random forest,
    we can train multiple decision trees in parallel on separate random subsets of
    the training data. Predictions are made using these individual decision trees,
    and all of them are combined to come up with the final prediction. Majority voting
    is typically used as a way of combining the individual decision tree predictions
    into a final one.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的探索性分析中，我们发现数据集中存在一些偏差，不幸的是，其根本原因尚不清楚。为了衡量模型的*公平性*，我们现在将构建一个预测成人收入的模型。我们将为此目的使用随机森林模型。正如你在第3章中学到的，随机森林是一种结合决策树的方法，具体使用的是袋装技术。这个模型的示意图如图8.7所示。训练数据以表格或矩阵形式输入到随机森林模型中。请注意，分类特征被编码为数值。使用随机森林，我们可以在训练数据的独立随机子集上并行训练多个决策树。预测是通过这些单个决策树来进行的，并将它们全部结合起来得出最终预测。通常使用多数投票作为将单个决策树的预测组合成最终预测的方法。
- en: '![](../Images/CH08_F07_Thampi.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F07_Thampi.png)'
- en: Figure 8.7 An illustration of a random forest model for income prediction
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 成人收入预测的随机森林模型示意图
- en: As an exercise, write the code to train a random forest model on the adult income
    dataset. You can use the code examples from chapter 3 as a reference. Note that
    you can use the `LabelEncoder` class provided by Scikit-Learn to encode the categorical
    features into numerical values. Also, you can try the `RandomForestClassifier`
    class provided by Scikit-Learn to initialize and train the model. You can find
    the solution to this exercise in the GitHub repository associated with this book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，编写代码以在成人收入数据集上训练随机森林模型。你可以参考第3章中的代码示例。请注意，你可以使用Scikit-Learn提供的`LabelEncoder`类将分类特征编码为数值。此外，你也可以尝试使用Scikit-Learn提供的`RandomForestClassifier`类来初始化和训练模型。你可以在与本书相关的GitHub仓库中找到这个练习的解决方案。
- en: For the remainder of this chapter, we will use a random forest model trained
    using 10 estimators or decision trees with a maximum depth of 20 for each decision
    tree. The performance of this model is summarized in table 8.2\. We will consider
    four metrics for model evaluation, namely, accuracy, precision, recall, and F1\.
    These metrics were introduced in chapter 3, and we have repeatedly used them in
    previous chapters. We will also consider a baseline model that always predicts
    the majority class to be 0, that is, the income of the adult is always less than
    or equal to $50K. The performance of the random forest model is compared with
    this baseline. We can see that the random forest model outperforms the baseline
    on multiple metrics, achieving an accuracy of about 86% (+10% compared to the
    baseline), a precision of about 85% (+27% compared to the baseline), a recall
    of about 86% (+10% compared to the baseline), and an F1 of about 85% (+19% compared
    to the baseline).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章剩余部分，我们将使用一个使用10个估计器或决策树训练的随机森林模型，每个决策树的最大深度为20。该模型的表现总结在表8.2中。我们将考虑四个模型评估指标，即准确率、精确率、召回率和F1值。这些指标在第3章中已介绍，并在前几章中反复使用。我们还将考虑一个基线模型，该模型总是预测多数类为0，即成人的收入总是小于或等于$50K。我们将随机森林模型的表现与这个基线进行比较。我们可以看到，在多个指标上，随机森林模型优于基线，实现了约86%的准确率（比基线高10%），约85%的精确率（比基线高27%），约86%的召回率（比基线高10%），以及约85%的F1值（比基线高19%）。
- en: Table 8.2 Performance of the income-prediction random forest model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.2 收入预测随机森林模型的表现
- en: '|  | Accuracy (%) | Precision (%) | Recall (%) | F1 (%) |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '|  | 准确率 (%) | 精确率 (%) | 召回率 (%) | F1 (%) |'
- en: '| Baseline | 76.1 | 57.9 | 76.1 | 65.8 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| 基准 | 76.1 | 57.9 | 76.1 | 65.8 |'
- en: '| Random forest | 85.8 | 85.3 | 85.8 | 85.4 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| 随机森林 | 85.8 | 85.3 | 85.8 | 85.4 |'
- en: Let’s now interpret the random forest model in a couple of ways. First, let’s
    look at the importance of the input features as deemed by the random forest model.
    This will help us understand the importance of some of the protected group features,
    as shown in figure 8.8\. You can review the source code used to generate the plot
    in the GitHub repository associated with this book. We can see that age (a protected
    group) is the most important feature, followed by capital gains. Race and gender,
    however, seem to have low importance. It could be that race and gender are encoded
    in some of the other features. We can check this by looking at the correlations
    between the features. We can also understand how race and gender may interact
    with some of the other features using partial dependence plots (PDPs), as we saw
    in chapter 3.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在以几种方式解释随机森林模型。首先，让我们看看随机森林模型认为的输入特征的重要性。这将帮助我们理解一些受保护群体特征的重要性，如图8.8所示。您可以在与本书相关的GitHub存储库中查看用于生成此图的源代码。我们可以看到，年龄（一个受保护群体）是最重要的特征，其次是资本收益。然而，种族和性别似乎重要性较低。这可能是因为种族和性别被编码在其他一些特征中。我们可以通过查看特征之间的相关性来检查这一点。我们还可以使用局部相关图（PDPs），正如我们在第3章中看到的，来理解种族和性别可能如何与其他一些特征相互作用。
- en: '![](../Images/CH08_F08_Thampi.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F08_Thampi.png)'
- en: Figure 8.8 The importance of features learned by the random forest model
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 随机森林模型学习到的特征的重要性
- en: Next, we can use the SHAP technique to determine how the model makes a single
    prediction. As we learned in chapter 4, SHAP is a model-agnostic local interpretability
    technique that uses game-theoretic concepts to quantify the impact of features
    on a single model prediction. Figure 8.9 shows the SHAP explanation for an adult
    who earns more than $50K per year. Note that this data point was not used for
    training. We can see how each feature value pushes the model prediction from the
    base value to a score of 0.73 (i.e., 73% likelihood that the adult earns more
    than $50K). The most important feature values for this instance are capital gains,
    level of education, and hours worked per week in descending order.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以使用SHAP技术来确定模型如何做出单个预测。正如我们在第4章中学到的，SHAP是一种模型无关的局部可解释技术，它使用博弈论概念来量化特征对单个模型预测的影响。图8.9显示了年薪超过$50K的成年人的SHAP解释。请注意，这个数据点并未用于训练。我们可以看到每个特征值如何将模型预测从基值推动到0.73分（即成年人有73%的可能性年薪超过$50K）。对于这个实例，最重要的特征值按降序排列为资本收益、教育水平和每周工作时间。
- en: '![](../Images/CH08_F09_Thampi.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F09_Thampi.png)'
- en: Figure 8.9 SHAP explanation for a single prediction where salary is greater
    than $50K
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 对于年薪超过$50K的单个预测的SHAP解释
- en: We will revisit SHAP and dependency plots in the context of fairness again in
    section 8.3\. We will also discuss how to use other interpretability techniques
    that we have learned in this book, like network dissection and t-SNE. But before
    that, let’s learn about various notions of fairness.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在第8.3节中再次讨论SHAP和依赖图在公平性背景下的应用。我们还将讨论如何使用本书中学到的其他可解释性技术，如网络分解和t-SNE。但在那之前，让我们了解各种公平性的概念。
- en: 8.2 Fairness notions
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 公平性概念
- en: 'In the previous section, we trained a random forest model to make salary predictions.
    The objective of the model was to determine for each adult a binary outcome: whether
    or not they earn more than $50K. But were these predictions fair for various protected
    groups like gender and race? To formalize the definitions of various notions of
    fairness, let’s look at a simple illustration of the predictions made by the model
    and the relevant measurements required for fairness. Figure 8.10 depicts an illustration
    of the predictions made by the model projected on a two-dimensional plane. The
    random forest model splits the 2-D plane into two halves that separate the positive
    predictions (on the right half) from the negative predictions (on the left half).
    The actual labels of 20 adults have been projected onto this 2-D plane as well.
    Note that the position of the actual labels on the 2-D plane is irrelevant. What
    matters is whether the labels fall on the left half (where the model predicts
    negative, i.e., 0) or on the right half (where the model predicts positive, i.e.,
    1). The actual positive labels are shown as circles and the actual negative labels
    are shown as triangles.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们训练了一个随机森林模型来进行薪资预测。该模型的目标是为每个成年人确定一个二元结果：他们是否每年收入超过50K美元。但这些预测对于性别和种族等不同受保护群体是否公平呢？为了正式定义各种公平性的概念，让我们看看模型做出的预测以及为公平性所需的相关测量。图8.10展示了模型在二维平面上的预测示意图。随机森林模型将二维平面分为两部分，将正预测（在右侧）与负预测（在左侧）分开。20位成年人的实际标签也投影到了这个二维平面上。请注意，实际标签在二维平面上的位置无关紧要。重要的是标签是否落在左侧（模型预测为负，即0）或右侧（模型预测为正，即1）。实际正标签以圆圈表示，实际负标签以三角形表示。
- en: '![](../Images/CH08_F10_Thampi.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F10_Thampi.png)'
- en: Figure 8.10 An illustration of model predictions and measurements relevant for
    fairness notions
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.10 模型预测和与公平性概念相关的测量示意图
- en: 'Based on the illustration in figure 8.10, we can now define the following basic
    measurements:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 根据图8.10中的示意图，我们现在可以定义以下基本测量：
- en: '*Actual positive labels*—Data points for which the ground truth label in the
    dataset is positive. In figure 8.10, adults who earn more than $50K per year in
    the dataset are shown as circles. If we count the circles, the number of actual
    positive labels is equal to 12.'
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实际正标签*—数据集中真实标签为正的数据点。在图8.10中，数据集中每年收入超过50K美元的成年人被表示为圆圈。如果我们计算圆圈的数量，实际正标签的数量等于12。'
- en: '*Actual negative labels*—Data points for which the ground truth label in the
    dataset is negative. In figure 8.10, adults who earn less than or equal to $50K
    per year in the dataset are shown as triangles. The number of actual negative
    labels is, therefore, 8.'
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*实际负标签*—数据集中真实标签为负的数据点。在图8.10中，数据集中每年收入低于或等于50K美元的成年人被表示为三角形。因此，实际负标签的数量为8。'
- en: '*Predicted positive*—Data points for which the model predicts a positive outcome.
    In figure 8.10, data points that fall in the right half of the 2-D plane have
    a positive prediction. There are 10 data points that fall in that region. Hence,
    predicted positive measurement is 10.'
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测正标签*—模型预测为正结果的数据点。在图8.10中，落在二维平面右侧的数据点具有正预测。该区域有10个数据点。因此，预测正标签的测量值为10。'
- en: '*Predicted negative*—Data points for which the model predicts a negative outcome.
    In figure 8.10, these are points that fall in the left half of the 2-D plane.
    The predicted negative measurement is also 10.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测负标签*—模型预测为负结果的数据点。在图8.10中，这些是落在二维平面左侧的点。预测负标签的测量值也是10。'
- en: '*True positive*—In figure 8.10, the true positives are the circles that fall
    in the right half of the 2-D plane. They are essentially data points for which
    the model predicts positive, and the actual label is also positive. There are
    eight such circles, and, therefore, the number of true positives is 8\. We can
    also obtain this from the confusion matrix as well, where true positives are the
    cases where the model predicts 1 and the actual label is 1.'
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阳性*—在图8.10中，真阳性是落在2维平面右半部分的圆圈。它们实际上是模型预测为正，并且实际标签也是正的数据点。共有八个这样的圆圈，因此真阳性的数量是8。我们也可以从混淆矩阵中获取这个信息，其中真阳性是模型预测为1且实际标签也是1的情况。'
- en: '*True negative*—The true negatives, on the other hand, are the triangles that
    fall in the left half of the 2-D plane. They are data points for which the model
    predicts negative, and the actual label is also negative. In figure 8.10, we can
    see that the number of true negatives is 6\. From the confusion matrix, these
    are cases where the model predicts 0 and the actual label is 0.'
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*真阴性*—另一方面，真阴性是落在2维平面左半部分的三角形。它们是模型预测为负，并且实际标签也是负的数据点。在图8.10中，我们可以看到真阴性的数量是6。从混淆矩阵中，这些是模型预测为0且实际标签也是0的情况。'
- en: '*False positive*—The false positives are triangles that fall in the right half
    of the plane in figure 8.10\. They are data points for which the model predicts
    positive, but the actual label is negative. From the figure, the number of false
    positives is 2\. From the confusion matrix, these are cases where the model predicts
    1, but the actual label is 0.'
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阳性*—在图8.10的平面右半部分，假阳性是三角形。它们是模型预测为正，但实际标签为负的数据点。从图中可以看出，假阳性的数量是2。从混淆矩阵中，这些是模型预测为1但实际标签为0的情况。'
- en: '*False negative*—The false negatives are circles that fall in the left half
    of the 2-D plane. They are essentially data points for which the model predicts
    negative, but the actual label is positive. Since there are four circles in the
    left half of figure 8.10, the number of false negatives is 4\. From the confusion
    matrix, these are cases where the model predicts 0, but the actual label is 1.'
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*假阴性*—假阴性是落在2维平面左半部分的圆圈。它们实际上是模型预测为负，但实际标签为正的数据点。由于图8.10的左半部分有四个圆圈，因此假阴性的数量是4。从混淆矩阵中，这些是模型预测为0但实际标签为1的情况。'
- en: With these basic measurements in place, let’s now define various notions of
    fairness.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些基本测量到位之后，我们现在定义各种公平性的概念。
- en: 8.2.1 Demographic parity
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 人口统计学平等
- en: The first notion of fairness that we will consider is called *demographic parity*.
    The demographic parity notion is sometimes also called *independence*, *statistical
    parity*, and, legally, *disparate impact*. It asserts that for the model, parity
    exists in the positive prediction rates for different protected groups. Let’s
    look at the example illustrated in figure 8.11\. In the figure, the 20 adults—as
    we saw in figure 8.10—have been separated into two groups, A and B, one for each
    of the protected gender groups. Group A consists of male adults and has 10 data
    points in its 2-D plane. Group B consists of female adults with 10 data points
    in its 2-D plane.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要考虑的第一个公平性概念被称为*人口统计学平等*。人口统计学平等的概念有时也被称为*独立性*、*统计平等*，在法律上称为*不同影响*。它主张对于模型来说，不同受保护群体的正预测率是平等的。让我们看看图8.11中的示例。在图中，20个成年人——正如我们在图8.10中看到的——被分为两组，A和B，每组对应一个受保护的性别群体。A组由男性成年人组成，在2维平面上有10个数据点。B组由女性成年人组成，在2维平面上也有10个数据点。
- en: '![](../Images/CH08_F11_Thampi.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F11_Thampi.png)'
- en: Figure 8.11 An illustration of demographic parity for two protected gender groups
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.11 两个受保护性别群体的人口统计学平等示意图
- en: Based on the illustration in figure 8.11, we can now calculate the basic measurements
    described earlier. For male adults, there are six actual positives, four actual
    negatives, five predicted positives, and five predicted negatives. For female
    adults, we can see that the actual positives/negatives and predicted positive/negatives
    are the same as for male adults. The positive rate for both male and female adults
    is the proportion of adults in each group for which the model predicts positive.
    We can see from figure 8.11 that the positive rates for both male and female users
    are the same—equal to 50%. We can, therefore, assert that there is demographic
    parity between the two groups.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 基于图8.11的插图，我们现在可以计算之前描述的基本测量值。对于男性成年人，有六个实际阳性，四个实际阴性，五个预测阳性，和五个预测阴性。对于女性成年人，我们可以看到实际阳性/阴性以及预测阳性/阴性与男性成年人相同。男性和女性成年人的阳性率是模型预测为阳性的每个群体中成年人的比例。从图8.11中我们可以看到，男性和女性用户的阳性率相同——都是50%。因此，我们可以断言，这两组之间存在人口统计学平等。
- en: Let’s look at this from a practical standpoint. Assume that the model predictions
    are used to allocate a scarce resource, say, a housing loan. Let’s also assume
    that adults who earn more than $50K are more likely to be able to afford a house
    and pay off the loan. If a decision like a housing loan application is made based
    on the model prediction, where loans are granted to adults who earn more than
    $50K, then demographic parity will ensure that the loans are granted at an equal
    rate for both male and female adults. Demographic parity asserts that the model
    predicts that the salary of male and female adults is greater than $50K with equal
    likelihood.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从实际的角度来看这个问题。假设模型预测被用来分配一种稀缺资源，比如说，住房贷款。再假设收入超过5万美元的成年人更有可能负担得起房子并偿还贷款。如果像住房贷款申请这样的决策是基于模型预测来做的，那么贷款将只发放给收入超过5万美元的成年人，那么人口统计学平等将确保贷款以相同的比率发放给男性和女性成年人。人口统计学平等断言，模型以相同的可能性预测男性和女性成年人的工资都超过5万美元。
- en: 'Let’s now define demographic parity more formally and use this definition to
    check whether our random forest model is fair using this notion. Let’s represent
    the model predictions as *ŷ* and the protected group variable as *z*. The gender
    protected group can have two possible values for the variable *z*:: 0 for female
    adults and 1 for male adults. For the race protected group as well, the variable
    *z* can have two possible values: 0 for Black adults and 1 for white adults. Demographic
    parity requires that the probability that the model predicts positive for one
    protected group is similar or equal to the probability that the model predicts
    positive for the other protected group. The probability measures are similar if
    their ratio is between thresholds *τ*[1] and *τ*[2] where the thresholds are typically
    0.8 and 1.2, respectively. The thresholds are 0.8 and 1.2, to closely follow the
    80% rule in legal literature for disparate impact, as shown by the next equation.
    The probability measures are equal if the ratio is 1:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们更正式地定义人口统计学平等，并使用这个定义来检查我们的随机森林模型是否公平。让我们用*ŷ*表示模型预测，用*z*表示受保护群体变量。性别受保护群体变量*z*可以有两个可能的值：0代表女性成年人，1代表男性成年人。对于种族受保护群体，变量*z*也可以有两个可能的值：0代表黑人成年人，1代表白人成年人。人口统计学平等要求模型预测一个受保护群体为阳性的概率与模型预测另一个受保护群体为阳性的概率相似或相等。如果它们的比率在阈值*τ*[1]和*τ*[2]之间，则概率度量是相似的，这些阈值通常是0.8和1.2。阈值设置为0.8和1.2是为了紧密遵循法律文献中关于不同影响的80%规则，如下一个方程所示。如果比率是1，则概率度量是相等的：
- en: '![](../Images/CH08_F11_Thampi_equation01.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图8.11的插图](../Images/CH08_F11_Thampi_equation01.png)'
- en: 'Now, how would we use this definition for a protected group feature that is
    categorical but with more than two values? In this example, we considered only
    two races: white and Black. What if there were more races in the dataset? Note
    that individuals could be multiracial, where they identify with multiple races.
    We will treat them as a separate race to ensure there is no discrimination toward
    individuals who identify as multiple races. In such a scenario with more than
    two races, we would define the demographic parity ratio metric for each race and
    take a one-vs.-all strategy, where *z* = *0* represents the race of interest and
    *z* = *1* represents all the other races. Note that individuals who are multiracial
    could belong to multiple groups. We will then need to ensure that the demographic
    parity ratio is similar for every race when compared to all other races. How about
    for a protected group feature that is continuous, like age? In this case, we would
    need to split the continuous feature into discrete groups and then apply the one-vs.-all
    strategy.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何使用这个定义来处理一个分类但具有多个值的受保护群体特征？在这个例子中，我们只考虑了两种种族：白色和黑色。如果数据集中有更多种族怎么办？请注意，个人可能是多种族的，他们可能认同多个种族。我们将把它们视为一个单独的种族，以确保不会对认同多个种族的个人产生歧视。在这种情况下，如果有超过两个种族，我们将为每个种族定义人口比例率指标，并采用一对一的策略，其中*z*
    = *0*代表感兴趣的种族，*z* = *1*代表所有其他种族。请注意，多种族的个人可能属于多个群体。然后我们需要确保与所有其他种族相比，每个种族的人口比例率相似。对于一个连续的受保护群体特征，比如年龄呢？在这种情况下，我们需要将连续特征划分为离散组，然后应用一对一策略。
- en: 'With the definition now in place, let’s see if the random forest model is fair.
    The following code snippet evaluates the model using the demographic parity notion:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在定义已经就绪，让我们看看随机森林模型是否公平。以下代码片段使用人口比例概念评估模型：
- en: '[PRE0]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Loads the indices for male adults in the test set where the encoded gender
    = 1
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ① 加载测试集中编码性别为1的男性成年人的索引
- en: ② Loads the indices for female adults in the test set where the encoded gender
    = 0
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ② 加载测试集中编码性别为0的女性成年人的索引
- en: ③ Loads the indices for white adults in the test set where the encoded race
    = 1
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载测试集中编码种族为1的白色成年人的索引
- en: ④ Loads the indices for Black adults in the test set where the encoded race
    = 0
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 加载测试集中编码种族为0的黑色成年人的索引
- en: ⑤ Gets the model predictions for all the adults in the test set
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 获取测试集中所有成年人的模型预测
- en: ⑥ Obtains the model predictions for the two gender groups
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 获取两个性别群体的模型预测
- en: ⑦ Obtains the model predictions for the two race groups
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 获取两个种族群体的模型预测
- en: ⑧ Computes the demographic parity ratio for the two gender groups
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 计算两个性别群体的人口比例率
- en: ⑨ Computes the demographic parity ratio for the two race groups
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 计算两个种族群体的人口比例率
- en: 'Note that in this code snippet, we are using the label-encoded dataset and
    the model trained in section 8.1.2\. The label-encoded input features are stored
    in the `X_test` data frame, and the random forest model is named `adult_model`.
    Note that you can obtain the code used for data preparation and model training
    from the GitHub repository associated with this book. We are computing the demographic
    parity ratio as the ratio of average probability scores for predicting the positive
    class of one of the groups (female/Black adults) to its counterpart groups (male/white
    adults). Once we have computed the demographic parity ratios for the gender and
    race groups, we can plot the metric using the following code snippet:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在这个代码片段中，我们使用的是标签编码的数据集和第8.1.2节中训练的模型。标签编码的输入特征存储在`X_test`数据框中，随机森林模型命名为`adult_model`。请注意，您可以从与本书相关的GitHub存储库中获取数据准备和模型训练的代码。我们计算人口比例率作为预测一个群体（女性/黑色成年人）的阳性类的平均概率分数与对应群体（男性/白色成年人）的比率。一旦我们计算了性别和种族群体的人口比例率，我们可以使用以下代码片段绘制该指标：
- en: '[PRE1]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Helper function called plot_bar used to plot the bar chart
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ① 调用辅助函数plot_bar来绘制条形图
- en: ② Sets the thresholds for the demographic parity ratio
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ② 设置人口比例率阈值
- en: ③ Initializes a Matplotlib plot
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化一个Matplotlib图表
- en: ④ Plots the demographic parity ratios for gender and race as bar charts
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 绘制性别和种族的人口比例率条形图
- en: ⑤ Sets the label for the y-axis
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 设置y轴的标签
- en: ⑥ Limits the y-axis to values between –0.5 and 1.5
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将y轴的值限制在-0.5到1.5之间
- en: ⑦ Plots threshold_1 as a horizontal line
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 绘制阈值为threshold_1的水平线
- en: ⑧ Plots threshold_2 as a horizontal line
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 绘制阈值为threshold_2的水平线
- en: ⑨ Displays the legend for the plot
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 显示图表的图例
- en: The resulting plot is shown in figure 8.12\. We can see that the demographic
    parity ratios are 0.38 and 0.45, respectively, for gender and race. They are not
    within the threshold and, therefore, the random forest model is not fair for both
    protected groups using the demographic parity notion. We will see how to mitigate
    bias and train a fair model in section 8.4.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在图8.12中。我们可以看到，性别和种族的人口比例比分别为0.38和0.45。它们不在阈值范围内，因此，基于人口比例的随机森林模型对两个受保护群体来说都是不公平的。我们将在第8.4节中看到如何减轻偏差并训练一个公平的模型。
- en: '![](../Images/CH08_F12_Thampi.png)'
  id: totrans-117
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F12_Thampi.png)'
- en: Figure 8.12 Demographic parity ratios for gender and race
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 性别和种族的人口比例比
- en: 8.2.2 Equality of opportunity and odds
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 机会平等和概率
- en: The demographic parity notion is useful for scenarios where we want to ensure
    parity in the treatment of all protected groups, irrespective of their prevalence
    in the population. It ensures that the minority group is treated the same way
    as the majority group. In some scenarios, we might want to consider the distribution
    of the actual label for all protected groups. For example, if we are interested
    in employment opportunities, one group of individuals may be more interested in
    and qualified for certain jobs than other groups. We may not want to ensure parity
    in such a scenario because we may want to ensure that job opportunities are given
    to the right group of individuals who are more interested in and qualified for
    it. We can use the equality of opportunity and odds fairness notion in such a
    scenario.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 人口比例平等的概念对于我们需要确保所有受保护群体在人口中的处理平等的场景是有用的。它确保少数群体与多数群体以相同的方式被对待。在某些场景中，我们可能想要考虑所有受保护群体的实际标签的分布。例如，如果我们对就业机会感兴趣，一组个人可能比其他群体更感兴趣并且更有资格从事某些工作。在这种情况下，我们可能不希望确保平等，因为我们可能希望确保将就业机会给予那些对它更感兴趣并且更有资格的个人群体。在这种情况下，我们可以使用机会平等和概率公平的概念。
- en: Let’s go back to the illustration used for demographic parity to build up our
    intuition. In figure 8.13, the separation of the 20 adults in groups A (male)
    and B (female) is the same as what we saw for demographic parity in figure 8.11\.
    For equality of opportunity and odds, we are interested in measurements that take
    into account the distribution of the actual label for each of the protected groups.
    These measurements are computed in figure 8.13 as the true positive rate and the
    false positive rate. The true positive rate measures the probability that an actual
    positive is predicted positive and is computed as the ratio of the number of the
    true positives to the sum of the number of true positives and false negatives.
    In other words, the true positive rate measures the percentage of actual true
    cases that the model got right, also called recall. For group A (male), the true
    positive rate is about 66.7%, and for group B (female), the true positive rate
    is 50%. We say there is equality of opportunity when parity exists in the true
    positive rates between the groups. Because the true positive rates do not match
    in the toy example illustrated in figure 8.13, we can say that we have not achieved
    equality of opportunity for the gender protected group.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到用于人口比例的插图，以建立我们的直觉。在图8.13中，A组（男性）和B组（女性）的20名成年人的分离与我们在图8.11中看到的人口比例相同。对于机会平等和概率，我们感兴趣的测量是考虑到每个受保护群体的实际标签的分布。这些测量在图8.13中作为真正例率和假正例率来计算。真正例率衡量的是实际正例被预测为正例的概率，其计算为真正例数与真正例数和假负例数之和的比率。换句话说，真正例率衡量的是模型正确识别的实际真例的百分比，也称为召回率。对于A组（男性），真正例率约为66.7%，而对于B组（女性），真正例率为50%。当组间真正例率存在平等时，我们说存在机会平等。因为图8.13中展示的玩具示例中真正例率不匹配，我们可以说不存在性别受保护群体的机会平等。
- en: '![](../Images/CH08_F13_Thampi.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F13_Thampi.png)'
- en: Figure 8.13 An illustration of equality of opportunity and odds for two protected
    gender groups
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.13 两个受保护性别群体机会平等和概率的示意图
- en: Equality of odds extends the definition of equality of opportunity to another
    symmetric measurement called false positive rate. False positive rate measures
    the probability that an actual negative event is predicted as positive. It is
    computed as the ratio of the number of false positives to the sum of the number
    of false positives and true negatives. We can assert that equality of odds exists
    when there is parity in the true positive rates and false positive rates between
    the protected groups. In the toy example illustrated in figure 8.13, there is
    no parity in the true positive rates between groups A and B, so we cannot say
    equality of odds exists. Moreover, the false positive rates also do not match
    between the two groups.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 概率均等扩展了机会均等定义，引入了另一个对称的测量指标，称为假阳性率。假阳性率衡量的是实际负事件被预测为正事件的可能性。它被计算为假阳性数量与假阳性数量和真阴性数量之和的比率。我们可以断言，当受保护群体之间在真阳性率和假阳性率上存在均衡时，概率均等存在。在图8.13中展示的玩具示例中，A组和B组之间的真阳性率不存在均衡，因此我们无法说概率均等存在。此外，两组之间的假阳性率也不匹配。
- en: 'We can define equality of opportunity and odds more formally using the equations
    that follow. The first equation essentially computes the difference in the true
    positive rates between the two groups. The second equation computes the difference
    in the false positive rates between the two groups. Parity is present when the
    difference is equal to or close to 0\. This notion is different from the demographic
    parity notion in that it considers the distribution of the actual label—positive
    for the true positive rate and negative for the false positive rate. In addition,
    the demographic parity notion compares the probabilities as a ratio rather than
    additively, closely following the “80% rule” in the legal literature:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下方程更正式地定义机会均等和概率均等。第一个方程本质上计算了两组之间真阳性率的差异。第二个方程计算了两组之间假阳性率的差异。当差异等于或接近0时，存在均衡。这一概念与人口均衡概念不同，因为它考虑了实际标签的分布——对于真阳性率为正，对于假阳性率为负。此外，人口均衡概念比较概率时作为比率而不是加法，紧密遵循法律文献中的“80%规则”：
- en: ℙ(*ŷ* = 1|*z* = 0|*y* = 1) – ℙ(*ŷ* = 1|*z* = 1|*y* = 1)
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ℙ(*ŷ* = 1|*z* = 0|*y* = 1) – ℙ(*ŷ* = 1|*z* = 1|*y* = 1)
- en: ℙ(*ŷ* = 1|*z* = 0|*y* = 0) – ℙ(*ŷ* = 1|*z* = 1|*y* = 0)
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ℙ(*ŷ* = 1|*z* = 0|*y* = 0) – ℙ(*ŷ* = 1|*z* = 1|*y* = 0)
- en: 'Now let’s see if our random forest model is fair using this notion. We can
    compare the true positive rate and false positive rate using the receiver operator
    characteristic (ROC) curve. The ROC curve essentially plots the true positive
    rate against the false positive rate. For equality of opportunity and odds, we
    can then use the area under the curve (AUC) as an aggregate measure of performance
    to easily compare the performance of the model for each of the protected groups.
    We can look at the difference in the AUC between the groups to see how fair the
    model is. The next code snippet shows how to compute the true/false positive rates
    and the AUC:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用这个概念来看看我们的随机森林模型是否公平。我们可以使用接收者操作特征（ROC）曲线来比较真阳性率和假阳性率。ROC曲线本质上是在真阳性率与假阳性率之间绘制曲线。对于机会均等和概率均等，我们可以使用曲线下面积（AUC）作为性能的汇总度量，以便轻松比较每个受保护群体的模型性能。我们可以查看组间AUC的差异，以了解模型的公平性。下面的代码片段展示了如何计算真/假阳性率和AUC：
- en: '[PRE2]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Imports the roc_curve and auc helper functions from Scikit-Learn
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从Scikit-Learn导入roc_curve和auc辅助函数。
- en: ② Defines a helper function to compute the ROC and AUC for each of the protected
    groups
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ② 定义一个辅助函数来计算每个受保护群体的ROC和AUC。
- en: ③ Defines dictionaries for the true/false positive rates and AUC to store the
    metrics for each of the classes in the dataset
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 定义用于存储数据集中每个类别的真/假阳性率和AUC的字典。
- en: ④ For the actual label, computes the true/false positive rates and AUC and stores
    them in dictionaries
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 对于实际标签，计算真/假阳性率和AUC，并将它们存储在字典中。
- en: ⑤ Returns the dictionaries to the caller of the function
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将字典返回给函数的调用者。
- en: ⑥ Uses the helper function to compute the metrics for male adults
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 使用辅助函数计算成年男性的指标。
- en: ⑦ Uses the helper function to compute the metrics for female adults
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 使用辅助函数计算成年女性的指标。
- en: ⑧ Uses the helper function to compute the metrics for white adults
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 使用辅助函数计算成年白人的指标。
- en: ⑨ Uses the helper function to compute the metrics for Black adults
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 使用辅助函数计算成年黑人的指标。
- en: 'Once the metrics have been computed for each of the protected groups, we can
    use the following code snippet to plot the ROC curve:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为每个受保护群体计算了指标，我们可以使用以下代码片段来绘制ROC曲线：
- en: '[PRE3]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Sets the line width for the line chart
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ① 设置线图的线宽
- en: ② Initializes the Matplotlib plot consisting of one row and two columns
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化一个由一排两列组成的Matplotlib图表
- en: ③ In the first column, plots the ROC curve for male adults
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 在第一列中，绘制了男性成人的ROC曲线
- en: ④ In the first column, plots the ROC curve for female adults
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在第一列中，绘制了女性成人的ROC曲线
- en: ⑤ In the second column, plots the ROC curve for white adults
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 在第二列中，绘制了白人成人的ROC曲线
- en: ⑥ In the second column, plots the ROC curve for Black adults
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 在第二列中，绘制了黑人成人的ROC曲线
- en: ⑦ Annotates and labels the plot
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 注释并标记了图表
- en: 'The resulting plot is shown in figure 8.14\. The first column in the plot compares
    the ROC curves for the two gender groups: male and female. The second column compares
    the ROC curves for the two race groups: white and Black. The area under the curve
    is shown in the legend for both plots. We can see that the AUC is 0.89 for male
    adults and 0.92 for female adults. The difference is roughly 3% skewed toward
    female adults. The AUC for white adults, on the other hand, is 0.9 and for Black
    adults, 0.92\. The difference is roughly 2% skewed toward Black adults. Unlike
    demographic parity, unfortunately, no guidelines from the legal or research communities
    exist on what thresholds to use for the AUC difference metric to consider a model
    fair. In this chapter, if the difference is statistically significant, we will
    treat the model as unfair using the equality of opportunity and odds notion. We
    will see if these differences are significant using confidence intervals in section
    8.3.1.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图表显示在图8.14中。图表的第一列比较了两个性别组（男性和女性）的ROC曲线。第二列比较了两个种族组（白人和黑人）的ROC曲线。两个图表的曲线下面积在图例中显示。我们可以看到，男性成人的AUC为0.89，女性成人的AUC为0.92。差异大约有3%偏向女性成人。另一方面，白人成人的AUC为0.9，黑人成人的AUC为0.92。差异大约有2%偏向黑人成人。不幸的是，与人口平衡不同，法律或研究界没有关于考虑模型公平性时应使用哪些阈值的指南。在本章中，如果差异在统计上显著，我们将使用机会平等和赔率的概念将模型视为不公平。我们将在8.3.1节中通过置信区间查看这些差异是否显著。
- en: '![](../Images/CH08_F14_Thampi.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F14_Thampi.png)'
- en: Figure 8.14 Received operator characteristic (ROC) curve for gender and race
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.14 性别和种族的接收器操作特征（ROC）曲线
- en: 8.2.3 Other notions of fairness
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 其他公平性概念
- en: 'The most commonly used notions of fairness are demographic parity and equality
    of opportunity/odds. But, for awareness, let’s also look at the following other
    notions of fairness:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最常用的公平性概念是人口平衡和机会平等/赔率。但是，为了提醒，让我们也看看以下其他公平性概念：
- en: '*Predictive quality parity*—No difference exists in the prediction quality
    between different groups. The prediction quality can be either the accuracy of
    the model or any other performance metric, like F1.'
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预测质量平等*—不同组之间的预测质量没有差异。预测质量可以是模型的准确度或任何其他性能指标，如F1。'
- en: '*Treatment equality*—The model treats the groups equally, whereby parity exists
    in the false prediction rate. The false prediction rate is quantified as the ratio
    of the false negatives to the false positives.'
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*治疗平等*—模型对各组平等对待，其中假阳性率存在平衡。假阳性率被量化为假阴性数与假阳性数的比率。'
- en: '*Fairness through unawareness*—Fairness can be achieved by not explicitly using
    the protected attributes as features for prediction. In an ideal world, the other
    features used by the model are not correlated with the protected attributes, but
    this is almost always not the case. Hence, achieving fairness through unawareness
    is not guaranteed. We will see this in action in section 8.4.1.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过无意识实现的公平性*—可以通过不明确使用受保护属性作为预测特征来实现公平性。在一个理想的世界里，模型使用的其他特征与受保护属性不相关，但这几乎总是不成立。因此，通过无意识实现公平性不能保证。我们将在8.4.1节中看到这一点。'
- en: '*Counterfactual fairness*—A model is fair to an individual if it makes the
    same prediction if that individual belongs to another protected group in a counterfactual
    world.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*反事实公平性*—如果一个模型在反事实世界中，如果该个体属于另一个受保护群体，它仍然做出相同的预测，那么该模型对该个体是公平的。'
- en: We can divide all the notions of fairness into two categories—*group fairness*
    and *individual fairness*. Group fairness ensures that the model is fair for different
    protected groups. For the adult income dataset, protected groups are gender, race,
    and age. Individual fairness, on the other hand, ensures that the model makes
    similar predictions for similar individuals. For the adult income dataset, individuals
    can be similar based on their level of education, country of origin, or hours
    worked per week, to name a few examples. Table 8.3 shows which category the different
    notions of fairness belong to.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将所有公平性概念分为两类——*组公平性*和*个体公平性*。组公平性确保模型对不同受保护群体是公平的。对于成人收入数据集，受保护群体包括性别、种族和年龄。另一方面，个体公平性确保模型对相似个体做出相似的预测。对于成人收入数据集，个体可以根据其教育水平、原籍国或每周工作时间等因素相似，仅举几个例子。表8.3显示了不同公平性概念的所属类别。
- en: Table 8.3 Group and individual fairness notion
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.3 组合和个体公平性概念
- en: '| Fairness notion | Description | Category |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 公平性概念 | 描述 | 类别 |'
- en: '| Demographic parity | Parity in the positive prediction rates for different
    protected groups | Group |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 人口统计平等 | 不同受保护群体在正预测率上的平等 | 组 |'
- en: '| Equality of opportunity and odds | Parity in the true positive rates and
    false positive rates for different protected groups | Group |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 机会和赔率的平等 | 不同受保护群体在真阳性率和假阳性率上的平等 | 组 |'
- en: '| Predictive quality parity | Parity in the prediction quality for different
    protected groups | Group |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 预测质量平等 | 不同受保护群体在预测质量上的平等 | 组 |'
- en: '| Treatment equality | Parity in the false prediction rates for different protected
    groups | Group |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 处理平等 | 不同受保护群体在错误预测率上的平等 | 组 |'
- en: '| Fairness through unawareness | Fairness achieved by not explicitly using
    protected attributes as features for prediction | Individual |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 通过无意识实现公平性 | 通过不明确将受保护属性作为预测特征来实现公平性 | 个体 |'
- en: '| Counterfactual fairness | Similar prediction for an individual if that individual
    belonged to another protected group in a counterfactual world | Individual |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 反事实公平性 | 在反事实世界中，如果该个体属于另一个受保护群体，则对个体的预测相似 | 个体 |'
- en: 8.3 Interpretability and fairness
  id: totrans-166
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 可解释性和公平性
- en: 'In this section, we will learn how to use interpretability techniques to detect
    the source of discrimination due to the model. The source of discrimination can
    be broadly categorized into the following two groups:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将学习如何使用可解释性技术来检测模型导致的歧视来源。歧视的来源可以大致分为以下两组：
- en: '*Discrimination via input features*—Fairness issues that can be traced back
    to the input features.'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过输入特征进行歧视*——可以追溯到输入特征的公平性问题。'
- en: '*Discrimination via representation*—Fairness issues that are hard to trace
    back to the input features, especially for deep learning models that process inputs
    like images and text. For such cases, we could instead trace the source of discrimination
    to deep representations learned by the model.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*通过表示进行歧视*——难以追溯到输入特征的公平性问题，尤其是对于处理图像和文本等输入的深度学习模型。在这种情况下，我们可以将歧视的来源追溯到模型学习到的深度表示。'
- en: 8.3.1 Discrimination via input features
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 通过输入特征进行歧视
- en: Let’s first look at discrimination via input features. When we looked at the
    various fairness notions in section 8.2, we saw by processing the model output
    that the random forest model is not fair using the demographic parity and equality
    of opportunity/odds fairness measures. How can we explain these measures of fairness
    by tracing back to the inputs? We can make use of SHAP for this purpose. As we
    saw in chapter 4 and in section 8.1.2, SHAP decomposes the model output into Shapley
    values for each of the inputs. These Shapley values are of the same unit as the
    model output—if we sum up the Shapley values for all the features, we will get
    a value that matches the model output that measures the probability of predicting
    a positive outcome. We saw an illustration of this in section 8.1.2\. Because
    the Shapley values for the input features sum up to the model output, we can attribute
    differences in the model output (and, in turn, the fairness measures) between
    protected groups back to differences in the Shapley values for each of the inputs.
    This is how you would trace any discrimination or fairness issues back to the
    inputs.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先通过输入特征来观察歧视。当我们查看第8.2节中的各种公平概念时，我们通过处理模型输出看到，随机森林模型在人口统计学差异和机会平等/概率公平的公平性度量上是不公平的。我们如何通过追溯到输入来解释这些公平性度量？我们可以利用SHAP来完成这个目的。正如我们在第4章和第8.1.2节中看到的，SHAP将模型输出分解为每个输入的Shapley值。这些Shapley值与模型输出具有相同的单位——如果我们对所有特征求和Shapley值，我们将得到一个与模型输出相匹配的值，该值衡量预测正结果的概率。我们在第8.1.2节中看到了这个说明。因为输入特征的Shapley值之和等于模型输出，所以我们可以将保护群体之间模型输出（以及，相应地，公平性度量）的差异归因于每个输入的Shapley值的差异。这就是如何将任何歧视或公平性问题追溯到输入的方法。
- en: 'Let’s see this in action using code. The following code snippet defines a helper
    function to generate SHAP differences between protected groups and can be used
    to generate visualizations for differences in the model output traced back to
    the input:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过代码来观察这一过程。以下代码片段定义了一个辅助函数，用于生成保护群体之间的SHAP差异，并可用于生成模型输出中由输入引起的差异的可视化：
- en: '[PRE4]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '① Helper function to generate the SHAP group difference plot that takes six
    inputs. Input 1: DataFrame of feature values'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ① 生成SHAP群体差异图的辅助函数，该函数接受六个输入。输入1：特征值的DataFrame
- en: '② Input 2: Vector of target values'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ② 输入2：目标值的向量
- en: '③ Input 3: SHAP values generated for the input features'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 输入3：为输入特征生成的SHAP值
- en: '④ Input 4: Fairness notion that can be demographic_parity of equality_of_opportunity'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 输入4：公平概念可以是人口统计学差异或机会平等/概率公平
- en: '⑤ Input 5: Protected group that can be either gender or race'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 输入5：可以是性别或种族的保护群体
- en: '⑥ Input 6: Flag to indicate whether to trace the source of discrimination to
    the inputs'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 输入6：标志，指示是否将歧视的来源追溯到输入
- en: ⑦ Returns None for fairness notions and protected groups that are not supported
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 对于不支持公平概念和保护群体，返回None
- en: ⑧ Sets the label for the demographic parity notion
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 设置人口统计学差异概念的标签
- en: ⑨ Sets the label and processes only the actual positives for the equality of
    opportunity notion
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ⑨ 设置标签并仅处理机会平等概念的实际情况
- en: ⑩ Sets the label and mask for the gender protected group
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ⑩ 设置性别保护群体的标签和掩码
- en: ⑪ Sets the label and mask for the race protected group
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: ⑪ 设置种族保护群体的标签和掩码
- en: ⑫ Sets the label for visualization
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: ⑫ 设置可视化标签
- en: ⑬ Restricts the visualization to xmin and xmax
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ⑬ 限制可视化到xmin和xmax
- en: ⑭ Creates visualization when trace_to_input is set to True
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: ⑭ 当trace_to_input设置为True时创建可视化
- en: ⑮ Creates visualization when trace_to_input is set to False
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: ⑮ 当trace_to_input设置为False时创建可视化
- en: 'We’ll first use the helper function to check the demographic parity difference
    in the model output for the gender protected group, as shown in the next code
    sample. Note that we are looking at model predictions only in the test set. The
    `shap_values` variable contains the Shapley values for all the inputs and adults
    in the dataset. We generated this in section 8.1.2, and you can find the source
    code in the GitHub repository associated with this book:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将使用辅助函数来检查模型输出中性别保护群体的人口统计学差异，如下代码示例所示。请注意，我们只关注测试集中的模型预测。`shap_values`变量包含数据集中所有输入和成人的Shapley值。我们是在第8.1.2节中生成的，你可以在与本书相关的GitHub仓库中找到源代码：
- en: '[PRE5]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Extracts the indices of the inputs in the test set
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ① 提取测试集中输入的索引
- en: ② Invokes the helper function to generate the SHAP plot with the appropriate
    inputs
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ② 调用辅助函数以适当的输入生成SHAP图
- en: The resulting visualization is shown in figure 8.15\. Note that the difference
    can be positive or negative. If the difference is positive, then the model is
    biased toward male adults, and if the difference is negative, then the model is
    biased toward female adults. We can see in figure 8.15 that the random forest
    model is skewed toward male adults where it predicts positive (i.e., salary >
    $50K) more for male adults.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 结果可视化如图8.15所示。请注意，差异可以是正的也可以是负的。如果差异为正，则模型偏向于男性成年人，如果差异为负，则模型偏向于女性成年人。从图8.15中我们可以看到，随机森林模型在预测正例（即薪水>50K）时对男性成年人的预测更多，因此偏向于男性成年人。
- en: '![](../Images/CH08_F15_Thampi.png)'
  id: totrans-194
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F15_Thampi.png)'
- en: Figure 8.15 SHAP demographic parity difference in model output for gender
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.15 模型输出中性别的人口统计学差异
- en: 'To determine what is causing the demographic parity difference between male
    and female adults, we can trace it back to the input features using the following
    code snippet:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确定导致男性和女性成年人之间人口统计学差异的原因，我们可以使用以下代码片段将其追溯到输入特征：
- en: '[PRE6]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Invokes the helper function with the same inputs as before but with trace_to_input
    set to True
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用与之前相同的输入调用辅助函数，但将`trace_to_input`设置为True
- en: 'The resulting plot is shown in figure 8.16\. We can see that the bias is primarily
    coming from three features: relationship, gender, and marital status. By identifying
    the features that are causing the model to violate the demographic parity fairness
    notion, we can take a closer look at the data to understand what the cause of
    the bias is for these features, as we discussed in section 8.1.1.'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图如图8.16所示。我们可以看到，偏差主要来自三个特征：关系、性别和婚姻状况。通过确定导致模型违反人口统计学公平性概念的特性，我们可以更仔细地查看数据，了解这些特征的偏差原因，正如我们在第8.1.1节中讨论的那样。
- en: '![](../Images/CH08_F16_Thampi.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F16_Thampi.png)'
- en: Figure 8.16 SHAP demographic parity difference for gender traced back to the
    inputs
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.16 追踪到输入的性别人口统计学差异
- en: As an exercise, use the helper function to determine whether a difference exists
    in the equality of opportunity fairness measure and trace it back to the inputs.
    You can set the `notion` input parameter to `equality_of_opportunity` so that
    the function will look only at the difference in model outputs and Shapley values
    for the actual positives in the dataset.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，使用辅助函数来确定机会公平性度量中的差异是否存在，并将其追溯到输入。您可以将`notion`输入参数设置为`equality_of_opportunity`，这样函数将仅查看模型输出和Shapley值之间的差异，针对数据集中的实际正例。
- en: Figure 8.17 shows the resulting visualization for the model output. We can see
    that the difference in true positive rates between male and female adults is statistically
    significant where the model is skewed toward male adults when predicting a positive
    outcome. We can say, therefore, that the model is unfair using the equality of
    opportunity notion. You can trace the bias back to the inputs by setting the `trace_to_input`
    parameter to `True`.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17显示了模型输出的结果可视化。我们可以看到，男性和女性成年人在真正例率之间的差异在统计上具有显著性，当模型在预测正例时偏向于男性成年人。因此，我们可以说，该模型在机会公平性概念下是不公平的。您可以通过将`trace_to_input`参数设置为True来追踪偏差到输入。
- en: '![](../Images/CH08_F17_Thampi.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH08_F17_Thampi.png)'
- en: Figure 8.17 SHAP equality of opportunity difference for gender
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.17 性别的机会公平性差异
- en: 8.3.2 Discrimination via representation
  id: totrans-206
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 通过表示进行歧视
- en: In some cases, it is hard to trace the discrimination issue or differences in
    fairness measures back to the inputs. For example, if the input is an image or
    a text, it will be hard to trace differences in fairness measures back to pixel
    values or values in word representations. In such cases, a better option would
    be to detect any bias in the representations learned by the models. Let’s look
    at a simple example where the objective is to train a model to detect whether
    an image contains a doctor. Let’s suppose that we have trained a convolutional
    neural network (CNN) that predicts whether an image contains a doctor. If we want
    to check whether this model is biased toward any protected group like gender,
    we can make use of the network dissection framework that we learned in chapter
    6 to determine whether the model has learned any concepts specific to the protected
    attribute. The high-level process is shown in figure 8.18.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，很难将歧视问题或公平性度量中的差异追溯到输入。例如，如果输入是图像或文本，那么将公平性度量中的差异追溯到像素值或词表示中的值将很困难。在这种情况下，一个更好的选择是检测模型学习到的表示中的任何偏差。让我们看看一个简单的例子，其目标是训练一个模型来检测图像中是否包含医生。假设我们已经训练了一个卷积神经网络（CNN），该网络预测图像中是否包含医生。如果我们想检查这个模型是否对任何受保护的群体（如性别）存在偏见，我们可以利用我们在第6章中学到的网络剖析框架来确定模型是否学习到了任何与受保护属性相关的特定概念。高级过程如图8.18所示。
- en: '![](../Images/CH08_F18_Thampi.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18](../Images/CH08_F18_Thampi.png)'
- en: Figure 8.18 A high-level illustration of how to use the network dissection framework
    to check for bias in the learned representations
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.18 使用网络剖析框架检查学习到的表示中的偏差的高级说明
- en: In figure 8.18, we focus on the gender protected attribute. The first step is
    to define a dictionary of gender-specific concepts. The figure shows an example
    where an image has been labeled at the pixel level for the various gender concepts
    such as male, female, and nonbinary. The next step is to probe the pretrained
    network followed by quantifying the alignment of each unit and layer in the CNN
    with the gender-specific concepts. Once we have quantified the alignment, we can
    check how many unique detectors exist for each of the gender concepts. If one
    of the genders has more unique detectors, we can say that the model seems to have
    learned a representation that is biased to that gender.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在图8.18中，我们关注性别受保护属性。第一步是定义一个性别特定概念的字典。图中显示了一个例子，其中图像在像素级别上被标记为各种性别概念，如男性、女性和非二元。下一步是探测预训练网络，然后量化CNN中每个单元和层与性别特定概念的匹配度。一旦我们量化了匹配度，我们就可以检查每个性别概念有多少独特的检测器。如果某个性别有更多的独特检测器，那么我们可以说模型似乎学习到了一个对该性别有偏见的表示。
- en: Let’s now look at an example where the input to the model is in the form of
    text. We learned in chapter 7 how to come up with dense and distributed representations
    of words that convey semantic meaning. How can we check whether the representations
    learned by the model are biased toward a protected group? If we look at the doctor
    example, is the word *doctor* gender neutral or biased toward any gender? We can
    check this by using of the t-distributed stochastic neighbor embedding (t-SNE)
    technique that we learned in chapter 7\. We will, however, first need to come
    up with a taxonomy for words so that we know which words are gender neutral and
    which words are associated with a particular gender, like *male* or *female*.
    Once we have the taxonomy, we can use t-SNE to visualize how close the word *doctor*
    is to other words in the corpus. If the word *doctor* is closer to other gender-neutral
    words like *hospital* or *healthcare*, the representation learned by the model
    for *doctor* is not biased. If, on the other hand, the word *doctor* is closer
    to gender-specific words like *man* or *woman*, then the representation is biased.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一个例子，其中模型的输入是文本形式。我们在第7章学习了如何得出密集和分布式的词表示，这些表示传达语义意义。我们如何检查模型学习到的表示是否对受保护的群体存在偏见？如果我们看医生这个例子，单词*doctor*是中性的还是对任何性别有偏见？我们可以通过使用我们在第7章学习的t-distributed
    stochastic neighbor embedding（t-SNE）技术来检查这一点。然而，我们首先需要制定一个单词分类法，以便我们知道哪些单词是中性的，哪些单词与特定性别相关，如*male*或*female*。一旦我们有了分类法，我们就可以使用t-SNE来可视化单词*doctor*在语料库中与其他单词的接近程度。如果单词*doctor*与其他中性的单词（如*hospital*或*healthcare*）更接近，那么模型为*doctor*学习到的表示就没有偏见。另一方面，如果单词*doctor*与性别特定的单词（如*man*或*woman*）更接近，那么表示就是有偏见的。
- en: 8.4 Mitigating bias
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.4 缓解偏差
- en: 'We have the following three broad ways of mitigating bias in a model:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有以下三种广泛的方法来减轻模型中的偏差：
- en: '*Preprocessing*—We apply these methods before training the model in an aim
    to mitigate bias in the training dataset.'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*预处理*—我们在训练模型之前应用这些方法，目的是减轻训练数据集中的偏差。'
- en: '*In-processing*—We apply these methods during model training. The fairness
    notion is explicitly or implicitly incorporated into the learning algorithm such
    that the model optimizes not just for performance (like accuracy) but also for
    fairness.'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*内处理*—我们在模型训练期间应用这些方法。公平概念被明确或隐含地纳入学习算法中，使得模型不仅优化性能（如准确率），还优化公平性。'
- en: '*Postprocessing*—We apply these methods after model training to the predictions
    made by the model. The model predictions are calibrated to ensure that the fairness
    constraint is met.'
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后处理*—我们在模型训练后对模型的预测应用这些方法。模型预测经过校准，以确保满足公平约束。'
- en: We will focus on two preprocessing methods in this section.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将关注两种预处理方法。
- en: 8.4.1 Fairness through unawareness
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.1 无意识公平
- en: One common preprocessing method is to remove any protected features from the
    model. In certain regulated industries like housing, employment, and credit, it
    is forbidden by law to use any protected features as inputs to a model used for
    decision-making. For the random forest model that we’ve trained for adult income
    prediction, let’s try to remove the two protected features of interest—gender
    and race—and see if the model is fair using the equality of opportunity/odds notion.
    As an exercise, remove the label-encoded gender and race features from the random
    forest model and retrain the model using the same hyperparameters as earlier.
    Check out the solution in the GitHub repository associated with this book.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的预处理方法是移除模型中的任何受保护特征。在某些受监管的行业，如住房、就业和信贷，法律禁止将任何受保护特征作为决策模型输入。对于我们为成人收入预测训练的随机森林模型，让我们尝试移除两个感兴趣的受保护特征——性别和种族——并查看模型是否使用机会均等/概率均等概念公平。作为一个练习，从随机森林模型中移除标签编码的性别和种族特征，并使用之前相同的超参数重新训练模型。查看与此书相关的GitHub仓库中的解决方案。
- en: The performance of the retrained model in terms of the ROC curve is shown in
    figure 8.19\. As we saw in section 8.2.2, the ROC curve was used to plot the true
    positive rate against the false positive rate, and we can use the AUC obtained
    from this ROC curve to check whether the model is fair given the equality of opportunity
    and odds notion. For the previous random forest model that used gender and race
    as input features, the AUC difference was 3% between gender groups and 2% between
    race groups. By using fairness through unawareness, the difference between the
    race groups has reduced to 1%, but with no change to the difference between the
    gender groups. Fairness through unawareness, therefore, does not provide any fairness
    guarantees. Other features could be highly correlated with these protected groups
    and could act as a proxy for gender and race. Moreover, we also see a degradation
    in model performance for all the groups where the AUC has reduced when compared
    to the previous random forest model. As we mentioned earlier, certain regulated
    industries require us to use fairness through unawareness by law. Even though
    the model is not guaranteed to be fair, the law requires that such industries
    not use any protected features in the model.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练的模型在ROC曲线上的性能展示在图8.19中。正如我们在8.2.2节中看到的，ROC曲线被用来绘制真正例率与假正例率的关系，我们可以使用从该ROC曲线获得的AUC来检查模型在机会均等和概率均等概念下是否公平。对于之前使用性别和种族作为输入特征的随机森林模型，性别组之间的AUC差异为3%，种族组之间的差异为2%。通过使用无意识公平，种族组之间的差异减少到1%，但性别组之间的差异没有变化。因此，无意识公平并不提供任何公平保证。其他特征可能与这些受保护群体高度相关，并可能作为性别和种族的代理。此外，我们还观察到所有组的模型性能都有所下降，与之前的随机森林模型相比，AUC有所降低。正如我们之前提到的，某些受监管行业要求我们依法使用无意识公平。即使模型不能保证公平，法律也要求这些行业在模型中不使用任何受保护特征。
- en: '![](../Images/CH08_F19_Thampi.png)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F19_Thampi.png)'
- en: 'Figure 8.19 ROC curve for gender and race: fairness through unawareness'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.19 性别和种族的ROC曲线：无意识公平
- en: 8.4.2 Correcting label bias through reweighting
  id: totrans-223
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4.2 通过重新加权纠正标签偏差
- en: Heinrich Jiang and Ofir Nachum proposed another preprocessing technique in 2019
    that provides fairness guarantees. In the research paper published by the authors,
    available at [https://arxiv.org/abs/1901.04966](https://arxiv.org/abs/1901.04966),
    they provide a mathematical formulation of biases that could arise in the training
    dataset. They assume that biases could occur in the observed labels in the dataset
    (also called label bias), and this could be corrected by iteratively reweighting
    the data points in the training dataset without changing the observed labels.
    They provide theoretical guarantees for this algorithm for a variety of fairness
    notions, such as demographic parity and equality of opportunity/odds. You can
    refer to the original paper to learn more about the mathematical formulation and
    the proofs. In this section, we will provide an overview of the algorithm and
    use the implementation provided by the authors at [http://mng.bz/Ygjj](http://mng.bz/Ygjj).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: Heinrich Jiang和Ofir Nachum在2019年提出了一种另一种预处理技术，该技术提供了公平性保证。在作者发布的、可在[https://arxiv.org/abs/1901.04966](https://arxiv.org/abs/1901.04966)找到的研究论文中，他们提供了可能出现在训练数据集中的偏差的数学公式。他们假设偏差可能出现在数据集中的观察标签中（也称为标签偏差），并且可以通过迭代重新加权训练数据集中的数据点来纠正，而不改变观察到的标签。他们为各种公平性概念提供了该算法的理论保证，如人口统计的平等性和机会/概率的平等。您可以参考原始论文以了解更多关于数学公式和证明的信息。在本节中，我们将概述该算法，并使用作者在[http://mng.bz/Ygjj](http://mng.bz/Ygjj)提供的实现。
- en: The algorithm for correcting bias through reweighting is hinged on a key assumption.
    The labels observed in the dataset are based on an underlying true and unbiased
    set of labels that is unknown. The observed dataset is biased due to a labeler
    or process that is introducing the bias. The key assumption is that the source
    of this bias is unintentional and potentially due to unconscious or inherent biases.
    Based on this assumption, the authors of the paper mathematically prove that it
    is possible to build an unbiased classifier that is potentially trained on the
    unknown, unbiased dataset through reweighting of the features in the observed,
    biased dataset. This is illustrated in figure 8.20.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 通过重新加权纠正偏差的算法依赖于一个关键假设。数据集中观察到的标签基于一个未知的真实且无偏的标签集，这个标签集是未知的。观察到的数据集由于标签者或引入偏差的过程而存在偏差。关键假设是这种偏差的来源是无意的，并且可能由于无意识或固有的偏差。基于这个假设，论文的作者从数学上证明了通过重新加权观察到的、有偏差的数据集中的特征，可以构建一个无偏分类器，该分类器可能是在未知的、无偏的数据集上通过重新加权训练的。这如图8.20所示。
- en: '![](../Images/CH08_F20_Thampi.png)'
  id: totrans-226
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F20_Thampi.png)'
- en: Figure 8.20 Underlying assumption of the source of label bias in the reweighting
    approach
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.20 重新加权方法中标签偏差来源的潜在假设
- en: The label bias reweighting algorithm is iterative and is summarized in figure
    8.21\. Let’s assume that there are *K* protected groups in the dataset and *N*
    features. For the adult income dataset, the number of protected groups that are
    considered are four (two gender groups and two race groups). The dataset contains
    14 features. Before running the algorithm, we need to initialize coefficients
    for each of the protected groups with a value of 0\. We also need to initialize
    weights for each of the features with a value of 1.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 标签偏差重新加权算法是迭代的，总结如图8.21所示。假设数据集中有*K*个受保护组，*N*个特征。对于成人收入数据集，考虑的受保护组数量是四个（两个性别组和两个种族组）。数据集包含14个特征。在运行算法之前，我们需要为每个受保护组初始化系数，其值为0。我们还需要为每个特征初始化权重，其值为1。
- en: With the coefficient and weights initialized, the next step is to train a model
    using these weights. Because we are considering the random forest model in this
    chapter, the model trained in this step will be identical to the model trained
    in section 8.1.2\. The next step is to compute the fairness violations for this
    model for each of the *K* protected groups. The fairness violation is dependent
    on the specific notion that we are interested in. If the fairness notion is demographic
    parity, then the fairness violation for a protected group is the difference in
    the overall average positive rate for the model and the average positive rate
    for that specific protected group. If the fairness notion is equality of opportunity,
    then we need to consider the difference in the overall average true positive rate
    and the average true positive rate for the specific protected group. Once we have
    computed the fairness violations, the next step is to update the coefficients
    for each of the protected groups. The objective of the algorithm is to minimize
    the fairness violation, so we update the coefficients by subtracting the fairness
    violations from it. The final step is to then update the weights for each of the
    features using the coefficients for the protected groups. The formula for updating
    the weights is shown in figure 8.21, and you can see the derivation of this formula
    in the original paper published by the authors of this algorithm. The steps in
    this paragraph are then repeated *T* times where *T* is a hyperparameter that
    represents the number of iterations for which we want to run the algorithm.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 系数和权重初始化完成后，下一步是使用这些权重训练一个模型。由于本章我们考虑的是随机森林模型，因此这一步训练的模型将与第 8.1.2 节中训练的模型相同。下一步是为每个
    *K* 个受保护群体计算该模型的公平性违规。公平性违规取决于我们感兴趣的具体概念。如果公平性概念是人口平衡，那么受保护群体的公平性违规是模型的整体平均正面率与该特定受保护群体的平均正面率之间的差异。如果公平性概念是机会平等，那么我们需要考虑整体平均真正阳性率和特定受保护群体的平均真正阳性率之间的差异。一旦我们计算了公平性违规，下一步是更新每个受保护群体的系数。算法的目标是最小化公平性违规，因此我们通过从其中减去公平性违规来更新系数。最后一步是使用受保护群体的系数更新每个特征的权重。更新权重的公式如图
    8.21 所示，您可以在该算法作者发表的原论文中找到该公式的推导过程。本段中的步骤随后会重复 *T* 次，其中 *T* 是一个超参数，表示我们希望算法运行的迭代次数。
- en: '![](../Images/CH08_F21_Thampi.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图 8.21](../Images/CH08_F21_Thampi.png)'
- en: Figure 8.21 Label bias reweighting algorithm
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.21 标签偏差重加权算法
- en: 'Now let’s apply this algorithm to the adult income dataset the model trained
    earlier. But before running the algorithm, we first need to prepare the data using
    the following code snippet. The idea is to convert the label-encoded gender and
    race features into one-hot encoded features where there is one column for each
    of the four protected groups (male, female, white, and Black adults) and a value
    of 0 or 1 to indicate whether the adult belongs to that specific protected group:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将此算法应用于之前训练的成人收入数据集。但在运行算法之前，我们首先需要使用以下代码片段准备数据。思路是将标签编码的性别和种族特征转换为独热编码特征，其中有一个列对应于四个受保护群体（男性、女性、白人和黑人成年人）中的一个，并使用
    0 或 1 的值来表示成年人是否属于该特定受保护群体：
- en: '[PRE7]'
  id: totrans-233
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Imports the partial function provided by the Python functool library
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入 Python functool 库提供的部分函数
- en: ② Helper function to prepare the dataset for the label bias reweighting algorithm
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: ② 为标签偏差重加权算法准备数据集的辅助函数
- en: ③ Creates a copy of the original DataFrame and makes changes to the copy
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 创建原始 DataFrame 的副本并对副本进行修改
- en: ④ Helper function that maps each feature to its encoded value
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将每个特征映射到其编码值的辅助函数
- en: ⑤ Loops through all the protected features and creates separate columns for
    each group with their corresponding binary-encoded value
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 遍历所有受保护特征，并为每个群体创建具有相应二进制编码值的单独列
- en: ⑥ Drops the original protected feature columns from the copy of the DataFrame
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 从 DataFrame 的副本中删除原始受保护特征列
- en: ⑦ Returns the copy of the DataFrame with the new columns
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 返回带有新列的 DataFrame 副本
- en: 'You can then use this helper function to prepare the dataset as follows. Note
    that a mapping of each protected group to its corresponding label-encoded value
    is created before invoking the helper function:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以使用此辅助函数如下准备数据集。请注意，在调用辅助函数之前，为每个受保护群体创建了一个映射到其对应标签编码值的映射：
- en: '[PRE8]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① List of protected features to process
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ① 需要处理的受保护特征列表
- en: ② Mapping of each protected group to its corresponding label-encoded value
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ② 将每个受保护群体映射到其相应的标签编码值
- en: ③ Invokes a helper function to prepare the dataset for the label bias reweighting
    algorithm
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 调用辅助函数以准备数据集用于标签偏差重加权算法
- en: ④ Splits the new dataset into train and test sets
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将新的数据集分为训练集和测试集
- en: ⑤ Extracts the columns for the protected groups
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 提取受保护群体的列
- en: 'Once you have prepared the dataset, you can easily plug it into the label bias
    reweighting algorithm. You can find the source code for this algorithm in the
    GitHub repository ([http://mng.bz/Ygjj](http://mng.bz/Ygjj)) published by the
    authors of the paper. In the interest of space, we will not rehash that code in
    this section. As an exercise, run through the algorithm and determine the weights
    for each of the data points in the training dataset. Once you have determined
    the weights, you can then retrain the unbiased random forest model by using the
    following code snippet:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你准备好了数据集，你就可以轻松地将它插入到标签偏差重加权算法中。你可以在论文作者发布的GitHub仓库([http://mng.bz/Ygjj](http://mng.bz/Ygjj))中找到这个算法的源代码。为了节省空间，我们不会在本节中重复该代码。作为一个练习，运行该算法并确定训练数据集中每个数据点的权重。一旦确定了权重，你可以使用以下代码片段重新训练无偏的随机森林模型：
- en: '[PRE9]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Uses the helper function learned in chapter 3 to create the random forest
    model
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ① 使用第3章中学习的辅助函数创建随机森林模型
- en: ② Invokes the fit method and passes in the prepared dataset and the weights
    obtained using the label bias reweighting algorithm
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ② 调用fit方法并传入准备好的数据集和标签偏差重加权算法获得的权重
- en: The performance of the retrained model in terms of the ROC curve is shown in
    figure 8.22\. We can see that the differences in AUC between the gender groups
    and race groups are both 1%. This model is, therefore, fairer in terms of equality
    of opportunity and odds than the previously trained random forest model with gender
    and race as features and the model trained without those features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 重新训练的模型在ROC曲线上的性能如图8.22所示。我们可以看到，性别组和种族组之间的AUC差异都是1%。因此，这个模型在机会平等和概率方面比之前训练的以性别和种族为特征的随机森林模型以及没有这些特征的模型更加公平。
- en: '![](../Images/CH08_F22_Thampi.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH08_F22_Thampi.png)'
- en: Figure 8.22 ROC curve for gender and race after correcting for label bias
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.22纠正标签偏差后的性别和种族ROC曲线
- en: 8.5 Datasheets for datasets
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.5 数据集的数据表
- en: 'While exploring the adult income dataset in section 8.1.1, we noticed that
    some protected groups (such as female and Black adults) were not properly represented,
    and biases existed in the labels for these groups. We identified a few sources
    of bias, namely, sampling bias and label bias, but we could not identify the root
    cause of the bias. The primary reason for this is that the data-collection process
    for this dataset is unknown. In a paper published by Timnit Gebru and other researchers
    at Google and Microsoft in 2020, a standardized process was proposed to document
    datasets. The idea is for data creators to come up with a datasheet that answers
    key questions regarding the motivation, composition, data-collection process,
    and uses of a dataset. Some of the key questions are highlighted next, but a more
    thorough study of this can be found in the original research paper at [https://arxiv.org/pdf/1803.09010.pdf](https://arxiv.org/pdf/1803.09010.pdf):'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 在第8.1.1节探索成人收入数据集时，我们注意到一些受保护群体（如女性和黑人成年人）没有得到适当的代表，这些群体的标签存在偏见。我们确定了几个偏见来源，即抽样偏差和标签偏差，但我们无法确定偏差的根本原因。主要原因在于这个数据集的数据收集过程是未知的。在2020年，谷歌和微软的研究员Timnit
    Gebru和其他研究人员发表的一篇论文中，提出了一种标准化的流程来记录数据集。这个想法是让数据创建者制定一份数据表，回答关于数据集动机、组成、数据收集过程和用途的关键问题。以下是一些关键问题的概述，但更深入的研究可以在原始研究论文[https://arxiv.org/pdf/1803.09010.pdf](https://arxiv.org/pdf/1803.09010.pdf)中找到：
- en: Motivation
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 动机
- en: For what purpose was the dataset created? The goal of this question is to understand
    whether the dataset is meant for a specific task or to address a specific gap
    or need.
  id: totrans-258
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集创建的目的是什么？这个问题的目的是了解数据集是否针对特定任务或解决特定差距或需求。
- en: Who created the dataset? The goal is to identify an owner for the dataset, which
    could be an individual, team, company, organization, or institution.
  id: totrans-259
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁创建了数据集？目标是确定数据集的所有者，这可能是一个个人、团队、公司、组织或机构。
- en: Who funded the creation of the dataset? The goal is to understand whether the
    dataset is associated with a research grant or any other source of funding.
  id: totrans-260
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谁资助了数据集的创建？目标是了解数据集是否与研究拨款或其他资金来源相关联。
- en: Composition
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 组成
- en: What does the dataset represent? The goal is to understand whether the data
    represents documents, photos, videos, people, countries, or any other representation.
  id: totrans-262
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集代表什么？目标是了解数据是否代表文档、照片、视频、人物、国家或其他任何表示。
- en: How many examples in the dataset? This question is self-explanatory and is meant
    to understand the size of the dataset in terms of the number of data points or
    examples.
  id: totrans-263
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中有多少个示例？这个问题是显而易见的，旨在了解数据集的大小，即数据点或示例的数量。
- en: Does the dataset contain all possible examples, or is it a sample from a larger
    dataset? The goal is to understand whether the dataset is a sample from a larger
    dataset or population. This will help us check whether any sampling bias exists.
  id: totrans-264
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是否包含所有可能的示例，或者它是一个更大数据集或总体的样本？目标是了解数据集是否是从更大数据集或总体中抽取的样本。这将帮助我们检查是否存在任何抽样偏差。
- en: Is the dataset labeled? The goal is to check whether the dataset is raw or labeled.
  id: totrans-265
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是否已标记？目标是检查数据集是原始的还是标记的。
- en: Does the dataset rely on external sources? The goal is to identify whether there
    is any external source or dependency for the dataset such as websites, tweets
    on Twitter, or any other dataset.
  id: totrans-266
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集是否依赖于外部来源？目标是确定数据集是否存在任何外部来源或依赖，例如网站、Twitter上的推文或任何其他数据集。
- en: Collection process
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 收集过程
- en: How was the data acquired? This question helps us understand the data-collection
    process.
  id: totrans-268
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是如何获取的？这个问题有助于我们了解数据收集过程。
- en: What was the sampling strategy used, if applicable? This is an extension of
    the sampling question in the Composition section and helps us check whether any
    sampling bias exists.
  id: totrans-269
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果适用，使用了什么抽样策略？这是组成部分中抽样问题的扩展，帮助我们检查是否存在任何抽样偏差。
- en: Over what timeframe was the data collected?
  id: totrans-270
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是在什么时间段内收集的？
- en: Was the data collected from individuals directly or through a third party?
  id: totrans-271
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从个人直接收集数据还是通过第三方收集的？
- en: If the data is related to people, was their consent obtained for data collection?
    If the dataset is related to people, it is important that we work with experts
    in other domains like anthropology. The answer to this question is also essential
    to determine whether the dataset is compliant with regulations like the General
    Data Protection Regulation (GDPR) in the European Union (EU).
  id: totrans-272
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果数据与人物相关，是否已获得他们的数据收集同意？如果数据集与人物相关，我们与人类学等领域的专家合作非常重要。这个问题的答案也是确定数据集是否符合欧盟（EU）的通用数据保护条例（GDPR）等规定的重要部分。
- en: Does a mechanism for individuals exist to revoke consent in the future? This
    is also essential to determining whether the dataset is compliant with regulations.
  id: totrans-273
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在个人在未来撤销同意的机制？这也是确定数据集是否符合规定的重要部分。
- en: Uses
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用
- en: What will the dataset be used for? The goal is to identify all the possible
    tasks or uses for the dataset.
  id: totrans-275
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集将用于什么？目标是确定数据集的所有可能任务或用途。
- en: Should the dataset not be used for any tasks? The answer to this question will
    help us ensure that the dataset is not used for tasks that it isn’t intended for.
  id: totrans-276
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否不应将数据集用于任何任务？这个问题的答案将帮助我们确保数据集不会被用于其未打算用于的任务。
- en: Datasheets for datasets have already been adopted by research and industry.
    Some examples are the QuAC dataset used for question answering ([https://quac.ai/datasheet.pdf](https://quac.ai/datasheet.pdf)),
    the RecipeQA dataset consisting of cooking recipes ([http://mng.bz/GGnA](http://mng.bz/GGnA)),
    and the Open Images dataset ([https://github.com/amukka/openimages](https://github.com/amukka/openimages)).
    Although datasheets for datasets add additional overheard for dataset creators,
    they improve transparency and accountability, help us determine sources of bias
    if there are any, and ensure that we are compliant with regulations such as GDPR
    in the EU.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集的数据表已经被研究和工业界采用。一些例子包括用于问答的QuAC数据集([https://quac.ai/datasheet.pdf](https://quac.ai/datasheet.pdf))、包含烹饪食谱的RecipeQA数据集([http://mng.bz/GGnA](http://mng.bz/GGnA))和Open
    Images数据集([https://github.com/amukka/openimages](https://github.com/amukka/openimages))。尽管数据集的数据表为数据集创建者增加了额外的开销，但它们提高了透明度和问责制，帮助我们确定是否存在任何偏差的来源，并确保我们遵守欧盟的GDPR等规定。
- en: Summary
  id: totrans-278
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Various sources of bias could occur in a dataset, such as sampling bias and
    label bias. Sampling bias occurs when the dataset does not properly represent
    the true population. Labeling bias occurs when bias exists in the way that labels
    are recorded for various groups in the population.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集中可能存在各种偏差来源，例如采样偏差和标签偏差。采样偏差发生在数据集未能正确代表真实人口时。标签偏差发生在对人口中不同群体的标签记录方式存在偏差时。
- en: Various fairness notions include demographic parity, equality of opportunity
    and odds, predictive quality parity, fairness through unawareness, and counterfactual
    fairness. Commonly used fairness notions are demographic parity and equality of
    opportunity and odds.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 不同的公平性概念包括人口统计学上的平等、机会和概率的平等、预测质量的平等、通过无意识实现的公平以及反事实公平。常用的公平性概念包括人口统计学上的平等和机会及概率的平等。
- en: Demographic parity is sometimes also called independence or statistical parity
    and is legally known as disparate impact. It asserts that a model contains parity
    in the positive prediction rates for different protected groups. The demographic
    parity notion is useful for scenarios where we want to ensure parity in the treatment
    of all protected groups, irrespective of their prevalence in the population. It
    ensures that the minority group is treated the same way as the majority group.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 人口统计学上的平等有时也称为独立性或统计平等，在法律上被称为不同影响。它断言模型在不同受保护群体的积极预测率中包含相等性。人口统计学上的平等概念对于我们想要确保所有受保护群体在人口中的普遍性不受影响时保持平等的场景是有用的。它确保少数群体得到与多数群体相同的待遇。
- en: For scenarios where we want to consider the distribution of the actual label
    for all protected groups, we can use the equality of opportunity and odds fairness
    notions. We say that equality of opportunity exists when parity is present in
    the true positive rates between the groups. Equality of odds extends the definition
    of equality of opportunity to another symmetric measurement called false positive
    rate.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于我们想要考虑所有受保护群体实际标签分布的场景，我们可以使用机会和概率的公平性概念。我们说存在机会平等，当组间真实阳性率存在相等性时。机会平等将机会平等的定义扩展到另一个对称的测量，即假阳性率。
- en: 'We can categorize all the notions of fairness into two sets: group fairness
    and individual fairness. Group fairness ensures that the model is fair for different
    protected groups. Individual fairness, on the other hand, ensures that the model
    makes similar predictions for similar individuals.'
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将所有公平性概念分为两组：群体公平性和个体公平性。群体公平性确保模型对不同受保护群体是公平的。另一方面，个体公平性确保模型对相似个体做出相似的预测。
- en: 'We can use the interpretability techniques that we have learned in this book
    to detect a source of discrimination due to the model. The source of discrimination
    can be broadly categorized into two types: discrimination via input features and
    discrimination via representation.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用本书中学到的可解释性技术来检测模型导致的歧视来源。歧视的来源可以大致分为两种类型：通过输入特征的歧视和通过表示的歧视。
- en: Discrimination via input traces the discrimination or fairness issues back to
    the input features. We can use the SHAP interpretability technique to trace fairness
    issues back to the input.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过输入追踪将歧视或公平性问题追溯到输入特征。我们可以使用SHAP可解释性技术将公平性问题追溯到输入。
- en: These types of fairness issues are hard to trace back to the input features,
    especially for deep learning models that process inputs like images and text.
    For such cases, we could instead trace the source of discrimination to deep representations
    learned by the model. We can trace the source of discrimination to representations
    learned by the model using the network dissection framework and the t-SNE technique
    learned in chapters 6 and 7, respectively.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些类型的公平性问题难以追溯到输入特征，特别是对于处理图像和文本等输入的深度学习模型。在这种情况下，我们可以将歧视的来源追溯到模型学习到的深度表示。我们可以使用网络分解框架和第6章和第7章中学习的t-SNE技术来分别追踪歧视的来源。
- en: Examples of two techniques that we can use to mitigate bias are fairness through
    unawareness and a reweighting technique to correct label bias. Fairness through
    unawareness does not guarantee fairness, but the reweighting technique does provide
    fairness guarantees.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用两种技术来减轻偏差：通过无意识实现的公平和用于纠正标签偏差的重加权技术。通过无意识实现的公平并不保证公平，但重加权技术确实提供了公平保证。
- en: A standardized process exists to document datasets using datasheets. Datasheets
    aim to answer key questions regarding the motivation, composition, data-collection
    process, and uses of a dataset. Although datasheets for datasets add additional
    overheard for dataset creators, they improve transparency and accountability,
    help us determine sources of bias if there are any, and ensure that we are compliant
    with regulation such as GDPR in the EU.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存在一个标准化的流程，用于使用数据表来记录数据集。数据表旨在回答关于数据集动机、组成、数据收集过程以及使用的关键问题。尽管为数据集创建数据表会给数据集创建者带来额外的开销，但它们提高了透明度和问责制，帮助我们确定是否存在任何偏见来源，并确保我们符合欧盟的GDPR等法规。
