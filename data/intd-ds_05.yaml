- en: Chapter 6\. Join the NoSQL movement
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章. 加入NoSQL运动
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding NoSQL databases and why they’re used today
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解NoSQL数据库及其为何在今天被使用
- en: Identifying the differences between NoSQL and relational databases
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 识别NoSQL数据库和关系型数据库之间的差异
- en: Defining the ACID principle and how it relates to the NoSQL BASE principle
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义ACID原则及其与NoSQL BASE原则的关系
- en: Learning why the CAP theorem is important for multi-node database setup
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习CAP定理对于多节点数据库设置的重要性
- en: Applying the data science process to a project with the NoSQL database Elasticsearch
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据科学流程应用于使用NoSQL数据库Elasticsearch的项目
- en: 'This chapter is divided into two parts: a theoretical start and a practical
    finish.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章分为两部分：理论开始和实际结束。
- en: 'In the first part of this chapter we’ll look into NoSQL databases in general
    and answer these questions: Why do they exist? Why not until recently? What types
    are there and why should you care?'
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章的第一部分，我们将探讨NoSQL数据库的一般情况，并回答以下问题：为什么它们存在？为什么直到最近才出现？有哪些类型，为什么你应该关心？
- en: In part two we’ll tackle a real-life problem—disease diagnostics and profiling—using
    freely available data, Python, and a NoSQL database.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在第二部分，我们将解决一个现实生活中的问题——疾病诊断和画像——使用免费数据、Python和NoSQL数据库。
- en: No doubt you’ve heard about NoSQL databases and how they’re used religiously
    by many high-tech companies. But what are NoSQL databases and what makes them
    so different from the relational or SQL databases you’re used to? *NoSQL* is short
    for *Not Only Structured Query Language*, but although it’s true that NoSQL databases
    can allow you to query them with SQL, you don’t have to focus on the actual name.
    Much debate has already raged over the name and whether this group of new databases
    should even have a collective name at all. Rather, let’s look at what they represent
    as opposed to *relational database management systems (RDBMS)*. Traditional databases
    reside on a single computer or server. This used to be fine as a long as your
    data didn’t outgrow your server, but it hasn’t been the case for many companies
    for a long time now. With the growth of the internet, companies such as Google
    and Amazon felt they were held back by these single-node databases and looked
    for alternatives.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 毫无疑问，你已经听说过NoSQL数据库以及它们被许多高科技公司虔诚地使用。但NoSQL数据库是什么，它们与您所习惯的关系型或SQL数据库有什么不同？*NoSQL*代表的是*Not
    Only Structured Query Language*，尽管NoSQL数据库确实可以让你用SQL查询它们，但你不必专注于这个名字。关于这个名字的争论已经很多，甚至有人质疑这一组新数据库是否应该有一个集体名称。相反，让我们看看它们与*关系型数据库管理系统（RDBMS）*相比代表了什么。传统的数据库驻留在单个计算机或服务器上。只要你的数据没有超出你的服务器，这曾经是可以的，但现在对许多公司来说已经不再是这种情况了。随着互联网的增长，像谷歌和亚马逊这样的公司感到这些单节点数据库限制了它们，并寻求替代方案。
- en: 'Numerous companies use single-node NoSQL databases such as MongoDB because
    they want the flexible schema or the ability to hierarchically aggregate data.
    Here are several early examples:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 许多公司使用单节点NoSQL数据库，例如MongoDB，因为它们想要灵活的架构或能够分层聚合数据。以下是一些早期的例子：
- en: Google’s first NoSQL solution was Google BigTable, which marked the start of
    the *columnar databases.*^([[1](#ch06fn01)])
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 谷歌的第一个NoSQL解决方案是Google BigTable，这标志着*列式数据库*的开始。^([[1](#ch06fn01)])
- en: ¹
  id: totrans-13
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-14
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf).
  id: totrans-15
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看[http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf](http://static.googleusercontent.com/media/research.google.com/en//archive/bigtable-osdi06.pdf)。
- en: Amazon came up with Dynamo, *a key-value store.*^([[2](#ch06fn02)])
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊提出了Dynamo，*一个键值存储器*。^([[2](#ch06fn02)])
- en: ²
  id: totrans-17
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-18
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf).
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 查看[http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf](http://www.allthingsdistributed.com/files/amazon-dynamo-sosp2007.pdf)。
- en: 'Two more database types emerged in the quest for partitioning: the *document
    store* and the *graph database*.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在追求分区的过程中，出现了两种更多的数据库类型：*文档存储*和*图数据库*。
- en: We’ll go into detail on each of the four types later in the chapter.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章后面详细讨论这四种类型。
- en: 'Please note that, although size was an important factor, these databases didn’t
    originate solely from the need to handle larger volumes of data. Every *V* of
    big data has influence (volume, variety, velocity, and sometimes veracity). Graph
    databases, for instance, can handle network data. Graph database enthusiasts even
    claim that everything can be seen as a network. For example, how do you prepare
    dinner? With ingredients. These ingredients are brought together to form the dish
    and can be used along with other ingredients to form other dishes. Seen from this
    point of a view, ingredients and recipes are part of a network. But recipes and
    ingredients could also be stored in your relational database or a document store;
    it’s all how you look at the problem. Herein lies the strength of NoSQL: the ability
    to look at a problem from a different angle, shaping the data structure to the
    use case. As a data scientist, your job is to find the best answer to any problem.
    Although sometimes this is still easier to attain using RDBMS, often a particular
    NoSQL database offers a better approach.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，尽管大小是一个重要因素，但这些数据库并非仅仅源于处理更大数据量的需求。大数据的每个*V*（体积、种类、速度和有时是真实性）都有影响。例如，图数据库可以处理网络数据。图数据库的爱好者甚至声称一切都可以被视为网络。例如，你如何准备晚餐？用食材。这些食材被组合在一起形成菜肴，可以与其他食材一起使用来形成其他菜肴。从这个角度来看，食材和食谱是网络的一部分。但食谱和食材也可以存储在你的关系数据库或文档存储中；这完全取决于你如何看待问题。这正是NoSQL的强大之处：能够从不同的角度看待问题，将数据结构塑造成使用场景。作为数据科学家，你的工作是找到任何问题的最佳答案。尽管有时这仍然更容易通过RDBMS来实现，但通常特定的NoSQL数据库提供了一种更好的方法。
- en: Are relational databases doomed to disappear in companies with big data because
    of the need for partitioning? No, NewSQL platforms (not to be confused with NoSQL)
    are the RDBMS answer to the need for cluster setup. NewSQL databases follow the
    relational model but are capable of being divided into a distributed cluster like
    NoSQL databases. It’s not the end of relational databases and certainly not the
    end of SQL, as platforms like Hive translate SQL into MapReduce jobs for Hadoop.
    Besides, not every company needs big data; many do fine with small databases and
    the traditional relational databases are perfect for that.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于需要分区，关系数据库在公司的大数据中注定要消失吗？不，NewSQL平台（不要与NoSQL混淆）是RDBMS对集群设置需求的回答。NewSQL数据库遵循关系模型，但能够像NoSQL数据库一样被划分为分布式集群。这并不是关系数据库的终结，当然也不是SQL的终结，因为像Hive这样的平台将SQL转换为Hadoop的MapReduce作业。此外，并非每个公司都需要大数据；许多公司用小型数据库就能做得很好，传统的数据库关系数据库非常适合这种情况。
- en: If you look at the big data mind map shown in [figure 6.1](#ch06fig01), you’ll
    see four types of NoSQL databases.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看[图6.1](#ch06fig01)中显示的大数据思维导图，你会看到四种类型的NoSQL数据库。
- en: Figure 6.1\. NoSQL and NewSQL databases
  id: totrans-25
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1\. NoSQL和NewSQL数据库
- en: '![](Images/06fig01.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig01.jpg)'
- en: These four types are document store, key-value store, graph database, and column
    database. The mind map also includes the NewSQL partitioned relational databases.
    In the future this big split between NoSQL and NewSQL will become obsolete because
    every database type will have its own focus, while combining elements from both
    NoSQL and NewSQL databases. The lines are slowly blurring as RDBMS types get NoSQL
    features such as the column-oriented indexing seen in columnar databases. But
    for now it’s a good way to show that the old relational databases have moved past
    their single-node setup, while other database types are emerging under the NoSQL
    denominator.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 这四种类型是文档存储、键值存储、图数据库和列数据库。思维导图还包括NewSQL分区关系数据库。在未来，这种NoSQL和NewSQL之间的巨大分歧将变得过时，因为每种数据库类型都将有其自己的重点，同时结合NoSQL和NewSQL数据库的元素。随着RDBMS类型获得NoSQL功能，如列数据库中看到的列式索引，这些界限正在逐渐模糊。但就目前而言，这是一个很好的方式来展示旧的关系数据库已经超越了单节点设置，而其他数据库类型正在NoSQL的范畴下出现。
- en: Let’s look at what NoSQL brings to the table.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看NoSQL带来了什么。
- en: 6.1\. Introduction to NoSQL
  id: totrans-29
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1\. NoSQL简介
- en: As you’ve read, the goal of NoSQL databases isn’t only to offer a way to partition
    databases successfully over multiple nodes, but also to present fundamentally
    different ways to model the data at hand to fit its structure to its use case
    and not to how a relational database requires it to be modeled.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所读，NoSQL数据库的目标不仅仅是提供一种在多个节点上成功分区数据库的方法，而且还提供了一种根本不同的方式来建模手头的数据，以适应其结构并符合其使用场景，而不是如何按照关系数据库的要求来建模。
- en: To help you understand NoSQL, we’re going to start by looking at the core ACID
    principles of single-server relational databases and show how NoSQL databases
    rewrite them into BASE principles so they’ll work far better in a distributed
    fashion. We’ll also look at the CAP theorem, which describes the main problem
    with distributing databases across multiple nodes and how ACID and BASE databases
    approach it.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您理解NoSQL，我们将首先查看单服务器关系型数据库的核心ACID原则，并展示NoSQL数据库如何将它们重写为BASE原则，以便在分布式环境中工作得更好。我们还将探讨CAP定理，它描述了在多个节点上分布数据库的主要问题以及ACID和BASE数据库如何处理这个问题。
- en: '6.1.1\. ACID: the core principle of relational databases'
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. ACID：关系型数据库的核心原则
- en: 'The main aspects of a traditional relational database can be summarized by
    the concept ACID:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 传统关系型数据库的主要方面可以通过ACID概念来总结：
- en: '***Atomicity*** —The “all or nothing” principle. If a record is put into a
    database, it’s put in completely or not at all. If, for instance, a power failure
    occurs in the middle of a database write action, you wouldn’t end up with half
    a record; it wouldn’t be there at all.'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***原子性*** —“全有或全无”原则。如果一条记录被放入数据库，它要么完全放入，要么根本不放入。例如，如果在数据库写入操作中途发生断电，您不会得到半条记录；它根本不会在那里。'
- en: '***Consistency*** —This important principle maintains the integrity of the
    data. No entry that makes it into the database will ever be in conflict with predefined
    rules, such as lacking a required field or a field being numeric instead of text.'
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一致性*** —这个重要的原则维护了数据的完整性。任何进入数据库的条目都不会与预定义的规则发生冲突，例如缺少必需的字段或字段为数值而非文本。'
- en: '***Isolation*** —When something is changed in the database, nothing can happen
    on this exact same data at exactly the same moment. Instead, the actions happen
    in serial with other changes. Isolation is a scale going from low isolation to
    high isolation. On this scale, traditional databases are on the “high isolation”
    end. An example of low isolation would be Google Docs: Multiple people can write
    to a document at the exact same time and see each other’s changes happening instantly.
    A traditional Word document, on the other end of the spectrum, has high isolation;
    it’s locked for editing by the first user to open it. The second person opening
    the document can view its last saved version but is unable to see unsaved changes
    or edit the document without first saving it as a copy. So once someone has it
    opened, the most up-to-date version is completely isolated from anyone but the
    editor who locked the document.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***隔离性*** —当数据库中的某个东西发生变化时，在完全相同的时刻，对此数据的任何操作都无法发生。相反，这些操作会与其他更改串行执行。隔离性是一个从低隔离到高隔离的尺度。在这个尺度上，传统数据库位于“高隔离”的一端。低隔离的一个例子是Google
    Docs：多个人可以同时向文档中写入，并立即看到彼此的更改。在光谱的另一端，传统的Word文档具有高隔离性；它被第一个打开文档的用户锁定以供编辑。第二个打开文档的人可以查看其最后保存的版本，但无法查看未保存的更改或编辑文档，除非首先将其保存为副本。因此，一旦有人打开它，最新版本就完全隔离于除编辑该文档的人之外的所有人。'
- en: '***Durability*** —If data has entered the database, it should survive permanently.
    Physical damage to the hard discs will destroy records, but power outages and
    software crashes should not.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***持久性*** —如果数据已进入数据库，它应该永久存活。硬盘物理损坏会破坏记录，但断电和软件崩溃不应该。'
- en: 'ACID applies to all relational databases and certain NoSQL databases, such
    as the graph database Neo4j. We’ll further discuss graph databases later in this
    chapter and in [chapter 7](kindle_split_015.xhtml#ch07). For most other NoSQL
    databases another principle applies: BASE. To understand BASE and why it applies
    to most NoSQL databases, we need to look at the CAP Theorem.'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ACID适用于所有关系型数据库以及某些NoSQL数据库，例如图数据库Neo4j。我们将在本章和第7章[chapter 7](kindle_split_015.xhtml#ch07)中进一步讨论图数据库。对于大多数其他NoSQL数据库，另一个原则适用：BASE。为了理解BASE以及为什么它适用于大多数NoSQL数据库，我们需要看看CAP定理。
- en: '6.1.2\. CAP Theorem: the problem with DBs on many nodes'
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. CAP定理：多节点数据库的问题
- en: 'Once a database gets spread out over different servers, it’s difficult to follow
    the ACID principle because of the consistency ACID promises; the CAP Theorem points
    out why this becomes problematic. The CAP Theorem states that a database can be
    any two of the following things but never all three:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据库分散到不同的服务器上，由于ACID承诺的一致性，就很难遵循ACID原则。CAP定理指出为什么这会变成一个问题。CAP定理指出，数据库可以是以下三个属性中的任意两个，但永远不能是三个：
- en: '***Partition tolerant*** —The database can handle a network partition or network
    failure.'
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***分区容错*** —数据库可以处理网络分区或网络故障。'
- en: '***Available*** —As long as the node you’re connecting to is up and running
    and you can connect to it, the node will respond, even if the connection between
    the different database nodes is lost.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可用性*** —只要您连接到的节点处于运行状态并且您可以连接到它，节点就会响应，即使不同数据库节点之间的连接丢失。'
- en: '***Consistent*** —No matter which node you connect to, you’ll always see the
    exact same data.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一致性*** —无论您连接到哪个节点，您都会看到完全相同的数据。'
- en: 'For a single-node database it’s easy to see how it’s always available and consistent:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单节点数据库，很容易看出它总是可用且一致的：
- en: '***Available*** —As long as the node is up, it’s available. That’s all the
    CAP availability promises.'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可用性*** —只要节点处于运行状态，它就是可用的。这就是CAP可用性所承诺的一切。'
- en: '***Consistent*** —There’s no second node, so nothing can be inconsistent.'
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一致性*** —没有第二个节点，所以不可能出现不一致的情况。'
- en: Things get interesting once the database gets partitioned. Then you need to
    make a choice between availability and consistency, as shown in [figure 6.2](#ch06fig02).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据库分区时，事情变得有趣。然后您需要在可用性和一致性之间做出选择，如图6.2所示。
- en: 'Figure 6.2\. CAP Theorem: when partitioning your database, you need to choose
    between availability and consistency.'
  id: totrans-48
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.2\. CAP定理：在分区数据库时，您需要在可用性和一致性之间做出选择。
- en: '![](Images/06fig02.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig02.jpg)'
- en: 'Let’s take the example of an online shop with a server in Europe and a server
    in the United States, with a single distribution center. A German named Fritz
    and an American named Freddy are shopping at the same time on that same online
    shop. They see an item and only one is still in stock: a bronze, octopus-shaped
    coffee table. Disaster strikes, and communication between the two local servers
    is temporarily down. If you were the owner of the shop, you’d have two options:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以一个在欧洲有一个服务器在美国有一个服务器，并且有一个单一配送中心的在线商店为例。一个名叫Fritz的德国人和一个名叫Freddy的美国人同时在同一个在线商店购物。他们看到一件商品，只剩下一个库存：一个青铜色的八爪鱼形状的咖啡桌。灾难发生了，两个本地服务器之间的通信暂时中断。如果您是商店的所有者，您有两个选择：
- en: '***Availability*** —You allow the servers to keep on serving customers, and
    you sort out everything afterward.'
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可用性*** —您允许服务器继续为顾客提供服务，之后您再解决所有问题。'
- en: '***Consistency*** —You put all sales on hold until communication is reestablished.'
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***一致性*** —您将所有销售挂起，直到重新建立通信。'
- en: In the first case, Fritz and Freddy will both buy the octopus coffee table,
    because the last-known stock number for both nodes is “one” and both nodes are
    allowed to sell it, as shown in [figure 6.3](#ch06fig03).
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种情况下，Fritz和Freddy都会购买八爪鱼咖啡桌，因为两个节点最后已知的库存数量都是“一个”，并且两个节点都被允许出售它，如图6.3所示。
- en: 'Figure 6.3\. CAP Theorem: if nodes get disconnected, you can choose to remain
    available, but the data could become inconsistent.'
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.3\. CAP定理：如果节点断开连接，您可以选择保持可用性，但数据可能会变得不一致。
- en: '![](Images/06fig03_alt.jpg)'
  id: totrans-55
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig03_alt.jpg)'
- en: If the coffee table is hard to come by, you’ll have to inform either Fritz or
    Freddy that he won’t receive his table on the promised delivery date or, even
    worse, he will never receive it. As a good businessperson, you might compensate
    one of them with a discount coupon for a later purchase, and everything might
    be okay after all.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 如果咖啡桌很难找到，您必须通知Fritz或Freddy，他不会在承诺的交货日期收到他的桌子，或者更糟糕的是，他永远也收不到。作为一个优秀的商人，您可能会用一张折扣券补偿其中一个人，以便以后购买，这样一切可能都会好起来。
- en: The second option ([figure 6.4](#ch06fig04)) involves putting the incoming requests
    on hold temporarily.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择（[图6.4](#ch06fig04)）涉及暂时挂起传入的请求。
- en: 'Figure 6.4\. CAP Theorem: if nodes get disconnected, you can choose to remain
    consistent by stopping access to the databases until connections are restored'
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4\. CAP定理：如果节点断开连接，您可以选择在恢复连接之前停止对数据库的访问以保持一致性
- en: '![](Images/06fig04_alt.jpg)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig04_alt.jpg)'
- en: This might be fair to both Fritz and Freddy if after five minutes the web shop
    is open for business again, but then you might lose both sales and probably many
    more. Web shops tend to choose availability over consistency, but it’s not the
    optimal choice in all cases. Take a popular festival such as Tomorrowland. Festivals
    tend to have a maximum allowed capacity for safety reasons. If you sell more tickets
    than you’re allowed because your servers kept on selling during a node communication
    failure, you could sell double the number allowed by the time communications are
    reestablished. In such a case it might be wiser to go for consistency and turn
    off the nodes temporarily. A festival such as Tomorrowland is sold out in the
    first couple of hours anyway, so a little downtime won’t hurt as much as having
    to withdraw thousands of entry tickets.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这种做法可能对弗里茨和弗雷迪都公平，如果五分钟后网店重新开始营业，但你可能会失去这两笔销售，甚至更多。网店往往倾向于选择可用性而不是一致性，但这并不是所有情况下最佳的选择。以Tomorrowland这样的流行节日为例。出于安全考虑，节日通常会有一个最大允许容量。如果你因为服务器在节点通信故障期间继续售票而卖出了超出允许数量的门票，到通信恢复时，你可能已经卖出了允许数量的两倍。在这种情况下，选择一致性并暂时关闭节点可能更明智。像Tomorrowland这样的节日，门票通常在前几个小时就售罄了，所以短暂的停机不会像撤回数千张入场券那样造成太大的伤害。
- en: 6.1.3\. The BASE principles of NoSQL databases
  id: totrans-61
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.3. NoSQL数据库的BASE原则
- en: 'RDBMS follows the ACID principles; NoSQL databases that don’t follow ACID,
    such as the document stores and key-value stores, follow BASE. BASE is a set of
    much softer database promises:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 关系型数据库管理系统遵循ACID原则；不遵循ACID的NoSQL数据库，如文档存储和键值存储，遵循BASE。BASE是一组较为宽松的数据库承诺：
- en: '***Basically available*** —Availability is guaranteed in the CAP sense. Taking
    the web shop example, if a node is up and running, you can keep on shopping. Depending
    on how things are set up, nodes can take over from other nodes. Elasticsearch,
    for example, is a NoSQL document–type search engine that divides and replicates
    its data in such a way that node failure doesn’t necessarily mean service failure,
    via the process of *sharding*. Each *shard* can be seen as an individual database
    server instance, but is also capable of communicating with the other shards to
    divide the workload as efficiently as possible ([figure 6.5](#ch06fig05)). Several
    shards can be present on a single node. If each shard has a replica on another
    node, node failure is easily remedied by re-dividing the work among the remaining
    nodes.'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***基本可用性*** —在CAP意义上保证了可用性。以网店为例，如果一个节点处于运行状态，你可以继续购物。根据设置的不同，节点可以接管其他节点。例如，Elasticsearch是一个NoSQL文档类型搜索引擎，它通过分片和复制数据，使得节点故障不一定会导致服务故障，通过分片的过程来实现。每个*分片*可以看作是一个独立的数据库服务器实例，但它也能够与其他分片通信，以尽可能高效地分配工作（[图6.5](#ch06fig05)）。一个节点上可以存在多个分片。如果每个分片在另一个节点上都有一个副本，那么节点故障可以通过在剩余节点之间重新分配工作来轻松解决。'
- en: 'Figure 6.5\. Sharding: each shard can function as a self-sufficient database,
    but they also work together as a whole. The example represents two nodes, each
    containing four shards: two main shards and two replicas. Failure of one node
    is backed up by the other.'
  id: totrans-64
  prefs:
  - PREF_IND
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5. 分片：每个分片可以作为一个自给自足的数据库运行，但它们也作为一个整体协同工作。示例表示两个节点，每个节点包含四个分片：两个主分片和两个副本。一个节点的故障可以通过另一个节点得到备份。
- en: '![](Images/06fig05.jpg)'
  id: totrans-65
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![图片](Images/06fig05.jpg)'
- en: '***Soft state*** —The state of a system might change over time. This corresponds
    to the *eventual consistency principle*: the system might have to change to make
    the data consistent again. In one node the data might say “A” and in the other
    it might say “B” because it was adapted. Later, at conflict resolution when the
    network is back online, it’s possible the “A” in the first node is replaced by
    “B.” Even though no one did anything to explicitly change “A” into “B,” it will
    take on this value as it becomes consistent with the other node.'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***软状态*** —系统的状态可能会随时间变化。这对应于*最终一致性原则*：系统可能需要改变以使数据再次一致。在一个节点中，数据可能说“A”，而在另一个节点中可能说“B”，因为它们已经进行了调整。稍后，在网络恢复在线时的冲突解决过程中，第一个节点中的“A”可能被替换为“B”。即使没有人明确地将“A”改为“B”，它也会随着与其他节点的数据一致而采取这个值。'
- en: '***Eventual consistency*** —The database will become consistent over time.
    In the web shop example, the table is sold twice, which results in data inconsistency.
    Once the connection between the individual nodes is reestablished, they’ll communicate
    and decide how to resolve it. This conflict can be resolved, for example, on a
    first-come, first-served basis or by preferring the customer who would incur the
    lowest transport cost. Databases come with default behavior, but given that there’s
    an actual business decision to make here, this behavior can be overwritten. Even
    if the connection is up and running, latencies might cause nodes to become inconsistent.
    Often, products are kept in an online shopping basket, but putting an item in
    a basket doesn’t lock it for other users. If Fritz beats Freddy to the checkout
    button, there’ll be a problem once Freddy goes to check out. This can easily be
    explained to the customer: he was too late. But what if both press the checkout
    button at the exact same millisecond and both sales happen?'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**最终一致性** —数据库将随着时间的推移而变得一致。在网店示例中，表格被卖出了两次，这导致了数据不一致。一旦重新建立了各个节点之间的连接，它们将进行通信并决定如何解决冲突。这种冲突可以通过例如先到先得的方式或优先考虑运输成本最低的客户来解决。数据库具有默认行为，但鉴于这里有一个实际的业务决策需要做出，这种行为可以被覆盖。即使连接是正常运行的，延迟可能会导致节点变得不一致。通常，产品会被保存在在线购物车中，但将商品放入购物车并不会锁定它供其他用户使用。如果弗里茨比弗雷迪先按下结账按钮，那么当弗雷迪去结账时会出现问题。这可以很容易地向客户解释：他来得太晚了。但如果是两个人在精确的同一毫秒按下结账按钮，并且同时发生销售呢？'
- en: '|  |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**ACID versus BASE**'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '**ACID与BASE**'
- en: 'The BASE principles are somewhat contrived to fit *acid* and *base* from chemistry:
    an acid is a fluid with a low pH value. A base is the opposite and has a high
    pH value. We won’t go into the chemistry details here, but [figure 6.6](#ch06fig06)
    shows a mnemonic to those familiar with the chemistry equivalents of acid and
    base.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: BASE原则是为了适应化学中的*酸*和*碱*而设计的：酸是一种pH值低的液体。碱则相反，具有高pH值。我们在这里不会深入化学细节，但[图6.6](#ch06fig06)展示了对于那些熟悉酸和碱化学等效物的人来说的一个记忆法。
- en: 'Figure 6.6\. ACID versus BASE: traditional relational databases versus most
    NoSQL databases. The names are derived from the chemistry concept of the pH scale.
    A pH value below 7 is acidic; higher than 7 is a base. On this scale, your average
    surface water fluctuates between 6.5 and 8.5.'
  id: totrans-71
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. ACID与BASE：传统的关系型数据库与大多数NoSQL数据库。这些名称来源于pH值的化学概念。pH值低于7的是酸性；高于7的是碱性。在这个尺度上，平均地表水的pH值在6.5到8.5之间波动。
- en: '![](Images/06fig06_alt.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig06_alt.jpg)'
- en: '|  |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 6.1.4\. NoSQL database types
  id: totrans-74
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.4\. NoSQL数据库类型
- en: 'As you saw earlier, there are four big NoSQL types: key-value store, document
    store, column-oriented database, and graph database. Each type solves a problem
    that can’t be solved with relational databases. Actual implementations are often
    combinations of these. OrientDB, for example, is a *multi-model database*, combining
    NoSQL types. OrientDB is a graph database where each node is a document.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所见，有四种主要的NoSQL类型：键值存储、文档存储、列式数据库和图数据库。每种类型都解决了关系型数据库无法解决的问题。实际的实现通常是这些类型的组合。例如，OrientDB是一个*多模型数据库*，结合了NoSQL类型。OrientDB是一个图数据库，其中每个节点都是一个文档。
- en: 'Before going into the different NoSQL databases, let’s look at relational databases
    so you have something to compare them to. In data modeling, many approaches are
    possible. Relational databases generally strive toward *normalization*: making
    sure every piece of data is stored only once. Normalization marks their structural
    setup. If, for instance, you want to store data about a person and their hobbies,
    you can do so with two tables: one about the person and one about their hobbies.
    As you can see in [figure 6.7](#ch06fig07), an additional table is necessary to
    link hobbies to persons because of their *many-to-many relationship*: a person
    can have multiple hobbies and a hobby can have many persons practicing it.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入了解不同的NoSQL数据库之前，让我们看看关系型数据库，这样你就可以有所比较。在数据建模中，有许多可能的方法。关系型数据库通常追求*规范化*：确保每一条数据只存储一次。规范化标记了它们的结构设置。例如，如果你想存储关于一个人及其爱好数据，你可以使用两个表格：一个关于人的，一个关于他们的爱好。正如你在[图6.7](#ch06fig07)中看到的，由于*多对多关系*，需要额外的表格来将爱好与个人联系起来：一个人可以有多个爱好，一个爱好可以有很多人在练习。
- en: Figure 6.7\. Relational databases strive toward normalization (making sure every
    piece of data is stored only once). Each table has unique identifiers (primary
    keys) that are used to model the relationship between the entities (tables), hence
    the term relational.
  id: totrans-77
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7\. 关系型数据库追求规范化（确保每条数据只存储一次）。每个表都有唯一的标识符（主键），用于表示实体（表）之间的关系，因此被称为关系型。
- en: '![](Images/06fig07_alt.jpg)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig07_alt.jpg)'
- en: A full-scale relational database can be made up of many entities and linking
    tables. Now that you have something to compare NoSQL to, let’s look at the different
    types.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一个完整的关系型数据库可以由许多实体和链接表组成。现在你已经有了可以与NoSQL比较的东西，让我们来看看不同的类型。
- en: Column-oriented database
  id: totrans-80
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列式数据库
- en: Traditional relational databases are row-oriented, with each row having a row
    id and each field within the row stored together in a table. Let’s say, for example’s
    sake, that no extra data about hobbies is stored and you have only a single table
    to describe people, as shown in [figure 6.8](#ch06fig08). Notice how in this scenario
    you have slight denormalization because hobbies could be repeated. If the hobby
    information is a nice extra but not essential to your use case, adding it as a
    list within the Hobbies column is an acceptable approach. But if the information
    isn’t important enough for a separate table, should it be stored at all?
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 传统的关系型数据库是列式存储的，每行都有一个行ID，行内的每个字段都存储在表中。比如说，为了举例，我们不存储关于爱好额外的数据，并且只有一个表来描述人，如[图6.8](#ch06fig08)所示。注意在这个场景中，你会有轻微的反规范化，因为爱好可能会重复。如果爱好信息是一个很好的额外信息但不是你的用例所必需的，将其作为Hobbies列中的列表添加是一个可接受的方法。但如果信息不足以成为一个单独的表，那么是否应该存储它呢？
- en: Figure 6.8\. Row-oriented database layout. Every entity (person) is represented
    by a single row, spread over multiple columns.
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8\. 列式数据库布局。每个实体（人）由一行表示，分布在多个列中。
- en: '| Row ID | Name | Birthday | Hobbies |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '| 行ID | 姓名 | 生日 | 爱好 |'
- en: '| --- | --- | --- | --- |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 1 | Jos The Boss | 11-12-1985 | Archery, conquering the world |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '| 1 | Jos The Boss | 11-12-1985 | Archery, conquering the world |'
- en: '| 2 | Fritz von Braun | 27-1-1978 | Building things, surfing |'
  id: totrans-86
  prefs: []
  type: TYPE_TB
  zh: '| 2 | Fritz von Braun | 27-1-1978 | Building things, surfing |'
- en: '| 3 | Freddy Stark |   | Swordplay, lollygagging, archery |'
  id: totrans-87
  prefs: []
  type: TYPE_TB
  zh: '| 3 | Freddy Stark |   | Swordplay, lollygagging, archery |'
- en: '| 4 | Delphine Thewiseone | 16-9-1986 |   |'
  id: totrans-88
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Delphine Thewiseone | 16-9-1986 |   |'
- en: Every time you look up something in a row-oriented database, every row is scanned,
    regardless of which columns you require. Let’s say you only want a list of birthdays
    in September. The database will scan the table from top to bottom and left to
    right, as shown in [figure 6.9](#ch06fig09), eventually returning the list of
    birthdays.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 每次你在列式数据库中查找某项内容时，都会扫描每一行，无论你需要哪些列。比如说，你只想获取9月份的生日列表。数据库会从上到下、从左到右扫描整个表，如[图6.9](#ch06fig09)所示，最终返回生日列表。
- en: 'Figure 6.9\. Row-oriented lookup: from top to bottom and for every entry, all
    columns are taken into memory'
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9\. 列式查找：从上到下，对每个条目，所有列都被加载到内存中
- en: '![](Images/06fig09_alt.jpg)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig09_alt.jpg)'
- en: Indexing the data on certain columns can significantly improve lookup speed,
    but indexing every column brings extra overhead and the database is still scanning
    all the columns.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些列上索引数据可以显著提高查找速度，但为每个列索引都会带来额外的开销，数据库仍然会扫描所有列。
- en: Column databases store each column separately, allowing for quicker scans when
    only a small number of columns is involved; see [figure 6.10](#ch06fig10).
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列数据库将每个列分别存储，当只涉及少量列时，可以更快地进行扫描；参见[图6.10](#ch06fig10)。
- en: Figure 6.10\. Column-oriented databases store each column separately with the
    related row numbers. Every entity (person) is divided over multiple tables.
  id: totrans-94
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.10\. 列式数据库将每个列分别存储，并附带相关的行号。每个实体（人）分布在多个表中。
- en: '![](Images/06fig10_alt.jpg)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig10_alt.jpg)'
- en: This layout looks very similar to a row-oriented database with an index on every
    column. A database *index* is a data structure that allows for quick lookups on
    data at the cost of storage space and additional writes (index update). An index
    maps the row number to the data, whereas a column database maps the data to the
    row numbers; in that way counting becomes quicker, so it’s easy to see how many
    people like archery, for instance. Storing the columns separately also allows
    for optimized compression because there’s only one data type per table.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 这种布局看起来与每个列都有索引的行导向数据库非常相似。数据库*索引*是一种数据结构，它允许以存储空间和额外写入（索引更新）为代价快速查找数据。索引将行号映射到数据，而列数据库将数据映射到行号；这样计数就变得更快，例如，很容易看出有多少人喜欢射箭。将列分别存储还允许进行优化的压缩，因为每个表中只有一个数据类型。
- en: 'When should you use a row-oriented database and when should you use a column-oriented
    database? In a column-oriented database it’s easy to add another column because
    none of the existing columns are affected by it. But adding an entire record requires
    adapting all tables. This makes the row-oriented database preferable over the
    column-oriented database for online transaction processing (OLTP), because this
    implies adding or changing records constantly. The column-oriented database shines
    when performing analytics and reporting: summing values and counting entries.
    A row-oriented database is often the operational database of choice for actual
    transactions (such as sales). Overnight batch jobs bring the column-oriented database
    up to date, supporting lightning-speed lookups and aggregations using MapReduce
    algorithms for reports. Examples of column-family stores are Apache HBase, Facebook’s
    Cassandra, Hypertable, and the grandfather of wide-column stores, Google BigTable.'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在什么时候使用行导向数据库，什么时候使用列导向数据库？在列导向数据库中，很容易添加另一列，因为现有的任何列都不会受到影响。但添加整个记录需要调整所有表。这使得行导向数据库在在线事务处理（OLTP）方面优于列导向数据库，因为这意味着不断添加或更改记录。当执行分析和报告时，列导向数据库表现出色：求和值和计数条目。行导向数据库通常是实际交易（如销售）的操作数据库的选择。夜间批量作业使列导向数据库保持最新，支持使用MapReduce算法进行快速查找和聚合，以生成报告。列族存储的例子包括Apache
    HBase、Facebook的Cassandra、Hypertable以及宽列存储的鼻祖，Google BigTable。
- en: Key-value stores
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 键值存储
- en: Key-value stores are the least complex of the NoSQL databases. They are, as
    the name suggests, a collection of key-value pairs, as shown in [figure 6.11](#ch06fig11),
    and this simplicity makes them the most scalable of the NoSQL database types,
    capable of storing huge amounts of data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 键值存储是NoSQL数据库中最简单的。正如其名所示，它们是一系列键值对，如图6.11所示，这种简单性使它们成为NoSQL数据库类型中最可扩展的，能够存储大量数据。
- en: Figure 6.11\. Key-value stores store everything as a key and a value.
  id: totrans-100
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.11\. 键值存储将所有内容都存储为键和值。
- en: '![](Images/06fig11.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig11.jpg)'
- en: 'The value in a key-value store can be anything: a string, a number, but also
    an entire new set of key-value pairs encapsulated in an object. [Figure 6.12](#ch06fig12)
    shows a slightly more complex key-value structure. Examples of key-value stores
    are Redis, Voldemort, Riak, and Amazon’s Dynamo.'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 键值存储中的值可以是任何东西：一个字符串、一个数字，也可以是一个封装在对象中的整个新的键值对集合。[图6.12](#ch06fig12)显示了一个稍微复杂一些的键值结构。键值存储的例子有Redis、Voldemort、Riak和亚马逊的Dynamo。
- en: Figure 6.12\. Key-value nested structure
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.12\. 键值嵌套结构
- en: '![](Images/06fig12.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig12.jpg)'
- en: Document stores
  id: totrans-105
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 文档存储
- en: 'Document stores are one step up in complexity from key-value stores: a document
    store does assume a certain document structure that can be specified with a schema.
    Document stores appear the most natural among the NoSQL database types because
    they’re designed to store everyday documents as is, and they allow for complex
    querying and calculations on this often already aggregated form of data. The way
    things are stored in a relational database makes sense from a normalization point
    of view: everything should be stored only once and connected via foreign keys.
    Document stores care little about normalization as long as the data is in a structure
    that makes sense. A relational data model doesn’t always fit well with certain
    business cases. Newspapers or magazines, for example, contain articles. To store
    these in a relational database, you need to chop them up first: the article text
    goes in one table, the author and all the information about the author in another,
    and comments on the article when published on a website go in yet another. As
    shown in [figure 6.13](#ch06fig13), a newspaper article can also be stored as
    a single entity; this lowers the cognitive burden of working with the data for
    those used to seeing articles all the time. Examples of document stores are MongoDB
    and CouchDB.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 文档存储在键值存储之上增加了一步复杂性：文档存储确实假设了一种可以由模式指定的文档结构。文档存储在NoSQL数据库类型中看起来最为自然，因为它们被设计为以原始形式存储日常文档，并允许对这种通常已经聚合的数据形式进行复杂的查询和计算。关系数据库中存储数据的方式从规范化的角度来看是有意义的：一切都应该只存储一次，并通过外键连接。只要数据结构合理，文档存储对规范化的关注就很少。关系数据模型并不总是很好地与某些业务案例相匹配。例如，报纸或杂志包含文章。为了在关系数据库中存储这些文章，您需要首先将它们拆分：文章文本放在一个表中，作者及其所有信息放在另一个表中，当文章在网站上发布时，评论放在另一个表中。如图6.13所示，报纸文章也可以作为一个单一实体存储；这降低了那些习惯于经常看到文章的人处理数据时的认知负担。文档存储的例子包括MongoDB和CouchDB。
- en: Figure 6.13\. Document stores save documents as a whole, whereas an RDMS cuts
    up the article and saves it in several tables. The example was taken from the
    *Guardian* website.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.13。文档存储将整个文档保存下来，而关系型数据库管理系统（RDMS）会将文章拆分成几个表来保存。示例取自《卫报》网站。
- en: '![](Images/06fig13_alt.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig13_alt.jpg)'
- en: Graph databases
  id: totrans-109
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图数据库
- en: 'The last big NoSQL database type is the most complex one, geared toward storing
    relations between entities in an efficient manner. When the data is highly interconnected,
    such as for social networks, scientific paper citations, or capital asset clusters,
    graph databases are the answer. Graph or network data has two main components:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一种大型NoSQL数据库类型是最复杂的，旨在以高效的方式存储实体之间的关系。当数据高度互联时，例如社交网络、科学论文引用或资本资产集群，图数据库就是解决方案。图或网络数据有两个主要组成部分：
- en: '***Node*** —The entities themselves. In a social network this could be people.'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***节点*** — 实体本身。在社交网络中，这可能是指人。'
- en: '***Edge*** —The relationship between two entities. This relationship is represented
    by a line and has its own properties. An edge can have a direction, for example,
    if the arrow indicates who is whose boss.'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***边*** — 两个实体之间的关系。这种关系由一条线表示，并具有自己的属性。边可以有方向，例如，如果箭头表示谁是老板。'
- en: Graphs can become incredibly complex given enough relation and entity types.
    [Figure 6.14](#ch06fig14) already shows that complexity with only a limited number
    of entities. Graph databases like Neo4j also claim to uphold ACID, whereas document
    stores and key-value stores adhere to BASE.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 在关系和实体类型足够多的情况下，图可以变得极其复杂。[图6.14](#ch06fig14) 已经展示了仅使用有限数量的实体所展现的复杂性。像Neo4j这样的图数据库也声称遵守ACID，而文档存储和键值存储则遵循BASE。
- en: Figure 6.14\. Graph data example with four entity types (person, hobby, company,
    and furniture) and their relations without extra edge or node information
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.14。包含四种实体类型（人、爱好、公司和家具）及其关系的图数据示例，没有额外的边或节点信息
- en: '![](Images/06fig14_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig14_alt.jpg)'
- en: The possibilities are endless, and because the world is becoming increasingly
    interconnected, graph databases are likely to win terrain over the other types,
    including the still-dominant relational database. A ranking of the most popular
    databases and how they’re progressing can be found at [http://db-engines.com/en/ranking](http://db-engines.com/en/ranking).
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 可能性是无限的，由于世界正变得越来越互联，图数据库很可能会在其他类型数据库（包括仍然占主导地位的关系数据库）中赢得更多的市场份额。最流行的数据库排名及其进展情况可以在[http://db-engines.com/en/ranking](http://db-engines.com/en/ranking)找到。
- en: '[Figure 6.15](#ch06fig15) shows that with 9 entries, relational databases still
    dominate the top 15 at the time this book was written, and with the coming of
    NewSQL we can’t count them out yet. Neo4j, the most popular graph database, can
    be found at position 23 at the time of writing, with Titan at position 53.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.15](#ch06fig15) 显示，在本书编写时，尽管有9个条目，关系型数据库仍然占据着前15名的位置，而随着NewSQL的到来，我们也不能排除它们。最受欢迎的图数据库Neo4j在写作时位于第23位，Titan位于第53位。'
- en: Figure 6.15\. Top 15 databases ranked by popularity according to DB-Engines.com
    in March 2015
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.15\. 根据DB-Engines.com在2015年3月的数据，按受欢迎程度排名的前15个数据库
- en: '![](Images/06fig15_alt.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig15_alt.jpg)'
- en: Now that you’ve seen each of the NoSQL database types, it’s time to get your
    hands dirty with one of them.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经了解了每种NoSQL数据库类型，是时候动手实践其中之一了。
- en: '6.2\. Case study: What disease is that?'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2\. 案例研究：这是什么病？
- en: 'It has happened to many of us: you have sudden medical symptoms and the first
    thing you do is Google what disease the symptoms might indicate; then you decide
    whether it’s worth seeing a doctor. A web search engine is okay for this, but
    a more dedicated database would be better. Databases like this exist and are fairly
    advanced; they can be almost a virtual version of Dr. House, a brilliant diagnostician
    in the TV series *House M.D*. But they’re built upon well-protected data and not
    all of it is accessible by the public. Also, although big pharmaceutical companies
    and advanced hospitals have access to these virtual doctors, many general practitioners
    are still stuck with only their books. This information and resource asymmetry
    is not only sad and dangerous, it needn’t be there at all. If a simple, disease-specific
    search engine were used by all general practitioners in the world, many medical
    mistakes could be avoided.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 这对我们中的许多人来说都发生过：你突然出现医疗症状，首先做的事情就是用谷歌搜索这些症状可能表明的疾病；然后你决定是否值得去看医生。一个网络搜索引擎对于这个任务来说是可以的，但一个更专业的数据库会更好。这样的数据库确实存在，并且相当先进；它们几乎可以成为电视剧《豪斯医生》中那位杰出诊断专家豪斯医生的虚拟版本。但它们建立在受良好保护的数据之上，并非所有数据都对公众开放。此外，尽管大型制药公司和先进医院可以访问这些虚拟医生，但许多普通开业医生仍然只能依靠他们的书籍。这种信息和资源的不对称不仅令人悲哀且危险，实际上根本不需要存在。如果世界上所有普通开业医生都使用一个简单的、针对特定疾病的搜索引擎，许多医疗错误本可以避免。
- en: 'In this case study, you’ll learn how to build such a search engine here, albeit
    using only a fraction of the medical data that is freely accessible. To tackle
    the problem, you’ll use a modern NoSQL database called Elasticsearch to store
    the data, and the data science process to work with the data and turn it into
    a resource that’s fast and easy to search. Here’s how you’ll apply the process:'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，你将学习如何构建这样一个搜索引擎，尽管只使用了可自由获取的医疗数据的一小部分。为了解决这个问题，你将使用名为Elasticsearch的现代NoSQL数据库来存储数据，并使用数据科学流程来处理数据，将其转化为快速且易于搜索的资源。以下是应用此流程的方法：
- en: '**1**.  *Setting the research goal.*'
  id: totrans-124
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *设定研究目标。*'
- en: '**2**.  *Data collection*—You’ll get your data from Wikipedia. There are more
    sources out there, but for demonstration purposes a single one will do.'
  id: totrans-125
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *数据收集*——你将从维基百科获取数据。还有更多来源，但为了演示目的，一个就足够了。'
- en: '**3**.  *Data preparation*—The Wikipedia data might not be perfect in its current
    format. You’ll apply a few techniques to change this.'
  id: totrans-126
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  *数据准备*——维基百科数据在其当前格式下可能并不完美。你将应用一些技术来改变这一点。'
- en: '**4**.  *Data exploration*—Your use case is special in that step 4 of the data
    science process is also the desired end result: you want your data to become easy
    to explore.'
  id: totrans-127
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  *数据探索*——你的用例在数据科学流程的第4步也是期望的最终结果：你希望你的数据变得易于探索。'
- en: '**5**.  *Data modeling*—No real data modeling is applied in this chapter. Document-term
    matrices that are used for search are often the starting point for advanced topic
    modeling. We won’t go into that here.'
  id: totrans-128
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  *数据建模*——在本章中未应用实际的数据建模。用于搜索的文档-词矩阵通常是高级主题建模的起点。我们在这里不会深入探讨这一点。'
- en: '**6**.  *Presenting results*—To make data searchable, you’d need a user interface
    such as a website where people can query and retrieve disease information. In
    this chapter you won’t go so far as to build an actual interface. Your secondary
    goal: profiling a disease category by its keywords; you’ll reach this stage of
    the data science process because you’ll present it as a word cloud, such as the
    one in [figure 6.16](#ch06fig16).'
  id: totrans-129
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**6**.  *展示结果*——为了使数据可搜索，你需要一个用户界面，例如一个网站，人们可以在其中查询和检索疾病信息。在本章中，你不会构建一个实际的界面。你的次要目标：通过关键词分析疾病类别；你将达到数据科学过程的这一阶段，因为你将把它展示为一个词云，如图
    6.16 所示。'
- en: ''
  id: totrans-130
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Figure 6.16\. A sample word cloud on non-weighted diabetes keywords
  id: totrans-131
  prefs:
  - PREF_BQ
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.16\. 非加权糖尿病关键词的示例词云
- en: ''
  id: totrans-132
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '![](Images/06fig16.jpg)'
  id: totrans-133
  prefs:
  - PREF_BQ
  type: TYPE_IMG
  zh: '![](Images/06fig16.jpg)'
- en: 'To follow along with the code, you’ll need these items:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 要跟随代码进行操作，你需要以下这些物品：
- en: A Python session with the elasticsearch-py and Wikipedia libraries installed
    (`pip install elasticsearch` and `pip install wikipedia`)
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 安装了 elasticsearch-py 和 Wikipedia 库的 Python 会话（`pip install elasticsearch` 和
    `pip install wikipedia`）
- en: A locally set up Elasticsearch instance; see [appendix A](kindle_split_018.xhtml#app01)
    for installation instructions
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 本地设置的 Elasticsearch 实例；有关安装说明，请参阅附录 A（kindle_split_018.xhtml#app01）。
- en: The IPython library
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IPython 库
- en: '|  |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-139
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The code for this chapter is available to download from the Manning website
    for this book at [https://manning.com/books/introducing-data-science](https://manning.com/books/introducing-data-science)
    and is in IPython format.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的代码可以从本书的 Manning 网站下载，链接为 [https://manning.com/books/introducing-data-science](https://manning.com/books/introducing-data-science)，代码格式为
    IPython。
- en: '|  |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Elasticsearch: the open source search engine/NoSQL database**'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch：开源搜索引擎/NoSQL 数据库**'
- en: To tackle the problem at hand, diagnosing a disease, the NoSQL database you’ll
    use is Elasticsearch. Like MongoDB, Elasticsearch is a document store. But unlike
    MongoDB, Elasticsearch is a search engine. Whereas MongoDB is great at performing
    complex calculations and MapReduce jobs, Elasticsearch’s main purpose is full-text
    search. Elasticsearch will do basic calculations on indexed numerical data such
    as summing, counts, median, mean, standard deviation, and so on, but in essence
    it remains a search engine.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决当前的问题，即诊断疾病，你将使用的 NoSQL 数据库是 Elasticsearch。与 MongoDB 类似，Elasticsearch 是一个文档存储库。但与
    MongoDB 不同，Elasticsearch 是一个搜索引擎。MongoDB 在执行复杂计算和 MapReduce 作业方面非常出色，而 Elasticsearch
    的主要目的是全文搜索。Elasticsearch 将对索引的数值数据进行基本计算，例如求和、计数、中位数、平均值、标准差等，但本质上它仍然是一个搜索引擎。
- en: Elasticsearch is built on top of Apache Lucene, the Apache search engine created
    in 1999\. Lucene is notoriously hard to handle and is more a building block for
    more user-friendly applications than an end–to–end solution in itself. But Lucene
    is an enormously powerful search engine, and Apache Solr followed in 2004, opening
    for public use in 2006\. Solr (an open source, enterprise search platform) is
    built on top of Apache Lucene and is at this moment still the most versatile and
    popular open source search engine. Solr is a great platform and worth investigating
    if you get involved in a project requiring a search engine. In 2010 Elasticsearch
    emerged, quickly gaining in popularity. Although Solr can still be difficult to
    set up and configure, even for small projects, Elasticsearch couldn’t be easier.
    Solr still has an advantage in the number of possible plugins expanding its core
    functionality, but Elasticsearch is quickly catching up and today its capabilities
    are of comparable quality.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 是建立在 Apache Lucene 之上的，Apache Lucene 是 1999 年创建的 Apache 搜索引擎。Lucene
    以难以处理而闻名，它更多的是为更用户友好的应用程序提供构建块，而不是作为一个端到端解决方案本身。但 Lucene 是一个功能强大的搜索引擎，2004 年推出了
    Apache Solr，2006 年向公众开放。Solr（一个开源的企业级搜索平台）建立在 Apache Lucene 之上，目前仍然是功能最全面和最受欢迎的开源搜索引擎。Solr
    是一个优秀的平台，如果你参与了一个需要搜索引擎的项目，值得调查。2010 年 Elasticsearch 出现，迅速获得了人气。尽管 Solr 对于小型项目来说仍然可能难以设置和配置，但
    Elasticsearch 的设置却非常简单。Solr 在扩展其核心功能方面具有优势，但 Elasticsearch 正在迅速追赶，今天其功能质量相当。
- en: '|  |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '6.2.1\. Step 1: Setting the research goal'
  id: totrans-147
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1\. 第一步：设定研究目标
- en: Can you diagnose a disease by the end of this chapter, using nothing but your
    own home computer and the free software and data out there? Knowing what you want
    to do and how to do it is the first step in the data science process, as shown
    in [figure 6.17](#ch06fig17).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 你能在本章结束时仅使用自己的家用电脑和免费软件及数据来诊断疾病吗？知道你想要做什么以及如何去做是数据科学过程中的第一步，如图 6.17 所示。
- en: 'Figure 6.17\. Step 1 in the data science process: setting the research goal'
  id: totrans-149
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.17。数据科学流程步骤1：设定研究目标
- en: '![](Images/06fig17_alt.jpg)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig17_alt.jpg)'
- en: Your primary goal is to set up a disease search engine that would help general
    practitioners in diagnosing diseases.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的主要目标是建立一个疾病搜索引擎，帮助普通医生诊断疾病。
- en: 'Your secondary goal is to profile a disease: What keywords distinguish it from
    other diseases?'
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的次要目标是描述一种疾病：哪些关键词可以将其与其他疾病区分开来？
- en: This secondary goal is useful for educational purposes or as input to more advanced
    uses such as detecting spreading epidemics by tapping into social media. With
    your research goal and a plan of action defined, let’s move on to the data retrieval
    step.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这个次要目标在教育目的或作为更高级用途的输入（例如，通过社交媒体检测传播的流行病）很有用。在确定了研究目标和行动计划后，让我们继续到数据检索步骤。
- en: '6.2.2\. Steps 2 and 3: Data retrieval and preparation'
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2。步骤2和3：数据检索和准备
- en: Data retrieval and data preparation are two distinct steps in the data science
    process, and even though this remains true for the case study, we’ll explore both
    in the same section. This way you can avoid setting up local intermedia storage
    and immediately do data preparation while the data is being retrieved. Let’s look
    at where we are in the data science process (see [figure 6.18](#ch06fig18)).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检索和数据准备是数据科学流程中的两个不同步骤，尽管这在案例研究中仍然成立，但我们将在这同一节中探讨这两个步骤。这样，你可以在检索数据的同时立即进行数据准备，避免设置本地中间存储。让我们看看我们在数据科学流程中的位置（见[图6.18](#ch06fig18)）。
- en: 'Figure 6.18\. Data science process step 2: data retrieval. In this case there’s
    no internal data; all data will be fetched from Wikipedia.'
  id: totrans-156
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.18。数据科学流程步骤2：数据检索。在这种情况下，没有内部数据；所有数据都将从维基百科获取。
- en: '![](Images/06fig18_alt.jpg)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig18_alt.jpg)'
- en: 'As shown in [figure 6.18](#ch06fig18) you have two possible sources: internal
    data and external data.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.18](#ch06fig18)所示，你有两个可能的数据来源：内部数据和外部数据。
- en: '***Internal data*** —You have no disease information lying around. If you currently
    work for a pharmaceutical company or a hospital, you might be luckier.'
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***内部数据*** —你周围没有疾病信息。如果你目前为制药公司或医院工作，你可能更幸运。'
- en: '***External data*** —All you can use for this case is external data. You have
    several possibilities, but you’ll go with Wikipedia.'
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***外部数据*** —在这种情况下，你可以使用的外部数据仅限于外部数据。你有几个选择，但你会选择维基百科。'
- en: When you pull the data from Wikipedia, you’ll need to store it in your local
    Elasticsearch index, but before you do that you’ll need to prepare the data. Once
    data has entered the Elasticsearch index, it can’t be altered; all you can do
    then is query it. Look at the data preparation overview in [figure 6.19](#ch06fig19).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 当你从维基百科拉取数据时，你需要将其存储在你的本地Elasticsearch索引中，但在你这样做之前，你需要准备数据。一旦数据进入Elasticsearch索引，就不能更改；那时你能做的只是查询它。看看[图6.19](#ch06fig19)中的数据准备概述。
- en: 'Figure 6.19\. Data science process step 3: data preparation'
  id: totrans-162
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.19。数据科学流程步骤3：数据准备
- en: '![](Images/06fig19_alt.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig19_alt.jpg)'
- en: 'As shown in [figure 6.19](#ch06fig19) there are three distinct categories of
    data preparation to consider:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.19](#ch06fig19)所示，需要考虑三种不同的数据准备类别：
- en: '***Data cleansing*** —The data you’ll pull from Wikipedia can be incomplete
    or erroneous. Data entry errors and spelling mistakes are possible—even false
    information isn’t excluded. Luckily, you don’t need the list of diseases to be
    exhaustive, and you can handle spelling mistakes at search time; more on that
    later. Thanks to the Wikipedia Python library, the textual data you’ll receive
    is fairly clean already. If you were to scrape it manually, you’d need to add
    HTML cleaning, removing all HTML tags. The truth of the matter is full-text search
    tends to be fairly robust toward common errors such as incorrect values. Even
    if you dumped in HTML tags on purpose, they’d be unlikely to influence the results;
    the HTML tags are too different from normal language to interfere.'
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据清洗*** —从维基百科获取的数据可能是不完整或错误的。数据输入错误和拼写错误是可能的——甚至不包括错误信息。幸运的是，你不需要疾病列表详尽无遗，你可以在搜索时处理拼写错误；关于这一点，稍后会有更多说明。多亏了维基百科Python库，你将收到的文本数据已经相当干净。如果你要手动抓取，你需要添加HTML清理，移除所有HTML标签。实际上，全文搜索通常对常见错误如值错误相当稳健。即使你故意输入HTML标签，它们也不太可能影响结果；HTML标签与正常语言太不同，不会干扰。'
- en: '***Data transformation*** —You don’t need to transform the data much at this
    point; you want to search it as is. But you’ll make the distinction between page
    title, disease name, and page body. This distinction is almost mandatory for search
    result interpretation.'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据转换*** —在这个阶段，您不需要转换太多数据；您希望以原始形式搜索它。但您将区分页面标题、疾病名称和页面正文。这种区分对于搜索结果解释几乎是强制性的。'
- en: '***Combining data*** —All the data is drawn from a single source in this case,
    so you have no real need to combine data. A possible extension to this exercise
    would be to get disease data from another source and match the diseases. This
    is no trivial task because no unique identifier is present and the names are often
    slightly different.'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***数据合并*** —在这种情况下，所有数据都来自单一来源，因此您实际上没有合并数据的真正需要。这个练习的一个可能的扩展是从另一个来源获取疾病数据并匹配疾病。这是一个非同小可的任务，因为没有唯一的标识符，而且名称通常略有不同。'
- en: 'You can do data cleansing at only two stages: when using the Python program
    that connects Wikipedia to Elasticsearch and when running the Elasticsearch internal
    indexing system:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 您只能在两个阶段进行数据清理：当使用连接维基百科和Elasticsearch的Python程序时，以及当运行Elasticsearch内部索引系统时：
- en: '***Python*** —Here you define what data you’ll allow to be stored by your document
    store, but you won’t clean the data or transform the data at this stage, because
    Elasticsearch is better at it for less effort.'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Python*** —在这里，您定义您的文档存储将允许存储哪些数据，但在这个阶段，您不会清理或转换数据，因为Elasticsearch在这方面做得更好，而且更省力。'
- en: '***Elasticsearch*** —Elasticsearch will handle the data manipulation (creating
    the index) under the hood. You can still influence this process, and you’ll do
    so more explicitly later in this chapter.'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***Elasticsearch*** —Elasticsearch将在幕后处理数据操作（创建索引）。您仍然可以影响这个过程，您将在本章的后面更明确地这样做。'
- en: 'Now that you have an overview of the steps to come, let’s get to work. If you
    followed the instructions in the appendix, you should now have a local instance
    of Elasticsearch up and running. First comes data retrieval: you need information
    on the different diseases. You have several ways to get that kind of data. You
    could ask companies for their data or get data from Freebase or other open and
    free data sources. Acquiring your data can be a challenge, but for this example
    you’ll be pulling it from Wikipedia. This is a bit ironic because searches on
    the Wikipedia website itself are handled by Elasticsearch. Wikipedia used to have
    its own system build on top of Apache Lucene, but it became unmaintainable, and
    as of January 2014 Wikipedia began using Elasticsearch instead.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了即将到来的步骤概述，让我们开始工作。如果您遵循了附录中的说明，您现在应该有一个本地运行的Elasticsearch实例。首先进行数据检索：您需要有关不同疾病的信息。您有几种获取这种数据的方式。您可以向公司索求数据，或从Freebase或其他开放和免费数据源获取数据。获取数据可能是一个挑战，但在这个例子中，您将从维基百科获取数据。这有点讽刺，因为维基百科网站本身的搜索是由Elasticsearch处理的。维基百科曾经拥有一个基于Apache
    Lucene构建的自有系统，但这个系统变得难以维护，截至2014年1月，维基百科开始使用Elasticsearch。
- en: Wikipedia has a Lists of diseases page, as shown in [figure 6.20](#ch06fig20).
    From here you can borrow the data from the alphabetical lists.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 维基百科有一个疾病列表页面，如图6.20所示。[图6.20](#ch06fig20)。从这里，您可以借用字母顺序列表中的数据。
- en: Figure 6.20\. Wikipedia’s Lists of diseases page, the starting point for your
    data retrieval
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.20\. 维基百科的疾病列表页面，您数据检索的起点
- en: '![](Images/06fig20_alt.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片6.20的替代文本](Images/06fig20_alt.jpg)'
- en: You know what data you want; now go grab it. You could download the entire Wikipedia
    data dump. If you want to, you can download it to [http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki](http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 您知道您想要什么数据；现在去获取它。您可以下载整个维基百科数据存档。如果您愿意，您可以将其下载到[http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki](http://meta.wikimedia.org/wiki/Data_dump_torrents#enwiki)。
- en: Of course, if you were to index the entire Wikipedia, the index would end up
    requiring about 40 GB of storage. Feel free to use this solution, but for the
    sake of preserving storage and bandwidth, we’ll limit ourselves in this book to
    pulling only the data we intend to use. Another option is scraping the pages you
    require. Like Google, you can make a program crawl through the pages and retrieve
    the entire rendered HTML. This would do the trick, but you’d end up with the actual
    HTML, so you’d need to clean that up before indexing it. Also, unless you’re Google,
    websites aren’t too fond of crawlers scraping their web pages. This creates an
    unnecessarily high amount of traffic, and if enough people send crawlers, it can
    bring the HTTP server to its knees, spoiling the fun for everyone. Sending billions
    of requests at the same time is also one of the ways denial of service (DoA) attacks
    are performed. If you do need to scrape a website, script in a time gap between
    each page request. This way, your scraper more closely mimics the behavior of
    a regular website visitor and you won’t blow up their servers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，如果你要索引整个维基百科，索引最终可能需要大约 40 GB 的存储空间。你可以自由使用这个解决方案，但为了节省存储和带宽，我们在这本书中只提取我们打算使用的那些数据。另一种选择是抓取所需的页面。像
    Google 一样，你可以编写一个程序在页面之间进行爬取并检索整个渲染的 HTML。这将有效，但你最终会得到实际的 HTML，因此你需要在索引之前清理它。此外，除非你是
    Google，否则网站通常不太喜欢爬虫抓取它们的网页。这会创建不必要的流量，如果足够多的人发送爬虫，它可能会使 HTTP 服务器崩溃，让所有人的乐趣都受到影响。同时发送数十亿个请求也是执行拒绝服务（DoA）攻击的一种方式。如果你确实需要抓取网站，请在每个页面请求之间留出时间间隔。这样，你的爬虫会更接近普通网站访问者的行为，你也不会使他们的服务器崩溃。
- en: Luckily, the creators of Wikipedia are smart enough to know that this is exactly
    what would happen with all this information open to everyone. They’ve put an API
    in place from which you can safely draw your information. You can read more about
    it at [http://www.mediawiki.org/wiki/API:Main_page](http://www.mediawiki.org/wiki/API:Main_page).
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，维基百科的创建者足够聪明，知道当所有这些信息对每个人开放时会发生什么。他们已经建立了一个 API，你可以从中安全地获取信息。你可以在[http://www.mediawiki.org/wiki/API:Main_page](http://www.mediawiki.org/wiki/API:Main_page)了解更多相关信息。
- en: 'You’ll draw from the API. And Python wouldn’t be Python if it didn’t already
    have a library to do the job. There are several actually, but the easiest one
    will suffice for your needs: Wikipedia.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从 API 中获取数据。Python 如果没有现成的库来完成这项工作，那就不是 Python 了。实际上有几个库，但最简单的一个就足以满足你的需求：维基百科。
- en: 'Activate your Python virtual environment and install all the libraries you’ll
    need for the rest of the book:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 激活你的 Python 虚拟环境，并安装本书剩余部分所需的所有库：
- en: '[PRE0]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You’ll use Wikipedia to tap into Wikipedia. Elasticsearch is the main Elasticsearch
    Python library; with it you can communicate with your database.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 你将使用维基百科来访问维基百科。Elasticsearch 是主要的 Elasticsearch Python 库；通过它，你可以与你的数据库进行通信。
- en: 'Open your favorite Python interpreter and import the necessary libraries:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你喜欢的 Python 解释器并导入必要的库：
- en: '[PRE1]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: You’re going to draw data from the Wikipedia API and at the same time index
    on your local Elasticsearch instance, so first you need to prepare it for data
    acceptance.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 你将从维基百科 API 中获取数据，同时在你的本地 Elasticsearch 实例上进行索引，因此首先你需要为数据接受做好准备。
- en: '![](Images/170fig01_alt.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/170fig01_alt.jpg)'
- en: 'The first thing you need is a client. `Elasticsearch()` can be initialized
    with an address but the default is localhost:9200\. `Elasticsearch()` and `Elasticsearch
    (''localhost:9200'')` are thus the same thing: your client is connected to your
    local Elasticsearch node. Then you create an index named `"medical"`. If all goes
    well, you should see an `"acknowledged:true"` reply, as shown in [figure 6.21](#ch06fig21).'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先需要的是一个客户端。`Elasticsearch()` 可以用地址初始化，但默认是 localhost:9200。因此，`Elasticsearch()`
    和 `Elasticsearch ('localhost:9200')` 是同一件事：你的客户端连接到了你的本地 Elasticsearch 节点。然后你创建一个名为
    `"medical"` 的索引。如果一切顺利，你应该会看到一个 `"acknowledged:true"` 的回复，如图 6.21 所示。
- en: Figure 6.21\. Creating an Elasticsearch index with Python-Elasticsearch
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.21\. 使用 Python-Elasticsearch 创建 Elasticsearch 索引
- en: '![](Images/06fig21_alt.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig21_alt.jpg)'
- en: Elasticsearch claims to be schema-less, meaning you can use Elasticsearch without
    defining a database schema and without telling Elasticsearch what kind of data
    it needs to expect. Although this is true for simple cases, you can’t avoid having
    a schema in the long run, so let’s create one, as shown in the following listing.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 声称是无模式的，这意味着您可以在不定义数据库模式也不告诉 Elasticsearch 需要期望什么类型的数据的情况下使用 Elasticsearch。尽管这在简单情况下是正确的，但您最终无法避免拥有一个模式，所以让我们创建一个，如下所示。
- en: Listing 6.1\. Adding a mapping to the document type
  id: totrans-190
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1\. 向文档类型添加映射
- en: '![](Images/171fig01_alt.jpg)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/171fig01_alt.jpg)'
- en: 'This way you tell Elasticsearch that your index will have a document type called
    `"disease"`, and you supply it with the field type for each of the fields. You
    have three fields in a disease document: `name`, `title`, and `fulltext`, all
    of them of type `string`. If you hadn’t supplied the mapping, Elasticsearch would
    have guessed their types by looking at the first entry it received. If it didn’t
    recognize the field to be `boolean`, `double`, `float`, `long`, `integer`, or
    `date`, it would set it to `string`. In this case, you didn’t need to manually
    specify the mapping.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 这样您就告诉 Elasticsearch，您的索引将有一个名为 `"disease"` 的文档类型，并为每个字段提供了字段类型。疾病文档中有三个字段：`name`、`title`
    和 `fulltext`，它们都是 `string` 类型。如果您没有提供映射，Elasticsearch 会通过查看它接收到的第一个条目来猜测它们的类型。如果它没有识别出字段为
    `boolean`、`double`、`float`、`long`、`integer` 或 `date`，它就会将其设置为 `string`。在这种情况下，您不需要手动指定映射。
- en: 'Now let’s move on to Wikipedia. The first thing you want to do is fetch the
    List of diseases page, because this is your entry point for further exploration:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们转到维基百科。您首先想做的事情是获取疾病列表页面，因为这是您进一步探索的入口点：
- en: '[PRE2]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'You now have your first page, but you’re more interested in the listing pages
    because they contain links to the diseases. Check out the links:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在有了第一页，但您更感兴趣的是列表页面，因为它们包含指向疾病的链接。查看以下链接：
- en: '[PRE3]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The List of diseases page comes with more links than you’ll use. [Figure 6.22](#ch06fig22)
    shows the alphabetical lists starting at the sixteenth link.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 疾病列表页面包含的链接比您需要的要多。[图6.22](#ch06fig22) 展示了从第16个链接开始的字母列表。
- en: Figure 6.22\. Links on the Wikipedia page Lists of diseases. It has more links
    than you’ll need.
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.22\. 维基百科页面“疾病列表”上的链接。它包含的链接比您需要的多。
- en: '![](Images/06fig22_alt.jpg)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig22_alt.jpg)'
- en: '[PRE4]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'This page has a considerable array of links, but only the alphabetic lists
    interest you, so keep only those:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 这个页面有很多链接，但只有字母列表对您感兴趣，所以只保留那些：
- en: '[PRE5]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: You’ve probably noticed that the subset is hardcoded, because you know they’re
    the 16th to 43rd entries in the array. If Wikipedia were to add even a single
    link before the ones you’re interested in, it would throw off the results. A better
    practice would be to use regular expressions for this task. For exploration purposes,
    hardcoding the entry numbers is fine, but if regular expressions are second nature
    to you or you intend to turn this code into a batch job, regular expressions are
    recommended. You can find more information on them at [https://docs.python.org/2/howto/regex.html](https://docs.python.org/2/howto/regex.html).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能已经注意到子集是硬编码的，因为您知道它们是数组中的第16到第43个条目。如果维基百科在您感兴趣的链接之前添加任何链接，它就会影响结果。更好的做法是使用正则表达式来完成这项任务。出于探索目的，硬编码条目编号是可以的，但如果正则表达式对您来说很自然，或者您打算将此代码转换为批处理作业，建议使用正则表达式。您可以在[https://docs.python.org/2/howto/regex.html](https://docs.python.org/2/howto/regex.html)找到更多关于它们的信息。
- en: One possibility for a regex version would be the following code snippet.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 正则表达式版本的一个可能代码片段如下。
- en: '[PRE6]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '[Figure 6.23](#ch06fig23) shows the first entries of what you’re after: the
    diseases themselves.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.23](#ch06fig23) 展示了您所追求的第一批条目：疾病本身。'
- en: Figure 6.23\. First Wikipedia disease list, “list of diseases (0-9)”
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.23\. 第一批维基百科疾病列表，“疾病列表（0-9）”
- en: '![](Images/06fig23.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/06fig23.jpg)'
- en: '[PRE7]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It’s time to index the diseases. Once they’re indexed, both data entry and data
    preparation are effectively over, as shown in the following listing.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是索引疾病的时候了。一旦它们被索引，数据录入和数据准备就有效完成了，如下所示。
- en: Listing 6.2\. Indexing diseases from Wikipedia
  id: totrans-211
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.2\. 从维基百科索引疾病
- en: '![](Images/173fig01_alt.jpg)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/173fig01_alt.jpg)'
- en: 'Because each of the list pages will have links you don’t need, check to see
    if an entry is a disease. You indicate for each list what character the disease
    starts with, so you check for this. Additionally you exclude the links starting
    with “list” because these will pop up once you get to the L list of diseases.
    The check is rather naïve, but the cost of having a few unwanted entries is rather
    low because the search algorithms will exclude irrelevant results once you start
    querying. For each disease you index the disease name and the full text of the
    page. The name is also used as its index ID; this is useful for several advanced
    Elasticsearch features but also for quick lookup in the browser. For example,
    try this URL in your browser: http://localhost:9200/medical/diseases/11%20beta%20hydroxylase%20deficiency.
    The title is indexed separately; in most cases the link name and the page title
    will be identical and sometimes the title will contain an alternative name for
    the disease.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 因为每个列表页面都会有你不需要的链接，检查一个条目是否是疾病。你为每个列表指定疾病开始的字符，所以你检查这个。此外，你还排除了以“list”开头的链接，因为这些链接会在你到达疾病列表的
    L 时出现。这个检查相当简单，但由于搜索算法在开始查询时会排除无关结果，所以几个不想要的条目成本相当低。对于你索引的每个疾病，你将索引疾病名称和页面的全文。名称也用作其索引
    ID；这对于几个高级 Elasticsearch 功能很有用，但也可以用于浏览器中的快速查找。例如，在你的浏览器中尝试这个 URL：http://localhost:9200/medical/diseases/11%20beta%20hydroxylase%20deficiency。标题是单独索引的；在大多数情况下，链接名称和页面标题将是相同的，有时标题将包含疾病的替代名称。
- en: With at least a few diseases indexed it’s possible to make use of the Elasticsearch
    URI for simple lookups. Have a look at a full body search for the word *headache*
    in [figure 6.24](#ch06fig24). You can already do this while indexing; Elasticsearch
    can update an index and return queries for it at the same time.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 至少索引了几种疾病后，可以利用 Elasticsearch URI 进行简单的查找。看看[图 6.24](#ch06fig24)中对于单词 *headache*
    的全身搜索。你可以在索引的同时做这件事；Elasticsearch 可以同时更新索引并返回查询结果。
- en: Figure 6.24\. The Elasticsearch URL example buildup
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.24\. Elasticsearch URL 示例构建
- en: '![](Images/06fig24_alt.jpg)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6.24 替代](Images/06fig24_alt.jpg)'
- en: 'If you don’t query the index, you can still get a few results without knowing
    anything about the index. Specifying http://localhost:9200/medical/diseases/_search
    will return the first five results. For a more structured view on the data you
    can ask for the mapping of this document type at http://localhost:9200/medical/diseases/_mapping?pretty.
    The pretty `get` argument shows the returned JSON in a more readable format, as
    can be seen in [figure 6.25](#ch06fig25). The mapping does appear to be the way
    you specified it: all fields are type `string`.'
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你没有查询索引，你仍然可以在不知道索引的情况下获得一些结果。指定 http://localhost:9200/medical/diseases/_search
    将返回前五个结果。为了对数据进行更结构化的查看，你可以请求此文档类型的映射，在 http://localhost:9200/medical/diseases/_mapping?pretty。pretty
    `get` 参数以更易读的格式显示返回的 JSON，如[图 6.25](#ch06fig25)所示。映射似乎正是你指定的那样：所有字段类型都是 `string`。
- en: Figure 6.25\. Diseases document type mapping via Elasticsearch URL
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.25\. 通过 Elasticsearch URL 疾病文档类型映射
- en: '![](Images/06fig25.jpg)'
  id: totrans-219
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6.25](Images/06fig25.jpg)'
- en: The Elasticsearch URL is certainly useful, yet it won’t suffice for your needs.
    You still have diseases to diagnose, and for this you’ll send POST requests to
    Elasticsearch via your Elasticsearch Python library.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch URL 确实很有用，但不足以满足你的需求。你仍需要诊断疾病，为此你将通过 Elasticsearch Python 库向 Elasticsearch
    发送 POST 请求。
- en: With data retrieval and preparation accomplished, you can move on to exploring
    your data.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 数据检索和准备完成后，你可以继续探索你的数据。
- en: '6.2.3\. Step 4: Data exploration'
  id: totrans-222
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 步骤 4：数据探索
- en: It’s not lupus. It’s never lupus!
  id: totrans-223
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 不是狼疮。绝不是狼疮！
- en: ''
  id: totrans-224
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Dr. House of *House M.D.**'
  id: totrans-225
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*《豪斯医生》中的豪斯博士**'
- en: 'Data exploration is what marks this case study, because the primary goal of
    the project (disease diagnostics) is a specific way of exploring the data by querying
    for disease symptoms. [Figure 6.26](#ch06fig26) shows several data exploration
    techniques, but in this case it’s non-graphical: interpreting text search query
    results.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 数据探索是本案例研究的标志，因为项目的首要目标（疾病诊断）是通过查询疾病症状来探索数据的一种特定方式。[图 6.26](#ch06fig26)展示了几个数据探索技术，但在这个案例中是非图形化的：解释文本搜索查询结果。
- en: 'Figure 6.26\. Data science process step 4: data exploration'
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.26\. 数据科学流程步骤 4：数据探索
- en: '![](Images/06fig26_alt.jpg)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![图片 6.26 替代](Images/06fig26_alt.jpg)'
- en: 'The moment of truth is here: can you find certain diseases by feeding your
    search engine their symptoms? Let’s first make sure you have the basics up and
    running. Import the Elasticsearch library and define global search settings:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 真正的时刻到了：通过向搜索引擎提供症状，你能找到某些疾病吗？让我们首先确保你具备基本的知识并开始运行。导入 Elasticsearch 库并定义全局搜索设置：
- en: '[PRE8]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You’ll return only the first three results; the default is five.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 你将只返回前三个结果；默认为五个。
- en: 'Elasticsearch has an elaborate JSON query language; every search is a POST
    request to the server and will be answered with a JSON answer. Roughly, the language
    consists of three big parts: queries, filters, and aggregations. A *query* takes
    in search keywords and puts them through one or more analyzers before the words
    are looked up in the index. We’ll get deeper into analyzers a bit later in this
    chapter. A *filter* takes keywords like a query does but doesn’t try to analyze
    what you give it; it filters on the conditions we provide. Filters are thus less
    complex but many times more efficient because they’re also temporarily stored
    within Elasticsearch in case you use the same filter twice. *Aggregations* can
    be compared to the SQL group; buckets of words will be created, and for each bucket
    relevant statistics can be calculated. Each of these three compartments has loads
    of options and features, making elaborating on the entire language here impossible.
    Luckily, there’s no need to go into the complexity that Elasticsearch queries
    can represent. We’ll use the “Query string query language,” a way to query the
    data that closely resembles the Google search query language. If, for instance,
    you want a search term to be mandatory, you add a plus (+) sign; if you want to
    exclude the search term, you use a minus (-) sign. Querying Elasticsearch isn’t
    recommended because it decreases performance; the search engine first needs to
    translate the query string into its native JSON query language. But for your purposes
    it will work nicely; also, performance won’t be a factor on the several thousand
    records you have in your index. Now it’s time to query your disease data.'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: Elasticsearch 有一个复杂的 JSON 查询语言；每次搜索都是一个发送到服务器的 POST 请求，并将以 JSON 格式回答。大致来说，该语言由三个主要部分组成：查询、过滤和聚合。一个
    *查询* 会接收搜索关键词，并在索引中查找单词之前通过一个或多个分析器。我们将在本章稍后深入了解分析器。一个 *过滤* 与查询类似，但不会尝试分析你提供的内容；它根据我们提供的条件进行过滤。因此，过滤比查询更简单，但效率更高，因为它们也会在
    Elasticsearch 中临时存储，以防你两次使用相同的过滤条件。*聚合* 可以与 SQL 中的分组进行比较；将创建单词的桶，并为每个桶计算相关统计信息。这三个部分中的每一个都有大量的选项和功能，使得在这里详细阐述整个语言变得不可能。幸运的是，我们不需要深入了解
    Elasticsearch 查询可以表示的复杂性。我们将使用“查询字符串查询语言”，这是一种查询数据的方式，与 Google 搜索查询语言非常相似。例如，如果你想强制搜索项，你可以在前面添加一个加号（+）；如果你想排除搜索项，你使用减号（-）。不建议使用
    Elasticsearch 进行查询，因为它会降低性能；搜索引擎首先需要将查询字符串转换为它自己的原生 JSON 查询语言。但对你来说，它将工作得很好；此外，在索引中的几万条记录中，性能不会成为一个因素。现在，是时候查询你的疾病数据了。
- en: 'Project primary objective: diagnosing a disease by its symptoms'
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 项目主要目标：通过症状诊断疾病
- en: If you ever saw the popular television series *House M.D.*, the sentence “It’s
    never lupus” may sound familiar. Lupus is a type of autoimmune disease, where
    the body’s immune system attacks healthy parts of the body. Let’s see what symptoms
    your search engine would need to determine that you’re looking for lupus.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经看过流行的电视连续剧 *House M.D.*，句子“它绝不是狼疮”可能听起来很熟悉。狼疮是一种自身免疫性疾病，其中身体的免疫系统攻击身体的健康部分。让我们看看你的搜索引擎需要哪些症状来确定你正在寻找狼疮。
- en: 'Start off with three symptoms: fatigue, fever, and joint pain. Your imaginary
    patient has all three of them (and more), so make them all mandatory by adding
    a plus sign before each one:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 从三个症状开始：疲劳、发烧和关节痛。你的想象中的病人都有这三种症状（还有更多），所以通过在每个症状前添加一个加号使其都成为强制性的：
- en: Listing 6.3\. “simple query string” Elasticsearch query with three mandatory
    keywords
  id: totrans-236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. “简单查询字符串” Elasticsearch 查询包含三个强制关键词
- en: '![](Images/176fig01_alt.jpg)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/176fig01_alt.jpg)'
- en: 'In `searchBody`, which has a JSON structure, you specify the fields you’d like
    to see returned, in this case the name of the disease should suffice. You use
    the query string syntax to search in all the indexed fields: `fulltext`, `title`,
    and `name`. By adding `^` you can give each field a weight. If a symptom occurs
    in the title, it’s five times more important than in the open text; if it occurs
    in the name itself, it’s considered ten times as important. Notice how “joint
    pain” is wrapped in a pair of quotation marks. If you didn’t have the “” signs,
    *joint* and *pain* would have been considered as two separate keywords rather
    than a single phrase. In Elasticsearch this is called *phrase matching*. Let’s
    look at the results in [figure 6.27](#ch06fig27).'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在具有JSON结构的`searchBody`中，你指定你想要返回的字段，在这种情况下，疾病的名称就足够了。你使用查询字符串语法在所有索引字段中进行搜索：`fulltext`、`title`和`name`。通过添加`^`，你可以给每个字段赋予一个权重。如果一个症状出现在标题中，它比在开放文本中重要五倍；如果它出现在名称本身中，它被认为重要十倍。注意“关节痛”被一对引号包围。如果你没有引号，*关节*和*pain*将被视为两个单独的关键词，而不是一个短语。在Elasticsearch中，这被称为*短语匹配*。让我们看看[图6.27](#ch06fig27)中的结果。
- en: Figure 6.27\. Lupus first search with 34 results
  id: totrans-239
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.27\. 狼疮第一次搜索，共有34个结果
- en: '![](Images/06fig27_alt.jpg)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图6.27](Images/06fig27_alt.jpg)'
- en: '[Figure 6.27](#ch06fig27) shows the top three results returned out of 34 matching
    diseases. The results are sorted by their matching score, the variable `_score`.
    The matching score is no simple thing to explain; it takes into consideration
    how well the disease matches your query and how many times a keyword was found,
    the weights you gave, and so on. Currently, lupus doesn’t even show up in the
    top three results. Luckily for you, lupus has another distinct symptom: a rash.
    The rash doesn’t always show up on the person’s face, but it does happen and this
    is where lupus got its name: the face rash makes people vaguely resemble a wolf.
    Your patient has a rash but not the signature rash on the face, so add “rash”
    to the symptoms without mentioning the face.'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '![图6.27](#ch06fig27)显示了从34个匹配疾病中返回的前三个结果。结果按其匹配分数排序，即变量`_score`。匹配分数并不是一件简单的事情来解释；它考虑了疾病与你的查询的匹配程度，关键词出现的次数，你给出的权重等等。目前，狼疮甚至没有出现在前三名结果中。幸运的是，狼疮还有另一个独特的症状：皮疹。皮疹并不总是出现在人的脸上，但它确实会发生，这也是狼疮得名的原因：面部皮疹使人们略似狼。你的病人有皮疹，但没有面部特有的皮疹，所以将“皮疹”添加到症状中，而不提及其面部。'
- en: '[PRE9]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The results of the new search are shown in [figure 6.28](#ch06fig28).
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 新搜索的结果在[图6.28](#ch06fig28)中显示。
- en: Figure 6.28\. Lupus second search attempt with six results and lupus in the
    top three
  id: totrans-244
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.28\. 狼疮第二次搜索尝试，共有六个结果，狼疮位于前三名
- en: '![](Images/06fig28_alt.jpg)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图6.28](Images/06fig28_alt.jpg)'
- en: Now the results have been narrowed down to six and lupus is in the top three.
    At this point, the search engine says *Human Granulocytic Ehrlichiosis* (HGE)
    is more likely. HGE is a disease spread by ticks, like the infamous Lyme disease.
    By now a capable doctor would have already figured out which disease plagues your
    patient, because in determining diseases many factors are at play, more than you
    can feed into your humble search engine. For instance, the rash occurs only in
    10% of HGE and in 50% of lupus patients. Lupus emerges slowly, whereas HGE is
    set off by a tick bite. Advanced machine-learning databases fed with all this
    information in a more structured way could make a diagnosis with far greater certainty.
    Given that you need to make do with the Wikipedia pages, you need another symptom
    to confirm that it’s lupus. The patient experiences chest pain, so add this to
    the list.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 现在结果已经缩小到六个，狼疮位于前三名。此时，搜索引擎表示*人粒细胞无形体病*（HGE）的可能性更大。HGE是一种由蜱传播的疾病，就像臭名昭著的莱姆病一样。到目前为止，一个有能力的医生已经能够确定困扰你病人的疾病，因为在确定疾病时，许多因素都在发挥作用，远远超出了你那谦逊的搜索引擎所能处理的范围。例如，皮疹只出现在HGE的10%和狼疮患者的50%中。狼疮缓慢出现，而HGE是由蜱咬引起的。通过以更结构化的方式提供所有这些信息的先进机器学习数据库，可以以更高的确定性做出诊断。鉴于你需要依靠维基百科页面，你需要另一个症状来确认它是狼疮。病人有胸痛，所以将其添加到列表中。
- en: '[PRE10]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: The result is shown in [figure 6.29](#ch06fig29).
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 结果在[图6.29](#ch06fig29)中显示。
- en: 'Figure 6.29\. Lupus third search: with enough symptoms to determine it must
    be lupus'
  id: totrans-249
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.29\. 狼疮第三次搜索：有足够的症状来确定它一定是狼疮
- en: '![](Images/06fig29_alt.jpg)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图6.29](Images/06fig29_alt.jpg)'
- en: Seems like it’s lupus. It took a while to get to this conclusion, but you got
    there. Of course, you were limited in the way you presented Elasticsearch with
    the symptoms. You used only either single terms (“fatigue”) or literal phrases
    (“joint pain”). This worked out for this example, but Elasticsearch is more flexible
    than this. It can take regular expressions and do a fuzzy search, but that’s beyond
    the scope of this book, although a few examples are included in the downloadable
    code.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来是狼疮。得出这个结论花了一些时间，但你做到了。当然，你在向Elasticsearch展示症状的方式上受到了限制。你只使用了单个术语（“疲劳”）或直接短语（“关节疼痛”）。这个例子中这行得通，但Elasticsearch比这更灵活。它可以接受正则表达式并进行模糊搜索，但这超出了本书的范围，尽管在可下载的代码中包含了一些示例。
- en: 'Handling spelling mistakes: Damerau-Levenshtein'
  id: totrans-252
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 处理拼写错误：Damerau-Levenshtein
- en: 'Say someone typed “lupsu” instead of “lupus.” Spelling mistakes happen all
    the time and in all types of human-crafted documents. To deal with this data scientists
    often use Damerau-Levenshtein. The Damerau-Levenshtein distance between two strings
    is the number of operations required to turn one string into the other. Four operations
    are allowed to calculate the distance:'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 假设有人输入了“lupsu”而不是“lupus”。拼写错误在所有类型的人类文档中都会发生。为了处理这种情况，数据科学家通常会使用Damerau-Levenshtein。两个字符串之间的Damerau-Levenshtein距离是将一个字符串转换为另一个字符串所需的操作数。计算距离允许进行四种操作：
- en: '***Deletion*** —Delete a character from the string.'
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***删除*** —从字符串中删除一个字符。'
- en: '***Insertion*** —Add a character to the string.'
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***插入*** —向字符串中添加一个字符。'
- en: '***Substitution*** —Substitute one character for another. Without the substitution
    counted as one operation, changing one character into another would take two operations:
    one deletion and one insertion.'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***替换*** —用一个字符替换另一个字符。如果没有将替换计为一个操作，将一个字符更改为另一个字符将需要两个操作：一个删除和一个插入。'
- en: '***Transposition of two adjacent characters*** —Swap two adjacent characters.'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***两个相邻字符的置换*** —交换两个相邻字符。'
- en: This last operation (transposition) is what makes the difference between traditional
    Levenshtein distance and the Damerau-Levenshtein distance. It’s this last operation
    that makes our dyslexic spelling mistake fall within acceptable limits. Damerau-Levenshtein
    is forgiving of these transposition mistakes, which makes it great for search
    engines, but it’s also used for other things such as calculating the differences
    between DNA strings.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个操作（置换）是传统Levenshtein距离和Damerau-Levenshtein距离之间的区别。正是这个最后的操作使得我们的阅读障碍拼写错误在可接受的范围内。Damerau-Levenshtein对这种置换错误很宽容，这使得它在搜索引擎中非常出色，但它也被用于其他事情，例如计算DNA字符串之间的差异。
- en: '[Figure 6.30](#ch06fig30) shows how the transformation from “lupsu” to “lupus”
    is performed with a single transposition.'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6.30](#ch06fig30)展示了如何通过单个置换将“lupsu”转换为“lupus”。'
- en: Figure 6.30\. Adjacent character transposition is one of the operations in Damerau-Levenshtein
    distance. The other three are insertion, deletion, and substitution.
  id: totrans-260
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.30。相邻字符的置换是Damerau-Levenshtein距离中的操作之一。其他三个操作是插入、删除和替换。
- en: '![](Images/06fig30.jpg)'
  id: totrans-261
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig30.jpg)'
- en: 'With just this you’ve achieved your first objective: *diagnosing a disease*.
    But let’s not forget about your secondary project objective: *disease profiling*.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 仅此而已，你就已经实现了你的第一个目标：*诊断疾病*。但不要忘记你的次要项目目标：*疾病分析*。
- en: 'Project secondary objective: disease profiling'
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 项目次要目标：疾病分析
- en: 'What you want is a list of keywords fitting your selected disease. For this
    you’ll use the significant terms aggregation. The score calculation to determine
    which words are significant is once again a combination of factors, but it roughly
    boils down to a comparison of the number of times a term is found in the result
    set as opposed to all the other documents. This way Elasticsearch profiles your
    result set by supplying the keywords that distinguish it from the other data.
    Let’s do that on diabetes, a common disease that can take many forms:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 你想要的是一个适合你选定疾病的单词列表。为此，你将使用显著术语聚合。用于确定哪些单词是显著的分数计算又是一次因素组合，但大致可以归结为比较一个术语在结果集中出现的次数与所有其他文档相比。这样，Elasticsearch通过提供区分其他数据的关键词来对你的结果集进行配置文件。让我们以糖尿病为例，这是一种可以采取多种形式的常见疾病：
- en: Listing 6.4\. Significant terms Elasticsearch query for “diabetes”
  id: totrans-265
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.4。针对“糖尿病”的显著术语Elasticsearch查询
- en: '![](Images/ch06ex04-0.jpg)'
  id: totrans-266
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch06ex04-0.jpg)'
- en: '![](Images/ch06ex04-1.jpg)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch06ex04-1.jpg)'
- en: You see new code here. You got rid of the query string search and used a filter
    instead. The filter is encapsulated within the query part because search queries
    can be combined with filters. It doesn’t occur in this example, but when this
    happens, Elasticsearch will first apply the far more efficient filter before attempting
    the search. If you know you want to search in a subset of your data, it’s always
    a good idea to add a filter to first create this subset. To demonstrate this,
    consider the following two snippets of code. They yield the same results but they’re
    not the exact same thing.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 你在这里看到了新的代码。你放弃了查询字符串搜索，而是使用了过滤器。过滤器被封装在查询部分，因为搜索查询可以与过滤器结合使用。在示例中并未出现，但在此发生时，Elasticsearch将首先应用效率更高的过滤器，然后再尝试搜索。如果你知道你想要在数据子集中进行搜索，添加一个过滤器首先创建这个子集总是一个好主意。为了演示这一点，考虑以下两个代码片段。它们产生相同的结果，但它们并不完全相同。
- en: 'A simple query string searching for “diabetes” in the disease name:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 在疾病名称中搜索“糖尿病”的简单查询字符串：
- en: '[PRE11]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'A term filter filtering in all the diseases with “diabetes” in the name:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 一个过滤所有疾病名称中包含“糖尿病”的术语过滤器：
- en: '[PRE12]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Although it won’t show on the small amount of data at your disposal, the filter
    is way faster than the search. A search query will calculate a search score for
    each of the diseases and rank them accordingly, whereas a filter simply filters
    out all those that don’t comply. A filter is thus far less complex than an actual
    search: it’s either “yes” or “no” and this is evident in the output. The score
    is 1 for everything; no distinction is made within the result set. The output
    consists of two parts now because of the significant terms aggregation. Before
    you only had hits; now you have hits and aggregations. First, have a look at the
    hits in [figure 6.31](#ch06fig31).'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它不会显示在你可用的少量数据中，但过滤器的速度远远快于搜索。搜索查询将为每种疾病计算一个搜索分数并相应地排名，而过滤器只是过滤掉所有不符合条件的。因此，过滤器比实际搜索要简单得多：它只是“是”或“否”，这在输出中很明显。分数对所有内容都是1；在结果集中没有做出区分。由于显著术语聚合，输出现在由两部分组成，之前只有命中项；现在有命中项和聚合项。首先，看看[图6.31](#ch06fig31)中的命中项。
- en: Figure 6.31\. Hits output of filtered query with the filter “diabetes” on disease
    name
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.31\. 带有“糖尿病”过滤条件的疾病名称查询输出
- en: '![](Images/06fig31_alt.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig31_alt.jpg)'
- en: 'This should look familiar by now with one notable exception: all results have
    a score of 1\. In addition to being easier to perform, a filter is cached by Elasticsearch
    for awhile. This way, subsequent requests with the same filter are even faster,
    resulting in a huge performance advantage over search queries.'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在应该很熟悉了，只有一个显著的例外：所有结果都有1分的评分。除了更容易执行之外，过滤器还会被Elasticsearch缓存一段时间。这样，具有相同过滤器的后续请求甚至更快，这比搜索查询有巨大的性能优势。
- en: 'When should you use filters and when search queries? The rule is simple: use
    filters whenever possible and use search queries for full-text search when a ranking
    between the results is required to get the most interesting results at the top.'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该在什么时候使用过滤器，什么时候使用搜索查询？规则很简单：尽可能使用过滤器，当需要结果之间的排名以获取最有趣的结果放在顶部时，使用全文搜索查询。
- en: Now take a look at the significant terms in [figure 6.32](#ch06fig32).
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看[图6.32](#ch06fig32)中的显著术语。
- en: Figure 6.32\. Diabetes significant terms aggregation, first five keywords
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.32\. 糖尿病显著术语聚合，前五个关键词
- en: '![](Images/06fig32_alt.jpg)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig32_alt.jpg)'
- en: 'If you look at the first five keywords in [figure 6.32](#ch06fig32) you’ll
    see that the top four are related to the origin of diabetes. The following Wikipedia
    paragraph offers help:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看[图6.32](#ch06fig32)中的前五个关键词，你会发现前四个与糖尿病的起源相关。以下维基百科段落提供了帮助：
- en: The word diabetes (![](Images/182fig01.jpg) or ![](Images/182fig02.jpg)) comes
    from Latin diabētēs, which in turn comes from Ancient Greek διαβńτης (diabētēs)
    which literally means “a passer through; a siphon” [69]. Ancient Greek physician
    Aretaeus of Cappadocia (fl. 1st century CE) used that word, with the intended
    meaning “excessive discharge of urine,” as the name for the disease [70, 71, 72].
    Ultimately, the word comes from Greek διαβαívειv (diabainein), meaning “to pass
    through,” [69] which is composed of dia- (dia-), meaning “through” and βαívειv
    (bainein), meaning “to go” [70]. The word “diabetes” is first recorded in English,
    in the form diabete, in a medical text written around 1425.
  id: totrans-282
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 单词 diabetes（![](Images/182fig01.jpg) 或 ![](Images/182fig02.jpg)）来自拉丁语 diabētēs，它又来自古希腊语
    διαβήτης (diabētēs)，其字面意思是“通过者；虹吸管” [69]。古希腊医生卡帕多西亚的阿雷塔乌斯（公元1世纪活跃）使用这个词，其意图是“尿液过度排出”，作为疾病的名称
    [70, 71, 72]。最终，这个词来自希腊语 διαβαívειv (diabainein)，意为“通过”，[69] 由 dia-（dia-）意为“通过”和
    βαívειv (bainein) 意为“去” [70] 组成。单词“diabetes”首次记录在英语中，形式为 diabete，在1425年左右撰写的一篇医学文本中。
- en: ''
  id: totrans-283
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Wikipedia page Diabetes_mellitus*'
  id: totrans-284
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*维基百科页面 Diabetes_mellitus*'
- en: 'This tells you where the word *diabetes* comes from: “a passer through; a siphon”
    in Greek. It also mentions *diabainein* and *bainein*. You might have known that
    the most relevant keywords for a disease would be the actual definition and origin.
    Luckily we asked for 30 keywords, so let’s pick a few more interesting ones such
    as *ndi*. *ndi* is a lowercased version of *NDI*, or “Nephrogenic Diabetes Insipidus,”
    the most common acquired form of diabetes. Lowercase keywords are returned because
    that’s how they’re stored in the index when we put it through the standard analyzer
    when indexing. We didn’t specify anything at all while indexing, so the standard
    analyzer was used by default. Other interesting keywords in the top 30 are *avp*,
    a gene related to diabetes; *thirst*, a symptom of diabetes; and *Amiloride*,
    a medication for diabetes. These keywords do seem to profile diabetes, but we’re
    missing multi-term keywords; we stored only individual terms in the index because
    this was the default behavior. Certain words will never show up on their own because
    they’re not used that often but are still significant when used in combination
    with other terms. Currently we miss out on the relationship between certain terms.
    Take *avp*, for example; if *avp* were always written in its full form “Nephrogenic
    Diabetes Insipidus,” it wouldn’t be picked up. Storing *n-grams* (combinations
    of *n* number of words) takes up storage space, and using them for queries or
    aggregations taxes the search server. Deciding where to stop is a balance exercise
    and depends on your data and use case.'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 这告诉你单词 *diabetes* 的来源：“在希腊语中是‘通过者；虹吸管’”。它还提到了 *diabainein* 和 *bainein*。你可能知道，与疾病最相关的关键词将是实际定义和起源。幸运的是，我们请求了30个关键词，所以让我们挑选一些更有趣的，比如
    *ndi*。*ndi* 是 *NDI* 的小写版本，即“Nephrogenic Diabetes Insipidus”，这是最常见的糖尿病获得形式。小写关键词被返回，因为这是我们通过标准分析器索引时它们在索引中的存储方式。我们在索引时没有指定任何内容，所以默认使用了标准分析器。前30个有趣的关键词还包括
    *avp*，与糖尿病相关的基因；*thirst*，糖尿病的症状；以及 *Amiloride*，糖尿病的药物。这些关键词似乎确实可以描述糖尿病，但我们缺少多词关键词；我们仅在索引中存储了单个术语，因为这默认行为。某些单词单独出现时永远不会出现，因为它们不太常用，但与其他术语结合使用时仍然很重要。目前，我们错过了某些术语之间的关系。以
    *avp* 为例；如果 *avp* 总是以其完整形式“Nephrogenic Diabetes Insipidus”书写，它就不会被选中。存储 *n-grams*（*n*
    个单词的组合）会占用存储空间，并且使用它们进行查询或聚合会加重搜索服务器的负担。决定在哪里停止是一个平衡练习，取决于你的数据和用例。
- en: Generally, bigrams (combination of two terms) are useful because meaningful
    bigrams exist in the natural language, though 10-grams not so much. Bigram key
    concepts would be useful for disease profiling, but to create those bigram significant
    term aggregations you’d need them stored as bigrams in your index. As is often
    the case in data science, you’ll need to go back several steps to make a few changes.
    Let’s go back to the data preparation phase.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，二元组（两个术语的组合）是有用的，因为自然语言中存在有意义的二元组，尽管10-grams 并不多。对于疾病特征，二元组的关键概念将是有用的，但为了创建这些二元组显著术语聚合，你需要将它们作为二元组存储在你的索引中。正如数据科学中经常发生的那样，你需要退回几步来做出一些更改。让我们回到数据准备阶段。
- en: '6.2.4\. Step 3 revisited: Data preparation for disease profiling'
  id: totrans-287
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.4\. 步骤3回顾：疾病特征数据准备
- en: It shouldn’t come as a surprise that you’re back to data preparation, as shown
    in [figure 6.33](#ch06fig33). The data science process is an iterative one, after
    all. When you indexed your data, you did virtually no data cleansing or data transformations.
    You can add data cleansing now by, for instance, stop word filtering. *Stop words*
    are words that are so common that they’re often discarded because they can pollute
    the results. We won’t go into stop word filtering (or other data cleansing) here,
    but feel free to try it yourself.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.33](#ch06fig33)所示，您回到了数据准备阶段，这并不奇怪。毕竟，数据科学流程是一个迭代的过程。当您索引数据时，实际上几乎没有进行数据清洗或数据转换。您现在可以通过例如停用词过滤来添加数据清洗。*停用词*是如此常见，以至于它们通常被丢弃，因为它们可能会污染结果。我们不会在这里详细介绍停用词过滤（或其他数据清洗），但您可以自由尝试。
- en: 'Figure 6.33\. Data science process step 3: data preparation. Data cleansing
    for text can be stop word filtering; data transformation can be lowercasing of
    characters.'
  id: totrans-289
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.33\. 数据科学流程步骤3：数据准备。文本的数据清洗可以是停用词过滤；数据转换可以是字符的小写化。
- en: '![](Images/06fig33.jpg)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig33.jpg)'
- en: To index bigrams you need to create your own token filter and text analyzer.
    A *token filter* is capable of putting transformations on tokens. Your specific
    token filter needs to combine tokens to create *n*-grams, also called *shingles*.
    The default Elasticsearch tokenizer is called the standard tokenizer, and it will
    look for word boundaries, like the space between words, to cut the text into different
    tokens or terms. Take a look at the new settings for your disease index, as shown
    in the following listing.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 要索引二元组，您需要创建自己的标记过滤器和文本分析器。一个*标记过滤器*能够对标记进行转换。您的特定标记过滤器需要将标记组合起来创建*n*-gram，也称为*瓦片*。默认的Elasticsearch标记器称为标准标记器，它会寻找单词边界，如单词之间的空格，将文本切割成不同的标记或术语。请查看您疾病索引的新设置，如下所示。
- en: Listing 6.5\. Updating Elasticsearch index settings
  id: totrans-292
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.5\. 更新Elasticsearch索引设置
- en: '![](Images/184fig01_alt.jpg)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/184fig01_alt.jpg)'
- en: 'You create two new elements: the token filter called “my shingle filter” and
    a new analyzer called “my_shingle_analyzer.” Because *n*-grams are so common,
    Elasticsearch comes with a built-in shingle token filter type. All you need to
    tell it is that you want the bigrams `"min_shingle_size" : 2, "max_shingle_size"
    : 2`, as shown in [figure 6.34](#ch06fig34). You could go for trigrams and higher,
    but for demonstration purposes this will suffice.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: '您创建了两个新元素：一个名为“my shingle filter”的标记过滤器和一个名为“my_shingle_analyzer”的新分析器。由于*n*-gram非常常见，Elasticsearch自带了一个内置的瓦片标记过滤器类型。您需要告诉它的只是您想要二元组`"min_shingle_size"
    : 2, "max_shingle_size" : 2`，如[图6.34](#ch06fig34)所示。您可以选择三元组或更高阶的，但为了演示目的，这已经足够了。'
- en: Figure 6.34\. A shingle token filter to produce bigrams
  id: totrans-295
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.34\. 用于生成二元组的瓦片标记过滤器
- en: '![](Images/06fig34_alt.jpg)'
  id: totrans-296
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig34_alt.jpg)'
- en: The analyzer shown in [figure 6.35](#ch06fig35) is the combination of all the
    operations required to go from input text to index. It incorporates the shingle
    filter, but it’s much more than this. The tokenizer splits the text into tokens
    or terms; you can then use a lowercase filter so there’s no difference when searching
    for “Diabetes” versus “diabetes.” Finally, you apply your shingle filter, creating
    your bigrams.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 如[图6.35](#ch06fig35)所示的分析器是所有从输入文本到索引所需操作的组合。它包含了瓦片过滤器，但远不止于此。标记器将文本分割成标记或术语；然后您可以使用小写过滤器，这样在搜索“糖尿病”与“diabetes”时就没有区别。最后，您应用您的瓦片过滤器，创建您的二元组。
- en: Figure 6.35\. A custom analyzer with standard tokenization and a shingle token
    filter to produce bigrams
  id: totrans-298
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.35\. 带有标准标记化和瓦片标记过滤器的自定义分析器，用于生成二元组
- en: '![](Images/06fig35_alt.jpg)'
  id: totrans-299
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig35_alt.jpg)'
- en: Notice that you need to close the index before updating the settings. You can
    then safely reopen the index knowing that your settings have been updated. Not
    all setting changes require the index to be closed, but this one does. You can
    find an overview of what settings need the index to be closed at [http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html](http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在更新设置之前您需要关闭索引。然后您可以安全地重新打开索引，知道您的设置已经更新。并非所有设置更改都需要关闭索引，但这次需要。您可以在[http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html](http://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html)找到需要关闭索引的设置概览。
- en: The index is now ready to use your new analyzer. For this you’ll create a new
    document type, `diseases2`, with a new mapping, as shown in the following listing.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 索引现在可以准备使用你的新分析器了。为此，你将创建一个新的文档类型 `diseases2`，并使用新的映射，如下所示。
- en: Listing 6.6\. Create more advanced Elasticsearch doctype mapping
  id: totrans-302
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.6\. 创建更高级的 Elasticsearch doctype 映射
- en: '![](Images/185fig01_alt.jpg)'
  id: totrans-303
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/185fig01_alt.jpg)'
- en: 'Within `fulltext` you now have an extra parameter, `fields`. Here you can specify
    all the different isotopes of `fulltext`. You have only one; it goes by the name
    `shingles` and will analyze the `fulltext` with your new `my_shingle_analyzer`.
    You still have access to your original `fulltext`, and you didn’t specify an analyzer
    for this, so the standard one will be used as before. You can access the new one
    by giving the property name followed by its field name: `fulltext.shingles`. All
    you need to do now is go through the previous steps and index the data using the
    Wikipedia API, as shown in the following listing.'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `fulltext` 中，你现在有一个额外的参数，`fields`。在这里，你可以指定所有不同的 `fulltext` 同位素。你只有一个；它被称为
    `shingles`，将使用你的新 `my_shingle_analyzer` 分析 `fulltext`。你仍然可以访问你原始的 `fulltext`，你没有为这个指定分析器，所以将使用之前的标准分析器。你可以通过给出属性名后跟其字段名来访问新的一个：`fulltext.shingles`。你现在需要做的就是按照前面的步骤进行，并使用维基百科
    API 索引数据，如下所示。
- en: Listing 6.7\. Reindexing Wikipedia disease explanations with new doctype mapping
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.7\. 使用新的 doctype 映射重新索引维基百科疾病解释
- en: '![](Images/ch06ex07-0.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch06ex07-0.jpg)'
- en: '![](Images/ch06ex07-1.jpg)'
  id: totrans-307
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/ch06ex07-1.jpg)'
- en: There’s nothing new here, only this time you’ll index doc_type `diseases2` instead
    of `diseases`. When this is complete you can again move forward to step 4, data
    exploration, and check the results.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 这里没有新的内容，只是这次你将索引 doc_type `diseases2` 而不是 `diseases`。当这一步完成后，你又可以继续前进到第 4 步，数据探索，并检查结果。
- en: '6.2.5\. Step 4 revisited: Data exploration for disease profiling'
  id: totrans-309
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.5\. 重新审视第 4 步：疾病配置文件的数据探索
- en: 'You’ve once again arrived at data exploration. You can adapt the aggregations
    query and use your new field to give you bigram key concepts related to diabetes:'
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 你又一次到达了数据探索阶段。你可以调整聚合查询，并使用你的新字段来获取与糖尿病相关的大词组关键概念：
- en: Listing 6.8\. Significant terms aggregation on “diabetes” with bigrams
  id: totrans-311
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.8\. 基于 bigrams 对“糖尿病”进行显著术语聚合
- en: '[PRE13]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Your new aggregate, called `DiseaseBigrams`, uses the `fulltext.shingles` field
    to provide a few new insights into diabetes. These new key terms show up:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 你的新聚合，称为 `DiseaseBigrams`，使用 `fulltext.shingles` 字段为糖尿病提供一些新的见解。这些新关键词出现了：
- en: '***Excessive discharge*** —A diabetes patient needs to urinate frequently.'
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***过度排尿*** —糖尿病患者需要频繁排尿。'
- en: '***Causes polyuria*** —This indicates the same thing: diabetes causes the patient
    to urinate frequently.'
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***多尿的原因*** —这表明了相同的事情：糖尿病导致患者频繁排尿。'
- en: '***Deprivation test*** —This is actually a trigram, “water deprivation test”,
    but it recognized *deprivation test* because you have only bigrams. It’s a test
    to determine whether a patient has diabetes.'
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***剥夺测试*** —这实际上是一个三元组，“水剥夺测试”，但它识别了 *剥夺测试*，因为你只有 bigrams。这是一个用来确定患者是否患有糖尿病的测试。'
- en: '***Excessive thirst*** —You already found “thirst” with your unigram keyword
    search, but technically at that point it could have meant “no thirst.”'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***过度口渴*** —你已经通过单语元关键词搜索找到了“口渴”，但从技术上讲，在那个阶段它可能意味着“没有口渴”。'
- en: 'There are other interesting bigrams, unigrams, and probably also trigrams.
    Taken as a whole, they can be used to analyze a text or a collection of texts
    before reading them. Notice that you achieved the desired results without getting
    to the modeling stage. Sometimes there’s at least an equal amount of valuable
    information to be found in data exploration as in data modeling. Now that you’ve
    fully achieved your secondary objective, you can move on to step 6 of the data
    science process: presentation and automation.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 还有其他有趣的 bigrams、unigrams，可能还有 trigrams。整体来看，它们可以在阅读之前用来分析文本或文本集合。请注意，你没有到达建模阶段就实现了预期的结果。有时，数据探索中至少有与数据建模一样多的有价值信息。现在你已经完全实现了你的次要目标，可以继续到数据科学过程的第
    6 步：展示和自动化。
- en: '6.2.6\. Step 6: Presentation and automation'
  id: totrans-319
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.6\. 第 6 步：展示和自动化
- en: Your primary objective, disease diagnostics, turned into a self-service diagnostics
    tool by allowing a physician to query it via, for instance, a web application.
    You won’t build a website in this case, but if you plan on doing so, please read
    the sidebar “[Elasticsearch for web applications](#ch06sb03).”
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您的主要目标，疾病诊断，通过允许医生通过例如网络应用程序查询它，变成了一个自助诊断工具。在这种情况下，您不会构建一个网站，但如果您打算这样做，请阅读侧边栏“[Elasticsearch
    for web applications](#ch06sb03)。”
- en: '|  |'
  id: totrans-321
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Elasticsearch for web applications**'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '**Elasticsearch for web applications**'
- en: 'As with any other database, it’s bad practice to expose your Elasticsearch
    REST API directly to the front end of web applications. If a website can directly
    make POST requests to your database, anyone can just as easily delete your data:
    there’s always a need for an intermediate layer. This middle layer could be Python
    if that suits you. Two popular Python solutions would be Django or the Django
    REST framework in combination with an independent front end. Django is generally
    used to build *round-trip applications* (web applications where the server builds
    the front end dynamically, given the data from the database and a templating system).
    The Django REST framework is a plugin to Django, transforming Django into a REST
    service, enabling it to become part of single-page applications. A single-page
    application is a web application that uses a single web page as an anchor but
    is capable of dynamically changing the content by retrieving static files from
    the HTTP server and data from RESTful APIs. Both approaches (round-trip and single-page)
    are fine, as long as the Elasticsearch server itself isn’t open to the public,
    because it has no built-in security measures. Security can be added to Elasticsearch
    directly using “Shield,” an Elasticsearch payable service.'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何其他数据库一样，直接将您的Elasticsearch REST API暴露给网络应用程序的前端是一种不良做法。如果网站可以直接向您的数据库发送POST请求，任何人都可以轻易地删除您的数据：始终需要一个中间层。这个中间层可以是Python，如果您喜欢的话。两种流行的Python解决方案是Django或与独立前端结合使用的Django
    REST框架。Django通常用于构建*往返应用程序*（服务器根据数据库中的数据和一个模板系统动态构建前端，的Web应用程序）。Django REST框架是Django的一个插件，将Django转换成一个REST服务，使其能够成为单页应用程序的一部分。单页应用程序是一个使用单个网页作为锚点的Web应用程序，但它能够通过从HTTP服务器检索静态文件和从RESTful
    API获取数据来动态更改内容。这两种方法（往返和单页）都是可以的，只要Elasticsearch服务器本身不对公众开放，因为它没有内置的安全措施。可以直接使用“Shield”，一个Elasticsearch付费服务，向Elasticsearch添加安全措施。
- en: '|  |'
  id: totrans-324
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The secondary objective, disease profiling, can also be taken to the level of
    a user interface; it’s possible to let the search results produce a word cloud
    that visually summarizes the search results. We won’t take it that far in this
    book, but if you’re interested in setting up something like this in Python, use
    the word_cloud library (`pip install word_cloud`). Or if you prefer JavaScript,
    D3.js is a good way to go. You can find an example implementation at [http://www.jasondavies.com/wordcloud/#%2F%2Fwww.jasondavies.com%2Fwordcloud%2Fabout%2F](http://www.jasondavies.com/wordcloud/#%2F%2Fwww.jasondavies.com%2Fwordcloud%2Fabout%2F).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个目标，疾病分析，也可以提升到用户界面的水平；可以让搜索结果生成一个词云，直观地总结搜索结果。我们在这本书中不会走那么远，但如果您对在Python中设置类似的东西感兴趣，请使用word_cloud库（`pip
    install word_cloud`）。或者如果您更喜欢JavaScript，D3.js是一个不错的选择。您可以在[http://www.jasondavies.com/wordcloud/#%2F%2Fwww.jasondavies.com%2Fwordcloud%2Fabout%2F](http://www.jasondavies.com/wordcloud/#%2F%2Fwww.jasondavies.com%2Fwordcloud%2Fabout%2F)找到示例实现。
- en: Adding your keywords on this D3.js-driven website will produce a unigram word
    cloud like the one shown in [figure 6.36](#ch06fig36) that can be incorporated
    into the presentation of your project results. The terms aren’t weighted by their
    score in this case, but it already provides a nice representation of the findings.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个由D3.js驱动的网站上添加您的关键词将生成一个类似于[图6.36](#ch06fig36)的单词云，可以将其纳入您项目结果展示中。在这种情况下，术语不是根据它们的分数进行加权的，但它已经提供了对发现的一个很好的表示。
- en: Figure 6.36\. Unigram word cloud on non-weighted diabetes keywords from Elasticsearch
  id: totrans-327
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.36\. 来自Elasticsearch的非加权糖尿病关键词的单词云
- en: '![](Images/06fig36.jpg)'
  id: totrans-328
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/06fig36.jpg)'
- en: Many improvements are possible for your application, especially in the area
    of data preparation. But diving into all the possibilities here would take us
    too far; thus we’ve come to the end of this chapter. In the next one we’ll take
    a look at streaming data.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 您的应用程序有很多改进空间，尤其是在数据准备方面。但深入探讨所有可能性会让我们走得太远；因此，我们来到了这一章的结尾。在下一章中，我们将探讨流数据。
- en: 6.3\. Summary
  id: totrans-330
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 总结
- en: 'In this chapter, you learned the following:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了以下内容：
- en: '*NoSQL* stands for “Not Only Structured Query Language” and has arisen from
    the need to handle the exponentially increasing amounts and varieties of data,
    as well as the increasing need for more diverse and flexible schemas such as network
    and hierarchical structures.'
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*NoSQL* 代表“不仅限于结构化查询语言”，它源于处理指数级增长的数据量和种类以及对于更多样化和灵活的模式的日益增长的需求，如网络和层次结构。'
- en: 'Handling all this data requires database partitioning because no single machine
    is capable of doing all the work. When partitioning, the CAP Theorem applies:
    you can have availability or consistency but never both at the same time.'
  id: totrans-333
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理所有这些数据需要数据库分区，因为没有任何一台机器能够完成所有的工作。在分区时，CAP 定理适用：你可以拥有可用性或一致性，但永远不会同时拥有两者。
- en: 'Relational databases and graph databases hold to the ACID principles: atomicity,
    consistency, isolation, and durability. NoSQL databases generally follow the BASE
    principles: basic availability, soft state, and eventual consistency.'
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关系型数据库和图数据库遵循 ACID 原则：原子性、一致性、隔离性和持久性。NoSQL 数据库通常遵循 BASE 原则：基本可用性、软状态和最终一致性。
- en: The four biggest types of NoSQL databases
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大的四种 NoSQL 数据库类型
- en: '***Key-value stores*** —Essentially a bunch of key-value pairs stored in a
    database. These databases can be immensely big and are hugely versatile but the
    data complexity is low. A well-known example is Redis.'
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***键值存储*** — 实质上是一系列存储在数据库中的键值对。这些数据库可以非常大且非常灵活，但数据复杂性较低。一个著名的例子是 Redis。'
- en: '***Wide-column databases*** —These databases are a bit more complex than key-value
    stores in that they use columns but in a more efficient way than a regular RDBMS
    would. The columns are essentially decoupled, allowing you to retrieve data in
    a single column quickly. A well-known database is Cassandra.'
  id: totrans-337
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***宽列数据库*** — 这些数据库比键值存储更复杂，因为它们使用列，但比常规的关系型数据库管理系统（RDBMS）更高效。列基本上是解耦的，允许你快速检索单个列中的数据。一个著名的数据库是
    Cassandra。'
- en: '***Document stores*** —These databases are little bit more complex and store
    data as documents. Currently the most popular one is MongoDB, but in our case
    study we use Elasticsearch, which is both a document store and a search engine.'
  id: totrans-338
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***文档存储*** — 这些数据库稍微复杂一些，以文档的形式存储数据。目前最受欢迎的是 MongoDB，但在我们的案例研究中，我们使用的是 Elasticsearch，它既是文档存储也是搜索引擎。'
- en: '***Graph databases*** —These databases can store the most complex data structures,
    as they treat the entities and relations between entities with equal care. This
    complexity comes at a cost in lookup speed. A popular one is Neo4j, but GraphX
    (a graph database related to Apache Spark) is winning ground.'
  id: totrans-339
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***图数据库*** — 这些数据库可以存储最复杂的数据结构，因为它们以相同的方式关注实体及其之间的关系。这种复杂性在查找速度上是有代价的。一个流行的是
    Neo4j，但 GraphX（与 Apache Spark 相关的图数据库）正在赢得市场份额。'
- en: Elasticsearch is a document store and full-text search engine built on top of
    Apache Lucene, the open source search engine. It can be used to tokenize, perform
    aggregation queries, perform dimensional (faceted) queries, profile search queries,
    and much more.
  id: totrans-340
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Elasticsearch 是建立在 Apache Lucene 之上的文档存储和全文搜索引擎，Apache Lucene 是一个开源的搜索引擎。它可以用于分词、执行聚合查询、执行维度（分面）查询、分析搜索查询以及更多功能。
