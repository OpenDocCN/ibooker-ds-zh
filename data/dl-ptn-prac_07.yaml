- en: 5 Procedural design pattern
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5.过程设计模式
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing a procedural design pattern for a convolutional neural network
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍卷积神经网络的过程设计模式
- en: Decomposing the architecture of the procedural design pattern into macro- and
    micro-components
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将过程设计模式的架构分解为宏观和微观组件
- en: Coding former SOTA models with the procedural design pattern
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用过程设计模式编码前SOTA模型
- en: Prior to 2017, the majority of renditions of neural network models were coded
    in a batch scripting style. As AI researchers and experienced software engineers
    became increasingly involved in research and design, we started to see a shift
    in the coding of models that reflected software engineering principles for reuse
    and design patterns.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在2017年之前，大多数神经网络模型的实现都是用批处理脚本风格编写的。随着人工智能研究人员和经验丰富的软件工程师越来越多地参与研究和设计，我们开始看到模型编码的转向，这反映了软件工程原则的可重用性和设计模式。
- en: One of the earliest versions of using design patterns for neural network models
    was the use of a procedural style for reuse. A design pattern implies that there
    is a current best practice for constructing and coding a model that can be reapplied
    across a wide range of cases, such as image classification, object detection and
    tracking, facial recognition, image segmentation, super resolution, and style
    transfer.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 使用设计模式为神经网络模型编写代码的最早版本之一是使用可重用的过程风格。设计模式意味着存在一种当前的最佳实践，用于构建和编码一个模型，该模型可以在广泛的案例中重新应用，例如图像分类、目标检测和跟踪、面部识别、图像分割、超分辨率和风格迁移。
- en: So how did the introduction of design patterns aid in the advancement of CNNs
    (as well as in other architectures, such as transformers for NLP)? First, it aided
    other researchers in understanding and reproducing a model’s architecture. Decomposing
    a model into its reusable components, or patterns, provided a means for other
    practitioners to observe, understand, and then perform efficient device experiments.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，设计模式的引入是如何帮助CNN（以及其他架构，如NLP中的transformer）的进步的呢？首先，它帮助其他研究人员理解和复制一个模型的架构。将模型分解为其可重用组件或模式，为其他从业者提供了观察、理解和然后进行高效设备实验的手段。
- en: We can see this happening as early as the transition in AlexNet to VGG. The
    authors of AlexNet ([http://mng.bz/1ApV](http://mng.bz/1ApV)) had insufficient
    resources to run the AlexNet model on a single GPU. They designed a CNN architecture
    to run in parallel on two GPUs. To solve this problem, they came up with a design
    of having two mirrored convolution paths, a design that won the 2012 ILSVRC competition
    for image classification. Soon other researchers seized on the idea of having
    a convolutional pattern that is repeated, and they began studying the effects
    of the convolutional pattern in addition to analyzing the overall performance.
    In 2014, both GoogLeNet ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))
    and VGG ([https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf))
    based their models, and corresponding research papers, on using a convolutional
    pattern that is repeated in the model; these innovations became the winner and
    first runner-up, respectively, of the 2014 ILSVRC competition.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，早在AlexNet向VGG的过渡时期，这种情况就已经发生了。AlexNet的作者（[http://mng.bz/1ApV](http://mng.bz/1ApV)）没有足够的资源在单个GPU上运行AlexNet模型。他们设计了一个可以在两个GPU上并行运行的CNN架构。为了解决这个问题，他们提出了一个具有两个镜像卷积路径的设计，这个设计赢得了2012年ILSVRC图像分类竞赛。很快，其他研究人员抓住了拥有重复卷积模式的想法，他们开始研究卷积模式的影响，除了分析整体性能。2014年，GoogLeNet（[https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)）和VGG（[https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf)）基于模型和相应的研究论文，使用了模型中重复的卷积模式；这些创新分别成为了2014年ILSVRC竞赛的冠军和亚军。
- en: Understanding the architecture of the procedural design pattern is crucial if
    you are going to apply it to any model you are building. In this chapter, I will
    first show you how this pattern is built, by decomposing it into its macro-architectural
    components, then its micro-architectural groups and blocks. Once you see how the
    parts work, individually and together, you’ll be ready to start working with the
    code that builds these parts.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 理解过程设计模式的架构对于你打算将其应用于你构建的任何模型至关重要。在本章中，我将首先向你展示如何构建这个模式，通过将其分解为其宏观架构组件，然后是微观架构组和块。一旦你看到各个部分如何单独和共同工作，你就可以开始使用构建这些部分的代码了。
- en: 'To show how the procedural design pattern makes it easier to reproduce model
    components, we will then apply it to several formerly SOTA models: VGG, ResNet,
    ResNeXt, Inception, DenseNet, and SqueezeNet. This should give you both a deeper
    understanding of how these models work, as well as practical experience reproducing
    them. Some notable highlights of these architectures are as follows:'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示过程设计模式如何使模型组件的再现更容易，我们将将其应用于几个以前SOTA模型：VGG、ResNet、ResNeXt、Inception、DenseNet和SqueezeNet。这应该让您对这些模型的工作原理有更深入的理解，以及实际再现它们的经验。这些架构的一些显著特点如下：
- en: '*VGG*—2014 winner of image classification in ImageNet ILSVRC challenge'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*VGG*—2014年ImageNet ILSVRC挑战赛图像分类的获胜者'
- en: '*ResNet*—2015 winner of image classification in ImageNet ILSVRC challenge'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ResNet*—2015年ImageNet ILSVRC挑战赛图像分类的获胜者'
- en: '*ResNeXt*—2016, authors improve accuracy with introduction of wide convolutional
    layer'
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ResNeXt*—2016年，作者通过引入宽卷积层提高了准确性'
- en: '*Inception*—2014 winner for object detection in ImageNet ILSVRC challenge'
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Inception*—2014年ImageNet ILSVRC挑战赛物体检测的获胜者'
- en: '*DenseNet*—2017, authors introduced feature map reuse'
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DenseNet*—2017年，作者引入了特征图重用'
- en: '*SqueezeNet*—2016, authors introduce concept of a configurable component'
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*SqueezeNet*—2016年，作者引入了可配置组件的概念'
- en: We will briefly cover one procedural design pattern based on the Idiomatic design
    pattern for CNN models.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将简要介绍基于Idiomatic设计模式的一个基于过程的CNN模型设计模式。
- en: 5.1 Basic neural network architecture
  id: totrans-18
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 基本神经网络架构
- en: 'The *Idiomatic design pattern* views the model as consisting of an overall
    macro-architecture pattern, and then each macro-component, in turn, consisting
    of a micro-architecture design. The concept of a macro- and micro-architecture
    for a model was introduced in the research paper for SqueezeNet in 2016 ([https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360)).
    For a CNN, the macro architecture follows the convention of consisting of three
    macro components: a stem, a learner, and a task, as depicted in figure 5.1.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: '*Idiomatic设计模式*将模型视为由一个整体宏观架构模式组成，然后每个宏观组件依次由一个微观架构设计组成。模型宏观和微观架构的概念是在2016年SqueezeNet的研究论文中引入的([https://arxiv.org/abs/1602.07360](https://arxiv.org/abs/1602.07360))。对于CNN，宏观架构遵循由三个宏观组件组成的惯例：主干、学习器和任务，如图5.1所示。'
- en: '![](Images/CH05_F01_Ferlitsch.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F01_Ferlitsch.png)'
- en: 'Figure 5.1 The CNN macro-architecture is made up of three components: the stem,
    the learner, and the task.'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.1 CNN宏观架构由三个组件组成：主干、学习器和任务。
- en: As you can see, the stem component takes the input (the image) and does the
    initial coarse-level feature extraction, which becomes the input to the learner
    component. In this example, the stem includes a pre-stem group that does data
    preprocessing, and a stem convolution group that does coarse-level feature extraction.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，主干组件接收输入（图像）并执行初始的粗略级别特征提取，这成为学习组件的输入。在这个例子中，主干包括一个预主干组，它执行数据预处理，以及一个主干卷积组，它执行粗略级别的特征提取。
- en: The *learner*, which may be composed of any number of convolutional groups,
    then does the detailed feature extraction and representational learning from the
    extracted coarse features. The output from the learner component is referred to
    as the *latent space*.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，由任意数量的卷积组组成的*学习器*从提取的粗略特征中进行详细的特征提取和表示学习。学习组件的输出被称为*潜在空间*。
- en: The *task* component learns the task (classification, for example) from the
    representation of the input in the latent space.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务*组件从潜在空间中输入表示学习任务（例如分类）。'
- en: While this book focuses on CNNs, this macro-architecture of stem, learner, and
    task components can be applied to other neural network architectures, such as
    transformer networks with attention mechanisms in natural-language processing.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这本书主要关注CNN，但这种主干、学习器和任务组件的宏观架构可以应用于其他神经网络架构，例如在自然语言处理中具有注意力机制的Transformer网络。
- en: 'Looking at a skeleton template for the Idiomatic design pattern using the functional
    API, you can see the data flow between components at a high level. We will be
    using this template (in the following code block), and building on it, throughout
    the chapters that use the Idiomatic design pattern. The skeleton consists of two
    main components:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用功能API查看Idiomatic设计模式的骨架模板，您可以在高层次上看到组件之间的数据流。我们将使用这个模板（在以下代码块中），并在使用Idiomatic设计模式的章节中在此基础上构建。该骨架由两个主要组件组成：
- en: 'Function (procedural) input/output definitions of the major components: stem,
    learner, and task'
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 主要组件（过程）的输入/输出定义：主干、学习器和任务
- en: The input (tensor) flows through the major components
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入（张量）流经主要组件
- en: 'Here’s the skeleton template:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 这是骨架模板：
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Constructs the stem component
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建主干组件
- en: ❷ Constructs the learner component
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建学习组件
- en: ❸ Constructs the task component for a classifier
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为分类器构建任务组件
- en: ❹ Defines the input tensor
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义输入张量
- en: ❺ Assembles the model
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 组装模型
- en: In this example, the `Input` class defines the input tensor to the model; in
    the case of a CNN, it consists of the shape of the image. The tuple (224, 224,
    3) refers to a 224 × 224 RGB (three-channel) image. The `Model` class is the final
    step when coding the neural network using the TF.Keras functional API. This step
    is the final build step (referred to as the `compile()` method) of the model.
    The parameters to the `Model` class are the model input(s) tensor and output(s)
    tensor. In our example, we have a single input and single output tensor. Figure
    5.2 depicts these steps.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，`Input`类定义了模型的输入张量；对于CNN来说，它由图像的形状组成。元组（224，224，3）指的是一个224 × 224 RGB（三通道）图像。`Model`类是使用TF.Keras功能API编码神经网络时的最后一步。这一步是模型的最终构建步骤（称为`compile()`方法）。`Model`类的参数是模型输入（s）张量和输出（s）张量。在我们的例子中，我们有一个输入张量和输出张量。图5.2描述了这些步骤。
- en: '![](Images/CH05_F02_Ferlitsch.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F02_Ferlitsch.png)'
- en: 'Figure 5.2 The steps for building a CNN model: define input, construct components,
    compile into a graph'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 构建CNN模型的步骤：定义输入，构建组件，编译成图
- en: Now let’s take a closer look at the three macro-components.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们更详细地看看三个宏观组件。
- en: 5.2 Stem component
  id: totrans-40
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 主干组件
- en: 'The *stem component* is the entry point to the neural network. Its primary
    purpose is to perform the first (coarse-level) feature extraction while also reducing
    the feature maps to a size designed for the learner component. The number of feature
    maps and the feature map sizes outputted by the stem component are designed by
    balancing two criteria at the same time:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '**主干组件**是神经网络的人口点。其主要目的是执行第一层（粗粒度）特征提取，同时将特征图减少到适合学习组件的大小。主干组件输出的特征图数量和大小是通过同时平衡两个标准来设计的：'
- en: Maximize the feature extraction for coarse-level features. The goal here is
    to give the model enough information to learn finer-level features, within the
    model’s capacity.
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最大化粗粒度特征的特征提取。这里的目的是给模型提供足够的信息来学习更细粒度的特征，同时不超过模型的能力。
- en: Minimize the number of parameters in the downstream learner component. Ideally,
    you want to minimize the size of the feature maps and the time it takes to train
    the model, but without affecting the model’s performance.
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化下游学习组件中的参数数量。理想情况下，你希望最小化特征图的大小和训练模型所需的时间，但又不影响模型的表现。
- en: 'This initial task is performed by the *stem convolution group*. Let’s now look
    at some variations of stem groups from a select number of well-known CNN models:
    VGG, ResNet, ResNeXt, and Inception.'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 这个初始任务由**主干卷积组**执行。现在让我们看看一些来自知名CNN模型（VGG、ResNet、ResNeXt和Inception）的主干组的变体。
- en: 5.2.1 VGG
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 VGG
- en: The VGG architecture, winner of the 2014 ImageNet ILSVRC contest for image classification,
    is considered the father of modern CNNs, while AlexNet is considered the grandfather.
    VGG formalized the concept of constructing a CNN into components and groups by
    using a pattern. Prior to VGG, CNNs were constructed as ConvNets, whose usefulness
    did not go beyond academic novelties.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: VGG架构赢得了2014年ImageNet ILSVRC图像分类竞赛，被认为是现代CNN之父，而AlexNet被认为是祖父。VGG通过使用模式将CNN构建成组件和组的形式，正式化了这一概念。在VGG之前，CNN被构建为ConvNets，其用途并未超出学术上的新奇之处。
- en: VGGs were the first to have practical applications in production. For several
    years after its development, researchers continued to compare more modern SOTA
    architecture developments to the VGG and to use VGGs for the classification backbone
    of early SOTA object-detection models.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: VGG是第一个在生产中具有实际应用的。在其开发后的几年里，研究人员继续将更现代的SOTA架构发展与VGG进行比较，并使用VGG作为早期SOTA目标检测模型的分类骨干。
- en: The VGG, along with Inception, formalized the concept of having a first convolutional
    group that did a coarse-level feature extraction, which we now refer to as the
    *stem component*. Subsequent convolutional groups would then do finer levels of
    feature extraction and feature learning, which we now refer to as *representational
    learning*, and hence the term *learner* for this second major component.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: VGG，连同Inception，正式化了拥有一个进行粗略级别特征提取的第一卷积组的概念，我们现在称之为*stem组件*。随后的卷积组将进行更细级别的特征提取和学习，我们现在称之为*表征学习*，因此这个第二主要组件被称为*学习者*。
- en: 'Researchers eventually discovered a drawback of a VGG stem: it retained the
    size of the input (224 × 224) in the extracted coarse feature maps, resulting
    in an unnecessary number of parameters entering the learner. The quantity of parameters
    both increased the memory footprint as well as reduced performance for training
    and prediction. Researchers subsequently addressed this problem in later SOTA
    models by adding pooling in the stem component, reducing the output size of the
    coarse-level feature maps. This change decreased memory footprint while increasing
    performance, without a loss in accuracy.'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 研究人员最终发现VGG stem的一个缺点：它在提取的粗略特征图中保留了输入大小（224 × 224），导致进入学习者的参数数量过多。参数数量的增加不仅增加了内存占用，还降低了训练和预测的性能。研究人员随后在后续的SOTA模型中通过在stem组件中添加池化来解决此问题，减少了粗略级别特征图的输出大小。这种变化减少了内存占用，同时提高了性能，而没有损失精度。
- en: The convention of outputting 64 coarse-level feature maps continues today, though
    the stem of some modern CNNs may output 32 feature maps.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 输出64个粗略级别特征图的惯例至今仍然存在，尽管一些现代CNN的stem可能输出32个特征图。
- en: The VGG *stem component*, depicted in figure 5.3, was designed to take as an
    input a 224 × 224 × 3 image and to output 64 feature maps, each 224 × 224 in size.
    In other words, the VGG stem group did not do any size reduction of the feature
    maps.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3中所示的VGG *stem组件*被设计为以224 × 224 × 3图像作为输入，并输出64个特征图，每个特征图大小为224 × 224。换句话说，VGG
    stem组没有对特征图进行任何尺寸缩减。
- en: '![](Images/CH05_F03_Ferlitsch.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F03_Ferlitsch.png)'
- en: Figure 5.3 The VGG stem group uses a 3 × 3 filter for coarse-level feature extraction.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 VGG stem组使用3 × 3滤波器进行粗略级别特征提取。
- en: 'Now take a look at a code sample for coding the VGG stem component in the Idiomatic
    design pattern, which consists of a single convolutional layer (`Conv2D`). This
    layer uses a 3 × 3 filter for coarse-level feature extraction for 64 filters.
    It does not do any reduction in size of the feature maps. With a (224, 224, 3)
    image input (ImageNet dataset), the output from this stem group will be (224,
    224, 64):'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 现在看看一个代码示例，用于在Idiomatic设计模式中编码VGG stem组件，该模式由一个单一的卷积层（`Conv2D`）组成。这个层使用3 × 3滤波器对64个滤波器进行粗略级别特征提取，不进行特征图尺寸的缩减。对于（224,
    224, 3）图像输入（ImageNet数据集），这个stem组的输出将是（224, 224, 64）：
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for VGG is located on GitHub ([http://mng.bz/qe4w](https://shortener.manning.com/qe4w)).
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式为VGG编写的完整代码可以在GitHub上找到（[http://mng.bz/qe4w](https://shortener.manning.com/qe4w)）。
- en: 5.2.2 ResNet
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 ResNet
- en: The ResNet architecture, winner of the 2015 ImageNet ILSVRC contest for image
    classification, was one of the first to incorporate the conventional steps of
    both maximizing coarse-level feature extraction and minimizing parameters with
    feature map reduction. When comparing their model to VGG, the authors of ResNet
    found they could reduce the size of the extracted feature maps by a whopping 94%
    in the stem component, reducing memory footprint and increasing the model performance
    without affecting the accuracy.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet架构赢得了2015年ImageNet ILSVRC竞赛的图像分类奖项，它是第一个结合了最大化粗略级别特征提取和通过特征图减少最小化参数的常规步骤的架构。当将他们的模型与VGG进行比较时，ResNet的作者发现他们可以将stem组件中提取的特征图大小减少94%，从而减少内存占用并提高模型性能，而不影响精度。
- en: Note The process of comparing a newer model to the previous SOTA model is called
    an *ablation study*, and is common practice in the machine learning field. Basically,
    the researchers replicate the study of the previous model, and then use the same
    configuration (say, image augmentation or learning rate) for their new model.
    This allows them to make direct apple-to-apple comparisons with the earlier models.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：将较新模型与之前的SOTA模型进行比较的过程被称为*消融研究*，这在机器学习领域是一种常见的做法。基本上，研究人员会复制之前模型的研究，然后为新模型使用相同的配置（例如，图像增强或学习率）。这使得他们能够与早期模型进行直接的苹果对苹果的比较。
- en: The ResNet authors also chose to use an extremely large coarse filter of size
    7 × 7, which covered an area of 49 pixels. Their reasoning here was that the model
    needed a very large filter to be effective. The drawback was a substantial increase
    in matrix multiply, or matmul, operations in the stem component. Eventually, researchers
    subsequently found in later SOTA models a 5 × 5 filter to be as effective and
    more efficient. In conventional CNNs, the 5 × 5 filter is generally replaced with
    a stack of two 3 × 3 filters, with the first convolution being unstrided (no pooling)
    and the second convolution being strided (with feature pooling).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet的作者还选择使用一个极大的7 × 7粗滤波器，覆盖了49个像素的区域。他们的理由是模型需要一个非常大的滤波器才能有效。缺点是在基组件中矩阵乘法或matmul操作的大量增加。最终，研究人员在后来的SOTA模型中发现5
    × 5滤波器同样有效且更高效。在传统的CNN中，5 × 5滤波器通常被两个3 × 3滤波器的堆叠所取代，第一个卷积是无步长的（没有池化），第二个卷积是步长的（带有特征池化）。
- en: For several years, the ResNet v1 and refined v2 became the de facto architecture
    used in production for image classification, and the backbone in object-detection
    models. Beyond its improved performance and accuracy, public versions of pretrained
    ResNets for image classification, object detection, and image segmentation tasks
    were widely available, so this architecture became the standard for transfer learning
    as well. Even today, in high-profile model zoos, like TensorFlow Hub, pretrained
    ResNet v2 continues to be highly prevalent as the image-classification backbone.
    The more modern convention today for a pretrained image classification, however,
    is the smaller, faster, and more accurate EfficientNet. Figure 5.4 depicts the
    layers in a ResNet stem component.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 几年来，ResNet v1和改进的v2成为了图像分类生产中实际使用的默认架构，以及在目标检测模型中的骨干。除了其改进的性能和准确性之外，公开的预训练ResNets版本在图像分类、目标检测和图像分割任务中广泛可用，因此这种架构成为了迁移学习的标准。即使今天，在高调的模型动物园中，如TensorFlow
    Hub，预训练的ResNet v2仍然作为图像分类的骨干而高度流行。然而，今天更现代的预训练图像分类惯例是更小、更快、更准确的EfficientNet。图5.4展示了ResNet基组件中的层。
- en: '![](Images/CH05_F04_Ferlitsch.png)'
  id: totrans-62
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH05_F04_Ferlitsch.png)'
- en: Figure 5.4 The ResNet stem component aggressively reduces the feature map sizes
    with a strided convolution and max pooling.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.4 ResNet基组件通过步长卷积和最大池化积极减少特征图的大小。
- en: In ResNet, the stem component consists of one convolutional layer for coarse
    feature extraction. The model uses a 7 × 7 filter size to obtain coarse features
    over a wider window, under the theory it would extract larger features. The 7
    × 7 filter covers 49 pixels (in contrast to a 3 × 3, which covers 9 pixels). Using
    a much larger filter size also substantially increases the number of computations
    (matrix multiplies) per filter step (as the filter is slid across the image).
    On a per pixel basis, the 3 × 3 has 9 matrix multiplications, and the 7 × 7 has
    49\. After ResNet, the convention of using 7 × 7 to obtain larger coarse-level
    features was not pursued anymore.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在ResNet中，基组件由一个用于粗略特征提取的卷积层组成。该模型使用7 × 7的滤波器大小，在更宽的窗口上获取粗略特征，根据理论，这将提取更大的特征。7
    × 7的滤波器覆盖49个像素（相比之下，3 × 3的滤波器覆盖9个像素）。使用更大的滤波器大小也显著增加了每个滤波器步骤（因为滤波器在图像上滑动）的计算量（矩阵乘法）。以每个像素为基础，3
    × 3有9次矩阵乘法，而7 × 7有49次。在ResNet之后，使用7 × 7来获取更大粗略级特征的传统不再被追求。
- en: Note that both the VGG and ResNet stem components output 64 initial feature
    maps. This continues to be a fairly common convention, learned through trial and
    error by researchers.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，VGG和ResNet的基组件都输出64个初始特征图。这继续成为研究人员通过试错学习到的一个相当常见的惯例。
- en: For feature map reduction, the ResNet stem group does both a feature pooling
    step (strided convolution) and downsampling (max pooling).
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 对于特征图降维，ResNet基组件同时进行特征池化步骤（步长卷积）和下采样（最大池化）。
- en: The convolutional layer uses no padding when sliding the filter across the image.
    Thus, when the filter reaches the edge of the image, it stops. Since the last
    pixels before an edge do not have their own slide by the filter, the output size
    is smaller than the input size, as depicted in figure 5.5\. The consequence is
    that the size of the input and output feature maps are not preserved. For example,
    in a convolution of stride 1, filter size of 3 × 3, and input feature map size
    32 × 32, the output feature maps will be 30 × 30\. Calculating the loss in size
    is straightforward. If the filter size is *N* × *N*, the loss in size will be
    *N* – 1 pixels. In TF.Keras, this is specified with the keyword parameter `padding='valid'`
    to the `Conv2D` layer.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层在滑动滤波器穿过图像时不会使用填充。因此，当滤波器到达图像边缘时，它会停止。由于边缘前的最后几个像素没有自己的滑动，输出尺寸小于输入尺寸，如图5.5所示。结果是输入和输出特征图的尺寸没有得到保留。例如，在步长为1、滤波器大小为3×3、输入特征图大小为32×32的卷积中，输出特征图将是30×30。计算尺寸损失是直接的。如果滤波器大小是*N*
    × *N*，则尺寸损失将是*N* – 1个像素。在TF.Keras中，这是通过将关键字参数`padding='valid'`指定给`Conv2D`层来实现的。
- en: '![](Images/CH05_F05_Ferlitsch.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F05_Ferlitsch.png)'
- en: Figure 5.5 Options for padding and no padding result in different stopping locations
    for the filter.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 填充和不填充的选项导致滤波器的不同停止位置。
- en: Alternatively, we can slide the filter over the edge until the last row and
    column are covered. But part of the filter would hang over imaginary pixels. This
    way, the last pixels before an edge have their own slide by the filter, and the
    size of the output feature map is preserved.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我们可以将滤波器滑动到边缘，直到最后一行和最后一列都被覆盖。但滤波器的一部分会悬停在虚拟像素上。这样，边缘前的最后几个像素将有自己的滑动，输出特征图的尺寸得到保留。
- en: Several strategies exist for padding the imaginary pixels. The most common convention
    today is to pad the imaginary pixels with the same pixel value at the edge, as
    depicted in figure 5.5\. In TF.Keras, this is specified with the keyword parameter
    `padding='same'` to the `Conv2D` layer.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 存在几种填充虚拟像素的策略。如今最常用的惯例是在边缘使用相同的像素值填充虚拟像素，如图5.5所示。在TF.Keras中，这是通过将关键字参数`padding='same'`指定给`Conv2D`层来实现的。
- en: ResNet predated this convention and padded the imaginary pixels with zero values;
    this is why you see in the stem group the `ZeroPadding2D` layers, where a zero
    padding is placed around the image. Today we typically pad the image with the
    same padding and defer feature map size reduction to pooling or feature pooling.
    Through trial and error, researchers learned that this approach gave a better
    result in maintaining feature extraction information at the edge of the image.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ResNet在遵循这一惯例之前就已经存在，并且用零值填充了虚拟像素；这就是为什么你在主干组中看到`ZeroPadding2D`层，其中在图像周围放置了零填充。如今，我们通常使用相同的填充来填充图像，并将特征图尺寸的减少推迟到池化或特征池化。通过反复试验，研究人员发现这种方法在保持图像边缘特征提取信息方面效果更好。
- en: Figure 5.6 shows a convolution with padding on an image of size *H* × *W* ×
    3 (three channels for RGB). With a single filter, we would output a feature map
    of size *H* × *W* × 1.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6展示了在大小为*H* × *W* × 3（RGB的三个通道）的图像上使用填充的卷积。使用单个滤波器，我们将输出一个大小为*H* × *W* ×
    1的特征图。
- en: '![](Images/CH05_F06_Ferlitsch.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F06_Ferlitsch.png)'
- en: Figure 5.6 A padded convolution with a single filter produces the least variability
    of feature extraction.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 使用单个滤波器的填充卷积产生特征提取的最小变异性。
- en: Figure 5.7 shows a convolution with padding on an image of size *H* × *W* ×
    3 (three channels for RGB) with multiple filters *C*. Here, we would output a
    feature map of size *H* × *W* × *C* .
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7展示了在大小为*H* × *W* × 3（RGB的三个通道）的图像上使用多个滤波器*C*的卷积。在这里，我们将输出一个大小为*H* × *W*
    × *C*的特征图。
- en: '![](Images/CH05_F07_Ferlitsch.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F07_Ferlitsch.png)'
- en: Figure 5.7 A padded convolution with multiple filters proportionality increases
    the variability in feature extraction.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7使用多个滤波器的填充卷积按比例增加了特征提取的变异性。
- en: 'Would you ever see a stem convolution with only a single outputted feature
    map, as depicted in figure 5.6? The answer is no. That’s because a single filter
    can learn to extract only a single coarse feature. That’s not going to work for
    images! Even if our images are simple sequences of parallel lines (a single feature)
    and we just want to count the lines, it still won’t work: we can’t control which
    feature a filter will learn to extract. A certain amount of randomness remains
    in the process, so we need some redundancy to guarantee that enough of the filters
    will learn to extract the important features.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾见过如图5.6所示的单个输出特征图的主干卷积？答案是：没有。这是因为单个滤波器只能学习提取单个粗略特征。这对于图像来说是不行的！即使我们的图像是简单的平行线序列（一个特征）并且我们只想计数线条，这仍然是不行的：我们无法控制滤波器学习提取哪个特征。在这个过程中仍然存在一定程度的随机性，因此我们需要一些冗余来保证足够的滤波器能够学习提取重要特征。
- en: Would you ever output a single feature map, somewhere else in a CNN? The answer
    is yes. That would be an aggressive reduction by a 1 × 1 bottleneck convolution.
    A 1 × 1 bottleneck convolution is typically used for feature reuse between different
    convolutions in a CNN.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否曾在CNN的某个地方输出单个特征图？答案是：是的。这将是通过1 × 1瓶颈卷积进行的一种激进减少。1 × 1瓶颈卷积通常用于CNN中不同卷积之间的特征重用。
- en: Once again, it involves a tradeoff. On the one hand, you want to combine the
    benefits of feature extraction/learning at one place in the CNN with another place
    in the CNN (feature reuse). The problem is that reusing the entire previous feature
    maps, in number and size, would create a potential explosion in parameters. That
    resulting increase in memory footprint and reduction in speed offsets the benefit.
    The ResNet authors settled on the amount of feature reduction as the best tradeoff
    between accuracy on the one hand, and size and performance on the other.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，这涉及到权衡。一方面，你希望结合CNN中某处特征提取/学习的优势与另一处的优势（特征重用）。问题是，重用整个先前的特征图，在数量和大小上，可能会在参数上造成潜在的爆炸。这种增加的内存占用和速度降低抵消了好处。ResNet的作者选择了特征减少的量，这是在准确度、大小和性能之间最佳权衡的结果。
- en: 'Next, take a look at a sample for coding the ResNet stem component in the Idiomatic
    design pattern. The code demonstrates the sequential flow through the layers illustrated
    previously in figure 5.3 for the stem:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，看看使用Idiomatic设计模式编码ResNet主干组件的示例。该代码演示了通过图5.3中先前展示的层进行顺序流：
- en: The `Conv2D` layer uses 7 × 7 filter size for coarse-level feature extraction
    and `strides=(2, 2)` for feature pooling.
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Conv2D`层使用7 × 7滤波器大小进行粗略级特征提取，并使用`strides=(2, 2)`进行特征池化。'
- en: '`MaxPooling` layers perform downsampling for further feature map reduction.'
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MaxPooling`层执行下采样以进一步减少特征图。'
- en: 'It’s also worth noting that the ResNet was one of the first models to use the
    convention of batch normalization (`BatchNormalization`). Early conventions, now
    denoted as Conv-BN-RE, had the batch normalization follow the convolutional and
    dense layers. To remind you, batch normalization stabilizes neural networks by
    redistributing the outputs in a layer into a normal distribution. This allows
    the neural network to go deeper in layers without being prone to a vanishing or
    exploding gradient. For more details, see “Batch Normalization: Accelerating Deep
    Network Training by Reducing Internal Covariate Shift” by Sergey Ioffe and Christian
    Szegedy ([https://arxiv.org/ abs/1502.03167](https://arxiv.org/abs/1502.03167)).'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: '值得注意的是，ResNet是第一个使用批归一化（`BatchNormalization`）约定的模型之一。早期的约定，现在称为Conv-BN-RE，批归一化位于卷积和密集层之后。为了提醒你，批归一化通过将层的输出重新分配到正态分布来稳定神经网络。这允许神经网络在更深层次上运行而不会出现梯度消失或爆炸。更多详情，请参阅Sergey
    Ioffe和Christian Szegedy的论文“Batch Normalization: Accelerating Deep Network Training
    by Reducing Internal Covariate Shift”（[https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167)）。'
- en: '[PRE2]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The 224 × 224 images are zero padded (black—no signal) to be 230 × 230 images
    prior to the first convolution.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 224 × 224的图像在第一卷积之前被零填充（黑色——无信号）以成为230 × 230的图像。
- en: ❷ First convolutional layer, which uses a large (coarse) filter
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一卷积层，使用大（粗略）滤波器
- en: ❸ Pooled feature maps will be reduced by 75% with a stride of 2 × 2.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用2 × 2步长，池化后的特征图将减少75%。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for ResNet is located on GitHub ([http://mng.bz/7jK9)](https://shortener.manning.com/7jK9).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic程序重用设计模式为ResNet编写的完整代码版本可在GitHub上找到（[http://mng.bz/7jK9](https://shortener.manning.com/7jK9)）。
- en: 5.2.3 ResNeXt
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.3 ResNeXt
- en: Models that came after ResNet used the convention of same padding, which reduces
    the layers to a single strided convolution (feature pooling) and strided max pooling
    (downsampling), while maintaining the same computational complexity. The ResNeXt
    model ([https://arxiv.org/abs/1512.03385)](https://arxiv.org/abs/1512.03385) by
    Facebook AI Research (figure 5.8), along with Inception by Google Inc, introduced
    using wide residual blocks in the learner component. Don’t worry if you don’t
    know the ramifications of wide and deep residual blocks; I’ll explain those in
    chapter 6\. Here I just want you to know that the appearance of padding within
    the convolution emerged with the early SOTA wide residual models. As far as use
    in production, ResNeXt architectures and other wide CNNs seldom appear outside
    of memory-constrained devices; subsequent developments for size, speed, and accuracy
    are more prominent.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ResNet 之后出现的模型使用了相同填充的惯例，这减少了层到单个步进卷积（特征池化）和步进最大池化（下采样），同时保持了相同的计算复杂度。Facebook
    AI Research 的 ResNeXt 模型 ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))，以及谷歌公司的
    Inception，引入了在学习者组件中使用宽残差块。如果你不知道宽和深残差块的影响，我在第 6 章中会解释。在这里，我只是想让你知道，卷积中的填充出现在早期的
    SOTA 宽残差模型中。至于生产中的使用，ResNeXt 架构和其他宽 CNN 很少出现在内存受限的设备之外；后续在尺寸、速度和准确性方面的改进更为突出。
- en: '![](Images/CH05_F08_Ferlitsch.png)'
  id: totrans-93
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F08_Ferlitsch.png)'
- en: Figure 5.8 The ResNeXt stem component does aggressive feature map reduction
    with combined features and max pooling.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 ResNeXt 基础组件通过组合特征和最大池化进行积极的特征图减少。
- en: Note that by using the convention of same padding, there was no need to use
    `ZeroPadding` layers to maintain feature map sizes.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于使用了相同填充的惯例，因此没有必要使用 `ZeroPadding` 层来保持特征图大小。
- en: 'The following is a code sample for coding the ResNeXt stem group in the Idiomatic
    design pattern. In this example, you can see the contrast with the ResNet stem
    group; the `ZeroPadding` layers are absent, and replaced with `padding=''same''`
    for the `Conv2D` and `MaxPooling` layer:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个在 Idiomatic 设计模式中编码 ResNeXt 基础组件的代码示例。在这个例子中，你可以看到与 ResNet 基础组件的对比；`ZeroPadding`
    层不存在，而是用 `padding='same'` 替换了 `Conv2D` 和 `MaxPooling` 层：
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Uses padding='same' instead of a ZeroPadding2D as in VGG
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 padding='same' 而不是 VGG 中的 ZeroPadding2D
- en: In subsequent models, the 7 × 7 filter size was replaced with a smaller 5 ×
    5 filter, which had lower computational complexity. A common convention today
    is the 5 × 5 filter refactored into two 3 × 3 filters, which have the same representational
    power with lower computational complexity.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在后续的模型中，7 × 7 滤波器大小被替换为较小的 5 × 5 滤波器，它具有更低的计算复杂度。今天的常见惯例是将 5 × 5 滤波器重构为两个 3
    × 3 滤波器，它们具有相同的表示能力，但计算复杂度更低。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for ResNeXt is available on GitHub ([http://mng.bz/my6r](https://shortener.manning.com/my6r)).
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub ([http://mng.bz/my6r](https://shortener.manning.com/my6r)) 上有使用 Idiomatic
    程序重用设计模式为 ResNeXt 编写的完整代码示例。
- en: 5.2.4 Xception
  id: totrans-101
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.4 Xception
- en: The current convention replaces a single 5 × 5 with two 3 × 3 convolutional
    layers. The Xception ([https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    stem component shown in figure 5.9 is an example. The first 3 × 3 convolution
    is strided (feature pooling) and produces 32 filters, and the second 3 × 3 convolution
    is not strided and doubles the number of output feature maps to 64\. However,
    outside of its novelty in academics, the architecture of Xception was not adopted
    in production nor developed further by subsequent researchers.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当前惯例是用两个 3 × 3 卷积层替换一个单独的 5 × 5 卷积层。图 5.9 中显示的 Xception ([https://arxiv.org/abs/1610.02357](https://arxiv.org/abs/1610.02357))
    基础组件是一个例子。第一个 3 × 3 卷积是步进（特征池化）的，并产生 32 个过滤器，第二个 3 × 3 卷积没有步进，将输出特征图的数量加倍到 64。然而，尽管在学术上具有新颖性，Xception
    的架构并未在生产中得到采用，也没有被后续研究人员进一步发展。
- en: '![](Images/CH05_F09_Ferlitsch.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F09_Ferlitsch.png)'
- en: Figure 5.9 Xception stem group
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 Xception 基础组件
- en: 'In this example, for coding the Xception stem group in the Idiomatic design
    pattern, you see the two 3 × 3 convolutions (refactored 5 × 5), with the first
    convolution strided (feature pooling). Both convolutions are followed by the Conv-BN-RE
    form of batch normalization:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，对于在 Idiomatic 设计模式中编码 Xception 基础组件，你可以看到两个 3 × 3 卷积（重构的 5 × 5），第一个卷积是步进的（特征池化）。这两个卷积都紧跟着
    Conv-BN-RE 形式的批量归一化：
- en: '[PRE4]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ A 5 × 5 convolution refactored as two 3 × 3 convolutions
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将 5 × 5 卷积重构为两个 3 × 3 卷积
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for Xception is on GitHub ([http://mng.bz/5WzB](https://shortener.manning.com/5WzB)).
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上使用Idiomatic程序重用设计模式对Xception进行完整代码实现（[http://mng.bz/5WzB](https://shortener.manning.com/5WzB)）。
- en: 5.3 Pre-stem
  id: totrans-109
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 预前缀
- en: In 2019, we started to see the emergence of adding a *pre-stem group* to the
    stem component. The purpose of a pre-stem is to move into the graph (model) some
    or all of the data preprocessing that was performed upstream. Before the development
    of the pre-stem component, the data preprocessing took place in a separate module
    and then had to be replicated when the model was deployed for inference (for prediction)
    on future examples. Generally, this was done on a CPU. Many of the data preprocessing
    steps can be replaced by graph operations, however, and then executed more efficiently
    on a GPU, where the model typically is deployed.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 2019年，我们开始看到在基组件中添加一个*预前缀组*的兴起。预前缀的目的在于将一些或所有上游执行的数据预处理移动到图（模型）中。在预前缀组件开发之前，数据预处理发生在单独的模块中，然后在模型部署到未来示例上进行推理（预测）时必须重复执行。通常，这是在CPU上完成的。然而，许多数据预处理步骤可以被图操作所替代，然后在通常部署模型的GPU上更有效地执行。
- en: 'Pre-stems are also plug-and-play in that they can be added or removed from
    existing models and reused. I’ll present the technical details of pre-stems later.
    Here I just want to provide a summary of functions typically performed by a pre-stem
    group:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 预前缀也是即插即用的，它们可以被添加或从现有模型中移除并重用。我将在稍后介绍预前缀的技术细节。在这里，我只是想提供一个预前缀组通常执行的功能的摘要：
- en: Preprocessing
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理
- en: Adapting a model to a different input size
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将模型适应不同的输入大小
- en: Normalization
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正规化
- en: Augmentation
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增强处理
- en: Resizing and cropping
  id: totrans-116
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整大小和裁剪
- en: Translational and scale invariance
  id: totrans-117
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平移和尺度不变性
- en: Figure 5.10 depicts how a pre-stem group is added to an existing model. To attach
    a pre-stem, you create a new empty wrapper model, add the pre-stem, and then add
    the existing model. In the latter step, the output shape from the pre-stem group
    must match the input shape of the stem component of the existing model.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10描述了如何向现有模型添加预前缀组。要附加预前缀，你需要创建一个新的空包装器模型，添加预前缀，然后添加现有模型。在后一个步骤中，预前缀组的输出形状必须与现有模型基组件的输入形状相匹配。
- en: '![](Images/CH05_F10_Ferlitsch.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F10_Ferlitsch.png)'
- en: Figure 5.10 Pre-stem added to an existing model, which forms a new wrapper model
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 向现有模型添加预前缀，形成新的包装器模型
- en: 'The following is an example of a typical approach to adding a pre-stem group
    to an existing model. In this code, an empty `Sequential` wrapper model is instantiated.
    The pre-stem group is then added, followed by the existing model. This will work
    as long as the output tensors match the input tensors of the model (for example,
    (224, 224, 3)):'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将预前缀组添加到现有模型的典型方法示例。在这段代码中，实例化了一个空的`Sequential`包装器模型。然后添加预前缀组，接着添加现有模型。只要输出张量与模型的输入张量匹配（例如，(224,
    224, 3)），这将有效。
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Creates an empty wrapper model
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建一个空包装器模型
- en: ❷ Starts the wrapper model with the pre-stem
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用预前缀启动包装器模型
- en: ❸ Adds the existing model to the wrapper model
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将现有模型添加到包装器模型
- en: Next we will explain the design of the learner component, which the stem component
    will connect to.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将解释学习组件的设计，该组件将连接到基组件。
- en: 5.4 Learner component
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 学习组件
- en: The *learner component* is where we generally perform feature learning through
    more-detailed feature extraction. This process is also referred to as *representational*
    or *transformational* learning (as transformational learning is dependent on the
    task). The learner component consists of one or more convolutional groups, and
    each group consists of one or more convolutional blocks.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: '*学习组件*是我们通常通过更详细的特征提取执行特征学习的地方。这个过程也被称为*表示性*或*转换性*学习（因为转换性学习依赖于任务）。学习组件由一个或多个卷积组组成，每个组由一个或多个卷积块组成。'
- en: The convolutional blocks get assembled into groups based on a common model configuration
    attribute. The most common attributes for convolutional groups in conventional
    CNNs are the number of input or output filters, or the size of the input or output
    feature maps. For example, in ResNet, the configurable attributes for a group
    are the number of convolutional blocks, and the number of filters per block.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 根据共同的模型配置属性，卷积块被组装成组。在传统的 CNN 中，卷积组的常见属性是输入或输出滤波器的数量，或输入或输出特征图的大小。例如，在 ResNet
    中，组的可配置属性是卷积块的数量，以及每个块的滤波器数量。
- en: Figure 5.11 shows a configurable convolutional group. As you can see, the convolutional
    blocks correspond to the metaparameter for the number of blocks in the group.
    All but the last group in most SOTA architectures have the same number of output
    feature maps, which corresponds to the metaparameter for the number of input filters.
    The last block may change the number of feature maps outputted by the group (for
    example, by doubling), which corresponds to the metaparameter for the number of
    output filters. The final layer (labeled *[Feature] pooling block* in the image)
    refers to groups that have delayed downsampling, which corresponds to the metaparameter
    for the type of pooling.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 展示了一个可配置的卷积组。如图所示，卷积块对应于组中块数量的元参数。大多数 SOTA 架构中除了最后一个组之外的所有组具有相同数量的输出特征图，这对应于输入滤波器数量的元参数。最后一个块可能会改变组输出的特征图数量（例如，加倍），这对应于输出滤波器数量的元参数。最外层（在图像中标有*[特征]池化块*）指的是具有延迟下采样的组，这对应于池化类型的元参数。
- en: '![](Images/CH05_F11_Ferlitsch.png)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F11_Ferlitsch.png)'
- en: Figure 5.11 Convolutional group metaparameters for number of input/output filters
    and output feature map size
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.11 卷积组元参数：输入/输出滤波器数量和输出特征图大小
- en: The following code is a skeleton template (and example) for coding the learner
    component. In this example, the configuration attribute for the groups is passed
    as a list of dictionary values, one per group. The `learner()` function iterates
    through the list of group configuration attributes; each iteration is the group
    parameters (`group_ params`) for the corresponding group.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码是编码学习组件的骨架模板（和示例）。在这个例子中，组的配置属性作为字典值列表传递，每个组一个值。`learner()` 函数遍历组配置属性列表；每次迭代对应于相应组的组参数（`group_params`）。
- en: Correspondingly, the `group()` function iterates through the block parameters
    (`block_params`) for each block in the group. The `block()` function then constructs
    the block according to the block-specific configuration parameters that have been
    passed down to it.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 相应地，`group()` 函数遍历组中每个块的块参数（`block_params`）。然后，`block()` 函数根据传递给它的特定于块的配置参数构建块。
- en: 'As depicted in figure 5.11, the configurable attributes passed to the `block()`
    method as keyword arguments lists would be the number of input filters (`in_filters`),
    output filters (`out_filters`), and number of convolutional layers (`n_layers`).
    If the number of input and output filters is the same, a single keyword argument
    typically is used (`n_filters`):'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.11 所示，传递给 `block()` 方法的可配置属性作为关键字参数列表将是输入滤波器数量（`in_filters`）、输出滤波器数量（`out_filters`）和卷积层数量（`n_layers`）。如果输入和输出滤波器数量相同，通常使用单个关键字参数（`n_filters`）：
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Iterates through the dictionary values for each group attribute
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历每个组属性的字典值
- en: ❷ Iterates through the dictionary values for each block attribute
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历每个块属性的字典值
- en: ❸ Assembles the learner component by specifying the number of groups and filters
    per group
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 通过指定组数和每个组的滤波器数量来组装学习组件
- en: 5.4.1 ResNet
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.1 ResNet
- en: In ResNet50, 101, and 151, the learner component consists of four convolutional
    groups. The first group uses a nonstrided convolutional layer for the projection
    shortcut in the first convolutional block, which takes input from the stem component.
    The other three convolutional groups use a strided convolutional layer (feature
    pooling) in the projection shortcut for the first convolutional block. Figure
    5.12 shows this arrangement.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 在 ResNet50、101 和 151 中，学习组件由四个卷积组组成。第一个组使用非步进卷积层作为第一个卷积块的投影捷径，该块从主干组件接收输入。其他三个卷积组在第一个卷积块的投影捷径中使用步进卷积层（特征池化）。图
    5.12 展示了这种配置。
- en: '![](Images/CH05_F12_Ferlitsch.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F12_Ferlitsch.png)'
- en: Figure 5.12 In the ResNet learner component, the first group starts with a nonstrided
    projection shortcut block.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.12 在ResNet学习器组件中，第一个组以一个非步长的投影快捷方式块开始。
- en: Now we’ll look at an example application of the skeleton template for coding
    the learner component of a ResNet50\. Note that in the `learner()` function, we
    popped off the first group of configuration attributes. In this application, we
    did this because the first group starts with a nonstrided projection shortcut
    residual block, while all the remaining groups use a strided projection shortcut.
    Alternatively, we could have used a configuration attribute to indicate whether
    the first residual block is strided or not, and eliminated the special case (coding
    a separate block construction).
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将查看一个使用ResNet50学习器组件的骨架模板的示例应用。请注意，在`learner()`函数中，我们移除了第一个组的配置属性。在这个应用中，我们这样做是因为第一个组以一个非步长的投影快捷方式残差块开始，而所有剩余的组使用步长投影快捷方式。或者，我们也可以使用配置属性来指示第一个残差块是否步长，并消除特殊情况（编码单独的块构建）。
- en: '[PRE7]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ First residual group is not strided.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 首个残差组没有步长。
- en: ❷ Remaining residual groups are strided convolutions.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 剩余的残差组使用步长卷积。
- en: While ResNets continue to be used today as a stock model for the image classification
    backbone, the 50-layer ResNet50, depicted in figure 5.13, is the standard. At
    50 layers, the model gives high accuracy at reasonable size and performance. The
    larger ResNets at 101 and 151 layers provide only minor increases in accuracy
    but at substantial increase in size and reduction in performance.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管ResNets今天仍然被用作图像分类骨干的标准模型，但如图5.13所示的50层ResNet50是标准。在50层时，模型在合理的大小和性能下提供了高精度。更大的101层和151层的ResNet在精度上只有轻微的增加，但大小显著增加，性能降低。
- en: '![](Images/CH05_F13_Ferlitsch.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F13_Ferlitsch.png)'
- en: Figure 5.13 In the ResNet group macro-architecture, the first block uses a projection
    shortcut, and the remaining blocks use an identity link.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.13 在ResNet组宏架构中，第一个块使用投影快捷方式，而剩余的块使用身份链接。
- en: Each group starts with a residual block with a linear projection shortcut, followed
    by one or more residual blocks with an identity shortcut. All the residual blocks
    in a group have the same number of output filters. Each group successively doubles
    the number of output filters, and the residual block with a linear projection
    shortcut doubles the number of filters from the input to the group.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组以一个具有线性投影快捷方式的残差块开始，后面跟着一个或多个具有身份快捷方式的残差块。组中的所有残差块具有相同数量的输出滤波器。每个组依次将输出滤波器的数量加倍，具有线性投影快捷方式的残差块将输入到组中的滤波器数量加倍。
- en: The ResNets (for example, 50, 101, 152) consist of four convolutional groups;
    the output filters for the four groups follow the doubling convention, starting
    at 64, then 128, 256, and finally 512\. The number convention (50) refers to the
    number of convolutional layers, which determines the number of convolutional blocks
    in each convolutional group.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ResNets（例如，50、101、152）由四个卷积组组成；四个组的输出滤波器遵循加倍惯例，从64开始，然后是128、256，最后是512。数字惯例（50）指的是卷积层的数量，这决定了每个卷积组中的卷积块数量。
- en: 'The following is an example application of the skeleton template for coding
    the convolutional group of a ResNet50\. For the `group()` function, we pop off
    the first block’s configuration attributes, which we know for a ResNet is a projection
    block, and then iterate through the remaining blocks as identity blocks:'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用ResNet50卷积组骨架模板的示例应用。对于`group()`函数，我们移除了第一个块的配置属性，我们知道对于ResNet来说这是一个投影块，然后迭代剩余的块作为身份块：
- en: '[PRE8]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ First block in residual group uses linear projection shortcut link.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 残差组中的第一个块使用线性投影快捷链接。
- en: ❷ Remaining blocks use an identity shortcut link.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 剩余的块使用身份快捷链接。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for ResNet is on GitHub ([http://mng.bz/7jK9](https://shortener.manning.com/7jK9)).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上有一个使用Idiomatic procedure reuse设计模式为ResNet编写的完整代码示例 ([http://mng.bz/7jK9](https://shortener.manning.com/7jK9))。
- en: 5.4.2 DenseNet
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.4.2 DenseNet
- en: The learner component in a DenseNet [(https://arxiv.org/abs/1608.06993)](https://arxiv.org/abs/1608.06993)
    consists of four convolutional groups, as shown in figure 5.14\. Each group, with
    the exception of the last group, delays pooling to the end of the group, in what
    is called the *transitional block*. The last convolutional group has no transitional
    block, since no group follows. The feature maps will be pooled and flattened by
    the task component, so it is unnecessary (redundant) to pool at the end of the
    group. This pattern of deferring final pooling in the last group to the task component
    continues to be a common convention today.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet中的学习组件[(https://arxiv.org/abs/1608.06993)](https://arxiv.org/abs/1608.06993)由四个卷积组组成，如图5.14所示。除了最后一个组外，每个组都将池化延迟到组末尾，这被称为*过渡块*。最后一个卷积组没有过渡块，因为没有后续的组。特征图将由任务组件进行池化和展平，因此不需要（冗余）在组末进行池化。这种将最终池化延迟到最后一个组到任务组件的模式，至今仍是一个常见的惯例。
- en: '![](Images/CH05_F14_Ferlitsch.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F14_Ferlitsch.png)'
- en: Figure 5.14 DenseNet learner component consists of four convolutional groups
    with delayed pooling.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.14 DenseNet学习组件由四个具有延迟池化的卷积组组成。
- en: 'The following is an example implementation of using the skeleton template for
    coding the learner component of a DenseNet. Note that we pop off the last group
    configuration attributes before iterating through the groups. We treat the last
    group as a special case, as the group does not end in a transition block. Alternatively,
    we could have used a configuration parameter to indicate whether or not a group
    contains a transition block, eliminating the special case (that is, coding a separate
    block construction). The parameter `reduction` specifies the amount of feature
    map size reduction during delayed pooling:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个使用骨架模板编码DenseNet学习组件的示例实现。请注意，我们在遍历组之前移除最后一个组配置属性。我们将最后一个组视为一个特殊情况，因为该组不以过渡块结束。或者，我们也可以使用配置参数来指示一个组是否包含过渡块，从而消除特殊情况（即编写单独的块构造）。参数`reduction`指定了延迟池化期间特征图大小减少的量：
- en: '[PRE9]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Pops off the last dense group parameters and saves for the end
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 移除最后一个密集组参数并保存以供结尾使用
- en: ❷ Constructs all but the last dense group with an interceding transition block
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用中间过渡块构建除最后一个密集组之外的所有密集组
- en: ❸ Adds the last dense group without a transition block
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在没有过渡块的最后一个密集组中添加
- en: Let’s look at a convolutional group in a DenseNet (figure 5.15). It consists
    of only two types of convolutional blocks. The first blocks are DenseNet blocks
    for feature learning, and the last block is a transitional block for reducing
    the size of the feature maps prior to the next group, which is referred to as
    the *compression factor*.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看DenseNet中的一个卷积组（图5.15）。它仅由两种类型的卷积块组成。第一个块是用于特征学习的DenseNet块，最后一个块是一个过渡块，用于在下一个组之前减小特征图的大小，这被称为*压缩因子*。
- en: '![](Images/CH05_F15_Ferlitsch.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F15_Ferlitsch.png)'
- en: Figure 5.15 A DenseNet group consists of a sequence of dense blocks and a final
    transition block for dimensionality reduction in outputted feature maps.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.15 DenseNet组由一系列密集块和一个用于在输出特征图中进行降维的最终过渡块组成。
- en: A DenseNet block is essentially a residual block, except that in place of adding
    (matrix add operation) the identity link to the output, it is concatenated. In
    a ResNet, the information from previous inputs is passed only one block forward.
    Using concatenation, the information from the feature maps accumulates, and each
    block passes all the accumulative information forward to all subsequent blocks.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: DenseNet块本质上是一个残差块，除了在输出处不添加（矩阵加法操作）恒等链接，而是进行连接。在ResNet中，先前输入的信息只向前传递一个块。使用连接，特征图的信息累积，每个块将所有累积的信息向前传递给所有后续块。
- en: This concatenation of feature maps would result in a continued growth in size
    of feature maps and corresponding parameters as we go deeper in layers. To control
    (reduce) the growth, the transitional block at the end of each convolutional block
    compresses (reduces) the size of the concatenated feature maps. Otherwise, without
    the reduction, the number of parameters to learn would grow substantially as we
    grow deeper, resulting in taking longer to train without a benefit in increased
    accuracy.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 这种特征图的连接会导致随着层数的加深，特征图的大小和相应的参数持续增长。为了控制（减少）增长，每个卷积块末尾的过渡块压缩（减小）了连接的特征图的尺寸。否则，如果没有缩减，随着层数的加深，需要学习的参数数量将显著增加，导致训练时间延长，而准确率没有提高。
- en: 'The following is an example implementation for coding a DenseNet convolutional
    group:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个编码 DenseNet 卷积组的示例实现：
- en: '[PRE10]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Constructs a group of densely connected residual blocks
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 构建一组密集连接的残差块
- en: ❷ Constructs interceding transition block
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 构建中间过渡块
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for DenseNet is on GitHub ([http://mng.bz/6N0o](https://shortener.manning.com/6N0o)).
    Next we will explain the design of the task component, which the learner component
    will connect to.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GitHub 上使用 Idiomatic 程序重用设计模式对 DenseNet 进行完整代码实现的示例是 ([http://mng.bz/6N0o](https://shortener.manning.com/6N0o))。接下来，我们将解释任务组件的设计，学习组件将连接到该组件。
- en: 5.5 Task component
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 任务组件
- en: 'The *task component* is where we generally perform task learning. In large
    conventional CNNs for image classification, this component typically consists
    of two layers:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*任务组件*是我们通常执行任务学习的地方。在用于图像分类的大规模传统 CNN 中，这个组件通常由两层组成：'
- en: '*Bottleneck layer*—Performs dimensionality reduction of final feature maps
    into latent space'
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*瓶颈层*——将最终特征图的维度缩减到潜在空间'
- en: '*Classifier layer*—Performs the task the model is learning'
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分类层*——执行模型正在学习的任务'
- en: The output from the learner component is the final reduced size of the feature
    maps (for example, 4 × 4 pixels). The bottleneck layer does the final dimensionality
    reduction of the final feature maps, which is then inputted to the classifier
    layer for classification.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 学习组件的输出是特征图的最终减小尺寸（例如，4 × 4 像素）。瓶颈层执行最终特征图的维度缩减，然后输入到分类层进行分类。
- en: For the remainder of this section, we will describe the task component in the
    context of an image classifier; we refer to this as the *classification component*.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节的剩余部分，我们将以图像分类器为例描述任务组件；我们将其称为 *分类组件*。
- en: 5.5.1 ResNet
  id: totrans-183
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.1 ResNet
- en: For a ResNet50, the number of feature maps is 2048\. The first layer in the
    classifier component is both flattening the feature maps into a 1D vector and
    reducing the size, using `GlobalAveragePooling2D`, for example. This flattening/reduction
    layer is also referred to as the bottleneck layer, as stated previously. Following
    the bottleneck layer is a `Dense` layer that does the classification.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 ResNet50，特征图的数目是 2048。分类组件的第一个层既将特征图展平成 1D 向量，又使用 `GlobalAveragePooling2D`
    等方法减小尺寸。这个展平/缩减层也被称为瓶颈层，如前所述。瓶颈层之后是一个 `Dense` 层，用于分类。
- en: Figure 5.16 depicts the ResNet50 classifier. The input to the classifier component
    is the final feature maps from the learner component (latent space), which is
    then passed through `GlobalAveragePooling2D`, which reduces the size of each feature
    map to a single pixel and flattens it into a 1D vector (bottleneck). The output
    from this bottleneck layer is passed through the `Dense` layer, where the number
    of nodes corresponds to the number of classes. The output is the probability distribution
    for all classes, squashed to add up to 100% by a softmax activation.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 描述了 ResNet50 分类器。分类组件的输入来自学习组件的最终特征图（潜在空间），然后通过 `GlobalAveragePooling2D`，将每个特征图的尺寸减小到单个像素，并将其展平成一个
    1D 向量（瓶颈）。从这个瓶颈层输出的内容通过 `Dense` 层，其中节点的数量对应于类别的数量。输出是所有类别的概率分布，通过 softmax 激活函数压缩，使其总和达到
    100%。
- en: '![](Images/CH05_F16_Ferlitsch.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F16_Ferlitsch.png)'
- en: Figure 5.16 ResNet classifier group
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.16 ResNet 分类组
- en: 'The following is an example of coding this approach to a classifier component,
    consisting of `GlobalAveragePooling2D` for flattening and dimensionality reduction,
    followed by the `Dense` layer for classification:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将此方法编码为分类组件的示例，包括用于展平和维度缩减的 `GlobalAveragePooling2D`，然后是用于分类的 `Dense` 层：
- en: '[PRE11]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Uses global average pooling to reduce and flatten the feature maps (latent
    space) into a 1D feature vector (bottleneck layer)
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用全局平均池化将特征图（潜在空间）减少并展平成一个1D特征向量（瓶颈层）
- en: ❷ The fully connected Dense layer for the final classification of the input
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于输入最终分类的完全连接的Dense层
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for ResNet is available on GitHub ([http://mng.bz/7jK9](https://shortener.manning.com/7jK9)).
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式为ResNet提供的完整代码版本可在GitHub上找到([http://mng.bz/7jK9](https://shortener.manning.com/7jK9))。
- en: 5.5.2 Multilayer output
  id: totrans-193
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.2 多层输出
- en: In earlier deployed ML production systems, models were treated as independent
    algorithms, and we would be interested only in the final output (the prediction).
    Today, we build not models, but applications that are an amalgamation, or composition,
    of models. As a result, we no longer treat the task component as a single output.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 在早期部署的机器学习生产系统中，模型被视为独立的算法，我们只对最终输出（预测）感兴趣。今天，我们构建的不是模型，而是由模型混合或组合而成的应用程序。因此，我们不再将任务组件视为单个输出。
- en: 'Instead, we see it as having four outputs, depending on how the model is connected
    to other models in the application. These outputs are as follows:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们认为它有四个输出，这取决于模型如何连接到应用程序中的其他模型。这些输出如下：
- en: Feature extraction
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取
- en: High dimensionality (encoding)
  id: totrans-197
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高维度（编码）
- en: Low dimensionality (embedding)—feature vector
  id: totrans-198
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 低维度（嵌入）—特征向量
- en: Prediction
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测
- en: Prediction pre-activation (probabilities)—soft targets
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测预激活（概率）—软目标
- en: Post-activation (outputs)—hard targets
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 激活后（输出）—硬目标
- en: Later chapters cover the purposes of these outputs (chapter 9 on autoencoders,
    chapter 11 on transfer learning, and chapter 14 for pretext tasks in training
    pipelines), and you will see that each layer in the classifier has two parallel
    outputs. In the multi-output of a conventional classifier depicted in figure 5.17,
    you can see that the input to the task component is also an independent output
    of the model, referred to as the *encoding*. The encoding is then passed through
    a global average pooling for dimensionality reduction, further reducing the size
    of the features extracted by the learner component. The output from the global
    average pooling is also an independent output of the model, referred to as the
    *embedding*.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 后续章节将介绍这些输出的目的（第9章关于自编码器，第11章关于迁移学习，第14章关于训练管道中的预训练任务），你将看到分类器中的每一层都有两个并行输出。在图5.17中描述的传统分类器的多输出中，你可以看到任务组件的输入也是模型的独立输出，被称为*编码*。编码随后通过全局平均池化进行降维，进一步减小学习组件提取的特征的大小。全局平均池化的输出也是模型的独立输出，被称为*嵌入*。
- en: '![](Images/CH05_F17_Ferlitsch.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F17_Ferlitsch.png)'
- en: Figure 5.17 Multi-output classifier group with four outputs—two for feature
    extraction sharing and two for probability distribution
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.17 多输出分类器组具有四个输出—两个用于特征提取共享，两个用于概率分布
- en: The embedding is then passed to a pre-activation dense layer (prior to the softmax
    activation). The output from the pre-activation layer is also an independent output
    of the model, referred to as the *pre-activation probability distribution*. This
    probability distribution is then passed through a softmax for the post-activation
    probability distribution, making the fourth independent output of the model. All
    these outputs can then be used by downstream tasks.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入随后传递到一个预激活的密集层（在softmax激活之前）。预激活层的输出也是模型的独立输出，被称为*预激活概率分布*。这个概率分布随后通过softmax得到激活后的概率分布，成为模型的第四个独立输出。所有这些输出都可以被下游任务使用。
- en: 'Let’s describe a simple real-world example for using a multi-output task component:
    estimating the cost of repairs from a photo of a vehicle. We want estimates on
    two categories: costs for minor damage like dings and scratches, and costs for
    major damage like collision damage. We could attempt to do this in a single task
    component that acts as a regressor to output a real value (dollar value), but
    we would really be overloading the task component during training because it’s
    learning both tiny values (minor damage) and large values (major damage). During
    training, the wide distribution in the values likely will keep the model from
    converging.'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们描述一个简单的现实世界示例，即使用多输出任务组件：从车辆照片中估计维修成本。我们希望对两个类别进行估计：轻微损坏（如凹痕和划痕）的成本，以及重大损坏（如碰撞损坏）的成本。我们可能会尝试在一个任务组件中完成这项工作，该组件作为回归器输出一个实值（美元值），但我们在训练期间实际上是在过度加载任务组件，因为它在学习很小的值（轻微损坏）和大的值（重大损坏）。在训练期间，值的广泛分布可能会阻止模型收敛。
- en: 'The approach is to solve this as two separate task components: one for minor
    and one for major damage. The minor damage task component will learn only tiny
    values, and the major damage task component will learn only large values—so both
    task components should converge during training.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法是将这个问题解决为两个独立的任务组件：一个用于轻微损坏，一个用于重大损坏。轻微损坏的任务组件将只学习很小的值，而重大损坏的任务组件将只学习大的值——因此，两个任务组件应该在训练过程中收敛。
- en: Next, we consider which output level we share with each of the two tasks. For
    the minor damage, we are looking at tiny objects. While we are not covering object
    detection, the historical problem with object classification with small objects
    was that the cropped feature maps after being pooled contained too little spatial
    information. The fix was to do the object classification from feature maps at
    an earlier convolution; the feature maps would then be of sufficient size, so
    that when a tiny object is cropped out, enough spatial information remains for
    object classification.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们考虑与两个任务共享哪个输出级别。对于轻微损坏，我们关注的是微小的物体。虽然我们没有涵盖物体检测，但历史上在小型物体上进行物体分类的问题在于，在池化后的裁剪特征图中包含的空间信息太少。解决方案是从更早的卷积层中的特征图进行物体分类；这样，特征图就会足够大，当裁剪出微小的物体时，仍然保留足够的空间信息以进行物体分类。
- en: We have a comparable issue in our example. For minor damage, the objects (each
    ding) will be very small, and we need larger feature maps to detect them. So for
    this purpose, we connect the high-dimensional encoding, prior to averaging and
    pooling, to the task that performs minor damage estimating. On the other hand,
    major collision damage does not require much detail. If the fender has a dent,
    it has to be replaced no matter the size or location of the dent, for instance.
    So for this purpose, we connect the low-dimensional embedding, after averaging
    and pooling, to the task that performs major damage estimating. Figure 5.18 illustrates
    this example.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中存在一个类似的问题。对于轻微的损坏，物体（每个凹痕）将会非常小，我们需要更大的特征图来检测它们。因此，为了这个目的，我们在平均和池化之前将高维编码连接到执行轻微损坏估计的任务。另一方面，重大的碰撞损坏不需要很多细节。例如，如果保险杠有凹痕，无论凹痕的大小或位置如何，都必须更换。因此，为了这个目的，我们在平均和池化之后将低维嵌入连接到执行重大损坏估计的任务。图5.18展示了这个例子。
- en: '![](Images/CH05_F18_Ferlitsch.png)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F18_Ferlitsch.png)'
- en: Figure 5.18 Multitask component using multi-outputs from shared model top for
    estimating vehicle repair costs
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18展示了使用共享模型顶部的多输出从多任务组件估计车辆维修成本
- en: 'The following is an example implementation of coding the multi-outputs to a
    classifier component. The feature extraction and prediction outputs are implemented
    by capturing the tensor inputs to each layer. At the end of the classifier, we
    replace returning a single output with returning a tuple of all four outputs:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是将多输出编码到分类组件的示例实现。特征提取和预测输出是通过捕获每一层的张量输入来实现的。在分类器的末尾，我们将返回单个输出替换为返回所有四个输出的元组：
- en: '[PRE12]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ High-dimensionality feature extraction (encoding)
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 高维特征提取（编码）
- en: ❷ Low-dimensionality feature extraction (embedding)
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 低维特征提取（嵌入）
- en: ❸ Pre-activation probabilities (soft labels)
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预激活概率（软标签）
- en: ❹ Post-activation probabilities (hard labels)
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 后激活概率（硬标签）
- en: ❺ Returns a tuple of all four outputs
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回所有四个输出的元组
- en: 5.5.3 SqueezeNet
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.5.3 SqueezeNet
- en: 'In compact models, particularly for mobile devices, `GlobalAveraging2D` followed
    by a `Dense` layer is replaced with a `Conv2D` using a softmax activation. The
    number of filters in `Conv2D` is set to the number of classes, and then followed
    by `GlobalAveraging2D` for the flattening into the number of classes. The “SqueezeNet”
    paper by Forrest Iandola et al. ([https://arxiv.org/pdf/1602.07360.pdf](https://arxiv.org/pdf/1602.07360.pdf))
    explains the reasoning for replacing the dense layer with a convolution layer:
    “Note the lack of fully connected layers in SqueezeNet; this design choice was
    inspired by the NiN (Lin et al., 2013) architecture.”'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在紧凑模型中，尤其是对于移动设备，`GlobalAveraging2D`后面跟着一个`Dense`层被使用softmax激活的`Conv2D`所取代。`Conv2D`中的滤波器数量设置为类别的数量，然后是`GlobalAveraging2D`以展平到类别的数量。"SqueezeNet"论文由Forrest
    Iandola等人撰写([https://arxiv.org/pdf/1602.07360.pdf](https://arxiv.org/pdf/1602.07360.pdf))，解释了用卷积层替换密集层的理由："注意SqueezeNet中没有全连接层；这个设计选择受到了NiN（Lin
    et al., 2013）架构的启发。"
- en: Figure 5.19 is a coding example of a SqueezeNet that uses this approach to the
    classifier component. SqueezeNet was developed in 2016 by DeepScale, the University
    of California at Berkeley, and Stanford University for mobile devices, and was
    SOTA at the time.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19是一个使用此方法对分类组件进行编码的SqueezeNet示例。SqueezeNet于2016年由DeepScale、加州大学伯克利分校和斯坦福大学为移动设备开发，当时是SOTA。
- en: '![](Images/CH05_F19_Ferlitsch.png)'
  id: totrans-222
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH05_F19_Ferlitsch.png)'
- en: Figure 5.19 SqueezeNet classifier group
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 SqueezeNet分类组件
- en: You can see that instead of a dense layer, it uses a 1 × 1 convolution, in which
    the number of filters corresponds to the number of classes (*C*). In this way,
    the 1 × 1 convolution is learning a probability distribution for the classes instead
    of a projection of the input feature maps. The resulting (*C*) feature maps are
    then each reduced into a single real value for the probability distribution and
    flattened into a 1D output vector. For example, if each feature map outputted
    by the 1 × 1 convolution is 3 × 3 in size (9 pixels), the pixel with the highest
    value is chosen as the probability for the corresponding class. The 1D vector
    is then squashed by a softmax activation so all the probabilities add up to 1.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到，它使用的是1 × 1卷积而不是密集层，其中滤波器的数量对应于类别的数量（*C*）。这样，1 × 1卷积正在学习类别的概率分布，而不是输入特征图的投影。然后得到的（*C*）个特征图被每个减少到一个单一的真实值用于概率分布，并展平成一个1D输出向量。例如，如果1
    × 1卷积输出的每个特征图大小为3 × 3（9像素），则选择值最高的像素作为对应类别的概率。然后1D向量通过softmax激活函数进行压缩，使得所有概率之和为1。
- en: Let’s contrast this to the global average pooling and dense layer approach we’ve
    discussed in large SOTA models. Let’s assume the size of the final feature maps
    is 3 × 3 (9 pixels). We then average the 9 pixels to a single value and do a probability
    distribution based on a single average value per feature map. In the method used
    by SqueezeNet, the convolutional layer that does the probability distribution
    sees the 9-pixel feature map (versus an averaged single pixel) and has more pixels
    to learn the probability distribution. This was likely chosen by SqueezeNet’s
    authors to offset the lesser amount of feature extraction/learning with a smaller
    model bottom.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们将其与我们之前在大规模SOTA模型中讨论的全局平均池化和密集层方法进行对比。假设最终特征图的大小为3 × 3（9像素）。然后我们将9个像素平均到一个值，并根据每个特征图的单个平均值进行概率分布。在SqueezeNet使用的方法中，执行概率分布的卷积层看到的是9像素的特征图（而不是平均的单个像素），并且有更多的像素来学习概率分布。这可能是SqueezeNet的作者为了补偿较小模型底部的较少特征提取/学习而做出的选择。
- en: 'The following is a sample for coding the SqueezeNet classifier component. In
    this example, the number of filters for the `Conv2D` is the number of classes
    (`n_classes`), which is then followed by `GlobalAveragePooling2D`. Since this
    layer is a static (not learned) layer, it has no activation parameter, so we must
    explicitly follow it with a softmax activation layer:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对SqueezeNet分类组件进行编码的示例。在这个例子中，`Conv2D`的滤波器数量是类别的数量（`n_classes`），然后是`GlobalAveragePooling2D`。由于这个层是一个静态层（未学习），它没有激活参数，因此我们必须明确地跟随一个softmax激活层：
- en: '[PRE13]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ Sets the number of filters equal to the number of output classes
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将滤波器的数量设置为输出类别的数量
- en: ❷ Reduces each feature map (class) to a single value (soft label)
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每个特征图（类别）减少到一个单一值（软标签）
- en: ❸ Uses softmax to squash all the class probabilities to add up to 100%
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用softmax将所有类别概率压缩，使其总和达到100%
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for SqueezeNet is located on GitHub ([http://mng.bz/XYmv](https://shortener.manning.com/XYmv)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式对SqueezeNet的完整代码实现位于GitHub上([http://mng.bz/XYmv](https://shortener.manning.com/XYmv))。
- en: '5.6 Beyond computer vision: NLP'
  id: totrans-232
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 超越计算机视觉：NLP
- en: As mentioned in chapter 1, the design patterns I explain in the context of computer
    vision have comparable principles and patterns in natural language processing
    and structured data. To see how a procedural design pattern can be applied to
    NLP, let’s look at an example from a type of NLP, natural-language understanding
    (NLU).
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 如第1章所述，我在计算机视觉的背景下解释的设计模式在自然语言处理和结构化数据中也有类似的原则和模式。为了了解过程设计模式如何应用于NLP，让我们看看自然语言理解（NLU）这类NLP的一个例子。
- en: 5.6.1 Natural-language understanding
  id: totrans-234
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 自然语言理解
- en: Let’s start by taking a look at the general model architecture for NLU in figure
    5.20\. In NLU, the model learns to understand the text and learns to perform a
    task based on that understanding. Examples of tasks include classifying the text,
    sentiment analysis, and entity extraction.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从查看图5.20中NLU的一般模型架构开始。在NLU中，模型学习理解文本，并基于这种理解执行任务。任务的例子包括对文本进行分类、情感分析和实体提取。
- en: '![](Images/CH05_F20_Ferlitsch.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F20_Ferlitsch.png)'
- en: Figure 5.20 As with all deep learning models, NLU models consist of stem, learner,
    and task components. The differences lie inside each component.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 与所有深度学习模型一样，NLU模型由词干、学习者和任务组件组成。不同之处在于每个组件内部。
- en: We might be classifying medical documents by type; for example, identifying
    each as a prescription, doctor’s note, claim submission, or other document. For
    sentiment analysis, the task might be determining whether a review was favorable
    or unfavorable (binary classification) or ranking from unfavorable to favorable
    (multiclass classification). For entity extraction, our task might be extracting
    health vitals from lab results and doctor/nurse notes.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可能会根据类型对医疗文档进行分类；例如，识别每个文档是处方、医生笔记、索赔提交或其他文档。对于情感分析，任务可能是确定评论是正面还是负面（二分类）或从负面到正面的排名（多分类）。对于实体提取，我们的任务可能是从实验室结果和医生/护士笔记中提取健康指标。
- en: 'An NLU model is decomposed into the same components that make up all deep learning
    models: stem, learner, and task. The differences lie in what happens in each component.'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一个NLU模型被分解成所有深度学习模型都包含的相同组件：词干、学习者和任务。不同之处在于每个组件中发生的事情。
- en: In an NLU model, the stem consists of an encoder. Its purpose is to convert
    the string representation of the text into a numeric-based vector, referred to
    as an *embedding*. This embedding is of a higher dimensionality than the string
    input and contains richer contextual information about the words, or characters,
    or sentences. The stem encoder is actually another model that has been pretrained.
    Think of the stem encoder as a dictionary. For each word, the low dimensionality,
    it outputs all the possible meanings, the high dimensionality. A common example
    of an embedding is a vector of *N* dimensions, in which each element represents
    another word, and the value indicates how closely this word is related to the
    other word.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个NLU模型中，词干由一个编码器组成。其目的是将文本的字符串表示转换为基于数字的向量，称为*嵌入*。这个嵌入的维度比字符串输入更高，并包含关于单词、字符或句子的更丰富的上下文信息。词干编码器实际上是一个已经预训练的另一个模型。将词干编码器想象成一个字典。对于每个单词，它输出所有可能的含义，从低维度到高维度。嵌入的一个常见例子是*N*维度的向量，其中每个元素代表另一个单词，其值表示这个单词与其他单词的相关程度。
- en: Next, the embeddings are passed to the learner component. In an NLU model, the
    learner consists of one or more encoder groups, which consist, in turn, of one
    or more encoder blocks. Each of these blocks is based on a design pattern, such
    as an attention block in a transformer model, and the assembly of the blocks and
    groups is based on design principles for encoder patterns.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，嵌入被传递到学习器组件。在一个NLU模型中，学习器由一个或多个编码器组组成，这些组又由一个或多个编码器块组成。每个这些块都基于一个设计模式，例如在转换器模型中的注意力块，而块和组的组装基于编码器模式的设计原则。
- en: You probably notice that both the stem and learner refer to an encoder. *They
    are not the same type of encoder in each component.* Having the same name for
    two different things can be a bit confusing, so I’ll clarify. When we talk about
    the encoder that generates embeddings, we will refer to it as the *stem encoder*
    ; otherwise, we are referring to the encoder in the learner.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，茎和学习者都指的是编码器。*它们在每个组件中不是同一种类型的编码器。* 两个不同的事物使用相同的名称可能会有些令人困惑，所以我将进行澄清。当我们谈论生成嵌入的编码器时，我们将称之为*茎编码器*；否则，我们指的是学习者中的编码器。
- en: The purpose of the encoder in the learner is to convert the embeddings to a
    lower-dimensionality representation of the meaning of the text, which is called
    an *intermediate representation*. This is comparable to learning the essential
    features in an image in a CNN.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 学习者中编码器的目的是将嵌入转换为文本意义的低维表示，这被称为*中间表示*。这与在CNN中学习图像的基本特征相似。
- en: The task component is remarkably similar to the computer vision counterpart.
    The intermediate representation is flattened into a 1D vector and pooled. The
    pooled representation, for classification and semantic analysis, is passed to
    a softmax dense layer to predict a probability distribution across the classes
    or semantic rankings.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 任务组件与计算机视觉的对应组件非常相似。中间表示被展平成一个一维向量并进行了池化。对于分类和语义分析，池化表示被传递到一个softmax密集层，以预测跨类或语义排名的概率分布。
- en: 'As for entity extraction, the task component is comparable to the task component
    for an object detection model; you are learning two tasks: classifying the extracted
    entity, and fine-tuning the location boundary in the text of the extracted entity.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 至于实体提取，任务组件与对象检测模型的任务组件相当；你正在学习两个任务：对提取的实体进行分类，以及在提取实体的文本中微调位置边界。
- en: 5.6.2 Transformer architecture
  id: totrans-246
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 Transformer架构
- en: 'Let’s now look at another aspect of modern (NLU) models that’s comparable to
    the SOTA in computer vision. As mentioned in chapter 1, a major change in NLU
    occurred with the introduction of the Transformer model architecture by Google
    Brain in 2017, and the corresponding paper “Attention is All You Need” by Ashishh
    Vaswani et al. ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)).
    The Transformer architecture addressed a challenging problem in NLU: how to handle
    text sequences that are essentially comparable to a time-series—that is, the meaning
    is dependent on the sequence ordering of the words. Previously to the Transformer
    architecture, NLU models were implemented as recurrent neural networks (RNNs),
    which would retain the sequence ordering of the text and learn the importance
    (long memory) or non-importance (short memory) of the words.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一下现代（NLU）模型的一个方面，它与计算机视觉中的SOTA相当。如第1章所述，NLU的一个重大变化是在2017年谷歌大脑引入Transformer模型架构以及相应的论文“Attention
    is All You Need”由Ashishh Vaswani等人发表([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762))。Transformer架构解决了NLU中的一个难题：如何处理本质上类似于时间序列的文本序列——即，意义依赖于单词的序列顺序。在Transformer架构之前，NLU模型被实现为循环神经网络（RNNs），这些网络会保留文本的序列顺序并学习单词的重要性（长记忆）或非重要性（短记忆）。
- en: What the Transformer model did is introduce a new mechanism called *attention*
    that transformed NLU models from a time-series to a spatial model. Instead of
    looking at words, or characters, or sentences as a sequence, we take a chunk of
    words and represent them spatially, like an image. The model learns to extract
    essential context—the features. The attention mechanism acts similarly to the
    identity link in a residual network. It adds attention—weight—to the contexts
    that are more important.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer模型所做的是引入了一种新的机制，称为*注意力*，它将NLU模型从时间序列转换为空间模型。我们不再将单词、字符或句子视为序列，而是将一组单词作为一个块来表示，就像图像一样。模型学习提取关键上下文——特征。注意力机制在残差网络中的身份链接作用相似。它为更重要的上下文添加了注意力——权重。
- en: Figure 5.21 shows an attention block in a transformer architecture. The input
    to the block is a set of context maps, comparable to feature maps, from the previous
    block. The attention mechanism adds weight to portions of the context block that
    are more important to the contextual understanding (indicating to pay attention
    here). The attention context maps are then passed to a feed-forward layer that
    outputs the next set of context maps.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21展示了在转换器架构中的一个注意力块。该块输入是一组来自前一个块的上下文图，类似于特征图。注意力机制为对上下文理解更为重要的上下文块部分添加权重（表示在此处需要关注）。然后，注意力上下文图被传递到一个前馈层，该层输出下一组上下文图。
- en: '![](Images/CH05_F21_Ferlitsch.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH05_F21_Ferlitsch.png)'
- en: Figure 5.21 An attention block adds weight to the portion of the context that
    is more important to the understanding of the text.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.21 一个注意力块为对文本理解更为重要的上下文部分添加权重。
- en: In the next chapter, we will cover wide convolutional neural networks, a design
    pattern that focuses on wider layers instead of deeper layers.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将介绍宽卷积神经网络，这是一种关注较宽层而不是较深层的架构模式。
- en: Summary
  id: totrans-253
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Using a design pattern to design and code a CNN makes models more understandable,
    saves time, ensures that the model represents best SOTA practices, and is easy
    for others to reproduce.
  id: totrans-254
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用设计模式来设计和编码卷积神经网络可以使模型更易于理解，节省时间，确保模型代表了最佳SOTA实践，并且易于他人复现。
- en: The procedural design pattern uses the software engineering principle of reuse
    that is widely practiced by software engineers.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 程序设计模式使用了软件工程中的重用原则，这是软件工程师广泛实践的原则。
- en: The macro-architecture consists of stem, learner, and task components that define
    the flow through the model and where/what type of learning occurs.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宏架构由主干、学习者和任务组件组成，这些组件定义了模型中的流程以及在哪里/进行何种类型的学习。
- en: The micro-architecture consists of group and block design patterns that define
    how the model performs learning.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 微架构由定义模型如何执行学习的组和块设计模式组成。
- en: The purpose of a pre-stem group is to extend existing (pretrained) models for
    upstream data preprocessing, image augmentation, and adaptations to other deployed
    environments. Implementing pre-stems as plug-and-play provides ML ops to deploy
    models without accompanying upstream code.
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理组的目的在于扩展现有的（预训练）模型以用于上游数据预处理、图像增强以及对其他部署环境的适应。将预处理器作为即插即用模块实现，为机器学习操作提供了部署模型而不需要附带上游代码的能力。
- en: The purpose of the task component is to learn a model-specific task from the
    latent space, encoding learning during feature extraction and representational
    learning.
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任务组件的目的是从潜在空间中学习一个特定于模型的任务，编码在特征提取和表示学习过程中的学习。
- en: The purpose of a multiple-layer output is to extend interconnectivity between
    models for complex tasks in the most efficient way while maintaining performance
    objectives.
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多层输出的目的是以最有效的方式扩展模型之间的互连性，同时保持性能目标。
- en: The attention mechanism in a transformer provided the method to sequentially
    learn the essential features in a manner comparable to computer vision, without
    the need for a recurrent network.
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在转换器中的注意力机制提供了以类似于计算机视觉的方式按顺序学习关键特征的方法，而不需要循环网络。
