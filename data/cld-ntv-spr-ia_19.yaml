- en: 15 Continuous delivery and GitOps
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 15 持续交付和GitOps
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Understanding continuous delivery and release management
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解持续交付和发布管理
- en: Configuring Spring Boot for production with Kustomize
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Kustomize配置Spring Boot以供生产使用
- en: Deploying in production with GitOps and Kubernetes
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用GitOps和Kubernetes进行生产部署
- en: Chapter after chapter, we have gone through patterns, principles, and best practices
    for working with cloud native applications, and we’ve built a bookshop system
    using Spring Boot and Kubernetes. It’s time to deploy Polar Bookshop to production.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 一章又一章，我们已经探讨了与云原生应用一起工作的模式、原则和最佳实践，并使用Spring Boot和Kubernetes构建了一个书店系统。现在是时候将Polar书店部署到生产环境中了。
- en: I expect you have the projects of the Polar Bookshop system in separate Git
    repositories stored on GitHub. If you haven’t followed along in the previous chapters,
    you can refer to the Chapter15/15-begin folder in the source code accompanying
    the book, and use it as a foundation to define those repositories.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我预计您已经将Polar书店系统的项目存储在GitHub上的单独Git仓库中。如果您没有跟随前面的章节，可以参考书中源代码的Chapter15/15-begin文件夹，并以此为基础定义这些仓库。
- en: This chapter will guide you through some final aspects of preparing applications
    for production. First I’ll discuss versioning strategies for release candidates
    and how to design the acceptance stage of a deployment pipeline. Then you’ll see
    how to configure Spring Boot applications for production and deploy them on a
    Kubernetes cluster in a public cloud. Next, I’ll show you how to complete the
    deployment pipeline by implementing the production stage. Finally, you’ll use
    Argo CD to implement continuous deployment based on the GitOps principles.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将指导您了解准备生产应用的一些最终方面。首先，我将讨论发布候选人的版本控制策略以及如何设计部署管道的验收阶段。然后，您将了解如何配置Spring Boot应用以供生产使用，并在公共云中的Kubernetes集群上部署它们。接下来，我将向您展示如何通过实现生产阶段来完善部署管道。最后，您将使用Argo
    CD根据GitOps原则实现持续部署。
- en: Note The source code for the examples in this chapter is available in the Chapter15/15-begin
    and Chapter15/15-end folders, containing the initial and final states of the project
    ([https://github.com/ThomasVitale/cloud-native-spring-in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action)).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：本章示例的源代码可在Chapter15/15-begin和Chapter15/15-end文件夹中找到，包含项目的初始和最终状态（[https://github.com/ThomasVitale/cloud-native-spring-in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action)）。
- en: '15.1 Deployment pipeline: Acceptance stage'
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.1 部署管道：验收阶段
- en: 'Continuous delivery is one of the fundamental practices we have identified
    that can support us in our journey to achieve the cloud native goals: speed, resilience,
    scale, and cost optimization. It’s a holistic approach for delivering high-quality
    software quickly, reliably, and safely. The main idea behind continuous delivery
    is that an application is always in a releasable state. The primary pattern for
    adopting continuous delivery is the deployment pipeline, which goes from code
    commit to releasable software. It should be automated as much as possible and
    represent the only path to production.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 持续交付是我们确定的支持我们实现云原生目标（速度、弹性、可扩展性和成本优化）的基本实践之一。这是一种全面的方法，用于快速、可靠和安全地交付高质量软件。持续交付背后的主要思想是，应用程序始终处于可发布状态。采用持续交付的主要模式是部署管道，它从代码提交到可发布软件。应尽可能自动化，并代表通往生产的唯一路径。
- en: 'Chapter 3 explained that a deployment pipeline can be composed of three key
    stages: commit stage, acceptance stage, and production stage. Throughout the book,
    we have automated the commit stage as a workflow in GitHub Actions. After a developer
    commits new code to the mainline, this stage goes through build, unit tests, integration
    tests, static code analysis, and packaging. At the end of this stage, an executable
    application artifact is published to an artifact repository. That is a *release
    candidate*.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 第3章解释说，部署管道可以由三个关键阶段组成：提交阶段、验收阶段和生产阶段。在整个书中，我们已经将提交阶段自动化为GitHub Actions中的一个工作流程。开发者在主线提交新代码后，此阶段将经历构建、单元测试、集成测试、静态代码分析和打包。在这个阶段的最后，一个可执行的应用程序工件被发布到工件仓库。这被称为*发布候选*。
- en: This section will cover how we can version release candidates for continuous
    delivery. Then you’ll learn more about the acceptance stage, its purpose, and
    its outcome. Finally, I’ll show you how to implement a minimal workflow in GitHub
    Actions for the acceptance stage. At the end of this stage, the release candidate
    will be ready to be deployed to production.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本节将介绍我们如何为持续交付版本发布候选版本进行版本控制。然后你将了解更多关于接受阶段、其目的和结果。最后，我将向你展示如何在 GitHub Actions
    中实现接受阶段的简化工作流程。在这个阶段结束时，发布候选版本将准备好部署到生产环境。
- en: 15.1.1 Versioning release candidates for continuous delivery
  id: totrans-13
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.1 为持续交付版本发布候选版本
- en: The output of the commit stage of the deployment pipeline is a release candidate.
    That’s the deployable artifact for an application. In our case, it’s a container
    image. All the subsequent steps in the pipeline will evaluate the quality of that
    container image through different tests. If no issue is found, the release candidate
    is ultimately deployed to production and released to users.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管道的提交阶段输出是一个发布候选版本。这是应用程序的可部署工件。在我们的例子中，它是一个容器镜像。管道中的所有后续步骤将通过不同的测试来评估该容器镜像的质量。如果没有发现任何问题，发布候选版本最终将被部署到生产环境并发布给用户。
- en: A release candidate is stored in an artifact repository. If it’s a JAR, it would
    be stored in a Maven repository. In our case, it’s a container image and will
    be stored in a container registry. In particular, we’ll use GitHub Container Registry.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 发布候选版本存储在工件存储库中。如果它是一个 JAR 文件，它将被存储在 Maven 存储库中。在我们的例子中，它是一个容器镜像，并将存储在容器注册库中。特别是，我们将使用
    GitHub 容器注册库。
- en: Each release candidate must be uniquely identified. So far, we have used the
    implicit latest tag for all container image versions. Also, we ignored the 0.0.1-SNAPSHOT
    version configured in each Spring Boot project by default in Gradle. How should
    we version release candidates?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 每个发布候选版本都必须具有唯一标识。到目前为止，我们已经为所有容器镜像版本使用了隐式的最新标签。此外，我们还忽略了 Gradle 中每个 Spring
    Boot 项目默认配置的 0.0.1-SNAPSHOT 版本。我们应该如何对发布候选版本进行版本控制？
- en: A popular strategy is *semantic versioning* ([https://semver.org](https://semver.org)).
    It consists of identifiers in the form of <major>.<minor>.<patch>. Optionally,
    you can also add a hyphen at the end, followed by a string, marking a pre-release.
    By default, a Spring Boot project generated from Spring Initializr ([https://start.spring.io](https://start.spring.io))
    is initialized with version 0.0.1-SNAPSHOT, which identifies a snapshot release.
    A variation of this strategy is *calendar versioning* ([https://calver.org](https://calver.org)),
    which combines the concepts of semantic versioning with date and time.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 一个流行的策略是*语义版本控制* ([https://semver.org](https://semver.org))。它由形式为 <major>.<minor>.<patch>
    的标识符组成。可选地，你还可以在末尾添加一个连字符，后跟一个字符串，标记为预发布版本。默认情况下，从 Spring Initializr ([https://start.spring.io](https://start.spring.io))
    生成的 Spring Boot 项目初始化为版本 0.0.1-SNAPSHOT，这标识了一个快照发布。这种策略的变体是*日历版本控制* ([https://calver.org](https://calver.org))，它将语义版本控制的概念与日期和时间相结合。
- en: Both those strategies are broadly used for open source projects and software
    released as products to customers, because they provide implicit information about
    what a new release contains. For example, we expect a new major version to contain
    new functionality and API changes incompatible with the previous major version.
    On the other hand, we would expect a patch to have a limited scope and guarantee
    backward compatibility.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种策略都广泛用于开源项目和作为产品发布给客户的软件，因为它们提供了关于新发布包含内容的隐含信息。例如，我们期望新的大版本包含新的功能和对前一个大版本不兼容的
    API 变更。另一方面，我们预计补丁将具有有限的范围并保证向后兼容性。
- en: Note If you’re working on software projects for which semantic versioning makes
    sense, I recommend checking out JReleaser, a release automation tool. “Its goal
    is to simplify creating releases and publishing artifacts to multiple package
    managers while providing customizable options” ([https://jreleaser.org](https://jreleaser.org)).
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你正在从事适合语义版本控制的软件项目，我建议查看 JReleaser，一个发布自动化工具。“其目标是简化创建发布并将工件发布到多个包管理器的过程，同时提供可定制的选项”
    ([https://jreleaser.org](https://jreleaser.org))。
- en: 'Semantic versioning will require some form of manual step to assign a version
    number based on the content of the release artifact: Does it contain breaking
    changes? Does it only contain bug fixes? When we have a number, it’s still not
    clear what’s included in the new release artifact, so we need to use Git tags
    and define a mapping between Git commit identifiers and version numbers.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 语义版本控制将需要某种形式的手动步骤来根据发布实物的内容分配版本号：它是否包含破坏性更改？它是否仅包含错误修复？当我们有一个数字时，新发布实物的具体内容仍然不清楚，因此我们需要使用Git标签并定义Git提交标识符和版本号之间的映射。
- en: Things get even more challenging for snapshot artifacts. Let’s consider a Spring
    Boot project as an example. By default, we start with version 0.0.1-SNAPSHOT.
    Until we’re ready to cut the 0.0.1 release, every time we push new changes to
    the main branch, the commit stage will be triggered, and a new release candidate
    will be published with the number 0.0.1-SNAPSHOT. All release candidates will
    have the same number until version 0.0.1 is released. This approach doesn’t ensure
    traceability of changes. Which commits are included in release candidate 0.0.1-SNAPSHOT?
    We can’t tell. Furthermore, it’s affected by the same unreliability as using latest.
    Any time we retrieve the artifact, it might be different from the last time.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 对于快照工件来说，情况变得更加复杂。让我们以Spring Boot项目为例。默认情况下，我们以版本0.0.1-SNAPSHOT开始。直到我们准备好发布0.0.1版本，每次我们将新更改推送到主分支时，都会触发提交阶段，并发布一个新的带有编号0.0.1-SNAPSHOT的发布候选。所有发布候选都将具有相同的编号，直到版本0.0.1发布。这种方法并不能确保变更的可追溯性。哪些提交包含在发布候选0.0.1-SNAPSHOT中？我们无法得知。此外，它还受到与使用最新版本相同的不可靠性影响。每次我们检索工件时，它可能与上次不同。
- en: When it comes to continuous delivery, using an approach like semantic versioning
    is not ideal for uniquely identifying release candidates. When we follow the principles
    of continuous integration, we’ll have many release candidates built daily. And
    every release candidate can potentially be promoted to production. Will we have
    to update the semantic version for each new code commit, with a different approach
    based on its content (major, minor, patch)? The path from code commit to production
    should be automated as much as possible, trying to eliminate manual intervention.
    If we go with continuous deployment, even the promotion to production will happen
    automatically. What should we do?
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到持续交付时，使用类似于语义版本控制的方法来唯一标识发布候选并不理想。当我们遵循持续集成的原则时，我们每天都会构建许多发布候选。而且每个发布候选都有可能被提升到生产环境。我们是否需要为每个新的代码提交更新语义版本，根据其内容（主要、次要、补丁）采用不同的方法？从代码提交到生产的路径应该尽可能地自动化，试图消除人工干预。如果我们采用持续部署，甚至提升到生产的过程也将自动完成。我们应该怎么做？
- en: One solution would be using the Git commit hash to version release candidates—that
    would be automated, traceable, and reliable, and you wouldn’t need Git tags. You
    could use the commit hash as is (for example, 486105e261cb346b87920aaa4ea6dce6eebd6223)
    or use it as the base for generating a more human-friendly number. For example,
    you could prefix it with a timestamp or with an increasing sequence number, with
    the goal of making it possible to tell which release candidate is the newest (for
    example, 20220731210356-486105e261cb346b87920aaa4ea6dce6eebd6223).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是使用Git提交哈希来版本发布候选——这将自动化、可追踪且可靠，而且你不需要Git标签。你可以直接使用提交哈希（例如，486105e261cb346b87920aaa4ea6dce6eebd6223）或者将其作为生成更易于人类阅读的数字的基础。例如，你可以在其前面加上时间戳或递增的序列号，目的是使人们能够判断哪个发布候选是最新的（例如，20220731210356-486105e261cb346b87920aaa4ea6dce6eebd6223）。
- en: Still, semantic versioning and similar strategies have their place in continuous
    delivery. They can be used as *display names* in addition to the unique identifier,
    as Dave Farley suggests in his book *Continuous Delivery Pipelines* (2021). That
    would be a way to provide users with information about the release candidate while
    still making it possible to benefit from continuous delivery.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管如此，语义版本控制和类似策略在持续交付中仍有其位置。除了唯一的标识符外，它们还可以作为*显示名称*使用，正如戴夫·法雷利在其著作《持续交付管道》（2021年）中建议的那样。这将是一种在提供有关发布候选信息的同时，仍然能够从持续交付中受益的方法。
- en: For Polar Bookshop, we’ll adopt a simple solution and use the Git commit hash
    directly to identify our release candidates. Therefore, we’ll ignore the version
    number configured in the Gradle project (which could instead be used as the display
    version name). For example, a release candidate for Catalog Service would be ghcr.io/
    <your_github_username>/catalog-service:<commit-hash>.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 对于极地书店（Polar Bookshop），我们将采用简单的解决方案，直接使用 Git 提交哈希来识别我们的发布候选版本。因此，我们将忽略在 Gradle
    项目中配置的版本号（它可以用作显示版本名称）。例如，目录服务的发布候选版本将是 ghcr.io/<你的GitHub用户名>/catalog-service:<commit-hash>。
- en: Now that we have a strategy, let’s see how we can implement it for Catalog Service.
    Go to your Catalog Service project (catalog-service), and open the commit-stage.yml
    file within the .github/workflows folder. We previously defined a VERSION environment
    variable to hold the release candidate’s unique identifier. At the moment, it’s
    statically set to latest. Let’s replace that with ${{ github.sha }}, which will
    be dynamically resolved to the current Git commit hash by GitHub Actions. For
    convenience, we’ll also add the latest tag to the newest release candidate, which
    is useful for local development scenarios.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经有了策略，让我们看看我们如何为目录服务（Catalog Service）实现它。前往你的目录服务项目（catalog-service），并在
    .github/workflows 文件夹中打开 commit-stage.yml 文件。我们之前定义了一个 VERSION 环境变量来保存发布候选版本的唯一标识符。目前，它被静态设置为最新版本。让我们将其替换为
    ${{ github.sha }}，这将由 GitHub Actions 动态解析为当前的 Git 提交哈希。为了方便，我们还将最新的标签添加到最新的发布候选版本中，这在本地开发场景中非常有用。
- en: Listing 15.1 Using the Git commit hash to version release candidates
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.1 使用 Git 提交哈希来版本发布候选版本
- en: '[PRE0]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Publishes a release candidate with a version equal to the Git commit hash
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 发布一个版本等于 Git 提交哈希的发布候选版本
- en: ❷ Adds the “latest” tag to the newest release candidate
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为最新的发布候选版本添加“latest”标签
- en: After updating the workflow, commit your changes and push them to GitHub. That
    will trigger the execution of the commit stage workflow (figure 15.1). The outcome
    will be a container image published to GitHub Container Registry, versioned with
    the current Git commit hash and the additional latest tag.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 更新工作流程后，提交你的更改并将其推送到 GitHub。这将触发提交阶段工作流程的执行（图15.1）。结果将是一个发布到 GitHub 容器注册表的容器镜像，版本号为当前的
    Git 提交哈希和额外的最新标签。
- en: '![15-01](../Images/15-01.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![15-01](../Images/15-01.png)'
- en: Figure 15.1 The commit stage goes from code commit to release candidate published
    to an artifact repository.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.1 提交阶段从代码提交到发布候选版本发布到工件存储库。
- en: Once the pipeline is executed successfully, you’ll be able to see the newly
    published container image from your catalog-service repository main page on GitHub.
    In the sidebar you’ll find a Packages section with a “catalog-service” item. Click
    that, and you’ll be directed to the container repository for Catalog Service (figure
    15.2). When using the GitHub Container Registry, container images are stored next
    to the source code, which is very convenient.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦管道成功执行，你将能够在 GitHub 上你的 catalog-service 仓库主页上看到新发布的容器镜像。在侧边栏中，你会找到一个包含“catalog-service”项目的“包”部分。点击它，你将被引导到目录服务的容器存储库（图15.2）。当使用
    GitHub 容器注册表时，容器镜像存储在源代码旁边，这非常方便。
- en: '![15-02](../Images/15-02.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![15-02](../Images/15-02.png)'
- en: Figure 15.2 In our case, release candidates are container images published to
    GitHub Container Registry.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.2 在我们的情况下，发布候选版本是发布到 GitHub 容器注册表的容器镜像。
- en: At this point, the container image (our release candidate) is uniquely identified
    and ready to go through the acceptance stage. That’s the topic of the next section.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个阶段，容器镜像（我们的发布候选版本）已经具有唯一标识并准备进入验收阶段。这就是下一节的主题。
- en: 15.1.2 Understanding the acceptance stage of the deployment pipeline
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.2 理解部署管道的验收阶段
- en: The acceptance stage of the deployment pipeline is triggered whenever a new
    release candidate is published to the artifact repository at the end of the commit
    stage. It consists of deploying the application to a production-like environment
    and running additional tests to increase the confidence in its releasability.
    The tests that run in the acceptance stage are usually slow, but we should strive
    to keep the whole deployment pipeline’s execution under one hour.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管道的验收阶段在提交阶段结束时，每当新的发布候选版本发布到工件存储库时都会被触发。它包括将应用程序部署到类似生产的环境并运行额外的测试，以提高其可发布性的信心。在验收阶段运行的测试通常很慢，但我们应努力将整个部署管道的执行时间控制在一个小时以内。
- en: In chapter 3, you learned about the software test classification provided by
    the *Agile Testing Quadrants* (figure 15.3). The quadrants classify software tests
    based on whether they are technology or business facing, and whether they support
    development teams or are used to critique the project.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在第3章中，你学习了*敏捷测试象限*（图15.3）提供的软件测试分类。象限根据测试是否面向技术或业务，以及是否支持开发团队或用于批评项目来对软件测试进行分类。
- en: '![15-03](../Images/15-03.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![15-03](../Images/15-03.png)'
- en: Figure 15.3 The Agile Testing Quadrants are a taxonomy helpful for planning
    a software testing strategy.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.3 敏捷测试象限是规划软件测试策略的有用分类法。
- en: In the commit stage, we mainly focus on the first quadrant, including unit and
    integration tests. They are technology-facing tests that support the team, ensuring
    they build the *software right*. On the other hand, the acceptance stage focuses
    on the second and fourth quadrants and tries to eliminate the need for manual
    regression testing. This stage includes functional and non-functional acceptance
    tests.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在提交阶段，我们主要关注第一象限，包括单元和集成测试。它们是面向技术的测试，支持团队，确保他们构建的*软件正确*。另一方面，接受阶段关注第二和第四象限，并试图消除手动回归测试的需求。这一阶段包括功能和非功能性接受测试。
- en: The *functional acceptance tests* are business-facing tests that support development
    teams, ensuring they are building the *right software*. They take on the user
    perspective and are usually implemented via *executable specifications* using
    a high-level domain-specific language (DSL), which is then translated into a lower-level
    programming language. For example, you could use Cucumber ([https://cucumber.io](https://cucumber.io))
    to write scenarios like “browse the book catalog” or “place a book order” in human-friendly
    plain text. Those scenarios can then be executed and verified using a programming
    language like Java.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*功能接受测试*是面向业务的测试，支持开发团队，确保他们正在构建*正确的软件*。它们从用户的角度出发，通常通过使用高级领域特定语言（DSL）的*可执行规范*来实现，然后将其翻译成低级编程语言。例如，您可以使用Cucumber
    ([https://cucumber.io](https://cucumber.io))用人类友好的纯文本编写“浏览图书目录”或“下订单”等场景。然后，可以使用Java等编程语言执行和验证这些场景。'
- en: In the acceptance stage, we can also verify the *quality attributes* of a release
    candidate via *non-functional acceptance tests*. For example, we could run performance
    and load tests using a tool like Gatling ([https://gatling.io](https://gatling.io)),
    security and compliance tests, and resilience tests. In this last case, we could
    embrace *chaos engineering*, a discipline made popular by Netflix and consisting
    of making certain parts of the system fail to verify how the rest will react and
    how resilient the system is to failures. For Java applications, you can look at
    Chaos Monkey for Spring Boot ([https://codecentric.github.io/chaos-monkey-spring-boot](https://codecentric.github.io/chaos-monkey-spring-boot)).
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在接受阶段，我们还可以通过*非功能性接受测试*来验证候选版本的*质量属性*。例如，我们可以使用Gatling ([https://gatling.io](https://gatling.io))等工具运行性能和负载测试，安全性和合规性测试，以及弹性测试。在最后一种情况下，我们可以采用*混沌工程*，这是一种由Netflix推广的学科，包括使系统的一部分失败，以验证其余部分如何反应以及系统对失败的弹性。对于Java应用程序，您可以查看Spring
    Boot的Chaos Monkey ([https://codecentric.github.io/chaos-monkey-spring-boot](https://codecentric.github.io/chaos-monkey-spring-boot))。
- en: Note How about the third quadrant? Following the continuous delivery principles,
    we strive not to include manual tests in the deployment pipeline. Yet we usually
    need them. They are particularly important for software products aimed at end
    users like web and mobile applications. Therefore, we run them on the side in
    the form of *exploratory testing* and *usability testing*, so that we ensure more
    freedom for testers and fewer constraints on the pace and timing required by continuous
    integration and the deployment pipeline.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 注意关于第三象限的问题？遵循持续交付的原则，我们努力不在部署管道中包含手动测试。然而，我们通常需要它们。对于面向最终用户（如网页和移动应用）的软件产品尤其重要。因此，我们以*探索性测试*和*可用性测试*的形式在旁边运行它们，以确保测试人员有更多的自由度，并对持续集成和部署管道所需的节奏和时间有更少的限制。
- en: An essential feature of the acceptance stage is that all tests are run against
    a production-like environment to ensure the best reliability. The deployment would
    follow the same procedure and scripts as production and could be tested via dedicated
    system tests (first quadrant).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 接受阶段的一个基本特征是，所有测试都是在类似生产环境的情况下运行的，以确保最佳可靠性。部署将遵循与生产相同的程序和脚本，并且可以通过专门的系统测试（第一象限）进行测试。
- en: If a release candidate passes all the tests in the acceptance stage, that means
    it’s in a *releasable* state and can be delivered and deployed to production.
    Figure 15.4 illustrates inputs and outputs for the commit and acceptance stages
    in a deployment pipeline.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 如果候选发布版本在接受阶段通过所有测试，这意味着它处于*可发布*状态，可以被交付并部署到生产环境中。图15.4说明了部署管道中提交和接受阶段的输入和输出。
- en: '![15-04](../Images/15-04.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![15-04](../Images/15-04.png)'
- en: Figure 15.4 The commit stage goes from code commit to a release candidate, which
    then goes through the acceptance stage. If it passes all the tests, it’s ready
    for production.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.4 提交阶段从代码提交到候选发布版本，然后通过接受阶段。如果它通过所有测试，它就准备好投入生产。
- en: 15.1.3 Implementing the acceptance stage with GitHub Actions
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.1.3 使用GitHub Actions实现接受阶段
- en: In this section, you’ll see how to implement the skeleton of a workflow for
    the acceptance stage using GitHub Actions. Throughout the book we’ve focused on
    unit and integration tests, which we run in the commit stage. For the acceptance
    stage, we would need to write functional and non-functional acceptance tests.
    That’s out of scope for this book, but I still want to show you some principles
    for designing the workflow using Catalog Service as an example.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，您将了解如何使用GitHub Actions实现接受阶段的流程框架。在本书中，我们一直关注单元和集成测试，这些测试在提交阶段运行。对于接受阶段，我们需要编写功能性和非功能性接受测试。这超出了本书的范围，但我仍然想通过以目录服务为例，向您展示一些设计工作流程的原则。
- en: Open your Catalog Service project (catalog-service), and create a new acceptance-stage.yml
    file within the .github/workflows folder. The acceptance stage is triggered whenever
    a new release candidate is published to the artifact repository. One option for
    defining such a trigger is listening for the events published by GitHub whenever
    the commit stage workflow has completed a run.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您的目录服务项目（catalog-service），在 .github/workflows 文件夹中创建一个新的 acceptance-stage.yml
    文件。每当新发布候选版本发布到工件存储库时，就会触发接受阶段。定义此类触发器的一个选项是监听GitHub在提交阶段工作流程完成运行时发布的事件。
- en: Listing 15.2 Triggering the acceptance stage after the commit stage is done
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.2 在提交阶段完成后触发接受阶段
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ The name of the workflow
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 工作流程的名称
- en: ❷ This workflow is triggered when the Commit Stage workflow completes a run.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当提交阶段工作流程完成运行时，此工作流程被触发。
- en: ❸ This workflow runs only on the main branch.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 此工作流程仅在主分支上运行。
- en: However, that’s not enough. Following the continuous integration principles,
    developers commit often during the day and repeatedly trigger the commit stage.
    Since the commit stage is much faster than the acceptance stage, we risk creating
    a bottleneck. When an acceptance stage run has completed, we are not interested
    in verifying all the release candidates that have queued up in the meantime. We
    are only interested in the newest one, so the others can be discarded. GitHub
    Actions provides a mechanism for handling this scenario via concurrency controls.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这还不够。遵循持续集成原则，开发者一天中会频繁提交，并反复触发提交阶段。由于提交阶段比接受阶段快得多，我们可能会创建一个瓶颈。当接受阶段运行完成后，我们并不感兴趣验证在此期间排队的所有候选发布版本。我们只对最新的一个感兴趣，所以其他的可以丢弃。GitHub
    Actions通过并发控制机制提供了一个处理这种场景的方法。
- en: Listing 15.3 Configuring concurrency for the workflow execution
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.3 配置工作流程执行的并发性
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Ensures that only one workflow runs at a time
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 确保一次只运行一个工作流程
- en: Next, you would define several jobs to run in parallel against a production-like
    environment, accomplishing functional and non-functional acceptance tests. For
    our example, we’ll simply print a message, since we haven’t implemented the autotests
    for this stage.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将定义几个并行运行在类似生产环境中的作业，以完成功能性和非功能性接受测试。在我们的例子中，我们只是简单地打印一条消息，因为我们还没有实现此阶段的自动测试。
- en: Listing 15.4 Running functional and non-functional acceptance tests
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.4 运行功能性和非功能性接受测试
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The job runs only if the commit stage completed successfully.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 只有当提交阶段成功完成后，作业才会运行。
- en: Note The acceptance tests could be run against a staging environment that closely
    resembles production. The application could be deployed using the staging overlay
    we configured in the previous chapter.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：接受测试可以针对与生产环境非常相似的阶段环境运行。可以使用我们在上一章中配置的阶段覆盖来部署应用程序。
- en: At this point, push your changes to your GitHub catalog-service repository,
    and have a look at how GitHub first runs the commit stage workflow (triggered
    by your code commit) and then the acceptance stage workflow (triggered by the
    commit stage workflow completing successfully). Figure 15.5 shows the result of
    the acceptance stage workflow’s execution.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，将你的更改推送到你的GitHub catalog-service仓库，并查看GitHub首先如何运行提交阶段的工作流程（由你的代码提交触发），然后是验收阶段的工作流程（由提交阶段工作流程成功完成触发）。图15.5显示了验收阶段工作流程的执行结果。
- en: '![15-05](../Images/15-05.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![15-05](../Images/15-05.png)'
- en: Figure 15.5 The commit stage goes from code commit to a release candidate, which
    then goes through the acceptance stage. If it passes all the tests, it’s ready
    for production.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.5 提交阶段从代码提交到发布候选者，然后通过验收阶段。如果它通过了所有测试，它就准备好投入生产了。
- en: Polar Labs
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Polar Labs
- en: It’s time to apply what you learned in this section to Edge Service, Dispatcher
    Service, and Order Service.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将你在本节中学到的知识应用到边缘服务、调度服务和订单服务上了。
- en: Update the commit stage workflow so that each release candidate is uniquely
    identified.
  id: totrans-73
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 更新提交阶段的工作流程，以便每个发布候选者都能唯一标识。
- en: Push your changes to GitHub, ensure that the workflow is completed successfully,
    and check that a container image is published to GitHub Container Registry.
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的更改推送到GitHub，确保工作流程成功完成，并检查是否已将容器镜像发布到GitHub容器注册库。
- en: Create an acceptance stage workflow, push your changes to GitHub, and verify
    that it is triggered correctly once the commit stage workflow is completed.
  id: totrans-75
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个验收阶段的工作流程，将你的更改推送到GitHub，并验证在提交阶段的工作流程完成后是否正确触发。
- en: In the source code repository accompanying the book, you can check the final
    result in the Chapter15/15-end folder ([https://github.com/ThomasVitale/cloud-native-spring
    -in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action)).
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书附带的源代码仓库中，你可以在Chapter15/15-end文件夹中检查最终结果（[https://github.com/ThomasVitale/cloud-native-spring-in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action)）。
- en: Deploying to production requires a combination of a release candidate and its
    configuration. Now that we’ve validated that a release candidate is ready for
    production, it’s time to customize its configuration.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 部署到生产环境需要发布候选者及其配置的组合。现在我们已经验证了发布候选者已准备好投入生产，是时候定制其配置了。
- en: 15.2 Configuring Spring Boot for production
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.2 为生产配置Spring Boot
- en: We’re getting closer and closer to deploying cloud native applications to a
    Kubernetes cluster in production. So far we have worked with local clusters using
    minikube. We now need a full-fledged Kubernetes cluster for our production environment.
    Before you continue reading this section, follow the instructions in appendix
    B (sections B.1 through B.6) to initialize a Kubernetes cluster on the DigitalOcean
    public cloud. You’ll also find some tips if you want to use a different cloud
    provider.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 我们越来越接近在生产环境中将云原生应用程序部署到Kubernetes集群。到目前为止，我们一直在使用minikube进行本地集群的工作。现在我们需要一个完整的Kubernetes集群用于我们的生产环境。在你继续阅读本节之前，请按照附录B（B.1至B.6节）中的说明在DigitalOcean公共云上初始化一个Kubernetes集群。如果你想使用不同的云提供商，你也会找到一些提示。
- en: Once you have a Kubernetes cluster up and running in the cloud, you can continue
    reading this section, which will cover the additional configuration we need to
    provide our Spring Boot applications with before deploying them to the production
    environment.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你在云中启动并运行了一个Kubernetes集群，你就可以继续阅读本节，它将涵盖在将Spring Boot应用程序部署到生产环境之前我们需要提供的额外配置。
- en: In the previous chapter, you learned about Kustomize and the overlay technique
    for managing customizations for different deployment environments on top of a
    common base. You also tried your hand at customizing the Catalog Service deployment
    for a staging environment. In this section we’ll do something similar for production.
    Extending what you saw in chapter 14, I’ll show you how to customize volume mounts
    for ConfigMaps and Secrets. Also, you’ll see how to configure the CPU and memory
    for containers running in Kubernetes, and you’ll learn more about how the Paketo
    Buildpacks manage resources for the Java Virtual Machine (JVM) within each container.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了Kustomize和overlay技术，用于在公共基础之上管理不同部署环境的自定义配置。你还尝试了为预发布环境定制目录服务部署。在本节中，我们将为生产环境做类似的事情。扩展第14章中你看到的内容，我将向你展示如何为ConfigMaps和Secrets自定义卷挂载。你还将了解如何为在Kubernetes中运行的容器配置CPU和内存，并了解Paketo
    Buildpacks如何管理每个容器内Java虚拟机（JVM）的资源。
- en: 15.2.1 Defining a configuration overlay for production
  id: totrans-82
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.1 定义生产环境的配置覆盖
- en: First we need to define a new overlay to customize the deployment of Catalog
    Service for a production environment. As you’ll probably remember from the previous
    chapter, the Kustomization base for Catalog Service is stored in the catalog-service
    repository. We keep the overlays in the polar-deployment repository.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要定义一个新的覆盖层以自定义生产环境中的目录服务部署。正如你可能从上一章中记得的那样，目录服务的Kustomization基础存储在catalog-service仓库中。我们将覆盖层保存在polar-deployment仓库中。
- en: Go ahead and create a new “production” folder within kubernetes/applications/
    catalog-service (in the polar-deployment repository). We’ll use it to store all
    customizations related to the production environment. Any base or overlay requires
    a kustomization.yml file, so let’s create one for the production overlay. Remember,
    in the following listing, to replace <your_github_username> with your GitHub username
    in lowercase. Also, replace <release_sha> with the unique identifier associated
    with your latest release candidate for Catalog Service. You can retrieve that
    version from the Packages section of your catalog-service GitHub repository main
    page.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在kubernetes/applications/catalog-service（在polar-deployment仓库中）内创建一个新的“production”文件夹。我们将使用它来存储与生产环境相关的所有自定义设置。任何基础或覆盖都需要一个kustomization.yml文件，因此让我们为生产覆盖创建一个。记住，在以下列表中，将<your_github_username>替换为你的GitHub用户名（小写）。同时，将<release_sha>替换与你的目录服务最新发布候选版本关联的唯一标识符。你可以从你的catalog-service
    GitHub仓库主页的软件包部分检索该版本。
- en: Listing 15.5 Defining an overlay for production on top of a remote base
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.5 在远程基础之上定义生产环境的覆盖
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The git commit hash (sha) identifying your latest release candidate
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 识别你最新发布候选的git提交哈希（sha）
- en: Note I’ll assume that all the GitHub repositories you created for Polar Bookshop
    are publicly accessible. If that’s not the case, you can go to the specific repository
    page on GitHub and access the Settings section for that repository. Scroll to
    the bottom of the settings page, and make the package public by clicking the Change
    Visibility button.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：我将假设你为Polar Bookshop创建的所有GitHub仓库都是公开可访问的。如果不是这样，你可以转到GitHub上的特定仓库页面，并访问该仓库的设置部分。滚动到设置页面的底部，通过点击更改可见性按钮使包公开。
- en: Customizing environment variables
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 定制环境变量
- en: The first customization we’ll apply is an environment variable to activate the
    prod Spring profile for Catalog Service. Following the same approach as in the
    previous chapter, create a patch-env.yml file within the production overlay for
    Catalog Service (kubernetes/applications/catalog-service/production).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将应用的第一项自定义设置是一个环境变量，用于激活目录服务的prod Spring配置文件。遵循与上一章相同的方法，在目录服务的生产覆盖层（kubernetes/applications/catalog-service/production）内创建一个patch-env.yml文件。
- en: Listing 15.6 A patch for customizing environment variables in a container
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.6 在容器中定制环境变量的补丁
- en: '[PRE5]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Defines which Spring profiles should be activated
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义应激活哪些Spring配置文件
- en: Next, we need to instruct Kustomize to apply the patch. In the kustomization.yml
    file for the production overlay of Catalog Service, list the patch-env.yml file
    as follows.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要指导Kustomize应用补丁。在目录服务的生产覆盖层的kustomization.yml文件中，如下列出patch-env.yml文件。
- en: Listing 15.7 Getting Kustomize to apply the patch for environment variables
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.7 让Kustomize应用环境变量补丁
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Section containing the list of patches to apply, according to the strategic
    merge strategy
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 包含要应用补丁列表的章节，根据战略合并策略
- en: ❷ The patch for customizing the environment variables passed to the Catalog
    Service container
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为目录服务容器传递的环境变量定制的补丁
- en: Customizing Secrets and volumes
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 定制密钥和卷
- en: In the previous chapter, you learned how to define ConfigMaps and Secrets, and
    you saw how to mount them as volumes to Spring Boot containers. In the base Kustomization
    we have no Secrets configured, since we are relying on the same default values
    from development. In production we need to pass different URLs and credentials
    to make it possible for Catalog Service to access the PostgreSQL database and
    Keycloak.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了如何定义ConfigMaps和Secrets，并看到了如何将它们作为卷挂载到Spring Boot容器中。在基础Kustomization中，我们没有配置任何Secrets，因为我们依赖于开发中的相同默认值。在生产中，我们需要传递不同的URL和凭证，以便目录服务能够访问PostgreSQL数据库和Keycloak。
- en: When you set up the production environment on DigitalOcean earlier, you also
    created a Secret with the credentials to access the PostgreSQL database (polar-postgres-catalog-credentials)
    and another for Keycloak (keycloak-issuer-resourceserver-secret). Now we can mount
    them as volumes to the Catalog Service container, similar to what we did with
    ConfigMaps in chapter 14\. We’ll do that in a dedicated patch.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当您之前在 DigitalOcean 上设置生产环境时，您还创建了一个包含访问 PostgreSQL 数据库凭据的秘密（polar-postgres-catalog-credentials）以及另一个用于
    Keycloak 的秘密（keycloak-issuer-resourceserver-secret）。现在我们可以将它们作为卷挂载到目录服务容器上，类似于我们在第
    14 章中处理 ConfigMaps 的方式。我们将在一个专门的补丁中这样做。
- en: Create a patch-volumes.yml file within the production overlay for Catalog Service
    (kubernetes/applications/catalog-service/production), and configure the patch
    as shown in listing 15.8\. When Kustomize applies this patch to the base deployment
    manifests, it will merge the ConfigMap volume defined in the base with the Secret
    volumes defined in the patch.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在目录服务（kubernetes/applications/catalog-service/production）的生产覆盖层中创建一个 patch-volumes.yml
    文件，并按照清单 15.8 中的配置设置补丁。当 Kustomize 将此补丁应用到基础部署清单时，它将合并基础中定义的 ConfigMap 卷和补丁中定义的秘密卷。
- en: Listing 15.8 Mounting Secrets as volumes to the Catalog Service container
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 将秘密作为卷挂载到目录服务容器的清单 15.8
- en: '[PRE7]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Mounts the volume with the Secret containing the PostgreSQL credentials
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将包含 PostgreSQL 凭据的秘密挂载到卷上
- en: ❷ Mounts the volume with the Secret containing the Keycloak issuer URL
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将包含 Keycloak 发起者 URL 的秘密挂载到卷上
- en: ❸ Defines a volume from the Secret containing the PostgreSQL credentials
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义一个包含 PostgreSQL 凭据的秘密卷
- en: ❹ Defines a volume from the Secret containing the Keycloak issuer URL
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 定义一个包含 Keycloak 发起者 URL 的密钥的卷
- en: Then, just like you learned in the previous section, we need to reference the
    patch in the kustomization.yml file for the production overlay.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，就像您在上一节中学到的，我们需要在用于生产覆盖层的 kustomization.yml 文件中引用该补丁。
- en: Listing 15.9 Getting Kustomize to apply the patch for mounting Secrets
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 15.9 让 Kustomize 应用挂载秘密的补丁
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Defines a patch for mounting Secrets as volumes
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个挂载秘密为卷的补丁
- en: Currently, the Secrets are configured to be provided to the container, but Spring
    Boot is not aware of them yet. In the next section I’ll show you how to instruct
    Spring Boot to load those Secrets as config trees.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，秘密被配置为提供给容器，但 Spring Boot 还没有意识到它们。在下一节中，我将向您展示如何指导 Spring Boot 加载这些秘密作为配置树。
- en: Customizing ConfigMaps
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义 ConfigMaps
- en: The base Kustomization for Catalog Service instructs Kustomize to generate a
    catalog-config ConfigMap starting with an application.yml file. As you learned
    in the previous chapter, we can ask Kustomize to add an additional file to that
    same ConfigMap, application-prod.yml, which we know takes precedence over the
    base application.yml file. That’s how we’re going to customize the application
    configuration for production.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 目录服务的基础 Kustomization 指导 Kustomize 从应用程序.yml 文件开始生成一个 catalog-config ConfigMap。正如您在上一章中学到的，我们可以要求
    Kustomize 向同一个 ConfigMap 添加一个额外的文件，即 application-prod.yml，我们知道它优先于基础应用程序.yml 文件。这就是我们将如何为生产定制应用程序配置。
- en: First, create an application-prod.yml file within the production overlay for
    Catalog Service (kubernetes/applications/catalog-service/production). We’ll use
    this property file to configure a custom greeting. We also need to instruct Spring
    Boot to load the Secrets as config trees, using the spring.config.import property.
    For more information on config trees, refer to chapter 14.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，在目录服务（kubernetes/applications/catalog-service/production）的生产覆盖层中创建一个 application-prod.yml
    文件。我们将使用这个属性文件来配置自定义问候语。我们还需要指导 Spring Boot 使用 spring.config.import 属性加载秘密作为配置树。有关配置树的更多信息，请参阅第
    14 章。
- en: Listing 15.10 Production-specific configuration for Catalog Service
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 清单 15.10 目录服务特定的生产配置
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Imports configuration from the path where volumes with Secrets are mounted.
    Make sure you include the final slash, or the import will fail.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从挂载秘密卷的路径导入配置。确保包含最后的斜杠，否则导入将失败。
- en: Next, we can rely on the ConfigMap Generator provided by Kustomize to combine
    the application-prod.yml file (defined in the production overlay) with the application.yml
    file (defined in the base Kustomization), within the same catalog-config ConfigMap.
    Go ahead and update the kustomization.yml file for the production overlay as follows.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以依赖 Kustomize 提供的 ConfigMap 生成器，将生产覆盖层中定义的应用程序-prod.yml 文件（定义在基础 Kustomization
    中）与应用程序.yml 文件（定义在基础 Kustomization 中）结合到同一个 catalog-config ConfigMap 中。请更新生产覆盖层的
    kustomization.yml 文件如下。
- en: Listing 15.11 Merging property files within the same ConfigMap
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.11 在同一 ConfigMap 内合并属性文件
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Merges this ConfigMap with the one defined in the base Kustomization
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将此 ConfigMap 与在基本 Kustomization 中定义的 ConfigMap 合并
- en: ❷ The additional property file added to the ConfigMap
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 添加到 ConfigMap 的附加属性文件
- en: ❸ The same ConfigMap name used in the base Kustomization
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在基本 Kustomization 中使用的相同 ConfigMap 名称
- en: Customizing image name and version
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义镜像名称和版本
- en: The next step is updating the image name and version, following the same procedure
    we used in the previous chapter. This time we’ll be able to use a proper version
    number for the container image (our release candidate).
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是更新镜像名称和版本，按照我们在上一章中使用的相同程序。这次我们将能够为容器镜像使用适当的版本号（我们的发布候选）。
- en: 'First, make sure you have the kustomize CLI installed on your computer. You
    can refer to the instructions at [https://kustomize.io](https://kustomize.io).
    If you’re on macOS or Linux, you can install kustomize with the following command:
    brew install kustomize.'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，请确保您已在计算机上安装了 kustomize CLI。您可以参考[https://kustomize.io](https://kustomize.io)上的说明。如果您使用的是
    macOS 或 Linux，可以使用以下命令安装 kustomize：brew install kustomize。
- en: 'Then open a Terminal window, navigate to the production overlay for Catalog
    Service (kubernetes/applications/catalog-service/production), and run the following
    command to define which image and version to use for the catalog-service container.
    Remember to replace <your_github_username> with your GitHub username in lowercase.
    Also, replace <sha> with the unique identifier associated with your latest release
    candidate for Catalog Service. You can retrieve that version from the Packages
    section of your catalog-service GitHub repository main page:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，打开一个终端窗口，导航到目录服务（kubernetes/applications/catalog-service/production）的生产覆盖层，并运行以下命令以定义用于目录服务容器的镜像和版本。请记住用小写替换
    <your_github_username> 为您的 GitHub 用户名。同时，用 <sha> 替换与您最新的目录服务发布候选关联的唯一标识符。您可以从
    catalog-service GitHub 仓库主页的“软件包”部分检索该版本：
- en: '[PRE11]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This command will automatically update the kustomization.yml file with the new
    configuration, as you can see in the following listing.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令将自动更新 kustomization.yml 文件，以包含新的配置，如下所示。
- en: Listing 15.12 Configuring the image name and version for the container
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.12 配置容器镜像名称和版本
- en: '[PRE12]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The name of the container as defined in the Deployment manifest
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 Deployment 清单中定义的容器名称
- en: ❷ The new image name for the container (with your GitHub username in lowercase)
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 容器的新镜像名称（包含您的小写 GitHub 用户名）
- en: ❸ The new tag for the container (with your release candidate’s unique identifier)
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 容器的新标签（包含您的发布候选的唯一标识符）
- en: Note Images published to GitHub Container Registry will have the same visibility
    as the related GitHub code repository. I’ll assume that all the images you build
    for Polar Bookshop are publicly accessible via the GitHub Container Registry.
    If that’s not the case, you can go to the specific repository page on GitHub and
    access the Packages section for that repository. Then select Package Settings
    from the sidebar menu, scroll to the bottom of the settings page, and make the
    package public by clicking the Change Visibility button.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：发布到 GitHub Container Registry 的镜像将具有与相关 GitHub 代码仓库相同的可见性。我将假设您为 Polar Bookshop
    构建的镜像都可通过 GitHub Container Registry 公共访问。如果不是这样，您可以访问 GitHub 上的特定仓库页面，并进入该仓库的“软件包”部分。然后从侧边栏菜单中选择“软件包设置”，滚动到设置页面的底部，通过点击“更改可见性”按钮使软件包公开。
- en: 'Currently we use the release candidate’s unique identifier in two places: the
    URL for the remote base and the image tag. Whenever a new release candidate is
    promoted to production, we need to remember to update both of them. Even better,
    we should automate the update. I’ll describe that later when we implement the
    production stage of the deployment pipeline.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们在两个地方使用发布候选的唯一标识符：远程基本 URL 和镜像标签。每当新的发布候选被提升到生产环境时，我们需要记住更新这两个标识符。更好的是，我们应该自动化更新。我将在实现部署管道的生产阶段时描述这一点。
- en: Customizing the number of replicas
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义副本数量
- en: Cloud native applications are supposed to be highly available, but only one
    instance of Catalog Service is deployed by default. Similar to what we did for
    the staging environment, let’s customize the number of replicas for the application.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 云原生应用应该具有高可用性，但默认情况下只部署了 Catalog Service 的一个实例。类似于我们对预发布环境所做的那样，让我们自定义应用程序的副本数量。
- en: Open the kustomization.yml file within the production overlay for Catalog Service
    (kubernetes/applications/catalog-service/production) and define two replicas for
    the catalog-service container.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 打开 Catalog Service（kubernetes/applications/catalog-service/production）的生产覆盖层中的
    kustomization.yml 文件，并为 catalog-service 容器定义两个副本。
- en: Listing 15.13 Configuring the number of replicas for the container
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.13 配置容器的副本数量
- en: '[PRE13]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ The name of the Deployment you’re defining the number of replicas for
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 你正在定义副本数量的部署的名称
- en: ❷ The number of replicas
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 副本的数量
- en: Note In a real scenario, you would probably want Kubernetes to dynamically scale
    applications in and out depending on the current workload, rather than providing
    a fixed number. Dynamic scaling is a pivotal feature of any cloud platform. In
    Kubernetes, it’s implemented by a dedicated component called Horizontal Pod Autoscaler
    based on well-defined metrics, such as the CPU consumption per container. For
    more information, refer to the Kubernetes documentation ([https://kubernetes.io/docs](https://kubernetes.io/docs)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在实际场景中，你可能希望 Kubernetes 根据当前的工作负载动态地扩展和缩减应用程序，而不是提供一个固定的数量。动态扩展是任何云平台的关键特性。在
    Kubernetes 中，它通过一个名为水平 Pod 自动缩放器（Horizontal Pod Autoscaler）的专用组件来实现，该组件基于定义良好的指标，例如每个容器的
    CPU 消耗。有关更多信息，请参阅 Kubernetes 文档（[https://kubernetes.io/docs](https://kubernetes.io/docs)）。
- en: The next section will cover configuring CPU and memory for Spring Boot containers
    running in Kubernetes.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将介绍如何在 Kubernetes 中为运行 Spring Boot 容器配置 CPU 和内存。
- en: 15.2.2 Configuring CPU and memory for Spring Boot containers
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.2 配置 Spring Boot 容器的 CPU 和内存
- en: When dealing with containerized applications, it’s best to assign *resource
    limits* explicitly. In chapter 1 you learned that containers are isolated contexts
    leveraging Linux features, like namespaces and cgroups, to partition and limit
    resources among processes. However, suppose you don’t specify any resource limits.
    In that case, each container will have access to the whole CPU set and memory
    available on the host machine, with the risk of some of them taking up more resources
    than they should and causing other containers to crash due to a lack of resources.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理容器化应用程序时，最好明确分配 *资源限制*。在第 1 章中，你了解到容器利用 Linux 功能，如命名空间和 cgroups，来隔离进程的上下文，并在进程之间划分和限制资源。然而，如果你没有指定任何资源限制，那么每个容器都将能够访问主机机器上可用的全部
    CPU 集和内存，这可能导致其中一些容器占用比应有的更多资源，并导致其他容器由于资源不足而崩溃。
- en: For JVM-based applications like Spring Boot, defining CPU and memory limits
    is even more critical because they will be used to properly size items like JVM
    thread pools, heap memory, and non-heap memory. Configuring those values has always
    been a challenge for Java developers, and it’s critical since they directly affect
    application performance. Fortunately, if you use the Paketo implementation of
    Cloud Native Buildpacks included in Spring Boot, you don’t need to worry about
    that. When you packaged the Catalog Service application with Paketo in chapter
    6, a *Java Memory Calculator* component was included automatically. When you run
    the containerized application, that component will configure the JVM memory based
    on the resource limits assigned to the container. If you don’t specify any limits,
    the results will be unpredictable, which is not what you want.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于 JVM 的应用程序，如 Spring Boot，定义 CPU 和内存限制尤为重要，因为它们将被用来正确地调整 JVM 线程池、堆内存和非堆内存等项目的大小。配置这些值一直是
    Java 开发者的挑战，并且由于它们直接影响应用程序性能，因此至关重要。幸运的是，如果你使用包含在 Spring Boot 中的 Paketo 实现的 Cloud
    Native Buildpacks，你就不必担心这个问题。当你使用 Paketo 在第 6 章中打包 Catalog Service 应用程序时，会自动包含一个
    *Java 内存计算器* 组件。当你运行容器化应用程序时，该组件将根据分配给容器的资源限制配置 JVM 内存。如果你没有指定任何限制，结果将是不可预测的，这并不是你想要的。
- en: There’s also an economic aspect to consider. If you run your applications in
    a public cloud, you’re usually charged based on how many resources you consume.
    Consequently, you’ll probably want to be in control of how much CPU and memory
    each of your containers can use to avoid nasty surprises when the bill arrives.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 还有一个经济因素需要考虑。如果你在公有云中运行应用程序，你通常会被根据你消耗的资源数量来收费。因此，你可能希望控制每个容器可以使用的 CPU 和内存量，以避免账单到来时的意外。
- en: When it comes to orchestrators like Kubernetes, there’s another critical issue
    related to resources that you should consider. Kubernetes schedules Pods to be
    deployed in any of the cluster nodes. But what if a Pod is assigned to a node
    that has insufficient resources to run the container correctly? The solution is
    to declare the minimum CPU and memory a container needs to operate (*resource
    requests*). Kubernetes will use that information to deploy a Pod to a specific
    node only if it can guarantee the container will get at least the requested resources.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到像 Kubernetes 这样的编排器时，还有一个与资源相关的关键问题，您应该考虑。Kubernetes 会调度 Pod 部署到集群的任何节点。但如果一个
    Pod 被分配到一个资源不足的节点，无法正确运行容器怎么办？解决方案是声明容器运行所需的最小 CPU 和内存（*资源请求*）。只有当 Kubernetes
    可以保证容器至少获得请求的资源时，它才会将 Pod 部署到特定的节点。
- en: Resource requests and limits are defined per container. You can specify both
    requests and limits in a Deployment manifest. We haven’t defined any limits in
    the base manifests for Catalog Service because we’ve been operating in a local
    environment and we didn’t want to constrain it too much in terms of resource requirements.
    However, production workloads should always contain resource configurations. Let’s
    look at how we can do that for the production deployment of Catalog Service.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 资源请求和限制是按容器定义的。您可以在部署清单中指定请求和限制。由于我们一直在本地环境中运行，并且不想在资源需求方面过度限制它，所以我们尚未为目录服务的基础清单定义任何限制。然而，生产工作负载应始终包含资源配置。让我们看看我们如何为目录服务生产部署执行此操作。
- en: Assigning resource requests and limits to a container
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 为容器分配资源请求和限制
- en: It shouldn’t be a surprise that we’ll use a patch to apply CPU and memory configurations
    to Catalog Service. Create a patch-resources.yml file within the production overlay
    for Catalog Service (kubernetes/applications/catalog-service/production), and
    define both requests and limits for the container resources. Even though we’re
    considering a production scenario, we’ll use low values to optimize the resource
    usage in your cluster and avoid incurring additional costs. In a real-world scenario,
    you might want to analyze more carefully which requests and limits would be appropriate
    for your use case.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 并不令人惊讶，我们将使用补丁来应用 CPU 和内存配置到目录服务。在目录服务的生产覆盖层（kubernetes/applications/catalog-service/production）中创建一个
    patch-resources.yml 文件，并定义容器资源的请求和限制。尽管我们考虑的是生产场景，但我们会使用较低的值来优化您集群中的资源使用，并避免产生额外的成本。在现实世界的场景中，您可能需要仔细分析哪些请求和限制适合您的用例。
- en: Listing 15.14 Configuring resource requests and limits for the container
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.14 为容器配置资源请求和限制
- en: '[PRE14]'
  id: totrans-157
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Minimum amount of resources required by the container to operate
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 容器运行所需的最小资源量
- en: ❷ The container is guaranteed 756 MiB.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 容器保证获得 756 MiB。
- en: ❸ The container is guaranteed CPU cycles equivalent to 0.1 CPU.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 容器保证获得相当于 0.1 个 CPU 的 CPU 周期。
- en: ❹ Maximum amount of resources the container is allowed to consume
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 容器允许消耗的最大资源量
- en: ❺ The container can consume 756 MiB at most.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 容器最多可以消耗 756 MiB。
- en: ❻ The container can consume CPU cycles equivalent to 2 CPUs at most.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 容器最多可以消耗相当于 2 个 CPU 的 CPU 周期。
- en: Next, open the kustomization.yml file in the production overlay for Catalog
    Service, and configure Kustomize to apply the patch.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，打开目录服务的生产覆盖层中的 kustomization.yml 文件，并配置 Kustomize 应用补丁。
- en: Listing 15.15 Applying the patch for defining resource requests and limits
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.15 应用定义资源请求和限制的补丁
- en: '[PRE15]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Configures resource requests and limits
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 配置资源请求和限制
- en: In listing 15.14, the memory request and limit are the same, but that’s not
    true for the CPU. The following section will explain the reasoning behind those
    choices.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 15.14 中，内存请求和限制是相同的，但 CPU 并不是这样。接下来的部分将解释这些选择的理由。
- en: Optimizing CPU and memory for Spring Boot applications
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 优化 Spring Boot 应用程序的 CPU 和内存
- en: The amount of CPU available to a container directly affects the startup time
    of a JVM-based application like Spring Boot. In fact, the JVM leverages as much
    CPU as available to run the initialization tasks concurrently and reduce the startup
    time. After the startup phase, the application will use much lower CPU resources.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 容器可用的 CPU 数量直接影响基于 JVM 的应用程序（如 Spring Boot）的启动时间。实际上，JVM 利用所有可用的 CPU 来并行执行初始化任务，从而减少启动时间。启动阶段之后，应用程序将使用远低于
    CPU 资源。
- en: A common strategy is to define the CPU request (resources.requests.cpu) with
    the amount the application will use under normal conditions, so that it’s always
    guaranteed to have the resources required to operate correctly. Then, depending
    on the system, you may decide to specify a higher CPU limit or omit it entirely
    (resources .limits.cpu) to optimize performance at startup so that the application
    can use as much CPU as available on the node at that moment.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的策略是定义 CPU 请求（resources.requests.cpu）为应用程序在正常条件下将使用的数量，以确保始终保证有足够的资源来正确运行。然后，根据系统，你可以决定指定更高的
    CPU 限制或完全省略它（resources.limits.cpu），以优化启动时的性能，使应用程序能够使用节点上当时可用的所有 CPU。
- en: '*CPU* is a *compressible resource*, meaning that a container can consume as
    much of it as is available. When it hits the limit (either because of resources.limits.cpu
    or because there’s no more CPU available on the node), the operating system starts
    throttling the container process, which keeps running but with possibly lower
    performance. Since it’s compressible, not specifying a CPU limit can be a valid
    option sometimes to gain a performance boost. Still, you’ll probably want to consider
    the specific scenario and evaluate the consequences of such a decision.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: '*CPU* 是一种 *可压缩资源*，这意味着容器可以消耗其可用量的任何部分。当它达到限制（无论是由于 resources.limits.cpu 还是节点上没有更多的
    CPU 可用），操作系统开始限制容器进程，使其继续运行，但可能性能会降低。由于它是可压缩的，因此有时不指定 CPU 限制可以是一个有效的选项，以获得性能提升。然而，你可能会想考虑特定的场景并评估这种决定的后果。'
- en: Unlike CPU, *memory* is a *non-compressible resource*. If a container hits the
    limit (either because of resources.limits.memory or because there’s no more memory
    available on the node), a JVM-based application will throw the dreadful OutOfMemoryError,
    and the operating system will terminate the container process with an OOMKilled
    (OutOfMemory killed) status. There is no throttling. Setting the correct memory
    value is, therefore, particularly important. There’s no shortcut to inferring
    the proper configuration; you must monitor the application running under normal
    conditions. That’s true for both CPU and memory.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与 CPU 不同，*内存* 是一种 *不可压缩资源*。如果一个容器达到限制（无论是由于 resources.limits.memory 还是节点上没有更多的内存可用），基于
    JVM 的应用程序将抛出可怕的 OutOfMemoryError，操作系统将以 OOMKilled（OutOfMemory killed）状态终止容器进程。没有限制。因此，设置正确的内存值尤为重要。没有捷径可以推断出正确的配置；你必须监控在正常条件下运行的应用程序。这对
    CPU 和内存都适用。
- en: Once you find a suitable value for how much memory your application needs, I
    recommend you use it both as a request (resources.requests.memory) and as a limit
    (resources.limits.memory). The reason for that is deeply connected to how the
    JVM works, and particularly how the JVM heap memory behaves. Growing and shrinking
    the container memory dynamically will affect the application’s performance, since
    the heap memory is dynamically allocated based on the memory available to the
    container. Using the same value for the request and the limit ensures that a fixed
    amount of memory is always guaranteed, resulting in better JVM performance. Furthermore,
    it allows the Java Memory Calculator provided by the Paketo Buildpacks to configure
    the JVM memory in the most efficient way.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你找到了适合你应用程序所需内存的合适值，我建议你将其同时用作请求（resources.requests.memory）和限制（resources.limits.memory）。这样做的原因与
    JVM 的工作方式密切相关，尤其是 JVM 堆内存的行为。动态地增加和减少容器内存将影响应用程序的性能，因为堆内存是基于容器可用的内存动态分配的。使用相同的值作为请求和限制确保始终保证固定数量的内存，从而提高
    JVM 性能。此外，它允许 Paketo Buildpacks 提供的 Java 内存计算器以最有效的方式配置 JVM 内存。
- en: I’ve mentioned the Java Memory Calculator a few times now. The following section
    will expand on the subject.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经多次提到了 Java 内存计算器。接下来的部分将扩展这个主题。
- en: Configuring resources for the JVM
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 配置 JVM 资源
- en: The Paketo Buildpacks used by the Spring Boot plugin for Gradle/Maven provide
    a Java Memory Calculator component when building container images for Java applications.
    This component implements an algorithm that has been refined and improved over
    the years, thanks to the Pivotal (now VMware Tanzu) experience with running containerized
    Java workloads in the cloud.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Spring Boot 插件用于 Gradle/Maven 的 Paketo Buildpacks 在构建 Java 应用程序的容器镜像时提供了一个 Java
    内存计算器组件。这个组件实现了一个经过多年精炼和改进的算法，这得益于 Pivotal（现在是 VMware Tanzu）在云中运行容器化 Java 工作负载的经验。
- en: 'In a production scenario, the default configuration is a good starting point
    for most applications. However, it can be too resource-demanding for local development
    or demos. One way to make the JVM consume fewer resources is to lower the default
    250 JVM thread count for imperative applications. For that reason, we’ve been
    using the BPL_JVM_THREAD_COUNT environment variable to configure a low number
    of threads for the two Servlet-based applications in Polar Bookshop: Catalog Service
    and Config Service. Reactive applications are already configured with fewer threads,
    since they are much more resource-efficient than their imperative counterparts.
    For that reason, we haven’t customized the thread count for Edge Service, Order
    Service, or Dispatcher Service.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产场景中，默认配置对于大多数应用程序来说是一个好的起点。然而，对于本地开发或演示来说，它可能过于资源密集。减少JVM消耗资源的一种方法是将强制应用程序的默认250
    JVM线程数降低。因此，我们一直在使用BPL_JVM_THREAD_COUNT环境变量来为极地书店的两个基于Servlet的应用程序（目录服务和配置服务）配置较少的线程数。反应式应用程序已经配置了较少的线程，因为它们比它们的强制对应物更加资源高效。因此，我们没有为边缘服务、订单服务或调度服务定制线程数。
- en: Note The Paketo team is working on extending the Java Memory Calculator to provide
    a low-profile mode, which will be helpful when working locally or on low-volume
    applications. In the future, it will be possible to control the memory configuration
    mode via a flag rather than having to tweak the individual parameters. You can
    find more information about this feature on the GitHub project for Paketo Buildpacks
    ([http://mng.bz/5Q87](http://mng.bz/5Q87)).
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 注意Paketo团队正在努力扩展Java内存计算器以提供低配置模式，这对于在本地工作或处理低流量应用程序时非常有帮助。将来，将能够通过一个标志来控制内存配置模式，而不是必须调整单个参数。你可以在Paketo
    Buildpacks的GitHub项目上找到更多关于此功能的信息（[http://mng.bz/5Q87](http://mng.bz/5Q87)）。
- en: 'The JVM has two main memory areas: heap and non-heap. The Calculator focuses
    on computing values for the different non-heap memory parts according to a specific
    formula. The remaining memory resources are assigned to the heap. If the default
    configuration is not good enough, you can customize it as you prefer. For example,
    I experienced some memory issues with an imperative application handling session
    management with Redis. It required more direct memory than was configured by default.
    In that case, I used the standard -XX:MaxDirectMemorySize=50M JVM setting via
    the JAVA_TOOL_OPTIONS environment variable and increased the maximum size for
    the direct memory from 10 MB to 50 MB. If you customize the size of a specific
    memory region, the Calculator will adjust the allocation of the remaining areas
    accordingly.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: JVM有两个主要的内存区域：堆和非堆。计算器专注于根据特定公式计算不同非堆内存部分的值。剩余的内存资源分配给堆。如果默认配置不够好，你可以按自己的喜好进行自定义。例如，我在处理Redis会话管理的强制应用程序中遇到了一些内存问题。它需要的直接内存比默认配置的多。在这种情况下，我通过JAVA_TOOL_OPTIONS环境变量使用了标准的-XX:MaxDirectMemorySize=50M
    JVM设置，并将直接内存的最大大小从10 MB增加到50 MB。如果你自定义了特定内存区域的大小，计算器将相应地调整剩余区域的分配。
- en: Note Memory handling in the JVM is a fascinating topic that would require its
    own book to fully cover. Therefore, I won’t go into details regarding how to configure
    it.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在JVM中内存处理是一个引人入胜的话题，要完全涵盖它可能需要一本自己的书。因此，我不会深入讲解如何配置它。
- en: Since we are configuring deployments for production, let’s update the thread
    count for Catalog Service using a more suitable number like 100. In a real-world
    scenario, I would recommend starting with the default value of 250 as a baseline.
    For Polar Bookshop, I’m trying to compromise between showing what an actual production
    deployment would look like and minimizing the resources you need to consume (and
    perhaps pay for) on a public cloud platform.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们正在为生产环境配置部署，让我们使用一个更合适的数字，例如100，来更新目录服务的线程数。在实际场景中，我建议从默认值250作为基准开始。对于极地书店，我正在尝试在展示实际生产部署的样子和最小化你在公共云平台上需要消耗（或许需要支付）的资源之间取得妥协。
- en: We can update the thread count for Catalog Service in the patch we defined earlier
    to customize environment variables. Open the patch-env.yml file in the production
    overlay for Catalog Service (kubernetes/applications/catalog-service/production),
    and update the JVM thread count as follows.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在之前定义的补丁中更新目录服务的线程数以自定义环境变量。打开目录服务（kubernetes/applications/catalog-service/production）的生产覆盖层中的patch-env.yml文件，并按以下方式更新JVM线程数。
- en: Listing 15.16 Number of JVM threads used by the Java Memory Calculator
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.16 Java内存计算器使用的JVM线程数
- en: '[PRE16]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ The number of threads considered in the memory calculation
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在内存计算中考虑的线程数
- en: That was the last configuration change we needed to make before deploying the
    application in production. We’ll do that next.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 那是我们部署应用程序到生产之前需要做的最后一个配置更改。我们将在下一步进行。
- en: 15.2.3 Deploying Spring Boot in production
  id: totrans-188
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.2.3 在生产中部署Spring Boot
- en: Our end goal is to automate the full process from code commit to production.
    Before looking into the production stage of the deployment pipeline, let’s verify
    that the customizations we’ve defined so far are correct by deploying Catalog
    Service in production manually.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是自动化从代码提交到生产的整个流程。在查看部署管道的生产阶段之前，让我们通过手动在生产中部署目录服务来验证我们迄今为止定义的自定义设置是否正确。
- en: 'As you learned in the previous chapter, we can use the Kubernetes CLI to deploy
    applications on Kubernetes from a Kustomization overlay. Open a Terminal window,
    navigate to the production overlay folder for Catalog Service (polar-deployment/
    kubernetes/applications/catalog-service/production), and run the following command
    to deploy the application via Kustomize:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在前一章所学，我们可以使用Kubernetes CLI从Kustomization覆盖中部署Kubernetes上的应用程序。打开一个终端窗口，导航到目录服务的生产覆盖文件夹（polar-deployment/kubernetes/applications/catalog-service/production），并运行以下命令通过Kustomize部署应用程序：
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'You can follow their progress and see when the two application instances are
    ready to accept requests by running this command:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行此命令来跟踪它们的进度，并查看两个应用实例何时准备好接受请求：
- en: '[PRE18]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'For additional information on the deployment, you can keep using the Kubernetes
    CLI or rely on Octant, a tool that lets you visualize your Kubernetes workloads
    via a convenient GUI. As explained in chapter 7, you can start Octant with the
    command octant. Furthermore, the application logs might be interesting for verifying
    that Catalog Service is running correctly:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 关于部署的更多信息，您可以使用Kubernetes CLI或依赖Octant，这是一个允许您通过方便的GUI可视化Kubernetes工作负载的工具。如第7章所述，您可以使用命令octant启动Octant。此外，应用程序日志可能对验证目录服务是否正确运行很有趣：
- en: '[PRE19]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'The application is not exposed outside the cluster yet (for that, we need Edge
    Service), but you can use the port-forwarding functionality to forward traffic
    from your local environment on port 9001 to the Service running in the cluster
    on port 80:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序尚未对外集群公开（为此，我们需要边缘服务），但您可以使用端口转发功能将本地环境上的端口9001的流量转发到集群中运行的端口80上的服务：
- en: '[PRE20]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Note The process started by the kubectl port-forward command will keep running
    until you explicitly stop it with Ctrl-C.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：由kubectl port-forward命令启动的过程将一直运行，直到您使用Ctrl-C显式停止它。
- en: 'Now you can call Catalog Service from your local machine on port 9001, and
    the request will be forwarded to the Service object inside the Kubernetes cluster.
    Open a new Terminal window and call the root endpoint exposed by the application
    to verify that the polar.greeting value specified in the ConfigMap for the prod
    Spring profile is used instead of the default one:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以从本地机器上的端口9001调用目录服务，请求将被转发到Kubernetes集群内的服务对象。打开一个新的终端窗口，调用应用程序公开的根端点，以验证prod
    Spring配置文件中指定的polar.greeting值被用于默认值：
- en: '[PRE21]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Congratulations! You are officially in production! When you’re done, you can
    terminate the port-forwarding with Ctrl-C. Finally, delete the deployment by running
    the following command from the production overlay folder for Catalog Service:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！您正式进入生产阶段！完成操作后，您可以使用Ctrl-C终止端口转发。最后，从目录服务的生产覆盖文件夹运行以下命令删除部署：
- en: '[PRE22]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Kubernetes provides the infrastructure for implementing different types of deployment
    strategies. When we update our application manifests with a new release version
    and apply them to the cluster, Kubernetes performs a *rolling update*. This strategy
    consists in incrementally updating Pod instances with new ones and guarantees
    zero downtime for the user. You saw that in action in the previous chapter.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为实施不同类型的部署策略提供了基础设施。当我们使用新版本更新应用程序清单并将其应用到集群中时，Kubernetes执行一个*滚动更新*。此策略通过增量更新Pod实例来保证使用新实例，并确保用户零停机时间。您在前一章中已经看到了这一过程。
- en: By default, Kubernetes adopts the rolling update strategy, but there are other
    techniques that you can employ based on the standard Kubernetes resources or you
    can rely on a tool like Knative. For example, you might want to use *blue/green
    deployments*, consisting of deploying the new version of the software in a second
    production environment. By doing that, you can test one last time that everything
    runs correctly. When the environment is ready, you move the traffic from the first
    (*blue*) to the second (*green*) production environment.[¹](#pgfId-1022859)
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Kubernetes 采用滚动更新策略，但你也可以根据标准的 Kubernetes 资源使用其他技术，或者依赖 Knative 这样的工具。例如，你可能想使用**蓝/绿部署**，即在第二个生产环境中部署软件的新版本。通过这样做，你可以最后一次测试确保一切运行正确。当环境准备就绪时，你将流量从第一个（*蓝色*）生产环境转移到第二个（*绿色*）生产环境。[¹](#pgfId-1022859)
- en: Another deployment technique is the *canary release*. It’s similar to the blue/green
    deployment, but the traffic from the blue to the green environment is moved gradually
    over time. The goal is to roll out the change to a small subset of users first,
    perform some verifications, and then do the same for more and more users until
    everyone is using the new version.[²](#pgfId-1022901) Both blue/green deployments
    and canary releases provide a straightforward way to roll back changes.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种部署技术是**金丝雀发布**。它与蓝/绿部署类似，但蓝色到绿色环境的流量会随着时间的推移逐渐移动。目标是首先将更改部署给一小部分用户，进行一些验证，然后为越来越多的用户重复此过程，直到所有人都使用新版本。[²](#pgfId-1022901)
    蓝色/绿色部署和金丝雀发布都提供了一种简单的方法来回滚更改。
- en: Note If you’re interested in learning more about deployment and release strategies
    on Kubernetes, I recommend reading chapter 5 of *Continuous Delivery for Kubernetes*
    by Mauricio Salatino, published by Manning ([https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-5](https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-5)).
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你对在 Kubernetes 上的部署和发布策略感兴趣，我建议阅读 Mauricio Salatino 所著的《Kubernetes 的持续交付》第
    5 章，由 Manning 出版（[https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-5](https://livebook.manning.com/book/continuous-delivery-for-kubernetes/chapter-5)）。
- en: 'Currently, every time you commit changes, a new release candidate is ultimately
    published and approved if it passes successfully through the commit and acceptance
    stages. Then you need to copy the version number of the new release candidate
    and paste it into the Kubernetes manifests before you can update the application
    in production manually. In the next section, you’ll see how to automate that process
    by implementing the final part of the deployment pipeline: the production stage.'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，每次提交更改时，都会发布一个新的候选版本，如果它成功通过提交和接受阶段，最终会被批准。然后你需要复制新候选版本的版本号，并将其粘贴到 Kubernetes
    清单中，这样你才能手动更新生产环境中的应用程序。在下一节中，你将看到如何通过实现部署管道的最后一部分：生产阶段来自动化这个过程。
- en: '15.3 Deployment pipeline: Production stage'
  id: totrans-208
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.3 部署管道：生产阶段
- en: 'We started implementing a deployment pipeline back in chapter 3, and we have
    come a long way since then. We’ve automated all the steps from code commit up
    to having a release candidate ready for production. There are still two operations
    that we have performed manually so far: updating the production scripts with the
    new application version, and deploying it to Kubernetes.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从第 3 章开始实施部署管道，从那时起我们已经走了很长的路。我们已经自动化了从代码提交到准备好生产环境候选版本的所有步骤。到目前为止，我们仍然有两个手动操作：使用新应用程序版本更新生产脚本，并将其部署到
    Kubernetes。
- en: In this section, we’ll start looking at the final part of a deployment pipeline,
    the production stage, and I’ll show you how to implement it as a workflow in GitHub
    Actions.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将开始探讨部署管道的最后一部分，即生产阶段，我将向你展示如何在 GitHub Actions 中将其实现为一个工作流程。
- en: 15.3.1 Understanding the production stage of the deployment pipeline
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.1 理解部署管道的生产阶段
- en: After a release candidate has gone through the commit and acceptance stages,
    we are confident enough to deploy it to production. The production stage can be
    triggered manually or automatically, depending on whether you’d like to achieve
    *continuous deployment*.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在候选版本经过提交和接受阶段之后，我们对其部署到生产环境有足够的信心。生产阶段可以手动或自动触发，这取决于你是否希望实现**持续部署**。
- en: '*Continuous delivery* is “a software development discipline where you build
    software in such a way that the software can be released to production at any
    time.”[³](#pgfId-1024519) The key part is understanding that the software *can*
    be released to production, but it doesn’t *have to*. That’s a common source of
    confusion between continuous delivery and continuous deployment. If you also want
    to take the newest release candidate and deploy it to production automatically,
    then you would have *continuous deployment*.'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '*持续交付* 是“一种软件开发学科，你以这种方式构建软件，使得软件可以随时发布到生产。”[³](#pgfId-1024519) 关键部分是理解软件 *可以*
    发布到生产，但它 *不必*。这是持续交付和持续部署之间常见的混淆来源。如果你还想自动将最新发布候选版本部署到生产，那么你将拥有 *持续部署*。'
- en: 'The production stage consists of two main steps:'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 生产阶段由两个主要步骤组成：
- en: Update the deployment scripts (in our case, the Kubernetes manifests) with the
    new release version.
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用新版本更新部署脚本（在我们的案例中，是 Kubernetes 清单）。
- en: Deploy the application to the production environment.
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将应用程序部署到生产环境。
- en: Note An optional third step would be to run some final automated tests to verify
    that the deployment was successful. Perhaps you could reuse the same system tests
    that you will have included in the acceptance stage to verify the deployment in
    a staging environment.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：可选的第三步是运行一些最终自动化测试，以验证部署是否成功。也许你可以重用你将在验收阶段包含的系统测试，以验证在预发布环境中的部署。
- en: The next section will show you how to implement the first step of the production
    stage using GitHub Actions, and we’ll discuss some implementation strategies for
    the second step. We’ll aim to automate the whole path from code commit to production
    and achieve continuous deployment.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将向您展示如何使用 GitHub Actions 实现生产阶段的第一步，并且我们将讨论第二步的一些实现策略。我们的目标是自动化从代码提交到生产的整个流程，并实现持续部署。
- en: 15.3.2 Implementing the production stage with GitHub Actions
  id: totrans-219
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.3.2 使用 GitHub Actions 实现生产阶段
- en: Compared to the previous stages, implementing the production stage of a deployment
    pipeline can differ a lot depending on several factors. Let’s start by focusing
    on the first step of the production stage.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前阶段相比，实现部署管道的生产阶段可能因几个因素而大不相同。让我们首先关注生产阶段的第一步。
- en: At the end of the acceptance stage, we have a release candidate that’s proven
    to be ready for production. After that, we need to update the Kubernetes manifests
    in our production overlay with the new release version. When we’re keeping both
    the application source code and deployment scripts in the same repository, the
    production stage could be listening to a specific event published by GitHub whenever
    the acceptance stage completes successfully, much like how we configured the flow
    between the commit and acceptance stages.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 在验收阶段结束时，我们有一个经过验证的发布候选版本，表明其已准备好投入生产。之后，我们需要更新生产叠加中的 Kubernetes 清单以包含新版本。当我们保持应用程序源代码和部署脚本在同一个仓库中时，生产阶段可以监听
    GitHub 在验收阶段成功完成后发布的特定事件，就像我们配置提交和验收阶段之间的流程一样。
- en: In our case, we are keeping the deployment scripts in a separate repository,
    which means that whenever the acceptance stage workflow completes its execution
    in the application repository, we need to notify the production stage workflow
    in the deployment repository. GitHub Actions provides the option of implementing
    this notification process via a custom event. Let’s see how it works.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将部署脚本保存在一个单独的仓库中，这意味着每当应用程序仓库中的验收阶段工作流程完成执行后，我们需要通知部署仓库中的生产阶段工作流程。GitHub
    Actions 提供了通过自定义事件实现此通知过程的选项。让我们看看它是如何工作的。
- en: Open your Catalog Service project (catalog-service), and go to the acceptance-stage.yml
    file within the .github/workflows folder. After all the acceptance tests have
    run successfully, we have to define a final step that will send a notification
    to the polar-deployment repository and ask it to update the Catalog Service production
    manifests with the new release version. That will be the trigger for the production
    stage, which we’ll implement in a moment.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 打开您的目录服务项目（catalog-service），进入 .github/workflows 文件夹中的 acceptance-stage.yml
    文件。在所有验收测试成功运行后，我们必须定义一个最终步骤，该步骤将向 polar-deployment 仓库发送通知，并要求它使用新版本更新目录服务生产清单。这将触发生产阶段，我们将在稍后实现它。
- en: Listing 15.17 Triggering the production stage in the deployment repository
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.17 在部署仓库中触发生产阶段
- en: '[PRE23]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Defines relevant data as environment variables
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义相关数据作为环境变量
- en: ❷ Runs only when all functional and non-functional acceptance tests are completed
    successfully
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅当所有功能性和非功能性验收测试都成功完成后运行
- en: ❸ An action to send an event to another repository and trigger a workflow
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 一个向另一个仓库发送事件并触发工作流程的操作
- en: ❹ A token to grant the action permission to send events to another repository
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 一个令牌，授予操作向另一个仓库发送事件的权限
- en: ❺ The repository to notify
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通知的仓库
- en: ❻ A name to identify the event (this is up to you)
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 用于识别事件的名称（这取决于您）
- en: ❼ The payload of the message sent to the other repository. Add any information
    that the other repository might need to perform its operations.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 发送到其他仓库的消息的有效负载。添加其他仓库可能需要执行其操作的信息。
- en: With this new step, if no error is found during the execution of the acceptance
    tests, a notification is sent to the polar-deployment repository to trigger an
    update for Catalog Service.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 通过这一新步骤，如果在执行验收测试期间没有发现错误，则会向 polar-deployment 仓库发送通知以触发目录服务的更新。
- en: By default, GitHub Actions doesn’t allow you to trigger workflows located in
    other repositories, even if they both belong to you or your organization. Therefore,
    we need to provide the repository-dispatch action with an access token that grants
    it such permissions. The token can be a personal access token (PAT), a GitHub
    tool that we used in chapter 6.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，GitHub Actions 不允许您触发位于其他仓库中的工作流程，即使它们都属于您或您的组织。因此，我们需要向 repository-dispatch
    动作提供一个访问令牌，以授予其这样的权限。该令牌可以是一个个人访问令牌（PAT），这是我们第 6 章中使用的 GitHub 工具。
- en: Go to your GitHub account, navigate to Settings > Developer Settings > Personal
    Access Token, and choose Generate New Token. Input a meaningful name, and assign
    it the workflow scope to give the token permissions to trigger workflows in other
    repositories (figure 15.6). Finally, generate the token and copy its value. GitHub
    will show you the token value only once. Make sure you save it since you’ll need
    it soon.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 请访问您的 GitHub 账户，导航到设置 > 开发者设置 > 个人访问令牌，并选择生成新令牌。输入一个有意义的名称，并分配工作流程范围以授予令牌在其他仓库中触发工作流程的权限（图
    15.6）。最后，生成令牌并复制其值。GitHub 只会向您展示一次令牌值。请确保您保存它，因为您很快就会需要它。
- en: '![15-06](../Images/15-06.png)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![15-06](../Images/15-06.png)'
- en: Figure 15.6 A personal access token (PAT) granting permissions to trigger workflows
    in other repositories
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 图 15.6 授予在其他仓库中触发工作流程权限的个人访问令牌（PAT）
- en: Next, go to your Catalog Service repository on GitHub, navigate to the Settings
    tab, and then select Secrets > Actions. On that page, choose New Repository Secret,
    name it DISPATCH_TOKEN (the same name we used in listing 15.17), and input the
    value of the PAT you generated earlier. Using the Secrets feature provided by
    GitHub, we can provide the PAT securely to the acceptance stage workflow.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，前往您的 GitHub 上的 Catalog Service 仓库，导航到设置选项卡，然后选择 Secrets > Actions。在该页面上，选择新建仓库密钥，命名为
    DISPATCH_TOKEN（与我们在列表 15.17 中使用的名称相同），并输入您之前生成的 PAT 的值。通过 GitHub 提供的密钥功能，我们可以安全地将
    PAT 提供给验收阶段的工作流程。
- en: Warning As explained in chapter 3, when using actions from the GitHub marketplace,
    you should handle them like any other third-party application and manage the security
    risks accordingly. In the acceptance stage, we provide an access token to a third-party
    action with permissions to manipulate repositories and workflows. You shouldn’t
    do that light-heartedly. In this case, I trusted the author of the action and
    decided to trust the action with the token.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 如第 3 章所述，当使用 GitHub 市场上的操作时，您应像处理任何其他第三方应用程序一样处理它们，并相应地管理安全风险。在验收阶段，我们向一个具有操作仓库和工作流程权限的第三方操作提供了访问令牌。您不应轻率地这样做。在这种情况下，我信任该操作的开发者，并决定信任该操作与令牌。
- en: Don’t commit your changes to the catalog-service repository yet. We’ll do that
    later. At this point, we have implemented the trigger for the production stage,
    but we haven’t initialized the final stage yet. Let’s move on to the Polar Deployment
    repository and do that.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 请勿将您的更改提交到 catalog-service 仓库。我们稍后再做。到目前为止，我们已经实现了生产阶段的触发器，但尚未初始化最终阶段。让我们继续转到
    Polar Deployment 仓库并完成它。
- en: Open your Polar Deployment project (polar-deployment), and create a production-stage.yml
    file within a new .github/workflows folder. The production stage is triggered
    whenever the acceptance stage from an application repository dispatches an app_
    delivery event. The event itself contains contextual information about the application
    name, image, and version for the newest release candidate. Since the application-specific
    information is parameterized, we can use this workflow for all the applications
    of the Polar Bookshop system, not only Catalog Service.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的Polar部署项目（polar-deployment），在新的 .github/workflows 文件夹内创建一个 production-stage.yml
    文件。每当应用存储库中的接受阶段触发 app_delivery 事件时，就会触发生产阶段。该事件本身包含有关应用名称、镜像和最新发布候选版本的相关信息。由于应用特定信息是参数化的，因此我们可以使用此工作流程为Polar书店系统的所有应用提供服务，而不仅仅是目录服务。
- en: 'The first job of the production stage is updating the production Kubernetes
    manifests with the new release version. This job will consist of three steps:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 生产阶段的第一项任务是使用新的发布版本更新生产Kubernetes清单。这项工作将包括三个步骤：
- en: Check out the polar-deployment source code.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 检出 polar-deployment 源代码。
- en: Update the production Kustomization with the new version for the given application.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用给定应用的最新版本更新生产Kustomization。
- en: Commit the changes to the polar-deployment repository.
  id: totrans-245
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将更改提交到 polar-deployment 存储库。
- en: We can implement those three steps as follows.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这三个步骤实现如下。
- en: Listing 15.18 Updating the image version upon a new application delivery
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 列表15.18 在新的应用交付时更新镜像版本
- en: '[PRE24]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Executes the workflow only when a new app_delivery event is received, dispatched
    from another repository
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅在接收到来自另一个存储库的新 app_delivery 事件时执行工作流程
- en: ❷ Saves the event payload data as environment variables for convenience
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将事件有效负载数据保存为环境变量以方便使用
- en: ❸ Checks out the repository
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检出存储库
- en: ❹ Navigates to the production overlay for the given application
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 导航到给定应用的生产覆盖层
- en: ❺ Updates the image name and version via Kustomize for the given application
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过Kustomize更新给定应用的镜像名称和版本
- en: ❻ Updates the tag used by Kustomize to access the correct base manifests stored
    in the application repository
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 更新Kustomize使用的标签，以访问存储在应用存储库中的正确基本清单
- en: ❼ An action to commit and push the changes applied to the current repository
    from the previous step
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 从上一步骤应用更改并提交和推送当前存储库的更改的操作
- en: ❽ Details about the commit operation
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 提交操作的详细信息
- en: That’s all we need for now. Commit and push the changes to your remote polar-deployment
    on GitHub. Then go back to your Catalog Service project, commit your previous
    changes to the acceptance stage, and push them to your remote catalog-service
    on GitHub.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 目前我们需要的就这些了。将更改提交并推送到GitHub上的远程polar-deployment。然后回到你的目录服务项目，将之前的更改提交到接受阶段，并将它们推送到GitHub上的远程catalog-service。
- en: The new commit to the catalog-service repository will trigger the deployment
    pipeline. First, the commit stage will produce a container image (our release
    candidate) and publish it to GitHub Container Registry. Then the acceptance stage
    will fictitiously run further tests on the application and finally send a notification
    (a custom app_delivery event) to the polar-deployment repository. The event triggers
    the production stage, which will update the production Kubernetes manifests for
    Catalog Service and commit the changes to the polar-deployment repository. Figure
    15.7 illustrates the inputs and outputs for the three stages of the deployment
    pipeline.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 对 catalog-service 存储库的新提交将触发部署管道。首先，提交阶段将生成一个容器镜像（我们的发布候选版本）并将其发布到GitHub容器注册库。然后，接受阶段将对应用进行进一步的测试，并最终向
    polar-deployment 存储库发送通知（自定义的 app_delivery 事件）。该事件触发生产阶段，将更新目录服务的生产Kubernetes清单并将更改提交到
    polar-deployment 存储库。图15.7说明了部署管道三个阶段的输入和输出。
- en: '![15-07](../Images/15-07.png)'
  id: totrans-259
  prefs: []
  type: TYPE_IMG
  zh: '![15-07](../Images/15-07.png)'
- en: Figure 15.7 The commit stage goes from code commit to a release candidate, which
    goes through the acceptance stage. If it passes all the tests, the production
    stage updates the deployment manifests.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.7 提交阶段从代码提交到发布候选版本，然后进入接受阶段。如果它通过了所有测试，生产阶段将更新部署清单。
- en: Go to your GitHub projects and follow the execution of the three stages. In
    the end, you’ll find a new commit in your polar-deployment repository, which was
    submitted by GitHub Actions and contains a change to the Catalog Service production
    overlay so it uses the newest release version.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 前往你的 GitHub 项目，并跟踪三个阶段的执行过程。最后，你会在 polar-deployment 仓库中找到一个新提交，这是由 GitHub Actions
    提交的，其中包含对 Catalog Service 生产叠加层的更改，使其使用最新的发布版本。
- en: 'Perfect! We just got rid of the first of the two remaining manual steps: updating
    the deployment scripts with the newest release version. We still have to apply
    the Kubernetes manifests to the cluster manually, using the Kubernetes CLI. The
    second step of the production stage will take care of automating the application
    deployment whenever a new version is promoted to production. That’s the topic
    of the next section.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 完美！我们刚刚消除了剩余的两个手动步骤中的第一个：使用最新发布版本更新部署脚本。我们仍然需要手动使用 Kubernetes CLI 将 Kubernetes
    清单应用到集群中。生产阶段第二步将负责在将新版本提升到生产时自动部署应用程序。这是下一节的主题。
- en: Polar Labs
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: Polar Labs
- en: It’s time to apply what you learned in this section to Edge Service, Dispatcher
    Service, and Order Service.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将你在这节中学到的知识应用到 Edge Service、Dispatcher Service 和 Order Service 中了。
- en: Generate a PAT with a workflow scope for each application. It’s a security best
    practice not to reuse tokens for multiple purposes.
  id: totrans-265
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为每个应用程序生成具有工作流程范围的 PAT。不要重复使用令牌进行多个目的，这是一种安全最佳实践。
- en: For each application, save the PAT as a Secret from the GitHub repository page.
  id: totrans-266
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个应用程序，将 PAT 作为秘密从 GitHub 仓库页面保存。
- en: Update the acceptance stage workflow with a final step that sends a notification
    to the production stage with information about the newest release candidate.
  id: totrans-267
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用一个最终步骤更新验收阶段工作流程，该步骤将发送通知到生产阶段，其中包含有关最新发布候选版本的信息。
- en: Push your changes to GitHub, ensure the workflow is completed successfully,
    and check that the production stage workflow in the polar-deployment repository
    is triggered correctly.
  id: totrans-268
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将你的更改推送到 GitHub，确保工作流程成功完成，并检查 polar-deployment 仓库中的生产阶段工作流程是否正确触发。
- en: Edge Service is the only application available through the public internet,
    and it requires an additional patch to configure the Ingress to block requests
    to the Actuator endpoints from outside the cluster. You can get the additional
    patch from the applications/edge-service/production folder, within Chapter15/15-end/polar-deployment.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: Edge Service 是唯一可以通过公共互联网访问的应用程序，并且需要额外的补丁来配置 Ingress 以阻止对集群外 Actuator 端点的请求。你可以从
    applications/edge-service/production 文件夹中的 Chapter15/15-end/polar-deployment 获取额外的补丁。
- en: For simplicity, we accept that the Actuator endpoints are available without
    authentication from within the cluster. Internal applications like Catalog Service
    are not affected, since their Actuator endpoints are not accessible through Spring
    Cloud Gateway. On the other hand, the Edge Service ones are currently accessible
    via the public internet.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，我们假设 Actuator 端点在集群内无需身份验证即可访问。像 Catalog Service 这样的内部应用程序不受影响，因为它们的
    Actuator 端点无法通过 Spring Cloud Gateway 访问。另一方面，Edge Service 的那些目前可以通过公共互联网访问。
- en: That’s not safe in a production environment. A simple way of fixing that is
    configuring the Ingress to block any request to the /actuator/** endpoints from
    outside the cluster. They will all still be available from within the cluster
    so that the health probes can work. We are using an NGINX-based Ingress Controller,
    so we can use its configuration language to express a *deny rule* for the Actuator
    endpoints.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这在生产环境中并不安全。一种简单的修复方法是配置 Ingress 以阻止对集群外 /actuator/** 端点的任何请求。它们仍然可以从集群内部访问，以便健康检查可以工作。我们使用基于
    NGINX 的 Ingress Controller，因此我们可以使用其配置语言来为 Actuator 端点表达一个 *拒绝规则*。
- en: In the source code repository accompanying the book, you can check the final
    results in the Chapter15/15-end folder ([https://github.com/ThomasVitale/cloud-native-spring-in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action)).
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 在伴随书籍的源代码仓库中，你可以在 Chapter15/15-end 文件夹中检查最终结果 ([https://github.com/ThomasVitale/cloud-native-spring-in-action](https://github.com/ThomasVitale/cloud-native-spring-in-action))。
- en: 15.4 Continuous deployment with GitOps
  id: totrans-273
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 15.4 使用 GitOps 进行持续部署
- en: 'Traditionally, continuous deployment is implemented by adding a further step
    to the production stage of the deployment pipeline. This additional step would
    authenticate with the target platform (such as a virtual machine or a Kubernetes
    cluster) and deploy the new version of the application. In recent years, a different
    approach has become more and more popular: GitOps. The term was coined by Alexis
    Richardson, CEO and founder of Weaveworks ([www.weave.works](http://www.weave.works)).'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，持续部署是通过在部署管道的生产阶段添加额外步骤来实现的。这个额外的步骤会与目标平台（如虚拟机或Kubernetes集群）进行认证并部署应用程序的新版本。近年来，一种不同的方法越来越受欢迎：GitOps。这个术语是由Weaveworks的首席执行官和创始人Alexis
    Richardson提出的([www.weave.works](http://www.weave.works))。
- en: GitOps is a set of practices for operating and managing software systems, enabling
    continuous delivery and deployment while ensuring agility and reliability. Compared
    to the traditional approach, GitOps favors decoupling between delivery and deployment.
    Instead of having the pipeline *pushing* deployments to the platform, it’s the
    platform itself *pulling* the desired state from a source repository and performing
    deployments. In the first case, the deployment step is implemented within the
    production stage workflow. In the second case, which will be our focus, the deployment
    is still theoretically considered part of the production stage, but the implementation
    differs.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: GitOps是一套操作和管理软件系统的实践，它使持续交付和部署成为可能，同时确保敏捷性和可靠性。与传统方法相比，GitOps更倾向于交付和部署之间的解耦。不是管道*推送*部署到平台，而是平台本身从源代码库*拉取*期望状态并执行部署。在前一种情况下，部署步骤是在生产阶段工作流程中实现的。在后一种情况下，我们将重点关注，部署在理论上仍被视为生产阶段的一部分，但其实现方式不同。
- en: GitOps doesn’t enforce specific technologies, but it’s best implemented with
    Git and Kubernetes. That will be our focus.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: GitOps不强制使用特定技术，但最好与Git和Kubernetes一起实现。这将是我们的重点。
- en: 'The GitOps Working Group, part of the CNCF, defines GitOps in terms of four
    principles ([https://opengitops.dev](https://opengitops.dev)):'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: CNCF的一部分，GitOps工作组将GitOps定义为四个原则([https://opengitops.dev](https://opengitops.dev))：
- en: '*Declarative*—“A system managed by GitOps must have its desired state expressed
    declaratively.”'
  id: totrans-278
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*声明式*—“由GitOps管理的系统必须以声明式表达其期望状态。”'
- en: Working with Kubernetes, we can express the desired state via YAML files (manifests).
  id: totrans-279
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与Kubernetes一起工作，我们可以通过YAML文件（清单）表达期望状态。
- en: Kubernetes manifests declare what we want to achieve, not how. The platform
    is responsible for finding a way to achieve the desired state.
  id: totrans-280
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes清单声明了我们想要实现的内容，而不是如何实现。平台负责找到实现期望状态的方法。
- en: '*Versioned and immutable*—“Desired state is stored in a way that enforces immutability,
    versioning and retains a complete version history.”'
  id: totrans-281
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*版本化和不可变*—“期望状态以强制不可变、版本化和保留完整版本历史记录的方式存储。”'
- en: Git is the preferred choice for ensuring the desired state is versioned and
    the whole history retained. That makes it possible, among other things, to roll
    back to a previous state with ease.
  id: totrans-282
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Git是确保期望状态版本化和保留整个历史记录的首选选择。这使得轻松回滚到先前状态成为可能。
- en: The desired state stored in Git is immutable and represents the single source
    of truth.
  id: totrans-283
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储在Git中的期望状态是不可变的，并且代表单一的真实来源。
- en: '*Pulled automatically*—“Software agents automatically pull the desired state
    declarations from the source.”'
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*自动拉取*—“软件代理自动从源代码中拉取期望状态声明。”'
- en: Examples of software agents (*GitOps agents*) are Flux ([https://fluxcd.io](https://fluxcd.io)),
    Argo CD ([https://argoproj.github.io/cd](https://argoproj.github.io/cd)), and
    kapp-controller ([https://carvel.dev/kapp-controller](https://carvel.dev/kapp-controller)).
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件代理（*GitOps代理*）的例子包括Flux ([https://fluxcd.io](https://fluxcd.io))、Argo CD ([https://argoproj.github.io/cd](https://argoproj.github.io/cd))和kapp-controller
    ([https://carvel.dev/kapp-controller](https://carvel.dev/kapp-controller))。
- en: Rather than granting CI/CD tools like GitHub Actions full access to the cluster
    or running commands manually, we grant the GitOps agent access to a source like
    Git so that it pulls changes automatically.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不是授予CI/CD工具（如GitHub Actions）对集群的完全访问权限或手动运行命令，而是授予GitOps代理对Git等源代码的访问权限，以便它自动拉取更改。
- en: '*Continuously reconciled*—“Software agents continuously observe actual system
    state and attempt to apply the desired state.”'
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*持续协调*—“软件代理持续观察实际系统状态并尝试应用期望状态。”'
- en: Kubernetes is composed of controllers that keep observing the system and ensuring
    the actual state of the cluster matches the desired state.
  id: totrans-288
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kubernetes由控制器组成，它们持续观察系统并确保集群的实际状态与期望状态相匹配。
- en: On top of that, GitOps ensures that it’s the right desired state to be considered
    in the cluster. Whenever a change is detected in the Git source, the agent steps
    up and reconciles the desired state with the cluster.
  id: totrans-289
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在此基础上，GitOps确保集群中考虑的期望状态是正确的。每当Git源检测到变化时，代理就会介入并与集群协调期望状态。
- en: Figure 15.8 illustrates the result of applying the GitOps principles.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8展示了应用GitOps原则的结果。
- en: '![15-08](../Images/15-08.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![15-08](../Images/15-08.png)'
- en: Figure 15.8 Every time the production stage workflow updates the deployment
    repository, the GitOps controller reconciles the desired and actual states.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.8 每当生产阶段工作流程更新部署仓库时，GitOps控制器都会对期望状态和实际状态进行协调。
- en: If you consider the four principles, you’ll notice that we’ve applied the first
    two already. We expressed the desired state for our applications declaratively
    using Kubernetes manifests and Kustomize. And we stored the desired state in a
    Git repository on GitHub (polar-deployment), making it versioned and immutable.
    We are still missing a software agent that automatically pulls the desired state
    declarations from the Git source and continuously reconciles them inside the Kubernetes
    cluster, therefore achieving continuous deployment.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你考虑这四个原则，你会注意到我们已经应用了前两个。我们使用Kubernetes清单和Kustomize声明性地表达了我们应用程序的期望状态，并将其存储在GitHub上的Git仓库（polar-deployment）中，使其版本化和不可变。我们仍然缺少一个软件代理，它会自动从Git源拉取期望状态声明，并在Kubernetes集群内部持续协调它们，从而实现持续部署。
- en: We’ll start by installing Argo CD ([https://argo-cd.readthedocs.io](https://argo-cd.readthedocs.io)),
    a GitOps software agent. Then we’ll configure it to complete the final step of
    the deployment pipeline and let it monitor our polar-deployment repository. Whenever
    there’s a change in the application manifests, Argo CD will apply the changes
    to our production Kubernetes cluster.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先安装Argo CD（[https://argo-cd.readthedocs.io](https://argo-cd.readthedocs.io)），这是一个GitOps软件代理。然后我们将配置它以完成部署管道的最后一步，并让它监控我们的polar-deployment仓库。每当应用程序清单发生变化时，Argo
    CD都会将更改应用到我们的生产Kubernetes集群。
- en: 15.4.1 Implementing GitOps with Argo CD
  id: totrans-295
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.1 使用Argo CD实现GitOps
- en: 'Let’s start by installing the Argo CD CLI. Refer to the project website for
    installation instructions ([https://argo-cd.readthedocs.io](https://argo-cd.readthedocs.io)).
    If you are on macOS or Linux, you can use Homebrew as follows:'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先安装Argo CD CLI。请参考项目网站上的安装说明（[https://argo-cd.readthedocs.io](https://argo-cd.readthedocs.io)）。如果你使用的是macOS或Linux，你可以使用Homebrew如下操作：
- en: '[PRE25]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We’ll use the CLI to instruct Argo CD about which Git repository to monitor,
    and we’ll configure it to apply changes to the cluster to achieve continuous deployment
    automatically. But first we need to deploy Argo CD to the production Kubernetes
    cluster.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用CLI来指示Argo CD监控哪个Git仓库，并将其配置为自动将更改应用到集群中，以实现持续部署。但首先我们需要将Argo CD部署到生产Kubernetes集群。
- en: Note I’ll assume your Kubernetes CLI is still configured to access the production
    cluster on DigitalOcean. You can check that with kubectl config current-context.
    If you need to change the context, you can run kubectl config use-context <context-name>.
    A list of all the contexts available can be retrieved from kubectl config get-contexts.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我将假设你的Kubernetes CLI仍然配置为访问DigitalOcean上的生产集群。你可以使用kubectl config current-context来检查这一点。如果你需要更改上下文，你可以运行kubectl
    config use-context <context-name>。可以通过kubectl config get-contexts获取所有可用上下文的列表。
- en: Open a Terminal window, go to your Polar Deployment project (polar-deployment),
    and navigate to the kubernetes/platform/production/argocd folder. You should have
    copied that folder over to your repository when you set up the production cluster.
    If that’s not the case, please do so now from the source code repository accompanying
    this book (Chapter15/15-end/polar-deployment/platform/production/argocd).
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 打开一个终端窗口，进入你的Polar部署项目（polar-deployment），然后导航到kubernetes/platform/production/argocd文件夹。当你设置生产集群时，你应该已经将此文件夹复制到你的仓库中。如果不是这种情况，请现在从本书附带源代码仓库（Chapter15/15-end/polar-deployment/platform/production/argocd）中执行此操作。
- en: 'Then run the following script to install Argo CD into the production cluster.
    Feel free to open the file and look at the instructions before running it:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 然后运行以下脚本将Argo CD安装到生产集群中。在运行之前，你可以自由地打开文件并查看说明：
- en: '[PRE26]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Tip You might need to make the script executable first, with the command chmod
    +x deploy.sh.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：你可能需要首先使用命令 chmod +x deploy.sh 使脚本可执行。
- en: 'The deployment of Argo CD consists of several components, including a convenient
    web interface where you can visualize and control all the deployments controlled
    by Argo CD. For now, we’ll use the CLI. During the installation, Argo CD will
    have autogenerated a password for the admin account (the username is admin). Run
    the following command to fetch the password value (it will take a few seconds
    before the value is available):'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: Argo CD 的部署由几个组件组成，包括一个方便的 Web 界面，你可以在这里可视化并控制所有由 Argo CD 控制的部署。目前，我们将使用 CLI。在安装过程中，Argo
    CD 将为管理员账户（用户名为 admin）自动生成密码。运行以下命令以获取密码值（在值可用之前可能需要几秒钟）：
- en: '[PRE27]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Next, let’s identify the external IP address assigned to Argo CD server:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们确定分配给 Argo CD 服务器的公网 IP 地址：
- en: '[PRE28]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The platform might take a few minutes to provision a load balancer for Argo
    CD. During the provisioning, the EXTERNAL-IP column will show a <pending> status.
    Wait and try again until an IP address is shown. Note it down, because we’re going
    to use it soon.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 平台可能需要几分钟来为 Argo CD 配置负载均衡器。在配置过程中，EXTERNAL-IP 列将显示 <待处理> 状态。等待并重试，直到显示 IP 地址。注意记录下来，因为我们很快就会用到它。
- en: 'Since the Argo CD server is now exposed via a public load balancer, we can
    use the external IP address to access its services. For this example, we’ll use
    the CLI, but you can achieve the same results by opening <argocd-external-ip>
    (the IP address assigned to your Argo CD server) in a browser window. Either way,
    you’ll have to log in with the auto-generated admin account. The username is admin,
    and the password is the one you fetched earlier. Be aware that you might get a
    warning, since you are not using HTTPS:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Argo CD 服务器现在通过公共负载均衡器公开，我们可以使用外部 IP 地址来访问其服务。对于此示例，我们将使用 CLI，但你也可以通过在浏览器窗口中打开
    <argocd-external-ip>（分配给你的 Argo CD 服务器的 IP 地址）来实现相同的结果。无论如何，你都需要使用自动生成的管理员账户登录。用户名是
    admin，密码是之前获取的密码。请注意，你可能会收到警告，因为你没有使用 HTTPS：
- en: '[PRE29]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: It’s now time to see continuous deployment in action with GitOps. I’ll assume
    you have been through all the previous sections of this chapter. At this point,
    the commit stage of your Catalog Service repository on GitHub (catalog-service)
    should have built a container image, the acceptance stage should have triggered
    the Polar Deployment repository on GitHub (polar-deployment), and the production
    stage should have updated the production overlay for Catalog Service with the
    newest release version (polar-deployment/kubernetes/applications/catalog-service/production).
    Now we’ll configure Argo CD to monitor the production overlay for Catalog Service
    and synchronize it with the production cluster whenever it detects a change in
    the repository. In other words, Argo CD will continuously deploy new versions
    of Catalog Service as made available by the deployment pipeline.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候通过 GitOps 看一下持续部署的实际操作了。我将假设你已经阅读了本章的所有前几节。到目前为止，你的 GitHub（catalog-service）目录服务仓库的提交阶段应该已经构建了一个容器镜像，验收阶段应该已经触发了
    GitHub（polar-deployment）上的 Polar Deployment 仓库，生产阶段应该已经使用最新的发布版本（polar-deployment/kubernetes/applications/catalog-service/production）更新了目录服务的生产覆盖层。现在我们将配置
    Argo CD 监控目录服务的生产覆盖层，并在检测到存储库中的更改时与生产集群同步。换句话说，Argo CD 将持续部署由部署管道提供的目录服务的新版本。
- en: '[PRE30]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Creates a catalog-service application in Argo CD
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 Argo CD 中创建 catalog-service 应用程序
- en: ❷ The Git repository to monitor for changes. Insert your GitHub username.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 监控更改的 Git 仓库。插入你的 GitHub 用户名。
- en: ❸ The folder to monitor for changes within the configured repository
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 监控配置的存储库内更改的文件夹
- en: ❹ The Kubernetes cluster where the application should be deployed. We are using
    the default cluster configured in the kubectl context.
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 应该部署应用程序的 Kubernetes 集群。我们正在使用 kubectl 上下文中配置的默认集群。
- en: ❺ The namespace where the application should be deployed. We are using the “default”
    namespace.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 应该部署应用程序的命名空间。我们正在使用“默认”命名空间。
- en: ❻ Configures Argo CD to automatically reconcile the desired state in the Git
    repo with the actual state in the cluster
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 配置 Argo CD 自动将 Git 仓库中期望的状态与集群中的实际状态进行协调
- en: ❼ Configures Argo CD to delete old resources after a synchronization automatically
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 配置 Argo CD 在同步后自动删除旧资源
- en: 'You can verify the status of the continuous deployment of Catalog Service with
    the following command (I have filtered the results for the sake of clarity):'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令验证 Catalog Service 的持续部署状态（为了清晰起见，我已经过滤了结果）：
- en: '[PRE31]'
  id: totrans-321
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Argo CD has automatically applied the production overlay for Catalog Service
    (polar-deployment/kubernetes/applications/catalog-service/production) to the cluster.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: Argo CD 已自动将 Catalog Service（polar-deployment/kubernetes/applications/catalog-service/production）的生产叠加应用到集群中。
- en: 'Once all the resources listed by the previous command have the Synced status,
    we can verify that the application is running correctly. The application is not
    exposed outside the cluster yet, but you can use the port-forwarding functionality
    to forward traffic from your local environment on port 9001 to the Service running
    in the cluster on port 80:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦前一个命令列出的所有资源都达到同步状态，我们就可以验证应用程序是否运行正确。应用程序尚未在集群外部暴露，但您可以使用端口转发功能将本地环境上的端口
    9001 的流量转发到集群中运行在端口 80 的服务：
- en: '[PRE32]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Next, call the root point exposed by the application. We expect to get the value
    we configured for the polar.greeting property in the Catalog Service production
    overlay.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，调用应用程序暴露的根点。我们预计会得到我们在 Catalog Service 生产叠加中配置的 polar.greeting 属性的值。
- en: '[PRE33]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Perfect! In one step we automated not only the first deployment but also any
    future updates. Argo CD will detect any change in the production overlay for Catalog
    Service and apply the new manifests to the cluster immediately. There could be
    a new release version to deploy, but it could also be a change to the production
    overlay. For example, let’s try configuring a different value for the polar.greeting
    property.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！我们一步就自动化了不仅第一次部署，还包括任何未来的更新。Argo CD 将检测 Catalog Service 生产叠加中的任何更改，并立即将新的清单应用到集群中。可能会有新的发布版本需要部署，但也可能是生产叠加的更改。例如，让我们尝试为
    polar.greeting 属性配置不同的值。
- en: Open your Polar Deployment project (polar-deployment), go to the production
    overlay for Catalog service (kubernetes/applications/catalog-service/production),
    and update the value of the polar.greeting property in the application-prod.yml
    file.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 打开你的 Polar Deployment 项目（polar-deployment），转到 Catalog 服务的生产叠加（kubernetes/applications/catalog-service/production），并在
    application-prod.yml 文件中更新 polar.greeting 属性的值。
- en: Listing 15.19 Updating the production-specific configuration for the app
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 15.19 更新应用程序的生产特定配置
- en: '[PRE34]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Then commit and push the changes to your remote polar-deployment repository
    on GitHub. By default, Argo CD checks the Git repository for changes every three
    minutes. It will notice the change and apply the Kustomization again, resulting
    in a new ConfigMap being generated by Kustomize and a rolling restart of the Pods
    to refresh the configuration. Once the deployment in the cluster is in sync with
    the desired state in the Git repo (you can check this with argocd app get catalog-service),
    call the root endpoint exposed by Catalog Service again. We’ll expect to get the
    value we have just updated. If you get a network error, it might be that the port-forwarding
    process was interrupted. Run kubectl port-forward service/catalog-service 9001:80
    again to fix it:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，将更改提交并推送到你在 GitHub 上的远程 polar-deployment 仓库。默认情况下，Argo CD 每三分钟检查 Git 仓库中的更改。它会注意到更改并重新应用
    Kustomization，这将导致 Kustomize 生成新的 ConfigMap，并滚动重启 Pod 以刷新配置。一旦集群中的部署与 Git 仓库中的所需状态同步（你可以使用
    argocd app get catalog-service 检查这一点），再次调用 Catalog Service 暴露的根端点。我们预计会得到我们刚刚更新的值。如果你得到网络错误，可能是端口转发过程被中断了。再次运行
    kubectl port-forward service/catalog-service 9001:80 来修复它：
- en: '[PRE35]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Great! We have finally achieved continuous deployment! Pause for a minute and
    celebrate with a beverage of your choice. You deserve it!
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们终于实现了持续部署！暂停一分钟，用你喜欢的饮料庆祝一下。你应得的！
- en: Polar Labs
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: Polar Labs
- en: It’s time to apply what you learned in this section to Edge Service, Dispatcher
    Service, and Order Service.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候将本节中学到的知识应用到 Edge 服务、Dispatcher 服务和 Order 服务上了。
- en: Using the Argo CD CLI, register each of the remaining applications as we did
    for Catalog Service. Remember to authenticate to Argo CD first, as explained earlier.
  id: totrans-336
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Argo CD CLI，像为 Catalog Service 那样注册剩余的每个应用程序。记住，首先需要按照前面解释的进行认证。
- en: For each application, verify that Argo CD has synchronized the desired state
    from the polar-deployment repository with the actual state in the cluster.
  id: totrans-337
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个应用程序，验证 Argo CD 是否已将 polar-deployment 仓库中的所需状态与集群中的实际状态同步。
- en: In the case of problems with Argo CD, you can use the argocd app get catalog-service
    command to verify the synchronization status or directly use the web interface
    available at <argocd-external-ip>. For troubleshooting Kubernetes resources, you
    can take advantage of Octant or use one of the techniques explained in the last
    section of chapter 7\.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 如果Argo CD出现问题，您可以使用argocd app get catalog-service命令来验证同步状态，或者直接使用在<argocd-external-ip>上可用的Web界面。对于Kubernetes资源的故障排除，您可以利用Octant或使用第7章最后部分中解释的技术之一。
- en: 15.4.2 Putting it all together
  id: totrans-339
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 15.4.2 整合所有内容
- en: If you followed along and completed all the Polar Labs, you’ll now have the
    whole Polar Bookshop system up and running in a production Kubernetes cluster
    in the public cloud. That’s a huge accomplishment! In this section, we’ll give
    it a try and refine a few last points. Figure 15.9 shows the status of the applications
    from the Argo CD GUI, accessible via the <argocd-external-ip> address discovered
    earlier.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您跟随步骤并完成了所有Polar Labs，那么现在您已经在公共云中的生产Kubernetes集群中运行了整个Polar Bookshop系统。这是一个巨大的成就！在本节中，我们将尝试并完善一些最后要点。图15.9显示了通过之前发现的<argocd-external-ip>地址可访问的Argo
    CD GUI中的应用程序状态。
- en: '![15-09](../Images/15-09.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![15-09](../Images/15-09.png)'
- en: Figure 15.9 The Argo CD GUI shows an overview of all the applications managed
    via a GitOps flow.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图15.9 Argo CD GUI显示了通过GitOps流程管理的所有应用的概览。
- en: 'So far, we have worked with Catalog Service, an internal application that is
    not exposed outside the cluster. For that reason, we relied on the port-forwarding
    functionality to test it out. Now that the whole system is deployed, we can access
    the applications as intended: via the Edge Service. The platform automatically
    configures a load balancer with an external IP address whenever we deploy an Ingress
    resource. Let’s discover the external IP address for the Ingress sitting in front
    of Edge Service:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们与目录服务一起工作，这是一个不暴露在集群外部的内部应用程序。因此，我们依赖端口转发功能来测试它。现在整个系统已部署，我们可以按照预期访问应用程序：通过Edge服务。每次我们部署Ingress资源时，平台都会自动配置一个带有外部IP地址的负载均衡器。让我们发现Edge服务前面的Ingress的外部IP地址：
- en: '[PRE36]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Using the Ingress external IP address, you can use Polar Bookshop from the public
    internet. Open a browser window and navigate to <ip-address>.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Ingress的外部IP地址，您可以从公共互联网使用Polar Bookshop。打开浏览器窗口并导航到<ip-address>。
- en: Try logging in as Isabelle. Feel free to add some books and browse the catalog.
    Then log out and log in again, this time as Bjorn. Verify that you can’t create
    or edit books, but you can place orders.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试以Isabelle的身份登录。您可以随意添加一些书籍并浏览目录。然后登出并再次登录，这次以Bjorn的身份。验证您无法创建或编辑书籍，但您可以下订单。
- en: When you are done testing the application using the two accounts, log out and
    ensure that you can’t access the Actuator endpoints by visiting <ip-address>/actuator/health,
    for example. NGINX, the technology that powers the Ingress Controller, will reply
    with a 403 response.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 当您使用两个账户测试完应用程序后，请登出并确保您不能通过访问<ip-address>/actuator/health等来访问Actuator端点。作为Ingress
    Controller的驱动技术，NGINX将回复403响应。
- en: Note If you’d like to provision the Grafana observability stack, refer to the
    instructions in the source code repository accompanying the book.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果您想部署Grafana可观察性堆栈，请参阅本书附带的源代码存储库中的说明。
- en: Great job! When you’re done using the production cluster, follow the last section
    of appendix B to delete all the cloud resources from DigitalOcean. That’s fundamental
    to avoid incurring unexpected costs.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 干得好！当您完成生产集群的使用后，请按照附录B的最后部分操作，从DigitalOcean删除所有云资源。这是避免意外费用的基本做法。
- en: Summary
  id: totrans-350
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The idea behind continuous delivery is that an application is always in a releasable
    state.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持续交付背后的理念是应用程序始终处于可发布状态。
- en: When the delivery pipeline completes its execution, you’ll obtain an artifact
    (the container image) you can use to deploy the application in production.
  id: totrans-352
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当交付管道完成执行时，您将获得一个工件（容器镜像），您可以使用它来在生产环境中部署应用程序。
- en: When it comes to continuous delivery, each release candidate should be uniquely
    identifiable.
  id: totrans-353
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在持续交付方面，每个发布候选版本都应该具有唯一标识。
- en: Using the Git commit hash, you can ensure uniqueness, traceability, and automation.
    Semantic versioning can be used as the *display name* communicated to users and
    customers.
  id: totrans-354
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Git提交哈希，您可以确保唯一性、可追溯性和自动化。语义版本控制可以用作传达给用户和客户的*显示名称*。
- en: At the end of the commit stage, a release candidate is delivered to the artifact
    repository. Next, the acceptance stage deploys the application in a production-like
    environment and runs functional and non-functional tests. If they all succeed,
    the release candidate is ready for production.
  id: totrans-355
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在提交阶段结束时，一个发布候选版本会被提交到工件存储库。接下来，验收阶段会在类似生产的环境中部署应用程序，并运行功能性和非功能性测试。如果所有测试都通过，则发布候选版本就准备好投入生产。
- en: The Kustomize approach to configuration customization is based on the concepts
    of bases and overlays. Overlays are built on top of base manifests and customized
    via patches.
  id: totrans-356
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Kustomize 对配置定制的处理方法基于基础和覆盖的概念。覆盖是在基础清单之上构建的，并通过补丁进行定制。
- en: You saw how to define patches for customizing environment variables, Secrets
    mounted as volumes, CPU and memory resources, ConfigMaps, and Ingress.
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您已经看到了如何定义补丁来定制环境变量、作为卷挂载的 Secrets、CPU 和内存资源、ConfigMaps 以及 Ingress。
- en: The final part of a deployment pipeline is the production stage, where the deployment
    manifests are updated with the newest release version and ultimately deployed.
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署管道的最后一部分是生产阶段，在这个阶段，部署清单会更新为最新的发布版本，并最终部署。
- en: Deployment can be push-based or pull-based.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署可以是基于推送的或基于拉取的。
- en: GitOps is a set of practices for operating and managing software systems.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitOps 是一套用于操作和管理软件系统的实践。
- en: GitOps is based on four principles according to which a system deployment should
    be declarative, versioned and immutable, pulled automatically, and continuously
    reconciled.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitOps 基于以下四个原则：系统部署应该是声明性的、版本化的、不可变的、自动拉取的，并且持续进行协调。
- en: Argo CD is a software agent running in a cluster that automatically pulls the
    desired state from a source repository and applies it to the cluster whenever
    the two states diverge. That’s how we implemented continuous deployment.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Argo CD 是一个在集群中运行的软件代理，它自动从源代码库拉取所需状态，并在两个状态发生分歧时将其应用到集群中。这就是我们实现持续部署的方式。
- en: '* * *'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.) See M. Fowler, “BlueGreenDeployment,” *MartinFowler.com*, March 1, 2010,
    [http://mng.bz/WxOl](http://mng.bz/WxOl).
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: (1.) 请参阅 M. Fowler 的文章“BlueGreenDeployment”，发表于 *MartinFowler.com*，2010年3月1日，[http://mng.bz/WxOl](http://mng.bz/WxOl).
- en: ^(2.) See D. Sato, “CanaryRelease,” *MartinFowler.com*, June 25, 2014, [http://mng.bz/8Mz5](http://mng.bz/8Mz5).
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: (2.) 请参阅 D. Sato 的文章“CanaryRelease”，发表于 *MartinFowler.com*，2014年6月25日，[http://mng.bz/8Mz5](http://mng.bz/8Mz5).
- en: ^(3.) See M. Fowler, “ContinuousDelivery,” *MartinFowler.com*, May 30, 2013,
    [http://mng.bz/7yXV](http://mng.bz/7yXV).
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: (3.) 请参阅 M. Fowler 的文章“ContinuousDelivery”，发表于 *MartinFowler.com*，2013年5月30日，[http://mng.bz/7yXV](http://mng.bz/7yXV).
