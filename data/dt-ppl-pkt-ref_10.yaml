- en: Chapter 10\. Measuring and Monitoring Pipeline Performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第10章。衡量和监控流水线性能
- en: Even the most well-designed data pipelines are not meant to be “set and forget.”
    The practice of measuring and monitoring the performance of pipelines is essential.
    You owe it to your team and stakeholders to set, and live up to, expectations
    when it comes to the reliability of your pipelines.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是设计最为良好的数据流水线也不应被视为“设定并忘记”。测量和监控流水线性能的实践至关重要。在流水线的可靠性方面，您有责任对团队和利益相关者设定并履行期望。
- en: 'This chapter outlines some tips and best practices for doing something that
    data teams deliver to others but surprisingly don’t always invest in themselves:
    collecting data and measuring performance of their work.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了一些技巧和最佳实践，这些技巧和实践通常由数据团队交付给其他人，但令人惊讶的是，他们并不总是在自己身上进行这些投资：收集数据并衡量其工作性能。
- en: Key Pipeline Metrics
  id: totrans-3
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 关键流水线指标
- en: Before you can determine what data you need to capture throughout your pipelines,
    you must first decide what metrics you want to track.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定需要在整个流水线中捕获哪些数据之前，首先必须决定要跟踪哪些指标。
- en: 'Choosing metrics should start with identifying what matters to you and your
    stakeholders. Some examples include the following:'
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 选择指标应该从确定对您和利益相关者重要的事物开始。一些示例包括以下内容：
- en: How many validation tests (see [Chapter 8](ch08.xhtml#ch08)) are run, and what
    percent of the total tests run pass
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 运行了多少验证测试（请参阅[第8章](ch08.xhtml#ch08)），以及总测试中通过的百分比是多少
- en: How frequently a specific DAG runs successfully
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特定DAG成功运行的频率有多高
- en: The total runtime of a pipeline over the course of weeks, months, and years
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流水线在几周、几个月和几年内的总运行时间
- en: The good news is that gathering the data needed to calculate such metrics is
    within reach. As you’ll see in the following sections, it’s possible to capture
    this data directly from infrastructure built earlier in this book; in particular,
    see Airflow ([Chapter 7](ch07.xhtml#ch07)) and the data validation framework ([Chapter 8](ch08.xhtml#ch08)).
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 收集计算这些指标所需的数据并不难，这在你看过接下来的章节后就会明白。可以直接从本书中早先构建的基础设施中捕获这些数据；尤其是，请参阅Airflow（[第7章](ch07.xhtml#ch07)）和数据验证框架（[第8章](ch08.xhtml#ch08)）。
- en: Prepping the Data Warehouse
  id: totrans-10
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 准备数据仓库
- en: Before you can monitor and report on the performance of your pipelines, you
    must of course capture the data required for such measurement. Thankfully, as
    a data professional you have the tools to do so right in front of you! Your data
    warehouse is the best place to store log data from each step in your data pipeline.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在监控和报告流水线性能之前，您当然需要捕获这些测量所需的数据。幸运的是，作为数据专业人士，您拥有在您面前的工具来执行此操作！您的数据仓库是存储来自数据流水线每个步骤的日志数据的最佳场所。
- en: In this section, I define the structure of the tables you’ll use to store data
    from Airflow and the data validation framework defined in [Chapter 8](ch08.xhtml#ch08).
    This data will later be used to develop the metrics essential to measuring pipeline
    performance.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我定义了用于存储来自Airflow和数据验证框架中定义的数据的表结构（请参阅[第8章](ch08.xhtml#ch08)）。这些数据稍后将用于开发衡量流水线性能的关键指标。
- en: I’d like to point out that there are numerous other data points you may want
    to track and report on. I like these two examples because they cover the basics
    and should inspire other tracking and measurements specific to your data infrastructure.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 我想指出，您可能希望跟踪和报告的其他数据点众多。我喜欢这两个示例，因为它们涵盖了基础知识，并应激发其他与您的数据基础架构相关的跟踪和测量。
- en: A Data Infrastructure Schema
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据基础架构模式
- en: 'First, you’ll need a table to store the history of DAG runs from Airflow. Recall
    from [Chapter 7](ch07.xhtml#ch07) that Airflow is used to execute each step in
    a data pipeline. It also keeps the history of each DAG run. Before you extract
    that data, you need a table to load it into. The following is a definition for
    a table named `dag_run_history`. It should be created in your data warehouse in
    whatever schema you load data into during data ingestion:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，您需要一张表来存储来自Airflow的DAG运行历史记录。回想一下[第7章](ch07.xhtml#ch07)中提到，Airflow用于执行数据流水线中的每个步骤。它还保留了每个DAG运行的历史记录。在提取这些数据之前，您需要一个表来加载这些数据。以下是名为`dag_run_history`的表的定义，应该在数据摄取过程中用于加载数据的模式中创建：
- en: '[PRE0]'
  id: totrans-16
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In addition to reporting on the performance of DAGs, it’s important to provide
    insight into data validity. In [Chapter 8](ch08.xhtml#ch08), I defined a simple,
    Python-based data validation framework. In this chapter I will extend it so that
    it logs the results of each validation test to the data warehouse. The following
    table, named `validation_run_history`, will be the destination of validation test
    results. I suggest creating it in the same schema of your data warehouse where
    ingested data lands upon load:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 除了报告DAG的性能外，还需要提供关于数据有效性的见解。在[第8章](ch08.xhtml#ch08)中，我定义了一个简单的基于Python的数据验证框架。在本章中，我将扩展它，以便将每个验证测试的结果记录到数据仓库中。名为`validation_run_history`的以下表将成为验证测试结果的目标。建议将其创建在您的数据仓库中与加载的数据相同的模式中：
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The rest of this chapter implements the logic to populate and make use of the
    data loaded into the prior two tables.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分实现了填充和利用先前两个表中加载数据的逻辑。
- en: Logging and Ingesting Performance Data
  id: totrans-20
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 记录和摄入性能数据
- en: Now it’s time to populate the two tables you created in your data warehouse
    in the previous section. The first will be populated by building a data ingestion
    job just like you learned about in Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05).
    The second will require an enhancement to the data validation application first
    introduced in [Chapter 8](ch08.xhtml#ch08).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是时候填充您在前一节中在数据仓库中创建的两个表了。第一个将通过构建一个数据摄入作业来填充，就像您在[第4章](ch04.xhtml#ch04)和[第5章](ch05.xhtml#ch05)中学到的那样。第二个将需要对首次在[第8章](ch08.xhtml#ch08)中介绍的数据验证应用程序进行增强。
- en: Ingesting DAG Run History from Airflow
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从Airflow摄入DAG运行历史记录
- en: To populate the `dag_run_history` table you created in your data warehouse in
    the previous section, you’ll need to extract data from the Airflow application
    database you configured in [“Apache Airflow Setup and Overview”](ch07.xhtml#apache-airflow-setup).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 要填充您在前一节中在数据仓库中创建的`dag_run_history`表，您需要从您在[“Apache Airflow Setup and Overview”](ch07.xhtml#apache-airflow-setup)中配置的Airflow应用程序数据库中提取数据。
- en: In that section, I chose to use a Postgres database for Airflow to use, so the
    following extraction code follows the model defined in [“Extracting Data from
    a PostgreSQL Database”](ch04.xhtml#extract-data-postgressql). Note that I am choosing
    to load data incrementally, which is easy, thanks to the auto-incrementing `id`
    column of the `dag_run` table in the Airflow database. The output of this extraction
    (defined in [Example 10-1](#ex_1001)) is a CSV file named *dag_run_extract.csv*,
    which is uploaded to the S3 bucket you set up in [Chapter 4](ch04.xhtml#ch04).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在该部分中，我选择使用Postgres数据库供Airflow使用，因此以下提取代码遵循[“从PostgreSQL数据库提取数据”](ch04.xhtml#extract-data-postgressql)中定义的模型。请注意，由于Airflow数据库中的`dag_run`表的自动递增`id`列，我选择增量加载数据是很容易的（定义在[示例10-1](#ex_1001)中）。其提取结果是一个名为*dag_run_extract.csv*的CSV文件，上传到您在[第4章](ch04.xhtml#ch04)中设置的S3存储桶。
- en: 'Before you execute the code, you’ll need to add one new section to the *pipeline.conf*
    file from [Chapter 4](ch04.xhtml#ch04). As the following shows, it must contain
    the connection details for the Airflow database you set up in [Chapter 7](ch07.xhtml#ch07):'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行代码之前，您需要向来自[第4章](ch04.xhtml#ch04)的*pipeline.conf*文件添加一个新部分。如下所示，它必须包含您在[第7章](ch07.xhtml#ch07)中设置的Airflow数据库的连接详细信息：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Example 10-1\. airflow_extract.py
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-1\. airflow_extract.py
- en: '[PRE3]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Once the extraction is complete, you can load the contents of the CSV file into
    your data warehouse as described in detail in [Chapter 5](ch05.xhtml#ch05). [Example 10-2](#ex_1002)
    defines how to do so if you have a Redshift data warehouse.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 提取完成后，您可以按照[第5章](ch05.xhtml#ch05)中详细描述的方法将CSV文件的内容加载到数据仓库中。如果您有一个Redshift数据仓库，[示例10-2](#ex_1002)定义了如何执行此操作。
- en: Example 10-2\. airflow_load.py
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-2\. airflow_load.py
- en: '[PRE4]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: You may want to run the ingestion once manually, but you can later schedule
    it via an Airflow DAG as I describe in a later section of this chapter.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 您可能希望手动运行一次摄入过程，但稍后可以通过Airflow DAG按计划运行，正如本章后面的部分所述。
- en: Adding Logging to the Data Validator
  id: totrans-33
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 向数据验证器添加日志记录
- en: 'To log the results of the validation tests first introduced in [Chapter 8](ch08.xhtml#ch08),
    I’ll add a function to the *validator.py* script called `log_result`. Because
    the script already connects to the data warehouse to run validation tests, I reuse
    the connection and simply `INSERT` a record with the test result:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 为了记录首次在[第8章](ch08.xhtml#ch08)中介绍的验证测试的结果，我将在*validator.py*脚本中添加一个名为`log_result`的函数。因为脚本已经连接到数据仓库以运行验证测试，所以我重用连接并简单地`INSERT`一个记录与测试结果：
- en: '[PRE5]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As a final modification, you’ll need to call the new function after a test is
    run. [Example 10-3](#ex_1003) defines the updated validator in its entirety after
    the logging code is added. With this addition, each time a validation test is
    run, the result is logged in the `validation_run_history` table.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的修改，您需要在测试运行后调用新函数。在添加日志记录代码后，[示例10-3](#ex_1003)完整定义了更新的验证器。通过此添加，每次运行验证测试时，结果都会记录在`validation_run_history`表中。
- en: I suggest running a few validation tests to generate test data for examples
    that follow. For more on running validation tests, please refer to [Chapter 8](ch08.xhtml#ch08).
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 我建议运行几个验证测试以生成接下来示例所需的测试数据。有关运行验证测试的更多信息，请参阅[第8章](ch08.xhtml#ch08)。
- en: Example 10-3\. validator_logging.py
  id: totrans-38
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-3\. validator_logging.py
- en: '[PRE6]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: For more on running validation tests, please see [Chapter 8](ch08.xhtml#ch08).
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 有关运行验证测试的更多信息，请参阅[第8章](ch08.xhtml#ch08)。
- en: Transforming Performance Data
  id: totrans-41
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换性能数据
- en: Now that you’re capturing key events from your pipelines and storing them in
    your data warehouse, you can make use of them to report on pipeline performance.
    The best way to do that is to build a simple data pipeline!
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已经捕获了管道中的关键事件并将它们存储在数据仓库中，您可以利用它们来报告管道的性能。最好的方法是构建一个简单的数据管道！
- en: Refer to the ELT pattern introduced in [Chapter 3](ch03.xhtml#ch03) and used
    throughout this book. The work to build a pipeline for reporting on the performance
    of each pipeline is nearly complete. The extract and load (EL) steps were taken
    care of in the previous section. All you have left is the transform (T) step.
    For this pipeline, that means turning the data from Airflow DAG runs, and other
    actions you’ve chosen to log, into the performance metrics you set out to measure
    and hold yourself accountable to.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 参考本书中引入的ELT模式（[第3章](ch03.xhtml#ch03)）以及整本书中使用的模式。构建用于报告每个管道性能的管道的工作几乎完成。在前一节中已处理提取和加载（EL）步骤。您所剩下的就是转换（T）步骤。对于此管道，这意味着将来自Airflow
    DAG运行的数据以及您选择记录的其他操作转换为您打算测量和对自己负责的性能指标。
- en: In the following subsections, I define transformations to create data models
    for some of the key metrics discussed earlier in the chapter.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下小节中，我定义了用于创建本章前面讨论过的一些关键指标数据模型的转换。
- en: DAG Success Rate
  id: totrans-45
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DAG成功率
- en: As you’ll recall from [Chapter 6](ch06.xhtml#ch06), you must consider the granularity
    of the data you want to model. In this case, I’d like to measure the success rate
    of each DAG by day. This level of granularity allows me to measure the success
    of either individual DAGs or multiple DAGs daily, weekly, monthly, or yearly.
    Whether the DAGs run once a day or more, this model will support a success rate.
    [Example 10-4](#ex_1004) defines the SQL to build the model. Note that this is
    a fully refreshed model for simplicity.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在[第6章](ch06.xhtml#ch06)中记得的那样，您必须考虑您希望建模的数据的粒度。在这种情况下，我想测量每个DAG按天的成功率。这种粒度水平使我能够每天、每周、每月或每年测量单个或多个DAG的成功率。无论DAG每天运行一次还是更多次，此模型都将支持成功率。[示例10-4](#ex_1004)定义了用于构建该模型的SQL。请注意，这是一个完全刷新的模型以简化操作。
- en: Example 10-4\. dag_history_daily.sql
  id: totrans-47
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例10-4\. dag_history_daily.sql
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: From the `dag_history_daily` table, you can measure the success rate of a single,
    or all DAGs, over a given date range. Here are a few examples based on runs of
    some DAGs defined in [Chapter 7](ch07.xhtml#ch07), but you’ll see data based on
    your own Airflow DAG run history. Make sure to run at least one ingestion of Airflow
    data (defined earlier in this chapter) to populate `dag_history_daily`.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以从`dag_history_daily`表中，测量单个或所有DAG在给定日期范围内的成功率。以下是基于在[第7章](ch07.xhtml#ch07)中定义的一些DAG运行的示例，但您将看到基于您自己的Airflow
    DAG运行历史数据。确保至少运行一次Airflow数据的摄取（在本章前面定义）以填充`dag_history_daily`。
- en: 'Here is a query to return the success rate by DAG. You can of course filter
    to a given DAG or date range. Note that you must `CAST` the `dag_run_count` as
    a `DECIMAL` to calculate a fractional success rate:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个查询，用于按DAG返回成功率。当然，您可以按给定的DAG或日期范围进行筛选。请注意，您必须将`dag_run_count`强制转换为`DECIMAL`以计算分数成功率：
- en: '[PRE8]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The output of the query will look something like this:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的输出将如下所示：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: DAG Runtime Change Over Time
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DAG运行时间随时间的变化
- en: Measuring the runtime of DAGs over time is often used to keep track of DAGs
    that are taking longer to complete over time, thus creating risk of data in the
    warehouse becoming stale. I’ll use the `dag_history_daily` table I created in
    the last subsection to calculate the average runtime of each DAG by day.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 经常使用时间来测量DAG的运行时间，以便跟踪随着时间推移完成时间较长的DAG，从而创建数据仓库中数据过时的风险。我将使用我在上一小节中创建的`dag_history_daily`表来计算每个DAG按日平均运行时间。
- en: Note that in the following query I only include successful DAG runs, but you
    may want to report on long-running DAG runs that failed (perhaps due to a timeout!)
    in some cases. Also keep in mind that because multiple runs of a given DAG may
    occur in a single day, I must average the runtimes of such DAGs in the query.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在以下查询中，我仅包括成功的DAG运行，但在某些情况下，您可能希望报告运行时间长且失败的DAG运行（也许是由于超时！）。还要记住，由于给定DAG的多次运行可能在单个日内发生，因此我必须在查询中对这些DAG的运行时间进行平均。
- en: Finally, because the `dag_history_daily` table is granular by date and `dag_state`,
    I don’t really need to sum the `runtime_seconds` and `dag_run_count`, but as a
    best practice I do. Why? If I, or another analyst, decided to change the logic
    to do something like include failed DAG runs as well, then the `SUM()` function
    would be required, yet easily missed.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，因为`dag_history_daily`表按日期和`dag_state`进行了细分，我实际上不需要对`runtime_seconds`和`dag_run_count`求和，但作为最佳实践，我会这样做。为什么呢？如果我或另一位分析师决定更改逻辑以包括失败的DAG运行等操作，那么`SUM()`函数就会是必需的，但很容易被忽视。
- en: 'Here is the query for the `elt_pipeline_sample` DAG from [Chapter 7](ch07.xhtml#ch07):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是从[第7章](ch07.xhtml#ch07)中的`elt_pipeline_sample` DAG的查询：
- en: '[PRE10]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output of the query will look something like this:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的输出将类似于以下内容：
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Validation Test Volume and Success Rate
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 验证测试量和成功率
- en: Thanks to the additional logging you added to the data validator earlier in
    this chapter, it’s now possible to measure the success rate of validation tests,
    as well as the overall volume of tests run.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 由于您在本章前面为数据验证器添加的额外日志，现在可以测量验证测试的成功率以及运行的测试总量。
- en: '[Example 10-5](#ex_1005) defines a new data model called `validator_summary_daily`
    that calculates and stores the results of each validator test at daily granularity.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-5](#ex_1005) 定义了一个名为`validator_summary_daily`的新数据模型，用于按日粒度计算和存储每个验证器测试的结果。'
- en: Example 10-5\. validator_summary_daily.sql
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-5\. validator_summary_daily.sql
- en: '[PRE12]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Though the logic to create `validator_summary_daily` is fairly straightforward,
    it’s worth calling out the `test_composite_name` column. In the absence of a unique
    name for each validation test (an enhancement worth considering), `test_composite_name`
    is the combination of the two scripts and operator for the test. It acts as a
    composite key that can be used to group validation test runs. For example, here
    is the SQL to calculate the percentage of time that each test passes. You can
    of course look at this by day, week, month, or any other time range you’d like:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管创建`validator_summary_daily`的逻辑相当简单，但还是值得注意`test_composite_name`列。在没有为每个验证测试指定唯一名称（一个值得考虑的增强功能）的情况下，`test_composite_name`是测试的两个脚本和运算符的组合。它充当了一个可以用来分组验证测试运行的复合键。例如，以下是计算每个测试通过时间百分比的SQL。当然，您可以按天、周、月或其他任何时间范围查看这些内容：
- en: '[PRE13]'
  id: totrans-68
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The output will look something like this:'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 输出将类似于以下内容：
- en: '[PRE14]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'As for the volume of test runs, you may want to view this by date, test, or
    both. As noted earlier, it’s important to keep this value in context. As you grow
    the number and complexity of your pipelines, you can use this measure to ensure
    that you’re keeping up on testing the validity of data throughout pipelines. The
    following SQL produces both the test count and the success rate by date. This
    is a dataset that you can plot on a double y-axis line chart or similar visualization:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 至于测试运行的数量，您可能希望按日期、测试或两者查看。如前所述，保持此值的上下文很重要。随着管道数量和复杂性的增加，您可以使用此措施确保在整个管道中测试数据的有效性。以下SQL通过日期生成测试计数和成功率。这是一个数据集，您可以在双Y轴线图或类似可视化上绘制：
- en: '[PRE15]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'The results will look something like this:'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 结果将类似于以下内容：
- en: '[PRE16]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Orchestrating a Performance Pipeline
  id: totrans-75
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 管理性能管道
- en: With the code from the previous sections in hand, you can create a new Airflow
    DAG to schedule and orchestrate a pipeline to ingest and transform the pipeline
    performance data. It may feel a bit recursive, but you can use the existing infrastructure
    you have for this type of operation. Keep in mind that this backward-looking reporting
    focused on insights and not something mission critical like uptime monitoring
    or alerting on pipelines. You never want to use the same infrastructure to do
    that!
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 有了前几节的代码，你可以创建一个新的Airflow DAG来调度和编排一个管道，以摄取和转换管道性能数据。这可能感觉有点递归，但你可以利用你已有的基础设施来执行这种类型的操作。请记住，这种向后看的报告侧重于洞察力，而不是像运行时间监控或管道警报那样的使命关键任务。你永远不想使用相同的基础设施来执行这些任务！
- en: The Performance DAG
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 性能DAG
- en: 'A DAG to orchestrate all of the steps defined in this chapter will look familiar
    based on examples from [Chapter 7](ch07.xhtml#ch07). Per [Example 10-3](#ex_1003),
    the results from the validation tests are already logging in the data warehouse.
    That means that there are only a few steps needed in this pipeline:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 基于[第 7章](ch07.xhtml#ch07)中的示例，编排所有在本章中定义的步骤的DAG将会很熟悉。根据[示例 10-3](#ex_1003)，验证测试的结果已经记录在数据仓库中。这意味着在这个管道中只需要进行少数几个步骤：
- en: Extract data from the Airflow database (per [Example 10-1](#ex_1001)).
  id: totrans-79
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Airflow数据库提取数据（参见[示例 10-1](#ex_1001)）。
- en: Load data from the Airflow extract into the warehouse (per [Example 10-2](#ex_1002)).
  id: totrans-80
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从Airflow提取的数据加载到数据仓库中（参见[示例 10-2](#ex_1002)）。
- en: Transform the Airflow history (per [Example 10-4](#ex_1004)).
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换Airflow历史（参见[示例 10-4](#ex_1004)）。
- en: Transform the data validation history (per [Example 10-5](#ex_1005)).
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换数据验证历史（参见[示例 10-5](#ex_1005)）。
- en: '[Example 10-6](#ex_1006) is the source for the Airflow DAG, and [Figure 10-1](#fig_1001)
    shows the DAG in graph form.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: '[示例 10-6](#ex_1006) 是Airflow DAG的源代码，而[图 10-1](#fig_1001) 显示了该DAG的图形形式。'
- en: '![dppr 1001](Images/dppr_1001.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![dppr 1001](Images/dppr_1001.png)'
- en: Figure 10-1\. Graph view of the pipeline_performance DAG.
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 10-1\. 流水线性能DAG的图形视图。
- en: Example 10-6\. pipeline_performance.py
  id: totrans-86
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 10-6\. pipeline_performance.py
- en: '[PRE17]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Performance Transparency
  id: totrans-88
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 性能透明度
- en: 'With a working pipeline to measure the performance of your production pipelines
    and data validation tests, there’s one last thing to keep in mind: sharing the
    resulting insights with your data team and stakeholders. Transparency of the pipeline
    performance is key to building trust with stakeholders and creating a sense of
    ownership and pride on your team.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 有了一个工作中的管道来衡量你的生产管道和数据验证测试的性能，还有一件事要记住：与你的数据团队和利益相关者分享由此产生的见解的重要性。管道性能的透明度对于建立与利益相关者的信任，以及在你的团队中创建归属感和自豪感至关重要。
- en: 'Here are a few tips for making use of the data and insights generated throughout
    this chapter:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些利用本章节生成的数据和见解的技巧：
- en: Leverage visualization tools
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 利用可视化工具
- en: Make the metrics from the data models you created accessible in the same visualization
    tools that your stakeholders use. That might be Tableau, Looker, or a similar
    product. Whatever it is, make sure it’s where stakeholders and your team are going
    every day, anyway.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 使你创建的数据模型的度量指标能够在与你的利益相关者使用的相同可视化工具中访问。可能是Tableau、Looker或类似产品。无论是什么，确保它是你的团队和利益相关者每天都会去的地方。
- en: Share summarized metrics regularly
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 定期分享总结的度量指标
- en: Share summarized metrics at least monthly (if not weekly) via email, Slack,
    or some other place that your team and stakeholders keep an eye on.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 至少每月（如果不是每周）通过电子邮件、Slack或团队和利益相关者关注的其他地方分享总结的度量指标。
- en: Watch trends, not just current values
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 观察趋势，而不仅仅是当前值
- en: Both on dashboards and summaries you share, don’t just share the latest values
    of each metric. Include change over time as well, and ensure that negative trends
    are pointed out as often as positive ones.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在你分享的仪表板和总结中，不仅仅分享每个指标的最新值。还包括随时间变化的情况，并确保负面趋势像正面趋势一样频繁地指出。
- en: React to trends
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 反应趋势
- en: Sharing trends in metrics isn’t just for show. It’s an opportunity to react
    and improve. Are validation tests failing at a higher rate than the month before?
    Dig into why, make changes, and watch future trends to measure the impact of your
    work.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 分享度量指标的趋势不仅仅是为了展示。这是一个反应和改进的机会。验证测试的失败率比前一个月高吗？深入挖掘原因，做出改变，并观察未来的趋势来衡量你的工作的影响。
