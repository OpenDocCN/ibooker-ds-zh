- en: 3 Balancing immediate and long-term goals
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 平衡短期和长期目标
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: You will learn about the challenges of learning from sequential feedback and
    how to properly balance immediate and long-term goals.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解从序列性反馈中学习的挑战以及如何正确平衡短期和长期目标。
- en: You will develop algorithms that can find the best policies of behavior in sequential
    decision-making problems modeled with MDPs.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将开发出能够找到用MDP建模的序列决策问题中最佳行为策略的算法。
- en: You will find the optimal policies for all environments for which you built
    MDPs in the previous chapter.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将找到你在上一章中为构建的MDP的所有环境的最佳策略。
- en: In preparing for battle I have always found that plans are useless, but planning
    is indispensable.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在准备战斗时，我总是发现计划是没有用的，但规划是必不可少的。
- en: — Dwight D. Eisenhower United States Army five-star general and 34th President
    of the United States
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 德怀特·D·艾森豪威尔，美国陆军五星上将和第34任美国总统
- en: 'In the last chapter, you built an MDP for the BW, BSW, and FL environments.
    MDPs are the motors moving RL environments. They define the problem: they describe
    how the agent interacts with the environment through state and action spaces,
    the agent’s goal through the reward function, how the environment reacts from
    the agent’s actions through the transition function, and how time should impact
    behavior through the discount factor.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你为BW、BSW和FL环境构建了MDP。MDP是推动强化学习环境的引擎。它们定义了问题：它们描述了智能体如何通过状态和动作空间与环境交互，智能体的目标通过奖励函数，环境如何通过转移函数对智能体的动作做出反应，以及时间如何通过折现因子影响行为。
- en: In this chapter, you’ll learn about algorithms for solving MDPs. We first discuss
    the objective of an agent and why simple plans are not sufficient to solve MDPs.
    We then talk about the two fundamental algorithms for solving MDPs under a technique
    called *dynamic policy iteration* (PI).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习解决马尔可夫决策过程（MDP）的算法。我们首先讨论智能体的目标以及为什么简单的计划不足以解决MDP问题。然后，我们讨论在称为**动态策略迭代**（PI）的技术下解决MDP的两个基本算法。
- en: 'You’ll soon notice that these methods in a way “cheat”: they require full access
    to the MDP, and they depend on knowing the dynamics of the environment, which
    is something we can’t always obtain. However, the fundamentals you’ll learn are
    still useful for learning about more advanced algorithms. In the end, VI and PI
    are the foundations from which virtually every other RL (and DRL) algorithm originates.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 你很快就会注意到，这些方法在某种程度上“作弊”：它们需要完全访问MDP，并且依赖于了解环境的动态，而这并不是我们总能获得的。然而，你将学习的这些基础知识对于了解更高级的算法仍然是有用的。最终，价值迭代（VI）和策略迭代（PI）是几乎所有其他强化学习（和深度强化学习）算法的基础。
- en: You’ll also notice that when an agent has full access to an MDP, there’s no
    uncertainty because you can look at the dynamics and rewards and calculate expectations
    directly. Calculating expectations directly means that there’s no need for exploration;
    that is, there’s no need to balance exploration and exploitation. There’s no need
    for interaction, so there’s no need for trial-and-error learning. All of this
    is because the feedback we’re using for learning in this chapter isn’t evaluative
    but supervised instead.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 你还会注意到，当一个智能体完全访问MDP时，没有不确定性，因为你可以查看动态和奖励并直接计算期望。直接计算期望意味着不需要探索；也就是说，不需要平衡探索和利用。不需要交互，因此不需要试错学习。所有这一切都是因为我们在本章用于学习的反馈不是评估性的，而是监督性的。
- en: Remember, in DRL, agents learn from feedback that’s simultaneously sequential
    (as opposed to one shot), evaluative (as opposed to supervised), and sampled (as
    opposed to exhaustive). What I’m doing in this chapter is eliminating the complexity
    that comes with learning from evaluative and sampled feedback, and studying sequential
    feedback in isolation. In this chapter, we learn from feedback that’s *sequential*,
    supervised, and exhaustive.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在深度强化学习（DRL）中，智能体从同时具有序列性（与一次性相反）、评估性（与监督性相反）和采样性（与穷举性相反）的反馈中学习。在本章中，我所做的是消除从评估性和采样性反馈中带来的复杂性，并单独研究序列性反馈。在本章中，我们从序列性、监督性和穷举性的反馈中学习。
- en: The objective of a decision-making agent
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决策智能体的目标
- en: 'At first, it seems the agent’s goal is to find a sequence of actions that will
    maximize the return: the sum of rewards (discounted or undiscounted—depending
    on the value of gamma) during an episode or the entire life of the agent, depending
    on the task.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 起初，似乎智能体的目标是找到一系列动作，以最大化回报：在一段时间或智能体的整个生命周期内，根据任务的不同，是奖励的总和（折现或未折现，取决于伽马值）。
- en: Let me introduce a new environment to explain these concepts more concretely.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 让我介绍一个新的环境来更具体地解释这些概念。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe Slippery Walk Five
    (SWF) environment |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| ![图标](../Images/icons_Concrete.png) | 一个具体的例子：滑稽的走步五（SWF）环境'
- en: '|  | The Slippery Walk Five (SWF) is a one-row grid-world environment (a walk),
    that’s stochastic, similar to the Frozen Lake, and it has only five non-terminal
    states (seven total if we count the two terminal).![](../Images/03_01__Sidebar01.png)The
    Slippery Walk Five environmentThe agent starts in *s, H* is a hole, *g* is the
    goal and provides a +1 reward. |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '|  | 滑稽的走步五（SWF）是一个单行网格世界环境（一次走步），它是随机的，类似于冰冻湖，并且它只有五个非终止状态（如果计算两个终止状态，总共是七个）。![侧边栏图片](../Images/03_01__Sidebar01.png)滑稽的走步五环境智能体从*s,
    H*开始，*H*是洞，*g*是目标，提供+1奖励。|'
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe return *G* |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学：回报 *G*'
- en: '|  | ![](../Images/03_01__Sidebar02.png) |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏图片](../Images/03_01__Sidebar02.png) |'
- en: 'You can think of returns as backward looking—“how much you got” from a past
    time step; but another way to look at it is as a “reward-to-go”—basically, forward
    looking. For example, imagine an episode in the SWF environment went this way:
    *3/0, 4/0, 5/0, 4/0, 5/0, 6/1*. What’s the return of this trajectory/episode?'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以将回报视为向后看——“从过去的时间步中得到了多少”；但另一种看待它的方式是“未来的奖励”——基本上是向前看。例如，想象在SWF环境中有一个这样的场景：*3/0,
    4/0, 5/0, 4/0, 5/0, 6/1*。这个轨迹/场景的回报是多少？
- en: Well, if we use discounting the math would work out this way.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，如果我们使用折现，数学将这样计算。
- en: '![](../Images/03_02.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_02.png)'
- en: Discounted return in the slippery walk five environment
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 滑稽的走步五环境中的折现回报
- en: If we don’t use discounting, well, the return would be 1 for this trajectory
    and all trajectories that end in the right-most cell, state 6, and 0 for all trajectories
    that terminate in the left-most cell, state 0.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们不使用折现，那么，对于这个轨迹以及所有以最右侧单元格、状态6结束的轨迹，回报将是1，而对于所有以最左侧单元格、状态0结束的轨迹，回报将是0。
- en: In the SWF environment, it’s evident that going right is the best thing to do.
    It may seem, therefore, that all the agent must find is something called a *plan*—that
    is, a sequence of actions from the *gOAL* state. But this doesn’t always work.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在SWF环境中，很明显向右走是最好的选择。因此，似乎所有智能体必须找到的只是一些被称为*计划*的东西——即从*目标*状态开始的一系列动作。但这并不总是有效。
- en: '![](../Images/03_03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_03.png)'
- en: A solid plan in the SWF environment
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: SWF环境中的稳固计划
- en: In the FL environment, a plan would look like the following.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL环境中，计划看起来如下。
- en: '![](../Images/03_04.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_04.png)'
- en: A solid plan in the FL environment
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: FL环境中的稳固计划
- en: But this isn’t enough! The problem with plans is they don’t account for stochasticity
    in environments, and both the SWF and FL are stochastic*;* actions taken won’t
    always work the way we intend. What would happen if, due to the environment’s
    stochasticity, our agent lands on a cell not covered by our plan?
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 但这还不够！计划的问题在于它们没有考虑到环境中的随机性，而SWF和FL都是随机的*；*采取的动作不一定总是按照我们的意图行事。如果由于环境的随机性，我们的智能体落在我们的计划未覆盖的单元格上，会发生什么？
- en: '![](../Images/03_05.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_05.png)'
- en: A possible hole in our plan
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们计划中可能存在的漏洞
- en: Same happens in the FL environment.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL环境中也发生同样的事情。
- en: '![](../Images/03_06.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_06.png)'
- en: Plans aren’t enough in stochastic environments
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在随机环境中，计划是不够的
- en: 'What the agent needs to come up with is called a *policy*. Policies are universal
    plans; policies cover all possible states. We need to plan for every possible
    state. Policies can be *s*tochastic or deterministic: the policy can return action-probability
    distributions or single actions for a given state (or observation). For now, we’re
    working with deterministic policies, which is a lookup table that maps actions
    to states.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体需要提出的是所谓的*策略*。策略是通用的计划；策略覆盖所有可能的状态。我们需要为每个可能的状态进行规划。策略可以是*随机*的或确定的：策略可以为给定状态（或观察）返回动作概率分布或单个动作。目前，我们正在使用确定性的策略，这是一个将动作映射到状态的查找表。
- en: '![](../Images/03_07.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_07.png)'
- en: Optimal policy in the SWF environment
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: SWF环境中的最优策略
- en: In the SWF environment, the optimal policy is always going right, for every
    single state. Great, but there are still many unanswered questions. For instance,
    how much reward should I expect from this policy? Because, even though we know
    how to act optimally, the environment might send our agent backward to the hole
    even if we always select to go toward the goal. This is why returns aren’t enough.
    The agent is really looking to maximize the expected return; that means the return
    taking into account the environment’s stochasticity.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在SWF环境中，最优策略总是向右走，对于每个状态。很好，但仍然有许多未解决的问题。例如，我应该从这个策略中期望多少奖励？因为，尽管我们知道如何采取最优行动，但环境可能会将我们的智能体送回洞中，即使我们总是选择朝向目标前进。这就是为什么回报还不够。智能体真正寻求的是最大化期望回报；这意味着考虑环境随机性的回报。
- en: Also, we need a method to automatically find optimal policies, because in the
    FL example, for instance, it isn’t obvious at all what the optimal policy looks
    like!
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们需要一种方法来自动找到最优策略，因为在FL示例中，例如，最优策略看起来并不明显！
- en: 'There are a few components that are kept internal to the agent that can help
    it find optimal behavior: there are policies, there can be multiple policies for
    a given environment, and in fact, in certain environments, there may be multiple
    optimal policies. Also, there are value functions to help us keep track of return
    estimates. There’s a single optimal value function for a given MDP, but there
    may be multiple value functions in general.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 在智能体内部保留了一些组件，可以帮助它找到最优行为：有策略，对于给定的环境可能有多个策略，实际上，在特定环境中，可能有多个最优策略。此外，还有价值函数帮助我们跟踪回报估计。对于给定的MDP，有一个单一的最优价值函数，但通常可能有多个价值函数。
- en: Let’s look at all the components internal to a reinforcement learning agent
    that allow them to learn and find optimal policies, with examples to make all
    of this more concrete.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看强化学习智能体内部的所有组件，这些组件允许它们学习和找到最优策略，并用示例使所有这些更加具体。
- en: 'Policies: Per-state action prescriptions'
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略：每个状态的动作规定
- en: 'Given the stochasticity in the Frozen Lake environment (and most reinforcement
    learning problems,) the agent needs to find a policy, denoted as *π*. A policy
    is a function that prescribes actions to take for a given nonterminal state. (Remember,
    policies can be stochastic: either directly over an action or a probability distribution
    over actions. We’ll expand on stochastic policies in later chapters.)'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到Frozen Lake环境中的随机性（以及大多数强化学习问题），智能体需要找到一个策略，记为π。策略是为给定非终端状态规定采取行动的函数。（记住，策略可以是随机的：直接针对一个动作或动作的概率分布。我们将在后面的章节中扩展随机策略。）
- en: Here’s a sample policy.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个示例策略。
- en: '![](../Images/03_08.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_08.png)'
- en: A randomly generated policy
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 一个随机生成的策略
- en: 'One immediate question that arises when looking at a policy is this: how good
    is this policy? If we find a way to put a number to policies, we could also ask
    the question, how much better is this policy compared to another policy?'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 当查看一个策略时，一个立即出现的问题是这样的：这个策略有多好？如果我们找到一种方法给策略赋予数字，我们也可以问，这个策略比另一个策略好多少？
- en: '![](../Images/03_09.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_09.png)'
- en: How can we compare policies?
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何比较策略？
- en: 'State-value function: What to expect from here?'
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态价值函数：从这里可以期待什么？
- en: Something that’d help us compare policies is to put numbers to states for a
    given policy. That is, if we’re given a policy and the MDP, we should be able
    to calculate the expected return starting from every single state (we care mostly
    about the START state). How can we calculate how valuable being in a state is?
    For instance, if our agent is in state 14 (to the left of the GOAL), how is that
    better than being in state 13 (to the left of 14)? And precisely how much better
    is it? More importantly, under which policy would we have better results, the
    Go-get-it or the Careful policy?
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 要帮助我们比较策略，我们可以为给定策略中的状态赋予数字。也就是说，如果我们给定一个策略和MDP，我们应该能够从每个单一状态开始计算期望回报（我们主要关心起始状态）。我们如何计算处于某个状态的价值？例如，如果我们的智能体处于状态14（目标左侧），这比处于状态13（14左侧）好多少？以及它究竟好多少？更重要的是，在哪种策略下我们会得到更好的结果，是“积极进取”策略还是“谨慎”策略？
- en: Let’s give it a quick try with the Go-get-it policy. What is the value of being
    in state 14 under the Go-get-it policy?
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用“积极进取”策略快速尝试一下。在“积极进取”策略下，处于状态14的价值是多少？
- en: '![](../Images/03_10.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_10.png)'
- en: What’s the value of being in state 14 when running the Go-get-it policy?
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在运行“积极进取”策略时，处于状态14的价值是多少？
- en: Okay, so it isn’t that straightforward to calculate the value of state 14 when
    following the Go-get-it policy because of the dependence on the values of other
    states (10 and 14, in this case), which we don’t have either. It’s like the chicken-or-the-egg
    problem. Let’s keep going.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，按照“立即行动”策略计算状态14的值并不那么简单，因为依赖于其他状态（在这个例子中是状态10和14）的值，而这些值我们也没有。这就像“先有鸡还是先有蛋”的问题。让我们继续前进。
- en: We defined the *π*. Remember, we’re under stochastic environments, so we must
    account for all the possible ways the environment can react to our policy! That’s
    what an expectation gives us.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义了π。记住，我们处于随机环境中，因此我们必须考虑环境对我们策略的所有可能反应！这正是期望值给我们的。
- en: 'We now define the value of a state s when following a policy *π*: the value
    of a state s under policy *π* is the expectation of returns if the agent follows
    policy *π* starting from state *s*. Calculate this for every state, and you get
    the state-value function, or V-function or value function. It represents the expected
    return when following policy *π* from state *s*.'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义在遵循策略π时状态s的值：状态s在策略π下的值是如果智能体从状态s开始遵循策略π，则回报的期望值。为每个状态计算这个值，你得到状态值函数，或V函数或值函数。它表示遵循策略π从状态s开始时的期望回报。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe state-value function
    *v* |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式'
- en: '|  | ![](../Images/03_10__Sidebar03.png) |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/03_10__Sidebar03.png) |'
- en: These equations are fascinating. A bit of a mess given the recursive dependencies,
    but still interesting. Notice how the value of a state depends recursively on
    the value of possibly many other states, which values may also depend on others,
    including the original state!
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方程非常迷人。由于递归依赖关系，有点混乱，但仍然很有趣。注意状态的价值如何递归地依赖于可能许多其他状态的值，而这些值也可能依赖于其他状态，包括原始状态！
- en: The recursive relationship between states and successive states will come back
    in the next section when we look at algorithms that can iteratively solve these
    equations and obtain the state-value function of any policy in the FL environment
    (or any other environment, really).
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 状态和后续状态之间的递归关系将在下一节中再次出现，当我们查看可以迭代求解这些方程并获取FL环境（或任何其他环境）中任何策略的状态值函数的算法时。
- en: For now, let’s continue exploring other components commonly found in RL agents.
    We’ll learn how to calculate these values later in this chapter. Note that the
    state-value function is often referred to as the value function, or even the V-function,
    or more simply *v**π(s)*. It may be confusing, but you’ll get used to it.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们继续探索RL智能体中常见的其他组件。我们将在本章后面学习如何计算这些值。请注意，状态值函数通常被称为值函数，甚至称为V函数，或者更简单地说，*v**π(s)*。这可能有点令人困惑，但你会习惯的。
- en: 'Action-value function: What should I expect from here if I do this?'
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作值函数：如果我这样做，我应该期望从这里得到什么？
- en: Another critical question that we often need to ask isn’t merely about the value
    of a state but the value of taking action *a* in a state *s*. Differentiating
    answers to this kind of question will help us decide between actions.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个我们经常需要问的至关重要的问题不仅仅是关于状态的值，而是在状态s中采取动作a的值。区分这类问题的答案将帮助我们决定采取哪种动作。
- en: For instance, notice that the Go-get-it policy goes right when in state 14,
    but the Careful policy goes down. But which action is better? More specifically,
    which action is better under each policy? That is, what is the value of going
    down, instead of right, and then following the Go-get-it policy, and what is the
    value of going right, instead of down, and then following the Careful policy?
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，注意“立即行动”策略在状态14时是向右走的，但“谨慎”策略是向下走的。但哪个动作更好？更具体地说，在每种策略下哪个动作更好？也就是说，向下走而不是向右走的值是多少，以及向右走而不是向下走的值是多少，然后遵循“立即行动”策略？
- en: 'By comparing between different actions under the same policy, we can select
    better actions, and therefore improve our policies. The *action-value function*,
    also known as *Q-function* or *Q^π**(s,a)*, captures precisely this: the expected
    return if the agent follows policy *π* after taking action *a* in state *s*.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 通过比较同一策略下的不同动作，我们可以选择更好的动作，从而改进我们的策略。动作值函数，也称为Q函数或Q^π**(s,a)*，精确地捕捉了这一点：智能体在状态s中采取动作a后遵循策略π的期望回报。
- en: 'In fact, when we care about improving policies, which is often referred to
    as the control problem, we need action-value functions. Think about it: if you
    don’t have an MDP, how can you decide what action to take merely by knowing the
    values of all states? V-functions don’t capture the dynamics of the environment.
    The Q-function, on the other hand, does somewhat capture the dynamics of the environment
    and allows you to improve policies without the need for MDPs. We expand on this
    fact in later chapters.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，当我们关心改进策略时，这通常被称为控制问题，我们需要动作值函数。想想看：如果你没有MDP，你怎么能仅仅通过知道所有状态的价值来决定采取什么动作？V函数没有捕捉到环境的动态。另一方面，Q函数在一定程度上捕捉了环境的动态，并允许你在不需要MDP的情况下改进策略。我们将在后面的章节中进一步阐述这一事实。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe action-value function
    *Q* |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '![图片](../Images/icons_Math.png) 显示数学公式动作值函数*Q*'
- en: '|  | ![](../Images/03_10_Sidebar04.png) |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/03_10_Sidebar04.png) |'
- en: 'Action-advantage function: How much better if I do that?'
  id: totrans-71
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作优势函数：如果我这么做会好多少？
- en: Another type of value function is derived from the previous two. The *action-advantage
    function*, also known as *advantage function*, *a-function*, or *a^π**(s, a),*
    is the difference between the action-value function of action *a* in state *s*
    and the state-value function of state *s* under policy *π*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种价值函数是从前两种函数推导出来的。这种*动作优势函数*，也称为*优势函数*、*a函数*或*a^π**(s, a)*，是动作*a*在状态*s*的动作值函数与在策略*π*下状态*s*的状态值函数之间的差值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe action-advantage function
    *A* |'
  id: totrans-73
  prefs: []
  type: TYPE_TB
  zh: '![图片](../Images/icons_Math.png) 显示数学公式动作优势函数*A*'
- en: '|  | ![](../Images/03_10_Sidebar05.png) |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/03_10_Sidebar05.png) |'
- en: 'The advantage function describes how much better it is to take action *a* instead
    of following policy *π*: the advantage of choosing action *a* over the default
    action.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 优势函数描述了采取动作*a*而不是遵循策略*π*的好处：选择动作*a*相对于默认动作的优势。
- en: Look at the different value functions for a (dumb) policy in the SWF environment.
    Remember, these values depend on the policy. In other words, the *Q*π*(s, a)*
    assumes you’ll follow policy *π* (always left in the following example) and right
    after taking action *a* in state *s*.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 看看SWF环境中（愚蠢的）策略的不同价值函数。记住，这些值取决于策略。换句话说，*Q*π*(s, a)*假设你会遵循策略*π*（在以下示例中始终向左）并在状态*s*采取动作*a*之后。
- en: '![](../Images/03_11.png)'
  id: totrans-77
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_11.png)'
- en: State-value, action-value, and action-advantage functions
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数、动作值函数和动作优势函数
- en: Optimality
  id: totrans-79
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 最优性
- en: Policies, state-value functions, action-value functions, and action-advantage
    functions are the components we use to describe, evaluate, and improve behaviors.
    We call it *Optimality* when these components are the best they can be.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 策略、状态值函数、动作值函数和动作优势函数是我们用来描述、评估和改进行为的组件。当这些组件处于最佳状态时，我们称之为*最优性*。
- en: An *Optimal policy* is a policy that for every state can obtain expected returns
    greater than or equal to any other policy. An optimal state-value function is
    a state-value function with the maximum value across all policies for all states.
    Likewise, an optimal action-value function is an action-value function with the
    maximum value across all policies for all state-action pairs. The optimal action-advantage
    function follows a similar pattern, but notice an optimal advantage function would
    be equal to or less than zero for all state-action pairs, since no action could
    have any advantage from the optimal state-value function.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '*最优策略*是一种策略，对于每个状态，它可以获得大于或等于任何其他策略的预期回报。最优状态值函数是所有策略对所有状态的最大值的状态值函数。同样，最优动作值函数是所有策略对所有状态-动作对的最大值的动作值函数。最优动作优势函数遵循类似的模式，但请注意，最优优势函数对于所有状态-动作对都等于或小于零，因为没有任何动作可以从最优状态值函数中获得任何优势。'
- en: Also, notice that although there could be more than one optimal policy for a
    given MDP, there can only be one optimal state-value function, optimal action-value
    function, and optimal action-advantage function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，请注意，尽管对于给定的MDP可能有多个最优策略，但只有一个最优状态值函数、最优动作值函数和最优动作优势函数。
- en: You may also notice that if you had the optimal V-function, you could use the
    MDP to do a one-step search for the optimal Q-function and then use this to build
    the optimal policy. On the other hand, if you had the optimal Q-function, you
    don’t need the MDP at all. You could use the optimal Q-function to find the optimal
    V-function by merely taking the maximum over the actions. And you could obtain
    the optimal policy using the optimal Q-function by taking the argmax over the
    actions.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能也注意到，如果你有了最优的V函数，你可以使用MDP进行一步搜索以找到最优的Q函数，然后使用这个函数构建最优策略。另一方面，如果你有了最优的Q函数，你根本不需要MDP。你可以通过仅对动作取最大值来使用最优Q函数找到最优的V函数。你也可以通过对动作取argmax来使用最优Q函数获得最优策略。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe Bellman optimality equations
    |'
  id: totrans-84
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Math.png) | 展示数学The Bellman最优方程 |'
- en: '|  | ![](../Images/03_11__Sidebar06.png) |'
  id: totrans-85
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/03_11__Sidebar06.png) |'
- en: Planning optimal sequences of actions
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 规划最优动作序列
- en: We have state-value functions to keep track of the values of states, action-value
    functions to keep track of the values of state-action pairs, and action-advantage
    functions to show the “advantage” of taking specific actions. We have equations
    for all of these to evaluate current policies, that is, to go from policies to
    value functions, and to calculate and find optimal value functions and, therefore,
    optimal policies.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有状态值函数来跟踪状态的价值，动作值函数来跟踪状态-动作对的价值，动作优势函数来显示采取特定动作的“优势”。我们有所有这些方程来评估当前策略，即从策略到值函数，并计算和找到最优值函数，因此找到最优策略。
- en: Now that we’ve discussed the reinforcement learning problem formulation, and
    we’ve defined the objective we are after, we can start exploring methods for finding
    this objective. Iteratively computing the equations presented in the previous
    section is one of the most common ways to solve a reinforcement learning problem
    and obtain optimal policies when the dynamics of the environment, the MDPs, are
    known. Let’s look at the methods.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了强化学习问题的公式化，并且我们已经定义了我们追求的目标，我们可以开始探索寻找这个目标的方法。迭代计算前一小节中提出的方程是解决强化学习问题并获得已知环境动态、MDPs时的最优策略的最常见方法之一。让我们看看这些方法。
- en: 'Policy evaluation: Rating policies'
  id: totrans-89
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 策略评估：评估策略
- en: We talked about comparing policies in the previous section. We established that
    policy *π* is better than or equal to policy *πq*'* if the expected return is
    better than or equal to *πq*'* for all states. Before we can use this definition,
    however, we must devise an algorithm for evaluating an arbitrary policy. Such
    an algorithm is known as an *iterative policy evaluation* or just *policy evaluation*.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在上一节中讨论了比较策略。我们确定，如果对于所有状态，策略π的期望回报优于或等于策略πq'*，则策略π*优于或等于策略πq*'*。然而，在我们能够使用这个定义之前，我们必须设计一个评估任意策略的算法。这种算法被称为*迭代策略评估*或简称为*策略评估*。
- en: The policy-evaluation algorithm consists of calculating the V-function for a
    given policy by sweeping through the state space and iteratively improving estimates.
    We refer to the type of algorithm that takes in a policy and outputs a value function
    as an algorithm that solves the **prediction problem**, which is calculating the
    values of a predetermined policy.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估算法包括通过遍历状态空间并迭代改进估计来计算给定策略的V函数。我们将接受策略并输出值函数的算法称为解决**预测问题**的算法，即计算预定策略的价值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe policy-evaluation equation
    |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Math.png) | 展示数学The policy-evaluation equation |'
- en: '|  | ![](../Images/03_11__Sidebar07.png) |'
  id: totrans-93
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/03_11__Sidebar07.png) |'
- en: Using this equation, we can iteratively approximate the true V-function of an
    arbitrary policy. The iterative policy-evaluation algorithm is guaranteed to converge
    to the value function of the policy if given enough iterations, more concretely
    as we approach infinity. In practice, however, we use a small threshold to check
    for changes in the value function we’re approximating. Once the changes in the
    value function are less than this threshold, we stop.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，我们可以迭代地近似任意策略的真实V函数。迭代策略评估算法在给定足够迭代次数的情况下，保证收敛到策略的价值函数，更具体地说，当我们接近无穷大时。然而，在实践中，我们使用一个小的阈值来检查我们近似的价值函数的变化。一旦价值函数的变化小于这个阈值，我们就停止。
- en: Let’s see how this algorithm works in the SWF environment, for the always-left
    policy.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个算法在SWF环境中，对于始终向左的策略是如何工作的。
- en: '![](../Images/03_12.png)'
  id: totrans-96
  prefs: []
  type: TYPE_IMG
  zh: '![03_12.png](../Images/03_12.png)'
- en: Initial calculations of policy evaluation
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 政策评估的初始计算
- en: You then calculate the values for all states 0–6, and when done, move to the
    next iteration. Notice that to calculate *v*[2]*π(s)* you’d have to use the estimates
    obtained in the previous iteration, *v*[1]*π(s)*. This technique of calculating
    an estimate from an estimate is referred to as *bootstrapping*, and it’s a widely
    used technique in RL (including DRL).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，你计算所有状态0-6的值，完成后，进入下一个迭代。请注意，为了计算*v*[2]*π(s)*，你必须使用前一个迭代中获得的估计值，*v*[1]*π(s)*。这种从估计值计算估计值的技术被称为*bootstrapping*，并且在强化学习（包括深度强化学习）中得到了广泛的应用。
- en: Also, it’s important to notice that the *K*’s here are iterations across estimates,
    but they’re not interactions with the environment. These aren’t episodes that
    the agent is out and about selecting actions and observing the environment. These
    aren’t time steps either. Instead, these are the iterations of the iterative policy-evaluation
    algorithm. Do a couple more of these estimates. The following table shows you
    the results you should get.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，重要的是要注意，这里的*K*是估计之间的迭代，但它们并不是与环境交互。这些不是智能体在外部选择动作并观察环境的回合。这些也不是时间步。相反，这些是迭代策略评估算法的迭代。进行更多的这些估计。下表显示了您应该得到的结果。
- en: '| k | V^π(0) | V^π(1) | V^π(2) | V^π(3) | V^π(4) | V^π(5) | V^π(6) |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| k | V^π(0) | V^π(1) | V^π(2) | V^π(3) | V^π(4) | V^π(5) | V^π(6) |'
- en: '| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |'
- en: '| 1 | 0 | 0 | 0 | 0 | 0 | 0.1667 | 0 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0 | 0 | 0 | 0 | 0 | 0.1667 | 0 |'
- en: '| 2 | 0 | 0 | 0 | 0 | 0.0278 | 0.2222 | 0 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0 | 0 | 0 | 0 | 0.0278 | 0.2222 | 0 |'
- en: '| 3 | 0 | 0 | 0 | 0.0046 | 0.0463 | 0.2546 | 0 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0 | 0 | 0 | 0.0046 | 0.0463 | 0.2546 | 0 |'
- en: '| 4 | 0 | 0 | 0.0008 | 0.0093 | 0.0602 | 0.2747 | 0 |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 0 | 0 | 0.0008 | 0.0093 | 0.0602 | 0.2747 | 0 |'
- en: '| 5 | 0 | 0.0001 | 0.0018 | 0.0135 | 0.0705 | 0.2883 | 0 |'
  id: totrans-106
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 0 | 0.0001 | 0.0018 | 0.0135 | 0.0705 | 0.2883 | 0 |'
- en: '| 6 | 0 | 0.0003 | 0.0029 | 0.0171 | 0.0783 | 0.2980 | 0 |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 0 | 0.0003 | 0.0029 | 0.0171 | 0.0783 | 0.2980 | 0 |'
- en: '| 7 | 0 | 0.0006 | 0.0040 | 0.0202 | 0.0843 | 0.3052 | 0 |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 0 | 0.0006 | 0.0040 | 0.0202 | 0.0843 | 0.3052 | 0 |'
- en: '| 8 | 0 | 0.0009 | 0.0050 | 0.0228 | 0.0891 | 0.3106 | 0 |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 8 | 0 | 0.0009 | 0.0050 | 0.0228 | 0.0891 | 0.3106 | 0 |'
- en: '| 9 | 0 | 0.0011 | 0.0059 | 0.0249 | 0.0929 | 0.3147 | 0 |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 9 | 0 | 0.0011 | 0.0059 | 0.0249 | 0.0929 | 0.3147 | 0 |'
- en: '| 10 | 0 | 0.0014 | 0.0067 | 0.0267 | 0.0959 | 0.318 | 0 |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 0 | 0.0014 | 0.0067 | 0.0267 | 0.0959 | 0.318 | 0 |'
- en: '| ... | ... | ... | ... | ... | ... | ... | ... |'
  id: totrans-112
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... | ... | ... | ... |'
- en: '| 104 | 0 | 0.0027 | 0.011 | 0.0357 | 0.1099 | 0.3324 | 0 |'
  id: totrans-113
  prefs: []
  type: TYPE_TB
  zh: '| 104 | 0 | 0.0027 | 0.011 | 0.0357 | 0.1099 | 0.3324 | 0 |'
- en: What are several of the things the resulting state-value function tells us?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，结果的状态值函数告诉我们哪些事情？
- en: Well, to begin with, we can say we get a return of 0.0357 in expectation when
    starting an episode in this environment and following the always-left policy.
    Pretty low.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们可以说，在这个环境中开始一个回合并遵循始终向左的策略时，我们期望获得0.0357的回报。相当低。
- en: We can also say, that even when we find ourselves in state 1 (the leftmost non-terminal
    state), we still have a chance, albeit less than one percent, to end up in the
    GOAL cell (state 6). To be exact, we have a 0.27% chance of ending up in the GOAL
    state when we’re in state 1\. And we select left all the time! Pretty interesting.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以说，即使我们发现自己处于状态1（最左边的非终端状态），我们仍然有不到1%的机会最终进入目标单元格（状态6）。更准确地说，当我们处于状态1时，我们有0.27%的机会进入目标状态。而且我们一直选择向左！非常有趣。
- en: Interestingly also, due to the stochasticity of this environment, we have a
    3.57% chance of reaching the GOAL cell (remember this environment has 50% action
    success, 33.33% no effects, and 16.66% backward). Again, this is when under an
    always-left policy. Still, the Left action could send us right, then right and
    right again, or left, right, right, right, right, and so on.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，由于这个环境的随机性，我们有3.57%的机会到达目标单元格（记住这个环境有50%的动作成功率，33.33%无效果，16.66%反向）。再次强调，这是在始终向左的策略下。尽管如此，左边的动作可能会让我们向右移动，然后再次向右，或者向左，然后向右，向右，向右，等等。
- en: Think about how the probabilities of trajectories combine. Also, pay attention
    to the iterations and how the values propagate backward from the reward (transition
    from state 5 to state 6) one step at a time. This backward propagation of the
    values is a common characteristic among RL algorithms and comes up again several
    times.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑轨迹概率的组合方式。同时，注意迭代过程以及值是如何从奖励（从状态5到状态6的转换）一步一步向后传播的。这种值的反向传播是强化学习算法的常见特征，并且在多个地方再次出现。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe policy-evaluation algorithm
    |'
  id: totrans-119
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python政策评估算法 |'
- en: '|  |'
  id: totrans-120
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① This is a full implementation of the policy-evaluation algorithm. All we need
    is the policy we’re trying to evaluate and the MDP the policy runs on. The discount
    factor, gamma, defaults to 1, and theta is a small number that we use to check
    for convergence.② Here we initialize the first-iteration estimates of the state-value
    function to zero.③ We begin by looping “forever” ...④ We initialize the current-iteration
    estimates to zero as well.⑤ And then loop through all states to estimate the state-value
    function.⑥ See here how we use the policy pi to get the possible transitions.⑦
    Each transition tuple has a probability, next state, reward, and a done flag indicating
    whether the ‘next_state’ is terminal or not.⑧ We calculate the value of that state
    by summing up the weighted value of that transition.⑨ Notice how we use the ‘done’
    flag to ensure the value of the next state when landing on a terminal state is
    zero. We don’t want infinite sums.⑩ At the end of each iteration (a state sweep),
    we make sure that the state-value functions are changing; otherwise, we call it
    converged.⑪ Finally, “copy” to get ready for the next iteration or return the
    latest state-value function. |
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ① 这是对策略评估算法的完整实现。我们需要的只是我们想要评估的策略以及策略运行的MDP。折现因子gamma默认为1，theta是一个小数，我们用它来检查收敛性。②
    在这里，我们将状态值函数的第一迭代估计初始化为零。③ 我们首先通过“永远”循环……④ 我们也将当前迭代的估计初始化为零。⑤ 然后遍历所有状态来估计状态值函数。⑥
    看这里我们如何使用策略π来获取可能的转换。⑦ 每个转换元组都有一个概率、下一个状态、奖励和一个表示“下一个状态”是否为终端状态的done标志。⑧ 我们通过计算该转换的加权值来计算该状态的价值。⑨
    注意我们如何使用‘done’标志来确保当落在终端状态时下一个状态的价值为零。我们不希望有无限的和。⑩ 在每次迭代（状态扫描）结束时，我们确保状态值函数正在变化；否则，我们称之为收敛。⑪
    最后，“复制”以准备下一次迭代或返回最新的状态值函数。
- en: Let’s now run policy evaluation in the randomly generated policy presented earlier
    for the FL environment.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将在之前为FL环境生成的随机策略中运行策略评估。
- en: '![](../Images/03_13.png)'
  id: totrans-124
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_13.png)'
- en: Recall the randomly generated policy
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 回忆一下随机生成的策略
- en: The following shows the progress policy evaluation makes on accurately estimating
    the state-value function of the randomly generated policy after only eight iterations.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 以下显示了策略评估在仅经过八次迭代后对随机生成策略的状态值函数的准确估计所取得的进展。
- en: '![](../Images/03_14.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_14.png)'
- en: Policy evaluation on the randomly generated policy for the FL environment
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: FL环境随机生成策略的策略评估
- en: '![](../Images/03_15.png)'
  id: totrans-129
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_15.png)'
- en: State-value function of the randomly generated policy
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 随机生成策略的状态值函数
- en: This final state-value function is the state-value function for this policy.
    Note that even though this is still an estimate, because we’re in a discrete state
    and action spaces, we can assume this to be the actual value function when using
    gamma of 0.99.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这个最终的状态值函数是此策略的状态值函数。请注意，尽管这仍然是一个估计值，因为我们处于离散的状态和动作空间，我们可以假设当使用gamma为0.99时，这实际上是实际的价值函数。
- en: In case you’re wondering about the state-value functions of the two policies
    presented earlier, here are the results.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想知道之前展示的两个策略的状态值函数，这里有一些结果。
- en: '![](../Images/03_16.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_16.png)'
- en: Results of policy evolution
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 策略演化的结果
- en: 'It seems being a Go-get-it policy doesn’t pay well in the FL environment! Fascinating
    results, right? But a question arises: Are there any better policies for this
    environment?'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL环境中，似乎成为一个“积极进取”的策略并不划算！有趣的结果，对吧？但是，一个问题出现了：这个环境中有没有更好的策略？
- en: 'Policy improvement: Using ratings to get better'
  id: totrans-136
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 政策改进：利用评级获得更好的结果
- en: 'The motivation is clear now. You have a way of evaluating any policy. This
    already gives you some freedom: you can evaluate many policies and rank them by
    the state-value function of the START state. After all, that number tells you
    the expected cumulative reward the policy in question will obtain if you run many
    episodes. Cool, right?'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在动机很明确了。你有一种评估任何策略的方法。这已经给了你一些自由：你可以评估许多策略，并按起始状态的状态值函数对它们进行排名。毕竟，这个数字告诉你，如果你运行许多剧集，所讨论的策略将获得的预期累积奖励。酷，对吧？
- en: No! Makes no sense. Why would you randomly generate a bunch of policies and
    evaluate them all? First, that’s a total waste of computing resources, but more
    importantly, it gives you no guarantee that you’re finding better and better policies.
    There has to be a better way.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 不！毫无意义。你为什么会随机生成一堆策略并评估它们？首先，这完全是浪费计算资源，更重要的是，这并不能保证你找到更好的策略。必须有一种更好的方法。
- en: 'The key to unlocking this problem is the action-value function, the Q-function.
    Using the V-function and the MDP, you get an estimate of the Q-function. The Q-function
    will give you a glimpse of the values of all actions for all states, and these
    values, in turn, can hint at how to improve policies. Take a look at the Q-function
    of the Careful policy and ways we can improve this policy:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的关键是动作值函数，即Q函数。使用V函数和MDP，你可以得到Q函数的估计。Q函数将为你展示所有状态下所有动作的值，而这些值反过来又可能暗示如何改进策略。看看Careful策略的Q函数以及我们可以如何改进这个策略：
- en: '![](../Images/03_17.png)'
  id: totrans-140
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_17.png)'
- en: How can the Q-function help us improve policies?
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: Q函数如何帮助我们改进策略？
- en: 'Notice how if we act greedily with respect to the Q-function of the policy,
    we obtain a new policy: Careful+. Is this policy any better? Well, policy evaluation
    can tell us! Let’s find out!'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，如果我们对策略的Q函数采取贪婪行动，我们就会得到一个新的策略：Careful+。这个策略是否更好？嗯，策略评估可以告诉我们！让我们来看看！
- en: '![](../Images/03_18.png)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_18.png)'
- en: State-value function of the Careful policy
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Careful策略的状态值函数
- en: 'The new policy is better than the original policy. This is great! We used the
    state-value function of the original policy and the MDP to calculate its action-value
    function. Then, acting greedily with respect to the action-value function gave
    us an improved policy. This is what the *policy-improvement* algorithm does: it
    calculates an action-value function using the state-value function and the MDP,
    and it returns a *greedy* policy with respect to the action-value function of
    the original policy. Let that sink in, it’s pretty important.'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 新策略比原始策略更好。这太棒了！我们使用了原始策略的状态值函数和MDP来计算其动作值函数。然后，根据动作值函数采取贪婪行动给了我们一个改进的策略。这就是*策略改进*算法所做的事情：它使用状态值函数和MDP计算动作值函数，并返回一个关于原始策略动作值函数的*贪婪*策略。让它沉淀下来，这很重要。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe policy-improvement equation
    |'
  id: totrans-146
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式策略改进方程'
- en: '|  | ![](../Images/03_18__Sidebar09.png) |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/03_18__Sidebar09.png) |'
- en: This is how the policy-improvement algorithm looks in Python.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是策略改进算法在Python中的样子。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe policy-improvement algorithm
    |'
  id: totrans-149
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python策略改进算法'
- en: '|  |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Very simple algorithm. It takes the state-value function of the policy you
    want to improve, V, and the MDP, P (and gamma, optionally).② Then, initialize
    the Q-function to zero (technically, you can initialize these randomly, but let’s
    keep things simple).③ Then loop through the states, actions, and transitions.④
    Flag indicating whether next_state is terminal or not⑤ We use those values to
    calculate the Q-function.⑥ Finally, we obtain a new, greedy policy by taking the
    argmax of the Q-function of the original policy. And there, you have a likely
    improved policy. |
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ① 非常简单的算法。它需要你想要改进的策略的状态值函数V，MDP，P（以及可选的gamma）。② 然后，将Q函数初始化为零（技术上，你可以随机初始化这些值，但让我们保持简单）。③
    然后，遍历状态、动作和转换。④ 标记表示下一个状态是否为终端状态。⑤ 我们使用这些值来计算Q函数。⑥ 最后，通过取原始策略Q函数的最大值，我们获得一个新的贪婪策略。这样，你就可能得到了一个改进的策略。|
- en: 'The natural next questions are these: Is there a better policy than this one?
    Can we do any better than *again*? Maybe! But, there’s only one way to find out.
    Let’s give it a try!'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 自然接下来要问的问题是这些：有没有比这个更好的策略？我们能做得比*再次*更好吗？也许可以！但，只有一种方法可以找到答案。让我们试试看！
- en: '![](../Images/03_19.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_19.png)'
- en: Can we improve over the Careful+ policy?
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否改进Careful+策略？
- en: I ran policy evaluation on the Careful+ policy, and then policy improvement.
    The Q-functions of Careful and Careful+ are different, but the greedy policies
    over the Q-functions are identical. In other words, there’s no improvement this
    time.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 我对Careful+策略进行了策略评估，然后进行了策略改进。Careful和Careful+的Q函数不同，但基于Q函数的贪婪策略是相同的。换句话说，这次没有改进。
- en: No improvement occurs because the Careful+ policy is an optimal policy of the
    FL environment (with gamma 0.99). We only needed one improvement over the Careful
    policy because this policy was good to begin with.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 没有任何改进发生，因为Careful+策略是FL环境（gamma为0.99）的一个最优策略。我们只需要对Careful策略进行一次改进，因为该策略一开始就很好。
- en: Now, even if we start with an adversarial policy designed to perform poorly,
    alternating over policy evaluation and improvement would still end up with an
    optimal policy. Want proof? Let’s do it! Let’s make up an adversarial policy for
    FL environment and see what happens.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，即使我们从一个旨在表现不佳的对抗性策略开始，交替进行策略评估和改进，最终也会得到一个最优策略。需要证明吗？让我们试试！让我们为FL环境想出一个对抗性策略，看看会发生什么。
- en: '![](../Images/03_20.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_20.png)'
- en: Adversarial policy for the FL environment
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: FL环境的对抗性策略
- en: 'Policy iteration: Improving upon improved behaviors'
  id: totrans-161
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 政策迭代：改进改进行为
- en: The plan with this adversarial policy is to alternate between policy evaluation
    and policy improvement until the policy coming out of the policy-improvement phase
    no longer yields a different policy. The fact is that, if instead of starting
    with an adversarial policy, we start with a randomly generated policy, this is
    what an algorithm called *policy iteration* does.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个对抗性策略的计划是在策略评估和策略改进之间交替，直到策略改进阶段输出的策略不再产生不同的策略。事实上，如果我们不是从一个对抗性策略开始，而是从一个随机生成的策略开始，这就是一个称为*策略迭代*的算法所做的事情。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe policy-iteration algorithm
    |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python 政策迭代算法 |'
- en: '|  |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Policy iteration is simple, and it only needs the MDP (including gamma).②
    The first step is to create a randomly generated policy. Anything here should
    do. I create a list of random actions and then map them to states.③ Here I’m keeping
    a copy of the policy before we modify it.④ Get the state-value function of the
    policy.⑤ Get an improved policy.⑥ Then, check if the new policy is any different.⑦
    If it’s different, we do it all over again.⑧ If it’s not, we break out of the
    loop and return an optimal policy and the optimal state-value function. |
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ① 策略迭代很简单，只需要MDP（包括gamma）。② 第一步是创建一个随机生成的策略。这里任何东西都可以。我创建了一个随机动作列表，并将其映射到状态。③
    在我们修改策略之前，我保留了一个策略的副本。④ 获取策略的状态值函数。⑤ 获取一个改进的策略。⑥ 然后，检查新策略是否有所不同。⑦ 如果它不同，我们再次做所有这些。⑧
    如果它没有变化，我们就跳出循环，返回一个最优策略和最优状态值函数。 |
- en: Great! But, let’s first try it starting with the adversarial policy and see
    what happens.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！但是，让我们先尝试从对抗性策略开始，看看会发生什么。
- en: '![](../Images/03_21.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_21.png)'
- en: Improving upon the adversarial policy 1/2
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 改进对抗性策略 1/2
- en: '![](../Images/03_22.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_22.png)'
- en: Improving upon the adversarial policy 2/2
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 改进对抗性策略 2/2
- en: As mentioned, alternating policy evaluating and policy improvement yields an
    optimal policy and state-value function regardless of the policy you start with.
    Now, a few points I’d like to make about this sentence.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，交替进行策略评估和策略改进，无论你从什么策略开始，都会得到一个最优策略和状态值函数。现在，我想就这句话提出几点。
- en: Notice how I use “*an* optimal policy,” but also use “*the* optimal state-value
    function.” This is not a coincidence or a poor choice of words; this is, in fact,
    a property that I’d like to highlight again. An MDP can have more than one optimal
    policy, but it can only have a single optimal state-value function. It’s not too
    hard to wrap your head around that.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我如何使用“*一个*最优策略”，同时也使用“*一个*最优状态值函数”。这不是巧合或用词不当；事实上，这是我想要再次强调的一个属性。一个MDP可以有多于一个的最优策略，但它只能有一个最优状态值函数。这并不难理解。
- en: 'State-value functions are collections of numbers. Numbers can have infinitesimal
    accuracy, because they’re numbers. There will be only one optimal state-value
    function (the collection with the highest numbers for all states). However, a
    state-value function may have actions that are equally valued for a given state;
    this includes the optimal state-value function. In this case, there could be multiple
    optimal policies, each optimal policy selecting a different, but equally valued,
    action. Take a look: the FL environment is a great example of this.'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 状态值函数是一组数字。数字可以有无限小的精度，因为它们是数字。将只有一个最优状态值函数（所有状态中数值最高的集合）。然而，状态值函数可能包含对于给定状态具有同等价值的动作；这包括最优状态值函数。在这种情况下，可能会有多个最优策略，每个最优策略选择不同的、但具有同等价值的动作。看看：FL环境是这种情况的一个很好的例子。
- en: '![](../Images/03_23.png)'
  id: totrans-175
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_23.png)'
- en: The FL environment has multiple optimal policies
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: FL环境具有多个最优策略
- en: By the way, it’s not shown here, but all the actions in a terminal state have
    the same value, zero, and therefore a similar issue that I’m highlighting in state
    6.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 顺便说一下，这里没有显示，但终端状态中的所有动作都具有相同的值，即零，因此这与我在状态6中强调的问题类似。
- en: 'As a final note, I want to highlight that policy iteration is guaranteed to
    converge to the exact optimal policy: the mathematical proof shows it will not
    get stuck in local optima. However, as a practical consideration, there’s one
    thing to be careful about. If the action-value function has a tie (for example,
    right/left in state 6), we must make sure not to break ties randomly. Otherwise,
    policy improvement could keep returning different policies, even without any real
    improvement. With that out of the way, let’s look at another essential algorithm
    for finding optimal state-value functions and optimal policies.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最后的注意事项，我想强调政策迭代保证收敛到精确的最优策略：数学证明显示它不会陷入局部最优。然而，作为一个实际考虑，有一件事需要注意。如果动作值函数存在平局（例如，状态6中的右/左），我们必须确保不要随机打破平局。否则，策略改进可能会不断返回不同的策略，即使没有任何真正的改进。解决了这个问题之后，让我们来看看另一个寻找最优状态值函数和最优策略的基本算法。
- en: 'Value iteration: Improving behaviors early'
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 值迭代：早期改进行为
- en: 'You probably notice the way policy evaluation works: values propagate consistently
    on each iteration, but slowly. Take a look.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了策略评估的工作方式：值在每个迭代中一致地传播，但速度很慢。看看。
- en: '![](../Images/03_24.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_24.png)'
- en: Policy evaluation on the always-left policy on the SWF environment
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: SWF环境中的始终左策略的政策评估
- en: The image shows a single state-space sweep of policy evaluation followed by
    an estimation of the Q-function. We do this by using the truncated estimate of
    the V-function and the MDP, on each iteration. By doing so, we can more easily
    see that even after the first iteration, a greedy policy over the early Q-function
    estimates would be an improvement. Look at the Q-values for state 5 in the first
    iteration; changing the action to point towards the GOAL state is obviously already
    better.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图像显示了政策评估的单次状态空间扫描，随后是对Q函数的估计。我们通过在每个迭代中使用V函数和MDP的截断估计来完成这项工作。通过这样做，我们可以更容易地看到，即使在第一次迭代之后，对早期Q函数估计的贪婪策略也是一种改进。看看第一次迭代中状态5的Q值；将动作改为指向GOAL状态显然已经更好。
- en: In other words, even if we *value iteration* (VI).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，即使我们进行*值迭代*（VI）。
- en: VI can be thought of “greedily greedifying policies,” because we calculate the
    greedy policy as soon as we can, greedily. VI doesn’t wait until we have an accurate
    estimate of the policy before it improves it, but instead, VI truncates the policy-evaluation
    phase after a single state sweep. Take a look at what I mean by “greedily greedifying
    policies.”
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: VI可以被看作是“贪婪地贪婪化策略”，因为我们尽可能贪婪地计算贪婪策略。VI不会等到我们有策略的准确估计后再进行改进，而是VI在单次状态扫描后截断策略评估阶段。看看我所说的“贪婪地贪婪化策略”是什么意思。
- en: '![](../Images/03_25.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/03_25.png)'
- en: Greedily greedifying the always-left policy of the SFW environment
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 贪婪地改进SFW环境的始终左策略
- en: If we start with a randomly generated policy, instead of this adversarial policy
    always-left for the SWF environment, VI would still converge to the optimal state-value
    function. VI is a straightforward algorithm that can be expressed in a single
    equation.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们从一个随机生成的策略开始，而不是SWF环境中的这个对抗性的始终左策略，VI仍然会收敛到最优状态值函数。VI是一个简单的算法，可以用一个方程表示。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe value-iteration equation
    |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 显示数学公式值迭代方程'
- en: '|  | ![](../Images/03_25__Sidebar12.png) |'
  id: totrans-190
  prefs: []
  type: TYPE_TB
  zh: '| ![侧边栏图片](../Images/03_25__Sidebar12.png) |'
- en: Notice that in practice, in VI, we don’t have to deal with policies at all.
    VI doesn’t have any separate evaluation phase that runs to convergence. While
    the goal of VI is the same as the goal of PI—to find the optimal policy for a
    given MDP—VI happens to do this through the value functions; thus the name value
    iteration.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在实践中，在VI中，我们根本不需要处理策略。VI没有单独的评价阶段，该阶段运行到收敛。虽然VI的目标与PI的目标相同——为给定的MDP找到最优策略——VI恰好通过值函数来完成这项工作；因此得名值迭代。
- en: Again, we only have to keep track of a V-function and a Q-function (depending
    on implementation). Remember that to get the greedy policy over a Q-function,
    we take the arguments of the maxima (argmax) over the actions of that Q-function.
    Instead of improving the policy by taking the argmax to get a better policy and
    then evaluating this improved policy to obtain a value function again, we directly
    calculate the maximum (max, instead of argmax) value across the actions to be
    used for the next sweep over the states.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，我们只需要跟踪一个V函数和一个Q函数（具体取决于实现方式）。记住，为了在Q函数上获得贪婪策略，我们取该Q函数动作的最大值（argmax）。而不是通过取argmax来改善策略以获得更好的策略，然后评估这个改进的策略以再次获得价值函数，我们直接计算用于下一次状态遍历的动作的最大（max，而不是argmax）值。
- en: Only at the end of the VI algorithm, after the Q-function converges to the optimal
    values, do we extract the optimal policy by taking the argmax over the actions
    of the Q-function, as before. You’ll see it more clearly in the code snippet on
    the next page.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 只有在VI算法的末尾，当Q函数收敛到最优值后，我们才通过在Q函数的动作上取argmax来提取最优策略，就像之前一样。你将在下一页的代码片段中更清楚地看到这一点。
- en: One important thing to highlight is that whereas VI and PI are two different
    algorithms, in a more general view, they are two instances of *generalized policy
    iteration* (GPI). GPI is a general idea in RL in which policies are improved using
    their value function estimates, and value function estimates are improved toward
    the actual value function for the current policy. Whether you wait for the perfect
    estimates or not is just a detail.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 需要强调的一个重要事项是，虽然VI和PI是两种不同的算法，但从更广泛的角度来看，它们是*广义策略迭代*（GPI）的两个实例。GPI是强化学习中的一个通用概念，其中策略通过其价值函数估计来改进，而价值函数估计则朝着当前策略的实际价值函数改进。你是否等待完美的估计只是一个细节。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe value-iteration algorithm
    |'
  id: totrans-195
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python价值迭代算法'
- en: '|  |'
  id: totrans-196
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Like policy iteration, value iteration is a method for obtaining optimal policies.
    For this, we need an MDP (including gamma). Theta is the convergence criteria.
    1e-10 is sufficiently accurate.② First thing is to initialize a state-value function.
    Know that a V-function with random numbers should work fine.③ We get in this loop
    and initialize a Q-function to zero.④ Notice this one over here has to be zero.
    Otherwise, the estimate would be incorrect.⑤ Then, for every transition of every
    action in every state, we ...⑥ ... calculate the action-value function ...⑦ ...
    notice, using V, which is the old “truncated” estimate.⑧ After each sweep over
    the state space, we make sure the state-value function keeps changing. Otherwise,
    we found the optimal V-function and should break out.⑨ Thanks to this short line,
    we don’t need a separate policy-improvement phase. It’s not a direct replacement,
    but instead a combination of improvement and evaluation.⑩ Only at the end do we
    extract the optimal policy and return it along with the optimal state-value function.
    |
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ① 与策略迭代一样，价值迭代是获得最优策略的方法。为此，我们需要一个MDP（包括gamma）。Theta是收敛标准。1e-10足够准确。② 首先是要初始化一个状态价值函数。要知道，带有随机数的V函数应该可以正常工作。③
    我们进入这个循环，并将Q函数初始化为零。④ 注意这里必须为零。否则，估计将是不正确的。⑤ 然后，对于每个状态中每个动作的每个转换，我们 ...⑥ ... 计算动作价值函数
    ...⑦ ... 注意，使用V，这是旧的“截断”估计。⑧ 在每次遍历状态空间后，我们确保状态价值函数持续变化。否则，我们找到了最优的V函数，应该跳出循环。⑨
    多亏了这一行，我们不需要单独的策略改进阶段。它不是直接替换，而是一种改进和评估的组合。⑩ 只有在最后，我们提取最优策略并将其与最优状态价值函数一起返回。 |
- en: Summary
  id: totrans-199
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The objective of a reinforcement learning agent is to maximize the expected
    return, which is the total reward over multiple episodes. For this, agents must
    use policies, which can be thought of as universal plans. Policies prescribe actions
    for states. They can be deterministic, meaning they return single actions, or
    stochastic, meaning they return probability distributions. To obtain policies,
    agents usually keep track of several summary values. The main ones are state-value,
    action-value, and action-advantage functions.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习代理的目标是最大化期望回报，即多个回合的总奖励。为此，代理必须使用策略，这可以被视为通用计划。策略为状态指定动作。它们可以是确定性的，意味着它们返回单个动作，或者随机性的，意味着它们返回概率分布。为了获得策略，代理通常跟踪几个汇总值。主要的是状态价值、动作价值和动作优势函数。
- en: State-value functions summarize the expected return from a state. They indicate
    how much reward the agent will obtain from a state until the end of an episode
    in expectation. Action-value functions summarize the expected return from a state-action
    pair. This type of value function tells the expected reward-to-go *after* an agent
    selects a specific action in a given state. Action-value functions allow the agent
    to compare across actions and therefore solve the control problem. Action-advantage
    functions show the agent how much better than the default it can do if it were
    to opt for a specific state-action pair. All of these value functions are mapped
    to specific policies, perhaps an optimal policy. They depend on following what
    the policies prescribe until the end of the episode.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 状态价值函数总结了从状态中期望获得的回报。它们表明代理从状态到一期的结束预期将获得多少奖励。动作价值函数总结了从状态-动作对的期望回报。这种类型的价值函数告诉代理在给定状态下选择特定动作后预期的后续奖励。动作价值函数允许代理比较不同的动作，从而解决控制问题。动作优势函数显示了代理如果选择特定的状态-动作对，可以比默认情况做得更好。所有这些价值函数都映射到特定的策略，可能是最优策略。它们取决于遵循策略直到一期的结束。
- en: Policy evaluation is a method for estimating a value function from a policy
    and an MDP. Policy improvement is a method for extracting a greedy policy from
    a value function and an MDP. Policy iteration consists of alternating between
    policy-evaluation and policy improvement to obtain an optimal policy from an MDP.
    The policy evaluation phase may run for several iterations before it accurately
    estimates the value function for the given policy. In policy iteration, we wait
    until policy evaluation finds this accurate estimate. An alternative method, called
    value iteration, truncates the policy-evaluation phase and exits it, entering
    the policy-improvement phase early.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 策略评估是从策略和MDP中估计价值函数的方法。策略改进是从价值函数和MDP中提取贪婪策略的方法。策略迭代包括在策略评估和策略改进之间交替，以从MDP中获得最优策略。策略评估阶段可能需要运行多次迭代，才能准确估计给定策略的价值函数。在策略迭代中，我们等待策略评估找到这个准确的估计。另一种称为价值迭代的方法，截断策略评估阶段并提前退出，进入策略改进阶段。
- en: 'The more general view of these methods is generalized policy iteration, which
    describes the interaction of two processes to optimize policies: one moves value
    function estimates closer to the real value function of the current policy, another
    improves the current policy using its value function estimates, getting progressively
    better and better policies as this cycle continues.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 这些方法的更一般观点是广义策略迭代，它描述了两个过程之间的相互作用以优化策略：一个过程将价值函数估计值逐渐接近当前策略的真实价值函数，另一个过程使用其价值函数估计值来改进当前策略，随着这个循环的持续，策略会逐渐变得越来越好。
- en: By now, you
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Know the objective of a reinforcement learning agent and the different statistics
    it may hold at any given time
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解强化学习代理的目标以及它在任何给定时间可能持有的不同统计数据
- en: Understand methods for estimating value functions from policies and methods
    for improving policies from value functions
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解从策略中估计价值函数的方法，以及从价值函数中改进策略的方法
- en: Can find optimal policies in sequential decision-making problems modeled by
    MDPs
  id: totrans-207
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以在由MDP建模的顺序决策问题中找到最优策略
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 在自己的工作中努力并分享你的发现'
- en: '|  | At the end of every chapter, I’ll give several ideas on how to take what
    you’ve learned to the next level. If you’d like, share your results with the rest
    of the world, and make sure to check out what others have done, too. It’s a win-win
    situation, and hopefully, you’ll take advantage of it.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 在每一章的结尾，我会给出几个想法，告诉你如何将所学内容提升到下一个层次。如果你愿意，可以与全世界分享你的成果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch01_tf01:** Supervised, unsupervised, and reinforcement learning are
    essential machine learning branches. And while it’s crucial to know the differences,
    it’s equally important to know the similarities. Write a post analyzing how these
    different approaches compare and how they could be used together to solve an AI
    problem. All branches are going after the same goal: to create artificial general
    intelligence, it’s vital for all of us to better understand how to use the tools
    available.'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf01:** 监督学习、无监督学习和强化学习是机器学习的三个基本分支。虽然了解它们之间的区别至关重要，但同样重要的是了解它们之间的相似之处。写一篇帖子分析这些不同的方法如何比较，以及它们如何可以一起用来解决人工智能问题。所有分支都追求同一个目标：创造通用人工智能，对我们所有人来说，更好地理解如何使用可用的工具是至关重要的。'
- en: '**#gdrl_ch01_tf02:** I wouldn’t be surprised if you don’t have a machine learning
    or computer science background, yet are still interested in what this book has
    to offer. One essential contribution is to post resources from other fields that
    study decision-making. Do you have an operations research background? Psychology,
    philosophy, or neuroscience background? Control theory? Economics? How about you
    create a list of resources, blog posts, YouTube videos, books, or any other medium
    and share it with the rest of us also studying decision-making?'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf02:** 如果你没有机器学习或计算机科学背景，但对这本书提供的内容感兴趣，这并不会让我感到惊讶。一个重要的贡献是发布来自其他研究决策领域的资源。你有没有运筹学背景？心理学、哲学或神经科学背景？控制理论？经济学？你为什么不创建一个资源列表、博客文章、YouTube
    视频书籍或其他任何形式的列表，并与我们这些也在研究决策的其他人分享呢？'
- en: '**#gdrl_ch01_tf03:** Part of the text in this chapter has information that
    could be better explained through graphics, tables, and other forms. For instance,
    I talked about the different types of reinforcement learning agents (value-based,
    policy-based, actor-critic, model-based, gradient-free). Why don’t you grab text
    that’s dense, distill the knowledge, and share your summary with the world?'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf03:** 本章的部分文本包含一些信息，这些信息通过图形、表格和其他形式可能解释得更好。例如，我谈到了不同类型的强化学习代理（基于价值的、基于策略的、演员-评论家、基于模型的、无梯度）。你为什么不抓取那些密集的文字，提炼知识，并将你的总结与世界分享呢？'
- en: '**#gdrl_ch01_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch01_tf04:** 在每一章中，我都在使用最后一个标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他你工作过的事情。没有什么比为自己创造作业更令人兴奋的了。确保分享你设定去调查的内容以及你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from this list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here is a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@mimoralea（我会转发），并使用这个列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的工作。|
