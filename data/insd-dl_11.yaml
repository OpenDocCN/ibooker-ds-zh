- en: 9 Generative adversarial networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: æœ¬ç« æ¶µç›–
- en: Working with generative models for fully connected and convolutional networks
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç”Ÿæˆæ¨¡å‹å¤„ç†å…¨è¿æ¥å’Œå·ç§¯ç½‘ç»œ
- en: Encoding concepts using latent vectors
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ½œåœ¨å‘é‡ç¼–ç æ¦‚å¿µ
- en: Training two networks that cooperate
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: è®­ç»ƒä¸¤ä¸ªç›¸äº’åä½œçš„ç½‘ç»œ
- en: Manipulating generation using a conditional model
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ä»¶æ¨¡å‹æ“çºµç”Ÿæˆ
- en: Manipulating generation with vector arithmetic
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å‘é‡ç®—æœ¯æ“çºµç”Ÿæˆ
- en: Most of what we have learned thus far has been a *one-to-one* mapping. Every
    input has one correct class/output. The dog can only be a â€œdogâ€; the sentence
    is only â€œpositiveâ€ or â€œnegative.â€ But we can also encounter *one-to-many* problems
    where there is more than one possible answer. For example, we may have the concept
    of â€œsevenâ€ as input and need to create several different kinds of pictures of
    the digit 7\. Or, to colorize an old black-and-white photograph, we could produce
    multiple possible color images that were all equally valid. For one-to-many problems,
    we can use a *generative adversarial network* (GAN). Like other unsupervised models
    such as the autoencoder, we can use the representation a GAN learns as the input
    to other AI/ML algorithms and tasks. But the representation a GAN learns is often
    more meaningful, allowing us to manipulate our data in new ways. For example,
    we could take a picture of a frowning person and have the algorithm alter the
    image so the person is smiling.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬æ‰€å­¦çš„çŸ¥è¯†å¤§å¤šæ˜¯ä¸€ç§**ä¸€å¯¹ä¸€**çš„æ˜ å°„ã€‚æ¯ä¸ªè¾“å…¥éƒ½æœ‰ä¸€ä¸ªæ­£ç¡®çš„ç±»åˆ«/è¾“å‡ºã€‚ç‹—åªèƒ½æ˜¯â€œç‹—â€ï¼›å¥å­åªèƒ½æ˜¯â€œæ­£é¢â€æˆ–â€œè´Ÿé¢â€ã€‚ä½†æˆ‘ä»¬ä¹Ÿå¯èƒ½é‡åˆ°**å¤šå¯¹ä¸€**çš„é—®é¢˜ï¼Œå…¶ä¸­å­˜åœ¨å¤šä¸ªå¯èƒ½çš„ç­”æ¡ˆã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½æœ‰â€œä¸ƒâ€è¿™ä¸ªæ¦‚å¿µä½œä¸ºè¾“å…¥ï¼Œéœ€è¦åˆ›å»ºå‡ ç§ä¸åŒç±»å‹çš„æ•°å­—7çš„å›¾ç‰‡ã€‚æˆ–è€…ï¼Œä¸ºäº†ç»™è€å¼çš„é»‘ç™½ç…§ç‰‡ä¸Šè‰²ï¼Œæˆ‘ä»¬å¯ä»¥ç”Ÿæˆå¤šä¸ªå¯èƒ½çš„æœ‰æ•ˆå½©è‰²å›¾åƒã€‚å¯¹äºå¤šå¯¹ä¸€é—®é¢˜ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨**ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ**ï¼ˆGANï¼‰ã€‚åƒè‡ªç¼–ç å™¨ç­‰å…¶ä»–æ— ç›‘ç£æ¨¡å‹ä¸€æ ·ï¼Œæˆ‘ä»¬å¯ä»¥å°†GANå­¦ä¹ åˆ°çš„è¡¨ç¤ºä½œä¸ºå…¶ä»–AI/MLç®—æ³•å’Œä»»åŠ¡çš„è¾“å…¥ã€‚ä½†GANå­¦ä¹ åˆ°çš„è¡¨ç¤ºé€šå¸¸æ›´æœ‰æ„ä¹‰ï¼Œå…è®¸æˆ‘ä»¬ä»¥æ–°çš„æ–¹å¼æ“çºµæˆ‘ä»¬çš„æ•°æ®ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥æ‹æ‘„ä¸€ä¸ªçš±çœ‰çš„äººçš„ç…§ç‰‡ï¼Œå¹¶è®©ç®—æ³•æ”¹å˜å›¾åƒï¼Œä½¿è¿™ä¸ªäººå¾®ç¬‘ã€‚
- en: 'GANs are currently one of the most successful types of *generative models*:
    models that can create new data that looks like the training data without just
    making naive copies. If you have enough data and GPU compute, you can create some
    *very* realistic images with GANs. Figure 9.1 shows an example of what the latest
    and greatest GANs can achieve. As a GAN gets better at generating content, it
    often becomes better at manipulating content as well. We could, for example, use
    a GAN to change the personâ€™s hair color in figure 9.1 or give them a frown. The
    website [https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)
    shows more examples of what a high-end GAN can do.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: GANæ˜¯ç›®å‰æœ€æˆåŠŸçš„ç”Ÿæˆæ¨¡å‹ç±»å‹ä¹‹ä¸€ï¼šå¯ä»¥åˆ›å»ºçœ‹èµ·æ¥åƒè®­ç»ƒæ•°æ®çš„æ–°çš„æ•°æ®ï¼Œè€Œä¸ä»…ä»…æ˜¯åˆ¶ä½œç®€å•çš„å¤åˆ¶ã€‚å¦‚æœä½ æœ‰è¶³å¤Ÿçš„æ•°æ®å’ŒGPUè®¡ç®—èƒ½åŠ›ï¼Œä½ å¯ä»¥ä½¿ç”¨GANåˆ›å»ºä¸€äº›**éå¸¸**é€¼çœŸçš„å›¾åƒã€‚å›¾9.1å±•ç¤ºäº†æœ€æ–°çš„æœ€å…ˆè¿›çš„GANèƒ½å¤Ÿè¾¾åˆ°çš„æ•ˆæœã€‚éšç€GANåœ¨ç”Ÿæˆå†…å®¹æ–¹é¢å˜å¾—æ›´å¥½ï¼Œå®ƒé€šå¸¸ä¹Ÿå˜å¾—æ›´æ“…é•¿æ“çºµå†…å®¹ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨GANæ¥æ”¹å˜å›¾9.1ä¸­äººç‰©çš„å‘è‰²æˆ–è®©ä»–ä»¬çš±çœ‰ã€‚ç½‘ç«™[https://thispersondoesnotexist.com](https://thispersondoesnotexist.com)å±•ç¤ºäº†é«˜ç«¯GANèƒ½å¤Ÿåšåˆ°çš„æ›´å¤šç¤ºä¾‹ã€‚
- en: '![](../Images/CH09_F01_Raff.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F01_Raff.png)'
- en: Figure 9.1 This person does not actually exist! The image was generated using
    a GAN called StyleGAN (https://github.com/lucidrains/stylegan2-pytorch).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.1 æ­¤äººå®é™…ä¸Šå¹¶ä¸å­˜åœ¨ï¼è¯¥å›¾åƒæ˜¯ä½¿ç”¨åä¸ºStyleGANçš„GANï¼ˆhttps://github.com/lucidrains/stylegan2-pytorchï¼‰ç”Ÿæˆçš„ã€‚
- en: Similar to autoencoders, GANs learn in a self-supervised fashion, so we do not
    *need* labeled data to create a GAN. There are ways to make GANs use labels, though,
    so they can also be supervised models, depending on what you want to do. You could
    use a GAN for anything you might use an autoencoder for, but GANs tend to be the
    go-to tool for making realistic-looking synthetic data (e.g., visualizing a product
    for a user), manipulating images, and solving one-to-many problems that have multiple
    valid outputs. GANs have become so effective that hundreds of specialized variants
    exist for different kinds of problems like text generation, image manipulation,
    audio, data augmentation, and more. This chapter focuses on the fundamentals of
    how GANs work and fail, to give you a good foundation for understanding other
    specialized versions.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸è‡ªç¼–ç å™¨ç±»ä¼¼ï¼ŒGANsä»¥è‡ªç›‘ç£çš„æ–¹å¼å­¦ä¹ ï¼Œå› æ­¤æˆ‘ä»¬ä¸éœ€è¦æ ‡è®°æ•°æ®æ¥åˆ›å»ºGANã€‚å°½ç®¡å¦‚æ­¤ï¼Œä¹Ÿæœ‰æ–¹æ³•ä½¿GANsä½¿ç”¨æ ‡ç­¾ï¼Œè¿™æ ·å®ƒä»¬ä¹Ÿå¯ä»¥æ˜¯ç›‘ç£æ¨¡å‹ï¼Œå…·ä½“å–å†³äºä½ æƒ³è¦åšä»€ä¹ˆã€‚ä½ å¯ä»¥ä½¿ç”¨GANåšä»»ä½•ä½ å¯èƒ½ä½¿ç”¨è‡ªç¼–ç å™¨åšçš„äº‹æƒ…ï¼Œä½†GANsé€šå¸¸æ˜¯ç”¨ä½œåˆ¶ä½œçœ‹èµ·æ¥é€¼çœŸçš„åˆæˆæ•°æ®çš„é¦–é€‰å·¥å…·ï¼ˆä¾‹å¦‚ï¼Œä¸ºç”¨æˆ·å¯è§†åŒ–äº§å“ï¼‰ï¼Œæ“çºµå›¾åƒä»¥åŠè§£å†³å…·æœ‰å¤šä¸ªæœ‰æ•ˆè¾“å‡ºçš„å¤šå¯¹å¤šé—®é¢˜ã€‚GANså·²ç»å˜å¾—å¦‚æ­¤æœ‰æ•ˆï¼Œä»¥è‡³äºå­˜åœ¨æ•°ç™¾ç§é’ˆå¯¹ä¸åŒç±»å‹é—®é¢˜çš„ä¸“é—¨å˜ä½“ï¼Œå¦‚æ–‡æœ¬ç”Ÿæˆã€å›¾åƒå¤„ç†ã€éŸ³é¢‘ã€æ•°æ®å¢å¼ºç­‰ã€‚æœ¬ç« é‡ç‚¹ä»‹ç»GANså¦‚ä½•å·¥ä½œå’Œå¤±è´¥çš„åŸºæœ¬åŸç†ï¼Œä¸ºä½ ç†è§£å…¶ä»–ä¸“é—¨ç‰ˆæœ¬æä¾›ä¸€ä¸ªè‰¯å¥½çš„åŸºç¡€ã€‚
- en: First, we learn how GANs work through an adversarial game played between two
    competing neural networks, a *discriminator* and a *generator*. The first GAN
    we build will follow the original approach so we can show the most common issue
    with GANs, *mode collapse*, which can prevent them from learning. Then we learn
    about an improved approach to training GANs that helps reduce (but not solve)
    this problem. The mode-collapse issue canâ€™t be *solved*, but it can be reduced,
    and we show how using an improved Wasserstein GAN. The Wasserstein GAN makes it
    easier to learn a reasonable GAN, which we then use to show how you can make a
    *convolutional* GAN for image generation and a conditional GAN that includes label
    information. The last technical concept we discuss about GANs is manipulating
    the content a GAN produces by altering the latent space it produces. We end this
    chapter with a brief discussion of the many things that can be done with GANS
    and the ethical responsibilities involved in using such a powerful approach.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œæˆ‘ä»¬é€šè¿‡ä¸¤ä¸ªç«äº‰æ€§ç¥ç»ç½‘ç»œä¹‹é—´çš„å¯¹æŠ—æ¸¸æˆæ¥å­¦ä¹ GANsçš„å·¥ä½œåŸç†ï¼Œä¸€ä¸ªç§°ä¸º*åˆ¤åˆ«å™¨*ï¼Œå¦ä¸€ä¸ªç§°ä¸º*ç”Ÿæˆå™¨*ã€‚æˆ‘ä»¬æ„å»ºçš„ç¬¬ä¸€ä¸ªGANå°†éµå¾ªåŸå§‹æ–¹æ³•ï¼Œè¿™æ ·æˆ‘ä»¬å¯ä»¥å±•ç¤ºGANsæœ€å¸¸è§çš„å¸¸è§é—®é¢˜ï¼Œå³*æ¨¡å¼åå¡Œ*ï¼Œè¿™å¯èƒ½ä¼šé˜»æ­¢å®ƒä»¬å­¦ä¹ ã€‚ç„¶åï¼Œæˆ‘ä»¬å°†äº†è§£ä¸€ç§æ”¹è¿›çš„GANsè®­ç»ƒæ–¹æ³•ï¼Œè¿™æœ‰åŠ©äºå‡å°‘ï¼ˆä½†æ— æ³•è§£å†³ï¼‰è¿™ä¸ªé—®é¢˜ã€‚æ¨¡å¼åå¡Œé—®é¢˜æ— æ³•*è§£å†³*ï¼Œä½†å¯ä»¥é€šè¿‡ä½¿ç”¨æ”¹è¿›çš„Wasserstein
    GANæ¥å‡å°‘ã€‚Wasserstein GANä½¿å¾—å­¦ä¹ ä¸€ä¸ªåˆç†çš„GANå˜å¾—æ›´å®¹æ˜“ï¼Œç„¶åæˆ‘ä»¬ä½¿ç”¨å®ƒæ¥å±•ç¤ºå¦‚ä½•åˆ¶ä½œç”¨äºå›¾åƒç”Ÿæˆçš„*å·ç§¯*GANå’ŒåŒ…å«æ ‡ç­¾ä¿¡æ¯çš„æ¡ä»¶GANã€‚å…³äºGANsçš„æœ€åä¸€ä¸ªæŠ€æœ¯æ¦‚å¿µæ˜¯æˆ‘ä»¬é€šè¿‡æ”¹å˜å®ƒäº§ç”Ÿçš„æ½œåœ¨ç©ºé—´æ¥æ“çºµGANäº§ç”Ÿçš„å†…å®¹ã€‚æˆ‘ä»¬ä»¥å¯¹ä½¿ç”¨å¦‚æ­¤å¼ºå¤§çš„æ–¹æ³•æ‰€æ¶‰åŠçš„è®¸å¤šäº‹æƒ…å’Œé“å¾·è´£ä»»çš„ç®€è¦è®¨è®ºç»“æŸæœ¬ç« ã€‚
- en: 9.1 Understanding generative adversarial networks
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 ç†è§£ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
- en: 'When we talk about a GAN, there are usually two subnetworks: G, which is called
    the *generator* network; and D, the *discriminator* network. These networks have
    competing goals, making them adversaries. Figure 9.2 shows how they interact.
    The generator wants to create new data that looks realistic, and the discriminator
    wants to determine if an input came from the *real* dataset or is a *fake* provided
    by the generator.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: å½“æˆ‘ä»¬è°ˆè®ºGANæ—¶ï¼Œé€šå¸¸æœ‰ä¸¤ä¸ªå­ç½‘ç»œï¼šGï¼Œè¢«ç§°ä¸º*ç”Ÿæˆå™¨*ç½‘ç»œï¼›å’ŒDï¼Œç§°ä¸º*åˆ¤åˆ«å™¨*ç½‘ç»œã€‚è¿™äº›ç½‘ç»œå…·æœ‰ç«äº‰ç›®æ ‡ï¼Œä½¿å®ƒä»¬æˆä¸ºå¯¹æ‰‹ã€‚å›¾9.2æ˜¾ç¤ºäº†å®ƒä»¬å¦‚ä½•ç›¸äº’ä½œç”¨ã€‚ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯åˆ›å»ºçœ‹èµ·æ¥é€¼çœŸçš„æ–°æ•°æ®å¹¶å°†å…¶æä¾›ç»™åˆ¤åˆ«å™¨ã€‚åˆ¤åˆ«å™¨çš„ç›®æ ‡æ˜¯ç¡®å®šä¸€ä¸ªè¾“å…¥æ˜¯å¦æ¥è‡ª*çœŸå®*æ•°æ®é›†æˆ–æ˜¯ç”±ç”Ÿæˆå™¨æä¾›çš„*ä¼ªé€ *æ•°æ®ã€‚
- en: '![](../Images/CH09_F02_Raff.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F02_Raff.png)'
- en: Figure 9.2 A high-level example of how the generator and discriminator interact.
    The generator has a goal of generating fake data and giving it to the discriminator.
    The discriminator has the goal of determining whether an image came from the GAN.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.2 ç”Ÿæˆå™¨å’Œåˆ¤åˆ«å™¨å¦‚ä½•ç›¸äº’ä½œç”¨çš„ç¤ºä¾‹ã€‚ç”Ÿæˆå™¨çš„ç›®æ ‡æ˜¯ç”Ÿæˆä¼ªé€ æ•°æ®å¹¶å°†å…¶æä¾›ç»™åˆ¤åˆ«å™¨ã€‚åˆ¤åˆ«å™¨çš„ç›®æ ‡æ˜¯ç¡®å®šå›¾åƒæ˜¯å¦æ¥è‡ªGANã€‚
- en: GANs can get pretty complex, so we will start at a high level and expand the
    details one level at a time. Our next level of this detail is shown in figure
    9.3, and we will walk through it slowly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: GANså¯èƒ½ç›¸å½“å¤æ‚ï¼Œå› æ­¤æˆ‘ä»¬å°†ä»é«˜å±‚æ¬¡å¼€å§‹ï¼Œé€æ­¥å±•å¼€ç»†èŠ‚ã€‚å›¾9.3æ˜¾ç¤ºäº†è¿™ä¸€ç»†èŠ‚çš„ä¸‹ä¸€çº§ï¼Œæˆ‘ä»¬å°†ç¼“æ…¢åœ°ä»‹ç»å®ƒã€‚
- en: '![](../Images/CH09_F03_Raff.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F03_Raff.png)'
- en: Figure 9.3 Diagram of how GANs are generally trained. The generator G learns
    to assign meaning to random values z to produce meaningful outputs ![](../Images/tilde_x.png).
    The discriminator receives real and fake data but doesnâ€™t know whichâ€”and tries
    to learn the difference in a standard classification problem. Then Gâ€™s loss is
    computed on Dâ€™s output but is the inverse. G gets an error when D does not, and
    G gets no error when D does.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.3 GANsé€šå¸¸çš„è®­ç»ƒè¿‡ç¨‹å›¾ã€‚ç”Ÿæˆå™¨Gå­¦ä¹ ä¸ºéšæœºå€¼zèµ‹äºˆæ„ä¹‰ä»¥äº§ç”Ÿæœ‰æ„ä¹‰çš„è¾“å‡º![~x](../Images/tilde_x.png)ã€‚åˆ¤åˆ«å™¨æ¥æ”¶çœŸå®å’Œè™šå‡æ•°æ®ï¼Œä½†ä¸çŸ¥é“å“ªä¸€ä¸ªæ˜¯çœŸå®çš„â€”â€”å¹¶è¯•å›¾åœ¨ä¸€ä¸ªæ ‡å‡†çš„åˆ†ç±»é—®é¢˜ä¸­å­¦ä¹ å·®å¼‚ã€‚ç„¶åGçš„æŸå¤±æ˜¯åœ¨Dçš„è¾“å‡ºä¸Šè®¡ç®—çš„ï¼Œä½†å®ƒæ˜¯ç›¸åçš„ã€‚å½“Dæ²¡æœ‰é”™è¯¯æ—¶ï¼ŒGä¼šæœ‰é”™è¯¯ï¼Œè€Œå½“Dæœ‰é”™è¯¯æ—¶ï¼ŒGæ²¡æœ‰é”™è¯¯ã€‚
- en: 'G works by taking in a *latent* vector **z** âˆˆ â„^m and predicting an output
    the same size and shape as the real data. We get to choose the number of dimensions
    m (itâ€™s a hyperparameter) for the latent vector z: it should be large enough to
    represent different concepts in our data (which takes some manual trial and error).
    For example, some latent properties we might want to learn are smile/frown, hair
    color, and hairstyle. The values of z are called *latent* because we never observe
    what they actually are: the model has to infer them from the data. If we do a
    good job, the generator G can use this latent representation z as a smaller and
    more compact representation of the dataâ€”kind of like a lossy form of compression.
    You can think of the generator as a sketch artist who has to take in a description
    (the latent vector z) and, from that, construct an accurate picture for the output!
    The same way a sketch artistâ€™s drawing is a product of how they *interpret* your
    description, the meaning of these latent vectors depends on how the generator
    G interprets them. In practice, we use a simple representation of each latent
    variable being sampled from a Gaussian distribution (i.e, *z*[i] âˆ¼ *N*(0,1)).
    This makes it easy to sample new values of z so that we can produce and control
    synthetic data. It is up to the generator to learn how to make something meaningful
    out of these values.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: Gé€šè¿‡æ¥æ”¶ä¸€ä¸ª*æ½œåœ¨*å‘é‡**z** âˆˆ â„^må¹¶é¢„æµ‹ä¸çœŸå®æ•°æ®å¤§å°å’Œå½¢çŠ¶ç›¸åŒçš„è¾“å‡ºæ¥å·¥ä½œã€‚æˆ‘ä»¬å¯ä»¥é€‰æ‹©æ½œåœ¨å‘é‡zçš„ç»´åº¦æ•°mï¼ˆå®ƒæ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼‰ï¼šå®ƒåº”è¯¥è¶³å¤Ÿå¤§ï¼Œå¯ä»¥è¡¨ç¤ºæˆ‘ä»¬æ•°æ®ä¸­çš„ä¸åŒæ¦‚å¿µï¼ˆè¿™éœ€è¦ä¸€äº›æ‰‹åŠ¨å°è¯•å’Œé”™è¯¯ï¼‰ã€‚ä¾‹å¦‚ï¼Œæˆ‘ä»¬å¯èƒ½æƒ³è¦å­¦ä¹ çš„æ½œåœ¨å±æ€§åŒ…æ‹¬å¾®ç¬‘/çš±çœ‰ã€å‘è‰²å’Œå‘å‹ã€‚zçš„å€¼è¢«ç§°ä¸º*æ½œåœ¨*ï¼Œå› ä¸ºæˆ‘ä»¬ä»æœªè§‚å¯Ÿåˆ°å®ƒä»¬å®é™…æ˜¯ä»€ä¹ˆï¼šæ¨¡å‹å¿…é¡»ä»æ•°æ®ä¸­æ¨æ–­å®ƒä»¬ã€‚å¦‚æœæˆ‘ä»¬åšå¾—å¥½ï¼Œç”Ÿæˆå™¨Gå¯ä»¥ä½¿ç”¨è¿™ä¸ªæ½œåœ¨è¡¨ç¤ºzä½œä¸ºæ•°æ®çš„ä¸€ä¸ªæ›´å°ã€æ›´ç´§å‡‘çš„è¡¨ç¤ºâ€”â€”æœ‰ç‚¹åƒæœ‰æŸå‹ç¼©çš„å½¢å¼ã€‚ä½ å¯ä»¥æŠŠç”Ÿæˆå™¨æƒ³è±¡æˆä¸€ä¸ªç´ æè‰ºæœ¯å®¶ï¼Œä»–å¿…é¡»æ¥æ”¶ä¸€ä¸ªæè¿°ï¼ˆæ½œåœ¨å‘é‡zï¼‰ï¼Œç„¶åæ ¹æ®è¿™ä¸ªæè¿°ä¸ºè¾“å‡ºæ„å»ºä¸€ä¸ªå‡†ç¡®çš„å›¾ç‰‡ï¼åŒæ ·ï¼Œä¸€ä¸ªç´ æè‰ºæœ¯å®¶çš„ç”»ä½œæ˜¯ä»–ä»¬å¦‚ä½•*è§£é‡Š*ä½ çš„æè¿°çš„ç»“æœï¼Œè¿™äº›æ½œåœ¨å‘é‡çš„æ„ä¹‰å–å†³äºç”Ÿæˆå™¨Gå¦‚ä½•è§£é‡Šå®ƒä»¬ã€‚åœ¨å®è·µä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨æ¯ä¸ªæ½œåœ¨å˜é‡ä»é«˜æ–¯åˆ†å¸ƒï¼ˆå³*z*[i]
    âˆ¼ *N*(0,1)ï¼‰é‡‡æ ·çš„ç®€å•è¡¨ç¤ºã€‚è¿™ä½¿å¾—æˆ‘ä»¬å¯ä»¥è½»æ¾åœ°é‡‡æ ·æ–°çš„zå€¼ï¼Œä»è€Œå¯ä»¥äº§ç”Ÿå’Œæ§åˆ¶åˆæˆæ•°æ®ã€‚ç”Ÿæˆå™¨å¿…é¡»å­¦ä¹ å¦‚ä½•ä»è¿™äº›å€¼ä¸­åˆ¶ä½œå‡ºæœ‰æ„ä¹‰çš„ç‰©å“ã€‚
- en: D has the goal of determining whether an input is from *real* data or *fake*
    data. So Dâ€™s job is a simple classification problem. If the input **x** âˆˆ â„^d
    came from real data, the label *y* = 1 = *y*[1] = real. If the input x came from
    the generator G, the label is *y* = 0 = *y*[0] fake. This is similar to an autoencoder
    where we use a supervised loss function, but the labels are trivial. We are essentially
    using all of the training data to define the real class, and anything output by
    G belongs to the fake class.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: Dçš„ç›®æ ‡æ˜¯ç¡®å®šè¾“å…¥æ•°æ®æ˜¯æ¥è‡ª*çœŸå®*æ•°æ®è¿˜æ˜¯*è™šå‡*æ•°æ®ã€‚å› æ­¤ï¼ŒDçš„å·¥ä½œæ˜¯ä¸€ä¸ªç®€å•çš„åˆ†ç±»é—®é¢˜ã€‚å¦‚æœè¾“å…¥**x** âˆˆ â„^dæ¥è‡ªçœŸå®æ•°æ®ï¼Œåˆ™æ ‡ç­¾*y*
    = 1 = *y*[1] = realã€‚å¦‚æœè¾“å…¥xæ¥è‡ªç”Ÿæˆå™¨Gï¼Œåˆ™æ ‡ç­¾æ˜¯*y* = 0 = *y*[0] fakeã€‚è¿™ç±»ä¼¼äºä¸€ä¸ªè‡ªç¼–ç å™¨ï¼Œæˆ‘ä»¬ä½¿ç”¨ç›‘ç£æŸå¤±å‡½æ•°ï¼Œä½†æ ‡ç­¾æ˜¯å¹³å‡¡çš„ã€‚æˆ‘ä»¬å®é™…ä¸Šä½¿ç”¨æ‰€æœ‰è®­ç»ƒæ•°æ®æ¥å®šä¹‰çœŸå®ç±»åˆ«ï¼Œè€ŒGè¾“å‡ºçš„ä»»ä½•å†…å®¹éƒ½å±äºè™šå‡ç±»åˆ«ã€‚
- en: 9.1.1 Â The loss computations
  id: totrans-22
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 æŸå¤±è®¡ç®—
- en: 'This last part may seem complicated: D and G are both computing their loss
    from Dâ€™s output, and Gâ€™s loss is somehow the opposite of Dâ€™s. Letâ€™s be explicit
    about what is involved. We use â„“(â‹…,â‹…) to represent our classification loss (binary
    cross entropy of softmax, which we have used many times). There are two models
    (G and D) with two classes (*y*[real] and *y*[fake]), giving the four combinations
    in this table:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æœ€åä¸€éƒ¨åˆ†å¯èƒ½çœ‹èµ·æ¥å¾ˆå¤æ‚ï¼šDå’ŒGéƒ½ä»Dçš„è¾“å‡ºä¸­è®¡ç®—å®ƒä»¬çš„æŸå¤±ï¼Œè€ŒGçš„æŸå¤±åœ¨æŸç§ç¨‹åº¦ä¸Šæ˜¯Dçš„ç›¸åã€‚è®©æˆ‘ä»¬æ˜ç¡®ä¸€ä¸‹æ¶‰åŠçš„å†…å®¹ã€‚æˆ‘ä»¬ä½¿ç”¨â„“(â‹…,â‹…)æ¥è¡¨ç¤ºæˆ‘ä»¬çš„åˆ†ç±»æŸå¤±ï¼ˆsoftmaxçš„äºŒè¿›åˆ¶äº¤å‰ç†µï¼Œæˆ‘ä»¬å¤šæ¬¡ä½¿ç”¨è¿‡ï¼‰ã€‚æœ‰ä¸¤ä¸ªæ¨¡å‹ï¼ˆGå’ŒDï¼‰å’Œä¸¤ä¸ªç±»åˆ«(*y*[real]å’Œ*y*[fake]ï¼‰ï¼Œç»™å‡ºè¡¨ä¸­çš„å››ç§ç»„åˆï¼š
- en: '![](../Images/CH09_F03_Table.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F03_Table.png)'
- en: 'When working on Dâ€™s loss *loss*[D], we have real data and fake data. For real
    data, we want to say that it looks real. Itâ€™s pretty straightforward, but letâ€™s
    annotate the equation anyway:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: å½“å¤„ç† D çš„æŸå¤± *loss*[D] æ—¶ï¼Œæˆ‘ä»¬æœ‰çœŸå®æ•°æ®å’Œä¼ªé€ æ•°æ®ã€‚å¯¹äºçœŸå®æ•°æ®ï¼Œæˆ‘ä»¬å¸Œæœ›è¯´å®ƒçœ‹èµ·æ¥æ˜¯çœŸå®çš„ã€‚è¿™ç›¸å½“ç›´æ¥ï¼Œä½†è®©æˆ‘ä»¬ä»ç„¶æ ‡æ³¨è¿™ä¸ªæ–¹ç¨‹ï¼š
- en: '![](../Images/CH09_UN01_Raff.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN01_Raff.png)'
- en: 'That gives us Dâ€™s loss on real data. Its loss on fake data is also a straightforward
    classification, except we replace the real data x with the generatorâ€™s output
    *G*(**z**), and we use *y*[fake] as the target because the input to D is fake
    data from G. Again, letâ€™s annotate this:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç»™å‡ºäº† D åœ¨çœŸå®æ•°æ®ä¸Šçš„æŸå¤±ã€‚å®ƒåœ¨ä¼ªé€ æ•°æ®ä¸Šçš„æŸå¤±ä¹Ÿæ˜¯ä¸€ä¸ªç›´æ¥çš„åˆ†ç±»ï¼Œé™¤äº†æˆ‘ä»¬å°†çœŸå®æ•°æ® x æ›¿æ¢ä¸ºç”Ÿæˆå™¨çš„è¾“å‡º *G*(**z**)ï¼Œæˆ‘ä»¬ä½¿ç”¨ *y*[ä¼ªé€ ]
    ä½œä¸ºç›®æ ‡ï¼Œå› ä¸º D çš„è¾“å…¥æ˜¯æ¥è‡ª G çš„ä¼ªé€ æ•°æ®ã€‚å†æ¬¡ï¼Œè®©æˆ‘ä»¬æ ‡æ³¨è¿™ä¸€ç‚¹ï¼š
- en: '![](../Images/CH09_UN02_Raff.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN02_Raff.png)'
- en: 'On to the generator Gâ€™s loss. For real data, it is easy: G does not care what
    D says about real data, so nothing happens to G when real data is used. But for
    fake data, G does care about what D says. It wants D to call its fake data real,
    because that means it has successfully tricked D. This involves swapping the label.
    Letâ€™s annotate that equation, too:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯ç”Ÿæˆå™¨ G çš„æŸå¤±ã€‚å¯¹äºçœŸå®æ•°æ®ï¼Œè¿™å¾ˆç®€å•ï¼šG ä¸å…³å¿ƒ D å¯¹çœŸå®æ•°æ®çš„è¯´æ³•ï¼Œæ‰€ä»¥å½“ä½¿ç”¨çœŸå®æ•°æ®æ—¶ï¼ŒG æ²¡æœ‰ä»»ä½•å˜åŒ–ã€‚ä½†å¯¹äºä¼ªé€ æ•°æ®ï¼ŒG æ˜¯å…³å¿ƒçš„ã€‚å®ƒå¸Œæœ›
    D å°†å…¶ä¼ªé€ æ•°æ®ç§°ä¸ºçœŸå®æ•°æ®ï¼Œå› ä¸ºè¿™æ„å‘³ç€å®ƒå·²ç»æˆåŠŸåœ°æ¬ºéª—äº† Dã€‚è¿™æ¶‰åŠåˆ°æ ‡ç­¾çš„äº¤æ¢ã€‚è®©æˆ‘ä»¬ä¹Ÿæ ‡æ³¨è¿™ä¸ªæ–¹ç¨‹ï¼š
- en: '![](../Images/CH09_UN03_Raff.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN03_Raff.png)'
- en: Putting the losses together
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: å°†æŸå¤±åˆå¹¶
- en: 'Now that we know how to compute all the components of the loss, we can combine
    them. We have to keep them listed as two losses because we are training two different
    networks. First we have the discriminatorâ€™s loss:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬çŸ¥é“äº†å¦‚ä½•è®¡ç®—æŸå¤±çš„å„ä¸ªç»„æˆéƒ¨åˆ†ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬ç»„åˆèµ·æ¥ã€‚æˆ‘ä»¬å¿…é¡»å°†å®ƒä»¬åˆ—ä¸ºä¸¤ä¸ªæŸå¤±ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨è®­ç»ƒä¸¤ä¸ªä¸åŒçš„ç½‘ç»œã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰åˆ¤åˆ«å™¨çš„æŸå¤±ï¼š
- en: '![](../Images/ch9-eqs-to-illustrator0x.png)'
  id: totrans-33
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ch9-eqs-to-illustrator0x.png)'
- en: This is a simple combination of the two values from the table on the previous
    page. Weâ€™ve just been a little more explicit that this loss is computed over all
    the data n (which will become batches instead of the entire dataset). Since a
    `for` loop is involved with the summation Î£, we also write **z** âˆ¼ ğ’©(0,1) to be
    explicit that a different random vector z is used each time.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åªæ˜¯å°†ä¸Šä¸€é¡µè¡¨ä¸­çš„ä¸¤ä¸ªå€¼ç®€å•ç»„åˆèµ·æ¥ã€‚æˆ‘ä»¬åªæ˜¯æ›´æ˜ç¡®åœ°æŒ‡å‡ºï¼Œè¿™ä¸ªæŸå¤±æ˜¯åœ¨æ‰€æœ‰æ•°æ® n ä¸Šè®¡ç®—çš„ï¼ˆè¿™å°†æˆä¸ºæ‰¹æ¬¡è€Œä¸æ˜¯æ•´ä¸ªæ•°æ®é›†ï¼‰ã€‚ç”±äºæ¶‰åŠåˆ°æ±‚å’Œ Î£ çš„
    `for` å¾ªç¯ï¼Œæˆ‘ä»¬ä¹Ÿå†™ **z** âˆ¼ ğ’©(0,1) ä»¥æ˜ç¡®æŒ‡å‡ºæ¯æ¬¡ä½¿ç”¨ä¸åŒçš„éšæœºå‘é‡ zã€‚
- en: 'We have the generatorâ€™s loss, which has only one summation symbol because G
    does not care what D says about real data. G only cares about the fake data, and
    G is successful if it tricks D into calling fake data real. This gets us the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬æœ‰ç”Ÿæˆå™¨çš„æŸå¤±ï¼Œå®ƒåªæœ‰ä¸€ä¸ªæ±‚å’Œç¬¦å·ï¼Œå› ä¸º G ä¸å…³å¿ƒ D å¯¹çœŸå®æ•°æ®çš„è¯´æ³•ã€‚G åªå…³å¿ƒä¼ªé€ æ•°æ®ï¼Œå¦‚æœ G èƒ½æ¬ºéª— D å°†ä¼ªé€ æ•°æ®ç§°ä¸ºçœŸå®æ•°æ®ï¼Œé‚£ä¹ˆ
    G å°±æ˜¯æˆåŠŸçš„ã€‚è¿™å¯¼è‡´ä»¥ä¸‹ç»“æœï¼š
- en: '![](../Images/ch9-eqs-to-illustrator1x.png)'
  id: totrans-36
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/ch9-eqs-to-illustrator1x.png)'
- en: ğ”¼xpectation
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: æœŸæœ›
- en: You may notice something odd about the equations involving the random data z.
    We are summing n items but never access anything. It does not matter how big the
    dataset is; we could sample half as many values of z or twice as many. We would
    never get an index out of bounds unless we kept increasing the value of n.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°æ¶‰åŠéšæœºæ•°æ® z çš„æ–¹ç¨‹ä¸­æœ‰äº›å¥‡æ€ªçš„åœ°æ–¹ã€‚æˆ‘ä»¬åœ¨æ±‚å’Œ n é¡¹ï¼Œä½†ä»æœªè®¿é—®è¿‡ä»»ä½•ä¸œè¥¿ã€‚æ•°æ®é›†æœ‰å¤šå¤§å¹¶ä¸é‡è¦ï¼›æˆ‘ä»¬å¯ä»¥é‡‡æ · z çš„å€¼çš„ä¸€åŠæˆ–ä¸¤å€ã€‚é™¤éæˆ‘ä»¬ä¸æ–­å¢åŠ 
    n çš„å€¼ï¼Œå¦åˆ™æˆ‘ä»¬æ°¸è¿œä¸ä¼šè¶…å‡ºç´¢å¼•èŒƒå›´ã€‚
- en: This is because we are approximating what we could call an *expectation*. The
    equation 1/*n* Î£[i]^n[=1] â„“ (*D*(*G*(**z**âˆ¼ ğ’©(0,1)), *y*[real] basically asks,
    â€œWhat do we expect the average value of â„“(*D*(*G*(**z**),*y*[real]) to be if **z**
    âˆ¼ ğ’©(0,1)? Another way to write this is
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯å› ä¸ºæˆ‘ä»¬æ­£åœ¨è¿‘ä¼¼æˆ‘ä»¬å¯èƒ½ç§°ä¹‹ä¸ºæœŸæœ›çš„ä¸œè¥¿ã€‚æ–¹ç¨‹ 1/*n* Î£[i]^n[=1] â„“ (*D*(*G*(**z**âˆ¼ ğ’©(0,1)), *y*[çœŸå®]
    åŸºæœ¬ä¸Šæ˜¯åœ¨é—®ï¼Œâ€œå¦‚æœ **z** âˆ¼ ğ’©(0,1)ï¼Œæˆ‘ä»¬æœŸæœ› â„“(*D*(*G*(**z**),*y*[çœŸå®]) çš„å¹³å‡å€¼æ˜¯å¤šå°‘ï¼Ÿå¦ä¸€ç§å†™æ³•æ˜¯
- en: '![](../Images/CH09_F03_EQ03.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F03_EQ03.png)'
- en: which is the mathematical way to ask what the exact answer is if you set *n*
    = âˆ. Obviously, we donâ€™t have time to sample forever, but this expectation-based
    symbol is very common when reading about GANs. For this reason, itâ€™s worth spending
    a little extra time getting familiar with this symbol so you are prepared to read
    other materials.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ˜¯ä»¥æ•°å­¦æ–¹å¼è¯¢é—®å¦‚æœä½ å°† *n* è®¾ç½®ä¸º âˆï¼Œç¡®åˆ‡ç­”æ¡ˆæ˜¯ä»€ä¹ˆã€‚æ˜¾ç„¶ï¼Œæˆ‘ä»¬æ²¡æœ‰æ—¶é—´æ°¸è¿œé‡‡æ ·ï¼Œä½†åŸºäºæœŸæœ›çš„è¿™ç§ç¬¦å·åœ¨é˜…è¯»å…³äº GANs çš„å†…å®¹æ—¶éå¸¸å¸¸è§ã€‚å› æ­¤ï¼ŒèŠ±ç‚¹é¢å¤–çš„æ—¶é—´ç†Ÿæ‚‰è¿™ä¸ªç¬¦å·æ˜¯å€¼å¾—çš„ï¼Œè¿™æ ·ä½ å°±å¯ä»¥å‡†å¤‡å¥½é˜…è¯»å…¶ä»–ææ–™ã€‚
- en: 'The symbol ğ”¼ in the previous equation stands for *expectation*. If you have
    a distribution p and a function f, writing: ğ”¼[*z* âˆ¼ *p*]*f*(*z*) is a fancy way
    of saying â€œIf I sample values z from the distribution p *forever*, what is the
    average value of *f*(*z*)?â€ Sometimes we can use math to prove what the expectation
    will be and compute the result directlyâ€”but that is not going to happen here.
    Instead, we can do something like sample one value for every item in a batch.
    This approach is an *approximation* of the expectation because we sample for a
    *finite* number of steps instead of an *infinite* number.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: å‰ä¸€ä¸ªæ–¹ç¨‹ä¸­çš„ç¬¦å· ğ”¼ ä»£è¡¨ *æœŸæœ›*ã€‚å¦‚æœä½ æœ‰ä¸€ä¸ªåˆ†å¸ƒ p å’Œä¸€ä¸ªå‡½æ•° fï¼Œå†™ä½œ ğ”¼[*z* âˆ¼ *p*]*f*(*z*) æ˜¯ä¸€ç§èŠ±å“¨çš„è¯´æ³•ï¼Œâ€œå¦‚æœæˆ‘ä»åˆ†å¸ƒ
    p ä¸­æ— é™æœŸåœ°æŠ½å–å€¼ zï¼Œé‚£ä¹ˆ *f*(*z*) çš„å¹³å‡å€¼æ˜¯å¤šå°‘ï¼Ÿâ€æœ‰æ—¶æˆ‘ä»¬å¯ä»¥ç”¨æ•°å­¦æ¥è¯æ˜æœŸæœ›å°†æ˜¯ä»€ä¹ˆï¼Œå¹¶ç›´æ¥è®¡ç®—ç»“æœâ€”â€”ä½†åœ¨è¿™é‡Œä¸ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚ç›¸åï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®æŠ½å–ä¸€ä¸ªå€¼ã€‚è¿™ç§æ–¹æ³•æ˜¯æœŸæœ›çš„
    *è¿‘ä¼¼*ï¼Œå› ä¸ºæˆ‘ä»¬æŠ½å–çš„æ˜¯ *æœ‰é™* æ­¥éª¤è€Œä¸æ˜¯ *æ— é™* æ­¥éª¤ã€‚
- en: 'Letâ€™s close out this detour by rewriting the *loss*[D] using the expectation
    symbol. We have a summation for Dâ€™s predictions on real data, because there is
    a finite amount of real data. We have an expectation for Dâ€™s predictions on fake
    data, because there is infinite fake data. This gets us the following equation:'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬é€šè¿‡ç”¨æœŸæœ›ç¬¦å·é‡å†™ *æŸå¤±*[D] æ¥ç»“æŸè¿™æ¬¡å°æ’æ›²ã€‚å¯¹äº D åœ¨çœŸå®æ•°æ®ä¸Šçš„é¢„æµ‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæ±‚å’Œï¼Œå› ä¸ºçœŸå®æ•°æ®é‡æ˜¯æœ‰é™çš„ã€‚å¯¹äº D åœ¨ä¼ªé€ æ•°æ®ä¸Šçš„é¢„æµ‹ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªæœŸæœ›ï¼Œå› ä¸ºä¼ªé€ æ•°æ®æ˜¯æ— é™çš„ã€‚è¿™ç»™æˆ‘ä»¬ä»¥ä¸‹æ–¹ç¨‹ï¼š
- en: '![](../Images/CH09_F03_EQ04.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F03_EQ04.png)'
- en: 9.1.2 Â The GAN games
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 GAN æ¸¸æˆ
- en: We have talked about how the two losses *loss*[D] and *loss*[G] are computed
    for the discriminator and generator, respectively. How do we train two networks
    with two losses? By taking turns computing *loss*[D] and *loss*[G], updating both
    networks from their respective losses, and repeating. This turns the training
    procedure into a kind of game between G and D. Each gets its own score for how
    well itâ€™s doing at the game, and each tries to improve its score to the detriment
    of the other.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»è®¨è®ºäº†å¦‚ä½•åˆ†åˆ«è®¡ç®—åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„ä¸¤ä¸ªæŸå¤± *loss*[D] å’Œ *loss*[G]ã€‚æˆ‘ä»¬å¦‚ä½•è®­ç»ƒå…·æœ‰ä¸¤ä¸ªæŸå¤±çš„ä¸¤ç§ç½‘ç»œï¼Ÿé€šè¿‡è½®æµè®¡ç®— *loss*[D]
    å’Œ *loss*[G]ï¼Œä»å„è‡ªçš„æŸå¤±æ›´æ–°ä¸¤ä¸ªç½‘ç»œï¼Œå¹¶é‡å¤ã€‚è¿™ä½¿å¾—è®­ç»ƒè¿‡ç¨‹å˜æˆäº† G å’Œ D ä¹‹é—´çš„ä¸€ç§æ¸¸æˆã€‚æ¯ä¸ªéƒ½æ ¹æ®è‡ªå·±åœ¨æ¸¸æˆä¸­çš„è¡¨ç°å¾—åˆ°è‡ªå·±çš„åˆ†æ•°ï¼Œæ¯ä¸ªéƒ½è¯•å›¾æé«˜è‡ªå·±çš„åˆ†æ•°ï¼ŒæŸå®³å¯¹æ–¹çš„åˆ†æ•°ã€‚
- en: The game (training process) starts with the generator producing horrible, random-looking
    results, which are easy for the discriminator to separate from real data (see
    figure 9.4). D tries to predict the source (training data or G) based on the image
    and gets a loss computed from both real and fake data. Gâ€™s loss is computed over
    only the fake data and is the same setup as Dâ€™s loss but with the label swapped.
    The label is swapped because G wants D to say real instead of fake for Gâ€™s work.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ä¸ªæ¸¸æˆï¼ˆè®­ç»ƒè¿‡ç¨‹ï¼‰å¼€å§‹äºç”Ÿæˆå™¨äº§ç”Ÿç³Ÿç³•çš„ã€çœ‹èµ·æ¥éšæœºçš„ç»“æœï¼Œè¿™äº›ç»“æœå¯¹åˆ¤åˆ«å™¨æ¥è¯´å¾ˆå®¹æ˜“ä¸çœŸå®æ•°æ®åŒºåˆ†å¼€æ¥ï¼ˆè§å›¾ 9.4ï¼‰ã€‚D å°è¯•æ ¹æ®å›¾åƒé¢„æµ‹æ¥æºï¼ˆè®­ç»ƒæ•°æ®æˆ–
    Gï¼‰ï¼Œå¹¶å¾—åˆ°ä»çœŸå®å’Œä¼ªé€ æ•°æ®è®¡ç®—å‡ºçš„æŸå¤±ã€‚G çš„æŸå¤±ä»…ä»ä¼ªé€ æ•°æ®è®¡ç®—ï¼Œä¸ D çš„æŸå¤±è®¾ç½®ç›¸åŒï¼Œä½†æ ‡ç­¾å·²äº¤æ¢ã€‚æ ‡ç­¾äº¤æ¢æ˜¯å› ä¸º G æƒ³è®© D è¯´ G çš„å·¥ä½œæ˜¯çœŸå®çš„è€Œä¸æ˜¯ä¼ªé€ çš„ã€‚
- en: '![](../Images/CH09_F04_Raff.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F04_Raff.png)'
- en: Figure 9.4 The start of training a GAN. D receives a batch of multiple images.
    Some come from the real training data, and others are fake (created by G). For
    every prediction, D receives a loss based on whether it was correct or not about
    the source of the image (real versus fake). Gâ€™s loss is computed only from the
    discriminatorâ€™s prediction.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.4 GAN è®­ç»ƒçš„å¼€å§‹ã€‚D æ¥æ”¶ä¸€ä¸ªåŒ…å«å¤šä¸ªå›¾åƒçš„æ‰¹æ¬¡ã€‚å…¶ä¸­ä¸€äº›æ¥è‡ªçœŸå®è®­ç»ƒæ•°æ®ï¼Œå…¶ä»–çš„æ˜¯ä¼ªé€ çš„ï¼ˆç”± G åˆ›å»ºï¼‰ã€‚å¯¹äºæ¯ä¸€æ¬¡é¢„æµ‹ï¼ŒD éƒ½ä¼šæ ¹æ®å›¾åƒçš„æ¥æºï¼ˆçœŸå®ä¸ä¼ªé€ ï¼‰æ˜¯å¦æ­£ç¡®æ¥æ”¶ä¸€ä¸ªæŸå¤±ã€‚G
    çš„æŸå¤±ä»…ä»åˆ¤åˆ«å™¨çš„é¢„æµ‹è®¡ç®—ã€‚
- en: Because Gâ€™s loss is based on what the discriminator D says, it learns to alter
    its predictions to look more like what D thinks real data looks like. As the generator
    G improves, the discriminator D needs to improve how it distinguishes between
    real and fake data. The cycle repeats forever (or until convergence). Figure 9.5
    shows how the results might evolve over multiple rounds of this game.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: å› ä¸º G çš„æŸå¤±æ˜¯åŸºäºåˆ¤åˆ«å™¨ D çš„è¯´æ³•ï¼Œå®ƒå­¦ä¼šäº†æ”¹å˜å…¶é¢„æµ‹ï¼Œä½¿å…¶çœ‹èµ·æ¥æ›´åƒ D è®¤ä¸ºçš„çœŸå®æ•°æ®çš„æ ·å­ã€‚éšç€ç”Ÿæˆå™¨ G çš„æ”¹è¿›ï¼Œåˆ¤åˆ«å™¨ D éœ€è¦æ”¹è¿›å…¶åŒºåˆ†çœŸå®å’Œä¼ªé€ æ•°æ®çš„æ–¹æ³•ã€‚è¿™ä¸ªå¾ªç¯æ°¸è¿œé‡å¤ï¼ˆæˆ–è€…ç›´åˆ°æ”¶æ•›ï¼‰ã€‚å›¾
    9.5 å±•ç¤ºäº†åœ¨è¿™ä¸ªæ¸¸æˆçš„å¤šè½®ä¸­ç»“æœå¯èƒ½å¦‚ä½•æ¼”å˜ã€‚
- en: '![](../Images/CH09_F05_Raff.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F05_Raff.png)'
- en: Figure 9.5 Multiple rounds of training G and D together. Early on, D has an
    easy time because the fake data looks random. Eventually G gets better and tricks
    D. D learns to better distinguish fake data, which lets G get better at making
    more realistic data. Eventually, both D and G are pretty good at their jobs.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.5 å¤šè½®è®­ç»ƒ G å’Œ Dã€‚ä¸€å¼€å§‹ï¼ŒD å¾ˆå®¹æ˜“ï¼Œå› ä¸ºä¼ªé€ çš„æ•°æ®çœ‹èµ·æ¥å¾ˆéšæœºã€‚æœ€ç»ˆ G å˜å¾—æ›´å¥½ï¼Œæ¬ºéª—äº† Dã€‚D å­¦ä¹ æ›´å¥½åœ°åŒºåˆ†ä¼ªé€ æ•°æ®ï¼Œè¿™ä½¿å¾— G
    åœ¨åˆ¶ä½œæ›´çœŸå®çš„æ•°æ®æ–¹é¢å˜å¾—æ›´å¥½ã€‚æœ€ç»ˆï¼ŒD å’Œ G éƒ½åœ¨å„è‡ªçš„å·¥ä½œä¸Šåšå¾—ç›¸å½“ä¸é”™ã€‚
- en: 'Weâ€™ve given the complete description of how to set up a GAN for training. There
    is another detail about how to consider or interpret the GAN setup called *min-max*.
    If we think of D as returning the probability that an input comes from real data,
    we want to alter D to *maximize* Dâ€™s probability on real data while simultaneously
    altering G to *minimize* Dâ€™s performance on fake data. This useful concept comes
    with a scary-looking equation:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ç»™å‡ºäº†å¦‚ä½•è®¾ç½® GAN è¿›è¡Œè®­ç»ƒçš„å®Œæ•´æè¿°ã€‚å…³äºå¦‚ä½•è€ƒè™‘æˆ–è§£é‡Š GAN è®¾ç½®çš„å¦ä¸€ä¸ªç»†èŠ‚è¢«ç§°ä¸º *min-max*ã€‚å¦‚æœæˆ‘ä»¬è®¤ä¸º D æ˜¯è¿”å›è¾“å…¥æ¥è‡ªçœŸå®æ•°æ®çš„æ¦‚ç‡ï¼Œæˆ‘ä»¬å¸Œæœ›æ”¹å˜
    D ä»¥ *æœ€å¤§åŒ–* D åœ¨çœŸå®æ•°æ®ä¸Šçš„æ¦‚ç‡ï¼ŒåŒæ—¶æ”¹å˜ G ä»¥ *æœ€å°åŒ–* D åœ¨ä¼ªé€ æ•°æ®ä¸Šçš„æ€§èƒ½ã€‚è¿™ä¸ªæœ‰ç”¨çš„æ¦‚å¿µä¼´éšç€ä¸€ä¸ªçœ‹èµ·æ¥å¾ˆå“äººçš„æ–¹ç¨‹ï¼š
- en: '![](../Images/ch9-eqs-to-illustrator3x.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾åƒæè¿°](../Images/ch9-eqs-to-illustrator3x.png)'
- en: I donâ€™t like this equation for explaining GANs at a practical levelâ€”it looks
    scarier than I think it needs to be. I include it because you will see that equation
    a lot when reading about GANs. But if you look carefully, this is the same equation
    we started with. You can replace the expectations ğ”¼ with summations Î£, and you
    can replace log (*D*(**x**)) with â„“(*D*(**x**),*y*[real]), and log (1âˆ’*D*(*G*(**z**)))
    with â„“(*D*(*G*(**z**)),*y*[fake]). That gets you exactly what we had at the start.
    We solve this min-max game by taking turns optimizing D and G for their respective
    goals. I also like writing it using loss functions â„“ because then itâ€™s a little
    more obvious that we could change the loss functions to change the behavior of
    our GAN. Changing the loss â„“ in a GAN is extremely common, so itâ€™s an important
    detail I donâ€™t want to hide.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä¸å–œæ¬¢è¿™ä¸ªæ–¹ç¨‹æ¥è§£é‡Š GAN åœ¨å®é™…å±‚é¢çš„æƒ…å†µâ€”â€”å®ƒçœ‹èµ·æ¥æ¯”æˆ‘è®¤ä¸ºçš„æ›´å“äººã€‚æˆ‘åŒ…æ‹¬å®ƒæ˜¯å› ä¸ºä½ ä¼šåœ¨é˜…è¯»æœ‰å…³ GAN çš„å†…å®¹æ—¶ç»å¸¸çœ‹åˆ°è¿™ä¸ªæ–¹ç¨‹ã€‚ä½†å¦‚æœä½ ä»”ç»†çœ‹ï¼Œè¿™å®é™…ä¸Šæ˜¯æˆ‘ä»¬åœ¨å¼€å§‹æ—¶ä½¿ç”¨çš„åŒä¸€ä¸ªæ–¹ç¨‹ã€‚ä½ å¯ä»¥ç”¨æ±‚å’Œ
    Î£ æ›¿æ¢æœŸæœ› ğ”¼ï¼Œç”¨ â„“(*D*(**x**),*y*[real]) æ›¿æ¢ log (*D*(**x**))ï¼Œç”¨ â„“(*D*(*G*(**z**)),*y*[fake])
    æ›¿æ¢ log (1âˆ’*D*(*G*(**z**)))ã€‚è¿™æ ·ä½ å°±èƒ½å¾—åˆ°æˆ‘ä»¬æœ€åˆæ‹¥æœ‰çš„ä¸œè¥¿ã€‚æˆ‘ä»¬é€šè¿‡è½®æµä¼˜åŒ– D å’Œ G ä»¥å®ç°å®ƒä»¬å„è‡ªçš„ç›®æ ‡æ¥è§£å†³è¿™ä¸ª min-max
    æ¸¸æˆã€‚æˆ‘ä¹Ÿå–œæ¬¢ç”¨æŸå¤±å‡½æ•° â„“ æ¥å†™å®ƒï¼Œå› ä¸ºè¿™æ ·ä¼šæ›´æ˜æ˜¾åœ°è¡¨æ˜æˆ‘ä»¬å¯ä»¥æ”¹å˜æŸå¤±å‡½æ•°æ¥æ”¹å˜æˆ‘ä»¬ GAN çš„è¡Œä¸ºã€‚åœ¨ GAN ä¸­æ”¹å˜æŸå¤± â„“ æ˜¯æå…¶å¸¸è§çš„ï¼Œæ‰€ä»¥è¿™æ˜¯ä¸€ä¸ªé‡è¦çš„ç»†èŠ‚ï¼Œæˆ‘ä¸æƒ³éšç’ã€‚
- en: 9.1.3 Â Implementing our first GAN
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.3 å®ç°æˆ‘ä»¬çš„ç¬¬ä¸€ä¸ª GAN
- en: 'Now that we understand the game played by a generator G and discriminator D,
    we can begin to implement it. We need to define `Module`s for both D and G, writing
    a new training loop that takes turns updating D and G, respectively and some extra
    helper code to record useful information and visualize our results. First, letâ€™s
    predefine some values: the `batch_size` B, the number of `neurons` for our hidden
    layers, and the number of epochs `num_epochs` for training our GAN. Two new things
    are `latent_d` and `out_shape`.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»ç†è§£äº†ç”Ÿæˆå™¨ G å’Œåˆ¤åˆ«å™¨ D æ‰€ç©çš„æ¸¸æˆï¼Œæˆ‘ä»¬å¯ä»¥å¼€å§‹å®æ–½å®ƒäº†ã€‚æˆ‘ä»¬éœ€è¦ä¸º D å’Œ G å®šä¹‰ `Module`sï¼Œç¼–å†™ä¸€ä¸ªæ–°çš„è®­ç»ƒå¾ªç¯ï¼Œè¯¥å¾ªç¯è½®æµæ›´æ–°
    D å’Œ Gï¼Œå¹¶æ·»åŠ ä¸€äº›é¢å¤–çš„è¾…åŠ©ä»£ç æ¥è®°å½•æœ‰ç”¨çš„ä¿¡æ¯å’Œå¯è§†åŒ–æˆ‘ä»¬çš„ç»“æœã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬é¢„å®šä¹‰ä¸€äº›å€¼ï¼šæ‰¹å¤§å° `batch_size` Bï¼Œæˆ‘ä»¬éšè—å±‚çš„ `neurons`
    æ•°é‡ï¼Œä»¥åŠè®­ç»ƒæˆ‘ä»¬çš„ GAN çš„ `num_epochs` è½®æ•°ã€‚ä¸¤ä¸ªæ–°çš„æ˜¯ `latent_d` å’Œ `out_shape`ã€‚
- en: The variable `latent_d` is the number of dimensions in our latent variable z.
    We can make this smaller or larger; it is a new hyperparameter for our model.
    If we have too few or too many dimensions, we have difficulty training. My recommendation
    is to start out at 64 or 128 and keep increasing the dimension until you get results
    that look good. We also have a variable `out_shape` that is used purely to reshape
    the output of our network to the given dimensions. It starts with `-1` for the
    batch dimension. This way, the results from the generator will be whatever shape
    we want. This will be more important later when we make a convolutional GAN; for
    now, we start with fully connected networks. The `fcLayer` function gives us a
    shorthand for building the models in this chapter.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: å˜é‡ `latent_d` æ˜¯æˆ‘ä»¬æ½œåœ¨å˜é‡ z çš„ç»´åº¦æ•°ã€‚æˆ‘ä»¬å¯ä»¥ä½¿è¿™ä¸ªå€¼æ›´å°æˆ–æ›´å¤§ï¼›è¿™æ˜¯æˆ‘ä»¬æ¨¡å‹çš„ä¸€ä¸ªæ–°è¶…å‚æ•°ã€‚å¦‚æœæˆ‘ä»¬ç»´åº¦å¤ªå°‘æˆ–å¤ªå¤šï¼Œæˆ‘ä»¬ä¼šæœ‰å›°éš¾è¿›è¡Œè®­ç»ƒã€‚æˆ‘çš„å»ºè®®æ˜¯ä»
    64 æˆ– 128 å¼€å§‹ï¼Œå¹¶ç»§ç»­å¢åŠ ç»´åº¦ï¼Œç›´åˆ°ä½ å¾—åˆ°çœ‹èµ·æ¥å¾ˆå¥½çš„ç»“æœã€‚æˆ‘ä»¬è¿˜æœ‰ä¸€ä¸ªå˜é‡ `out_shape`ï¼Œå®ƒçº¯ç²¹ç”¨äºå°†ç½‘ç»œçš„è¾“å‡ºé‡å¡‘ä¸ºç»™å®šçš„ç»´åº¦ã€‚å®ƒä»
    `-1` å¼€å§‹ï¼Œç”¨äºæ‰¹å¤„ç†ç»´åº¦ã€‚è¿™æ ·ï¼Œç”Ÿæˆå™¨çš„ç»“æœå°†æ˜¯æˆ‘ä»¬æƒ³è¦çš„ä»»ä½•å½¢çŠ¶ã€‚è¿™å°†åœ¨æˆ‘ä»¬åˆ¶ä½œå·ç§¯ GAN æ—¶å˜å¾—æ›´é‡è¦ï¼›ç°åœ¨ï¼Œæˆ‘ä»¬ä»¥å…¨è¿æ¥ç½‘ç»œå¼€å§‹ã€‚`fcLayer`
    å‡½æ•°ä¸ºæˆ‘ä»¬æä¾›äº†æ„å»ºæœ¬ç« ä¸­æ¨¡å‹çš„å¿«æ·æ–¹å¼ã€‚
- en: 'Hereâ€™s the code:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»£ç ï¼š
- en: '[PRE0]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: â¶ You could also do (â€“1, 1, 28, 28) for one channel, but that makes NumPy code
    a little more cumbersome later.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨ï¼ˆâ€“1ï¼Œ1ï¼Œ28ï¼Œ28ï¼‰æ¥è¡¨ç¤ºä¸€ä¸ªé€šé“ï¼Œä½†è¿™ä¼šä½¿NumPyä»£ç ç¨æ˜¾ç¹çã€‚
- en: â· Our helper function
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: â· æˆ‘ä»¬çš„è¾…åŠ©å‡½æ•°
- en: Defining the D and G Networks
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: å®šä¹‰Då’ŒGç½‘ç»œ
- en: Next, we implement a function that defines both the generator G and the discriminator
    D. The generator needs to know `latent_d` because that will be the input size.
    Both G and D need to know the `out_shape` because it is the output of the generator
    *and* the input to the discriminator. Iâ€™ve also included an optional flag `sgimoidG`
    to control when the generator should end with a `nn.Sigmoid` activation *Ïƒ*(â‹…).
    For some of the GANs we train in this chapter, we want to apply *Ïƒ*(â‹…) at the
    end so the output is constrained to be in the range [0,1] because our MNIST data
    is also constrained to [0,1]. But we also do an example problem where there is
    no such constraint. Iâ€™ve also arbitrarily defined a few hidden layers with a LeakyReLU
    and layer normalization (LN).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œè¯¥å‡½æ•°å®šä¹‰äº†ç”Ÿæˆå™¨Gå’Œåˆ¤åˆ«å™¨Dã€‚ç”Ÿæˆå™¨éœ€è¦çŸ¥é“ `latent_d`ï¼Œå› ä¸ºè¿™å°†ä½œä¸ºè¾“å…¥å¤§å°ã€‚Gå’ŒDéƒ½éœ€è¦çŸ¥é“ `out_shape`ï¼Œå› ä¸ºå®ƒæ˜¯ç”Ÿæˆå™¨çš„è¾“å‡ºä¹Ÿæ˜¯åˆ¤åˆ«å™¨çš„è¾“å…¥ã€‚æˆ‘è¿˜åŒ…æ‹¬äº†ä¸€ä¸ªå¯é€‰æ ‡å¿—
    `sgimoidG`ï¼Œç”¨äºæ§åˆ¶ç”Ÿæˆå™¨ä½•æ—¶ä»¥ `nn.Sigmoid` æ¿€æ´» *Ïƒ*(â‹…) ç»“æŸã€‚å¯¹äºæœ¬ç« ä¸­æˆ‘ä»¬è®­ç»ƒçš„ä¸€äº›GANï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨è¾“å‡ºè¢«çº¦æŸåœ¨[0,1]èŒƒå›´å†…æ—¶åº”ç”¨
    *Ïƒ*(â‹…)ï¼Œå› ä¸ºæˆ‘ä»¬çš„MNISTæ•°æ®ä¹Ÿè¢«çº¦æŸåœ¨[0,1]èŒƒå›´å†…ã€‚ä½†æˆ‘ä»¬è¿˜ä¸¾äº†ä¸€ä¸ªæ²¡æœ‰è¿™ç§çº¦æŸçš„ä¾‹å­ã€‚æˆ‘è¿˜ä»»æ„å®šä¹‰äº†ä¸€äº›å…·æœ‰LeakyReLUå’Œå±‚å½’ä¸€åŒ–ï¼ˆLNï¼‰çš„éšè—å±‚ã€‚
- en: 'Hereâ€™s the code:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»£ç ï¼š
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: â¶ np.prod multiplies each value in the shape, giving us the total number of
    needed outputs. abs removes the impact of â€œâ€“1" for the batch dimension.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ `np.prod` å‡½æ•°å°†å½¢çŠ¶ä¸­çš„æ¯ä¸ªå€¼ç›¸ä¹˜ï¼Œä»è€Œå¾—åˆ°æ‰€éœ€è¾“å‡ºçš„æ€»æ•°ã€‚`abs` å‡½æ•°æ¶ˆé™¤äº†æ‰¹ç»´åº¦ä¸­â€œâ€“1â€çš„å½±å“ã€‚
- en: â· Reshapes the output to whatever D expects
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: â· å°†è¾“å‡ºé‡å¡‘ä¸ºDæœŸæœ›çš„ä»»ä½•å½¢çŠ¶
- en: â¸ Sometimes we do or donâ€™t want G to return a sigmoid value (i.e., [0,1]), so
    we rap it in a conditional.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ æœ‰æ—¶æˆ‘ä»¬å¸Œæœ›æˆ–ä¸å¸Œæœ›Gè¿”å›ä¸€ä¸ªsigmoidå€¼ï¼ˆå³ï¼Œ[0,1]ï¼‰ï¼Œæ‰€ä»¥æˆ‘ä»¬å°†å…¶æ”¾åœ¨ä¸€ä¸ªæ¡ä»¶è¯­å¥ä¸­ã€‚
- en: â¹ D has one output for a binary classification problem.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ Dæœ‰ä¸€ä¸ªè¾“å‡ºç”¨äºäºŒåˆ†ç±»é—®é¢˜ã€‚
- en: 'With this function, we can quickly define a new G and D model by calling the
    `simpleGAN` function:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡è°ƒç”¨ `simpleGAN` å‡½æ•°å¿«é€Ÿå®šä¹‰æ–°çš„Gå’ŒDæ¨¡å‹ï¼š
- en: '[PRE2]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: A starting recipe for GANs
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: GANçš„èµ·å§‹é…æ–¹
- en: 'GANs are notorious for being difficult to train. The first attempt often does
    not work well, and it can take a lot of manual fiddling to get them to work and
    produce nice crisp results like the example at the start of this chapter. There
    are a lot of tricks online for getting GANs to train well, but some of them are
    specific to certain types of GANs. Others are more reliable and work well for
    multiple architectures. Here are my recommendations when trying to build any GAN:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: GANå› å…¶éš¾ä»¥è®­ç»ƒè€Œè‡­åæ˜­è‘—ã€‚ç¬¬ä¸€æ¬¡å°è¯•å¾€å¾€æ•ˆæœä¸ä½³ï¼Œå¯èƒ½éœ€è¦å¤§é‡æ‰‹åŠ¨è°ƒæ•´æ‰èƒ½ä½¿å…¶å·¥ä½œå¹¶äº§ç”Ÿåƒæœ¬ç« å¼€å¤´ç¤ºä¾‹é‚£æ ·çš„æ¸…æ™°ç»“æœã€‚ç½‘ä¸Šæœ‰å¾ˆå¤šæŠ€å·§å¯ä»¥å¸®åŠ©GANè®­ç»ƒè‰¯å¥½ï¼Œä½†å…¶ä¸­ä¸€äº›ä»…é€‚ç”¨äºç‰¹å®šç±»å‹çš„GANã€‚å…¶ä»–åˆ™æ›´å¯é ï¼Œé€‚ç”¨äºå¤šç§æ¶æ„ã€‚ä»¥ä¸‹æ˜¯æˆ‘å°è¯•æ„å»ºä»»ä½•GANæ—¶çš„å»ºè®®ï¼š
- en: Use the LeakyReLU activation function with a large leak value like *Î±* = 0.1
    or *Î±* = 0.2. It is extra important that our discriminator not have vanishing
    gradients, because the gradient for training G must first go all the way through
    D! The larger leak values help avoid this problem.
  id: totrans-75
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨å…·æœ‰å¤§æ³„æ¼å€¼ï¼ˆå¦‚ *Î±* = 0.1 æˆ– *Î±* = 0.2ï¼‰çš„LeakyReLUæ¿€æ´»å‡½æ•°ã€‚å¯¹äºæˆ‘ä»¬åˆ¤åˆ«å™¨æ²¡æœ‰æ¶ˆå¤±æ¢¯åº¦è¿™ä¸€ç‚¹å°¤ä¸ºé‡è¦ï¼Œå› ä¸ºè®­ç»ƒGçš„æ¢¯åº¦å¿…é¡»é¦–å…ˆå®Œå…¨é€šè¿‡Dï¼è¾ƒå¤§çš„æ³„æ¼å€¼æœ‰åŠ©äºé¿å…è¿™ä¸ªé—®é¢˜ã€‚
- en: Use LN instead of batch normalization (BN). Some have found that they get their
    best results with BN. In other cases we can *prove* that BN causes problems. So
    I prefer to start with LN, which has more consistent and reliable performance
    when training GANs. If Iâ€™m struggling to get that last ounce of performance, I
    try replacing LN with BN in *only* the generator.
  id: totrans-76
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨LNè€Œä¸æ˜¯æ‰¹å½’ä¸€åŒ–ï¼ˆBNï¼‰ã€‚æœ‰äº›äººå‘ç°ä»–ä»¬ç”¨BNå¯ä»¥å¾—åˆ°æœ€å¥½çš„ç»“æœã€‚åœ¨å…¶ä»–æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥ *è¯æ˜* BNä¼šå¯¼è‡´é—®é¢˜ã€‚å› æ­¤ï¼Œæˆ‘æ›´å–œæ¬¢ä»LNå¼€å§‹ï¼Œå®ƒåœ¨è®­ç»ƒGANæ—¶å…·æœ‰æ›´ä¸€è‡´å’Œå¯é çš„æ€§èƒ½ã€‚å¦‚æœæˆ‘åœ¨åŠªåŠ›è·å¾—æœ€åä¸€ä¸æ€§èƒ½æ—¶ï¼Œæˆ‘ä¼šå°è¯•ä»…åœ¨ç”Ÿæˆå™¨ä¸­å°†LNæ›¿æ¢ä¸ºBNã€‚
- en: Use the Adam optimizer specifically with the learning rate *Î·* = 0.0001 and
    *Î²*[1] = 0 and *Î²*[2] = 0.9. This is slower than Adamâ€™s normal defaults but works
    better for GANs. This is the last thing I would change to try to get better results.
  id: totrans-77
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ç‰¹å®šäºAdamä¼˜åŒ–å™¨çš„å­¦ä¹ ç‡ *Î·* = 0.0001 å’Œ *Î²*[1] = 0 ä»¥åŠ *Î²*[2] = 0.9ã€‚è¿™æ¯”Adamçš„æ­£å¸¸é»˜è®¤å€¼æ…¢ï¼Œä½†æ›´é€‚åˆGANã€‚è¿™æ˜¯æˆ‘æœ€åä¼šå°è¯•æ›´æ”¹ä»¥è·å¾—æ›´å¥½ç»“æœçš„äº‹æƒ…ã€‚
- en: Implementing the GAN training loop
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°GANè®­ç»ƒå¾ªç¯
- en: 'How do we train our two networks? Each network G and D has its own optimizer,
    and we will take turns using them! For this reason, we *do not* use the same `train_network`
    function that we have used for most of the book. This is one reason it is important
    to learn the framework and the tools it provides: not everything can be easily
    abstracted away and work with *every* type of neural network you might want to
    train in the future.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¦‚ä½•è®­ç»ƒæˆ‘ä»¬çš„ä¸¤ä¸ªç½‘ç»œï¼Ÿæ¯ä¸ªç½‘ç»œGå’ŒDéƒ½æœ‰è‡ªå·±çš„ä¼˜åŒ–å™¨ï¼Œæˆ‘ä»¬å°†è½®æµä½¿ç”¨å®ƒä»¬ï¼å› æ­¤ï¼Œæˆ‘ä»¬*ä¸*ä½¿ç”¨æˆ‘ä»¬åœ¨æœ¬ä¹¦çš„å¤§éƒ¨åˆ†å†…å®¹ä¸­ä½¿ç”¨çš„ç›¸åŒçš„`train_network`å‡½æ•°ã€‚è¿™æ˜¯å­¦ä¹ æ¡†æ¶åŠå…¶æä¾›çš„å·¥å…·å¾ˆé‡è¦çš„ä¸€ä¸ªåŸå› ï¼šå¹¶ä¸æ˜¯æ‰€æœ‰ä¸œè¥¿éƒ½å¯ä»¥è½»æ˜“æŠ½è±¡å‡ºæ¥ï¼Œå¹¶ä¸”ä¸*ä»»ä½•*ä½ æœªæ¥å¯èƒ½æƒ³è¦è®­ç»ƒçš„ç¥ç»ç½‘ç»œä¸€èµ·å·¥ä½œã€‚
- en: 'First letâ€™s do some setup. Letâ€™s move our models to the GPU, specify our binary
    cross-entropy loss function (because real versus fake is a binary problem), and
    set up our two different optimizers for each network:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œè®©æˆ‘ä»¬åšä¸€äº›è®¾ç½®ã€‚è®©æˆ‘ä»¬å°†æˆ‘ä»¬çš„æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Šï¼ŒæŒ‡å®šæˆ‘ä»¬çš„äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼ˆå› ä¸ºçœŸå®ä¸ä¼ªé€ æ˜¯ä¸€ä¸ªäºŒå…ƒé—®é¢˜ï¼‰ï¼Œå¹¶ä¸ºæ¯ä¸ªç½‘ç»œè®¾ç½®ä¸¤ä¸ªä¸åŒçš„ä¼˜åŒ–å™¨ï¼š
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: â¶ Initializes the BCEWithLogitsLoss function. The BCE loss is for binary classification
    problems, which ours is (real versus fake).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ åˆå§‹åŒ–BCEWithLogitsLosså‡½æ•°ã€‚BCEæŸå¤±ç”¨äºäºŒå…ƒåˆ†ç±»é—®é¢˜ï¼Œæˆ‘ä»¬çš„é—®é¢˜å°±æ˜¯è¿™æ ·ï¼ˆçœŸå®ä¸ä¼ªé€ ï¼‰ã€‚
- en: â· Establishes a convention for real and fake labels during training
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: â· åœ¨è®­ç»ƒæœŸé—´å»ºç«‹çœŸå®å’Œä¼ªé€ æ ‡ç­¾çš„çº¦å®š
- en: â¸ Sets up Adam optimizers for G and D
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ä¸ºGå’ŒDè®¾ç½®Adamä¼˜åŒ–å™¨
- en: 'Next we grab MNIST as our dataset. We wonâ€™t really use the test set in this
    chapter since we are focusing on generating new data:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬æŠ“å–MNISTä½œä¸ºæˆ‘ä»¬çš„æ•°æ®é›†ã€‚åœ¨æœ¬ç« ä¸­ï¼Œæˆ‘ä»¬ä¸ä¼šçœŸæ­£ä½¿ç”¨æµ‹è¯•é›†ï¼Œå› ä¸ºæˆ‘ä»¬ä¸“æ³¨äºç”Ÿæˆæ–°çš„æ•°æ®ï¼š
- en: '[PRE4]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Now we need to train this GAN. We break the process into two steps, one for
    D and one for G, as shown in figure 9.6.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦è®­ç»ƒè¿™ä¸ªGANã€‚æˆ‘ä»¬å°†è¿™ä¸ªè¿‡ç¨‹åˆ†ä¸ºä¸¤ä¸ªæ­¥éª¤ï¼Œä¸€ä¸ªç”¨äºDï¼Œä¸€ä¸ªç”¨äºGï¼Œå¦‚å›¾9.6æ‰€ç¤ºã€‚
- en: '![](../Images/CH09_F06_Raff.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F06_Raff.png)'
- en: Figure 9.6 Steps for training the GAN. For several epochs, we run both steps.
    In step 1, we compute the discriminator loss on real and fake data, then update
    the weights of D using its optimizer. In step 2, we update the generator. Since
    both steps need an output from G, we reuse *G*(**z**) between the two steps.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.6 GANçš„è®­ç»ƒæ­¥éª¤ã€‚åœ¨å‡ ä¸ªepochä¸­ï¼Œæˆ‘ä»¬è¿è¡Œè¿™ä¸¤ä¸ªæ­¥éª¤ã€‚åœ¨ç¬¬1æ­¥ä¸­ï¼Œæˆ‘ä»¬åœ¨çœŸå®å’Œä¼ªé€ æ•°æ®ä¸Šè®¡ç®—åˆ¤åˆ«å™¨æŸå¤±ï¼Œç„¶åä½¿ç”¨å…¶ä¼˜åŒ–å™¨æ›´æ–°Dçš„æƒé‡ã€‚åœ¨ç¬¬2æ­¥ä¸­ï¼Œæˆ‘ä»¬æ›´æ–°ç”Ÿæˆå™¨ã€‚ç”±äºè¿™ä¸¤ä¸ªæ­¥éª¤éƒ½éœ€è¦Gçš„è¾“å‡ºï¼Œæˆ‘ä»¬åœ¨ä¸¤ä¸ªæ­¥éª¤ä¹‹é—´é‡ç”¨*G*(**z**)ã€‚
- en: 'Hopefully, by this point you are starting to feel comfortable mixing code and
    math notation. This diagram specifies almost every detail we need to successfully
    implement a GAN! Letâ€™s translate it into complete code. First we have the `for`
    loops and some setup. We use two arrays to store the loss at each batch so we
    can look at it after training. The loss function can be particularly informative
    for GANs. We also create *y*[real] and *y*[fake], which that are used in both
    steps and move the data onto the right device. Thatâ€™s shown in the following code:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: å¸Œæœ›åˆ°è¿™ä¸€ç‚¹ï¼Œä½ å·²ç»å¼€å§‹æ„Ÿåˆ°èˆ’é€‚åœ°æ··åˆä»£ç å’Œæ•°å­¦ç¬¦å·ã€‚è¿™å¼ å›¾å‡ ä¹åˆ—å‡ºäº†æˆ‘ä»¬æˆåŠŸå®ç°ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰æ‰€éœ€çš„æ‰€æœ‰ç»†èŠ‚ï¼è®©æˆ‘ä»¬å°†å…¶ç¿»è¯‘æˆå®Œæ•´çš„ä»£ç ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬æœ‰`for`å¾ªç¯å’Œä¸€äº›è®¾ç½®ã€‚æˆ‘ä»¬ä½¿ç”¨ä¸¤ä¸ªæ•°ç»„æ¥å­˜å‚¨æ¯ä¸ªæ‰¹æ¬¡çš„æŸå¤±ï¼Œä»¥ä¾¿åœ¨è®­ç»ƒåæŸ¥çœ‹ã€‚æŸå¤±å‡½æ•°å¯¹äºGANæ¥è¯´å¯ä»¥ç‰¹åˆ«æœ‰ä¿¡æ¯é‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†*y*[çœŸå®]å’Œ*y*[ä¼ªé€ ]ï¼Œè¿™ä¸¤ä¸ªå˜é‡åœ¨ä¸¤ä¸ªæ­¥éª¤ä¸­éƒ½ä¼šä½¿ç”¨ï¼Œå¹¶å°†æ•°æ®ç§»åŠ¨åˆ°æ­£ç¡®çš„è®¾å¤‡ä¸Šã€‚ä»¥ä¸‹ä»£ç å±•ç¤ºäº†è¿™ä¸€ç‚¹ï¼š
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: â¶ Preps the batch and makes labels
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ å‡†å¤‡æ‰¹æ¬¡å¹¶åˆ¶ä½œæ ‡ç­¾
- en: 'Now we can move on to step 1: updating the discriminator. Implementing this
    does not take much work. We call `backward` after each error component is computed,
    as a mild efficiency optimization. But we still add the errors together later
    to save the combined error. The big trick here that you should pay attention to
    is the use of `fake.detach()` when passing the fake image into D. The `detach()`
    method returns a new version of the same object that will not pass a gradient
    back any further. We do this because the `fake` object was computed using `G`,
    so naively using the `fake` object in our calculation would cause us to input
    a gradient for `G` that *benefits the discriminator* because we are computing
    the discriminatorâ€™s loss! Since step 1 is only supposed to alter `D` and a gradient
    for `G` at this point would be counterproductive to `G`â€™s goals (it wants to beat
    the discriminator!), we call `.detach()` so `G` does not get any gradient.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥è¿›è¡Œç¬¬ä¸€æ­¥ï¼šæ›´æ–°åˆ¤åˆ«å™¨ã€‚å®ç°è¿™ä¸€ç‚¹ä¸éœ€è¦åšå¤ªå¤šå·¥ä½œã€‚æˆ‘ä»¬è®¡ç®—å®Œæ¯ä¸ªé”™è¯¯ç»„ä»¶åè°ƒç”¨ `backward`ï¼Œä½œä¸ºä¸€ä¸ªè½»å¾®çš„æ•ˆç‡ä¼˜åŒ–ã€‚ä½†æˆ‘ä»¬ä»ç„¶ç¨åæŠŠé”™è¯¯åŠ åœ¨ä¸€èµ·ä»¥ä¿å­˜ç»„åˆé”™è¯¯ã€‚è¿™é‡Œçš„ä¸€ä¸ªå¤§æŠ€å·§æ˜¯ä½ åº”è¯¥æ³¨æ„åˆ°çš„ï¼Œå°±æ˜¯åœ¨å°†å‡å›¾åƒä¼ é€’ç»™
    D æ—¶ä½¿ç”¨ `fake.detach()`ã€‚`detach()` æ–¹æ³•è¿”å›ä¸€ä¸ªç›¸åŒå¯¹è±¡çš„æ–°ç‰ˆæœ¬ï¼Œè¯¥ç‰ˆæœ¬ä¸ä¼šè¿›ä¸€æ­¥ä¼ é€’æ¢¯åº¦ã€‚æˆ‘ä»¬è¿™æ ·åšæ˜¯å› ä¸º `fake` å¯¹è±¡æ˜¯ä½¿ç”¨
    `G` è®¡ç®—çš„ï¼Œæ‰€ä»¥å¤©çœŸåœ°ä½¿ç”¨ `fake` å¯¹è±¡åœ¨æˆ‘ä»¬çš„è®¡ç®—ä¸­ä¼šå¯¼è‡´æˆ‘ä»¬è¾“å…¥ä¸€ä¸ªå¯¹ `G` æœ‰åˆ©çš„æ¢¯åº¦ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨è®¡ç®—åˆ¤åˆ«å™¨çš„æŸå¤±ï¼ç”±äºç¬¬ä¸€æ­¥åªæ˜¯åº”è¯¥æ”¹å˜
    `D` å’Œæ­¤æ—¶çš„ `G` æ¢¯åº¦å¯¹ `G` çš„ç›®æ ‡ï¼ˆå®ƒæƒ³æ‰“è´¥åˆ¤åˆ«å™¨ï¼ï¼‰æ˜¯æœ‰å®³çš„ï¼Œæ‰€ä»¥æˆ‘ä»¬è°ƒç”¨ `.detach()` ä»¥ç¡®ä¿ `G` ä¸è·å¾—ä»»ä½•æ¢¯åº¦ã€‚
- en: 'Hereâ€™s the code:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™é‡Œæ˜¯ä»£ç ï¼š
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: â¶ Real data
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ çœŸå®æ•°æ®
- en: â· Trains with an all-fake batch and generates a batch of latent vectors *z*
    âˆ¼ ğ’©(![](../Images/vec_0.png), 1).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: â· ä½¿ç”¨å…¨å‡æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒå¹¶ç”Ÿæˆä¸€ä¸ªæ½œåœ¨å‘é‡æ‰¹æ¬¡ *z* âˆ¼ ğ’©(![](../Images/vec_0.png), 1)ã€‚
- en: â¸ Generates a fake image batch with G and classifies the fake batch with D.
    We save this to reuse in step 2.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ä½¿ç”¨ G ç”Ÿæˆä¸€ä¸ªå‡å›¾åƒæ‰¹æ¬¡ï¼Œå¹¶ä½¿ç”¨ D å¯¹å‡æ‰¹æ¬¡è¿›è¡Œåˆ†ç±»ã€‚æˆ‘ä»¬å°†æ­¤ä¿å­˜ä»¥åœ¨ç¬¬äºŒæ­¥ä¸­é‡ç”¨ã€‚
- en: â¹ **x**[fake] = *G*(**z**)
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ **x**[fake] = *G*(**z**)
- en: âº Calculate Dâ€™s loss on the all-fake batch; note the use of fake.detach(). â„“(*D*(*x*[real]),*y*[real])
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: âº åœ¨å…¨å‡æ‰¹æ¬¡ä¸Šè®¡ç®— D çš„æŸå¤±ï¼›æ³¨æ„ä½¿ç”¨ fake.detach()ã€‚â„“(*D*(*x*[real]),*y*[real])
- en: â» Calculates the gradients for this batch
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: â» è®¡ç®—è¯¥æ‰¹æ¬¡çš„æ¢¯åº¦
- en: â¼ Adds the gradients from the all-real and all-fake batches
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: â¼ å°†å…¨çœŸå®å’Œå…¨å‡æ‰¹æ¬¡çš„æ¢¯åº¦ç›¸åŠ 
- en: â½ Updates D
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: â½ æ›´æ–° D
- en: 'The final part of our loop body is step 2: updating the generator G. We reuse
    the `fake` object so we donâ€™t have to create a new one, which saves us time. Since
    we also *want* to alter G, we use the original `fake` object directlyâ€”no call
    to `.fake()`. Very little code is needed here, and we append the errors of G and
    D into a list so we can plot them afterward:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¾ªç¯ä½“çš„æœ€åä¸€éƒ¨åˆ†æ˜¯ç¬¬äºŒæ­¥ï¼šæ›´æ–°ç”Ÿæˆå™¨ Gã€‚æˆ‘ä»¬é‡ç”¨ `fake` å¯¹è±¡ï¼Œè¿™æ ·æˆ‘ä»¬å°±ä¸éœ€è¦åˆ›å»ºä¸€ä¸ªæ–°çš„å¯¹è±¡ï¼Œè¿™å¯ä»¥èŠ‚çœæˆ‘ä»¬çš„æ—¶é—´ã€‚ç”±äºæˆ‘ä»¬è¿˜æƒ³æ”¹å˜
    Gï¼Œæ‰€ä»¥æˆ‘ä»¬ç›´æ¥ä½¿ç”¨åŸå§‹çš„ `fake` å¯¹è±¡â€”â€”æ²¡æœ‰è°ƒç”¨ `.fake()`ã€‚è¿™é‡Œéœ€è¦çš„ä»£ç éå¸¸å°‘ï¼Œæˆ‘ä»¬å°† G å’Œ D çš„é”™è¯¯è¿½åŠ åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œä»¥ä¾¿ç¨åç»˜åˆ¶å®ƒä»¬ï¼š
- en: '[PRE7]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'â¶ Calculates Gâ€™s loss based on this output: â„“(*D*(**x**[fake]),Â *y*[real])'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ åŸºäºæ­¤è¾“å‡ºè®¡ç®— G çš„æŸå¤±ï¼šâ„“(*D*(**x**[fake]),Â *y*[real])
- en: â· Calculates gradients for G
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: â· ä¸º G è®¡ç®—æ¢¯åº¦
- en: â¸ Updates G
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ æ›´æ–° G
- en: Inspecting the results
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: æ£€æŸ¥ç»“æœ
- en: 'Running that code successfully trains a GAN. Because the latents z come from
    a Gaussian distribution (**z** âˆ¼ ğ’©(![](../Images/vec_0.png), **I** )  , we can
    easily sample them and compute *G*(*z*) to get synthetic data. We can also look
    at the value of *D*(*G*(*z*)) to see what the discriminator thinks about each
    sampleâ€™s realism. The way we trained the model, a value of 1 would indicate that
    the discriminator thinks the input is definitely real, and 0 would indicate that
    the discriminator thinks it is definitely fake. The following code samples some
    new latents into a variable called `noise`, which is another common name you will
    see for the latent object in a GAN. We make our fake digits with G and compute
    the scores for how real they look with D:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: è¿è¡Œè¯¥ä»£ç æˆåŠŸè®­ç»ƒäº†ä¸€ä¸ª GANã€‚å› ä¸ºæ½œåœ¨çš„ z æ¥è‡ªé«˜æ–¯åˆ†å¸ƒï¼ˆ**z** âˆ¼ ğ’©(![](../Images/vec_0.png), **I** )
    ï¼‰ï¼Œæˆ‘ä»¬å¯ä»¥è½»æ¾åœ°é‡‡æ ·å®ƒä»¬å¹¶è®¡ç®— *G*(*z*) æ¥è·å–åˆæˆæ•°æ®ã€‚æˆ‘ä»¬è¿˜å¯ä»¥æŸ¥çœ‹ *D*(*G*(*z*)) çš„å€¼ï¼Œä»¥äº†è§£åˆ¤åˆ«å™¨å¯¹æ¯ä¸ªæ ·æœ¬çœŸå®æ€§çš„çœ‹æ³•ã€‚æˆ‘ä»¬è®­ç»ƒæ¨¡å‹çš„æ–¹å¼æ˜¯ï¼Œå€¼ä¸º
    1 è¡¨ç¤ºåˆ¤åˆ«å™¨è®¤ä¸ºè¾“å…¥è‚¯å®šæ˜¯çœŸå®çš„ï¼Œè€Œ 0 è¡¨ç¤ºåˆ¤åˆ«å™¨è®¤ä¸ºå®ƒè‚¯å®šæ˜¯å‡çš„ã€‚ä»¥ä¸‹ä»£ç å°†ä¸€äº›æ–°çš„æ½œåœ¨å€¼é‡‡æ ·åˆ°ä¸€ä¸ªåä¸º `noise` çš„å˜é‡ä¸­ï¼Œè¿™æ˜¯ä½ åœ¨ GAN
    ä¸­å°†çœ‹åˆ°çš„å¦ä¸€ä¸ªå¸¸è§åç§°ã€‚æˆ‘ä»¬ç”¨ G ç”Ÿæˆå‡æ•°å­—å¹¶è®¡ç®—å®ƒä»¬çœ‹èµ·æ¥æœ‰å¤šçœŸå®çš„åˆ†æ•°ï¼š
- en: '[PRE8]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: â¶ **z** âˆ¼ ğ’©(![](../Images/vec_0.png), **I**)
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ **z** âˆ¼ ğ’©(![](../Images/vec_0.png), **I**)
- en: 'Next is some Matplotlib code to plot all of the generated images, with the
    score in red above each digit. The plot automatically resizes based on the batch
    size we are using. The code quickly computes the largest square of images that
    can be filled from the given batch. We make `scores` an optional argument, as
    our future GANs wonâ€™t have the same kind of score:'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯ä¸€æ®µMatplotlibä»£ç ï¼Œç”¨äºç»˜åˆ¶æ‰€æœ‰ç”Ÿæˆçš„å›¾åƒï¼Œæ¯ä¸ªæ•°å­—ä¸Šæ–¹éƒ½æœ‰çº¢è‰²çš„åˆ†æ•°ã€‚è¯¥å›¾ä¼šæ ¹æ®æˆ‘ä»¬ä½¿ç”¨çš„æ‰¹é‡å¤§å°è‡ªåŠ¨è°ƒæ•´å¤§å°ã€‚è¯¥ä»£ç å¿«é€Ÿè®¡ç®—å¯ä»¥ä»ç»™å®šæ‰¹æ¬¡å¡«å……çš„æœ€å¤§æ­£æ–¹å½¢å›¾åƒã€‚æˆ‘ä»¬å°†`scores`ä½œä¸ºä¸€ä¸ªå¯é€‰å‚æ•°ï¼Œå› ä¸ºæˆ‘ä»¬çš„æœªæ¥GANså¯èƒ½ä¸ä¼šæœ‰è¿™ç§ç±»å‹çš„åˆ†æ•°ï¼š
- en: '[PRE9]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: â¶ This code assumes we are working with black-and-white images.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ æ­¤ä»£ç å‡è®¾æˆ‘ä»¬æ­£åœ¨å¤„ç†é»‘ç™½å›¾åƒã€‚
- en: '![](../Images/CH09_UN04_Raff.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN04_Raff.png)'
- en: Weâ€™ve generated synthetic data, and some of it looks quite good! Some of the
    samples donâ€™t look realistic, which is OK. The problem of learning a generative
    model is intrinsically more challenging than learning a discriminative model.
    If you run this code again, you should get new results and start noticing some
    issues.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»ç”Ÿæˆäº†åˆæˆæ•°æ®ï¼Œå…¶ä¸­ä¸€äº›çœ‹èµ·æ¥ç›¸å½“ä¸é”™ï¼ä¸€äº›æ ·æœ¬çœ‹èµ·æ¥ä¸å¤ªçœŸå®ï¼Œè¿™æ˜¯å¯ä»¥æ¥å—çš„ã€‚å­¦ä¹ ç”Ÿæˆæ¨¡å‹çš„é—®é¢˜æœ¬è´¨ä¸Šæ¯”å­¦ä¹ åˆ¤åˆ«æ¨¡å‹æ›´å…·æŒ‘æˆ˜æ€§ã€‚å¦‚æœä½ å†æ¬¡è¿è¡Œæ­¤ä»£ç ï¼Œä½ åº”è¯¥ä¼šå¾—åˆ°æ–°çš„ç»“æœå¹¶å¼€å§‹æ³¨æ„åˆ°ä¸€äº›é—®é¢˜ã€‚
- en: The first pattern you might notice is that the generator likes a few digits
    more than others. When I run this, I usually see a lot of 0s, 3s, and 8s generated.
    Other digits are much rarer. Second, the discriminator is almost always correct
    in calling the generated samples fake. These two issues are related.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ å¯èƒ½ä¼šæ³¨æ„åˆ°çš„ç¬¬ä¸€ä¸ªæ¨¡å¼æ˜¯ç”Ÿæˆå™¨æ›´å–œæ¬¢ä¸€äº›æ•°å­—è€Œä¸æ˜¯å…¶ä»–æ•°å­—ã€‚å½“æˆ‘è¿è¡Œè¿™ä¸ªæ—¶ï¼Œæˆ‘é€šå¸¸çœ‹åˆ°å¾ˆå¤š0sã€3så’Œ8sè¢«ç”Ÿæˆã€‚å…¶ä»–æ•°å­—éå¸¸ç½•è§ã€‚ç¬¬äºŒï¼Œåˆ¤åˆ«å™¨å‡ ä¹æ€»æ˜¯æ­£ç¡®åœ°ç§°ç”Ÿæˆçš„æ ·æœ¬ä¸ºä¼ªé€ çš„ã€‚è¿™ä¸¤ä¸ªé—®é¢˜ç›¸å…³ã€‚
- en: The repetition of a few digits is a problem because it means our generator is
    not modeling the *whole* distribution. This makes its outputs as a whole less
    realistic, even if individual digits look good. The discriminator being very good
    at detecting fakes is a potential problem because we use the binary cross entropy
    (BCE) loss! Since BCE involves a sigmoid, if D gets *too* good and predicts 0%
    that a sample is malicious, it will cause a vanishing gradient for G. This is
    because Gâ€™s gradient comes from the opposite of Dâ€™s predictions. If D does a perfect
    job predicting, G canâ€™t learn.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: å‡ ä¸ªæ•°å­—çš„é‡å¤æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºè¿™æ„å‘³ç€æˆ‘ä»¬çš„ç”Ÿæˆå™¨æ²¡æœ‰å¯¹æ•´ä¸ªåˆ†å¸ƒè¿›è¡Œå»ºæ¨¡ã€‚è¿™ä½¿å¾—å…¶æ•´ä½“è¾“å‡ºä¸å¤ªçœŸå®ï¼Œå³ä½¿å•ä¸ªæ•°å­—çœ‹èµ·æ¥ä¸é”™ã€‚åˆ¤åˆ«å™¨åœ¨æ£€æµ‹ä¼ªé€ æ–¹é¢éå¸¸å‡ºè‰²æ˜¯ä¸€ä¸ªæ½œåœ¨é—®é¢˜ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†äºŒå…ƒäº¤å‰ç†µï¼ˆBCEï¼‰æŸå¤±ï¼ç”±äºBCEæ¶‰åŠsigmoidï¼Œå¦‚æœDå˜å¾—éå¸¸å¥½å¹¶é¢„æµ‹æ ·æœ¬æ˜¯æ¶æ„çš„æ¦‚ç‡ä¸º0%ï¼Œè¿™å°†å¯¼è‡´Gçš„æ¢¯åº¦æ¶ˆå¤±ã€‚è¿™æ˜¯å› ä¸ºGçš„æ¢¯åº¦æ¥è‡ªDé¢„æµ‹çš„ç›¸åæ–¹å‘ã€‚å¦‚æœDé¢„æµ‹å¾—éå¸¸å®Œç¾ï¼ŒGå°±æ— æ³•å­¦ä¹ ã€‚
- en: 'The risk that D might win the game with perfect predictions is important because
    the game between D and G is unfair. What do I mean by unfair? Letâ€™s look at the
    loss of the discriminator and generator over the training. We can quickly plot
    the loss for G and D and compare them to see how they are doing:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Då¯èƒ½ä¼šé€šè¿‡å®Œç¾çš„é¢„æµ‹èµ¢å¾—æ¸¸æˆçš„å±é™©å¾ˆé‡è¦ï¼Œå› ä¸ºDå’ŒGä¹‹é—´çš„æ¸¸æˆæ˜¯ä¸å…¬å¹³çš„ã€‚æˆ‘è¯´çš„ä¸å…¬å¹³æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿè®©æˆ‘ä»¬çœ‹çœ‹åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ã€‚æˆ‘ä»¬å¯ä»¥å¿«é€Ÿç»˜åˆ¶Gå’ŒDçš„æŸå¤±ï¼Œå¹¶å°†å®ƒä»¬è¿›è¡Œæ¯”è¾ƒï¼Œçœ‹çœ‹å®ƒä»¬çš„è¿›å±•æƒ…å†µï¼š
- en: '[PRE10]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '![](../Images/CH09_UN05_Raff.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN05_Raff.png)'
- en: While the generator has improved significantly, it always does worse than D.
    The discriminator starts at and stays near a loss of zero the entire time because
    it has an intrinsically easier problem to solve. Discrimination is always easier
    than generation! Thus, it becomes easy for D to win the game against the generator
    G. This is part of why the generator tends to focus on a few digits and ignore
    the rest. Early on, the generator G finds that some digits are easier to trick
    D with than others. There is no penalty for G other than fooling D, so it puts
    all its effort into whatever seems to be working best.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: å°½ç®¡ç”Ÿæˆå™¨å·²ç»æ˜¾è‘—æ”¹è¿›ï¼Œä½†å®ƒå§‹ç»ˆæ¯”Dè¡¨ç°å·®ã€‚åˆ¤åˆ«å™¨åœ¨æ•´ä¸ªè¿‡ç¨‹ä¸­å§‹ç»ˆä¿æŒåœ¨é›¶æŸå¤±é™„è¿‘ï¼Œå› ä¸ºå®ƒæœ‰ä¸€ä¸ªæœ¬è´¨ä¸Šæ›´å®¹æ˜“è§£å†³çš„é—®é¢˜ã€‚åˆ¤åˆ«æ€»æ˜¯æ¯”ç”Ÿæˆæ›´å®¹æ˜“ï¼å› æ­¤ï¼ŒDå¾ˆå®¹æ˜“åœ¨æ¸¸æˆä¸­å¯¹ç”Ÿæˆå™¨Gå–å¾—èƒœåˆ©ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆç”Ÿæˆå™¨å€¾å‘äºå…³æ³¨å‡ ä¸ªæ•°å­—è€Œå¿½ç•¥å…¶ä»–æ•°å­—çš„éƒ¨åˆ†åŸå› ã€‚æ—©æœŸï¼Œç”Ÿæˆå™¨Gå‘ç°ä¸€äº›æ•°å­—æ¯”å…¶ä»–æ•°å­—æ›´å®¹æ˜“ç”¨å®ƒä»¬æ¥æ¬ºéª—Dã€‚é™¤äº†æ¬ºéª—Dä¹‹å¤–ï¼ŒGæ²¡æœ‰å…¶ä»–æƒ©ç½šï¼Œæ‰€ä»¥å®ƒå°†æ‰€æœ‰åŠªåŠ›éƒ½æŠ•å…¥åˆ°ä¼¼ä¹æ•ˆæœæœ€å¥½çš„ä»»ä½•äº‹ç‰©ä¸Šã€‚
- en: Why is discrimination easier than generation?
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆåˆ¤åˆ«æ¯”ç”Ÿæˆæ›´å®¹æ˜“ï¼Ÿ
- en: 'Letâ€™s take a quick detour, which you can skip if you are happy to simply trust
    that discrimination is easier. A little math will show *why* it is easier. Letâ€™s
    start with a quick refresher on Bayesâ€™s rule:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¿«é€Ÿåç¦»ä¸€ä¸‹ï¼Œå¦‚æœä½ æ„¿æ„ç®€å•åœ°ç›¸ä¿¡åˆ¤åˆ«æ¯”ç”Ÿæˆæ›´å®¹æ˜“ï¼Œä½ å¯ä»¥è·³è¿‡è¿™ä¸€éƒ¨åˆ†ã€‚ä¸€ç‚¹æ•°å­¦å°†å±•ç¤ºä¸ºä»€ä¹ˆå®ƒæ›´å®¹æ˜“ã€‚è®©æˆ‘ä»¬ä»Bayeså®šç†çš„å¿«é€Ÿå¤ä¹ å¼€å§‹ï¼š
- en: '![](../Images/CH09_UN06_Raff.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN06_Raff.png)'
- en: If we wanted to write out the problem in terms of statistics, the generator
    G is trying to learn what is called the *joint distribution*. There is real data
    x with labels y, and the joint distribution is denoted as â„™(**x**,*y*). If you
    can sample from the joint distribution, you have a generative model. If it is
    doing a good job, you should get all the different classes that y might indicate
    exist.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬æƒ³ç”¨ç»Ÿè®¡æœ¯è¯­æ¥æè¿°è¿™ä¸ªé—®é¢˜ï¼Œç”Ÿæˆå™¨ G æ­£åœ¨å°è¯•å­¦ä¹ è¢«ç§°ä¸º*è”åˆåˆ†å¸ƒ*çš„ä¸œè¥¿ã€‚å­˜åœ¨çœŸå®æ•°æ® x å’Œæ ‡ç­¾ yï¼Œè”åˆåˆ†å¸ƒè¡¨ç¤ºä¸º â„™(**x**,*y*ï¼‰ã€‚å¦‚æœä½ å¯ä»¥ä»è”åˆåˆ†å¸ƒä¸­é‡‡æ ·ï¼Œä½ å°±æœ‰äº†ä¸€ä¸ªç”Ÿæˆæ¨¡å‹ã€‚å¦‚æœå®ƒåšå¾—å¾ˆå¥½ï¼Œä½ åº”è¯¥å¾—åˆ°
    y å¯èƒ½æŒ‡ç¤ºå­˜åœ¨çš„æ‰€æœ‰ä¸åŒç±»åˆ«ã€‚
- en: The discriminatorâ€™s task is a conditional distribution that we write as â„™(*y*âˆ£**x**).
    The way to read this is "what is the probability (â„™) of the label y given (âˆ£)
    the inputs **x**).?â€
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨çš„ä»»åŠ¡æ˜¯æ¡ä»¶åˆ†å¸ƒï¼Œæˆ‘ä»¬å°†å…¶å†™ä½œ â„™(*y*âˆ£**x**ï¼‰ã€‚é˜…è¯»è¿™ä¸ªè¡¨è¾¾å¼çš„æ–¹å¼æ˜¯ï¼šâ€œç»™å®šè¾“å…¥ **x**ï¼Œæ ‡ç­¾ y çš„æ¦‚ç‡ï¼ˆâ„™ï¼‰æ˜¯å¤šå°‘ï¼Ÿâ€
- en: 'If we already have the joint distribution, we can immediately recover the conditional
    distribution with one application of Bayesâ€™s rule:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å·²ç»æœ‰è”åˆåˆ†å¸ƒï¼Œæˆ‘ä»¬å¯ä»¥ç«‹å³é€šè¿‡åº”ç”¨è´å¶æ–¯å®šç†æ¢å¤æ¡ä»¶åˆ†å¸ƒï¼š
- en: '![](../Images/ch9-eqs-to-illustrator4xa.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch9-eqs-to-illustrator4xa.png)'
- en: The bottom missing term is not an issue because it can be computed as â„™(**x**)
    = âˆ‘[*y*âˆˆ*Y*] â„™(**x**,*y*). So if we know G, we get a discriminator D for free!
    But there is no way to rearrange this equation to get G from D without having
    *something extra*. The problem of generation (joint distributions) is fundamentally
    more information and thus more difficult than discrimination (conditional distributions).
    This is why D has an easier task than G.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: åº•éƒ¨ç¼ºå¤±çš„é¡¹ä¸æ˜¯é—®é¢˜ï¼Œå› ä¸ºå®ƒå¯ä»¥è®¡ç®—ä¸º â„™(**x**) = âˆ‘[*y*âˆˆ*Y*] â„™(**x**,*y*ï¼‰ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬çŸ¥é“ Gï¼Œæˆ‘ä»¬å°±å¯ä»¥å…è´¹å¾—åˆ°ä¸€ä¸ªåˆ¤åˆ«å™¨
    Dï¼ä½†æ˜¯æ²¡æœ‰æ–¹æ³•å¯ä»¥é‡æ–°æ’åˆ—è¿™ä¸ªæ–¹ç¨‹ï¼Œä» D ä¸­å¾—åˆ° G è€Œä¸æ·»åŠ â€œé¢å¤–çš„ä¸œè¥¿â€ã€‚ç”Ÿæˆï¼ˆè”åˆåˆ†å¸ƒï¼‰çš„é—®é¢˜æœ¬è´¨ä¸Šéœ€è¦æ›´å¤šä¿¡æ¯ï¼Œå› æ­¤æ¯”åˆ¤åˆ«ï¼ˆæ¡ä»¶åˆ†å¸ƒï¼‰æ›´éš¾ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ
    D çš„ä»»åŠ¡æ¯” G æ›´å®¹æ˜“ã€‚
- en: 9.2 Mode collapse
  id: totrans-132
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 æ¨¡å¼åå¡Œ
- en: The issue with our generator only producing some of the digits is a phenomenon
    called *mode collapse*. Because the generator has a harder problem to solve, it
    tries to find ways to cheat at the game. One way to cheat is to generate only
    the data that is *easy* and ignore other cases. If the generator can produce a
    perfect 0 every time that fools the discriminator, it will win the gameâ€”even if
    it canâ€™t generate any other digits! To make a better generator, we need to address
    this issue; in this section, we do another experiment to better understand it.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”Ÿæˆå™¨åªç”Ÿæˆä¸€äº›æ•°å­—çš„é—®é¢˜æ˜¯ä¸€ä¸ªç§°ä¸º*æ¨¡å¼åå¡Œ*çš„ç°è±¡ã€‚å› ä¸ºç”Ÿæˆå™¨æœ‰ä¸€ä¸ªæ›´éš¾è§£å†³çš„é—®é¢˜ï¼Œå®ƒè¯•å›¾æ‰¾åˆ°åœ¨æ¸¸æˆä¸­ä½œå¼Šçš„æ–¹æ³•ã€‚ä¸€ç§ä½œå¼Šçš„æ–¹æ³•æ˜¯åªç”Ÿæˆ*å®¹æ˜“*çš„æ•°æ®ï¼Œå¿½ç•¥å…¶ä»–æƒ…å†µã€‚å¦‚æœç”Ÿæˆå™¨æ¯æ¬¡éƒ½èƒ½ç”Ÿæˆå®Œç¾çš„
    0 æ¥æ¬ºéª—åˆ¤åˆ«å™¨ï¼Œå®ƒå°±ä¼šèµ¢å¾—æ¸¸æˆâ€”â€”å³ä½¿å®ƒä¸èƒ½ç”Ÿæˆä»»ä½•å…¶ä»–æ•°å­—ï¼ä¸ºäº†åˆ¶ä½œæ›´å¥½çš„ç”Ÿæˆå™¨ï¼Œæˆ‘ä»¬éœ€è¦è§£å†³è¿™ä¸ªé—®é¢˜ï¼›åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬è¿›è¡Œå¦ä¸€ä¸ªå®éªŒæ¥æ›´å¥½åœ°ç†è§£å®ƒã€‚
- en: 'The easiest data points are usually associated with a *mode* of the dataâ€™s
    distribution. The mode is the most common value found in a distribution. We use
    it to help understand the mode-collapse problem. To do this, we generate a grid
    of Gaussian variables. The following code does that such that the Gaussians are
    centered at zero:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€å®¹æ˜“çš„æ•°æ®ç‚¹é€šå¸¸ä¸æ•°æ®çš„*æ¨¡å¼*ç›¸å…³è”ã€‚æ¨¡å¼æ˜¯åœ¨åˆ†å¸ƒä¸­æ‰¾åˆ°çš„æœ€å¸¸è§å€¼ã€‚æˆ‘ä»¬ç”¨å®ƒæ¥å¸®åŠ©ç†è§£æ¨¡å¼åå¡Œé—®é¢˜ã€‚ä¸ºæ­¤ï¼Œæˆ‘ä»¬ç”Ÿæˆä¸€ä¸ªé«˜æ–¯å˜é‡çš„ç½‘æ ¼ã€‚ä»¥ä¸‹ä»£ç æ˜¯è¿™æ ·åšçš„ï¼Œä½¿å¾—é«˜æ–¯å˜é‡ä»¥é›¶ä¸ºä¸­å¿ƒï¼š
- en: '[PRE11]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: â¶ How large should the grid be?
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç½‘æ ¼åº”è¯¥æœ‰å¤šå¤§ï¼Ÿ
- en: â· How many samples per item in the grid?
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: â· ç½‘æ ¼ä¸­æ¯ä¸ªé¡¹ç›®çš„æ ·æœ¬æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ
- en: 'Next is some quick code that loops through every item in the grid and computes
    some samples. We use a small standard deviation in our samples so it is easy to
    see that there are nine well-separated modes:'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥æ˜¯ä¸€äº›å¿«é€Ÿä»£ç ï¼Œå®ƒéå†ç½‘æ ¼ä¸­çš„æ¯ä¸ªé¡¹ç›®å¹¶è®¡ç®—ä¸€äº›æ ·æœ¬ã€‚æˆ‘ä»¬åœ¨æ ·æœ¬ä¸­ä½¿ç”¨å°çš„æ ‡å‡†å·®ï¼Œè¿™æ ·å°±å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°å­˜åœ¨ä¹ä¸ªåˆ†ç¦»è‰¯å¥½çš„æ¨¡å¼ï¼š
- en: '[PRE12]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: â¶ We store all the data in here.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ æˆ‘ä»¬åœ¨è¿™é‡Œå­˜å‚¨æ‰€æœ‰æ•°æ®ã€‚
- en: â· These two loops go to the centers of each mean.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: â· è¿™ä¸¤ä¸ªå¾ªç¯åˆ°è¾¾æ¯ä¸ªå‡å€¼çš„ä¸­å¿ƒã€‚
- en: â¸ Samples a bunch of tightly clustered points
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ æ ·æœ¬äº†ä¸€ç»„ç´§å¯†èšé›†çš„ç‚¹
- en: â¹ Shifts this random sample to have a specific x-axis position
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ å°†è¿™ä¸ªéšæœºæ ·æœ¬åç§»åˆ°ç‰¹å®šçš„ x è½´ä½ç½®
- en: âº Shifts on the y-axis
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: âº y è½´ä¸Šçš„åç§»
- en: â» Collects all the samples together
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: â» æ”¶é›†æ‰€æœ‰æ ·æœ¬
- en: â¼ Converts this list into one large NumPy tensor of shape (N, 2)
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: â¼ å°†è¿™ä¸ªåˆ—è¡¨è½¬æ¢æˆä¸€ä¸ªå½¢çŠ¶ä¸º (N, 2) çš„å¤§å‹ NumPy å¼ é‡
- en: 'Finally we can plot these samples! We use a kernel density estimate (kde) plot
    that smooths the visual of the 2D grid. We have enough samples that each mode
    looks *perfect*, and we can clearly see nine modes in a nice grid:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: æœ€åæˆ‘ä»¬å¯ä»¥ç»˜åˆ¶è¿™äº›æ ·æœ¬ï¼æˆ‘ä»¬ä½¿ç”¨æ ¸å¯†åº¦ä¼°è®¡ï¼ˆkdeï¼‰å›¾æ¥å¹³æ»‘ 2D ç½‘æ ¼çš„è§†è§‰æ•ˆæœã€‚æˆ‘ä»¬æœ‰è¶³å¤Ÿçš„æ ·æœ¬ï¼Œæ¯ä¸ªæ¨¡å¼çœ‹èµ·æ¥éƒ½*å®Œç¾*ï¼Œæˆ‘ä»¬å¯ä»¥æ¸…æ¥šåœ°çœ‹åˆ°åœ¨ä¸€ä¸ªæ¼‚äº®çš„ç½‘æ ¼ä¸­å­˜åœ¨ä¹ä¸ªæ¨¡å¼ï¼š
- en: '[PRE13]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: â¶ Plots perfect toy data
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç»˜åˆ¶å®Œç¾çš„ç©å…·æ•°æ®
- en: '![](../Images/CH09_UN07_Raff.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN07_Raff.png)'
- en: 'Now we have some nice simple data; surely our GAN should be able to solve this
    task. There are only two variables, and the distributions are Gaussiansâ€”practically
    the easiest and simplest distribution we could ever hope to have. Letâ€™s set up
    a GAN for this problem and see what happens! The following code quickly creates
    a dataset using the `TensorDataset` class. We are using 32 latent dimensions since
    this is a small problem that does not need 128, and we are using 256 neurons per
    layer. This should be more than necessary to learn the problem:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰ä¸€äº›ç®€å•æ¼‚äº®çš„æ•°æ®ï¼›å½“ç„¶ï¼Œæˆ‘ä»¬çš„GANåº”è¯¥èƒ½å¤Ÿè§£å†³è¿™ä¸ªé—®é¢˜ã€‚åªæœ‰ä¸¤ä¸ªå˜é‡ï¼Œåˆ†å¸ƒæ˜¯é«˜æ–¯åˆ†å¸ƒâ€”â€”å®é™…ä¸Šæ˜¯æˆ‘ä»¬èƒ½å¸Œæœ›å¾—åˆ°çš„æœ€å®¹æ˜“å’Œæœ€ç®€å•çš„åˆ†å¸ƒã€‚è®©æˆ‘ä»¬ä¸ºè¿™ä¸ªé—®é¢˜è®¾ç½®ä¸€ä¸ªGANï¼Œçœ‹çœ‹ä¼šå‘ç”Ÿä»€ä¹ˆï¼ä»¥ä¸‹ä»£ç ä½¿ç”¨`TensorDataset`ç±»å¿«é€Ÿåˆ›å»ºä¸€ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬ä½¿ç”¨32ä¸ªæ½œåœ¨ç»´åº¦ï¼Œå› ä¸ºè¿™æ˜¯ä¸€ä¸ªå°é—®é¢˜ï¼Œä¸éœ€è¦128ä¸ªç»´åº¦ï¼Œå¹¶ä¸”æˆ‘ä»¬æ¯å±‚ä½¿ç”¨256ä¸ªç¥ç»å…ƒã€‚è¿™åº”è¯¥è¶³å¤Ÿå­¦ä¹ è¿™ä¸ªé—®é¢˜ï¼š
- en: '[PRE14]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: â¶ A new GAN with just two output features for our toy problem
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä¸ºæˆ‘ä»¬çš„ç©å…·é—®é¢˜åˆ›å»ºäº†ä¸€ä¸ªä»…æœ‰ä¸¤ä¸ªè¾“å‡ºç‰¹å¾çš„æ–°GAN
- en: 'With that in place, we can rerun the `for` loop we made to train our GANs for
    100 epochs. Then we can generate some synthetic data, which we quickly do with
    another `no_grad` block:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨æ­¤åŸºç¡€ä¸Šï¼Œæˆ‘ä»¬å¯ä»¥é‡æ–°è¿è¡Œæˆ‘ä»¬åˆ›å»ºçš„`for`å¾ªç¯æ¥è®­ç»ƒæˆ‘ä»¬çš„GANs 100ä¸ªepochã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ç”Ÿæˆä¸€äº›åˆæˆæ•°æ®ï¼Œè¿™å¯ä»¥é€šè¿‡å¦ä¸€ä¸ª`no_grad`å—å¿«é€Ÿå®Œæˆï¼š
- en: '[PRE15]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: â¶ Samples some random **z** âˆ¼ ğ’©(0,1)
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä»éšæœº **z** âˆ¼ ğ’©(0,1) ä¸­é‡‡æ ·ä¸€äº›
- en: â· Creates the fake data *G*(**z**)
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: â· åˆ›å»ºå‡æ•°æ® *G*(**z**)
- en: 'Now letâ€™s visualize the generated samples with the following `kdeplot` call.
    Ideally, we should see the same image with a grid of nine circles. Instead, you
    usually see that only one or two of the nine Gaussians were generated by G! On
    top of that, the GAN does not always do a particularly good job of learning just
    one Gaussian. Instead of getting the larger shape and width, it learns a mode
    of the mode, focusing on the very center area with the most samples:'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨è®©æˆ‘ä»¬ä½¿ç”¨ä»¥ä¸‹`kdeplot`è°ƒç”¨å¯è§†åŒ–ç”Ÿæˆçš„æ ·æœ¬ã€‚ç†æƒ³æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åº”è¯¥çœ‹åˆ°ä¹ä¸ªåœ†åœˆçš„ç½‘æ ¼å›¾åƒã€‚ç„¶è€Œï¼Œä½ é€šå¸¸åªä¼šçœ‹åˆ°å…¶ä¸­ä¸€ä¸¤ä¸ªé«˜æ–¯åˆ†å¸ƒæ˜¯ç”±Gç”Ÿæˆçš„ï¼æ›´ç³Ÿç³•çš„æ˜¯ï¼ŒGANå¹¶ä¸æ€»æ˜¯èƒ½å¾ˆå¥½åœ°å­¦ä¹ ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒã€‚å®ƒä¸æ˜¯å­¦ä¹ æ›´å¤§çš„å½¢çŠ¶å’Œå®½åº¦ï¼Œè€Œæ˜¯å­¦ä¹ äº†ä¸€ä¸ªæ¨¡å¼çš„æ¨¡å¼ï¼Œä¸“æ³¨äºæ ·æœ¬æœ€å¤šçš„ä¸­å¿ƒåŒºåŸŸï¼š
- en: '[PRE16]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: â¶ Plots what G learned of our toy data
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç»˜åˆ¶Gä»æˆ‘ä»¬çš„ç©å…·æ•°æ®ä¸­å­¦åˆ°çš„å†…å®¹
- en: â· Manually sets the x-axis to the range our dataset originally had
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: â· æ‰‹åŠ¨è®¾ç½®xè½´çš„èŒƒå›´ï¼Œä½¿å…¶ä¸æˆ‘ä»¬çš„æ•°æ®é›†åŸå§‹èŒƒå›´ä¸€è‡´
- en: â¸ Same for the y-axis
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ åŒæ ·é€‚ç”¨äºyè½´
- en: '![](../Images/CH09_UN08_Raff.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN08_Raff.png)'
- en: This behavior is an example of the mode-collapse phenomena. Instead of learning
    about the entire input space, the GAN picked the most convenient option for itself
    and overfit to just that. Hopefully, this toy problem shows you how easy it is
    for GANs to fall into the mode-collapse trap. All the tricks we learn in this
    chapter, and even the most cutting-edge approaches, only lessen this issueâ€”we
    have not solved it. Even when you train a GAN that seems to work well on your
    problems, you should manually inspect the results and compare them against the
    real data. Look for patterns and styles that appear in the real data but not the
    generated outputs to identify whether mode collapse is happening and how severe
    it is. A GAN that generates good-looking outputs but has collapsed onto a few
    modes can easily trick you into thinking your results are better than they actually
    are.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§è¡Œä¸ºæ˜¯æ¨¡å¼åå¡Œç°è±¡çš„ä¸€ä¸ªä¾‹å­ã€‚GANä¸æ˜¯å­¦ä¹ æ•´ä¸ªè¾“å…¥ç©ºé—´ï¼Œè€Œæ˜¯é€‰æ‹©äº†å¯¹è‡ªå·±æœ€æ–¹ä¾¿çš„é€‰é¡¹ï¼Œå¹¶è¿‡åº¦æ‹Ÿåˆåˆ°è¿™ä¸€ç‚¹ã€‚å¸Œæœ›è¿™ä¸ªç©å…·é—®é¢˜èƒ½è®©ä½ çœ‹åˆ°GANé™·å…¥æ¨¡å¼åå¡Œé™·é˜±æ˜¯å¤šä¹ˆå®¹æ˜“ã€‚æˆ‘ä»¬åœ¨è¿™ä¸ªç« èŠ‚ä¸­å­¦åˆ°çš„æ‰€æœ‰æŠ€å·§ï¼Œç”šè‡³æ˜¯æœ€å‰æ²¿çš„æ–¹æ³•ï¼Œéƒ½åªèƒ½å‡è½»è¿™ä¸ªé—®é¢˜â€”â€”æˆ‘ä»¬è¿˜æ²¡æœ‰è§£å†³è¿™ä¸ªé—®é¢˜ã€‚å³ä½¿ä½ è®­ç»ƒçš„GANåœ¨ä½ çš„é—®é¢˜ä¸Šä¼¼ä¹è¡¨ç°è‰¯å¥½ï¼Œä½ ä¹Ÿåº”è¯¥æ‰‹åŠ¨æ£€æŸ¥ç»“æœï¼Œå¹¶å°†å®ƒä»¬ä¸çœŸå®æ•°æ®è¿›è¡Œæ¯”è¾ƒã€‚å¯»æ‰¾åœ¨çœŸå®æ•°æ®ä¸­å‡ºç°ä½†åœ¨ç”Ÿæˆè¾“å‡ºä¸­ä¸å‡ºç°çš„æ¨¡å¼å’Œé£æ ¼ï¼Œä»¥ç¡®å®šæ˜¯å¦å‘ç”Ÿäº†æ¨¡å¼åå¡Œä»¥åŠå…¶ä¸¥é‡ç¨‹åº¦ã€‚ä¸€ä¸ªç”Ÿæˆçœ‹èµ·æ¥å¾ˆå¥½çš„è¾“å‡ºä½†åå¡Œåˆ°å‡ ä¸ªæ¨¡å¼ä¸Šçš„GANå¾ˆå®¹æ˜“è®©ä½ è¯¯ä»¥ä¸ºä½ çš„ç»“æœæ¯”å®é™…æƒ…å†µè¦å¥½ã€‚
- en: '9.3 Wasserstein GAN: Mitigating mode collapse'
  id: totrans-165
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 Wasserstein GANï¼šç¼“è§£æ¨¡å¼åå¡Œ
- en: GANs are a highly active area of research right now, with many different approaches
    to deal with the mode-collapse problem. This section talks about one approach
    that has proven to be a reliable improvement and from which many others are building
    more sophisticated solutions. It is called the *Wasserstein GAN* (WGAN).[Â¹](#fn32)
    The name Wasserstein comes from the math for deriving this improvement, which
    we are not going to discuss. Instead, we will talk about the result and why it
    works.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: GANsï¼ˆç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼‰ç›®å‰æ˜¯ä¸€ä¸ªé«˜åº¦æ´»è·ƒçš„ç ”ç©¶é¢†åŸŸï¼Œæœ‰ä¼—å¤šä¸åŒçš„æ–¹æ³•æ¥å¤„ç†æ¨¡å¼åå¡Œé—®é¢˜ã€‚æœ¬èŠ‚å°†è®¨è®ºä¸€ç§å·²è¢«è¯æ˜æ˜¯å¯é çš„æ”¹è¿›æ–¹æ³•ï¼Œè®¸å¤šå…¶ä»–æ–¹æ³•éƒ½æ˜¯åŸºäºå®ƒæ„å»ºçš„æ›´å¤æ‚çš„è§£å†³æ–¹æ¡ˆã€‚è¿™ç§æ–¹æ³•è¢«ç§°ä¸º*Wasserstein
    GAN*ï¼ˆWGANï¼‰ã€‚[Â¹](#fn32) â€œWassersteinâ€è¿™ä¸ªåå­—æ¥æºäºæ¨å¯¼è¿™ç§æ”¹è¿›çš„æ•°å­¦æ–¹æ³•ï¼Œæˆ‘ä»¬åœ¨æ­¤ä¸è¿›è¡Œè®¨è®ºã€‚ç›¸åï¼Œæˆ‘ä»¬å°†è®¨è®ºç»“æœä»¥åŠä¸ºä»€ä¹ˆå®ƒæœ‰æ•ˆã€‚
- en: 'As before, we will write the loss function in two parts: the loss for the discriminator
    and the loss for the generator. In this new solution, we do not use a sigmoid
    activation for the output of the discriminator D: that way, we have fewer vanishing
    gradient issues. The discriminator will still output a single value, though.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: å’Œä¹‹å‰ä¸€æ ·ï¼Œæˆ‘ä»¬å°†æŸå¤±å‡½æ•°åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼šåˆ¤åˆ«å™¨çš„æŸå¤±å’Œç”Ÿæˆå™¨çš„æŸå¤±ã€‚åœ¨è¿™ä¸ªæ–°æ–¹æ¡ˆä¸­ï¼Œæˆ‘ä»¬ä¸å¯¹åˆ¤åˆ«å™¨Dçš„è¾“å‡ºä½¿ç”¨sigmoidæ¿€æ´»å‡½æ•°ï¼šè¿™æ ·ï¼Œæˆ‘ä»¬å°±æœ‰æ›´å°‘çš„æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ã€‚å°½ç®¡å¦‚æ­¤ï¼Œåˆ¤åˆ«å™¨ä»ç„¶ä¼šè¾“å‡ºä¸€ä¸ªå•ä¸€å€¼ã€‚
- en: 9.3.1 Â WGAN discriminator loss
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 WGANåˆ¤åˆ«å™¨æŸå¤±
- en: The loss of the discriminator is the difference between Dâ€™s score on fake data
    and Dâ€™s score on real data. That looks like *D*(*G*(**z**)) âˆ’ *D*(**x**), so D
    wants to *minimize* this value as much as possible. This score canâ€™t saturate,
    which helps mitigate issues with vanishing gradients. More important, we will
    include a *penalty on the complexity of* D. The idea is that we want to handicap
    D so it has to work harder to learn more powerful models, which we hope makes
    the game between D and G a fair fight. We want to do this because D has the advantage
    of an easier problem. The discriminatorâ€™s loss becomes
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: åˆ¤åˆ«å™¨çš„æŸå¤±æ˜¯Då¯¹å‡æ•°æ®çš„è¯„åˆ†ä¸Då¯¹çœŸå®æ•°æ®çš„è¯„åˆ†ä¹‹é—´çš„å·®å¼‚ã€‚è¿™çœ‹èµ·æ¥åƒæ˜¯ *D*(*G*(**z**)) âˆ’ *D*(**x**)ï¼Œæ‰€ä»¥Då°½å¯èƒ½å¤šåœ°æƒ³è¦*æœ€å°åŒ–*è¿™ä¸ªå€¼ã€‚è¿™ä¸ªåˆ†æ•°ä¸èƒ½é¥±å’Œï¼Œè¿™æœ‰åŠ©äºç¼“è§£æ¢¯åº¦æ¶ˆå¤±çš„é—®é¢˜ã€‚æ›´é‡è¦çš„æ˜¯ï¼Œæˆ‘ä»¬å°†åŒ…æ‹¬å¯¹Dçš„*å¤æ‚æ€§*çš„æƒ©ç½šã€‚æˆ‘ä»¬çš„æƒ³æ³•æ˜¯è®©Då¤„äºä¸åˆ©åœ°ä½ï¼Œè¿™æ ·å®ƒå°±å¿…é¡»æ›´åŠªåŠ›åœ°å­¦ä¹ æ›´å¼ºå¤§çš„æ¨¡å‹ï¼Œæˆ‘ä»¬å¸Œæœ›è¿™èƒ½è®©Då’ŒGä¹‹é—´çš„æ¸¸æˆæ›´åŠ å…¬å¹³ã€‚æˆ‘ä»¬ä¹‹æ‰€ä»¥è¿™æ ·åšï¼Œæ˜¯å› ä¸ºDæœ‰ä¸€ä¸ªæ›´å®¹æ˜“çš„é—®é¢˜ã€‚åˆ¤åˆ«å™¨çš„æŸå¤±å˜ä¸º
- en: '![](../Images/ch9-eqs-to-illustrator5x.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/ch9-eqs-to-illustrator5x.png)'
- en: We annotate this equation in a moment, but first letâ€™s talk through it as you
    might see in a paper. Remember that by default, the goal is to minimize a loss
    function. What does minimizing this equation do? First, there is the subtraction
    of the G-score term and the D-score term. G-score gives a score of how realistic
    the generatorâ€™s output is, and D-score gives a score of how realistic real data
    looks. To minimize this, the discriminator D wants to minimize (large negative
    values!) the G-score and maximize (large positive values!) the D-score. So, large
    score values indicate more realism. Because the scores are not bounded by anything,
    we are really comparing the relative scores of fake data to those of real data.
    The important part here is that regardless of how well the discriminator is doing,
    the two values are just numbers subtracted, so the gradient should be easy to
    compute without any numerical concerns.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç¨åä¼šå¯¹è¿™ä¸ªæ–¹ç¨‹è¿›è¡Œæ³¨é‡Šï¼Œä½†é¦–å…ˆè®©æˆ‘ä»¬åƒåœ¨è®ºæ–‡ä¸­çœ‹åˆ°çš„é‚£æ ·ï¼Œé€å¥è®²è§£ä¸€ä¸‹ã€‚è®°ä½ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼Œç›®æ ‡æ˜¯ä½¿æŸå¤±å‡½æ•°æœ€å°åŒ–ã€‚æœ€å°åŒ–è¿™ä¸ªæ–¹ç¨‹æœ‰ä»€ä¹ˆä½œç”¨å‘¢ï¼Ÿé¦–å…ˆï¼Œæœ‰Gåˆ†æ•°é¡¹å’ŒDåˆ†æ•°é¡¹çš„å‡æ³•ã€‚Gåˆ†æ•°ç»™å‡ºäº†ç”Ÿæˆå™¨è¾“å‡ºçœŸå®æ€§çš„è¯„åˆ†ï¼Œè€ŒDåˆ†æ•°ç»™å‡ºäº†çœŸå®æ•°æ®å¤–è§‚çš„è¯„åˆ†ã€‚ä¸ºäº†æœ€å°åŒ–è¿™ä¸ªï¼Œåˆ¤åˆ«å™¨Dæƒ³è¦æœ€å°åŒ–ï¼ˆå¤§è´Ÿå€¼ï¼ï¼‰Gåˆ†æ•°å¹¶æœ€å¤§åŒ–ï¼ˆå¤§æ­£å€¼ï¼ï¼‰Dåˆ†æ•°ã€‚å› æ­¤ï¼Œé«˜åˆ†æ•°å€¼è¡¨ç¤ºæ›´é«˜çš„çœŸå®æ€§ã€‚å› ä¸ºåˆ†æ•°æ²¡æœ‰å—åˆ°ä»»ä½•é™åˆ¶ï¼Œæˆ‘ä»¬å®é™…ä¸Šæ˜¯åœ¨æ¯”è¾ƒå‡æ•°æ®ä¸çœŸå®æ•°æ®çš„ç›¸å¯¹åˆ†æ•°ã€‚è¿™é‡Œçš„é‡è¦éƒ¨åˆ†æ˜¯ï¼Œæ— è®ºåˆ¤åˆ«å™¨åšå¾—æœ‰å¤šå¥½ï¼Œè¿™ä¸¤ä¸ªå€¼åªæ˜¯ç›¸å‡çš„æ•°å­—ï¼Œæ‰€ä»¥æ¢¯åº¦åº”è¯¥å¾ˆå®¹æ˜“è®¡ç®—ï¼Œæ²¡æœ‰ä»»ä½•æ•°å€¼é—®é¢˜ã€‚
- en: The other interesting addition is the *complexity penalty* term on the right
    side. How does this penalize complexity? It does so by taking the norm (âˆ¥ â‹… âˆ¥[2])
    of the gradient (âˆ‡) of the discriminator D. The value Ïµ is selected randomly in
    the range [0,1], so the penalty is applied to a random mix of real and fake data,
    which makes sure D canâ€™t cheat by learning simple functions for part of the data
    and complex functions for the other part. Finally, the value Î» is a hyper-parameter
    we control. The larger we set it, the more we penalize complexity.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ä¸ªæœ‰è¶£çš„è¡¥å……æ˜¯å³ä¾§çš„*å¤æ‚æ€§æƒ©ç½š*é¡¹ã€‚å®ƒæ˜¯å¦‚ä½•æƒ©ç½šå¤æ‚æ€§çš„å‘¢ï¼Ÿå®ƒé€šè¿‡å–åˆ¤åˆ«å™¨Dçš„æ¢¯åº¦ï¼ˆâˆ‡ï¼‰çš„èŒƒæ•°ï¼ˆâˆ¥ â‹… âˆ¥[2]ï¼‰æ¥åšåˆ°è¿™ä¸€ç‚¹ã€‚å€¼Ïµæ˜¯åœ¨[0,1]èŒƒå›´å†…éšæœºé€‰æ‹©çš„ï¼Œæ‰€ä»¥æƒ©ç½šåº”ç”¨äºçœŸå®å’Œå‡æ•°æ®çš„éšæœºæ··åˆï¼Œè¿™ç¡®ä¿Dä¸èƒ½é€šè¿‡å­¦ä¹ éƒ¨åˆ†æ•°æ®çš„ç®€å•å‡½æ•°å’Œå¦ä¸€éƒ¨åˆ†æ•°æ®çš„å¤æ‚å‡½æ•°æ¥ä½œå¼Šã€‚æœ€åï¼Œå€¼Î»æ˜¯æˆ‘ä»¬æ§åˆ¶çš„è¶…å‚æ•°ã€‚æˆ‘ä»¬è®¾ç½®å¾—è¶Šå¤§ï¼Œå¯¹å¤æ‚æ€§çš„æƒ©ç½šå°±è¶Šå¤§ã€‚
- en: 'Now that we have walked through the parts of this new loss for D, letâ€™s repeat
    the equation with some color-coded annotations summarizing what is happening:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å·²ç»èµ°è¿‡äº†Dè¿™ä¸ªæ–°æŸå¤±å‡½æ•°çš„å„ä¸ªéƒ¨åˆ†ï¼Œè®©æˆ‘ä»¬ç”¨ä¸€äº›å½©è‰²æ³¨é‡Šé‡å¤è¿™ä¸ªæ–¹ç¨‹ï¼Œæ€»ç»“ä¸€ä¸‹æ­£åœ¨å‘ç”Ÿçš„äº‹æƒ…ï¼š
- en: '![](../Images/CH09_UN09_Raff.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN09_Raff.png)'
- en: That our complexity term includes a gradient in the loss function means computing
    the loss requires taking the gradient of a gradient! Working out the math for
    that is difficult, but luckily PyTorch is here to do it for us. We can focus on
    understanding what this equation says and how to derive it.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨æŸå¤±å‡½æ•°ä¸­åŒ…å«ä¸€ä¸ªæ¢¯åº¦é¡¹æ„å‘³ç€è®¡ç®—æŸå¤±éœ€è¦è®¡ç®—æ¢¯åº¦çš„æ¢¯åº¦ï¼è§£å†³è¿™ä¸ªæ•°å­¦é—®é¢˜å¾ˆå›°éš¾ï¼Œä½†å¹¸è¿çš„æ˜¯PyTorchåœ¨è¿™é‡Œä¸ºæˆ‘ä»¬åšäº†è¿™ä»¶äº‹ã€‚æˆ‘ä»¬å¯ä»¥ä¸“æ³¨äºç†è§£è¿™ä¸ªæ–¹ç¨‹è¯´äº†ä»€ä¹ˆä»¥åŠå¦‚ä½•æ¨å¯¼å®ƒã€‚
- en: 'A gradient tells you which direction to move in to minimize a function, and
    larger values mean you are at a steeper part of the function you are trying to
    minimize. A simple function that always returns the same value (i.e., no change)
    has a gradient of zero, which is the only way to get the complexity penalty to
    zero. So the further our function gets from simply returning the same value, the
    higher its penalty will be. Thatâ€™s the trick to how this part of the equation
    penalizes a complex model. Getting the penalty term to zero requires returning
    the same value under all circumstances, essentially ignoring the input. That is
    a uselessly simple function, but that will minimize that part of the loss: there
    is always a pull on D that wants it to move away from learning anything complex.
    In general, whenever you see something like âˆ¥âˆ‡*f*(*x*)âˆ¥ in a loss function, you
    should understand it as penalizing complexity.'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: æ¢¯åº¦å‘Šè¯‰ä½ æœå“ªä¸ªæ–¹å‘ç§»åŠ¨ä»¥æœ€å°åŒ–ä¸€ä¸ªå‡½æ•°ï¼Œè¾ƒå¤§çš„å€¼æ„å‘³ç€ä½ å¤„äºä½ è¯•å›¾æœ€å°åŒ–çš„å‡½æ•°çš„æ›´é™¡å³­çš„éƒ¨åˆ†ã€‚ä¸€ä¸ªæ€»æ˜¯è¿”å›ç›¸åŒå€¼ï¼ˆå³æ²¡æœ‰å˜åŒ–ï¼‰çš„ç®€å•å‡½æ•°æœ‰ä¸€ä¸ªé›¶æ¢¯åº¦ï¼Œè¿™æ˜¯å”¯ä¸€ä½¿å¤æ‚åº¦æƒ©ç½šä¸ºé›¶çš„æ–¹æ³•ã€‚å› æ­¤ï¼Œæˆ‘ä»¬çš„å‡½æ•°è¶Šè¿œç¦»ç®€å•åœ°è¿”å›ç›¸åŒå€¼ï¼Œå…¶æƒ©ç½šå°±è¶Šé«˜ã€‚è¿™å°±æ˜¯è¿™ä¸ªæ–¹ç¨‹çš„è¿™ä¸€éƒ¨åˆ†å¦‚ä½•æƒ©ç½šå¤æ‚æ¨¡å‹çš„å°æŠ€å·§ã€‚è¦ä½¿æƒ©ç½šé¡¹ä¸ºé›¶ï¼Œéœ€è¦åœ¨æ‰€æœ‰æƒ…å†µä¸‹è¿”å›ç›¸åŒçš„å€¼ï¼Œè¿™æœ¬è´¨ä¸Šæ„å‘³ç€å¿½ç•¥è¾“å…¥ã€‚è¿™æ˜¯ä¸€ä¸ªæ— ç”¨çš„ç®€å•å‡½æ•°ï¼Œä½†å®ƒå°†æœ€å°åŒ–æŸå¤±çš„è¿™ä¸€éƒ¨åˆ†ï¼šæ€»æ˜¯æœ‰ä¸€ä¸ªæ‹‰åŠ›ä½œç”¨äºDï¼Œå¸Œæœ›å®ƒè¿œç¦»å­¦ä¹ ä»»ä½•å¤æ‚çš„ä¸œè¥¿ã€‚ä¸€èˆ¬æ¥è¯´ï¼Œå½“ä½ åœ¨ä¸€ä¸ªæŸå¤±å‡½æ•°ä¸­çœ‹åˆ°ç±»ä¼¼âˆ¥âˆ‡*f*(*x*)âˆ¥çš„ä¸œè¥¿æ—¶ï¼Œä½ åº”è¯¥ç†è§£å®ƒæ˜¯åœ¨æƒ©ç½šå¤æ‚æ€§ã€‚
- en: 9.3.2 Â WGAN generator loss
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 WGANç”Ÿæˆå™¨æŸå¤±
- en: 'That was a lot of exposition about the discriminator and its fancy new loss
    function. The generatorâ€™s loss, by comparison, is much simpler. The discriminator
    wants to maximize the G-score, and the generator wants to minimize the G-score!
    Our loss is the G-score with the sign flipped so G is minimized:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: å…³äºåˆ¤åˆ«å™¨å’Œå…¶æ–°æ½®çš„æŸå¤±å‡½æ•°ï¼Œæˆ‘ä»¬å·²ç»åšäº†å¾ˆå¤šé˜è¿°ã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼Œç”Ÿæˆå™¨çš„æŸå¤±å‡½æ•°è¦ç®€å•å¾—å¤šã€‚åˆ¤åˆ«å™¨æƒ³è¦æœ€å¤§åŒ–Gåˆ†æ•°ï¼Œè€Œç”Ÿæˆå™¨æƒ³è¦æœ€å°åŒ–Gåˆ†æ•°ï¼æˆ‘ä»¬çš„æŸå¤±æ˜¯å¸¦æœ‰ç¬¦å·åè½¬çš„Gåˆ†æ•°ï¼Œè¿™æ ·Gå°±è¢«æœ€å°åŒ–äº†ï¼š
- en: '*loss*[G] = âˆ’ *D*(*G*(**z**))'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '*loss*[G] = âˆ’ *D*(*G*(**z**))'
- en: Why is the generatorâ€™s loss so much simpler? It ties back to three reasons we
    have already discussed. First, the generator G does not care what D says about
    real data x. This means we can remove the *D*(**x**) term from the loss. Second,
    D has the advantage of an easier problem. The complexity penalty was to disadvantage
    D, and since we are talking about Gâ€™s loss, we do not want to disadvantage G with
    an unnecessary penalty. Third, Gâ€™s goal is to trick D. The only remaining part
    of the G-score is *D*(*G*(**z**)). Since G has the opposite goal, we shove a negative
    sign in front, giving us the final loss âˆ’ *D*(*G*(**z**)).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸ºä»€ä¹ˆç”Ÿæˆå™¨çš„æŸå¤±å¦‚æ­¤ç®€å•ï¼Ÿè¿™å›åˆ°äº†æˆ‘ä»¬å·²ç»è®¨è®ºè¿‡çš„ä¸‰ä¸ªåŸå› ã€‚é¦–å…ˆï¼Œç”Ÿæˆå™¨Gä¸å…³å¿ƒDå¯¹çœŸå®æ•°æ®xçš„çœ‹æ³•ã€‚è¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥ä»æŸå¤±ä¸­ç§»é™¤*D*(**x**)é¡¹ã€‚å…¶æ¬¡ï¼ŒDæœ‰ä¸€ä¸ªæ›´å®¹æ˜“çš„é—®é¢˜ã€‚å¤æ‚åº¦æƒ©ç½šæ˜¯ä¸ºäº†ä½¿Då¤„äºä¸åˆ©åœ°ä½ï¼Œè€Œå½“æˆ‘ä»¬è°ˆè®ºGçš„æŸå¤±æ—¶ï¼Œæˆ‘ä»¬ä¸æƒ³ç”¨ä¸€ä¸ªä¸å¿…è¦çš„æƒ©ç½šæ¥ä½¿Gå¤„äºä¸åˆ©åœ°ä½ã€‚ç¬¬ä¸‰ï¼ŒGçš„ç›®æ ‡æ˜¯æ¬ºéª—Dã€‚Gåˆ†æ•°å‰©ä¸‹çš„å”¯ä¸€éƒ¨åˆ†æ˜¯*D*(*G*(**z**))ã€‚ç”±äºGæœ‰ä¸€ä¸ªç›¸åçš„ç›®æ ‡ï¼Œæˆ‘ä»¬åœ¨å‰é¢æ”¾ä¸€ä¸ªè´Ÿå·ï¼Œå¾—åˆ°æœ€ç»ˆçš„æŸå¤±
    âˆ’ *D*(*G*(**z**))ã€‚
- en: 9.3.3 Â Implementing WGAN
  id: totrans-181
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 å®ç°WGAN
- en: This new approach is often abbreviated WGAN-GP, where the GP stands for gradient
    penalty.[Â²](#fn33) WGAN-GP helps solve the mode-collapse problem by reducing the
    opportunities for vanishing gradients and leveling the playing field between G
    and D so that G has an easier time keeping up with D. Letâ€™s train a new GAN with
    this approach on the toy grid and see if it helps.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™ç§æ–°çš„æ–¹æ³•é€šå¸¸ç¼©å†™ä¸ºWGAN-GPï¼Œå…¶ä¸­GPä»£è¡¨æ¢¯åº¦æƒ©ç½šã€‚[Â²](#fn33) WGAN-GPé€šè¿‡å‡å°‘æ¢¯åº¦æ¶ˆå¤±çš„æœºä¼šï¼Œå¹¶åœ¨Gå’ŒDä¹‹é—´å¹³è¡¡ç«äº‰ç¯å¢ƒï¼Œä½¿å¾—Gæ›´å®¹æ˜“è·Ÿä¸ŠDï¼Œä»è€Œå¸®åŠ©è§£å†³æ¨¡å¼åå¡Œé—®é¢˜ã€‚è®©æˆ‘ä»¬ç”¨è¿™ç§æ–¹æ³•åœ¨ç©å…·ç½‘æ ¼ä¸Šè®­ç»ƒä¸€ä¸ªæ–°çš„GANï¼Œçœ‹çœ‹å®ƒæ˜¯å¦æœ‰æ‰€å¸®åŠ©ã€‚
- en: Since we will be using the WGAN-GP multiple times, weâ€™ll define a function`train_wgan`
    that does the work. This function follows the same setup and organization as our
    original GAN training loop, but each section of the function will have some changes.
    This includes the prep at the function start, giving the inputs to D and G, adding
    the gradient penalty, and computing the final WGAN-GP loss. We first define the
    function with arguments to take in the networks, the loader, the number of latent
    dimensions, how many epochs to train for, and the device to use.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºæˆ‘ä»¬å°†å¤šæ¬¡ä½¿ç”¨WGAN-GPï¼Œæˆ‘ä»¬å°†å®šä¹‰ä¸€ä¸ª`train_wgan`å‡½æ•°æ¥å®Œæˆè¿™é¡¹å·¥ä½œã€‚è¿™ä¸ªå‡½æ•°éµå¾ªæˆ‘ä»¬åŸå§‹GANè®­ç»ƒå¾ªç¯ç›¸åŒçš„è®¾ç½®å’Œç»„ç»‡ï¼Œä½†å‡½æ•°çš„æ¯ä¸ªéƒ¨åˆ†éƒ½ä¼šæœ‰ä¸€äº›å˜åŒ–ã€‚è¿™åŒ…æ‹¬å‡½æ•°å¼€å§‹æ—¶çš„å‡†å¤‡ï¼Œå‘Då’ŒGæä¾›è¾“å…¥ï¼Œæ·»åŠ æ¢¯åº¦æƒ©ç½šï¼Œä»¥åŠè®¡ç®—æœ€ç»ˆçš„WGAN-GPæŸå¤±ã€‚æˆ‘ä»¬é¦–å…ˆå®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œå¸¦æœ‰å‚æ•°æ¥æ¥æ”¶ç½‘ç»œã€åŠ è½½å™¨ã€æ½œåœ¨ç»´åº¦çš„æ•°é‡ã€è®­ç»ƒçš„epochæ•°ä»¥åŠè¦ä½¿ç”¨çš„è®¾å¤‡ã€‚
- en: Updating the training prep
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–°è®­ç»ƒå‡†å¤‡
- en: 'This code uses a simple trick: `if` statements around `isinstance(data,Â tuple)Â orÂ len(data)`.
    This way, we have a training loop that will work when the user gives a `Dataloader`
    with only the unlabeled data x or when given the data and labels y (but it wonâ€™t
    use the labels; we just wonâ€™t throw any errors about it):'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™æ®µä»£ç ä½¿ç”¨äº†ä¸€ä¸ªç®€å•çš„æŠ€å·§ï¼šåœ¨ `isinstance(data, tuple) or len(data)` å‘¨å›´çš„ `if` è¯­å¥ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬æœ‰ä¸€ä¸ªè®­ç»ƒå¾ªç¯ï¼Œå½“ç”¨æˆ·æä¾›ä¸€ä¸ªåªåŒ…å«æœªæ ‡è®°æ•°æ®
    x çš„ `Dataloader` æˆ–æä¾›æ•°æ® yï¼ˆä½†ä¸ä¼šä½¿ç”¨æ ‡ç­¾ï¼›æˆ‘ä»¬åªæ˜¯ä¸ä¼šä¸ºæ­¤æŠ›å‡ºä»»ä½•é”™è¯¯ï¼‰æ—¶ï¼Œå®ƒå°†å·¥ä½œï¼š
- en: '[PRE17]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: â¶ Sets up Adam optimizer for D
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä¸º D è®¾ç½® Adam ä¼˜åŒ–å™¨
- en: â· Sets up Adam optimizer for G
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: â· ä¸º G è®¾ç½® Adam ä¼˜åŒ–å™¨
- en: Updating the D and G Inputs
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´æ–° D å’Œ G è¾“å…¥
- en: 'In the body of the loop, we compute the results of D on real and fake data.
    An important change here is that we are *not* calling the `detach()` function
    on `fake` when it goes into the discriminator, because we need to include G in
    the gradient penalty calculation in the next step:'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å¾ªç¯ä½“ä¸­ï¼Œæˆ‘ä»¬è®¡ç®— D åœ¨çœŸå®å’Œä¼ªé€ æ•°æ®ä¸Šçš„ç»“æœã€‚è¿™é‡Œçš„ä¸€ä¸ªé‡è¦å˜åŒ–æ˜¯ï¼Œå½“ `fake` è¿›å…¥åˆ¤åˆ«å™¨æ—¶ï¼Œæˆ‘ä»¬å¹¶æ²¡æœ‰è°ƒç”¨ `detach()` å‡½æ•°ï¼Œå› ä¸ºæˆ‘ä»¬éœ€è¦åœ¨ä¸‹ä¸€æ­¥çš„æ¢¯åº¦æƒ©ç½šè®¡ç®—ä¸­åŒ…å«
    Gï¼š
- en: '[PRE18]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'â¶ Step 1: D-score, G-score, and gradient penalty. How well does D work on real
    data?'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ æ­¥éª¤ 1ï¼šD åˆ†æ•°ã€G åˆ†æ•°å’Œæ¢¯åº¦æƒ©ç½šã€‚D åœ¨çœŸå®æ•°æ®ä¸Šçš„è¡¨ç°å¦‚ä½•ï¼Ÿ
- en: â· Trains with an all-fake batch and generates a batch of latent vectors
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: â· ä½¿ç”¨æ‰€æœ‰ä¼ªé€ çš„æ‰¹æ¬¡è¿›è¡Œè®­ç»ƒå¹¶ç”Ÿæˆä¸€æ‰¹æ½œåœ¨å‘é‡
- en: â¸ Generates a fake image batch with G
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ä½¿ç”¨ G ç”Ÿæˆä¸€ä¸ªä¼ªé€ çš„å›¾åƒæ‰¹æ¬¡
- en: â¹ Classifies the all-fake batch with D
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ ä½¿ç”¨ D å¯¹æ‰€æœ‰ä¼ªé€ çš„æ‰¹æ¬¡è¿›è¡Œåˆ†ç±»
- en: Calculating the Gradient Penalty
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—æ¢¯åº¦æƒ©ç½š
- en: With `fake` computed, we can compute the gradient penalty. First we select random
    values in the range of [0,1] for Ïµ in the `eps` variable. We have to make sure
    it has as many axes as the training data to make the tensors multiply together.
    So if our data is of shape (*B*,*D*), `eps` will have a shape of (*B*,1). If our
    data has a shape of (*B*,*C*,*W*,*H*), `eps` needs a shape of (*B*,1,1,1). This
    way, every item in the batch is multiplied by one value, and we can compute the
    `mixed` input that goes into D.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ `fake` è®¡ç®—å‡ºæ¥åï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—æ¢¯åº¦æƒ©ç½šã€‚é¦–å…ˆï¼Œæˆ‘ä»¬åœ¨ `eps` å˜é‡ä¸­ä¸º Ïµ é€‰æ‹© [0,1] èŒƒå›´å†…çš„éšæœºå€¼ã€‚æˆ‘ä»¬å¿…é¡»ç¡®ä¿å®ƒæœ‰ä¸è®­ç»ƒæ•°æ®ç›¸åŒçš„è½´ï¼Œä»¥ä¾¿å¼ é‡å¯ä»¥ç›¸ä¹˜ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬çš„æ•°æ®å½¢çŠ¶ä¸º
    (*B*,*D*)ï¼Œ`eps` å°†å…·æœ‰å½¢çŠ¶ (*B*,1)ã€‚å¦‚æœæˆ‘ä»¬çš„æ•°æ®å½¢çŠ¶ä¸º (*B*,*C*,*W*,*H*)ï¼Œ`eps` éœ€è¦å½¢çŠ¶ä¸º (*B*,1,1,1)ã€‚è¿™æ ·ï¼Œæ‰¹æ¬¡ä¸­çš„æ¯ä¸ªé¡¹ç›®éƒ½ä¹˜ä»¥ä¸€ä¸ªå€¼ï¼Œæˆ‘ä»¬å¯ä»¥è®¡ç®—è¿›å…¥
    D çš„ `mixed` è¾“å…¥ã€‚
- en: The next line of code has an `autograd.grad` function call. To be honest, this
    function frightens me, and I have to look it up every time I need to use it. It
    does the work of computing the gradient âˆ‡ for us in a way that PyTorch can use
    to compute gradients.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹ä¸€è¡Œä»£ç ä¸­æœ‰ä¸€ä¸ª `autograd.grad` å‡½æ•°è°ƒç”¨ã€‚è¯´å®è¯ï¼Œè¿™ä¸ªå‡½æ•°è®©æˆ‘æ„Ÿåˆ°å®³æ€•ï¼Œæ¯æ¬¡éœ€è¦ä½¿ç”¨å®ƒæ—¶æˆ‘éƒ½å¾—æŸ¥ä¸€ä¸‹ã€‚å®ƒä»¥ PyTorch å¯ä»¥ç”¨æ¥è®¡ç®—æ¢¯åº¦çš„æ–¹å¼ä¸ºæˆ‘ä»¬è®¡ç®—æ¢¯åº¦
    âˆ‡ã€‚
- en: 'Basically, this function does the same thing as calling `.backward()`, except
    it returns the object as a new tensor instead of storing it in the `.grad` field.
    Itâ€™s OK if you donâ€™t understand this function, as it is niche in its use. But
    Iâ€™ll give a quick high-level explanation for those who are interested. `outputs=output`
    tells PyTorch what we would have called `.backward()` on, and `inputs=mixed` tells
    PyTorch the initial inputs that gave this result. `grad_outputs=torch.ones_like(output)`
    gives PyTorch an initial value to start the gradient calculation with, which is
    set to all ones so we end up with our desired gradient for all parts. The options
    `create_graph=True,Â retain_graph=True` tell PyTorch that we want automatic differentiation
    to work on the result (this does the gradient of a gradient):'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: åŸºæœ¬ä¸Šï¼Œè¿™ä¸ªå‡½æ•°æ‰§è¡Œçš„åŠŸèƒ½ä¸è°ƒç”¨ `.backward()` ç›¸åŒï¼Œä½†å®ƒè¿”å›ä¸€ä¸ªæ–°å¼ é‡ä½œä¸ºå¯¹è±¡ï¼Œè€Œä¸æ˜¯å°†å…¶å­˜å‚¨åœ¨ `.grad` å­—æ®µä¸­ã€‚å¦‚æœä½ ä¸ç†è§£è¿™ä¸ªå‡½æ•°ï¼Œé‚£ä¹Ÿæ— æ‰€è°“ï¼Œå› ä¸ºå®ƒçš„ç”¨é€”å¾ˆä¸“ä¸šã€‚ä½†æˆ‘ä¼šç»™é‚£äº›æ„Ÿå…´è¶£çš„äººæä¾›ä¸€ä¸ªå¿«é€Ÿçš„é«˜çº§è§£é‡Šã€‚`outputs=output`
    å‘Šè¯‰ PyTorch æˆ‘ä»¬ä¼šè°ƒç”¨ `.backward()` çš„å†…å®¹ï¼Œè€Œ `inputs=mixed` å‘Šè¯‰ PyTorch å¯¼è‡´è¿™ä¸ªç»“æœçš„åˆå§‹è¾“å…¥ã€‚`grad_outputs=torch.ones_like(output)`
    ç»™ PyTorch æä¾›ä¸€ä¸ªåˆå§‹å€¼æ¥å¼€å§‹æ¢¯åº¦è®¡ç®—ï¼Œå°†å…¶è®¾ç½®ä¸ºå…¨ 1ï¼Œè¿™æ ·æˆ‘ä»¬æœ€ç»ˆå¾—åˆ°æ‰€æœ‰éƒ¨åˆ†çš„æœŸæœ›æ¢¯åº¦ã€‚é€‰é¡¹ `create_graph=True, retain_graph=True`
    å‘Šè¯‰ PyTorch æˆ‘ä»¬å¸Œæœ›è‡ªåŠ¨å¾®åˆ†åœ¨ç»“æœä¸Šå·¥ä½œï¼ˆè¿™åšäº†æ¢¯åº¦çš„æ¢¯åº¦ï¼‰ï¼š
- en: '[PRE19]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: â¶ Now calculate tensors to use to compute the gradient penalty term.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç°åœ¨è®¡ç®—ç”¨äºè®¡ç®—æ¢¯åº¦æƒ©ç½šé¡¹çš„å¼ é‡ã€‚
- en: â· Calculates Dâ€™s loss
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: â· è®¡ç®— D çš„æŸå¤±
- en: â¸ Updates D
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ æ›´æ–° D
- en: Calculating the WGAN-GP loss
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®— WGAN-GP æŸå¤±
- en: With the `grad` variable finally in hand, we can quickly compute the total loss.
    The `*10` is the Î» term controlling how strong the penalty is. Iâ€™ve hardcoded
    it to 10 because thatâ€™s a good default value for this approach, but better coding
    would make `lambda` an argument of the function with a default value of 10.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: å½“ `grad` å˜é‡æœ€ç»ˆåˆ°æ‰‹æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥å¿«é€Ÿè®¡ç®—æ€»æŸå¤±ã€‚`*10` æ˜¯æ§åˆ¶æƒ©ç½šå¼ºåº¦çš„ Î» é¡¹ã€‚æˆ‘å°†å…¶ç¡¬ç¼–ç ä¸º 10ï¼Œå› ä¸ºè¿™åœ¨è¿™ä¸ªæ–¹æ³•ä¸­æ˜¯ä¸€ä¸ªå¾ˆå¥½çš„é»˜è®¤å€¼ï¼Œä½†æ›´å¥½çš„ç¼–ç ä¼šå°†
    `lambda` ä½œä¸ºå‡½æ•°çš„å‚æ•°ï¼Œé»˜è®¤å€¼ä¸º 10ã€‚
- en: 'The second step computes the update to G in the final chunk of code, where
    we begin by re-zeroing out the gradients. This is a defensive coding step due
    to the trickiness of keeping track of when we have and have not altered the gradients.
    Then we sample a new latent z in the `noise` variable, compute `-D(G(noise))`,
    take the average, and do our updates:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ç¬¬äºŒæ­¥åœ¨ä»£ç çš„æœ€åä¸€æ®µä¸­è®¡ç®—Gçš„æ›´æ–°ï¼Œæˆ‘ä»¬é¦–å…ˆé‡æ–°å°†æ¢¯åº¦å½’é›¶ã€‚è¿™æ˜¯ä¸€ä¸ªé˜²å¾¡æ€§ç¼–ç æ­¥éª¤ï¼Œå› ä¸ºè·Ÿè¸ªæˆ‘ä»¬ä½•æ—¶ä»¥åŠä½•æ—¶æ²¡æœ‰æ”¹å˜æ¢¯åº¦æ˜¯å¾ˆæ£˜æ‰‹çš„ã€‚ç„¶åæˆ‘ä»¬åœ¨`noise`å˜é‡ä¸­é‡‡æ ·ä¸€ä¸ªæ–°çš„æ½œåœ¨å˜é‡zï¼Œè®¡ç®—`-D(G(noise))`ï¼Œå–å¹³å‡å€¼ï¼Œç„¶åè¿›è¡Œæ›´æ–°ï¼š
- en: '[PRE20]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'â¶ Step 2: -D(G(z))'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç¬¬2æ­¥ï¼š-D(G(z))
- en: â· Since we just updated D, perform another forward pass of the all-fake batch
    through D.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: â· ç”±äºæˆ‘ä»¬åˆšåˆšæ›´æ–°äº†Dï¼Œæ‰§è¡Œä¸€ä¸ªæ‰€æœ‰ä¼ªé€ æ‰¹æ¬¡é€šè¿‡Dçš„å‰å‘ä¼ é€’ã€‚
- en: â¸ Calculates Gâ€™s loss based on this output
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ åŸºäºæ­¤è¾“å‡ºè®¡ç®—Gçš„æŸå¤±
- en: â¹ Calculates gradients for G
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ è®¡ç®—Gçš„æ¢¯åº¦
- en: âº Updates G
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: âº æ›´æ–°G
- en: 'We end the function by recoding the loss for each of G and D to look at later,
    and return those losses as the result of calling the function:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬é€šè¿‡è®°å½•Gå’ŒDå„è‡ªçš„æŸå¤±æ¥ç»“æŸå‡½æ•°ï¼Œä»¥ä¾¿ç¨åæŸ¥çœ‹ï¼Œå¹¶å°†è¿™äº›æŸå¤±ä½œä¸ºå‡½æ•°è°ƒç”¨çš„ç»“æœè¿”å›ï¼š
- en: '[PRE21]'
  id: totrans-214
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'With this new `train_wgan` function in hand, we can attempt to train a new
    Wasserstein GAN on the earlier toy problem with nine Gaussians in a 3 Ã— 3 grid.
    We simply call the following code snippet:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰äº†è¿™ä¸ªæ–°çš„`train_wgan`å‡½æ•°ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•åœ¨æ—©æœŸç”¨ä¹ä¸ªé«˜æ–¯åˆ†å¸ƒåœ¨3 Ã— 3ç½‘æ ¼ä¸Šçš„ç©å…·é—®é¢˜ä¸Šè®­ç»ƒä¸€ä¸ªæ–°çš„Wasserstein GANã€‚æˆ‘ä»¬åªéœ€è°ƒç”¨ä»¥ä¸‹ä»£ç ç‰‡æ®µï¼š
- en: '[PRE22]'
  id: totrans-216
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Results with less mode collapse
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: å‡å°‘æ¨¡å¼åå¡Œçš„ç»“æœ
- en: 'The following code generates some new samples. The results are not perfect,
    but they are *much* better than we got before. The GAN has learned to cover a
    wider portion of the input data, and the shape of the distribution is closer to
    the ground truth that we know. We can generally see all nine modes represented,
    which is good, although some are not as strong as the others:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç ç”Ÿæˆä¸€äº›æ–°çš„æ ·æœ¬ã€‚ç»“æœå¹¶ä¸å®Œç¾ï¼Œä½†å®ƒä»¬æ¯”æˆ‘ä»¬ä¹‹å‰å¾—åˆ°çš„å¥½å¾—å¤šã€‚GANå·²ç»å­¦ä¼šäº†è¦†ç›–æ›´å¹¿æ³›çš„è¾“å…¥æ•°æ®ï¼Œåˆ†å¸ƒçš„å½¢çŠ¶æ›´æ¥è¿‘æˆ‘ä»¬çŸ¥é“çš„çœŸå€¼ã€‚æˆ‘ä»¬å¯ä»¥ä¸€èˆ¬åœ°çœ‹åˆ°æ‰€æœ‰ä¹ä¸ªæ¨¡å¼éƒ½è¢«è¡¨ç¤ºå‡ºæ¥ï¼Œè¿™æ˜¯å¥½çš„ï¼Œå°½ç®¡å…¶ä¸­ä¸€äº›ä¸å¦‚å…¶ä»–å¼ºçƒˆï¼š
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '![](../Images/CH09_UN10_Raff.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN10_Raff.png)'
- en: The mode-collapse issue is not solved by the Wasserstein approach, but it is
    greatly improved. A downside of the Wasserstein approach, and of most GANs, is
    that it requires many more iterations to converge. A common trick to improve WGAN
    training is to update the discriminator five times for every update to the generator,
    which further increases the total number of epochs to get the best possible results.
    You should do closer to 200 to 400 epochs for most problems to train a really
    good GAN. Even with WGAN, you will often see it collapse if you train it again.
    This is part of the notoriety of training a GAN, but more data and more training
    epochs will improve it.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: æ¨¡å¼åå¡Œé—®é¢˜å¹¶æ²¡æœ‰é€šè¿‡Wassersteinæ–¹æ³•å¾—åˆ°è§£å†³ï¼Œä½†å®ƒå¾—åˆ°äº†æå¤§çš„æ”¹å–„ã€‚Wassersteinæ–¹æ³•ä»¥åŠå¤§å¤šæ•°GANçš„ä¸€ä¸ªç¼ºç‚¹æ˜¯å®ƒéœ€è¦æ›´å¤šçš„è¿­ä»£æ‰èƒ½æ”¶æ•›ã€‚æé«˜WGANè®­ç»ƒçš„ä¸€ä¸ªå¸¸è§æŠ€å·§æ˜¯å¯¹äºç”Ÿæˆå™¨çš„æ¯æ¬¡æ›´æ–°ï¼Œæ›´æ–°åˆ¤åˆ«å™¨äº”æ¬¡ï¼Œè¿™è¿›ä¸€æ­¥å¢åŠ äº†è·å¾—æœ€ä½³å¯èƒ½ç»“æœæ‰€éœ€çš„æ€»epochæ•°ã€‚å¯¹äºå¤§å¤šæ•°é—®é¢˜ï¼Œä½ åº”è¯¥è¿›è¡Œå¤§çº¦200åˆ°400ä¸ªepochæ¥è®­ç»ƒä¸€ä¸ªçœŸæ­£å¥½çš„GANã€‚å³ä½¿ä½¿ç”¨WGANï¼Œå¦‚æœä½ å†æ¬¡è®­ç»ƒå®ƒï¼Œä½ ç»å¸¸ä¼šçœ‹åˆ°å®ƒåå¡Œã€‚è¿™æ˜¯è®­ç»ƒGANçš„è‡­åæ˜­è‘—çš„ä¸€éƒ¨åˆ†ï¼Œä½†æ›´å¤šçš„æ•°æ®å’Œæ›´å¤šçš„è®­ç»ƒepochå°†æé«˜å…¶æ€§èƒ½ã€‚
- en: 'With that said, letâ€™s go back to our original MNIST problem and see if our
    WGAN-GP improves the results. We redefine the latent dimension size as 128 and
    the output shape to match that of MNIST. Again we have a sigmoid output for G
    because MNIST values are all in the range of [0,1]. The following code trains
    this with our new approach:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: è¯è™½å¦‚æ­¤ï¼Œè®©æˆ‘ä»¬å›åˆ°æˆ‘ä»¬æœ€åˆçš„MNISTé—®é¢˜ï¼Œçœ‹çœ‹æˆ‘ä»¬çš„WGAN-GPæ˜¯å¦æé«˜äº†ç»“æœã€‚æˆ‘ä»¬é‡æ–°å®šä¹‰æ½œåœ¨ç»´åº¦å¤§å°ä¸º128ï¼Œè¾“å‡ºå½¢çŠ¶ä¸MNISTç›¸åŒ¹é…ã€‚æˆ‘ä»¬å†æ¬¡ä¸ºGä½¿ç”¨sigmoidè¾“å‡ºï¼Œå› ä¸ºMNISTçš„å€¼éƒ½åœ¨[0,1]çš„èŒƒå›´å†…ã€‚ä»¥ä¸‹ä»£ç ä½¿ç”¨æˆ‘ä»¬çš„æ–°æ–¹æ³•è¿›è¡Œè®­ç»ƒï¼š
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Note Why update the discriminator D more times than you update the generator
    G? So the discriminator has more changes to update and catch what the generator
    G is doing. Thatâ€™s desirable because G can only become as good as the discriminator
    D. This doesnâ€™t give too much of an unfair advantage to D, thanks to the complexity
    penalty.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼šä¸ºä»€ä¹ˆæ›´æ–°åˆ¤åˆ«å™¨Dçš„æ¬¡æ•°è¦å¤šäºæ›´æ–°ç”Ÿæˆå™¨Gçš„æ¬¡æ•°ï¼Ÿè¿™æ ·åˆ¤åˆ«å™¨å°±æœ‰æ›´å¤šæœºä¼šæ›´æ–°å¹¶æ•æ‰åˆ°ç”Ÿæˆå™¨Gçš„è¡Œä¸ºã€‚è¿™æ˜¯å¯å–çš„ï¼Œå› ä¸ºGåªèƒ½å˜å¾—å’Œåˆ¤åˆ«å™¨Dä¸€æ ·å¥½ã€‚ç”±äºå¤æ‚æ€§æƒ©ç½šï¼Œè¿™å¹¶æ²¡æœ‰ç»™Då¸¦æ¥å¤ªå¤§çš„ä¸å…¬å¹³ä¼˜åŠ¿ã€‚
- en: 'Now we generate some synthetic data, but we wonâ€™t look at the scores: the scores
    are no longer probabilities, so it is much harder to interpret them as a single
    value. This is why we made `scores` an optional argument for `plot_gen_imgs`:'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬ç”Ÿæˆä¸€äº›åˆæˆæ•°æ®ï¼Œä½†æˆ‘ä»¬ä¸ä¼šæŸ¥çœ‹åˆ†æ•°ï¼šåˆ†æ•°ä¸å†æ˜¯æ¦‚ç‡ï¼Œå› æ­¤å¾ˆéš¾å°†å…¶è§£é‡Šä¸ºå•ä¸ªå€¼ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†`scores`ä½œä¸º`plot_gen_imgs`çš„å¯é€‰å‚æ•°ï¼š
- en: '[PRE25]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: '![](../Images/CH09_UN11_Raff.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN11_Raff.png)'
- en: 'Overall, our MNIST GAN samples look much better than before. You should be
    able to find an example of all 10 digits in most samples, even if some of the
    digits are a little uglier or malformed. You should also see more variety of styles
    within a single specific digit! These are subjectively massive improvements in
    our GANâ€™s quality. One downside is that our 0s, 3s, and 8s are not as crisp as
    they were when we had mode collapse: our WGAN-GP could use more training epochs,
    and itâ€™s doing *more* than it was before. When the original GAN collapsed, it
    got to use its entire network to represent only three digits. Now each digit has
    less of the network because we are representing all 10 digits. This is part of
    why I recommend comparing your generated results to your real data to decide if
    the results are good. High-quality collapsed results can lure you into a false
    belief about the quality of or GAN.'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“è€Œè¨€ï¼Œæˆ‘ä»¬çš„MNIST GANæ ·æœ¬çœ‹èµ·æ¥æ¯”ä»¥å‰å¥½å¾—å¤šã€‚ä½ åº”è¯¥èƒ½åœ¨å¤§å¤šæ•°æ ·æœ¬ä¸­æ‰¾åˆ°æ‰€æœ‰10ä¸ªæ•°å­—çš„ä¾‹å­ï¼Œå³ä½¿æœ‰äº›æ•°å­—æœ‰ç‚¹éš¾çœ‹æˆ–ä¸è§„åˆ™ã€‚ä½ ä¹Ÿåº”è¯¥çœ‹åˆ°å•ä¸ªç‰¹å®šæ•°å­—å†…çš„æ ·å¼æ›´åŠ å¤šæ ·åŒ–ï¼è¿™äº›éƒ½æ˜¯æˆ‘ä»¬GANè´¨é‡çš„ä¸»è§‚å·¨å¤§æ”¹è¿›ã€‚ä¸€ä¸ªç¼ºç‚¹æ˜¯ï¼Œæˆ‘ä»¬çš„0ã€3å’Œ8ç°åœ¨æ²¡æœ‰æˆ‘ä»¬ä¹‹å‰åœ¨æ¨¡å¼å´©æºƒæ—¶é‚£ä¹ˆæ¸…æ™°ï¼šæˆ‘ä»¬çš„WGAN-GPå¯èƒ½éœ€è¦æ›´å¤šçš„è®­ç»ƒå‘¨æœŸï¼Œè€Œä¸”å®ƒç°åœ¨æ­£åœ¨åš*æ›´å¤š*çš„äº‹æƒ…ã€‚å½“åŸå§‹GANå´©æºƒæ—¶ï¼Œå®ƒå¯ä»¥ä½¿ç”¨æ•´ä¸ªç½‘ç»œæ¥è¡¨ç¤ºä»…ä¸‰ä¸ªæ•°å­—ã€‚ç°åœ¨æ¯ä¸ªæ•°å­—å ç”¨çš„ç½‘ç»œæ›´å°‘ï¼Œå› ä¸ºæˆ‘ä»¬æ­£åœ¨è¡¨ç¤ºæ‰€æœ‰10ä¸ªæ•°å­—ã€‚è¿™ä¹Ÿæ˜¯æˆ‘å»ºè®®å°†ç”Ÿæˆçš„ç»“æœä¸çœŸå®æ•°æ®æ¯”è¾ƒä»¥å†³å®šç»“æœæ˜¯å¦è‰¯å¥½çš„åŸå› ä¹‹ä¸€ã€‚é«˜è´¨é‡çš„å´©æºƒç»“æœå¯èƒ½ä¼šè®©ä½ å¯¹GANçš„è´¨é‡äº§ç”Ÿé”™è¯¯çš„çœ‹æ³•ã€‚
- en: 'We can also plot the losses of both the discriminator and generator again.
    The following code does this and uses a convolution to smooth the average loss
    so we can focus on the trends. When interpreting the loss for a WGAN-GP, remember
    that the generator G wants a large loss, and the discriminator D wants a small
    loss:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬è¿˜å¯ä»¥å†æ¬¡ç»˜åˆ¶åˆ¤åˆ«å™¨å’Œç”Ÿæˆå™¨çš„æŸå¤±ã€‚ä»¥ä¸‹ä»£ç æ‰§è¡Œæ­¤æ“ä½œï¼Œå¹¶ä½¿ç”¨å·ç§¯æ¥å¹³æ»‘å¹³å‡æŸå¤±ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥å…³æ³¨è¶‹åŠ¿ã€‚åœ¨è§£é‡ŠWGAN-GPçš„æŸå¤±æ—¶ï¼Œè¯·è®°ä½ï¼Œç”Ÿæˆå™¨Gå¸Œæœ›æœ‰ä¸€ä¸ªå¤§çš„æŸå¤±ï¼Œè€Œåˆ¤åˆ«å™¨Då¸Œæœ›æœ‰ä¸€ä¸ªå°çš„æŸå¤±ï¼š
- en: '[PRE26]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '![](../Images/CH09_UN12_Raff.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN12_Raff.png)'
- en: Interpreting this figure, G is improving with more training, which is good and
    a reason to keep training. But the discriminator D seems to have stopped improving,
    which may be a sign that D isnâ€™t strong enough to learn a better representation
    now. This is an issue because Gâ€™s quality will only improve as D gets better at
    the task. A major change we can make is to implement a convolutional GAN so the
    generator (and discriminator) get the structural prior of convolutions and (we
    hope) both do better!
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: è§£é‡Šè¿™ä¸ªå›¾ï¼ŒGéšç€è®­ç»ƒçš„è¿›è¡Œè€Œæ”¹è¿›ï¼Œè¿™æ˜¯å¥½äº‹ï¼Œä¹Ÿæ˜¯ç»§ç»­è®­ç»ƒçš„ç†ç”±ã€‚ä½†æ˜¯ï¼Œåˆ¤åˆ«å™¨Dä¼¼ä¹å·²ç»åœæ­¢äº†æ”¹è¿›ï¼Œè¿™å¯èƒ½æ˜¯ä¸€ä¸ªè¿¹è±¡ï¼Œè¡¨æ˜Dç°åœ¨ä¸è¶³ä»¥å­¦ä¹ æ›´å¥½çš„è¡¨ç¤ºã€‚è¿™æ˜¯ä¸€ä¸ªé—®é¢˜ï¼Œå› ä¸ºGçš„è´¨é‡åªæœ‰å½“Dåœ¨ä»»åŠ¡ä¸Šå˜å¾—æ›´å¥½æ—¶æ‰ä¼šæé«˜ã€‚æˆ‘ä»¬å¯ä»¥åšå‡ºçš„ä¸€ä¸ªä¸»è¦æ”¹å˜æ˜¯å®ç°å·ç§¯GANï¼Œè¿™æ ·ç”Ÿæˆå™¨ï¼ˆå’Œåˆ¤åˆ«å™¨ï¼‰å°±èƒ½è·å¾—å·ç§¯çš„ç»“æ„å…ˆéªŒï¼Œå¹¶ä¸”ï¼ˆæˆ‘ä»¬å¸Œæœ›ï¼‰ä¸¤è€…éƒ½èƒ½åšå¾—æ›´å¥½ï¼
- en: 9.4 Convolutional GAN
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4 å·ç§¯GAN
- en: As with most deep learning applications, the best way to improve your results
    with a GAN is to choose an architecture that makes sense for your problem. Since
    we are working on images, we can use a convolutional architecture to improve our
    results. This requires a bit of finagling around the latent representation z.
    Our code creates a vector of shape (*B*,*D*) for D dimensions in z. What shape
    should our latent be for a convolutional G?
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸å¤§å¤šæ•°æ·±åº¦å­¦ä¹ åº”ç”¨ä¸€æ ·ï¼Œè¦æ”¹å–„GANçš„ç»“æœï¼Œæœ€ä½³æ–¹æ³•æ˜¯é€‰æ‹©ä¸€ä¸ªé€‚åˆä½ é—®é¢˜çš„æ¶æ„ã€‚ç”±äºæˆ‘ä»¬æ­£åœ¨å¤„ç†å›¾åƒï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨å·ç§¯æ¶æ„æ¥æé«˜æˆ‘ä»¬çš„ç»“æœã€‚è¿™éœ€è¦å¯¹æ½œåœ¨è¡¨ç¤ºzè¿›è¡Œä¸€äº›è°ƒæ•´ã€‚æˆ‘ä»¬çš„ä»£ç ä¸ºzä¸­çš„Dç»´åº¦åˆ›å»ºäº†ä¸€ä¸ªå½¢çŠ¶ä¸º(*B*,*D*)çš„å‘é‡ã€‚å¯¹äºå·ç§¯Gï¼Œæˆ‘ä»¬çš„æ½œåœ¨åº”è¯¥æ˜¯ä»€ä¹ˆå½¢çŠ¶ï¼Ÿ
- en: 9.4.1 Â Designing a convolutional generator
  id: totrans-235
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 è®¾è®¡å·ç§¯ç”Ÿæˆå™¨
- en: One approach is to make a latent tensor of shape (*B*,*C*â€²,*W*â€²,*H*â€²), so it
    is already shaped like an image but does not have the correct output shape of
    (*B*,*C*,*W*,*H*). We make the width and height of this latent smaller than the
    real data and use transposed convolutions to expand up to the size of our real
    data. But for *C*â€², we want to use more channels than our original data has. This
    way, the model can learn to interpret the latent channels as having different
    kinds of meaning, and having only one or three channels would likely be too small.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸€ç§æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ªå½¢çŠ¶ä¸º(*B*,*C*â€²,*W*â€²,*H*â€²)çš„æ½œåœ¨å¼ é‡ï¼Œè¿™æ ·å®ƒå·²ç»åƒå›¾åƒä¸€æ ·å½¢çŠ¶ï¼Œä½†æ²¡æœ‰æ­£ç¡®çš„è¾“å‡ºå½¢çŠ¶(*B*,*C*,*W*,*H*)ã€‚æˆ‘ä»¬ä½¿è¿™ä¸ªæ½œåœ¨çš„å¼ é‡çš„å®½åº¦å’Œé«˜åº¦æ¯”çœŸå®æ•°æ®å°ï¼Œå¹¶ä½¿ç”¨è½¬ç½®å·ç§¯å°†å…¶æ‰©å±•åˆ°çœŸå®æ•°æ®çš„å¤§å°ã€‚ä½†å¯¹äº*C*â€²ï¼Œæˆ‘ä»¬å¸Œæœ›ä½¿ç”¨æ¯”åŸå§‹æ•°æ®æ›´å¤šçš„é€šé“ã€‚è¿™æ ·ï¼Œæ¨¡å‹å°±å¯ä»¥å­¦ä¼šå°†æ½œåœ¨é€šé“è§£é‡Šä¸ºå…·æœ‰ä¸åŒç±»å‹çš„æ„ä¹‰ï¼Œè€Œä¸”åªæœ‰ä¸€ä¸ªæˆ–ä¸‰ä¸ªé€šé“å¯èƒ½å¤ªå°ã€‚
- en: 'The following code sets up our parameters for training a *convolutional GAN*.
    First we determine the `start_size` that will be the initial width and height
    *W*â€² and *H*â€². We make *W*â€² = *W*/4 so we can do two rounds of expansion with
    a transposed convolution. We canâ€™t make it any smaller without some ugly code
    because that gets us down to 28/4 = 7 pixels: dividing by 2 again would be 3.5
    pixels, and thatâ€™s not something we can easily deal with. Next we define how many
    `latent_channels` *C*â€² as 16\. This is a hyperparameter to choose, and Iâ€™ve simply
    chosen a small value that I would increase if my results did not work well. Since
    we have a latent variable with a shape of (*B*,*C*â€²,*W*â€²,*H*â€²), we define the
    `in_shape` tuple to represent this as the counterpart to the `out_shape` tuple
    we have been using. Our network will use `in_shape` to reshape the input to G;
    `out_shape` will still be used to reshape the output of G and the input to D:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç è®¾ç½®äº†è®­ç»ƒ*å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ*çš„å‚æ•°ã€‚é¦–å…ˆï¼Œæˆ‘ä»¬ç¡®å®š`start_size`ï¼Œå®ƒå°†æ˜¯åˆå§‹å®½åº¦Wâ€²å’Œé«˜åº¦Hâ€²ã€‚æˆ‘ä»¬å°†Wâ€²è®¾ç½®ä¸ºW/4ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥é€šè¿‡è½¬ç½®å·ç§¯è¿›è¡Œä¸¤è½®æ‰©å±•ã€‚å¦‚æœæˆ‘ä»¬ä¸ä½¿ç”¨ä¸€äº›ä¸‘é™‹çš„ä»£ç ï¼Œå°±ä¸èƒ½å†å°äº†ï¼Œå› ä¸ºé‚£ä¼šè®©æˆ‘ä»¬é™åˆ°28/4=7åƒç´ ï¼šå†æ¬¡é™¤ä»¥2å°†æ˜¯3.5åƒç´ ï¼Œè¿™ä¸æ˜¯æˆ‘ä»¬å®¹æ˜“å¤„ç†çš„äº‹æƒ…ã€‚æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®šä¹‰`latent_channels`
    *C*â€²ä¸º16ã€‚è¿™æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œæˆ‘ç®€å•åœ°é€‰æ‹©äº†ä¸€ä¸ªè¾ƒå°çš„å€¼ï¼Œå¦‚æœæˆ‘çš„ç»“æœä¸å¥½ï¼Œæˆ‘ä¼šå¢åŠ å®ƒã€‚ç”±äºæˆ‘ä»¬æœ‰ä¸€ä¸ªå½¢çŠ¶ä¸º(*B*,*C*â€²,*W*â€²,*H*â€²)çš„æ½œåœ¨å˜é‡ï¼Œæˆ‘ä»¬å®šä¹‰`in_shape`å…ƒç»„æ¥è¡¨ç¤ºå®ƒï¼Œä½œä¸ºæˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨çš„`out_shape`å…ƒç»„çš„å¯¹åº”ç‰©ã€‚æˆ‘ä»¬çš„ç½‘ç»œå°†ä½¿ç”¨`in_shape`æ¥é‡å¡‘Gçš„è¾“å…¥ï¼›`out_shape`ä»å°†ç”¨äºé‡å¡‘Gçš„è¾“å‡ºå’ŒDçš„è¾“å…¥ï¼š
- en: '[PRE27]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: â¶ Initial width and height so we can do two rounds of transposed convolution
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ åˆå§‹å®½åº¦å’Œé«˜åº¦ï¼Œä»¥ä¾¿æˆ‘ä»¬å¯ä»¥è¿›è¡Œä¸¤è½®è½¬ç½®å·ç§¯
- en: â· Number of values we need in the latent space
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: â· æ½œåœ¨ç©ºé—´ä¸­éœ€è¦çš„å€¼æ•°é‡
- en: This code sets up the variables we need for our problem while also being easy
    to adjust for new problems. If you have larger images, experiment with more than
    two rounds of expansion and more than 16 latent channels, which can be easily
    achieved by altering the first two lines.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: æ­¤ä»£ç åœ¨è®¾ç½®æˆ‘ä»¬é—®é¢˜çš„å˜é‡åŒæ—¶ï¼Œä¹Ÿä¾¿äºè°ƒæ•´ä»¥é€‚åº”æ–°çš„é—®é¢˜ã€‚å¦‚æœä½ æœ‰æ›´å¤§çš„å›¾åƒï¼Œå¯ä»¥å°è¯•ä½¿ç”¨è¶…è¿‡ä¸¤è½®çš„æ‰©å±•å’Œè¶…è¿‡16ä¸ªæ½œåœ¨é€šé“ï¼Œè¿™å¯ä»¥é€šè¿‡æ›´æ”¹å‰ä¸¤è¡Œè½»æ¾å®ç°ã€‚
- en: Implementing convolutional helper functions
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å·ç§¯è¾…åŠ©å‡½æ•°
- en: 'Now we need a few variables before defining our convolutional architecture.
    We start our generator with `n_filters=32` filters, and we use a leak rate of
    0.2 for our LeakyReLU activation. We also define the kernel size as `k_size=5`.
    Usually we use more depth with a kernel size of 3 for our CNNs. For GANs, I often
    find that a slightly larger kernel size improves results. Our transposed convolutions,
    in particular, will have smoother outputs if we use a kernel size that is a multiple
    of the stride, so we give them a separate kernel size of `k_size_t=4`. This helps
    ensure that when the transposed convolution expands the output, each location
    in the output gets an even number of contributions to its value:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨å®šä¹‰æˆ‘ä»¬çš„å·ç§¯æ¶æ„ä¹‹å‰ï¼Œæˆ‘ä»¬éœ€è¦å‡ ä¸ªå˜é‡ã€‚æˆ‘ä»¬ä»¥`n_filters=32`ä¸ªè¿‡æ»¤å™¨å¼€å§‹æˆ‘ä»¬çš„ç”Ÿæˆå™¨ï¼Œå¹¶ä½¿ç”¨LeakyReLUæ¿€æ´»å‡½æ•°çš„æ¼ç‡0.2ã€‚æˆ‘ä»¬è¿˜å®šä¹‰äº†æ ¸å¤§å°ä¸º`k_size=5`ã€‚é€šå¸¸ï¼Œæˆ‘ä»¬ä½¿ç”¨3x3çš„æ ¸å¤§å°æ¥å¢åŠ CNNçš„æ·±åº¦ã€‚å¯¹äºGANï¼Œæˆ‘ç»å¸¸å‘ç°ç¨å¾®å¤§ä¸€ç‚¹çš„æ ¸å¤§å°å¯ä»¥æ”¹å–„ç»“æœã€‚ç‰¹åˆ«æ˜¯ï¼Œå¦‚æœæˆ‘ä»¬ä½¿ç”¨æ­¥é•¿çš„å€æ•°ä½œä¸ºæ ¸å¤§å°ï¼Œæˆ‘ä»¬çš„è½¬ç½®å·ç§¯å°†ä¼šæœ‰æ›´å¹³æ»‘çš„è¾“å‡ºï¼Œæ‰€ä»¥æˆ‘ä»¬ç»™å®ƒä»¬ä¸€ä¸ªå•ç‹¬çš„æ ¸å¤§å°`k_size_t=4`ã€‚è¿™æœ‰åŠ©äºç¡®ä¿å½“è½¬ç½®å·ç§¯æ‰©å±•è¾“å‡ºæ—¶ï¼Œè¾“å‡ºä¸­çš„æ¯ä¸ªä½ç½®éƒ½ä¼šå¾—åˆ°å…¶å€¼çš„å‡åŒ€æ•°é‡çš„è´¡çŒ®ï¼š
- en: '[PRE28]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: â¶ Number of channels in the latent space
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ æ½œåœ¨ç©ºé—´ä¸­çš„é€šé“æ•°
- en: â· Kernel size used by default for the convolutional GAN
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: â· é»˜è®¤ç”¨äºå·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œçš„æ ¸å¤§å°
- en: â¸ Default kernel size for transposed convolutions
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ è½¬ç½®å·ç§¯çš„é»˜è®¤æ ¸å¤§å°
- en: â¹ Helper function to create a hidden convolutional layer
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ åˆ›å»ºéšè—å·ç§¯å±‚çš„è¾…åŠ©å‡½æ•°
- en: âº Like the cnnLayer, but we use a transposed convolution to expand in size
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: âº ä¸cnnLayerç±»ä¼¼ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨è½¬ç½®å·ç§¯æ¥æ‰©å±•å¤§å°
- en: Implementing a convolutional GAN
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: å®ç°å·ç§¯ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ
- en: 'Now we define our CNN GAN. For G, we start with a `View(in_shape)` so the latent
    vector z becomes the desired input shape we specified: (*B*,*C*â€²,*W*â€²,*H*â€²). Next
    come a few rounds of convolutions, activation, and LN. Remember that for CNNs,
    we need to keep track of the width and height of the input, which is why we have
    the MNIST height of 28 by 28 encoded into our architecture. We follow our simple
    pattern of using two or three rounds of CNN layers before and after each transposed
    convolution:'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å®šä¹‰æˆ‘ä»¬çš„CNN GANã€‚å¯¹äºGï¼Œæˆ‘ä»¬ä»`View(in_shape)`å¼€å§‹ï¼Œè¿™æ ·æ½œåœ¨å‘é‡zå°±å˜æˆäº†æˆ‘ä»¬æŒ‡å®šçš„æ‰€éœ€è¾“å…¥å½¢çŠ¶ï¼š(*B*,*C*â€²,*W*â€²,*H*â€²)ã€‚æ¥ä¸‹æ¥æ˜¯ä¸€ç³»åˆ—çš„å·ç§¯ã€æ¿€æ´»å’ŒLNæ“ä½œã€‚è®°ä½ï¼Œå¯¹äºCNNï¼Œæˆ‘ä»¬éœ€è¦è·Ÿè¸ªè¾“å…¥çš„å®½åº¦å’Œé«˜åº¦ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬å°†MNISTçš„é«˜åº¦28x28ç¼–ç åˆ°æˆ‘ä»¬çš„æ¶æ„ä¸­ã€‚æˆ‘ä»¬éµå¾ªåœ¨æ¯æ¬¡è½¬ç½®å·ç§¯å‰åä½¿ç”¨ä¸¤åˆ°ä¸‰ä¸ªCNNå±‚çš„ç®€å•æ¨¡å¼ï¼š
- en: '[PRE29]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 9.4.2 Â Designing a convolutional discriminator
  id: totrans-253
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 è®¾è®¡å·ç§¯åˆ¤åˆ«å™¨
- en: 'Next, we implement the discriminator D. This takes a few more departures from
    our normal setup. Again, these are some tricks you can use to help improve your
    GANs:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: æ¥ä¸‹æ¥ï¼Œæˆ‘ä»¬å®ç°åˆ¤åˆ«å™¨ Dã€‚è¿™éœ€è¦ä»æˆ‘ä»¬çš„æ­£å¸¸è®¾ç½®ä¸­åšå‡ºä¸€äº›æ›´å¤šçš„æ”¹å˜ã€‚å†æ¬¡ï¼Œè¿™äº›æ˜¯ä¸€äº›å¯ä»¥å¸®åŠ©æ”¹è¿›ä½ çš„ GANs çš„æŠ€å·§ï¼š
- en: D and G are asymmetric. We can make D a smaller network than G to help give
    G an advantage in the competition. Even with the gradient penalty in WGAN-GP,
    D still has an easier job to learn. A rough rule of thumb Iâ€™ve found helpful as
    a starting point is D having two-thirds as many layers as G. Then plot the loss
    of D and G over a training run to see if D is getting stuck (loss does not decrease)
    and should become larger, or if G is stuck (loss does not increase), at which
    point maybe D should shrink or G should become larger.
  id: totrans-255
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: D å’Œ G æ˜¯éå¯¹ç§°çš„ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ D çš„ç½‘ç»œæ¯” G å°ï¼Œä»¥å¸®åŠ© G åœ¨ç«äº‰ä¸­å–å¾—ä¼˜åŠ¿ã€‚å³ä½¿åœ¨ WGAN-GP ä¸­çš„æ¢¯åº¦æƒ©ç½šä¸‹ï¼ŒD ä»ç„¶æœ‰ä¸€ä¸ªæ›´å®¹æ˜“çš„å­¦ä¹ ä»»åŠ¡ã€‚æˆ‘å‘ç°çš„ä¸€ä¸ªæœ‰ç”¨çš„ç»éªŒæ³•åˆ™æ˜¯
    D çš„å±‚æ•°æ˜¯ G çš„ä¸‰åˆ†ä¹‹äºŒã€‚ç„¶ååœ¨ä¸€ä¸ªè®­ç»ƒè¿è¡Œä¸­ç»˜åˆ¶ D å’Œ G çš„æŸå¤±ï¼Œä»¥æŸ¥çœ‹ D æ˜¯å¦é™·å…¥å›°å¢ƒï¼ˆæŸå¤±æ²¡æœ‰ä¸‹é™ï¼‰ï¼Œæ­¤æ—¶åº”è¯¥å˜å¾—æ›´å¤§ï¼Œæˆ–è€… G æ˜¯å¦é™·å…¥å›°å¢ƒï¼ˆæŸå¤±æ²¡æœ‰å¢åŠ ï¼‰ï¼Œæ­¤æ—¶ä¹Ÿè®¸
    D åº”è¯¥ç¼©å°æˆ– G åº”è¯¥å˜å¾—æ›´å¤§ã€‚
- en: Switch to average pooling with `nn.AvgPool2d` instead of max pooling. This helps
    maximize gradient flow because every pixel contributes equally to an answer, so
    all pixels get a share of the gradient.
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `nn.AvgPool2d` è€Œä¸æ˜¯æœ€å¤§æ± åŒ–åˆ‡æ¢åˆ°å¹³å‡æ± åŒ–ã€‚è¿™æœ‰åŠ©äºæœ€å¤§åŒ–æ¢¯åº¦æµåŠ¨ï¼Œå› ä¸ºæ¯ä¸ªåƒç´ éƒ½å¯¹ç­”æ¡ˆæœ‰ç›¸åŒçš„è´¡çŒ®ï¼Œæ‰€ä»¥æ‰€æœ‰åƒç´ éƒ½å¾—åˆ°æ¢¯åº¦çš„ä¸€éƒ¨åˆ†ã€‚
- en: End your network with some aggressive pooling, which is most easily done with
    the `AdaptiveAvgPool` or `AdaptiveMaxPool` function. This can help keep D from
    keying off the exact location of a value when what we really care about is the
    overall look of the content.
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `AdaptiveAvgPool` æˆ– `AdaptiveMaxPool` å‡½æ•°ç»“æŸä½ çš„ç½‘ç»œï¼Œè¿™æœ€å®¹æ˜“åšåˆ°ã€‚è¿™å¯ä»¥å¸®åŠ© D ä¸ä¾èµ–äºå€¼çš„ç²¾ç¡®ä½ç½®ï¼Œè€Œæˆ‘ä»¬æ‰€çœŸæ­£å…³å¿ƒçš„æ˜¯å†…å®¹çš„æ•´ä½“å¤–è§‚ã€‚
- en: 'The following code incorporates these ideas into the definition of D. Again,
    we are using LN, so we need to keep track of the height and width, starting at
    full size and then shrinking. We use using 4 Ã— 4 for the adaptive pooling size
    at the end, so the next linear layer has 4 Ã— 4 = 16 values per channel to take
    as input. If we were doing this on 64 Ã— 64 or 256 Ã— 256 images, I might push the
    adaptive pooling up to 7 Ã— 7 or 9 Ã— 9:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹ä»£ç å°†è¿™äº›æƒ³æ³•çº³å…¥ D çš„å®šä¹‰ä¸­ã€‚å†æ¬¡ï¼Œæˆ‘ä»¬ä½¿ç”¨ LNï¼Œå› æ­¤æˆ‘ä»¬éœ€è¦è·Ÿè¸ªé«˜åº¦å’Œå®½åº¦ï¼Œä»å…¨å°ºå¯¸å¼€å§‹ç„¶åç¼©å°ã€‚æˆ‘ä»¬æœ€ç»ˆä½¿ç”¨ 4 Ã— 4 ä½œä¸ºè‡ªé€‚åº”æ± åŒ–çš„å¤§å°ï¼Œå› æ­¤ä¸‹ä¸€ä¸ªçº¿æ€§å±‚æœ‰
    4 Ã— 4 = 16 ä¸ªå€¼æ¯ä¸ªé€šé“ä½œä¸ºè¾“å…¥ã€‚å¦‚æœæˆ‘ä»¬å¯¹ 64 Ã— 64 æˆ– 256 Ã— 256 çš„å›¾åƒåšè¿™ä»¶äº‹ï¼Œæˆ‘å¯èƒ½ä¼šå°†è‡ªé€‚åº”æ± åŒ–æå‡åˆ° 7 Ã— 7 æˆ–
    9 Ã— 9ï¼š
- en: '[PRE30]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: â¶ To avoid sparse gradients, we are using average instead of max pooling.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä¸ºäº†é¿å…ç¨€ç–æ¢¯åº¦ï¼Œæˆ‘ä»¬ä½¿ç”¨å¹³å‡æ± åŒ–è€Œä¸æ˜¯æœ€å¤§æ± åŒ–ã€‚
- en: â· This is adaptive pooling, so we know the size is 4 Ã— 4 at this point to be
    more aggressive in pooling (often helpful for convolutional GANs) and to make
    coding easier.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: â· è¿™æ˜¯ä¸€ç§è‡ªé€‚åº”æ± åŒ–ï¼Œå› æ­¤æˆ‘ä»¬çŸ¥é“åœ¨è¿™ä¸ªç‚¹ä¸Šå¤§å°æ˜¯ 4 Ã— 4ï¼Œä»¥ä¾¿åœ¨æ± åŒ–æ—¶æ›´åŠ æ¿€è¿›ï¼ˆé€šå¸¸å¯¹å·ç§¯ GANs æœ‰å¸®åŠ©ï¼‰å¹¶ä¸”ä½¿ç¼–ç æ›´å®¹æ˜“ã€‚
- en: Training and Inspecting our Convolutional GAN
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: è®­ç»ƒå’Œæ£€æŸ¥æˆ‘ä»¬çš„å·ç§¯ GAN
- en: 'By moving the `View` logic into the network instead of the training code, we
    can reuse the `train_wgan` function for our CNN GANs. The following code trains
    them up:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: é€šè¿‡å°† `View` é€»è¾‘ç§»åŠ¨åˆ°ç½‘ç»œè€Œä¸æ˜¯è®­ç»ƒä»£ç ä¸­ï¼Œæˆ‘ä»¬å¯ä»¥ä¸ºæˆ‘ä»¬çš„ CNN GANs é‡å¤ä½¿ç”¨ `train_wgan` å‡½æ•°ã€‚ä»¥ä¸‹ä»£ç è®­ç»ƒå®ƒä»¬ï¼š
- en: '[PRE31]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Ten epochs is not a lot of time for training a GAN, but next we visualize some
    random samples from the CNN GANs, and the digits look much better than they previously
    did. Again, it is not surprising that a convolutional approach works better on
    images, but we did need to learn a few extra tricks to make this work well:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: åä¸ªæ—¶æœŸå¯¹äºè®­ç»ƒä¸€ä¸ª GAN æ¥è¯´ä¸æ˜¯å¾ˆå¤šæ—¶é—´ï¼Œä½†æ¥ä¸‹æ¥æˆ‘ä»¬å¯è§†åŒ–äº†ä¸€äº› CNN GANs çš„éšæœºæ ·æœ¬ï¼Œæ•°å­—çœ‹èµ·æ¥æ¯”ä¹‹å‰å¥½å¾—å¤šã€‚å†æ¬¡ï¼Œå·ç§¯æ–¹æ³•åœ¨å›¾åƒä¸Šå·¥ä½œå¾—æ›´å¥½å¹¶ä¸ä»¤äººæƒŠè®¶ï¼Œä½†æˆ‘ä»¬ç¡®å®éœ€è¦å­¦ä¹ ä¸€äº›é¢å¤–çš„æŠ€å·§æ¥ä½¿è¿™å·¥ä½œå¾—å¾ˆå¥½ï¼š
- en: '[PRE32]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '![](../Images/CH09_UN13_Raff.png)'
  id: totrans-267
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN13_Raff.png)'
- en: 'Would more epochs of training improve our WGAN-GP? The following code again
    plots a smoothed version of the G and D losses over every batch. There is a clear
    trend that Gâ€™s score is increasing, which indicates that G is getting better at
    generating; but Dâ€™s score is flat-lining (not getting better or worse). More training
    might improve this, but itâ€™s not a guarantee. It would be better if Dâ€™s score
    was decreasing (because D wants negative values), making the graph expand in a
    funnel-like shape (the first 300 iterations have this shape). That would mean
    both G and D were improving, and we would be more confident that more epochs of
    training would improve the results of the GAN:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: æ›´å¤šçš„è®­ç»ƒè½®æ•°ä¼šæ”¹å–„æˆ‘ä»¬çš„WGAN-GPå—ï¼Ÿä»¥ä¸‹ä»£ç å†æ¬¡ç»˜åˆ¶äº†æ¯ä¸ªæ‰¹æ¬¡ä¸­Gå’ŒDæŸå¤±çš„å¹³æ»‘ç‰ˆæœ¬ã€‚æœ‰ä¸€ä¸ªæ˜æ˜¾çš„è¶‹åŠ¿ï¼ŒGçš„å¾—åˆ†åœ¨å¢åŠ ï¼Œè¿™è¡¨æ˜Gåœ¨ç”Ÿæˆæ–¹é¢å˜å¾—æ›´å¥½ï¼›ä½†Dçš„å¾—åˆ†å´ä¿æŒå¹³ç¨³ï¼ˆæ²¡æœ‰å˜å¥½æˆ–å˜åï¼‰ã€‚æ›´å¤šçš„è®­ç»ƒå¯èƒ½ä¼šæ”¹å–„è¿™ä¸€ç‚¹ï¼Œä½†è¿™å¹¶ä¸ä¿è¯ã€‚å¦‚æœDçš„å¾—åˆ†åœ¨ä¸‹é™ï¼ˆå› ä¸ºDæƒ³è¦è´Ÿå€¼ï¼‰ï¼Œä½¿å¾—å›¾è¡¨å‘ˆæ¼æ–—çŠ¶ï¼ˆå‰300æ¬¡è¿­ä»£å°±æœ‰è¿™ç§å½¢çŠ¶ï¼‰ï¼Œé‚£å°±æ›´å¥½äº†ã€‚è¿™æ„å‘³ç€Gå’ŒDéƒ½åœ¨æ”¹è¿›ï¼Œæˆ‘ä»¬æ›´æœ‰ä¿¡å¿ƒæ›´å¤šçš„è®­ç»ƒè½®æ•°ä¼šæ”¹å–„GANçš„ç»“æœï¼š
- en: '[PRE33]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '![](../Images/CH09_UN14_Raff.png)'
  id: totrans-270
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN14_Raff.png)'
- en: 9.5 Conditional GAN
  id: totrans-271
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 æ¡ä»¶GAN
- en: Another way to help improve the training of GANs is to create a *conditional
    GAN*. Conditional GANs are supervised instead of unsupervised because we use the
    label y that belongs to each data point x. This is a fairly simple change, as
    shown in figure 9.7.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§å¸®åŠ©æ”¹å–„GANè®­ç»ƒçš„æ–¹æ³•æ˜¯åˆ›å»ºä¸€ä¸ª*æ¡ä»¶GAN*ã€‚æ¡ä»¶GANæ˜¯ç›‘ç£å­¦ä¹ è€Œä¸æ˜¯æ— ç›‘ç£å­¦ä¹ ï¼Œå› ä¸ºæˆ‘ä»¬ä½¿ç”¨äº†å±äºæ¯ä¸ªæ•°æ®ç‚¹xçš„æ ‡ç­¾yã€‚å¦‚å›¾9.7æ‰€ç¤ºï¼Œè¿™æ˜¯ä¸€ä¸ªç›¸å½“ç®€å•çš„æ”¹å˜ã€‚
- en: '![](../Images/CH09_F07_Raff.png)'
  id: totrans-273
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F07_Raff.png)'
- en: Figure 9.7 Example of a conditional GAN where D made errors and G did well.
    The only change is that we include the label y of the data as an input to both
    G and D. G gets the label so it knows what to create, and D gets the label so
    it knows what it is looking for.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.7å±•ç¤ºäº†æ¡ä»¶ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANï¼‰çš„ä¾‹å­ï¼Œå…¶ä¸­Dï¼ˆåˆ¤åˆ«å™¨ï¼‰å‡ºç°äº†é”™è¯¯ï¼Œè€ŒGï¼ˆç”Ÿæˆå™¨ï¼‰è¡¨ç°è‰¯å¥½ã€‚å”¯ä¸€çš„æ”¹å˜æ˜¯æˆ‘ä»¬å°†æ•°æ®çš„æ ‡ç­¾yä½œä¸ºè¾“å…¥åŒæ—¶æä¾›ç»™Gå’ŒDã€‚Gé€šè¿‡è·å–æ ‡ç­¾æ¥çŸ¥é“å®ƒéœ€è¦åˆ›å»ºä»€ä¹ˆï¼Œè€ŒDé€šè¿‡è·å–æ ‡ç­¾æ¥çŸ¥é“å®ƒåœ¨å¯»æ‰¾ä»€ä¹ˆã€‚
- en: Instead of asking the model to predict y from x, we tell G to generate an example
    of y. You can think of a normal GAN as asking the model to generate any realistic
    data and a conditional GAN as asking the model to generate realistic data that
    we can classify as y. This helps the model by giving it extra information. It
    doesnâ€™t have to figure out on its own how many classes there are because you are
    tell the model G what kind of data to generate by providing the label y. To make
    this happen, we also provide the information about y to the discriminator D.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ä¸æ˜¯è¦æ±‚æ¨¡å‹ä»xé¢„æµ‹yï¼Œè€Œæ˜¯å‘Šè¯‰Gç”Ÿæˆyçš„ä¸€ä¸ªç¤ºä¾‹ã€‚ä½ å¯ä»¥å°†æ­£å¸¸çš„GANè§†ä¸ºè¦æ±‚æ¨¡å‹ç”Ÿæˆä»»ä½•çœŸå®æ•°æ®ï¼Œè€Œæ¡ä»¶GANåˆ™è¦æ±‚æ¨¡å‹ç”Ÿæˆæˆ‘ä»¬å¯ä»¥å°†å…¶åˆ†ç±»ä¸ºyçš„çœŸå®æ•°æ®ã€‚è¿™é€šè¿‡æä¾›é¢å¤–ä¿¡æ¯å¸®åŠ©æ¨¡å‹ã€‚å®ƒä¸éœ€è¦è‡ªå·±ç¡®å®šæœ‰å¤šå°‘ä¸ªç±»åˆ«ï¼Œå› ä¸ºé€šè¿‡æä¾›æ ‡ç­¾yï¼Œä½ å‘Šè¯‰æ¨¡å‹Gè¦ç”Ÿæˆä»€ä¹ˆç±»å‹çš„æ•°æ®ã€‚ä¸ºäº†å®ç°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬è¿˜å‘åˆ¤åˆ«å™¨Dæä¾›äº†å…³äºyçš„ä¿¡æ¯ã€‚
- en: Another way to think about this is that a conditional GAN allows us to learn
    a *one-to-many* mapping. All of our previous neural networks have been *one-to-one*.
    For any input x, there is one correct output y. But as figure 9.8 demonstrates,
    a conditional model lets us create many valid outputs for any one valid input.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: å¦ä¸€ç§æ€è€ƒæ–¹å¼æ˜¯ï¼Œæ¡ä»¶GANå…è®¸æˆ‘ä»¬å­¦ä¹ ä¸€ç§*ä¸€å¯¹ä¸€*æ˜ å°„ã€‚æˆ‘ä»¬ä¹‹å‰æ‰€æœ‰çš„ç¥ç»ç½‘ç»œéƒ½æ˜¯*ä¸€å¯¹ä¸€*çš„ã€‚å¯¹äºä»»ä½•è¾“å…¥xï¼Œéƒ½æœ‰ä¸€ä¸ªæ­£ç¡®çš„è¾“å‡ºyã€‚ä½†å¦‚å›¾9.8æ‰€ç¤ºï¼Œæ¡ä»¶æ¨¡å‹è®©æˆ‘ä»¬ä¸ºä»»ä½•æœ‰æ•ˆçš„è¾“å…¥åˆ›å»ºå¤šä¸ªæœ‰æ•ˆçš„è¾“å‡ºã€‚
- en: '![](../Images/CH09_F08_Raff.png)'
  id: totrans-277
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_F08_Raff.png)'
- en: Figure 9.8 Example of how a conditional GAN is a one-to-many mapping. The input
    is â€œcat,â€ and the output on the right shows multiple valid outputs. To get this
    diversity of outputs, the latent variable z gives G a way to create the multiple
    outputs.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾9.8å±•ç¤ºäº†æ¡ä»¶GANä½œä¸ºä¸€å¯¹ä¸€æ˜ å°„çš„ä¾‹å­ã€‚è¾“å…¥æ˜¯â€œçŒ«â€ï¼Œå³ä¾§çš„è¾“å‡ºæ˜¾ç¤ºäº†å¤šä¸ªæœ‰æ•ˆçš„è¾“å‡ºã€‚ä¸ºäº†è·å¾—è¿™ç§å¤šæ ·åŒ–çš„è¾“å‡ºï¼Œæ½œåœ¨å˜é‡zä¸ºGæä¾›äº†åˆ›å»ºå¤šä¸ªè¾“å‡ºçš„æ–¹æ³•ã€‚
- en: Having a one-to-many mapping with conditional GANs also allows us to start manipulating
    what G produces. Given a single latent vector z, we can ask the model to produce
    a 1 using *G*(**z**âˆ£*y*=1) or a 3 by using *G*(**z**âˆ£*y*=3). If we do both of
    these things using the same value of z, the generated 1 and 3 will share similar
    characteristics like line thickness and slant.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ¡ä»¶GANè¿›è¡Œä¸€å¯¹ä¸€æ˜ å°„ä¹Ÿå…è®¸æˆ‘ä»¬å¼€å§‹æ“çºµGç”Ÿæˆçš„ç»“æœã€‚ç»™å®šä¸€ä¸ªå•ä¸ªçš„æ½œåœ¨å‘é‡zï¼Œæˆ‘ä»¬å¯ä»¥è¦æ±‚æ¨¡å‹ä½¿ç”¨*G*(**z**âˆ£*y*=1)ç”Ÿæˆä¸€ä¸ª1ï¼Œæˆ–è€…ä½¿ç”¨*G*(**z**âˆ£*y*=3)ç”Ÿæˆä¸€ä¸ª3ã€‚å¦‚æœæˆ‘ä»¬ä½¿ç”¨ç›¸åŒçš„zå€¼åšè¿™ä¸¤ä»¶äº‹ï¼Œç”Ÿæˆçš„1å’Œ3å°†å…·æœ‰ç›¸ä¼¼çš„ç‰¹å¾ï¼Œå¦‚çº¿æ¡ç²—ç»†å’Œå€¾æ–œã€‚
- en: 9.5.1 Â Implementing a conditional GAN
  id: totrans-280
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 å®ç°æ¡ä»¶GAN
- en: 'We can implement a conditional GAN without much work. To do so, we first need
    to modify G and D to take two inputs: the latent z and a label y. Letâ€™s define
    a `ConditionalWrapper` class that does this for us. Our approach is that `Conditional-`
    `Wrapper` will take in a normal G or D network and use it as a subnetwork, similar
    to how we implemented U-Net. `ConditionalWrapper` takes z and y and combine them
    into a single new latent value áº‘. Then we pass the new latent áº‘ to the original
    network (either G or D).'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥ä¸è´¹å¹ç°ä¹‹åŠ›å®ç°æ¡ä»¶GANã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œæˆ‘ä»¬é¦–å…ˆéœ€è¦ä¿®æ”¹Gå’ŒDä»¥æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼šæ½œåœ¨å˜é‡zå’Œæ ‡ç­¾yã€‚è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ª`ConditionalWrapper`ç±»ï¼Œå®ƒä¸ºæˆ‘ä»¬å®Œæˆè¿™é¡¹å·¥ä½œã€‚æˆ‘ä»¬çš„æ–¹æ³•æ˜¯`Conditional-`
    `Wrapper`å°†æ¥å—ä¸€ä¸ªæ­£å¸¸çš„Gæˆ–Dç½‘ç»œï¼Œå¹¶å°†å…¶ç”¨ä½œå­ç½‘ç»œï¼Œç±»ä¼¼äºæˆ‘ä»¬å®ç°U-Netçš„æ–¹å¼ã€‚`ConditionalWrapper`æ¥å—zå’Œyï¼Œå¹¶å°†å®ƒä»¬ç»„åˆæˆä¸€ä¸ªæ–°çš„æ½œåœ¨å€¼áº‘ã€‚ç„¶åæˆ‘ä»¬å°†æ–°çš„æ½œåœ¨å€¼áº‘ä¼ é€’ç»™åŸå§‹ç½‘ç»œï¼ˆGæˆ–Dï¼‰ã€‚
- en: The following `Module` definition implements this idea. In the constructor,
    we create an `nn.Embedding` layer to convert the labels y into vectors the same
    size as z. We also create a `combiner` network, which takes an input twice the
    size of the latent vector z and returns an output the same size as z. This network
    is intentionally small, with no more than two hidden layers, so it is just enough
    to combine our two input values z and y.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹`Module`å®šä¹‰å®ç°äº†è¿™ä¸ªæƒ³æ³•ã€‚åœ¨æ„é€ å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬åˆ›å»ºäº†ä¸€ä¸ª`nn.Embedding`å±‚ï¼Œå°†æ ‡ç­¾yè½¬æ¢ä¸ºä¸zç›¸åŒå¤§å°çš„å‘é‡ã€‚æˆ‘ä»¬è¿˜åˆ›å»ºäº†ä¸€ä¸ª`combiner`ç½‘ç»œï¼Œå®ƒæ¥å—ä¸€ä¸ªå¤§å°æ˜¯æ½œåœ¨å‘é‡zä¸¤å€çš„è¾“å…¥ï¼Œå¹¶è¿”å›ä¸€ä¸ªä¸zç›¸åŒå¤§å°çš„è¾“å‡ºã€‚è¿™ä¸ªç½‘ç»œæ•…æ„å¾ˆå°ï¼Œæ²¡æœ‰è¶…è¿‡ä¸¤ä¸ªéšè—å±‚ï¼Œæ‰€ä»¥å®ƒåˆšå¥½è¶³å¤Ÿç»„åˆæˆ‘ä»¬çš„ä¸¤ä¸ªè¾“å…¥å€¼zå’Œyã€‚
- en: The `forward` function then gets to operate with very few steps. It embeds y
    into a vector. Since yâ€™s embedding and z have the same shape, we can concatenate
    them together to make a double-sized input. This goes into the `combiner` and
    then right into the original `net` (either G or D).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: '`forward`å‡½æ•°éšåå¯ä»¥ä»¥éå¸¸å°‘çš„æ­¥éª¤è¿›è¡Œæ“ä½œã€‚å®ƒå°†yåµŒå…¥åˆ°ä¸€ä¸ªå‘é‡ä¸­ã€‚ç”±äºyçš„åµŒå…¥å’Œzå…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œæˆ‘ä»¬å¯ä»¥å°†å®ƒä»¬è¿æ¥åœ¨ä¸€èµ·ä»¥å½¢æˆä¸€ä¸ªåŒå€å¤§å°çš„è¾“å…¥ã€‚è¿™ä¸ªè¾“å…¥è¿›å…¥`combiner`ï¼Œç„¶åç›´æ¥è¿›å…¥åŸå§‹`net`ï¼ˆGæˆ–Dï¼‰ã€‚'
- en: 'Hereâ€™s the code:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ä¸‹é¢æ˜¯ä»£ç ï¼š
- en: '[PRE34]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: â¶ Figures out the number of latent parameters from the latent shape
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç¡®å®šæ½œåœ¨å½¢çŠ¶çš„æ½œåœ¨å‚æ•°æ•°é‡
- en: â· Creates an embedding layer to convert labels to vectors
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: â· åˆ›å»ºä¸€ä¸ªåµŒå…¥å±‚ä»¥å°†æ ‡ç­¾è½¬æ¢ä¸ºå‘é‡
- en: â¸ In the forward function, we concatenate the label and original date into one
    vector. Then his combiner takes that extra-large tensor and creates a new tensor
    the size of just the original input_shape. This does the work of merging the conditional
    information (from label_embedding) into the latent vector.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ åœ¨å‰å‘å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬å°†æ ‡ç­¾å’ŒåŸå§‹æ•°æ®è¿æ¥æˆä¸€ä¸ªå‘é‡ã€‚ç„¶åè¿™ä¸ª`combiner`æ¥å—è¿™ä¸ªéå¸¸å¤§çš„å¼ é‡ï¼Œå¹¶åˆ›å»ºä¸€ä¸ªå¤§å°ä»…ä¸ºåŸå§‹`input_shape`çš„æ–°å¼ é‡ã€‚è¿™å®Œæˆäº†å°†æ¡ä»¶ä¿¡æ¯ï¼ˆæ¥è‡ªæ ‡ç­¾åµŒå…¥ï¼‰åˆå¹¶åˆ°æ½œåœ¨å‘é‡ä¸­çš„å·¥ä½œã€‚
- en: â¹ One FC layer
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ ä¸€ä¸ªå…¨è¿æ¥å±‚
- en: âº A second FC layer, but first the linear layer and activation are applied
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: âº ç¬¬äºŒä¸ªå…¨è¿æ¥å±‚ï¼Œä½†é¦–å…ˆåº”ç”¨çº¿æ€§å±‚å’Œæ¿€æ´»å‡½æ•°
- en: â» So we can reshape the output and apply normalizing based on the target output
    shape. This makes the conditional wrapper useful for linear and convolutional
    models.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: â» å› æ­¤æˆ‘ä»¬å¯ä»¥é‡å¡‘è¾“å‡ºå¹¶åŸºäºç›®æ ‡è¾“å‡ºå½¢çŠ¶è¿›è¡Œå½’ä¸€åŒ–ã€‚è¿™ä½¿å¾—æ¡ä»¶åŒ…è£…å™¨å¯¹çº¿æ€§æ¨¡å‹å’Œå·ç§¯æ¨¡å‹éå¸¸æœ‰ç”¨ã€‚
- en: â¼ The forward function takes an input and produces an output.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: â¼ å‰å‘å‡½æ•°æ¥æ”¶ä¸€ä¸ªè¾“å…¥å¹¶äº§ç”Ÿä¸€ä¸ªè¾“å‡ºã€‚
- en: â½ If no label was given, letâ€™s pick one at random.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: â½ å¦‚æœæ²¡æœ‰ç»™å‡ºæ ‡ç­¾ï¼Œæˆ‘ä»¬å¯ä»¥éšæœºé€‰æ‹©ä¸€ä¸ªã€‚
- en: â¾ Embeds the label and reshapes it as desired
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: â¾ å°†æ ‡ç­¾åµŒå…¥å¹¶æŒ‰éœ€é‡å¡‘
- en: â¿ Makes sure the label embd and data x have the same shape so we can concatenate
    them
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: â¿ ç¡®ä¿æ ‡ç­¾åµŒå…¥å’Œæ•°æ®xå…·æœ‰ç›¸åŒçš„å½¢çŠ¶ï¼Œè¿™æ ·æˆ‘ä»¬å°±å¯ä»¥å°†å®ƒä»¬è¿æ¥èµ·æ¥
- en: â“« Concatenates the latent input with the embedded label
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: â“« å°†æ½œåœ¨è¾“å…¥ä¸åµŒå…¥çš„æ ‡ç­¾è¿æ¥
- en: â“¬ Returns the result of the network on the combined inputs
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: â“¬ è¿”å›ç½‘ç»œåœ¨ç»„åˆè¾“å…¥ä¸Šçš„ç»“æœ
- en: 9.5.2 Â Training a conditional GAN
  id: totrans-298
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 Â è®­ç»ƒæ¡ä»¶GAN
- en: 'With this code, it is easy to convert our normal fully connected GANs to conditional
    GANs. The following snippet of code creates a new fully connected GAN. The only
    thing we change is to define the number of `classes=10` and wrap G and D independently
    using our new `ConditionalWrapper`:'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨æ­¤ä»£ç ï¼Œå°†æˆ‘ä»¬çš„æ™®é€šå…¨è¿æ¥ç”Ÿæˆå¯¹æŠ—ç½‘ç»œï¼ˆGANsï¼‰è½¬æ¢ä¸ºæ¡ä»¶GANså˜å¾—å®¹æ˜“ã€‚ä»¥ä¸‹ä»£ç ç‰‡æ®µåˆ›å»ºäº†ä¸€ä¸ªæ–°çš„å…¨è¿æ¥GANã€‚æˆ‘ä»¬å”¯ä¸€æ”¹å˜çš„æ˜¯å®šä¹‰`classes=10`çš„æ•°é‡ï¼Œå¹¶ä½¿ç”¨æˆ‘ä»¬æ–°çš„`ConditionalWrapper`ç‹¬ç«‹åŒ…è£…Gå’ŒDï¼š
- en: '[PRE35]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now we need a way to train these conditional models, because the `train_wgan`
    function does not use the labels. This is another easy change. We can define a
    new function `train_c_wgan` that has the *exact* same code, except that any time
    we have `G(noise)` in our code, we change it to `G(noise,Â class_label)`. Similarly,
    any time we see something like `D(real)`, we change it to `D(real,Â class_label)`.
    Thatâ€™s itâ€”we just add `,Â class_label)` any time we used `G` or `D`! This gives
    us the tools and code to create conditional GANs. The following block of code
    trains the ones we just defined:'
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬éœ€è¦ä¸€ç§æ–¹æ³•æ¥è®­ç»ƒè¿™äº›æ¡ä»¶æ¨¡å‹ï¼Œå› ä¸º `train_wgan` å‡½æ•°ä¸ä½¿ç”¨æ ‡ç­¾ã€‚è¿™æ˜¯ä¸€ä¸ªç®€å•çš„æ”¹åŠ¨ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ä¸€ä¸ªæ–°çš„å‡½æ•° `train_c_wgan`ï¼Œå®ƒå…·æœ‰å®Œå…¨ç›¸åŒçš„ä»£ç ï¼Œé™¤äº†æ¯æ¬¡æˆ‘ä»¬åœ¨ä»£ç ä¸­æœ‰
    `G(noise)` æ—¶ï¼Œæˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º `G(noise,Â class_label)`ã€‚åŒæ ·ï¼Œæ¯æ¬¡æˆ‘ä»¬çœ‹åˆ°ç±»ä¼¼ `D(real)` çš„å†…å®¹æ—¶ï¼Œæˆ‘ä»¬å°†å…¶æ›´æ”¹ä¸º
    `D(real,Â class_label)`ã€‚å°±æ˜¯è¿™æ ·â€”â€”æˆ‘ä»¬åªéœ€åœ¨æ¯æ¬¡ä½¿ç”¨ `G` æˆ– `D` æ—¶æ·»åŠ  `,Â class_label)`ï¼è¿™ä¸ºæˆ‘ä»¬æä¾›äº†åˆ›å»ºæ¡ä»¶
    GANs çš„å·¥å…·å’Œä»£ç ã€‚ä»¥ä¸‹ä»£ç å—è®­ç»ƒäº†æˆ‘ä»¬åˆšåˆšå®šä¹‰çš„æ¨¡å‹ï¼š
- en: '[PRE36]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 9.5.3 Â Controlling the generation with conditional GANs
  id: totrans-303
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.3 ä½¿ç”¨æ¡ä»¶ GANs æ§åˆ¶ç”Ÿæˆ
- en: Letâ€™s visualize the results of this new GAN and show how we can control the
    process at the same time. The following code generates 10 different latent vectors
    z and makes 10 copies of each z value. Then we create a `labels` tensor that counts
    from 0 to 9, covering all 10 classes. This way, we create *G*(**z**âˆ£*y*=0), *G*(**z**âˆ£*y*=1),
    â€¦, *G*(**z**âˆ£*y*=9).
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: è®©æˆ‘ä»¬å¯è§†åŒ–è¿™ä¸ªæ–° GAN çš„ç»“æœï¼Œå¹¶å±•ç¤ºæˆ‘ä»¬å¦‚ä½•åŒæ—¶æ§åˆ¶è¿™ä¸ªè¿‡ç¨‹ã€‚ä»¥ä¸‹ä»£ç ç”Ÿæˆ 10 ä¸ªä¸åŒçš„æ½œåœ¨å‘é‡ zï¼Œå¹¶ä¸ºæ¯ä¸ª z å€¼åˆ›å»º 10 ä¸ªå‰¯æœ¬ã€‚ç„¶åæˆ‘ä»¬åˆ›å»ºä¸€ä¸ª
    `labels` å¼ é‡ï¼Œä» 0 åˆ° 9 è®¡æ•°ï¼Œè¦†ç›–æ‰€æœ‰ 10 ä¸ªç±»åˆ«ã€‚è¿™æ ·ï¼Œæˆ‘ä»¬åˆ›å»º *G*(**z**âˆ£*y*=0), *G*(**z**âˆ£*y*=1),
    â€¦, *G*(**z**âˆ£*y*=9)ã€‚
- en: Each digit is generated while using the same latent vector. Because the conditional
    controls the class that is generated, the latent z is forced to learn the *style*.
    If you look across each row, you will see that in each case, some basic styles
    are maintained across all the outputs, irrespective of the class.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: æ¯ä¸ªæ•°å­—éƒ½æ˜¯åœ¨ä½¿ç”¨ç›¸åŒçš„æ½œåœ¨å‘é‡ç”Ÿæˆã€‚å› ä¸ºæ¡ä»¶æ§åˆ¶ç”Ÿæˆçš„ç±»åˆ«ï¼Œæ½œåœ¨ z è¢«è¿«å­¦ä¹  *é£æ ¼*ã€‚å¦‚æœä½ æŸ¥çœ‹æ¯ä¸€è¡Œï¼Œä½ å°†çœ‹åˆ°åœ¨æ¯ç§æƒ…å†µä¸‹ï¼Œä¸€äº›åŸºæœ¬é£æ ¼åœ¨æ‰€æœ‰è¾“å‡ºä¸­éƒ½è¢«ä¿æŒï¼Œæ— è®ºç±»åˆ«å¦‚ä½•ã€‚
- en: 'Hereâ€™s the code:'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: ä»¥ä¸‹æ˜¯ä»£ç ï¼š
- en: '[PRE37]'
  id: totrans-307
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: â¶ Generates 10 latent noise vectors, and repeats them 10 times. We reuse the
    same latent codes.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ç”Ÿæˆ 10 ä¸ªæ½œåœ¨å™ªå£°å‘é‡ï¼Œå¹¶é‡å¤ 10 æ¬¡ã€‚æˆ‘ä»¬é‡ç”¨ç›¸åŒçš„æ½œåœ¨ä»£ç ã€‚
- en: â· Counts from 0 to 9 and then wraps back around to 0\. This is done 10 times.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: â· ä» 0 åˆ° 9 è®¡æ•°ï¼Œç„¶åå›åˆ° 0ã€‚è¿™é‡å¤äº† 10 æ¬¡ã€‚
- en: â¸ The same latent in noise is used to generate 10 images, but we change the
    label each time.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ä½¿ç”¨ç›¸åŒçš„æ½œåœ¨å™ªå£°ç”Ÿæˆ 10 å¼ å›¾åƒï¼Œä½†æ¯æ¬¡éƒ½æ›´æ”¹æ ‡ç­¾ã€‚
- en: â¹ When we plot the results, we should see a grid of digits from 0 to 9, where
    each row uses the same latent vector and shares similar visual properties.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ å½“æˆ‘ä»¬ç»˜åˆ¶ç»“æœæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥çœ‹åˆ°ä» 0 åˆ° 9 çš„æ•°å­—ç½‘æ ¼ï¼Œå…¶ä¸­æ¯ä¸€è¡Œä½¿ç”¨ç›¸åŒçš„æ½œåœ¨å‘é‡ï¼Œå¹¶å…·æœ‰ç›¸ä¼¼çš„å¯è§†å±æ€§ã€‚
- en: '![](../Images/CH09_UN15_Raff.png)'
  id: totrans-312
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN15_Raff.png)'
- en: Notice that if the 0 at the start of a row has thick lines, all the digits in
    that row have thick lines. If 0 is slanted to the right, all the digits are slanted
    to the right. This is how conditional GANs let us control what is generated. We
    could extend this approach to even more conditioned properties, which would give
    us more control over what and how G generates its outputs. But this requires a
    lot of labeled outputs, which is now always a possibility.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: æ³¨æ„ï¼Œå¦‚æœä¸€è¡Œå¼€å¤´çš„ 0 æœ‰ç²—çº¿ï¼Œé‚£ä¹ˆè¯¥è¡Œä¸­æ‰€æœ‰çš„æ•°å­—éƒ½æœ‰ç²—çº¿ã€‚å¦‚æœ 0 å‘å³å€¾æ–œï¼Œé‚£ä¹ˆæ‰€æœ‰çš„æ•°å­—éƒ½å‘å³å€¾æ–œã€‚è¿™å°±æ˜¯æ¡ä»¶ GANs è®©æˆ‘ä»¬æ§åˆ¶ç”Ÿæˆå†…å®¹çš„æ–¹å¼ã€‚æˆ‘ä»¬å¯ä»¥å°†è¿™ç§æ–¹æ³•æ‰©å±•åˆ°æ›´å¤šæ¡ä»¶å±æ€§ï¼Œè¿™å°†ä½¿æˆ‘ä»¬èƒ½å¤Ÿæ›´å¤šåœ°æ§åˆ¶
    G ç”Ÿæˆè¾“å‡ºå†…å®¹å’Œæ–¹å¼ã€‚ä½†è¿™éœ€è¦å¤§é‡çš„æ ‡è®°è¾“å‡ºï¼Œè€Œè¿™ç°åœ¨æ€»æ˜¯æœ‰å¯èƒ½çš„ã€‚
- en: 9.6 Walking the latent space of GANs
  id: totrans-314
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 éå† GANs çš„æ½œåœ¨ç©ºé—´
- en: We have seen that GANs can do an impressive job creating fake data, and the
    latent representation z begins to learn interesting properties about the data
    as it trains. This is true even when we donâ€™t have any labels. But we can also
    control the results generated by a GAN by altering the latent vector z itself.
    Doing so lets us manipulate images by labeling only a few of them so we can determine
    the right way to alter z.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å·²ç»çœ‹åˆ° GANs å¯ä»¥åœ¨åˆ›å»ºå‡æ•°æ®æ–¹é¢åšå¾—éå¸¸å‡ºè‰²ï¼Œéšç€è®­ç»ƒçš„è¿›è¡Œï¼Œæ½œåœ¨è¡¨ç¤º z å¼€å§‹å­¦ä¹ å…³äºæ•°æ®çš„æœ‰è¶£å±æ€§ã€‚å³ä½¿æˆ‘ä»¬æ²¡æœ‰æ ‡ç­¾ï¼Œè¿™ä¹Ÿæ˜¯æ­£ç¡®çš„ã€‚ä½†æˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡æ”¹å˜æ½œåœ¨å‘é‡
    z æœ¬èº«æ¥æ§åˆ¶ GAN ç”Ÿæˆçš„ç»“æœã€‚è¿™æ ·åšå¯ä»¥è®©æˆ‘ä»¬é€šè¿‡ä»…æ ‡è®°å°‘é‡å›¾åƒæ¥æ“çºµå›¾åƒï¼Œä»è€Œç¡®å®šå¦‚ä½•æ­£ç¡®åœ°æ”¹å˜ zã€‚
- en: 9.6.1 Â Getting models from the Hub
  id: totrans-316
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.1 ä» Hub è·å–æ¨¡å‹
- en: Since training GANs is expensive, in this section we use the PyTorch *Hub* to
    download a pretrained GAN. The Hub is a repository where people can upload useful
    pretrained models, and PyTorch has built-in integration to download and use those
    models. Weâ€™ll download an example trained on higher-resolution images of faces
    for something more fun to look at than MNIST.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ç”±äºè®­ç»ƒ GAN æ˜¯æ˜‚è´µçš„ï¼Œåœ¨æœ¬èŠ‚ä¸­æˆ‘ä»¬ä½¿ç”¨ PyTorch *Hub* ä¸‹è½½ä¸€ä¸ªé¢„è®­ç»ƒçš„ GANã€‚Hub æ˜¯ä¸€ä¸ªäººä»¬å¯ä»¥ä¸Šä¼ æœ‰ç”¨é¢„è®­ç»ƒæ¨¡å‹çš„å­˜å‚¨åº“ï¼ŒPyTorch
    å†…ç½®äº†å¯¹è¿™äº›æ¨¡å‹çš„ä¸‹è½½å’Œä½¿ç”¨é›†æˆã€‚æˆ‘ä»¬å°†ä¸‹è½½ä¸€ä¸ªåœ¨æ›´é«˜åˆ†è¾¨ç‡çš„é¢éƒ¨å›¾åƒä¸Šè®­ç»ƒçš„ç¤ºä¾‹ï¼Œä»¥ä¾¿æ¯” MNIST æ›´æœ‰è¶£åœ°æŸ¥çœ‹ã€‚
- en: 'First things first: we load the desired GAN model from the Hub. This happens
    with the `hub.load` function, where the first argument is the repository to load
    from, and the following arguments depend on the repository. In this case, we load
    a model called PGAN that was trained on a high-resolution dataset of celebrities.
    We also load the `torchvision` package, which has vision-specific extensions to
    PyTorch:'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: é¦–å…ˆè¦åšçš„ï¼šæˆ‘ä»¬ä» Hub åŠ è½½æ‰€éœ€çš„ GAN æ¨¡å‹ã€‚è¿™æ˜¯é€šè¿‡ `hub.load` å‡½æ•°å®Œæˆçš„ï¼Œå…¶ä¸­ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦åŠ è½½çš„å­˜å‚¨åº“ï¼Œåç»­å‚æ•°å–å†³äºå­˜å‚¨åº“ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åŠ è½½äº†ä¸€ä¸ªåä¸º
    PGAN çš„æ¨¡å‹ï¼Œè¯¥æ¨¡å‹æ˜¯åœ¨åäººé«˜åˆ†è¾¨ç‡æ•°æ®é›†ä¸Šè®­ç»ƒçš„ã€‚æˆ‘ä»¬è¿˜åŠ è½½äº† `torchvision` åŒ…ï¼Œå®ƒä¸º PyTorch æä¾›äº†è§†è§‰ç‰¹å®šçš„æ‰©å±•ï¼š
- en: '[PRE38]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Specifics of the PGAN Hub Model
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: PGAN Hub æ¨¡å‹çš„å…·ä½“ç»†èŠ‚
- en: The code loaded into the Hub defined a `buildNoiseData` function, which takes
    the number of samples we want to generate and produces noise vectors to generate
    that many images. We can then use the modelâ€™s `test` method to generate images
    from the noise. Letâ€™s try it!
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: åŠ è½½åˆ° Hub çš„ä»£ç å®šä¹‰äº†ä¸€ä¸ª `buildNoiseData` å‡½æ•°ï¼Œå®ƒæ¥å—æˆ‘ä»¬æƒ³è¦ç”Ÿæˆçš„æ ·æœ¬æ•°é‡ï¼Œå¹¶ç”Ÿæˆç”¨äºç”Ÿæˆè¿™ä¹ˆå¤šå›¾åƒçš„å™ªå£°å‘é‡ã€‚ç„¶åæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ¨¡å‹çš„
    `test` æ–¹æ³•ä»å™ªå£°ç”Ÿæˆå›¾åƒã€‚è®©æˆ‘ä»¬è¯•è¯•å§ï¼
- en: '[PRE39]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Using the `torchvision` helper function to plot the images, you should see
    a synthetically generated man and woman:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ `torchvision` è¾…åŠ©å‡½æ•°ç»˜åˆ¶å›¾åƒï¼Œä½ åº”è¯¥ä¼šçœ‹åˆ°ä¸€ä¸ªåˆæˆçš„ç”·æ€§å’Œå¥³æ€§ï¼š
- en: '[PRE40]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '![](../Images/CH09_UN16_Raff.png)'
  id: totrans-325
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN16_Raff.png)'
- en: 'Just as with our GANs earlier, these images were created by learning to convert
    noise into realistic-looking images! If we print the noise values, we see randomly
    sampled values from the Gaussian distribution (there is no magic hidden under
    the hood):'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: å°±åƒæˆ‘ä»¬ä¹‹å‰çš„ GAN ä¸€æ ·ï¼Œè¿™äº›å›¾åƒæ˜¯é€šè¿‡å­¦ä¹ å°†å™ªå£°è½¬æ¢ä¸ºé€¼çœŸå›¾åƒåˆ›å»ºçš„ï¼å¦‚æœæˆ‘ä»¬æ‰“å°å™ªå£°å€¼ï¼Œæˆ‘ä»¬ä¼šçœ‹åˆ°æ¥è‡ªé«˜æ–¯åˆ†å¸ƒçš„éšæœºé‡‡æ ·å€¼ï¼ˆä¸‹é¢æ²¡æœ‰éšè—ä»»ä½•é­”æ³•ï¼‰ï¼š
- en: '[PRE41]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 9.6.2 Â Interpolating GAN output
  id: totrans-328
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.2 Â æ’å€¼ GAN è¾“å‡º
- en: 'A cool property of GANs is that the latent values z that are learned tend to
    be well behaved when you manipulate them mathematically. So if we take our two
    noise samples, we can interpolate between the noise vectors (say, 50% of one and
    50% of the other) to produce interpolations of the images. This is often called
    *walking the latent space*: if you have one latent vector and walk some distance
    to a second latent vector, you end up with something that represents a mix of
    your original and destination latents.'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: GAN çš„ä¸€ä¸ªé…·ç‰¹æ€§æ˜¯ï¼Œå½“ä½ ä»æ•°å­¦ä¸Šæ“ä½œæ—¶ï¼Œå­¦ä¹ åˆ°çš„æ½œåœ¨å€¼ z å¾€å¾€è¡¨ç°å¾—å¾ˆå¥½ã€‚æ‰€ä»¥å¦‚æœæˆ‘ä»¬å–æˆ‘ä»¬çš„ä¸¤ä¸ªå™ªå£°æ ·æœ¬ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨å™ªå£°å‘é‡ä¹‹é—´è¿›è¡Œæ’å€¼ï¼ˆæ¯”å¦‚è¯´ï¼Œä¸€ä¸ªçš„
    50% å’Œå¦ä¸€ä¸ªçš„ 50%ï¼‰ï¼Œä»¥ç”Ÿæˆå›¾åƒçš„æ’å€¼ã€‚è¿™é€šå¸¸è¢«ç§°ä¸º *åœ¨æ½œåœ¨ç©ºé—´ä¸­è¡Œèµ°*ï¼šå¦‚æœä½ æœ‰ä¸€ä¸ªæ½œåœ¨å‘é‡ï¼Œå¹¶èµ°åˆ°ç¬¬äºŒä¸ªæ½œåœ¨å‘é‡çš„ä¸€å®šè·ç¦»ï¼Œä½ æœ€ç»ˆå¾—åˆ°çš„æ˜¯ä»£è¡¨ä½ åŸå§‹å’Œç›®æ ‡æ½œåœ¨å€¼çš„æ··åˆä½“ã€‚
- en: 'If we apply this to our two samples, the male image slowly transitions to female,
    the hair darkens to brown, and the smile widens. In the code, we take eight steps
    in the latent space from the first image to the second, changing the ratio of
    how much each latent contributes to the result:'
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: å¦‚æœæˆ‘ä»¬å°†æ­¤åº”ç”¨äºæˆ‘ä»¬çš„ä¸¤ä¸ªæ ·æœ¬ï¼Œç”·æ€§å›¾åƒä¼šé€æ¸è¿‡æ¸¡åˆ°å¥³æ€§ï¼Œå¤´å‘å˜æš—æˆæ£•è‰²ï¼Œç¬‘å®¹å˜å®½ã€‚åœ¨ä»£ç ä¸­ï¼Œæˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªå›¾åƒåˆ°ç¬¬äºŒä¸ªå›¾åƒåœ¨æ½œåœ¨ç©ºé—´ä¸­ç§»åŠ¨äº†å…«æ­¥ï¼Œæ”¹å˜äº†æ¯ä¸ªæ½œåœ¨å¯¹ç»“æœè´¡çŒ®çš„æ¯”ä¾‹ï¼š
- en: '[PRE42]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: â¶ Place to save interpolated images
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä¿å­˜æ’å€¼å›¾åƒçš„ä½ç½®
- en: â· Takes step/steps of the first latent and (1 - step/steps) of the second, aka
    walking
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: â· å–ç¬¬ä¸€ä¸ªæ½œåœ¨å‘é‡çš„æ­¥é•¿/æ­¥æ•°å’Œç¬¬äºŒä¸ªçš„ (1 - æ­¥é•¿/æ­¥æ•°)ï¼Œä¹Ÿå°±æ˜¯è¡Œèµ°
- en: â¸ Generates images from the interpolations
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ä»æ’å€¼ä¸­ç”Ÿæˆå›¾åƒ
- en: â¹ When visualized, the generated outputs look like a mix.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: â¹ å½“å¯è§†åŒ–ç”Ÿæˆçš„è¾“å‡ºæ—¶ï¼Œçœ‹èµ·æ¥åƒæ˜¯ä¸€ç§æ··åˆã€‚
- en: '![](../Images/CH09_UN17_Raff.png)'
  id: totrans-336
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN17_Raff.png)'
- en: We can take this even further. If we are willing to label a few instances from
    our dataset, we can extract vectors with semantic meaning. Figure 9.9 shows how
    this might work at a high level.
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬ç”šè‡³å¯ä»¥æ›´è¿›ä¸€æ­¥ã€‚å¦‚æœæˆ‘ä»¬æ„¿æ„å¯¹æˆ‘ä»¬çš„æ•°æ®é›†ä¸­çš„å‡ ä¸ªå®ä¾‹è¿›è¡Œæ ‡è®°ï¼Œæˆ‘ä»¬å¯ä»¥æå–å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„å‘é‡ã€‚å›¾ 9.9 å±•ç¤ºäº†è¿™åœ¨é«˜å±‚æ¬¡ä¸Šå¯èƒ½çš„å·¥ä½œæ–¹å¼ã€‚
- en: '![](../Images/CH09_F09_Raff.png)'
  id: totrans-338
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F09_Raff.png)'
- en: 'Figure 9.9 Example of a generated image and the kinds of semantic vectors you
    might be able to find. The left shows the original latent and associated output
    from G. By adding these semantic vectors, we can alter what is generated. These
    are called latent vectors because we didnâ€™t tell the GAN what they are: they were
    hidden from the GAN, and it had to learn them on its own from patterns in the
    data.'
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: å›¾ 9.9 ç”Ÿæˆå›¾åƒçš„ç¤ºä¾‹ä»¥åŠä½ å¯èƒ½èƒ½æ‰¾åˆ°çš„è¯­ä¹‰å‘é‡ç±»å‹ã€‚å·¦ä¾§æ˜¾ç¤ºçš„æ˜¯ G çš„åŸå§‹æ½œåœ¨å˜é‡åŠå…¶ç›¸å…³è¾“å‡ºã€‚é€šè¿‡æ·»åŠ è¿™äº›è¯­ä¹‰å‘é‡ï¼Œæˆ‘ä»¬å¯ä»¥æ”¹å˜ç”Ÿæˆçš„å›¾åƒã€‚è¿™äº›è¢«ç§°ä¸ºæ½œåœ¨å‘é‡ï¼Œå› ä¸ºæˆ‘ä»¬æ²¡æœ‰å‘Šè¯‰
    GAN å®ƒä»¬æ˜¯ä»€ä¹ˆï¼šå®ƒä»¬å¯¹ GAN æ˜¯éšè—çš„ï¼ŒGAN å¿…é¡»ä»æ•°æ®æ¨¡å¼ä¸­è‡ªå·±å­¦ä¹ å®ƒä»¬ã€‚
- en: 'This basically means it is possible to find a vector **z**[smile] that we can
    add to any other latent vector to make someone smile, or subtract from another
    latent vector to remove a smile. The great thing is that we *never told G about
    any of these latent properties*. G learns to do this on its own, and if we can
    discover the semantic vectors, we can make these alterations! To do this, letâ€™s
    first generate a bunch of random images of people to use:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™åŸºæœ¬ä¸Šæ„å‘³ç€æˆ‘ä»¬å¯ä»¥æ‰¾åˆ°ä¸€ä¸ªå‘é‡ **z**[smile]ï¼Œæˆ‘ä»¬å¯ä»¥å°†å…¶æ·»åŠ åˆ°ä»»ä½•å…¶ä»–æ½œåœ¨å‘é‡ä¸­ï¼Œä½¿å…¶æŸäººå¾®ç¬‘ï¼Œæˆ–è€…ä»å¦ä¸€ä¸ªæ½œåœ¨å‘é‡ä¸­å‡å»ä»¥ç§»é™¤å¾®ç¬‘ã€‚å¥½äº‹æ˜¯ï¼Œæˆ‘ä»¬
    *ä»æœªå‘Šè¯‰ G å…³äºä»»ä½•è¿™äº›æ½œåœ¨å±æ€§çš„ä¿¡æ¯*ã€‚G å­¦ä¹ è‡ªå·±è¿™æ ·åšï¼Œå¦‚æœæˆ‘ä»¬èƒ½å‘ç°è¯­ä¹‰å‘é‡ï¼Œæˆ‘ä»¬å°±å¯ä»¥è¿›è¡Œè¿™äº›ä¿®æ”¹ï¼ä¸ºæ­¤ï¼Œè®©æˆ‘ä»¬é¦–å…ˆç”Ÿæˆä¸€äº›éšæœºçš„å›¾åƒæ¥ä½¿ç”¨ï¼š
- en: '[PRE43]'
  id: totrans-341
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: â¶ Gets consistent results
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ è·å–ä¸€è‡´çš„ç»“æœ
- en: â· Creates random generations
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: â· é€šè¿‡æ·»åŠ æ€§åˆ«å‘é‡åˆ°æˆ‘ä»¬çš„åŸå§‹æ½œåœ¨å‘é‡æ¥ç”Ÿæˆæ–°çš„å›¾åƒ
- en: â¸ Visualizes them
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ å¯è§†åŒ–å®ƒä»¬
- en: '![](../Images/CH09_UN18_Raff.png)'
  id: totrans-345
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN18_Raff.png)'
- en: 9.6.3 Â Labeling latent dimensions
  id: totrans-346
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.6.3 æ ‡è®°æ½œåœ¨ç»´åº¦
- en: 'Now we have 32 example faces. If we can identify certain properties that we
    care about for each generated image, we can label them and attempt to learn what
    parts of the noise control different aspects of the output.[Â³](#fn34) Letâ€™s identify
    the individuals who are male/female and those who are smiling or not. I created
    an array for each property, corresponding to each image, so we have 32 labels
    for â€œmaleâ€ and 32 labels for â€œsmiling.â€ Essentially, we are creating our own labels
    y for properties we care about, but we will be able to extract these semantic[â´](#fn35)
    vectors with very few labeled examples. G has already done the hard work of learning
    the concept on its own:'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æœ‰ 32 ä¸ªç¤ºä¾‹é¢å­”ã€‚å¦‚æœæˆ‘ä»¬èƒ½è¯†åˆ«å‡ºæˆ‘ä»¬å¯¹æ¯ä¸ªç”Ÿæˆçš„å›¾åƒå…³å¿ƒçš„æŸäº›å±æ€§ï¼Œæˆ‘ä»¬å°±å¯ä»¥å¯¹å®ƒä»¬è¿›è¡Œæ ‡è®°ï¼Œå¹¶å°è¯•å­¦ä¹ å“ªäº›å™ªå£°æ§åˆ¶äº†è¾“å‡ºçš„ä¸åŒæ–¹é¢ã€‚[Â³](#fn34)
    è®©æˆ‘ä»¬è¯†åˆ«å‡ºç”·æ€§å’Œå¥³æ€§ï¼Œä»¥åŠé‚£äº›å¾®ç¬‘æˆ–ä¸å¾®ç¬‘çš„äººã€‚æˆ‘ä¸ºæ¯ä¸ªå±æ€§åˆ›å»ºäº†ä¸€ä¸ªæ•°ç»„ï¼Œå¯¹åº”äºæ¯ä¸ªå›¾åƒï¼Œå› æ­¤æˆ‘ä»¬æœ‰ 32 ä¸ªâ€œç”·æ€§â€æ ‡ç­¾å’Œ 32 ä¸ªâ€œå¾®ç¬‘â€æ ‡ç­¾ã€‚æœ¬è´¨ä¸Šï¼Œæˆ‘ä»¬æ­£åœ¨ä¸ºæˆ‘ä»¬å…³å¿ƒçš„å±æ€§åˆ›å»ºè‡ªå·±çš„æ ‡ç­¾
    yï¼Œä½†æˆ‘ä»¬å°†èƒ½å¤Ÿç”¨å¾ˆå°‘çš„æ ‡è®°ç¤ºä¾‹æå–è¿™äº›è¯­ä¹‰[â´](#fn35) å‘é‡ã€‚G å·²ç»ç‹¬ç«‹å­¦ä¹ äº†æ¦‚å¿µï¼š
- en: '[PRE44]'
  id: totrans-348
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: â¶ Labels for which images are apparently male or smiling. I went through the
    generated images manually to create these lists.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ ä¸ºå“ªäº›å›¾åƒæ˜¾ç„¶æ˜¯ç”·æ€§æˆ–å¾®ç¬‘çš„è¿›è¡Œæ ‡è®°ã€‚æˆ‘æ‰‹åŠ¨æ£€æŸ¥ç”Ÿæˆçš„å›¾åƒä»¥åˆ›å»ºè¿™äº›åˆ—è¡¨ã€‚
- en: â· Converts the shape from (32) to (32, 1)
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: â· å°†å½¢çŠ¶ä» (32) è½¬æ¢ä¸º (32, 1)
- en: Calculating a semantic vector
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: è®¡ç®—è¯­ä¹‰å‘é‡
- en: 'Now we want to calculate the average male and average non-male vector. We do
    the same thing for smiling, so letâ€™s define a simple function that uses the binary
    labels `male` and `smile` to extract the vector that represents the difference.
    The more labels we generate, the better the results will be:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬æƒ³è®¡ç®—å¹³å‡ç”·æ€§å‘é‡å’Œå¹³å‡éç”·æ€§å‘é‡ã€‚æˆ‘ä»¬å¯¹å¾®ç¬‘ä¹ŸåšåŒæ ·çš„äº‹æƒ…ï¼Œæ‰€ä»¥è®©æˆ‘ä»¬å®šä¹‰ä¸€ä¸ªç®€å•çš„å‡½æ•°ï¼Œè¯¥å‡½æ•°ä½¿ç”¨äºŒè¿›åˆ¶æ ‡ç­¾ `male` å’Œ `smile`
    æ¥æå–è¡¨ç¤ºå·®å¼‚çš„å‘é‡ã€‚æˆ‘ä»¬ç”Ÿæˆçš„æ ‡ç­¾è¶Šå¤šï¼Œç»“æœè¶Šå¥½ï¼š
- en: '[PRE45]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: â¶ Gets the average of everything with class label 0
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ è·å–æ‰€æœ‰å…·æœ‰ç±»åˆ«æ ‡ç­¾ 0 çš„å¹³å‡å€¼
- en: â· Average of everything with class label 1
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: â· æ‰€æœ‰å…·æœ‰ç±»åˆ«æ ‡ç­¾ 1 çš„å¹³å‡å€¼
- en: â¸ Takes the difference between the averages to approximate the difference between
    the latent concepts
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ é€šè¿‡å–å¹³å‡å€¼ä¹‹é—´çš„å·®å¼‚æ¥è¿‘ä¼¼æ½œåœ¨æ¦‚å¿µä¹‹é—´çš„å·®å¼‚
- en: Manipulating images with semantic vectors
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ä½¿ç”¨è¯­ä¹‰å‘é‡æ“çºµå›¾åƒ
- en: 'Now we can use the `extractVec` function to extract a male gender vector. If
    we add it to any latent vector z, we get a new latent vector that becomes more
    male. Everything else in the generated image should generally stay the same, such
    as background, hair color, head position, etc. Letâ€™s try it:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ç°åœ¨æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ `extractVec` å‡½æ•°æå–ä¸€ä¸ªç”·æ€§æ€§åˆ«å‘é‡ã€‚å¦‚æœæˆ‘ä»¬å°†å…¶æ·»åŠ åˆ°ä»»ä½•æ½œåœ¨å‘é‡ z ä¸­ï¼Œæˆ‘ä»¬å°±ä¼šå¾—åˆ°ä¸€ä¸ªæ–°çš„æ½œåœ¨å‘é‡ï¼Œä½¿å…¶æ›´å…·ç”·æ€§ç‰¹å¾ã€‚ç”Ÿæˆçš„å›¾åƒä¸­çš„å…¶ä»–æ‰€æœ‰å†…å®¹é€šå¸¸åº”ä¿æŒä¸å˜ï¼Œä¾‹å¦‚èƒŒæ™¯ã€å‘è‰²ã€å¤´éƒ¨ä½ç½®ç­‰ã€‚è®©æˆ‘ä»¬è¯•è¯•ï¼š
- en: '[PRE46]'
  id: totrans-359
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: â¶ Extracts the gender vector
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: â¶ æå–æ€§åˆ«å‘é‡
- en: â· Generates new images by adding the gender vector to our original latent vectors
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: â· é€šè¿‡å°†æ€§åˆ«å‘é‡æ·»åŠ åˆ°æˆ‘ä»¬çš„åŸå§‹æ½œåœ¨å‘é‡æ¥ç”Ÿæˆæ–°çš„å›¾åƒ
- en: â¸ Plots the results
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: â¸ ç»˜åˆ¶ç»“æœ
- en: '![](../Images/CH09_UN19_Raff.png)'
  id: totrans-363
  prefs: []
  type: TYPE_IMG
  zh: '![å›¾ç‰‡](../Images/CH09_UN19_Raff.png)'
- en: 'Overall, the results are pretty decent. Not perfect, but we did not use many
    examples to discover these vectors. We can also subtract this gender vector to
    remove male-ness from the images. In this case, we would be making each image
    appear more feminine. The following code does this by simply changing `+` to `-`:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: æ€»ä½“æ¥è¯´ï¼Œç»“æœç›¸å½“ä¸é”™ã€‚è™½ç„¶ä¸æ˜¯å®Œç¾æ— ç¼ºï¼Œä½†æˆ‘ä»¬å¹¶æ²¡æœ‰ä½¿ç”¨å¾ˆå¤šä¾‹å­æ¥å‘ç°è¿™äº›å‘é‡ã€‚æˆ‘ä»¬è¿˜å¯ä»¥å‡å»è¿™ä¸ªæ€§åˆ«å‘é‡ï¼Œä»å›¾åƒä¸­å»é™¤ç”·æ€§ç‰¹å¾ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä¼šè®©æ¯ä¸ªå›¾åƒçœ‹èµ·æ¥æ›´å¥³æ€§åŒ–ã€‚ä»¥ä¸‹ä»£ç é€šè¿‡ç®€å•åœ°æ”¹å˜`+`ä¸º`-`æ¥å®ç°è¿™ä¸€ç‚¹ï¼š
- en: '[PRE47]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '![](../Images/CH09_UN20_Raff.png)'
  id: totrans-366
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN20_Raff.png)'
- en: 'Maybe you want everyone to be happy! We can do the same thing with the `smile`
    vector we extracted to make everyone smile more:'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: ä¹Ÿè®¸ä½ å¸Œæœ›æ¯ä¸ªäººéƒ½å¿«ä¹ï¼æˆ‘ä»¬å¯ä»¥ä½¿ç”¨æˆ‘ä»¬æå–çš„`smile`å‘é‡æ¥åšåŒæ ·çš„äº‹æƒ…ï¼Œè®©æ¯ä¸ªäººç¬‘å¾—æ›´å¤šï¼š
- en: '[PRE48]'
  id: totrans-368
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '![](../Images/CH09_UN21_Raff.png)'
  id: totrans-369
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_UN21_Raff.png)'
- en: 9.7 Ethics in deep learning
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 æ·±åº¦å­¦ä¹ ä¸­çš„ä¼¦ç†
- en: In chapter 4, we took a brief detour to talk about the importance of understanding
    how we are modeling the world and how using those models can impact the way others
    perceive the world. This is a conversation worth revisiting now that we have learned
    about GANs.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ç¬¬4ç« ä¸­ï¼Œæˆ‘ä»¬ç®€è¦åœ°åç¦»äº†ä¸»é¢˜ï¼Œè®¨è®ºäº†ç†è§£æˆ‘ä»¬å¦‚ä½•å»ºæ¨¡ä¸–ç•Œä»¥åŠä½¿ç”¨è¿™äº›æ¨¡å‹å¦‚ä½•å½±å“ä»–äººæ„ŸçŸ¥ä¸–ç•Œçš„é‡è¦æ€§ã€‚ç°åœ¨æˆ‘ä»¬å·²ç»äº†è§£äº†GANsï¼Œè¿™æ˜¯ä¸€ä¸ªå€¼å¾—å†æ¬¡è®¨è®ºçš„è¯é¢˜ã€‚
- en: The semantic concepts GANs learn are based on data and do not necessarily represent
    the truth about how the world works. For example, most people identify and present
    as male or female, which is reflected in our data, so a GAN learns a somewhat
    linear relationship between male and female as a uniform spectrum. But that does
    not match the reality for many people who do not fit into a bucket of just male
    or female. The manipulation of gender in an image using GANs is thus something
    you should give thought to before exposing it as part of any system.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: GANså­¦ä¹ åˆ°çš„è¯­ä¹‰æ¦‚å¿µåŸºäºæ•°æ®ï¼Œå¹¶ä¸ä¸€å®šä»£è¡¨ä¸–ç•Œè¿ä½œçš„çœŸå®æƒ…å†µã€‚ä¾‹å¦‚ï¼Œå¤§å¤šæ•°äººéƒ½ä¼šè®¤åŒå¹¶è¡¨ç°å‡ºè‡ªå·±æ˜¯ç”·æ€§æˆ–å¥³æ€§ï¼Œè¿™åœ¨æˆ‘ä»¬çš„æ•°æ®ä¸­æœ‰æ‰€ä½“ç°ï¼Œå› æ­¤GANå­¦ä¹ åˆ°ç”·æ€§å’Œå¥³æ€§ä¹‹é—´çš„ä¸€ç§ç›¸å¯¹çº¿æ€§çš„å…³ç³»ï¼Œä½œä¸ºä¸€ä¸ªç»Ÿä¸€çš„å…‰è°±ã€‚ä½†è¿™å¹¶ä¸ç¬¦åˆè®¸å¤šä¸é€‚åˆä»…é™äºç”·æ€§æˆ–å¥³æ€§ç±»åˆ«çš„äººçš„ç°å®ã€‚å› æ­¤ï¼Œåœ¨ä½¿ç”¨GANå¯¹å›¾åƒè¿›è¡Œæ€§åˆ«æ“çºµä¹‹å‰ï¼Œä½ åº”è¯¥ä¸‰æ€è€Œåè¡Œã€‚
- en: 'For any system (machine learning or otherwise) you build, you should ask yourself
    some simple questions: How will it impact the majority of people who use it? How
    will it impact the minority of people who use it? Are there people who benefit
    or are hurt by the system, and *should* those people receive that benefit or detriment?
    Could your deployment changes peopleâ€™s behavior in positive or negative ways or
    be misused by even well-intentioned users? Generally, think about *what could
    go wrong* at a micro and macro scale.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: å¯¹äºä½ æ„å»ºçš„ä»»ä½•ç³»ç»Ÿï¼ˆæœºå™¨å­¦ä¹ æˆ–å…¶ä»–ï¼‰ï¼Œä½ åº”è¯¥é—®è‡ªå·±ä¸€äº›ç®€å•çš„é—®é¢˜ï¼šå®ƒå°†å¦‚ä½•å½±å“ä½¿ç”¨è¯¥ç³»ç»Ÿçš„å¤šæ•°äººï¼Ÿå®ƒå°†å¦‚ä½•å½±å“ä½¿ç”¨è¯¥ç³»ç»Ÿçš„å°‘æ•°äººï¼Ÿæ˜¯å¦æœ‰å—ç›Šæˆ–å—æŸçš„äººï¼Œè¿™äº›äººæ˜¯å¦åº”è¯¥å¾—åˆ°è¿™ç§åˆ©ç›Šæˆ–æŸå®³ï¼Ÿä½ çš„éƒ¨ç½²æ˜¯å¦ä¼šä»¥ç§¯ææˆ–æ¶ˆæçš„æ–¹å¼æ”¹å˜äººä»¬çš„è¡Œä¸ºï¼Œæˆ–è€…ç”šè‡³è¢«æœ‰è‰¯å¥½æ„å›¾çš„ç”¨æˆ·æ»¥ç”¨ï¼Ÿé€šå¸¸ï¼Œä»å¾®è§‚å’Œå®è§‚å±‚é¢æ€è€ƒ*å¯èƒ½å‡ºé”™çš„åœ°æ–¹*ã€‚
- en: 'Iâ€™m not attempting to prescribe any philosophical or moral belief system on
    you. Ethics is a very complex topic, and I canâ€™t do it justice in a subsection
    of a chapter (and it is not the topic of this book). But with deep learning comes
    the ability to automate many different things. This can be a boon to society and
    free people from laborious and draining work, but it can also amplify and replicate
    undesirable inequalities at a newly efficient scale. For this reason, I want you
    to be aware and start training yourself to think about these kinds of considerations.
    Here are a few links to resources you can use to grow your understanding of these
    concerns:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: æˆ‘å¹¶ä¸æ˜¯è¯•å›¾å‘ä½ å¼ºåŠ ä»»ä½•å“²å­¦æˆ–é“å¾·ä¿¡å¿µä½“ç³»ã€‚ä¼¦ç†æ˜¯ä¸€ä¸ªéå¸¸å¤æ‚çš„è¯é¢˜ï¼Œæˆ‘æ— æ³•åœ¨ä¸€ä¸ªç« èŠ‚çš„å°èŠ‚ä¸­å…¬æ­£åœ°å¤„ç†å®ƒï¼ˆè€Œä¸”è¿™ä¹Ÿä¸æ˜¯æœ¬ä¹¦çš„ä¸»é¢˜ï¼‰ã€‚ä½†æ˜¯ï¼Œéšç€æ·±åº¦å­¦ä¹ çš„å‘å±•ï¼Œæˆ‘ä»¬èƒ½å¤Ÿè‡ªåŠ¨åŒ–è®¸å¤šä¸åŒçš„äº‹æƒ…ã€‚è¿™å¯èƒ½ä¼šç»™ç¤¾ä¼šå¸¦æ¥å¥½å¤„ï¼Œè®©äººä»¬ä»ç¹é‡å’Œè€—äººçš„å·¥ä½œä¸­è§£æ”¾å‡ºæ¥ï¼Œä½†å®ƒä¹Ÿå¯èƒ½ä»¥æ–°çš„é«˜æ•ˆè§„æ¨¡æ”¾å¤§å’Œå¤åˆ¶ä¸å¸Œæœ›å­˜åœ¨çš„å·®å¼‚ã€‚å› æ­¤ï¼Œæˆ‘å¸Œæœ›ä½ æœ‰æ‰€è­¦è§‰ï¼Œå¹¶å¼€å§‹è®­ç»ƒè‡ªå·±æ€è€ƒè¿™äº›ç±»å‹çš„è€ƒè™‘ã€‚ä»¥ä¸‹æ˜¯ä¸€äº›ä½ å¯ä»¥ä½¿ç”¨çš„èµ„æºé“¾æ¥ï¼Œä»¥å¸®åŠ©ä½ åŠ æ·±å¯¹è¿™äº›é—®é¢˜çš„ç†è§£ï¼š
- en: 'First, an older paper that captured many of these concerns in a broader perspective:
    B. Friedman and H. Nissenbaum, â€œBias in computer systems,â€ *ACM Trans. Inf. Syst.*
    vol. 14, no. 3, pp. 330-347, 1996,Â [https://doi.org/10.1145/230538.230561](https://doi.org/10.1145/230538.230561),
    [https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf](https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf).'
  id: totrans-375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: é¦–å…ˆï¼Œä¸€ç¯‡è¾ƒè€çš„æ–‡ç« ï¼Œå®ƒä»æ›´å¹¿æ³›çš„è§’åº¦æ•æ‰äº†è®¸å¤šè¿™äº›æ‹…å¿§ï¼šB. Friedmanå’ŒH. Nissenbaumï¼Œâ€œè®¡ç®—æœºç³»ç»Ÿä¸­çš„åè§â€ï¼Œ*ACM Trans.
    Inf. Syst.* ç¬¬14å·ï¼Œç¬¬3æœŸï¼Œç¬¬330-347é¡µï¼Œ1996å¹´ï¼Œ[https://doi.org/10.1145/230538.230561](https://doi.org/10.1145/230538.230561)ï¼Œ[https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf](https://nissenbaum.tech.cornell.edu/papers/Bias%20in%20Computer%20Systems.pdf)ã€‚
- en: I mentioned Kate Crawford (www.katecrawford.net) previously, and I also recommend
    Timnit Gebru ([https://scholar.google.com/citations?user=lemnAcwAAAAJ](https://scholar.google.com/citations?user=lemnAcwAAAAJ))
    and Reuben Binns ([https://www.reubenbinns.com/](https://www.reubenbinns.com/)).
  id: totrans-376
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä¹‹å‰æåˆ°äº†å‡¯ç‰¹Â·å…‹åŠ³ç¦å¾·ï¼ˆwww.katecrawford.netï¼‰ï¼Œæˆ‘è¿˜æ¨èè’‚å§†å°¼ç‰¹Â·ç›–å¸ƒå¢ï¼ˆ[https://scholar.google.com/citations?user=lemnAcwAAAAJ](https://scholar.google.com/citations?user=lemnAcwAAAAJ)ï¼‰å’Œé²æœ¬Â·å®¾æ–¯ï¼ˆ[https://www.reubenbinns.com/](https://www.reubenbinns.com/)ï¼‰ã€‚
- en: 'In particular, I like the paper â€œFairness in machine learning: lessons from
    political philosophy,â€ R. Binns, *Proceedings of Machine Learning Research* vol.
    81, pp. 1â€“11, 2018, [http://proceedings.mlr.press/v81/binns18a/binns18a.pdf](http://proceedings.mlr.press/v81/binns18a/binns18a.pdf).
    It is accessible and poses different perspectives on what the tricky concept of
    fairness means.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å°¤å…¶æ˜¯é²æœ¬Â·å®¾æ–¯çš„è®ºæ–‡â€œæœºå™¨å­¦ä¹ ä¸­çš„å…¬å¹³æ€§ï¼šæ”¿æ²»å“²å­¦çš„å¯ç¤ºâ€ï¼ŒR. Binnsï¼Œ*æœºå™¨å­¦ä¹ ç ”ç©¶ä¼šè®®è®ºæ–‡é›†* ç¬¬ 81 å·ï¼Œç¬¬ 1-11 é¡µï¼Œ2018 å¹´ï¼Œ[http://proceedings.mlr.press/v81/binns18a/binns18a.pdf](http://proceedings.mlr.press/v81/binns18a/binns18a.pdf)ã€‚å®ƒæ˜“äºç†è§£ï¼Œå¹¶ä»ä¸åŒçš„è§’åº¦é˜è¿°äº†æ£˜æ‰‹çš„å…¬å¹³æ€§æ¦‚å¿µæ„å‘³ç€ä»€ä¹ˆã€‚
- en: You donâ€™t need to master these things to be a good deep learning researcher
    or practitioner, but you *should* learn to think about and consider them. If you
    know enough to simply say, â€œThis might be a problem; we should get some guidance,â€
    you are better equipped than many practitioners today.
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: ä½ ä¸éœ€è¦æŒæ¡è¿™äº›çŸ¥è¯†å°±èƒ½æˆä¸ºä¸€åä¼˜ç§€çš„æ·±åº¦å­¦ä¹ ç ”ç©¶äººå‘˜æˆ–å®è·µè€…ï¼Œä½†ä½ *åº”è¯¥*å­¦ä¼šæ€è€ƒå’Œè€ƒè™‘è¿™äº›é—®é¢˜ã€‚å¦‚æœä½ çŸ¥é“å¾—è¶³å¤Ÿå¤šï¼Œä»…ä»…è¯´ï¼Œâ€œè¿™å¯èƒ½ä¼šæ˜¯ä¸€ä¸ªé—®é¢˜ï¼›æˆ‘ä»¬åº”è¯¥å¯»æ±‚ä¸€äº›æŒ‡å¯¼â€ï¼Œä½ æ¯”è®¸å¤šä»Šå¤©çš„å®è·µè€…æ›´æœ‰å‡†å¤‡ã€‚
- en: The ability to manipulate images, in particular, has led to a research area
    and problem known as *deep fakes*. GANs are a big component of this issue. They
    have been used to mimic voices and alter videos so the mouth movements match supplied
    audio, and they are a powerful tool to mislead others. You now know enough to
    start building or using systems that can do this, and with it comes a responsibility
    to consider those actions and ramifications. If you release code online, could
    it be easily misappropriated to harm others? What are the benefits of placing
    that code into the public, and do they outweigh the harms? These are situations
    where you may also want to think about how you could mitigate these harms. If
    you create a generator that could be misappropriated, perhaps you could work on
    a detector that can tell whether an image came from your model/approach and thus
    help detect any malignant use.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: å°¤å…¶æ˜¯æ“çºµå›¾åƒçš„èƒ½åŠ›ï¼Œå¯¼è‡´äº†è¢«ç§°ä¸º*æ·±åº¦ä¼ªé€ *çš„ç ”ç©¶é¢†åŸŸå’Œé—®é¢˜ã€‚GANs æ˜¯è¿™ä¸ªé—®é¢˜çš„å…³é”®ç»„æˆéƒ¨åˆ†ã€‚å®ƒä»¬è¢«ç”¨æ¥æ¨¡ä»¿å£°éŸ³å’Œæ”¹å˜è§†é¢‘ï¼Œä½¿å£å‹ä¸æä¾›çš„éŸ³é¢‘ç›¸åŒ¹é…ï¼Œå¹¶ä¸”å®ƒä»¬æ˜¯è¯¯å¯¼ä»–äººçš„å¼ºå¤§å·¥å…·ã€‚ä½ ç°åœ¨å·²ç»çŸ¥é“è¶³å¤Ÿå¤šçš„çŸ¥è¯†æ¥å¼€å§‹æ„å»ºæˆ–ä½¿ç”¨èƒ½å¤Ÿåšåˆ°è¿™ä¸€ç‚¹çš„ç³»ç»Ÿï¼Œè€Œè¿™ä¹Ÿå¸¦æ¥äº†è€ƒè™‘è¿™äº›è¡ŒåŠ¨å’Œåæœçš„è´£ä»»ã€‚å¦‚æœä½ å°†ä»£ç å‘å¸ƒåˆ°ç½‘ä¸Šï¼Œå®ƒæ˜¯å¦å®¹æ˜“è¢«æ»¥ç”¨æ¥ä¼¤å®³ä»–äººï¼Ÿå°†ä»£ç æ”¾å…¥å…¬å…±é¢†åŸŸçš„åˆ©ç›Šæ˜¯ä»€ä¹ˆï¼Ÿè¿™äº›åˆ©ç›Šæ˜¯å¦è¶…è¿‡äº†å±å®³ï¼Ÿåœ¨è¿™äº›æƒ…å†µä¸‹ï¼Œä½ ä¹Ÿå¯èƒ½æƒ³è¦è€ƒè™‘å¦‚ä½•å‡è½»è¿™äº›å±å®³ã€‚å¦‚æœä½ åˆ›å»ºäº†ä¸€ä¸ªå¯èƒ½è¢«æ»¥ç”¨çš„ç”Ÿæˆå™¨ï¼Œä¹Ÿè®¸ä½ å¯ä»¥è‡´åŠ›äºå¼€å‘ä¸€ä¸ªæ£€æµ‹å™¨ï¼Œå¯ä»¥åˆ¤æ–­å›¾åƒæ˜¯å¦æ¥è‡ªä½ çš„æ¨¡å‹/æ–¹æ³•ï¼Œä»è€Œå¸®åŠ©æ£€æµ‹ä»»ä½•æ¶æ„ä½¿ç”¨ã€‚
- en: These may seem like abstract problems, but as you build your skills and abilities,
    they will be real concerns. For example, a recent report[âµ](#fn36) has shown that
    deep fakes are being used to swindle people out of hundreds of thousands of dollars.
    My goal is not to force you into any particular action; as I said earlier, ethics
    is not a black-and-white issue with easy answers. But it is something you should
    begin to give thoughtful consideration when doing your work.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: è¿™äº›é—®é¢˜å¯èƒ½çœ‹èµ·æ¥å¾ˆæŠ½è±¡ï¼Œä½†éšç€ä½ æŠ€èƒ½å’Œèƒ½åŠ›çš„æå‡ï¼Œå®ƒä»¬å°†æˆä¸ºå®é™…é—®é¢˜ã€‚ä¾‹å¦‚ï¼Œæœ€è¿‘çš„ä¸€ä»½æŠ¥å‘Š[âµ](#fn36)æ˜¾ç¤ºï¼Œæ·±åº¦ä¼ªé€ æ­£è¢«ç”¨æ¥éª—å–äººä»¬æ•°åä¸‡ç¾å…ƒã€‚æˆ‘çš„ç›®æ ‡ä¸æ˜¯å¼ºè¿«ä½ é‡‡å–ä»»ä½•ç‰¹å®šçš„è¡ŒåŠ¨ï¼›æ­£å¦‚æˆ‘ä¹‹å‰æ‰€è¯´çš„ï¼Œé“å¾·ä¸æ˜¯ä¸€ä¸ªé»‘ç™½åˆ†æ˜ã€ç­”æ¡ˆç®€å•çš„é—®é¢˜ã€‚ä½†è¿™æ˜¯ä½ åœ¨å·¥ä½œæ—¶åº”è¯¥å¼€å§‹æ·±æ€ç†Ÿè™‘çš„é—®é¢˜ã€‚
- en: Exercises
  id: totrans-381
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: ç»ƒä¹ 
- en: Share and discuss your solutions on the Manning online platform at Inside Deep
    Learning Exercises ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945)).
    Once you submit your own answers, you will be able to see the solutions submitted
    by other readers, and see which ones the author judges to be the best.
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: åœ¨ Manning åœ¨çº¿å¹³å° Inside Deep Learning ç»ƒä¹ ä¸­åˆ†äº«å’Œè®¨è®ºä½ çš„è§£å†³æ–¹æ¡ˆ([https://liveproject.manning.com/project/945](https://liveproject.manning.com/project/945))ã€‚ä¸€æ—¦ä½ æäº¤äº†è‡ªå·±çš„ç­”æ¡ˆï¼Œä½ å°†èƒ½å¤Ÿçœ‹åˆ°å…¶ä»–è¯»è€…æäº¤çš„è§£å†³æ–¹æ¡ˆï¼Œå¹¶çœ‹åˆ°ä½œè€…è®¤ä¸ºå“ªäº›æ˜¯æœ€å¥½çš„ã€‚
- en: Another trick for improving a convolutional generator G is to start with one
    or two fully connected hidden layers and reshape their output into a tensor (*B*,*C*â€²,*W*â€²,*H*â€²).
    Try implementing this yourself. Do you think the results look better?
  id: totrans-383
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æé«˜å·ç§¯ç”Ÿæˆå™¨ G çš„å¦ä¸€ä¸ªæŠ€å·§æ˜¯ä»ä¸€ä¸ªæˆ–ä¸¤ä¸ªå…¨è¿æ¥çš„éšè—å±‚å¼€å§‹ï¼Œå¹¶å°†å®ƒä»¬çš„è¾“å‡ºé‡å¡‘ä¸ºä¸€ä¸ªå¼ é‡ (*B*,*C*â€²,*W*â€²,*H*â€²)ã€‚è¯•ç€äº²è‡ªå®ç°è¿™ä¸ªæ–¹æ³•ã€‚ä½ è®¤ä¸ºç»“æœçœ‹èµ·æ¥æ›´å¥½å—ï¼Ÿ
- en: Our `combiner` network in the `ConditionalWrapper` class is designed for fully
    connected networks. This would be a problem for our discriminator D to use. Modify
    the `ConditionalWrapper` class to define a small CNN for the `combiner` when the
    `input_shape` indicates that the `main_network` is a CNN. Use this to train a
    conditional CNN GAN.
  id: totrans-384
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬åœ¨ `ConditionalWrapper` ç±»ä¸­çš„ `combiner` ç½‘ç»œæ˜¯ä¸ºå…¨è¿æ¥ç½‘ç»œè®¾è®¡çš„ã€‚è¿™å°†æ˜¯æˆ‘ä»¬åˆ¤åˆ«å™¨ D ä½¿ç”¨æ—¶çš„é—®é¢˜ã€‚ä¿®æ”¹ `ConditionalWrapper`
    ç±»ï¼Œå½“ `input_shape` æŒ‡ç¤º `main_network` æ˜¯ CNN æ—¶ï¼Œä¸º `combiner` å®šä¹‰ä¸€ä¸ªå°å‹ CNNã€‚ä½¿ç”¨æ­¤æ–¹æ³•è®­ç»ƒæ¡ä»¶
    CNN GANã€‚
- en: One challenge of manipulating a new image x is getting a latent vector z for
    it. Modify the `train_wgan` function to take in an optional network E that is
    the *encoder* network. E takes an image x and tries to predict the latent vector
    z that generates x. So it will have a loss of âˆ¥*E*(*G*(*z*)) âˆ’ *z*âˆ¥[2]. Test the
    encoder, and visualize the images generated by *G*(*z*), then *G*(*E*(*G*(*Z*))),
    and then *G*(*E*(*G*(*E*(*G*(*Z*))))) (i.e., generate an image, then generate
    the encoded version of that image, and then generate the encoded version of the
    previous image).
  id: totrans-385
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: æ“ä½œæ–°å›¾åƒ x çš„ä¸€ä¸ªæŒ‘æˆ˜æ˜¯ä¸ºå…¶è·å–ä¸€ä¸ªæ½œåœ¨å‘é‡ zã€‚ä¿®æ”¹ `train_wgan` å‡½æ•°ä»¥æ¥å—ä¸€ä¸ªå¯é€‰çš„ç½‘ç»œ Eï¼Œè¯¥ç½‘ç»œæ˜¯ *ç¼–ç å™¨* ç½‘ç»œã€‚E æ¥å—å›¾åƒ
    x å¹¶å°è¯•é¢„æµ‹ç”Ÿæˆ x çš„æ½œåœ¨å‘é‡ zã€‚å› æ­¤ï¼Œå®ƒå°†æœ‰ä¸€ä¸ªæŸå¤± âˆ¥*E*(*G*(*z*)) âˆ’ *z*âˆ¥[2]ã€‚æµ‹è¯•ç¼–ç å™¨ï¼Œå¹¶å¯è§†åŒ– *G*(*z*) ç”Ÿæˆçš„å›¾åƒï¼Œç„¶å
    *G*(*E*(*G*(*Z*)))ï¼Œç„¶å *G*(*E*(*G*(*E*(*G*(*Z*)))))ï¼ˆå³ç”Ÿæˆä¸€ä¸ªå›¾åƒï¼Œç„¶åç”Ÿæˆè¯¥å›¾åƒçš„ç¼–ç ç‰ˆæœ¬ï¼Œç„¶åç”Ÿæˆå‰ä¸€ä¸ªå›¾åƒçš„ç¼–ç ç‰ˆæœ¬ï¼‰ã€‚
- en: You can make GANs that solve complex tasks like *inpainting*, where you need
    to fill in a missing part of an image. Replace generator G with the U-Net architecture
    from the previous chapter. Use `RandomErasing` transform to create noisy inputs
    ![](../Images/tilde_x.png) that are the input to your GAN. The discriminator remains
    the same, and there is no longer a latent variable z. Train this model, and check
    how well it does at the inpainting task.
  id: totrans-386
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½ å¯ä»¥åˆ›å»ºè§£å†³å¤æ‚ä»»åŠ¡ï¼ˆå¦‚ *ä¿®å¤*ï¼‰çš„ GANï¼Œå…¶ä¸­ä½ éœ€è¦å¡«å……å›¾åƒçš„ç¼ºå¤±éƒ¨åˆ†ã€‚ç”¨ä¸Šä¸€ç« çš„ U-Net æ¶æ„æ›¿æ¢ç”Ÿæˆå™¨ Gã€‚ä½¿ç”¨ `RandomErasing`
    è½¬æ¢åˆ›å»ºå™ªå£°è¾“å…¥ ![](../Images/tilde_x.png)ï¼Œè¿™æ˜¯ GAN çš„è¾“å…¥ã€‚åˆ¤åˆ«å™¨ä¿æŒä¸å˜ï¼Œå¹¶ä¸”ä¸å†æœ‰æ½œåœ¨å˜é‡ zã€‚è®­ç»ƒæ­¤æ¨¡å‹ï¼Œå¹¶æ£€æŸ¥å®ƒåœ¨ä¿®å¤ä»»åŠ¡ä¸Šçš„è¡¨ç°å¦‚ä½•ã€‚
- en: Once you get exercise 3 working, make a new version where your U-Net G takes
    in the noisy image ![](../Images/tilde_x.png) and a latent vector z. How does
    this model behave differently than the first one?
  id: totrans-387
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä¸€æ—¦ä½ è®©ç»ƒä¹  3 è¿è¡Œèµ·æ¥ï¼Œå°±åˆ›å»ºä¸€ä¸ªæ–°çš„ç‰ˆæœ¬ï¼Œå…¶ä¸­ä½ çš„ U-Net G æ¥æ”¶å™ªå£°å›¾åƒ ![](../Images/tilde_x.png) å’Œæ½œåœ¨å‘é‡
    zã€‚è¿™ä¸ªæ¨¡å‹ä¸ç¬¬ä¸€ä¸ªæ¨¡å‹çš„è¡Œä¸ºæœ‰ä½•ä¸åŒï¼Ÿ
- en: '**Challenging:** Read the paper â€œImage-to-Image Translation with ConditionalAdversarial
    Networksâ€ ([https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004)).
    It defines a GAN called pix2pix. See if you can understand most of the paper.
    If you feel daring, try to implement it yourself, as you have now learned about
    and used every component in the paper!'
  id: totrans-388
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**æŒ‘æˆ˜æ€§ï¼š** é˜…è¯»è®ºæ–‡â€œImage-to-Image Translation with Conditional Adversarial Networksâ€
    ([https://arxiv.org/abs/1611.07004](https://arxiv.org/abs/1611.07004))ã€‚å®ƒå®šä¹‰äº†ä¸€ä¸ªåä¸º
    pix2pix çš„ GANã€‚çœ‹çœ‹ä½ æ˜¯å¦èƒ½ç†è§£è®ºæ–‡çš„å¤§éƒ¨åˆ†å†…å®¹ã€‚å¦‚æœä½ è§‰å¾—æœ‰å‹‡æ°”ï¼Œå°è¯•è‡ªå·±å®ç°å®ƒï¼Œå› ä¸ºä½ å·²ç»äº†è§£å¹¶ä½¿ç”¨äº†è®ºæ–‡ä¸­çš„æ¯ä¸ªç»„ä»¶ï¼'
- en: Using the PGAN model, generate more images and come up with your ownsemantic
    vectors to manipulate them.
  id: totrans-389
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: ä½¿ç”¨ PGAN æ¨¡å‹ç”Ÿæˆæ›´å¤šå›¾åƒï¼Œå¹¶åˆ›å»ºè‡ªå·±çš„è¯­ä¹‰å‘é‡æ¥æ“ä½œå®ƒä»¬ã€‚
- en: Summary
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: æ‘˜è¦
- en: GANs work by setting up two networks that play a game against each other. Each
    helps the other learn.
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN é€šè¿‡è®¾ç½®ä¸¤ä¸ªç›¸äº’å¯¹æŠ—çš„ç½‘ç»œæ¥å·¥ä½œã€‚æ¯ä¸ªç½‘ç»œéƒ½å¸®åŠ©å¯¹æ–¹å­¦ä¹ ã€‚
- en: GANs are a generative modeling approach, which means the generator G model has
    a harder task than the discriminator D. This mismatch in difficulty contributes
    to a problem called mode collapse, where the generator focuses only on the easier
    parts of the data.
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN æ˜¯ä¸€ç§ç”Ÿæˆå»ºæ¨¡æ–¹æ³•ï¼Œè¿™æ„å‘³ç€ç”Ÿæˆå™¨ G æ¨¡å‹æ¯”åˆ¤åˆ«å™¨ D æ¨¡å‹æœ‰æ›´å›°éš¾çš„ä»»åŠ¡ã€‚è¿™ç§éš¾åº¦å·®å¼‚å¯¼è‡´äº†ä¸€ä¸ªç§°ä¸ºæ¨¡å¼åå¡Œçš„é—®é¢˜ï¼Œå…¶ä¸­ç”Ÿæˆå™¨åªå…³æ³¨æ•°æ®çš„è¾ƒå®¹æ˜“çš„éƒ¨åˆ†ã€‚
- en: A number of nuanced tricks to reduce vanishing gradients help us train GANs
    faster and better for both fully connected and convolutional architectures.
  id: totrans-393
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: å‡ ç§å‡å°‘æ¢¯åº¦æ¶ˆå¤±çš„ç»†å¾®æŠ€å·§å¸®åŠ©æˆ‘ä»¬æ›´å¿«ã€æ›´å¥½åœ°è®­ç»ƒ GANï¼Œæ— è®ºæ˜¯å…¨è¿æ¥è¿˜æ˜¯å·ç§¯æ¶æ„ã€‚
- en: GANs can suffer from mode collapse, causing them to over-focus on the most common
    or easiest thing instead of learning to generate a diversity of outputs.
  id: totrans-394
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GAN å¯èƒ½ä¼šé­å—æ¨¡å¼åå¡Œï¼Œå¯¼è‡´å®ƒä»¬è¿‡åº¦å…³æ³¨æœ€å¸¸è§æˆ–æœ€ç®€å•çš„äº‹ç‰©ï¼Œè€Œä¸æ˜¯å­¦ä¹ ç”Ÿæˆå¤šæ ·åŒ–çš„è¾“å‡ºã€‚
- en: The Wasserstein GAN adds a penalty to the discriminator that prevents it from
    becoming too good at the GAN game, helping reduce mode collapse.
  id: totrans-395
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Wasserstein GAN åœ¨åˆ¤åˆ«å™¨ä¸Šå¢åŠ äº†ä¸€ä¸ªæƒ©ç½šé¡¹ï¼Œé˜²æ­¢å®ƒè¿‡äºæ“…é•¿ GAN æ¸¸æˆï¼Œä»è€Œæœ‰åŠ©äºå‡å°‘æ¨¡å¼åå¡Œã€‚
- en: We can condition the output of a GAN on some labels, allowing us to make one-to-many
    models and control what the GAN generates.
  id: totrans-396
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: æˆ‘ä»¬å¯ä»¥åœ¨æŸäº›æ ‡ç­¾ä¸Šå¯¹ GAN çš„è¾“å‡ºè¿›è¡Œæ¡ä»¶åŒ–ï¼Œä½¿æˆ‘ä»¬èƒ½å¤Ÿåˆ›å»ºä¸€å¯¹ä¸€æˆ–å¤šå¯¹ä¸€æ¨¡å‹ï¼Œå¹¶æ§åˆ¶ GAN ç”Ÿæˆçš„å†…å®¹ã€‚
- en: GANs learn semantically meaningful latent representations. We can extract vectors
    that repeat a concept and use them to manipulate images.
  id: totrans-397
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANså­¦ä¹ å…·æœ‰è¯­ä¹‰æ„ä¹‰çš„æ½œåœ¨è¡¨ç¤ºã€‚æˆ‘ä»¬å¯ä»¥æå–é‡å¤æŸä¸ªæ¦‚å¿µçš„å‘é‡ï¼Œå¹¶ä½¿ç”¨å®ƒä»¬æ¥æ“çºµå›¾åƒã€‚
- en: GANs have elevated the need to consider the ethical implications of your models,
    and you should always ask yourself what could go wrong when deploying a model.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GANsæé«˜äº†è€ƒè™‘æ¨¡å‹ä¼¦ç†å½±å“çš„éœ€æ±‚ï¼Œå¹¶ä¸”åœ¨éƒ¨ç½²æ¨¡å‹æ—¶ï¼Œä½ åº”è¯¥å§‹ç»ˆé—®è‡ªå·±å¯èƒ½ä¼šå‡ºä»€ä¹ˆé—®é¢˜ã€‚
- en: '* * *'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: Â¹ Introduced by M. Arjovsky, S. Chintala, and L. Bottou, â€œWasserstein generative
    adversarial networks,â€ *Proceedings of the 34th International Conference on Machine
    Learning*, vol. 70, pp. 214â€“223, 2017\. We show the version by I. Gulrajani, F.
    Ahmed, M. Arjovsky, V. Dumoulin, and A. C. Courville, â€œImproved training of Wasserstein
    GANs,â€ *Advances in Neural Information Processing Systems*, vol. 30, pp. 5767â€“5777,
    2017.[â†©](#fnref32)
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: Â¹ ç”±M. Arjovskyã€S. Chintalaå’ŒL. Bottouæå‡ºï¼Œâ€œWassersteinç”Ÿæˆå¯¹æŠ—ç½‘ç»œâ€ï¼Œ*ç¬¬34å±Šå›½é™…æœºå™¨å­¦ä¹ ä¼šè®®è®ºæ–‡é›†*ï¼Œç¬¬70å·ï¼Œç¬¬214â€“223é¡µï¼Œ2017å¹´ã€‚æˆ‘ä»¬å±•ç¤ºäº†I.
    Gulrajaniã€F. Ahmedã€M. Arjovskyã€V. Dumoulinå’ŒA. C. Courvilleçš„ç‰ˆæœ¬ï¼Œâ€œæ”¹è¿›Wasserstein GANçš„è®­ç»ƒâ€ï¼Œ*ç¥ç»ä¿¡æ¯å¤„ç†ç³»ç»Ÿè¿›å±•*ï¼Œç¬¬30å·ï¼Œç¬¬5767â€“5777é¡µï¼Œ2017å¹´ã€‚[â†©](#fnref32)
- en: Â² There is a WGAN that came first, but I find WGAN-GP easier to use and more
    consistent in its behavior[â†©](#fnref33)
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: Â² è™½ç„¶æœ‰ä¸€ä¸ªå…ˆå‡ºç°çš„WGANï¼Œä½†æˆ‘å‘ç°WGAN-GPæ›´å®¹æ˜“ä½¿ç”¨ï¼Œå¹¶ä¸”å…¶è¡Œä¸ºæ›´ä¸€è‡´ã€‚[â†©](#fnref33)
- en: Â³ Better ways to do this is an active research problem, but our simple approach
    works shockingly well.[â†©](#fnref34)
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: Â³ æ›´å¥½çš„æ–¹æ³•æ˜¯ä¸€ä¸ªæ´»è·ƒçš„ç ”ç©¶é—®é¢˜ï¼Œä½†æˆ‘ä»¬çš„ç®€å•æ–¹æ³•æ•ˆæœæƒŠäººã€‚[â†©](#fnref34)
- en: â´ People have many different mental models of what they mean by *semantic*,
    and itâ€™s a pet peeve for many researchers. We are not going down that rabbit hole.[â†©](#fnref35)
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: â´ äººä»¬å¯¹äºâ€œè¯­ä¹‰â€çš„å«ä¹‰æœ‰ç€è®¸å¤šä¸åŒçš„å¿ƒç†æ¨¡å‹ï¼Œè¿™å¯¹è®¸å¤šç ”ç©¶äººå‘˜æ¥è¯´æ˜¯ä¸ªçƒ¦æ¼ã€‚æˆ‘ä»¬ä¸ä¼šé™·å…¥è¿™ä¸ªå…”å­æ´ã€‚[â†©](#fnref35)
- en: âµ L. Edmonds, â€œScammer used deepfake video to impersonate U.S. Admiral on Skype
    chat and swindle nearly $300,000 out of a California widow,â€ *Daily Mail*, 2020,
    [http://mng.bz/2j0X](http://mng.bz/2j0X).[â†©](#fnref36)
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: âµ L. Edmondsï¼Œâ€œè¯ˆéª—è€…ä½¿ç”¨æ·±åº¦ä¼ªé€ è§†é¢‘åœ¨SkypeèŠå¤©ä¸­å†’å……ç¾å›½æµ·å†›ä¸Šå°†ï¼Œä»åŠ å·å¯¡å¦‡é‚£é‡Œéª—èµ°äº†è¿‘30ä¸‡ç¾å…ƒï¼Œâ€*æ¯æ—¥é‚®æŠ¥*ï¼Œ2020å¹´ï¼Œ[http://mng.bz/2j0X](http://mng.bz/2j0X)ã€‚[â†©](#fnref36)
