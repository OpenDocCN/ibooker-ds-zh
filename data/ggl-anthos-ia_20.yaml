- en: Appendix C. Compute environment running on VMware
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 附录C. 在VMware上运行的计算环境
- en: Jarosław Gajewski
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Jarosław Gajewski
- en: 'This chapter covers:'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖：
- en: Understanding VMware and Anthos architecture
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解VMware和Anthos架构
- en: Deployment of the management plane
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理平面的部署
- en: Deploying a user cluster
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户集群的部署
- en: Anthos networking load-balancer options
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos网络负载均衡器选项
- en: As you already know, Google provides a managed Kubernetes service called Google
    Kubernetes Engine (GKE) hosted on their infrastructure. This managed solution
    has solved multiple problems for Kubernetes consumers including lifecycle management,
    security and physical resource delivery. While deploying applications in the Cloud
    has been growing each year, many enterprises still have a significant footprint
    of resources in their own data centers.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所知，谷歌提供了一种名为Google Kubernetes Engine (GKE) 的托管Kubernetes服务，该服务托管在其基础设施上。这种托管解决方案已为Kubernetes消费者解决了多个问题，包括生命周期管理、安全和物理资源交付。虽然每年在云中部署应用程序都在增长，但许多企业在其自己的数据中心中仍然拥有大量的资源。
- en: Google recognized that organizations may have various reasons to keep certain
    workloads on-prem, including latency concerns, data concerns, and other regulatory
    requirements that may limit Cloud usage. To help companies address workloads that
    must remain on-prem but still want to leverage Cloud native technologies like
    Kubernetes, on-prem GKE was born - now known as Anthos.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 谷歌认识到，组织可能出于各种原因而希望将某些工作负载保留在本地，包括延迟担忧、数据担忧以及可能限制云使用的其他监管要求。为了帮助公司解决必须保留在本地但仍然希望利用Kubernetes等云原生技术的工作负载，本地GKE应运而生——现在被称为Anthos。
- en: On-prem implementations introduce some requirements that companies must provide
    to allow provisioning of Anthos infrastructure components. We will not go into
    details of constraints that are related to a particular version of Anthos, or
    the VMware requirements since it’s always evolving. In general there is a requirement
    to provide VMware vSphere resources[^([1])](#ftn1) that can be used for hosting
    management control planes, Kubernetes infrastructure and application workloads.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 本地实施引入了一些要求，公司必须提供以允许配置Anthos基础设施组件。我们不会深入探讨与特定版本的Anthos相关的约束条件，或VMware要求，因为它们始终在演变。一般来说，需要提供VMware
    vSphere资源[^([1])](#ftn1)，这些资源可用于托管管理控制平面、Kubernetes基础设施和应用工作负载。
- en: Anthos on VMware deploys a management control plane built on Kubernetes, the
    admin cluster, by provisioning vSphere Virtual Machines (VMs). Every new user
    cluster is provisioned as a collection of dedicated VMs with a Kubernetes control
    plane and user nodes, fully managed by the admin cluster. Load balancing can be
    delivered as VMs managed by Anthos on VMware instances or external load balancers,
    depending on use case and implementation architecture.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在VMware上部署的Anthos使用Kubernetes构建的管理控制平面，通过配置vSphere虚拟机（VMs）。每个新的用户集群都配置为包含具有Kubernetes控制平面和用户节点的专用VM集合，由管理集群完全管理。负载均衡可以由Anthos在VMware实例上管理的VM或外部负载均衡器提供，具体取决于用例和实施架构。
- en: In this chapter we will explain the various deployment scenarios and requirements
    to install Anthos on VMware in an on-prem data center, beginning with why you
    may want to deploy Anthos on-prem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将解释在本地数据中心安装Anthos的各种部署场景和需求，从您可能为什么想要在本地部署Anthos开始。
- en: C.1 Why should I use Anthos on VMware?
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.1 为什么我应该使用VMware上的Anthos？
- en: Let’s address the first question - Why did Google elect to support VMware with
    the initial version of Anthos? Most enterprises have an existing VMware footprint,
    leveraging a virtual platform providing a consistent environment and API for automated,
    full-stack provisioning. By supporting VMware, Google provides organizations the
    ability to create Kubernetes clusters leveraging existing investments, infrastructure,
    and scaling and node management capabilities.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来解决第一个问题——为什么谷歌选择在Anthos的初始版本中支持VMware？大多数企业已经拥有现有的VMware足迹，利用一个虚拟平台提供一致的环境和API，用于自动化、全栈配置。通过支持VMware，谷歌为组织提供了利用现有投资、基础设施以及扩展和节点管理能力来创建Kubernetes集群的能力。
- en: In general there are multiple drivers and constraints that require businesses
    to keep certain data and workloads running in dedicated data centers (DCs). Most
    common requirements are requirements for proximity to data or users, regulatory,
    or the need to leverage existing on-premises investments.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，有多种驱动因素和约束条件要求企业将某些数据和工作负载保留在专用数据中心（DCs）中运行。最常见的要求是靠近数据或用户、监管要求，或利用现有本地投资的必要性。
- en: Companies must increase application availability and scalability to remain competitive.
    Developing applications in containers will provide efficiency, agility, and portability
    - which provide developers the ability to increase their deployment agility and
    velocity. Organizations that already have DC’s may be able to provide a Cloud-like
    experience for their developers cheaper than moving to a CSP. Leveraging on-prem
    capacity, you can create a cloud-like experience that feels similar to using Kubernetes
    on GCP, offering your developers a cohesive experience for clusters running either
    on or off-prem. It also provides the ability to run multi-cluster architectures
    across different environments providing better reliability, as well allow workloads
    to burst into the cloud to use more on-demand resources to satisfy short spikes
    in load, before scaling back down to on-prem workers.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 公司必须提高应用程序的可用性和可扩展性以保持竞争力。在容器中开发应用程序将提供效率、敏捷性和可移植性——这为开发者提供了增加部署敏捷性和速度的能力。已经拥有数据中心（DC）的组织可能能够以比迁移到云服务提供商（CSP）更低的成本为开发者提供类似云的服务。利用本地容量，您可以创建类似云的体验，感觉类似于在GCP上使用Kubernetes，为您的开发者提供在本地或异地运行的集群的统一体验。它还提供了在多个环境中运行多集群架构的能力，提供更好的可靠性，并允许工作负载在云中爆发，使用更多按需资源来满足负载的短期峰值，然后在本地工作者上缩回。
- en: Kubernetes management, maintenance, integration and lifecycle management requires
    a significant amount of time and skills. Running clusters at a production scale
    increases the daily challenges, diverting business attention from applications
    into infrastructure management. That’s where Anthos on VMware comes in, by including
    conformant, security tested versions of Kubernetes, bundled with integrations
    into a variety of network services and most importantly, enterprise support for
    Kubernetes and all Anthos components. Last, but not least, all clusters are unified,
    providing full management from the GKE console.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的管理、维护、集成和生命周期管理需要大量的时间和技能。在生产规模上运行集群增加了日常挑战，将业务注意力从应用程序转向基础设施管理。这正是VMware上的Anthos发挥作用的地方，它包括符合标准的、经过安全测试的Kubernetes版本，捆绑了各种网络服务的集成，最重要的是，为Kubernetes和所有Anthos组件提供企业级支持。最后但同样重要的是，所有集群都是统一的，从GKE控制台提供全面管理。
- en: 'For enterprise ready architecture there is a common need to integrate new created
    platforms and services into existing enterprise identity solutions. Based on company
    needs, authorization to GKE clusters on-prem can be provided by:'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 对于企业级架构，通常需要将新创建的平台和服务集成到现有的企业身份解决方案中。根据公司需求，可以通过以下方式在本地提供对GKE集群的授权：
- en: company Active Directory Federation ServicesOpenID Connect[^([2])](#ftn2) based
    providers integrations, like
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Active Directory 联邦服务OpenID Connect[^([2])](#ftn2)的提供者集成，例如
- en: Google based,
  id: totrans-19
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Google，
- en: company Active Directory Federation Services (ADFS) based
  id: totrans-20
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于 Active Directory 联邦服务（ADFS）的公司
- en: Lightweight Directory Access Protocol (LDAP)
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 轻量级目录访问协议（LDAP）
- en: Anthos on VMware includes a number of addons to help administrators and security
    officers enable management, shift to git-based management and progressive deployments
    to achieve deployment velocity and agility in Enterprise. Anthos Configuration
    Management (ACM) allows full implementation of GitOps approach for infrastructure
    as a code (IaaC). It includes an Open Policy Agent based admission controller,
    providing security constraints and enterprise standards in a cloud native infrastructure.
    Policy COntroller includes multiple predefined constraints, and it can be extended
    to any custom policies from business or security. Both tools are described in
    detail in separate chapters but it’s important to highlight this they are out
    of the box extensions available on Anthos on VMware implementation.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: VMware上的Anthos包含一系列插件，帮助管理员和安全官员启用管理、转向基于git的管理以及渐进式部署，以实现企业中的部署速度和敏捷性。Anthos配置管理（ACM）允许全面实现基础设施即代码（IaaC）的GitOps方法。它包括基于Open
    Policy Agent的准入控制器，为云原生基础设施提供安全约束和企业标准。策略控制器包括多个预定义的约束，并且可以扩展到来自业务或安全的任何自定义策略。这两个工具在单独的章节中有详细描述，但重要的是要强调，它们是Anthos
    on VMware实现上现成的扩展。
- en: In the next section, we will look at the architecture requirements to successfully
    deploy Anthos on a VMware cluster.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将探讨成功在VMware集群上部署Anthos所需的架构要求。
- en: C.2 Anthos on VMware Architecture
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.2 VMware上的Anthos架构
- en: Anthos on VMware is implemented as a collection of VMs deployed on VMware vSphere
    clusters. In that chapter we will go into the technical requirements for an Anthos
    on-prem implementation.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on VMware作为一组在VMware vSphere集群上部署的虚拟机实现。在那一章中，我们将讨论Anthos on-prem实现的技術要求。
- en: Starting with the general requirements, Anthos on VMware must be installed on
    a vSphere cluster backed by a standard or distributed virtual switches or VMware
    NSX-T software defined networking. Since Google updates Anthos often, detailed
    requirements related to the most recent version is available on GCP Anthos on
    VMware documentation page[^([3])](#ftn3).
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 从一般要求开始，Anthos on VMware必须安装在由标准或分布式虚拟交换机或VMware NSX-T软件定义网络支持的vSphere集群上。由于Google经常更新Anthos，有关最新版本的详细要求可在GCP
    Anthos on VMware文档页面[^([3])](#ftn3)中找到。
- en: Just like the OSS Kubernetes Release, Anthos has an agile release cycle[^([4])](#ftn4),
    inline with the entire Anthos release, it follows monthly patch release cycles
    and quarter cycle for version upgrades. It’s recommended to deploy version updates,
    we can upgrade directly to any version of same minor release or next minor release,
    for example we can upgrade from version 1.9.0 to 1.9.4 or directly to 1.10.1,
    but to upgrade from 1.9.X to 1.11.X you need to perform upgrade to 1.10.X first.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 就像OSS Kubernetes发布一样，Anthos有一个敏捷的发布周期[^([4])](#ftn4)，与整个Anthos发布保持一致，它遵循每月补丁发布周期和季度版本升级周期。建议部署版本更新，我们可以直接升级到同一次要版本的任何版本或下一个次要版本，例如，我们可以从版本1.9.0升级到1.9.4或直接升级到1.10.1，但要从1.9.X升级到1.11.X，您需要首先升级到1.10.X。
- en: '![C_01](../../OEBPS/Images/C_01.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![C_01](../../OEBPS/Images/C_01.png)'
- en: Figure C.1 Upgrade options
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.1 升级选项
- en: To simplify the update process and to allow testing of new releases, starting
    with Anthos version 1.7.0, user clusters can be updated before the admin cluster
    upgrade. As a result, you can spin up a new cluster with a newer version of Anthos,
    test it, and based on the results, start a lifecycle process to upgrade the entire
    environment to the new version. Once the user clusters are upgraded, the admin
    cluster can be updated in a suitable window without pressure to do that upfront.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化更新过程并允许测试新版本，从Anthos版本1.7.0开始，可以在管理员集群升级之前更新用户集群。因此，您可以启动一个具有较新Anthos版本的新的集群，对其进行测试，并根据结果，启动生命周期过程以将整个环境升级到新版本。一旦用户集群升级，管理员集群就可以在合适的时间窗口内进行更新，而无需急于进行。
- en: Now that you know a high level view of the Anthos on VMware, we can move on
    to details to install a new cluster.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经了解了Anthos on VMware的高级视图，我们可以继续到安装新集群的细节。
- en: 'An on-prem deployment consists of an admin workstation, equipped with all required
    tools for the deployment of the admin cluster and user cluster(s). The installation
    requires three configuration files that must be configured for the environment:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 本地部署包括一个管理员工作站，配备部署管理员集群和用户集群（s）所需的所有工具。安装需要三个配置文件，这些文件必须配置为环境：
- en: Admin workstation configuration file
  id: totrans-33
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员工作站配置文件
- en: Admin cluster configuration file
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理员集群配置文件
- en: User cluster configuration file
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户集群配置文件
- en: Each of the files are in YAML format, and contain mandatory and optional sections.
    By default each optional section is commented out and each configuration element
    is followed by short description and usage, providing an easy to follow, self-documented
    configuration file.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件都是YAML格式，包含必选和可选部分。默认情况下，每个可选部分都是注释掉的，每个配置元素后面都跟着简短的描述和用法，提供了一个易于遵循、自我文档化的配置文件。
- en: 'Some of the options in the configuration files include:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件中的一些选项包括：
- en: Configuring the installation to use an on-prem registry for Anthos component
    images
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置安装以使用本地注册表存储Anthos组件镜像
- en: Google service accounts for GCP integration including the accounts to use for
    Anthos connect, and cloud logging
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Google服务帐户用于GCP集成，包括用于Anthos连接和云日志的帐户。
- en: OIDC configuration
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: OIDC配置
- en: vSphere configuration
  id: totrans-41
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vSphere配置
- en: Load-Balancer configuration
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负载均衡器配置
- en: The configuration contains options for a base cluster configuration, with the
    one exception of enabling Cloud Run. Additional elements like Anthos Config Management,
    Anthos Policy Controller or Service Mesh can be installed or removed at any time
    during the cluster life cycle.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 配置包含基本集群配置的选项，只有一个例外是启用Cloud Run。像Anthos Config Management、Anthos Policy Controller或Service
    Mesh这样的附加元素可以在集群生命周期中的任何时候安装或删除。
- en: The overall deployment flow is presented in Fig 3 - Anthos on VMware Deployment
    flow. All options and optional steps are marked by dotted lines.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 整体部署流程在图 3 - Anthos on VMware 部署流程中展示。所有选项和可选步骤都用虚线标记。
- en: '![C_02](../../OEBPS/Images/C_02.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![C_02](../../OEBPS/Images/C_02.png)'
- en: Figure C.2 Anthos on VMware Deployment flow
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.2 Anthos on VMware 部署流程
- en: In the next section we will explain how to deploy the management plane, which
    is the first step for a new on-prem deployment.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释如何部署管理平面，这是新本地部署的第一步。
- en: Deploying the Management plane
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管理平面
- en: The creation of the management plane begins with the deployment of the admin
    workstation, providing the tools to create and provide administration for the
    admin and user clusters. This VM has all of the software required to create an
    Anthos cluster on VMware. Using an admin workstation, Google can guarantee consistency
    and remove the responsibility to create and maintain tools from administrators.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 管理平面的创建始于管理工作站的部署，提供创建和管理管理员和用户集群的工具。此虚拟机拥有创建 VMware 上 Anthos 集群所需的所有软件。使用管理工作站，Google
    可以保证一致性，并从管理员那里移除创建和维护工具的责任。
- en: Admin Workstation Preparation
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 管理工作站准备
- en: Deployment of an admin workstation requires preparation, the graphic below shows
    the steps to create the admin workstation.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 部署管理工作站需要准备，下面的图形显示了创建管理工作站的步骤。
- en: '![C_03](../../OEBPS/Images/C_03.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![C_03](../../OEBPS/Images/C_03.png)'
- en: Figure C.3 Admin workstation deployment
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.3 管理工作站部署
- en: The first step is to install the Google Cloud SDK on your local machine. This
    can be any Linux, Windows (workstation or server) or MAC OS machine.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是在您的本地机器上安装 Google Cloud SDK。这可以是任何 Linux、Windows（工作站或服务器）或 MAC OS 机器。
- en: Once you have installed the SDK, you need to download the gkeadm utility. Using
    the gsutil utility, download the file using the command below.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 安装 SDK 后，您需要下载 gkeadm 工具。使用 gsutil 工具，使用以下命令下载文件。
- en: '[PRE0]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you are using Linux, you will need to make the binary executable using chmod.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您使用的是 Linux，您需要使用 chmod 命令使二进制文件可执行。
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now that you have gkeadm downloaded, you can deploy the admin workstation.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经下载了 gkeadm，您可以部署管理工作站。
- en: C.3 Deploying the Admin workstation
  id: totrans-60
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.3 部署管理工作站
- en: 'Deployment of the admin workstation is based on a single YAML configuration
    file, which has four main sections:'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 管理工作站的部署基于一个单一的 YAML 配置文件，它有四个主要部分：
- en: gcp
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: gcp
- en: vCenter
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: vCenter
- en: proxyUrl
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: proxyUrl
- en: adminWorkstation
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: adminWorkstation
- en: The GCP section is dedicated to GCP integration configuration elements. At the
    time of writing this book, only one element is required, the componentAccessServiceAccountKeyPath,
    which defines a path to a component service account. This service account has
    rights to access Anthos on VMware binaries and consume the related APIs. If different
    projects are used for monitoring logging etc that account must have serviceusage.serviceUsageViewer
    and iam.roleViewer on each or those projects.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: GCP 部分专门用于 GCP 集成配置元素。在撰写本书时，只需要一个元素，即 componentAccessServiceAccountKeyPath，它定义了一个组件服务账户的路径。此服务账户有权访问
    VMware 上的 Anthos 二进制文件并使用相关的 API。如果使用不同的项目进行监控、日志记录等，则该账户必须在每个或那些项目中具有 serviceusage.serviceUsageViewer
    和 iam.roleViewer 权限。
- en: '[PRE2]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The second section in the configuration file describes integration with a vCenter
    server. It covers the credentials subsection with definitions for vCenter, and
    a credentials file containing the username and password required for vSphere access.
    A credential file, named credential.yaml, is shown below.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 配置文件的第二部分描述了与 vCenter 服务器的集成。它涵盖了凭证子部分，包括 vCenter 的定义，以及包含用于 vSphere 访问的用户名和密码的凭证文件。以下是一个名为
    credential.yaml 的凭证文件示例。
- en: '[PRE3]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The required vCenter information includes the datacenter name, datastore name,
    cluster name, network, and the vCenter root certificate. Optionally, a resource
    pool name and folder can be specified to place the VM in.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 所需的 vCenter 信息包括数据中心名称、数据存储名称、集群名称、网络和 vCenter 根证书。可选地，可以指定资源池名称和文件夹，以便将虚拟机放置在其中。
- en: '[PRE4]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: if you have a self signed certificate for your vCenter server, which needs to
    be added to the caCertPath value, you can obtain it using curl, as shown in the
    command below.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您为您的 vCenter 服务器有一个自签名的证书，需要添加到 caCertPath 值中，您可以使用 curl 获取它，如下面的命令所示。
- en: '[PRE5]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Once downloaded, unzip the file, which will extract the certificate in the certs/lin
    folder.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 下载完成后，解压缩文件，它将在 certs/lin 文件夹中提取证书。
- en: If your environment requires a proxy server to access the internet, you can
    configure the proxyUrl section. This configuration parameter is used only by the
    gkeadm command during the VM deployment.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您的环境需要代理服务器来访问互联网，您可以配置proxyUrl部分。此配置参数仅在VM部署期间由gkeadm命令使用。
- en: '[PRE6]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When a proxy is configured, you will also need to add the appropriate addresses
    to the OS or system no_proxy variable. This configuration is specific for each
    company and deployment - a full explanation of how proxy servers work is beyond
    the scope of this book. As a starting point, you may need to add your vCenter
    server, local registry (if configured), and the CIDR range for the ESX hosts.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 当配置代理时，您还需要将适当的地址添加到操作系统或系统no_proxy变量中。此配置针对每个公司和部署都是特定的 - 代理服务器如何工作的完整解释超出了本书的范围。作为一个起点，您可能需要添加您的vCenter服务器、本地注册表（如果已配置）以及ESX主机的CIDR范围。
- en: 'The Last section is the only one that comes partly pre populated during configuration
    file generation:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一个部分是配置文件生成过程中部分预填充的唯一部分：
- en: dataDiskName
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataDiskName
- en: dataDiskMB
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: dataDiskMB
- en: Name of the VM
  id: totrans-81
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虚拟机的名称
- en: Amount of cpus
  id: totrans-82
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CPU数量
- en: Amount of memory in MB
  id: totrans-83
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 内存大小（以MB为单位）
- en: Base disk size in GB
  id: totrans-84
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础磁盘大小（以GB为单位）
- en: 'NOTE: dataDisk folder where new disk is created must exist. As result you must
    create it manually upfront.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：新磁盘创建的dataDisk文件夹必须存在。因此，您必须事先手动创建它。
- en: The admin workstation can be assigned an IP address using a static IP assignment
    or by a DHCP server. Your implementation choice is defined in the network subsection
    of the configuration file using the ipAllocationMode property.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 管理工作站可以使用静态IP分配或通过DHCP服务器分配IP地址。您的实现选择在配置文件的网络子节中使用ipAllocationMode属性定义。
- en: For DHCP use cases ipAllocationMode must be defined as DHCP, and all other child
    network configuration elements remain undefined.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 对于DHCP用例，ipAllocationMode必须定义为DHCP，并且所有其他子网络配置元素保持未定义。
- en: When using a static IP assignment, the ipAllocationMode property must be set
    to “static”, followed by IP, gateway, netmask and DNS configuration. The DNS value
    can be defined as an array of properties with multiple values.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 当使用静态IP分配时，ipAllocationMode属性必须设置为“static”，然后是IP、网关、子网掩码和DNS配置。DNS值可以定义为具有多个值的属性数组。
- en: Finally, set the NTP server used by the admin workstation. It’s mandatory to
    use NTP that is in sync with vSphere infrastructure otherwise time differences
    will cause deployment failures.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，设置管理工作站使用的NTP服务器。必须使用与vSphere基础设施同步的NTP，否则时间差异将导致部署失败。
- en: Two example configuration files are shown below, the first has been configured
    to use a static IP and the second has been configured to use DHCP.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了两个示例配置文件，第一个已配置为使用静态IP，第二个已配置为使用DHCP。
- en: '[PRE7]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Now we can create the admin workstation on our vSphere infrastructure using
    the gkeadm utility.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用gkeadm实用程序在我们的vSphere基础设施上创建管理工作站。
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Adding the auto-create-service-accounts flag allows you to automatically create
    the associated service accounts in your project.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 添加auto-create-service-accounts标志允许您在项目中自动创建相关的服务帐户。
- en: Once the admin workstation has been created, you are ready to deploy the admin
    cluster. In the next section, we will go through the steps to create your admin
    cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦创建管理工作站，您就可以部署管理集群了。在下一节中，我们将介绍创建管理集群的步骤。
- en: Creating an Admin Cluster
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 创建管理集群
- en: The admin cluster is the key component of the Anthos control plane. It is responsible
    for the supervision of Anthos on VMware implementations, and the provisioning
    and management of user clusters. It’s deployed as a Kubernetes cluster using a
    single control plane node and two worker nodes [Fig 8].
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 管理集群是Anthos控制平面的关键组件。它负责监督在VMware上的Anthos实现，以及用户集群的配置和管理。它作为一个使用单个控制平面节点和两个工作节点的Kubernetes集群部署[图8]。
- en: The control plane node will provide the Kubernetes API server for the admin
    control plane, the admin cluster scheduler, the etcd database, audit proxy and
    any load balancer integrated pods.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 控制平面节点将为管理控制平面提供Kubernetes API服务器、管理集群调度器、etcd数据库、审计代理以及任何集成的负载均衡器Pod。
- en: Worker nodes are providing resources for Kubernetes addons like kube-dns, cloud
    monitoring (former stackdriver) or vSphere pods.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点为Kubernetes插件如kube-dns、云监控（前身为stackdriver）或vSphere pods提供资源。
- en: In addition to the admin control plane and addons, the admin cluster hosts the
    user control planes. As a result, the user clusters API server, scheduler, etcd,
    etcd maintenance and monitoring pods are all hosted on the admin cluster.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 除了管理控制平面和插件外，管理集群还托管用户控制平面。因此，用户集群的 API 服务器、调度器、etcd、etcd 维护和监控 Pod 都托管在管理集群上。
- en: '![C_04](../../OEBPS/Images/C_04.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![C_04](../../OEBPS/Images/C_04.png)'
- en: Figure C.4 Anthos on VMware admin cluster architecture
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.4 Anthos on VMware 管理集群架构
- en: To create an admin cluster, you will need to SSH into your admin workstation
    that was created in the last section. SSH into the admin workstation using the
    key that was created when you deployed the admin workstation, located at .ssh/gke-admin-workstation..
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建管理集群，您需要 SSH 进入上一节中创建的管理工作站。使用在部署管理工作站时创建的密钥连接到管理工作站，该密钥位于 .ssh/gke-admin-workstation
    目录下。
- en: '[PRE9]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Similar to the admin workstation creation process, the admin cluster uses a
    YAML file that is divided into sections. The vCenter, gkeconnect, stackdriver
    and gcrkeypath sections are pre populated with values gathered from the admin
    workstation YAML[^([5])](#ftn5) file, while all other sections must be filed in
    for your deployment, prior to creating the cluster.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 与管理工作站创建过程类似，管理集群使用一个分为几个部分的 YAML 文件。vCenter、gkeconnect、stackdriver 和 gcrkeypath
    部分预先填充了从管理工作站 YAML[^([5])](#ftn5) 文件中收集的值，而所有其他部分必须在创建集群之前填写。
- en: You can use the included admin cluster configuration file, or you can generate
    a new admin cluster configuration file using the gkectl tool that is already installed
    on admin workstation.. Unlike the pre-created template file, any templates generated
    manually using gkectl will not contain any pre-populated values. To create a new
    file, use the gkectl create-config admin option.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用包含的管理集群配置文件，或者您可以使用已安装在管理工作站上的 gkectl 工具生成一个新的管理集群配置文件。与预先创建的模板文件不同，使用
    gkectl 手动生成的任何模板都不会包含任何预先填充的值。要创建新文件，请使用 gkectl create-config admin 选项。
- en: '[PRE10]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Both creation methods will contain the same sections, the first two sections
    of the configuration file must remain unchanged, defining the API version and
    cluster type.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 两种创建方法将包含相同的部分，配置文件的前两个部分必须保持不变，定义 API 版本和集群类型。
- en: '[PRE11]'
  id: totrans-109
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: The next section is dedicated to the vSphere configuration, containing the requirements
    for the Virtual Machines and disks placement.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分是 vSphere 配置，包含对虚拟机和磁盘放置的要求。
- en: 'TIP: A good practice is to always use fully qualified domain name for vCenter
    and avoid usage of IP addresses in production environments.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 'TIP: 良好的做法是始终使用完全限定的域名 (FQDN) 用于 vCenter，并在生产环境中避免使用 IP 地址。'
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'TIP: A good practice is to place the data disk into a folder. The dataDisk
    property must point to the folder that exists. Anthos on VMware creates a virtual
    machine disk (VMDK) to hold the Kubernetes object data for the admin cluster,
    which is created by the installer for you, so make sure that name is unique.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 'TIP: 良好的做法是将数据磁盘放入一个文件夹中。dataDisk 属性必须指向存在的文件夹。Anthos on VMware 创建一个虚拟机磁盘 (VMDK)
    来存储管理集群的 Kubernetes 对象数据，这是安装程序为您创建的，因此请确保名称是唯一的。'
- en: 'TIP: If you would prefer to not use Resource Pool and place admin cluster resources
    directly under cluster level, provide “<clusterName>/Resources” in resoucrePool
    configuration.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 'TIP: 如果您希望不使用资源池并将管理集群资源直接放置在集群级别，请在资源池配置中提供“<clusterName>/Resources”。'
- en: In the next section we define the IP settings for admin cluster nodes, services
    and pods. These settings will also be used for the user cluster master nodes deployment.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一个部分中，我们定义管理集群节点、服务和 Pod 的 IP 设置。这些设置也将用于用户集群主节点的部署。
- en: First, we must define whether the Anthos on VMware admin cluster nodes and user
    cluster master nodes will use DHCP or static IP assignments. If a static option
    is used, an additional YAML file must be defined for IP address assignments; this
    file is specified in the ipBlockFilePath property.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们必须定义 Anthos on VMware 管理集群节点和用户集群主节点将使用 DHCP 还是静态 IP 分配。如果使用静态选项，必须定义一个额外的
    YAML 文件来指定 IP 地址分配；此文件在 ipBlockFilePath 属性中指定。
- en: The next two properties are dedicated for the Kubernetes service and pod CIDR
    ranges, which are detailed in Table 1 below. They are used by Kubernetes pods
    and services and are described in detail in the chapter Computing environment
    built on Kubernetes. The assigned network ranges must not overlap between each
    other or with any external services that are consumed by the management plane,
    for example, any internet proxy used for communication with GCP.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的两个属性专门用于Kubernetes服务和pod CIDR范围，这些范围在下面的表1中详细说明。它们由Kubernetes pods和服务使用，并在基于Kubernetes构建的计算环境章节中详细描述。分配的网络范围之间不得相互重叠，也不得与由管理平面使用的任何外部服务重叠，例如用于与GCP通信的任何互联网代理。
- en: 'TIP: Due to fact that Anthos on VMware operates in Island Mode IP addresses
    used for Pods and Services are not routable into dataceneter network. That means
    you can use same IPs for every new cluster.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: TIP：由于Anthos on VMware在孤岛模式下运行，Pod和Service使用的IP地址无法路由到数据中心网络。这意味着您可以为每个新的集群使用相同的IP地址。
- en: Finally, the last section defines the target vSphere network name that the Kubernetes
    Nodes will use once provisioned.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一节定义了Kubernetes节点一旦配置后将要使用的目标vSphere网络名称。
- en: Table C.1 Admin Cluster Properties
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 表C.1 管理集群属性
- en: '| Kubernetes resource | Description |'
  id: totrans-121
  prefs: []
  type: TYPE_TB
  zh: '| Kubernetes资源 | 描述 |'
- en: '| network |'
  id: totrans-122
  prefs: []
  type: TYPE_TB
  zh: '| network |'
- en: '| ipMode | Parent key for type and ipBlockFilePath |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ipMode | type和ipBlockFilePath的父键|'
- en: '| type | IP mode to use ("dhcp" or "static") |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| type | 要使用的IP模式（"dhcp"或"static"）|'
- en: '| ipBlockFilePath | Path to yaml configuration file used for static IP assignment.
    Must be used in conjunction with type: static key value pair |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ipBlockFilePath | 用于静态IP分配的yaml配置文件的路径。必须与type: static键值对一起使用|'
- en: '| serviceCIDR | Kubernetes service CIDR used for control plane deployed services.
    Minimal size 128 addresses |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| serviceCIDR | 用于控制平面部署服务的Kubernetes服务CIDR。最小地址数128个|'
- en: '| podCIDR | Kubernetes pods CIDR used for control plane deployed services.
    Minimal size 2048 addresses |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| podCIDR | 用于控制平面部署服务的Kubernetes pods CIDR。最小地址数2048个|'
- en: '| vCenter | Parent key for networkName |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| vCenter | networkName的父键|'
- en: '| networkName | vSphere portgroup name where admin cluster nodes and user cluster
    master nodes are assigned to. |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| networkName | 管理集群节点和用户集群主节点分配到的vSphere端口组名称。|'
- en: An example configuration is shown below.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 下面显示了示例配置。
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: As we mentioned, the Node IP assignments can be configured via a static configuration
    file. The path to such a file must be specified under ipBlockFilePath key that
    must be uncommented and taken into account only when ipMode.type key is set to
    static. Additionally DNS and NTP servers must be specified and search domain defined,
    as shown in the example below.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们提到的，节点IP分配可以通过静态配置文件进行配置。此类文件的路径必须在ipBlockFilePath键下指定，该键必须取消注释，并且仅在ipMode.type键设置为static时才考虑。此外，必须指定DNS和NTP服务器，并定义搜索域，如下面的示例所示。
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'The static host configuration file is built using two main configuration keys:
    hostconfig and blocks. The Hostconfig defines information about DNS servers, NTP
    servers and search domains. The blocks define netmask and gateway for Kubernetes
    nodes followed by an array of hostnames and corresponding IP addresses for them.'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 静态主机配置文件使用两个主要配置键构建：hostconfig和blocks。Hostconfig定义了DNS服务器、NTP服务器和搜索域的信息。blocks定义了Kubernetes节点的子网掩码和网关，随后是一个主机名数组及其对应的IP地址。
- en: '| Property key | Property description |'
  id: totrans-135
  prefs: []
  type: TYPE_TB
  zh: '| 属性键 | 属性描述 |'
- en: '| blocks |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| blocks |'
- en: '| netmask | Network netmask |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '| netmask | 网络子网掩码|'
- en: '| gateway | Network gateway |'
  id: totrans-138
  prefs: []
  type: TYPE_TB
  zh: '| gateway | 网络网关 |'
- en: '| ips | Array of ip and hostname keys with corresponding values. |'
  id: totrans-139
  prefs: []
  type: TYPE_TB
  zh: '| ips | 与对应值相对应的IP和主机名键的数组。|'
- en: '[PRE15]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'TIP: IP addresses assigned to nodes are not assigned in the order defined in
    the file. They are randomly picked from the pool of available IPs during resizing
    and upgrade operations.'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: TIP：分配给节点的IP地址不是按照文件中定义的顺序分配的。它们在调整大小和升级操作期间从可用的IP池中随机选择。
- en: 'The next section of the configuration is for the cluster Load Balancing. Anthos
    on VMware requires a load balancer to provide a Virtual IP (VIP) to the Kubernetes
    API server. For your clusters you can choose between using the integrated load-balancer,
    based on MetalLB, using a F5, or any other load-balancer using a manual configuration.
    MetalLB is becoming a popular solution for bare-metal based implementations including
    VMware[^([6])](#ftn6) outside of Hyperscaller build in solutions. Enablement of
    MetalLB on the admin cluster is limited to definition of kind: MetalLB in the
    admin cluster configuration file as presented below.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '配置的下一部分是集群负载均衡。Anthos on VMware需要一个负载均衡器为Kubernetes API服务器提供虚拟IP（VIP）。对于您的集群，您可以选择使用基于MetalLB的集成负载均衡器、F5或其他任何使用手动配置的负载均衡器。MetalLB正在成为包括VMware[^([6])](#ftn6)在内的裸金属实现中的一种流行解决方案，这些解决方案不包括HyperCaller构建的解决方案。在管理集群上启用MetalLB仅限于在管理集群配置文件中定义kind:
    MetalLB，如下所示。'
- en: '[PRE16]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: We will explain the options in greater detail in the Load Balancer section of
    this chapter.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章的负载均衡器部分更详细地解释这些选项。
- en: To make sure that Kubernetes control plane nodes will be distributed across
    different ESXi hosts, Anthos supports vSphere anti-affinity groups. Such an implementation
    guarantees that a physical ESXi host failure will only impact a single Kubernetes
    node or addon node providing a production-grade configurations control plane.
    This value should be set to true to leverage anti-affinity rules, or false to
    disable any use of anti-affinity rules.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 为了确保Kubernetes控制平面节点将分布在不同ESXi主机上，Anthos支持vSphere反亲和性组。这种实现保证了物理ESXi主机故障只会影响单个Kubernetes节点或提供生产级配置控制平面的附加节点。此值应设置为true以利用反亲和性规则，或设置为false以禁用任何反亲和性规则的使用。
- en: '[PRE17]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: You can monitor the cluster using Google Cloud Logging by setting the appropriate
    values in the stackdriver section of the configuration file.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过在配置文件的stackdriver部分设置适当的值，使用Google Cloud Logging来监控集群。
- en: Logs and metrics can be sent to a dedicated GCP project, or the same project
    where the cluster is being created. You will need to supply the projectID that
    you want to use for the logs, the cluster location, VPC options, the service account
    key file with the appropriate permissions to the project, and your decision to
    enable or disable vSphere metrics.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 日志和指标可以发送到专门的GCP项目，或者创建集群所在的同一项目。您需要提供用于日志的项目ID、集群位置、VPC选项、具有适当项目权限的服务账户密钥文件，以及您启用或禁用vSphere指标的决定。
- en: '[PRE18]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Moreover, you can also integrate audit logs from the cluster's API server with
    Cloud Audit Logs. You must specify the project that integration should target
    (it can be the same project used for your Cloud Operations integration), the cluster
    location, and a service account key with the appropriate permissions.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，您还可以将集群API服务器的审计日志与云审计日志集成。您必须指定集成应针对的项目（可以是用于云操作集成的同一项目）、集群位置以及具有适当权限的服务账户密钥。
- en: '[PRE19]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: It’s important to make sure that problems on Kubernetes nodes are detected and
    fixed quickly, and similar to a GKE cluster, Anthos on VMware uses a Node Problem
    Detector. The detector watches for possible node problems and reports them as
    events and conditions. When any Kubelet becomes unhealthy or ContainerRuntimeUnhealthy
    conditions are reported by the kubelet or docker systemd service, the auto repair
    functionality will try to restart them automatically.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 确保Kubernetes节点上的问题能够快速检测和修复非常重要，类似于GKE集群，Anthos on VMware使用节点问题检测器。检测器会监视可能出现的节点问题，并将它们作为事件和条件报告。当任何Kubelet变得不健康或由kubelet或docker
    systemd服务报告ContainerRuntimeUnhealthy条件时，自动修复功能将尝试自动重启它们。
- en: Anthos on VMware clusters auto repair functionality allows to automatically
    creating Kubernetes node VMs when they are deleted by mistake or recreation of
    unresponsive faulty Virtual Machines. It can be enabled or disabled in the cluster
    deployment configuration file, by setting the autoRepair enabled option to either
    true or false.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on VMware集群的自动修复功能允许在它们被错误删除或无法响应的虚拟机被重新创建时自动创建Kubernetes节点虚拟机。它可以在集群部署配置文件中通过将autoRepair
    enabled选项设置为true或false来启用或禁用。
- en: '[PRE20]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: When enabled, cluster-health-controller deployment is created on the corresponding
    cluster in the kube-system namespace. If the node is indicated as unhealthy, it
    is drained and recreated, as shown in Figure C.5.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 当启用时，集群健康控制器部署将在kube-system命名空间中对应的集群上创建。如果节点被标记为不健康，它将被移除并重新创建，如图C.5所示。
- en: '![C_05](../../OEBPS/Images/C_05.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![C_05](../../OEBPS/Images/C_05.png)'
- en: Figure C.5 Node auto repair process
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.5节点自动修复过程
- en: 'NOTE: To disable auto repair functionality on admin cluster cluster-health-controller
    deployment must be deleted from admin cluster.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要在管理集群上禁用自动修复功能，必须从管理集群中删除cluster-health-controller部署。
- en: It is possible to deploy Anthos on VMware from a private docker registry instead
    of gcr.io. To configure your deployment to use a private registry, you need to
    set the values in the privateRegistry section of the configuration file. You will
    need to supply values for the registry address, the CA for the registry, and the
    reference to the credentials to use in the credentials file.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 可以从私有docker注册库而不是gcr.io部署Anthos。要配置您的部署以使用私有注册库，您需要在配置文件的privateRegistry部分设置值。您需要提供注册库地址、注册库的CA以及用于凭证文件的凭证引用。
- en: '[PRE21]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: That completes the admin cluster configuration file configuration, now let’s
    move to the user cluster configuration.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 这就完成了管理集群配置文件的配置，现在让我们转向用户集群配置。
- en: Security is very important for Anthos based Kubernetes implementation. Anthos
    on VMware introduced secret encryption capability to ensure they are encrypted
    at rest without requirement for external Key Management Service. As a result,
    before a secret is stored in the etcd database, it is encrypted. To enable or
    disable that functionality edit the secretsEncryption section of the configuration
    file.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 对于基于Anthos的Kubernetes实现，安全性非常重要。Anthos on VMware引入了密钥加密功能，以确保它们在静态存储时加密，无需外部密钥管理服务。因此，在密钥存储在etcd数据库之前，它会被加密。要启用或禁用该功能，请编辑配置文件的secretsEncryption部分。
- en: '[PRE22]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'TIP: Anytime key version is updated a new key is generated and secrets are
    re-encrypted using that new key. You can enforce key rotation using gkectl update
    command, as result all existing and new secrets are encrypted in use of new key
    and old one is securly removed.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：每当密钥版本更新时，都会生成一个新的密钥，并使用该新密钥重新加密密钥。您可以使用gkectl update命令强制执行密钥轮换，结果所有现有和新密钥都将使用新密钥加密，旧密钥将被安全删除。
- en: User Cluster Creation
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群创建
- en: Each new user cluster is required to be connected to an admin cluster, in fact,
    there is no way to create a workload cluster without an admin cluster. A single
    admin cluster can manage multiple user clusters but a single user cluster can
    be supervised by only one admin cluster.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新的用户集群都需要连接到一个管理集群，实际上，没有管理集群就无法创建工作负载集群。一个管理集群可以管理多个用户集群，但单个用户集群只能由一个管理集群监督。
- en: 'Each provisioned user cluster can be deployed in two configurations, with,
    or without, a Highly Available (HA) production-grade management plane. As presented
    in the below drawing, clusters with HA enabled are built with 3 admin nodes (User
    Cluster #2 in the drawing), and without HA, using a single node (User Cluster
    #1 in the drawing). The Single node management plane consumes less compute resources
    but in case of a node or physical host failure, the ability to manage the cluster
    is lost[^([7])](#ftn7). In HA mode, a single master node failure does not impact
    the Kubernetes cluster configuration management ability.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 每个已配置的用户集群都可以部署在两种配置中，带有或没有高可用性（HA）的生产级管理平面。如图所示，启用HA的集群使用3个管理节点构建（图中的用户集群#2），而没有HA的则使用单个节点（图中的用户集群#1）。单节点管理平面消耗较少的计算资源，但在节点或物理主机故障的情况下，管理集群的能力会丢失。在HA模式下，单个主节点故障不会影响Kubernetes集群配置管理能力。
- en: '![C_06](../../OEBPS/Images/C_06.png)'
  id: totrans-168
  prefs: []
  type: TYPE_IMG
  zh: '![C_06](../../OEBPS/Images/C_06.png)'
- en: Figure C.6 Anthos on VMware user clusters architecture
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.6 Anthos on VMware用户集群架构
- en: 'IMPORTANT: After deployment, the number of admin nodes cannot be changed without
    full cluster re-creation.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 重要：部署后，不能在不重新创建整个集群的情况下更改管理节点的数量。
- en: Every new user cluster is placed in a dedicated namespace in the admin cluster.
    It’s used to host and deliver services, deployments, pods and ReplicaSets for
    management purposes.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 每个新的用户集群都会放置在管理集群中的一个专用命名空间中。它用于托管和交付服务、部署、Pod和ReplicaSets以进行管理目的。
- en: The Namespace name is inline with the cluster name, allowing you to easily grab
    all details by referring to it via kubectl get all -n {{ clusterName }}. Any user
    cluster namespace will be hosted on dedicated nodes that are added to the admin
    cluster when you create the user cluster. The nodes will be labeled with the cluster
    name and when the cluster management pods are created, they will use a node selector
    to force their placement on the dedicated user cluster nodes.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 命名空间名称与集群名称一致，允许您通过 kubectl get all -n {{ clusterName }} 轻松获取所有详细信息。任何用户集群命名空间都将托管在添加到管理员集群中以创建用户集群的专用节点上。这些节点将标记为集群名称，当创建集群管理器
    pods 时，它们将使用节点选择器来强制将它们放置在专用用户集群节点上。
- en: Other, non-system namespaces, are created on top of workload cluster nodes as
    presented on below picture
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 其他非系统命名空间是在工作负载集群节点之上创建的，如下图中所示
- en: '![C_07](../../OEBPS/Images/C_07.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![C_07](../../OEBPS/Images/C_07.png)'
- en: Figure C.7 Anthos on VMware user cluster namespaces
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.7 Anthos 在 VMware 用户集群命名空间
- en: Similar to an admin cluster, a user cluster deployment is based on a YAML configuration
    file. The first two sections of the configuration file must remain unchanged,
    defining the API and cluster type.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 与管理员集群类似，用户集群部署基于 YAML 配置文件。配置文件的前两部分必须保持不变，定义 API 和集群类型。
- en: '[PRE23]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'TIP: You can convert old versions of configuration files using: gkectl create-config
    cluster --config $MyAwsomeClusterConfigFile.yaml --from MyOldConfigFile.yaml --version
    v1'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：您可以使用以下命令将旧版本的配置文件转换为配置文件：gkectl create-config cluster --config $MyAwsomeClusterConfigFile.yaml
    --from MyOldConfigFile.yaml --version v1
- en: The next section is where you provide the name of the new cluster and the Anthos
    on VMware version. The cluster name must be unique within a GCP project and the
    version must be inline with the admin cluster version.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分是您提供新集群名称和 Anthos on VMware 版本的地方。集群名称必须在 GCP 项目内是唯一的，版本必须与管理员集群版本一致。
- en: '[PRE24]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: The next section is optional. It is used to manage vSphere integration and worker
    nodes placement. It is strongly recommended to separate admin and workload compute
    resources on vSphere level to ensure Kubernetes management plane availability.
    This guarantees resources for each Anthos on VMware cluster in case of resource
    saturation or limited access to vSphere. Following that practise your user cluster
    worker nodes will be placed in resource pool and datastore[^([8])](#ftn8) defined
    under the vCenter section. Additionally user clusters can be deployed into separate
    VMware datacenters if required. To make sure rights separation is properly applied
    for vSphere resources if’s recommended to use dedicated account for user cluster
    vCenter communication.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分是可选的。它用于管理 vSphere 集成和工作节点放置。强烈建议在 vSphere 层面上分离管理员和工作负载计算资源，以确保 Kubernetes
    管理平面的可用性。这保证了在资源饱和或对 vSphere 访问有限的情况下，每个 Anthos on VMware 集群都有资源。按照此做法，您的用户集群工作节点将被放置在
    vCenter 部分下定义的资源池和数据存储[^([8])](#ftn8)。此外，如果需要，用户集群还可以部署到单独的 VMware 数据中心。为了确保 vSphere
    资源正确应用了权限分离，建议为用户集群 vCenter 通信使用专用账户。
- en: '[PRE25]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'TIP: You can decide to use only a single property like vCenter.resourcePool.
    In such case comment other lines adding # at the beginning of line and configuration
    of commented property will be inherited from admin cluster configuration.'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '提示：您可以选择只使用单个属性，例如 vCenter.resourcePool。在这种情况下，注释其他行，在行首添加 #，注释属性的配置将继承自管理员集群配置。'
- en: The networking section has the same structure as the admin nodes described in
    the admin cluster section extended with capability to define additional network
    interfaces that can be used for Kubernetes payload.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 网络部分的结构与管理员集群部分中描述的管理节点相同，并扩展了定义可用于 Kubernetes 有效负载的附加网络接口的能力。
- en: '[PRE26]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Or in case of static IP assignment:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，在静态 IP 分配的情况下：
- en: '[PRE27]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: At the beginning of that chapter we stated that the admin plane can be HA protected
    or not. Such a decision is configured under the masterNode.replicas section of
    the cluster configuration file by definition adequately of 3 or 1 replicas.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 在该章节的开头，我们提到管理平面可以是高可用性保护的，也可以不是。这样的决定是通过集群配置文件中的 masterNode.replicas 部分配置的，定义为
    3 或 1 个副本。
- en: We can also scale up the cpu and memory of master nodes if required under this
    section or setup auto resize capability.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果需要，您可以在本节中扩展主节点 cpu 和内存，或者设置自动调整大小功能。
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'TIP: Configuration file is key: value based. All values defined under quotation
    marks “” are interpreted as strings and without quotation marks as integers. All
    number based configuration elements like amount of replicas, cpus or memory must
    be specified as integers'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：配置文件是键值对形式的。所有在引号“”下定义的值都被解释为字符串，而没有引号则解释为整数。所有基于数字的配置元素，如副本数量、CPU或内存，都必须指定为整数。
- en: User cluster worker nodes are defined as pools of nodes. This allows you to
    have different sizes of nodes in the same Kubernetes cluster with labels and taints
    applied to node objects. Finally, the last configuration element of each defined
    node pool is the node Operating System, offering either Google’s hardened Ubuntu
    image or Google’s immutable Container-Optimized OS (COS). If using bundled loadbalancer
    type - MetalLB - at least one of the pools must have enableLoadBalancer configuration
    set as true.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群工作节点被定义为节点池。这允许您在同一个Kubernetes集群中拥有不同大小的节点，并为节点对象应用标签和污点。最后，每个定义的节点池的最后配置元素是节点操作系统，提供Google的强化Ubuntu镜像或Google的不变容器优化操作系统（COS）。如果使用捆绑的负载均衡器类型-MetalLB-，至少有一个池必须将enableLoadBalancer配置设置为true。
- en: '[PRE29]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'NOTE: Worker node virtual machines will be named inline with defined node pool
    name followed by random numbers and letters i.e. My-1st-node-pool-sxA7hs7.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：工作节点虚拟机将被命名为与定义的节点池名称一致，后跟随机数字和字母，例如My-1st-node-pool-sxA7hs7。
- en: During the cluster creation, anti-affinity groups are created on vSphere with
    the worker nodes placed inside them. This vSphere functionality allows us to distribute
    the worker node VMs across different vSphere hosts in a cluster, avoiding placing
    too many nodes on the same physical host. As a result, in the case of a VMware
    ESXi host failure, only a limited number of Kubernetes nodes are impacted, decreasing
    impact into hosted services.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在集群创建过程中，vSphere上会创建反亲和性组，并将工作节点放置在其中。这个vSphere功能允许我们在集群的不同vSphere主机之间分配工作节点虚拟机，避免在同一个物理主机上放置过多的节点。因此，在VMware
    ESXi主机故障的情况下，只有有限数量的Kubernetes节点受到影响，从而降低对托管服务的影响。
- en: To enable antiAffinityGroups, it’s mandatory to have at least 3 ESXi hosts in
    the vSphere cluster. You can enable or disable this feature under the configuration
    file antiAffinityGroups.enabled section by changing the default value to either
    true or false.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 要启用反亲和性组，vSphere集群中至少需要3个ESXi主机。您可以通过在配置文件中更改antiAffinityGroups.enabled部分的默认值来启用或禁用此功能，将其设置为true或false。
- en: '[PRE30]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: By default all workload clusters can be accessed in use of auto generated kubeconfig
    files. In such cases no additional configuration is required but access scope
    is not limited and significantly hard to manage. To solve this problem, Anthos
    on VMware clusters have an option to integrate into external identity providers
    via OpenID Connect (OIDC) and grant access to namespaces or clusters using Kubernetes
    authorization (Fig 12).
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，所有工作负载集群都可以通过自动生成的kubeconfig文件访问。在这种情况下，不需要额外的配置，但访问范围不受限制，并且管理起来非常困难。为了解决这个问题，Anthos
    on VMware集群有一个选项可以通过OpenID Connect (OIDC)集成到外部身份提供者，并使用Kubernetes授权（图12）授予对命名空间或集群的访问权限。
- en: '![C_08](../../OEBPS/Images/C_08.png)'
  id: totrans-199
  prefs: []
  type: TYPE_IMG
  zh: '![C_08](../../OEBPS/Images/C_08.png)'
- en: Figure C.8 Kubernetes Cluster and namespace based access
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.8 基于Kubernetes集群和命名空间的访问
- en: You can integrate the cluster into an existing Active Directory Federation Services
    (ADFS), Google, Okta or any other certified OpenID provider[^([9])](#ftn9). To
    configure these settings, we must provide all provider specific information to
    leverage Anthos Identyty Service and edit ClienConfig file after cluster creation[^([10])](#ftn10).
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将集群集成到现有的Active Directory Federation Services (ADFS)、Google、Okta或任何其他认证的OpenID提供者[^([9])](#ftn9)。要配置这些设置，我们必须提供所有提供者特定的信息以利用Anthos
    Identity Service，并在集群创建后编辑ClientConfig文件[^([10])](#ftn10)。
- en: 'TIP: To restore user cluster kubeconfig you can trigger: kubectl --kubeconfig
    $ADMIN_CLUSTER_KUBECONFIG get secrets -n $USER_CLUSTER_NAME admin -o jsonpath=''{.data.admin\.conf}''
    | base64 -d > $USER_CLUSTER_NAME-kubeconfig'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：要恢复用户集群kubeconfig，可以触发：kubectl --kubeconfig $ADMIN_CLUSTER_KUBECONFIG get
    secrets -n $USER_CLUSTER_NAME admin -o jsonpath='{.data.admin\.conf}' | base64
    -d > $USER_CLUSTER_NAME-kubeconfig
- en: The next option, similar to admin cluster, is the option to enable or disable
    auto repair functionality on configuration file level
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个选项，类似于管理员集群，是在配置文件级别启用或禁用自动修复功能。
- en: '[PRE31]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The key difference from the admin cluster configuration is that it can be easily
    changed by applying changes into the YAML configuration file and triggering a
    gkectl update command.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 与管理员集群配置的关键区别在于，它可以通过将更改应用到YAML配置文件并触发gkectl update命令来轻松更改。
- en: The last part to be covered before we move on to networking is storage. By default
    Anthos on VMware include the vSphere Kubernetes volume plugin that allows dynamically
    provisioning vSphere VMDK disks on top of datastores[^([11])](#ftn11) attached
    to vCenter clusters. After a new user cluster is created, it is configured with
    a default storage class that points to the vSphere datastore. Besides the volume
    connector, newly deployed clusters automatically get the vSphere Container Storage
    Interface (CSI). The CSI is a standard API which allows you to connect directly
    to compatible storage, bypassing vSphere storage. It’s worth mentioning that Anthos
    on VMware clusters still support the use of in-tree vSphere Cloud Provider volume
    plugin that enables a direct connection to storage, bypassing vSphere storage
    as well. However, due to known limitations, like lack of dynamic provisioning
    support, it’s not recommended to use the in-tree plugin - you should use the CSI
    driver instead.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们继续讨论网络之前，需要覆盖的最后一部分是存储。默认情况下，VMware上的Anthos包括vSphere Kubernetes卷插件，该插件允许在连接到vCenter集群的数据存储上动态配置vSphere
    VMDK磁盘[^([11])](#ftn11)。在创建新的用户集群后，它配置了一个默认的存储类，该类指向vSphere数据存储。除了卷连接器外，新部署的集群自动获得vSphere容器存储接口（CSI）。CSI是一个标准API，它允许您直接连接到兼容的存储，绕过vSphere存储。值得一提的是，VMware上的Anthos集群仍然支持使用树内vSphere云提供商卷插件，该插件允许直接连接到存储，绕过vSphere存储。然而，由于已知的限制，如缺乏动态配置支持，不建议使用树内插件
    - 您应使用CSI驱动程序。
- en: We managed to define compute and storage components that are used for Anthos
    on VMware deployment. Let's summarize it. Our build is based on the admin workstation,
    admin cluster and user clusters deployed and hosted on a vSphere environment.
    The picture below presents resource separation based on resource pools dedicated
    for every user cluster and combined admin cluster resources with user master nodes.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经定义了用于VMware上Anthos部署的计算和存储组件。让我们总结一下。我们的构建基于管理员工作站、管理员集群和部署在vSphere环境中的用户集群。下面的图片展示了基于为每个用户集群专用的资源池以及与用户主节点结合的管理集群资源的资源分离。
- en: '![C_09](../../OEBPS/Images/C_09.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![C_09](../../OEBPS/Images/C_09.png)'
- en: Figure C.9 Anthos on VMware resource distribution
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.9 VMware上Anthos的资源分布
- en: You have learned a lot about the compute part of Anthos on VMware implementation.
    In the next section, we will go through details for communication capabilities,
    requirements and limitations depending on network made implementation choice.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经了解了关于VMware上Anthos实现计算部分的很多内容。在下一节中，我们将详细介绍根据网络实施选择进行的通信能力、要求和限制。
- en: C.3.1 Anthos Networking
  id: totrans-211
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3.1 Anthos 网络
- en: To understand the role that networking plays in an Anthos cluster, we need to
    understand that an Anthos cluster consists of two, different, networking models.
    The first is the vSphere network where the entire infrastructure is placed and
    the other is Kubernetes networking.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 要理解网络在Anthos集群中的作用，我们需要了解Anthos集群由两种不同的网络模型组成。第一个是放置整个基础设施的vSphere网络，另一个是Kubernetes网络。
- en: At the beginning of this chapter we stated that Anthos on VMware does not require
    any Software Defined Networking applied on top of vSphere infrastructure and can
    be fully VLAN based.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我们提到VMware上的Anthos不需要在vSphere基础设施上应用任何软件定义网络，并且可以完全基于VLAN。
- en: IP Management
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: IP管理
- en: As we already familiarized ourselves with deployment flow, let’s go deeper in
    configuration and architecture elements.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 既然我们已经熟悉了部署流程，让我们更深入地探讨配置和架构元素。
- en: Besides the configuration files mentioned in previous chapters, there can be
    additional required depending on the deployment scenario. When DHCP is used, all
    nodes have assigned IP addresses from it as presented on Figure C.10.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 除了前几章中提到的配置文件外，根据部署场景还可能有额外的需求。当使用DHCP时，所有节点都从它那里分配了IP地址，如图C.10所示。
- en: '![C_10](../../OEBPS/Images/C_10.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![C_10](../../OEBPS/Images/C_10.png)'
- en: Figure C.10 DHCP based deployment
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.10 基于 DHCP 的部署
- en: If deployment does not utilize DHCP services for node IP allocation, additional
    host configuration files must be created for the admin cluster and each user cluster
    (Figure C.11). Some organizations consider statically assigned addresses to be
    the most stable implementation since it removes any DHCP problems or lease expiration
    for nodes will not introduce any disturbance for node communication. However,
    while it may eliminate any DHCP concerns, it introduces management overhead for
    host configuration files preparation and scalability limitations of created clusters.
    The best implementation is something that you must decide for your cluster and
    organization.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 如果部署没有利用 DHCP 服务进行节点 IP 分配，则必须为管理员集群和每个用户集群创建额外的主机配置文件（图 C.11）。一些组织认为静态分配的地址是最稳定的实现，因为它消除了任何
    DHCP 问题或节点租约到期，不会对节点通信造成任何干扰。然而，虽然它可能消除任何 DHCP 问题，但它引入了主机配置文件准备的管理开销和创建集群的可扩展性限制。最佳实现是您必须为您的集群和组织决定的事情。
- en: '![C_11](../../OEBPS/Images/C_11.png)'
  id: totrans-220
  prefs: []
  type: TYPE_IMG
  zh: '![C_11](../../OEBPS/Images/C_11.png)'
- en: Figure C.11 Static IP assignment scenario
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.11 静态 IP 分配场景
- en: It’s possible to follow a mixed deployment scenario where both DHCP based and
    non DHCP based clusters are deployed as presented in the picture below.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 可以遵循混合部署场景，其中既部署了基于 DHCP 的集群，也部署了非 DHCP 基于的集群，如下面的图片所示。
- en: '![C_12](../../OEBPS/Images/C_12.png)'
  id: totrans-223
  prefs: []
  type: TYPE_IMG
  zh: '![C_12](../../OEBPS/Images/C_12.png)'
- en: Figure C.12 Mixed DHCP and Static IP assignment scenario
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.12 混合 DHCP 和静态 IP 分配场景
- en: With this configuration, we can have an Admin cluster using static IP addresses
    for its management plane and user Kubernetes master nodes, DHCP for 1st user cluster
    and static IP addresses for 2nd user cluster Kubernetes worker nodes or the complete
    opposite. As IP address change of Kubernetes nodes introduces significant problems
    for storage access, it’s recommended to use static IP assignment for admin cluster
    nodes where DHCP can be used for short-lived user clusters.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此配置，我们可以有一个使用静态 IP 地址进行其管理平面和用户 Kubernetes 主节点管理的管理员集群，为第一个用户集群使用 DHCP，为第二个用户集群的
    Kubernetes 工作节点使用静态 IP 地址，或者完全相反。由于 Kubernetes 节点 IP 地址的变化会对存储访问造成重大问题，因此建议为管理员集群节点使用静态
    IP 分配，而 DHCP 可以用于短期用户集群。
- en: 'There are a few constraints related to mixed deployment:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 与混合部署相关有一些限制：
- en: IP assignment must be identical for the entire admin cluster and user cluster
    master nodes as they share the same IP address pool
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: IP 分配必须在整个管理员集群和用户集群主节点中相同，因为它们共享相同的 IP 地址池
- en: All user cluster worker node pools must use the same IP assignment method for
    the entire cluster
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有用户集群工作节点池必须使用整个集群相同的 IP 分配方法
- en: Separate user clusters can use different IP assignment methods even if they
    are managed by the same admin cluster
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 即使由同一个管理员集群管理，不同的用户集群也可以使用不同的 IP 分配方法
- en: So far, we have talked about IP assigned options and arguments for and against
    each of the implementations. Now let's talk about detailed network implementation
    configuration, good practices and recommendations for both management and workload.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经讨论了 IP 分配选项以及每种实现的优缺点。现在让我们谈谈详细的网络实施配置、管理和平摊工作负载的良好实践和建议。
- en: Management plane
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 管理平面
- en: Looking deeper into the management plane network configuration there are two
    elements we need to communicate with, depending on activity that is intended to
    be performed. The first element we must deploy for Anthos on VMware is the admin
    workstation, which is fully preconfigured and hardened by Google.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 深入到管理平面网络配置，我们需要根据要执行的活动与两个元素进行通信。我们必须为 VMware 上的 Anthos 部署的第一个元素是管理员工作站，该工作站由
    Google 完全预配置并加固。
- en: The Second communication point is the entire admin cluster, hosting all admin
    nodes and user cluster master nodes. Both mandate communication with VMware infrastructure
    to automatically deploy Virtual Machines. There is no technical requirement to
    separate admin workstation, nodes and vSphere infrastructure but from a security
    perspective it is highly recommended to isolate those networks on layer 2 as presented
    in the picture below.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个通信点是整个管理员集群，托管所有管理员节点和用户集群主节点。两者都要求与 VMware 基础设施通信以自动部署虚拟机。没有技术要求将管理员工作站、节点和
    vSphere 基础设施分开，但从安全角度来看，强烈建议在图下所示的第二层上隔离这些网络。
- en: '![C_13](../../OEBPS/Images/C_13.png)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![C_13](../../OEBPS/Images/C_13.png)'
- en: Figure C.13 Anthos on VMware vSphere networking
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 图 C.13 Anthos 在 VMware vSphere 网络上的 Anthos
- en: As Anthos on VMware clusters are integrated into GCP console they mandate communication
    with the external world. This connection can be achieved using a direct internet
    connection or via an internet proxy.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Anthos on VMware集群集成到GCP控制台，它们需要与外部世界进行通信。这种连接可以通过直接互联网连接或通过互联网代理来实现。
- en: The default networking model for a new Anthos cluster is known as island mode.
    This means that Pods are allowed to talk to each other but prevented by default
    from being reached from external networks. Another important note is that outgoing
    traffic from Pods to services located outside are NATed by node IP addresses.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 新的Anthos集群的默认网络模型被称为孤岛模式。这意味着Pod可以相互通信，但默认情况下被阻止从外部网络访问。另一个重要的注意事项是，Pod到位于外部的服务的出站流量通过节点IP地址进行NAT。
- en: The same applies to services. They can overlap between clusters but must not
    overlap with Pod subnet (Figure C.14). Additionally Pods and Services subnets
    must not overlap with external services consumed by cluster i.e. internet proxy
    or NTP[^([12])](#ftn12) otherwise traffic will not be routed outside the created
    cluster.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 同样的规则适用于服务。它们可以在集群之间重叠，但不能与Pod子网重叠（图C.14）。此外，Pod和服务的子网也不能与集群消耗的外部服务重叠，例如互联网代理或NTP[^([12])](#ftn12)，否则流量将无法路由到创建的集群外部。
- en: '![C_14](../../OEBPS/Images/C_14.png)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![C_14](../../OEBPS/Images/C_14.png)'
- en: Figure C.14 Pods and services
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.14 Pods和services
- en: Both service CIDR and Pod CIDR are defined in the admin cluster configuration
    YAML file, and built-in preflight checks make sure that IP addresses for both
    do not overlap.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 服务CIDR和Pod CIDR都在管理集群配置YAML文件中定义，内置的预检检查确保两者的IP地址不重叠。
- en: '[PRE32]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Load Balancers
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 负载均衡器
- en: 'To manage a Kubernetes cluster you must reach it’s Kubernetes API server. Admin
    cluster exposes it via LoadBalancer IP that can be configured in 3 flavours depending
    on type. At the time of writing this chapter, Anthos on VMware supports the following
    types: MetalLB, F5BigIp and ManualLB that replace SeeSaw.'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 要管理Kubernetes集群，您必须访问其Kubernetes API服务器。管理集群通过LoadBalancer IP公开它，可以根据类型配置3种风味。在撰写本章时，Anthos
    on VMware支持以下类型：MetalLB、F5BigIp和ManualLB，它们取代了SeeSaw。
- en: Bundled - MetalLB
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 内置 - MetalLB
- en: MetalLB is Google developed open source[^([13])](#ftn13) Cloud Native Computing
    Foundation sandbox project network load-balancer implementation for Kubernetes
    clusters. It’s running on bare metal implementations allowing use of LoadBalancer
    services inside any cluster.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB是Google开发的针对Kubernetes集群的开源[^([13])](#ftn13) Cloud Native Computing Foundation沙盒项目网络负载均衡器实现。它运行在裸机实现上，允许在任何集群中使用LoadBalancer服务。
- en: 'MetalLB addresses two requirements that are part of hyperscaller Kubernetes
    implementations but lacking in on-premises ones, external announcement and address
    allocation. Address allocation provides you capability to automatically assign
    IP address to LoadBalancer service that is created without a need to specify it
    manually. Moreover you can create multiple IP address pools that can be used in
    parallel depending on your needs, for example pool for private IP addresses that
    are used to expose services internally and pool of IP addresses that provides
    external access. As soon as an IP address is assigned it must be announced on
    the network, and their external announcement feature is coming into play. MetalLB
    can be deployed in two modes: layer 2 mode and BGP mode.'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB解决了超大规模Kubernetes实现中的一部分需求，但在本地环境中却缺乏，即外部公告和地址分配。地址分配提供了自动将IP地址分配给创建的LoadBalancer服务的功能，无需手动指定。此外，您可以根据需要创建多个IP地址池，可以并行使用，例如用于内部服务暴露的私有IP地址池和提供外部访问的IP地址池。一旦分配了IP地址，它必须在网络上进行公告，这时其外部公告功能就派上用场。MetalLB可以以两种模式部署：二层模式和BGP模式。
- en: 'Current implementation of Anthos on VMware is using layer 2 mode only. In layer
    2 implementation external announcement is managed in use of standard address discovery
    protocols: ARP for IPv4 and NDP for IPv6\. Each Kubernetes service is presented
    as dedicated MetalLB load balancer, as result when multiple services are created
    traffic is distributed across load balancer nodes. Such implementation has advantages
    and constraints. Key constraint is related to the fact that all traffic for service
    IP is always going to one node where kube-proxy spreads it to all service pods.
    As a result service bandwidth is always limited to single node network bandwidth.
    In case of node failure service is automatically failed over. Such a process should
    take no longer than 10 seconds. When looking at advantages for MetalLB layer 2
    implementation for sure we must mention the fact that it’s fully in cluster implementation
    without any special requirements from physical networks in the area of hardware
    etc. Layer 2 implementations do not introduce any limitation to the amount of
    load balancers created per network as soon as there are IP addresses available
    to be assigned. That is a consequence of using memberlist Go library to maintain
    cluster membership list and member failure detection using gossip based protocol
    instead of for example Virtual Router redundancy Protocol[^([14])](#ftn14).'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 当前VMware上的Anthos的实现仅使用层2模式。在层2实现中，外部通告通过使用标准地址发现协议进行管理：IPv4使用ARP，IPv6使用NDP。每个Kubernetes服务都作为专用的MetalLB负载均衡器呈现，因此当创建多个服务时，流量会在负载均衡器节点之间分配。这种实现具有优点和限制。关键限制与这样一个事实相关，即服务IP的所有流量都将始终流向一个节点，在那里kube-proxy将其传播到所有服务Pod。因此，服务带宽始终限制在单个节点的网络带宽。在节点故障的情况下，服务将自动故障转移。这个过程不应超过10秒。当考虑MetalLB层2实现的优点时，我们当然必须提到这样一个事实，即它是完全集群内的实现，没有任何对物理网络（如硬件等）的特殊要求。层2实现不会对每个网络创建的负载均衡器数量引入任何限制，只要可用IP地址可以分配。这是使用memberlist
    Go库维护集群成员列表和成员故障检测使用基于Gossip协议而不是例如虚拟路由器冗余协议[^([14])](#ftn14)的结果。
- en: '![C_15](../../OEBPS/Images/C_15.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![C_15](../../OEBPS/Images/C_15.png)'
- en: Figure C.15 Admin cluster networking with metalLB
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.15 使用metalLB的管理员集群网络
- en: 'INFO: Admin Cluster Kubernetes VIP is not leveraging MetalLB as the admin cluster
    does not HA implement. All User Clusters are using MetalLB deployment for it’s
    Kubernetes VIP exposure.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: INFO：管理员集群Kubernetes VIP没有利用MetalLB，因为管理员集群没有实现高可用性。所有用户集群都使用MetalLB部署来暴露其Kubernetes
    VIP。
- en: MetalLB is part of Anthos on VMware covered under Anthos license and standard
    support inline with the chosen support model, including lifecycle management activities
    for each release.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: MetalLB是Anthos on VMware的一部分，在Anthos许可下得到覆盖，并且与所选支持模型一起提供标准支持，包括每个版本的生命周期管理活动。
- en: Integrated - F5
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 集成 - F5
- en: The second option to introduce load-balancer capabilities is integration with
    F5 BIG-IP load balancer - called integrated load balancer mode. Compared to MetalLB,
    F5 infrastructure must be prepared upfront and is not deployed automatically by
    Google. For Anthos on VMware, the BIG-IP provides external access and L3/4 load-balancing
    services. When integrated load balancer mode is defined, Anthos on VMware automatically
    performs preflight checks and installs single supported version of F5 BIG-IP container
    ingress services (CIS) controller.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 引入负载均衡器功能的第二种选择是与F5 BIG-IP负载均衡器集成——称为集成负载均衡器模式。与MetalLB相比，F5基础设施必须提前准备，并且不是由Google自动部署。对于VMware上的Anthos，BIG-IP提供外部访问和L3/4负载均衡服务。当定义集成负载均衡器模式时，VMware上的Anthos会自动执行预检查并安装单个支持的F5
    BIG-IP容器入口服务（CIS）控制器。
- en: '![C_16](../../OEBPS/Images/C_16.png)'
  id: totrans-255
  prefs: []
  type: TYPE_IMG
  zh: '![C_16](../../OEBPS/Images/C_16.png)'
- en: Figure C.16 Admin Cluster networking with F5 BIG-IP
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.16 使用F5 BIG-IP的管理员集群网络
- en: Production license provides up to 40 Gbps throughput for Anthos on VMware load
    balancer.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 生产许可证为在VMware负载均衡器上的Anthos提供高达40 Gbps的吞吐量。
- en: BIG-IP integration is fully supported by Google inline with the support compatibility
    matrix but licensed separately under F5 licensing.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: BIG-IP集成在Google的支持兼容性矩阵内得到完全支持，但根据F5许可单独授权。
- en: Manual Load Balancer
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 手动负载均衡器
- en: To allow flexibility and capability to use existing load balancing infrastructure
    of your choice, Anthos on VMware can be deployed in use of manual mode configuration
    of load balancer. In such an implementation there is a need to set up a load balancer
    with Kubernetes API VIP before cluster deployment starts. Configuration steps
    depend on the load balancer you are using. Google provides detailed documentation
    describing BIG-IP and Citrix configuration steps. It’s important that in manual
    mode you cannot expose Services of type LoadBalancer to external clients.
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 为了允许灵活性和使用现有负载均衡基础设施的能力，Anthos on VMware可以通过手动模式配置负载均衡器进行部署。在这种实现中，在集群部署开始之前需要设置一个带有Kubernetes
    API VIP的负载均衡器。配置步骤取决于你使用的负载均衡器。Google提供了详细的文档，描述了BIG-IP和Citrix的配置步骤。在手动模式下，你不能将类型为LoadBalancer的服务暴露给外部客户端。
- en: Due to lack of automated integration with manual load balancing mode, Google
    does not provide support and any encounter issues with the load balancer must
    be managed with the load balancer's vendor.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 由于缺乏与手动负载均衡模式的自动化集成，Google不提供支持，并且任何与负载均衡器相关的问题都必须由负载均衡器的供应商管理。
- en: We learned already about all three mode types. Let's have a look into configuration
    files. Configuration is covered under the dedicated loadBalancer section of the
    admin config yaml file and will vary depending on chosen option.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经了解了所有三种模式类型。让我们看看配置文件。配置在admin配置yaml文件的专用loadBalancer部分中，将根据所选选项而有所不同。
- en: For MetalLB configuration in the admin cluster we must define the loadbalancer
    kind as MetalLB and provide Kubernetes API service VIP.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 对于admin集群中的MetalLB配置，我们必须定义负载均衡器kind为MetalLB并提供Kubernetes API服务VIP。
- en: '[PRE33]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'When F5 BIG-IP integrated mode is chosen, the load balancer section must be
    changed to kind: F5BigIP. Entire MetalLB section (enabled by default on new generated
    config file) must be commented and f5BigIp section must be defined with credentials
    file and partition details.'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '当选择F5 BIG-IP集成模式时，负载均衡器部分必须更改为kind: F5BigIP。整个MetalLB部分（默认情况下在新生成的配置文件中启用）必须注释掉，并且必须使用凭据文件和分区详细信息定义f5BigIp部分。'
- en: '[PRE34]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Last use case is covering manual load balancing. In such a scenario we must
    define kind: ManualLB and comment out the seesaw section. Next we must manually
    define the NodePort configuration options.'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: '最后一个用例是涵盖手动负载均衡。在这种情况下，我们必须定义kind: ManualLB并注释掉seesaw部分。接下来，我们必须手动定义NodePort配置选项。'
- en: '[PRE35]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: User Clusters
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群
- en: User cluster networking is based on the same principles as for admin clusters,
    externed for workload deployment capabilities. Every cluster is deployed in Island
    Mode, where services and Pods CIDRs must not overlap in the user cluster configuration
    file.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群的网络基于与admin集群相同的原理，但为了工作负载部署能力而外部化。每个集群都以孤岛模式部署，其中服务和Pods CIDR在用户集群配置文件中不得重叠。
- en: '[PRE36]'
  id: totrans-271
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Again we have three modes of load-balancer deployment and integration: bundled,
    integrated and manual.'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 我们再次有三种负载均衡器部署和集成模式：捆绑式、集成式和手动式。
- en: Bundled deployment similarly uses MetalLB implementation. You already learned
    that master nodes of the user cluster are deployed into the admin cluster node
    network and IPs are assigned from predefined static pool or DHCP servers. Kubernetes
    API service is also exposed automatically in the same network but its IP address
    must be defined manually in the user cluster configuration file.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 捆绑式部署同样使用MetalLB实现。你已经了解到用户集群的主节点被部署到admin集群节点网络中，并且IP地址是从预定义的静态池或DHCP服务器分配的。Kubernetes
    API服务也自动在同一个网络中暴露，但其IP地址必须在用户集群配置文件中手动定义。
- en: User cluster worker nodes can be deployed into the same network as admin nodes
    or into separate dedicated network. The second option is preferred as it allows
    the separation of traffic from the management plane. Each new user cluster comes
    with a new dedicated MetalLB load-balancer for the dataplane. Control plane VIP
    for Kubernetes API is always co hosted in the admin cluster load balancer instance.
    User Cluster ingress VIP is automatically deployed into the cluster worker nodes
    pool network. As a result you can limit MetalLB to be hosted on a dedicated node
    pool instead of all nodes in the user cluster.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群的工作节点可以部署在与admin节点相同的网络中，或者部署到单独的专用网络中。第二种选择更受欢迎，因为它允许将管理平面的流量分离。每个新的用户集群都附带一个新的专用MetalLB负载均衡器用于数据平面。Kubernetes
    API的控制平面VIP始终与admin集群负载均衡器实例共同托管。用户集群的ingress VIP自动部署到集群工作节点池网络中。因此，你可以将MetalLB限制在专用节点池上托管，而不是在用户集群的所有节点上。
- en: '![C_17](../../OEBPS/Images/C_17.png)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![C_17](../../OEBPS/Images/C_17.png)'
- en: Figure C.17 User Cluster networking with MetalLB
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.17 使用MetalLB的用户集群网络
- en: In multi-cluster deployment user clusters can share a single network or use
    a dedicated one. When using a single one, make sure that node IPs are not overlapping
    in configuration files.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 在多集群部署中，用户集群可以共享单个网络或使用专用网络。当使用单个网络时，请确保配置文件中的节点IP地址不重叠。
- en: As we already mentioned, MetalLB is a fully bundled load-balancer. That means
    every time when service type LoadBalancer is created VIP is automatically created
    on load-balancer and traffic sent to the VIP is forwarded to Service. MetalLB
    has IP address management (IPAM) as a result IP addresses for each service are
    assigned automatically. Definition of IP pools and nodes that host MetalLB VIPs
    is defined in user-cluster load balancer configuration file section as presented
    bellow
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们已经提到的，MetalLB是一个完全捆绑的负载均衡器。这意味着每次创建服务类型LoadBalancer时，VIP都会在负载均衡器上自动创建，发送到VIP的流量将被转发到服务。由于MetalLB具有IP地址管理（IPAM），因此每个服务的IP地址都会自动分配。IP池和托管MetalLB
    VIPs的节点的定义在用户集群负载均衡器配置文件部分中，如下所示。
- en: '[PRE37]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Additionally we must allow MetalLB service on one (or more) Virtual Machine
    pools in worker pools section described already in details earlier in the book
    by defining configuration in form “enableLoadBalancer: true”'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '此外，我们必须在已详细描述的worker pools部分允许MetalLB服务在一个（或多个）虚拟机池上，通过定义配置为“enableLoadBalancer:
    true”的形式。'
- en: Integrated load balancing allows integration into F5 BIG-IP and provision LoadBalancer
    service type automatically similar to the way we described for admin cluster integration.
    In that scenario, the integration point can be the same for admin and user clusters
    and there is no need to use separate instances of F5s. It’s important to remember
    that every new cluster must be pre-configured and properly prepared on the load
    balancer side before deployment.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 集成负载均衡允许集成到F5 BIG-IP，并自动提供LoadBalancer服务类型，类似于我们描述的admin集群集成。在这种情况下，集成点可以是admin和用户集群相同的，不需要使用F5的单独实例。重要的是要记住，每个新的集群在部署之前必须在负载均衡器侧预先配置并适当准备。
- en: '![C_18](../../OEBPS/Images/C_18.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![C_18](../../OEBPS/Images/C_18.png)'
- en: Figure C.18 User Cluster networking with F5 BIG-IP
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.18 使用F5 BIG-IP的用户集群网络
- en: Manual mode implementations for user cluster load balancers are following the
    same rules, introducing the same constraints and limitations as for admin clusters.
    Every service exposure mandates contact with external teams and manual activities
    as described in official documentation[^([15])](#ftn15).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 用户集群负载均衡器的手动模式实现遵循相同的规则，引入相同的约束和限制，如admin集群。每个服务暴露都需要与外部团队联系，并执行官方文档中描述的手动活动[^([15])](#ftn15)。
- en: In this section of Anthos on VMware, we went through different network configuration
    options for management and workload clusters. It’s important to remember that
    similarly to IP assignment, we can decide to use a single load-balancer integration
    mode or choose different modes for different clusters. As a result we can have
    a MetalLB based management cluster, one MetalLB user cluster, second F5 BIG-IP
    integrated user cluster and third manually integrated Citrix Netscaler load balancer.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们探讨了Anthos on VMware中管理集群和工作负载集群的不同网络配置选项。重要的是要记住，与IP分配类似，我们可以选择使用单个负载均衡器集成模式，或者为不同的集群选择不同的模式。因此，我们可以拥有基于MetalLB的管理集群、一个MetalLB用户集群、第二个集成F5
    BIG-IP的用户集群以及第三个手动集成的Citrix Netscaler负载均衡器。
- en: '[PRE38]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: C.3.2 GCP integration capabilities
  id: totrans-287
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: C.3.2 GCP集成功能
- en: In previous sections we talked about compute and network architecture of Anthos
    on VMware. In that section we will cover different integration capabilities for
    GCP services to leverage a single panel of glass for all Anthos clusters regardless
    if they are deployed on premise or on other clouds .
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们讨论了Anthos on VMware的计算和网络架构。在本节中，我们将介绍GCP服务的不同集成功能，以便利用单个控制面板来管理所有Anthos集群，无论它们是在本地部署还是在其他云上。
- en: As you already noticed during deployment of new admin and user clusters it’s
    mandatory to define a GCP project that it will be integrated into. That enables
    the connect agent to properly register and establish communication with the hub
    as described in detail in the chapter Operations Management. In general GKE Connect
    is performing two activities, enabling connectivity and authentication to register
    new clusters.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在部署新的管理员和用户集群时已经注意到的，定义一个将被集成到其中的GCP项目是强制性的。这使连接代理能够正确注册并与中心建立通信，如操作管理章节中详细描述的那样。一般来说，GKE
    Connect执行两个活动，启用连接和身份验证以注册新集群。
- en: '![C_19](../../OEBPS/Images/C_19.png)'
  id: totrans-290
  prefs: []
  type: TYPE_IMG
  zh: '![C_19](../../OEBPS/Images/C_19.png)'
- en: Figure C.19 Fleet relationship to Connect Agents
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 图C.19 舰队与连接代理的关系
- en: For that purpose two dedicated service accounts are used. That section is mandatory
    and must be properly defined for the user cluster. Note that due to the fact that
    the agent service account is using Workload Identity functionality it does not
    require a key file.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 为此目的，使用了两个专用的服务帐户。该部分是强制性的，并且必须为用户集群正确定义。请注意，由于代理服务帐户使用工作负载身份功能，因此它不需要密钥文件。
- en: '[PRE39]'
  id: totrans-293
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'NOTE: GKE connect plays a significant role in on prem integration into GCP.
    That allows us to utilize directly from GCP console Cloud Marketplace, Cloud Run,
    options to integrate with CICD toolchain via Anthos authorization without a need
    to expose Kubernetes API externally.'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：GKE connect在本地到GCP集成中发挥着重要作用。这使我们能够直接从GCP控制台云市场、云运行选项中利用它，通过Anthos授权与CICD工具链集成，而无需公开Kubernetes
    API。
- en: Anthos on VMware has the ability to send infrastructure logs and metrics to
    GCP Cloud Monitoring. It applies for both admin and user clusters. We can choose
    to send only Kubernetes related metrics or include vSphere metrics as well. Metrics
    for each cluster can be sent to different projects.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos on VMware具有将基础设施日志和指标发送到GCP云监控的能力。这适用于管理员和用户集群。我们可以选择仅发送与Kubernetes相关的指标，也可以包括vSphere指标。每个集群的指标可以发送到不同的项目。
- en: '[PRE40]'
  id: totrans-296
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Another integration feature that is applied for both admin and user clusters
    is the capability to send Kubernetes API server audit logs. As previously described,
    we can choose the project and location region where logs are stored and the service
    account that is used for that integration.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个适用于管理员和用户集群的集成功能是能够发送Kubernetes API服务器审计日志。如前所述，我们可以选择存储日志的项目和位置区域以及用于该集成服务帐户。
- en: '[PRE41]'
  id: totrans-298
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Last two integration features are applied to user clusters only. First of them
    is an option to consume from Google Cloud Platform console CloudRun for Anthos
    (described in detail in “Anthos, the serverless compute engine (Knative)” chapter)
    and deploy services directly into Anthos on VMware clusters. There is not much
    configuration there as the service itself is leveraging connect functionality
    and deploying Knative in a dedicated namespace of the user cluster. That means
    it must be enabled on the same project as Anthos on which the VMware cluster is
    registered. You will learn more about CloudRun for Anthos and Knative on Anthos
    Cloud Run chapter and related documentation[^([16])](#ftn16).
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两个集成功能仅适用于用户集群。第一个是选择从Google Cloud Platform控制台云运行 Anthos（在“Anthos，无服务器计算引擎（Knative）”章节中详细描述）中消费，并将服务直接部署到VMware集群上的Anthos。在这里没有太多配置，因为服务本身正在利用连接功能，并在用户集群的专用命名空间中部署Knative。这意味着它必须在与VMware集群注册的Anthos相同的项目中启用。您将在“Anthos
    Cloud Run”章节和相关的文档中了解更多关于云运行 Anthos 和 Knative 的信息[^([16])](#ftn16)。
- en: Feature list description is closed by metering. After enabling the metering
    feature, the user cluster sends resource usage and consumption data to Google
    Bigquery. It allows us to analyze it and size our clusters inline with actual
    demand or expose them and present them as reports for example in Data Studio.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 功能列表描述以计量结束。启用计量功能后，用户集群将资源使用和消耗数据发送到Google Bigquery。这使我们能够分析它，并根据实际需求调整集群大小，或者将其公开并作为报告等展示。
- en: '[PRE42]'
  id: totrans-301
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: C.4 Summary
  id: totrans-302
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: C.4 摘要
- en: Anthos on VMware is a great option to consume cloud native capabilities on top
    of existing vSphere infrastructure
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现有的vSphere基础设施之上消费云原生功能的Anthos on VMware是一个很好的选择
- en: 'Architecture is composed from two elements:'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 架构由两个元素组成：
- en: user clusters that are responsible for delivery of resources for hosted applications
  id: totrans-305
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责托管应用程序资源交付的用户集群
- en: admin control plane that is responsible for management and control of deployed
    user clusters
  id: totrans-306
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负责管理和控制已部署用户集群的管理员控制平面
- en: It can be used as an add-on to co-hosted Virtual Machines as well as dedicated
    on prem Kubernetes implementations that are ready for hybrid cloud implementation
    of cloud native journey.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以用作与托管虚拟机共存的附加组件，以及为混合云实施和云原生之旅准备的本地 Kubernetes 实现。
- en: We can leverage existing VMware skills for infrastructure management, keep full
    visibility of Cloud Operations and consume GCP services if needed from your own
    data center.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以利用现有的 VMware 技能进行基础设施管理，保持对云操作的全面可见性，并在需要时从自己的数据中心消费 GCP 服务。
- en: Setup can be independent and self contained in use of bundled features like
    MetalLB or integrated into existing infrastructure and what automation capabilities
    and constraints it brings
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 设置可以是独立的和自包含的，使用捆绑功能如 MetalLB，或者集成到现有基础设施中，以及它带来的自动化能力和约束。
- en: Cluster configurations may vary depending on purpose, size and availability
    requirements.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集群配置可能因目的、规模和可用性要求而异。
- en: '* * *'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: '[^([1])](#ftnref1) [https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic#resource_requirements_for_admin_workstation_admin_cluster_and_user_clusters](https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic#resource_requirements_for_admin_workstation_admin_cluster_and_user_clusters)'
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([1])](#ftnref1) [https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic#resource_requirements_for_admin_workstation_admin_cluster_and_user_clusters](https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic#resource_requirements_for_admin_workstation_admin_cluster_and_user_clusters)'
- en: '[^([2])](#ftnref2) [https://developers.google.com/identity/protocols/oauth2/openid-connect](https://developers.google.com/identity/protocols/oauth2/openid-connect)'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([2])](#ftnref2) [https://developers.google.com/identity/protocols/oauth2/openid-connect](https://developers.google.com/identity/protocols/oauth2/openid-connect)'
- en: '[^([3])](#ftnref3) [https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic](https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic)'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([3])](#ftnref3) [https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic](https://cloud.google.com/anthos/gke/docs/on-prem/how-to/vsphere-requirements-basic)'
- en: '[^([4])](#ftnref4) [https://cloud.google.com/anthos/clusters/docs/on-prem/version-history](https://cloud.google.com/anthos/clusters/docs/on-prem/version-history)'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([4])](#ftnref4) [https://cloud.google.com/anthos/clusters/docs/on-prem/version-history](https://cloud.google.com/anthos/clusters/docs/on-prem/version-history)'
- en: '[^([5])](#ftnref5) Sections are populated if --auto--create-service-accounts
    flag is used'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([5])](#ftnref5) 如果使用 --auto--create-service-accounts 标志，则会填充部分。'
- en: '[^([6])](#ftnref6) [https://metallb.universe.tf/installation/clouds/](https://metallb.universe.tf/installation/clouds/)'
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([6])](#ftnref6) [https://metallb.universe.tf/installation/clouds/](https://metallb.universe.tf/installation/clouds/)'
- en: '[^([7])](#ftnref7) vSphere High Availability feature can mitigate that behavior
    and decrease downtime of Kubernetes API to minutes until VM is restarted on new
    host. vSphere HA will not protect against Virtual Machine corruption.'
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([7])](#ftnref7) vSphere 高可用性功能可以减轻这种行为，并将 Kubernetes API 的停机时间减少到分钟，直到虚拟机在新主机上重新启动。vSphere
    HA 不会保护虚拟机免受损坏。'
- en: '[^([8])](#ftnref8) For vSAN based deployments all nodes must be placed on the
    same datastore.'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([8])](#ftnref8) 对于基于 vSAN 的部署，所有节点都必须放置在同一个数据存储上。'
- en: '[^([9])](#ftnref9) Anthos on VMware supports all certified OpenID providers.
    Full list can be found under [https://openid.net/certification/](https://openid.net/certification/)'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([9])](#ftnref9) Anthos on VMware 支持所有认证的 OpenID 提供商。完整列表可以在 [https://openid.net/certification/](https://openid.net/certification/)
    下找到。'
- en: '[^([10])](#ftnref10) [https://cloud.google.com/anthos/identity/setup/per-cluster](https://cloud.google.com/anthos/identity/setup/per-cluster)'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([10])](#ftnref10) [https://cloud.google.com/anthos/identity/setup/per-cluster](https://cloud.google.com/anthos/identity/setup/per-cluster)'
- en: '[^([11])](#ftnref11) vSphere datastores can be backed by any block device,
    vSAN or NFS storage'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([11])](#ftnref11) vSphere 数据存储可以由任何块设备、vSAN 或 NFS 存储支持。'
- en: '[^([12])](#ftnref12) Network Time Protocol'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([12])](#ftnref12) 网络时间协议'
- en: '[^([13])](#ftnref13) [https://github.com/metallb/metallb](https://github.com/metallb/metallb)'
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([13])](#ftnref13) [https://github.com/metallb/metallb](https://github.com/metallb/metallb)'
- en: '[^([14])](#ftnref14) [https://tools.ietf.org/html/rfc3768](https://tools.ietf.org/html/rfc3768)'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([14])](#ftnref14) [https://tools.ietf.org/html/rfc3768](https://tools.ietf.org/html/rfc3768)'
- en: '[^([15])](#ftnref15) [https://cloud.google.com/anthos/clusters/docs/on-prem/how-to/manual-load-balance](https://cloud.google.com/anthos/clusters/docs/on-prem/how-to/manual-load-balance)'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: '[^([15])](#ftnref15) [https://cloud.google.com/anthos/clusters/docs/on-prem/how-to/manual-load-balance](https://cloud.google.com/anthos/clusters/docs/on-prem/how-to/manual-load-balance)'
- en: '[^([16])](#ftnref16) [https://cloud.google.com/anthos/run/docs/install/outside-gcp/vmware](https://cloud.google.com/anthos/run/docs/install/outside-gcp/vmware)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
