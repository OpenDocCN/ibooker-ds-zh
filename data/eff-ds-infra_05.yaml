- en: 5 Practicing scalability and performance
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 实践可扩展性和性能
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Developing a realistic, performant data science project iteratively
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逐步开发一个现实、高性能的数据科学项目
- en: Using the compute layer to power demanding operations, such as parallelized
    model training
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用计算层来支持对计算要求高的操作，例如并行化模型训练
- en: Optimizing the performance of numerical Python code
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化数值 Python 代码的性能
- en: Using various techniques to make your workflows more scalable and performant
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用各种技术使你的工作流程更具可扩展性和性能
- en: In the previous chapter, we discussed how scalability is not only about being
    able to handle more demanding algorithms or handle more data. At the organizational
    level, the infrastructure should scale to a large number of projects developed
    by a large number of people. We recognized that scalability and performance are
    separate concerns—you can have one without the other. In fact, the different dimensions
    of scalability and performance can be at odds with each other.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们讨论了可扩展性不仅关乎能够处理更复杂的算法或处理更多数据。在组织层面上，基础设施应该扩展到由大量人员开发的大量项目。我们认识到，可扩展性和性能是两个独立的问题——你可以有其中一个而没有另一个。事实上，可扩展性和性能的不同维度可能相互矛盾。
- en: 'Imagine having an experienced engineer implementing a highly optimized, high-performance
    solution in the C++ language. Although the solution scales at the technical level,
    it is not very scalable organizationally if no one else in the team knows the
    C++ language. Conversely, you can imagine a very high-level ML solution that builds
    models with a click of a button. Everyone knows how to click the button, but the
    solution is too inflexible to scale to a wide variety of projects and is unable
    to handle large amounts of data. This chapter advocates for a pragmatic approach
    to scalability and performance that features the following points:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，一个经验丰富的工程师正在用 C++ 语言实现一个高度优化、高性能的解决方案。尽管这个解决方案在技术层面上可以扩展，但如果团队中没有其他人懂得
    C++ 语言，那么在组织层面上它的可扩展性就不是很强。相反，你可以想象一个非常高级的机器学习解决方案，只需点击一下按钮就能构建模型。每个人都知道如何点击按钮，但这个解决方案过于不灵活，无法扩展到各种不同的项目，并且无法处理大量数据。本章提倡一种务实的方法来处理可扩展性和性能，其特点如下：
- en: Effective infrastructure needs to handle a wide variety of projects, so instead
    of a one-size-fits-all solution, it can provide an easy-to-use toolbox of robust
    ways to achieve *good enough* scalability and performance.
  id: totrans-8
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有效的基础设施需要处理各种项目，因此，而不是提供一个一刀切解决方案，它可以提供一个易于使用的工具箱，包含稳健的方法来实现*足够好*的可扩展性和性能。
- en: To address organizational scalability—we want to make projects understandable
    by the widest number of people—our main tool is *simplicity*. People have a limited
    cognitive bandwidth, so overengineering and overoptimizing incur a real human
    cost.
  id: totrans-9
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了解决组织可扩展性——我们希望让尽可能多的人理解项目——我们的主要工具是*简单性*。人们的认知带宽有限，因此过度工程化和过度优化会带来真实的人类成本。
- en: We can summarize these two points in a simple mnemonic, presented in figure
    5.1.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用一个简单的助记符来总结这两点，如图 5.1 所示。
- en: '![CH05_F01_Tuulos](../../OEBPS/Images/CH05_F01_Tuulos.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F01_Tuulos](../../OEBPS/Images/CH05_F01_Tuulos.png)'
- en: Figure 5.1 A pragmatic approach to scalability
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 一种务实的方法来处理可扩展性
- en: 'Here, *simple* refers to the idea that anyone new to the project can view the
    source code and quickly comprehend how it works. *Complex* is the opposite: it
    takes a lot of effort to understand how the code works. *Slow* means that the
    solution may hit scalability limits, and it makes people wait for results longer
    than what would be optimal, but, nonetheless, *it works*. *Fast* means that the
    solution is perfectly adequate for the problem at hand: it scales well enough
    and provides results quickly enough.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，*简单*指的是任何新加入项目的人都可以查看源代码，并快速理解其工作原理。*复杂*则相反：理解代码的工作原理需要付出很多努力。*慢*意味着解决方案可能会遇到可扩展性的限制，它使得人们等待结果的时间比理想状态更长，但无论如何，*它还是可以工作的*。*快*意味着解决方案对于当前的问题来说完全足够：它有足够的可扩展性，并且可以快速提供结果。
- en: 'By optimizing for simplicity, we also optimize for the *validity* of results.
    As Tony Hoare, a famous computer scientist, has said, “There are two ways to write
    code: write code so simple there are obviously no bugs in it, or write code so
    complex that there are no obvious bugs in it.” Because data science applications
    tend to be statistical by nature—bugs and biases can lurk in models without producing
    clear error messages—you should prefer simple code over non-obvious bugs. Only
    when it is absolutely clear that the application requires higher scalability or
    performance should you increase its complexity proportionally.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 通过优化简单性，我们也优化了结果的有效性。正如著名计算机科学家托尼·霍尔所说：“有两种编写代码的方式：编写如此简单的代码以至于显然没有错误，或者编写如此复杂的代码以至于没有明显的错误。”由于数据科学应用程序本质上是统计性的——错误和偏差可能潜伏在模型中而不产生清晰的错误信息，因此您应该更喜欢简单的代码而不是不明显的问题。只有当应用程序确实需要更高的扩展性或性能时，您才应该按比例增加其复杂性。
- en: In this chapter, we will develop a realistic ML application that has nontrivial
    requirements for scalability and performance. We practice developing a data science
    application incrementally, always striving for the simplest possible approach
    that is correct and delivers the desired results. In other words, we want to stay
    on the first row of figure 5.1\. We will demonstrate a number of approaches that
    help in achieving good enough scalability and performance.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将开发一个对扩展性和性能有非平凡要求的实用机器学习（ML）应用程序。我们练习逐步开发数据科学应用程序，始终追求最简单的方法，这种方法是正确的并产生预期的结果。换句话说，我们希望保持在图5.1的第一行。我们将展示一些有助于实现足够好的扩展性和性能的方法。
- en: 'We will use the tools introduced in the previous chapter: vertical and horizontal
    scalability using the compute layer. Although it is possible to run the examples
    on a laptop, they are more fun and realistic if you have a cloud-based compute
    layer like AWS Batch set up as instructed earlier. As before, we will use Metaflow
    to demonstrate the concepts and get hands-on practice, but you can adapt the examples
    to other frameworks because the general principles are framework-agnostic. You
    can find all code listings for this chapter at [http://mng.bz/yvRE](http://mng.bz/yvRE).'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用上一章中介绍的工具：使用计算层进行垂直和水平扩展。虽然可以在笔记本电脑上运行示例，但如果您已经按照之前的说明设置了基于云的计算层（如AWS Batch），那么它们将更有趣和更真实。像以前一样，我们将使用Metaflow来展示概念并获得实际操作经验，但您可以将示例适应到其他框架，因为通用原则是框架无关的。您可以在[http://mng.bz/yvRE](http://mng.bz/yvRE)找到本章的所有代码列表。
- en: '5.1 Starting simple: Vertical scalability'
  id: totrans-17
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 从简单开始：垂直扩展性
- en: We will start building a realistic ML application that uses natural language
    processing (NLP) to model and analyze Yelp reviews. We will follow the spiral
    approach, introduced in chapter 3 and depicted in figure 5.2, to develop the application.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将开始构建一个使用自然语言处理（NLP）来建模和分析Yelp评论的实用机器学习（ML）应用程序。我们将遵循第3章中介绍的螺旋方法，如图5.2所示，来开发该应用程序。
- en: '![CH05_F02_Tuulos](../../OEBPS/Images/CH05_F02_Tuulos.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F02_Tuulos](../../OEBPS/Images/CH05_F02_Tuulos.png)'
- en: Figure 5.2 The spiral approach with optimizations as the last step
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.2 螺旋方法，优化作为最后一步
- en: 'Although the topic of this chapter is *practicing scalability*, the following
    steps precede any scalability concerns, as shown in figure 5.2:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管本章的主题是*实践扩展性*，但以下步骤在图5.2中显示，在考虑任何扩展性问题之前：
- en: Understand the business problem thoroughly. Maybe the business context allows
    us to go with a simpler, less scalable, but more obviously correct solution.
  id: totrans-22
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 深入了解业务问题。也许业务环境允许我们采用更简单、扩展性较低但更明显正确的解决方案。
- en: Get access to relevant input data, and make sure that the data is correct and
    will stay correct. Also, estimate the scale and the growth rate of the data.
  id: totrans-23
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取相关输入数据，并确保数据是正确的，并且将保持正确。同时，估计数据的规模和增长率。
- en: Make sure that the results of your application can be consumed properly and
    that they produce the desired action.
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 确保您的应用程序的结果可以被正确消费，并且它们产生预期的行动。
- en: Develop a small-scale but functional prototype that allows you to test the application
    with real data to ensure its correctness end-to-end.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开发一个小型但功能齐全的原型，以便您可以使用真实数据测试应用程序，确保其端到端正确性。
- en: 'To implement these steps, we can choose the simplest scalability approach that
    allows us to build a functional prototype. The first version doesn’t have to be
    particularly scalable. We can fix it later, after we have confirmed that everything
    else works. To quote Kent Beck, an accomplished software architect, our order
    of priorities should be: “Make it work, make it right, make it fast.”'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 为了实施这些步骤，我们可以选择最简单的可扩展方法，这样我们就能构建一个功能原型。第一个版本不需要特别可扩展。我们可以在确认其他一切正常后，再对其进行修复。引用软件架构师Kent
    Beck的话，我们的优先顺序应该是：“先让它工作，再让它正确，最后让它快速。”
- en: '5.1.1 Example: Clustering Yelp reviews'
  id: totrans-27
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 示例：聚类Yelp评论
- en: 'Let’s start with a hypothetical business problem: a startup wants to build
    a better version of Yelp, the review site. To understand the strengths and weaknesses
    of Yelp’s product, they want to analyze the different types of reviews that people
    have contributed to Yelp.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从假设的商业问题开始：一家初创公司希望构建一个更好的Yelp版本，一个评论网站。为了了解Yelp产品的优势和劣势，他们希望分析人们为Yelp贡献的不同类型的评论。
- en: We don’t have any existing taxonomy of reviews, so instead of classifying reviews
    to known buckets, we will rely on *unsupervised learning*, which, in this case,
    groups Yelp reviews to sets of similar-looking reviews. You can read more about
    unsupervised learning and document clustering in the documentation for Scikit-Learn
    at [http://mng.bz/M5Mm](http://mng.bz/M5Mm).
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们没有现有的评论分类法，所以我们不会将评论分类到已知的类别中，而是将依赖于*无监督学习*，在这种情况下，将Yelp评论分组为看起来相似的评论集。你可以在Scikit-Learn的文档中了解更多关于无监督学习和文档聚类的信息，见[http://mng.bz/M5Mm](http://mng.bz/M5Mm)。
- en: To accomplish the task, we can access a publicly available corpus of 650,000
    Yelp reviews. The dataset is publicly available by Fast.AI ([https://course.fast.ai/datasets](https://course.fast.ai/datasets))
    and is conveniently hosted in AWS S3 by the Registry of Open Data on AWS at [https://registry.opendata.aws/fast-ai-nlp/](https://registry.opendata.aws/fast-ai-nlp/).
    The dataset is about 500 MB uncompressed, so it is large enough to practice scalability
    but small enough to be handled on any medium-size cloud instance or a workstation.
    A good starting point is to see what the data looks like. A Jupyter notebook works
    well for this purpose, as depicted in figure 5.3.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 为了完成这个任务，我们可以访问一个包含65万条Yelp评论的公开可用语料库。数据集由Fast.AI([https://course.fast.ai/datasets](https://course.fast.ai/datasets))公开提供，并且方便地托管在AWS
    S3的AWS开放数据注册处[https://registry.opendata.aws/fast-ai-nlp/](https://registry.opendata.aws/fast-ai-nlp/)。数据集未压缩时大约有500
    MB，因此足够大，可以练习可扩展性，但足够小，可以在任何中型云实例或工作站上处理。一个好的起点是查看数据看起来是什么样子。Jupyter笔记本非常适合这个目的，如图5.3所示。
- en: '![CH05_F03_Tuulos](../../OEBPS/Images/CH05_F03_Tuulos.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F03_Tuulos](../../OEBPS/Images/CH05_F03_Tuulos.png)'
- en: Figure 5.3 Inspecting the Yelp dataset in a notebook
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 在笔记本中检查Yelp数据集
- en: The next listing shows the code used in figure 5.3.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个列表显示了图5.3中使用的代码。
- en: Listing 5.1 Inspecting the Yelp review dataset
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.1 检查Yelp评论数据集
- en: '[PRE0]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Uses Metaflow’s built-in S3 client to load the publicly available Yelp dataset
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用Metaflow内置的S3客户端加载公开可用的Yelp数据集
- en: ❷ Extracts a data file from the tar package
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从tar包中提取数据文件
- en: ❸ Loads all reviews, one review per line, in a list
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将所有评论按行加载到一个列表中
- en: ❹ Prints the first two reviews as a sample
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印前两个评论作为示例
- en: Here, we use Metaflow’s built-in S3 client to load data from Amazon S3—you will
    learn more about it in chapter 7\. The dataset is stored in a compressed tar archive,
    which we uncompress to extract reviews. There’s one review per line, prefixed
    by a star rating. Our application doesn’t need to care about the rating column.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用Metaflow内置的S3客户端从Amazon S3加载数据——你将在第7章中了解更多关于它的内容。数据集存储在一个压缩的tar归档中，我们解压缩它以提取评论。每条评论一行，前面带有星级评分。我们的应用程序不需要关心评分列。
- en: Recommendation The dataset used in this chapter, yelp_review_full_csv.tgz, is
    about 200 MB. Downloading it over a slow internet connection might take a few
    minutes. If the examples feel too slow on a laptop, consider using a cloud workstation,
    such as AWS Cloud9 IDE, to execute all examples in this chapter.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：本章中使用的数据集yelp_review_full_csv.tgz大约有200 MB。通过慢速互联网连接下载可能需要几分钟。如果笔记本电脑上的示例感觉太慢，请考虑使用云工作站，例如AWS
    Cloud9 IDE，以执行本章中的所有示例。
- en: 'You can see a few samples of the review data in figure 5.3\. As expected, reviews
    are arbitrary paragraphs of written English. Before we can perform any clustering
    on the data, we must convert the strings to a numerical representation. Such a
    vectorization step is a common preprocessing step in machine learning tasks that
    involve natural language. Don’t worry if you haven’t done any NLP before: we will
    cover everything you need to know in the next subsection.'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在图 5.3 中看到一些评论数据的样本。正如预期的那样，评论是任意段落的书面英语。在我们可以对数据进行任何聚类之前，我们必须将字符串转换为数值表示。这种向量化步骤是涉及自然语言的机器学习任务中的常见预处理步骤。如果您之前没有进行过任何自然语言处理（NLP），请不要担心：我们将在下一小节中涵盖您需要了解的所有内容。
- en: One-minute primer to natural language processing
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理一分钟入门
- en: A classic way to encode natural language in a numerical form is called the *bag-of-words*
    representation. Using the bag-of-words model, we can represent a collection of
    documents as a matrix where each row is a document and columns correspond to all
    unique words across all documents. The order of the columns and rows is arbitrary.
    The values of the matrix indicate the number of times each word occurs in a document.
    Figure 5.4 illustrates the concept.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 将自然语言编码为数值形式的一种经典方法是称为*词袋*表示。使用词袋模型，我们可以将一组文档表示为一个矩阵，其中每一行是一个文档，列对应于所有文档中的所有唯一单词。列和行的顺序是任意的。矩阵的值表示每个单词在文档中出现的次数。图
    5.4 说明了这个概念。
- en: '![CH05_F04_Tuulos](../../OEBPS/Images/CH05_F04_Tuulos.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F04_Tuulos](../../OEBPS/Images/CH05_F04_Tuulos.png)'
- en: Figure 5.4 Bag-of-words matrix
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 词袋矩阵
- en: Note how most values in the matrix in figure 5.4 are zero. This is expected,
    because practically all documents contain only a small subset of all possible
    words. Hence, it is common to encode the matrix as a *sparse matrix*, meaning
    that we use a data structure that allows us to store only the nonzero elements
    of the matrix. In the following examples, we will use Scikit-Learn’s scipy.sparse
    module to store documents as bag-of-words sparse matrices. We will dive deeper
    into how these matrices are implemented internally in section 5.3.1.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 注意图 5.4 中的矩阵中大多数值都是零。这是预期的，因为实际上所有文档都只包含所有可能单词的小子集。因此，通常将矩阵编码为*稀疏矩阵*，这意味着我们使用一种数据结构，只存储矩阵的非零元素。在接下来的例子中，我们将使用
    Scikit-Learn 的 scipy.sparse 模块将文档存储为词袋稀疏矩阵。我们将在 5.3.1 节中深入了解这些矩阵是如何在内部实现的。
- en: You can get surprisingly good results in many NLP tasks, like classification
    and clustering, using a simple bag-of-words representation, despite the fact that
    it loses the order of words. In a few of the examples that follow, we will perform
    document clustering. We want to group documents in K non-overlapping groups or
    clusters so that documents assigned to the same cluster are maximally *similar*
    to each other. Crucially, you have to choose the number of clusters K beforehand—there
    isn’t a universally agreed-upon method within the data science community of choosing
    it automatically.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管词袋表示丢失了单词的顺序，但您可以在许多自然语言处理（NLP）任务中，如分类和聚类中，使用简单的词袋表示获得令人惊讶的好结果。在接下来的几个例子中，我们将执行文档聚类。我们希望将文档分组到
    K 个非重叠的组或簇中，使得分配到同一簇的文档彼此之间尽可能*相似*。关键的是，您必须事先选择簇的数量 K——在数据科学社区中并没有一个普遍认同的自动选择它的方法。
- en: In this example, *document similarity* refers to the number of common words
    among documents. For instance, the first two documents in figure 5.4 have two
    words in common (are, fun) versus the third document, which has at most one word
    in common with them. Hence, it makes sense to assign the first two documents in
    one cluster and the third document in a separate cluster.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，*文档相似度*指的是文档之间共同单词的数量。例如，图 5.4 中的前两个文档有两个共同单词（are, fun），而第三个文档与它们最多只有一个共同单词。因此，将前两个文档分配到一个簇中，而将第三个文档分配到单独的簇中是有意义的。
- en: To perform clustering, we will use the most well-known clustering technique,
    the K-means clustering algorithm, as implemented in Scikit-Learn. If you are curious,
    you can read more about K-means at [http://mng.bz/aJlY](http://mng.bz/aJlY). The
    details of the algorithm don’t matter here, besides the fact that it is a somewhat
    demanding algorithm computationally—the execution time grows quadratically in
    terms of the matrix size. This makes it a fun and realistic test case for scalability
    and performance.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行聚类，我们将使用最著名的聚类技术，即 K-means 聚类算法，该算法在 Scikit-Learn 中实现。如果您感兴趣，可以在 [http://mng.bz/aJlY](http://mng.bz/aJlY)
    上了解更多关于 K-means 的信息。算法的细节在这里并不重要，除了它是一个计算上有些要求较高的算法——执行时间随矩阵大小的平方增长。这使得它成为测试可扩展性和性能的一个有趣且现实的情况。
- en: 5.1.2 Practicing vertical scalability
  id: totrans-51
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 练习垂直可扩展性
- en: 'We encountered the idea of vertical scalability in the previous chapter when
    we introduced the @batch and @resources decorators. This is by far the easiest
    form of scaling: vertical scalability refers to the idea of handling more compute
    and larger datasets just by using larger instances. Naturally, you can’t rely
    on vertical scaling on your laptop because you can’t add more CPU cores or memory
    to it programmatically. In contrast, it is easy to achieve vertical scaling with
    a cloud-based compute layer like AWS Batch, which can deliver any instances available
    in the cloud.'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章介绍 @batch 和 @resources 装饰器时，我们遇到了垂直可扩展性的概念。这是迄今为止最简单的一种扩展形式：垂直可扩展性指的是通过使用更大的实例来处理更多的计算和更大的数据集。自然地，您不能在笔记本电脑上依赖垂直扩展，因为您不能通过编程方式添加更多的
    CPU 核心或内存。相比之下，使用基于云的计算层（如 AWS Batch）很容易实现垂直扩展，它可以提供云中可用的任何实例。
- en: Let’s start by loading the Yelp reviews dataset and constructing a corresponding
    bag-of-words representation of it. We will use the same dataset in many flows,
    so we will develop utility functions to handle data to avoid duplicating the same
    code across flows. As usual in Python, we store the function in a separate file,
    a *module*, which the flows can import.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从加载 Yelp 评论数据集并构建其相应的词袋表示开始。我们将在许多流程中使用相同的数据集，因此我们将开发实用函数来处理数据，以避免在流程中重复相同的代码。在
    Python 中，我们通常将函数存储在单独的文件中，即模块，流程可以导入这些模块。
- en: 'Our module contains two utility functions: load_yelp_reviews, which downloads
    the dataset and extracts a list of documents from it, and make_matrix, which converts
    a list of documents to a bag-of-words matrix. The load_yelp_reviews function looks
    very similar to the code we used to inspect data in a notebook (listing 5.1).
    In fact, a benefit of creating a separate module is that we can use the same module
    in our Metaflow flow and in a notebook.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模块包含两个实用函数：load_yelp_reviews，它从数据集中下载并提取文档列表，以及make_matrix，它将文档列表转换为词袋矩阵。load_yelp_reviews
    函数看起来与我们用来在笔记本中检查数据的代码非常相似（列表 5.1）。实际上，创建单独模块的好处之一是我们可以在 Metaflow 流程和笔记本中使用相同的模块。
- en: Recommendation It is considered a good practice to place related functions in
    logically organized modules. You can share the modules across many flows and use
    them in notebooks as well. Furthermore, you can test modules independently.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐：将相关函数放置在逻辑组织良好的模块中是一种良好的实践。您可以在多个流程中共享这些模块，并在笔记本中使用它们。此外，您还可以独立测试模块。
- en: Create a file named scale_data.py, and store the code from the next listing
    in it.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个名为 scale_data.py 的文件，并将下一列表中的代码存储在其中。
- en: Listing 5.2 Functions to process the Yelp review dataset
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.2 处理 Yelp 评论数据集的函数
- en: '[PRE1]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Loads the dataset and extracts a list of documents from it
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从数据集中加载并提取文档列表
- en: ❷ Uses Metaflow’s built-in S3 client to load the publicly available Yelp dataset
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 Metaflow 内置的 S3 客户端加载公开可用的 Yelp 数据集
- en: ❸ Extracts a data file from the tar package
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从 tar 包中提取数据文件
- en: ❹ Returns the first num_docs lines from the file
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从文件中返回前 num_docs 行
- en: ❺ Converts a list of documents to a bag-of-words matrix
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将文档列表转换为词袋矩阵
- en: ❻ CountVectorizer creates the matrix.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ CountVectorizer 创建矩阵。
- en: ❼ Creates a list of column labels
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 创建列标签列表
- en: As in our notebook example, the function loads the publicly available Yelp dataset
    from S3 using Metaflow’s built-in S3 client, which we will cover in more detail
    in chapter 7\. The load_yelp_reviews function takes a single argument, num_docs,
    which indicates how many documents (reviews, one per line) to read from the dataset.
    We will use num_docs to control the dataset size.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们的笔记本示例所示，该函数使用Metaflow内置的S3客户端从S3加载公开可用的Yelp数据集，我们将在第7章中更详细地介绍。load_yelp_reviews函数接受一个参数num_docs，表示要从数据集中读取多少个文档（每行一个评论）。我们将使用num_docs来控制数据集大小。
- en: 'The make_matrix function takes a list of documents in docs and uses Scikit-Learn’s
    CountVectorizer to create a matrix from documents. We give it the following parameters:'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: make_matrix函数接受一个包含文档的列表docs，并使用Scikit-Learn的CountVectorizer从文档创建一个矩阵。我们给它以下参数：
- en: min_df specifies that included words need to occur in at least 10 documents.
    This gets rid of many typos and other spurious words.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: min_df指定包含的单词至少需要在10个文档中出现。这消除了许多错误和其它无关的单词。
- en: max_df specifies that all words that occur in more than 10% of all documents
    are excluded. They are typically common English words that are rather uninformative
    for our use case.
  id: totrans-69
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: max_df指定所有在所有文档中出现超过10%的单词将被排除。它们通常是常见的英语单词，对于我们的用例来说，它们并不提供太多信息。
- en: binary can be used to indicate that only the occurrence of a word in a document
    matters. The result is 0 or 1, regardless of how many times the word occurs in
    the document. Finally, we create a list of column labels so we know which word
    corresponds to which column.
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: binary可以用来表示文档中单词的出现即可。结果是0或1，不管单词在文档中出现的次数多少。最后，我们创建一个列标签列表，以便我们知道哪个单词对应哪个列。
- en: 'Composing your workflows from modular components like scale_data.py offers
    many benefits: you can test them independently, use them in a notebook during
    prototyping, and package and share them across multiple projects. Metaflow packages
    all modules in the current working directory together with the flow, so they are
    available automatically inside containers executed by the compute layer. We will
    discuss this and other topics related to management of software libraries in chapter
    6.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 从像scale_data.py这样的模块化组件中构建你的工作流程提供了许多好处：你可以独立测试它们，在原型设计期间在笔记本中使用它们，并将它们打包和共享到多个项目中。Metaflow将当前工作目录中的所有模块与流程一起打包，因此它们在由计算层执行的容器内自动可用。我们将在第6章讨论这个以及其他与软件库管理相关的话题。
- en: Recommendation Implement complex business logic, like modeling code, as separate
    modules that can be called by the workflow. This makes the logic easier to test
    in notebooks and by automated test suites. It also makes the modules shareable
    across multiple workflows.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 建议：将复杂的企业逻辑，如建模代码，作为可以由工作流程调用的单独模块实现。这使得逻辑更容易在笔记本和自动化测试套件中进行测试。这也使得模块可以在多个工作流程之间共享。
- en: 'Next, let’s compose a simple flow that we can use to test the function. We
    follow the spiral approach: we start with a simple flow that does almost nothing
    and keep adding functionality to it iteratively. In the same directory where you
    stored scale_data.py, create a new file, kmeans_flow_v1.py with the following
    code.'
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们构建一个简单的流程，我们可以用它来测试函数。我们遵循螺旋方法：我们从一个几乎什么也不做的简单流程开始，并迭代地添加功能。在存储scale_data.py的同一目录中，创建一个新文件，名为kmeans_flow_v1.py，包含以下代码。
- en: Listing 5.3 The first iteration of KMeansFlow
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.3 KMeansFlow的第一迭代
- en: '[PRE2]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Imports the module we created earlier and uses it to load the dataset
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入我们之前创建的模块，并使用它来加载数据集
- en: 'The start step imports the module we created in listing 5.2 and uses it to
    load the dataset. Use the parameter num_docs to control the dataset size. The
    default is to load only the first 1,000 documents. The uncompressed dataset is
    about 500 MB, so executing this example requires more than half a gigabyte of
    memory. The directory structure needs to look like the following to ensure that
    Metaflow packages all modules properly:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 开始步骤导入我们在列表5.2中创建的模块，并使用它来加载数据集。使用参数num_docs来控制数据集大小。默认情况下，只加载前1,000个文档。未压缩的数据集大约有500
    MB，因此执行此示例需要超过半GB的内存。目录结构需要如下所示，以确保Metaflow能够正确地打包所有模块：
- en: '[PRE3]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'You can choose the name for the directory my_dir freely. Make sure your working
    directory is my_dir and execute the flow as usual:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以自由地为目录命名my_dir。确保你的工作目录是my_dir，并像往常一样执行流程：
- en: '[PRE4]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Especially on a local laptop, this might take a few minutes to execute. If all
    goes well, it should complete without any errors. This is a good sanity check
    before we start adding more functionality to the flow.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 尤其是在本地笔记本电脑上，这可能需要几分钟才能执行。如果一切顺利，它应该会无错误地完成。在我们开始向流程添加更多功能之前，这是一个很好的合理性检查。
- en: Defining dependencies with @conda
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 @conda 定义依赖项
- en: Next, let’s expand KmeansFlow to perform clustering using the K-means algorithm.
    We can use an off-the-shelf implementation of K-means provided by Scikit-Learn.
    In chapter 3, we installed Scikit-Learn simply by running pip install on the local
    workstation. However, the locally installed library isn’t automatically available
    in all containers that are executed by the compute layer.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们扩展 KmeansFlow 以使用 K-means 算法进行聚类。我们可以使用 Scikit-Learn 提供的现成 K-means 实现。在第
    3 章中，我们通过在本地工作站上运行 pip install 简单地安装了 Scikit-Learn。然而，本地安装的库在计算层执行的所有容器中并不自动可用。
- en: We could choose a container image that has all the libraries we need preinstalled,
    but managing multiple images that match the needs of every project can get cumbersome.
    As an alternative, Metaflow provides a more flexible approach, the @conda decorator,
    which doesn’t require you to create or find suitable images manually.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以选择一个预装所有所需库的容器镜像，但管理多个与每个项目需求匹配的镜像可能会变得繁琐。作为替代方案，Metaflow 提供了一种更灵活的方法，即
    @conda 装饰器，它不需要你手动创建或查找合适的镜像。
- en: 'In general, dependency management is a nontrivial topic that we will cover
    in detail in the next chapter. For the purposes of this section, you will just
    need to make sure that you have the Conda package manager installed—see the appendix
    for instructions. After this, all you need to do is to include the @conda_base
    line in your code as shown in listing 5.4:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，依赖项管理是一个非平凡的话题，我们将在下一章中详细讨论。为了本节的目的，你只需要确保你已经安装了 Conda 包管理器——请参阅附录中的说明。之后，你只需要在代码中包含如列表
    5.4 所示的 @conda_base 行：
- en: '[PRE5]'
  id: totrans-86
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: This line instructs Metaflow to install the Python version 3.8.3 with Scikit-Learn
    version 0.24.1 in all compute layers where the code is executed, including local
    runs.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 这行代码指示 Metaflow 在所有执行代码的计算层中安装 Python 3.8.3 和 Scikit-Learn 0.24.1 版本，包括本地运行。
- en: Note When you run a flow with @conda for the first time, Metaflow resolves all
    dependencies that are needed and uploads them to S3\. This might take a few minutes,
    particularly if you are executing the code on your laptop. Be patient—this happens
    only during the first execution!
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当你第一次使用 @conda 运行流程时，Metaflow 会解析所有所需的依赖项并将它们上传到 S3。这可能需要几分钟时间，尤其是如果你在笔记本电脑上执行代码时。请耐心等待——这仅在第一次执行时发生！
- en: The next code snippet expands the first version of KMeansFlow from listing 5.3\.
    It uses our scale_data module to create a bag-of-words matrix from the Yelp dataset.
    The matrix is clustered in a new step, train_kmeans.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段扩展了列表 5.3 中的 KMeansFlow 的第一个版本。它使用我们的 scale_data 模块从 Yelp 数据集中创建一个词袋矩阵。矩阵在新的步骤
    train_kmeans 中进行聚类。
- en: Listing 5.4 The final version of KMeansFlow
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.4 KMeansFlow 的最终版本
- en: '[PRE6]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Declares the libraries we will need in tasks
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在任务中声明我们将需要的库
- en: ❷ Requires 4 GB of memory to preprocess data
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 预处理数据需要 4 GB 的内存
- en: ❸ Imports the module we created earlier and uses it to load the dataset
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 导入我们之前创建的模块并使用它来加载数据集
- en: ❹ Requires 4 GB of memory and 16 CPU cores to run K-means
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 运行 K-means 需要 4 GB 的内存和 16 个 CPU 核心
- en: ❺ Uses a profile to measure and prints the time it takes to run K-means
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用配置文件来测量并打印运行 K-means 所需的时间
- en: ❻ Runs K-Means and stores the results in clusters
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 运行 K-Means 并将结果存储在聚类中
- en: Save the code to kmeans_flow_v2.py. You can see that we used the @resources
    decorator to declare that the start and train_kmeans steps require 4 GB of memory.
    The extra gigabytes are needed to keep the bag-of-words matrix in memory and account
    for all the related overhead. The train_kmeans step uses a small utility function
    provided by Metaflow, profile, which measures and prints the time needed to execute
    a block of code. Its single argument is a prefix included in the output, so you
    know which block of code the measurement relates to.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 将代码保存到 kmeans_flow_v2.py。你可以看到我们使用了 @resources 装饰器来声明 start 和 train_kmeans 步骤需要
    4 GB 的内存。额外的 GB 是为了保持词袋矩阵在内存中，并考虑到所有相关的开销。train_kmeans 步骤使用 Metaflow 提供的一个小型实用函数
    profile，它测量并打印执行代码块所需的时间。它的单个参数是包含在输出中的前缀，这样你知道测量与哪个代码块相关。
- en: Tip Use the profile context manager in Metaflow to quickly figure out how long
    certain operations inside a step take.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：在Metaflow中使用profile上下文管理器可以快速了解步骤内某些操作的耗时。
- en: The KMeans object is used to run the algorithm. It takes the number of clusters
    (the K in K-means), here called n_clusters, as an argument, which we set to a
    somewhat arbitrary number, 10\. In the next section, we will explore other values
    for K. The next argument, verbose, provides some information about the progress
    of the algorithm, and n_init=1 implies that we need to run the algorithm only
    once. A higher number might lead to better results. The result of the algorithm
    is an array, stored in self.clusters, which assigns each document to a cluster.
    We will take a deeper look at the results in the next section.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 使用KMeans对象来运行算法。它接受簇的数量（K-means中的K），这里称为n_clusters，作为参数，我们将其设置为某个相当随意的数字，10。在下一节中，我们将探索K的其他值。下一个参数verbose提供了关于算法进度的某些信息，而n_init=1意味着我们只需要运行算法一次。更高的数字可能会导致更好的结果。算法的结果是一个数组，存储在self.clusters中，它将每个文档分配到一个簇中。我们将在下一节中更深入地查看结果。
- en: 'Run the flow with AWS Batch as follows:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式使用AWS Batch运行流程：
- en: '[PRE7]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: The --environment=conda flag is required to indicate that we want to handle
    dependencies using Conda. If you want to run the code locally or your compute
    environment doesn’t have instances with 4 GB of memory and 16 CPU cores available,
    you can process fewer documents by specifying a smaller value for the num-docs
    parameter, such as run --num-docs 10000. Hopefully, the run completes successfully—that’s
    all we have to accomplish for now. We will dig deeper in the results in the next
    section.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`--environment=conda`标志是必须的，以表明我们希望使用Conda来处理依赖关系。如果您想在本地运行代码或您的计算环境没有可用4
    GB内存和16个CPU核心的实例，您可以通过指定`num-docs`参数的较小值来处理更少的文档，例如运行`--num-docs 10000`。希望这次运行能够成功完成——这是我们目前要完成的所有事情。我们将在下一节中深入探讨结果。
- en: 'Here’s what’s interesting about this flow: the dataset contains 650,000 documents,
    and the corresponding matrix contains about 48,000 unique words. Hence, the matrix
    size is 650,000 * 48,000, which would take more than 100 GB of memory if it was
    stored as a dense matrix. Because it is stored as a sparse matrix, it will fit
    in less than 4 GB. Running K-means for a matrix of this size is computationally
    expensive. Luckily, the implementation in Scikit-Learn can parallelize the algorithm
    automatically over multiple CPU cores that we requested with @resources(cpu=16).
    As a result, you can make K-means complete faster just by increasing the number
    of CPU cores, as illustrated in figure 5.5.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 关于这个流程有趣的地方是：数据集包含650,000个文档，相应的矩阵包含大约48,000个独特的单词。因此，矩阵的大小是650,000 * 48,000，如果以密集矩阵的形式存储，将需要超过100
    GB的内存。因为它以稀疏矩阵的形式存储，所以它将小于4 GB。对于这样大小的矩阵运行K-means是计算密集型的。幸运的是，Scikit-Learn中的实现可以自动在请求的多个CPU核心上并行化算法，即使用`@resources(cpu=16)`。因此，您只需增加CPU核心的数量，就可以使K-means更快地完成，如图5.5所示。
- en: '![CH05_F05_Tuulos](../../OEBPS/Images/CH05_F05_Tuulos.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F05_Tuulos](../../OEBPS/Images/CH05_F05_Tuulos.png)'
- en: Figure 5.5 Execution time vs. the number of CPU cores used
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.5 执行时间与使用的CPU核心数的关系
- en: 'For this dataset, you can get up to 40% performance improvement by using multiple
    CPU cores. As an exercise, you can try another, faster version of K-means called
    MiniBatchKMeans. The sweet spot for @resources depends on the dataset size, the
    algorithm, and its implementation. In this case, going beyond four cores doesn’t
    seem to offer much of a benefit. Beyond this number, the amount of work executed
    by an additional core isn’t enough to justify the communication and coordination
    overhead. In general, this behavior is typical for most modern ML algorithm implementations:
    the more CPU cores or GPUs you run them on, the faster they will run, up to a
    limit.'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个数据集，您可以通过使用多个CPU核心获得高达40%的性能提升。作为一个练习，您可以尝试另一种名为MiniBatchKMeans的更快版本的K-means。@resources的最佳选择取决于数据集大小、算法及其实现。在这种情况下，超过四个核心似乎没有带来太多好处。超过这个数字，额外核心执行的工作量不足以证明通信和协调开销的合理性。一般来说，这种行为是大多数现代机器学习算法实现中典型的：您在更多的CPU核心或GPU上运行它们，它们的运行速度就会更快，直到达到一个极限。
- en: Note In contrast to the memory limit, the CPU limit is a *soft limit* on AWS
    Batch. The task can use all available CPU cores on the instance if there are no
    other tasks executing on it simultaneously. The limit applies when multiple tasks
    need to share the instance. In most cases, this behavior is desirable, but it
    makes benchmarking tricky, because a task with, for example, four CPUs, may end
    up using more CPU cores opportunistically.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：与内存限制不同，CPU限制是AWS Batch上的一个**软限制**。如果没有其他任务同时在该实例上执行，任务可以使用实例上所有可用的CPU核心。当多个任务需要共享实例时，此限制适用。在大多数情况下，这种行为是可取的，但它使得基准测试变得复杂，因为具有例如四个CPU的任务可能会偶然使用更多的CPU核心。
- en: The most appealing feature of vertical scaling is its effortlessness—the data
    scientist just needs to modify one line of code, @resources, to handle larger
    datasets faster, as shown earlier. Although this approach doesn’t provide infinite
    scalability, oftentimes, it is just enough for the task at hand, as discussed
    in the next subsection.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 垂直扩展最吸引人的特点是它的**无难度**——数据科学家只需修改一行代码，即@resources，就可以更快地处理更大的数据集，如前所述。尽管这种方法不提供无限的扩展性，但通常，它对于手头的任务来说已经足够了，正如下一小节所讨论的。
- en: 5.1.3 Why vertical scalability?
  id: totrans-110
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 为什么需要垂直扩展？
- en: 'From an engineering point of view, it is easy to have a knee-jerk reaction
    to vertical scalability: it is not “real” scalability—it hits a hard ceiling at
    the largest available instance size. As of writing of this book, the largest commonly
    available EC2 instances on AWS provide 48 CPU cores and 768 GB of memory. Clearly,
    if you need more cores or more memory, vertical scalability won’t help you.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 从工程角度来看，对垂直扩展有一个本能的反应：它不是“真正的”扩展性——它在最大可用实例大小上达到一个硬上限。截至本书编写时，AWS上最常用的最大EC2实例提供48个CPU核心和768
    GB的内存。显然，如果你需要更多的核心或更多的内存，垂直扩展无法帮助你。
- en: Instead of relying on potentially misleading intuitions, it is a good idea to
    carefully evaluate whether vertical scalability could be sufficient and for how
    long. Thanks to the simplicity of vertical scalability, you can build the first
    version of an application quickly and verify its correctness and value before
    complicating the implementation. Surprisingly often, vertical scalability is just
    good enough.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是依赖可能具有误导性的直觉，仔细评估垂直扩展是否足够以及可以持续多长时间是一个好主意。得益于垂直扩展的简单性，你可以快速构建应用程序的第一个版本，并在使实现复杂化之前验证其正确性和价值。令人惊讶的是，垂直扩展通常已经足够好。
- en: Simplicity boosts performance and productivity
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 简单性提升性能和生产力
- en: It is hard to beat vertical scalability when it comes to simplicity. You can
    write the most idiomatic, readable, and straightforward Python code and just adjust
    the numbers in @resources to match the requirements of the code. It has no new
    paradigms to learn. If and when things fail, no hidden layers of abstraction cause
    surprises or convoluted error messages. This is hugely important for the productivity
    and autonomy of data scientists—they can focus on modeling without having to worry
    about scalability or performance too much.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 在简单性方面，很难超越垂直扩展。你可以编写最地道、可读性和直接的Python代码，只需调整@resources中的数字以匹配代码的要求。它没有新的范式需要学习。如果事情失败了，没有隐藏的抽象层会导致惊喜或复杂的错误信息。这对于数据科学家的生产力和自主性非常重要——他们可以专注于建模，而无需过多担心可扩展性或性能。
- en: When it comes to implementing machine learning and data science workloads in
    Python, the simplicity of the user-facing APIs is deceiving. Under the hood, the
    best libraries are highly optimized using low-level languages like C++ so they
    can leverage the available resources on a single instance very efficiently.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到在Python中实现机器学习和数据科学工作负载时，用户界面API的简单性具有欺骗性。在底层，最好的库使用C++等低级语言进行了高度优化，以便能够非常有效地利用单个实例上的可用资源。
- en: Consider the example of modeling frameworks. In the early 2010s, much effort
    was put into providing scalable implementation of traditional ML algorithms on
    top of scalable frameworks like Hadoop and Spark. These frameworks are good examples
    of inherently scalable systems that have a limited single-computer performance.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 以建模框架为例。在2010年代初，人们投入了大量精力在Hadoop和Spark等可扩展框架之上提供传统机器学习算法的可扩展实现。这些框架是具有有限单机性能的固有可扩展系统的良好例子。
- en: In the latter half of the 2010s, it started becoming obvious that single-computer,
    vertically-scaled training algorithms, like the ones provided by XGBoost or Tensorflow,
    could easily outperform their nominally more scalable counterparts, particularly
    when accelerated with GPUs. Not only were these implementations much faster, they
    were also considerably easier to operate and develop with because they lacked
    the inherent complexity of scalable, distributed systems.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在2010年代后半期，开始变得明显的是，单机、垂直扩展的训练算法，如XGBoost或Tensorflow提供的算法，可以轻易地超越它们名义上更可扩展的对应者，尤其是在GPU加速的情况下。这些实现不仅速度更快，而且由于缺乏可扩展、分布式系统的固有复杂性，它们在操作和开发上也容易得多。
- en: 'This highlights the difference of scalability and performance: a system consisting
    of multiple low-performance but parallel units can scale well, but such a distributed
    system tends to incur inherent communication and coordination overhead, which
    can make it *slower* than its nondistributed counterparts for small- and medium-sized
    workloads. Notably, sometimes unscalable approaches can outperform their scalable
    counterparts, even with unexpectedly large workloads.'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 这突出了可扩展性和性能之间的差异：由多个低性能但并行单元组成的系统可以很好地扩展，但这样的分布式系统往往会产生固有的通信和协调开销，这可能会使其对于中小型工作负载比其非分布式对应者*更慢*。值得注意的是，有时不可扩展的方法甚至可以优于其可扩展的对应者，即使是在意外的大工作量下。
- en: Scalability tip Don’t overestimate the performance of nominally scalable systems,
    and don’t underestimate the performance of a single instance solution. When in
    doubt, benchmark. Even if a distributed solution might scale better, carefully
    consider its operational cost and the cost to the data scientist’s productivity.
    For concrete examples of this effect, see a paper titled “Scalability! But at
    What COST?” published in 2015 by Frank McSherry and others ([http://mng.bz/OogO](http://mng.bz/OogO)).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性技巧 不要高估名义上可扩展系统的性能，也不要低估单实例解决方案的性能。如有疑问，请进行基准测试。即使分布式解决方案可能具有更好的可扩展性，也要仔细考虑其运营成本和对数据科学家生产力的成本。关于这一效果的具体例子，请参阅2015年由Frank
    McSherry等人发表的论文《可扩展性！但代价是什么？》([http://mng.bz/OogO](http://mng.bz/OogO))。
- en: Consider the nature of your problem
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑你问题的本质
- en: When thinking about the nature of your problem, consider the following two factors,
    which are often underappreciated. First, real-life scalability always has an upper
    bound. In the physical world, nothing is truly infinite. In the examples presented
    in this chapter, we use a static dataset of 650,000 Yelp reviews. With a static
    dataset like this, it is easy to see that scalability isn’t likely to be a concern.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 当思考你问题的本质时，考虑以下两个经常被低估的因素。首先，现实生活中的可扩展性总是有一个上限。在物理世界中，没有什么真正是无限的。在本章中提供的例子中，我们使用了一个包含650,000条Yelp评论的静态数据集。对于这样的静态数据集，很容易看出可扩展性不太可能成为一个问题。
- en: Even if the dataset wasn’t static, its size may be naturally capped. For instance,
    consider a dataset providing statistics for each US ZIP code. There are about
    40,000 ZIP codes in the US, and the number is pretty much guaranteed to stay stable.
    If you can get your workflow to handle about 40,000 ZIP codes without trouble,
    you shouldn’t have any scalability concerns.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 即使数据集不是静态的，其大小可能自然有限。例如，考虑一个为每个美国ZIP代码提供统计数据的数据集。美国大约有40,000个ZIP代码，这个数字几乎可以保证保持稳定。如果你能轻松处理大约40,000个ZIP代码，那么你就不应该有任何可扩展性的担忧。
- en: Second, a single modern computer can have a surprising amount of resources.
    Anything in the order of billions might feel like a lot to a human being, but
    keep in mind that a single modern computer can do about two billion arithmetic
    operations *per second* and can keep about 27 billion English words *in memory
    at the same time*. Computers just operate at a different order of magnitude than
    our limited cognition.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 第二，一台现代计算机可以拥有惊人的资源量。对于人类来说，任何数量级达到数十亿的都可能感觉很多，但请记住，一台现代计算机每秒可以进行大约20亿次的算术运算，并且可以同时存储大约270亿个英语单词。计算机的运行速度比我们有限的认知要高得多。
- en: The effect is even more pronounced with specialized hardware like GPUs that
    are widely used to train deep neural networks and other modern model architectures.
    As of the writing of this book, AWS provides instances with eight high-performance
    GPUs. You can use a machine like this to train huge models. Because the communication
    overhead between GPU and CPU cores is minimal within a machine, a single instance
    can outperform a cluster of instances that is supposed to have more computing
    power, looking at the raw numbers.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 使用广泛用于训练深度神经网络和其他现代模型架构的专用硬件（如GPU）时，这种影响更为明显。截至本书编写时，AWS提供了具有八个高性能GPU的实例。你可以使用这样的机器来训练大型模型。由于机器内部GPU和CPU核心之间的通信开销最小，单个实例可以超越应该具有更多计算能力的实例集群，从原始数据来看。
- en: Last, computers haven’t stopped growing. Although the single-core performance
    hasn’t grown much over the past 10 years, the number of cores in a single instance
    has, as well as the amount of memory and the speed of local storage. As an example,
    consider storing data about every Netflix subscriber in an in-memory data frame
    on a single instance. It might feel like a bad idea because the number of Netflix
    subscribers quickly grows every year!
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，计算机的增长并未停止。尽管在过去10年中单核性能增长不大，但单个实例中的核心数量、内存容量以及本地存储速度都在增长。以一个例子来说明，考虑将每个Netflix订阅者的数据存储在单个实例的内存数据帧中。这可能会感觉像是一个糟糕的想法，因为Netflix订阅者的数量每年都在迅速增长！
- en: Instead of relying on a gut reaction, it is a good idea to check the math. In
    2011, Netflix had 26 million subscribers, and the largest AWS instance provided
    60 GB of memory, meaning you could have stored about 2.3 KB of data per subscriber.
    In 2021, Netflix had 207 million subscribers, and the largest AWS instance provides
    768 GB of memory, so, surprisingly, today you have more headroom for per-subscriber
    data—3.7 KB. In other words, if your use case fits within these bounds, you could
    have handled scalability simply by updating the number in @resources for over
    10 years and counting!
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 与依赖直觉反应相比，检查数学计算是一个好主意。2011年，Netflix有2600万订阅者，最大的AWS实例提供了60 GB的内存，这意味着每个订阅者可以存储大约2.3
    KB的数据。到2021年，Netflix有2.07亿订阅者，最大的AWS实例提供了768 GB的内存，因此，出人意料的是，今天每个订阅者的数据容量更大——3.7
    KB。换句话说，如果你的用例适合这些限制，你只需通过更新@资源中的数字，就可以简单地处理可扩展性超过10年，并且还在继续！
- en: Scalability tip Estimate the upper bound of your growth before worrying about
    scalability. If the upper bound is low enough compared to the available compute
    resources over time, you might not need to worry about scalability at all.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 可扩展性技巧 在担心可扩展性之前，先估计你的增长上限。如果与可用计算资源相比，上限足够低，你可能根本不需要担心可扩展性。
- en: 5.2 Practicing horizontal scalability
  id: totrans-128
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 实践水平可扩展性
- en: As suggested by the previous section, it is always advisable to start a project
    by thinking through the problem as well as the scale and the growth rate of the
    data and compute involved. If nothing suggests otherwise, vertical scalability
    is a good starting point. What would suggest otherwise? In other words, in what
    scenarios should you consider using multiple parallel instances instead of a single
    one?
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，始终建议在开始项目时，不仅要思考问题本身，还要考虑涉及的数据和计算规模以及增长率。如果没有其他迹象表明应该这样做，垂直可扩展性是一个好的起点。什么迹象会表明应该这样做？换句话说，在什么情况下，你应该考虑使用多个并行实例而不是单个实例？
- en: 5.2.1 Why horizontal scalability?
  id: totrans-130
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 为什么需要水平可扩展性？
- en: 'As a rule of thumb, you should consider horizontal scalability, that is, using
    multiple instances, if you answer “yes” to any of the following three questions:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 作为一项经验法则，如果你对以下三个问题中的任何一个回答“是”，你应该考虑水平可扩展性，即使用多个实例：
- en: Are there significant chunks of compute in your workflow that are *embarrassingly
    parallel*, meaning they can perform an operation without sharing any data besides
    inputs?
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的工作流程中是否有大量的计算部分是*令人尴尬地并行*的，这意味着它们可以在不共享任何数据（除了输入）的情况下执行操作？
- en: Is the dataset size too large to be conveniently handled on the largest available
    instance type?
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据集的大小是否太大，以至于无法方便地在最大的实例类型上处理？
- en: Are there compute-intensive algorithms, such as model training, that are too
    demanding to be executed on a single instance?
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否存在计算密集型的算法，如模型训练，它们对单个实例来说要求过高？
- en: 'The items are listed in the order of descending frequency: the first two scenarios
    are way more typical than the last one. Let’s unpack the scenarios one by one.'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这些项目按频率降序排列：前两种情况比最后一种情况典型得多。让我们逐一分析这些情况。
- en: Embarrassingly parallel tasks
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 令人尴尬的并行任务
- en: In distributed computing, a problem is called *embarrassingly parallel* if “little
    or no effort is needed to separate the problem into a number of parallel tasks.”
    In the context of Metaflow-style workflows, the definition coincides with dynamic
    foreach tasks that don’t need to share any data with each other, besides sharing
    common input data.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在分布式计算中，如果一个问题是“几乎不需要努力就可以将问题分解成多个并行任务”，那么这个问题被称为*令人尴尬的并行*。在Metaflow风格的工作流程的上下文中，这个定义与不需要相互共享任何数据（除了共享公共输入数据）的动态foreach任务相一致。
- en: 'Cases like this are common in data science applications: training a number
    of independent models, fetching multiple datasets, benchmarking a number of algorithms
    against a dataset, to name a few. You might be able to handle all these cases
    using vertical scalability, possibly utilizing multiple cores on a single instance,
    but typically there’s little gain in doing so.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况在数据科学应用中很常见：训练多个独立的模型，获取多个数据集，将多个算法与数据集进行基准测试，仅举几例。你可能能够通过垂直扩展来处理所有这些情况，可能利用单个实例上的多个核心，但通常这样做的好处很小。
- en: Remember that the key motivations for vertical scalability are simplicity and
    performance. The implementation of a typical embarrassingly parallel task looks
    practically the same, regardless of whether it is executed on a single instance
    or multiple—it is just a function or a Metaflow step. Because tasks are fully
    independent, there’s no performance penalty in executing them on separate instances.
    Hence, in cases like these, you can unlock massive scalability by using foreach
    in Metaflow without sacrificing simplicity or performance. We will practice one
    common case of embarrassing parallelism in the next section, hyperparameter search,
    which involves building many independent models.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，垂直扩展的关键动机是简单性和性能。典型地，一个令人尴尬的并行任务的实现看起来在单实例或多实例上执行时几乎相同——它只是一个函数或Metaflow步骤。因为任务是完全独立的，所以在单独的实例上执行它们不会产生性能惩罚。因此，在这种情况下，你可以通过在Metaflow中使用foreach来解锁巨大的可扩展性，而不会牺牲简单性或性能。我们将在下一节练习一个常见的令人尴尬的并行案例，即超参数搜索，这涉及到构建许多独立的模型。
- en: Large datasets
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 大数据集
- en: As of the writing of this book, the largest commonly available EC2 instance
    has 768 GB of memory. This means that just by relying on vertical scalability,
    you can load maybe 100 GB of data in a pandas DataFrame that has quite a bit of
    memory overhead. If you need to handle more data, which is not uncommon these
    days, it is not feasible to solely rely on vertical scalability.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 就本书的撰写而言，目前可用的最大常用EC2实例具有768 GB的内存。这意味着仅仅依靠垂直扩展，你可以在具有相当大内存开销的pandas DataFrame中加载大约100
    GB的数据。如果你需要处理更多的数据，这在当今并不罕见，仅仅依靠垂直扩展是不可行的。
- en: In many cases, the easiest approach to handling large datasets is to use a compute
    layer, like Apache Spark, that is specifically optimized for such use cases. Another
    approach is to divide data to smaller individual chunks or shards so that each
    shard can be handled on a large instance. We will discuss these approaches in
    chapter 7.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 在许多情况下，处理大数据集的最简单方法之一是使用专门针对此类用例优化的计算层，例如Apache Spark。另一种方法是将数据分成更小的单个块或碎片，以便每个碎片可以在大型实例上处理。我们将在第7章讨论这些方法。
- en: Distributed algorithms
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 分布式算法
- en: Besides input data, it is possible that the algorithm to be executed, say, model
    training, is too demanding for a single instance. For instance, this could be
    the case with a massive-scale computer vision model. Although it is common to
    have a dataset that is larger than a single instance, having a model of this scale
    is much less so. A single P4-type AWS instance with eight GPUs and over a terabyte
    of memory can fit a huge model.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 除了输入数据外，可能要执行的计算算法，例如模型训练，对单个实例来说要求过高。例如，这可能适用于大规模计算机视觉模型。尽管拥有比单个实例更大的数据集很常见，但拥有这种规模的模型则很少见。一个配备八个GPU和超过一TB内存的单个P4型AWS实例可以容纳一个巨大的模型。
- en: Operating distributed model training and other distributed algorithms can be
    nontrivial. Networking must be optimized for low-latency communication, instances
    must be placed accordingly to minimize latency, and a pool of tightly coupled
    instances must be carefully coordinated and synchronized, all while keeping in
    mind that instances may fail at any time.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 运行分布式模型训练和其他分布式算法可能并不简单。网络必须优化以实现低延迟通信，实例必须相应地放置以最小化延迟，并且必须仔细协调和同步紧密耦合的实例池，同时考虑到实例可能随时会失败。
- en: Fortunately, many modern machine learning libraries like PyTorch Lightning provide
    abstractions that make distributed training a bit easier. In addition, specialized
    compute layers like Amazon Sagemaker and Google’s Cloud TPUs can manage the complexity
    for you.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，许多现代机器学习库，如 PyTorch Lightning，提供了抽象，使得分布式训练变得容易一些。此外，专门的计算层，如 Amazon Sagemaker
    和 Google 的 Cloud TPUs，可以为你管理复杂性。
- en: 'Many real-world data science applications employ a mixture of many of these
    approaches: data is preprocessed using Spark and loaded as shards in parallel
    to produce a matrix or a tensor that is trained on a single vertically scaled
    instance. The beauty of organizing applications as workflows is that you can choose
    the most appropriate way to scale each step of the workflow, instead of trying
    to fit all tasks into a single paradigm. The next section demonstrates a straightforward
    but common case of embarrassing parallelism: building multiple K-means models
    in parallel to find the best-performing model.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 许多现实世界的数据科学应用采用了这些方法中的许多：使用 Spark 预处理数据，并并行加载为分片，以生成一个在单个垂直扩展实例上训练的矩阵或张量。将应用程序组织为工作流的好处在于，你可以选择最适合工作流每一步的扩展方式，而不是试图将所有任务都适应到一个单一的模式中。下一节将演示一个简单但常见的尴尬并行案例：并行构建多个
    K-means 模型以找到性能最佳的模型。
- en: '5.2.2 Example: Hyperparameter search'
  id: totrans-148
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 示例：超参数搜索
- en: 'In this section, we expand our earlier K-means example by showing how to address
    a common task in machine learning projects: *hyperparameter search and optimization*,
    using horizontal scalability. In the previous section, we simply fixed the number
    of clusters, the *K* parameter in K-means, to 10 clusters without any justification.
    The number of clusters is a *hyperparameter* for the K-means algorithm—a parameter
    that we need to define before the algorithm runs.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们通过展示如何使用水平扩展来解决机器学习项目中一个常见任务：*超参数搜索和优化*，来扩展我们之前提到的 K-means 示例。在前一节中，我们简单地固定了簇的数量，即
    K-means 中的 *K* 参数，为 10 个簇，而没有给出任何理由。簇的数量是 K-means 算法的一个 *超参数*——一个在算法运行之前我们需要定义的参数。
- en: Typically, there isn’t an unambiguously right choice for the hyperparameter
    values. Because we can’t define the right values in advance, it is often desirable
    to run the algorithm with a number of different hyperparameter values and choose
    the one that performs the best. Sophisticated hyperparameter optimizers can generate
    new values on the fly and stop when results don’t seem to improve. A simpler approach,
    demonstrated here, is just to define a list of hyperparameters to try in advance
    and evaluate the results in the end. A benefit of this simple approach is that
    the algorithm can be evaluated for each parametrization independently—a perfect
    use case for horizontal scalability.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，对于超参数值并没有一个明确正确的选择。因为我们无法事先定义正确的值，所以通常希望运行算法使用多个不同的超参数值，并选择表现最好的一个。复杂的超参数优化器可以动态生成新值，并在结果不再改善时停止。这里展示的简单方法只是预先定义一个要尝试的超参数列表，并在最后评估结果。这种简单方法的优点是算法可以独立地对每个参数化进行评估——这是一个非常适合水平扩展的理想用例。
- en: How to define “the best result” for a clustering algorithm is a nontrivial question
    in itself. You can group documents in many equally good ways. In this example,
    the choice is yours. You can characterize each cluster by listing the most frequent
    words in the cluster. As we will show next, you can look at the clusters and decide
    which hyperparameter values produce the most entertaining results. Listing 5.5
    shows a function, top_words, which computes the most frequent words for each cluster.
    Save it to a separate module, analyze_kmeans.py.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 如何定义聚类算法的“最佳结果”本身就是一个非平凡的问题。你可以以许多同样好的方式对文档进行分组。在这个例子中，选择权在你。你可以通过列出簇中最频繁出现的单词来表征每个簇。正如我们将展示的，你可以查看簇并决定哪些超参数值会产生最有趣的结果。列表
    5.5 展示了一个函数，top_words，它计算每个簇中最频繁的单词。将其保存到一个单独的模块中，analyze_kmeans.py。
- en: Listing 5.5 Computing the top words for each cluster
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.5 计算每个簇的顶级单词
- en: '[PRE8]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Processes each of the K clusters
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 处理 K 个簇中的每一个
- en: ❷ Chooses rows from the bag-of-words matrix that correspond to the current cluster
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从词袋矩阵中选择与当前簇对应的行
- en: ❸ Computes columnwise sums, that is, the frequencies of each word for the cluster
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算列的总和，即每个单词在簇中的频率
- en: ❹ Sorts the columns by their frequencies and produces a list of top-20 words
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 按频率对列进行排序，并生成一个包含顶级 20 个单词的列表
- en: 'This function uses a number of tricks from the NumPy package: it chooses a
    subset of rows from a matrix using where, it produces columnwise sums with sum(axis=0),
    and produces column indices in the sorted order using argsort. If you are curious,
    you can take a look at the NumPy documentation to learn more about these functions.
    Knowing NumPy is not critical for this example.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数使用了来自NumPy包的一些技巧：它使用where从矩阵中选择行的一个子集，使用sum(axis=0)生成列的总和，并使用argsort生成排序后的列索引。如果您好奇，可以查看NumPy文档以了解更多关于这些函数的信息。了解NumPy对于此示例不是必需的。
- en: We will use Metaflow’s foreach construct, introduced in section 3.2.3, to run
    a number of K-means algorithms in parallel, leveraging AWS Batch as an autoscaling
    compute layer. The corresponding flow is visualized in figure 5.6.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Metaflow的foreach构造，如第3.2.3节中引入的，以并行运行多个K-means算法，利用AWS Batch作为自动扩展的计算层。相应的流程在图5.6中可视化。
- en: '![CH05_F06_Tuulos](../../OEBPS/Images/CH05_F06_Tuulos.png)'
  id: totrans-160
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F06_Tuulos](../../OEBPS/Images/CH05_F06_Tuulos.png)'
- en: Figure 5.6 The DAG corresponding to ManyKmeansFlow
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.6 ManyKmeansFlow对应的DAG
- en: This demonstrates horizontal scalability in action. The flow trains a number
    of K-means clusterings concurrently, for various values of the parameter K (the
    number of clusters), and produces a summary of the results for analysis. The following
    listing shows the code for the flow, ManyKmeansFlow, which is based on our earlier
    KMeansFlow.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这展示了横向扩展的实际应用。该流程并行训练了多个K-means聚类，针对参数K（聚类数量）的多个值，并生成结果摘要以供分析。以下列表显示了流程的代码，ManyKmeansFlow，它基于我们之前提到的KMeansFlow。
- en: Listing 5.6 Searching hyperparameters for K-means
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.6 搜索K-means的超参数
- en: '[PRE9]'
  id: totrans-164
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Produces a bag-of-words matrix using the scale_data module
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用scale_data模块生成词袋矩阵
- en: ❷ Defines a list of hyperparameters for foreach
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为foreach定义一组超参数
- en: ❸ Trains a K-means clustering inside foreach
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在foreach中训练K-means聚类
- en: ❹ Produces a list of top words for each cluster
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 为每个聚类生成顶级词汇列表
- en: ❺ Groups all top lists by the hyperparameter value
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 按超参数值分组所有顶级列表
- en: 'The highlight of this flow is the list of hyperparameters defined in k_params.
    We produce 10 separate clusterings, that is, 10 separate train_kmeans tasks, varying
    the number of clusters between 5 and 50\. Depending on the configuration of your
    compute environment, all the 10 tasks may run in parallel. The system scales nearly
    perfectly: If producing one clustering takes about two minutes, you can produce
    10 clusterings in parallel in about two minutes as well! You can run the flow
    with the following code:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 此流程的亮点是k_params中定义的超参数列表。我们生成了10个独立的聚类，即10个独立的train_kmeans任务，聚类数量在5到50之间变化。根据您的计算环境配置，所有10个任务可能并行运行。系统儿乎完美地扩展：如果生成一个聚类需要大约两分钟，那么您也可以在两分钟内并行生成10个聚类！您可以使用以下代码运行此流程：
- en: '[PRE10]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: By default, Metaflow will run at most 16 tasks in parallel, and, hence, it will
    submit at most 16 (in this case, 10) tasks to AWS Batch simultaneously. Depending
    on your compute environment, AWS Batch may decide to run multiple containers concurrently
    on a single instance, or it may decide to launch more instances to handle the
    queued tasks. You can control the level of parallelism with the --max-workers
    option, as described in section 3.2.4.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，Metaflow最多并行运行16个任务，因此它将最多提交16个（在这种情况下，10个）任务到AWS Batch。根据您的计算环境，AWS Batch可能会决定在单个实例上并行运行多个容器，或者它可能会决定启动更多实例来处理队列中的任务。您可以使用--max-workers选项控制并行级别，如第3.2.4节所述。
- en: This example illustrates the benefits of a cloud-based compute layer, like AWS
    Batch. Not only can you launch larger instances, as we covered in the previous
    section, but you can *launch any number of them in parallel*. Without a scalable
    compute layer, you would have to execute the K-means algorithms sequentially,
    which would take about 20 minutes, in contrast to getting results in two minutes,
    thanks to horizontal scalability. As instructed in the previous chapter, we use
    --with retry to handle any transient failures, which are bound to happen occasionally
    when running hundreds of Batch jobs in parallel.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 此示例说明了基于云的计算层（如AWS Batch）的好处。您不仅可以启动更大的实例，正如我们在上一节中提到的，还可以*并行启动任意数量的实例*。如果没有可扩展的计算层，您将不得不按顺序执行K-means算法，这将花费大约20分钟，而由于横向扩展，您可以在两分钟内获得结果。根据上一章的指示，我们使用--with
    retry来处理任何可能偶尔发生的暂时性故障，当并行运行数百个Batch作业时，这些故障是不可避免的。
- en: Currently, Metaflow can handle thousands of concurrent tasks without issues.
    Considering that each task can request high @resources, you can employ tens of
    thousands of CPU cores and nearly a petabyte of RAM in a single workflow!
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，Metaflow可以无问题地处理数千个并发任务。考虑到每个任务可能需要大量的@资源，你可以在单个工作流中部署数万个CPU核心和近一个PB的RAM！
- en: Inspecting results
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 检查结果
- en: 'When the run completes, you will have 10 different clusterings available for
    inspection. You can inspect the results using the Metaflow Client API on an interactive
    Python shell or using a notebook, as described in section 3.3.2\. Here’s how to
    do it easily. First, access the results of the latest run as follows:'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当运行完成时，你将拥有10种不同的聚类结果可供检查。你可以使用Metaflow客户端API在交互式Python shell或使用笔记本来检查结果，如第3.3.2节所述。以下是轻松完成此操作的方法。首先，按照以下方式访问最新运行的输出：
- en: '[PRE11]'
  id: totrans-177
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Thanks to the fact that Metaflow persists results of all runs, you can inspect
    and compare results between runs, too.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 由于Metaflow会持久化所有运行的输出，因此你还可以检查和比较运行之间的结果。
- en: 'We produced an artifact top in the join step, which contains a conveniently
    aggregated set of top words for each cluster, keyed by the hyperparameter value,
    that is, the number of clusters. Here, we take a peek at the results of the K-means
    with 40 clusters as follows:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在连接步骤中，我们生成了一个名为“top”的工件，它包含每个聚类的方便聚合的顶级单词集合，以超参数值为键，即聚类的数量。在这里，我们看一下40个聚类的K-means算法的结果如下：
- en: '[PRE12]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '40 clusters are available for inspection. Let’s spot-check a few of them, looking
    at the top 5 words and their frequencies in the cluster like so:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 有40个聚类可供检查。让我们抽查几个，查看聚类中的前5个单词及其频率，如下所示：
- en: '[PRE13]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'which seems to be about pizza reviews. The next one seems to be about hotels:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 这看起来是关于披萨评论的。下一个看起来是关于酒店的：
- en: '[PRE14]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'And this one seems to be about car rentals:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 这一个看起来是关于汽车租赁的：
- en: '[PRE15]'
  id: totrans-186
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: By default, K-means doesn’t guarantee that results will always be the same.
    For instance, random initialization causes results to be slightly different in
    every run, so don’t expect to see exactly the same results as shown here. As an
    exercise, you can come up with a creative way to tabulate and visualize the clusterings
    in a notebook.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，K-means不能保证结果总是相同的。例如，随机初始化会导致每次运行的结果略有不同，因此不要期望看到与这里显示的完全相同的结果。作为一个练习，你可以想出一个创造性的方法在笔记本中表格化和可视化聚类。
- en: 5.3 Practicing performance optimization
  id: totrans-188
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 实践性能优化
- en: 'Thus far we have learned about the two main dimensions of scalability: vertical
    scalability (using a bigger instance) and horizontal scalability (using more instances).
    Notably, both of these techniques allow us to handle larger datasets and more
    demanding algorithms with minimal changes in the code. Scalability like this is
    appealing from a productivity point of view: the cloud allows us to solve problems
    by throwing more hardware at them so we can spare human resources for more interesting
    tasks.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经学习了可扩展性的两个主要维度：垂直可扩展性（使用更大的实例）和水平可扩展性（使用更多实例）。值得注意的是，这两种技术都允许我们在代码变化最小的情况下处理更大的数据集和更复杂的需求。从生产力的角度来看，这种可扩展性很有吸引力：云服务允许我们通过投入更多的硬件来解决问题，从而节省人力资源去完成更有趣的任务。
- en: In what situations should we consider spending a data scientist’s time on optimizing
    code by hand? Consider that you have a computationally expensive algorithm, say,
    model training, implemented as a Python function. Executing the function takes
    five hours. Depending on how the algorithm is implemented, vertical scalability
    might not help much if the algorithm can’t take advantage of multiple CPU cores,
    more memory, or GPUs efficiently. Horizontal scalability doesn’t help, either.
    You could execute the function in 100 parallel tasks, but each one of them would
    still take five hours, so the total execution time is still five hours.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 在什么情况下我们应该考虑花费数据科学家的时间手动优化代码？考虑一下你有一个计算成本高昂的算法，比如模型训练，实现为一个Python函数。执行该函数需要五小时。根据算法的实现方式，如果算法不能有效地利用多个CPU核心、更多内存或GPU，垂直可扩展性可能帮助不大。水平可扩展性也无法帮助。你可以在100个并行任务中执行该函数，但每个任务仍然需要五小时，因此总执行时间仍然是五小时。
- en: A critical question is whether it matters that the execution takes five hours.
    Maybe the workflow is scheduled to run nightly, so it doesn’t make a huge difference
    whether it runs for five or two hours—the results are ready by morning in any
    case. However, if the function needs to be run hourly, we have a problem. It is
    simply impossible to produce hourly results with a function that takes five hours
    to execute.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 一个关键问题是执行时间是否为五小时是否重要。也许工作流程是每晚运行，所以运行五小时或两小时并没有太大区别——无论如何，结果都会在早上准备好。然而，如果函数需要每小时运行一次，我们就遇到了问题。用需要五小时执行时间的函数来产生每小时的结果是不可能的。
- en: In a situation like this, a data scientist needs to spend time rethinking the
    algorithm to make it more performant. A key lesson of the next section is that
    even performance optimization is a spectrum. The data scientist doesn’t have to
    throw away the Python implementation and rewrite the algorithm, say, in C++. They
    can use number of tools and techniques to gradually optimize the algorithm to
    the point that it is *good enough*.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，数据科学家需要花时间重新思考算法，使其更高效。下一个部分的关键教训是，即使是性能优化也是一个连续的过程。数据科学家不必丢弃Python实现并重新编写算法，比如用C++。他们可以使用多种工具和技术，逐步优化算法，直到它足够好。
- en: The next section will walk through a realistic example that shows how a numerically
    intensive algorithm that is initially implemented in straightforward Python can
    be gradually optimized to the point where its performance becomes comparable to
    a sophisticated multicore C++ implementation. As you will see, a simple optimization
    that is easy to implement yields 80% of benefits. Squeezing out the last 20% of
    performance takes 80% of the time.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个部分将逐步通过一个现实生活中的例子，展示一个最初用简单的Python实现的数值密集型算法是如何逐渐优化到其性能可以与复杂的C++多核实现相媲美的程度。正如你将看到的，一个简单且易于实现的优化可以带来80%的效益。挤出最后20%的性能需要80%的时间。
- en: Performance tip Premature optimization is the root of all evil. Don’t worry
    about performance until you have exhausted all other easier options. If you must
    optimize performance, know when to stop.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 性能提示：过早优化是万恶之源。在你用尽所有其他更简单的选项之前，不要担心性能问题。如果你必须优化性能，要知道何时停止。
- en: '5.3.1 Example: Computing a co-occurrence matrix'
  id: totrans-195
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.1 示例：计算共现矩阵
- en: 'The previous sections used a bag-of-words matrix that records the relationship
    between documents and words. From this matrix, we can derive another interesting
    matrix: *a word-word co-occurrence matrix*. The co-occurrence matrix records how
    frequently a word occurs in the same document with any other word. It can be useful
    for understanding semantic similarity between words, and having the matrix makes
    it possible to compute various word-level metrics quickly. Figure 5.7 expands
    our earlier bag-of-words example to show the corresponding co-occurrence matrix.'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 前面的部分使用了一个词袋矩阵来记录文档和单词之间的关系。从这个矩阵中，我们可以推导出另一个有趣的矩阵：*单词-单词共现矩阵*。共现矩阵记录了一个单词在同一个文档中与任何其他单词一起出现的频率。这对于理解单词之间的语义相似性可能很有用，并且有了这个矩阵，可以快速计算各种单词级指标。图5.7扩展了我们早期的词袋示例，以显示相应的共现矩阵。
- en: '![CH05_F07_Tuulos](../../OEBPS/Images/CH05_F07_Tuulos.png)'
  id: totrans-197
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F07_Tuulos](../../OEBPS/Images/CH05_F07_Tuulos.png)'
- en: Figure 5.7 From original documents to a word-word co-occurrence matrix
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.7 从原始文档到单词-单词共现矩阵
- en: If the dimensions of a bag-of-word matrix are N*M, where N is the number of
    documents and M is the number of unique words, the corresponding co-occurrence
    matrix has the dimensions of M*M. Crucially, the bag-of-words-matrix contains
    all the information we need to construct the corresponding co-occurrence matrix.
    A simple way to construct a co-occurrence matrix is to iterate through all rows
    of the bag-of-matrix matrix and, for each row, produce all pairs of words and
    increase their count in the co-occurrence matrix.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 如果词袋矩阵的维度是N*M，其中N是文档的数量，M是唯一单词的数量，那么相应的共现矩阵的维度是M*M。关键的是，词袋矩阵包含了我们构建相应共现矩阵所需的所有信息。构建共现矩阵的一个简单方法是通过遍历词袋矩阵的所有行，并对每一行，生成所有单词对并增加它们在共现矩阵中的计数。
- en: 'We can take advantage of the fact that the bag-of-words matrix is a sparse
    matrix, that is, it doesn’t store any zero entries, which wouldn’t affect the
    co-occurrence matrix anyway. To be able to design an efficient algorithm to process
    the data, let’s take a peek under the hood of scipy.sparse.csr_matrix, which implements
    the sparse matrix. The *compressed sparse row* (CSR) matrix in SciPy is composed
    of three dense arrays, as illustrated in figure 5.8:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用词袋矩阵是一个稀疏矩阵的事实，即它不存储任何零条目，这无论如何都不会影响共现矩阵。为了能够设计一个高效的算法来处理数据，让我们看看scipy.sparse.csr_matrix的内部，它实现了稀疏矩阵。SciPy中的*压缩稀疏行*（CSR）矩阵由三个密集数组组成，如图5.8所示：
- en: indptr indicates where each row begins and ends in the indices array. This array
    is needed, because each document can contain a different number of unique words.
    The indptr array contains an extra element in the end, to indicate the length
    of the last document.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: indptr表示每个行在indices数组中的开始和结束位置。这个数组是必需的，因为每个文档可以包含不同数量的唯一词。indptr数组在末尾包含一个额外的元素，以指示最后一个文档的长度。
- en: indices indicates which columns (words) have nonzero values on this row.
  id: totrans-202
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: indices表示哪些列（词）在此行上有非零值。
- en: data contains the frequency of each word in each document. It is aligned with
    the indices array.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: data包含每个文档中每个词的频率。它与indices数组对齐。
- en: '![CH05_F08_Tuulos](../../OEBPS/Images/CH05_F08_Tuulos.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F08_Tuulos](../../OEBPS/Images/CH05_F08_Tuulos.png)'
- en: Figure 5.8 The internal data structures of scipy.sparse.csr_matrix
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.8 scipy.sparse.csr_matrix的内部数据结构
- en: To construct the co-occurrence matrix, we don’t care how many times a word occurs
    in a document. Hence, our bag-of-words matrix can be a binary matrix, and, correspondingly,
    the data array is redundant because it is all ones.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 构建共现矩阵时，我们并不关心一个词在文档中出现的次数。因此，我们的词袋矩阵可以是一个二元矩阵，相应地，数据数组是冗余的，因为它全部是1。
- en: Equipped with this problem definition and knowledge of the data structure behind
    the sparse matrix, we can implement the first variant of an algorithm that computes
    a co-occurrence matrix for the Yelp dataset.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 配备了这个问题定义以及关于稀疏矩阵背后的数据结构的知识，我们可以实现一个算法的第一个变体，该算法用于计算Yelp数据集的共现矩阵。
- en: 'Variant 1: A plain Python implementation'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 变体1：纯Python实现
- en: Imagine that a data scientist needs to compute a co-occurrence matrix as a preprocessing
    step for another algorithm. Maybe the simplest approach would be to compute co-occurrences
    directly on the text data, iterating documents string by string, but this would
    be a very slow approach. Let’s assume that the data scientist has already produced
    a bag-of-words matrix using our scale_data module. The data scientist writes an
    algorithm that iterates through the sparse matrix to construct a co-occurrence
    matrix. Their solution could look like the one shown next.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果数据科学家需要计算共现矩阵作为另一个算法的预处理步骤，可能会采取的最简单的方法是直接在文本数据上计算共现，逐个文档地迭代字符串，但这将是一个非常慢的方法。假设数据科学家已经使用我们的scale_data模块生成了一个词袋矩阵。数据科学家编写了一个算法，遍历稀疏矩阵以构建共现矩阵。他们的解决方案可能看起来像下面所示。
- en: 'Listing 5.7 First variant: Co-occurrences in pure Python'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.7 第一个变体：纯Python中的共现
- en: '[PRE16]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Iterates over all rows of the bag-of-words matrix
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历词袋矩阵的所有行
- en: ❷ Iterates over all nonzero columns (words) in a row
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 遍历一行中的所有非零列（词）
- en: ❸ Increments the diagonal of the matrix, which indicates the word frequency
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 增加矩阵的对角线，这表示词的频率
- en: ❹ Increments the word-word pair count—the matrix is symmetric
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 增加词-词对计数——矩阵是对称的
- en: ❺ A utility function to create a new co-occurrence matrix
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 创建新共现矩阵的实用函数
- en: ❻ An interface function that creates and populates a new co-occurrence matrix
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 创建并填充新共现矩阵的接口函数
- en: 'Save the code in listing 5.7 in a separate module named cooc_plain.py. This
    time, the exact filename matters, as we will see soon. The implementation is straightforward:
    we iterate over all pairs of words over all rows and increment word-word counts
    in the target matrix as we go. Besides the core algorithm, simple_cooc, we include
    a helper function to allocate a new co-occurrence matrix of the right shape, new_cooc,
    and a function to create and populate the matrix, compute_cooc, which we will
    use as an entry point to the module. These functions will come in handy soon.'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表 5.7 中的代码保存在一个名为 cooc_plain.py 的单独模块中。这次，确切的文件名很重要，因为我们很快就会看到。实现很简单：我们遍历所有行中的所有单词对，并在执行过程中递增目标矩阵中的单词-单词计数。除了核心算法
    simple_cooc 之外，我们还包含一个辅助函数来分配正确形状的新共现矩阵，new_cooc，以及一个创建并填充矩阵的函数 compute_cooc，我们将将其用作模块的入口点。这些函数很快就会派上用场。
- en: Let’s create a flow to test the algorithm. The next listing shows a flow that
    supports pluggable algorithms. We can use it to test other variants besides cooc_plain.py,
    too. Save the flow to cooc_flow.py.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个流程来测试算法。接下来的列表显示了一个支持可插拔算法的流程。我们也可以用它来测试 cooc_plain.py 之外的其它变体。将流程保存到
    cooc_flow.py。
- en: Listing 5.8 A flow that produces a co-occurrence matrix
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.8：生成共现矩阵的流程
- en: '[PRE17]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ We will need Numba later, so we will include it as a dependency.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们稍后会需要 Numba，所以我们将将其作为依赖项包含。
- en: ❷ Specifies which algorithm variant to use
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定要使用的算法变体
- en: ❸ The start step uses scale_data similar to our previous examples.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 开始步骤使用 scale_data，类似于我们之前的示例。
- en: ❹ Note binary=True, which indicates that we need only a binary matrix.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 注意 binary=True，这表示我们只需要一个二进制矩阵。
- en: ❺ Producing the result can take quite a bit of memory.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 生成结果可能需要相当多的内存。
- en: ❻ Loads a pluggable variant defined by the algo parameter
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 加载由 algo 参数定义的可插拔变体
- en: ❼ Computes the co-occurrence matrix
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算共现矩阵
- en: Here, the start step is familiar from our previous examples. The only difference
    is that we specify binary=True because for co-occurrences, we don’t care about
    the frequency of words in a document. The compute_cooc supports pluggable algorithms.
    instead of hardcoding an import statement, we choose which variant to import based
    on a parameter, algo. We use Python’s built-in importlib.import_module to import
    a module from a file prefixed with the string cooc_.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，开始步骤与我们的前例相似。唯一的区别是我们指定 binary=True，因为对于共现，我们不在乎文档中单词的频率。compute_cooc 支持可插拔算法。我们不是硬编码导入语句，而是根据参数
    algo 选择要导入的变体。我们使用 Python 内置的 importlib.import_module 从以 cooc_ 为前缀的文件中导入模块。
- en: Tip Dynamic loading of modules using importlib.import_module is a great way
    to implement plugins for flows. Often, the overall structure of DAG doesn’t change
    between plugins, so you can keep the DAG static but choose the desired functionality
    on the fly.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：使用 importlib.import_module 动态加载模块是实现流程插件的好方法。通常，DAG 的整体结构在插件之间不会改变，因此你可以保持
    DAG 静态，但可以动态选择所需的功能。
- en: 'Let’s start by testing the flow locally with a small subset of data, 1,000
    documents, as follows:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先通过一小部分数据，1,000 个文档，在本地测试流程，如下所示：
- en: '[PRE18]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This will use the cooc_plain module that we defined previously. It seems executing
    the algorithm with 1,000 rows (documents) and 1,148 columns (words) takes about
    five seconds, which, on first glance, doesn’t seem too bad.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 这将使用我们之前定义的 cooc_plain 模块。看起来使用 1,000 行（文档）和 1,148 列（单词）执行算法大约需要五秒钟，乍一看，似乎并不太糟糕。
- en: Try increasing --num-docs to 10,000\. Now, the algorithm takes 74 seconds! If
    you dare, you can try executing it with the full dataset. It takes ages to finish.
    You can try executing it --with batch varying @resources, but the timings won’t
    nudge—this algorithm is not performant nor scalable. If the task is to produce
    a co-occurrence matrix for the full dataset of 650,000 documents, clearly this
    algorithm is not going to work.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试将 --num-docs 增加到 10,000。现在，算法需要 74 秒！如果你敢尝试，可以用完整的数据集执行它。这需要花费很长时间才能完成。你可以尝试使用
    --with batch varying @resources 执行它，但时间不会有太大变化——这个算法既不高效也不可扩展。如果任务是生成包含 650,000
    个文档的完整数据集的共现矩阵，显然这个算法是不行的。
- en: 'Variant 2: Leveraging a high-performance library'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 变体 2：利用高性能库
- en: Whenever a basic Python implementation turns out to be too slow, it is advisable
    to try to find an off-the-shelf library that contains an optimized implementation
    of the same algorithm. Given the vastness of the Python data science ecosystem,
    you often learn that someone has implemented a suitable solution already. Or,
    even if a perfectly matching solution doesn’t exist, someone might have implemented
    an efficient building block that we can use to optimize our solution.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 当一个基本的Python实现变得太慢时，尝试找到一个包含相同算法优化实现的现成库是明智的。鉴于Python数据科学生态系统的庞大，您通常会了解到有人已经实现了合适的解决方案。或者，即使没有完全匹配的解决方案，也可能有人实现了我们用来优化解决方案的高效构建块。
- en: Unfortunately, in the case of our example, it seems that Scikit-Learn doesn’t
    provide a function for computing a co-occurrence matrix. For the purposes of our
    example, we ignore the fact that a suitable implementation might exist elsewhere.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，在我们的例子中，Scikit-Learn似乎没有提供计算共现矩阵的函数。为了我们的示例，我们忽略了一个合适的实现可能存在于其他地方的事实。
- en: 'The data scientist turns to a colleague who has experience in high-performance
    numerical computing. The colleague points out that the algorithm implemented by
    cooc_plain is effectively an algorithm for matrix multiplication. It turns out
    that computing a co-occurrence matrix *C* based on a binary bag-of-words matrix
    *B* corresponds exactly to the following matrix equation:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家转向一位在高性能数值计算方面有经验的同学。这位同学指出，cooc_plain实现的算法实际上是一个矩阵乘法算法。结果发现，基于二进制词袋矩阵B计算共现矩阵C正好对应以下矩阵方程：
- en: C = B^TB
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: C = B^TB
- en: where B^T denotes the transpose (rotation) of the matrix of B. If you multiply
    an M*N matrix with an N*M matrix, the result is an M*M matrix, which, in this
    case, is exactly our co-occurrence matrix. Implementing this variant of the algorithm
    is embarrassingly simple, as shown in the following code listing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 其中，B^T表示矩阵B的转置（旋转）。如果您将一个M*N矩阵与一个N*M矩阵相乘，结果是M*M矩阵，在这种情况下，正好是我们的共现矩阵。实现这个算法的变体非常简单，如下面的代码列表所示。
- en: 'Listing 5.9 Second variant: Leveraging linear algebra'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表5.9 第二个变体：利用线性代数
- en: '[PRE19]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Save the code in listing 5.9 in cooc_linalg.py, for a variant based on linear
    algebra. Thanks to pluggable algorithm support in CoocFlow, we can test this variant
    simply by running the following code:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 将列表5.9中的代码保存在cooc_linalg.py中，这是一个基于线性代数的变体。多亏了CoocFlow的可插拔算法支持，我们可以通过运行以下代码简单地测试这个变体：
- en: '[PRE20]'
  id: totrans-244
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: This variant finishes in 1.5 seconds instead of 74 seconds—a 50× speedup! The
    code is simpler and faster than the original Python version, which is perfect.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变体在1.5秒内完成，而不是74秒——速度提升了50倍！代码比原始的Python版本更简单、更快，这非常完美。
- en: Because the code performs so well, you can go ahead and try it with the full
    dataset by specifying --num-docs 1000000. Alas, the run will likely fail due to
    an out-of-memory exception. You can resort to vertical scaling and try it --with
    batch, but even with 64 GB, the run keeps failing.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 由于代码表现如此出色，您可以尝试使用完整的数据集，通过指定--num-docs 1000000来运行。然而，由于内存不足异常，运行可能会失败。您可以尝试垂直扩展并使用--with
    batch来运行，但即使有64 GB的内存，运行仍然会失败。
- en: A problem with this variant is that taking the transpose of the original matrix,
    mtx.T, makes a copy of the full dataset, doubling the memory requirement in addition
    to having to store the co-occurrence matrix in memory. Although the cooc_plain
    variant doesn’t perform well, at least it was more space efficient, avoiding unnecessary
    copies.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变体的问题在于，取原始矩阵mtx.T的转置会复制整个数据集，除了需要在内存中存储共现矩阵外，还会使内存需求翻倍。尽管cooc_plain变体表现不佳，但至少它更节省空间，避免了不必要的复制。
- en: In this case, you can keep increasing memory requirements until the algorithm
    completes successfully. Given the elegant simplicity of this algorithm, relying
    on vertical scalability would be an appealing solution. However, for the sake
    of discussion, let’s say the data scientist can’t rely on the highest-memory instances,
    so they must keep seeking a more optimal variant.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您可以不断增加内存需求，直到算法成功完成。鉴于这个算法的优雅简洁，依赖垂直扩展可能是一个吸引人的解决方案。然而，为了讨论的目的，让我们假设数据科学家不能依赖最高内存实例，因此他们必须继续寻找更优的变体。
- en: 'Variant 3: Compiling Python with Numba'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 变体3：使用Numba编译Python
- en: Colleagues of our data scientist point out that the main problem with variant
    1 is that it is written in Python. Had it been written in C++, it would likely
    perform much better while not wasting space unnecessarily like variant 2\. This
    is a valid argument from a technical point of view, but including a piece of custom
    C++ in a Python project feels like a hassle, not to mention that our data scientist
    isn’t familiar with C++.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 我们数据科学家的同事指出，变体 1 的主要问题是它是用 Python 编写的。如果它用 C++ 编写，它可能会表现得更好，而不会像变体 2 那样浪费空间。这是一个从技术角度来看合理的论点，但将一段自定义的
    C++ 代码包含在 Python 项目中感觉像是一种麻烦，更不用说我们的数据科学家对 C++ 不熟悉了。
- en: Fortunately, a few libraries can help make numerical Python code faster by compiling
    it to machine code, similar to what a C++ compiler would do. The most well-known
    compilers for Python are Cython ([cython.org](https://cython.org/)) and Numba
    ([numba.pydata.org](https://numba.pydata.org/)).
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，一些库可以帮助通过将其编译成机器代码来加快数值 Python 代码的速度，类似于 C++ 编译器所做的那样。最著名的 Python 编译器是
    Cython ([cython.org](https://cython.org/)) 和 Numba ([numba.pydata.org](https://numba.pydata.org/))。
- en: These compilers can’t magically make any piece of Python as fast as C++, but
    they shine at optimizing functions that perform numerical computation, typically
    using NumPy arrays. In other words, a function like simple_cooc that performs
    loops over a few NumPy arrays should be squarely in the domain of these compilers.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这些编译器不能神奇地将任何 Python 代码块的速度提升到 C++ 的速度，但它们在优化执行数值计算的函数方面表现出色，通常使用 NumPy 数组。换句话说，像
    simple_cooc 这样的函数，它对几个 NumPy 数组执行循环，应该完全属于这些编译器的领域。
- en: The next code snippet shows how to compile the simple_cooc function on the fly
    using Numba. Save this variant to cooc_numba.py.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码片段展示了如何使用 Numba 在线编译 simple_cooc 函数。将这个变体保存到 cooc_numba.py。
- en: 'Listing 5.10 Third variant: Co-occurrences with Numba'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.10 第三变体：使用 Numba 的共现
- en: '[PRE21]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Compiles the simple_cooc function on the fly using Numba
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用 Numba 在线编译 simple_cooc 函数
- en: The hard thing about using Numba is writing a function that avoids using any
    idiomatic Python constructs like objects and dictionaries and avoids allocating
    memory. You must focus on simple arithmetic on arrays, like simple_cooc does.
    Once you have managed to do this, using Numba is easy. Like listing 5.10 shows,
    all you have to do is call the jit function and pass the function to be called
    as an argument.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Numba 的难点在于编写一个避免使用任何 Python 习惯用法（如对象和字典）并避免分配内存的函数。你必须专注于数组上的简单算术，就像 simple_cooc
    所做的那样。一旦你做到了这一点，使用 Numba 就变得简单了。就像列表 5.10 所示，你所要做的就是调用 jit 函数并将要调用的函数作为参数传递。
- en: The result is a version of the given function, here fast_cooc, that is typically
    significantly faster than the original version. This magic is made possible by
    the fact that Numba compiles the function to machine code, which is virtually
    indistinguishable from a version written in C++. The nopython=True flag indicates
    that the function doesn’t use any Python constructs, so a slow compatibility layer
    with Python can be avoided.
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是给定函数（这里指 fast_cooc）的一个版本，通常比原始版本快得多。这种神奇的效果得益于 Numba 将函数编译成机器代码，这几乎与用 C++
    编写的版本无法区分。nopython=True 标志表示该函数不使用任何 Python 构造，因此可以避免与 Python 的慢速兼容层。
- en: 'Test this variant as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 按以下方式测试这个变体：
- en: '[PRE22]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: This version of the algorithm takes 2.7 seconds, so it is slightly slower than
    cooc_linalg, which ran for 1.5 seconds. The difference is understandable because
    the Numba timing includes compilation time as well. Notably, this version doesn’t
    take any extra space. This variant is able to handle the full dataset in 50 seconds—not
    too shabby!
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法版本耗时 2.7 秒，因此比 cooc_linalg 慢一些，后者运行时间为 1.5 秒。这种差异是可以理解的，因为 Numba 的时间包括编译时间。值得注意的是，这个版本不占用任何额外空间。这个变体能够在
    50 秒内处理完整数据集——这并不算差劲！
- en: 'Variant 4: Parallelizing the algorithm over multiple CPU cores'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 变体 4：在多个 CPU 核心上并行化算法
- en: 'Although variant 3 is able to handle the full dataset quite quickly, it is
    a fundamentally unscalable algorithm: adding more memory or CPU resources doesn’t
    make it any faster or capable of having larger matrices. Presumably, one could
    get results even faster if the algorithm was able to leverage multiple CPU cores
    that we can easily request with @resources.'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管变体 3 能够相当快地处理完整数据集，但它是一个本质上不可扩展的算法：增加更多内存或 CPU 资源并不会使其更快或能够处理更大的矩阵。假设如果算法能够利用我们能够轻松请求的多个
    CPU 核心的话，可能会得到更快的结果。
- en: 'Note that this optimization comes with a cost: the implementation is more complex
    than variants 2 and 3\. One needs to be much more careful in confirming that it
    works correctly. As it often happens with performance optimizations, the first
    20% of the effort can bring 80% of the benefits. Spending time to squeeze the
    last bits of performance is not worth it for most use cases. Consider this variant
    through this cautionary lens.'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，这种优化是有代价的：实现比变体 2 和 3 更复杂。需要非常小心地确认它是否正确工作。正如性能优化经常发生的那样，最初的 20% 的努力可以带来
    80% 的好处。在大多数用例中，花费时间来挤压最后一点性能是不值得的。通过这个警告的视角考虑这个变体。
- en: Looking at the algorithm in simple_cooc, we can see that the outer loop iterates
    over the rows of the input matrix. Could we split the input matrix so that each
    CPU core handles only a subset, a shard, of the rows? A challenge is a row may
    update any location of the result matrix that would need to be shared across all
    CPU cores. Sharing writable data across multiple worker processes or threads is
    a hard problem that we would rather avoid.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 观察简单_cooc 中的算法，我们可以看到外层循环遍历输入矩阵的行。我们能否将输入矩阵分割，使得每个 CPU 核心只处理行的一个子集，即一个碎片？一个挑战是某一行可能会更新结果矩阵的任何位置，这需要所有
    CPU 核心共享。在多个工作进程或线程之间共享可写数据是一个难题，我们宁愿避免。
- en: 'A simple insight comes to the rescue: we can let each thread write to a private
    copy of the co-occurrence matrix, which we can simply sum together in the end.
    A downside is that the memory consumption increases again, but in contrast to
    variant 2, we need copies of co-occurrence matrices that are smaller than the
    full dataset. Save the variant in the next listing in cooc_multicore.py.'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 一个简单的洞察力挽救了局面：我们可以让每个线程写入一个私有的共现矩阵副本，我们可以在最后简单地将它们相加。一个缺点是内存消耗再次增加，但与变体 2 相比，我们需要的共现矩阵副本比完整数据集要小。将此变体保存在下一个列表中的
    cooc_multicore.py。
- en: 'Listing 5.11 Fourth variant: Using Numba with multiple CPU cores'
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 5.11 第四种变体：使用 Numba 和多个 CPU 核心
- en: '[PRE23]'
  id: totrans-268
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Shards the input matrix to num_cpu equal-size shards or batches, each with
    a private output matrix
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将输入矩阵分割成与num_cpu数量相等的等大小碎片或批次，每个碎片或批次都有一个私有的输出矩阵
- en: ❷ Uses a thread pool to process the batches over num_cpu threads on multiple
    CPU cores in parallel
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用线程池以并行方式在多个 CPU 核心上处理批次，每个 CPU 核心使用 num_cpu 个线程
- en: ❸ Waits for the threads to complete
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 等待线程完成
- en: ❹ Aggregates the private output matrices to a single output matrix
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将私有输出矩阵聚合到一个单一的输出矩阵中
- en: ❺ Notes the nogil=True flag, which allow Python to execute the threads in a
    truly parallel fashion
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 注意 nogil=True 标志，它允许 Python 以真正并行的方式执行线程
- en: 'The algorithm works by splitting the rows of the input matrix into num_cpu
    equal-size shards or batches. Each batch gets its own private output matrix, created
    with cooc .copy(), so the threads don’t need locking or another way to coordinate
    updates. The batches are submitted to a thread pool that has num_cpu worker threads.
    After the threads have completed populating their private subsets of the co-occurrence
    matrix, the results are merged in the final output matrix. You can benchmark this
    version against earlier variants with the following code:'
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通过将输入矩阵的行分割成 num_cpu 个等大小的碎片或批次来工作。每个批次都得到自己的私有输出矩阵，通过 cooc.copy() 创建，这样线程就不需要锁定或以其他方式协调更新。批次被提交给具有
    num_cpu 个工作线程的线程池。在线程完成填充它们私有的共现矩阵子集后，结果被合并到最终的输出矩阵中。你可以使用以下代码与早期变体进行基准测试：
- en: '[PRE24]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: If you have come across multithreading in Python before, you may have heard
    of the *Global Interpreter Lock* (GIL) in Python (if you are curious, you can
    learn more about GIL at [https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock)).
    In short, the GIL prevents multiple threads executing Python code from running
    in parallel effectively. However, the GIL limitation doesn’t apply to this algorithm,
    because we use Numba to compile the Python code to machine code, similar to variant
    3\. Therefore, in the Python interpreter’s point of view, we are executing not
    Python code but rather native code, which is free from the limitations of GIL.
    We just need to remember to add nogil=True to the jit call to remind Numba about
    this detail.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前在 Python 中遇到过多线程，你可能听说过 Python 中的 *全局解释器锁*（GIL）（如果你好奇，你可以在 [https://wiki.python.org/moin/GlobalInterpreterLock](https://wiki.python.org/moin/GlobalInterpreterLock)
    上了解更多关于 GIL 的信息）。简而言之，GIL 阻止多个线程有效地并行执行 Python 代码。然而，GIL 的限制并不适用于这个算法，因为我们使用 Numba
    将 Python 代码编译成机器码，类似于第 3 个变体。因此，从 Python 解释器的角度来看，我们执行的不是 Python 代码，而是本地代码，它不受
    GIL 的限制。我们只需要记住在 jit 调用中添加 nogil=True，以提醒 Numba 注意这个细节。
- en: This variant is a great example of how going multicore is not a panacea. Although
    reading the rows of the matrix is an embarrassingly parallel problem, requiring
    no coordination between threads, writing the output isn’t. In this case, the cost
    we pay is duplication of the output matrix. Another approach would be to apply
    locking to the output matrix. In any case, each new thread increases the cost
    a bit. As a result, adding cores helps, but only to a limit, similar to what we
    experienced with the parallelization of K-means in section 4.3.1\. Figure 5.9
    illustrates the effect.
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这个变体是说明如何实现多核并非万能的一个很好的例子。尽管读取矩阵的行是一个令人尴尬的并行问题，不需要线程间的协调，但写入输出却不是。在这种情况下，我们付出的代价是输出矩阵的重复。另一种方法是应用锁定到输出矩阵上。无论如何，每个新线程都会增加一点成本。因此，增加核心有助于提高性能，但仅限于一定程度，类似于我们在第
    4.3.1 节中并行化 K-means 经验到的。图 5.9 展示了这种效果。
- en: '![CH05_F09_Tuulos](../../OEBPS/Images/CH05_F09_Tuulos.png)'
  id: totrans-278
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F09_Tuulos](../../OEBPS/Images/CH05_F09_Tuulos.png)'
- en: Figure 5.9 Execution time vs. the number CPU cores in the multithreaded case
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 多线程情况下的执行时间与 CPU 核心数的关系
- en: Figure 5.9 shows that running the algorithm with num_cpu=1 takes about 100 seconds
    for a version of the full matrix. For this dataset, the sweet spot seems to be
    at num_cpu=4, which improves performance by about 40%. Beyond this, the overhead
    of creating and aggregating per-thread output matrices overtakes the benefits
    of handling increasingly small input shards in each thread.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 显示，使用 num_cpu=1 运行算法需要大约 100 秒来处理完整矩阵的版本。对于这个数据集，最佳点似乎在 num_cpu=4，这提高了大约
    40% 的性能。超过这个点，创建和聚合每个线程的输出矩阵的开销超过了处理每个线程中越来越小的输入分片的好处。
- en: Summarizing the variants
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 总结变体
- en: 'This section illustrated a realistic journey of optimizing performance of a
    numerically intensive algorithm as follows:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 本节展示了优化数值密集型算法性能的实际情况如下：
- en: First, we started with a simple version of the algorithm.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们从算法的简单版本开始。
- en: It turned out that the algorithm really isn’t performant enough, given the requirements
    of the use case, so we evaluated whether there’s an easy way out by resorting
    to vertical or horizontal scalability. It turned out that neither of these approaches
    speeded up execution adequately with the simple algorithm.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结果证明，考虑到用例的要求，该算法的性能确实不够，所以我们评估了通过垂直或水平扩展来找到简单解决方案的可能性。结果发现，这两种方法都没有足够地加快简单算法的执行速度。
- en: We evaluated whether an off-the-shelf optimization implementation was available.
    We figured out an elegant and performant solution based on simple linear algebra.
    However, the solution had the side effect of increased memory consumption, which
    we could solve by vertical scalability. Had this been a real use case, variant
    2, with vertical scalability (high-memory instances), seems like a good choice
    based on effort, performance, and maintainability.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们评估了是否有现成的优化实现。我们找到了一个基于简单线性代数的优雅且高效的解决方案。然而，这个解决方案的副作用是增加了内存消耗，我们可以通过垂直扩展来解决。如果这是一个真实用例，基于努力、性能和可维护性的考虑，具有垂直扩展（高内存实例）的变体
    2 似乎是一个不错的选择。
- en: To illustrate a more advanced optimization, we introduced Numba, which worked
    well for this use case. However, the default implementation doesn’t take advantage
    of multiple CPU cores.
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为了说明更高级的优化，我们引入了 Numba，它在这种用例中表现良好。然而，默认实现没有充分利用多个 CPU 核心。
- en: Finally, we implemented a compiled, multicore variant, the performance of which
    should compare favorably against a well-optimized, parallelized C++ algorithm.
    Yet, we managed to do it all in fewer than 100 lines of Python.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们实现了一个编译的、多核的变体，其性能应该与优化良好的并行化 C++ 算法相媲美。然而，我们只用不到 100 行 Python 就完成了所有工作。
- en: Figure 5.10 shows a performance benchmark of the four variants.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 展示了四种变体的性能基准。
- en: '![CH05_F10_Tuulos](../../OEBPS/Images/CH05_F10_Tuulos.png)'
  id: totrans-289
  prefs: []
  type: TYPE_IMG
  zh: '![CH05_F10_Tuulos](../../OEBPS/Images/CH05_F10_Tuulos.png)'
- en: Figure 5.10 Performance of the four variants vs. the number of documents
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.10 四种变体与文档数量的性能对比
- en: 'When the dataset is small enough—100 documents here—the implementation doesn’t
    make any difference. With 1,000 documents, the plain Python implementation starts
    being clearly slower, although in absolute terms, the execution time may be tolerable.
    At 10,000 documents and beyond, the plain version becomes practically unusable,
    so it is excluded from the large-scale benchmarks. The three performant variants
    are nearly equally performant, except the linalg variant runs out of memory with
    64 GB at the highest scale. The complexity of the multicore variant starts paying
    off only with the full dataset. Figure 5.10 condenses the key learnings of this
    section as follows:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集足够小（这里为 100 个文档）时，实现不会产生任何差异。当有 1,000 个文档时，纯 Python 实现开始明显变慢，尽管在绝对意义上，执行时间可能是可接受的。在
    10,000 个文档及以上时，纯版本实际上变得无法使用，因此被排除在大规模基准测试之外。三个高性能变体几乎同样高效，除了 linalg 变体在最高扩展级别时耗尽内存。多核变体的复杂性只有在完整数据集上才会开始显现效果。图
    5.10 如下总结了本节的关键学习经验：
- en: The original plain implementation was impractically inefficient. Something needed
    to be done.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 原始的纯实现效率极低，需要采取一些措施。
- en: A simple observation of how to leverage an existing high-performance library
    to implement the linalg solution produced massive speedups with minimal effort.
  id: totrans-293
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过简单观察如何利用现有的高性能库来实现 linalg 解决方案，以最小的努力实现了巨大的速度提升。
- en: It was possible to implement a custom solution, multicore, that performed even
    better than linalg, but the implementation is significantly more complex.
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现一个定制的、多核的解决方案是可能的，其性能甚至优于 linalg，但实现要复杂得多。
- en: We were able to implement all solutions in Python without having to switch to
    a high-performance but low-level language like C++.
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能够在不切换到像 C++ 这样高性能但低级语言的情况下，用 Python 实现所有解决方案。
- en: These learnings are readily applicable to many other numerically intensive Python
    tasks. The modern Python data science ecosystem provides a hugely powerful and
    versatile toolkit for implementing high-performance algorithms.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 这些经验可以轻松应用于许多其他数值密集型 Python 任务。现代 Python 数据科学生态系统提供了一个功能强大且多才多艺的工具包，用于实现高性能算法。
- en: 5.3.2 Recipe for fast-enough workflows
  id: totrans-297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3.2 快速工作流程的配方
- en: 'Now that we have practiced scalability and performance in a number of scenarios,
    we can summarize the learnings of this chapter in a simple recipe that you can
    apply to your own use cases as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经在多个场景中实践了可扩展性和性能，我们可以将本章的学习总结成一个简单的配方，你可以将其应用于自己的用例，如下所示：
- en: Start with the simplest possible approach. A simple, obviously correct solution
    provides a robust foundation for gradual optimization.
  id: totrans-299
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从最简单的方法开始。一个简单且明显正确的解决方案为逐步优化提供了一个稳健的基础。
- en: If you are concerned that the approach is not scalable, think when and how you
    will hit the limits in practice. If the answer is never, or at least not any time
    soon, you can increase complexity only when it becomes necessary.
  id: totrans-300
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果你担心这种方法不可扩展，考虑在实际情况中何时以及如何达到极限。如果答案是永远不会，或者至少不是很快，你只能在必要时增加复杂性。
- en: Use vertical scalability to make the simple version work with realistic input
    data.
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用垂直扩展性使简单版本能够处理现实输入数据。
- en: If the initial implementation can’t take the advantage of hardware resources
    provided by vertical scalability, consider using an off-the-shelf optimized library
    that can.
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果初始实现不能利用垂直扩展性提供的硬件资源，可以考虑使用现成的优化库。
- en: If the workflow contains embarrassingly parallel parts and/or data can be easily
    sharded, leverage horizontal scalability for parallelism.
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果工作流程包含令人尴尬的并行部分，或者数据可以轻松分片，可以利用水平扩展性来实现并行处理。
- en: If the workflow is still too slow, carefully analyze where the bottlenecks lie.
    Consider whether simple performance optimizations could remove the bottleneck,
    maybe using one of the tools from the Python data science toolkit.
  id: totrans-304
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果工作流程仍然太慢，仔细分析瓶颈所在。考虑是否可以通过简单的性能优化来消除瓶颈，也许可以使用Python数据科学工具包中的工具。
- en: If the workflow is still too slow, which is rare, consider using specialized
    compute layers that can leverage distributed algorithms and specialized hardware.
  id: totrans-305
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果工作流程仍然太慢，这种情况很少见，可以考虑使用能够利用分布式算法和专用硬件的专用计算层。
- en: Summary
  id: totrans-306
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: It is advisable to start with a simple approach, initially optimizing for correctness
    rather than performance or scalability.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议从简单的方法开始，最初优化的是正确性，而不是性能或可扩展性。
- en: 'Vertical scalability contributes to the productivity of data scientists: they
    can make workflows handle more data and more demanding computation using simple
    and understandable Python code, just by requesting more hardware resources from
    the cloud.'
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直扩展性有助于数据科学家的生产力：他们可以通过从云中请求更多的硬件资源，仅使用简单易懂的Python代码，就让工作流程处理更多的数据和更复杂的需求。
- en: 'Horizontal scalability is useful in handling three scenarios: embarrassingly
    parallel tasks, large datasets, and distributed algorithms.'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 水平扩展性在处理三种场景时很有用：令人尴尬的并行任务、大型数据集和分布式算法。
- en: It is advisable to postpone performance optimizations until they become absolutely
    necessary. When optimizing performance, look for simple optimizations that can
    yield big benefits by analyzing bottlenecks carefully.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 建议将性能优化推迟到绝对必要时再进行。在优化性能时，寻找通过仔细分析瓶颈可以带来巨大收益的简单优化。
- en: The Python data science ecosystem includes a huge number of tools that can help
    optimize performance gradually. In particular, many problems can be solved by
    using foundational packages like Scikit-Learn, NumPy, and Numba.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python数据科学生态系统包括大量工具，可以帮助逐步优化性能。特别是，许多问题可以通过使用基础包如Scikit-Learn、NumPy和Numba来解决。
- en: You can easily combine horizontal and vertical scalability and high-performance
    code in a single workflow to address the scalability needs of your task at hand.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你可以轻松地将水平扩展性、垂直扩展性和高性能代码结合在一个工作流程中，以满足当前任务的可扩展性需求。
