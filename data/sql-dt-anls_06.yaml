- en: Chapter 6\. Anomaly Detection
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章. 异常检测
- en: An *anomaly* is something that is different from other members of the same group.
    In data, an anomaly is a record, an observation, or a value that differs from
    the remaining data points in a way that raises concerns or suspicions. Anomalies
    go by a number of different names, including *outliers*, *novelties*, *noise*,
    *deviations*, and *exceptions*, to name a few. I’ll use the terms *anomaly* and
    *outlier* interchangeably throughout this chapter, and you may see the other terms
    used in discussions of this topic as well. Anomaly detection can be the end goal
    of an analysis or a step within a broader analysis project.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*异常* 是指与同一群体的其他成员不同的东西。在数据中，异常是一条记录、一个观察结果或一个数值，其与其余数据点不同，从而引起关注或怀疑。异常有很多不同的名称，包括
    *离群值*、*新奇值*、*噪声*、*偏差* 和 *异常值* 等等。在本章节中，我将交替使用 *异常* 和 *离群值* 这两个术语，你可能也会在讨论中看到其他术语。异常检测可以是分析的最终目标，也可以是更广泛分析项目中的一步。'
- en: 'Anomalies typically have one of two sources: real events that are extreme or
    otherwise unusual, or errors introduced during data collection or processing.
    While many of the steps used to detect outliers are the same regardless of the
    source, how we choose to handle a particular anomaly depends on the root cause.
    As a result, understanding the root cause and distinguishing between the two types
    of causes is important to the analysis process.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 异常通常有两种来源：真实事件中的极端或其他异常，或者在数据收集或处理过程中引入的错误。尽管检测离群值的许多步骤不论根源如何都是相同的，但我们选择如何处理特定异常取决于根本原因。因此，理解根本原因并区分这两种原因对分析过程非常重要。
- en: Real events can generate outliers for a variety of reasons. Anomalous data can
    signal fraud, network intrusion, structural defects in a product, loopholes in
    policies, or product use that wasn’t intended or envisioned by the developers.
    Anomaly detection is widely used to root out financial fraud, and cybersecurity
    also makes use of this type of analysis. Sometimes anomalous data results not
    because a bad actor is trying to exploit a system but because a customer is using
    a product in an unexpected way. For example, I knew someone who used a fitness-tracking
    app, which was intended for running, cycling, walking, and similar activities,
    to record data from his outings at the auto race track. He hadn’t found a better
    option and wasn’t thinking about how anomalous the speed and distance values for
    a car on a track are compared to those recorded for bike rides or running. When
    anomalies can be tracked to a real process, deciding what to do with them requires
    a good understanding of the analysis to be done, as well as domain knowledge,
    terms of use, and sometimes the legal system that governs the product.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 真实事件可能因多种原因产生离群值。异常数据可以标志欺诈、网络入侵、产品结构缺陷、政策漏洞，或者产品使用并非开发人员预期或设想的方式。异常检测广泛用于查找金融欺诈，网络安全也利用这种类型的分析。有时异常数据的产生并非因为有人试图利用系统，而是因为客户以意想不到的方式使用产品。例如，我认识一个人，他使用健身跟踪应用来记录他在赛车场的行程，尽管该应用是为跑步、骑行、步行等活动而设计的。他当时没有找到更好的选项，也没有考虑到在赛车场上记录的速度和距离值与自行车骑行或跑步记录相比有多么异常。当可以追踪到真实过程的异常时，决定如何处理这些异常需要对分析过程、领域知识、使用条款以及有时管理产品的法律制度有很好的理解。
- en: Data can also contain anomalies because of errors in collection or processing.
    Manually entered data is notorious for typos and incorrect data. Changes to forms,
    fields, or validation rules can introduce unexpected values, including nulls.
    Behavior tracking of web and mobile applications is common; however, any change
    to how and when this logging is done can introduce anomalies. I’ve spent enough
    hours diagnosing changes in metrics that I’ve learned to ask up front whether
    any logging was recently changed. Data processing can introduce outliers when
    some values are filtered erroneously, processing steps fail to complete, or data
    is loaded multiple times, creating duplicates. When anomalies result from data
    processing, we can generally be more confident in correcting or discarding those
    values. Of course, fixing the upstream data entry or processing is always a good
    idea, if possible, to prevent future quality problems.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 数据也可能因为收集或处理中的错误而包含异常。手动输入的数据以错字和不正确的数据而臭名昭著。表单、字段或验证规则的更改可能引入意外值，包括空值。常见的行为追踪网络和移动应用程序，但是任何关于何时如何进行记录的更改都可能引入异常。我花了足够多的时间诊断指标变化，我已经学会了事先询问是否最近更改了任何记录。数据处理可能会因为一些值被错误地过滤、处理步骤未能完成，或者数据被多次加载而产生重复而引入异常值。当异常来自数据处理时，我们通常可以更有信心地纠正或丢弃这些值。当然，如果可能的话，修复上游数据输入或处理总是个好主意，以防止未来的质量问题。
- en: In this chapter, I’ll first discuss some of the reasons to use SQL for this
    type of analysis and places in which it falls short. Then I’ll introduce the earthquakes
    data set that will be used in the examples in the rest of the chapter. After that,
    I’ll introduce the basic tools that we have at our disposal in SQL for detecting
    outliers. Then I’ll discuss the various forms of outliers that we can apply the
    tools to find. Once we’ve detected and understood anomalies, the next step is
    to decide what to do with them. Anomalies need not always be problematic, as they
    are in fraud detection, cyberattack detection, and health system monitoring. The
    techniques in this chapter can also be used to detect unusually good customers
    or marketing campaigns, or positive shifts in customer behavior. Sometimes the
    goal of anomaly detection is to pass the anomalies on to other humans or machines
    to deal with, but often this is a step in a wider analysis, so I’ll wrap up with
    various options for correcting anomalies.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我首先讨论了使用 SQL 进行此类分析的一些原因及其局限性。然后，我会介绍地震数据集，该数据集将用于本章其余部分的示例。接着，我将介绍我们在
    SQL 中用于检测异常值的基本工具。然后我会讨论我们可以应用这些工具来找到的各种异常形式。一旦我们检测并理解了异常，下一步就是决定如何处理它们。异常并不总是问题，比如在欺诈检测、网络攻击检测和健康系统监测中。本章的技术也可以用于检测异常好的客户或者市场营销活动，或者客户行为的积极变化。有时异常检测的目标是将异常传递给其他人或机器处理，但通常这只是更广泛分析的一步，所以最后我会总结各种纠正异常的选项。
- en: Capabilities and Limits of SQL for Anomaly Detection
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: SQL 对异常检测的能力和限制
- en: SQL is a versatile and powerful language for many data analysis tasks, though
    it can’t do everything. When performing anomaly detection, SQL has a number of
    strengths, as well as some drawbacks that make other languages or tools better
    choices for some tasks.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: SQL 是一个多才多艺、强大的语言，适用于许多数据分析任务，尽管它不能做到所有事情。在执行异常检测时，SQL 有许多优点，也有一些缺点，使得其他语言或工具对某些任务更合适。
- en: SQL is worth considering when the data set is already in a database, as we previously
    saw with time series and text analysis in Chapters [3](ch03.xhtml#time_series_analysis)
    and [5](ch05.xhtml#text_analysis), respectively. SQL leverages the computational
    power of the database to perform calculations over many records quickly. Particularly
    with large tables of data, transferring out of a database and into another tool
    is time consuming. Working within a database makes even more sense when anomaly
    detection is a step in a larger analysis that will be done in SQL. Code written
    in SQL can be examined to understand why particular records were flagged as outliers,
    and SQL will remain consistent over time even as the data flowing into a database
    changes.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据集已经在数据库中时，SQL是值得考虑的选项，正如我们之前在第 [3](ch03.xhtml#time_series_analysis) 和 [5](ch05.xhtml#text_analysis)
    章节中分别看到的时间序列和文本分析。SQL利用数据库的计算能力快速执行大量记录的计算。特别是对于大型数据表，将数据从数据库转移到另一个工具是耗时的。在SQL中进行异常检测作为更大分析步骤的一部分是有意义的。通过分析SQL中编写的代码，可以理解为什么特定记录被标记为异常值，并且即使数据流入数据库发生变化，SQL也能保持一致。
- en: On the negative side, SQL does not have the statistical sophistication that
    is available in packages developed for languages like R and Python. SQL has several
    standard statistical functions, but additional, more complex statistical calculations
    may be too slow or intense for some databases. For use cases requiring very rapid
    response, such as fraud or intrusion detection, analyzing data in a database may
    simply not be appropriate, since there is often lag in loading data, particularly
    to analytics databases. A common workflow is to use SQL to do the initial analysis
    and determine typical minimum, maximum, and average values and then develop more
    real-time monitoring using a streaming service or special real-time data stores.
    Detecting types of outlier patterns and then implementing in streaming services
    or special real-time data stores can be an option, however. Finally, SQL code
    is rule based, as we saw in [Chapter 5](ch05.xhtml#text_analysis). It is very
    good for handling a known set of conditions or criteria, but SQL will not automatically
    adjust for the types of changing patterns seen with rapidly changing adversaries.
    Machine learning approaches, and the languages associated with them, are often
    a better choice for these applications.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，SQL并没有像R和Python等语言开发的软件包中那样丰富的统计学功能。SQL有一些标准的统计函数，但对于一些数据库来说，额外的复杂统计计算可能会太慢或者太复杂。对于需要非常快速响应的使用场景，比如欺诈或入侵检测，分析数据库中的数据可能不合适，因为加载数据通常会有延迟，特别是对于分析数据库而言。一种常见的工作流是使用SQL进行初始分析，并确定典型的最小值、最大值和平均值，然后使用流式服务或特殊的实时数据存储开发更实时的监控。然而，检测异常模式类型，然后在流式服务或特殊的实时数据存储中实施，也是一个选择。最后，SQL代码是基于规则的，正如我们在
    [第5章](ch05.xhtml#text_analysis) 中看到的。它非常适合处理已知的一组条件或标准，但SQL不会自动调整以适应快速变化的对手所见到的变化模式类型。对于这些应用程序，机器学习方法及其关联的语言通常是更好的选择。
- en: Now that we’ve discussed the advantages of SQL and when to use it instead of
    another language or tool, let’s take a look at the data we’ll be using for examples
    in this chapter before moving on to the code itself.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经讨论了SQL的优点以及何时使用它而不是另一种语言或工具，让我们先看一下本章节示例中将使用的数据，然后再进入代码本身。
- en: The Data Set
  id: totrans-11
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据集
- en: The data for the examples in this chapter is a set of records for all earthquakes
    recorded by the US Geological Survey (USGS) from 2010 to 2020\. The USGS provides
    the data in a number of formats, including real-time feeds, at *[*https://earthquake.usgs.gov/earthquakes/feed*](https://earthquake.usgs.gov/earthquakes/feed)*.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 本章示例的数据是由美国地质调查局（USGS）从2010年到2020年记录的所有地震事件组成的一组记录。USGS提供多种格式的数据，包括实时数据源，网址为*[*https://earthquake.usgs.gov/earthquakes/feed*](https://earthquake.usgs.gov/earthquakes/feed)*。
- en: The data set contains approximately 1.5 million records. Each record represents
    a single earthquake event and includes information such as the timestamp, location,
    magnitude, depth, and source of the information. A sample of the data is shown
    in [Figure 6-1](#sample_of_the_earthquakes_data). A full [data dictionary](https://oreil.ly/NjgCt)
    is available on the USGS site.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含约150万条记录。每条记录代表一个单独的地震事件，包括时间戳、位置、震级、深度以及信息来源等信息。数据的示例显示在 [图6-1](#sample_of_the_earthquakes_data)
    中。美国地质调查局网站上提供了完整的 [数据字典](https://oreil.ly/NjgCt)。
- en: '![](Images/sfda_0601.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0601.png)'
- en: Figure 6-1\. Sample of the `earthquakes` data
  id: totrans-15
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-1\. `earthquakes`数据样本
- en: Earthquakes are caused by sudden slips along faults in the tectonic plates that
    exist on the outer surface of the earth. Locations on the edges of these plates
    experience many more, and more dramatic, earthquakes than other places. The so-called
    Ring of Fire is a region along the rim of the Pacific Ocean in which many earthquakes
    occur. Various locations within this region, including California, Alaska, Japan,
    and Indonesia, will appear frequently in our analysis.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 地震是由地球表面上存在的构造板块之间的断层突然滑动引起的。这些板块边缘的位置经历比其他地方更多且更剧烈的地震。所谓的“火环”是沿太平洋边缘的一个地区，这个地区发生了许多地震。该地区内的各个位置，包括加利福尼亚州、阿拉斯加州、日本和印度尼西亚，将在我们的分析中频繁出现。
- en: '*Magnitude* is a measure of the size of an earthquake at its source, as measured
    by its seismic waves. Magnitude is recorded on a logarithmic scale, meaning that
    the amplitude of a magnitude 5 earthquake is 10 times that of a magnitude 4 earthquake.
    The actual measurement of earthquakes is fascinating but beyond the scope of this
    book. The [USGS website](https://earthquake.usgs.gov) is a good place to start
    if you want to learn more.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: '*震级*是地震在其源头的大小测量，由其地震波测量。震级记录在对数尺度上，这意味着震级5的地震振幅是震级4地震的10倍。地震的实际测量是迷人的，但超出了本书的范围。如果您想了解更多信息，可以从[美国地质调查局（USGS）网站](https://earthquake.usgs.gov)开始。'
- en: Detecting Outliers
  id: totrans-18
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 检测异常值
- en: Although the idea of an anomaly or outlier—a data point that is very different
    from the rest—seems straightforward, actually finding one in any particular data
    set poses some challenges. The first challenge has to do with knowing when a value
    or data point is common or rare, and the second is setting a threshold for marking
    values on either side of this dividing line. As we go through the `earthquakes`
    data, we’ll profile the depths and magnitudes in order to develop an understanding
    of which values are normal and which are unusual.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管异常值或异常值——与其他数据极为不同的数据点的概念似乎很简单，但实际在任何特定数据集中找到一个的挑战很大。第一个挑战涉及知道何时一个值或数据点是常见的或罕见的，第二个是设置标记这条分界线两侧值的阈值。当我们浏览`earthquakes`数据时，我们将分析深度和震级，以便了解哪些值是正常的，哪些是不寻常的。
- en: Generally, the larger or more complete the data set, the easier it is to make
    a judgment on what is truly anomalous. In some instances, we have labeled or “ground
    truth” values to which we can refer. A label is generally a column in the data
    set that indicates whether the record is normal or an outlier. Ground truth can
    be obtained from industry or scientific sources or from past analysis and might
    tell us, for example, that any earthquake greater than magnitude 7 is an anomaly.
    In other cases, we must look to the data itself and apply reasonable judgment.
    For the remainder of the chapter, we’ll assume that we have a large enough data
    set to do just that, though of course there are outside references we could consult
    on typical and extreme earthquake magnitudes.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 一般来说，数据集越大或越完整，我们就越容易判断什么是真正的异常。在某些情况下，我们有标记或“地面真相”值可供参考。标签通常是数据集中指示记录是正常还是异常的列。地面真相可以从行业或科学来源获得，也可以从过去的分析中获得，例如，告诉我们任何大于7级的地震都是异常情况。在其他情况下，我们必须查看数据本身并应用合理的判断。在本章的其余部分，我们假设我们有足够大的数据集来做到这一点，尽管当然我们可以查阅有关典型和极端地震震级的外部参考资料。
- en: Our tools for detecting outliers using the data set itself fall into a few categories.
    First, we can sort or *ORDER BY* the values in the data. This can optionally be
    combined with various *GROUP BY* clauses to find outliers by frequency. Second,
    we can use SQL’s statistical functions to find extreme values at either end of
    a value range. Finally, we can graph data and inspect it visually.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用数据集本身来检测异常值的工具分为几类。首先，我们可以对数据中的值进行排序或*ORDER BY*。这可以选择性地与各种*GROUP BY*子句结合使用，以按频率查找异常值。其次，我们可以使用SQL的统计函数来查找值范围两端的极端值。最后，我们可以绘制数据并进行视觉检查。
- en: Sorting to Find Anomalies
  id: totrans-22
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用排序查找异常值
- en: One of the basic tools we have for finding outliers is sorting the data, accomplished
    with the *ORDER BY* clause. The default behavior of *ORDER BY* is to sort ascending
    (*ASC*). To sort in descending order, add *DESC* after the column. An *ORDER BY*
    clause can include one or more columns, and each column can be sorted ascending
    or descending, independently of the others. Sorting starts with the first column
    specified. If a second column is specified, the results of the first sort are
    then sorted by the second column (retaining the first sort), and so on through
    all the columns in the clause.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 找出异常值的基本工具之一是对数据进行排序，可以通过*ORDER BY*子句完成。*ORDER BY*的默认行为是升序排序（*ASC*）。要按降序排序，请在列名后添加*DESC*。*ORDER
    BY*子句可以包括一个或多个列，并且每个列可以独立于其他列按升序或降序排序。排序从指定的第一列开始。如果指定了第二列，则第一次排序的结果将按第二列进行排序（保留第一次排序），依此类推，直到子句中的所有列都排序完毕。
- en: Tip
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Since ordering happens after the database has calculated the rest of the query,
    many databases allow you to reference the query columns by number instead of by
    name. SQL Server is an exception; it requires the full name. I prefer the numbering
    syntax because it results in more compact code, particularly when query columns
    include lengthy calculations or function syntax.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 由于排序发生在数据库计算查询的其余部分之后，许多数据库允许您按列号而不是按名称引用查询列。SQL Server是一个例外；它需要完整的名称。我更喜欢编号语法，因为它可以生成更紧凑的代码，特别是当查询列包含冗长的计算或函数语法时。
- en: 'For example, we can sort the `earthquakes` table by `mag,` the magnitude:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以按`earthquakes`表中的`mag`（震级）排序：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'This returns a number of rows of nulls. Let’s make a note that the data set
    can contain null values for magnitude—a possible outlier in itself. We can exclude
    the null values:'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 这将返回若干行空值。让我们注意，数据集可能包含幅度的空值——这本身可能就是一个异常值。我们可以排除空值：
- en: '[PRE1]'
  id: totrans-29
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: There is only one value greater than 9, and there are only two additional values
    greater than 8.5\. In many contexts, these would not appear to be particularly
    large values. However, with a little domain knowledge about earthquakes, we can
    recognize that these values are in fact both very large and unusual. The USGS
    provides a list of the [20 largest earthquakes in the world](https://oreil.ly/gHUhy).
    All of them are magnitude 8.4 or larger, while only five are magnitude 9.0 or
    larger, and three occurred between 2010 and 2020, the time period covered by our
    data set.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个大于9的值，而且只有另外两个大于8.5的值。在许多情境中，这些看起来可能并不是特别大的值。然而，通过一点关于地震的领域知识，我们可以认识到这些值实际上都非常大且不寻常。USGS提供了一份世界上[最大20次地震的列表](https://oreil.ly/gHUhy)。其中所有地震的震级都大于8.4，而只有五次地震的震级大于9.0，其中三次发生在我们数据集涵盖的2010至2020年期间。
- en: 'Another way to consider whether values are anomalies within a data set is to
    calculate their frequency. We can `count` the `id` field and *GROUP BY* the `mag`
    to find the number of earthquakes per magnitude. The number of earthquakes per
    magnitude is then divided by the total number of earthquakes, which can be found
    using a `sum` window function. All window functions require an *OVER* clause with
    a *PARTITION BY* and/or *ORDER BY* clause. Since the denominator should count
    all the records, I have added a *PARTITION BY* 1, which is a way to force the
    database to make it a window function but still read from the entire table. Finally,
    the result set is *ORDER*ed *BY* the magnitude:'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种考虑值是否在数据集中异常的方法是计算它们的频率。我们可以统计`id`字段并按`mag`分组，以找出每个震级的地震数量。然后将每个震级的地震数量除以使用`sum`窗口函数找到的总地震数量。所有窗口函数都需要*PARTITION
    BY*和/或*ORDER BY*子句的*OVER*子句。由于分母应计算所有记录，我添加了*PARTITION BY* 1，这是一种强制数据库使其成为窗口函数但仍从整个表中读取的方法。最后，结果集按震级*ORDER*
    BY排序：
- en: '[PRE2]'
  id: totrans-32
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'There is only one each of the earthquakes that are over 8.5 in magnitude, but
    there are two that registered 8.3\. By the value 6.9, there are double digits
    of earthquakes, but those still represent a very small percentage of the data.
    In our investigation, we should also check the other end of the sorting, the smallest
    values, by sorting ascending instead of descending:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个地震震级超过8.5，但有两次注册了8.3。到6.9这个数值时，地震次数已经有两位数了，但这仍然只代表数据的非常小的百分比。在我们的调查中，我们还应检查排序的另一端，即最小值，通过升序而不是降序排序：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: At the low end of values, –9.99 and –9 occur more frequently than we might expect.
    Although we can’t take the logarithm of zero or a negative number, a logarithm
    can be negative when the argument is greater than zero and less than one. For
    example, log(0.5) is equal to approximately –0.301\. The values –9.99 and –9 represent
    extremely small earthquake magnitudes, and we might question whether such small
    quakes could really be detected. Given the frequency of these values, I suspect
    they represent an unknown value rather than a truly tiny earthquake, and thus
    we may consider them anomalies.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在值的低端，–9.99和–9出现的频率比我们预期的要高。尽管我们不能取零或负数的对数，当参数大于零且小于一时，对数可以为负。例如，log(0.5)约等于–0.301。值–9.99和–9代表极小的地震震级，我们可能怀疑这样小的地震是否真的可以被检测到。考虑到这些值的频率，我怀疑它们代表一个未知值，而不是一个真正微小的地震，因此我们可以认为它们是异常值。
- en: 'In addition to sorting the overall data, it can be useful to *GROUP BY* one
    or more attribute fields to find anomalies within subsets of the data. For example,
    we might want to check the highest and lowest magnitudes recorded for specific
    geographies in the `place` field:'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对整体数据进行排序外，还可以通过*GROUP BY*一个或多个属性字段来查找数据子集内的异常。例如，我们可能希望检查在`place`字段中特定地理区域记录的最高和最低震级：
- en: '[PRE4]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: “Northern California” is the most common `place` in the data set, and inspecting
    just the subset for it, we can see that the high and low values are not nearly
    as extreme as those for the data set as a whole. Earthquakes over 5.0 magnitude
    are not uncommon overall, but they are outliers for “Northern California.”
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: “北加利福尼亚”是数据集中最常见的`place`，仅检查其子集时，我们可以看到其高低值远不及整个数据集那么极端。整体而言，5.0级以上的地震并不罕见，但对于“北加利福尼亚”来说，它们是异常值。
- en: Calculating Percentiles and Standard Deviations to Find Anomalies
  id: totrans-39
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 计算百分位数和标准偏差以发现异常
- en: 'Sorting and optionally grouping data and then reviewing the results visually
    is a useful approach for spotting anomalies, particularly when the data has values
    that are very extreme. Without domain knowledge, however, it might not be obvious
    that a 9.0 magnitude earthquake is such an anomaly. Quantifying the extremity
    of data points adds another layer of rigor to the analysis. There are two ways
    to do this: with percentiles or with standard deviations.'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 对数据进行排序并选择性地分组，然后在视觉上审查结果是发现异常的有用方法，特别是当数据具有非常极端值时。然而，如果没有领域知识，一个9.0级的地震是这样的异常可能并不明显。量化数据点的极端性为分析增加了另一层严谨性。有两种方法可以做到这一点：使用百分位数或标准偏差。
- en: Percentiles represent the proportion of values in a distribution that are less
    than a particular value. The median of a distribution is the value at which half
    of the population has a lower value and half has a higher value. The median is
    so commonly used that it has its own SQL function, `median`, in many but not all
    databases. Other percentiles can be calculated as well. For example, we can find
    the 25th percentile, where 25% of the values are lower and 75% are higher, or
    the 89th percentile, where 89% of values are lower and 11% are higher. Percentiles
    are often found in academic contexts, such as standardized testing, but they can
    be applied to any domain.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 百分位数表示在分布中小于特定值的值的比例。分布的中位数是一个值，其中一半的人口有较低的值，另一半有较高的值。中位数如此常用，以至于它在许多但不是所有数据库中都有自己的SQL函数`median`。还可以计算其他百分位数。例如，我们可以找到第25百分位数，其中25%的值较低，75%较高，或者第89百分位数，其中89%的值较低，11%较高。百分位数通常在学术背景下发现，例如标准化测试，但可以应用于任何领域。
- en: 'SQL has a window function, `percent_rank`, that returns the percentile for
    each row within a partition. As with all window functions, the sorting direction
    is controlled with an *ORDER BY* statement. Similar to the `rank` function, `percent_rank`
    does not take any argument; it operates over all the rows returned by the query.
    The basic form is:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SQL有一个窗口函数，`percent_rank`，它返回分区内每行的百分位数。与所有窗口函数一样，排序方向由*ORDER BY*语句控制。类似于`rank`函数，`percent_rank`不接受任何参数；它适用于查询返回的所有行。基本形式如下：
- en: '[PRE5]'
  id: totrans-43
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'Both the *PARTITION BY* and the *ORDER BY* are optional, but the function requires
    something in the *OVER* clause, and specifying the ordering is always a good idea.
    To find the percentile of the magnitudes of each earthquake for each place, we
    can first calculate the `percent_rank` for each row in the subquery and then count
    the occurrences of each magnitude in the outer query. Note that it’s important
    to calculate the `percent_rank` first, before doing any aggregation, so that repeating
    values are taken into account in the calculation:'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*PARTITION BY* 和 *ORDER BY* 都是可选的，但函数需要*OVER*子句中的内容，并且指定排序总是一个好主意。要找到每个地震的每个地点的震级的百分位数，我们可以首先在子查询中为每一行计算`percent_rank`，然后在外部查询中计算每个震级的出现次数。注意，首先计算`percent_rank`非常重要，在进行任何聚合之前，以便在计算中考虑重复值：'
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Within Northern California, the magnitude 5.6 earthquake has a percentile of
    1, or 100%, indicating that all of the other values are less than this one. The
    magnitude –1.6 earthquake has a percentile of 0, indicating that no other data
    points are smaller.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 在北加州地区，震级为5.6的地震的百分位数为1，即100%，表示所有其他数值均小于此值。震级为-1.6的地震的百分位数为0，表示没有比这个更小的数据点。
- en: 'In addition to finding the exact percentile of each row, SQL can carve the
    data set into a specified number of buckets and return the bucket each row belongs
    to with a function called `ntile`. For example, we might want to carve the data
    set up into 100 buckets:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 除了找到每行的确切百分位数外，SQL还可以将数据集划分为指定数量的桶，并返回每行所属的桶，使用称为`ntile`的函数。例如，我们可能希望将数据集分成100个桶：
- en: '[PRE7]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Looking at the results for “Central Alaska,” we see that the three earthquakes
    greater than 5 are in the 100th percentile, 1.5 falls within the 79th percentile,
    and the smallest values of –0.5 fall in the first percentile. After calculating
    these values, we can then find the boundaries of each ntile, using `max` and `min`.
    For this example, we’ll use four ntiles to keep the display simpler, but any positive
    integer is allowed in the `ntile` argument:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 查看“中央阿拉斯加”的结果时，我们发现大于5的三个地震处于100百分位数，1.5位于79百分位数，最小值-0.5位于第一百分位数。计算这些值后，我们可以使用`max`和`min`找到每个ntile的边界。对于这个例子，我们将使用四个ntile来简化显示，但`ntile`参数允许任何正整数：
- en: '[PRE8]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The highest ntile, 4, which represents the 75th to 100th percentiles, has the
    widest range, spanning from 1.4 to 5.4\. On the other hand, the middle 50 percent
    of values, which include ntiles 2 and 3, range only from 0.8 to 1.4.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 表示75到100百分位数的最高ntile为4，具有最宽的范围，从1.4到5.4。另一方面，包括ntile 2和3的中间50%的值范围仅从0.8到1.4。
- en: 'In addition to finding the percentile or ntile for each row, we can calculate
    specific percentiles across the entire result set of a query. To do this, we can
    use the `percentile_cont` function or the `percentile_disc` function. Both are
    window functions, but with a slightly different syntax than other window functions
    discussed previously because they require a *WITHIN GROUP* clause. The form of
    the functions is:'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 除了为每一行找到百分位数或ntile之外，我们还可以计算查询结果集中特定百分位数。为此，我们可以使用`percentile_cont`函数或`percentile_disc`函数。两者都是窗口函数，但与先前讨论的其他窗口函数语法略有不同，因为它们需要一个*WITHIN
    GROUP*子句。函数的形式是：
- en: '[PRE9]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The numeric is a value between 0 and 1 that represents the percentile to return.
    For example, 0.25 returns the 25th percentile. The *ORDER BY* clause specifies
    the field to return the percentile from, as well as the ordering. *ASC* or *DESC*
    can optionally be added, with *ASC* the default, as in all *ORDER BY* clauses
    in SQL. The *OVER* (*PARTITION BY*...) clause is optional (and confusingly, some
    databases don’t support it, so check your documentation if you run into errors).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 数字是一个介于0和1之间的值，表示要返回的百分位数。例如，0.25返回第25百分位数。*ORDER BY* 子句指定要从中返回百分位数的字段，以及排序方式。*ASC*
    或 *DESC* 可以选择性地添加，*ASC* 是默认值，就像SQL中所有*ORDER BY* 子句一样。*OVER* (*PARTITION BY*...)
    子句是可选的（令人困惑的是，一些数据库不支持它，请在遇到错误时查看您的文档）。
- en: 'The `percentile_cont` function will return an interpolated (calculated) value
    that corresponds to the exact percentile but that may not exist in the data set.
    The `percentile_disc` (discontinuous percentile) function, on the other hand,
    returns the value in the data set that is closest to the requested percentile.
    For large data sets, or for ones with fairly continuous values, there is often
    little practical difference between the output of the two functions, but it’s
    worth considering which is more appropriate for your analysis. Let’s take a look
    at an example to see how this looks in practice. We’ll calculate the 25th, 50th
    (or median), and 75th percentile magnitudes for all nonnull magnitudes in Central
    Alaska:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: '`percentile_cont`函数将返回一个插值（计算）值，该值对应于精确的百分位数，但在数据集中可能不存在。另一方面，`percentile_disc`（不连续百分位数）函数返回数据集中最接近请求百分位数的值。对于大数据集或具有相当连续值的数据集，这两个函数的输出通常几乎没有实际差异，但考虑哪个更适合您的分析是值得的。让我们看一个示例，看看这在实践中是什么样子。我们将计算中阿拉斯加所有非空大小的第25、50（或中位数）和75百分位数大小：'
- en: '[PRE10]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The query returns the requested percentiles, summarized across the data set.
    Notice that the values correspond to the maximum values for ntiles 1, 2, and 3
    calculated in the previous example. Percentiles for different fields can be calculated
    within the same query by changing the field in the *ORDER BY* clause:'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 查询返回请求的百分位数，总结在数据集中。请注意，这些值对应于在上一个例子中计算的ntiles 1、2和3的最大值。可以通过在*ORDER BY*子句中更改字段来计算同一查询中不同字段的百分位数：
- en: '[PRE11]'
  id: totrans-58
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Unlike other window functions, `percentile_cont` and `percentile_disc` require
    a *GROUP BY* clause at the query level when other fields are present in the query.
    For example, if we want to consider two areas within Alaska, and so include the
    `place` field, the query must also include it in the *GROUP BY*, and the percentiles
    are calculated per `place`:'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 与其他窗口函数不同，当查询中存在其他字段时，`percentile_cont`和`percentile_disc`函数在查询级别需要*GROUP BY*子句。例如，如果我们想要考虑阿拉斯加内的两个地区，因此包括`place`字段，查询还必须在*GROUP
    BY*中包括它，并且百分位数是按`place`计算的：
- en: '[PRE12]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: With these functions, we can find any percentile required for analysis. Since
    the median value is so commonly calculated, a number of databases have implemented
    a `median` function that has only one argument, the field for which to calculate
    the median. This is a handy and certainly much simpler syntax, but note that the
    same can be accomplished with `percentile_cont` if a `median` function is not
    available.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些函数，我们可以找到分析所需的任何百分位数。由于中位数值通常被计算，许多数据库实现了一个只有一个参数的`median`函数，用于计算中位数的字段。这是一种方便且明显简单得多的语法，但请注意，如果`median`函数不可用，也可以使用`percentile_cont`实现同样的功能。
- en: Tip
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: The `percentile` and `median` functions can be slow and computationally intensive
    on large data sets. This is because the database must sort and rank all the records,
    usually in memory. Some database vendors have implemented approximate versions
    of the functions, such as `approximate_percentile`, that are much faster and return
    results very close to the function that calculates the entire data set.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: '`percentile`和`median`函数在大数据集上可能速度较慢且计算密集。这是因为数据库通常必须在内存中对所有记录进行排序和排名。一些数据库供应商已实现了近似版本的函数，例如`approximate_percentile`，它们要快得多，并返回与计算整个数据集的函数非常接近的结果。'
- en: Finding the percentiles or ntiles of a data set allows us to add some quantification
    to anomalies. We’ll see later in the chapter how these values also give us some
    tools for handling anomalies in data sets. Since percentiles are always scaled
    between 0 and 100, however, they don’t give a sense of just how unusual certain
    values are. For that we can turn to additional statistical functions supported
    by SQL.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 找到数据集的百分位数或ntiles可以使我们对异常添加一些量化。我们将在本章后面看到，这些值还为我们处理数据集中的异常提供了一些工具。然而，由于百分位数始终在0和100之间缩放，它们并不能表达某些值有多么不寻常。为此，我们可以转向SQL支持的额外统计函数。
- en: 'To measure how extreme values in a data set are, we can use the *standard deviation*.
    The standard deviation is a measure of the variation in a set of values. A lower
    value means less variation, while a higher number means more variation. When data
    is normally distributed around the mean, about 68% of the values lie within +/–
    one standard deviation from the mean, and about 95% lie within two standard deviations.
    The standard deviation is calculated as the square root of the sum of differences
    from the mean, divided by the number of observations:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了衡量数据集中极端值的程度，我们可以使用*标准偏差*。标准偏差是一组值变化程度的度量。较低的值表示变化较小，而较高的数字表示变化较大。当数据围绕均值正态分布时，大约
    68% 的值位于距均值的一标准偏差范围内，约 95% 位于两个标准偏差内。标准偏差计算公式为从均值差异总和的平方根除以观察数：
- en: <math alttext="StartRoot sigma-summation left-parenthesis x Subscript i Baseline
    minus mu right-parenthesis squared slash upper N EndRoot"><msqrt><mrow><mo>∑</mo>
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>/</mo> <mi>N</mi></mrow></msqrt></math>
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: <math alttext="StartRoot sigma-summation left-parenthesis x Subscript i Baseline
    minus mu right-parenthesis squared slash upper N EndRoot"><msqrt><mrow><mo>∑</mo>
    <msup><mrow><mo>(</mo><msub><mi>x</mi> <mi>i</mi></msub> <mo>-</mo><mi>μ</mi><mo>)</mo></mrow>
    <mn>2</mn></msup> <mo>/</mo> <mi>N</mi></mrow></msqrt></math>
- en: In this formula, *x[i]* is an observation, μ is the average of all the observations,
    ∑ indicates that all of the values should be summed, and *N* is the number of
    observations. Refer to any good statistics text or online resource^([1](ch06.xhtml#ch01fn8))
    for more information about how the standard deviation is derived.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个公式中，*x[i]* 是一个观察值，μ 是所有观察值的平均值，∑ 表示应对所有值求和，*N* 是观察值的数量。有关标准偏差如何推导的更多信息，请参考任何良好的统计文本或在线资源^([1](ch06.xhtml#ch01fn8))。
- en: 'Most databases have three standard deviation functions. The `stddev_pop` function
    finds the standard deviation of a population. If the data set represents the entire
    population, as is often the case with a customer data set, use the `stddev_pop`.
    The `stddev_samp` finds the standard deviation of a sample and differs from the
    above formula by dividing by *N* – 1 instead of *N*. This has the effect of increasing
    the standard deviation, reflecting the loss of accuracy when only a sample of
    the entire population is used. The `stddev` function available in many databases
    is identical to the `stddev_samp` function and may be used simply because it is
    shorter. If you’re working with data that is a sample, such as from a survey or
    study from a larger population, use the `stddev_samp` or `stddev`. In practice,
    when you are working with large data sets, there is usually little difference
    between the `stddev_pop` and `stddev_samp` results. For example, across the 1.5
    million records in the `earthquakes` table, the values diverge only after five
    decimal places:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数数据库都有三个标准偏差函数。`stddev_pop` 函数用于找出总体的标准偏差。如果数据集代表整个人口，通常使用 `stddev_pop`。`stddev_samp`
    找出样本的标准偏差，与上述公式不同之处在于除以 *N* - 1 而不是 *N*。这会增加标准偏差，反映仅使用整体人口的样本时的精度损失。在许多数据库中可用的
    `stddev` 函数与 `stddev_samp` 函数相同，可以简单地使用它因为长度较短。如果你处理的是来自调查或研究的样本数据，例如较大人口中的样本数据，应使用
    `stddev_samp` 或 `stddev`。实际上，在处理大数据集时，`stddev_pop` 和 `stddev_samp` 的结果通常几乎没有差异。例如，在
    `earthquakes` 表的 150 万记录中，仅在小数点后五位数值有所不同：
- en: '[PRE13]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: These differences are small enough that in most practical applications, it doesn’t
    matter which standard deviation function you use.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 这些差异很小，在大多数实际应用中，使用哪个标准偏差函数并不重要。
- en: With this function, we can now calculate the number of standard deviations from
    the mean for each value in the data set. This value is known as the *z-score*
    and is a way of standardizing data. Values that are above the average have a positive
    z-score, and those below the average have a negative z-score. [Figure 6-2](#standard_deviations_and_z_scores_for_a)
    shows how z-scores and standard deviations relate to the normal distribution.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此函数，我们现在可以计算数据集中每个值相对于均值的标准偏差数。这个值称为*z-分数*，是标准化数据的一种方式。高于平均值的值有正的 z-分数，低于平均值的值有负的
    z-分数。[图 6-2](#standard_deviations_and_z_scores_for_a)显示了 z-分数和标准偏差与正态分布的关系。
- en: '![](Images/sfda_0602.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0602.png)'
- en: Figure 6-2\. Standard deviations and z-scores for a normal distribution
  id: totrans-73
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 正态分布的标准偏差和 z 分数
- en: To find the z-scores for the earthquakes, first calculate the average and standard
    deviation for the entire data set in a subquery. Then *JOIN* this back to the
    data set using a Cartesian *JOIN*, so that the average and standard deviation
    values are *JOIN*ed to each earthquake row. This is accomplished with the `1 =
    1` syntax, since most databases require that some *JOIN* condition be specified.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到地震的Z分数，首先在子查询中计算整个数据集的平均值和标准差。然后使用笛卡尔*JOIN*将这些值与数据集*JOIN*回来，以便平均值和标准差值与每个地震行*JOIN*。这是通过`1
    = 1`语法实现的，因为大多数数据库要求指定某些*JOIN*条件。
- en: 'In the outer query, subtract the average magnitude from each individual magnitude
    and then divide by the standard deviation:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在外部查询中，从每个单独的震级中减去平均震级，然后除以标准差：
- en: '[PRE14]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: The largest earthquakes have a z-score of almost 6, whereas the smallest (excluding
    the –9 and –9.99 earthquakes that appear to be data entry anomalies) have z-scores
    close to 3\. We can conclude that the largest earthquakes are more extreme outliers
    than the ones at the low end.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的地震的Z分数接近6，而最小的（排除看起来是数据输入异常的-9和-9.99地震）的Z分数接近3。我们可以得出结论，最大的地震比低端的那些更为极端的异常值。
- en: Graphing to Find Anomalies Visually
  id: totrans-78
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 用图表直观地查找异常值
- en: In addition to sorting the data and calculating percentiles and standard deviations
    to find anomalies, visualizing the data in one of several graph formats can also
    help in finding anomalies. As we’ve seen in previous chapters, one strength of
    graphs is their ability to summarize and present many data points in a compact
    form. By inspecting graphs, we can often spot patterns and outliers that we might
    otherwise miss if only considering the raw output. Finally, graphs assist in the
    task of describing the data, and any potential problems with the data related
    to anomalies, to other people.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 除了对数据进行排序和计算百分位数和标准差以找到异常值外，将数据可视化为几种图表格式之一也有助于发现异常值。正如我们在前面章节中所见，图表的一个优势是它们能够以紧凑的形式总结和呈现许多数据点。通过检查图表，我们通常可以发现模式和异常值，如果仅考虑原始输出，可能会被忽略。最后，图表有助于描述数据以及与数据相关的任何潜在问题，特别是异常值，向其他人传达信息。
- en: 'In this section, I’ll present three types of graphs that are useful for anomaly
    detection: bar graphs, scatter plots, and box plots. The SQL needed to generate
    output for these graphs is straightforward, though you might need to enlist pivoting
    strategies discussed in previous chapters, depending on the capabilities and limitations
    of the software used to create the graphs. Any major BI tool or spreadsheet software,
    or languages such as Python or R, will be able to produce these graph types. The
    graphs in this section were created using Python with Matplotlib.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将介绍三种对异常检测有用的图表类型：条形图、散点图和箱线图。生成这些图表所需的SQL很简单，尽管根据所用软件的能力和限制，你可能需要使用前面章节讨论过的数据透视策略。任何主要的商业智能工具或电子表格软件，以及诸如Python或R的编程语言，都能够生成这些图表类型。本节中的图表是使用Python和Matplotlib创建的。
- en: The *bar graph* is used to plot a histogram or distribution of the values in
    a field and is useful for both characterizing the data and spotting outliers.
    The full extent of values are plotted along one axis, and the number of occurrences
    of each value is plotted on the other axis. The extreme high and low values are
    interesting, as is the shape of the plot. We can quickly determine whether the
    distribution is approximately normal (symmetric around a peak or average value),
    has another type of distribution, or has peaks at particular values.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图用于绘制字段值的直方图或分布图，并且对于描述数据和发现异常值都很有用。所有值的全范围沿一个轴绘制，每个值的出现次数沿另一个轴绘制。极高和极低的值是有趣的，以及绘图的形状。我们可以快速确定分布是否近似正态（围绕峰值或平均值对称），是否具有其他类型的分布，或者是否在特定值处有峰值。
- en: To graph a histogram for the earthquake magnitudes, first create a data set
    that groups the magnitudes and counts the earthquakes. Then plot the output, as
    in [Figure 6-3](#distribution_of_earthquake_magnitudes).
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 要为地震震级绘制直方图，首先创建一个数据集，分组震级并计算地震数量。然后绘制输出，如[图6-3](#distribution_of_earthquake_magnitudes)所示。
- en: '[PRE15]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '![](Images/sfda_0603.png)'
  id: totrans-84
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0603.png)'
- en: Figure 6-3\. Distribution of earthquake magnitudes
  id: totrans-85
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-3\. 地震震级分布
- en: The graph extends from –10.0 to +10.0, which makes sense given our previous
    exploration of the data. It peaks and is roughly symmetric around a value in the
    range of 1.1 to 1.4 with almost 40,000 earthquakes of each magnitude, but it has
    a second peak of almost 20,000 earthquakes around the value 4.4\. We’ll explore
    the reason for this second peak in the next section on forms of anomalies. The
    extreme values are hard to spot in this graph, however, so we might want to zoom
    in on a subsection of the graph, as in [Figure 6-4](#a_zoomed_in_view_of_the_distribution_of).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 该图的范围从–10.0到+10.0，这在我们之前对数据的探索中是有意义的。它峰值大约在1.1到1.4范围内，每个震级约有40,000次地震，但它在约4.4处有第二个峰值，约有20,000次地震。我们将在下一节关于异常形式的部分探讨这第二个峰值的原因。在这张图中很难看到极端值，因此我们可能希望放大图表的某个部分，如[图 6-4](#a_zoomed_in_view_of_the_distribution_of)：
- en: '![](Images/sfda_0604.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0604.png)'
- en: Figure 6-4\. A zoomed-in view of the distribution of earthquake magnitudes,
    focused on the highest magnitudes
  id: totrans-88
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 地震震级分布的放大视图，聚焦于最高震级
- en: Here the frequencies of these very high-intensity earthquakes are easier to
    see, as is the decrease in frequency from more than 10 to only 1 as the value
    goes from the low 7s to over 8\. Thankfully these temblors are extremely rare.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，这些非常高强度地震的频率更容易看到，从低7到超过8的值从超过10下降到仅为1。幸运的是，这些地震非常罕见。
- en: 'A second type of graph that can be used to characterize data and spot outliers
    is the *scatter plot*. A scatter plot is appropriate when the data set contains
    at least two numeric values of interest. The x-axis displays the range of values
    of the first data field, the y-axis displays the range of values of the second
    data field, and a dot is graphed for every pair of x and y values in the data
    set. For example, we can graph the magnitude against the depth of earthquakes
    in the data set. First, query the data to create a data set of each pair of values.
    Then graph the output, as in [Figure 6-5](#scatter_plot_of_the_magnitude_and_dept):'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种用于描述数据和发现异常值的图表是*散点图*。当数据集包含至少两个感兴趣的数值时，散点图是合适的。x轴显示第一个数据字段的值范围，y轴显示第二个数据字段的值范围，在数据集中的每对x和y值上绘制一个点。例如，我们可以绘制数据集中地震的震级与深度之间的散点图。首先，查询数据以创建每对值的数据集。然后像[图 6-5](#scatter_plot_of_the_magnitude_and_dept)那样绘制输出：
- en: '[PRE16]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '![](Images/sfda_0605.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0605.png)'
- en: Figure 6-5\. Scatter plot of the magnitude and depth of earthquakes
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 震级和深度的散点图
- en: In this graph, we can see the same range of magnitudes, now plotted against
    the depths, which range from just below zero to around 700 kilometers. Interestingly,
    the high depth values, over 300, correspond to magnitudes that are roughly 4 and
    higher. Perhaps such deep earthquakes can be detected only after they reach a
    minimum magnitude. Note that, due to the volume of data, I have taken a shortcut
    and grouped the values by magnitude and depth combination, rather than plotting
    all 1.5 million data points. The count of earthquakes can be used to size each
    circle in the scatter, as in [Figure 6-6](#scatter_plot_of_the_magnitude_and_depth),
    which is zoomed in to the range of magnitudes from 4.0 to 7.0, and depths from
    0 to 50 km.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这张图中，我们可以看到相同幅度的地震，现在根据深度绘制，深度从零以下到大约700公里不等。有趣的是，高深度值超过300对应于大约4以上的震级。也许只有当这些深部地震达到最小震级后才能检测到。请注意，由于数据量大，我采取了一种捷径，将值按照震级和深度组合分组，而不是绘制所有150万数据点。地震的计数可以用来调整散点图中每个圆的大小，例如[图 6-6](#scatter_plot_of_the_magnitude_and_depth)，该图放大了震级从4.0到7.0，深度从0到50公里的范围。
- en: '![](Images/sfda_0606.png)'
  id: totrans-95
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0606.png)'
- en: Figure 6-6\. Scatter plot of the magnitude and depth of earthquakes, zoomed
    in and with circles sized by the number of earthquakes
  id: totrans-96
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. 震级和深度的散点图，放大并且圆圈的大小根据地震数量调整
- en: A third type of graph useful in finding and analyzing outliers is the *box plot*,
    also known as the *box-and-whisker plot*. These graphs summarize data in the middle
    of the range of values while retaining the outliers. The graph type is named for
    the box, or rectangle, in the middle. The line that forms the bottom of the rectangle
    is located at the 25th percentile value, the line that forms the top is located
    at the 75th percentile, and the line through the middle is located at the 50th
    percentile, or median, value. Percentiles should be familiar from our discussion
    in the preceding section. The “whiskers” of the box plot are lines that extend
    out from the box, typically to 1.5 times the *interquartile range*. The interquartile
    range is simply the difference between the 75th percentile value and the 25th
    percentile value. Any values beyond the whiskers are plotted on the graph as outliers.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在查找和分析异常值时非常有用的第三种图表类型是*箱线图*，也称为*箱须图*。这些图表总结了值范围中间的数据，同时保留了异常值。该图表类型以中间的箱子或矩形命名。形成矩形底部的线位于第25百分位值，形成顶部的线位于第75百分位值，而中间的线位于第50百分位值，即中位数。百分位数在前一节中已经讨论过了。箱线图的“盒须”是从箱子伸出的线，通常延伸到*四分位距*的1.5倍。四分位距简单来说是第75百分位值与第25百分位值之间的差值。超出盒须的任何值都会作为异常值绘制在图表上。
- en: Tip
  id: totrans-98
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Whichever software or programming language you use for graphing box plots will
    take care of the calculations of the percentiles and interquartile range. Many
    also offer options to plot the whiskers based on standard deviations from the
    mean, or on wider percentiles such as the 10th and 90th. The calculation will
    always be symmetric around the midpoint (such as one standard deviation above
    and below the mean), but the length of the upper and lower whiskers can differ
    based on the data.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 无论您使用哪种软件或编程语言来绘制箱线图，都会自动处理百分位数和四分位距的计算。许多软件还提供基于标准差（如均值上下的一个标准差）或更广泛百分位（如第10和第90百分位）的选项来绘制盒须。计算始终围绕中点对称（例如均值上下的一个标准差），但上下盒须的长度可能会因数据而异。
- en: 'Typically, all of the values are plotted in a box plot. Since the data set
    is so large, for this example we’ll look at the subset of 16,036 earthquakes that
    include “Japan” in the `place` field. First, create the data set with SQL, which
    is a simple *SELECT* of all of the `mag` values that meet the filter criteria:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 典型情况下，所有值都会在箱线图中绘制。由于数据集非常庞大，例如，我们将查看包含“日本”在`place`字段中的16,036次地震子集。首先，使用SQL创建数据集，这是一个简单的*SELECT*，选择所有符合筛选条件的`mag`值：
- en: '[PRE17]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Then create a box plot in our graphing software of choice, as shown in [Figure 6-7](#box_plot_showing_magnitude_distribution).
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在我们选择的图形软件中创建一个箱线图，如[图6-7](#box_plot_showing_magnitude_distribution)所示。
- en: '![](Images/sfda_0607.png)'
  id: totrans-103
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0607.png)'
- en: Figure 6-7\. Box plot showing magnitude distribution of earthquakes in Japan
  id: totrans-104
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-7\. 展示了日本地震震级分布的箱线图
- en: 'Although the graphing software will often provide this information, we can
    also find the key values for the box plot with SQL:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管绘图软件通常会提供这些信息，我们还可以使用SQL查找箱线图的关键值：
- en: '[PRE18]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: The median Japanese earthquake had a magnitude of 4.5, and the whiskers extend
    from 3.7 to 5.3\. The plotted circles represent outlier earthquakes, both small
    and large. The Great Tohoku Earthquake of 2011, at 9.1, is an obvious outlier,
    even among the larger earthquakes Japan experienced.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 日本地震的中位数为4.5，盒须从3.7延伸至5.3。绘制的圆圈代表异常地震，包括小型和大型地震。2011年的大东北地震，震级为9.1，显然是一个异常值，即使在日本经历的较大地震中也是如此。
- en: Warning
  id: totrans-108
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: In my experience, box plots are one of the more difficult visualizations to
    explain to those who don’t have a statistics background, or who don’t spend all
    day making and looking at visualizations. The interquartile range is a particularly
    confusing concept, though the notion of outliers seems to make sense to most people.
    If you’re not absolutely sure your audience knows how to interpret a box plot,
    take the time to explain it in clear but not overly technical terms. I keep a
    drawing like [Figure 6-8](#diagram_of_parts_of_a_box_plot_left_squ) that explains
    the parts of a box plot and send it along with my work “just in case” my audience
    needs a refresher.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的经验中，箱线图是向那些没有统计背景或整天制作和查看可视化的人解释起来比较困难的可视化之一。四分位距尤其是一个特别令人困惑的概念，尽管异常值的概念似乎对大多数人来说是有意义的。如果你不确定你的观众绝对知道如何解读箱线图，请花时间用清晰但不过于技术化的术语来解释它。我保留了一幅像[图 6-8](#diagram_of_parts_of_a_box_plot_left_squ)这样解释箱线图各部分的图画，并随我的工作一起发送，以防万一我的观众需要复习。
- en: '![](Images/sfda_0608.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0608.png)'
- en: Figure 6-8\. Diagram of parts of a box plot
  id: totrans-111
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-8\. 箱线图各部分示意图
- en: 'Box plots can also be used to compare across groupings of the data to further
    identify and diagnose where outliers occur. For example, we can compare earthquakes
    in Japan in different years. First add the year of the `time` field into the SQL
    output and then graph, as in [Figure 6-9](#box_plots_by_year_of_magnitudes_of_eart):'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 箱线图也可以用来比较数据的不同分组，进一步识别和诊断异常值出现的位置。例如，我们可以比较日本不同年份的地震情况。首先将`time`字段的年份添加到SQL输出中，然后绘制图表，如[图 6-9](#box_plots_by_year_of_magnitudes_of_eart)所示：
- en: '[PRE19]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '![](Images/sfda_0609.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0609.png)'
- en: Figure 6-9\. Box plot of magnitudes of earthquakes in Japan, by year
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. 日本地震震级的箱线图，按年份划分
- en: Although the median and the range of the boxes fluctuate a bit from year to
    year, they are consistently between 4 and 5\. Japan experienced large outlier
    earthquakes every year, with at least one greater than 6.0, and in six of the
    years it experienced at least one earthquake at or larger than 7.0\. Japan is
    undoubtedly a very seismically active region.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管箱子的中位数和范围从一年到一年会有些波动，但它们始终在4到5之间。日本每年都经历着大的离群地震，至少有一次大于6.0，而在六年中，至少有一次地震达到或超过7.0。日本无疑是一个地震活动非常频繁的地区。
- en: Bar graphs, scatter plots, and box plots are commonly used to detect and characterize
    outliers in data sets. They allow us to quickly absorb the complexity of large
    amounts of data and to start to tell the story behind it. Along with sorting,
    percentiles, and standard deviations, graphs are an important part of the anomaly
    detection toolkit. With these tools in hand, we’re ready to discuss the various
    forms that anomalies can take in addition to those we’ve seen so far.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 条形图、散点图和箱线图通常用于检测和描述数据集中的异常值。它们使我们能够快速理解大量数据的复杂性，并开始讲述背后的故事。除了排序、百分位数和标准偏差之外，图表是异常检测工具包的重要组成部分。有了这些工具，我们准备讨论异常可能采取的各种形式，除了我们到目前为止见过的形式。
- en: Forms of Anomalies
  id: totrans-118
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 异常形式
- en: 'Anomalies can come in all shapes and sizes. In this section, I will discuss
    three general categories of anomalies: values, counts or frequencies, and presence
    or absence. These are starting points for investigating any data set, either as
    a profiling exercise or because anomalies are suspected. Outliers and other unusual
    values are often specific to a particular domain, so in general the more you know
    about how and why the data was generated, the better. However, these patterns
    and techniques for spotting anomalies are good starting places for investigation.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 异常可以呈现各种形状和大小。在本节中，我将讨论三种一般类型的异常：值、计数或频率以及存在或不存在。这些是调查任何数据集的起点，无论是作为概要性练习还是因为怀疑存在异常值。离群值和其他异常值通常特定于特定领域，因此总体而言，你对数据生成的方式和原因了解得越多，就越好。然而，这些发现异常的模式和技术是调查的良好起点。
- en: Anomalous Values
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常值
- en: Perhaps the most common type of anomaly, and the first thing that comes to mind
    on this topic, is when single values are either extremely high or low outliers,
    or when values in the middle of the distribution are otherwise unusual.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 可能是最常见的异常类型，也是这个主题首先想到的事情，就是单个值要么是极高的或极低的离群值，要么是分布中间的值非常不寻常。
- en: 'In the last section, we looked at several ways to find outliers, through sorting,
    percentiles and standard deviations, and graphing. We discovered that the earthquakes
    data set has both unusually large values for the magnitude and some values that
    appear to be unusually small. The magnitudes also contain varying numbers of *significant
    digits*, or digits to the right of the decimal point. For example, we can look
    at a subset of values around 1 and find a pattern that repeats throughout the
    data set:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们探讨了几种寻找异常值的方法，包括排序、百分位数、标准差和绘图。我们发现地震数据集在震级方面既有异常大的值，也有异常小的值。震级还包含不同数量的*有效数字*，或者小数点后的数字。例如，我们可以查看围绕1附近的一些数值子集，并找到数据集中反复出现的模式：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Every once in a while there is a value with 8 significant digits. Many values
    have two significant digits, but having only a single significant digit is more
    common. This is likely due to different levels of precision in the instruments
    collecting the magnitude data. Additionally, the database does not display a second
    significant digit when that digit is zero, so “1.10” appears simply as “1.1.”
    However, the large number of records at “1.1” indicates that this is not just
    a display issue. Depending on the purpose of the analysis, we may or may not want
    to adjust the values to all have the same number of significant digits by rounding.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 偶尔会有8个有效数字的数值出现。许多数值有两个有效数字，但只有一个有效数字更为普遍。这可能是由于收集震级数据的仪器精度不同所致。此外，当第二个有效数字为零时，数据库不显示第二个有效数字，因此“1.10”看起来简单地显示为“1.1”。然而，“1.1”处的大量记录表明这不仅仅是一个显示问题。根据分析目的，我们可能需要通过四舍五入来调整数值，使其具有相同数量的有效数字。
- en: 'Often in addition to finding anomalous values, understanding why they happened
    or other attributes that are correlated with anomalies is useful. This is where
    creativity and data detective work come into play. For example, 1,215 records
    in the data set have very high depth values of more than 600 kilometers. We might
    want to know where these outliers occurred or how they were collected. Let’s take
    a look at the source, which we can find in the `net` (for network) field:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 除了找到异常值之外，理解它们发生的原因或与异常相关的其他属性也是有用的。这是创造性和数据探索工作的体现。例如，数据集中有1,215条记录的深度值超过600公里。我们可能想知道这些异常值发生在哪里或者它们是如何收集的。让我们来看看来源，我们可以在`net`（表示网络）字段中找到。
- en: '[PRE21]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The USGS site indicates that this source is the [USGS National Earthquake Information
    Center, PDE](https://earthquake.usgs.gov/data/comcat/contributor/us). This is
    not terribly informative, however, so let’s check the `place` values, which contain
    the earthquake locations:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 美国地质调查局（[USGS国家地震信息中心，PDE](https://earthquake.usgs.gov/data/comcat/contributor/us)）网站指示了这一信息来源。然而，这并不是特别详细，因此让我们来检查一下`place`值，其中包含地震的位置信息：
- en: '[PRE22]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Visual inspection suggests that many of these very deep earthquakes happen
    around Ndoi Island in Fiji. However, the place includes a distance and direction
    component, such as “100km NW of,” that makes summarization more difficult. We
    can apply some text parsing to focus on the place itself for better insights.
    For places that contain some values and then “ of ” and some more values, split
    on the “ of ” string and take the second part:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 目视检查表明，这些非常深的地震多发生在斐济的**Ndoi岛**附近。然而，该地点包括距离和方向组成部分，如“100km NW of”，这使得总结变得更加困难。我们可以应用一些文本解析技术，集中关注地点本身，以获取更好的见解。
- en: '[PRE23]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: We can now say with more confidence that the majority of the very deep values
    were recorded for earthquakes somewhere in Fiji, with a particular concentration
    around the small volcanic island of Ndoi. The analysis could continue to get more
    complex, for example, by parsing the text to group together all earthquakes recorded
    in the greater region, which would reveal that after Fiji, other very deep earthquakes
    have been recorded around Vanuatu and the Philippines.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以更加确信，大部分非常深的数值记录来自斐济某处地震，特别是集中在小火山岛Ndoi周围。分析可以进一步复杂化，例如通过解析文本将所有记录在更大区域内的地震分组在一起，这将揭示在斐济之后，其他非常深的地震也发生在瓦努阿图和菲律宾附近。
- en: 'Anomalies can come in the form of misspellings, variations in capitalization,
    or other text errors. The ease of finding these depends on the number of distinct
    values, or *cardinality*, of the field. Differences in capitalization can be detected
    by counting both the distinct values and the distinct values when a `lower` or
    `upper` function is applied:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 异常可以以拼写错误、大小写变化或其他文本错误的形式出现。发现这些错误的难易程度取决于字段的不同值的数量，或者*基数*。在应用`lower`或`upper`函数时，大小写的差异可以通过计算不同的值和应用函数后的不同值来检测：
- en: '[PRE24]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'There are 24 distinct values of the `type` field, but 25 different forms. To
    find the specific types, we can use a calculation to flag those values whose lowercase
    form doesn’t match the actual value. Including the count of records for each form
    will help contextualize so that we can later decide how to handle the values:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: '`type`字段有24个不同的值，但是有25种不同的形式。为了找到具体的类型，我们可以进行计算，标记那些小写形式与实际值不匹配的值。包括每种形式的记录计数将有助于在后续决定如何处理这些值时提供背景信息：'
- en: '[PRE25]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The anomalous value of “Ice quake” is easy to spot, since it is the only value
    for which the flag calculation returns `false`. Since there is only one record
    with this value, compared to 10,136 with the lowercase form, we can assume that
    it can be grouped together with the other records. Other text functions can be
    applied, such as `trim` if we suspect that the values contain extra leading or
    trailing spaces, or `replace` if we suspect that certain spellings have multiple
    forms, such as the number “2” and the word “two.”
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: “Ice quake”的异常值很容易识别，因为它是唯一一个返回`false`的标志计算值。由于只有一条记录包含此值，相比于低级别形式的10,136条记录，我们可以假设它可以与其他记录分组在一起。可以应用其他文本函数，如`trim`，如果我们怀疑值包含额外的前导或尾随空格，或者`replace`，如果我们怀疑某些拼写有多个形式，比如数字“2”和单词“two”。
- en: 'Misspellings can be more difficult to discover than other variations. If a
    known set of correct values and spellings exists, it can be used to validate the
    data either through an *OUTER JOIN* to a table containing the values or with a
    CASE statement combined with an IN list. In either case, the goal is to flag values
    that are unexpected or invalid. Without such a set of correct values, our options
    are often either to apply domain knowledge or to make educated guesses. In the
    `earthquakes` table, we can look at the `type` values with only a few records
    and then try to determine if there is another, more common value that can be substituted:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 拼写错误可能比其他变体更难发现。如果存在已知的正确值和拼写的集合，可以通过与包含这些值的表进行外连接或使用CASE语句与IN列表来验证数据。在任何情况下，目标是标记意外或无效的值。如果没有这样的正确值集合，我们的选择通常是应用领域知识或进行合理的猜测。在`earthquakes`表中，我们可以查看`type`值，只有少数记录，然后尝试确定是否有其他更常见的值可以替代：
- en: '[PRE26]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: We looked at “Ice Quake” previously and decided it was likely the same as “ice
    quake.” There is only one record for “rockslide,” though we might consider this
    close enough to another of the values, “landslide,” which has 15 records. “Collapse”
    is more ambiguous, since the data set includes both “mine collapse” and “building
    collapse.” What we do with these, or whether we do anything at all, depends on
    the goal of the analysis, as I’ll discuss later in [“Handling Anomalies”](#handling_anomalies).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看过“冰震”并认为它很可能与“ice quake”相同。对于“rockslide”只有一条记录，尽管我们可能认为这足够接近另一个值“landslide”，后者有15条记录。“崩溃”更加模糊，因为数据集包含“矿井坍塌”和“建筑物坍塌”两种情况。我们对这些值的处理，或者是否需要处理，取决于分析的目标，我将在[“处理异常”](#handling_anomalies)中讨论。
- en: Anomalous Counts or Frequencies
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 异常计数或频率
- en: Sometimes anomalies come not in the form of individual values but in the form
    of patterns or clusters of activity in the data. For example, a customer spending
    $100 on an ecommerce site may not be unusual, but that same customer spending
    $100 every hour over the course of 48 hours would almost certainly be an anomaly.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 有时异常不是以单个值的形式出现，而是以数据中的模式或活动簇群的形式出现。例如，在电子商务网站上，客户每小时花费100美元可能并不罕见，但是同一个客户在48小时内每小时都花费100美元几乎肯定是异常。
- en: There are a number of dimensions on which clusters of activity can indicate
    anomalies, many of them dependent on the context of the data. Time and location
    are both common across many data sets and are features of the `earthquakes` data
    set, so I will use them to illustrate the techniques in this section. Keep in
    mind that these techniques can often be applied to other attributes as well.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 有许多维度可以表明活动的集群可能指示异常，其中许多依赖于数据的上下文。时间和位置在许多数据集中都很常见，并且是`earthquakes`数据集的特征，因此我将使用它们来说明本节中的技术。请记住，这些技术通常也可以应用于其他属性。
- en: Events that happen with unusual frequency over a short time span can indicate
    anomalous activity. This can be good, such as when a celebrity unexpectedly promotes
    a product, leading to a burst of sales of that product. They can also be bad,
    such as when unusual spikes indicate fraudulent credit card use or attempts to
    bring a website down with a flood of traffic. To understand these types of anomalies
    and whether there are deviations from the normal trend, we first apply appropriate
    aggregations and then use the techniques introduced earlier in this chapter, along
    with time series analysis techniques discussed in [Chapter 3](ch03.xhtml#time_series_analysis).
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 在短时间内频繁发生的事件可能表明异常活动。这可能是好事，例如当名人突然推广某种产品时导致该产品销量激增。它们也可能是坏事，例如当异常的峰值表明欺诈性信用卡使用或试图通过大量流量使网站崩溃时。为了理解这些类型的异常以及是否存在与正常趋势的偏差，我们首先应用适当的聚合，然后使用本章早期介绍的技术，以及[第三章](ch03.xhtml#time_series_analysis)中讨论的时间序列分析技术。
- en: 'In the following examples, I’ll go through a series of steps and queries that
    will help us understand the normal patterns and hunt for unusual ones. This is
    an iterative process that uses data profiling, domain knowledge, and insights
    from previous query results to guide each step. We’ll start our journey by checking
    the counts of earthquakes by year, which we can do by truncating the `time` field
    to the year level, and counting the records. For databases that don’t support
    `date_trunc`, consider `extract` or `trunc` instead:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的示例中，我将逐步介绍一系列步骤和查询，这些步骤和查询将帮助我们理解正常模式并寻找异常模式。这是一个迭代过程，利用数据分析、领域知识以及先前查询结果的洞察来引导每一步。我们将从按年计算地震数量开始，可以通过将`time`字段截断到年级别并计数记录来实现。对于不支持`date_trunc`的数据库，考虑使用`extract`或`trunc`代替：
- en: '[PRE27]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'We can see that 2011 and 2012 had low numbers of earthquakes compared to other
    years. There was also a sharp increase in records in 2018 that was sustained through
    2019 and 2020\. This seems unusual, and we can hypothesize that the earth became
    more seismically active suddenly, that there is an error in the data such as duplication
    of records, or that something changed in the data collection process. Let’s drill
    down to month level to see if this trend persists at a more granular level of
    time:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，2011年和2012年的地震数量较其他年份少。2018年记录数量显著增加，并在2019年和2020年保持增长。这似乎是不寻常的，我们可以假设地球突然变得更具地震活跃性，数据存在错误如记录重复，或者数据收集过程发生了变化。让我们进一步分析到月度水平，看看这一趋势是否在更详细的时间粒度上持续存在：
- en: '[PRE28]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The output is displayed in [Figure 6-10](#number_of_earthquakes_per_month).
    We can see that although the number of earthquakes varies from month to month,
    there does appear to be an overall increase starting in 2017\. We can also see
    that there are three outlier months, in April 2010, July 2018, and July 2019.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 输出显示在[图 6-10](#number_of_earthquakes_per_month)中。我们可以看到，尽管地震数量每个月都有所变化，但从2017年开始似乎整体上有所增加。我们还可以看到，有三个异常月份，分别是2010年4月，2018年7月和2019年7月。
- en: '![](Images/sfda_0610.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0610.png)'
- en: Figure 6-10\. Number of earthquakes per month
  id: totrans-150
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-10\. 每月地震数量
- en: 'From here we can continue checking the data at more granular time periods,
    perhaps optionally filtering the result set by a range of dates to focus in on
    these anomalous stretches of time. After narrowing in on the specific days or
    even times of day to pinpoint when the spikes occurred, we might want to break
    the data down further by other attributes in the data set. This can help explain
    the anomalies or at least narrow down the conditions in which they occurred. For
    example, it turns out that the increase in earthquakes starting in 2017 can be
    at least partially explained by the `status` field. The status indicates whether
    the event has been reviewed by a human (“reviewed”) or was directly posted by
    a system without review (“automatic”):'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里开始，我们可以继续检查更精细的时间段内的数据，也许可以选择性地通过一系列日期范围来过滤结果集，以便集中关注这些异常时间段。在缩小到特定的日期甚至每天的具体时间以确定何时发生尖峰后，我们可能希望进一步按数据集中的其他属性拆分数据。这可以帮助解释异常情况，或者至少缩小它们发生的条件范围。例如，结果表明，从2017年开始地震增加至少部分可以解释为`status`字段。状态指示事件是否已由人类审查（“reviewed”）或直接由系统发布而未经审查（“automatic”）：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: The trends of “automatic” and “reviewed” status are plotted in [Figure 6-11](#number_of_earthquakes_per_monthcomma_sp).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: “automatic”和“reviewed”状态的趋势在[图6-11](#number_of_earthquakes_per_monthcomma_sp)中绘制。
- en: '![](Images/sfda_0611.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0611.png)'
- en: Figure 6-11\. Number of earthquakes per month, split by status
  id: totrans-155
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 每月地震数量，按状态分割
- en: In the graph, we can see that the outlier counts in July 2018 and July 2019
    are due to large increases in the number of “automatic”-status earthquakes, whereas
    the spike in April 2010 was in “reviewed”-status earthquakes. A new type of automatic
    recording equipment may have been added to the data set in 2017, or perhaps there
    hasn’t been enough time to review all the recordings yet.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在图表中，我们可以看到2018年7月和2019年7月的异常计数是由于“automatic”状态地震数量大幅增加，而2010年4月的尖峰是在“reviewed”状态地震中发生的。可能在2017年添加了新型自动记录设备到数据集中，或者也许还没有足够时间审查所有记录。
- en: 'Analyzing location in data sets that have that information can be another powerful
    way to find and understand anomalies. The `earthquakes` table contains information
    about many thousands of very small earthquakes, potentially obscuring our view
    of the very large, very noteworthy earthquakes. Let’s look at the locations of
    the biggest quakes, those of magnitude 6 or larger, and see where they cluster
    geographically:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 分析具有该信息的数据集中的位置是发现和理解异常的另一种强大方式。`earthquakes`表包含有关成千上万的非常小地震的信息，可能会遮蔽我们对非常大、非常显著地震的视野。让我们看看震级大于或等于6级的大地震的位置分布：
- en: '[PRE30]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'In contrast to time, where we queried at progressively more granular levels,
    the `place` values are already so granular that it’s a bit difficult to grasp
    the full picture, although the Honshu, Japan, region clearly stands out. We can
    apply some of the text analysis techniques from [Chapter 5](ch05.xhtml#text_analysis)
    to parse and then group the geographic information. In this case, we’ll use `split_part`
    to remove the direction text (such as “near the coast of” or “100km N of”) that
    often appears at the beginning of the `place` field:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与时间相比，在逐步更精细的级别进行查询的地方，`place`值已经如此精细化，以至于有些难以理解整体图景，尽管日本本州地区显然突出。我们可以应用来自[第5章](ch05.xhtml#text_analysis)的一些文本分析技术来解析并分组地理信息。在这种情况下，我们将使用`split_part`来删除`place`字段开头经常出现的方向文字（如“靠近海岸”或“100km
    N of”）：
- en: '[PRE31]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The region around Honshu, Japan, experienced 89 earthquakes, making it not only
    the location of the largest earthquake in the data set but also an outlier in
    the number of very large earthquakes recorded. We could continue to parse, clean,
    and group the `place` values to gain a more refined picture of where major earthquakes
    occur in the world.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 日本本州周围地区经历了89次地震，不仅是数据集中最大地震的发生地，而且是记录的大型地震数量的异常值。我们可以继续解析、清理和分组`place`值，以更精确地了解世界上发生重大地震的位置。
- en: Finding anomalous counts, sums, or frequencies in data is usually an exercise
    that involves a number of rounds of querying different levels of granularity in
    succession. It’s common to start broad, then go more granular, zoom out again
    to compare to baseline trends, and zoom in again on specific splits or dimensions
    of the data. Fortunately, SQL is a great tool for this sort of rapid iteration.
    Combining techniques, especially from time series analysis, discussed in [Chapter 3](ch03.xhtml#time_series_analysis),
    and text analysis, discussed in [Chapter 5](ch05.xhtml#text_analysis), will bring
    even more richness to the analysis.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据中找到异常的计数、总和或频率通常需要一系列不同粒度查询的练习。通常从广泛的范围开始，然后更详细地查询，再次放大以与基线趋势进行比较，然后再次对数据的特定分割或维度进行详细查询是很常见的。幸运的是，SQL
    是进行这种快速迭代的好工具。结合特别是来自时间序列分析的技术（如[第 3 章](ch03.xhtml#time_series_analysis)中讨论的）和文本分析（如[第
    5 章](ch05.xhtml#text_analysis)中讨论的），将为分析带来更多丰富性。
- en: Anomalies from the Absence of Data
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据缺失的异常
- en: We’ve seen how unusually high frequencies of events can signal anomalies. Keep
    in mind that the absence of records can also signal anomalies. For example, the
    heartbeat of a patient undergoing surgery is monitored. The absence of a heartbeat
    at any time generates an alert, as do irregularities in the heartbeat. In many
    contexts, however, detecting the absence of data is difficult if you’re not specifically
    looking for it. Customers don’t always announce they are about to churn. They
    simply stop using the product or service and quietly drop out of the data set.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到异常高频事件可以标志异常。请记住，记录缺失也可能标志异常。例如，手术中接受监控的病人的心跳。任何时候心跳缺失都会触发警报，就像心跳的不规则一样。然而，在许多情况下，如果您没有专门寻找它，检测数据缺失是困难的。客户并不总是宣布他们即将流失。他们只是停止使用产品或服务，然后悄悄地退出数据集。
- en: One way to ensure that absences in data are noticed is to use techniques from
    cohort analysis, discussed in [Chapter 4](ch04.xhtml#cohort_analysis). In particular,
    a *JOIN* to a date series or data dimension, to ensure that a record exists for
    every entity whether or not it was present in that time period, makes absences
    easier to detect.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 确保数据中的缺失被注意到的一种方法是使用队列分析技术，如[第 4 章](ch04.xhtml#cohort_analysis)中讨论的。特别是，*JOIN*
    到日期系列或数据维度，确保每个实体都存在记录，无论其是否在该时间段内出现，这样可以更容易地检测到缺失。
- en: 'Another way to detect absence is to query for gaps, or time since last seen.
    Some regions are more prone to large earthquakes due to the way tectonic plates
    are arranged around the globe. We’ve also detected some of this in the data in
    our previous examples. Earthquakes are notoriously hard to predict, even when
    we have a sense of where they are likely to occur. This doesn’t stop some people
    from speculating about the next “big one” simply due to the amount of time that
    has passed since the last one. We can use SQL to find the gaps between large earthquakes
    and the time since the most recent one:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 检测缺失的另一种方法是查询间隙或距上次出现的时间。由于地球上构造板块的排列方式，一些地区更容易发生大地震。我们在先前的例子中也检测到了一些这种情况。即使我们知道它们可能发生的地方，地震通常很难预测。这并不能阻止一些人简单地因为距离上次大地震的时间过长而猜测下一次“大地震”。我们可以使用
    SQL 来查找大地震之间的间隙和自上次大地震以来的时间：
- en: '[PRE32]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In the innermost subquery, the `place` field is parsed and cleaned, returning
    larger regions or countries, along with the time of each earthquake, for all earthquakes
    of magnitude 5 or greater. The second subquery uses a `lead` function to find
    the `time` of the next earthquake, if any, for each place and time, and the `gap`
    between each earthquake and the next one. The `max` window function returns the
    most recent earthquake for each place. The outer query calculates the days since
    the latest 5+ earthquake in the data set, using the `extract` function to return
    just the days from the interval that is returned when two dates are subtracted.
    Since the data set includes records only through the end of 2020, the timestamp
    “2020-12-31 23:59:59” is used, though `current_timestamp` or an equivalent expression
    would be appropriate if the data were refreshed on an ongoing basis. Days are
    extracted in a similar fashion from the average and max of the `gap` value.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 在最内层子查询中，`place`字段被解析和清理，返回更大的地区或国家，以及每次5级或更大地震的时间。第二个子查询使用`lead`函数查找每个地点和时间的下一个地震的`time`，以及每次地震与下一个地震之间的`gap`。`max`窗口函数返回每个地点的最近地震。外部查询使用`extract`函数计算数据集中距离最新的5级以上地震的天数，只返回两个日期相减后的间隔中的天数。由于数据集仅包括到2020年底的记录，因此时间戳“2020-12-31
    23:59:59”被使用，尽管如果数据定期刷新，`current_timestamp`或等效表达式也是合适的。类似地，从`gap`值的平均值和最大值中提取天数。
- en: The time since the last major earthquake in a location may have little predictive
    power in practice, but in many domains, gaps and time since last seen metrics
    have practical applications. Understanding typical gaps between actions sets a
    baseline against which the current gap can be compared. When the current gap is
    within range of historical values, we might judge that a customer is retained,
    but when the current gap is much longer, the risk of churn increases. The result
    set from a query that returns historical gaps can itself become the subject of
    an anomaly detection analysis, answering questions such as the longest amount
    of time that a customer was gone before subsequently returning.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 一个地点自上次大地震以来的时间可能在实践中没有多少预测力，但在许多领域，间隔和自上次出现以来的时间度量具有实际应用。了解动作之间的典型间隔可以建立一个基准，用来比较当前的间隔。当当前间隔在历史值的范围内时，我们可能会判断客户已经保持，但是当当前间隔较长时，流失的风险就增加了。从返回历史间隔的查询结果集本身可以成为异常检测分析的主题，回答诸如客户在离开后多长时间才返回的最长时间等问题。
- en: Handling Anomalies
  id: totrans-170
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理异常
- en: Anomalies can appear in data sets for a number of reasons and can take a number
    of forms, as we’ve just seen. After detecting anomalies, the next step is to handle
    them in some fashion. How this is done depends on both the source of the anomaly—underlying
    process or data quality issue—and the end goal of the data set or analysis. The
    options include investigation without changes, removal, replacement, rescaling,
    and fixing upstream.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 异常可能出现在数据集中的许多原因和形式，正如我们刚刚看到的。检测到异常后，下一步是以某种方式处理它们。如何处理取决于异常的来源——潜在过程或数据质量问题——以及数据集或分析的最终目标。选项包括进行调查而不进行更改、删除、替换、重新缩放和上游修复。
- en: Investigation
  id: totrans-172
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 调查
- en: Finding, or attempting to find, the cause of an anomaly is usually the first
    step in deciding what to do with it. This part of the process can be both fun
    and frustrating—fun in the sense that tracking down and solving a mystery engages
    our skills and creativity, but frustrating in the sense that we’re often working
    under time pressure and tracking down anomalies can feel like going down an endless
    series of rabbit holes, leading us to wonder whether an entire analysis is flawed.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 发现或试图找出异常的原因通常是决定如何处理它的第一步。这一过程既有趣又令人沮丧——有趣的是追踪和解决谜团激发了我们的技能和创造力，但令人沮丧的是我们经常在时间紧迫的情况下工作，追踪异常就像是进入一个无尽的兔子洞，使我们怀疑整个分析是否存在缺陷。
- en: When I’m investigating anomalies, my process usually involves a series of queries
    that bounce back and forth between searching for patterns and looking at specific
    examples. A true outlier value is easy to spot. In such cases, I will usually
    query for the entire row that contains the outlier for clues as to the timing,
    source, and any other attributes that are available. Next, I’ll check records
    that share those attributes to see if they have values that seem unusual. For
    example, I might check to see whether other records on the same day have normal
    or unusual values. Traffic from a particular website or purchases of a particular
    product might reveal other anomalies.
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 当我调查异常时，我的过程通常涉及一系列查询，这些查询在搜索模式和查看特定示例之间来回跳转。真正的异常值很容易辨认。在这种情况下，我通常会查询包含异常值的整行，以获取有关时间、来源和其他可用属性的线索。接下来，我会检查具有相同属性的记录，看看它们的值是否看起来不寻常。例如，我可能会检查同一天的其他记录是否具有正常或异常值。来自特定网站的流量或特定产品的购买可能会显示其他异常情况。
- en: After investigating the source and attributes of anomalies when working on data
    produced internally in my organization, I get in touch with the stakeholders or
    product owners. Sometimes there is a known bug or flaw, but often enough there
    is a real issue in a process or system that needs to be addressed, and context
    information is useful. For external or public data sets, there may not be an opportunity
    to find the root cause. In these cases, my goal is to gather enough information
    to decide which of the options discussed next is appropriate.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在调查内部组织生成的数据的异常源和属性后，我会与利益相关者或产品所有者联系。有时会出现已知的错误或缺陷，但通常会存在需要解决的真正问题或过程或系统中的问题，并且上下文信息很有用。对于外部或公共数据集，可能无法找到根本原因。在这些情况下，我的目标是收集足够的信息，以决定下一步讨论的选项中哪一种适合。
- en: Removal
  id: totrans-176
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 删除
- en: One option for dealing with data anomalies is to simply remove them from the
    data set. If there is reason to suspect that there was an error in the data collection
    that might affect the entire record, removal is appropriate. Removal is also a
    good option when the data set is large enough that dropping a few records is unlikely
    to affect the conclusions. Another good reason to use removal is when the outliers
    are so extreme that they would skew the results enough that entirely inappropriate
    conclusions would be drawn.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数据异常的一个选项是简单地从数据集中删除它们。如果有理由怀疑数据收集中存在可能影响整个记录的错误，那么删除就是合适的。当数据集足够大以至于删除几条记录不太可能影响结论时，删除也是一个好选择。使用删除的另一个好理由是，当异常值非常极端以至于会使结果偏离到完全不适当的结论时。
- en: 'We saw previously that the `earthquakes` data set contains a number of records
    with a magnitude of –9.99 and a few with –9\. Since the earthquakes these values
    would correspond to are extremely small, we might suspect that they are erroneous
    values or were simply entered when the actual magnitude was unknown. Removing
    records with these values is straightforward in the *WHERE* clause:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到 `earthquakes` 数据集包含许多震级为 –9.99 和少数为 –9 的记录。由于这些值对应的地震非常小，我们可能会怀疑它们是错误值或者仅在实际震级未知时输入。在
    *WHERE* 子句中，删除具有这些值的记录非常简单：
- en: '[PRE33]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Before removing the records, however, we might want to determine whether including
    the outliers actually makes a difference to the output. For example, we might
    want to know if removing the outliers affects the average magnitude, since averages
    can easily be skewed by outliers. We can do this by calculating the average across
    the entire data set, as well as the average excluding the extreme low values,
    using a CASE statement to exclude them:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，在删除记录之前，我们可能需要确定包含异常值是否真的会影响输出结果。例如，我们可能想知道删除异常值是否会影响平均幅度，因为异常值很容易使平均值偏离。我们可以通过计算整个数据集的平均值以及使用
    CASE 语句来排除极端低值的平均值来进行这项工作：
- en: '[PRE34]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'The averages are different only at the third significant digit (1.625 versus
    1.627), which is a fairly small difference. However, if we filter just to Yellowstone
    National Park, where many of the –9.99 values occur, the difference is more dramatic:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这些平均值仅在第三个有效数字处有所不同（1.625 对比 1.627），这是一个相当小的差异。然而，如果我们仅筛选黄石国家公园，那里有许多值为 –9.99
    的记录时，差异就更加显著：
- en: '[PRE35]'
  id: totrans-183
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: Although these are still small values, the difference between an average of
    0.46 and 0.92 is big enough that we would likely choose to remove the outliers.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些值仍然很小，0.46 和 0.92 的平均差异足够大，我们很可能选择删除异常值。
- en: 'Notice that there are two options for doing so: either in the *WHERE* clause,
    which removes the outliers from all the results, or in a CASE statement, which
    removes them only from specific calculations. Which option you choose depends
    on the context of the analysis, as well as on whether it is important to preserve
    the rows in order to retain total counts, or useful values in other fields.'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，有两种处理方式：一种是在*WHERE*子句中，从所有结果中移除异常值；另一种是在CASE语句中，仅从特定计算中移除异常值。选择哪种方式取决于分析的上下文以及是否重要保留行以保留总计数或其他字段中的有用值。
- en: Replacement with Alternate Values
  id: totrans-186
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 替换为备用值
- en: Anomalous values can often be handled by replacing them with other values rather
    than removing entire records. An alternate value can be a default, a substitute
    value, the nearest numerical value within a range, or a summary statistic such
    as the average or median.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 异常值通常可以通过替换为其他值而不是移除整个记录来处理。备用值可以是默认值、替代值、范围内最接近的数值或诸如平均值或中位数的汇总统计量。
- en: 'We’ve seen previously that null values can be replaced with a default using
    the `coalesce` function. When values are not necessarily null but are problematic
    for some other reason, a CASE statement can be used to substitute a default value.
    For example, rather than report on all the various seismic events, we might want
    to group the types that are *not* earthquakes into a single “Other” value:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到，可以使用`coalesce`函数将空值替换为默认值。当值不一定为空但由于其他原因有问题时，可以使用CASE语句将默认值替换。例如，我们可能希望将*非*地震的类型分组为单个“其他”值：
- en: '[PRE36]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: This reduces the amount of detail in the data, of course, but it can also be
    a way to summarize a data set that has a number of outlier values for `type`,
    as we saw previously. When you know that outlier values are incorrect, and you
    know the correct value, replacing them with a CASE statement is also a solution
    that preserves the row in the overall data set. For example, an extra 0 might
    have been added to the end of a record, or a value might have been recorded in
    inches instead of miles.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这会减少数据中的细节，但也可以是汇总具有多个`type`异常值的数据集的一种方法，正如我们之前看到的。当您知道异常值是不正确的，并且您知道正确的值时，使用CASE语句替换它们也是一种保留整体数据集中行的解决方案。例如，可能在记录的末尾添加了额外的0，或者以英寸而不是英里记录了一个值。
- en: 'Another option for handling numeric outliers is to replace the extreme values
    with the nearest high or low value that is not extreme. This approach maintains
    much of the range of values but prevents misleading averages that can result from
    extreme outliers. *Winsorization* is a specific technique for this, where outliers
    are set to a specific percentile of the data. For example, values above the 95th
    percentile are set to the 95th percentile value, while values below the 5th percentile
    are set to the 5th percentile value. To calculate this in SQL, we first calculate
    the 5th and 95th percentile values:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 处理数值异常值的另一种选项是用最接近的高值或低值替换极端值。这种方法保持了大部分值范围，但防止了由极端异常值导致的误导性平均值。*Winsorization*是一种特定的技术，其中异常值被设置为数据的特定百分位。例如，将超过第95百分位的值设置为第95百分位的值，将低于第5百分位的值设置为第5百分位的值。要在SQL中计算这一点，我们首先计算第5和第95百分位值：
- en: '[PRE37]'
  id: totrans-192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'We can put this calculation in a subquery and then use a CASE statement to
    handle setting values for outliers below the 5th percentile and above the 95th.
    Note the Cartesian *JOIN* that allows us to compare the percentile values with
    each individual magnitude:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将此计算放在子查询中，然后使用CASE语句来处理将异常值设置为第5百分位以下和第95百分位以上的值。请注意，笛卡尔*JOIN*允许我们将百分位值与每个单独的幅度进行比较：
- en: '[PRE38]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: The 5th percentile value is 0.12, while the 95th percentile is 4.5\. Values
    below and above these thresholds are changed to the threshold in the `mag_winsorize`
    field. Values between these thresholds remain the same. There is no set percentile
    threshold for winsorizing. The 1st and 99th percentiles or even the 0.01th and
    99.9th percentiles can be used depending on the requirements for the analysis
    and how prevalent and extreme the outliers are.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第5百分位值为0.12，而第95百分位为4.5。低于这些阈值和高于这些阈值的值将更改为`mag_winsorize`字段中的阈值。在这些阈值之间的值保持不变。对于winsorizing没有设定百分位阈值。分析要求和异常值的普遍性和极端程度将根据需要使用第1和99百分位或甚至第0.01和99.9百分位。
- en: Rescaling
  id: totrans-196
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 重新缩放
- en: Rather than filtering out records or changing the values of outliers, rescaling
    values provides a path that retains all the values but makes analysis and graphing
    easier.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 而不是过滤记录或更改异常值的值，重新缩放值提供了一条路径，保留所有值但使分析和绘图更容易。
- en: We discussed the z-score previously, but it’s worth pointing out that this can
    be used as a way to rescale values. The z-score is useful because it can be used
    with both positive and negative values.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前讨论过z分数，但值得指出的是，这可以用作重新缩放值的一种方式。z分数很有用，因为它既可以用于正数也可以用于负数。
- en: Another common transformation is converting to logarithmic (log) scale. The
    benefit of transforming values into log scale is that they retain the same ordering,
    but small numbers get spread out more. Log transformations can also be transformed
    back into the original scale, easing interpretation. A downside is that the log
    transformation cannot be used on negative numbers. In the `earthquakes` data set,
    we learned that the magnitude is already expressed in log scale. The magnitude
    9.1 Great Tohoku Earthquake is extreme, but the value would appear even more extreme
    were it not expressed in log scale!
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种常见的变换是转换为对数（log）比例。将值转换为对数比例的好处在于它们保持相同的顺序，但小数字会更加分散。对数变换也可以转换回原始比例，便于解释。缺点是对数变换不能用于负数。在`earthquakes`数据集中，我们了解到震级已经用对数比例表示。9.1级的东北大地震是极端的，但如果不用对数比例表示，该值看起来会更极端！
- en: 'The `depth` field is measured in kilometers. Here we’ll query both the depth
    and the depth with the `log` function applied and then graph the output in Figures
    [6-12](#distribution_of_earthquakes_by_depthcom) and [6-13](#distribution_of_earthquakes_by_depth_on)
    in order to demonstrate the difference. The `log` function uses base 10 as a default.
    To reduce the result set for easier graphing, the depth is also rounded to one
    significant digit using the `round` function. The table is filtered to exclude
    values less than 0.05, as these would round to zero or less than zero:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: '`depth`字段以公里为单位。在这里，我们将查询深度和应用`log`函数的深度，然后将输出绘制在图[6-12](#distribution_of_earthquakes_by_depthcom)和[6-13](#distribution_of_earthquakes_by_depth_on)中，以展示差异。`log`函数默认使用10作为底数。为了减少结果集以便更轻松地绘图，深度还使用`round`函数将其四舍五入到一位有效数字。表被过滤以排除小于0.05的值，因为这些值会四舍五入为零或小于零：'
- en: '[PRE39]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '![](Images/sfda_0612.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0612.png)'
- en: Figure 6-12\. Distribution of earthquakes by depth, with unadjusted depths
  id: totrans-203
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12。地震深度分布，未调整深度
- en: '![](Images/sfda_0613.png)'
  id: totrans-204
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/sfda_0613.png)'
- en: Figure 6-13\. Distribution of earthquakes by depth on a log scale
  id: totrans-205
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13。地震深度的对数比例分布
- en: In [Figure 6-12](#distribution_of_earthquakes_by_depthcom), it’s apparent that
    there are a large number of earthquakes between 0.05 and maybe 20, but beyond
    that it’s difficult to see the distribution since the x-axis stretches all the
    way to 700 to capture the range of the data. When the depth is transformed to
    a log scale in [Figure 6-13](#distribution_of_earthquakes_by_depth_on), however,
    the distribution of the smaller values is much easier to see. Notably, the spike
    at 1.0, which corresponds to a depth of 10 kilometers, is apparent.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图6-12](#distribution_of_earthquakes_by_depthcom)中，显然有大量的地震在0.05到20之间，但超过这个范围的分布难以看清楚，因为x轴延伸到700以捕捉数据的范围。然而，当深度转换为对数比例时，在[图6-13](#distribution_of_earthquakes_by_depth_on)中，较小值的分布要容易得多。值得注意的是，高于1.0的尖峰对应深度10公里时是明显的。
- en: Tip
  id: totrans-207
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 小贴士
- en: 'Other types of scale transformations, while not necessarily appropriate for
    removing outliers, can be accomplished with SQL. Some common ones include:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 其他类型的比例变换，虽然不一定适合去除异常值，但可以通过SQL完成。一些常见的包括：
- en: 'Square root: use the `sqrt` function'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平方根：使用`sqrt`函数
- en: 'Cube root: use the `cbrt` function'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 立方根：使用`cbrt`函数
- en: 'Reciprocal transformation: 1 / `field_name`'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 倒数变换：1 / `field_name`
- en: 'Change the units, such as inches to feet or pounds to kilograms: multiply or
    divide by the appropriate conversion factor with * or /.'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 更改单位，例如将英寸转换为英尺或磅转换为千克：用*或/乘以或除以适当的转换因子。
- en: Rescaling can be done in SQL code, or often alternatively in the software or
    coding language used for graphing. The log transformation is particularly useful
    when there is a large spread of positive values and the patterns that are important
    to detect exist in the lower values.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在SQL代码中进行重新缩放，或者通常在用于绘图的软件或编程语言中进行。对数变换在存在大量正值的情况下特别有用，并且重要的检测模式存在于较低值中时。
- en: As with all analysis, deciding how to handle anomalies depends on the purpose
    and the amount of context or domain knowledge you have about the data set. Removing
    outliers is the simplest method, but to retain all the records, techniques such
    as winsorizing and rescaling work well.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 与所有分析一样，决定如何处理异常取决于目的以及您对数据集的上下文或领域知识量。删除异常值是最简单的方法，但为了保留所有记录，诸如 winsorizing
    和重新调整的技术也很有效。
- en: Conclusion
  id: totrans-215
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: Anomaly detection is a common practice in analysis. The goal may be to detect
    the outliers, or it may be to manipulate them in order to prepare a data set for
    further analysis. In either case, the basic tools of sorting, calculating percentiles,
    and graphing the output of SQL queries can help you find them efficiently. Anomalies
    come in many varieties, with outlying values, unusual bursts of activity, and
    unusual absences being most common. Domain knowledge is almost always helpful
    as you go through the process of finding and gathering information about the causes
    of anomalies. Options for dealing with anomalies include investigation, removal,
    replacement with alternate values, and rescaling the data. The choice depends
    heavily on the goal, but any of these paths can be accomplished with SQL. In the
    next chapter, we’ll turn our attention to experimentation, where the goal is to
    figure out whether a whole group of subjects differs from the norm of the control
    group.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 异常检测在分析中是一种常见的实践。目标可能是检测异常值，也可能是操作这些异常值，以便为进一步分析准备数据集。在任何情况下，排序、计算百分位数以及绘制 SQL
    查询输出的基本工具可以帮助您有效地找到它们。异常情况有多种类型，包括异常值、异常活动突发以及异常缺失，这些是最常见的。领域知识几乎总是在您进行发现和收集有关异常原因的信息过程中有所帮助。处理异常的选项包括调查、移除、替换为替代值和重新调整数据。选择取决于目标，但这些路径都可以通过
    SQL 实现。在下一章中，我们将把注意力转向实验，目标是弄清整个受试者组是否与对照组的标准不同。
- en: ^([1](ch06.xhtml#ch01fn8-marker)) *[*https://www.mathsisfun.com/data/standard-deviation-formulas.html*](https://www.mathsisfun.com/data/standard-deviation-formulas.html)*
    has a good explanation.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch06.xhtml#ch01fn8-marker)) *[*https://www.mathsisfun.com/data/standard-deviation-formulas.html*](https://www.mathsisfun.com/data/standard-deviation-formulas.html)*
    有一个很好的解释。
