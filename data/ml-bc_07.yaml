- en: 7 Neural networks and deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 神经网络和深度学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Convolutional neural networks for image classification
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络用于图像分类
- en: TensorFlow and Keras—frameworks for building neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow和Keras——构建神经网络的框架
- en: Using pretrained neural networks
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用预训练的神经网络
- en: Internals of a convolutional neural network
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络的内部结构
- en: Training a model with transfer learning
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用迁移学习训练模型
- en: Data augmentations—the process of generating more training data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据增强——生成更多训练数据的过程
- en: Previously, we only dealt with tabular data—data in CSV files. In this chapter,
    we’ll work with a completely different type of data—images.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们只处理表格数据——CSV文件中的数据。在本章中，我们将处理一种完全不同的数据类型——图像。
- en: The project we prepared for this chapter is classification of clothes. We will
    predict if an image of clothing is a T-shirt, a shirt, a skirt, a dress, or something
    else.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们为本章准备的项目是衣服的分类。我们将预测一张衣服的图片是T恤、衬衫、裙子、连衣裙还是其他东西。
- en: This is an image classification problem. To solve it, we will learn how to train
    a deep neural network using TensorFlow and Keras to recognize the types of clothes.
    The materials of this chapter will help you start using neural networks and perform
    any similar image classification project.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个图像分类问题。为了解决它，我们将学习如何使用TensorFlow和Keras训练深度神经网络来识别衣服的类型。本章的材料将帮助您开始使用神经网络并执行任何类似的图像分类项目。
- en: Let’s start!
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们开始吧！
- en: 7.1 Fashion classification
  id: totrans-12
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 时尚分类
- en: Imagine that we work at an online fashion marketplace. Our users upload thousands
    of images every day to sell their clothes. We want to help our users create listings
    faster by automatically recommending the right category for their clothes.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，我们在一个在线时尚市场上工作。我们的用户每天上传成千上万张图片来销售他们的衣服。我们希望通过自动推荐合适的类别来帮助用户更快地创建商品列表。
- en: 'To do it, we need a model for classifying images. Previously, we covered multiple
    models for classification: logistic regression, decision trees, random forests,
    and gradient boosting. These models work great with tabular data, but it’s quite
    difficult to use them for images.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，我们需要一个用于图像分类的模型。之前，我们介绍了多种分类模型：逻辑回归、决策树、随机森林和梯度提升。这些模型在处理表格数据时效果很好，但使用它们来处理图像相当困难。
- en: 'To solve our problem, we need a different type of model: a convolutional neural
    network, a special model used for images. These neural networks consist of many
    layers, and that’s why they are often called “deep.” Deep learning is a part of
    machine learning that deals with deep neural networks.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决我们的问题，我们需要一种不同类型的模型：卷积神经网络，这是一种专门用于图像的特殊模型。这些神经网络由许多层组成，这就是为什么它们通常被称为“深度”。深度学习是机器学习的一部分，它处理深度神经网络。
- en: The frameworks for training these models are also different from what we saw
    previously, so in this chapter we use TensorFlow and Keras instead of Scikit-learn.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 训练这些模型的框架也与我们之前看到的框架不同，因此在本章中我们使用TensorFlow和Keras而不是Scikit-learn。
- en: The plan for our project is
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 我们项目的计划是
- en: First, we download the dataset and use a pretrained model to classify images.
  id: totrans-18
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先，我们下载数据集并使用预训练的模型来分类图像。
- en: Then, we talk about neural networks, and see how they work internally.
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 然后，我们将讨论神经网络，并了解它们是如何在内部工作的。
- en: After that, we adjust the pretrained neural network for solving our tasks.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 之后，我们调整预训练的神经网络来解决我们的任务。
- en: Finally, we expand our dataset by generating many more images from the images
    we have.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最后，我们通过从我们已有的图像中生成更多图像来扩展我们的数据集。
- en: 'For evaluating the quality of our models, let’s use accuracy: the percentage
    of items we classified correctly.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估我们模型的质量，让我们使用准确率：我们正确分类的项目百分比。
- en: It’s not possible to cover all the theory behind deep learning in just one chapter.
    In this book, we focus on the most fundamental parts, which is enough for completing
    the project of this chapter and other similar projects about image classification.
    When we come across concepts that are nonessential for completing this project,
    for details, we refer to CS231n—a course about neural networks from Stanford University.
    The course notes are available online at [cs231n.github.io](https://cs231n.github.io/).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个章节中不可能涵盖深度学习背后的所有理论。在本书中，我们专注于最基本的部分，这对于完成本章的项目和其他类似的图像分类项目已经足够。当我们遇到对完成此项目非必要的概念时，我们会参考CS231n——斯坦福大学关于神经网络的课程。课程笔记可在[cs231n.github.io](https://cs231n.github.io/)上在线获取。
- en: The code for this project is available in the book’s GitHub repository at [https://github.com/alexeygrigorev/mlbookcamp-code](https://github.com/alexeygrigorev/mlbookcamp-code),
    in the folder chapter-07-neural-nets. There are multiple notebooks in this folder.
    For most of the chapter, we need 07-neural-nets-train.ipynb. For section 7.5,
    we use 07-neural-nets-test.ipynb.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 该项目的代码可在本书的 GitHub 仓库中找到，网址为 [https://github.com/alexeygrigorev/mlbookcamp-code](https://github.com/alexeygrigorev/mlbookcamp-code)，在
    chapter-07-neural-nets 文件夹中。该文件夹中有多个笔记本。对于本章的大部分内容，我们需要 07-neural-nets-train.ipynb。对于
    7.5 节，我们使用 07-neural-nets-test.ipynb。
- en: 7.1.1 GPU vs. CPU
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.1 GPU 与 CPU 对比
- en: Training a neural network is a computationally demanding process, and it requires
    powerful hardware to make it faster. To speed up training, we usually use GPUs—graphical
    processing units, or, simply, graphic cards.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 训练神经网络是一个计算密集型过程，需要强大的硬件来加快速度。为了加快训练速度，我们通常使用 GPU——图形处理单元，或者简单地说，显卡。
- en: For this chapter, a GPU is not required. You can do everything on your laptop,
    but without a GPU, it will be approximately eight times slower than with a GPU.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，不需要 GPU。你可以在你的笔记本电脑上完成所有操作，但没有 GPU，速度将比有 GPU 慢大约八倍。
- en: 'If you have a GPU card, you need to install special drivers from TensorFlow
    to use it. (Check the official documentation of TensorFlow for more details: [https://www
    .tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu).) Alternatively,
    you can rent a preconfigured GPU server. For example, we can use AWS SageMaker
    to rent a Jupyter Notebook instance with everything already set up. Refer to appendix
    E for details on how to use SageMaker. Other cloud providers also have servers
    with GPU, but we do not cover them in this book. Regardless of the environment
    you use, the code works anywhere, as long as you can install Python and TensorFlow
    there.'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一张 GPU 显卡，你需要从 TensorFlow 安装特殊的驱动程序来使用它。（有关详细信息，请参阅 TensorFlow 的官方文档：[https://www.tensorflow.org/install/gpu](https://www.tensorflow.org/install/gpu)。）或者，你可以租用一个预配置的
    GPU 服务器。例如，我们可以使用 AWS SageMaker 来租用一个已经设置好的 Jupyter Notebook 实例。有关如何使用 SageMaker
    的详细信息，请参阅附录 E。其他云服务提供商也有带 GPU 的服务器，但本书中不涉及它们。无论你使用什么环境，只要能安装 Python 和 TensorFlow，代码都可以在任何地方运行。
- en: 'After deciding where to run the code, we can go to the next step: downloading
    the dataset.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 决定代码运行位置后，我们可以进行下一步：下载数据集。
- en: 7.1.2 Downloading the clothing dataset
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.2 下载服装数据集
- en: First, let’s create a folder for this project and call it 07-neural-nets.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们为这个项目创建一个文件夹，并将其命名为 07-neural-nets。
- en: 'For this project, we need a dataset of clothes. We will use a subset of the
    clothing dataset (for more information, check [https://github.com/alexeygrigorev/clothing-dataset](https://github.com/alexeygrigorev/clothing-dataset)),
    which contains around 3,800 images of 10 different classes. The data is available
    in a GitHub repository. Let’s clone it:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个项目，我们需要一个服装数据集。我们将使用服装数据集的一个子集（更多信息请查看 [https://github.com/alexeygrigorev/clothing-dataset](https://github.com/alexeygrigorev/clothing-dataset)），它包含大约
    3,800 张 10 个不同类别的图片。数据可在 GitHub 仓库中找到。让我们克隆它：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: If you’re doing this in AWS SageMaker, you can execute this command in a cell
    of the notebook. Just add the exclamation sign (“!”) before the command (figure 7.1)`.`
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用 AWS SageMaker 进行操作，你可以在笔记本的一个单元中执行此命令。只需在命令前加上感叹号（“！”）（图 7.1）。`
- en: '![](../Images/07-01.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-01.png)'
- en: 'Figure 7.1 Executing a shell script command in Jupyter: simply add the exclamation
    sign (“!”) in front of the command.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.1 在 Jupyter 中执行 shell 脚本命令：只需在命令前加上感叹号（“！”）。
- en: 'The dataset is already split into folders (figure 7.2):'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集已经按文件夹划分（图 7.2）：
- en: 'train: Images for training a model (3,068 images)'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练集：用于训练模型的图片（3,068 张图片）
- en: 'validation: Images for validating (341 image)'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集：用于验证的图片（341 张图片）
- en: 'test: Images for testing (372 images)'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 测试集：用于测试的图片（372 张图片）
- en: '![](../Images/07-02.png)'
  id: totrans-41
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-02.png)'
- en: Figure 7.2 The dataset is already split into train, validation, and test.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.2 数据集已分为训练集、验证集和测试集。
- en: 'Each of these folders has 10 subfolders: one subfolder for each type of clothing
    (figure 7.3).'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 每个文件夹都有 10 个子文件夹：每个子文件夹对应一种服装类型（图 7.3）。
- en: '![](../Images/07-03.png)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-03.png)'
- en: Figure 7.3 Images in the dataset are organized in subfolders.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.3 数据集中的图片按子文件夹组织。
- en: As we see, this dataset contains 10 classes of clothes, from dresses and hats,
    to shorts and shoes.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，这个数据集包含 10 类服装，从连衣裙和帽子到短裤和鞋子。
- en: Each subfolder contains images of only one class (figure 7.4).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 每个子文件夹只包含一个类别的图片（图 7.4）。
- en: '![](../Images/07-04.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-04.png)'
- en: Figure 7.4 The content of the pants folder
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.4 裤子文件夹的内容
- en: In these pictures, the clothing items have different colors and the background
    is different. Some items are on the floor, some are spread out on a bed or a table,
    and some are hung in front of a neutral background.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些图片中，服装物品有不同的颜色，背景也不同。有些物品放在地板上，有些散布在床上或桌子上，有些则挂在无色背景前。
- en: 'With this variety of images, it’s not possible to use the methods we previously
    covered. We need a special type of model: neural networks. This model also requires
    different tools, and we cover them next.'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这些多样的图像，我们无法使用之前介绍的方法。我们需要一种特殊类型的模型：神经网络。此模型还需要不同的工具，我们将在下一章介绍。
- en: 7.1.3 TensorFlow and Keras
  id: totrans-52
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.3 TensorFlow和Keras
- en: 'If you use AWS SageMaker, you don’t need to install anything: it already has
    all the required libraries.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你使用AWS SageMaker，你不需要安装任何东西：它已经包含了所有必需的库。
- en: But if you use your laptop with Anaconda, or run the code somewhere else, you
    need to install TensorFlow—a library for building neural networks.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 但是如果你使用带有Anaconda的笔记本电脑，或者在其他地方运行代码，你需要安装TensorFlow——一个用于构建神经网络库。
- en: 'Use `pip` to do it:'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`pip`来完成：
- en: '[PRE1]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: TensorFlow is a low-level framework, and it’s not always easy to use. In this
    chapter, we use Keras—a higher-level library built on top of TensorFlow. Keras
    makes training neural networks a lot simpler. It comes preinstalled together with
    TensorFlow, so we don’t need to install anything extra.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: TensorFlow是一个低级框架，它并不总是容易使用。在本章中，我们使用Keras——一个建立在TensorFlow之上的高级库。Keras使训练神经网络变得简单得多。它随TensorFlow一起预安装，所以我们不需要安装任何额外的东西。
- en: Note Previously, Keras was not a part of TensorFlow, and you can find many examples
    on the internet where it’s still a separate library. However, the interface of
    Keras hasn’t changed significantly, so most of the examples you may discover still
    work in the new Keras.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：以前，Keras不是TensorFlow的一部分，你可以在互联网上找到许多它仍然是独立库的例子。然而，Keras的接口并没有发生显著变化，所以你可能发现的多数例子在新Keras中仍然有效。
- en: At the time of writing, the latest version of TensorFlow was 2.3.0 and AWS SageMaker
    used TensorFlow version 2.1.0\. The difference in versions is not a problem; the
    code from this chapter works for both versions, and it will most likely work for
    all TensorFlow 2 versions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在撰写本文时，TensorFlow的最新版本是2.3.0，AWS SageMaker使用TensorFlow版本2.1.0。版本之间的差异不是问题；本章中的代码适用于这两个版本，并且很可能会适用于所有TensorFlow
    2版本。
- en: 'We’re ready to start and create a new notebook called chapter-07-neural-nets.
    As usual, we begin by importing NumPy and MatplotLib:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我们准备开始，创建一个新的笔记本，名为chapter-07-neural-nets。像往常一样，我们首先导入NumPy和Matplotlib：
- en: '[PRE2]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'Next, import TensorFlow and Keras:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，导入TensorFlow和Keras：
- en: '[PRE3]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: The preparation work is done, and now we can take a look at the images we have.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 准备工作已完成，现在我们可以查看我们拥有的图像。
- en: 7.1.4 images
  id: totrans-65
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1.4 图像
- en: 'Keras offers a special function for loading images called `load_img`. Let’s
    import it:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个用于加载图像的特殊函数，称为`load_img`。让我们导入它：
- en: '[PRE4]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Note When Keras was a separate package, the imports looked like this:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：当Keras是一个独立的包时，导入看起来是这样的：
- en: '[PRE5]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: If you find some old Keras code on the internet and want to use it with the
    latest versions of TensorFlow, simply add `tensorflow`. at the beginning when
    importing it. Most likely, it will be enough to make it work.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在互联网上找到一些旧的Keras代码，并希望用它与TensorFlow的最新版本一起使用，只需在导入时在前面添加`tensorflow`。这很可能会让它正常工作。
- en: 'Let’s use this function to take a look at one of the images:'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用这个函数来查看其中的一张图像：
- en: '[PRE6]'
  id: totrans-72
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: After executing the cell, we should see an image of a T-shirt (figure 7.5).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 执行单元格后，我们应该看到一件T恤的图像（图7.5）。
- en: '![](../Images/07-05.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-05.png)'
- en: Figure 7.5 An image of a T-shirt from the train set
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 训练集中的T恤图像
- en: To use this image in a neural network, we need to resize it because the models
    always expect images of a certain size. For example, the network we use in this
    chapter requires a 150 × 150 image or an 299 × 299 image.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 要在神经网络中使用此图像，我们需要调整它的大小，因为模型总是期望图像为特定的大小。例如，我们本章中使用的网络需要一个150 × 150的图像或一个299
    × 299的图像。
- en: 'To resize the image, specify the `target_size` parameter:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 要调整图像大小，请指定`target_size`参数：
- en: '[PRE7]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: As a result, the image becomes square and a bit squashed (figure 7.6).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，图像变成了方形并且有点压扁（图7.6）。
- en: '![](../Images/07-06.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-06.png)'
- en: Figure 7.6 To resize an image, use the `target_size` parameter.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 调整图像大小，使用`target_size`参数。
- en: Let’s now use a neural network to classify this image.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将使用神经网络来对这张图像进行分类。
- en: 7.2 Convolutional neural networks
  id: totrans-83
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 卷积神经网络
- en: Neural networks are a class of machine learning models for solving classification
    and regression problems. Our problem is a classification problem—we need to determine
    the category of an image.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是一类用于解决分类和回归问题的机器学习模型。我们的问题是分类问题——我们需要确定图像的类别。
- en: 'However, our problem is special: we’re dealing with images. This is why we
    need a special type of neural network—a convolutional neural network, which can
    extract visual patterns from an image and use them to make predictions.'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们的问题是特殊的：我们处理的是图像。这就是为什么我们需要一种特殊的神经网络类型——卷积神经网络，它可以从图像中提取视觉模式并使用它们进行预测。
- en: Pre-trained neural networks are available on the internet, so let’s see how
    we can use one of them for this project.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 预训练神经网络在互联网上可用，让我们看看我们如何可以使用其中一个模型来完成这个项目。
- en: 7.2.1 Using a pretrained model
  id: totrans-87
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.1 使用预训练模型
- en: Training a convolutional neural network from scratch is a time-consuming process
    and requires a lot of data and powerful hardware. It may take weeks of nonstop
    training for large datasets like ImageNet with 14 million images. (Check [image-net.org](http://www.image-net.org/)
    for more information.)
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从头开始训练卷积神经网络是一个耗时过程，需要大量数据和强大的硬件。对于像ImageNet这样的大型数据集（包含1400万张图像），可能需要几周不间断的训练。（更多信息请访问[image-net.org](http://www.image-net.org/)）
- en: 'Luckily, we don’t need to do it ourselves: we can use pretrained models. Usually,
    these models are trained on ImageNet and can be used for general-purpose image
    classification.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们不需要自己来做这件事：我们可以使用预训练模型。通常，这些模型是在ImageNet上训练的，可以用于通用图像分类。
- en: It’s very simple, and we don’t even need to download anything ourselves—Keras
    will take care of it automatically. We can use many different types of models
    (called *architectures*). You can find a good summary of available pretrained
    models in the official Keras documentation ([https://keras.io/api/applications/](https://keras.io/api/applications/)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这非常简单，我们甚至不需要自己下载任何东西——Keras会自动处理。我们可以使用许多不同类型的模型（称为*架构*）。您可以在官方Keras文档中找到可用预训练模型的良好总结（[https://keras.io/api/applications/](https://keras.io/api/applications/)）。
- en: 'For this chapter, we’ll use Xception, a relatively small model that has good
    performance. First, we need to import the model itself and some helpful functions:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本章，我们将使用Xception，这是一个相对较小的模型，但性能良好。首先，我们需要导入模型本身和一些有用的函数：
- en: '[PRE8]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We imported three things:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 我们导入了三样东西：
- en: '`Xception`: the actual model'
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Xception`: 实际模型'
- en: '`preprocess_input`: a function for preparing the image to be used by the model'
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`preprocess_input`: 准备图像以便模型使用的函数'
- en: '`decode_prediction`: a function for decoding the model’s prediction'
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`decode_prediction`: 解码模型预测的函数'
- en: 'Let’s load this model:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载这个模型：
- en: '[PRE9]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We specify two parameters here:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里指定了两个参数：
- en: '`weights`: We want to use a pretrained model from ImageNet.'
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`weights`: 我们希望使用从ImageNet预训练的模型。'
- en: '`imput_shape`: The size of the input images: height, width, and the number
    of channels. We resize the images to 299 × 299, and each image has three channels:
    red, green and blue.'
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`input_shape`: 输入图像的大小：高度、宽度和通道数。我们将图像调整大小到299 × 299，每个图像有三个通道：红色、绿色和蓝色。'
- en: When we load it for the first time, it downloads the actual model from the internet.
    After it’s done, we can use it.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们第一次加载它时，它会从互联网下载实际模型。完成后，我们就可以使用它了。
- en: 'Let’s test it on the image we saw previously. First, we load it using the `load_img`
    function:'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在之前看到的图像上测试它。首先，我们使用`load_img`函数加载它：
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The `img` variable is an `Image` object, which we need to convert to a NumPy
    array. It’s easy to do:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: '`img`变量是一个`Image`对象，我们需要将其转换为NumPy数组。这很容易做到：'
- en: '[PRE11]'
  id: totrans-106
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'This array should have the same shape as the image. Let’s check it:'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数组应该与图像具有相同的形状。让我们检查一下：
- en: '[PRE12]'
  id: totrans-108
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We see `(299,` `299,` `3)`. It contains three dimensions (figure 7.7):'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到`(299,` `299,` `3)`。它包含三个维度（图7.7）：
- en: 'The width of the image: 299'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像宽度：299
- en: 'The height of the image: 299'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像高度：299
- en: 'The number of channels: red, green, blue'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通道数：红色、绿色、蓝色
- en: '![](../Images/07-07.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-07.png)'
- en: Figure 7.7 After converting, an image becomes a NumPy array of shape width ×
    height × number of channels.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 转换后，图像变成一个形状为宽度 × 高度 × 通道数的NumPy数组。
- en: 'This matches the input shape we specified when loading the neural network.
    However, the model doesn’t expect to get just a single image. It gets a *batch*
    of images—several images put together in one array. This array should have four
    dimensions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 这与我们加载神经网络时指定的输入形状相匹配。然而，模型并不期望只得到一张单独的图像。它得到的是一个*批次*的图像——几个图像组合在一个数组中。这个数组应该有四个维度：
- en: The number of images
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 图像数量
- en: The width
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 宽度
- en: The height
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高度
- en: The number of channels
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通道数
- en: 'For example, for 10 images, the shape is `(10,` `299,` `299,` `3)`. Because
    we have just one image, we need to create a batch with this single image:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于10张图像，其形状是`(10,` `299,` `299,` `3)`。因为我们只有一张图像，我们需要创建一个包含这张单张图像的批次：
- en: '[PRE13]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Note If we had several images, for example, `x`, `y` and `z`, we’d write
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果我们有多个图像，例如，`x`，`y`和`z`，我们会写成
- en: '[PRE14]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Let’s check its shape:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们检查它的形状：
- en: '[PRE15]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: As we see, it’s `(1,` `299,` `299,` `3)`—it’s one image of size 299 × 299 with
    three channels.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见，它是`(1,` `299,` `299,` `3)`——它是一张大小为299 × 299且有三个通道的图像。
- en: 'Before we can apply the model to our image, we need to prepare it with the
    `preprocess_input` function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们可以将模型应用于我们的图像之前，我们需要使用`preprocess_input`函数来准备它：
- en: '[PRE16]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This function converts the integers between 0 and 255 in the original array
    to numbers between –1 and 1.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 这个函数将原始数组中的0到255之间的整数转换为-1到1之间的数字。
- en: Now, we’re ready to use the model.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好使用模型了。
- en: 7.2.2 Getting predictions
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2.2 获取预测
- en: 'To apply the model, use the `predict` method:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 要应用模型，使用`predict`方法：
- en: '[PRE17]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Let’s take a look at this array:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看这个数组：
- en: '[PRE18]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This array is quite large—it contains 1,000 elements (figure 7.8).
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数组相当大——它包含1000个元素（图7.8）。
- en: '![](../Images/07-08.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-08.png)'
- en: Figure 7.8 The output of the pretrained Xception model
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 预训练的Xception模型的输出
- en: This Xception model predicts whether an image belongs to one of 1,000 classes,
    so each element in the prediction array is the probability of belonging to one
    of these classes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 这个Xception模型预测图像是否属于1000个类别之一，所以预测数组中的每个元素都是属于这些类别之一的概率。
- en: 'We don’t know what these classes are, so it’s difficult to make sense from
    this prediction just by looking at the numbers. Luckily, we can use a function,
    `decode_` `predictions`, that decodes the prediction into meaningful class names:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不知道这些类别是什么，所以仅通过查看数字很难理解这个预测。幸运的是，我们可以使用一个函数，`decode_` `predictions`，将预测解码成有意义的类别名称：
- en: '[PRE19]'
  id: totrans-141
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'It shows the top five most likely classes for this image:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了这张图像最有可能的前五个类别：
- en: '[PRE20]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: Not quite the result we expected. Most likely, images like this T-shirt are
    not common in ImageNet, and that’s why the result isn’t useful for our problem.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 并非我们预期的结果。很可能是像这样的T恤在ImageNet中并不常见，这就是为什么结果对我们问题没有用。
- en: Even though these results aren’t particularly helpful for us, we can use this
    neural network as a base model for solving our problem.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这些结果对我们来说并不特别有帮助，但我们仍然可以将这个神经网络作为解决我们问题的基模型。
- en: To understand how we can do it, we should first get a feeling for how convolutional
    neural networks work. Let’s see what happens inside the model when we invoke the
    `predict` method.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 要了解我们如何做到这一点，我们首先应该对卷积神经网络的工作方式有一个感觉。让我们看看当我们调用`predict`方法时模型内部发生了什么。
- en: 7.3 Internals of the model
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 模型的内部结构
- en: All neural networks are organized in layers. We take an image, pass it through
    all the layers, and, at the end, get the predictions (figure 7.9).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 所有神经网络都是按层组织的。我们取一个图像，通过所有层，最后得到预测（图7.9）。
- en: '![](../Images/07-09.png)'
  id: totrans-149
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-09.png)'
- en: Figure 7.9 A neural network consists of multiple layers.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 神经网络由多个层组成。
- en: Usually, a model has a lot of layers. For example, the Xception model we use
    here has 71 layers. That’s why these neural networks are called “deep” neural
    networks—because they have many layers.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，一个模型有很多层。例如，我们这里使用的Xception模型有71层。这就是为什么这些神经网络被称为“深度”神经网络——因为它们有很多层。
- en: For a convolutional neural network, the most important layers are
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 对于卷积神经网络来说，最重要的层是
- en: Convolutional layers
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积层
- en: Dense layers
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 密集层
- en: First, let’s take a look at convolutional layers.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们看看卷积层。
- en: 7.3.1 Convolutional layers
  id: totrans-156
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 卷积层
- en: Even though “convolutional layer” sounds complicated, it’s nothing more than
    a set of *filters*—small “images” with simple shapes like stripes (figure 7.10).
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 即使“卷积层”听起来很复杂，它也不过是**一组过滤器**——形状简单的“图像”，如条纹（图7.10）。
- en: '![](../Images/07-10.png)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-10.png)'
- en: Figure 7.10 Examples of filters for a convolutional layer (not from a real network)
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 卷积层的过滤器示例（非真实网络）
- en: The filters in a convolutional layer are learned by the model during training.
    However, because we are using a pretrained neural network, we don’t need to worry
    about it; we already have the filters.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层中的过滤器是在训练过程中由模型学习的。然而，因为我们使用的是预训练的神经网络，所以我们不需要担心它；我们已经有过滤器了。
- en: To apply a convolutional layer to a picture, we slide each filter across this
    image. For example, we can slide it from left to right and from top to bottom
    (figure 7.11).
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 要将卷积层应用于图片，我们将每个过滤器在这个图像上滑动。例如，我们可以从左到右和从上到下滑动（图7.11）。
- en: '![](../Images/07-11.png)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11](../Images/07-11.png)'
- en: Figure 7.11 To apply a filter, we slide it over an image.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 应用过滤器时，我们将其在图像上滑动。
- en: While sliding, we compare the content of the filter with the content of the
    image under the filter. For each comparison, we record the degree of similarity.
    This way, we get a *feature map*—an array with numbers, where a large number means
    a match between the filter and the image, and a low number means no match (figure 7.12).
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在滑动过程中，我们比较过滤器的内容与过滤器下图像的内容。对于每次比较，我们记录相似度。这样，我们得到一个*特征图*——一个包含数字的数组，其中大数字表示过滤器和图像之间的匹配，而小数字表示不匹配（图7.12）。
- en: '![](../Images/07-12.png)'
  id: totrans-165
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12](../Images/07-12.png)'
- en: Figure 7.12 A feature map is a result of applying a filter to an image. A high
    value in the map corresponds to areas with a high degree of similarity between
    the image and the filter.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 特征图是应用过滤器到图像的结果。图中高值对应于图像与过滤器之间具有高度相似性的区域。
- en: So, a feature map tells us where on the image we can find the shape from the
    filter.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，特征图告诉我们可以在图像的哪个位置找到过滤器中的形状。
- en: One convolutional layer consists of many filters, so we actually get multiple
    feature maps—one for each filter (figure 7.13).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 一个卷积层由许多过滤器组成，因此我们实际上得到了多个特征图——每个过滤器对应一个（图7.13）。
- en: '![](../Images/07-13.png)'
  id: totrans-169
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13](../Images/07-13.png)'
- en: 'Figure 7.13 Each convolutional layer contains many filters, so we get a set
    of feature maps: one for each filter that we use.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 每个卷积层包含许多过滤器，因此我们得到一组特征图：每个我们使用的过滤器对应一个。
- en: Now we can take the output of one convolutional layer and use it as the input
    to the next layer.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以将一个卷积层的输出用作下一层的输入。
- en: From the previous layer we know the location of different stripes and other
    simple shapes. When two simple shapes occur in the same location, they form more
    complex patterns—crosses, angles, or circles.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 从前一层我们知道不同条纹和其他简单形状的位置。当两个简单形状出现在相同的位置时，它们会形成更复杂的图案——十字形、角度或圆形。
- en: 'That’s what the filters of the next layer do: they combine shapes from the
    previous layer into more complex structures. The deeper we go down the network,
    the more complex patterns the network can recognize (figure 7.14).'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是下一层的过滤器所做的事情：它们将前一层中的形状组合成更复杂的结构。网络越深，网络能够识别的复杂模式就越多（图7.14）。
- en: '![](../Images/07-14.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图7.14](../Images/07-14.png)'
- en: Figure 7.14 Deeper convolutional layers can detect progressively more complex
    features of the image.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 深层卷积层可以检测图像的越来越复杂的特征。
- en: We repeat this process to detect more and more complex shapes. This way, the
    network “learns” some distinctive features of the image. For clothes, it can be
    short or long sleeves or the type of neck. For animals, it can be pointy or floppy
    ears or the presence of whiskers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 我们重复这个过程来检测越来越多复杂的形状。这样，网络“学习”了图像的一些独特特征。对于衣服，可能是短袖或长袖或领口的类型。对于动物，可能是尖耳朵或软耳朵或胡须的存在。
- en: 'At the end, we get a vector representation of an image: a one-dimensional array,
    where each position corresponds to some high-level visual features. Some parts
    of the array may correspond to sleeves, whereas other parts represent ears and
    whiskers. At this level, it’s usually difficult to make sense from these features,
    but they have enough discriminative power to distinguish between a T-shirt and
    pants or between a cat and a dog.'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到一个图像的向量表示：一个一维数组，其中每个位置对应某些高级视觉特征。数组的某些部分可能对应袖子，而其他部分则代表耳朵和胡须。在这个层面上，通常很难从这些特征中得出意义，但它们具有足够的区分能力，可以区分T恤和裤子，或猫和狗。
- en: Now we need to use this vector representation to combine these high-level features
    and arrive at the final decision. For that, we use a different kind of layers—dense
    layers.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要使用这种向量表示来组合这些高级特征，并得出最终决策。为此，我们使用不同类型的层——密集层。
- en: 7.3.2 Dense layers
  id: totrans-179
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 密集层
- en: Dense layers process the vector representation of an image and translate these
    visual features to the actual class—T-shirt, dress, jacket, or other class (figure 7.15).
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 密集层处理图像的向量表示，并将这些视觉特征转换为实际的类别——T恤、连衣裙、夹克或其他类别（图7.15）。
- en: '![](../Images/07-15.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![图7.13](../Images/07-15.png)'
- en: Figure 7.15 Convolutional layers transform an image into its vector representation,
    and dense layers translate the vector representation into the actual label.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15 卷积层将图像转换为向量表示，密集层将向量表示转换为实际标签。
- en: To understand how it works, let’s take a step back and think how we could use
    logistic regression for classifying images.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了理解它是如何工作的，让我们退一步思考如何使用逻辑回归对图像进行分类。
- en: Suppose we want to build a binary classification model for predicting whether
    an image is a T-shirt. In this case, the input to logistic regression is the vector
    representation of an image—a feature vector *x*.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要构建一个二分类模型来预测图像是否为T恤。在这种情况下，逻辑回归的输入是图像的向量表示——特征向量 *x*。
- en: 'From chapter 3, we know that to make the prediction, we need to combine the
    features in *x* with the weights vector *w* and then apply the sigmoid function
    to get the final prediction:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 从第3章，我们知道为了进行预测，我们需要将 *x* 中的特征与权重向量 *w* 结合，然后应用sigmoid函数以获得最终的预测：
- en: sigmoid(*x^T* *w*)
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid(*x^T* *w*)
- en: We can show it visually by taking all the components of the vector *x* and connecting
    them to the output—the probability of being a T-shirt (figure 7.16).
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将向量 *x* 的所有成分连接到输出——成为T恤的概率（图7.16）来直观地展示它。
- en: '![](../Images/07-16.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-16.png)'
- en: 'Figure 7.16 Logistic regression: we take all the components of the feature
    vector *x* and combine them to get the prediction.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 逻辑回归：我们将特征向量 *x* 的所有成分组合起来以获得预测。
- en: What if we need to make predictions for multiple classes? For example, we may
    want to know if we have an image of a T-shirt, shirt, or dress. In this case,
    we can build multiple logistic regressions—one for each class (figure 7.17).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们需要对多个类别进行预测呢？例如，我们可能想知道是否有一张T恤、衬衫或连衣裙的图片。在这种情况下，我们可以为每个类别构建多个逻辑回归模型——每个类别一个（图7.17）。
- en: '![](../Images/07-17.png)'
  id: totrans-191
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-17.png)'
- en: Figure 7.17 To predict multiple classes, we train multiple logistic regression
    models.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17 为了预测多个类别，我们训练多个逻辑回归模型。
- en: By putting together multiple logistic regression models, we just created a small
    neural network!
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 通过组合多个逻辑回归模型，我们仅仅创建了一个小型神经网络！
- en: To make it visually simpler, we can combine the outputs into one layer—the output
    layer (figure 7.18).
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使它看起来更简单，我们可以将输出合并到一个层——输出层（图7.18）。
- en: '![](../Images/07-18.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-18.png)'
- en: Figure 7.18 Multiple logistic regressions put together form a small neural network.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 多个逻辑回归模型组合在一起形成一个小型神经网络。
- en: When we have 10 classes we want to predict, we have 10 elements in the output
    layer. To make a prediction, we look at each element of the output layer and take
    the one with the highest score.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们想要预测10个类别时，输出层有10个元素。为了进行预测，我们查看输出层的每个元素，并选择得分最高的一个。
- en: 'In this case, we have a network with one layer: the layer that converts the
    input to the output.'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们有一个网络，只有一个层：将输入转换为输出的层。
- en: This layer is called a *dense layer*. It’s “dense” because it connects each
    element of the input with all the elements of its output. For this reason, these
    layers are sometimes called “fully connected” (figure 7.19).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这一层被称为 *密集层*。它被称为“密集”是因为它将输入的每个元素与其输出的所有元素相连接。因此，这些层有时被称为“全连接”（图7.19）。
- en: '![](../Images/07-19.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-19.png)'
- en: Figure 7.19 A dense layer connects every element of its input with every element
    of its output.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 密集层将其输入的每个元素与其输出的每个元素相连接。
- en: However, we don’t have to stop at just one output layer. We can add more layers
    between the input and the final output (figure 7.20).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不必仅仅停留在只有一个输出层。我们可以在输入和最终输出之间添加更多层（图7.20）。
- en: '![](../Images/07-20.png)'
  id: totrans-203
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-20.png)'
- en: 'Figure 7.20 A neural network with two layers: one inner layer and one output
    layer'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 具有两个层的神经网络：一个内部层和一个输出层
- en: So, when we invoke `predict`, the image first goes through a series of convolutional
    layers. This way, we extract the vector representation of this image. Next, this
    vector representation goes through a series of dense layers, and we get the final
    prediction (figure 7.21).
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，当我们调用 `predict` 时，图像首先通过一系列卷积层。这样，我们提取了该图像的向量表示。接下来，这个向量表示通过一系列密集层，我们得到最终的预测（图7.21）。
- en: '![](../Images/07-21.png)'
  id: totrans-206
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-21.png)'
- en: Figure 7.21 In a convolutional neural network, an image first goes through a
    series of convolutional layers, and then through a series of dense layers.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 在卷积神经网络中，图像首先通过一系列卷积层，然后通过一系列密集层。
- en: Note In this book, we give a simplified and high-level overview of the internals
    of convolutional neural networks. Many other layers exist in addition to convolutional
    layers and dense layers. For a more in-depth introduction to this topic, check
    the CS231n notes ([cs231n.github.io/convolutional-networks](https://cs231n.github.io/convolutional-networks/)).
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在这本书中，我们给出了卷积神经网络内部结构的简化和高层次概述。除了卷积层和密集层之外，还存在许多其他层。对于这个主题的更深入介绍，请查看CS231n笔记([cs231n.github.io/convolutional-networks](https://cs231n.github.io/convolutional-networks/))。
- en: Now let’s get back to code and see how we can adjust a pretrained neural network
    for our project.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们回到代码，看看我们如何调整预训练神经网络以适应我们的项目。
- en: 7.4 Training the model
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 训练模型
- en: 'Training a convolutional neural network takes a lot of time and requires a
    lot of data. But there’s a shortcut: we can use *transfer learning*, an approach
    where we adapt a pretrained model to our problem.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 训练卷积神经网络需要大量时间和数据。但有一个捷径：我们可以使用*迁移学习*，这是一种将预训练模型适应我们问题的方法。
- en: 7.4.1 Transfer learning
  id: totrans-212
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 迁移学习
- en: The difficulty in training usually comes from convolutional layers. To be able
    to extract a good vector representation from an image, the filters need to learn
    good patterns. For that, the network has to see many different images—the more,
    the better. But once we have a good vector representation, training dense layers
    is relatively easy.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 训练的困难通常来自卷积层。为了能够从图像中提取良好的向量表示，过滤器需要学习良好的模式。为此，网络需要看到许多不同的图像——越多越好。但一旦我们有了良好的向量表示，训练密集层就相对容易了。
- en: This means that we can take a neural network pretrained on ImageNet and use
    it for solving our problem. This model has already learned good filters. So, we
    take this model and keep the convolutional layers, but drop the dense layers and
    instead train new ones (figure 7.22).
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这意味着我们可以使用在ImageNet上预训练的神经网络来解决我们的问题。这个模型已经学习了良好的过滤器。因此，我们保留这个模型的卷积层，但丢弃密集层，并训练新的（如图7.22所示）。
- en: '![](../Images/07-22.png)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-22.png)'
- en: Figure 7.22 To adapt a pretrained model to a new domain, we keep the old convolutional
    layers but train new dense layers.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 为了将预训练模型适应新领域，我们保留旧的卷积层，但训练新的密集层。
- en: In this section, we do exactly that. But before we can start training, we need
    to get our dataset ready.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们正是这样做的。但在我们开始训练之前，我们需要准备好我们的数据集。
- en: 7.4.2 Loading the data
  id: totrans-218
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 加载数据
- en: 'In previous chapters, we loaded the entire dataset into memory and used it
    to get *X*—the matrix with features. With images, it’s more difficult: we may
    not have enough memory to keep all the images.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们将整个数据集加载到内存中，并使用它来获取*X*——包含特征的矩阵。对于图像来说，这更困难：我们可能没有足够的内存来存储所有图像。
- en: 'Keras comes with a solution—`ImageDataGenerator`. Instead of loading the entire
    dataset into memory, it loads the images from disk in small batches. Let’s use
    it:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: Keras提供了一个解决方案——`ImageDataGenerator`。它不是将整个数据集加载到内存中，而是从磁盘以小批次加载图像。让我们来使用它：
- en: '[PRE21]'
  id: totrans-221
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Applies the preprocess_input function to each image
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 对每张图像应用`preprocess_input`函数
- en: We already know that images need to be preprocessed using the `preprocess_input`
    function. That’s why we need to tell `ImageDataGenerator` how the data should
    be prepared.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道图像需要使用`preprocess_input`函数进行预处理。这就是为什么我们需要告诉`ImageDataGenerator`数据应该如何准备。
- en: 'We have a generator now, so we just need to point it to the directory with
    the data. For that, use the `flow_from_directory` method:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个生成器，所以我们只需要将其指向包含数据的目录。为此，使用`flow_from_directory`方法：
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Loads all the images from the train directory
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从训练目录中加载所有图像
- en: ❷ Resizes the images to 150 × 150
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将图像调整大小到150 × 150
- en: ❸ Loads the images in batches of 32 images
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 以32张图像的批次加载图像
- en: For our initial experiments, we use small images of size 150 × 150\. This way,
    it’s faster to train the model. Also, the small size makes it possible to use
    a laptop for training.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的初步实验中，我们使用150 × 150大小的小图像。这样，训练模型会更快。此外，小尺寸使得使用笔记本电脑进行训练成为可能。
- en: We have 10 classes of clothing in our dataset, and images of each class are
    stored in a separate directory. For example, all T-shirts are stored in the t-shirt
    folder. The generator can use the folder structure to infer the label for each
    image.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集中有10类服装，每类的图像都存储在单独的目录中。例如，所有T恤都存储在T恤文件夹中。生成器可以使用文件夹结构来推断每张图像的标签。
- en: 'When we execute the cell, it informs us how many images there are in the train
    dataset and how many classes:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们执行单元格时，它会告诉我们训练数据集中有多少张图像以及有多少个类别：
- en: '[PRE23]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now we repeat the same process for the validation dataset:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们对验证数据集重复相同的过程：
- en: '[PRE24]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: Like previously, we use the train dataset for training the model and the validation
    dataset for selecting the best parameters.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 像之前一样，我们使用训练数据集来训练模型，使用验证数据集来选择最佳参数。
- en: We have loaded the data, and now we’re ready to train a model.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经加载了数据，现在我们准备训练一个模型。
- en: 7.4.3 Creating the model
  id: totrans-237
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 创建模型
- en: First, we need to load the base model—this is the pretrained model that we’re
    using for extracting the vector representation from images. Like previously, we
    also use Xception, but this time, we include only the part with pretrained convolutional
    layers. After that, we add our own dense layers.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们需要加载基础模型——这是我们用于从图像中提取向量表示的预训练模型。像之前一样，我们也使用Xception，但这次我们只包括预训练的卷积层部分。之后，我们添加我们自己的密集层。
- en: 'So, let’s create the base model:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们创建基础模型：
- en: '[PRE25]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Uses the model pretrained on ImageNet
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用在ImageNet上预训练的模型
- en: ❷ Keeps only the convolutional layers
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 仅保留卷积层
- en: ❸ Images should be 150 × 150 with three channels.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 图像应为150 × 150像素，具有三个通道。
- en: 'Note the `include_top` parameter: this way, we explicitly specify that we’re
    not interested in the dense layers of the pretrained neural network, only in the
    convolutional layers. In Keras terminology, the “top” is the set of final layers
    of the network (figure 7.23).'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 注意`include_top`参数：这样，我们明确指定我们对其预训练神经网络的密集层不感兴趣，只对卷积层感兴趣。在Keras术语中，“top”是指网络的最终层集（图7.23）。
- en: '![](../Images/07-23.png)'
  id: totrans-245
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-23.png)'
- en: Figure 7.23 In Keras, the input to the network is on the bottom and the output
    is on the top, so `include_top=False` means “don’t include the final dense layers.”
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.23 在Keras中，网络的输入在底部，输出在顶部，所以`include_top=False`意味着“不包括最终的密集层”。
- en: 'We don’t want to train the base model; attempting to do so will destroy all
    the filters. So, we “freeze” the base model by setting the `trainable` parameter
    to False:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不想训练基础模型；尝试这样做将破坏所有过滤器。因此，我们将基础模型“冻结”，通过将`trainable`参数设置为False：
- en: '[PRE26]'
  id: totrans-248
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now let’s build the clothing classification model:'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们构建服装分类模型：
- en: '[PRE27]'
  id: totrans-250
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Input images should be 150 × 150 with three channels.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输入图像应为150 × 150像素，具有三个通道。
- en: ❷ Uses the base_model to extract the high-level features.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用base_model提取高级特征。
- en: '❸ Extracts the vector representation: converts the output of base_model to
    a vector'
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 提取向量表示：将base_model的输出转换为向量
- en: '❹ Adds a dense layer of size 10: one element for each class'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 添加一个大小为10的密集层：每个类别一个元素
- en: ❺ Combines the inputs and the outputs into a Keras model
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将输入和输出组合成一个Keras模型
- en: The way we build the model is called the “functional style.” It may be confusing
    at first, so let’s take a look at each line individually.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们构建模型的方式被称为“功能风格”。一开始可能会有些困惑，所以让我们逐行查看。
- en: 'First, we specify the input and the size of the arrays we expect:'
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们指定输入和期望的数组大小：
- en: '[PRE28]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Next, we create the base model:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建基础模型：
- en: '[PRE29]'
  id: totrans-260
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Even though `base_model` is already a model, we use it as a function and give
    it two parameters—`inputs`, and `training=False`:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管`base_model`已经是一个模型，但我们将其用作函数，并给它两个参数——`inputs`和`training=False`：
- en: The first parameter says what will be the input to `base_model`. It will come
    from `inputs`.
  id: totrans-262
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数说明`base_model`的输入将是什么。它将从`inputs`中来。
- en: The second parameter (`training=False`) is optional and says that we don’t want
    to train the base model.
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第二个参数（`training=False`）是可选的，表示我们不希望训练基础模型。
- en: 'The result is `base`, which is a *functional component* (like `base_model`)
    that we can combine with other components. We use it as the input to the next
    layer:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是`base`，这是一个*功能组件*（类似于`base_model`），我们可以将其与其他组件结合使用。我们将其用作下一层的输入：
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we create a pooling layer—a special construction that allows us to convert
    the output of a convolutional layer (a 3-D array) into a vector (a one-dimensional
    array).
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们创建一个池化层——一种特殊的结构，允许我们将卷积层的输出（一个三维数组）转换为向量（一个一维数组）。
- en: After creating it, we immediately invoke it with `base` as the argument. This
    way, we say that the input to this layer comes from `base`.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 创建后，我们立即用`base`作为参数调用它。这样，我们说这个层的输入来自`base`。
- en: 'This may be a bit confusing because we create a layer and immediately connect
    it to base. We can rewrite it to make it simpler to understand:'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能有点令人困惑，因为我们创建了一个层，然后立即将其连接到基础。我们可以重写它以使其更容易理解：
- en: '[PRE31]'
  id: totrans-269
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Creates a pooling layer first
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 首先创建一个池化层
- en: ❷ Connects it to base
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 连接到基础模型
- en: 'As a result, we get `vector`. This is another functional component that we
    connect to the next layer—a dense layer:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 结果，我们得到`vector`。这是另一个功能组件，我们将其连接到下一层——一个密集层：
- en: '[PRE32]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Similarly, we first create the layer, and then connect it to `vector`. For now,
    we create a network with only one dense layer. It’s enough to get started.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，我们首先创建层，然后将其连接到`vector`。目前，我们创建了一个只有一个密集层的网络。这足以开始。
- en: Now the result is `outputs`—the final result that we want to get out of the
    network.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在结果是`outputs`——我们想要从网络中获取的最终结果。
- en: 'So, in our case, the data comes into `inputs` and goes out of `outputs`. We
    just need to do one final step—wrap both `inputs` and `outputs` into a `Model`
    class:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的情况下，数据进入`inputs`并从`outputs`流出。我们只需要进行最后一步——将`inputs`和`outputs`都包裹在一个`Model`类中：
- en: '[PRE33]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We need to specify two parameters here:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在这里指定两个参数：
- en: What the model will get as input, which is `inputs` in our case
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型将获取的输入，在我们的例子中是`inputs`
- en: What the output of the model is, which is `outputs`
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的输出是什么，即`outputs`
- en: Let’s take a step back and look at the model definition code again, following
    the flow of data from `inputs` to `outputs` (figure 7.24).
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步再次查看模型定义代码，按照从`inputs`到`outputs`的数据流（图7.24）。
- en: '![](../Images/07-24.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-24.png)'
- en: 'Figure 7.24 The flow of data: an image goes to `inputs`, then `base_model`
    converts it to `base`, then pooling converts it to `vector`, and then a dense
    layer converts it to `output`. At the end, `inputs` and `outputs` go to a Keras
    model.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.24 数据流：一个图像进入`inputs`，然后`base_model`将其转换为`base`，接着池化层将其转换为`vector`，然后密集层将其转换为`output`。最后，`inputs`和`outputs`进入一个Keras模型。
- en: To make it easier to visualize, we can think of every line of code as a block,
    which gets the data from the previous block, transforms it, and passes to the
    next block (figure 7.25).
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使其更容易可视化，我们可以将每一行代码视为一个块，它从上一个块获取数据，对其进行转换，并将其传递给下一个块（图7.25）。
- en: '![](../Images/07-25.png)'
  id: totrans-285
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-25.png)'
- en: 'Figure 7.25 The flow of data: each line of Keras code as a block'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.25 数据流：每一行Keras代码作为一个块
- en: So, we have created a model that can take in an image, get the vector representation
    using the base model, and make the final prediction with a dense layer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们创建了一个模型，它可以接收一个图像，使用基础模型获取其向量表示，并通过密集层进行最终预测。
- en: Let’s train it now.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们开始训练它。
- en: 7.4.4 Training the model
  id: totrans-289
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 训练模型
- en: 'We have specified the model: the input, the elements of the model (the base
    model, the pooling layer), and the final output layer.'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经指定了模型：输入、模型的元素（基础模型、池化层）以及最终的输出层。
- en: Now we need to train it. For that, we need an *optimizer*, which adjusts the
    weights of a network to make it better at doing its task.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们需要训练它。为此，我们需要一个*优化器*，它调整网络的权重以使其在执行任务时表现得更好。
- en: We won’t cover the details of how optimizers work—that’s beyond the scope for
    this book, and it’s not required to finish the project. But if you’d like to learn
    more about them, check the CS231n notes ([https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/)).
    You can see the list of available optimizers in the official documentation for
    Keras ([https://keras.io/api/optimizers/](https://keras.io/api/optimizers/)).
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会详细介绍优化器的工作原理——这超出了本书的范围，并且完成项目不需要它。但如果你想了解更多关于它们的信息，请查看CS231n笔记([https://cs231n.github.io/neural-networks-3/](https://cs231n.github.io/neural-networks-3/))。你可以在Keras的官方文档中查看可用的优化器列表([https://keras.io/api/optimizers/](https://keras.io/api/optimizers/))。
- en: For our project, we will use the Adam optimization algorithm—a good default
    choice, and in most cases, using it is sufficient.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的项目，我们将使用Adam优化算法——这是一个良好的默认选择，并且在大多数情况下，使用它就足够了。
- en: 'Let’s create it:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建它：
- en: '[PRE34]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'Adam requires one parameter: the learning rate, which specifies how fast our
    network learns.'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: Adam需要一个参数：学习率，它指定了我们的网络学习速度有多快。
- en: The learning rate may significantly affect the quality of our network. If we
    set it too high, the network learns too fast and may accidentally skip some important
    details. In this case, the predictive performance is not optimal. If we set it
    too low, the network takes too long to train, so the training process is highly
    ineffective.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率可能会显著影响我们网络的质量。如果我们设置得太高，网络学习得太快，可能会意外地跳过一些重要的细节。在这种情况下，预测性能不是最优的。如果我们设置得太低，网络训练时间过长，因此训练过程非常低效。
- en: We will later adjust this parameter. For now, we set it to 0.01—a good default
    value to start with.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们稍后会调整这个参数。目前，我们将其设置为0.01——一个良好的默认值以开始。
- en: To train a model, the optimizer needs to know whether the model is doing well.
    For that, it uses a loss function, which becomes smaller as the network becomes
    better. The goal of the optimizer is to minimize this loss.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练一个模型，优化器需要知道模型表现如何。为此，它使用一个损失函数，随着网络的改善，这个损失函数会变小。优化器的目标是使这个损失最小化。
- en: 'The `keras.losses` package offers many different losses. Here’s a list of the
    most important ones:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: '`keras.losses`包提供了许多不同的损失函数。以下是最重要的几个：'
- en: '`BinaryCrossentropy`: For training a binary classifier'
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BinaryCrossentropy`：用于训练二元分类器'
- en: '`CategoricalCrossentropy`: For training a classification model with multiple
    classes'
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`CategoricalCrossentropy`：用于训练具有多个类别的分类模型'
- en: '`MeanSquaredError`: For training a regression model'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`MeanSquaredError`：用于训练回归模型'
- en: 'Because we need to classify clothing into 10 different classes, we use the
    categorical cross-entropy loss:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们需要将衣物分类到10个不同的类别中，所以我们使用分类交叉熵损失：
- en: '[PRE35]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'For this loss, we specify one parameter: `from_logits=True`. We need to do
    this because the last layer of our network outputs raw scores (called “logits”),
    not probabilities. The official documentation recommends doing this for numerical
    stability ([https://www .tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy)).'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个损失函数，我们指定了一个参数：`from_logits=True`。我们需要这样做，因为我们的网络最后一层输出的是原始分数（称为“logits”），而不是概率。官方文档建议这样做以提高数值稳定性（[https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy](https://www.tensorflow.org/api_docs/python/tf/keras/losses/CategoricalCrossentropy))。
- en: 'Note Alternatively, we could define the last layer of the network like this:'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：或者，我们也可以这样定义网络的最后一层：
- en: '[PRE36]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'In this case, we explicitly tell the network to output probabilities: softmax
    is similar to sigmoid but for multiple classes. Then the output is not “logits”
    anymore, so we can drop this parameter:'
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们明确告诉网络输出概率：softmax类似于sigmoid，但适用于多个类别。因此，输出不再是“logits”，所以我们可以省略这个参数：
- en: '[PRE37]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Now let’s put the optimizer and the loss together. For that, we use the `compile`
    method of our model:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将优化器和损失函数结合起来。为此，我们使用模型的`compile`方法：
- en: '[PRE38]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'In addition to the optimizer and the loss, we also specify metrics we want
    to track during training. We’re interested in accuracy: the percentage of images
    with correct predictions.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 除了优化器和损失函数，我们还指定了在训练期间想要跟踪的指标。我们感兴趣的是准确率：正确预测的图像百分比。
- en: 'Our model is ready for training! To do it, use the `fit` method:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们模型已经准备好进行训练了！要执行训练，请使用`fit`方法：
- en: '[PRE39]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'We specify three parameters:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 我们指定了三个参数：
- en: '`train_ds`: The dataset for training'
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`train_ds`：训练数据集'
- en: '`epochs`: The number of times it will go over the training data'
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`epochs`：模型将遍历训练数据的次数'
- en: '`validation_data`: The dataset for evaluation'
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`validation_data`：用于评估的数据集'
- en: One iteration over the entire training dataset is called an *epoch*. The more
    iterations we do, the better the network learns the training dataset.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 对整个训练数据集进行一次迭代称为一个*epoch*。我们进行的迭代越多，网络对训练数据集的学习就越好。
- en: At some point, it can learn the dataset so well that it starts overfitting.
    To know when this happens, we need to monitor the performance of our model on
    the validation dataset. That’s why we specify the `validation_data` parameter.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个时刻，模型可能会对数据集学习得非常好，以至于开始过拟合。为了知道何时发生这种情况，我们需要监控模型在验证数据集上的性能。这就是为什么我们指定了`validation_data`参数。
- en: 'When we start training, Keras informs us about the progress:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们开始训练时，Keras会告诉我们进度：
- en: '[PRE40]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: From that we can see
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 从这里我们可以看到
- en: 'The speed of training: how long each epoch takes.'
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度：每个epoch所需的时间。
- en: The accuracy on the train and validation datasets. We should monitor the accuracy
    on the validation set to make sure the model doesn’t start overfitting. For example,
    if the validation accuracy decreases for multiple epochs, it’s a sign of overfitting.
  id: totrans-326
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证数据集上的准确率。我们应该监控验证集上的准确率，以确保模型不会开始过拟合。例如，如果验证准确率在多个epoch中下降，这可能是一个过拟合的迹象。
- en: The loss on training and validation. We’re not interested in loss—it’s less
    intuitive and the values are harder to interpret.
  id: totrans-327
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练和验证的损失。我们不太关心损失——它不太直观，数值也难以解释。
- en: Note Your results will likely be different. The overall predictive performance
    of the model should be similar, but the exact numbers will not be the same. With
    neural networks, it’s a lot more difficult to ensure perfect reproducibility,
    even with fixing random seeds.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您得到的结果可能会有所不同。模型的总体预测性能应该相似，但具体的数字可能不会相同。在使用神经网络时，即使固定随机种子，确保完美的可重复性也变得更加困难。
- en: As you can see, the model quickly becomes 99% accurate on the train dataset,
    but the score on validation stays around 80% for all the epochs (figure 7.26).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，模型在训练数据集上的准确率迅速达到99%，但验证分数在所有epoch中都保持在80%左右（图7.26）。
- en: '![](../Images/07-26.png)'
  id: totrans-330
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-26.png)'
- en: Figure 7.26 The accuracy on the train and validation datasets evaluated after
    each epoch
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26 每个epoch后评估的训练和验证数据集上的准确率
- en: The perfect accuracy on the train data doesn’t necessarily mean that our model
    overfits, but it’s a good sign that we should adjust the learning rate parameter.
    We have previously mentioned that it’s an important parameter, so let’s tune it
    now.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 训练数据上的完美准确率并不一定意味着我们的模型过拟合，但这是一个我们应该调整学习率参数的好迹象。我们之前提到这是一个重要的参数，所以现在让我们调整它。
- en: Exercise 7.1
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 练习7.1
- en: Transfer learning is the process of using a pretrained model (a base model)
    for converting an image to its vector representation and then training another
    model on top of it.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 迁移学习是将预训练模型（基础模型）用于将图像转换为它的向量表示，然后在上面训练另一个模型的过程。
- en: a) True
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: a) 正确
- en: b) False
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: b) 错误
- en: 7.4.5 Adjusting the learning rate
  id: totrans-337
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.5 调整学习率
- en: 'We have started with a learning rate of 0.01\. It’s a good starting point,
    but it’s not necessarily the best rate: we have seen that our model learns too
    fast and after a few epochs predicts the train set with 100% accuracy.'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始时使用的学习率是0.01。这是一个好的起点，但并不一定是最佳的学习率：我们发现我们的模型学习得太快，在几个epoch之后就能以100%的准确率预测训练集。
- en: Let’s experiment and try other values for this parameter.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进行实验，尝试为这个参数尝试其他值。
- en: First, to make it easier, we should put the logic for model creating in a separate
    function. This function takes learning rate as a parameter.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，为了简化，我们应该将模型创建的逻辑放在一个单独的函数中。这个函数将学习率作为参数。
- en: Listing 7.1 A function for creating a model
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列7.1 创建模型的函数
- en: '[PRE41]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'We’ve tried 0.01, so let’s try 0.001:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经尝试了0.01，所以让我们尝试0.001：
- en: '[PRE42]'
  id: totrans-344
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'We can also try an even smaller value of 0.0001:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以尝试更小的值0.0001：
- en: '[PRE43]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: As we see (figure 7.27), for 0.001, the training accuracy doesn’t go up as fast
    as with 0.01, but with 0.0001 it goes up very slowly. The network in this case
    learns too slow—it *underfits*.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们所见（图7.27），对于0.001，训练准确率上升的速度不如0.01快，但使用0.0001上升得非常慢。在这种情况下，网络学习得太慢——它*欠拟合*。
- en: '![](../Images/07-27.png)'
  id: totrans-348
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-27.png)'
- en: Figure 7.27 The performance of our model with learning rates of 0.001 and 0.0001
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27 使用学习率0.001和0.0001的模型性能
- en: If we look at validation scores for all the learning rates (figure 7.28), we
    see that the learning rate of 0.001 is the best one.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们查看所有学习率的验证分数（图7.28），我们会看到0.001的学习率是最好的。
- en: '![](../Images/07-28.png)'
  id: totrans-351
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/07-28.png)'
- en: Figure 7.28 The accuracy of our model on the validation set for three different
    learning rates
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.28 三种不同学习率下我们的模型在验证集上的准确率
- en: For the learning rate of 0.001, the best accuracy is 83% (table 7.1).
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 对于0.001的学习率，最佳准确率是83%（表7.1）。
- en: Table 7.1 The validation accuracy with different values of dropout rate
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 不同dropout率值的验证准确率
- en: '| Learning rate | 0.01 | 0.001 | 0.0001 |'
  id: totrans-355
  prefs: []
  type: TYPE_TB
  zh: '| 学习率 | 0.01 | 0.001 | 0.0001 |'
- en: '| Validation accuracy | 82.7% | 83.0% | 78.0% |'
  id: totrans-356
  prefs: []
  type: TYPE_TB
  zh: '| 验证准确率 | 82.7% | 83.0% | 78.0% |'
- en: Note Your numbers may be slightly different. It’s also possible that in your
    experiments, the learning rate of 0.01 achieves slightly better results than 0.001.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您的数字可能略有不同。也有可能是在您的实验中，0.01的学习率比0.001实现了略好的结果。
- en: The difference between 0.01 and 0.001 is not significant. But if we look at
    the accuracy on the training data, with 0.01, it overfits the training data a
    lot faster. At some point, it even achieves an accuracy of 100%. When the discrepancy
    between the performance on train and validation sets is high, the risk of overfitting
    is also high. So, we should prefer the learning rate of 0.001.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 0.01和0.001之间的差异并不显著。但如果我们查看训练数据的准确率，使用0.01，它更快地过拟合训练数据。在某个点上，它甚至达到了100%的准确率。当训练集和验证集的性能差异很大时，过拟合的风险也高。因此，我们应该优先选择0.001的学习率。
- en: After training is done, we need to save the model. Now we’ll see how to do it.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，我们需要保存模型。现在我们将看看如何进行。
- en: 7.4.6 Saving the model and checkpointing
  id: totrans-360
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.6 保存模型和检查点
- en: 'Once the model is trained, we can save it using the `save_weights` method:'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后，我们可以使用`save_weights`方法保存它：
- en: '[PRE44]'
  id: totrans-362
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'We need to specify the following:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要指定以下内容：
- en: 'The output file: `''xception_v1_model.h5''`'
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输出文件：`'xception_v1_model.h5'`
- en: 'The format: h5, which is a format for saving binary data'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 格式：h5，这是一种用于保存二进制数据的格式
- en: You may have noticed that while training, the performance of our model on the
    validation set jumps up and down. This way, after 10 iterations, we don’t necessarily
    have the best model—maybe the best performance was achieved on iteration 5 or
    6.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到，在训练过程中，我们的模型在验证集上的性能上下波动。这样，经过10次迭代后，我们不一定能得到最佳模型——也许最佳性能是在第5或第6次迭代时实现的。
- en: We can save the model after each iteration, but it generates too much data.
    And if we rent a server in the cloud, it can quickly take all the available space.
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在每次迭代后保存模型，但这会产生太多的数据。如果我们租用云服务器，它很快就会占用所有可用空间。
- en: Instead, we can save the model only when it’s better than the previous best
    score on validation. For example, if the previous best accuracy is 0.8, but we
    have improved it to 0.91, we save the model. Otherwise, we continue the training
    process without saving the model.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们只有在模型在验证集上的最佳得分比之前更好时才保存模型。例如，如果之前的最佳准确率是0.8，但我们将其提高到了0.91，我们就保存模型。否则，我们继续训练过程而不保存模型。
- en: 'This process is called *model checkpointing*. Keras has a special class for
    doing it: `ModelCheckpoint`. Let’s use it:'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*模型检查点*。Keras有一个专门用于此目的的类：`ModelCheckpoint`。让我们使用它：
- en: '[PRE45]'
  id: totrans-370
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: ❶ Specifies the filename template for saving the models
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 指定了保存模型的文件名模板
- en: ❷ Saves the model only when it’s better than previous iterations
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 只有在比之前的迭代更好时才保存模型
- en: ❸ Uses the accuracy on validation for selecting the best model
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用验证准确率来选择最佳模型
- en: 'The first parameter is a template for the filename. Let’s take a look at it
    again:'
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是文件名的模板。让我们再看一遍：
- en: '[PRE46]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'It has two parameters inside:'
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 它内部有两个参数：
- en: '`{epoch:02d}` is replaced by the number of the epoch.'
  id: totrans-377
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{epoch:02d}`被替换为epoch的编号。'
- en: '`{val_accuracy:.3f}` is replaced by the validation accuracy.'
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`{val_accuracy:.3f}`被替换为验证准确率。'
- en: Because we set `save_best_only` to `True`, `ModelCheckpoint` keeps track of
    the best accuracy and saves the results to disk each time the accuracy improves.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们将`save_best_only`设置为`True`，`ModelCheckpoint`会跟踪最佳准确率，并在每次准确率提高时将结果保存到磁盘。
- en: We implement `ModelCheckpoint` as a callback—a way to execute anything after
    each epoch finishes. In this particular case, the callback evaluates the model
    and saves the result if the accuracy gets better.
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将`ModelCheckpoint`实现为一个回调——在每个epoch完成后执行任何操作的方式。在这种情况下，回调评估模型，并在准确率提高时保存结果。
- en: 'We can use it by passing it to the `callbacks` argument of the `fit` method:'
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过将其传递给`fit`方法的`callbacks`参数来使用它：
- en: '[PRE47]'
  id: totrans-382
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: ❶ Creates a new model
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 创建了一个新的模型
- en: ❷ Specifies the list of callbacks to be used during training
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定了训练期间要使用的回调列表
- en: After a few iterations, we already have some models saved to disk (figure 7.29).
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 经过几次迭代后，我们已经在磁盘上保存了一些模型（图7.29）。
- en: '![](../Images/07-29.png)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-29.png)'
- en: Figure 7.29 Because the `ModelCheckpoint` callback saves the model only when
    it improves, we only have 4 files with our model, not 10.
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.29 由于`ModelCheckpoint`回调只在模型改进时保存模型，所以我们只有4个包含我们模型的文件，而不是10个。
- en: We’ve learned how to store the best model. Now let’s improve our model by adding
    more layers to the network.
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学会了如何存储最佳模型。现在让我们通过向网络添加更多层来改进我们的模型。
- en: 7.4.7 Adding more layers
  id: totrans-389
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.7 添加更多层
- en: 'Previously, we trained a model with one dense layer:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们用一个密集层训练了一个模型：
- en: '[PRE48]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: We don’t have to restrict ourselves to just one layer, so let’s add another
    layer between the base model and the last layer with predictions (figure 7.30).
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不必限制自己只使用一层，因此让我们在基础模型和预测的最后一层之间添加另一层（图7.30）。
- en: '![](../Images/07-30.png)'
  id: totrans-393
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-30.png)'
- en: Figure 7.30 We add another dense layer between the vector representation and
    the output.
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.30 我们在向量表示和输出之间添加了另一个密集层。
- en: 'For example, we can add a dense layer of size 100:'
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以添加一个大小为100的密集层：
- en: '[PRE49]'
  id: totrans-396
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: ❶ Adds another dense layer of size 100
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 添加了一个大小为100的密集层
- en: ❷ Instead of connecting outputs to vector, connects it to inner
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 不是将输出连接到向量，而是连接到内部
- en: 'Note There’s no particular reason for selecting the size of 100 for the inner
    dense layer. We should treat it as a parameter: as with the learning rate, we
    can try different values and see which one leads to better performance on validation.
    In this chapter, we will not experiment with changing the size of the inner layer,
    but feel free to do so.'
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：选择100作为内部密集层的大小没有特别的原因。我们应该将其视为一个参数：就像学习率一样，我们可以尝试不同的值，看看哪个能带来更好的验证性能。在本章中，我们不会尝试改变内部层的大小，但你可以自由尝试。
- en: This way, we added a layer between the base model and the outputs (figure 7.31).
  id: totrans-400
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们在基础模型和输出之间添加了一个层（图 7.31）。
- en: '![](../Images/07-31.png)'
  id: totrans-401
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-31.png)'
- en: Figure 7.31 A new `inner` layer added between `vector` and `outputs`
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.31 在 `vector` 和 `outputs` 之间添加了一个新的 `inner` 层
- en: 'Let’s take another look at the line with the new dense layer:'
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次看看带有新密集层的行：
- en: '[PRE50]'
  id: totrans-404
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Here, we set the `activation` parameter to `relu`.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 这里，我们将 `activation` 参数设置为 `relu`。
- en: Remember that we get a neural network by putting together multiple logistic
    regressions. In logistic regression, sigmoid is used for converting the raw score
    to probability. But for inner layers, we don’t need probabilities, and we can
    replace sigmoid with other functions. These functions are called *activation functions*.
    ReLU (Rectified Linear Unit) is one of them, and for inner layers, it’s a better
    choice than sigmoid.
  id: totrans-406
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们通过组合多个逻辑回归来得到神经网络。在逻辑回归中，sigmoid 用于将原始分数转换为概率。但对于内部层，我们不需要概率，可以用其他函数替换
    sigmoid。这些函数被称为 *激活函数*。ReLU（修正线性单元）是其中之一，对于内部层来说，它比 sigmoid 是更好的选择。
- en: The sigmoid function suffers from the vanishing gradient problem, which makes
    training deep neural networks impossible. ReLU solves this problem. To read more
    about this problem, and about activation functions in general, please refer to
    the CS231n notes ([https://cs231n.github.io/neural-networks-1/](https://cs231n.github.io/neural-networks-1/)).
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: sigmoid 函数存在梯度消失问题，这使得训练深层神经网络变得不可能。ReLU 解决了这个问题。要了解更多关于这个问题以及一般激活函数的信息，请参阅
    CS231n 笔记（[https://cs231n.github.io/neural-networks-1/](https://cs231n.github.io/neural-networks-1/)）。
- en: With another layer, our chances of overfitting increase significantly. To avoid
    that, we need to add regularization to our model. Next, we’ll see how to do it.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 添加另一个层后，我们过拟合的风险显著增加。为了避免这种情况，我们需要在模型中添加正则化。接下来，我们将看到如何做到这一点。
- en: 7.4.8 Regularization and dropout
  id: totrans-409
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.8 正则化和 dropout
- en: '*Dropout* is a special technique for fighting overfitting in neural networks.
    The main idea behind dropout is freezing a part of a dense layer when training.
    At each iteration, the part to freeze is chosen randomly. Only the unfrozen part
    is trained, and the frozen part is not touched at all.'
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: '*Dropout* 是一种用于对抗神经网络过拟合的特殊技术。Dropout 的主要思想是在训练时冻结密集层的一部分。在每次迭代中，随机选择要冻结的部分。只有未冻结的部分被训练，冻结的部分则完全不接触。'
- en: If some parts of the network are ignored, the model overall is less likely to
    overfit. When the network goes over a batch of images, the frozen part of a layer
    doesn’t see this data—it’s turned off. This way, it’s more difficult for the network
    to memorize the images (figure 7.32).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 如果忽略网络的一部分，整体模型不太可能过拟合。当网络处理一批图像时，层的冻结部分看不到这些数据——它是关闭的。这样，网络记住图像就更加困难（图 7.32）。
- en: '![](../Images/07-32a.png)'
  id: totrans-412
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-32a.png)'
- en: (A) Two dense layers without dropout
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: (A) 两个没有 dropout 的密集层
- en: '![](../Images/07-32b.png)'
  id: totrans-414
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-32b.png)'
- en: (B) Two dense layers with dropout
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: (B) 两个带有 dropout 的密集层
- en: Figure 7.32 With dropout, the connections to frozen nodes are dropped out.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.32 带有 dropout 时，连接到冻结节点的连接被丢弃。
- en: For every batch, the part to freeze is selected randomly, so the network learns
    to extract patterns from incomplete information, which makes it more robust and
    less likely to overfit.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个批次，随机选择要冻结的部分，这样网络就会学会从不完全信息中提取模式，这使得它更加鲁棒，并且不太可能过拟合。
- en: We can control the strength of dropout by setting the dropout rate—the fraction
    of elements in a layer to be frozen at each step.
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过设置 dropout 率——每步冻结层中元素的分数来控制 dropout 的强度。
- en: 'To do this in Keras, we add a `Dropout` layer after the first `Dense` layer
    and set up the dropout rate:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Keras 中，我们在第一个 `Dense` 层之后添加一个 `Dropout` 层，并设置 dropout 率：
- en: '[PRE51]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This way, we add another block in the network—the dropout block (figure 7.33).
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们在网络中添加了另一个块——dropout 块（图 7.33）。
- en: '![](../Images/07-33.png)'
  id: totrans-422
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-33.png)'
- en: Figure 7.33 `Dropout` is another block between the `inner` layer and the `outputs`
    layer.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.33 `Dropout` 是 `inner` 层和 `outputs` 层之间另一个块。
- en: Let’s train this model. To make it easier, we first need to update the `make_model`
    function and add another parameter there for controlling the dropout rate.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们训练这个模型。为了简化，我们首先需要更新 `make_model` 函数，并在那里添加一个参数来控制 dropout 率。
- en: Listing 7.2 A function for creating a model with dropout
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 7.2 创建具有 dropout 的模型的功能
- en: '[PRE52]'
  id: totrans-426
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'Let’s try four different values for the `droprate` parameter to see how the
    performance of our model changes:'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尝试为 `droprate` 参数设置四个不同的值，看看我们的模型性能如何变化：
- en: '0.0: Nothing gets frozen, so this is equivalent to not including the dropout
    layer at all.'
  id: totrans-428
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0.0: 没有任何内容被冻结，因此这相当于完全没有包括dropout层。'
- en: '0.2: Only 20% of the layer gets frozen,'
  id: totrans-429
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0.2: 只有20%的层被冻结，'
- en: '0.5: Half of the layer is frozen.'
  id: totrans-430
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0.5: 层的一半被冻结。'
- en: '0.8: Most of the layer (80%) is frozen.'
  id: totrans-431
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '0.8: 大多数层（80%）被冻结。'
- en: 'With dropout, it takes more time to train a model: at each step, only a part
    of our network learns, so we need to make more steps. This means that we should
    increase the number of epochs when training.'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 使用dropout，训练模型需要更多的时间：在每一步中，只有我们网络的一部分在学习，因此我们需要走更多的步骤。这意味着我们应该在训练时增加epoch的数量。
- en: 'So, let’s train it:'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们开始训练：
- en: '[PRE53]'
  id: totrans-434
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: ❶ Modifies droprate to experiment with different values
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 修改droprate以实验不同的值
- en: ❷ Trains a model for more epochs than previously
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练模型比之前更多的epochs
- en: When it finishes, repeat this for other values of the `droprate` parameter by
    copying the code to another cell and changing the value to 0.2, 0.5, and 0.8.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 当它完成后，通过复制代码到另一个单元格并更改值为0.2、0.5和0.8来重复此操作。
- en: From the results on the validation dataset, we see that there’s no significant
    difference between 0.0, 0.2, and 0.5\. However, 0.8 is worse—we made it really
    difficult for the network to learn anything (figure 7.34).
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 从验证数据集的结果来看，0.0、0.2和0.5之间没有显著差异。然而，0.8更差——我们使网络学习变得非常困难（图7.34）。
- en: '![](../Images/07-34.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-34.png)'
- en: Figure 7.34 The accuracy on the validation set is similar for dropout rates
    of 0.0, 0.2, and 0.5\. However, it’s worse for 0.8.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.34 对于dropout率为0.0、0.2和0.5的情况，验证集上的准确率相似。然而，对于0.8，它更差。
- en: The best accuracy we could achieve is 84.5% for the dropout rate of 0.5 (table
    7.2).
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能达到的最佳准确率是0.5 dropout率下的84.5%（表7.2）。
- en: Table 7.2 The validation accuracy with different values of dropout rate
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 不同dropout率值下的验证准确率
- en: '| Dropout rate | 0.0 | 0.2 | 0.5 | 0.8 |'
  id: totrans-443
  prefs: []
  type: TYPE_TB
  zh: '| Dropout率 | 0.0 | 0.2 | 0.5 | 0.8 |'
- en: '| Validation accuracy | 84.2% | 84.2% | 84.5% | 82.4% |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '| 验证准确率 | 84.2% | 84.2% | 84.5% | 82.4% |'
- en: Note You may have different results, and it’s possible that a different value
    for dropout rate achieves the best accuracy.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：您可能得到不同的结果，并且可能存在不同的dropout率值可以达到最佳准确率。
- en: In cases like this, when there’s no visible difference between accuracy on the
    validation dataset, it’s useful to look at the accuracy on the train set as well
    (figure 7.35).
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，当验证数据集上的准确率没有明显差异时，查看训练集上的准确率也是有用的（图7.35）。
- en: '![](../Images/07-35.png)'
  id: totrans-447
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-35.png)'
- en: Figure 7.35 With a dropout rate of 0.0, the network overfits quickly, whereas
    the rate of 0.8 makes it really difficult to learn.
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.35 当dropout率为0.0时，网络快速过拟合，而0.8的率使学习变得非常困难。
- en: With no dropout, the model quickly memorizes the entire train dataset, and after
    10 epochs, it becomes 99.9% accurate. With a dropout rate of 0.2, it needs more
    time to overfit the training dataset, whereas for 0.5, it hasn’t reached the perfect
    accuracy, even after 30 iterations. By setting the rate to 0.8, we make it really
    difficult for the network to learn anything, so the accuracy is low even on the
    training dataset.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 没有dropout的情况下，模型会快速记住整个训练数据集，经过10个epoch后，准确率达到99.9%。当dropout率为0.2时，它需要更多的时间来过拟合训练数据集，而对于0.5，即使经过30次迭代，也没有达到完美的准确率。将率设置为0.8，我们使网络学习变得非常困难，因此即使在训练数据集上，准确率也较低。
- en: We can see that with a dropout rate of 0.5, the network doesn’t overfit as fast
    as others, while maintaining the same level of accuracy on the validation dataset
    as 0.0 and 0.2\. Thus, we should prefer the model we trained with the dropout
    rate of 0.5 to other models.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，当dropout率为0.5时，网络不像其他网络那样快速过拟合，同时保持了与0.0和0.2相同的验证数据集准确率。因此，我们应该优先选择使用0.5
    dropout率训练的模型。
- en: By adding another layer and dropout, we have increased the accuracy from 83%
    to 84%. Even though this increase is not significant for this particular case,
    dropout is a powerful tool for fighting overfitting, and we should use it when
    making our models more complex.
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 通过添加另一个层和dropout，我们将准确率从83%提高到84%。尽管这种增加对于这个特定案例来说并不显著，但dropout是抵抗过拟合的有力工具，我们在使模型更复杂时应该使用它。
- en: In addition to dropout, we can use other ways to fight overfitting. For example,
    we can generate more data. In the next section, we’ll see how to do it.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 除了dropout，我们还可以使用其他方法来抵抗过拟合。例如，我们可以生成更多数据。在下一节中，我们将看到如何做到这一点。
- en: Exercise 7.2
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 练习7.2
- en: In dropout, we
  id: totrans-454
  prefs: []
  type: TYPE_NORMAL
  zh: 在dropout中，我们
- en: a) Remove a part of a model completely
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: a) 完全移除模型的一部分
- en: b) Freeze a random part of a model, so it doesn’t get updated during one iteration
    of training
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: b) 冻结模型的一部分，使其在训练过程中的一个迭代中不被更新
- en: c) Freeze a random part of a model, so it doesn’t get used during the entire
    training process
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: c) 冻结模型的一部分，使其在整个训练过程中不被使用
- en: 7.4.9 Data augmentation
  id: totrans-458
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.9 数据增强
- en: Getting more data is always a good idea, and it’s usually the best thing we
    can do to improve the quality of our model. Unfortunately, it’s not always possible
    to get more data.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 获取更多数据总是好主意，通常这是我们提高模型质量所能做的最好的事情。不幸的是，并不总是能够获取更多数据。
- en: 'For images, however, we can generate more data from existing images. For example:'
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 对于图像，我们可以从现有图像中生成更多数据。例如：
- en: Flip an image vertically and horizontally.
  id: totrans-461
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 垂直和水平翻转图像。
- en: Rotate an image.
  id: totrans-462
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 旋转图像。
- en: Zoom in or out a bit.
  id: totrans-463
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微放大或缩小。
- en: Change an image in other ways.
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以其他方式改变图像。
- en: The process of generating more data from an existing dataset is called *data
    augmentation* (figure 7.36).
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 从现有数据集中生成更多数据的过程称为*数据增强*（图7.36）。
- en: '![](../Images/07-36.png)'
  id: totrans-466
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-36.png)'
- en: Figure 7.36 We can generate more training data by modifying existing images.
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.36 我们可以通过修改现有图像来生成更多训练数据。
- en: The easiest way to create a new image from an existing one is to flip it horizontally,
    vertically, or both (figure 7.37).
  id: totrans-468
  prefs: []
  type: TYPE_NORMAL
  zh: 从现有图像创建新图像的最简单方法是将它水平、垂直或两者都翻转（图7.37）。
- en: '![](../Images/07-37.png)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-37.png)'
- en: Figure 7.37 Flipping an image horizontally and vertically
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.37 水平和垂直翻转图像
- en: In our case, horizontal flipping might not make much sense, but vertical flipping
    should be useful.
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，水平翻转可能没有太大意义，但垂直翻转应该是有用的。
- en: Note If you’re curious how these images are generated, check the notebook 07-augmentations.ipynb
    in the GitHub repository for this book.
  id: totrans-472
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：如果你对如何生成这些图像感兴趣，请查看GitHub仓库中本书的07-augmentations.ipynb笔记本。
- en: 'Rotating is another image-manipulation strategy that we can use: we can generate
    a new image by rotating an existing one by some degree (figure 7.38).'
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 旋转是另一种我们可以使用的图像处理策略：我们可以通过旋转现有图像一定角度来生成新的图像（图7.38）。
- en: '![](../Images/07-38.png)'
  id: totrans-474
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-38.png)'
- en: Figure 7.38 Rotating an image. If the rotation degree is negative, the image
    is rotated counterclockwise.
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.38 旋转图像。如果旋转角度为负，图像将逆时针旋转。
- en: Shear is another possible transformation. It skews the image by “pulling” it
    by one of its sides. When the shear is positive, we pull the right side down,
    and when it’s negative, we pull the right side up (figure 7.39).
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 剪切是另一种可能的变换。它通过“拉动”图像的一侧来扭曲图像。当剪切为正时，我们向下拉动右侧，当它为负时，我们向上拉动右侧（图7.39）。
- en: '![](../Images/07-39.png)'
  id: totrans-477
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-39.png)'
- en: Figure 7.39 The shear transformation. We pull the image up or down by its right
    side.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.39 剪切变换。我们通过其右侧拉动图像上下移动。
- en: 'At first glance, the effect of shear and rotation may look similar, but actually,
    they are quite different. Shear changes the geometrical shape of an image, but
    rotation doesn’t: it only rotates an image (figure 7.40).'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 初看，剪切和旋转的效果可能看起来相似，但实际上它们相当不同。剪切会改变图像的几何形状，但旋转不会：它只会旋转图像（图7.40）。
- en: '![](../Images/07-40.png)'
  id: totrans-480
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-40.png)'
- en: Figure 7.40 Shear changes the geometrical shape of an image by pulling it, so
    a square becomes a parallelogram. Rotation doesn’t change the shape, so a square
    remains a square.
  id: totrans-481
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.40 剪切通过拉动图像改变其几何形状，因此正方形变成平行四边形。旋转不会改变形状，所以正方形仍然是正方形。
- en: Next, we can shift an image horizontally (figure 7.41) or vertically (figure 7.42).
  id: totrans-482
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以水平（图7.41）或垂直（图7.42）移动图像。
- en: '![](../Images/07-41.png)'
  id: totrans-483
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-41.png)'
- en: Figure 7.41 Shifting an image horizontally. Positive values shift the image
    to the left, whereas negative values shift it to the right.
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.41 水平移动图像。正值将图像向左移动，而负值将图像向右移动。
- en: '![](../Images/07-42.png)'
  id: totrans-485
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-42.png)'
- en: Figure 7.42 Shifting an image vertically. Positive values shift the image to
    the top, whereas negative values shift it to the bottom.
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.42 垂直移动图像。正值将图像向上移动，而负值将图像向下移动。
- en: Finally, we can zoom an image in or out (figure 7.43).
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以放大或缩小图像（图7.43）。
- en: '![](../Images/07-43.png)'
  id: totrans-488
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-43.png)'
- en: Figure 7.43 Zooming in or out. When the zoom factor is smaller than 1, we zoom
    in. If it’s larger than 1, we zoom out.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.43 放大或缩小。当缩放因子小于1时，我们放大；如果大于1，我们缩小。
- en: What is more, we can combine multiple data augmentation strategies. For example,
    we can take an image, flip it horizontally, zoom out, and then rotate it.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可以结合多种数据增强策略。例如，我们可以取一个图像，水平翻转，缩小，然后旋转。
- en: By applying different augmentations to the same image, we can generate many
    more new images (figure 7.44).
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 通过对同一图像应用不同的增强，我们可以生成更多的新图像（图 7.44）。
- en: '![](../Images/07-44.png)'
  id: totrans-492
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-44.png)'
- en: Figure 7.44 10 new images generated from the same image
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.44 10 张由同一图像生成的新图像
- en: Keras provides a built-in way of augmenting a dataset. It’s based on `ImageDataGenerator`,
    which we have already used for reading the images.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: Keras 提供了一种内置的数据增强方法。它基于 `ImageDataGenerator`，我们之前已经用它来读取图像。
- en: The generator takes in many arguments. Previously, we used only `preprocessing_
    function`—it’s needed to preprocess the images. Others are available, and many
    of them are responsible for augmenting the dataset.
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 生成器接受许多参数。之前，我们只使用了`preprocessing_function`——它是预处理图像所需的。其他参数也可用，其中许多负责增强数据集。
- en: 'For example, we can create a new generator:'
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，我们可以创建一个新的生成器：
- en: '[PRE54]'
  id: totrans-497
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Let’s take a closer look at these parameters:'
  id: totrans-498
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更仔细地看看这些参数：
- en: '`rotation_range=30`: Rotate an image by a random degree between –30 and 30.'
  id: totrans-499
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`rotation_range=30`: 将图像随机旋转-30到30度之间的任意角度。'
- en: '`width_shift_range=30`: Shift an image horizontally by a value between –30
    and 30 pixels.'
  id: totrans-500
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`width_shift_range=30`: 将图像水平移动，移动值在-30到30像素之间。'
- en: '`height_shift_range=30`: Shift an image vertically by a value between –30 and
    30 pixels.'
  id: totrans-501
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`height_shift_range=30`: 将图像垂直移动，移动值在-30到30像素之间。'
- en: '`shear_range=10`: Apply the shear transformation by a value between –10 and
    10 (also in pixels).'
  id: totrans-502
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`shear_range=10`: 通过-10到10（也是像素）之间的值应用剪切变换。'
- en: '`zoom_range=0.2`: Apply the zoom transformation using the zoom factor between
    0.8 and 1.2 (1 – 0.2 and 1 + 0.2).'
  id: totrans-503
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`zoom_range=0.2`: 使用0.8到1.2（1 - 0.2和1 + 0.2）之间的缩放因子应用缩放变换。'
- en: '`horizontal_flip=True`: Randomly flip an image horizontally.'
  id: totrans-504
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`horizontal_flip=True`: 随机水平翻转图像。'
- en: '`vertical_flip=False`: Don’t flip an image vertically.'
  id: totrans-505
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`vertical_flip=False`: 不要垂直翻转图像。'
- en: 'For our project, let’s take a small set of these augmentations:'
  id: totrans-506
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的项目，让我们选择这些增强中的一小部分：
- en: '[PRE55]'
  id: totrans-507
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'Next, we use the generator in the same way as previously:'
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们像之前一样使用生成器：
- en: '[PRE56]'
  id: totrans-509
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 'We need to apply augmentations only to training data. We don’t use it for validation:
    we want to make our evaluation consistent and be able to compare a model trained
    on the augmented dataset with a model trained without augmentations.'
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需要将增强应用于训练数据。我们不使用它进行验证：我们希望使我们的评估保持一致，并能够比较在增强数据集上训练的模型和在未增强数据集上训练的模型。
- en: 'So, we load the validation dataset using exactly the same code as before:'
  id: totrans-511
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们使用与之前完全相同的代码加载验证数据集：
- en: '[PRE57]'
  id: totrans-512
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: 'We’re ready to train a new model now:'
  id: totrans-513
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们已经准备好训练一个新的模型：
- en: '[PRE58]'
  id: totrans-514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Note We omit the code for model checkpointing here for brevity. Add it if you
    want to save the best model.
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了简洁起见，这里省略了模型检查点的代码。如果您想保存最佳模型，请添加它。
- en: To train this model, we need even more epochs than previously. Data augmentation
    is also a regularization strategy. Instead of training on the same image over
    and over again, the network sees a different variation of the same image for every
    epoch. This makes it more difficult for the model to memorize the data, and it
    decreases the chances of overfitting.
  id: totrans-516
  prefs: []
  type: TYPE_NORMAL
  zh: 为了训练这个模型，我们需要比之前更多的epoch。数据增强也是一种正则化策略。我们不是反复在相同的图像上训练，而是在每个epoch中，网络看到的是同一图像的不同变体。这使得模型更难记住数据，并减少了过拟合的可能性。
- en: After training this model, we managed to improve the accuracy by 1%, from 84%
    to 85%.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 训练此模型后，我们成功将准确率提高了1%，从84%提高到85%。
- en: This improvement is not really significant. But we have experimented a lot,
    and we could do this relatively quickly because we used small images of size 150
    × 150\. Now we can apply everything we have learned so far to larger images.
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 这种改进并不真正显著。但我们已经进行了大量实验，并且我们可以相对快速地做到这一点，因为我们使用了150 × 150的小图像。现在我们可以将我们迄今为止所学的一切应用到更大的图像上。
- en: Exercise 7.3
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 7.3
- en: Data augmentation helps fight overfitting because
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 数据增强有助于防止过拟合，因为
- en: a) The model doesn’t get to see the same images over and over again.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: a) 模型不会反复看到相同的图像。
- en: b) It adds a lot of variety into the dataset—rotations and other image transformations.
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: b) 它为数据集增加了大量多样性——旋转和其他图像变换。
- en: c) It generates examples of images that may exist, but the model overwise wouldn’t
    have seen.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: c) 它生成可能存在的图像示例，但模型在其他情况下不会看到。
- en: d) All of the above.
  id: totrans-524
  prefs: []
  type: TYPE_NORMAL
  zh: d) 所有上述内容。
- en: 7.4.10 Training a larger model
  id: totrans-525
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.10 训练更大的模型
- en: 'Even for people it may be challenging to understand what kind of item is in
    a small 150 × 150 image. It’s also difficult for a computer: it’s not easy to
    see the important details, so the model may confuse pants and shorts or T-shirts
    and shirts.'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 即使对于人来说，理解一个150 × 150的小图像中包含什么类型的物品可能都很有挑战性。对于计算机来说也是如此：很难看到重要的细节，因此模型可能会混淆裤子与短裤或T恤与衬衫。
- en: By increasing the size of images from 150 × 150 to 299 × 299, it’ll be easier
    for the network to see more details and, therefore, achieve greater accuracy.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 通过将图像的大小从150 × 150增加到299 × 299，网络将更容易看到更多细节，因此可以达到更高的准确率。
- en: Note Training a model on larger images takes approximately four times longer
    than on small images. If you don’t have access to a computer with a GPU, you don’t
    have to run the code in this section. Conceptually, the process is the same, and
    the only difference is the input size.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在较大图像上训练模型的时间大约是小图像的四倍。如果您没有访问带有GPU的计算机，您不需要运行本节中的代码。从概念上讲，过程是相同的，唯一的区别是输入大小。
- en: 'So, let’s modify our function for creating a model. For that, we need to take
    the code of `make_model` (listing 7.2) and adjust it in two places:'
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，让我们修改我们创建模型的函数。为此，我们需要调整`make_model`（列表7.2）的代码，并在两个地方进行调整：
- en: The `input_shape` argument of Xception
  id: totrans-530
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Xception的`input_shape`参数
- en: The `C` argument for input
  id: totrans-531
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入的`C`参数
- en: In both these cases, we need to replace `(150,` `150,` `3)` with `(299,` `299,`
    `3)`.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，我们需要将`(150,` `150,` `3)`替换为`(299,` `299,` `3)`。
- en: Next, we need to adjust the `target_size` parameter for the train and validation
    generators. We replace `(150,` `150)` by `(299,` `299)`, and everything else stays
    the same.
  id: totrans-533
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要调整训练和验证生成器的`target_size`参数。我们将`(150,` `150)`替换为`(299,` `299)`，其他一切保持不变。
- en: Now we’re ready to train a model!
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好训练一个模型了！
- en: '[PRE59]'
  id: totrans-535
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: Note To save the model, add checkpointing.
  id: totrans-536
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：为了保存模型，请添加检查点。
- en: This model achieves accuracy of around 89% on the validation data. This is a
    considerable improvement over the previous model.
  id: totrans-537
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型在验证数据上达到了约89%的准确率。这比之前的模型有了相当大的改进。
- en: We have trained a model, so now it’s time to use it.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了一个模型，现在是时候使用它了。
- en: 7.5 Using the model
  id: totrans-539
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.5 使用模型
- en: Previously, we trained multiple models. The best one is the model we trained
    on large images—it has 89% accuracy. The second-best model has an accuracy of
    85%.
  id: totrans-540
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，我们训练了多个模型。最好的模型是我们在大图像上训练的模型——它有89%的准确率。第二好的模型准确率为85%。
- en: Let’s now use these models to make predictions. To use a model, we first need
    to load it.
  id: totrans-541
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用这些模型进行预测。要使用模型，我们首先需要加载它。
- en: 7.5.1 Loading the model
  id: totrans-542
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.1 加载模型
- en: You can either use the model you trained yourself or download the model we trained
    for the book and use it.
  id: totrans-543
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用自己训练的模型，或者下载我们为书籍训练的模型并使用它。
- en: 'To download them, go to the releases section of the book’s GitHub repository
    and look for Models for Chapter 7: Deep learning (figure 7.45). Alternatively,
    go to this URL: [https://github.com/alexeygrigorev/mlbookcamp-code/releases/tag/chapter7-model](https://github.com/alexeygrigorev/mlbookcamp-code/releases/tag/chapter7-model).'
  id: totrans-544
  prefs: []
  type: TYPE_NORMAL
  zh: 要下载它们，请访问书籍GitHub仓库的发布部分，并查找第7章：深度学习（图7.45）的模型。或者，访问此URL：[https://github.com/alexeygrigorev/mlbookcamp-code/releases/tag/chapter7-model](https://github.com/alexeygrigorev/mlbookcamp-code/releases/tag/chapter7-model)。
- en: '![](../Images/07-45.png)'
  id: totrans-545
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-45.png)'
- en: Figure 7.45 You can download the models we trained for this chapter from the
    book’s GitHub repository.
  id: totrans-546
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.45 您可以从书籍的GitHub仓库下载我们为这一章训练的模型。
- en: 'Then download the large model trained on 299 × 299 images (xception_v4_large).
    To use it, load the model using the `load_model` function from the `models` package:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 然后下载在299 × 299图像上训练的大模型（xception_v4_large）。要使用它，请使用`models`包中的`load_model`函数加载模型：
- en: '[PRE60]'
  id: totrans-548
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: We have already used both the train and validation datasets. We have finished
    the training process, so now it’s time to evaluate this model on test data.
  id: totrans-549
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用了训练和验证数据集。我们已经完成了训练过程，现在是时候在测试数据上评估这个模型了。
- en: 7.5.2 Evaluating the model
  id: totrans-550
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.2 评估模型
- en: 'To load the test data, we follow the same approach: we use `ImageDataGenerator`
    but point to the test directory. Let’s do it:'
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 要加载测试数据，我们遵循相同的方法：我们使用`ImageDataGenerator`，但指向测试目录。让我们来做：
- en: '[PRE61]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: ❶ Use (150, 150) if you’re using the small model.
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果您使用的是小模型，请使用(150, 150)。
- en: 'Evaluating a model in Keras is as simple as invoking the `evaluate` method:'
  id: totrans-554
  prefs: []
  type: TYPE_NORMAL
  zh: 在Keras中评估一个模型就像调用`evaluate`方法一样简单：
- en: '[PRE62]'
  id: totrans-555
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'It applies the model to all the data in the test folder and shows the evaluation
    metrics of loss and accuracy:'
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 它将模型应用于测试文件夹中的所有数据，并显示了损失和准确率的评估指标：
- en: '[PRE63]'
  id: totrans-557
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: Our model shows 90% accuracy on the test dataset, which is comparable with the
    performance on the validation dataset (89%).
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型在测试数据集上显示了90%的准确率，这与验证数据集上的性能相当（89%）。
- en: 'If we repeat the same process for the small dataset, we see that the performance
    is worse:'
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们对小数据集重复同样的过程，我们会看到性能更差：
- en: '[PRE64]'
  id: totrans-560
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: The accuracy is 82%, whereas on the validation it’s 85%. The model performed
    worse on the test dataset.
  id: totrans-561
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率是82%，而在验证数据集上是85%。模型在测试数据集上的表现更差。
- en: 'It may be because of random fluctuations: the size of validation and test sets
    are not large, only 300 examples. So the model could get lucky on validation and
    unlucky on test.'
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是因为随机波动：验证集和测试集的大小都不大，只有300个示例。因此，模型可能在验证集上运气好，在测试集上运气差。
- en: However, it could be a sign of overfitting. By repeatedly evaluating the model
    on the validation dataset, we picked the model that got very lucky. Maybe this
    luck is not generalizable, and that’s why the model performs worse on the data
    it hasn’t seen previously.
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这可能是过拟合的迹象。通过反复在验证数据集上评估模型，我们选择了表现非常幸运的模型。也许这种幸运不具有普遍性，这就是为什么模型在之前未见过的数据上的表现较差。
- en: Now let’s see how we can apply the model to individual images to get predictions.
  id: totrans-564
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看如何将模型应用于单个图像以获取预测。
- en: 7.5.3 Getting the predictions
  id: totrans-565
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5.3 获取预测
- en: 'If we want to apply the model to a single image, we need to do the same thing
    `ImageDataGenerator` performs internally:'
  id: totrans-566
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想将模型应用于单个图像，我们需要做与`ImageDataGenerator`内部执行相同的事情：
- en: Load an image.
  id: totrans-567
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加载一张图片。
- en: Preprocess it.
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预处理它。
- en: 'We already know how to load an image. We can use `load_img` for that:'
  id: totrans-569
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经知道如何加载图片。我们可以使用`load_img`来做这件事：
- en: '[PRE65]'
  id: totrans-570
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: It’s a picture of pants (figure 7.46).
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一张裤子的图片（图7.46）。
- en: '![](../Images/07-46.png)'
  id: totrans-572
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-46.png)'
- en: Figure 7.46 An image of pants from the train dataset
  id: totrans-573
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.46 训练数据集中的一张裤子图片
- en: 'Next, we preprocess the image:'
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们预处理图片：
- en: '[PRE66]'
  id: totrans-575
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: 'And, finally, we get the predictions:'
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们得到预测结果：
- en: '[PRE67]'
  id: totrans-577
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: 'We can see the predictions for the image by checking the first row of predictions:
    `pred[0]` (figure 7.47).'
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过检查预测的第一行来查看图像的预测：`pred[0]`（图7.47）。
- en: '![](../Images/07-47.png)'
  id: totrans-579
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-47.png)'
- en: Figure 7.47 The predictions of our model. It’s an array with 10 elements, one
    for each class.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.47 我们模型的预测。它是一个包含10个元素的数组，每个类别一个。
- en: The result is an array with 10 elements, where each element contains the score.
    The higher the score, the more likely the image is to belong to the respective
    class.
  id: totrans-581
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个包含10个元素的数组，每个元素包含一个分数。分数越高，图像属于相应类别的可能性就越大。
- en: To get the element with the highest score, we can use the `argmax` method. It
    returns the index of the element with the highest score (figure 7.48).
  id: totrans-582
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取得分最高的元素，我们可以使用`argmax`方法。它返回得分最高的元素的索引（图7.48）。
- en: '![](../Images/07-48.png)'
  id: totrans-583
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/07-48.png)'
- en: Figure 7.48 The `argmax` function returns the element with the highest score.
  id: totrans-584
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.48 `argmax`函数返回得分最高的元素。
- en: 'To know which label corresponds to class 4, we need to get the mapping. It
    can be extracted from a data generator. But let’s put it manually to a dictionary:'
  id: totrans-585
  prefs: []
  type: TYPE_NORMAL
  zh: 要知道哪个标签对应于类别4，我们需要获取映射。它可以从数据生成器中提取。但让我们手动将其放入字典中：
- en: '[PRE68]'
  id: totrans-586
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: 'To get the label, simply look it up in the dictionary:'
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取标签，只需在字典中查找：
- en: '[PRE69]'
  id: totrans-588
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: 'As we see, the label is “pants,” which is correct. Also note that the label
    “shorts” has a high positive score: pants and shorts are quite similar visually.
    But “pants” is clearly the winner.'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们所见，标签是“裤子”，这是正确的。此外，请注意标签“短裤”有一个很高的正分：裤子和短裤在视觉上非常相似。但“裤子”显然是赢家。
- en: We will use this code in the next chapter, where we will productionize our model.
  id: totrans-590
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章中使用这段代码，我们将在这个章节中将我们的模型投入生产。
- en: 7.6 Next steps
  id: totrans-591
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.6 下一步
- en: We’ve learned the basics we need for training a classification model for predicting
    the type of clothes. We’ve covered a lot of material, but there is a lot more
    to learn than we could fit into this chapter. You can explore this topic more
    by doing the exercises.
  id: totrans-592
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了训练用于预测衣服类型的分类模型所需的基本知识。我们涵盖了大量的材料，但还有更多内容需要学习，这些内容超出了本章的范围。你可以通过做练习来更深入地探索这个主题。
- en: 7.6.1 Exercises
  id: totrans-593
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.1 练习
- en: 'For deep learning, the more data we have, the better. But the dataset we used
    for this project is not large: we trained our model on only 3,068 images. To make
    it better, we can add more training data. You can find more pictures of clothes
    in other data sources; for example, at [https://www.kaggle.com/dqmonn/ zalando-store-crawl](https://www.kaggle.com/dqmonn/zalando-store-crawl),
    [https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset](https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset),
    or [https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6).
    Try adding more pictures to the training data, and see if it improves the accuracy
    on the validation dataset.'
  id: totrans-594
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于深度学习，我们拥有的数据越多，效果越好。但这个项目使用的数据集并不大：我们仅在3,068张图像上训练了我们的模型。为了使其更好，我们可以添加更多训练数据。你可以在其他数据源中找到更多衣服的图片；例如，在[https://www.kaggle.com/dqmonn/zalando-store-crawl](https://www.kaggle.com/dqmonn/zalando-store-crawl)、[https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset](https://www.kaggle.com/paramaggarwal/fashion-product-images-dataset)或[https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6](https://www.kaggle.com/c/imaterialist-fashion-2019-FGVC6)。尝试向训练数据中添加更多图片，并看看是否提高了验证数据集上的准确率。
- en: Augmentations help us train better models. In this chapter, we used only the
    most basic augmentation strategies. You can further explore this topic and try
    other kinds of image modifications. For example, add rotations and shifting, and
    see if it helps the model achieve better performance.
  id: totrans-595
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 扩增有助于我们训练更好的模型。在本章中，我们只使用了最基本的扩充策略。你可以进一步探索这个主题并尝试其他类型的图像修改。例如，添加旋转和移动，看看是否有助于模型获得更好的性能。
- en: In addition to the built-in way of augmenting the dataset, we have special libraries
    for doing this. One of them is Albumentations ([https://github.com/ albumentations-team/albumentations](https://github.com/albumentations-team/albumentations)),
    which contains many more image manipulation algorithms. You can also experiment
    with it and see which augmentations work well for this problem.
  id: totrans-596
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 除了内置的数据集扩充方式，我们还有专门的库来做这件事。其中之一是Albumentations（[https://github.com/albumentations-team/albumentations](https://github.com/albumentations-team/albumentations)），它包含更多的图像处理算法。你还可以尝试它并看看哪些扩充对这个问题有效。
- en: 'Many different pretrained models are available. We used Xception, but many
    others are out there. You can try them and see if they give a better performance.
    With Keras, it’s quite simple to use a different model: just import from a different
    package. For example, you can try ResNet50 and compare it with the results from
    Xception. Check the documentation for more information ([https://keras.io/api/applications/](https://keras.io/api/applications/)).'
  id: totrans-597
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 有许多可用的预训练模型。我们使用了Xception，但还有很多其他模型。你可以尝试它们并看看它们是否提供了更好的性能。使用Keras，使用不同模型非常简单：只需从不同的包中导入。例如，你可以尝试ResNet50并将其与Xception的结果进行比较。查看文档以获取更多信息（[https://keras.io/api/applications/](https://keras.io/api/applications/)）。
- en: 7.6.2 Other projects
  id: totrans-598
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6.2 其他项目
- en: 'There are many image classification projects that you can do:'
  id: totrans-599
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以做的许多图像分类项目：
- en: Cats or dogs ([https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats))
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 猫还是狗（[https://www.kaggle.com/c/dogs-vs-cats](https://www.kaggle.com/c/dogs-vs-cats)）
- en: Hotdog or not hotdog ([https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog))
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 热狗还是不是热狗（[https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog](https://www.kaggle.com/dansbecker/hot-dog-not-hot-dog)）
- en: Predicting the category of images from Avito’s dataset (online classifieds)
    ([https://www.kaggle.com/c/avito-duplicate-ads-detection](https://www.kaggle.com/c/avito-duplicate-ads-detection)).
    Note that many duplicates appear in this dataset, so be careful when splitting
    the data for validation. It might be a good idea to use the train/test split that
    the organizators prepared and do a bit of extra cleaning to make sure there are
    no duplicate images.
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Avito数据集（在线分类）预测图像类别（[https://www.kaggle.com/c/avito-duplicate-ads-detection](https://www.kaggle.com/c/avito-duplicate-ads-detection)）。请注意，这个数据集中存在许多重复项，因此在分割数据用于验证时要小心。使用组织者准备的训练/测试分割并进行一些额外的清理以确保没有重复图像可能是个好主意。
- en: Summary
  id: totrans-603
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: TensorFlow is a framework for building and using neural networks. Keras is a
    library on top of TensorFlow that makes training models simpler.
  id: totrans-604
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: TensorFlow是一个用于构建和使用神经网络的框架。Keras是TensorFlow之上的一个库，它使模型训练变得更加简单。
- en: 'For image processing, we need a special kind of neural network: a convolutional
    neural network. It consists of a series of convolutional layers followed by a
    series of dense layers.'
  id: totrans-605
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于图像处理，我们需要一种特殊的神经网络：卷积神经网络。它由一系列卷积层和一系列密集层组成。
- en: The convolutional layers in a neural network convert an image to its vector
    representation. This representation contains high-level features. The dense layers
    use these features to make the prediction.
  id: totrans-606
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络中的卷积层将图像转换为它的向量表示。这种表示包含高级特征。密集层使用这些特征来进行预测。
- en: We don’t need to train a convolutional neural network from scratch. We can use
    pretrained models on ImageNet for general-purpose classification.
  id: totrans-607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们不需要从头开始训练卷积神经网络。我们可以使用 ImageNet 上的预训练模型进行通用分类。
- en: Transfer learning is the process of adjusting a pretrained model to our problem.
    We keep the original convolutional layers but create new dense layers. This significantly
    reduces the time we need to train a model.
  id: totrans-608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 迁移学习是将预训练模型调整到我们问题的过程。我们保留原始的卷积层，但创建新的密集层。这显著减少了训练模型所需的时间。
- en: We use dropout to prevent overfitting. At each iteration, it freezes a random
    part of the network, so only the other part can be used for training. This allows
    the network to generalize better.
  id: totrans-609
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们使用 dropout 来防止过拟合。在每次迭代中，它会冻结网络的一部分，这样只有其他部分才能用于训练。这使网络能够更好地泛化。
- en: We can create more training data from existing images by rotating them, flipping
    them vertically and horizontally, and doing other transformations. This process
    is called data augmentation, and it adds more variability to the data and reduces
    the risk of overfitting.
  id: totrans-610
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过旋转、垂直和水平翻转现有图像以及进行其他转换来从现有图像中创建更多训练数据。这个过程称为数据增强，它增加了数据的变异性，并减少了过拟合的风险。
- en: In this chapter, we have trained a convolutional neural network for classifying
    images of clothing. We can save it, load it, and use it inside a Jupyter Notebook.
    This is not enough to use it in production.
  id: totrans-611
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们训练了一个用于分类服装图像的卷积神经网络。我们可以将其保存、加载，并在 Jupyter Notebook 中使用它。但这还不足以在生产环境中使用。
- en: 'In the next chapter, we show how to use it in production and talk about two
    ways of productionizing deep learning models: TensorFlow Lite in AWS Lambda and
    TensorFlow Serving in Kubernetes.'
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将展示如何在生产环境中使用它，并讨论两种生产化深度学习模型的方式：AWS Lambda 中的 TensorFlow Lite 和 Kubernetes
    中的 TensorFlow Serving。
- en: Answers to exercises
  id: totrans-613
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习答案
- en: Exercise 7.1 A) True
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习 7.1 A) 正确
- en: Exercise 7.2 B) Freeze a random part of a model, so it doesn’t get updated during
    one iteration of training.
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习 7.2 B) 冻结模型的一部分，使其在训练的一个迭代中不更新。
- en: Exercise 7.3 D) All of the above
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 练习 7.3 D) 以上所有
