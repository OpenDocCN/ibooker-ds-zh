- en: '13 Final notes: Container-native Vert.x'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 最终笔记：容器原生 Vert.x
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Efficiently building container images with Jib
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Jib 高效构建容器镜像
- en: Configuring Vert.x clustering to work in a Kubernetes cluster
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置 Vert.x 集群以在 Kubernetes 集群中工作
- en: Deploying Vert.x services to a Kubernetes cluster
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 Vert.x 服务部署到 Kubernetes 集群
- en: Using Skaffold and Minikube for local development
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Skaffold 和 Minikube 进行本地开发
- en: Exposing health checks and metrics
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 公开健康检查和指标
- en: By now you should have a solid understanding of what a reactive application
    is, and how Vert.x can help you build scalable, resource-efficient, and resilient
    services. In this chapter we’ll discuss some of the main concerns related to deploying
    and operating a Vert.x application in a Kubernetes cluster container environment.
    You will learn how to prepare Vert.x services to work well in Kubernetes and how
    to use efficient tools to package container images and run them locally. You will
    also learn how to expose health checks and metrics to better integrate services
    in a container environment.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，您应该对反应式应用程序是什么以及 Vert.x 如何帮助您构建可扩展、资源高效和具有弹性的服务有一个坚实的理解。在本章中，我们将讨论与在 Kubernetes
    集群容器环境中部署和运行 Vert.x 应用程序相关的一些主要问题。您将学习如何准备 Vert.x 服务以便在 Kubernetes 中良好运行，以及如何使用高效的工具打包容器镜像并在本地运行它们。您还将学习如何公开健康检查和指标，以更好地将服务集成到容器环境中。
- en: This chapter is optional, given that the core objectives of the book are about
    teaching yourself reactive concepts and practices. Still, Kubernetes is a popular
    deployment target, and it is worth learning how to make Vert.x applications first-class
    citizens in such environments.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑到本书的核心目标是教授您自我学习反应式概念和实践，本章是可选的。然而，Kubernetes 是一个流行的部署目标，学习如何使 Vert.x 应用程序成为此类环境中的首选公民是值得的。
- en: In this chapter I’ll assume you have a basic understanding of containers, Docker,
    and Kubernetes, which are covered in depth in other books such as Marko Lukša’s
    *Kubernetes in Action*, second edition (Manning, 2020) and *Docker in Action*,
    second edition, by Jeff Nickoloff and Stephen Kuenzli (Manning, 2019). If you
    don’t know much about those topics, you should still be able to understand and
    run the examples in this chapter, and you’ll learn some Kubernetes basics along
    the way, but I won’t spend time explaining the core concepts of Kubernetes, such
    as *pods* and *services*, or describe the subtleties of the `kubectl` command-line
    tool.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将假设您对容器、Docker 和 Kubernetes 有基本的了解，这些内容在其他书籍中有深入介绍，例如 Marko Lukša 的 *Kubernetes
    in Action* 第二版（Manning，2020）和 Jeff Nickoloff 以及 Stephen Kuenzli 编著的 *Docker in
    Action* 第二版（Manning，2019）。如果您对这些问题了解不多，您仍然能够理解和运行本章中的示例，并且您将在学习过程中了解一些 Kubernetes
    基础知识，但我不将花费时间解释 Kubernetes 的核心概念，如 *pods* 和 *services*，或描述 `kubectl` 命令行工具的细微差别。
- en: Tool versions
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 工具版本
- en: 'The chapter was written and tested with the following tool versions:'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 本章是在以下工具版本下编写和测试的：
- en: Minikube 1.11.0
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Minikube 1.11.0
- en: Skaffold 1.11.0
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Skaffold 1.11.0
- en: k9s 0.20.5 (optional)
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: k9s 0.20.5（可选）
- en: Dive 0.9.2 (optional)
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Dive 0.9.2（可选）
- en: 13.1 Heat sensors in a cloud
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 云中的热传感器
- en: In this final chapter, we’ll go back to a use case based on heat sensors, as
    it will be simpler than working with the 10k steps challenge application.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的最后，我们将回到基于热传感器的用例，因为它将比处理 10k 步挑战应用程序更简单。
- en: In this scenario, heat sensors regularly publish temperature updates, and an
    API can be used to retrieve the latest temperatures from all sensors, and also
    to identify sensors where the temperature is abnormal. The application is based
    on three microservices that you can find in the source code Git repository and
    that are illustrated in figure 13.1.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个场景中，热传感器会定期发布温度更新，可以使用 API 从所有传感器检索最新的温度，并且可以识别出温度异常的传感器。该应用程序基于三个微服务，您可以在源代码
    Git 仓库中找到这些微服务，如图 13.1 所示。
- en: 'Here is what each services does:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是每个服务的作用：
- en: '`heat-sensor-service`--Represents a heat sensor that publishes temperature
    updates over the Vert.x event bus. It exposes an HTTP API to fetch the current
    temperature.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heat-sensor-service`--代表一个通过 Vert.x 事件总线发布温度更新的热传感器。它提供了一个 HTTP API 用于获取当前温度。'
- en: '`sensor-gateway`--Collects temperature updates from all heat sensor services
    over the Vert.x event bus. It exposes an HTTP API for retrieving the latest temperature
    values.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`sensor-gateway`--通过 Vert.x 事件总线收集所有热传感器服务的温度更新。它提供了一个 HTTP API 用于检索最新的温度值。'
- en: '`heat-api`--An HTTP API for retrieving the latest temperature values and for
    detecting the sensors where temperatures are not within expected bounds.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`heat-api`--一个用于检索最新温度值和检测温度不在预期范围内的传感器的HTTP API。'
- en: The heat sensor service needs to be scaled to simulate multiple sensors, whereas
    the sensor gateway and API services work fine with just one instance of each.
    That being said, the latter two do not share state, so they can also be scaled
    to multiple instances if the workload requires it.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 热传感器服务需要扩展以模拟多个传感器，而传感器网关和API服务只需每个实例一个即可正常工作。换句话说，后两者不共享状态，因此如果工作负载需要，它们也可以扩展到多个实例。
- en: '![](../Images/CH13_F01_Ponge.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_F01_Ponge.png)'
- en: Figure 13.1 Use case overview
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 用例概述
- en: The heat API is the only service meant to be exposed outside the cluster. The
    sensor gateway is a cluster-internal service. The heat sensor services should
    just be deployed as instances inside the cluster, but they do not require a load
    balancer. The Vert.x cluster manager uses Hazelcast.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 热API是唯一旨在集群外部暴露的服务。传感器网关是集群内部服务。热传感器服务应仅作为集群内部的实例部署，但它们不需要负载均衡器。Vert.x集群管理器使用Hazelcast。
- en: Let’s quickly see the noteworthy code portions in these service implementations.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速查看这些服务实现中值得注意的代码部分。
- en: 13.1.1 Heat sensor service
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 热传感器服务
- en: The heat sensor service is based on the code found in the early chapters of
    this book, especially that of chapter 3\. The `update` method called from a timer
    set in the `scheduleNextUpdate` method has been updated as follows.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 热传感器服务基于本书早期章节中的代码，特别是第3章的内容。从`scheduleNextUpdate`方法中设置的定时器调用的`update`方法已更新如下。
- en: Listing 13.1 The new `update` method
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.1 新的`update`方法
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Publish to the event bus.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 发布到事件总线。
- en: ❷ Prepare a JSON temperature update payload.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 准备一个JSON温度更新有效负载。
- en: We still have the same logic, and we publish a JSON temperature update document
    to the event bus. We’ve also introduced the `makeJsonPayload` method, as it is
    also used for the HTTP endpoint, as shown next.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们仍然有相同的逻辑，并将JSON温度更新文档发布到事件总线。我们还引入了`makeJsonPayload`方法，因为它也用于HTTP端点，如下所示。
- en: Listing 13.2 Getting heat sensor data over HTTP
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.2 通过HTTP获取热传感器数据
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Send JSON data
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 发送JSON数据
- en: Finally we get the service configuration from environment variables in the `HeatSensor`
    verticle’s `start` method, as follows.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们在`HeatSensor` verticle的`start`方法中从环境变量获取服务配置，如下所示。
- en: Listing 13.3 Getting the sensor configuration from environment variables
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.3 从环境变量获取传感器配置
- en: '[PRE2]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Access the environment variables.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 访问环境变量。
- en: ❷ Get the HTTP port.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取HTTP端口。
- en: ❸ Get the event-bus destination.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取事件总线目标。
- en: Environment variables are great because they are easy to override when running
    the service. Since they are exposed as a Java `Map`, we can take advantage of
    the `getOrDefault` method to have default values.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量很棒，因为当运行服务时很容易覆盖它们。由于它们以Java `Map`的形式暴露，我们可以利用`getOrDefault`方法来设置默认值。
- en: Vert.x also provides the `vertx-config` module (not covered in this book) if
    you need more advanced configuration, like combining files, environment variables,
    and distributed registries. You can learn more about it in the Vert.x website
    documentation ([https://vertx.io/docs/](https://vertx.io/docs/)). For most cases,
    however, parsing a few environment variables using the Java `System` class is
    much simpler.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: Vert.x还提供了`vertx-config`模块（本书未涉及），如果您需要更高级的配置，如合并文件、环境变量和分布式注册表。您可以在Vert.x网站文档中了解更多信息（[https://vertx.io/docs/](https://vertx.io/docs/)）。然而，对于大多数情况，使用Java
    `System`类解析几个环境变量要简单得多。
- en: 13.1.2 Sensor gateway
  id: totrans-46
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 传感器网关
- en: The sensor gateway collects temperature updates from the heat sensor services
    over Vert.x event-bus communications. First, it fetches configuration from environment
    variables, as shown in listing 13.3, because it needs an HTTP port number and
    an event bus destination to listen to. The `start` method sets an event-bus consumer,
    as in the following listing.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器网关通过Vert.x事件总线通信从热传感器服务收集温度更新。首先，它从环境变量中获取配置，如列表13.3所示，因为它需要一个HTTP端口号和一个事件总线目标来监听。`start`方法设置了一个事件总线消费者，如下所示。
- en: Listing 13.4 Gateway event-bus consumer
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.4 网关事件总线消费者
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Register a handler.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 注册一个处理程序。
- en: ❷ Put it in a map.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将其放入映射中。
- en: Each incoming JSON update is put in a `data` field, which is a `HashMap<String,`
    `JsonObject>`, to store the last update of each sensor.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 每个传入的JSON更新都放入一个`data`字段中，该字段是一个`HashMap<String, JsonObject>`，用于存储每个传感器的最后更新。
- en: The HTTP API exposes the collected sensor data over the `/data` endpoint, which
    is handled by the following code.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: HTTP API通过`/data`端点暴露收集到的传感器数据，该端点由以下代码处理。
- en: Listing 13.5 Gateway data requests HTTP handler
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.5 网关数据请求HTTP处理器
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Collect entries in a JSON array.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在JSON数组中收集条目。
- en: ❷ Put the array in a JSON document and send it.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将数组放入JSON文档并发送。
- en: This method prepares a JSON response by assembling all collected data into an
    array, which is then wrapped in a JSON document.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法通过将所有收集到的数据组装成一个数组来准备JSON响应，然后将其包装在JSON文档中。
- en: 13.1.3 Heat API
  id: totrans-59
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.3 Heat API
- en: This service provides all sensor data, or just the data for services where temperatures
    are outside an expected correct value range. To do so, it makes HTTP requests
    to the sensor gateway.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 此服务提供所有传感器数据，或者仅提供温度超出预期正确值范围的服务数据。为此，它向传感器网关发送HTTP请求。
- en: The configuration is again provided through environment variables, as follows.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 配置再次通过环境变量提供，如下所示。
- en: Listing 13.6 Heat API configuration environment variables
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.6 Heat API配置环境变量
- en: '[PRE5]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The sensor gateway address
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 传感器网关地址
- en: ❷ The sensor gateway port number
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 传感器网关端口号
- en: ❸ The correct temperature lower bound
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 正确的温度下限
- en: ❹ The correct temperature higher bound
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 正确的温度上限
- en: The service resolves the sensor gateway address as well as the correct temperature
    range using environment variables. As you will see later, we can override the
    values when deploying the service to a cluster.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 服务使用环境变量解析传感器网关地址以及正确的温度范围。正如你稍后将会看到的，我们可以在将服务部署到集群时覆盖这些值。
- en: The `start` method configures the web client to make HTTP requests to the sensor
    gateway, and it also uses a Vert.x web router to expose API endpoints.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: '`start`方法配置Web客户端向传感器网关发送HTTP请求，并使用Vert.x Web路由器暴露API端点。'
- en: Listing 13.7 Heat API web client and routes
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.7 Heat API Web客户端和路由
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Prebind the web client host and port for requests.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 预绑定Web客户端主机和端口以发送请求。
- en: ❷ The router that exposes the API endpoints
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 暴露API端点的路由器
- en: Data is fetched from the sensor gateway with HTTP `GET` requests, as shown in
    the following listing.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 数据通过HTTP `GET`请求从传感器网关获取，如下所示。
- en: Listing 13.8 Fetching sensor data
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.8 获取传感器数据
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Make a request to /data.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向/data发送请求。
- en: ❷ Call the action handler.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调用动作处理器。
- en: ❸ Handle errors.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 处理错误。
- en: The `fetchData` method is generic, with a custom action given as the second
    parameter, so the two HTTP endpoints that we are exposing can reuse the request
    logic.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetchData`方法通用，第二个参数提供了一个自定义动作，因此我们暴露的两个HTTP端点可以重用请求逻辑。'
- en: The implementation of the `fetchAllData` method is shown next.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: '`fetchAllData`方法的实现如下所示。'
- en: Listing 13.9 Fetching all sensor data
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.9 获取所有传感器数据
- en: '[PRE8]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: This method doesn’t do anything special besides completing the HTTP request
    with the JSON data.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 此方法除了完成带有JSON数据的HTTP请求外，没有做任何特别的事情。
- en: The `sensorsOverLimits` method shown next is more interesting, as it filters
    the data.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的`sensorsOverLimits`方法更有趣，因为它会过滤数据。
- en: Listing 13.10 Filtering out-of-range sensor data
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.10 过滤超出范围的传感器数据
- en: '[PRE9]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ An array to collect over-limit data
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于收集超出限制数据的数组
- en: ❷ Use a Java stream to filter entries.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用Java流过滤条目。
- en: ❸ Cast from Object to JsonObject.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从Object转换为JsonObject。
- en: ❹ Filter based on temperature values.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 根据温度值进行过滤。
- en: ❺ Add to the array.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将其添加到数组中。
- en: ❻ Assemble the final JSON response.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 组装最终的JSON响应。
- en: The `sensorsOverLimits` method keeps only the entries where the temperature
    is not within the expected range. To do so, we take a functional processing approach
    using Java collection streams, and then return the response. Note that the `data`
    array in the response JSON document may be empty if all sensor values are correct.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '`sensorsOverLimits`方法仅保留温度不在预期范围内的条目。为此，我们采用功能处理方法，使用Java集合流，然后返回响应。请注意，如果所有传感器值都正确，响应JSON文档中的`data`数组可能为空。'
- en: Now that you have seen the main interesting points in the three service implementations,
    we can move on to the topic of actually deploying them in a Kubernetes cluster.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经看到了三个服务实现中的主要有趣点，我们可以继续讨论如何在Kubernetes集群中实际部署它们。
- en: 13.1.4 Deploying to a local cluster
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.4 在本地集群中部署
- en: There are many ways to run a local Kubernetes cluster. Docker Desktop embeds
    Kubernetes, so it may be all you need to run Kubernetes, if you have it running
    on your machine.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 运行本地 Kubernetes 集群有许多方法。Docker Desktop 内嵌 Kubernetes，因此如果您已经在您的机器上运行它，可能只需要它来运行
    Kubernetes。
- en: Minikube is another reliable option offered by the Kubernetes project ([https://
    minikube.sigs.k8s.io/docs/](https://minikube.sigs.k8s.io/docs/)). It deploys a
    small virtual machine on Windows, macOS, or Linux, which makes it perfect for
    creating disposable clusters for development. If anything goes wrong, you can
    easily destroy a cluster and start anew.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 是 Kubernetes 项目提供的另一个可靠选项 ([https://minikube.sigs.k8s.io/docs/](https://minikube.sigs.k8s.io/docs/))。它在
    Windows、macOS 或 Linux 上部署一个小型虚拟机，这使得它非常适合创建用于开发的可丢弃集群。如果发生任何问题，您可以轻松地销毁集群并重新开始。
- en: Another benefit of Minikube is that it offers environment variables for Docker
    daemons, so you can have your locally built container images available right inside
    the cluster. In other Kubernetes configurations, you would have to push images
    to private or public registries, which can slow the development feedback loop,
    especially when pushing a few hundred megabytes to public registries over a slow
    internet connection.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: Minikube 的另一个好处是它为 Docker 守护进程提供了环境变量，因此您可以将本地构建的容器镜像直接放在集群内部。在其他 Kubernetes
    配置中，您需要将镜像推送到私有或公共注册表，这可能会减慢开发反馈循环，尤其是在通过慢速互联网连接将几百兆字节推送到公共注册表时。
- en: I am assuming that you will use Minikube here, but feel free to use any other
    option.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我假设您将在这里使用 Minikube，但请随意使用任何其他选项。
- en: Tip If you have never used Kubernetes before, welcome! Although you will not
    become a Kubernetes expert by reading this section, running the commands should
    still give you an idea of what it is all about. The main concepts behind Kubernetes
    are quite simple, once you go beyond the vast ecosystem and terminology.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果您之前从未使用过 Kubernetes，欢迎加入！尽管通过阅读本节您不会成为 Kubernetes 专家，但运行命令应该仍然能给您一个大致的了解。一旦超越庞大的生态系统和术语，Kubernetes
    的主要概念相当简单。
- en: The following listing shows how to create a cluster with four CPUs and 8 GB
    of memory.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表显示了如何创建具有四个 CPU 和 8 GB 内存集群的方法。
- en: Listing 13.11 Creating a Minikube cluster
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.11 创建 Minikube 集群
- en: '[PRE10]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Enable the ingress add-on.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 启用 ingress 扩展。
- en: The flags and the output will differ based on your operating system and software
    versions. You may need to adjust these by looking at the current Minikube documentation
    (which may have been updated by the time you read this chapter). I allocated four
    CPUs and 8 GB of memory because this is comfortable on my laptop, but you could
    be fine with one CPU and less RAM.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 标志和输出将根据您的操作系统和软件版本而有所不同。您可能需要通过查看当前的 Minikube 文档（在您阅读此章节时可能已更新）来调整这些设置。我分配了四个
    CPU 和 8 GB 的内存，因为在我的笔记本电脑上这很舒适，但您可能只需要一个 CPU 和更少的 RAM。
- en: You can access a web dashboard by running the `minikube dashboard` command.
    Using the Minikube dashboard, you can look at the various Kubernetes resources
    and even perform some (limited) operations, such as scaling a service up and down
    or looking into logs.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过运行 `minikube dashboard` 命令来访问一个 Web 仪表板。使用 Minikube 仪表板，您可以查看各种 Kubernetes
    资源，甚至执行一些（有限的）操作，例如调整服务的上下文或查看日志。
- en: 'There is another dashboard that I find particularly efficient and can recommend
    that you try: K9s ([https://k9scli.io](https://k9scli.io)). It works as a command-line
    tool, and it can very quickly move between Kubernetes resources, access pod logs,
    update replica counts, and so on.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一个我发现特别高效且可以推荐的仪表板：K9s ([https://k9scli.io](https://k9scli.io))。它作为一个命令行工具，可以非常快速地在
    Kubernetes 资源之间切换，访问 pod 日志，更新副本数量等等。
- en: 'Kubernetes has a command-line tool called `kubectl` that you can use to perform
    any actions: deploying services, collecting logs, configuring DNS, and more. `kubectl`
    is the Swiss army knife of Kubernetes. We could use `kubectl` to apply the Kubernetes
    resource definitions found in each service’s `k8s/` folder. I will later describe
    the resources in the `k8s/` folders. If you are new to Kubernetes, all you need
    to know right now is that these files tell Kubernetes how to deploy the three
    services in this chapter.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 有一个名为 `kubectl` 的命令行工具，您可以使用它执行任何操作：部署服务、收集日志、配置 DNS 等。`kubectl`
    是 Kubernetes 的瑞士军刀。我们可以使用 `kubectl` 应用每个服务 `k8s/` 文件夹中找到的 Kubernetes 资源定义。我将在稍后描述
    `k8s/` 文件夹中的资源。如果您是 Kubernetes 新手，您现在需要了解的是，这些文件告诉 Kubernetes 如何部署本章中的三个服务。
- en: There is a better tool for improving your local Kubernetes development experience
    called Skaffold ([https://skaffold.dev](https://skaffold.dev)). Instead of using
    Gradle (or Maven) to build the services and package them, and then using `kubectl`
    to deploy to Kubernetes, Skaffold is able to do it all for us, avoiding unnecessary
    builds using caching, performing deployments, aggregating all logs, and cleaning
    everything on exit.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个更好的工具可以改善您的本地Kubernetes开发体验，名为Skaffold ([https://skaffold.dev](https://skaffold.dev))。与使用Gradle（或Maven）构建服务和打包它们，然后使用`kubectl`部署到Kubernetes相比，Skaffold能够为我们完成所有这些工作，避免了不必要的构建使用缓存，执行部署，汇总所有日志，并在退出时清理一切。
- en: You first need to download and install Skaffold on your machine. Skaffold works
    out of the box with Minikube, so no additional configuration is needed. All it
    needs is a skaffold.yaml resource descriptor, as shown in the following listing
    (and is included at the root of the chapter13 folder in the Git repository).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 您首先需要在您的机器上下载并安装Skaffold。Skaffold与Minikube无缝工作，因此不需要任何额外的配置。它只需要一个skaffold.yaml资源描述符，如下所示（并在Git仓库的第13章文件夹的根目录中包含）。
- en: Listing 13.12 Skaffold configuration
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.12 Skaffold配置
- en: '[PRE11]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Name of a container image to produce
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要生成的容器镜像的名称
- en: ❷ Project containing the source code
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 包含源代码的项目
- en: ❸ Also apply YAML files.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 还应用YAML文件。
- en: From the chapter13 folder, you can run `skaffold dev`, and it will build the
    projects, deploy container images, expose logs, and watch for file changes. Figure
    13.2 shows a screenshot of Skaffold running.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 从第13章文件夹中，您可以运行 `skaffold dev`，它将构建项目、部署容器镜像、公开日志以及监视文件更改。图13.2显示了Skaffold运行的截图。
- en: '![](../Images/CH13_F02_Ponge.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH13_F02_Ponge.png)'
- en: Figure 13.2 Screenshot of Skaffold running services
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.2 Skaffold运行服务的截图
- en: Congratulations, you now have the services running in your (local) cluster!
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜，您现在已经在您的（本地）集群中运行了服务！
- en: You don’t have to use Skaffold, but for a good local development experience,
    this is a tool you can rely on. It hides some of the complexity of the `kubectl`
    command-line interface, and it bridges the gap between project build tools (such
    as Gradle or Maven) and the Kubernetes environment.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 您不必使用Skaffold，但为了获得良好的本地开发体验，这是一个您可以信赖的工具。它隐藏了一些`kubectl`命令行界面的复杂性，并且它弥合了项目构建工具（如Gradle或Maven）与Kubernetes环境之间的差距。
- en: The following listing shows a few commands that check on the services deployed
    in a cluster.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了一些检查集群中部署的服务的命令。
- en: Listing 13.13 Checking the exposed services
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.13检查公开的服务
- en: '[PRE12]'
  id: totrans-124
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Network tunnel, run in a separate terminal
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 网络隧道，在单独的终端中运行
- en: ❷ Get the services.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取服务。
- en: The `minikube tunnel` command is important for accessing `LoadBalancer` services,
    and it should be run in a separate terminal. Note that it will likely require
    you to enter your password, as the command needs to adjust your current network
    settings.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`minikube tunnel` 命令对于访问 `LoadBalancer` 服务非常重要，并且应该在单独的终端中运行。请注意，它可能需要您输入密码，因为该命令需要调整您当前的网络设置。'
- en: 'You can alternatively use the following Minikube command to obtain a URL for
    a `LoadBalancer` service without `minikube tunnel`:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用以下Minikube命令来获取`LoadBalancer`服务的URL，而无需`minikube tunnel`：
- en: '[PRE13]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip The IP addresses for the services will be different on your machine. They
    will also change as you delete and create new services, so don’t make any assumptions
    about IP addresses in Kubernetes.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：服务的IP地址在您的机器上可能会有所不同。随着您删除和创建新的服务，它们也会发生变化，所以不要对Kubernetes中的IP地址做出任何假设。
- en: This works because Minikube also exposes `LoadBalancer` services as `NodePort`
    on the Minikube instance IP address. Both methods are equivalent when using Minikube,
    but the one using `minikube tunnel` is closer to what you would get with a production
    cluster, since the service is accessed via a cluster-external IP address.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这之所以有效，是因为Minikube也将`LoadBalancer`服务作为`NodePort`暴露在Minikube实例IP地址上。当使用Minikube时，这两种方法都是等效的，但使用`minikube
    tunnel`的方法更接近您在生产集群中会得到的结果，因为服务是通过集群外部IP地址访问的。
- en: Now that you have a way to access the heat API service, you can make a few requests.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您有了访问heat API服务的方法，您可以发出一些请求。
- en: Listing 13.14 Interacting with the heat API service
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.14与heat API服务交互
- en: '[PRE14]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Get all data.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取所有数据。
- en: ❷ Get out-of-range sensor data.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取超出范围的传感器数据。
- en: You can also access the sensor gateway using port forwarding, as shown next.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 您还可以使用端口转发来访问传感器网关，如下所示。
- en: Listing 13.15 Interacting with the sensor gateway
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 列表13.15与传感器网关交互
- en: '[PRE15]'
  id: totrans-139
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Port forward from a service to a local port (run in a separate terminal).
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从服务到本地端口的端口转发（在单独的终端中运行）。
- en: ❷ Call the service.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调用服务。
- en: The `kubectl port-forward` command must be run in another terminal, and as long
    as it is running, the local port 8080 forwards to the sensor gateway service inside
    the cluster. This is very convenient for accessing anything that is running in
    the cluster without being exposed as a `LoadBalancer` service.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 必须在另一个终端中运行 `kubectl port-forward` 命令，并且只要它在运行，本地端口 8080 就会转发到集群内部的服务网关。这对于访问集群中运行的所有内容来说非常方便，而无需将其暴露为
    `LoadBalancer` 服务。
- en: Finally, we can make a DNS query to see how the heat sensor headless services
    are resolved. The following listing uses a third-party image that contains the
    `dig` tool, which can be used to make DNS requests.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以进行 DNS 查询以查看热传感器无头服务是如何解析的。以下列表使用了一个包含 `dig` 工具的第三方镜像，该工具可以用来发送 DNS 请求。
- en: Listing 13.16 DNS query to discover the headless heat sensor services
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.16 DNS 查询以发现无头热传感器服务
- en: '[PRE16]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ An image with dig installed
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 安装了 dig 的镜像
- en: ❷ Run a DNS query.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 运行一个 DNS 查询。
- en: Now if we increase the number of replicas, as in the following listing, we can
    see that the DNS reflects the change.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 现在如果我们增加副本的数量，如以下列表所示，我们可以看到 DNS 反映了这一变化。
- en: Listing 13.17 Increasing the number of heat sensor service replicas
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.17 增加热传感器服务副本的数量
- en: '[PRE17]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Scale to five replicas.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 扩展到五个副本。
- en: Also, if we make HTTP requests like in listing 13.14, we can see that we have
    data from five sensors.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，如果我们像在列表 13.14 中那样发送 HTTP 请求，我们可以看到有来自五个传感器的数据。
- en: Now that we have deployed the services and interacted with them, let’s look
    at how deployment in Kubernetes works for Vert.x services.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经部署了服务并与它们进行了交互，让我们看看 Kubernetes 中 Vert.x 服务的部署是如何工作的。
- en: 13.2 Making the services work in Kubernetes
  id: totrans-154
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 在 Kubernetes 中使服务工作
- en: Making a service *work* in Kubernetes is fairly transparent for the most part,
    especially when it has been designed to be agnostic of the target runtime environment.
    Whether it runs in a container, in a virtual machine, or on bare-metal should
    never be an issue. Still, there are some aspects where adaptation and configuration
    need to be done due to how Kubernetes works.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Kubernetes 中使服务 *工作* 对于大多数情况来说相当透明，特别是当它被设计为对目标运行时环境无感知时。无论它是在容器中、虚拟机中还是在裸机上运行，都不应该成为问题。尽管如此，由于
    Kubernetes 的工作方式，仍有一些方面需要适应和配置。
- en: In our case, the only major adaptation that has to be made is configuring the
    cluster manager so instances can discover themselves and messages can be sent
    across the distributed event bus. The rest is just a matter of building container
    images of the services and writing Kubernetes resource descriptors to deploy the
    services.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下，唯一需要进行的重大适应是配置集群管理器，以便实例可以自我发现，并且可以在分布式事件总线之间发送消息。其余的只是构建服务的容器镜像并将 Kubernetes
    资源描述符写入以部署服务的问题。
- en: Let’s start by talking about building container images.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从讨论构建容器镜像开始。
- en: 13.2.1 Building container images
  id: totrans-158
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.1 构建容器镜像
- en: There are many ways to build a container image, which technically is based on
    the *OCI Image Format* (OCIIF; [https://github.com/opencontainers/image-spec](https://github.com/opencontainers/image-spec)).
    The most basic way to build such an image is to write a `Dockerfile` and use the
    `docker` `build` command to build an image. Note that `Dockerfile` descriptors
    can be used by other tools such as Podman ([https://podman.io/](https://podman.io/))
    or Buildah ([https://github.com/containers/buildah](https://github.com/containers/buildah)),
    so you don’t actually need Docker to build container images.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 构建容器镜像有很多方法，技术上基于 *OCI Image Format* (OCIIF; [https://github.com/opencontainers/image-spec](https://github.com/opencontainers/image-spec))。构建此类镜像的最基本方法是编写一个
    `Dockerfile` 并使用 `docker` `build` 命令来构建镜像。请注意，`Dockerfile` 描述符可以被其他工具如 Podman
    ([https://podman.io/](https://podman.io/)) 或 Buildah ([https://github.com/containers/buildah](https://github.com/containers/buildah))
    使用，因此实际上您不需要 Docker 来构建容器镜像。
- en: You could thus choose a base image with Java, and then copy a self-contained
    executable Jar file to be run. While this approach is simple and works just fine,
    it means that for every change in the source code, you need to build a new image
    layer of the size of the Jar file that includes all dependencies such as Vert.x,
    Netty, and more. The compiled classes of a service typically weigh a few kilobytes,
    while a self-contained Jar file weighs a few megabytes.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你可以选择一个带有 Java 的基础镜像，然后复制一个自包含的可执行 Jar 文件以运行。虽然这种方法简单且效果良好，但它意味着每次源代码发生变化时，你都需要构建一个新的镜像层，其大小与包含所有依赖项（如
    Vert.x、Netty 等）的 Jar 文件大小相当。一个服务的编译类通常只有几千字节，而一个自包含的 Jar 文件则有几兆字节。
- en: Alternatively, you can either craft a `Dockerfile` with multiple stages and
    layers, or you can use a tool like Jib to automatically do the equivalent for
    you ([https://github.com/GoogleContainerTools/jib](https://github.com/GoogleContainerTools/jib)).
    As shown in figure 13.3, Jib assembles different layers to make a container image.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，你可以创建一个包含多个阶段和层的 `Dockerfile`，或者使用 Jib 这样的工具自动为你完成等效操作（[https://github.com/GoogleContainerTools/jib](https://github.com/GoogleContainerTools/jib)）。如图
    13.3 所示，Jib 将不同的层组装成一个容器镜像。
- en: Project dependencies are put just above the base image; they are typically bigger
    than the application code and resources, and they also tend not to change very
    often, except when upgrading versions and adding new dependencies. When a project
    has snapshot dependencies, they appear as a layer on top of the fixed-version
    dependencies, because newer snapshots appear frequently. The resources and class
    files change more often, and they are typically light on disk use, so they end
    up on top. This clever layering approach does not just save disk space; it also
    improves build time, since layers can often be reused.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 项目依赖项放置在基础镜像之上；它们通常比应用程序代码和资源大，而且它们也不太经常改变，除非升级版本或添加新的依赖项。当一个项目有快照依赖项时，它们会作为固定版本依赖项之上的一个层出现，因为新的快照频繁出现。资源和类文件更改得更频繁，它们在磁盘上的使用通常较少，因此它们最终位于顶部。这种巧妙的分层方法不仅节省了磁盘空间，还提高了构建时间，因为层通常可以重用。
- en: '![](../Images/CH13_F03_Ponge.png)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH13_F03_Ponge.png)'
- en: Figure 13.3 Container image layers with Jib
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 使用 Jib 的容器镜像层
- en: Jib offers Maven and Gradle plugins, and it builds container images by deriving
    information from a project. Jib is also great because it is purely written in
    Java and it does not need Docker to build images, so you can produce container
    images without any third-party tools. It can also publish container images to
    registries and Docker daemons, which is useful in development.
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: Jib 提供了 Maven 和 Gradle 插件，并通过从项目中提取信息来构建容器镜像。Jib 还很棒，因为它完全用 Java 编写，并且不需要 Docker
    来构建镜像，因此你可以不使用任何第三方工具来生成容器镜像。它还可以将容器镜像发布到注册表和 Docker 守护程序，这在开发中非常有用。
- en: Once the Jib plugin has been applied, all you need is a few configuration elements,
    as in the following listing for a Gradle build (the Maven version is equivalent,
    albeit done in XML).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦应用了 Jib 插件，你只需要几个配置元素，如下所示为 Gradle 构建示例（Maven 版本类似，但使用 XML 实现）。
- en: Listing 13.18 Configuring the Jib Gradle plugin
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.18 配置 Jib Gradle 插件
- en: '[PRE18]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Base image
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 基础镜像
- en: ❷ Image name
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 图片名称
- en: ❸ Image tags
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 镜像标签
- en: ❹ Main class to run
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 要运行的主类
- en: ❺ JVM tuning flags
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ JVM 调优标志
- en: ❻ Ports to be exposed by the container
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 容器需要暴露的端口
- en: ❼ Run as this user.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 以此用户身份运行。
- en: The base image comes from the AdoptOpenJDK project, which publishes many builds
    of OpenJDK ([https://adoptopenjdk.net](https://adoptopenjdk.net)). Here we are
    using OpenJDK 11 as a *Java Runtime Environment* (JRE) rather than a full *Java
    Development Kit* (JDK). This saves disk space, as we just need a runtime, and
    a JDK image is bigger than a JRE image. The `ubi-minimal` part is because we use
    an AdoptOpenJDK build variant based on the Red Hat Universal Base Image, where
    the “minimal” variant minimizes the embedded dependencies.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 基础镜像来自 AdoptOpenJDK 项目，该项目发布了多个 OpenJDK 构建（[https://adoptopenjdk.net](https://adoptopenjdk.net)）。在这里，我们使用
    OpenJDK 11 作为 *Java 运行时环境*（JRE）而不是完整的 *Java 开发工具包*（JDK）。这节省了磁盘空间，因为我们只需要运行时，而
    JDK 镜像比 JRE 镜像大。`ubi-minimal` 部分是因为我们使用基于 Red Hat Universal Base Image 的 AdoptOpenJDK
    构建变体，其中“minimal”变体最小化了嵌入的依赖项。
- en: Jib needs to know the main class to execute as well as the ports to be exposed
    outside the container. In the case of the heat sensor and sensor gateway services,
    we need to expose port 8080 for the HTTP service and port 5701 for the Vert.x
    clustering with Hazelcast. The JVM tuning is limited to disabling the JVM bytecode
    verifier so it boots marginally faster, and also using /dev/urandom for random
    number generation (the default /dev/random pseudo-file may block when a container
    starts and there isn’t enough entropy). Finally, we run as user `nobody` in group
    `nobody` to ensure the process runs as an unprivileged user inside the container.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: Jib 需要知道要执行的主类以及要暴露在容器外的端口。在热量传感器和传感器网关服务的情况下，我们需要暴露端口 8080 以供 HTTP 服务使用，以及端口
    5701 以供与 Hazelcast 的 Vert.x 聚合。JVM 调优仅限于禁用 JVM 字节码验证器，以便略微加快启动速度，并使用 /dev/urandom
    进行随机数生成（默认的 /dev/random 伪文件可能在容器启动时由于熵不足而阻塞）。最后，我们以用户 `nobody` 和组 `nobody` 运行，以确保进程在容器内以非特权用户身份运行。
- en: You can build an image and inspect it as shown next.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以构建一个镜像并按以下所示进行检查。
- en: Listing 13.19 Building a service container image to a Docker daemon
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.19 将服务容器镜像构建到 Docker 守护进程
- en: '[PRE19]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Build a container image for the heat sensor service and push it to a local
    Docker daemon.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为热量传感器服务构建一个容器镜像并将其推送到本地 Docker 守护进程。
- en: ❷ Inspect the container image.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查容器镜像。
- en: All three services’ container images build the same way. The only configuration
    difference is that the heat API service only exposes port 8080, since it does
    not need a cluster manager.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 所有三种服务的容器镜像都是按照相同的方式构建的。唯一的配置差异是热量 API 服务只暴露端口 8080，因为它不需要集群管理器。
- en: Tip You can use a tool like Dive ([https://github.com/wagoodman/dive](https://github.com/wagoodman/dive))
    if you are curious about the content of the different layers produced to prepare
    the container images of the three services.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：如果你对生成容器镜像的三种服务的不同层的内容感兴趣，可以使用 Dive（[https://github.com/wagoodman/dive](https://github.com/wagoodman/dive)）这样的工具。
- en: Speaking of clustering, there is configuration work to be done!
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 说到聚类，还有配置工作要做！
- en: 13.2.2 Clustering and Kubernetes
  id: totrans-186
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.2 聚类和 Kubernetes
- en: Both Hazelcast and Infinispan, which you used in chapter 3, by default use multicast
    communications to discover nodes. This is great for local testing and many bare-metal
    server deployments, but multicast communications are not possible in a Kubernetes
    cluster. If you run the containers as is on Kubernetes, the heat sensor services
    and sensor gateway instances will not be able to communicate over the event bus.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中使用的 Hazelcast 和 Infinispan 默认使用多播通信来发现节点。这对于本地测试和许多裸机服务器部署来说很棒，但在 Kubernetes
    集群中无法使用多播通信。如果你在 Kubernetes 上以原样运行容器，热量传感器服务和传感器网关实例将无法通过事件总线进行通信。
- en: 'These cluster managers can, of course, be configured to perform service discovery
    in Kubernetes. We will briefly cover the case of Hazelcast, where two discovery
    modes are possible:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这些集群管理器可以被配置为在 Kubernetes 中执行服务发现。我们将简要介绍 Hazelcast 的情况，其中有两种可能的发现模式：
- en: Hazelcast can connect to the Kubernetes API to listen for and discover pods
    matching a request, such as a desired label and value.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hazelcast 可以连接到 Kubernetes API，以监听和发现与请求匹配的 pod，例如所需的标签和值。
- en: Hazelcast can periodically make DNS queries to discover all pods for a given
    Kubernetes (headless) service.
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hazelcast 可以定期进行 DNS 查询以发现给定 Kubernetes（无头）服务的所有 pod。
- en: The DNS approach is more limited.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: DNS 方法更为有限。
- en: Instead, let’s use the Kubernetes API and configure Hazelcast to use it. By
    default, the Hazelcast Vert.x cluster manager reads configuration from a cluster.xml
    resource. The following listing shows the relevant configuration excerpt of the
    heat-sensor-service/ src/main/resource/cluster.xml file.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，让我们使用 Kubernetes API 并配置 Hazelcast 使用它。默认情况下，Hazelcast Vert.x 集群管理器从 cluster.xml
    资源中读取配置。以下列表显示了 heat-sensor-service/src/main/resource/cluster.xml 文件的相关配置摘录。
- en: Listing 13.20 Kubernetes configuration for Hazelcast discovery
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.20 Hazelcast 发现的 Kubernetes 配置
- en: '[PRE20]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Disable multicast communications.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 禁用多播通信。
- en: ❷ Enable the Kubernetes discovery strategy.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 启用 Kubernetes 发现策略。
- en: ❸ Match services with the label vertx-in-action.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将服务与标签 vertx-in-action 匹配。
- en: ❹ Match services with the value chapter13 for label vertx-in-action.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将服务与标签 vertx-in-action 的值 chapter13 匹配。
- en: We disable the default discovery mechanism and enable the Kubernetes ones. Here
    Hazelcast forms clusters of pods that belong to a service where a `vertx-in-action`
    label is defined with the value `chapter13`. Since we opened port 5701, the pods
    will be able to connect. Note that the configuration is the same for the sensor
    gateway.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们禁用了默认的发现机制，并启用了 Kubernetes 的发现机制。在这里，Hazelcast 形成了属于具有 `vertx-in-action` 标签且值为
    `chapter13` 的服务的 pod 集群。由于我们打开了端口 5701，pod 将能够连接。请注意，配置对于传感器网关是相同的。
- en: Since Hazelcast needs to read from the Kubernetes API, we need to ensure that
    we have permissions using the Kubernetes role-based access control (RBAC). To
    do so, we need to apply the `ClusterRoleBinding` resource of the following listing
    and the k8s/rbac.yaml file.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Hazelcast 需要从 Kubernetes API 中读取，我们需要确保我们有使用 Kubernetes 基于角色的访问控制 (RBAC)
    的权限。为此，我们需要应用以下列表中的 `ClusterRoleBinding` 资源和 k8s/rbac.yaml 文件。
- en: Listing 13.21 RBAC to grant view access to the Kubernetes API
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.21 RBAC 授予 Kubernetes API 查看访问权限
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Resource type
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 资源类型
- en: ❷ View role reference
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 查看角色参考
- en: The last thing we need to do is ensure that the heat sensor and gateway services
    run with clustering enabled. In both cases the code is similar. The following
    listing shows the `main` method for the heat sensor service.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 我们最后需要确保热传感器和网关服务在启用集群的情况下运行。在这两种情况下，代码是相似的。以下列表显示了热传感器服务的 `main` 方法。
- en: Listing 13.22 Enabling clustering for the heat sensor service
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.22 启用热传感器服务的集群功能
- en: '[PRE22]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Get the IPv4 address of the host.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取主机的 IPv4 地址。
- en: ❷ Customize the event-bus options.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 自定义事件总线选项。
- en: ❸ Set the host address.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置主机地址。
- en: ❹ Set the host that other nodes need to talk to.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置其他节点需要通信的主机。
- en: ❺ Start Vert.x in cluster mode.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 以集群模式启动 Vert.x。
- en: We start a clustered Vert.x context and pass options to customize the event-bus
    configuration. In most cases you don’t need to do any extra tuning here, but in
    the context of Kubernetes, clustering will likely resolve to `localhost` rather
    the actual host IPv4 address. This is why we first resolve the IPv4 address and
    then set the event-bus configuration host to that address, so the other nodes
    can talk to it.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 我们启动一个集群化的 Vert.x 上下文，并传递选项来自定义事件总线配置。在大多数情况下，你在这里不需要进行任何额外的调整，但在 Kubernetes
    的上下文中，集群可能会解析为 `localhost` 而不是实际的 IPv4 地址。这就是为什么我们首先解析 IPv4 地址，然后将事件总线配置的主机设置为该地址，这样其他节点就可以与之通信。
- en: Tip The event-bus network configuration performed in listing 13.22 will be done
    automatically in future Vert.x releases. I show it here because it can help you
    troubleshoot distributed event-bus configuration issues in contexts other than
    Kubernetes.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：列表 13.22 中执行的事件总线网络配置将在未来的 Vert.x 版本中自动完成。我在这里展示它是因为它可以帮助你在 Kubernetes 之外的环境中调试分布式事件总线配置问题。
- en: 13.2.3 Kubernetes deployment and service resources
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.2.3 Kubernetes 部署和服务资源
- en: Now that you know how to put your services into containers and how to make sure
    Vert.x clustering works in Kubernetes, we need to discuss resource descriptors.
    Indeed, Kubernetes needs some descriptors to deploy container images to pods and
    expose services.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道如何将你的服务放入容器中，以及如何确保 Vert.x 集群在 Kubernetes 中工作，我们需要讨论资源描述符。实际上，Kubernetes
    需要一些描述符来部署容器镜像到 pod 中并公开服务。
- en: Let’s start with the heat sensor service’s *deployment descriptor,* shown in
    the following listing.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从热传感器服务的 *部署描述符* 开始，如下所示。
- en: Listing 13.23 Heat sensor service deployment descriptor
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.23 热传感器服务部署描述符
- en: '[PRE23]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ This is a deployment resource.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这是一个部署资源。
- en: ❷ Name of the deployment
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 部署的名称
- en: ❸ Deploy four instances by default.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 默认部署四个实例。
- en: ❹ Rolling update configuration for Hazelcast
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ Hazelcast 的滚动更新配置
- en: ❺ Container image to deploy
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 部署的容器镜像
- en: 'This deployment descriptor by default deploys four pods of the `vertx-in-action/
    heat-sensor-service` container image. Deploying pods is a good first step, but
    we also need a *service definition* that maps to these pods. This is especially
    important for Hazelcast: remember that these instances discover themselves through
    Kubernetes services with the label `vertx-in-action` and value `chapter13`.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 此部署描述符默认部署了四个 `vertx-in-action/ heat-sensor-service` 容器镜像的 pod。部署 pod 是一个好的第一步，但我们还需要一个
    *服务定义* 来映射这些 pod。这对于 Hazelcast 尤为重要：记住，这些实例通过具有标签 `vertx-in-action` 且值为 `chapter13`
    的 Kubernetes 服务来自我发现。
- en: Kubernetes performs *rolling updates* when a deployment is updated by progressively
    replacing pods of the older configuration with pods of the newer configuration.
    It is best to set the values of `maxSurge` and `maxUnavailable` to `1`. When you
    do so, Kubernetes replaces pods one after the other, so the cluster state is smoothly
    transferred to the new pods. You can avoid this configuration and let Kubernetes
    be more aggressive when rolling updates, but the cluster state may be inconsistent
    for some time.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 当部署通过逐步替换旧配置的 pod 为新配置的 pod 来更新时，Kubernetes 执行 *滚动更新*。最好将 `maxSurge` 和 `maxUnavailable`
    的值设置为 `1`。这样做时，Kubernetes 会依次替换 pod，因此集群状态会平稳地转移到新 pod。您可以避免此配置，并让 Kubernetes
    在滚动更新时更加激进，但集群状态可能在一段时间内不一致。
- en: The following listing shows the *service resource definition*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表显示了 *服务资源定义*。
- en: Listing 13.24 Heat sensor service definition
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.24 热传感器服务定义
- en: '[PRE24]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: ❶ Label used for Hazelcast discovery
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于 Hazelcast 发现的标签
- en: ❷ We want a “headless” service.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们想要一个“无头”服务。
- en: ❸ Matches pods with this label/value pair
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 匹配具有此标签/值对的 pod
- en: ❹ The ports to expose
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 要公开的端口
- en: The service descriptor exposes a *headless* service, which is to say that there
    is no load balancing among the pods. Because each service is a sensor, they cannot
    be taken one for the other. Headless services can instead be discovered using
    DNS queries that return the list of all pods. You saw in listing 13.16 how headless
    services could be discovered using DNS queries.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 服务描述符公开了一个 *无头* 服务，这意味着在 pod 之间没有负载均衡。因为每个服务都是一个传感器，它们不能互相替代。无头服务可以通过返回所有 pod
    列表的 DNS 查询来发现。您在列表 13.16 中看到了如何使用 DNS 查询来发现无头服务。
- en: The deployment descriptor for the sensor gateway is nearly identical to that
    of the heat sensor service, as you can see in the next listing.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 传感器网关的部署描述符几乎与热传感器服务的描述符相同，您可以在下面的列表中看到。
- en: Listing 13.25 Sensor gateway deployment descriptor
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.25 传感器网关部署描述符
- en: '[PRE25]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: ❶ Container image
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 容器镜像
- en: Aside from the names, you can note that we did not specify the replica count,
    which by default is 1\. The service definition is shown in the following listing.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 除了名称外，您还可以注意到我们没有指定副本计数，默认值为 1。服务定义如下所示。
- en: Listing 13.26 Sensor gateway service definition
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.26 传感器网关服务定义
- en: '[PRE26]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Cluster-internal load balancing
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 集群内部负载均衡
- en: Now we expose a service that does load balancing. If we start further pods,
    the traffic will be load balanced between them. A `ClusterIP` service is load
    balanced, but it is not exposed outside the cluster.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们公开了一个进行负载均衡的服务。如果我们启动更多的 pod，流量将在它们之间进行负载均衡。`ClusterIP` 服务进行负载均衡，但它不会暴露在集群外部。
- en: The heat API deployment is very similar to the deployments we’ve already done,
    except that there is configuration to pass through environment variables. The
    following listing shows the interesting portion of the descriptor in the `spec.template.spec
    .containers` section.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 热 API 部署与我们已经完成的部署非常相似，除了有配置要传递环境变量。以下列表显示了 `spec.template.spec.containers`
    部分描述符中的有趣部分。
- en: Listing 13.27 Heat API deployment excerpt
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.27 热 API 部署摘录
- en: '[PRE27]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ Define environment variables.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义环境变量。
- en: ❷ Override the LOW_TEMP environment value.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 覆盖 LOW_TEMP 环境值。
- en: ❸ Get a value from a ConfigMap resource.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从 ConfigMap 资源中获取值。
- en: Environment variables can be either passed directly by value, as for `LOW_TEMP`,
    or passed through the indirection of a `ConfigMap` resource, as in the following
    listing.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 环境变量可以直接通过值传递，例如 `LOW_TEMP`，或者通过 `ConfigMap` 资源的中介传递，如下所示。
- en: Listing 13.28 Configuration map example
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.28 配置映射示例
- en: '[PRE28]'
  id: totrans-252
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: ❶ Name of the ConfigMap resource
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ `ConfigMap` 资源名称
- en: ❷ Value for the gateway_hostname key
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `gateway_hostname` 键的值
- en: 'By passing environment variables through a `ConfigMap`, we can change configuration
    without having to update the heat API deployment descriptor. Note the value of
    `gateway _hostname`: this is the name used to resolve the service with DNS inside
    the Kubernetes cluster. Here `default` is the Kubernetes namespace, `svc` designates
    a service resource, and `cluster.local` resolves to the `cluster.local` domain
    name (remember that we are using a local development cluster).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 通过通过 `ConfigMap` 传递环境变量，我们可以更改配置，而无需更新 heat API 部署描述符。注意 `gateway _hostname`
    的值：这是在 Kubernetes 集群内部使用 DNS 解析服务时使用的名称。在这里 `default` 是 Kubernetes 命名空间，`svc`
    指定一个服务资源，而 `cluster.local` 解析为 `cluster.local` 域名（记住我们正在使用本地开发集群）。
- en: Finally, the following listing shows how to expose the heat sensor API as an
    externally load-balanced service.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下列表显示了如何将热传感器API作为外部负载均衡服务暴露出来。
- en: Listing 13.29 Heat API service definition
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.29 热API服务定义
- en: '[PRE29]'
  id: totrans-258
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Load balance externally.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 外部负载均衡。
- en: A `LoadBalancer` service is exposed outside the cluster. It can also be mapped
    to a host name using an *Ingress*, but this is not something that we will cover.[1](#pgfId-1035764)
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: '`LoadBalancer`服务在集群外部暴露。它也可以通过*Ingress*映射到主机名，但这不是我们将要涵盖的内容。[1](#pgfId-1035764)'
- en: We have now covered deploying the services to Kubernetes, so you may think that
    we are done. Sure, the services work great in Kubernetes as-is, but we can make
    the integration even better!
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经涵盖了将服务部署到Kubernetes的过程，所以你可能认为我们已经完成了。当然，服务在Kubernetes中运行得很好，但我们可以通过做两件事来使集成更好！
- en: 13.3 First-class Kubernetes citizens
  id: totrans-262
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 Kubernetes的一等公民
- en: 'As you have seen, the services that we deployed work fine in Kubernetes. That
    being said, we can make them first-class Kubernetes citizens by doing two things:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，我们部署的服务在Kubernetes中运行良好。但话虽如此，我们可以通过做两件事来使它们成为Kubernetes的一等公民：
- en: Exposing health and readiness checks
  id: totrans-264
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露健康和就绪性检查
- en: Exposing metrics
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 暴露指标
- en: This is important to ensure that a cluster knows how services behave, so that
    it can restart services or scale them up and down.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 这很重要，以确保集群了解服务的行为，以便它可以重启服务或进行扩展和缩减。
- en: 13.3.1 Health checks
  id: totrans-267
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.1 健康检查
- en: When Kubernetes starts a pod, it assumes that it can serve requests on the exposed
    ports, and that the application is running fine as long as the process is running.
    If a process crashes, Kubernetes will restart its pod. Also, if a process consumes
    too much memory, Kubernetes will kill it and restart its pod.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 当Kubernetes启动一个Pod时，它假设它可以在公开的端口上处理请求，并且只要进程在运行，应用程序就运行良好。如果进程崩溃，Kubernetes会重启其Pod。此外，如果进程消耗了过多的内存，Kubernetes会将其杀死并重启其Pod。
- en: 'We can do better by having a process *inform* Kubernetes about how it is doing.
    There are two important concepts in health checking:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过让进程*通知* Kubernetes其状态来做得更好。健康检查中有两个重要概念：
- en: '*Liveness checks* allow a service to report if it is working correctly, or
    if it is failing and needs to be restarted.'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*活跃性检查* 允许一个服务报告它是否正在正确工作，或者它是否失败并需要重启。'
- en: '*Readiness checks* allow a service to report that it is ready to accept traffic.'
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*就绪性检查* 允许一个服务报告它已准备好接受流量。'
- en: Liveness checks are important because a process may be working, yet be stuck
    with a fatal error, or be stuck in, say, an infinite loop. Liveness probes can
    be based on files, TCP ports, and HTTP endpoints. When probes fail beyond a threshold,
    Kubernetes restarts the pod.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 活跃性检查很重要，因为一个进程可能正在运行，但可能卡在致命错误中，或者卡在，比如说，一个无限循环中。活跃性探测可以基于文件、TCP端口和HTTP端点。当探测失败超过阈值时，Kubernetes会重启Pod。
- en: The heat sensor service and sensor gateway can provide simple health-check reporting
    using HTTP. As long as the HTTP endpoint is responding, that means the service
    is operating. The following listing shows how to add health-check capability to
    these services.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 热传感器服务和传感器网关可以使用HTTP提供简单的健康检查报告。只要HTTP端点在响应，就意味着服务正在运行。以下列表显示了如何向这些服务添加健康检查功能。
- en: Listing 13.30 Simple HTTP health-check probe
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.30 简单HTTP健康检查探测
- en: '[PRE30]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Add a route for a health check.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为健康检查添加路由。
- en: ❷ JSON payload to say the service is up
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ JSON有效负载表示服务正在运行
- en: ❸ Vert.x web handler for health checks
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于健康检查的Vert.x Web处理器
- en: 'With HTTP probes, Kubernetes is interested in the HTTP status code of the response:
    200 means the check succeeded, and anything else means that there is a problem.
    It is a loose convention to return a JSON document with a `status` field and the
    value `UP` or `DOWN`. Additional data can be in the document, such as messages
    from various checks being done. This data is mostly useful when logged for diagnosis
    purposes.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用HTTP探测，Kubernetes对响应的HTTP状态码感兴趣：200表示检查成功，其他任何东西都表示存在问题。返回包含`status`字段和值`UP`或`DOWN`的JSON文档是一种松散的约定。文档中还可以包含其他数据，例如来自各种检查的消息。这些数据在用于诊断目的时最有用。
- en: We then have to let Kubernetes know about the probe, as in the following listing.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们必须让Kubernetes了解探测，如下列所示。
- en: Listing 13.31 Heat sensor service liveness probe
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.31 热传感器服务活跃性探测
- en: '[PRE31]'
  id: totrans-282
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Define a liveness probe.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义活跃性探测。
- en: ❷ Specify the HTTP endpoint.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 指定HTTP端点。
- en: ❸ Initial delay before doing checks
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在进行检查之前的初始延迟
- en: ❹ Interval between checks
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 检查之间的间隔
- en: ❺ Check timeout
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 检查超时
- en: Here the liveness checks start after 15 seconds, happen every 15 seconds, and
    time out after 5 seconds. We can check this by looking at the logs of one of the
    heat sensor service pods.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，活性和就绪性检查在 15 秒后开始，每 15 秒发生一次，并在 5 秒后超时。我们可以通过查看热传感器服务 Pod 的日志来检查这一点。
- en: Listing 13.32 Health checks in logs
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.32 日志中的健康检查
- en: '[PRE32]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ The pod name will be different on your machine.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Pod 名称在您的机器上可能会有所不同。
- en: To get a pod name and check the logs, you can look at the output of `kubectl
    logs`. Here we see that the checks indeed happen every 15 seconds.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取 Pod 名称并检查日志，您可以查看 `kubectl logs` 的输出。在这里我们看到检查确实每 15 秒发生一次。
- en: The case of the heat API is more interesting, as we can define both liveness
    and readiness checks. The API needs the sensor gateway, so its readiness depends
    on that of the gateway. First, we have to define two routes for liveness and readiness
    checks, as shown in the next listing.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 热API的情况更有趣，因为我们可以定义活性和就绪性检查。API需要传感器网关，因此它的就绪性取决于网关的就绪性。首先，我们必须为活性和就绪性检查定义两个路由，如下一个列表所示。
- en: Listing 13.33 Health-check routes of the heat API service
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.33 热API服务的健康检查路由
- en: '[PRE33]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Readiness check
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 就绪性检查
- en: ❷ Liveness check
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 活性检查
- en: 'The implementation of the `livenessCheck` method is identical to that of listing
    13.30: if the service responds, it is alive. There is no condition under which
    the service would respond yet be in a state where a restart would be required.
    The service can, however, be unable to accept traffic because the sensor gateway
    is not available, which will be reported by the following readiness check.'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '`livenessCheck` 方法的实现与列表 13.30 相同：如果服务响应，则表示它处于活动状态。没有条件会导致服务响应但需要重启。然而，如果传感器网关不可用，服务可能无法接受流量，这将在以下就绪性检查中报告。'
- en: Listing 13.34 Readiness check of the heat API service
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.34 热API服务的就绪性检查
- en: '[PRE34]'
  id: totrans-300
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Make a request to the sensor gateway.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 向传感器网关发送请求。
- en: ❷ Send a 200 status.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 发送 200 状态。
- en: ❸ Report a failure.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 报告一个失败。
- en: ❹ Give the error message in the report.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在报告中给出错误信息。
- en: To perform a readiness check, we make a request to the sensor gateway health-check
    endpoint. We could actually make any other request that allows us to know if the
    service is available. We then respond to the readiness check with either an HTTP
    200 or 503.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 要执行就绪性检查，我们向传感器网关的健康检查端点发送请求。我们实际上可以发送任何其他请求，以便我们知道服务是否可用。然后我们用 HTTP 200 或 503
    响应就绪性检查。
- en: The configuration in the deployment resource is shown in the following listing.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 部署资源中的配置如下所示。
- en: Listing 13.35 Configuring health checks for the heat API service
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.35 配置热API服务的健康检查
- en: '[PRE35]'
  id: totrans-308
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Define a readiness probe.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义一个就绪性探针。
- en: As you can see, a readiness probe is configured very much like a liveness probe.
    We have defined `initialDelaySeconds` to be five seconds; this is because the
    initial Hazelcast discovery takes a few seconds, so the sensor gateway hasn’t
    deployed its verticle before this has been completed.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，就绪性探针的配置与活性和就绪性探针非常相似。我们已定义 `initialDelaySeconds` 为五秒；这是因为初始 Hazelcast
    发现需要几秒钟，所以传感器网关在此完成之前还没有部署其垂直结构。
- en: We can check the effect by taking down all instances of the sensor gateway,
    as shown next.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过关闭所有传感器网关的实例来检查效果，如下所示。
- en: Listing 13.36 Scaling down the sensor gateway to 0 replicas
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.36 将传感器网关缩放到 0 个副本
- en: '[PRE36]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Scale to 0.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 缩放到 0。
- en: ❷ List all pods in the default namespace.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 列出默认命名空间中的所有 Pod。
- en: 'You should wait a few seconds before listing the pods, and observe that the
    heat API pod becomes marked as `0/1` ready. This is because the readiness checks
    have failed, so the pod will not receive traffic anymore. You can try running
    the following query and see an immediate error:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 在列出 Pod 之前，您应该等待几秒钟，并观察热API Pod被标记为 `0/1` 就绪。这是因为就绪性检查失败了，所以 Pod 将不再接收流量。您可以尝试运行以下查询并立即看到错误：
- en: '[PRE37]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Now if we scale back to one instance, we’ll get back to a working state, as
    shown in the following listing.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们缩放到一个实例，我们将恢复到工作状态，如下所示。
- en: Listing 13.37 Scaling up the sensor gateway to one replica
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.37 将传感器网关扩展到 1 个副本
- en: '[PRE38]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Scale up to one instance.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 扩展到 1 个实例。
- en: You can now make successful HTTP requests again.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以再次成功发送 HTTP 请求。
- en: note The action you perform in a health or readiness check depends on what your
    service does. As a general rule, you should perform an action that has no side
    effect in the system. For instance, if your service needs to report a failed health
    check when a database connection is down, a safe action should be to perform a
    small SQL query. By contrast, doing a data insertion SQL query has side effects,
    and this is probably not how you want to check whether the database connection
    is working.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: note 执行健康或就绪性检查时采取的操作取决于您的服务做什么。作为一个一般规则，您应该执行一个没有副作用系统的操作。例如，如果您的服务需要在数据库连接断开时报告失败的健康检查，一个安全的操作是执行一个小型的
    SQL 查询。相比之下，执行数据插入 SQL 查询有副作用，这可能不是您检查数据库连接是否工作的方式。
- en: 13.3.2 Metrics
  id: totrans-324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.3.2 指标
- en: Vert.x can be configured to report metrics on various items like event-bus communications,
    network communications, and more. Monitoring metrics is important, because they
    can be used to check how a service is doing and to trigger alerts. For instance,
    you can have an alert that causes Kubernetes to scale up a service when the throughput
    or latency of a given URL endpoint is above a threshold.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: Vert.x 可以配置为报告各种项目的指标，如事件总线通信、网络通信等。监控指标很重要，因为它们可以用来检查服务的表现并触发警报。例如，您可以为给定 URL
    端点的吞吐量或延迟超过阈值时导致 Kubernetes 扩展服务的警报。
- en: I will show you how to expose metrics from Vert.x, but the other topics, like
    visualization, alerting, and auto-scaling are vastly complex and are outside the
    scope of this book.
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 我将向您展示如何从 Vert.x 中暴露指标，但其他主题，如可视化、警报和自动扩展，非常复杂，并且超出了本书的范围。
- en: Vert.x exposes metrics over popular technologies such as JMX, Dropwizard, Jolokia,
    and Micrometer.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: Vert.x 通过 JMX、Dropwizard、Jolokia 和 Micrometer 等流行技术暴露指标。
- en: 'We will be using Micrometer and Prometheus. Micrometer ([https://micrometer
    .io/](https://micrometer.io/)) is interesting because it is an abstraction over
    metric-reporting backends such as InfluxDB and Prometheus. Prometheus is a metrics
    and alerting project that is popular in the Kubernetes ecosystem ([https://prometheus.io/](https://prometheus.io/)).
    It also works in *pull* mode: Prometheus is configured to periodically collect
    metrics from services, so your services are not impacted by Prometheus being unavailable.'
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 Micrometer 和 Prometheus。Micrometer ([https://micrometer.io/](https://micrometer.io/))
    很有趣，因为它是一个在指标报告后端（如 InfluxDB 和 Prometheus）之上的抽象。Prometheus 是一个在 Kubernetes 生态系统中流行的指标和警报项目
    ([https://prometheus.io/](https://prometheus.io/))。它还以 *拉取* 模式工作：Prometheus 被配置为定期从服务中收集指标，因此您的服务不会受到
    Prometheus 不可用的影响。
- en: We will be adding metrics to the sensor gateway as it receives both event-bus
    and HTTP traffic; it is the most solicited service of the use case. To do that,
    we first have to add two dependencies as follows.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 当传感器网关接收事件总线和 HTTP 流量时，我们将添加指标；它是用例中最受请求的服务。为此，我们首先必须添加以下两个依赖项。
- en: Listing 13.38 Adding metrics support
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.38 添加指标支持
- en: '[PRE39]'
  id: totrans-331
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Vert.x Micrometer support
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Vert.x Micrometer 支持
- en: ❷ Micrometer support for Prometheus
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Prometheus 的 Micrometer 支持
- en: The sensor gateway needs clustering and metrics when starting Vert.x from the
    `main` method. We need to enable metrics as follows.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 当从 `main` 方法启动 Vert.x 时，传感器网关需要集群和指标。我们需要按照以下方式启用指标。
- en: Listing 13.39 Enabling Micrometer/Prometheus metrics
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.39 启用 Micrometer/Prometheus 指标
- en: '[PRE40]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: ❶ Event-bus configuration, just like before
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 事件总线配置，就像之前一样
- en: ❷ Enable Micrometer metrics with Prometheus.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用 Prometheus 启用 Micrometer 指标。
- en: ❸ Also publish metric quantiles.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 还发布指标分位数。
- en: We now have to define an HTTP endpoint for metrics to be available. The Vert.x
    Micrometer module offers a Vert.x web handler to make it easy, as shown in the
    following listing.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在必须定义一个 HTTP 端点，以便指标可用。Vert.x Micrometer 模块提供了一个 Vert.x 网络处理程序，使其变得简单，如下面的列表所示。
- en: Listing 13.40 Exposing a metrics endpoint over HTTP
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.40 通过 HTTP 暴露指标端点
- en: '[PRE41]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Expose at the /metrics path.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 /metrics 路径上暴露。
- en: ❷ Log requests.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 记录请求。
- en: ❸ Premade handler
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 预制处理程序
- en: It is a good idea to intercept metric requests and log them. This is useful
    when configuring Prometheus to check if it is collecting any metrics.
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 截获指标请求并记录它们是一个好主意。这在配置 Prometheus 以检查它是否正在收集任何指标时很有用。
- en: You can test the output using port forwarding.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用端口转发来测试输出。
- en: Listing 13.41 Testing metric reports
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.41 测试指标报告
- en: '[PRE42]'
  id: totrans-349
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: ❶ Port forwarding in one terminal
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在一个终端中进行端口转发
- en: ❷ Check the metrics output.
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 检查指标输出。
- en: Prometheus metrics are exposed in a simple text format. As you can see when
    running the preceding commands, by default lots of interesting metrics are reported,
    like response times, open connections, and more. You can also define your own
    metrics using the Vert.x Micrometer module APIs and expose them just like the
    default ones.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: Prometheus指标以简单的文本格式公开。正如你在运行前面的命令时可以看到的，默认情况下会报告许多有趣的指标，如响应时间、打开的连接等。你还可以使用Vert.x
    Micrometer模块API定义自己的指标，并像默认指标一样公开它们。
- en: You will find instructions and Kubernetes descriptors for configuring the Prometheus
    operator to consume metrics from the sensor gateway in the chapter13/ k8s-metrics
    folder of the book’s Git repository. You will also find a pointer to make a dashboard
    with Grafana that looks like the one in figure 13.4.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在本书的Git仓库的13/ k8s-metrics文件夹中找到配置Prometheus操作员以从传感器网关消耗指标的说明和Kubernetes描述符。你还可以找到一个指向使用Grafana创建类似图13.4的仪表板的指针。
- en: '![](../Images/CH13_F04_Ponge.png)'
  id: totrans-354
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH13_F04_Ponge.png)'
- en: Figure 13.4 Metrics dashboard using Grafana
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.4 使用Grafana的指标仪表板
- en: Grafana is a popular dashboard tool that can consume data from many sources,
    including Prometheus databases ([https://grafana.com/](https://grafana.com/)).
    All you need is to connect visualizations and queries. Fortunately, dashboards
    can be shared as JSON documents. Check the pointers in the Git repository if you
    want to reproduce the dashboard in figure 13.4\.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: Grafana是一个流行的仪表板工具，可以从许多来源消费数据，包括Prometheus数据库([https://grafana.com/](https://grafana.com/))。你所需要做的就是连接可视化和查询。幸运的是，仪表板可以作为JSON文档共享。如果你想重现图13.4中的仪表板，请检查Git仓库中的指针。
- en: 13.4 The end of the beginning
  id: totrans-357
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 开始的结束
- en: All good things come to an end, and this chapter concludes our journey toward
    reactive applications with Vert.x. We started this book with the fundamentals
    of asynchronous programming and Vert.x. Asynchronous programming is key to building
    scalable services, but it comes with challenges, and you saw how Vert.x helped
    in making this programming style simple and enjoyable. In the second part of this
    book we used a realistic application scenario to study the key Vert.x modules
    for databases, web, security, messaging, and event streaming. This allowed us
    to build an end-to-end reactive application made up of several microservices.
    By the end of the book, you saw a methodology based on a combination of load and
    chaos testing to ensure service resilience and responsiveness. This is important,
    as reactive is not just about scalability, but is also about writing services
    that can cope with failures. We concluded with notes on deploying Vert.x services
    in a Kubernetes cluster, something Vert.x is a natural fit for.
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 所有美好的事物都有结束的时候，这一章标志着我们使用Vert.x迈向反应式应用的旅程的结束。我们这本书从异步编程和Vert.x的基础开始。异步编程是构建可扩展服务的关键，但它也带来了挑战，你看到了Vert.x如何帮助使这种编程风格变得简单和愉快。本书的第二部分，我们使用一个现实的应用场景来研究Vert.x的关键模块，包括数据库、Web、安全、消息传递和事件流。这使得我们能够构建一个由多个微服务组成的端到端反应式应用。到本书结束时，你看到了一个基于负载和混沌测试组合的方法，以确保服务的弹性和响应性。这很重要，因为反应式不仅仅是关于可扩展性，还关于编写能够处理失败的服务。我们以在Kubernetes集群中部署Vert.x服务的注意事项结束，这是Vert.x非常适合的事情。
- en: Of course, we did not cover all that’s in Vert.x, but you will easily find your
    way through the project’s website and documentation. The Vert.x community is welcoming,
    and you can get in touch over mailing lists and chat. Last but not least, most
    of the skills that you have learned by reading this book translate to technologies
    other than Vert.x. Reactive is not a technology that you pick off the shelf. A
    technology like Vert.x will only get you half of the way to reactive; there is
    a craft and mindset required in systems design to achieve solid scalability, fault
    resiliency, and ultimately responsiveness.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们没有涵盖Vert.x中的所有内容，但你可以轻松地在项目的网站和文档中找到你的路径。Vert.x社区很欢迎，你可以通过邮件列表和聊天进行联系。最后但同样重要的是，你通过阅读这本书学到的许多技能可以应用到Vert.x以外的技术中。反应式技术不是你可以从货架上拿来的。像Vert.x这样的技术只能让你达到反应式的一半；在系统设计中，需要一种工艺和心态来实现稳定的可扩展性、容错性和最终的反应性。
- en: On a more personal note, I hope that you enjoyed reading this book as much as
    I enjoyed the experience of writing it. I’m looking forward to hearing from you
    in online discussions, and if we happen to attend an event together, I will be
    more than happy to meet you in person!
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在更个人化的方面，我希望您阅读这本书的经历和我写作这本书的经历一样愉快。我期待在在线讨论中收到您的反馈，如果我们有幸一起参加活动，我将非常高兴亲自见到您！
- en: Have fun, and take care!
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 玩得开心，保重！
- en: Summary
  id: totrans-362
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Vert.x applications can easily be deployed to Kubernetes clusters with no need
    for Kubernetes-specific modules.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vert.x应用程序可以轻松部署到Kubernetes集群，无需Kubernetes特定的模块。
- en: The Vert.x distributed event bus works in Kubernetes by configuring the cluster
    manager discovery mode.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Vert.x分布式事件总线通过配置集群管理器发现模式在Kubernetes中工作。
- en: It is possible to have a fast, local Kubernetes development experience using
    tools like Minikube, Skaffold, and Jib.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Minikube、Skaffold和Jib等工具，可以拥有快速、本地的Kubernetes开发体验。
- en: Exposing health checks and metrics is a good practice for operating services
    in a cluster.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在集群中操作服务时，公开健康检查和指标是一种良好的实践。
- en: '* * *'
  id: totrans-367
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.For more on Ingresses and other Kubernetes topics, see Marko Lukša’s *Kubernetes
    in Action*, second edition (Manning, 2020).
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 1.有关Ingress和其他Kubernetes主题的更多信息，请参阅Marko Lukša的《Kubernetes实战》第二版（Manning，2020）。
