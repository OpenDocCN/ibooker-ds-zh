- en: 4 Balancing the gathering and use of information
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 平衡信息的收集和使用
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn about the challenges of learning from evaluative feedback and
    how to properly balance the gathering and utilization of information.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解从评估反馈中学习的挑战以及如何正确平衡信息的收集和利用。
- en: You will develop exploration strategies that accumulate low levels of regret
    in problems with unknown transition function and reward signals.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将开发探索策略，在具有未知转移函数和奖励信号的问题中积累低水平的遗憾。
- en: You will write code with trial-and-error learning agents that learn to optimize
    their behavior through their own experiences in many-options, one-choice environments
    known as multi-armed bandits.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将使用试错学习代理编写代码，这些代理通过在多选项、单选择环境（称为多臂老虎机）中的自身经验来学习优化其行为。
- en: Uncertainty and expectation are the joys of life. Security is an insipid thing.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 不确定性和期望是生活的乐趣。安全感是一种乏味的东西。
- en: — William Congreve English playwright and poet of the Restoration period and
    political figure in the British Whig Party
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: —— 威廉·康格里夫，英国复辟时期的剧作家、诗人，以及英国辉格党政治人物
- en: No matter how small and unimportant a decision may seem, every decision you
    make is a trade-off between information gathering and information exploitation.
    For example, when you go to your favorite restaurant, should you order your favorite
    dish, yet again, or should you request that dish you’ve been meaning to try? If
    a Silicon Valley startup offers you a job, should you make a career move, or should
    you stay put in your current role?
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 无论一个决策看起来多么小或不重要，你做出的每一个决策都是在信息收集和信息利用之间的权衡。例如，当你去你最喜欢的餐厅时，你应该再次点你最喜欢的菜，还是应该要求尝试你一直想尝试的菜？如果一个硅谷初创公司给你提供一份工作，你应该做出职业变动，还是应该留在你当前的角色中？
- en: 'These kinds of questions illustrate the exploration-exploitation dilemma and
    are at the core of the reinforcement learning problem. It boils down to deciding
    when to acquire knowledge and when to capitalize on knowledge previously learned.
    It’s a challenge to know whether the good we already have is good enough. When
    do we settle? When do we go for more? What are your thoughts: is a bird in the
    hand worth two in the bush or not?'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题说明了探索-利用困境，并且是强化学习问题的核心。这归结为决定何时获取知识以及何时利用已学到的知识。知道我们已有的好处是否足够好是一个挑战。我们何时满足？何时追求更多？你的想法是什么：手头的一只鸟是否比树上的两只鸟更有价值？
- en: The main issue is that rewarding moments in life are relative; you have to compare
    events to see a clear picture of their value. For example, I’ll bet you felt amazed
    when you were offered your first job. You perhaps even thought that was the best
    thing that ever happened to you. But, then life continues, and you experience
    things that appear even more rewarding—maybe, when you get a promotion, a raise,
    or get married, who knows!
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 主要问题是生活中的奖励时刻是相对的；你必须比较事件才能清楚地了解它们的价值。例如，我敢打赌，当你被提供第一份工作时，你可能会感到惊讶。你甚至可能认为那是你一生中最美好的事情。但是，然后生活继续，你经历的事情看起来甚至更有回报——也许，当你得到晋升、加薪或结婚时，谁知道呢！
- en: 'And that’s the core issue: even if you rank moments you have experienced so
    far by “how amazing” they felt, you can’t know what’s the most amazing moment
    you could experience in your life—life is uncertain; you don’t have life’s transition
    function and reward signal, so you must keep on exploring. In this chapter, you
    learn about how important it is for your agent to explore when interacting with
    uncertain environments, problems in which the MDP isn’t available for planning.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是：即使你根据“多么令人惊叹”的感觉对迄今为止经历的时刻进行排名，你也不知道你一生中可能经历的最令人惊叹的时刻是什么——生活是不确定的；你没有生活的转移函数和奖励信号，所以你必须继续探索。在本章中，你将了解你的代理在与不确定环境互动时探索的重要性，这些问题中MDP不可用于规划。
- en: 'In the previous chapter, you learned about the challenges of learning from
    sequential feedback and how to properly balance immediate and long-term goals.
    In this chapter, we examine the challenges of learning from evaluative feedback,
    and we do so in environments that aren’t sequential, but one-shot instead: *multi-armed
    bandits (MABs)*).'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了从顺序反馈中学习的挑战以及如何正确平衡短期和长期目标。在本章中，我们检查从评估反馈中学习的挑战，我们这样做是在非顺序环境中，而是单次环境：*多臂老虎机（MABs）*）。
- en: 'MABs isolate and expose the challenges of learning from evaluative feedback.
    We’ll dive into many different techniques for balancing exploration and exploitation
    in these particular type of environments: single-state environments with multiple
    options, but a single choice. Agents will operate under uncertainty, that is,
    they won’t have access to the MDP. However, they will interact with one-shot environments
    without the sequential component.'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: MABs隔离并暴露了从评估性反馈中学习的挑战。我们将深入研究在特定类型环境中平衡探索和利用的许多不同技术：具有多个选项的单状态环境，但只有一个选择。智能体将在不确定性下操作，也就是说，它们将无法访问MDP。然而，它们将与没有序列组件的一次性环境进行交互。
- en: Remember, in DRL, agents learn from feedback that’s simultaneously *evaluative*
    feedback in isolation. Let’s get to it.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在深度强化学习（DRL）中，智能体从同时是*评估性*反馈的反馈中学习。让我们开始吧。
- en: The challenge of interpreting evaluative feedback
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解释评估性反馈的挑战
- en: In the last chapter, when we solved the FL environment, we knew beforehand how
    the environment would react to any of our actions. Knowing the exact transition
    function and reward signal of an environment allows us to compute an optimal policy
    using planning algorithms, such as PI and VI, without having to interact with
    the environment at all.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，当我们解决了FL环境时，我们事先就知道环境会对我们的任何行动如何反应。知道环境的精确转移函数和奖励信号使我们能够使用规划算法（如PI和VI）来计算最优策略，而无需与环境进行任何交互。
- en: But, knowing an MDP in advance oversimplifies things, perhaps unrealistically.
    We cannot always assume we’ll know with precision how an environment will react
    to our actions—that’s not how the world works. We could opt for learning such
    things, as you’ll learn in later chapters, but the bottom line is that we need
    to let our agents interact and experience the environment by themselves, learning
    this way to behave optimally, solely from their own experience. This is what’s
    called trial-and-error learning.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，事先知道一个马尔可夫决策过程（MDP）可能会过分简化问题，可能是不切实际的。我们并不能总是精确地知道环境会对我们的行动如何反应——世界并不是这样运作的。我们可以选择学习这些事情，正如你将在后面的章节中学到的，但关键是我们需要让我们的智能体自己通过与环境的交互和体验来学习，这样它们才能仅从自己的经验中学习到最优的行为。这就是所谓的试错学习。
- en: 'In RL, when the agent learns to behave from interaction with the environment,
    the environment asks the agent the same question over and over: what do you want
    to do now? This question presents a fundamental challenge to a decision-making
    agent. What action should it do now? Should the agent exploit its current knowledge
    and select the action with the highest current estimate? Or should it explore
    actions that it hasn’t tried enough? But many additional questions follow: when
    do you know your estimates are good enough? How do you know you have tried an
    apparently bad action enough? And so on.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在强化学习（RL）中，当智能体通过与环境交互来学习行为时，环境会不断地问智能体同样的问题：你现在想做什么？这个问题对一个决策智能体提出了一个基本挑战。它现在应该采取什么行动？智能体应该利用其当前的知识并选择当前估计最高的行动？还是应该探索它还没有尝试足够的行动？但是，还有许多其他问题：你什么时候知道你的估计足够好了？你怎么知道你已经尝试了一个明显不好的行动足够多了？等等。
- en: '![](../Images/04_01.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04_01.png)'
- en: You will learn more effective ways for dealing with the exploration-exploitation
    trade-off
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你将学习到处理探索-利用权衡的更有效的方法
- en: 'This is the key intuition: exploration builds the knowledge that allows for
    effective exploitation, and maximum exploitation is the ultimate goal of any decision
    maker.'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 这是关键直觉：探索构建了允许有效利用的知识，而最大化的利用是任何决策者的最终目标。
- en: 'Bandits: Single-state decision problems'
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 老虎机：单状态决策问题
- en: '*multi-armed bandits* (MAB) are a special case of an RL problem in which the
    size of the state space and horizon equal one. A MAB has multiple actions, a single
    state, and a greedy horizon; you can also think of it as a “many-options, single-choice”
    environment. The name comes from slot machines (bandits) with multiple arms to
    choose from (more realistically, multiple slot machines to choose from).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: '*多臂老虎机*（MAB）是强化学习问题的一个特殊情况，其中状态空间的大小和时间步长等于一。MAB有多个动作，一个状态和一个贪婪的时间步长；你也可以将其视为一个“多选项，单选择”的环境。这个名字来源于可以选择多个臂（老虎机）的投币机（更现实的是，可以选择多个投币机）。 '
- en: '![](../Images/04_02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04_02.png)'
- en: Multi-armed bandit problem
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 多臂老虎机问题
- en: 'There are many commercial applications for the methods coming out of MAB research.
    Advertising companies need to find the right way to balance showing you an ad
    they predict you’re likely to click on and showing you a new ad with the potential
    of it being an even better fit for you. Websites that raise money, such as charities
    or political campaigns, need to balance between showing the layout that has led
    to the most contributions and new designs that haven’t been sufficiently utilized
    but still have potential for even better outcomes. Likewise, e-commerce websites
    need to balance recommending you best-seller products as well as promising new
    products. In medical trials, there’s a need to learn the effects of medicines
    in patients as quickly as possible. Many other problems benefit from the study
    of the exploration-exploitation trade-off: oil drilling, game playing, and search
    engines, to name a few. Our reason for studying MABs isn’t so much a direct application
    to the real world, but instead how to integrate a suitable method for balancing
    exploration and exploitation in RL agents.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: MAB研究得出的方法在商业应用中有很多。广告公司需要找到一种正确的方式，在预测你可能点击的广告和展示有潜力成为更适合你的新广告之间取得平衡。为慈善机构或政治运动筹集资金的网站需要在展示导致最多贡献的布局和尚未充分利用但仍有更好结果潜力的新设计之间取得平衡。同样，电子商务网站需要在推荐畅销产品以及推荐有潜力的新产品之间取得平衡。在临床试验中，需要尽快了解药物对患者的效果。许多其他问题都受益于探索-利用权衡的研究：石油钻探、游戏和搜索引擎，仅举几例。我们研究MAB的原因并不是将其直接应用于现实世界，而是如何将一种适合平衡探索和利用的方法集成到RL代理中。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathMulti-armed bandit |'
  id: totrans-26
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学多臂老虎机 |'
- en: '|  | ![](../Images/04_02__Sidebar01.png) |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/04_02__Sidebar01.png) |'
- en: 'Regret: The cost of exploration'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 悔过：探索的成本
- en: The goal of MABs is very similar to that of RL. In RL, the agent needs to maximize
    the expected cumulative discounted reward (maximize the expected return). This
    means to get as much reward (maximize) through the course of an episode (cumulative)
    as soon as possible (if discounted—later rewards are discounted more) despite
    the environment’s stochasticity (expected). This makes sense when the environment
    has multiple states and the agent interacts with it for multiple time steps per
    episode. But in MABs, while there are multiple episodes, we only have a single
    chance of selecting an action in each episode.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: MAB的目标与RL的目标非常相似。在RL中，代理需要最大化期望的累积折现奖励（最大化期望回报）。这意味着在环境具有多个状态且代理在每个集中与它进行多次交互的情况下，尽快（如果折现——后期奖励折现更多）通过整个集获得尽可能多的奖励（最大化）。这在环境具有多个状态且代理在每个集中与它进行多次交互的情况下是有意义的。但在MAB中，尽管有多个集，我们每个集只有一个选择动作的机会。
- en: 'Therefore, we can exclude the words that don’t apply to the MAB case from the
    RL goal: we remove “cumulative” because there’s only a single time step per episode,
    and “discounted” because there are no next states to account for. This means,
    in MABs, the goal is for the agent to maximize the expected reward. Notice that
    the word “expected” stays there because there’s stochasticity in the environment.
    In fact, that’s what MAB agents need to learn: the underlying probability distribution
    of the reward signal.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们可以从RL目标中排除不适用于MAB案例的词语：我们移除了“累积”，因为在每个集中只有一个时间步，以及“折现”，因为没有下一个状态需要考虑。这意味着，在MAB中，目标是让代理最大化期望奖励。请注意，“期望”这个词仍然存在，因为环境中存在随机性。实际上，这正是MAB代理需要学习的东西：奖励信号的潜在概率分布。
- en: However, if we leave the goal to “maximize the expected reward,” it wouldn’t
    be straightforward to compare agents. For instance, let’s say an agent learns
    to maximize the expected reward by selecting random actions in all but the final
    episode, while a much more sample-efficient agent uses a clever strategy to determine
    the optimal action quickly. If we only compare the final-episode performance of
    these agents, which isn’t uncommon to see in RL, these two agents would have equally
    good performance, which is obviously not what we want.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，如果我们把目标定为“最大化期望奖励”，那么比较代理就不再简单。例如，假设一个代理通过在除了最后一集之外的所有集中选择随机动作来学习最大化期望奖励，而一个样本效率更高的代理则使用一种巧妙策略快速确定最佳动作。如果我们只比较这些代理在最后一集的表现，这在强化学习中并不少见，这两个代理将会有同样好的表现，这显然不是我们想要的。
- en: A robust way to capture a more complete goal is for the agent to maximize the
    per-episode expected reward while still minimizing the total expected reward loss
    of rewards across all episodes. To calculate this value, called *total regret*,
    we sum the per-episode difference of the true expected reward of the optimal action
    and the true expected reward of the selected action. Obviously, the lower the
    total regret, the better. Notice I use the word true here; to calculate the regret,
    you must have access to the MDP. That doesn’t mean your agent needs the MDP, only
    that you need it to compare agents’ exploration strategy efficiency.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 捕获更完整目标的一个稳健方法是，让智能体在最大化每集期望奖励的同时，仍然最小化所有集奖励的总期望损失。为了计算这个值，称为*总遗憾*，我们将每集真实期望奖励与所选动作的真实期望奖励之间的差异相加。显然，总遗憾越低，越好。注意我在这里使用“真实”这个词；为了计算遗憾，你必须能够访问MDP。这并不意味着你的智能体需要MDP，只是你需要它来比较智能体的探索策略效率。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathTotal regret equation |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学公式总遗憾方程 |'
- en: '|  | ![](../Images/04_02_Sidebar02.png) |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/04_02_Sidebar02.png) |'
- en: Approaches to solving MAB environments
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 解决MAB环境的策略
- en: There are three major kinds of approaches to tackling MABs. The most popular
    and straightforward approach involves exploring by injecting randomness in our
    action-selection process; that is, our agent will exploit most of the time, and
    sometimes it’ll explore using randomness. This family of approaches is called
    *random exploration strategies*. A basic example of this family would be a strategy
    that selects the greedy action most of the time, and with an epsilon threshold,
    it chooses uniformly at random. Now, multiple questions arise from this strategy;
    for instance, should we keep this epsilon value constant throughout the episodes?
    Should we maximize exploration early on? Should we periodically increase the epsilon
    value to ensure the agent always explores?
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 解决多臂老虎机（MAB）问题主要有三种方法。最流行且最直接的方法是在我们的动作选择过程中注入随机性来进行探索；也就是说，我们的智能体大部分时间会利用，有时会使用随机性进行探索。这一系列方法被称为*随机探索策略*。这个家族的一个基本例子是，大部分时间选择贪婪动作的策略，并且有一个epsilon阈值，随机均匀选择。现在，从这个策略中产生了多个问题；例如，我们应该在整个集数中保持这个epsilon值不变吗？我们应该在早期最大化探索吗？我们应该定期增加epsilon值以确保智能体始终进行探索？
- en: Another approach to dealing with the exploration-exploitation dilemma is to
    be optimistic. Yep, your mom was right. The family of *Optimistic exploration
    strategies* is a more systematic approach that quantifies the uncertainty in the
    decision-making problem and increases the preference for states with the highest
    uncertainty. The bottom line is that being optimistic will naturally drive you
    toward uncertain states because you’ll assume that states you haven’t experienced
    yet are the best they can be. This assumption will help you explore, and as you
    explore and come face to face with reality, your estimates will get lower and
    lower as they approach their true values.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理探索-利用困境的方法是保持乐观。是的，你妈妈是对的。*乐观探索策略*这一系列策略是一种更系统的方法，它量化决策问题中的不确定性，并增加对具有最高不确定性的状态的偏好。底线是，保持乐观会自然地将你引向不确定状态，因为你将假设你尚未经历的状态是最好的。这个假设将帮助你进行探索，随着你探索并直面现实，你的估计将随着它们接近真实值而越来越低。
- en: The third approach to dealing with the exploration-exploitation dilemma is the
    family of *information state-space exploration strategies*. These strategies will
    model the information state of the agent as part of the environment. Encoding
    the uncertainty as part of the state space means that an environment state will
    be seen differently when unexplored or explored. Encoding the uncertainty as part
    of the environment is a sound approach but can also considerably increase the
    size of the state space and, therefore, its complexity.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 处理探索-利用困境的第三种方法是*信息状态空间探索策略*这一系列策略。这些策略将智能体的信息状态建模为环境的一部分。将不确定性作为状态空间的一部分意味着，当未探索或已探索时，环境状态将被以不同的方式看待。将不确定性作为环境的一部分是一种合理的方法，但也会显著增加状态空间的大小及其复杂性。
- en: In this chapter, we’ll explore a few instances of the first two approaches.
    We’ll do this in a handful of different MAB environments with different properties,
    pros and cons, and this will allow us to compare the strategies in depth.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一章中，我们将探讨前两种方法的一些实例。我们将在具有不同属性、优点和缺点的一小批不同的MAB环境中这样做，这将使我们能够深入比较策略。
- en: It’s important to notice that the estimation of the Q-function in MAB environments
    is pretty straightforward and something all strategies will have in common. Because
    MABs are one-step environments, to estimate the Q-function we need to calculate
    the per-action average reward. In other words, the estimate of an action *a* is
    equal to the total reward obtained when selecting action *a*, divided by the number
    of times action *a* has been selected.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在多臂老虎机（MAB）环境中对Q函数的估计相当直接，并且所有策略都会具有共同点。因为MAB是单步环境，为了估计Q函数，我们需要计算每个动作的平均奖励。换句话说，动作*a*的估计等于选择动作*a*时获得的总奖励除以动作*a*被选择的次数。
- en: It’s essential to highlight that there are no differences in how the strategies
    we evaluate in this chapter estimate the Q-function; the only difference is in
    how each strategy uses the Q-function estimates to select actions.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 必须强调的是，我们在这个章节中评估的策略在估计Q函数方面没有差异；唯一的区别在于每个策略如何使用Q函数估计来选择动作。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe slippery bandit
    walk (SBW) environment is back! |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体例子滑动老虎机步（SBW）环境又回来了！|'
- en: '|  | The first MAB environment that we’ll consider is one we have played with
    before: the bandit slippery walk (BSW).![](../Images/04_04_Sidebar03a.png)The
    bandit slippery walk environmentRemember, BSW is a grid world with a single row,
    thus, a walk. But a special feature of this walk is that the agent starts at the
    middle, and any action sends the agent to a terminal state immediately. Because
    it is a one-time-step, it’s a bandit environment.BSW is a two-armed bandit, and
    it can appear to the agent as a two-armed Bernoulli bandit. Bernoulli bandits
    pay a reward of +1 with a probability *p* and a reward of 0 with probability *q
    = 1 – p*. In other words, the reward signal is a Bernoulli distribution.In the
    BSW, the two terminal states pay either 0 or +1\. If you do the math, you’ll notice
    that the probability of a +1 reward when selecting action 0 is 0.2, and when selecting
    action 1 is 0.8\. But your agent doesn’t know this, and we won’t share that info.
    The question we’re trying to ask is this: how quickly can your agent figure out
    the optimal action? How much total regret will agents accumulate while learning
    to maximize expected rewards? Let’s find out.![](../Images/04_04_Sidebar03b.png)Bandit
    slippery walk graph |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|  | 我们将要考虑的第一个MAB环境是我们之前玩过的：老虎机滑动步（BSW）。![](../Images/04_04_Sidebar03a.png)老虎机滑动步环境记住，BSW是一个单行的网格世界，因此是一个步。但这个步的特殊之处在于，智能体从中间开始，任何动作都会立即将智能体送入终端状态。因为它是一次性步，所以它是一个老虎机环境。BSW是一个双臂老虎机，它对智能体来说可能看起来像是一个双臂伯努利老虎机。伯努利老虎机以概率*p*支付+1的奖励，以概率*q
    = 1 – p*支付0的奖励。换句话说，奖励信号是一个伯努利分布。在BSW中，两个终端状态支付0或+1。如果你做数学计算，你会注意到选择动作0时获得+1奖励的概率是0.2，选择动作1时是0.8。但你的智能体不知道这一点，我们也不会分享这个信息。我们试图问的问题是：你的智能体需要多快才能找出最佳动作？在学会最大化期望奖励的过程中，智能体会积累多少总遗憾？让我们来看看！![](../Images/04_04_Sidebar03b.png)老虎机滑动步图
    |'
- en: 'Greedy: Always exploit'
  id: totrans-44
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 贪婪：总是利用
- en: The first strategy I want you to consider isn’t really a strategy but a baseline,
    instead. I already mentioned we need to have some exploration in our algorithms;
    otherwise, we risk convergence to a suboptimal action. But, for the sake of comparison,
    let’s consider an algorithm with no exploration at all.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我想要你考虑的第一个策略实际上不是一个策略，而是一个基线。我已经提到我们需要在我们的算法中有些探索；否则，我们可能会收敛到一个次优动作。但是，为了比较，让我们考虑一个完全没有探索的算法。
- en: This baseline is called a *greedy strategy*, or *pure exploitation strategy*.
    The greedy action-selection approach consists of always selecting the action with
    the highest estimated value. While there’s a chance for the first action we choose
    to be the best overall action, the likelihood of this lucky coincidence decreases
    as the number of available actions increases.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基线被称为*贪婪策略*，或*纯利用策略*。贪婪的动作选择方法包括始终选择具有最高估计值的动作。虽然我们选择的第一动作可能是最佳整体动作，但随着可用的动作数量增加，这种幸运巧合的可能性会降低。
- en: '![](../Images/04_05.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04_05.png)'
- en: Pure exploitation in the BSW
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: BSW中的纯利用
- en: As you might have expected, the greedy strategy gets stuck with the first action
    immediately. If the Q-table is initialized to zero, and there are no negative
    rewards in the environment, the greedy strategy will always get stuck with the
    first action.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所料，贪婪策略会立即陷入第一个动作。如果Q表初始化为零，并且环境中没有负面奖励，贪婪策略将始终陷入第一个动作。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonPure exploitation strategy
    |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python纯利用策略'
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ① Almost all strategies have the same bookkeeping code for estimating Q-values.②
    We initialize the Q-function and the count array to all zeros.③ These other variables
    are for calculating statistics and not necessary.④ Here we enter the main loop
    and interact with the environment.⑤ Easy enough, we select the action that maximizes
    our estimated Q-values.⑥ Then, pass it to the environment and receive a new reward.⑦
    Finally, we update the counts and the Q-table.⑧ Then, we update the statistics
    and start a new episode. |
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: ① 几乎所有策略都有相同的记账代码来估计Q值。② 我们将Q函数和计数数组初始化为零。③ 这些其他变量用于计算统计数据，不是必要的。④ 我们进入主循环并与环境交互。⑤
    足够简单，我们选择最大化我们估计的Q值的动作。⑥ 然后，将其传递给环境并接收新的奖励。⑦ 最后，我们更新计数和Q表。⑧ 然后，我们更新统计数据并开始新的回合。|
- en: 'I want you to notice the relationship between a greedy strategy and time. If
    your agent only has one episode left, the best thing is to act greedily. If you
    know you only have one day to live, you’ll do things you enjoy the most. To some
    extent, this is what a greedy strategy does: it does the best it can do with your
    current view of life assuming limited time left.'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我想让你注意到贪婪策略和时间之间的关系。如果你的代理只剩下一回合，最好的做法是贪婪行动。如果你知道自己只剩一天可活，你会做你最享受的事情。在某种程度上，这就是贪婪策略所做的：它尽其所能，假设剩下的时间有限。
- en: And this is a reasonable thing to do when you have limited time left; however,
    if you don’t, then you appear to be shortsighted because you can’t trade-off immediate
    satisfaction or reward for gaining of information that would allow you better
    long-term results.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 当你剩下的时间有限时，这样做是合理的；然而，如果你不这样做，那么你看起来是短视的，因为你不能为了获得信息而牺牲即时的满足感或奖励，这些信息将允许你获得更好的长期结果。
- en: 'Random: Always explore'
  id: totrans-56
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 随机：始终探索
- en: 'Let’s also consider the opposite side of the spectrum: a strategy with exploration
    but no exploitation at all. This is another fundamental baseline that we can call
    a *random strategy* or a *pure exploration strategy*. This is simply an approach
    to action selection with no exploitation at all. The sole goal of the agent is
    to gain information.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们也考虑光谱的另一端：一种具有探索但没有利用的策略。这是另一个基本的基线，我们可以称之为*随机策略*或*纯探索策略*。这仅仅是一种没有任何利用的行动选择方法。代理的唯一目标是获取信息。
- en: Do you know people who, when starting a new project, spend a lot of time “researching”
    without jumping into the water? Me too! They can take weeks just reading papers.
    Remember, while exploration is essential, it must be balanced well to get maximum
    gains.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 你认识那些在开始一个新项目时，花很多时间“研究”而不跳入水中的吗？我也是！他们可以花几周时间阅读论文。记住，虽然探索是必要的，但它必须很好地平衡，以获得最大收益。
- en: 'A random strategy is obviously not a good strategy either and will also give
    you suboptimal results. Similar to exploiting all the time, you don’t want to
    explore all the time, either. We need algorithms that can do both exploration
    and exploitation: gaining and using information.'
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 随机策略显然也不是一个好的策略，也会给你带来次优的结果。与一直利用类似，你也不想一直探索。我们需要能够同时进行探索和利用的算法：获取并使用信息。
- en: I left a note in the code snippet, and I want to restate and expand on it. The
    pure exploration strategy I presented is one way to explore, that is, random exploration.
    But you can think of many other ways. Perhaps based on counts, that is, how many
    times you try one action versus the others, or maybe based on the variance of
    the reward obtained.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 我在代码片段中留下了一条注释，并想重新阐述并扩展它。我提出的纯探索策略是探索的一种方式，即随机探索。但你可以考虑许多其他方式。也许基于计数，即你尝试一个动作与尝试其他动作的次数相比，或者也许基于获得的奖励的方差。
- en: 'Let that sink in for a second: while there’s only a single way to exploit,
    there are multiple ways to explore. Exploiting is nothing but doing what you think
    is best; it’s pretty straightforward. You think A is best, and you do A. Exploring,
    on the other hand, is much more complex. It’s obvious you need to collect information,
    but how is a different question. You could try gathering information to support
    your current beliefs. You could gather information to attempt proving yourself
    wrong. You could explore based on confidence, or based on uncertainty. The list
    goes on.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们稍微思考一下：虽然只有一种利用的方法，但有多种探索的方法。利用只是做你认为最好的事情；这很简单。你认为 A 是最好的，你就做 A。另一方面，探索要复杂得多。显然，你需要收集信息，但如何收集信息是另一个问题。你可以尝试收集信息来支持你的当前信念。你可以收集信息来试图证明自己是错的。你可以基于自信或基于不确定性进行探索。这个列表可以继续下去。
- en: 'The bottom line is intuitive: exploitation is your goal, and exploration gives
    you information about obtaining your goal. You must gather information to reach
    your goals, that is clear. But, in addition to that, there are several ways to
    collect information, and that’s where the challenge lies.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 核心是直观的：利用是你的目标，探索为你提供实现目标的信息。你必须收集信息以达到你的目标，这是显而易见的。但是，除了这一点之外，还有几种收集信息的方法，这就是挑战所在。
- en: 'Epsilon-greedy: Almost always greedy and sometimes random'
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: epsilon-greedy：几乎总是贪婪，有时是随机的
- en: Let’s now combine the two baselines, pure exploitation and pure exploration,
    so that the agent can exploit, but also collect information to make informed decisions.
    The hybrid strategy consists of acting greedily most of the time and exploring
    randomly every so often.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们结合两种基线策略，即纯利用和纯探索，这样智能体就可以利用，同时也能收集信息以做出明智的决策。混合策略包括大多数时候贪婪行动，偶尔随机探索。
- en: This strategy, referred to as the *epsilon-greedy strategy*, works surprisingly
    well. If you select the action you think is best almost all the time, you’ll get
    solid results because you’re still selecting the action believed to be best, but
    you’re also selecting actions you haven’t tried sufficiently yet. This way, your
    action-value function has an opportunity to converge to its true value; this will,
    in turn, help you obtain more rewards in the long term.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略，被称为 *epsilon-greedy 策略*，效果出奇地好。如果您几乎每次都选择您认为最好的动作，您将获得稳定的结果，因为您仍在选择被认为最好的动作，但同时也选择了您尚未充分尝试的动作。这样，您的动作值函数就有机会收敛到其真实值；这反过来又可以帮助您在长期内获得更多奖励。
- en: '![](../Images/04_07.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/04_07.png)'
- en: Epsilon-greedy in the BSW
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: BSW 中的 epsilon-greedy
- en: '| ![](../Images/icons_Python.png) | I Speak PythonEpsilon-greedy strategy |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说 PythonEpsilon-greedy 策略 |'
- en: '|  |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-70
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '① Same as before: removed the boilerplate② The epsilon-greedy strategy is surprisingly
    effective for its simplicity. It consists of selecting an action randomly every
    so often. First thing is to draw a random number and compare to a hyperparameter
    “epsilon.”③ If the drawn number is greater than epsilon, we select the greedy
    action, the action with the highest estimated value.④ Otherwise, we explore by
    selecting an action randomly.⑤ Realize that this may very well yield the greedy
    action because we’re selecting an action randomly from all available actions,
    including the greedy action. You aren’t really exploring with epsilon probability,
    but a little less than that depending on the number of actions.⑥ Removed the estimation
    and stats code |'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: ① 与之前相同：删除了样板文本② epsilon-greedy 策略因其简单性而令人惊讶地有效。它包括偶尔随机选择一个动作。首先，需要抽取一个随机数并与超参数“epsilon”进行比较。③
    如果抽取的数字大于 epsilon，我们选择贪婪动作，即具有最高估计值的动作。④ 否则，我们通过随机选择一个动作来进行探索。⑤ 请注意，这可能会非常有效地产生贪婪动作，因为我们是从所有可用的动作中随机选择动作，包括贪婪动作。您并不是真的以
    epsilon 概率进行探索，但这个概率略低于动作的数量。⑥ 删除了估计和统计代码 |
- en: The epsilon-greedy strategy is a random exploration strategy because we use
    randomness to select the action. First, we use randomness to choose whether to
    exploit or explore, but also we use randomness to select an exploratory action.
    There are other random-exploration strategies, such as softmax (discussed later
    in this chapter), that don’t have that first random decision point.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: epsilon-greedy 策略是一种随机探索策略，因为我们使用随机性来选择动作。首先，我们使用随机性来决定是利用还是探索，但我们还使用随机性来选择一个探索性动作。还有其他随机探索策略，例如
    softmax（将在本章后面讨论），它们没有那个第一个随机决策点。
- en: I want you to notice that if epsilon is 0.5 and you have two actions, you can’t
    say your agent will explore 50% of the time, if by “explore” you mean selecting
    the non-greedy action. Notice that the “exploration step” in epsilon-greedy includes
    the greedy action. In reality, your agent will explore a bit less than the epsilon
    value depending on the number of actions.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 我想让您注意，如果ε值为0.5，并且您有两个动作，您不能说您的智能体将会有50%的时间进行探索，如果“探索”意味着选择非贪婪动作。请注意，ε-greedy中的“探索步骤”包括贪婪动作。实际上，您的智能体将探索的次数略少于ε值，这取决于动作的数量。
- en: 'Decaying epsilon-greedy: First maximize exploration, then exploitation'
  id: totrans-74
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 衰减ε-greedy：首先最大化探索，然后最大化利用
- en: 'Intuitively, early on when the agent hasn’t experienced the environment enough
    is when we’d like it to explore the most; while later, as it obtains better estimates
    of the value functions, we want the agent to exploit more and more. The mechanics
    are straightforward: start with a high epsilon less than or equal to one, and
    decay its value on every step. This strategy, called *decaying epsilon-greedy
    strategy*, can take many forms depending on how you change the value of epsilon.
    Here I’m showing you two ways.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 直观上，当智能体对环境的体验还不足时，我们希望它进行最多的探索；而后期，当它获得更好的价值函数估计时，我们希望智能体进行更多的利用。机制很简单：从大于或等于1的高ε值开始，并在每一步衰减其值。这种策略被称为*衰减ε-greedy策略*，其形式取决于您如何改变ε的值。这里我向您展示了两种方法。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonLinearly decaying epsilon-greedy
    strategy |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python线性衰减的ε-greedy策略|'
- en: '|  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Again, boilerplate is gone!② Linearly decaying epsilon-greedy consists of
    making epsilon decay linearly with the number of steps. We start by calculating
    the number of episodes we’d like to decay epsilon to the minimum value.③ Then,
    calculate the value of epsilon for the current episode.④ After that, every thing
    is the same as the epsilon-greedy strategy.⑤ Stats are removed here. |
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ① 再次，样板代码已删除！② 线性衰减的ε-greedy策略是通过使ε值随着步数的增加而线性衰减。我们首先计算我们希望将ε衰减到最小值的回合数。③ 然后，计算当前回合的ε值。④
    之后，一切与ε-greedy策略相同。⑤ 这里也移除了统计数据。|
- en: '| ![](../Images/icons_Python.png) | I Speak PythonExponentially decaying epsilon-greedy
    strategy |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python指数衰减的ε-greedy策略|'
- en: '|  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE3]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① FYI, not complete code② Here we calculate the exponentially decaying epsilons.
    Now, notice you can calculate all of these values at once, and only query an array
    of pre-computed values as you go through the loop.③ Everything else is the same
    as before.④ And stats are removed again, of course. |
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ① 供您参考，代码不完整② 在这里，我们计算指数衰减的ε值。现在，请注意，你可以一次性计算出所有这些值，然后在循环过程中只查询预计算的值数组。③ 其他一切与之前相同。④
    当然，统计数据也被移除了。|
- en: 'There are many other ways you can handle the decaying of epsilon: from a simple
    1/episode to dampened sine waves. There are even different implementations of
    the same linear and exponential techniques presented. The bottom line is that
    the agent should explore with a higher chance early and exploit with a higher
    chance later. Early on, there’s a high likelihood that value estimates are wrong.
    Still, as time passes and you acquire knowledge, the likelihood that your value
    estimates are close to the actual values increases, which is when you should explore
    less frequently so that you can exploit the knowledge acquired.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以处理ε衰减的许多其他方法：从简单的1/回合到阻尼正弦波。甚至有相同线性技术和指数技术的不同实现。底线是，智能体应该早期以更高的概率进行探索，并在后期以更高的概率进行利用。早期，价值估计出错的可能性很高。然而，随着时间的推移和知识的积累，您的价值估计接近实际值的可能性增加，这时您应该减少探索的频率，以便利用获得的知识。
- en: 'Optimistic initialization: Start off believing it’s a wonderful world'
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 乐观初始化：一开始就相信这是一个美好的世界
- en: Another interesting approach to dealing with the exploration-exploitation dilemma
    is to treat actions that you haven’t sufficiently explored as if they were the
    best possible actions—like you’re indeed in paradise. This class of strategies
    is known as *Optimism in the face of uncertainty*. The optimistic initialization
    strategy is an instance of this class.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种处理探索-利用困境的有趣方法是，将您尚未充分探索的动作视为最佳可能动作——就像您真的在天堂一样。这类策略被称为*面对不确定性的乐观主义*。乐观初始化策略是这个类别的一个实例。
- en: '![](../Images/04_08.png)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04_08.png)'
- en: Optimistic initialization in the BSW
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: BSW中的乐观初始化
- en: 'The mechanics of the optimistic initialization strategy are straightforward:
    we initialize the Q-function to a high value and act greedily using these estimates.
    Two points to clarify: First “a high value” is something we don’t have access
    to in RL, which we’ll address this later in this chapter; but for now, pretend
    we have that number in advance. Second, in addition to the Q-values, we need to
    initialize the *counts* to a value higher than one. If we don’t, the Q-function
    will change too quickly, and the effect of the strategy will be reduced.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 乐观初始化策略的机制很简单：我们将Q函数初始化为高值，并使用这些估计贪婪地行动。需要澄清两点：首先，“高值”在强化学习（RL）中是我们无法获取的，我们将在本章后面解决这个问题；但就目前而言，假设我们提前知道了这个数字。其次，除了Q值之外，我们还需要将*计数*初始化为一个大于一的值。如果我们不这样做，Q函数会变化得太快，策略的效果将会减弱。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonOptimistic initialization
    strategy |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python乐观初始化策略 |'
- en: '|  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE4]'
  id: totrans-92
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '① In this strategy, we start initializing the Q-values to an optimistic value.②
    We also initialize the counts that will serve as an uncertainty measure: the higher
    the more certain.③ Removed some code here④ After that, we always select the action
    with the highest estimated value, similar to the “pure exploitation” strategy.⑤
    Removed more code |'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ① 在这个策略中，我们首先将Q值初始化为一个乐观的值。② 我们还初始化了将作为不确定性度量：数值越高，越确定。③ 这里移除了一些代码④ 之后，我们总是选择估计值最高的动作，类似于“纯利用”策略。⑤
    移除了更多代码 |
- en: Interesting, right? Momma was right! Because the agent initially expects to
    obtain more reward than it actually can, it goes around exploring until it finds
    sources of reward. As it gains experience, the “naiveness” of the agent goes away,
    that is, the Q-values get lower and lower until they converge to their actual
    values.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 很有趣，对吧？妈妈是对的！因为智能体最初期望获得的奖励比实际能获得的更多，它会四处探索，直到找到奖励来源。随着它获得经验，“天真”的智能体逐渐消失，也就是说，Q值会越来越低，直到收敛到它们的实际值。
- en: Again, by initializing the Q-function to a high value, we encourage the exploration
    of unexplored actions. As the agent interacts with the environment, our estimates
    will start converging to lower, but more accurate, estimates, allowing the agent
    to find and converge to the action with the actual highest payoff.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，通过将Q函数初始化为高值，我们鼓励探索未探索的动作。随着智能体与环境的交互，我们的估计值将开始收敛到更低的、但更准确的估计值，使智能体能够找到并收敛到实际最高收益的动作。
- en: The bottom line is if you’re going to act greedily, at least be optimistic.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，如果你打算贪婪地行动，至少要保持乐观。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleTwo-armed Bernoulli
    bandit environment |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 一个具体例子两臂伯努利老虎机环境 |'
- en: '|  | Let’s compare specific instantiations of the strategies we have presented
    so far on a set of two-armed Bernoulli bandit environments.Two-armed Bernoulli
    bandit environments have a single non-terminal state and two actions. Action 0
    has an α chance of paying a +1 reward, and with 1–α, it will pay 0 rewards. Action
    1 has a β chance of paying a +1 reward, and with 1–β, it will pay 0 rewards.This
    is similar to the BSW to an extent. BSW has complimentary probabilities: action
    0 pays +1 with α probability, and action 1 pays +1 with 1–α chance. In this kind
    of bandit environment, these probabilities are independent; they can even be equal.Look
    at my depiction of the two-armed Bernoulli bandit MDP.![](../Images/04_08__Sidebar10.png)Two-armed
    Bernoulli bandit environmentsIt’s crucial you notice there are many different
    ways of representing this environment. And in fact, this isn’t how I have it written
    in code, because there’s much redundant and unnecessary information.Consider,
    for instance, the two terminal states. One could have the two actions transitioning
    to the same terminal state. But, you know, drawing that would make the graph too
    convoluted.The important lesson here is you’re free to build and represent environments
    your own way; there isn’t a single correct answer. There are definitely multiple
    incorrect ways, but there are also multiple correct ways. Make sure to *explore*!Yeah,
    I went there. |'
  id: totrans-98
  prefs: []
  type: TYPE_TB
  zh: '|  | 让我们比较我们在一系列双臂伯努利赌博机环境中提出的策略的具体实例。双臂伯努利赌博机环境有一个非终止状态和两个动作。动作0有α的概率支付+1奖励，而1-α的概率将支付0奖励。动作1有β的概率支付+1奖励，而1-β的概率将支付0奖励。这在一定程度上类似于BSW。BSW有互补的概率：动作0以α的概率支付+1，动作1以1-α的概率支付+1。在这种赌博机环境中，这些概率是独立的；它们甚至可以相等。看看我对双臂伯努利赌博机MDP的描述！![侧边栏10](../Images/04_08__Sidebar10.png)双臂伯努利赌博机环境重要的是要注意，有几种不同的方式来表示这个环境。实际上，这并不是我在代码中这样写的，因为这里有很多冗余和不必要的信息。例如，考虑两个终止状态。两个动作可以转换到同一个终止状态。但是，你知道，画出来会让图变得太复杂了。这里的重要教训是你可以自由地以自己的方式构建和表示环境；没有唯一的正确答案。当然有多个错误的方式，但也有多个正确的方式。确保要*探索*！是的，我提到了那里。
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpSimple exploration strategies
    in two-armed Bernoulli bandit environments |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| ![计分板](../Images/icons_Tally.png) | 计分板简单探索策略在双臂伯努利赌博机环境中'
- en: '|  | I ran two hyperparameter instantiations of all strategies presented so
    far: the epsilon-greedy, the two decaying, and the optimistic approach, along
    with the pure exploitation and exploration baselines on five two-armed Bernoulli
    bandit environments with probabilities α and β initialized uniformly at random,
    and five seeds. Results are means across 25 runs.![](../Images/04_08_Sidebar11.png)The
    best performing strategy in this experiment is the optimistic with 1.0 initial
    Q-values and 10 initial counts. All strategies perform pretty well, and these
    weren’t highly tuned, so it’s just for the fun of it and nothing else. Head to
    chapter 4’s Notebook and play, have fun. |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '|  | 我运行了所有至今为止提出的策略的两个超参数实例：ε-贪婪、两种衰减和乐观方法，以及纯利用和探索基线，在五个带有概率α和β的双臂伯努利赌博机环境中，这些概率随机初始化，以及五个种子。结果是在25次运行中的平均值！![侧边栏11](../Images/04_08_Sidebar11.png)在这个实验中表现最好的策略是具有1.0初始Q值和10个初始计数的乐观方法。所有策略都表现得很不错，这些策略并没有经过高度调整，所以这只是为了乐趣，没有其他原因。前往第4章的笔记本，玩一玩，享受乐趣。
    |'
- en: '| ![](../Images/icons_Details.png) | It''s In The DetailsSimple strategies
    in the two-armed Bernoulli bandit environments |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| ![细节](../Images/icons_Details.png) | 细节简单策略在双臂伯努利赌博机环境中'
- en: '|  | Let’s talk about several of the details in this experiment.First, I ran
    five different seeds (12, 34, 56, 78, 90) to generate five different two-armed
    Bernoulli bandit environments. Remember, all Bernoulli bandits pay a +1 reward
    with certain probability for each arm.The resulting environments and their probability
    of payoff look as follows:Two-armed bandit with seed 12:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 让我们讨论这个实验中的几个细节。首先，我运行了五个不同的种子（12、34、56、78、90）来生成五个不同的双臂伯努利赌博机环境。记住，所有伯努利赌博机以一定的概率为每个臂支付+1奖励。结果环境和它们的支付概率如下：带有种子12的双臂赌博机：'
- en: 'Probability of reward: [0.41630234, 0.5545003 ]'
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励概率：[0.41630234, 0.5545003 ]
- en: 'Two-armed bandit with seed 34:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 带有种子34的双臂赌博机：
- en: 'Probability of reward: [0.88039337, 0.56881791]'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励概率：[0.88039337, 0.56881791]
- en: 'Two-armed bandit with seed 56:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 带有种子56的双臂赌博机：
- en: 'Probability of reward: [0.44859284, 0.9499771 ]'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励概率：[0.44859284, 0.9499771 ]
- en: 'Two-armed bandit with seed 78:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 带有种子78的双臂赌博机：
- en: 'Probability of reward: [0.53235706, 0.84511988]'
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励概率：[0.53235706, 0.84511988]
- en: 'Two -armed bandit with seed 90:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 带有种子90的双臂投币机：
- en: 'Probability of reward: [0.56461729, 0.91744039]'
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 奖励概率：[0.56461729, 0.91744039]
- en: The mean optimal value across all seeds is 0.83.All of the strategies were run
    against each of the environments above with five different seeds (12, 34, 56,
    78, 90) to smooth and factor out the randomness of the results. For instance,
    I first used seed 12 to create a Bernoulli bandit, then I used seeds 12, 34, and
    so on, to get the performance of each strategy under the environment created with
    seed 12.Then, I used seed 34 to create another Bernoulli bandit and used 12, 34,
    and so on, to evaluate each strategy under the environment created with seed 34\.
    I did this for all strategies in all five environments. Overall, the results are
    the means over the five environments and five seeds, so 25 different runs per
    strategy.I tuned each strategy independently but also manually. I used approximately
    10 hyperparameter combinations and picked the top two from those. |
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 所有种子下的平均最优值是0.83。所有策略都在上述每个环境中针对五个不同的种子（12, 34, 56, 78, 90）运行，以平滑和消除结果中的随机性。例如，我首先使用种子12创建了一个伯努利投币机，然后使用种子12、34等，以获取在种子12创建的环境中每个策略的性能。然后，我使用种子34创建另一个伯努利投币机，并使用12、34等来评估在种子34创建的环境中每个策略的性能。我为所有策略在所有五个环境中都这样做。总体而言，结果是五个环境和五个种子上的平均值，因此每个策略有25次运行。我独立地调整了每个策略，但也进行了手动调整。我使用了大约10种超参数组合，并从那些组合中选择了前两个。|
- en: Strategic exploration
  id: totrans-113
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 战略性探索
- en: Alright, imagine you’re tasked with writing a reinforcement learning agent to
    learn driving a car. You decide to implement an epsilon-greedy exploration strategy.
    You flash your agent into the car’s computer, start the car, push that beautiful
    bright green button, and then your car starts exploring. It will flip a coin and
    decide to explore with a random action, say to drive on the other side of the
    road. Like it? Right, me neither. I hope this example helps to illustrate the
    need for different exploration strategies.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，想象一下你被分配了一个任务，那就是编写一个强化学习代理来学习驾驶汽车。你决定实现一个ε-贪婪探索策略。你将你的代理闪入汽车的电脑中，启动汽车，按下那个漂亮的明绿色按钮，然后你的汽车开始探索。它将掷硬币并决定随机采取一个动作，比如开到马路对面。你喜欢吗？对，我也不喜欢。我希望这个例子能帮助说明不同探索策略的必要性。
- en: Let me be clear that this example is, of course, an exaggeration. You wouldn’t
    put an untrained agent directly into the real world to learn. In reality, if you’re
    trying to use RL in a real car, drone, or in the real world in general, you’d
    first pre-train your agent in simulation, and/or use more sample-efficient methods.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我明确一点，这个例子当然是夸张的。你不会直接将未经训练的代理放入现实世界去学习。在现实中，如果你试图在真正的汽车、无人机或一般现实世界中使用RL，你首先会在模拟中预训练你的代理，或者使用更高效的采样方法。
- en: But, my point holds. If you think about it, while humans explore, we don’t explore
    randomly. Maybe infants do. But not adults. Maybe imprecision is the source of
    our randomness, but we don’t randomly marry someone just because (unless you go
    to Vegas.) Instead, I’d argue that adults have a more strategic way of exploring.
    We know that we’re sacrificing short- for long-term satisfaction. We know we want
    to acquire information. We explore by trying things we haven’t sufficiently tried
    but have the potential to better our lives. Perhaps, our exploration strategies
    are a combination of estimates and their uncertainty. For instance, we might prefer
    a dish that we’re likely to enjoy, and we haven’t tried, over a dish that we like
    okay, but we get every weekend. Perhaps we explore based on our “curiosity” or
    our prediction error. For instance, we might be more inclined to try new dishes
    at a restaurant that we thought would be okay-tasting food, but it resulted in
    the best food you ever had. That “prediction error” and that “surprise” could
    be our metric for exploration at times.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我的观点仍然成立。如果你这么想，虽然人类在探索，但我们并不是随机探索。也许婴儿是这样的。但不是成年人。也许不精确是我们随机性的来源，但我们不会仅仅因为（除非你去拉斯维加斯）就随机嫁给某人。相反，我认为成年人有更战略性的探索方式。我们知道我们在牺牲短期满足以换取长期满足。我们知道我们想要获取信息。我们通过尝试那些我们没有充分尝试但有可能改善我们生活的事情来探索。也许，我们的探索策略是估计及其不确定性的组合。例如，我们可能更喜欢那些我们可能喜欢但尚未尝试的菜肴，而不是那些我们每周都喜欢的菜肴。也许我们根据我们的“好奇心”或预测误差来探索。例如，我们可能更倾向于尝试在一家我们认为味道可能不错的餐厅的新菜肴，但结果却是你吃过的最好的食物。那种“预测误差”和那种“惊喜”有时可能成为我们探索的指标。
- en: In the rest of this chapter, we’ll look at slightly more advanced exploration
    strategies. Several are still random exploration strategies, but they apply this
    randomness in proportion to the current estimates of the actions. Other exploration
    strategies take into account the confidence and uncertainty levels of the estimates.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将探讨一些稍微更高级的探索策略。其中一些仍然是随机探索策略，但它们将这种随机性应用于动作的当前估计值。其他探索策略则考虑了估计的置信度和不确定性水平。
- en: All this being said, I want to reiterate that the epsilon-greedy strategy (and
    its decaying versions) is still the most popular exploration strategy in use today,
    perhaps because it performs well, perhaps because of its simplicity. Maybe it’s
    because most reinforcement learning environments today live inside a computer,
    and there are very few safety concerns with the virtual world. It’s important
    for you to think hard about this problem. Balancing the exploration versus exploitation
    trade-off, the gathering and utilization of information is central to human intelligence,
    artificial intelligence, and reinforcement learning. I’m certain the advancements
    in this area will have a big impact in the fields of artificial intelligence,
    reinforcement learning, and all other fields interested in this fundamental trade-off.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 话虽如此，我想重申，epsilon-greedy策略（及其衰减版本）仍然是今天使用最广泛的探索策略，这可能是因为它表现良好，也可能是因为它的简单性。也许是因为大多数今天的强化学习环境都生活在计算机中，虚拟世界中的安全问题很少。认真思考这个问题很重要。平衡探索与利用之间的权衡，收集和利用信息是人类智能、人工智能和强化学习的核心。我确信这一领域的进步将对人工智能、强化学习以及所有对这一基本权衡感兴趣的领域产生重大影响。
- en: 'Softmax: Select actions randomly in proportion to their estimates'
  id: totrans-119
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: Softmax：按估计值成比例随机选择动作
- en: 'Random exploration strategies make more sense if they take into account Q-value
    estimates. By doing so, if there is an action that has a really low estimate,
    we’re less likely to try it. There’s a strategy, called *softmax strategy*, that
    does this: it samples an action from a probability distribution over the action-value
    function such that the probability of selecting an action is proportional to its
    current action-value estimate. This strategy, which is also part of the family
    of random exploration strategies, is related to the epsilon-greedy strategy because
    of the injection of randomness in the exploration phase. Epsilon-greedy samples
    uniformly at random from the full set of actions available at a given state, while
    softmax samples based on preferences of higher valued actions.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 如果随机探索策略考虑了Q值估计，那么它们更有意义。通过这样做，如果有一个动作的估计值非常低，我们尝试它的可能性就会降低。有一种称为*softmax策略*的策略就是这样做的：它从动作值函数的概率分布中抽取动作，使得选择动作的概率与其当前的动作值估计成正比。这种策略也是随机探索策略家族的一部分，因为它在探索阶段注入了随机性。epsilon-greedy策略从给定状态可用的完整动作集中均匀随机抽取，而softmax策略则基于高值动作的偏好进行抽取。
- en: By using the softmax strategy, we’re effectively making the action-value estimates
    an indicator of preference. It doesn’t matter how high or low the values are;
    if you add a constant to all of them, the probability distribution will stay the
    same. You put preferences over the Q-function and sample an action from a probability
    distribution based on this preference. The difference between Q-value estimates
    will create a tendency to select actions with the highest estimates more often,
    and actions with the lowest estimates less frequently.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 通过使用softmax策略，我们实际上是将动作值估计作为偏好的指标。值的高低并不重要；如果你给所有值加上一个常数，概率分布将保持不变。你将偏好放在Q函数上，并从这个偏好中根据概率分布抽取一个动作。Q值估计之间的差异将产生一种倾向，即更频繁地选择估计值最高的动作，而较少选择估计值最低的动作。
- en: We can also add a hyperparameter to control the algorithm’s sensitivity to the
    differences in Q-value estimates. That hyperparameter, called the temperature
    (a reference to statistical mechanics), works in such a way that as it approaches
    infinity, the preferences over the Q-values are equal. Basically, we sample an
    action uniformly. But, as the temperature value approaches zero, the action with
    the highest estimated value will be sampled with probability of one. Also, we
    can decay this hyperparameter either linearly, exponentially, or another way.
    But, in practice, for numerical stability reasons, we can’t use infinity or zero
    as the temperature; instead, we use a very high or very low positive real number,
    and normalize these values.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以添加一个超参数来控制算法对Q值估计差异的敏感性。这个超参数被称为温度（参考统计力学），其工作方式是这样的：当它接近无穷大时，对Q值的偏好是相等的。基本上，我们均匀地采样一个动作。但是，当温度值接近零时，具有最高估计值的动作将以100%的概率被采样。此外，我们可以以线性、指数或其他方式衰减这个超参数。但在实践中，出于数值稳定性的原因，我们不能使用无穷大或零作为温度；相反，我们使用一个非常高或非常低的正实数，并归一化这些值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathSoftmax exploration strategy
    |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | Show Me The MathSoftmax探索策略 |'
- en: '|  | ![](../Images/04_08_Sidebar13.png) |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/04_08_Sidebar13.png) |'
- en: '| ![](../Images/icons_Python.png) | I Speak PythonSoftmax strategy |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说PythonSoftmax策略 |'
- en: '|  |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE5]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Code was removed here for simplicity.② First, we calculate the linearly decaying
    temperature the same way we did with the linearly decaying epsilon.③ I make sure
    min_temp isn’t 0, to avoid div by zero. Check the Notebook for details.④ Next
    we calculate the probabilities by applying the softmax function to the Q-values.⑤
    Normalize for numeric stability.⑥ Finally, we make sure we got good probabilities
    and select the action based on them.⑦ Code was removed here too. |
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为了简化，这里删除了代码。② 首先，我们以与线性衰减epsilon相同的方式计算线性衰减的温度。③ 我确保min_temp不是0，以避免除以零。详细信息请查看笔记本。④
    接下来，我们通过将softmax函数应用于Q值来计算概率。⑤ 归一化以保持数值稳定性。⑥ 最后，我们确保得到了良好的概率，并根据这些概率选择动作。⑦ 同样，这里也删除了代码。
    |
- en: 'UCB: It’s not about optimism, it’s about realistic optimism'
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: UCB：这不仅仅是乐观，而是现实的乐观
- en: In the last section, I introduced the optimistic initialization strategy. This
    is a clever (and perhaps philosophical) approach to dealing with the exploration
    versus exploitation trade-off, and it’s the simplest method in the optimism in
    the face of uncertainty family of strategies. But, there are two significant inconveniences
    with the specific algorithm we looked at. First, we don’t always know the maximum
    reward the agent can obtain from an environment. If you set the initial Q-value
    estimates of an optimistic strategy to a value much higher than its actual maximum
    value, unfortunately, the algorithm will perform sub-optimally because the agent
    will take many episodes (depending on the “counts” hyperparameter) to bring the
    estimates near the actual values. But even worse, if you set the initial Q-values
    to a value lower than the environment’s maximum, the algorithm will no longer
    be optimistic, and it will no longer work.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我介绍了乐观初始化策略。这是一种巧妙（也许可以说是哲学性的）处理探索与利用权衡的方法，并且是面对不确定性时乐观策略家族中最简单的方法。但是，我们之前考虑的特定算法有两个显著的不便之处。首先，我们并不总是知道智能体可以从环境中获得的最大奖励。如果你将乐观策略的初始Q值估计设置得远高于其实际最大值，不幸的是，算法将表现不佳，因为智能体需要很多个回合（取决于“计数”超参数）才能将估计值接近实际值。但更糟糕的是，如果你将初始Q值设置得低于环境的最大值，算法将不再乐观，并且将不再起作用。
- en: The second issue with this strategy as we presented it is that the “counts”
    variable is a hyperparameter and it needs tuning, but in reality, what we’re trying
    to represent with this variable is the uncertainty of the estimate, which shouldn’t
    be a hyperparameter. A better strategy, instead of believing everything is roses
    from the beginning and arbitrarily setting certainty measure values, follows the
    same principles as optimistic initialization while using statistical techniques
    to calculate the value estimates uncertainty and uses that as a bonus for exploration.
    This is what the *upper confidence bound* (UCB) strategy does.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 这种策略的第二个问题是我们所提出的，即“counts”变量是一个超参数，需要调整，但在现实中，我们试图用这个变量表示的是估计的不确定性，这不应该是一个超参数。一个更好的策略，而不是一开始就相信一切都会顺利，随意设置确定性度量值，遵循乐观初始化的相同原则，同时使用统计技术来计算值估计的不确定性，并将其作为探索的额外奖励。这正是*上置信界*（UCB）策略所做的事情。
- en: In UCB, we’re still optimistic, but it’s a more a realistic optimism; instead
    of blindly hoping for the best, we look at the uncertainty of value estimates.
    The more uncertain a Q-value estimate, the more critical it is to explore it.
    Note that it’s no longer about believing the value will be the “maximum possible,”
    though it might be! The new metric that we care about here is uncertainty; we
    want to give uncertainty the benefit of the doubt.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 在UCB中，我们仍然保持乐观，但这是一种更现实的乐观；而不是盲目地希望最好的结果，我们关注价值估计的不确定性。Q值估计的不确定性越大，探索它的必要性就越大。请注意，这不再是相信值将是“最大可能的”，尽管它可能是！我们在这里关心的新指标是不确定性；我们希望给不确定性带来好处。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathUpper confidence bound (UCB)
    equation |'
  id: totrans-133
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示我数学上置信界（UCB）方程 |'
- en: '|  | ![](../Images/04_08_Sidebar15.png) |'
  id: totrans-134
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏15.png](../Images/04_08_Sidebar15.png) |'
- en: To implement this strategy, we select the action with the highest sum of its
    Q-value estimate and an action-uncertainty bonus U. That is, we’re going to add
    a bonus, upper confidence bound *uU* bonus value to the Q-value estimates, because
    we are more confident of the Q-value estimates; they’re not as critical to explore.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 要实现这种策略，我们选择具有最高Q值估计和动作不确定性奖励U的动作。也就是说，我们将添加一个奖励，即上置信界*uU*奖励值，因为我们对Q值估计更有信心；它们不是那么需要探索。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonUpper confidence bound (UCB)
    strategy |'
  id: totrans-136
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python上置信界（UCB）策略 |'
- en: '|  |'
  id: totrans-137
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE6]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '① Code removed for brevity.② We first select all actions once to avoid division
    by zero.③ Then, proceed to calculating the confidence bounds.④ Last, we pick the
    action with the highest value with an uncertainty bonus: the more uncertain the
    value of the action, the higher the bonus.⑤ Stats code removed for brevity. |'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ① 为了简洁起见，删除了代码。② 我们首先选择所有动作一次，以避免除以零。③ 然后，继续计算置信界限。④ 最后，我们选择具有最高价值并带有不确定性奖励的动作：动作的价值越不确定，奖励就越高。⑤
    为了简洁起见，删除了统计代码。
- en: 'On a practical level, if you plot *u* as a function of the episodes and counts,
    you’ll notice it’s much like an exponentially decaying function with a few differences.
    Instead of the smooth decay exponential functions show, there’s a sharp decay
    early on and a long tail. This makes it so that early on when the episodes are
    low, there’s a higher bonus for smaller differences between actions, but as more
    episode pass, and counts increase, the difference in bonuses for uncertainty become
    smaller. In other words, a 0 versus 100 attempts should give a higher bonus to
    0 than to a 100 in a 100 versus 200 attempts. Finally, the *c* hyperparameter
    controls the scale of the bonus: a higher *c* means higher bonuses, lower *c*
    lower bonuses.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际层面上，如果你将*u*作为剧集和计数的函数绘制出来，你会注意到它与指数衰减函数非常相似，但有几点不同。与显示平滑衰减的指数函数不同，它一开始有急剧的衰减和长长的尾巴。这使得在剧集数量低的时候，动作之间的较小差异会有更高的奖励，但随着剧集的增加和计数的增加，不确定性奖励的差异会变得较小。换句话说，在100次尝试与200次尝试中，0次尝试应该比100次尝试得到更高的奖励。最后，*c*超参数控制奖励的规模：*c*值越高，奖励越高，*c*值越低，奖励越低。
- en: 'Thompson sampling: Balancing reward and risk'
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 汤普森抽样：平衡奖励和风险
- en: The UCB algorithm is a frequentist approach to dealing with the exploration
    versus exploitation trade-off because it makes minimal assumptions about the distributions
    underlying the Q-function. But other techniques, such as Bayesian strategies,
    can use priors to make reasonable assumptions and exploit this knowledge. The
    *thompson sampling* strategy is a sample-based probability matching strategy that
    allows us to use Bayesian techniques to balance the exploration and exploitation
    trade-off.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: UCB算法是一种频率派处理探索与利用权衡的方法，因为它对Q函数背后的分布做出了最小的假设。但其他技术，如贝叶斯策略，可以使用先验来做出合理的假设并利用这些知识。*汤普森采样*策略是一种基于样本的概率匹配策略，它允许我们使用贝叶斯技术来平衡探索与利用的权衡。
- en: A simple way to implement this strategy is to keep track of each Q-value as
    a Gaussian (a.k.a. normal) distribution. In reality, you can use any other kind
    of probability distribution as prior; beta distributions, for instance, are a
    common choice. In our case, the Gaussian mean is the Q-value estimate, and the
    Gaussian standard deviation measures the uncertainty of the estimate, which are
    updated on each episode.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 实现这种策略的一种简单方法是跟踪每个Q值作为高斯分布（即正态分布）。实际上，你可以使用任何其他类型的概率分布作为先验；例如，beta分布是一个常见的选择。在我们的情况下，高斯均值是Q值估计，高斯标准差衡量估计的不确定性，这些都会在每个剧集中进行更新。
- en: '![](../Images/04_09.png)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/04_09.png)'
- en: Comparing two action-value functions represented as Gaussian distributions
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 比较表示为高斯分布的两个动作值函数
- en: As the name suggests, in Thompson sampling, we sample from these normal distributions
    and pick the action that returns the highest sample. Then, to update the Gaussian
    distributions’ standard deviation, we use a formula similar to the UCB strategy
    in which, early on when the uncertainty is higher, the standard deviation is more
    significant; therefore, the Gaussian is broad. But as the episodes progress, and
    the means shift toward better and better estimates, the standard deviations gets
    lower, and the Gaussian distribution shrinks, and so its samples are more and
    more likely to be near the estimated mean.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如其名称所示，在汤普森采样中，我们从这些正态分布中进行采样，并选择返回最高样本值的动作。然后，为了更新高斯分布的标准差，我们使用一个类似于UCB策略的公式，在早期，当不确定性较高时，标准差更为重要；因此，高斯分布较宽。但随着剧集的进行，均值逐渐向更好的估计值移动，标准差降低，高斯分布缩小，因此其样本更有可能接近估计的均值。
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThompson sampling strategy
    |'
  id: totrans-147
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python汤普森采样策略 |'
- en: '|  |'
  id: totrans-148
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[PRE7]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Initialization code removed② In our implementation, we’ll sample numbers from
    the Gaussian distributions. Notice how the ‘scale’ which is the width of the Gaussian
    (the standard deviation) shrinks with the number of times we try each action.
    Also, notice how ‘alpha’ controls the initial width of the Gaussian, and ‘beta’
    the rate at which they shrink.③ Then, we select the action with the highest sample.④
    Stats code removed |
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化代码已移除② 在我们的实现中，我们将从高斯分布中采样数字。注意‘scale’（即高斯宽度，标准差）随着我们尝试每个动作的次数而缩小。同时，注意‘alpha’控制高斯初始宽度，而‘beta’控制它们缩小的速率。③
    然后，我们选择样本值最高的动作。④ 统计代码已移除 |
- en: 'In this particular implementation, I use two hyperparameters: alpha, to control
    the scale of the Gaussian, or how large the initial standard deviation will be,
    and beta, to shift the decay such that the standard deviation shrinks more slowly.
    In practice, these hyperparameters need little tuning for the examples in this
    chapter because, as you probably already know, a standard deviation of just five,
    for instance, is almost a flat-looking Gaussian representing over a ten-unit spread.
    Given our problems have rewards (and Q-values) between 0 and 1, and approximately
    between –3 and 3 (the example coming up next), we wouldn’t need any Gaussian with
    standard deviations too much greater than 1.'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个特定的实现中，我使用了两个超参数：alpha，用于控制高斯的比例，即初始标准差的大小，以及beta，用于调整衰减速率，使得标准差缩小得更慢。在实践中，这些超参数对于本章的示例需要很少的调整，因为，正如你可能已经知道的，标准差仅为五，例如，几乎是一个看起来平坦的高斯分布，代表着超过十单位的范围。鉴于我们的问题具有介于0和1之间的奖励（和Q值），以及大约介于-3和3之间（下一个示例），我们不需要标准差太大的高斯分布。
- en: Finally, I want to reemphasize that using Gaussian distributions is perhaps
    not the most common approach to Thompson sampling. Beta distributions seem to
    be the favorites here. I prefer Gaussian for these problems because of their symmetry
    around the mean, and because their simplicity makes them suitable for teaching
    purposes. However, I encourage you to dig more into this topic and share what
    you find.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我想再次强调，使用高斯分布可能是Thompson抽样的最不常见的方法。Beta分布似乎是这里的首选。我之所以喜欢这些问题的Beta分布，是因为它们围绕均值的对称性，以及它们的简单性使它们适合教学目的。然而，我鼓励你更深入地研究这个主题，并分享你发现的内容。
- en: '| ![](../Images/icons_Tally.png) | Tally it UpAdvanced exploration strategies
    in two-armed Bernoulli bandit environments |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 总结高级探索策略在双臂伯努利伯努利环境中的表现 |'
- en: '|  | I ran two hyperparameter instantiations of each of the new strategies
    introduced: the softmax, the UCB, and the Thompson approach, along with the pure
    exploitation and exploration baselines, and the top-performing simple strategies
    from earlier on the same five two-armed Bernoulli bandit environments. This is
    again a total of 10 agents in five environments across five seeds. It’s a 25 runs
    total per strategy. The results are averages across these runs.![](../Images/04_09_Sidebar18.png)Besides
    the fact that the optimistic strategy uses domain knowledge that we cannot assume
    we’ll have, the results indicate that the more advanced approaches do better.
    |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|  | 我对每种新策略进行了两次超参数实例化：softmax、UCB和Thompson方法，以及纯利用和探索基线，以及之前在相同五个双臂伯努利伯努利环境中的表现最好的简单策略。这又是五个环境中五个种子下的总共10个代理。每个策略总共运行25次。这些结果是这些运行的平均值！[](../Images/04_09_Sidebar18.png)除了乐观策略使用了我们不能假设拥有的领域知识之外，结果还表明更先进的方法表现更好。
    |'
- en: '| ![](../Images/icons_Concrete.png) | A Concrete Example10-armed Gaussian bandit
    environments |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Concrete.png) | 具体示例10臂高斯伯努利环境 |'
- en: '|  | 10-armed Gaussian bandit environments still have a single non-terminal
    state; they’re bandit environments. As you probably can tell, they have ten arms
    or actions instead of two like their Bernoulli counterparts. But, the probability
    distributions and reward signals are different from the Bernoulli bandits. First,
    Bernoulli bandits have a probability of payoff of p, and with 1–p, the arm won’t
    pay anything. Gaussian bandits, on the other hand, will always pay something (unless
    they sample a 0—more on this next). Second, Bernoulli bandits have a binary reward
    signal: you either get a +1 or a 0\. Instead, Gaussian bandits pay every time
    by sampling a reward from a Gaussian distribution.![](../Images/04_09_Sidebar19a.png)10-armed
    Gaussian banditTo create a 10-armed Gaussian bandit environment, you first sample
    from a standard normal (Gaussian with mean 0 and variance 1) distribution 10 times
    to get the optimal action-value function **(a)*, and variance 1. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '|  | 10臂高斯伯努利环境仍然只有一个非终止状态；它们是伯努利环境。正如你可能已经猜到的，它们有十个臂或动作，而不是像它们的伯努利对应物那样有两个。但是，概率分布和奖励信号与伯努利伯努利不同。首先，伯努利伯努利有一个收益概率p，而1-p，臂不会支付任何东西。另一方面，高斯伯努利总是支付一些东西（除非它们采样一个0——关于这一点稍后讨论）。其次，伯努利伯努利有一个二进制奖励信号：你要么得到+1，要么得到0。相反，高斯伯努利每次都会支付，通过从高斯分布中采样奖励！[](../Images/04_09_Sidebar19a.png)10臂高斯伯努利伯努利为了创建一个10臂高斯伯努利环境，你首先从标准正态分布（均值为0，方差为1）中采样10次，以获得最佳动作值函数**（a）**，方差为1。
    |'
- en: '| ![](../Images/icons_Math.png) | Show Me The Math10-armed Gaussian bandit
    reward function |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式10臂高斯伯努利奖励函数 |'
- en: '|  | ![](../Images/04_09_Sidebar19b.png) |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/04_09_Sidebar19b.png) |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpAdvanced exploration strategies
    in 10-armed Gaussian bandit environments |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tally.png) | 总结高级探索策略在10臂高斯伯努利环境中的表现 |'
- en: '|  | I ran the same hyperparameter instantiations of the simple strategies
    introduced earlier, now on five 10-armed Gaussian bandit environments. This is
    obviously an “unfair” experiment because these techniques can perform well in
    this environment if properly tuned, but my goal is to show that the most advanced
    strategies still do well with the old hyperparameters, despite the change of the
    environment. You’ll see that in the next example.![](../Images/04_09_Sidebar21.png)Look
    at that, several of the most straightforward strategies have the lowest total
    regret and the highest expected reward across the five different scenarios. Think
    about that for a sec! |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '|  | 我运行了之前介绍过的简单策略的超参数实例化，现在是在五个10臂高斯赌博机环境中。这显然是一个“不公平”的实验，因为这些技术如果调整得当，在这个环境中可以表现得很好，但我的目标是展示，尽管环境发生了变化，最先进策略仍然可以用旧的超参数做得很好。你将在下一个例子中看到这一点！![图片](../Images/04_09_Sidebar21.png)看吧，几个最直接策略在五个不同场景中具有最低的总遗憾值和最高的预期奖励。想想看！
    |'
- en: '| ![](../Images/icons_Tally.png) | Tally it UpAdvanced exploration strategies
    in 10-armed Gaussian bandit environments |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Tally.png) | 总结：10臂高斯赌博机环境中的高级探索策略 |'
- en: '|  | I then ran the advanced strategies with the same hyperparameters as before.
    I also added the two baselines and the top two performing simple strategies in
    the 10-armed Gaussian bandit. As with all other experiments, this is a total of
    25 five runs.![](../Images/04_09_Sidebar20.png)This time only the advanced strategies
    make it on top, with a pretty decent total regret. What you should do now is head
    to the Notebook and have fun! Please, also share with the community your results,
    if you run additional experiments. I can’t wait to see how you extend these experiments.
    Enjoy! |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '|  | 我随后使用了与之前相同的超参数运行了高级策略。我还添加了两个基线以及10臂高斯赌博机中表现最好的两个简单策略。与其他所有实验一样，这总共是25次运行！![图片](../Images/04_09_Sidebar20.png)这次只有高级策略取得了领先，总体遗憾值相当不错。现在你应该去笔记本上享受乐趣！请记住，如果你运行了额外的实验，也请与社区分享你的结果。我迫不及待地想看看你如何扩展这些实验。享受吧！
    |'
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: 'Learning from evaluative feedback is a fundamental challenge that makes reinforcement
    learning unique. When learning from evaluative feedback, that is, +1, +1.345,
    +1.5, –100, –4, your agent doesn’t know the underlying MDP and therefore cannot
    determine what the maximum reward it can obtain is. Your agent “thinks”: “Well,
    I got a +1, but I don’t know, maybe there’s a +100 under this rock?” This uncertainty
    in the environment forces you to design agents that explore.'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 从评估反馈中学习是一个基本挑战，这使得强化学习与众不同。当从评估反馈中学习时，即+1、+1.345、+1.5、-100、-4，你的智能体不知道底层的MDP，因此无法确定它能获得的最大奖励是多少。你的智能体“想”：
    “好吧，我得到了+1，但我不确定，也许这块石头下面有一个+100？”这种环境中的不确定性迫使你设计出探索的智能体。
- en: 'But as you learned, you can’t take exploration lightly. Fundamentally, exploration
    wastes cycles that could otherwise be used for maximizing reward, for exploitation,
    yet, your agent can’t maximize reward, or at least pretend it can, without gathering
    information first, which is what exploration does. All of a sudden, your agent
    has to learn to balance exploration and exploitation; it has to learn to compromise,
    to find an equilibrium between two crucial yet competing sides. We’ve all faced
    this fundamental trade-off in our lives, so these issues should be intuitive to
    you: “A bird in the hand is worth two in the bush,” yet “A man’s reach should
    exceed his grasp.” Pick your poison, and have fun doing it, just don’t get stuck
    to either one. Balance them!'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 但正如你所学的，你不能轻视探索。从根本上讲，探索浪费了本可以用来最大化奖励的周期，用于利用，然而，你的智能体在收集信息之前无法最大化奖励，或者至少假装它能，这就是探索的作用。突然之间，你的智能体必须学会平衡探索和利用；它必须学会妥协，在两个至关重要的但相互竞争的方面之间找到平衡。我们都在生活中面临过这种基本的权衡，所以这些问题应该对你来说很直观：“手中有鸟胜于林中两鸟”，然而“人的抱负应超越其能力”。选择你的毒药，享受这个过程，只是不要陷入其中任何一个。平衡它们！
    |
- en: Knowing this fundamental trade-off, we introduced several different techniques
    to create agents, or strategies, for balancing exploration and exploitation. The
    epsilon-greedy strategy does it by exploiting most of the time and exploring only
    a fraction. This exploration step is done by sampling an action at random. Decaying
    epsilon-greedy strategies capture the fact that agents need more exploration at
    first because they need to gather information to start making a right decision,
    but they should quickly begin to exploit to ensure they don’t accumulate regret,
    which is a measure of how far from optimal we act. Decaying epsilon-greedy strategies
    decay epsilon as episodes increase and, hopefully, as our agent gathers information.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 了解这个基本权衡后，我们介绍了几种不同的技术来创建智能体或策略，以平衡探索和利用。epsilon-greedy策略通过大部分时间利用和一小部分时间探索来实现。这一探索步骤是通过随机采样一个动作来完成的。衰减epsilon-greedy策略捕捉到智能体最初需要更多探索的事实，因为它们需要收集信息以开始做出正确的决策，但它们应该迅速开始利用，以确保它们不会积累遗憾，遗憾是衡量我们行动距离最优程度的一个指标。衰减epsilon-greedy策略随着剧集的增加而衰减epsilon，希望随着我们的智能体收集信息而增加。
- en: 'But then we learned about other strategies that try to ensure that “hopefully”
    is more likely. These strategies take into account estimates and their uncertainty
    and potential and select accordingly: optimistic initialization, UCB, Thompson
    sampling, and although softmax doesn’t really use uncertainty measures, it explores
    by selecting randomly in the proportion of the estimates.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 但然后我们学习了其他策略，这些策略试图确保“希望”更有可能实现。这些策略考虑了估计及其不确定性、潜力和可能性，并据此进行选择：乐观初始化、UCB、Thompson抽样，尽管softmax实际上并不使用不确定性度量，但它通过在估计比例中随机选择来探索。
- en: By now, you
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand that the challenge of learning from evaluative feedback is because
    agents cannot see the underlying MDP governing their environments
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解从评估反馈中学习的挑战在于智能体无法看到控制其环境的底层MDP
- en: Learned that the exploration versus exploitation trade-off rises from this problem
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学到探索与利用之间的权衡源于这个问题
- en: Know about many strategies that are commonly used for dealing with this issue
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解许多常用的策略来处理这个问题
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| ![推文图标](../Images/icons_Tweet.png) | 在自己的工作中努力并分享你的发现 |'
- en: '|  | Here are several ideas on how to take what you’ve learned to the next
    level. If you’d like, share your results with the rest of the world and make sure
    to check out what others have done, too. It’s a win-win situation, and hopefully,
    you’ll take advantage of it.'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些想法，可以帮助你将所学知识提升到新的水平。如果你愿意，可以将你的结果分享给全世界，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch04_tf01:** There are many more techniques for solving bandit environments.
    Try exploring other resources out there and tell us techniques that are important.
    Research Bayesian approaches to action selection, and also, action-selection strategies
    that are based on information gain. What is information gain, again? Why is this
    important in the context of RL? Can you develop other interesting action-selection
    strategies, including decaying strategies that use information to decay the exploration
    rate of agents? For instance, imagine an agent the decays epsilon based on state
    visits—perhaps on another metric.'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch04_tf01:** 解决多臂老虎机环境的技巧还有很多。尝试探索其他资源，并告诉我们哪些技巧是重要的。研究基于贝叶斯的方法进行动作选择，以及基于信息增益的动作选择策略。什么是信息增益？为什么在强化学习（RL）的背景下这很重要？你能开发出其他有趣的动作选择策略，包括使用信息衰减探索率的衰减策略吗？例如，想象一个根据状态访问次数衰减epsilon的智能体——也许基于另一个指标。'
- en: '**#gdrl_ch04_tf02:** Can you think of a few other bandit environments that
    are interesting to examine? Clone my bandit repository ([https://github.com/mimoralea/gym-bandits](https://github.com/mimoralea/gym-bandits)
    -which is forked, too,) and add a few other bandit environments to it.'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch04_tf02:** 你能想到一些其他有趣的带臂老虎机环境来研究吗？克隆我的带臂老虎机仓库([https://github.com/mimoralea/gym-bandits](https://github.com/mimoralea/gym-bandits)
    -它也被分叉了，)并向其中添加一些其他带臂老虎机环境。'
- en: '**#gdrl_ch04_tf03:** After bandit environments, but before reinforcement learning
    algorithms, there’s another kind of environment called contextual bandit problems.
    What are these kinds of problems? Can you help us understand what these are? But,
    don’t just create a blog post about them. Also create a Gym environment with contextual
    bandits. Is that even possible? Create those environments in a Python package,
    and another Python package with algorithms that can solve contextual bandit environments.'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch04_tf03:** 在Bandit环境之后，但在强化学习算法之前，还有一种被称为上下文Bandit问题的环境。这些是什么类型的问题？你能帮助我们理解这些是什么吗？但是，不要只是写一篇关于它们的博客文章。还要创建一个带有上下文Bandit的Gym环境。这是否可能？将这些环境创建在一个Python包中，并创建另一个Python包，其中包含可以解决上下文Bandit环境的算法。'
- en: '**#gdrl_ch04_tf04:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch04_tf04:** 在每一章中，我都在使用最后的标签作为一个总标签。自由使用这个标签来讨论任何与你在这章中工作的相关内容。没有比你自己创建的作业更有趣的了。确保分享你设定要调查的内容和你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from this list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@提及我 @mimoralea（我会转发），并使用这个列表中的特定标签来帮助感兴趣的人找到你的结果。没有对错之分；你分享你的发现，并检查他人的发现。利用这个机会社交，做出贡献，让自己更受关注！我们正在等待你的加入！以下是一条推文示例：“嘿，@mimoralea。我创建了一篇博客文章，列出了学习深度强化学习的资源列表。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的作品。|
