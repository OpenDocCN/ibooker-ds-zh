- en: 9 Extracting value from large data sets with AI
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 9 使用AI从大型数据集中提取价值
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Using Amazon Comprehend for named entity recognition (NER)
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Amazon Comprehend进行命名实体识别（NER）
- en: Understanding Comprehend’s modes of operations (asynchronous, batch, and synchronous)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解Comprehend的操作模式（异步、批量、同步）
- en: Using asynchronous Comprehend services
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用异步Comprehend服务
- en: Triggering Lambda functions using S3 notifications
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用S3通知触发Lambda函数
- en: Handling errors in Lambdas using a dead-letter queue
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用死信队列处理Lambda中的错误
- en: Processing results from Comprehend
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 处理Comprehend的结果
- en: Chapter 8 dealt with the challenge of gathering unstructured data from websites
    for use in machine learning analysis. This chapter builds on the serverless web
    crawler from chapter 8\. This time, we are concerned with using machine learning
    to extract meaningful insights from the data we gathered. If you didn’t work through
    chapter 8, you should go back and do so now before proceeding with this chapter,
    as we will be building directly on top of the web crawler. If you are already
    comfortable with that content, we can dive right in and add the information extraction
    parts.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章处理了从网站收集非结构化数据用于机器学习分析的问题。本章在第8章的无服务器网页爬虫基础上进行构建。这次，我们关注的是使用机器学习从我们收集的数据中提取有意义的见解。如果你没有完成第8章的内容，你应该现在回去完成它，然后再继续本章，因为我们将在网页爬虫的基础上直接构建。如果你已经熟悉了这些内容，我们可以直接进入并添加信息提取部分。
- en: 9.1 Using AI to extract significant information from web pages
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.1 使用AI从网页中提取重要信息
- en: Let’s remind ourselves of the grand vision for our chapter 8 scenario--finding
    relevant developer conferences to attend. We want to facilitate a system that
    allows people to search for conferences and speakers of interest to them. In chapter
    8’s web crawler, we built a system that solved the first part of this scenario--gathering
    data on conferences.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下第8章场景的宏伟愿景——寻找要参加的相关开发者会议。我们希望建立一个系统，让人们能够搜索他们感兴趣的和会议演讲者。在第8章的网页爬虫中，我们构建了一个系统，解决了这个场景的第一部分——收集会议数据。
- en: However, we don’t want users to have to manually search through all of the unstructured
    website text we have gathered. Instead, we want to present them with conferences,
    the event locations and dates and a list of people who may be speaking at these
    conferences.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们不想让用户手动搜索我们收集的所有非结构化网站文本。相反，我们希望向他们展示会议、事件地点和日期以及可能在这些会议上发表演讲的人名单。
- en: Extracting this meaningful data from unstructured text is a non-trivial problem--at
    least, it was before the recent advancement of managed AI services.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化文本中提取这些有意义的资料是一个非同小可的问题——至少，在最近管理人工智能服务进步之前是这样的。
- en: Let’s revisit the requirements overview diagram for our scenario from chapter
    8\. This time, we are highlighting the relevant parts for this chapter.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再次回顾第8章场景的需求概述图。这次，我们突出显示本章的相关部分。
- en: '![](../Images/CH09_F01_Elger.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F01_Elger.png)'
- en: Figure 9.1 This chapter deals with extraction of event and speaker information
    from the data we already gathered.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.1 本章处理从我们已收集的数据中提取事件和演讲者信息。
- en: 9.1.1 Understanding the problem
  id: totrans-16
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.1 理解问题
- en: The challenge of extracting important information from unstructured text is
    known as *named entity recognition (NER)*. A *named entity* can be a person, location,
    or organization. It can also refer to dates and numerical quantities. NER is a
    challenging problem and the subject of much research. It is certainly not a completely
    solved problem. Since its results cannot be guaranteed to be 100% accurate, we
    must take this into consideration. Human result-checking may be required, depending
    on the application. For example, suppose you have a system that is required to
    detect locations in a body of text. Now suppose one of the sentences in the text
    mentions the Apollo 11 command module, “Columbia.” NER might identity this as
    a location instead part of a spacecraft! Every named entity recognition system
    will give a likelihood score for each recognition result, and this value never
    reaches 100%.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 从非结构化文本中提取重要信息的问题被称为 *命名实体识别 (NER)*。一个 *命名实体* 可以是人、地点或组织。它也可以指日期和数值。NER 是一个具有挑战性的问题，也是许多研究的话题。这绝对不是一个完全解决的问题。由于其结果不能保证达到
    100% 的准确性，我们必须考虑这一点。根据应用情况，可能需要人工结果检查。例如，假设你有一个需要检测文本中位置的系统。现在假设文本中的一句话提到了阿波罗
    11 号指令舱，“哥伦比亚”。NER 可能将其识别为地点而不是航天器的一部分！每个命名实体识别系统都会为每个识别结果提供一个可能性分数，而这个值永远不会达到
    100%。
- en: For our conference event information extraction scenario, we are aiming to extract
    the names of people, locations, and dates from website data. This will then be
    stored and made accessible to users.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的会议事件信息提取场景，我们的目标是从网站数据中提取人名、地点和日期。然后这些信息将被存储并供用户访问。
- en: 9.1.2 Extending the architecture
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1.2 扩展架构
- en: We are about to design and deploy a serverless system to extract the required
    information from conference web pages. Let’s take a look at the architecture components
    for this chapter, using the categories outlined in the canonical serverless architecture
    from chapter 1\. This is shown in figure 9.2.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将设计和部署一个无服务器系统，从会议网页中提取所需信息。让我们看看本章的架构组件，使用第 1 章中提到的标准无服务器架构中的类别。这如图 9.2
    所示。
- en: '![](../Images/CH09_F02_Elger.png)'
  id: totrans-21
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F02_Elger.png)'
- en: Figure 9.2 Serverless entity extraction system architecture. The system is composed
    of synchronous Lambda functions orchestrated using a step function. Data is stored
    in the item store S3 bucket introduced in chapter 8\. Bucket notifications from
    S3 trigger our asynchronous services.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 图 9.2 服务器端无实体提取系统架构。系统由使用步骤函数编排的同步 Lambda 函数组成。数据存储在第 8 章中介绍的项存储 S3 存储桶中。S3
    的存储桶通知触发我们的异步服务。
- en: There is less variety of services and communication channels than with previous
    chapters. From an AWS service point of view, this chapter will be relatively simple.
    The new aspects being introduced include Amazon Comprehend features and S3 event
    notifications as a trigger for data processing.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 与前几章相比，服务和通信渠道的种类较少。从 AWS 服务角度来看，本章将相对简单。新引入的方面包括 Amazon Comprehend 功能和 S3 事件通知作为数据处理触发器。
- en: 9.2 Understanding Comprehend’s entity recognition APIs
  id: totrans-24
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.2 理解 Comprehend 的实体识别 API
- en: Amazon Comprehend has more than one supported interface for entity recognition.
    Before we go into further detail on how data flows through the system, let’s take
    some time to understand how Comprehend works and what impacts that may have on
    our architecture.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend 支持多种实体识别接口。在我们进一步详细说明数据如何通过系统流动之前，让我们花些时间来了解 Comprehend 的工作原理以及它可能对我们架构产生的影响。
- en: The three entity recognition interfaces in Amazon Comprehend are outlined in
    table 9.1
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Amazon Comprehend 中的三个实体识别接口在表 9.1 中概述。
- en: Table 9.1 Amazon Comprehend modes of operation
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 表 9.1 Amazon Comprehend 操作模式
- en: '| API | Description | Limits |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| API | 描述 | 限制 |'
- en: '| On-demand entity recognition | A single piece of text is analyzed. Results
    are returned synchronously. | Up to 5,000 characters only. |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| 按需实体识别 | 分析单个文本片段。结果同步返回。 | 仅支持最多 5,000 个字符。 |'
- en: '| Batch entity recognition | Multiple pieces of text are analyzed. Results
    are returned synchronously. | Up to 25 documents, each up to 5,000 characters
    only. |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 批量实体识别 | 分析多个文本片段。结果同步返回。 | 每个文档最多 25 个，每个文档最多 5,000 个字符。 |'
- en: '| Asynchronous entity recognition | Multiple large pieces of text are analyzed.
    Text is read from S3 and results are written to S3 asynchronously. | Only one
    request per second, 100 KB per document, 5 GB max for all documents. |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 异步实体识别 | 多个大型文本被分析。文本从S3读取，结果异步写入S3。 | 每秒仅一个请求，每份文档100 KB，所有文档最大5 GB。 |'
- en: For full details on Comprehend limits, see the Amazon Comprehend *Guidelines
    and Limits* documentation.[1](#pgfId-1104701)
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 有关Comprehend限制的完整详细信息，请参阅亚马逊Comprehend的 *指南和限制* 文档。[1](#pgfId-1104701)
- en: 'For our purposes, we wish to analyze documents larger than 5,000 characters,
    so we must choose the asynchronous mode of operation. This mode requires us to
    use two APIs: *StartEntititesDetectionJob* to initiate analysis, and *DescribeEntitiesDetectionJob*
    if we wish to poll the status of the job.'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 为了我们的目的，我们希望分析超过5,000个字符的文档，因此我们必须选择异步操作模式。此模式要求我们使用两个API：*StartEntitiesDetectionJob*
    用于启动分析，以及 *DescribeEntitiesDetectionJob* 如果我们希望轮询作业状态。
- en: 'Comprehend returns entity recognition results as an array. Each array element
    contains the following properties:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Comprehend以数组的形式返回实体识别结果。每个数组元素包含以下属性：
- en: '`Type`--The entity type that has been recognized: `PERSON`, `LOCATION`, `ORGANIZATION`,
    `COMMERCIAL_ITEM`, `EVENT`, `DATE`, `QUANTITY`, `TITLE`, or `OTHER`.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: '`Type`--已识别的实体类型：`PERSON`、`LOCATION`、`ORGANIZATION`、`COMMERCIAL_ITEM`、`EVENT`、`DATE`、`QUANTITY`、`TITLE`
    或 `OTHER`。'
- en: '`Score`--The confidence score in the analysis result. This is a value between
    `0` and `1`.'
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Score`--分析结果中的置信度分数。这是一个介于 `0` 和 `1` 之间的值。'
- en: '`Text`--The text of the entity that has been recognized.'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Text`--已识别的实体文本。'
- en: '`BeginOffset`--The begin offset of the entity within the text.'
  id: totrans-38
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`BeginOffset`--实体在文本中的起始偏移量。'
- en: '`EndOffset`--The end offset of the entity within the text.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`EndOffset`--实体在文本中的结束偏移量。'
- en: To get a feel for how Comprehend works, let’s run a once-off test using the
    AWS Command Line Interface. Using the shell is a useful way to familiarize yourself
    with any new AWS service.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解Comprehend的工作方式，让我们使用AWS命令行界面运行一次性的测试。使用shell是熟悉任何新AWS服务的一种有用方式。
- en: Tip Chapter 2 and appendix A introduced the AWS CLI. In addition to the normal
    AWS CLI, Amazon has released an interactive version called *AWS Shell* ([https://github.com/awslabs/aws-shell](https://github.com/awslabs/aws-shell)).
    It supports interactive help and command auto-completion. If you use the AWS CLI
    to learn and explore new services, it’s worth looking at AWS Shell.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 提示：第二章和附录A介绍了AWS CLI。除了正常的AWS CLI外，亚马逊还发布了一个名为 *AWS Shell* 的交互式版本([https://github.com/awslabs/aws-shell](https://github.com/awslabs/aws-shell))。它支持交互式帮助和命令自动完成。如果您使用AWS
    CLI来学习和探索新服务，查看AWS Shell是值得的。
- en: We are going to analyze some sample text available in the code repository under
    `chapter8-9/sample-text/apollo.txt`. The paragraph of text is taken from the *Apollo
    11* page on Wikipedia.[2](#pgfId-1104723) The sample text is shown in the next
    listing.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将分析代码仓库中 `chapter8-9/sample-text/apollo.txt` 下的样本文本。文本段落取自维基百科上的 *阿波罗11号*
    页面。[2](#pgfId-1104723) 样本文本将在下一列表中展示。
- en: 'Listing 9.1 Sample text for entity recognition: `apollo.txt`'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.1 实体识别样本文本：`apollo.txt`
- en: '[PRE0]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'We can run on-demand entity recognition using the CLI using the following commands:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下命令在CLI上运行按需实体识别：
- en: '[PRE1]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The output of this command, saved to `results.json`, gives an indication of
    how Comprehend provides analysis results for entity recognition tasks. Table 9.2
    shows some of the results obtained for this command in tabular format.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 此命令的输出，保存到 `results.json`，表明Comprehend如何为实体识别任务提供分析结果。表9.2以表格格式显示了此命令的一些结果。
- en: Table 9.2 Comprehend entity recognition sample results
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 表9.2 Comprehend实体识别样本结果
- en: '| Type | Text | Score | BeginOffset | EndOffset |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| 类型 | 文本 | 分数 | 起始偏移量 | 结束偏移量 |'
- en: '| `ORGANIZATION` | Apollo 11 | 0.49757930636405900 | 0 | 9 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| `ORGANIZATION` | 阿波罗11号 | 0.49757930636405900 | 0 | 9 |'
- en: '| `LOCATION` | Moon | 0.9277622103691100 | 62 | 66 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| `LOCATION` | 月球 | 0.9277622103691100 | 62 | 66 |'
- en: '| `PERSON` | Neil Armstrong | 0.9994082450866700 | 78 | 92 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| `PERSON` | 尼尔·阿姆斯特朗 | 0.9994082450866700 | 78 | 92 |'
- en: '| `PERSON` | Buzz Aldrin | 0.9906044602394100 | 116 | 127 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| `PERSON` | 巴兹·奥尔德林 | 0.9906044602394100 | 116 | 127 |'
- en: '| `OTHER` | American | 0.6279735565185550 | 139 | 147 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| `OTHER` | 美国 | 0.6279735565185550 | 139 | 147 |'
- en: '| `ORGANIZATION` | Apollo | 0.23635128140449500 | 169 | 175 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| `ORGANIZATION` | 阿波罗 | 0.23635128140449500 | 169 | 175 |'
- en: '| `COMMERCIAL_ITEM` | Lunar Module Eagle | 0.7624998688697820 | 176 | 194 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| `COMMERCIAL_ITEM` | 月球模块鹰 | 0.7624998688697820 | 176 | 194 |'
- en: '| `DATE` | "July 20, 1969" | 0.9936476945877080 | 198 | 211 |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| `DATE` | "1969年7月20日" | 0.9936476945877080 | 198 | 211 |'
- en: '| `QUANTITY` | first person | 0.8917713761329650 | 248 | 260 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| `QUANTITY` | 第一人称 | 0.8917713761329650 | 248 | 260 |'
- en: '| `QUANTITY` | about two and a quarter hours | 0.9333438873291020 | 395 | 424
    |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| `QUANTITY` | 大约两个半小时 | 0.9333438873291020 | 395 | 424 |'
- en: '| `QUANTITY` | 21.5 kg | 0.995818555355072 | 490 | 497 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| `QUANTITY` | 21.5公斤 | 0.995818555355072 | 490 | 497 |'
- en: '| `LOCATION` | Earth | 0.9848601222038270 | 534 | 539 |'
  id: totrans-61
  prefs: []
  type: TYPE_TB
  zh: '| `LOCATION` | 地球 | 0.9848601222038270 | 534 | 539 |'
- en: '| `PERSON` | Michael Collins | 0.9996771812438970 | 562 | 577 |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '| `PERSON` | 迈克尔·柯林斯 | 0.9996771812438970 | 562 | 577 |'
- en: '| `LOCATION` | Columbia | 0.9617793560028080 | 602 | 610 |'
  id: totrans-63
  prefs: []
  type: TYPE_TB
  zh: '| `LOCATION` | 哥伦比亚 | 0.9617793560028080 | 602 | 610 |'
- en: It’s clear that very accurate results can be obtained from Comprehend entity
    recognition with very little effort.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 很明显，通过Comprehend实体识别可以以非常小的努力获得非常准确的结果。
- en: 'In order to get these kinds of results for every web page crawled from conference
    sites, we will use the asynchronous entity recognition API. This means we will
    have to handle the following characteristics of this API:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 为了从会议网站爬取的每个网页中获得这些结果，我们将使用异步实体识别API。这意味着我们必须处理以下API的特性：
- en: Entity recognition jobs on Comprehend take longer to run in asynchronous mode.
    Each job may take from 5 to 10 minutes. This is much longer than synchronous jobs,
    but the trade-off is that asynchronous jobs can process much larger documents.
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Comprehend上进行的实体识别作业在异步模式下运行时间较长。每个作业可能需要5到10分钟。这比同步作业要长得多，但权衡是异步作业可以处理更大的文档。
- en: To avoid hitting API throttling limits, we will avoid more than one request
    per second, and submit multiple web pages to each job.
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了避免触碰到API的限速限制，我们将避免每秒发送超过一个请求，并将多个网页提交给每个任务。
- en: The asynchronous API in Amazon Comprehend writes results to a configured S3
    bucket. We will process results by using notification triggers on the S3 bucket.
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 亚马逊Comprehend中的异步API将结果写入配置的S3存储桶。我们将通过S3存储桶上的通知触发器来处理结果。
- en: The web crawler from chapter 8 wrote a text file (`page.txt`) for each web page
    to an S3 bucket. In order to start entity recognition, we will make a copy of
    this in a separate staging folder in S3\. This way, we can check the contents
    of the staging folder for new text files to be processed. When processing has
    started, we will delete the file from the staging area. The original file (`page.txt`)
    will remain in place in the `sites` folder permanently, so it is available for
    further processing if required later.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第8章中的网络爬虫为每个网页在S3存储桶中写了一个文本文件（`page.txt`）。为了开始实体识别，我们将在S3的单独的临时文件夹中创建这个文件的副本。这样，我们可以检查临时文件夹中的新文本文件以供处理。当处理开始时，我们将从临时区域删除文件。原始文件（`page.txt`）将永久保留在`sites`文件夹中，因此如果以后需要，它仍然可用于进一步处理。
- en: Let’s go ahead and implement the simple service that will create a copy of the
    text file in the staging area.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续实现一个简单的服务，该服务将在临时区域创建文本文件的副本。
- en: 9.3 Preparing data for information extraction
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.3 准备信息提取数据
- en: The staging area that contains files ready to be processed will be a directory
    in the item store S3 bucket called `incoming-texts`. We will use S3 notification
    triggers to react to new `page.txt` files arriving in the bucket from the web
    crawler. Each file will then be copied to `incoming-texts/`.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 包含准备处理文件的临时区域将是项目存储S3存储桶中的一个目录，称为`incoming-texts`。我们将使用S3通知触发器来响应从网络爬虫到达存储桶中的新`page.txt`文件。然后，每个文件将被复制到`incoming-texts/`。
- en: 9.3.1 Getting the code
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.1 获取代码
- en: The code for the preparation service is in the directory `chapter8-9/preparation-service`.
    This directory contains a `serverless.yml`. We will explain the contents as we
    go before deploying and testing the preparation service.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 准备服务的代码位于`chapter8-9/preparation-service`目录中。此目录包含一个`serverless.yml`文件。在部署和测试准备服务之前，我们将逐步解释其内容。
- en: 9.3.2 Creating an S3 event notification
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.2 创建S3事件通知
- en: The preparation service is largely composed of one simple function with an event
    notification. Let’s explore `serverless.yml` in detail to see how this works.
    The next listing shows an extract of this file, where we encounter our first S3
    bucket event notification.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 准备服务主要由一个具有事件通知的简单函数组成。让我们详细探索`serverless.yml`，看看它是如何工作的。接下来的列表显示了该文件的摘录，其中我们遇到了第一个S3存储桶事件通知。
- en: Listing 9.2 Preparation service `serverless.yml` file extract
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.2 准备服务 `serverless.yml` 文件摘录
- en: '[PRE2]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ To use S3 existing buckets as event triggers, you must be using Serverless
    Framework 1.47.0 or later.[3](#pgfId-1115850) This line enforces that requirement.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 要使用S3现有的存储桶作为事件触发器，您必须使用Serverless Framework 1.47.0或更高版本。[3](#pgfId-1115850)
    这一行强制执行了该要求。
- en: ❷ A new version of every Lambda function is created every time we deploy. The
    serverless-prune-plugin takes care of removing old versions of Lambda functions
    as they accumulate.[4](#pgfId-1115872)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 每次我们部署时，都会为每个Lambda函数创建一个新的版本。serverless-prune-plugin负责在版本积累时移除旧的Lambda函数版本。[4](#pgfId-1115872)
- en: ❸ We want to use CloudFormation Sub function with pseudo parameters such as
    ${AWS::AccountId} in our configuration,[5](#pgfId-1115900) but this syntax conflicts
    with the Serverless Framework’s variable syntax.[6](#pgfId-1115922) serverless-pseudo-parameters[7](#pgfId-1115944)
    solves this by allowing us to use a simpler syntax (#{AWS::AccountId}) instead.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们希望在配置中使用带有伪参数（如${AWS::AccountId}）的CloudFormation子函数，[5](#pgfId-1115900)，但此语法与Serverless
    Framework的变量语法冲突。[6](#pgfId-1115922) serverless-pseudo-parameters[7](#pgfId-1115944)通过允许我们使用更简单的语法（#{AWS::AccountId}）来解决此问题。
- en: ❹ Just as in previous chapters, we use serverless-dotenv-plugin[8](#pgfId-1115966)
    to load environment variables from the .env file.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 正如前几章所述，我们使用serverless-dotenv-plugin[8](#pgfId-1115966)从.env文件中加载环境变量。
- en: ❺ We give our function permissions to read to and write from the item store
    bucket.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们为函数授予读取和写入项目存储桶的权限。
- en: ❻ The S3 event handling function is defined in handler.js. The function name
    is exported as prepare.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ S3事件处理函数在handler.js中定义。函数名称被导出为prepare。
- en: ❼ The Lambda trigger is defined as an S3 notification. The notifications will
    match any object (file) created in the bucket with the suffix page.txt.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ Lambda触发器被定义为S3通知。通知将匹配在桶中创建的任何带有page.txt后缀的对象（文件）。
- en: ❽ This ensures that the Serverless Framework will not attempt to create the
    bucket. Instead it will create a notification trigger on the existing item store
    bucket.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 这确保了Serverless Framework不会尝试创建桶。相反，它将在现有的项目存储桶上创建一个通知触发器。
- en: We have just declared the function, its resources, and triggers for the preparation
    handler. We can now move on to the implementation of this function.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚声明了函数、其资源以及准备处理器的触发器。现在我们可以继续实现这个函数。
- en: CloudFormation and S3 notification triggers
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: CloudFormation和S3通知触发器
- en: CloudFormation is a fantastic way to define Infrastructure as Code in a way
    that supports logically grouped resources and rollback in the event of any failure.
    One disadvantage, however, is that CloudFormation is not as flexible as the AWS
    SDK for creating all resource types.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: CloudFormation是一种定义基础设施即代码的绝佳方式，它支持逻辑上分组资源，并在任何失败事件发生时提供回滚功能。然而，一个缺点是，与AWS SDK创建所有资源类型相比，CloudFormation不够灵活。
- en: One example of this is with bucket notifications. Using CloudFormation, notifications
    can only be added when the bucket resource is created.[9](#pgfId-1107894) We would
    prefer to be able to add notifications to existing buckets for any service in
    our system.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这的一个例子是与桶通知一起使用。使用CloudFormation，只有在创建桶资源时才能添加通知。[9](#pgfId-1107894)我们更希望能够在我们的系统中为任何服务添加现有桶的通知。
- en: The Serverless Framework provides a great workaround for this problem. By using
    an `s3` event type with the property `existing:` `true`, the framework uses the
    AWS SDK under the hood to add a new notification to an existing bucket. This is
    achieved using *CloudFormation custom resources*, a useful workaround when official
    CloudFormation support falls short of your needs. For more information on custom
    resources, see the AWS documentation.[10](#pgfId-1107901)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: Serverless Framework为这个问题提供了一个很好的解决方案。通过使用具有属性`existing:` `true`的`s3`事件类型，框架在底层使用AWS
    SDK向现有桶添加一个新的通知。这是通过使用*CloudFormation自定义资源*实现的，这是一种有用的解决方案，当官方CloudFormation支持不足以满足您的需求时。有关自定义资源的更多信息，请参阅AWS文档。[10](#pgfId-1107901)
- en: 9.3.3 Implementing the preparation handler
  id: totrans-92
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.3 实现准备处理程序
- en: The goal of the preparation service’s `handler` module is to perform any processing
    required in order for the text to be ready for entity recognition. In our case,
    this is simply a case of putting the text in the right folder with the right filename
    for processing. The preparation service’s `handler` module is shown in the following
    listing.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 准备服务`handler`模块的目标是执行任何必要的处理，以便文本准备好进行实体识别。在我们的案例中，这仅仅是把文本放入正确的文件夹，并使用正确的文件名进行处理。准备服务的`handler`模块如下所示。
- en: Listing 9.3 Preparation service `handler.js` extract
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.3 准备服务`handler.js`摘录
- en: '[PRE3]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ Every S3 notification event is an array of length 1.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 每个S3通知事件都是一个长度为1的数组。
- en: ❷ Object keys are URL-encoded when they arrive in S3 events.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当对象键到达S3事件时，它们会被URL编码。
- en: ❸ The key for the staging area copy is created by replacing the prefix and the
    filename from the incoming key string.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 阶段区域副本的键是通过替换传入键字符串中的前缀和文件名来创建的。
- en: ❹ The S3 object’s contents are written to the target key.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将S3对象的内写入目标键。
- en: 9.3.4 Adding resilience with a dead letter queue (DLQ)
  id: totrans-100
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.4 使用死信队列（DLQ）增加弹性
- en: Before we deploy the preparation services, let’s deal with the issue of resilience
    and retries. If our event handler fails to process the event, we risk the event
    being lost. Lambda will retry our function twice.[11](#pgfId-1105049) If our function
    does not successfully handle the event during any of these invocation attempts,
    there will be no further automatic retries.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署准备服务之前，让我们处理弹性和重试的问题。如果我们的事件处理程序无法处理事件，我们可能会丢失事件。Lambda将重试我们的函数两次。[11](#pgfId-1105049)
    如果我们的函数在任何这些调用尝试中未能成功处理事件，将不会有进一步的自动重试。
- en: Luckily, we can configure a dead-letter queue (DLQ) for any Lambda function.
    This is where unprocessed events go after automatic retries have failed. Once
    they are in the DLQ, it is up to us to decide how to reprocess them.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，我们可以为任何Lambda函数配置一个死信队列（DLQ）。在自动重试失败后，未处理的事件将进入这里。一旦它们在DLQ中，就取决于我们决定如何重新处理它们。
- en: A DLQ may be an SQS queue or SNS topic. SNS (Simple Notification Service) is
    used for pub/sub messaging, a topic covered in chapter 2\. SQS (Simple Queue Service)
    is used for point-to-point messaging. We are going to use an SQS queue for our
    DLQ, since we only need one consumer. The DLQ interaction is illustrated in figure
    9.3.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: DLQ可能是一个SQS队列或SNS主题。SNS（简单通知服务）用于pub/sub消息，这是第2章中介绍的内容。SQS（简单队列服务）用于点对点消息。我们将使用SQS队列作为我们的DLQ，因为我们只需要一个消费者。DLQ交互如图9.3所示。
- en: '![](../Images/CH09_F03_Elger.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F03_Elger.png)'
- en: Figure 9.3 A DLQ facilitates inspection and reprocessing of events that have
    caused a Lambda execution failure.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.3 一个DLQ有助于检查和重新处理导致Lambda执行失败的的事件。
- en: 'This is how we will handle unprocessed messages:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这是我们处理未处理消息的方式：
- en: We set an SQS queue as the DLQ for the `prepare` Lambda function.
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们为`prepare` Lambda函数设置了一个SQS队列作为DLQ。
- en: Unprocessed messages are sent to our queue after all retry attempts have failed.
  id: totrans-108
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在所有重试尝试失败后，未处理的消息将被发送到我们的队列。
- en: We can inspect the SQS queue in the AWS Console intermittently. In a production
    scenario, we would ideally set up a CloudWatch alarm to alert us when the number
    of messages in this queue exceeds zero. To keep it simple, we are not going to
    create a CloudWatch alarm in this chapter.[12](#pgfId-1105068)
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以间歇性地检查AWS控制台中的SQS队列。在生产场景中，我们理想情况下会设置一个CloudWatch警报，在队列中的消息数量超过零时提醒我们。为了简化，我们不会在本章中创建CloudWatch警报。[12](#pgfId-1105068)
- en: We will create a second Lambda function whose sole purpose is to retrieve messages
    from the DLQ and pass them back to the original `prepare` Lambda function. This
    can be invoked manually when we notice unprocessed messages and have taken steps
    to remedy the underlying problem.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们将创建第二个Lambda函数，其唯一目的是从DLQ检索消息并将它们传递回原始的`prepare` Lambda函数。当我们注意到未处理的消息并已采取措施解决根本问题时，可以手动调用此函数。
- en: 9.3.5 Creating the DLQ and retry handler
  id: totrans-111
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.5 创建DLQ和重试处理程序
- en: In chapters 2 and 3, we used an SQS queue to trigger a Lambda function. In the
    case of the DLQ, we don’t want our retry Lambda to be automatically triggered.
    Since we will manually invoke the retry Lambda, the retry handler must manually
    read messages from the SQS queue. Let’s take a look at the additions to `serverless.yml`.
    The following listing shows the relevant extracts. You can find the complete configuration
    file in `chapter8-9/preparation-service`.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在第2章和第3章中，我们使用SQS队列来触发Lambda函数。在DLQ的情况下，我们不希望我们的重试Lambda函数自动触发。由于我们将手动调用重试Lambda，重试处理程序必须手动从SQS队列中读取消息。让我们看看`serverless.yml`的添加部分。以下列表显示了相关摘录。您可以在`chapter8-9/preparation-service`中找到完整的配置文件。
- en: Listing 9.4 Preparation service `serverless.yml` DLQ extract
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.4 准备服务`serverless.yml` DLQ摘录
- en: '[PRE4]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ The DLQ queue name is different for each deployed stage to avoid naming conflicts.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ DLQ队列名称对于每个部署阶段都不同，以避免命名冲突。
- en: ❷ The Lambda requires four permissions to read and process the messages in the
    DLQ
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Lambda需要四个权限来读取和处理DLQ中的消息
- en: ❸ onError is used to set the DLQ of the prepare Lambda function to the SQS queue
    ARN.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用`onError`将准备Lambda函数的DLQ设置为SQS队列的ARN。
- en: ❹ The retry Lambda function is configured without any event triggers. The DLQ
    queue is configured using an environment variable.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 重试Lambda函数配置时没有事件触发器。DLQ队列使用环境变量配置。
- en: ❺ We set the message retention period for the DLQ to one day. This should be
    reasonably long so undelivered messages can be manually recovered. The maximum
    message retention value is 14 days.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们将DLQ的消息保留期设置为一天。这应该足够长，以便可以手动恢复未投递的消息。最大消息保留值是14天。
- en: 'The Lambda handler is implemented in the `retry` function in `dlq-handler.js`.
    When invoked, its goal is to perform the following sequence of actions:'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda处理器在`dlq-handler.js`文件中的`retry`函数中实现。当被调用时，其目标是执行以下一系列操作：
- en: Retrieve a batch of messages from the DLQ.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从DLQ中检索一批消息。
- en: For each message, extract the original event from the message.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每条消息，从消息中提取原始事件。
- en: Invoke the `prepare` function by loading the `handler` module and calling `prepare`
    directly with the event, and wait for a success or failure response.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过加载`handler`模块并直接使用事件调用`prepare`函数来调用`prepare`函数，并等待成功或失败响应。
- en: If the event has succeeded, delete the message from the DLQ.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果事件已成功，则从DLQ中删除该消息。
- en: Proceed to process the next message until all messages in the batch have been
    processed.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 继续处理下一条消息，直到批次中的所有消息都已被处理。
- en: DLQ handling is a common pattern that we wish to apply to multiple Lambda functions,
    so we have extracted it into a separate, open source NPM module, `lambda-dlq-retry`.[13](#pgfId-1105131)
    The use of this module makes the retry implementation simpler. Let’s take a look
    at `dlq-handler.js`, shown in the following listing.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: DLQ处理是我们希望应用于多个Lambda函数的常见模式，因此我们将其提取到一个独立的、开源的NPM模块中，即`lambda-dlq-retry`。[13](#pgfId-1105131)
    使用此模块简化了重试实现。让我们看看下面的`dlq-handler.js`，如下所示。
- en: Listing 9.5 Preparation service DLQ handler
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.5 准备服务DLQ处理器
- en: '[PRE5]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ The lambda-dlq-retry module is imported.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入lambda-dlq-retry模块。
- en: ❷ The module containing the prepare function for the preparation service is
    required.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 需要包含准备服务的准备函数的模块。
- en: ❸ We export a DLQ retry handler, created for us by the lambda-dlq-retry module
    using the specified handler. You may pass a logger instance. If debug logging
    is turned on, this will produce log entries relating to DLQ retries.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们导出DLQ重试处理器，这是lambda-dlq-retry模块为我们创建的，使用指定的处理器。您可以传递一个日志记录实例。如果启用了调试日志记录，这将产生与DLQ重试相关的日志条目。
- en: It’s worth mentioning that `lambda-dlq-retry` processes messages in batches
    of up to 10\. This can be configured by setting an alternative value in the environment
    variable, `DLQ_RETRY_MAX_MESSAGES`.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 值得注意的是，`lambda-dlq-retry`以最多10条消息的批次处理消息。这可以通过在环境变量中设置一个替代值来配置，即`DLQ_RETRY_MAX_MESSAGES`。
- en: 9.3.6 Deploying and testing the preparation service
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3.6 部署和测试准备服务
- en: We have created four Lambda functions so far in this chapter. It’s worthwhile
    reviewing these before we deploy and run them so we can start gaining a clear
    understanding of how they work together. Figure 9.4 revisits our service architecture
    from the beginning of the chapter. The sections we have covered already are highlighted.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经在本章中创建了四个Lambda函数。在部署和运行它们之前回顾这些函数是很有价值的，这样我们就可以开始清楚地了解它们是如何协同工作的。图9.4回顾了本章开头我们的服务架构。我们已经涵盖的部分被突出显示。
- en: Before we deploy the preparation service, make sure you have set up `.env` in
    the `chapter8-9` directory as outlined in chapter 8\. This contains the item store
    bucket name environment variable. Once that’s done, we can proceed with the usual
    steps to build and deploy!
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们部署准备服务之前，请确保您已在`chapter8-9`目录中设置了`.env`，如第8章所述。这包含项目存储桶名称环境变量。一旦完成，我们就可以继续进行构建和部署的常规步骤！
- en: '[PRE6]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: '![](../Images/CH09_F04_Elger.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH09_F04_Elger.png)'
- en: Figure 9.4 So far, we have implemented Lambda functions for text preparation,
    getting a batch of text files, starting entity recognition, and checking recognition
    progress.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.4到目前为止，我们已经实现了用于文本准备、获取一批文本文件、启动实体识别和检查识别进度的Lambda函数。
- en: 'To test our function, we can manually upload a file to the item store bucket
    with the suffix `page.txt`. We can then check to see if it is copied to the `incoming-texts`
    staging area. We can use the sample text we already have from our simple Comprehend
    test:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的函数，我们可以手动将文件上传到带有后缀`page.txt`的项目存储桶中。然后我们可以检查它是否已复制到`incoming-texts`的预演区域。我们可以使用我们从简单的Comprehend测试中已有的示例文本：
- en: '[PRE7]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'To check the logs for the `prepare` function, we can use the Serverless `logs`
    command. This will take CloudWatch logs for the function and print them on the
    console. Because we used the `pino` module for logging in chapter 8, we can format
    them nicely for readable output by piping the output to the `pino-pretty` module:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查 `prepare` 函数的日志，我们可以使用 Serverless 的 `logs` 命令。这将打印函数的 CloudWatch 日志到控制台。由于我们在第
    8 章中使用了 `pino` 模块进行日志记录，我们可以通过将输出管道传输到 `pino-pretty` 模块来格式化它们，以便于阅读输出：
- en: '[PRE8]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: You should see some output similar to that shown in the following listing.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该看到类似于以下列表的输出。
- en: Listing 9.6 Preparation service log output
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.6 准备服务日志输出
- en: '[PRE9]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'You can then check the S3 bucket for the contents of the file in the staging
    area:'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，您可以检查 S3 桶中暂存区域中文件的包含内容：
- en: '[PRE10]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Lastly, we are going to test the DLQ retry functionality. There’s no point
    in having a process to handle recovery from a failure if it has not been tested
    and verified to work! To simulate an error, we are going to withdraw read permissions
    to the S3 bucket. Comment out the `GetObject` permission from the Lambda IAM role
    policy in `serverless.yml` as follows:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将测试 DLQ 重试功能。如果没有经过测试和验证以确保其工作，那么拥有一个处理失败恢复的过程是没有意义的！为了模拟错误，我们将撤销对 S3 桶的读取权限。在
    `serverless.yml` 中注释掉 Lambda IAM 角色策略中的 `GetObject` 权限，如下所示：
- en: '[PRE11]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Deploy the updated preparation service with the modified IAM role:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 使用修改后的 IAM 角色部署更新的准备服务：
- en: '[PRE12]'
  id: totrans-151
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'We can run the same test again using a different S3 key (path):'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用不同的 S3 密钥（路径）再次运行相同的测试：
- en: '[PRE13]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This time, we should observe an error in the `prepare` function logs:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这次，我们应该在 `prepare` 函数日志中观察到错误：
- en: '[PRE14]'
  id: totrans-155
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You’ll see this error two more times: once after one minute, and again after
    two additional minutes. This is because AWS Lambda is automatically retrying.
    After the three attempts have failed, you should see the message arriving in the
    DLQ.'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 您将看到此错误出现两次：一次在一分钟后，再次在额外两分钟后。这是因为 AWS Lambda 自动重试。在三次尝试失败后，您应该看到消息到达 DLQ。
- en: 'We will use the AWS Console to inspect errors before attempting redelivery:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 AWS 控制台在尝试重发之前检查错误：
- en: Browse to the SQS console and select the preparation service DLQ from the queue
    list. You will notice that the message count is set to 1.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 浏览到 SQS 控制台，并从队列列表中选择准备服务 DLQ。您会注意到消息计数设置为 1。
- en: Right-click on the queue in the list and select the View/Delete Messages option.
    Select Start Polling for Messages, and then select Stop Now once our undelivered
    S3 event message has come into view.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在列表中的队列上右键单击，并选择查看/删除消息选项。选择开始轮询消息，然后在我们未投递的 S3 事件消息出现后选择立即停止。
- en: To see the full message, select More Details. We now see the full text of the
    S3 event that resulted in an error in the `prepare` Lambda function.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 要查看完整消息，请选择更多详情。我们现在看到了导致 `prepare` Lambda 函数出现错误的完整 S3 事件文本。
- en: This is valuable information for troubleshooting the original message. By selecting
    the second tab, Message Attributes, we can also see the error message along with
    the request ID. This ID matches the Lambda function invocation, and can be used
    to correlate the error back to the logs in CloudWatch. You may notice that the
    “error code” is shown here to be 200\. This value can be ignored, as it is always
    set to 200 for DLQ messages.
  id: totrans-161
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这是解决原始消息问题的宝贵信息。通过选择第二个标签页，消息属性，我们还可以看到错误消息以及请求 ID。此 ID 与 Lambda 函数调用匹配，可以用来将错误关联回
    CloudWatch 中的日志。您可能会注意到“错误代码”在这里显示为 200。此值可以忽略，因为它总是设置为 200 用于 DLQ 消息。
- en: 'Next, test redelivery by restoring the correct permissions in `serverless.yml`.
    Uncomment the `s3:GetObject` line and redeploy with `sls deploy`. We can choose
    to trigger the retry Lambda through the AWS Console, the AWS CLI, or using the
    Serverless Framework `invoke` commands. The following command uses the AWS CLI:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，通过在 `serverless.yml` 中恢复正确的权限来测试重发。取消注释 `s3:GetObject` 行，并使用 `sls deploy`
    重新部署。我们可以选择通过 AWS 控制台、AWS CLI 或使用 Serverless Framework 的 `invoke` 命令来触发重试 Lambda。以下命令使用
    AWS CLI：
- en: '[PRE15]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'If you run this and inspect the output in `/tmp/dlq-retry-output`, you should
    see a simple JSON object (`{"count":` `1}`). This means that one message has been
    processed and delivered! We can inspect the output of the retry Lambda as we did
    before, using the `sls logs` command:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您运行此命令并检查 `/tmp/dlq-retry-output` 中的输出，您应该看到一个简单的 JSON 对象（`{"count":` `1}`）。这意味着一条消息已被处理并投递！我们可以像之前一样使用
    `sls logs` 命令检查重试 Lambda 的输出：
- en: '[PRE16]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: This will show that the S3 event has been successfully processed this time.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这将表明这次 S3 事件已成功处理。
- en: 9.4 Managing throughput with text batches
  id: totrans-167
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.4.1 使用文本批次管理吞吐量
- en: Now we have a separate staging area, along with a preparation service to populate
    it with text from conference web pages as files are created by the web crawler.
    We have also decided to use the asynchronous Comprehend API and process text in
    batches. Our next step is to create a simple Lambda to retrieve a batch of text
    files to be processed.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个单独的临时区域，以及一个准备服务，用于在网页爬虫创建文件时填充它。我们还决定使用异步Comprehend API并批量处理文本。我们的下一步是创建一个简单的Lambda函数，用于检索要处理的文本文件批次。
- en: 9.4.1 Getting the code
  id: totrans-169
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.1 获取代码
- en: 'The `getTextBatch` function can be found in the `extraction-servicehandler`
    module. The extraction service includes the rest of the functionality for this
    chapter, as it deals with extraction and reporting of extraction results:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: '`getTextBatch`函数可以在`extraction-servicehandler`模块中找到。提取服务包括本章其余的功能，因为它处理提取和提取结果的报告：'
- en: '[PRE17]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 9.4.2 Retrieving batches of text for extraction
  id: totrans-172
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.4.2 获取用于提取的文本批次
- en: The source code for `getTextBatch` is shown in the next listing. This function
    uses the S3 `listObjectsV2` API to read files in the staging area up to a specified
    limit.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: '`getTextBatch`函数的源代码如下所示。此函数使用S3的`listObjectsV2` API读取临时区域（incoming-texts）中的文件，直到达到指定的限制。'
- en: Listing 9.7 The `getTextBatch` function
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.7 `getTextBatch`函数
- en: '[PRE18]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Read up to 25 keys from the staging area (incoming-texts).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从临时区域（incoming-texts）读取最多25个键。
- en: ❷ Modify file names to remove the incoming-texts/ prefix from the batch results.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 修改文件名，以从批处理结果中移除incoming-texts/前缀。
- en: ❸ The batch of transformed file names is returned along with a count indicating
    the batch size.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回转换后的文件名批次，并附带一个表示批次大小的计数。
- en: We will wait to deploy the extraction service in full, so let’s test this using
    the `sls invoke local` command. Bear in mind that although we are executing the
    function locally, it is calling out to S3\. As a result, your `AWS_` environment
    variables should be set here to ensure you are authorized to execute these SDK
    calls.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将等待完全部署提取服务，所以让我们使用`sls invoke local`命令进行测试。请注意，尽管我们是在本地执行函数，但它正在调用S3。因此，你的`AWS_`环境变量应该在这里设置，以确保你有权执行这些SDK调用。
- en: 'We run the function locally as follows:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如下本地运行该函数：
- en: '[PRE19]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: You should see some output similar to the following listing.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 你应该看到类似于以下列表的输出。
- en: Listing 9.8 Sample output from `getTextBatch`
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.8 `getTextBatch`的示例输出
- en: '[PRE20]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 9.5 Asynchronous named entity abstraction
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.5 异步命名实体抽象
- en: We already have a means to get a batch of text from conference web pages. Let’s
    now build a function to take a set of text files and initiate entity recognition.
    Remember that we are using asynchronous entity recognition in Comprehend. With
    this method, input files are stored in S3\. We can poll Comprehend to check the
    status of the recognition job, and results are written to a specified path in
    an S3 bucket.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有一种方法可以从会议网页中获取一批文本。现在，让我们构建一个函数，用于从一组文本文件中启动实体识别。记住，我们在Comprehend中使用异步实体识别。使用这种方法，输入文件存储在S3中。我们可以轮询Comprehend以检查识别作业的状态，并将结果写入S3桶中指定的路径。
- en: 9.5.1 Get the code
  id: totrans-187
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.1 获取代码
- en: The code for the extraction service is in the directory `chapter8-9/extraction-service`.
    Our `startBatchProcessing` and `checkActiveJobs` functions can be found in `handler.js`.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 提取服务的代码位于`chapter8-9/extraction-service`目录中。我们的`startBatchProcessing`和`checkActiveJobs`函数可以在`handler.js`中找到。
- en: 9.5.2 Starting an entity recognition job
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.5.2 启动实体识别作业
- en: The AWS SDK for Comprehend provides us with the `startEntitiesDetectionJob`
    function.[14](#pgfId-1105334) It requires us to specify an input path for *all*
    text files in S3 to be processed. We want to ensure that no text files are omitted
    from processing. To achieve this, we will copy files to be processed to a batch
    directory, and only delete the source files once the `startEntitiesDetectionJob`
    call has succeeded.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: AWS Comprehend SDK为我们提供了`startEntitiesDetectionJob`函数。[14](#pgfId-1105334) 它要求我们指定一个输入路径，用于处理S3中所有文本文件。我们希望确保没有文本文件被遗漏处理。为了实现这一点，我们将要处理的文件复制到批处理目录中，并且只有在`startEntitiesDetectionJob`调用成功后才会删除源文件。
- en: This can be seen in the `startBatchProcessing` Lambda function in the extraction
    service’s `handler.js`, shown in the next listing.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以在提取服务的`handler.js`中的`startBatchProcessing` Lambda函数中看到，如下面的列表所示。
- en: Listing 9.9 Extraction service handler `startBatchProcessing` function
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.9 提取服务处理程序`startBatchProcessing`函数
- en: '[PRE21]'
  id: totrans-193
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ The event is passed an array of paths. The paths are relative to the incoming_texts
    prefix. This set of paths makes up the batch.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 事件传递一个路径数组。这些路径相对于incoming_texts前缀。这些路径集构成了批处理。
- en: ❷ We generate a batch ID based on the current time. This is used to create the
    batch directory in S3.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 我们基于当前时间生成一个批处理ID。这用于在S3中创建批处理目录。
- en: ❸ All files in the batch are copied to the batch directory before processing.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在处理之前，批处理中的所有文件都复制到批处理目录中。
- en: ❹ The S3 copyObject API requires the CopySource property to be URL-encoded.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ S3 copyObject API需要URL编码的CopySource属性。
- en: ❺ We pass the batch ID to our startEntityRecognition function so all files in
    the batch can be analyzed together.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 我们将批处理ID传递给我们的startEntityRecognition函数，以便批处理中的所有文件可以一起分析。
- en: ❻ When the batch recognition has started, we proceed to deleting all input paths
    in the incoming_texts directory.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 批处理识别开始后，我们继续删除incoming_texts目录中的所有输入路径。
- en: We can now see how, by copying files into a batch directory, we ensure that
    each file in `incoming_texts` will be processed. Any error in starting the batch
    recognition job will leave the file in `incoming_texts` so it can be reprocessed
    using a subsequent batch.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看到，通过将文件复制到批处理目录中，我们确保`incoming_texts`中的每个文件都将被处理。启动批处理识别作业时出现的任何错误都会使文件留在`incoming_texts`中，以便可以使用后续的批处理重新处理。
- en: We just saw a reference to the `startEntityRecognition` function. This is the
    function responsible for creating the parameters for Comprehend’s `startEntitiesDetectionJob`
    API. Listing 9.10 shows the code for this function.
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚看到了对`startEntityRecognition`函数的引用。这个函数负责为Comprehend的`startEntitiesDetectionJob`
    API创建参数。列表9.10显示了该函数的代码。
- en: Listing 9.10 Extraction service `startBatchProcessing` Lambda function
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.10 提取服务`startBatchProcessing` Lambda函数
- en: '[PRE22]'
  id: totrans-203
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ For ease of manual troubleshooting, we use the generated batch ID as the job
    name.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 为了便于手动故障排除，我们使用生成的批处理ID作为作业名称。
- en: ❷ The job requires an IAM role with permissions to read and write to the S3
    bucket. The role definition can be found in extraction-service/serverless.yml.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 作业需要一个具有读取和写入S3存储桶权限的IAM角色。角色定义可以在extraction-service/serverless.yml中找到。
- en: ❸ We need to tell Comprehend that each file in the S3 folder represents a single
    document. The other option is ONE_DOC_PER_LINE.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们需要告诉Comprehend，S3文件夹中的每个文件代表一个单独的文档。另一种选择是ONE_DOC_PER_LINE。
- en: ❹ The path to the files in the batch is the path where our files have just been
    copied.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 批处理中文件的路径是我们刚刚复制文件的路径。
- en: ❺ Comprehend results are written to an output folder designated by the batch
    ID.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 将Comprehend结果写入由批处理ID指定的输出文件夹。
- en: The `startBatchProcessing` function is the core of the functionality in this
    chapter. It passes the extracted text through to AWS Comprehend, the managed AI
    service that performs the extraction of significant data.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`startBatchProcessing`函数是本章功能的核心。它将提取的文本传递到AWS Comprehend，这是一个托管AI服务，负责提取重要数据。'
- en: 9.6 Checking entity recognition progress
  id: totrans-210
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.6 检查实体识别进度
- en: Before we try out our entity recognition job processing, we will take a look
    at `checkActiveJobs`. This is a simple Lambda function that will use the Comprehend
    API to report on the status of jobs that are *in progress*. For manual progress
    checking, you can also take a look at the Comprehend section of the AWS Management
    Console. When we know how many jobs are in progress, we can know when to start
    more jobs and control the number of concurrent Comprehend job executions. The
    code for `checkActiveJobs` is shown in the next listing.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们尝试我们的实体识别作业处理之前，我们将查看`checkActiveJobs`。这是一个简单的Lambda函数，它将使用Comprehend API来报告正在进行的作业的状态。对于手动进度检查，您还可以查看AWS管理控制台的Comprehend部分。当我们知道有多少作业正在进行时，我们可以知道何时开始更多作业并控制并发Comprehend作业执行的数量。`checkActiveJobs`的代码在下一列表中显示。
- en: Listing 9.11 Extraction service `checkActiveJobs` Lambda function
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.11 提取服务`checkActiveJobs` Lambda函数
- en: '[PRE23]'
  id: totrans-213
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ The listEntitiesDetectionJobs API is invoked, filtering on in-progress jobs.
    To limit the number potentially returned, we cap the number of results to a maximum.
    We have chosen 10 for this value.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 调用listEntitiesDetectionJobs API，根据正在进行的作业进行过滤。为了限制可能返回的数量，我们将结果数量限制在最大值。我们选择了10作为这个值。
- en: ❷ The results are transformed to give us an output containing the total number
    of in-progress jobs (not exceeding our maximum job count value of 10) and a summary
    of each job.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 结果被转换，以给我们一个包含正在进行作业总数（不超过我们的最大作业计数值10）以及每个作业摘要的输出。
- en: 'We now have three Lambda functions that can be used together to perform entity
    recognition for batches of files:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有三个 Lambda 函数可以一起使用，以对文件批次执行实体识别：
- en: '`getTextBatch` to select a limited number of files for processing.'
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `getTextBatch` 选择要处理的有限数量的文件。
- en: '`startBatchProcessing` to start execution of entity recognition for a batch
    of files.'
  id: totrans-218
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `startBatchProcessing` 启动文件批次的实体识别执行。
- en: '`checkActiveJobs` to report on the number of recognition jobs in progress.
    This will come in handy later when we tie all of our entity extraction logic together.'
  id: totrans-219
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '`checkActiveJobs` 用于报告正在进行中的识别作业数量。这将在我们整合所有实体提取逻辑时非常有用。'
- en: We have tested `getTextBatch` already using `sls` `invoke` `local`. Next, we
    will deploy the extraction service and start processing on a batch of sample text
    files, to see how these functions fit together in practice.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用 `sls` `invoke` `local` 测试了 `getTextBatch`。接下来，我们将部署提取服务并开始处理一批样本文本文件，以了解这些函数在实际应用中的配合情况。
- en: 9.7 Deploying and testing batch entity recognition
  id: totrans-221
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.7 部署和测试批量实体识别
- en: 'To test our function, we are going to first deploy the extraction service.
    This is done in the same way as all our other Serverless Framework deployments:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试我们的函数，我们首先部署提取服务。这与其他所有 Serverless Framework 部署的方式相同：
- en: '[PRE24]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We can now use the Serverless Framework CLI to invoke our remote function. We
    will pass the `startBatchProcessing` Lambda function a simple JSON-encoded array
    of paths. For this example, we will use the two files already present in our `incoming-texts`
    S3 directory. These files contain the Apollo 11 sample text. Later, we will be
    performing entity recognition on real conference web page data!
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以使用 Serverless Framework CLI 来调用我们的远程函数。我们将传递一个简单的 JSON 编码的路径数组给 `startBatchProcessing`
    Lambda 函数。在这个例子中，我们将使用 `incoming-texts` S3 目录中已有的两个文件。这些文件包含阿波罗 11 的样本文本。稍后，我们将对真实的会议网页数据进行实体识别！
- en: '[PRE25]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'If the execution is successful, you should see something like the following
    output--a JSON object containing the batch ID:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 如果执行成功，你应该会看到以下类似的输出——一个包含批次 ID 的 JSON 对象：
- en: '[PRE26]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Next, we will run `checkActiveJobs` to report on the number of active Comprehend
    jobs.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将运行 `checkActiveJobs` 来报告正在进行的 Comprehend 作业数量。
- en: Listing 9.12 `checkActiveJobs` output
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 9.12 `checkActiveJobs` 输出
- en: '[PRE27]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: ❶ The total number of jobs in progress
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 正在进行的作业总数
- en: ❷ The job ID is generated by Comprehend.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 作业 ID 由 Comprehend 生成。
- en: ❸ The job name matches the batch ID we generated.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 作业名称与我们所生成的批次 ID 匹配。
- en: After 5-10 minutes, running `checkActiveJobs` again will report zero in-progress
    jobs. At this point, you can inspect the output of the job.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 经过 5-10 分钟后，再次运行 `checkActiveJobs` 将报告零个正在进行的作业。此时，你可以检查作业的输出。
- en: 'The `extraction-service` directory contains a shell script that may be used
    to conveniently find, extract, and output the results of a batch job. To run it,
    execute the following command:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '`extraction-service` 目录包含一个 shell 脚本，可以方便地查找、提取和输出批量作业的结果。要运行它，请执行以下命令：'
- en: '[PRE28]'
  id: totrans-236
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: The `<BATCH_ID>` placeholder can be replaced with the batch ID value you saw
    when `startBatchProcessing` was executed. Running this script will print JSON
    representing Comprehend entity recognition results for each sample text. In our
    example so far, both files in the batch have the same sample text about Apollo
    11\.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: '`<BATCH_ID>` 占位符可以被替换为在执行 `startBatchProcessing` 时看到的批次 ID 值。运行此脚本将打印出代表每个样本文本的
    Comprehend 实体识别结果的 JSON。在我们目前的例子中，批次中的两个文件都包含关于阿波罗 11 的相同样本文本。'
- en: 9.8 Persisting recognition results
  id: totrans-238
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.8 持久化识别结果
- en: We have seen how to manually run entity extraction functions from the command
    line and verify the NER output. For our end-to-end application for conference
    site crawling and analysis, we want to persist our entity extraction results.
    This way, we can use an API to serve extracted names for people, locations, and
    dates for our conference-seeking audience!
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到如何从命令行手动运行实体提取功能并验证命名实体识别（NER）的输出。对于我们的端到端应用，用于会议网站爬取和分析，我们希望持久化我们的实体提取结果。这样，我们可以使用API为寻找会议的观众提供提取的人名、地点和日期！
- en: Entity result processing will be driven by the arrival of Comprehend results
    in the output folder that we configured when we started the entity recognition
    job. Just as with the preparation service, we’ll use an S3 bucket notification.
    You will find the configuration of the `processEntityResults` function in the
    `serverless.yml` for the extraction service. The relevant section is reproduced
    in the next listing.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 实体结果处理将由Comprehend结果到达我们在启动实体识别作业时配置的输出文件夹驱动。就像准备服务一样，我们将使用S3桶通知。你将在提取服务的`serverless.yml`中找到`processEntityResults`函数的配置。相关部分在下一列表中重现。
- en: Listing 9.13 `serverless.yml` extract for `processEntityResults`
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.13 `processEntityResults`的`serverless.yml`提取
- en: '[PRE29]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ The notification configuration is in the same bucket as the preparation service
    S3 bucket notification. This time, the key suffix/prefix is different.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 通知配置与准备服务S3桶通知位于同一桶中。这次，键后缀/前缀不同。
- en: ❷ All Comprehend results are persisted to entity-results, as we specified in
    the call to startEntitiesDetectionJob.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 所有Comprehend结果都持久化到实体结果中，正如我们在启动实体检测作业时指定的。
- en: ❸ Comprehend writes other, temporary files. We are only interested in the final
    results, stored in output.tar.gz.
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ Comprehend还写入其他临时文件。我们只对存储在output.tar.gz中的最终结果感兴趣。
- en: 'When results arrive, we’ll use our notified Lambda function to extract results
    and persist them in the frontier service. Because the frontier service maintains
    state for all URLs, it’s convenient to store the results along with the crawling/extraction
    state. Let’s break down all the required steps:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 当结果到达时，我们将使用我们的通知Lambda函数提取结果并将它们持久化到前沿服务。由于前沿服务维护所有URL的状态，因此将结果与爬取/提取状态一起存储很方便。让我们分解所有必需的步骤：
- en: The S3 notification triggers the `processEntityResults` function.
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: S3通知触发`processEntityResults`函数。
- en: The object is fetched from S3 as a stream.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对象作为流从S3中检索。
- en: The stream is unzipped and extracted.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 流被解压并提取。
- en: Each JSON line in the output is parsed.
  id: totrans-250
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输出中的每一行JSON都被解析。
- en: The structure of each Comprehend result entry is transformed to a more accessible
    data structure. The results are grouped by entity type (PERSON, LOCATION, and
    so on).
  id: totrans-251
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 每个Comprehend结果条目的结构被转换为一个更易于访问的数据结构。结果按实体类型（人物、地点等）分组。
- en: The seed and URL for the web page are derived from the path (key) of the S3
    object.
  id: totrans-252
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 网页的种子和URL是从S3对象的路径（键）中派生的。
- en: The transformed recognition results are sent to the frontier service.
  id: totrans-253
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 转换后的识别结果被发送到前沿服务。
- en: The Lambda function and associated internal functions (`handleEntityResultLines`,
    `storeEntityResults`) can be found in the extraction service’s `handler.js` module.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Lambda函数和相关内部函数（`handleEntityResultLines`，`storeEntityResults`）可以在提取服务的`handler.js`模块中找到。
- en: 9.9 Tying it all together
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.9 整合所有内容
- en: The last task in our conference site crawling and recognition application is
    to tie all of the functionality together so all sites are automatically analyzed
    as the crawler makes new page data available.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大会网站爬取和识别应用中的最后一个任务是整合所有功能，以便在爬虫提供新页面数据时自动分析所有网站。
- en: Just as we did in chapter 8, we are going to employ AWS Step Functions for this
    job.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8章中所做的那样，我们将为此作业使用AWS步骤函数。
- en: 9.9.1 Orchestrating entity extraction
  id: totrans-258
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.1 协调实体提取
- en: Figure 9.5 shows the control logic implemented in the step function and how
    it relates to the Lambda functions we have built.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5显示了步骤函数中实现的控制逻辑以及它与我们所构建的Lambda函数之间的关系。
- en: '![](../Images/CH09_F05_Elger.png)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F05_Elger.png)'
- en: Figure 9.5 The logical steps in the extraction service are orchestrated using
    an AWS step function. This ensures we have control over how many machine learning
    jobs are executed concurrently. It is also extensible to support advanced error
    recovery scenarios.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5 提取服务中的逻辑步骤是通过AWS步骤函数编排的。这确保了我们能够控制同时执行多少机器学习作业。它也可以扩展以支持高级错误恢复场景。
- en: Our conference data extraction process is a continuous loop that checks for
    newly crawled page text, and starts asynchronous entity recognition according
    to a configured limit of concurrent jobs. As we have seen, the result processing
    is a separate, asynchronous process, driven by the arrival of Comprehend results
    in the S3 bucket.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的大会数据提取过程是一个连续循环，检查新抓取的页面文本，并根据配置的并发作业限制启动异步实体识别。正如我们所见，结果处理是一个独立的、异步的过程，由S3桶中Comprehend结果的到达驱动。
- en: Figure 9.5 is a slight simplification of the step function. Step functions don’t
    actually support continuously executing events; the maximum execution time is
    one year. It is also mandatory to have a reachable `End` state in the function.
    In order to deal with this, we have added some additional logic to the step function.
    We will terminate execution of the function after 100 iterations. This is a safety
    measure to avoid forgetting about a long-running job, potentially resulting in
    surprising AWS costs! The following listing shows a condensed view of the step
    function YAML. The full version is contained in the extraction service’s `serverless.yml`.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.5是对步进函数的轻微简化。步进函数实际上不支持连续执行事件；最大执行时间为一年。在函数中必须有可到达的`End`状态。为了处理这种情况，我们在步进函数中添加了一些额外的逻辑。我们将终止函数的执行，在100次迭代后。这是一个安全措施，以避免忘记长时间运行的任务，可能造成令人惊讶的AWS费用！以下列表显示了步进函数YAML的压缩视图。完整版本包含在提取服务的`serverless.yml`文件中。
- en: Listing 9.14 Condensed entity extraction step-function configuration
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 列表9.14 简化实体提取步进函数配置
- en: '[PRE30]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ The start state initializes the iteration count to 100.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始状态将迭代计数初始化为100。
- en: ❷ The Iteration task is the start point for the loop. It invokes a Lambda function
    to decrement the count.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 迭代任务是循环的起点。它调用一个Lambda函数来减少计数。
- en: ❸ Check the number of iterations. When the loop has been performed 100 times,
    the state machine terminates.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 检查迭代次数。当循环执行了100次后，状态机终止。
- en: ❹ Now that we’ve run the checkActiveJobs function, we can compare the number
    of active jobs to the limit (10).
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 现在我们已经运行了checkActiveJobs函数，我们可以将活动作业的数量与限制（10）进行比较。
- en: ❺ Retrieve the batch of incoming texts. If there are no texts available, we
    wait. If there is at least one item, we start an entity recognition job.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 获取传入文本的批次。如果没有可用的文本，我们等待。如果至少有一个项目，我们开始一个实体识别作业。
- en: ❻ The wait period of 30 seconds is one variable that controls the throughput
    of data. We could also increase the maximum batch size and the number of concurrent
    Comprehend jobs.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 30秒的等待期是控制数据吞吐量的一个变量。我们还可以增加最大批次大小和并发Comprehend作业的数量。
- en: The simple iterator function is provided in the `handler.js` module contained
    in `extraction-service`.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 简单迭代函数提供在`extraction-service`中的`handler.js`模块。
- en: 9.9.2 End-to-end data extraction testing
  id: totrans-273
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.2 端到端数据提取测试
- en: We have completed building our final serverless AI application! You have covered
    a great deal of serverless architecture, learned many incredibly powerful AI services,
    and built some pretty amazing AI-enabled systems. Congratulations on reaching
    this milestone! It’s time to reward yourself by running our end-to-end conference
    data crawling and extraction application in full. Let’s kick off the web crawler
    with the URL of a conference website. Then, sit back and observe our automated
    extraction logic kick into action, as the details of conferences and speakers
    detected using AI start to appear.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经完成了最终的无服务器AI应用程序的构建！您已经覆盖了大量无服务器架构，学习了众多非常强大的AI服务，并构建了一些相当惊人的AI赋能系统。恭喜您达到这一里程碑！现在是时候通过运行我们的端到端会议数据爬取和提取应用程序来奖励自己了。让我们用会议网站的URL启动网络爬虫。然后，坐下来观察我们的自动化提取逻辑开始行动，随着使用AI检测到的会议和演讲者的详细信息开始出现。
- en: Just as we did at the end of chapter 8, we will start the web crawler with a
    seed URL. This time, we’ll pick a real conference website!
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第8章末所做的那样，我们将使用种子URL启动网络爬虫。这次，我们将选择一个真实的会议网站！
- en: '[PRE31]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'We will also start the entity extraction step function in the same way. This
    command requires no JSON input:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也将以相同的方式启动实体提取步进函数。此命令不需要JSON输入：
- en: '[PRE32]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: In both cases, you will have to replace the step function ARN with the correct
    values for your deployment. Recall from chapter 8 that the AWS CLI command required
    to retrieve these is
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 在这两种情况下，您都需要将步进函数的ARN替换为您的部署的正确值。回想一下第8章中，检索这些值所需的AWS CLI命令是
- en: '[PRE33]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Once the state machines are running, you can view them in the AWS Console Step
    Functions section, and monitor their progress by clicking on the states as transitions
    occur. Figure 9.6 shows the progress for the entity extraction state machine.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦状态机开始运行，您可以在AWS控制台步骤函数部分查看它们，通过点击状态来监控它们的进度，状态转换发生时。图9.6显示了实体提取状态机的进度。
- en: '![](../Images/CH09_F06_Elger.png)'
  id: totrans-282
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH09_F06_Elger.png)'
- en: Figure 9.6 Monitoring the progress of the entity extraction state machine
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.6 监控实体提取状态机的进度
- en: 9.9.3 Viewing conference data extraction results
  id: totrans-284
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.9.3 查看会议数据提取结果
- en: 'Building a front-end UI for the application is beyond the scope of this chapter,
    so a handy script for inspecting results is available in `scripts/get_extracted
    _entities.js`. By running this script, a DynamoDB query will be executed to find
    extracted entities for a given seed URL in the frontier table. These results are
    then aggregated to generate a CSV file summarizing the number of appearances,
    and an average score for each entity found using the machine learning process.
    The script is executed as follows:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 为应用程序构建前端UI超出了本章的范围，因此提供了一个方便的脚本`scripts/get_extracted_entities.js`来检查结果。通过运行此脚本，将在前沿表中执行DynamoDB查询，以找到给定种子URL提取的实体。然后，这些结果将汇总以生成一个CSV文件，总结每个实体的出现次数，以及使用机器学习过程找到的每个实体的平均分数。脚本执行方式如下：
- en: '[PRE34]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The script uses the AWS SDK, so AWS credentials must be configured in the shell.
    The script will print the name of the CSV file generated. For this example, it
    will be `https-dt-x-io.csv`. Open the CSV using an application such as Excel to
    inspect the outcome. Figure 9.7 shows our results for this conference website.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 脚本使用AWS SDK，因此必须在shell中配置AWS凭证。脚本将打印生成的CSV文件名称。对于此示例，它将是`https-dt-x-io.csv`。使用Excel等应用程序打开CSV文件以检查结果。图9.7显示了我们对该会议网站的结果。
- en: '![](../Images/CH09_F07_Elger.png)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![图9.7](../Images/CH09_F07_Elger.png)'
- en: Figure 9.7 Monitoring the progress of the entity extraction state machine
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 图9.7 监控实体提取状态机的进度
- en: We have filtered to show only PERSON entities in this case. The results include
    every person mentioned across all pages of the crawled site! This conference has
    some great speakers, including both authors of this book!
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已过滤仅显示此案例中的PERSON实体。结果包括爬取网站所有页面中提到的每个人！这次会议有一些杰出的演讲者，包括本书的作者！
- en: Feel free to try other conference sites to test the limits of our conference
    crawler and extractor. As always, bear in mind your usage costs with AWS. Comprehend
    costs can be expensive as volumes grow,[15](#pgfId-1105678) though a free tier
    is available. If in doubt, stop any running step function state machines, and
    remove the deployed application as soon as you are done testing. The `chapter8-9`
    code directory includes a `clean.sh` script to help you with this!
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 随意尝试其他会议网站以测试我们会议爬虫和提取器的限制。一如既往，请记住您的AWS使用成本。随着数据量的增长，Comprehend的成本可能会很高[15](#pgfId-1105678)，尽管有免费层可用。如有疑问，停止任何正在运行的步骤函数状态机，并在测试完成后尽快删除已部署的应用程序。`chapter8-9`代码目录中包含一个`clean.sh`脚本，可以帮助您完成此操作！
- en: 9.10 Wrapping up
  id: totrans-292
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 9.10 总结
- en: You have made it to the end of the last chapter. Congratulations on sticking
    with it and getting this far! In this book, we have built
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经到达了最后一章的结尾。恭喜您坚持下来并走这么远！在这本书中，我们构建了
- en: An image recognition system with object detection
  id: totrans-294
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个具有物体检测功能的图像识别系统
- en: A voice-driven task management app
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个语音驱动的任务管理应用
- en: A chatbot
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个聊天机器人
- en: An automated identity document scanner
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个自动化的身份文件扫描器
- en: An AI integration for e-commerce systems, to determine the sentiment behind
    customer product reviews, categorize them using a custom classifier, and forward
    them to the correct department
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于电子商务系统的AI集成，用于确定客户产品评论背后的情感，使用自定义分类器对它们进行分类，并将它们转发到正确的部门
- en: An event web site crawler that uses entity recognition to find information on
    conferences, including speaker profiles and event location
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个使用实体识别来查找会议信息的事件网站爬虫，包括演讲者资料和活动地点
- en: We have also covered a lot of ideas, tools, techniques, and architectural practices.
    Though Serverless and AI are fast-evolving topics, these foundational principles
    are designed to endure as you build amazing AI-enabled serverless systems.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还涵盖了许多想法、工具、技术和架构实践。尽管无服务器和AI是快速发展的主题，但这些基础原则旨在在您构建令人惊叹的AI赋能无服务器系统时保持其适用性。
- en: We are grateful that you have devoted your time to *AI as a Service*. To learn
    more, check out the fourTheorem blog ([https://fourtheorem.com/blog](https://fourtheorem.com/blog))
    where you will find more articles on AI, serverless architecture, and more.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 我们很感激您抽出时间关注*AI as a Service*。想了解更多信息，请查看fourTheorem博客([https://fourtheorem.com/blog](https://fourtheorem.com/blog))，在那里您可以找到更多关于AI、无服务器架构等方面的文章。
- en: 'For all our updates on these topics, follow us on Twitter and LinkedIn:'
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 关注我们Twitter和LinkedIn上的所有这些主题更新：
- en: Peter Elger--@pelger--linkedin.com/in/peterelger
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 彼得·埃尔格--@pelger--linkedin.com/in/peterelger
- en: Eóin Shanaghy - @eoins - linkedin.com/in/eoins
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 埃欧恩·沙纳基 - @eoins - linkedin.com/in/eoins
- en: Summary
  id: totrans-305
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Event-driven computing is achieved using S3 notifications and AWS Lambda.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用S3通知和AWS Lambda实现事件驱动计算。
- en: A dead-letter queue captures undelivered messages. It can be implemented with
    AWS Lambda and SQS to prevent data loss.
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 死信队列捕获未投递的消息。它可以与AWS Lambda和SQS一起实现，以防止数据丢失。
- en: Named entity recognition is the process of automatically identifying entities
    such as names, places, and dates in text.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 命名实体识别是自动识别文本中如名称、地点和日期等实体的过程。
- en: Amazon Comprehend has multiple modes of operation that can be selected depending
    on the quantity of text being analyzed.
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据要分析文本的数量，Amazon Comprehend有多种操作模式可供选择。
- en: Comprehend can be used to perform asynchronous batch entity recognition.
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comprehend可用于执行异步批量实体识别。
- en: Step functions can be used to control the concurrency and throughput of asynchronous
    AI analysis jobs.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 步进函数可用于控制异步人工智能分析作业的并发性和吞吐量。
- en: The machine learning analysis data produced by Comprehend can be extracted and
    transformed according to the application’s business requirements.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Comprehend产生的机器学习分析数据可以根据应用程序的业务需求进行提取和转换。
- en: Warning Please ensure that you fully remove all cloud resources deployed in
    this chapter in order to avoid additional charges!
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 警告 请确保您完全删除本章中部署的所有云资源，以避免额外收费！
- en: '* * *'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.Amazon Comprehend Guidelines and Limits, [http://mng.bz/2WAa](http://mng.bz/2WAa).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 1. Amazon Comprehend指南和限制，[http://mng.bz/2WAa](http://mng.bz/2WAa)。
- en: 2.Apollo 11, Wikipedia, reproduced under the Creative Commons Attribution-ShareAlike
    License, [https://en.wikipedia.org/wiki/Apollo_11](https://en.wikipedia.org/wiki/Apollo_11).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 2. 阿波罗11号，维基百科，根据Creative Commons Attribution-ShareAlike许可重新发布，[https://en.wikipedia.org/wiki/Apollo_11](https://en.wikipedia.org/wiki/Apollo_11)。
- en: 3.Serverless Framework, Using Existing Buckets, [http://mng.bz/1g7q](http://mng.bz/1g7q).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 3. 无服务器框架，使用现有存储桶，[http://mng.bz/1g7q](http://mng.bz/1g7q)。
- en: 4.Serverless Prune Plugin, [https://github.com/claygregory/serverless-prune-plugin](https://github.com/claygregory/serverless-prune-plugin).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 4. 无服务器修剪插件，[https://github.com/claygregory/serverless-prune-plugin](https://github.com/claygregory/serverless-prune-plugin)。
- en: 5.The Sub Function and CloudFormation variables, [http://mng.bz/P18R](http://mng.bz/P18R).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 5. 子函数和CloudFormation变量，[http://mng.bz/P18R](http://mng.bz/P18R)。
- en: 6.Serverless Framework Variables, [http://mng.bz/Jx8Z](http://mng.bz/Jx8Z).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 6. 无服务器框架变量，[http://mng.bz/Jx8Z](http://mng.bz/Jx8Z)。
- en: 7.Serverless Pseudo Parameters Plugin, [http://mng.bz/wpE5](http://mng.bz/wpE5).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 7. 无服务器伪参数插件，[http://mng.bz/wpE5](http://mng.bz/wpE5)。
- en: 8.Serverless Dotenv Plugin, [http://mng.bz/qNBx](http://mng.bz/qNBx).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 8. 无服务器Dotenv插件，[http://mng.bz/qNBx](http://mng.bz/qNBx)。
- en: 9.[CloudFormation AWS::S3::NotificationConfiguration, http://mng.bz/7GeQ.](http://mng.bz/7GeQ)
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 9. [CloudFormation AWS::S3::NotificationConfiguration, http://mng.bz/7GeQ.](http://mng.bz/7GeQ)
- en: 10.[AWS CloudFormation Templates Custom Resources, http://mng.bz/mNm8.](http://mng.bz/mNm8)
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 10. [AWS CloudFormation模板自定义资源，http://mng.bz/mNm8.](http://mng.bz/mNm8)
- en: 11.Lambda Asynchronous Invocation, [http://mng.bz/5pN7](http://mng.bz/5pN7).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 11. Lambda异步调用，[http://mng.bz/5pN7](http://mng.bz/5pN7)。
- en: 12.For details on creating a CloudWatch Alarm based on the SQS queue message
    count, see [http://mng.bz/6AjR](http://mng.bz/6AjR).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 12. 有关根据SQS队列消息计数创建CloudWatch警报的详细信息，请参阅[http://mng.bz/6AjR](http://mng.bz/6AjR)。
- en: 13.lambda-dlq-retry is available at [https://github.com/eoinsha/lambda-dlq-retry](https://github.com/eoinsha/lambda-dlq-retry).
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 13. lambda-dlq-retry可在[https://github.com/eoinsha/lambda-dlq-retry](https://github.com/eoinsha/lambda-dlq-retry)找到。
- en: 14.startEntitiesDetectionJob, AWS SDK for Javascript, [http://mng.bz/oRND](http://mng.bz/oRND).
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 14. startEntitiesDetectionJob，AWS JavaScript SDK，[http://mng.bz/oRND](http://mng.bz/oRND)。
- en: 15.Amazon Comprehend Costs, [https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/).
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 15. Amazon Comprehend费用，[https://aws.amazon.com/comprehend/pricing/](https://aws.amazon.com/comprehend/pricing/)。
