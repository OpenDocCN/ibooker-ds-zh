- en: 6 Wide convolutional neural networks
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 6 宽卷积神经网络
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Introducing the wide convolutional layer design pattern
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 介绍宽卷积层设计模式
- en: Understanding the advantages of wide versus deep layers
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解宽层与深层的优势
- en: Refactoring micro-architecture patterns to decrease computational complexity
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将微架构模式重构以降低计算复杂性
- en: Coding former SOTA wide convolutional models with the procedural design pattern
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用过程设计模式编码前SOTA宽卷积模型
- en: Up to now, we’ve focused on networks with deeper layers, block layers, and shortcuts
    in residual networks for image-related tasks such as classification, object localization,
    and image segmentation. Now we are going to take a look at networks with wide,
    rather than deep, convolutional layers. Starting in 2014 with Inception v1 (GoogLeNet)
    and 2015 with ResNeXt and Inception v2, neural network designs moved into wide
    layers, reducing the need for going deeper in layers. Essentially, a wide-layer
    design means having multiple convolutions in parallel and then concatenating their
    outputs. In contrast, deeper layers have sequential con-volutions and aggregate
    their outputs.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直专注于具有更深层、块层和残差网络中的捷径的网络，用于图像相关的任务，如分类、目标定位和图像分割。现在我们将探讨具有宽而不是深的卷积层的网络。从2014年的Inception
    v1（GoogLeNet）开始，到2015年的ResNeXt和Inception v2，神经网络设计转向了宽层，减少了深层层的需求。本质上，宽层设计意味着并行进行多个卷积，然后将它们的输出连接起来。相比之下，深层层具有顺序卷积并聚合它们的输出。
- en: So what led to the experimentation with wide-layer design patterns? At the time,
    researchers understood that for models to gain in accuracy, they needed more capacity.
    More specifically, they needed to have overcapacity for redundancy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，是什么导致了宽层设计模式的实验？当时，研究人员了解到，为了提高模型的准确性，它们需要更多的容量。更具体地说，它们需要具有冗余的过量容量。
- en: Early work with VGG ([https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf))
    and ResNet v1 ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    demonstrated that the capacity added by deeper layers did indeed increase accuracy.
    For example, AlexNet (2012) was the first convolutional neural network submission
    as well as the winner in the ILSVRC challenge, achieving a top-5 category error
    of 15.3%, which was 10% better than the 2011 winner. ZFNet built on AlexNet and
    became the 2013 winner with a top-5 error of 14.8%. Then, in 2014, VGG pushed
    deeper in layers, becoming first runner-up with a top-5 error rate of 7.3%, while
    ResNet went even deeper in 2015, and took first place with a top-5 error rate
    of 3.57%.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 早期与VGG ([https://arxiv.org/pdf/1409.1556.pdf](https://arxiv.org/pdf/1409.1556.pdf))
    和 ResNet v1 ([https://arxiv.org/abs/1512.03385](https://arxiv.org/abs/1512.03385))
    的合作工作表明，更深层的容量确实增加了准确性。例如，AlexNet（2012）是第一个卷积神经网络提交作品，也是ILSVRC挑战赛的获胜者，实现了5类错误率为15.3%，比2011年的获胜者提高了10%。ZFNet基于AlexNet，并在2013年成为获胜者，5类错误率为14.8%。然后，在2014年，VGG在层中进一步加深，以7.3%的5类错误率成为第一名，而ResNet在2015年甚至更深，以3.57%的5类错误率获得第一名。
- en: But these designs all hit roadblocks that limited the depth of the layers, and
    thus the ability to add more capacity. A major problem was vanishing and exploding
    gradients. As deeper layers were added to the models, the weights in these deeper
    layers were more likely to either get too small (vanishing) or too large (exploding).
    During training, the models would crash, much like a computer program crashes.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 但这些设计都遇到了瓶颈，限制了层的深度，从而限制了增加容量的能力。一个主要问题是梯度消失和梯度爆炸。当向模型添加更深层时，这些深层层的权重更有可能变得太小（消失）或太大（爆炸）。在训练过程中，模型会崩溃，就像计算机程序崩溃一样。
- en: The introduction of batch normalization in 2015 partially solved this problem.
    The authors of the seminal paper ([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167))
    hypothesized that redistributing the weights during training at each layer into
    a normal distribution would solve the problem of weights getting too small or
    too big in deeper layers. Other researchers validated the hypothesis, and batch
    normalization became a convention that continues today.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 2015年批量归一化的引入在某种程度上解决了这个问题。该开创性论文的作者 ([https://arxiv.org/abs/1502.03167](https://arxiv.org/abs/1502.03167))
    假设，在训练过程中将每层的权重重新分配到正态分布中，可以解决深层层中权重变得太小或太大的问题。其他研究人员验证了这一假设，批量归一化成为今天仍在继续的惯例。
- en: 'But another problem still remained with going deeper with layers: memorization.
    It turned out that deeper layers, which added the overcapacity for higher accuracy,
    were more likely to memorize the data than the shallower layers. That is, with
    overcapacity, the examples from the training data could snap into the nodes instead
    of generalizing from the training data. When you see increasing accuracy during
    training on the training data, but plummeting accuracy on examples not seen during
    training, we say the model is *overfitting*. And it was overfitting that indicated
    the models were memorizing rather than learning. Adding some noise in the deeper
    layers, like dropout and Gaussian noise, reduced memorization but did not eliminate
    it.'
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 但在加深层数的过程中，仍然存在一个问题：记忆化。结果证明，更深层的网络，通过增加过载来提高准确性，比浅层网络更容易记忆数据。也就是说，在过载的情况下，训练数据中的例子可能会直接映射到节点上，而不是从训练数据中泛化。当你在训练数据上看到准确率提高，但在训练过程中未看到的例子上准确率急剧下降时，我们说模型是*过拟合*的。而过拟合表明模型是在记忆而不是在学习。在深层添加一些噪声，如dropout和高斯噪声，可以减少记忆化，但并不能完全消除。
- en: But what if we added capacity by making convolutions in the shallower layers
    wider instead? That added capacity would reduce the need to go deeper in layers,
    where memorization occurred. Take, for example, ResNeXt, which was first runner-up
    in the 2016 ILSVRC competition. It replaced the sequential convolution in a residual
    block with a parallel convolution to add capacity in shallow layers.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 但如果我们通过在浅层中使卷积更宽来增加容量会怎样呢？这增加的容量将减少在发生记忆化的深层中进一步深化的需求。以ResNeXt为例，它在2016年ILSVRC竞赛中获得了第二名。它将残差块中的顺序卷积替换为并行卷积，以在浅层增加容量。
- en: This chapter covers the evolution in design of wide layers, starting with the
    principle of the naive inception module, which was resigned in Inception v1, and
    then the wide-layer block refinements in Inception v2 and v3\. We will also look
    at the parallel evolution in the design of wide layers in ResNeXt by Facebook
    AI Research and Wide Residual Network by Paris Tech.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了宽层设计演变的过程，从原始的inception模块原理开始，该原理在Inception v1中被重新设计，然后是Inception v2和v3中的宽层块改进。我们还将探讨Facebook
    AI Research在ResNeXt中以及巴黎理工学院在Wide Residual Network中的宽层设计并行演变。
- en: 6.1 Inception v1
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.1 Inception v1
- en: '*Inception v1* ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842)),
    which won the 2014 ILSVRC contest for object detection under its original name,
    GoogLeNet, introduced the *inception module*. This convolutional layer has parallel
    convolutions of different filter sizes, with the outputs from each convolution
    getting concatenated together. The idea here was that instead of trying to pick
    the best filter size for a layer, each layer has multiple filter sizes in parallel,
    and the layer “learns which size is the best.”'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: '*Inception v1* ([https://arxiv.org/abs/1409.4842](https://arxiv.org/abs/1409.4842))，在原名GoogLeNet下赢得了2014年ILSVRC竞赛中的目标检测奖项，引入了*inception模块*。这个卷积层具有不同滤波器尺寸的并行卷积，每个卷积的输出被连接在一起。这里的想法是，而不是试图为某一层选择最佳的滤波器尺寸，每一层都有多个并行滤波器尺寸，层“学习哪个尺寸是最佳的”。'
- en: For example, assume you’ve designed a model with multiple layers of convolutions,
    but you don’t know what filter size will give you the best result. Say you want
    to know which of three sizes—3 × 3, 5 × 5, or 7 × 7—would give you the best accuracy.
    To compare the accuracy, you would have to make three versions of the model, one
    for each filter size, and train each one. But say you now want to know the best
    filter size on each layer. Perhaps the first layer should be a 7 × 7, the next
    5 × 5, and the remainder 3 × 3 (or some other combination). Depending on the depth,
    this could mean hundreds or even thousands of possible combinations. Training
    each combination would be an enormous undertaking.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设你设计了一个具有多层卷积的模型，但你不知道哪个滤波器尺寸会给你最佳的结果。比如说，你想知道三个尺寸——3 × 3、5 × 5或7 × 7——哪个能给你最好的准确率。为了比较准确率，你将不得不制作三个版本的模型，每个滤波器尺寸一个，并训练每一个。但假设你现在想知道每个层的最佳滤波器尺寸。也许第一层应该是7
    × 7，下一层是5 × 5，其余的是3 × 3（或任何其他组合）。根据深度，这可能意味着数百甚至数千种可能的组合。训练每一种组合将是一项巨大的工作。
- en: Instead, the inception module design solved the problem by having every feature
    map pass through parallel convolutions of different filter sizes at each convolutional
    layer. This innovation allowed the model to learn the appropriate filter size
    with a single version and training of a model instance.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，inception 模块的设计通过在每个卷积层让每个特征图通过不同滤波器大小的并行卷积来解决该问题。这种创新使得模型能够通过单个模型实例的版本和训练来学习适当的滤波器大小。
- en: 6.1.1 Naive inception module
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.1 朴素 inception 模块
- en: Figure 6.1 shows the *naive inception module*, which demonstrates this approach.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 展示了 *朴素 inception 模块*，展示了这种方法。
- en: '![](Images/CH06_F01_Ferlitsch.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F01_Ferlitsch.png)'
- en: Figure 6.1 The naive inception module that was the theoretical base for experimenting
    with various variants of the inception module
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.1 作为实验各种 inception 模块变体的理论基础的朴素 inception 模块
- en: 'The native inception module is a convolutional block. Input to the block is
    passed through four branches: a pooling layer for dimensionality reduction, and
    1 × 1, 3 × 3, and 5 × 5 convolutional layers. The outputs from the pooling and
    the other convolutional layers are then concatenated together.'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 原生的 inception 模块是一个卷积块。块输入通过四个分支：一个用于降维的池化层，以及 1 × 1、3 × 3 和 5 × 5 的卷积层。然后，池化和其他卷积层的输出被连接在一起。
- en: 'The different filter sizes capture different levels of detail. The 1 × 1 convolution
    captures fine details of features, while the 5 × 5 captures more-abstract features.
    You can see this process in the example implementation of a naive inception module.
    The input from a previous block (layer) `x` is branched and passed through a max
    pooling layer, 1 × 1, 3 × 3, and 5 × 5 convolutions, which are then concatenated
    together:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的滤波器大小捕捉不同的细节级别。1 × 1 卷积捕捉特征的细微细节，而 5 × 5 捕捉更抽象的特征。您可以在朴素 inception 模块的示例实现中看到这个过程。来自先前块（层）`x`
    的输入分支并通过最大池化层、1 × 1、3 × 3 和 5 × 5 卷积，然后这些卷积被连接在一起：
- en: '[PRE0]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The inception branches, where x is the previous layer
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ inception 分支，其中 x 是前一层
- en: ❷ Concatenates the outputs from the four branches together
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将四个分支的输出连接在一起
- en: 'By setting `padding=''same''`, the height and width dimensions of the input
    are preserved. This allows concatenating the corresponding outputs from each branch
    together. For example, if the input was 256 feature maps of size 28 × 28, the
    dimensions at the branch layers would be as follows, where `?` is a placeholder
    for the batch size:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 通过设置 `padding='same'`，输入的高度和宽度维度得到保留。这允许将每个分支的相应输出连接在一起。例如，如果输入是 256 个大小为 28
    × 28 的特征图，分支层的维度如下，其中 `?` 是批处理大小的占位符：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'After the concatenation, the output would be as follows:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 连接后，输出结果如下：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Let’s now see how we got to these numbers. First, both the convolution and max
    pooling are nonstrided (meaning they have a stride of 1), so there is no downsampling
    of the feature maps. Second, since we set `padding='same'`, we won’t have any
    loss in pixel width/height for the edges. So the size of the feature maps outputted
    will be the same as the input, hence the 28 × 28 in the output.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看我们是如何得到这些数字的。首先，卷积和最大池化都是非步进（意味着它们有步长 1），所以特征图没有下采样。其次，由于我们设置了 `padding='same'`，我们不会在边缘丢失任何像素宽度/高度。因此，输出的特征图大小将与输入相同，因此在输出中是
    28 × 28。
- en: Now, let’s look at the max pooling branch, which outputs the same number of
    feature maps that come in, so that’s why we get 256\. The number of feature maps
    for the three convolutional branches is equal to the number of filters, so that
    would be 64, 96, 48\. Then if we add up all the feature maps from the branches
    for the concatenation, we get 464.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看最大池化分支，它输出的特征图数量与输入相同，因此我们得到 256。三个卷积分支的特征图数量等于滤波器的数量，因此会是 64、96、48。然后如果我们把所有分支的特征图加起来，我们得到
    464。
- en: 'A `summary``()` for a naive inception module shows 544,000 parameters to train:'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 对于一个朴素 inception 模块，`summary()` 显示有 544,000 个参数需要训练：
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'If the `padding=''same''` argument were to be left out (defaults to `padding=''valid''`),
    the shapes would be as follows instead:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果省略 `padding='same'` 参数（默认为 `padding='valid'`），形状将如下所示：
- en: '[PRE4]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Since the width and height dimensions do not match, if you tried to concatenate
    these layers, you would get the following error:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 由于宽度和高度维度不匹配，如果您尝试连接这些层，您将得到以下错误：
- en: '[PRE5]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: The naive inception module was a theoretical principle of the Inception v1 authors.
    When the authors went to the ILSVRC contest, they refactored the module by using
    a bottleneck residual block design, referred to as the Inception v1 module. This
    module maintained accuracy and was computationally less expensive to train.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 原始的Inception模块是Inception v1作者的原理。当作者参加ILSVRC比赛时，他们通过使用瓶颈残差块设计重构了模块，称为Inception
    v1模块。该模块保持了准确性，并且在训练时计算成本更低。
- en: 6.1.2 Inception v1 module
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.2 Inception v1模块
- en: Inception v1 introduced a further dimensionality reduction by adding a 1 × 1
    bottleneck convolution to the pooling, 3 × 3, and 5 × 5 branches. This dimension
    reduction reduced the overall computational complexity by nearly two-thirds.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1通过在池化、3 × 3和5 × 5分支中添加1 × 1瓶颈卷积，进一步实现了维度降低。这种维度降低将整体计算复杂度降低了近三分之二。
- en: At this point, you may be asking, why use a 1 × 1 convolution? How can a 1-pixel
    filter being scanned across each channel learn any feature? The use of a 1 × 1
    convolution acts like glue code. The 1 × 1 convolutions are either used to expand
    or reduce the number of channels in the output while preserving the channel size
    (shape). Expanding the number of channels is referred to as a *linear projection*;
    we discussed this in section 5.3.1.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，你可能想知道，为什么使用1 × 1卷积？一个1像素的过滤器如何能够学习到任何特征？1 × 1卷积的使用就像粘合代码。1 × 1卷积要么用于扩展或减少输出中的通道数，同时保持通道大小（形状）。扩展通道数被称为*线性投影*；我们在5.3.1节中讨论了这一点。
- en: Reducing, also known as a *bottleneck**,* is used to reduce the number of channels
    between the input to a block and the input to a convolutional layer. The linear
    projection and bottleneck convolutions are analogous to upsampling and down-sampling,
    except that we are not expanding or reducing the *size* of the channels, but the
    *number* of them. In this case, as we are reducing the number of channels, we
    could say we are compressing the data—and that’s why we use the term *dimensionality
    reduction*. We could do this using a static algorithm, or, as in this case, we
    *learn the optimal method* *to reduce the number of channels*. This is analogous
    to max pooling and feature pooling; in max pooling, we use a static algorithm
    to reduce the size of the channels, and with feature pooling, we *learn the optimal
    method to reduce the size*.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 减少，也称为*瓶颈*，用于减少块输入和卷积层输入之间的通道数。线性投影和瓶颈卷积类似于上采样和下采样，除了我们不是扩展或减少通道的*大小*，而是减少通道的*数量*。在这种情况下，因为我们正在减少通道数，所以我们可以说我们正在压缩数据——这就是我们使用术语*维度降低*的原因。我们可以使用静态算法来完成这项工作，或者，正如这个例子中，我们*学习最优方法*来减少通道数。这与最大池化和特征池化类似；在最大池化中，我们使用静态算法来减少通道的大小，而在特征池化中，我们*学习最优方法来减少大小*。
- en: Figure 6.2 shows the Inception v1 block (module). The 3 × 3 and 5 × 5 branches
    are preceded by a 1 × 1 bottleneck convolution, and the pooling branch is followed
    by a 1 × 1 bottleneck convolution.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2展示了Inception v1模块。3 × 3和5 × 5分支之前有一个1 × 1瓶颈卷积，而池化分支之后有一个1 × 1瓶颈卷积。
- en: '![](Images/CH06_F02_Ferlitsch.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F02_Ferlitsch.png)'
- en: Figure 6.2 The design of the Inception v1 block (module), which was used in
    the 2014 ILSVRC submission
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.2 Inception v1模块（模块）的设计，该模块被用于2014年ILSVRC提交
- en: 'And here is an example of an Inception v1 block (module), where the pooling,
    3 × 3, and 5 × 5 convolution branches have an additional 1 × 1 bottleneck convolution:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个Inception v1模块（模块）的示例，其中池化、3 × 3和5 × 5卷积分支增加了额外的1 × 1瓶颈卷积：
- en: '[PRE6]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ The inception branches, where x is the previous layer
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Inception分支，其中x是前一层
- en: ❷ Concatenates the feature maps from the branches together
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将分支的特征图连接在一起
- en: 'A `summary()` for these layers shows 198,000 parameters to train, in contrast
    to 544,000 parameters in the native inception module with the use of the bottleneck
    convolution for dimensionality reduction. The model’s designers were able to maintain
    the same accuracy level as the naive inception module, but with faster training
    and improved performance on prediction (inference):'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些层的`summary()`显示需要训练198,000个参数，相比之下，使用瓶颈卷积进行维度降低的原生Inception模块有544,000个参数。模型的设计者能够保持与原始Inception模块相同的准确度水平，但训练速度更快，预测（推理）性能得到改善：
- en: '[PRE7]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'As you can see in figure 6.3, the Inception v1 architecture, when retrofitted
    into the procedural design pattern, consists of four components: stem, learner,
    classifier, and auxiliary classifier. Overall, the macro-architecture represented
    by the procedural design pattern is the same as previous SOTA models I’ve shown,
    except for the addition of the auxiliary classifiers. As the diagram shows, the
    learner component consists of five convolutional groups, and each group has a
    different number of convolutional blocks. The second and fourth convolutional
    groups are the only groups with a single convolutional block, and are the groups
    connected with an auxiliary classifier.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 正如您在图6.3中看到的，当Inception v1架构被重构为程序设计模式时，它由四个组件组成：主干、学习器、分类器和辅助分类器。总体而言，程序设计模式表示的宏观架构与之前我展示的SOTA模型相同，只是增加了辅助分类器。如图所示，学习器组件由五个卷积组组成，每个组有不同的卷积块数量。第二和第四个卷积组是唯一只有一个卷积块的组，并且是与辅助分类器相连的组。
- en: '![](Images/CH06_F03_Ferlitsch.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F03_Ferlitsch.png)'
- en: Figure 6.3 In the Inception v1 macro-architecture, the auxiliary classifiers
    were added after the second and fourth inception groups.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.3 在Inception v1宏观架构中，辅助分类器被添加到第二和第四个Inception组之后。
- en: The fact that Inception v1 won first place in the 2014 ILSVRC challenge for
    object detection demonstrated the utility of exploring design patterns for wide
    layers in conjunction with deeper layers.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1在2014年ILSVRC挑战中对象检测项目中获得第一名的成绩，证明了在深度层的同时探索宽层设计模式的有用性。
- en: Note that I’ve been referring to modules as blocks, which is the term used in
    this design pattern. Next, we will explore each component in more detail.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我一直在将模块称为块，这是在此设计模式中使用的术语。接下来，我们将更详细地探讨每个组件。
- en: 6.1.3 Stem
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.3 主干
- en: The stem is the entry point into the neural network. The inputs (images) are
    processed by a sequential (deep) set of convolutions and max pooling, much like
    a conventional ConvNet.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 主干是进入神经网络的人口。输入（图像）通过一系列（深度）卷积和最大池化进行处理，就像传统的ConvNet一样。
- en: Let’s dive a little deeper into the stem so you can see how its construction
    differed from conventional SOTA stems of the time (figure 6.4). Inception used
    a very coarse 7 × 7 initial filter followed by a very aggressive feature map reduction
    consisting of two strided convolutions and two max poolings. On the other hand,
    it progressively increased the number of feature maps from 64 to 192\. Inception
    was coded without the benefit of being able to move the filter past the edge in
    a convolutional. So to preserve the reducing height and width by half during the
    reduction, zero padding was added.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨一下主干部分，以便您可以看到其结构与当时传统SOTA主干的不同（见图6.4）。Inception使用了一个非常粗略的7 × 7初始滤波器，随后是一个非常激进的特性图减少，包括两个步进卷积和两个最大池化。另一方面，它逐渐将特性图的数量从64增加到192。Inception在卷积中无法移动滤波器到边缘的情况下进行编码。因此，为了在减少过程中保持高度和宽度减半，添加了零填充。
- en: '![](Images/CH06_F04_Ferlitsch.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F04_Ferlitsch.png)'
- en: Figure 6.4 The Inception v1 stem consists of a coarse 7 × 7 filter followed
    by a detailed 3 × 3 filter, along with dimensionality reduction after each convolution
    with max pooling.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.4 Inception v1主干由一个粗略的7 × 7滤波器和一个详细的3 × 3滤波器组成，每个卷积后都进行最大池化以降低维度。
- en: 6.1.4 Learner
  id: totrans-63
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.4 学习器
- en: The learner is a set of nine inception blocks in five groups, shown previously
    in figure 6.3 and again in figure 6.5\. The wider groups in the diagram represent
    a group of two or three inception blocks, and the thinner ones are a single inception
    block, for a total of nine inception blocks. The fourth and seventh blocks (single
    blocks) are separated out to highlight they have an additional component, the
    auxiliary classifier.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 学习器是由五个组中的九个Inception块组成的集合，如图6.3和图6.5所示。图中较宽的组代表两个或三个Inception块的一组，而较细的组是一个单独的Inception块，总共九个Inception块。第四和第七个块（单个块）被单独列出，以突出它们有一个额外的组件，即辅助分类器。
- en: '![](Images/CH06_F05_Ferlitsch.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F05_Ferlitsch.png)'
- en: Figure 6.5 The group configuration and number of blocks in the Inception v1
    learner component.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.5 Inception v1学习组件的组配置和块数量。
- en: 6.1.5 Auxiliary classifiers
  id: totrans-67
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.5 辅助分类器
- en: The auxiliary classifiers are a set of two classifiers, acting as auxiliaries
    (aids) in training the neural network. Each auxiliary classifier consists of a
    convolutional layer, a dense layer, and a final softmax activation function (figure
    6.6). Softmax (as you may already know) is an equation from statistics that takes
    as input a set of independent probabilities (from 0 to 1) and squashes the set
    such that all the probabilities add up to 1\. In a final dense layer with one
    node per class, each node makes an independent prediction (from 0 to 1), and by
    passing the values through a softmax function, the sum of the predicted probabilities
    for all classes will add up to 1.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助分类器是一组两个分类器，作为训练神经网络的辅助工具。每个辅助分类器由一个卷积层、一个密集层和一个最终的softmax激活函数（图6.6）组成。softmax（你可能已经知道）是统计学中的一个方程，它接受一组独立的概率（从0到1）作为输入，并将该组压缩，使得所有概率加起来等于1。在一个每个类别有一个节点的最终密集层中，每个节点都会做出独立的预测（从0到1），通过将值传递给softmax函数，所有类别的预测概率之和将等于1。
- en: '![](Images/CH06_F06_Ferlitsch.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F06_Ferlitsch.png)'
- en: Figure 6.6 Inception v1/v2 auxiliary classifier group
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6 Inception v1/v2辅助分类器组
- en: The Inception v1 architecture introduced the concept of an auxiliary classifier.
    The principle here is that as a neural network gets deeper in layers (as the front
    layers get further away from the final classifier), the front layers are more
    exposed to a vanishing gradient and increased time (increased number of epochs)
    to train the weights in the frontmost layers. Figure 6.7 illustrates this process.
    As updates to weights are propagated from the back layers, the updates tend to
    get progressively smaller.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1架构引入了辅助分类器的概念。这里的原理是，随着神经网络层数的加深（随着前层离最终分类器越来越远），前层更容易受到梯度消失和增加的训练时间（增加的epoch数量）的影响，以训练最前层的权重。图6.7展示了这一过程。当权重更新从后层传播过来时，更新往往会逐渐变小。
- en: '![](Images/CH06_F07_Ferlitsch.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F07_Ferlitsch.png)'
- en: Figure 6.7 Weight updates get progressively smaller through layers during backward
    propagation.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.7在反向传播过程中，通过层传递的权重更新逐渐变小。
- en: As theorized by the Inception v1 authors, this can lead to two problems they
    wanted to fix. First, if the updates get too small, the multiplication operation
    may result in a number too small to be represented by the floating-point computer
    hardware (this is referred to as the *vanishing gradient*).
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 如Inception v1的作者们所理论化的，这可能导致两个他们想要解决的问题。首先，如果更新变得太小，乘法运算可能会导致一个太小以至于无法由浮点计算机硬件表示的数字（这被称为*梯度消失*）。
- en: The other problem is that if the updates to the early layers are much smaller
    than the later layers, they will take longer to converge and increase training
    time. Additionally, if the later layers converge early and early layers converge
    later, then the later layers may start memorizing the data while the early layers
    are still learning to generalize.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个问题是在早期层的更新远小于后期层时，它们将需要更长的时间来收敛并增加训练时间。此外，如果后期层提前收敛而早期层较晚收敛，那么后期层可能会开始记住数据，而早期层仍在学习泛化。
- en: The Inception v1 authors’ theory is that at the semi-deep layers, there is some
    information to predict, or classify, the input, albeit with less accuracy than
    the final classifier. These earlier classifiers are closer to the front layers
    and thus less prone to a vanishing gradient. During training, the cost function
    becomes a combination of the losses of the auxiliary classifiers and the final
    classifier. In other words, the authors thought that combining the losses from
    the auxiliary and final classifier would result in more uniform updates to the
    weights across all layers, alleviating the vanishing gradient and decreasing training
    time.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v1的作者们认为，在半深层，有一些信息可以用来预测或分类输入，尽管其准确性不如最终分类器。这些早期的分类器更靠近前层，因此不太容易受到梯度消失的影响。在训练过程中，损失函数成为辅助分类器和最终分类器损失的组合。换句话说，作者们认为，将辅助和最终分类器的损失组合起来，将导致所有层权重的更新更加均匀，从而缓解梯度消失并减少训练时间。
- en: The auxiliary classifier depicted in figure 6.6 is used in both Inception v1
    and Inception v2\. Subsequent to Inception, the practice of using an auxiliary
    classifier was not pursued for two reasons. First, as models went even deeper
    in layers, the issue of vanishing (and exploding) gradients became more pronounced
    in the deeper layers than the front layers, so the theory did not pan out in deeper
    neural networks. Second, the introduction of batch normalization in 2015 uniformly
    addressed this issue across all layers.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.6中展示的辅助分类器在Inception v1和Inception v2中都被使用。在Inception之后，由于两个原因，没有继续使用辅助分类器的做法。首先，随着模型层级的加深，梯度消失（和爆炸）的问题在深层比在前面层更为明显，因此该理论在深层神经网络中并未得到验证。其次，2015年引入的批量归一化在所有层面上统一解决了这个问题。
- en: When compared to the VGG design, Inception v1 eliminated adding additional dense
    layers in the classifier (in both the auxiliary and final classifier), in contrast
    to VGG, which had two additional 4096-node dense layers. The authors theorized
    that the additional dense nodes were unnecessary for training the final dense
    layer for classification. This substantially reduced the computational complexity
    without reduction in accuracy on the classifier. In subsequent SOTA models, researchers
    found they could eliminate all preceding dense layers between the bottleneck and
    final classifier layer, without reduction in accuracy.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 与VGG设计相比，Inception v1在分类器中（无论是辅助还是最终分类器）消除了添加额外的密集层，而VGG则有两个额外的4096节点密集层。作者理论认为，额外的密集节点对于训练最终的分类密集层是不必要的。这大大降低了计算复杂度，而没有在分类器上降低准确性。在随后的SOTA模型中，研究人员发现他们可以消除瓶颈和最终分类器层之间的所有先前密集层，而不会降低准确性。
- en: Inception was one of the last SOTA models that used a dropout layer in the classifiers
    for regularization to reduce overfitting. After the subsequent introduction of
    batch normalization, researchers observed that the normalization added a small
    amount of regularization on a per layer basis, and was more effective at generalization
    than dropout. Eventually, researchers would introduce explicit layer-by-layer
    regularization known as *weight regularization*, further improving regularization.
    Thus, subsequently the use of dropout was phased out.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: Inception是最后一批使用分类器中的dropout层进行正则化以减少过拟合的SOTA模型之一。在随后的批量归一化引入后，研究人员观察到归一化在每个层上添加了少量的正则化，并且比dropout更有效地促进了泛化。最终，研究人员引入了称为*权重正则化*的显式逐层正则化，进一步提高了正则化效果。因此，随后dropout的使用逐渐被淘汰。
- en: 6.1.6 Classifier
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1.6 分类器
- en: Figure 6.8 depicts the final (non-auxiliary) classifier in both training the
    neural network and in prediction. Note that for prediction, the auxiliary classifiers
    are removed. The classifier implements a global average pooling step as two layers;
    the first layer (`AveragePooling2D`) does an average pooling of each feature map
    into 1 × 1 feature maps, which is then followed by a flatten layer to flatten
    into a 1D vector. Prior to the `Dense` layer for classification, a `Dropout` layer
    was used for regularization—which was a common practice of the time.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8展示了在训练神经网络和预测中使用的最终（非辅助）分类器。请注意，在预测时，辅助分类器被移除。该分类器通过两层实现全局平均池化步骤；第一层（`AveragePooling2D`）对每个特征图进行平均池化，形成1
    × 1的特征图，随后通过一个展平层将其展平成一个一维向量。在分类的`Dense`层之前，使用了一个`Dropout`层进行正则化——这是当时的一种常见做法。
- en: '![](Images/CH06_F08_Ferlitsch.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F08_Ferlitsch.png)'
- en: Figure 6.8 In the Inception final classifier, the pooling into 1 × 1 pixel maps
    and flattening is done as two steps.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.8 在Inception最终分类器中，池化到1 × 1像素图和展平是作为两个步骤完成的。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for Inception v1 is on GitHub ([http://mng.bz/oGnd](https://shortener.manning.com/oGnd)).
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式为Inception v1编写的完整代码可以在GitHub上找到([http://mng.bz/oGnd](https://shortener.manning.com/oGnd))。
- en: Next, let’s see how Inception v2 introduced the concept of factorization of
    computationally expensive convolutions.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们看看Inception v2如何引入了计算密集型卷积分解的概念。
- en: '6.2 Inception v2: Factoring convolutions'
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.2 Inception v2：分解卷积
- en: The larger a filter (kernel) size is in a convolution, the more computationally
    expensive it is. The paper that presented the *Inception v2* architecture calculated
    that the 5 × 5 convolution in the inception module was 2.78 times more computationally
    expensive than a 3 × 3\. In other words, a 5 × 5 filter requires almost three
    times more matmul operations, requiring more time for training and prediction.
    The authors’ goal was to find a way to replace 5 × 5 filters with a 3 × 3 filter
    to reduce training/prediction time without sacrificing the model’s accuracy.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在卷积中，滤波器（核）的大小越大，计算成本就越高。提出 *Inception v2* 架构的论文计算出，inception 模块中的 5 × 5 卷积比
    3 × 3 卷积计算成本高 2.78 倍。换句话说，5 × 5 滤波器需要几乎三倍的 matmul 操作，需要更多的时间进行训练和预测。作者的目标是找到一种方法，用
    3 × 3 滤波器替换 5 × 5 滤波器，以减少训练/预测时间，同时不牺牲模型的准确率。
- en: Inception v2 introduced *factorization* for the more expensive convolutions
    in an inception module to reduce computational complexity, and reduce information
    loss from representational bottlenecks. Figure 6.9 depicts representational loss.
    In this depiction, we show a 5 × 5 filter that covers an area of 25 pixels. During
    each slide of the filter, the area of 25 pixels is replaced (represented by) a
    single pixel. This single pixel in the corresponding feature map in a subsequent
    pooling operation will be reduced by one-half. The representational loss is the
    ratio of compression, in this case 50 (25 to 0.5). For a smaller 3 × 3 filter,
    the representational loss is 18 (9 to 0.5).
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 引入了**分解**来降低 inception 模块中更昂贵的卷积的计算复杂度，并减少从表示瓶颈中损失的信息。图 6.9 描述了表示损失。在这个描述中，我们展示了一个覆盖
    25 像素区域的 5 × 5 滤波器。在每次滤波器滑动时，25 像素区域被（表示为）一个单独的像素所替代。在后续池化操作中，对应特征图中的这个单独像素将被减半。表示损失是压缩比，在这种情况下为
    50（25 到 0.5）。对于较小的 3 × 3 滤波器，表示损失为 18（9 到 0.5）。
- en: '![](Images/CH06_F09_Ferlitsch.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F09_Ferlitsch.png)'
- en: Figure 6.9 Representational loss between filter and outputted pixel value after
    subsequent pooling
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.9 滤波器与后续池化后输出的像素值之间的表示损失
- en: In the Inception v2 module, the 5 × 5 filter is replaced by a stack of two 3
    × 3 filters, which results in a reduction of computational complexity of the replaced
    5 × 5 filter by 33%.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Inception v2 模块中，5 × 5 滤波器被两个 3 × 3 滤波器的堆栈所取代，这导致替换的 5 × 5 滤波器的计算复杂度降低了 33%。
- en: Additionally, representational bottleneck loss occurs when large differences
    in filter sizes exist. By replacing the 5 × 5 with two 3 × 3 filters, all the
    non-bottleneck filters are now of the same size, and the overall accuracy of the
    Inception v2 architecture increases over Inception v1.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，当存在滤波器大小的大差异时，会发生表示瓶颈损失。通过用两个 3 × 3 滤波器替换 5 × 5 滤波器，所有非瓶颈滤波器现在具有相同的大小，并且
    Inception v2 架构的整体准确率超过 Inception v1。
- en: 'Figure 6.10 illustrates the Inception v2 block: the 5 × 5 convolution in v1
    is replaced by two 3 × 3 convolutions.'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 展示了 Inception v2 块：v1 中的 5 × 5 卷积被两个 3 × 3 卷积所取代。
- en: '![](Images/CH06_F10_Ferlitsch.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F10_Ferlitsch.png)'
- en: Figure 6.10 In the Inception v2 block, the 5 × 5 convolution is replaced by
    a more-efficient stack of two 3 × 3 convolutions.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.10 在 Inception v2 块中，5 × 5 卷积被更高效的两个 3 × 3 卷积堆栈所取代。
- en: Inception v2 also added using a post-activation batch normalization (Conv-BN-ReLU)
    for each convolutional layer. Since batch normalization was not introduced until
    2015, the 2014 Inception v1 did not have the benefit of using this technique.
    Figure 6.11 shows the difference between the previous convolution without a batch
    normalization (Conv-ReLU) and a post-activation batch normalization.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v2 还增加了在每个卷积层使用后激活批量归一化（Conv-BN-ReLU）。由于批量归一化直到 2015 年才被引入，因此 2014
    年的 Inception v1 没有使用这种技术的优势。图 6.11 展示了添加批量归一化（Conv-ReLU）前后的前一个卷积层和激活之间的差异。
- en: '![](Images/CH06_F11_Ferlitsch.png)'
  id: totrans-97
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F11_Ferlitsch.png)'
- en: Figure 6.11 A comparison between convolutional layer and activation before and
    after the convention of adding batch normalization
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.11 添加批量归一化前后卷积层和激活的比较
- en: 'The following is a code example of an Inception v2 module, which differs from
    v1 as follows:'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个 Inception v2 模块的代码示例，它与 v1 的不同之处如下：
- en: Each convolution layer is followed by a batch normalization.
  id: totrans-100
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个卷积层后面都跟着一个批量归一化。
- en: The v1 5 × 5 convolution is replaced by two 3 × 3 convolutions, and the factorization
    of the more-expensive 5 × 5 to less-expensive pair of 3 × 3 convolutions reduced
    computational complexity and information loss from representational bottlenecks.
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: v1 的 5 × 5 卷积被两个 3 × 3 卷积所取代，将更昂贵的 5 × 5 分解为更便宜的 3 × 3 卷积对，从而降低了计算复杂性和信息损失，减少了表示瓶颈。
- en: '[PRE8]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Uses post-activation batch normalization
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 使用后激活批量归一化
- en: ❷ The Inception branches, where x is the previous layer
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ Inception 分支，其中 x 是前一层
- en: ❸ Concatenates the feature maps from the branches together
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将分支的特征图连接在一起
- en: A `summary()` for these layers shows 169,000 parameters to train, when compared
    to 198,000 for the inception v1 module.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 与 Inception v1 模块的 198,000 个参数相比，这些层的 `summary()` 显示需要训练 169,000 个参数。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for Inception v2 is located on GitHub ([http://mng.bz/oGnd](https://shortener.manning.com/oGnd)).
    Next, we will describe how the Inception architecture was redesigned in Inception
    v3.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Idiomatic 程序重用设计模式对 Inception v2 的完整代码实现位于 GitHub 上 ([http://mng.bz/oGnd](https://shortener.manning.com/oGnd))。接下来，我们将描述
    Inception 架构在 Inception v3 中的重设计过程。
- en: '6.3 Inception v3: Architecture redesign'
  id: totrans-108
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.3 Inception v3：架构重设计
- en: The *Inception v3* introduced a new design to the macro-architecture, as well
    as redesigning the stem group and using only a single auxiliary classifier. Christian
    Szegedy et al. referred to this arrangement in the title of their paper, “Rethinking
    the Inception Architecture” ([https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567)).
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*Inception v3* 引入了新的宏观架构设计，同时重新设计了主干组，并仅使用一个辅助分类器。Christian Szegedy 等人在其论文标题中提到了这种安排，“重新思考
    Inception 架构” ([https://arxiv.org/abs/1512.00567](https://arxiv.org/abs/1512.00567))。'
- en: The authors noted that substantial strides had been made in recent years in
    improving accuracy and lowering parameter size by going both deeper and wider.
    AlexNet had 60 million parameters, and VGG had three times that many, whereas
    Inception v1 had only 5 million. The authors emphasized the need for efficient
    architectures that could move models into real-world use and provide further efficiency
    in parameters while achieving higher accuracy gains.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 作者指出，近年来在提高准确率和降低参数大小方面取得了显著进展，这既涉及更深层次的架构，也涉及更宽的架构。AlexNet 有 6000 万个参数，VGG
    的参数是其三倍，而 Inception v1 的参数只有 500 万。作者强调了需要高效架构，这些架构可以将模型应用于现实世界，并在参数数量减少的同时实现更高的准确率提升。
- en: In their opinion, the Inception v1/v2 architecture was too complex for this
    purpose. For example, doubling the size of the filter banks for more capacity
    would increase the number of parameters by four times. The motivation for the
    redesign was to simplify the architecture, while maintaining computational gains
    when scaled and improving on the accuracy of existing SOTA models of the time.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们看来，Inception v1/v2 架构对于这个目的来说过于复杂。例如，为了增加容量而将滤波器组的尺寸加倍，会使参数数量增加四倍。重设计的动机是简化架构，同时在扩展时保持计算收益，并提高当时
    SOTA 模型的准确性。
- en: 'In the redesign, the convolutional blocks were refactored so the architecture
    could be scaled up efficiently. Figure 6.12 shows the macro-architecture: the
    learner component is composed of three groups (A, B, and C) for feature learning,
    as well as two grid reduction groups for feature map reduction. In addition, the
    number of auxiliary classifiers is reduced from two to one.'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 在重设计中，卷积块被重构，以便架构可以高效地扩展。图 6.12 显示了宏观架构：学习组件由三个组（A、B 和 C）组成，用于特征学习，以及两个用于特征图减少的网格减少组。此外，辅助分类器的数量从两个减少到一个。
- en: '![](Images/CH06_F12_Ferlitsch.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F12_Ferlitsch.png)'
- en: Figure 6.12 The redesigned Inception v3 macro-architecture simplified the Inception
    v1 and v2 architecture.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 图 6.12 重设计的 Inception v3 宏观架构简化了 Inception v1 和 v2 架构。
- en: Let’s spend the next few sections looking more closely at these redesigned components.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们接下来更详细地看看这些重设计的组件。
- en: 6.3.1 Inception groups and blocks
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.1 Inception 组和块
- en: 'Four design principles shaped the redesign of the Inception architecture:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 四个设计原则塑造了 Inception 架构的重设计。
- en: Avoid representational loss.
  id: totrans-118
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 避免表示损失。
- en: Higher-dimensional representations are easier to process locally within a network.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 高维表示更容易在网络中局部处理。
- en: Spatial aggregation can be done over lower-dimensional embeddings without much
    loss in representational power.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在低维嵌入上执行空间聚合不会在表示能力上造成很大损失。
- en: Balance the width and depth of the network.
  id: totrans-121
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 平衡网络的宽度和深度。
- en: The micro-architecture of the Inception reflected the first design principle,
    of reducing information loss from representational bottlenecks, by achieving a
    more gradual reduction in feature map size. It also addressed principle 4, balancing
    the width and depth of the convolutional layers. The authors observed that the
    optimal improvement occurred when the increase in width and depth was done in
    parallel, and the computational budget is balanced between the two. Consequently,
    the model increases both width and depth in order to contribute to higher-quality
    networks.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: Inception的微架构反映了第一个设计原则，即通过实现特征图大小的更渐进式减少来减少从表示瓶颈中损失的信息。它还解决了原则4，即平衡卷积层的宽度和深度。作者观察到，当宽度和深度的增加是并行进行，并且计算预算在这两者之间平衡时，最优的改进发生了。因此，该模型在宽度和深度上同时增加，以贡献于更高品质的网络。
- en: Let’s zoom in even more now, and look at the redesigns for groups A, B, and
    C. The blocks in group A remain the same as in the earlier versions, while group
    B and C differ. The output feature map sizes for groups A, B, and C are 35 × 35,
    17 × 17, and 8 × 8, respectively. Note the gradual reduction in feature map sizes,
    as each group reduces by half the *H* × *W*. This gradual reduction across the
    three groups reflects design principle 1, reducing representational loss, as the
    network goes deeper.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们进一步放大，看看A、B和C组的重新设计。A组的块与早期版本相同，而B组和C组有所不同。A、B和C组的输出特征图大小分别为35 × 35、17
    × 17和8 × 8。注意特征图大小的逐渐减少，因为每个组将*H* × *W*减半。这种在三组中的逐渐减少反映了设计原则1，即减少表示损失，因为网络变得更深。
- en: Figures 6.13 and 6.14 show the blocks in groups B and C, respectively. In these
    two groups, some of the *N* × *N* convolutions are factored into spatial separable
    convolutions of *N* × 1 and 1 × N. This adjustment reflected design principle
    3, which states that spatial separable convolutions, when done on lower dimensionality
    of the feature maps, do not lose representational power.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13和6.14分别显示了B组和C组的块。在这两个组中，一些*N* × *N*卷积被分解为*N* × 1和1 × N的空间可分离卷积。这种调整反映了设计原则3，即当在特征图的较低维度上执行时，空间可分离卷积不会丢失表示能力。
- en: '![](Images/CH06_F13_Ferlitsch.png)'
  id: totrans-125
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F13_Ferlitsch.png)'
- en: Figure 6.13 Inception v3 block 17 × 17 (group B) using a spatial separable convolution
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.13 使用空间可分离卷积的Inception v3块17 × 17（组B）
- en: '![](Images/CH06_F14_Ferlitsch.png)'
  id: totrans-127
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F14_Ferlitsch.png)'
- en: Figure 6.14 Inception v3 block 8 × 8 (group C) using a parallel spatial separable
    convolution
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.14 使用并行空间可分离卷积的Inception v3块8 × 8（组C）
- en: In the earlier versions of Inception, the feature maps between convolutional
    groups were pooled, in order to reduce dimensionality, and doubled in number,
    which, the researchers argued, caused representational loss (design principle
    1). They proposed that the feature map reduction stage between groups could be
    done instead with parallel convolutions and pooling, as depicted in figure 6.15\.
    However, after Inception v3, this approach for eliminating representational loss
    during reduction was not further pursued.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在Inception的早期版本中，为了减少维度，卷积组之间的特征图进行了池化，数量加倍，研究人员认为这导致了表示损失（设计原则1）。他们提出，组之间的特征图减少阶段可以用并行卷积和池化来完成，如图6.15所示。然而，在Inception
    v3之后，这种在减少过程中消除表示损失的方法没有进一步追求。
- en: '![](Images/CH06_F15_Ferlitsch.png)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F15_Ferlitsch.png)'
- en: Figure 6.15 Using parallel pooling of feature maps to reduce representational
    loss
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.15 使用特征图并行池化以减少表示损失
- en: The parallel pooling after groups A and B is shown in figures 6.16 and 6.17,
    respectively. Termed *grid reduction*, these pooling blocks reduce the number
    of feature maps (or channels) from the output of the previous group to match the
    input of the next group. Thus, grid reduction block A reduces from 35 × 35 to
    17 × 17, and grid reduction block B reduces from 17 × 17 to 8 × 8 (satisfying
    design principle 1).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: A组和B组之后的并行池化分别显示在图6.16和6.17中。被称为*网格减少*的这些池化块将前一个组的输出特征图（或通道）的数量减少，以匹配下一个组的输入。因此，网格减少块A从35
    × 35减少到17 × 17，网格减少块B从17 × 17减少到8 × 8（满足设计原则1）。
- en: '![](Images/CH06_F16_Ferlitsch.png)'
  id: totrans-133
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F16_Ferlitsch.png)'
- en: Figure 6.16 Inception v3 grid reduction block 17 × 17 (group A)
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.16 Inception v3网格减少块17 × 17（组A）
- en: Additionally, reflecting design principle 3, the 7 × 7 convolution, along with
    some of the 3 × 3 convolutions in groups B and C, and grid reduction in group
    B are replaced by a spatial convolution of (7 × 1, 1 × 7) and (3 × 1, 1 × 3),
    respectively.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，反映设计原则3，7 × 7卷积，以及B组和C组中的一些3 × 3卷积，以及B组中的网格减少，分别被(7 × 1, 1 × 7)和(3 × 1, 1
    × 3)的空间卷积所取代。
- en: '![](Images/CH06_F17_Ferlitsch.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F17_Ferlitsch.png)'
- en: Figure 6.17 Inception v3 grid reduction block 8 × 8 (group B)
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.17 Inception v3网格减少块8 × 8（组B）
- en: Now let’s compare a normal convolution from Inception v1 and v2, which we have
    been calling simply a normal convolution, to a spatially separable convolution
    from v3.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来比较Inception v1和v2中的正常卷积，我们将其简单地称为正常卷积，与v3中的空间可分离卷积。
- en: 6.3.2 Normal convolution
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.2 正常卷积
- en: In a *normal convolution*, the kernel (for example, 3 × 3) is applied across
    the height (*H*), width (*W*) and depth (*D*) channels. Each time the kernel is
    moved, the number of matrix multiply operations equals the number of pixels as
    *H* × *W* × *D*.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 在*正常卷积*中，核（例如，3 × 3）应用于高度(*H*)、宽度(*W*)和深度(*D*)通道。每次核移动时，矩阵乘法操作的次数等于像素的数量，即*H*
    × *W* × *D*。
- en: For example, an RGB image (which has three channels) with a 3 × 3 kernel applied
    across all three channels uses 3 × 3 × 3 = 27 matrix multiply (matmul) operations,
    producing an *N* × *M* × 1 (for example, 8 × 8 × 1) feature map (per kernel),
    where *N* and *M* are the resulting height and width of the feature map; see figure
    6.18
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，一个具有三个通道的RGB图像（应用3 × 3核到所有三个通道）使用了3 × 3 × 3 = 27次矩阵乘法（matmul）操作，生成一个*N* ×
    *M* × 1（例如，8 × 8 × 1）的特征图（每个核），其中*N*和*M*是特征图的结果高度和宽度；参见图6.18
- en: '![](Images/CH06_F18_Ferlitsch.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F18_Ferlitsch.png)'
- en: Figure 6.18 A padded convolution with a single filter
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.18 带有一个滤波器的填充卷积
- en: If we specify 256 filters for the output of the convolution, we have 256 kernels
    to train. In the RGB example using 256 3 × 3 kernels, that means 6912 matrix multiply
    operations each time the kernels move; see figure 6.19\. Thus, even with a small
    kernel size of 3 × 3, normal convolutions become computationally expensive as
    we increase the number of output feature maps for more representational power.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们指定卷积的输出为256个滤波器，我们就有256个核需要训练。在RGB示例中使用256个3 × 3核的情况下，这意味着每次核移动时需要进行6912次矩阵乘法操作；参见图6.19。因此，即使核大小很小（3
    × 3），随着我们增加输出特征图的数量以获得更强的表示能力，正常卷积的计算成本也会变得很高。
- en: '![](Images/CH06_F19_Ferlitsch.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F19_Ferlitsch.png)'
- en: Figure 6.19 Padded convolution with multiple filters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.19 带有多个滤波器的填充卷积
- en: 6.3.3 Spatial separable convolution
  id: totrans-147
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.3 空间可分离卷积
- en: In contrast, a *spatial separable convolution* factors a 2D kernel (for example,
    3 × 3) into two smaller 1D kernels. If we represent the 2D kernel as *H* × *W*,
    the factored two smaller 1D kernels would be *H* × 1 and 1 × *W*. This factorization
    reduces the total number of computations by one-third. While this factorization
    does not always maintain representational equivalence, the researchers demonstrated
    they were able to maintain representational equivalence in Inception v3\. Figure
    6.20 compares a normal convolution to a separable convolution.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，*空间可分离卷积*将2D核（例如，3 × 3）分解为两个较小的1D核。如果我们用*H* × *W*表示2D核，那么分解的两个较小的1D核将是*H*
    × 1和1 × *W*。这种分解将计算总量减少了一半。虽然这种分解并不总是保持表示等价性，但研究人员证明了他们在Inception v3中能够保持表示等价性。图6.20比较了正常卷积和可分离卷积。
- en: In the RGB example with a 3 × 3 kernel, a normal convolution would be 3 × 3
    × 3 (channels) = 27 matrix multiply operations each time the kernel is moved.
    In the same RGB example with a factored 3 × 3 kernel, a spatial separable convolution
    would be (3 × 1 × 3) + (1 × 3 × 3) = 18 matrix multiply operations each time the
    kernel is moved. Thus, the number of matrix multiply operations is reduced by
    a third (18 / 27).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用3 × 3核的RGB示例中，每次核移动时，正常卷积会有3 × 3 × 3（通道）= 27次矩阵乘法操作。在相同的RGB示例中，使用因式分解的3 ×
    3核，空间可分离卷积每次核移动时会有(3 × 1 × 3) + (1 × 3 × 3) = 18次矩阵乘法操作。因此，矩阵乘法操作的数量减少了三分之一（18
    / 27）。
- en: '![](Images/CH06_F20_Ferlitsch.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F20_Ferlitsch.png)'
- en: Figure 6.20 Normal vs. spatial separable convolution
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.20 正常卷积与空间可分离卷积的比较
- en: In the RGB example using 256 3 × 3 kernels, we have 4608 matrix multiply operations
    each time the kernels move, in contrast to a normal convolution, which would have
    6912 matrix multiply operations.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用256个3 × 3核的RGB示例中，每次核移动时我们有4608次矩阵乘法操作，与正常卷积相比，正常卷积会有6912次矩阵乘法操作。
- en: 6.3.4 Stem redesign and implementation
  id: totrans-153
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.4 茎重新设计和实现
- en: By the time Inception v3 was designed, it was a common practice to replace coarse
    5 × 5 filters with a stack of two 3 × 3 filters, which is computationally less
    (18 versus 25 matrix matmul operations) and retains representational power. Using
    the same principle, the authors theorized that a 7 × 7 coarse-level convolution,
    which is computationally expensive (49 matmul per move), could be replaced by
    a stack of three 3 × 3 convolutions (27 matmul per move). This reduced the number
    of parameters in the stem component while retaining representational power.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 到Inception v3被设计出来的时候，用两个3 × 3的卷积堆叠替换粗略的5 × 5滤波器已经成为一种常见的做法，这比单个5 × 5滤波器计算量更小（18次矩阵乘法操作与25次相比）并且保留了表示能力。使用同样的原理，作者理论化地认为，一个计算成本较高的7
    × 7粗略级卷积（每次移动49次矩阵乘法），可以被三个3 × 3卷积的堆叠（每次移动27次矩阵乘法）所替代。这减少了茎组件中的参数数量，同时保留了表示能力。
- en: '![](Images/CH06_F21_Ferlitsch.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F21_Ferlitsch.png)'
- en: Figure 6.21 Inception v3 stem group consisting of a stack of three 3 × 3 convolutions
    replacing the 7 × 7 convolution in v1/v2
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21 Inception v3茎组，由替换v1/v2中7 × 7卷积的三个3 × 3卷积堆叠组成
- en: 'Figure 6.21 shows how the 7 × 7 convolution in the stem convolution group was
    factorized and replaced by a stack of three 3 × 3 convolutions, as follows:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.21展示了茎卷积组中的7 × 7卷积是如何分解并替换为三个3 × 3卷积的堆叠的，如下所示：
- en: The first 3 × 3 is a strided convolution (`strides=2, 2`) which performs a feature
    map reduction.
  id: totrans-158
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一个3 × 3是一个步长卷积（`strides=2, 2`），它执行特征图减少。
- en: The second 3 × 3 is a regular convolution.
  id: totrans-159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个3 × 3是一个常规卷积。
- en: The third 3 × 3 doubles the number of filters.
  id: totrans-160
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第三个3 × 3将滤波器的数量翻倍。
- en: Inception v3 would be one of the last SOTA models based on a factorized (or
    unfactorized) 7 × 7 coarse filter. Today’s current practice is factorized (or
    unfactorized) 5 × 5.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3将是基于分解（或非分解）的7 × 7粗滤波器的最后一批SOTA模型之一。今天的当前做法是分解（或非分解）的5 × 5。
- en: 'And the next code example is an implementation of the Inception v3 stem group,
    which consists of the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个代码示例是实现Inception v3的茎组，它包括以下内容：
- en: Stack of three 3 × 3 convolutions (factorized 7 × 7), in which first convolution
    is strided for feature pooling (25% size of input shape).
  id: totrans-163
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 三个3 × 3卷积的堆叠（分解的7 × 7），其中第一个卷积用于特征池化（输入形状的25%大小）。
- en: A max pooling layer for further dimensionality reduction of the feature maps
    (6% size of input shape).
  id: totrans-164
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个最大池化层用于进一步减少特征图的维度（输入形状的6%大小）。
- en: 1 × 1 linear projection convolution to expand the number of feature maps from
    64 to 80.
  id: totrans-165
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 1 × 1线性投影卷积将特征图的数量从64扩展到80。
- en: A 3 × 3 convolution for further dimensionality expansion to 192 feature maps.
  id: totrans-166
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 一个3 × 3的卷积用于进一步将维度扩展到192个特征图。
- en: A second max pooling layer for further dimensionality reduction of the feature
    maps (1.5% size of input shape).
  id: totrans-167
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二个最大池化层用于进一步减少特征图的维度（输入形状的1.5%大小）。
- en: '[PRE9]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Inception v3 stem, 7 × 7 is replaced by a stack of 3 × 3 convolutions
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Inception v3茎，7 × 7被替换为一个3 × 3卷积的堆叠
- en: ❷ A 1 × 1 linear projection convolution
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个1 × 1线性投影卷积
- en: ❸ Feature map expansion (dimensionality expansion)
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 特征图扩展（维度扩展）
- en: ❹ Feature map pooling (dimensionality reduction)
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 特征图池化（维度减少）
- en: A `summary()` for the stem group shows 614,000 parameters to train with input
    (229, 229, 3).
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 对于茎组的一个`summary()`显示有614,000个参数用于输入（229, 229, 3）训练。
- en: 6.3.5 Auxiliary classifier
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3.5 辅助分类器
- en: Another change to Inception v3 was to reduce the two auxiliary classifiers to
    a single auxiliary, and to further simplify it, as depicted in figure 6.22\. The
    authors explained that they made these changes because they “found that auxiliary
    classifiers did not result in improved convergence early in the training.” By
    retaining a single classifier, it appears they shot for the midpoint.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: Inception v3的另一个变化是将两个辅助分类器减少到一个，并进一步简化，如图6.22所示。作者解释说，他们做出这些改变是因为“他们发现辅助分类器在训练初期并没有导致收敛性的改善。”通过保留单个分类器，他们似乎瞄准了中间点。
- en: '![](Images/CH06_F22_Ferlitsch.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F22_Ferlitsch.png)'
- en: Figure 6.22 Inception v3 auxiliary group
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.22 Inception v3辅助组
- en: Likewise, they adopted the convention of the time to remove additional dense
    layers before the final classifier, further reducing parameters. Earlier researchers
    had established that removing additional dense layers (prior to the dense layer
    for classification) resulted in no loss in accuracy.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，他们采用了当时的一种惯例，在最终的分类器之前移除额外的密集层，进一步减少参数。早期的研究人员已经确立，移除额外的密集层（在分类密集层之前）不会导致准确度下降。
- en: 'The auxiliary classifier was further simplified to the following:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 辅助分类器进一步简化为以下内容：
- en: An average pooling (`AveragePooling2D`) layer, which reduces each feature map
    to a single 1 × 1 matrix
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个平均池化层（`AveragePooling2D`），将每个特征图减少到一个1 × 1的矩阵
- en: A 3 × 3 convolution (`Conv2D`) layer, which outputs 768 1 × 1 feature maps
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个3 × 3卷积层（`Conv2D`），输出768个1 × 1特征图
- en: A flattening layer to flatten (`Flatten`) the feature maps to a 768-element
    1D vector
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个展平层（`Flatten`）将特征图展平为768个元素的1D向量
- en: A final dense (`Dense`) layer for classification
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个用于分类的最终密集层（`Dense`）
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for Inception v3 is on GitHub ([http://mng.bz/oGnd](http://mng.bz/oGnd)).
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式为Inception v3编写的完整代码可以在GitHub上找到（[http://mng.bz/oGnd](http://mng.bz/oGnd)）。
- en: '6.4 ResNeXt: Wide residual neural networks'
  id: totrans-185
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.4 ResNeXt：宽残差神经网络
- en: Facebook AI Research’s *ResNeXt*, which was the first runner-up of the 2016
    ILSVRC competition for ImageNet, introduced a wide residual block that uses a
    split-transform-merge pattern for parallel convolutions. This architecture for
    parallel convolutions is referred to as a *group convolution*.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: Facebook AI Research的*ResNeXt*，是2016年ILSVRC竞赛ImageNet的第二名，引入了一个使用分割-变换-合并模式进行并行卷积的宽残差块。这种并行卷积的架构被称为*组卷积*。
- en: The number of parallel convolutions constitutes the width, and is called the
    *cardi-nality*. For example, in the 2016 competition, the ResNeXt architecture
    used a cardinality of 32, meaning each ResNeXt layer consisted of 32 parallel
    convolutions.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 并行卷积的数量构成了宽度，称为*基数*。例如，在2016年的比赛中，ResNeXt架构使用了32个基数，这意味着每个ResNeXt层由32个并行卷积组成。
- en: The idea here was that adding parallel convolutions would help a model gain
    in accuracy without the necessity of going deeper, which is more prone to memorization.
    In their ablation study ([https://arxiv.org/abs/1611.05431](https://arxiv.org/abs/1611.05431)),
    Saining Xie et al. compared ResNeXt with 50, 101, and 200 layers with ResNet,
    and 101 and 200 layers with Inception v3 on the ImageNet dataset. In all cases,
    ResNeXt architecture at the same depth layer achieved higher accuracy.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的想法是添加并行卷积可以帮助模型在不加深层的情况下提高准确性，而加深层更容易导致记忆化。在他们的消融研究中（[https://arxiv.org/abs/1611.05431](https://arxiv.org/abs/1611.05431)），Saining
    Xie等人比较了ResNeXt与ResNet的50、101和200层，以及Inception v3的101和200层在ImageNet数据集上的表现。在所有情况下，相同深度层的ResNeXt架构都实现了更高的准确性。
- en: If you look at pretrained model repositories, such as TensorFlow Hub, you can
    see that the SE-ResNeXt variant has slightly higher computational and more accuracy,
    and is favored as the image classification backbone.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你查看预训练模型存储库，例如TensorFlow Hub，你可以看到SE-ResNeXt变体具有略高的计算量和更高的准确性，并且被选为图像分类的主干。
- en: 6.4.1 ResNeXt block
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.1 ResNeXt块
- en: 'At each ResNeXt layer, the input from the previous layer is split across the
    parallel convolutions, and the output (feature maps) from each convolution is
    concatenated back together. Finally, the input to the layer is matrix-added to
    the concatenated output (identity link) to form the residual block. This set of
    layers is referred to as *split-transform-merge* *and* *scale* operations. Defining
    these terms will help clarify the operation:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个ResNeXt层中，从前一层的输入被分配到并行卷积中，每个卷积的输出（特征图）被连接回一起。最后，将层的输入矩阵加到连接的输出（恒等连接）上，形成残差块。这一系列层被称为*分割-变换-合并*和*缩放*操作。定义这些术语将有助于阐明操作：
- en: '*Split* refers to the splitting of the feature maps into groups based on the
    cardinality.'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*分割*指的是根据基数将特征图分成组。'
- en: '*Transform* is what happens in the parallel convolutions for each group.'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*变换*是指每个组中并行卷积中发生的事情。'
- en: '*Merge* refers to the concatenation operation of the resulting feature maps.'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*合并*指的是结果特征图的连接操作。'
- en: '*Scale* indicates the add operation in the identity link.'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*缩放*表示恒等连接中的加法操作。'
- en: The goal of the split-transform-merge operation was to increase accuracy without
    increasing parameters. It did this by converting an elementary transformation
    (*w* × *x*) into a network-in-neuron aggregated transformation.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 分割-变换-合并操作的目标是在不增加参数的情况下提高准确性。它是通过将基本变换（*w* × *x*）转换为网络内神经元聚合变换来实现的。
- en: 'Now for the architecture that implements these concepts. As you can see in
    figure 6.23, the wide residual block group of ResNeXt consists of the following:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在来看实现这些概念的架构。如图6.23所示，ResNeXt的宽残差块组包括以下内容：
- en: A first bottleneck convolution (1 × 1 kernel)
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个第一瓶颈卷积（1 × 1核）
- en: A split-branch-concatenate convolution of cardinality *N* (group convolution)
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个基数*N*的分割-分支-连接卷积（组卷积）
- en: A final bottleneck convolution (1 × 1 kernel)
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个最终的瓶颈卷积（1 × 1核）
- en: An identity link (shortcut) between the input and the final convolution output
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 输入和最终卷积输出之间的恒等连接（快捷连接）
- en: '![](Images/CH06_F23_Ferlitsch.png)'
  id: totrans-202
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F23_Ferlitsch.png)'
- en: Figure 6.23 Residual next block with identity shortcut that implements the split-transform-merge
    and scale operations
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.23 实现分割-变换-合并和缩放操作的具有恒等快捷连接的残差块
- en: 'Let’s take a closer look at the split-transform-merge operation of the group
    convolution (figure 6.24). Here’s how the three major steps get applied:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地看看组卷积的分割-变换-合并操作（图6.24）。以下是三个主要步骤的应用方式：
- en: 'Split: The input (feature maps) are evenly split into *N* groups (where *N*
    is the cardinality).'
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割：输入（特征图）被均匀分割成*N*组（其中*N*是基数）。
- en: 'Transform: Each group is passed through a separate 3 × 3 convolution.'
  id: totrans-206
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 变换：每个组通过一个独立的3 × 3卷积。
- en: 'Merge: All the transformed groups are concatenated back together.'
  id: totrans-207
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并：将所有变换后的组连接在一起。
- en: '![](Images/CH06_F24_Ferlitsch.png)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/CH06_F24_Ferlitsch.png)'
- en: Figure 6.24 ResNeXt group convolution that implements the split-transform-merge
    operation
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.24 实现分割-变换-合并操作的ResNeXt组卷积
- en: The first bottleneck convolution performs dimensionality reduction by reducing
    (compressing) the number of input feature maps. We saw a similar use for the bottleneck
    convolution when we looked at the bottleneck residual block in chapter 5 and at
    the inception module in sections 6.1 through 6.3.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个瓶颈卷积通过减少（压缩）输入特征图的数量来执行维度降低。我们在第5章查看瓶颈残差块以及在6.1至6.3节查看Inception模块时看到了瓶颈卷积的类似用途。
- en: After the bottleneck convolution, the feature maps are split among the parallel
    convolutions according to the cardinality. For example, if the number of input
    feature maps (or channels) is 128 and the cardinality is 32, each parallel convolution
    will get 4 feature maps, which is the number of feature maps divided by the cardinality,
    or 128 divided by 32.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在瓶颈卷积之后，特征图根据基数分配到并行卷积中。例如，如果输入特征图的数量（或通道数）为128，基数是32，每个并行卷积将获得4个特征图，这是特征图数量除以基数，即128除以32。
- en: The outputs from the parallel convolutions are then concatenated back into a
    full set of feature maps, which are then passed through a final bottleneck convolution
    for another dimensionality reduction. As in the residual block, there is an identity
    link between the input to and output from the ResNeXt block, which is then matrix-added.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 然后将并行卷积的输出连接回完整的特征图集合，然后通过最终的瓶颈卷积进行另一轮维度降低。与残差块类似，输入到ResNeXt块和从ResNeXt块输出的输入之间存在恒等连接，然后进行矩阵加法。
- en: 'The following coding example of a ResNeXt block consists of four code sequences:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个ResNeXt块的编码示例，它由四个代码序列组成：
- en: The block input (shortcut) is passed through a 1 × 1 bottleneck convolution
    for dimensionality reduction.
  id: totrans-214
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 块输入（快捷连接）通过一个1 × 1瓶颈卷积进行维度降低。
- en: The split-transform operation (group convolution).
  id: totrans-215
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 分割-变换操作（组卷积）。
- en: The merge operation (concatenation).
  id: totrans-216
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 合并操作（连接）。
- en: The input is matrix-added to the output from the merge operation (identity link)
    as the scale operation.
  id: totrans-217
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 输入与合并操作的输出（恒等连接）矩阵相加作为缩放操作。
- en: '[PRE10]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Shortcut link is a 1 × 1 bottleneck convolution for dimensionality reduction
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 短路连接是一个用于维度降低的1 × 1瓶颈卷积
- en: ❷ Calculates the number of channels per group by dividing by cardinality (width)
    size
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过除以基数（宽度）大小来计算每个组的通道数
- en: ❸ Performs the split-transform step
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 执行分割-变换步骤
- en: ❹ Performs the merge step by concatenating the outputs from the group convolutions
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过连接组卷积的输出执行合并步骤
- en: ❺ 1 × 1 linear projection for dimensionality restoration
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 1 × 1线性投影以恢复维度
- en: ❻ Adds the shortcut to the output of the block
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将快捷连接添加到块的输出中
- en: Note In this code listing, the `Lambda()` method performs the splitting of the
    feature maps. The sequence `z[:,` `:,` `:,` `i` `*` `filters_card:i` `*` `filters_card`
    `+` `filters_card]` is a sliding window that splits the input feature maps along
    the fourth dimension; the fourth dimensions are the channels *B* × *H* × *W* ×
    *C*.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在此代码列表中，`Lambda()`方法执行特征图的分割。序列`z[:, :, :, i * filters_card:i * filters_card
    + filters_card]`是一个滑动窗口，它沿着第四维分割输入特征图；第四维是通道 *B* × *H* × *W* × *C*。
- en: 6.4.2 ResNeXt architecture
  id: totrans-226
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4.2 ResNeXt架构
- en: The architecture, depicted in figure 6.25, starts with a stem convolution group
    for the input, consisting of a 7 × 7 convolution that is then passed through a
    max pooling layer for reducing the data.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 如图6.25所示，该架构从输入的茎卷积组开始，包括一个7×7的卷积，然后通过最大池化层来减少数据。
- en: '![](Images/CH06_F25_Ferlitsch.png)'
  id: totrans-228
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F25_Ferlitsch.png)'
- en: Figure 6.25 ResNeXt learner component showing feature pooling between convolution
    groups
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.25展示了ResNeXt学习组件在卷积组之间的特征池化
- en: 'Following the stem are four groups of ResNeXt blocks. Each group progressively
    doubles the number of filters outputted as compared to the input. Between each
    block is a strided convolution, which serves two purposes:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 在茎卷积组之后是四个ResNeXt块组。每个组相对于输入逐步将输出的滤波器数量翻倍。在每个块之间是一个步长卷积，它有两个作用：
- en: It reduces the data by 75% (feature pooling).
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将数据减少75%（特征池化）。
- en: It doubles the filters from the output of the previous layer, so when the identity
    link is made between the input of this layer and its output, the number of filters
    match for the matrix addition operation.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它将前一层输出的滤波器数量翻倍，因此当在当前层的输入和输出之间建立恒等连接时，滤波器的数量与矩阵加法操作匹配。
- en: After the final ResNeXt group, the output is passed to the classifier component.
    The classifier consists of a max pooling layer and a flattening layer, which flattens
    the input into a 1D vector, and then passes it to a single dense layer for classification.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在最终的ResNeXt组之后，输出被传递到分类组件。分类器由一个最大池化层和一个展平层组成，展平层将输入展平成一个一维向量，然后将其传递到一个单层密集层进行分类。
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for ResNeXt is on GitHub ([http://mng.bz/my6r](http://mng.bz/my6r)).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在GitHub上（[http://mng.bz/my6r](http://mng.bz/my6r)）有一个使用Idiomatic procedure reuse设计模式为ResNeXt编写的完整代码示例。
- en: 6.5 Wide residual network
  id: totrans-235
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.5宽残差网络
- en: The *wide residual network* (*WRN*), introduced in 2016 by researchers at ParisTech,
    took another approach to wide convolutional neural networks. The researchers operated
    from the theory that as a model goes deeper in layers, feature reuse is diminished
    and therefore training takes longer. They did a study using residual networks
    and added a parameter for a multiplier on the number of filters (width) per residual
    block. This decreased the depth. When they tested this design, they found that
    a WRN with just 16 layers could outperform other SOTA architectures.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 2016年由巴黎理工学院的学者们提出的**宽残差网络**（*WRN*），对宽卷积神经网络采取了另一种方法。研究者们从理论出发，认为随着模型层级的加深，特征复用减少，因此训练时间更长。他们使用残差网络进行了一项研究，并为每个残差块中的滤波器数量（宽度）添加了一个乘数参数。这减少了网络的深度。当他们对这种设计进行测试时，发现只有16层的WRN就能超越其他SOTA架构。
- en: Soon, a design called DenseNet would demonstrate another alternative for dealing
    with feature reuse in deeper layers. As with the WRN, DenseNet worked on the assumption
    that increasing feature reuse would lead to more representational power and higher
    accuracy. DenseNet, however, achieved the reuse with a feature map concatenation
    of the input with the output of each residual block.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 很快，一个名为DenseNet的设计将展示另一种处理深层特征复用的方法。与WRN类似，DenseNet基于增加特征复用将导致更强的表示能力和更高的准确性的假设。然而，DenseNet通过将输入与每个残差块的输出进行特征图拼接来实现复用。
- en: In their ablation study, “Wide Residual Networks” ([https://arxiv.org/pdf/ 1605.07146.pdf](https://arxiv.org/pdf/1605.07146.pdf)),
    Sergey Zagoruyko and Nikos Komodakis applied their widening principal to a ResNet50,
    which they called a WRN-50-2, and they found that it outperformed an even deeper
    ResNet101\. Today’s SOTA models adopt the principle of using both wide and deep
    layers to achieve higher performance, faster training, and less memorization.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在他们的消融研究中，“Wide Residual Networks”([https://arxiv.org/pdf/1605.07146.pdf](https://arxiv.org/pdf/1605.07146.pdf))，Sergey
    Zagoruyko和Nikos Komodakis将他们的加宽原则应用于ResNet50，他们称之为WRN-50-2，并发现它优于更深层的ResNet101。今天的SOTA模型采用使用宽层和深层的原则来实现更高的性能、更快的训练和更少的记忆化。
- en: 6.5.1 WRN-50-2 architecture
  id: totrans-239
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.1 WRN-50-2架构
- en: 'This WRN model used the following design considerations:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 该WRN模型采用了以下设计考虑：
- en: Use pre-activation batch normalization (BN-RE-Conv) for faster training, as
    in the ResNet v2.
  id: totrans-241
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用预激活批量归一化（BN-RE-Conv）以实现更快的训练，就像在ResNet v2中一样。
- en: Use two 3 × 3 convolutionals (B(3, 3)), as in ResNet34, instead of the less
    representational expressive bottleneck residual block (B(1,3,1)) in ResNet50\.
    The rationale here is based on the fact that the bottleneck design helped reduce
    parameters for increasing accuracy, *as networks went deeper.* By going *wider*
    for accuracy, the network is shallower, and can therefore retain the more representational
    expressive stack.
  id: totrans-242
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用两个3 × 3卷积（B(3, 3)），如ResNet34中所示，而不是ResNet50中更不具表现力的瓶颈残差块（B(1,3,1)）。这里的理由是基于瓶颈设计有助于减少参数以增加准确性的事实，*随着网络的加深*。通过*更宽*来提高准确性，网络变得更浅，因此可以保留更具表现力的堆叠。
- en: Denote l as the number of convolutional layers per group and *k* as the width
    factor to multiply the number of filters.
  id: totrans-243
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 用l表示每个组中卷积层的数量，用*k*表示乘以过滤器数量的宽度因子。
- en: Move the dropout operation from the top layers (which was the convention) to
    in between each convolutional layer in the residual blocks and after the ReLU.
    The reasoning here was to perturb the batch normalization.
  id: totrans-244
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将dropout操作从顶层（这是惯例）移动到残差块中的每个卷积层之间以及ReLU之后。这里的推理是为了扰动批量归一化。
- en: In the macro-architecture in figure 6.26, you can see three of these principles
    at work, so that each convolutional group doubles the number of output features.
    Each convolution uses a pre-activation batch normalization (design principle 1).
    Each residual block within the group uses a B(3,3) residual block (design principle
    2). And metaparameter *k* is used for the width multiplier on the number of filters
    per convolution (design principle 3). Not depicted is the dropout in the residual
    blocks (design principle 4).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在图6.26的宏观架构中，你可以看到这三个原则在起作用，因此每个卷积组将输出特征的数量翻倍。每个卷积都使用预激活批量归一化（设计原则1）。组内的每个残差块使用B(3,3)残差块（设计原则2）。并且元参数*k*用于每个卷积的过滤器数量宽度乘数（设计原则3）。未展示的是残差块中的dropout（设计原则4）。
- en: '![](Images/CH06_F26_Ferlitsch.png)'
  id: totrans-246
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F26_Ferlitsch.png)'
- en: Figure 6.26 In the WRN macro-architecture, each convolution group progressively
    doubles the number of output feature maps.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.26 在WRN宏观架构中，每个卷积组逐渐将输出特征图的数量翻倍。
- en: 6.5.2 Wide residual block
  id: totrans-248
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5.2 宽残差块
- en: Let’s focus on the wide residual block, which is composed of the various residual
    groups. Figure 6.27 shows that both 3 × 3 convolutions (B(3,3)) have their number
    of filters multiplied by a configurable width factor (*k*). Between the 3 × 3
    convolution is a dropout layer for block-level regularization. Otherwise, the
    wide residual block design is identical to the ResNet34 residual block.
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们关注宽残差块，它由各种残差组组成。图6.27显示，两个3 × 3卷积（B(3,3)）的过滤器数量都乘以一个可配置的宽度因子(*k*)。在3 × 3卷积之间是一个用于块级正则化的dropout层。否则，宽残差块的设计与ResNet34残差块相同。
- en: '![](Images/CH06_F27_Ferlitsch.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F27_Ferlitsch.png)'
- en: Figure 6.27 Wide residual block with identity shortcut
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.27 带有标识快捷方式的宽残差块
- en: 'And here is a coding example of a wide residual block:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是一个宽残差块的编码示例：
- en: '[PRE11]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Remembers the input
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 记住输入
- en: ❷ First 3 × 3 convolution using pre-activation batch normalization
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 第一个3 × 3卷积使用预激活批量归一化
- en: ❸ Second 3 × 3 convolution using pre-activation batch normalization
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 第二个3 × 3卷积使用预激活批量归一化
- en: ❹ Dropout after the ReLU to perturb the batch normalization
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ ReLU之后的dropout以扰动批量归一化
- en: ❺ Identity link, adds the input to the output of the block
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 标识链接，将输入添加到块的输出
- en: A complete code rendition using the Idiomatic procedure reuse design pattern
    for WRN is on GitHub ([http://mng.bz/n2oa](https://shortener.manning.com/n2oa)).
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 使用Idiomatic过程重用设计模式为WRN提供完整代码实现可在GitHub上找到 ([http://mng.bz/n2oa](https://shortener.manning.com/n2oa)).
- en: '6.6 Beyond computer vision: Structured data'
  id: totrans-260
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 6.6 超越计算机视觉：结构化数据
- en: Let’s take a look at how wide and deep layer concepts evolved in models for
    structured data. Prior to 2016, most applications for structured data continued
    to use classical machine learning methods, as opposed to deep learning. Unlike
    the unstructured data used in computer vision, structured data has a diversity
    of inputs, including numeric, categorical, and feature engineered. This range
    of inputs meant that going deep in layers of dense layers was not so effective
    in getting models to learn the nonlinear relationships between the input features
    and the corresponding labels.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看结构化数据模型中宽度和深度层概念是如何演变的。在2016年之前，大多数结构化数据的应用继续使用经典机器学习方法，而不是深度学习。与计算机视觉中使用的非结构化数据不同，结构化数据具有多种输入，包括数值、分类和特征工程。这种输入范围意味着在密集层的层中深入挖掘并不那么有效，以使模型学习输入特征和相应标签之间的非线性关系。
- en: 'Figure 6.28 depicts the pre-2016 approach of applying deep learning to structural
    data. In this approach, all the feature inputs are processed by a sequence of
    dense layers—going deep, where the hidden dense layers are essentially the learner.
    The output from the last dense layer is then passed to the task component. The
    task component is comparable to the task component in computer vision. The output
    from the last dense layer is already a 1D vector. There may be some additional
    pooling of the vector, which is then passed to a final dense layer with an activation
    function that corresponds to the task: linear or ReLU for regression, sigmoid
    for binary classification, and softmax for multiclass classification.'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28展示了在2016年之前将深度学习应用于结构化数据的方法。在这种方法中，所有特征输入都通过一系列密集层进行处理——深入其中，隐藏的密集层本质上就是学习器。最后密集层的输出随后传递到任务组件。任务组件与计算机视觉中的任务组件相当。最后密集层的输出已经是一个一维向量。该向量可能还会进行一些额外的池化操作，然后传递到一个具有对应于任务的激活函数的最终密集层：对于回归，使用线性或ReLU；对于二分类，使用sigmoid；对于多分类，使用softmax。
- en: '![](Images/CH06_F28_Ferlitsch.png)'
  id: totrans-263
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH06_F28_Ferlitsch.png)'
- en: Figure 6.28 The approach to structured-data models before 2016 used a deep-layer
    DNN.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.28 在2016年之前，结构化数据模型的方法使用了深层DNN。
- en: For structured data, you want to learn both *memorization* and *generalization*.
    Memorization is learning the co-occurrences of feature values (the covariant relationships).
    Generalization is learning new feature combinations that are not seen in the training
    data distribution but that would be seen in the data distribution when deployed.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 对于结构化数据，你希望学习记忆和泛化。记忆是学习特征值的共现（协变关系）。泛化是学习在训练数据分布中没有看到但部署时会在数据分布中看到的新特征组合。
- en: I will use face detection to illustrate the difference between memorization
    and generalization. In memorization, portions of the network learn to lock in
    on patterns for specific clusters of samples (for example, specific eye patterns
    or skin tones). When locked in, this part of the network will signal extremely
    high confidence on similar examples, but low confidence on comparable examples.
    As more and more of the neural network becomes locked in, the neural network degenerates
    into a decision tree. This is comparable to the set of rules in classical AI for
    an expert system. If an example matches a pattern that the expert coded, it is
    recognized. Otherwise, it can’t be recognized.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 我将使用人脸检测来阐述记忆和泛化的区别。在记忆中，网络的某些部分学会锁定特定样本群（例如，特定的眼睛模式或肤色）的模式。一旦锁定，这部分网络将在相似示例上发出极高的置信度信号，但在可比示例上置信度较低。随着越来越多的神经网络被锁定，神经网络退化成决策树。这类似于经典AI中专家系统的规则集。如果一个示例与专家编码的模式匹配，它就会被识别。否则，它无法被识别。
- en: For example, let’s assume the neural network is locked in on patterns such as
    eyes, skin tone, piercings, glasses, hats, hair occlusion, and facial hair. We
    then submit an image of a child with face paint and the model doesn’t recognize
    a face. You could then retrain with images of face paint, but with lock-in, you
    need to further increase the parameter capacity of the model to memorize the new
    pattern. And then there is another and another pattern—hence the problem with
    expert systems.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设神经网络锁定在眼睛、肤色、穿孔、眼镜、帽子、头发遮挡和面部毛发等模式上。然后我们提交一张有面部彩绘的孩子的图像，模型无法识别出人脸。然后你可以使用面部彩绘的图像重新训练，但由于锁定，你需要进一步增加模型的参数容量来记忆新的模式。然后还有另一个模式和另一个模式——这就是专家系统的问题所在。
- en: In generalization, redundant clusters of nodes weakly signal recognition of
    a pattern and collectively act as an ensemble within the model. The more redundant
    weakly signaling clusters in the model, the more likely the model will generalize
    to recognize a pattern it was not trained on.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 在泛化过程中，冗余的节点簇弱信号识别模式，并在模型中集体作为集成。模型中冗余弱信号簇越多，模型泛化以识别未训练过的模式的可能性就越大。
- en: Then, in 2016, Google Research published the model architecture wide-and-deep
    network and corresponding paper, “Wide & Deep Learning for Recommender Systems”
    by Heng-Tze Cheng et al. ([https://arxiv.org/pdf/1606.07792.pdf](https://arxiv.org/pdf/1606.07792.pdf)).
    While the paper was specific to improving recommender models, this model has been
    widely used across different structured-data model types. Recommendation was an
    interesting challenge because it makes use of both generalization and memorization.
    The goal was to make niche recommendations with high-value conversions that required
    generalization, in addition to memorization for widespread common co-occurrences.
    The wide-and-deep architecture combines both memorization and generalization in
    one model. It is essentially two models that combine at the task component.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，在2016年，谷歌研究发布了宽度和深度网络模型架构及其相应的论文，“推荐系统中的宽度和深度学习”由Heng-Tze Cheng等人撰写([https://arxiv.org/pdf/1606.07792.pdf](https://arxiv.org/pdf/1606.07792.pdf))。虽然这篇论文是针对改进推荐模型而特定的，但这种模型已被广泛应用于不同结构化数据模型类型。推荐是一个有趣的挑战，因为它同时利用了泛化和记忆化。目标是进行具有高价值转换的利基推荐，这需要泛化，除了记忆化广泛常见的共现之外。宽度和深度架构在一个模型中结合了记忆化和泛化。它本质上是由两个在任务组件中结合的模型组成。
- en: Figure 6.29 shows the wide-and-deep architecture. This architecture is also
    a *multimodal* architecture, in that it takes two separate inputs of different
    types.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29展示了宽度和深度架构。这个架构也是一个*多模态*架构，因为它接受两种不同类型的两个单独输入。
- en: '![](Images/CH06_F29_Ferlitsch.png)'
  id: totrans-271
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH06_F29_Ferlitsch.png)'
- en: Figure 6.29 The inputs are divided between the wide and deep layers, and the
    outputs of the layers are combined for the task component.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 图6.29 展示了输入在宽层和深度层之间的分配，以及层的输出结合到任务组件中。
- en: Let’s dive a little deeper into this architecture. The learner component consists
    of two sections, a multilayer deep neural network and a single-layer wide dense
    layer. The wide dense layer acts as a linear regressor and memorizes high-frequency
    co-occurrences. The deep neural network learns the nonlinearity and generalizes
    to the low-frequency (sparse) co-occurrences and co-occurrences not seen in the
    training data. The input to the wide dense layer are the base features (non-cross-features),
    which have been feature preprocessed, and transformed features (for example, one-hot
    encoding of categorical features). They are inputted directly into the wide dense
    layer, and as such there is no stem. The inputs to the multilayer dense neural
    network are the base features and the cross-features. In this case, a stem component
    converts the combined features into an embedding by using an encoder.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更深入地探讨这个架构。学习组件由两部分组成，一个多层深度神经网络和一个单层宽密集层。宽密集层充当线性回归器并记忆高频共现。深度神经网络学习非线性并推广到低频（稀疏）共现以及训练数据中未出现的共现。宽密集层的输入是基础特征（非交叉特征），这些特征已经过特征预处理，并转换为转换特征（例如，分类特征的独热编码）。它们直接输入到宽密集层，因此没有主干。多层密集神经网络的输入是基础特征和交叉特征。在这种情况下，一个主干组件通过使用编码器将组合特征转换为嵌入。
- en: The output from both the wide dense layer and the multilayer deep neural networks
    are then combined in the task component and may be additionally pooled. The task
    component is essentially the same as in a computer vision model. Both the wide
    dense layer and the multilayer deep neural network layers are trained together.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，从宽密集层和多层深度神经网络输出的结果在任务组件中结合，并且可能还会进行额外的池化。任务组件基本上与计算机视觉模型中的相同。宽密集层和多层深度神经网络层一起训练。
- en: Summary
  id: totrans-275
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: One approach to reducing exposure to memorization in deeper layers was to use
    parallel convolutions. This allowed for shallower convolutional neural networks
    to address the overcapacity.
  id: totrans-276
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少深层网络中记忆化暴露的一种方法是使用并行卷积。这允许使用更浅的卷积神经网络来解决过拟合问题。
- en: Representational equivalence occurs when a convolution design pattern can be
    refactored into another pattern that is computationally less expensive and smaller
    (in the number of parameters). With factorization, the model maintains the same
    level of information (or feature) extraction with less computational requirements.
    This allows models to be smaller, train faster, and reduce latency in prediction.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当卷积设计模式可以被重构为计算成本更低且更小（在参数数量上）的另一种模式时，就会发生表示等价。通过分解，模型在更少的计算需求下保持了相同水平的信息（或特征）提取。这允许模型更小、训练更快，并减少预测的延迟。
- en: The concept of refactoring a normal convolution into a computationally smaller
    spatial separable convolution was introduced in the Inception design. Inception
    demonstrates representational equivalence in preserving performance objectives
    on the ImageNet dataset.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Inception设计中引入了将常规卷积重构为计算量更小的空间可分离卷积的概念。Inception在ImageNet数据集上展示了在保持性能目标方面的表示等价性。
- en: ResNeXt introduced the split-transform-merge pattern in parallel group convolutions.
    This pattern increased accuracies from prior residual networks without going deeper
    in layers.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: ResNeXt在并行分组卷积中引入了分割-变换-合并模式。这种模式在不加深层的情况下提高了先前残差网络的准确性。
- en: The purpose of moving batch normalization from post- to pre-activation in WRN
    was to increase model accuracy. Pre-activation batch normalization further reduced
    the need to go deeper, which in turn reduced the need for regularization to prevent
    memorization. The pre-activation method increased training speed enough that slightly
    a higher learning rate could be used to achieve comparable convergence.
  id: totrans-280
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将批归一化从WRN的后激活阶段移动到前激活阶段的目的是提高模型准确性。前激活批归一化进一步减少了需要加深层的必要性，从而减少了防止过拟合的正则化需求。前激活方法提高了训练速度，使得可以使用略高的学习率来实现可比的收敛。
- en: A width multiplier for widening layers was added to WRN as a metaparameter for
    finding a width in a shallow wide residual network. The result was a model that
    would perform as well (in terms of accuracy) as a deeper residual network.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在WRN中添加了一个宽度乘数作为元参数，用于在浅层宽残差网络中寻找宽度，从而得到一个在准确性方面（即准确度）与更深层的残差网络表现相当（或一样好）的模型。
- en: Modern deep learning models for structured data use both wide and deep layers;
    wide layers do memorization, and deep layers do generalization.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 现代用于结构化数据的深度学习模型同时使用宽层和深层；宽层负责记忆，而深层负责泛化。
