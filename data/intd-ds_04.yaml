- en: Chapter 5\. First steps in big data
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第 5 章\. 大数据的第一步
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: 'Taking your first steps with two big data applications: Hadoop and Spark'
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个大数据应用程序：Hadoop 和 Spark 的第一步
- en: Using Python to write big data jobs
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Python 编写大数据作业
- en: Building an interactive dashboard that connects to data stored in a big data
    database
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建一个连接到存储在大数据数据库中的数据的交互式仪表板
- en: Over the last two chapters, we’ve steadily increased the size of the data. In
    [chapter 3](kindle_split_011.xhtml#ch03) we worked with data sets that could fit
    into the main memory of a computer. [Chapter 4](kindle_split_012.xhtml#ch04) introduced
    techniques to deal with data sets that were too large to fit in memory but could
    still be processed on a single computer. In this chapter you’ll learn to work
    with technologies that can handle data that’s so large a single node (computer)
    no longer suffices. In fact it may not even fit on a hundred computers. Now that’s
    a challenge, isn’t it?
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在过去的两章中，我们稳步增加了数据的大小。在第 3 章（kindle_split_011.xhtml#ch03）中，我们处理的数据集可以适应计算机的主内存。第
    4 章（kindle_split_012.xhtml#ch04）介绍了处理无法适应内存但仍然可以在单个计算机上处理的数据集的技术。在本章中，你将学习如何使用可以处理如此大量数据的技术，单个节点（计算机）已不再足够。实际上，它甚至可能无法适应一百台计算机。现在，这是一个挑战，不是吗？
- en: 'We’ll stay as close as possible to the way of working from the previous chapters;
    the focus is on giving you the confidence to work on a big data platform. To do
    this, the main part of this chapter is a case study. You’ll create a dashboard
    that allows you to explore data from lenders of a bank. By the end of this chapter
    you’ll have gone through the following steps:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将尽可能接近前几章的工作方式；重点是让你对在大数据平台上工作充满信心。为此，本章的主要内容是一个案例研究。你将创建一个仪表板，让你可以探索一家银行的贷款人数据。到本章结束时，你将完成以下步骤：
- en: Load data into Hadoop, the most common big data platform.
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据加载到最常见的大数据平台 Hadoop 中。
- en: Transform and clean data with Spark.
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Spark 转换和清理数据。
- en: Store it into a big data database called Hive.
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将其存储到名为 Hive 的大数据数据库中。
- en: Interactively visualize this data with Qlik Sense, a visualization tool.
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Qlik Sense，一个可视化工具，交互式地可视化这些数据。
- en: All this (apart from the visualization) will be coordinated from within a Python
    script. The end result is a dashboard that allows you to explore the data, as
    shown in [figure 5.1](#ch05fig01).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些（除了可视化）都将通过 Python 脚本进行协调。最终结果是允许你探索数据的仪表板，如图 5.1 所示（#ch05fig01）。
- en: Figure 5.1\. Interactive Qlik dashboard
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.1\. 交互式 Qlik 仪表板
- en: '![](Images/05fig01_alt.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![图片 5.1](Images/05fig01_alt.jpg)'
- en: Bear in mind that we’ll only scratch the surface of both practice and theory
    in this introductory chapter on big data technologies. The case study will touch
    three big data technologies (Hadoop, Spark, and Hive), but only for data manipulation,
    not model building. It will be up to you to combine the big data technologies
    you get to see here with the model-building techniques we touched upon in previous
    chapters.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在本章关于大数据技术的入门章节中，我们只会触及实践和理论的一小部分。案例研究将涉及三种大数据技术（Hadoop、Spark 和 Hive），但仅限于数据处理，而不是模型构建。将你在这里看到的大数据技术与我们在前几章中提到的模型构建技术相结合，将取决于你。
- en: 5.1\. Distributing data storage and processing with frameworks
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 使用框架分配数据存储和处理
- en: New big data technologies such as Hadoop and Spark make it much easier to work
    with and control a cluster of computers. Hadoop can scale up to thousands of computers,
    creating a cluster with petabytes of storage. This enables businesses to grasp
    the value of the massive amount of data available.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 新的大数据技术，如 Hadoop 和 Spark，使得与计算机集群的交互和工作控制变得更加容易。Hadoop 可以扩展到数千台计算机，创建一个拥有千兆字节存储空间的集群。这使得企业能够掌握大量可用数据的价值。
- en: '5.1.1\. Hadoop: a framework for storing and processing large data sets'
  id: totrans-17
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. Hadoop：存储和处理大数据集的框架
- en: 'Apache Hadoop is a framework that simplifies working with a cluster of computers.
    It aims to be all of the following things and more:'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Hadoop 是一个简化与计算机集群工作的框架。它旨在成为以下所有事物以及更多：
- en: '***Reliable*** —By automatically creating multiple copies of the data and redeploying
    processing logic in case of failure.'
  id: totrans-19
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可靠性*** —通过自动创建数据的多个副本并在发生故障时重新部署处理逻辑来确保可靠性。'
- en: '***Fault tolerant*** —It detects faults and applies automatic recovery.'
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***容错性*** —它检测故障并应用自动恢复。'
- en: '***Scalable*** —Data and its processing are distributed over clusters of computers
    (horizontal scaling).'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可扩展性*** —数据和其处理分布在计算机集群上（横向扩展）。'
- en: '***Portable*** —Installable on all kinds of hardware and operating systems.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***可移植性*** — 可安装在各种硬件和操作系统上。'
- en: The core framework is composed of a distributed file system, a resource manager,
    and a system to run distributed programs. In practice it allows you to work with
    the distributed file system almost as easily as with the local file system of
    your home computer. But in the background, the data can be scattered among thousands
    of servers.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 核心框架由分布式文件系统、资源管理器和运行分布式程序的系统组成。在实践中，它允许你几乎像使用家用计算机的本地文件系统一样轻松地使用分布式文件系统。但在幕后，数据可以在数千个服务器之间分散。
- en: The different components of Hadoop
  id: totrans-24
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Hadoop的不同组件
- en: At the heart of Hadoop we find
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hadoop的核心，我们发现
- en: A distributed file system (HDFS)
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分布式文件系统（HDFS）
- en: A method to execute programs on a massive scale (MapReduce)
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一种在大型规模上执行程序的方法（MapReduce）
- en: A system to manage the cluster resources (YARN)
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理集群资源的系统（YARN）
- en: On top of that, an ecosystem of applications arose ([figure 5.2](#ch05fig02)),
    such as the databases Hive and HBase and frameworks for machine learning such
    as Mahout. We’ll use Hive in this chapter. Hive has a language based on the widely
    used SQL to interact with data stored inside the database.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，还出现了一个应用生态系统（[图5.2](#ch05fig02)），例如Hive和HBase数据库以及机器学习框架如Mahout。在本章中，我们将使用Hive。Hive使用基于广泛使用的SQL的语言与数据库中存储的数据交互。
- en: Figure 5.2\. A sample from the ecosystem of applications that arose around the
    Hadoop Core Framework
  id: totrans-30
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2\. Hadoop核心框架周围出现的应用生态系统的一个示例
- en: '![](Images/05fig02_alt.jpg)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig02_alt.jpg)'
- en: It’s possible to use the popular tool Impala to query Hive data up to 100 times
    faster. We won’t go into Impala in this book, but more information can be found
    at [http://impala.io/](http://impala.io/). We already had a short intro to MapReduce
    in [chapter 4](kindle_split_012.xhtml#ch04), but let’s elaborate a bit here because
    it’s such a vital part of Hadoop.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用流行的工具Impala以高达100倍的速度查询Hive数据。本书中不会详细介绍Impala，但更多信息可以在[http://impala.io/](http://impala.io/)找到。我们已经在第4章（kindle_split_012.xhtml#ch04）中简要介绍了MapReduce，但在这里让我们详细说明一下，因为它对Hadoop来说如此重要。
- en: 'MapReduce: How Hadoop achieves parallelism'
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: MapReduce：Hadoop如何实现并行性
- en: Hadoop uses a programming method called MapReduce to achieve parallelism. A
    MapReduce algorithm splits up the data, processes it in parallel, and then sorts,
    combines, and aggregates the results back together. However, the MapReduce algorithm
    isn’t well suited for interactive analysis or iterative programs because it writes
    the data to a disk in between each computational step. This is expensive when
    working with large data sets.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop使用一种称为MapReduce的编程方法来实现并行性。MapReduce算法将数据分割，并行处理，然后对结果进行排序、合并和汇总。然而，MapReduce算法不适合交互式分析或迭代程序，因为它在每一步计算之间将数据写入磁盘。当处理大型数据集时，这很昂贵。
- en: Let’s see how MapReduce would work on a small fictitious example. You’re the
    director of a toy company. Every toy has two colors, and when a client orders
    a toy from the web page, the web page puts an order file on Hadoop with the colors
    of the toy. Your task is to find out how many color units you need to prepare.
    You’ll use a MapReduce-style algorithm to count the colors. First let’s look at
    a simplified version in [figure 5.3](#ch05fig03).
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看MapReduce在一个小型虚构示例中的工作方式。你是玩具公司的总监。每个玩具都有两种颜色，当客户从网页上订购玩具时，网页会将订单文件放入Hadoop中，包含玩具的颜色。你的任务是找出你需要准备多少颜色单元。你将使用MapReduce风格的算法来计数颜色。首先让我们看看[图5.3](#ch05fig03)中的简化版本。
- en: Figure 5.3\. A simplified example of a MapReduce flow for counting the colors
    in input texts
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3\. 计算输入文本中颜色计数的MapReduce流程的简化示例
- en: '![](Images/05fig03.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig03.jpg)'
- en: 'As the name suggests, the process roughly boils down to two big phases:'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 正如其名所示，这个过程大致可以分为两个主要阶段：
- en: '***Mapping phase*** —The documents are split up into key-value pairs. Until
    we reduce, we can have many duplicates.'
  id: totrans-39
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***映射阶段*** — 文档被分割成键值对。在我们减少之前，可能会有很多重复。'
- en: '***Reduce phase*** —It’s not unlike a SQL “group by.” The different unique
    occurrences are grouped together, and depending on the reducing function, a different
    result can be created. Here we wanted a count per color, so that’s what the reduce
    function returns.'
  id: totrans-40
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***减少阶段*** — 这与SQL的“分组”操作类似。不同的唯一发生事件被分组在一起，根据减少函数的不同，可以创建不同的结果。在这里，我们想要每个颜色的计数，所以这就是减少函数返回的内容。'
- en: In reality it’s a bit more complicated than this though.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，这比这要复杂一些。
- en: The whole process is described in the following six steps and depicted in [figure
    5.4](#ch05fig04).
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 整个过程在以下六个步骤中描述，并在[图5.4](#ch05fig04)中展示。
- en: Figure 5.4\. An example of a MapReduce flow for counting the colors in input
    texts
  id: totrans-43
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4\. 计算输入文本中颜色数量的MapReduce流程示例
- en: '![](Images/05fig04_alt.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig04_alt.jpg)'
- en: '**1**.  Reading the input files.'
  id: totrans-45
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  读取输入文件。'
- en: '**2**.  Passing each line to a mapper job.'
  id: totrans-46
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  将每一行传递给映射器作业。'
- en: '**3**.  The mapper job parses the colors (keys) out of the file and outputs
    a file for each color with the number of times it has been encountered (value).
    Or more technically said, it maps a key (the color) to a value (the number of
    occurrences).'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  映射器作业从文件中解析颜色（键）并输出一个文件，其中包含该颜色出现的次数（值）。或者更技术地说，它将一个键（颜色）映射到一个值（出现的次数）。'
- en: '**4**.  The keys get shuffled and sorted to facilitate the aggregation.'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  键被洗牌和排序以方便聚合。'
- en: '**5**.  The reduce phase sums the number of occurrences per color and outputs
    one file per key with the total number of occurrences for each color.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  汇总每个颜色的出现次数，并为每个键输出一个包含每个颜色总出现次数的文件。'
- en: '**6**.  The keys are collected in an output file.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**6**.  键被收集到一个输出文件中。'
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-52
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'While Hadoop makes working with big data easy, setting up a good working cluster
    still isn’t trivial, but cluster managers such as Apache Mesos do ease the burden.
    In reality, many (mid-sized) companies lack the competence to maintain a healthy
    Hadoop installation. This is why we’ll work with the Hortonworks Sandbox, a pre-installed
    and configured Hadoop ecosystem. Installation instructions can be found in [section
    1.5](kindle_split_009.xhtml#ch01lev1sec5): An introductory working example of
    Hadoop.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 Hadoop 让处理大数据变得容易，但设置一个良好的工作集群仍然不是一件简单的事情，但像 Apache Mesos 这样的集群管理器确实减轻了负担。实际上，许多（中等规模）公司缺乏维护健康
    Hadoop 安装的技能。这就是为什么我们将使用 Hortonworks Sandbox，这是一个预安装和配置好的 Hadoop 生态系统。安装说明可以在[第1.5节](kindle_split_009.xhtml#ch01lev1sec5)中找到：Hadoop的入门级工作示例。
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now, keeping the workings of Hadoop in mind, let’s look at Spark.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，考虑到 Hadoop 的工作原理，让我们来看看 Spark。
- en: '5.1.2\. Spark: replacing MapReduce for better performance'
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. Spark：用更好的性能替换 MapReduce
- en: Data scientists often do interactive analysis and rely on algorithms that are
    inherently iterative; it can take awhile until an algorithm converges to a solution.
    As this is a weak point of the MapReduce framework, we’ll introduce the Spark
    Framework to overcome it. Spark improves the performance on such tasks by an order
    of magnitude.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家经常进行交互式分析，并依赖于本质上迭代的算法；算法收敛到解决方案可能需要一段时间。由于这是 MapReduce 框架的弱点，我们将介绍 Spark
    框架来克服它。Spark 通过一个数量级提高了此类任务的性能。
- en: What is Spark?
  id: totrans-58
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 什么是Spark？
- en: Spark is a cluster computing framework similar to MapReduce. Spark, however,
    doesn’t handle the storage of files on the (distributed) file system itself, nor
    does it handle the resource management. For this it relies on systems such as
    the Hadoop File System, YARN, or Apache Mesos. Hadoop and Spark are thus complementary
    systems. For testing and development, you can even run Spark on your local system.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 是一个类似于 MapReduce 的集群计算框架。然而，Spark 并不处理（分布式）文件系统上的文件存储，也不处理资源管理。为此，它依赖于像
    Hadoop 文件系统、YARN 或 Apache Mesos 这样的系统。因此，Hadoop 和 Spark 是互补的系统。对于测试和开发，你甚至可以在本地系统上运行
    Spark。
- en: How does Spark solve the problems of MapReduce?
  id: totrans-60
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark 如何解决 MapReduce 的问题？
- en: 'While we oversimplify things a bit for the sake of clarity, Spark creates a
    kind of shared RAM memory between the computers of your cluster. This allows the
    different workers to share variables (and their state) and thus eliminates the
    need to write the intermediate results to disk. More technically and more correctly
    if you’re into that: Spark uses Resilient Distributed Datasets (RDD), which are
    a distributed memory abstraction that lets programmers perform in-memory computations
    on large clusters in a fault-tolerant way.^([[1](#ch05fn01)]) Because it’s an
    in-memory system, it avoids costly disk operations.'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了清晰起见，我们对事情进行了一些简化，但 Spark 在你的集群计算机之间创建了一种共享的RAM内存。这使得不同的工作者可以共享变量（及其状态），从而消除了将中间结果写入磁盘的需要。更技术性地，如果你感兴趣的话：Spark
    使用弹性分布式数据集（RDD），这是一种分布式内存抽象，允许程序员以容错的方式在大型集群上执行内存计算.^([[1](#ch05fn01)]) 因为它是一个内存系统，所以避免了昂贵的磁盘操作。
- en: ¹
  id: totrans-62
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-63
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See [https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf](https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf).
  id: totrans-64
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 见 [https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf](https://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf)。
- en: The different components of the Spark ecosystem
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark 生态系统中的不同组件
- en: Spark core provides a NoSQL environment well suited for interactive, exploratory
    analysis. Spark can be run in batch and interactive mode and supports Python.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 核心提供了一个非常适合交互式、探索性分析的 NoSQL 环境。Spark 可以以批处理和交互式模式运行，并支持 Python。
- en: Spark has four other large components, as listed below and depicted in [figure
    5.5](#ch05fig05).
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 有四个其他大型组件，如下所示，并在 [图 5.5](#ch05fig05) 中展示。
- en: Figure 5.5\. The Spark framework when used in combination with the Hadoop framework
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.5\. 使用 Hadoop 框架结合 Spark 框架时的 Spark 框架
- en: '![](Images/05fig05_alt.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig05_alt.jpg)'
- en: '**1**.  Spark streaming is a tool for real-time analysis.'
  id: totrans-70
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. Spark streaming 是一个用于实时分析的工具。'
- en: '**2**.  Spark SQL provides a SQL interface to work with Spark.'
  id: totrans-71
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. Spark SQL 提供了与 Spark 一起工作的 SQL 接口。'
- en: '**3**.  MLLib is a tool for machine learning inside the Spark framework.'
  id: totrans-72
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. MLLib 是 Spark 框架内机器学习的工具。'
- en: '**4**.  GraphX is a graph database for Spark. We’ll go deeper into graph databases
    in [chapter 7](kindle_split_015.xhtml#ch07).'
  id: totrans-73
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. GraphX 是 Spark 的图数据库。我们将在 [第 7 章](kindle_split_015.xhtml#ch07) 中更深入地探讨图数据库。'
- en: Now let’s dip our toes into loan data using Hadoop, Hive, and Spark.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们用 Hadoop、Hive 和 Spark 来浅尝贷款数据。
- en: '5.2\. Case study: Assessing risk when loaning money'
  id: totrans-75
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2. 案例研究：在贷款时评估风险
- en: 'Enriched with a basic understanding of Hadoop and Spark, we’re now ready to
    get our hands dirty on big data. The goal of this case study is to have a first
    experience with the technologies we introduced earlier in this chapter, and see
    that for a large part you can (but don’t have to) work similarly as with other
    technologies. Note: The portion of the data used here isn’t that big because that
    would require serious bandwidth to collect it and multiple nodes to follow along
    with the example.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在对 Hadoop 和 Spark 有基本了解的基础上，我们现在可以开始接触大数据了。本案例研究的目的是让我们对在本章中较早介绍的技术有一个初步的体验，并看到在很大一部分情况下，你可以（但不必）像使用其他技术一样工作。注意：这里使用的数据量不是很大，因为这需要收集它需要大量的带宽，并且需要多个节点来跟随示例。
- en: What we’ll use
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用
- en: Horton Sandbox on a virtual machine. If you haven’t downloaded and imported
    this to VM software such as VirtualBox, please go back to [section 1.5](kindle_split_009.xhtml#ch01lev1sec5)
    where this is explained. Version 2.3.2 of the Horton Sandbox was used when writing
    this chapter.
  id: totrans-78
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Horton Sandbox 在虚拟机上。如果你还没有将其下载并导入到虚拟机软件（如 VirtualBox）中，请回到 [第 1.5 节](kindle_split_009.xhtml#ch01lev1sec5)，那里有相关说明。在编写本章时使用了
    Horton Sandbox 的 2.3.2 版本。
- en: 'Python libraries: Pandas and pywebhdsf. They don’t need to be installed on
    your local virtual environment this time around; we need them directly on the
    Horton Sandbox. Therefore we need to fire up the Horton Sandbox (on VirtualBox,
    for instance) and make a few preparations.'
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Python 库：Pandas 和 pywebhdsf。这次你不需要在本地虚拟环境中安装它们；我们需要它们直接在 Horton Sandbox 上。因此，我们需要启动
    Horton Sandbox（例如在 VirtualBox 上）并进行一些准备。
- en: In the Sandbox command line there are several things you still need to do for
    this all to work, so connect to the command line. You can do this using a program
    like PuTTY. If you’re unfamiliar with PuTTY, it offers a command line interface
    to servers and can be downloaded freely at [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html).
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Sandbox 命令行中，还有一些事情你需要做才能使这一切都工作，所以连接到命令行。你可以使用像 PuTTY 这样的程序来完成这个任务。如果你不熟悉
    PuTTY，它提供了一个到服务器的命令行界面，并且可以免费从 [http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html](http://www.chiark.greenend.org.uk/~sgtatham/putty/download.html)
    下载。
- en: The PuTTY login configuration is shown in [figure 5.6](#ch05fig06).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: PuTTY 登录配置如图 5.6 所示。
- en: Figure 5.6\. Connecting to Horton Sandbox using PuTTY
  id: totrans-82
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.6\. 使用 PuTTY 连接到 Horton Sandbox
- en: '![](Images/05fig06.jpg)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig06.jpg)'
- en: The default user and password are (at the time of writing) “root” and “hadoop”,
    respectively. You’ll need to change this password at the first login, though.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 默认用户名和密码（在编写本书时）分别是“root”和“hadoop”。不过，你需要在第一次登录时更改此密码。
- en: 'Once connected, issue the following commands:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 连接成功后，执行以下命令：
- en: '**`yum -y install python-pip`** —This installs pip, a Python package manager.'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`yum -y install python-pip`** — 这将安装 pip，一个 Python 软件包管理器。'
- en: '**`pip install git+https://github.com/DavyCielen/pywebhdfs.git –upgrade`**
    —At the time of writing there was a problem with the pywebhdfs library and we
    fixed that in this fork. Hopefully you won’t require this anymore when you read
    this; the problem has been signaled and should be resolved by the maintainers
    of this package.'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pip install git+https://github.com/DavyCielen/pywebhdfs.git –upgrade`**
    —在撰写本文时，pywebhdfs库存在问题，我们在这次分支中修复了它。希望你在阅读本文时不再需要它；问题已经报告，应该由该包的维护者解决。'
- en: '**`pip install pandas`** —To install Pandas. This usually takes awhile because
    of the dependencies.'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**`pip install pandas`** —安装Pandas。这通常需要一段时间，因为存在依赖关系。'
- en: An .ipynb file is available for you to open in Jupyter or (the older) Ipython
    and follow along with the code in this chapter. Setup instructions for Horton
    Sandbox are repeated there; make sure to run the code directly on the Horton Sandbox.
    Now, with the preparatory business out of the way, let’s look at what we’ll need
    to do.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 可供你打开的.ipynb文件可以在Jupyter或（较老的）Ipython中使用，并跟随本章中的代码。Horton Sandbox的设置说明在那里重复；请确保直接在Horton
    Sandbox上运行代码。现在，准备工作已经完成，让我们看看我们需要做什么。
- en: 'In this exercise, we’ll go through several more of the data science process
    steps:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个练习中，我们将通过更多数据科学流程步骤：
- en: 'Step 1: The research goal. This consists of two parts:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 1：研究目标。这包括两个部分：
- en: Providing our manager with a dashboard
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为我们的经理提供仪表板
- en: Preparing data for other people to create their own dashboards
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为其他人准备数据以创建他们自己的仪表板
- en: 'Step 2: Data retrieval'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 2：数据检索
- en: Downloading the data from the lending club website
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从Lending Club网站下载数据
- en: Putting the data on the Hadoop File System of the Horton Sandbox
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将数据放在Horton Sandbox的Hadoop文件系统中
- en: 'Step 3: Data preparation'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 3：数据准备
- en: Transforming this data with Spark
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Spark转换这些数据
- en: Storing the prepared data in Hive
  id: totrans-99
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将准备好的数据存储在Hive中
- en: 'Steps 4 & 6: Exploration and report creation'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤 4 & 6：探索和报告创建
- en: Visualizing the data with Qlik Sense
  id: totrans-101
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Qlik Sense可视化数据
- en: We have no model building in this case study, but you’ll have the infrastructure
    in place to do this yourself if you want to. For instance, you can use SPARK Machine
    learning to try to predict when someone will default on his debt.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 在本案例研究中，我们没有建立模型，但如果你想要的话，你将拥有自己建立模型的基础设施。例如，你可以使用SPARK机器学习来尝试预测某人何时会违约。
- en: It’s time to meet the Lending Club.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候认识Lending Club了。
- en: '5.2.1\. Step 1: The research goal'
  id: totrans-104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 步骤 1：研究目标
- en: 'The Lending Club is an organization that connects people in need of a loan
    with people who have money to invest. Your boss also has money to invest and wants
    information before throwing a substantial sum on the table. To achieve this, you’ll
    create a report for him that gives him insight into the average rating, risks,
    and return for lending money to a certain person. By going through this process,
    you make the data accessible in a dashboard tool, thus enabling other people to
    explore it as well. In a sense this is the secondary goal of this case: opening
    up the data for self-service BI. Self-service Business Intelligence is often applied
    in data-driven organizations that don’t have analysts to spare. Anyone in the
    organization can do the simple slicing and dicing themselves while leaving the
    more complicated analytics for the data scientist.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: Lending Club是一个将需要贷款的人与有资金投资的人连接起来的组织。你的老板也有资金可以投资，在投入大量资金之前，他需要信息。为此，你将为他创建一份报告，让他了解向某个人贷款的平均评级、风险和回报。通过这个过程，你使数据在仪表板工具中变得可访问，从而使得其他人也能探索这些数据。从某种意义上说，这是本案例的次要目标：开放数据以实现自助式商业智能。自助式商业智能通常应用于没有分析师可用的数据驱动型组织。组织中的任何人都能够自己进行简单的切片和切块，而将更复杂的分析留给数据科学家。
- en: We can do this case study because the Lending Club makes anonymous data available
    about the existing loans. By the end of this case study, you’ll create a report
    similar to [figure 5.7](#ch05fig07).
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以进行这个案例研究，因为Lending Club提供了现有贷款的匿名数据。在本案例研究结束时，你将创建一个类似于[图 5.7](#ch05fig07)
    的报告。
- en: Figure 5.7\. The end result of this exercise is an explanatory dashboard to
    compare a lending opportunity to similar opportunities.
  id: totrans-107
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.7\. 本练习的最终结果是用于比较贷款机会的解释性仪表板。
- en: '![](Images/05fig07_alt.jpg)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig07_alt.jpg)'
- en: First things first, however; let’s get ourselves data.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们获取数据。
- en: '5.2.2\. Step 2: Data retrieval'
  id: totrans-110
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 步骤 2：数据检索
- en: It’s time to work with the Hadoop File System (or hdfs). First we’ll send commands
    through the command line and then through the Python scripting language with the
    help of the pywebhdfs package.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 是时候与 Hadoop 文件系统（或 hdfs）一起工作了。首先，我们将通过命令行发送命令，然后通过 Python 脚本语言（借助 pywebhdfs
    包）发送命令。
- en: The Hadoop file system is similar to a normal file system, except that the files
    and folders are stored over multiple servers and you don’t know the physical address
    of each file. This is not unfamiliar if you’ve worked with tools such as Dropbox
    or Google Drive. The files you put on these drives are stored somewhere on a server
    without you knowing exactly on which server. As on a normal file system, you can
    create, rename, and delete files and folders.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 文件系统类似于一个正常的文件系统，除了文件和文件夹存储在多个服务器上，你不知道每个文件的物理地址。如果你使用过 Dropbox 或 Google
    Drive 等工具，这并不陌生。你放在这些驱动器上的文件存储在服务器上的某个地方，但你不知道确切在哪个服务器上。就像在正常文件系统中一样，你可以创建、重命名和删除文件和文件夹。
- en: Using the command line to interact with the Hadoop file system
  id: totrans-113
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用命令行与 Hadoop 文件系统交互
- en: Let’s first retrieve the currently present list of directories and files in
    the Hadoop root folder using the command line. Type the command `hadoop fs –ls
    /` in PuTTY to achieve this.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先使用命令行检索 Hadoop 根目录下当前存在的目录和文件列表。在 PuTTY 中输入命令 `hadoop fs –ls /` 来实现这一点。
- en: Make sure you turn on your virtual machine with the Hortonworks Sandbox before
    attempting a connection. In PuTTY you should then connect to 127.0.0.1:2222, as
    shown before in [figure 5.6](#ch05fig06).
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 确保在尝试连接之前启动你的 Hortonworks Sandbox 虚拟机。在 PuTTY 中，你应该连接到 127.0.0.1:2222，如之前在[图
    5.6](#ch05fig06)中所示。
- en: The output of the Hadoop command is shown in [figure 5.8](#ch05fig08). You can
    also add arguments such as `hadoop fs –ls –R /` to get a recursive list of all
    the files and subdirectories.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: Hadoop 命令的输出显示在[图 5.8](#ch05fig08)中。你还可以添加如 `hadoop fs –ls –R /` 这样的参数来获取所有文件和子目录的递归列表。
- en: 'Figure 5.8\. Output from the Hadoop list command: hadoop fs –ls /. The Hadoop
    root folder is listed.'
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8\. Hadoop 列表命令的输出：hadoop fs –ls /。Hadoop 根目录被列出。
- en: '![](Images/05fig08_alt.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig08_alt.jpg)'
- en: 'We’ll now create a new directory “chapter5” on hdfs to work with during this
    chapter. The following commands will create the new directory and give everybody
    access to the folder:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将在 hdfs 上创建一个新的目录“chapter5”，以便在本章中使用。以下命令将创建新目录并允许每个人访问文件夹：
- en: '[PRE0]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: You probably noticed a pattern here. The Hadoop commands are very similar to
    our local file system commands (POSIX style) but start with Hadoop fs and have
    a dash - before each command. [Table 5.1](#ch05table01) gives an overview of popular
    file system commands on Hadoop and their local file system command counterparts.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能已经注意到了这里的模式。Hadoop 命令与我们的本地文件系统命令（POSIX 风格）非常相似，但以 Hadoop fs 开头，每个命令前都有一个破折号
    -。[表 5.1](#ch05table01) 提供了 Hadoop 上流行的文件系统命令及其本地文件系统命令对应关系的概述。
- en: Table 5.1\. List of common Hadoop file system commands
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表 5.1\. 常见 Hadoop 文件系统命令列表
- en: '| Goal | Hadoop file system command | Local file system command |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '| 目标 | Hadoop 文件系统命令 | 本地文件系统命令 |'
- en: '| --- | --- | --- |'
  id: totrans-124
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| Get a list of files and directories from a directory | hadoop fs –ls URI
    | ls URI |'
  id: totrans-125
  prefs: []
  type: TYPE_TB
  zh: '| 从目录中获取文件和目录列表 | hadoop fs –ls URI | ls URI |'
- en: '| Create a directory | hadoop fs –mkdir URI | mkdir URI |'
  id: totrans-126
  prefs: []
  type: TYPE_TB
  zh: '| 创建目录 | hadoop fs –mkdir URI | mkdir URI |'
- en: '| Remove a directory | hadoop fs –rm –r URI | rm –r URI |'
  id: totrans-127
  prefs: []
  type: TYPE_TB
  zh: '| 删除目录 | hadoop fs –rm –r URI | rm –r URI |'
- en: '| Change the permission of files | hadoop fs –chmod MODE URI | chmod MODE URI
    |'
  id: totrans-128
  prefs: []
  type: TYPE_TB
  zh: '| 修改文件权限 | hadoop fs –chmod MODE URI | chmod MODE URI |'
- en: '| Move or rename file | hadoop fs –mv OLDURI NEWURI | mv OLDURI NEWURI |'
  id: totrans-129
  prefs: []
  type: TYPE_TB
  zh: '| 移动或重命名文件 | hadoop fs –mv OLDURI NEWURI | mv OLDURI NEWURI |'
- en: There are two special commands you’ll use often. These are
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 你会经常使用两个特殊命令。这些是
- en: Upload files from the local file system to the distributed file system (`hadoop
    fs –put LOCALURI REMOTEURI)`.
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从本地文件系统上传文件到分布式文件系统（`hadoop fs –put LOCALURI REMOTEURI`）。
- en: Download a file from the distributed file system to the local file system (`hadoop
    –get REMOTEURI)`.
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从分布式文件系统下载文件到本地文件系统（`hadoop –get REMOTEURI`）。
- en: Let’s clarify this with an example. Suppose you have a .CSV file on the Linux
    virtual machine from which you connect to the Linux Hadoop cluster. You want to
    copy the .CSV file from your Linux virtual machine to the cluster hdfs. Use the
    command `hadoop –put mycsv.csv /data`.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用一个例子来澄清这一点。假设你在 Linux 虚拟机上有一个 .CSV 文件，你通过这个虚拟机连接到 Linux Hadoop 集群。你想要将 .CSV
    文件从你的 Linux 虚拟机复制到集群的 hdfs。使用命令 `hadoop –put mycsv.csv /data`。
- en: Using PuTTY we can start a Python session on the Horton Sandbox to retrieve
    our data using a Python script. Issue the “`pyspark`” command in the command line
    to start the session. If all is well you should see the welcome screen shown in
    [figure 5.9](#ch05fig09).
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 PuTTY，我们可以在 Horton Sandbox 上启动一个 Python 会话，使用 Python 脚本检索我们的数据。在命令行中输入“`pyspark`”命令以启动会话。如果一切顺利，你应该会看到如图
    5.9 所示的欢迎屏幕。
- en: Figure 5.9\. The welcome screen of Spark for interactive use with Python
  id: totrans-135
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9\. Spark 与 Python 交互使用的欢迎屏幕
- en: '![](Images/05fig09_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig09_alt.jpg)'
- en: Now we use Python code to fetch the data for us, as shown in the following listing.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们使用 Python 代码来为我们获取数据，如下所示。
- en: Listing 5.1\. Drawing in the Lending Club loan data
  id: totrans-138
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.1\. 绘制 Lending Club 贷款数据
- en: '![](Images/129fig01_alt.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/129fig01_alt.jpg)'
- en: We download the file “LoanStats3d.csv.zip” from the Lending Club’s website at
    [https://resources.lendingclub.com/LoanStats3d.csv.zip](https://resources.lendingclub.com/LoanStats3d.csv.zip)
    and unzip it. We use methods from the requests, zipfile, and stringio Python packages
    to respectively download the data, create a virtual file, and unzip it. This is
    only a single file; if you want all their data you could create a loop, but for
    demonstration purposes this will do. As we mentioned before, an important part
    of this case study will be data preparation with big data technologies. Before
    we can do so, however, we need to put it on the Hadoop file system. PyWebHdfs
    is a package that allows you to interact with the Hadoop file system from Python.
    It translates and passes your commands to rest calls for the webhdfs interface.
    This is useful because you can use your favorite scripting language to automate
    tasks, as shown in the following listing.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 Lending Club 的网站 [https://resources.lendingclub.com/LoanStats3d.csv.zip](https://resources.lendingclub.com/LoanStats3d.csv.zip)
    下载了文件“LoanStats3d.csv.zip”并解压它。我们使用 requests、zipfile 和 stringio Python 包中的方法分别下载数据、创建虚拟文件和解压它。这只是一个单个文件；如果你想要所有数据，你可以创建一个循环，但为了演示目的，这样就可以了。正如我们之前提到的，这个案例研究的一个重要部分将是使用大数据技术进行数据准备。然而，在我们这样做之前，我们需要将其放在
    Hadoop 文件系统中。PyWebHdfs 是一个包，允许你从 Python 与 Hadoop 文件系统交互。它将你的命令翻译并传递给 webhdfs 接口的
    rest 调用。这很有用，因为你可以使用你喜欢的脚本语言来自动化任务，如下所示。
- en: Listing 5.2\. Storing data on Hadoop
  id: totrans-141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.2\. 在 Hadoop 上存储数据
- en: '![](Images/130fig01_alt.jpg)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/130fig01_alt.jpg)'
- en: We had already downloaded and unzipped the file in [listing 5.1](#ch05ex01);
    now in [listing 5.2](#ch05ex02) we made a sub-selection of the data using Pandas
    and stored it locally. Then we created a directory on Hadoop and transferred the
    local file to Hadoop. The downloaded data is in .CSV format and because it’s rather
    small, we can use the Pandas library to remove the first line and last two lines
    from the file. These contain comments and will only make working with this file
    cumbersome in a Hadoop environment. The first line of our code imports the Pandas
    package, while the second line parses the file into memory and removes the first
    and last two data lines. The third code line saves the data to the local file
    system for later use and easy inspection.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在 [列表 5.1](#ch05ex01) 中下载并解压了文件；现在在 [列表 5.2](#ch05ex02) 中，我们使用 Pandas 对数据进行子选择并存储到本地。然后我们在
    Hadoop 上创建了一个目录，并将本地文件传输到 Hadoop。下载的数据是 .CSV 格式，因为它相对较小，我们可以使用 Pandas 库从文件中删除第一行和最后两行。这些行包含注释，在
    Hadoop 环境中处理此文件会变得繁琐。我们的代码的第一行导入了 Pandas 包，第二行将文件解析到内存中并删除第一行和最后两行数据。第三行代码将数据保存到本地文件系统以供以后使用和检查。
- en: 'Before moving on, we can check our file using the following line of code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在继续之前，我们可以使用以下代码行检查我们的文件：
- en: '[PRE1]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: The PySpark console should tell us our file is safe and well on the Hadoop system,
    as shown in [figure 5.10](#ch05fig10).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark 控制台应该会告诉我们我们的文件在 Hadoop 系统上是安全且良好的，如图 5.10 所示。
- en: Figure 5.10\. Retrieve file status on Hadoop via the PySpark console
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. 通过 PySpark 控制台在 Hadoop 上检索文件状态
- en: '![](Images/05fig10_alt.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig10_alt.jpg)'
- en: With the file ready and waiting for us on Hadoop, we can move on to data preparation
    using Spark, because it’s not clean enough to directly store in Hive.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 文件已经准备好并等待我们在 Hadoop 上进行数据准备，因为数据还不够干净，不能直接存储到 Hive。
- en: '5.2.3\. Step 3: Data preparation'
  id: totrans-150
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 第 3 步：数据准备
- en: Now that we’ve downloaded the data for analysis, we’ll use Spark to clean the
    data before we store it in Hive.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经下载了用于分析的数据，在将其存储到 Hive 之前，我们将使用 Spark 对数据进行清理。
- en: Data preparation in Spark
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: Spark 中的数据准备
- en: Cleaning data is often an interactive exercise, because you spot a problem and
    fix the problem, and you’ll likely do this a couple of times before you have clean
    and crisp data. An example of dirty data would be a string such as “UsA”, which
    is improperly capitalized. At this point, we no longer work in jobs.py but use
    the PySpark command line interface to interact directly with Spark.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗数据通常是一个交互式练习，因为你发现了问题并修复了问题，你可能会在得到干净清晰的数据之前这样做几次。一个脏数据的例子可能是一个像“UsA”这样的字符串，它被错误地大写。在这个时候，我们不再在
    jobs.py 中工作，而是使用 PySpark 命令行界面直接与 Spark 交互。
- en: Spark is well suited for this type of interactive analysis because it doesn’t
    need to save the data after each step and has a much better model than Hadoop
    for sharing data between servers (a kind of distributed memory).
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: Spark 非常适合这种交互式分析，因为它不需要在每一步后保存数据，并且比 Hadoop 在服务器之间共享数据（一种分布式内存）有更好的模型。
- en: 'The transformation consists of four parts:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 转换包括四个部分：
- en: '**1**.  Start up PySpark (should still be open from [section 5.2.2](#ch05lev2sec4))
    and load the Spark and Hive context.'
  id: totrans-156
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 启动 PySpark（应该仍然在[第 5.2.2 节](#ch05lev2sec4)中打开）并加载 Spark 和 Hive 上下文。'
- en: '**2**.  Read and parse the .CSV file.'
  id: totrans-157
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 读取并解析 .CSV 文件。'
- en: '**3**.  Split the header line from the data.'
  id: totrans-158
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 从数据中分割标题行。'
- en: '**4**.  Clean the data.'
  id: totrans-159
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. 清洗数据。'
- en: Okay, onto business. The following listing shows the code implementation in
    the PySpark console.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，让我们进入正题。下面的列表显示了 PySpark 控制台中代码的实现。
- en: Listing 5.3\. Connecting to Apache Spark
  id: totrans-161
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. 连接到 Apache Spark
- en: '![](Images/ch05ex03-0.jpg)'
  id: totrans-162
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch05ex03-0.jpg)'
- en: '![](Images/ch05ex03-1.jpg)'
  id: totrans-163
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch05ex03-1.jpg)'
- en: Let’s dive a little further into the details for each step.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们进一步深入了解每个步骤的细节。
- en: 'Step 1: Starting up Spark in interactive mode and loading the context'
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 1 步：以交互模式启动 Spark 并加载上下文
- en: The Spark context import isn’t required in the PySpark console because a context
    is readily available as variable `sc`. You might have noticed this is also mentioned
    when opening PySpark; check out [figure 5.9](#ch05fig09) in case you overlooked
    it. We then load a Hive context to enable us to work interactively with Hive.
    If you work interactively with Spark, the Spark and Hive contexts are loaded automatically,
    but if you want to use it in batch mode you need to load it manually. To submit
    the code in batch you would use the `spark-submit filename.py` command on the
    Horton Sandbox command line.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 在 PySpark 控制台中不需要导入 Spark 上下文，因为上下文作为变量 `sc` 可用。你可能已经注意到，在打开 PySpark 时也提到了这一点；如果你错过了，请查看[图
    5.9](#ch05fig09)。然后我们加载一个 Hive 上下文，以便我们可以交互式地使用 Hive。如果你交互式地使用 Spark，Spark 和 Hive
    上下文会自动加载，但如果你想以批处理模式使用它，你需要手动加载。要在批处理中提交代码，请在 Horton Sandbox 命令行上使用 `spark-submit
    filename.py` 命令。
- en: '[PRE2]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: With the environment set up, we’re ready to start parsing the .CSV file.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 环境设置完毕后，我们准备开始解析 .CSV 文件。
- en: 'Step 2: Reading and parsing the .CSV file'
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 2 步：读取和解析 .CSV 文件
- en: Next we read the file from the Hadoop file system and split it at every comma
    we encounter. In our code the first line reads the .CSV file from the Hadoop file
    system. The second line splits every line when it encounters a comma. Our .CSV
    parser is naïve by design because we’re learning about Spark, but you can also
    use the .CSV package to help you parse a line more correctly.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们从 Hadoop 文件系统中读取文件，并在遇到的每个逗号处分割它。在我们的代码中，第一行从 Hadoop 文件系统中读取 .CSV 文件。第二行在遇到逗号时分割每一行。我们的
    .CSV 解析器设计上是天真的，因为我们正在学习 Spark，但你也可以使用 .CSV 包来帮助你更正确地解析一行。
- en: '[PRE3]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Notice how similar this is to a functional programming approach. For those who’ve
    never encountered it, you can naïvely read `lambda r:r.split(',')` as “for every
    input r (a row in this case), split this input r when it encounters a comma.”
    As in this case, “for every input” means “for every row,” but you can also read
    it as “split every row by a comma.” This functional-like syntax is one of my favorite
    characteristics of Spark.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 注意这与函数式编程方法是多么相似。对于那些从未遇到过它的人来说，你可以天真地读 `lambda r:r.split(',')` 为“对于每个输入 r（在这种情况下是一行），当它遇到逗号时分割这个输入
    r。”就像在这种情况下，“对于每个输入”意味着“对于每一行”，但你也可以把它读作“通过逗号分割每一行。”这种类似函数的语法是 Spark 我最喜欢的特性之一。
- en: 'Step 3: Split the header line from the data'
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 3 步：从数据中分割标题行
- en: 'To separate the header from the data, we read in the first line and retain
    every line that’s not similar to the header line:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将标题与数据分开，我们读取第一行并保留与标题行不相似的每一行：
- en: '[PRE4]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Following the best practices in big data, we wouldn’t have to do this step because
    the first line would already be stored in a separate file. In reality, .CSV files
    do often contain a header line and you’ll need to perform a similar operation
    before you can start cleaning the data.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 遵循大数据的最佳实践，我们不需要执行此步骤，因为第一行已经存储在单独的文件中。实际上，.CSV 文件通常包含标题行，你需要在开始清理数据之前执行类似的操作。
- en: 'Step 4: Clean the data'
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 4 步：清理数据
- en: In this step we perform basic cleaning to enhance the data quality. This allows
    us to build a better report.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一步中，我们执行基本的清理以增强数据质量。这使我们能够构建更好的报告。
- en: After the second step, our data consists of arrays. We’ll treat every input
    for a lambda function as an array now and return an array. To ease this task,
    we build a helper function that cleans. Our cleaning consists of reformatting
    an input such as “10,4%” to 0.104 and encoding every string as utf-8, as well
    as replacing underscores with spaces and lowercasing all the strings. The second
    line of code calls our helper function for every line of the array.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二步之后，我们的数据由数组组成。现在我们将每个 lambda 函数的输入视为一个数组，并返回一个数组。为了简化这项任务，我们构建了一个辅助函数来清理。我们的清理包括将输入“10,4%”重新格式化为
    0.104，将每个字符串编码为 utf-8，以及将下划线替换为空格并将所有字符串转换为小写。第二行代码为数组中的每一行调用我们的辅助函数。
- en: '[PRE5]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Our data is now prepared for the report, so we need to make it available for
    our reporting tools. Hive is well suited for this, because many reporting tools
    can connect to it. Let’s look at how to accomplish this.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据现在已准备好用于报告，因此我们需要使其对我们的报告工具可用。Hive 非常适合此目的，因为许多报告工具都可以连接到它。让我们看看如何完成这项任务。
- en: Save the data in Hive
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在 Hive 中保存数据
- en: 'To store data in Hive we need to complete two steps:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 要在 Hive 中存储数据，我们需要完成两个步骤：
- en: '**1**.  Create and register metadata.'
  id: totrans-184
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 创建并注册元数据。'
- en: '**2**.  Execute SQL statements to save data in Hive.'
  id: totrans-185
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 执行 SQL 语句以在 Hive 中保存数据。'
- en: In this section, we’ll once again execute the next piece of code in our beloved
    PySpark shell, as shown in the following listing.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将再次在我们的挚爱 PySpark shell 中执行下一块代码，如下所示。
- en: Listing 5.4\. Storing data in Hive (full)
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. 在 Hive 中存储数据（完整）
- en: '![](Images/ch05ex04-0.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch05ex04-0.jpg)'
- en: '![](Images/ch05ex04-1.jpg)'
  id: totrans-189
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/ch05ex04-1.jpg)'
- en: Let’s drill deeper into each step for a bit more clarification.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入每个步骤以获得更多的澄清。
- en: 'Step 1: Create and register metadata'
  id: totrans-191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 1 步：创建并注册元数据
- en: Many people prefer to use SQL when they work with data. This is also possible
    with Spark. You can even read and store data in Hive directly as we’ll do. Before
    you can do that, however, you’ll need to create metadata that contains a column
    name and column type for every column.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人更喜欢在处理数据时使用 SQL。使用 Spark 也是可能的。你甚至可以直接读取和存储数据在 Hive 中，就像我们将会做的那样。然而，在这样做之前，你需要创建包含每个列的列名和列类型的元数据。
- en: The first line of code is the imports. The second line parses the field name
    and the field type and specifies if a field is mandatory. The `StructType` represents
    rows as an array of structfields. Then you place it in a dataframe that’s registered
    as a (temporary) table in Hive.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 第一行代码是导入。第二行解析字段名和字段类型，并指定字段是否为必需的。`StructType` 将行表示为一个结构字段的数组。然后你将其放置在一个注册为（临时）表的
    dataframe 中。
- en: '[PRE6]'
  id: totrans-194
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: With the metadata ready, we’re now able to insert the data into Hive.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 元数据准备就绪后，我们现在能够将数据插入到 Hive 中。
- en: 'Step 2: Execute queries and store table in Hive'
  id: totrans-196
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第 2 步：执行查询并将表存储在 Hive 中
- en: Now we’re ready to use a SQL-dialect on our data. First we’ll make a summary
    table that counts the number of loans per purpose. Then we store a subset of the
    cleaned raw data in Hive for visualization in Qlik.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经准备好在数据上使用 SQL 语法。首先，我们将创建一个汇总表，统计每个目的的贷款数量。然后，我们将清洗后的原始数据的一个子集存储在 Hive
    中，以便在 Qlik 中进行可视化。
- en: Executing SQL-like commands is as easy as passing a string that contains the
    SQL-command to the `sqlContext.sql` function. Notice that we aren’t writing pure
    SQL because we’re communicating directly with Hive. Hive has its own SQL-dialect
    called HiveQL. In our SQL, for instance, we immediately tell it to store the data
    as a Parquet file. Parquet is a popular big data file format.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 执行类似 SQL 的命令就像传递一个包含 SQL 命令的字符串到 `sqlContext.sql` 函数一样简单。请注意，我们不是在编写纯 SQL，因为我们正在直接与
    Hive 通信。Hive 有自己的 SQL 语法，称为 HiveQL。在我们的 SQL 中，例如，我们立即告诉它将数据存储为 Parquet 文件。Parquet
    是一种流行的大数据文件格式。
- en: '[PRE7]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: With the data stored in Hive, we can connect our visualization tools to it.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 数据存储在 Hive 中后，我们可以将我们的可视化工具连接到它。
- en: '5.2.4\. Step 4: Data exploration & Step 6: Report building'
  id: totrans-201
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.4\. 第 4 步：数据探索 & 第 6 步：报告构建
- en: We’ll build an interactive report with Qlik Sense to show to our manager. Qlik
    Sense can be downloaded from [http://www.qlik.com/try-or-buy/download-qlik-sense](http://www.qlik.com/try-or-buy/download-qlik-sense)
    after subscribing to their website. When the download begins you will be redirected
    to a page containing several informational videos on how to install and work with
    Qlik Sense. It’s recommended to watch these first.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用Qlik Sense构建一个交互式报告，以展示给我们的经理。在订阅他们的网站后，可以从[http://www.qlik.com/try-or-buy/download-qlik-sense](http://www.qlik.com/try-or-buy/download-qlik-sense)下载Qlik
    Sense。下载开始时，您将被重定向到一个包含有关如何安装和使用Qlik Sense的多个信息视频的页面。建议您先观看这些视频。
- en: We use the Hive ODBC connector to read data from Hive and make it available
    for Qlik. A tutorial on installing ODBC connectors in Qlik is available. For major
    operating systems, this can be found at [http://hortonworks.com/hdp/addons/](http://hortonworks.com/hdp/addons/).
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用Hive ODBC连接器从Hive读取数据并将其提供给Qlik。有关在Qlik中安装ODBC连接器的教程可用。对于主要操作系统，这可以在[http://hortonworks.com/hdp/addons/](http://hortonworks.com/hdp/addons/)找到。
- en: '|  |'
  id: totrans-204
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-205
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In Windows, this might not work out of the box. Once you install the ODBC, make
    sure to check your Windows ODBC manager (CTRL+F and look for ODBC). In the manager,
    go to “System-DSN” and select the “Sample Hive Hortonworks DSN”. Make sure your
    settings are correct (as shown in [figure 5.11](#ch05fig11)) or Qlik won’t connect
    to the Hortonworks Sandbox.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在Windows上，这可能不会立即工作。一旦安装了ODBC，请确保检查您的Windows ODBC管理器（按CTRL+F并查找ODBC）。在管理器中，转到“系统-DSN”并选择“Sample
    Hive Hortonworks DSN”。确保您的设置正确（如图[图5.11](#ch05fig11)所示），否则Qlik将无法连接到Hortonworks
    Sandbox。
- en: Figure 5.11\. Windows Hortonworks ODBC configuration
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.11\. Windows Hortonworks ODBC配置
- en: '![](Images/05fig11.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig11.jpg)'
- en: '|  |'
  id: totrans-209
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s hope you didn’t forget your Sandbox password; as you can see in [figure
    5.11](#ch05fig11), you need it again.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 希望您没有忘记Sandbox密码；如[图5.11](#ch05fig11)所示，您还需要它。
- en: Now open Qlik Sense. If installed in Windows you should have gotten the option
    to place a shortcut to the .exe on your desktop. Qlik isn’t freeware; it’s a commercial
    product with a bait version for single customers, but it will suffice for now.
    In the last chapter we’ll create a dashboard using free JavaScript libraries.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 现在打开Qlik Sense。如果在Windows上安装，您应该在桌面上获得将.exe快捷方式放置在桌面上的选项。Qlik不是免费软件；它是一个商业产品，为单个客户提供诱饵版本，但就目前而言，它足够了。在最后一章中，我们将使用免费的JavaScript库创建仪表板。
- en: Qlik can either take the data directly into memory or make a call every time
    to Hive. We’ve chosen the first method because it works faster.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: Qlik可以直接将数据加载到内存中，或者每次都调用Hive。我们选择了第一种方法，因为它更快。
- en: 'This part has three steps:'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这部分有三个步骤：
- en: '**1**.  Load data inside Qlik with an ODBC connection.'
  id: totrans-214
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 使用ODBC连接在Qlik中加载数据。'
- en: '**2**.  Create the report.'
  id: totrans-215
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 创建报告。'
- en: '**3**.  Explore data.'
  id: totrans-216
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 探索数据。'
- en: Let start with the first step, loading data into Qlik.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从第一步开始，将数据加载到Qlik中。
- en: 'Step 1: Load data in Qlik'
  id: totrans-218
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 步骤1：在Qlik中加载数据
- en: When you start Qlik Sense it will show you a welcome screen with the existing
    reports (called apps), as shown in [figure 5.12](#ch05fig12).
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 当您启动Qlik Sense时，它将显示一个包含现有报告（称为应用）的欢迎屏幕，如图[图5.12](#ch05fig12)所示。
- en: Figure 5.12\. The Qlik Sense welcome screen
  id: totrans-220
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.12\. Qlik Sense欢迎屏幕
- en: '![](Images/05fig12_alt.jpg)'
  id: totrans-221
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig12_alt.jpg)'
- en: To start a new app, click on the *Create new app* button on the right of the
    screen, as shown in [figure 5.13](#ch05fig13). This opens up a new dialog box.
    Enter “[chapter 5](#ch05)” as the new name of our app.
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动一个新的应用，请点击屏幕右侧的“创建新应用”按钮，如图[图5.13](#ch05fig13)所示。这会打开一个新的对话框。将我们的应用的新名称输入为“[第5章](#ch05)”。
- en: Figure 5.13\. The Create new app message box
  id: totrans-223
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.13\. 创建新应用的消息框
- en: '![](Images/05fig13.jpg)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig13.jpg)'
- en: A confirmation box appears ([figure 5.14](#ch05fig14)) if the app is created
    successfully.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 如果应用创建成功，将出现一个确认框（[图5.14](#ch05fig14)）。
- en: Figure 5.14\. A box confirms that the app was created successfully.
  id: totrans-226
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.14\. 一个框确认应用已成功创建。
- en: '![](Images/05fig14.jpg)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig14.jpg)'
- en: Click on the Open app button and a new screen will prompt you to add data to
    the application ([figure 5.15](#ch05fig15)).
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“打开应用”按钮，将弹出一个新屏幕，提示您向应用添加数据（[图5.15](#ch05fig15)）。
- en: Figure 5.15\. A start-adding-data screen pops up when you open a new app.
  id: totrans-229
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.15\. 当您打开新应用时，会弹出添加数据屏幕。
- en: '![](Images/05fig15.jpg)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig15.jpg)'
- en: Click on the Add data button and choose ODBC as a data source ([figure 5.16](#ch05fig16)).
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 点击“添加数据”按钮，选择ODBC作为数据源（[图5.16](#ch05fig16)）。
- en: Figure 5.16\. Choose ODBC as data source in the Select a data source screen
  id: totrans-232
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.16\. 在选择数据源屏幕中选择ODBC作为数据源
- en: '![](Images/05fig16_alt.jpg)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig16_alt.jpg)'
- en: In the next screen ([figure 5.17](#ch05fig17)) select User DSN, Hortonworks,
    and specify the root as username and hadoop as a password (or the new one you
    gave when logging into the Sandbox for the first time).
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一屏（图5.17）中选择用户DSN，Hortonworks，并将根目录指定为用户名，密码为hadoop（或您首次登录Sandbox时提供的密码）。
- en: Figure 5.17\. Choose Hortonworks on the User DSN and specify the username and
    password.
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.17\. 在用户DSN中选择Hortonworks并指定用户名和密码。
- en: '![](Images/05fig17_alt.jpg)'
  id: totrans-236
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig17_alt.jpg)'
- en: '|  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The Hortonworks option doesn’t show up by default. You need to install the HDP
    2.3 ODBC connector for this option to appear (as stated before). If you haven’t
    succeeded in installing it at this point, clear instructions for this can be found
    at [https://blogs.perficient.com/multi-shoring/blog/2015/09/29/how-to-connect-hortonworks-hive-from-qlikview-with-odbc-driver/](https://blogs.perficient.com/multi-shoring/blog/2015/09/29/how-to-connect-hortonworks-hive-from-qlikview-with-odbc-driver/).
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: Hortonworks选项默认不显示。您需要安装HDP 2.3 ODBC连接器才能使此选项显示（如前所述）。如果您在此阶段尚未成功安装它，可以在[https://blogs.perficient.com/multi-shoring/blog/2015/09/29/how-to-connect-hortonworks-hive-from-qlikview-with-odbc-driver/](https://blogs.perficient.com/multi-shoring/blog/2015/09/29/how-to-connect-hortonworks-hive-from-qlikview-with-odbc-driver/)找到有关此的详细说明。
- en: '|  |'
  id: totrans-240
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Click on the arrow pointing to the right to go to the next screen.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 点击指向右侧的箭头以进入下一屏。
- en: Choose the Hive data, and default as user in the next screen ([figure 5.18](#ch05fig18)).
    Select raw as Tables to select and select every column for import; then click
    the button Load and Finish to complete this step.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一屏（[图5.18](#ch05fig18)）中选择Hive数据，默认为用户。选择“原始”作为表以选择，并选择所有列进行导入；然后点击“加载并完成”按钮以完成此步骤。
- en: Figure 5.18\. Hive interface raw data column overview
  id: totrans-243
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.18\. Hive界面原始数据列概述
- en: '![](Images/05fig18_alt.jpg)'
  id: totrans-244
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig18_alt.jpg)'
- en: After this step, it will take a few seconds to load the data in Qlik ([figure
    5.19](#ch05fig19)).
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤之后，将花费几秒钟时间在Qlik中加载数据（[图5.19](#ch05fig19)）。
- en: Figure 5.19\. A confirmation that the data is loaded in Qlik
  id: totrans-246
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.19\. 数据已加载到Qlik的确认
- en: '![](Images/05fig19_alt.jpg)'
  id: totrans-247
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig19_alt.jpg)'
- en: 'Step 2: Create the report'
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 第2步：创建报告
- en: Choose Edit the sheet to start building the report. This will add the report
    editor ([figure 5.20](#ch05fig20)).
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 选择“编辑工作表”以开始构建报告。这将添加报告编辑器（[图5.20](#ch05fig20)）。
- en: Figure 5.20\. An editor screen for reports opens
  id: totrans-250
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.20\. 报告编辑器屏幕
- en: '![](Images/05fig20_alt.jpg)'
  id: totrans-251
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig20_alt.jpg)'
- en: '**Substep 1: Adding a selection filter to the report** The first thing we’ll
    add to the report is a selection box that shows us why each person wants a loan.
    To achieve this, drop the title measure from the left asset panel on the report
    pane and give it a comfortable size and position ([figure 5.21](#ch05fig21)).
    Click on the Fields table so you can drag and drop fields.'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: '**子步骤1：向报告中添加选择过滤器** 我们将首先向报告中添加一个选择框，以显示每个人为什么想要贷款。为此，将左侧资产面板中的标题度量拖放到报告面板，并给它一个舒适的大小和位置（[图5.21](#ch05fig21)）。点击字段表，以便您可以拖放字段。'
- en: Figure 5.21\. Drag the title from the left Fields pane to the report pane.
  id: totrans-253
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.21\. 将标题从左侧字段窗格拖动到报告窗格。
- en: '![](Images/05fig21_alt.jpg)'
  id: totrans-254
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig21_alt.jpg)'
- en: '**Substep 2: Adding a KPI to the report** A KPI chart shows an aggregated number
    for the total population that’s selected. Numbers such as the average interest
    rate and the total number of customers are shown in this chart ([figure 5.22](#ch05fig22)).'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: '**子步骤2：向报告中添加KPI** KPI图表显示了所选总人口的聚合数值。图表中显示了平均利率和客户总数等数值（[图5.22](#ch05fig22)）。'
- en: Figure 5.22\. An example of a KPI chart
  id: totrans-256
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.22\. KPI图表的示例
- en: '![](Images/05fig22.jpg)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig22.jpg)'
- en: Adding a KPI to a report takes four steps, as listed below and shown in [figure
    5.23](#ch05fig23).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 向报告中添加KPI需要四个步骤，如下所示，并在[图5.23](#ch05fig23)中展示。
- en: Figure 5.23\. The four steps to add a KPI chart to a Qlik report
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.23\. 向Qlik报告中添加KPI图表的四个步骤
- en: '![](Images/05fig23_alt.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/05fig23_alt.jpg)'
- en: '**1**.  *Choose a chart*—Choose KPI as the chart and place it on the report
    screen; resize and position to your liking.'
  id: totrans-261
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *选择一个图表*—选择KPI作为图表，并将其放置在报告屏幕上；根据您的喜好调整大小和位置。'
- en: '**2**.  *Add a measure*—Click the add measure button inside the chart and select
    int_rate.'
  id: totrans-262
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *添加一个度量*—点击图表内的添加度量按钮，并选择int_rate。'
- en: '**3**.  *Choose an aggregation method*—Avg(int_rate).'
  id: totrans-263
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  *选择一个聚合方法*—Avg(int_rate).'
- en: '**4**.  *Format the chart*—On the right pane, fill in average interest rate
    as Label.'
  id: totrans-264
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  *格式化图表*—在右侧面板中，填写平均利率作为标签。'
- en: 'In total we’ll add four KPI charts to our report, so you’ll need to repeat
    these steps for the following KPI’s:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 总共我们将向报告中添加四个关键绩效指标图表，因此您需要为以下关键绩效指标重复这些步骤：
- en: Average interest rate
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均利率
- en: Total loan amount
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总贷款金额
- en: Average loan amount
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平均贷款金额
- en: Total recoveries
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总回收金额
- en: '**Substep 3: Adding bar charts to our report** Next we’ll add four bar charts
    to the report. These will show the different numbers for each risk grade. One
    bar chart will explain the average interest rate per risk group, and another will
    show us the total loan amount per risk group ([figure 5.24](#ch05fig24)).'
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: '**子步骤3：将柱状图添加到报告中** 接下来，我们将向报告中添加四个柱状图。这些图表将展示每个风险等级的不同数值。一个柱状图将解释每个风险组的平均利率，另一个将显示每个风险组的总贷款金额（[图5.24](#ch05fig24)）。'
- en: Figure 5.24\. An example of a bar chart
  id: totrans-271
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.24\. 柱状图的示例
- en: '![](Images/05fig24.jpg)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig24.jpg)'
- en: Adding a bar chart to a report takes five steps, as listed below and shown in
    [figure 5.25](#ch05fig25).
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 将柱状图添加到报告中需要五个步骤，如下所示，并在[图5.25](#ch05fig25)中展示。
- en: Figure 5.25\. Adding a bar chart takes five steps.
  id: totrans-274
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.25\. 添加柱状图需要五个步骤。
- en: '![](Images/05fig25_alt.jpg)'
  id: totrans-275
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig25_alt.jpg)'
- en: '**1**.  *Choose a chart*—Choose bar chart as the chart and place it on the
    report screen; resize and position to your liking.'
  id: totrans-276
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *选择图表*—选择柱状图作为图表，并将其放置在报告屏幕上；根据您的喜好调整大小和位置。'
- en: '**2**.  *Add a measure*—Click the Add measure button inside the chart and select
    int_rate.'
  id: totrans-277
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *添加度量值*—点击图表内的“添加度量值”按钮，并选择int_rate。'
- en: '**3**.  *Choose an aggregation method*—Avg(int_rate).'
  id: totrans-278
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  *选择聚合方法*—Avg(int_rate)。'
- en: '**4**.  *Add a dimension*—Click Add dimension, and choose grade as the dimension.'
  id: totrans-279
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  *添加维度*—点击“添加维度”，并选择grade作为维度。'
- en: '**5**.  *Format the chart*—On the right pane, fill in average interest rate
    as Label.'
  id: totrans-280
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  *格式化图表*—在右侧面板中，填写平均利率作为标签。'
- en: 'Repeat this procedure for the following dimension and measure combinations:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 重复此程序，以下维度和度量值组合：
- en: Average interest rate per grade
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个等级的平均利率
- en: Average loan amount per grade
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个等级的平均贷款金额
- en: Total loan amount per grade
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个等级的总贷款金额
- en: Total recoveries per grade
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每个等级的总回收金额
- en: '**Substep 4: Adding a cross table to the report** Suppose you want to know
    the average interest rate paid by directors of risk group C. In this case you
    want to get a measure (interest rate) for a combination of two dimensions (job
    title and risk grade). This can be achieved with a pivot table such as in [figure
    5.26](#ch05fig26).'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '**子步骤4：将交叉表添加到报告中** 假设您想知道风险组C的董事支付的平均利率。在这种情况下，您需要为两个维度（职位和风险等级）的组合获取一个度量值（利率）。这可以通过如图5.26所示的交叉表实现。'
- en: Figure 5.26\. An example of a pivot table, showing the average interest rate
    paid per job title/risk grade combination
  id: totrans-287
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.26\. 交叉表的示例，显示了按职位/风险等级组合支付的平均利率
- en: '![](Images/05fig26_alt.jpg)'
  id: totrans-288
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig26_alt.jpg)'
- en: Adding a pivot table to a report takes six steps, as listed below and shown
    in [figure 5.27](#ch05fig27).
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 将交叉表添加到报告中需要六个步骤，如下所示，并在[图5.27](#ch05fig27)中展示。
- en: Figure 5.27\. Adding a pivot table takes six steps.
  id: totrans-290
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.27\. 将交叉表添加到报告中需要六个步骤。
- en: '![](Images/05fig27_alt.jpg)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig27_alt.jpg)'
- en: '**1**.  *Choose a chart*—Choose pivot table as the chart and place it on the
    report screen; resize and position to your liking.'
  id: totrans-292
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  *选择图表*—选择交叉表作为图表，并将其放置在报告屏幕上；根据您的喜好调整大小和位置。'
- en: '**2**.  *Add a measure*—Click the Add measure button inside the chart and select
    int_rate.'
  id: totrans-293
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  *添加度量值*—点击图表内的“添加度量值”按钮，并选择int_rate。'
- en: '**3**.  *Choose an aggregation method*—Avg(int_rate).'
  id: totrans-294
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  *选择聚合方法*—Avg(int_rate)。'
- en: '**4**.  *Add a row dimension*—Click Add dimension, and choose emp_title as
    the dimension.'
  id: totrans-295
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**.  *添加行维度*—点击“添加维度”，并选择emp_title作为维度。'
- en: '**5**.  *Add a column dimension*—Click Add data, choose column, and select
    grade.'
  id: totrans-296
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**.  *添加列维度*—点击“添加数据”，选择列，并选择grade。'
- en: '**6**.  *Format the chart*—On the right pane, fill in average interest rate
    as Label.'
  id: totrans-297
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**6**.  *格式化图表*—在右侧面板中，填写平均利率作为标签。'
- en: After resizing and repositioning, you should achieve a result similar to [figure
    5.28](#ch05fig28). Click the Done button on the left and you’re ready to explore
    the data.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 调整大小和重新定位后，您应该得到一个类似于[图5.28](#ch05fig28)的结果。点击左侧的“完成”按钮，您就可以开始探索数据了。
- en: Figure 5.28\. The end result in edit mode
  id: totrans-299
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.28\. 编辑模式下的最终结果
- en: '![](Images/05fig28_alt.jpg)'
  id: totrans-300
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig28_alt.jpg)'
- en: 'Step 3: Explore the data'
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 步骤3：探索数据
- en: The result is an interactive graph that updates itself based on the selections
    you make. Why don’t you try to look for the information from directors and compare
    them to artists? To achieve this, hit the emp_title in the pivot table and type
    director in the search field. The result looks like [figure 5.29](#ch05fig29).
    In the same manner, we can look at the artists, as shown in [figure 5.30](#ch05fig30).
    Another interesting insight comes from comparing the rating for home-buying purposes
    with debt consolidation purposes.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 结果是一个根据你选择的选项自动更新的交互式图表。你为什么不尝试从导演那里寻找信息，并将它们与艺术家进行比较呢？为了实现这一点，点击交叉表中的 emp_title
    并在搜索字段中输入导演。结果看起来像[图 5.29](#ch05fig29)。以同样的方式，我们可以查看艺术家，如图[图 5.30](#ch05fig30)所示。另一个有趣的见解来自于比较用于购房目的的评级与债务重组目的的评级。
- en: Figure 5.29\. When we select directors, we can see that they pay an average
    rate of 11.97% for a loan.
  id: totrans-303
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.29\. 当我们选择导演时，我们可以看到他们为贷款支付的平均利率为 11.97%。
- en: '![](Images/05fig29_alt.jpg)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig29_alt.jpg)'
- en: Figure 5.30\. When we select artists, we see that they pay an average interest
    rate of 13.32% for a loan.
  id: totrans-305
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.30\. 当我们选择艺术家时，我们看到他们为贷款支付的平均利率为 13.32%。
- en: '![](Images/05fig30_alt.jpg)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/05fig30_alt.jpg)'
- en: 'We finally did it: We created the report our manager craves, and in the process
    we opened the door for other people to create their own reports using this data.
    An interesting next step for you to ponder on would be to use this setup to find
    those people likely to default on their debt. For this you can use the Spark Machine
    learning capabilities driven by online algorithms like the ones demonstrated in
    [chapter 4](kindle_split_012.xhtml#ch04).'
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 我们终于做到了：我们创建了经理渴望的报告，在这个过程中，我们也为其他人使用这些数据创建自己的报告打开了大门。你可以思考的一个有趣的下一步是使用这个设置来找到那些可能违约债务的人。为此，你可以使用由在线算法驱动的
    Spark 机器学习功能，如第 4 章中演示的那样[章节 4](kindle_split_012.xhtml#ch04)。
- en: In this chapter we got a hands-on introduction to the Hadoop and Spark frameworks.
    We covered a lot of ground, but be honest, Python makes working with big data
    technologies dead easy. In the next chapter we’ll dig deeper into the world of
    NoSQL databases and come into contact with more big data technologies.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们亲身体验了 Hadoop 和 Spark 框架。我们覆盖了很多内容，但说实话，Python 使得与大数据技术打交道变得极其简单。在下一章中，我们将更深入地探讨
    NoSQL 数据库的世界，并接触到更多的大数据技术。
- en: 5.3\. Summary
  id: totrans-309
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3\. 摘要
- en: In this chapter you learned that
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你了解到
- en: Hadoop is a framework that enables you to store files and distribute calculations
    amongst many computers.
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 是一个框架，它使你能够存储文件并在多台计算机之间分配计算。
- en: Hadoop hides all the complexities of working with a cluster of computers for
    you.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 为你隐藏了与计算机集群一起工作的所有复杂性。
- en: An ecosystem of applications surrounds Hadoop and Spark, ranging from databases
    to access control.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Hadoop 和 Spark 周围围绕着一系列的应用生态系统，从数据库到访问控制。
- en: Spark adds a shared memory structure to the Hadoop Framework that’s better suited
    for data science work.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark 向 Hadoop 框架中添加了一个更适合数据科学工作的共享内存结构。
- en: In the chapter case study we used PySpark (a Python library) to communicate
    with Hive and Spark from Python. We used the pywebhdfs Python library to work
    with the Hadoop library, but you could do as well using the OS command line.
  id: totrans-315
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章案例研究中，我们使用了 PySpark（一个 Python 库）从 Python 与 Hive 和 Spark 通信。我们使用了 pywebhdfs
    Python 库来与 Hadoop 库一起工作，但你也可以使用操作系统命令行来完成这项工作。
- en: It’s easy to connect a BI tool such as Qlik to Hadoop.
  id: totrans-316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 BI 工具如 Qlik 连接到 Hadoop 非常容易。
