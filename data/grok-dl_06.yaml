- en: 'Chapter 7\. How to picture neural networks: in your head and on paper'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. 如何在头脑中和纸上描绘神经网络
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**在本章中**'
- en: Correlation summarization
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相关性总结
- en: Simplified visualization
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简化可视化
- en: Seeing the network predict
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 观察网络预测
- en: Visualizing using letters instead of pictures
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用字母而不是图片来可视化
- en: Linking variables
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连接变量
- en: The importance of visualization tools
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可视化工具的重要性
- en: “Numbers have an important story to tell. They rely on you to give them a clear
    and convincing voice.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “数字有一个重要的故事要讲。它们依赖于你给予它们一个清晰而有说服力的声音。”
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Stephen Few, IT innovator, teacher, and consultant*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*斯蒂芬·弗，IT创新者、教师和顾问*'
- en: It’s time to simplify
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 是时候简化了
- en: It’s impractical to think about everything all the time. Mental tools can help
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 总是考虑所有事情是不切实际的。心理工具可以帮助
- en: '[Chapter 6](kindle_split_014.xhtml#ch06) finished with a code example that
    was quite impressive. Just the neural network contained 35 lines of incredibly
    dense code. Reading through it, it’s clear there’s a lot going on; and that code
    includes over 100 pages of concepts that, when combined, can predict whether it’s
    safe to cross the street.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: '[第6章](kindle_split_014.xhtml#ch06)以一个相当令人印象深刻的代码示例结束。仅仅神经网络就包含了35行非常密集的代码。阅读它时，很明显有很多事情在进行；并且这段代码包括超过100页的概念，当结合在一起时，可以预测是否安全过马路。'
- en: I hope you’re continuing to rebuild these examples from memory in each chapter.
    As the examples get larger, this exercise becomes less about remembering specific
    letters of code and more about remembering concepts and then rebuilding the code
    based on those concepts.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望你在每一章中都能继续通过记忆来重建这些示例。随着示例的增大，这个练习就不再仅仅是记住代码的具体字母，而是更多地关于记住概念，然后根据这些概念重建代码。
- en: In this chapter, this construction of efficient concepts in your mind is exactly
    what I want to talk about. Even though it’s not an architecture or experiment,
    it’s perhaps the most important value I can give you. In this case, I want to
    show how I summarize all the little lessons in an efficient way in my mind so
    that I can do things like build new architectures, debug experiments, and use
    an architecture on new problems and new datasets.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，这种在你的脑海中构建高效概念的结构正是我想讨论的。尽管它不是一个架构或实验，但它可能是我能给你提供的最重要的价值。在这种情况下，我想展示我是如何在我的脑海中以高效的方式总结所有的小课程，以便我能做诸如构建新的架构、调试实验以及在新的问题和新的数据集上使用架构之类的事情。
- en: Let’s start by reviewing the concepts you’ve learned so far
  id: totrans-16
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们先回顾一下你到目前为止学到的概念
- en: This book began with small lessons and then built layers of abstraction on top
    of them. We began by talking about the ideas behind machine learning in general.
    Then we progressed to how individual linear nodes (or *neurons*) learned, followed
    by horizontal groups of neurons (layers) and then vertical groups (stacks of layers).
    Along the way, we discussed how learning is actually just reducing error to 0,
    and we used calculus to discover how to change each weight in the network to help
    move the error in the direction of 0.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书从小的课程开始，然后在它们之上构建抽象层。我们首先讨论了机器学习背后的思想。然后我们进步到单个线性节点（或*神经元*）如何学习，然后是水平组神经元（层）和垂直组（层堆叠）。在这个过程中，我们讨论了学习实际上只是将错误减少到0，我们使用微积分来发现如何改变网络中的每个权重，以帮助将错误移动到0的方向。
- en: 'Next, we discussed how neural networks search for (and sometimes create) correlation
    between the input and output datasets. This last idea allowed us to overlook the
    previous lessons on how individual neurons behaved because it concisely summarizes
    the previous lessons. The sum total of the neurons, gradients, stacks of layers,
    and so on lead to a single idea: neural networks find and create correlation.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们讨论了神经网络如何搜索（有时创建）输入和输出数据集之间的相关性。这个最后的想法使我们能够忽略之前关于单个神经元行为的课程，因为它简洁地总结了之前的课程。神经元、梯度、层堆叠等总和导致一个单一的想法：神经网络寻找并创建相关性。
- en: 'Holding onto this idea of correlation instead of the previous smaller ideas
    is important to learning deep learning. Otherwise, it would be easy to become
    overwhelmed with the complexity of neural networks. Let’s create a name for this
    idea: the *correlation summarization*.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 保留这个相关性概念而不是之前较小的概念对于学习深度学习很重要。否则，很容易被神经网络的复杂性所淹没。让我们为这个想法起一个名字：*相关性总结*。
- en: Correlation summarization
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 相关性总结
- en: This is the key to sanely moving forward to more advanced neural networks
  id: totrans-21
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这是向更高级的神经网络前进的合理关键
- en: '|  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Correlation summarization**'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: '**相关性总结**'
- en: Neural networks seek to find direct and indirect correlation between an input
    layer and an output layer, which are determined by the input and output datasets,
    respectively.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络试图找到输入层和输出层之间的直接和间接相关性，这些层分别由输入和输出数据集决定。
- en: '|  |'
  id: totrans-25
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: At the 10,000-foot level, this is what all neural networks do. Given that a
    neural network is really just a series of matrices connected by layers, let’s
    zoom in slightly and consider what any particular weight matrix is doing.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在10,000英尺的高度上，这是所有神经网络都在做的事情。鉴于神经网络实际上只是一系列通过层连接的矩阵，让我们稍微放大一下，考虑一下任何特定的权重矩阵正在做什么。
- en: '|  |'
  id: totrans-27
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Local correlation summarization**'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: '**局部相关性总结**'
- en: Any given set of weights optimizes to learn how to correlate its input layer
    with what the output layer says it should be.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一组权重都会优化以学习如何将其输入层与输出层所说的内容相关联。
- en: '|  |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When you have only two layers (input and output), the weight matrix knows what
    the output layer says it should be based on the output dataset. It looks for correlation
    between the input and output datasets because they’re captured in the input and
    output layers. But this becomes more nuanced when you have multiple layers, remember?
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 当你只有两层（输入和输出）时，权重矩阵知道输出层基于输出数据集所说的内容。它寻找输入和输出数据集之间的相关性，因为它们被捕获在输入和输出层中。但当你有多个层时，这会变得更加复杂，记得吗？
- en: '|  |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Global correlation summarization**'
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: '**全局相关性总结**'
- en: What an earlier layer says it should be can be determined by taking what a later
    layer says it should be and multiplying it by the weights in between them. This
    way, later layers can tell earlier layers what kind of signal they need, to ultimately
    find correlation with the output. This cross-communication is called *backpropagation*.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 一个早期层所说的内容可以通过将后续层所说的内容乘以它们之间的权重来确定。这样，后续层可以告诉早期层它们需要的信号类型，最终找到与输出的相关性。这种交叉通信被称为*反向传播*。
- en: '|  |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When global correlation teaches each layer what it should be, local correlation
    can optimize weights locally. When a neuron in the final layer says, “I need to
    be a little higher,” it then proceeds to tell all the neurons in the layer immediately
    preceding it, “Hey, previous layer, send me higher signal.” They then tell the
    neurons preceding them, “Hey. Send us higher signal.” It’s like a giant game of
    telephone—at the end of the game, every layer knows which of its neurons need
    to be higher and lower, and the local correlation summarization takes over, updating
    the weights accordingly.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 当全局相关性教给每一层它应该是什么时，局部相关性可以在局部优化权重。当最终层的神经元说，“我需要稍微高一点”时，它就会继续告诉它前面层的所有神经元，“嘿，前一层，给我更高的信号。”然后它们会告诉它们前面的神经元，“嘿，给我们更高的信号。”这就像一场巨大的电话游戏——游戏结束时，每一层都知道它的哪些神经元需要更高或更低，然后局部相关性总结接管，相应地更新权重。
- en: The previously overcomplicated visualization
  id: totrans-37
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 之前过于复杂的可视化
- en: While simplifying the mental picture, let’s simplify the visualization as well
  id: totrans-38
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 在简化心理图的同时，让我们也简化一下可视化。
- en: At this point, I expect the visualization of neural networks in your head is
    something like the picture shown here (because that’s the one we used). The input
    dataset is in `layer_0`, connected by a weight matrix (a bunch of lines) to `layer_1`,
    and so on. This was a useful tool to learn the basics of how collections of weights
    and layers come together to learn a function.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在这一点上，我预计你脑海中神经网络的可视化就像这里显示的图片（因为这是我们用的那个）。输入数据集在`layer_0`中，通过权重矩阵（一堆线条）连接到`layer_1`，依此类推。这是一个有用的工具，用于学习如何将权重集合和层组合起来学习一个函数。
- en: But moving forward, this picture has too much detail. Given the correlation
    summarization, you already know you no longer need to worry about how individual
    weights are updated. Later layers already know how to communicate to earlier layers
    and tell them, “Hey, I need higher signal” or “Hey, I need lower signal.” Truth
    be told, you don’t really care about the weight values anymore, only that they’re
    behaving as they should, properly capturing correlation in a way that generalizes.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 但向前推进，这幅图过于详细。考虑到相关性总结，你已经知道你不再需要担心单个权重的更新方式。后续层已经知道如何与早期层通信，并告诉它们，“嘿，我需要更高的信号”或“嘿，我需要更低的信号”。说实话，你不再关心权重值，只关心它们是否按预期行为，以适当的方式捕捉相关性，并实现泛化。
- en: To reflect this change, let’s update the visualization on paper. We’ll also
    do a few other things that will make sense later. As you know, the neural network
    is a series of weight matrices. When you’re using the network, you also end up
    creating vectors corresponding to each layer.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反映这种变化，让我们在纸上更新可视化。我们还将做一些其他事情，这些事情在以后会变得有意义。正如你所知，神经网络是一系列权重矩阵。当你使用网络时，你也会创建与每一层对应的向量。
- en: In the figure, the weight matrices are the lines going from node to node, and
    the vectors are the strips of nodes. For example, `weights_1_2` is a matrix, `weights_0_1`
    is a matrix, and `layer_1` is a vector.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个图中，权重矩阵是节点之间的线条，向量是节点条带。例如，`weights_1_2`是一个矩阵，`weights_0_1`是一个矩阵，`layer_1`是一个向量。
- en: In later chapters, we’ll arrange vectors and matrices in increasingly creative
    ways, so instead of all this detail showing each node connected by each weight
    (which gets hard to read if we have, say, 500 nodes in `layer_1`), let’s instead
    think in general terms. Let’s think of them as vectors and matrices of arbitrary
    size.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将以越来越有创意的方式排列向量和矩阵，因此，而不是显示每个节点通过每个权重连接的细节（如果我们有500个节点在`layer_1`中，这会变得难以阅读），让我们从一般的角度来思考。让我们把它们看作任意大小的向量和矩阵。
- en: '![](Images/f0136-01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0136-01.jpg)'
- en: The simplified visualization
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 简化的可视化
- en: Neural networks are like LEGO bricks, and each brick is a vector or matrix
  id: totrans-46
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络就像乐高积木，每一块积木都是一个向量或矩阵
- en: Moving forward, we’ll build new neural network architectures in the same way
    people build new structures with LEGO pieces. The great thing about the correlation
    summarization is that all the bits and pieces that lead to it (backpropagation,
    gradient descent, alpha, dropout, mini-batching, and so on) don’t depend on a
    particular configuration of the LEGOs. No matter how you piece together the series
    of matrices, gluing them together with layers, the neural network will try to
    learn the pattern in the data by modifying the weights between wherever you put
    the input layer and the output layer.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我们将以人们用乐高积木搭建新结构的方式构建新的神经网络架构。关于相关性总结的妙处在于，导致它的所有片段（反向传播、梯度下降、alpha、dropout、小批量等）并不依赖于乐高积木的特定配置。无论你如何拼接矩阵序列，用层将其粘合在一起，神经网络都会通过修改输入层和输出层之间的权重来尝试学习数据中的模式。
- en: To reflect this, we’ll build all the neural networks with the pieces shown at
    right. The strip is a vector, the box is a matrix, and the circles are individual
    weights. Note that the box can be viewed as a “vector of vectors,” horizontally
    or vertically.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 为了反映这一点，我们将使用右边的部件构建所有神经网络。条带是向量，方框是矩阵，圆圈是单个权重。请注意，方框可以看作是“向量的向量”，水平或垂直排列。
- en: '![](Images/f0137-01.jpg)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0137-01.jpg)'
- en: '![](Images/f0137-02.jpg)'
  id: totrans-50
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0137-02.jpg)'
- en: '|  |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The big takeaway**'
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: '**主要收获**'
- en: 'The picture at left still gives you all the information you need to build a
    neural network. You know the shapes and sizes of all the layers and matrices.
    The detail from before isn’t necessary when you know the correlation summarization
    and everything that went into it. But we aren’t finished: we can simplify even
    further.'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 左边的图片仍然提供了构建神经网络所需的所有信息。你知道所有层和矩阵的形状和大小。当你知道相关性总结及其所有内容时，之前的细节就不再必要了。但我们还没有完成：我们可以进一步简化。
- en: '|  |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Simplifying even further
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 进一步简化
- en: The dimensionality of the matrices is determined by the layers
  id: totrans-56
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 矩阵的维度由层决定
- en: In the previous section, you may have noticed a pattern. Each matrix’s dimensionality
    (number of rows and columns) has a direct relationship to the dimensionality of
    the layers before and after them. Thus, we can simplify the visualization even
    further.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你可能已经注意到了一个模式。每个矩阵的维度（行数和列数）与其前后层的维度有直接关系。因此，我们可以进一步简化可视化。
- en: Consider the visualization shown at right. We still have all the information
    needed to build a neural network. We can infer that `weights_0_1` is a (3 × 4)
    matrix because the previous layer (`layer_0`) has three dimensions and the next
    layer (`layer_1`) has four dimensions. Thus, in order for the matrix to be big
    enough to have a single weight connecting each node in `layer_0` to each node
    in `layer_1`, it must be a (3 × 4) matrix.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑右边的可视化。我们仍然有构建神经网络所需的所有信息。我们可以推断出`weights_0_1`是一个（3 × 4）的矩阵，因为前一层（`layer_0`）有三个维度，下一层（`layer_1`）有四个维度。因此，为了使矩阵足够大，以便有一个单独的权重将`layer_0`中的每个节点与`layer_1`中的每个节点连接起来，它必须是一个（3
    × 4）的矩阵。
- en: This allows us to start thinking about the neural networks using the correlation
    summarization. All this neural network will to do is adjust the weights to find
    correlation between `layer_0` and `layer_2`. It will do this using all the methods
    mentioned so far in this book. But the different configurations of weights and
    layers between the input and output layers have a strong impact on whether the
    network is successful in finding correlation (and/or how fast it finds correlation).
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 这使我们开始思考使用相关性总结的神经网络。所有这些神经网络要做的就是调整权重，以找到`layer_0`和`layer_2`之间的相关性。它将使用本书中提到的所有方法来完成这项工作。但是，输入层和输出层之间权重和层的不同配置对网络是否成功找到相关性（以及/或找到相关性的速度）有重大影响。
- en: '![](Images/f0138-01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0138-01.jpg)'
- en: The particular configuration of layers and weights in a neural network is called
    its *architecture*, and we’ll spend the majority of the rest of this book discussing
    the pros and cons of various architectures. As the correlation summarization reminds
    us, the neural network adjusts weights to find correlation between the input and
    output layers, sometimes even inventing correlation in the hidden layers. Different
    architectures *channel signal to make correlation easier to discover*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络中层的特定配置和权重被称为其*架构*，我们将在本书的剩余大部分内容中讨论各种架构的优缺点。正如相关性总结提醒我们的，神经网络调整权重以找到输入层和输出层之间的相关性，有时甚至在隐藏层中发明相关性。不同的架构*使信号通道更容易发现相关性*。
- en: '|  |'
  id: totrans-62
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Good neural architectures channel signal so that correlation is easy to discover.
    Great architectures also filter noise to help prevent overfitting.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 好的神经网络架构使信号通道，以便更容易发现相关性。伟大的架构还过滤噪声，以帮助防止过拟合。
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Much of the research into neural networks is about finding new architectures
    that can find correlation faster and generalize better to unseen data. We’ll spend
    the vast majority of the rest of this book discussing new architectures.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数关于神经网络的研究都是关于寻找新的架构，这些架构可以更快地找到相关性，并且更好地泛化到未见过的数据。我们将在本书的剩余大部分内容中讨论新的架构。
- en: Let’s see this network predict
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 让我们看看这个网络如何预测
- en: Let’s picture data from the streetlight example flowing through the system
  id: totrans-67
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们想象街灯示例中的数据流经系统
- en: In figure 1, a single datapoint from the streetlight dataset is selected. `layer_0`
    is set to the correct values.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 在图1中，从街灯数据集中选择了一个单个数据点。`layer_0`被设置为正确的值。
- en: '![](Images/f0139-01.jpg)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0139-01.jpg)'
- en: In figure 2, four different weighted sums of `layer_0` are performed. The four
    weighted sums are performed by `weights_0_1`. As a reminder, this process is called
    *vector-matrix multiplication*. These four values are deposited into the four
    positions of `layer_1` and passed through the `relu` function (setting negative
    values to 0). To be clear, the third value from the left in `layer_1` would have
    been negative, but the `relu` function sets it to 0.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在图2中，对`layer_0`执行了四个不同的加权求和。这四个加权求和由`weights_0_1`执行。提醒一下，这个过程被称为*向量矩阵乘法*。这四个值被存入`layer_1`的四个位置，并通过`relu`函数（将负值设置为0）传递。为了清楚起见，`layer_1`中从左数第三个值原本会是负数，但`relu`函数将其设置为0。
- en: '![](Images/f0139-02.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0139-02.jpg)'
- en: As shown in figure 3, final step performs a weighted average of `layer_1`, again
    using the vector-matrix multiplication process. This yields the number 0.9, which
    is the network’s final prediction.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 如图3所示，最终步骤执行了`layer_1`的加权平均，再次使用向量矩阵乘法过程。这产生了数字0.9，这是网络的最终预测。
- en: '![](Images/f0139-03.jpg)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0139-03.jpg)'
- en: '|  |'
  id: totrans-74
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Review: Vector-matrix multiplication**'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: '**复习：向量矩阵乘法**'
- en: Vector-matrix multiplication performs multiple *weighted sums* of a vector.
    The matrix must have the same number of rows as the vector has values, so that
    each column in the matrix performs a unique weighted sum. Thus, if the matrix
    has four columns, four weighted sums will be generated. The weightings of each
    sum are performed depending on the values of the matrix.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 向量矩阵乘法执行多个*加权求和*的向量。矩阵必须具有与向量值相同的行数，以便矩阵中的每一列执行一个独特的加权求和。因此，如果矩阵有四列，将生成四个加权求和。每个求和的加权根据矩阵的值执行。
- en: '|  |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Visualizing using letters instead of pictures
  id: totrans-78
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用字母而不是图片进行可视化
- en: All these pictures and detailed explanations are actually a simpl- le piece
    of algebra
  id: totrans-79
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 所有这些图片和详细解释实际上只是简单的代数
- en: Just as we defined simpler pictures for the matrix and vector, we can perform
    the same visualization in the form of letters.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们为矩阵和向量定义了简单的图示，我们也可以用字母的形式进行相同的可视化。
- en: How do you visualize a *matrix* using math? Pick a capital letter. I try to
    pick one that’s easy to remember, such as *W* for “weights.” The little 0 means
    it’s probably one of several *W*s. In this case, the network has two. Perhaps
    surprisingly, I could have picked any capital letter. The little 0 is an extra
    that lets me call all my weight matrices *W* so I can tell them apart. It’s your
    visualization; make it easy to remember.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 如何用数学来可视化一个 *矩阵*？选择一个大写字母。我尽量选择一个容易记住的，比如 *W* 代表“权重”。小写的 0 表示它可能是几个 *W* 中的一个。在这种情况下，网络有两个。也许令人惊讶的是，我可以选择任何大写字母。小写的
    0 是一个额外的标识，让我可以调用所有的权重矩阵 *W*，以便区分它们。这是你的可视化；让它容易记住。
- en: '![](Images/f0140-01.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0140-01.jpg)'
- en: How do you visualize a *vector* using math? Pick a lowercase letter. Why did
    I choose the letter *l*? Well, because I have a bunch of vectors that are layers,
    I thought *l* would be easy to remember. Why did I choose to call it *l*-zero?
    Because I have multiple layers, it seems nice to make all them *l*s and number
    them instead of having to think of new letters for every layer. There’s no wrong
    answer here.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如何用数学来可视化一个 *向量*？选择一个小写字母。为什么我选择了字母 *l*？嗯，因为我有一堆层向量，我觉得 *l* 容易记住。为什么我选择称之为 *l*-zero？因为我有多个层，让所有它们都变成
    *l* 并编号，而不是为每一层想新字母，看起来很合适。这里没有错误答案。
- en: If that’s how to visualize matrices and vectors in math, what do all the pieces
    in the network look like? At right, you can see a nice selection of variables
    pointing to their respective sections of the neural network. But defining them
    doesn’t show how they relate. Let’s combine the variables via vector-matrix multiplication.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这样在数学中可视化矩阵和向量，那么网络中的所有部分看起来是什么样子？在右侧，你可以看到一组变量指向神经网络各自的区域。但定义它们并不显示它们之间的关系。让我们通过向量-矩阵乘法来组合这些变量。
- en: '![](Images/f0140-02.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0140-02.jpg)'
- en: Linking the variables
  id: totrans-86
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 链接变量
- en: The letters can be combined to indicate functions and operations
  id: totrans-87
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 字母可以组合起来表示函数和操作
- en: 'Vector-matrix multiplication is simple. To visualize that two letters are being
    multiplied by each other, put them next to each other. For example:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 向量-矩阵乘法很简单。为了可视化两个字母相互乘法，将它们并排放置。例如：
- en: '| Algebra | Translation |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '| 代数 | 翻译 |'
- en: '| --- | --- |'
  id: totrans-90
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| *l*[0]*W*[0] | “Take the layer 0 vector and perform vector-matrix multiplication
    with the weight matrix 0.” |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '| *l*[0]*W*[0] | “使用层 0 向量与权重矩阵 0 进行向量-矩阵乘法。” |'
- en: '| *l*[1]*W*[1] | “Take the layer 1 vector and perform vector-matrix multiplication
    with the weight matrix 1.” |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '| *l*[1]*W*[1] | “取层 1 向量并与权重矩阵 1 进行向量-矩阵乘法。” |'
- en: You can even throw in arbitrary functions like `relu` using notation that looks
    almost exactly like the Python code. This is crazy-intuitive stuff.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 你甚至可以加入像 `relu` 这样的任意函数，使用看起来几乎与 Python 代码完全相同的符号。这是疯狂直观的东西。
- en: '| *l*[1] = *relu*(*l*[0]*W*[0]) | “To create the layer 1 vector, take the layer
    0 vector and perform vector-matrix multiplication with the weight matrix 0; then
    perform the relu function on the output (setting all negative numbers to 0).”
    |'
  id: totrans-94
  prefs: []
  type: TYPE_TB
  zh: '| *l*[1] = *relu*(*l*[0]*W*[0]) | “为了创建层 1 向量，取层 0 向量并与权重矩阵 0 进行向量-矩阵乘法；然后对输出执行
    relu 函数（将所有负数设置为 0）。” |'
- en: '| *l*[2] = *l*[1]*W*[1] | “To create the layer 2 vector, take the layer 1 vector
    and perform vector-matrix multiplication with the weight matrix 1.” |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| *l*[2] = *l*[1]*W*[1] | “为了创建层 2 向量，取层 1 向量并与权重矩阵 1 进行向量-矩阵乘法。” |'
- en: If you notice, the layer 2 algebra contains layer 1 as an input variable. This
    means you can represent the *entire neural network* in one expression by chaining
    them together.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你注意到，层 2 的代数包含了层 1 作为输入变量。这意味着你可以通过链接它们来用一个表达式表示整个 *神经网络*。
- en: '| *l*[2] = *relu*(*l*[0]*W*[0])*W*[1] | Thus, all the logic in the forward
    propagation step can be contained in this one formula. Note: baked into this formula
    is the assumption that the vectors and matrices have the right dimensions. |'
  id: totrans-97
  prefs: []
  type: TYPE_TB
  zh: '| *l*[2] = *relu*(*l*[0]*W*[0])*W*[1] | 因此，前向传播步骤中的所有逻辑都可以包含在这个公式中。注意：这个公式中包含了向量和矩阵具有正确维度的假设。|'
- en: Everything side by side
  id: totrans-98
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 所有东西并排在一起
- en: Let’s see the visualization, algebra formula, and Python code in one place
  id: totrans-99
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们在一个地方看到可视化、代数公式和 Python 代码。
- en: I don’t think much dialogue is necessary on this page. Take a minute and look
    at each piece of forward propagation through these four different ways of seeing
    it. It’s my hope that you’ll truly grok forward propagation and understand the
    architecture by seeing it from different perspectives, all in one place.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为在这个页面上不需要太多对话。花一分钟时间看看通过这四种不同的方式看到的前向传播的每一部分。我希望你能真正理解前向传播，并通过从不同角度观察，在一个地方理解架构。
- en: '![](Images/f0142-01_alt.jpg)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0142-01_alt.jpg)'
- en: The importance of visualization tools
  id: totrans-102
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 可视化工具的重要性
- en: We’re going to be studying new architectures
  id: totrans-103
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 我们将研究新的架构
- en: In the following chapters, we’ll be taking these vectors and matrices and combining
    them in some creative ways. My ability to describe each architecture for you is
    entirely dependent on our having a mutually agreed-on language for describing
    them. Thus, please don’t move beyond this chapter until you can clearly see how
    forward propagation manipulates these vectors and matrices, and how these various
    forms of describing them are articulated.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的章节中，我们将以一些创造性的方式将这些矢量和矩阵结合起来。我描述每个架构的能力完全取决于我们是否有一个共同的语言来描述它们。因此，请在你能够清楚地看到前向传播如何操作这些矢量和矩阵，以及如何用各种形式描述它们之前，不要超出这一章。
- en: '|  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Key takeaway**'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: '**关键要点**'
- en: Good neural architectures channel signal so that correlation is easy to discover.
    Great architectures also filter noise to help prevent overfitting.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 良好的神经网络架构能够有效地引导信号，使得相关性易于发现。优秀的架构还能够过滤噪声，帮助防止过拟合。
- en: '|  |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: As mentioned previously, a neural architecture controls how signal flows through
    a network. How you create these architectures will affect the ways in which the
    network can detect correlation. You’ll find that you want to create architectures
    that maximize the network’s ability to focus on the areas where meaningful correlation
    exists, and minimize the network’s ability to focus on the areas that contain
    noise.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，神经网络架构控制着信号在网络中的流动方式。你创建这些架构的方式将影响网络检测相关性的方式。你会发现，你需要创建能够最大化网络关注存在有意义相关性的区域的架构，并最小化网络关注包含噪声的区域的架构。
- en: But different datasets and domains have different characteristics. For example,
    image data has different kinds of signal and noise than text data. Even though
    neural networks can be used in many situations, different architectures will be
    better suited to different problems because of their ability to locate certain
    types of correlations. So, for the next few chapters, we’ll explore how to modify
    neural networks to specifically find the correlation you’re looking for. See you
    there!
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 但是不同的数据集和领域具有不同的特性。例如，图像数据与文本数据具有不同类型的信号和噪声。尽管神经网络可以在许多情况下使用，但由于它们定位特定类型相关性的能力，不同的架构将更适合不同的问题。因此，在接下来的几章中，我们将探讨如何修改神经网络以特别寻找你正在寻找的相关性。那里见！
