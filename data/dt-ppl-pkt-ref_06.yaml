- en: Chapter 6\. Transforming Data
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第六章 转换数据
- en: In the ELT pattern defined in [Chapter 3](ch03.xhtml#ch03), once data has been
    ingested into a data lake or data warehouse ([Chapter 4](ch04.xhtml#ch04)), the
    next step in a pipeline is data transformation. Data transformation can include
    both noncontextual manipulation of data and modeling of data with business context
    and logic in mind.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '在定义于[第三章](ch03.xhtml#ch03)中的ELT模式中，一旦数据被摄入到数据湖或数据仓库中（见[第四章](ch04.xhtml#ch04)），管道中的下一步是数据转换。数据转换可以包括对数据的非上下文操纵以及考虑业务上下文和逻辑建模的数据。 '
- en: If the purpose of the pipeline is to produce business insight or analysis, then
    in addition to any noncontextual transformations, data is further transformed
    into data models. Recall from [Chapter 2](ch02.xhtml#ch02) that a data model structures
    and defines data in a format that is understood and optimized for data analysis.
    A data model is represented as one or more tables in a data warehouse.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 如果管道的目的是生成业务洞察或分析，则除了任何非上下文转换之外，数据还会进一步转换为数据模型。 从[第二章](ch02.xhtml#ch02)回想起，数据模型以数据仓库中的一种或多种表形式来表示并定义数据，以便于数据分析。
- en: Though data engineers at times build noncontextual transformation in a pipeline,
    it’s become typical for data analysts and analytics engineers to handle the vast
    majority of data transformations. People in these roles are more empowered than
    ever thanks to the emergence of the ELT pattern (they have the data they need
    right in the warehouse!) and supporting tools and frameworks designed with SQL
    as their primary language.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管数据工程师有时在管道中构建非上下文转换，但现在数据分析师和分析工程师处理绝大部分数据转换已成为典型做法。 由于ELT模式的出现（他们在仓库中就有所需的数据！）以及支持以SQL为主要语言设计的工具和框架，这些角色比以往任何时候都更有权力。
- en: This chapter explores both noncontextual transformations that are common to
    nearly every data pipeline as well as data models that power dashboards, reports,
    and one-time analysis of a business problem. Because SQL is the language of the
    data analyst and analytics engineer, most transformation code samples are written
    in SQL. I include a few samples written in Python to illustrate when it makes
    sense to tightly couple noncontextual transformations to a data ingestion using
    powerful Python libraries.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 本章既探讨了几乎每个数据管道都常见的非上下文转换，也讨论了用于仪表板、报告和业务问题一次性分析的数据模型。 由于SQL是数据分析师和分析工程师的语言，因此大多数转换代码示例都是用SQL编写的。
    我包括了一些用Python编写的示例，以说明何时将非上下文转换紧密耦合到使用强大的Python库的数据摄入中是合理的。
- en: As with the data ingestions in Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05),
    the code samples are highly simplified and meant as a starting point to more complex
    transformations. To learn how to run and manage dependencies between transformations
    and other steps in a pipeline, see [Chapter 8](ch08.xhtml#ch08).
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 就像第[四](ch04.xhtml#ch04)章和[五](ch05.xhtml#ch05)章中的数据摄取一样，代码样例极为简化，并作为管道中更复杂转换的起点。
    要了解如何运行和管理转换与管道中其他步骤之间的依赖关系，请参见[第八章](ch08.xhtml#ch08)。
- en: Noncontextual Transformations
  id: totrans-6
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 非上下文转换
- en: 'In [Chapter 3](ch03.xhtml#ch03), I briefly noted the existence of the EtLT
    sub-pattern, where the lowercase *t* represents some noncontextual data transformations,
    such as the following:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第三章](ch03.xhtml#ch03)中，我简要提到了EtLT子模式的存在，其中小写字母*t*代表某些非上下文数据转换，例如以下内容：
- en: Deduplicate records in a table
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在表中去重记录
- en: Parse URL parameters into individual components
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将URL参数解析为单独的组件
- en: Though there are countless examples, by providing code samples for these transformations
    I hope to cover some common patterns of noncontextual transformations. The next
    section talks about when it makes sense to perform these transformations as part
    of data ingestion (EtLT) versus post-ingestion (ELT).
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然有无数例子，但通过提供这些转换的代码示例，我希望涵盖一些非上下文转换的常见模式。 下一节讨论何时以数据摄入（EtLT）和后摄入（ELT）的一部分执行这些转换是合理的。 '
- en: Deduplicating Records in a Table
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 在表中去重记录
- en: 'Though not ideal, it is possible for duplicate records to exist in a table
    of data that has been ingested into a data warehouse. There are a number of reasons
    it happens:'
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管不是理想的，但在数据被摄入到数据仓库的表中存在重复记录是可能的。 出现这种情况有很多原因：
- en: An incremental data ingestion mistakenly overlaps a previous ingestion time
    window and picks up some records that were already ingested in a previous run.
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增量数据摄取误以前摄取时间窗口重叠，并提取了在先前运行中已摄取的一些记录。
- en: Duplicate records were inadvertently created in a source system.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在源系统中无意中创建了重复记录。
- en: Data that was backfilled overlapped with subsequent data loaded into the table
    during ingestion.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据回填时，与后续加载到表中的数据重叠。
- en: Whatever the reason, checking for and removing duplicate records is best performed
    using SQL queries. Each of the following SQL queries refers to the `Orders` table
    in a database shown in [Table 6-1](#orders_table_with_duplicates). The table contains
    five records, two of which are duplicates. Though there are three records for
    `OrderId` 1, the second and fourth rows are exactly the same. The goal of this
    example is to identify this duplication and resolve it. Though this example has
    two records that are exactly the same, the logic in the following code samples
    is valid if there are three, four or even more copies of the same record in the
    table.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 不管原因是什么，检查和删除重复记录最好使用SQL查询来执行。以下每个SQL查询都涉及到数据库中的`Orders`表，如[表 6-1](#orders_table_with_duplicates)所示。该表包含五条记录，其中两条是重复的。虽然对于`OrderId`为1的记录有三条，但第二行和第四行完全相同。本示例的目标是识别这种重复并解决它。虽然本示例有两条完全相同的记录，但以下代码示例的逻辑在表中有三条、四条甚至更多相同记录时也是有效的。
- en: Table 6-1\. Orders table with duplicates
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-1\. 带有重复记录的订单表
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| OrderId | OrderStatus | LastUpdated |'
- en: '| --- | --- | --- |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Backordered | 2020-06-01 |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 缺货 | 2020-06-01 |'
- en: '| 1 | Shipped | 2020-06-09 |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 |'
- en: '| 2 | Shipped | 2020-07-11 |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 已发货 | 2020-07-11 |'
- en: '| 1 | Shipped | 2020-06-09 |'
  id: totrans-23
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 |'
- en: '| 3 | Shipped | 2020-07-12 |'
  id: totrans-24
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 已发货 | 2020-07-12 |'
- en: 'If you’d like to create a populate such an `Orders` table for use in Examples
    [6-1](#ex_0601) and [6-2](#ex_0602), here is the SQL to do so:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您想要创建一个用于示例[6-1](#ex_0601)和[6-2](#ex_0602)的`Orders`表，以下是可以使用的SQL：
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Identifying duplicate records in a table is simple. You can use the `GROUP
    BY` and `HAVING` statements in SQL. The following query returns any duplicate
    records along with a count of how many there are:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 简单地识别表中的重复记录。您可以使用SQL中的`GROUP BY`和`HAVING`语句。以下查询返回任何重复记录以及它们的数量：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'When executed, the query returns the following:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，查询返回以下结果：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Now that you know that at least one duplicate exists, you can remove the duplicate
    records. I’m going to cover two ways to do so. The method you choose depends on
    many factors related to the optimization of your database as well as your preference
    in SQL syntax. I suggest trying both and comparing runtimes.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您知道至少存在一个重复记录，可以删除这些重复记录。我将介绍两种方法来实现。您选择的方法取决于与数据库优化相关的许多因素以及您对SQL语法的偏好。我建议尝试两种方法并比较运行时间。
- en: The first method is to use a sequence of queries. The first query creates a
    copy of the table from the original using the `DISTINCT` statement. The result
    of the first query is a result set with only four rows, since the two duplicate
    rows are turned into one thanks to `DISTINCT`. Next, the original table is truncated.
    Finally, the deduplicated version of the dataset is inserted into the original
    table, as shown in [Example 6-1](#ex_0601).
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种方法是使用一系列查询。第一个查询使用`DISTINCT`语句从原始表创建表的副本。第一个查询的结果集只有四行，因为两个重复的行被`DISTINCT`转换为一个行。接下来，原始表被截断。最后，去重后的数据集被插入到原始表中，如[示例
    6-1](#ex_0601)所示。
- en: Example 6-1\. distinct_orders_1.sql
  id: totrans-33
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-1\. distinct_orders_1.sql
- en: '[PRE3]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Warning
  id: totrans-35
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: After the `TRUNCATE` operation on the `Orders` table, the table will be empty
    until the following `INSERT` operation is complete. During that time, the `Orders`
    table is empty and essentially not accessible by any user or process that queries
    it. While the `INSERT` operation may not take long, for very large tables you
    may consider dropping the `Orders` table and then renaming `distinct_orders` to
    `Orders` instead.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在对`Orders`表执行`TRUNCATE`操作后，直到完成以下`INSERT`操作之前，该表将为空。在此期间，`Orders`表为空，基本上不可由任何查询它的用户或进程访问。虽然`INSERT`操作可能不会花费太多时间，但对于非常大的表，您可能考虑删除`Orders`表，然后将`distinct_orders`重命名为`Orders`。
- en: Another approach is to use a *window function* to group duplicate rows and assign
    them row numbers to identify which ones to delete and which one to keep. I’ll
    use the `ROW_NUMBER` function to rank the records, and the `PARTITION BY` statement
    to group the records by each column. By doing this, any group of records with
    more than one match (our duplicates) will get assigned a `ROW_NUMBER` greater
    than 1.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种方法是使用*窗口函数*对重复行进行分组，并分配行号以标识哪些行应删除，哪些行应保留。我将使用`ROW_NUMBER`函数对记录进行排名，并使用`PARTITION
    BY`语句按每列分组记录。通过这样做，任何具有多个匹配项（我们的重复项）的记录将被分配一个大于1的`ROW_NUMBER`。
- en: If you executed [Example 6-1](#ex_0601), please make sure to refresh the `Orders`
    table using the `INSERT` statements from earlier in this section so that it again
    contains what is shown in [Table 6-1](#orders_table_with_duplicates). You’ll want
    to have a duplicate row to work with for the following sample!
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您执行了[示例 6-1](#ex_0601)，请确保使用本节早期的`INSERT`语句刷新`Orders`表，使其再次包含[表 6-1](#orders_table_with_duplicates)中显示的内容。您将希望有一个重复行用于以下示例！
- en: 'Here is what happens when such a query is run on the `Orders` table:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当在`Orders`表上运行这样的查询时，会发生以下情况：
- en: '[PRE4]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'The result of the query is as follows:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 查询的结果如下：
- en: '[PRE5]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: As you can see, the third row in the result set has a `dup_count` value of `2`,
    as it is a duplicate of the record right above it. Now, just like the first approach,
    you can create a table with the deduplicated records, truncate the `Orders` table,
    and finally insert the cleaned dataset into `Orders`. [Example 6-2](#ex_0602)
    shows the full source.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，结果集中的第三行具有`dup_count`值为`2`，因为它与其上方的记录是重复的。现在，就像第一种方法一样，您可以创建一个包含去重记录的表，截断`Orders`表，最后将清理后的数据集插入`Orders`中。[示例 6-2](#ex_0602)显示了完整的源代码。
- en: Example 6-2\. distinct_orders_2.sql
  id: totrans-44
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-2\. distinct_orders_2.sql
- en: '[PRE6]'
  id: totrans-45
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Regardless of which approach you take, the result is a deduplicated version
    of the `Orders` table, as shown in [Table 6-2](#orders_table_without_duplicates).
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 无论采用哪种方法，结果都是`Orders`表的去重版本，如[表 6-2](#orders_table_without_duplicates)所示。
- en: Table 6-2\. Orders table without duplicates
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-2\. Orders表（无重复项）
- en: '| OrderId | OrderStatus | LastUpdated |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '| OrderId | OrderStatus | LastUpdated |'
- en: '| --- | --- | --- |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 1 | Backordered | 2020-06-01 |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 后订购 | 2020-06-01 |'
- en: '| 1 | Shipped | 2020-06-09 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 |'
- en: '| 2 | Shipped | 2020-07-11 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 已发货 | 2020-07-11 |'
- en: '| 3 | Shipped | 2020-07-12 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 已发货 | 2020-07-12 |'
- en: Parsing URLs
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解析URL
- en: Parsing out segments of URLs is a task with little or no business context involved.
    There are a number of URL components that can be parsed out in a transform step
    and stored in individual columns in a database table.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 解析URL片段是一个与业务背景几乎无关的任务。有许多URL组件可以在转换步骤中解析，并存储在数据库表的各个列中。
- en: 'For example, consider the following URL:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，请考虑以下URL：
- en: '*https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale*'
  id: totrans-57
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*https://www.mydomain.com/page-name?utm_content=textlink&utm_medium=social&utm_source=twitter&utm_campaign=fallsale*'
- en: 'There are six components that are valuable and can be parsed and stored into
    individual columns:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 有六个有价值且可以解析并存储为单独列的组件：
- en: 'The domain: *www.domain.com*'
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 域名：*www.domain.com*
- en: 'The URL path: */page-name*'
  id: totrans-60
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: URL路径：*/page-name*
- en: 'utm_content parameter value: *textlink*'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: utm_content参数值：*textlink*
- en: 'utm_medium parameter value: *social*'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: utm_medium参数值：*social*
- en: 'utm_source parameter value: *twitter*'
  id: totrans-63
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: utm_source参数值：*twitter*
- en: 'utm_campaign parameter value: *fallsale*'
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: utm_campaign参数值：*fallsale*
- en: Parsing URLs is possible in both SQL and Python. The time when you’re running
    the transform and where the URLs are stored will help guide your decision on which
    to use. For example, if you’re following an EtLT pattern and can parse the URLs
    after extraction from a source but before loading into a table in a data warehouse,
    Python is an excellent option. I will start by providing an example in Python,
    followed by SQL.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 解析URL可以在SQL和Python中进行。在运行转换时和URL存储的位置将指导您决定使用哪种语言。例如，如果您遵循EtLT模式并且可以在从源提取后但加载到数据仓库表之前解析URL，则Python是一个非常好的选择。我将首先提供Python示例，然后是SQL。
- en: 'First, install the `urllib3` Python library using `pip`. (See [“Setting Up
    Your Python Environment”](ch04.xhtml#setup-python-enviro) for instructions on
    Python configuration):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，使用`pip`安装`urllib3` Python库。（有关Python配置说明，请参见[“设置Python环境”](ch04.xhtml#setup-python-enviro)）：
- en: '[PRE7]'
  id: totrans-67
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Then, use the `urlsplit` and `parse_qs` functions to parse out the relevant
    components of the URL. In the following code sample, I do so and print out the
    results:'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，使用`urlsplit`和`parse_qs`函数解析URL的相关组件。在下面的代码示例中，我这样做并打印出结果：
- en: '[PRE8]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'When executed, the code sample produces the following:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当执行时，代码示例会产生以下结果：
- en: '[PRE9]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: As in the data ingestion code samples from Chapters [4](ch04.xhtml#ch04) and
    [5](ch05.xhtml#ch05), you can also parse and write out each parameter to a CSV
    file to be loaded into your data warehouse to complete the ingestion. [Example 6-3](#ex_0603)
    contains the code sample that does so for the example URL, but you’ll likely be
    iterating through more than one URL!
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 就像第[4](ch04.xhtml#ch04)章和第[5](ch05.xhtml#ch05)章中的数据摄入代码示例一样，你也可以解析并将每个参数写入CSV文件，以加载到数据仓库中完成摄入。[示例 6-3](#ex_0603)包含了完成此操作的代码示例，但你可能需要迭代处理多个URL！
- en: Example 6-3\. url_parse.sql
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-3\. url_parse.sql
- en: '[PRE10]'
  id: totrans-74
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If you need to parse URLs that have already been loaded into the data warehouse
    using SQL, you may have a more difficult task ahead. Though some data warehouse
    vendors provide functions to parse URLs, others do not. Snowflake, for instance,
    provides a function called `PARSE_URL` that parses a URL into its components and
    returns the result as a JSON object. For example, if you want to parse the URL
    in the preceding example, the result will look like this:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你需要解析已加载到数据仓库中的URL，使用SQL可能会更具挑战性。虽然一些数据仓库供应商提供解析URL的函数，但其他则没有。例如，Snowflake提供了一个名为`PARSE_URL`的函数，将URL解析为其组件，并将结果作为JSON对象返回。例如，如果你想解析前面示例中的URL，结果将如下所示：
- en: '[PRE11]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '[PRE12]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: If you are using Redshift or another data warehouse platform without built-in
    URL parsing, you’ll need to make use of custom string parsing or regular expressions.
    For example, Redshift has a function called `REGEXP_SUBSTR`. Given the difficultly
    of parsing URLs in most data warehouses, I recommend parsing using Python or another
    language during data ingestion and loading in the structured URL components.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你正在使用Redshift或其他没有内置URL解析功能的数据仓库平台，你需要使用自定义字符串解析或正则表达式。例如，Redshift有一个名为`REGEXP_SUBSTR`的函数。考虑到大多数数据仓库中解析URL的难度，我建议在数据摄入时使用Python或其他语言进行解析，并加载结构化的URL组件。
- en: When to Transform? During or After Ingestion?
  id: totrans-79
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 何时进行转换？在摄入期间还是之后？
- en: 'Data transformations without business context, like the ones in the previous
    section, can be run either during or after data ingestion from a technical standpoint.
    However, there are some reasons you should consider running them as part of the
    ingestion process (the EtLT pattern):'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 从技术角度来看，不像前面部分中没有业务背景的数据转换可以在数据摄入期间或之后运行。然而，有些原因你应该考虑将它们作为摄入过程的一部分运行（EtLT模式）：
- en: '*The transformation is easiest to do using a language besides SQL*: Like parsing
    URLs in an earlier example, if you find it’s a lot easier to make use of Python
    libraries to handle the transformation, then do so as part of the data ingestion.
    In the ELT pattern, transforms done post-ingestion are limited to data modeling
    that is performed by data analysts who are typically most comfortable in SQL.'
  id: totrans-81
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*使用除SQL以外的语言进行转换最简单*：就像在前面的示例中解析URL一样，如果你发现使用Python库处理转换要容易得多，那么作为数据摄入的一部分就这样做吧。在ELT模式中，摄入后的转换仅限于数据建模，由通常在SQL中最为熟悉的数据分析师执行。'
- en: '*The transformation is addressing a data quality issue*: It’s best to address
    data quality as early in a pipeline as possible ([Chapter 9](ch09.xhtml#ch09)
    has more detail on this topic). For example, in the previous section I provided
    an example of identifying and removing duplicate records in data that’s been ingested.
    There’s no reason to take the risk of a data analyst getting tripped up by duplicated
    data if you can catch it and fix it at the point of ingestion. Even though the
    transform is written in SQL, it can be run at the tail end of the ingestion rather
    than waiting for the analyst to transform the data.'
  id: totrans-82
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '*该转换正在解决数据质量问题*：最好尽早在管道中解决数据质量问题（第[9](ch09.xhtml#ch09)章更详细地讨论了这个主题）。例如，在前面的部分中，我提供了一个识别和删除已摄入数据中重复记录的示例。如果可以在摄入点捕捉并修复重复数据，就没有理由让数据分析师因为重复数据而遭受困扰。尽管该转换是用SQL编写的，但可以在摄入的最后阶段运行，而不必等待分析师转换数据。'
- en: When it comes to transformations that involve business logic, it’s best to keep
    those separate from data ingestions. As you’ll see in the next section, this type
    of transformation is referred to as *data modeling*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 当涉及到包含业务逻辑的转换时，最好将其与数据摄入分开。正如你将在下一节看到的，这种类型的转换被称为*数据建模*。
- en: Data Modeling Foundations
  id: totrans-84
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据建模基础
- en: Modeling data for use in analysis, dashboards, and reports is a topic worth
    dedicating an entire book to. However, there are some principles for modeling
    data in the ELT pattern that I discuss in this section.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 为了分析、仪表板和报告使用而建模的数据是一个值得专门撰写一本书的主题。但是，在ELT模式中建模数据的一些原则我在本节中进行了讨论。
- en: Unlike the previous section, data modeling is where business context is taken
    into account in the transform step of the ELT pattern in a pipeline. Data models
    make sense of all the data that’s been loaded into the warehouse from various
    sources during the extract and load steps (data ingestion).
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 不同于前一节，数据建模是ELT模式管道中的转换步骤考虑业务上下文的地方。数据模型理清了从各种来源在提取和加载步骤（数据摄入）中加载到仓库中的所有数据。
- en: Key Data Modeling Terms
  id: totrans-87
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 关键数据建模术语
- en: 'When I use the term *data models* in this section, I’m referring to individual
    SQL tables in a data warehouse. In the sample data models, I’ll focus on two properties
    of models:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，当我使用术语*数据模型*时，我指的是数据仓库中的单个SQL表。在样本数据模型中，我将专注于模型的两个属性：
- en: Measures
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 度量
- en: These are things you want to measure! Examples include a count of customers
    and a dollar value of revenue.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是你想要衡量的事物！例如客户数量和收入的金额。
- en: Attributes
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 属性
- en: These are things by which you want to filter or group in a report or dashboard.
    Examples include dates, customer names, and countries.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是你想在报告或仪表板中进行过滤或分组的内容。例如日期、客户名称和国家。
- en: In addition, I will speak to the granularity of a data model. *Granularity*
    is the level of detail stored in a data model. For example, a model that must
    provide the number of orders placed each day would require daily granularity.
    If it had to answer the question of how many orders were placed each hour, then
    it would require hourly granularity.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我将谈到数据模型的粒度。*粒度*是存储在数据模型中的详细级别。例如，一个必须提供每天下订单数量的模型需要日粒度。如果它必须回答每小时下订单数量的问题，那么它需要小时粒度。
- en: '*Source tables* are tables that were loaded into the data warehouse or data
    lake via a data ingestion as described in Chapters [4](ch04.xhtml#ch04) and [5](ch05.xhtml#ch05).
    In data modeling, models are built from both source tables as well as other data
    models.'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '*源表*是通过数据摄入（如第[4](ch04.xhtml#ch04)章和第[5](ch05.xhtml#ch05)章描述）加载到数据仓库或数据湖中的表。在数据建模中，模型是从源表以及其他数据模型构建的。'
- en: Modeling Fully Refreshed Data
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全刷新数据建模
- en: When modeling data that has been fully reloaded, such as described in [“Extracting
    Data from a MySQL Database”](ch04.xhtml#extract-data-mysql), you are confronted
    with a table (or multiple tables) that contain the latest state of a source data
    store. For example, [Table 6-3](#a_fully_refreshed_orders_table) shows records
    in an `Orders` table similar to the one in [Table 6-2](#orders_table_without_duplicates),
    but with only the latest records rather than a full history. Notice that the `Backordered`
    record for `OrderId` 1 is not present in this version. This is what would be present
    if the table were loaded in full from the source database to the data warehouse.
    In other words, it looks like the current state of the `Orders` table in the source
    system at the time of data ingestion.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 当建模已完全重新加载的数据时，比如在[“从MySQL数据库中提取数据”](ch04.xhtml#extract-data-mysql)中描述的情况下，你会遇到包含源数据存储的最新状态的表（或多个表）。例如，[表 6-3](#a_fully_refreshed_orders_table)展示了类似于[表 6-2](#orders_table_without_duplicates)中的`Orders`表的记录，但只包含最新的记录，而不是完整的历史记录。请注意，`OrderId`为1的`Backordered`记录在此版本中不存在。这就是如果从源数据库完整加载到数据仓库中，表看起来像源系统中的`Orders`表当前状态的原因。
- en: The other differences from [Table 6-2](#orders_table_without_duplicates) are
    a fourth column named `CustomerId`, which stores the identifier of the customer
    who placed the order, and a fifth column with the `OrderTotal`, which is the dollar
    value of the order.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 与[表 6-2](#orders_table_without_duplicates)的其他差异是第四列名为`CustomerId`，存储下订单客户的标识符，以及第五列`OrderTotal`，即订单的金额。
- en: Table 6-3\. A fully refreshed Orders table
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-3\. 完全刷新的Orders表
- en: '| OrderId | OrderStatus | OrderDate | CustomerId | OrderTotal |'
  id: totrans-99
  prefs: []
  type: TYPE_TB
  zh: '| OrderId | OrderStatus | OrderDate | CustomerId | OrderTotal |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-100
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 1 | Shipped | 2020-06-09 | 100 | 50.05 |'
  id: totrans-101
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 已发货 | 2020-06-09 | 100 | 50.05 |'
- en: '| 2 | Shipped | 2020-07-11 | 101 | 57.45 |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 已发货 | 2020-07-11 | 101 | 57.45 |'
- en: '| 3 | Shipped | 2020-07-12 | 102 | 135.99 |'
  id: totrans-103
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 已发货 | 2020-07-12 | 102 | 135.99 |'
- en: '| 4 | Shipped | 2020-07-12 | 100 | 43.00 |'
  id: totrans-104
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 已发货 | 2020-07-12 | 100 | 43.00 |'
- en: In addition to the `Orders` table, consider the `Customers` table, shown in
    [Table 6-4](#a_fully_refreshed_customers_table), which has also been loaded into
    the warehouse in full (meaning it contains the current state of each customer
    record).
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`Orders`表之外，还要考虑已在仓库中完全加载的`Customers`表，显示在[表 6-4](#a_fully_refreshed_customers_table)中（即包含每个客户记录的当前状态）。
- en: Table 6-4\. A fully refreshed Customers table
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-4\. 完全刷新的Customers表
- en: '| CustomerId | CustomerName | CustomerCountry |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '| CustomerId | CustomerName | CustomerCountry |'
- en: '| --- | --- | --- |'
  id: totrans-108
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| 100 | Jane | USA |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| 100 | Jane | 美国 |'
- en: '| 101 | Bob | UK |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '| 101 | Bob | 英国 |'
- en: '| 102 | Miles | UK |'
  id: totrans-111
  prefs: []
  type: TYPE_TB
  zh: '| 102 | Miles | 英国 |'
- en: If you’d like to create these tables in a database for use in the following
    sections, you can use the following SQL statements to do so. Note that if you
    created the version of the `Orders` table in [“Deduplicating Records in a Table”](#deduplicating-records-table),
    you’ll need to `DROP` it first.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望在接下来的几节中在数据库中创建这些表格，您可以使用以下SQL语句来执行。请注意，如果您创建了[“在表中去重记录”](#deduplicating-records-table)版本的`Orders`表，您需要首先进行`DROP`操作。
- en: '[PRE13]'
  id: totrans-113
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Consider the need to create a data model that can be queried to answer the
    following questions:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑需要创建一个数据模型，以便查询来回答以下问题：
- en: How much revenue was generated from orders placed from a given country in a
    given month?
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某个国家在某个月份下的订单生成了多少收入？
- en: How many orders were placed on a given day?
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定的一天中有多少订单？
- en: Though the sample tables contain only a few records, imagine a case where both
    tables contain millions of records. While answering those questions is quite straightforward
    using a SQL query, when the data volume is high, query execution time and the
    volume of data in a model can be reduced if the data model is aggregated to some
    extent.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然示例表仅包含少量记录，但请想象一下，如果两个表包含数百万条记录的情况。虽然使用SQL查询回答这些问题非常直接，但当数据量较大时，通过在某种程度上对数据模型进行聚合可以减少查询执行时间和模型中的数据量。
- en: 'If those questions are the only two requirements of the data model, there are
    two measures it must provide:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这些问题是数据模型的唯二要求，它必须提供两个措施：
- en: Total Revenue
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 总收入
- en: Order Count
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订单计数
- en: 'In addition, there are two attributes which the model must allow a query to
    filter or group the data by:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，模型必须允许根据以下两个属性对数据进行过滤或分组查询：
- en: Order Country
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订单国家
- en: Order Date
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 订单日期
- en: Finally, the granularity of the model is daily, since the smallest unit of time
    in the requirements is by day.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，模型的粒度是按日，因为需求中的最小时间单位是按日。
- en: 'In this highly simplified data model, I’ll first define the structure of the
    model (a SQL table) and then insert the data sourced from a join of both tables:'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个高度简化的数据模型中，我将首先定义模型的结构（一个SQL表），然后插入从两个表连接源的数据：
- en: '[PRE14]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Now, you can query the model to answer the questions set out in the requirements:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您可以查询模型来回答需求中列出的问题：
- en: '[PRE15]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'With the sample data from Tables [6-3](#a_fully_refreshed_orders_table) and
    [6-4](#a_fully_refreshed_customers_table), the query returns the following results:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 使用表[6-3](#a_fully_refreshed_orders_table)和[6-4](#a_fully_refreshed_customers_table)中的示例数据，查询返回以下结果：
- en: '[PRE16]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[PRE17]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This returns the following:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 返回以下结果：
- en: '[PRE18]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: Slowly Changing Dimensions for Fully Refreshed Data
  id: totrans-134
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 完全刷新数据的慢变化维度
- en: Because data that’s been ingested as a full refresh overwrites changes to existing
    data (such as a record in `Customers`), a more advanced data modeling concept
    is often implemented to track historical changes.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 因为完全刷新的数据（如`Customers`中的记录）会覆盖现有数据的更改，通常会实现更高级的数据建模概念以跟踪历史更改。
- en: For example, in the next section you’ll make use of a `Customers` table that’s
    been loaded incrementally and contains updates to CustomerId 100\. As you will
    see in [Table 6-6](#the_incrementally_loaded_customers_staging_table), that customer
    has a second record indicating that the value of her `CustomerCountry` changed
    from “USA” to “UK” on 2020-06-20\. That means that when she placed `OrderId` 4
    on 2020-07-12 she no longer lived in the USA.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在下一节中，您将使用已增量加载的`Customers`表格，并包含对客户号为 100 的更新。正如您将在 [表 6-6](#the_incrementally_loaded_customers_staging_table)
    中看到的那样，该客户有第二条记录表明她的`CustomerCountry`值于 2020-06-20 从“美国”更改为“英国”。这意味着她在 2020-07-12
    下订单 4 时不再居住在美国。
- en: When analyzing the order history, an analyst might want to allocate a customer’s
    orders to where they lived at the time of an order. With incrementally refreshed
    data it’s a bit easier to do that, as you’ll see in the next section. With fully
    refreshed data, it’s necessary to keep a full history of the `Customers` table
    between each ingestion and keep track of those changes on your own.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在分析订单历史时，分析师可能希望将客户的订单分配到订单时所居住的地方。使用增量刷新数据，这样做稍微容易一些，如下一节所示。对于完全刷新的数据，需要在每次摄入之间保留`Customers`表格的完整历史记录，并自行跟踪这些更改。
- en: The method for doing so is defined in Kimball (dimensional) modeling and referred
    to as a *slowly changing dimension* or *SCD*. When dealing with fully refreshed
    data, I often make use of Type II SCDs, which add a new record to a table for
    each change to an entity, including the date range that the record was valid.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在 Kimball（维度）建模中定义，并称为*慢变化维度*或*SCD*。在处理完全刷新的数据时，我经常使用 II 型 SCD，为实体的每次更改向表格中添加新记录，包括记录有效的日期范围。
- en: A Type II SCD with Jane’s customer records would look something like [Table 6-5](#a_type_ii_scd_with_customer_data).
    Note that the latest record expires on a date in very distant future. Some Type
    II SCDs use NULL for unexpired records, but a date far in the future makes querying
    the table a bit less error prone, as you’ll see in a moment.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 简单来说，简的客户记录的 II 型 SCD 如 [表 6-5](#a_type_ii_scd_with_customer_data) 所示。请注意，最新记录的过期日期非常遥远。一些
    II 型 SCD 使用 NULL 表示未过期记录，但远未来的日期使得查询表格时出错的可能性稍低，稍后您将看到。
- en: Table 6-5\. A Type II SCD with customer data
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-5\. 具有客户数据的 II 型 SCD
- en: '| CustomerId | CustomerName | CustomerCountry | ValidFrom | Expired |'
  id: totrans-141
  prefs: []
  type: TYPE_TB
  zh: '| CustomerId | CustomerName | CustomerCountry | ValidFrom | Expired |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-142
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 100 | Jane | USA | 2019-05-01 7:01:10 | 2020-06-20 8:15:34 |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| 100 | Jane | 美国 | 2019-05-01 7:01:10 | 2020-06-20 8:15:34 |'
- en: '| 100 | Jane | UK | 2020-06-20 8:15:34 | 2199-12-31 00:00:00 |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '| 100 | Jane | 英国 | 2020-06-20 8:15:34 | 2199-12-31 00:00:00 |'
- en: 'You can create and populate this table in your database using the following
    SQL statements:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用以下 SQL 语句在数据库中创建和填充此表：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'You can join the SCD with the `Orders` table you created earlier to get the
    properties of the customer record at the time of the order. To do so, in addition
    to joining on the `CustomerId`, you’ll also need to join on the date range in
    the SCD that the order was placed in. For example, this query will return the
    country that Jane’s `Customers_scd` record indicated she lived in at the time
    each of her orders was placed:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以将此 SCD 与您之前创建的`Orders`表格连接起来，以获取订单时的客户记录属性。为此，除了使用`CustomerId`进行连接外，您还需要使用订单放置时
    SCD 中的日期范围进行连接。例如，此查询将返回简的`Customers_scd`记录指示她当时住在的国家：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[PRE21]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Though this logic is all you need to make use of SCDs in data modeling, keeping
    SCDs up to date can be a challenge. In the case of the `Customers` table, you’ll
    need to take a snapshot of it after each ingestion and look for any `CustomerId`
    records that have changed. The best approach for doing so depends on which data
    warehouse and which data orchestration tools you are using. If you are interested
    in implementing SCDs, I suggest learning the fundamentals of Kimball modeling,
    which is outside the scope of this book. For more in-depth reading on the subject,
    I suggest the book *The Data Warehouse Toolkit*, by Ralph Kimball and Margy Ross
    (Wiley, 2013).
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管此逻辑足以在数据建模中使用 SCD，但保持 SCD 的最新状态可能是一个挑战。对于`Customers`表格，您需要在每次摄入后对其进行快照，并查找任何已更改的`CustomerId`记录。如何处理这些取决于您使用的数据仓库和数据编排工具。如果您有兴趣实施
    SCD，请学习 Kimball 建模的基础知识，这超出了本书的范围。如需更深入地了解此主题，请参阅 Ralph Kimball 和 Margy Ross 的书籍*数据仓库工具包*（Wiley,
    2013）。
- en: Modeling Incrementally Ingested Data
  id: totrans-151
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逐步建模增量摄入的数据
- en: Recall from [Chapter 4](ch04.xhtml#ch04) that data ingested incrementally contains
    not only the current state of the source data, but also historical records from
    since the ingestion started. For example, consider the same `Orders` table as
    in the prior section, but with a new customers table named `Customers_staging`
    that is ingested incrementally. As you can see in [Table 6-6](#the_incrementally_loaded_customers_staging_table),
    there are new columns for the `UpdatedDate` value of the record, as well as a
    new record for `CustomerId` 100 indicating that Jane’s `CustomerCountry` (where
    she lives) changed from the US to the UK on 2020-06-20.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想来自[第四章](ch04.xhtml#ch04)的信息，增量摄取的数据不仅包含源数据的当前状态，还包括自摄入开始以来的历史记录。例如，考虑与先前部分相同的`Orders`表，但是有一个名为`Customers_staging`的新客户表，它是增量摄入的。正如您在[表6-6](#the_incrementally_loaded_customers_staging_table)中所看到的，记录的`UpdatedDate`值有所更新，并且对于`CustomerId`
    100的新记录表明，简的`CustomerCountry`（她居住的地方）于2020年06月20日从美国变更为英国。
- en: Table 6-6\. The incrementally loaded Customers_staging table
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-6\. 增量加载的Customers_staging表
- en: '| CustomerId | CustomerName | CustomerCountry | LastUpdated |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| CustomerId | CustomerName | CustomerCountry | LastUpdated |'
- en: '| --- | --- | --- | --- |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 100 | Jane | USA | 2019-05-01 7:01:10 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 100 | Jane | 美国 | 2019-05-01 7:01:10 |'
- en: '| 101 | Bob | UK | 2020-01-15 13:05:31 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 101 | Bob | 英国 | 2020-01-15 13:05:31 |'
- en: '| 102 | Miles | UK | 2020-01-29 9:12:00 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 102 | Miles | 英国 | 2020-01-29 9:12:00 |'
- en: '| 100 | Jane | UK | 2020-06-20 8:15:34 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 100 | Jane | 英国 | 2020-06-20 8:15:34 |'
- en: 'You can create and populate the `Customers_staging` table in your database
    for use in the following examples using these SQL statements:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 可以使用以下 SQL 语句在数据库中创建和填充`Customers_staging`表，以便在接下来的示例中使用：
- en: '[PRE22]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Recall the questions the model needs to answer from the previous section, which
    I’ll apply to the model in this section as well:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 请回想前一节模型需要回答的问题，我将在本节中也应用到模型上：
- en: How much revenue was generated from orders placed from a given country in a
    given month?
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在给定月份内来自特定国家下的订单生成了多少收入？
- en: How many orders were placed on a given day?
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在特定日子下有多少订单？
- en: Before you can build your data model in this case, you’ll need to decide how
    you want to handle changes to records in the `Customer` table. In the example
    of Jane, which country should her two orders in the `Orders` table be allocated
    to? Should they both be allocated to her current country (UK) or each to the country
    that she lived in at the time of the order (the US and the UK respectively)?
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，在构建数据模型之前，您需要决定如何处理`Customer`表中记录的更改。例如，在简的例子中，她在`Orders`表中的两个订单应分配给哪个国家？它们应该都分配给她当前的国家（英国）还是每个订单应该分配到下单时她所在的国家（分别是美国和英国）？
- en: 'The choice you make is based on the logic required by the business case, but
    the implementation of each is a bit different. I’ll start with an example of allocating
    to her current country. I’ll do this by building a data model similar to the one
    in the previous section, but using only the most current record for each `CustomerId`
    in the `Customers_staging` table. Note that because the second question in the
    requirements for the model requires daily granularity, I’ll build the model at
    the date level:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 您所做的选择基于业务案例所需的逻辑，但每个实现都有些不同。我将以分配给她当前国家的示例开始。我将通过构建与前一节中相似的数据模型来完成这一点，但仅使用`Customers_staging`表中每个`CustomerId`的最新记录。请注意，由于模型要求的第二个问题需要每日粒度，我将在日期级别上构建模型：
- en: '[PRE23]'
  id: totrans-167
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'When answering the question of how much revenue was generated from orders placed
    in a given country in a given month, both of Jane’s orders are allocated to the
    UK, even though you might expect to see the 50.05 from her order in June to be
    allocated to the US, given that’s where she lived at the time:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 当回答来自特定国家在给定月份内的订单生成了多少收入时，简的两个订单都分配给了英国，尽管您可能期望从她在美国生活时，将6月份的订单分配给美国，从而得到50.05：
- en: '[PRE24]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[PRE25]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: If instead you want to allocate orders to the country that the customers lived
    in at the time of order, then building the model requires a change in logic. Instead
    of finding the most recent record in `Customers_staging` for each `CustomerId`
    in the *common table expression* (CTE), I instead find the most recent record
    that was updated on or before the time of each order placed by each customer.
    In other words, I want the information about the customer that was valid at the
    time they placed the order. That information is stored in the version of their
    `Customer_staging` record when the order was placed. Any later updates to their
    customer information didn’t occur until after that particular order was placed.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您希望根据客户下订单时所在国家来分配订单，那么构建模型就需要改变逻辑。不再查找每个`Customers_staging`中每个`CustomerId`的最近记录，而是查找每个客户下订单时更新时间在订单放置时间之前或同时的最近记录。换句话说，我希望获得客户下订单时的有效信息。该信息存储在他们的`Customer_staging`记录版本中，在该特定订单放置之后，直到后续更新为止。
- en: 'The `customer_pit` (*pit* is short for “point-in-time”) CTE in the following
    sample contains the `MAX(cs.LastUpdated)` for each `CustomerId`/`OrderId` pair.
    I use that information in the final `SELECT` statement to populate the data model.
    Note that I must join based on both the `OrderId` and `CustomerId` in this query.
    Here is the final SQL for the `order_summary_daily_pit` model:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例中的`customer_pit`（*pit*是“时间点”）CTE包含每个`CustomerId`/`OrderId`对的`MAX(cs.LastUpdated)`。我在最终的`SELECT`语句中使用这些信息来填充数据模型。请注意，在此查询中，必须根据`OrderId`和`CustomerId`进行连接。以下是`order_summary_daily_pit`模型的最终SQL：
- en: '[PRE26]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'When you run the same query as before, you’ll see that the revenue from Jane’s
    first order is allocated to the US in June 2020, while the second order in July
    2020 remains allocated to the UK as expected:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 运行与之前相同的查询后，您将看到Jane的第一笔订单的收入在2020年6月分配给了美国，而第二笔订单在2020年7月分配给了英国，正如预期的那样：
- en: '[PRE27]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Modeling Append-Only Data
  id: totrans-177
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建模追加数据
- en: '*Append-only data* (or *insert-only data*) is immutable data that is ingested
    into a data warehouse. Each record in such a table is some kind of event that
    never changes. An example is a table of all page views on a website. Each time
    the data ingestion runs, it appends new page views to the table but never updates
    or deletes previous events. What occurred in the past happened and cannot be changed.'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '*仅追加数据*（或*仅插入数据*）是指不可变数据，用于导入数据仓库。这种表中的每条记录都是一些永不改变的事件。例如，网站上所有页面浏览的表格。每次数据导入运行时，都会将新的页面浏览追加到表中，但不会更新或删除之前的事件。过去发生的事情已经发生，无法改变。'
- en: Modeling append-only data is similar to modeling fully refreshed data. However,
    you can optimize the creation and refresh of data models built off of such data
    by taking advantage of the fact that once records are inserted, they’ll never
    change.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 建模追加数据类似于建模完全刷新数据。然而，您可以通过利用一旦记录插入，它们永远不会更改的事实来优化基于这种数据构建的数据模型的创建和刷新。
- en: '[Table 6-7](#pageviews_table) is an example of an append-only table named `PageViews`
    containing record page views on a website. Each record in the table represents
    a customer viewing a page on a company’s website. New records, representing page
    views logged since the last ingestion, are appended to the table each time the
    data ingestion job runs.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '[表 6-7](#pageviews_table)是一个名为`PageViews`的追加数据表的示例，其中记录了网站上的页面浏览。表中的每条记录代表客户在公司网站上浏览页面的情况。每次数据导入作业运行时，都会将新的记录追加到表中，代表上次导入以来记录的页面浏览。'
- en: Table 6-7\. PageViews table
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 表 6-7\. 页面浏览表
- en: '| CustomerId | ViewTime | UrlPath | utm_medium |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| CustomerId | ViewTime | UrlPath | utm_medium |'
- en: '| --- | --- | --- | --- |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 100 | 2020-06-01 12:00:00 | /home | social |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 2020-06-01 12:00:00 | /home | social |'
- en: '| 100 | 2020-06-01 12:00:13 | /product/2554 | NULL |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 2020-06-01 12:00:13 | /product/2554 | NULL |'
- en: '| 101 | 2020-06-01 12:01:30 | /product/6754 | search |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 2020-06-01 12:01:30 | /product/6754 | search |'
- en: '| 102 | 2020-06-02 7:05:00 | /home | NULL |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 102 | 2020-06-02 7:05:00 | /home | NULL |'
- en: '| 101 | 2020-06-02 12:00:00 | /product/2554 | social |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 2020-06-02 12:00:00 | /product/2554 | social |'
- en: You can create and populate the `PageViews` table in your database for use in
    the following examples using these SQL queries.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下SQL查询在数据库中创建和填充`PageViews`表，以便在接下来的示例中使用。
- en: '[PRE29]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Note that a real table with page view data would contain dozens or more columns
    storing attributes about the page viewed, the referring URL, the user’s browser
    version, and more.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，实际包含页面浏览数据的表可能包含几十个或更多列，存储有关浏览页面属性、引荐URL、用户浏览器版本等的属性。
- en: 'Now, I’ll define a data model that is designed to answer the following questions.
    I’ll be using the `Customers` table defined in [Table 6-4](#a_fully_refreshed_customers_table)
    earlier in this chapter to identify the country that each customer resides in:'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我将定义一个数据模型，旨在回答以下问题。我将使用本章前文中定义的`Customers`表（[表 6-4](#a_fully_refreshed_customers_table)）来确定每位客户所居住的国家：
- en: How many page views are there for each `UrlPath` on the site by day?
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每天站点上每个`UrlPath`的页面浏览量是多少？
- en: How many page views do customers from each country generate each day?
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 每天每个国家的客户生成多少页面浏览量？
- en: The granularity of the data model is daily. There are three attributes required.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 数据模型的粒度是每天一次。有三个必需的属性。
- en: The date (no timestamp required) of the page view
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面查看的日期（无需时间戳）
- en: The `UrlPath` of the page view
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面查看的`UrlPath`
- en: The country that the customer viewing the page resides in
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查看页面的客户所在的国家
- en: 'There is only one metric required:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个必需的度量标准：
- en: A count of page views
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 页面浏览量的计数
- en: 'The structure of the model is as follows:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的结构如下：
- en: '[PRE30]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: To populate the model for the first time, the logic is the same as in the “Modeling
    Fully Refreshed Data” section of this chapter. All records from the `PageViews`
    table are included in the population of `pageviews_daily`. [Example 6-4](#ex_0604)
    shows the SQL.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 要首次填充模型，逻辑与本章“完全刷新数据建模”部分相同。从`PageViews`表中的所有记录都包括在`pageviews_daily`的填充中。[示例 6-4](#ex_0604)显示了SQL。
- en: Example 6-4\. pageviews_daily.sql
  id: totrans-204
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 示例 6-4\. pageviews_daily.sql
- en: '[PRE31]'
  id: totrans-205
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'To answer one of the questions required by the model (how many page views do
    customers from each country generate each day?), the following SQL will do the
    trick:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答模型要求的一个问题（每天每个国家的客户生成多少页面浏览量？），以下SQL将解决问题：
- en: '[PRE32]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '[PRE33]'
  id: totrans-208
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'Now consider what to do when the next ingestion of data into the `PageViews`
    table runs. New records are added, but all existing records remain untouched.
    To update the `pageviews_daily` model, you have two options:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 现在考虑下次将数据摄入`PageViews`表时该做什么。会添加新记录，但所有现有记录保持不变。要更新`pageviews_daily`模型，有两个选项：
- en: Truncate the `pageviews_daily` table and run the same `INSERT` statement you
    used to populate it for the first time. In this case, you are *fully refreshing*
    the model.
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 截断`pageviews_daily`表，并运行与首次填充它时使用的相同`INSERT`语句。在这种情况下，您正在*完全刷新*模型。
- en: Only load new records from `PageViews` into `pageviews_daily`. In this case,
    you are *incrementally refreshing* the model.
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 仅将新记录从`PageViews`加载到`pageviews_daily`中。在这种情况下，您正在*增量刷新*模型。
- en: The first option is the least complex and less likely to result in any logical
    errors on the part of the analyst building the model. If the `INSERT` operation
    runs quickly enough for your use case, then I suggest taking this path. Beware,
    however! While the full refresh of the model might run quickly enough when it’s
    first developed, as the `PageViews` and `Customers` datasets grow, the runtime
    of the refresh will grow as well.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 第一种选项最简单，而且在建模的分析师运行模型时不太可能导致逻辑错误。如果`INSERT`操作在您的使用情况下运行得足够快，请选择这条路线。但是要注意！虽然模型的完全刷新在初次开发时可能运行得足够快，但随着`PageViews`和`Customers`数据集的增长，刷新的运行时间也会增加。
- en: The second option is a little more complicated but may result in a shorter runtime
    when you’re working with larger datasets. The tricky part of an incremental refresh
    in this case is the fact that the `pageviews_daily` table is granular to the day
    (date with no timestamp), while new records ingested into the `PageViews` table
    are granular to a full timestamp.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 第二个选项稍微复杂一些，但在处理较大数据集时可能会导致更短的运行时间。在这种情况下，增量刷新的棘手之处在于`pageviews_daily`表粒度为天（无时间戳的日期），而摄入到`PageViews`表中的新记录粒度为完整时间戳。
- en: Why is that a problem? It’s unlikely that you refreshed `pageviews_daily` at
    the end of a full day of records. In other words, though `pageviews_daily` has
    data for 2020-06-02, it’s possible that new records for that day will be loaded
    into `PageViews` in the next ingestion run.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么这是个问题？很可能您在记录一个完整的日期结束时没有刷新`pageviews_daily`。换句话说，虽然`pageviews_daily`有2020-06-02的数据，但可能会在下一次摄入运行中加载该日的新记录到`PageViews`中。
- en: '[Table 6-8](#pageviews_table_2) shows just that case. Two new records have
    been appended to the previous version of `PageViews` from [Table 6-7](#pageviews_table).
    The first of the new page views occurred on 2020-06-02, and the second was on
    the following day.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '[Table 6-8](https://example.org/pageviews_table_2) 就展示了这种情况。两条新记录已添加到之前版本的`PageViews`表格（来自[Table 6-7](https://example.org/pageviews_table)）。第一次新页面浏览发生在2020-06-02，第二次是第二天。'
- en: Table 6-8\. PageViews table with additional records
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: Table 6-8\. PageViews表格与额外记录
- en: '| CustomerId | ViewTime | UrlPath | utm_medium |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '| CustomerId | ViewTime | UrlPath | utm_medium |'
- en: '| --- | --- | --- | --- |'
  id: totrans-218
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| 100 | 2020-06-01 12:00:00 | /home | social |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 2020-06-01 12:00:00 | /home | social |'
- en: '| 100 | 2020-06-01 12:00:13 | /product/2554 | NULL |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 100 | 2020-06-01 12:00:13 | /product/2554 | NULL |'
- en: '| 101 | 2020-06-01 12:01:30 | /product/6754 | search |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 2020-06-01 12:01:30 | /product/6754 | search |'
- en: '| 102 | 2020-06-02 7:05:00 | /home | NULL |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 102 | 2020-06-02 7:05:00 | /home | NULL |'
- en: '| 101 | 2020-06-02 12:00:00 | /product/2554 | social |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 2020-06-02 12:00:00 | /product/2554 | social |'
- en: '| 102 | 2020-06-02 12:03:42 | /home | NULL |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 102 | 2020-06-02 12:03:42 | /home | NULL |'
- en: '| 101 | 2020-06-03 12:25:01 | /product/567 | social |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| 101 | 2020-06-03 12:25:01 | /product/567 | social |'
- en: 'Before I attempt to incremental refresh the `pageviews_daily` model, take a
    look at a snapshot of what it looks like currently:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 在我尝试增量刷新`pageviews_daily`模型之前，先来看一下当前的快照：
- en: '[PRE34]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: '[PRE35]'
  id: totrans-228
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'You can now insert the two new records shown in [Table 6-8](#pageviews_table_2)
    into your database using the following SQL statements:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，你可以使用以下SQL语句将[Table 6-8](https://example.org/pageviews_table_2)中显示的两条新记录插入到你的数据库中：
- en: '[PRE36]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'As a first attempt at an incremental refresh, you might simply include records
    from `PageViews` with a timestamp greater than the current `MAX(view_date)` in
    `pageviews_daily` (2020-06-02) into `pageviews_daily`. I’ll try that, but instead
    of inserting into `pageviews_daily`, I’ll create another copy of it called `pageviews_daily_2`
    and use that for this example. Why? Well, as you’ll see in a moment, this is not
    the correct approach! The SQL would look like the following:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 作为第一次增量刷新的尝试，你可以简单地将`PageViews`中时间戳大于当前`MAX(view_date)`（2020-06-02）的记录加入到`pageviews_daily`中。我会尝试这样做，但不是直接插入到`pageviews_daily`，而是创建另一个名为`pageviews_daily_2`的副本并用于本示例。为什么呢？因为，正如你马上会看到的，这并不是正确的方法！对应的SQL代码如下所示：
- en: '[PRE37]'
  id: totrans-232
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: However, as you can see in the following code, you’ll end up with several duplicate
    records because all events from 2020-06-02 at midnight and after are included
    in the refresh. In other words, page views from 2020-06-02 that were previously
    accounted for in the model are counted again. That’s because we don’t have the
    full timestamp stored in the daily granular `pageviews_daily` (and the copy named
    `pageviews_daily_2`). If this version of the model was used for reporting or analysis,
    the number of page views would be overstated!
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如你在下面的代码中看到的，你最终会得到几个重复的记录，因为所有2020-06-02午夜及以后的事件都包含在刷新中。换句话说，之前在模型中已经计算过的2020-06-02的页面浏览又被计算了一次。这是因为我们在每日粒度的`pageviews_daily`（及其副本`pageviews_daily_2`）中没有存储完整的时间戳。如果这个模型版本用于报告或分析，页面浏览次数将被高估！
- en: '[PRE38]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[PRE39]'
  id: totrans-235
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'If you sum up `view_count` by date, you’ll see that there are five page views
    on 2020-06-02 instead of the actual count of three from [Table 6-8](#pageviews_table_2).
    That’s because the two page views from that day that were previously added to
    `pageviews_daily_2` were added again:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 如果按日期汇总`view_count`，你会看到2020-06-02有五次页面浏览，而不是来自[Table 6-8](https://example.org/pageviews_table_2)的实际计数为三次。这是因为之前添加到`pageviews_daily_2`的两次页面浏览再次被添加进来：
- en: '[PRE40]'
  id: totrans-237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: '[PRE41]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'Another approach that many analysts take is to store the full timestamp of
    the final record from the `PageViews` table and use it as the next starting point
    for the incremental refresh. Like last time, I’ll create a new table (this time
    called `pageviews_daily_3`) for this attempt as it is the incorrect solution:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 许多分析师采取的另一种方法是存储`PageViews`表格中最后记录的完整时间戳，并将其用作增量刷新的下一个起始点。像上次一样，我会创建一个新表格（这次称为`pageviews_daily_3`），这并不是正确的解决方案：
- en: '[PRE42]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 'Again, if you take a look at the new version of `pageviews_daily_3`, you’ll
    notice something nonideal. Although the total number of page views for 2020-06-02
    is now correct (`3`), there are two rows that are the same (view_date of `2020-06-02`,
    `url_path` of `/home`, and `customer_country` of `UK`):'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，如果你查看新版本的`pageviews_daily_3`，你会注意到一些非理想的地方。虽然2020-06-02的总页面浏览次数现在是正确的（`3`），但有两行是相同的（`view_date`为`2020-06-02`，`url_path`为`/home`，`customer_country`为`UK`）：
- en: '[PRE43]'
  id: totrans-242
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[PRE44]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: Thankfully, in this case, the answer to the question of how many page views
    there are by day and country is correct. However, it’s wasteful to store data
    we don’t need. Those two records could have been combined into a single one with
    a `view_count` value of 2\. Though the sample table is small in this case, it’s
    not uncommon for such tables to have many billion records in reality. The number
    of unnecessary duplicated records add up and wastes storage and future query time.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，在这种情况下，按日和国家计算的页面浏览量是正确的。然而，存储不需要的数据是浪费的。这两条记录本可以合并成一条，视图计数值为2。尽管这种情况下示例表格很小，但在实际情况下，这样的表格通常会有数十亿条记录。不必要的重复记录数量会增加存储和未来查询时间的浪费。
- en: 'A better approach is to assume that more data has been loaded during the latest
    day (or week, month, and so on, based on the granularity of the table) in the
    model. The approach I’ll take is as follows:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 更好的方法是假设在最近一天（或一周、一个月等，根据表格的粒度）中加载了更多数据。我将采取的方法如下：
- en: Make a copy of `pageviews_daily` called `tmp_pageviews_daily` with all records
    up through the second to last day that it currently contains. In this example,
    that means all data through 2020-06-01.
  id: totrans-246
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建名为`tmp_pageviews_daily`的`pageviews_daily`副本，其中包含截止到当前包含的倒数第二天的所有记录。在本例中，这意味着所有数据都截至到2020-06-01。
- en: Insert all records from the source table (`PageViews`) into the copy starting
    on the next day (2020-06-02).
  id: totrans-247
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有源表（`PageViews`）的记录插入到从第二天（2020-06-02）开始的副本中。
- en: Truncate `pageviews_daily` and load the data from `tmp_pageviews_daily` into
    it.
  id: totrans-248
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 截断`pageviews_daily`并将数据从`tmp_pageviews_daily`加载到其中。
- en: Drop `tmp_pageviews_daily`.
  id: totrans-249
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 删除`tmp_pageviews_daily`。
- en: 'The final, and correct, SQL for the incremental refresh of the model is as
    follows:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 最终，模型增量刷新的正确SQL如下：
- en: '[PRE45]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Finally, the following is the result of the proper incremental refresh. The
    total count of page views is correct, and the data is stored as efficiently as
    possible, given the requirements of the model:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，以下是适当的增量刷新结果。页面浏览总数正确，且数据存储尽可能高效，符合模型要求：
- en: '[PRE46]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '[PRE47]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Modeling Change Capture Data
  id: totrans-255
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 模型变更捕获数据
- en: Recall from [Chapter 4](ch04.xhtml#ch04) that data ingested via CDC is stored
    in a specific way in the data warehouse after ingestion. For example, [Table 6-9](#the_orders_cdc_table)
    shows the contents of a table named `Orders_cdc` that’s been ingested via CDC.
    It contains the history of three orders in a source system.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 请回顾[第四章](ch04.xhtml#ch04)，通过CDC摄取的数据在摄取后以特定方式存储在数据仓库中。例如，[表6-9](#the_orders_cdc_table)显示了通过CDC摄取的名为`Orders_cdc`的表的内容。它包含源系统中三个订单的历史记录。
- en: Table 6-9\. The Orders_cdc table
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 表6-9\. Orders_cdc表
- en: '| EventType | OrderId | OrderStatus | LastUpdated |'
  id: totrans-258
  prefs: []
  type: TYPE_TB
  zh: '| EventType | OrderId | OrderStatus | LastUpdated |'
- en: '| --- | --- | --- | --- |'
  id: totrans-259
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- |'
- en: '| insert | 1 | Backordered | 2020-06-01 12:00:00 |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '| 插入 | 1 | Backordered | 2020-06-01 12:00:00 |'
- en: '| update | 1 | Shipped | 2020-06-09 12:00:25 |'
  id: totrans-261
  prefs: []
  type: TYPE_TB
  zh: '| 更新 | 1 | Shipped | 2020-06-09 12:00:25 |'
- en: '| delete | 1 | Shipped | 2020-06-10 9:05:12 |'
  id: totrans-262
  prefs: []
  type: TYPE_TB
  zh: '| 删除 | 1 | Shipped | 2020-06-10 9:05:12 |'
- en: '| insert | 2 | Backordered | 2020-07-01 11:00:00 |'
  id: totrans-263
  prefs: []
  type: TYPE_TB
  zh: '| 插入 | 2 | Backordered | 2020-07-01 11:00:00 |'
- en: '| update | 2 | Shipped | 2020-07-09 12:15:12 |'
  id: totrans-264
  prefs: []
  type: TYPE_TB
  zh: '| 更新 | 2 | Shipped | 2020-07-09 12:15:12 |'
- en: '| insert | 3 | Backordered | 2020-07-11 13:10:12 |'
  id: totrans-265
  prefs: []
  type: TYPE_TB
  zh: '| 插入 | 3 | Backordered | 2020-07-11 13:10:12 |'
- en: 'You can create and populate the `Orders_cdc` table with the following SQL statements:'
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下SQL语句创建并填充`Orders_cdc`表：
- en: '[PRE48]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Order 1’s record was first created when the order was placed, but in a status
    of `Backordered`. Eight days later, the record was updated in the source system
    when it shipped. A day later the record was deleted in the source system for some
    reason. Order 2 took a similar journey but was never deleted. Order 3 was first
    inserted when it was placed and has never been updated. Thanks to CDC, we not
    only know the current state of all orders, but also their full history.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 订单1的记录首次创建于下单时，但状态为`Backordered`。八天后，该记录在发货时在源系统中更新。一天后，由于某种原因，该记录在源系统中被删除。订单2经历了类似的旅程，但从未被删除。订单3在下单时首次插入，从未更新过。多亏了CDC，我们不仅知道所有订单的当前状态，还知道它们的完整历史。
- en: 'How to model data stored in this way depends on what questions the data model
    sets out to answer. For example, you might want to report on the current status
    of all orders for use on an operational dashboard. Perhaps the dashboard needs
    to display the number of orders currently in each state. A simple model would
    look something like this:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 如何建模以这种方式存储的数据取决于数据模型旨在回答什么问题。例如，您可能希望报告操作仪表板上所有订单的当前状态。也许仪表板需要显示每个状态中当前订单的数量。一个简单的模型可能看起来像这样：
- en: '[PRE49]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In this example, I use a CTE instead of a subquery to find the `MAX(LastUpdated)`
    timestamp for each `OrderId`. I then join the resulting CTE to the `Orders_cdc`
    table to get the `OrderStatus` of the most recent record for each order.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在此示例中，我使用了CTE而不是子查询来查找每个`OrderId`的`MAX(LastUpdated)`时间戳。然后，我将结果CTE与`Orders_cdc`表连接，以获取每个订单最新记录的`OrderStatus`。
- en: 'To answer the original question, you can see that two orders have an `OrderStatus`
    of `Shipped` and one is still `Backordered`:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 要回答最初的问题，您可以看到两个订单的`OrderStatus`为`Shipped`，还有一个订单仍然是`Backordered`：
- en: '[PRE50]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '[PRE51]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Is this the right answer to the question, however? Recall that while the latest
    status of `OrderId` 1 was currently `Shipped`, the `Order` record was deleted
    from the source database. Though that may seem like a poor system design, let’s
    say for now that when an order is canceled by a customer, it gets deleted from
    the source system. To take deletions into account, I’ll make a minor modification
    to the model refresh to ignore deletes:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这是否是正确的答案呢？请记住，虽然`OrderId` 1的最新状态目前是`Shipped`，但`Order`记录已从源数据库中删除。尽管这可能看起来像是一个糟糕的系统设计，但现在假设当客户取消订单时，订单将从源系统中删除。为了考虑到删除的情况，我将对模型刷新进行小的修改以忽略删除操作：
- en: '[PRE52]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'As you can see, the deleted order is no longer considered:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，删除的订单不再被考虑：
- en: '[PRE53]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '[PRE54]'
  id: totrans-279
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'Another common use for CDC-ingested data is making sense of the changes themselves.
    For example, perhaps an analyst wants to know how long, on average, orders take
    to go from a `Backordered` to `Shipped` status. I’ll again use a CTE (two this
    time!) to find the first date that each order was `Backordered` and `Shipped`.
    I’ll then subtract the two to get how many days each order that has been both
    backordered and shipped was in a status of `Backordered`. Note that this logic
    intentionally ignores `OrderId` 3, which is currently backordered but hasn’t yet
    shipped:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: CDC摄入数据的另一个常见用途是理解变更本身。例如，也许分析师想要知道订单从`Backordered`到`Shipped`状态平均需要多长时间。我将再次使用CTE（这次是两个！）来查找每个订单首次`Backordered`和`Shipped`的日期。然后我将这两个日期相减，以获取每个既是backordered又已经shipped的订单处于`Backordered`状态的天数。请注意，此逻辑有意忽略了`OrderId`
    3，该订单当前是backordered，但尚未发货：
- en: '[PRE55]'
  id: totrans-281
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: 'You can see the backorder times of each order as well as use the `AVG()` function
    to answer the original question:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以看到每个订单的backorder时间，以及使用`AVG()`函数来回答最初的问题：
- en: '[PRE56]'
  id: totrans-283
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[PRE57]'
  id: totrans-284
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[PRE58]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[PRE59]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: There are numerous other use cases for data that you have a full change history
    of, but just like modeling data that’s been fully loaded or is append-only, there
    are some common best practices and considerations.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有完整更改历史记录的数据，还有许多其他用例，但就像对已完全加载或仅追加的数据进行建模一样，有一些常见的最佳实践和考虑因素。
- en: Like the previous section, there are potential performance gains to be made
    by taking advantage of the fact that data ingested via CDC is loaded incrementally
    rather than fully refreshed. However, as noted in that section, there are times
    when the performance gain is not worth the added complexity of an incremental
    model refresh instead of a full refresh. In the case of working with CDC data,
    I find this to be true most times. The additional complexity of dealing with both
    updates and deletes is often enough to make a full refresh the preferred path.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 像前一节一样，通过利用CDC摄入的数据是增量加载而不是完全刷新，可以实现潜在的性能提升。但是，正如在该部分中指出的那样，有时性能增益并不值得采用增量模型刷新而不是全刷新的复杂性。在处理CDC数据时，我发现这种情况大多数情况下是正确的。处理更新和删除的额外复杂性通常足以使全刷新成为首选路径。
