- en: Part 2\. Classification
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二部分. 分类
- en: 'Now that we’ve covered some basic machine learning terminology, and your tidyverse
    skills are developing, let’s finally start learning some practical machine learning
    skills. The rest of the book is split into four parts:'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经覆盖了一些基本的机器学习术语，并且您的tidyverse技能正在发展，让我们最终开始学习一些实用的机器学习技能。本书的其余部分分为四个部分：
- en: Classification
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分类
- en: Regression
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归
- en: Dimension reduction
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 维度降低
- en: Clustering
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 聚类
- en: Within each of these parts, each chapter will focus on a different algorithm
    (or algorithms). Each chapter will start by explaining the theory behind how the
    algorithm learns, in a graphical way, and the rest of the chapter turns our knowledge
    into skills by applying the algorithm to a real dataset.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在这些各个部分中，每一章都将专注于不同的算法（或算法集）。每一章将从解释算法学习背后的理论开始，以图形化的方式呈现，然后本章的其余部分将通过将算法应用于真实数据集来将我们的知识转化为技能。
- en: Recall from [chapter 1](kindle_split_010.html#ch01) that classification and
    regression are both supervised learning tasks. They’re supervised because we have
    a ground truth we can use to train the models. We’re going to begin by focusing
    on the prediction of categorical variables in [chapters 3](kindle_split_013.html#ch03)
    through [8](kindle_split_018.html#ch08), so welcome to the classification part
    of the book. Alongside teaching you how the algorithms work and how to use them,
    I’ll also be teaching you a range of other machine learning skills, such as how
    to evaluate the performance of your models and how to tune models to maximize
    their performance. By the time you’ve completed this part of the book, I hope
    you’ll feel very confident using the mlr package in R for machine learning tasks.
    The mlr package creates a very simple, repetitive work flow for any machine learning
    task and will make your learning much simpler. Once we’ve completed the classification
    part of the book, we’ll move on to predicting continuous variables in the regression
    part.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第1章](kindle_split_010.html#ch01)，分类和回归都是监督学习任务。它们是监督学习，因为我们有一个可以用来训练模型的基准真实值。我们将从关注第3章到第8章（kindle_split_013.html#ch03
    到 kindle_split_018.html#ch08）中的分类变量的预测开始，欢迎来到本书的分类部分。在向您介绍算法的工作原理和使用方法的同时，我还会向您介绍一系列其他机器学习技能，例如如何评估模型性能以及如何调整模型以最大化其性能。当您完成本书的这一部分时，我希望您会对使用R中的mlr包进行机器学习任务感到非常有信心。mlr包为任何机器学习任务创建了一个非常简单、重复的工作流程，这将使您的学习变得更加简单。一旦我们完成了本书的分类部分，我们将转向回归部分，预测连续变量。
- en: Chapter 3\. Classifying based on similarities with k-nearest neighbors
  id: totrans-8
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章. 基于k近邻相似性的分类
- en: '*This chapter covers*'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding the bias-variance trade-off
  id: totrans-10
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解偏差-方差权衡
- en: Underfitting vs. overfitting
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合与过拟合
- en: Using cross-validation to assess model performance
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用交叉验证来评估模型性能
- en: Building a k-nearest neighbors classifier
  id: totrans-13
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建k近邻分类器
- en: Tuning hyperparameters
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 调整超参数
- en: This is probably the most important chapter of the entire book. In it, I’m going
    to show you how the k-nearest neighbors (kNN) algorithm works, and we’re going
    to use it to classify potential diabetes patients. In addition, I’m going to use
    the kNN algorithm to teach you some essential concepts in machine learning that
    we will rely on for the rest of the book.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能是整本书中最重要的章节。在这章中，我将向您展示k近邻（kNN）算法是如何工作的，我们将用它来对潜在的糖尿病患者进行分类。此外，我还会使用kNN算法向您介绍一些机器学习中的基本概念，这些概念我们将贯穿整本书。
- en: By the end of this chapter, not only will you understand and be able to use
    the kNN algorithm to make classification models, but you will be able to validate
    its performance and tune it to improve its performance as much as possible. Once
    the model is built, you’ll learn how to pass new, unseen data into it and get
    the data’s predicted classes (the value of the categorical or grouping variable
    we are trying to predict). I’ll introduce you to the extremely powerful mlr package
    in R, which contains a mouth-watering number of machine learning algorithms and
    greatly simplifies all of our machine learning tasks.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，您不仅将理解和能够使用kNN算法来构建分类模型，而且您还将能够验证其性能并调整它以尽可能提高其性能。一旦模型构建完成，您将学习如何将新的、未见过的数据输入其中，并获取数据的预测类别（我们试图预测的类别或分组变量的值）。我将向您介绍R中的极其强大的mlr包，它包含大量令人垂涎的机器学习算法，极大地简化了我们的所有机器学习任务。
- en: 3.1\. What is the k-nearest neighbors algorithm?
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1. k近邻算法是什么？
- en: 'I think the simple things in life are the best: playing Frisbee in the park,
    walking my dog, playing board games with my family, and using the kNN algorithm.
    Some machine learning practitioners look down on kNN a little because it’s very
    simplistic. In fact, kNN is arguably *the* simplest machine learning algorithm,
    and this is one of the reasons I like it so much. In spite of its simplicity,
    kNN can provide surprisingly good classification performance, and its simplicity
    makes it easy to interpret.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为生活中的简单事情是最好的：在公园里玩飞盘、遛狗、和家人玩桌游，以及使用kNN算法。一些机器学习从业者对kNN有点不屑，因为它非常简单。事实上，kNN可能是*最*简单的机器学习算法，这也是我喜欢它的一个原因。尽管它很简单，但kNN可以提供令人惊讶的分类性能，而且它的简单性使得它易于解释。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-20
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that, because kNN uses labeled data, it is a supervised learning algorithm.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，因为kNN使用标记数据，所以它是一个监督学习算法。
- en: '|  |'
  id: totrans-22
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.1.1\. How does the k-nearest neighbors algorithm learn?
  id: totrans-23
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.1\. k-最近邻算法是如何学习的？
- en: So how does kNN learn? Well, I’m going to use snakes to help me explain. I’m
    from the UK, where—some people are surprised to learn—we have a few native species
    of snake. Two examples are the grass snake and the adder, which is the only venomous
    snake in the UK. But we also have a cute, limbless reptile called a slow worm,
    which is commonly mistaken for a snake.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，kNN是如何学习的呢？好吧，我将用蛇来帮助我解释。我来自英国，有些人可能会惊讶地了解到，我们有一些本土的蛇种。两个例子是草蛇和蝰蛇，这是英国唯一的毒蛇。但我们还有一种可爱、无肢的爬行动物，叫做蚺蜥，它通常被误认为是蛇。
- en: Imagine that you work for a reptile conservation project aiming to count the
    numbers of grass snakes, adders, and slow worms in a woodland. Your job is to
    build a model that allows you to quickly classify reptiles you find into one of
    these three classes. When you find one of these animals, you only have enough
    time to rapidly estimate its length and some measure of how aggressive it is toward
    you, before it slithers away (funding is very scarce for your project). A reptile
    expert helps you manually classify the observations you’ve made so far, but you
    decide to build a kNN classifier to help you quickly classify future specimens
    you come across.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在一个旨在统计林地中草蛇、蝰蛇和蚺蜥数量的爬行动物保护项目中工作。你的任务是构建一个模型，让你能够快速地将你发现的爬行动物分类到这三个类别之一。当你发现这些动物之一时，你只有足够的时间快速估计它的长度以及它对你攻击性的某种度量，然后它就会滑走（你的项目资金非常紧张）。一位爬行动物专家帮助你手动分类你迄今为止所做的观察，但你决定构建一个kNN分类器来帮助你快速分类你将来遇到的样本。
- en: Look at the plot of data before classification in [figure 3.1](#ch03fig01).
    Each of our cases is plotted against body length and aggression, and the species
    identified by your expert is indicated by the shape of the datum. You go into
    the woodland again and collect data from three new specimens, which are shown
    by the black crosses.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.1](#ch03fig01)中查看分类前的数据图。我们的每个案例都是针对身体长度和攻击性绘制的，而你专家所识别的物种由数据点的形状表示。你再次进入林地并收集了三个新样本的数据，这些样本用黑色十字表示。
- en: Figure 3.1\. Body length and aggression of reptiles. Labeled cases for adders,
    grass snakes, and slow worms are indicated by their shape. New, unlabeled data
    are shown by black crosses.
  id: totrans-27
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.1\. 爬行动物的身体长度和攻击性。蝰蛇、草蛇和蚺蜥的标记案例由其形状表示。新的未标记数据用黑色十字表示。
- en: '![](fig3-1.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图3-1](fig3-1.jpg)'
- en: 'We can describe the kNN algorithm (and other machine learning algorithms) in
    terms of two phases:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用两个阶段来描述kNN算法（以及其他机器学习算法）：
- en: The training phase
  id: totrans-30
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 训练阶段
- en: The prediction phase
  id: totrans-31
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 预测阶段
- en: The training phase of the kNN algorithm consists only of storing the data. This
    is unusual among machine learning algorithms (as you’ll learn in later chapters),
    and it means that most of the computation is done during the prediction phase.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: kNN算法的训练阶段仅包括存储数据。这在机器学习算法中是不寻常的（你将在后面的章节中了解到），这意味着大部分计算都是在预测阶段完成的。
- en: During the prediction phase, the kNN algorithm calculates the *distance* between
    each new, unlabeled case and all the labeled cases. When I say “distance,” I mean
    their nearness in terms of the aggression and body-length variables, not how far
    away in the woods you found them! This distance metric is often called *Euclidean
    distance*, which in two or even three dimensions is easy to visualize in your
    head as the straight-line distance between two points on a plot (this distance
    is shown in [figure 3.2](#ch03fig02)). This is calculated in as many dimensions
    as are present in the data.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在预测阶段，kNN算法计算每个新的、未标记的案例与所有标记案例之间的*距离*。当我说“距离”时，我是指它们在攻击性和身体长度变量方面的接近程度，而不是你在树林中找到它们的距离！这个距离度量通常称为*欧几里得距离*，在二维甚至三维中，你可以将其想象为图上两点之间的直线距离（这个距离在[图3.2](#ch03fig02)中显示）。这是在数据中存在的维度数上计算的。
- en: 'Figure 3.2\. The first step of the kNN algorithm: calculating distance. The
    lines represent the distance between one of the unlabeled cases (the cross) and
    each of the labeled cases.'
  id: totrans-34
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.2。kNN算法的第一步：计算距离。线条代表一个未标记案例（十字架）与每个标记案例之间的距离。
- en: '![](fig3-2.jpg)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![图3-2](fig3-2.jpg)'
- en: Next, for each unlabeled case, the algorithm ranks the neighbors from the nearest
    (most similar) to the furthest (the least similar). This is shown in [figure 3.3](#ch03fig03).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，对于每个未标记的案例，算法将邻居从最近的（最相似的）到最远的（最不相似的）进行排序。这如图[图3.3](#ch03fig03)所示。
- en: 'Figure 3.3\. The second step of the kNN algorithm: ranking the neighbors. The
    lines represent the distance between one of the unlabeled cases (the cross) and
    each of the labeled cases. The numbers represent the ranked distance between the
    unlabeled case (the cross) and each labeled case (1 = closest).'
  id: totrans-37
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.3。kNN算法的第二步：排序邻居。线条代表一个未标记案例（十字架）与每个标记案例之间的距离。数字代表未标记案例（十字架）与每个标记案例（1 = 最接近）之间的排序距离。
- en: '![](fig3-3.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图3-3](fig3-3.jpg)'
- en: The algorithm identifies the *k*-labeled cases (neighbors) nearest to each unlabeled
    case. *k* is an integer specified by us (I’ll cover how we choose *k* in [section
    3.1](#ch03lev1sec1)). In other words, find the *k*-labeled cases that are most
    similar in terms of their variables to the unlabeled case. Finally, each of the
    k-nearest neighbor cases “votes” on which class the unlabeled data belongs in,
    based on the nearest neighbor’s own class. In other words, whatever class most
    of the k-nearest neighbors belong to is what the unlabeled case is classified
    as.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 算法识别每个未标记案例最近的*k*个标记案例（邻居）。*k*是一个我们指定的整数（我将在[3.1节](#ch03lev1sec1)中介绍我们如何选择*k*）。换句话说，找到与未标记案例在变量方面最相似的*k*个标记案例。最后，每个k个最近邻案例“投票”决定未标记数据属于哪个类别，基于最近邻自己的类别。换句话说，k个最近邻中大多数属于的类别就是未标记案例被分类为的类别。
- en: '|  |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-41
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Because all of its computation is done during the prediction phase, kNN is said
    to be a *lazy learner*.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 因为所有的计算都是在预测阶段完成的，所以kNN被称为*懒惰学习器*。
- en: '|  |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Let’s work through [figure 3.4](#ch03fig04) and see this in practice. When we
    set *k* to 1, the algorithm finds the single labeled case that is most similar
    to each of the unlabeled data items. Each of the unlabeled reptiles is closest
    to a member of the grass snake class, so they are all assigned to this class.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过[图3.4](#ch03fig04)来实际操作，看看这个方法。当我们把*k*设为1时，算法找到与每个未标记数据项最相似的单一标记案例。每个未标记的爬行动物最接近草蛇类的一个成员，所以它们都被分配到这个类别。
- en: 'Figure 3.4\. The final step of the kNN algorithm: identifying the k-nearest
    neighbors and taking the majority vote. Lines connect the unlabeled data with
    their one, three, and five nearest neighbors. The majority vote in each scenario
    is indicated by the shape drawn under each cross.'
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.4。kNN算法的最终步骤：识别k个最近邻并进行多数投票。线条将未标记的数据与它们的单个、三个和五个最近邻连接起来。每个场景的多数投票由每个十字架下绘制的形状表示。
- en: '![](fig3-4.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图3-4](fig3-4.jpg)'
- en: When we set *k* to 3, the algorithm finds the three labeled cases that are most
    similar to each of the unlabeled data items. As you can see in the figure, two
    of the unlabeled cases have nearest neighbors belonging to more than one class.
    In this situation, each nearest neighbor “votes” for its own class, and the majority
    vote wins. This is very intuitive because if a single unusually aggressive grass
    snake happens to be the nearest neighbor to an as-yet-unlabeled adder, it will
    be outvoted by the neighboring adders in the data.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们将 *k* 设置为 3 时，算法会找到与每个未标记数据项最相似的三个标记案例。正如你在图中可以看到的，两个未标记案例的最近邻属于多个类别。在这种情况下，每个最近邻“投票”支持它自己的类别，多数投票获胜。这非常直观，因为如果一条异常好斗的草蛇恰好是尚未标记的蝰蛇的最近邻，它将被数据中的邻近蝰蛇所投票击败。
- en: Hopefully now you can see how this extends to other values of *k*. When we set
    *k* to 5, for example, the algorithm simply finds the five nearest cases to the
    unlabeled data and takes the majority vote as the class of the unlabeled case.
    Notice that in all three scenarios, the value of *k* directly impacts how each
    unlabeled case is classified.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 希望现在你能看到这是如何扩展到 *k* 的其他值的。例如，当我们把 *k* 设置为 5 时，算法只是找到与未标记数据最近的五个案例，并将多数投票作为未标记案例的类别。请注意，在这三种情况下，*k*
    的值直接影响到每个未标记案例的分类。
- en: '|  |'
  id: totrans-49
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-50
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The kNN algorithm can actually be used for both classification *and* regression
    problems! I’ll show you how in [chapter 12](kindle_split_023.html#ch12), but the
    only difference is that instead of taking the majority class vote, the algorithm
    finds the mean or median of the nearest neighbors’ values.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: kNN 算法实际上可以用于分类 *和* 回归问题！我将在[第 12 章](kindle_split_023.html#ch12)中向你展示如何做到这一点，但唯一的区别是，算法不是采取多数类投票，而是找到最近邻值的平均值或中位数。
- en: '|  |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 3.1.2\. What happens if the vote is tied?
  id: totrans-53
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.1.2\. 如果投票结果平局会怎样？
- en: It may happen that all of the k-nearest neighbors belong to different classes
    and that the vote results in a tie. What happens in this situation? Well, one
    way we can avoid this in a two-class classification problem (when the data can
    only belong to one of two, mutually exclusive groups) is to ensure that we pick
    odd numbers of *k*. This way, there will always be a deciding vote. But what about
    in situations like our reptile classification problem, where we have more than
    two groups?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有可能所有 k 个最近邻属于不同的类别，投票结果出现平局。在这种情况下会发生什么？嗯，在二分类问题中（当数据只能属于两个互斥组之一时），我们可以通过确保选择奇数个
    *k* 来避免这种情况。这样，总会有一票决定。但在像我们的爬行动物分类问题这样的情况下，我们有超过两个组怎么办？
- en: One way of dealing with this situation is to decrease *k* until a majority vote
    can be won. But this doesn’t help if an unlabeled case is equidistant between
    its two nearest neighbors.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 处理这种情况的一种方法是将 *k* 减小，直到可以赢得多数投票。但如果一个未标记案例与它的两个最近邻等距，这并没有帮助。
- en: 'Instead, a more common (and pragmatic) approach is to randomly assign cases
    with no majority vote to one of the classes. In practice, the proportion of cases
    that have ties among their nearest neighbors is very small, so this has a limited
    impact on the classification accuracy of the model. However, if you have many
    ties in your data, your options are as follows:'
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，一种更常见（也更实用）的方法是将没有多数投票的案例随机分配到其中一个类别。实际上，在最近邻之间出现平局的案例比例非常小，所以这对模型的分类精度影响有限。然而，如果你数据中的平局很多，你的选择如下：
- en: Choose a different value of *k*.
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择不同的 *k* 值。
- en: Add a small amount of noise to the data.
  id: totrans-58
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向数据添加少量噪声。
- en: Consider using a different algorithm! I’ll show you how you can compare the
    performance of different algorithms on the same problem at the end of [chapter
    8](kindle_split_018.html#ch08).
  id: totrans-59
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑使用不同的算法！我将在[第 8 章](kindle_split_018.html#ch08)的末尾向你展示如何比较同一问题的不同算法的性能。
- en: 3.2\. Building your first kNN model
  id: totrans-60
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2\. 构建你的第一个 kNN 模型
- en: Imagine that you work in a hospital and are trying to improve the diagnosis
    of patients with diabetes. You collect diagnostic data over a few months from
    suspected diabetes patients and record whether they were diagnosed as healthy,
    chemically diabetic, or overtly diabetic. You would like to use the kNN algorithm
    to train a model that can predict which of these classes a new patient will belong
    to, so that diagnoses can be improved. This is a three-class classification problem.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你在医院工作，并试图改善糖尿病患者的诊断。你从疑似糖尿病患者那里收集了几个月的诊断数据，并记录他们是否被诊断为健康、化学性糖尿病或显性糖尿病。你希望使用kNN算法训练一个模型，可以预测新患者属于这些类别中的哪一个，以便提高诊断。这是一个三分类问题。
- en: 'We’re going to start with a simple, naive way of building a kNN model and then
    gradually improve it throughout the rest of the chapter. First things first—let’s
    install the mlr package and load it along with the tidyverse:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从构建一个简单的、直观的kNN模型开始，然后在接下来的章节中逐步改进它。首先的事情是——让我们安装mlr包并加载它以及tidyverse：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '|  |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-65
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: Installing the mlr package could take several minutes. You only need to do this
    once.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 安装mlr包可能需要几分钟。您只需要做一次。
- en: '|  |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.2.1\. Loading and exploring the diabetes dataset
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.1\. 加载和探索糖尿病数据集
- en: 'Now, let’s load some data built into the mclust package, convert it into a
    tibble, and explore it a little (recall from [chapter 2](kindle_split_011.html#ch02)
    that a tibble is the tidyverse way of storing rectangular data): see [listing
    3.1](#ch03ex01). We have a tibble with 145 cases and 4 variables. The `class`
    factor shows that 76 of the cases were non-diabetic (`Normal`), 36 were chemically
    diabetic (`Chemical`), and 33 were overtly diabetic (`Overt`). The other three
    variables are continuous measures of the level of blood glucose and insulin after
    a glucose tolerance test (`glucose` and `insulin`, respectively), and the steady-state
    level of blood glucose (`sspg`).'
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载mclust包中内置的一些数据，将其转换为tibble，并对其进行一些探索（回想一下[第2章](kindle_split_011.html#ch02)中提到的，tibble是tidyverse存储矩形数据的方式）：见[列表3.1](#ch03ex01)。我们有一个包含145个案例和4个变量的tibble。`class`因子显示76个案例是非糖尿病的（`Normal`），36个是化学性糖尿病的（`Chemical`），33个是显性糖尿病的（`Overt`）。其他三个变量是葡萄糖耐量测试后血糖和胰岛素水平的连续测量（分别称为`glucose`和`insulin`），以及血糖的稳态水平（`sspg`）。
- en: Listing 3.1\. Loading the diabetes data
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.1\. 加载糖尿病数据
- en: '[PRE1]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: To show how these variables are related, they are plotted against each other
    in [figure 3.5](#ch03fig05). The code to generate these plots is in [listing 3.2](#ch03ex02).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 为了展示这些变量之间的关系，它们被绘制在一起，见[图3.5](#ch03fig05)。生成这些图表的代码在[列表3.2](#ch03ex02)中。
- en: Figure 3.5\. Plotting the relationships between variables in `diabetesTib`.
    All three combinations of the continuous variables are shown, shaded by class.
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.5\. 在`diabetesTib`中绘制变量之间的关系。所有连续变量的三种组合都显示出来，并用颜色阴影表示类别。
- en: '![](fig3-5_alt.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![fig3-5_alt.jpg]'
- en: Listing 3.2\. Plotting the diabetes data
  id: totrans-75
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表3.2\. 绘制糖尿病数据
- en: '[PRE2]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Looking at the data, we can see there are differences in the continuous variables
    among the three classes, so let’s build a kNN classifier that we can use to predict
    diabetes status from measurements of future patients.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 观察数据，我们可以看到三个类别之间连续变量的差异，因此让我们构建一个kNN分类器，我们可以用它来预测未来患者的糖尿病状态。
- en: '|  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Reproduce the plot of `glucose` versus `insulin` shown in [figure 3.5](#ch03fig05),
    but use shapes rather than colors to indicate which class each case belongs to.
    Once you’ve done this, modify your code to represent the classes using shape *and*
    color.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制[图3.5](#ch03fig05)中`glucose`与`insulin`的关系图，但使用形状而不是颜色来表示每个案例所属的类别。完成此操作后，修改您的代码以使用形状*和*颜色来表示类别。
- en: '|  |'
  id: totrans-81
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Our dataset only consists of continuous predictor variables, but often we may
    be working with categorical predictor variables too. The kNN algorithm can’t handle
    categorical variables natively; they need to first be encoded somehow, or distance
    metrics other than Euclidean distance must be used.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据集只包含连续预测变量，但通常我们可能还要处理分类预测变量。kNN算法不能直接处理分类变量；它们需要首先以某种方式编码，或者必须使用除欧几里得距离之外的距离度量。
- en: It’s also very important for kNN (and many machine learning algorithms) to scale
    the predictor variables by dividing them by their standard deviation. This preserves
    the relationships between the variables, but ensures that variables measured on
    larger scales aren’t given more importance by the algorithm. In the current example,
    if we divided the `glucose` and `insulin` variables by 1,000,000, then predictions
    would rely mostly on the value of the `sspg` variable. We don’t need to scale
    the predictors ourselves because, by default, the kNN algorithm wrapped by the
    mlr package does this for us.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于kNN（以及许多机器学习算法）来说，通过将预测变量除以它们的方差来缩放预测变量也非常重要。这保留了变量之间的关系，但确保算法不会因为变量测量在更大的尺度上而给予它们更多的重视。在当前示例中，如果我们把`glucose`和`insulin`变量除以1000000，那么预测将主要依赖于`sspg`变量的值。我们不需要自己缩放预测变量，因为默认情况下，由mlr包包装的kNN算法会为我们完成这项工作。
- en: 3.2.2\. Using mlr to train your first kNN model
  id: totrans-84
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.2\. 使用mlr训练第一个kNN模型
- en: 'We understand the problem we’re trying to solve (classifying new patients into
    one of three classes), and now we need to train the kNN algorithm to build a model
    that will solve that problem. Building a machine learning model with the mlr package
    has three main stages:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 我们理解我们试图解决的问题（将新患者分类为三个类别之一），现在我们需要训练kNN算法来构建一个可以解决该问题的模型。使用mlr包构建机器学习模型有三个主要阶段：
- en: '***Define the task.*** The task consists of the data and what we want to do
    with it. In this case, the data is `diabetesTib`, and we want to classify the
    data with the `class` variable as the target variable.'
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***定义任务。*** 任务由数据和我们要对其做什么组成。在这种情况下，数据是`diabetesTib`，我们想要使用`class`变量作为目标变量进行数据分类。'
- en: '***Define the learner.*** The learner is simply the name of the algorithm we
    plan to use, along with any additional arguments the algorithm accepts.'
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***定义学习器。*** 学习器只是我们计划使用的算法的名称，以及算法接受的任何附加参数。'
- en: '***Train the model.*** This stage is what it sounds like: you pass the task
    to the learner, and the learner generates a model that you can use to make future
    predictions.'
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '***训练模型。*** 这个阶段正如其名：你将任务传递给学习器，学习器生成一个你可以用来进行未来预测的模型。'
- en: '|  |'
  id: totrans-89
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-90
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: This may seem unnecessarily cumbersome, but splitting the task, learner, and
    model into different stages is very useful. It means we can define a single task
    and apply multiple learners to it, or define a single learner and test it with
    multiple different tasks.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来有些过于繁琐，但将任务、学习器和模型分成不同的阶段是非常有用的。这意味着我们可以定义一个单独的任务并对其应用多个学习器，或者定义一个单独的学习器并用多个不同的任务对其进行测试。
- en: '|  |'
  id: totrans-92
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '3.2.3\. Telling mlr what we’re trying to achieve: Defining the task'
  id: totrans-93
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.3\. 告诉mlr我们想要实现的目标：定义任务
- en: Let’s begin by defining our task. The components needed to define a task are
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从定义我们的任务开始。定义任务所需的组件
- en: The data containing the predictor variables (variables we hope contain the information
    needed to make predictions/solve our problem)
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 包含预测变量的数据（我们希望包含用于做出预测/解决我们问题的所需信息的变量）
- en: The target variable we want to predict
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要预测的目标变量
- en: For supervised learning, the target variable will be categorical if we have
    a classification problem, and continuous if we have a regression problem. For
    unsupervised learning, we omit the target variable from our task definition, as
    we don’t have access to labeled data. The components of a task are shown in [figure
    3.6](#ch03fig06).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于监督学习，如果存在分类问题，目标变量将是分类的，如果存在回归问题，目标变量将是连续的。对于无监督学习，我们从任务定义中省略目标变量，因为我们没有访问到标记数据。任务组件在[图3.6](#ch03fig06)中展示。
- en: Figure 3.6\. Defining a task in mlr. A task definition consists of the data
    containing the predictor variables and, for classification and regression problems,
    a target variable we want to predict. For unsupervised learning, the target is
    omitted.
  id: totrans-98
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.6\. 在mlr中定义一个任务。任务定义包括包含预测变量的数据，以及对于分类和回归问题，我们想要预测的目标变量。对于无监督学习，目标变量被省略。
- en: '![](fig3-6.jpg)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-6.jpg)'
- en: 'We want to build a classification model, so we use the `makeClassifTask()`
    function to define a classification task. When we build regression and clustering
    models in [parts 3](kindle_split_019.html#part03) and [5](kindle_split_028.html#part05)
    of the book, we’ll use `makeRegrTask()` and `makeClusterTask()`, respectively.
    We supply the name of our tibble as the `data` argument and the name of the factor
    that contains the class labels as the `target` argument:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想构建一个分类模型，所以我们使用`makeClassifTask()`函数来定义一个分类任务。当我们构建第3部分和第5部分的回归和聚类模型时，我们将分别使用`makeRegrTask()`和`makeClusterTask()`。我们将我们的tibble的名称作为`data`参数，将包含类别标签的因子的名称作为`target`参数：
- en: '[PRE3]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|  |'
  id: totrans-102
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You may notice a warning message from mlr when you build the task, stating that
    your data is not a pure `data.frame` (it’s a tibble). This isn’t a problem, because
    the function will convert the tibble into a `data.frame` for you.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到当构建任务时，mlr会发出一个警告消息，指出你的数据不是一个纯`data.frame`（它是一个tibble）。这不是问题，因为该函数会为你将tibble转换为`data.frame`。
- en: '|  |'
  id: totrans-105
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'If we call the task, we can see it’s a classification task on the `diabetesTib`
    tibble, whose target is the `class` variable. We also get some information about
    the number of observations and the number of different types of variables (often
    called *features* in machine learning lingo). Some additional information includes
    whether we have missing data, the number of observations in each class, and which
    class is considered to be the “positive” class (only relevant for two-class tasks):'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们调用任务，我们可以看到它是在`diabetesTib` tibble上的分类任务，其目标是`class`变量。我们还获得了有关观测数和不同类型变量（在机器学习术语中通常称为*特征*）数量的信息。一些其他信息包括是否有缺失数据，每个类别的观测数，以及哪个类别被认为是“阳性”类别（仅适用于双类任务）：
- en: '[PRE4]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '3.2.4\. Telling mlr which algorithm to use: Defining the learner'
  id: totrans-108
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.4\. 告诉mlr使用哪个算法：定义学习器
- en: 'Next, let’s define our learner. The components needed to define a learner are
    as follows:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们定义我们的学习器。定义学习器所需的组件如下：
- en: 'The class of algorithm we are using:'
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用的算法类别：
- en: '`"classif."` for classification'
  id: totrans-111
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"classif."`用于分类'
- en: '`"regr."` for regression'
  id: totrans-112
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"regr."`用于回归'
- en: '`"cluster."` for clustering'
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"cluster."`用于聚类'
- en: '`"surv."` and `"multilabel."` for predicting survival and multilabel classification,
    which I won’t discuss'
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`"surv."`和`"multilabel."`用于预测生存和多层分类，这里不做讨论'
- en: The algorithm we are using
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们正在使用的算法
- en: Any additional options we may wish to use to control the algorithm
  id: totrans-116
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可能希望使用的任何其他选项来控制算法
- en: As you’ll see, the first and second components are combined together in a single
    character argument to define which algorithm will be used (for example, `"classif.knn"`).
    The components of a learner are shown in [figure 3.7](#ch03fig07).
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所看到的，第一个和第二个组件组合成一个字符参数来定义将使用哪个算法（例如，`"classif.knn"`）。学习器的组件在[图3.7](#ch03fig07)中显示。
- en: Figure 3.7\. Defining a learner in mlr. A learner definition consists of the
    class of algorithm you want to use, the name of the individual algorithm, and,
    optionally, any additional arguments to control the algorithm’s behavior.
  id: totrans-118
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.7\. 在mlr中定义学习器。学习器定义包括你想要使用的算法类别、单个算法的名称，以及可选的任何其他参数来控制算法的行为。
- en: '![](fig3-7.jpg)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-7.jpg)'
- en: We use the `makeLearner()` function to define a learner. The first argument
    to the `makeLearner()` function is the algorithm that we’re going to use to train
    our model. In this case, we want to use the kNN algorithm, so we supply `"classif.knn"`
    as the argument. See how this is the class (`"classif.`) joined to the name (`knn"`)
    of the algorithm?
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用`makeLearner()`函数来定义一个学习器。`makeLearner()`函数的第一个参数是我们将要用于训练模型的算法。在这种情况下，我们想使用kNN算法，所以我们提供`"classif.knn"`作为参数。看看这是如何将类（`"classif."`）与算法的名称（`knn"`)结合在一起的吗？
- en: 'The argument `par.vals` stands for parameter values, which allows us to specify
    the number of k-nearest neighbors we want the algorithm to use. For now, we’ll
    just set this to 2, but we’ll discuss how to choose *k* soon:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 参数`par.vals`代表参数值，这允许我们指定算法要使用的k-最近邻的数量。现在，我们将其设置为2，但我们将很快讨论如何选择*k*：
- en: '[PRE5]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '|  |'
  id: totrans-123
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**How to list all of mlr’s algorithms**'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '**如何列出mlr的所有算法**'
- en: The mlr package has a large number of machine learning algorithms that we can
    give to the `makeLearner()` function, more than I can remember without checking!
    To list all the available learners, simply use
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: mlr包有大量的机器学习算法可以提供给`makeLearner()`函数，多到我不检查就无法记住！要列出所有可用的学习器，只需使用
- en: '[PRE6]'
  id: totrans-126
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Or list them by function:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 或者按功能列表：
- en: '[PRE7]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: If you’re ever unsure which algorithms are available to you or which argument
    to pass to `makeLearner()` for a particular algorithm, use these functions to
    remind yourself.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不确定有哪些算法可供选择，或者对于特定的算法，应该传递给`makeLearner()`函数的哪个参数，可以使用这些函数来提醒自己。
- en: '|  |'
  id: totrans-130
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '3.2.5\. Putting it all together: Training the model'
  id: totrans-131
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.2.5. 将所有内容组合起来：训练模型
- en: Now that we’ve defined our task and our learner, we can now train our model.
    The components needed to train a model are the learner and task we defined earlier.
    The whole process of defining the task and learner and combining them to train
    the model is shown in [figure 3.8](#ch03fig08).
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的任务和我们的学习器，我们现在可以训练我们的模型了。训练模型所需的组件是我们之前定义的学习器和任务。定义任务和学习器并将它们组合起来以训练模型的全过程在[图3.8](#ch03fig08)中展示。
- en: Figure 3.8\. Training a model in mlr. Training a model simply consists of combining
    a learner with a task.
  id: totrans-133
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.8. 在mlr中训练模型。训练模型简单来说就是将学习器与任务相结合。
- en: '![](fig3-8.jpg)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-8.jpg)'
- en: 'This is achieved with the `train()` function, which takes the learner as the
    first argument and the task as its second argument:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 这是通过`train()`函数实现的，它将学习器作为第一个参数，将任务作为第二个参数：
- en: '[PRE8]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'We have our model, so let’s pass the data through it to see how it performs.
    The `predict()` function takes unlabeled data and passes it through the model
    to get the predicted classes. The first argument is the model, and the data being
    passed to it is given as the `newdata` argument:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经有了我们的模型，现在让我们通过它传递数据来查看它的表现。`predict()`函数接受未标记的数据并将其通过模型传递以获取预测类别。第一个参数是模型，传递给它的数据作为`newdata`参数给出：
- en: '[PRE9]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: We can pass these predictions as the first argument of the `performance()` function.
    This function compares the classes predicted by the model to the true classes,
    and returns *performance metrics* of how well the predicted and true values match
    each other. Use of the `predict()` and `performance()` functions is illustrated
    in [figure 3.9](#ch03fig09).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些预测作为`performance()`函数的第一个参数传递。这个函数将模型预测的类别与真实类别进行比较，并返回预测值和真实值匹配程度的*性能指标*。`predict()`和`performance()`函数的使用在[图3.9](#ch03fig09)中得到了说明。
- en: Figure 3.9\. A summary of the `predict()` and `performance()` functions of mlr.
    `predict()` passes observations into a model and outputs the predicted values.
    `performance()` compares these predicted values to the cases’ true values and
    outputs one or more performance metrics summarizing the similarity between the
    two.
  id: totrans-140
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.9. mlr的`predict()`和`performance()`函数的总结。`predict()`函数将观测值传递给模型并输出预测值。`performance()`函数将这些预测值与案例的真实值进行比较，并输出一个或多个性能指标，总结这两个值之间的相似性。
- en: '![](fig3-9_alt.jpg)'
  id: totrans-141
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-9_alt.jpg)'
- en: 'We specify which performance metrics we want the function to return by supplying
    them as a list to the `measures` argument. The two measures I’ve asked for are
    `mmce`, the *mean misclassification error*; and `acc`, or *accuracy*. MMCE is
    simply the proportion of cases classified as a class other than their true class.
    Accuracy is the opposite of this: the proportion of cases that were correctly
    classified by the model. You can see that the two sum to 1.00:'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过将它们作为列表提供给`measures`参数来指定我们希望函数返回的性能指标。我请求的两个指标是`mmce`，即*平均误分类误差*；和`acc`，或*准确率*。MMCE简单地说就是被错误分类为非真实类别的案例比例。准确率是它的对立面：模型正确分类的案例比例。你可以看到这两个指标的总和为1.00：
- en: '[PRE10]'
  id: totrans-143
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So our model is correctly classifying 95.2% of cases! Does this mean it will
    perform well on new, unseen patients? The truth is that *we don’t know*. Evaluating
    model performance by asking it to make predictions on data you used to train it
    in the first place tells you very little about how the model will perform when
    making predictions on completely unseen data. Therefore, you should *never* evaluate
    model performance this way. Before we discuss why, I want to introduce an important
    concept called the bias-variance trade-off.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的模型正确分类了95.2%的案例！这是否意味着它在处理新、未见过的患者时表现良好？事实是，*我们不知道*。通过要求模型在最初用于训练它的数据上做出预测来评估模型性能，告诉你关于模型在完全未见过的数据上做出预测时表现如何的信息非常有限。因此，你*永远*不应该以这种方式评估模型性能。在我们讨论原因之前，我想介绍一个重要的概念，称为偏差-方差权衡。
- en: '3.3\. Balancing two sources of model error: The bias-variance trade-off'
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3. 平衡模型误差的两个来源：偏差-方差权衡
- en: 'There is a concept in machine learning that is so important, and misunderstood
    by so many people, that I want to take the time to explain it well: the *bias-variance
    trade-off*. Let’s start with an example. A colleague sends you data about emails
    your company has received and asks you to build a model to classify incoming emails
    as junk or not junk (this is, of course, a classification problem). The dataset
    has 30 variables consisting of observations like the number of characters in the
    email, the presence of URLs, and the number of email addresses it was sent to,
    in addition to whether the email was junk or not.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习中有一个概念非常重要，但很多人对此理解有误，我想花时间好好解释一下：*偏差-方差权衡*。让我们从一个例子开始。一位同事给你发送了公司收到的电子邮件数据，并请你构建一个模型来分类
    incoming emails 为垃圾邮件或非垃圾邮件（这当然是一个分类问题）。数据集包含30个变量，包括电子邮件中的字符数、URL的存在以及发送到的电子邮件地址数量，以及电子邮件是否为垃圾邮件。
- en: You lazily build a classification model using only four of the predictor variables
    (because it’s nearly lunch and they’re serving katsu curry today). You send the
    model to your colleague, who implements it as the company’s junk filter.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 你懒洋洋地构建了一个仅使用四个预测变量的分类模型（因为快到午餐时间了，今天有咖喱鸡块供应）。你将模型发送给你的同事，他将其作为公司的垃圾邮件过滤器实施。
- en: 'A week later, your colleague comes back to you, complaining that the junk filter
    is performing badly and is consistently misclassifying certain types of emails.
    You pass the data you used to train the model back into the model, and find it
    correctly classifies only 60% of the emails. You decide that you may have *underfitted*
    the data: in other words, your model was too simple and was *biased* toward misclassifying
    certain types of emails.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 一周后，你的同事回来抱怨垃圾邮件过滤器表现不佳，并且持续错误地分类某些类型的电子邮件。你将用于训练模型的原始数据重新输入到模型中，发现它只正确分类了60%的电子邮件。你决定你可能对数据进行了*欠拟合*：换句话说，你的模型过于简单，并且倾向于错误分类某些类型的电子邮件。
- en: 'You go back to the data, and this time you include all 30 variables as predictors
    in your model. You pass the data back through your model and find that it correctly
    classifies 98% of the emails: an improvement, surely! You send this second model
    to your colleague and tell them you are certain it’s better. Another week goes
    by, and again, your colleague comes to you and complains that the model is performing
    badly: it’s misclassifying many emails, and in a somewhat unpredictable manner.
    You decide that you have *overfitted* the data: in other words, your model was
    too complex and is modeling noise in the data that you used to train it. Now,
    when you give new datasets to the model, there is a lot of *variance* in the predictions
    it gives. A model that is overfitted will perform well on the data used to train
    it, but poorly on new data.'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 你回到数据，这次你在模型中将所有30个变量作为预测变量。你将数据再次通过模型，发现它正确分类了98%的电子邮件：一个改进，当然！你将这个第二个模型发送给你的同事，并告诉他们你确信它更好。又过去了一周，你的同事再次来找你抱怨模型表现不佳：它错误地分类了许多电子邮件，并且以一种不可预测的方式。你决定你已经对数据进行了*过拟合*：换句话说，你的模型过于复杂，正在模拟你用于训练它的数据中的噪声。现在，当你给模型提供新的数据集时，它给出的预测有很大的*方差*。一个过拟合的模型在训练数据上表现良好，但在新的数据上表现较差。
- en: 'Underfitting and overfitting are two important sources of error in model building.
    In underfitting, we have included too few predictors or too simple a model to
    adequately describe the relationships/patterns in the data. The result is a model
    that is said to be *biased*: a model that performs poorly on both the data we
    use to train it and on new data.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建中，欠拟合和过拟合是两种重要的错误来源。在欠拟合中，我们包含的预测变量太少或模型过于简单，无法充分描述数据中的关系/模式。结果是，模型被认为是*有偏差的*：一个在训练数据和新的数据上表现都差的模型。
- en: '|  |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-152
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: Because we typically like to explain away as much variation in our data as possible,
    and because we often have many more variables than are important for our problem,
    underfitting is less frequently a problem than overfitting.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们通常喜欢解释掉数据中的尽可能多的变化，并且因为我们通常有比问题重要的变量多得多，所以欠拟合不如过拟合常见。
- en: '|  |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Overfitting is the opposite of underfitting and describes the situation where
    we include too many predictors or too complex a model, such that we are modeling
    not only the relationships/patterns in our data, but also the *noise*. Noise in
    a dataset is variation that is not systematically related to variables we have
    measured, but rather is due to inherent variability and/or error in measurement
    of our variables. The pattern of noise is very specific to an individual dataset,
    so if we start to model the noise, our model may perform very well on the data
    we trained it on but give quite variable results for future datasets.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 过拟合是欠拟合的对立面，描述了包含过多预测因子或过于复杂的模型的情况，以至于我们不仅模拟了数据中的关系/模式，还模拟了*噪声*。数据集中的噪声是与我们所测量的变量没有系统关系的变异，而是由于我们变量的固有变异和/或测量误差。噪声的模式对单个数据集非常具体，因此如果我们开始模拟噪声，我们的模型可能在训练数据上表现非常好，但对于未来的数据集给出相当可变的结果。
- en: 'Underfitting and overfitting both introduce error and reduce the *generalizability*
    of the model: the ability of the model to generalize to future, unseen data. They
    are also opposed to each other: somewhere between a model that underfits and has
    bias, and a model that overfits and has variance, is an optimal model that balances
    the bias-variance trade-off; see [figure 3.10](#ch03fig10).'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 欠拟合和过拟合都引入了误差并降低了模型的*泛化能力*：模型泛化到未来、未见数据的能力。它们也是对立的：在欠拟合且有偏差的模型和过拟合且有方差的模型之间，有一个平衡偏差-方差权衡的最优模型；参见[图3.10](#ch03fig10)。
- en: Figure 3.10\. The bias-variance trade-off. Generalization error is the proportion
    of erroneous predictions a model makes and is a result of overfitting and underfitting.
    The error associated with overfitting (too complex a model) is variance. The error
    associated with underfitting (too simple a model) is bias. The error associated
    with overfitting (too complex a model) is variance. An optimal model balances
    this trade-off.
  id: totrans-157
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.10. 偏差-方差权衡。泛化误差是模型做出错误预测的比例，是过拟合和欠拟合的结果。与过拟合（模型过于复杂）相关的误差是方差。与欠拟合（模型过于简单）相关的误差是偏差。与过拟合（模型过于复杂）相关的误差是方差。一个最优模型平衡了这种权衡。
- en: '![](fig3-10.jpg)'
  id: totrans-158
  prefs: []
  type: TYPE_IMG
  zh: '![图3-10](fig3-10.jpg)'
- en: Now, look at [figure 3.11](#ch03fig11). Can you see that the underfit model
    poorly represents the patterns in the data, and the overfit model is too granular
    and models noise in the data instead of the real patterns?
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，看看[图3.11](#ch03fig11)。你能看到欠拟合模型在表示数据中的模式方面表现不佳，而过拟合模型过于细致，并且模型了数据中的噪声而不是真实模式吗？
- en: Figure 3.11\. Examples of underfitting, optimal fitting, and overfitting for
    a two-class classification problem. The dotted line represents a decision boundary.
  id: totrans-160
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.11. 对于一个二分类问题的欠拟合、最优拟合和过拟合的示例。虚线代表决策边界。
- en: '![](fig3-11_alt.jpg)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![图3-11_替代](fig3-11_alt.jpg)'
- en: In the case of our kNN algorithm, selecting a small value of *k* (where only
    a small number of very similar cases are included in the vote) is more likely
    to model the noise in our data, resulting in a more complex model that is overfit
    and will produce a lot of variance when we use it to classify future patients.
    In contrast, selecting a large value of *k* (where more neighbors are included
    in the vote) is more likely to miss local differences in our data, resulting in
    a less complex model that is underfit and is biased toward misclassifying certain
    types of patients. I promise you’ll learn how to select *k* soon!
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的kNN算法的情况下，选择一个小的*k*值（其中只包含少量非常相似的案例进行投票）更有可能模拟我们数据中的噪声，导致一个更复杂的模型，当使用它来分类未来的患者时会产生很多方差。相比之下，选择一个大的*k*值（其中包含更多的邻居进行投票）更有可能错过我们数据中的局部差异，导致一个不太复杂的模型，是欠拟合的，并且倾向于错误分类某些类型的患者。我保证你很快就会学到如何选择*k*！
- en: So the question you’re probably asking now is, “How do I tell if I’m under-
    or overfitting?” The answer is a technique called *cross-validation*.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 所以你现在可能想知道的问题可能是，“我如何判断我是欠拟合还是过拟合？”答案是称为*交叉验证*的技术。
- en: 3.4\. Using cross-validation to tell if we’re overfitting or underfitting
  id: totrans-164
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4. 使用交叉验证来判断我们是否过拟合或欠拟合
- en: 'In the email example, once you had trained the second, overfit model, you tried
    to evaluate its performance by seeing how well it classified data you had used
    to train it. I mentioned that this is an extremely bad idea, and here is why:
    a model will almost always perform better on the data you trained it with than
    on new, unseen data. You can build a model that is extremely overfit, modeling
    all of the noise in the dataset, and you would never know, because passing the
    data back through the model gives you good predictive accuracy.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在电子邮件示例中，一旦你训练了第二个过度拟合的模型，你试图通过查看它对你用于训练的数据的分类效果来评估其性能。我提到这是一个极其糟糕的想法，原因如下：模型几乎总是会在你训练它的数据上比在新未见过的数据上表现更好。你可以构建一个极度过度拟合的模型，模拟数据集中的所有噪声，而你永远不会知道，因为将数据再次通过模型会给你良好的预测准确性。
- en: The answer is to evaluate the performance of your model on data it hasn’t seen
    yet. One way you could do this would be to train the model on all of the data
    available to you and then, over the next weeks and months, as you collect new
    data, pass it through your model and evaluate how the model performs. This approach
    is very slow and inefficient, and could make model building take years!
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 答案是评估你的模型在尚未见过的数据上的性能。你可以采取的一种方法是在所有可用的数据上训练模型，然后，在接下来的几周和几个月里，随着你收集新的数据，将其通过模型并评估模型的性能。这种方法非常缓慢且效率低下，可能会使模型构建耗时数年！
- en: 'Instead, we typically split our data in two. We use one portion to train the
    model: this portion is called the *training set*. We use the remaining portion,
    which the algorithm never sees during training, to test the model: this portion
    is the *test set*. We then evaluate how close the model’s predictions on the test
    set are to their true values. We summarize the closeness of these predictions
    with *performance metrics* that we’ll explore in [section 3.1](#ch03lev1sec1).
    Measuring how well the trained model performs on the test set helps us determine
    whether our model will perform well on unseen data, or whether we need to improve
    it further.'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们通常将数据分成两部分。我们使用一部分来训练模型：这部分被称为*训练集*。我们使用剩下的部分，算法在训练过程中从未见过，来测试模型：这部分是*测试集*。然后，我们评估模型在测试集上的预测与真实值之间的接近程度。我们用*性能指标*来总结这些预测的接近程度，我们将在[3.1节](#ch03lev1sec1)中探讨这些指标。测量训练模型在测试集上的表现有助于我们确定我们的模型是否会在未见过的数据上表现良好，或者我们是否需要进一步改进它。
- en: This process is called *cross-validation* (CV), and it is an extremely important
    approach in any supervised machine learning pipeline. Once we have cross-validated
    our model and are happy with its performance, we then use all the data we have
    (including the data in the test set) to train the final model (because typically,
    the more data we train our model with, the less bias it will have).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程被称为*交叉验证*（CV），在任何监督机器学习流程中都是一个极其重要的方法。一旦我们交叉验证了模型并且对其性能感到满意，我们就使用我们拥有的所有数据（包括测试集中的数据）来训练最终的模型（因为通常，我们训练模型的数据越多，其偏差就越小）。
- en: 'There are three common cross-validation approaches:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 常见的交叉验证方法有三种：
- en: Holdout cross-validation
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 保留法交叉验证
- en: K-fold cross-validation
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: K折交叉验证
- en: Leave-one-out cross-validation
  id: totrans-172
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 留一法交叉验证
- en: 3.5\. Cross-validating our kNN model
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.5\. 对我们的kNN模型进行交叉验证
- en: 'Let’s start by reminding ourselves of the task and learner we created earlier:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们先回顾一下我们之前创建的任务和学习器：
- en: '[PRE11]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Great! Before we train the final model on all the data, let’s cross-validate
    the learner. Ordinarily, you would decide on a CV strategy most appropriate for
    your data; but for the purposes of demonstration, I’m going to show you holdout,
    k-fold, *and* leave-one-out CV.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！在我们用所有数据训练最终模型之前，让我们先交叉验证学习器。通常，你会决定最适合你数据的CV策略；但为了演示目的，我将向你展示保留法、k折和*留一法*交叉验证。
- en: 3.5.1\. Holdout cross-validation
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.1\. 保留法交叉验证
- en: 'Holdout CV is the simplest method to understand: you simply “hold out” a random
    proportion of your data as your test set, and train your model on the remaining
    data. You then pass the test set through the model and calculate its performance
    metrics (we’ll talk about these soon). You can see a scheme of holdout CV in [figure
    3.12](#ch03fig12).'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 保留法交叉验证是理解起来最简单的方法：你只需随机保留你数据的一部分作为测试集，并在剩余的数据上训练你的模型。然后，你将测试集通过模型并计算其性能指标（我们很快会讨论这些）。你可以在[图3.12](#ch03fig12)中看到保留法交叉验证的方案。
- en: Figure 3.12\. Holdout CV. The data is randomly split into a training set and
    test set. The training set is used to train the model, which is then used to make
    predictions on the test set. The similarity of the predictions to the true values
    of the test set is used to evaluate model performance.
  id: totrans-179
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.12\. 保留集交叉验证。数据被随机分为训练集和测试集。训练集用于训练模型，然后使用该模型在测试集上进行预测。预测值与测试集真实值的相似性用于评估模型性能。
- en: '![](fig3-12.jpg)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![图3-12](fig3-12.jpg)'
- en: 'When following this approach, you need to decide what proportion of the data
    to use as the test set. The larger the test set is, the smaller your training
    set will be. Here’s the confusing part: performance estimation by CV is also subject
    to error and the bias-variance trade-off. If your test set is too small, then
    the estimate of performance is going to have high variance; but if the training
    set is too small, then the estimate of performance is going to have high bias.
    A commonly used split is to use two-thirds of the data for training and the remaining
    one-third as a test set, but this depends on the number of cases in the data,
    among other things.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 在遵循此方法时，您需要决定使用多少比例的数据作为测试集。测试集越大，训练集就越小。这里令人困惑的部分是：通过交叉验证进行性能估计也受误差和偏差-方差权衡的影响。如果您的测试集太小，那么性能估计将具有高方差；但如果训练集太小，那么性能估计将具有高偏差。常用的分割方法是使用三分之二的数据进行训练，剩余的三分之一作为测试集，但这取决于数据中的案例数量等因素。
- en: Making a holdout resampling description
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 创建保留集重采样描述
- en: 'The first step when employing any CV in mlr is to make a resampling description,
    which is simply a set of instructions for how the data will be split into test
    and training sets. The first argument to the `makeResampleDesc()` function is
    the CV method we’re going to use: in this case, `"Holdout"`. For holdout CV, we
    need to tell the function what proportion of the data will be used as the training
    set, so we supply this to the `split` argument:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 在mlr中应用任何CV的第一步是创建一个重采样描述，这仅仅是一组关于如何将数据分割为测试集和训练集的指令。`makeResampleDesc()`函数的第一个参数是我们将要使用的CV方法：在这种情况下，`"Holdout"`。对于保留集交叉验证，我们需要告诉函数将使用多少比例的数据作为训练集，因此我们将其提供给`split`参数：
- en: '[PRE12]'
  id: totrans-184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: I’ve included an additional, optional argument, `stratify = TRUE`. It asks the
    function to ensure that when it splits the data into training and test sets, it
    tries to maintain the proportion of each class of patient in each set. This is
    important in classification problems like ours, where the groups are very unbalanced
    (we have more healthy patients than both other groups combined) because, otherwise,
    we could get a test set with very few of one of our smaller classes.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 我包括了一个额外的、可选的参数，`stratify = TRUE`。它要求函数在将数据分割为训练集和测试集时，尝试保持每个集合中每个患者类别的比例。这在我们的分类问题中很重要，因为组别非常不平衡（我们拥有的健康患者比其他两组的总和还多），否则我们可能会得到一个包含我们较小类别中非常少数量的测试集。
- en: Performing holdout CV
  id: totrans-186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 进行保留集交叉验证
- en: 'Now that we’ve defined how we’re going to cross-validate our learner, we can
    run the CV using the `resample()` function. We supply the learner and task that
    we created, and the resampling method we defined a moment ago, to the `resample()`
    function. We also ask it to give us measures of MMCE and accuracy:'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了如何交叉验证我们的学习器，我们可以使用`resample()`函数运行CV。我们将我们创建的学习者和任务以及我们刚才定义的重采样方法提供给`resample()`函数。我们还要求它给我们提供MMCE和准确度的度量：
- en: '[PRE13]'
  id: totrans-188
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'The `resample()` function prints the performance measures when you run it,
    but you can access them by extracting the `$aggr` component from the `resampling`
    object:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 当您运行`resample()`函数时，它会打印性能指标，但您可以通过从`resampling`对象中提取`$aggr`组件来访问它们：
- en: '[PRE14]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'You’ll notice two things:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 您会注意到两点：
- en: The accuracy of the model as estimated by holdout cross-validation is less than
    when we evaluated its performance on the data we used to train the full model.
    This exemplifies my point earlier that models will perform better on the data
    that trained them than on unseen data.
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过保留集交叉验证估计的模型准确度低于我们在使用训练完整模型的数据上评估其性能时的准确度。这证明了我之前提到的观点，即模型在训练它们的数据上表现会比在未见过的数据上更好。
- en: Your performance metrics will probably be different than mine. In fact, run
    the `resample()` function over and over again, and you’ll get a very different
    result each time! The reason for this *variance* is that the data is randomly
    split into the test and training sets. Sometimes the split is such that the model
    performs well on the test set; sometimes the split is such that it performs poorly.
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你的性能指标可能和我的不同。实际上，反复运行 `resample()` 函数，你每次都会得到一个非常不同的结果！这种 *方差* 的原因是数据被随机分割成测试集和训练集。有时分割使得模型在测试集上表现良好；有时分割使得模型表现不佳。
- en: '|  |'
  id: totrans-194
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: Use the `makeResampleDesc()` function to create another holdout resampling description
    that uses 10% of the data as the test set and does *not* use stratified sampling
    (don’t overwrite your existing resampling description).
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `makeResampleDesc()` 函数创建另一个保留样本重采样描述，该描述使用10%的数据作为测试集，并且不使用分层抽样（不要覆盖你现有的重采样描述）。
- en: '|  |'
  id: totrans-197
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Calculating a confusion matrix
  id: totrans-198
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算混淆矩阵
- en: To get a better idea of which groups are being correctly classified and which
    are being misclassified, we can construct a confusion matrix. A *confusion matrix*
    is simply a tabular representation of the true and predicted class of each case
    in the test set.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更好地了解哪些组被正确分类，哪些组被错误分类，我们可以构建一个混淆矩阵。一个 *混淆矩阵* 简单地是测试集中每个案例的真实和预测类别的表格表示。
- en: 'With mlr, we can calculate the confusion matrix using the `calculateConfusionMatrix()`
    function. The first argument is the `$pred` component of our `holdoutCV` object,
    which contains the true and predicted classes of the test set. The optional argument
    `relative` asks the function to show the proportion of each class in the true
    and predicted class labels:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 使用mlr，我们可以通过 `calculateConfusionMatrix()` 函数计算混淆矩阵。第一个参数是我们 `holdoutCV` 对象的
    `$pred` 组件，它包含测试集的真实和预测类别。可选参数 `relative` 请求函数显示真实和预测类别标签中每个类别的比例：
- en: '[PRE15]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: The absolute confusion matrix is easier to interpret. The rows show the true
    class labels, and the columns show the predicted labels. The numbers represent
    the number of cases in every combination of true class and predicted class. For
    example, in this matrix, 11 patients were correctly classified as chemically diabetic,
    but one was erroneously classified as healthy. Correctly classified patients are
    found on the diagonal of the matrix (where true class == predicted class).
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对混淆矩阵更容易解释。行显示真实的类别标签，列显示预测的标签。数字代表真实类别和预测类别组合中每种情况的数量。例如，在这个矩阵中，11名患者被正确分类为化学性糖尿病患者，但其中一名被错误地分类为健康人。正确分类的患者位于矩阵的对角线上（即真实类别等于预测类别）。
- en: The relative confusion matrix looks a little more intimidating, but the principal
    is the same. This time, instead of the number of cases for each combination of
    true class and predicted class, we have the proportion. The number before the
    `/` is the proportion of the row in this column, and the number after the `/`
    is the proportion of the column in this row. For example, in this matrix, 92%
    of chemically diabetic patients were correctly classified, while 8% were misclassified
    as healthy. (Do you see that these are the proportions for the numbers I used
    for the absolute confusion matrix?)
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 相对混淆矩阵看起来有点吓人，但原理是相同的。这次，我们不是每个真实类别和预测类别组合的案例数量，而是比例。`/` 前面的数字是这一列中该行的比例，`/`
    后面的数字是该行中这一列的比例。例如，在这个矩阵中，92%的化学性糖尿病患者被正确分类，而8%被错误地分类为健康人。（你注意到这些比例和我在绝对混淆矩阵中使用的数字是一样的吗？）
- en: Confusion matrices help us understand which classes our model classifies well
    and which ones it does worse at classifying. For example, based on this confusion
    matrix, it looks like our model struggles to distinguish healthy patients from
    chemically diabetic ones.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 混淆矩阵帮助我们了解模型在哪些类别上分类得很好，在哪些类别上分类得较差。例如，根据这个混淆矩阵，看起来我们的模型在区分健康患者和化学性糖尿病患者方面有困难。
- en: '|  |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-206
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Does your confusion matrix look different than mine? Of course it does! The
    confusion matrix is based on the prediction made on the test set; and because
    the test set is selected at random in holdout CV, the confusion matrix will change
    every time you rerun CV.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你的混淆矩阵看起来和我的不一样吗？当然不一样了！混淆矩阵是基于对测试集的预测结果；由于测试集是在保留样本交叉验证中随机选择的，因此每次重新运行交叉验证时，混淆矩阵都会发生变化。
- en: '|  |'
  id: totrans-208
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As the performance metrics reported by holdout CV depend so heavily on how much
    of the data we use as the training and test sets, I try to avoid it unless my
    model is very expensive to train, so I generally prefer k-fold CV. The only real
    benefit of this method is that it is computationally less expensive than the other
    forms of CV. This can make it the only viable CV method for computationally expensive
    algorithms. But the purpose of CV is to get as accurate an estimation of model
    performance as possible, and holdout CV may give you very different results each
    time you apply it, because not all of the data is used in the training set and
    test set. This is where the other forms of CV come in.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 由于保留法交叉验证报告的性能指标严重依赖于我们用作训练集和测试集的数据量，因此我尽量避免使用它，除非我的模型训练成本非常高，所以我通常更喜欢k-fold交叉验证。这种方法唯一的真正好处是，它比其他形式的交叉验证计算成本更低。这可以使它成为计算成本高昂的算法的唯一可行交叉验证方法。但交叉验证的目的是尽可能准确地估计模型性能，而保留法交叉验证可能会在每次应用时给出非常不同的结果，因为并非所有数据都用于训练集和测试集。这就是其他形式的交叉验证发挥作用的地方。
- en: 3.5.2\. K-fold cross-validation
  id: totrans-210
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.2. K-fold交叉验证
- en: In k-fold CV, we randomly split the data into approximately equal-sized chunks
    called *folds*. Then we reserve one of the folds as a test set and use the remaining
    data as the training set (just like in holdout). We pass the test set through
    the model and make a record of the relevant performance metrics. Now, we use a
    different fold of the data as our test set and do the same thing. We continue
    until all the folds have been used once as the test set. We then get an average
    of the performance metric as an estimate of model performance. You can see a scheme
    of k-fold CV in [figure 3.13](#ch03fig13).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在k-fold交叉验证中，我们将数据随机分成大约等大小的块，称为*折叠*。然后我们保留其中一个折叠作为测试集，并使用剩余的数据作为训练集（就像保留法一样）。我们将测试集通过模型，并记录相关的性能指标。现在，我们使用数据的不同折叠作为我们的测试集，并做同样的事情。我们继续这样做，直到所有折叠都至少被用作一次测试集。然后我们得到性能指标的平均值，作为模型性能的估计。您可以在[图3.13](#ch03fig13)中看到k-fold交叉验证的方案。
- en: Figure 3.13\. K-fold CV. The data is randomly split into near equally sized
    folds. Each fold is used as the test set once, with the rest of the data used
    as the training set. The similarity of the predictions to the true values of the
    test set is used to evaluate model performance.
  id: totrans-212
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.13. K-fold交叉验证。数据被随机分成几乎等大小的块。每个块被用作测试集一次，其余数据用作训练集。使用测试集的真实值来评估模型性能的相似性。
- en: '![](fig3-13.jpg)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-13.jpg)'
- en: '|  |'
  id: totrans-214
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-215
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: It’s important to note that each case in the data appears in the test set only
    once in this procedure.
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要注意，在这个程序中，数据中的每个案例在测试集中只出现一次。
- en: '|  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: This approach will typically give a more accurate estimate of model performance
    because every case appears in the test set once, and we are averaging the estimates
    over many runs. But we can improve this a little by using *repeated* k-fold CV,
    where, after the previous procedure, we shuffle the data around and perform it
    again.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法通常会给出模型性能的更准确估计，因为每个案例在测试集中只出现一次，并且我们在多次运行中平均估计。但我们可以通过使用*重复*的k-fold交叉验证来稍微改进这一点，在这种方法中，在之前的程序之后，我们重新排列数据并再次执行。
- en: For example, a commonly chosen value of *k* for k-fold is 10\. Again, this depends
    on the size of the data, among other things, but it is a reasonable value for
    many datasets. This means we split the data into 10 nearly equal-sized chunks
    and perform the CV. If we repeat this procedure 5 times, then we have 10-fold
    CV repeated 5 times (this is *not* the same as 50-fold CV), and the estimate of
    model performance will be the average of 50 different runs.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，对于k-fold交叉验证，通常选择的k值是10。同样，这取决于数据的大小等因素，但对于许多数据集来说，这是一个合理的值。这意味着我们将数据分成10个几乎等大小的块，并执行交叉验证。如果我们重复这个程序5次，那么我们就有10次交叉验证重复了5次（这不同于50次交叉验证），模型性能的估计将是50次不同运行的平均值。
- en: Therefore, if you have the computational power, it is usually preferred to use
    repeated k-fold CV instead of ordinary k-fold. This is what we’ll be using in
    many examples in this book.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，如果您有计算能力，通常更倾向于使用重复的k-fold交叉验证而不是普通的k-fold交叉验证。这正是本书许多示例中将使用的方法。
- en: Performing k-fold CV
  id: totrans-221
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行k-fold交叉验证
- en: 'We perform k-fold CV in the same way as holdout. This time, when we make our
    resampling description, we tell it we’re going to use repeated k-fold cross-validation
    (`"RepCV"`), and we tell it how many folds we want to split the data into. The
    default number of folds is 10, which is often a good choice, but I want to show
    you how you can explicitly control the splits. Next, we tell the function that
    we want to repeat the 10-fold CV 50 times with the `reps` argument. This gives
    us 500 performance measures to average across! Again, we ask for the classes to
    be stratified among the folds:'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 我们以与保留法相同的方式进行k折交叉验证。这次，当我们创建重采样描述时，我们告诉它我们将使用重复的k折交叉验证（`"RepCV"`），并告诉它我们想要将数据分成多少折。默认折数是10，这通常是一个不错的选择，但我想向你展示如何显式地控制分割。接下来，我们告诉函数我们想要使用
    `reps` 参数重复10折交叉验证50次。这给我们提供了500个性能指标来平均！再次，我们要求在折之间分层分配类别：
- en: '[PRE16]'
  id: totrans-223
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Now let’s extract the average performance measures:'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们提取平均性能指标：
- en: '[PRE17]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The model correctly classified 89.8% of cases on average—much lower than when
    we predicted the data we used to train the model! Rerun the `resample()` function
    a few times, and compare the average accuracy after each run. The estimate is
    much more stable than when we repeated holdout CV.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型平均正确分类了89.8%的案例——这比我们预测用于训练模型的那些数据时低得多！多次重新运行 `resample()` 函数，并比较每次运行后的平均准确率。估计值比我们重复保留法交叉验证时的估计值更稳定。
- en: '|  |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-228
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: We’re usually only interested in the average performance measures, but you can
    access the performance measure from every iteration by running `kFoldCV$measures.test`.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通常只对平均性能指标感兴趣，但你可以通过运行 `kFoldCV$measures.test` 来访问每次迭代的性能指标。
- en: '|  |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Choosing the number of repeats
  id: totrans-231
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 选择重复次数
- en: Your goal when cross-validating a model is to get as accurate and stable an
    estimate of model performance as possible. Broadly speaking, the more repeats
    you can do, the more accurate and stable these estimates will become. At some
    point, though, having more repeats won’t improve the accuracy or stability of
    the performance estimate.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 当交叉验证模型时，你的目标是尽可能准确地估计模型性能。一般来说，你能够进行的重复越多，这些估计的准确性和稳定性就越高。然而，在某个点上，增加重复次数不会提高性能估计的准确性和稳定性。
- en: So how do you decide how many repeats to perform? A sound approach is to choose
    a number of repeats that is computationally reasonable, run the process a few
    times, and see if the average performance estimate varies a lot. If not, great.
    If it does vary a lot, you should increase the number of repeats.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，你如何决定执行多少次重复？一个合理的方法是选择一个计算上合理的重复次数，运行几次过程，看看平均性能估计是否变化很大。如果没有，那就很好。如果变化很大，你应该增加重复次数。
- en: '|  |'
  id: totrans-234
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: 'Define two new resampling descriptions: one that performs 3-fold CV repeated
    5 times, and one that performs 3-fold CV repeated 500 times (don’t overwrite your
    existing description). Use the `resample()` function to cross-validate the kNN
    algorithm using both of these resampling descriptions. Repeat the resampling five
    times for each method, and see which one gives more stable results.'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 定义两个新的重采样描述：一个执行5次重复的3折交叉验证，另一个执行500次重复的3折交叉验证（不要覆盖你现有的描述）。使用 `resample()` 函数使用这两种重采样描述对kNN算法进行交叉验证。每种方法重复重采样5次，看看哪一个给出更稳定的结果。
- en: '|  |'
  id: totrans-237
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Calculating a confusion matrix
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算混淆矩阵
- en: 'Now, let’s build the confusion matrix based on the repeated k-fold CV:'
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们基于重复的k折交叉验证构建混淆矩阵：
- en: '[PRE18]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '|  |'
  id: totrans-241
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that the number of cases is much larger. This is because we repeated
    the procedure 50 times.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 注意案例数量大得多。这是因为我们重复了50次该过程。
- en: '|  |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 3.5.3\. Leave-one-out cross-validation
  id: totrans-245
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.5.3\. 留一法交叉验证
- en: 'Leave-one-out CV can be thought of as the extreme of k-fold CV: instead of
    breaking the data into folds, we reserve a single observation as a test case,
    train the model on the whole of the rest of the data, and then pass the test case
    through it and record the relevant performance metrics. Next, we do the same thing
    but select a different observation as the test case. We continue doing this until
    every observation has been used once as the test case, where we take the average
    of the performance metrics. You can see a scheme of leave-one-out CV in [figure
    3.14](#ch03fig14).'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 留一法交叉验证可以被视为k折交叉验证的极端形式：我们不是将数据分成折，而是保留一个观测值作为测试案例，在其余所有数据上训练模型，然后通过它传递测试案例并记录相关的性能指标。接下来，我们做同样的事情，但选择不同的观测值作为测试案例。我们继续这样做，直到每个观测值都作为测试案例使用过一次，我们取性能指标的平均值。你可以在[图3.14](#ch03fig14)中看到留一法交叉验证的方案。
- en: Because the test set is only a single observation, leave-one-out CV tends to
    give quite variable estimates of model performance (because the performance estimate
    of each iteration depends on correctly labeling that single test case). But it
    can give less-variable estimates of model performance than k-fold when your dataset
    is small. When you have a small dataset, splitting it up into *k* folds will leave
    you with a very small training set. The variance of a model trained on a small
    dataset tends to be higher because it will be more influenced by sampling error/unusual
    cases. Therefore, leave-one-out CV is useful for small datasets where splitting
    it into *k* folds would give variable results. It is also computationally less
    expensive than repeated, k-fold CV.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 由于测试集只有一个观测值，留一法交叉验证往往会对模型性能给出相当可变估计（因为每个迭代的性能估计都取决于正确标记那个单个测试案例）。但是，当你的数据集较小时，它给出的模型性能估计比k折交叉验证更稳定。当你有一个小数据集时，将其分成*k*折会让你剩下一个非常小的训练集。在小数据集上训练的模型的方差往往较高，因为它会受到抽样误差/异常情况的影响更大。因此，留一法交叉验证对于将数据集分成*k*折会给出可变结果的小数据集是有用的。它也比重复的k折交叉验证计算成本更低。
- en: Figure 3.14\. Leave-one-out CV is the extreme of k-fold, where we reserve a
    single case as the test set and train the model on the remaining data. The similarity
    of the predictions to the true values of the test set is used to evaluate model
    performance.
  id: totrans-248
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.14。留一法交叉验证是k折交叉验证的极端形式，我们保留一个案例作为测试集，在剩余数据上训练模型。使用测试集的真实值与预测值的相似性来评估模型性能。
- en: '![](fig3-14_alt.jpg)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-14_alt.jpg)'
- en: '|  |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: A supervised learning model that has not been cross-validated is virtually useless,
    because you have no idea whether the predictions it makes on new data will be
    accurate or not.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 一个未经交叉验证的监督学习模型几乎毫无用处，因为你不知道它在新的数据上做出的预测是否准确。
- en: '|  |'
  id: totrans-253
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Performing leave-one-out CV
  id: totrans-254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 执行留一法交叉验证
- en: 'Creating a resampling description for leave-one-out is just as simple as for
    holdout and k-fold CV. We specify leave-one-out CV when making the resample description
    by supplying `LOO` as the argument to the method. Because the test set is only
    a single case, we obviously can’t stratify with leave-one-out. Also, because each
    case is used once as the test set, with all the other data used as the training
    set, there’s no need to repeat the procedure:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 创建留一法重采样的描述与留出法和k折交叉验证一样简单。我们在创建重采样描述时指定留一法交叉验证，通过将`LOO`作为方法的参数。因为测试集只有一个案例，所以我们显然不能使用留一法进行分层。另外，因为每个案例都作为测试集使用一次，其余所有数据作为训练集，所以没有必要重复该过程：
- en: '[PRE19]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '|  |'
  id: totrans-257
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 4**'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: 'Try to create two new leave-one-out resampling descriptions: one that uses
    stratified sampling, and one that repeats the procedure five times. What happens?'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试创建两个新的留一法重采样描述：一个使用分层抽样，另一个重复该过程五次。会发生什么？
- en: '|  |'
  id: totrans-260
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Now, let’s run the CV and get the average performance measures:'
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行交叉验证并获取平均性能指标：
- en: '[PRE20]'
  id: totrans-262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: If you rerun the CV over and over again, you’ll find that for this model and
    data, the performance estimate is more variable than for k-fold but less variable
    than for the holdout we ran earlier.
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你反复运行交叉验证，你会发现对于这个模型和数据，性能估计的变异性比k折交叉验证更大，但比我们之前运行的留出法更小。
- en: Calculating a confusion matrix
  id: totrans-264
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 计算混淆矩阵
- en: 'Once again, let’s look at the confusion matrix:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，让我们看看混淆矩阵：
- en: '[PRE21]'
  id: totrans-266
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: So you now know how to apply three commonly used types of cross-validation!
    If we’ve cross-validated our model and are happy that it will perform well enough
    on unseen data, then we would train the model on all of the data available to
    us, and use this to make future predictions.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，你现在知道了如何应用三种常用的交叉验证类型！如果我们已经交叉验证了我们的模型，并且满意它在未见过的数据上表现良好，那么我们就会在所有可用的数据上训练模型，并使用它来做出未来的预测。
- en: But I think we can still improve our kNN model. Remember how earlier, we manually
    choose a value of 2 for *k*? Well, randomly picking a value of *k* isn’t very
    clever, and there are much better ways we can find the optimal value.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 但是我认为我们仍然可以改进我们的 kNN 模型。记得之前我们手动选择 *k* 的值为 2 吗？好吧，随机选择一个 *k* 的值并不聪明，而且我们有更好的方法来找到最佳值。
- en: '3.6\. What algorithms can learn, and what they must be told: Parameters- s
    and hyperparameters'
  id: totrans-269
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.6. 算法可以学习的内容以及它们必须被告知的内容：参数和超参数
- en: Machine learning models often have *parameters* associated with them. A parameter
    is a variable or value that is estimated from the data and that is internal to
    the model and controls how it makes predictions on new data. An example of a model
    parameter is the slope of a regression line.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习模型通常与它们相关的 *参数* 有关。参数是从数据中估计的变量或值，它是模型内部的，并控制模型如何对新数据进行预测。模型参数的一个例子是回归线的斜率。
- en: 'In the kNN algorithm, *k* is not a parameter, because the algorithm doesn’t
    estimate it from the data (in fact, the kNN algorithm doesn’t actually learn any
    parameters). Instead, *k* is what’s known as a *hyperparameter*: a variable or
    option that controls how a model makes predictions but is *not* estimated from
    the data. As data scientists, we don’t have to provide parameters to our models;
    we simply provide the data, and the algorithms learn the parameters for themselves.
    We do, however, need to provide whatever hyperparameters they require. You’ll
    see throughout this book that different algorithms require and use different hyperparameters
    to control how they learn their models.'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在 kNN 算法中，*k* 不是一个参数，因为算法不会从数据中估计它（实际上，kNN 算法实际上并没有学习任何参数）。相反，*k* 是所谓的 *超参数*：一个控制模型如何进行预测的变量或选项，但它
    *不是* 从数据中估计的。作为数据科学家，我们不需要向我们的模型提供参数；我们只需提供数据，算法会自己学习参数。然而，我们确实需要提供它们所需的任何超参数。你会在本书中看到，不同的算法需要并使用不同的超参数来控制它们如何学习模型。
- en: 'So because *k* is a hyperparameter of the kNN algorithm, it can’t be estimated
    by the algorithm itself, and it’s up to us to choose a value. How do we decide?
    Well, there are three ways you can choose *k* or, in fact, any hyperparameter:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，由于 *k* 是 kNN 算法的超参数，它不能由算法本身估计，而需要我们选择一个值。我们如何决定呢？好吧，有三种方法你可以选择 *k*，或者实际上任何超参数：
- en: '***Pick a “sensible” or default value that has worked on similar problems before.***
    This option is a bad idea. You have no way of knowing whether the value of *k*
    you’ve chosen is the best one. Just because a value worked on other datasets doesn’t
    mean it will perform well on this dataset. This is the choice of the lazy data
    scientist who doesn’t care much about getting the most from their data.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***选择一个“合理”或默认值，这个值在之前类似的问题上已经有效。*** 这个选项是个坏主意。你无法知道你选择的 *k* 值是否是最佳的。仅仅因为某个值在其他数据集上有效，并不意味着它会在当前数据集上表现良好。这是懒惰的数据科学家选择，他们不太关心从数据中获得最大价值。'
- en: '***Manually try a few different values, and see which one gives you the best
    performance.*** This option is a bit better. The idea here is that you pick a
    few sensible values of *k*, build a model with each of them, and see which model
    performs the best. This is better because you’re more likely to find the best-performing
    value of *k*; but you’re still not guaranteed to find it, and doing this manually
    could be tedious and slow. This is the choice of the data scientist who cares
    but doesn’t really know what they’re doing.'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***手动尝试几个不同的值，看看哪个给你最好的性能。*** 这个选项稍微好一些。这里的想法是，你选择几个合理的 *k* 值，为每个值构建一个模型，并查看哪个模型表现最好。这更好，因为你更有可能找到最佳性能的
    *k* 值；但你仍然不能保证找到它，而且手动做这件事可能会很繁琐且缓慢。这是关心但不太清楚自己在做什么的数据科学家的选择。'
- en: '***Use a procedure called hyperparameter tuning to automate the selection process.***
    This solution is the best. It maximizes the likelihood of you finding the best-performing
    value of *k* while also automating the process for you. This is the method we’ll
    be using throughout the book.'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***使用一种称为超参数调优的流程来自动化选择过程。*** 这种解决方案是最好的。它最大化了你找到最佳性能的 *k* 值的可能性，同时为你自动化了这一过程。这是我们将在整本书中使用的这种方法。'
- en: '|  |'
  id: totrans-276
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-277
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: While the third option is generally the best if possible, some algorithms are
    so computationally expensive that they prohibit extensive hyperparameter tuning,
    in which case you may have to settle for manually trying different values.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然在可能的情况下，第三个选项通常是最好的，但有些算法计算成本非常高，以至于它们禁止进行广泛的超参数调整，在这种情况下，你可能不得不手动尝试不同的值。
- en: '|  |'
  id: totrans-279
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: But how does changing the value of *k* impact model performance? Well, values
    of *k* that are too low may start to model noise in the data. For example, if
    we set *k* = 1, then a healthy patient could be misclassified as chemically diabetic
    just because a single chemically diabetic patient with an unusually low insulin
    level was their nearest neighbor. In this situation, instead of just modeling
    the systematic differences between the classes, we’re also modeling the noise
    and unpredictable variability in the data.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 但改变 *k* 的值会如何影响模型性能呢？嗯，*k* 的值太低可能会导致开始模拟数据中的噪声。例如，如果我们设置 *k* = 1，那么一个健康的病人可能会被错误地分类为化学性糖尿病患者，仅仅是因为他们最近的邻居是一个胰岛素水平异常低的化学性糖尿病患者。在这种情况下，我们不仅模拟了类之间的系统性差异，还在模拟数据中的噪声和不可预测的变异性。
- en: On the other hand, if we set *k* too high, a large number of dissimilar patients
    will be included in the vote, and the model will be insensitive to local differences
    in the data. This is, of course, the bias-variance trade-off we talked about earlier.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，如果我们设置 *k* 太高，大量的不同病人将被包括在投票中，模型将不会对数据的局部差异敏感。这当然是我们在前面讨论过的偏差-方差权衡。
- en: 3.7\. Tuning k to improve the model
  id: totrans-282
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.7\. 调整 *k* 以改进模型
- en: Let’s apply hyperparameter tuning to optimize the value of *k* for our model.
    An approach we *could* follow would be to build models with different values of
    *k* using our full dataset, pass the data back through the model, and see which
    value of *k* gives us the best performance. This is bad practice, because there’s
    a large chance we’ll get a value of *k* that overfits the dataset we tuned it
    on. So once again, we rely on CV to help us guard against overfitting.
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们应用超参数调优来优化模型中 *k* 的值。我们可以采取的一种方法是用不同的 *k* 值构建模型，使用我们的完整数据集，然后将数据再次通过模型，看看哪个
    *k* 值能给出最佳性能。这是不好的做法，因为我们有很大可能会得到一个过度拟合我们所调优的数据集的 *k* 值。所以，我们又依靠交叉验证（CV）来帮助我们防止过拟合。
- en: 'The first thing we need to do is define a range of values over which mlr will
    try, when tuning *k*:'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是定义一个范围，mlr 将在调整 *k* 时尝试这个范围：
- en: '[PRE22]'
  id: totrans-285
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: The `makeDiscreteParam()` function inside the `makeParamSet()` function allows
    us to specify that the hyperparameter we’re going to be tuning is *k*, and that
    we want to search the values between 1 and 10 for the best value of *k*. As its
    name suggests, `makeDiscreteParam()` is used to define discrete hyperparameter
    values, such as *k* in kNN, but there are also functions to define continuous
    and logical hyperparameters that we’ll explore later in the book. The `makeParamSet()`
    function defines the hyperparameter space we defined as a parameter set, and if
    we wanted to tune more than one hyperparameter during tuning, we would simply
    separate them by commas inside this function.
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`makeDiscreteParam()` 函数位于 `makeParamSet()` 函数内部，允许我们指定我们将要调整的超参数是 *k*，并且我们想要在
    1 和 10 之间搜索最佳的 *k* 值。正如其名称所暗示的，`makeDiscreteParam()` 用于定义离散的超参数值，例如 kNN 中的 *k*，但书中还会探索定义连续和逻辑超参数的函数。`makeParamSet()`
    函数定义了我们定义的超参数空间作为参数集，如果我们想在调整过程中调整多个超参数，我们只需在这个函数内部用逗号简单地分隔它们。'
- en: 'Next, we define how we want mlr to search the parameter space. There are a
    few options for this, and in later chapters we’ll explore others, but for now
    we’re going to use the *grid search* method. This is probably the simplest method:
    it tries every single value in the parameter space when looking for the best-performing
    value. For tuning continuous hyperparameters, or when we are tuning several hyperparameters
    at once, grid search becomes prohibitively expensive, so other methods like *random
    search* are preferred:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义mlr如何搜索参数空间。这里有几种选择，在后面的章节中我们将探讨其他方法，但现在是使用*网格搜索*方法。这可能是最简单的方法：在寻找最佳性能值时，它会尝试参数空间中的每一个值。对于调整连续超参数，或者当我们同时调整多个超参数时，网格搜索变得过于昂贵，因此更倾向于使用像*随机搜索*这样的其他方法：
- en: '[PRE23]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Next, we define how we’re going to cross-validate the tuning procedure, and
    we’re going to use my favorite: repeated k-fold CV. The principle here is that
    for every value in the parameter space (integers 1 to 10), we perform repeated
    k-fold CV. For each value of *k*, we take the average performance measure across
    all those iterations and compare it with the average performance measures for
    all the other values of *k* we tried. This will hopefully give us the value of
    *k* that performs best:'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们定义我们将如何交叉验证调整过程，我将使用我最喜欢的：重复k折交叉验证。这里的原理是，对于参数空间中的每一个值（整数1到10），我们执行重复k折交叉验证。对于*k*的每一个值，我们取所有这些迭代中的平均性能度量，并将其与所有其他*k*值的平均性能度量进行比较。这可能会给我们提供性能最佳的*k*值：
- en: '[PRE24]'
  id: totrans-290
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'Now, we call the `tuneParams()` function to perform the tuning:'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们调用`tuneParams()`函数来执行调整：
- en: '[PRE25]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: The first and second arguments are the names of the algorithm and task we’re
    applying, respectively. We give our CV strategy as the `resampling` argument,
    the hyperparameter space we define as the `par.set` argument, and the search procedure
    to the `control` argument.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个和第二个参数分别是我们要应用的算法和任务的名称。我们将我们的CV策略作为`resampling`参数，我们将定义的超参数空间作为`par.set`参数，将搜索过程作为`control`参数。
- en: 'If we call our `tunedK` object, we get the best-performing value of *k*, 7,
    and the average MMCE value for that value. We can access the best-performing value
    of *k* directly by selecting the `$x` component:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们调用我们的`tunedK`对象，我们得到性能最佳的*k*值，7，以及该值的平均MMCE值。我们可以直接通过选择`$x`组件来访问性能最佳的*k*值：
- en: '[PRE26]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'We can also visualize the tuning process (the result of this code is shown
    in [figure 3.15](#ch03fig15)):'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以可视化调整过程（此代码的结果显示在[图3.15](#ch03fig15)中）：
- en: '[PRE27]'
  id: totrans-297
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now we can train our final model, using our tuned value of *k*:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以使用调整后的*k*值来训练我们的最终模型：
- en: '[PRE28]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Figure 3.15\. The MMCE values from fitting the kNN model with different values
    of *k* during a grid search
  id: totrans-300
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.15\. 在网格搜索过程中，使用不同值的*k*拟合kNN模型时的MMCE值
- en: '![](fig3-15.jpg)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](fig3-15.jpg)'
- en: This is as simple as wrapping the `makeLearner()` function, where we make a
    new kNN learner, inside the `setHyperPars()` function, and providing the tuned
    value of *k* as the `par.vals` argument. We then train our final model as before,
    using the `train()` function.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 这就像将`makeLearner()`函数（我们创建一个新的kNN学习器）包裹在`setHyperPars()`函数中一样简单，并提供调整后的*k*值作为`par.vals`参数。然后我们使用`train()`函数像以前一样训练我们的最终模型。
- en: 3.7.1\. Including hyperparameter tuning in cross-validation
  id: totrans-303
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.1\. 将超参数调整纳入交叉验证
- en: Now, when we perform some kind of preprocessing on our data or model, such as
    tuning hyperparameters, it’s important to include this preprocessing *inside*
    our CV, so that we cross-validate the whole model-training procedure. This takes
    the form of nested CV, where an inner loop cross-validates different values of
    our hyperparameter (just as we did earlier), and then the winning hyperparameter
    value gets passed to an outer CV loop. In the outer CV loop, the winning hyperparameters
    are used for each fold.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，当我们对我们的数据或模型进行某种预处理时，例如调整超参数，将这个预处理包括在我们的CV中是很重要的，这样我们就可以交叉验证整个模型训练过程。这采取的形式是嵌套CV，其中内部循环交叉验证我们的超参数的不同值（就像我们之前做的那样），然后获胜的超参数值被传递到外部CV循环。在外部CV循环中，获胜的超参数用于每个折。
- en: 'Nested CV proceeds like this:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌套CV的过程如下：
- en: Split the data into training and test sets (this can be done using the holdout,
    k-fold, or leave-one-out method). This division is called the *outer loop*.
  id: totrans-306
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据分为训练集和测试集（这可以通过保留、k折或留一法来完成）。这种划分称为*外部循环*。
- en: The training set is used to cross-validate each value of our hyperparameter
    search space (using whatever method we decide). This is called the *inner loop*.
  id: totrans-307
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练集交叉验证我们超参数搜索空间中的每个值（使用我们决定的方法）。这被称为“内层循环”。
- en: The hyperparameter that gives the best cross-validated performance from each
    inner loop is passed to the outer loop.
  id: totrans-308
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 来自每个内层循环的最佳交叉验证性能超参数被传递到外层循环。
- en: A model is trained on each training set of the outer loop, using the best hyperparameter
    from its inner loop. These models are used to make predictions on their test sets.
  id: totrans-309
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在外层循环的每个训练集上训练一个模型，使用其内层循环中的最佳超参数。这些模型用于对其测试集进行预测。
- en: The average performance metrics of these models across the outer loop are then
    reported as an estimate of how the model will perform on unseen data.
  id: totrans-310
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 这些模型在外层循环中的平均性能指标被报告为对模型在未见数据上表现的估计。
- en: If you prefer a graphical explanation, take a look at [figure 3.16](#ch03fig16).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你更喜欢图形化的解释，请查看[图3.16](#ch03fig16)。
- en: Figure 3.16\. Nested CV. The dataset is split into folds. For each fold, the
    training set is used to create sets of inner k-fold CV. Each of these inner sets
    cross-validates a single hyperparameter value by splitting the data into training
    and test sets. For each fold in these inner sets, a model is trained using the
    training set and evaluated on the test set, using that set’s hyperparameter value.
    The hyperparameter from each inner CV loop that gives the best-performing model
    is used to train the models on the outer loop.
  id: totrans-312
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图3.16。嵌套交叉验证。数据集被分成几个折。对于每个折，训练集被用来创建内层k折交叉验证的集合。这些内层集合通过将数据分成训练集和测试集来交叉验证单个超参数值。对于这些内层集合中的每个折，使用训练集训练一个模型，并使用该集合的超参数值在该测试集上进行评估。来自每个内层交叉验证循环的最佳性能超参数被用来在外层循环中训练模型。
- en: '![](fig3-16.jpg)'
  id: totrans-313
  prefs: []
  type: TYPE_IMG
  zh: '![图3-16](fig3-16.jpg)'
- en: In the example in [figure 3.16](#ch03fig16), the outer loop is 3-fold CV. For
    each fold, inner sets of 4-fold CV are applied, only using the training set from
    the outer loop. This 4-fold cross-validation is used to evaluate the performance
    of each hyperparameter value we’re searching over. The winning value of *k* (the
    one that gives the best performance) is then passed to the outer loop, which is
    then used to train the model, and its performance is evaluated on the test set.
    Can you see that we’re cross-validating the whole model-building process, including
    hyperparameter tuning?
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图3.16](#ch03fig16)的例子中，外层循环是3折交叉验证。对于每个折，应用4折交叉验证的内层集合，仅使用外层循环的训练集。这个4折交叉验证用于评估我们正在搜索的每个超参数值的性能。*k*（给出最佳性能的值）的获胜值随后传递到外层循环，然后用于训练模型，并在测试集上评估其性能。你能看到我们正在交叉验证整个模型构建过程，包括超参数调整吗？
- en: What’s the purpose of this? It validates our entire model-building procedure,
    including the hyperparameter-tuning step. The cross-validated performance estimate
    we get from this procedure should be a good representation of how we expect our
    model to perform on completely new, unseen data.
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 这有什么目的呢？它验证了我们的整个模型构建过程，包括超参数调整步骤。从这个过程中我们得到的交叉验证性能估计应该很好地代表我们期望模型在完全新的、未见数据上的表现。
- en: 'The process looks pretty complicated, but it is extremely easy to perform with
    mlr. First, we define how we’re going to perform the inner and outer CV:'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 这个过程看起来相当复杂，但使用mlr执行起来却非常简单。首先，我们定义我们将如何执行内层和外层交叉验证：
- en: '[PRE29]'
  id: totrans-317
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: I’ve chosen to perform ordinary k-fold cross-validation for the inner loop (10
    is the default number of folds) and 10-fold CV, repeated 5 times, for the outer
    loop.
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 我选择在外层循环中执行普通k折交叉验证（默认折数为10）和10折CV，重复5次。
- en: 'Next, we make what’s called a *wrapper*, which is basically a learner tied
    to some preprocessing step. In our case, this is hyperparameter tuning, so we
    create a tuning wrapper with `makeTuneWrapper()`:'
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们创建一个所谓的“包装器”，这基本上是一个与某些预处理步骤相关联的学习器。在我们的例子中，这是超参数调整，因此我们使用`makeTuneWrapper()`创建一个调整包装器：
- en: '[PRE30]'
  id: totrans-320
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Here, we supply the algorithm as the first argument and pass our inner CV procedure
    as the `resampling` argument. We supply our hyperparameter search space as the
    `par.set` argument and our `gridSearch` method as the `control` argument (remember
    that we created these two objects earlier). This “wraps” together the learning
    algorithm with the hyperparameter tuning procedure that will be applied inside
    the inner CV loop.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将算法作为第一个参数传递，并将我们的内部CV过程作为`resampling`参数。我们将超参数搜索空间作为`par.set`参数，并将我们的`gridSearch`方法作为`control`参数（记住我们之前创建了这两个对象）。这“包装”了学习算法和将在内部CV循环中应用的超参数调整过程。
- en: 'Now that we’ve defined our inner and outer CV strategies and our tuning wrapper,
    we run the nested CV procedure:'
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经定义了我们的内部和外部CV策略以及我们的调整包装器，我们运行嵌套CV过程：
- en: '[PRE31]'
  id: totrans-323
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: The first argument is the wrapper we created a moment ago, the second argument
    is the name of the task, and we supply our outer CV strategy as the resampling
    argument. Now sit back and relax—this could take a while!
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个参数是我们刚才创建的包装器，第二个参数是任务的名称，我们提供我们的外部CV策略作为重采样参数。现在请放松并休息——这可能需要一段时间！
- en: 'Once it finishes, you can print the average MMCE:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦完成，你可以打印平均MMCE：
- en: '[PRE32]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: Your MMCE value will probably be a little different than mine due to the random
    nature of the validation procedure, but the model is estimated to correctly classify
    91.4% of cases on unseen data. That’s not bad; and now that we’ve cross-validated
    our model properly, we can be confident we’re not overfitting our data.
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 由于验证过程的随机性，你的MMCE值可能与我有所不同，但模型估计在未见过的数据上正确分类了91.4%的案例。这还不错；现在我们已经正确地交叉验证了我们的模型，我们可以自信地认为我们没有过度拟合数据。
- en: 3.7.2\. Using our model to make predictions
  id: totrans-328
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 3.7.2\. 使用我们的模型进行预测
- en: 'We have our model, and we’re free to use it to classify new patients! Let’s
    imagine that some new patients come to the clinic:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有了我们的模型，我们可以自由地使用它来对新患者进行分类！让我们想象一些新患者来到诊所：
- en: '[PRE33]'
  id: totrans-330
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'We can pass these patients into our model and get their predicted diabetes
    status:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将这些患者输入我们的模型，并获取他们的预测糖尿病状态：
- en: '[PRE34]'
  id: totrans-332
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: Congratulations! Not only have you built your first machine learning model,
    but we’ve covered some reasonably complex theory, too. In the next chapter, we’re
    going to learn about logistic regression, but first I want to list the strengths
    and weaknesses of the k-nearest neighbor algorithm.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 恭喜！你不仅构建了你的第一个机器学习模型，我们还覆盖了一些相当复杂的理论。在下一章中，我们将学习逻辑回归，但首先我想列出k-最近邻算法的优缺点。
- en: 3.8\. Strengths and weaknesses of kNN
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.8\. kNN的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    kNN will perform well for your task.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪些算法会对给定的任务表现良好，但以下是一些优势和弱点，这将帮助你决定kNN是否适合你的任务。
- en: 'The strengths of the kNN algorithm are as follows:'
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: kNN算法的优势如下：
- en: The algorithm is very simple to understand.
  id: totrans-337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法非常简单易懂。
- en: There is no computational cost during the learning process; all the computation
    is done during prediction.
  id: totrans-338
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在学习过程中没有计算成本；所有计算都是在预测过程中完成的。
- en: It makes no assumptions about the data, such as how it’s distributed.
  id: totrans-339
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对数据没有任何假设，例如数据的分布情况。
- en: 'The weaknesses of the kNN algorithm are these:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: kNN算法的弱点如下：
- en: It cannot natively handle categorical variables (they must be recoded first,
    or a different distance metric must be used).
  id: totrans-341
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不能原生地处理分类变量（它们必须首先重新编码，或者必须使用不同的距离度量）。
- en: When the training set is large, it can be computationally expensive to compute
    the distance between new data and all the cases in the training set.
  id: totrans-342
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当训练集很大时，计算新数据与训练集中所有案例之间的距离可能会很昂贵。
- en: The model can’t be interpreted in terms of real-world relationships in the data.
  id: totrans-343
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型不能从数据中的现实世界关系来解释。
- en: Prediction accuracy can be strongly impacted by noisy data and outliers.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测准确性可能会受到噪声数据和异常值的影响。
- en: In high-dimensional datasets, kNN tends to perform poorly. This is due to a
    phenomenon you’ll learn about in [chapter 5](kindle_split_015.html#ch05), called
    the *curse of dimensionality*. In brief, in high dimensions the distances between
    the cases start to look the same, so finding the nearest neighbors becomes difficult.
  id: totrans-345
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在高维数据集中，kNN往往表现不佳。这是由于你将在[第5章](kindle_split_015.html#ch05)中了解到的一种现象，称为“维度诅咒”。简而言之，在高维中，案例之间的距离开始看起来相同，因此找到最近邻变得困难。
- en: '|  |'
  id: totrans-346
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 5**'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习5**'
- en: Load the iris dataset using the `data()` function, and build a kNN model to
    classify its three species of iris (including tuning the *k* hyperparameter).
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`data()`函数加载鸢尾花数据集，并构建一个kNN模型来对它的三种鸢尾花种类进行分类（包括调整*k*超参数）。
- en: '|  |'
  id: totrans-349
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-350
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 6**'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习6**'
- en: Cross-validate this iris kNN model using nested CV, where the outer CV is holdout
    with a two-thirds split.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌套交叉验证来交叉验证这个鸢尾花kNN模型，其中外部交叉验证采用三分之二的比例进行保留。
- en: '|  |'
  id: totrans-353
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-354
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 7**'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习7**'
- en: Repeat the nested CV as in the previous exercise, but using 5-fold, non-repeated
    CV as the outer loop. Which of these methods gives you a more stable MMCE estimate
    when you repeat them?
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 重复前一个练习中的嵌套CV，但使用5折、非重复CV作为外部循环。当你重复这些方法时，哪种方法给你更稳定的MMCE估计？
- en: '|  |'
  id: totrans-357
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Summary
  id: totrans-358
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: kNN is a simple supervised learning algorithm that classifies new data based
    on the class membership of its nearest *k* cases in the training set.
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: kNN是一种简单的监督学习算法，它根据训练集中最近的*k*个案例的类别成员资格对新数据进行分类。
- en: To create a machine learning model in mlr, we create a task and a learner, and
    then train the model using them.
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要在mlr中创建机器学习模型，我们创建一个任务和一个学习器，然后使用它们来训练模型。
- en: MMCE is the mean misclassification error, which is the proportion of misclassified
    cases in a classification problem. It is the opposite of accuracy.
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: MMCE是平均误分类误差，即在分类问题中误分类案例的比例。它与准确度相反。
- en: The bias-variance trade-off is the balance between two types of error in predictive
    accuracy. Models with high bias are underfit, and models with high variance are
    overfit.
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 偏差-方差权衡是预测准确度中两种类型错误的平衡。具有高偏差的模型欠拟合，而具有高方差模型的模型过拟合。
- en: Model performance should never be evaluated on the data used to train it; cross-validation
    should be used, instead.
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型性能不应在用于训练的数据上评估；应使用交叉验证。
- en: Cross-validation is a set of techniques for evaluating model performance by
    splitting the data into training and test sets.
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 交叉验证是一组通过将数据分为训练集和测试集来评估模型性能的技术。
- en: Three common types of cross-validation are holdout, where a single split is
    used; k-fold, where the data is split into *k* chunks and the validation performed
    on each chunk; and leave-one-out, where the test set is a single case.
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 三种常见的交叉验证类型是保留法，其中使用单个分割；k折法，其中数据被分割成*k*个块，并在每个块上执行验证；以及留一法，其中测试集是一个单独的案例。
- en: Hyperparameters are options that control how machine learning algorithms learn,
    which cannot be learned by the algorithm itself. Hyperparameter tuning is the
    best way to find optimal hyperparameters.
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 超参数是控制机器学习算法如何学习的选项，这些选项不能由算法本身学习。超参数调整是找到最佳超参数的最佳方式。
- en: If we perform a data-dependent preprocessing step, such as hyperparameter tuning,
    it’s important to incorporate this in our cross-validation strategy, using nested
    cross-validation.
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们执行一个数据依赖的前处理步骤，例如超参数调整，那么将其纳入我们的交叉验证策略中，使用嵌套交叉验证是很重要的。
- en: Solutions to exercises
  id: totrans-368
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习的解答
- en: 'Plot the `glucose` and `insulin` variables against each other, representing
    the `class` variable using shape, and then using shape *and* color:'
  id: totrans-369
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`葡萄糖`和`胰岛素`变量相互绘制，使用形状表示`类别`变量，然后使用形状*和*颜色：
- en: '[PRE35]'
  id: totrans-370
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Create a holdout resampling description that uses 10% of the cases as the test
    set and does not use stratified sampling:'
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 创建一个使用10%的案例作为测试集且不使用分层抽样的留一法重采样描述：
- en: '[PRE36]'
  id: totrans-372
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Compare the stability of the performance estimates of 3-fold cross-validation
    repeated 5 times or 500 times:'
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 比较重复5次或500次的3折交叉验证的性能估计的稳定性：
- en: '[PRE37]'
  id: totrans-374
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'Attempt to make leave-one-out resampling descriptions that use stratified sampling
    and repeated sampling:'
  id: totrans-375
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试制作使用分层抽样和重复抽样的留一法重采样描述：
- en: '[PRE38]'
  id: totrans-376
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE38]'
- en: 'Load the iris dataset, and build a kNN model to classify its three species
    of iris (including tuning the *k* hyperparameter):'
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 加载鸢尾花数据集，并构建一个kNN模型来对它的三种鸢尾花种类进行分类（包括调整*k*超参数）：
- en: '[PRE39]'
  id: totrans-378
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Cross-validate this iris kNN model using nested cross-validation, where the
    outer cross-validation is holdout with a two-thirds split:'
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用嵌套交叉验证交叉验证这个鸢尾花kNN模型，其中外部交叉验证采用三分之二的比例进行保留：
- en: '[PRE40]'
  id: totrans-380
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE40]'
- en: Repeat the nested cross-validation using 5-fold, non-repeated cross-validation
    as the outer loop. Which of these methods gives you a more stable MMCE estimate
    when you repeat them?
  id: totrans-381
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用5折、非重复交叉验证作为外部循环重复嵌套交叉验证。当你重复这些方法时，哪种方法给你更稳定的MMCE估计？
- en: '[PRE41]'
  id: totrans-382
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE41]'
- en: Chapter 4\. Classifying based on odds with logistic regression
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第4章\. 基于概率的逻辑回归进行分类
- en: '*This chapter covers*'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with the logistic regression algorithm
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与逻辑回归算法一起工作
- en: Understanding feature engineering
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解特征工程
- en: Understanding missing value imputation
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解缺失值插补
- en: 'In this chapter, I’m going to add a new classification algorithm to your toolbox:
    *logistic regression*. Just like the k-nearest neighbors algorithm you learned
    about in the previous chapter, logistic regression is a supervised learning method
    that predicts class membership. Logistic regression relies on the equation of
    a straight line and produces models that are very easy to interpret and communicate.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将向你工具箱中添加一个新的分类算法：**逻辑回归**。就像你在上一章中学到的k-最近邻算法一样，逻辑回归是一种监督学习方法，它预测类成员资格。逻辑回归依赖于直线的方程，并产生易于解释和传达的模型。
- en: Logistic regression can handle continuous (without discrete categories) and
    categorical (with discrete categories) predictor variables. In its simplest form,
    logistic regression is used to predict a binary outcome (cases can belong to one
    of two classes), but variants of the algorithm can handle multiple classes as
    well. Its name comes from the algorithm’s use of the *logistic function*, an equation
    that calculates the probability that a case belongs to one of the classes.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归可以处理连续（没有离散类别）和分类（有离散类别）的预测变量。在其最简单形式中，逻辑回归用于预测二元结果（案例可以属于两个类别之一），但算法的变体也可以处理多个类别。它的名字来源于算法使用**逻辑函数**，这是一个计算案例属于某一类别的概率的方程。
- en: While logistic regression is most certainly a classification algorithm, it uses
    *linear regression* and the equation for a straight line to combine the information
    from multiple predictors. In this chapter, you’ll learn how the logistic function
    works and how the equation for a straight line is used to build a model.
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然逻辑回归确实是一种分类算法，但它使用**线性回归**和直线的方程来结合多个预测器的信息。在本章中，你将学习逻辑函数是如何工作的，以及直线的方程是如何用来构建模型的。
- en: '|  |'
  id: totrans-391
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-392
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If you’re already familiar with linear regression, a key distinction between
    linear and logistic regression is that the former learns the relationship between
    predictor variables and a *continuous* outcome variable, whereas the latter learns
    the relationship between predictor variables and a *categorical* outcome variable.
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你已经熟悉线性回归，线性回归和逻辑回归之间的一个关键区别是，前者学习预测变量与一个**连续**的输出变量之间的关系，而后者学习预测变量与一个**分类**的输出变量之间的关系。
- en: '|  |'
  id: totrans-394
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: By the end of this chapter, you will have applied the skills you learned in
    [chapters 2](kindle_split_011.html#ch02) and [3](kindle_split_013.html#ch03) to
    prepare your data and build, interpret, and evaluate the performance of a logistic
    regression model. You will also have learned what *missing value imputation* is,
    a method for filling in missing data with sensible values when working with algorithms
    that cannot handle missing values. You will apply a basic form of missing value
    imputation as a strategy to deal with missing data.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章结束时，你将应用在[第2章](kindle_split_011.html#ch02)和[第3章](kindle_split_013.html#ch03)中学到的技能来准备你的数据，并构建、解释和评估逻辑回归模型的表现。你还将了解什么是**缺失值插补**，这是一种在处理无法处理缺失值的算法时，用合理的值填充缺失数据的方法。你将应用缺失值插补的基本形式作为处理缺失数据的一种策略。
- en: 4.1\. What is logistic regression?
  id: totrans-396
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.1\. 什么是逻辑回归？
- en: Imagine that you’re the curator of fifteenth-century art at a museum. When works
    of art, allegedly by famous painters, come to the museum, it’s your job to determine
    whether they are genuine or fake (a two-class classification problem). You have
    access to the chemical analysis performed on each painting, and you are aware
    that many forgeries of this period used paints with lower copper content than
    the original paintings. You can use logistic regression to learn a model that
    tells you the probability of a painting being an original based on the copper
    content of its paint. The model will then assign this painting to the class with
    the highest probability (see [figure 4.1](#ch04fig01)).
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是博物馆中15世纪艺术品的馆长。当据说由著名画家创作的艺术品来到博物馆时，你的任务是确定它们是真迹还是赝品（一个二分类问题）。你可以访问对每幅画进行的化学分析，并且你知道这个时期的许多赝品使用的颜料铜含量低于原作。你可以使用逻辑回归来学习一个模型，告诉你根据画中颜料的铜含量判断画作为真迹的概率。然后，该模型将把这幅画分配给概率最高的类别（见图4.1）。
- en: Figure 4.1\. Logistic regression learns models that output the probability (*p*)
    of new data belonging to each of the classes. Typically, new data is assigned
    the class to which it has the highest probability of belonging. The dotted arrow
    indicates that there are additional steps in calculating the probabilities, which
    we’ll discuss in [section 4.1.1](#ch04lev2sec1).
  id: totrans-398
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.1\. 逻辑回归学习模型，输出新数据属于每个类别的概率（*p*）。通常，新数据被分配给它最有可能属于的类别。虚线箭头表示在计算概率时还有额外的步骤，我们将在[4.1.1节](#ch04lev2sec1)中讨论。
- en: '![](fig4-1_alt.jpg)'
  id: totrans-399
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-1_alt.jpg)'
- en: '|  |'
  id: totrans-400
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-401
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The algorithm is commonly applied to two-class classification problems (this
    is referred to as *binomial* logistic regression), but a variant called *multinomial*
    logistic regression handles classification problems where you have three or more
    classes.
  id: totrans-402
  prefs: []
  type: TYPE_NORMAL
  zh: 该算法通常应用于二分类问题（这被称为*二项式*逻辑回归），但一种称为*多项式*逻辑回归的变体可以处理有三个或更多类别的分类问题。
- en: '|  |'
  id: totrans-403
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Logistic regression is a very popular classification algorithm, especially in
    the medical community, partly because of how interpretable the model is. For every
    predictor variable in our model, we get an estimate of just how the value of that
    variable impacts the probability that a case belongs to one class over another.
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归是一个非常流行的分类算法，在医学界尤其受欢迎，部分原因是因为模型的解释性。在我们的模型中，对于每个预测变量，我们都会得到一个估计值，说明该变量的值是如何影响案例属于某一类别而不是另一类别的概率的。
- en: We know that logistic regression learns models that estimate the probability
    of new cases belonging to each class. Let’s delve into how the algorithm learns
    the model.
  id: totrans-405
  prefs: []
  type: TYPE_NORMAL
  zh: 我们知道逻辑回归学习模型，这些模型估计新案例属于每个类别的概率。让我们深入了解算法是如何学习模型的。
- en: 4.1.1\. How does logistic regression learn?
  id: totrans-406
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.1\. 逻辑回归是如何学习的？
- en: Take a look at the (imaginary) data in [figure 4.2](#ch04fig02). I’ve plotted
    the copper content of a sample of paintings we know to be real or forgeries against
    their class as if it were a continuous variable between 0 and 1\. We can see that,
    on average, the forgeries contain less copper in their paint than the originals.
    We *could* model this relationship with a straight line, as shown in the figure.
    This approach works well when your predictor variable has a linear relationship
    with a *continuous* variable that you want to predict (we’ll cover this in [chapter
    9](kindle_split_020.html#ch09)); but as you can see, it doesn’t do a good job
    of modeling the relationship between a continuous variable and a *categorical*
    one.
  id: totrans-407
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下[图4.2](#ch04fig02)中的（想象中的）数据。我绘制了我们已知是真实或伪造的样本画作中的铜含量，将其类别作为0到1之间的连续变量。我们可以看到，平均而言，伪造品在其油漆中的铜含量比原作少。我们可以用直线来模拟这种关系，如图所示。当你的预测变量与你要预测的连续变量有线性关系时，这种方法效果很好（我们将在[第9章](kindle_split_020.html#ch09)中介绍）；但正如你所见，它并不能很好地模拟连续变量与类别变量之间的关系。
- en: Figure 4.2\. Plotting copper content against class. The y-axis displays the
    categorical class membership as if it were a continuous variable, with forgeries
    and originals taking the values of 0 and 1, respectively. The solid line represents
    a poor attempt to model a linear relationship between copper content and class.
    The dashed line at y = 0.5 indicates the threshold of classification.
  id: totrans-408
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.2\. 将铜含量与类别进行绘图。y轴显示类别成员资格，仿佛它是一个连续变量，伪造品和原作分别取值为0和1。实线代表尝试用线性关系来模拟铜含量与类别之间的关系。y
    = 0.5处的虚线表示分类的阈值。
- en: '![](fig4-2.jpg)'
  id: totrans-409
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-2.jpg)'
- en: As shown in the figure, we could find the copper content at which the straight
    line passes halfway between 0 and 1, and classify paintings with copper content
    below this value as forgeries and paintings above the value as originals. This
    might result in many misclassifications, so a better approach is needed.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，我们可以找到铜含量，使得直线穿过0和1之间的中点，并将铜含量低于此值的画作归类为伪造品，高于此值的画作归类为原作。这可能会导致许多误分类，因此需要更好的方法。
- en: We can better model the relationship between copper content and class membership
    using the *logistic* function, which is shown in [figure 4.3](#ch04fig03). The
    logistic function is an S-shaped curve that maps a continuous variable (copper
    content, in our case) onto values between 0 and 1\. This does a much better job
    of representing the relationship between copper content and whether a painting
    is an original or forgery. The figure shows a logistic function fit to the same
    data as in [figure 4.2](#ch04fig02). We could find the copper content at which
    the logistic function passes halfway between 0 and 1, and classify paintings with
    copper content below this value as forgeries and paintings above the value as
    originals. This typically results in fewer misclassifications than when we do
    this using a straight line.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用*逻辑*函数更好地建模铜含量与类别成员之间的关系，如图4.3所示。逻辑函数是一个S形曲线，将连续变量（在我们的例子中是铜含量）映射到0和1之间的值。这比用直线表示铜含量与画作是否为原作之间的关系要好得多。该图显示了与图4.2相同的逻辑函数拟合数据。我们可以找到逻辑函数在0和1之间通过一半值时的铜含量，并将铜含量低于此值的画作归类为伪造品，高于此值的画作归类为原作。这通常比我们使用直线分类时产生的误分类要少。
- en: Figure 4.3\. Modeling the data with the logistic function. The S-shaped curve
    represents the logistic function fitted to the data. The center of the curve passes
    through the mean of copper content and maps it between 0 and 1.
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.3\. 使用逻辑函数建模数据。S形曲线表示拟合数据的逻辑函数。曲线的中心通过铜含量的平均值，并将其映射到0和1之间。
- en: '![](fig4-3.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![图4.3](fig4-3.jpg)'
- en: Importantly, as the logistic function maps our *x* variable between the values
    of 0 and 1, we can interpret its output as the probability of a case with a particular
    copper content being an original painting. Take another look at [figure 4.3](#ch04fig03).
    Can you see that as copper content increases, the logistic function approaches
    1? This represents the fact that, on average, original paintings have a higher
    copper content, so if you pick a painting at random and find that it has a copper
    content of 20, it has a ~ 0.99 or 99% probability of being an original.
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是，由于逻辑函数将我们的*x*变量映射到0和1之间的值，我们可以将其输出解释为具有特定铜含量的案例是原作的概率。再看看图4.3。你能看到随着铜含量的增加，逻辑函数趋近于1吗？这表示，平均而言，原作铜含量较高，因此如果你随机选择一幅画，发现其铜含量为20，那么它有大约0.99或99%的概率是原作。
- en: '|  |'
  id: totrans-415
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-416
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If I had coded the grouping variable the other way around (with forgeries being
    1 and originals being 0), then the logistic function would approach 1 for low
    values of copper and approach 0 for high values. We would simply interpret the
    output as the probability of being a forgery, instead.
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我以另一种方式编码分组变量（伪造品为1，原作为0），那么逻辑函数在低铜含量时趋近于1，在高铜含量时趋近于0。我们只需将输出解释为伪造品的概率即可。
- en: '|  |'
  id: totrans-418
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'The opposite is also true: as copper content decreases, the logistic function
    approaches 0\. This represents the fact that, on average, forgeries have lower
    copper content, so if you pick a painting at random and find it has a copper content
    of 7, it has a ~ 0.99 or 99% probability of being a forgery.'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 反之亦然：随着铜含量的减少，逻辑函数趋近于0。这表示，平均而言，伪造品铜含量较低，因此如果你随机选择一幅画，发现其铜含量为7，那么它有大约0.99或99%的概率是伪造品。
- en: Great! We can estimate the probability of a painting being an original by using
    the logistic function. But what if we have more than one predictor variable? Because
    probabilities are bounded between 0 and 1, it’s difficult to combine the information
    from two predictors. For example, say the logistic function estimates that a painting
    has a 0.6 probability of being an original for one predictor variable, and a 0.7
    probability for the other predictor. We can’t simply add these estimates together,
    because they would be larger than 1, and this wouldn’t make sense.
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们可以通过使用逻辑函数来估计一幅画是原作的概率。但如果我们有多个预测变量呢？由于概率被限制在0和1之间，很难结合两个预测变量的信息。例如，假设逻辑函数估计一幅画对于某个预测变量有0.6的概率是原作，对于另一个预测变量有0.7的概率。我们不能简单地将这些估计相加，因为它们会超过1，这没有意义。
- en: Instead, we can take these probabilities and convert them into their *log odds*
    (the “raw” output from logistic regression models). To introduce log odds, let
    me first explain what I mean by *odds*, and the difference between odds and probability.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以将这些概率转换为它们的*对数几率*（逻辑回归模型的“原始”输出）。为了介绍对数几率，让我首先解释一下我所说的*几率*，以及几率和概率之间的区别。
- en: The odds of a painting being an original are
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: 画作是原作的几率
- en: equation 4.1\.
  id: totrans-423
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式4.1。
- en: '![](eq4-1.jpg)'
  id: totrans-424
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-1](eq4-1.jpg)'
- en: You may come across this written as
  id: totrans-425
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会看到它写成
- en: equation 4.2\.
  id: totrans-426
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式4.2。
- en: '![](eq4-2.jpg)'
  id: totrans-427
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-2](eq4-2.jpg)'
- en: Odds are a convenient way of representing the likelihood of something occurring.
    They tell us how much more likely an event is to occur, rather than how likely
    it is *not* to occur.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 几率是表示某事发生可能性的便捷方式。它们告诉我们事件发生的可能性有多大，而不是不发生的可能性有多大。
- en: In *The Empire Strikes Back*, C3PO says that the odds of “successfully navigating
    an asteroid field are approximately 3,720 to 1!” What C3PO was trying to tell
    Han and Leia was that the probability of successfully navigating an asteroid field
    is approximately 3,720 times smaller than the probability of *unsuccessfully*
    navigating it. Simply stating the odds is often a more convenient way of representing
    likelihood because we know that, for every 1 asteroid field that was successfully
    navigated, 3,720 were not! Additionally, whereas probability is bounded between
    0 and 1, odds can take any positive value.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在《帝国反击战》中，C3PO说“成功穿越小行星带的几率大约是3,720比1！”C3PO试图告诉汉和莉娅的是，成功穿越小行星带的概率大约是不成功穿越它的概率的3,720倍小。仅仅陈述几率通常是一种更方便表示可能性的方式，因为我们知道，对于每成功穿越一个小行星带，就有3,720个没有成功穿越！此外，虽然概率介于0和1之间，但几率可以取任何正值。
- en: '|  |'
  id: totrans-430
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Despite being a highly intelligent protocol droid, C3PO got his odds the wrong
    way around (as many people do). He *should* have said the odds of successfully
    navigating an asteroid field are approximately 1 to 3,720!
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管C3PO是一个高度智能的礼仪机器人，但他把几率搞错了（正如许多人所做的那样）。他*应该*说的是成功穿越小行星带的几率大约是1比3,720！
- en: '|  |'
  id: totrans-433
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '[Figure 4.4](#ch04fig04) shows copper content plotted against the odds of a
    painting being an original. Notice that the odds are not bounded between 0 and
    1, and that they take on positive values.'
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: '[图4.4](#ch04fig04)显示了铜含量与画作是否为原作的几率之间的关系。请注意，几率不在0和1之间有界，并且它们取正值。'
- en: 'As we can see, though, the relationship between the copper content of the paint
    and the odds of a painting being an original is not linear. Instead, if we take
    the natural logarithm (log with a base of *e*, abbreviated as *ln*) of the odds,
    we get the *log odds*:'
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，正如我们所看到的，油漆中的铜含量与画作是否为原作的几率之间的关系不是线性的。相反，如果我们对几率取自然对数（以e为底的对数，简写为*ln*），我们得到的是*对数几率*：
- en: equation 4.3\.
  id: totrans-436
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式4.3。
- en: '![](eq4-3.jpg)'
  id: totrans-437
  prefs: []
  type: TYPE_IMG
  zh: '![方程式4-3](eq4-3.jpg)'
- en: '|  |'
  id: totrans-438
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-439
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: '[Equation 4.3](#ch04equ03), which converts probabilities into log odds, is
    also called the *logit* function. You will often see *logit regression* and *logistic
    regression* used interchangeably.'
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: '[方程式4.3](#ch04equ03)，它将概率转换为对数几率，也被称为*logit*函数。你经常会看到*logit回归*和*逻辑回归*被互换使用。'
- en: '|  |'
  id: totrans-441
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 4.4\. Plotting the odds of being an original against copper content.
    The probabilities derived from the logistic function were converted into odds
    and plotted against copper content. Odds can take any positive value. The straight
    line represents a poor attempt to model a linear relationship between copper content
    and odds.
  id: totrans-442
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.4。将原作几率与铜含量绘制在一起。从逻辑函数中导出的概率被转换为几率，并绘制在铜含量上。几率可以取任何正值。直线代表尝试将铜含量与几率之间的线性关系模型化的一次糟糕尝试。
- en: '![](fig4-4.jpg)'
  id: totrans-443
  prefs: []
  type: TYPE_IMG
  zh: '![图4-4](fig4-4.jpg)'
- en: 'I’ve taken the natural logarithm of the odds shown in [figure 4.3](#ch04fig03)
    to generate their log odds, and plotted these log odds against copper content
    in [figure 4.5](#ch04fig05). Hurray! We have a linear relationship between our
    predictor variable and the log odds of a painting being an original. Also notice
    that log odds are completely unbounded: they can extend to positive and negative
    infinity. When interpreting log odds'
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 我已经对[图4.3](#ch04fig03)中显示的几率取了自然对数，以生成它们的对数几率，并将这些对数几率与铜含量绘制在[图4.5](#ch04fig05)中。太棒了！我们预测变量与画作是否为原作的对数几率之间存在线性关系。此外，请注意，对数几率是完全无界的：它们可以延伸到正负无穷大。在解释对数几率时
- en: A positive value means something is more likely to occur than to not occur.
  id: totrans-445
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正值意味着某事发生的可能性大于不发生的可能性。
- en: A negative value means something is less likely to occur than to occur.
  id: totrans-446
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 负值意味着某事发生的可能性小于不发生的可能性。
- en: Log odds of 0 means something is as likely to occur as not to occur.
  id: totrans-447
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对数几率为0意味着某事发生的可能性与不发生的可能性相同。
- en: Figure 4.5\. Plotting the log odds of being an original against copper content.
    The odds were converted into log odds using the logit function and plotted against
    copper content. Log odds are unbounded and can take any value. The straight line
    represents the linear relationship between copper content and log odds.
  id: totrans-448
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.5。绘制成为原作的对数几率与铜含量的关系图。几率通过logit函数转换为对数几率，并绘制在铜含量上。对数几率是无界的，可以取任何值。直线代表铜含量与对数几率之间的线性关系。
- en: '![](fig4-5.jpg)'
  id: totrans-449
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-5.jpg)'
- en: When discussing [figure 4.4](#ch04fig04), I highlighted that the relationship
    between copper content and the odds of being an original painting was not linear.
    Next, I showed you in [figure 4.5](#ch04fig05) that the relationship between copper
    content and log odds *was* linear. In fact, linearizing this relationship is why
    we take the natural logarithm of the odds. Why did I make such a big deal about
    there being a linear relationship between our predictor variable and its log odds?
    Well, modeling a straight line is easy. Recall from [chapter 1](kindle_split_010.html#ch01)
    that all an algorithm needs to learn to model a straight-line relationship is
    the y-intercept and the slope of the line. So logistic regression learns the log
    odds of a painting being an original when copper content is 0 (the y-intercept),
    and how the log odds change with increasing copper content (the slope).
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 当讨论[图4.4](#ch04fig04)时，我强调了铜含量与成为原作画作几率之间的关系不是线性的。接着，我在[图4.5](#ch04fig05)中展示了铜含量与对数几率之间的关系是线性的。实际上，线性化这种关系是我们取几率自然对数的原因。为什么我对预测变量与其对数几率之间存在线性关系如此重视呢？因为建模直线是容易的。回想一下[第1章](kindle_split_010.html#ch01)，算法要学习建模直线关系只需要y截距和直线的斜率。因此，逻辑回归学习当铜含量为0（y截距）时画作成为原作的对数几率，以及随着铜含量增加对数几率如何变化（斜率）。
- en: '|  |'
  id: totrans-451
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-452
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The more influence a predictor variable has on the log odds, the steeper the
    slope will be, while variables that have no predictive value will have a slope
    that is nearly horizontal.
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 预测变量对对数几率的影响越大，斜率就越陡峭，而没有任何预测价值的变量将具有几乎水平的斜率。
- en: '|  |'
  id: totrans-454
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Additionally, having a linear relationship means that when we have multiple
    predictor variables, we can add their contributions to the log odds together to
    get the overall log odds of a painting being an original, based on the information
    from all of its predictors.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，具有线性关系意味着当我们有多个预测变量时，我们可以将它们的贡献相加到对数几率中，以获得基于所有预测变量的整体对数几率，即画作成为原作的对数几率。
- en: Now, how do we get from the straight-line relationship between copper content
    and the log odds of being an original, to making predictions about new paintings?
    The model calculates the log odds of our new data being an original painting using
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们如何从铜含量与成为原作的对数几率之间的直线关系，过渡到对新画作进行预测呢？模型通过以下方式计算新数据成为原作画作的对数几率：
- en: '*log odds* = *y-intercept* + *slope* * *copper*'
  id: totrans-457
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数几率* = *y截距* + *斜率* * *铜*'
- en: 'where we add the y-intercept and the product of the slope and the value of
    copper in our new painting. Once we’ve calculated the log odds of the new painting,
    we convert it into the probability of being an original using the logistic function:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的新画作中，我们添加了y截距以及斜率和铜值的乘积。一旦我们计算出新画作的对数几率，我们就使用逻辑函数将其转换为成为原作的几率：
- en: equation 4.4\.
  id: totrans-459
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 公式4.4。
- en: '![](eq4-4.jpg)'
  id: totrans-460
  prefs: []
  type: TYPE_IMG
  zh: '![](eq4-4.jpg)'
- en: where *p* is the probability, *e* is Euler’s number (a fixed constant ~ 2.718),
    and *z* is the log odds of a particular case.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p* 是概率，*e* 是欧拉数（一个固定常数 ~ 2.718），*z* 是特定情况的对数几率。
- en: Then, quite simply, if the probability of a painting being an original is >
    0.5, it is classified as an original. If the probability is < 0.5, it is classified
    as a forgery. This conversion of log odds to odds to probabilities is illustrated
    in [figure 4.6](#ch04fig06).
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，简单来说，如果画作成为原作的几率大于0.5，它就被分类为原作。如果几率小于0.5，它就被分类为赝品。这种将对数几率转换为几率再转换为概率的过程在[图4.6](#ch04fig06)中得到了说明。
- en: '|  |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-464
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: This threshold probability is 0.5 by default. In other words, if there is more
    than a 50% chance that a case belongs to the positive class, assign it to the
    positive class. We can alter this threshold, however, in situations where we need
    to be *really* sure before classifying a case as belonging to the positive class.
    For example, if we’re using the model to predict whether a patient needs high-risk
    surgery, we want to be really sure before going ahead with the procedure!
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，这个阈值概率是0.5。换句话说，如果一个案例属于阳性类的几率超过50%，则将其分配到阳性类。然而，在需要在我们将案例分类为阳性类之前“非常”确定的情况下，我们可以改变这个阈值。例如，如果我们使用该模型来预测患者是否需要高风险手术，我们在进行手术之前一定要非常确定！
- en: '|  |'
  id: totrans-466
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 4.6\. Summary of how logistic regression models predict class membership.
    Data is converted into log odds (logits), which are converted into odds and then
    into the probability of belonging to the “positive” class. Cases are assigned
    to the positive class if their probability exceeds a threshold probability (0.5
    by default).
  id: totrans-467
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.6. 总结了逻辑回归模型预测类别成员的方法。数据被转换为对数几率（logits），然后转换为几率，最后转换为属于“阳性”类的概率。如果案例的概率超过一个阈值概率（默认为0.5），则将其分配到阳性类。
- en: '![](fig4-6_alt.jpg)'
  id: totrans-468
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-6_alt.jpg)'
- en: You will often see the model
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会看到模型
- en: '*log odds* = *y-intercept* + *slope* * *copper*'
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数几率* = *y截距* + *斜率* * *铜*'
- en: rewritten as in [equation 4.5](#ch04equ05).
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 重写为[方程4.5](#ch04equ05)。
- en: equation 4.5\.
  id: totrans-472
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4.5。
- en: '![](eq4-5.jpg)'
  id: totrans-473
  prefs: []
  type: TYPE_IMG
  zh: '![](eq4-5.jpg)'
- en: Don’t be scared by this! Look at [equation 4.5](#ch04equ05) again. This is the
    way statisticians represent models that predict straight lines, and it is exactly
    the same as the equation describing log odds. The logistic regression model predicts
    the log odds (on the left of the equals) by adding the y-intercept (*β*[0]) and
    the slope of the line (*β*[copper]) multiplied by the value of copper (*x*[copper]).
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被吓到！再看一遍[方程4.5](#ch04equ05)。这是统计学家表示预测直线模型的方式，它与描述对数几率的方程完全相同。逻辑回归模型通过添加y截距（*β*[0]）和线的斜率（*β*[铜]）乘以铜的值（*x*[铜]）来预测对数几率（等号左边）。
- en: 'You may be wondering: why are you showing me equations when you promised me
    you wouldn’t? Well, in most situations, we won’t have a single predictor; we’ll
    have many. By representing the model in this way, you can see how it can be used
    to combine multiple predictors together *linearly*: in other words, by adding
    their effects together.'
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道：你为什么在我答应你不会的时候还给我展示方程？好吧，在大多数情况下，我们不会只有一个预测因子；我们会有很多。通过以这种方式表示模型，你可以看到它如何可以用来线性地组合多个预测因子：换句话说，通过将它们的效果相加。
- en: 'Let’s say we also include the amount of the metal lead as a predictor for whether
    a painting is an original or not. The model will instead look like this:'
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们还将金属铅的含量作为预测绘画是否为原作的一个指标。模型将看起来像这样：
- en: equation 4.6\.
  id: totrans-477
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程4.6。
- en: '![](eq4-6.jpg)'
  id: totrans-478
  prefs: []
  type: TYPE_IMG
  zh: '![](eq4-6.jpg)'
- en: An example of what this model might look like is shown in [figure 4.7](#ch04fig07).
    With two predictor variables, we can represent the model as a plane, with the
    log odds shown on the vertical axis. The same principle applies for more than
    two predictors, but it’s difficult to visualize on a 2D surface.
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型可能的样子示例显示在[图4.7](#ch04fig07)中。有两个预测变量时，我们可以将模型表示为一个平面，对数几率显示在垂直轴上。对于超过两个预测因子的情况，同样适用这个原理，但在二维表面上难以可视化。
- en: 'Now, for any painting we pass into our model, the model does the following:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，对于任何我们传递给模型的绘画，模型会执行以下操作：
- en: Multiplies its copper content by the slope for copper
  id: totrans-481
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将其铜含量乘以铜的斜率
- en: Multiplies the lead content by the slope for lead
  id: totrans-482
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将铅的含量乘以铅的斜率
- en: Adds these two values and the y-intercept together to get the log odds of that
    painting being an original
  id: totrans-483
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将这两个值和y截距相加，得到该绘画为原作的对数几率
- en: Converts the log odds into a probability
  id: totrans-484
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将对数几率转换为概率
- en: Classifies the painting as an original if the probability is > 0.5, or classifies
    the painting as a forgery if the probability is < 0.5
  id: totrans-485
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果概率大于0.5，则将绘画分类为原作，如果概率小于0.5，则将绘画分类为赝品
- en: Figure 4.7\. Visualizing a logistic regression model with two predictors. Copper
    content and lead content are plotted on the x- and z-axes, respectively. Log odds
    are plotted on the y-axis. The plane shown inside the plot represents the linear
    model that combines the intercept and the slopes of copper content and lead content
    to predict log odds.
  id: totrans-486
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.7\. 可视化具有两个预测变量的逻辑回归模型。铜含量和铅含量分别绘制在 x 轴和 z 轴上。对数几率绘制在 y 轴上。图中显示的平面代表结合截距和铜含量、铅含量的斜率来预测对数几率的线性模型。
- en: '![](fig4-7.jpg)'
  id: totrans-487
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-7](fig4-7.jpg)'
- en: 'We can extend the model to include as many predictor variables as we want:'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将模型扩展到包括我们想要的任何数量的预测变量：
- en: equation 4.7\.
  id: totrans-489
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程式 4.7\.
- en: '![](eq4-7.jpg)'
  id: totrans-490
  prefs: []
  type: TYPE_IMG
  zh: '![方程式 4-7](eq4-7.jpg)'
- en: where *k* is the number of predictor variables in the dataset and the ... represents
    all the variables in between.
  id: totrans-491
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *k* 是数据集中预测变量的数量，... 表示所有中间变量。
- en: '|  |'
  id: totrans-492
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-493
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Remember in [chapter 3](kindle_split_013.html#ch03), when I explained the difference
    between parameters and hyperparameters? Well, *β*[0], *β*[1], and so on are model
    parameters, because they are learned by the algorithm from the data.
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在[第 3 章](kindle_split_013.html#ch03)，当我解释参数和超参数之间的区别时？嗯，*β*[0]，*β*[1]等等是模型参数，因为它们是由算法从数据中学习的。
- en: '|  |'
  id: totrans-495
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The whole procedure for classifying new paintings is summarized in [figure 4.8](#ch04fig08).
    First, we convert the copper and lead values of our new data into their log odds
    (logits) by using the linear model learned by the algorithm. Next, we convert
    the log odds into their probabilities using the logistic function. Finally, if
    the probability is > 0.5, we classify the painting as an original; and if its
    probability is < 0.5, we classify it as a forgery.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 对新画作进行分类的整个流程总结在[图 4.8](#ch04fig08)中。首先，我们使用算法学习到的线性模型将新数据的铜和铅值转换为它们的对数几率（logits）。接下来，我们使用逻辑函数将对数几率转换为概率。最后，如果概率大于
    0.5，我们将画作分类为原作；如果其概率小于 0.5，我们将其分类为赝品。
- en: Figure 4.8\. The process of classifying new paintings. The predictor variable
    values of three paintings are converted into log odds based on the learned model
    parameters (intercept and slopes). The log odds are converted into probabilities
    (*p*), and if *p* > 0.5, the case is classified as the “positive” class.
  id: totrans-497
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 4.8\. 对新画作进行分类的过程。三幅画作的预测变量值根据学习到的模型参数（截距和斜率）转换为对数几率。对数几率转换为概率（*p*），如果 *p*
    > 0.5，则将案例分类为“阳性”类别。
- en: '![](fig4-8_alt.jpg)'
  id: totrans-498
  prefs: []
  type: TYPE_IMG
  zh: '![图 4-8 替代](fig4-8_alt.jpg)'
- en: '|  |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-500
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Although the first and third paintings in [figure 4.8](#ch04fig08) were both
    classified as forgeries, they had very different probabilities. As the probability
    of the third painting is much smaller than the probability of the first, we can
    be more confident that painting 3 is a forgery than we are confident that painting
    1 is a forgery.
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图 4.8 中的第一幅和第三幅画作都被分类为赝品，但它们的概率却非常不同。由于第三幅画作的几率远小于第一幅，我们可以更有信心地认为画作 3 是赝品，而不是我们有信心画作
    1 是赝品。
- en: '|  |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 4.1.2\. What if we have more than two classes?
  id: totrans-503
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.1.2\. 如果我们有超过两个类别怎么办？
- en: The previous scenario is an example of binomial logistic regression. In other
    words, the decision about which class to assign to new data can take on only one
    of two named categories (*bi* and *nomos* from Latin and Greek, respectively).
    But we can use a variant of logistic regression to predict one of multiple classes.
    This is called *multinomial logistic regression*, because there are now multiple
    possible categories to choose from.
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的场景是二项逻辑回归的一个例子。换句话说，关于将哪个类别分配给新数据的决定只能采取两种命名的类别之一（分别来自拉丁语和希腊语的*bi*和*nomos*）。但我们可以使用逻辑回归的变体来预测多个类别中的一个。这被称为*多项式逻辑回归*，因为现在有多个可能的类别可供选择。
- en: In multinomial logistic regression, instead of estimating a single logit for
    each case, the model estimates a logit for each case *for each of the output classes*.
    These logits are then passed into an equation called the *softmax function* which
    turns these logits into probabilities for each class, that sum to 1 (see [figure
    4.9](#ch04fig09)). Then whichever class has the largest probability is selected
    as the output class.
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 在多项式逻辑回归中，模型不是为每个案例估计一个logit，而是为每个案例的每个输出类别估计一个logit。然后，将这些logit传递到一个称为*softmax函数*的方程中，该函数将这些logit转换为每个类别的概率，这些概率之和为1（见[图
    4.9](#ch04fig09)）。然后，选择具有最大概率的类别作为输出类别。
- en: Figure 4.9\. Summary of the softmax function. In the binomial case, only one
    logit is needed per case (the logit for the positive class). Where there are multiple
    classes (a, b, and c in this example), the model estimates one logit per class
    for each case. The softmax function maps these logits to probabilities that sum
    to one. The case is assigned to the class with the largest probability.
  id: totrans-506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.9\. softmax函数的总结。在二项式情况下，每个案例只需要一个logit（正类的logit）。在存在多个类别的情况下（本例中的 a，b 和
    c），模型为每个案例估计每个类别的logit。softmax函数将这些logits映射到总和为1的概率。案例被分配给概率最大的类别。
- en: '![](fig4-9_alt.jpg)'
  id: totrans-507
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-9_alt.jpg)'
- en: '|  |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-509
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: You will sometimes see *softmax regression* and *multinomial logistic regression*
    used interchangeably.
  id: totrans-510
  prefs: []
  type: TYPE_NORMAL
  zh: 你有时会看到 *softmax回归* 和 *多项式逻辑回归* 被互换使用。
- en: '|  |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The `classif.logreg` learner wrapped by mlr will only handle binomial logistic
    regression. There isn’t currently an implementation of ordinary multinomial logistic
    regression wrapped by mlr. We can, however, use the `classif.LiblineaRL1LogReg`
    learner to perform multinomial logistic regression (although it has some differences
    I won’t discuss).
  id: totrans-512
  prefs: []
  type: TYPE_NORMAL
  zh: 由mlr包装的`classif.logreg`学习器只会处理二项逻辑回归。目前还没有mlr包装的普通多项式逻辑回归实现。然而，我们可以使用`classif.LiblineaRL1LogReg`学习器来执行多项式逻辑回归（尽管它有一些差异，我不会讨论）。
- en: '|  |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**The softmax function**'
  id: totrans-514
  prefs: []
  type: TYPE_NORMAL
  zh: '**softmax函数**'
- en: It isn’t necessary for you to memorize the softmax function, so feel free to
    skip this, but the softmax function is defined as
  id: totrans-515
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要记住softmax函数，所以可以自由跳过这部分，但softmax函数的定义如下
- en: '![](pg98-1.jpg)'
  id: totrans-516
  prefs: []
  type: TYPE_IMG
  zh: '![](pg98-1.jpg)'
- en: where *p*[a] is the probability of a case belonging to class a, *e* is Euler’s
    number (a fixed constant ~ 2.718), and logit[a], logit[b], and logit[c] are the
    logits for this case for being in classes a, b, and c, respectively.
  id: totrans-517
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[a] 是一个案例属于类别 a 的概率，*e* 是欧拉数（一个固定常数 ~ 2.718），而 logit[a]，logit[b] 和 logit[c]
    分别是这个案例属于类别 a，b 和 c 的 logits。
- en: If you’re a math buff, this can be generalized to any number of classes using
    the equation
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你是一个数学爱好者，这可以推广到任意数量的类别，使用以下方程
- en: '![](pg98-2.jpg)'
  id: totrans-519
  prefs: []
  type: TYPE_IMG
  zh: '![](pg98-2.jpg)'
- en: where *p*[j] is the probability of being in class j, and ![](pg98-3.jpg) means
    to sum the *e*^(logits) from class 1 to class K (where there are K classes in
    total).
  id: totrans-520
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*[j] 是属于类别 j 的概率，而 ![](pg98-3.jpg) 表示从类别 1 到类别 K（总共有 K 个类别）的 *e*^(logits)
    的和。
- en: Write your own implementation of the softmax function in R, and try plugging
    other vectors of numbers into it. You’ll find that it always maps the input to
    an output where all the elements sum to 1.
  id: totrans-521
  prefs: []
  type: TYPE_NORMAL
  zh: 在R中编写自己的softmax函数实现，并尝试将其他数字向量插入其中。你会发现它总是将输入映射到所有元素之和为1的输出。
- en: '|  |'
  id: totrans-522
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Now that you know how logistic regression works, you’re going to build your
    first binomial logistic regression model.
  id: totrans-523
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了逻辑回归是如何工作的，你将构建你的第一个二项逻辑回归模型。
- en: 4.2\. Building your first logistic regression model
  id: totrans-524
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.2\. 构建你的第一个逻辑回归模型
- en: Imagine that you’re a historian interested in the RMS *Titanic*, which famously
    sank in 1912 after colliding with an iceberg. You want to know whether socioeconomic
    factors influenced a person’s probability of surviving the disaster. Luckily,
    such socioeconomic data is publicly available!
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你是一位对1912年著名沉没的RMS *泰坦尼克号* 感兴趣的历史学家。你想要知道社会经济因素是否影响了一个人在灾难中幸存的概率。幸运的是，这样的社会经济数据是公开可用的！
- en: 'Your aim is to build a binomial logistic regression model to predict whether
    a passenger would survive the *Titanic* disaster, based on data such as their
    gender and how much they paid for their ticket. You’re also going to interpret
    the model to decide which variables were important in influencing the probability
    of a passenger surviving. Let’s start by loading the mlr and tidyverse packages:'
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 你的目标是构建一个二项逻辑回归模型来预测乘客是否会在 *泰坦尼克号* 灾难中幸存，基于他们的性别和票价等数据。你还将解释模型以决定哪些变量在影响乘客幸存概率方面很重要。让我们首先加载mlr和tidyverse包：
- en: '[PRE42]'
  id: totrans-527
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: 4.2.1\. Loading and exploring the Titanic dataset
  id: totrans-528
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.1\. 加载和探索泰坦尼克号数据集
- en: Now let’s load the data, which is built into the `titanic` package, convert
    it into a tibble (with `as_tibble()`), and explore it a little. We have a tibble
    containing 891 cases and 12 variables of passengers of the *Titanic*. Our goal
    is to train a model that can use the information in these variables to predict
    whether a passenger would survive the disaster.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们加载数据，该数据包含在`titanic`包中，将其转换为tibble（使用`as_tibble()`），并对其进行初步探索。我们有一个包含891个案例和12个乘客变量的tibble，这些乘客来自*泰坦尼克号*。我们的目标是训练一个模型，可以使用这些变量中的信息来预测乘客是否能在灾难中幸存。
- en: Listing 4.1\. Loading and exploring the *Titanic* dataset
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.1\. 加载和探索*泰坦尼克号*数据集
- en: '[PRE43]'
  id: totrans-531
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: 'The tibble contains the following variables:'
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: tibble包含以下变量：
- en: '`PassengerId`—An arbitrary number unique to each passenger'
  id: totrans-533
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`PassengerId`—每个乘客独特的任意数字'
- en: '`Survived`—An integer denoting survival (1 = survived, 0 = died)'
  id: totrans-534
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Survived`—表示生存的整数（1 = 幸存，0 = 死亡）'
- en: '`Pclass`—Whether the passenger was housed in first, second, or third class'
  id: totrans-535
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Pclass`—乘客是否被安排在头等舱、二等舱或三等舱'
- en: '`Name`—A character vector of the passengers’ names'
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Name`—乘客名字的字符向量'
- en: '`Sex`—A character vector containing “male” and “female”'
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Sex`—包含“男性”和“女性”的字符向量'
- en: '`Age`—The age of the passenger'
  id: totrans-538
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Age`—乘客的年龄'
- en: '`SibSp`—The combined number of siblings and spouses on board'
  id: totrans-539
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`SibSp`—船上兄弟姐妹和配偶的总数'
- en: '`Parch`—The combined number of parents and children on board'
  id: totrans-540
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Parch`—船上父母和孩子的总数'
- en: '`Ticket`—A character vector with each passenger’s ticket number'
  id: totrans-541
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Ticket`—包含每个乘客票号的字符向量'
- en: '`Fare`—The amount of money each passenger paid for their ticket'
  id: totrans-542
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Fare`—每位乘客为他们的票支付的金额'
- en: '`Cabin`—A character vector of each passenger’s cabin number'
  id: totrans-543
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Cabin`—每个乘客的舱号的字符向量'
- en: '`Embarked`—A character vector of which port passengers embarked from'
  id: totrans-544
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Embarked`—乘客登船的港口的字符向量'
- en: The first thing we’re going to do is use tidyverse tools to clean and prepare
    the data for modeling.
  id: totrans-545
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将要做的第一件事是使用tidyverse工具来清理和准备数据以供建模。
- en: '4.2.2\. Making the most of the data: Feature engineering and feature selection'
  id: totrans-546
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.2\. 充分利用数据：特征工程和特征选择
- en: 'Rarely will you be working with a dataset that is ready for modeling straight
    away. Typically, we need to perform some cleaning first to ensure that we get
    the most from the data. This includes steps such as converting data to the correct
    types, correcting mistakes, and removing irrelevant data. The `titanicTib` tibble
    is no exception; we need to clean it up before we can pass it to the logistic
    regression algorithm. We’ll perform three tasks:'
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 很少会有一个数据集可以直接用于建模。通常，我们需要先进行一些清理以确保我们能从数据中获得最大价值。这包括将数据转换为正确的类型、纠正错误和删除无关数据等步骤。`titanicTib`
    tibble也不例外；在我们将其传递给逻辑回归算法之前，我们需要对其进行清理。我们将执行三个任务：
- en: Convert the `Survived`, `Sex`, and `Pclass` variables into factors.
  id: totrans-548
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`Survived`、`Sex`和`Pclass`变量转换为因子。
- en: Create a new variable called `FamSize` by adding `SibSp` and `Parch` together.
  id: totrans-549
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过将`SibSp`和`Parch`相加创建一个新的变量`FamSize`。
- en: Select the variables we believe to be of predictive value for our model.
  id: totrans-550
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择我们认为对我们模型有预测价值的变量。
- en: If a variable should be a factor, it’s important to let R know it’s a factor,
    so that R treats it appropriately. We can see from the output of `titanicTib`
    in [listing 4.1](#ch04ex01) that `Survived` and `Pclass` are both integer vectors
    (`<int>` is shown above their columns in the output) and that `Sex` is a character
    vector (`<chr>` above the column). Each of these variables should be treated as
    a factor because it represents discrete differences between cases that are repeated
    throughout the dataset.
  id: totrans-551
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个变量应该是因子，那么让R知道它是一个因子是很重要的，这样R就会适当地处理它。我们可以从[列表4.1](#ch04ex01)中`titanicTib`的输出中看到，`Survived`和`Pclass`都是整数向量（输出中列上方显示`<int>`），而`Sex`是一个字符向量（列上方显示`<chr>`）。每个这些变量都应该被视为因子，因为它代表了在整个数据集中重复出现的案例之间的离散差异。
- en: We might hypothesize that the number of family members a passenger has on board
    might impact their survival. For example, people with many family members may
    be reluctant to board a lifeboat that doesn’t have enough room for their whole
    family. While the `SibSp` and `Parch` variables contain this information separated
    by siblings and spouses, and parents and children, respectively, it may be more
    informative to combine these into a single variable containing overall family
    size.
  id: totrans-552
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以假设乘客在船上的家庭成员数量可能会影响他们的生存。例如，有众多家庭成员的人可能不愿意登上没有足够空间容纳整个家庭的救生艇。虽然`SibSp`和`Parch`变量分别按兄弟姐妹和配偶、父母和子女来存储这些信息，但将它们合并成一个包含整体家庭规模的单一变量可能更有信息量。
- en: 'This is an extremely important machine learning task called *feature engineering*:
    the modification of variables in your dataset to improve their predictive value.
    Feature engineering comes in two flavors:'
  id: totrans-553
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个极其重要的机器学习任务，称为*特征工程*：修改你的数据集中的变量以提高它们的预测价值。特征工程有两种形式：
- en: '***Feature extraction—*** Predictive information is held in a variable, but
    in a format that is not useful. For example, let’s say you have a variable that
    contains the year, month, day, and time of day of certain events occurring. The
    time of day has important predictive value, but the year, month, and day do not.
    For this variable to be useful in your model, you would need to extract only the
    time-of-day information as a new variable.'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***特征提取—*** 预测信息存储在一个变量中，但以一个无用的格式。例如，假设你有一个变量，其中包含某些事件发生的年份、月份、日期和一天中的时间。一天中的时间具有重要的预测价值，但年份、月份和日期没有。为了使这个变量在你的模型中变得有用，你需要提取仅包含一天中时间信息的新变量。'
- en: '***Feature creation—*** Existing variables are combined to create new ones.
    Merging the `SibSp` and `Parch` variables to create `FamSize` is an example.'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***特征创建—*** 将现有变量合并以创建新的变量。将`SibSp`和`Parch`变量合并以创建`FamSize`就是一个例子。'
- en: Using feature extraction and feature creation allows us to extract predictive
    information present in our dataset but not currently in a format that maximizes
    its usefulness.
  id: totrans-556
  prefs: []
  type: TYPE_NORMAL
  zh: 使用特征提取和特征创建使我们能够提取数据集中存在的预测信息，但当前格式并未最大化其有用性。
- en: Finally, we will often have variables in our data that have no predictive value.
    For example, does knowing the passenger’s name or cabin number help us predict
    survival? Possibly not, so let’s remove them. Including variables with little
    or no predictive value adds noise to the data and will negatively impact how our
    models perform, so it’s best to remove them.
  id: totrans-557
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们数据中的变量可能没有任何预测价值。例如，知道乘客的名字或船舱号码能帮助我们预测生存吗？可能不能，所以让我们移除它们。包括预测价值很小或没有的变量会给数据增加噪声，并会负面影响我们的模型表现，所以最好移除它们。
- en: 'This is another extremely important machine learning task called *feature selection*,
    and it is pretty much what it sounds like: keeping variables that add predictive
    value, and removing those that don’t. Sometimes it’s obvious to us as humans whether
    variables are useful predictors or not. Passenger name, for example, would not
    be useful because every passenger has a different name! In these situations, it’s
    common sense to remove such variables. Often, however, it’s not so obvious, and
    there are more sophisticated ways we can automate the feature-selection process.
    We’ll explore this in later chapters.'
  id: totrans-558
  prefs: []
  type: TYPE_NORMAL
  zh: 这又是一个极其重要的机器学习任务，称为*特征选择*，基本上就是它听起来那样：保留那些增加预测价值的变量，移除那些没有的。有时对我们人类来说，是否变量是有用的预测因子是显而易见的。例如，乘客姓名可能没有用，因为每个乘客都有一个不同的名字！在这些情况下，移除这样的变量是常识。然而，通常情况下，这并不那么明显，我们可以有更复杂的方法来自动化特征选择过程。我们将在后面的章节中探讨这一点。
- en: All three of these tasks (converting to factors, feature engineering, and feature
    selection) are performed in [listing 4.2](#ch04ex02). I’ve made our lives easier
    by defining a vector of the variables we wish to convert into factors, and then
    using the `mutate_at()` function to turn them all into factors. The `mutate_at()`
    function is like the `mutate()` function, but it allows us to mutate multiple
    columns at once. We supply the existing variables as a character vector to the
    `.vars` argument and tell it what we want to do to those variables using the `.funs`
    argument. In this case, we supply the vector of variables we defined, and the
    “factor” function to convert them into factors. We pipe the result of this into
    a `mutate()` function call that defines a new variable, `FamSize`, which is the
    sum of `SibSp` and `Parch`. Finally, we pipe the result of this into a `select()`
    function call, to select only the variables we believe may have some predictive
    value for our model.
  id: totrans-559
  prefs: []
  type: TYPE_NORMAL
  zh: 这三个任务（转换为因子、特征工程和特征选择）都在 [列表 4.2](#ch04ex02) 中执行。我通过定义一个我们希望转换为因子的变量向量来简化了我们的工作，然后使用
    `mutate_at()` 函数将它们全部转换为因子。`mutate_at()` 函数类似于 `mutate()` 函数，但它允许我们一次修改多个列。我们将现有变量作为字符向量提供给
    `.vars` 参数，并使用 `.funs` 参数告诉它对这些变量要做什么。在这种情况下，我们提供了我们定义的变量向量，以及将它们转换为因子的“factor”函数。我们将这个结果通过管道传递给一个
    `mutate()` 函数调用，该调用定义了一个新变量 `FamSize`，它是 `SibSp` 和 `Parch` 的总和。最后，我们将这个结果通过管道传递给一个
    `select()` 函数调用，以选择我们相信可能对我们的模型有某些预测价值的变量。
- en: Listing 4.2\. Cleaning the *Titanic* data, ready for modeling
  id: totrans-560
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.2\. 清理 *泰坦尼克号* 数据，准备建模
- en: '[PRE44]'
  id: totrans-561
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: When we print our new tibble, we can see that `Survived`, `Pclass`, and `Sex`
    are now factors (`<fct>` shown above their columns in the output); we have our
    new variable, `FamSize`; and we have removed irrelevant variables.
  id: totrans-562
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们打印我们的新 tibble 时，我们可以看到 `Survived`、`Pclass` 和 `Sex` 现在是因子（在输出中显示为 `<fct>`
    在其列上方）；我们有我们的新变量 `FamSize`；并且我们移除了无关变量。
- en: '|  |'
  id: totrans-563
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Have I been too hasty in removing the `Name` variable from the tibble? Hidden
    in this variable are the salutations for each passenger (Miss, Mrs., Mr., Master,
    and so on), which may have predictive value. Using this information would require
    feature extraction.
  id: totrans-565
  prefs: []
  type: TYPE_NORMAL
  zh: 我在从 tibble 中移除 `Name` 变量时是否过于草率？这个变量中隐藏着每位乘客的称呼（小姐、夫人、先生、少爷等），这些可能具有预测价值。使用这些信息需要进行特征提取。
- en: '|  |'
  id: totrans-566
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.2.3\. Plotting the data
  id: totrans-567
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.3\. 绘制数据
- en: Now that we’ve cleaned our data a little, let’s plot it to get better insight
    into the relationships in the data. Here’s a little trick to simplify plotting
    multiple variables together using ggplot2\. Let’s convert the data into an untidy
    format, such that each of the predictor variable names is held in one column,
    and its values are held in another column, using the `gather()` function (refresh
    your memory of this by looking at the end of [chapter 2](kindle_split_011.html#ch02)).
  id: totrans-568
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经稍微清理了数据，让我们绘制它以更好地了解数据中的关系。这里有一个使用 ggplot2 简化绘制多个变量的小技巧。让我们将数据转换为不整洁的格式，使得每个预测变量名称在一个列中，其值在另一个列中，使用
    `gather()` 函数（通过查看第 2 章末尾来刷新你的记忆 [chapter 2](kindle_split_011.html#ch02))。
- en: '|  |'
  id: totrans-569
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-570
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The `gather()` function will warn that “attributes are not identical across
    measure variables; they will be dropped.” This is simply warning you that the
    variables you are gathering together don’t have the same factor levels. Ordinarily
    this might mean you’ve collapsed variables you didn’t mean to, but in this case
    we can safely ignore the warning.
  id: totrans-571
  prefs: []
  type: TYPE_NORMAL
  zh: '`gather()` 函数将警告“度量变量之间的属性不相同；它们将被丢弃。”这只是一个警告，告诉你你正在收集在一起的变量没有相同的因子水平。通常这可能会意味着你意外地合并了你不打算合并的变量，但在这个情况下我们可以安全地忽略这个警告。'
- en: '|  |'
  id: totrans-572
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 4.3\. Creating an untidy tibble for plotting
  id: totrans-573
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.3\. 创建用于绘制的杂乱 tibble
- en: '[PRE45]'
  id: totrans-574
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'We now have an untidy tibble with three columns: one containing the `Survived`
    factor, one containing the names of the predictor variables, and one containing
    their values.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在有一个包含三个列的不整洁 tibble：一列包含 `Survived` 因子，一列包含预测变量的名称，另一列包含它们的值。
- en: '|  |'
  id: totrans-576
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-577
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Note that the value column is a character vector (`<chr>`). This is because
    it contains “male” and “female” from the `Sex` variable. As a column can only
    hold data of a single type, all the numerical data is also converted into characters.
  id: totrans-578
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，值列是一个字符向量 (`<chr>`）。这是因为它包含了来自 `Sex` 变量的“男性”和“女性”。由于一列只能持有单一类型的数据，所有数值数据也被转换为字符。
- en: '|  |'
  id: totrans-579
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: You may be wondering why we’re doing this. Well, it allows us to use ggplot2’s
    *faceting* system to plot our different variables together. In [listing 4.4](#ch04ex04),
    I take the `titanicUntidy` tibble, filter for the rows that *do not* contain the
    `Pclass` or `Sex` variables (as these are factors, we’ll plot them separately),
    and pipe this data into a `ggplot()` call.
  id: totrans-580
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能想知道我们为什么要这样做。嗯，这使我们能够使用ggplot2的*分面*系统来一起绘制我们的不同变量。在[列表4.4](#ch04ex04)中，我选取了`titanicUntidy`
    tibble，筛选出不包含`Pclass`或`Sex`变量（因为这些是因素，我们将单独绘制它们）的行，并将这些数据通过管道传递到`ggplot()`调用中。
- en: Listing 4.4\. Creating subplots for each continuous variable
  id: totrans-581
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.4\. 为每个连续变量创建子图
- en: '[PRE46]'
  id: totrans-582
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: In the `ggplot()` function call, we supply `Survived` as the x aesthetic and
    `Value` as the y aesthetic (coercing it into a numeric vector with `as.numeric()`
    because it was converted into a character by our `gather()` function call earlier).
    Next—and here’s the cool bit—we ask ggplot2 to *facet* by the `Variable` column,
    using the `facet_wrap()` function, and allow the y-axis to vary between the facets.
    Faceting allows us to draw subplots of our data, indexed by some faceting variable.
    Finally, we add a violin geometric object, which is similar to a box plot but
    also shows the density of data along the y-axis. The resulting plot is shown in
    [figure 4.10](#ch04fig10).
  id: totrans-583
  prefs: []
  type: TYPE_NORMAL
  zh: 在`ggplot()`函数调用中，我们提供`Survived`作为x美学，`Value`作为y美学（通过`as.numeric()`将其强制转换为数值向量，因为我们的`gather()`函数调用之前将其转换为字符）。接下来——这是亮点——我们要求ggplot2按`Variable`列分面，使用`facet_wrap()`函数，并允许y轴在分面之间变化。分面使我们能够绘制数据的子图，这些子图由分面变量索引。最后，我们添加一个小提琴几何对象，它类似于箱线图，但也显示了y轴上的数据密度。结果图表显示在[图4.10](#ch04fig10)中。
- en: Figure 4.10\. Faceted plot of `Survived` against `FamSize` and `Fare`. Violin
    plots show the density of data along the y-axis. The lines on each violin represent
    the first quartile, median, and third quartile (from lowest to highest).
  id: totrans-584
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.10\. `Survived`与`FamSize`和`Fare`的分面图。小提琴图显示了y轴上的数据密度。每个小提琴上的线条代表第一四分位数、中位数和第三四分位数（从最低到最高）。
- en: '![](fig4-10_alt.jpg)'
  id: totrans-585
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-10_alt.jpg)'
- en: 'Can you see how the faceting worked? Rows in the data with different values
    of `Variable` are plotted on different subplots! This is why we needed to gather
    the data into an untidy format: so we could supply a single variable for ggplot2
    to facet by.'
  id: totrans-586
  prefs: []
  type: TYPE_NORMAL
  zh: 你能看出分面是如何工作的吗？具有不同`Variable`值的行被绘制在不同的子图中！这就是为什么我们需要将数据收集到不整洁的格式中：这样我们就可以提供一个变量供ggplot2分面使用。
- en: '|  |'
  id: totrans-587
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-588
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Redraw the plot in [figure 4.10](#ch04fig10), but add a `geom_point()` layer,
    setting the `alpha` argument to 0.05 and the `size` argument to 3\. Does this
    make the violin plot make more sense?
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制[图4.10](#ch04fig10)中的图表，但添加一个`geom_point()`层，将`alpha`参数设置为0.05，将`size`参数设置为3。这会使小提琴图更有意义吗？
- en: '|  |'
  id: totrans-590
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now let’s do the same thing for the factors in our dataset by filtering the
    data for rows that contain *only* the `Pclass` and `Sex` variables. This time,
    we want to see what proportion of passengers in each level of the factors survived.
    To do so, we plot the factor levels on the x-axis by supplying `Value` as the
    x aesthetic mapping; and we want to use different colors to denote survival versus
    non-survival, so we supply `Survived` as the fill aesthetic. We facet by `Variable`
    as before and add a bar geometric object with the argument `position = "fill"`,
    which stacks the data for survivors and non-survivors such that they sum to 1
    to show us the proportion of each. The resulting plot is shown in [figure 4.11](#ch04fig11).
  id: totrans-591
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过筛选只包含`Pclass`和`Sex`变量的数据行来对我们数据集中的因素做同样的事情。这次，我们想看到因素每个级别的乘客生存比例。要做到这一点，我们将因素级别放在x轴上，通过提供`Value`作为x美学映射；我们还想用不同的颜色来表示生存与非生存，所以我们提供`Survived`作为填充美学。我们像以前一样按`Variable`分面，并添加一个带有`position
    = "fill"`参数的条形几何对象，这样就可以堆叠生存者和非生存者的数据，使它们相加等于1，以显示每个的比例。结果图表显示在[图4.11](#ch04fig11)中。
- en: Listing 4.5\. Creating subplots for each categorical variable
  id: totrans-592
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.5\. 为每个分类变量创建子图
- en: '[PRE47]'
  id: totrans-593
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Figure 4.11\. Faceted plot of `Survived` against `Pclass` and `Sex`. Filled
    bars represent the proportion of passengers at each level of the factors that
    survived (1 = survival).
  id: totrans-594
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图4.11\. `Survived`与`Pclass`和`Sex`的分面图。填充条形表示因素每个级别的乘客生存比例（1 = 生存）。
- en: '![](fig4-11_alt.jpg)'
  id: totrans-595
  prefs: []
  type: TYPE_IMG
  zh: '![](fig4-11_alt.jpg)'
- en: '|  |'
  id: totrans-596
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-597
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In the `filter()` function calls in [listings 4.4](#ch04ex04) and [4.5](#ch04ex05),
    I used the `&` and `|` operators to mean “and” and “or,” respectively.
  id: totrans-598
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表4.4](#ch04ex04)和[4.5](#ch04ex05)中的`filter()`函数调用中，我使用了`&`和`|`运算符分别表示“和”和“或”。
- en: '|  |'
  id: totrans-599
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: So it seems like passengers who survived tended to have slightly more family
    members on board (perhaps contradicting our hypothesis), although passengers with
    very large families on board tended not to survive. Age doesn’t seem to have had
    an obvious impact on survival, but being female meant you would be much more likely
    to survive. Paying more for your fare increased your probability of survival,
    as did being in a higher class (though the two probably correlate).
  id: totrans-600
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似乎幸存下来的乘客倾向于在船上拥有更多的家庭成员（可能与我们假设相矛盾），尽管在船上拥有非常大家庭的乘客往往不会幸存。年龄似乎对生存没有明显的影响，但女性意味着你更有可能幸存。为你的船票支付更多费用增加了你幸存的可能性，同样，处于更高等级也是如此（尽管这两个可能相关）。
- en: '|  |'
  id: totrans-601
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 2**'
  id: totrans-602
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: Redraw the plot in [figure 4.11](#ch04fig11), but change the `geom_bar()` argument
    `position` equal to `"dodge"`. Do this again, but make the `position` argument
    equal to `"stack"`. Can you see the difference between the three methods?
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 重新绘制[图 4.11](#ch04fig11)中的图表，但将`geom_bar()`参数`position`设置为`"dodge"`。再次这样做，但将`position`参数设置为`"stack"`。你能看到三种方法之间的区别吗？
- en: '|  |'
  id: totrans-604
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 4.2.4\. Training the model
  id: totrans-605
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.4\. 训练模型
- en: Now that we have our cleaned data, let’s create a task, learner, and model with
    mlr (specifying `"classif.logreg"` to use logistic regression as our learner).
    By setting the argument `predict.type = "prob"`, the trained model will output
    the estimated probabilities of each class when making predictions on new data,
    rather than just the predicted class membership.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了清洗过的数据，让我们使用mlr创建一个任务、学习者和模型（指定`"classif.logreg"`以使用逻辑回归作为我们的学习器）。通过设置参数`predict.type
    = "prob"`，训练好的模型在对新数据进行预测时将输出每个类别的估计概率，而不仅仅是预测的类别成员资格。
- en: Listing 4.6\. Creating a task and learner, and training a model
  id: totrans-607
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.6\. 创建任务和学习者，并训练模型
- en: '[PRE48]'
  id: totrans-608
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: Whoops! Something went wrong. What does the error message say? Hmm, it seems
    we have some missing data from the `Age` variable, and the logistic regression
    algorithm doesn’t know how to handle that. Let’s have a look at this variable.
    (I’m only displaying the first 60 elements to save room, but you can print the
    entire vector.)
  id: totrans-609
  prefs: []
  type: TYPE_NORMAL
  zh: 哎呀！出错了。错误信息是什么？嗯，看起来我们在`Age`变量中有些缺失数据，逻辑回归算法不知道如何处理这种情况。让我们看看这个变量。（我仅显示前60个元素以节省空间，但你可以打印整个向量。）
- en: Listing 4.7\. Counting missing values in the `Age` variable
  id: totrans-610
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 4.7\. 在`Age`变量中计数缺失值
- en: '[PRE49]'
  id: totrans-611
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: Ah, we have lots of NAs (177 in fact!), which is R’s way of labeling missing
    data.
  id: totrans-612
  prefs: []
  type: TYPE_NORMAL
  zh: 啊，我们有很多NA（实际上有177个！），这是R标记缺失数据的方式。
- en: 4.2.5\. Dealing with missing data
  id: totrans-613
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.5\. 处理缺失数据
- en: 'There are two ways to handle missing data:'
  id: totrans-614
  prefs: []
  type: TYPE_NORMAL
  zh: 处理缺失数据有两种方法：
- en: Simply exclude cases with missing data from the analysis
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单地从分析中排除具有缺失数据的案例
- en: Apply an *imputation* mechanism to fill in the gaps
  id: totrans-616
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用一个*插补*机制来填补空白
- en: The first option may be valid when the ratio of cases with missing values to
    complete cases is very small. In that case, omitting cases with missing data is
    unlikely to have a large impact on the performance of our model. It is a simple,
    if not elegant, solution to the problem.
  id: totrans-617
  prefs: []
  type: TYPE_NORMAL
  zh: 当缺失值案例与完整案例的比例非常小的时候，第一个选项可能是有效的。在这种情况下，省略具有缺失数据的案例不太可能对我们的模型性能产生重大影响。这是一个简单（如果不是优雅）的解决方案。
- en: The second option, missing value imputation, is the process by which we use
    some algorithm to estimate what those missing values would have been, replace
    the NAs with these estimates, and use this imputed dataset to train our model.
    There are many different ways of estimating the values of missing data, and we’ll
    use more sophisticated ones throughout the book, but for now, we’ll employ mean
    imputation, where we simply take the mean of the variable with missing data and
    replace missing values with that.
  id: totrans-618
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选项，缺失值插补，是我们使用某种算法估计那些缺失值可能是什么，用这些估计值替换NA，并使用这个插补数据集来训练我们的模型的过程。有几种不同的方法可以估计缺失数据的值，我们将在整本书中使用更复杂的方法，但就目前而言，我们将采用均值插补，即简单地取缺失数据的变量的平均值，并用这个值替换缺失值。
- en: In [listing 4.8](#ch04ex08), I use mlr’s `impute()` function to replace the
    missing data. The first argument is the name of the data, and the `cols` argument
    asks us which columns we want to impute and what method we want to apply. We supply
    the `cols` argument as a list of the column names, separated by commas if we have
    more than one. Each column listed should be followed by an `=` sign and the imputation
    method (`imputeMean()` uses the mean of the variable to replace NAs). I save the
    imputed data structure as an object, `imp`, and use `sum(is.na())` to count the
    number of missing values from the data.
  id: totrans-619
  prefs: []
  type: TYPE_NORMAL
  zh: 在[代码列表4.8](#ch04ex08)中，我使用mlr的`impute()`函数替换缺失数据。第一个参数是数据名称，`cols`参数询问我们想要填充哪些列以及想要应用哪种方法。如果我们有多个列，我们将`cols`参数作为列名称的列表提供，列名称之间用逗号分隔。每个列名称后面应跟一个`=`符号和填充方法（`imputeMean()`使用变量的平均值来替换NAs）。我将填充后的数据结构保存为对象`imp`，并使用`sum(is.na())`来计算数据中的缺失值数量。
- en: Listing 4.8\. Imputing missing values in the `Age` variable
  id: totrans-620
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表4.8. 在`Age`变量中填充缺失值
- en: '[PRE50]'
  id: totrans-621
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: We can see that those 177 missing values have all been imputed!
  id: totrans-622
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，那些177个缺失值都已经填充了！
- en: 4.2.6\. Training the model (take two)
  id: totrans-623
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.2.6. 训练模型（第二部分）
- en: Okay, we’ve imputed those pesky missing values with the mean and created the
    new object `imp`. Now let’s try again by creating a task using the imputed data.
    The `imp` object contains both the imputed data and a description for the imputation
    process we used. To extract the data, we simply use `imp$data`.
  id: totrans-624
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，我们已经用平均值填充了那些讨厌的缺失值，并创建了新的对象`imp`。现在让我们再次尝试，通过使用填充数据创建一个任务。`imp`对象包含填充后的数据和我们所使用的填充过程的描述。要提取数据，我们只需使用`imp$data`。
- en: Listing 4.9\. Training a model on imputed data
  id: totrans-625
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表4.9. 在填充数据上训练模型
- en: '[PRE51]'
  id: totrans-626
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: This time, no error messages. Next, let’s cross-validate our model to estimate
    how it will perform.
  id: totrans-627
  prefs: []
  type: TYPE_NORMAL
  zh: 这次没有错误信息。接下来，让我们交叉验证我们的模型以估计其性能。
- en: 4.3\. Cross-validating the logistic regression model
  id: totrans-628
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.3. 交叉验证逻辑回归模型
- en: Remember that when we cross-validate, we should cross-validate our entire model-building
    procedure. This should include any data-dependent preprocessing steps, such as
    missing value imputation. In [chapter 3](kindle_split_013.html#ch03), we used
    a wrapper function to wrap together our learner and our hyperparameter tuning
    procedure. This time, we’re going to create a wrapper for our learner and our
    missing value imputation.
  id: totrans-629
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，当我们进行交叉验证时，我们应该交叉验证整个模型构建过程。这应该包括任何数据相关的预处理步骤，例如缺失值填充。在[第3章](kindle_split_013.html#ch03)中，我们使用包装函数将我们的学习者和超参数调整过程包装在一起。这次，我们将为我们的学习者和缺失值填充创建一个包装器。
- en: 4.3.1\. Including missing value imputation in cross-validation
  id: totrans-630
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.1. 在交叉验证中包含缺失值填充
- en: The `makeImputeWrapper()` function wraps together a learner (given as the first
    argument) and an imputation method. Notice how we specify the imputation method
    in exactly the same way as for the `impute()` function in [listing 4.8](#ch04ex08),
    by supplying a list of columns and their imputation method.
  id: totrans-631
  prefs: []
  type: TYPE_NORMAL
  zh: '`makeImputeWrapper()`函数将一个学习器（作为第一个参数给出）和一个填充方法包装在一起。注意我们如何以与[代码列表4.8](#ch04ex08)中的`impute()`函数完全相同的方式指定填充方法，通过提供列的列表及其填充方法。'
- en: Listing 4.10\. Wrapping together the learner and the imputation method
  id: totrans-632
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表4.10. 包装学习器和填充方法
- en: '[PRE52]'
  id: totrans-633
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Now let’s apply stratified, 10-fold cross-validation, repeated 50 times, to
    our wrapped learner.
  id: totrans-634
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们应用分层10折交叉验证，重复50次，到我们的包装学习器。
- en: '|  |'
  id: totrans-635
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that we first define our resampling method using `makeResampleDesc()`
    and then use `resample()` to run the cross-validation.
  id: totrans-637
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们首先使用`makeResampleDesc()`定义我们的重采样方法，然后使用`resample()`运行交叉验证。
- en: '|  |'
  id: totrans-638
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Because we’re supplying our wrapped learner to the `resample()` function, for
    each fold of the cross-validation, the mean of the `Age` variable in the training
    set will be used to impute any missing values.
  id: totrans-639
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们向`resample()`函数提供了包装后的学习器，对于交叉验证的每个折叠，训练集中`Age`变量的平均值将用于填充任何缺失值。
- en: Listing 4.11\. Cross-validating our model-building process
  id: totrans-640
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 代码列表4.11. 交叉验证模型构建过程
- en: '[PRE53]'
  id: totrans-641
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: As this is a two-class classification problem, we have access to a few extra
    performance metrics, such as the false positive rate (`fpr`) and false negative
    rate (`fnr`). In the cross-validation procedure in [listing 4.11](#ch04ex11),
    we ask for accuracy, false positive rate, and false negative rate to be reported
    as performance metrics. We can see that although on average across the repeats
    our model correctly classified 79.6% of passengers, it incorrectly classified
    29.9% of passengers who died as having survived (false positives), and incorrectly
    classified 14.4% of passengers who survived as having died (false negatives).
  id: totrans-642
  prefs: []
  type: TYPE_NORMAL
  zh: 由于这是一个二分类问题，我们可以访问一些额外的性能指标，例如假阳性率（`fpr`）和假阴性率（`fnr`）。在[列表 4.11](#ch04ex11)中的交叉验证过程中，我们要求报告准确率、假阳性率和假阴性率作为性能指标。我们可以看到，尽管在重复实验中，我们的模型平均正确分类了79.6%的乘客，但它错误地将29.9%的死亡乘客分类为幸存者（假阳性），并将14.4%的幸存乘客分类为死亡（假阴性）。
- en: 4.3.2\. Accuracy is the most important performance metric, right?
  id: totrans-643
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.3.2. 准确率是最重要的性能指标，对吗？
- en: You might think that the accuracy of a model’s predictions is the defining metric
    of its performance. Often, this is the case, but sometimes, it’s not.
  id: totrans-644
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为模型预测的准确率是其性能的衡量标准。通常情况下是这样的，但有时并非如此。
- en: Imagine that you work for a bank as a data scientist in the fraud-detection
    department. It’s your job to build a model that predicts whether credit card transactions
    are legitimate or fraudulent. Let’s say that out of 100,000 credit card transactions,
    only 1 is fraudulent. Because fraud is relatively rare, (and because they’re serving
    pizza for lunch today), you decide to build a model that simply classifies all
    transactions as legitimate.
  id: totrans-645
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你是一家银行的数据科学家，在欺诈检测部门工作。你的任务是构建一个模型，预测信用卡交易是合法的还是欺诈的。比如说，在100,000笔信用卡交易中，只有1笔是欺诈的。由于欺诈相对较少（并且因为他们今天午餐在提供披萨），你决定构建一个模型，简单地将所有交易分类为合法。
- en: The model accuracy is 99.999%. Pretty good? Of course not! The model isn’t able
    to identify *any* fraudulent transactions and has a false negative rate of 100%!
  id: totrans-646
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的准确率为99.999%。很好吗？当然不是！该模型无法识别任何欺诈交易，并且具有100%的假阴性率！
- en: The lesson here is that you should evaluate model performance *in the context
    of your particular problem*. Another example could be building a model that will
    guide doctors to use an unpleasant treatment, or not, for a patient. In the context
    of this problem, it may be acceptable to incorrectly *not* give a patient the
    unpleasant treatment, but it is imperative that you don’t incorrectly give a patient
    the treatment if they don’t need it!
  id: totrans-647
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的教训是，你应该在特定问题的背景下评估模型性能。另一个例子可能是构建一个模型，指导医生为患者使用或不使用不愉快的治疗方法。在这个问题的背景下，可能可以接受错误地不给予患者不愉快的治疗，但如果你错误地给予患者不需要的治疗，这是绝对必要的！
- en: If positive events are rare (as in our fraudulent credit card example), or if
    it is particularly important that you don’t misclassify positive cases as negative,
    you should favor models that have a low false negative rate. If negative events
    are rare, or if it is particularly important that you don’t misclassify negative
    cases as positive (as in our medical treatment example), you should favor models
    that have a low false positive rate.
  id: totrans-648
  prefs: []
  type: TYPE_NORMAL
  zh: 如果积极事件很少见（如我们欺诈信用卡的例子），或者如果你特别重要，不要将阳性案例错误分类为阴性，你应该选择具有低假阴性率的模型。如果阴性事件很少见，或者如果你特别重要，不要将阴性案例错误分类为阳性（如我们的医疗治疗例子），你应该选择具有低假阳性率的模型。
- en: Take a look at [https://mlr.mlr-org.com/articles/tutorial/measures.html](https://mlr.mlr-org.com/articles/tutorial/measures.html)
    to see all the performance measures currently wrapped by mlr and the situations
    in which they can be used.
  id: totrans-649
  prefs: []
  type: TYPE_NORMAL
  zh: 查看以下链接以了解mlr目前包含的所有性能指标及其可用情况：[https://mlr.mlr-org.com/articles/tutorial/measures.html](https://mlr.mlr-org.com/articles/tutorial/measures.html)。
- en: '4.4\. Interpreting the model: The odds ratio'
  id: totrans-650
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.4. 解释模型：优势比
- en: I mentioned at the start of the chapter that logistic regression is very popular
    because of how interpretable the model parameters (the y-intercept, and the slopes
    for each of the predictors) are. To extract the model parameters, we must first
    turn our mlr model object, `logRegModel`, into an R model object using the `getLearnerModel()`
    function. Next, we pass this R model object as the argument to the `coef()` function,
    which stands for *coefficients* (another term for parameters), so this function
    returns the model parameters.
  id: totrans-651
  prefs: []
  type: TYPE_NORMAL
  zh: 我在章节开头提到，逻辑回归之所以非常流行，是因为模型参数（y截距和每个预测因子的斜率）的可解释性。为了提取模型参数，我们必须首先使用`getLearnerModel()`函数将我们的mlr模型对象`logRegModel`转换为R模型对象。接下来，我们将这个R模型对象作为`coef()`函数的参数传递，该函数代表*系数*（参数的另一个术语），因此这个函数返回模型参数。
- en: Listing 4.12\. Extracting model parameters
  id: totrans-652
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.12\. 提取模型参数
- en: '[PRE54]'
  id: totrans-653
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The intercept is the log odds of surviving the *Titanic* disaster when all continuous
    variables are 0 and the factors are at their reference levels. We tend to be more
    interested in the slopes than the y-intercept, but these values are in log odds
    units, which are difficult to interpret. Instead, people commonly convert them
    into *odds ratios*.
  id: totrans-654
  prefs: []
  type: TYPE_NORMAL
  zh: 截距是在所有连续变量均为0且因素变量处于其参考水平时，生存“泰坦尼克号”灾难的对数概率。我们通常对斜率比y截距更感兴趣，但这些值是以对数概率单位表示的，难以解释。相反，人们通常将它们转换为*概率比*。
- en: An odds ratio is, well, a ratio of odds. For example, if the odds of surviving
    the *Titanic* if you’re female are about 7 to 10, and the odds of surviving if
    you’re male are 2 to 10, then the odds ratio for surviving if you’re female is
    3.5\. In other words, if you were female, you would have been 3.5 times more likely
    to survive than if you were male. Odds ratios are a very popular way of interpreting
    the impact of predictors on an outcome, because they are easily understood.
  id: totrans-655
  prefs: []
  type: TYPE_NORMAL
  zh: 概率比（odds ratio）实际上就是概率之比。例如，如果你是女性，在“泰坦尼克号”上的生存概率大约是7比10，而如果你是男性，生存概率是2比10，那么如果你是女性的生存概率比是3.5。换句话说，如果你是女性，你生存的可能性比男性高3.5倍。概率比是解释预测因子对结果影响的一种非常流行的方式，因为它们很容易理解。
- en: 4.4.1\. Converting model parameters into odds ratios
  id: totrans-656
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.1\. 将模型参数转换为概率比
- en: How do we get from log odds to odds ratios? By taking their exponent (*e*^(log
    odds)). We can also calculate 95% confidence intervals using the `confint()` function,
    to help us decide how strong the evidence is that each variable has predictive
    value.
  id: totrans-657
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何从对数概率转换为概率比？通过取它们的指数（*e*^(对数概率)）。我们还可以使用`confint()`函数计算95%置信区间，以帮助我们决定每个变量具有预测价值的证据强度。
- en: Listing 4.13\. Converting model parameters into odds ratios
  id: totrans-658
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.13\. 将模型参数转换为概率比
- en: '[PRE55]'
  id: totrans-659
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: Most of these odds ratios are less than 1\. An odds ratio less than 1 means
    an event is *less* likely to occur. It’s usually easier to interpret these if
    you divide 1 by them. For example, the odds ratio for surviving if you were male
    is 0.06, and 1 divided by 0.06 = 16.7\. This means that, holding all other variables
    constant, men were 16.7 times *less* likely to survive than women.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: 这些概率比中大多数都小于1。小于1的概率比意味着事件发生的可能性*较低*。如果你将1除以它们，通常更容易解释这些值。例如，如果你是男性的生存概率比是0.06，而1除以0.06
    = 16.7。这意味着，在保持所有其他变量不变的情况下，男性比女性生存的可能性低16.7倍。
- en: For continuous variables, we interpret the odds ratio as how much more likely
    a passenger is to survive for every one-unit increase in the variable. For example,
    for every additional family member, a passenger was 1/0.78 = 1.28 times less likely
    to survive.
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 对于连续变量，我们解释概率比是指乘客在变量每增加一个单位时生存可能性增加多少。例如，对于每个额外的家庭成员，乘客的生存可能性降低了1/0.78 = 1.28倍。
- en: For factors, we interpret the odds ratio as how much more likely a passenger
    is to survive, compared to the reference level for that variable. For example,
    we have odds ratios for `Pclass2` and `Pclass3`, which are how many more times
    passengers in classes 2 and 3 are likely to survive compared to those in class
    1, respectively.
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 对于因素变量，我们解释概率比是指乘客相对于该变量的参考水平生存可能性增加多少。例如，我们有`Pclass2`和`Pclass3`的概率比，分别表示2等和3等舱乘客相对于1等舱乘客生存可能性增加的倍数。
- en: The 95% confidence intervals indicate the strength of the evidence that each
    variable has predictive value. An odds ratio of 1 means the odds are equal and
    the variable has no impact on prediction. Therefore, if the 95% confidence intervals
    include the value 1, such as those for the `Fare` variable, then this *may* suggest
    that this variable isn’t contributing anything.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 95%置信区间表明每个变量具有预测价值的证据强度。优势比率为1表示机会相等，变量对预测没有影响。因此，如果95%置信区间包括值1，例如`Fare`变量的那些，那么这*可能*表明这个变量没有做出贡献。
- en: 4.4.2\. When a one-unit increase doesn’t make sense
  id: totrans-664
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 4.4.2. 当单位增加没有意义时
- en: A one-unit increase often isn’t easily interpretable. Say you get an odds ratio
    that says for every additional ant in an anthill, that anthill is 1.000005 times
    more likely to survive a termite attack. How can you comprehend the importance
    of such a small odds ratio?
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 单位增加通常不容易解释。比如说，你得到一个优势比率为，对于每个额外的蚂蚁，蚁丘存活于白蚁攻击的可能性是1.000005倍。你如何理解这样一个小的优势比率的重要性？
- en: 'When it doesn’t make sense to think in one-unit increases, a popular technique
    is to log[2] transform the continuous variables instead, before training the model
    with them. This won’t impact the predictions made by the model, but now the odds
    ratio can be interpreted this way: every time the number of ants *doubles*, the
    anthill is *x* times more likely to survive. This will give much larger and much
    more interpretable odds ratios.'
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 当考虑单位增加没有意义时，一种流行的技术是在训练模型之前对连续变量进行log[2]变换。这不会影响模型做出的预测，但现在优势比率可以这样解释：每次蚂蚁数量*翻倍*，蚁丘存活于白蚁攻击的可能性是*x*倍。这将给出更大且更易于解释的优势比率。
- en: 4.5\. Using our model to make predictions
  id: totrans-667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.5. 使用我们的模型进行预测
- en: We’ve built, cross-validated, and interpreted our model, and now it would be
    nice to use the model to make predictions on new data. This scenario is a little
    unusual in that we’ve built a model based on a historical event, so (hopefully!)
    we won’t be using it to predict survival of another Titanic disaster. Nevertheless,
    I want to illustrate to you how to make predictions with a logistic regression
    model, the same as you can for any other supervised algorithm. Let’s load some
    unlabeled passenger data, clean it ready for prediction, and pass it through our
    model.
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经构建、交叉验证并解释了我们的模型，现在使用该模型对新数据进行预测将是一件很棒的事情。这种情况有些不寻常，因为我们基于历史事件构建了一个模型，所以（希望！）我们不会用它来预测另一场泰坦尼克号的灾难。尽管如此，我想向你展示如何使用逻辑回归模型进行预测，就像你可以对任何其他监督算法做的那样。让我们加载一些未标记的乘客数据，对其进行清理以便预测，并将其通过我们的模型。
- en: Listing 4.14\. Using our model to make predictions on new data
  id: totrans-669
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表4.14. 使用我们的模型对新数据进行预测
- en: '[PRE56]'
  id: totrans-670
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: 4.6\. Strengths and weaknesses of logistic regression
  id: totrans-671
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4.6. 逻辑回归的优势和弱点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    logistic regression will perform well for you.
  id: totrans-672
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪种算法对于特定任务会表现良好，但以下是一些优势和弱点，这将帮助你决定逻辑回归是否适合你。
- en: 'The strengths of the logistic regression algorithm are as follows:'
  id: totrans-673
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法的优势如下：
- en: It can handle both continuous and categorical predictors.
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理连续和分类预测变量。
- en: The model parameters are very interpretable.
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型参数非常易于解释。
- en: Predictor variables are *not* assumed to be normally distributed.
  id: totrans-676
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测变量*不*假设是正态分布的。
- en: 'The weaknesses of the logistic regression algorithm are these:'
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑回归算法的弱点如下：
- en: It won’t work when there is complete separation between classes.
  id: totrans-678
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当类别之间完全分离时，它将不起作用。
- en: It assumes that the classes are *linearly separable*. In other words, it assumes
    that a flat surface in *n*-dimensional space (where *n* is the number of predictors)
    can be used to separate the classes. If a curved surface is required to separate
    the classes, logistic regression will underperform compared to some other algorithms.
  id: totrans-679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设类别是*线性可分*的。换句话说，它假设在*n*-维空间（其中*n*是预测变量的数量）中的一个平面可以用来分离类别。如果需要曲面来分离类别，与一些其他算法相比，逻辑回归的表现将不佳。
- en: It assumes a linear relationship between each predictor and the log odds. If,
    for example, cases with low and high values of a predictor belong to one class,
    but cases with medium values of the predictor belong to another class, this linearity
    will break down.
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设每个预测变量与对数几率之间存在线性关系。例如，如果预测变量的低值和高值案例属于一个类别，而预测变量的中等值案例属于另一个类别，这种线性关系就会破裂。
- en: '|  |'
  id: totrans-681
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-682
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 3**'
- en: Repeat the model-building process, but omit the `Fare` variable. Does it make
    a difference to model performance as estimated by cross-validation? Why?
  id: totrans-683
  prefs: []
  type: TYPE_NORMAL
  zh: 重复模型构建过程，但省略`Fare`变量。交叉验证估计的模型性能是否有差异？为什么？
- en: '|  |'
  id: totrans-684
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-685
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 4**'
  id: totrans-686
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 4**'
- en: 'Extract the salutations from the `Name` variable, and convert any that aren’t
    `"Mr"`, `"Dr"`, `"Master"`, `"Miss"`, `"Mrs"`, or `"Rev"` to `"Other"`. Look at
    the following code for a hint as to how to extract the salutations with the `str_split()`
    function from the stringr tidyverse package:'
  id: totrans-687
  prefs: []
  type: TYPE_NORMAL
  zh: 从`Name`变量中提取称呼，并将任何不是`"Mr"`、`"Dr"`、`"Master"`、`"Miss"`、`"Mrs"`或`"Rev"`的称呼转换为`"Other"`。查看以下代码，了解如何使用stringr
    tidyverse包中的`str_split()`函数提取称呼：
- en: '[PRE57]'
  id: totrans-688
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '|  |'
  id: totrans-689
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-690
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 5**'
  id: totrans-691
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 5**'
- en: Build a model that includes `Salutation` as another predictor, and cross-validate
    it. Does this improve model performance?
  id: totrans-692
  prefs: []
  type: TYPE_NORMAL
  zh: 构建一个包含`Salutation`作为另一个预测变量的模型，并进行交叉验证。这会提高模型性能吗？
- en: '|  |'
  id: totrans-693
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-694
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Logistic regression is a supervised learning algorithm that classifies new data
    by calculating the probabilities of the data belonging to each class.
  id: totrans-695
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归是一种监督学习算法，通过计算数据属于每个类的概率来对新数据进行分类。
- en: Logistic regression can handle continuous and categorical predictors, and models
    a linear relationship between the predictors and the log odds of belonging to
    the positive class.
  id: totrans-696
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 逻辑回归可以处理连续和分类预测变量，并建模预测变量与属于正类对数几率之间的线性关系。
- en: Feature engineering is the process by which we extract information from, or
    create new variables from, existing variables to maximize their predictive value.
  id: totrans-697
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征工程是从现有变量中提取信息或创建新变量的过程，以最大化其预测价值。
- en: Feature selection is the process of choosing which variables in a dataset have
    predictive value for machine learning models.
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征选择是选择数据集中哪些变量对机器学习模型具有预测价值的过程。
- en: Imputation is a strategy for dealing with missing data, where some algorithm
    is used to estimate what the missing values would have been. You learned how to
    apply mean imputation for the *Titanic* dataset.
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 假设缺失值处理策略，其中使用某些算法来估计缺失值可能是什么。你学习了如何为*泰坦尼克号*数据集应用均值填充。
- en: Odds ratios are an informative way of interpreting the impact each of our predictors
    has on the odds of a case belonging to the positive class. They can be calculated
    by taking the exponent of the model slopes (*e*^(log odds)).
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 几率比是一种解释每个预测变量对案例属于正类几率影响的有信息量的方式。它们可以通过取模型斜率的指数（*e*^(对数几率)）来计算。
- en: Solutions to exercises
  id: totrans-701
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题解答
- en: 'Redraw the violin plots, adding a `geom_point()` layer with transparency:'
  id: totrans-702
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新绘制小提琴图，添加一个透明度的`geom_point()`层：
- en: '[PRE58]'
  id: totrans-703
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE58]'
- en: 'Redraw the bar plots, but use the `"dodge"` and `"stack"` position arguments:'
  id: totrans-704
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新绘制条形图，但使用`"dodge"`和`"stack"`位置参数：
- en: '[PRE59]'
  id: totrans-705
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Build the model, but omit the `Fare` variable:'
  id: totrans-706
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建模型，但省略`Fare`变量：
- en: '[PRE60]'
  id: totrans-707
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE60]'
- en: Omitting the `Fare` variable makes little difference to model performance, because
    it has no additional predictive value to the `Pclass` variable (look at the odds
    ratio and confidence interval for `Fare` in [listing 4.13](#ch04ex13)).
  id: totrans-708
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 忽略`Fare`变量对模型性能的影响很小，因为它对`Pclass`变量没有额外的预测价值（查看[列表 4.13](#ch04ex13)中`Fare`的几率比和置信区间）。
- en: 'Extract salutations from the `Name` variable (there are many ways of doing
    this, so don’t worry if your way is different than mine):'
  id: totrans-709
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从`Name`变量中提取称呼（有多种方法可以做到这一点，所以不要担心你的方法与我的不同）：
- en: '[PRE61]'
  id: totrans-710
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Build a model using `Salutation` as a predictor:'
  id: totrans-711
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`Salutation`作为预测变量构建模型：
- en: '[PRE62]'
  id: totrans-712
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The feature extraction paid off! Including `Salutation` as a predictor improved
    model performance.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 特征提取效果显著！将`Salutation`作为预测变量提高了模型性能。
- en: Chapter 5\. Classifying by maximizing separation with discriminant analysis
  id: totrans-714
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第5章：通过判别分析最大化分离进行分类
- en: '*This chapter covers*'
  id: totrans-715
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Understanding linear and quadratic discriminant analysis
  id: totrans-716
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解线性与二次判别分析
- en: Building discriminant analysis classifiers to predict wines
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 构建判别分析分类器以预测葡萄酒
- en: '*Discriminant analysis* is an umbrella term for multiple algorithms that solve
    classification problems (where we wish to predict a categorical variable) in a
    similar way. While there are various discriminant analysis algorithms that learn
    slightly differently, they all find a new representation of the original data
    that maximizes the separation between the classes.'
  id: totrans-718
  prefs: []
  type: TYPE_NORMAL
  zh: '*判别分析* 是一个总称，指的是多种解决分类问题（我们希望预测一个分类变量）的算法。虽然不同的判别分析算法学习方式略有不同，但它们都找到了原始数据的新表示，以最大化类之间的分离。'
- en: Recall from [chapter 1](kindle_split_010.html#ch01) that predictor variables
    are the variables we hope contain the information needed to make predictions on
    new data. Discriminant function analysis algorithms find a new representation
    of the predictor variables (which must be continuous) by combining them together
    into new variables that best *discriminate* the classes. This combination of predictor
    variables often has the handy benefit of reducing the number of predictors to
    a much smaller number. Because of this, despite discriminant analysis algorithms
    being classification algorithms, they are similar to some of the dimension-reduction
    algorithms we’ll encounter in [part 4](kindle_split_024.html#part04) of the book.
  id: totrans-719
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第1章](kindle_split_010.html#ch01)，预测变量是我们希望包含对新数据进行预测所需信息的变量。判别函数分析算法通过将它们组合成新的变量（这些变量必须是连续的）来找到预测变量的新表示，这些新变量最好地
    *区分* 类别。这种预测变量的组合通常有一个有用的好处，即减少预测变量的数量到一个更小的数量。正因为如此，尽管判别分析算法是分类算法，但它们与我们在本书第4部分（kindle_split_024.html#part04）中将要遇到的某些降维算法相似。
- en: '|  |'
  id: totrans-720
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: '*Dimension reduction* is the process of learning how the information in a set
    of variables can be condensed into a smaller number of variables, with as little
    information loss as possible.'
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: '*降维* 是一个过程，它学习如何将一组变量中的信息尽可能少地损失地压缩成更少的变量。'
- en: '|  |'
  id: totrans-723
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.1\. What is discriminant analysis?
  id: totrans-724
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1\. 什么是判别分析？
- en: In this section, you’ll learn why discriminant analysis is useful and how it
    works. Imagine that you want to find out if you can predict how patients will
    respond to a drug based on their gene expression. You measure the expression level
    of 1,000 genes and record whether they respond positively, negatively, or not
    at all to the drug (a three-class classification problem).
  id: totrans-725
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解判别分析为何有用以及它是如何工作的。想象一下，你想要找出是否可以根据患者的基因表达预测他们对药物的反应。你测量了1,000个基因的表达水平，并记录它们对药物的反应是积极、消极还是完全没有反应（一个三分类问题）。
- en: 'A dataset that has as many predictor variables as this (and it isn’t rare to
    find datasets this large) presents a few problems:'
  id: totrans-726
  prefs: []
  type: TYPE_NORMAL
  zh: 一个具有如此多预测变量（并且找到这样大的数据集并不罕见）的数据集会带来一些问题：
- en: The data is very difficult to explore and plot manually.
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据非常难以手动探索和绘制。
- en: There may be many predictor variables that contain no or very little predictive
    information.
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可能存在许多预测变量，它们几乎不包含或只包含很少的预测信息。
- en: We have the *curse of dimensionality* to contend with (a problem algorithms
    encounter when trying to learn patterns in high-dimensional data).
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们必须应对 *维度灾难*（算法在尝试学习高维数据中的模式时遇到的问题）。
- en: In our gene expression example, it would be nearly impossible to plot all 1,000
    genes in such a way that we could interpret the similarities/differences between
    the classes. Instead, we could use discriminant analysis to take all that information
    and condense it into a manageable number of *discriminant functions*, each of
    which is a combination of the original variables. Put another way, discriminant
    analysis takes the predictor variables as input and finds a new, lower-dimensional
    representation of those variables that maximizes the separation between the classes.
    Therefore, while discriminant analysis is a classification technique, it employs
    dimension reduction to achieve its goal. This is illustrated in [figure 5.1](#ch05fig01).
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的基因表达示例中，要这样绘制所有1,000个基因，以便我们能够解释类之间的相似性/差异性几乎是不可能的。相反，我们可以使用判别分析将所有这些信息压缩成可管理的数量的
    *判别函数*，每个函数都是原始变量的组合。换句话说，判别分析将预测变量作为输入，并找到这些变量的新、低维表示，以最大化类之间的分离。因此，尽管判别分析是一种分类技术，但它使用降维来实现其目标。这如图5.1所示。
- en: '|  |'
  id: totrans-731
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-732
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Due to their dimensionality reduction, discriminant analysis algorithms are
    popular techniques for classification problems where you have many continuous
    predictor variables.
  id: totrans-733
  prefs: []
  type: TYPE_NORMAL
  zh: 由于它们的降维，判别分析算法是分类问题中流行的技术，在这些问题中，你有很多连续的预测变量。
- en: '|  |'
  id: totrans-734
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Figure 5.1\. Discriminant analysis algorithms take the original data and combine
    continuous predictor variables together into new variables that maximize the separation
    of the classes.
  id: totrans-735
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.1。判别分析算法将原始数据与连续预测变量结合，形成新的变量，这些变量最大化类别的分离。
- en: '![](fig5-1_alt.jpg)'
  id: totrans-736
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-1_alt.jpg)'
- en: 'The number of these discriminant functions will be the smaller of these:'
  id: totrans-737
  prefs: []
  type: TYPE_NORMAL
  zh: 这些判别函数的数量将是以下两者中的较小者：
- en: The number of classes minus 1
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类别数减去1
- en: The number of predictor variables
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测变量的数量
- en: In the gene expression example, the information contained in those 1,000 predictor
    variables would be condensed into just 2 variables (three classes minus 1). We
    could now easily plot these two new variables against each other to see how separable
    our three classes are!
  id: totrans-740
  prefs: []
  type: TYPE_NORMAL
  zh: 在基因表达示例中，那些1,000个预测变量包含的信息将浓缩成仅仅2个变量（三个类别减去1）。现在我们可以轻松地将这两个新变量相互对比，以查看我们的三个类别是如何分离的！
- en: As you learned in [chapter 4](kindle_split_014.html#ch04), including predictor
    variables that contain little or no predictive value adds noise, which can negatively
    impact how the learned model performs. When discriminant analysis algorithms learn
    their discriminant functions, greater weight or importance is given to predictors
    that better discriminate the classes. Predictors that contain little or no predictive
    value are given less weight and contribute less to the final model. To a degree,
    this lower weighting of uninformative predictors mitigates their impact on model
    performance.
  id: totrans-741
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在[第4章](kindle_split_014.html#ch04)中学到的，包括包含很少或没有预测价值的预测变量会增加噪声，这可能会对学习到的模型的表现产生负面影响。当判别分析算法学习它们的判别函数时，会给予更好地区分类别的预测变量更大的权重或重要性。包含很少或没有预测价值的预测变量得到的权重较小，对最终模型的贡献也较小。在一定程度上，这种对无信息预测变量的低权重减轻了它们对模型性能的影响。
- en: '|  |'
  id: totrans-742
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-743
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Despite mitigating the impact of weak predictors, a discriminant analysis model
    will still tend to perform better after performing feature selection (removing
    weakly predictive predictors).
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管可以减轻弱预测变量的影响，但在进行特征选择（移除弱预测变量）后，判别分析模型仍然倾向于表现更好。
- en: '|  |'
  id: totrans-745
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The curse of dimensionality is a terrifying-sounding phenomenon that causes
    problems when working with high-dimensional data (data with many predictor variables).
    As the *feature space* (the set of all possible combinations of predictor variables)
    increases, the data in that space becomes more *sparse*. Put more plainly, for
    the same number of cases in a dataset, if you increase the feature space, the
    cases get further apart from each other, and there is more empty space between
    them. This is demonstrated in [figure 5.2](#ch05fig02) by going from a one-feature
    space to a three-feature space.
  id: totrans-746
  prefs: []
  type: TYPE_NORMAL
  zh: 维度灾难是一个听起来令人恐惧的现象，当处理高维数据（具有许多预测变量的数据）时会引起问题。随着*特征空间*（所有可能的预测变量组合的集合）的增加，该空间中的数据变得更加*稀疏*。更简单地说，对于数据集中相同数量的案例，如果你增加特征空间，案例彼此之间的距离会变得更远，它们之间的空隙也会更大。这通过从一维特征空间到三维特征空间的转换在[图5.2](#ch05fig02)中得到了演示。
- en: Figure 5.2\. Data becomes more sparse as the number of dimensions increases.
    Two classes are shown in one-, two-, and three-dimensional feature spaces. The
    dotted lines in the three-dimensional representation are to clarify the position
    of the points along the z-axis. Note the increasing empty space with increased
    dimensions.
  id: totrans-747
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.2。随着维度的增加，数据变得更加稀疏。两个类别在一维、二维和三维特征空间中显示。三维表示中的虚线用于澄清点在z轴上的位置。注意随着维度的增加，空隙越来越大。
- en: '![](fig5-2_alt.jpg)'
  id: totrans-748
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-2_alt.jpg)'
- en: The consequence of this increase in dimensionality is that an area of the feature
    space may have very few cases occupying it, so an algorithm is more likely to
    learn from “exceptional” cases in the data. When algorithms learn from exceptional
    cases, this results in models that are overfit and have a lot of variance in their
    predictions. This is the curse of dimensionality.
  id: totrans-749
  prefs: []
  type: TYPE_NORMAL
  zh: 维度增加的后果是特征空间的一个区域可能只有很少的案例占据，因此算法更有可能从数据中的“异常”案例中学习。当算法从异常案例中学习时，这会导致过度拟合的模型，其预测的方差很大。这就是维度灾难。
- en: '|  |'
  id: totrans-750
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-751
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: As the number of predictor variables increases linearly, the number of cases
    would need to increase exponentially to maintain the same density in the feature
    space.
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 随着预测变量数量的线性增加，案例的数量需要指数级增加，以保持特征空间中的相同密度。
- en: '|  |'
  id: totrans-753
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: This isn’t to say that having more variables is bad, however! For most problems,
    adding predictors with valuable information improves the predictive accuracy of
    a model . . . until it doesn’t (until we get diminishing returns). So how do we
    guard against overfitting due to the curse of dimensionality? By performing feature
    selection (as we did in [chapter 4](kindle_split_014.html#ch04)) to include only
    variables that have predictive value, and/or by performing dimension reduction.
    You will learn about a number of specific dimension-reduction algorithms in [part
    4](kindle_split_024.html#part04) of this book, but discriminant analysis actually
    performs dimension reduction as part of its learning procedure.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是说变量越多就越不好！对于大多数问题，添加具有有价值信息的预测变量可以提高模型的预测精度……直到它不再这样做（直到我们得到递减的回报）。那么我们如何防止由于维度诅咒导致的过拟合？通过执行特征选择（正如我们在[第4章](kindle_split_014.html#ch04)中所做的那样），只包括具有预测价值的变量，以及/或者通过执行降维。你将在本书的[第4部分](kindle_split_024.html#part04)中了解到许多具体的降维算法，但判别分析实际上在它的学习过程中执行降维。
- en: '|  |'
  id: totrans-755
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-756
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: The phenomenon of the predictive power of a model increasing as the number of
    predictor variables increases, but then decreasing again as we continue to add
    more predictors, is called the *Hughes phenomenon*, after the statistician G.
    Hughes.
  id: totrans-757
  prefs: []
  type: TYPE_NORMAL
  zh: 当预测变量的数量增加时，模型的预测能力会增加，但当我们继续添加更多预测变量时，这种增加又会再次减少，这种现象被称为*休斯现象*，以统计学家G. Hughes的名字命名。
- en: '|  |'
  id: totrans-758
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Discriminant analysis isn’t one algorithm but instead comes in many flavors.
    I’m going to teach you the two most fundamental and commonly used algorithms:'
  id: totrans-759
  prefs: []
  type: TYPE_NORMAL
  zh: 判别分析不是一个算法，而是有许多不同的版本。我将教你两种最基本且最常用的算法：
- en: Linear discriminant analysis (LDA)
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性判别分析（LDA）
- en: Quadratic discriminant analysis (QDA)
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二次判别分析（QDA）
- en: In the next section, you’ll learn how these algorithms work and how they differ.
    For now, suffice it to say that LDA and QDA learn linear (straight-line) and curved
    decision boundaries between classes, respectively.
  id: totrans-762
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，你将了解这些算法是如何工作的以及它们之间的区别。现在，只需说LDA和QDA分别学习线性（直线）和曲线决策边界之间的类别即可。
- en: 5.1.1\. How does discriminant analysis learn?
  id: totrans-763
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.1\. 如何进行判别分析的学习？
- en: 'I’ll start by explaining how LDA works, and then I’ll generalize this to QDA.
    Imagine that we have two predictor variables we are trying to use to separate
    two classes in our data (see [figure 5.3](#ch05fig03)). LDA aims to learn a new
    representation of the data that separates the *centroid* of each class, while
    keeping the within-class variance as low as possible. A centroid is simply the
    point in the feature space that is the mean of all the predictors (a vector of
    means, one for each dimension). Then LDA finds a line through the origin that,
    when the data is *projected* onto it, simultaneously does the following:'
  id: totrans-764
  prefs: []
  type: TYPE_NORMAL
  zh: 我将从解释LDA的工作原理开始，然后将其推广到QDA。想象一下，我们有两个预测变量，我们试图使用这些变量来分离数据中的两个类别（参见[图5.3](#ch05fig03)）。LDA的目标是学习数据的新表示，该表示可以分离每个类别的*质心*，同时尽可能保持类内方差最低。质心简单地是特征空间中所有预测变量的平均值（一个均值向量，每个维度一个）。然后LDA找到一条通过原点的线，当数据被*投影*到这条线上时，它同时完成以下操作：
- en: Maximizes the difference between the class centroids along the line
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着线最大化类中心之间的差异
- en: Minimizes the within-class variance along the line
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 沿着线最小化类内方差
- en: 'To choose this line, the algorithm maximizes the expression in [equation 5.1](#ch05equ01)
    over all possible axes:'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 为了选择这条线，算法在所有可能的轴上最大化[方程5.1](#ch05equ01)中的表达式：
- en: equation 5.1\.
  id: totrans-768
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 5.1。
- en: '![](eq5-1.jpg)'
  id: totrans-769
  prefs: []
  type: TYPE_IMG
  zh: '![eq5-1.jpg](eq5-1.jpg)'
- en: The numerator is the difference between the class means (![](pg119-3.jpg) and
    ![](pg119-4.jpg) for the means of class 1 and class 2, respectively), squared
    to ensure that the value is positive (because we don’t know which will be bigger).
    The denominator is the sum of variances of each class along the line (![](pg119-1.jpg)
    and ![](pg119-2.jpg) for the variances of class 1 and class 2, respectively).
    The intuition behind this is that we want the means of the classes to be as separated
    as possible, with the scatter/variance within each class to be as small as possible.
  id: totrans-770
  prefs: []
  type: TYPE_NORMAL
  zh: 分子是类别均值之间的差异（![](pg119-3.jpg) 和 ![](pg119-4.jpg) 分别为类别1和类别2的均值），平方以确保值为正（因为我们不知道哪个会更大）。分母是每个类别沿线的方差之和（![](pg119-1.jpg)
    和 ![](pg119-2.jpg) 分别为类别1和类别2的方差）。这种直觉背后的想法是我们希望类别的均值尽可能分离，每个类别内的散点/方差尽可能小。
- en: Figure 5.3\. Learning a discriminant function in two dimensions. LDA learns
    a new axis such that, when the data is projected onto it (dashed lines), it maximizes
    the difference between class means while minimizing intra-class variance. ![](xmacr.jpg)
    and *s*² are the mean and variance of each class along the new axis, respectively.
  id: totrans-771
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.3. 在二维中学习判别函数。LDA学习一个新轴，使得当数据投影到它上（虚线）时，它最大化类别均值之间的差异，同时最小化类内方差。![](xmacr.jpg)
    和 *s*² 分别是每个类别沿新轴的均值和方差。
- en: '![](fig5-3_alt.jpg)'
  id: totrans-772
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-3_alt.jpg)'
- en: Figure 5.4\. Constructing a new axis that only maximizes class centroid separation
    doesn’t fully resolve the classes (left example). Constructing a new axis that
    maximizes centroid separation while also minimizing variance within each class
    results in better separation of the classes (right). ![](xmacr.jpg) and *s*² are
    the mean and variance of each class along the new axis, respectively.
  id: totrans-773
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.4. 仅通过构建一个最大化类别质心分离的新轴并不能完全解决类别问题（左侧示例）。构建一个同时最大化质心分离并最小化每个类别内方差的轴，可以更好地分离类别（右侧）。![](xmacr.jpg)
    和 *s*² 分别是每个类别沿新轴的均值和方差。
- en: '![](fig5-4.jpg)'
  id: totrans-774
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-4.jpg)'
- en: Why not simply find the line that maximizes the separation of the centroids?
    Because the line that best separates the centroids doesn’t guarantee the best
    separation of the cases in the different classes. This is illustrated in [figure
    5.4](#ch05fig04). In the example on the left, a new axis is drawn that simply
    maximizes the separation of the centroids of the two classes. When we project
    the data onto this new axis, the classes are not fully resolved because the relatively
    high variance means they overlap with each other. In the example on the right,
    however, the new axis tries to maximize centroid separation while minimizing the
    variance of each class along that axis. This results in centroids that are slightly
    closer together, but much smaller variances, such that the cases from the two
    classes are fully separated.
  id: totrans-775
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么不简单地找到最大化质心分离的线呢？因为最佳分离质心的线并不能保证在不同类别中案例的最佳分离。这可以在[图5.4](#ch05fig04)中看到。在左侧的示例中，画出一个新轴，它仅仅最大化两个类别的质心分离。当我们把数据投影到这个新轴上时，由于相对较高的方差，类别并没有完全分离。然而，在右侧的示例中，新轴试图在最大化质心分离的同时，最小化每个类别沿该轴的方差。这导致质心稍微靠近，但方差却小得多，从而使得两个类别的案例完全分离。
- en: 'This new axis is called a *discriminant function*, and it is a linear combination
    of the original variables. For example, a discriminant function could be described
    by this equation:'
  id: totrans-776
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新轴被称为**判别函数**，它是原始变量的线性组合。例如，一个判别函数可以由以下方程描述：
- en: '*DF = –0.5* × *var[1] + 1.2* × *var[2] + 0.85* × *var[3]*'
  id: totrans-777
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*DF = –0.5* × *var[1] + 1.2* × *var[2] + 0.85* × *var[3]*'
- en: In this way, the discriminant function (DF) in this equation is a linear combination
    of variables *var*[1], *var*[2] and *var*[3]. The combination is linear because
    we are simply adding together the contributions from each variable. The values
    that each variable is multiplied by are called the *canonical discriminant function
    coefficients* and weight each variable by how much it contributes to class separation.
    In other words, variables that contribute most to class separation will have larger
    absolute canonical DF coefficients (positive or negative). Variables that contain
    little or no class-separation information will have canonical DF coefficients
    closer to zero.
  id: totrans-778
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，该方程中的判别函数（DF）是变量*var*[1]、*var*[2]和*var*[3]的线性组合。这种组合是线性的，因为我们只是将每个变量的贡献相加。每个变量乘以的值被称为*典型判别函数系数*，并按每个变量对类别分离的贡献程度对其进行加权。换句话说，对类别分离贡献最大的变量将具有较大的绝对典型判别函数系数（正或负）。包含少量或没有类别分离信息的变量将具有接近零的典型判别函数系数。
- en: '|  |'
  id: totrans-779
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Linear discriminant analysis vs. principal component analysis**'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性判别分析 vs. 主成分分析**'
- en: If you’ve come across principal component analysis (PCA) before, you might be
    wondering how it differs from linear discriminant analysis (LDA). PCA is an unsupervised
    learning algorithm for dimension reduction, meaning that, unlike LDA, it doesn’t
    rely on labeled data.
  id: totrans-781
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你之前遇到过主成分分析（PCA），你可能想知道它与线性判别分析（LDA）有何不同。PCA是一种无监督的降维学习算法，这意味着，与LDA不同，它不依赖于标记数据。
- en: While both algorithms can be used to reduce the dimensionality of the dataset,
    they do so in different ways and to achieve different goals. Whereas LDA creates
    new axes that maximize class separation, so that we can classify new data using
    these new axes, PCA creates new axes that maximize the variance of the data projected
    onto them. Rather than classification, the goal of PCA is to explain as much of
    the variation and information in the data as possible, using only a small number
    of new axes. This new, lower-dimensional representation can then be fed into other
    machine learning algorithms. (If you’re unfamiliar with PCA, don’t worry! You’ll
    learn about it in depth in [chapter 13](kindle_split_025.html#ch13).)
  id: totrans-782
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这两个算法都可以用来降低数据集的维度，但它们以不同的方式这样做，以达到不同的目标。LDA创建新的轴，最大化类别分离，这样我们就可以使用这些新轴来对新数据进行分类，而PCA创建新的轴，最大化投影到这些轴上的数据的方差。PCA的目标不是分类，而是尽可能多地解释数据中的变异和信息，只使用少量新轴。然后，这种新的、低维度的表示可以输入到其他机器学习算法中。（如果你对PCA不熟悉，不要担心！你将在第13章中深入了解它。）
- en: If you want to reduce the dimensionality of data with labeled class membership,
    you should typically favor LDA over PCA. If you want to reduce the dimensionality
    of unlabeled data, you should favor PCA (or one of the many other dimension-reduction
    algorithms we’ll discuss in [part 4](kindle_split_024.html#part04) of the book).
  id: totrans-783
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想要降低具有标记类别成员资格的数据的维度，你应该通常优先考虑LDA而不是PCA。如果你想要降低未标记数据的维度，你应该优先考虑PCA（或书中第4部分[part
    4](kindle_split_024.html#part04)中我们将讨论的许多其他降维算法）。
- en: '|  |'
  id: totrans-784
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.1.2\. What if we have more than two classes?
  id: totrans-785
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.2\. 如果我们有超过两个类别怎么办？
- en: Discriminant analysis can handle classification problems with more than two
    classes. But how does it learn the best axis in this situation? Instead of trying
    to maximize the separation between class centroids, it maximizes the separation
    between each class centroid and the *grand centroid* of the data (the centroid
    of all the data, ignoring class membership). This is illustrated in [figure 5.5](#ch05fig05),
    where we have two continuous measurements made on cases from three classes. The
    class centroids are shown with triangles, and the grand centroid is indicated
    by a cross.
  id: totrans-786
  prefs: []
  type: TYPE_NORMAL
  zh: 判别分析可以处理多于两个类别的分类问题。但它是如何在这种情况下学习最佳轴的呢？它不是试图最大化类别质心的分离，而是最大化每个类别质心与数据的*总体质心*（忽略类别成员资格的所有数据的质心）之间的分离。这在[图5.5](#ch05fig05)中得到了说明，其中我们对来自三个类别的案例进行了两个连续的测量。类别质心用三角形表示，总体质心用十字表示。
- en: Figure 5.5\. When there are more than two classes, LDA maximizes the distance
    between each class centroid (triangles) and the grand centroid (cross) while minimizing
    intra-class variance. Once the first discriminant function is found, a second
    is constructed that is orthogonal to it. The original data can be plotted against
    these functions.
  id: totrans-787
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.5\. 当类别超过两个时，LDA最大化每个类别质心（三角形）与总体质心（交叉）之间的距离，同时最小化类内方差。一旦找到第一个判别函数，就构建第二个与它正交的函数。原始数据可以与这些函数进行绘图。
- en: '![](fig5-5_alt.jpg)'
  id: totrans-788
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-5_alt.jpg)'
- en: LDA first finds the axis that best separates the class centroids from the grand
    centroid that minimizes the variance of each class along it. Then, LDA constructs
    a second DF that is *orthogonal* to the first. This simply means the second DF
    must be perpendicular to the first (at a right angle in this 2D example).
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: LDA首先找到最佳分离类别质心与总体质心的轴，同时最小化沿其的每个类别的方差。然后，LDA构建第二个与第一个正交的DF。这仅仅意味着第二个DF必须与第一个垂直（在这个2D例子中是直角）。
- en: '|  |'
  id: totrans-790
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-791
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'The number of DFs will be whichever is smaller: (number of classes) minus 1,
    or the number of predictor variables.'
  id: totrans-792
  prefs: []
  type: TYPE_NORMAL
  zh: DF的数量将是以下两者中较小的一个：（类别数）减1，或预测变量数。
- en: '|  |'
  id: totrans-793
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: The data is then projected onto these new axes such that each case gets a *discriminant
    score* for each function (its value along the new axis). These discriminant scores
    can be plotted against each other to form a new representation of the original
    data.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，数据被投影到这些新轴上，使得每个案例对每个函数都有一个*判别得分*（它在新轴上的值）。这些判别得分可以相互绘制，以形成原始数据的新表示。
- en: But what’s the big deal? We’ve gone from having two predictor variables to having
    . . . two predictor variables! In fact, can you see that all we’ve done is center
    and scale the data, and rotate it around zero? When we only have two variables,
    discriminant analysis cannot perform any dimension reduction because the number
    of DFs is the smaller of the number of classes minus 1 and the number of variables
    (and we only have two variables).
  id: totrans-795
  prefs: []
  type: TYPE_NORMAL
  zh: 但这有什么大不了的？我们从一个预测变量变成了……两个预测变量！实际上，你能看到我们所做的只是将数据居中和缩放，并围绕零旋转？当我们只有两个变量时，判别分析无法执行任何降维，因为DF的数量是类别数减1和变量数中的较小者（我们只有两个变量）。
- en: But what about when we have more than two predictor variables? [Figure 5.6](#ch05fig06)
    shows an example where we have three predictor variables (*x*, *y*, and *z*) and
    three classes. Just as in [figure 5.5](#ch05fig05), LDA finds the DF that maximizes
    the separation between each class centroid and the grand centroid, while minimizing
    the variance along it. This line extends through a three-dimensional space.
  id: totrans-796
  prefs: []
  type: TYPE_NORMAL
  zh: 但当我们有超过两个预测变量时怎么办？[图5.6](#ch05fig06)展示了有一个三个预测变量（*x*、*y*和*z*）和三个类别的例子。就像在[图5.5](#ch05fig05)中一样，LDA找到最大化每个类别质心与总体质心之间分离度的DF，同时最小化沿其的方差。这条线穿过三维空间。
- en: Figure 5.6\. When there are more than two predictors, the cube represents a
    feature space with three predictor variables (*x*, *y*, and *z*) and three classes
    (dotted lines help indicate the position of each case along the *z*-axis). Discriminant
    function 1 (DF1) is found, and then DF2, which is orthogonal to DF1, is found.
    Dotted lines indicate “shadows” of DF1 and DF2 to help show their depth along
    the z-axis. The data can be projected onto DF1 and DF2.
  id: totrans-797
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.6\. 当预测变量超过两个时，立方体代表一个包含三个预测变量（*x*、*y*和*z*）和三个类别（虚线有助于指示每个案例沿*z*-轴的位置）的特征空间。找到了判别函数1（DF1），然后找到与DF1正交的DF2。虚线表示DF1和DF2在z轴上的“阴影”，有助于显示它们的深度。数据可以被投影到DF1和DF2上。
- en: '![](fig5-6_alt.jpg)'
  id: totrans-798
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-6_alt.jpg)'
- en: Next, LDA finds the second DF (which is orthogonal to the first), which also
    tries to maximize separation while minimizing variance. Because we only have three
    classes (and the number of DFs is the smaller of the number of classes minus 1
    or the number of predictors), we stop at two DFs. By taking the discriminant scores
    of each case in the data (the values of each case along the two DFs), we can plot
    our data in only two dimensions.
  id: totrans-799
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，LDA找到第二个DF（与第一个正交），它也试图最大化分离度同时最小化方差。因为我们只有三个类别（且DF的数量是类别数减1和预测变量数中的较小者），所以我们停在两个DF上。通过取数据中每个案例的判别得分（每个案例沿两个DF的值），我们可以在仅两个维度上绘制我们的数据。
- en: '|  |'
  id: totrans-800
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-801
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The first DF always does the best job at separating the classes, followed by
    the second, the third, and so on.
  id: totrans-802
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个DF总是做得最好，其次是第二个、第三个，依此类推。
- en: '|  |'
  id: totrans-803
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: LDA has taken a three-dimensional dataset and combined the information in those
    three variables into two new variables that maximize the separation between the
    classes. That’s pretty cool—but if instead of just three predictor variables we
    had 1,000 (as in the example I used earlier), LDA would *still* condense all this
    information into only 2 variables! That’s super cool.
  id: totrans-804
  prefs: []
  type: TYPE_NORMAL
  zh: LDA已经将一个三维数据集合并，将这三个变量中的信息合并成两个新的变量，这些变量最大化了类别的分离。这很酷——但如果不是只有三个预测变量，而是有1,000个（如我之前使用的示例中所示），LDA仍然会将所有这些信息压缩成只有2个变量！这太酷了。
- en: '5.1.3\. Learning curves instead of straight lines: QDA'
  id: totrans-805
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.3\. 用曲线而不是直线来学习：QDA
- en: LDA performs well if the data within each class is normally distributed across
    all the predictor variables, and the classes have similar *covariances*. Covariance
    simply means how much one variable increases/decreases when another variable increases/decreases.
    So LDA assumes that for each class in the dataset, the predictor variables *covary*
    with each other the same amount.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 当每个类别的数据在所有预测变量上都是正态分布，并且类别具有相似的*协方差*时，LDA表现良好。协方差简单地说就是当一个变量增加/减少时，另一个变量增加/减少的程度。因此，LDA假设对于数据集中的每个类别，预测变量之间的协方差是相同的。
- en: This often isn’t the case, and classes have different covariances. In this situation,
    QDA tends to perform better than LDA because it doesn’t make this assumption (though
    it still assumes the data is normally distributed). Instead of learning straight
    lines that separate the classes, QDA learns curved lines. It is also well suited,
    therefore, to situations in which classes are best separated by a nonlinear decision
    boundary. This is illustrated in [figure 5.7](#ch05fig07).
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常并不是情况，不同的类别有不同的协方差。在这种情况下，QDA往往比LDA表现更好，因为它不做出这个假设（尽管它仍然假设数据是正态分布的）。QDA不是学习直线来分隔类别，而是学习曲线。因此，它也非常适合那些类别最好通过非线性决策边界来分隔的情况。这可以在[图5.7](#ch05fig07)中看到。
- en: Figure 5.7\. Examples of two classes which have equal covariance (the relationship
    between variable 1 and 2 is the same for both classes) and different covariances.
    Ovals represent distributions of data within each class. Quadratic and linear
    DFs (QDF and LDF) are shown. The projection of the classes with different covariances
    onto each DF is shown.
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图5.7\. 具有相同协方差（变量1和2之间的关系对于两个类别都是相同的）和不同协方差的两个类别的示例。椭圆形代表每个类别内的数据分布。展示了二次和线性DFs（QDF和LDF）。展示了具有不同协方差类别的每个DF上的投影。
- en: '![](fig5-7_alt.jpg)'
  id: totrans-809
  prefs: []
  type: TYPE_IMG
  zh: '![图5-7](fig5-7_alt.jpg)'
- en: In the example on the left in the figure, the two classes are normally distributed
    across both variables and have equal covariances. We can see that the covariances
    are equal because, for both classes, as variable 1 increases, variable 2 decreases
    by the same amount. In this situation, LDA and QDA will find similar DFs, although
    LDA is slightly less prone to overfitting than QDA because it is less flexible.
  id: totrans-810
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中的左侧示例中，两个类别在两个变量上都是正态分布的，并且具有相同的协方差。我们可以看到协方差是相等的，因为对于两个类别，当变量1增加时，变量2以相同的量减少。在这种情况下，LDA和QDA将找到相似的DFs，尽管LDA比QDA更不容易过拟合，因为它不太灵活。
- en: In the example on the right in the figure, the two classes are normally distributed,
    but their covariances are different. In this situation, QDA will find a curved
    DF that, when the data is projected onto it, will tend to do a better job of separating
    the classes than a linear DF.
  id: totrans-811
  prefs: []
  type: TYPE_NORMAL
  zh: 在图中的右侧示例中，两个类别都是正态分布的，但它们的协方差不同。在这种情况下，QDA将找到一个曲线DF，当数据投影到它上面时，将比线性DF更好地分隔类别。
- en: 5.1.4\. How do LDA and QDA make predictions?
  id: totrans-812
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.1.4\. LDA和QDA如何进行预测？
- en: Whichever method you’ve chosen, the DFs have been constructed, and you’ve reduced
    your high-dimensional data into a small number of discriminants. How do LDA and
    QDA use this information to classify new observations? They use an extremely important
    statistical theorem called *Bayes’ rule*.
  id: totrans-813
  prefs: []
  type: TYPE_NORMAL
  zh: 无论你选择了哪种方法，DFs都已经构建，你已经将你的高维数据减少到少数几个判别变量。LDA和QDA如何使用这些信息来对新观测进行分类？它们使用一个极其重要的统计定理，称为*贝叶斯定理*。
- en: 'Bayes’ rule provides us with a way of answering the following question: given
    the values of the predictor variables for any case in our data, what is the probability
    of that case belonging to class k? This is written as *p(k|x)*, where *k* represents
    membership in class k, and *x* represents the values of the predictor variables.
    We would read this as “the probability of belonging to class k, given the data,
    *x*.” This is given by Bayes’ rule:'
  id: totrans-814
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理为我们提供了一种回答以下问题的方法：给定数据中任何案例的预测变量值，该案例属于类别 k 的概率是多少？这表示为 *p(k|x)*，其中 *k*
    代表属于类别 k 的成员资格，而 *x* 代表预测变量的值。我们会读作“在数据 *x* 的条件下，属于类别 k 的概率。”这是由贝叶斯定理给出的：
- en: equation 5.2\.
  id: totrans-815
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 5.2。
- en: '![](eq5-2.jpg)'
  id: totrans-816
  prefs: []
  type: TYPE_IMG
  zh: '![](eq5-2.jpg)'
- en: Don’t be scared by this! There are only four terms in the equation, and I’m
    going to walk you through them. You already know *p(k|x)* is the probability of
    a case belonging to class k given the data. This is called the *posterior probability*.
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 不要被这个吓到！方程中只有四个术语，我将带你逐一了解它们。你已经知道 *p(k|x)* 是在给定数据的情况下，一个案例属于类别 k 的概率。这被称为 *后验概率*。
- en: '*p(x|k)* is the same thing, but flipped around: what is the probability of
    observing this data, given the case belongs to class k? Put another way: if this
    case *was* in class k, what is the *likelihood* of it having these values of the
    predictor variables? This is called the *likelihood*.'
  id: totrans-818
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(x|k)* 与此相同，但方向相反：在案例属于类别 k 的情况下，观察这些数据点的概率是什么？换句话说：如果这个案例 *确实* 在类别 k 中，那么它具有这些预测变量值的
    *似然性* 是多少？这被称为 *似然性*。'
- en: '*p(k)* is called the *prior probability* and is simply the probability of any
    case belonging to class k. This is the proportion of all cases in the data that
    belong to class *k*. For example, if 30% of cases were in class k, *p(k)* would
    equal 0.3.'
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: '*p(k)* 被称为 *先验概率*，它简单地表示任何案例属于类别 k 的概率。这是数据中属于类别 *k* 的所有案例的比例。例如，如果 30% 的案例属于类别
    k，那么 *p(k)* 等于 0.3。'
- en: Finally, *p(x)* is the probability of observing a case with exactly these predictor
    values in the dataset. This is called the *evidence*. Estimating the evidence
    is often very difficult (because each case in the dataset may have a unique combination
    of predictor values), and it only serves to make all the posterior probabilities
    sum to 1\. Therefore, we can omit the evidence from the equation and say that
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，*p(x)* 是在数据集中观察到一个具有这些预测变量值的案例的概率。这被称为 *证据*。估计证据通常非常困难（因为数据集中的每个案例可能都有独特的预测变量值的组合），它只用于确保所有后验概率之和为
    1。因此，我们可以从方程中省略证据，并说
- en: equation 5.3\.
  id: totrans-821
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 方程 5.3。
- en: '![](eq5-3.jpg)'
  id: totrans-822
  prefs: []
  type: TYPE_IMG
  zh: '![](eq5-3.jpg)'
- en: where the ∝ symbol means the values on either side of it are *proportional*
    to each other instead of *equal* to each other. In a more digestible way,
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 ∝ 符号表示其两侧的值是 *成比例的*，而不是 *相等的*。以更易于理解的方式，
- en: '*posterior* ∝ *likelihood* × *prior*'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后验* ∝ *似然* × *先验*'
- en: 'The prior probability for a case (*p(k)*) is easy to work out: it’s the proportion
    of cases in the dataset that belong to class *k*. But how do we calculate the
    likelihood (*p(x|k)*)? The likelihood is calculated by projecting the data onto
    its DFs and estimating its *probability density*. The probability density is the
    relative probability of observing a case with a particular combination of discriminant
    scores.'
  id: totrans-825
  prefs: []
  type: TYPE_NORMAL
  zh: 案例的先验概率 (*p(k)*) 很容易计算：它是数据集中属于类别 *k* 的案例的比例。但我们如何计算似然 (*p(x|k)*) 呢？似然是通过将数据投影到其
    DF 上并估计其 *概率密度* 来计算的。概率密度是观察具有特定判别分数组合的案例的相对概率。
- en: Discriminant analysis assumes that the data is normally distributed, so it estimates
    the probability density by fitting a normal distribution to each class across
    each DF. The center of each normal distribution is the class centroid, and its
    standard deviation is one unit on the discriminant axis. This is illustrated in
    [figure 5.8](#ch05fig08) for a single DF and for two DFs (the same thing happens
    in more than two dimensions but is difficult to visualize). You can see that cases
    near the class centroid along the discriminant axes have a high probability density
    for that class, and cases far away have a lower probability density.
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 判别分析假设数据是正态分布的，因此它通过将每个类别拟合到每个 DF 上来估计概率密度。每个正态分布的中心是类别质心，其标准差是判别轴上的一个单位。这在
    [图 5.8](#ch05fig08) 中对单个 DF 和两个 DF 进行了说明（在超过两个维度的情况下也会发生相同的事情，但难以可视化）。你可以看到，沿着判别轴靠近类别质心的案例具有该类的高概率密度，而远离的案例具有较低的概率密度。
- en: Figure 5.8\. The probability density of each class is assumed to be normally
    distributed, where the center of each distribution is the centroid of the class.
    This is shown for one DF (for classes k and j) and for two.
  id: totrans-827
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.8\. 每个类别的概率密度假设为正态分布，其中每个分布的中心是类别的质心。这在一个 DF（对于类别 k 和 j）以及两个 DF 中显示。
- en: '![](fig5-8_alt.jpg)'
  id: totrans-828
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-8_alt.jpg)'
- en: 'Once the probability density is estimated for a case for a given class, it
    can be passed into the equation:'
  id: totrans-829
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦为给定类别的案例估计了概率密度，就可以将其传递到方程中：
- en: '*posterior* = *likelihood* × *prior*'
  id: totrans-830
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后验概率* = *似然率* × *先验概率*'
- en: The posterior probability is estimated for each class, and the class that has
    the highest probability is what the case is classified as.
  id: totrans-831
  prefs: []
  type: TYPE_NORMAL
  zh: 为每个类别估计后验概率，并且具有最高概率的类别就是案例被分类为的类别。
- en: '|  |'
  id: totrans-832
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-833
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The prior probability (proportion of cases in that class) is important because
    if the classes are severely imbalanced, despite a case being far from the centroid
    of a class, the case could be more likely to belong to that class simply because
    there are so many more cases in it.
  id: totrans-834
  prefs: []
  type: TYPE_NORMAL
  zh: 前验概率（该类别的案例比例）很重要，因为如果类别严重不平衡，尽管案例离类别的质心很远，但由于该类别中有许多更多的案例，案例更有可能属于该类别。
- en: '|  |'
  id: totrans-835
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Bayes’ rule is very important in statistics and machine learning. Don’t worry
    if you don’t quite understand it yet; that’s by design. I want to introduce you
    to it gently now, and we’ll cover it in more depth in [chapter 6](kindle_split_016.html#ch06).
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 贝叶斯定理在统计学和机器学习中非常重要。如果你现在还不完全理解它，请不要担心；这是故意的。我现在想轻轻地介绍它，我们将在第 6 章（[kindle_split_016.html#ch06](https://kindle_split_016.html#ch06)）中更深入地探讨它。
- en: 5.2\. Building your first linear and quadratic discriminant models
  id: totrans-837
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2\. 构建你的第一个线性判别分析和二次判别分析模型
- en: 'Now that you know how discriminant analysis works, you’re going to build your
    first LDA model. If you haven’t already, load the mlr and tidyverse packages:'
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道了判别分析是如何工作的，你将构建你的第一个 LDA 模型。如果你还没有做的话，请加载 mlr 和 tidyverse 包：
- en: '[PRE63]'
  id: totrans-839
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 5.2.1\. Loading and exploring the wine dataset
  id: totrans-840
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.1\. 加载和探索葡萄酒数据集
- en: In this section, you’ll learn how to build and evaluate the performance of linear
    and quadratic discriminant analysis models. Imagine that you’re a detective in
    a murder mystery. A local wine producer, Ronald Fisher, was poisoned at a dinner
    party when someone replaced the wine in the carafe with wine poisoned with arsenic.
  id: totrans-841
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何构建和评估线性判别分析和二次判别分析模型的表现。想象一下，你是一名侦探，正在一个谋杀谜案中。一位当地葡萄酒生产商，罗纳德·费希尔，在一次晚宴上被人用含有砒霜的葡萄酒替换了酒壶中的葡萄酒而被毒害。
- en: Three other (rival) wine producers were at the party and are your prime suspects.
    If you can trace the wine to one of those three vineyards, you’ll find your murderer.
    As luck would have it, you have access to some previous chemical analysis of the
    wines from each of the vineyards, and you order an analysis of the poisoned carafe
    at the scene of the crime. Your task is to build a model that will tell you which
    vineyard the wine with the arsenic came from and, therefore, the guilty party.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 三位其他（竞争对手）葡萄酒生产商也在晚宴上，他们是你的主要嫌疑人。如果你能追踪到这些葡萄园之一，你就能找到凶手。幸运的是，你能够访问到来自每个葡萄园的葡萄酒的一些之前的化学分析，并且你要求对犯罪现场中的有毒酒壶进行分析。你的任务是构建一个模型，告诉你含有砒霜的葡萄酒来自哪个葡萄园，因此找出罪犯。
- en: Let’s load the wine data built into the HDclassif package (after installing
    it), convert it into a tibble, and explore it a little. We have a tibble containing
    178 cases and 14 variables of measurements made on various wine bottles.
  id: totrans-843
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载 HDclassif 包中内置的葡萄酒数据，将其转换为 tibble，并对其进行一些探索。我们有一个包含 178 个案例和 14 个变量（对各种葡萄酒瓶进行的测量）的
    tibble。
- en: Listing 5.1\. Loading and exploring the wine dataset
  id: totrans-844
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.1\. 加载和探索葡萄酒数据集
- en: '[PRE64]'
  id: totrans-845
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: Often, as data scientists, we receive data that is messy or not well curated.
    In this case, the names of the variables are missing! We could continue working
    with `V1`, `V2`, and so on, but it would be hard to keep track of which variable
    is which. So we’re going to manually add the variable names. Who said the life
    of a data scientist was glamorous? Then, we’ll convert the `class` variable to
    a factor.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，作为数据科学家，我们接收到的数据可能是杂乱无章的或者整理得不够好。在这种情况下，变量的名称缺失！我们可以继续使用 `V1`、`V2` 等等，但很难追踪哪个变量是哪个。因此，我们将手动添加变量名称。谁说数据科学家的生活是光鲜亮丽的？然后，我们将
    `class` 变量转换为因子。
- en: Listing 5.2\. Cleaning the dataset
  id: totrans-847
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.2\. 清洗数据集
- en: '[PRE65]'
  id: totrans-848
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: That’s much better. We can see that we have 13 continuous measurements made
    on 178 bottles of wine, where each measurement is the amount of a different compound/element
    in the wine. We also have a single categorical variable, `Class`, which tells
    us which vineyard the bottle comes from.
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 这好多了。我们可以看到，我们对 178 瓶葡萄酒进行了 13 次连续测量，其中每次测量都是葡萄酒中不同化合物/元素的含量。我们还有一个单个的分类变量 `Class`，它告诉我们这瓶酒来自哪个葡萄园。
- en: '|  |'
  id: totrans-850
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-851
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Lots of people consider it good form to keep variable names lowercase. I don’t
    mind so much so long as my style is consistent. Therefore, notice that I changed
    the name of the grouping variable `class` to `Class`.
  id: totrans-852
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人认为保持变量名小写是一种好的做法。只要我的风格一致，我就不太介意。因此，请注意，我将分组变量 `class` 的名称更改为 `Class`。
- en: '|  |'
  id: totrans-853
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 5.2.2\. Plotting the data
  id: totrans-854
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.2\. 绘制数据
- en: Let’s plot the data to get an idea of how the compounds vary between the vineyards.
    As for the *Titanic* dataset in [chapter 4](kindle_split_014.html#ch04), we’re
    going to gather the data into an untidy format so we can facet by each of the
    variables.
  id: totrans-855
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制数据，以了解化合物在葡萄园之间的变化情况。至于 [第 4 章](kindle_split_014.html#ch04) 中的 *Titanic*
    数据集，我们将数据收集到一个杂乱格式中，这样我们就可以按每个变量进行分面。
- en: Listing 5.3\. Creating an untidy tibble for plotting
  id: totrans-856
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.3\. 创建用于绘图的杂乱 tibble
- en: '[PRE66]'
  id: totrans-857
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The resulting plot is shown in [figure 5.9](#ch05fig09).
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示在 [图 5.9](#ch05fig09) 中。
- en: Figure 5.9\. Box and whiskers plots of each continuous variable in the data
    against vineyard number. For the box and whiskers, the thick horizontal line represents
    the median, the box represents the interquartile range (IQR), the whiskers represent
    the Tukey range (1.5 times the IQR above and below the quartiles), and the dots
    represent data outside of the Tukey range.
  id: totrans-859
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.9\. 数据中每个连续变量与葡萄园编号的箱线图。对于箱线图，粗横线代表中位数，箱子代表四分位距（IQR），胡须代表 Tukey 范围（四分位数以上和以下
    1.5 倍的 IQR），点代表 Tukey 范围之外的数据。
- en: '![](fig5-9_alt.jpg)'
  id: totrans-860
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-9_alt.jpg)'
- en: A data scientist (and detective working the case) looking at this data would
    jump for joy! Look at how many obvious differences there are between wines from
    the three different vineyards. We should easily be able to build a well-performing
    classification model because the classes are so separable.
  id: totrans-861
  prefs: []
  type: TYPE_NORMAL
  zh: 一个数据科学家（以及正在处理这个案件的大侦探）看到这些数据会非常高兴！看看来自三个不同葡萄园的葡萄酒之间有多少明显的差异。由于类别非常可分，我们应该能够轻松构建一个表现良好的分类模型。
- en: 5.2.3\. Training the models
  id: totrans-862
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 5.2.3\. 训练模型
- en: Let’s define our task and learner, and build a model as usual. This time, we
    supply `"classif.lda"` as the argument to `makeLearner()` to specify that we’re
    going to use LDA.
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的任务和学习者，并像往常一样构建一个模型。这次，我们将 `"classif.lda"` 作为 `makeLearner()` 函数的参数，以指定我们将使用
    LDA。
- en: '|  |'
  id: totrans-864
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-865
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: LDA and QDA have no hyperparameters to tune and are therefore said to have a
    *closed-form solution*. In other words, all the information that LDA and QDA need
    is in the data. Their performance is also unaffected by variables on different
    scales. They will give the same result whether the data is scaled or not!
  id: totrans-866
  prefs: []
  type: TYPE_NORMAL
  zh: LDA 和 QDA 没有需要调整的超参数，因此它们被称为具有 *闭式解*。换句话说，LDA 和 QDA 所需要的信息都在数据中。它们的性能也不受不同尺度变量的影响。无论数据是否缩放，它们都会给出相同的结果！
- en: '|  |'
  id: totrans-867
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 5.4\. Creating the task and learner, and training the model
  id: totrans-868
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.4\. 创建任务和学习者，并训练模型
- en: '[PRE67]'
  id: totrans-869
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: '|  |'
  id: totrans-870
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-871
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 3](kindle_split_013.html#ch03) that the `makeClassifTask()`
    function warns us that our data is a tibble and not a pure `data.frame`. This
    warning can be safely ignored.
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [第 3 章](kindle_split_013.html#ch03)，`makeClassifTask()` 函数会警告我们的数据是 tibble
    而不是纯 `data.frame`。这个警告可以安全地忽略。
- en: '|  |'
  id: totrans-873
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Let’s extract the model information using the `getLearnerModel()` function,
    and get DF values for each case using the `predict()` function. By printing `head(ldaPreds)`,
    we can see that the model has learned two DFs, `LD1` and `LD2`, and that the `predict()`
    function has indeed returned the values for these functions for each case in our
    `wineTib` dataset.
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用 `getLearnerModel()` 函数提取模型信息，并使用 `predict()` 函数获取每个案例的 DF 值。通过打印 `head(ldaPreds)`，我们可以看到模型已经学习了两个
    DF，`LD1` 和 `LD2`，并且 `predict()` 函数确实为我们的 `wineTib` 数据集中的每个案例返回了这些函数的值。
- en: Listing 5.5\. Extracting DF values for each case
  id: totrans-875
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.5\. 提取每个案例的 DF 值
- en: '[PRE68]'
  id: totrans-876
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: To visualize how well these two learned DFs separate the bottles of wine from
    the three vineyards, let’s plot them against each other. We start by piping the
    `wineTib` dataset into a `mutate()` call where we create a new column for each
    of the DFs. We pipe this mutated tibble into a `ggplot()` call and set `LD1`,
    `LD2`, and `Class` as the x, y, and color aesthetics, respectively. Finally, we
    add a `geom_point()` layer to add dots, and a `stat_ellipse()` layer to draw 95%
    confidence ellipses around each class.
  id: totrans-877
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这两个学习到的 DF 如何将来自三个葡萄园的酒瓶分开，让我们将它们相互绘制。我们首先将 `wineTib` 数据集通过一个 `mutate()`
    调用，为每个 DF 创建一个新列。然后，我们将这个变异的 tibble 通过一个 `ggplot()` 调用，并将 `LD1`、`LD2` 和 `Class`
    分别设置为 x、y 和颜色美学。最后，我们添加一个 `geom_point()` 层来添加点，并添加一个 `stat_ellipse()` 层来在每个类别周围绘制
    95% 置信椭圆。
- en: Listing 5.6\. Plotting the DF values against each other
  id: totrans-878
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.6\. 将 DF 值相互绘制
- en: '[PRE69]'
  id: totrans-879
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: The resulting plot is shown in [figure 5.10](#ch05fig10).
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图示在 [图 5.10](#ch05fig10) 中。
- en: Figure 5.10\. Plotting the DFs against each other. The values for `LD1` and
    `LD2` for each case are plotted against each other, shaded by their class.
  id: totrans-881
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 5.10\. 将 DF 相互绘制。每个案例的 `LD1` 和 `LD2` 值相互绘制，并按其类别着色。
- en: '![](fig5-10_alt.jpg)'
  id: totrans-882
  prefs: []
  type: TYPE_IMG
  zh: '![](fig5-10_alt.jpg)'
- en: Looking good. Can you see that LDA has reduced our 13 predictor variables into
    just two DFs that do an excellent job of separating the wines from each of the
    vineyards?
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来不错。你能看到 LDA 将我们的 13 个预测变量减少到仅仅两个 DF，这两个 DF 在将葡萄酒与每个葡萄园分开方面做得非常出色吗？
- en: Next, let’s use exactly the same procedure to build a QDA model.
  id: totrans-884
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们使用完全相同的程序来构建一个 QDA 模型。
- en: Listing 5.7\. Plotting the DF values against each other
  id: totrans-885
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.7\. 将 DF 值相互绘制
- en: '[PRE70]'
  id: totrans-886
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '|  |'
  id: totrans-887
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-888
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注释
- en: Sadly, it isn’t easy to extract the DFs from the implementation of QDA that
    mlr uses, to plot them as we did for LDA.
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 很遗憾，从 mlr 实现的 QDA 中提取 DF 并绘制它们，就像我们对 LDA 所做的那样，并不容易。
- en: '|  |'
  id: totrans-890
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now, let’s cross-validate our LDA and QDA models together to estimate how they’ll
    perform on new data.
  id: totrans-891
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将我们的 LDA 和 QDA 模型一起进行交叉验证，以估计它们在新数据上的表现。
- en: Listing 5.8\. Cross-validating the LDA and QDA models
  id: totrans-892
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 5.8\. 交叉验证 LDA 和 QDA 模型
- en: '[PRE71]'
  id: totrans-893
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: 'Great! Our LDA model correctly classified 98.8% of wine bottles on average.
    There isn’t much room for improvement here, but our QDA model managed to correctly
    classify 99.2% of cases! Let’s also look at the confusion matrices (interpreting
    them is part of the chapter’s exercises):'
  id: totrans-894
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！我们的 LDA 模型平均正确分类了 98.8% 的酒瓶。这里几乎没有改进的余地，但我们的 QDA 模型设法正确分类了 99.2% 的案例！让我们也看看混淆矩阵（解释它们是本章练习的一部分）：
- en: '[PRE72]'
  id: totrans-895
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: 'Now, detective, the chemical analysis of the poisoned wine is in. Let’s use
    our QDA model to predict which vineyard it came from:'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，侦探，毒酒化学分析的结果出来了。让我们使用我们的 QDA 模型来预测它来自哪个葡萄园：
- en: '[PRE73]'
  id: totrans-897
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: The model predicts that the poisoned bottle came from vineyard 1\. Time to go
    and make an arrest!
  id: totrans-898
  prefs: []
  type: TYPE_NORMAL
  zh: 模型预测毒害的瓶子来自葡萄园 1。是时候去逮捕了！
- en: '|  |'
  id: totrans-899
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Ronald Fisher**'
  id: totrans-900
  prefs: []
  type: TYPE_NORMAL
  zh: '**罗纳德·费舍尔**'
- en: You may be happy to know that, in the real world, Ronald Fisher wasn’t poisoned
    at a dinner party. This is, perhaps, fortunate for you, because Sir Ronald Fisher
    (1890-1962) was a famous biostatistician who went on to be called the father of
    statistics. Fisher developed many statistical tools and concepts we use today,
    including discriminant analysis. In fact, linear discriminant analysis is commonly
    confused with *Fisher’s discriminant analysis*, the original form of discriminant
    analysis that Fisher developed (but which is slightly different).
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会很高兴地知道，在现实世界中，罗纳德·费舍尔并没有在晚宴上中毒。这可能对你来说是个幸运的事，因为罗纳德·费舍尔爵士（1890-1962）是一位著名的生物统计学家，后来被称为统计学的奠基人。费舍尔开发了今天我们使用的许多统计工具和概念，包括判别分析。事实上，线性判别分析通常与
    *费舍尔的判别分析* 混淆，这是费舍尔开发的判别分析的原型（但略有不同）。
- en: However, Fisher was also a proponent of eugenics, the belief that some races
    are superior to others. In fact, he shared his opinions in a 1952 UNESCO statement
    called “The Race Question,” in which he said that “the groups of mankind differ
    profoundly in their innate capacity for intellectual and emotional development”
    ([https://unesdoc.unesco.org/ark:/48223/pf0000073351](https://unesdoc.unesco.org/ark:/48223/pf0000073351)).
    Perhaps now you don’t feel so sorry for our murder mystery victim.
  id: totrans-902
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，费舍尔也是优生学的支持者，认为某些种族比其他种族优越。事实上，他在 1952 年的联合国教科文组织声明“种族问题”中分享了他的观点，他说“人类群体在先天的智力和情感发展能力上存在深刻差异”
    ([https://unesdoc.unesco.org/ark:/48223/pf0000073351](https://unesdoc.unesco.org/ark:/48223/pf0000073351))。也许现在你不再为我们的谋杀之谜受害者感到那么遗憾了。
- en: '|  |'
  id: totrans-903
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 5.3\. Strengths and weaknesses of LDA and QDA
  id: totrans-904
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.3. LDA和QDA的优点和缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    LDA and QDA will perform well for you.
  id: totrans-905
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪种算法会对给定的任务表现良好，但以下是一些优点和缺点，这将帮助您决定LDA和QDA是否适合您。
- en: 'The strengths of the LDA and QDA algorithms are as follows:'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: LDA和QDA算法的优点如下：
- en: They can reduce a high-dimensional feature space into a much more manageable
    number.
  id: totrans-907
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以将高维特征空间减少到更易于管理的数量。
- en: They can be used for classification or as a preprocessing (dimension reduction)
    technique for other classification algorithms that may perform better on the dataset.
  id: totrans-908
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以用作分类或作为其他可能在该数据集上表现更好的分类算法的预处理（降维）技术。
- en: QDA can learn curved decision boundaries between classes (this isn’t the case
    for LDA).
  id: totrans-909
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QDA可以学习类之间的曲线决策边界（LDA不是这种情况）。
- en: 'The weaknesses of the LDA and QDA algorithms are these:'
  id: totrans-910
  prefs: []
  type: TYPE_NORMAL
  zh: LDA和QDA算法的缺点如下：
- en: They can only handle continuous predictors (although recoding a categorical
    variable as numeric *may* help in some cases).
  id: totrans-911
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们只能处理连续预测因子（尽管在某些情况下将分类变量重新编码为数值可能有所帮助）。
- en: They assume the data is normally distributed across the predictors. If the data
    is not, performance will suffer.
  id: totrans-912
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们假设数据在预测因子上是正态分布的。如果数据不是，性能将受到影响。
- en: LDA can only learn linear decision boundaries between classes (this isn’t the
    case for QDA).
  id: totrans-913
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA只能学习类之间的线性决策边界（QDA不是这种情况）。
- en: LDA assumes equal covariances of the classes, and performance will suffer if
    this isn’t the case (this isn’t the case for QDA).
  id: totrans-914
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA假设类的协方差相等，如果这种情况不成立（对于QDA来说情况就是这样），性能将受到影响。
- en: QDA is more flexible than LDA and so can be more prone to overfitting.
  id: totrans-915
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QDA比LDA更灵活，因此更容易过拟合。
- en: '|  |'
  id: totrans-916
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-917
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习1**'
- en: Interpret the confusion matrices shown in the previous section.
  id: totrans-918
  prefs: []
  type: TYPE_NORMAL
  zh: 解释上一节中显示的混淆矩阵。
- en: Which model is better at identifying wines from vineyard 3?
  id: totrans-919
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型在识别3号葡萄园的酒方面更好？
- en: Does our LDA model misclassify more wines from vineyard 2 as being from vineyard
    1 or vineyard 3?
  id: totrans-920
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的LDA模型是否将更多来自2号葡萄园的酒误分类为来自1号或3号葡萄园？
- en: '|  |'
  id: totrans-921
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-922
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 2**'
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习2**'
- en: Extract the discriminant scores from our LDA model, and use only these as the
    predictors for a kNN model (including tuning *k*). Experiment with your own cross-validation
    strategy. Look back at [chapter 3](kindle_split_013.html#ch03) if you need a refresher
    on training a kNN model.
  id: totrans-924
  prefs: []
  type: TYPE_NORMAL
  zh: 从我们的LDA模型中提取判别得分，并仅使用这些作为kNN模型的预测因子（包括调整*k*）。尝试您自己的交叉验证策略。如果您需要复习如何训练kNN模型，请回顾[第3章](kindle_split_013.html#ch03)。
- en: '|  |'
  id: totrans-925
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-926
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Discriminant analysis is a supervised learning algorithm that projects the data
    onto a lower-dimensional representation to create discriminant functions.
  id: totrans-927
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别分析是一种监督学习算法，它将数据投影到低维表示以创建判别函数。
- en: Discriminant functions are linear combinations of the original (continuous)
    variables that maximize the separation of class centroids while minimizing the
    variance of each class along them.
  id: totrans-928
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别函数是原始（连续）变量的线性组合，它们最大化类质心的分离，同时最小化每个类沿其的方差。
- en: Discriminant analysis comes in many flavors, the most fundamental of which are
    LDA and QDA.
  id: totrans-929
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别分析有多种形式，其中最基本的是LDA和QDA。
- en: LDA learns linear decision boundaries between classes and assumes that classes
    are normally distributed and have equal covariances.
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: LDA学习类之间的线性决策边界，并假设类是正态分布的，并且具有相等的协方差。
- en: QDA can learn curved decision boundaries between classes and assumes that each
    class is normally distributed, but does *not* assume equal covariances.
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: QDA可以学习类之间的曲线决策边界，并假设每个类都是正态分布的，但**不**假设协方差相等。
- en: The number of discriminant functions is the smaller of the number of classes
    minus 1, or the number of predictor variables.
  id: totrans-932
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 判别函数的数量是类数减1和预测变量数中的较小者。
- en: Class prediction uses Bayes’ rule to estimate the posterior probability of a
    case belonging to each of the classes.
  id: totrans-933
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 类预测使用贝叶斯规则来估计案例属于每个类的后验概率。
- en: Solutions to exercises
  id: totrans-934
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习解答
- en: 'Interpret the confusion matrices:'
  id: totrans-935
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 解释混淆矩阵：
- en: Our QDA model is better at identifying wines from vineyard 3\. It misclassified
    12 as from vineyard 2, whereas the LDA model misclassified 23.
  id: totrans-936
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的QDA模型在识别3号葡萄园的酒方面表现更好。它将12号酒误分类为2号葡萄园，而LDA模型将23号酒误分类。
- en: Our LDA model misclassifies more cases from vineyard 2 as from vineyard 3 than
    as from vineyard 1.
  id: totrans-937
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们的LDA模型将来自葡萄园2的案例误分类为来自葡萄园3的案例，多于来自葡萄园1的案例。
- en: 'Use the discriminant scores from the LDA as predictors in a kNN model:'
  id: totrans-938
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将LDA的判别分数用作kNN模型的预测变量：
- en: '[PRE74]'
  id: totrans-939
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE74]'
- en: Chapter 6\. Classifying with naive Bayes and support vector machines
  id: totrans-940
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第6章\. 使用朴素贝叶斯和支持向量机进行分类
- en: '*This chapter covers*'
  id: totrans-941
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with the naive Bayes algorithm
  id: totrans-942
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯算法工作
- en: Understanding the support vector machine algorithm
  id: totrans-943
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解支持向量机算法
- en: Tuning many hyperparameters simultaneously with a random search
  id: totrans-944
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机搜索同时调整许多超参数
- en: 'The naive Bayes and support vector machine (SVM) algorithms are supervised
    learning algorithms for classification. Each algorithm learns in a different way.
    The naive Bayes algorithm uses *Bayes’ rule*, which you learned about in [chapter
    5](kindle_split_015.html#ch05), to estimate the probability of new data belonging
    to one of the classes in the dataset. The case is then assigned to the class with
    the highest probability. The SVM algorithm looks for a *hyperplane* (a surface
    that has one less dimension than there are predictor variables) that separates
    the classes. The position and direction of this hyperplane depend on *support
    vectors*: cases that lie closest to the boundary between the classes.'
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯和支持向量机（SVM）算法是用于分类的监督学习算法。每个算法以不同的方式学习。朴素贝叶斯算法使用*贝叶斯定理*，这是你在[第5章](kindle_split_015.html#ch05)中学到的，来估计新数据属于数据集中某一类别的概率。然后，将案例分配给概率最高的类别。SVM算法寻找一个*超平面*（一个维度比预测变量少的表面），将类别分开。这个超平面的位置和方向取决于*支持向量*：位于类别边界最近的案例。
- en: '|  |'
  id: totrans-946
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-947
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: While commonly used for classification, the SVM algorithm can also be used for
    regression problems. I won’t discuss how here, but if you’re interested (and want
    to explore SVMs in more depth generally), see *Support Vector Machines* by Andreas
    Christmann and Ingo Steinwart (Springer, 2008).
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SVM算法常用于分类，但它也可以用于回归问题。这里不会讨论这一点，但如果您感兴趣（并希望更深入地探索SVM），请参阅Andreas Christmann和Ingo
    Steinwart所著的《支持向量机》（Springer，2008年）。
- en: '|  |'
  id: totrans-949
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The naive Bayes and SVM algorithms have different properties that make each
    suitable in different circumstances. For example, naive Bayes can mix both continuous
    and categorical predictors natively, while for SVMs, categorical variables must
    first be recoded into a numerical format. On the other hand, SVMs are excellent
    at finding decision boundaries between classes that are not linearly separable,
    by adding a new dimension to the data that reveals a linear boundary. The naive
    Bayes algorithm will rarely outperform an SVM trained on the same problem, but
    naive Bayes tends to perform well for problems like spam detection and text classification.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯和SVM算法具有不同的特性，使它们在不同的环境下都适用。例如，朴素贝叶斯可以原生地混合连续和分类预测变量，而SVM则需要首先将分类变量重新编码为数值格式。另一方面，SVM在寻找非线性可分类别的决策边界方面表现出色，通过向数据添加一个新维度来揭示线性边界。朴素贝叶斯算法很少会优于在相同问题上训练的SVM，但朴素贝叶斯在诸如垃圾邮件检测和文本分类等问题上往往表现良好。
- en: Models trained using naive Bayes also have a probabilistic interpretation. For
    each case on which the model makes predictions, the model outputs the probability
    of that case belonging to one class over another, giving us a measure of certainty
    in our prediction. This is useful for situations in which we may want to further
    scrutinize cases with probabilities close to 50%. Conversely, models trained using
    the SVM algorithm typically don’t output easily interpretable probabilities, but
    have a *geometric* interpretation. In other words, they partition the feature
    space and classify cases based on which partition they fall within. SVMs are more
    computationally expensive to train than naive Bayes models, so if a naive Bayes
    model performs well for your problem, there may be no reason to choose a model
    that is more computationally expensive to train.
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 使用朴素贝叶斯训练的模型也具有概率解释。对于模型做出预测的每个案例，模型会输出该案例属于某一类别而非另一类别的概率，这为我们提供了预测确定性的度量。这在我们需要进一步审查概率接近50%的案例的情况下很有用。相反，使用SVM算法训练的模型通常不会输出易于解释的概率，但具有几何解释。换句话说，它们将特征空间划分为几个部分，并根据案例所属的部分进行分类。与朴素贝叶斯模型相比，SVM模型在训练时计算成本更高，因此如果朴素贝叶斯模型在您的问题上表现良好，可能没有必要选择一个训练成本更高的模型。
- en: By the end of this chapter, you’ll know how the naive Bayes and SVM algorithms
    work and how to apply them to your data. You will also have learned how to tune
    several hyperparameters simultaneously, because the SVM algorithm has many of
    them. And you will understand how to apply the more pragmatic approach of using
    a *random search*—instead of the grid search we applied in [chapter 3](kindle_split_013.html#ch03)—to
    find the combination of hyperparameters that performs best.
  id: totrans-952
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解朴素贝叶斯和SVM算法是如何工作的，以及如何将它们应用于你的数据。你还将学会如何同时调整多个超参数，因为SVM算法有很多这样的超参数。你还将理解如何应用更实用的方法，即使用
    *随机搜索*——而不是我们在第3章中应用的网格搜索——来找到表现最佳的超参数组合。
- en: 6.1\. What is the naive Bayes algorithm?
  id: totrans-953
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.1. 什么是朴素贝叶斯算法？
- en: In the last chapter, I introduced you to Bayes’ rule (named after the mathematician
    Thomas Bayes). I showed how discriminant analysis algorithms use Bayes’ rule to
    predict the probability of a case belonging to each of the classes, based on its
    discriminant function values. The naive Bayes algorithm works in exactly the same
    way, except that it doesn’t perform dimension reduction as discriminant analysis
    does, and it can handle categorical, as well as continuous, predictors. In this
    section, I hope to convey a deeper understanding of how Bayes’ rule works with
    a few examples.
  id: totrans-954
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我向你介绍了贝叶斯定理（以数学家托马斯·贝叶斯的名字命名）。我展示了判别分析算法如何使用贝叶斯定理，根据其判别函数值预测案例属于每个类别的概率。朴素贝叶斯算法以完全相同的方式工作，除了它不像判别分析那样执行降维，并且它可以处理分类变量，以及连续变量。在本节中，我希望通过几个例子传达对贝叶斯定理如何工作的更深入理解。
- en: Imagine that 0.2% of the population have unicorn disease (symptoms include obsession
    with glitter and compulsive rainbow drawing). The test for unicorn disease has
    a true positive rate of 90% (if you have the disease, the test will detect it
    90% of the time). When tested, 5% of the whole population get a positive result
    from the test. Based on this information, if you get a positive result from the
    test, what is the probability you have unicorn disease?
  id: totrans-955
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，0.2%的人口患有独角兽病（症状包括对闪光的痴迷和强迫性的彩虹绘画）。独角兽病的测试具有90%的真阳性率（如果你有这种病，测试90%的时间会检测到它）。当进行测试时，整个人口的5%从测试中获得阳性结果。基于这些信息，如果你从测试中获得阳性结果，你患有独角兽病的概率是多少？
- en: 'Many people’s instinct is to say 90%, but this doesn’t account for how prevalent
    the disease is and the proportion of tests that are positive (which also includes
    false positives). So how do we estimate the probability of having the disease,
    given a positive test result? Well, we use Bayes’ rule. Let’s remind ourselves
    of what Bayes’ rule is:'
  id: totrans-956
  prefs: []
  type: TYPE_NORMAL
  zh: 许多人本能地会说90%，但这没有考虑到疾病的普遍性和阳性测试的比例（这也包括假阳性）。那么，我们如何估计在阳性测试结果下患病的概率呢？嗯，我们使用贝叶斯定理。让我们提醒一下贝叶斯定理是什么：
- en: '![](pg137-1.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg137-1.jpg)'
- en: Where
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*p*(*k*|*x*) is the probability of having the disease (*k*) given a positive
    test result (*x*). This is called the *posterior probability*.'
  id: totrans-959
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*k*|*x*) 是在阳性测试结果(*x*)下患有疾病(*k*)的概率。这被称为 *后验概率*。'
- en: '*p*(*x*|*k*) is the probability of getting a positive test result if you *do*
    have the disease. This is called the *likelihood*.'
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*x*|*k*) 是如果你确实患有疾病，获得阳性测试结果的概率。这被称为 *似然*。'
- en: '*p*(*k*) is the probability of having the disease regardless of any test. This
    is the proportion of people in the population with the disease and is called the
    *prior probability*.'
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*k*) 是不考虑任何测试的患病概率。这是人群中患病者的比例，被称为 *先验概率*。'
- en: '*p*(*x*) is the probability of getting a positive test result and includes
    the true positives and false positives. This is called the *evidence*.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*(*x*) 是获得阳性测试结果（包括真阳性假阳性）的概率。这被称为 *证据*。'
- en: 'We can rewrite this in plain English:'
  id: totrans-963
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以用普通英语重写：
- en: '![](pg137-2.jpg)'
  id: totrans-964
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg137-2.jpg)'
- en: 'So our likelihood (the probability of getting a positive test result if we
    do have unicorn disease) is 90%, or 0.9 expressed as a decimal. Our prior probability
    (the proportion of people with unicorn disease) is 0.2%, or 0.002 as a decimal.
    Finally, our evidence (the probability of getting a positive test result) is 5%,
    or 0.05 as a decimal. You can see all these values illustrated in [figure 6.1](#ch06fig01).
    Now we simply substitute in these values into Bayes’ rule:'
  id: totrans-965
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的似然（如果我们确实有独角兽病，获得阳性测试结果的可能性）是90%，或者以小数表示为0.9。我们的先验概率（独角兽病患者的比例）是0.2%，或者以小数表示为0.002。最后，我们的证据（获得阳性测试结果的可能性）是5%，或者以小数表示为0.05。您可以在[图6.1](#ch06fig01)中看到所有这些值的说明。现在我们只需将这些值代入贝叶斯定理：
- en: '![](pg137-3.jpg)'
  id: totrans-966
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg137-3.jpg)'
- en: 'Phew! After taking into account the prevalence of the disease and the proportion
    of tests that are positive (including false positives), a positive test means
    we have only a 3.6% chance of actually having the disease—much better than 90%!
    This is the power of Bayes’ rule: it allows you to incorporate prior information
    to get a more accurate estimation of *conditional probabilities* (the probability
    of something, given the data).'
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 呼吁！在考虑疾病的流行率和测试结果为阳性的比例（包括假阳性）后，阳性测试结果意味着我们实际上患有该病的可能性只有3.6%——比90%好得多！这是贝叶斯定理的力量：它允许你结合先验信息，以获得对*条件概率*（给定数据的概率）的更准确估计。
- en: Figure 6.1\. Using Bayes’ rule to calculate the posterior probability of having
    unicorn disease, given a positive test result. The priors are the proportion of
    people with or without the disease. The likelihoods are the probabilities of getting
    positive or negative test results for each disease status. The evidence is the
    probability of getting a positive test result (true positives plus the false positives).
  id: totrans-968
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.1\. 使用贝叶斯定理计算在测试结果为阳性时拥有独角兽病的后验概率。先验是患有或未患病的比例。似然是每种疾病状态下获得阳性或阴性测试结果的可能性。证据是获得阳性测试结果（真阳性加上假阳性）的概率。
- en: '![](fig6-1.jpg)'
  id: totrans-969
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig6-1.jpg)'
- en: 6.1.1\. Using naive Bayes for classification
  id: totrans-970
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.1\. 使用朴素贝叶斯进行分类
- en: Let’s take another, more machine learning–focused, example. Imagine that you
    have a database of tweets from the social media platform Twitter, and you want
    to build a model that automatically classifies each tweet into a topic. The topics
    are
  id: totrans-971
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再举一个更侧重于机器学习的例子。想象一下，你有一个来自社交媒体平台Twitter的推文数据库，你想建立一个模型，自动将每条推文分类到某个主题。这些主题是
- en: Politics
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 政治
- en: Sports
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 体育
- en: Movies
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 电影
- en: Other
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 其他
- en: 'You create four categorical predictor variables:'
  id: totrans-976
  prefs: []
  type: TYPE_NORMAL
  zh: 你创建了四个分类预测变量：
- en: Whether the word *opinion* is present
  id: totrans-977
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包含单词*opinion*
- en: Whether the word *score* is present
  id: totrans-978
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包含单词*score*
- en: Whether the word *game* is present
  id: totrans-979
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包含单词*game*
- en: Whether the word *cinema* is present
  id: totrans-980
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 是否包含单词*cinema*
- en: '|  |'
  id: totrans-981
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-982
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: I’m keeping things simple for this example. If we were really trying to build
    a model to predict tweet topics, we would need to include many more words than
    this!
  id: totrans-983
  prefs: []
  type: TYPE_NORMAL
  zh: 我在这个例子中保持内容简单。如果我们真的试图建立一个模型来预测推文主题，我们需要包括比这更多的单词！
- en: '|  |'
  id: totrans-984
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: For each of our four topics, we can express the probability of a case belonging
    to that topic as
  id: totrans-985
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的四个主题中的每一个，我们可以将一个案例属于该主题的概率表示为
- en: '![](pg138.jpg)'
  id: totrans-986
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg138.jpg)'
- en: 'Now that we have more than one predictor variable, *p*(words|topic) is the
    likelihood of a tweet having that exact combination of words present, given the
    tweet is in that topic. We estimate this by finding the likelihood of having this
    combination of values of each predictor variable *individually*, given that the
    tweet belongs to that topic, and multiply them together. This looks like this:'
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有多个预测变量，*p*(words|topic)是在该主题下推文具有该确切单词组合的似然性。我们通过找到每个预测变量值的似然性，假设推文属于该主题，并将它们相乘来估计这个值。这看起来是这样的：
- en: '![](pg139-1_alt.jpg)'
  id: totrans-988
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg139-1_alt.jpg)'
- en: 'For example, if a tweet contains the words *opinion*, *score*, and *game*,
    but not *cinema*, then the likelihood would be as follows for any particular topic:'
  id: totrans-989
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果一个推文包含单词*opinion*、*score*和*game*，但不包含*cinema*，那么对于任何特定主题，似然性如下：
- en: '![](pg139-2_alt.jpg)'
  id: totrans-990
  prefs: []
  type: TYPE_IMG
  zh: '![图片](pg139-2_alt.jpg)'
- en: Now, the likelihood of a tweet containing a certain word if it’s in a particular
    topic is simply the proportion of tweets from that topic that contain that word.
    Multiplying the likelihoods together from each predictor variable gives us the
    likelihood of observing this *combination* of predictor variable values (this
    combination of words), given a particular class.
  id: totrans-991
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果一个推文包含某个特定主题中的某个词，那么这个推文在该主题中的似然率就是包含该词的推文在该主题中的比例。将每个预测变量的似然率相乘，我们得到观察这个*组合*（这些词的组合）的似然率，前提是它属于特定的类别。
- en: This is what makes naive Bayes “naive.” By estimating the likelihood for each
    predictor variable individually and then multiplying them, we are making the very
    strong assumption that the predictor variables are *independent*. In other words,
    we are assuming that the value of one variable has no relationship to the value
    of another one. In the majority of cases, this assumption is not true. For example,
    if a tweet contains the word *score*, it may be more likely to also include the
    word *game*.
  id: totrans-992
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是为什么朴素贝叶斯被称为“朴素”。通过单独估计每个预测变量的似然率，然后相乘，我们做出了一个非常强的假设，即预测变量是*独立的*。换句话说，我们假设一个变量的值与另一个变量的值没有关系。在大多数情况下，这个假设是不成立的。例如，如果一个推文包含单词*score*，那么它可能也更可能包含单词*game*。
- en: In spite of this naive assumption being wrong quite often, naive Bayes tends
    to perform well even in the presence of non-independent predictors. Having said
    this, strongly dependent predictor variables will impact performance.
  id: totrans-993
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这种朴素假设经常是错误的，但朴素贝叶斯即使在存在非独立预测变量的情况下也往往表现良好。话虽如此，高度相关的预测变量将影响性能。
- en: 'So the likelihood and prior probabilities are fairly simple to compute and
    are the parameters learned by the algorithm; but what about the evidence (*p*(words))?
    In practice, because the values of the predictor variables are usually reasonably
    unique to each case in the data, calculating the evidence (the probability of
    observing that combination of values) is very difficult. As the evidence is really
    just a normalizing constant that makes all the posterior probabilities sum to
    1, we can discard it and simply multiply the likelihood and prior probability:'
  id: totrans-994
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，似然率和先验概率的计算相对简单，是算法学习到的参数；但关于证据（*p*(words)）呢？在实践中，由于预测变量的值通常对数据中的每个案例都是相对独特的，计算证据（观察该值组合的概率）是非常困难的。由于证据实际上只是一个归一化常数，使得所有后验概率之和为1，我们可以忽略它，只需将似然率和先验概率相乘即可：
- en: '*posterior* ∝ *likelihood* × *prior*'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*后验概率* ∝ *似然率* × *先验概率*'
- en: 'Note that instead of an = sign, I use ∝ to mean “proportional to,” because
    without the evidence to normalize the equation, the posterior is no longer equal
    to the likelihood times the prior. This is okay, though, because proportionality
    is good enough to find the most likely class. Now, for each tweet, we calculate
    the relative posterior probability for each of the topics:'
  id: totrans-996
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我使用∝而不是等号=，表示“成比例于”，因为没有证据来归一化方程，后验概率就不再等于似然率乘以先验概率。尽管如此，这是可以接受的，因为成比例性足以找到最可能的类别。现在，对于每条推文，我们计算每个主题的相对后验概率：
- en: '| *p*(*politics*&#124;*words*) ∝ *p*(*words*&#124;*politics*) × *p*(*politics*)
    |'
  id: totrans-997
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*politics*|*words*) ∝ *p*(*words*|*politics*) × *p*(*politics*) |'
- en: '| *p*(*sports*&#124;*words*) ∝ *p*(*words*&#124;*sports*) × *p*(*sports*) |'
  id: totrans-998
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*sports*|*words*) ∝ *p*(*words*|*sports*) × *p*(*sports*) |'
- en: '| *p*(*movies*&#124;*words*) ∝ *p*(*words*&#124;*movies*) × *p*(*movies*) |'
  id: totrans-999
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*movies*|*words*) ∝ *p*(*words*|*movies*) × *p*(*movies*) |'
- en: '| *p*(*other*&#124;*words*) ∝ *p*(*words*&#124;*other*) × *p*(*other*) |'
  id: totrans-1000
  prefs: []
  type: TYPE_TB
  zh: '| *p*(*other*|*words*) ∝ *p*(*words*|*other*) × *p*(*other*) |'
- en: Then we assign the tweet to the topic with the highest relative posterior probability.
  id: totrans-1001
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将推文分配给相对后验概率最高的主题。
- en: 6.1.2\. Calculating the likelihood for categorical and continuous predictors
  id: totrans-1002
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.1.2\. 计算分类和连续预测变量的似然率
- en: When we have a categorical predictor (such as whether a word is present or not),
    naive Bayes uses that proportion of training cases in that particular class, with
    that value of the predictor. When we have a continuous variable, naive Bayes (typically)
    assumes that the data within each group is normally distributed. The probability
    density of each case based on this fitted normal distribution is then used to
    estimate the likelihood of observing this value of the predictor in that class.
    In this way, cases near the mean of the normal distribution for a particular class
    will have high probability density for that class, and cases far away from the
    mean will have a low probability density. This is the same way you saw discriminant
    analysis calculate the likelihood in [figure 5.7](kindle_split_015.html#ch05fig07)
    in [chapter 5](kindle_split_015.html#ch05).
  id: totrans-1003
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们有一个分类预测变量（例如，一个单词是否存在）时，朴素贝叶斯使用该特定类别中训练案例的该预测变量的比例。当我们有一个连续变量时，朴素贝叶斯（通常）假设每个组内的数据是正态分布的。然后，根据这个拟合的正态分布，每个案例的概率密度被用来估计在该类别中观察到该预测变量值的可能性。这样，接近特定类别正态分布均值的案例将具有该类别的高概率密度，而远离均值的案例将具有低概率密度。这与你在第5章中看到的判别分析在[图5.7](kindle_split_015.html#ch05fig07)中计算似然性的方式相同。
- en: When your data has a mixture of categorical and continuous predictors, because
    naive Bayes assumes independence between data values, it simply uses the appropriate
    method for estimating the likelihood, depending on whether each predictor is categorical
    or continuous.
  id: totrans-1004
  prefs: []
  type: TYPE_NORMAL
  zh: 当你的数据包含分类和连续预测变量的混合时，因为朴素贝叶斯假设数据值之间是独立的，它将简单地使用适当的方法来估计似然性，这取决于每个预测变量是分类的还是连续的。
- en: 6.2\. Building your first naive Bayes model
  id: totrans-1005
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.2. 构建您的第一个朴素贝叶斯模型
- en: 'In this section, I’ll teach you how to build and evaluate the performance of
    a naive Bayes model to predict political party affiliation. Imagine that you’re
    a political scientist. You’re looking for common voting patterns in the mid-1980s
    that would predict whether a US congressperson was a Democrat or Republican. You
    have the voting record of each member of the House of Representatives in 1984,
    and you identify 16 key votes that you believe most strongly split the two political
    parties. Your job is to train a naive Bayes model to predict whether a congressperson
    was a Democrat or a Republican, based on how they voted throughout the year. Let’s
    start by loading the mlr and tidyverse packages:'
  id: totrans-1006
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您介绍如何构建和评估一个朴素贝叶斯模型以预测政党归属。想象一下，你是一位政治学家。你正在寻找20世纪80年代中期的共同投票模式，以预测美国国会议员是民主党人还是共和党人。您拥有1984年众议院每位成员的投票记录，并确定了16个您认为最能将两个政党区分开的投票。您的任务是训练一个朴素贝叶斯模型，根据议员一整年的投票情况来预测他们是否是民主党人或共和党人。让我们先加载mlr和tidyverse包：
- en: '[PRE75]'
  id: totrans-1007
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 6.2.1\. Loading and exploring the HouseVotes84 dataset
  id: totrans-1008
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.1. 加载和探索HouseVotes84数据集
- en: Now let’s load the data, which is built into the mlbench package, convert it
    into a tibble (with `as_tibble()`), and explore it.
  id: totrans-1009
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们加载数据，该数据内置在mlbench包中，将其转换为tibble（使用`as_tibble()`），并对其进行探索。
- en: '|  |'
  id: totrans-1010
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1011
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that a tibble is just a tidyverse version of a data frame that helps
    make our lives a little easier.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，tibble只是tidyverse版本的数据框，它有助于使我们的生活更加轻松。
- en: '|  |'
  id: totrans-1013
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We have a tibble containing 435 cases and 17 variables of members of the House
    Representatives in 1984\. The `Class` variable is a factor indicating political
    party membership, and the other 16 variables are factors indicating how the individuals
    voted on each of the 16 votes. A value of `y` means they voted in favor, a value
    of `n` means they voted against, and a missing value (`NA`) means the individual
    either abstained or did not vote. Our goal is to train a model that can use the
    information in these variables to predict whether a congressperson was a Democrat
    or Republican, based on how they voted.
  id: totrans-1014
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含1984年众议院435名成员的17个变量的tibble。`Class`变量是一个因子，表示政党成员资格，其他16个变量是因子，表示个人在每个16个投票上的投票情况。`y`值表示他们投了赞成票，`n`值表示他们投了反对票，而缺失值（`NA`）表示个人弃权或未投票。我们的目标是训练一个模型，该模型可以使用这些变量中的信息，根据议员如何投票来预测他们是否是民主党人或共和党人。
- en: Listing 6.1\. Loading and exploring the `HouseVotes84` dataset
  id: totrans-1015
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.1. 加载和探索`HouseVotes84`数据集
- en: '[PRE76]'
  id: totrans-1016
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: '|  |'
  id: totrans-1017
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1018
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Ordinarily I would manually give names to unnamed columns to make it clearer
    what I’m working with. In this example, the variable names are the names of votes
    and are a little cumbersome, so we’ll stick with V1, V2, and so on. If you want
    to see what issue each vote was for, run `?mlbench::HouseVotes84`.
  id: totrans-1019
  prefs: []
  type: TYPE_NORMAL
  zh: 通常我会手动给未命名的列命名，以便更清楚地知道我在处理什么。在这个例子中，变量名称是投票的名称，有点繁琐，所以我们将坚持使用 V1、V2 等等。如果你想查看每个投票是针对什么问题，请运行
    `?mlbench::HouseVotes84`。
- en: '|  |'
  id: totrans-1020
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: It looks like we have a few missing values (`NA`s) in our tibble. Let’s summarize
    the number of missing values in each variable using the `map_dbl()` function.
    Recall from [chapter 2](kindle_split_011.html#ch02) that `map_dbl()` iterates
    a function over every element of a vector/list (or, in this case, every column
    of a tibble), applies a function to that element, and returns a vector containing
    the function output.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来我们的 tibble 中有一些缺失值（`NA`）。让我们使用 `map_dbl()` 函数总结每个变量中缺失值的数量。回想一下 [第 2 章](kindle_split_011.html#ch02)，`map_dbl()`
    会遍历向量/列表（或在这种情况下，tibble 的每一列）中的每个元素，对该元素应用一个函数，并返回包含函数输出的向量。
- en: The first argument to the `map_dbl()` function is the name of the data we’re
    going to apply the function to, and the second argument is the function we want
    to apply. I’ve chosen to use an anonymous function (using the `~` symbol as shorthand
    for `function(.)`.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: '`map_dbl()` 函数的第一个参数是要应用函数的数据名称，第二个参数是我们想要应用的函数。我选择使用匿名函数（使用 `~` 符号作为 `function(.)`
    的简写）。'
- en: '|  |'
  id: totrans-1023
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1024
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 2](kindle_split_011.html#ch02) that an *anonymous* function
    is a function that we define on the fly instead of predefining a function and
    assigning it to an object.
  id: totrans-1025
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下 [第 2 章](kindle_split_011.html#ch02)，一个 *匿名* 函数是我们即时定义的函数，而不是预先定义一个函数并将其分配给一个对象。
- en: '|  |'
  id: totrans-1026
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Our function passes each vector to `sum(is.na(.))` to count the number of missing
    values in that vector. This function is applied to each column of the tibble and
    returns the number of missing values for each.
  id: totrans-1027
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能将每个向量传递给 `sum(is.na(.))` 来计算该向量中缺失值的数量。这个函数应用于 tibble 的每一列，并返回每一列的缺失值数量。
- en: Listing 6.2\. Using the `map_dbl()` function to show missing values
  id: totrans-1028
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.2\. 使用 `map_dbl()` 函数显示缺失值
- en: '[PRE77]'
  id: totrans-1029
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'Every column in our tibble has missing values except the `Class` variable!
    Luckily, the naive Bayes algorithm can handle missing data in two ways:'
  id: totrans-1030
  prefs: []
  type: TYPE_NORMAL
  zh: 除了 `Class` 变量外，我们 tibble 中的每一列都有缺失值！幸运的是，朴素贝叶斯算法可以以两种方式处理缺失数据：
- en: By omitting the variables with missing values for a particular case, but still
    using that case to train the model
  id: totrans-1031
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过省略特定情况下的缺失值变量，但仍然使用该情况来训练模型
- en: By omitting that case entirely from the training set
  id: totrans-1032
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过完全从训练集中省略该情况
- en: By default, the naive Bayes implementation that mlr uses is to keep cases and
    drop variables. This usually works fine if the ratio of missing to complete values
    for the majority of cases is quite small. However, if you have a small number
    of variables and a large proportion of missing values, you may wish to omit the
    cases instead (and, more broadly, consider whether your dataset is sufficient
    for training).
  id: totrans-1033
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，mlr 使用的朴素贝叶斯实现是保留案例并删除变量。如果大多数案例中缺失值与完整值的比例相当小，这通常工作得很好。然而，如果你有少量变量并且缺失值的比例很大，你可能希望省略这些案例（并且更广泛地考虑你的数据集是否足够用于训练）。
- en: '|  |'
  id: totrans-1034
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 1**'
  id: totrans-1035
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 1**'
- en: 'Use the `map_dbl()` function as we did in [listing 6.2](#ch06ex02) to count
    the number of `y` values in each column of `votesTib`. Hint: Use `which(. == "y")`
    to return the rows in each column that equal `y`.'
  id: totrans-1036
  prefs: []
  type: TYPE_NORMAL
  zh: 使用我们在 [列表 6.2](#ch06ex02) 中使用的 `map_dbl()` 函数来统计 `votesTib` 每一列中 `y` 值的数量。提示：使用
    `which(. == "y")` 来返回每一列中等于 `y` 的行。
- en: '|  |'
  id: totrans-1037
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 6.2.2\. Plotting the data
  id: totrans-1038
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.2\. 绘制数据
- en: Let’s plot our data to get a better understanding of the relationships between
    political party and votes. Once again, we’ll use our trick to gather the data
    into an untidy format so we can facet across the predictors. Because we’re plotting
    categorical variables against each other, we set the `position` argument of the
    `geom_bar()` function to `"fill"`, which creates stacked bars for `y`, `n`, and
    `NA` responses that sum to 1.
  id: totrans-1039
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制我们的数据，以更好地理解政治党和投票之间的关系。再一次，我们将使用我们的技巧将数据收集到一个不整洁的格式中，这样我们就可以在预测变量上分面。因为我们正在绘制相互之间的分类变量，我们将
    `geom_bar()` 函数的 `position` 参数设置为 `"fill"`，这为 `y`、`n` 和 `NA` 响应创建了堆叠条形图，它们的总和为
    1。
- en: Listing 6.3\. Plotting the `HouseVotes84` dataset
  id: totrans-1040
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.3\. 绘制 `HouseVotes84` 数据集
- en: '[PRE78]'
  id: totrans-1041
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: The resulting plot is shown in [figure 6.2](#ch06fig02). We can see there are
    some very clear differences in opinion between Democrats and Republicans!
  id: totrans-1042
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在 [图 6.2](#ch06fig02)。我们可以看到，民主党和共和党之间存在着一些非常明显的意见差异！
- en: Figure 6.2\. Filled bar charts showing the proportion of Democrats and Republicans
    that voted for (`y`) or against (`n`) or abstained (`NA`) on 16 different votes.
  id: totrans-1043
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.2\. 填充条形图显示了在 16 次不同投票中民主党人和共和党人投票支持 (`y`)、反对 (`n`) 或弃权 (`NA`) 的比例。
- en: '![](fig6-2_alt.jpg)'
  id: totrans-1044
  prefs: []
  type: TYPE_IMG
  zh: '![](fig6-2_alt.jpg)'
- en: 6.2.3\. Training the model
  id: totrans-1045
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.2.3\. 训练模型
- en: Now let’s create our task and learner, and build our model. We set the `Class`
    variable as the classification target of the `makeClassifTask()` function, and
    the algorithm we supply to the `makeLearner()` function is `"classif.naiveBayes"`.
  id: totrans-1046
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们创建我们的任务和学习者，并构建我们的模型。我们将 `Class` 变量设置为 `makeClassifTask()` 函数的分类目标，并将我们提供给
    `makeLearner()` 函数的算法设置为 `"classif.naiveBayes"`。
- en: Listing 6.4\. Creating the task and learner, and training the model
  id: totrans-1047
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.4\. 创建任务和学习者，并训练模型
- en: '[PRE79]'
  id: totrans-1048
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The model training completes with no errors because naive Bayes can handle missing
    data.
  id: totrans-1049
  prefs: []
  type: TYPE_NORMAL
  zh: 模型训练完成后没有错误，因为朴素贝叶斯可以处理缺失数据。
- en: Next, we’ll use 10-fold cross-validation repeated 50 times to evaluate the performance
    of our model-building procedure. Again, because this is a two-class classification
    problem, we have access to the false positive rate and false negative rate, and
    so we ask for these as well in the `measures` argument to the `resample()` function.
  id: totrans-1050
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将使用重复 50 次的 10 折交叉验证来评估我们的模型构建过程的性能。同样，因为这是一个双类分类问题，我们可以访问假阳性率和假阴性率，所以我们也在
    `resample()` 函数的 `measures` 参数中请求这些。
- en: Listing 6.5\. Cross-validating the naive Bayes model
  id: totrans-1051
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.5\. 交叉验证朴素贝叶斯模型
- en: '[PRE80]'
  id: totrans-1052
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Our model correctly predicts 90% of test set cases in our cross-validation.
    That’s not bad! Now let’s use our model to predict the political party of a new
    politician, based on their votes.
  id: totrans-1053
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的交叉验证中，我们的模型正确预测了 90% 的测试集案例。这还不错！现在让我们使用我们的模型来预测新政治家的政党，基于他们的投票。
- en: Listing 6.6\. Using the model to make predictions
  id: totrans-1054
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.6\. 使用模型进行预测
- en: '[PRE81]'
  id: totrans-1055
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: Our model predicts that the new politician is a Democrat.
  id: totrans-1056
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型预测这位新政治家是民主党人。
- en: '|  |'
  id: totrans-1057
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Exercise 2**'
  id: totrans-1058
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习 2**'
- en: Wrap your naive Bayes model inside the `getLearnerModel()` function. Can you
    identify the prior probabilities and the likelihoods for each vote?
  id: totrans-1059
  prefs: []
  type: TYPE_NORMAL
  zh: 将您的朴素贝叶斯模型包裹在 `getLearnerModel()` 函数中。您能识别出每个投票的先验概率和似然吗？
- en: '|  |'
  id: totrans-1060
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 6.3\. Strengths and weaknesses of naive Bayes
  id: totrans-1061
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.3\. 朴素贝叶斯的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    naive Bayes will perform well for your task.
  id: totrans-1062
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪些算法会对给定的任务表现良好，但以下是一些优势和劣势，这将帮助您决定朴素贝叶斯是否会对您的任务表现良好。
- en: 'The strengths of naive Bayes are as follows:'
  id: totrans-1063
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的优点如下：
- en: It can handle both continuous and categorical predictor variables.
  id: totrans-1064
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理连续和分类预测变量。
- en: It’s computationally inexpensive to train.
  id: totrans-1065
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练它计算上并不昂贵。
- en: It commonly performs well on topic classification problems where we want to
    classify documents based on the words they contain.
  id: totrans-1066
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它通常在基于包含的单词对文档进行分类的主题分类问题上表现良好。
- en: It has no hyperparameters to tune.
  id: totrans-1067
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它没有需要调整的超参数。
- en: It is probabilistic and outputs the probabilities of new data belonging to each
    class.
  id: totrans-1068
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是概率性的，并输出新数据属于每个类别的概率。
- en: It can handle cases with missing data.
  id: totrans-1069
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理缺失数据的情况。
- en: 'The weaknesses of naive Bayes are these:'
  id: totrans-1070
  prefs: []
  type: TYPE_NORMAL
  zh: 朴素贝叶斯的劣势如下：
- en: It assumes that continuous predictor variables are normally distributed (typically),
    and performance will suffer if they’re not.
  id: totrans-1071
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设连续预测变量是正态分布的（通常是），如果它们不是，性能将受到影响。
- en: It assumes that predictor variables are independent of each other, which usually
    isn’t true. Performance will suffer if this assumption is severely violated.
  id: totrans-1072
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它假设预测变量之间相互独立，这通常并不真实。如果这个假设被严重违反，性能将受到影响。
- en: 6.4\. What is the support vector machine (SVM) algorithm?
  id: totrans-1073
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.4\. 什么是支持向量机 (SVM) 算法？
- en: In this section, you’ll learn how the SVM algorithm works and how it can add
    an extra dimension to data to make the classes linearly separable. Imagine that
    you would like to predict whether your boss will be in a good mood or not (a very
    important machine learning application). Over a couple of weeks, you record the
    number of hours you spend playing games at your desk and how much money you make
    the company each day. You also record your boss’s mood the next day as good or
    bad (they’re very binary). You decide to use the SVM algorithm to build a classifier
    that will help you decide whether you need to avoid your boss on a particular
    day. The SVM algorithm will learn a linear hyperplane that separates the days
    your boss is in a good mood from the days they are in a bad mood. The SVM algorithm
    is also able to add an extra dimension to the data to find the best hyperplane.
  id: totrans-1074
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解 SVM 算法的工作原理以及它如何为数据添加一个额外的维度，使其类别线性可分。想象一下，你想要预测你的老板是否会心情愉快（这是一个非常重要的机器学习应用）。在接下来的几周里，你记录了你在办公桌前玩游戏的小时数以及你每天为公司赚多少钱。你还记录了第二天老板的心情，是好的还是坏的（他们非常二元）。你决定使用
    SVM 算法构建一个分类器，帮助你决定是否需要在某一天避免见到你的老板。SVM 算法将学习一个线性超平面，将老板心情好的日子与心情不好的日子分开。SVM 算法还能够为数据添加一个额外的维度，以找到最佳的超平面。
- en: 6.4.1\. SVMs for linearly separable data
  id: totrans-1075
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.1\. 线性可分数据的 SVM
- en: Take a look at the data shown in [figure 6.3](#ch06fig03). The plots show the
    data you recorded on the mood of your boss, based on how hard you’re working and
    how much money you’re making the company.
  id: totrans-1076
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下 [图 6.3](#ch06fig03) 中显示的数据。这些图显示了基于你工作努力程度和为公司赚多少钱，你记录的老板心情数据。
- en: 'The SVM algorithm finds an optimal linear hyperplane that separates the classes.
    A hyperplane is a surface that has one less dimension than there are variables
    in the dataset. For a two-dimensional feature space, such as in the example in
    [figure 6.3](#ch06fig03), a hyperplane is simply a straight line. For a three-dimensional
    feature space, a hyperplane is a surface. It’s hard to picture hyperplanes in
    a four or more dimensional feature space, but the principle is the same: they
    are surfaces that cut through the feature space.'
  id: totrans-1077
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 算法找到一个最优的线性超平面，将类别分开。超平面是一个比数据集中的变量少一个维度的表面。对于二维特征空间，例如 [图 6.3](#ch06fig03)
    中的例子，超平面就是一个简单的直线。对于三维特征空间，超平面是一个表面。在四维或更高维度的特征空间中，超平面很难想象，但原理是相同的：它们是穿过特征空间的表面。
- en: Figure 6.3\. The SVM algorithm finds a hyperplane (solid line) that passes through
    the feature space. An optimal hyperplane is one that maximizes the margin around
    itself (dotted lines). The margin is a region around the hyperplane that touches
    the fewest cases. Support vectors are shown with double circles.
  id: totrans-1078
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 6.3\. SVM 算法找到一个通过特征空间的超平面（实线）。一个最优的超平面是最大化其周围边界的超平面（虚线）。边界是一个围绕超平面的区域，接触到的案例最少。支持向量用双圆圈表示。
- en: '![](fig6-3_alt.jpg)'
  id: totrans-1079
  prefs: []
  type: TYPE_IMG
  zh: '![图 6.3](fig6-3_alt.jpg)'
- en: For problems where the classes are fully, linearly separable, there may be many
    different hyperplanes that do just as good a job at separating the classes in
    the training data. To find an optimal hyperplane (which will, hopefully, generalize
    better to unseen data), the algorithm finds the hyperplane that maximizes the
    *margin* around itself. The margin is a distance around the hyperplane that touches
    the fewest training cases. The cases in the data that touch the margin are called
    *support vectors* because they support the position of the hyperplane (hence,
    the name of the algorithm).
  id: totrans-1080
  prefs: []
  type: TYPE_NORMAL
  zh: 对于类别完全线性可分的问题，可能存在许多不同的超平面，它们在训练数据中分离类别的效果一样好。为了找到一个最优的超平面（希望它能更好地泛化到未见过的数据），算法找到最大化其周围
    *边界* 的超平面。边界是围绕超平面的一个距离，接触到的训练案例最少。接触边界的案例被称为 *支持向量*，因为它们支撑着超平面的位置（因此，算法的名称）。
- en: The support vectors are the most important cases in the training set because
    they define the boundary between the classes. Not only this, but the hyperplane
    that the algorithm learns is entirely dependent on the position of the support
    vectors and none of the other cases in the training set. Take a look at [figure
    6.4](#ch06fig04). If we move the position of one of the support vectors, then
    we move the position of the hyperplane. If, however, we move a non-support vector
    case, there is no influence on the hyperplane at all!
  id: totrans-1081
  prefs: []
  type: TYPE_NORMAL
  zh: 支持向量是训练集中最重要的案例，因为它们定义了类之间的边界。不仅如此，算法学习的超平面完全依赖于支持向量的位置，而训练集中的其他案例则不然。看看[图6.4](#ch06fig04)。如果我们移动其中一个支持向量的位置，那么超平面的位置也会移动。然而，如果我们移动一个非支持向量案例，对超平面没有任何影响！
- en: Figure 6.4\. The position of the hyperplane is entirely dependent on the position
    of support vectors. Moving a support vector moves the hyperplane from its original
    position (dotted line) to a new position (top two plots). Moving a non-support
    vector has no impact on the hyperplane (bottom two plots).
  id: totrans-1082
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.4。超平面的位置完全取决于支持向量的位置。移动一个支持向量会将超平面从其原始位置（虚线）移动到新的位置（顶部两个图）。移动一个非支持向量对超平面没有任何影响（底部两个图）。
- en: '![](fig6-4_alt.jpg)'
  id: totrans-1083
  prefs: []
  type: TYPE_IMG
  zh: '![](fig6-4_alt.jpg)'
- en: 'SVMs are extremely popular right now. That’s mainly for three reasons:'
  id: totrans-1084
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs目前非常受欢迎。这主要基于三个原因：
- en: They are good at finding ways of separating non-linearly separable classes.
  id: totrans-1085
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们擅长找到分离非线性可分类的方法。
- en: They tend to perform well for a wide variety of tasks.
  id: totrans-1086
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们在各种任务中表现良好。
- en: We now have the computational power to apply them to larger, more complex datasets.
  id: totrans-1087
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们现在有了足够的计算能力来处理更大、更复杂的数据集。
- en: 'This last point is important because it highlights a potential downside of
    SVMs: they tend to be more computationally expensive to train than many other
    classification algorithms. For this reason, if you have a very large dataset,
    and computational power is limited, it may be economical for you to try cheaper
    algorithms first and see how they perform.'
  id: totrans-1088
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一点很重要，因为它突显了SVMs的一个潜在缺点：它们在训练时往往比许多其他分类算法更耗费计算资源。因此，如果你有一个非常大的数据集，且计算能力有限，尝试更便宜的算法并观察其表现可能对你来说更经济。
- en: '|  |'
  id: totrans-1089
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1090
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Usually, we favor predictive performance over speed. But a computationally cheap
    algorithm that performs well enough for your problem may be preferable to you
    than one that is very expensive. Therefore, I may try cheaper algorithms before
    trying the expensive ones.
  id: totrans-1091
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，我们更倾向于预测性能而非速度。但一个计算成本较低且足以解决你问题的算法，可能比你选择的昂贵算法更受欢迎。因此，我可能会在尝试昂贵算法之前先尝试更便宜的算法。
- en: '|  |'
  id: totrans-1092
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-1093
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**How does the SVM algorithm find the optimal hyperplane?**'
  id: totrans-1094
  prefs: []
  type: TYPE_NORMAL
  zh: '**SVM算法是如何找到最优超平面的？**'
- en: The math underpinning how SVMs work is complex, but if you’re interested, here
    are some of the basics of how the hyperplane is learned. Recall from [chapter
    4](kindle_split_014.html#ch04) that the equation for a straight line can be written
    as *y* = *ax* + *b*, where *a* and *b* are the slope and y-intercept of the line,
    respectively. We could rearrange this equation to be *y* – *ax* – *b* = 0 by shifting
    all the terms onto one side of the equals sign. Using this formulation, we can
    say that any point that falls on the line is one that satisfies this equation
    (the expression will equal zero).
  id: totrans-1095
  prefs: []
  type: TYPE_NORMAL
  zh: 支持SVM工作原理的数学原理很复杂，但如果你感兴趣，这里有一些关于如何学习超平面的基础知识。回想一下[第4章](kindle_split_014.html#ch04)，一条直线的方程可以写成
    *y* = *ax* + *b*，其中 *a* 和 *b* 分别是直线的斜率和y截距。我们可以通过将所有项移到等号的一侧来重新排列这个方程，使其成为 *y*
    – *ax* – *b* = 0。使用这种公式，我们可以说任何落在直线上的点都满足这个方程（表达式将等于零）。
- en: You’ll often see the equation for a hyperplane given as *wx* + *b* = 0, where
    *w* is the vector (–*b* –*a* 1), *x* is the vector (1 *x y*), and *b* is still
    the intercept. In the same way that any point that lies on a straight line satisfies
    *y* – *ax* – *b* = 0, any point that falls on a hyperplane satisfies the equation
    *wx* + *b* = 0.
  id: totrans-1096
  prefs: []
  type: TYPE_NORMAL
  zh: 你经常会看到超平面的方程被表示为 *wx* + *b* = 0，其中 *w* 是向量（–*b* –*a* 1），*x* 是向量（1 *x y*），而 *b*
    仍然是截距。与任何位于直线上的点都满足 *y* – *ax* – *b* = 0 的方式相同，任何位于超平面上的点都满足方程 *wx* + *b* = 0。
- en: 'The vector *w* is orthogonal or *normal* to the hyperplane. Therefore, by changing
    the intercept, *b*, we can create new hyperplanes that are parallel to the original.
    By changing *b* (and rescaling *w*) we can arbitrarily define the hyperplanes
    that mark the margins as *wx* + *b* = –1 and *wx* + *b* = +1\. The distance between
    these margins is given by 2/||*w*||, where ||*w*|| is ![](pg148.jpg). As we want
    to find the hyperplane that maximizes this distance, we need to minimize ||*w*||
    while ensuring that each case is classified correctly. The algorithm does this
    by making sure all cases in one class lie below *wx* + *b* = –1 and all cases
    in the other class lie above *wx* + *b* = +1\. A simple way of doing this is to
    multiply the predicted value of each case by its corresponding label (–1 or +1),
    making all outputs positive. This creates the constraint that the margins must
    satisfy *y[i]*(*wx[i]* + *b*) ≥ 1\. The SVM algorithm therefore tries to solve
    the following minimization problem:'
  id: totrans-1097
  prefs: []
  type: TYPE_NORMAL
  zh: 向量 *w* 与超平面正交或垂直。因此，通过改变截距 *b*，我们可以创建与原始超平面平行的新的超平面。通过改变 *b*（以及重新缩放 *w*），我们可以任意定义标记间隔为
    *wx* + *b* = –1 和 *wx* + *b* = +1 的超平面。这些间隔之间的距离由 2/||*w*|| 给出，其中 ||*w*|| 是 ![图片](pg148.jpg)。由于我们想要找到最大化这个距离的超平面，我们需要在确保每个案例被正确分类的同时最小化
    ||*w*||。算法通过确保一个类别的所有案例都位于 *wx* + *b* = –1 下方，而另一个类别的所有案例都位于 *wx* + *b* = +1 之上来实现这一点。一种简单的方法是将每个案例的预测值乘以其相应的标签（–1
    或 +1），使所有输出都为正。这创建了间隔必须满足的约束 *y[i]*(*wx[i]* + *b*) ≥ 1。因此，SVM 算法试图解决以下最小化问题：
- en: minimize ||*w*|| subject to *y[i]*(*wx[i]* + *b*) ≥ 1 for *i* = 1 ... *N*.
  id: totrans-1098
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最小化 ||*w*||，同时满足 *y[i]*(*wx[i]* + *b*) ≥ 1 对于 *i* = 1 ... *N*。
- en: '|  |'
  id: totrans-1099
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 6.4.2\. What if the classes aren’t fully separable?
  id: totrans-1100
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.2. 如果类别不是完全可分的，会怎样？
- en: In the illustrations I’ve shown you so far, the classes have been fully separable.
    This is so I could clearly show you how the positioning of the hyperplane is chosen
    to maximize the margin. But what about situations where the classes are *not*
    completely separable? How can the algorithm find a hyperplane when there is no
    margin that won’t have cases *inside* it?
  id: totrans-1101
  prefs: []
  type: TYPE_NORMAL
  zh: 在我之前向你展示的示例中，类别是完全可分的。这样做是为了清楚地向你展示如何选择超平面的位置以最大化间隔。但是，当类别*不是*完全可分时怎么办？当没有间隔时，算法如何找到超平面？其中会有案例*位于*间隔内部。
- en: The original formulation of SVMs uses what is often referred to as a *hard margin*.
    If an SVM uses a hard margin, then no cases are allowed to fall within the margin.
    This means that, if the classes are not fully separable, the algorithm will fail.
    This is, of course, a massive problem, because it relegates *hard-margin SVM*
    to handling only “easy” classification problems where the training set can be
    clearly partitioned into its component classes. As a result, an extension to the
    SVM algorithm called *soft-margin SVM* is much more commonly used. In soft-margin
    SVM, the algorithm still learns a hyperplane that best separates the classes,
    but it allows cases to fall inside its margin.
  id: totrans-1102
  prefs: []
  type: TYPE_NORMAL
  zh: SVMs 的原始公式使用通常被称为*硬间隔*的方法。如果一个 SVM 使用硬间隔，则不允许任何案例落在间隔内。这意味着，如果类别不是完全可分的，算法将失败。这当然是一个大问题，因为它将*硬间隔
    SVM*限制在只能处理“简单”的分类问题，其中训练集可以清楚地划分为其组成部分类别。因此，一个称为*软间隔 SVM*的 SVM 算法的扩展被更广泛地使用。在软间隔
    SVM 中，算法仍然学习一个最佳地分离类别的超平面，但它允许案例落在其间隔内。
- en: 'The soft-margin SVM algorithm still tries to find the hyperplane that best
    separates the classes, but it is penalized for having cases inside its margin.
    How severe the penalty is for having a case inside the margin is controlled by
    a hyperparameter that controls how “hard” or “soft” the margin is (we’ll discuss
    this hyperparameter and how it affects the position of the hyperplane later in
    the chapter). The harder the margin is, the fewer cases will be inside it; the
    hyperplane will depend on a smaller number of support vectors. The softer the
    margin is, the more cases will be inside it; the hyperplane will depend on a larger
    number of support vectors. This has consequences for the bias-variance trade-off:
    if our margin is too hard, we might overfit the noise near the decision boundary,
    whereas if our margin is too soft, we might underfit the data and learn a decision
    boundary that does a bad job of separating the classes.'
  id: totrans-1103
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔支持向量机（SVM）算法仍然试图找到最佳的超平面来分离类别，但它会因为在其边界内有案例而受到惩罚。案例在边界内受到的惩罚程度由一个超参数控制，该超参数控制边界的“硬”或“软”程度（我们将在本章后面讨论这个超参数及其对超平面位置的影响）。边界越硬，内部案例越少；超平面将依赖于更少的支持向量。边界越软，内部案例越多；超平面将依赖于更多的支持向量。这对偏差-方差权衡有影响：如果我们的边界太硬，我们可能会在决策边界附近的噪声上过度拟合，而如果我们的边界太软，我们可能会欠拟合数据，并学习到一个在分离类别方面做得不好的决策边界。
- en: 6.4.3\. SVMs for non-linearly separable data
  id: totrans-1104
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.3. 用于非线性可分数据的SVM
- en: 'Great! So far the SVM algorithm seems quite simple—and for linearly separable
    classes like in our boss-mood example, it is. But I mentioned that one of the
    strengths of the SVM algorithm is that it can learn decision boundaries between
    classes that are *not* linearly separable. I’ve told you that the algorithm learns
    linear hyperplanes, so this seems like a contradiction. Well, here’s what makes
    the SVM algorithm so powerful: it can add an extra dimension to your data to find
    a linear way to separate nonlinear data.'
  id: totrans-1105
  prefs: []
  type: TYPE_NORMAL
  zh: 太好了！到目前为止，SVM算法看起来相当简单——对于像我们老板情绪示例中的线性可分类别，它确实是这样的。但我提到SVM算法的一个优点是它可以学习非线性可分类别之间的决策边界。我已经告诉你该算法学习线性超平面，这似乎是一种矛盾。好吧，这就是SVM算法之所以强大的原因：它可以为你的数据添加一个额外的维度，以找到一种线性方式来分离非线性数据。
- en: Figure 6.5\. The SVM algorithm adds an extra dimension to linearly separate
    the data. The classes in the original data are not linearly separable. The SVM
    algorithm adds an extra dimension that, in a two-dimensional feature space, can
    be illustrated as a “stretching” of the data into a third dimension. This additional
    dimension allows the data to be linearly separated. When this hyperplane is projected
    back onto the original two dimensions, it appears as a curved decision boundary.
  id: totrans-1106
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.5. SVM算法为线性分离数据添加了一个额外的维度。原始数据中的类别无法线性分离。SVM算法添加了一个额外的维度，在二维特征空间中，可以将其表示为数据的“拉伸”到第三维度。这个额外的维度允许数据被线性分离。当这个超平面投影回原始的两个维度时，它看起来像是一个弯曲的决策边界。
- en: '![](fig6-5_alt.jpg)'
  id: totrans-1107
  prefs: []
  type: TYPE_IMG
  zh: '![](fig6-5_alt.jpg)'
- en: Take a look at the example in [figure 6.5](#ch06fig05). The classes are not
    linearly separable using the two predictor variables. The SVM algorithm adds an
    extra dimension to the data, such that a linear hyperplane can separate the classes
    in this new, higher-dimensional space. We can visualize this as a sort of deformation
    or stretching of the feature space. The extra dimension is called a *kernel*.
  id: totrans-1108
  prefs: []
  type: TYPE_NORMAL
  zh: 看一下[图6.5](#ch06fig05)中的示例。使用两个预测变量无法线性分离类别。SVM算法为数据添加了一个额外的维度，使得在这个新的、更高维的空间中，线性超平面可以分离类别。我们可以将这视为特征空间的一种变形或拉伸。这个额外的维度被称为**核函数**。
- en: '|  |'
  id: totrans-1109
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1110
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 5](kindle_split_015.html#ch05) that discriminant analysis
    condenses the information from the predictor variables into a smaller number of
    variables. Contrast this to the SVM algorithm, which expands the information from
    the predictor variables into an extra variable!
  id: totrans-1111
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第5章](kindle_split_015.html#ch05)中的内容，判别分析将预测变量的信息压缩成更少的变量。这与SVM算法形成对比，SVM算法将预测变量的信息扩展到一个额外的变量！
- en: '|  |'
  id: totrans-1112
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '|  |'
  id: totrans-1113
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Why is it called a kernel?**'
  id: totrans-1114
  prefs: []
  type: TYPE_NORMAL
  zh: '**为什么叫核函数？**'
- en: The word *kernel* may confuse you (it certainly confuses me). It has nothing
    to do with the kernel in computing (the bit of your operating system that directly
    interfaces with the computer hardware), or kernels in corn or fruit.
  id: totrans-1115
  prefs: []
  type: TYPE_NORMAL
  zh: 词语**核函数**可能会让你感到困惑（它确实让我感到困惑）。它与计算机中的核函数（直接与计算机硬件接口的操作系统的一部分）或玉米或水果中的核没有关系。
- en: The truth is that the reason they are called kernels is murky. In 1904, a German
    mathematician named David Hilbert published *Grundzüge einer allgemeinen theorie
    der linearen integralgleichungen* (*Principles of a general theory of linear integral
    equations*). In this book, Hilbert uses the word *kern* to mean the *core* of
    an integral equation. In 1909, an American mathematician called Maxime Bôcher
    published *An introduction to the study of integral equations* in which he translates
    Hilbert’s use of the word *kern* to *kernel*.
  id: totrans-1116
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，它们被称为核的原因并不明确。1904年，一位名叫David Hilbert的德国数学家发表了《Grundzüge einer allgemeinen
    theorie der linearen integralgleichungen》（*线性积分方程的一般理论原理*）。在这本书中，Hilbert使用*kern*这个词来表示积分方程的*核心*。1909年，一位名叫Maxime
    Bôcher的美国数学家发表了《An introduction to the study of integral equations》，其中他将Hilbert对*kern*的使用翻译成了*kernel*。
- en: The mathematics of kernel functions evolved from the work in these publications
    and took the name *kernel* with them. The extremely confusing thing is that multiple,
    seemingly unrelated concepts in mathematics include the word *kernel*!
  id: totrans-1117
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数的数学是从这些出版物的工作中演变而来的，并且带着这个名字*核*。令人极其困惑的是，数学中包含*核*这个词的多个看似无关的概念！
- en: '|  |'
  id: totrans-1118
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'How does the algorithm find this new kernel? It uses a mathematical transformation
    of the data called a *kernel function*. There are many kernel functions to choose
    from, each of which applies a different transformation to the data and is suitable
    for finding linear decision boundaries for different situations. [Figure 6.6](#ch06fig06)
    shows examples of situations where some common kernel functions can separate non-linearly
    separable data:'
  id: totrans-1119
  prefs: []
  type: TYPE_NORMAL
  zh: 算法是如何找到这个新核的？它使用了一种称为*核函数*的数据数学变换。有许多核函数可供选择，每个核函数都对数据进行不同的变换，适用于找到不同情况下的线性决策边界。[图6.6](#ch06fig06)显示了某些常见核函数可以分离非线性可分数据的示例：
- en: Linear kernel (equivalent to no kernel)
  id: totrans-1120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性核（相当于没有核）
- en: Polynomial kernel
  id: totrans-1121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式核
- en: Gaussian radial basis kernel
  id: totrans-1122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 高斯径向基核
- en: Sigmoid kernel
  id: totrans-1123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Sigmoid核
- en: The type of kernel function for a given problem isn’t learned from the data—we
    have to specify it. Because of this, the choice of kernel function is a *categorical*
    hyperparameter (a hyperparameter that takes discrete, not continuous values).
    Therefore, the best approach for choosing the best-performing kernel is with hyperparameter
    tuning.
  id: totrans-1124
  prefs: []
  type: TYPE_NORMAL
  zh: 对于给定问题的核函数类型并不是从数据中学习得到的——我们必须指定它。正因为如此，核函数的选择是一个**分类**超参数（一个取离散值而不是连续值的超参数）。因此，选择最佳性能核函数的最佳方法是进行超参数调整。
- en: Figure 6.6\. Examples of kernel functions. For each example, the solid line
    indicates the decision boundary (projected back onto the original feature space),
    and the dashed lines indicate the margin. With the exception of the linear kernel,
    imagine that the cases of one of the groups are raised off the page in a third
    dimension.
  id: totrans-1125
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.6\. 核函数的示例。对于每个示例，实线表示决策边界（投影回原始特征空间），虚线表示边界。除了线性核外，想象一下其中一个组的一个案例在第三维中被抬离页面。
- en: '![](fig6-6_alt.jpg)'
  id: totrans-1126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](fig6-6_alt.jpg)'
- en: 6.4.4\. Hyperparameters of the SVM algorithm
  id: totrans-1127
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.4\. SVM算法的超参数
- en: This is where SVMs become fun/difficult/painful, depending on your problem,
    computational budget, and sense of humor. We need to tune quite a lot of hyperparameters
    when building an SVM. This, coupled with the fact that training a single model
    can be moderately expensive, can make training an optimally performing SVM take
    quite a long time. You’ll see this in the worked example in [section 6.5.2](#ch06lev2sec12).
  id: totrans-1128
  prefs: []
  type: TYPE_NORMAL
  zh: 这取决于你的问题、计算预算和幽默感，SVM变得有趣/困难/痛苦。在构建SVM时，我们需要调整相当多的超参数。这，加上训练单个模型可能相当昂贵的事实，可能会使训练一个最优性能的SVM花费相当长的时间。你将在[第6.5.2节](#ch06lev2sec12)的工作示例中看到这一点。
- en: 'So the SVM algorithm has quite a few hyperparameters to tune, but the most
    important ones to consider are as follows:'
  id: totrans-1129
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM算法有很多超参数需要调整，但最重要的考虑因素如下：
- en: The *kernel* hyperparameter (shown in [figure 6.6](#ch06fig06))
  id: totrans-1130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核*超参数（如图6.6所示）'
- en: The *degree* hyperparameter, which controls how “bendy” the decision boundary
    will be for the polynomial kernel (shown in [figure 6.6](#ch06fig06))
  id: totrans-1131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度*超参数，它控制多项式核的决策边界将有多“弯曲”（如图6.6所示）'
- en: The *cost* or *C* hyperparameter, which controls how “hard” or “soft” the margin
    is (shown in [figure 6.7](#ch06fig07))
  id: totrans-1132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本*或*C*超参数，它控制边界有多“硬”或“软”（如图6.7所示）'
- en: The *gamma* hyperparameter, which controls how much influence individual cases
    have on the position of the decision boundary (shown in [figure 6.7](#ch06fig07))
  id: totrans-1133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*gamma*超参数控制单个案例对决策边界位置的影响程度（如图6.7所示）'
- en: The effects of the kernel function and *degree* hyperparameter are shown in
    [figure 6.6](#ch06fig06). Note the difference in the shape of the decision boundary
    between the second- and third-degree polynomials.
  id: totrans-1134
  prefs: []
  type: TYPE_NORMAL
  zh: 核函数和*degree*超参数的影响在[图6.6](#ch06fig06)中展示。注意二次和三次多项式决策边界的形状差异。
- en: '|  |'
  id: totrans-1135
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1136
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The higher the degree of the polynomial, the more bendy and complex a decision
    boundary can be learned, but this has the potential to overfit the training set.
  id: totrans-1137
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式的度数越高，学习的决策边界可以越弯曲和复杂，但这可能导致过拟合训练集。
- en: '|  |'
  id: totrans-1138
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The *cost* (also called *C*) hyperparameter in soft-margin SVMs assigns a cost
    or penalty to having cases inside the margin or, put another way, tells the algorithm
    how bad it is to have cases inside the margin. A low cost tells the algorithm
    that it’s acceptable to have more cases inside the margin and will result in wider
    margins that are less influenced by local differences near the class boundary.
    A high cost imposes a harsher penalty on having cases inside the boundary and
    will result in narrower margins that are more influenced by local differences
    near the class boundary. The effect of *cost* is illustrated for a linear kernel
    in the top part of [figure 6.6](#ch06fig06).
  id: totrans-1139
  prefs: []
  type: TYPE_NORMAL
  zh: 软间隔SVM中的*cost*（也称为*C*）超参数将成本或惩罚分配给边缘内的案例，或者换句话说，告诉算法边缘内案例有多糟糕。低成本告诉算法边缘内可以有更多案例，这将导致更宽的间隔，受类别边界附近局部差异的影响较小。高成本对边缘内案例施加更严厉的惩罚，将导致更窄的间隔，受类别边界附近局部差异的影响较大。*cost*的影响在[图6.6](#ch06fig06)的顶部部分展示。
- en: '|  |'
  id: totrans-1140
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1141
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Cases inside the margin are also support vectors, as moving them would change
    the position of the hyperplane.
  id: totrans-1142
  prefs: []
  type: TYPE_NORMAL
  zh: 边缘内的案例也是支持向量，因为移动它们会改变超平面的位置。
- en: '|  |'
  id: totrans-1143
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The *gamma* hyperparameter controls the influence that each case has on the
    position of the hyperplane and is used by all the kernel functions except the
    linear kernel. Think of each case in the training set jumping up and down shouting,
    “Me! Me! Classify me correctly!” The larger *gamma* is, the more attention-seeking
    each case is, and the more granular the decision boundary will be (potentially
    leading to overfitting). The smaller *gamma* is, the less attention-seeking each
    case will be, and the less granular the decision boundary will be (potentially
    leading to underfitting). The effect of *gamma* is illustrated for a Gaussian
    radial basis kernel in the bottom part of [figure 6.7](#ch06fig07).
  id: totrans-1144
  prefs: []
  type: TYPE_NORMAL
  zh: '*gamma*超参数控制每个案例对超平面位置的影响，除了线性核以外的所有核函数都会使用它。想象训练集中的每个案例都在上下跳跃，大声喊道，“我！我！正确分类我！”*gamma*值越大，每个案例越想吸引注意，决策边界将越细粒度（可能导致过拟合）。*gamma*值越小，每个案例越不吸引注意，决策边界将越粗粒度（可能导致欠拟合）。*gamma*的影响在[图6.7](#ch06fig07)的底部部分展示。'
- en: Figure 6.7\. The impact of the cost and *gamma* hyperparameters. Larger values
    of the *cost* hyperparameter give greater penalization for having cases inside
    the margin. Larger values of the *gamma* hyperparameter mean individual cases
    have greater influence on the position of the decision boundary, leading to more
    complex decision boundaries.
  id: totrans-1145
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.7. 成本和*gamma*超参数的影响。*成本*超参数的值越大，对边缘内案例的惩罚越重。*gamma*超参数的值越大，意味着单个案例对决策边界的位置影响越大，导致决策边界更加复杂。
- en: '![](fig6-7.jpg)'
  id: totrans-1146
  prefs: []
  type: TYPE_IMG
  zh: '![](fig6-7.jpg)'
- en: So the SVM algorithm has multiple hyperparameters to tune! I’ll show you how
    we can tune these simultaneously using mlr in [section 6.5.2](#ch06lev2sec12).
  id: totrans-1147
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，SVM算法有多个超参数需要调整！我将向您展示如何使用mlr同时调整这些超参数，请参阅[第6.5.2节](#ch06lev2sec12)。
- en: 6.4.5\. What if we have more than two classes?
  id: totrans-1148
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.4.5. 如果我们有多于两个类别怎么办？
- en: 'So far, I’ve only shown you examples of two-class classification problems.
    This is because the SVM algorithm is inherently geared toward separating two classes.
    But can we use it for multiclass problems (where we’re trying to predict more
    than two classes)? Absolutely! When there are more than two classes, instead of
    creating a single SVM, we make multiple models and let them fight it out to predict
    the most likely class for new data. There are two ways of doing this:'
  id: totrans-1149
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我只展示了二元分类问题的例子。这是因为SVM算法本质上是为了分离两个类别而设计的。但我们能否用它来解决多类问题（我们试图预测多于两个类别的情况）？当然可以！当有多个类别时，我们不是创建一个单一的SVM，而是创建多个模型，并让它们竞争以预测新数据的可能性最大的类别。有两种方法可以实现这一点：
- en: One-versus-all
  id: totrans-1150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一对抗所有
- en: One-versus-one
  id: totrans-1151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一对一
- en: In the one-versus-all (also called *one versus rest*) approach, we create as
    many SVM models as there are classes. Each SVM model describes a hyperplane that
    best separates one class from *all the other classes*. Hence the name, one-versus-all.
    When we classify new, unseen cases, the models play a game of *winner takes all*.
    Put simply, the model that puts the new case on the “correct” side of its hyperplane
    (the side with the class it separates from all the others) wins. The case is then
    assigned to the class that the model was trying to separate from the others. This
    is illustrated in the plot on the left in [figure 6.8](#ch06fig08).
  id: totrans-1152
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对一（也称为一对一对抗所有）方法中，我们创建与类别数量相等的SVM模型。每个SVM模型描述了一个最佳的超平面，该超平面能够将一个类别与所有其他类别分开。因此得名一对一对抗所有。当我们对新的、未见过的案例进行分类时，模型会玩一场“赢家通吃”的游戏。简单来说，将新案例放在其超平面的“正确”一侧（与所有其他类别分开的那一侧）的模型获胜。然后，该案例被分配给模型试图与之分开的类别。这如图6.8中的左侧所示。
- en: Figure 6.8\. One-versus-all and one-versus-one approaches to multiclass SVMs.
    In the one-versus-all approach, a hyperplane is learned per class, separating
    it from all the other cases. In the one-versus-one approach, a hyperplane is learned
    for every pair of classes, separating them while ignoring the data from the other
    classes.
  id: totrans-1153
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.8\. 多类SVM的一对一和一对一方法。在一对一方法中，为每个类别学习一个超平面，将其与其他所有案例分开。在一对一方法中，为每一对类别学习一个超平面，在忽略其他类别的数据的同时将它们分开。
- en: '![](fig6-8.jpg)'
  id: totrans-1154
  prefs: []
  type: TYPE_IMG
  zh: '![图6-8](fig6-8.jpg)'
- en: In the one-versus-one approach, we create an SVM model for every pair of classes.
    Each SVM model describes a hyperplane that best separates one class from *one
    other class*, ignoring data from the other classes. Hence, the name, one-versus-one.
    When we classify new, unseen cases, each model casts a vote. For example, if one
    model separates classes A and B, and the new data falls on the B side of the decision
    boundary, that model will vote for B. This continues for all the models, and the
    majority class vote wins. This is illustrated in the plot on the right in [figure
    6.8](#ch06fig08).
  id: totrans-1155
  prefs: []
  type: TYPE_NORMAL
  zh: 在一对一方法中，我们为每一对类别创建一个SVM模型。每个SVM模型描述了一个最佳的超平面，该超平面能够将一个类别与另一个类别分开，而忽略其他类别的数据。因此，得名一对一。当我们对新的、未见过的案例进行分类时，每个模型都会投一票。例如，如果一个模型将类别A和B分开，并且新的数据落在决策边界的B侧，那么这个模型将投票给B。这个过程会持续对所有模型进行，多数类别的投票获胜。这如图6.8中的右侧所示。
- en: Which do we choose? Well in practice, there is usually little difference in
    the performance of the two methods. Despite training more models (for more than
    three classes), one-versus-one is sometimes less computationally expensive than
    one-versus-all. This is because, although we’re training more models, the training
    sets are smaller (because of the ignored cases). The implementation of the SVM
    algorithm called by mlr uses the one-versus-one approach.
  id: totrans-1156
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该选择哪一种？实际上，两种方法的性能通常没有太大差异。尽管训练了更多的模型（对于多于三个类别），但一对一有时比一对一对抗所有在计算上更节省。这是因为，尽管我们训练了更多的模型，但训练集更小（因为忽略了案例）。mlr调用的SVM算法实现使用一对一方法。
- en: There is, however, a problem with these approaches. There will often be regions
    of the feature space in which none of the models gives a clear winning class.
    Can you see the triangular space between the hyperplanes on the left in [figure
    6.8](#ch06fig08)? If a new case appeared inside this triangle, none of the three
    models would clearly win outright. This is a sort of classification no-man’s land.
    Though not as obvious in [figure 6.8](#ch06fig08), this also occurs with the one-versus-one
    approach.
  id: totrans-1157
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这些方法存在一个问题。特征空间中往往会有一些区域，其中没有任何一个模型给出明确的胜出类别。你能在[图6.8](#ch06fig08)中看到超平面之间的三角形空间吗？如果一个新的案例出现在这个三角形内部，三个模型中的任何一个都不会明显胜出。这是一种分类的无人区。虽然在[图6.8](#ch06fig08)中不是很明显，这种情况也出现在单对一方法中。
- en: 'If there is no outright winner when predicting a new case, a technique called
    *Platt scaling* is used (named after computer scientist John Platt). Platt scaling
    takes the distances of the cases from each hyperplane and converts them into probabilities
    using the logistic function. Recall from [chapter 4](kindle_split_014.html#ch04)
    that the logistic function maps a continuous variable to probabilities between
    0 and 1\. Using Platt scaling to make predictions proceeds like this:'
  id: totrans-1158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果在预测新案例时没有明确的胜者，将使用一种称为*Platt缩放*的技术（以计算机科学家约翰·Platt命名）。Platt缩放法将每个案例到每个超平面的距离转换为概率，使用逻辑函数。回想一下[第4章](kindle_split_014.html#ch04)中提到的逻辑函数将连续变量映射到0到1之间的概率。使用Platt缩放法进行预测的过程如下：
- en: 'For every hyperplane (whether we use one-versus-all or one-versus-one):'
  id: totrans-1159
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个超平面（无论我们使用单对多还是单对一）：
- en: Measure the distance of each case from the hyperplane.
  id: totrans-1160
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测量每个案例到超平面的距离。
- en: Use the logistic function to convert these distances into probabilities.
  id: totrans-1161
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用逻辑函数将这些距离转换为概率。
- en: Classify new data as belonging to the class of the hyperplane that has the highest
    probability.
  id: totrans-1162
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新数据分类为具有最高概率的超平面所属的类别。
- en: If this seems confusing, take a look at [figure 6.9](#ch06fig09). We’re using
    the one-versus-all approach in the figure, and we have generated three separate
    hyperplanes (one to separate each class from the rest). The dashed arrows in the
    figure indicate distance in either direction, away from the hyperplanes. Platt
    scaling converts these distances into probabilities using the logistic function
    (the class each hyperplane separates from the rest has positive distance).
  id: totrans-1163
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这看起来很困惑，请查看[图6.9](#ch06fig09)。图中我们采用了单对多方法，并生成了三个独立的超平面（每个超平面用于将每个类别与其它类别分开）。图中的虚线箭头表示距离，即从超平面出发的任意方向的距离。Platt缩放法使用逻辑函数将这些距离转换为概率（每个超平面将其它类别分开时具有正距离）。
- en: When we classify new, unseen data, the distance of the new data is converted
    into a probability using each of the three S-shaped curves, and the case is classified
    as the one that gives the highest probability. Handily, all of this is taken care
    of for us in the implementation of SVM called by mlr. If we supply a three-class
    classification task, we will get a one-versus-one SVM model with Platt scaling
    without having to change our code.
  id: totrans-1164
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们分类新的、未见过的数据时，新数据的距离会使用三个S形曲线中的每一个转换为概率，并将案例分类为给出最高概率的那个。方便的是，所有这些在mlr中调用SVM的实现时都为我们处理好了。如果我们提供一个三分类任务，我们将得到一个带有Platt缩放的单对一SVM模型，而无需更改我们的代码。
- en: Figure 6.9\. How Platt scaling is used to get probabilities for each hyperplane.
    This example shows a one-versus-all approach (it also applies to one-versus-one).
    For each hyperplane, the distance of each case from the hyperplane is recorded
    (indicated by double-headed arrows). These distances are converted into probabilities
    using the logistic function.
  id: totrans-1165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图6.9。如何使用Platt缩放法为每个超平面获取概率。这个例子展示了单对多方法（它也适用于单对一）。对于每个超平面，记录每个案例到超平面的距离（由双头箭头表示）。这些距离使用逻辑函数转换为概率。
- en: '![](fig6-9_alt.jpg)'
  id: totrans-1166
  prefs: []
  type: TYPE_IMG
  zh: '![图片6-9](fig6-9_alt.jpg)'
- en: 6.5\. Building your first SVM model
  id: totrans-1167
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.5. 构建你的第一个SVM模型
- en: In this section, I’ll teach you how to build an SVM model and tune multiple
    hyperparameters simultaneously. Imagine that you’re sick and tired of receiving
    so many spam emails (maybe you don’t need to imagine!). It’s difficult for you
    to be productive because you get so many emails requesting your bank details for
    a mysterious Ugandan inheritance, and trying to sell you Viagra.
  id: totrans-1168
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你如何构建SVM模型并同时调整多个超参数。想象一下，你厌倦了收到如此多的垃圾邮件（也许你不需要想象！）！由于你收到了许多要求你提供银行详情以获取神秘的乌干达遗产的邮件，以及试图向你推销伟哥的邮件，你很难保持生产力。
- en: You decide to perform a feature extraction on the emails you receive over a
    few months, which you manually class as spam or not spam. These features include
    things like the number of exclamation marks and the frequency of certain words.
    With this data, you want to make an SVM that you can use as a spam filter, which
    will classify new emails as spam or not spam.
  id: totrans-1169
  prefs: []
  type: TYPE_NORMAL
  zh: 你决定对你收到的几个月的电子邮件进行特征提取，这些电子邮件你手动分类为垃圾邮件或非垃圾邮件。这些特征包括感叹号的数量和某些单词的频率等。有了这些数据，你想要制作一个SVM，可以用作垃圾邮件过滤器，将新电子邮件分类为垃圾邮件或非垃圾邮件。
- en: 'In this section, you’ll learn how to train an SVM model and tune multiple hyperparameters
    simultaneously. Let’s start by loading the mlr and tidyverse packages:'
  id: totrans-1170
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何训练一个SVM模型并同时调整多个超参数。让我们首先加载mlr和tidyverse包：
- en: '[PRE82]'
  id: totrans-1171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 6.5.1\. Loading and exploring the spam dataset
  id: totrans-1172
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.1\. 加载和探索垃圾邮件数据集
- en: Now let’s load the data, which is built into the kernlab package, convert it
    into a tibble (with `as_tibble()`), and explore it.
  id: totrans-1173
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来加载数据，这些数据是kernlab包内置的，将其转换为tibble（使用 `as_tibble()`），并对其进行探索。
- en: '|  |'
  id: totrans-1174
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1175
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: The kernlab package should have been installed along with mlr as a suggested
    package. If you get an error when trying to load the data, you may need to install
    it with `install.packages("kernlab")`.
  id: totrans-1176
  prefs: []
  type: TYPE_NORMAL
  zh: kernlab包应该作为mlr的建议包一起安装。如果在尝试加载数据时遇到错误，你可能需要使用 `install.packages("kernlab")`
    来安装它。
- en: '|  |'
  id: totrans-1177
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: We have a tibble containing 4,601 emails and 58 variables extracted from emails.
    Our goal is to train a model that can use the information in these variables to
    predict whether a new email is spam or not.
  id: totrans-1178
  prefs: []
  type: TYPE_NORMAL
  zh: 我们有一个包含4,601封电子邮件和58个从电子邮件中提取的变量的tibble。我们的目标是训练一个模型，该模型可以使用这些变量中的信息来预测新电子邮件是否为垃圾邮件。
- en: '|  |'
  id: totrans-1179
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1180
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Except for the factor `type`, which denotes whether an email is spam, all of
    the variables are continuous, because the SVM algorithm cannot handle categorical
    predictors.
  id: totrans-1181
  prefs: []
  type: TYPE_NORMAL
  zh: 除了表示电子邮件是否为垃圾邮件的因子 `type` 之外，所有变量都是连续的，因为SVM算法无法处理分类预测变量。
- en: '|  |'
  id: totrans-1182
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 6.7\. Loading and exploring the spam dataset
  id: totrans-1183
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.7\. 加载和探索垃圾邮件数据集
- en: '[PRE83]'
  id: totrans-1184
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: '|  |'
  id: totrans-1185
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1186
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: This dataset has a lot of features! I’m not going to discuss the meaning of
    each one, but you can see a description of what they mean by running `?kernlab::spam`.
  id: totrans-1187
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有很多特征！我不会讨论每个特征的含义，但你可以通过运行 `?kernlab::spam` 来查看它们的描述。
- en: '|  |'
  id: totrans-1188
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 6.5.2\. Tuning our hyperparameters
  id: totrans-1189
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.2\. 调整我们的超参数
- en: Let’s define our task and learner. This time, we supply `"classif.svm"` as the
    argument to `makeLearner()` to specify that we’re going to use SVM.
  id: totrans-1190
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的任务和学习器。这次，我们将 `"classif.svm"` 作为 `makeLearner()` 的参数传递，以指定我们将使用SVM。
- en: Listing 6.8\. Creating the task and learner
  id: totrans-1191
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.8\. 创建任务和学习器
- en: '[PRE84]'
  id: totrans-1192
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Before we train our model, we need to tune our hyperparameters. To find out
    which hyperparameters are available for tuning for an algorithm, we simply pass
    the name of the algorithm in quotes to `getParamSet()`. For example, [listing
    6.9](#ch06ex09) shows how to print the hyperparameters for the SVM algorithm.
    I’ve removed some rows and columns of the output to make it fit, but the most
    important columns are there:'
  id: totrans-1193
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练模型之前，我们需要调整我们的超参数。要找出算法中可用于调整的超参数，我们只需将算法的名称用引号传递给 `getParamSet()`。例如，[列表6.9](#ch06ex09)
    展示了如何打印SVM算法的超参数。我已经删除了一些输出行和列以使其适合，但最重要的列都在那里：
- en: The row name is the name of the hyperparameter.
  id: totrans-1194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 行名是超参数的名称。
- en: '`Type` is whether the hyperparameter takes numeric, integer, discrete, or logical
    values.'
  id: totrans-1195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Type` 表示超参数是取数值、整数、离散值还是逻辑值。'
- en: '`Def` is the default value (the value that will be used if you don’t tune the
    hyperparameter).'
  id: totrans-1196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Def` 是默认值（如果你不调整超参数，将使用的值）。'
- en: '`Constr` defines the constraints for the hyperparameter: either a set of specific
    values or a range of acceptable values.'
  id: totrans-1197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Constr` 定义了超参数的约束：要么是一组特定值，要么是可接受值的范围。'
- en: '`Req` defines whether the hyperparameter is required by the learner.'
  id: totrans-1198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Req` 定义了超参数是否被学习器所必需。'
- en: '`Tunable` is logical and defines whether that hyperparameter can be tuned (some
    algorithms have options that cannot be tuned but can be set by the user).'
  id: totrans-1199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`Tunable` 是逻辑值，表示该超参数是否可以被调整（某些算法有用户可以设置但不能调整的选项）。'
- en: Listing 6.9\. Printing available SVM hyperparameters
  id: totrans-1200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.9\. 打印可用的SVM超参数
- en: '[PRE85]'
  id: totrans-1201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: 'The SVM algorithm is sensitive to variables being on different scales, so it’s
    usually a good idea to scale the predictors first. Notice the scale hyperparameter:
    it tells us that the algorithm will scale the data for us by default.'
  id: totrans-1202
  prefs: []
  type: TYPE_NORMAL
  zh: SVM 算法对变量处于不同尺度敏感，因此通常在首先缩放预测变量是一个好主意。注意缩放超参数：它告诉我们算法将默认为我们缩放数据。
- en: '|  |'
  id: totrans-1203
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Extracting the possible values for a hyperparameter**'
  id: totrans-1204
  prefs: []
  type: TYPE_NORMAL
  zh: '**提取超参数的可能值**'
- en: While the `getParamSet()` function is useful, I don’t find it particularly simple
    to extract information from. If you call `str(getParamSet("classif.svm"))`, you’ll
    see that it has a reasonably complex structure.
  id: totrans-1205
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然 `getParamSet()` 函数很有用，但我发现从它中提取信息并不特别简单。如果你调用 `str(getParamSet("classif.svm"))`，你会看到它有一个相当复杂的结构。
- en: 'To extract information about a particular hyperparameter, you need to call
    `getParamSet("classif.svm")$pars$[HYPERPAR]` (where `[HYPERPAR]` is replaced by
    the hyperparameter you’re interested in). To extract the possible values for that
    hyperparameter, you append `$values` to the call. For example, the following extracts
    the possible kernel functions:'
  id: totrans-1206
  prefs: []
  type: TYPE_NORMAL
  zh: 要提取有关特定超参数的信息，你需要调用 `getParamSet("classif.svm")$pars$[HYPERPAR]`（其中 `[HYPERPAR]`
    被你感兴趣的超参数所替换）。要提取该超参数的可能值，你需要在调用中附加 `$values`。例如，以下提取了可能的核函数：
- en: '[PRE86]'
  id: totrans-1207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '|  |'
  id: totrans-1208
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'These are the most important hyperparameters for us to tune:'
  id: totrans-1209
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是我们需要调整的最重要超参数：
- en: '*Kernel*'
  id: totrans-1210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核*'
- en: '*Cost*'
  id: totrans-1211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本*'
- en: '*Degree*'
  id: totrans-1212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度数*'
- en: '*Gamma*'
  id: totrans-1213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Gamma*'
- en: '[Listing 6.10](#ch06ex10) defines the hyperparameters we want to tune. We’re
    going to start by defining a vector of kernel functions we wish to tune.'
  id: totrans-1214
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 6.10](#ch06ex10) 定义了我们想要调整的超参数。我们将从定义一个我们希望调整的核函数向量开始。'
- en: '|  |'
  id: totrans-1215
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Notice that I omit the linear kernel. This is because the linear kernel is the
    same as the polynomial kernel with *degree* = 1, so we’ll just make sure we include
    1 as a possible value for the *degree* hyperparameter. Including the linear kernel
    *and* the first-degree polynomial kernel is simply a waste of computing time.
  id: totrans-1217
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我省略了线性核。这是因为线性核与 *度数* = 1 的多项式核相同，所以我们将确保将 1 作为 *度数* 超参数的可能值之一。包括线性核和一阶多项式核纯粹是浪费计算时间。
- en: '|  |'
  id: totrans-1218
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Next, we use the `makeParamSet()` function to define the hyperparameter space
    we wish to tune over. To the `makeParamSet()` function, we supply the information
    needed to define each hyperparameter we wish to tune, separated by commas. Let’s
    break this down line by line:'
  id: totrans-1219
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `makeParamSet()` 函数来定义我们希望调整的超参数空间。我们将定义我们希望调整的每个超参数所需的信息提供给 `makeParamSet()`
    函数，用逗号分隔。让我们逐行分析：
- en: The *kernel* hyperparameter takes discrete values (the name of the kernel function),
    so we use the `makeDiscreteParam()` function to define its values as the vector
    of kernels we created.
  id: totrans-1220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*核* 超参数取离散值（核函数的名称），因此我们使用 `makeDiscreteParam()` 函数来定义其值为我们所创建的核函数向量。'
- en: The *degree* hyperparameter takes integer values (whole numbers), so we use
    the `makeIntegerParam()` function and define the lower and upper values we wish
    to tune over.
  id: totrans-1221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*度数* 超参数取整数值（整数），因此我们使用 `makeIntegerParam()` 函数并定义我们希望调整的下限和上限值。'
- en: The *cost* and *gamma* hyperparameters take numeric values (any number between
    zero and infinity), so we use the `makeNumericParam()` function to define the
    lower and upper values we wish to tune over.
  id: totrans-1222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*成本* 和 *gamma* 超参数取数值（零和无穷大之间的任何数），因此我们使用 `makeNumericParam()` 函数来定义我们希望调整的下限和上限值。'
- en: For each of these functions, the first argument is the name of the hyperparameter
    given by `getParamSet("classif.svm")`, in quotes.
  id: totrans-1223
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这些函数中的每一个，第一个参数是由 `getParamSet("classif.svm")` 给出的超参数名称，用引号括起来。
- en: Listing 6.10\. Defining the hyperparameter space for tuning
  id: totrans-1224
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.10\. 定义调整超参数的空间
- en: '[PRE87]'
  id: totrans-1225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: 'Cast your mind back to [chapter 3](kindle_split_013.html#ch03), when we tuned
    *k* for the kNN algorithm. We used the grid search procedure during tuning to
    try every value of *k* that we defined. This is what the grid search method does:
    it tries every combination of the hyperparameter space you define and finds the
    best-performing combination.'
  id: totrans-1226
  prefs: []
  type: TYPE_NORMAL
  zh: 想想 [第 3 章](kindle_split_013.html#ch03)，当我们为 kNN 算法调整 *k* 时。我们在调整过程中使用了网格搜索过程来尝试我们定义的
    *k* 的每一个值。这就是网格搜索方法所做的事情：它尝试你定义的超参数空间中的每一个组合，并找到性能最佳的组合。
- en: Grid search is great because, provided you specify a sensible hyperparameter
    space to search over, it will always find the best-performing hyperparameters.
    But look at the hyperparameter space we defined for our SVM. Let’s say we wanted
    to try values for the *cost* and *gamma* hyperparameters from 0.1 to 10, in steps
    of 0.1 (that’s 100 values of each). We’re trying three kernel functions and three
    values of the *degree* hyperparameter. To perform a grid search over this parameter
    space would require training a model 90,000 times! In such a situation, if you
    have the time, patience, and computational budget for such a grid search, then
    good for you. I, for one, have better things I could be doing with my computer!
  id: totrans-1227
  prefs: []
  type: TYPE_NORMAL
  zh: 网格搜索非常出色，因为只要你指定一个合理的超参数搜索空间，它总是会找到最佳性能的超参数。但看看我们为我们的SVM定义的超参数空间。假设我们想要尝试从0.1到10的*cost*和*gamma*超参数的值，步长为0.1（这意味着每个参数有100个值）。我们正在尝试三种核函数和*degree*超参数的三个值。要在这样的参数空间上进行网格搜索，需要训练模型90,000次！在这种情况下，如果你有足够的时间、耐心和计算预算来进行这样的网格搜索，那么恭喜你。至少对我来说，我可以用我的电脑做更有意义的事情！
- en: 'Instead, we can employ a technique called *random search*. Rather than trying
    every possible combination of parameters, random search proceeds as follows:'
  id: totrans-1228
  prefs: []
  type: TYPE_NORMAL
  zh: 相反，我们可以采用一种称为*随机搜索*的技术。随机搜索不是尝试所有可能的参数组合，而是按照以下步骤进行：
- en: Randomly select a combination of hyperparameter values.
  id: totrans-1229
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 随机选择一个超参数值组合。
- en: Use cross-validation to train and evaluate a model using those hyperparameter
    values.
  id: totrans-1230
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用交叉验证来训练和评估使用这些超参数值的模型。
- en: Record the performance metric of the model (usually mean misclassification error
    for classification tasks).
  id: totrans-1231
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 记录模型的性能指标（通常对于分类任务是平均误分类错误）。
- en: Repeat (iterate) steps 1 to 3 as many times as your computational budget allows.
  id: totrans-1232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复（迭代）步骤1到3，直到你的计算预算允许。
- en: Select the combination of hyperparameter values that gave you the best-performing-
    model.
  id: totrans-1233
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择给你带来最佳性能模型的超参数值组合。
- en: Unlike grid search, random search isn’t guaranteed to find the best set of hyperparameter
    values. However, with enough iterations, it can usually find a good combination
    that performs well. By using random search, we can run 500 combinations of hyperparameter
    values, instead of all 90,000 combinations.
  id: totrans-1234
  prefs: []
  type: TYPE_NORMAL
  zh: 与网格搜索不同，随机搜索并不能保证找到最佳的超参数值组合。然而，随着迭代次数的增加，它通常可以找到一个表现良好的组合。通过使用随机搜索，我们可以运行500个超参数值的组合，而不是全部90,000个组合。
- en: Let’s define our random search using the `makeTuneControlRandom()` function.
    We use the `maxit` argument to tell the function how many iterations of the random
    search procedure we want to use. You should try to set this as high as your computational
    budget allows, but in this example we’ll stick to 20 to prevent the example from
    taking too long. Next, we describe our cross-validation procedure. Remember I
    said in [chapter 3](kindle_split_013.html#ch03) that I prefer k-fold cross-validation
    unless the process is computationally expensive. Well, this is computationally
    expensive, so we’re compromising by using holdout cross-validation instead.
  id: totrans-1235
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用`makeTuneControlRandom()`函数来定义我们的随机搜索。我们使用`maxit`参数来告诉函数我们想要使用多少次随机搜索过程的迭代。你应该尽量设置这个值，但要考虑到你的计算预算，在这个例子中，我们将坚持使用20次以防止示例运行时间过长。接下来，我们将描述我们的交叉验证过程。记得我在[第3章](kindle_split_013.html#ch03)中提到，除非过程计算成本高昂，否则我更喜欢k折交叉验证。但是，这个过程计算成本很高，所以我们妥协使用保留交叉验证。
- en: Listing 6.11\. Defining the random search
  id: totrans-1236
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表6.11\. 定义随机搜索
- en: '[PRE88]'
  id: totrans-1237
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: There’s something else we can do to speed up this process. R, as a language,
    doesn’t make that much use of multithreading (using multiple CPUs simultaneously
    to accomplish a task). However, one of the benefits of the mlr package is that
    it allows multithreading to be used with its functions. This helps you use multiple
    cores/CPUs on your computer to accomplish tasks such as hyperparameter tuning
    and cross-validation much more quickly.
  id: totrans-1238
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以采取其他措施来加快这一过程。R语言作为一种语言，并不大量使用多线程（同时使用多个CPU来完成任务）。然而，mlr包的一个好处是它允许在函数中使用多线程。这可以帮助你使用计算机上的多个核心/CPU来更快地完成超参数调整和交叉验证等任务。
- en: '|  |'
  id: totrans-1239
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1240
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: If you don’t know how many cores your computer has, you can find out in R by
    running `parallel::detectCores()`. (If your computer only has one core, the 90s
    called—they want their computer back.)
  id: totrans-1241
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你不知道你的计算机有多少个核心，你可以在R中运行`parallel::detectCores()`来找出（如果你的计算机只有一个核心，90秒后——他们想要他们的电脑回来）。
- en: '|  |'
  id: totrans-1242
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'To run an mlr process in parallel, we place its code between the `parallelStartSocket()`
    and `parallelStop()` functions from the parallelMap package. To start our hyperparameter
    tuning process, we call the `tuneParams()` function and supply the following as
    arguments:'
  id: totrans-1243
  prefs: []
  type: TYPE_NORMAL
  zh: 要并行运行一个mlr过程，我们将它的代码放在parallelMap包中的`parallelStartSocket()`和`parallelStop()`函数之间。为了开始我们的超参数调整过程，我们调用`tuneParams()`函数并传递以下参数：
- en: First argument = name of the learner
  id: totrans-1244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数 = 学习者的名称
- en: '`task` = name of our task'
  id: totrans-1245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`task` = 我们任务的名称'
- en: '`resampling` = cross-validation procedure (defined in [listing 6.11](#ch06ex11))'
  id: totrans-1246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resampling` = 交叉验证过程（定义在[列表 6.11](#ch06ex11)）'
- en: '`par.set` = hyperparameter space (defined in [listing 6.10](#ch06ex10))'
  id: totrans-1247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`par.set` = 超参数空间（定义在[列表 6.10](#ch06ex10)）'
- en: '`control` = search procedure (random search, defined in [listing 6.11](#ch06ex11))'
  id: totrans-1248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control` = 搜索过程（随机搜索，定义在[列表 6.11](#ch06ex11)）'
- en: This code between the `parallelStartSocket()` and `parallelStop()` functions
    is shown in [listing 6.12](#ch06ex12). Notice that the downside of running cross-validation
    processes in parallel is that we no longer get a running update of how far we’ve
    got.
  id: totrans-1249
  prefs: []
  type: TYPE_NORMAL
  zh: 在`parallelStartSocket()`和`parallelStop()`函数之间的代码显示在[列表 6.12](#ch06ex12)中。请注意，并行运行交叉验证过程的缺点是我们不再能获得进度更新的实时信息。
- en: '|  |'
  id: totrans-1250
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Warning
  id: totrans-1251
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: The computer I’m writing this on has four cores, and this code takes nearly
    a minute to run on it. It is of the utmost importance that you go make a cup of
    tea while it runs. Milk and no sugar, please.
  id: totrans-1252
  prefs: []
  type: TYPE_NORMAL
  zh: 我编写这段代码的电脑有四个核心，这段代码在这台电脑上运行几乎需要一分钟。在它运行的时候，你最好去泡一杯茶。请加牛奶，不加糖。
- en: '|  |'
  id: totrans-1253
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Listing 6.12\. Performing hyperparameter tuning
  id: totrans-1254
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.12\. 执行超参数调整
- en: '[PRE89]'
  id: totrans-1255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '|  |'
  id: totrans-1256
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1257
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: The *degree* hyperparameter only applies to the polynomial kernel function,
    and the *gamma* hyperparameter doesn’t apply to the linear kernel. Does this create
    errors when the random search selects combinations that don’t make sense? Nope.
    If the random search selects the sigmoid kernel, for example, it simply ignores
    the value of the *degree* hyperparameter.
  id: totrans-1258
  prefs: []
  type: TYPE_NORMAL
  zh: '*degree* 超参数仅适用于多项式核函数，而*gamma*超参数不适用于线性核。这会在随机搜索选择不合理的组合时创建错误吗？不。如果随机搜索选择了sigmoid核，例如，它将简单地忽略*degree*超参数的值。'
- en: '|  |'
  id: totrans-1259
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Welcome back after our interlude! You can print the best-performing hyperparameter
    values and the performance of the model built with them by calling `tunedSvm`,
    or extract just the named values (so you can train a new model using them) by
    calling `tunedSvm$x`. Looking at the following listing, we can see that the first-degree
    polynomial kernel function (equivalent to the linear kernel function) gave the
    model that performs the best, with a *cost* of 5.8 and *gamma* of 1.56.
  id: totrans-1260
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的插曲之后欢迎回来！你可以通过调用`tunedSvm`来打印最佳性能的超参数值和用它们构建的模型的性能，或者通过调用`tunedSvm$x`来提取仅命名的值（这样你可以使用它们来训练一个新的模型）。查看以下列表，我们可以看到一阶多项式核函数（相当于线性核函数）给出了性能最佳的模型，其*成本*为
    5.8，*gamma* 为 1.56。
- en: Listing 6.13\. Extracting the winning hyperparameter values from tuning
  id: totrans-1261
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.13\. 从调整中提取获胜的超参数值
- en: '[PRE90]'
  id: totrans-1262
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Your values are probably different than mine. This is the nature of the random
    search: it may find different winning combinations of hyperparameter values each
    time it’s run. To reduce this variance, we should commit to increasing the number
    of iterations the search makes.'
  id: totrans-1263
  prefs: []
  type: TYPE_NORMAL
  zh: 你的值可能和我不同。这就是随机搜索的本质：每次运行时，它可能会找到不同的超参数值的获胜组合。为了减少这种差异，我们应该承诺增加搜索的迭代次数。
- en: 6.5.3\. Training the model with the tuned hyperparameters
  id: totrans-1264
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 6.5.3\. 使用调整后的超参数训练模型
- en: Now that we’ve tuned our hyperparameters, let’s build our model using the best-performing
    combination. Recall from [chapter 3](kindle_split_013.html#ch03) that we use the
    `setHyperPars()` function to combine a learner with a set of predefined hyperparameter
    values. The first argument is the learner we want to use, and the `par.vals` argument
    is the object containing our tuned hyperparameter values. We then train a model
    using our `tunedSvm` learner with the `train()` function.
  id: totrans-1265
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调整了超参数，让我们使用表现最好的组合来构建我们的模型。回想一下[第 3 章](kindle_split_013.html#ch03)，我们使用`setHyperPars()`函数将一个学习者和一组预定义的超参数值结合起来。第一个参数是我们想要使用的学习者，而`par.vals`参数是包含我们调整后的超参数值的对象。然后我们使用`train()`函数和我们的`tunedSvm`学习者来训练一个模型。
- en: Listing 6.14\. Training the model with tuned hyperparameters
  id: totrans-1266
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.14\. 使用调整后的超参数训练模型
- en: '[PRE91]'
  id: totrans-1267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE91]'
- en: '|  |'
  id: totrans-1268
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1269
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Because we already defined our learner in [listing 6.8](#ch06ex08), we could
    simply have run `setHyperPars(svm, par.vals = tunedSvmPars$x)` to achieve the
    same result.
  id: totrans-1270
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经在[列表 6.8](#ch06ex08)中定义了我们的学习器，我们可以简单地运行 `setHyperPars(svm, par.vals =
    tunedSvmPars$x)` 来达到相同的结果。
- en: '|  |'
  id: totrans-1271
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 6.6\. Cross-validating our SVM model
  id: totrans-1272
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.6\. 交叉验证我们的 SVM 模型
- en: We’ve built a model using tuned hyperparameters. In this section, we’ll cross-validate
    the model to estimate how it will perform on new, unseen data.
  id: totrans-1273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经使用调整过的超参数构建了一个模型。在本节中，我们将交叉验证该模型以估计它在新的、未见过的数据上的表现。
- en: Recall from [chapter 3](kindle_split_013.html#ch03) that it’s important to cross-validate
    *the entire model-building process*. This means any *data-dependent* steps in
    our model-building process (such as hyperparameter tuning) need to be included
    in our cross-validation. If we don’t include them, our cross-validation is likely
    to give an overoptimistic estimate (a *biased* estimate) of how well the model
    will perform.
  id: totrans-1274
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第 3 章](kindle_split_013.html#ch03)回忆一下，交叉验证 *整个模型构建过程* 是很重要的。这意味着我们模型构建过程中的任何
    *数据相关* 步骤（例如超参数调整）都需要包含在我们的交叉验证中。如果我们不包括它们，我们的交叉验证很可能会给出一个过于乐观的估计（一个 *有偏* 估计）关于模型将如何表现。
- en: '|  |'
  id: totrans-1275
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: What counts as a data-*independent* step in model building? Things like removing
    nonsense variables by hand, changing variable names and types, and replacing a
    missing value code with `NA`. These steps are data-independent because they would
    be the same regardless of the values in the data.
  id: totrans-1277
  prefs: []
  type: TYPE_NORMAL
  zh: 在模型构建中，什么算作一个数据-*独立*步骤？例如，手动删除无意义变量、更改变量名称和类型，以及用 `NA` 替换缺失值代码。这些步骤是数据独立的，因为无论数据中的值如何，它们都会是相同的。
- en: '|  |'
  id: totrans-1278
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Recall also that to include hyperparameter tuning in our cross-validation, we
    need to use a *wrapper function* that wraps together our learner and hyperparameter
    tuning process. The cross-validation process is shown in [listing 6.15](#ch06ex15).
  id: totrans-1279
  prefs: []
  type: TYPE_NORMAL
  zh: 还要记住，为了将超参数调整纳入我们的交叉验证，我们需要使用一个 *包装函数*，该函数将我们的学习器和超参数调整过程包装在一起。交叉验证过程在[列表 6.15](#ch06ex15)中展示。
- en: Because mlr will use nested cross-validation (where hyperparameter tuning is
    performed in the inner loop, and the winning combination of values is passed to
    the outer loop), we first define our outer cross-validation strategy using the
    `makeResamplDesc()` function. In this example, I’ve chosen 3-fold cross-validation
    for the outer loop. For the inner loop, we’ll use the `cvForTuning` resampling
    description defined in [listing 6.11](#ch06ex11) (holdout cross-validation with
    a 2/3 split).
  id: totrans-1280
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 mlr 将使用嵌套交叉验证（其中超参数调整在内循环中执行，获胜的值组合传递到外循环），我们首先使用 `makeResamplDesc()` 函数定义我们的外部交叉验证策略。在这个例子中，我选择了外循环的
    3 折交叉验证。对于内循环，我们将使用在[列表 6.11](#ch06ex11)中定义的 `cvForTuning` 交叉验证描述（保留 2/3 的数据作为验证集）。
- en: 'Next, we make our wrapped learner using the `makeTuneWrapper()` function. The
    arguments are as follows:'
  id: totrans-1281
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用 `makeTuneWrapper()` 函数构建我们的包装学习器。参数如下：
- en: First argument = name of the learner
  id: totrans-1282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 第一个参数 = 学习器的名称
- en: '`resampling` = inner loop cross-validation strategy'
  id: totrans-1283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`resampling` = 内循环交叉验证策略'
- en: '`par.set` = hyperparameter space (defined in [listing 6.10](#ch06ex10))'
  id: totrans-1284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`par.set` = 超参数空间（在[列表 6.10](#ch06ex10)中定义）'
- en: '`control` = search procedure (defined in [listing 6.11](#ch06ex11))'
  id: totrans-1285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`control` = 搜索过程（在[列表 6.11](#ch06ex11)中定义）'
- en: As the cross-validation will take a while, it’s prudent to start parallelization
    with the `parallelStartSocket()` function. Now, to run our nested cross-validation,
    we call the `resample()` function, where the first argument is our wrapped learner,
    the second argument is our task, and the third argument is our outer cross-validation
    strategy.
  id: totrans-1286
  prefs: []
  type: TYPE_NORMAL
  zh: 由于交叉验证需要一段时间，我们明智地使用 `parallelStartSocket()` 函数开始并行化。现在，为了运行我们的嵌套交叉验证，我们调用 `resample()`
    函数，其中第一个参数是我们的包装学习器，第二个参数是我们的任务，第三个参数是我们的外部交叉验证策略。
- en: '|  |'
  id: totrans-1287
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1288
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes a little over a minute on my four-core computer. In the meantime,
    you know what to do. Milk and no sugar, please. Do you have any cake?
  id: totrans-1289
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核计算机上需要超过一分钟的时间。与此同时，你知道该做什么。请加牛奶不加糖。你有蛋糕吗？
- en: '|  |'
  id: totrans-1290
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 6.15\. Cross-validating the model-building process
  id: totrans-1291
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.15\. 模型构建过程的交叉验证
- en: '[PRE92]'
  id: totrans-1292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE92]'
- en: Now let’s take a look at the result of our cross-validation procedure by printing
    the contents of the `cvWithTuning` object.
  id: totrans-1293
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们通过打印 `cvWithTuning` 对象的内容来查看我们的交叉验证过程的结果。
- en: Listing 6.16\. Extracting the cross-validation result
  id: totrans-1294
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 6.16\. 提取交叉验证结果
- en: '[PRE93]'
  id: totrans-1295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE93]'
- en: We’re correctly classifying 1 – 0.099 = 0.901 = 90.1% of emails as spam or not
    spam. Not bad for a first attempt!
  id: totrans-1296
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正确地将1 - 0.099 = 0.901 = 90.1%的电子邮件分类为垃圾邮件或非垃圾邮件。对于第一次尝试来说，还不错！
- en: 6.7\. Strengths and weaknesses of the SVM algorithm
  id: totrans-1297
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 6.7. SVM算法的优点和缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    the SVM algorithm will perform well for your case.
  id: totrans-1298
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常不容易判断哪些算法会对给定的任务表现良好，但以下是一些优点和缺点，这将帮助您决定SVM算法是否会在您的案例中表现良好。
- en: 'The strengths of the SVM algorithm are as follows:'
  id: totrans-1299
  prefs: []
  type: TYPE_NORMAL
  zh: SVM算法的优点如下：
- en: It’s very good at learning complex nonlinear decision boundaries.
  id: totrans-1300
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它非常擅长学习复杂的非线性决策边界。
- en: It performs very well on a wide variety of tasks.
  id: totrans-1301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它在各种任务上表现都非常出色。
- en: It makes no assumptions about the distribution of the predictor variables.
  id: totrans-1302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它不对预测变量的分布做出假设。
- en: 'The weaknesses of the SVM algorithm are these:'
  id: totrans-1303
  prefs: []
  type: TYPE_NORMAL
  zh: SVM算法的缺点如下：
- en: It is one of the most computationally expensive algorithms to train.
  id: totrans-1304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它是训练成本最高的算法之一。
- en: It has multiple hyperparameters that need to be tuned simultaneously.
  id: totrans-1305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它具有多个需要同时调整的超参数。
- en: It can only handle continuous predictor variables (although recoding a categorical
    variable as numeric *may* help in some cases).
  id: totrans-1306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它只能处理连续的自变量（尽管在某些情况下将分类变量编码为数值*可能*有所帮助）。
- en: '|  |'
  id: totrans-1307
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 3**'
  id: totrans-1308
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习3**'
- en: The `NA` values in `votesTib` include when politicians abstained from voting,
    so recode these values into the value `"a"` and cross-validate a naive Bayes model
    including them. Does this improve performance?
  id: totrans-1309
  prefs: []
  type: TYPE_NORMAL
  zh: '`votesTib`中的`NA`值包括政治家弃权的情况，因此将这些值重新编码为值`"a"`，并对包含这些值的朴素贝叶斯模型进行交叉验证。这会提高性能吗？'
- en: '|  |'
  id: totrans-1310
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '|  |'
  id: totrans-1311
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Exercise 4**'
  id: totrans-1312
  prefs: []
  type: TYPE_NORMAL
  zh: '**练习4**'
- en: 'Perform another random search for the best SVM hyperparameters using nested
    cross-validation. This time, limit your search to the linear kernel (so no need
    to tune *degree* or *gamma*), search over the range 0.1 and 100 for the *cost*
    hyperparameter, and increase the number of iterations to 100\. Warning: This took
    nearly 12 minutes to complete on my machine!'
  id: totrans-1313
  prefs: []
  type: TYPE_NORMAL
  zh: 使用嵌套交叉验证进行另一轮随机搜索以找到最佳SVM超参数。这次，将搜索限制在线性核（因此不需要调整*degree*或*gamma*），在*cost*超参数的范围内搜索0.1和100，并将迭代次数增加到100。警告：在我的机器上完成这项操作几乎需要12分钟！
- en: '|  |'
  id: totrans-1314
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Summary
  id: totrans-1315
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The naive Bayes and support vector machine (SVM) algorithms are supervised learners
    for classification problems.
  id: totrans-1316
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯和支持向量机（SVM）算法是用于分类问题的监督学习器。
- en: Naive Bayes uses Bayes’ rule (defined in [chapter 5](kindle_split_015.html#ch05))
    to estimate the probability of new data belonging to each of the possible output
    classes.
  id: totrans-1317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯使用贝叶斯定理（在第5章中定义）来估计新数据属于每个可能的输出类别的概率。
- en: The SVM algorithm finds a hyperplane (a surface with one less dimension than
    there are predictors) that best separates the classes.
  id: totrans-1318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM算法找到一个超平面（一个维度比预测变量少的表面），它能最好地分离类别。
- en: While naive Bayes can handle both continuous and categorical predictor variables,
    the SVM algorithm can only handle continuous predictors.
  id: totrans-1319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然朴素贝叶斯可以处理连续和分类的自变量，但支持向量机（SVM）算法只能处理连续的自变量。
- en: Naive Bayes is computationally cheap, while the SVM algorithm is one of the
    most expensive algorithms.
  id: totrans-1320
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 朴素贝叶斯在计算上成本低廉，而SVM算法是最昂贵的算法之一。
- en: The SVM algorithm can use kernel functions to add an extra dimension to the
    data that helps find a linear decision boundary.
  id: totrans-1321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM算法可以使用核函数为数据添加一个额外的维度，这有助于找到线性决策边界。
- en: The SVM algorithm is sensitive to the values of its hyperparameters, which must
    be tuned to maximize performance.
  id: totrans-1322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: SVM算法对超参数的值很敏感，必须调整以最大化性能。
- en: The mlr package allows parallelization of intensive processes, such as hyperparameter
    tuning, by using the parallelMap package.
  id: totrans-1323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: mlr包允许通过使用parallelMap包并行化密集型过程，如超参数调整。
- en: Solutions to exercises
  id: totrans-1324
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 练习题解答
- en: 'Use the `map_dbl()` function to count the number of `y` values in each column
    of `votesTib`:'
  id: totrans-1325
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`map_dbl()`函数计算`votesTib`中每列的`y`值的数量：
- en: '[PRE94]'
  id: totrans-1326
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE94]'
- en: 'Extract the prior probabilities and likelihoods from your naive Bayes model:'
  id: totrans-1327
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从您的朴素贝叶斯模型中提取先验概率和似然度：
- en: '[PRE95]'
  id: totrans-1328
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE95]'
- en: 'Recode the `NA` values from the `votesTib` tibble, and cross-validate a model
    including these values:'
  id: totrans-1329
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`votesTib`中的`NA`值重新编码，并对包含这些值的模型进行交叉验证：
- en: '[PRE96]'
  id: totrans-1330
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE96]'
- en: 'Perform a random search restricted to the linear kernel, tuning over a larger
    range of values for the *cost* hyperparameter:'
  id: totrans-1331
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在线性核中执行随机搜索，对*成本*超参数的范围进行更大范围的调整：
- en: '[PRE97]'
  id: totrans-1332
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE97]'
- en: Chapter 7\. Classifying with decision trees
  id: totrans-1333
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章\. 使用决策树进行分类
- en: '*This chapter covers*'
  id: totrans-1334
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Working with decision trees
  id: totrans-1335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与决策树一起工作
- en: Using the recursive partitioning algorithm
  id: totrans-1336
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用递归分割算法
- en: An important weakness of decision trees
  id: totrans-1337
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的一个重要弱点
- en: 'There’s nothing like the great outdoors. I live in the countryside, and when
    I walk my dog in the woods, I’m reminded just how much we rely on trees. Trees
    produce the atmosphere we breathe, create habitats for wildlife, provide us with
    food, and are surprisingly good at making predictions. Yes, you read that right:
    trees are good at making predictions. But before you go asking the birch in your
    back garden for next week’s lottery numbers, I should clarify that I’m referring
    to several supervised learning algorithms that use a branching tree structure.
    This family of algorithms can be used to solve both classification and regression
    tasks, can handle continuous and categorical predictors, and are naturally suited
    to solving multiclass classification problems.'
  id: totrans-1338
  prefs: []
  type: TYPE_NORMAL
  zh: 没有什么比伟大的户外更棒了。我住在乡村，当我带着我的狗在树林里散步时，我就会想起我们有多么依赖树木。树木产生我们呼吸的空气，为野生动物创造栖息地，为我们提供食物，并且出人意料地擅长预测。是的，你没听错：树木擅长预测。但在你向你家后院的桦树询问下周的彩票号码之前，我应该澄清我指的是几个使用分支树结构的监督学习算法。这个算法家族可以用于解决分类和回归任务，可以处理连续和分类预测因子，并且自然适合解决多类分类问题。
- en: '|  |'
  id: totrans-1339
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1340
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that a predictor variable is a variable we believe may contain information
    about the value of our outcome variable. Continuous predictors can have any numeric
    value on their measurement scale, while categorical variables can have only finite,
    discrete values/categories.
  id: totrans-1341
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，预测变量是我们认为可能包含关于我们的结果变量值的信息的变量。连续预测变量可以在它们的测量尺度上具有任何数值，而分类变量只能具有有限、离散的值/类别。
- en: '|  |'
  id: totrans-1342
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The basic premise of all tree-based classification algorithms is that they learn
    a sequence of questions that separates cases into different classes. Each question
    has a binary answer, and cases will be sent down the left or right branch depending
    on which criteria they meet. There can be branches within branches; and once the
    model is learned, it can be graphically represented as a tree. Have you ever played
    the game 20 Questions, where you have to guess what object someone is thinking
    of by asking yes-or-no questions? What about the game Guess Who, where you have
    to guess the other player’s character by asking questions about their appearance?
    These are examples of tree-based classifiers.
  id: totrans-1343
  prefs: []
  type: TYPE_NORMAL
  zh: 所有基于树的分类算法的基本前提是它们学习一系列问题，这些问题将案例分为不同的类别。每个问题都有二元答案，案例将根据它们满足的哪个标准被发送到左分支或右分支。分支中可以有分支；一旦模型被学习，它可以用图形表示为树。你有没有玩过20个问题的游戏，你必须通过提出是或否的问题来猜测某人正在想什么物体？又或者猜猜看的游戏，你必须通过询问关于他们外观的问题来猜测其他玩家的角色？这些都是基于树的分类器的例子。
- en: By the end of this chapter, you’ll see how such simple, interpretable models
    can be used to make predictions. We’ll finish the chapter by highlighting an important
    weakness of decision trees, which you’ll learn how to overcome in the next chapter.
  id: totrans-1344
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将看到如何使用这样简单、可解释的模型进行预测。我们将通过强调决策树的一个重要弱点来结束本章，你将在下一章中学习如何克服这个弱点。
- en: 7.1\. What is the recursive partitioning algorithm?
  id: totrans-1345
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 递归分割算法是什么？
- en: In this section, you’ll learn how decision tree algorithms—and specifically,
    the *recursive partitioning* (rpart) algorithm—work to learn a tree structure.
    Imagine that you want to create a model to represent the way people commute to
    work, given features of the vehicle. You gather information on the vehicles, such
    as how many wheels they have, whether they have an engine, and their weight. You
    could formulate your classification process as a series of sequential questions.
    Every vehicle is evaluated at each question and moves either left or right in
    the model depending on how its features satisfy the question. An example of such
    a model is shown in [figure 7.1](#ch07fig01).
  id: totrans-1346
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将了解决策树算法——特别是*递归划分*（rpart）算法——是如何学习树结构的。想象一下，你想要创建一个模型来表示人们根据车辆特征上下班的方式。你收集有关车辆的信息，例如它们有多少轮子，是否有引擎，以及它们的重量。你可以将你的分类过程表述为一系列连续的问题。每辆车在每个问题上进行评估，并根据其特征如何满足问题在模型中向左或向右移动。这种模型的例子在[图7.1](#ch07fig01)中展示。
- en: Figure 7.1\. The structure of a decision tree. The root node is the node that
    contains all the data prior to splitting. Nodes are split by a splitting criterion
    into two branches, each of which leads to another node. Nodes that do not split
    any further are called *leaves*.
  id: totrans-1347
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1. 决策树的结构。根节点是包含所有在分裂之前的数据的节点。节点通过分裂标准被分成两个分支，每个分支都指向另一个节点。不再分裂的节点被称为*叶子*。
- en: '![](fig7-1_alt.jpg)'
  id: totrans-1348
  prefs: []
  type: TYPE_IMG
  zh: '![图7-1](fig7-1_alt.jpg)'
- en: Notice that our model has a branching, tree-like structure, where each question
    splits the data into two branches. Each branch can lead to additional questions,
    which have branches of their own. The question parts of the tree are called *nodes*,
    and the very first question/node is called the *root node*. Nodes have one branch
    leading to them and two branches leading away from them. Nodes at the end of a
    series of questions are called *leaf nodes* or *leaves*. Leaf nodes have a single
    branch leading to them but no branches leading away from them. When a case finds
    its way down the tree into a leaf node, it progresses no further and is classified
    as the majority class within that leaf. It may seem strange to you (it does to
    me, anyway) that the root is at the top and the leaves are at the bottom, but
    this is the way tree-based models are usually represented.
  id: totrans-1349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们的模型具有分支和树状结构，其中每个问题将数据分成两个分支。每个分支可以引出更多的问题，这些问题又有自己的分支。树的提问部分被称为*节点*，而非常第一个问题/节点被称为*根节点*。节点有一个分支指向它们，有两个分支从它们离开。一系列问题结束处的节点被称为*叶节点*或*叶子*。叶节点只有一个分支指向它们，但没有分支从它们离开。当一个案例找到其路径进入叶节点时，它不再进一步发展，并被分类为该叶节点内的多数类。对你来说（至少对我来说）可能看起来很奇怪，根节点在顶部而叶子在底部，但这是基于树的模型通常表示的方式。
- en: '|  |'
  id: totrans-1350
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1351
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Although not shown in this small example, it is perfectly fine (and common)
    to have questions about the same feature in different parts of the tree.
  id: totrans-1352
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在这个小示例中没有展示，但在树的不同的部分对同一特征提出问题是完全正常（并且常见）的。
- en: '|  |'
  id: totrans-1353
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: This all seems simple so far. But in the previous simplistic example, we could
    have constructed this ourselves by hand. (In fact, I did!) So tree-based models
    aren’t necessarily learned by machine learning. A decision tree could be an established
    HR process for dealing with disciplinary action, for example. You could have a
    tree-based approach to deciding which flight to buy (is the price above your budget,
    is the airline reliable, is the food terrible, and so on). So how can we learn
    the structure of a decision tree automatically for complex datasets with many
    features? Enter the rpart algorithm.
  id: totrans-1354
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，这一切似乎都很简单。但在之前的简单示例中，我们完全可以通过手工构建这样的模型。（事实上，我就是这么做的！）因此，基于树的模型并不一定是通过机器学习来学习的。例如，决策树可能是一个现成的HR流程，用于处理纪律处分。你可以采用基于树的策略来决定购买哪架航班（价格是否超出你的预算，航空公司是否可靠，食物是否糟糕，等等）。那么，我们如何自动学习具有许多特征的复杂数据集的决策树结构呢？这就引入了rpart算法。
- en: '|  |'
  id: totrans-1355
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1356
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Tree-based models can be used for both classification *and* regression tasks,
    so you may see them described as *classification and regression trees* (CART).
    However, CART is a trademarked algorithm whose code is proprietary. The rpart
    algorithm is simply an open source implementation of CART. You’ll learn how to
    use trees for regression tasks in [chapter 12](kindle_split_023.html#ch12).
  id: totrans-1357
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的模型可以用于*分类*和*回归*任务，因此你可能会看到它们被描述为*分类和回归树*（CART）。然而，CART是一个商标算法，其代码是专有的。rpart算法只是CART的开源实现。你将在[第12章](kindle_split_023.html#ch12)中学习如何使用树进行回归任务。
- en: '|  |'
  id: totrans-1358
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'At each stage of the tree-building process, the rpart algorithm considers all
    of the predictor variables and selects the predictor that does the best job of
    discriminating the classes. It starts at the root and then, at each branch, looks
    again for the next feature that will best discriminate the classes of the cases
    that took that branch. But how does rpart decide on the best feature at each split?
    This can be done a few different ways, and rpart offers two approaches: the difference
    in *entropy* (called the *information gain*) and the difference in *Gini index*
    (called the *Gini gain*). The two methods usually give very similar results; but
    the Gini index (named after the sociologist and statistician Corrado Gini) is
    slightly faster to compute, so we’ll focus on it.'
  id: totrans-1359
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建树的每个阶段，rpart算法考虑所有预测变量，并选择那个在区分类别方面做得最好的预测变量。它从根节点开始，然后在每个分支上再次寻找将最好地区分该分支上案例类别的下一个特征。但是，rpart是如何决定每个分割的最佳特征的？这可以通过几种不同的方式来完成，rpart提供了两种方法：*熵*的差异（称为*信息增益*）和*基尼指数*的差异（称为*基尼增益*）。这两种方法通常给出非常相似的结果；但基尼指数（以社会学家和统计学家Corrado
    Gini命名）计算速度略快，所以我们重点关注它。
- en: '|  |'
  id: totrans-1360
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1361
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: The Gini index is the default method rpart uses to decide how to split the tree.
    If you’re concerned that you’re missing the best-performing model, you can always
    compare Gini index and entropy during hyperparameter tuning.
  id: totrans-1362
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼指数是rpart默认用于决定如何分割树的指标。如果你担心你可能会错过最佳性能的模型，你可以在超参数调整期间始终比较基尼指数和熵。
- en: '|  |'
  id: totrans-1363
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.1.1\. Using Gini gain to split the tree
  id: totrans-1364
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 使用基尼增益来分割树
- en: 'In this section, I’ll show you how Gini gain is calculated to find the best
    split for a particular node when growing a decision tree. Entropy and the Gini
    index are two ways of trying to measure the same thing: *impurity*. Impurity is
    a measure of how heterogeneous the classes are within a node.'
  id: totrans-1365
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示如何计算基尼增益，以找到在生长决策树时特定节点的最佳分割。熵和基尼指数是试图衡量同一事物（*不纯度*）的两种方式。不纯度是衡量节点内类别异质性的度量。
- en: '|  |'
  id: totrans-1366
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1367
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: If a node contains only a single class (which would make it a leaf), it would
    be said to be *pure*.
  id: totrans-1368
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个节点只包含一个类别（这将使其成为叶子节点），则可以说它是*纯*的。
- en: '|  |'
  id: totrans-1369
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: By estimating the impurity (with whichever method you choose) that would result
    from using each predictor variable for the next split, the algorithm can choose
    the feature that will result in the smallest impurity. Put another way, the algorithm
    chooses the feature that will result in subsequent nodes that are as homogeneous
    as possible.
  id: totrans-1370
  prefs: []
  type: TYPE_NORMAL
  zh: 通过估计使用每个预测变量进行下一次分割将产生的*不纯度*（无论你选择哪种方法），算法可以选择将导致最小不纯度的特征。换句话说，算法选择将导致后续节点尽可能同质的特征。
- en: So what does the Gini index look like? [Figure 7.2](#ch07fig02) shows an example
    split. We have 20 cases in a parent node belonging to two classes, A and B. We
    split the node into two leaves based on some criterion. In the left leaf, we have
    11 cases from class A and 3 from class B. In the right leaf, we have 5 from class
    B and 1 from class A.
  id: totrans-1371
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，基尼系数看起来是什么样子呢？[图7.2](#ch07fig02) 展示了一个示例分割。我们有一个父节点，包含属于两个类别A和B的20个案例。我们根据某些标准将该节点分割成两个叶子节点。在左叶子节点中，我们有来自类别A的11个案例和来自类别B的3个案例。在右叶子节点中，我们有来自类别B的5个案例和来自类别A的1个案例。
- en: Figure 7.2\. An example decision tree split for 20 cases belonging to classes
    A and B
  id: totrans-1372
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 属于类别A和B的20个案例的示例决策树分割
- en: '![](fig7-2.jpg)'
  id: totrans-1373
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-2.jpg)'
- en: We want to know the *Gini gain* of this split. The Gini gain is the difference
    between the Gini index of the parent node and the Gini index of the split. Looking
    at our example in [figure 7.2](#ch07fig02), the Gini index for any node is calculated
    as
  id: totrans-1374
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想知道这个分割的*基尼增益*。基尼增益是父节点基尼指数与分割基尼指数之间的差异。查看我们的示例[图7.2](#ch07fig02)，任何节点的基尼指数计算如下
- en: Gini index = 1 –(*p*(*A*)² + *p*(*B*)²)
  id: totrans-1375
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尼指数 = 1 –(*p*(*A*)² + *p*(*B*)²)
- en: where *p*(*A*) and *p*(*B*) are the proportions of cases belonging to classes
    A and B, respectively. So the Gini indices for the parent node and the left and
    right leaves are shown in [figure 7.3](#ch07fig03).
  id: totrans-1376
  prefs: []
  type: TYPE_NORMAL
  zh: 其中*p*(*A*)和*p*(*B*)分别是属于类别A和B的案例比例。因此，父节点、左叶和右叶的基尼指数如[图7.3](#ch07fig03)所示。
- en: Figure 7.3\. Calculating the Gini index of the parent node and the left and
    right leaves
  id: totrans-1377
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3\. 计算父节点和左右叶子的基尼指数
- en: '![](fig7-3.jpg)'
  id: totrans-1378
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-3.jpg)'
- en: 'Now that we have the Gini indices for the left and right leaves, we can calculate
    the Gini index for the split as a whole. The Gini index of the split is the sum
    of the left and right Gini indices multiplied by the proportion of cases they
    accepted from the parent node:'
  id: totrans-1379
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了左右叶子的基尼指数，我们可以计算整个分割的基尼指数。分割的基尼指数是左右基尼指数的总和乘以它们从父节点接受的案例比例：
- en: '![](pg171-1_alt.jpg)'
  id: totrans-1380
  prefs: []
  type: TYPE_IMG
  zh: '![](pg171-1_alt.jpg)'
- en: And the Gini gain (the difference between the Gini indices of the parent node
    and the split) is simply
  id: totrans-1381
  prefs: []
  type: TYPE_NORMAL
  zh: 并且基尼增益（父节点和分割的基尼指数之间的差异）简单地是
- en: Gini gain = 0.48 – 0.32 = 0.16
  id: totrans-1382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基尼增益 = 0.48 – 0.32 = 0.16
- en: where 0.48 is the Gini index of the parent, as calculated in [figure 7.3](#ch07fig03).
  id: totrans-1383
  prefs: []
  type: TYPE_NORMAL
  zh: 其中0.48是父节点的基尼指数，如[图7.3](#ch07fig03)中计算所得。
- en: The Gini gain at a particular node is calculated for each predictor variable,
    and the predictor that generates the largest Gini gain is used to split that node.
    This process is repeated for every node as the tree grows.
  id: totrans-1384
  prefs: []
  type: TYPE_NORMAL
  zh: 在特定节点上，为每个预测变量计算基尼增益，并使用产生最大基尼增益的预测变量来分割该节点。随着树的生长，这个过程会重复应用于每个节点。
- en: '|  |'
  id: totrans-1385
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Generalizing the Gini index to any number of classes**'
  id: totrans-1386
  prefs: []
  type: TYPE_NORMAL
  zh: '**将基尼指数推广到任意数量的类别**'
- en: In this example, we’ve considered only two classes, but the Gini index of a
    node is easily calculable for problems that have many classes. In that situation,
    the equation for Gini index generalizes to
  id: totrans-1387
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个例子中，我们只考虑了两个类别，但对于有多个类别的实际问题，节点的基尼指数很容易计算。在这种情况下，基尼指数的方程推广为
- en: '![](pg171-2.jpg)'
  id: totrans-1388
  prefs: []
  type: TYPE_IMG
  zh: '![](pg171-2.jpg)'
- en: which is just a fancy way of saying that we calculate *p*(*class[k]*)² for each
    class from 1 to *K* (the number of classes), add them all up, and subtract this
    value from 1.
  id: totrans-1389
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是说我们计算每个类别从1到*K*（类别数量）的*p*(*class[k]*)²，将它们全部加起来，然后从1中减去这个值的一种花哨的说法。
- en: If you’re interested, the equation for entropy is
  id: totrans-1390
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你对熵的公式感兴趣，它是
- en: '![](pg171-3.jpg)'
  id: totrans-1391
  prefs: []
  type: TYPE_IMG
  zh: '![](pg171-3.jpg)'
- en: which is just a fancy way of saying that we calculate –*p*(*class*) × log[2]*p*(*class*)
    for each class from 1 to *K* (the number of classes) and add them all up (which
    becomes a subtraction because the first term is negative). As for Gini gain, the
    information gain is calculated as the entropy of the parent minus the entropy
    of the split (which is calculated exactly the same way as the Gini index for the
    split).
  id: totrans-1392
  prefs: []
  type: TYPE_NORMAL
  zh: 这只是说我们计算每个类别从1到*K*（类别数量）的-p*(*class*) × log[2]*p*(*class*)，并将它们全部加起来（由于第一个项是负数，这变成了减法）。至于基尼增益，信息增益是父节点的熵减去分割的熵（分割的熵计算方式与分割的基尼指数完全相同）。
- en: '|  |'
  id: totrans-1393
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.1.2\. What about continuous and multilevel categorical predictors?
  id: totrans-1394
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 关于连续和多级分类预测变量怎么办？
- en: 'In this section, I’ll show you how the splits are chosen for continuous and
    categorical predictor variables. When a predictor variable is *dichotomous* (has
    only two levels), it’s quite obvious how to use it for a split: cases with one
    value go left, and cases with the other value go right. Decision trees can also
    split the cases using continuous variables, but what value is chosen as the split
    point? Have a look at the example in [figure 7.4](#ch07fig04). We have cases from
    three classes plotted against two continuous variables. The feature space is split
    into rectangles by each node. At the first node, the cases are split into those
    with a value of variable 2, greater than or less than 20\. The cases that make
    it to the second node are further split into those with a value of variable 1,
    greater than or less than 10,000.'
  id: totrans-1395
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示如何为连续和分类预测变量选择分割。当一个预测变量是二元的（只有两个级别）时，如何用它进行分割是很明显的：具有一个值的案例向左走，具有另一个值的案例向右走。决策树也可以使用连续变量来分割案例，但分割点选择什么值呢？看看[图7.4](#ch07fig04)中的例子。我们有来自三个类别的案例，它们与两个连续变量相关。特征空间由每个节点分割成矩形。在第一个节点，案例被分割成变量2的值大于或小于20的案例。到达第二个节点的案例进一步被分割成变量1的值大于或小于10,000的案例。
- en: Figure 7.4\. How splitting is performed for continuous predictors. Cases belonging
    to three classes are plotted against two continuous variables. The first node
    splits the feature space into rectangles based on the value of variable 2\. The
    second node further splits the variable 2 ≥ 20 feature space into rectangles based
    on the value of variable 1.
  id: totrans-1396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4\. 如何对连续预测变量进行分割。属于三个类别的案例是针对两个连续变量绘制的。第一个节点根据变量2的值将特征空间分割成矩形。第二个节点进一步根据变量1的值将变量2
    ≥ 20的特征空间分割成矩形。
- en: '![](fig7-4_alt.jpg)'
  id: totrans-1397
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-4_alt.jpg)'
- en: '|  |'
  id: totrans-1398
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1399
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Notice that the variables are on vastly different scales. The rpart algorithm
    isn’t sensitive to variables being on different scales, so there’s no need to
    scale and center your predictors!
  id: totrans-1400
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到变量处于截然不同的尺度上。rpart算法对变量处于不同尺度不敏感，因此不需要对预测变量进行缩放和中心化！
- en: '|  |'
  id: totrans-1401
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: But how is the exact split point chosen for a continuous predictor? Well, the
    cases in the training set are arranged in order of the continuous variable, and
    the Gini gain is evaluated for the midpoint between each adjacent pair of cases.
    If the greatest Gini gain among all predictor variables is one of these midpoints,
    then this is chosen as the split for that node. This is illustrated in [figure
    7.5](#ch07fig05).
  id: totrans-1402
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，对于一个连续预测变量，如何选择确切的分割点呢？嗯，训练集中的案例是按照连续变量的顺序排列的，并且评估了每对相邻案例之间的中点处的基尼增益。如果所有预测变量中最大的基尼增益是这些中点之一，那么这个中点就被选为该节点的分割点。这可以在[图7.5](#ch07fig05)中看到。
- en: 'A similar procedure is used for categorical predictors with more than two levels.
    First, the Gini index is computed for each level of the predictor (using the proportion
    of each class that has that value of the predictor). The factor levels are arranged
    in order of their Gini indices, and the Gini gain is evaluated for a split between
    each adjacent pair of levels. Take a look at the example in [figure 7.6](#ch07fig06).
    We have a factor with three levels (A, B, and C): we evaluate the Gini index of
    each and find that their values are B < A < C. Now we evaluate the Gini gain for
    the splits B versus A and C, and C versus B and A.'
  id: totrans-1403
  prefs: []
  type: TYPE_NORMAL
  zh: 对于具有超过两个水平的分类预测变量，使用类似的程序。首先，计算预测变量的每个水平的基尼指数（使用具有该预测变量值的每个类的比例）。因素水平按照它们的基尼指数顺序排列，并且评估相邻水平之间的分割的基尼增益。看看[图7.6](#ch07fig06)中的例子。我们有一个有三个水平（A、B和C）的因素：我们评估每个水平的基尼指数，并发现它们的值是B
    < A < C。现在我们评估分割B与A和C，以及C与B和A的基尼增益。
- en: In this way, we can create a binary split from categorical variables with many
    predictors without having to try every single possible combination of level splits
    (2*^m*^(–1), where *m* is the number of levels of the variable). If the split
    B versus A and C is found to have the greatest Gini gain, then cases reaching
    this node will go down one branch if they have a value of B for this variable,
    and will go down the other branch if they have a value of A or C.
  id: totrans-1404
  prefs: []
  type: TYPE_NORMAL
  zh: 这样，我们可以在不需要尝试每个可能的水平分割组合（2^m^(-1)，其中*m*是变量的水平数）的情况下，从具有许多预测变量的分类变量中创建二分分割。如果发现分割B与A和C具有最大的基尼增益，那么具有该变量B值的案例将沿着一个分支向下，而具有A或C值的案例将沿着另一个分支向下。
- en: Figure 7.5\. How the split point is chosen for continuous predictors. Cases
    (circles) are arranged in order of their value of the continuous predictor. The
    midpoint between each adjacent pair of cases is considered as a candidate split,
    and the Gini gain is calculated for each. If one of these splits has the highest
    Gini gain of any candidate split, it will be used to split the tree at this node.
  id: totrans-1405
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5\. 如何为连续预测变量选择分割点。案例（圆圈）按照它们的连续预测变量的值排列。考虑每个相邻案例对之间的中点作为候选分割点，并计算每个的基尼增益。如果这些分割中的任何一个具有任何候选分割中最高的基尼增益，它将用于在此节点分割树。
- en: '![](fig7-5_alt.jpg)'
  id: totrans-1406
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-5_alt.jpg)'
- en: Figure 7.6\. How the split point is chosen for categorical predictors. The Gini
    index of each factor level is calculated using the proportion of cases from each
    class with that factor level. The factor levels are arranged in order of their
    Gini indices, and the Gini gain is evaluated for each split between adjacent levels.
  id: totrans-1407
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. 如何为分类预测变量选择分割点。每个因素水平的基尼指数是通过计算具有该因素水平的每个类别的案例比例来计算的。因素水平按照它们的基尼指数顺序排列，并且评估了相邻水平之间的每个分割的基尼增益。
- en: '![](fig7-6_alt.jpg)'
  id: totrans-1408
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-6_alt.jpg)'
- en: 7.1.3\. Hyperparameters of the rpart algorithm
  id: totrans-1409
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. rpart算法的超参数
- en: 'In this section, I’ll show you which hyperparameters need to be tuned for the
    rpart algorithm, what they do, and why we need to tune them in order to get the
    best-performing tree possible. Decision tree algorithms are described as *greedy*.
    By greedy, I don’t mean they take an extra helping at the buffet line; I mean
    they search for the split that will perform best *at the current node*, rather
    than the one that will produce the best result globally. For example, a particular
    split might discriminate the classes best at the current node but result in poor
    separation further down that branch. Conversely, a split that results in poor
    separation at the current node may yield better separation further down the tree.
    Decision tree algorithms would never pick this second split because they only
    look at *locally optimal* splits, instead of *globally optimal* ones. There are
    three issues with this approach:'
  id: totrans-1410
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示对于rpart算法，哪些超参数需要调整，它们的作用，以及为什么我们需要调整它们以获得性能最佳的树。决策树算法被描述为*贪婪的*。这里的“贪婪”并不是指它们在自助餐线上多拿一份；我的意思是它们寻找的是在当前节点上表现最佳的分割，而不是全局上产生最佳结果的分割。例如，某个分割可能在当前节点上对类别进行最佳区分，但在该分支的更深处导致分离效果差。相反，在当前节点上导致分离效果差的分割可能在树的更深处产生更好的分离。决策树算法永远不会选择第二个分割，因为它们只关注*局部最优*的分割，而不是*全局最优*的分割。这种方法的三个问题是：
- en: The algorithm isn’t guaranteed to learn a globally optimal model.
  id: totrans-1411
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 算法不保证学习到全局最优模型。
- en: If left unchecked, the tree will continue to grow deeper until all the leaves
    are pure (of only one class).
  id: totrans-1412
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果不加以控制，树将继续生长，直到所有叶子都是纯的（仅有一个类别）。
- en: For large datasets, growing extremely deep trees becomes computationally expensive.
  id: totrans-1413
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于大型数据集，生长非常深的树在计算上变得昂贵。
- en: While it’s true that rpart isn’t guaranteed to learn a globally optimal model,
    the depth of the tree is of greater concern to us. Besides the computational cost,
    growing a full-depth tree until all the leaves are pure is very likely to overfit
    the training set and create a model with high variance. This is because as the
    feature space is split up into smaller and smaller pieces, we’re much more likely
    to start modeling the noise in the data.
  id: totrans-1414
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然rpart不保证学习到全局最优模型，但树的深度对我们来说更为重要。除了计算成本外，将树生长到所有叶子都是纯的，很可能过度拟合训练集并创建一个具有高方差度的模型。这是因为随着特征空间被分割成越来越小的部分，我们更有可能开始模拟数据中的噪声。
- en: 'How do we guard against such extravagant tree building? There are two ways
    of doing it:'
  id: totrans-1415
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何防止这种过度构建的树？有两种方法可以做到：
- en: Grow a full tree, and then *prune* it.
  id: totrans-1416
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 首先生长一棵完整的树，然后*剪枝*。
- en: Employ *stopping criteria*.
  id: totrans-1417
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 采用*停止标准*。
- en: In the first approach, we allow the greedy algorithm to grow its full, overfit
    tree, and then we get out our garden shears and remove leaves that don’t meet
    certain criteria. This process is imaginatively named *pruning*, because we end
    up removing branches and leaves from our tree. This is sometimes called *bottom-up*
    pruning because we start from the leaves and prune up toward the root.
  id: totrans-1418
  prefs: []
  type: TYPE_NORMAL
  zh: 在第一种方法中，我们允许贪婪算法生长完整的、过度拟合的树，然后我们拿出我们的园艺剪刀，移除不符合某些标准的叶子。这个过程被富有想象力地命名为*剪枝*，因为我们最终从树中移除了分支和叶子。这有时被称为*自底向上*剪枝，因为我们从叶子开始，向上修剪到根。
- en: In the second approach, we include conditions during tree building that will
    force splitting to stop if certain criteria aren’t met. This is sometimes called
    *top-down* pruning because we are pruning the tree as it grows down from the root.
  id: totrans-1419
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二种方法中，我们在构建树的过程中包含条件，如果未满足某些标准，将强制停止分割。这有时被称为*自顶向下*剪枝，因为我们是从根向下修剪树的。
- en: Both approaches may yield comparable results in practice, but there is a slight
    computational edge to top-down pruning because we don’t need to grow full trees
    and then prune them back. For this reason, we will use the stopping criteria approach.
  id: totrans-1420
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，两种方法可能产生相似的结果，但自顶向下剪枝在计算上略有优势，因为我们不需要先生长完整的树然后再修剪它们。因此，我们将使用停止标准方法。
- en: 'The stopping criteria we can apply at each stage of the tree-building process
    are as follows:'
  id: totrans-1421
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在树构建过程的每个阶段应用的停止标准如下：
- en: Minimum number of cases in a node before splitting
  id: totrans-1422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割前节点中的最小案例数
- en: Maximum depth of the tree
  id: totrans-1423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树的最大深度
- en: Minimum improvement in performance for a split
  id: totrans-1424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割的最小性能提升
- en: Minimum number of cases in a leaf
  id: totrans-1425
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 叶子中的最小案例数
- en: These criteria are illustrated in [figure 7.7](#ch07fig07). For each candidate
    split during tree building, each of these criteria is evaluated and must be passed
    for the node to be split further.
  id: totrans-1426
  prefs: []
  type: TYPE_NORMAL
  zh: 这些标准在[图7.7](#ch07fig07)中得到了说明。在构建树的过程中，对每个候选分割，都会评估这些标准，并且节点必须通过这些标准才能进一步分割。
- en: Figure 7.7\. Hyperparameters of rpart. Important nodes are highlighted in each
    example, and the numbers in each node represent the number of cases. The *minsplit*,
    *maxdepth*, *cp*, and *minbucket* hyperparameters all simultaneously constrain
    the splitting of each node.
  id: totrans-1427
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7\. rpart的超参数。在每个示例中突出显示重要节点，每个节点中的数字代表案例数。*minsplit*、*maxdepth*、*cp* 和 *minbucket*
    这几个超参数同时约束每个节点的分割。
- en: '![](fig7-7_alt.jpg)'
  id: totrans-1428
  prefs: []
  type: TYPE_IMG
  zh: '![](fig7-7_alt.jpg)'
- en: The minimum number of cases needed to split a node is called *minsplit* by rpart.
    If a node has fewer than the specified number, the node will not be split further.
    The maximum depth of the tree is called *maxdepth* by rpart. If a node is already
    at this depth, it will not be split further. The minimum improvement in performance
    is, confusingly, not the Gini gain of a split. Instead, a statistic called the
    *complexity parameter* (*cp* in rpart) is calculated for each level of depth of
    the tree. If the *cp* value of a depth is less than the chosen threshold value,
    the nodes at this level will not be split further. In other words, if adding another
    layer to the tree doesn’t improve the performance of the model by *cp*, don’t
    split the nodes. The *cp* value is calculated as
  id: totrans-1429
  prefs: []
  type: TYPE_NORMAL
  zh: rpart中将分割节点所需的最小案例数称为 *minsplit*。如果一个节点少于指定的数量，则该节点将不会进一步分割。rpart中将树的最高深度称为
    *maxdepth*。如果一个节点已经达到这个深度，则该节点将不会进一步分割。性能的最小改进，令人困惑的是，不是分割的基尼增益。相反，为树的每个深度级别计算一个称为
    *复杂性参数* 的统计量（rpart中的 *cp*）。如果一个深度的 *cp* 值小于选择的阈值值，则该级别的节点将不会进一步分割。换句话说，如果添加另一层到树中不通过
    *cp* 改善模型的性能，则不要分割节点。*cp* 值的计算如下
- en: '![](pg175.jpg)'
  id: totrans-1430
  prefs: []
  type: TYPE_IMG
  zh: '![](pg175.jpg)'
- en: where *p*(incorrect) is the proportion of incorrectly classified cases at a
    particular depth of the tree, and *n*(splits) is the number of splits at that
    depth. The indices *l* and *l* + 1 indicate the current depth (*l*) and one depth
    above (*l* + 1). This reduces to the difference in incorrectly classified cases
    in one depth compared to the depth above it, divided by the number of new splits
    added to the tree. If this seems a bit abstract at the moment, we’ll work through
    an example when we build our own decision tree in section 7.7.
  id: totrans-1431
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *p*(错误) 是在树的特定深度上错误分类案例的比例，而 *n*(分割) 是在该深度上的分割数。索引 *l* 和 *l* + 1 表示当前深度 (*l*)
    和比它高一个深度的深度 (*l* + 1)。这相当于将一个深度与比它高一个深度的深度中错误分类案例的差异除以添加到树中的新分割数。如果现在这个概念听起来有点抽象，我们将在第7.7节中构建自己的决策树时通过一个例子来解释。
- en: Finally, the minimum number of cases in a leaf is called *minbucket* by rpart.
    If splitting a node would result in leaves containing fewer cases than *minbucket*,
    the node will not be split.
  id: totrans-1432
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，rpart中将叶节点中的最小案例数称为 *minbucket*。如果分割一个节点会导致叶节点包含的案例数少于 *minbucket*，则该节点将不会进行分割。
- en: These four criteria combined can make for very stringent and complicated stopping
    criteria. Because the values of these criteria cannot be learned directly from
    the data, they are hyperparameters. What do we do with hyperparameters? Tune them!
    So when we build a model with rpart, we will tune these stopping criteria to get
    values that give us the best-performing model.
  id: totrans-1433
  prefs: []
  type: TYPE_NORMAL
  zh: 这四个标准结合在一起可以形成非常严格和复杂的停止标准。因为这些标准的值不能直接从数据中学习，所以它们是超参数。我们对超参数怎么办？调整它们！因此，当我们使用rpart构建模型时，我们将调整这些停止标准以获得最佳性能模型的值。
- en: '|  |'
  id: totrans-1434
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1435
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Recall from [chapter 3](kindle_split_013.html#ch03) that a variable or option
    than controls how an algorithm learns, but which cannot be learned from the data,
    is called a *hyperparameter*.
  id: totrans-1436
  prefs: []
  type: TYPE_NORMAL
  zh: 从[第3章](kindle_split_013.html#ch03)回忆起，一个变量或选项控制算法如何学习，但不能从数据中学习，被称为 *超参数*。
- en: '|  |'
  id: totrans-1437
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.2\. Building your first decision tree model
  id: totrans-1438
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 构建你的第一个决策树模型
- en: 'In this section, you’re going to learn how to build a decision tree with rpart
    and how to tune its hyperparameters. Imagine that you work in public engagement
    at a wildlife sanctuary. You’re tasked with creating an interactive game for children,
    to teach them about different animal classes. The game asks the children to think
    of any animal in the sanctuary, and then asks them questions about the physical
    characteristics of that animal. Based on the responses the child gives, the model
    should tell the child what class their animal belongs to (mammal, bird, reptile,
    and so on). It’s important for your model to be general enough that it can be
    used at other wildlife sanctuaries. Let’s start by loading the mlr and tidyverse
    packages:'
  id: totrans-1439
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，你将学习如何使用rpart构建决策树以及如何调整其超参数。想象一下，你在野生动物保护区从事公众参与工作。你的任务是创建一个互动游戏，向孩子们介绍不同的动物类别。游戏要求孩子们想到保护区中的任何一种动物，然后询问他们关于该动物身体特征的问题。根据孩子给出的回答，模型应该告诉孩子他们的动物属于哪个类别（哺乳动物、鸟类、爬行动物等）。对于你的模型来说，重要的是它足够通用，可以在其他野生动物保护区使用。让我们首先加载mlr和tidyverse包：
- en: '[PRE98]'
  id: totrans-1440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE98]'
- en: 7.3\. Loading and exploring the zoo dataset
  id: totrans-1441
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3\. 加载和探索zoo数据集
- en: Let’s load the zoo dataset that is built into the mlbench package, convert it
    into a tibble, and explore it. We have a tibble containing 101 cases and 17 variables
    of observations made on various animals; 16 of these variables are logical, indicating
    the presence or absence of some characteristic, and the `type` variable is a factor
    containing the animal classes we wish to predict.
  id: totrans-1442
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们加载内置在mlbench包中的zoo数据集，将其转换为tibble，并对其进行探索。我们有一个包含101个案例和17个变量的tibble，这些变量是关于各种动物观察的结果；其中16个变量是逻辑变量，表示某些特征的缺失或存在，而`type`变量是一个因子，包含我们希望预测的动物类别。
- en: Listing 7.1\. Loading and exploring the zoo dataset
  id: totrans-1443
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. 加载和探索zoo数据集
- en: '[PRE99]'
  id: totrans-1444
  prefs: []
  type: TYPE_PRE
  zh: '[PRE99]'
- en: Unfortunately, mlr won’t let us create a task with logical predictors, so let’s
    convert them into factors instead. There are a few ways to do this, but dplyr’s
    `mutate_if()` function comes in handy here. This function takes the data as the
    first argument (or we could have piped this in with `%>%`). The second argument
    is our criterion for selecting columns, so here I’ve used `is.logical` to consider
    only the logical columns. The final argument is what to do with those columns,
    so I’ve used `as.factor` to convert the logical columns into factors. This will
    leave the existing factor `type` untouched.
  id: totrans-1445
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，mlr不允许我们创建带有逻辑预测器的任务，所以让我们将它们转换为因子。有几种方法可以做到这一点，但dplyr的`mutate_if()`函数在这里很有用。这个函数将数据作为第一个参数（或者我们可以使用`%>%`管道将其引入）。第二个参数是我们选择列的标准，所以我使用了`is.logical`来考虑只有逻辑列。最后一个参数是针对这些列要做什么，所以我使用了`as.factor`将逻辑列转换为因子。这将保留现有的因子`type`不变。
- en: Listing 7.2\. Converting logical variables to factors
  id: totrans-1446
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2\. 将逻辑变量转换为因子
- en: '[PRE100]'
  id: totrans-1447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE100]'
- en: '|  |'
  id: totrans-1448
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Tip
  id: totrans-1449
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: Alternatively, I could have used `mutate_all(zooTib, as.factor)`, because the
    `type` column is already a factor.
  id: totrans-1450
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，我本可以使用`mutate_all(zooTib, as.factor)`，因为`type`列已经是因子类型。
- en: '|  |'
  id: totrans-1451
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 7.4\. Training the decision tree model
  id: totrans-1452
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 训练决策树模型
- en: In this section, I’ll walk you through training a decision tree model using
    the rpart algorithm. We’ll tune the algorithm’s hyperparameters and train a model
    using the optimal hyperparameter combination.
  id: totrans-1453
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将指导你使用rpart算法训练决策树模型。我们将调整算法的超参数，并使用最佳超参数组合来训练模型。
- en: Let’s define our task and learner, and build a model as usual. This time, we
    supply `"classif.rpart"` as the argument to `makeLearner()` to specify that we’re
    going to use rpart.
  id: totrans-1454
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义我们的任务和学习器，并像往常一样构建模型。这次，我们将`"classif.rpart"`作为`makeLearner()`的参数传递，以指定我们将使用rpart。
- en: Listing 7.3\. Creating the task and learner
  id: totrans-1455
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3\. 创建任务和学习器
- en: '[PRE101]'
  id: totrans-1456
  prefs: []
  type: TYPE_PRE
  zh: '[PRE101]'
- en: 'Next, we need to perform hyperparameter tuning. Recall that the first step
    is to define a hyperparameter space over which we want to search. Let’s look at
    the hyperparameters available to us for the rpart algorithm, in [listing 7.4](#ch07ex04).
    We’ve already discussed the most important hyperparameters for tuning: *minsplit*,
    *minbucket*, *cp*, and *maxdepth*. There are a few others you may find useful
    to know about.'
  id: totrans-1457
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们需要进行超参数调整。回想一下，第一步是在我们想要搜索的超参数空间中定义超参数。让我们看看rpart算法为我们提供的超参数，在[列表7.4](#ch07ex04)中。我们已经讨论了调整时最重要的超参数：*minsplit*、*minbucket*、*cp*和*maxdepth*。还有一些其他你可能想了解的超参数。
- en: The *maxcompete* hyperparameter controls how many candidate splits can be displayed
    for each node in the model summary. The model summary shows the candidate splits
    in order of how much they improved the model (Gini gain). It may be useful to
    understand what the next-best split was after the one that was actually used,
    but tuning *maxcompete* doesn’t affect model performance, only its summary.
  id: totrans-1458
  prefs: []
  type: TYPE_NORMAL
  zh: '*maxcompete* 超参数控制模型总结中每个节点可以显示多少个候选分割。模型总结按它们提高模型（基尼增益）的顺序显示候选分割。了解实际使用的下一个最佳分割可能是有用的，但调整
    *maxcompete* 不会影响模型性能，只会影响其总结。'
- en: The *maxsurrogate* hyperparameter is similar to *maxcompete* but controls how
    many *surrogate splits* are shown. A surrogate split is a split used if a particular
    case is missing data for the actual split. In this way, rpart can handle missing
    data as it learns which splits can be used in place of missing variables. The
    *maxsurrogate* hyperparameter controls how many of these surrogates to retain
    in the model (if a case is missing a value for the main split, it is passed to
    the first surrogate split, then to the second surrogate if it is also missing
    a value for the first surrogate, and so on). Although we don’t have any missing
    data in our dataset, future cases we wish to predict might. We *could* set this
    to zero to save some computation time, which is equivalent to not using surrogate
    variables, but doing so might reduce the accuracy of predictions made on future
    cases with missing data. The default value of 5 is usually fine.
  id: totrans-1459
  prefs: []
  type: TYPE_NORMAL
  zh: '*maxsurrogate* 超参数类似于 *maxcompete*，但它控制显示多少个 *代理分割*。代理分割是在特定案例缺少实际分割数据时使用的分割。通过这种方式，rpart
    可以处理缺失数据，因为它学习哪些分割可以用作缺失变量的替代。*maxsurrogate* 超参数控制模型中保留多少个这些代理（如果主分割缺少值，它将被传递到第一个代理分割，然后如果它也缺少第一个代理的值，它将被传递到第二个代理，依此类推）。尽管我们的数据集中没有缺失数据，但我们希望预测的未来案例可能会有。我们可以将其设置为0以节省一些计算时间，这相当于不使用代理变量，但这样做可能会降低对未来案例缺失数据做出的预测的准确性。默认值5通常是可以的。'
- en: '|  |'
  id: totrans-1460
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1461
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Recall from [chapter 6](kindle_split_016.html#ch06) that we can quickly count
    the number of missing values per column of a data.frame or tibble by running `map_dbl(zooTib,
    ~sum(is.na(.)))`.
  id: totrans-1462
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第6章](kindle_split_016.html#ch06)，我们可以通过运行 `map_dbl(zooTib, ~sum(is.na(.))))`
    快速计算每个数据框或tibble列中缺失值的数量。
- en: '|  |'
  id: totrans-1463
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The *usesurrogate* hyperparameter controls how the algorithm uses surrogate
    splits. A value of zero means surrogates will not be used, and cases with missing
    data will not be classified. A value of 1 means surrogates will be used, but if
    a case is missing data for the actual split *and* for all the surrogate splits,
    that case will not be classified. The default value of 2 means surrogates will
    be used, but a case with missing data for the actual split and for all the surrogate
    splits will be sent down the branch that contained the most cases. The default
    value of 2 is usually appropriate.
  id: totrans-1464
  prefs: []
  type: TYPE_NORMAL
  zh: '*usesurrogate* 超参数控制算法如何使用代理分割。值为0表示不会使用代理，并且带有缺失数据的案例将不会被分类。值为1表示将使用代理，但如果一个案例的实际分割和所有代理分割都缺少数据，该案例将不会被分类。默认值2表示将使用代理，但一个实际分割和所有代理分割都缺少数据的案例将被发送到包含最多案例的分支。默认值2通常是合适的。'
- en: '|  |'
  id: totrans-1465
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1466
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If you have cases that are missing data for the actual split *and* all the surrogate
    splits for a node, you should probably consider the impact missing data is having
    on the quality of your dataset!
  id: totrans-1467
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一些实际分割和所有代理分割都缺少数据的案例，你应该考虑缺少数据对你的数据集质量产生的影响！
- en: '|  |'
  id: totrans-1468
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 7.4\. Printing available rpart hyperparameters
  id: totrans-1469
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4\. 打印可用的rpart超参数
- en: '[PRE102]'
  id: totrans-1470
  prefs: []
  type: TYPE_PRE
  zh: '[PRE102]'
- en: Now, let’s define the hyperparameter space we want to search over. We’re going
    to tune the values of *minsplit* (an integer), *minbucket* (an integer), *cp*
    (a numeric), and *maxdepth* (an integer).
  id: totrans-1471
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们定义我们想要搜索的超参数空间。我们将调整 *minsplit*（一个整数）、*minbucket*（一个整数）、*cp*（一个数值）和 *maxdepth*（一个整数）的值。
- en: '|  |'
  id: totrans-1472
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1473
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Remember that we use `makeIntegerParam()` and `makeNumericParam()` to define
    the search spaces for integer and numeric hyperparameters, respectively.
  id: totrans-1474
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，我们使用 `makeIntegerParam()` 和 `makeNumericParam()` 分别定义整数和数值超参数的搜索空间。
- en: '|  |'
  id: totrans-1475
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 7.5\. Defining the hyperparameter space for tuning
  id: totrans-1476
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.5\. 定义超参数空间以进行调整
- en: '[PRE103]'
  id: totrans-1477
  prefs: []
  type: TYPE_PRE
  zh: '[PRE103]'
- en: Next, we can define how we’re going to search the hyperparameter space we defined
    in [listing 7.5](#ch07ex05). Because the hyperparameter space is quite large,
    we’re going to use a random search rather than a grid search. Recall from [chapter
    6](kindle_split_016.html#ch06) that a random search is not exhaustive (will not
    try every hyperparameter combination) but will randomly select combinations as
    many times (iterations) as we tell it to. We’re going to use 200 iterations.
  id: totrans-1478
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们可以定义如何搜索我们在[列表 7.5](#ch07ex05)中定义的超参数空间。由于超参数空间相当大，我们将使用随机搜索而不是网格搜索。回想一下[第
    6 章](kindle_split_016.html#ch06)，随机搜索不是穷尽的（不会尝试每个超参数组合），但会随机选择组合，次数（迭代）如我们告诉它的那样。我们将使用200次迭代。
- en: In [listing 7.6](#ch07ex06) we also define our cross-validation strategy for
    tuning. Here, I’m going to use ordinary 5-fold cross-validation. Recall from [chapter
    3](kindle_split_013.html#ch03) that this will split the data into five folds and
    use each fold as the test set once. For each test set, a model will be trained
    on the rest of the data (the training set). This will be performed for each combination
    of hyperparameter values tried by the random search.
  id: totrans-1479
  prefs: []
  type: TYPE_NORMAL
  zh: 在[列表 7.6](#ch07ex06)中，我们还定义了用于调整的交叉验证策略。在这里，我将使用普通的5折交叉验证。回想一下[第 3 章](kindle_split_013.html#ch03)，这将把数据分成五个部分，并且每次使用其中一个部分作为测试集。对于每个测试集，将在剩余的数据（训练集）上训练一个模型。这将针对随机搜索尝试的每个超参数值组合进行。
- en: '|  |'
  id: totrans-1480
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1481
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: 'Ordinarily, if classes are imbalanced, I would use stratified sampling. Here,
    though, because we have very few cases in some of the classes, there are not enough
    cases to stratify (try it: you’ll get an error). For this example, we won’t stratify;
    but in situations where you have very few cases in a class, you should consider
    whether there is enough data to justify keeping that class in the model.'
  id: totrans-1482
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，如果类别不平衡，我会使用分层抽样。然而，在这里，由于某些类别中案例非常少，没有足够多的案例来进行分层（试一试：你会得到一个错误）。在这个例子中，我们不会进行分层；但在你有一个类别中案例非常少的情况下，你应该考虑是否有足够的数据来证明保留该类别在模型中的合理性。
- en: '|  |'
  id: totrans-1483
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 7.6\. Defining the random search
  id: totrans-1484
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.6\. 定义随机搜索
- en: '[PRE104]'
  id: totrans-1485
  prefs: []
  type: TYPE_PRE
  zh: '[PRE104]'
- en: Finally, let’s perform our hyperparameter tuning!
  id: totrans-1486
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们执行超参数调整！
- en: Listing 7.7\. Performing hyperparameter tuning
  id: totrans-1487
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.7\. 执行超参数调整
- en: '[PRE105]'
  id: totrans-1488
  prefs: []
  type: TYPE_PRE
  zh: '[PRE105]'
- en: To speed things up, we first start parallelization by running `parallelStartSocket()`,
    setting the number of CPUs equal to the number we have available.
  id: totrans-1489
  prefs: []
  type: TYPE_NORMAL
  zh: 为了加快速度，我们首先通过运行`parallelStartSocket()`来启动并行化，将 CPU 的数量设置为可用的数量。
- en: '|  |'
  id: totrans-1490
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1491
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: If you want to use your computers for other things while tuning occurs, you
    may wish to set the number of CPUs used to fewer than the maximum available to
    you.
  id: totrans-1492
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想在调整过程中使用计算机做其他事情，你可能希望将使用的 CPU 数量设置为小于你可用的最大数量。
- en: '|  |'
  id: totrans-1493
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Then we use the `tuneParams()` function to start the tuning process. The arguments
    are the same as we’ve used previously: the first is the learner, the second is
    the task, `resampling` is the cross-validation method, `par.set` is the hyperparameter
    space, and `control` is the search method. Once it’s completed, we stop parallelization
    and print our tuning results.'
  id: totrans-1494
  prefs: []
  type: TYPE_NORMAL
  zh: 然后我们使用`tuneParams()`函数开始调整过程。参数与之前使用的一样：第一个是学习者，第二个是任务，`resampling`是交叉验证方法，`par.set`是超参数空间，`control`是搜索方法。一旦完成，我们停止并行化并打印调整结果。
- en: '|  |'
  id: totrans-1495
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1496
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 30 seconds to run on my four-core machine.
  id: totrans-1497
  prefs: []
  type: TYPE_NORMAL
  zh: 在我的四核机器上运行大约需要30秒。
- en: '|  |'
  id: totrans-1498
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The rpart algorithm isn’t nearly as computationally expensive as the support
    vector machine (SVM) algorithm we used for classification in [chapter 6](kindle_split_016.html#ch06).
    Therefore, despite tuning four hyperparameters, the tuning process doesn’t take
    as long (which means we can perform more search iterations).
  id: totrans-1499
  prefs: []
  type: TYPE_NORMAL
  zh: rpart 算法在计算上并不像我们在[第 6 章](kindle_split_016.html#ch06)中用于分类的支持向量机（SVM）算法那样昂贵。因此，尽管调整了四个超参数，调整过程并不需要花费很长时间（这意味着我们可以进行更多的搜索迭代）。
- en: 7.4.1\. Training the model with the tuned hyperparameters
  id: totrans-1500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1\. 使用调整后的超参数训练模型
- en: Now that we’ve tuned our hyperparameters, we can train our final model using
    them. Just like in the previous chapter, we use the `setHyperPars()` function
    to create a learner using the tuned hyperparameters, which we access using `tunedTreePars$x`.
    We can then train the final model using the `train()` function, as usual.
  id: totrans-1501
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经调整了超参数，我们可以使用它们来训练最终的模型。就像上一章一样，我们使用`setHyperPars()`函数创建一个使用调整超参数的学习者，我们通过`tunedTreePars$x`来访问它。然后我们可以使用`train()`函数像往常一样训练最终的模型。
- en: Listing 7.8\. Training the final tuned model
  id: totrans-1502
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.8\. 训练最终调整后的模型
- en: '[PRE106]'
  id: totrans-1503
  prefs: []
  type: TYPE_PRE
  zh: '[PRE106]'
- en: One of the wonderful things about decision trees is how interpretable they are.
    The easiest way to interpret the model is to draw a graphical representation of
    the tree. There are a few ways of plotting decision tree models in R, but my favorite
    is the `rpart.plot()` function from the package of the same name. Let’s install
    the rpart.plot package first and then extract the model data using the `getLearnerModel()`
    function.
  id: totrans-1504
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树的一个美妙之处在于其可解释性。解释模型的最简单方法就是绘制树的图形表示。在R中绘制决策树模型有多种方法，但我最喜欢的是同名的`rpart.plot()`函数。让我们首先安装rpart.plot包，然后使用`getLearnerModel()`函数提取模型数据。
- en: Listing 7.9\. Plotting the decision tree
  id: totrans-1505
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.9\. 绘制决策树
- en: '[PRE107]'
  id: totrans-1506
  prefs: []
  type: TYPE_PRE
  zh: '[PRE107]'
- en: The first argument of the `rpart.plot()` function is the model data. Because
    we trained this model using mlr, the function will give us a warning that it cannot
    find the data used to train the model. We can safely ignore this warning, but
    if it irritates you as much as it irritates me, you can prevent it by supplying
    the argument `roundint = FALSE`. The function will also complain if we have more
    classes that its default color palette (neediest function ever!). Either ignore
    this or ask for a different palette by setting the `box.palette` argument equal
    to one of the predefined palettes (run `?rpart.plot` for a list of available palettes).
    The `type` argument changes how the tree is displayed. I quite like the simplicity
    of option 5, but check `?rpart.plot` to experiment with the other options.
  id: totrans-1507
  prefs: []
  type: TYPE_NORMAL
  zh: '`rpart.plot()`函数的第一个参数是模型数据。因为我们使用mlr训练了这个模型，所以函数会给出一个警告，表示它找不到用于训练模型的数据。我们可以安全地忽略这个警告，但如果它像对我一样让你感到烦恼，你可以通过提供`roundint
    = FALSE`参数来防止它。如果我们的类别比其默认调色板（最需要的函数！）多，该函数也会抱怨。要么忽略它，要么通过设置`box.palette`参数为预定义调色板之一来请求不同的调色板（运行`?rpart.plot`以获取可用调色板的列表）。`type`参数改变树显示的方式。我非常喜欢选项5的简洁性，但请检查`?rpart.plot`以实验其他选项。'
- en: The plot generated by [listing 7.9](#ch07ex09) is shown in [figure 7.8](#ch07fig08).
    Can you see how simple and interpretable the tree is? When predicting the classes
    of new cases, they start at the top (the root) and follow the branches based on
    the splitting criterion at each node.
  id: totrans-1508
  prefs: []
  type: TYPE_NORMAL
  zh: 由[列表7.9](#ch07ex09)生成的图显示在[图7.8](#ch07fig08)中。你能看到这棵树是多么简单且易于理解吗？在预测新案例的类别时，它们从顶部（根节点）开始，根据每个节点的分割标准跟随分支。
- en: The first node asks whether the animal produces milk or not. This split was
    chosen because it has the highest Gini gain of all candidate splits (it immediately
    discriminates mammals, which make up 41% of the training set from the other classes).
    The leaf nodes tell us which class is classified by that node and the proportions
    of each class in that node. For example, the leaf node that classifies cases as
    mollusc.et.al contains 83% mollusc.et.al cases and 17% insect cases. The percentage
    at the bottom of each leaf indicates the percentage of cases in the training set
    in this leaf.
  id: totrans-1509
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个节点询问动物是否产奶。这个分割被选择是因为它在所有候选分割中具有最高的Gini增益（它立即区分出哺乳动物，占训练集的41%，而其他类别）。叶节点告诉我们该节点分类了哪个类别以及该节点中每个类别的比例。例如，将案例分类为mollusc.et.al的叶节点包含83%的mollusc.et.al案例和17%的昆虫案例。每个叶节点底部的百分比表示该叶节点中训练集中案例的百分比。
- en: To inspect the *cp* values for each split, we can use the `printcp()` function.
    This function takes the model data as the first argument and an optional `digits`
    argument specifying how many decimal places to print in the output. There is some
    useful information in the output, such as the variables actually used for splitting
    the data and the root node error (the error before any splits). Finally, the output
    includes a table of the *cp* values for each split.
  id: totrans-1510
  prefs: []
  type: TYPE_NORMAL
  zh: 要检查每个分割的*cp*值，我们可以使用`printcp()`函数。此函数将模型数据作为第一个参数，并可选地使用`digits`参数指定输出中要打印的小数位数。输出中包含一些有用的信息，例如实际用于分割数据的变量和根节点错误（任何分割之前的错误）。最后，输出还包括每个分割的*cp*值表。
- en: Figure 7.8\. Graphical representation of our decision tree model. The splitting
    criterion is shown for each node. Each leaf node shows the predicted class, the
    proportion of each of the classes in that leaf, and the proportion of all cases
    in that leaf.
  id: totrans-1511
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8\. 我们决策树模型的图形表示。每个节点显示了分割标准。每个叶节点显示了预测的类别，该叶节点中每个类别的比例，以及该叶节点中所有案例的比例。
- en: '![](fig7-8_alt.jpg)'
  id: totrans-1512
  prefs: []
  type: TYPE_IMG
  zh: '![fig7-8_alt.jpg]'
- en: Listing 7.10\. Exploring the model
  id: totrans-1513
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.10\. 探索模型
- en: '[PRE108]'
  id: totrans-1514
  prefs: []
  type: TYPE_PRE
  zh: '[PRE108]'
- en: 'Remember that in [section 7.1.3](#ch07lev2sec3), I showed you how the *cp*
    values were calculated:'
  id: totrans-1515
  prefs: []
  type: TYPE_NORMAL
  zh: 记住，在 [第 7.1.3 节](#ch07lev2sec3) 中，我向您展示了 *cp* 值是如何计算的：
- en: '![](pg182.jpg)'
  id: totrans-1516
  prefs: []
  type: TYPE_IMG
  zh: '![](pg182.jpg)'
- en: So that you can get a better understanding of what the *cp* value means, let’s
    work through how the *cp* values were calculated in the table in [listing 7.10](#ch07ex10).
  id: totrans-1517
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让您更好地理解 *cp* 值的含义，让我们通过查看 [列表 7.10](#ch07ex10) 中的表格来了解 *cp* 值是如何计算的。
- en: The *cp* value for the first split is
  id: totrans-1518
  prefs: []
  type: TYPE_NORMAL
  zh: 第一次分割的 *cp* 值为
- en: '![](pg183-1.jpg)'
  id: totrans-1519
  prefs: []
  type: TYPE_IMG
  zh: '![](pg183-1.jpg)'
- en: The *cp* value for the second split is
  id: totrans-1520
  prefs: []
  type: TYPE_NORMAL
  zh: 第二次分割的 *cp* 值为
- en: '![](pg183-2.jpg)'
  id: totrans-1521
  prefs: []
  type: TYPE_IMG
  zh: '![](pg183-2.jpg)'
- en: and so on. If any candidate split would yield a *cp* value lower than the threshold
    set by tuning, the node is not split further.
  id: totrans-1522
  prefs: []
  type: TYPE_NORMAL
  zh: 等等。如果任何候选分割会产生低于调整设置的阈值的 *cp* 值，则节点不会进一步分割。
- en: '|  |'
  id: totrans-1523
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1524
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: For a detailed summary of the model, run `summary(treeModelData)`. The output
    is quite long (and gets longer the deeper your tree goes), so I won’t print it
    here. It includes the *cp* table, orders the predictors by their importance, and
    displays the primary and surrogate splits for each node.
  id: totrans-1525
  prefs: []
  type: TYPE_NORMAL
  zh: 要获取模型的详细摘要，请运行 `summary(treeModelData)`。输出相当长（随着树的深入而变长），所以这里不打印。它包括 *cp* 表，按重要性排序预测变量，并显示每个节点的初级和替代分割。
- en: '|  |'
  id: totrans-1526
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.5\. Cross-validating our decision tree model
  id: totrans-1527
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 对我们的决策树模型进行交叉验证
- en: 'In this section, we’ll cross-validate our model-building process, including
    hyperparameter tuning. We’ve done this a few times already now, but it’s so important
    that I’m going to reiterate: you *must* include data-dependent preprocessing in
    your cross-validation. This includes the hyperparameter tuning we performed in
    [listing 7.7](#ch07ex07).'
  id: totrans-1528
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将交叉验证我们的模型构建过程，包括超参数调整。我们已经这样做了几次，但这是如此重要，以至于我要重申：您 *必须* 在交叉验证中包含数据相关的预处理。这包括我们在
    [列表 7.7](#ch07ex07) 中执行的超参数调整。
- en: First, we define our outer cross-validation strategy. This time I’m using 5-fold
    cross-validation as my outer cross-validation loop. We’ll use the `cvForTuning`
    resampling description we made in [listing 7.6](#ch07ex06) for the inner loop.
  id: totrans-1529
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们定义我们的外部交叉验证策略。这次我使用 5 折交叉验证作为我的外部交叉验证循环。我们将使用在 [列表 7.6](#ch07ex06) 中创建的
    `cvForTuning` 重采样描述作为内部循环。
- en: Next, we create our wrapper by “wrapping together” our learner and hyperparameter
    tuning process. We supply our inner cross-validation strategy, hyperparameter
    space, and search method to the `makeTuneWrapper()` function.
  id: totrans-1530
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过“包装在一起”我们的学习器和超参数调整过程来创建包装器。我们将内部交叉验证策略、超参数空间和搜索方法提供给 `makeTuneWrapper()`
    函数。
- en: Finally, we can start parallelization with the `parallelStartSocket()` function,
    and start the cross-validation process with the `resample()` function. The `resample()`
    function takes our wrapped learner, task, and outer cross-validation strategy
    as arguments.
  id: totrans-1531
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用 `parallelStartSocket()` 函数开始并行化，并使用 `resample()` 函数开始交叉验证过程。`resample()`
    函数将我们的包装学习器、任务和外部交叉验证策略作为参数。
- en: '|  |'
  id: totrans-1532
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 2 minutes on my four-core machine.
  id: totrans-1534
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要 2 分钟。
- en: '|  |'
  id: totrans-1535
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 7.11\. Cross-validating the model-building process
  id: totrans-1536
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.11\. 交叉验证模型构建过程
- en: '[PRE109]'
  id: totrans-1537
  prefs: []
  type: TYPE_PRE
  zh: '[PRE109]'
- en: Now let’s look at the cross-validation result and see how our model-building
    process performed.
  id: totrans-1538
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们看看交叉验证结果，看看我们的模型构建过程表现如何。
- en: Listing 7.12\. Extracting the cross-validation result
  id: totrans-1539
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.12\. 提取交叉验证结果
- en: '[PRE110]'
  id: totrans-1540
  prefs: []
  type: TYPE_PRE
  zh: '[PRE110]'
- en: Hmm, that’s a little disappointing, isn’t it? During hyperparameter tuning,
    the best hyperparameter combination gave us a mean misclassification error (MMCE)
    of 0.0698 (you likely got a different value). But our cross-validated estimate
    of model performance gives us an MMCE of 0.12\. Quite a large difference! What’s
    going on? Well, this is an example of overfitting. Our model is performing better
    during hyperparameter tuning than during cross-validation. This is also a good
    example of why it’s important to include hyperparameter tuning inside our cross-validation
    procedure.
  id: totrans-1541
  prefs: []
  type: TYPE_NORMAL
  zh: 嗯，这有点令人失望，不是吗？在超参数调整期间，最佳超参数组合给我们带来了平均误分类误差 (MMCE) 为 0.0698（你很可能得到了不同的值）。但我们的交叉验证模型性能估计给出了
    MMCE 为 0.12。相当大的差异！发生了什么事？嗯，这是一个过度拟合的例子。我们的模型在超参数调整期间的表现比在交叉验证期间要好。这也是为什么在交叉验证过程中包含超参数调整很重要的好例子。
- en: 'We’ve just discovered the main problem with the rpart algorithm (and decision
    trees in general): they tend to produce models that are overfit. How do we overcome
    this problem? The answer is to use an *ensemble method*, an approach where we
    use multiple models to make predictions for a single task. In the next chapter,
    I’ll show you how ensemble methods work, and we’ll use them to vastly improve
    our decision tree model. I suggest that you save your .R file, as we’re going
    to continue using the same dataset and task in the next chapter. This is so I
    can highlight for you how much better these ensemble techniques are, compared
    to ordinary decision trees.'
  id: totrans-1542
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚发现了rpart算法（以及决策树一般）的主要问题：它们倾向于产生过拟合的模型。我们如何克服这个问题？答案是使用**集成方法**，这是一种我们使用多个模型对一个单一任务进行预测的方法。在下一章中，我将向您展示集成方法是如何工作的，我们将使用它们来大幅提高我们的决策树模型。我建议您保存您的.R文件，因为我们将在下一章继续使用相同的dataset和task。这样我就可以向您突出显示这些集成技术相比普通决策树有多好。
- en: 7.6\. Strengths and weaknesses of tree-based algorithms
  id: totrans-1543
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6. 基于树的算法的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    decision trees will perform well for you.
  id: totrans-1544
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪些算法会对给定的任务表现良好，但以下是一些优势和弱点，这将帮助您决定决策树是否适合您。
- en: 'The strengths of tree-based algorithms are as follows:'
  id: totrans-1545
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法的优点如下：
- en: The intuition behind tree-building is quite simple, and each individual tree
    is very interpretable.
  id: totrans-1546
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树构建背后的直觉非常简单，每个单独的树都非常可解释。
- en: It can handle categorical and continuous predictor variables.
  id: totrans-1547
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理分类和连续预测变量。
- en: It makes no assumptions about the distribution of the predictor variables.
  id: totrans-1548
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它对预测变量的分布没有假设。
- en: It can handle missing values in sensible ways.
  id: totrans-1549
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以以合理的方式处理缺失值。
- en: It can handle continuous variables on different scales.
  id: totrans-1550
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理不同尺度的连续变量。
- en: 'The weakness of tree-based algorithms is this:'
  id: totrans-1551
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法的弱点如下：
- en: Individual trees are very susceptible to overfitting—so much so that they are
    rarely used.
  id: totrans-1552
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 单个树模型非常容易过拟合——如此之容易以至于它们很少被使用。
- en: Summary
  id: totrans-1553
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The rpart algorithm is a supervised learner for both classification and regression
    problems.
  id: totrans-1554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: rpart算法是用于分类和回归问题的监督学习器。
- en: Tree-based learners start with all the cases in the root node and find sequential
    binary splits until cases find themselves in leaf nodes.
  id: totrans-1555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于树的算法从根节点中的所有案例开始，找到顺序的二分分割，直到案例找到自己在叶节点中。
- en: Tree construction is a greedy process and can be limited by setting stopping
    criteria (such as the minimum number of cases required in a node before it can
    be split).
  id: totrans-1556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 树构建是一个贪婪的过程，可以通过设置停止标准（例如，在节点可以分割之前所需的案例的最小数量）来限制。
- en: The Gini gain is a criterion used to decide which predictor variable will result
    in the best split at a particular node.
  id: totrans-1557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Gini增益是一个用于决定在特定节点上哪个预测变量将导致最佳分割的标准。
- en: Decision trees have a tendency to overfit the training set.
  id: totrans-1558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树有过度拟合训练集的倾向。
- en: Chapter 8\. Improving decision trees with random forests and boosting
  id: totrans-1559
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章. 使用随机森林和boosting改进决策树
- en: '*This chapter covers*'
  id: totrans-1560
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章涵盖**'
- en: Understanding ensemble methods
  id: totrans-1561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解集成方法
- en: Using bagging, boosting, and stacking
  id: totrans-1562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用bagging、boosting和stacking
- en: Using the random forest and XGBoost algorithms
  id: totrans-1563
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用随机森林和XGBoost算法
- en: Benchmarking multiple algorithms against the same task
  id: totrans-1564
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将多个算法与同一任务进行基准测试
- en: 'In the last chapter, I showed you how we can use the recursive partitioning
    algorithm to train decision trees that are very interpretable. We finished by
    highlighting an important limitation of decision trees: they have a tendency to
    overfit the training set. This results in models that generalize poorly to new
    data. As a result, individual decision trees are rarely used, but they can become
    extremely powerful predictors when many trees are combined together.'
  id: totrans-1565
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我向您展示了我们可以如何使用递归分割算法来训练非常可解释的决策树。我们通过强调决策树的一个重要限制结束：它们有过度拟合训练集的倾向。这导致模型对新数据的泛化能力差。因此，单个决策树很少被使用，但当许多树组合在一起时，它们可以成为极其强大的预测因子。
- en: By the end of this chapter, you’ll understand the difference between ordinary
    decision trees and *ensemble methods*, such as *random forest* and *gradient boosting*,
    which combine multiple trees to make predictions. Finally, as this is the last
    chapter in the classification part of the book, you’ll learn what *benchmarking*
    is and how to use it to find the best-performing algorithm for a particular problem.
    Benchmarking is the process of letting a bunch of different learning algorithms
    fight it out to select the one that performs best for a particular problem.
  id: totrans-1566
  prefs: []
  type: TYPE_NORMAL
  zh: 到本章结束时，你将了解普通决策树和*集成方法*（如*随机森林*和*梯度提升*），这些方法通过结合多个树来进行预测之间的区别。最后，由于这是本书分类部分的最后一章，你将学习什么是*基准测试*以及如何使用它来找到特定问题的最佳性能算法。基准测试是让一组不同的学习算法竞争，以选择在特定问题中表现最佳的算法的过程。
- en: We will continue to work with the zoo dataset we were using in the previous
    chapter. If you no longer have the `zooTib`, `zooTask`, and `tunedTree` objects
    defined in your global environment (run `ls()` to find out), just rerun [listings
    7.1](kindle_split_017.html#ch07ex01) through [7.8](kindle_split_017.html#ch07ex08)
    from the previous chapter.
  id: totrans-1567
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将继续使用我们在上一章中使用过的动物园数据集。如果你在你的全局环境中不再有`zooTib`、`zooTask`和`tunedTree`对象定义（运行`ls()`以查找），只需重新运行上一章的[列表7.1](kindle_split_017.html#ch07ex01)到[7.8](kindle_split_017.html#ch07ex08)。
- en: '8.1\. Ensemble techniques: Bagging, boosting, and stacking'
  id: totrans-1568
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1. 集成技术：Bagging、Boosting 和 Stacking
- en: 'In this section, I’ll show you what ensemble methods are and how they can be
    used to improve the performance of tree-based models. Imagine that you wanted
    to know what a country’s views were on a particular issue. What would you consider
    to be a better barometer of public opinion: the opinion of a single person you
    ask on the street, or the collective vote of many people at the ballot box? In
    this scenario, the decision tree is the single person on the street. You create
    a single model, pass it new data, and ask its opinion as to what the predicted
    output should be. Ensemble methods, on the other hand, are the collective vote.'
  id: totrans-1569
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示什么是集成方法以及它们如何被用来提高基于树的模型的性能。想象一下，如果你想知道一个国家在某个特定问题上的观点，你会认为哪个更能反映公众舆论：你在街上询问的某一个人的观点，还是许多人在投票箱上的集体投票？在这个场景中，决策树就是街上那个人。你创建一个单一模型，传递新数据，并询问其关于预测输出的观点。另一方面，集成方法是集体投票。
- en: The idea behind ensemble methods is that instead of training a single model,
    you train multiple models (sometimes hundreds or even thousands of models). Next,
    you ask the opinion of each of those models as to what the predicted output should
    be for new data. You then consider the votes from all the models when making the
    final prediction. The idea is that predictions informed by a majority vote will
    have less variance than predictions made by a lone model.
  id: totrans-1570
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法背后的想法是，而不是训练一个单一模型，你训练多个模型（有时是数百甚至数千个模型）。接下来，你询问每个模型关于新数据预测输出的观点。然后，在做出最终预测时，你考虑所有模型的投票。想法是，基于多数投票的预测将比单个模型做出的预测具有更小的方差。
- en: 'There are three different ensemble methods:'
  id: totrans-1571
  prefs: []
  type: TYPE_NORMAL
  zh: 有三种不同的集成方法：
- en: Bootstrap aggregating
  id: totrans-1572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bootstrap aggregating
- en: Boosting
  id: totrans-1573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting
- en: Stacking
  id: totrans-1574
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Stacking
- en: Let’s discuss each of these in more detail.
  id: totrans-1575
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们更详细地讨论这些内容。
- en: '8.1.1\. Training models on sampled data: Bootstrap aggregating'
  id: totrans-1576
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.1. 在样本数据上训练模型：Bootstrap aggregating
- en: In this section, I’ll explain the principle of the bootstrap aggregating ensemble
    technique, and how this is used in an algorithm called *random forest*. Machine
    learning algorithms can be sensitive to noise resulting from outliers and measurement
    error. If noisy data exists in our training set, then our models are more likely
    to have high variance when making predictions on future data. How can we train
    a learner that makes use of all the data available to us, but can look past this
    noisy data and reduce prediction variance? The answer is to use *b*ootstrap *agg*regat*ing*
    (or *bagging* for short).
  id: totrans-1577
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释bootstrap aggregating集成技术的原理，以及这是如何在称为*随机森林*的算法中使用的。机器学习算法可能对异常值和测量误差产生的噪声敏感。如果我们的训练集中存在噪声数据，那么我们的模型在预测未来数据时更有可能具有高方差。我们如何训练一个利用我们所有可用数据的学习者，但可以忽略这些噪声数据并减少预测方差？答案是使用*b*ootstrap
    *agg*regat*ing*（或简称为*bagging*）。
- en: 'The premise of bagging is quite simple:'
  id: totrans-1578
  prefs: []
  type: TYPE_NORMAL
  zh: Bagging的前提非常简单：
- en: Decide how many sub-models you’re going to train.
  id: totrans-1579
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 决定你将要训练多少个子模型。
- en: For each sub-model, randomly sample cases from the training set, with replacement,
    until you have a sample the same size as the original training set.
  id: totrans-1580
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个子模型，从训练集中随机采样案例，有放回地采样，直到你有一个与原始训练集大小相同的样本。
- en: Train a sub-model on each sample of cases.
  id: totrans-1581
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在每个案例样本上训练一个子模型。
- en: Pass new data through each sub-model, and let them vote on the prediction.
  id: totrans-1582
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将新数据通过每个子模型，并让他们对预测进行投票。
- en: The *modal* prediction (the most frequent prediction) from all the sub-models
    is used as the predicted output.
  id: totrans-1583
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 所有子模型的**模态**预测（最频繁的预测）被用作预测输出。
- en: The most critical part of bagging is the random sampling of the cases. Imagine
    that you’re playing Scrabble and have the bag of 100 letter tiles. Now imagine
    that you put your hand into the bag, blindly rummage around a little, pull out
    a tile, and write down what letter you got. This is taking a random sample. Then,
    crucially, you put the tile back. This is called *replacement*, and sampling with
    replacement simply means putting the values back after you’ve drawn them. This
    means the same value could be drawn again. You continue to do this until you have
    drawn 100 random samples, the same number as are in the bag to begin with. This
    process is called *bootstrapping* and is an important technique in statistics
    and machine learning. Your bootstrap sample of 100 tiles should do a reasonable
    job of reflecting the frequencies of each letter in the original bag.
  id: totrans-1584
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法最关键的部分是案例的随机采样。想象一下你在玩Scrabble游戏，手里有一袋100个字母棋子。现在想象你把手伸进袋子，盲目地翻找一番，抽出一个棋子，并写下你得到的字母。这就是随机采样。然后，关键的是，你把棋子放回去。这被称为**放回**，有放回地采样简单来说就是在你抽取值之后将它们放回。这意味着相同的值可能再次被抽取。你继续这样做，直到你抽取了100个随机样本，这与最初袋子里的数量相同。这个过程被称为**自举**，是统计学和机器学习中的一个重要技术。你的100个棋子的自举样本应该能够合理地反映原始袋子中每个字母的频率。
- en: 'So why does training sub-models on bootstrap samples of the training set help
    us? Imagine that cases are distributed over their feature space. Each time we
    take a bootstrap sample, because we are sampling with replacement, we are more
    likely to select a case near the center of that distribution than a case that
    lies near the extremes of the distribution. Some of the bootstrap samples may
    contain many extreme values and make poor predictions on their own, but here’s
    the second crucial part of bagging: we *aggregate* the predictions of all these
    models. This simply means we let them all make their predictions and then take
    the majority vote. The effect of this is a sort of averaging out of all the models,
    which reduces the impact of noisy data and reduces overfitting. Bagging for decision
    trees is illustrated in [figure 8.1](#ch08fig01).'
  id: totrans-1585
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，为什么在训练集的自举样本上训练子模型能帮助我们呢？想象一下案例分布在它们的特征空间中。每次我们进行自举样本采样时，因为我们是在有放回地采样，所以我们更有可能选择一个位于分布中心的案例，而不是一个位于分布极端的案例。一些自举样本可能包含许多极端值，并且它们自己做出的预测可能很差，但这里是袋装法的第二个关键部分：我们**聚合**了所有这些模型的预测。这仅仅意味着我们让它们都做出预测，然后进行多数投票。这种效果是所有模型的一种平均，这减少了噪声数据的影响，并减少了过拟合。决策树的袋装法在[图8.1](#ch08fig01)中展示。
- en: Figure 8.1\. Bootstrap aggregating (bagging) with decision trees. Multiple decision
    trees are learned in parallel, each one trained on a bootstrap sample of cases
    from the training set. When predicting new data, each tree makes a prediction,
    and the modal (most frequent) prediction wins.
  id: totrans-1586
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1\. 使用决策树的袋装法（bootstrap aggregating，bagging）。并行学习多个决策树，每个决策树都训练于训练集中案例的自举样本。在预测新数据时，每棵树做出一个预测，并且模态（最频繁）的预测获胜。
- en: '![](fig8-1_alt.jpg)'
  id: totrans-1587
  prefs: []
  type: TYPE_IMG
  zh: '![图8-1](fig8-1_alt.jpg)'
- en: Bagging (and, as you’ll learn, boosting and stacking) is a technique that can
    be applied to any supervised machine learning algorithm. Having said this, it
    works best on algorithms that tend to create low-bias, high-variance models, such
    as decision trees. In fact, there is a famous and very popular implementation
    of bagging for decision trees called *random forest*. Why is it called random
    forest? Well, it uses many random samples from the training set to train a decision
    tree. What do many trees make? A forest!
  id: totrans-1588
  prefs: []
  type: TYPE_NORMAL
  zh: 袋装法（以及你将学习的提升和堆叠）是一种可以应用于任何监督机器学习算法的技术。话虽如此，它最适合那些倾向于创建低偏差、高方差模型的算法，例如决策树。事实上，有一个著名的、非常流行的决策树袋装法实现，称为**随机森林**。为什么它被称为随机森林呢？因为它使用了训练集中的许多随机样本来训练决策树。许多树能做什么？一个森林！
- en: '|  |'
  id: totrans-1589
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1590
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: Although the “no free lunch” theorem still applies (as mentioned in [chapter
    1](kindle_split_010.html#ch01)), individual decision trees seldom perform better
    than their random forest counterparts. For this reason, I may build a decision
    tree to get a broad understanding of the relationships in my data, but I tend
    to jump straight in with an ensemble technique for predictive modeling.
  id: totrans-1591
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管仍然适用“没有免费午餐”定理（如[第1章](kindle_split_010.html#ch01)中提到的），单个决策树很少比它们的随机森林对应物表现更好。因此，我可能会构建一个决策树来对数据中的关系有一个广泛的理解，但我倾向于直接使用集成技术进行预测建模。
- en: '|  |'
  id: totrans-1592
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: So the random forest algorithm uses bagging to create a large number of trees.
    These trees are saved as part of the model; when we pass the model new data, each
    tree makes its own prediction, and the modal prediction is returned. The random
    forest algorithm has one extra trick up its sleeve, however. At each node of a
    particular tree, the algorithm randomly selects a proportion of the predictor
    variables it will consider for that split. At the next node, the algorithm makes
    another random selection of predictor variables it will consider for that split,
    and so on. While this may seem counterintuitive, the result of randomly sampling
    cases *and* randomly sampling features is to create individual trees that are
    highly *uncorrelated*.
  id: totrans-1593
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，随机森林算法使用bagging来创建大量树。这些树作为模型的一部分被保存；当我们向模型传递新数据时，每棵树都会做出自己的预测，并返回模型预测。然而，随机森林算法还有一个额外的技巧。在特定树的每个节点上，算法随机选择一个比例的预测变量，它将考虑用于该分割。在下一个节点上，算法再次随机选择用于该分割的预测变量，依此类推。虽然这看起来可能有些不合逻辑，但随机抽样案例和随机抽样特征的组合结果是创建出高度*非相关*的单独树。
- en: '|  |'
  id: totrans-1594
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1595
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: If some variables in the data are highly predictive of the outcome, then these
    variables will be selected as split criteria for many of the trees. Trees that
    contain the same splits as each other don’t contribute any more information. This
    is why it’s desirable to have uncorrelated trees, so that different trees contribute
    different predictive information. Randomly sampling *cases* reduces the impact
    that noise and outlying cases have on the model.
  id: totrans-1596
  prefs: []
  type: TYPE_NORMAL
  zh: 如果数据中的一些变量高度预测结果，那么这些变量将被选为许多树的分割标准。包含相同分割的树不会提供更多信息。这就是为什么希望有非相关树，以便不同的树提供不同的预测信息。随机抽样*案例*可以减少噪声和异常案例对模型的影响。
- en: '|  |'
  id: totrans-1597
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '8.1.2\. Learning from the previous models’ mistakes: Boosting'
  id: totrans-1598
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.2. 从先前模型的错误中学习：Boosting
- en: In this section, I’ll explain the principle of the boosting ensemble technique
    and how it is used in algorithms called *AdaBoost*, *XGBoost*, and others. With
    bagging, the individual models are trained in parallel. In contrast, boosting
    is an ensemble technique that, again, trains many individual models, but builds
    them sequentially. Each additional model seeks to correct the mistakes of the
    previous ensemble of models.
  id: totrans-1599
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释boosting集成技术的原理以及它在称为*AdaBoost*、*XGBoost*和其他算法中的应用。与bagging不同，boosting是一种集成技术，它再次训练许多单个模型，但按顺序构建它们。每个额外的模型都试图纠正先前集成模型的错误。
- en: Just like bagging, boosting can be applied to any supervised machine learning
    algorithm. However, boosting is most beneficial when using *weak learners* as
    the sub-models. By *weak learner*, I don’t mean someone who keeps failing their
    driving test; I mean a model that only does a little better at making predictions
    than a random guess. For this reason, boosting has been traditionally applied
    to shallow decision trees. By *shallow*, I mean a decision tree that doesn’t have
    many levels of depth, or may have only a single split.
  id: totrans-1600
  prefs: []
  type: TYPE_NORMAL
  zh: 就像bagging一样，boosting可以应用于任何监督机器学习算法。然而，当使用*弱学习器*作为子模型时，boosting最有益。这里的*弱学习器*不是指那些驾驶考试总是不及格的人；我指的是一个在预测上仅略好于随机猜测的模型。因此，boosting传统上应用于浅层决策树。这里的*浅层*是指没有很多层深度的决策树，或者可能只有一个分割。
- en: '|  |'
  id: totrans-1601
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1602
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Decision trees with only one split are imaginatively called *decision stumps*.
    You can see an example of a decision stump if you look back at [figure 7.2](kindle_split_017.html#ch07fig02).
  id: totrans-1603
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一个分割的决策树被富有想象力地称为*决策桩*。如果你回顾[图7.2](kindle_split_017.html#ch07fig02)，你可以看到一个决策桩的例子。
- en: '|  |'
  id: totrans-1604
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The function of boosting is to combine many weak learners together to form one
    strong ensemble learner. The reason we use weak learners is that there is no improvement
    in model performance when boosting with strong learners versus weak learners.
    So why waste computational resources training hundreds of strong, probably more
    complex learners, when we can get the same performance by training weak, less
    complex ones?
  id: totrans-1605
  prefs: []
  type: TYPE_NORMAL
  zh: 提升的功能是将许多弱学习器组合在一起形成一个强大的集成学习器。我们使用弱学习器的原因是，与弱学习器相比，使用强学习器进行提升并不会提高模型性能。所以，为什么我们要浪费计算资源来训练数百个强大、可能更复杂的学习者，当我们可以通过训练弱、更简单的一些来获得相同性能时？
- en: 'There are two methods of boosting, which differ in the way they correct the
    mistakes of the previous set of models:'
  id: totrans-1606
  prefs: []
  type: TYPE_NORMAL
  zh: 提升模型性能有两种方法，它们在纠正先前模型错误的方式上有所不同：
- en: Adaptive boosting
  id: totrans-1607
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 自适应提升
- en: Gradient boosting
  id: totrans-1608
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 梯度提升
- en: 'Weighting incorrectly predicted cases: Adaptive boosting'
  id: totrans-1609
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重错误预测的案例：自适应提升
- en: There is only one well-known adaptive boosting algorithm, which is the famous
    AdaBoost algorithm published in 1997\. AdaBoost works as follows. Initially, all
    cases in the training set have the same importance, or *weight*. An initial model
    is trained on a bootstrap sample of the training set where the probability of
    a case being sampled is proportional to its weight (all equal at this point).
    The cases that this initial model incorrectly classifies are given more weight/importance,
    while cases that it correctly classifies are given less weight/importance.
  id: totrans-1610
  prefs: []
  type: TYPE_NORMAL
  zh: 只有一种著名的自适应提升算法，那就是1997年发表的著名AdaBoost算法。AdaBoost的工作原理如下。最初，训练集中的所有案例都具有相同的重要性，或者说*权重*。一个初始模型在训练集的bootstrap样本上训练，其中案例被采样的概率与其权重成正比（在这个点上都是相等的）。这个初始模型错误分类的案例被赋予更多的权重/重要性，而正确分类的案例则被赋予较少的权重/重要性。
- en: The next model takes another bootstrap sample from the training set, but the
    weights are no longer equal. Remember that the probability of a case being sampled
    is proportional to its weight. So, a case with twice as much weight as another
    case is twice as likely to be sampled (and more likely to be sampled repeatedly).
    This ensures that cases incorrectly classified by the previous model are more
    likely to be featured in the bootstrap for the subsequent model. The subsequent
    model is therefore more likely to learn rules that will correctly classify these
    cases.
  id: totrans-1611
  prefs: []
  type: TYPE_NORMAL
  zh: 下一个模型从训练集中再次进行bootstrap采样，但权重不再相等。记住，案例被采样的概率与其权重成正比。因此，一个权重是另一个案例两倍的案例更有可能被采样（并且更有可能被反复采样）。这确保了先前模型错误分类的案例更有可能出现在后续模型的bootstrap中。因此，后续模型更有可能学习到能够正确分类这些案例的规则。
- en: Once we have at least two models, the data are classified based on an aggregated
    vote, just like in bagging. Cases that are incorrectly classified by the majority
    vote are then given more weight, and cases that are correctly classified by the
    majority vote are given less weight. Perhaps slightly confusingly, the models
    themselves also have a weight. This model weight is based on how many mistakes
    a particular model makes (more mistakes, less weight). If you only have two models
    in an ensemble, one of which predicts group A and the other of which predicts
    group B, the model with the higher weight wins the vote.
  id: totrans-1612
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们至少有两个模型，数据就会根据聚合投票进行分类，就像在bagging中一样。然后，被多数投票错误分类的案例会被赋予更多的权重，而被多数投票正确分类的案例则会被赋予较少的权重。可能有些令人困惑的是，模型本身也有权重。这个模型权重基于特定模型犯的错误数量（错误越多，权重越少）。如果你只有一个集成中的两个模型，其中一个预测组A，另一个预测组B，那么权重更高的模型将赢得投票。
- en: 'This process continues: a new model is added to the ensemble, all the models
    vote, weights are updated, and the next model samples the data based on the new
    weights. Once we reach the maximum number of predefined trees, the process stops,
    and we get our final ensemble model. This is illustrated in [figure 8.2](#ch08fig02).
    Think about the impact this is having: new models are correcting the mistakes
    of the previous set of models. This is why boosting is an excellent way of reducing
    bias. However, just like bagging, it also reduces variance, because we’re also
    taking bootstrap samples! When unseen cases are passed to the final model for
    prediction, each tree votes individually (like in bagging), but each vote is weighted
    by the model weight.'
  id: totrans-1613
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程持续进行：向集成中添加新的模型，所有模型进行投票，更新权重，下一个模型根据新的权重采样数据。一旦达到预定义的最大树的数量，过程停止，我们得到最终的集成模型。这如图8.2所示。[#ch08fig02]。考虑这种影响：新的模型正在纠正先前模型集的错误。这就是为什么提升是一种减少偏差的优秀方法。然而，就像袋装一样，它也减少了方差，因为我们也在进行自助采样！当未知的案例被传递到最终模型进行预测时，每棵树单独投票（就像在袋装中一样），但每个投票都由模型权重加权。
- en: Figure 8.2\. Adaptive boosting with decision trees. An initial model is trained
    on a random sample of the training set. Correctly classified cases get lower weights,
    while incorrectly classified cases get higher weights (indicated by data point
    size). The probability of subsequent models sampling each case is proportional
    to the case’s weight. As trees are added, they vote to form an ensemble model,
    the predictions of which are used to update the weights at every iteration.
  id: totrans-1614
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.2\. 基于决策树的自适应提升。初始模型在训练集的随机样本上训练。正确分类的案例获得较低的权重，而错误分类的案例获得较高的权重（由数据点大小表示）。后续模型采样每个案例的概率与案例的权重成正比。随着树的增加，它们投票形成一个集成模型，其预测用于在每次迭代中更新权重。
- en: '![](fig8-2_alt.jpg)'
  id: totrans-1615
  prefs: []
  type: TYPE_IMG
  zh: '![](fig8-2_alt.jpg)'
- en: '|  |'
  id: totrans-1616
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**How are the model weights and case weights calculated?**'
  id: totrans-1617
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型权重和案例权重是如何计算的？**'
- en: The model weight is calculated as
  id: totrans-1618
  prefs: []
  type: TYPE_NORMAL
  zh: 模型权重计算如下
- en: '![](pg191-1.jpg)'
  id: totrans-1619
  prefs: []
  type: TYPE_IMG
  zh: '![](pg191-1.jpg)'
- en: where ln is the natural logarithm and *p*(incorrect) is the proportion of incorrectly
    classified cases.
  id: totrans-1620
  prefs: []
  type: TYPE_NORMAL
  zh: 其中ln是自然对数，*p*(incorrect)是错误分类案例的比例。
- en: The case weights are calculated as
  id: totrans-1621
  prefs: []
  type: TYPE_NORMAL
  zh: 案例权重计算如下
- en: '![](pg191-2_alt.jpg)'
  id: totrans-1622
  prefs: []
  type: TYPE_IMG
  zh: '![](pg191-2_alt.jpg)'
- en: 'This notation simply means that for cases correctly classified, we use the
    formula on the top; and for cases incorrectly classified, we use the formula on
    the bottom. The only subtle difference is that the model weight is negative for
    cases that were correctly classified. Plug some numbers into these formulas: you’ll
    find that the formula for correctly classified cases decreases their weight, while
    the formula for incorrectly classified cases increases it.'
  id: totrans-1623
  prefs: []
  type: TYPE_NORMAL
  zh: 这种表示法只是意味着对于正确分类的案例，我们使用顶部的公式；对于错误分类的案例，我们使用底部的公式。唯一的细微差别是，对于正确分类的案例，模型权重是负的。将这些数字代入这些公式：你会发现，正确分类案例的公式会降低它们的权重，而错误分类案例的公式会增加它们的权重。
- en: '|  |'
  id: totrans-1624
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 'Learning from the previous models’ residuals: Gradient boosting'
  id: totrans-1625
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 从先前模型的残差中进行学习：梯度提升
- en: Gradient boosting is very similar to adaptive boosting, only differing in the
    way it corrects the mistakes of the previous models. Rather than weighting the
    cases differently depending on the accuracy of their classification, subsequent
    models try to predict the *residuals* of the previous ensemble of models.
  id: totrans-1626
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升与自适应提升非常相似，只是它在纠正先前模型错误的方式上有所不同。后续模型不是根据分类的准确性来不同地加权案例，而是尝试预测先前集成模型的*残差*。
- en: A *residual*, or residual error, is the difference between the true value (the
    “observed” value) and the value predicted by a model. This is easier to understand
    when thinking about predicting a continuous variable (regression). Imagine that
    you’re trying to predict how much debt a person has. If an individual has a real
    debt of $2,500, but our model predicts they have a debt of $2,100, the residual
    is $400\. It’s called a residual because it’s the error left over after the model
    has made its prediction.
  id: totrans-1627
  prefs: []
  type: TYPE_NORMAL
  zh: 一个*残差*，或残差误差，是真实值（“观察”值）与模型预测值之间的差异。当考虑预测一个连续变量（回归）时，这更容易理解。想象一下，你正在尝试预测一个人有多少债务。如果一个人实际债务为2,500美元，但我们的模型预测他们有2,100美元的债务，那么残差是400美元。它被称为残差，因为这是模型做出预测后留下的误差。
- en: It’s a bit harder to think of a residual for a classification model, but we
    can quantify the residual error of a classification model as
  id: totrans-1628
  prefs: []
  type: TYPE_NORMAL
  zh: 对于分类模型来说，思考残差可能有点困难，但我们可以将分类模型的残差误差量化为
- en: The proportion of all cases incorrectly classified
  id: totrans-1629
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 所有被错误分类的案例的比例
- en: The *log loss*
  id: totrans-1630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*对数损失*'
- en: The proportion of cases that were misclassified is pretty self-explanatory.
    The log loss is similar but more greatly penalizes a model that makes incorrect
    classifications *confidently*. If your friend tells you with “absolute certainty”
    that Helsinki is the capital of Sweden (it’s not), you’d think less of them than
    if they said they “think it might be” the capital. This is how log loss treats
    misclassification error. For either method, models that give the correct classifications
    will have a lower error than those that make lots of misclassifications. Which
    method is better? Once again it depends, so we’ll let hyperparameter tuning choose
    the best one.
  id: totrans-1631
  prefs: []
  type: TYPE_NORMAL
  zh: 被错误分类的案例比例相当直观。对数损失类似，但更严厉地惩罚那些自信地做出错误分类的模型。如果你的朋友“绝对肯定”地告诉你赫尔辛基是瑞典的首都（它不是），你可能会对他们有更少的信心，而如果他们说是“可能”，你可能会对他们有更多的信心。这就是对数损失对待错误分类误差的方式。对于任何一种方法，给出正确分类的模型将比那些做出大量错误分类的模型具有更低的误差。哪种方法更好？再一次，这取决于具体情况，所以我们将让超参数调整来选择最好的一个。
- en: '|  |'
  id: totrans-1632
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1633
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Using the proportion of misclassified cases as the residual error tends to result
    in models that are a little more tolerant of a small number of misclassified cases
    than using the log loss. These measures of residual error that are minimized at
    each iteration are called *loss functions*.
  id: totrans-1634
  prefs: []
  type: TYPE_NORMAL
  zh: 使用被错误分类的案例比例作为残差误差往往会导致模型对少量错误分类的案例的容忍度略高，而使用对数损失则不然。在每个迭代中最小化的这些残差误差度量被称为*损失函数*。
- en: '|  |'
  id: totrans-1635
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: So in gradient boosting, subsequent models are chosen that minimize the residual
    error of the previous ensemble of models. By minimizing the residual error, subsequent
    models will, in effect, favor the correct classification of cases that were previously
    misclassified (thereby modeling the residuals).
  id: totrans-1636
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在梯度提升中，后续模型的选择是为了最小化先前模型集的残差误差。通过最小化残差误差，后续模型实际上将更倾向于正确分类先前被错误分类的案例（从而建模残差）。
- en: '|  |'
  id: totrans-1637
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Calculating log loss**'
  id: totrans-1638
  prefs: []
  type: TYPE_NORMAL
  zh: '**计算对数损失**'
- en: It isn’t necessary for you to know the formula for log loss, but for math buffs
    who are interested, it is calculated as
  id: totrans-1639
  prefs: []
  type: TYPE_NORMAL
  zh: 你不需要知道对数损失的公式，但对于对数学感兴趣的数学爱好者来说，它是这样计算的
- en: '![](pg192.jpg)'
  id: totrans-1640
  prefs: []
  type: TYPE_IMG
  zh: '![](pg192.jpg)'
- en: 'where *N* is the number of cases, *K* is the number of classes, ln is the natural
    logarithm, *y*[ik] is an indicator as to whether label *k* is the correct classification
    for case *i*, and *p[ik]* is the proportion of cases belonging to the same class
    as case *i* that were correctly classified. We can read this as follows:'
  id: totrans-1641
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *N* 是案例数量，*K* 是类别数量，ln 是自然对数，*y*[ik] 是一个指示器，表示标签 *k* 是否是案例 *i* 的正确分类，*p[ik]*
    是属于与案例 *i* 相同类别的、被正确分类的案例的比例。我们可以这样读：
- en: 'For every case in the training set:'
  id: totrans-1642
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于训练集中的每个案例：
- en: Take the proportion of cases belonging to the same class as that case that were
    correctly classified.
  id: totrans-1643
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取与该案例属于同一类别的、被正确分类的案例的比例。
- en: Take the natural logarithm of these proportions.
  id: totrans-1644
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 取这些比例的自然对数。
- en: Sum these logs.
  id: totrans-1645
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 求和这些对数。
- en: Multiply by –1 / *N*.
  id: totrans-1646
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 乘以 –1 / *N*。
- en: '|  |'
  id: totrans-1647
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Gradient boosting doesn’t necessarily train sub-models on samples of the training
    set. If we choose to sample the training set, the process is called *stochastic
    gradient boosting* (*stochastic* just means “random,” but it is a good word to
    impress your friends with). Sampling in stochastic gradient descent is usually
    *without replacement*, which means it isn’t a bootstrap sample. We don’t need
    to replace each case during sampling because it’s not important to sample cases
    based on their weights (like in AdaBoost ) and there is little impact on performance.
    Just like for AdaBoost and random forest, it’s a good idea to sample the training
    set, because doing so reduces variance. The proportion of cases we sample from
    the training set can be tuned as a hyperparameter.
  id: totrans-1648
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度提升不一定在训练集的样本上训练子模型。如果我们选择对训练集进行采样，这个过程被称为*随机梯度提升*（*随机*只是意味着“随机”，但这是一个能让你在朋友面前炫耀的好词）。在随机梯度下降中采样通常是*不放回*的，这意味着它不是一个自举样本。我们不需要在采样过程中替换每个案例，因为根据它们的权重（如AdaBoost）进行采样并不重要，并且对性能的影响很小。就像AdaBoost和随机森林一样，对训练集进行采样是个好主意，因为这样做可以减少方差。我们从训练集中采样的案例比例可以作为超参数进行调整。
- en: There are a number of gradient boosting algorithms around, but probably the
    best known is the XGBoost (extreme gradient boosting) algorithm. Published in
    2014, XGBoost is an extremely popular classification and regression algorithm.
    Its popularity is due to how well it performs on a wide range of tasks, as it
    tends to outperform most other supervised learning algorithms. Many Kaggle (an
    online community that runs machine learning competitions) data science competitions
    have been won using XGBoost, and it has become the supervised learning algorithm
    many data scientists try before anything else.
  id: totrans-1649
  prefs: []
  type: TYPE_NORMAL
  zh: 目前有众多梯度提升算法，但可能最广为人知的是XGBoost（极端梯度提升）算法。2014年发布的XGBoost是一种非常流行的分类和回归算法。它的流行归功于它在各种任务上的出色表现，因为它往往能超越大多数其他监督学习算法。许多Kaggle（一个运行机器学习竞赛的在线社区）数据科学竞赛都是使用XGBoost赢得的，它已成为许多数据科学家在尝试其他算法之前首选的监督学习算法。
- en: 'While XGBoost is an implementation of gradient boosting, it has a few tricks
    up its sleeve:'
  id: totrans-1650
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然XGBoost是梯度提升的一种实现，但它还有一些小技巧：
- en: It can build different branches of each tree *in parallel*, speeding up model
    building.
  id: totrans-1651
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以**并行**地构建每棵树的各个分支，从而加快模型构建速度。
- en: It can handle missing data.
  id: totrans-1652
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它可以处理缺失数据。
- en: It employs *regularization*. You’ll learn more about this in [chapter 11](kindle_split_022.html#ch11),
    but it prevents individual predictors from having too large of an impact on predictions
    (this helps to prevent overfitting).
  id: totrans-1653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它采用了**正则化**。你将在第11章中了解更多关于它的内容，但它可以防止单个预测器对预测产生太大的影响（这有助于防止过拟合）。
- en: '|  |'
  id: totrans-1654
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1655
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 小贴士
- en: There are even more recent gradient boosting algorithms available, such as LightGBM
    and CatBoost. These are not currently wrapped by the mlr package, so we’ll stick
    with XGBoost, but feel free to explore them yourself!
  id: totrans-1656
  prefs: []
  type: TYPE_NORMAL
  zh: 还有更多最新的梯度提升算法可用，如LightGBM和CatBoost。这些算法目前尚未被mlr包封装，因此我们将坚持使用XGBoost，但你可以自由探索它们！
- en: '|  |'
  id: totrans-1657
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '8.1.3\. Learning from predictions made by other models: Stacking'
  id: totrans-1658
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.1.3. 从其他模型的预测中学习：堆叠
- en: 'In this section, I’ll explain the principle of the stacking ensemble technique
    and how it is used to combine predictions from multiple algorithms. Stacking is
    an ensemble technique that, while valuable, isn’t as commonly used as bagging
    and boosting. For this reason, I won’t discuss it in a lot of detail, but if you’re
    interested in learning more, I recommend *Ensemble Methods: Foundations and Algorithms*
    by Zhi-Hua Zhou (Chapman and Hall/CRC, 2012).'
  id: totrans-1659
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将解释堆叠集成技术的原理以及它是如何用于结合多个算法的预测的。堆叠是一种集成技术，虽然很有价值，但不如袋装和提升算法常用。因此，我不会过多地讨论它，但如果你对了解更多感兴趣，我推荐周志华的《集成方法：基础与算法》（Chapman
    and Hall/CRC，2012年）。
- en: In bagging and boosting, the learners are often (but don’t always have to be)
    *homogeneous*. Put another way, all of the sub-models were learned by the same
    algorithm (decision trees). Stacking explicitly uses different algorithms to learn
    the sub-models. For example, we may choose to use the kNN algorithm (from [chapter
    3](kindle_split_013.html#ch03)), logistic regression algorithm (from [chapter
    4](kindle_split_014.html#ch04)), and the SVM algorithm (from [chapter 6](kindle_split_016.html#ch06))
    to build three independent *base models*.
  id: totrans-1660
  prefs: []
  type: TYPE_NORMAL
  zh: 在袋装和提升中，学习器通常是（但不必总是）**同质化的**。换句话说，所有子模型都是由相同的算法（决策树）学习的。堆叠明确使用不同的算法来学习子模型。例如，我们可能选择使用kNN算法（来自第3章），逻辑回归算法（来自第4章），以及SVM算法（来自第6章）来构建三个独立的**基础模型**。
- en: 'The idea behind stacking is that we create base models that are good at learning
    different patterns in the feature space. One model may then be good at predicting
    in one area of the feature space but makes mistakes in another area. One of the
    other models may do a good job of predicting values in an area of the feature
    space where the others do poorly. So here’s the key in stacking: the predictions
    made by the base models are used as predictor variables (along with all the original
    predictors) by another model: the *stacked model*. This stacked model is then
    able to learn from the predictions made by the base models to make more accurate
    predictions of its own. Stacking can be tedious and complicated to implement,
    but it usually results in improved model performance if you use base learners
    that are different enough from each other.'
  id: totrans-1661
  prefs: []
  type: TYPE_NORMAL
  zh: Stacking背后的想法是创建擅长学习特征空间中不同模式的基模型。一个模型可能在特征空间的一个区域预测得很好，但在另一个区域犯错误。另一个模型可能在其他模型表现不佳的特征空间区域中很好地预测值。因此，Stacking的关键在于：基模型的预测被用作预测变量（连同所有原始预测变量）由另一个模型使用：*堆叠模型*。这个堆叠模型随后能够从基模型的预测中学习，以做出更准确的预测。Stacking可能既繁琐又复杂，但如果你使用足够不同的基学习器，它通常会导致模型性能的提高。
- en: I hope I’ve conveyed a basic understanding of ensemble techniques, in particular
    the random forest and XGBoost algorithms. In the next section, we’ll use these
    two algorithms to train models on our zoo task and see which performs the best!
  id: totrans-1662
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望我已经传达了对集成技术的基本理解，特别是随机森林和XGBoost算法。在下一节中，我们将使用这两种算法在我们的动物园任务上训练模型，并看看哪种表现最好！
- en: '|  |'
  id: totrans-1663
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-1664
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: Ensemble methods like bagging, boosting, and stacking are not strictly machine
    learning algorithms in their own right. They are algorithms that can be *applied*
    to other machine learning algorithms. For example, I’ve described bagging and
    boosting here as being applied to decision trees. This is because ensembling is
    most commonly applied to tree-based learners; but we could just as easily apply
    bagging and boosting to other machine learning algorithms, such as kNN and linear
    regression.
  id: totrans-1665
  prefs: []
  type: TYPE_NORMAL
  zh: 集成方法，如bagging、boosting和stacking，本身并不是严格的机器学习算法。它们是可以应用于其他机器学习算法的算法。例如，我在这里将bagging和boosting描述为应用于决策树。这是因为集成通常最常应用于基于树的学习者；但我们同样可以将bagging和boosting应用于其他机器学习算法，如kNN和线性回归。
- en: '|  |'
  id: totrans-1666
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 8.2\. Building your first random forest model
  id: totrans-1667
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2\. 构建您的第一个随机森林模型
- en: 'In this section, I’ll show you how to build a random forest model (using bootstrapping
    to train many trees and aggregating their predictions) and how to tune its hyperparameters.
    There are four important hyperparameters for us to consider:'
  id: totrans-1668
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何构建随机森林模型（使用自助法训练多个树并聚合它们的预测）以及如何调整其超参数。以下是我们需要考虑的四个重要超参数：
- en: '***ntree—*** The number of individual trees in the forest'
  id: totrans-1669
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***ntree—*** 森林中单独树的数量'
- en: '***mtry—*** The number of features to randomly sample at each node'
  id: totrans-1670
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***mtry—*** 在每个节点随机采样的特征数量'
- en: '***nodesize—*** The minimum number of cases allowed in a leaf (the same as
    *minbucket* in rpart)'
  id: totrans-1671
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***nodesize—*** 叶子中允许的最小案例数（与rpart中的*minbucket*相同）'
- en: '***maxnodes—*** The maximum number of leaves allowed'
  id: totrans-1672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***maxnodes—*** 允许的最大叶子数'
- en: 'Because we’re aggregating the votes of many trees in random forest, the more
    trees we have, the better. There is no downside to having more trees aside from
    computational cost: at some point, we get diminishing returns. Rather than tuning
    this value, I usually fix it to a number of trees I know fits my computational
    budget, generally several hundred to the low thousands. Later in this section,
    I’ll show you how to tell if you’ve used enough trees, or if you can reduce your
    tree number to speed up training times.'
  id: totrans-1673
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们在随机森林中聚合了许多树的投票，因此树的数量越多，效果越好。除了计算成本之外，没有增加树的数量的缺点：在某个点上，我们得到的是递减的回报。我通常不会调整这个值，而是将其固定为一个我知道适合我的计算预算的数字，通常是几百到低千位数。在本节的后面部分，我将向您展示如何判断您是否已经使用了足够的树，或者是否可以减少树的数量以加快训练时间。
- en: 'The other three hyperparameters—*mtry*, *nodesize*, and *maxnodes*—will need
    tuning, though, so let’s get started. We’ll continue with our `zooTask` that we
    defined in the last chapter (if you no longer have `zooTask` defined in your global
    environment, just rerun [listings 7.1](kindle_split_017.html#ch07ex01), [7.2](kindle_split_017.html#ch07ex02),
    and [7.3](kindle_split_017.html#ch07ex03)). The first thing to do is create a
    learner with the `makeLearner()` function. This time, our learner is `"classif.randomForest"`:'
  id: totrans-1674
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管其他三个超参数—*mtry*、*nodesize*和*maxnodes*—需要调整，但让我们开始吧。我们将继续使用我们在上一章中定义的`zooTask`（如果你在你的全局环境中不再有`zooTask`定义，只需重新运行[列表7.1](kindle_split_017.html#ch07ex01)、[7.2](kindle_split_017.html#ch07ex02)和[7.3](kindle_split_017.html#ch07ex03)）。首先要做的事情是使用`makeLearner()`函数创建一个学习器。这次，我们的学习器是`"classif.randomForest"`：
- en: '[PRE111]'
  id: totrans-1675
  prefs: []
  type: TYPE_PRE
  zh: '[PRE111]'
- en: Next, we’ll create the hyperparameter space we’re going to tune over. To begin
    with, we want to fix the number of trees at 300, so we simply specify `lower =
    300` and `upper = 300` in its `makeIntegerParam()` call. We have 16 predictor
    variables in our dataset, so let’s search for an optimal value of *mtry* between
    6 and 12\. Because some of our groups are very small (probably too small), we’ll
    need to allow our leaves to have a small number of cases in them, so we’ll tune
    *nodesize* between 1 and 5\. Finally, we don’t want to constrain the tree size
    too much, so we’ll search for a *maxnodes* value between 5 and 20.
  id: totrans-1676
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将创建我们将要调整的超参数空间。首先，我们希望将树的数量固定在300，所以我们只需在其`makeIntegerParam()`调用中指定`lower
    = 300`和`upper = 300`。在我们的数据集中有16个预测变量，所以让我们在6到12之间寻找*mtry*的最佳值。因为我们的某些组非常小（可能太小），我们需要允许我们的叶子节点包含少量案例，所以我们将*nodesize*调整在1到5之间。最后，我们不想过多地约束树的大小，所以我们将搜索*maxnodes*值在5到20之间。
- en: Listing 8.1\. Tuning the random forest hyperparameters
  id: totrans-1677
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1\. 调整随机森林超参数
- en: '[PRE112]'
  id: totrans-1678
  prefs: []
  type: TYPE_PRE
  zh: '[PRE112]'
- en: '***1* Creates the hyperparameter tuning space**'
  id: totrans-1679
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建超参数调整空间**'
- en: '***2* Defines a random search method with 100 iterations**'
  id: totrans-1680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定义了一个具有100次迭代的随机搜索方法**'
- en: '***3* Defines a 5-fold cross-validation strategy**'
  id: totrans-1681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 定义了5折交叉验证策略**'
- en: '***4* Tunes the hyperparameters**'
  id: totrans-1682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 调整超参数**'
- en: '***5* Prints the tuning results**'
  id: totrans-1683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 打印调整结果**'
- en: 'Now let’s train a final model by using `setHyperPars()` to make a learner with
    our tuned hyperparameters, and then passing it to the `train()` function:'
  id: totrans-1684
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过使用`setHyperPars()`来创建具有调整超参数的学习器，并将其传递给`train()`函数来训练一个最终模型：
- en: '[PRE113]'
  id: totrans-1685
  prefs: []
  type: TYPE_PRE
  zh: '[PRE113]'
- en: How do we know if we’ve included enough trees in our forest? We can plot the
    mean *out-of-bag* error against the tree number. When building a random forest,
    remember that we take a bootstrap sample of cases for each tree. The out-of-bag
    error is the mean prediction error for each case, by trees that *did not* include
    that case in their bootstrap. Out-of-bag error estimation is specific to algorithms
    that use bagging and allows us to estimate the performance of the forest as it
    grows.
  id: totrans-1686
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何知道我们在森林中是否包含了足够的树？我们可以将平均的*袋外误差*与树的数量进行绘图。在构建随机森林时，请记住我们为每棵树取一个自助样本。袋外误差是每个案例的预测误差的平均值，这些误差是由*没有*包含该案例在其自助样本中的树产生的。袋外误差估计是特定于使用袋装算法的算法，它允许我们估计森林随着其增长的性能。
- en: The first thing we need to do is extract the model information using the `getLearnerModel()`
    function. Then we can simply call `plot()` on this model data object (specifying
    what colors and linetypes to use for each class). Let’s add a legend using the
    `legend()` function so we know what we’re looking at.
  id: totrans-1687
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要做的第一件事是使用`getLearnerModel()`函数提取模型信息。然后我们可以在该模型数据对象上简单地调用`plot()`函数（指定每个类使用的颜色和线型）。让我们使用`legend()`函数添加一个图例，以便我们知道我们在看什么。
- en: Listing 8.2\. Plotting the out-of-bag error
  id: totrans-1688
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.2\. 绘制袋外误差图
- en: '[PRE114]'
  id: totrans-1689
  prefs: []
  type: TYPE_PRE
  zh: '[PRE114]'
- en: The resulting plot is shown in [figure 8.3](#ch08fig03). You won’t be able to
    see the line color in the print version of the book, but you will in the ebook
    or if you reproduce the plot yourself in R. The plot shows the mean out-of-bag
    error for each class (separate lines and a line for the mean) against different
    numbers of trees in the forest. Can you see that once we have at least 100 trees
    in the forest, our error estimates stabilize? This indicates that we have enough
    trees in our forest (and could even use fewer). If you train a model and the mean
    out-of-bag error doesn’t stabilize, you should add more trees!
  id: totrans-1690
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在[图8.3](#ch08fig03)中。你无法在书的打印版本中看到线条颜色，但在电子书或如果你自己在R中重现该图时可以看到。该图显示了每个类别的平均袋外误差（单独的线条和平均值的线条）与森林中树的不同数量之间的关系。你能看到一旦森林中有至少100棵树，我们的误差估计就会稳定吗？这表明我们的森林中有足够的树（甚至可以使用更少的树）。如果你训练的模型中平均袋外误差没有稳定，你应该添加更多的树！
- en: Okay, so we’re happy there are enough trees in our forest. Now let’s properly
    cross-validate our model-building procedure, including hyperparameter tuning.
    We’ll start by defining our outer cross-validation strategy as ordinary 5-fold
    cross-validation.
  id: totrans-1691
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，所以我们很高兴我们的森林里有足够的树木。现在让我们正确地交叉验证我们的模型构建过程，包括超参数调整。我们将首先定义我们的外部交叉验证策略，即普通的5折交叉验证。
- en: Figure 8.3\. Plotting the mean out-of-bag error against tree number. For a given
    forest size during training, the mean out-of-bag error is plotted on the y-axis
    for each class (different lines) and for the overall out of bag (OOB) error. The
    out-of-bag error is the mean prediction error for each case, by trees that *did
    not* include that case in their bootstrap sample. The y-axis shows the mean out-of-bag
    error across all cases.
  id: totrans-1692
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3\. 绘制平均袋外误差与树数量的关系图。在训练过程中，对于给定的森林大小，平均袋外误差在y轴上针对每个类别（不同线条）和整体袋外（OOB）误差进行绘制。袋外误差是每个案例的平均预测误差，由那些*没有*将此案例包含在其自助样本中的树进行预测。y轴显示所有案例的平均袋外误差。
- en: '![](fig8-3_alt.jpg)'
  id: totrans-1693
  prefs: []
  type: TYPE_IMG
  zh: '![](fig8-3_alt.jpg)'
- en: Listing 8.3\. Cross-validating the model-building process
  id: totrans-1694
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3\. 交叉验证模型构建过程
- en: '[PRE115]'
  id: totrans-1695
  prefs: []
  type: TYPE_PRE
  zh: '[PRE115]'
- en: Wow! Look how much better our random forest model performs compared to our original
    decision tree (remind yourself by looking at [listing 7.12](kindle_split_017.html#ch07ex12)
    in the last chapter)! Bagging has greatly improved our classification accuracy.
    Next, let’s see if XGBoost can do even better.
  id: totrans-1696
  prefs: []
  type: TYPE_NORMAL
  zh: 哇！看看我们的随机森林模型与原始决策树相比表现得有多好（通过查看上一章中的[列表7.12](kindle_split_017.html#ch07ex12)来提醒自己）！Bagging大大提高了我们的分类准确率。接下来，让我们看看XGBoost是否能做得更好。
- en: 8.3\. Building your first XGBoost model
  id: totrans-1697
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3\. 构建你的第一个XGBoost模型
- en: 'In this section, I’ll show you how to build an XGBoost model and how to tune
    its hyperparameters. There are eight (!) important hyperparameters for us to consider:'
  id: totrans-1698
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何构建XGBoost模型以及如何调整其超参数。我们有八个(!)重要的超参数需要考虑：
- en: '***eta—*** Known as the *learning rate*. This is a number between 0 and 1,
    which model weights are multiplied by to give their final weight. Setting this
    value below 1 slows down the learning process because it “shrinks” the improvements
    made by each additional model. Preventing the ensemble from learning too quickly
    prevents overfitting. A low value is generally better but will make model training
    take much longer because many model sub-models are needed to achieve good prediction
    accuracy.'
  id: totrans-1699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***eta—*** 也称为*学习率*。这是一个介于0和1之间的数字，模型权重乘以以给出它们的最终权重。将此值设置为1以下会减慢学习过程，因为它“缩小”了每个额外模型所做的改进。防止集成学习得太快可以防止过拟合。通常，低值更好，但会使模型训练时间更长，因为需要许多模型子模型才能达到良好的预测准确率。'
- en: '***gamma—*** The minimum amount of splitting by which a node must improve the
    predictions. Similar to the *cp* value we tuned for rpart.'
  id: totrans-1700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***gamma—*** 节点必须通过的最小分割量来提高预测。类似于我们为rpart调整的*cp*值。'
- en: '***max_depth—*** The maximum levels deep that each tree can grow.'
  id: totrans-1701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***max_depth—*** 每棵树可以生长的最大深度。'
- en: '***min_child_weight—*** The minimum degree of impurity needed in a node before
    attempting to split it (if a node is pure enough, don’t try to split it again).'
  id: totrans-1702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***min_child_weight—*** 在尝试分割节点之前，节点中需要的最小不纯度度数（如果一个节点足够纯净，就不再尝试分割它）。'
- en: '***subsample—*** The proportion of cases to be randomly sampled (without replacement)
    for each tree. Setting this to 1 uses all the cases in the training set.'
  id: totrans-1703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***subsample—*** 每棵树随机采样（不替换）的案例比例。将此设置为1使用训练集中的所有案例。'
- en: '***colsample_bytree—*** The proportion of predictor variables sampled for each
    tree. We could also tune *colsample_bylevel* and *colsample_bynode*, which instead
    sample predictors for each level of depth in a tree and at each node, respectively.'
  id: totrans-1704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***colsample_bytree—*** 每棵树中采样的预测变量的比例。我们也可以调整 *colsample_bylevel* 和 *colsample_bynode*，它们分别在每个树的每个深度级别和每个节点上采样预测变量。'
- en: '***nrounds—*** The number of sequentially built trees in the model.'
  id: totrans-1705
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***nrounds—*** 模型中按顺序构建的树的数目。'
- en: '***eval_metric—*** The type of residual error/loss function we’re going to
    use. For multiclass classification, this will either be the proportion of cases
    that were incorrectly classified (called *merror* by XGBoost) or the log loss
    (called *mlogloss* by XGBoost).'
  id: totrans-1706
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***eval_metric—*** 我们将要使用的残差误差/损失函数的类型。对于多类分类，这将是错误分类的案例比例（由 XGBoost 称为 *merror*）或对数损失（由
    XGBoost 称为 *mlogloss*）。'
- en: 'The first thing to do is create a learner with the `makeLearner()` function.
    This time, our learner is `"classif.xgboost"`:'
  id: totrans-1707
  prefs: []
  type: TYPE_NORMAL
  zh: '首先要做的事情是使用 `makeLearner()` 函数创建一个学习器。这次，我们的学习器是 `"classif.xgboost"`:'
- en: '[PRE116]'
  id: totrans-1708
  prefs: []
  type: TYPE_PRE
  zh: '[PRE116]'
- en: Irritatingly, XGBoost only likes to play with numerical predictor variables.
    Our predictors are currently factors, so we’ll need to mutate them into numerics
    and then define a new task with this mutated tibble. I’ve used the `mutate_at()`
    function to convert all the variables except `type` (by setting `.vars = vars(-type)`)
    into numerics (by setting `.funs = as.numeric)`.
  id: totrans-1709
  prefs: []
  type: TYPE_NORMAL
  zh: 让人烦恼的是，XGBoost 只喜欢与数值预测变量玩耍。我们的预测变量目前是因子，所以我们需要将它们转换为数值，然后定义一个新的任务，使用这个转换后的
    tibble。我使用了 `mutate_at()` 函数将除了 `type` 之外的所有变量（通过设置 `.vars = vars(-type)`）转换为数值（通过设置
    `.funs = as.numeric`）。
- en: Listing 8.4\. Converting factors into numerics
  id: totrans-1710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.4\. 将因子转换为数值
- en: '[PRE117]'
  id: totrans-1711
  prefs: []
  type: TYPE_PRE
  zh: '[PRE117]'
- en: '|  |'
  id: totrans-1712
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1713
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In our example, it doesn’t make a difference that our predictors are all numeric.
    This is because most of our predictors are binary except *legs*, which makes sense
    as a numeric variable. However, if we have a factor with many discrete levels,
    does it make sense to treat it as numeric? In theory, no; but in practice, it
    can work quite well. We simply recode each level of the factor as an arbitrary
    integer and let the decision tree find the best split for us. This is called *numerical
    encoding* (and is what we’ve done to the variables in our dataset). You may have
    heard of another method of encoding categorical features called *one-hot encoding*.
    While I won’t discuss one-hot encoding here, I want to mention that one-hot encoding
    factors for tree-based models often results in *poor performance*.
  id: totrans-1714
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们的预测变量都是数值的这一点并没有什么区别。这是因为我们的大部分预测变量都是二元的，除了 *legs*，它作为一个数值变量是有意义的。然而，如果我们有一个具有许多离散级别的因子，将其视为数值是否有意义？从理论上讲，没有；但在实践中，它可以非常有效。我们只需将因子的每个级别重新编码为一个任意的整数，然后让决策树为我们找到最佳的分割。这被称为
    *数值编码*（这是我们对我们数据集中的变量所做的那样）。你可能听说过另一种编码分类特征的方法，称为 *独热编码*。虽然我不会在这里讨论独热编码，但我想要提到的是，基于树的模型的独热编码因子通常会导致
    *性能不佳*。
- en: '|  |'
  id: totrans-1715
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Now we can define our hyperparameter space for tuning.
  id: totrans-1716
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以定义我们的超参数空间以进行调整。
- en: '|  |'
  id: totrans-1717
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1718
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes about 3 minutes on my four-core machine.
  id: totrans-1719
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上大约需要3分钟。
- en: '|  |'
  id: totrans-1720
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 8.5\. Tuning XGBoost hyperparameters
  id: totrans-1721
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.5\. 调整 XGBoost 超参数
- en: '[PRE118]'
  id: totrans-1722
  prefs: []
  type: TYPE_PRE
  zh: '[PRE118]'
- en: Because more trees are usually better until we stop seeing a benefit, I don’t
    usually tune the *nrounds* hyperparameter but set it based on my computational
    budget to start with (here I’ve set it to 20 by making the `lower` and `upper`
    arguments the same). Once we’ve built the model, we can check if the error flattens
    out after a certain number of trees and decide if we need more or can use fewer
    (just like we did for the random forest model).
  id: totrans-1723
  prefs: []
  type: TYPE_NORMAL
  zh: 因为通常情况下，更多的树会更好，直到我们看不到任何收益为止，所以我通常不会调整 *nrounds* 超参数，而是根据我的计算预算来设置它（在这里，我将其设置为20，通过使
    `lower` 和 `upper` 参数相同）。一旦我们构建了模型，我们就可以检查在构建了一定数量的树之后错误是否趋于平稳，并决定我们是否需要更多的树或者可以使用更少的树（就像我们对随机森林模型所做的那样）。
- en: Once we’ve defined our hyperparameter space, we define our search method as
    a random search with 1,000 iterations. I like to set the number of iterations
    as high as I can, especially as we’re tuning so many hyperparameters simultaneously.
    We define our cross-validation strategy as ordinary 5-fold cross-validation and
    then run the tuning procedure. Because XGBoost will use all of our cores to parallelize
    the building of each tree (take a look at your CPU usage during hyperparameter
    tuning), we won’t parallelize the tuning procedure as well.
  id: totrans-1724
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们定义了超参数空间，我们将搜索方法定义为具有 1,000 次迭代的随机搜索。我喜欢将迭代次数设置得尽可能高，尤其是在我们同时调整许多超参数时。我们将交叉验证策略定义为普通的
    5 折交叉验证，然后运行调整程序。因为 XGBoost 将使用所有核心并行化构建每个树（在超参数调整期间查看您的 CPU 使用情况），所以我们不会并行化调整程序。
- en: Now let’s train our final XGBoost model using our tuned hyperparameters. You
    should be starting to get familiar with this now. We first use `setHyperPars()`
    to make a learner, and then pass it to the `train()` function.
  id: totrans-1725
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用调整好的超参数训练最终的 XGBoost 模型。你现在应该开始熟悉这个过程了。我们首先使用 `setHyperPars()` 创建一个学习器，然后将其传递给
    `train()` 函数。
- en: Listing 8.6\. Training the final tuned model
  id: totrans-1726
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.6\. 训练最终的调整模型
- en: '[PRE119]'
  id: totrans-1727
  prefs: []
  type: TYPE_PRE
  zh: '[PRE119]'
- en: Let’s plot the loss function against the iteration number to get an idea of
    whether we included enough trees.
  id: totrans-1728
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们绘制损失函数与迭代次数的对比图，以了解我们是否包含了足够的树。
- en: Listing 8.7\. Plotting iteration number against log loss
  id: totrans-1729
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.7\. 绘制迭代次数与对数损失的对比图
- en: '[PRE120]'
  id: totrans-1730
  prefs: []
  type: TYPE_PRE
  zh: '[PRE120]'
- en: First, we extract the model data using `getLearnerModel()`. Next, we can extract
    a data frame containing the loss function data for each iteration with the `$evaluation_log`
    component of the model data. This contains the columns `iter` (iteration number)
    and `train_mlogloss` (the log loss for that iteration). We can plot these against
    each other to see if the loss has flattened out (indicating that we have trained
    enough trees).
  id: totrans-1731
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用 `getLearnerModel()` 提取模型数据。接下来，我们可以使用模型数据的 `$evaluation_log` 组件提取包含每个迭代损失函数数据的
    DataFrame。这包含 `iter`（迭代次数）和 `train_mlogloss`（该迭代的对数损失）列。我们可以将它们相互绘制以查看损失是否已经平坦化（这表明我们已经训练了足够的树）。
- en: '|  |'
  id: totrans-1732
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1733
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: My hyperparameter tuning selected log loss as the best loss function. If yours
    selected classification error, you will need to use `$train_merror` here instead
    of `$train_mlogloss`.
  id: totrans-1734
  prefs: []
  type: TYPE_NORMAL
  zh: 我的超参数调整选择了对数损失作为最佳损失函数。如果您的选择了分类错误，您将需要在这里使用 `$train_merror` 而不是 `$train_mlogloss`。
- en: '|  |'
  id: totrans-1735
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The resulting plot from [listing 8.7](#ch08ex07) is shown in [figure 8.4](#ch08fig04).
    Can you see that the log loss flattens out after around 15 iterations? This means
    we’ve trained enough trees and aren’t wasting computational resources by training
    too many.
  id: totrans-1736
  prefs: []
  type: TYPE_NORMAL
  zh: '[列表 8.7](#ch08ex07) 的结果图显示在 [图 8.4](#ch08fig04) 中。你能看到对数损失在大约 15 次迭代后平坦化吗？这意味着我们已经训练了足够的树，并没有通过训练过多的树浪费计算资源。'
- en: It’s also possible to plot the individual trees in the ensemble, which is a
    nice way of interpreting the model-building process (unless you have a huge number
    of trees). For this, we need to install the DiagrammeR package first and then
    pass the model data object as an argument to the XGBoost package function `xgb.plot.tree()`.
    We can also specify which trees to plot with the `trees` argument.
  id: totrans-1737
  prefs: []
  type: TYPE_NORMAL
  zh: 还可以绘制集成中的单个树，这是一种解释模型构建过程的好方法（除非你有大量的树）。为此，我们首先需要安装 DiagrammeR 包，然后将模型数据对象作为参数传递给
    XGBoost 包的函数 `xgb.plot.tree()`。我们还可以使用 `trees` 参数指定要绘制的树。
- en: Figure 8.4\. Plotting log loss against the number of trees during model building.
    The curve flattens out after 15 trees, suggesting there is no benefit to adding
    more trees to the model.
  id: totrans-1738
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.4\. 在模型构建过程中绘制对数损失与树数量的对比图。曲线在 15 棵树之后平坦化，表明增加更多树到模型中没有任何好处。
- en: '![](fig8-4_alt.jpg)'
  id: totrans-1739
  prefs: []
  type: TYPE_IMG
  zh: '![](fig8-4_alt.jpg)'
- en: Listing 8.8\. Plotting individual decision trees
  id: totrans-1740
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.8\. 绘制单个决策树
- en: '[PRE121]'
  id: totrans-1741
  prefs: []
  type: TYPE_PRE
  zh: '[PRE121]'
- en: The resulting graphic is shown in [figure 8.5](#ch08fig05). Notice that the
    trees we’re using are shallow, and some are decision stumps (tree 2 doesn’t even
    have a split).
  id: totrans-1742
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图形显示在 [图 8.5](#ch08fig05) 中。注意我们使用的树都很浅，其中一些是决策树桩（第 2 棵树甚至没有分裂）。
- en: '|  |'
  id: totrans-1743
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Tip
  id: totrans-1744
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 提示
- en: I won’t discuss the information shown in each node in [figure 8.5](#ch08fig05),
    but for a better understanding you can run `?xgboost::xgb.plot.tree`. You can
    also represent the final ensemble as a single tree structure by using `xgboost::xgb
    .plot.multi.trees(xgbModelData)`; this helps you to interpret your model as a
    whole.
  id: totrans-1745
  prefs: []
  type: TYPE_NORMAL
  zh: 我不会讨论[图8.5](#ch08fig05)中每个节点显示的信息，但为了更好地理解，你可以运行`?xgboost::xgb.plot.tree`。你还可以使用`xgboost::xgb
    .plot.multi.trees(xgbModelData)`将最终集成表示为单个树结构；这有助于你整体地解释你的模型。
- en: '|  |'
  id: totrans-1746
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Finally, let’s cross-validate our model-building process exactly as we did for
    our random forest and rpart models.
  id: totrans-1747
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们像对随机森林和rpart模型所做的那样，对我们的模型构建过程进行交叉验证。
- en: '|  |'
  id: totrans-1748
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Warning
  id: totrans-1749
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 警告
- en: This takes nearly 15 minutes on my four-core machine! I strongly suggest you
    do something else during this time.
  id: totrans-1750
  prefs: []
  type: TYPE_NORMAL
  zh: 这在我的四核机器上几乎需要15分钟！我强烈建议你在这段时间内做些其他事情。
- en: '|  |'
  id: totrans-1751
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 8.9\. Plotting individual decision trees
  id: totrans-1752
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.9. 绘制单个决策树
- en: '[PRE122]'
  id: totrans-1753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE122]'
- en: Phenomenal! The cross-validation estimates that our model has an accuracy of
    1 – 0.039 = 0.961 = 96.1%! Go XGBoost!
  id: totrans-1754
  prefs: []
  type: TYPE_NORMAL
  zh: 太棒了！交叉验证估计我们的模型准确率为1 - 0.039 = 0.961 = 96.1%！加油XGBoost！
- en: Figure 8.5\. Plotting individual trees from our XGBoost model
  id: totrans-1755
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5. 从我们的XGBoost模型绘制单个树
- en: '![](fig8-5_alt.jpg)'
  id: totrans-1756
  prefs: []
  type: TYPE_IMG
  zh: '![](fig8-5_alt.jpg)'
- en: 8.4\. Strengths and weaknesses of tree-based algorithms
  id: totrans-1757
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 基于树的算法的优缺点
- en: While it often isn’t easy to tell which algorithms will perform well for a given
    task, here are some strengths and weaknesses that will help you decide whether
    random forest or XGBoost will perform well for you.
  id: totrans-1758
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通常很难判断哪些算法对特定任务表现良好，但以下是一些优势和劣势，将帮助你决定随机森林或XGBoost是否适合你。
- en: 'The strengths of the random forest and XGBoost algorithms are as follows:'
  id: totrans-1759
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林和XGBoost算法的优点如下：
- en: They can handle categorical and continuous predictor variables (though XGBoost
    requires some numerical encoding).
  id: totrans-1760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理分类和连续预测变量（尽管XGBoost需要一些数值编码）。
- en: They make no assumptions about the distribution of the predictor variables.
  id: totrans-1761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们对预测变量的分布没有做出任何假设。
- en: They can handle missing values in sensible ways.
  id: totrans-1762
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以以合理的方式处理缺失值。
- en: They can handle continuous variables on different scales.
  id: totrans-1763
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理不同尺度的连续变量。
- en: Ensemble techniques can drastically improve model performance over individual
    trees. XGBoost in particular is excellent at reducing both bias and variance.
  id: totrans-1764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成技术可以显著提高模型性能，超过单个树。特别是XGBoost在减少偏差和方差方面表现出色。
- en: 'The weaknesses of tree-based algorithms are these:'
  id: totrans-1765
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的算法的缺点如下：
- en: Random forest reduces variance compared to rpart but does not reduce bias (XGBoost
    reduces both).
  id: totrans-1766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 与rpart相比，随机森林降低了方差，但没有降低偏差（XGBoost两者都降低）。
- en: XGBoost can be computationally expensive to tune because it has many hyperparameters
    and grows trees sequentially.
  id: totrans-1767
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: XGBoost的调整可能很昂贵，因为它有很多超参数并且按顺序生长树。
- en: 8.5\. Benchmarking algorithms against each other
  id: totrans-1768
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5. 对比基准算法
- en: In this section, I’ll teach you what *benchmarking* is, and we’ll use it to
    compare the performance of several algorithms on a particular task. The classification
    drawer of your toolbox has lots of algorithms in it now! Experience is a great
    way to choose an algorithm for a particular task. But remember, we are always
    subject to the “no free lunch” theorem. You may find yourself surprised sometimes
    that a simpler algorithm outperforms a more complex one for a particular task.
    A good way of deciding which algorithm will perform best on a particular task
    is to perform a *benchmarking* experiment.
  id: totrans-1769
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将教你们什么是*基准测试*，我们将用它来比较特定任务上几个算法的性能。你的工具箱中的分类器现在有很多算法！经验是选择特定任务算法的好方法。但请记住，我们总是受到“没有免费午餐”定理的约束。有时你可能会惊讶地发现，一个简单的算法在特定任务上比一个更复杂的算法表现更好。决定哪个算法在特定任务上表现最好的好方法是进行*基准测试*实验。
- en: Benchmarking is simple. You create a list of learners you’re interested in trying,
    and let them fight it out to find the one that learns the best-performing model.
    Let’s do this with `xgbTask`.
  id: totrans-1770
  prefs: []
  type: TYPE_NORMAL
  zh: 基准测试很简单。你创建一个你感兴趣尝试的学习者列表，让他们竞争以找到学习最佳性能模型的一个。让我们用`xgbTask`来做这件事。
- en: Listing 8.10\. Plotting individual decision trees
  id: totrans-1771
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.10. 绘制单个决策树
- en: '[PRE123]'
  id: totrans-1772
  prefs: []
  type: TYPE_PRE
  zh: '[PRE123]'
- en: First, we create a list of learner algorithms including k-nearest neighbors
    (`"classif .knn"`), multinomial logistic regression (`"classif.LiblineaRL1LogReg"`),
    support vector machine (`"classif.svm"`), our `tunedTree` model that we trained
    in the previous chapter, and the `tunedForest` and `tunedXgb` models that we trained
    in this chapter. If you no longer have the `tunedTree` model defined in your global
    environment, rerun [listings 7.1](kindle_split_017.html#ch07ex01) through [7.8](kindle_split_017.html#ch07ex08).
  id: totrans-1773
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们创建了一个包含学习算法列表，包括 k 近邻（`"classif .knn"`）、多项式逻辑回归（`"classif.LiblineaRL1LogReg"`）、支持向量机（`"classif.svm"`）、我们在上一章中训练的
    `tunedTree` 模型，以及我们在本章中训练的 `tunedForest` 和 `tunedXgb` 模型。如果你在你的全局环境中不再有定义的 `tunedTree`
    模型，请重新运行 [7.1](kindle_split_017.html#ch07ex01) 到 [7.8](kindle_split_017.html#ch07ex08)
    的代码。
- en: '|  |'
  id: totrans-1774
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-1775
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 备注
- en: This isn’t quite a fair comparison, because the first three learners will be
    trained using default hyperparameters, whereas the tree-based models have been
    tuned.
  id: totrans-1776
  prefs: []
  type: TYPE_NORMAL
  zh: 这并不是一个完全公平的比较，因为前三个学习者将使用默认的超参数进行训练，而基于树的模型已经进行了调整。
- en: '|  |'
  id: totrans-1777
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'We define our cross-validation method using `makeResampleDesc()`. This time,
    I’ve opted for 10-fold cross-validation repeated 5 times. It’s important to note
    that mlr is clever here: while the data is partitioned randomly into folds for
    each repeat, *the same partitioning* is used for every learner. Put more plainly,
    for each cross-validation repeat, each learner in the benchmark gets exactly the
    same training set and test set.'
  id: totrans-1778
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用 `makeResampleDesc()` 定义我们的交叉验证方法。这次，我选择了重复 5 次的 10 折交叉验证。重要的是要注意，mlr 在这里很聪明：虽然数据在每次重复时随机分成几部分，但
    *相同的划分* 被用于每个学习者。更简单地说，对于每次交叉验证重复，基准中的每个学习者在每个交叉验证重复中都得到完全相同的训练集和测试集。
- en: Finally, we use the `benchmark()` function to run the benchmark experiment.
    The first argument is the list of learners, the second argument is the name of
    the task, and the third argument is the cross-validation method.
  id: totrans-1779
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们使用 `benchmark()` 函数运行基准实验。第一个参数是学习者的列表，第二个参数是任务的名称，第三个参数是交叉验证方法。
- en: What did I tell you about no free lunches? The humble k-nearest neighbors is
    performing better on this task than the mighty XGBoost algorithm—even though we
    didn’t tune it!
  id: totrans-1780
  prefs: []
  type: TYPE_NORMAL
  zh: 我之前告诉过你关于免费午餐的事情吗？谦逊的 k 近邻算法在这个任务上的表现比强大的 XGBoost 算法还要好——即使我们没有对其进行调整！
- en: Summary
  id: totrans-1781
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: The random forest and XGBoost algorithms are supervised learners for both classification
    and regression problems.
  id: totrans-1782
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机森林和 XGBoost 算法是用于分类和回归问题的监督学习器。
- en: Ensemble techniques construct multiple sub-models to result in a model that
    performs better than any one of its components alone.
  id: totrans-1783
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 集成技术构建多个子模型，以产生一个比其单个组件单独表现更好的模型。
- en: Bagging is an ensemble technique that trains multiple sub-models in parallel
    on bootstrap samples of the training set. Each sub-model then votes on the prediction
    for new cases. Random forest is an example of a bagging algorithm.
  id: totrans-1784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Bagging 是一种集成技术，它并行地在训练集的 bootstrap 样本上训练多个子模型。然后每个子模型对新案例的预测进行投票。随机森林是 bagging
    算法的一个例子。
- en: Boosting is an ensemble technique that trains multiple sub-models sequentially,
    where each subsequent sub-model focuses on the mistakes of the previous set of
    sub-models. AdaBoost and XGBoost are examples of boosting algorithms.
  id: totrans-1785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Boosting 是一种集成技术，它按顺序训练多个子模型，其中每个后续子模型专注于前一组子模型的错误。AdaBoost 和 XGBoost 是 boosting
    算法的例子。
- en: Benchmarking allows us to compare the performance of multiple algorithms/ models
    on a single task.
  id: totrans-1786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基准测试使我们能够比较多个算法/模型在单个任务上的性能。
