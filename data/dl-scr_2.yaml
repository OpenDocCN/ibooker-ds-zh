- en: Part 3\. One step beyond
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3部分\. 超越一步
- en: 'In [part 1](kindle_split_011.xhtml#part01) of this book, you got a basic understanding
    of what search engines and deep neural networks are, how they work, and how they
    can work together to create smarter search engines. [Part 2](kindle_split_014.xhtml#part02)
    dove into the technical details of major deep neural network applications for
    search engines, mostly using recurrent neural networks and word/document embeddings
    to give users more relevant results. In this part of the book, we’ll tackle more-advanced
    topics and challenges by extending the applications of neural networks to two
    new areas: searching text in multiple languages using machine translation ([chapter
    7](kindle_split_020.xhtml#ch07)), and searching for images using convolutional
    neural networks ([chapter 8](kindle_split_021.xhtml#ch08)). Finally, in [chapter
    9](kindle_split_022.xhtml#ch09), we’ll look at the thing that makes the biggest
    difference in production scenarios: performance, whether plain speed when training
    and predicting, or accuracy of results. You’ll see an example of how to tune a
    neural network model to reach good accuracy in a reasonable training time. In
    addition, we’ll look at how to deal with continuous streams of data for neural
    search.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的[第1部分](kindle_split_011.xhtml#part01)中，您对搜索引擎和深度神经网络有了基本的了解，包括它们是如何工作的，以及它们如何协同工作以创建更智能的搜索引擎。[第2部分](kindle_split_014.xhtml#part02)深入探讨了搜索引擎中主要深度神经网络应用的技术细节，主要使用循环神经网络和词/文档嵌入来为用户提供更相关的结果。在本部分书中，我们将通过扩展神经网络的应用到两个新的领域来处理更高级的主题和挑战：使用机器翻译在多种语言中搜索文本([第7章](kindle_split_020.xhtml#ch07))，以及使用卷积神经网络搜索图像([第8章](kindle_split_021.xhtml#ch08))。最后，在第[第9章](kindle_split_022.xhtml#ch09)中，我们将探讨在生产场景中影响最大的因素：性能，无论是训练和预测时的纯速度，还是结果的准确性。您将看到如何调整神经网络模型以在合理的训练时间内达到良好的准确性。此外，我们还将探讨如何处理神经搜索的连续数据流。
- en: Chapter 7\. Searching across languages
  id: totrans-2
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章\. 跨语言搜索
- en: '*This chapter covers*'
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Cross-language information retrieval
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 跨语言信息检索
- en: Statistical machine translation
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计机器翻译
- en: Seq2seq models for machine translation
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译的seq2seq模型
- en: Word embeddings for machine translation
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器翻译中的词嵌入
- en: Comparing the effectiveness of machine translation methods for search
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 比较搜索中机器翻译方法的有效性
- en: In this chapter, we’ll focus on expanding your ability to serve users who speak,
    read, and write queries in languages other than the language in which documents
    are written. Specifically, you’ll see how to use machine translation to build
    a search engine that can automatically translate queries so those queries can
    be used to search and deliver content from multiple languages. We’ll spend some
    time looking at how this translation ability can be useful in various contexts,
    from common web searches to more specific cases where it’s important not to miss
    search results due to a language barrier. The benefit of being able to automatically
    translate queries is that your search engines gain the ability to reach more users,
    without requiring you to store multiple copies of each text document in different
    languages.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将专注于扩展您为使用除文档编写语言以外的语言进行查询、阅读和编写查询的用户提供服务的能力。具体来说，您将了解如何使用机器翻译来构建一个能够自动翻译查询的搜索引擎，以便这些查询可以用于搜索和提供来自多种语言的内容。我们将花一些时间探讨这种翻译能力在不同情境中的用途，从常见的网络搜索到更具体的情况，在这些情况下，由于语言障碍而错过搜索结果是很重要的。能够自动翻译查询的好处是，您的搜索引擎能够接触到更多用户，而无需您存储每种文本文档的不同语言的多个副本。
- en: 7.1\. Serving users who speak multiple languages
  id: totrans-10
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 服务多语言用户
- en: Many of the scenarios presented in earlier chapters focused on vertical search
    engines, or search engines that are specific to an often small, well-defined domain,
    such as a search engine for movie reviews. In this chapter, which explores the
    challenge of retrieving useful information for users speaking different languages,
    there’s no better fit than web search, or searching over data from everywhere
    on the World Wide Web. We use web search on an everyday basis with search engines
    like Google Search, Bing, and Baidu. Although a lot of online content is written
    in languages spoken by a huge number of people (like English), there are still
    many users who need to retrieve information and hope to find that information
    by using their native language.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在前几章中提出的许多场景都集中在垂直搜索引擎上，或者说是针对一个通常较小、定义明确的领域的搜索引擎，例如电影评论的搜索引擎。在本章中，我们探讨了为说不同语言的用户检索有用信息所带来的挑战，没有比网络搜索，或者说在万维网上的所有数据上搜索更好的选择了。我们每天都会使用谷歌搜索、必应和Baidu等搜索引擎进行网络搜索。尽管大量在线内容是用被大量人群使用的语言（如英语）编写的，但仍有许多用户需要检索信息，并希望用他们的母语找到这些信息。
- en: You may wonder what the point of this discussion is. If you have a Wikipedia
    page written in Italian, it will surely be indexed by, for example, Google Search,
    and you’ll be able to search for it by writing a query on Google Search in Italian,
    as in [figure 7.1](#ch07fig01).
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会想知道这次讨论的目的是什么。如果你有一个用意大利语编写的维基百科页面，它肯定会被例如谷歌搜索这样的搜索引擎索引，你可以在谷歌搜索中用意大利语输入查询来搜索它，就像[图7.1](#ch07fig01)中所示。
- en: Figure 7.1\. Searching for “rete neurale,” Italian for “neural network”
  id: totrans-13
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1\. 搜索“rete neurale”，这是意大利语中的“神经网络”
- en: '![](Images/07fig01_alt.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/07fig01_alt.jpg)'
- en: Realistically, though, when searching, especially for tech-related topics, it’s
    often expedient to write queries in English. This is because the amount of information
    available in English, especially for tech topics, often far outweighs the amount
    written in other languages. A user whose first language is Italian (or Danish,
    or Chinese, and so on) writes a query in English to maximize the chance of getting
    as many relevant results as possible. Those results will then include only documents
    written in English. And the fact is that search results written in English aren’t
    always as helpful to users as results written in their native language. Let me
    explain by showing what you can do for a query written in English from a user
    whose native language is Italian. As you can see in [figure 7.2](#ch07fig02),
    the query written in English also returned a search result in Italian, shown on
    the right. In cases like this, when a query is performed by a logged-in user,
    the search engine can look up the user’s native language and include results in
    that language in addition to results that match the original query (in this case,
    in English).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，在搜索时，尤其是对于技术相关的话题，通常更方便用英语编写查询。这是因为英语中可用的信息量，尤其是技术话题的信息量，通常远远超过其他语言中编写的数量。一个母语为意大利语（或丹麦语、中文等等）的用户用英语编写查询，以最大限度地提高获得尽可能多的相关结果的机会。这些结果将只包括用英语编写的文档。事实上，用英语编写的搜索结果并不总是像用用户的母语编写的搜索结果那样对用户有帮助。让我通过展示一个意大利语母语的用户的英语查询可以做什么来解释这一点。正如你在[图7.2](#ch07fig02)中可以看到的，用英语编写的查询也返回了意大利语搜索结果，显示在右侧。在这种情况下，当登录用户执行查询时，搜索引擎可以查找用户的母语，并包括该语言的结果，以及与原始查询匹配的结果（在这种情况下，英语）。
- en: Figure 7.2\. Searching for “artificial neural network” and getting results in
    Italian as well as English
  id: totrans-16
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 搜索“人工神经网络”并得到意大利语和英语的结果
- en: '![](Images/07fig02_alt.jpg)'
  id: totrans-17
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/07fig02_alt.jpg)'
- en: How is this helpful for the user? Think about reading your favorite book in
    your native language, as opposed to reading it in a language that you studied
    at school. Even though you may be able to understand the content of the foreign-language
    version of the book, it may take you extra time and effort, and you may miss some
    subtle or especially difficult parts. The same applies to documents on the web.
    The Wikipedia entry for “artificial neural network,” for example, exists in many
    different languages, making it more easily understood by more users. Imagine a
    search engine that not only shows the English entry (which matches a query written
    in English), but also highlights the entry written in the native language of the
    user who entered the query. This search engine better serves the needs of more
    users.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这对用户有什么帮助？想想用你的母语阅读你最喜欢的书，而不是用你在学校学习的语言阅读。即使你可能能够理解书的非母语版本的内容，这也可能需要你额外的时间和精力，你可能会错过一些微妙或特别困难的段落。同样，这也适用于网络上的文档。例如，“人工神经网络”的维基百科条目就有许多不同的语言版本，这使得它更容易被更多用户理解。想象一下，一个搜索引擎不仅显示与用英语编写的查询相匹配的英语条目，而且还突出显示用查询用户的母语编写的条目。这个搜索引擎更好地满足了更多用户的需求。
- en: You can equip your search engine to return both types of results by incorporating
    *machine translation* (MT) tools into your search engine. With machine translation,
    a program can translate a sentence from an input language into the corresponding
    version in a target language. In the rest of this chapter, you’ll see how to use
    MT tools to perform text translation at query time, resulting in improved recall
    and precision for search engine queries across multiple languages.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过将机器翻译（MT）工具集成到搜索引擎中来配置搜索引擎以返回这两种类型的结果。在机器翻译中，程序可以将输入语言中的句子翻译成目标语言的对应版本。在本章的其余部分，你将了解如何使用MT工具在查询时进行文本翻译，从而提高跨多种语言的搜索引擎查询的召回率和精确率。
- en: 7.1.1\. Translating documents vs. queries
  id: totrans-20
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 翻译文档与查询
- en: Imagine having to build a search engine with capabilities similar to the ones
    briefly outlined in the previous section, for a nonprofit entity that supports
    refugees around the world with administrative and legal services. A search engine
    for such an organization would help refugees find appropriate documentation, for
    example, to fill out asylum requests. Each and every country around the world
    probably requires different documents and forms to be completed and signed; requirements
    may also vary depending on the country the applicant comes from. Users of such
    a platform may speak their native language but not the language of their host
    country. So if refugees from Iceland are seeking asylum in Brazil, they’ll need
    to retrieve documents that may be written in Portuguese. If users don’t know Portuguese,
    how can they know what to include in the search query for the information they’re
    seeking?
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你需要为支持全球难民的行政和法律服务的非营利组织构建一个具有类似之前部分简要概述的功能的搜索引擎。这样的组织的搜索引擎可以帮助难民找到适当的文件，例如填写庇护申请。世界上每个国家可能都需要填写和签署不同的文件和表格；要求也可能根据申请人的国籍而有所不同。这样的平台用户可能说他们的母语，但不说他们所在国家的语言。所以，如果冰岛难民进巴西寻求庇护，他们需要检索可能是用葡萄牙语编写的文件。如果用户不知道葡萄牙语，他们如何知道在搜索查询中包含什么信息？
- en: 'Regardless of the situation, you can assume that users want to be able to retrieve
    content in their mother tongue whenever possible. There are two straightforward
    ways to do this using MT:'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 无论情况如何，你可以假设用户希望在可能的情况下能够用他们的母语检索内容。使用MT有两种简单的方法可以实现这一点：
- en: Use MT programs to translate queries in order to find matches in more than one
    language.
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用MT程序翻译查询，以便在多种语言中找到匹配项。
- en: Have content created in one language, and use MT programs to create translated
    copies of the documents so queries can match the translated versions.
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果内容是用一种语言创建的，可以使用MT程序创建文档的翻译副本，以便查询可以匹配翻译版本。
- en: 'These options aren’t mutually exclusive: you can have one or the other or both.
    What fits best depends on the use case.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 这些选项不是互斥的：你可以选择其中之一、两者之一或两者都选。最适合的选择取决于使用场景。
- en: Consider customer reviews on sites like Amazon and Airbnb. Such reviews are
    often written in the reviewer’s native language, so for the purpose of easy consumption
    of search results, it may be good to translate those reviews when they reach the
    user.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑像亚马逊和Airbnb这样的网站上的客户评价。这样的评价通常是用评论者的母语写的，因此为了便于用户消费搜索结果，当这些评价到达用户时，翻译这些评价可能是个好主意。
- en: 'Another good case for translating search results is question-answering systems.
    Answering questions uses an information retrieval system where the user specifies
    their intent in the form of a question written in natural language (such as “Who
    was elected president of U.S.A. in 2009?”). The system replies with an answer:
    a piece of (hopefully informative) text related to the question (such as “Barack
    Obama”).'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 将搜索结果翻译成另一种语言的一个很好的例子是问答系统。回答问题使用的是信息检索系统，用户通过自然语言（如“2009年谁被选为美国总统？”）的形式指定他们的意图。系统会回复一个答案：与问题相关的（希望是有用的）文本片段（如“巴拉克·奥巴马”）。
- en: 'On the other hand, for web search, as discussed in the previous section, it
    may be good to translate the query to get results in different languages, because
    doing so allows more choices for end users. Once that’s done, you need to make
    an important decision about ranking: how do you rank results that come from the
    translated query?'
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，对于网络搜索，如前所述，将查询翻译成不同语言的结果可能是个好主意，因为这样做可以为最终用户提供更多选择。一旦这样做，你需要做出一个重要的决定：如何对来自翻译查询的结果进行排名？
- en: In the case of a refugee searching in Icelandic for documents written in Portuguese,
    if the user searches for “pólitísk hæli” (the Icelandic version of “politic asylum”),
    the query is translated into Portuguese (“asilo politico”). In such use cases,
    results from both the original and translated queries are retrieved. For the specific
    use case of a user who’s an asylum seeker, the documents returned from the translated
    query are more important, because they’re the ones the user will need to fill
    out and submit to the local authorities.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 在一个寻求在冰岛用葡萄牙语写的文件的难民进行搜索的情况下，如果用户搜索“pólitísk hæli”（“政治避难”的冰岛语版本），查询会被翻译成葡萄牙语（“asilo
    politico”）。在这种情况下，会检索来自原始和翻译查询的结果。对于用户是寻求庇护者的特定用例，从翻译查询返回的文档更重要，因为它们是用户需要填写并提交给当地当局的文档。
- en: In web search, that may not be always the case. Let’s get back to the example
    of the Wikipedia page for “artificial neural networks.” The English version of
    the page has much more information than the Italian version. Depending on various
    factors, such as the user’s interests and preferred topics, the search engine
    may decide to rank the translated page lower than the original, because it’s less
    informative. If a deep learning researcher performs a web search for “artificial
    neural networks,” the Italian version of the “artificial neural network” page
    won’t be useful to them, because the amount of information is less, compared to
    the original English page. If, instead, the user is a newbie on the topic, reading
    a page in their native language will probably help them grasp the topic. Although
    a lot depends on the use case, if you decide to use MT in a search engine, it’s
    a good idea to rank the additional results the same as or higher than the “normal”
    results.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络搜索中，情况可能并非总是如此。让我们回到“人工神经网络”的维基百科页面这个例子。该页面的英文版本比意大利文版本包含更多信息。根据各种因素，如用户的兴趣和偏好主题，搜索引擎可能会决定将翻译后的页面排名低于原始页面，因为它的信息量较少。如果一个深度学习研究人员进行“人工神经网络”的网页搜索，那么“人工神经网络”的意大利文版本对他们来说可能没有用，因为信息量比原始英文页面少。相反，如果用户是该主题的新手，阅读用母语写的页面可能有助于他们掌握该主题。尽管使用案例有很大差异，但如果决定在搜索引擎中使用机器翻译，将额外结果排名与“正常”结果相同或更高是个好主意。
- en: The rest of this chapter focuses on translating queries rather than translating
    documents; the principles are similar whether translating short or long pieces
    of text. On the other hand, from a technical perspective, working with very short
    text (such as a search query) or very long text (such as a long article) is usually
    more difficult than working with single sentences.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分主要关注翻译查询而不是翻译文档；无论翻译的是短文本还是长文本，原则都是相似的。另一方面，从技术角度来看，处理非常短的文本（如搜索查询）或非常长的文本（如长篇文章）通常比处理单个句子更困难。
- en: 7.1.2\. Cross-language search
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2. 跨语言搜索
- en: Let’s take a quick look at how to incorporate MT into a search engine to translate
    user queries. In web search, the MT task is usually performed in the search engine;
    nothing is said to the user about it. For the other use cases mentioned, users
    may want to specify the desired language for the search results; an asylum seeker
    will know the best language for the legal documents they need, but this information
    may be not available to the search system.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们快速看一下如何将机器翻译集成到搜索引擎中，以翻译用户查询。在网页搜索中，机器翻译任务通常在搜索引擎中执行；对此没有对用户进行说明。对于其他提到的用例，用户可能希望指定搜索结果的期望语言；寻求庇护者将知道他们需要的法律文件的最佳语言，但这个信息可能对搜索系统不可用。
- en: Going forward, I’ll assume you have a set of MT tools that can translate from
    the language of the user query to other languages and that your search engine
    contains documents in many different languages—a common setup for cross-language
    information retrieval for web search. The tools for performing MT can be implemented
    in many different ways; as we go continue through the chapter, you’ll see a few
    different methods of MT. It’s common for such tools to be able to translate text
    from a *source* language to a *target* language. Imagine you have a query written
    in Icelandic, as mentioned earlier, and you have three models that can translate
    from Icelandic to English, from English to Icelandic, and from Italian to English,
    respectively. The search engine needs to be able to choose the right tool for
    translating the query. If you pick the Italian-to-English tool, then no translation
    or, even worse, a bad translation may come from the model. This may cause retrieval
    of unwanted results, which of course is bad. Even when the inappropriate model
    gives no translation, CPU and memory resources are used, and therefore the attempt
    may negatively impact performance without giving a useful outcome.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的内容中，我将假设您拥有一套可以将用户查询语言翻译成其他语言的机器翻译工具，并且您的搜索引擎包含多种不同语言的文档——这是网络搜索中跨语言信息检索的常见设置。执行机器翻译的工具可以以多种方式实现；随着我们继续本章的学习，您将看到几种不同的机器翻译方法。这样的工具通常能够将文本从*源语言*翻译成*目标语言*。想象一下，您有一个用冰岛语写的查询，如前所述，您有三个模型可以分别将冰岛语翻译成英语、将英语翻译成冰岛语以及将意大利语翻译成英语。搜索引擎需要能够选择正确的工具来翻译查询。如果您选择了意大利语到英语的工具，那么模型可能不会进行翻译，甚至更糟，可能会产生错误的翻译。这可能会导致检索到不希望的结果，这当然是不好的。即使不合适的模型没有进行翻译，CPU和内存资源也会被使用，因此这种尝试可能会对性能产生负面影响，而不会产生有用的结果。
- en: To mitigate such issues, it’s a good practice to place a *language detector*
    program on top of MT models. A language detector receives an input text and outputs
    the language of the input sequence. You can think of it as a text classifier whose
    output classes are language codes (`en`, `it`, `ic`, `pt`, and so on). With the
    language detector providing the user’s query language, you can choose the right
    MT model to translate the query. The output text from all of the MT models will
    be sent to the search engine as an additional query together with the original
    query; you can think of it as using a Boolean OR operator between the original
    and translated versions of the query (such as “pólitísk hæli OR political asylum”).
    [Figure 7.3](#ch07fig03) shows an example flow for using MT at query time.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 为了减轻这些问题，将*语言检测器*程序放在机器翻译模型之上是一个好的做法。语言检测器接收输入文本并输出输入序列的语言。您可以将它视为一个文本分类器，其输出类别是语言代码（`en`、`it`、`ic`、`pt`等等）。有了语言检测器提供用户查询语言，您可以选择正确的机器翻译模型来翻译查询。所有机器翻译模型的输出文本将作为附加查询与原始查询一起发送到搜索引擎；您可以将它视为在查询的原始版本和翻译版本之间使用布尔或运算符（例如，“pólitísk
    hæli OR political asylum”）。[图7.3](#ch07fig03)展示了在查询时使用机器翻译的一个示例流程。
- en: Figure 7.3\. Query translation flow
  id: totrans-36
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3\. 查询翻译流程
- en: '![](Images/07fig03_alt.jpg)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![图片7.3](Images/07fig03_alt.jpg)'
- en: Let’s look at how cross-language search can be implemented on top of Apache
    Lucene. For now, we’ll keep the MT part a bit abstract. In the following sections,
    we’ll go over different types of MT models and examine the advantages and weaknesses
    of each. In particular, we’ll focus on why most research and industries have switched
    from *statistical machine translation* (based on statistical analysis of probability
    distributions for words and phrases) to *neural machine translation* based on
    the use of neural networks.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看如何在Apache Lucene之上实现跨语言搜索。目前，我们将机器翻译部分保持抽象。在接下来的章节中，我们将介绍不同类型的机器翻译模型，并探讨每种模型的优缺点。特别是，我们将关注为什么大多数研究和行业已经从基于词和短语概率分布统计分析的**统计机器翻译**（statistical
    machine translation）转向基于神经网络使用的**神经机器翻译**（neural machine translation）。
- en: 7.1.3\. Querying in multiple languages on top of Lucene
  id: totrans-39
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. 在Lucene之上进行多语言查询
- en: 'Let’s continue with the example of asylum seekers. Suppose I’m an Italian refugee
    in the United States, and I need to fill out some legal documents. I type a query
    in Italian, looking for documents to enter the United States. Here’s what the
    search engine should do:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续以寻求庇护者为例。假设我是一个在美国的意大利难民，需要填写一些法律文件。我用意大利语输入查询，寻找进入美国的文件。以下是搜索引擎应该执行的操作：
- en: '[PRE0]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* Input query**'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入查询**'
- en: '***2* Language detection output**'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 语言检测输出**'
- en: '***3* Translated query**'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 翻译后的查询**'
- en: '***4* Enhanced query containing both the original and translated queries separated
    by a Boolean OR clause**'
  id: totrans-45
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 包含原始和翻译查询的增强查询，两者之间用布尔OR子句分隔**'
- en: 'As you may guess, the “magic” happens during the parsing of the user-entered
    query. Here’s a simplified sequence of operations performed by the query parser:'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所猜，"魔法"发生在用户输入查询的解析过程中。以下是查询解析器执行的一系列简化操作：
- en: '**1**.  The query parser reads the input query.'
  id: totrans-47
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 查询解析器读取输入查询。'
- en: '**2**.  The query parser passes the input query to the language detector.'
  id: totrans-48
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 查询解析器将输入查询传递给语言检测器。'
- en: '**3**.  The language detector determines the language of the input query.'
  id: totrans-49
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**. 语言检测器确定输入查询的语言。'
- en: '**4**.  The query parser chooses MT models that can translate the identified
    language into other languages.'
  id: totrans-50
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**4**. 查询解析器选择可以将识别出的语言翻译成其他语言的机器翻译模型。'
- en: '**5**.  Each selected MT model translates the input query into another language.'
  id: totrans-51
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**5**. 每个选定的机器翻译模型将输入查询翻译成另一种语言。'
- en: '**6**.  The query parser aggregates the input and the translated text in OR
    clauses of a Boolean query.'
  id: totrans-52
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**6**. 查询解析器将输入和翻译后的文本聚合到布尔查询的OR子句中。'
- en: You’ll extend a Lucene `QueryParser` whose main method `#parse` transforms a
    `String` into a Lucene `Query` object.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 你将扩展一个Lucene `QueryParser`，其主方法`#parse`将一个`String`转换成一个Lucene `Query`对象。
- en: Listing 7.1\. Creating a `BooleanQuery` containing the original query
  id: totrans-54
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. 创建包含原始查询的`BooleanQuery`
- en: '[PRE1]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* Creates a Boolean query in Lucene**'
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在Lucene中创建布尔查询**'
- en: '***2* Parses the original user query and adds it to the Boolean query as an
    OR clause**'
  id: totrans-57
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 解析原始用户查询并将其作为OR子句添加到布尔查询中**'
- en: Then the input query language is extracted by a language detector tool. (There
    are many different ways that can be done; for now, we won’t focus on that.) You’ll
    use the `LanguageDetector` tool from the Apache OpenNLP project ([http://opennlp.apache.org](http://opennlp.apache.org)).
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 然后通过语言检测工具提取输入查询的语言。目前有众多不同的方法可以实现；我们暂时不会关注这一点。你将使用Apache OpenNLP项目中的`LanguageDetector`工具([http://opennlp.apache.org](http://opennlp.apache.org))。
- en: Listing 7.2\. Detecting the language of the query
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.2\. 检测查询的语言
- en: '[PRE2]'
  id: totrans-60
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '***1* Performs language detection**'
  id: totrans-61
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 执行语言检测**'
- en: '***2* Gets the language code (en, it, and so on)**'
  id: totrans-62
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 获取语言代码（en，it等）**'
- en: Here you assume you’ve already loaded the models to perform machine translation,
    such as in a `Map` whose key is the language code (`en` for English, `it` for
    Italian, and so on) and whose value is a `Collection` of `TranslatorTool`s. For
    the moment, it doesn’t matter how `TranslatorTool` is implemented; we’ll focus
    on that in later sections.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，你假设你已经加载了用于执行机器翻译的模型，例如在一个`Map`中，其键是语言代码（如英语的`en`，意大利语的`it`等），其值是一个`TranslatorTool`的`Collection`。目前，`TranslatorTool`的实现方式并不重要；我们将在后面的章节中关注这一点。
- en: Listing 7.3\. Choosing the correct `TranslatorTool`s
  id: totrans-64
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3\. 选择正确的`TranslatorTool`s
- en: '[PRE3]'
  id: totrans-65
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '***1* Gets the tools that can translate from the detected language into other
    languages**'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取可以将检测到的语言翻译成其他语言的工具**'
- en: Now that you have the MT tools loaded, you can use them to create additional
    Boolean clauses to be added to the final query.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经加载了MT工具，你可以使用它们来创建额外的布尔子句，并将其添加到最终的查询中。
- en: Listing 7.4\. Translating a query and building a query with the translated text
  id: totrans-68
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.4\. 翻译查询并构建包含翻译文本的查询
- en: '[PRE4]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '***1* Translates the input query**'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将输入查询翻译成目标语言**'
- en: '***2* Iterates over all possible translations of the input query**'
  id: totrans-71
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历输入查询的所有可能的翻译**'
- en: '***3* Gets the translation text (each translation consists of the text and
    its score, representing the quality of the translation)**'
  id: totrans-72
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取翻译文本（每个翻译都由文本及其分数组成，表示翻译的质量）**'
- en: '***4* Parses the translated query and adds it to the Boolean query to be returned**'
  id: totrans-73
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 解析翻译后的查询并将其添加到要返回的布尔查询中**'
- en: '***5* Finalizes the process of building the Boolean query**'
  id: totrans-74
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 完成构建布尔查询的过程**'
- en: With this code, you’re all set with a query parser that lets you create queries
    in multiple languages. The missing part is implementing the `TranslatorTool` interface
    in the best possible way. To do that, we’ll take a quick journey into different
    ways of addressing the MT task. First we’ll look at a statistical MT tool, and
    then we’ll move to methods based on neural networks; this will help you understand
    the main challenges of translating text and how using neural network–based models
    generally provides better MT models.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 使用此代码，你已经准备好了一个查询解析器，它允许你用多种语言创建查询。缺失的部分是以最佳方式实现`TranslatorTool`接口。为了做到这一点，我们将简要探讨处理机器翻译任务的不同方法。首先，我们将查看统计机器翻译工具，然后转向基于神经网络的模型；这将帮助你了解翻译文本的主要挑战，以及使用基于神经网络的模型通常如何提供更好的机器翻译模型。
- en: 7.2\. Statistical machine translation
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 统计机器翻译
- en: '*Statistical machine translation* (SMT) uses statistical approaches to predict
    what target word or sentence is the most probable translation of an input word
    or sentence. For example, an SMT program should be able to answer the question,
    “What’s the most probable English translation of the word ‘hombre’?” To do that,
    you train a statistical model over a parallel corpus. A *parallel corpus* is a
    collection of text fragments (documents, sentences, or even words) where each
    piece of content comes in two versions: the source language (such as Spanish)
    and the target language (such as English). Here’s an example:'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计机器翻译* (SMT) 使用统计方法来预测目标词或句子是输入词或句子的最可能的翻译。例如，SMT程序应该能够回答“单词‘hombre’最可能的英语翻译是什么？”这个问题。为了做到这一点，你需要在平行语料库上训练一个统计模型。*平行语料库*是一组文本片段（文档、句子或甚至单词）的集合，其中每个内容都有两个版本：源语言（如西班牙语）和目标语言（如英语）。以下是一个例子：'
- en: '[PRE5]'
  id: totrans-78
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'A *statistical model* is a model that can calculate the probability of source
    and target text fragments. A correctly trained statistical model for MT will answer
    the question about the most probable translation for a text fragment by providing
    the translation together with its probability:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: '*统计模型*是一种可以计算源文本和目标文本片段概率的模型。一个正确训练的统计机器翻译模型将通过对文本片段最可能的翻译提供翻译及其概率来回答关于文本片段最可能的翻译的问题：'
- en: '[PRE6]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'The probability of a translated text fragment will help you decide whether
    the translation can be considered good and therefore whether it should be used
    for search. An SMT model evaluates the probability of many possible translations
    and only returns the one with the highest probability. If you ask the SMT model
    to output all the probabilities for the example query “hombre,” you’ll see high
    probabilities for good translations and low probabilities for unrelated translations,
    as in this sample output:'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 翻译文本片段的概率将帮助你决定翻译是否可以被认为是好的，因此是否应该用于搜索。统计机器翻译模型评估许多可能的翻译的概率，并只返回概率最高的那个。如果你要求SMT模型输出示例查询“hombre”的所有概率，你会看到好的翻译有高概率，而无关的翻译有低概率，如下所示：
- en: '[PRE7]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Under the hood, the SMT model calculates the probability of each possible translation
    and records the translation that has the best probability. Such an algorithm looks
    like this in pseudocode:^([[1](#ch07fn01)])
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 在幕后，SMT模型计算每个可能的翻译的概率，并记录概率最高的翻译。这样的算法在伪代码中看起来是这样的：^([[1](#ch07fn01)])
- en: ¹
  id: totrans-84
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-85
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See also Bayes’ theorem, [https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem).
  id: totrans-86
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见贝叶斯定理，[https://en.wikipedia.org/wiki/Bayes%27_theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem)。
- en: '[PRE8]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '***1* Calculates the probability of the current target word given the source
    word “hombre”**'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 计算给定源词“hombre”的目标词的概率**'
- en: '***2* If the probability is higher than the current highest probability, you
    have a new best translation.**'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 如果概率高于当前最高概率，则有一个新的最佳翻译。**'
- en: '***3* Records the best translation**'
  id: totrans-90
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 记录最佳翻译**'
- en: '***4* Records the best translation probability**'
  id: totrans-91
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 记录最佳翻译概率**'
- en: 'The algorithm isn’t complex; the only missing piece is how to calculate probabilities
    like p(*e*) and p(*f*|*e*). In information theory and statistics, p(*f*|*e*) is
    the conditional probability of *e*, given *f*. Generally speaking, you can think
    of it as the probability of the event *e* occurring as a consequence of event
    *f*. In this case, “events” are pieces of text! Without going too deep into statistics,
    you can think of word probabilities relying on counting the frequencies of words.
    For example, p(man) would be equal to the number of times the word *man* appears
    in the parallel corpus. Similarly, you can assume p(hombre|man) is equal to the
    number of times the word *man* appears in a sentence in the target language that’s
    paired with a sentence in Spanish containing *hombre*. Let’s look at the following
    three parallel sentences: two of them contain *man* in the source language and
    *hombre* in the target sentence; the other contains *man* in the source sentence
    but not *hombre* in the target:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 算法并不复杂；唯一缺少的部分是如何计算像 p(*e*) 和 p(*f*|*e*) 这样的概率。在信息理论和统计学中，p(*f*|*e*) 是在给定 *f*
    的 *e* 的条件概率。一般来说，你可以将其视为事件 *e* 作为事件 *f* 的结果发生的概率。在这种情况下，“事件”是文本片段！不深入统计学，你可以将单词概率视为依赖于单词频率的计数。例如，p(man)
    将等于单词 *man* 在平行语料库中出现的次数。同样，你可以假设 p(hombre|man) 等于在目标语言中与包含 *hombre* 的西班牙语句子配对的句子中
    *man* 出现的次数。让我们看看以下三个平行句子：其中两个在源语言中包含 *man*，在目标句子中包含 *hombre*；另一个在源句中包含 *man*，但在目标句中不包含
    *hombre*：
- en: '[PRE9]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this case, p(hombre|man) equals 2\. As another example, in the parallel sentences,
    p(senor|man) equals 1 because the third parallel sentence contains *man* in the
    source sentence and *senor* in the target sentence. In summary, *hombre* is translated
    to *man* because, among the many possible alternatives, *man* is the most frequently
    used English word when a Spanish sentence contains *hombre*.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，p(hombre|man) 等于 2。作为另一个例子，在平行句子中，p(senor|man) 等于 1，因为第三个平行句子在源句中包含 *man*，在目标句中包含
    *senor*。总之，*hombre* 被翻译成 *man*，因为在许多可能的替代方案中，*man* 是当西班牙语句子包含 *hombre* 时最常使用的英语单词。
- en: You’ve learned some of the basics of SMT. You’ll also get to know some of the
    challenges that make this task harder than it may seem from this introduction;
    they’re important to know, because neural machine translation is less affected
    by such problems—part of the rationale behind the current switch from SMT to neural
    machine translation (NMT).
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经学习了一些SMT的基础知识。你还将了解到一些使这项任务比从介绍中看似更困难的挑战；了解这些很重要，因为神经机器翻译受此类问题的影响较小——这是当前从SMT转向神经机器翻译（NMT）的部分原因。
- en: 7.2.1\. Alignment
  id: totrans-96
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 对齐
- en: 'In the previous section, you learned that you can build a statistical model
    to translate text. This translation happens by estimating probabilities based
    on the frequency of words. In practice, though, there are other factors at play.
    For example, the co-occurrence of two words *f* and *e* in two source and target
    sentences doesn’t mean one is the translation of the other. In the previously
    mentioned sentences, the words *a* and *hombre* co-occur more frequently than
    *hombre* and *man*:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你了解到你可以构建一个统计模型来翻译文本。这种翻译是通过根据单词的频率估计概率来实现的。然而，在实践中，还有其他因素在起作用。例如，在两个源句和目标句中，两个单词
    *f* 和 *e* 的共现并不意味着一个是另一个的翻译。在前面提到的句子中，单词 *a* 和 *hombre* 的共现频率比 *hombre* 和 *man*
    更高：
- en: '[PRE10]'
  id: totrans-98
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: So p(hombre|a) = 3, and p(hombre|man) = 2 Does that mean *a* is English for
    *hombre*? Of course not! This information is important when deciding whether the
    right translation for *hombre* is *a* or *man*.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，p(hombre|a) = 3，p(hombre|man) = 2。这意味着 *a* 是 *hombre* 的英语吗？当然不是！当决定 *hombre*
    的正确翻译是 *a* 还是 *man* 时，这个信息很重要。
- en: 'But translated words aren’t always perfectly aligned. Consider the third parallel
    sentence: the correct translation for *man* is *senor* in that context. But *man*
    is in the third position in the source sentence, whereas *senor* is in the second
    position in the target sentence:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 但翻译的单词并不总是完美对齐。考虑第三个平行句子：在那个上下文中，*man* 的正确翻译是 *senor*。但 *man* 在源句中的位置是第三位，而
    *senor* 在目标句中的位置是第二位：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'The task of dealing with words placed at different positions in source and
    target sentences is called *word alignment*, and it plays an important role in
    the effectiveness of SMT. SMT models usually define an *alignment function* that
    maps, for example, a Spanish target word at position *i* to an English source
    word at position *j*. The mapping for the sentence transforms the positions according
    to the indices 1 → 1, 2 → 3, 3 → 2:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 处理源句和目标句中不同位置放置的单词的任务被称为 *词对齐*，它在统计机器翻译的有效性中起着重要作用。统计机器翻译模型通常定义一个 *对齐函数*，例如，将西班牙语目标单词在位置
    *i* 映射到英语源单词在位置 *j*。句子的映射根据索引 1 → 1，2 → 3，3 → 2 转换位置：
- en: '[PRE12]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* “a” and “un” are at the same position.**'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1** “a” 和 “un” 处于相同的位置。'
- en: '***2* “man” and “senor” are one position apart.**'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2** “man” 和 “senor” 相隔一个位置。'
- en: 'Another example where word alignment plays an important role is when there’s
    no one-to-one mapping between words in different languages. This is especially
    true with languages that don’t originate from the same root language. Let’s take
    another example of an English-to-Spanish parallel sentence:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个例子是，当不同语言中的单词之间没有一对一映射时，词对齐扮演着重要角色。这尤其适用于不是源自同一语源语言的语言。让我们再举一个英语到西班牙语的平行句子的例子：
- en: '[PRE13]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'There are two special cases here:'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有两种特殊情况：
- en: The words *I live* in English are translated into the single word *vivo* in
    Spanish.
  id: totrans-109
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英语中的单词 *I live* 被翻译成西班牙语的单一单词 *vivo*。
- en: The word *USA* in English is translated into the two words *Estados Unidos*
    in Spanish.
  id: totrans-110
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 英语中的单词 *USA* 被翻译成西班牙语的两个单词 *Estados Unidos*。
- en: 'The word-alignment function will need to also take care of these cases:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 词对齐函数还需要处理以下情况：
- en: '[PRE14]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 7.2.2\. Phrase-based translation
  id: totrans-113
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2. 基于短语的翻译
- en: So far, we’ve discussed how to translate single words. But, as in many other
    areas of natural language processing, translating a single word is difficult without
    knowing the context. Phrase-based translation aims to reduce the amount of error
    due to the lack of information when translating single words. Generally, performing
    phrase-based translation requires more data to train a good statistical model,
    but it can handle longer sentences better, and it’s often more accurate than word-based
    statistical models. All the things you learned for word-based SMT models apply
    to phrase-based models; the only difference is that the translation units aren’t
    words, but phrases.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们讨论了如何翻译单个单词。但是，正如在自然语言处理的许多其他领域一样，在不了解上下文的情况下翻译单个单词是困难的。基于短语的翻译旨在减少由于缺乏信息而导致的翻译错误。通常，执行基于短语的翻译需要更多的数据来训练一个好的统计模型，但它可以更好地处理较长的句子，并且通常比基于单词的统计模型更准确。你所学的所有关于基于单词的统计机器翻译模型的知识都适用于基于短语的模型；唯一的区别是翻译单元不是单词，而是短语。
- en: When a phrase-based model receives an input text, it breaks the text into phrases.
    Each phrase is translated independently, and then the per-phrase translations
    are reordered using a phrase-alignment function. Until the success of neural models
    for MT, phrase (and hierarchical) SMT models were the de facto standard for MT
    and were used in many tools, such as Google Translate.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 当基于短语的模型接收到一个输入文本时，它会将文本分解成短语。每个短语都是独立翻译的，然后使用短语对齐函数对每个短语的翻译进行重新排序。在神经模型在机器翻译中取得成功之前，短语（和分层）统计机器翻译模型是事实上的标准，并被用于许多工具中，例如
    Google Translate。
- en: 7.3\. Working with parallel corpora
  id: totrans-116
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3. 使用平行语料库
- en: 'As you probably realize, one of the most important aspects of machine learning
    is having a lot of good-quality data. MT models are usually trained on parallel
    corpora: (text) datasets provided in two languages so that words, sentences, and
    so on in the source language can be mapped to words, sentences, and so on in the
    target language.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 如你或许已经意识到的，机器学习最重要的方面之一是拥有大量高质量的数据。机器翻译模型通常在平行语料库上训练：提供两种语言的（文本）数据集，以便源语言中的单词、句子等可以映射到目标语言中的单词、句子等。
- en: A very useful resource for those interested in MT is the Open Parallel Corpus
    (OPUS, [http://opus.nlpl.eu](http://opus.nlpl.eu)). It provides many parallel
    resources; you can select the source and target languages, and you’ll be shown
    a list of parallel corpora in different formats. Each parallel corpus is usually
    provided in different XML formats, or dedicated MT formats like the one from the
    Moses project ([www.statmt.org/moses](http://www.statmt.org/moses)). Sometimes
    translation dictionaries with word frequencies are also available.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 对于那些对机器翻译感兴趣的人来说，一个非常有用的资源是开放平行语料库 (OPUS, [http://opus.nlpl.eu](http://opus.nlpl.eu))。它提供了许多平行资源；你可以选择源语言和目标语言，然后你会看到不同格式的平行语料库列表。每个平行语料库通常以不同的
    XML 格式或 Moses 项目（[www.statmt.org/moses](http://www.statmt.org/moses)）的专用机器翻译格式提供。有时也会提供包含词频的翻译词典。
- en: In this context, let’s set up a small tool to parse the Translation Memory eXchange
    (TMX) format ([https://en.wikipedia.org/wiki/Translation_Memory_eXchange](https://en.wikipedia.org/wiki/Translation_Memory_eXchange)).
    Although the TMX specification isn’t new, a lot of the existing parallel corpora
    are available in TMX format on the OPUS project, so it’s useful to be able to
    work with TMX when you train your first NMT model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个上下文中，让我们设置一个小工具来解析翻译记忆交换 (TMX) 格式 ([https://zh.wikipedia.org/wiki/Translation_Memory_eXchange](https://zh.wikipedia.org/wiki/Translation_Memory_eXchange))。尽管
    TMX 规范并不新，但现有的许多平行语料库都在 OPUS 项目中以 TMX 格式提供，因此在你训练第一个神经机器翻译 (NMT) 模型时能够处理 TMX 格式是有用的。
- en: 'The TMX file format uses one `tu` XML node per parallel sentence. Each `tu`
    node has two `tuv` child elements: one for the source sentence and one for the
    target sentence. And each of those nodes has a `seg` node containing the actual
    text.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: TMX 文件格式使用一个 `tu` XML 节点表示一个平行句子。每个 `tu` 节点有两个 `tuv` 子元素：一个用于源句子，一个用于目标句子。并且每个这些节点都有一个包含实际文本的
    `seg` 节点。
- en: 'Here’s a sample from a TMX file for translating from English to Italian:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个从 TMX 文件中提取的示例，用于从英语翻译到意大利语：
- en: '[PRE15]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: In the end, you’re interested in getting the contents of the `tuv` and `seg`
    XML nodes. You want to collect parallel sentences where you can obtain the source
    and target text. To do so, you first create a `ParallelSentence` class.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你感兴趣的是获取 `tuv` 和 `seg` XML 节点的内容。你想要收集可以获取源文本和目标文本的平行句子。为此，你首先创建一个 `ParallelSentence`
    类。
- en: Listing 7.5\. A class for parallel sentences
  id: totrans-124
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.5\. 用于平行句子的类
- en: '[PRE16]'
  id: totrans-125
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: Next, let’s create a `TMXParser` class to extract a `Collection` of parallel
    sentences from TMX files.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个 `TMXParser` 类来从 TMX 文件中提取平行句子的 `Collection`。
- en: Listing 7.6\. Parsing and iterating through the parallel corpus
  id: totrans-127
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.6\. 解析并遍历平行语料库
- en: '[PRE17]'
  id: totrans-128
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The `TMXParser` will look inside all `tu`, `tuv`, and `seg` nodes and build
    the `Collection`:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: '`TMXParser` 将检查所有 `tu`、`tuv` 和 `seg` 节点并构建 `Collection`：'
- en: '[PRE18]'
  id: totrans-130
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '***1* Creates a parser on a TMX file, specifying the source and target languages**'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在 TMX 文件上创建一个解析器，指定源语言和目标语言**'
- en: '***2* Reads the file**'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 读取文件**'
- en: '***3* Creates an XMLEventReader: a utility class that emits events every time
    it reads XML elements**'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个 XMLEventReader：这是一个实用类，每次读取 XML 元素时都会发出事件**'
- en: '***4* Iterates over each XML event (nodes, attributes, and so on)**'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 遍历每个 XML 事件（节点、属性等）**'
- en: '***5* Intercepts tu nodes**'
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 截获 tu 节点**'
- en: '***6* Parses the tu nodes and reads the contained parallel sentences**'
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 解析 tu 节点并读取包含的平行句子**'
- en: 'We won’t dig too far into the code for extracting the `ParallelSentence`s,
    because parsing XML isn’t the primary focus here. For the sake of completeness,
    here’s the important part of the `parseEvent` method:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入挖掘提取 `ParallelSentence` 的代码，因为解析 XML 不是这里的重点。为了完整性，以下是 `parseEvent` 方法的重要部分：
- en: '[PRE19]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '***1* Closing tu element. The ParallelSentence is ready.**'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 关闭 tu 元素。平行句子准备就绪。**'
- en: '***2* Reads the language code from the tuv element**'
  id: totrans-140
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从 tuv 元素中读取语言代码**'
- en: '***3* Reads the text from the seg element**'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从 seg 元素中读取文本**'
- en: Using the generated parallel sentences, you can train an MT model—either statistical,
    as described in the previous section, or neural, as you’ll see next.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 使用生成的平行句子，你可以训练一个机器翻译模型——无论是统计模型，如前节所述，还是神经模型，如你接下来将要看到的。
- en: 7.4\. Neural machine translation
  id: totrans-143
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. 神经机器翻译
- en: With all that background about SMT and parallel corpora, you’re now ready to
    learn about why and how neural networks are used in the context of MT applied
    to search. Imagine you’re an engineer who has the task of building a search engine
    for a nonprofit organization that helps refugees from all around the world gather
    information about required legal documentation for each country. You need MT models
    for as many language pairs as possible (for example, Spanish to English, Swahili
    to English, English to Spanish, and so on). Training statistical models based
    on explicit probability estimation, like the word- or phrase-based SMT models
    discussed earlier, would be time-consuming because of the amount of manual work
    such an approach usually takes. For example, word alignment would require a lot
    of work for each of the language pairs.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 在了解了关于统计机器翻译和并行语料库的所有背景信息后，你现在可以学习为什么以及如何在搜索应用中利用神经网络。想象一下，你是一名工程师，你的任务是为一家人道主义组织构建一个搜索引擎，该组织帮助来自世界各地的难民收集每个国家所需的法律文件信息。你需要尽可能多的语言对（例如，西班牙语到英语、斯瓦希里语到英语、英语到西班牙语等）的MT模型。基于显式概率估计（如之前讨论的基于词或短语的SMT模型）训练统计模型会非常耗时，因为这种方法通常需要大量的手动工作。例如，词对齐需要对每一对语言进行大量工作。
- en: When the first NMT models were introduced, one of their most intriguing features
    was that they didn’t require much tuning. When Ilya Sutskever presented the work
    he and his coauthors did on an encoder-decoder architecture for NMT,^([[2](#ch07fn02)])
    he stated, “We use minimum innovation for maximum results.”^([[3](#ch07fn03)])
    That turned out to be one of the best qualities of this type of model.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 当最初的NMT模型被介绍时，它们最吸引人的特点之一是它们不需要太多的调整。当伊利亚·苏茨克维展示他和他的合著者关于NMT的编码器-解码器架构的工作时，^([[2](#ch07fn02)])
    他表示，“我们用最小的创新换取最大的成果。”^([[3](#ch07fn03)]) 这最终成为了这类模型最好的品质之一。
- en: ²
  id: totrans-146
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-147
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Ilya Sutskever, Oriol Vinyals, and Quoc V. Le, “Sequence to Sequence Learning
    with Neural Networks,” September 10, 2014, [https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215).
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 伊利亚·苏茨克维、奥里奥尔·维尼亚尔斯和吴国光·莱， “使用神经网络的序列到序列学习，”2014年9月10日，[https://arxiv.org/abs/1409.3215](https://arxiv.org/abs/1409.3215).
- en: ³
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '“NIPS: Oral Session 4 - Ilya Sutskever,” Microsoft Research, August 18, 2016,
    [https://www.youtube.com/watch?v=-uyXE7dY5H0](https://www.youtube.com/watch?v=-uyXE7dY5H0).'
  id: totrans-151
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '“NIPS: 口头报告会 4 - 伊利亚·苏茨克维，”微软研究院，2016年8月18日，[https://www.youtube.com/watch?v=-uyXE7dY5H0](https://www.youtube.com/watch?v=-uyXE7dY5H0).'
- en: 'This approach uses a deep, long short-term memory (LSTM) network whose output
    is a big vector, the *thought vector* mentioned in [chapter 3](kindle_split_015.xhtml#ch03),
    and then feeds the sequence (and the thought vector) to another decoder LSTM that
    generates the translated sequence. Over time, different “flavors” of NMT models
    have been proposed, but the main idea of using an encoder-decoder network was
    a milestone: it was the first model fully based on neural networks to beat SMT
    models in an MT task.'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法使用一个深度、长短期记忆（LSTM）网络，其输出是一个大向量，即第3章中提到的*思维向量*，然后将序列（以及思维向量）输入到另一个解码器LSTM中，生成翻译序列。随着时间的推移，已经提出了不同“风味”的NMT模型，但使用编码器-解码器网络的主要思想是一个里程碑：它是第一个在MT任务中击败SMT模型的完全基于神经网络的模型。
- en: These models are flexible for mapping sequences to sequences in different domains,
    not just for MT. For example, you used seq2seq encoder-decoder models to perform
    query expansion in [chapter 3](kindle_split_015.xhtml#ch03), and thought vectors
    to retrieve related content in [chapter 6](kindle_split_018.xhtml#ch06). Now we’ll
    go a bit deeper into how such models work and how the sequences flow into and
    out of them.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 这些模型在将序列映射到不同领域的序列方面非常灵活，而不仅仅是用于机器翻译。例如，你在第3章中使用了seq2seq编码器-解码器模型来进行查询扩展，并在第6章中使用了思维向量来检索相关内容。现在我们将更深入地探讨这类模型的工作原理以及序列如何流入和流出它们。
- en: 7.4.1\. Encoder-decoder models
  id: totrans-154
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1\. 编码器-解码器模型
- en: At a high level, the encoder LSTM reads and encodes a sequence of the source
    text into a fixed-length vector, the thought vector. A decoder LSTM then outputs
    a translated version of the source sentence from the encoded vector. The encoder–decoder
    system is trained to maximize the probability of a correct translation, given
    a source sentence. So, to some extent, these encoder-decoder networks, like many
    other deep learning–based models, are a statistical model! The difference with
    respect to “traditional” SMT is that NMT models learn to maximize the correctness
    of a generated translation via neural networks, and they do so in an end-to-end
    fashion. For example, there’s no need for dedicated tools for word alignment;
    an encoder-decoder network only needs a huge collection of source/target sentence
    pairs.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，编码器LSTM读取并编码源文本的序列到一个固定长度的向量，即思维向量。然后解码器LSTM从编码向量输出源句子的翻译版本。编码器-解码器系统被训练以最大化给定源句子的正确翻译的概率。因此，在某种程度上，这些编码器-解码器网络，就像许多其他基于深度学习的模型一样，是一种统计模型！与“传统”统计机器翻译相比，差异在于NMT模型通过神经网络学习最大化生成翻译的正确性，并且它们以端到端的方式进行。例如，不需要专门的工具进行词对齐；编码器-解码器网络只需要大量的源/目标句子对集合。
- en: 'The key features of encoder-decoder models are as follows:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器-解码器模型的关键特性如下：
- en: They’re easy to set up and understand—the model is intuitive.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们易于设置和理解——模型直观。
- en: They can handle variable-length input and output sequences.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以处理可变长度的输入和输出序列。
- en: They produce input sequence embeddings that can be used in different ways.
  id: totrans-159
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们产生可用于不同方式的输入序列嵌入。
- en: They can be used for seq2seq mapping tasks in various domains.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可用于各个领域的seq2seq映射任务。
- en: They’re an end-to-end tool, as just explained.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们是一个端到端工具，正如刚才所解释的。
- en: Let’s break down the graph shown in [figure 7.4](#ch07fig04) to better understand
    what’s in each part of the model and how the parts work together. The encoder
    is made up of a recurrent neural network (RNN), usually an LSTM or another alternative
    like gated recurrent units (GRUs^([[4](#ch07fn04)])), which we don’t expand on
    here. Remember that the main difference between a feed-forward network and an
    RNN is that the latter has recurrent layers that make it possible to easily work
    with unbounded sequences of inputs while keeping the size of the input layer fixed.
    The encoder RNN is usually deep, so it has more than one hidden recurrent layer.
    Just as you saw when we introduced RNNs in [chapter 3](kindle_split_015.xhtml#ch03),
    you can add more hidden layers if the translation quality is poor even when a
    lot of training data is provided. In general, between two and five recurrent layers
    is enough for training sets on the order of magnitude of tens of gigabytes. The
    output of the encoder network is the thought vector, which corresponds to the
    last time step of the last hidden layer of the encoder network. For instance,
    if the encoder has four hidden layers, the last time step of the fourth layer
    will represent the thought vector.
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分解[图7.4](#ch07fig04)中显示的图表，以便更好地理解模型每个部分的内容以及这些部分是如何协同工作的。编码器由一个循环神经网络（RNN）组成，通常是LSTM或另一种替代方案，如门控循环单元（GRUs^([[4](#ch07fn04)]))，这里我们不做详细展开。记住，与前馈网络相比，RNN的主要区别在于后者具有循环层，这使得它能够轻松地处理无界长度的输入序列，同时保持输入层的大小固定。编码器RNN通常是深层的，因此它具有多个隐藏的循环层。正如我们在[第3章](kindle_split_015.xhtml#ch03)中介绍RNN时所看到的，如果即使提供了大量训练数据，翻译质量仍然不佳，你可以添加更多的隐藏层。一般来说，对于大约十吉字节量级的训练集，两到五个循环层就足够了。编码器网络的输出是思维向量，对应于编码器网络最后一个隐藏层的最后一个时间步。例如，如果编码器有四个隐藏层，第四层的最后一个时间步将代表思维向量。
- en: ⁴
  id: totrans-163
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ⁴
- en: ''
  id: totrans-164
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: See the well-known paper by Kyunghyun Cho et al., “Learning Phrase Representations
    Using RNN Encoder-Decoder for Statistical Machine Translation,” June 3, 2014,
    [https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078).
  id: totrans-165
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参考Kyunghyun Cho等人撰写的著名论文，“使用RNN编码器-解码器学习短语表示以进行统计机器翻译”，2014年6月3日，[https://arxiv.org/abs/1406.1078](https://arxiv.org/abs/1406.1078)。
- en: Figure 7.4\. An encoder-decoder model
  id: totrans-166
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4\. 编码器-解码器模型
- en: '![](Images/07fig04.jpg)'
  id: totrans-167
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig04.jpg)'
- en: For simplicity, let’s consider translating a sentence with four words, written
    by an Italian user who’s looking for information about entering the UK with an
    Italian identity card. The source sentence could be something like “carta id per
    gb.” The encoder network is fed one word of the sentence at each time step. After
    four time steps, the encoder network has been fed all four words in the input
    sentence, as shown in [figure 7.5](#ch07fig05).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，让我们考虑将一个由意大利用户撰写的句子翻译成四个单词，该用户正在寻找有关使用意大利身份证进入英国的信息。源句子可能类似于“carta id
    per gb。”编码网络在每个时间步接收句子中的一个单词。经过四个时间步后，编码网络已经接收了输入句子中的所有四个单词，如图7.5所示。
- en: Figure 7.5\. An encoder network with four hidden recurrent layers
  id: totrans-169
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5\. 具有四个隐藏循环层的编码网络
- en: '![](Images/07fig05_alt.jpg)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig05_alt.jpg)'
- en: '|  |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-172
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In practice, the input sequence is often reversed, because it turns out the
    neural network usually gives better results that way.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，输入序列通常会被反转，因为事实证明神经网络以这种方式通常给出更好的结果。
- en: '|  |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: When you learned about word2vec in [chapter 2](kindle_split_013.xhtml#ch02),
    you saw that words are often transformed into one-hot-encoded vectors to be used
    in a neural network. Word embeddings were an output of the word2vec algorithm.
    The encoder network does something similar, using an *embedding layer*. You transform
    the input words into one-hot-encoded vectors, and the network’s input layer has
    a dimension equal to the size of the vocabulary of words in the collection of
    source sentences. Remember that a one-hot-encoded vector for a certain word, such
    as *gb*, is a vector with a single 1 for the vector index assigned to that word,
    and 0 in all the remaining positions. Before the recurrent layer, the one-hot-encoded
    vector is transformed into a word embedding of a layer with a lower dimension
    than the input layer. This layer is the embedding layer, and its output is a vector
    representation of the word (a word embedding) similar to the one obtained using
    word2vec.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 当你在第2章中学习word2vec时，你看到单词通常被转换成one-hot编码的向量，以便在神经网络中使用。Word embeddings是word2vec算法的输出。编码网络做的是类似的事情，使用一个*嵌入层*。你将输入单词转换成one-hot编码的向量，网络的输入层维度等于源句子集合中单词词汇表的大小。记住，对于某个特定的单词，如*gb*，one-hot编码的向量是一个只有一个1的向量，该1对应于分配给该单词的向量索引，其余位置都是0。在循环层之前，one-hot编码的向量被转换成一个比输入层维度低的层的词嵌入。这个层是嵌入层，其输出是单词的向量表示（词嵌入），类似于使用word2vec获得的。
- en: Looking closer at the encoder network layers, you see a stack similar to that
    shown in [figure 7.6](#ch07fig06). This input layer consists of 10 neurons, which
    implies that the source language contains only 10 words; in reality, the input
    layer may contain tens of thousands of neurons. The embedding layer reduces the
    input word size and generates a vector whose values aren’t just 0s and 1s, but
    real values. The embedding layer output vector is then passed over to the recurrent
    layers.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察编码网络层，你会看到一个类似于图7.6所示的堆叠。这个输入层由10个神经元组成，这意味着源语言只包含10个单词；实际上，输入层可能包含成千上万的神经元。嵌入层减少了输入单词的大小，并生成一个值不仅仅是0和1的向量。嵌入层输出向量随后传递到循环层。
- en: Figure 7.6\. Encoder network layers (up to the second hidden recurrent layer)
    with a dictionary of 10 words
  id: totrans-177
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. 具有包含10个单词的字典的编码网络层（直到第二个隐藏循环层）
- en: '![](Images/07fig06_alt.jpg)'
  id: totrans-178
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig06_alt.jpg)'
- en: 'After processing the last word in the input sequence, a special token (such
    as `<EOS>`: end of sentence) is passed to the network to signal that the input
    is finished and decoding should start. This makes it easier to handle variable-length
    input sequences, because decoding won’t start until the `<EOS>` token is received.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理输入序列中的最后一个单词后，一个特殊的标记（例如 `<EOS>`：句子结束）被传递到网络中，以表示输入已完成，应开始解码。这使得处理可变长度的输入序列变得更容易，因为解码只有在接收到
    `<EOS>` 标记后才会开始。
- en: The decoding part mirrors the encoding part. The only difference is that the
    decoder (see [figure 7.7](#ch07fig07)) receives both the fixed-length vector and
    one source word at each time step.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 解码部分与编码部分相似。唯一的区别是解码器（见图7.7）在每个时间步接收固定长度的向量和源单词。
- en: Figure 7.7\. A decoder network with four hidden recurrent layers
  id: totrans-181
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.7\. 具有四个隐藏循环层的解码网络
- en: '![](Images/07fig07_alt.jpg)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig07_alt.jpg)'
- en: No embedding layer is used in the decoder. The probability values in the output
    layer of the decoder network are used to sample a word from the dictionary at
    each time step. Let’s look now at an encoder-decoder LSTM with DL4J in action.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器中没有使用嵌入层。解码器网络输出层的概率值在每个时间步用于从字典中采样一个词。现在让我们看看使用DL4J的编码器-解码器LSTM的实际操作。
- en: 7.4.2\. Encoder-decoder for MT in DL4J
  id: totrans-184
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2\. DL4J中的MT编码器-解码器
- en: DL4J lets you declare the architecture of your neural network via a *computational
    graph*. This is a common paradigm in the deep learning framework; similar patterns
    are used in other popular deep learning tools such as TensorFlow, Keras, and others.
    With a computational graph for a neural network, you can declare which layers
    exist and how they’re connected to one another.
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: DL4J允许您通过*计算图*声明您的神经网络架构。这是深度学习框架中的一种常见范式；类似的模式也用于其他流行的深度学习工具，如TensorFlow、Keras等。对于神经网络，您可以使用计算图来声明哪些层存在以及它们是如何相互连接的。
- en: 'Let’s consider the encoder network layers defined in the previous section.
    You have an input layer, an embedding layer, and two recurrent (LSTM) layers (shown
    as visualized by the DL4J UI in [figure 7.8](#ch07fig08)). The encoder network
    computational graph is as follows:'
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑上一节中定义的编码器网络层。您有一个输入层、一个嵌入层和两个循环（LSTM）层（如图7.8所示）。编码器网络计算图如下：
- en: '[PRE20]'
  id: totrans-187
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '***1* Specifies an input type for an RNN**'
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 指定RNN的输入类型**'
- en: '***2* Creates an embedding layer**'
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个嵌入层**'
- en: '***3* The embedding layer expects a number of inputs equal to the size of the
    word dictionary.**'
  id: totrans-190
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 嵌入层期望的输入数量等于词字典的大小。**'
- en: '***4* Output embedding vector width**'
  id: totrans-191
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 输出嵌入向量宽度**'
- en: '***5* Embedding layer input**'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 嵌入层输入**'
- en: '***6* Adds the first encoder layer**'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 添加第一个编码器层**'
- en: '***7* The first layer of the encoder is an LSTM layer.**'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 编码器的第一层是LSTM层。**'
- en: '***8* Uses a tanh function in the LSTM layers**'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 在LSTM层中使用tanh函数**'
- en: '***9* The encoder layer takes inputs from the embeddingEncoder layer.**'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 编码层从嵌入编码器层接收输入。**'
- en: '***10* Adds the second layer of the encoder (another LSTM layer)**'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 添加编码器的第二层（另一个LSTM层）**'
- en: '***11* The encoder2 layer takes inputs from the encoder layer.**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 编码器2层从编码器层接收输入。**'
- en: Figure 7.8\. Encoder layers
  id: totrans-199
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8\. 编码层
- en: '![](Images/07fig08.jpg)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig08.jpg)'
- en: 'The decoder part contains two LSTM layers and an output layer (see [figure
    7.9](#ch07fig09)). Translated words are sampled from the output values generated
    by the softmax function on the output layer:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 解码部分包含两个LSTM层和一个输出层（见图7.9）。翻译词是从softmax函数在输出层生成的输出值中采样的：
- en: '[PRE21]'
  id: totrans-202
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '***1* The decoder recurrent layers are also based on LSTMs.**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 解码器循环层也是基于LSTMs的。**'
- en: '***2* Normal RNN output layer**'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 正常RNN输出层**'
- en: '***3* The output is a probability distribution generated by the softmax activation.**'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 输出是由softmax激活函数生成的概率分布。**'
- en: '***4* The cost function to be used is multiclass cross entropy.**'
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 要使用的代价函数是多类交叉熵。**'
- en: Figure 7.9\. Decoder layers
  id: totrans-207
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9\. 解码层
- en: '![](Images/07fig09.jpg)'
  id: totrans-208
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig09.jpg)'
- en: 'At this point, you may think you’re finished, but you’re still missing the
    glue that connects the encoder with the decoder. This consists of the following:'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你可能认为你已经完成了，但你仍然缺少连接编码器和解码器的粘合剂。这包括以下内容：
- en: The thought vector layer, which captures the distributed representation of the
    source word used by the decoder to generate the correct translated word
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 思想向量层，它捕获了解码器用于生成正确翻译词的源词的分布式表示
- en: A side input used by the decoder to keep track of the words it generates
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 解码器用于跟踪其生成的词的侧输入
- en: The graph will look slightly more complex than you may expect, because the decoding
    side of the neural network uses both the thought vector and the outputs it generates
    as well, at each time step. The decoder network starts generating translated words
    as soon as it receives a special word (such as *go*) on a dedicated input. At
    that time step, the decoder fetches both the value from the thought vector generated
    by the encoder and this special word, and generates its first decoded word. In
    the next time step, it uses the just-generated decoded word as new input, together
    with the thought vector value, to generate the subsequent word—and so forth, until
    it generates a special word (such as `EOS`) that stops the decoding.
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 由于神经网络解码侧在每个时间步都使用思维向量和它生成的输出，因此这个图看起来可能比你预期的要复杂一些。解码器网络在接收到一个特殊单词（如*go*）时立即开始生成翻译单词。在那个时间步，解码器获取编码器生成的思维向量值和这个特殊单词，并生成它的第一个解码单词。在下一个时间步，它使用刚刚生成的解码单词作为新的输入，以及思维向量值，来生成后续的单词——以此类推，直到它生成一个特殊单词（如`EOS`）来停止解码。
- en: In summary, the thought vector layer is fed the last time step of the final
    recurrent (LSTM) layer of the encoder network and used as input to the decoder
    together with a word at each decoding time step, as illustrated in [figure 7.10](#ch07fig10).
    The complete model looks like [figure 7.11](#ch07fig11).
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 总结来说，思维向量层接收编码器网络最终循环（LSTM）层的最后一个时间步的输入，并将其与每个解码时间步的一个单词一起作为解码器的输入，如图7.10所示。[图7.11](#ch07fig11)展示了完整的模型。
- en: Figure 7.10\. Connecting layers
  id: totrans-214
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10\. 连接层
- en: '![](Images/07fig10.jpg)'
  id: totrans-215
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig10.jpg)'
- en: Figure 7.11\. Encoder-decoder model with two LSTMs layers per side
  id: totrans-216
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11\. 每侧有两个LSTM层的编码器-解码器模型
- en: '![](Images/07fig11.jpg)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig11.jpg)'
- en: 'The connections between the encoder and the decoder, shown in [figure 7.10](#ch07fig10),
    are implemented by the following code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 编码器和解码器之间的连接，如图7.10所示，是通过以下代码实现的：
- en: '[PRE22]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '***1* Only the last time step of the encoder output is recorded in the thought
    vector.**'
  id: totrans-220
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 只记录编码器输出的最后一个时间步到思维向量中。**'
- en: '***2* Creates a new time-series input for the decoder, initialized with the
    values from the thought vector**'
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为解码器创建一个新的时间序列输入，初始化为思维向量的值**'
- en: '***3* Prepares the decoder to receive merged inputs from the thought vector
    and the decoder side input**'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 准备解码器接收来自思维向量和解码器侧输入的合并输入**'
- en: 'With this computational graph built, you’re ready to train the network with
    a parallel corpus. In order to do so, you build a `ParallelCorpusProcessor` that
    processes the parallel corpus: for example, in the form of a TMX file downloaded
    from the OPUS project. This processor extracts the source and target sentences
    and builds the dictionary of words. Then it will be used to provide the input
    and output sequences required for training the encoder-decoder model:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 建立了这个计算图后，你就可以使用并行语料库来训练网络了。为了做到这一点，你构建一个`ParallelCorpusProcessor`来处理并行语料库：例如，以从OPUS项目下载的TMX文件的形式。这个处理器提取源句和目标句，并构建单词字典。然后它将被用来提供训练编码器-解码器模型所需的输入和输出序列：
- en: '[PRE23]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '***1* TMX file containing the parallel corpus**'
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 包含并行语料库的TMX文件**'
- en: '***2* Parses the TMX file and extracts source and target sentences based on
    the language codes (for example, “it” for the source, “en” for the target)**'
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 解析TMX文件，并根据语言代码（例如，“it”为源语言，“en”为目标语言）提取源句和目标句**'
- en: '***3* Processes the corpus**'
  id: totrans-227
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 处理语料库**'
- en: '***4* Retrieves the corpus dictionary**'
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取语料库字典**'
- en: '***5* Retrieves the parallel sentences**'
  id: totrans-229
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取并行句子**'
- en: 'The dictionary is now used to set up the network: the dictionary size defines
    the number of inputs (for one-hot-encoded vectors). In this case, the dictionary
    is a `Map` whose keys are the words and whose value is a number used to identify
    each word when feeding it into the embedding layer. The sentences and the dictionary
    are needed to build an iterator over the parallel sentences. A `DataSetIterator`
    over the parallel corpus is then used to train the network across different epochs
    (an *epoch* of training is a full round of training on all the available training
    examples from the training set):'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 现在字典被用来设置网络：字典的大小定义了输入的数量（对于one-hot编码的向量）。在这种情况下，字典是一个`Map`，其键是单词，其值是用于在将其输入到嵌入层时识别每个单词的数字。句子和字典被用来构建并行句子的迭代器。然后使用`DataSetIterator`在并行语料库上训练网络，跨越不同的时代（训练的一个时代是在所有可用的训练示例上完成的一次完整训练轮）：
- en: '[PRE24]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '***1* Builds the network using the computational graph**'
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用计算图构建网络**'
- en: '***2* Builds the iterator over the parallel corpus**'
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 构建并行语料库的迭代器**'
- en: '***3* Iterates over the corpus**'
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历语料库**'
- en: '***4* Extracts a batch of input and output sequences**'
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 提取一批输入和输出序列**'
- en: '***5* Trains the network over the current batch**'
  id: totrans-236
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在当前批次上训练网络**'
- en: The network now begins to learn to generate English sequences from Italian sequences.
    [Figure 7.12](#ch07fig12) shows the network error decreasing.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，网络开始学习从意大利语序列生成英语序列。图7.12[图7.12](#ch07fig12)显示了网络错误逐渐减少。
- en: Figure 7.12\. Encoder-decoder network training
  id: totrans-238
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12\. 编码器-解码器网络训练
- en: '![](Images/07fig12_alt.jpg)'
  id: totrans-239
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/07fig12_alt.jpg)'
- en: 'The translation performed by the network consists of a feed-forward pass for
    all the words in the input sequence across the encoder and decoder networks. The
    encoder network implements the `TranslatorTool` API, and the `output` method performs
    the feed-forward pass on the neural network. That gives the translated version
    of the source sentence:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 网络执行的翻译包括对输入序列中所有单词在编码器和解码器网络中的前向传递。编码器网络实现了`TranslatorTool` API，而`output`方法在神经网络上执行前向传递。这给出了源句子的翻译版本：
- en: '[PRE25]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'The `output` method transforms the text sequence into a vector and then passes
    it along the encoder and decoder networks. The text vector is fed into the network
    using the word indexes generated by the `ParallelCorpusProcessor`. So you transform
    a `String` into a `List<Double>`, which is the ordered list of word indexes corresponding
    to each token in the source sequence:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '`output`方法将文本序列转换为向量，然后将其传递到编码器和解码器网络。文本向量通过`ParallelCorpusProcessor`生成的单词索引输入到网络中。因此，你将`String`转换为`List<Double>`，这是对应于源序列中每个标记的单词索引的有序列表：'
- en: '[PRE26]'
  id: totrans-243
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'Now you prepare the actual vectors to be used as input to both the encoder
    (the `input` vector) and the decoder (the `decode` vector), and perform separate
    feed-forward passes for the encoder and decoder networks. The encoder feed-forward
    pass is as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你准备实际向量，用作编码器（`input`向量）和解码器（`decode`向量）的输入，并对编码器和解码器网络执行单独的前向传递。编码器的前向传递如下：
- en: '[PRE27]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'The decoder feed-forward pass is slightly more complex because it expects to
    use the thought vector generated by the encoder pass *and* the source sequence
    token vectors. So, at each time step, the decoder performs a translation, given
    the thought vector and a source sentence token vector:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 解码器的前向传递稍微复杂一些，因为它期望使用编码器传递生成的思想向量和源序列标记向量。因此，在每一个时间步，解码器根据思想向量和源句子标记向量执行翻译：
- en: '[PRE28]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Finally everything is set to start translating queries using the encoder-decoder
    network. (In practice, you’d perform the training phase outside of the search
    workflow.) Once training is finished, the model is persisted to disk and then
    loaded by the query parser defined at the beginning of this chapter:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，一切准备就绪，开始使用编码器-解码器网络翻译查询。（在实践中，你会在搜索工作流程之外执行训练阶段。）一旦训练完成，模型就会被持久化到磁盘，然后由本章开头定义的查询解析器加载：
- en: '[PRE29]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'The query parser is created using the encoder-decoder network for Italian sentences
    (and the language detector tool):'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 查询解析器是使用意大利语句子的编码器-解码器网络（以及语言检测工具）创建的：
- en: '[PRE30]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The query parser’s internal logging will tell you how it’s translating incoming
    queries. Suppose an Italian user wants to know whether their identity card is
    valid in the UK. Their query, written in Italian, is translated to English as
    follows using the encoder-decoder network:'
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 查询解析器的内部日志会告诉你它是如何翻译传入的查询的。假设一个意大利用户想知道他们的身份证在英国是否有效。他们的查询，用意大利语写成，如下使用编码器-解码器网络翻译成英语：
- en: '[PRE31]'
  id: totrans-253
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: This wraps up the end-to-end solution for machine translation based on an encoder-decoder
    model using LSTM networks. Many MT production systems use such models or extensions
    of them. One of the key advantages of using NMT is that it generally results in
    accurate translations, given enough training data—but such models can require
    significant computational resources when training. In the next section, we’ll
    examine another approach to implementing MT programs that uses word and document
    embeddings (word2vec, paragraph vectors, and so on). When compared with models
    like the one implemented in this section, it may not be able to achieve the same
    level of accuracy, but it requires far less computational resources and therefore
    may be a good compromise.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 这总结了基于使用LSTM网络的编码器-解码器模型进行机器翻译的端到端解决方案。许多机器翻译生产系统使用此类模型或其扩展。使用NMT的一个关键优势是，在提供足够的训练数据的情况下，通常会产生准确的翻译——但此类模型在训练时可能需要大量的计算资源。在下一节中，我们将检查另一种实现MT程序的方法，该方法使用词汇和文档嵌入（word2vec、段落向量等）。与本章实现的模型相比，它可能无法达到相同的准确度，但它需要的计算资源要少得多，因此可能是一个好的折衷方案。
- en: 7.5\. Word and document embeddings for multiple languages
  id: totrans-255
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5. 多语言词汇和文档嵌入
- en: Previous chapters used word embeddings (dense vectors representing words’ semantics),
    in particular the word2vec model, both to generate synonyms to enrich the text
    of documents to be indexed, and to define a ranking function that better captures
    the relevance of search results. In [chapter 6](kindle_split_018.xhtml#ch06),
    you saw a paragraph vector algorithm that learns dense vectors of sequences of
    text (entire documents or portions of them, such as paragraphs or sentences),
    and you used it to recommend similar content and create another (yet more powerful)
    ranking function. Now you’ll see each of those neural network algorithms applied
    to the task of translating text.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章使用了词汇嵌入（表示词汇语义的密集向量），特别是word2vec模型，既用于生成同义词以丰富待索引文档的文本，也用于定义一个更好地捕捉搜索结果相关性的排名函数。在[第6章](kindle_split_018.xhtml#ch06)中，你看到了一个段落向量算法，该算法学习文本序列（整个文档或其部分，如段落或句子）的密集向量，并使用它来推荐相似内容并创建另一个（更强大）的排名函数。现在，你将看到这些神经网络算法被应用于翻译文本的任务。
- en: 7.5.1\. Linear projected monolingual embeddings
  id: totrans-257
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1. 线性投影单语嵌入
- en: One of the key aspects of the word vectors generated by the word2vec model is
    that when such vectors are plotted as points in a vector space, words with similar
    meanings are placed close to one another. Soon after the publication of the paper
    that introduced word2vec, the same researchers wondered what would happen to word
    embeddings if they came from the same data but were translated. Would there be
    any relation between the word vectors for a piece of English text and the same
    text written in Spanish? They discovered that there were significant geometric
    similarities in the relations that hold between the same words in different languages.
    For example, the distribution of numbers and animals in English and Spanish is
    similar if their respective word vectors are plotted, as you can see in [figure
    7.13](#ch07fig13).
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: word2vec模型生成的词汇向量的一个关键方面是，当这些向量在向量空间中作为点绘制时，具有相似意义的词汇被放置在彼此附近。在介绍word2vec的论文发表后不久，同样的研究人员想知道，如果这些嵌入来自相同的数据但进行了翻译，会发生什么。英语文本的词汇向量与用西班牙语写的相同文本之间会有任何关系吗？他们发现，不同语言中相同词汇之间的关系存在显著的几何相似性。例如，如果将英语和西班牙语的数字和动物分布绘制出来，它们是相似的，正如你在[图7.13](#ch07fig13)中可以看到的那样。
- en: Figure 7.13\. English and Spanish embeddings from the paper “Exploiting Similarities
    among Languages for Machine Translation” by Mikolov et al.
  id: totrans-259
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.13. 来自Mikolov等人论文“利用语言间的相似性进行机器翻译”的英语和西班牙语嵌入。
- en: '![](Images/07fig13_alt.jpg)'
  id: totrans-260
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/07fig13_alt.jpg)'
- en: These visual and geometric similarities suggested that a function that can transform
    a word vector from the English embedding space into a word vector from the Spanish
    embedding space would be a good candidate for translating words. Such a function
    is called a *linear projection* because it’s sufficient to multiply the source
    vector (for an English word) by a certain *translation vector* to project the
    source word into a target word (in Spanish). Let’s assume you have a small vector
    with two dimensions <0.1, 0.2> for the English word *cat* from the word2vec model
    of the English text (in practice, this will never happen; real-life dimensions
    for word embeddings are usually in the order of hundreds or thousands). You can
    learn a transformation matrix that will approximate the source vector for *cat*
    in the corresponding vector <0.07, 0.22> of the word *gato* in the Spanish embedding
    space. A transformation matrix multiplies its weights by the input vector and
    outputs a projected vector.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 这些视觉和几何相似性表明，一个可以将英语嵌入空间中的词向量转换为西班牙语嵌入空间中的词向量的函数将是翻译单词的良好候选者。这种函数被称为*线性投影*，因为它只需要将源向量（对于英语单词）乘以一个特定的*翻译向量*，就可以将源单词投影到目标单词（西班牙语）。假设您有一个包含两个维度<0.1,
    0.2>的小向量，代表来自英语文本word2vec模型的英语单词*cat*（实际上，这种情况永远不会发生；现实生活中的词嵌入维度通常在数百或数千的量级）。您可以学习一个转换矩阵，该矩阵将近似*cat*在西班牙语嵌入空间中相应的向量<0.07,
    0.22>中的源向量。转换矩阵通过乘以其权重并输出投影向量来工作。
- en: To make this more practical, let’s set this up in DL4J using the same English-to-Italian
    parallel corpus used for the encoder-decoder. You’ll get a parallel corpus and
    build two independent word2vec models, one for the source language and one for
    the target language.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这更加实用，让我们在DL4J中使用与编码器-解码器相同的英语到意大利语的平行语料库来设置。您将获得一个平行语料库并构建两个独立的word2vec模型，一个用于源语言，一个用于目标语言。
- en: Listing 7.7\. Building two independent word2vec models
  id: totrans-263
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.7\. 构建两个独立的word2vec模型
- en: '[PRE32]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: '***1* Parses the parallel corpus file**'
  id: totrans-265
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 解析平行语料库文件**'
- en: '***2* Creates two separate collections for source and target sentences**'
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为源句和目标句创建两个独立的集合**'
- en: '***3* Trains two word2vec models: one from the source sentences and one from
    the target sentences**'
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 训练两个word2vec模型：一个来自源句子，一个来自目标句子**'
- en: '***4* The embedding dimensions, equal to the size of the hidden layer of the
    word2vec model, must be consistent across the two models.**'
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 嵌入维度，等于word2vec模型隐藏层的大小，必须在两个模型中保持一致。**'
- en: '***5* Trains two word2vec models: one from the source sentences and one from
    the target sentences**'
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 训练两个word2vec模型：一个来自源句子，一个来自目标句子**'
- en: '***6* The embedding dimensions, equal to the size of the hidden layer of the
    word2vec model, must be consistent across the two models.**'
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 嵌入维度，等于word2vec模型隐藏层的大小，必须在两个模型中保持一致。**'
- en: 'In this case, you also need extra information about word translations, not
    just raw source and target text. You need to be able to say which Italian word
    is the translation of each English word in the parallel corpus. You can obtain
    this information either from a dictionary (containing information such as *cat*
    = *gato*) or from a word-aligned corpus, where positional information about the
    source and target words is available for each parallel sentence. In the OPUS portal,
    it’s easy to find dictionary files with one word translation per line:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您还需要有关单词翻译的额外信息，而不仅仅是原始的源文本和目标文本。您需要能够说出平行语料库中每个英语单词的意大利语翻译是哪个单词。您可以从字典（包含如*cat*
    = *gato*之类的信息）或从词对齐语料库中获取这些信息，其中每个平行句子都提供了源单词和目标单词的位置信息。在OPUS门户中，很容易找到每行包含一个单词翻译的字典文件：
- en: '[PRE33]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: 'You can parse the dictionary with the following line of code:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下代码行解析字典：
- en: '[PRE34]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: At this point, you’ve learned word embeddings for both the English sentences
    and the Italian ones. The next step is to build a translation matrix. To do this,
    you need to put the word embeddings for English and Italian into two separate
    matrixes. Each matrix contains a row for each word, and each row consists of the
    embedding relative to the given word. From those matrixes, you learn the projection
    matrix.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，您已经学习了英语句子和意大利语句子的词嵌入。下一步是构建一个翻译矩阵。为此，您需要将英语和意大利语的词嵌入放入两个独立的矩阵中。每个矩阵包含每个单词的一行，每行包含相对于给定单词的嵌入。从这些矩阵中，您学习到投影矩阵。
- en: Listing 7.8\. Putting embeddings from each word2vec model in a separate matrix
  id: totrans-276
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.8\. 将每个 word2vec 模型的嵌入放入单独的矩阵中
- en: '[PRE35]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: With the two matrixes in place, the projection matrix can be learned using various
    methods. The goal is to minimize the distance between each target word vector
    and its corresponding source word vector multiplied by the transformation matrix.
    This example uses an algorithm for linear regression called *normal equation*.
    We’ll skip the details; the key point is that this approach finds the combination
    of values in the projection matrix that will give the best translation results.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 在两个矩阵就位的情况下，可以使用各种方法来学习投影矩阵。目标是使每个目标词向量与其对应的源词向量乘以变换矩阵的距离最小化。本例使用了一种称为**正规方程**的线性回归算法。我们将跳过细节；关键点是这种方法找到投影矩阵中值的组合，这将给出最佳的翻译结果。
- en: Listing 7.9\. Finding the projection matrix
  id: totrans-279
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.9\. 寻找投影矩阵
- en: '[PRE36]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: '***1* Inverts the source vectors matrix**'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 反转源向量矩阵**'
- en: '***2* Calculates the translation matrix**'
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 计算翻译矩阵**'
- en: This ends the training phase. All of this is now encapsulated in a `TranslatorTool`
    called `LinearProjectionMTEmbeddings`. The training steps can be performed either
    in a constructor or in a dedicated method (such as `LinearProjectionMTEmbeddings#train`).
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 这标志着训练阶段的结束。所有这些现在都被封装在一个名为 `TranslatorTool` 的 `LinearProjectionMTEmbeddings`
    中。训练步骤可以在构造函数中或在一个专用方法（如 `LinearProjectionMTEmbeddings#train`）中执行。
- en: 'From this point on, you can use the two word2vec models in conjunction with
    the projection matrix to translate words. For each source word, you check that
    you have a word embedding for it and then multiply that vector by the projection
    matrix. Such a candidate vector represents the approximation of the target word
    vector. Finally, you look for the nearest neighbor of the candidate vector in
    the target embedding space: the word associated with the resulting vector is the
    translation you’re looking for.'
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 从这一点开始，您可以使用两个 word2vec 模型和投影矩阵来翻译单词。对于每个源词，您检查是否有该词的词嵌入，然后将其向量乘以投影矩阵。这样的候选向量代表了目标词向量的近似。最后，您在目标嵌入空间中寻找候选向量的最近邻：与结果向量关联的词就是您要找的翻译。
- en: Listing 7.10\. Decoding a source word into a target word
  id: totrans-285
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.10\. 将源词解码为目标词
- en: '[PRE37]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: '***1* Checks whether the source word2vec model has a word vector for the source
    word**'
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 检查源词2vec模型是否有源词的词向量**'
- en: '***2* Retrieves the word embedding**'
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 获取词嵌入**'
- en: '***3* Multiplies the source vector by the projection matrix**'
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将源向量乘以投影矩阵**'
- en: '***4* Finds the candidate nearest-neighbor words**'
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 寻找候选最近邻词**'
- en: '***5* Adds the translations to the final result, including a score based on
    the distance between the source and target words**'
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将翻译添加到最终结果中，包括基于源词和目标词之间距离的得分**'
- en: You can perform a word-by-word translation on longer text sequences by extracting
    tokens from the input text sequence and applying the `decodeWord` method to each
    source word.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过从输入文本序列中提取标记并应用 `decodeWord` 方法到每个源词上来对较长的文本序列进行逐词翻译。
- en: Listing 7.11\. Translating text using `LinearProjectionMTEmbeddings`
  id: totrans-293
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.11\. 使用 `LinearProjectionMTEmbeddings` 翻译文本
- en: '[PRE38]'
  id: totrans-294
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '***1* Splits the input text into tokens (words)**'
  id: totrans-295
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将输入文本分割成标记（单词）**'
- en: '***2* Translates one word at a time and gets exactly one translation each**'
  id: totrans-296
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 逐词翻译并获取每个翻译的精确翻译**'
- en: '***3* Accumulates the translation score**'
  id: totrans-297
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 累积翻译得分**'
- en: '***4* Accumulates the translated words in a StringBuilder**'
  id: totrans-298
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将翻译的单词累积到 StringBuilder 中**'
- en: '***5* Generates the resulting translation with the translated text and score**'
  id: totrans-299
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 生成包含翻译文本和得分的最终翻译结果**'
- en: You’re finally ready to run some test translations.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在终于可以运行一些测试翻译了。
- en: Listing 7.12\. Testing `LinearProjectionMTEmbeddings`
  id: totrans-301
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.12\. 测试 `LinearProjectionMTEmbeddings`
- en: '[PRE39]'
  id: totrans-302
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: '***1* Test input words and sentences**'
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 测试输入单词和句子**'
- en: '***2* Parallel corpus file**'
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 平行语料库文件**'
- en: '***3* Parallel dictionary file**'
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 平行词典文件**'
- en: '***4* Trains the models and projection matrix for the LinearProjectionMTEmbeddings**'
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 训练 LinearProjectionMTEmbeddings 的模型和投影矩阵**'
- en: '***5* For each input text, returns the top translation**'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对于每个输入文本，返回最佳翻译**'
- en: 'You can expect good results, especially for translations of single words. This
    approach performs each translation in isolation, without using the surrounding
    words, so there’s room for improvement. In the following output, I’ve manually
    added an accuracy tag to each translation (in pointy brackets), to help readers
    who don’t know Italian:'
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以期待良好的结果，尤其是对于单词的翻译。这种方法对每个翻译都是独立进行的，不使用周围的单词，因此还有改进的空间。在以下输出中，我手动为每个翻译添加了一个准确度标签（在尖括号内），以帮助那些不了解意大利语的人：
- en: '[PRE40]'
  id: totrans-309
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: The outputs are okay although not perfect; you’d expect a properly trained encoder-decoder
    model to work better than this, but the amount of time and computational resources
    required are typically so much lower with linear projected machine translation
    embeddings that people working with low-resource systems may be willing to accept
    the compromise. In addition, word2vec models can be reused in other contexts.
    For example, you can use these projected embeddings for machine translation to
    make search more effective, and you can also use word2vec models in ranking or
    synonym expansion. You can select which word2vec model to use at search time,
    using a language detection tool like the one you used for query expansion.
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然输出结果还不错，但并不完美；你可能会期望一个经过良好训练的编码器-解码器模型表现得更好，但与线性投影机器翻译嵌入相比，所需的时间和计算资源通常要低得多，因此使用低资源系统的开发者可能愿意接受这种妥协。此外，word2vec模型可以在其他环境中重复使用。例如，你可以使用这些投影嵌入进行机器翻译，使搜索更有效，你还可以在排名或同义词扩展中使用word2vec模型。你可以在搜索时选择使用哪种word2vec模型，使用像你用于查询扩展的语言检测工具。
- en: Summary
  id: totrans-311
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Machine translation can be useful in the context of search to improve the user
    experience for users who speak various languages.
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在搜索的上下文中，机器翻译可以用来改善使用各种语言的用户的使用体验。
- en: Statistical models can achieve good translation accuracy, but the amount of
    pair tuning required for each language is nontrivial.
  id: totrans-313
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 统计模型可以实现良好的翻译准确度，但每个语言所需的成对调整量是相当大的。
- en: Neural machine translation models provide ways to learn to translate sequences
    of text into different languages in a less articulated yet more powerful way.
  id: totrans-314
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经机器翻译模型提供了以更简洁但更强大的方式学习将文本序列翻译成不同语言的方法。
- en: Chapter 8\. Content-based image search
  id: totrans-315
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第8章\. 基于内容的图像搜索
- en: '*This chapter covers*'
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Searching for images based on their content
  id: totrans-317
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据内容搜索图像
- en: Working with convolutional neural networks
  id: totrans-318
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用卷积神经网络工作
- en: Using query by example to search for similar images
  id: totrans-319
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用示例查询来搜索类似图像
- en: Traditionally, most users use search engines by writing text queries and consuming
    (reading) text results. For that reason, most of this book is focused on showing
    you ways neural networks can help users search through text documents. So far,
    you’ve seen how to
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 传统上，大多数用户通过编写文本查询并消费（阅读）文本结果来使用搜索引擎。因此，本书的大部分内容都专注于向你展示神经网络如何帮助用户通过文本文档进行搜索。到目前为止，你已经看到了如何
- en: Use word2vec to generate synonyms from the data ingested into the search engine,
    which makes it easier for users to find documents they may otherwise miss
  id: totrans-321
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用word2vec从搜索引擎中摄取的数据生成同义词，这使得用户更容易找到他们可能错过的文档
- en: Expand search queries under the hood via recurrent neural networks (RNNs), giving
    the search engine the ability to express a query in more ways without asking the
    user to write all of them
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过循环神经网络（RNNs）在底层扩展搜索查询，使搜索引擎能够以更多方式表达查询，而无需用户编写所有这些
- en: Rank text search results using word and document embeddings, thus providing
    more-relevant search results to end users
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用单词和文档嵌入对文本搜索结果进行排名，从而为最终用户提供更相关的搜索结果
- en: Translate text queries with the seq2seq model to improve how the search engine
    works with text written in multiple languages and better serve users speaking
    different languages
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用seq2seq模型将文本查询翻译成多种语言，以改善搜索引擎处理多种语言文本的方式，并更好地服务说不同语言的用户
- en: 'But users increasingly expect search engines to be “smarter” and to be able
    to handle more than just written text queries. Users want search engines to search
    the web using voice, as with the built-in microphone on a smartphone, and to return
    not just text documents, but also relevant images, videos, and other formats.
    In addition to web search, it’s becoming the norm for other types of search engines
    to index images and videos as well as text. A newspaper website, for instance,
    consists of more than text articles: on the homepage of any newspaper, you’ll
    find multimedia content in addition to text. Therefore, a search engine for these
    websites needs to index images and video as well as text.'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，用户越来越期望搜索引擎能够“更智能”，能够处理不仅仅是文本查询。用户希望搜索引擎能够使用智能手机内置麦克风进行语音搜索，并返回不仅仅是文本文档，还包括相关的图像、视频和其他格式。除了网络搜索之外，其他类型的搜索引擎索引图像和视频以及文本已成为常态。例如，报纸网站不仅包含文本文章：在任何报纸的主页上，您都会发现除了文本之外的多媒体内容。因此，这些网站的搜索引擎需要索引图像和视频以及文本。
- en: 'For some time now, databases have indexed images using *metadata*: written
    information about an image, such as its title or a description of its contents,
    that’s attached to the image. Traditional information retrieval techniques, as
    well as the newer approaches described in this book, use metadata tags to help
    users find the pictures they’re looking for. But manually crafting and inputting
    descriptions and tags for every image you need to index is tedious, time consuming,
    and prone to subjective error—one indexer’s couch, after all, may be another indexer’s
    sofa. Wouldn’t it be nice if you could index images and make them searchable just
    as they are, without any manual intervention?'
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 近年来，数据库一直使用*元数据*来索引图像：附加到图像上的关于图像的书面信息，例如其标题或其内容的描述。传统的信息检索技术以及本书中描述的新方法都使用元数据标签来帮助用户找到他们想要的图片。但是，手动为需要索引的每一张图像编写描述和标签是繁琐的、耗时的，并且容易受到主观错误的影响——毕竟，一个索引者的沙发可能对另一个索引者来说就是沙发。如果能够不进行任何手动干预，直接索引图像并使其可搜索，那岂不是很好？
- en: 'In this chapter, we’ll look at how to do just that: outfit a search engine
    with image search that allows users to search through images based on their contents
    rather than based on text descriptions of their contents. To build this kind of
    image search, we’ll use convolutional neural networks, which are a special type
    of deep neural network.'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将探讨如何做到这一点：为搜索引擎配备图像搜索功能，使用户能够根据图像内容而不是根据其内容的文本描述进行搜索。为了构建这种图像搜索，我们将使用卷积神经网络，这是一种特殊的深度神经网络。
- en: A search engine for images works by indexing image features. When we talk about
    machine learning, a *feature* represents semantically relevant data that we want
    to capture in order to solve a particular task. More concretely, when dealing
    with images, an image feature can be represented by specific image points or regions
    (for example, high-contrast regions, shapes, edges, and so on). I’ll start by
    touching on the traditional ways of extracting important semantics from images,
    because we can use these techniques as a guide to the challenges of extracting
    features from images. This is a key step, because the extracted features can then
    be used to compare images, make and answer queries, and perform other tasks that
    a search engine needs to do.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 图像搜索引擎通过索引图像特征来工作。当我们谈论机器学习时，一个*特征*代表我们想要捕获以解决特定任务的语义相关数据。更具体地说，当处理图像时，图像特征可以表示为特定的图像点或区域（例如，高对比度区域、形状、边缘等）。我将首先简要介绍从图像中提取重要语义的传统方法，因为这些技术可以作为指导我们面对从图像中提取特征挑战的指南。这是一个关键步骤，因为提取的特征可以用来比较图像、提出和回答查询，以及执行搜索引擎需要执行的其他任务。
- en: Then I’ll show you a different and better way to extract image features using
    deep neural networks, which requires less manual work and no handcrafted feature
    extractors. Finally, we’ll look into how to incorporate the extracted features
    in a search engine, while also taking into account performance based on the time
    and space required to manage this type of image search.
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我将向您展示一种不同且更好的方法来使用深度神经网络提取图像特征，这需要更少的手动工作，并且不需要手工制作的特征提取器。最后，我们将探讨如何在搜索引擎中整合提取的特征，同时考虑到管理此类图像搜索所需的时间和空间性能。
- en: '|  |'
  id: totrans-330
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: Note
  id: totrans-331
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: In this chapter, images are discussed rather than videos, for simplicity’s sake.
    A video is essentially a sequence of images with attached audio bits, so you can
    certainly apply the approaches in this chapter to a video search scenario as well
    as an image search scenario.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，为了简化起见，讨论的是图片而不是视频。视频本质上是一系列带有附加音频比特的图片，因此你当然可以将本章中的方法应用于视频搜索场景以及图片搜索场景。
- en: '|  |'
  id: totrans-333
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 8.1\. Image contents and search
  id: totrans-334
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1\. 图片内容和搜索
- en: 'Back in [chapter 1](kindle_split_012.xhtml#ch01), I gave a brief introduction
    to one of deep learning’s most promising aspects: representation learning. *Representation
    learning* is the task of taking input data (for example, images) and automatically
    extracting features that make it easy for a program to resolve a particular problem
    (such as recognizing which objects are shown in an image, how similar two images
    are, and so on). A good representation of a certain image should be expressive,
    meaning it should ideally provide information about different aspects of the image
    (objects contained, light, exposure, and so on) while also making it easy to compare
    single aspects (for example, you may want to determine whether two images contain
    a butterfly by comparing such learned representations). At a high level, learning
    an image representation using a deep neural network commonly follows the simple
    flow shown in [figure 8.1](#ch08fig01), where pixels are converted to edges, edges
    to shapes, and shapes to objects.'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第1章](kindle_split_012.xhtml#ch01)中，我简要介绍了深度学习最有希望的一个方面：表示学习。*表示学习*是指从输入数据（例如，图片）中自动提取特征的任务，这些特征使得程序能够轻松解决特定问题（例如，识别图片中显示的对象、比较两张图片的相似度等）。一个好的图片表示应该是表达性的，这意味着它理想上应该提供关于图片不同方面的信息（包含的对象、光线、曝光等），同时使比较单个方面变得容易（例如，你可能想通过比较学习到的表示来确定两张图片是否都包含蝴蝶）。在较高层次上，使用深度神经网络学习图片表示通常遵循简单的流程，如[图8.1](#ch08fig01)所示，其中像素被转换为边缘，边缘转换为形状，形状转换为对象。
- en: Figure 8.1\. Learning image abstractions incrementally
  id: totrans-336
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.1\. 逐步学习图像抽象
- en: '![](Images/08fig01_alt.jpg)'
  id: totrans-337
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig01_alt.jpg)'
- en: 'Let’s consider an image stored on the hard disk of a computer and see what
    such a binary representation tells us about its contents. Can you quickly open
    an image file as you would a text file, and immediately recognize what the image
    shows? The answer is no. If you look at the raw contents (for example, using the
    Linux `cat` command) of the file of an image showing, say, a butterfly, you see
    nothing that can tell you about its contents:'
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑存储在电脑硬盘上的图片，看看这种二进制表示如何告诉我们其内容。你能像打开文本文件一样快速打开一个图片文件，并立即识别出图片显示的内容吗？答案是：不能。如果你查看显示蝴蝶的图片文件的原始内容（例如，使用Linux的`cat`命令），你将看不到任何可以告诉你其内容的信息：
- en: '[PRE41]'
  id: totrans-339
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: 'The image file shows a butterfly when opened with the proper program: you can
    use tools to “view” images, but a computer isn’t able to automatically recognize
    what the image contains or tell you if it’s a picture of an old lady, a wild animal
    in a landscape, or whatever else it might be. The binary content representation
    of an image isn’t good for telling you that there’s a butterfly in it.'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 打开正确的程序时，图片文件会显示一只蝴蝶：你可以使用工具来“查看”图片，但电脑无法自动识别图片包含的内容，或者告诉你它是一张老妇人的照片、风景中的野生动物，或者任何其他可能的内容。图片的二进制内容表示并不适合告诉你其中有一只蝴蝶。
- en: Deep learning (DL), however, *can* help you learn a representation that, when
    used properly, can tell you more about image contents. In this case, a deep neural
    network could tell you that the image features a butterfly. A DL algorithm usually
    accomplishes this by learning more and more information at each deep layer. For
    instance, in the first layers, it learns edges, at successive layers it learns
    shapes, and in the final layers it learns objects (like a butterfly, or a portion
    of one) so that it can tell what an image contains. Additionally, this information
    from all the layers is often encoded in a dense vector representation for each
    image. Later in this chapter, we’ll unpack this quick overview of the process,
    and you’ll finally meet deep neural networks that can learn image representations.
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习（DL），然而，*可以*帮助你学习一种表示，当正确使用时，可以告诉你更多关于图像内容的信息。在这种情况下，一个深度神经网络可以告诉你图像中有一只蝴蝶。深度学习算法通常通过在每一层学习越来越多的信息来完成这一任务。例如，在最开始的层，它学习边缘，在随后的层中学习形状，在最终的层中学习对象（如蝴蝶或其一部分），以便它可以告诉图像包含什么。此外，所有这些层的信息通常被编码在每个图像的密集向量表示中。在本章的后面部分，我们将详细解释这个过程的快速概述，你将最终遇到可以学习图像表示的深度神经网络。
- en: If you’ve ever tried to create a postcard by using an image (royalty-free, of
    course) available on the internet, then you may have experienced problems when
    searching for images relevant to a specific topic of interest. Let’s say, for
    instance, that you bought a model car for your nephew or niece, and you want to
    print a postcard of a car to use as a card that you can write on and send to him
    or her. So you go to a search engine for images—perhaps Google Images or Adobe
    Stock—and type something like “sports car” in the query box. The important thing
    to understand about this process is that users look for images that contain a
    particular object or a specific feature. For example, you may want a “red sports
    car” or a “vintage sports car.” Search engines for images often use a mechanism
    called *query by example* (QBE) where you upload or take a picture to be used
    as your input query. The search engine then returns images similar to the one
    that is input.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你曾经尝试过使用互联网上可用的图像（当然是无版权的）制作明信片，那么你可能遇到过在搜索与特定感兴趣主题相关的图像时的问题。比如说，比如你为你的侄子或侄女买了一辆模型车，你想打印一张汽车明信片作为可以写上并寄给他的卡片。所以你去了图像搜索引擎——可能是谷歌图片或Adobe
    Stock——并在查询框中输入类似“跑车”的内容。理解这个过程的一个重要事情是，用户寻找包含特定对象或特定特征的图像。例如，你可能想要一辆“红色跑车”或一辆“复古跑车”。图像搜索引擎通常使用一种称为*示例查询*（QBE）的机制，其中你上传或拍摄一张图片作为你的输入查询。然后搜索引擎返回与输入图像相似的图像。
- en: 'Let’s freeze our running query for a moment and look at how this QBE process
    works. We’ll start by thinking about how images are produced: how a digital camera
    or a graphics app creates and stores a picture. Snap a picture with a camera,
    and a file is stored somewhere that contains binary data (0s and 1s). You can
    think of this image stored in a computer as a grid with a certain width and height,
    where each cell in the grid is called a *pixel* and each pixel has a certain color.
    A colored pixel can be represented in different ways, and several color models
    are used to describe colors. For the sake of simplicity, we’ll pick the most common
    scheme, *RGB* (Red, Green, Blue), in which each color is made by a mixture of
    some red, some green, and some blue. Each of those three colors has a range of
    values from 0 to 255, indicating the amount of red, green, and blue to be used
    in each combination (there’s not just *one* red). Each such value can then be
    represented with 8 binary values (2^8 = 256) and thus contains all the possible
    ranges from 0 to 255\. So an RGB image has a grid whose pixels are made from binary
    values representing their colors.^([[1](#ch08fn01)]) For example, the color *red*
    is R:255, G:0, B:0; *blue* is R:0, G:0, B:255, and so on.'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们暂时冻结我们的运行查询，看看这个QBE过程是如何工作的。我们将首先思考图像是如何产生的：数字相机或图形应用程序如何创建和存储图片。用相机拍一张照片，文件就会存储在某个地方，包含二进制数据（0s和1s）。您可以将存储在计算机中的此图像想象成一个具有特定宽度和高度的网格，其中网格中的每个单元格称为*像素*，每个像素都有一定的颜色。彩色像素可以用不同的方式表示，并且使用几种颜色模型来描述颜色。为了简单起见，我们将选择最常用的方案，*RGB*（红色、绿色、蓝色），其中每种颜色都是由一些红色、一些绿色和一些蓝色混合而成的。这三种颜色中的每一种都有从0到255的值范围，表示每种组合中要使用的红色、绿色和蓝色的数量（不仅仅是*一种*红色）。每个这样的值可以用8个二进制值表示（2^8
    = 256），因此包含从0到255的所有可能范围。所以RGB图像有一个网格，其像素由表示其颜色的二进制值组成.^([[1](#ch08fn01)]) 例如，颜色*红色*是R:255,
    G:0, B:0；*蓝色*是R:0, G:0, B:255，等等。
- en: ¹
  id: totrans-344
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-345
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: Although in practice images can have lots of different formats and color schemes,
    the core problem is that images are usually stored as plain binaries, optionally
    with metadata that usually doesn’t tell anything about their contents.
  id: totrans-346
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 虽然在实际应用中图像可能有多种不同的格式和色彩方案，但核心问题是图像通常以纯二进制形式存储，可选地带有通常不会说明其内容的元数据。
- en: With this in mind, let’s unfreeze our query. How can you match a query for “sports
    car” when images are just series of bits? In the following sections, you’ll see
    a few different ways you can make queries and images match, and learn some techniques
    for finding the particular sports car you want.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个想法，让我们解冻我们的查询。当图像只是位序列时，如何匹配“跑车”的查询呢？在接下来的几节中，您将看到几种不同的方法，您可以使用这些方法使查询和图像匹配，并学习一些找到您想要的特定跑车的技术。
- en: '8.2\. A look back: Text-based image retrieval'
  id: totrans-348
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2. 回顾：基于文本的图像检索
- en: Users naturally tend to think about images in terms of what objects they contain
    (like sports cars), rather than their RGB values. But shapes and colors are better
    for specifying the information need, as in the thing they’re looking for, whether
    it’s a red sports car, a Formula 1 sports car, or some other kind.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 用户自然倾向于从包含的对象（如跑车）的角度来考虑图像，而不是它们的RGB值。但形状和颜色更适合指定信息需求，比如他们正在寻找的东西，无论是红色跑车、一级方程式跑车还是其他某种跑车。
- en: A less-smart but common approach to mitigating the problem of matching text
    queries with binary images is to add metadata to images during indexing. You’re
    indexing images, but each one has a relevant text caption or description. This
    allows you to do a normal search with a text query; the search will return images
    that have metadata text attached to them that matches the query. Conceptually,
    this isn’t much different from a regular full-text search, except the search results
    are images instead of document titles or excerpts.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 一种不那么智能但常见的解决匹配文本查询与二进制图像问题的方法是，在索引过程中向图像添加元数据。您正在索引图像，但每个图像都有一个相关的文本标题或描述。这使得您可以使用文本查询进行正常搜索；搜索将返回具有与查询匹配的元数据文本的图像。从概念上讲，这与常规全文搜索没有太大区别，只是搜索结果为图像而不是文档标题或摘录。
- en: Using the sports car query, let’s assume there are four images that can match
    that query. During indexing, you can ingest both the image data and a small caption
    describing each image; see [figure 8.2](#ch08fig02). The image data is used to
    return the actual image content to the end user (in the search results list),
    and the text description of the image is indexed to match queries and images (as
    you’ll see in the next section, in [figure 8.3](#ch08fig03)).
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 使用跑车查询，假设有四张图像可以匹配该查询。在索引过程中，你可以同时摄取图像数据和描述每张图像的小标题；参见[图 8.2](#ch08fig02)。图像数据用于向最终用户返回实际的图像内容（在搜索结果列表中），而图像的文本描述被索引以匹配查询和图像（正如你将在下一节中看到的，在[图
    8.3](#ch08fig03)中）。
- en: Figure 8.2\. Manually captioned sports car images
  id: totrans-352
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.2. 手动标注的跑车图像
- en: '![](Images/08fig02_alt.jpg)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig02_alt.jpg)'
- en: If you search for “sports car,” the search engine will return all the images
    shown in [figure 8.2](#ch08fig02). If you search for “black sports car,” only
    two of them will appear in the results list (recall that using double quotes in
    a query forces matches on the entire phrase “black sports car” rather on the single
    words “black,” “sports,” and “car”).
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你搜索“跑车”，搜索引擎将返回[图 8.2](#ch08fig02)中显示的所有图像。如果你搜索“黑色跑车”，只有其中两个会在结果列表中显示（记住，在查询中使用双引号会强制匹配整个短语“黑色跑车”而不是单个单词“黑色”、“跑车”和“车”）。
- en: 'This approach can be performed in Lucene in a straightforward way. You store
    the image binary as it is but index a manually entered description of the image
    (the description won’t be returned with the search results):'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法在 Lucene 中可以简单地执行。你可以直接存储图像的二进制数据，但将手动输入的图像描述（描述不会随搜索结果返回）进行索引：
- en: '[PRE42]'
  id: totrans-356
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '***1* Obtains the image content as a byte[]**'
  id: totrans-357
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取图像内容作为 byte[]**'
- en: '***2* Writes an image description as a String**'
  id: totrans-358
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将图像描述作为字符串写入**'
- en: '***3* Adds the image binary content as a stored field**'
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将图像二进制内容作为存储字段添加**'
- en: '***4* Adds the image description as a text field**'
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将图像描述作为文本字段添加**'
- en: '***5* Indexes the image document**'
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 索引图像文档**'
- en: '***6* Commits the index changes**'
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 提交索引更改**'
- en: 'At search time, a simple text query can be used:'
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索时，可以使用简单的文本查询：
- en: '[PRE43]'
  id: totrans-364
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '***1* Opens an IndexReader over the index containing the images**'
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在包含图像的索引上打开 IndexReader**'
- en: '***2* Creates an IndexSearcher to run the query**'
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建一个 IndexSearcher 来运行查询**'
- en: '***3* Runs a query for “black sports car” on the caption field**'
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在标题字段上对“黑色跑车”运行查询**'
- en: '***4* Fetches each matching document**'
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取每个匹配的文档**'
- en: '***5* Retrieves the “binary” field**'
  id: totrans-369
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 获取“二进制”字段**'
- en: '***6* Retrieves the actual image as a binary and does something with it**'
  id: totrans-370
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 获取实际的图像作为二进制数据并对其进行处理**'
- en: 'This approach can work for a small number of images. But it’s very common to
    have data of a size in the order of magnitude of millions or billions of documents.
    Even a small online shop that makes postcards will probably have hundreds or thousands
    of images. In many cases, it isn’t possible to ask people to undertake the (not
    very pleasant) task of looking at each and every image and coming up with good
    descriptive text. And, sometimes, such text isn’t good enough for all search cases.
    (In production systems, it isn’t uncommon to have issues like “Why isn’t the query
    ‘black sports car’ returning the black fluorescent sports car? Please change the
    description so that it can match such a query.”) In summary, this approach doesn’t
    scale, and it’s only as good as the quality of the descriptions: poor descriptions
    lead to irrelevant search results.'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 这种方法适用于少量图像。但是，通常会有数量级为百万或十亿的文档大小的数据。即使是制作明信片的小型在线商店也可能有数百或数千张图像。在许多情况下，不可能要求人们查看每一张图像并编写好的描述性文本。有时，这样的文本对于所有搜索情况来说也不够好。（在生产系统中，出现像“为什么查询‘黑色跑车’没有返回黑色荧光跑车？请更改描述以便它可以匹配此类查询。”的问题并不罕见。）总之，这种方法无法扩展，并且其效果仅取决于描述的质量：差的描述会导致不相关的搜索结果。
- en: 8.3\. Understanding images
  id: totrans-372
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3. 理解图像
- en: 'As I said, an image can be described in various ways, and the most common is
    to specify the people, objects, animals, and other recognizable objects it contains:
    for example, “This is a picture of a man.” Additionally, you can mention descriptive
    details, such as “This image shows a tall man.” As you can see in [figure 8.3](#ch08fig03),
    however, such brief descriptions are prone to ambiguity. The ambiguity comes from
    the simple fact that one object or entity can be described in many different ways.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我所说，图像可以用各种方式来描述，最常见的方式是指定它包含的人、物体、动物和其他可识别的物体：例如，“这是一张男人的照片。”此外，你可以提及描述性细节，例如，“这幅图像展示了一个高个子男人。”然而，正如你在[图8.3](#ch08fig03)中可以看到的，这样的简短描述容易产生歧义。这种歧义源于这样一个简单的事实：一个物体或实体可以用许多不同的方式来描述。
- en: Figure 8.3\. Some images described as “tall man”
  id: totrans-374
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.3。一些被描述为“高个子男人”的图像
- en: '![](Images/08fig03_alt.jpg)'
  id: totrans-375
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig03_alt.jpg)'
- en: All three images in the figure certainly fit the description of a “tall man”
    that might be used as a text query. The image in the middle, however, is different
    from the others. Yes, it’s a picture of a tall man, but it’s also a picture of
    a player from the Houston Rockets NBA basketball team. So other phrases, including
    “basketball player,” “houston rockets player,” and “basketball player wearing
    a 35 numbered jersey” describe that image as well. It’s impossible for a human
    tasked with the job of writing short metatags to think of every possible way an
    image can be described.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 图中的三幅图像无疑符合“高个子男人”这一描述，可能被用作文本查询。然而，中间的图像与其他图像不同。是的，它是一张高个子男人的照片，但它也是休斯顿火箭队NBA篮球队球员的照片。因此，其他短语，包括“篮球运动员”、“休斯顿火箭队球员”和“身穿35号球衣的篮球运动员”也描述了这幅图像。对于被指派编写简短元标签的人类来说，思考图像可能被描述的每一种可能方式是不可能的。
- en: In the same vein, a description like “basketball player wearing a 35 numbered
    jersey” would perfectly fit not only the center image in [figure 8.3](#ch08fig03),
    but also the images in [figure 8.4](#ch08fig04), which are of entirely different
    players and teams. In this case, the user may be looking for one kind of image
    and get an entirely different kind, even though both have the same descriptive
    metatag and would appear in the search results.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，描述“身穿35号球衣的篮球运动员”不仅适合[图8.3](#ch08fig03)中的中心图像，也适合[图8.4](#ch08fig04)中的图像，这些图像是不同球员和球队的。在这种情况下，用户可能正在寻找一种类型的图像，却得到了完全不同的图像，尽管两者都有相同的描述性元标签，并且会出现在搜索结果中。
- en: Figure 8.4\. Some images described as “basketball player wearing a 35 numbered
    jersey”
  id: totrans-378
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.4。一些被描述为“身穿35号球衣的篮球运动员”的图像
- en: '![](Images/08fig04_alt.jpg)'
  id: totrans-379
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig04_alt.jpg)'
- en: These simple examples teach you that text is extremely prone to mismatches,
    because a single entity (a person, an animal, an object, and so on) can be described
    in many different ways. This makes the quality of search results dependent on
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 这些简单的例子教你，文本极其容易产生不匹配，因为单个实体（一个人、一个动物、一个物体等）可以用许多不同的方式来描述。这使得搜索结果的质量依赖于
- en: The way the user defines queries
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户定义查询的方式
- en: The way documents are written
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 文档的编写方式
- en: You’ve already seen such problems in the context of search—that’s one of the
    reasons why we use synonyms, query expansion, and so on. The search engine should
    be smart enough to be able to enhance queries and indexed documents.
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经在搜索的背景下看到了这样的问题——这就是我们为什么使用同义词、查询扩展等原因之一。搜索引擎应该足够智能，能够增强查询和索引文档。
- en: In contrast, images, visually speaking, are generally less affected by this
    kind of ambiguity. Let’s take the first image described as “tall man” and imagine
    you find images that are visually similar to it, as in [figure 8.5](#ch08fig05).
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，从视觉上来说，图像通常不太受这种歧义的影响。让我们以被描述为“高个子男人”的第一幅图像为例，想象你找到了与之视觉上相似的图像，如[图8.5](#ch08fig05)所示。
- en: Figure 8.5\. Some visually similar images
  id: totrans-385
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.5。一些视觉上相似的图像
- en: '![](Images/08fig05_alt.jpg)'
  id: totrans-386
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig05_alt.jpg)'
- en: 'An input image allows for a better definition of what’s in the image, regardless
    of the different ways it can be textually described. At the same time, it’s easy
    to say whether an image is *not* similar to the input one: for example, the basketball
    player image from [figure 8.3](#ch08fig03) is clearly different from the images
    in [figure 8.5](#ch08fig05), in terms of both the color and type of clothing the
    man is wearing.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 输入图像允许更好地定义图像中的内容，无论它可以用多少种不同的文本方式来描述。同时，很容易判断图像是否与输入图像不相似：例如，[图8.3](#ch08fig03)中的篮球运动员图像在颜色和服装类型上显然与[图8.5](#ch08fig05)中的图像不同。
- en: 'Using sample images instead of text as input queries (also known as *querying
    by example*) is very common in image search platforms where systems try to extract
    semantic information from the images for accurate retrieval rather than having
    text metadata describe each image. Users express their query intent by means of
    a visual query. Just as with text queries, the quality of the query has an impact
    on the relevance of the results. Thinking in terms of text for a second may help:
    the query “red car” can return results ranging from a toy car to a Formula 1 race
    car, as long as it’s red. If instead the query is “red sports car” or “Formula
    1 red race car,” then the range of possibly relevant results will be less broad
    and less vague. The same applies to visual queries and search results: the more
    accurate the query (the visual description of the information needed), the better
    the search results. With images, what makes the difference isn’t the user’s ability
    to write a “good” query—instead, the algorithm responsible for extracting the
    information to be indexed and searching for images is most significant. Capturing
    objects and their features (color, light, shape, and so on) in an image, for example,
    is one of the challenges in this area.'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 使用样本图像而不是文本作为输入查询（也称为*示例查询*）在图像搜索平台上非常常见，在这些平台上，系统试图从图像中提取语义信息以进行准确检索，而不是让文本元数据描述每张图像。用户通过视觉查询来表达他们的查询意图。就像文本查询一样，查询的质量会影响结果的相关性。暂时从文本的角度思考一下可能有助于理解：查询“红色汽车”可以返回从玩具车到一级方程式赛车等各种结果，只要它是红色的。如果查询改为“红色跑车”或“一级方程式红色赛车”，那么可能的相关结果范围将更窄、更具体。同样，这也适用于视觉查询和搜索结果：查询越准确（所需信息的视觉描述），搜索结果越好。在图像中，区分差异的不是用户编写“好”查询的能力——相反，负责提取索引信息并搜索图像的算法最为关键。例如，在图像中捕捉对象及其特征（颜色、光线、形状等）是这一领域的挑战之一。
- en: Now you have all the pieces you need to start looking at some algorithms to
    extract information from images and represent them in a way that makes it possible
    to run queries that return meaningful results.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经拥有了开始研究一些算法所需的所有组件，这些算法可以从图像中提取信息并以一种方式表示它们，使得能够运行查询并返回有意义的搜索结果。
- en: 8.3.1\. Image representations
  id: totrans-390
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.1. 图像表示
- en: The biggest challenge at this point is how to describe images in a way that
    makes it possible to find similar images. In the example, you want to create a
    postcard to accompany a gift. It would be great if you could take a picture of
    the gift with a camera and use that as a query to the image search engine. That
    way, the postcard will have a nice-looking picture that somehow suggests what’s
    inside the gift box when you give it to the recipient.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 目前最大的挑战是如何描述图像，以便能够找到相似图像。在示例中，你想要为礼物制作一张明信片。如果能用相机拍下礼物的照片，并将其作为图像搜索引擎的查询，那就太好了。这样，明信片将有一张漂亮的图片，当给收件人时，能以某种方式暗示礼物盒里的内容。
- en: 'Although images are made up of pixels, it isn’t possible to perform a plain
    pixel comparison. Pixel values alone don’t provide enough information about what’s
    in an image. One problem is that a pixel represents only a very tiny portion of
    the image, and it gives no information about its context. A red pixel may be part
    of a red apple or a red car: there’s no way to determine which one it comes from
    by looking at pixels alone. Even if pixels alone gave useful global information
    about an image, a large, high-quality image these days may contain millions of
    pixels, so performing a pixel-by-pixel comparison wouldn’t be computationally
    efficient. Additionally, even two pictures of the same object taken with the same
    camera in the exact same conditions (light, exposure, and so on), but taken from
    two slightly different angles, will probably generate very different binary images,
    pixel-wise.'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管图像由像素组成，但无法直接进行像素比较。仅凭像素值本身并不能提供足够的信息来描述图像中的内容。一个问题是像素仅代表图像的一小部分，并且它不提供任何关于其上下文的信息。一个红色像素可能是红色苹果或红色汽车的一部分：仅通过观察像素无法确定它来自哪个对象。即使像素本身提供了关于图像的有用全局信息，如今一个大型、高质量的图像可能包含数百万个像素，因此逐像素比较在计算上并不高效。此外，即使是在完全相同的条件下（光线、曝光等）使用同一台相机拍摄的两个相同对象的图片，但如果从两个略微不同的角度拍摄，可能生成非常不同的二值图像，像素层面上会有很大差异。
- en: In this case, you want to take a picture of your gift—a red model sports car—while
    you’re at home, without caring about lighting conditions and the exact angle you
    shoot the photo from. For example, you might take a picture like the one in [figure
    8.6](#ch08fig06). And you want the image search engine to return a nice picture
    of an actual red sports car, preferably the very same car model, as in [figure
    8.7](#ch08fig07).
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，您想在在家的时候拍摄您的礼物——一辆红色模型跑车——而不关心光照条件和您拍摄照片的确切角度。例如，您可能会拍摄像[图8.6](#ch08fig06)中的照片。并且您希望图像搜索引擎返回一张真正的红色跑车的漂亮图片，最好是同一款车型，就像[图8.7](#ch08fig07)中所示。
- en: Figure 8.6\. The red toy sports car you want to give as a gift
  id: totrans-394
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.6. 您想要作为礼物赠送的红色玩具跑车
- en: '![](Images/08fig06_alt.jpg)'
  id: totrans-395
  prefs: []
  type: TYPE_IMG
  zh: '![图像8.6](Images/08fig06_alt.jpg)'
- en: Figure 8.7\. A red sports car photo like the one you want retrieved by the search
    engine, based on the picture of a toy
  id: totrans-396
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.7. 一张您希望搜索引擎检索的红色跑车照片，基于玩具照片
- en: '![](Images/08fig07_alt.jpg)'
  id: totrans-397
  prefs: []
  type: TYPE_IMG
  zh: '![图像8.6](Images/08fig07_alt.jpg)'
- en: To overcome the problem of pixels providing poor information, the most widely
    used technique to create searchable images is to extract *visual features* from
    them and index those features instead of “just” the pixels. These visual features
    promise to provide information that can be used to look for the contents of an
    image. Features are usually represented by sets of numbers or vectors.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服像素提供信息不佳的问题，创建可搜索图像最广泛使用的技术是从像素中提取*视觉特征*并对这些特征进行索引，而不是“仅仅”对像素进行索引。这些视觉特征承诺提供可用于查找图像内容的信息。特征通常由一组数字或向量表示。
- en: '|  |'
  id: totrans-399
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Note
  id: totrans-400
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 注意
- en: You’ll see in the next section what this means, when we look at a couple of
    feature extraction techniques. Understanding how feature extraction works with
    non-neural network–based methods is useful to set the basis for the kind of semantics
    they can convey and how different (and less human readable) they are with respect
    to features extracted by DL techniques. As you’ll learn later in this chapter,
    the amount of engineering effort spent on DL techniques is much less than is required
    to design an accurate algorithm for feature extraction. Also important is the
    fact that at the time of writing, DL-based feature extraction beats every handcrafted
    feature extraction algorithm.
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到这意味着什么，当我们查看一些特征提取技术时。了解特征提取如何与非神经网络方法协同工作对于确定它们可以传达的语义类型以及它们与深度学习技术提取的特征相比（以及它们如何不同且更难以理解）非常有用。正如您在本章后面将学到的，用于深度学习技术的工程工作量远小于设计一个准确的特征提取算法所需的工作量。同样重要的是，在撰写本文时，基于深度学习的特征提取优于所有手工制作的特征提取算法。
- en: '|  |'
  id: totrans-402
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: The search engine must be able to work with such features to find similar images
    in the QBE scenario. It will extract features from the images at indexing time
    and from the example query image at query time. So feature extraction is important
    for understanding what’s in the image; but another important aspect is how to
    efficiently compare features of different images. Feature-indexing techniques
    will impact the amount of disk space required to store such inverted indexes;
    fast search algorithms for features are required to efficiently retrieve images
    at search time.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎必须能够处理这些特征，以便在QBE场景中找到相似图像。它将在索引时从图像中提取特征，在查询时从示例查询图像中提取特征。因此，特征提取对于理解图像内容非常重要；但另一个重要方面是如何有效地比较不同图像的特征。特征索引技术将影响存储此类倒排索引所需的磁盘空间量；需要快速的特征搜索算法以在搜索时有效地检索图像。
- en: 'Visual features can be of different types:'
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 视觉特征可以是不同类型的：
- en: They can refer to *global features* like the colors used across the image, identified
    textures, or global or average values for RGB and other color models (CMYK, HSV,
    and so on).
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以指代*全局特征*，如图像中使用的颜色、识别的纹理或RGB和其他颜色模型（如CMYK、HSV等）的全局或平均值。
- en: They can refer to *local features* (extracted from portions of the image) like
    edges, corners, or other interesting key points in image cells (as in methods
    like scale-invariant feature transform, speeded-up robust features, difference
    of Gaussians, and so on, discussed later in the chapter).
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它们可以指代*局部特征*（从图像的部分提取），如边缘、角点或其他有趣的图像单元中的关键点（如后面章节中讨论的尺度不变特征变换、加速鲁棒特征、高斯差等）。
- en: They can be learned end to end as semantic abstractions that are close to the
    human cognition process, thanks to the use of deep neural networks.
  id: totrans-407
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于使用了深度神经网络，它们可以作为接近人类认知过程的语义抽象从头到尾进行学习。
- en: The first two types are often referred to as *handcrafted* features because
    the respective algorithms have been designed and tuned for the purpose based on
    heuristics. Many DL-based models for image representations feed the network layers
    with image pixels (inputs to the neural network) and learn to classify images
    (the network output classes); during training, the neural network *learns* features
    automatically—this is the third type of features.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 前两种类型通常被称为*手工*特征，因为相应的算法是基于启发式方法设计和调整的。许多用于图像表示的基于深度学习的模型将图像像素（神经网络输入）馈送到网络层，并学习对图像进行分类（网络输出类别）；在训练过程中，神经网络*学习*特征——这是第三种类型的特征。
- en: Let’s now look at some methods to extract both local and global handcrafted
    features. Then we’ll focus on DL-based feature learning for images.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看一些提取局部和全局手工特征的方法。然后我们将关注基于深度学习的图像特征学习。
- en: 8.3.2\. Feature extraction
  id: totrans-410
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.3.2. 特征提取
- en: Many cameras allow you to review a picture as soon as it’s taken. Some also
    provide information about the amount of color contained in the picture for each
    of the three RGB channels (red, green, blue). Let’s take as an example a picture
    of a butterfly, shown in [figure 8.8](#ch08fig08). The camera used to take that
    picture provides its color histogram, shown in [figure 8.9](#ch08fig09).
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 许多相机允许您在拍照后立即查看照片。一些相机还提供有关图片中每个RGB通道（红色、绿色、蓝色）所含颜色的信息。以蝴蝶照片为例，如图8.8所示[figure
    8.8](#ch08fig08)。拍摄该照片的相机提供了其颜色直方图，如图8.9所示[figure 8.9](#ch08fig09)。
- en: Figure 8.8\. A picture of a butterfly
  id: totrans-412
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.8. 蝴蝶照片
- en: '![](Images/08fig08_alt.jpg)'
  id: totrans-413
  prefs: []
  type: TYPE_IMG
  zh: '![蝴蝶照片](Images/08fig08_alt.jpg)'
- en: Figure 8.9\. Color histogram for the butterfly picture
  id: totrans-414
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.9. 蝴蝶照片的颜色直方图
- en: '![](Images/08fig09_alt.jpg)'
  id: totrans-415
  prefs: []
  type: TYPE_IMG
  zh: '![图像特征索引](Images/08fig09_alt.jpg)'
- en: A color histogram is a representation of the distribution of the three color
    channels’ possible values (for example, from 0 to 255) among the pixels. For example,
    if a certain pixel has a red channel value of 4 and another pixel has the same
    value, the color histogram for the red channel for that image will have a size
    of 2 for the value 4 (two pixels have a red channel value of 4). This process,
    applied to all the channels and pixels in a certain image, produces three red,
    green, and blue graphs like those shown in [figure 8.9](#ch08fig09). The color
    histogram is an example of a very simple, intuitive global feature that can be
    used to describe an image. We’ll look next at global and local feature extractors.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色直方图是三个颜色通道可能值（例如，从0到255）在像素中的分布表示。例如，如果一个像素的红色通道值为4，另一个像素具有相同的值，那么该图像红色通道的颜色直方图在值4处将有大小为2（两个像素具有红色通道值为4）。将此过程应用于某个图像中的所有通道和像素，将产生三个红色、绿色和蓝色图表，如图8.9所示[figure
    8.9](#ch08fig09)。颜色直方图是一个可以用来描述图像的非常简单、直观的全局特征示例。接下来，我们将探讨全局和局部特征提取器。
- en: Global features
  id: totrans-417
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 全局特征
- en: 'Instead of indexing images by manually tagging them with captions or descriptions,
    you can index the image binaries accompanied by their extracted features, as in
    [figure 8.10](#ch08fig10). To do so, you can use the open source library Lucene
    Image Retrieval (LIRE, licensed under the GNU GPL 2 license) to extract the color
    histogram from an image. LIRE provides a lot of useful tools for working with
    images, which are Lucene friendly. (At the time of writing, it doesn’t yet support
    any DL-based methods to extract image features.) Here’s an example:'
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: 与手动用标题或描述标记图像以进行索引不同，您可以索引带有提取特征的图像二进制文件，如图8.10所示[figure 8.10](#ch08fig10)。为此，您可以使用开源库Lucene
    Image Retrieval (LIRE，许可协议为GNU GPL 2许可)从图像中提取颜色直方图。LIRE提供了许多用于处理图像的有用工具，这些工具与Lucene兼容。（在撰写本文时，它还不支持任何基于深度学习的图像特征提取方法。）以下是一个示例：
- en: '[PRE44]'
  id: totrans-419
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: '***1* The image file**'
  id: totrans-420
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 图像文件**'
- en: '***2* Creates a color histogram object**'
  id: totrans-421
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建颜色直方图对象**'
- en: '***3* Reads the image from the file**'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 从文件中读取图像**'
- en: '***4* Extracts the color histogram from the image**'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从图像中提取颜色直方图**'
- en: '***5* Extracts the color histogram feature vectors as a double array**'
  id: totrans-424
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从双数组中提取颜色直方图特征向量**'
- en: Figure 8.10\. Indexing images with their features
  id: totrans-425
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.10. 使用特征对图像进行索引
- en: '![](Images/08fig10_alt.jpg)'
  id: totrans-426
  prefs: []
  type: TYPE_IMG
  zh: '![图像特征索引](Images/08fig10_alt.jpg)'
- en: Such a global representation of images has the advantage of being human interpretable
    and usually efficient in terms of performance. But if you think for a moment of
    the fact that the color histogram image representation is bound to the color distribution
    over the image (disregarding position), it’s not hard to realize that two different
    images with the same subject (such as a butterfly) may have very different color
    distributions. Consider the butterfly image shown earlier, in [figure 8.8](#ch08fig08),
    and another image of a butterfly, shown in [figure 8.11](#ch08fig11).
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的图像全局表示具有人类可解释性和通常在性能方面效率高的优点。但如果你稍微思考一下这样一个事实，即颜色直方图图像表示受限于图像上的颜色分布（不考虑位置），那么你很容易意识到，具有相同主题（例如蝴蝶）的两个不同图像可能会有非常不同的颜色分布。考虑前面显示的蝴蝶图像，在[图8.8](#ch08fig08)中，以及另一张蝴蝶图像，在[图8.11](#ch08fig11)中。
- en: Figure 8.11\. Another picture of a butterfly
  id: totrans-428
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.11\. 另一张蝴蝶的图片
- en: '![](Images/08fig11_alt.jpg)'
  id: totrans-429
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig11_alt.jpg)'
- en: 'Although the butterfly is the primary subject in both images, they have different
    color schemes: as you can see in electronic versions of this book, in [figure
    8.8](#ch08fig08), the main colors are yellow and green; whereas in [figure 8.11](#ch08fig11),
    the main colors are red, blue, and yellow. Comparisons of images based on histograms
    are mostly based on color distributions. The images’ histograms look very different
    (see [figure 8.12](#ch08fig12)), so the images won’t be considered similar by
    the search engine. Remember at this point that you aren’t running a search yet—you’re
    analyzing the histogram feature and trying to understand what kind of information
    it can give you.'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然蝴蝶在这两张图像中都是主要主题，但它们的颜色方案不同：正如你在本书的电子版中可以看到的，在[图8.8](#ch08fig08)中，主要颜色是黄色和绿色；而在[图8.11](#ch08fig11)中，主要颜色是红色、蓝色和黄色。基于直方图的图像比较主要基于颜色分布。这两张图像的直方图看起来非常不同（见[图8.12](#ch08fig12)），因此搜索引擎不会认为这两张图像相似。记住，此时你还没有运行搜索——你正在分析直方图特征，并试图了解它能够提供什么样的信息。
- en: Figure 8.12\. Comparing histograms of two butterfly images
  id: totrans-431
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.12\. 比较两张蝴蝶图像的直方图
- en: '![](Images/08fig12_alt.jpg)'
  id: totrans-432
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig12_alt.jpg)'
- en: 'The color histogram scheme is just one of many possible ways to extract global
    features, but in general they suffer from the problem that it’s difficult to capture
    image details. For example, the first butterfly image doesn’t just contain a butterfly:
    there are also flowers and leaves. Such entities aren’t captured by the color
    histogram; roughly speaking, such histograms tell you, “There’s a certain amount
    of light green, another amount of yellow, a small portion of white, some black,
    and so on.” One situation where global features can work well is duplicate image
    detection, where the searcher is looking for an image very similar to, if not
    exactly the same as, the one at hand.'
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色直方图方案只是提取全局特征的可能方法之一，但通常它们存在一个问题，即难以捕捉图像细节。例如，第一张蝴蝶图像不仅仅包含蝴蝶：还有花朵和叶子。这些实体没有被颜色直方图捕捉到；简而言之，这样的直方图告诉你，“有一定量的浅绿色，另一部分黄色，一小部分白色，一些黑色，等等。”全局特征可以很好地工作的一个情况是重复图像检测，其中搜索者正在寻找与手头图像非常相似，如果不是完全相同的图像。
- en: One detail that would help immensely is distinguishing background regions of
    a photo from the central image. We’d like the representations of the two butterfly
    images to somehow understand that the regions containing the butterfly are more
    important than the background portions.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: 一个非常有帮助的细节是区分照片的背景区域和中心图像。我们希望两个蝴蝶图像的表示能够理解包含蝴蝶的区域比背景部分更重要。
- en: Local features
  id: totrans-435
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 局部特征
- en: In contrast to global features, local features can more accurately capture details
    of portions of images. So if you want to make a program detect potentially interesting
    objects (for example, a butterfly) in an image, a common approach is to start
    by splitting that image into smaller cells, and then look in those cells for relevant
    shapes or objects. Let’s see how this works in [figure 8.13](#ch08fig13), using
    the same butterfly picture, but now split into smaller cells.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: 与全局特征相比，局部特征可以更准确地捕捉图像部分细节。因此，如果你想编写一个程序在图像中检测可能有趣的对象（例如蝴蝶），一个常见的方法是首先将图像分割成更小的单元格，然后在这些单元格中寻找相关的形状或对象。让我们看看[图8.13](#ch08fig13)中的工作方式，使用相同的蝴蝶图片，但现在分割成更小的单元格。
- en: Figure 8.13\. Splitting an image into smaller cells
  id: totrans-437
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.13\. 将图像分割成更小的单元格
- en: '![](Images/08fig13_alt.jpg)'
  id: totrans-438
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig13_alt.jpg)'
- en: 'Once the image is split into smaller parts (such as squares), the task of extracting
    local features consists of two steps:'
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被分割成更小的部分（例如正方形），提取局部特征的任务包括两个步骤：
- en: '**1**.  Find interesting points (rather than objects).'
  id: totrans-440
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**. 找到有趣点（而不是对象）。'
- en: '**2**.  Encode interesting points with respect to the local region into a descriptor
    that can be used later to match interesting regions.'
  id: totrans-441
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**. 将有关局部区域的有趣点编码到可以用于以后匹配有趣区域的描述符中。'
- en: But what does *interesting* mean in this context? You’re looking for points
    that delimit or center regions of the image that contain objects. The final goal
    is still to have a way to find objects and represent them using features that
    are comparable. Given two images containing butterflies, you want features that
    carry this information in both. Each image is usually represented as a feature
    vector—a number of features—so that if you compute the distance (for example,
    the cosine distance) between image feature vectors, images containing the same
    or similar objects should be close (have a low distance value).
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 但在这个上下文中，“有趣”是什么意思？你正在寻找界定或定位包含对象的图像区域的点。最终目标是仍然有一种方法来找到对象，并使用可比较的特征来表示它们。给定包含蝴蝶的两个图像，你希望特征在两者中都携带这种信息。每个图像通常表示为一个特征向量——一系列特征——因此，如果你计算图像特征向量之间的距离（例如，余弦距离），包含相同或相似对象的图像应该接近（具有低距离值）。
- en: Typical kinds of local features include human-understandable visual features
    like edges and corners. But in practice, local feature–extraction techniques like
    *scale-invariant feature transform* (SIFT) and *speeded-up robust features* (SURF)
    are used.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的局部特征包括人类可理解的视觉特征，如边缘和角点。但在实践中，使用像**尺度不变特征变换**（SIFT）和**加速鲁棒特征**（SURF）这样的局部特征提取技术。
- en: '|  |'
  id: totrans-444
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**SIFT**'
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: '**SIFT**'
- en: Finding edges is a relatively simple task that can be solved using mathematical
    tools like Fourier, Laplace, or Gabor transforms; the SIFT and SURF algorithms
    are more complex but also more powerful. With SIFT, for example, it’s possible
    to recognize important regions in an image so that an object and a rotated version
    of the same object produce the same or similar local features. This means that
    with SIFT-based features, images that contain the same rotated objects can be
    recognized as similar.
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 找到边缘是一个相对简单的任务，可以使用傅里叶、拉普拉斯或高博尔变换等数学工具来解决；SIFT和SURF算法更复杂但更强大。例如，使用SIFT，可以识别图像中的重要区域，使得一个物体和该物体的旋转版本产生相同或相似的局部特征。这意味着基于SIFT的特征，包含相同旋转物体的图像可以被识别为相似。
- en: We won’t dive into the details of SIFT, because that’s not part of the focus
    of this book; but briefly, it uses a *filter* called *Laplacian of Gaussian* to
    recognize interesting points in an image. You can think of a filter as a mask
    applied to the image. A Laplacian of Gaussian filter produces an image where edges
    and other key points are highlighted, and most other points are no longer visible.
    The filter is applied to a preprocessed version of the image, so the resulting
    image is represented in a scale-invariant manner. After the application of such
    a filter, the interesting points are made orientation invariant by recording the
    orientation of each interesting point, so each time it’s compared with other points,
    the orientation component is integrated in each calculation or comparison operation.
    Finally, all the found local features are encoded in a single comparable descriptor/feature
    vector.
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不会深入探讨SIFT的细节，因为那不是本书关注的重点；但简要来说，它使用一个称为**高斯拉普拉斯**的**滤波器**来识别图像中的有趣点。你可以将滤波器想象为应用于图像的掩模。高斯拉普拉斯滤波器产生一个边缘和其他关键点被突出显示的图像，而大多数其他点不再可见。滤波器应用于图像的预处理版本，因此生成的图像以尺度不变的方式表示。应用此类滤波器后，通过记录每个有趣点的方向，使有趣点变得方向不变，因此每次与其他点比较时，方向成分都会整合到每个计算或比较操作中。最后，所有找到的局部特征都被编码在一个单一的、可比较的描述符/特征向量中。
- en: '|  |'
  id: totrans-448
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 'Local features are representations of portions of an image. A single image
    is associated with several local features. But you need a single representation
    of an image so that:'
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 局部特征是图像部分的表示。单个图像可以关联到多个局部特征。但你需要一个图像的单个表示，以便：
- en: The final image representation contains information about all the interesting
    local points.
  id: totrans-450
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最终的图像表示包含了所有有趣局部点的信息。
- en: Efficient comparison can be performed at query time (one feature vector versus
    many feature vectors).
  id: totrans-451
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 查询时可以高效地进行比较（一个特征向量与多个特征向量进行比较）。
- en: 'To do that, local features need to be aggregated into a single representation
    (the feature vector). A common approach is to aggregate local features using the
    *bag-of-visual-words* (BOVW) model. You may recall the bag-of-words model from
    earlier in the book: in such a model, a document is represented as a vector whose
    size is equal to the number of words in all the existing documents. Each position
    in the vector is tied to a certain word: if the value is 1 (or any value larger
    than zero: for example, calculated using term frequency–inverse document frequency
    [TF-IDF]), then the related document contains that word; otherwise, the value
    is 0.'
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 要做到这一点，需要将局部特征聚合成一个单一表示（特征向量）。一种常见的方法是使用*视觉词袋*（BOVW）模型来聚合局部特征。你可能还记得书中早些时候提到的词袋模型：在这种模型中，一个文档被表示为一个向量，其大小等于所有现有文档中单词的总数。向量中的每个位置都与一个特定的单词相关联：如果值为1（或任何大于零的值：例如，使用词频-逆文档频率[TF-IDF]计算），则相关文档包含该单词；否则，值为0。
- en: Recall the sample bag-of-words representations for some documents in [chapter
    5](kindle_split_017.xhtml#ch05), shown in [table 8.1](#ch08table01).
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下[第5章](kindle_split_017.xhtml#ch05)中一些文档的样本词袋表示，如表8.1[表8.1](#ch08table01)所示。
- en: Table 8.1\. Bag-of-words representations
  id: totrans-454
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.1\. 词袋表示
- en: '| Terms | bernhard | bio | dive | hypothesis | in | influence | into | life
    | mathematical | riemann |'
  id: totrans-455
  prefs: []
  type: TYPE_TB
  zh: '| 术语 | bernhard | bio | dive | hypothesis | in | influence | into | life |
    mathematical | riemann |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-456
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| **doc1** | 1.28 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.28 |'
  id: totrans-457
  prefs: []
  type: TYPE_TB
  zh: '| **doc1** | 1.28 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 0.0 | 1.0 | 0.0 | 1.28 |'
- en: '| **doc2** | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
  id: totrans-458
  prefs: []
  type: TYPE_TB
  zh: '| **doc2** | 1.0 | 1.0 | 0.0 | 0.0 | 1.0 | 1.0 | 0.0 | 0.0 | 0.0 | 0.0 |'
- en: In the BOVW model, each value of the vector is greater than zero if the image
    has the local feature corresponding to that position. So instead of the word “bernhard”
    or “bio” in the text case, the BOVW model will have “local-feature1,” “local-feature2,”
    and so on. Each image is represented according the same principle, but using clustered
    local features instead of words; see [table 8.2](#ch08table02).
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 在BOVW模型中，如果图像具有对应位置的局部特征，则向量中的每个值都大于零。因此，在文本情况下，BOVW模型将会有“local-feature1”、“local-feature2”等，而不是文本中的“bernhard”或“bio”。每个图像都是根据相同的原理表示的，但使用聚类局部特征而不是单词；参见[表8.2](#ch08table02)。
- en: Table 8.2\. Bag-of-visual-words representations
  id: totrans-460
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.2\. 视觉词袋表示
- en: '| Features | local-feature1 | local-feature2 | local-feature3 | local-feature4
    | local-feature5 |'
  id: totrans-461
  prefs: []
  type: TYPE_TB
  zh: '| 特征 | local-feature1 | local-feature2 | local-feature3 | local-feature4 |
    local-feature5 |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-462
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| **image1** | 0.3 | 0.0 | 0.0 | 0.4 | 0.0 |'
  id: totrans-463
  prefs: []
  type: TYPE_TB
  zh: '| **image1** | 0.3 | 0.0 | 0.0 | 0.4 | 0.0 |'
- en: '| **image2** | 0.5 | 0.7 | 0.0 | 0.8 | 1.0 |'
  id: totrans-464
  prefs: []
  type: TYPE_TB
  zh: '| **image2** | 0.5 | 0.7 | 0.0 | 0.8 | 1.0 |'
- en: Using local feature extractors, like SIFT, each image comes with a number of
    descriptors that may vary depending on image quality, image size, and other factors.
  id: totrans-465
  prefs: []
  type: TYPE_NORMAL
  zh: 使用局部特征提取器，如SIFT，每个图像都附带一定数量的描述符，这些描述符可能因图像质量、图像大小和其他因素而异。
- en: The BOVW model involves an additional preprocessing step to identify a fixed
    number of local features. Let’s assume that for a dataset of images, SIFT extracts
    local features for each image, but some have tens and others hundreds of features.
    To create a shared vocabulary of local features, all the local features are collected
    together, and a clustering algorithm such as k-means is performed over them to
    extract *n* centroids. The centroids are the words for the BOVW model.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: BOVW模型涉及一个额外的预处理步骤来识别一定数量的局部特征。假设对于图像数据集，SIFT为每个图像提取局部特征，但有些有数十个特征，而有些有数百个特征。为了创建一个共享的局部特征词汇表，将所有局部特征收集在一起，并对它们执行聚类算法（如k-means）以提取*n*个质心。质心是BOVW模型中的单词。
- en: 'If you look at a clear sky on a dark night, you’ll see many different stars.
    Each star can be considered a cluster point: a local feature. Now imagine that
    the brightest stars in the sky have more stars near them (in reality, the brightness
    of a star depends on distance, size, age, radioactivity, and other factors). Under
    those conditions, the brightest stars are the cluster centroids; you can use them
    to represent all the points with some approximation. So instead of billions of
    stars (local features), you consider only tens or hundreds of stars: the circled
    centroids. That’s what clustering algorithms do (see [figure 8.14](#ch08fig14)).'
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你在一个黑暗的夜晚看一个晴朗的天空，你会看到许多不同的星星。每一颗星星都可以被认为是一个簇点：一个局部特征。现在想象一下，天空中最亮的星星周围有更多的星星（在现实中，星星的亮度取决于距离、大小、年龄、放射性等因素）。在这些条件下，最亮的星星是簇中心点；你可以用它们来代表所有点的某种近似。所以，而不是数十亿颗星星（局部特征），你只考虑几十或几百颗星星：圆圈中的中心点。这就是聚类算法所做的（见[图
    8.14](#ch08fig14)）。
- en: Figure 8.14\. Stars, clusters, and centroids
  id: totrans-468
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.14\. 星星、簇和中心点
- en: '![](Images/08fig14_alt.jpg)'
  id: totrans-469
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig14_alt.jpg)'
- en: 'It’s now possible to use LIRE to create image feature vectors using a BOVW
    model. First, you extract local features with SIFT and generate a vocabulary of
    visual words using a clustering algorithm like k-means:'
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 现在可以使用 LIRE 通过 BOVW 模型创建图像特征向量。首先，使用 SIFT 提取局部特征，并使用如 k-means 这样的聚类算法生成视觉词汇表：
- en: '[PRE45]'
  id: totrans-471
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: '***1* Iterates over all the images**'
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 遍历所有图像**'
- en: '***2* Creates a local feature extractor based on the SIFT algorithm**'
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 基于 SIFT 算法创建局部特征提取器**'
- en: '***3* Reads the image contents**'
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 读取图像内容**'
- en: '***4* Performs the SIFT algorithm on the given image**'
  id: totrans-475
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 在给定图像上执行 SIFT 算法**'
- en: '***5* Extracts all the SIFT local features**'
  id: totrans-476
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 提取所有 SIFT 的局部特征**'
- en: '***6* Adds all the SIFT features for the current image as points for clustering**'
  id: totrans-477
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将当前图像的所有 SIFT 特征作为聚类点**'
- en: '***7* Performs k-means clustering for a predefined number of steps**'
  id: totrans-478
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 执行预定义步骤数量的 k-means 聚类**'
- en: '***8* Extracts the generated clusters**'
  id: totrans-479
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 提取生成的簇**'
- en: 'This code computes all the visual words as a fixed number of clusters. With
    the visual vocabulary in place, the local features of each image are compared
    with the cluster centroids to calculate the final value of each visual word. This
    task is performed by the BOVW model, which calculates the Euclidean distance between
    SIFT features and cluster centroids:'
  id: totrans-480
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码将所有视觉词汇计算为固定数量的聚类。有了视觉词汇表，每个图像的局部特征将与聚类中心点进行比较，以计算每个视觉词汇的最终值。这项任务由 BOVW 模型执行，该模型计算
    SIFT 特征与聚类中心点之间的欧几里得距离：
- en: '[PRE46]'
  id: totrans-481
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: '***1* Iterates again over all the images**'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 再次遍历所有图像**'
- en: '***2* Extracts the SIFT local features again. SIFT features can be temporarily
    cached per image in a map to avoid computing them twice.**'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 再次提取 SIFT 的局部特征。SIFT 特征可以按图像临时存储在映射中，以避免重复计算。**'
- en: '***3* Creates a BOVW instance**'
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建一个 BOVW 实例**'
- en: '***4* Computes a single vector representation for the current image, given
    local SIFT features and centroids**'
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 根据局部 SIFT 特征和中心点计算当前图像的单个向量表示**'
- en: '***5* Extracts feature vectors**'
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 提取特征向量**'
- en: This code gives a single feature-vector representation for each image that you
    can use in image searches.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码为每个图像提供一个特征向量表示，可用于图像搜索。
- en: In the examples, global feature extraction uses a simple color histogram extractor,
    and local feature extraction uses SIFT in conjunction with BOVW. These are just
    some of several algorithms that can be used to perform explicit feature extraction.
    For example, for global feature extraction, alternatives include the fuzzy-color
    approach, which is a bit more flexible. For local feature extraction, SURF (mentioned
    earlier) is a variant of SIFT that’s more robust and usually better in terms of
    speed.
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: 在示例中，全局特征提取使用简单的颜色直方图提取器，局部特征提取使用 SIFT 与 BOVW 结合。这些只是可以用来执行显式特征提取的几种算法中的一些。例如，对于全局特征提取，其他选择包括模糊颜色方法，它更加灵活。对于局部特征提取，SURF（前面提到过）是
    SIFT 的一个变种，它更稳健，通常在速度方面表现更好。
- en: The main advantage of the color histogram feature extractor is its simplicity
    and intuitiveness; the main advantage of SIFT, SURF, and other local feature extractors
    is that they perform well for identifying objects in smaller portions of an image
    in a scale- and rotation-invariant way. In practice, a production system needs
    an approach that gives the best guarantees in terms of accuracy, speed, engineering
    effort, and maintenance required to make the entire system work. Once you have
    a feature vector of a fixed dimension representing an image, the indexing and
    search strategies make the most difference in terms of speed, as you’ll see later
    in this chapter. Regarding engineering effort, maintenance, and accuracy, the
    global and local feature extractors discussed so far have been overtaken by DL
    architectures. The central point is that features aren’t manually extracted but
    rather are learned through a deep neural network.
  id: totrans-489
  prefs: []
  type: TYPE_NORMAL
  zh: 颜色直方图特征提取器的主要优势是其简单性和直观性；SIFT、SURF和其他局部特征提取器的主要优势是它们以尺度和旋转不变的方式在图像的较小部分中很好地识别物体。在实践中，一个生产系统需要一个在准确性、速度、工程努力和维护方面提供最佳保证的方法，以确保整个系统正常工作。一旦您有一个代表图像的固定维度的特征向量，索引和搜索策略在速度方面就最为关键，您将在本章后面看到。至于工程努力、维护和准确性，到目前为止所讨论的全局和局部特征提取器已被深度学习架构所超越。关键是特征不是手动提取的，而是通过深度神经网络学习得到的。
- en: In the next section, you’ll see how that makes feature extraction a straightforward
    end-to-end learning process, from pixels to feature vectors. Such DL-generated
    features are also typically better in terms of a semantic understanding of visual
    objects.
  id: totrans-490
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，您将看到这如何使特征提取成为一个从像素到特征向量的简单端到端学习过程。这种深度学习生成的特征在语义理解视觉对象方面通常也更好。
- en: 8.4\. Deep learning for image representation
  id: totrans-491
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.4. 图像表示的深度学习
- en: So far in this chapter, we’ve extracted features from images. Learning representations
    of data is what has made DL so successful in recent years. Computer vision was
    the first field where DL outperformed previous state-of-the-art approaches; in
    computer vision, computers are tasked with recognizing objects in images or videos.
    This can be used in a variety of applications, from retina scans, to identify
    driving violations (such as identifying vehicles overtaking where it’s not permitted),
    optical character recognition, and so on. The technology’s success is driving
    DL researchers and engineers to work on increasingly difficult tasks such as,
    for example, driverless cars.
  id: totrans-492
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们已经从图像中提取了特征。学习数据的表示是近年来深度学习取得成功的关键。计算机视觉是第一个深度学习优于先前最先进方法的领域；在计算机视觉中，计算机的任务是在图像或视频中识别对象。这可以用于各种应用，从视网膜扫描，到识别驾驶违规（如识别在禁止超车的地方超车的车辆），光学字符识别等等。这项技术的成功正在推动深度学习研究人员和工程师致力于越来越困难的任务，例如，例如，自动驾驶汽车。
- en: Some famous results of DL applied to images include LeNet ([http://yann.lecun.com/exdb/lenet](http://yann.lecun.com/exdb/lenet)),
    a neural network that can recognize handwritten and machine-printed digits; and
    AlexNet ([http://mng.bz/6j4y](http://mng.bz/6j4y)), a neural network that can
    recognize objects in an image. AlexNet is particularly interesting for the image
    search scenario, because it was able to categorize (assign a category to) a certain
    image among 1,000 different very fine-grained categories. For example, it can
    differentiate between very similar dogs of different canine breeds, as shown in
    [figure 8.15](#ch08fig15).
  id: totrans-493
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习应用于图像的一些著名结果包括LeNet（[http://yann.lecun.com/exdb/lenet](http://yann.lecun.com/exdb/lenet)），这是一个能够识别手写和机器打印数字的神经网络；以及AlexNet（[http://mng.bz/6j4y](http://mng.bz/6j4y)），这是一个能够识别图像中物体的神经网络。AlexNet对于图像搜索场景尤其有趣，因为它能够在1,000个非常精细的类别中分类（将类别分配给）某个图像。例如，它可以区分不同犬种非常相似的狗，如图8.15所示。
- en: Figure 8.15\. Image of dogs classified by AlexNet
  id: totrans-494
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.15. AlexNet分类的狗的图像
- en: '![](Images/08fig15_alt.jpg)'
  id: totrans-495
  prefs: []
  type: TYPE_IMG
  zh: '![图像](Images/08fig15_alt.jpg)'
- en: Both LeNet and AlexNet use a special kind of feed-forward (artificial) neural
    network called a *convolutional neural network* (CNN or ConvNet). In recent years,
    CNNs have been applied not just to images and videos but also to sound and text;
    they’re very flexible and can be used for a variety of tasks.
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: LeNet和AlexNet都使用一种特殊的**前馈（人工）神经网络**，称为**卷积神经网络**（CNN或ConvNet）。近年来，CNNs不仅应用于图像和视频，还应用于声音和文本；它们非常灵活，可以用于各种任务。
- en: At the beginning of this chapter, I mentioned that you can use DL to find increasingly
    abstract structures in images. Researchers have discovered that this is what CNNs
    do during the training phase. As the number of layers grows, layers closer to
    the input learn raw features like edges and corners, while layers placed toward
    the end of the deep neural network learn features that represent shapes and objects.
    Going forward, you’ll learn the architecture of CNNs, how to train them and set
    them up, and finally, how to extract features for image search (as in [figure
    8.16](#ch08fig16)).
  id: totrans-497
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章开头，我提到你可以使用深度学习来在图像中找到越来越抽象的结构。研究人员发现，这正是CNN在训练阶段所做的事情。随着层数的增加，靠近输入的层学习原始特征，如边缘和角落，而位于深度神经网络末尾的层学习代表形状和物体的特征。接下来，你将学习CNN的架构、如何训练和设置它们，以及最后如何提取图像搜索的特征（如[图8.16](#ch08fig16)所示）。
- en: Figure 8.16\. Indexing images with their features, extracted by a neural network
  id: totrans-498
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.16. 使用神经网络提取的特征对图像进行索引
- en: '![](Images/08fig16_alt.jpg)'
  id: totrans-499
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig16_alt.jpg)'
- en: 8.4.1\. Convolutional neural networks
  id: totrans-500
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.1. 卷积神经网络
- en: 'Despite their names, the connection between artificial neural networks and
    how the human brain works isn’t obvious. Most common neural network architectures
    have a fixed architecture: often neurons are fully connected, whereas the neurons
    in the brain don’t have such fixed (and simple) structures. CNNs were originally
    inspired by how the visual cortex in the human brain works: dedicated cells take
    care of certain portions of the image, passing the information to other cells
    that elaborate the information in a flow similar to the one you’re going to see
    for a CNN. A fundamental difference in how CNNs work with respect to other types
    of neural networks is that they don’t handle *flat signal* inputs (for example,
    dense, one-hot-encoded vectors).'
  id: totrans-501
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管它们的名称如此，人工神经网络与人类大脑工作方式之间的联系并不明显。大多数常见的神经网络架构具有固定的架构：通常神经元是完全连接的，而大脑中的神经元没有这样的固定（且简单）的结构。CNN最初是受人类大脑视觉皮层工作方式的启发：专门的细胞负责图像的某些部分，将信息传递给其他细胞，这些细胞以类似于你将要看到的CNN中的流动方式处理信息。CNN与其他类型神经网络工作方式的一个基本区别是，它们不处理*平坦信号*输入（例如，密集的、独热编码的向量）。
- en: 'When we looked at creating a color histogram for an image, I mentioned that
    images are commonly represented using RGB: a single pixel is described by three
    different values for the red, green, and blue channels. If you extend that to
    an entire image, with many different pixels, you’ll have a representation for
    an image of width *X* and height *Y* consisting of three different matrixes for
    each of the three RGB components, each with *Y* rows and *X* columns. For example,
    an image 3 × 3 pixels in size would have 3 matrixes with 9 values each. An RGB
    code of R:31, G:39, and B:201 would generate the color shown in [figure 8.17](#ch08fig17)
    (visible in the e-book).'
  id: totrans-502
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们讨论为图像创建颜色直方图时，我提到图像通常使用RGB来表示：一个像素由红色、绿色和蓝色通道的三个不同值来描述。如果你将这个概念扩展到整个图像，包含许多不同的像素，你将得到一个宽度为
    *X* 和高度为 *Y* 的图像表示，它由三个不同的矩阵组成，每个矩阵对应三个RGB组件，每个矩阵有 *Y* 行和 *X* 列。例如，一个3 × 3像素大小的图像将会有3个矩阵，每个矩阵有9个值。RGB代码R:31,
    G:39, 和 B:201将生成[图8.17](#ch08fig17)中显示的颜色（在电子书中可见）。
- en: Figure 8.17\. Sample RGB pixel value
  id: totrans-503
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.17. 样本RGB像素值
- en: '![](Images/08fig17.jpg)'
  id: totrans-504
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig17.jpg)'
- en: If you imagine such a value placed in the first element of the second row of
    the 3 × 3 image, the RGB matrixes might look as shown in [tables 8.3](#ch08table03)–[8.5](#ch08table05)
    (the bold values represent the pixel in [figure 8.17](#ch08fig17)).
  id: totrans-505
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你想象这样的值放在3 × 3图像的第二行的第一个元素中，RGB矩阵可能看起来像[表8.3](#ch08table03)–[8.5](#ch08table05)中所示（粗体值代表[图8.17](#ch08fig17)中的像素）。
- en: Table 8.3\. Red channel
  id: totrans-506
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.3. 红色通道
- en: '| 0 | 4 | 0 |'
  id: totrans-507
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 4 | 0 |'
- en: '| **31** | 8 | 3 |'
  id: totrans-508
  prefs: []
  type: TYPE_TB
  zh: '| **31** | 8 | 3 |'
- en: '| 1 | 12 | 39 |'
  id: totrans-509
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 12 | 39 |'
- en: Table 8.4\. Blue channel
  id: totrans-510
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.4. 蓝色通道
- en: '| 10 | 40 | 31 |'
  id: totrans-511
  prefs: []
  type: TYPE_TB
  zh: '| 10 | 40 | 31 |'
- en: '| **39** | 0 | 0 |'
  id: totrans-512
  prefs: []
  type: TYPE_TB
  zh: '| **39** | 0 | 0 |'
- en: '| 87 | 101 | 18 |'
  id: totrans-513
  prefs: []
  type: TYPE_TB
  zh: '| 87 | 101 | 18 |'
- en: Table 8.5\. Green channel
  id: totrans-514
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 表8.5. 绿色通道
- en: '| 37 | 46 | 1 |'
  id: totrans-515
  prefs: []
  type: TYPE_TB
  zh: '| 37 | 46 | 1 |'
- en: '| **201** | 8 | 53 |'
  id: totrans-516
  prefs: []
  type: TYPE_TB
  zh: '| **201** | 8 | 53 |'
- en: '| 0 | 0 | 10 |'
  id: totrans-517
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 0 | 10 |'
- en: Instead of a single matrix of words or character vectors, a neural network needs
    to handle three matrixes for each input image, one per color channel. This poses
    severe performance issues when you’re handling images with conventional feed-forward,
    fully connected neural networks. Very small images of size 100 × 100 would need
    100 * 100 * 3 = 30,000 learnable weights just for the first layer. With a medium-sized
    image (1024 × 768), the first layer would need more than 2,000,000 parameters
    (1024 * 768 * 3 = 2,359,296)!
  id: totrans-518
  prefs: []
  type: TYPE_NORMAL
  zh: 与单个单词矩阵或字符向量矩阵不同，神经网络需要为每个输入图像处理三个矩阵，每个颜色通道一个。当你用传统的前馈、全连接神经网络处理图像时，这会带来严重的性能问题。尺寸为100×100的非常小的图像只需要为第一层学习30,000个可学习的权重！对于中等大小的图像（1024×768），第一层就需要超过2,000,000个参数（1024
    * 768 * 3 = 2,359,296）！
- en: CNNs solve the problem of handling by training over large inputs, adopting a
    lightweight design in layers and neuron connections. Fewer connections means fewer
    weights to be learned by the network. And fewer weights makes learning less computationally
    complex and also faster. Not all neurons in this type of layer are always connected
    to neurons in the preceding layer; such neurons have a *receptive field* of a
    certain configurable size that defines the local region of the input matrixes
    they’re connected to. Therefore, some neurons aren’t connected to the entire input
    region and hence don’t have an attached weight. Such layers called *convolutional
    layers* and are the main building block of CNNs (together with *pooling layers*;
    see [figure 8.18](#ch08fig18)).
  id: totrans-519
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通过在大输入上训练，采用轻量级的设计在层和神经元连接中解决问题。更少的连接意味着网络需要学习的权重更少。更少的权重使得学习过程计算复杂度更低，也更快。在这种类型的层中，并非所有神经元都始终连接到前一层的神经元；这些神经元有一个可配置大小的*感受野*，它定义了它们连接到的输入矩阵的局部区域。因此，一些神经元并没有连接到整个输入区域，因此没有附加的权重。这种层被称为*卷积层*，是CNN的主要构建块（与*池化层*一起；见[图8.18](#ch08fig18)）。
- en: Figure 8.18\. Building blocks (and flow) of CNNs
  id: totrans-520
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.18. CNN的构建块（和流程）
- en: '![](Images/08fig18_alt.jpg)'
  id: totrans-521
  prefs: []
  type: TYPE_IMG
  zh: '![图8.18的替代图片](Images/08fig18_alt.jpg)'
- en: Back when I briefly introduced the SIFT feature extractor, I mentioned the Laplacian
    of Gaussian (LoG) filter, which identifies the interesting points in an image.
    Convolutional layers have that same responsibility; but in contrast with the LoG
    filter, which is fixed, convolutional filters are *learned* during the network
    training phase to best adapt to the images in the training set (see [figure 8.19](#ch08fig19)).
  id: totrans-522
  prefs: []
  type: TYPE_NORMAL
  zh: 在我简要介绍SIFT特征提取器时，我提到了高斯拉普拉斯（LoG）滤波器，它可以识别图像中的有趣点。卷积层有同样的责任；但与固定不变的LoG滤波器不同，卷积滤波器是在网络训练阶段*学习*的，以最佳适应训练集中的图像（见[图8.19](#ch08fig19)）。
- en: Figure 8.19\. Convolutional layer
  id: totrans-523
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.19. 卷积层
- en: '![](Images/08fig19_alt.jpg)'
  id: totrans-524
  prefs: []
  type: TYPE_IMG
  zh: '![图8.19的替代图片](Images/08fig19_alt.jpg)'
- en: Convolutional layers have a configurable depth (4, in [figure 8.19](#ch08fig19)),
    a number of filters, and some other configuration hyperparameters. The layer’s
    filters contain the parameters (weights) that are learned by the network via backpropagation
    during training. You can think of each filter as a small window over the entire
    image that changes the input pixels it’s currently “seeing”; the filter is slid
    over the entire image so that it’s applied to all of the input values. This sliding
    filtering is the convolution operation that gives the name to this type of layer
    (and to the network).
  id: totrans-525
  prefs: []
  type: TYPE_NORMAL
  zh: 卷积层具有可配置的深度（图8.19中的4），滤波器数量和一些其他配置超参数。层的滤波器包含网络在训练期间通过反向传播学习到的参数（权重）。你可以将每个滤波器想象为覆盖整个图像的小窗口，它会改变它当前“看到”的输入像素；滤波器在整个图像上滑动，以便应用于所有输入值。这种滑动滤波就是赋予这种类型层（和网络）名称的卷积操作。
- en: A 5 × 5 filter has 25 weights, so it sees 25 pixels at a time. Mathematically
    speaking, the filter computes the dot product between the 25 values of the pixels
    and the 25 weights of the filter. Suppose a convolutional layer receives a 100
    × 100 × 3 input image (also called an *input volume* because it has 3 dimensions).
    If the layer has 10 filters, the output is a volume of 100 × 100 × 10 values.
    The 10 generated 100 × 100 matrixes (1 for each filter) are called *activation
    maps*.
  id: totrans-526
  prefs: []
  type: TYPE_NORMAL
  zh: 一个5×5的滤波器有25个权重，因此一次可以看到25个像素。从数学上讲，滤波器计算像素的25个值和滤波器的25个权重的点积。假设一个卷积层接收一个100×100×3的输入图像（也称为*输入体积*，因为它有3个维度）。如果该层有10个滤波器，输出是一个100×100×10值的体积。这10个生成的100×100矩阵（每个滤波器一个）被称为*激活图*。
- en: When the filter slides over the input values, it moves one value/pixel at a
    time. But sometimes, the filter can slide two or three values at a time (for example,
    on the width axis) to reduce the number of generated outputs. This parameter for
    the move size is generally called `stride`. Sliding one value at a time is `stride
    = 1`, sliding by two values means `stride = 2`, and so on.
  id: totrans-527
  prefs: []
  type: TYPE_NORMAL
  zh: 当过滤器在输入值上滑动时，它每次移动一个值/像素。但有时，过滤器可以一次移动两个或三个值（例如，在宽度轴上）以减少生成的输出数量。这个移动大小的参数通常称为`stride`。每次移动一个值是`stride
    = 1`，移动两个值意味着`stride = 2`，依此类推。
- en: CNNs also reduce the computational burden of training with large input volumes
    by adopting a way to control the number of weights to be learned. Imagine all
    the neurons in [figure 8.19](#ch08fig19) having a certain depth (for example,
    `depth = 2`). Then they will share the same weights. This technique is called
    *parameter sharing*.
  id: totrans-528
  prefs: []
  type: TYPE_NORMAL
  zh: CNN还通过采用控制需要学习的权重数量的方法来减少使用大输入体积进行训练的计算负担。想象一下[图8.19](#ch08fig19)中的所有神经元都有一定的深度（例如，`depth
    = 2`）。然后它们将共享相同的权重。这种技术称为*参数共享*。
- en: In the end, the primary differences between convolutional layers and normal
    fully connected neural network layers is that convolutional neurons are only connected
    to a local region of the input, and some neurons in a convolutional layer share
    parameters.
  id: totrans-529
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，卷积层和普通全连接神经网络层之间的主要区别是，卷积神经元仅连接到输入的一个局部区域，并且卷积层中的某些神经元共享参数。
- en: Pooling layers
  id: totrans-530
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 池化层
- en: 'A pooling layer’s responsibility is to downsample the input volume: it reduces
    the input size while trying to maintain the most important information. This has
    the advantage of reducing the computational complexity and the number of parameters
    to be learned for successive layers (for example, other convolutional layers).
    Pooling layers aren’t associated with weights to be learned; they look at portions
    of the input volume and extract one or more values, depending on the chosen function.
    Common functions are `max` and `average`.'
  id: totrans-531
  prefs: []
  type: TYPE_NORMAL
  zh: 池化层的职责是对输入体积进行下采样：它在尝试保持最重要的信息的同时减少输入大小。这具有减少计算复杂性和后续层（例如，其他卷积层）需要学习的参数数量的优势。池化层不与需要学习的权重相关联；它们查看输入体积的部分并提取一个或多个值，具体取决于所选函数。常见的函数是`max`和`average`。
- en: Like convolutional layers, pooling layers have a configurable receptive field
    size and `stride`. For example, a pooling layer with a receptive field size of
    2 and `stride` of 2 with a `max` function will take four values from the input
    volume and output the maximum value from those input values.
  id: totrans-532
  prefs: []
  type: TYPE_NORMAL
  zh: 与卷积层一样，池化层具有可配置的感受野大小和`stride`。例如，具有感受野大小为2和`stride`为2的`max`函数池化层将从输入体积中提取四个值，并输出这些输入值中的最大值。
- en: CNN training
  id: totrans-533
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: CNN训练
- en: You’ve learned about the main building blocks of CNNs. Let’s stack them together
    to create an actual CNN and see how such a network is trained. Remember that the
    main goal is to extract feature vectors that capture the notion of semantically
    similar images.
  id: totrans-534
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经了解了CNN的主要构建块。让我们将它们堆叠起来创建一个实际的CNN，并看看这样的网络是如何训练的。记住，主要目标是提取能够捕捉语义相似图像概念的特征向量。
- en: A typical CNN architecture usually involves at least one (or more) convolutional
    layers, followed by
  id: totrans-535
  prefs: []
  type: TYPE_NORMAL
  zh: 典型的CNN架构通常至少包含一个（或更多）卷积层，后面跟着
- en: A dense, fully connected layer to hold the feature vectors for the images
  id: totrans-536
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个密集的、全连接的层来保存图像的特征向量
- en: An output layer containing class scores for each of the classes an image can
    be tagged with
  id: totrans-537
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个输出层，包含图像可以标记的每个类别的类别得分
- en: A CNN is usually trained in a supervised way using training examples whose input
    is an image and whose expected outputs are a set of classes the image belongs
    to.
  id: totrans-538
  prefs: []
  type: TYPE_NORMAL
  zh: CNN通常使用训练示例以监督方式进行训练，这些示例的输入是图像，预期的输出是图像所属的一组类别。
- en: Let’s look at a known dataset that has been used a lot in computer vision research.
    The CIFAR dataset ([www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))
    contains thousands of images labeled with 10 categories (see [figure 8.20](#ch08fig20)).
    Images from the CIFAR dataset are color images, each of which is 32 × 32 pixels
    (very small). The first layer will therefore receive inputs of 32 * 32 * 3 = 3,072
    values.
  id: totrans-539
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看一个在计算机视觉研究中被广泛使用的已知数据集。CIFAR数据集([www.cs.toronto.edu/~kriz/cifar.html](http://www.cs.toronto.edu/~kriz/cifar.html))包含数千张标记为10个类别的图像（见[图8.20](#ch08fig20))。CIFAR数据集中的图像是彩色图像，每个图像为32
    × 32像素（非常小）。因此，第一层将接收32 * 32 * 3 = 3,072个值的输入。
- en: Figure 8.20\. Some examples from the CIFAR dataset
  id: totrans-540
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.20\. CIFAR数据集的一些示例
- en: '![](Images/08fig20_alt.jpg)'
  id: totrans-541
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig20_alt.jpg)'
- en: Let’s create a simple CNN with two convolution + pooling layers, one dense layer,
    and the output layer; see [figure 8.21](#ch08fig21). You expect the network to
    produce an evaluation of the likelihood that the input image belongs to any of
    the 10 categories; [figure 8.22](#ch08fig22) shows some example output (the image
    was generated using the ConvNetJS CIFAR-10 demo at [https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html)).
  id: totrans-542
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们创建一个简单的CNN，包含两个卷积+池化层，一个密集层和输出层；参见[图8.21](#ch08fig21)。你期望网络能够评估输入图像属于10个类别中的任何一个的概率；[图8.22](#ch08fig22)展示了部分示例输出（图像使用ConvNetJS
    CIFAR-10演示生成，演示地址为[https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html](https://cs.stanford.edu/people/karpathy/convnetjs/demo/cifar10.html))。
- en: Figure 8.21\. A simple CNN with two convolution + pooling layers
  id: totrans-543
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.21\. 具有两个卷积+池化层的简单CNN
- en: '![](Images/08fig21_alt.jpg)'
  id: totrans-544
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig21_alt.jpg)'
- en: Figure 8.22\. Testing a CNN on the CIFAR dataset
  id: totrans-545
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.22\. 在CIFAR数据集上测试CNN
- en: '![](Images/08fig22_alt.jpg)'
  id: totrans-546
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig22_alt.jpg)'
- en: As you can see, during CNN training, no feature engineering has to be performed;
    the feature vectors can be drained from the final dense layer, end to end. You
    “just” need lots of images with labels!
  id: totrans-547
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，在CNN训练过程中，无需进行特征工程；特征向量可以直接从最终的密集层中提取，从头到尾。你“只需”需要大量带有标签的图像！
- en: This architecture is a simple example of a CNN. Many things can be changed in
    the fundamental design and in the many hyperparameters. For example, adding more
    convolutional layers has been shown to improve accuracy. The size of the receptive
    field and the depth of convolutional layers or pooling operations (max, average,
    and so on) are all powerful aspects that can be tweaked to improve accuracy.
  id: totrans-548
  prefs: []
  type: TYPE_NORMAL
  zh: 此架构是CNN的一个简单示例。在基本设计和许多超参数中可以进行许多更改。例如，增加更多的卷积层已被证明可以提高准确率。感受野的大小和卷积层或池化操作（最大值、平均值等）的深度都是可以调整以改进准确率的有效方面。
- en: Setting up a CNN in DL4J
  id: totrans-549
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 在DL4J中设置CNN
- en: The CNN from the previous section can easily be implemented in Deeplearning4j.
    DL4J comes with a utility class to iterate and train over the CIFAR dataset, so
    let’s use that to train the CNN.
  id: totrans-550
  prefs: []
  type: TYPE_NORMAL
  zh: 上一节中的CNN可以轻松地在Deeplearning4j中实现。DL4J自带一个实用类，可以迭代并训练CIFAR数据集，因此我们可以使用它来训练CNN。
- en: Listing 8.1\. Setting up a CNN for CIFAR in DL4J
  id: totrans-551
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.1\. 在DL4J中为CIFAR设置CNN
- en: '[PRE47]'
  id: totrans-552
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: '***1* Height of input images**'
  id: totrans-553
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 输入图像的高度**'
- en: '***2* Width of input images**'
  id: totrans-554
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 输入图像的宽度**'
- en: '***3* Number of image channels to be used**'
  id: totrans-555
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* 要使用的图像通道数**'
- en: '***4* Number of training examples to drain from the CIFAR dataset**'
  id: totrans-556
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4* 从CIFAR数据集中提取的训练示例数量**'
- en: '***5* Size of the mini-batch**'
  id: totrans-557
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5* 小批量的大小**'
- en: '***6* Number of epochs to train the network**'
  id: totrans-558
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6* 训练网络的轮数**'
- en: '***7* Sets up the network architecture**'
  id: totrans-559
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**7* 设置网络架构**'
- en: '***8* Creates an iterator over the CIFAR dataset**'
  id: totrans-560
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**8* 创建CIFAR数据集的迭代器**'
- en: '***9* Trains the network**'
  id: totrans-561
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**9* 训练网络**'
- en: '***10* Saves the model for later use**'
  id: totrans-562
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**10* 保存模型以供以后使用**'
- en: The model architecture is defined by the `getSimpleCifarCNN` method, which is
    shown next and in [figure 8.23](#ch08fig23).
  id: totrans-563
  prefs: []
  type: TYPE_NORMAL
  zh: 模型架构由`getSimpleCifarCNN`方法定义，该方法将在下文和[图8.23](#ch08fig23)中展示。
- en: Figure 8.23\. Resulting model from the DL4J UI
  id: totrans-564
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图8.23\. DL4J UI生成的结果模型
- en: '![](Images/08fig23_alt.jpg)'
  id: totrans-565
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/08fig23_alt.jpg)'
- en: Listing 8.2\. Configuring the CNN
  id: totrans-566
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.2\. 配置CNN
- en: '[PRE48]'
  id: totrans-567
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: '***1* First convolution layer**'
  id: totrans-568
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 第一个卷积层**'
- en: '***2* First pooling layer**'
  id: totrans-569
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**2* 第一个池化层**'
- en: '***3* Second convolution layer**'
  id: totrans-570
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**3* 第二个卷积层**'
- en: '***4* Second pooling layer**'
  id: totrans-571
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**4* 第二个池化层**'
- en: '***5* Dense layer (the one from which you’ll extract features)**'
  id: totrans-572
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**5* 密集层（你将从中提取特征的那个）**'
- en: '***6* Output layer**'
  id: totrans-573
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**6* 输出层**'
- en: Once the CNN has finished training, you’re ready to use the network outputs.
  id: totrans-574
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦CNN完成训练，你就可以使用网络输出了。
- en: 'Think back to the color histogram or BOVW models—you obtained a feature vector
    for each image. A CNN gives you more than that: the dense layer close to the output
    layer contains the feature vectors you can use to compare images, and you also
    have a trained CNN that you can use to tag new images.'
  id: totrans-575
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下颜色直方图或BOVW模型——你为每个图像获得了一个特征向量。CNN给你提供了更多：接近输出层的密集层包含了你可以用来比较图像的特征向量，你还有一个可以用来标记新图像的已训练CNN。
- en: After training has finished, if you want to index the feature vectors learned
    for each image by the convolutional neural network, you have to iterate again
    over the image dataset, perform a feed-forward computation for each image, and
    extract the feature vectors generated by the CNN.
  id: totrans-576
  prefs: []
  type: TYPE_NORMAL
  zh: 训练完成后，如果你想索引卷积神经网络为每个图像学习到的特征向量，你必须再次遍历图像数据集，对每个图像执行前向计算，并提取CNN生成的特征向量。
- en: Listing 8.3\. Extracting feature vectors
  id: totrans-577
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.3\. 提取特征向量
- en: '[PRE49]'
  id: totrans-578
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: '***1* Obtains the iterator over the images to be processed**'
  id: totrans-579
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取要处理的图像的迭代器**'
- en: '***2* Iterates over the dataset**'
  id: totrans-580
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历数据集**'
- en: '***3* Iterates over each batch**'
  id: totrans-581
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历每个批次**'
- en: '***4* Iterates over each image from the current batch**'
  id: totrans-582
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 遍历当前批次中的每一张图像**'
- en: '***5* Performs a feed-forward pass, without training, with the current image
    (pixels as input)**'
  id: totrans-583
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对当前图像（像素作为输入）执行不带训练的前向传递**'
- en: '***6* Extracts the image representation stored in the dense layer before the
    final output layer**'
  id: totrans-584
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 提取存储在最终输出层之前密集层的图像表示**'
- en: '***7* Extracts the classification scores for the current image**'
  id: totrans-585
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 提取当前图像的分类得分**'
- en: '***8* Processes (stores) the image feature-vector representation**'
  id: totrans-586
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 处理（存储）图像特征向量表示**'
- en: You’re now ready to learn how you can efficiently index and search the feature
    vectors extracted by a CNN (although this applies generally for any feature vector).
  id: totrans-587
  prefs: []
  type: TYPE_NORMAL
  zh: 你现在可以学习如何高效地索引和搜索由CNN提取的特征向量（尽管这适用于任何特征向量）。
- en: 8.4.2\. Image search
  id: totrans-588
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.2\. 图像搜索
- en: 'Let’s return to the example from the beginning of the chapter: given a picture
    taken with your smartphone camera, you want to find a professional picture to
    use as a card with a gift. You need to do the following:'
  id: totrans-589
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到本章开头的例子：给定用智能手机相机拍摄的照片，你想要找到一张专业照片用作礼物的卡片。你需要做以下事情：
- en: '**1**.  Feed the input image to the CNN.'
  id: totrans-590
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**1**.  将输入图像输入到CNN中。'
- en: '**2**.  Extract the generated feature vectors.'
  id: totrans-591
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**2**.  提取生成的特征向量。'
- en: '**3**.  Use the feature vectors to make a query to find similar images in the
    search engine.'
  id: totrans-592
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '**3**.  使用特征向量在搜索引擎中查询相似图像。'
- en: You saw how to perform the first two steps in the previous section. In this
    section, you’ll learn how to perform the query efficiently.
  id: totrans-593
  prefs: []
  type: TYPE_NORMAL
  zh: 你在前一节中看到了如何执行前两个步骤。在本节中，你将学习如何高效地执行查询。
- en: 'An obvious way to perform the query would be to perform a comparison between
    the input-image feature vectors and the feature vectors of all the images stored
    in the search engine. Imagine extracting the feature vectors from a CNN like the
    one in the previous section, and putting them on a graph: the points represent
    similar images that are close to one another. This is the same line of thought
    we applied to word and document embeddings. So you can compute the distance between
    the feature vector from the input image and the feature vectors of all the other
    images and, for example, return the top 10 images that have the least-distant
    feature vectors. From a computational perspective, this approach won’t scale,
    because the time taken to perform a query grows linearly with the number of images
    in the search engine. In real life, such nearest-neighbor algorithms are often
    approximated: they perform better, but at the cost of accuracy. Such an approximated
    nearest-neighbor search algorithm may not return the exact closest items with
    respect to the input image, but it will still return close neighbors, much more
    quickly.'
  id: totrans-594
  prefs: []
  type: TYPE_NORMAL
  zh: 执行查询的一个明显方法是在输入图像特征向量和存储在搜索引擎中的所有图像特征向量之间进行比较。想象一下从类似于前一节的CNN中提取特征向量，并将它们放在图上：点代表彼此相似的图像。这与我们应用于单词和文档嵌入的思路相同。因此，你可以计算输入图像特征向量与所有其他图像特征向量之间的距离，例如，返回特征向量距离最短的10张图像。从计算的角度来看，这种方法不会扩展，因为执行查询所需的时间会随着搜索引擎中图像数量的线性增长而增长。在现实生活中，这样的最近邻算法通常被近似：它们表现更好，但以精度为代价。这样的近似最近邻搜索算法可能不会返回与输入图像精确最接近的项目，但它仍然会返回接近的邻居，并且速度更快。
- en: In Lucene, you can use the (experimental) `FloatPointNearestNeighbor` class,
    which provides an approximate nearest-neighbor function, or implement an approximate
    nearest-neighbor search using *locality-sensitive hashing* (LSH). `FloatPointNearestNeighbor`
    is more expensive at search time, with no additional space footprint on the index;
    LSH increases the size of the index, because it requires you to store more than
    just the feature vectors, but it’s faster at search time. We’ll start by using
    the `FloatPointNearestNeighbor` class, and then look at LSH.
  id: totrans-595
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lucene中，你可以使用（实验性的）`FloatPointNearestNeighbor`类，它提供了一个近似最近邻函数，或者使用*局部敏感哈希*（LSH）实现近似最近邻搜索。`FloatPointNearestNeighbor`在搜索时成本更高，但不会在索引上增加额外的空间占用；LSH会增加索引的大小，因为它需要你存储不仅仅是特征向量，但在搜索时更快。我们将首先使用`FloatPointNearestNeighbor`类，然后看看LSH。
- en: Using FloatPointNearestNeighbor
  id: totrans-596
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用FloatPointNearestNeighbor
- en: To use `FloatPointNearestNeighbor`, you need to extract the CNN feature vectors
    and index them in Lucene as points. Recent Lucene versions have support for *n*-dimensional
    points (another way to see a vector) based on the *k*-d tree algorithm ([https://en.wikipedia.org/wiki/K-d_tree](https://en.wikipedia.org/wiki/K-d_tree)).
    So the feature vector you extract from the CNN is indexed using a dedicated field
    type called `FloatPoint`.
  id: totrans-597
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用`FloatPointNearestNeighbor`，你需要提取CNN特征向量并将它们在Lucene中作为点索引。最近的Lucene版本支持基于*k*-d树算法（[https://en.wikipedia.org/wiki/K-d_tree](https://en.wikipedia.org/wiki/K-d_tree)）的*n*-维点（另一种看待向量的方式）。因此，从CNN中提取的特征向量使用一个专门的字段类型`FloatPoint`进行索引。
- en: Listing 8.4\. Indexing a feature vector as a point
  id: totrans-598
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.4\. 将特征向量作为点索引
- en: '[PRE50]'
  id: totrans-599
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: '***1* Obtains the feature vector generated by the CNN**'
  id: totrans-600
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取CNN生成的特征向量**'
- en: '***2* Converts it to a float array**'
  id: totrans-601
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将其转换为浮点数组**'
- en: '***3* Indexes the feature vector as a Lucene FloatPoint**'
  id: totrans-602
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将特征向量作为Lucene FloatPoint索引**'
- en: 'Unfortunately, as of Lucene 7, `FloatPoint`s can index points whose dimension
    is at most 8\. Feature vectors are usually much bigger than that: for example,
    our example CNN for CIFAR generates feature vectors whose dimension is 1,024\.
    You’ll need to reduce the `float[]` used to instantiate `FloatPoint` from having
    1,024 values to holding at most 8.'
  id: totrans-603
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，截至Lucene 7，`FloatPoint`s可以索引维度最多为8的点。特征向量通常比这大得多：例如，我们的CIFAR示例CNN生成的特征向量维度是1,024。你需要将用于实例化`FloatPoint`的`float[]`从具有1,024个值减少到最多8个。
- en: You can try to reduce the number of dimensions in vectors while retaining the
    most important information; this technique is also called *dimensionality reduction*.
    There are various dimensionality reduction algorithms, and we’ll look at one that
    you can also reuse in other scenarios.
  id: totrans-604
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以尝试在保留最重要的信息的同时减少向量的维度数；这种技术也称为*降维*。有各种降维算法，我们将看看一个你还可以在其他场景中重用的算法。
- en: A common dimensionality reduction algorithm is *principal component analysis*
    (PCA). As the name suggests, PCA identifies the most important features from a
    feature-vector set and throws away the others. The feature vectors extracted from
    the CNNs have 1,024 values each. You want to use PCA to merge each feature vector’s
    1,024 values into at most 8 different values. With PCA, you transform a point/vector
    on a graph that has 1,024 coordinates into a point/vector on a graph that has
    8 coordinates.
  id: totrans-605
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常见的降维算法是*主成分分析*（PCA）。正如其名所示，PCA从特征向量集中识别最重要的特征，并丢弃其他特征。从CNN中提取的特征向量每个都有1,024个值。你希望使用PCA将每个特征向量的1,024个值合并成最多8个不同的值。使用PCA，你将具有1,024个坐标的图上的点/向量转换成具有8个坐标的图上的点/向量。
- en: Intuitively, a PCA algorithm goes through the values of each feature in each
    vector to find the features whose values differ the most (have the highest variance).
    Such features are considered the most important. PCA doesn’t discard the others;
    rather, it builds new features from them, to avoid losing information. The features
    with the highest variance have more weight when building out the new features.
    PCA will combine the 1,024-sized feature vectors extracted from the CNN into 8
    new features, so that you can index each feature vector as a Lucene point.
  id: totrans-606
  prefs: []
  type: TYPE_NORMAL
  zh: 直观来说，PCA算法会遍历每个向量中每个特征值的值，以找到差异最大（具有最高方差）的特征。这些特征被认为是最重要的。PCA不会丢弃其他特征；相反，它从它们中构建新的特征，以避免信息丢失。在构建新特征时，具有最高方差的特征具有更大的权重。PCA会将从CNN中提取的1,024大小的特征向量组合成8个新特征，这样你就可以将每个特征向量作为Lucene点索引。
- en: PCA can be implemented in several ways; because you’re dealing with vectors,
    you could stack them together in a big matrix and use matrix factorization algorithms,
    such as non-negative matrix factorization, truncated singular value decomposition,
    and others. For the sake of feature-vector indexing, we won’t go into details
    about how such PCA algorithms work, because they’re out of the scope of this book.
    DL4J provides tools to implement PCA, so we’ll use them instead.
  id: totrans-607
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析 (PCA) 可以以多种方式实现；因为你处理的是向量，你可以将它们堆叠在一起形成一个大的矩阵，并使用矩阵分解算法，例如非负矩阵分解、截断奇异值分解等。为了特征向量的索引，我们不会深入探讨此类
    PCA 算法的工作原理，因为它们超出了本书的范围。DL4J 提供了实现 PCA 的工具，因此我们将使用它们。
- en: CIFAR has about 50,000 images of 1,024 dimensions each, so you have a huge matrix
    with 50,000 rows (the number of feature vectors) and 1,024 columns (the feature
    vectors dimension). You want to reduce that to a 50,000 × 8 matrix.
  id: totrans-608
  prefs: []
  type: TYPE_NORMAL
  zh: CIFAR 大约有 50,000 张 1,024 维度的图像，因此你有一个包含 50,000 行（特征向量的数量）和 1,024 列（特征向量维度）的巨大矩阵。你希望将其减少到
    50,000 × 8 的矩阵。
- en: Listing 8.5\. Building the image feature-vectors matrix
  id: totrans-609
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.5\. 构建图像特征向量矩阵
- en: '[PRE51]'
  id: totrans-610
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: '***1* Creates the weights matrix**'
  id: totrans-611
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建权重矩阵**'
- en: '***2* Iterates over the entire (CIFAR) dataset**'
  id: totrans-612
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历整个 (CIFAR) 数据集**'
- en: '***3* Performs feed-forward on the CNN**'
  id: totrans-613
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 对 CNN 执行前馈**'
- en: '***4* Extracts feature vectors from the dense layer**'
  id: totrans-614
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从密集层中提取特征向量**'
- en: '***5* Stores the feature vectors in the weights matrix**'
  id: totrans-615
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将特征向量存储在权重矩阵中**'
- en: With the entire feature-vectors matrix built, you can run PCA and obtain vectors
    that are small enough to be indexed as `FloatPoint`s in Lucene. Note that because
    this matrix is so big, it may take a while (for example, several minutes on a
    modern laptop) for PCA to complete.
  id: totrans-616
  prefs: []
  type: TYPE_NORMAL
  zh: 在构建完整的特征向量矩阵后，你可以运行 PCA 并获得足够小，可以以 `FloatPoint` 的形式索引的向量。请注意，由于这个矩阵非常大，PCA 完成可能需要一段时间（例如，在现代化的笔记本电脑上可能需要几分钟）。
- en: Listing 8.6\. Reducing the vector dimensions to 8
  id: totrans-617
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.6\. 将向量维度减少到 8
- en: '[PRE52]'
  id: totrans-618
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: '***1* Target-vector size**'
  id: totrans-619
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 目标向量大小**'
- en: '***2* Performs PCA on the weights matrix**'
  id: totrans-620
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对权重矩阵执行 PCA**'
- en: 'Although this should work well, you can generate smaller feature vectors of
    better quality by borrowing a technique for compressing word embeddings from the
    paper “Simple and Effective Dimensionality Reduction for Word Embeddings”^([[2](#ch08fn02)])
    and use it for image vectors, too. This technique is based on the combination
    of PCA and a postprocessing algorithm to highlight which features of an embedding
    are “stronger” than the others. The postprocessing algorithm for stronger embeddings
    is described in the paper “All-but-the-Top: Simple and Effective Postprocessing
    for Word Representations.”^([[3](#ch08fn03)]) You can implement the postprocessing
    in DL4J as follows.'
  id: totrans-621
  prefs: []
  type: TYPE_NORMAL
  zh: '虽然这应该效果很好，但你可以通过借鉴来自论文“Simple and Effective Dimensionality Reduction for Word
    Embeddings”^([[2](#ch08fn02)]) 的压缩词嵌入技术来生成更小的、质量更好的特征向量，并将其用于图像向量。这项技术基于 PCA 和后处理算法的组合，以突出嵌入中哪些特征比其他特征“更强”。更强的嵌入的后处理算法在论文“All-but-the-Top:
    Simple and Effective Postprocessing for Word Representations.”^([[3](#ch08fn03)])
    中进行了描述。你可以在 DL4J 中如下实现后处理。'
- en: ²
  id: totrans-622
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-623
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By Vikas Raunak, [https://arxiv.org/abs/1708.03629](https://arxiv.org/abs/1708.03629).
  id: totrans-624
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由 Vikas Raunak 提供，[https://arxiv.org/abs/1708.03629](https://arxiv.org/abs/1708.03629)。
- en: ³
  id: totrans-625
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-626
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: By Jiaqi Mu, Suma Bhat, and Pramod Viswanath, [https://arxiv.org/abs/1702.01417](https://arxiv.org/abs/1702.01417).
  id: totrans-627
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 由 Jiaqi Mu、Suma Bhat 和 Pramod Viswanath 提供，[https://arxiv.org/abs/1702.01417](https://arxiv.org/abs/1702.01417)。
- en: Listing 8.7\. Postprocessing for stronger embeddings
  id: totrans-628
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.7\. 强化嵌入的后处理
- en: '[PRE53]'
  id: totrans-629
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: '***1* Removes the mean values from each embedding in the weights matrix**'
  id: totrans-630
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 从权重矩阵中的每个嵌入中移除平均值**'
- en: '***2* Performs PCA on the resulting weights matrix**'
  id: totrans-631
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 对生成的权重矩阵执行 PCA**'
- en: '***3* Emphasizes each vector’s specific values**'
  id: totrans-632
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 强调每个向量的特定值**'
- en: '***4* Subtracts the principal component values for each vector**'
  id: totrans-633
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 从每个向量中减去主成分值**'
- en: '***5* Returns the modified weights matrix**'
  id: totrans-634
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 返回修改后的权重矩阵**'
- en: The entire algorithm for this modified version of dimensionality reduction for
    embeddings performs postprocessing on the weights matrix, followed by PCA, and
    again followed by postprocessing on the reduced weights matrix.
  id: totrans-635
  prefs: []
  type: TYPE_NORMAL
  zh: 此修改后的嵌入降维算法的整个算法对权重矩阵执行后处理，然后是 PCA，接着是对减少的权重矩阵再次执行后处理。
- en: Listing 8.8\. Dimensionality reduction with postprocessing of embeddings
  id: totrans-636
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.8\. 嵌入的后处理降维
- en: '[PRE54]'
  id: totrans-637
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: '***1* Postprocesses the original feature-vector values**'
  id: totrans-638
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 后处理原始特征向量值**'
- en: '***2* Performs PCA to obtain eight-dimensional feature vectors**'
  id: totrans-639
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 执行主成分分析以获得八维特征向量**'
- en: '***3* Postprocesses the reduced feature-vector values**'
  id: totrans-640
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 后处理降维特征向量的值**'
- en: You can now iterate over the weights matrix and index each row as a `FloatPoint`
    in Lucene.
  id: totrans-641
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在可以遍历权重矩阵，并将每一行作为 Lucene 中的 `FloatPoint` 索引。
- en: Listing 8.9\. Indexing feature vectors
  id: totrans-642
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.9\. 索引特征向量
- en: '[PRE55]'
  id: totrans-643
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '***1* Creates an IndexWriter**'
  id: totrans-644
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建 IndexWriter**'
- en: '***2* Iterates over the rows of the reduced weights matrix**'
  id: totrans-645
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历降维权重矩阵的行**'
- en: '***3* Indexes the vector as a FloatPoint**'
  id: totrans-646
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将向量索引为 FloatPoint**'
- en: '***4* Indexes the label related to the current vector**'
  id: totrans-647
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 索引与当前向量相关的标签**'
- en: '***5* Indexes the document**'
  id: totrans-648
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 索引文档**'
- en: '***6* Commits changes**'
  id: totrans-649
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 提交更改**'
- en: Now that you have images indexed by means of their feature vectors, you can
    query by an example image and find the most similar images in the search engine.
    So, to run some tests, you get a random indexed image, extract its feature vectors,
    and then perform a search using the `FloatPointNearestNeighbor` class.
  id: totrans-650
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您已经通过特征向量对图像进行了索引，您可以通过示例图像进行查询，并在搜索引擎中找到最相似的图像。因此，为了进行一些测试，您获取一个随机索引的图像，提取其特征向量，然后使用
    `FloatPointNearestNeighbor` 类进行搜索。
- en: Listing 8.10\. Nearest-neighbor search
  id: totrans-651
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.10\. 最近邻搜索
- en: '[PRE56]'
  id: totrans-652
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '***1* Gets the document associated with the randomly generated ID**'
  id: totrans-653
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取与随机生成的 ID 相关的文档**'
- en: '***2* Extracts the input-image features, and performs a nearest-neighbor search
    returning the top three results**'
  id: totrans-654
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 提取输入图像的特征，并执行最近邻搜索，返回前三个结果**'
- en: '***3* Iterates over the search results**'
  id: totrans-655
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历搜索结果**'
- en: 'For example, you expect the nearest neighbors of images of dogs to be labeled
    as dogs as well. Here’s some sample output:'
  id: totrans-656
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，您期望狗的图像的最近邻也被标记为狗。以下是一些示例输出：
- en: '[PRE57]'
  id: totrans-657
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: You’ve completed the flow from extracting features, to indexing, and finally
    to searching through images. I mentioned that you can improve the search performance
    by adopting an algorithm called *locality-sensitive hashing*; the next section
    introduces it and looks at one possible implementation in Lucene.
  id: totrans-658
  prefs: []
  type: TYPE_NORMAL
  zh: 您已经完成了从提取特征到索引，最后到通过图像进行搜索的流程。我提到您可以通过采用一种称为 *局部敏感哈希* 的算法来提高搜索性能；下一节将介绍它，并探讨
    Lucene 中的一种可能的实现。
- en: 8.4.3\. Locality-sensitive hashing
  id: totrans-659
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 8.4.3\. 局部敏感哈希
- en: The simplest possible implementation of a *k*-nearest-neighbor algorithm goes
    through all the existing images in the search engine and compares the input-image
    feature vector with each indexed-image feature vector, keeping the *k*-closest
    ones only. Those are the input-nearest neighbors—the search results. This is what
    we implemented in the previous section.
  id: totrans-660
  prefs: []
  type: TYPE_NORMAL
  zh: '*k* 最近邻算法的最简单实现是遍历搜索引擎中的所有现有图像，并将输入图像的特征向量与每个索引图像的特征向量进行比较，只保留最近的 *k* 个。这些是输入最近邻——搜索结果。这是我们之前章节中实现的内容。'
- en: 'Recall the earlier example of stars and clustering: if you plot the image feature
    vectors on a graph and apply a clustering algorithm, you obtain clusters and centroids.
    Each image belongs to a cluster, and each cluster has a centroid, which is the
    center of the cluster. Instead of comparing the input-image feature vectors to
    all the vectors from all the images, you can compare them against only the centroids’
    feature vectors. The number of clusters is usually much smaller than the number
    of points (vectors), so this speeds up the comparison. Once you’ve found the nearest
    cluster, you can decide whether to stop and keep all the other vectors belonging
    to the cluster as nearest neighbors, or perform a second round of nearest-neighbor
    search against the other feature vectors belonging to the nearest cluster.'
  id: totrans-661
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下之前关于星星和聚类的例子：如果您在图上绘制图像特征向量并应用聚类算法，您将获得集群和质心。每个图像都属于一个集群，每个集群都有一个质心，它是集群的中心。您不必将输入图像的特征向量与所有图像的所有向量进行比较，而只需将它们与质心的特征向量进行比较。集群的数量通常比点的数量（向量）少得多，因此这可以加快比较速度。一旦您找到了最近的集群，您就可以决定是否停止并保留属于该集群的所有其他向量作为最近邻，或者对最近集群的其他特征向量进行第二轮最近邻搜索。
- en: This basic idea can be implemented in a number of ways. Of course, you can run
    a k-means clustering algorithm over the feature vectors, and index the centroids
    with a special label (for example, adding a dedicated field that only centroids
    have) so that during search, an initial query is performed to fetch the centroids.
    With the centroids available, one or two executions of an exact or approximate
    nearest-neighbor algorithm can be performed (first over the centroids, and then
    over the nearest cluster points).
  id: totrans-662
  prefs: []
  type: TYPE_NORMAL
  zh: 这个基本想法可以通过多种方式实现。当然，你可以在特征向量上运行k-means聚类算法，并用特殊标签索引质心（例如，添加一个仅质心拥有的专用字段），以便在搜索过程中执行初始查询以获取质心。有了质心，可以执行一个或两个精确或近似最近邻算法的执行（首先在质心上，然后在与最近簇点相关的点上）。
- en: One problem with this is a cluster needs to be maintained and kept up to date;
    as new images are indexed, the cluster and, consequently, the centroids may change.
    This might require you to have to run the clustering algorithm several times.
    The same applies to the dimensionality reduction algorithm required to index small
    vectors.
  id: totrans-663
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方法的一个问题是需要维护并保持集群的更新；随着新图像被索引，集群以及随之而来的质心可能会发生变化。这可能会要求你多次运行聚类算法。同样适用于索引小向量所需的降维算法。
- en: A lighter-weight but nice approach is to use hash functions and hash tables
    to find near-duplicates. Hash functions are just one way that deterministic functions
    can always transform an input into the same output. (It isn’t possible to recover
    the input value from the output value.) The reason to choose hash functions for
    this task is that they’re very good at detecting near-duplicates. When two values
    produce the same output, they cause a *hash collision*. When a hash function is
    applied to several different inputs, and you want to quickly retrieve those inputs,
    they can be collected in a hash table. The nice thing about hash tables is that
    you can retrieve an item via hashing; rather than you having to look for it by
    scrolling through all the items, the hash function tells you its position in the
    hash table.
  id: totrans-664
  prefs: []
  type: TYPE_NORMAL
  zh: 一种更轻量但很不错的做法是使用哈希函数和哈希表来查找近似重复项。哈希函数只是确定性函数始终将输入转换为相同输出的方式之一。（从输出值中无法恢复输入值。）选择哈希函数来完成这项任务的原因是它们非常擅长检测近似重复项。当两个值产生相同的输出时，它们会导致*哈希冲突*。当哈希函数应用于几个不同的输入，并且你想要快速检索这些输入时，它们可以收集在哈希表中。哈希表的好处是你可以通过哈希来检索一个项目；而不是你必须通过滚动所有项目来查找它，哈希函数告诉你它在哈希表中的位置。
- en: With LSH, input-image feature vectors are passed through several different hash
    functions so that similar items map to the same *buckets* (hash tables). Internally,
    the purpose of LSH is to maximize the probability of a hash collision for two
    similar items. When an input image is fed to LSH, it passes its feature vectors
    through several hash functions, and the bucket where the input-image feature vectors
    end up says what images the input image needs to be compared with. This operation
    is just as quick as hashing functions, which are usually fast. Additionally, by
    using special types of hash functions, you can usually map similar inputs into
    the same buckets.
  id: totrans-665
  prefs: []
  type: TYPE_NORMAL
  zh: 在LSH中，输入图像特征向量通过几个不同的哈希函数传递，以便相似项映射到相同的*桶*（哈希表）。内部，LSH的目的是最大化两个相似项发生哈希冲突的概率。当输入图像被输入到LSH中时，它通过几个哈希函数传递其特征向量，输入图像特征向量最终所在的桶表示需要与输入图像进行比较的图像。这个操作与哈希函数一样快，通常很快。此外，通过使用特殊的哈希函数类型，你通常可以将相似的输入映射到相同的桶中。
- en: In Lucene, you implement this approach by creating a dedicated `Analyzer`. The
    LSH `Analyzer` you’re going to build will perform some steps to produce hash values
    or buckets that are stored in the index, just like plain text. So although you
    can use `FloatPoint` fields to work with feature vectors as points in the vector
    space, you can also use Lucene’s text capabilities for LSH. You store hash values
    generated by LSH as plain tokens.
  id: totrans-666
  prefs: []
  type: TYPE_NORMAL
  zh: 在Lucene中，你可以通过创建一个专门的`Analyzer`来实现这种方法。你将要构建的LSH `Analyzer`将执行一些步骤以生成存储在索引中的哈希值或桶，就像纯文本一样。所以尽管你可以使用`FloatPoint`字段来处理特征向量作为向量空间中的点，但你也可以使用Lucene的文本能力来实现LSH。你将LSH生成的哈希值作为普通标记存储。
- en: 'The LSH algorithm will create hashes for portions of a feature vector, as well
    as for the entire feature vector. This is done to maximize the probability of
    matching. First you tokenize the feature vector and extract each feature with
    its position. For example, from the feature vector `<0.1, 0.2, 0.3, 0.4, 0.5>`,
    you’ll obtain the following tokens: 0.1 (position 0), 0.2 (position 1), 0.3 (position
    2), 0.4 (position 3), 0.5 (position 4). You can incorporate the position of each
    token in the token text so that the hash function applied to the token text is
    calculated based on the position of each token. The entire feature vector is also
    kept.'
  id: totrans-667
  prefs: []
  type: TYPE_NORMAL
  zh: LSH算法将为特征向量的部分以及整个特征向量创建哈希值。这是为了最大化匹配的概率。首先，你对特征向量进行分词，并提取每个特征及其位置。例如，从特征向量`<0.1,
    0.2, 0.3, 0.4, 0.5>`中，你会得到以下标记：0.1（位置0），0.2（位置1），0.3（位置2），0.4（位置3），0.5（位置4）。你可以将每个标记的位置包含在标记文本中，这样应用于标记文本的哈希函数就是基于每个标记的位置来计算的。整个特征向量也保留。
- en: 'Then you’ll create ngrams of each individual token: you don’t make hashes of
    the entire vector or single features, but rather of the entire vector and portions
    of it. For example, the bigram of the feature vector `<0.1, 0.2, 0.3, 0.4, 0.5>`
    is `0.1_0.2, 0.2_0.3, 0.3_0.4, 0.4_0.5`.'
  id: totrans-668
  prefs: []
  type: TYPE_NORMAL
  zh: 然后你将为每个单独的标记创建n-gram：你不对整个向量或单个特征创建哈希，而是对整个向量及其部分创建哈希。例如，特征向量`<0.1, 0.2, 0.3,
    0.4, 0.5>`的双语是`0.1_0.2, 0.2_0.3, 0.3_0.4, 0.4_0.5`。
- en: Finally, you’ll apply LSH by using Lucene’s built-in `MinHash` filter. The `MinHash`
    filter applies several hash functions to the terms, generating the corresponding
    hash values.
  id: totrans-669
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，你将通过使用Lucene内置的`MinHash`过滤器来应用LSH。`MinHash`过滤器将多个哈希函数应用于术语，生成相应的哈希值。
- en: Listing 8.11\. The `LSHAnalyzer` class
  id: totrans-670
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.11\. `LSHAnalyzer`类
- en: '[PRE58]'
  id: totrans-671
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '***1* Tokenizes the features of the feature vector**'
  id: totrans-672
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将特征向量的特征进行分词**'
- en: '***2* Attaches the position information to each token**'
  id: totrans-673
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将位置信息附加到每个标记上**'
- en: '***3* Creates feature ngrams**'
  id: totrans-674
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 创建特征n-gram**'
- en: '***4* Applies the LSH filter**'
  id: totrans-675
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 应用LSH过滤器**'
- en: 'To use LSH, you need to use this analyzer (as you’ve seen in other parts of
    this book) at both indexing and search time over the field where you index feature
    vectors. Note that with LSH, you don’t need to reduce the feature vectors as you
    did in the previous section: the feature vectors can be kept as they are (for
    example, 1,024 values) and passed to `LSHAnalyzer`, which creates the feature-vector
    hash values.'
  id: totrans-676
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用LSH，你需要在索引特征向量的字段上，在索引和搜索时都使用这个分析器（如你在本书的其他部分所看到的）。注意，与之前章节中不同，使用LSH时，你不需要像之前那样减少特征向量：特征向量可以保持原样（例如，1,024个值）并传递给`LSHAnalyzer`，它将创建特征向量的哈希值。
- en: As you did before, you configure the `LSHAnalyzer` to be used for the field
    that will host the hash values.
  id: totrans-677
  prefs: []
  type: TYPE_NORMAL
  zh: 如你之前所做的那样，你配置`LSHAnalyzer`用于将哈希值存储的字段。
- en: Listing 8.12\. Configuring `LSHAnalyzer` for the “lsh” field
  id: totrans-678
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.12\. 为“lsh”字段配置`LSHAnalyzer`
- en: '[PRE59]'
  id: totrans-679
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: '***1* Creates a map to contain per-field analyzers**'
  id: totrans-680
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建一个映射来包含每个字段的分词器**'
- en: '***2* Whenever a Document has a field named “lsh”, uses LSHAnalyzer**'
  id: totrans-681
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 当文档有一个名为“lsh”的字段时，使用LSHAnalyzer**'
- en: '***3* Creates a per-field Analyzer**'
  id: totrans-682
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为每个字段创建一个分析器**'
- en: '***4* Creates the indexing configuration with the defined Analyzers**'
  id: totrans-683
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使用定义的分析器创建索引配置**'
- en: '***5* Creates the IndexWriter to index Lucene documents**'
  id: totrans-684
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 创建IndexWriter以索引Lucene文档**'
- en: Once you’ve set up the indexing configuration, you can proceed to index feature
    vectors. Assuming you extracted feature vectors for each image in a matrix (for
    example, called `weights`) where each row has 1,024 columns (the feature values),
    you can index each row in a field called `lsh` that’s processed by `LSHAnalyzer`.
  id: totrans-685
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你设置了索引配置，你就可以继续索引特征向量。假设你从每个图像中提取了特征向量，并将其存储在一个矩阵中（例如，称为`weights`），其中每行有1,024列（特征值），你可以在名为`lsh`的字段中索引每一行，该字段由`LSHAnalyzer`处理。
- en: Listing 8.13\. Index feature vectors in an LSH field
  id: totrans-686
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.13\. 在LSH字段中索引特征向量
- en: '[PRE60]'
  id: totrans-687
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '***1* Iterates over the images by their labels (for example, “dog,” “deer,”
    “car,” and so on for the CIFAR dataset)**'
  id: totrans-688
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 通过标签迭代图像（例如，对于CIFAR数据集，“dog”、“deer”、“car”等）**'
- en: '***2* Gets the feature vector from the weights matrix**'
  id: totrans-689
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 从权重矩阵中获取特征向量**'
- en: '***3* Converts the feature-vector float[] to a String**'
  id: totrans-690
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 将特征向量float[]转换为字符串**'
- en: '***4* Indexes the current image label**'
  id: totrans-691
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 索引当前图像的标签**'
- en: '***5* Indexes the current image feature vector in the “lsh” field**'
  id: totrans-692
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在“lsh”字段中索引当前图像的特征向量**'
- en: '***6* Indexes the document**'
  id: totrans-693
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 索引文档**'
- en: '***7* Persists changes on disk**'
  id: totrans-694
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 在磁盘上持久化更改**'
- en: To query for similar images using LSH, you retrieve the feature vector of the
    query image, extract its token hashes, and run a simple text query using those
    hashes.
  id: totrans-695
  prefs: []
  type: TYPE_NORMAL
  zh: 要使用 LSH 查询相似图像，你需要检索查询图像的特征向量，提取其令牌哈希，并使用这些哈希运行一个简单的文本查询。
- en: Listing 8.14\. Querying using `LSHAnalyzer`
  id: totrans-696
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.14\. 使用 `LSHAnalyzer` 进行查询
- en: '[PRE61]'
  id: totrans-697
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: '***1* Gets the query-image feature-vector String**'
  id: totrans-698
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取查询图像特征向量的字符串**'
- en: '***2* Creates the LSHAnalyzer**'
  id: totrans-699
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 创建 LSHAnalyzer**'
- en: '***3* Gets the token hashes of the feature vector using the LSHAnalyzer**'
  id: totrans-700
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用 LSHAnalyzer 获取特征向量的令牌哈希**'
- en: '***4* Creates a Boolean query**'
  id: totrans-701
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 创建布尔查询**'
- en: '***5* For each token hash, creates a term query (with a constant score)**'
  id: totrans-702
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对于每个令牌哈希，创建一个项查询（具有恒定分数）**'
- en: '***6* Finalizes query creation**'
  id: totrans-703
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 完成查询创建**'
- en: '***7* Runs the LSH query, and takes the three top results**'
  id: totrans-704
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 运行 LSH 查询，并取前三个结果**'
- en: With LSH, you can generally get similar candidates faster than you could by
    querying using a nearest-neighbor search, at the cost of more index space being
    occupied by the feature-vector terms produced by `LSHAnalyzer`. The speed benefits
    of LSH are especially clear when the number of images in the index is very large.
    In addition, reducing the feature-vector dimensions to a small value (such as
    8, as in the previous section) can sometimes be very computationally expensive;
    LSH doesn’t require such preprocessing of feature vectors, so it may be a better
    choice than nearest-neighbor in such scenarios, regardless of the query time.
  id: totrans-705
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 LSH，你通常可以比使用最近邻搜索查询更快地获得相似候选者，但代价是 `LSHAnalyzer` 生成的特征向量术语占用的索引空间更多。当索引中的图像数量非常大时，LSH
    的速度优势尤为明显。此外，将特征向量维度减少到较小的值（如前节中的 8）有时可能非常计算密集；LSH 不需要这样的特征向量预处理，因此在这样的场景中，它可能比最近邻搜索更好，无论查询时间如何。
- en: 8.5\. Working with unlabeled images
  id: totrans-706
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.5\. 处理未标记的图像
- en: In this section, we’ll touch the case where you have a set of unlabeled images
    and you can’t create a training set where each image is tagged with the proper
    classes (like deer, automobile, ship, track, and so on in the CIFAR dataset).
  id: totrans-707
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将讨论你有一组未标记的图像，并且你不能创建一个训练集，其中每个图像都标记了适当的类别（例如，在 CIFAR 数据集中像鹿、汽车、船、轨道等）的情况。
- en: This can be your own set of images you want to be able to search for. As you’ve
    seen in the previous sections, you need to have a vector representation for each
    image to search for it based on its contents. But if your images don’t have labels,
    you can’t generate their feature vectors leveraging the CNN architecture seen
    in the previous sections.
  id: totrans-708
  prefs: []
  type: TYPE_NORMAL
  zh: 这可以是你想要能够搜索的图像集。正如你在前面的章节中看到的，你需要为每个要搜索的图像有一个向量表示，以便根据其内容进行搜索。但是，如果你的图像没有标签，你就不能利用前面章节中看到的
    CNN 架构生成它们的特征向量。
- en: To overcome this problem, you’ll use a type of neural network whose task is
    to learn to encode the input data, usually with a lower dimensionality than the
    original one, and then reconstruct that. Such neural networks, called *autoencoders*,
    are typically built so that one part of the network encodes the input into a vector
    (also known as *latent representation*) with a fixed size, and then this vector
    is transformed back again in the original input data, which is also used as the
    target output. Such autoencoders can be used, for example, to transform an image
    vector into an eight-dimensional vector to allow indexing it as a Lucene `FloatPoint`.
    The part of the autoencoder that transforms the input data into another vector
    with a desired dimensionality (in our case, it may be eight), is called the *encoder*.
    The part of the network that transforms the latent representation back into the
    original data is called the *decoder*. Most commonly, the structure of the encoder
    and the decoder is the same, just mirrored, as you can see in the example of an
    autoencoder in [figure 8.24](#ch08fig24).
  id: totrans-709
  prefs: []
  type: TYPE_NORMAL
  zh: 为了克服这个问题，你将使用一种类型的神经网络，其任务是学习如何编码输入数据，通常比原始数据具有更低的维度，然后重建它。这种称为 *自动编码器* 的神经网络通常构建得使得网络的一部分将输入编码成一个固定大小的向量（也称为
    *潜在表示*），然后这个向量再次转换回原始输入数据，这也被用作目标输出。这种自动编码器可以用来将图像向量转换成一个八维向量，以便将其作为 Lucene `FloatPoint`
    进行索引。将输入数据转换成具有所需维度（在我们的例子中可能是八维）的向量的自动编码器部分称为 *编码器*。将潜在表示转换回原始数据的网络部分称为 *解码器*。最常见的是，编码器和解码器的结构是相同的，只是镜像，正如你在
    [图 8.24](#ch08fig24) 的自动编码器示例中看到的那样。
- en: Figure 8.24\. An autoencoder
  id: totrans-710
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 8.24\. 一个自动编码器
- en: '![](Images/08fig24_alt.jpg)'
  id: totrans-711
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/08fig24_alt.jpg)'
- en: There are many “variations” of autoencoders. For the case of generating a compact
    latent representation of large image vectors, you’ll use a *variational autoencoder*
    (or VAE). A variational autoencoder generates latent representations that follow
    a unit Gaussian distribution.
  id: totrans-712
  prefs: []
  type: TYPE_NORMAL
  zh: 变分自编码器有许多“变体”。对于生成大型图像向量的紧凑潜在表示的情况，你会使用一个*变分自编码器*（或 VAE）。变分自编码器生成的潜在表示遵循单位高斯分布。
- en: To test the usage of an autoencoder with unlabeled data, you’ll still use the
    CIFAR dataset, but you won’t use the classes attached to each image to train the
    network. You’ll instead use them to evaluate whether the search results are good
    after training has finished. But the important part of this approach is that you’ll
    have a way to generate a dense vector representation, like a feature vector, for
    your images, even when they’re not labeled.
  id: totrans-713
  prefs: []
  type: TYPE_NORMAL
  zh: 为了测试使用未标记数据的自编码器的用法，你仍然会使用 CIFAR 数据集，但不会使用附加到每个图像的类别来训练网络。相反，你将使用它们来评估训练完成后搜索结果的好坏。但这种方法的重要部分是，即使图像未标记，你也将有一种方法生成密集的向量表示，如特征向量。
- en: Let’s build a VAE in DL4J with a latent representation of size 8 and two hidden
    layers for both encoder and decoder. The first hidden layer will have 256 neurons,
    and the second one will have 128 neurons.
  id: totrans-714
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在 DL4J 中构建一个具有大小为 8 的潜在表示和两个编码器和解码器隐藏层的 VAE。第一个隐藏层将有 256 个神经元，第二个将有 128 个神经元。
- en: Listing 8.15\. Variational autoencoder configuration
  id: totrans-715
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.15\. 变分自编码器配置
- en: '[PRE62]'
  id: totrans-716
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: '***1* Uses the VAE-specific builder class**'
  id: totrans-717
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用 VAE 特定的构建器类**'
- en: '***2* Defines each hidden layer size for the encoder**'
  id: totrans-718
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 定义编码器每个隐藏层的大小**'
- en: '***3* Defines each hidden layer size for the decoder**'
  id: totrans-719
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 定义解码器每个隐藏层的大小**'
- en: '***4* The size of the input data**'
  id: totrans-720
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 输入数据的大小**'
- en: '***5* The size of the latent representation**'
  id: totrans-721
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 潜在表示的大小**'
- en: You want to use the CIFAR images to train the VAE; however, as discussed in
    previous sections, images are composed of multiple channels. In the case of CIFAR,
    each image is associated with three matrixes of size 32 × 32\. Even if you use
    one single channel, the autoencoder expects a vector, not a matrix. To fix that,
    you have to reshape a 32 × 32 matrix into a 1024-sized vector, which can be done
    by a *reshaping* operation, as shown in the following code. For simplicity, we
    assume using grayscale CIFAR images, so only one channel instead of three.
  id: totrans-722
  prefs: []
  type: TYPE_NORMAL
  zh: 你想使用 CIFAR 图像来训练 VAE；然而，如前几节所述，图像由多个通道组成。在 CIFAR 的情况下，每个图像都与三个大小为 32 × 32 的矩阵相关联。即使你只使用一个通道，自编码器也期望一个向量，而不是一个矩阵。为了解决这个问题，你必须将一个
    32 × 32 的矩阵重塑为一个 1024 大小的向量，这可以通过以下代码中的*重塑*操作来完成。为了简单起见，我们假设使用灰度 CIFAR 图像，因此只有一个通道而不是三个。
- en: Listing 8.16\. Reshaping CIFAR images for ingestion in the variational autoencoder
  id: totrans-723
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.16\. 重塑 CIFAR 图像以供变分自编码器摄取
- en: '[PRE63]'
  id: totrans-724
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: '***1* Reads the CIFAR dataset**'
  id: totrans-725
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 读取 CIFAR 数据集**'
- en: '***2* Stores the reshaped data into a Collection to be used for training the
    VAE**'
  id: totrans-726
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将重塑后的数据存储到集合中，用于训练 VAE**'
- en: '***3* Iterates through the images**'
  id: totrans-727
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历图像**'
- en: '***4* Reshapes the image from 32 × 32 to 1**'
  id: totrans-728
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将图像从 32 × 32 重塑为 1**'
- en: '***5* Adds the reshaped image to the Collection**'
  id: totrans-729
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将重塑的图像添加到集合中**'
- en: Once images have been reshaped, you can feed them into the VAE for training.
  id: totrans-730
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦图像被重塑，你就可以将它们输入到 VAE 中进行训练。
- en: Listing 8.17\. Pretraining the variational autoencoder
  id: totrans-731
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.17\. 预训练变分自编码器
- en: '[PRE64]'
  id: totrans-732
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: '***1* Converts the Collection into a DL4J DataSetIterator**'
  id: totrans-733
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将集合转换为 DL4J DataSetIterator**'
- en: '***2* Trains the VAE for a number of epochs**'
  id: totrans-734
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 训练 VAE 多个时期**'
- en: As soon as training has finished, you can finally index each image latent representation
    into a Lucene index. For the sake of a simpler evaluation, you can also index
    the labels of each image into the search engine. This way, you can compare the
    labels of the query image with the labels of the resulting images.
  id: totrans-735
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，你就可以将每个图像的潜在表示索引到 Lucene 索引中。为了简化评估，你还可以将每个图像的标签索引到搜索引擎中。这样，你可以比较查询图像的标签与结果图像的标签。
- en: Listing 8.18\. Indexing image vectors extracted from VAE
  id: totrans-736
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 8.18\. 从 VAE 中提取的图像向量索引
- en: '[PRE65]'
  id: totrans-737
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: '***1* Gets the VAE to extract image vectors**'
  id: totrans-738
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 获取 VAE 提取图像向量**'
- en: '***2* Iterates through the CIFAR images**'
  id: totrans-739
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 遍历 CIFAR 图像**'
- en: '***3* Gets the label attached to the current image**'
  id: totrans-740
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 获取当前图像附加的标签**'
- en: '***4* Makes the VAE perform a feed-forward pass with the current reshaped image
    as input**'
  id: totrans-741
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 使 VAE 使用当前重塑的图像作为输入进行前向传播**'
- en: '***5* Indexes the document with its image vector and its label**'
  id: totrans-742
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 使用图像向量和其标签索引文档**'
- en: '***6* Stores the extracted features for each image into a List so you can use
    them later for querying**'
  id: totrans-743
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 将每个图像提取的特征存储到列表中，以便您以后可以用于查询**'
- en: With all the images indexed with their latent representation and label, you
    can use Lucene’s `FloatPointNearestNeighbor` to perform a nearest-neighbor search.
    To see whether the results are good without looking at each and every query and
    the resulting image data, you can check if the query and each resulting image
    share the same label.
  id: totrans-744
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有图像及其潜在表示和标签索引后，您可以使用Lucene的`FloatPointNearestNeighbor`进行最近邻搜索。为了在不查看每个查询和结果图像数据的情况下判断结果是否良好，您可以检查查询和每个结果图像是否具有相同的标签。
- en: Listing 8.19\. Querying by image using nearest-neighbor
  id: totrans-745
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表8.19。使用最近邻进行图像查询
- en: '[PRE66]'
  id: totrans-746
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '***1* Picks a random number**'
  id: totrans-747
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 选择一个随机数**'
- en: '***2* Fetches a Document with the random number as its document identifier**'
  id: totrans-748
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 使用随机数作为文档标识符获取文档**'
- en: '***3* Performs a nearest-neighbor search using the image vector associated
    with the document identifier**'
  id: totrans-749
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 使用与文档标识符关联的图像向量执行最近邻搜索**'
- en: '***4* Prints the query image label**'
  id: totrans-750
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 打印查询图像标签**'
- en: '***5* Prints the resulting image document identifier and label**'
  id: totrans-751
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 打印结果图像文档标识符和标签**'
- en: 'We expect query and result images to share the same label most of the times.
    You can check that in the following output:'
  id: totrans-752
  prefs: []
  type: TYPE_NORMAL
  zh: 我们期望查询图像和结果图像在大多数情况下具有相同的标签。您可以在以下输出中检查这一点：
- en: '[PRE67]'
  id: totrans-753
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: As expected, most of the results share the label with the query. Note that you
    can also use the locality-sensitive hashing technique described in the previous
    section instead of nearest-neighbor search.
  id: totrans-754
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，大多数结果与查询共享相同的标签。请注意，您也可以使用上一节中描述的局部敏感哈希技术来代替最近邻搜索。
- en: Summary
  id: totrans-755
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Searching through binary content like images requires learning a representation
    that can capture visual semantics that can be compared across images.
  id: totrans-756
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索二进制内容（如图像）需要学习一种可以捕获图像视觉语义并跨图像进行比较的表示。
- en: Traditional techniques for feature extraction have limits and require significant
    engineering effort.
  id: totrans-757
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征提取的传统技术有限，并且需要大量的工程努力。
- en: Convolutional neural networks are at the core of the recent rise of DL, because
    they can learn image-representation abstractions (edges, shapes, and objects)
    incrementally during network training.
  id: totrans-758
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 卷积神经网络是深度学习最近兴起的核心，因为它们可以在网络训练期间增量地学习图像表示抽象（边缘、形状和对象）。
- en: CNNs can be used to extract feature vectors from images that can be used to
    search for similar images.
  id: totrans-759
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: CNNs可以用来从图像中提取特征向量，这些向量可以用于搜索相似图像。
- en: Locality-sensitive hashing techniques can be used as an alternative to the nearest-neighbor
    approach for image search based on feature vectors.
  id: totrans-760
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 局部敏感哈希技术可以用作基于特征向量的图像搜索的最近邻方法的替代方案。
- en: Autoencoders can help extract image vectors if your images aren’t labeled.
  id: totrans-761
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果您的图像没有标签，自编码器可以帮助提取图像向量。
- en: Chapter 9\. A peek at performance
  id: totrans-762
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第9章。性能一瞥
- en: '*This chapter covers*'
  id: totrans-763
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Setting up DL models in production
  id: totrans-764
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在生产环境中设置DL模型
- en: Optimizing performance and deployment
  id: totrans-765
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 优化性能和部署
- en: Getting real-life neural search systems to work with data streams
  id: totrans-766
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将真实世界的神经搜索系统与数据流相结合
- en: 'After reading the previous eight chapters, you hopefully have gained a broad
    understanding of deep learning and how it can improve search. At this point, you
    should be ready to make the most out of DL when setting up successful search engine
    systems for your users. Along the way, however, you may have wondered about applying
    these ideas to real-world production systems:'
  id: totrans-767
  prefs: []
  type: TYPE_NORMAL
  zh: 在阅读了前面的八章之后，您可能已经对深度学习及其如何改进搜索有了广泛的理解。此时，您应该准备好在为用户设置成功的搜索引擎系统时充分利用深度学习。然而，在旅途中，您可能已经想知道如何将这些想法应用于现实世界的生产系统：
- en: How are these approaches applied in practice in a production scenario?
  id: totrans-768
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这些方法在实际生产场景中是如何应用的？
- en: Will adding these DL algorithms have a serious impact on the time and space
    constraints of your systems?
  id: totrans-769
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 添加这些DL算法会对您系统的时序和空间约束产生严重影响吗？
- en: How big is that impact, and which parts or processes (such as searching versus
    indexing) will be affected?
  id: totrans-770
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这种影响有多大，哪些部分或过程（如搜索与索引）将受到影响？
- en: In this chapter, I’ll address these practical concerns and discuss the considerations
    you’ll need to think about as you apply DL and neural networks to your search
    engine. We’ll look at the performance bits when search engines and neural networks
    work side by side, and I’ll provide some example-driven suggestions for applying
    these DL techniques in practice.
  id: totrans-771
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我将解决这些实际问题，并讨论你在将深度学习（DL）和神经网络应用于搜索引擎时需要考虑的因素。我们将探讨搜索引擎和神经网络并行工作时性能方面的问题，并提供一些基于实例的建议，以在实际应用中应用这些深度学习技术。
- en: 'The previous chapters explored several different search problems DL can help
    solve. If you think about the application of the word2vec model for synonym expansion
    ([chapter 2](kindle_split_013.xhtml#ch02)) or recurrent neural networks to expand
    queries ([chapter 3](kindle_split_015.xhtml#ch03)), you may recall that data flows
    in and out of neural networks and in and out of search engines. We can consider
    a search engine and a neural network as two separate components in a real-world
    software architecture. A neural network needs to be trained to predict accurate
    outputs. At the same time, a search engine must ingest data so that users can
    search for it. To use DL to produce more-effective search results, we need the
    neural network to be effective. These are somewhat conflicting requirements that
    bring up a few logistical questions:'
  id: totrans-772
  prefs: []
  type: TYPE_NORMAL
  zh: 前几章探讨了深度学习可以帮助解决的几个不同的搜索问题。如果你考虑word2vec模型在同义词扩展（[第2章](kindle_split_013.xhtml#ch02)）或循环神经网络在扩展查询（[第3章](kindle_split_015.xhtml#ch03)）中的应用，你可能还记得数据在神经网络和搜索引擎之间流入和流出。我们可以将搜索引擎和神经网络视为现实世界软件架构中的两个独立组件。神经网络需要经过训练以预测准确的输出。同时，搜索引擎必须摄取数据，以便用户可以搜索它。为了使用深度学习产生更有效的搜索结果，我们需要神经网络是有效的。这些是有些冲突的要求，引发了一些物流问题：
- en: Should training happen before indexing?
  id: totrans-773
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练应该在索引之前进行吗？
- en: Or should indexing happen first?
  id: totrans-774
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 或者索引应该先进行吗？
- en: Can you combine those data-feeding tasks?
  id: totrans-775
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你能将这些数据输入任务合并吗？
- en: How do you handle updates to the data?
  id: totrans-776
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你如何处理数据的更新？
- en: We’ll answer some of these questions as we look at the considerations you need
    to take into account when launching real-world deployments of search engines using
    neural networks.
  id: totrans-777
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们查看使用神经网络在现实世界部署搜索引擎时需要考虑的因素时，我们将回答这些问题。
- en: 9.1\. Performance and the promises of deep learning
  id: totrans-778
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.1 深度学习的性能与承诺
- en: 'New DL architectures are published continuously to solve more and more complex
    tasks. We’ve looked at some of them in this book: for example, generating text
    ([chapters 3](kindle_split_015.xhtml#ch03) and [4](kindle_split_016.xhtml#ch04)),
    translating text from one language into another ([chapter 7](kindle_split_020.xhtml#ch07)),
    classifying and representing images based on their contents ([chapter 8](kindle_split_021.xhtml#ch08)),
    and more. Not just entire models, but also new types of activation functions,
    cost functions, backpropagation algorithm optimizations, weight-initialization
    schemes, and others are constantly being researched and published.'
  id: totrans-779
  prefs: []
  type: TYPE_NORMAL
  zh: 持续发布新的深度学习架构来解决越来越多和越来越复杂的任务。我们在本书中探讨了一些：例如，生成文本（[第3章](kindle_split_015.xhtml#ch03)和[第4章](kindle_split_016.xhtml#ch04)）、将一种语言翻译成另一种语言（[第7章](kindle_split_020.xhtml#ch07)）、根据内容对图像进行分类和表示（[第8章](kindle_split_021.xhtml#ch08)）等等。不仅整个模型，而且新的激活函数类型、损失函数、反向传播算法优化、权重初始化方案等也在不断研究和发布。
- en: 'The DL concepts introduced in this book apply to recent-past, current, and
    (hopefully) newer neural network architectures. If you’re responsible for a search
    engine infrastructure, you’ll probably look for approaches that researchers have
    demonstrated work best for a specific task (also known as *state of the art*).
    For example, think about machine translation or image search: at the time of writing,
    the state of the art for machine translation is represented by sequence-to-sequence
    models, such as encoder-decoder networks with attention.^([[1](#ch09fn01)]) So
    you’d want to implement those state-of-the-art models, and you’d expect them to
    give you good results like those you can read about in the related research papers.
    In those cases, the first challenge is to reproduce the model described in the
    paper and then to make it work effectively on your data and infrastructure. To
    do so,'
  id: totrans-780
  prefs: []
  type: TYPE_NORMAL
  zh: 本书介绍的深度学习概念适用于最近、当前以及（希望）更新的神经网络架构。如果你负责搜索引擎基础设施，你可能会寻找研究人员已经证明对特定任务（也称为**最佳实践**）效果最好的方法。例如，考虑机器翻译或图像搜索：在撰写本文时，机器翻译的最佳实践由序列到序列模型代表，例如具有注意力的编码器-解码器网络。[^([1](#ch09fn01))]
    因此，你希望实现这些最佳实践模型，并期望它们能给你带来像你在相关研究论文中读到的那样好的结果。在这些情况下，第一个挑战是重现论文中描述的模型，然后使其在你的数据和基础设施上有效运行。为此，
- en: ¹
  id: totrans-781
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ¹
- en: ''
  id: totrans-782
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See this recent research that even discards RNNs: Ashish Vaswani et al., “Attention
    Is All You Need,” [http://mng.bz/nQZK](http://mng.bz/nQZK).'
  id: totrans-783
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参考这篇最近的研究，甚至摒弃了RNN：Ashish Vaswani等人，“Attention Is All You Need”，[http://mng.bz/nQZK](http://mng.bz/nQZK)。
- en: The neural network must provide accurate results.
  id: totrans-784
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络必须提供准确的结果。
- en: The neural network must provide results quickly.
  id: totrans-785
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络必须快速提供结果。
- en: The software and hardware must be adequate for the computational load, in terms
    of time and space (and, remember, training is costly).
  id: totrans-786
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 软件和硬件必须能够满足计算负载，无论是时间还是空间（记住，训练是昂贵的）。
- en: In the next section, we’ll run through the entire process of implementing a
    neural network model to solve a specific task and see what common steps you may
    have to take along the way to solve these challenges.
  id: totrans-787
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将回顾实现神经网络模型以解决特定任务的整个过程，并看看在解决这些挑战的过程中你可能需要采取哪些常见步骤。
- en: 9.1.1\. From model design to production
  id: totrans-788
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 9.1.1. 从模型设计到生产
- en: In [chapter 8](kindle_split_021.xhtml#ch08), you saw convolutional neural networks
    in action, classifying images. Once training was finished, you used the network
    to extract feature vectors to be indexed and searched by the search engine. But
    we didn’t consider the accuracy of the neural network classifications. Let’s now
    track some numbers for accuracy, training, and prediction times on the road to
    building a good neural network model to use in conjunction with a search engine.
    We’ll return to the CIFAR dataset we used in [chapter 8](kindle_split_021.xhtml#ch08)
    and see how to gradually adjust the neural network model to improve accuracy while
    keeping reasonable training timings; we’ll go through it step by step, as you
    would in your own project.
  id: totrans-789
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第8章](kindle_split_021.xhtml#ch08)中，你看到了卷积神经网络在分类图像中的应用。一旦训练完成，你使用该网络提取特征向量以供搜索引擎索引和搜索。但我们没有考虑神经网络的分类准确性。现在，让我们跟踪一些关于准确性、训练和预测时间的数字，以便在构建用于搜索引擎的神经网络模型的过程中，我们将回到[第8章](kindle_split_021.xhtml#ch08)中使用的CIFAR数据集，并看看如何逐步调整神经网络模型以提高准确性，同时保持合理的训练时间；我们将一步一步地进行，就像你在自己的项目中做的那样。
- en: Indexing is usually costly with real-world data. CIFAR is only a few tens of
    thousands of images, but many live deployments have to index hundreds of thousands,
    millions, or billions of images or documents. If you index 100 million images
    with their feature vectors, you don’t want to have to repeat the process as you
    might need to if the feature vectors don’t accurately reflect the image contents,
    so the user experience isn’t great. So you’ll usually run a few experiments and
    evaluations before indexing feature vectors.
  id: totrans-790
  prefs: []
  type: TYPE_NORMAL
  zh: 在实际数据中，索引通常成本高昂。CIFAR只有几万张图片，但许多实际部署需要索引数十万、数百万甚至数十亿张图片或文档。如果你用特征向量索引了1亿张图片，你不想重复这个过程，因为你可能需要这样做，如果特征向量没有准确反映图片内容，用户体验就不会很好。因此，你通常会运行一些实验和评估，然后再索引特征向量。
- en: 'Let’s start with a convolutional neural network (CNN) similar to one of the
    first CNN-based architectures that achieved good results for categorizing images:
    the LeNet architecture ([http://yann.lecun.com/exdb/lenet](http://yann.lecun.com/exdb/lenet)).
    This is a simple CNN similar to the one you set up in [chapter 8](kindle_split_021.xhtml#ch08),
    but with slightly different configuration parameters for convolution depth, receptive-field
    size, stride, and dense-layer dimensionality (see [figure 9.1](#ch09fig01)).'
  id: totrans-791
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从与第一个基于CNN的架构之一相似的卷积神经网络（CNN）开始，该架构在图像分类方面取得了良好的结果：LeNet架构([http://yann.lecun.com/exdb/lenet](http://yann.lecun.com/exdb/lenet))。这是一个简单的CNN，类似于你在第8章中设置的CNN，但具有不同的卷积深度、感受野大小、步长和密集层维度的配置参数（见图9.1）。
- en: Figure 9.1\. Example LeNet model
  id: totrans-792
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.1. LeNet示例模型
- en: '![](Images/09fig01_alt.jpg)'
  id: totrans-793
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig01_alt.jpg)'
- en: The model contains two sequences of convolutional layers followed by a max pooling
    layer and a fully connected layer. The filters are size 5 × 5, the first convolutional
    layer’s depth is 28, and the second convolutional layer’s depth is 10\. The dense
    layer is size 500\. The max pooling layers have `stride` equal to 2.
  id: totrans-794
  prefs: []
  type: TYPE_NORMAL
  zh: 该模型包含两个卷积层序列，后面跟着一个最大池化层和一个全连接层。过滤器大小为5 × 5，第一个卷积层的深度为28，第二个卷积层的深度为10。密集层的大小为500。最大池化层的步长为2。
- en: Listing 9.1\. LeNet type of model
  id: totrans-795
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.1. LeNet类型的模型
- en: '[PRE68]'
  id: totrans-796
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: This model is old, so you shouldn’t expect it to perform too well, but it’s
    a good practice to start with a small model and see how far it gets.
  id: totrans-797
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型比较老，所以你不应该期望它表现得太好，但一个好的做法是从一个小模型开始，看看它能走多远。
- en: You’ll train over 2,000 examples from the CIFAR dataset at first, to get some
    quick feedback about how good the model parameters are. If the model begins to
    diverge too soon, you can avoid loading huge training sets before discovering
    it.
  id: totrans-798
  prefs: []
  type: TYPE_NORMAL
  zh: 你首先将在CIFAR数据集上训练2,000个示例，以快速了解模型参数有多好。如果模型开始过早地发散，你可以在发现之前避免加载巨大的训练集。
- en: Listing 9.2\. Training over 2,000 samples from CIFAR
  id: totrans-799
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.2. 在CIFAR上训练2,000个样本
- en: '[PRE69]'
  id: totrans-800
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '***1* You use only 2,000 random samples from the CIFAR dataset.**'
  id: totrans-801
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 你只使用CIFAR数据集的2,000个随机样本。**'
- en: Model evaluation
  id: totrans-802
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 模型评估
- en: To monitor how well the neural network learns to categorize the images, you’ll
    monitor the training process with the DL4J UI. In the best possible case, you’d
    see the score steadily decrease towards 0, but in this case, as shown in [figure
    9.2](#ch09fig02), it decreases very slowly without ever reaching a point close
    to 0\. Recall that the score is a measure of the amount of error the neural network
    commits when trying to predict the classes for each input image. So, with these
    stats, you don’t expect it to perform very well.
  id: totrans-803
  prefs: []
  type: TYPE_NORMAL
  zh: 为了监控神经网络学习对图像进行分类的效果，你将使用DL4J UI监控训练过程。在最佳情况下，你会看到分数稳步下降到0，但在这个案例中，如图9.2所示，它下降得非常缓慢，从未接近0。回想一下，分数是衡量神经网络在尝试预测每个输入图像的类别时犯错的数量的度量。因此，有了这些统计数据，你不会期望它表现得太好。
- en: Figure 9.2\. LeNet training
  id: totrans-804
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.2. LeNet训练
- en: '![](Images/09fig02_alt.jpg)'
  id: totrans-805
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig02_alt.jpg)'
- en: To evaluate the accuracy of predictions for a machine learning model, it’s always
    a good practice to separate a collection of data used for training (the training
    set) from a collection of data to be used for testing the quality of a model (the
    test set). During training, the model may *overfit* the data and therefore give
    good accuracy on the training set while being unable to generalize well over slightly
    different data, so using a test set helps in finding out how well a model can
    work on data that it hasn’t previously trained with.
  id: totrans-806
  prefs: []
  type: TYPE_NORMAL
  zh: 为了评估机器学习模型的预测准确性，将用于训练的数据集（训练集）与用于测试模型质量的数据集（测试集）分开，始终是一个好的做法。在训练过程中，模型可能会对数据进行过度拟合，因此在对训练集有很好的准确率的同时，却无法很好地泛化到稍微不同的数据上，所以使用测试集有助于找出模型在没有之前训练过的数据上能工作得有多好。
- en: A separate iterator over a different set of images can be created and passed
    to DL4J tools for evaluation.
  id: totrans-807
  prefs: []
  type: TYPE_NORMAL
  zh: 可以创建一个针对不同图像集的单独迭代器，并将其传递给DL4J工具进行评估。
- en: Listing 9.3\. Model evaluation with DL4J
  id: totrans-808
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.3. 使用DL4J进行模型评估
- en: '[PRE70]'
  id: totrans-809
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '***1* Creates a test set iterator**'
  id: totrans-810
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 创建测试集迭代器**'
- en: '***2* Instantiates the DL4J evaluation tool**'
  id: totrans-811
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 实例化DL4J评估工具**'
- en: '***3* Iterates over the test dataset**'
  id: totrans-812
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 遍历测试数据集**'
- en: '***4* Fetches the next mini-batch of data (100 examples, in this case)**'
  id: totrans-813
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 获取下一个迷你批次的数据（在这个案例中是100个示例）**'
- en: '***5* Performs prediction over the current batch**'
  id: totrans-814
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 在当前批次上进行预测**'
- en: '***6* Performs evaluation using the actual output and the CIFAR output labels**'
  id: totrans-815
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 使用实际输出和 CIFAR 输出标签进行评估**'
- en: '***7* Prints the statistics on the standard output**'
  id: totrans-816
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 在标准输出上打印统计信息**'
- en: 'The evaluation stats include metrics like accuracy, precision, recall F1 score,
    and confusion matrix (the *F1 score* is a measure whose value ranges between 0
    and 1 and which takes into account precision and recall):'
  id: totrans-817
  prefs: []
  type: TYPE_NORMAL
  zh: 评估统计数据包括准确率、精确率、召回率 F1 分数和混淆矩阵（*F1 分数*是一个值介于 0 和 1 之间的度量，它考虑了精确率和召回率）：
- en: '[PRE71]'
  id: totrans-818
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: In the confusion matrix, you can see that for the class `airplane` in the first
    row, 31 samples have been correctly assigned to the `airplane` class, but about
    the same number of predictions (26) were assigned the incorrect class `ship` for
    an `airplane` image. Ideally, a confusion matrix will contain high values on the
    right diagonal and low values everywhere else.
  id: totrans-819
  prefs: []
  type: TYPE_NORMAL
  zh: 在混淆矩阵中，你可以看到在第一行中，对于`airplane`类别，有 31 个样本被正确分配到`airplane`类别，但大约相同数量的预测（26 个）将一个`airplane`图像错误地分配到了错误的类别`ship`。理想情况下，混淆矩阵将在右对角线上包含高值，而在其他地方包含低值。
- en: 'Changing the `numSamples` value to `5000` and performing training and evaluation
    again, you expect better results:'
  id: totrans-820
  prefs: []
  type: TYPE_NORMAL
  zh: 将`numSamples`值更改为`5000`并再次进行训练和评估，你期望得到更好的结果：
- en: '[PRE72]'
  id: totrans-821
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: The F1 score went up by 9% (0.30 versus 0.21), which is a big step forward,
    but getting good results only about 30% of the time wouldn’t be appropriate in
    production.
  id: totrans-822
  prefs: []
  type: TYPE_NORMAL
  zh: F1 分数提高了 9%（从 0.30 提高到 0.21），这是一个很大的进步，但在生产中，只有大约 30% 的时间得到好结果是不合适的。
- en: 'You may recall that neural network training uses the backpropagation algorithm
    (eventually with variations, depending on the specific architecture, such as backpropagation
    through time for recurrent neural networks). The backpropagation algorithm aims
    to reduce the prediction error committed by the network by adjusting the weights
    so that the overall error rate decreases. At some point, the algorithm will find
    a set of weights (such as the weights attached to connections between neurons
    in different layers) with the lowest possible error, but this may take a long
    time, depending on features of the data used for training:'
  id: totrans-823
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能还记得，神经网络训练使用反向传播算法（最终会根据具体架构有所变化，例如循环神经网络的时间反向传播）。反向传播算法旨在通过调整权重来减少网络犯下的预测错误，从而使整体错误率降低。在某个时刻，算法将找到一组权重（例如，连接不同层神经元之间的权重）以实现最低可能的错误，但这可能需要很长时间，具体取决于用于训练的数据特征：
- en: '***Diversity in training examples—*** Some text is written in formal language,
    and other text is written in slang. Or some images are pictures taken during daylight,
    and others were taken at night.'
  id: totrans-824
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***训练示例的多样性—*** 一些文本是用正式语言编写的，而其他文本则使用俚语。或者有些图像是在白天拍摄的，而其他图像是在夜间拍摄的。'
- en: '***Noise in the training examples—*** Some text has typos or grammatical errors.
    Or some images are of poor quality or contain watermarks or other types of noise
    that makes training more difficult.'
  id: totrans-825
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***训练示例中的噪声—*** 一些文本有拼写错误或语法错误。或者有些图像质量较差或包含水印或其他类型的噪声，这使得训练更加困难。'
- en: 'The ability of neural network training to converge to a good set of weights
    also depends a great deal on the tuning parameters, such as the *learning rate*:
    I already mentioned this, but it’s worth repeating that this is a fundamental
    aspect to get right. A learning rate that’s too high will make training fail,
    and a learning rate that’s too low will make training take too long to converge
    to a good set of weights.'
  id: totrans-826
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络训练收敛到一组良好权重的能 力在很大程度上也取决于调整参数，例如*学习率*：我已经提到过这一点，但值得重复的是，这是正确设置的一个基本方面。学习率过高会导致训练失败，而学习率过低会使训练花费太长时间才能收敛到一组良好的权重。
- en: '[Figure 9.3](#ch09fig03) shows the training loss of the same neural network
    but with different learning rates. You can clearly see that both learning rates
    converge over time to the same set of weights. Learning begins at time t0; let’s
    consider what will happen if you stop training at time t1 or t2\. If you stop
    training after a small number of iterations (before time t1), you’ll exclude the
    high learning rate, because it will make the loss increase instead of decrease.
    If you stop training at time t2, you’ll instead discard the low learning rate,
    because it will keep the same score as the high learning rate, or begin to increase.
    It’s therefore a good idea to come up with a few possible architectures with reasonable
    parameter settings and run some experiments.'
  id: totrans-827
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 9.3](#ch09fig03) 展示了相同神经网络但使用不同学习率的训练损失。你可以清楚地看到，两个学习率都会随着时间的推移收敛到同一组权重。学习从时间
    t0 开始；让我们考虑如果你在时间 t1 或 t2 停止训练会发生什么。如果你在迭代次数很少时停止训练（在时间 t1 之前），你会排除高学习率，因为它会使损失增加而不是减少。如果你在时间
    t2 停止训练，你将丢弃低学习率，因为它会保持与高学习率相同的分数，或者开始增加。因此，提出几个具有合理参数设置的可能的架构并运行一些实验是个好主意。'
- en: Figure 9.3\. Loss plotted for the same neural networks but trained with different
    learning rates
  id: totrans-828
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 9.3\. 不同学习率训练的相同神经网络的损失曲线
- en: '![](Images/09fig03_alt.jpg)'
  id: totrans-829
  prefs: []
  type: TYPE_IMG
  zh: '![图片 9.3](Images/09fig03_alt.jpg)'
- en: In DL4J `Updater` implementations, you can set the learning rate for your neural
    network.
  id: totrans-830
  prefs: []
  type: TYPE_NORMAL
  zh: 在 DL4J 的 `Updater` 实现中，你可以为你的神经网络设置学习率。
- en: Listing 9.4\. Setting the learning rate
  id: totrans-831
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 9.4\. 设置学习率
- en: '[PRE73]'
  id: totrans-832
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: '***1* Sets the learning rate to 0.01**'
  id: totrans-833
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将学习率设置为 0.01**'
- en: Adding more weights
  id: totrans-834
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 添加更多权重
- en: Using more weights to be learned is likely to cause training to take more time
    and resources; a common mistake is to add layers or increase the size of layers
    as much as possible. But adding layers can help when the network doesn’t have
    enough training power to fit over lots of different training examples, such as
    when the number of weights is far less than the number of examples, and the neural
    network is having a hard time converging to a good set of weights (perhaps the
    score doesn’t go below a certain value).
  id: totrans-835
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更多需要学习的权重可能会导致训练需要更多的时间和资源；一个常见的错误是尽可能多地添加层或增加层的尺寸。但是，当网络没有足够的训练能力来适应大量的不同训练示例时，添加层可能会有所帮助，例如当权重的数量远少于示例数量，神经网络难以收敛到一组好的权重（可能分数不会低于某个值）。
- en: 'The code defined in the previous sections trained a relatively lightweight
    CNN with 5,000 examples. Let’s see what happens if you make the convolutional
    layers deeper (depth of 96 and 256, respectively). The training time for 5,000
    examples increases from 10 minutes to 1 hour, with the following evaluation stats:'
  id: totrans-836
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中定义的代码训练了一个相对轻量级的 CNN，有 5,000 个示例。让我们看看如果你使卷积层更深（深度分别为 96 和 256）会发生什么。5,000
    个示例的训练时间从 10 分钟增加到 1 小时，以下是一些评估统计数据：
- en: '[PRE74]'
  id: totrans-837
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: In this case, it wasn’t worth adding more power to the network.
  id: totrans-838
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，增加网络的功率并不值得。
- en: 'Working with deep neural networks in production requires some experience, but
    it isn’t magic. The number of weights to be learned is an important factor: the
    number of data points in the training set should always be less than the number
    of weights. Possible consequences of not following this rule are overfitting and
    difficulties in converging.'
  id: totrans-839
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中使用深度神经网络需要一些经验，但这并不是魔法。要学习的权重量是一个重要因素：训练集中的数据点数量应该始终少于权重的数量。不遵循此规则的可能后果是过拟合和收敛困难。
- en: 'Let’s do some reasoning about the data. You have tiny images of 32 × 32 pixels.
    CNNs learn features over time with convolutional layers while downsampling with
    pooling layers. Maybe it would help to give the initial convolution layer a few
    more weights but give the pooling layer a `stride` value of 2 instead of 1\. You
    expect training the network to achieve slightly better results in less time:'
  id: totrans-840
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对数据进行一些推理。你有一些 32 × 32 像素的微小图像。CNN 通过卷积层在时间上学习特征，同时通过池化层进行下采样。也许给初始卷积层添加更多权重，但将池化层的
    `stride` 值设置为 2 而不是 1 会有所帮助。你期望训练网络在更短的时间内达到略微更好的结果：
- en: '[PRE75]'
  id: totrans-841
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: Training finishes in 5 minutes instead of 7, thanks to the change to the pooling
    layer, and the result quality also improved. This may not seem like much, but
    it would make a real difference when training over the entire dataset.
  id: totrans-842
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对池化层的更改，训练时间缩短到 5 分钟而不是 7 分钟，并且结果质量也得到了提升。这可能看起来不多，但在整个数据集上进行训练时会有真正的区别。
- en: Training with more data
  id: totrans-843
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 使用更多数据进行训练
- en: So far, you’ve performed experiments with only a few examples from the CIFAR
    dataset. To better understand how well the CNN model can work, you need to train
    it with more data.
  id: totrans-844
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你只使用CIFAR数据集的几个示例进行了实验。为了更好地理解CNN模型可以工作得有多好，你需要用更多数据进行训练。
- en: 'You have more than 50,000 images in CIFAR: you should split the dataset in
    such a way that most of it is used for training, but many images are available
    to perform evaluation.'
  id: totrans-845
  prefs: []
  type: TYPE_NORMAL
  zh: 你在CIFAR中有超过50,000张图像：你应该以这种方式分割数据集，即大部分用于训练，但有许多图像可用于评估。
- en: Before using the full dataset, it’s important to note the time taken by training
    with respect to the available hardware and requirements of a production scenario.
    The first iteration of training for 10 epochs with 2,000 images took 3 minutes
    on a normal laptop; 5,000 images training for 10 epochs took 7 minutes. These
    are acceptable times for experiments where you want quick feedback, but training
    over the full dataset for several epochs may take hours—time that would be better
    used if you knew in advance what to change.
  id: totrans-846
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用全数据集之前，重要的是要注意与可用硬件和实际场景要求相关的训练时间。使用2,000张图像进行10个周期的第一次训练在普通笔记本电脑上花费了3分钟；5,000张图像进行10个周期的训练花费了7分钟。对于需要快速反馈的实验来说，这些时间是可接受的，但跨越整个数据集进行几个周期的训练可能需要数小时——如果事先知道要更改什么，这些时间会更好利用。
- en: 'Now, let’s run the current settings over 50,000 images for training and 10,000
    for evaluation. You expect better evaluation metric results and a lower score
    at the end of training:'
  id: totrans-847
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们用当前设置在50,000张图像上进行训练，10,000张图像进行评估。你期望更好的评估指标结果，并在训练结束时得到更低的分数：
- en: '[PRE76]'
  id: totrans-848
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'After training on almost the entire training set, you got a 0.41 F1 score after
    almost 3 hours (on a normal laptop). You can’t yet be satisfied with the accuracy
    of the model: it would make errors 59% of the time.'
  id: totrans-849
  prefs: []
  type: TYPE_NORMAL
  zh: 在对几乎整个训练集进行训练后，经过近3小时（在普通笔记本电脑上）的训练，你得到了0.41的F1分数。你还不应对模型的准确性感到满意：它会有59%的时间出错。
- en: In this case, it’s useful to look at the loss curve, shown in [figure 9.4](#ch09fig04).
    The curve is decreasing and might keep doing so if you had more data. Unfortunately,
    for this case you don’t, unless you use a smaller test set.
  id: totrans-850
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，查看损失曲线是有用的，如图9.4所示。[图9.4](#ch09fig04)。曲线在下降，如果你有更多数据，它可能会继续下降。不幸的是，对于这个案例，你没有更多数据，除非你使用更小的测试集。
- en: Figure 9.4\. CNN full training loss graph
  id: totrans-851
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.4\. CNN完整训练损失图
- en: '![](Images/09fig04_alt.jpg)'
  id: totrans-852
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig04_alt.jpg)'
- en: Adjusting batch size
  id: totrans-853
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 调整批量大小
- en: 'One thing you can look into when you have such curves is whether you’re using
    the wrong size for the `batch` parameter. A *batch* or *mini-batch* is a collection
    of training examples that are put together and fed to the neural network as a
    single batched input. For example, instead of feeding one image at a time, and
    thus an input volume (a set of stacked matrixes) at a time, you can squash a number
    of input volumes together. Doing so usually has two consequences:'
  id: totrans-854
  prefs: []
  type: TYPE_NORMAL
  zh: 当你有这样的曲线时，你可以考虑的是你是否使用了错误的`batch`参数大小。一个`batch`或`mini-batch`是一组组合在一起并作为单个批量输入馈送到神经网络的训练示例。例如，你不必一次只喂一个图像，因此一次只喂一个输入体积（一组堆叠的矩阵），你可以将多个输入体积压在一起。这样做通常有两个后果：
- en: Training is faster.
  id: totrans-855
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练速度更快。
- en: Training is less prone to overfitting.
  id: totrans-856
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练更不容易过拟合。
- en: If the mini-batch parameter is set to 1, you’ll see a curve that, especially
    in the first iterations, increases and decreases significantly. On the other hand,
    if you have a mini-batch that’s too big, the network may not be able to learn
    about specific patterns and features that rarely occur in inputs.
  id: totrans-857
  prefs: []
  type: TYPE_NORMAL
  zh: 如果将小批量参数设置为1，你将看到一个曲线，特别是在最初的迭代中，显著增加和减少。另一方面，如果你有一个过大的小批量，网络可能无法学习到输入中很少出现的特定模式和特征。
- en: 'It’s possible that a flat loss curve is related to a batch size (100, in this
    case) that’s too large for the data. To see whether something like this will make
    a difference, it’s useful to perform quick tests on small portions of the dataset.
    The changes in settings can be proven later with full-dataset training if you
    get encouraging results. So let’s set the `batch` parameter to 48, train on 5,000
    examples, and perform evaluation on 1,000 images. You expect a less smooth curve,
    along with lower loss, and hope for better accuracy:'
  id: totrans-858
  prefs: []
  type: TYPE_NORMAL
  zh: 可能一个平坦的损失曲线与一个过大的批量大小（在这个例子中是100）有关。为了看看类似这种情况是否会有所不同，对数据集的小部分进行快速测试是有用的。如果你得到鼓舞人心的结果，这些设置的变化可以在全数据集训练中证明。所以，让我们将`batch`参数设置为48，在5,000个示例上进行训练，并在1,000张图像上进行评估。你期望曲线不那么平滑，损失更低，并希望准确性更好：
- en: '[PRE77]'
  id: totrans-859
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: 'As you can see in these results and in [figure 9.5](#ch09fig05), reducing the
    batch size helped: a loss close to the minimum was reached much faster than with
    the batch size set to 100. But training took more time: 9 minutes instead of the
    previous time of 7 minutes. A difference of 2 minutes might be noticed on a larger
    scale, but it’s acceptable if the training time pays off with a significantly
    better F1 score.'
  id: totrans-860
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在这些结果和[图9.5](#ch09fig05)中看到的，减少批次大小有帮助：接近最小值的损失比将批次大小设置为100时更快地达到。但训练时间更长：9分钟而不是之前的7分钟。在更大规模上可能会注意到2分钟的差异，但如果训练时间能换来显著更好的F1分数，这是可以接受的。
- en: Figure 9.5\. Training with a batch size of 48
  id: totrans-861
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.5\. 使用48个批次的训练
- en: '![](Images/09fig05_alt.jpg)'
  id: totrans-862
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig05_alt.jpg)'
- en: 'The F1 score improved from 0.30 to 0.32\. So reducing the batch size seems
    to be a good idea that you need to prove with a full training. We won’t compare
    the F1 score of a small training set like this with the F1 score reached when
    training over 50,000 images, because that wouldn’t be fair and might mislead (and
    frustrate) our efforts. But can you do better with an even smaller batch size?
    Let’s set the batch size to 24 and see:'
  id: totrans-863
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数从0.30提高到0.32。因此，减少批次大小似乎是一个好主意，你需要通过完整的训练来证明这一点。我们不会将如此小训练集的F1分数与在超过50,000张图像上进行训练时达到的F1分数进行比较，因为这并不公平，可能会误导（并挫败）我们的努力。但是，你能用更小的批次大小做得更好吗？让我们将批次大小设置为24并看看：
- en: '[PRE78]'
  id: totrans-864
  prefs: []
  type: TYPE_PRE
  zh: '[PRE78]'
- en: As you can see in [figure 9.6](#ch09fig06), the curve is much sharper, and the
    loss is close to that with `batch` set to 48\. The F1 score is higher (0.33),
    but training took 13 minutes instead of 9.
  id: totrans-865
  prefs: []
  type: TYPE_NORMAL
  zh: 如您在[图9.6](#ch09fig06)中看到的，曲线更加陡峭，损失接近将`batch`设置为48时的损失。F1分数更高（0.33），但训练时间从9分钟增加到13分钟。
- en: Figure 9.6\. Training with a batch size of 24
  id: totrans-866
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.6\. 使用24个批次的训练
- en: '![](Images/09fig06_alt.jpg)'
  id: totrans-867
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig06_alt.jpg)'
- en: Evaluate and iterate
  id: totrans-868
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 评估和迭代
- en: 'At this point, you have to make a decision: can you afford more costly training
    in terms of time and computational resources (which may mean more money—for example,
    if you’re running training in production over cloud services) to get better numbers?
    A good practice is to save the different models you generate together with their
    evaluation metrics and training times so that you can pick them up in a later
    stage when you need to make decisions.'
  id: totrans-869
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，你必须做出决定：你是否能承担更多昂贵的训练，包括时间和计算资源（这可能意味着更多的金钱——例如，如果你在云服务上运行训练）以获得更好的结果？一个好的做法是将你生成的不同模型及其评估指标和训练时间一起保存，这样你可以在稍后需要做出决定时取用它们。
- en: 'With a smaller batch size, the neural network should be able to better handle
    more-diverse inputs, but the curve is sharper. Training the latest model with
    50,000 examples gave the following evaluation results after 5 hours of training
    on a laptop:'
  id: totrans-870
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更小的批次大小，神经网络应该能够更好地处理更多样化的输入，但曲线更加陡峭。在笔记本电脑上训练最新的模型，使用50,000个示例，经过5小时的训练后给出了以下评估结果：
- en: '[PRE79]'
  id: totrans-871
  prefs: []
  type: TYPE_PRE
  zh: '[PRE79]'
- en: The F1 score of 0.41 improved by 10% to reach a not-bad 0.51\. But this still
    isn’t something you’d ship to end users. With such a number, if users looked for
    images of a deer, they might get only five deer—the remaining images would show
    cats, dogs, or even trucks and ships!
  id: totrans-872
  prefs: []
  type: TYPE_NORMAL
  zh: F1分数从0.41提高了10%，达到0.51，但这仍然不是你可以提供给最终用户的东西。使用这样的数字，如果用户寻找鹿的图片，他们可能只能得到五张鹿的图片——其余的图片会显示猫、狗，甚至卡车和船只！
- en: You tried using more deep convolutional layers, but it didn’t help. You’ve seen
    that accuracy improves with the amount of data used. Batch size has proved to
    be an important parameter to get right even during the prototyping phase, to get
    better results, but changes in batch size have an effect on the training time.
  id: totrans-873
  prefs: []
  type: TYPE_NORMAL
  zh: 你尝试使用更多的深度卷积层，但这没有帮助。你已经看到，随着使用的数据量的增加，准确性会提高。批次大小已被证明是一个重要的参数，即使在原型设计阶段也需要正确设置，以获得更好的结果，但批次大小的变化会影响训练时间。
- en: 'But there are still a number of factors to consider:'
  id: totrans-874
  prefs: []
  type: TYPE_NORMAL
  zh: 但仍有许多因素需要考虑：
- en: Train for more epochs.
  id: totrans-875
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 进行更多个epoch的训练。
- en: Check the weights and bias initialization.
  id: totrans-876
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 检查权重和偏差的初始化。
- en: Look into *regularization* options.
  id: totrans-877
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 考虑正则化选项。
- en: Change the way the neural network updates its weights during backpropagation
    (the *updater* algorithm).
  id: totrans-878
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 改变神经网络在反向传播过程中更新其权重的机制（*更新器*算法）。
- en: Determine whether adding layers would help in this case.
  id: totrans-879
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 确定是否添加层会有所帮助。
- en: Let’s look at all of these options.
  id: totrans-880
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看所有这些选项。
- en: Epochs
  id: totrans-881
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 个epoch
- en: The example currently uses 10 epochs, so the neural network sees the same input
    batches 10 times. The rationale is that the network should be able to get the
    right weights for those inputs with a higher probability if it “sees” them multiple
    times. Low numbers like 5, 10, and 30 are common during the development phase
    when the network is being designed, but you may change this value when training
    your final model. If you increase the number of epochs but don’t see any significant
    improvement, the network probably can’t do more with the current setup for that
    data; in that case, you need to change something else.
  id: totrans-882
  prefs: []
  type: TYPE_NORMAL
  zh: 当前示例使用10个epoch，因此神经网络会看到相同的输入批次10次。其理由是，如果网络多次“看到”这些输入，它应该有更高的概率得到这些输入的正确权重。在开发阶段，当网络正在设计时，像5、10和30这样的低数字很常见，但您在训练最终模型时可以更改此值。如果您增加了epoch的数量但未看到任何显著的改进，那么网络可能无法在当前的数据设置中做更多的事情；在这种情况下，您需要更改其他一些东西。
- en: 'Changing the number of epochs from 10 to 20 in this case gives the following
    results:'
  id: totrans-883
  prefs: []
  type: TYPE_NORMAL
  zh: 将epoch的数量从10增加到20，可以得到以下结果：
- en: '[PRE80]'
  id: totrans-884
  prefs: []
  type: TYPE_PRE
  zh: '[PRE80]'
- en: Training took 28 minutes; see [figure 9.7](#ch09fig07).
  id: totrans-885
  prefs: []
  type: TYPE_NORMAL
  zh: 训练耗时28分钟；参见[图9.7](#ch09fig07)。
- en: Figure 9.7\. Loss curve for 20 epochs
  id: totrans-886
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.7. 20个epoch的损失曲线
- en: '![](Images/09fig07_alt.jpg)'
  id: totrans-887
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig07_alt.jpg)'
- en: Weight initialization
  id: totrans-888
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 权重初始化
- en: Think about the neural network before it receives any input. All the neurons
    have activation functions and connections. As the network begins to receive inputs
    and backpropagates output error, it starts to change the weights attached to each
    connection. A surprisingly effective change you can make to your neural network
    is the way those weights are initialized. A lot of research has shown that weight
    initialization has a significant impact on the effectiveness of training.^([[2](#ch09fn02)])
  id: totrans-889
  prefs: []
  type: TYPE_NORMAL
  zh: 在考虑神经网络在接收任何输入之前，所有神经元都有激活函数和连接。随着网络开始接收输入并反向传播输出误差，它开始改变每个连接附加的权重。您可以对神经网络进行的一个非常有效的变化是这些权重的初始化方式。许多研究表明，权重初始化对训练的有效性有显著影响。[^([2](#ch09fn02))]
- en: ²
  id: totrans-890
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ²
- en: ''
  id: totrans-891
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: 'See Xavier Glorot and Yoshua Bengio, “Understanding the Difficulty of Training
    Deep Feedforward Neural Networks,” in *Proceedings of the 13th International Conference
    on Artificial Intelligence and Statistics (AISTATS)* (2010, Chia Laguna Resort,
    Sardinia, Italy), [http://mng.bz/vNZM](http://mng.bz/vNZM); and Kaiming He et
    al., “Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification,” [https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852).'
  id: totrans-892
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '参见Xavier Glorot和Yoshua Bengio的“Understanding the Difficulty of Training Deep
    Feedforward Neural Networks”，发表于第13届国际人工智能与统计会议（AISTATS）论文集（2010年，意大利撒丁岛基亚拉纳度假村），[http://mng.bz/vNZM](http://mng.bz/vNZM)；以及Kaiming
    He等人“Delving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet
    Classification”，[https://arxiv.org/abs/1502.01852](https://arxiv.org/abs/1502.01852)。'
- en: 'Simple things you can do to initialize weights are to set them all to zero
    or set them to random numbers. A few chapters back, you saw how the learning algorithm
    (backpropagation) makes the network weights change: you can think of this visually
    as moving a point on an error surface (see [figure 9.8](#ch09fig08)). A point
    on such a surface represents a set of weights, and the minimum-height point represents
    the point where the weights make the network commit the least possible error.'
  id: totrans-893
  prefs: []
  type: TYPE_NORMAL
  zh: 初始化权重的一些简单方法是将它们全部设置为0或将它们设置为随机数。在几章之前，您看到了学习算法（反向传播）如何使网络权重发生变化：您可以将其可视化为在误差表面上移动一个点（参见[图9.8](#ch09fig08)）。此类表面上的一个点代表一组权重，而最低点的位置代表权重使网络产生尽可能小误差的点。
- en: Figure 9.8\. Error surface with some points of interest
  id: totrans-894
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.8. 具有趣味点的误差表面
- en: '![](Images/09fig08_alt.jpg)'
  id: totrans-895
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig08_alt.jpg)'
- en: 'The highest point in the image represents a set of weights with a high error,
    the point in the middle represents a set of weights with an average error, and
    the lowest point represents the point with the lowest possible error. The backpropagation
    algorithm will hopefully make the network weights move from their starting position
    to the point marked at the bottom. Now think about the weight initialization:
    it will be responsible for setting the starting point for the algorithm when looking
    for the best set of weights. With a weight initialization of 0, the network weights
    may be at the white point in the center: not bad, and not good. With a random
    initialization, you may get lucky and place the weights near the bottom point
    (but it’s unlikely) or set the weights somewhere far from there, such as the point
    marked at the top. This starting position influences the ability of backpropagation
    to ever reach the bottom point or may at least make the process longer and more
    difficult. Thus, good initialization for the neural network’s weights is crucial
    for successful training.'
  id: totrans-896
  prefs: []
  type: TYPE_NORMAL
  zh: 图像中的最高点代表一组具有高误差的权重，中间的点代表一组具有平均误差的权重，最低点代表具有可能最低误差的点。反向传播算法有望使网络权重从起始位置移动到标记在底部的点。现在考虑权重初始化：它将负责在寻找最佳权重集时设置算法的起始点。如果权重初始化为0，网络权重可能位于中心的白色点：不错，但也不算好。如果随机初始化，你可能幸运地让权重靠近底部点（但这种情况不太可能）或者将权重设置在远离那里的某个位置，例如标记在顶部的点。这个起始位置会影响反向传播到达底部点的能力，或者至少会使这个过程更长、更困难。因此，为神经网络权重选择良好的初始化对于成功的训练至关重要。
- en: 'A good, commonly used weight initialization is called *Xavier initialization*.
    Basically, it initializes the weights of the neural network by drawing them from
    a distribution with zero mean and a specific variance for each neuron. The initial
    weight depends on the number of neurons with outgoing connections to that specific
    neuron. You can set this in DL4J in a specific layer with the following code:'
  id: totrans-897
  prefs: []
  type: TYPE_NORMAL
  zh: 一种常用且良好的权重初始化方法称为**Xavier初始化**。基本上，它通过从具有零均值和每个神经元特定方差的分布中抽取权重来初始化神经网络的权重。初始权重取决于连接到该特定神经元的输出连接神经元的数量。您可以在DL4J中通过以下代码在特定层中设置此选项：
- en: '[PRE81]'
  id: totrans-898
  prefs: []
  type: TYPE_PRE
  zh: '[PRE81]'
- en: '***1* Initializes the weights of the given layer using Xavier distribution**'
  id: totrans-899
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 使用Xavier分布初始化给定层的权重**'
- en: Regularization
  id: totrans-900
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 正则化
- en: Earlier, when the number of inputs in a single batch was reduced, we noticed
    the loss curve becoming less smooth. This is because, with fewer batches, the
    learning algorithm is more prone to overfitting (see [figure 9.9](#ch09fig09)).
  id: totrans-901
  prefs: []
  type: TYPE_NORMAL
  zh: 之前，当单个批次的输入数量减少时，我们注意到损失曲线变得不那么平滑。这是因为，随着批次的减少，学习算法更容易过拟合（见[图9.9](#ch09fig09)）。
- en: Figure 9.9\. Sharpening loss curve with smaller batch sizes
  id: totrans-902
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.9. 使用较小批量大小的锐化损失曲线
- en: '![](Images/09fig09_alt.jpg)'
  id: totrans-903
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig09_alt.jpg)'
- en: 'It’s often useful to introduce regularization methods in your neural network
    training algorithm. This helps because of the small batch size, but that’s a good
    practice in general. The amount of regularization to use depends on the use case:'
  id: totrans-904
  prefs: []
  type: TYPE_NORMAL
  zh: 在您的神经网络训练算法中引入正则化方法通常很有用。这有助于小批量大小，但总的来说这是一个好的实践。使用的正则化量取决于具体的应用场景：
- en: '[PRE82]'
  id: totrans-905
  prefs: []
  type: TYPE_PRE
  zh: '[PRE82]'
- en: 'With regularization and weight initializations in place, let’s perform another
    round of training for 10 epochs on 5,000 images. Here are the final results:'
  id: totrans-906
  prefs: []
  type: TYPE_NORMAL
  zh: 在正则化和权重初始化到位后，让我们在5,000张图像上再进行一轮10个周期的训练。以下是最终结果：
- en: '[PRE83]'
  id: totrans-907
  prefs: []
  type: TYPE_PRE
  zh: '[PRE83]'
- en: Training took 16 minutes, but as you can see in [figure 9.10](#ch09fig10), the
    loss is decreasing much more quickly and to a lower value than with previous settings.
    As expected, the F1 score is high with relatively few training examples.
  id: totrans-908
  prefs: []
  type: TYPE_NORMAL
  zh: 训练耗时16分钟，但正如你在[图9.10](#ch09fig10)中可以看到，损失下降得更快，并且比之前的设置下降到更低的值。正如预期的那样，F1分数在相对较少的训练示例中很高。
- en: Figure 9.10\. Optimum tuning
  id: totrans-909
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.10. 最佳调整
- en: '![](Images/09fig10_alt.jpg)'
  id: totrans-910
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig10_alt.jpg)'
- en: 'Having noticed improvements with a greater number of epochs, let’s increase
    it to 20, as we did earlier:'
  id: totrans-911
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到随着周期数的增加，性能有所提高，让我们将其增加到20，就像我们之前做的那样：
- en: '[PRE84]'
  id: totrans-912
  prefs: []
  type: TYPE_PRE
  zh: '[PRE84]'
- en: 'Although training time increases to 19 minutes, the curve looks more or less
    similar, and, surprisingly, the F1 score remains unchanged (see [figure 9.11](#ch09fig11)).
    There are a few possible reasons for that: the first is that you may need more
    data.'
  id: totrans-913
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管训练时间增加到19分钟，但曲线看起来或多或少相似，而且令人惊讶的是，F1分数保持不变（见[图9.11](#ch09fig11)）。这种情况可能有几个原因：第一个可能是你可能需要更多的数据。
- en: Figure 9.11\. Optimum tuning for 20 epochs
  id: totrans-914
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.11. 20个周期的最佳调整
- en: '![](Images/09fig11_alt.jpg)'
  id: totrans-915
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig11_alt.jpg)'
- en: 'Let’s evaluate the accuracy of the last settings using the entire dataset of
    50,000 images (see [figure 9.12](#ch09fig12)):'
  id: totrans-916
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用包含50,000张图像的整个数据集（见[图9.12](#ch09fig12)）来评估最后设置的准确性：
- en: '[PRE85]'
  id: totrans-917
  prefs: []
  type: TYPE_PRE
  zh: '[PRE85]'
- en: Figure 9.12\. Training loss curve for the entire dataset
  id: totrans-918
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.12：整个数据集的训练损失曲线
- en: '![](Images/09fig12_alt.jpg)'
  id: totrans-919
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig12_alt.jpg)'
- en: Reaching good numbers isn’t always easy and may require several iterations of
    the process just described. Looking at recent research is always a good idea to
    find out whether better solutions exist for various aspects of neural networks.
    To some extent, tuning a neural network can seem like an art; experience helps,
    but getting to know the math and dynamics of learning is key for coming up with
    effective models and settings.
  id: totrans-920
  prefs: []
  type: TYPE_NORMAL
  zh: 达到良好的数字并不总是容易的，可能需要多次迭代上述过程。查看最近的研究总是一个好的主意，以了解是否存在针对神经网络各个方面的更好解决方案。在某种程度上，调整神经网络可以像一门艺术；经验有帮助，但了解数学和学习的动态对于提出有效的模型和设置至关重要。
- en: 9.2\. Indexes and neurons working together
  id: totrans-921
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.2：索引和神经元协同工作
- en: We just went through an end-to-end process to set up and tweak a deep neural
    network to achieve the best results in terms of accuracy. We also briefly noted
    the time required to train the entire network. With that set, only half the problem
    is solved. The goal is to use DL models in the context of search to provide more-meaningful
    search results to end users. Now, the question is how to use and update those
    DL models together with search engines.
  id: totrans-922
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚完成了一个端到端的过程，以设置和调整深度神经网络，以实现准确率方面的最佳结果。我们还简要提到了训练整个网络所需的时间。这样，问题只解决了一半。目标是使用深度学习模型在搜索的背景下为最终用户提供更有意义的搜索结果。现在的问题是，如何使用和更新这些深度学习模型以及搜索引擎。
- en: Assume for a moment that you have a pretrained model that perfectly fits the
    data to be indexed. You index text documents and want to use, for example, a pretrained
    seq2seq model to extract thought vectors to be used in the ranking function by
    the search engine. A straightforward solution is to establish a document-indexing
    pipeline where the document text is first sent to the seq2seq model, and then
    the corresponding thought vector is extracted and indexed together with the document
    text into the search engine. You can see in [figure 9.13](#ch09fig13) that the
    actions and responsibilities of the neural network and the search engine are heavily
    interleaved.
  id: totrans-923
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你有一个预训练的模型，它完美地适合要索引的数据。你索引文本文档，并想使用，例如，预训练的seq2seq模型来提取用于搜索引擎排名函数的思想向量。一个直接的方法是建立一个文档索引管道，其中文档文本首先发送到seq2seq模型，然后提取相应的思想向量，并将其与文档文本一起索引到搜索引擎中。你可以在[图9.13](#ch09fig13)中看到，神经网络和搜索引擎的动作和责任是高度交织的。
- en: Figure 9.13\. Neural network and search engine interactions, indexing time
  id: totrans-924
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.13：神经网络和搜索引擎交互，索引时间
- en: '![](Images/09fig13_alt.jpg)'
  id: totrans-925
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig13_alt.jpg)'
- en: At search time, the seq2seq model is again used to extract thought vectors from
    the query (see [figure 9.14](#ch09fig14)). The ranking function then performs
    scoring using the query and document thought vectors (previously stored in the
    index).
  id: totrans-926
  prefs: []
  type: TYPE_NORMAL
  zh: 在搜索时间，seq2seq模型再次用于从查询中提取思想向量（见[图9.14](#ch09fig14)）。然后，排名函数使用查询和文档思想向量（之前存储在索引中）进行评分。
- en: Figure 9.14\. Neural network and search engine interactions, search time
  id: totrans-927
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.14：神经网络和搜索引擎交互，搜索时间
- en: '![](Images/09fig14_alt.jpg)'
  id: totrans-928
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/09fig14_alt.jpg)'
- en: 'Looking at these graphs, you may think everything seems reasonable. But the
    neural network may introduce overhead for both indexing and search:'
  id: totrans-929
  prefs: []
  type: TYPE_NORMAL
  zh: 看到这些图表，你可能觉得一切看起来都很合理。但神经网络可能在索引和搜索方面引入额外的开销：
- en: '***Neural network prediction time—*** How long does the neural network take
    to extract thought vectors for documents at indexing time? How long does the neural
    network take to extract thought vectors for queries at search time?'
  id: totrans-930
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***神经网络预测时间——*** 神经网络在索引时间提取文档思想向量需要多长时间？神经网络在搜索时间提取查询思想向量需要多长时间？'
- en: '***Search engine index size—*** How much space do generated embeddings take
    in addition to storage space used by text documents?'
  id: totrans-931
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***搜索引擎索引大小——*** 生成的嵌入除了文本文档使用的存储空间外，还需要多少空间？'
- en: In general, the most critical aspect for performance is the query/search phase.
    You can’t expect users to wait for seconds just because your ranking function
    returns better results. In most cases, users won’t ever know what’s behind the
    search box—they just expect it to be fast and reliable, and to give good results.
  id: totrans-932
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，性能最关键的因素是查询/搜索阶段。你不能期望用户因为你的排名函数返回更好的结果而等待几秒钟。在大多数情况下，用户永远不会知道搜索框背后的内容——他们只期望它快速、可靠，并能给出好的结果。
- en: The previous section addressed the accuracy of results while noting the training
    times. You also need to track the time taken by the network to compute a full
    feed-forward pass from the input to the layer from which you get the network output.
  id: totrans-933
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的部分在指出训练时间的同时也讨论了结果准确性。您还需要跟踪网络从输入到获取网络输出的层的完整正向传递所需的时间。
- en: In the case of an encoder-decoder network, the feed-forward pass of the encoder
    side of the network only needs to extract thought vectors. The decoder side of
    the network is only used if you also want to use the input text to perform training
    using a target output (if you have one).
  id: totrans-934
  prefs: []
  type: TYPE_NORMAL
  zh: 在编码器-解码器网络的情况下，网络编码器部分的正向传递只需要提取思维向量。网络解码器部分仅在您还想使用输入文本通过目标输出进行训练时（如果您有的话）才会使用。
- en: The overhead in indexing must also be taken into account. In a “static” scenario
    where you ingest a set of documents, even if it’s huge, that may not be important,
    because you can accept an aggregate overhead of 1 or 2 hours if it only happens
    once. But re-indexing or high-volume concurrent indexing may be problematic. *Re-indexing*
    means indexing the entire corpus of documents in the search engine again from
    scratch. This is usually done due to a change in the configuration of text analysis
    pipelines or because a document processor is added to extract more metadata.
  id: totrans-935
  prefs: []
  type: TYPE_NORMAL
  zh: 索引时的开销也必须考虑。在一个“静态”场景中，即使您处理的文档集很大，这可能也不重要，因为如果您只发生一次，您可以接受1到2小时的总体开销。但是，重新索引或高量并发索引可能会出现问题。“重新索引”意味着从零开始再次在搜索引擎中索引整个文档库。这通常是因为文本分析管道配置发生变化或添加了一个文档处理器以提取更多元数据。
- en: For example, let’s take a simple search engine based on Lucene with no query-expansion
    capability. To use the word2vec model to expand synonyms at indexing time, you
    need to take all the existing documents and re-index them so the resulting inverted
    index also contains the words/synonyms extracted by word2vec. The bigger the index,
    the greater the impact of re-indexing will be.
  id: totrans-936
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，让我们以一个基于Lucene的简单搜索引擎为例，它没有查询扩展功能。为了在索引时间使用word2vec模型扩展同义词，您需要重新索引所有现有文档，以便生成的反向索引也包含由word2vec提取的单词/同义词。索引越大，重新索引的影响就越大。
- en: 'Concurrency is another aspect: can the neural network deal with concurrent
    inputs? This is an implementation detail and may depend on the specific technology
    used to implement your model, but it must be taken into account both at indexing
    time (multiple parallel indexing processes) and at search time (multiple users
    searching at the same time).'
  id: totrans-937
  prefs: []
  type: TYPE_NORMAL
  zh: 并发性是另一个方面：神经网络能否处理并发输入？这是一个实现细节，可能取决于实现您模型所使用的具体技术，但必须在索引时间（多个并行索引进程）和搜索时间（多个用户同时搜索）时都予以考虑。
- en: 'Embeddings, and dense vectors in general, can have many dimensions. Efficiently
    storing them is an open problem. In the real world, the choices may be limited
    by the capabilities of the search engine technology used. In Lucene, for example,
    dense vectors can be indexed as any of the following:'
  id: totrans-938
  prefs: []
  type: TYPE_NORMAL
  zh: 嵌入和密集向量通常可以有多个维度。有效地存储它们是一个未解决的问题。在现实世界中，选择可能受到所使用的搜索引擎技术的限制。例如，在Lucene中，密集向量可以按以下任何一种方式索引：
- en: '***Binaries—*** Every vector is stored like an unqualified binary, and all
    embedding processing is done when the binary is fetched.'
  id: totrans-939
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***二进制文件——*** 每个向量都像未指定二进制一样存储，并且所有嵌入处理都是在获取二进制文件时进行的。'
- en: '***n-dimensional points—*** Every vector is stored as a point with many dimensions
    (one for each vector dimension). Basic geometric and nearest-neighbor queries
    can be performed. At the moment, Lucene can index up to 8-dimensional vectors,
    so you’ll have to reduce higher-dimensional vectors (for example, 100-dimensional
    word vectors) to at most 8-dimensional vectors to index them in Lucene (like we
    did in [chapter 8](kindle_split_021.xhtml#ch08) with PCA for image feature vectors).'
  id: totrans-940
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***n维点—*** 每个向量都存储为一个具有多个维度（每个维度对应一个向量维度）的点。可以进行基本的几何和最近邻查询。目前，Lucene 可以索引最多
    8 维的向量，因此你必须将更高维度的向量（例如，100 维的词向量）减少到最多 8 维的向量，以便在 Lucene 中索引它们（就像我们在[第 8 章](kindle_split_021.xhtml#ch08)中用
    PCA 对图像特征向量所做的那样）。'
- en: '***Text—*** It may sound weird at first, but with an appropriate design, vectors
    can be indexed and searched over like text units.^([[3](#ch09fn03)])'
  id: totrans-941
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***文本—*** 起初可能听起来有些奇怪，但通过适当的设计，向量可以被索引并像文本单元一样进行搜索.^([[3](#ch09fn03)])'
- en: ³
  id: totrans-942
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ³
- en: ''
  id: totrans-943
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
- en: See Jan Rygl et al., “Semantic Vector Encoding and Similarity Search Using Fulltext
    Search Engines,” [https://arxiv.org/pdf/1706.00957.pdf](https://arxiv.org/pdf/1706.00957.pdf).
  id: totrans-944
  prefs:
  - PREF_IND
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 参见 Jan Rygl 等人，“使用全文搜索引擎进行语义向量编码和相似性搜索”，[https://arxiv.org/pdf/1706.00957.pdf](https://arxiv.org/pdf/1706.00957.pdf)。
- en: Other libraries like Vespa ([http://vespa.ai](http://vespa.ai)) and search platforms
    like Apache Solr ([https://lucene.apache.org/solr](https://lucene.apache.org/solr))
    and Elasticsearch ([www.elastic.co/products/elasticsearch](http://www.elastic.co/products/elasticsearch))
    may offer more or different options.
  id: totrans-945
  prefs: []
  type: TYPE_NORMAL
  zh: 其他库如 Vespa ([http://vespa.ai](http://vespa.ai)) 和搜索引擎平台如 Apache Solr ([https://lucene.apache.org/solr](https://lucene.apache.org/solr))
    以及 Elasticsearch ([www.elastic.co/products/elasticsearch](http://www.elastic.co/products/elasticsearch))
    可能提供更多或不同的选项。
- en: 9.3\. Working with streams of data
  id: totrans-946
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 9.3\. 处理数据流
- en: All the examples in this book use static datasets. Static datasets are great
    for illustrative purposes, because they make it easier to focus on a particular
    set of data. Also, when building a search engine, it’s common to start with a
    set of documents (text and/or images) that you want to index. But as a search
    engine goes into production and begins to be used, new documents will probably
    need to be ingested.
  id: totrans-947
  prefs: []
  type: TYPE_NORMAL
  zh: 本书中的所有示例都使用静态数据集。静态数据集非常适合说明目的，因为它们使得关注特定数据集变得更加容易。此外，在构建搜索引擎时，通常从一组你想要索引的文档（文本和/或图像）开始。但是，随着搜索引擎投入生产并开始使用，可能需要摄取新的文档。
- en: Consider an application that allows users to search for popular posts from social
    networks on various different topics. You might start with a set of downloaded
    or purchased posts, but because the focus is on popular posts, you need to keep
    ingesting data as trends change over time. A similar application might work on
    news rather than on social network posts. You can download a news corpus like
    the NYT Annotated Corpus ([https://catalog.ldc.upenn.edu/LDC2008T19](https://catalog.ldc.upenn.edu/LDC2008T19)),
    but every day, the application must ingest many new articles so that users can
    search for them.
  id: totrans-948
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑一个允许用户搜索来自不同主题的社交网络热门帖子的应用程序。你可能从一组下载或购买的帖子开始，但由于重点是热门帖子，你需要随着时间趋势的变化持续摄取数据。类似的应用程序可能用于新闻而不是社交网络帖子。你可以下载像
    NYT Annotated Corpus ([https://catalog.ldc.upenn.edu/LDC2008T19](https://catalog.ldc.upenn.edu/LDC2008T19))
    这样的新闻语料库，但每天应用程序必须摄取许多新文章，以便用户可以搜索它们。
- en: 'These days, it’s common to use a *streaming architecture* to address incoming
    flows of data. In a streaming architecture, data flows in continuously from one
    or more sources and is transformed by functions stacked in a pipeline. Data can
    be transformed, aggregated, or dropped at any time and finally reaches a *sink*:
    the final stage of the pipeline, such as a persistence system like a database
    or a search engine index.'
  id: totrans-949
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，使用*流式架构*来处理 incoming flows of data 是常见的做法。在流式架构中，数据从一个或多个来源连续流入，并通过管道中堆叠的函数进行转换。数据可以在任何时候进行转换、聚合或丢弃，最终到达*汇点*：管道的最终阶段，例如持久化系统如数据库或搜索引擎索引。
- en: In the previous example, a streaming architecture can continuously ingest posts
    from social networks and index them into the search engine. Another application
    working with the indexed data can read the index and expose search features to
    end users. But you’re working with neural networks, so you need to train the neural
    network models you want to use.
  id: totrans-950
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的例子中，流式架构可以持续摄取来自社交网络的帖子并将它们索引到搜索引擎中。另一个处理索引数据的应用程序可以读取索引并向最终用户暴露搜索功能。但是，你正在处理神经网络，因此你需要训练你想要使用的神经网络模型。
- en: 'As an example scenario, let’s build an application to continuously find the
    most relevant posts for each of a set of predefined topics; see [figure 9.15](#ch09fig15).
    To do so, you’ll use a streaming architecture to continuously do the following:'
  id: totrans-951
  prefs: []
  type: TYPE_NORMAL
  zh: 作为示例场景，让我们构建一个应用程序，以连续找到一组预定义主题中的每个主题的最相关帖子；参见[图9.15](#ch09fig15)。为此，您将使用流架构连续执行以下操作：
- en: Ingest posts from social networks (Twitter, in this case).
  id: totrans-952
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从社交网络（在这种情况下为Twitter）摄取帖子。
- en: Train different neural network models to extract document embeddings.
  id: totrans-953
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练不同的神经网络模型以提取文档嵌入。
- en: Index text and embeddings in Lucene.
  id: totrans-954
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Lucene中索引文本和嵌入。
- en: For each ranking model and for each topic, write out the most relevant posts.
  id: totrans-955
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于每个排名模型和每个主题，写出最相关的帖子。
- en: Figure 9.15\. Streaming application for continuous training, indexing, and search
    for social media posts
  id: totrans-956
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图9.15\. 用于社交媒体帖子连续训练、索引和搜索的流应用程序
- en: '![](Images/09fig15_alt.jpg)'
  id: totrans-957
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/09fig15_alt.jpg)'
- en: Finally, you’ll quickly evaluate which of the different ranking models (neural
    or not) is more promising. Such an application could be used, for example, in
    a preproduction phase to help choose the ranking model that works best for a production
    application.
  id: totrans-958
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，您将快速评估不同的排名模型（神经或非神经）哪个更有前途。此类应用程序可用于例如在预生产阶段帮助选择最适合生产应用程序的排名模型。
- en: 'To set up the streaming architecture, let’s use Apache Flink ([http://flink.apache.org](http://flink.apache.org)),
    a framework and distributed processing engine for computations over data streams.
    The Flink streaming pipeline will do the following:'
  id: totrans-959
  prefs: []
  type: TYPE_NORMAL
  zh: 为了设置流架构，让我们使用Apache Flink ([http://flink.apache.org](http://flink.apache.org))，这是一个用于数据流计算的平台和分布式处理引擎。Flink流管道将执行以下操作：
- en: Stream posts from the Twitter social network ([http://twitter.com](http://twitter.com))
    that contain certain keywords.
  id: totrans-960
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从包含特定关键词的Twitter社交网络([http://twitter.com](http://twitter.com))中流式传输帖子。
- en: Extract each tweet’s text, language, user, and so on.
  id: totrans-961
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 提取每条推文的文本、语言、用户等信息。
- en: 'Extract document embeddings using two separate models: paragraph vectors and
    word2vec averaged word embeddings.'
  id: totrans-962
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用两个不同的模型提取文档嵌入：段落向量和word2vec平均词嵌入。
- en: Index each tweet with its text, language, user, and document embeddings in Lucene.
  id: totrans-963
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在Lucene中索引每条推文及其文本、语言、用户和文档嵌入。
- en: Run predefined queries on all of the indexed data using different ranking models
    (classic and neural).
  id: totrans-964
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用不同的排名模型（经典和神经）在所有索引数据上运行预定义的查询。
- en: Write the output in a CSV file that can be analyzed at a later stage to assess
    the quality of the search results.
  id: totrans-965
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将输出写入一个CSV文件，以便在稍后阶段分析以评估搜索结果的质量。
- en: The output file will tell you how the different ranking models reacted to changing
    data with respect to a set of fixed queries for certain topics. This will provide
    valuable information about how well the ranking models adapt to new posts. If
    a ranking model keeps giving the same results despite changing data, it probably
    isn’t the best option for an application that aims to capture trending data.
  id: totrans-966
  prefs: []
  type: TYPE_NORMAL
  zh: 输出文件将告诉您不同的排名模型如何针对一组固定的查询对某些主题的数据变化做出反应。这将提供有关排名模型如何适应新帖子的宝贵信息。如果一个排名模型在数据变化的情况下仍然给出相同的结果，那么它可能不是旨在捕获趋势数据的最佳选择。
- en: First, let’s define a stream of data coming from Twitter.
  id: totrans-967
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们定义来自Twitter的数据流。
- en: Listing 9.5\. Defining a stream of Twitter data with Flink
  id: totrans-968
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.5\. 使用Flink定义Twitter数据流
- en: '[PRE86]'
  id: totrans-969
  prefs: []
  type: TYPE_PRE
  zh: '[PRE86]'
- en: '***1* Defines a Flink execution environment**'
  id: totrans-970
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 定义Flink执行环境**'
- en: '***2* Loads security credentials for accessing Twitter**'
  id: totrans-971
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 加载用于访问Twitter的安全凭据**'
- en: '***3* Creates a new Flink source for Twitter data**'
  id: totrans-972
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 为Twitter数据创建一个新的Flink源**'
- en: '***4* Defines the topics to be used to fetch posts from Twitter (only tweets
    containing those keywords will be ingested)**'
  id: totrans-973
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 定义用于从Twitter获取帖子（仅包含这些关键词的推文将被摄取）的主题**'
- en: '***5* Adds the per-topic filter to the Twitter source**'
  id: totrans-974
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 将按主题的过滤器添加到Twitter源**'
- en: '***6* Creates a stream over the Twitter data**'
  id: totrans-975
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 在Twitter数据上创建一个流**'
- en: '***7* Starts by converting raw text into a JSON format for tweets**'
  id: totrans-976
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 首先将原始文本转换为JSON格式以用于推文**'
- en: This listing performs the required configuration to start ingesting tweets that
    contain the keywords/topics “neural search,” “natural language processing,” “lucene,”
    “deep learning,” “word embeddings,” and “manning.”
  id: totrans-977
  prefs: []
  type: TYPE_NORMAL
  zh: 此列表执行所需的配置以开始摄取包含关键词/主题“神经搜索”、“自然语言处理”、“Lucene”、“深度学习”、“词嵌入”和“Manning”的推文。
- en: 'You’ll next define a series of functions to work on the tweets. We’ll also
    focus on implementation details regarding performance. For example, does it make
    sense to run the predefined queries every time a new tweet comes in? Perhaps it’s
    better to do this when you have more data (such as 20 tweets) that can influence
    scoring. For this reason, you’ll define a *count window* function that will pass
    the data to the next function only when it has received 20 tweets. In addition,
    updating a neural network model with just one sample usually isn’t a good idea:
    using a larger training batch is less prone to fluctuating training error (leading
    to a smoother learning curve).'
  id: totrans-978
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，您将定义一系列函数来处理推文。我们还将关注与性能相关的实现细节。例如，每次有新推文到来时运行预定义的查询是否有意义？也许在您有更多数据（例如20条推文）可以影响评分时进行此操作更好。因此，您将定义一个*计数窗口*函数，只有当它收到20条推文时才会将数据传递给下一个函数。此外，仅使用一个样本更新神经网络模型通常不是一个好主意：使用较大的训练批次更不容易出现波动的训练错误（导致学习曲线更平滑）。
- en: Listing 9.6\. Manipulating the streaming data
  id: totrans-979
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.6\. 操作流数据
- en: '[PRE87]'
  id: totrans-980
  prefs: []
  type: TYPE_PRE
  zh: '[PRE87]'
- en: '***1* Output CSV file**'
  id: totrans-981
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输出CSV文件**'
- en: '***2* Defines a count window over the streaming data**'
  id: totrans-982
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 在流数据上定义计数窗口**'
- en: '***3* Updates models, extracts features, and updates the index**'
  id: totrans-983
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 更新模型，提取特征，并更新索引**'
- en: '***4* Runs predefined queries**'
  id: totrans-984
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 运行预定义的查询**'
- en: '***5* Transforms the output in a way that’s suitable for composing a CSV file**'
  id: totrans-985
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 以适合组成CSV文件的方式转换输出**'
- en: '`ModelAndIndexUpdateFunction` is responsible for updating the neural network
    models and for indexing the documents in Lucene. In theory, you can split it into
    many tiny functions; but for the sake of readability, it’s easier to split the
    ingesting and searching processes into only two functions. You can theoretically
    use as many neural ranking models as you want; this example uses the ones defined
    in [chapters 5](kindle_split_017.xhtml#ch05) and [6](kindle_split_018.xhtml#ch06),
    using word2vec and paragraph vectors, respectively, to influence ranking.'
  id: totrans-986
  prefs: []
  type: TYPE_NORMAL
  zh: '`ModelAndIndexUpdateFunction`负责更新神经网络模型和在Lucene中索引文档。理论上，您可以将其拆分成许多很小的函数；但为了可读性，将摄取和搜索过程拆分成仅两个函数更容易。理论上，您可以使用尽可能多的神经排名模型；此示例使用在[第5章](kindle_split_017.xhtml#ch05)和[第6章](kindle_split_018.xhtml#ch06)中定义的模型，分别使用word2vec和段落向量来影响排名。'
- en: After ingesting each tweet, both paragraph vectors and word2vec models are used
    to generate two separate embeddings. The vectors are indexed together with the
    tweet text and used by the `ParagraphVectorsSimilarity` and `WordEmbeddingsSimilarity`
    classes at retrieval time.
  id: totrans-987
  prefs: []
  type: TYPE_NORMAL
  zh: 在处理完每条推文后，都会使用段落向量和word2vec模型生成两个独立的嵌入。这些向量与推文文本一起索引，并在检索时由`ParagraphVectorsSimilarity`和`WordEmbeddingsSimilarity`类使用。
- en: Listing 9.7\. Function for updating the model and indexing
  id: totrans-988
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.7\. 更新模型和索引的函数
- en: '[PRE88]'
  id: totrans-989
  prefs: []
  type: TYPE_PRE
  zh: '[PRE88]'
- en: '***1* Iterates over the current batch of tweets**'
  id: totrans-990
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 遍历当前批次的推文**'
- en: '***2* Creates a Lucene document for the current tweet’s text**'
  id: totrans-991
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为当前推文的文本创建一个Lucene文档**'
- en: '***3* Infers the paragraph vector, and updates the model**'
  id: totrans-992
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 推断段落向量，并更新模型**'
- en: '***4* Indexes the paragraph vector**'
  id: totrans-993
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 索引段落向量**'
- en: '***5* Infers a document vector from word2vec, and updates the model**'
  id: totrans-994
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 从word2vec推断文档向量，并更新模型**'
- en: '***6* Indexes the averaged word vector**'
  id: totrans-995
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 索引平均词向量**'
- en: '***7* Indexes the document**'
  id: totrans-996
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 索引文档**'
- en: '***8* Commits all the tweets to Lucene**'
  id: totrans-997
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 将所有推文提交到Lucene**'
- en: '***9* Closes the IndexWriter (releasing resources)**'
  id: totrans-998
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 关闭IndexWriter（释放资源）**'
- en: '***10* Passes the commit identifier to the next function (this can be used
    to track changes to the index over time)**'
  id: totrans-999
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 将提交标识符传递给下一个函数（这可以用于跟踪索引随时间的变化）**'
- en: '`MultiRetrieverFunction` contains some basic Lucene search code to run the
    fixed queries (such as “deep learning search”) over the entire index with different
    ranking functions. First, it sets up `IndexSearcher`s, each of which uses a different
    Lucene `Similarity`.'
  id: totrans-1000
  prefs: []
  type: TYPE_NORMAL
  zh: '`MultiRetrieverFunction`包含一些基本的Lucene搜索代码，用于在具有不同排名函数的整个索引上运行固定查询（例如“深度学习搜索”）。首先，它设置`IndexSearcher`s，每个`IndexSearcher`使用不同的Lucene`Similarity`。'
- en: Listing 9.8\. Setting up `IndexSearcher`s
  id: totrans-1001
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表9.8\. 设置`IndexSearcher`s
- en: '[PRE89]'
  id: totrans-1002
  prefs: []
  type: TYPE_PRE
  zh: '[PRE89]'
- en: '***1* IndexSearchers with different Similarities are kept in this Map.**'
  id: totrans-1003
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 在这个Map中保持不同相似度的IndexSearcher**'
- en: '***2* Creates an IndexSearcher for the ClassicSimilarity (TF-IDF)**'
  id: totrans-1004
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 为ClassicSimilarity（TF-IDF）创建一个IndexSearcher**'
- en: '***3* Sets the ClassicSimilarity in the IndexSearcher**'
  id: totrans-1005
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在IndexSearcher中设置ClassicSimilarity**'
- en: '***4* Puts the IndexSearcher in the Map**'
  id: totrans-1006
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 将IndexSearcher放入Map**'
- en: '***5* Creates an IndexSearcher for BM25Similarity (Lucene’s default)**'
  id: totrans-1007
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 为 BM25Similarity（Lucene 的默认相似度）创建 IndexSearcher**'
- en: '***6* Creates an IndexSearcher for ParagraphVectorsSimilarity**'
  id: totrans-1008
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 为 ParagraphVectorsSimilarity 创建 IndexSearcher**'
- en: '***7* Creates an IndexSearcher for LMDirichletSimilarity**'
  id: totrans-1009
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 为 LMDirichletSimilarity 创建 IndexSearcher**'
- en: '***8* Creates an IndexSearcher for WordEmbeddingsSimilarity**'
  id: totrans-1010
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 为 WordEmbeddingsSimilarity 创建 IndexSearcher**'
- en: You can add as many ranking models as you want. Next, you iterate over the available
    `IndexSearcher`s and execute the same query for each of them. Finally, the results
    are written into a CSV file.
  id: totrans-1011
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以添加尽可能多的排名模型。接下来，你遍历可用的 `IndexSearcher`s，并为每个执行相同的查询。最后，结果被写入一个 CSV 文件。
- en: The output aggregated in the CSV file during an execution of `MultiRetrieverFunction`
    contains a line for each ranking model. Each line contains the name of the model
    first (`classic`, `bm25`, `average wv ranking`, `paragraph vectors ranking`, and
    so on), followed by a comma and the text of the first search result returned with
    that ranking model. Over time, you’ll get a huge CSV file containing the output
    of the same query for all the different ranking models.
  id: totrans-1012
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `MultiRetrieverFunction` 的执行过程中，CSV 文件中聚合的输出包含每个排名模型的一行。每一行首先包含模型的名称（`classic`、`bm25`、`average
    wv ranking`、`paragraph vectors ranking` 等），然后是逗号和该排名模型返回的第一个搜索结果的文本。随着时间的推移，你将得到一个包含所有不同排名模型相同查询输出的巨大
    CSV 文件。
- en: 'Let’s look at the results of two consecutive executions (manually tagged with
    `<iteration-1>` and `<iteration-2>` for the sake of better readability):'
  id: totrans-1013
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看两次连续执行的结果（为了更好的可读性，手动标记为 `<iteration-1>` 和 `<iteration-2>`）：
- en: '[PRE90]'
  id: totrans-1014
  prefs: []
  type: TYPE_PRE
  zh: '[PRE90]'
- en: 'Notice that the non-neural ranking models didn’t change their top search result,
    whereas those relying on embeddings adapted immediately to the new data: this
    is the kind of capability for which neural ranking models can be useful. Streaming
    architectures can keep up with high loads of data to be indexed into a search
    engine, evaluate best models, and carefully orchestrate how neural networks and
    search engines can best work together.'
  id: totrans-1015
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到非神经网络排名模型没有改变他们的顶级搜索结果，而依赖于嵌入的排名模型立即适应了新的数据：这正是神经网络排名模型可以发挥作用的类型。流式架构可以跟上大量数据被索引到搜索引擎中的高负载，评估最佳模型，并仔细编排神经网络和搜索引擎如何最佳协作。
- en: Summary
  id: totrans-1016
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: Training deep learning models isn’t always straightforward; tuning and adjustments
    for real-world scenarios are often needed.
  id: totrans-1017
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练深度学习模型并不总是直截了当的；针对现实场景的调整和调优通常是必需的。
- en: Search engines and neural networks are often two different systems that interact
    both at indexing and search time. It’s essential to monitor their performance
    in order to keep the overall user experience good in terms of response times.
  id: totrans-1018
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 搜索引擎和神经网络通常是两个不同的系统，它们在索引和搜索时都会进行交互。为了保持整体用户体验在响应时间方面良好，监控它们的性能是至关重要的。
- en: Real-world deployments, like the common streaming scenario, must account for
    load and concurrency and evaluate quality, to achieve the best possible search
    solution.
  id: totrans-1019
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在现实世界的部署中，如常见的流式场景，必须考虑负载和并发性，并评估质量，以实现最佳的搜索解决方案。
- en: Looking forward
  id: totrans-1020
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 展望未来
- en: We started this book by wondering whether it’s possible to use deep neural networks
    as smart assistants to help provide better search tools. Over the course of the
    chapters, we’ve touched on several aspects of common search engines where DL has
    significant potential to help users find what they’re looking for.
  id: totrans-1021
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始这本书时，在思考是否可以使用深度神经网络作为智能助手来帮助提供更好的搜索工具。在接下来的章节中，我们探讨了几个常见搜索引擎的方面，其中深度学习（DL）有显著潜力帮助用户找到他们想要的内容。
- en: I hope you’ve become more and more interested in this topic as we’ve looked
    at increasingly complex subjects and algorithms. This book has given you some
    tools and practical advice that you can use immediately; hopefully, it has also
    inspired you to see what can be done better and what issues remain unsolved, and
    to want to dive in. While I was writing this book, many new DL papers were published,
    including some related to search. New activation functions have been determined
    to be useful, and new models have been proposed with promising results. I encourage
    you not to stop here and to keep thinking about what you and your users need and
    how to get there creatively.
  id: totrans-1022
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望随着我们探讨越来越复杂的话题和算法，你对这个主题的兴趣也越来越浓厚。这本书为你提供了一些你可以立即使用的工具和实用建议；希望它也激发了你去思考哪些方面可以做得更好，哪些问题尚未解决，并促使你想要深入探索。在我撰写这本书的过程中，许多新的深度学习论文被发表，其中包括一些与搜索相关的研究。一些新的激活函数被证明是有用的，并且提出了具有前景的新模型。我鼓励你不要止步于此，而要继续思考你和你用户的需求，以及如何创造性地实现这些需求。
- en: We’re just beginning to scratch the surface of applying DL to information retrieval.
    You’ve learned about the foundations of neural search and are now ready to learn
    and do more by yourself. Have fun!
  id: totrans-1023
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚开始探索将深度学习应用于信息检索的表面。你已经了解了神经搜索的基础，现在你准备好自学并做更多的事情了。祝你好玩！
