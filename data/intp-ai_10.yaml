- en: 7 Understanding semantic similarity
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 7 理解语义相似性
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Learning dense word representations that capture semantic meaning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 学习捕获语义意义的密集词表示
- en: Visualizing semantic similarity of high-dimensional word embeddings using dimensionality-reduction
    techniques like PCA and t-SNE
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用PCA和t-SNE等降维技术可视化高维词嵌入的语义相似性
- en: Strengths and weaknesses of PCA and t-SNE
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: PCA 和 t-SNE 的优缺点
- en: Validating visualizations generated by PCA and t-SNE qualitatively and quantitatively
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对PCA和t-SNE生成的可视化进行定性和定量验证
- en: 'In the previous chapter, we switched our focus from interpreting the complex
    processing and operations that happen within a black-box model to interpreting
    the representations or features learned by the model. We specifically looked at
    the network dissection framework to understand what concepts are learned by the
    feature-learning layers in a convolutional neural network (CNN). The framework
    consisted of three key steps: concept definition, network probing, and alignment
    measurement. The concept definition step is all about data collection, specifically
    collecting a labeled dataset of concepts at the pixel level. This is the most
    time-consuming and crucial step. The next step is to probe the network and determine
    what units in the CNN respond to those predefined concepts. The final step involves
    quantifying how well the units’ responses align with the concepts. The framework
    overcame the limitations of visual attribution methods by coming up with quantitative
    interpretations in the form of human-understandable concepts.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们将关注点从解释黑盒模型内部发生的复杂处理和操作转向解释模型学习到的表示或特征。我们特别研究了网络剖析框架来了解卷积神经网络（CNN）中的特征学习层学习到了哪些概念。该框架包括三个关键步骤：概念定义、网络探测和对齐测量。概念定义步骤主要涉及数据收集，特别是收集像素级别的概念标注数据集。这是最耗时且最关键的步骤。下一步是探测网络，确定CNN中哪些单元对预定义的概念做出响应。最后一步是量化单元响应与概念的匹配程度。该框架通过提出以人类可理解的概念形式存在的定量解释，克服了视觉归因方法的局限性。
- en: In this chapter, we will continue with the topic of interpreting representations
    learned by deep neural networks but will switch our focus to natural language
    processing (NLP). NLP is a subfield in machine learning that deals with natural
    language. So far, we have been dealing with inputs in the form of images or in
    tabular form with numeric features. In NLP, we will deal with inputs in the form
    of text. We will specifically focus on how to represent text in a dense and semantically
    meaningful form and how to interpret words that are similar in meaning—that is,
    those that have semantic similarity—learned by those representations.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续探讨解释深度神经网络学习到的表示的主题，但将关注点转向自然语言处理（NLP）。NLP是机器学习的一个子领域，它处理自然语言。到目前为止，我们一直在处理图像或以数值特征表格形式输入。在NLP中，我们将处理文本形式的输入。我们将特别关注如何以密集和语义上有意义的形式表示文本，以及如何解释那些意义相似——即那些具有语义相似性的——由这些表示学习到的单词。
- en: We will first introduce a concrete example of analyzing sentiment in movie reviews.
    We will then learn about neural word embedding, an interesting branch of deep
    learning that is widely used to represent text in a semantically meaningful form.
    These word representations can then be used as inputs to a model for predicting
    the sentiment. The remainder of the chapter will focus on interpreting and visualizing
    semantic similarity from the word representations. We will specifically learn
    about linear and nonlinear dimensionality-reduction techniques such as principal
    component analysis (PCA) and t-distributed stochastic neighbor embedding (t-SNE).
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将首先介绍一个分析电影评论情感的具体例子。然后，我们将了解神经词嵌入，这是深度学习中的一个有趣分支，广泛用于以语义上有意义的形式表示文本。这些词表示可以用作预测情感的模型的输入。本章的其余部分将专注于从词表示中解释和可视化语义相似性。我们将特别学习线性和非线性降维技术，如主成分分析（PCA）和t分布随机邻域嵌入（t-SNE）。
- en: 7.1 Sentiment analysis
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.1 情感分析
- en: In this chapter, we are tasked by a movie website called Internet Movie Repository
    to determine the sentiments of reviews of movies. The objective is to determine
    whether a review is associated with a positive or negative emotion. This is illustrated
    in figure 7.1, where we have two movies and a couple of reviews for each of them.
    The ratings for both movies are shown purely for illustrative purposes. Based
    on the words or sequence of words in each review, we want to determine whether
    a review expresses a positive emotion or opinion or a negative one.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们接受了一个名为“互联网电影库”的电影网站的委托，确定电影评论的情感。目标是确定一个评论是否与积极或消极的情绪相关联。如图7.1所示，我们有两个电影和每个电影的几条评论。这两个电影的评分仅用于说明目的。基于每条评论中的单词或单词序列，我们想要确定评论表达的是积极情绪或观点，还是消极情绪。
- en: '![](../Images/CH07_F01_Thampi.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F01_Thampi.png)'
- en: Figure 7.1 Sentiment analysis of movie reviews
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.1 电影评论的情感分析
- en: The goal is to build an AI system that, given a review as input, determines
    whether the review conveys a positive or negative emotion. Given this information,
    we can formulate the problem as a binary classification problem. It will be similar
    to the binary classifiers that we saw in chapters 4 and 5, but rather than dealing
    with tabular data with numeric features or images, we are dealing with a sequence
    of words, as shown in figure 7.2\. The input to the model is a sequence of words
    representing the review, and the output is a score that represents the probability
    that the sentiment of the review is positive.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是构建一个AI系统，给定一个评论作为输入，判断该评论是否传达了积极或消极的情绪。有了这个信息，我们可以将问题表述为一个二元分类问题。这将会类似于我们在第4章和第5章中看到的二元分类器，但与处理具有数值特征的表格数据或图像不同，我们处理的是一系列单词，如图7.2所示。模型的输入是一系列代表评论的单词，输出是一个分数，表示评论情感为积极的概率。
- en: '![](../Images/CH07_F02_Thampi.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F02_Thampi.png)'
- en: Figure 7.2 Sentiment binary classifier
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2 情感二分类器
- en: 'The sentiment analysis model in figure 7.2 is shown as a black box. We’ll cover
    the specifics of the model in section 7.3.4\. Before we jump into how to build
    the model, we want to answer the following two key questions:'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.2中的情感分析模型被表示为一个黑盒。我们将在7.3.4节中详细介绍模型的具体情况。在我们深入构建模型的方法之前，我们想要回答以下两个关键问题：
- en: How do we represent a word in a form that the model can process?
  id: totrans-17
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何将一个单词表示成模型可以处理的形式？
- en: How do we model a sequence of words and build a classifier based on that?
  id: totrans-18
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何对一系列单词进行建模并基于此构建分类器？
- en: The main focus of this book is on answering the first question. We will learn
    about deep learning models that can be used to represent words in a dense and
    semantically meaningful form and how to interpret them. Once we have a good way
    of representing words, answering the second question—how to build a model that
    processes a sequence of words—becomes more straightforward. Although this is not
    the main focus of this book, we will briefly look at sequence modeling and how
    to interpret such models using techniques that we have learned in the previous
    chapters. Before we jump into word representations, let’s explore the dataset
    of movie reviews first and figure out why we need a good representation of the
    words to be able to build the sentiment classifier.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 本书的主要重点是回答第一个问题。我们将学习可以用来以密集和语义上有意义的形式表示单词的深度学习模型，以及如何解释它们。一旦我们找到了表示单词的好方法，回答第二个问题——如何构建处理一系列单词的模型——就会变得更加直接。尽管这并不是本书的主要焦点，但我们将简要介绍序列建模以及如何使用我们在前几章中学到的技术来解释这类模型。在我们深入单词表示之前，让我们首先探索电影评论数据集，弄清楚为什么我们需要一个好的单词表示来构建情感分类器。
- en: 7.2 Exploratory data analysis
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.2 探索性数据分析
- en: 'In this section, we’ll explore the movie review dataset and determine whether
    we can engineer any numeric features to train a simpler logistic regression or
    tree-based model. The main objective is to determine the need for coming up with
    semantically meaningful word representations and to model sequences of words.
    We will be using the `torchtext` package provided by PyTorch to load and process
    the dataset. The `torchtext` package is similar to `torchvision` in that it provides
    various data-processing utilities, popular datasets, and models for NLP. We can
    install the package using pip as follows:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将探索电影评论数据集，并确定我们是否可以构建任何数值特征来训练一个更简单的逻辑回归或基于树的模型。主要目标是确定是否需要提出具有语义意义的词表示以及建模词序列。我们将使用PyTorch提供的`torchtext`包来加载和处理数据集。`torchtext`包与`torchvision`类似，因为它提供了各种数据处理实用工具、流行数据集和NLP模型。我们可以使用pip安装此包，如下所示：
- en: '[PRE0]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'In addition to `torchtext`, we will also install spaCy, a popular NLP library
    that we will use for string tokenization. Tokenization is the process of splitting
    a string of text into discrete components or tokens, such as words and punctuations.
    A naive tokenization method is to split a string of text on spaces, but this method
    does not take into account punctuation. The `spaCy` library provides more sophisticated
    ways of tokenizing strings in various languages. We will focus on the English
    language in this chapter and, therefore, use a model called `en_core_web_sm` for
    string tokenization. The spaCy library and the model can be installed as follows:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 除了`torchtext`，我们还将安装spaCy，这是一个流行的NLP库，我们将用它来进行字符串分词。分词是将文本字符串分割成离散组件或标记的过程，例如单词和标点符号。一种简单的分词方法是按空格分割文本字符串，但这种方法没有考虑到标点符号。`spaCy`库提供了在多种语言中分词字符串的更复杂的方法。在本章中，我们将专注于英语语言，因此使用名为`en_core_web_sm`的模型进行字符串分词。spaCy库和模型可以按照以下方式安装：
- en: '[PRE1]'
  id: totrans-24
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'With all the libraries in place, we can now load the movie review dataset as
    follows:'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有库都就绪后，我们现在可以按照以下方式加载电影评论数据集：
- en: '[PRE2]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ① Imports PyTorch and the relevant utilities from torchtext
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入PyTorch和来自torchtext的相关实用工具
- en: ② Initializes the Field class with the tokenizer for the movie review text
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用电影评论文本的标记器初始化Field类
- en: ③ Initializes the LabelField class to load the sentiment labels as float
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化LabelField类以将情感标签加载为浮点数
- en: ④ Loads the movie review dataset and splits it into train and test sets
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 加载电影评论数据集并将其分为训练集和测试集
- en: Let’s now look at some key summary statistics from this dataset, such as the
    number of reviews in the train and test sets, the proportion of positive and negative
    reviews, and the number of words in each review, as summarized in table 7.1\.
    In the interest of space, we will not show the source code for this, but you can
    obtain it from the GitHub repository associated with this book.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们现在看看这个数据集的一些关键汇总统计数据，例如训练集和测试集中的评论数量、正面和负面评论的比例以及每条评论中的单词数量，这些数据总结在表7.1中。为了节省空间，我们不会展示相应的源代码，但你可以从与本书相关的GitHub仓库中获取它。
- en: Table 7.1 Key statistics from the movie review dataset
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.1 电影评论数据集的关键统计数据
- en: '| Statistics | Train set | Test set |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 统计信息 | 训练集 | 测试集 |'
- en: '| Number of reviews | 25,000 | 25,000 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 评论数量 | 25,000 | 25,000 |'
- en: '| Proportion of positive reviews | 50% | 50% |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 正面评论比例 | 50% | 50% |'
- en: '| Proportion of negative reviews | 50% | 50% |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 负面评论比例 | 50% | 50% |'
- en: '| Number of words in positive reviews | Minimum | 14 | 11 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 正面评论中的单词数量 | 最小值 | 14 | 11 |'
- en: '| Median | 202 | 198 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | 202 | 198 |'
- en: '| Maximum | 2789 | 2640 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 2789 | 2640 |'
- en: '| Number of words in negative reviews | Minimum | 11 | 5 |'
  id: totrans-40
  prefs: []
  type: TYPE_TB
  zh: '| 负面评论中的单词数量 | 最小值 | 11 | 5 |'
- en: '| Median | 203 | 203 |'
  id: totrans-41
  prefs: []
  type: TYPE_TB
  zh: '| 中位数 | 203 | 203 |'
- en: '| Maximum | 1827 | 1290 |'
  id: totrans-42
  prefs: []
  type: TYPE_TB
  zh: '| 最大值 | 1827 | 1290 |'
- en: From table 7.1, we can observe that the train and test sets have an equal number
    of movie reviews—25,000 each. The reviews are equally split between positive and
    negative reviews for both sets. We can also observe that the summary statistics
    of the number of words in a review are similar across both train and test sets.
    We can see some differences between positive and negative reviews, especially
    the minimum and maximum number of words per review. Besides understanding the
    dataset, the reason for looking at some of these key summary statistics is to
    determine whether we can engineer certain numeric features and build a simple
    logistic regression or tree-based classifier for sentiment analysis.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 从表7.1中，我们可以观察到训练集和测试集的电影评论数量相等——每个集都有25,000条。两个集合中的正面和负面评论都平均分配。我们还可以观察到，在训练集和测试集中，评论中单词数量的汇总统计相似。我们可以看到正面和负面评论之间的一些差异，特别是每条评论的最小和最大单词数。除了理解数据集之外，查看这些关键汇总统计的原因是为了确定我们是否可以构建某些数值特征，并构建一个简单的逻辑回归或基于树的分类器来进行情感分析。
- en: In that vein, let’s look at the distribution of the number of words per review,
    comparing positive and negative sentiments. Are there any differences in the number
    of words for positive and negative reviews? And if so, are negative reviews usually
    longer or shorter than positive reviews? We can answer these questions by looking
    at figure 7.3\. You can find the source code to generate this plot in the GitHub
    repository associated with this book.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 沿着这个思路，让我们看看每条评论中单词数量的分布，比较正面和负面情感。正面和负面评论的单词数量是否有差异？如果有，负面评论通常比正面评论长还是短？我们可以通过查看图7.3来回答这些问题。您可以在与本书相关的GitHub仓库中找到生成此图表的源代码。
- en: '![](../Images/CH07_F03_Thampi.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F03_Thampi.png)'
- en: Figure 7.3 Distribution of the number of words per review—positive versus negative
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.3 每条评论中单词数量的分布——正面与负面
- en: In figure 7.3, we can see no glaring differences between positive and negative
    reviews in terms of the number of words. Looking at the number of words, therefore,
    does not accurately predict whether a review is positive or negative.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 在图7.3中，我们可以看到在单词数量方面，正面和负面评论之间没有明显的差异。因此，仅通过查看单词数量并不能准确预测评论是正面还是负面。
- en: '![](../Images/CH07_F04_Thampi.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F04_Thampi.png)'
- en: Figure 7.4 Word cloud for positive reviews
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.4 正面评论的词云
- en: How about the frequency or occurrences of words? Are any words more common in
    positive or negative reviews? Figure 7.4 shows a word cloud of all the common
    words in positive reviews. The word cloud was generated after some data cleaning
    where very common words such as *a*, *the*, *is*, *at*, *which*, and *on* (also
    called stop words) and punctuation have been removed. You can access the code
    to remove all the stop words and clean the data in the GitHub repository associated
    with this book. In a word cloud, the larger the word, the more frequently it occurs
    in the reviews. We can see that for positive reviews, the most frequently occurring
    words are words like *film*, *movie*, *one*, and *character*. We also see words
    that convey positive sentiment like *love*, *great*, *good,* and *wonderful*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 关于单词的频率或出现次数呢？是否有某些单词在正面或负面评论中更常见？图7.4显示了正面评论中所有常见单词的词云。词云是在一些数据清理之后生成的，其中去除了非常常见的单词，如*a*、*the*、*is*、*at*、*which*和*on*（也称为停用词）以及标点符号。您可以在与本书相关的GitHub仓库中访问用于移除所有停用词并清理数据的代码。在词云中，单词越大，它在评论中出现的频率就越高。我们可以看到，对于正面评论，最频繁出现的单词是像*film*、*movie*、*one*和*character*这样的单词。我们还看到传达积极情绪的单词，如*love*、*great*、*good*和*wonderful*。
- en: '![](../Images/CH07_F05_Thampi.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F05_Thampi.png)'
- en: Figure 7.5 Word cloud for negative reviews
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5 负面评论的词云
- en: Figure 7.5 shows the word cloud for negative reviews. At first glance, we do
    see some of the same words as those in positive reviews—such as *film*, *movie*,
    *one*, and *character*—also commonly occurring in negative reviews. We also see
    some words that convey negative sentiment, such as *bad*, *unfortunately*, *poor*,
    and *stupid*. If we compare figures 7.4 and 7.5, there aren’t any glaring differences
    in word counts between positive and negative reviews. We could, however, find
    more signal from this word count feature by cleaning the dataset further using
    human knowledge and heuristics. We could, for instance, remove some of the neutral
    words such as *film*, *movie*, *one*, and *character*, just to name a few. As
    you can imagine, this supervised way of engineering features using some background
    knowledge of the language (identifying neutral words, for instance) and heuristics
    is quite time-consuming and not guaranteed to extend easily to other languages.
    We need a better way of representing words in a language, which will be the focus
    of the next section.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.5展示了负面评论的词云。乍一看，我们确实看到了一些与正面评论中相同的词语——例如*电影*、*电影*、*一个*和*角色*——这些词语也常见于负面评论中。我们还可以看到一些表达负面情绪的词语，例如*糟糕*、*不幸地*、*差劲*和*愚蠢*。如果我们比较图7.4和图7.5，正面评论和负面评论在词语数量上并没有明显的差异。然而，我们可以通过进一步使用人类知识和启发式方法清理数据集来从这一词语计数特征中找到更多的信号。例如，我们可以移除一些中性的词语，如*电影*、*电影*、*一个*和*角色*，仅举几例。正如你所想象的那样，这种使用一些语言背景知识（例如识别中性词语）和启发式方法进行特征工程的方法是相当耗时且不一定容易扩展到其他语言的。我们需要一种更好的方法来表示语言中的词语，这将是下一节的重点。
- en: 7.3 Neural word embeddings
  id: totrans-54
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.3 神经词语嵌入
- en: In the previous section, we saw how difficult it is to come up with numeric
    features to train a sentiment analysis model. We will now learn how to represent
    words in a numeric form that encodes as much of the meaning as possible. We can
    then use these word representations to train a sentiment analysis model. Before
    we jump in, let’s get the terminology out of the way. Dense representations of
    words that encode semantic meaning are called word *embeddings**,* *word vectors*,
    or *distributed representations*. Representations or word embeddings learned by
    neural networks are called *neural word embeddings*. We will focus on neural word
    embeddings in this chapter.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了提出数值特征以训练情感分析模型是多么困难。现在，我们将学习如何以数值形式表示词语，尽可能多地编码其意义。然后我们可以使用这些词语表示来训练情感分析模型。在我们深入之前，让我们先澄清一下术语。编码语义意义的词语的密集表示称为词语*嵌入*、*词语向量*或*分布式表示*。由神经网络学习的词语表示或词语嵌入称为*神经词语嵌入*。在本章中，我们将重点关注神经词语嵌入。
- en: We need to be aware of a few more NLP terms. We will use the term *corpus* to
    refer to the body of text that we will be processing. For the movie review example,
    the corpus would be all of the movie reviews in the dataset. We will use the term
    *vocabulary* to refer to the words within the text corpus.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还需要了解一些更多的NLP术语。我们将使用术语*语料库*来指代我们将要处理的文本体。对于电影评论示例，语料库将是数据集中所有的电影评论。我们将使用术语*词汇表*来指代文本语料库中的词语。
- en: 7.3.1 One-hot encoding
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.1 单热编码
- en: Now let’s look at a naive way of representing words that shows the need for
    word embeddings. This exercise highlights the need to come up with more sophisticated
    ways of representing words in a dense, semantically meaningful form. Assume we
    have a corpus of text consisting of *V* words in its vocabulary. The vocabulary
    size *V* is typically quite large. Let’s look at the example shown in figure 7.6\.
    In the figure, we can see the words in the corpus listed in the table on the left.
    From the table, we can see that the corpus consists of more than 10,000 words.
    Each word in the corpus is assigned an index in the table.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看一种表示词语的简单方法，这表明了词语嵌入的需求。这个练习突出了需要提出更复杂的方法来以密集、语义有意义的形式表示词语。假设我们有一个包含词汇表中的*V*个词语的文本语料库。词汇表大小*V*通常相当大。让我们看看图7.6所示的例子。在图中，我们可以看到语料库中的词语列在左边的表中。从表中我们可以看到，语料库由超过10,000个词语组成。语料库中的每个词语在表中都被分配了一个索引。
- en: '![](../Images/CH07_F06_Thampi.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F06_Thampi.png)'
- en: Figure 7.6 An illustration of one-hot encoded vectors
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.6 单热编码向量的示意图
- en: A naive way of representing words in the corpus is to use a vector with a size
    equal to the vocabulary size *V*, where each entry in the vector corresponds to
    a word in the corpus. In figure 7.6, we can see representations for words in the
    phrase “movie is a masterpiece.” A naive presentation for the word *this* consists
    of a vector where the entries for each of the other words is 0 and the value at
    the position or index for the word *this* is 1\. Similarly, for the other words
    in the sentence, we can see a vector of all zeros, except at the index for the
    word where the value is 1\. This sort of representation is called *one-hot encoding*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在语料库中表示单词的一种天真方法是用一个大小等于词汇量大小 *V* 的向量，其中向量的每个条目对应于语料库中的一个单词。在图7.6中，我们可以看到短语“movie
    is a masterpiece.”中单词的表示。单词 *this* 的天真表示由一个向量组成，其中其他每个单词的条目都是0，而单词 *this* 的位置或索引处的值是1。同样，对于句子中的其他单词，我们可以看到一个全为零的向量，除了单词所在索引处的值为1。这种表示方式被称为
    *独热编码*。
- en: As we can see in the figure, one-hot encoding uses an extremely sparse representation
    for the words where the vectors are mostly zeros, with only a single 1\. It does
    not encode any semantic information about a word. Words that occur frequently
    together or that are similar in meaning are hard to identify using this representation.
    The size of the vector is also large. We will need a vector as large as the vocabulary
    to represent words. Processing such vectors would be extremely inefficient in
    terms of computing and storage.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，独热编码使用了一种极端稀疏的单词表示，其中向量大部分为零，只有一个1。它不编码任何关于单词的语义信息。使用这种表示方法很难识别经常一起出现或意义相似的单词。向量的尺寸也很大。我们需要一个与词汇量一样大的向量来表示单词。处理这样的向量在计算和存储方面将极其低效。
- en: Note that the representation in figure 7.6 is indeed a very naive representation.
    We have ways of improving the representation by removing the stop words. This
    should reduce the size of the one-hot-encoded word vector used to represent each
    word. Another alternative is to use the *bag of words* (*BoW*) model. The BoW
    model essentially maps each word to a number that represents how frequently it
    occurs in the corpus. In a BoW representation, stop words would typically have
    larger numeric values because they frequently occur in the language. We could
    either remove these stop words, or we could use another representation called
    *term frequency inverse document frequency* (*TF-IDF* for short). The TF-IDF model
    essentially weights inversely the frequency of occurrence of each word in the
    corpus of reviews with the number of reviews that contain that word. This model
    is a good way of filtering out stop words because they will be associated with
    a lower numeric value. The numeric value is lower because such words occur frequently
    across reviews. Both BoW and TF-IDF are efficient ways of representing words,
    but they still do not encode the semantic information about a word.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，图7.6中的表示确实是一种非常天真表示。我们有方法通过去除停用词来改进表示。这应该会减少用于表示每个单词的独热编码单词向量的尺寸。另一种选择是使用
    *词袋模型* (*BoW*)。BoW模型本质上将每个单词映射到一个数字，该数字表示它在语料库中出现的频率。在BoW表示中，停用词通常具有更大的数值，因为它们在语言中频繁出现。我们可以选择删除这些停用词，或者我们可以使用另一种称为
    *词频逆文档频率* (*TF-IDF*) 的表示方法。TF-IDF模型本质上将每个单词在评论语料库中出现的频率与包含该单词的评论数量成反比。这种模型是过滤停用词的好方法，因为它们将与较低的数值相关联。数值较低是因为这些词在评论中频繁出现。BoW和TF-IDF都是表示单词的高效方法，但它们仍然不编码关于单词的语义信息。
- en: 7.3.2 Word2Vec
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.2 Word2Vec
- en: 'We can overcome the limitations of one-hot encoding and other more efficient
    representations like BoW and TF-IDF by using *Word2Vec* (short for Word to Vector)
    embeddings. The key idea behind Word2Vec is to look at words in context. We can
    encode meaning by looking at words that typically occur together. Let’s look at
    an example and come up with some notation. In figure 7.7, we can see the same
    phrase as before, “movie is a masterpiece.” The figure also shows a context with
    a window size equal to 3, that is, a context consisting of three tokens or words:
    *movie*, *is*, and *a*. The window size is equivalent to the number of tokens
    or words in the context. We denote the center word in the context as *w*[t], the
    word immediately to the left as *w*[t–1], and the word immediately to the right
    as *w*[t+1]. Words to the left and right of the center word are also called *surrounding
    words* or *context words*.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过使用*Word2Vec*（即Word to Vector）嵌入来克服one-hot编码和其他更有效的表示方法（如BoW和TF-IDF）的局限性。Word2Vec背后的关键思想是观察单词在上下文中的情况。我们可以通过观察通常一起出现的单词来编码意义。让我们来看一个例子并给出一些符号。在图7.7中，我们可以看到之前相同的短语，“电影是一部杰作。”该图还显示了一个窗口大小等于3的上下文，即由三个标记或单词组成：*电影*、*是*和*一个*。窗口大小等于上下文中的标记或单词数量。我们用*w*[t]表示上下文中的中心词，用*w*[t–1]表示紧靠其左边的单词，用*w*[t+1]表示紧靠其右边的单词。中心词左右两侧的单词也称为*周围词*或*上下文词*。
- en: '![](../Images/CH07_F07_Thampi.png)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F07_Thampi.png)'
- en: Figure 7.7 An illustration of context, window size, surrounding words, and center
    word
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.7 展示了上下文、窗口大小、周围单词和中心词的示例
- en: 'We can use two key neural network architectures to come up with Word2Vec embeddings:
    continuous bag of words (CBOW) and skip-gram, as shown in figure 7.8.'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用两种关键的神经网络架构来生成Word2Vec嵌入：连续词袋（CBOW）和skip-gram，如图7.8所示。
- en: '![](../Images/CH07_F08_Thampi.png)'
  id: totrans-69
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F08_Thampi.png)'
- en: Figure 7.8 An illustration of CBOW and skip-gram neural word embedding models
    for window size = 3
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.8 展示了窗口大小为3的CBOW和skip-gram神经词嵌入模型
- en: As seen in the figure, the idea behind the CBOW architecture is to predict the
    center word given the surrounding, or context, words. The underlying neural network
    architecture is the fully connected neural network consisting of an input layer,
    a hidden layer, and an output layer. The skip-gram architecture, on the other
    hand, predicts the surrounding or context words given the center word. The underlying
    neural network architecture is similar to that of CBOW. Both CBOW and skip-gram
    models are also similar in the sense that they try to predict neighboring words
    or words that typically occur together. But they differ in some respects. The
    skip-gram model has been shown to work well with small amounts of data and also
    represents less frequently occurring words well. The CBOW model, on the other
    hand, is faster to train and has been shown to come up with better representations
    for more frequently occurring words. The training processes for both models are
    equivalent. So, to keep things simple, let’s focus on one of them and take a closer
    look at the skip-gram training process.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如图中所示，CBOW架构背后的思想是根据周围或上下文单词预测中心词。其背后的神经网络架构是一个包含输入层、隐藏层和输出层的全连接神经网络。另一方面，skip-gram架构是根据中心词预测周围或上下文单词。其背后的神经网络架构与CBOW类似。CBOW和skip-gram模型在尝试预测邻近单词或通常一起出现的单词方面也是相似的。但它们在某些方面有所不同。skip-gram模型已被证明在少量数据下表现良好，并且能够很好地表示出现频率较低的单词。另一方面，CBOW模型训练速度更快，并且已被证明能够为出现频率较高的单词提供更好的表示。两种模型的训练过程是等效的。因此，为了简化问题，让我们关注其中之一，并更详细地研究skip-gram的训练过程。
- en: The first step in training the skip-gram word embedding is to come up with a
    training dataset. Given the corpus of text, the idea is to come up with a dataset
    consisting of center words as input and the corresponding surrounding, or context,
    words as output. We need to know the window size for the context prior to generating
    the dataset because the window size is an important hyperparameter for the training
    process. Let’s stick with the same window size of 3, as in the earlier model,
    and look at a concrete example, shown in figure 7.9\. In the figure, we are using
    the same example sentence as before. We set the context window at the start of
    the text (shown as context 1 in the figure) and identify the center word and surrounding
    words. We then come up with a training data table consisting of the center word
    as input and the surrounding words as output. In the table for context 1, the
    word *is* is associated with the two neighboring words, *movie* and *a*.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 跳字图词语嵌入训练的第一步是构建一个训练数据集。给定文本语料库，我们的想法是构建一个数据集，其中包含中心词语作为输入，相应的周围或上下文词语作为输出。在生成数据集之前，我们需要知道上下文窗口大小，因为窗口大小是训练过程中的一个重要超参数。让我们保持与早期模型相同的窗口大小3，并查看一个具体示例，如图7.9所示。在图中，我们使用与之前相同的示例句子。我们将上下文窗口设置在文本的开始处（如图中所示为上下文1）并识别中心词语和周围词语。然后我们构建一个训练数据表，其中包含中心词语作为输入，周围词语作为输出。在上下文1的表中，词语*is*与两个相邻词语*movie*和*a*相关联。
- en: '![](../Images/CH07_F09_Thampi.png)'
  id: totrans-73
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F09_Thampi.png)'
- en: Figure 7.9 Training data preparation for the skip-gram model
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.9 跳字图模型训练数据准备
- en: 'We then continue this process by sliding the window to the right by one word,
    as shown as context 2 in figure 7.9\. We will then add another entry to the training
    data table for the new center word and surrounding words. We repeat this process
    for all the text in the corpus. Once we have the training dataset consisting of
    input and output words, we are ready to train the skip-gram neural network. We
    can further simplify the training process by reformulating the problem as a binary
    classification problem as follows: instead of predicting the surrounding words
    given a center word, we predict whether a given pair of words are neighbors. A
    pair of words are neighbors if they occur within the context. We can use the training
    data table that we generated in figure 7.9 to come up with positive labels for
    this new binary classification formulation. This is shown in the top half of figure
    7.10 where the table of input and output (surrounding or context) words is transformed
    into a table of word pairs with a positive label (i.e., label = 1). The positive
    label denotes that the pair of words are neighbors.'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们通过将窗口向右滑动一个词语，如图7.9中的上下文2所示，继续这个过程。我们将为新的中心词语和周围词语添加另一个条目到训练数据表中。我们对语料库中的所有文本重复此过程。一旦我们有了包含输入和输出词语的训练数据集，我们就可以准备训练跳字图神经网络了。我们可以通过将问题重新表述为二分类问题来进一步简化训练过程：不是预测给定中心词语的周围词语，而是预测给定词语对是否为邻居。如果两个词语在上下文中出现，则它们是邻居。我们可以使用图7.9中生成的训练数据表来为这个新的二分类公式提供正标签。这如图7.10的上半部分所示，其中输入和输出（周围或上下文）词语的表被转换为一个具有正标签（即，标签=1）的词语对表。正标签表示这两个词语是邻居。
- en: '![](../Images/CH07_F10_Thampi.png)'
  id: totrans-76
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F10_Thampi.png)'
- en: Figure 7.10 Training data preparation with negative sampling
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.10 带负采样的训练数据准备
- en: How do we determine the negative labels, that is, the pairs of words that are
    not neighbors? We can do this using a process called *negative sampling*. For
    each word in the training data table from figure 7.9, we randomly sample a new
    word from the vocabulary. The choice of window size is important. If the window
    size is relatively small when compared to the number of words in the vocabulary,
    random sampling will ensure that the likelihood of the selected word being outside
    the context for the input word is small. This is shown in the bottom half of figure
    7.10\. For each pair of input word and random word, we assign a negative label
    (i.e., label = 0). These correspond to pairs of words that are not neighbors.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何确定负标签，即不是邻居的词语对？我们可以通过一个称为*负采样*的过程来完成这项工作。对于图7.9中的训练数据表中的每个词语，我们从词汇表中随机采样一个新词语。窗口大小的选择很重要。如果与词汇表中的词语数量相比，窗口大小相对较小，则随机采样将确保所选词语出现在输入词语上下文之外的可能性很小。这如图7.10的下半部分所示。对于每个输入词语和随机词语的对，我们分配一个负标签（即，标签=0）。这些对应于不是邻居的词语对。
- en: 'Once we have the training dataset for the new binary classification formulation,
    we are ready to train the skip-gram model. We will call the neural network model
    with this new formulation as skip-gram with negative sampling. The input words
    will be represented as one-hot-encoded vectors. Although the model is trained
    to determine whether two words are neighbors, the end objective of the training
    process is to learn neural word embeddings or dense representations for the words.
    This is the purpose of the hidden layer in the architecture. For the hidden layer,
    we will need to initialize the two matrices shown in figure 7.11: one embedding
    matrix and one context matrix. The embedding matrix consists of one row for each
    word in the vocabulary. The number of columns corresponds to the size of the word
    embedding or word vector used to represent the word. This is shown as *N* in figure
    7.11.'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了新二分类公式的训练数据集，我们就可以准备训练跳字模型了。我们将使用这种新公式的神经网络模型称为跳字模型与负采样。输入单词将被表示为一组one-hot编码向量。尽管模型被训练来决定两个单词是否是邻居，但训练过程的最终目标是学习单词的神经词嵌入或密集表示。这是架构中隐藏层的目的。对于隐藏层，我们需要初始化图7.11中显示的两个矩阵：一个嵌入矩阵和一个上下文矩阵。嵌入矩阵由词汇表中的每个单词一行组成。列数对应于用于表示单词的单词嵌入或单词向量的大小。这如图7.11中的
    *N* 所示。
- en: '![](../Images/CH07_F11_Thampi.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![图7.11](../Images/CH07_F11_Thampi.png)'
- en: Figure 7.11 Skip-gram with negative sampling training
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.11 跳字模型（Skip-gram）与负采样训练
- en: We also need to determine another hyperparameter, the embedding size, before
    training. The choice of embedding size determines how dense we want the representation
    to be. It also determines how much semantic information is captured in the representation.
    The context matrix is also of the same size as the embedding matrix. Both matrices
    are initialized with random values. The values in these matrices are parameters
    in the neural network that we aim to learn using the training dataset that we
    have generated in figure 7.10.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练之前，我们还需要确定另一个超参数，即嵌入大小。嵌入大小的选择决定了我们希望表示有多密集。它还决定了在表示中捕获了多少语义信息。上下文矩阵的大小与嵌入矩阵相同。这两个矩阵都使用随机值初始化。这些矩阵中的值是神经网络中的参数，我们旨在使用我们在图7.10中生成的训练数据集来学习这些参数。
- en: Let’s now take a closer look at the learning process. Figure 7.11 shows the
    two matrices and a row-wise dot-product operation being performed on them. The
    row-wise dot product essentially measures the similarity between two pairs of
    words. If we then pass the resulting vector through a sigmoid function, we will
    get a similarity, or probability, measure between 0 and 1\. We can then compare
    these scores with the true label for pairs of words in the training data and update
    the parameters accordingly. The parameters can be updated through backpropagation,
    as we learned in chapters 4 and 5.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来更详细地看看学习过程。图7.11显示了两个矩阵以及在这些矩阵上执行行内点积操作。行内点积本质上衡量了两个单词对之间的相似性。如果我们然后将得到的向量通过sigmoid函数，我们将得到一个介于0和1之间的相似性或概率度量。然后我们可以将这些分数与训练数据中单词对的真正标签进行比较，并相应地更新参数。参数可以通过反向传播来更新，正如我们在第4章和第5章中学到的。
- en: 'Once the learning process is complete, we can discard the context matrix and
    use the embedding matrix as a mapping of words to their corresponding neural word
    embeddings. We can obtain the mapping as follows: each row in the embedding matrix
    is a representation for a given word in the vocabulary. For instance, the first
    row in the matrix corresponds to the representation for word *w*[1]. The second
    row is a representation for word *w*[2], and so on.'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦学习过程完成，我们可以丢弃上下文矩阵，并使用嵌入矩阵作为单词到其对应的神经词嵌入的映射。我们可以通过以下方式获得映射：嵌入矩阵中的每一行都是词汇表中给定单词的表示。例如，矩阵中的第一行对应于单词
    *w*[1] 的表示。第二行是单词 *w*[2] 的表示，依此类推。
- en: 7.3.3 GloVe embeddings
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.3 GloVe嵌入
- en: The skip-gram with negative sampling model is a great way of coming up with
    a dense representation of words that captures the similarity between pairs of
    words that occur within a local context. The model does not do a great job of
    identifying stop words, however. Stop words, like *is*, *a*, *the*, and *this*,
    will be flagged as words similar to words like, say, *masterpiece* because they
    occur together in a local context. We can identify such stop words by looking
    at global statistics of words, that is, how frequently pairs of words occur within
    the whole corpus of text. The *global vectors* (also called *GloVe*) *model* is
    an improvement to skip-gram that captures both global and local statistics. Going
    forward, we will use pretrained GloVe word embeddings.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 跳过-负采样模型是生成单词密集表示的绝佳方式，该表示捕捉了在局部上下文中出现的单词对之间的相似性。然而，该模型在识别停用词方面做得并不好。像*is*、*a*、*the*和*this*这样的停用词将被标记为与像*masterpiece*这样的单词相似，因为它们在局部上下文中一起出现。我们可以通过查看单词的全局统计信息来识别这样的停用词，即单词对在整个文本语料库中出现的频率。*全局向量*（也称为*GloVe*）*模型*是跳过-负采样的改进，它捕捉了全局和局部统计信息。在接下来的工作中，我们将使用预训练的GloVe单词嵌入。
- en: 'We will not be training GloVe word embeddings from scratch using the movie
    review dataset but instead will use pretrained GloVe embeddings trained on a much
    larger corpus of text. A common corpus of text used to train word embeddings is
    Wikipedia. We have the following two ways of loading GloVe embeddings pretrained
    on the Wikipedia corpus:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不会使用电影评论数据集从头开始训练GloVe单词嵌入，而是将使用在更大文本语料库上预训练的预训练GloVe嵌入。用于训练单词嵌入的常见文本语料库是维基百科。我们有以下两种加载维基百科语料库上预训练的GloVe嵌入的方法：
- en: Using the `torchtext` package provided by PyTorch
  id: totrans-88
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用PyTorch提供的`torchtext`包
- en: Using `gensim`, a common open source Python library used for NLP
  id: totrans-89
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`gensim`，这是一个常用的开源Python库，用于自然语言处理
- en: 'The first approach of loading GloVe embeddings using `torchtext` is useful
    if we have to train another downstream model, like sentiment classification, that
    makes use of these embeddings as features in PyTorch. The second approach of loading
    GloVe embeddings using `gensim` is useful for analyzing the word embeddings because
    a lot of utility functions come right out of the box. We will use the former approach
    for training the sentiment classifier and the latter approach for interpreting
    the word embeddings. We can load the word embeddings using `torchtext` as follows:'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`torchtext`加载GloVe嵌入的第一种方法，如果我们必须训练另一个下游模型，例如情感分类，该模型将使用这些嵌入作为PyTorch中的特征时是有用的。使用`gensim`加载GloVe嵌入的第二种方法对于分析单词嵌入是有用的，因为许多实用函数都是现成的。我们将使用前一种方法来训练情感分类器，后一种方法来解释单词嵌入。我们可以按以下方式使用`torchtext`加载单词嵌入：
- en: '[PRE3]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ① Imports the vocab module in torchtext
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从torchtext导入词汇模块
- en: ② Initializes the GloVe class with the model pretrained on six billion words
    from the Wikipedia corpus
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用在维基百科上预训练的包含六十亿个单词的模型初始化GloVe类
- en: ③ Loads the GloVe embedding with size 100
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 加载大小为100的GloVe嵌入
- en: Note that the GloVe embeddings pretrained on six billion words from the Wikipedia
    corpus is loaded. The embedding size of the pretrained model is 100.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，已经加载了在维基百科语料库上预训练的包含六十亿个单词的GloVe嵌入。预训练模型的嵌入大小为100。
- en: 'If you have not installed `gensim` on your machine, you can do so by running
    the following command:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您尚未在您的机器上安装`gensim`，可以通过运行以下命令来完成：
- en: '[PRE4]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'We can then load the GloVe embeddings as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们可以按以下方式加载GloVe嵌入：
- en: '[PRE5]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ① Imports the relevant modules and classes from gensim
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从gensim导入相关模块和类
- en: ② Initializes the path to the pretrained GloVe embedding file
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化预训练GloVe嵌入文件的路径
- en: ③ Initializes the GloVe embedding
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化GloVe嵌入
- en: Note that with `gensim`, we need to download the pretrained GloVe embedding
    file. You can download the embedding pretrained on six billion words from Wikipedia
    with an embedding size of 100 from the GloVe project website ([https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/)).
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，使用`gensim`时，我们需要下载预训练的GloVe嵌入文件。您可以从GloVe项目网站（[https://nlp.stanford.edu/projects/glove/](https://nlp.stanford.edu/projects/glove/））下载在维基百科上预训练的包含六十亿个单词且嵌入大小为100的嵌入。
- en: 7.3.4 Model for sentiment analysis
  id: totrans-104
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.4 情感分析模型
- en: 'In section 7.1, we posited the following two key questions for building the
    sentiment analysis model:'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在7.1节中，我们提出了以下两个关键问题，用于构建情感分析模型：
- en: How do we represent a word in a form that the model can process?
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何以模型可以处理的形式表示一个单词？
- en: How do we model a sequence of words and build a classifier based on that?
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们如何对单词序列进行建模并基于此构建分类器？
- en: We have already answered the first question in the previous section by learning
    about neural word embeddings. The key focus of this chapter is on word embeddings
    and how to interpret them. For the sake of completeness, we answer the second
    question by providing a high-level overview of how to model a sequence of words
    to build a sentiment classifier.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经在上一节通过学习神经词嵌入回答了第一个问题。本章的关键重点是词嵌入及其解释。为了完整性，我们通过提供一个如何将单词序列建模以构建情感分类器的高级概述来回答第二个问题。
- en: The high-level architecture for the sentiment classifier is shown in the top
    half of figure 7.12\. It consists of two neural network architectures that are
    chained together. The first neural network is called a recurrent neural network
    (RNN), and the second neural network is a fully connected neural network, which
    we learned about in chapter 4\. Let’s take a closer look at RNNs, shown in the
    bottom half of figure 7.12.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分类器的高级架构如图7.12的上半部分所示。它由两个链式连接的神经网络架构组成。第一个神经网络被称为循环神经网络（RNN），第二个神经网络是完全连接的神经网络，我们在第4章中学习过。让我们更详细地看看RNNs，如图7.12的下半部分所示。
- en: '![](../Images/CH07_F12_Thampi.png)'
  id: totrans-110
  prefs: []
  type: TYPE_IMG
  zh: '![图7.12](../Images/CH07_F12_Thampi.png)'
- en: Figure 7.12 Sequence modeling and sentiment analysis using recurrent neural
    networks (RNNs)
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.12 使用循环神经网络（RNNs）进行序列建模和情感分析
- en: RNNs are typically used in analyzing sequences, like a sequence of words, as
    in the sentiment analysis problem, or time-series analysis, like weather forecasting.
    For the sentiment analysis problem, the RNN takes the sequence of words one at
    a time and produces a hidden state for each word, which is the representation
    of the previous inputs. The words are fed into the RNN using the neural word embedding
    representation learned in the previous section. Once all of the words have been
    fed into the RNN, the final hidden state is then used to train the feed-forward
    neural network for sentiment classification. We are glossing over a lot of detail
    here because this is not meant to be the primary focus of this chapter and the
    book. A great resource for learning more about RNNs and language models is the
    online course on NLP with deep learning from Stanford University ([http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: RNNs通常用于分析序列，如单词序列，如在情感分析问题中，或时间序列分析，如天气预报。对于情感分析问题，RNN逐个处理单词序列，并为每个单词生成一个隐藏状态，这是先前输入的表示。单词通过在前一节中学习的神经词嵌入表示输入到RNN中。一旦所有单词都已输入到RNN中，最终的隐藏状态就用于训练用于情感分类的前馈神经网络。这里我们省略了很多细节，因为这不是本章和本书的主要关注点。关于RNNs和语言模型的更多学习资源是斯坦福大学提供的NLP与深度学习在线课程（[http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/))。
- en: Transformer networks
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: Transformer网络
- en: A recent breakthrough in NLP has been *transformer networks*, proposed by a
    team at Google Research in 2017 in their seminal paper “Attention Is All You Need”
    ([https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)). Like RNNs,
    transformer networks or trans-formers are used to model sequential data. As we
    saw in section 7.3.4, RNNs process the input one word at a time in order. The
    output of the current word, or timestep—that is, the hidden state—is required
    before we can process the next word. It is hard to parallelize the training process,
    and, therefore, training RNNs is quite time-consuming. Transformers overcome this
    limitation by adopting the attention mechanism and do not require us to feed the
    input of words in order. Intuitively, the attention mechanism is similar to the
    convolution-based approach in convolutional neural networks (CNNs) where the interactions
    of words that occur closer together in sequences are modeled at lower layers and
    interactions of words that occur farther apart in sequences are modeled at higher
    layers. All the words are fed into the network at once, together with information
    on their relative and absolute positions.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理（NLP）领域的一个最近突破是*transformer网络*，由谷歌研究团队在2017年提出的开创性论文“Attention Is All
    You Need”（[https://arxiv.org/abs/1706.03762](https://arxiv.org/abs/1706.03762)）中提出。与RNNs类似，transformer网络或trans-formers用于建模序列数据。正如我们在7.3.4节中看到的，RNNs按顺序逐个处理输入的词。在处理下一个词之前，需要当前词的输出，即隐藏状态。这使得训练过程难以并行化，因此训练RNNs相当耗时。Transformers通过采用注意力机制克服了这一限制，并且不需要我们按顺序提供词的输入。直观地说，注意力机制类似于卷积神经网络（CNNs）中基于卷积的方法，其中序列中更接近的词之间的交互在较低层建模，而序列中较远的词之间的交互在较高层建模。所有词同时被输入到网络中，同时包含它们相对和绝对位置的信息。
- en: We are glossing over a lot of detail here—an entire chapter is required to do
    justice to this topic, but, unfortunately, that’s beyond the scope of this book.
    A great resource for learning more about transformers, with video lectures and
    lecture notes, is the online course on NLP with deep learning from Stanford University
    ([http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/)).
    Developments in the transformer network architecture include systems such as bidirectional
    encoder representations from transformers (BERT) and generative pretrained transformer
    (GPT). Pretrained word embeddings learned by transformers can be loaded in PyTorch
    using the popular open-source library provided by Hugging Face ([https://huggingface.co/transformers/](https://huggingface.co/transformers/)).
    The interpretability techniques that you will learn in the subsequent sections
    to understand semantic similarity learned by GloVE word embeddings can be extended
    to embeddings learned by transformer networks as well. The techniques are model-agnostic.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里我们省略了很多细节——要公正地讨论这个话题，需要整整一章的内容，但遗憾的是，这超出了本书的范围。关于学习更多关于transformers的内容，包括视频讲座和讲义，可以参考斯坦福大学提供的在线课程《深度学习与NLP》（[http://web.stanford.edu/class/cs224n/](http://web.stanford.edu/class/cs224n/))。transformer网络架构的发展包括双向编码器表示（BERT）和生成预训练transformer（GPT）等系统。transformers学习到的预训练词嵌入可以通过Hugging
    Face提供的流行开源库在PyTorch中加载（[https://huggingface.co/transformers/](https://huggingface.co/transformers/))）。在后续章节中，你将学习到的用于理解GloVE词嵌入学习的语义相似性的可解释技术，也可以扩展到由transformer网络学习的嵌入。这些技术是模型无关的。
- en: 7.4 Interpreting semantic similarity
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 7.4 解释语义相似性
- en: In the previous section, you learned how to obtain dense representation of words
    that encode semantic meaning using neural word embeddings. Now we will focus on
    understanding and interpreting semantic similarity from those learned word embeddings.
    You will learn how to measure semantic similarity and also how to visualize similarity
    between high-dimensional word embeddings in two dimensions.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你学习了如何获取使用神经词嵌入编码语义意义的密集表示的词的表示。现在我们将专注于理解和解释从这些学习到的词嵌入中得到的语义相似性。你将学习如何衡量语义相似性，以及如何将高维词嵌入在二维空间中可视化相似性。
- en: Before we start measuring and interpreting semantic similarity, the first step
    is to identify a few words where the meanings are varied and nuanced and we have
    a good understanding of the semantic similarity between them and other words that
    would be similar to them. This is similar to the concept definition step in the
    network dissection framework in chapter 6 in that we need a good human understanding
    of what specifically we want to measure and interpret. In the context of semantic
    similarity in neural word embeddings, we need an understanding or taxonomy of
    words to verify whether the neural word embeddings have learned the semantic meanings
    properly.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们开始测量和解释语义相似度之前，第一步是确定一些意义多变且细微的词汇，我们对它们与其他可能相似的词汇之间的语义相似度有很好的理解。这与第6章网络分解框架中的概念定义步骤类似，在那里我们需要对我们要具体测量和解释的内容有良好的人类理解。在神经词嵌入的语义相似度背景下，我们需要对词汇的理解或分类法，以验证神经词嵌入是否正确地学习了语义意义。
- en: 'We will look at two different sets of words to interpret semantic similarity.
    The first set of words is not necessarily related to the movie review or sentiment
    classification problem. The words are, however, meant to verify whether certain
    nuances of words are captured by the word embeddings. The first set (referred
    to as set 1) of words follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将查看两组不同的词汇来解释语义相似度。第一组词汇不一定与电影评论或情感分类问题相关。然而，这些词汇旨在验证某些词汇细微差别是否被词嵌入所捕捉。第一组（称为组1）的词汇如下：
- en: '*Basketball*'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*篮球*'
- en: '*Lebron*'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*勒布朗*'
- en: '*Ronaldo*'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*罗纳尔多*'
- en: '*Facebook*'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*Facebook*'
- en: '*Media*'
  id: totrans-124
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*媒体*'
- en: The meaning or link between these words can be obtained from the taxonomy, shown
    in figure 7.13\. In the figure, the words in the set are highlighted. We can see
    that within the category Sport, we have *Basketball* and *Football/Soccer*. A
    Sport also has personalities—*Lebron* and *Ronaldo* fall under the categories
    Sport and Personality. There is also a link between the sport personalities and
    their respective sports. *Lebron*, for instance, is linked with the sport *Basketball*
    and *Ronaldo* is linked with the sport *Football/Soccer*. Also, within the category
    Media, we have different types of media, such as *Television*, *Radio*, and *Internet*.
    Within the Internet category, there are companies like *Facebook* and *Google*.
    Figure 7.13 serves as a map of how words are linked, and we can use this to interpret
    semantic meaning in word embeddings.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 这些词汇之间的意义或联系可以从图7.13中所示的分类法中获得。在图中，该组中的词汇被突出显示。我们可以看到，在体育类别中，我们有*篮球*和*足球/足球赛*。体育类别也有个性——*勒布朗*和*罗纳尔多*属于体育和个性类别。体育个性和他们各自的运动之间也存在联系。例如，*勒布朗*与运动*篮球*相关联，而*罗纳尔多*与运动*足球/足球赛*相关联。此外，在媒体类别中，我们有不同类型的媒体，如*电视*、*广播*和*互联网*。在互联网类别中，有像*Facebook*和*Google*这样的公司。图7.13作为词汇之间如何关联的地图，我们可以使用它来解释词嵌入中的语义意义。
- en: '![](../Images/CH07_F13_Thampi.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F13_Thampi.png)'
- en: Figure 7.13 Taxonomy for the words in set 1
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.13 组1中词汇的分类
- en: 'The second set (referred to as set 2) of words is related to movie reviews.
    We will look at the following set of movies to see how they are related:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组（称为组2）的词汇与电影评论相关。我们将查看以下电影集，以了解它们是如何相关的：
- en: '*Godfather*'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*教父*'
- en: '*Goodfellas*'
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*好家伙*'
- en: '*Batman*'
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*蝙蝠侠*'
- en: '*Avengers*'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*复仇者联盟*'
- en: The taxonomy for the second set of words is shown in figure 7.14.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 第二组词汇的分类法如图7.14所示。
- en: '![](../Images/CH07_F14_Thampi.png)'
  id: totrans-134
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F14_Thampi.png)'
- en: Figure 7.14 Taxonomy for the words in set 2
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.14 组2中词汇的分类
- en: The movies in the set are highlighted. We have categorized the movies based
    on their genre and the location of shoot. Movies like *Godfather* and *Goodfellas*
    belong to the Gangster genre, and they are both shot in the location New York.
    Movies like *Batman* and *Avengers* are Superhero movies. *Batman* is based in
    the location *Gotham*, which is a fictitious place loosely based on *New York*.
    It is worth highlighting that such nuances and meanings for words are language-
    and context-dependent, and, therefore, we need a good understanding of this before
    we set out to interpret semantic meaning.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 该组中的电影被突出显示。我们根据电影的类型和拍摄地点对电影进行了分类。像*教父*和*好家伙*这样的电影属于黑帮类型，它们都在纽约拍摄。像*蝙蝠侠*和*复仇者联盟*这样的电影是超级英雄电影。*蝙蝠侠*以*哥谭*为基地，这是一个基于纽约的虚构地点。值得注意的是，这些词汇的细微差别和意义是语言和上下文相关的，因此在我们开始解释语义意义之前，我们需要对此有良好的理解。
- en: 7.4.1 Measuring similarity
  id: totrans-137
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.1 测量相似度
- en: Now that we have the words of interest, how do we quantify similarity between
    them? We are specifically interested in measuring similarity between representations
    of words or word embeddings. For ease of visualization, let’s first consider a
    simple example of word embeddings of size 2\. Suppose that we have two words,
    *Basketball* and *Football*, in this word embedding space, as shown in figure
    7.15\. These two words are represented in the figure as vectors W[1] and W[2],
    respectively.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经获得了感兴趣的单词，我们如何量化它们之间的相似度？我们特别感兴趣的是测量单词或词嵌入之间的表示相似度。为了便于可视化，让我们首先考虑一个大小为2的简单词嵌入示例。假设我们有两个单词，*篮球*和*足球*，在这个词嵌入空间中，如图7.15所示。这两个单词在图中分别表示为向量W[1]和W[2]。
- en: '![](../Images/CH07_F15_Thampi.png)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F15_Thampi.png)'
- en: Figure 7.15 An illustration of measuring similarity between word embeddings
    in 2-D space
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.15展示了在2-D空间中测量词嵌入之间相似性的示意图
- en: 'One way of measuring similarity between the word vectors W[1] and W[2] is to
    look at how close they are in the 2-D embedding space. The similarity measure
    should have the property that if the word vectors are close together, then they
    are more similar. If they are further apart, then they are less similar. A good
    metric that has this property is the cosine of the angle between the two vectors—cos(θ).
    This measurement is called cosine similarity. The mathematical formula for cosine
    similarity given word vectors W[1] and W[2] is shown next:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 测量词向量W[1]和W[2]之间相似度的一种方法是在2-D嵌入空间中观察它们有多接近。相似度度量应该具有以下属性：如果词向量彼此接近，则它们更相似。如果它们相距更远，则它们不太相似。具有这种属性的良好度量是两个向量之间角度的余弦值—cos(θ)。这种测量称为余弦相似度。给定词向量W[1]和W[2]的余弦相似度的数学公式如下：
- en: '![](../Images/CH07_F15_Thampi_equation01.png)'
  id: totrans-142
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F15_Thampi_equation01.png)'
- en: It is essentially the dot product of the word vectors divided by the product
    of the Euclidean norm, or magnitude, of the two vectors.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这实际上是两个向量欧几里得范数（或大小）的乘积除以词向量点积。
- en: 'Using `gensim`, we can easily obtain the words that are most similar to a given
    word as follows. In section 7.3.3, we saw how to load the GloVe word embedding
    using `gensim`. Once the embeddings have been initialized, we can obtain the top
    five most similar words for the first set of words using the following code:'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `gensim`，我们可以轻松地获取与给定单词最相似的单词，如下所示。在第7.3.3节中，我们看到了如何使用 `gensim` 加载GloVe词嵌入。一旦嵌入被初始化，我们可以使用以下代码获取第一组单词的前五个最相似单词：
- en: '[PRE6]'
  id: totrans-145
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ① Initializes an array with the first set of words
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ① 初始化一个数组，包含第一组单词
- en: ② We are interested in the top five most similar words.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: ② 我们对前五个最相似的单词感兴趣。
- en: ③ Initializes an array to store the most similar words
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化一个数组来存储最相似的单词
- en: ④ Iterates through each word
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 遍历每个单词
- en: ⑤ Gets the top five most similar words from the gensim model
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 从gensim模型中获取前五个最相似单词
- en: ⑥ Stores the similar words in array and print the results
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 将相似单词存储在数组中并打印结果
- en: The output of this code is summarized in table 7.2\. The top row consists of
    the words in the set. Each column in the table shows the top five words that are
    similar to the words in the topmost row of that column. The cosine similarity
    measure is also shown in parenthesis for the similar words. We can see from the
    table that the GloVe word embedding has indeed learned words that are semantically
    similar in meaning. The first column, for instance, shows all the words that are
    similar to *basketball*, and they are all sports. The second column shows all
    the words that are similar to *Lebron*, and they are all sports personalities
    that play basketball. The third column shows all sports personalities that play
    football or soccer. The fourth column shows companies that are similar to *Facebook*,
    that is, internet or web-based social media companies. The last column shows all
    the words that are similar to *Media*. As an exercise, do a similar analysis for
    the second set of words. The solution can be found in the GitHub repository associated
    with this book.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 本代码的输出总结在表7.2中。表的最上面一行包含该集合中的单词。表中的每一列显示与该列最上面一行单词相似的五个单词。相似单词的余弦相似度度量也以括号形式显示。从表中我们可以看出，GloVe词嵌入确实学习了在意义上语义相似的单词。例如，第一列显示了所有与*篮球*相似的单词，它们都是体育项目。第二列显示了所有与*勒布朗*相似的单词，它们都是打篮球的体育人物。第三列显示了所有踢足球或足球的体育人物。第四列显示了与*Facebook*相似的公司，即互联网或基于网络的社交媒体公司。最后一列显示了所有与*媒体*相似的单词。作为练习，对第二组单词进行类似的分析。解决方案可以在与本书相关的GitHub存储库中找到。
- en: Table 7.2 Top five similar words for the words in set 1
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 表7.2 集合1中单词的前五个相似单词
- en: '| Basketball | Lebron | Ronaldo | Facebook | Media |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 篮球 | 勒布朗 | 罗纳尔多 | Facebook | 媒体 |'
- en: '| Football (0.86) | Dwyane (0.79) | Ronaldinho (0.86) | Twitter (0.92) | News
    (0.77) |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 足球 (0.86) | 杜威恩 (0.79) | 罗纳尔迪尼奥 (0.86) | 推特 (0.92) | 新闻 (0.77) |'
- en: '| Hockey (0.8) | Shaquille (0.75) | Rivaldo (0.85) | MySpace (0.9) | Press
    (0.75) |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 冰球 (0.8) | 沙奎尔 (0.75) | 里瓦尔多 (0.85) | MySpace (0.9) | 新闻 (0.75) |'
- en: '| Soccer (0.8) | Bosh (0.72) | Beckham (0.84) | YouTube (0.81) | Television
    (0.75) |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 足球 (0.8) | 波什 (0.72) | 贝克汉姆 (0.84) | YouTube (0.81) | 电视 (0.75) |'
- en: '| NBA (0.78) | O’Neal (0.68) | Cristiano (0.84) | Google (0.75) | TV (0.73)
    |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| NBA (0.78) | 奥尼尔 (0.68) | 克里斯蒂亚诺 (0.84) | 谷歌 (0.75) | 电视 (0.73) |'
- en: '| Baseball (0.76) | Carmelo (0.68) | Robinho (0.82) | Web (0.74) | Internet
    (0.72) |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 棒球 (0.76) | 卡梅隆 (0.68) | 罗比尼奥 (0.82) | 网络 (0.74) | 互联网 (0.72) |'
- en: 'Let’s now also visualize the cosine similarity between the words in the first
    set. The following code shows how to compute the cosine similarity between pairs
    of words and how to visualize them:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们也可视化第一组单词之间的余弦相似度。以下代码显示了如何计算单词对的余弦相似度以及如何可视化它们：
- en: '[PRE7]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ① Imports Pandas to store the cosine similarity of word pairs in a DataFrame
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ① 导入Pandas以在DataFrame中存储单词对的余弦相似度
- en: ② Imports the cosine_similarity helper function from Scikit-Learn
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ② 从Scikit-Learn导入余弦相似度辅助函数
- en: ③ Imports the visualization-related libraries
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 导入可视化相关的库
- en: ④ Initializes the first set of words
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 初始化第一组单词
- en: ⑤ Creates an array with word pairs based on the initialized set of words
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 根据初始化的单词集创建单词对数组
- en: ⑥ Computes the cosine similarity for word pairs and stores it in an array
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ⑥ 计算单词对的余弦相似度并将其存储在数组中
- en: ⑦ Creates a DataFrame with the results
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ⑦ 创建包含结果的DataFrame
- en: ⑧ Uses the DataFrame to plot a bar chart
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ⑧ 使用DataFrame绘制条形图
- en: The resulting plot is shown in figure 7.16\. We can observe from the figure
    that *Basketball* and *Lebron* are much more similar to each other than to any
    other word. Also, the word *Basketball* is more similar to *Ronaldo* than to *Facebook*
    and *Media*, because we know from our taxonomy in figure 7.13 that *Basketball*
    and *Ronaldo* are linked to the category Sport. Using the taxonomy, we can make
    similar observations for the other pairs of words as well. The word *Facebook*,
    for instance, is much more similar to the word *Media* than any other word, because
    Facebook is a social media company.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在图7.16中。我们可以从图中观察到，*篮球*和*勒布朗*之间的相似度比与其他任何单词都要高。此外，单词*篮球*与*罗纳尔多*的相似度比与*Facebook*和*媒体*的相似度更高，因为我们从图7.13中的分类法中知道*篮球*和*罗纳尔多*与体育类别相关联。使用分类法，我们还可以对其他单词对进行类似的观察。例如，单词*Facebook*与单词*媒体*的相似度比与其他任何单词都要高，因为Facebook是一家社交媒体公司。
- en: '![](../Images/CH07_F16_Thampi.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F16_Thampi.png)'
- en: Figure 7.16 Visualization of cosine similarity of GloVe embeddings for word
    pairs in set 1
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.16 集合1中GloVe嵌入的单词对余弦相似度可视化
- en: As an exercise, write the code to visualize the cosine similarity for the pairs
    of movies in the second set. You can access the source code from the GitHub repository
    associated with this book, and the resulting plot is shown in figure 7.17\. We
    can observe from the figure that the two gangster movies, *Godfather* and *Goodfellas*,
    are more similar to each other than they are to the superhero movies *Batman*
    and *Avengers*. Similarly, the Superhero movies are closer together than they
    are to the Gangster movies. We can also see that *Godfather* and *Goodfellas*
    are more similar to *Batman* than *Avengers*. This could be because the movies
    are based in locations that are connected, as we established in our taxonomy in
    figure 7.14.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 作为练习，编写代码以可视化第二组电影对之间的余弦相似度。你可以从与本书相关的GitHub仓库中获取源代码，结果图如图7.17所示。我们可以从图中观察到，两部黑帮电影《教父》和《好家伙》彼此之间比它们与超级英雄电影《蝙蝠侠》和《复仇者联盟》更相似。同样，超级英雄电影彼此之间比它们与黑帮电影更接近。我们还可以看到，《教父》和《好家伙》与《蝙蝠侠》比与《复仇者联盟》更相似。这可能是因为电影基于相互连接的位置，正如我们在图7.14中的分类法中确立的那样。
- en: '![](../Images/CH07_F17_Thampi.png)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F17_Thampi.png)'
- en: Figure 7.17 Visualization of the cosine similarity of GloVe embeddings of various
    movie pairs in set 2
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.17展示了第二组中各种电影对GloVe嵌入的余弦相似度可视化
- en: 'We now have a way of measuring similarity between word embeddings, using the
    cosine similarity measure. Using a specific set of words for evaluation and their
    corresponding taxonomy, we have also validated that the GloVe word embeddings
    with 100 dimensions capture the semantic meaning of words really well. Let’s now
    see how we can come up with a visualization of the word embeddings in 2-D space,
    similar to the one illustrated in figure 7.15, without losing any of the semantic
    meaning. This is going to be the focus of the next two sections. You will specifically
    learn about two techniques: principal component analysis (PCA) and t-distributed
    stochastic neighbor embedding (t-SNE).'
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了测量词嵌入之间相似性的方法，即使用余弦相似度度量。使用特定的单词集及其对应的分类法进行评估，我们还验证了100维的GloVe词嵌入能够很好地捕捉单词的语义意义。现在让我们看看如何将词嵌入可视化在2维空间中，类似于图7.15中所示，同时不失任何语义意义。这将是下一两个章节的重点。你将特别学习两种技术：主成分分析（PCA）和t分布随机邻域嵌入（t-SNE）。
- en: 7.4.2 Principal component analysis (PCA)
  id: totrans-177
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.2 主成分分析（PCA）
- en: Principal component analysis (PCA) is a powerful technique for reducing the
    dimensionality of a dataset. Because we are dealing with word embeddings with
    100 dimensions, we want to reduce the dimensionality to 2 so that we can easily
    visualize the dataset. We want to reduce the dimensionality and, at the same time,
    capture as much of the variation or the semantic information as possible. Let’s
    see PCA in action by looking at a simple example. For the sake of illustration,
    we will look at word embeddings of size 2 and see how we can use PCA to reduce
    the dimensionality from 2 down to 1\. Figure 7.18 shows four words placed on a
    2-D plane. For ease of visualization, we are assuming that the embedding size
    is 2\. The goal is to visualize the word embeddings in one dimension—on a 1-D
    line. We can see that words 1 and 2 (*Doctor* and *Nurse*) are semantically similar
    because they are closer together in the 2-D space. Words 3 and 4 (*Athletics*
    and *Athlete*) are also semantically similar. Word pairs 1 and 2, however, are
    further away from word pairs 3 and 4 because they are not semantically similar.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 主成分分析（PCA）是一种强大的技术，用于降低数据集的维度。因为我们处理的是100维的词嵌入，我们希望将维度降低到2，这样我们就可以轻松地可视化数据集。我们希望在降低维度的同时，尽可能多地捕捉到变化或语义信息。让我们通过一个简单的例子来看看PCA的实际应用。为了说明，我们将查看大小为2的词嵌入，并看看我们如何使用PCA将维度从2降低到1。图7.18显示了四个词放置在二维平面上。为了便于可视化，我们假设嵌入大小为2。目标是可视化词嵌入在一维上——在一条1维线上。我们可以看到，单词1和2（“Doctor”和“Nurse”）在二维空间中彼此更接近，因此语义上相似。单词3和4（“Athletics”和“Athlete”）在语义上也是相似的。然而，单词对1和2与单词对3和4相比更远，因为它们在语义上不相似。
- en: '![](../Images/CH07_F18_Thampi.png)'
  id: totrans-179
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F18_Thampi.png)'
- en: Figure 7.18 An illustration of four words in an embedding space of size 2
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.18 在大小为2的嵌入空间中四个词的示意图
- en: The first step of PCA is to take the mean of the words across all dimensions
    and subtract the mean from the word embeddings. This is shown in figure 7.19,
    where the mean is represented by a large cross. The purpose of this transformation
    is to center the words around the mean, that is, place the mean of the data at
    the origin. By centering the word embeddings on the mean, we still preserve the
    distances between the words in 2-D space and, therefore, their semantic meaning.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: PCA的第一步是取所有维度的词的平均值，并从词嵌入中减去平均值。这如图7.19所示，其中平均值由一个大十字表示。这种转换的目的是将词围绕平均值中心化，即把数据的平均值放在原点。通过在平均值上中心化词嵌入，我们仍然保留了词在2-D空间中的距离，因此也保留了它们的语义意义。
- en: '![](../Images/CH07_F19_Thampi.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F19_Thampi.png)'
- en: Figure 7.19 An illustration of computing the mean and centering the words around
    the mean
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.19 计算平均值并将词围绕平均值中心化的说明
- en: Because we are interested in visualizing the word embeddings on a line, the
    next step of PCA is to fit a line through the word embeddings. The line of best
    fit is the one that minimizes the perpendicular distance between each word and
    the line. In other words, the goal is to minimize the projected distance between
    the words and the line or maximize the distance between the origin and the projection
    of each word on the line. Maximizing the distance between the projections on the
    origin will ensure that as much of the variation in the data is preserved as possible.
    This is shown in figure 7.20\. The line of best fit is also called the principal
    component. We are interested only in visualizing the words in 1-D, so there will
    be one only principal component.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们对在一条线上可视化词嵌入感兴趣，PCA的下一步就是通过词嵌入拟合一条线。最佳拟合线是使每个词与线之间的垂直距离最小化的线。换句话说，目标是使词与线之间的投影距离最小化或最大化原点到每个词在直线上的投影的距离。最大化原点到投影的距离将确保尽可能多地保留数据的变化。这如图7.20所示。最佳拟合线也称为主成分。我们只对在1-D中可视化词感兴趣，所以只有一个主成分。
- en: '![](../Images/CH07_F20_Thampi.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F20_Thampi.png)'
- en: Figure 7.20 An illustration of principal component
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.20 主成分的说明
- en: The final step is to project each word onto the principal component. This will
    serve as our visualization of the word embeddings in 1-D, as shown in figure 7.21.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是将每个词投影到主成分上。这将作为我们在1-D中可视化词嵌入的方式，如图7.21所示。
- en: '![](../Images/CH07_F21_Thampi.png)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F21_Thampi.png)'
- en: Figure 7.21 An illustration of word embeddings projected onto the principal
    component
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.21 将词嵌入投影到主成分的说明
- en: Now that we have an intuition of how PCA works, let’s extend the technique to
    multiple dimensions. Let’s represent all the word embeddings by the matrix *X*
    where the number of rows is equal to the number of words in the vocabulary and
    the number of columns is equal to the embedding size. Let’s represent the embedding
    size as *n*. The goal is to reduce the dimension of the words to size *k*, where
    for the purposes of visualization is typically 2 or 3.
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经对PCA的工作原理有了直观的了解，让我们将这项技术扩展到多个维度。让我们用矩阵*X*来表示所有的词嵌入，其中行数等于词汇表中的词的数量，列数等于嵌入大小。让我们用*n*来表示嵌入大小。目标是减少词的维度到大小*k*，在可视化的目的中通常是2或3。
- en: 'As we’ve seen through the visual example, the first step is to center the data
    to the mean. This is shown by the following equation, where the mean is subtracted
    from the embedding matrix *X*. The mean center data is represented by matrix *U*:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如通过可视化示例所见，第一步是将数据中心化到平均值。这由以下方程表示，其中平均值从嵌入矩阵*X*中减去。均值中心数据由矩阵*U*表示：
- en: '*U* = *X* – *X̄*'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '*U* = *X* – *X̄*'
- en: 'The next step is to compute the covariance of matrix *U*. This is shown by
    the next equation where the covariance matrix is represented by matrix *V*. The
    purpose of computing the covariance of matrix *U* is to estimate the variance
    across each of the embedding dimensions in the mean-centered data:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是计算矩阵*U*的协方差。这通过下一个方程表示，其中协方差矩阵由矩阵*V*表示。计算矩阵*U*的协方差的目的在于估计均值中心化数据中每个嵌入维度的方差：
- en: '*V* = *U^T**U*'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '*V* = *U^T**U*'
- en: 'Once you have the estimate of the variance, the next step is to compute the
    eigenvalues and eigenvectors for matrix *V* by solving the following characteristic
    equation. By solving for *λ*, we can obtain the roots for the equation, which
    will give us the eigenvalues. Note that in the next equation, “det” stands for
    determinant and the matrix *I* is the identity matrix. Once we have the eigenvalues,
    we can obtain the corresponding eigenvectors:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你有了方差的估计，下一步就是通过解以下特征方程来计算矩阵*V*的特征值和特征向量。通过求解*λ*，我们可以得到方程的根，这将给我们特征值。注意，在下一个方程中，“det”代表行列式，而矩阵*I*是单位矩阵。一旦我们有了特征值，我们就可以获得相应的特征向量：
- en: det(*V* – *λI*) = 0
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: det(*V* – *λI*) = 0
- en: 'The eigenvectors essentially give us the principal components. The magnitude
    of the eigenvalue gives us an estimate of the amount of variation captured by
    each of the principal components. We should then sort the vectors in descending
    order of eigenvalues and pick the top *k* principal components to project our
    data to. The top *k* principal components will capture as much of the variation
    in the data as possible. Let’s represent the matrix with the top *k* principal
    components (or eigenvectors) as *W*. The final step is to project the original
    word embeddings in *n*-dimensional space to the *k*-dimensional space by applying
    the following equation:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 特征向量基本上给我们主成分。特征值的幅度给我们一个估计，即每个主成分捕获的变异量。然后我们应该按特征值降序排序向量，并选择前*k*个主成分来投影我们的数据。前*k*个主成分将尽可能多地捕获数据中的变异。让我们用具有前*k*个主成分（或特征向量）的矩阵*W*来表示这个矩阵。最后一步是通过应用以下方程将原始单词嵌入从*n*-维空间投影到*k*-维空间：
- en: '*Y* = *W^T**X*'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '*Y* = *W^T**X*'
- en: 'Let’s now see PCA in action on the GloVe word embeddings. The first step is
    to prepare the data where we extract the word embeddings for the words that we
    are interested in visualizing. This is shown in the next code snippet where we
    extract the word embeddings for the words in set 1 and their corresponding top
    five similar words:'
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看PCA在GloVe单词嵌入上的实际应用。第一步是准备数据，其中我们提取了我们感兴趣可视化的单词的单词嵌入。这将在下一个代码片段中展示，其中我们提取了集合1中的单词及其对应的五个最相似单词的单词嵌入：
- en: '[PRE8]'
  id: totrans-200
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ① Creates lists with main words and similar words to visualize
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建包含主要单词和相似单词的列表以进行可视化
- en: ② Extracts the word embeddings for words to visualize
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: ② 提取用于可视化的单词嵌入
- en: 'Once we have prepared the data, we can run PCA and obtain the projections of
    the word embeddings in the lower-dimensional space. For ease of visualization,
    we will set the number of principal components to 2\. We can use the PCA implementation
    provided by the Scikit-Learn library. The following code shows how to obtain the
    principal components and then project the data onto them:'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们准备好了数据，我们就可以运行PCA，并在低维空间中获取单词嵌入的投影。为了便于可视化，我们将主成分的数量设置为2。我们可以使用Scikit-Learn库提供的PCA实现。以下代码展示了如何获取主成分并将数据投影到它们上：
- en: '[PRE9]'
  id: totrans-204
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ① Imports the PCA class from Scikit-Learn
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从Scikit-Learn导入PCA类
- en: ② Initializes the PCA class with two principal components
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: ② 使用两个主成分初始化PCA类
- en: ③ Sets the random state and obtains the best fit for the word vectors
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 设置随机状态并获得单词向量的最佳拟合
- en: ④ Projects the word vectors onto the principal components
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 将单词向量投影到主成分上
- en: ⑤ Creates a dictionary mapping from each word to its PCA word embeddings
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 创建一个将每个单词映射到其PCA单词嵌入的字典
- en: 'Once we have the projections of the word embeddings in 2-D space, we can easily
    visualize it using the Matplotlib and Seaborn libraries, as shown next:'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们在2维空间中有了单词嵌入的投影，我们就可以很容易地使用Matplotlib和Seaborn库进行可视化，如下所示：
- en: '[PRE10]'
  id: totrans-211
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ① Creates a DataFrame with 2-D PCA coordinates for each word
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ① 创建一个DataFrame，包含每个单词的2维PCA坐标
- en: ② Creates a scatterplot
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ② 创建散点图
- en: ③ Adds a legend and annotations for the scatterplot
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 为散点图添加图例和注释
- en: The resulting plot is shown in figure 7.22\. The main words in set 1 are shown
    in the legend, and their top five most similar words are illustrated using the
    symbol corresponding to each word. The word *Basketball*, for instance, is represented
    by a circle, and the word *Media* is represented by a diamond. Let’s take a moment
    to admire the output of the PCA technique. We are now able to visualize the original
    100-dimensional word embeddings in two dimensions! But does the PCA representation
    still preserve the semantic meaning captured in 100 dimensions? In figure 7.22,
    we do see the words similar to the main word clustered together, except for the
    word, *lebron*. Some basketball personalities like *bosh*, *dwyane*, and *carmelo*
    are closer to the football personalities than to their basketball peers.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 结果图显示在图7.22中。集合1中的主要单词在图例中显示，它们最相似的五个单词使用对应于每个单词的符号进行说明。例如，“Basketball”用圆圈表示，而“Media”用菱形表示。让我们花点时间来欣赏PCA技术的输出。现在我们能够将原始的100维单词嵌入可视化到二维空间中！但是，PCA表示是否仍然保留了在100维中捕获的语义意义呢？在图7.22中，我们可以看到与主要单词相似的单词聚集在一起，除了单词“lebron”。一些篮球人物如“bosh”、“dwyane”和“carmelo”与足球人物比与其篮球同伴更近。
- en: '![](../Images/CH07_F22_Thampi.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F22_Thampi.png)'
- en: Figure 7.22 Visualizing semantic similarity of GloVe word embeddings for words
    in set 1 using PCA
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.22 使用PCA可视化集合1中GloVe单词嵌入的语义相似性
- en: 'This is expected because we may not be capturing as much of the variation in
    the original dataset in just two dimensions. We can easily check this by running
    the following line of code:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 这是预料之中的，因为我们可能无法在仅仅两个维度中捕捉到原始数据集中的所有变化。我们可以通过运行以下代码行来轻松检查这一点：
- en: '[PRE11]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This code outputs the percentage of variation captured in each of the principal
    components. If we sum them up, we get roughly 49%. This means that by projecting
    the word embeddings to just two principal components, we are able to capture 49%
    of the variation in the data. As an exercise, try training a PCA with three principal
    components to see if a major chunk of the variance in the data can be captured.
    If so, try visualizing the embeddings in 3-D to see if the issues observed in
    2-D are resolved.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 此代码输出每个主成分中捕获的变化百分比。如果我们把它们加起来，我们得到大约49%。这意味着通过将单词嵌入投影到仅两个主成分上，我们能够捕捉到数据中49%的变化。作为一个练习，尝试用三个主成分训练PCA，看看数据中的大部分方差是否可以被捕捉。如果是这样，尝试将嵌入可视化到三维空间中，看看二维中观察到的問題是否得到解决。
- en: Although PCA is a powerful technique, it does suffer from some a major drawback.
    It assumes that the dataset or the word embeddings can be modeled linearly. This
    may not be the case for most of the datasets that we deal with. In the next section,
    we will learn about an even more powerful and popular technique called t-SNE that
    can generalize to nonlinear structures.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然PCA是一种强大的技术，但它确实存在一些主要的缺点。它假设数据集或单词嵌入可以线性建模。对于我们处理的大多数数据集来说，这可能并不成立。在下一节中，我们将学习一个更强大且更受欢迎的技术，称为t-SNE，它可以推广到非线性结构。
- en: 7.4.3 t-distributed stochastic neighbor embedding (t-SNE)
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.3 t分布随机邻域嵌入（t-SNE）
- en: t-SNE falls under the broad class of machine learning techniques called *manifold
    learning*, where the objective is to learn nonlinear structures from higher-dimensional
    data in lower dimensions. This technique is one of the most popular choices for
    visualizing higher-dimensional data. Let’s see it in action using a simple two-dimensional
    dataset where the goal is to visualize it in one dimension. In figure 7.23, we
    see the familiar example of four words in 2-D space on the left. The first step
    is to construct a similarity table for all pairs of words. This similarity table
    will give us a measure of similarity, or the probability of pairs of words being
    neighbors in the high-dimensional embedding space. Another way to look at it is
    to calculate the joint probability distribution for words in the high-dimensional
    embedding space. We will see how to do this mathematically in a bit.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: t-SNE属于被称为**流形学习**的机器学习技术广泛类别，其目标是学习从高维数据中提取低维的非线性结构。这项技术是可视化高维数据中最受欢迎的选择之一。让我们通过一个简单的二维数据集来观察它的实际应用，该数据集的目标是将数据可视化到一维。在图7.23中，我们看到了左侧2-D空间中四个单词的熟悉示例。第一步是为所有单词对构建一个相似性表。这个相似性表将给我们一个相似度的度量，或者在高维嵌入空间中单词对成为邻居的概率。另一种看待它的方法是计算高维嵌入空间中单词的联合概率分布。我们将在稍后展示如何从数学上完成这个操作。
- en: '![](../Images/CH07_F23_Thampi.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F23_Thampi.png)'
- en: Figure 7.23 Constructing a similarity table for word embeddings in high-dimensional
    space
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.23 在高维空间中构建单词嵌入的相似性表
- en: The next step is to randomly place all the words on a line, because we are interested
    in visualizing the word embeddings in 1-D space. This is shown on the left in
    figure 7.24\. Once we have placed the words randomly on the line, we should construct
    a similarity table for the words randomly represented on that 1-D space. This
    is shown on the right in figure 7.24\. The entries in the table that are different
    from the higher-dimensional joint probability distribution are highlighted. We
    will see how to mathematically compute this joint probability distribution for
    the lower-dimensional space shortly.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是将所有单词随机放置在一条线上，因为我们感兴趣的是在 1-D 空间中可视化单词嵌入。这如图 7.24 左侧所示。一旦我们将单词随机放置在线上，我们应该为在该
    1-D 空间中随机表示的单词构建一个相似性表。这如图 7.24 右侧所示。表中与高维联合概率分布不同的条目被突出显示。我们将在稍后看到如何数学地计算这个低维空间的联合概率分布。
- en: '![](../Images/CH07_F24_Thampi.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F24_Thampi.png)'
- en: Figure 7.24 Randomly placing words in the lower dimension and the corresponding
    similarity table
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.24 随机放置单词在低维空间及其对应的相似性表
- en: The final step is the t-SNE learning process, as shown in figure 7.25\. We must
    feed the joint probability distributions of the random lower-dimensional representation
    and the higher-dimensional representation into the learning algorithm. The objective
    of the learning algorithm is to update the lower-dimensional representation such
    that both probability distributions are similar. This will then give us a lower-dimensional
    visualization that preserves the probability distributions, or similarities, from
    the higher-dimensional space.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 最后一步是 t-SNE 学习过程，如图 7.25 所示。我们必须将随机低维表示和更高维表示的联合概率分布输入到学习算法中。学习算法的目标是更新低维表示，使得两个概率分布相似。这将给我们一个低维可视化，它保留了来自高维空间中的概率分布或相似性。
- en: '![](../Images/CH07_F25_Thampi.png)'
  id: totrans-230
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F25_Thampi.png)'
- en: Figure 7.25 t-SNE learning algorithm
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 图 7.25 t-SNE 学习算法
- en: 'Let’s now look at it mathematically. The first step is to construct a similarity
    table, or joint probability distribution, for the words in the higher-dimensional
    embedding space. For each word, we can project a Gaussian distribution centered
    on that word such that words that are closer to it have a higher probability and
    words that are further away from it have a lower probability. This is shown in
    the following equation, which computes the probability of a word x[j] being close
    to x[i]. The numerator is the Gaussian distribution centered on the word x[i]
    with a standard deviation of σ. The standard deviation σ is a hyperparameter for
    t-SNE, and we will see how to set this hyperparameter shortly. The denominator
    is a normalization factor to ensure that the probabilities are of a similar range
    for clusters of words with different densities:'
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们从数学的角度来分析。第一步是构建一个相似性表，或者说是联合概率分布，用于更高维嵌入空间中的单词。对于每个单词，我们可以投影一个以该单词为中心的高斯分布，使得离它更近的单词有更高的概率，而离它更远的单词有更低的概率。这在上面的方程中显示，该方程计算单词
    x[j] 接近 x[i] 的概率。分子是以单词 x[i] 为中心的高斯分布，标准差为 σ。标准差 σ 是 t-SNE 的超参数，我们将在稍后看到如何设置这个超参数。分母是一个归一化因子，以确保不同密度的单词簇的概率范围相似：
- en: '![](../Images/CH07_F25_Thampi_equation06.png)'
  id: totrans-233
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F25_Thampi_equation06.png)'
- en: 'Using this equation, we have a risk of the probability of word x[j] being a
    neighbor of word x[i] being different from the probability of word x[i] being
    a neighbor of x[j], because the two conditional probabilities come from different
    distributions. To ensure that the similarity measure is commutative, we will compute
    the final probability of two words x[i] and x[j] being neighbors as follows:'
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 使用这个方程，我们存在一个风险，即单词 x[j] 成为单词 x[i] 邻居的概率与单词 x[i] 成为 x[j] 邻居的概率不同，因为这两个条件概率来自不同的分布。为了确保相似度度量是可交换的，我们将计算两个单词
    x[i] 和 x[j] 成为邻居的最终概率如下：
- en: '![](../Images/CH07_F25_Thampi_equation07.png)'
  id: totrans-235
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F25_Thampi_equation07.png)'
- en: 'Once you have computed the joint probability distribution for the higher-dimensional
    embedding space, the next step is to place the words randomly on the lower-dimensional
    space. We should then compute the joint probability distribution for the lower-
    dimensional representation using the following equation. The equation essentially
    computes the probability that two words in the lower dimension represented as
    y[i] and y[j] are neighbors:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你计算了高维嵌入空间的联合概率分布，下一步是将单词随机放置在低维空间中。然后，我们应该使用以下方程计算低维表示的联合概率分布。该方程本质上计算了两个单词在低维表示中作为y[i]和y[j]的概率是邻居的概率：
- en: '![](../Images/CH07_F25_Thampi_equation08.png)'
  id: totrans-237
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F25_Thampi_equation08.png)'
- en: Note that a different distribution is used for the lower-dimensional representation.
    The numerator in the equation essentially is a t-distribution, hence the name
    t-SNE. Figure 7.26 shows the difference between the Gaussian distribution and
    the t-distribution. We can see that the t-distribution has a heavier tail (where
    the probability scores are not negligible for extreme values) on the right than
    the Gaussian distribution. We are exploiting this property of the t-distribution
    for the lower-dimensional space to ensure that points that may be moderately spaced
    in the higher dimension are not clumped together in the lower dimension.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，低维表示使用了不同的分布。方程中的分子本质上是一个t分布，因此得名t-SNE。图7.26显示了高斯分布和t分布之间的差异。我们可以看到，t分布的右侧（极端值概率分数不可忽略）比高斯分布有更重的尾部。我们正在利用t分布的这一特性来确保在更高维度的空间中可能适度分散的点在低维空间中不会聚集在一起。
- en: Once we have the joint distributions for both the higher-dimensional and lower-dimensional
    representations, the last step is to train an algorithm to update the lower-dimensional
    representation such that both distributions are similar. This optimization can
    be done by quantifying the gap between both the distributions. We can use the
    Kullback–Leibler (KL) divergence metric for this purpose.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们有了高维和低维表示的联合分布，最后一步是训练一个算法来更新低维表示，使得两个分布相似。可以通过量化两个分布之间的差距来完成此优化。我们可以使用Kullback–Leibler（KL）散度指标来完成此目的。
- en: '![](../Images/CH07_F26_Thampi.png)'
  id: totrans-240
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F26_Thampi.png)'
- en: Figure 7.26 A Gaussian distribution vs. t-distribution
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.26 高斯分布与t分布的比较
- en: 'The KL divergence is a measure of the entropy, or difference, between two distributions.
    The higher the value, the greater the difference. To be more precise, the KL divergence
    could range from 0 for identical distributions to infinity for vastly different
    distributions. The KL divergence metric can be computed as follows:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: KL散度是衡量两个分布之间的熵或差异的度量。值越高，差异越大。更精确地说，KL散度可以从相同分布的0到差异极大的分布的无限大。KL散度指标可以按以下方式计算：
- en: '![](../Images/CH07_F26_Thampi_equation09.png)'
  id: totrans-243
  prefs: []
  type: TYPE_IMG
  zh: '![图像](../Images/CH07_F26_Thampi_equation09.png)'
- en: The objective of the learning algorithm is to determine the distribution for
    the lower-dimensional representation such that the KL divergence metric is minimized.
    We can do this optimization by applying gradient descent and iteratively updating
    the lower-dimensional representation. The entire t-SNE algorithm has been implemented
    in the Scikit-Learn library.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 学习算法的目标是确定低维表示的分布，以使KL散度指标最小化。我们可以通过应用梯度下降和迭代更新低维表示来完成此优化。整个t-SNE算法已在Scikit-Learn库中实现。
- en: 'Before jumping into the code, there is one detail that we glossed over. Note
    that when computing the joint probability distribution for the higher-dimensional
    representation, we fit a Gaussian centered on each word with a standard deviation
    of *σ*. This standard deviation is an important hyperparameter for t-SNE. It is
    referred to as *perplexity*, which is a rough estimate of the number of close
    neighbors each word has. As we will see later, the choice of perplexity will drastically
    change the visualization of the word embeddings and is, therefore, an important
    hyperparameter. We can train t-SNE on the GloVE word embeddings using the next
    code. We are using the words from set 1 and their associated top five most similar
    words:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在跳入代码之前，有一个细节我们略过了。注意，在计算更高维表示的联合概率分布时，我们对每个词拟合了一个以σ为标准差的Gaussian分布。这个标准差是t-SNE的一个重要超参数，被称为“困惑度”，它是每个词有多少个近邻的大致估计。正如我们稍后将会看到的，困惑度的选择将极大地改变词嵌入的可视化，因此它是一个重要的超参数。我们可以使用以下代码在GloVE词嵌入上训练t-SNE。我们使用集合1中的单词及其相关的最相似的前五个单词：
- en: '[PRE12]'
  id: totrans-246
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ① Imports the TSNE class from Scikit-Learn
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: ① 从Scikit-Learn导入TSNE类
- en: ② Initializes the t-SNE hyperparameters
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: ② 初始化t-SNE超参数
- en: ③ Initializes the TSNE class and trains the model using the word vectors
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: ③ 初始化TSNE类并使用词向量训练模型
- en: ④ Obtains the t-SNE word embeddings in 2-D space
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: ④ 在2-D空间中获取t-SNE词嵌入
- en: ⑤ Creates a mapping from each word to its t-SNE embeddings
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: ⑤ 将每个词映射到其t-SNE嵌入
- en: Note that we have set the perplexity to 10\. We can reuse the code from the
    previous section on PCA to visualize the lower-dimensional t-SNE embeddings. The
    resulting figure is shown in figure 7.27.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们将困惑度设置为10。我们可以重用前节中PCA的代码来可视化低维t-SNE嵌入。结果图如图7.27所示。
- en: '![](../Images/CH07_F27_Thampi.png)'
  id: totrans-253
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F27_Thampi.png)'
- en: Figure 7.27 Visualizing semantic similarity of GloVe word embeddings for words
    in set 1 using t-SNE
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27 使用t-SNE可视化集合1中单词的GloVe词嵌入的语义相似性
- en: The visualization shown in figure 7.27 does look better qualitatively than PCA.
    We do see the basketball personalities clustered together and distinct from the
    football personalities cluster. This is still a qualitative assessment, and we
    will see how to validate these visualizations quantitatively in the following
    section.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.27所示的可视化在定性上看起来比PCA更好。我们确实看到篮球人物被聚集在一起，并且与足球人物集群明显不同。这仍然是一种定性评估，我们将在下一节中看到如何定量地验证这些可视化。
- en: Let’s see what happens when we set the perplexity to a large value, say, 100\.
    As an exercise, retrain the t-SNE model using a perplexity of 100 and visualize
    the resulting word embeddings. You can see the code in the GitHub repository associated
    with this book. The resulting plot is shown in figure 7.28.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看当我们将困惑度设置为一个大值时会发生什么，比如说100。作为一个练习，使用困惑度为100重新训练t-SNE模型，并可视化结果词嵌入。你可以在与本书相关的GitHub仓库中看到代码。结果图如图7.28所示。
- en: '![](../Images/CH07_F28_Thampi.png)'
  id: totrans-257
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/CH07_F28_Thampi.png)'
- en: Figure 7.28 t-SNE visualization with high perplexity
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.28 高困惑度下的t-SNE可视化
- en: We can see that the words are clustered in a random order, and all the words
    seem to be placed roughly in a circle. The authors of the t-SNE algorithm recommend
    the perplexity be set between 5 and 50\. The guideline is to use a higher perplexity
    value for denser datasets where there are dense clusters of words in the higher-dimensional
    space.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到单词以随机顺序聚集，所有的单词似乎都大致放置在一个圆圈中。t-SNE算法的作者建议将困惑度设置为5到50之间。指导原则是对于密集的数据集，其中在更高维空间中有密集的单词集群，应使用更高的困惑度值。
- en: 7.4.4 Validating semantic similarity visualizations
  id: totrans-260
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4.4 验证语义相似性可视化
- en: We have learned two techniques to visualize higher-dimensional word embeddings,
    namely, PCA and t-SNE. We evaluated each visualization qualitatively, but is there
    a way of validating them quantitatively? To validate the plots quantitatively,
    we can measure the cosine similarity between word pairs in the lower-dimensional
    representation and compare that with the higher-dimensional representation. We
    have already done this for the higher-dimensional representation in section 7.4.1
    (see figure 7.16). As an exercise, extend the code in section 7.4.1 to also visualize
    the embeddings generated by PCA and both the t-SNE models (perplexity = 10 and
    perplexity = 100). The resulting plot is shown in figure 7.29\. You can check
    out the solution in the GitHub repository associated with this book.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经学习了两种可视化高维词嵌入的技术，即PCA和t-SNE。我们对每种可视化进行了定性评估，但有没有一种方法可以定量验证它们呢？为了定量验证这些图，我们可以测量低维表示中词对之间的余弦相似度，并将其与高维表示进行比较。我们已经在7.4.1节中对此进行了操作（见图7.16）。作为一个练习，将7.4.1节中的代码扩展到也可视化由PCA和两种t-SNE模型（困惑度=10和困惑度=100）生成的嵌入。结果图如图7.29所示。您可以在与本书相关的GitHub存储库中查看解决方案。
- en: '![](../Images/CH07_F29_Thampi.png)'
  id: totrans-262
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/CH07_F29_Thampi.png)'
- en: Figure 7.29 Validating visualizations of semantic similarity
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 图7.29 验证语义相似性的可视化
- en: We can see that the PCA representation is not consistent with the original GloVe
    representation. For instance, *basketball* and *lebron* have lower similarity
    than *basketball* and *facebook* in the PCA representation. We can, however, see
    that the representation learned by t-SNE with a perplexity of 10 preserves as
    much of the similarity captured by the original GloVe embedding. The t-SNE with
    a perplexity of 100 shows all word pairs with similar meaning and is clearly the
    worst representation among the three. This sort of validation will be easier to
    do at scale than to qualitatively assess a 2-D visualization generated by PCA
    and t-SNE for all the words of interest.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以看到，PCA表示与原始GloVe表示不一致。例如，在PCA表示中，*basketball*和*lebron*的相似度低于*basketball*和*facebook*。然而，我们可以看到，困惑度为10的t-SNE学习的表示保留了原始GloVe嵌入捕获的大部分相似性。困惑度为100的t-SNE显示了所有具有相似意义的词对，并且在三种表示中显然是最差的。这种验证在规模上比定性评估PCA和t-SNE为所有感兴趣的词语生成的2-D可视化要容易得多。
- en: Summary
  id: totrans-265
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we focused on the field of natural language processing (NLP),
    specifically on the topic of representing words in a form that captures semantic
    meaning. We also learned how to interpret and visualize semantic similarity from
    these word representations using dimensionality-reduction techniques like PCA
    and t-SNE.
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在本章中，我们专注于自然语言处理（NLP）领域，特别是关于如何以捕捉语义意义的形式表示词语的话题。我们还学习了如何使用PCA和t-SNE等降维技术从这些词语表示中解释和可视化语义相似性。
- en: A naive way of representing words is using one-hot encoding. However, this representation
    is sparse and computationally inefficient and does not encode any semantic meaning.
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 表示词语的一种天真方式是使用独热编码。然而，这种表示是稀疏的，计算效率低下，并且不编码任何语义意义。
- en: Dense representations of words that encode semantic meaning are called word
    embeddings, word vectors, or distributed representations. Representations or word
    embeddings learned by neural networks are called neural word embeddings.
  id: totrans-268
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 编码语义意义的词语的密集表示被称为词嵌入、词向量或分布式表示。由神经网络学习的表示或词嵌入被称为神经词嵌入。
- en: Dense representations of words can be learned using neural network architectures
    like continuous bag of words (CBOW), skip-gram, and global vectors (GloVe).
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以使用连续词袋（CBOW）、跳字模型（skip-gram）和全局向量（GloVe）等神经网络架构来学习词语的密集表示。
- en: In the context of interpreting and visualizing semantic similarity in neural
    word embeddings, we need an understanding or taxonomy of words to verify whether
    the neural word embeddings have learned the semantic meanings properly.
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在解释和可视化神经词嵌入中的语义相似性的背景下，我们需要一个词语的理解或分类法来验证神经词嵌入是否正确地学习了语义意义。
- en: We can measure semantic similarity using the cosine similarity metric. The metric
    has a property where word embeddings that are closer together have a larger score
    than word embeddings that are further apart.
  id: totrans-271
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用余弦相似度度量来衡量语义相似性。该度量具有一个属性，即彼此更接近的词嵌入比彼此更远的词嵌入具有更高的分数。
- en: We can visualize higher-dimensional word embeddings in lower dimensions using
    dimensionality-reduction techniques such as principal component analysis (PCA)
    and t-distributed stochastic neighbor embedding (t-SNE).
  id: totrans-272
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以使用主成分分析（PCA）和t分布随机邻域嵌入（t-SNE）等降维技术，在低维空间中可视化高维词嵌入。
- en: 'Although PCA is a powerful technique, it does suffer from a major drawback:
    it assumes that the dataset or the word embeddings can be modeled linearly. This
    may not be the case for most of the datasets that we deal with.'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然主成分分析（PCA）是一种强大的技术，但它确实存在一个主要的缺点：它假设数据集或词嵌入可以线性建模。对于我们处理的大多数数据集来说，这可能并不成立。
- en: t-SNE falls under the broad class of machine learning techniques called manifold
    learning, where the objective is to learn nonlinear structures from higher-dimensional
    data in lower dimensions. The technique is one of the most popular choices for
    visualizing higher-dimensional data.
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: t-SNE属于机器学习技术中广泛分类的流形学习，其目标是学习从高维数据中提取低维的非线性结构。这项技术是可视化高维数据中最受欢迎的选择之一。
- en: We can quantitatively validate the visualizations generated by PCA and t-SNE
    by computing the cosine similarity for different pairs of words and checking to
    see if the degree of similarity is consistent with the original higher-dimensional
    representation.
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过计算不同词对之间的余弦相似度，并检查相似度是否与原始高维表示一致，来定量验证PCA和t-SNE生成的可视化。
