- en: 4 Optimizing the training process: Underfitting, overfitting, testing, and regularization
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 4 优化训练过程：欠拟合、过拟合、测试和正则化
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: what is underfitting and overfitting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是欠拟合和过拟合
- en: 'some solutions for avoiding overfitting: testing, the model complexity graph,
    and regularization'
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 避免过拟合的一些解决方案：测试、模型复杂度图和正则化
- en: calculating the complexity of the model using the L1 and L2 norms
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用L1和L2范数计算模型的复杂度
- en: picking the best model in terms of performance and complexity
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在性能和复杂度方面选择最佳模型
- en: '![](../Images/4-unnumb.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-unnumb.png)'
- en: This chapter is different from most of the chapters in this book, because it
    doesn’t contain a particular machine learning algorithm. Instead, it describes
    some potential problems that machine learning models may face and effective practical
    ways to solve them.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 本章与本书中的大多数章节不同，因为它不包含特定的机器学习算法。相反，它描述了机器学习模型可能遇到的一些潜在问题以及有效的实际解决方法。
- en: Imagine that you have learned some great machine learning algorithms, and you
    are ready to apply them. You go to work as a data scientist, and your first task
    is to build a machine learning model for a dataset of customers. You build it
    and put it in production. However, everything goes wrong, and the model doesn’t
    do a good job of making predictions. What happened?
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，你已经学习了一些优秀的机器学习算法，并且准备将它们应用到实践中。你作为一名数据科学家开始工作，你的第一个任务是针对客户数据集构建一个机器学习模型。你构建了模型并将其投入生产。然而，一切都不顺利，模型在预测方面表现不佳。发生了什么？
- en: 'It turns out that this story is common, because many things can go wrong with
    our models. Fortunately, we have several techniques to improve them. In this chapter,
    I show you two problems that happen often when training models: underfitting and
    overfitting. I then show you some solutions to avoid underfitting and overfitting
    our models: testing and validation, the model complexity graph, and regularization.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 结果表明，这个故事很常见，因为我们的模型可能会出现很多问题。幸运的是，我们有几种技术来改进它们。在本章中，我向你展示了在训练模型时经常出现的两个问题：欠拟合和过拟合。然后，我展示了避免欠拟合和过拟合模型的一些解决方案：测试和验证、模型复杂度图和正则化。
- en: Let’s explain underfitting and overfitting with the following analogy. Let’s
    say that we have to study for a test. Several things could go wrong during our
    study process. Maybe we didn’t study enough. There’s no way to fix that, and we’ll
    likely perform poorly in our test. What if we studied a lot but in the wrong way.
    For example, instead of focusing on learning, we decided to memorize the entire
    textbook word for word. Will we do well in our test? It’s likely that we won’t,
    because we simply memorized everything without learning. The best option, of course,
    would be to study for the exam properly and in a way that enables us to answer
    new questions that we haven’t seen before on the topic.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们用以下类比来解释欠拟合和过拟合。假设我们必须为考试做准备。在学习过程中可能会出现很多问题。也许我们没有学习足够的内容。这是无法修复的，我们很可能会在考试中表现不佳。如果我们学习了很多，但方式错误呢？例如，我们决定逐字逐句地记住整本教科书，而不是专注于学习。我们会在考试中表现得好吗？很可能会不好，因为我们只是简单地记住了所有内容而没有真正学习。当然，最好的选择是以正确的方式为考试做准备，并且以能够回答我们之前没有见过的关于该主题的新问题。
- en: In machine learning, *underfitting* looks a lot like not having studied enough
    for an exam. It happens when we try to train a model that is too simple, and it
    is unable to learn the data. *Overfitting* looks a lot like memorizing the entire
    textbook instead of studying for the exam. It happens when we try to train a model
    that is too complex, and it memorizes the data instead of learning it well. A
    good model, one that neither underfits nor overfits, is one that looks like having
    studied well for the exam. This corresponds to a good model that learns the data
    properly and can make good predictions on new data that it hasn’t seen.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，*欠拟合*很像考试前没有学习足够的内容。它发生在我们试图训练一个过于简单的模型，而这个模型无法学习数据。*过拟合*则很像为了考试而记住整本教科书而不是学习。它发生在我们试图训练一个过于复杂的模型，而这个模型只是记住数据而不是真正学习它。一个好的模型，既不过拟合也不欠拟合，就像考试前已经很好地学习了。这对应于一个能够正确学习数据并在未见过的新的数据上做出良好预测的好模型。
- en: Another way to think of underfitting and overfitting is when we have a task
    in hand. We can make two mistakes. We can oversimplify the problem and come up
    with a solution that is too simple. We can also overcomplicate the problem and
    come up with a solution that is too complex.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考欠拟合和过拟合的方式是当我们有一个任务在手。我们可以犯两个错误。我们可以过度简化问题，并提出一个过于简单的解决方案。我们也可以过度复杂化问题，并提出一个过于复杂的解决方案。
- en: 'Imagine if our task is to kill Godzilla, as shown in figure 4.1, and we come
    to battle equipped with nothing but a fly swatter. That is an example of an *oversimplification*.
    The approach won’t go well for us, because we underestimated the problem and came
    unprepared. This is underfitting: our dataset is complex, and we come to model
    it equipped with nothing but a simple model. The model will not be able to capture
    the complexities of the dataset.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 想象一下，如果我们的任务是杀死图4.1中所示的哥斯拉，而我们只带着苍蝇拍来战斗。这是一个*过度简化*的例子。这种方法对我们来说不会顺利，因为我们低估了问题，并且没有做好准备。这就是欠拟合：我们的数据集很复杂，而我们只带着一个简单的模型来建模。该模型将无法捕捉数据集的复杂性。
- en: 'In contrast, if our task is to kill a small fly and we use a bazooka to do
    the job, this is an example of an *overcomplication*. Yes, we may kill the fly,
    but we’ll also destroy everything at hand and put ourselves at risk. We overestimated
    the problem, and our solution wasn’t good. This is overfitting: our data is simple,
    but we try to fit it to a model that is too complex. The model will be able to
    fit our data, but it’ll memorize it instead of learning it. The first time I learned
    overfitting, my reaction was, “Well, that’s no problem. If I use a model that
    is too complex, I can still model my data, right?” Correct, but the real problem
    with overfitting is trying to get the model to make predictions on unseen data.
    The predictions will likely come out looking horrible, as we see later in this
    chapter.'
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，如果我们任务是杀死一只小苍蝇，而我们使用火箭筒来完成这项工作，这就是一个*过度复杂化*的例子。是的，我们可能会杀死苍蝇，但也会摧毁所有手头的东西，并使自己处于危险之中。我们高估了问题，我们的解决方案并不好。这就是过拟合：我们的数据很简单，但我们试图将其拟合到一个过于复杂的模型中。该模型将能够拟合我们的数据，但它会记住它而不是学习它。我第一次学习过拟合时，我的反应是，“嗯，这没什么问题。如果我使用一个过于复杂的模型，我仍然可以对我的数据进行建模，对吧？”正确，但过拟合的真正问题是试图让模型对未见过的数据进行预测。预测结果可能会非常糟糕，正如我们在本章后面看到的。
- en: '![](../Images/4-1.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-1.png)'
- en: 'Figure 4.1 Underfitting and overfitting are two problems that can occur when
    training our machine learning model. Left: Underfitting happens when we oversimplify
    the problem at hand, and we try to solve it using a simple solution, such as trying
    to kill Godzilla using a fly swatter. Right: Overfitting happens when we overcomplicate
    the solution to a problem and try to solve it with an exceedingly complicated
    solution, such as trying to kill a fly using a bazooka.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.1欠拟合和过拟合是在训练我们的机器学习模型时可能出现的两个问题。左：当我们将手头的问题过度简化，并试图用简单的解决方案来解决它时，就会发生欠拟合，例如试图用苍蝇拍杀死哥斯拉。右：当我们过度复杂化问题的解决方案，并试图用极其复杂的解决方案来解决它时，就会发生过拟合，例如试图用火箭筒杀死一只苍蝇。
- en: As we saw in the section “Paramaters and hyperparameters” in chapter 3, every
    machine learning model has hyperparameters, which are the knobs that we twist
    and turn before training the model. Setting the right hyperparameters for our
    model is of extreme importance. If we set some of them wrong, we are prone to
    underfit or overfit. The techniques that we cover in this chapter are useful to
    help us tune the hyperparameters correctly.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在第3章的“参数和超参数”部分中看到的，每个机器学习模型都有超参数，这些是我们训练模型之前可以旋转和调整的旋钮。为我们的模型设置正确的超参数至关重要。如果我们设置了一些错误的参数，我们很容易出现欠拟合或过拟合。本章中介绍的技术有助于我们正确调整超参数。
- en: 'To make these concepts clearer, we’ll look at an example with a dataset and
    several different models that are created by changing one particular hyperparameter:
    the degree of a polynomial.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使这些概念更清晰，我们将通过一个数据集和几个不同的模型来举例说明，这些模型是通过改变一个特定的超参数创建的：多项式的度数。
- en: 'You can find all the code for this chapter in the following GitHub repository:
    [https://github.com/luisguiserrano/manning/tree/master/Chapter_4_Testing_Overfitting_Underfitting](https://github.com/luisguiserrano/manning/tree/master/Chapter_4_Testing_Overfitting_Underfitting).'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以在以下GitHub仓库中找到本章的所有代码：[https://github.com/luisguiserrano/manning/tree/master/Chapter_4_Testing_Overfitting_Underfitting](https://github.com/luisguiserrano/manning/tree/master/Chapter_4_Testing_Overfitting_Underfitting)。
- en: An example of underfitting and overfitting using polynomial regression
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用多项式回归的欠拟合和过拟合的例子
- en: In this section, we see an example of overfitting and underfitting in the same
    dataset. Look carefully at the dataset in figure 4.2, and try to fit a polynomial
    regression model
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了同一数据集中过拟合和欠拟合的例子。仔细观察图4.2中的数据集，并尝试拟合一个多项式回归模型。
- en: (seen in the section “What if the data is not in a line?” in chapter 3). Let’s
    think of what kind of polynomial would fit this dataset. Would it be a line, a
    parabola, a cubic, or perhaps a polynomial of degree 100? Recall that the degree
    of a polynomial is the highest exponent present. For example, the polynomial 2*x*^(14)
    + 9*x*⁶ – 3*x* + 2 has degree 14.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: （见第3章“如果数据不在一条线上怎么办？”部分）。让我们思考一下什么样的多项式可以拟合这个数据集。会是直线、抛物线、三次多项式，或者可能是100阶多项式？回想一下，多项式的次数是最高次幂。例如，多项式2*x*^(14)
    + 9*x*⁶ – 3*x* + 2的次数是14。
- en: '![](../Images/4-2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-2.png)'
- en: 'Figure 4.2 In this dataset, we train some models and exhibit training problems
    such as underfitting and overfitting. If you were to fit a polynomial regression
    model to this dataset, what type of polynomial would you use: a line, a parabola,
    or something else?'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.2 在这个数据集中，我们训练了一些模型，并展示了训练问题，如欠拟合和过拟合。如果你要拟合这个数据集的多项式回归模型，你会使用什么类型的多项式：直线、抛物线，还是其他？
- en: I think that dataset looks a lot like a parabola that opens downward (a sad
    face). This is a polynomial of degree 2\. However, we are humans, and we eyeballed
    it. A computer can’t do that. A computer needs to try many values for the degree
    of the polynomial and somehow pick the best one. Let’s say the computer will try
    to fit it with polynomials of degrees 1, 2, and 10\. When we fit polynomials of
    degree 1 (a line), 2 (a quadratic), and 10 (a curve that oscillates at most nine
    times) to this dataset, we obtain the results shown in figure 4.3.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 我认为这个数据集看起来很像一个开口向下的抛物线（一个悲伤的面孔）。这是一个二阶多项式。然而，我们是人类，我们只能凭肉眼判断。计算机无法做到这一点。计算机需要尝试多项式次数的许多值，并从中选择最佳的一个。假设计算机将尝试用一阶、二阶和十阶多项式来拟合它。当我们用一阶（直线）、二阶（二次）和十阶（最多振荡九次）多项式拟合这个数据集时，我们得到了图4.3中所示的结果。
- en: '![](../Images/4-3.png)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-3.png)'
- en: Figure 4.3 Fitting three models to the same dataset. Model 1 is a polynomial
    of degree 1, which is a line. Model 2 is a polynomial of degree 2, or a quadratic.
    Model 3 is a polynomial of degree 10\. Which one looks like the best fit?
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.3 将三个模型拟合到同一数据集。模型1是一阶多项式，即一条直线。模型2是二阶多项式，或称二次多项式。模型3是十阶多项式。哪一个看起来像是最佳拟合？
- en: In figure 4.3 we see three models, model 1, model 2, and model 3\. Notice that
    model 1 is too simple, because it is a line trying to fit a quadratic dataset.
    There is no way we’ll find a good line to fit this dataset, because the dataset
    simply does not look like a line. Therefore, model 1 is a clear example of underfitting.
    Model 2, in contrast, fits the data pretty well. This model neither overfits nor
    underfits. Model 3 fits the data extremely well, but it completely misses the
    point. The data is meant to look like a parabola with a bit of noise, and the
    model draws a very complicated polynomial of degree 10 that manages to go through
    each one of the points but doesn’t capture the essence of the data. Model 3 is
    a clear example of overfitting.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.3中，我们看到三个模型，模型1、模型2和模型3。注意，模型1过于简单，因为它试图用一条直线拟合一个二次数据集。我们不可能找到一条好的直线来拟合这个数据集，因为这个数据集根本不像一条直线。因此，模型1是欠拟合的一个明显例子。相比之下，模型2很好地拟合了数据。这个模型既不过拟合也不欠拟合。模型3非常完美地拟合了数据，但它完全错过了重点。数据集应该看起来像一个带有一些噪声的抛物线，而该模型绘制了一个非常复杂、十次方的多项式，它设法通过了每一个点，但没有捕捉到数据的本质。模型3是过拟合的一个明显例子。
- en: 'To summarize the previous reasoning, here is an observation that we use throughout
    this chapter, and in many other episodes in this book: very simple models tend
    to underfit. Very complex models tend to overfit. The goal is to find a model
    that is neither too simple nor too complex and that captures the essence of our
    data well.'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 为了总结前面的推理，这里有一个我们在本章以及本书许多其他章节中使用的观察结果：非常简单的模型往往欠拟合。非常复杂的模型往往过拟合。目标是找到一个既不太简单也不太复杂，并且能够很好地捕捉我们数据本质的模型。
- en: 'We’re about to get to the challenging part. As humans, we know that the best
    fit is given by model 2\. But what does the computer see? The computer can only
    calculate error functions. As you may recall from chapter 3, we defined two error
    functions: absolute error and square error. For visual clarity, in this example
    we’ll use absolute error, which is the average of the sums of the absolute values
    of the distances from the points to the curve, although the same arguments would
    work with the square error. For model 1, the points are far from the model, so
    this error is large. For model 2, these distances are small, so the error is small.
    However, for model 3, the distances are zero because the points all fall in the
    actual curve! This means that the computer will think that the perfect model is
    model 3\. This is not good. We need a way to tell the computer that the best model
    is model 2 and that model 3 is overfitting. How can we do this? I encourage you
    to put this book down for a few minutes and think of some ideas yourself, because
    there are several solutions for this problem.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 我们即将进入挑战性的部分。作为人类，我们知道最佳拟合由模型2提供。但计算机看到的是什么？计算机只能计算误差函数。你可能还记得第3章中我们定义了两个误差函数：绝对误差和平方误差。为了视觉清晰，在这个例子中我们将使用绝对误差，即点到曲线的距离绝对值的总和的平均值，尽管相同的论点也可以用于平方误差。对于模型1，点远离模型，因此这个误差很大。对于模型2，这些距离很小，所以误差很小。然而，对于模型3，距离为零，因为所有点都落在实际曲线上！这意味着计算机将认为完美的模型是模型3。这并不好。我们需要一种方法告诉计算机最佳模型是模型2，而模型3是过度拟合。我们该如何做呢？我鼓励你在几分钟内放下这本书，自己思考一些想法，因为这个问题有几种解决方案。
- en: How do we get the computer to pick the right model? By testing
  id: totrans-31
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何让计算机选择正确的模型？通过测试
- en: One way to determine if a model overfits is by testing it, and that is what
    we do in this section. Testing a model consists of picking a small set of the
    points in the dataset and choosing to use them not for training the model but
    for testing the model’s performance. This set of points is called the *testing
    set*. The remaining set of points (the majority), which we use for training the
    model, is called the *training set*. Once we’ve trained the model on the training
    set, we use the testing set to evaluate the model. In this way, we make sure that
    the model is good at generalizing to unseen data, as opposed to memorizing the
    training set. Going back to the exam analogy, let’s imagine training and testing
    this way. Let’s say that the book we are studying for in the exam has 100 questions
    at the end. We pick 80 of them to train, which means we study
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 确定一个模型是否过度拟合的一种方法是通过测试，这正是我们在本节中要做的。测试一个模型包括从数据集中选择一小部分点，并选择不使用它们来训练模型，而是用来测试模型的表现。这个点集被称为*测试集*。剩余的点集（大多数），我们用它来训练模型，被称为*训练集*。一旦我们在训练集上训练了模型，我们就使用测试集来评估模型。这样，我们确保模型擅长泛化到未见过的数据，而不是记住训练集。回到考试的类比，让我们想象以这种方式进行培训和测试。假设我们为考试准备的书中最后有100个问题。我们选择80个来训练，这意味着我们学习
- en: them carefully, look up the answers, and learn them. Then we use the remaining
    20 questions to test ourselves—we try to answer them without looking at the book,
    as in an exam setting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细阅读，查找答案，并学习它们。然后我们使用剩下的20个问题来测试自己——我们尝试在不看书的情况下回答它们，就像在考试环境中一样。
- en: Now let’s see how this method looks with our dataset and our models. Notice
    that the real problem with model 3 is not that it doesn’t fit the data; it’s that
    it doesn’t generalize well to new data. In other words, if you trained model 3
    on that dataset and some new points appeared, would you trust the model to make
    good predictions with these new points? Probably not, because the model merely
    memorized the entire dataset without capturing its essence. In this case, the
    essence of the dataset is that it looks like a parabola that opens downward.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们看看这种方法在我们的数据集和模型中是如何表现的。请注意，模型3的真正问题不在于它不拟合数据；而在于它对新数据的泛化能力不好。换句话说，如果你在数据集上训练了模型3，并且出现了新的点，你会信任模型用这些新点做出良好的预测吗？可能不会，因为模型只是记住了整个数据集而没有抓住其本质。在这种情况下，数据集的本质是它看起来像向下开口的抛物线。
- en: In figure 4.4, we have drawn two white triangles in our dataset, representing
    the testing set. The training set corresponds to the black circles. Now let’s
    examine this figure in detail and see how these three models perform with both
    our training and our testing sets. In other words, let’s examine the error that
    the model produces in both datasets. We’ll refer to these two errors as the *training
    error* and the *testing error*.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 在图4.4中，我们在我们的数据集中画了两个白色三角形，代表测试集。训练集对应的是黑色圆圈。现在让我们详细检查这个图，看看这三个模型在训练集和测试集上的表现如何。换句话说，让我们检查模型在这两个数据集中产生的误差。我们将这两个误差称为*训练误差*和*测试误差*。
- en: The top row in figure 4.4 corresponds to the training set and the bottom row
    to the testing set. To illustrate the error, we have drawn vertical lines from
    the point to the model. The mean absolute error is precisely the average of the
    lengths of these lines. Looking at the top row we can see that model 1 has a large
    training error, model 2 has a small training error, and model 3 has a tiny training
    error (zero, in fact). Thus, model 3 does the best job on the training set.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4的顶部行对应于训练集，底部行对应于测试集。为了说明误差，我们从点到模型画了垂直线。平均绝对误差就是这些线长度的平均值。看顶部行，我们可以看到模型1有大的训练误差，模型2有小的训练误差，而模型3有极小的训练误差（实际上为零）。因此，模型3在训练集上做得最好。
- en: '![](../Images/4-4.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-4.png)'
- en: Figure 4.4 We can use this table to decide how complex we want our model. The
    columns represent the three models of degree 1, 2, and 10\. The columns represent
    the training and the testing error. The solid circles are the training set, and
    the white triangles are the testing set. The errors at each point can be seen
    as the vertical lines from the point to the curve. The error of each model is
    the mean absolute error given by the average of these vertical lengths. Notice
    that the training error goes down as the complexity of the model increases. However,
    the testing error goes down and then back up as the complexity increases. From
    this table, we conclude that out of these three models, the best one is model
    2, because it gives us a low testing error.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.4 我们可以使用这个表格来决定我们希望模型有多复杂。列代表三个模型，分别是1次、2次和10次方的模型。列代表训练误差和测试误差。实心圆圈是训练集，白色三角形是测试集。每个点的误差可以看作是从点到曲线的垂直线。每个模型的误差是这些垂直长度的平均值给出的平均绝对误差。注意，随着模型复杂度的增加，训练误差会下降。然而，测试误差在增加后会下降然后再上升。从这个表中，我们得出结论，在这三个模型中，最好的一个是模型2，因为它给我们带来了低的测试误差。
- en: 'However, when we get to the testing set, things change. Model 1 still has a
    large testing error, meaning that this is simply a bad model, underperforming
    with the training and the testing set: it underfits. Model 2 has a small testing
    error, which means it is a good model, because it fits both the training and the
    testing set well. Model 3, however, produces a large testing error. Because it
    did such a terrible job fitting the testing set, yet such a good job fitting the
    training set, we conclude that model 3 overfits.'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们到达测试集时，情况发生了变化。模型1仍然有大的测试误差，这意味着这只是一个坏模型，在训练集和测试集上都表现不佳：它欠拟合。模型2有小的测试误差，这意味着它是一个好模型，因为它很好地拟合了训练集和测试集。然而，模型3却产生了大的测试误差。因为它在拟合测试集上做得如此糟糕，而在拟合训练集上做得如此好，我们得出结论，模型3过拟合了。
- en: Let’s summarize what we have learned so far.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们总结一下到目前为止我们已经学到的内容。
- en: Models can
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 模型可以
- en: 'Underfit: use a model that is too simple to our dataset.'
  id: totrans-42
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合：使用一个对我们数据集来说过于简单的模型。
- en: 'Fit the data well: use a model that has the right amount of complexity for
    our dataset.'
  id: totrans-43
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好拟合数据：使用一个对我们数据集来说复杂度合适的模型。
- en: 'Overfit: use a model that is too complex for our dataset.'
  id: totrans-44
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合：使用一个对我们数据集来说过于复杂的模型。
- en: In the training set
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练集中
- en: The underfit model will do poorly (large training error).
  id: totrans-46
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合模型表现不佳（训练误差大）。
- en: The good model will do well (small training error).
  id: totrans-47
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的模型表现良好（训练误差小）。
- en: The overfit model will do very well (very small training error).
  id: totrans-48
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合模型表现非常好（训练误差非常小）。
- en: In the testing set
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集中
- en: The underfit model will do poorly (large testing error).
  id: totrans-50
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合模型表现不佳（测试误差大）。
- en: The good model will do well (small testing error).
  id: totrans-51
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 良好的模型表现良好（测试误差小）。
- en: The overfit model will do poorly (large testing error).
  id: totrans-52
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过拟合模型表现不佳（测试误差大）。
- en: Thus, the way to tell whether a model underfits, overfits, or is good, is to
    look at the training and testing errors. If both errors are high, then it underfits.
    If both errors are low, then it is a good model. If the training error is low
    and the testing error is high, then it overfits.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，判断一个模型是否欠拟合、过拟合或表现良好，是查看训练集和测试集的错误。如果两个错误都高，那么它欠拟合。如果两个错误都低，那么它是一个好的模型。如果训练错误低而测试错误高，那么它过拟合。
- en: How do we pick the testing set, and how big should it be?
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何选择测试集，它应该有多大？
- en: Here’s a question. Where did I pull those two new points from? If we are training
    a model in production where data is always flowing, then we can pick some of the
    new points as our testing data. But what if we don’t have a way to get new points,
    and all we have is our original dataset of 10 points? When this happens, we just
    sacrifice some of our data and use it as a test set. How much data? That depends
    on how much data we have and how well we want the model to do, but in practice,
    any value from 10% to 20% seems to work well.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一个问题。我从哪里得到这两个新点？如果我们在一个数据始终流动的生产环境中训练模型，那么我们可以选择一些新点作为我们的测试数据。但如果我们没有获取新点的方法，而我们只有10个点的原始数据集呢？当这种情况发生时，我们就牺牲一些数据，将其用作测试集。需要多少数据？这取决于我们有多少数据以及我们希望模型做得有多好，但在实践中，任何从10%到20%的值似乎都工作得很好。
- en: Can we use our testing data for training the model? No.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用测试数据来训练模型吗？不可以。
- en: 'In machine learning, we always need to follow an important rule: when we split
    our data into training and testing, we should use the training data for training
    the model, and for absolutely no reason should we touch the testing data while
    training the model or making decisions on the model’s hyperparameters. Failure
    to do so is likely to result in overfitting, even'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习中，我们始终需要遵循一个重要的规则：当我们把数据分成训练集和测试集时，我们应该使用训练数据来训练模型，而且在训练模型或对模型的超参数做出决策时，绝对不应该触碰测试数据。未能这样做很可能会导致过拟合，甚至
- en: if it’s not noticeable by a human. In many machine learning competitions, teams
    have submitted models they think are wonderful, only for them to fail miserably
    when tested on a secret dataset. This can be because the data scientists training
    the models somehow (perhaps inadvertently) used the testing data to train them.
    In fact, this rule is so important, we’ll make it the golden rule of this book.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 如果人类没有注意到。在许多机器学习竞赛中，团队提交了他们认为很棒的模型，但它们在测试一个秘密数据集时却惨败。这可能是因为训练模型的数据科学家（可能是无意中）使用了测试数据来训练它们。事实上，这条规则非常重要，我们将把它作为本书的金科玉律。
- en: golden rule Thou shalt never use your testing data for training.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 金科玉律 你绝不应该使用你的测试数据来训练。
- en: Right now, it seems like it’s an easy rule to follow, but as we’ll see, it’s
    a very easy rule to accidentally break.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，这似乎是一个容易遵循的规则，但正如我们将看到的，这是一个很容易意外违反的规则。
- en: As a matter of fact, we already broke the golden rule during this chapter. Can
    you tell where? I encourage you to go back and find where we broke it. We’ll see
    where in the next section.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，我们在本章中已经违反了金科玉律。你能告诉我在哪里吗？我鼓励你回去找到我们违反规则的地方。我们将在下一节中看到。
- en: Where did we break the golden rule, and how do we fix it? The validation set
  id: totrans-62
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们在哪里违反了金科玉律，以及我们如何纠正它？验证集
- en: In this section, we see where we broke the golden rule and learn a technique
    called validation, which will come to our rescue.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到我们违反了金科玉律，并学习了一种称为验证的技术，它将帮助我们解决问题。
- en: 'We broke the golden rule in the section “How do we get the computer to pick
    the right model.” Recall that we had three polynomial regression models: one of
    degree 1, one of degree 2, and one of degree 10, and we didn’t know which one
    to pick. We used our training data to train the three models, and then we used
    the testing data to decide which model to pick. We are not supposed to use the
    testing data to train our model or to make any decisions on the model or its hyperparameters.
    Once we do this, we are potentially overfitting! We potentially overfit every
    time we build a model that caters too much to our dataset.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在“我们如何让计算机选择正确的模型”这一节中违反了金科玉律。回想一下，我们有三个多项式回归模型：一个是1次方，一个是2次方，还有一个是10次方，我们不知道该选择哪一个。我们使用训练数据来训练这三个模型，然后使用测试数据来决定选择哪个模型。我们不应该使用测试数据来训练我们的模型或对其或其超参数做出任何决策。一旦我们这样做，我们就有可能过拟合！每次我们构建一个过度适应数据集的模型时，我们都有可能过拟合。
- en: 'What can we do? The solution is simple: we break our dataset even more. We
    introduce a new set, the *validation set*, which we then use to make decisions
    on our dataset. In summary, we break our dataset into the following three sets:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能做什么？解决方案很简单：我们将数据集进一步分割。我们引入一个新的集合，即*验证集*，然后我们使用它来对我们的数据集做出决策。总的来说，我们将数据集分割成以下三个集合：
- en: '**Training set**: for training all our models'
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**训练集**：用于训练所有我们的模型'
- en: '**Validation set**: for making decisions on which model to use'
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**验证集**：用于决定使用哪个模型'
- en: '**Testing set**: for checking how well our model did'
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**测试集**：用于检查我们的模型表现如何'
- en: Thus, in our example, we would have two more points to use for validation, and
    looking at the validation error should help us decide that the best model to use
    is model 2\. We should use the testing set at the very end, to see how well our
    model did. If the model is not good, we should throw everything away and start
    from scratch.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，在我们的例子中，我们会增加两个用于验证的点，查看验证错误应该有助于我们决定使用最佳的模型是模型2。我们应在最后使用测试集来查看模型的表现。如果模型不好，我们应该丢弃一切，从头开始。
- en: In terms of the sizes of the testing and validation sets, it is common to use
    a 60-20-20 split or an 80-10-10 split—in other words, 60% training, 20% validation,
    20% testing, or 80% training, 10% validation, 10% testing. These numbers are arbitrary,
    but they tend to work well, because they leave most of the data for training but
    still allow us to test the model in a big enough set.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在测试集和验证集的大小方面，通常使用60-20-20分割或80-10-10分割——换句话说，60%用于训练，20%用于验证，20%用于测试，或者80%用于训练，10%用于验证，10%用于测试。这些数字是任意的，但它们通常效果不错，因为它们为训练保留了大部分数据，但仍然允许我们在足够大的数据集上测试模型。
- en: 'A numerical way to decide how complex our model should be: The model complexity
    graph'
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 决定模型复杂度的数值方法：模型复杂度图
- en: In the previous sections, we learned how to use the validation set to help us
    decide which model was the best among three different ones. In this section, we
    learn about a graph called the *model complexity graph*, which helps us decide
    among many more models. Imagine that we have a different and much more complex
    dataset, and we are trying to build a polynomial regression model to fit it. We
    want to decide the degree of our model among the numbers between 0 and 10 (inclusive).
    As we saw in the previous section, the way to decide which model to use is to
    pick the one that has the smallest validation error.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何使用验证集来帮助我们决定在三个不同的模型中哪个是最好的。在本节中，我们将了解一个称为*模型复杂度图*的图表，它帮助我们决定在更多模型中的选择。想象一下，我们有一个不同且复杂得多的数据集，我们正在尝试构建一个多项式回归模型来拟合它。我们想要决定模型度数在0到10（包含）之间的哪个数值。正如我们在前面的章节中看到的，决定使用哪个模型的方法是选择具有最小验证错误的模型。
- en: However, plotting the training and testing errors can give us some valuable
    information and help us examine trends. In figure 4.5, you can see a plot in which
    the horizontal axis represents the degree of the polynomial in the model and the
    vertical axis represents the value of the error. The diamonds represent the training
    error, and the circles represent the validation error. This is the model complexity
    graph.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，绘制训练和测试错误图可以给我们提供一些有价值的信息，并帮助我们检查趋势。在图4.5中，你可以看到一个图表，其中水平轴代表模型中多项式的度数，垂直轴代表错误的值。菱形代表训练错误，圆圈代表验证错误。这就是模型复杂度图。
- en: '![](../Images/4-5.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图4-5](../Images/4-5.png)'
- en: Figure 4.5 The model complexity graph is an effective tool to help us determine
    the ideal complexity of a model to avoid underfitting and overfitting. In this
    model complexity graph, the horizontal axis represents the degree of several polynomial
    regression models, from 0 to 10 (i.e., the complexity of the model). The vertical
    axis represents the error, which in this case is given by the mean absolute error.
    Notice that the training error starts large and decreases as we move to the right.
    This is because the more complex our model is, the better it can fit the training
    data. The validation error, however, starts large, then decreases, and then increases
    again—very simple models can’t fit our data well (they underfit), whereas very
    complex models fit our training data but not our validation data because they
    overfit. A happy point in the middle is where our model neither underfits or overfits,
    and we can find it using the model complexity graph.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.5 模型复杂度图是帮助我们确定模型理想复杂度的有效工具，以避免欠拟合和过拟合。在这个模型复杂度图中，水平轴表示几个多项式回归模型的程度，从0到10（即模型的复杂度）。垂直轴表示错误，在这种情况下是由平均绝对误差给出的。请注意，随着我们向右移动，训练错误开始很大，然后减小。这是因为我们的模型越复杂，它就能更好地拟合训练数据。然而，验证错误开始很大，然后减小，然后再次增加——非常简单的模型无法很好地拟合我们的数据（它们欠拟合），而非常复杂的模型可以拟合我们的训练数据，但不能拟合我们的验证数据，因为它们过拟合。中间的一个快乐点是我们模型既不欠拟合也不过拟合，我们可以使用模型复杂度图找到它。
- en: Notice that in the model complexity graph in figure 4.5, the lowest value for
    the validation error occurs at degree 4, which means that for this dataset, the
    best-fitting model (among the ones we are considering) is a polynomial regression
    model of degree 4\. Looking at the left of the graph, we can see that when the
    degree of the polynomial is small, both the training and the validation errors
    are large, which implies that the models underfit. Looking at the right of the
    graph, we can see that the training error gets smaller and smaller, but the validation
    error gets larger and larger, which implies that the models overfit. The sweet
    spot happens around 4, which is the model we pick.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，在图4.5中的模型复杂度图中，验证错误的最低值出现在程度4处，这意味着对于这个数据集，最佳拟合模型（在我们考虑的模型中）是一个四次多项式回归模型。查看图的左侧，我们可以看到当多项式的程度较小时，训练和验证错误都很大，这表明模型欠拟合。查看图的右侧，我们可以看到训练错误越来越小，但验证错误越来越大，这表明模型过拟合。最佳点发生在大约4的位置，这是我们选择的模型。
- en: 'One benefit of the model complexity graph is that no matter how large our dataset
    is or how many different models we try, it always looks like two curves: one that
    always goes down (the training error) and one that goes down and then back up
    (the validation error). Of course, in a large and complex dataset, these curves
    may oscillate, and the behavior may be harder to spot. However, the model complexity
    graph is always a useful tool for data scientists to find a good spot in this
    graph and decide how complex their models should be to avoid both underfitting
    and overfitting.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 模型复杂度图的优点之一是，无论我们的数据集有多大，或者我们尝试了多少不同的模型，它总是看起来像两条曲线：一条始终下降（训练错误）和一条下降后再次上升（验证错误）。当然，在一个大而复杂的数据集中，这些曲线可能会波动，行为可能更难发现。然而，模型复杂度图始终是数据科学家寻找图中良好位置并决定模型复杂度以避免欠拟合和过拟合的有用工具。
- en: Why do we need such a graph if all we need to do is pick the model with the
    lowest validation error? This method is true in theory, but in practice, as a
    data scientist, you may have a much better idea of the problem you are solving,
    the constraints, and the benchmarks. If you see, for example, that the model with
    the smallest validation error is still quite complex, and that there is a much
    simpler model that has only a slightly higher validation error, you may be more
    inclined to pick that one. A great data scientist is one who can combine these
    theoretical tools with their knowledge about the use case to build the best and
    most effective models.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要这样的图表，如果我们只需要选择具有最低验证错误的模型呢？这种方法在理论上是正确的，但在实践中，作为一名数据科学家，你可能对你的问题、约束条件和基准有更深入的了解。例如，如果你看到具有最小验证错误的模型仍然相当复杂，而有一个更简单的模型，其验证错误仅略高，你可能更倾向于选择后者。一位伟大的数据科学家能够将理论工具与他们对用例的知识相结合，构建出最佳且最有效的模型。
- en: 'Another alternative to avoiding overfitting: Regularization'
  id: totrans-79
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 避免过拟合的另一种替代方案：正则化
- en: 'In this section, we discuss another useful technique to avoid overfitting in
    our models that doesn’t require a testing set: *regularization*. Regularization
    relies on the same observation we made in the section “An example of underfitting
    and overfitting using polynomial regression,” where we concluded that simple models
    tend to underfit and complex models tend to overfit. However, in the previous
    methods, we tested several models and selected the one that best balanced performance
    and complexity. In contrast, when we use regularization, we don’t need to train
    several models. We simply train the model once, but during the training, we try
    to not only improve the model’s performance but also reduce its complexity. The
    key to doing this is to measure both performance and complexity at the same time.'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们讨论另一种有用的技术，用于避免模型过拟合，而无需测试集：*正则化*。正则化依赖于我们在“使用多项式回归的欠拟合和过拟合示例”部分中做出的相同观察，我们得出结论，简单模型往往欠拟合，而复杂模型往往过拟合。然而，在先前的方法中，我们测试了多个模型，并选择了性能和复杂性最佳平衡的那个模型。相比之下，当我们使用正则化时，我们不需要训练多个模型。我们只需训练一次模型，但在训练过程中，我们不仅要提高模型性能，还要降低其复杂性。做到这一点的关键是同时衡量性能和复杂性。
- en: Before we get into the details, let’s discuss an analogy for thinking about
    measuring the performance and complexity of models. Imagine that we have three
    houses, and all of them have the same problem—the roof is leaking (figure 4.6).
    Three roofers come, and each fixes one of the houses. The first roofer used a
    bandage, the second one used roofing shingles, and the third one used titanium.
    From our intuition, it seems that the best one is roofer 2, because roofer 1 oversimplified
    the problem (underfitting) and roofer 3 overcomplicated it (overfitting).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们深入细节之前，让我们讨论一个用于思考衡量模型性能和复杂性的类比。想象我们有三座房子，它们都存在相同的问题——屋顶漏水（图4.6）。三位屋顶工来修理，每人修理一座房子。第一位屋顶工使用绷带，第二位使用屋顶瓦片，第三位使用钛。从我们的直觉来看，似乎第二位屋顶工是最好的，因为第一位屋顶工过于简化了问题（欠拟合），而第三位屋顶工过于复杂化（过拟合）。
- en: '![](../Images/4-6.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-6.png)'
- en: Figure 4.6 An analogy for underfitting and overfitting. Our problem consists
    of a broken roof. We have three roofers who can fix it. Roofer 1 comes with a
    bandage, roofer 2 comes with roofing shingles, and roofer 3 comes with a block
    of titanium. Roofer 1 oversimplified the problem, so they represent underfitting.
    Roofer 2 used a good solution. Roofer 3 overcomplicated the solution, so they
    represent overfitting.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.6 欠拟合和过拟合的类比。我们的问题是一个破损的屋顶。我们有三位可以修理它的屋顶工。屋顶工1带着绷带来，屋顶工2带着屋顶瓦片来，屋顶工3带着一块钛。屋顶工1过于简化了问题，因此代表欠拟合。屋顶工2使用了良好的解决方案。屋顶工3过于复杂化了解决方案，因此代表过拟合。
- en: 'However, we need to make our decisions using numbers, so let’s take some measurements.
    The way to measure the performance of the roofers is by how much water leaked
    after they fix their roof. They had the following scores:'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，我们需要用数字来做出决定，所以让我们做一些测量。衡量屋顶工性能的方法是他们在修理屋顶后漏水量有多少。他们的评分如下：
- en: Performance (in mL of water leaked)
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 性能（以漏水量计）
- en: 'Roofer 1: 1000 mL water'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 1: 1000 mL 水'
- en: 'Roofer 2: 1 mL water'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 2: 1 mL 水'
- en: 'Roofer 3: 0 mL water'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 3: 0 mL 水'
- en: 'It seems that roofer 1 had a terrible performance, because the roof was still
    leaking water. However, between roofers 2 and 3, which one do we pick? Perhaps
    roofer 3, who had a better performance? The performance measure is not good enough;
    it correctly removes roofer 1 from the equation, but it erroneously tells us to
    go with roofer 3, instead of roofer 2\. We need a measure of their complexity
    to help us make the right decision. A good measure of their complexity is how
    much they charged us to fix the roof, in dollars. The prices were as follows:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 看起来屋顶工1的表现很糟糕，因为屋顶仍在漏水。然而，在屋顶工2和3之间，我们选择哪一个？也许屋顶工3，因为他的表现更好？性能指标还不够好；它正确地排除了屋顶工1，但它错误地告诉我们选择屋顶工3，而不是屋顶工2。我们需要一个衡量他们复杂性的指标来帮助我们做出正确的决定。衡量他们复杂性的一个好指标是他们修理屋顶所收取的费用，以美元计。价格如下：
- en: Complexity (in price)
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 复杂性（以价格计）
- en: 'Roofer 1: $1'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 1: $1'
- en: 'Roofer 2: $100'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 2: $100'
- en: 'Roofer 3: $100,000'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 'Roofer 3: $100,000'
- en: 'Now we can tell that roofer 2 is better than roofer 3, because they had the
    same performance, but roofer 2 charged less. However, roofer 1 was the cheapest
    one—why on’t we go with this one? It seems that what we need is to combine the
    measures of performance and complexity. We can add the amount of water that the
    roof leaked and the price, to get the following:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以看出屋顶工 2 比屋顶工 3 更好，因为他们有相同的表现，但屋顶工 2 收费更低。然而，屋顶工 1 是最便宜的——为什么我们不选择这个呢？看起来我们需要做的是结合性能和复杂度的度量。我们可以加上屋顶漏水的量和价格，得到以下结果：
- en: Performance + complexity
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 性能 + 复杂度
- en: 'Roofer 1: 1001'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶工 1：1001
- en: 'Roofer 2: 101'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶工 2：101
- en: 'Roofer 3: 100,000'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 屋顶工 3：100,000
- en: 'Now it is clear that roofer 2 is the best one, which means that optimizing
    performance and complexity at the same time yields good results that are also
    as simple as possible. This is what regularization is about: measuring performance
    and complexity with two different error functions, and adding them to get a more
    robust error function. This new error function ensures that our model performs
    well and is not very complex. In the following sections, we get into more details
    on how to define these two error functions. But before that, let’s look at another
    overfitting example.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 现在很明显，屋顶工 2 是最好的，这意味着同时优化性能和复杂度可以得到既好又尽可能简单的结果。这就是正则化的目的：用两个不同的误差函数来衡量性能和复杂度，并将它们相加得到一个更鲁棒的误差函数。这个新的误差函数确保我们的模型表现良好且不太复杂。在接下来的章节中，我们将更详细地介绍如何定义这两个误差函数。但在那之前，让我们看看另一个过拟合的例子。
- en: 'Another example of overfitting: Movie recommendations'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个过拟合的例子：电影推荐
- en: 'In this section, we learn a more subtle way in which a model may overfit—this
    time not related to the degree of the polynomial but on the number of features
    and the size of the coefficients. Imagine that we have a movie streaming website,
    and we are trying to build a recommender system. For simplicity, imagine that
    we have only 10 movies: M1, M2, …, M10\. A new movie, M11, comes out, and we’d
    like to build a linear regression model to recommend movie 11 based on the previous
    10\. We have a dataset of 100 users. For each user, we have 10 features, which
    are the times (in seconds) that the user has watched each of the original 10 movies.
    If the user hasn’t watched a movie, then this amount is 0\. The label for each
    user is the amount of time the user watched movie 11\. We want to build a model
    that fits this dataset. Given that the model is a linear regression model, the
    equation for the predicted time the user will watch movie 11 is linear, and it
    will look like the following:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了一种更微妙的方式，模型可能会过拟合——这次与多项式的度数无关，而是与特征的数量和系数的大小有关。想象一下，我们有一个电影流媒体网站，我们正在尝试构建一个推荐系统。为了简单起见，假设我们只有
    10 部电影：M1, M2, …, M10。一部新电影 M11 出现了，我们希望基于之前的 10 部电影构建一个线性回归模型来推荐电影 11。我们有一个 100
    个用户的数据库。对于每个用户，我们有 10 个特征，这些特征是用户观看每部原始 10 部电影的时间（以秒为单位）。如果用户没有观看电影，那么这个量就是 0。每个用户的标签是用户观看电影
    11 的时间量。我们希望构建一个适合这个数据集的模型。鉴于模型是线性回归模型，预测用户将观看电影 11 的时间的方程是线性的，它看起来如下：
- en: '*ŷ* = *w*[1]*x*[1] + *w*[2]*x*[2] + *w*[3]*x*[3]+ *w*[4]*x*[4] + *w*[5]*x*[5]
    + *w*[6]*x*[6] + *w*[7]*x*[7] + *w*[8]*x*[8] ++ *w*[9]*x*[9] + *w*[10]*x*[10]
    + *b*,'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = *w*[1]*x*[1] + *w*[2]*x*[2] + *w*[3]*x*[3]+ *w*[4]*x*[4] + *w*[5]*x*[5]
    + *w*[6]*x*[6] + *w*[7]*x*[7] + *w*[8]*x*[8] ++ *w*[9]*x*[9] + *w*[10]*x*[10]
    + *b*，'
- en: where
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 其中
- en: '*ŷ* is the amount of time the model predicts that the user will watch movie
    11,'
  id: totrans-104
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*ŷ* 是模型预测用户将观看电影 11 的时间量，'
- en: '*x*[i] is the amount of time the user watched movie *i*, for *i* = 1, 2, …,
    10,'
  id: totrans-105
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*x*[i] 是用户观看电影 *i* 的时间量，对于 *i* = 1, 2, …, 10，'
- en: '*w*[i] is the weight associated to movie *i*, and'
  id: totrans-106
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*w*[i] 是与电影 *i* 相关的权重，并且'
- en: '*b* is the bias.'
  id: totrans-107
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b* 是偏差。'
- en: Now let’s test our intuition. Out of the following two models (given by their
    equation), which one (or ones) looks like it may be overfitting?
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来测试我们的直觉。在以下两个模型（由它们的方程给出）中，哪一个（或哪几个）看起来可能存在过拟合？
- en: '**Model 1**: *ŷ* = 2*x*[3] + 1.4*x*[7] – 0.5*x*[7] + 4'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型 1**：*ŷ* = 2*x*[3] + 1.4*x*[7] – 0.5*x*[7] + 4'
- en: '**Model 2**: *ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] +
    203*x*[6] + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型 2**：*ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] + 203*x*[6]
    + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
- en: If you think like me, model 2 seems a bit complicated, and it may be the one
    overfitting. The intuition here is that it’s unlikely that the time that a user
    watched movie 2 needs to be multiplied by –103 and then added to other numbers
    to obtain the prediction. This may fit the data well, but it definitely looks
    like it’s memorizing the data instead of learning it.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我一样思考，模型2似乎有点复杂，它可能是过度拟合的。这里的直觉是，用户观看电影2的时间乘以-103然后加到其他数字上以获得预测是不太可能的。这可能很好地拟合数据，但它肯定看起来像是在记忆数据而不是学习数据。
- en: Model 1, in contrast, looks much simpler, and it gives us some interesting information.
    From the fact that most of the coefficients are zero, except those for movies
    3, 7, and 9, it tells us that the only three movies that are related to movie
    11 are those three movies. Furthermore, from the fact that the coefficients of
    movies 3 and 7 are positive, the model tells us that if a user watched movie 3
    or movie 7, then they are likely to watch movie 11\. Because the coefficient of
    movie 9 is negative, then if the user watched movie 9, they are not likely to
    watch movie 11.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，模型1看起来要简单得多，并且它提供了一些有趣的信息。从大多数系数为零的事实来看，除了电影3、7和9的系数外，它告诉我们与电影11相关的只有这三部电影。此外，从电影3和7的系数为正的事实来看，模型告诉我们，如果一个用户观看了电影3或电影7，那么他们很可能也会观看电影11。因为电影9的系数为负，所以如果一个用户观看了电影9，他们不太可能观看电影11。
- en: Our goal is to have a model like model 1 and to avoid models like model 2\.
    But unfortunately, if model 2 produces a smaller error than model 2, then running
    the linear regression algorithm will select model 2 instead. What can we do? Here
    is where regularization comes to the rescue. The first thing we need is a measure
    that tells us that model 2 is much more complex than model 1.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是拥有像模型1这样的模型，并避免像模型2这样的模型。但不幸的是，如果模型2产生的误差比模型2小，那么运行线性回归算法将选择模型2。我们该怎么办？这正是正则化发挥作用的地方。我们首先需要的是一个告诉我们模型2比模型1复杂得多的度量。
- en: 'Measuring how complex a model is: L1 and L2 norm'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 衡量模型复杂度的方法：L1和L2范数
- en: In this section, we learn two ways to measure the complexity of a model. But
    before this, let’s look at models 1 and 2 from the previous section and try to
    come up with some formula that is low for model 1 and high for model 2.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习了两种衡量模型复杂度的方法。但在那之前，让我们看看上一节中的模型1和模型2，并尝试找出一些对模型1低而对模型2高的公式。
- en: 'Notice that a model with more coefficients, or coefficients of higher value,
    tends to be more complex. Therefore, any formula that matches this will work,
    such as the following:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到具有更多系数或系数值更高的模型往往更复杂。因此，任何符合这一点的公式都可以工作，例如以下公式：
- en: The sum of the absolute values of the coefficients
  id: totrans-117
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系数的绝对值之和
- en: The sum of the squares of the coefficients
  id: totrans-118
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系数的平方和
- en: The first one is called the *L1 norm*, and the second one is called the *L2
    norm*. They come from a more general theory of *L*^P spaces, named after the French
    mathematician Henri Lebesgue. We use absolute values and squares to get rid of
    the negative coefficients; otherwise, large negative numbers will cancel out with
    large positive numbers, and we may end up with a small value for a very complex
    model.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个被称为*L1范数*，第二个被称为*L2范数*。它们源自一个更一般的*L*^P空间理论，该理论以法国数学家亨利·勒贝格的名字命名。我们使用绝对值和平方来消除负系数；否则，大负数会与大的正数相抵消，我们可能会得到一个非常复杂模型的小值。
- en: 'But before we start calculating the norms, a small technicality: the bias in
    the models is not included in the L1 and L2 norm. Why? Well, the bias in the model
    is precisely the number of seconds that we expect a user to watch movie 11 if
    they haven’t watched any of the previous 10 movies. This number is not associated
    with the complexity of the model; therefore, we leave it alone. The calculation
    of the L1 norm for models 1 and 2 follows.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 但在我们开始计算范数之前，有一个小技术问题：模型中的偏差不包括在L1和L2范数中。为什么？好吧，模型中的偏差正是我们期望用户在观看前10部电影都没有观看的情况下观看电影11的秒数。这个数字与模型的复杂度无关；因此，我们让它保持原样。模型1和2的L1范数计算如下。
- en: 'Recall that the equations for the models are the following:'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，模型的方程如下：
- en: '**Model 1**: *ŷ* = 2*x*[3] + 1.4*x*[7] – 0.5*x*[7] + 8'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型1**：*ŷ* = 2*x*[3] + 1.4*x*[7] – 0.5*x*[7] + 8'
- en: '**Model 2**: *ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] +
    203*x*[6] + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型2**：*ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] + 203*x*[6]
    + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
- en: 'L1 norm:'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: L1范数：
- en: '**Model 1**: |2| + |1.4| + |–0.5| = 3.9'
  id: totrans-125
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**: |2| + |1.4| + |–0.5| = 3.9'
- en: '**Model 2**: |22| + |–103| + |–14| + |109| + |–93| + |203| + |87| + |–55| +
    |378| + |–25| = 1,089'
  id: totrans-126
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**: |22| + |–103| + |–14| + |109| + |–93| + |203| + |87| + |–55| + |378|
    + |–25| = 1,089'
- en: 'L2 norm:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 'L2范数:'
- en: '**Model 1**: 2² + 1.4² + (–0.5)² = 6.21'
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**: 2² + 1.4² + (–0.5)² = 6.21'
- en: '**Model 2**: 22² + (–103)² + (–14)² + 109² + (-93)² + 203² + 87² + (–55)² +
    378² + (–25)² = 227,131'
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**: 22² + (–103)² + (–14)² + 109² + (-93)² + 203² + 87² + (–55)² + 378²
    + (–25)² = 227,131'
- en: As expected, both the L1 and L2 norms of model 2 are much larger than the corresponding
    norms of model 1.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期，模型2的L1和L2范数都比模型1的相应范数大得多。
- en: 'The L1 and L2 norm can also be calculated on polynomials by taking either the
    sum of absolute values or the sum of squares of the coefficients, except for the
    constant coefficient. Let’s go back to the example at the beginning of this chapter,
    where our three models were a polynomial of degree 1 (a line), degree 2 (a parabola),
    and degree 10 (a curve that oscillates 9 times). Imagine that their formulas are
    the following:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2范数也可以通过对多项式的系数取绝对值之和或平方之和来计算，除了常数系数。让我们回到本章开头提到的例子，我们的三个模型分别是1次幂的多项式（一条直线）、2次幂的多项式（一个抛物线）和10次幂的多项式（一个振荡9次曲线）。想象一下它们的公式如下：
- en: '**Model 1**: *ŷ* = 2*x* + 3'
  id: totrans-132
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**: *ŷ* = 2*x* + 3'
- en: '**Model 2**: *ŷ* = –*x*² + 6*x* – 2'
  id: totrans-133
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**: *ŷ* = –*x*² + 6*x* – 2'
- en: '**Model 3**: *ŷ* = *x*⁹ + 4*x*⁸ – 9*x*⁷ + 3*x*⁶ – 14*x*⁵ – 2*x*⁴ – 9*x*³ +
    *x*² + 6*x* + 10'
  id: totrans-134
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型3**: *ŷ* = *x*⁹ + 4*x*⁸ – 9*x*⁷ + 3*x*⁶ – 14*x*⁵ – 2*x*⁴ – 9*x*³ + *x*²
    + 6*x* + 10'
- en: 'The L1 and L2 norms are calculated as follows:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2范数的计算如下：
- en: 'L1 norm:'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 'L1范数:'
- en: '**Model 1**: |2| = 2'
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**: |2| = 2'
- en: '**Model 2**: |–1| + |6| = 7'
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**: |–1| + |6| = 7'
- en: '**Model 3**: |1| + |4| + |–9| + |3| + |–14| + |–2| + |–9| + |1| + |6| = 49'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型3**: |1| + |4| + |–9| + |3| + |–14| + |–2| + |–9| + |1| + |6| = 49'
- en: 'L2 norm:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 'L2范数:'
- en: '**Model 1**: 2² = 2'
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型1**: 2² = 2'
- en: '**Model 2**: (–1)² + 6² = 37'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型2**: (–1)² + 6² = 37'
- en: '**Model 3**: 1² + 4² + (–9)² + 3² + (–14)² + (–2)² + (–9)² + 1² + 6² = 425'
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**模型3**: 1² + 4² + (–9)² + 3² + (–14)² + (–2)² + (–9)² + 1² + 6² = 425'
- en: Now that we are equipped with two ways to measure the complexity of the models,
    let’s embark into the training process.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经拥有了两种衡量模型复杂性的方法，让我们开始训练过程。
- en: 'Modifying the error function to solve our problem: Lasso regression and ridge
    regression'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 修改误差函数以解决问题：lasso回归和岭回归
- en: 'Now that we’ve done most of the heavy lifting, we’ll train a linear regression
    model using regularization. We have two measures for our model: a measure of performance
    (the error function) and a measure of complexity (the L1 or L2 norm).'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了大部分繁重的工作，我们将使用正则化训练一个线性回归模型。我们有两个衡量模型的标准：一个是性能的衡量（误差函数），另一个是复杂性的衡量（L1或L2范数）。
- en: 'Recall that in the roofer analogy, our goal was to find a roofer that provided
    both good quality and low complexity. We did this by minimizing the sum of two
    numbers: the measure of quality and the measure of complexity. Regularization
    consists of applying the same principle to our machine learning model. For this,
    we have two quantities: the regression error and the regularization term.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 记得在屋顶工人的类比中，我们的目标是找到一个既提供高质量又低复杂度的屋顶工人。我们通过最小化两个数的和来实现这一点：质量的衡量和复杂性的衡量。正则化就是将同样的原则应用到我们的机器学习模型上。为此，我们有两个量：回归误差和正则化项。
- en: regression error A measure of the quality of the model. In this case, it can
    be the absolute or square errors that we learned in chapter 3.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 回归误差：衡量模型质量的一个指标。在这种情况下，它可以是我们在第3章中学到的绝对误差或平方误差。
- en: regularization term A measure of the complexity of the model. It can be the
    L1 or the L2 norm of the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化项：衡量模型复杂性的一个指标。它可以是指模型的L1或L2范数。
- en: 'The quantity that we want to minimize to find a good and not too complex model
    is the modified error, defined as the sum of the two, as shown next:'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想要最小化的量，以找到一个既好又不复杂的模型，是修改后的误差，定义为这两个量的和，如下所示：
- en: Error = Regression error + Regularization term
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 误差 = 回归误差 + 正则化项
- en: 'Regularization is so common that the models themselves have different names
    based on what norm is used. If we train our regression model using the L1 norm,
    the model is called *lasso regression*. Lasso stands for “least absolute shrinkage
    and selection operator.” The error function follows:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 正则化如此普遍，以至于根据使用的范数，模型本身有不同的名称。如果我们使用L1范数训练我们的回归模型，该模型被称为*lasso回归*。Lasso代表“最小绝对收缩和选择算子”。误差函数如下：
- en: Lasso regression error = Regression error + L1 norm
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso回归误差 = 回归误差 + L1范数
- en: 'If, instead, we train the model using the L2 norm, it is called *ridge regression*.
    The name *ridge* comes from the shape of the error function, because adding the
    L2 norm term to the regression error function turns a sharp corner into a smooth
    valley when we plot it. The error function follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用L2 norm来训练模型，它被称为 *ridge regression*。这个名字 *ridge* 来自于误差函数的形状，因为当我们绘制它时，将L2
    norm项添加到回归误差函数中，将尖锐的角变成了平滑的谷。误差函数如下：
- en: Ridge regression error = Regression error + L2 norm
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: Ridge regression error = Regression error + L2 norm
- en: Both lasso and ridge regression work well in practice. The decision of which
    one to use comes down to some preferences that we’ll learn about in the upcoming
    sections. But before we get to that, we need to work out some details to make
    sure our regularized models work well.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: Lasso和ridge regression在实践中都表现良好。选择使用哪一个取决于我们将在接下来的章节中了解的一些偏好。但在我们到达那里之前，我们需要处理一些细节，以确保我们的正则化模型能够良好工作。
- en: 'Regulating the amount of performance and complexity in our model: The regularization
    parameter'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 调整模型中的性能和复杂度：正则化参数
- en: Because the process of training the model involves reducing the cost function
    as much as possible, a model trained with regularization, in principle, should
    have high performance and low complexity. However, there is some tug-of-war—trying
    to make the model perform better may make it more complex, whereas trying to reduce
    the complexity of the model may make it perform worse. Fortunately, most machine
    learning techniques come with knobs (hyperparameters) for the data scientist to
    turn and build the best possible models, and regularization is not an exception.
    In this section, we see how to use a hyperparameter to regulate between performance
    and complexity.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 因为训练模型的过程涉及尽可能减少成本函数，所以原则上，使用正则化的模型应该具有高性能和低复杂度。然而，存在一些拉锯战——试图让模型表现更好可能会使其更复杂，而试图降低模型的复杂度可能会使其表现更差。幸运的是，大多数机器学习技术都为数据科学家提供了可调节的旋钮（超参数），以构建最佳模型，正则化也不例外。在本节中，我们看看如何使用超参数在性能和复杂度之间进行调节。
- en: 'This hyperparameter is called the *regularization parameter*, and its goal
    is to determine if the model-training process should emphasize performance or
    simplicity. The regularization parameter is denoted by λ, the Greek letter *lambda*.
    We multiply the regularization term by λ, add it to the regression error, and
    use that result to train our model. The new error becomes the following:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 这个超参数被称为 *正则化参数*，其目标是确定模型训练过程是否应该强调性能或简单性。正则化参数用λ表示，即希腊字母 *lambda*。我们将正则化项乘以λ，加到回归误差上，并使用这个结果来训练我们的模型。新的误差如下：
- en: Error = Regression error + λ Regularization term
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: Error = Regression error + λ Regularization term
- en: Picking a value of 0 for λ cancels out the regularization term, and thus we
    end up with the same regression model we had in chapter 3\. Picking a large value
    for λ results in a simple model, perhaps of low degree, which may not fit our
    dataset very well. It is crucial to pick a good value for λ, and for this, validation
    is a useful technique. It is typical to choose powers of 10, such as 10, 1, 0.1,
    0.01, but this choice is somewhat arbitrary. Among these, we select the one that
    makes our model perform best in our validation set.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 将λ的值设为0会取消正则化项，因此我们最终得到与第3章中相同的回归模型。将λ的值设得很大会导致一个简单的模型，可能度数较低，可能不太适合我们的数据集。选择一个合适的λ值至关重要，为此，验证是一种有用的技术。通常选择10的幂，如10、1、0.1、0.01，但这种选择有些随意。在这些中，我们选择使我们的模型在验证集中表现最好的那个。
- en: Effects of L1 and L2 regularization in the coefficients of the model
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: L1和L2正则化对模型系数的影响
- en: In this section, we see crucial differences between L1 and L2 regularization
    and get some ideas about which one to use in different scenarios. At first glance,
    they look similar, but the effects they have on the coefficients is interesting,
    and, depending on what type of model we want, deciding between using L1 and L2
    regularization can be critical.
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将看到L1和L2正则化之间的关键差异，并得到一些在不同场景中使用哪一个的想法。乍一看，它们看起来很相似，但它们对系数产生的影响很有趣，而且，根据我们想要的模型类型，决定使用L1还是L2正则化可能是关键的。
- en: 'Let’s go back to our movie recommendation example, where we are building a
    regression model to predict the amount of time (in seconds) that a user will watch
    a movie, given the time that same user has watched 10 different movies. Imagine
    that we’ve trained the model, and the equation we get is the following:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的电影推荐示例，其中我们正在构建一个回归模型来预测用户观看电影的时间（以秒为单位），给定该用户观看10部不同电影的时间。想象一下我们已经训练了模型，得到的方程如下：
- en: '**Model**: *ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] + 203*x*[6]
    + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: '**模型**：*ŷ* = 22*x*[1] – 103*x*[2] – 14*x*[3] + 109*x*[4] – 93*x*[5] + 203*x*[6]
    + 87*x*[7] – 55*x*[8] + 378*x*[9] – 25*x*[10] + 8'
- en: 'If we add regularization and train the model again, we end up with a simpler
    model. The following two properties can be shown mathematically:'
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们添加正则化并再次训练模型，最终得到的模型将更简单。以下两个性质可以通过数学证明：
- en: If we use L1 regularization (lasso regression), you end up with a model with
    fewer coefficients. In other words, L1 regularization turns some of the coefficients
    into zero. Thus, we may end up with an equation like *ŷ* = 2*x*[3] + 1.4*x*[7]
    – 0.5*x*[9] + 8.
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用L1正则化（lasso回归），最终得到的模型将具有更少的系数。换句话说，L1正则化将一些系数变为零。因此，我们可能会得到一个类似 *ŷ* =
    2*x*[3] + 1.4*x*[7] – 0.5*x*[9] + 8 的方程。
- en: If we use L2 regularization (ridge regression), we end up with a model with
    smaller coefficients. In other words, L2 regularization shrinks all the coefficients
    but rarely turns them into zero. Thus, we may end up with an equation like *ŷ*
    = 0.2*x*[1] – 0.8*x*[2] – 1.1*x*[3] + 2.4*x*[4] – 0.03*x*[5] + 1.02*x*[6] + 3.1*x*[7]
    – 2*x*[8] + 2.9*x*[9] – 0.04*x*[10] + 8.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们使用L2正则化（ridge回归），最终得到的模型将具有更小的系数。换句话说，L2正则化会缩小所有系数，但很少将它们变为零。因此，我们可能会得到一个类似
    *ŷ* = 0.2*x*[1] – 0.8*x*[2] – 1.1*x*[3] + 2.4*x*[4] – 0.03*x*[5] + 1.02*x*[6]
    + 3.1*x*[7] – 2*x*[8] + 2.9*x*[9] – 0.04*x*[10] + 8 的方程。
- en: Thus, depending on what kind of equation we want to get, we can decide between
    using L1 and L2 regularization.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，根据我们想要得到的方程类型，我们可以决定使用L1还是L2正则化。
- en: 'A quick rule of thumb to use when deciding if we want to use L1 or L2 regularization
    follows: if we have too many features and we’d like to get rid of most of them,
    L1 regularization is perfect for that. If we have only few features and believe
    they are all relevant, then L2 regularization is what we need, because it won’t
    get rid of our useful features.'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 当决定是否要使用L1或L2正则化时，可以遵循的一个快速规则是：如果我们有很多特征并且想要去除其中大部分，L1正则化非常适合这种情况。如果我们只有很少的特征并且认为它们都相关，那么我们需要L2正则化，因为它不会去除我们的有用特征。
- en: 'An example of a problem in which we have many features and L1 regularization
    can help us is the movie recommendation system we studied in the section “Another
    example of overfitting: Movie recommendations.” In this model, each feature corresponded
    to one of the movies, and our goal is to find the few movies that were related
    to the one we’re interested in. Thus, we need a model for which most of the coefficients
    are zero, except for a few of them.'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 在“另一个过拟合示例：电影推荐”部分中，我们研究的一个问题中，我们有很多特征，L1正则化可以帮助我们。在这个模型中，每个特征对应一部电影，我们的目标是找到与我们感兴趣的电影相关的少数几部电影。因此，我们需要一个大多数系数为零，只有少数几个系数不为零的模型。
- en: 'An example in which we should use L2 regularization is the polynomial example
    at the beginning of the section “An example of underfitting using polynomial regression.”
    For this model, we had only one feature: *x*. L2 regularization would give us
    a good polynomial model with small coefficients, which wouldn’t oscillate very
    much and, thus, is less prone to overfitting. In the section “Polynomial regression,
    testing, and regularization with Turi Create,” we will see a polynomial example
    for which L2 regularization is the right one to use.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该使用L2正则化的一个例子是“使用多项式回归的欠拟合示例”部分开头的一个多项式示例。对于这个模型，我们只有一个特征：*x*。L2正则化会给我们一个具有小系数的良好多项式模型，它不会振荡得很厉害，因此不太容易过拟合。在“使用Turi
    Create进行多项式回归、测试和正则化”部分，我们将看到一个适合使用L2正则化的多项式示例。
- en: The resources corresponding to this chapter (appendix C) point to some places
    where you can dig deeper into the mathematical reasons why L1 regularization turns
    coefficients into zero, whereas L2 regularization turns them into small numbers.
    In the next section, we will learn how to get an intuition for it.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 与本章（附录C）相关的资源指向了一些地方，你可以深入挖掘为什么L1正则化将系数变为零，而L2正则化将它们变为小数的原因。在下一节中，我们将学习如何获得对此的直观理解。
- en: An intuitive way to see regularization
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 一种直观地看待正则化的方式
- en: In this section, we learn how the L1 and L2 norms differ in the way they penalize
    complexity. This section is mostly intuitive and is developed in an example, but
    if you’d like to see the formal mathematics behind them, please look at appendix
    B, “Using gradient descent for regularization.”
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习 L1 和 L2 范数在惩罚复杂度方面的不同。本节主要基于直观，并通过示例进行阐述，但如果你想看到它们背后的形式化数学，请参阅附录 B，“使用梯度下降进行正则化。”
- en: When we try to understand how a machine learning model operates, we should look
    beyond the error function. An error function says, “This is the error, and if
    you reduce it, you end up with a good model.” But that is like saying, “The secret
    to succeeding in life is to make as few mistakes as possible.” Isn’t a positive
    message better, such as, “These are the things you can do to improve your life,”
    as opposed to “These are the things you should avoid”? Let’s see regularization
    in this way.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们试图理解机器学习模型如何运作时，我们应该超越错误函数。错误函数会说，“这是错误，如果你减少它，你最终会得到一个好的模型。”但这就像说，“成功生活的秘诀是尽可能少犯错误。”一个积极的信息不是更好吗，比如，“这些都是你可以做的事情来改善你的生活”，而不是“这些都是你应该避免的事情”？让我们以这种方式看看正则化。
- en: In chapter 3, we learned the absolute and the square tricks, which give us a
    clearer glimpse into regression. At each stage in the training process, we simply
    pick a point (or several points) and move the line closer to those points. Repeating
    this process many times will eventually yield a good line fit. We can be more
    specific and repeat how we defined the linear regression algorithm in chapter
    3.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 在第 3 章中，我们学习了绝对值和平方技巧，这让我们对回归有了更清晰的了解。在训练过程的每个阶段，我们只是选择一个点（或几个点），并将线移动到这些点附近。重复这个过程多次，最终会得到一个好的线拟合。我们可以更具体地重复第
    3 章中定义的线性回归算法。
- en: Pseudocode for the linear regression algorithm
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法的伪代码
- en: '**Inputs**: A dataset of points'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**: 点的集合数据集'
- en: '**Outputs**: A linear regression model that fits that dataset'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**: 一个适合该数据集的线性回归模型'
- en: 'Procedure:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Pick a model with random weights and a random bias.
  id: totrans-182
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个具有随机权重和随机偏差的模型。
- en: 'Repeat many times:'
  id: totrans-183
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-184
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Slightly adjust the weights and bias to improve the prediction for that data
    point.
  id: totrans-185
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微调整权重和偏差以改善该数据点的预测。
- en: Enjoy your model!
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 享受你的模型吧！
- en: Can we use the same reasoning to understand regularization? Yes, we can.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能否用同样的推理来理解正则化？是的，我们可以。
- en: 'To simplify things, let’s say that we are in the middle of our training, and
    we want to make the model simpler. We can do this by reducing the coefficients.
    For simplicity, let’s say that our model has three coefficients: 3, 10, and 18\.
    Can we take a small step to decrease these three by a small amount? Of course
    we can, and here are two methods to do it. Both require a small number, λ, which
    we’ll set to 0.01 for now.'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简化问题，让我们假设我们正处于训练过程中，我们想要使模型更简单。我们可以通过减少系数来实现这一点。为了简单起见，让我们假设我们的模型有三个系数：3、10
    和 18。我们能否通过小幅度减少这三个系数？当然可以，这里有两种方法来做这件事。这两种方法都需要一个小的数字 λ，我们现在将其设置为 0.01。
- en: '**Method 1**: Subtract λ from each of the positive parameters, and add λ to
    each of the negative parameters. If they are zero, leave them alone.'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法 1**: 从每个正参数中减去 λ，并将 λ 添加到每个负参数中。如果它们为零，则保持不变。'
- en: '**Method 2**: Multiply all of them by 1 – λ. Notice that this number is close
    to 1, because λ is small.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '**方法 2**: 将所有参数乘以 1 – λ。注意这个数字接近 1，因为 λ 很小。'
- en: Using method 1, we get the numbers 2.99, 9.99, and 17.99.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方法 1，我们得到数字 2.99、9.99 和 17.99。
- en: Using method 2, we get the numbers 2.97, 9.9, and 17.82.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 使用方法 2，我们得到数字 2.97、9.9 和 17.82。
- en: 'In this case, λ behaves very much like a learning rate. In fact, it is closely
    related to the regularization rate (see “Using gradient descent for regularization”
    in appendix B for details). Notice that in both methods, we are shrinking the
    size of the coefficients. Now, all we have to do is to repeatedly shrink the coefficients
    at every stage of the algorithm. In other words, here is how we train the model
    now:'
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，λ 非常像学习率。事实上，它与正则化率密切相关（有关详细信息，请参阅附录 B 中的“使用梯度下降进行正则化”）。请注意，在这两种方法中，我们都在缩小系数的大小。现在，我们只需要在算法的每个阶段重复缩小系数。换句话说，这就是我们现在如何训练模型：
- en: '**Inputs**: A dataset of points'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**: 点的集合数据集'
- en: '**Outputs**: A linear regression model that fits that dataset'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**: 一个适合该数据集的线性回归模型'
- en: 'Procedure:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Pick a model with random weights and a random bias.
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个具有随机权重和随机偏差的模型。
- en: 'Repeat many times:'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-199
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Slightly adjust the weights and bias to improve the prediction for that particular
    data point.
  id: totrans-200
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微调整权重和偏差以改善该特定数据点的预测。
- en: '**Slightly shrink the coefficients using method 1 or method 2**.'
  id: totrans-201
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**使用方法1或方法2略微缩小系数**。'
- en: Enjoy your model!
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 享受你的模型吧！
- en: If we use method 1, we are training the model with L1 regularization, or lasso
    regression. If we use method 2, we are training it with L2 regularization, or
    ridge regression. There is a mathematical justification for this, which is described
    in appendix B, “Using gradient descent for regularization.”
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们使用方法1，我们正在用L1正则化或lasso回归训练模型。如果我们使用方法2，我们正在用L2正则化或ridge回归训练它。对此有一个数学上的解释，在附录B，“使用梯度下降进行正则化”中描述。
- en: 'In the previous section, we learned that L1 regularization tends to turn many
    coefficients into 0, whereas L2 regularization tends to decrease them but not
    turn them into zero. This phenomenon is now easier to see. Let’s say that our
    coefficient is 2, with a regularization parameter of λ = 0.01\. Notice what happens
    if we use method 1 to shrink our coefficient, and we repeat this process 200 times.
    We get the following sequence of values:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们了解到L1正则化倾向于将许多系数变为0，而L2正则化倾向于减小它们但不会将它们变为零。这一现象现在更容易看到。假设我们的系数是2，正则化参数为λ
    = 0.01。注意如果我们使用方法1来缩小我们的系数，并且重复这个过程200次会发生什么。我们得到以下数值序列：
- en: 2 → 1.99 → 1.98 → ··· → 0.02 → 0.01 → 0
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 2 → 1.99 → 1.98 → ··· → 0.02 → 0.01 → 0
- en: 'After 200 epochs of our training, the coefficient becomes 0, and it never changes
    again. Now let’s see what would happen if we apply method 2, again 200 times and
    with the same learning rate of *η* = 0.01\. We get the following sequence of values:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的训练200个epoch之后，系数变为0，并且再也没有变化。现在让我们看看如果我们再次应用方法2，再进行200次，并且使用相同的学习率*η* =
    0.01会发生什么。我们得到以下数值序列：
- en: 2 → 1.98 → 1.9602 → ··· → 0.2734 → 0.2707 → 0.2680
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 2 → 1.98 → 1.9602 → ··· → 0.2734 → 0.2707 → 0.2680
- en: Notice that the coefficient decreased dramatically, but it didn’t become zero.
    In fact, no matter how many epochs we run, the coefficient will never become zero.
    This is because when we multiply a non-negative number by 0.99 many times, the
    number will never become zero. This is illustrated in figure 4.7.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到系数急剧下降，但并没有变成零。事实上，无论我们运行多少个epoch，系数永远不会变成零。这是因为当我们多次将一个非负数乘以0.99时，这个数永远不会变成零。这如图4.7所示。
- en: '![](../Images/4-7.png)'
  id: totrans-209
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-7.png)'
- en: Figure 4.7 Both L1 and L2 shrink the size of the coefficient. L1 regularization
    (left) does it much faster, because it subtracts a fixed amount, so it is likely
    to eventually become zero. L2 regularization takes much longer, because it multiplies
    the coefficient by a small factor, so it never reaches zero.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.7 L1和L2正则化都缩小了系数的大小。L1正则化（左侧）做得更快，因为它减去一个固定值，所以它最终可能变成零。L2正则化需要更长的时间，因为它将系数乘以一个很小的因子，所以它永远不会达到零。
- en: Polynomial regression, testing, and regularization with Turi Create
  id: totrans-211
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Turi Create进行多项式回归、测试和正则化
- en: 'In this section, we see an example of polynomial regression with regularization
    in Turi Create. Here is the code for this section:'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到了Turi Create中带有正则化的多项式回归的示例。以下是本节的代码：
- en: '**Notebook**:'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**：'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_4_Testing_Overfitting_Underfitting/Polynomial_regression_regularization.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_4_Testing_Overfitting_Underfitting/Polynomial_regression_regularization.ipynb)'
  id: totrans-214
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_4_Testing_Overfitting_Underfitting/Polynomial_regression_regularization.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_4_Testing_Overfitting_Underfitting/Polynomial_regression_regularization.ipynb)'
- en: We start with our dataset, which is illustrated in figure 4.8\. We can see the
    curve that best fits this data is a parabola that opens downward (a sad face).
    Therefore, it is not a problem we can solve with linear regression—we must use
    polynomial regression. The dataset is stored in an SFrame called `data`, and the
    first few rows are shown in table 4.1.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从我们的数据集开始，如图4.8所示。我们可以看到最适合这些数据的曲线是一个开口向下的抛物线（一个悲伤的脸）。因此，这不是我们可以用线性回归解决的问题——我们必须使用多项式回归。数据集存储在一个名为`data`的SFrame中，前几行如表4.1所示。
- en: '![](../Images/4-8.png)'
  id: totrans-216
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/4-8.png)'
- en: Figure 4.8 The dataset. Notice that its shape is a parabola that opens downward,
    so using linear regression won’t work well. We’ll use polynomial regression to
    fit this dataset, and we’ll use regularization to tune our model.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: 图 4.8 数据集。注意，它的形状是一个向下开口的抛物线，所以使用线性回归效果不好。我们将使用多项式回归来拟合这个数据集，并使用正则化来调整我们的模型。
- en: Table 4.1 The first four rows of our dataset
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.1 我们数据集的前四行
- en: '| *x*   | *y*   |'
  id: totrans-219
  prefs: []
  type: TYPE_TB
  zh: '| *x*   | *y*   |'
- en: '| 3.4442185152504816 | 6.685961311021467 |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| 3.4442185152504816 | 6.685961311021467 |'
- en: '| -2.4108324970703663 | 4.690236225597948 |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| –2.4108324970703663 | 4.690236225597948 |'
- en: '| 0.11274721368608542 | 12.205789026637378 |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| 0.11274721368608542 | 12.205789026637378 |'
- en: '| -1.9668727392107255 | 11.133217991032268 |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| -1.9668727392107255 | 11.133217991032268 |'
- en: The way to do polynomial regression in Turi Create is to add many columns to
    our dataset, corresponding to the powers of the main feature, and to apply linear
    regression to this expanded dataset. If the main feature is, say, *x*, then we
    add columns with the values of *x*², *x*³, *x*⁴, and so on. Thus, our model is
    finding linear combinations of the powers of *x*, which are precisely polynomials
    in *x*. If the SFrame containing our data is called `data`, we use the following
    code to add columns for powers up to *x*^(199). The first few rows and columns
    of the resulting dataset appear in table 4.2.
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Turi Create 中进行多项式回归的方法是向我们的数据集中添加许多列，对应于主要特征的幂，然后对此扩展数据集应用线性回归。如果主要特征是，比如说，*x*，那么我们添加具有
    *x*²、*x*³、*x*⁴ 等值的列。因此，我们的模型正在寻找 *x* 幂的线性组合，这恰好是 *x* 上的多项式。如果包含我们数据的 SFrame 被称为
    `data`，我们使用以下代码添加到 *x*^(199) 的幂的列。结果数据集的前几行和列显示在表 4.2 中。
- en: '[PRE0]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Table 4.2 The top four rows and the leftmost five columns of our dataset. The
    column labeled *x*^*k* corresponds to the variable *x*^*k*, for *k* = 2, 3, and
    4\. The dataset has 200 columns.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 表 4.2 我们数据集的前四行和最左边的五列。标记为 *x*^*k* 的列对应于变量 *x*^*k*，其中 *k* = 2、3 和 4。数据集有 200
    列。
- en: '| x  | y   | x^2 | x^3 | x^4 |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| x  | y   | x^2 | x^3 | x^4 |'
- en: '| 3.445 | 6.686 | 11.863 | 40.858 | 140.722 |'
  id: totrans-228
  prefs: []
  type: TYPE_TB
  zh: '| 3.445 | 6.686 | 11.863 | 40.858 | 140.722 |'
- en: '| –2.411 | 4.690 | 5.812 | –14.012 | 33.781 |'
  id: totrans-229
  prefs: []
  type: TYPE_TB
  zh: '| –2.411 | 4.690 | 5.812 | –14.012 | 33.781 |'
- en: '| 0.113 | 12.206 | 0.013 | 0.001 | 0.000 |'
  id: totrans-230
  prefs: []
  type: TYPE_TB
  zh: '| 0.113 | 12.206 | 0.013 | 0.001 | 0.000 |'
- en: '| –1.967 | 11.133 | 3.869 | –7.609 | 14.966 |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| –1.967 | 11.133 | 3.869 | –7.609 | 14.966 |'
- en: Now, we apply linear regression to this large dataset with 200 columns. Notice
    that a linear regression model in this dataset looks like a linear combination
    of the variables in the columns. But because each column corresponds to a monomial,
    then the model obtained looks like a polynomial on the variable *x*.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将线性回归应用于这个有 200 列的大数据集。请注意，这个数据集中的线性回归模型看起来像是列中变量的线性组合。但是，因为每个列对应一个单项式，所以得到的模型看起来像是在变量
    *x* 上的多项式。
- en: 'Before we train any models, we need to split the data into training and testing
    datasets, using the following line of code:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们训练任何模型之前，我们需要使用以下行代码将数据分成训练集和测试集：
- en: '[PRE1]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Now our dataset is split into two datasets, the training set called *train*
    and the testing set called *test*. In the repository, a random seed is specified,
    so we always get the same results, although this is not necessary in practice.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们的数据集被分成两个数据集，训练集称为 *train*，测试集称为 *test*。在存储库中指定了一个随机种子，所以我们总是得到相同的结果，尽管在实际中这不是必需的。
- en: 'The way to use regularization in Turi Create is simple: all we need to do is
    specify the parameters `l1_penalty` and `l2_penalty` in the `create` method when
    we train the model. This penalty is precisely the regularization parameter we
    introduced in the section “Regulating the amount of performance and complexity
    in our model.” A penalty of 0 means we are not using regularization. Thus, we
    will train three different models with the following parameters:'
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Turi Create 中使用正则化的方法是简单的：我们只需要在训练模型时在 `create` 方法中指定参数 `l1_penalty` 和 `l2_penalty`。这个惩罚正是我们在“调节模型性能和复杂度”部分中引入的正则化参数。0
    的惩罚意味着我们没有使用正则化。因此，我们将使用以下参数训练三个不同的模型：
- en: 'No regularization model:'
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无正则化模型：
- en: '`l1_penalty=0`'
  id: totrans-238
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_penalty=0`'
- en: '`l2_penalty=0`'
  id: totrans-239
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2_penalty=0`'
- en: 'L1 regularization model:'
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1 正则化模型：
- en: '`l1_penalty=0.1`'
  id: totrans-241
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_penalty=0.1`'
- en: '`l2_penalty=0`'
  id: totrans-242
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2_penalty=0`'
- en: 'L2 regularization model:'
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2 正则化模型：
- en: '`l1_penalty=0`'
  id: totrans-244
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l1_penalty=0`'
- en: '`l2_penalty=0.1`'
  id: totrans-245
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`l2_penalty=0.1`'
- en: 'We train the models with the following three lines of code:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用以下三行代码来训练模型：
- en: '[PRE2]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The first model uses no regularization, the second one uses L1 regularization
    with a parameter of 0.1, and the third uses L2 regularization with a parameter
    of 0.1\. The plots of the resulting functions are shown in figure 4.9\. Notice
    that in this figure, the points in the training set are circles, and those in
    the testing set are triangles.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 第一个模型没有正则化，第二个模型使用L1正则化，参数为0.1，第三个模型使用L2正则化，参数为0.1。结果函数的图示如图4.9所示。注意，在这个图中，训练集的点用圆圈表示，测试集的点用三角形表示。
- en: '![](../Images/4-9.png)'
  id: totrans-249
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/4-9.png)'
- en: Figure 4.9 Three polynomial regression models for our dataset. The model on
    the left has no regularization, the model in the middle has L1 regularization
    with a parameter of 0.1, and the model on the right has L2 regularization with
    a parameter of 0.1.
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 图4.9展示了针对我们数据集的三个多项式回归模型。左侧的模型没有正则化，中间的模型具有L1正则化，参数为0.1，右侧的模型具有L2正则化，参数为0.1。
- en: Notice that the model with no regularization fits the training points really
    well, but it’s chaotic and doesn’t fit the testing points well. The model with
    L1 regularization does OK with both the training and the testing sets. But the
    model with L2 regularization does a wonderful job with both the training and the
    testing sets and also seems to be the one that really captures the shape of the
    data.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，没有正则化的模型非常吻合训练点，但它很混乱，并且没有很好地拟合测试点。L1正则化的模型在训练集和测试集上都表现不错。但L2正则化的模型在训练集和测试集上都做得很好，并且似乎真正捕捉到了数据的形状。
- en: Also note that for the three models, the boundary curve goes a bit crazy on
    the end points. This is completely understandable, because the endpoints have
    less data, and it is natural for the model to not know what to do when there’s
    no data. We should always evaluate models by how well they perform inside the
    boundaries of our dataset, and we should never expect a model to do well outside
    of those boundaries. Even we humans may not be able to make good predictions outside
    of the boundaries of the model. For instance, how do you think this curve would
    look outside of the dataset? Would it continue as parabola that opens downward?
    Would it oscillate forever like a sine function? If we don’t know this, we shouldn’t
    expect the model to know it. Thus, try to ignore the strange behavior at the end
    points in figure 4.9, and focus on the behavior of the model inside the interval
    where the data is located.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 还要注意，对于这三个模型，边界曲线在端点处有点疯狂。这是完全可以理解的，因为端点处的数据较少，当没有数据时，模型不知道该怎么做是很自然的。我们应该始终通过模型在我们数据集边界内的表现来评估模型，我们不应该期望模型在那些边界之外表现良好。即使我们人类可能也无法在模型边界之外做出良好的预测。例如，你认为这个曲线在数据集之外会是什么样子？它会继续作为向下开口的抛物线吗？它会像正弦函数一样永远振荡吗？如果我们不知道这些，我们就不应该期望模型知道。因此，尝试忽略图4.9中端点的奇怪行为，并关注模型在数据所在区间内的行为。
- en: To find the testing error, we use the following line of code, with the corresponding
    name of the model. This line of code returns the maximum error and the root mean
    square error (RMSE).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 要找到测试误差，我们使用以下代码行，并指定相应的模型名称。此代码行返回最大误差和均方根误差（RMSE）。
- en: '[PRE3]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The testing RMSE for the models follow:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的测试RMSE如下：
- en: 'Model with no regularization: 699.03'
  id: totrans-256
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 无正则化模型：699.03
- en: 'Model with L1 regularization: 10.08'
  id: totrans-257
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1正则化模型：10.08
- en: 'Model with L2 regularization: 3.41'
  id: totrans-258
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L2正则化模型：3.41
- en: 'The model with no regularization had a really large RMSE! Among the other two
    models, the one with L2 regularization performed much better. Here are two questions
    to think about:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 没有正则化的模型RMSE非常大！在其他两个模型中，具有L2正则化的模型表现要好得多。以下是一些需要思考的问题：
- en: Why did the model with L2 regularization perform better than the one with L1
    regularization?
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么L2正则化的模型比L1正则化的模型表现更好？
- en: Why does the model with L1 regularization look flat, whereas the model with
    L2 regularization managed to capture the shape of the data?
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为什么L1正则化的模型看起来很平坦，而L2正则化的模型却成功地捕捉到了数据的形状？
- en: 'The two questions have a similar answer, and to find it, we can look at the
    coefficients of the polynomials. These can be obtained with the following line
    of code:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个问题的答案类似，要找到答案，我们可以查看多项式的系数。这些可以通过以下代码行获得：
- en: '[PRE4]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Each polynomial has 200 coefficients, so we won’t display all of them here,
    but in table 4.3 you can see the first five coefficients for the three models.
    What do you notice?
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 每个多项式都有200个系数，所以我们在这里不会显示所有系数，但在表4.3中你可以看到三个模型的前五个系数。你注意到了什么？
- en: Table 4.3 The first five coefficients of the polynomials in our three models.
    Note that the model with no regularization has large coefficients, the model with
    L1 regularization has coefficients very close to 0, and the model with L2 regularization
    has small coefficients.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 表4.3 我们三个模型中多项式的第一个五个系数。注意，没有正则化的模型具有大的系数，具有L1正则化的模型具有非常接近0的系数，而具有L2正则化的模型具有小的系数。
- en: '| Coefficient | model_no_reg | model_L1_reg | model_L2_reg |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| 系数 | model_no_reg | model_L1_reg | model_L2_reg |'
- en: '| *x*⁰ = 1 | 8.41 | 0.57 | 13.24 |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| *x*⁰ = 1 | 8.41 | 0.57 | 13.24 |'
- en: '| *x*¹ | 15.87 | 0.07 | 0.87 |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| *x*¹ | 15.87 | 0.07 | 0.87 |'
- en: '| *x*² | 108.87 | –0.004 | –0.52 |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| *x*² | 108.87 | –0.004 | –0.52 |'
- en: '| *x*³ | –212.89 | 0.0002 | 0.006 |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| *x*³ | –212.89 | 0.0002 | 0.006 |'
- en: '| *x*⁴ | –97.13 | –0.0002 | –0.02 |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| *x*⁴ | –97.13 | –0.0002 | –0.02 |'
- en: 'To interpret table 4.3, we see the predictions for the three models are polynomials
    of degree 200\. The first terms look as follows:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解释表4.3，我们看到三个模型的预测都是200次方的多项式。第一项如下所示：
- en: 'Model with no regularization: *ŷ* = 8.41 + 15.87*x* + 108.87*x*² – 212.89*x*³
    – 97.13*x*⁴ + …'
  id: totrans-273
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 没有正则化的模型：*ŷ* = 8.41 + 15.87*x* + 108.87*x*² – 212.89*x*³ – 97.13*x*⁴ + …
- en: 'Model with L1 regularization: *ŷ* = 0.57 + 0.07*x* – 0.004*x*² + 0.0002*x*³
    – 0.0002*x*⁴ + …'
  id: totrans-274
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有L1正则化的模型：*ŷ* = 0.57 + 0.07*x* – 0.004*x*² + 0.0002*x*³ – 0.0002*x*⁴ + …
- en: 'Model with L2 regularization: *ŷ* = 13.24 + 0.87*x* – 0.52*x*² + 0.006*x*³
    – 0.02*x*⁴ + …'
  id: totrans-275
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 具有L2正则化的模型：*ŷ* = 13.24 + 0.87*x* – 0.52*x*² + 0.006*x*³ – 0.02*x*⁴ + …
- en: 'From these polynomials, we see the following:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些多项式中，我们可以看到以下情况：
- en: For the model with no regularization, all the coefficients are large. This means
    the polynomial is chaotic and not good for making predictions.
  id: totrans-277
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于没有正则化的模型，所有系数都很大。这意味着多项式是混沌的，不适合进行预测。
- en: For the model with L1 regularization, all the coefficients, except for the constant
    one (the first one), are tiny—almost 0\. This means that for the values close
    to zero, the polynomial looks a lot like the horizontal line with equation *ŷ*
    = 0.57\. This is better than the previous model but still not great for making
    predictions.
  id: totrans-278
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有L1正则化的模型，除了常数项（第一个项）之外的所有系数都非常小——几乎为0。这意味着对于接近零的值，多项式看起来非常像方程 *ŷ* = 0.57
    的水平线。这比之前的模型要好，但仍然不适合进行预测。
- en: For the model with L2 regularization, the coefficients get smaller as the degree
    grows but are still not so small. This gives us a decent polynomial for making
    predictions.
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于具有L2正则化的模型，随着次数的增长，系数会变小，但仍然不是那么小。这为我们提供了一个相当不错的多项式来进行预测。
- en: Summary
  id: totrans-280
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 概述
- en: When it comes to training models, many problems arise. Two problems that come
    up quite often are underfitting and overfitting.
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当涉及到训练模型时，会出现许多问题。出现频率较高的两个问题是欠拟合和过拟合。
- en: Underfitting occurs when we use a very simple model to fit our dataset. Overfitting
    occurs when we use an overly complex model to fit our dataset.
  id: totrans-282
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们使用一个非常简单的模型来拟合我们的数据集时，会发生欠拟合。当我们使用过于复杂的模型来拟合我们的数据集时，会发生过拟合。
- en: An effective way to tell overfitting and underfitting apart is by using a testing
    dataset.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过使用测试数据集，我们可以有效地区分过拟合和欠拟合。
- en: 'To test a model, we split the data into two sets: a training set and a testing
    set. The training set is used to train the model, and the testing set is used
    to evaluate the model.'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为了测试一个模型，我们将数据分为两个集合：训练集和测试集。训练集用于训练模型，测试集用于评估模型。
- en: The golden rule of machine learning is to never use our testing data for training
    or making decisions in our models.
  id: totrans-285
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器学习的黄金法则是不使用我们的测试数据来训练或在我们的模型中做出决策。
- en: The validation set is another portion of our dataset that we use to make decisions
    about the hyperparameters in our model.
  id: totrans-286
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 验证集是我们数据集的另一部分，我们用它来决定模型中的超参数。
- en: A model that underfits will perform poorly in the training set and in the validation
    set. A model that overfits will perform well in the training set but poorly in
    the validation set. A good model will perform well on both the training and the
    validation sets.
  id: totrans-287
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 欠拟合的模型在训练集和验证集上表现不佳。过拟合的模型在训练集上表现良好，但在验证集上表现不佳。一个好的模型在训练集和验证集上都会表现良好。
- en: The model complexity graph is used to determine the correct complexity of a
    model, so that it doesn’t underfit or overfit.
  id: totrans-288
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型复杂度图用于确定模型的正确复杂度，以确保它不会欠拟合或过拟合。
- en: Regularization is a very important technique to reduce overfitting in machine
    learning models. It consists of adding a measure of complexity (regularization
    term) to the error function during the training process.
  id: totrans-289
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正则化是减少机器学习模型过拟合的一个重要技术。它包括在训练过程中将复杂度的度量（正则化项）添加到误差函数中。
- en: The L1 and L2 norms are the two most common measures of complexity used in regularization.
  id: totrans-290
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: L1和L2范数是正则化中最常用的两种复杂度度量。
- en: Using the L1 norm leads to L1 regularization, or lasso regression. Using the
    L2 norm leads to L2 regularization, or ridge regression.
  id: totrans-291
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用L1范数会导致L1正则化，或套索回归。使用L2范数会导致L2正则化，或岭回归。
- en: L1 regularization is recommended when our dataset has numerous features, and
    we want to turn many of them into zero. L2 regularization is recommended when
    our dataset has few features, and we want to make them small but not zero.
  id: totrans-292
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当我们的数据集具有众多特征，并且我们希望将其中许多特征变为零时，建议使用L1正则化。当我们的数据集特征较少，我们希望将它们变得很小但不为零时，建议使用L2正则化。
- en: Exercises
  id: totrans-293
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 4.1
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.1
- en: We have trained four models in the same dataset with different hyperparameters.
    In the following table we have recorded the training and testing errors for each
    of the models.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在同一数据集上训练了四个模型，具有不同的超参数。在下面的表中，我们记录了每个模型的训练和测试误差。
- en: '| Model | Training error | Testing error |'
  id: totrans-296
  prefs: []
  type: TYPE_TB
  zh: '| 模型 | 训练误差 | 测试误差 |'
- en: '| 1 | 0.1 | 1.8 |'
  id: totrans-297
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 0.1 | 1.8 |'
- en: '| 2 | 0.4 | 1.2 |'
  id: totrans-298
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 0.4 | 1.2 |'
- en: '| 3 | 0.6 | 0.8 |'
  id: totrans-299
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 0.6 | 0.8 |'
- en: '| 4 | 1.9 | 2.3 |'
  id: totrans-300
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 1.9 | 2.3 |'
- en: Which model would you select for this dataset?
  id: totrans-301
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于这个数据集，你会选择哪个模型？
- en: Which model looks like it’s underfitting the data?
  id: totrans-302
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型看起来像是在欠拟合数据？
- en: Which model looks like it’s overfitting the data?
  id: totrans-303
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型看起来像是在过度拟合数据？
- en: Exercise 4.2
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 练习4.2
- en: 'We are given the following dataset:'
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 我们得到了以下数据集：
- en: '| *x*   | *y*   |'
  id: totrans-306
  prefs: []
  type: TYPE_TB
  zh: '| *x*   | *y*   |'
- en: '| 1 | 2 |'
  id: totrans-307
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 2 |'
- en: '| 2 | 2.5 |'
  id: totrans-308
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 2.5 |'
- en: '| 3 | 6 |'
  id: totrans-309
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 6 |'
- en: '| 4 | 14.5 |'
  id: totrans-310
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 14.5 |'
- en: '| 5 | 34 |'
  id: totrans-311
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 34 |'
- en: We train the polynomial regression model that predicts the value of *y* as *ŷ*,
    where
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练多项式回归模型，预测*y*的值为*ŷ*，其中
- en: '*ŷ* = 2*x*² – 5*x* + 4.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '*ŷ* = 2*x*² – 5*x* + 4.'
- en: 'If the regularization parameter is λ = 0.1 and the error function we’ve used
    to train this dataset is the mean absolute value (MAE), determine the following:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 如果正则化参数λ = 0.1，并且我们用来训练这个数据集的误差函数是平均绝对值（MAE），确定以下内容：
- en: The lasso regression error of our model (using the L1 norm)
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们模型（使用L1范数）的套索回归误差
- en: The ridge regression error of our model (using the L2 norm)
  id: totrans-316
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 我们模型（使用L2范数）的岭回归误差
