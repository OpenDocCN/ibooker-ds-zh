- en: 'Chapter 3\. Introduction to neural prediction: forward propagation'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第3章. 神经预测简介：前向传播
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章**'
- en: A simple network making a prediction
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个简单的网络进行预测
- en: What is a neural network, and what does it do?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 神经网络是什么，它做什么？
- en: Making a prediction with multiple inputs
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个输入进行预测
- en: Making a prediction with multiple outputs
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个输出进行预测
- en: Making a prediction with multiple inputs and outputs
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用多个输入和输出进行预测
- en: Predicting on predictions
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在预测上进行预测
- en: “I try not to get involved in the business of prediction. It’s a uick way to
    look like an idiot.”
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “我尽量避免参与预测业务。这很容易让人看起来像个白痴。”
- en: ''
  id: totrans-9
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Warren Ellis comic-book writer, novelist, and screenwriter*'
  id: totrans-10
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*沃伦·埃利斯，漫画书作家、小说家和编剧*'
- en: 'Step 1: Predict'
  id: totrans-11
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 第1步：预测
- en: This chapter is about prediction
  id: totrans-12
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 本章是关于预测的
- en: 'In the previous chapter, you learned about the paradigm *predict, compare,
    learn*. In this chapter, we’ll dive deep into the first step: *predict*. You may
    remember that the predict step looks a lot like this:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，你学习了关于预测、比较、学习的范例。在本章中，我们将深入探讨第一步：*预测*。你可能记得预测步骤看起来很像这样：
- en: '![](Images/f0022-01.jpg)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0022-01.jpg)'
- en: 'In this chapter, you’ll learn more about what these three different parts of
    a neural network prediction look like under the hood. Let’s start with the first
    one: the data. In your first neural network, you’re going to predict one datapoint
    at a time, like so:'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将了解神经网络预测的这三个不同部分在内部是如何工作的。让我们从第一个开始：数据。在你的第一个神经网络中，你将一次预测一个数据点，如下所示：
- en: '![](Images/f0022-02.jpg)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0022-02.jpg)'
- en: Later, you’ll find that the number of datapoints you process at a time has a
    significant impact on what a network looks like. You might be wondering, “How
    do I choose how many datapoints to propagate at a time?” The answer is based on
    whether you think the neural network can be accurate with the data you give it.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 以后你会发现，你一次处理的点数数量对网络的外观有重大影响。你可能想知道，“我如何选择一次传播多少点数？”答案是取决于你认为神经网络是否可以用你给它的数据准确预测。
- en: 'For example, if I’m trying to predict whether there’s a cat in a photo, I definitely
    need to show my network all the pixels of an image at once. Why? Well, if I sent
    you only one pixel of an image, could you classify whether the image contained
    a cat? Me neither! (That’s a general rule of thumb, by the way: always present
    enough information to the network, where “enough information” is defined loosely
    as how much a human might need to make the same prediction.)'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我在尝试预测照片中是否有猫，我肯定需要一次性向我的网络展示图像的所有像素。为什么？好吧，如果我只给你一个像素的图像，你能分类图像中是否含有猫吗？我也不能！（顺便说一句，这是一个一般性的规则：总是向网络提供足够的信息，其中“足够的信息”被宽泛地定义为人类可能需要做出相同预测所需的信息量。）
- en: 'Let’s skip over the network for now. As it turns out, you can create a network
    only after you understand the shape of the input and output datasets (for now,
    *shape* means “number of columns” or “number of datapoints you’re processing at
    once”). Let’s stick with a single prediction of the likelihood that the baseball
    team will win:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们先跳过网络。实际上，只有在你理解了输入和输出数据集的形状之后，你才能创建一个网络（现在，“形状”意味着“列数”或“你一次处理的点数数量”）。让我们专注于对棒球队能否获胜的单一预测：
- en: '![](Images/f0023-01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0023-01.jpg)'
- en: 'Now that you know you want to take one input datapoint and output one prediction,
    you can create a neural network. Because you have only one input datapoint and
    one output datapoint, you’re going to build a network with a single knob mapping
    from the input point to the output. (Abstractly, these “knobs” are actually called
    *weights*, and I’ll refer to them as such from here on out.) So, without further
    ado, here’s your first neural network, with a single weight mapping from the input
    “# toes” to the output “win?”:'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经知道你想要输入一个数据点并输出一个预测，你可以创建一个神经网络。因为你只有一个输入数据点和输出数据点，所以你将构建一个将输入点映射到输出点的单一控制网络。（抽象地说，这些“控制”实际上被称为*权重*，从现在起我将这样称呼它们。）所以，不再多言，这是你的第一个神经网络，它有一个单一的权重将输入“脚趾数量”映射到输出“赢？”：
- en: '![](Images/f0023-02.jpg)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0023-02.jpg)'
- en: As you can see, with one weight, this network takes in one datapoint at a time
    (average number of toes per player on the baseball team) and outputs a single
    prediction (whether it thinks the team will win).
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所见，这个网络一次只接受一个数据点（棒球队伍中每名球员的平均脚趾数量）并输出一个预测（它认为队伍是否会赢）。
- en: A simple neural network making a prediction
  id: totrans-24
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 一个简单的神经网络进行预测
- en: Let’s start with the simplest neural network possible
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 让我们从最简单的神经网络开始
- en: '![](Images/f0024-01_alt.jpg)'
  id: totrans-26
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0024-01_alt.jpg)'
- en: '![](Images/f0024-02_alt.jpg)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0024-02_alt.jpg)'
- en: '![](Images/f0024-03_alt.jpg)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0024-03_alt.jpg)'
- en: '![](Images/f0024-04_alt.jpg)'
  id: totrans-29
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0024-04_alt.jpg)'
- en: What is a neural network?
  id: totrans-30
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: '**什么是神经网络？**'
- en: Here is your first neural network
  id: totrans-31
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 这是你的第一个神经网络
- en: 'To start a neural network, open a Jupyter notebook and run this code:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 要启动神经网络，打开Jupyter笔记本并运行以下代码：
- en: '[PRE0]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '***1* The network**'
  id: totrans-34
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 网络**'
- en: 'Now, run the following:'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，运行以下代码：
- en: '[PRE1]'
  id: totrans-36
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '***1* How you use the network to predict something**'
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**1* 如何使用网络进行预测**'
- en: You just made your first neural network and used it to predict! Congratulations!
    The last line prints the prediction (`pred`). It should be 0.85\. So what is a
    neural network? For now, it’s one or more *weights* that you can multiply by the
    *input* data to make a *prediction*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 你刚刚制作了你的第一个神经网络，并使用它进行了预测！恭喜！最后一行打印了预测（`pred`）。它应该是0.85。那么，神经网络是什么？现在，它是一个或多个可以乘以输入数据的*权重*来做出*预测*。
- en: '|  |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**What is input data?**'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是输入数据？**'
- en: It’s a number that you recorded in the real world somewhere. It’s usually something
    that is easily knowable, like today’s temperature, a baseball player’s batting
    average, or yesterday’s stock price.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 它是在现实世界中的某个地方记录的数字。它通常是容易知道的事情，比如今天的温度、棒球运动员的打击率，或者昨天的股价。
- en: '**What is a prediction?**'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: '**什么是预测？**'
- en: A *prediction* is what the neural network tells you, *given the input data*,
    such as “given the temperature, it is **0%** likely that people will wear sweatsuits
    today” or “given a baseball player’s batting average, he is **30%** likely to
    hit a home run” or “given yesterday’s stock price, today’s stock price will be
    **101.52**.”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '**预测**是神经网络告诉你的内容，*给定输入数据*，例如“给定温度，今天人们穿运动服的可能性是**0%**”或“给定棒球运动员的打击率，他击出全垒打的概率是**30%**”或“给定昨天的股价，今天的股价将是**101.52**”。'
- en: '**Is this prediction always right?**'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '**这个预测总是正确吗？**'
- en: No. Sometimes a neural network will make mistakes, but it can learn from them.
    For example, if it predicts too high, it will adjust its weight to predict lower
    next time, and vice versa.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，神经网络会犯错误，但它可以从错误中学习。例如，如果它预测得过高，它将调整其权重以在下一次预测得更低，反之亦然。
- en: '**How does the network learn?**'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: '**网络是如何学习的？**'
- en: Trial and error! First, it tries to make a prediction. Then, it sees whether
    the prediction was too high or too low. Finally, it changes the weight (up or
    down) to predict more accurately the next time it sees the same input.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试和错误！首先，它尝试做出预测。然后，它查看预测是否过高或过低。最后，它改变权重（向上或向下）以在下一次看到相同输入时更准确地预测。
- en: '|  |'
  id: totrans-48
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: What does this neural network do?
  id: totrans-49
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 这个神经网络做什么？
- en: It multiplies the input by a weight. It “scales” the input by a certain amount
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它将输入乘以一个权重。它通过一定的量“缩放”输入
- en: In the previous section, you made your first prediction with a neural network.
    A neural network, in its simplest form, uses the power of *multiplication*. It
    takes an input datapoint (in this case, 8.5) and *multiplies* it by the weight.
    If the weight is 2, then the neural network will *double the input*. If the weight
    is 0.01, then the network will *divide* the input by 100\. As you can see, some
    weight values make the input *bigger*, and other values make it *smaller*.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，你使用神经网络进行了第一次预测。最简单的神经网络形式使用*乘法*的力量。它取一个输入数据点（在这种情况下，8.5）并将其与权重相乘。如果权重是2，那么神经网络将*加倍*输入。如果权重是0.01，那么网络将输入除以100。正如你所看到的，一些权重值使输入*变大*，而其他值使输入*变小*。
- en: '![](Images/f0026-01_alt.jpg)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0026-01_alt.jpg)'
- en: The interface for a neural network is simple. It accepts an `input` variable
    as *information* and a `weight` variable as *knowledge* and outputs a `prediction`.
    Every neural network you’ll ever see works this way. It uses the *knowledge* in
    the weights to interpret the *information* in the input data. Later neural networks
    will accept larger, more complicated `input` and `weight` values, but this same
    underlying premise will always ring true.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的界面很简单。它接受一个`input`变量作为*信息*和一个`weight`变量作为*知识*，并输出一个`prediction`。你将看到的每一个神经网络都是这样工作的。它使用权重中的*知识*来解释输入数据中的*信息*。后来的神经网络将接受更大、更复杂的`input`和`weight`值，但这个相同的根本前提始终是正确的。
- en: '![](Images/f0026-02_alt.jpg)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0026-02_alt.jpg)'
- en: In this case, the information is the average number of toes on a baseball team
    before a game. Notice several things. First, the neural network does *not* have
    access to any information except one instance. If, after this prediction, you
    were to feed in `number_of_toes[1]`, the network wouldn’t remember the prediction
    it made in the last timestep. A neural network knows only what you feed it as
    input. It forgets everything else. Later, you’ll learn how to give a neural network
    a “short-term memory” by feeding in multiple inputs at once.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，信息是比赛前棒球队的平均脚趾数。请注意以下几点。首先，神经网络没有访问任何除了一个实例之外的信息。如果在这次预测之后，你输入`number_of_toes[1]`，网络不会记得它在最后一个时间步长中做出的预测。神经网络只知道你作为输入提供的内容。它会忘记其他所有内容。稍后，你将学习如何通过一次输入多个输入来给神经网络一个“短期记忆”。
- en: '![](Images/f0027-01_alt.jpg)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0027-01_alt.jpg)'
- en: 'Another way to think about a neural network’s weight value is as a measure
    of *sensitivity* between the input of the network and its prediction. If the weight
    is very high, then even the tiniest input can create a really large prediction!
    If the weight is very small, then even large inputs will make small predictions.
    This sensitivity is akin to *volume*. “Turning up the weight” amplifies the prediction
    relative to the input: weight is a volume knob!'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种思考神经网络权重值的方法是将其视为网络输入和预测之间的*敏感性*度量。如果权重非常高，那么即使是最微小的输入也能产生一个非常大的预测！如果权重非常小，那么即使大的输入也会产生小的预测。这种敏感性类似于*音量*。“提高权重”放大了相对于输入的预测：权重就像一个音量旋钮！
- en: '![](Images/f0027-02_alt.jpg)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0027-02_alt.jpg)'
- en: In this case, what the neural network is really doing is applying a *volume
    knob* to the `number_of_toes` variable. In theory, this volume knob can tell you
    the likelihood that the team will win, based on the average number of toes per
    player on the team. This may or may not work. Truthfully, if the team members
    had an average of 0 toes, they would probably play terribly. But baseball is much
    more complex than this. In the next section, you’ll present multiple pieces of
    information at the same time so the neural network can make more-informed decisions.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，神经网络实际上是在对`number_of_toes`变量应用一个*音量旋钮*。理论上，这个音量旋钮可以告诉你，根据球队每个球员的平均脚趾数，球队获胜的可能性。这可能或可能不奏效。说实话，如果球队成员的平均脚趾数为0，他们可能会打得非常糟糕。但棒球比这要复杂得多。在下一节中，你将同时提供多份信息，以便神经网络可以做出更明智的决策。
- en: Note that neural networks don’t predict just positive numbers—they can also
    *predict negative numbers* and even take *negative numbers as input*. Perhaps
    you want to predict the probability that people will wear coats today. If the
    temperature is –10 degrees Celsius, then a negative weight will predict a high
    probability that people will wear their coats.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，神经网络不仅预测正数——它们还可以*预测负数*，甚至可以将*负数作为输入*。也许你想预测今天人们穿大衣的概率。如果温度是-10摄氏度，那么一个负权重将预测人们穿大衣的概率很高。
- en: '![](Images/f0027-03.jpg)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0027-03.jpg)'
- en: Making a prediction with multiple inputs
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个输入进行预测
- en: Neural networks can combine intelligence from multiple datapoints
  id: totrans-63
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络可以结合多个数据点的智能
- en: 'The previous neural network was able to take one datapoint as input and make
    one prediction based on that datapoint. Perhaps you’ve been wondering, “Is the
    average number of toes really a good predictor, all by itself?” If so, you’re
    onto something. What if you could give the network more information (at one time)
    than just the average number of toes per player? In that case, the network should,
    in theory, be able to make more-accurate predictions. Well, as it turns out, a
    network can accept multiple input datapoints at a time. Take a look at the next
    prediction:'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 之前的神经网络能够接受一个数据点作为输入，并基于该数据点做出一个预测。也许你一直在想，“平均脚趾数真的能作为一个好的预测指标吗？”如果是这样，你就有发现了。如果你能一次给网络提供比每个球员的平均脚趾数更多的信息，会怎样？在这种情况下，理论上，网络应该能够做出更准确的预测。好吧，实际上，网络可以一次接受多个输入数据点。看看下一个预测：
- en: '![](Images/f0028-01_alt.jpg)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0028-01_alt.jpg)'
- en: '![](Images/f0028-02_alt.jpg)'
  id: totrans-66
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0028-02_alt.jpg)'
- en: '![](Images/f0029-01_alt.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0029-01_alt.jpg)'
- en: '![](Images/f0029-02_alt.jpg)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0029-02_alt.jpg)'
- en: 'Multiple inputs: What does this neural network do?'
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个输入：这个神经网络做什么？
- en: It multiplies three inputs by three knob weights and sums them. T- This is a
    weighted sum
  id: totrans-70
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它将三个输入乘以三个旋钮权重，并将它们相加。T- 这是一个加权求和
- en: 'At the end of the previous section, you came to realize the limiting factor
    of your simple neural network: it was only a volume knob on one datapoint. In
    the example, that datapoint was a baseball team’s average number of toes per player.
    You learned that in order to make accurate predictions, you need to build neural
    networks that can *combine multiple inputs at the same time*. Fortunately, neural
    networks are perfectly capable of doing so.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节的结尾，你意识到你简单神经网络的限制因素：它只是一个数据点的音量旋钮。在示例中，这个数据点是棒球队伍每位球员的平均脚趾数。你了解到，为了做出准确的预测，你需要构建能够*同时结合多个输入*的神经网络。幸运的是，神经网络完全能够做到这一点。
- en: '![](Images/f0030-01_alt.jpg)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0030-01_alt.jpg)'
- en: This new neural network can accept *multiple inputs at a time* per prediction.
    This allows the network to combine various forms of information to make better-informed
    decisions. But the fundamental mechanism for using weights hasn’t changed. You
    still take each input and run it through its own volume knob. In other words,
    you multiply each input by its own weight.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这个新的神经网络可以一次接受每个预测的*多个输入*。这使得网络能够结合各种形式的信息，做出更明智的决策。但使用权重的根本机制并没有改变。你仍然需要将每个输入通过其自身的音量旋钮。换句话说，你需要将每个输入乘以其自身的权重。
- en: The new property here is that, because you have multiple inputs, you have to
    sum their respective predictions. Thus, you multiply each input by its respective
    weight and then sum all the local predictions together. This is called a *weighted
    sum of the input*, or a *weighted sum* for short. Some also refer to the weighted
    sum as a *dot product*, as you’ll see.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的新特性是，由于你有多个输入，你必须分别求和它们的预测值。因此，你需要将每个输入乘以其相应的权重，然后将所有局部预测值相加。这被称为*输入的加权求和*，或简称为*加权求和*。有些人也将加权求和称为*点积*，你将会看到。
- en: '|  |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**A relevant reminder**'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: '**一个相关的提醒**'
- en: 'The interface for the neural network is simple: it accepts an `input` variable
    as information and a `weights` variable as knowledge, and it outputs a prediction.'
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络的接口很简单：它接受一个`input`变量作为信息和一个`weights`变量作为知识，并输出一个预测。
- en: '|  |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '![](Images/f0031-01_alt.jpg)'
  id: totrans-79
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0031-01_alt.jpg)'
- en: This new need to process multiple inputs at a time justifies the use of a new
    tool. It’s called a *vector*, and if you’ve been following along in your Jupyter
    notebook, you’ve already been using it. A vector is nothing other than a *list
    of numbers*. In the example, `input` is a vector and `weights` is a vector. Can
    you spot any more vectors in the previous code? (There are three more.)
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 同时处理多个输入的新需求证明了使用新工具的必要性。这个工具被称为*向量*，如果你一直在你的Jupyter笔记本中跟随，你已经使用过它了。向量不过是一个*数字列表*。在示例中，`input`是一个向量，`weights`也是一个向量。你能在之前的代码中找到更多的向量吗？（还有三个。）
- en: As it turns out, vectors are incredibly useful whenever you want to perform
    operations involving groups of numbers. In this case, you’re performing a weighted
    sum between two vectors (a dot product). You’re taking two vectors of equal length
    (`input` and `weights`), multiplying each number based on its position (the first
    position in `input` is multiplied by the first position in `weights`, and so on),
    and then summing the resulting output.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 实际上，向量在你想执行涉及数字组的操作时非常有用。在这种情况下，你正在执行两个向量之间的加权求和（点积）。你正在取两个长度相等的向量（`input`和`weights`），根据其位置（`input`中的第一个位置乘以`weights`中的第一个位置，以此类推）乘以每个数字，然后将结果相加。
- en: 'Anytime you perform a mathematical operation between two vectors of equal length
    where you pair up values according to their position in the vector (again: position
    0 with 0, 1 with 1, and so on), it’s called an *elementwise* operation. Thus *elementwise
    addition* sums two vectors, and *elementwise multiplication* multiplies two vectors.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 任何时间当你对两个长度相等的向量进行数学运算，并且根据它们在向量中的位置配对值（再次强调：位置0与0配对，1与1配对，以此类推），这被称为*逐元素*运算。因此，*逐元素加法*是将两个向量相加，而*逐元素乘法*是两个向量的乘法。
- en: '|  |'
  id: totrans-83
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Challenge: Vector math**'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: '**挑战：向量数学**'
- en: 'Being able to manipulate vectors is a cornerstone technique for deep learning.
    See if you can write functions that perform the following operations:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 能够操作向量是深度学习的一个基石技术。看看你是否能编写执行以下操作的函数：
- en: '`def elementwise_multiplication(vec_a, vec_b)`'
  id: totrans-86
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def elementwise_multiplication(vec_a, vec_b)`'
- en: '`def elementwise_addition(vec_a, vec_b)`'
  id: totrans-87
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def elementwise_addition(vec_a, vec_b)`'
- en: '`def vector_sum(vec_a)`'
  id: totrans-88
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def vector_sum(vec_a)`'
- en: '`def vector_average(vec_a)`'
  id: totrans-89
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`def vector_average(vec_a)`'
- en: Then, see if you can use two of these methods to perform a dot product!
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，看看你是否可以使用这两种方法之一来执行点积！
- en: '|  |'
  id: totrans-91
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '![](Images/f0032-01_alt.jpg)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0032-01_alt.jpg)'
- en: 'The intuition behind how and why a dot product (weighted sum) works is easily
    one of the most important parts of truly understanding how neural networks make
    predictions. Loosely stated, a dot product gives you a *notion of similarity*
    between two vectors. Consider these examples:'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 点积（加权求和）如何以及为什么工作背后的直觉是真正理解神经网络如何进行预测的最重要部分之一。简单地说，点积给出了两个向量之间的“相似度”概念。考虑以下例子：
- en: '|'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE2]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '|'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: '[PRE3]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '|'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '|'
- en: The highest weighted sum (`w_sum(c,c)`) is between vectors that are exactly
    identical. In contrast, because `a` and `b` have no overlapping weight, their
    dot product is zero. Perhaps the most interesting weighted sum is between `c`
    and `e`, because `e` has a negative weight. This negative weight canceled out
    the positive similarity between them. But a dot product between `e` and itself
    would yield the number 2, despite the negative weight (double negative turns positive).
    Let’s become familiar with the various properties of the dot product operation.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 最大的加权求和(`w_sum(c,c)`)是在完全相同的向量之间。相比之下，因为`a`和`b`没有重叠的权重，它们的点积为零。也许最有趣的加权求和是在`c`和`e`之间，因为`e`有一个负权重。这个负权重抵消了它们之间的正相似性。但是，`e`与自身的点积会产生数字2，尽管有负权重（双重否定转为正数）。让我们熟悉点积操作的各项属性。
- en: 'Sometimes you can equate the properties of the dot product to a logical `AND`.
    Consider `a` and `b`:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 有时可以将点积的性质等同于逻辑“与”。考虑`a`和`b`：
- en: '[PRE4]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: If you ask whether both `a[0] AND b[0]` have value, the answer is no. If you
    ask whether both `a[1] AND b[1]` have value, the answer is again no. Because this
    is *always* true for all four values, the final score equals 0\. Each value fails
    the logical `AND`.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你问`a[0] AND b[0]`是否都有值，答案是“否”。如果你问`a[1] AND b[1]`是否都有值，答案仍然是“否”。因为这对于所有四个值都是“总是”成立的，所以最终分数等于0。每个值都未通过逻辑“与”。
- en: '[PRE5]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: '`b` and `c`, however, have one column that shares value. It passes the logical
    `AND` because `b[2]` *and* `c[2]` have weight. This column (and only this column)
    causes the score to rise to 1.'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，`b`和`c`有一个共享值的列。它通过了逻辑“与”，因为`b[2]`和`c[2]`都有权重。这一列（仅这一列）导致分数上升到1。
- en: '[PRE6]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Fortunately, neural networks are also able to model partial `AND`ing. In this
    case, `c` and `d` share the same column as `b` and `c`, but because `d` has only
    0.5 weight there, the final score is only 0.5\. We exploit this property when
    modeling probabilities in neural networks.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，神经网络也能够模拟部分“与”运算。在这种情况下，`c`和`d`与`b`和`c`共享相同的列，但由于`d`在那里只有0.5的权重，最终分数只有0.5。我们在神经网络建模概率时利用了这一属性。
- en: '[PRE7]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: In this analogy, negative weights tend to imply a logical `NOT` operator, given
    that any positive weight paired with a negative weight will cause the score to
    go down. Furthermore, if both vectors have negative weights (such as `w_sum(e,e)`),
    then the neural network will perform a *double negative* and add weight instead.
    Additionally, some might say it’s an `OR` after the `AND`, because if any of the
    rows show weight, the score is affected. Thus, for `w_sum(a,b)`, if (`a[0] AND
    b[0]`) `OR` (`a[1] AND b[1]`), and so on, then `w_sum(a,b)` returns a positive
    score. Furthermore, if one value is negative, then that column gets a `NOT`.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个类比中，负权重往往意味着逻辑“非”运算符，因为任何与负权重配对的正权重都会导致分数下降。此外，如果两个向量都具有负权重（例如`w_sum(e,e)`），那么神经网络将执行“双重否定”并添加权重。此外，有些人可能会说这是“与”之后的“或”，因为如果任何一行显示权重，分数就会受到影响。因此，对于`w_sum(a,b)`，如果(`a[0]
    AND b[0]`)“或”(`a[1] AND b[1]`)，依此类推，那么`w_sum(a,b)`返回一个正分数。此外，如果其中一个值是负数，那么该列就得到一个“非”。
- en: 'Amusingly, this gives us a kind of crude language for reading weights. Let’s
    read a few examples, shall we? These assume you’re performing `w_sum(input,weights)`
    and the “then” to these `if` statements is an abstract “then give high score”:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，这为我们提供了一种阅读权重的粗略语言。让我们读几个例子，好吗？这些假设你在执行`w_sum(input,weights)`，并且这些`if`语句的“然后”是一个抽象的“然后给出高分”：
- en: '[PRE8]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Notice in the last row that `weight[0] = 0.5` means the corresponding `input[0]`
    would have to be larger to compensate for the smaller weighting. And as I mentioned,
    this is a *very* crude approximate language. But I find it immensely useful when
    trying to picture in my head what’s going on under the hood. This will help you
    significantly in the future, especially when putting networks together in increasingly
    complex ways.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 注意在最后一行中，`weight[0] = 0.5` 表示相应的 `input[0]` 必须更大才能补偿较小的权重。正如我提到的，这是一种非常粗略的近似语言。但当我试图在脑海中想象底层的运作时，我发现它非常有用。这将在未来对你有很大帮助，尤其是在以越来越复杂的方式组合网络时。
- en: Given these intuitions, what does this mean when a neural network makes a prediction?
    Roughly speaking, it means the network gives a high score of the inputs based
    on *how similar they are to the weights*. Notice in the following example that
    `nfans` is completely ignored in the prediction because the weight associated
    with it is 0\. The most sensitive predictor is `wlrec` because its weight is 0.2\.
    But the dominant force in the high score is the number of toes (`ntoes`), not
    because the weight is the highest, but because the input combined with the weight
    is by far the highest.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 基于这些直觉，当神经网络进行预测时，这意味着什么？粗略地说，这意味着网络根据输入与权重的相似度给出高分数。注意在下面的示例中，`nfans` 在预测中被完全忽略，因为与之相关的权重是0。最敏感的预测器是
    `wlrec`，因为其权重是0.2。但高分的主导力量是脚趾的数量（`ntoes`），不是因为权重最高，而是因为输入与权重的组合远远是最高的。
- en: '![](Images/f0034-01_alt.jpg)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0034-01_alt.jpg)'
- en: 'Here are a few more points to note for further reference. You can’t shuffle
    weights: they have specific positions they need to be in. Furthermore, both the
    value of the weight *and* the value of the input determine the overall impact
    on the final score. Finally, a negative weight will cause some inputs to reduce
    the final prediction (and vice versa).'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些额外的要点需要注意，以供进一步参考。你不能随机排列权重：它们需要处于特定的位置。此外，权重的值和输入的值都会决定对最终得分的整体影响。最后，负权重将导致某些输入减少最终预测（反之亦然）。
- en: 'Multiple inputs: Complete runnable code'
  id: totrans-115
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个输入：完整的可运行代码
- en: The code snippets from this example come together in the following code, which
    creates and executes a neural network. For clarity, I’ve written everything out
    using basic properties of Python (lists and numbers). But a better way exists
    that we’ll begin using in the future.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这个例子中的代码片段组合在以下代码中，该代码创建并执行了一个神经网络。为了清晰起见，我使用Python的基本属性（列表和数字）将所有内容都写出来了。但有一种更好的方法，我们将在未来开始使用。
- en: '**Previous code**'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: '**之前的代码**'
- en: '[PRE9]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '***1* Input corresponds to every entry for the first game of the season.**'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入对应于赛季第一场比赛的每个条目。**'
- en: There’s a Python library called NumPy, which stands for “numerical Python.”
    It has very efficient code for creating vectors and performing common functions
    (such as dot products). Without further ado, here’s the same code in NumPy.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个名为NumPy的Python库，代表“数值Python”。它具有创建向量和执行常见函数（如点积）的高效代码。无需多言，以下是相同的代码，使用NumPy实现。
- en: '**NumPy code**'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy代码**'
- en: '[PRE10]'
  id: totrans-122
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: '***1* Input corresponds to every entry for the first game of the season.**'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 输入对应于赛季第一场比赛的每个条目。**'
- en: Both networks should print out `0.98`. Notice that in the NumPy code, you don’t
    have to create a `w_sum` function. Instead, NumPy has a `dot` function (short
    for “dot product”) you can call. Many functions you’ll use in the future have
    NumPy parallels.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 两个网络都应该输出 `0.98`。注意在NumPy代码中，你不需要创建一个 `w_sum` 函数。相反，NumPy有一个可以调用的 `dot` 函数（简称“点积”）。你将来会使用的许多函数都有NumPy的对应版本。
- en: Making a prediction with multiple outputs
  id: totrans-125
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个输出进行预测
- en: Neural networks can also make multiple predictions using only a single input
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络也可以仅使用单个输入进行多次预测
- en: Perhaps a simpler augmentation than multiple inputs is multiple outputs. Prediction
    occurs the same as if there were three disconnected single-weight neural networks.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 可能比多个输入更简单的一种增强是多输出。预测发生的方式与有三个独立单权重神经网络的预测相同。
- en: '![](Images/f0036-01_alt.jpg)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0036-01_alt.jpg)'
- en: The most important comment in this setting is to notice that the three predictions
    are completely separate. Unlike neural networks with multiple inputs and a single
    output, where the prediction is undeniably connected, this network truly behaves
    as three independent components, each receiving the same input data. This makes
    the network simple to implement.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种设置中，最重要的评论是要注意三个预测是完全独立的。与具有多个输入和单个输出的神经网络不同，那里的预测无疑是相互关联的，这个网络真正地表现为三个独立的组件，每个组件接收相同的数据输入。这使得网络易于实现。
- en: '![](Images/f0036-02_alt.jpg)'
  id: totrans-130
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0036-02_alt.jpg)'
- en: '![](Images/f0037-01_alt.jpg)'
  id: totrans-131
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0037-01_alt.jpg)'
- en: '![](Images/f0037-02_alt.jpg)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0037-02_alt.jpg)'
- en: Predicting with multiple inputs and outputs
  id: totrans-133
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 使用多个输入和输出进行预测
- en: Neural networks can predict multiple outputs given multiple inputs
  id: totrans-134
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络可以根据多个输入预测多个输出
- en: Finally, the way you build a network with multiple inputs or outputs can be
    combined to build a network that has both multiple inputs *and* multiple outputs.
    As before, a weight connects each input node to each output node, and prediction
    occurs in the usual way.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，构建具有多个输入或输出的网络的方式可以组合起来构建一个具有多个输入*和*多个输出的网络。就像之前一样，一个权重将每个输入节点连接到每个输出节点，并且预测以通常的方式进行。
- en: '![](Images/f0038-01_alt.jpg)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0038-01_alt.jpg)'
- en: '![](Images/f0038-02_alt.jpg)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0038-02_alt.jpg)'
- en: '![](Images/f0039-01_alt.jpg)'
  id: totrans-138
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0039-01_alt.jpg)'
- en: '![](Images/f0039-02_alt.jpg)'
  id: totrans-139
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0039-02_alt.jpg)'
- en: 'Multiple inputs and outputs: How does it work?'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 多个输入和输出：它是如何工作的？
- en: It performs three independent weighted sums of the input to make - three predictions
  id: totrans-141
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 它对输入执行三个独立的加权求和以做出 - 三个预测
- en: 'You can take two perspectives on this architecture: think of it as either three
    weights coming out of each input node, or three weights going into each output
    node. For now, I find the latter to be much more beneficial. Think about this
    neural network as three independent dot products: three independent weighted sums
    of the input. Each output node takes its own weighted sum of the input and makes
    a prediction.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以从两个角度看待这个架构：将其视为每个输入节点出来的三个权重，或者每个输出节点进入的三个权重。目前，我发现后者更有益。将这个神经网络视为三个独立的点积：三个独立的输入加权求和。每个输出节点对其输入进行自己的加权求和并做出预测。
- en: '![](Images/f0040-01_alt.jpg)'
  id: totrans-143
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0040-01_alt.jpg)'
- en: '![](Images/f0040-02_alt.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0040-02_alt.jpg)'
- en: '![](Images/f0041-01_alt.jpg)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0041-01_alt.jpg)'
- en: As mentioned earlier, we’re choosing to think about this network as a series
    of weighted sums. Thus, the previous code creates a new function called `vect_mat_mul`.
    This function iterates through each row of weights (each row is a vector) and
    makes a prediction using the `w_sum` function. It’s literally performing three
    consecutive weighted sums and then storing their predictions in a vector called
    `output`. A lot more weights are flying around in this one, but it isn’t that
    much more advanced than other networks you’ve seen.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，我们选择将这个网络视为一系列加权求和。因此，之前的代码创建了一个名为`vect_mat_mul`的新函数。这个函数遍历每个权重的行（每一行都是一个向量）并使用`w_sum`函数进行预测。它实际上执行了三个连续的加权求和，并将它们的预测存储在一个名为`output`的向量中。这里有很多权重在飞舞，但这并不比其他你见过的网络更高级。
- en: 'I want to use this *list of vectors* and *series of weighted sums* logic to
    introduce two new concepts. See the `weights` variable in step 1? It’s a list
    of vectors. A list of vectors is called a *matrix*. It’s as simple as it sounds.
    Commonly used functions use matrices. One of these is called *vector-matrix multiplication*.
    The series of weighted sums is exactly that: you take a vector and perform a dot
    product with every row in a matrix.^([[*](#ch03fn01)]) As you’ll find out in the
    next section, NumPy has special functions to help.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '我想使用这个*向量列表*和*一系列加权求和*的逻辑来介绍两个新概念。看看步骤1中的`weights`变量？它是一个向量列表。向量列表被称为*矩阵*。正如其名所示。常用的函数使用矩阵。其中之一被称为*向量-矩阵乘法*。一系列加权求和正是这样：你取一个向量并与矩阵中的每一行进行点积。^([[*](#ch03fn01)])
    你将在下一节中发现，NumPy有特殊的函数来帮助。 '
- en: ^*
  id: totrans-148
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: ^*
- en: ''
  id: totrans-149
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: If you’re experienced with linear algebra, the more formal definition stores/processes
    weights as column vectors instead of row vectors. This will be rectified shortly.
  id: totrans-150
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: 如果你熟悉线性代数，更正式的定义是将权重存储/处理为列向量而不是行向量。这将在稍后纠正。
- en: Predicting on predictions
  id: totrans-151
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 在预测上进行预测
- en: Neural networks can be stacked!
  id: totrans-152
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 神经网络可以堆叠！
- en: As the following figures make clear, you can also take the output of one network
    and feed it as input to another network. This results in two consecutive vector-matrix
    multiplications. It may not yet be clear why you’d predict this way; but some
    datasets (such as image classification) contain patterns that are too complex
    for a single-weight matrix. Later, we’ll discuss the nature of these patterns.
    For now, it’s sufficient to know this is possible.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 如以下图所示，你也可以将一个网络的输出作为输入提供给另一个网络。这会导致连续两次向量-矩阵乘法。你可能还不清楚为什么会有这样的预测；但有些数据集（如图像分类）包含过于复杂的模式，无法用一个单权重矩阵来表示。稍后，我们将讨论这些模式的特点。现在，只需知道这是可能的就足够了。
- en: '![](Images/f0042-01_alt.jpg)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0042-01_alt.jpg)'
- en: '![](Images/f0042-02_alt.jpg)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0042-02_alt.jpg)'
- en: '![](Images/f0043-01_alt.jpg)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0043-01_alt.jpg)'
- en: The following listing shows how you can do the same operations coded in the
    previous section using a convenient Python library called NumPy. Using libraries
    like NumPy makes your code faster and easier to read and write.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表展示了如何使用一个方便的 Python 库 NumPy 来执行上一节中用代码编写的相同操作。使用像 NumPy 这样的库可以使你的代码更快、更容易阅读和编写。
- en: '**NumPy version**'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: '**NumPy 版本**'
- en: '[PRE11]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: A quick primer on NumPy
  id: totrans-160
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: NumPy 快速入门
- en: NumPy does a few things for you. Let’s reveal the magic
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: NumPy 为你做了几件事情。让我们揭示这个魔法的秘密
- en: 'So far in this chapter, we’ve discussed two new types of mathematical tools:
    vectors and matrices. You’ve also learned about different operations that occur
    on vectors and matrices, including dot products, elementwise multiplication and
    addition, and vector-matrix multiplication. For these operations, you’ve written
    Python functions that can operate on simple Python `list` objects.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，在本章中，我们讨论了两种新的数学工具：向量和矩阵。你还了解到了在向量和矩阵上发生的不同操作，包括点积、逐元素乘法和加法以及向量-矩阵乘法。对于这些操作，你已经编写了可以在简单的
    Python `list` 对象上操作的 Python 函数。
- en: 'In the short term, you’ll keep writing and using these functions to be sure
    you fully understand what’s going on inside them. But now that I’ve mentioned
    NumPy and several of the big operations, I’d like to give you a quick rundown
    of basic NumPy use so you’ll be ready for the transition to NumPy-only chapters.
    Let’s start with the basics again: vectors and matrices.'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 在短期内，你需要继续编写和使用这些函数，以确保你完全理解它们内部的运作。但现在我已经提到了 NumPy 和几个主要操作，我想快速概述一下基本的 NumPy
    使用方法，以便你为过渡到仅使用 NumPy 的章节做好准备。让我们再次从基础知识开始：向量和矩阵。
- en: 'You can create vectors and matrices in multiple ways in NumPy. Most of the
    common techniques for neural networks are listed in the previous code. Note that
    the processes for creating a vector and a matrix are identical. If you create
    a matrix with only one row, you’re creating a vector. And, as in mathematics in
    general, you create a matrix by listing `(rows,columns)`. I say that only so you
    can remember the order: rows come first, columns come second. Let’s see some operations
    you can perform on these vectors and matrices:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在 NumPy 中，你可以以多种方式创建向量和矩阵。大多数神经网络中常用的技术在前面的代码中列出。请注意，创建向量和矩阵的过程是相同的。如果你只创建了一行的矩阵，你就是在创建一个向量。并且，正如数学中通常的做法一样，你通过列出
    `(rows,columns)` 来创建矩阵。我之所以这么说，只是为了让你记住顺序：行在前，列在后。让我们看看你可以对这些向量和矩阵执行的一些操作：
- en: '[PRE12]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: '***1* Multiplies every number in vector a by 0.1**'
  id: totrans-166
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 将向量 a 中的每个数乘以 0.1**'
- en: '***2* Multiplies every number in matrix c by 0.2**'
  id: totrans-167
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 将矩阵 c 中的每个数乘以 0.2**'
- en: '***3* Multiplies elementwise between a and b (columns paired)**'
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 在 a 和 b 之间逐元素相乘（列配对）**'
- en: '***4* Multiplies elementwise, then multiplies by 0.2**'
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 逐元素相乘，然后乘以 0.2**'
- en: '***5* Performs elementwise multiplication on every row of matrix c, because
    c has the same number of columns as a**'
  id: totrans-170
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 对矩阵 c 的每一行执行逐元素乘法，因为 c 的列数与 a 相同**'
- en: '***6* Because a and e don’t have the same number of columns, this throws “Value
    Error: operands could not be broadcast together with...”**'
  id: totrans-171
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 因为 a 和 e 的列数不同，这会导致“值错误：操作数无法一起广播...”**'
- en: '[PRE13]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '***1* A vector**'
  id: totrans-173
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 向量**'
- en: '***2* Another vector**'
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 另一个向量**'
- en: '***3* A matrix**'
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 矩阵**'
- en: '***4* 2 × 4 matrix of zeros**'
  id: totrans-176
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 2×4 的零矩阵**'
- en: '***5* Random 2 × 5 matrix of numbers between 0 and 1**'
  id: totrans-177
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 随机 2×5 矩阵，数值介于 0 和 1 之间**'
- en: '**Output**'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**'
- en: '[PRE14]'
  id: totrans-179
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: Go ahead and run all of the previous code. The first bit of “at first confusing
    but eventually heavenly” magic should be visible. When you multiply two variables
    with the `*` function, NumPy automatically detects what kinds of variables you’re
    working with and tries to figure out the operation you’re talking about. This
    can be mega-convenient but sometimes makes NumPy code a bit hard to read. Make
    sure you keep track of each variable type as you go along.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 现在运行所有之前的代码。第一部分“起初令人困惑但最终美妙”的魔法应该已经可见了。当你用`*`函数乘以两个变量时，NumPy会自动检测你正在处理什么类型的变量，并试图弄清楚你所说的操作。这可能会非常方便，但有时会使NumPy代码变得难以阅读。确保你在进行过程中跟踪每个变量的类型。
- en: The general rule of thumb for anything elementwise (`+`, `–`, `*`, `/`) is that
    either the two variables must have the *same* number of columns, or one of the
    variables must have only one column. For example, `print(a * 0.1)` multiplies
    a vector by a single number (a scalar). NumPy says, “Oh, I bet I’m supposed to
    do vector-scalar multiplication here,” and then multiples the scalar (0.1) by
    every value in the vector. This looks exactly the same as `print(c * 0.2)`, except
    NumPy knows that `c` is a matrix. Thus, it performs scalar-matrix multiplication,
    multiplying every element in `c` by 0.2\. Because the scalar has only one column,
    you can multiply it by anything (or divide, add, or subtract).
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 对于任何逐元素操作（`+`、`-`、`*`、`/`）的一般规则是，两个变量必须有相同的列数，或者其中一个变量必须只有一个列。例如，`print(a *
    0.1)`将一个向量乘以一个单个数字（一个标量）。NumPy会说，“哦，我敢打赌我应该在执行向量-标量乘法，”然后它将标量（0.1）乘以向量中的每个值。这看起来与`print(c
    * 0.2)`完全一样，但NumPy知道`c`是一个矩阵。因此，它执行标量-矩阵乘法，将`c`中的每个元素乘以0.2。因为标量只有一个列，你可以将它乘以任何东西（或除、加或减）。
- en: 'Next up: `print(a * b)`. NumPy first identifies that they’re both vectors.
    Because neither vector has only one column, NumPy checks whether they have an
    identical number of columns. They do, so NumPy knows to multiply each element
    by each element, based on their positions in the vectors. The same is true with
    addition, subtraction, and division.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来是`print(a * b)`。NumPy首先确定它们都是向量。因为这两个向量都没有只有一个列，NumPy会检查它们是否有相同数量的列。它们确实有，所以NumPy知道要根据它们在向量中的位置逐个元素相乘。加法、减法和除法也是同样的道理。
- en: '`print(a * c)` is perhaps the most elusive. `a` is a vector with four columns,
    and `c` is a (2 × 4) matrix. Neither has only one column, so NumPy checks whether
    they have the same number of columns. They do, so NumPy multiplies the vector
    `a` by each row of `c` (as if it were doing elementwise vector multiplication
    on each row).'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: '`print(a * c)`可能是最难以捉摸的。`a`是一个有四个列的向量，而`c`是一个(2 × 4)矩阵。它们都没有只有一个列，所以NumPy会检查它们是否有相同数量的列。它们确实有，所以NumPy会将向量`a`与`c`的每一行相乘（就像它在每一行上执行逐元素向量乘法一样）。'
- en: 'Again, the most confusing part is that all of these operations look the same
    if you don’t know which variables are scalars, vectors, or matrices. When you
    “read NumPy,” you’re really doing two things: reading the operations and keeping
    track of the *shape* (number of rows and columns) of each operation. It will take
    some practice, but eventually it becomes second nature. Let’s look at a few examples
    of matrix multiplication in NumPy, noting the input and output shapes of each
    matrix.'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，最令人困惑的部分是，如果你不知道哪些是标量、向量或矩阵，所有这些操作看起来都是一样的。当你“阅读NumPy”时，你实际上在做两件事：阅读操作并跟踪每个操作的*形状*（行数和列数）。这需要一些练习，但最终它会变得像本能一样。让我们看看NumPy中矩阵乘法的几个例子，并注意每个矩阵的输入和输出形状。
- en: '[PRE15]'
  id: totrans-185
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: '***1* Vector of length 4**'
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 长度为4的向量**'
- en: '***2* Matrix with 4 rows and 3 columns**'
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 4行3列的矩阵**'
- en: '**Output**'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**'
- en: '[PRE16]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'There’s one golden rule when using the `dot` function: if you put the `(rows,cols)`
    description of the two variables you’re “dotting” next to each other, neighboring
    numbers should always be the same. In this case, you’re dot-producting (1,4) with
    (4,3). It works fine and outputs (1,3). In terms of variable shape, you can think
    of it as follows, regardless of whether you’re dotting vectors or matrices: their
    *shape* (number of rows and columns) must line up. The columns of the left matrix
    must equal the rows on the right, such that `(a,b).dot(b,c) = (a,c)`.'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`dot`函数时有一个黄金法则：如果你将两个你要“点积”的变量的`(rows,cols)`描述放在一起，相邻的数字应该总是相同的。在这种情况下，你正在将(1,4)与(4,3)进行点积。它运行正常并输出(1,3)。从变量形状的角度来看，你可以这样想，无论你是点积向量还是矩阵：它们的*形状*（行数和列数）必须对齐。左矩阵的列必须等于右矩阵的行，这样`(a,b).dot(b,c)
    = (a,c)`。
- en: '[PRE17]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: '***1* Matrix with 2 rows and 4 columns**'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***1* 2行4列的矩阵**'
- en: '***2* Matrix with 4 rows and 3 columns**'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***2* 4行3列的矩阵**'
- en: '***3* Outputs (2,3)**'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***3* 输出 (2,3)**'
- en: '***4* Matrix with 2 rows and 1 column**'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***4* 2行1列的矩阵**'
- en: '***5* Matrix with 1 row and 3 columns**'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***5* 1行3列的矩阵**'
- en: '***6* Outputs (2,3)**'
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***6* 输出 (2,3)**'
- en: '***7* Throws an error; .T flips the rows and columns of a matrix.**'
  id: totrans-198
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***7* 抛出错误；.T翻转矩阵的行和列。**'
- en: '***8* Matrix with 4 rows and 5 columns**'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***8* 4行5列的矩阵**'
- en: '***9* Matrix with 6 rows and 5 columns**'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***9* 6行5列的矩阵**'
- en: '***10* Outputs (4,6)**'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***10* 输出 (4,6)**'
- en: '***11* Matrix with 5 rows and 4 columns**'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***11* 5行4列的矩阵**'
- en: '***12* Matrix with 5 rows and 6 columns**'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***12* 5行6列的矩阵**'
- en: '***13* Throws an error**'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '***13* 抛出错误**'
- en: Summary
  id: totrans-205
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: To predict, neural networks perform repeated weighted sums of the input
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 为了进行预测，神经网络执行输入的重复加权求和
- en: You’ve seen an increasingly complex variety of neural networks in this chapter.
    I hope it’s clear that a relatively small number of simple rules are used repeatedly
    to create larger, more advanced neural networks. The network’s intelligence depends
    on the weight values you give it.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 你在本章中看到了越来越多种类的神经网络。我希望你清楚，相对较少的简单规则被反复使用，以创建更大、更先进的神经网络。网络智能取决于你赋予它的权重值。
- en: Everything we’ve done in this chapter is a form of what’s called *forward propagation*,
    wherein a neural network takes input data and makes a prediction. It’s called
    this because you’re *propagating* activations *forward* through the network. In
    these examples, *activations* are all the numbers that are *not* weights and are
    unique for every prediction.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 本章我们所做的一切都是所谓的前向传播的形式，其中神经网络接收输入数据并做出预测。之所以称为前向传播，是因为你正在将激活信号*正向*传播通过网络。在这些例子中，*激活*是指所有不是权重且对每个预测都是唯一的数字。
- en: In the next chapter, you’ll learn how to set weights so your neural networks
    make accurate predictions. Just as prediction is based on several simple techniques
    that are repeated/stacked on top of each other, *weight learning* is also a series
    of simple techniques that are combined many times across an architecture. See
    you there!
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章，你将学习如何设置权重，以便你的神经网络做出准确的预测。正如预测是基于几个简单的技术，这些技术被重复/堆叠在一起一样，*权重学习*也是一个由许多简单技术组成的系列，这些技术在一个架构中被多次组合。那里见！
