- en: Chapter 65\. Use Model-Agnostic Explanations for Finding Bias in Black-Box Models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第65章。使用模型无关解释找出黑盒模型中的偏见
- en: Yiannis Kanellopoulos and
  id: totrans-1
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 伊亚尼斯·卡内洛普洛斯和
- en: Andreas Messalas
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 安德烈亚斯·梅萨拉斯
- en: '![](Images/Yiannis_Kanellopoulos.png)![](Images/Andreas_Messalas.png)'
  id: totrans-3
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/Yiannis_Kanellopoulos.png)![](Images/Andreas_Messalas.png)'
- en: Founder, Code4Thought
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 创始人，Code4Thought
- en: Data Scientist, Code4Thought
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家，Code4Thought
- en: 'The need to shed light on the opacity of “black-box” models is evident: Articles
    15 and 22 of the EU’s General Data Protection Regulation (2018), the OECD Principles
    on Artificial Intelligence (2019), and the [US Senate’s proposed Algorithmic Accountability
    Act](https://oreil.ly/2Xgzm) are some examples indicating that machine learning
    interpretability, along with machine learning accountability and fairness, has
    already (or should) become an integral characteristic for any application that
    makes automated decisions.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 揭示“黑盒”模型的不透明性的必要性显而易见：欧盟通用数据保护条例（2018年）的第15和第22条、OECD人工智能原则（2019年）以及[美国参议院提出的算法问责法案](https://oreil.ly/2Xgzm)等都表明，机器学习的可解释性，以及机器学习的问责性和公平性，已经（或应该）成为任何自动决策应用的一个重要特征。
- en: Since many organizations will be obliged to provide explanations about the decisions
    of their automated models, there will be a huge need for third-party organizations
    to assess interpretability, as this provides an additional level of integrity
    and objectivity to the whole audit process. Moreover, some organizations (especially
    start-ups) won’t have the resources to deal with interpretability issues, rendering
    third-party auditors necessary.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 由于许多组织将被迫就其自动化模型的决策提供解释，第三方机构将有巨大需求来评估可解释性，因为这为整个审计过程提供了额外的完整性和客观性。此外，一些组织（特别是初创企业）将无法应对可解释性问题，从而需要第三方审计机构。
- en: In this manner, however, intellectual property issues arise, since organizations
    will not want to disclose any information about the details of their models. Therefore,
    among the wide range of interpretability methods, model-agnostic approaches (i.e.,
    methods that are oblivious to a model’s details) are deemed to be appropriate
    for this purpose.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，这种方法可能会引发知识产权问题，因为组织不愿透露有关其模型细节的任何信息。因此，在广泛的可解释性方法中，模型无关方法（即不涉及模型细节的方法）被认为是适合这一目的的。
- en: Besides explaining the predictions of a black-box model, interpretability can
    also provide us with insight into erroneous behavior of our models, which may
    be caused by undesired patterns in our data. In this article, we will examine
    an example in which interpretability helps us identify gender bias in our data,
    using a model-agnostic method that utilizes surrogate models and Shapley values.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 除了解释黑盒模型的预测外，可解释性还能帮助我们洞察模型可能因数据中不良模式而导致的错误行为。本文将通过一个例子探讨可解释性如何帮助我们识别数据中的性别偏见，使用一种利用替代模型和Shapley值的模型无关方法。
- en: We use the Default of Credit Card Clients Dataset, which contains information
    (demographic factors, credit data, history of payment, and bill statements) about
    30,000 credit card clients, with the target label being if they defaulted on their
    next payment (i.e., in October 2005). The following figure breaks out the defaulting
    and nondefaulting bank customers by gender; the left and middle bars in each group
    represent the original distributions of female and male customers, while the right
    bar in each group depicts the newly constructed biased distribution of male customers.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 我们使用信用卡客户违约数据集，其中包含关于3万名信用卡客户的信息（人口统计因素、信用数据、支付历史和账单报表），目标标签是他们是否在下一个付款期限（即2005年10月）违约。下图展示了按性别分开的违约和非违约银行客户；每组中左侧和中间的柱状图代表女性和男性客户的原始分布，而每组中右侧的柱状图则显示了重新构建的男性客户偏倚分布。
- en: '![Image](Images/aeds_65in01.png)'
  id: totrans-11
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/aeds_65in01.png)'
- en: We distort the dataset by randomly selecting 957 male defaulters (i.e., one-third
    of the overall number of male defaulters), and we alter their label. This creates
    a new biased dataset with 34% male and 66% female defaulters and 41% male and
    59% female nondefaulters.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过随机选择957名男性违约者（即总男性违约者的三分之一），并改变其标签来扭曲数据集。这样一来，我们就得到了一个新的偏倚数据集，其中34%为男性违约者，66%为女性违约者，以及41%为男性非违约者，59%为女性非违约者。
- en: We then remove the gender feature from the dataset and take the predictions
    of a black-box model trained on this biased dataset (whose structure we are indifferent
    about). We then train a [surrogate](https://oreil.ly/hpq8U) XGBoost model, from
    which we extract the Shapley values that help us explain the predictions of the
    original model. More precisely, we use the Shapley values to pinpoint the most
    important features and then use them in the explanations through simple sentences
    in natural language.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们从数据集中删除性别特征，并采用在此偏倚数据集上训练的黑盒模型的预测结果（其结构我们不关心）。然后，我们训练一个[代理](https://oreil.ly/hpq8U)
    XGBoost 模型，从中提取Shapley值，帮助我们解释原始模型的预测。更准确地说，我们使用Shapley值来准确定位最重要的特征，然后通过简单的自然语言句子在解释中使用它们。
- en: We examine the explanations for a false negative prediction for a male customer
    (i.e., falsely predicted as a nondefaulter) and a false positive prediction for
    a female customer (i.e., falsely predicted as a defaulter). They are both unmarried
    university graduates with similar credit limits. However, the male customer delayed
    the last four payments, while the female delayed only the most recent one—see
    the following table.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 我们检查了对男性客户的误判为非违约者的错误预测解释，以及对女性客户的误判为违约者的错误预测解释。他们都是未婚大学毕业生，信用额度相似。然而，男性客户延迟了最后四次付款，而女性只延迟了最近一次—请参见以下表格。
- en: '|  |  |  |  | **Payment Status (months delayed)** |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  |  |  |  | **支付状态（延迟月数）** |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-16
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| **ID** | **Credit Limit** | **Education** | **Marital Status** | **Sept.**
    | **Aug.** | **July** | **June** | **May** | **April** |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| **ID** | **信用额度** | **教育程度** | **婚姻状况** | **九月** | **八月** | **七月** | **六月**
    | **五月** | **四月** |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| 26664 (*male*) | 80,000 | university | single | 5 | 4 | 3 | 2 | paid duly
    | U.R.C. |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '| 26664 (*男性*) | 80,000 | 大学 | 单身 | 5 | 4 | 3 | 2 | 按时付款 | U.R.C. |'
- en: '| 599 (*female*) | 60,000 | university | single | 2 | U.R.C.^([a](ch65.xhtml#idm45346851500424))
    | U.R.C. | U.R.C. | U.R.C. | U.R.C. |'
  id: totrans-20
  prefs: []
  type: TYPE_TB
  zh: '| 599 (*女性*) | 60,000 | 大学 | 单身 | 2 | U.R.C.^([a](ch65.xhtml#idm45346851500424))
    | U.R.C. | U.R.C. | U.R.C. |'
- en: '| ^([a](ch65.xhtml#idm45346851500424-marker)) Use of revolving credit. |'
  id: totrans-21
  prefs: []
  type: TYPE_TB
  zh: '| ^([a](ch65.xhtml#idm45346851500424-marker)) 使用循环信贷。 |'
- en: For the male customer, the delay for the September payment had a negative impact
    of 33% (i.e., contributing toward “Default”), as the following explanation indicates.
    However, counterintuitively, the delay for the August payment had a positive impact.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 对于男性客户，九月份的支付延迟对“违约”有33%的负面影响，正如以下解释所示。然而，令人反感的是，八月份的支付延迟却有积极影响。
- en: '![Image](Images/aeds_65in02.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/aeds_65in02.png)'
- en: For the female customer, the two-month delay for September also contributed
    negatively, but at a much greater percentage (47%) compared to the five-month
    delay of the male customer (33%); see the following figure for more details.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 对于女性客户，九月份的两个月延迟也产生了负面影响，但比男性客户的五个月延迟（33%）有更大的百分比（47%）；更多细节请参见下图。
- en: '![Image](Images/aeds_65in03.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/aeds_65in03.png)'
- en: Although the gender feature was not included in the training of the model, we
    observed with the help of the explanations that the gender bias was encoded into
    other features (e.g., positive contribution for the delay of payment for the male
    customer). Moreover, in observing the percentages of the impact in the explanations,
    we detected harsher treatment of the female customer by the model (e.g., greater
    negative impact for a lesser delay of payment). This strange behavior should alarm
    us and motivate us to get a better sample of the defaulters.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管在模型的训练中未包含性别特征，但我们在解释的帮助下观察到性别偏见已编码到其他特征中（例如，男性客户延迟支付对延迟的积极贡献）。此外，在观察解释中的影响百分比时，我们发现模型对女性客户的处理更加严厉（例如，较小的支付延迟造成更大的负面影响）。这种奇怪的行为应该引起我们的警觉，并激励我们获取更好的违约样本。
- en: In summary, in cases where the dataset contains real people, it is important
    to ensure that the model does not discriminate against one group over others.
    Explanations allow us to detect bias even if it is hidden, pinpoint unintended
    decision patterns of our black-box model, and motivate us to fix our data.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，在数据集包含真实人员的情况下，确保模型不会歧视某一群体是非常重要的。解释能够帮助我们发现即使是隐藏的偏见，也能准确指出我们黑盒模型意外的决策模式，并激励我们修正数据。
