- en: 8 Observability
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 8 可观察性
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Relating GitOps to observability
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将GitOps与可观察性联系起来
- en: Providing observability of Kubernetes to a cluster operator
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 为集群操作员提供Kubernetes的可观察性
- en: Enabling GitOps through Kubernetes observability
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过Kubernetes的可观察性启用GitOps
- en: Improving the observability of the system with GitOps
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过GitOps提高系统的可观察性
- en: Using tools and techniques to ensure your cloud-native applications are also
    observable
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用工具和技术确保您的云原生应用程序也具有可观察性
- en: Observability is vital to manage a system properly, determine if it is working
    correctly, and decide what changes are needed to fix, change, or improve its behavior
    (such as how to control the system). Observability has been an area of interest
    in the cloud-native community for some time, with many projects and products being
    developed to allow observability of systems and applications. The Cloud Native
    Computing Foundation recently formed a Special Interest Group (SIG) dedicated
    to observability.^([1](#pgfId-1091133))
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性对于正确管理系统、确定系统是否正常工作以及决定需要做出哪些更改以修复、更改或改进其行为（例如如何控制系统）至关重要。可观察性一直是云原生社区的一个研究热点，许多项目和产品被开发出来以允许系统和应用程序的可观察性。云原生计算基金会最近成立了一个专门致力于可观察性的特别兴趣小组（SIG）[1](#pgfId-1091133)。
- en: In this chapter, we will discuss observability in the context of GitOps and
    Kubernetes. As was mentioned earlier, GitOps is implemented by a GitOps operator
    (or controller) or Service that must manage and control the Kubernetes cluster.
    The key functionality of GitOps is comparing the desired state of the system (which
    is stored in Git) to the current actual state of the system and performing the
    required operations to converge the two. This means GitOps relies on the observability
    of Kubernetes and the application to do its job. But GitOps is also a system that
    must provide observability. We will explore both aspects of GitOps observability.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将讨论在GitOps和Kubernetes环境下的可观察性。正如之前提到的，GitOps是通过GitOps操作员（或控制器）或服务实现的，它必须管理和控制Kubernetes集群。GitOps的关键功能是将系统的期望状态（存储在Git中）与系统的当前实际状态进行比较，并执行必要的操作以使两者收敛。这意味着GitOps依赖于Kubernetes和应用程序的可观察性来完成其工作。但GitOps也是一个必须提供可观察性的系统。我们将探讨GitOps可观察性的两个方面。
- en: We recommend you read chapters 1 and 2 before reading this chapter.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我们建议您在阅读本章之前先阅读第1章和第2章。
- en: 8.1 What is observability?
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.1 什么是可观察性？
- en: Observability is a system capability, like reliability, scalability, or security,
    that must be designed and implemented into the system during system design, coding,
    and testing. In this section, we will explore the various ways GitOps and Kubernetes
    provide observability for a cluster. For example, what version of an application
    was most recently deployed to the cluster? Who deployed it? How many replicas
    were configured for the application a month ago? Can a decrease in application
    performance be correlated to changes to the application or Kubernetes configuration?
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性是一个系统能力，就像可靠性、可伸缩性或安全性一样，必须在系统设计、编码和测试期间设计和实现到系统中。在本节中，我们将探讨GitOps和Kubernetes为集群提供可观察性的各种方式。例如，最近部署到集群中的应用程序版本是什么？谁部署了它？一个月前为该应用程序配置了多少个副本？应用程序性能的下降是否可以与对应用程序或Kubernetes配置的更改相关联？
- en: When managing a system, the focus is on controlling that system and applying
    changes that *improve* the system in some way, whether through additional functionality,
    increasing performance, improving stability, or some other beneficial change.
    But how do you know how to control the system and what changes to make? Once the
    changes are applied, how do you know that they improved the system and didn’t
    make it worse?
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在管理一个系统时，重点是控制该系统并应用某种方式改进系统的更改，无论是通过增加功能、提高性能、提高稳定性还是其他有益的更改。但您如何知道如何控制系统以及要做出哪些更改？一旦应用了更改，您如何知道它们是否改进了系统而没有使其变得更糟？
- en: Remember back to the earlier chapters. We previously discussed how GitOps stores
    the desired state of a system in a declarative format in Git. A GitOps operator
    (or service) changes (controls) the system’s running state to match the system’s
    desired state. The GitOps operator must be able to observe the system being managed,
    which in our case is Kubernetes and the applications running on Kubernetes. What’s
    more, the GitOps operator itself must also provide observability so that ultimately
    the user can control GitOps.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 记得回到前面的章节。我们之前讨论了 GitOps 如何将系统的期望状态存储在 Git 中的声明性格式中。GitOps 操作员（或服务）通过改变（控制）系统的运行状态来匹配系统的期望状态。GitOps
    操作员必须能够观察被管理的系统，在我们的案例中是 Kubernetes 和在 Kubernetes 上运行的应用程序。更重要的是，GitOps 操作员本身也必须提供可观察性，以便最终用户可以控制
    GitOps。
- en: OK, but in practical terms, what does that really mean?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，但在实际操作中，这究竟意味着什么呢？
- en: 'As was mentioned earlier, observability is a capability of a system that encompasses
    multiple aspects. Each of these aspects must be designed and built into the system.
    Let’s briefly examine each of these three aspects: event logging, metrics, and
    tracing.'
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，可观察性是一个涵盖多个方面的系统功能。这些方面中的每一个都必须设计和构建到系统中。让我们简要地检查这三个方面：事件记录、指标和跟踪。
- en: '![](Images/CH08_F01_Yuen.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F01_Yuen.png)'
- en: 'Figure 8.1 Observability is composed of three primary aspects: event logging,
    metrics, and tracing. These aspects combine to provide operational insights that
    enable the proper management of the system.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.1 可观察性由三个主要方面组成：事件记录、指标和跟踪。这些方面结合在一起，提供了操作洞察力，使得正确管理系统成为可能。
- en: 8.1.1 Event logging
  id: totrans-18
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.1 事件记录
- en: Most developers are familiar with the concept of logging. As the code executes,
    log messages can be output that indicate significant events, errors, or changes.
    Each event log is timestamped and is an immutable record of a particular system
    component’s internal operation. When rare or unpredictable failures occur, the
    event log may provide context at a fine level of granularity, indicating what
    went wrong.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数开发者都熟悉日志的概念。随着代码的执行，可以输出日志消息来指示重要事件、错误或更改。每个事件日志都有时间戳，并且是特定系统组件内部操作的不可变记录。当发生罕见或不可预测的故障时，事件日志可能提供细粒度的上下文，指示出了什么问题。
- en: '![](Images/CH08_F02_Yuen.png)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F02_Yuen.png)'
- en: Figure 8.2 Event logs are timestamped and provides an immutable record of a
    particular system component’s internal operation.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.2 事件日志有时间戳，并提供特定系统组件内部操作的不可变记录。
- en: Often the first step in debugging an improperly behaving application is to look
    at the application’s log for clues. Logging is an invaluable tool for developers
    to observe and debug the system and applications. Logging and the retention of
    records may also be required for compliance with applicable industry standards.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在调试表现不当的应用程序时，通常的第一步是查看应用程序的日志以获取线索。日志是开发者观察和调试系统和应用程序的无价工具。日志和记录的保留也可能需要符合适用的行业标准。
- en: For Kubernetes, a fundamental aspect of observability is displaying the log
    output of all the various Pods in the cluster. Applications output debug information
    about their running state to stdout (standard out), captured by Kubernetes and
    saved to a file on the Kubernetes node that the Pod/container is running on. The
    logs of a particular Pod can be displayed with the `kubectl` `logs` `<pod_name>`
    command.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 对于 Kubernetes 来说，可观察性的一个基本方面是显示集群中所有各种 Pods 的日志输出。应用程序将有关其运行状态的调试信息输出到 stdout（标准输出），由
    Kubernetes 捕获并保存到 Pod/容器运行的 Kubernetes 节点上的文件。可以使用 `kubectl logs <pod_name>` 命令显示特定
    Pod 的日志。
- en: To illustrate various aspects of logging, metrics, and tracing, we will be using
    an example ride-sharing application called Hot ROD.^([2](#pgfId-1091175)) Let’s
    launch the application in our minikube cluster so we can take a look at its logs.
    Here is the manifest for the application deployment.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明日志、指标和跟踪的各个方面，我们将使用一个名为 Hot ROD 的共享出行应用程序作为示例.^([2](#pgfId-1091175)) 让我们在
    minikube 集群中启动该应用程序，以便我们可以查看其日志。以下是应用程序部署的清单。
- en: Listing 8.1 Hot ROD application deployment [(http://mng.bz/vzPJ)](https://shortener.manning.com/vzPJ)
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.1 Hot ROD 应用程序部署 [(http://mng.bz/vzPJ)](https://shortener.manning.com/vzPJ)
- en: '[PRE0]'
  id: totrans-26
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Deploy the Hot ROD application using the following commands:'
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 使用以下命令部署 Hot ROD 应用程序：
- en: '[PRE1]'
  id: totrans-28
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'Now let’s look at the log messages of the Pod:'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看看 Pod 的日志消息：
- en: '[PRE2]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: The output tells us that each of the microservices (`route`, `frontend`, and
    `customer`) is “Starting.” But at this point, there is not too much information
    in the log. And it may not be entirely clear if each of the microservices was
    successfully started.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 输出告诉我们，每个微服务（`路由`、`前端`和`客户`）都处于“启动”状态。但在此阶段，日志中并没有太多信息。而且，可能并不完全清楚每个微服务是否成功启动。
- en: 'Use the `minikube service hotrod-frontend` command to create a tunnel on your
    workstation to the `hotrod-frontend` service and open the URL in a web browser:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`minikube service hotrod-frontend`命令在您的工作站上为`hotrod-frontend`服务创建隧道，并在网页浏览器中打开URL：
- en: '[PRE3]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: This will open a web browser to the application. When it opens, click on each
    of the buttons to simulate requesting a ride for each customer.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个网页浏览器到应用程序。当它打开时，点击每个按钮以模拟为每位客户请求一次乘车。
- en: '![](Images/CH08_F03_Yuen.png)'
  id: totrans-35
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F03_Yuen.png)'
- en: Figure 8.3 A screenshot of the Hot ROD example application that simulates a
    ride-sharing system. Clicking the buttons at the top of the page initiates a process
    that invokes multiple microservices to match a customer with a driver and a route.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.3 Hot ROD示例应用程序的截图，该应用程序模拟了一个拼车系统。点击页面顶部的按钮将启动一个调用多个微服务的过程，以将客户与司机和路线匹配起来。
- en: 'Now, in another terminal window, let’s look at the logs for the application:'
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在另一个终端窗口中，让我们查看应用程序的日志：
- en: '[PRE4]'
  id: totrans-38
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Logging is very flexible in that it can also be used to infer a lot of information
    about the application. For example, you can see in the first line of the log snippet
    `HTTP request` `received`, indicating a frontend service request. You can also
    see log messages related to loading customer information, locating the nearest
    drivers, and so on. There is also a timestamp on each log message so you can calculate
    the amount of time taken for a particular request by subtracting the ending time
    from the starting time. You could also calculate the number of requests that were
    processed in a given interval. To do this type of log analysis at scale, you need
    cluster-level logging^([3](#pgfId-1091273)) and a central logging backend like
    Elasticsearch plus Kibana^([4](#pgfId-1091277)) or Splunk.^([5](#pgfId-1091281))
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 日志记录非常灵活，它还可以用来推断关于应用程序的大量信息。例如，您可以在日志片段的第一行看到`HTTP请求` `接收`，这表明一个前端服务请求。您还可以看到与加载客户信息、定位最近的司机等相关日志消息。每个日志消息上都有一个时间戳，这样您就可以通过从结束时间减去开始时间来计算特定请求所需的时间量。您还可以计算在给定时间间隔内处理了多少个请求。要进行此类大规模日志分析，您需要集群级别的日志记录^([3](#pgfId-1091273))以及像Elasticsearch加Kibana^([4](#pgfId-1091277))或Splunk.^([5](#pgfId-1091281))这样的中央日志后端。
- en: 'Click on a few more of the buttons in the Hot ROD application. We can determine
    the number of requests by counting the number of `HTTP request received` messages
    for the frontend service:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在Hot ROD应用程序中点击更多按钮。我们可以通过计算前端服务收到的`HTTP请求接收`消息的数量来确定请求数量：
- en: '[PRE5]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: From this output, we can see that there have been seven frontend requests received
    since the Pod was started.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以看到自Pod启动以来已经收到了七个前端请求。
- en: However, as critical and flexible as logging is, it is sometimes not the best
    tool to observe certain aspects of the system. Logs are a very low-level aspect
    of observability. Using log messages to derive metrics such as count of requests
    processed, requests per second, and so on, can be quite expensive and may not
    give all the information you need. Also, it is quite tricky, if not impossible,
    to determine the state of the system at any given moment by parsing log messages
    without a deep understanding of the code. Often log messages are coming from different
    threads and subprocesses of the system and must be correlated to one another to
    follow the system’s current state. So, while important, Pod logs are just barely
    scratching the surface of the observability capabilities of Kubernetes.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，尽管日志记录既关键又灵活，但它有时并不是观察系统某些方面的最佳工具。日志是可观察性的一个非常低级方面。使用日志消息来推导出诸如处理请求数量、每秒请求数等指标可能相当昂贵，并且可能无法提供您所需的所有信息。此外，如果没有对代码的深入理解，仅通过解析日志消息来确定系统在任何给定时刻的状态可能相当棘手，甚至不可能。通常，日志消息来自系统的不同线程和子进程，必须将它们相互关联以跟踪系统的当前状态。因此，虽然日志很重要，但Pod日志只是刚刚触及Kubernetes可观察性能力的表面。
- en: In the next section, we will see how metrics can be used to observe the system’s
    properties, instead of low-level log parsing.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将看到如何使用指标来观察系统的属性，而不是进行低级日志解析。
- en: Exercise 8.1
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.1
- en: 'Use the `kubectl` `logs` command to display the Hot ROD Pod’s log messages
    and look for any error messages (hint: `grep` for the string `ERROR`). If so,
    what are the types of errors you see?'
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl logs` 命令显示 Hot ROD Pod 的日志消息，并查找任何错误消息（提示：使用 `grep` 查找字符串 `ERROR`）。如果有，你看到了哪些类型的错误？
- en: 8.1.2 Metrics
  id: totrans-47
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.2 指标
- en: Another critical aspect of observability is metrics that measure the performance
    and operation of the system or application. At a fundamental level, metrics are
    a set of key-value pairs that provide information about the system’s operation.
    You can think of metrics as the observable properties of each component of the
    system. Some core resource metrics that apply to all components are CPU, memory,
    disk, and network utilization. Other metrics may be specific to the application,
    like the number of a particular type of error that has been encountered or the
    count of items in a queue waiting to be processed.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 可观测性的另一个关键方面是衡量系统或应用程序性能和操作的指标。在基本层面上，指标是一组键值对，提供了关于系统操作的详细信息。你可以将指标视为系统每个组件的可观察属性。一些适用于所有组件的核心资源指标包括
    CPU、内存、磁盘和网络利用率。其他指标可能特定于应用程序，例如遇到特定类型错误的数量或等待处理的队列中项的数量。
- en: '![](Images/CH08_F04_Yuen.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F04_Yuen.png)'
- en: Figure 8.4 Metrics are a set of key-value pairs that provide information about
    the system’s operation.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.4 指标是一组键值对，提供了关于系统操作的详细信息。
- en: 'Kubernetes provides basic metrics using an optional component called the `metrics-server`.
    The `metrics-server` can be enabled in minikube by running the following command:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用一个名为 `metrics-server` 的可选组件提供基本指标。可以通过运行以下命令在 minikube 中启用 `metrics-server`：
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: Once the `metrics-server` is enabled and you wait a few minutes for enough metrics
    to be collected, you can access the `metrics-server` [data using the commands](https://github.com/gitopsbook/sample-app-deployment)
    `kubectl top nodes` and `kubectl` `top` `pods`.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用 `metrics-server` 并等待几分钟以收集足够的指标，你就可以使用 `kubectl top nodes` 和 `kubectl top
    pods` 命令访问 `metrics-server` 的 [数据](https://github.com/gitopsbook/sample-app-deployment)。
- en: Listing 8.2 Output of `kubectl top nodes` and `kubectl top pods`
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.2 `kubectl top nodes` 和 `kubectl top pods` 的输出
- en: '[PRE7]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: This output shows the CPU and memory utilization of the node (minikube) and
    the running Pods.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 此输出显示了节点（minikube）和运行 Pods 的 CPU 和内存利用率。
- en: In addition to general CPU and memory utilization that is common across all
    Pods, applications can provide their own metrics by exposing an HTTP *metrics
    endpoint* that returns a list of metrics in the form of key-value pairs. Let’s
    look at the Hot ROD application metrics endpoint that we used in the previous
    section.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 除了所有 Pods 都有的通用 CPU 和内存利用率之外，应用程序可以通过公开一个返回键值对形式指标列表的 HTTP `metrics endpoint`
    来提供自己的指标。让我们看看我们在上一节中使用的 Hot ROD 应用程序指标端点。
- en: 'In another terminal, use the `kubectl port-forward` command to forward a port
    on your workstation to the metrics endpoint of Hot ROD, which is exposed on port
    8083 of the Pod:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在另一个终端中，使用 `kubectl port-forward` 命令将你的工作站上的一个端口转发到 Hot ROD 的指标端点，该端点在 Pod 的
    8083 端口上公开：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Once the port forward connection is established, open http://localhost:8083/metrics
    in your web browser or run `curl` `http://localhost:8083/metrics` from the command
    line.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦建立端口转发连接，请在您的网页浏览器中打开 http://localhost:8083/metrics，或者从命令行运行 `curl http://localhost:8083/metrics`。
- en: Listing 8.3 Output of Hot ROD metrics endpoint
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.3 Hot ROD 指标端点的输出
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: By themselves, metrics provide a snapshot of a system component’s performance
    and operation at a particular point in time. Often, metrics are collected periodically
    and stored in a time-series database to observe the metrics’ historical trends.
    In Kubernetes, this is typically done by a Cloud Native Computing Foundation (CNCF)
    open source project called Prometheus ([https://prometheus.io](https://prometheus.io)).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 指标本身提供了在特定时间点系统组件性能和操作的快照。通常，指标会定期收集并存储在时间序列数据库中，以观察指标的历史趋势。在 Kubernetes 中，这通常由一个名为
    Prometheus 的 Cloud Native Computing Foundation (CNCF) 开源项目来完成（[https://prometheus.io](https://prometheus.io)）。
- en: '![](Images/CH08_F05_Yuen.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F05_Yuen.png)'
- en: Figure 8.5 A single Prometheus deployment can scrape the metric endpoints of
    both nodes and Pods of a cluster. Metrics are scraped at a configurable interval
    and stored in a time-series database.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.5 单个 Prometheus 部署可以抓取集群中节点和 Pods 的指标端点。指标以可配置的间隔抓取并存储在时间序列数据库中。
- en: As was mentioned earlier in the chapter, some metrics can be inferred through
    the examination of log messages. Still, it is more efficient to have the system
    or application measure its metrics directly and provide programmatic access to
    query the metric values.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 如本章前面所述，一些指标可以通过检查日志消息来推断。然而，让系统或应用程序直接测量其指标并提供程序性访问以查询指标值更为高效。
- en: Exercise 8.2
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.2
- en: 'Find out the number of different HTTP requests. (Hint: search for metrics named
    `http_requests`). How many `GET` `/dispatch`, `GET` `/customer`, and `GET` `/route`
    requests have been processed by the application? How would you get similar information
    from the application’s logs?'
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 查找不同HTTP请求的数量。（提示：搜索名为`http_requests`的指标）。应用程序处理了多少`GET` `/dispatch`、`GET` `/customer`和`GET`
    `/route`请求？您如何从应用程序的日志中获取类似的信息？
- en: 8.1.3 Tracing
  id: totrans-69
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.3 跟踪
- en: Typically, distributed tracing data requires an application-specific agent that
    knows how to collect the detailed execution paths of the code being traced. A
    distributed tracing framework captures detailed data of how the system runs internally,
    from the initial end user request on through possibly dozens (hundreds?) of calls
    to different
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，分布式跟踪数据需要一个特定于应用程序的代理，该代理知道如何收集被跟踪代码的详细执行路径。分布式跟踪框架捕获系统内部运行的详细数据，从初始终端用户请求开始，通过可能数十（数百？）次调用到不同的
- en: microservices and other external dependencies, perhaps hosted on another system.
    Whereas metrics typically give an aggregated view of the application in a particular
    system, tracing data usually provides a detailed picture of an individual request’s
    execution flow, potentially across multiple services and systems. This is particularly
    important in the age of microservices where an “application” may utilize functionality
    from tens or hundreds of services and cross multiple operational boundaries. As
    mentioned earlier in section 8.1.1, the Hot ROD application is composed of four
    different microservices (frontend, customer, driver, and route) and two simulated
    storage backends (MySql and Redis).
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 微服务和其他外部依赖项，可能托管在另一个系统上。而指标通常提供特定系统中的应用程序的聚合视图，而跟踪数据通常提供单个请求执行流程的详细情况，可能跨越多个服务和系统。在微服务时代，一个“应用程序”可能利用来自数十或数百个服务的功能，跨越多个操作边界，这一点尤为重要。如前文第8.1.1节所述，Hot
    ROD应用程序由四个不同的微服务（前端、客户、司机和路线）和两个模拟存储后端（MySql和Redis）组成。
- en: '![](Images/CH08_F06_Yuen.png)'
  id: totrans-72
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F06_Yuen.png)'
- en: Figure 8.6 Tracing captures detailed data of how the system runs internally.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.6 跟踪捕获了系统内部运行的详细数据。
- en: 'To illustrate this, let’s look at one popular tracing framework, Jaeger, and
    the example Hot ROD application from sections 8.1.1 and 8.1.2\. First, install
    Jaeger on the minikube cluster and use the following commands to verify it is
    running successfully:'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明这一点，让我们看看一个流行的跟踪框架Jaeger，以及第8.1.1节和第8.1.2节中的示例Hot ROD应用程序。首先，在minikube集群上安装Jaeger，并使用以下命令验证它是否成功运行：
- en: '[PRE10]'
  id: totrans-75
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Now that Jaeger is running, we need to update the Hot ROD application deployment
    to send tracing data to Jaeger. This is done simply by adding the `JAEGER_AGENT
    _HOST` environment variable to the `hotrod` container, indicating the `jaeger-agent`
    service deployed by jaeger-all-in-one.yaml in the previous step:'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在Jaeger正在运行，我们需要更新Hot ROD应用程序部署，以便将跟踪数据发送到Jaeger。这可以通过简单地向`hotrod`容器添加`JAEGER_AGENT_HOST`环境变量来完成，指示在前一步骤中由jaeger-all-in-one.yaml部署的`jaeger-agent`服务：
- en: '[PRE11]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: Since we have configured the `hotrod-app` to send data to Jaeger, we need to
    generate some trace data by opening the `hotrod-app` UI and clicking a few buttons
    as we did in the event-logging section.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们已经配置了`hotrod-app`向Jaeger发送数据，我们需要通过打开`hotrod-app` UI并点击几个按钮来生成一些跟踪数据，就像我们在事件日志部分所做的那样。
- en: 'Use the `minikube service hotrod-frontend` command to create a tunnel on your
    workstation to the `hotrod-frontend` service and open the URL in a web browser:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`minikube service hotrod-frontend`命令在您的工作站上为`hotrod-frontend`服务创建隧道，并在网络浏览器中打开URL：
- en: '[PRE12]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: This will open a web browser to the application. When it opens, click on each
    of the buttons to simulate requesting a ride for each customer.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 这将打开一个网络浏览器到应用程序。当它打开时，点击每个按钮以模拟为每位客户请求一次乘车。
- en: 'Now that we should have some trace data, open the Jaeger UI by running `minikube`
    `service` `jaeger-query`:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们应该有一些跟踪数据，通过运行`minikube` `service` `jaeger-query`来打开Jaeger UI：
- en: '[PRE13]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: This will open the Jaeger UI in your default browser. Or, you can open the URL
    yourself that is listed in the previous output (such as http://127.0.0.1:51831).
    When you are finished with the Jaeger exercises in this chapter, you can press
    Ctrl-C to close the tunnel to the `jaeger-query` service.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 这将在你的默认浏览器中打开Jaeger UI。或者，你也可以打开之前输出中列出的URL（例如http://127.0.0.1:51831）。当你完成本章的Jaeger练习后，你可以按Ctrl-C关闭到`jaeger-query`服务的隧道。
- en: From the Jaeger UI, you can choose the service “frontend” and the operation
    “HTTP GET /dispatch,” and then click Find Traces to get a list of all the `GET`
    `/dispatch` call graph traces.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 从Jaeger UI中，你可以选择服务“前端”和操作“HTTP GET /dispatch”，然后点击查找跟踪以获取所有`GET` `/dispatch`调用图跟踪的列表。
- en: '![](Images/CH08_F07_Yuen.png)'
  id: totrans-86
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F07_Yuen.png)'
- en: Figure 8.7 The Jaeger Search tab displays the `GET` `/dispatch` requests from
    the frontend service that occurred in the last hour. A graph at the top right
    shows each request’s duration over time, with each circle’s size representing
    the number of spans in each request. The bottom-right lists of all the requests,
    and each row can be clicked for additional detail.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.7 Jaeger搜索标签显示在过去一小时中前端服务发生的`GET` `/dispatch`请求。右上角的图表显示了每个请求随时间的变化持续时间，每个圆圈的大小代表每个请求中的跨度数量。右下角列出了所有请求，并且可以点击每一行以获取更多详细信息。
- en: From there, you can select the trace to examine. The following screenshot shows
    a call graph of the frontend `GET` `/dispatch` request in the Jaeger UI.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 从那里，你可以选择要检查的跟踪。以下屏幕截图显示了Jaeger UI中前端`GET` `/dispatch`请求的调用图。
- en: '![](Images/CH08_F08_Yuen.png)'
  id: totrans-89
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F08_Yuen.png)'
- en: Figure 8.8 In this detailed view of a `GET` `/dispatch` trace, Jaeger displays
    all the call spans initiated from the originating request. This example shows
    such details as the duration and logs of an SQL `SELECT` call and the Redis `GetDriver`
    call’s return error.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.8 在这个`GET` `/dispatch`跟踪的详细视图下，Jaeger显示了从原始请求发起的所有调用跨度。这个例子展示了诸如SQL `SELECT`调用的持续时间以及Redis
    `GetDriver`调用返回错误等详细信息。
- en: As you can see from figure 8.8, there is a lot of valuable information regarding
    the `GET` `/dispatch` request processing. From this, you can see the breakdown
    of what code is being called, what its response is (success or failure), and how
    long it took. For example, in the screenshot, it appears that the SQL query used
    by this request took 930.37 ms. Is that good? The application developer can do
    more testing and dig deeper to see if that query can be optimized or if there
    is a different area of the code that would benefit from additional optimization.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 如图8.8所示，关于`GET` `/dispatch`请求处理有很多有价值的信息。从这里，你可以看到正在调用的代码的分解，它的响应（成功或失败），以及它花费了多长时间。例如，在屏幕截图中，这个请求使用的SQL查询耗时930.37毫秒。这是否合理？应用程序开发者可以进行更多测试并深入挖掘，看看这个查询是否可以优化，或者代码的哪个区域可以通过额外的优化受益。
- en: Again, as was mentioned earlier, a developer may sprinkle their code with log
    statements to have “tracing data” in their application logs, but this is a costly
    and inefficient approach. Using a proper framework for tracing is much more desirable
    and will provide a much better result in the long run.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 再次强调，正如之前提到的，开发者可以在他们的代码中添加日志语句，以便在应用程序日志中拥有“跟踪数据”，但这是一种成本高且效率低的方法。使用合适的跟踪框架要理想得多，并且从长远来看会提供更好的结果。
- en: As you can imagine, tracing data can be quite large, and it may not be feasible
    to collect the data for every single request. Tracing data is often sampled at
    a configured rate and may have a much shorter retention period than, for example,
    application logs or metrics.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 如你所想，跟踪数据可能相当大，并且可能无法为每个单独的请求收集数据。跟踪数据通常以配置的速率进行采样，并且可能比应用程序日志或指标有更短的保留期。
- en: What does tracing have to do with GitOps? Honestly, not much. But tracing is
    an essential and growing part of observability, with many new developments in
    tools and services that help provide, manage, and analyze distributed tracing
    data, so it’s important to understand how it fits in the overall observability
    system. It is also possible that in the future, tracing tools (such as OpenTelemetry)
    will be used for more aspects of observability by expanding to cover metrics and
    logging.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪与GitOps有什么关系？老实说，关系不大。但跟踪是可观察性的一个基本且不断发展的部分，有许多新工具和服务的发展有助于提供、管理和分析分布式跟踪数据，因此了解它在整体可观察性系统中的位置很重要。也有可能在未来，跟踪工具（如OpenTelemetry）将通过扩展到覆盖指标和日志来用于更多可观察性的方面。
- en: Exercise 8.3
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.3
- en: Identify the performance bottleneck in Hot ROD using the Jaeger distributed
    tracing platform.^([6](#pgfId-1091504)) To do this, open the Hot ROD UI in a browser
    window. Click a few customers in the Hot ROD UI (such as Japanese Desserts and
    Amazing Coffee Roasters) to schedule rides in the application. Actually, go crazy!
    Keep clicking the different customer buttons a bunch of times.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 Jaeger 分布式跟踪平台在 Hot ROD 中识别性能瓶颈.^([6](#pgfId-1091504)) 要这样做，请在浏览器窗口中打开 Hot
    ROD UI。在 Hot ROD UI 中点击几个客户（例如日本甜点和惊人的咖啡烘焙师）以在应用程序中安排行程。实际上，尽情点击不同的客户按钮。
- en: 'Once you’ve played around a bit with the application, open the Jaeger UI as
    described earlier in this section. Use the search capabilities to find traces
    for various services and requests to answer the following questions. You may want
    to change Limit Results to a larger value, like maybe 200:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你对应用程序进行了一些操作，请按照本节前面所述打开 Jaeger UI。使用搜索功能查找各种服务和请求的跟踪记录以回答以下问题。你可能需要将“限制结果”更改为更大的值，比如
    200：
- en: Do you have any traces containing errors? If so, what component is causing the
    errors?
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是否有包含错误的跟踪记录？如果有，是哪个组件导致了错误？
- en: Do you notice any difference in latency in requests when you click customer
    buttons slowly versus very quickly? Do you have any dispatch requests that take
    longer than 1000 ms?
  id: totrans-99
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 当你缓慢或非常快速地点击客户按钮时，你是否注意到请求延迟有任何差异？你是否有一些超过 1000 毫秒的调度请求？
- en: 'Search Jaeger for the trace with the longest latency according to the Hot ROD
    app using the driver ID (which is in bold). Hint: Use the Tags filter and search
    for “driver=T123456C,” where “T123456C” is the driver ID of your longest latency
    request.'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 Jaeger 搜索根据 Hot ROD 应用程序使用驾驶员 ID（粗体显示）的最长延迟跟踪记录。提示：使用“标签”过滤器并搜索“driver=T123456C”，其中“T123456C”是您最长延迟请求的驾驶员
    ID。
- en: Where is the application spending the most time? What span is the longest?
  id: totrans-101
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 应用程序在哪里花费的时间最多？哪个跨度是最长的？
- en: 'What do the logs of the longest span say? Hint: The log message starts with
    `Waiting` `for...`.'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 长跨度日志中说了什么？提示：日志消息以`Waiting` `for...`开头。
- en: 'Based on what you discovered in the previous question, what might be impacting
    the performance of the Hot ROD application? Examine the code at this link:'
  id: totrans-103
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据你在上一个问题中发现的内容，什么可能影响了 Hot ROD 应用程序的性能？请检查以下链接中的代码：
- en: '[http://mng.bz/QmRw](https://shortener.manning.com/QmRw)'
  id: totrans-104
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: '[http://mng.bz/QmRw](https://shortener.manning.com/QmRw)'
- en: Exercise 8.4
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.4
- en: 'Fix the Hot ROD application’s performance bottleneck by adding the `--fix-
    disable-db-conn-mutex` argument to the `hotrod` container and updating the deployment.
    (Hint: Look at the GitHub hotrod-app-jaeger.yaml file, and uncomment the appropriate
    line.) This simulates fixing a database lock contention in the code:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 通过向 `hotrod` 容器添加 `--fix-disable-db-conn-mutex` 参数并更新部署来解决 Hot ROD 应用程序的性能瓶颈。（提示：查看
    GitHub 上的 hotrod-app-jaeger.yaml 文件，并取消注释相应的行。）这模拟了在代码中修复数据库锁竞争：
- en: Update the `hotrod` container by adding the `--fix-disable-db-conn-mutex` argument.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过添加 `--fix-disable-db-conn-mutex` 参数更新 `hotrod` 容器。
- en: Redeploy the updated hotrod-app-jaeger.yaml file.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重新部署更新的 hotrod-app-jaeger.yaml 文件。
- en: Test the `hotrod` UI. Do you notice a difference in the latency of each dispatch?
    Can you get a dispatch request to take longer than 1000 ms?
  id: totrans-109
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 测试 `hotrod` UI。你是否注意到每个调度的延迟有差异？你是否能让调度请求超过 1000 毫秒？
- en: Look at the Jaeger UI. Do you notice the difference in the traces? What is different
    after adding the `--fix-disable-db-conn-mutex` argument?
  id: totrans-110
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 查看 Jaeger UI。你是否注意到跟踪记录的差异？添加 `--fix-disable-db-conn-mutex` 参数后有什么不同？
- en: To dive deeper into Jaeger and the Hot ROD sample application, refer to the
    blog and video links at the top of the README page.^([7](#pgfId-1091528))
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 要深入了解 Jaeger 和 Hot ROD 示例应用程序，请参阅 README 页面顶部的博客和视频链接.^([7](#pgfId-1091528))
- en: 8.1.4 Visualization
  id: totrans-112
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.4 可视化
- en: All of the discussed aspects of observability are really about the system providing
    data about itself. And it adds up to a lot of data that can be difficult to make
    sense of. The last aspect of observability is visualization tools that help convert
    all that observability data into information and insights.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 所讨论的可观察性的各个方面实际上都是关于系统提供自身数据。这累积了大量的数据，可能难以理解。可观察性的最后一个方面是可视化工具，它有助于将所有这些可观察性数据转换为信息和洞察。
- en: Many tools provide visualization of observability data. In the previous section,
    we discussed Jaeger, which provides visualization of trace data. But now, let’s
    look at another tool that provides visualization of the current running state
    of your Kubernetes cluster, the K8s Dashboard.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 许多工具提供了可观察性数据的可视化。在上一节中，我们讨论了Jaeger，它提供了跟踪数据的可视化。但现在，让我们看看另一个提供Kubernetes集群当前运行状态可视化的工具，即K8s仪表板。
- en: 'You can enable the K8s Dashboard minikube add-on^([8](#pgfId-1091538)) using
    the following command:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用以下命令启用K8s仪表板minikube插件^([8](#pgfId-1091538))：
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Stops minikube if it is already running
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 如果minikube已经在运行，则停止它
- en: ❷ Enables the dashboard add-on
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 启用仪表板插件
- en: 'Once enabled, you can start your minikube cluster and display the Dashboard:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦启用，您就可以启动minikube集群并显示仪表板：
- en: '[PRE15]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Dashboard add-on is started with minikube.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仪表板插件与minikube一起启动。
- en: The command `minikube dashboard` will open a browser window that looks similar
    to figure 8.9.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 命令`minikube dashboard`将打开一个类似于图8.9的浏览器窗口。
- en: '![](Images/CH08_F09_Yuen.png)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F09_Yuen.png)'
- en: Figure 8.9 The Overview page of the Kubernetes Dashboard shows the CPU and memory
    usages of the workloads currently running on the cluster, along with a summary
    chart of each type of workload (Deployments, Pods, and ReplicaSets). The page’s
    bottom-right pane shows a list of each workload `GET` `/dispatch`, and each row
    can be clicked for additional detail.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.9 Kubernetes仪表板的概览页面显示了集群上当前运行的工作负载的CPU和内存使用情况，以及每种类型工作负载的摘要图表（部署、Pod和ReplicaSet）。页面右下角的面板显示了每个工作负载`GET`
    `/dispatch`的列表，并且可以点击每一行以获取更多详细信息。
- en: The Kubernetes Dashboard provides visualization of many different aspects of
    your Kubernetes cluster, including the status of your Deployments, Pods, and ReplicaSets.
    We can see that there is a problem with the `bigpod` deployment. Upon seeing this,
    the Kubernetes administrator should take action to bring this deployment back
    into a healthy state.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes仪表板提供了对Kubernetes集群许多不同方面的可视化，包括您的部署、Pod和ReplicaSet的状态。我们可以看到`bigpod`部署存在问题。看到这一点后，Kubernetes管理员应该采取行动，将此部署恢复到健康状态。
- en: Exercise 8.5
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.5
- en: 'Install the Dashboard add-on on minikube. Open the Dashboard UI, explore the
    different panels available, and perform the following actions:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 在minikube上安装仪表板插件。打开仪表板UI，探索可用的不同面板，并执行以下操作：
- en: Switch the view to All Namespaces. How many total Pods are running on minikube?
  id: totrans-128
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 切换到所有命名空间视图。minikube上运行的总Pod数量是多少？
- en: Select the Pod panel. Filter the Pods list to those containing “dns” in the
    name by clicking on the filter icon in the top right of the Pod panel. Delete
    one of the DNS-related Pods by clicking the action icon for that Pod and choosing
    Delete. What happens?
  id: totrans-129
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择Pod面板。通过点击Pod面板右上角的过滤器图标，过滤Pod列表以包含名称中带有“dns”的Pod。通过点击该Pod的动作图标并选择删除来删除一个与DNS相关的Pod。会发生什么？
- en: Click on the plus icon (+) in the top right of the Dashboard UI, and select
    “Create from form.” Create a new NGINX deployment containing three Pods using
    the `nginx` image. Experiment with the “Create from input” and “Create from file”
    options, perhaps using code listings from earlier chapters.
  id: totrans-130
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在仪表板UI的右上角点击加号图标（+），然后选择“从表单创建”。使用`nginx`镜像创建一个新的包含三个Pods的NGINX部署。尝试使用“从输入创建”和“从文件创建”选项，也许可以使用前面章节中的代码列表。
- en: 8.1.5 Importance of observability in GitOps
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.1.5 可观察性在GitOps中的重要性
- en: Okay, so now you know what observability is and that it is generally a good
    thing, but you may wonder why a book on GitOps would devote a whole chapter to
    observability. What does observability have to do with GitOps?
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 好的，现在您已经知道了什么是可观察性，以及它通常是一件好事，但您可能想知道为什么一本关于GitOps的书会专门用一整章来介绍可观察性。可观察性跟GitOps有什么关系？
- en: As was discussed in chapter 2, using the declarative configuration of Kubernetes
    allows the desired state of the system to be precisely defined. But how do you
    know if the running state of the system has converged with the desired state?
    How do you know if a particular deployment was successful? More broadly, how can
    you tell if your system is working as intended? These are critical questions that
    GitOps and observability should help answer.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 正如第2章所讨论的，使用Kubernetes的声明式配置可以精确地定义系统的期望状态。但您如何知道系统的运行状态是否与期望状态一致？您如何知道特定的部署是否成功？更广泛地说，您如何判断系统是否按预期工作？这些问题是GitOps和可观察性应该帮助解决的问题。
- en: 'There are several aspects of observability in Kubernetes that are critical
    for the GitOps system to function well and answer critical questions about the
    system:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 中有几个可观察性的方面对于 GitOps 系统良好运行和回答关于系统的关键问题至关重要：
- en: Application health—Is the application operating correctly? If a new version
    of the application is deploying using GitOps, is the system “better” than before?
  id: totrans-135
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用健康状态—应用是否正常运行？如果应用的新版本正在使用 GitOps 进行部署，系统是否“更好”于之前？
- en: Application sync status—Is the running state of the application the same as
    the desired state defined in the deployment Git repo?
  id: totrans-136
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用同步状态—应用的运行状态是否与在部署 Git 仓库中定义的期望状态相同？
- en: Configuration drift—Has the application’s configuration been changed outside
    of the declarative GitOps system (such as either manually or imperatively)?
  id: totrans-137
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置漂移—应用配置是否在声明式 GitOps 系统之外（如手动或命令式）进行了更改？
- en: GitOps change log—What changes were recently made to the GitOps system? Who
    made them, and for what reason?
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitOps 变更日志—最近对 GitOps 系统做了哪些更改？谁做的，出于什么原因？
- en: The remainder of this chapter will cover how the observability of the Kubernetes
    system and the application allow these questions to be answered by the GitOps
    system and how, in turn, the GitOps system provides observability capabilities.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的其余部分将涵盖 Kubernetes 系统和应用的可观察性如何使 GitOps 系统能够回答这些问题，以及 GitOps 系统如何反过来提供可观察性功能。
- en: 8.2 Application health
  id: totrans-140
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.2 应用健康状态
- en: The first and most important way observability relates to GitOps is in the ability
    to observe application health. At the beginning of the chapter, we talked about
    how operating a system is really about managing that system so that overall it
    improves and gets better over time instead of getting worse. GitOps is key where
    the desired state of the system (one that presumably is “better” than the current
    state) is committed to the Git repo, and then the GitOps operator applies that
    desired state to the system, making it become the current state.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 可观察性与 GitOps 的第一和最重要的关联方式是观察应用健康状态。在本章开头，我们讨论了运营一个系统实际上就是管理该系统，使其整体随着时间的推移不断改进和变得更好，而不是变得更糟。GitOps
    在系统期望状态（假设比当前状态“更好”）提交到 Git 仓库，然后 GitOps 操作员将此期望状态应用到系统中，使其成为当前状态的地方至关重要。
- en: For example, imagine that when the Hot ROD application discussed earlier in
    this chapter was initially deployed, it had relatively small operating requirements.
    However, over time, as the application becomes more popular, the dataset grows
    and the memory allocated to the Pod is no longer enough. The Pod periodically
    runs out of memory and is terminated (an event called “OOMKilled”). The application
    health of the Hot ROD application would show that it is periodically crashing
    and restarting. The system operator could check-in a change to the Git deployment
    repository for the Hot ROD application that increases its requested memory. The
    GitOps operator would then apply that change, increasing the memory of the running
    Hot ROD application.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，想象一下，当在本章前面讨论的 Hot ROD 应用最初部署时，它有相对较小的运行需求。然而，随着时间的推移，随着应用变得越来越受欢迎，数据集增长，分配给
    Pod 的内存不再足够。Pod 会定期耗尽内存并被终止（称为“OOMKilled”的事件）。Hot ROD 应用的应用健康状态会显示它定期崩溃和重启。系统操作员可以检查
    Hot ROD 应用的 Git 部署仓库中的更改，以增加其请求的内存。然后 GitOps 操作员将应用该更改，增加运行中的 Hot ROD 应用的内存。
- en: Perhaps we could leave things there. After all, someone committed a change to
    the system, and GitOps should just do what it’s been told. But what if the committed
    change actually makes things worse? What if the application comes back up after
    deploying the latest change but then starts returning more errors than it did
    before? Or even worse, what if it doesn’t come back up at all? What if, in this
    example, the operator increased the memory for the Hot ROD application too much,
    causing other applications running on the cluster to run out of memory?
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 也许我们可以就此结束。毕竟，有人对系统进行了更改，GitOps 应该只是按照指示行事。但假设提交的更改实际上使事情变得更糟呢？假设在部署最新更改后，应用重新启动但开始返回比之前更多的错误？或者更糟的是，假设它根本无法重新启动？在这个例子中，如果操作员为
    Hot ROD 应用增加了过多的内存，导致集群上运行的其他应用耗尽内存怎么办？
- en: With GitOps, we would at least like to detect those conditions where, after
    a deployment, the system is “worse” than before and, at a minimum, alert the users
    that perhaps they should consider rolling back the most recent change. This is
    only possible if the system and application have strong observability characteristics
    that the GitOps operator can, well, observe.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 使用GitOps，我们至少希望检测到那些在部署后系统“变得更糟”的条件，并且至少提醒用户他们可能应该考虑回滚最近的变化。这只有在系统和应用程序具有GitOps操作员可以观察到的强大可观察性特征的情况下才可能实现。
- en: 8.2.1 Resource status
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.1 资源状态
- en: At a basic level, a key feature of Kubernetes that enables observability of
    its internal state is how, with a declarative configuration, the desired configuration
    and active state are both maintained. This allows every Kubernetes resource to
    be inspected at any time to see whether or not the resource is in the desired
    state. Each component or resource will be in a particular operational state at
    any given time. For example, a resource might be in an `INITIALIZED` or `NOT READY`
    state. It also might be in a `PENDING`, `RUNNING`, or `COMPLETED` state. Often
    the status of a resource is specific to its type.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在基本层面上，Kubernetes的一个关键特性是它如何通过声明性配置同时维护所需的配置和活动状态，从而使其内部状态可观察。这允许在任何时候检查每个Kubernetes资源，以查看资源是否处于所需状态。每个组件或资源在任何给定时间都将处于特定的操作状态。例如，资源可能处于`INITIALIZED`或`NOT
    READY`状态。它也可能处于`PENDING`、`RUNNING`或`COMPLETED`状态。通常，资源的状态与其类型特定。
- en: The first aspect of application health is determining that all the Kubernetes
    resources related to the application are in a good state. For example, have all
    the Pods been successfully scheduled, and are they in a `Running` state? Let’s
    take a look at how this can be determined.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 应用程序健康的第一方面是确定与应用程序相关的所有Kubernetes资源都处于良好状态。例如，所有Pod是否都已成功调度，并且它们是否处于`Running`状态？让我们看看如何确定这一点。
- en: 'Kubernetes provides additional information for each Pod that indicates if a
    Pod is healthy or not. Let’s run the `kubectl describe` command on the `etcd`
    Pod:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes为每个Pod提供了额外的信息，以指示Pod是否健康。让我们在`etcd` Pod上运行`kubectl describe`命令：
- en: '[PRE16]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'This output has been truncated for brevity, but take a closer look at it and
    compare it to the output from your minikube. Do you see any properties of the
    Pod that may help with observability? One very important (and obvious) one near
    the top is `Status: Running`, which indicates the phase the Pod is in. The Pod
    phase is a simple, high-level summary of where the Pod is in its life cycle. The
    conditions array, the reason and message fields, and the individual container
    status arrays contain more detail about the Pod’s status.'
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: '这个输出已被截断以节省空间，但请仔细查看它并与您minikube的输出进行比较。您是否看到了Pod的任何可能有助于可观察性的属性？最上面一个非常重要（且明显）的一个是`Status:
    Running`，这表示Pod所处的阶段。Pod阶段是对Pod在其生命周期中位置的简单、高级总结。条件数组、原因和消息字段以及单个容器状态数组包含了关于Pod状态的更多详细信息。'
- en: Table 8.1 Pod phases
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 表8.1 Pod阶段
- en: '| Phase values | Description |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 阶段值 | 描述 |'
- en: '| `Pending` | The Kubernetes system has accepted the Pod, but one or more container
    images have not been created. This includes time before being scheduled as well
    as time spent downloading images over the network, which could take a while. |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| `Pending` | Kubernetes系统已接受Pod，但一个或多个容器镜像尚未创建。这包括在调度之前的时间以及通过网络下载镜像所花费的时间，这可能需要一段时间。|'
- en: '| `Running` | The Pod has been bound to a node, and all of the containers have
    been created. At least one container is still running or is in the process of
    starting or restarting. |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| `Running` | Pod已被绑定到节点，并且所有容器都已创建。至少有一个容器仍在运行或正在启动或重启过程中。|'
- en: '| `Succeeded` | All containers in the Pod have terminated in success and will
    not be restarted. |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| `Succeeded` | Pod中的所有容器都已成功终止，并且不会被重新启动。|'
- en: '| `Failed` | All containers in the Pod have completed, and at least one container
    has terminated in failure. The container either exited with non-zero status or
    was terminated by the system. |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| `Failed` | Pod中的所有容器都已完成，并且至少有一个容器已失败终止。容器要么以非零状态退出，要么被系统终止。|'
- en: '| `Unknown` | For some reason, the Pod state could not be obtained, typically
    due to an error in communicating with the Pod’s host. |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| `Unknown` | 由于某种原因，无法获取Pod状态，通常是由于与Pod的主机通信错误。|'
- en: 'This is observability: the internal state of a Pod can be easily queried so
    decisions can be made about controlling the system. For GitOps, if a new version
    of an application is deployed, it is important to look at the resulting new Pod’s
    state to ensure its success.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 这就是可观察性：Pod 的内部状态可以轻松查询，因此可以做出关于控制系统的决策。对于 GitOps 来说，如果部署了应用程序的新版本，查看由此产生的新
    Pod 的状态以确保其成功是非常重要的。
- en: But the Pod state (phase) is only a summary, and, to understand why a Pod is
    in a particular state, you need to look at the `Conditions`. A Pod has four conditions
    that will be `True`, `False`, or `Unknown`.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 但 Pod 状态（阶段）只是一个总结，要了解 Pod 为什么处于特定状态，你需要查看 `Conditions`。Pod 有四个条件，可以是 `True`、`False`
    或 `Unknown`。
- en: Table 8.2 Pod conditions
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 表 8.2 Pod 条件
- en: '| Phase values | Description |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 阶段值 | 描述 |'
- en: '| `PodScheduled` | The Pod has been successfully scheduled to a node in the
    cluster. |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| `PodScheduled` | Pod 已成功调度到集群中的某个节点。|'
- en: '| `Initialized` | All init containers have started successfully. |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| `Initialized` | 所有 init 容器都已成功启动。|'
- en: '| `ContainersReady` | All containers in the Pod are ready. |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| `ContainersReady` | Pod 中的所有容器都已就绪。|'
- en: '| `Ready` | The Pod can serve requests and should be added to the load-balancing
    pools of all matching Services. |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| `Ready` | Pod 可以提供服务，应该被添加到所有匹配服务的负载均衡池中。|'
- en: Exercise 8.6
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.6
- en: Use the `kubectl describe` command to display the information of other Pods
    running in the `kube-system` Namespace.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 `kubectl describe` 命令显示在 `kube-system` 命名空间中运行的其它 Pods 的信息。
- en: An elementary example of how a Pod may get into a bad state is submitting a
    Pod with a manifest that requests more resources than are available in the cluster.
    This will cause the Pod to go into a `Pending` state. The status of the Pod can
    be “observed” by running `kubectl describe pod <pod_name>`.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: Pod 可能进入不良状态的一个基本例子是提交一个请求比集群中可用的资源更多的资源的 Pod。这将导致 Pod 进入 `Pending` 状态。可以通过运行
    `kubectl describe pod <pod_name>` 来“观察”Pod 的状态。
- en: Listing 8.4 http://mng.bz/yYZG
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 8.4 http://mng.bz/yYZG
- en: '[PRE17]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: ❶ Requests an impossible amount of memory
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 请求了不可能的内存量
- en: ❷ Requests an impossible number of CPU cores
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 请求了不可能数量的 CPU 核心
- en: 'If you apply this YAML and check the Pod status, you will notice that the Pod
    is `Pending`. When you run `kubectl` `describe`, you will see that the Pod is
    in a `Pending` state since the minikube cluster can’t satisfy the resource request
    for 999 GB of RAM or the request for 99 CPUs:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你应用此 YAML 并检查 Pod 状态，你会注意到 Pod 是 `Pending` 状态。当你运行 `kubectl describe` 时，你会看到
    Pod 处于 `Pending` 状态，因为 minikube 集群无法满足 999 GB RAM 或 99 个 CPU 的请求：
- en: '[PRE18]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Pod status is Pending.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ Pod 状态是 Pending。
- en: ❷ PodScheduled condition is False.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ PodScheduled 条件为 False。
- en: ❸ No node is available with sufficient memory or CPU to schedule the Pod.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 没有足够的内存或 CPU 的节点可以调度 Pod。
- en: Exercise 8.7
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.7
- en: 'Update bigpod.yaml with a more reasonable request for CPU and memory, and redeploy
    the Pod. (Hint: Change CPU to `99m` and memory to `999Ki`.) Run `kubectl describe`
    on the updated Pod, and compare the output with the output before your changes.
    What are the `Status`, `Conditions`, and `Events` of the updated Pod?'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 使用更合理的 CPU 和内存请求更新 bigpod.yaml，并重新部署 Pod。（提示：将 CPU 更改为 `99m`，内存更改为 `999Ki`。）在更新的
    Pod 上运行 `kubectl describe`，并将输出与更改前的输出进行比较。更新的 Pod 的 `Status`、`Conditions` 和 `Events`
    是什么？
- en: 8.2.2 Readiness and liveness
  id: totrans-180
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.2 准备就绪和活跃性
- en: If you look carefully at the Pod `Conditions` listed in table 8.2, something
    might stand out. The `Ready` state claims that “the Pod can serve requests.” How
    does it know? What if the Pod has to perform some initialization? How does Kubernetes
    know that the Pod is ready? The answer is that the Pod itself notifies Kubernetes
    when it is ready based on its own application-specific logic.
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你仔细查看表 8.2 中列出的 Pod `Conditions`，可能会发现一些不同之处。`Ready` 状态声称“Pod 可以提供服务。”它是如何知道的？如果
    Pod 需要执行一些初始化怎么办？Kubernetes 如何知道 Pod 已经就绪？答案是 Pod 本身会根据其自身的应用特定逻辑通知 Kubernetes
    它已经就绪。
- en: Kubernetes uses readiness probes to decide when the Pod is available for accepting
    traffic. Each container in the Pod can specify a readiness probe, in the form
    of a command or an HTTP request, that will indicate when the container is `Ready`.
    It is up to the container to provide this observability about its internal state.
    Once all the Pod containers are `Ready`, then the Pod itself is considered `Ready`
    and can be added to load balancers of matching Services and begin handling requests.
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 使用就绪探针来决定何时 Pod 可用于接受流量。Pod 中的每个容器都可以指定一个就绪探针，形式为命令或 HTTP 请求，以指示容器何时为
    `Ready`。容器需要提供关于其内部状态的可观察性。一旦所有 Pod 容器都 `Ready`，则 Pod 本身被认为是 `Ready`，可以被添加到匹配服务的负载均衡器中，并开始处理请求。
- en: Similarly, each container can specify a liveness probe that indicates if the
    container is alive and not, for example, in some sort of deadlock situation. Kubernetes
    uses liveness probes to know when to restart a container that has entered a bad
    state.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，每个容器可以指定一个存活探针，以指示容器是否存活，例如，是否处于某种死锁状态。Kubernetes 使用存活探针来确定何时重启进入不良状态的容器。
- en: So here again is an aspect of observability that is built into Kubernetes. Application
    Pods provide visibility to their internal state through readiness and liveness
    probes so that the Kubernetes system can decide how to control them. Application
    developers must properly implement these probes so that the application provides
    the correct observability of its operation.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，这里再次提到了 Kubernetes 内置的可观察性的一个方面。应用程序 Pod 通过就绪和存活探针提供其内部状态的可视性，以便 Kubernetes
    系统可以决定如何控制它们。应用程序开发者必须正确实现这些探针，以便应用程序提供其操作的正确可观察性。
- en: '![](Images/CH08_F10_Yuen.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F10_Yuen.png)'
- en: Figure 8.10 Kubernetes uses readiness and liveness probes to determine which
    Pods are available to accept traffic. Pod 1 is in a `Running` state, and both
    the readiness and liveness probes are passing. Pod 2 is in a `Pending` state and,
    while the liveness probe is passing, the readiness probe is not since the Pod
    is still starting up. Pod 3 passes the readiness probe but fails the liveness
    probe, meaning it will likely soon be restarted by kubelet.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.10 Kubernetes 使用就绪和存活探针来确定哪些 Pod 可用于接受流量。Pod 1 处于 `Running` 状态，就绪和存活探针都通过。Pod
    2 处于 `Pending` 状态，尽管存活探针通过，但就绪探针未通过，因为 Pod 正在启动。Pod 3 通过就绪探针但未通过存活探针，这意味着它可能很快会被
    kubelet 重启。
- en: Exercise 8.8
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.8
- en: Create a Pod spec that uses an init container to create a file and configure
    the liveness and readiness probes of the app container to expect that the file
    exists. Create the Pod and then see its behavior. Exec into the Pod to delete
    the file and see its behavior.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 Pod 规范，使用初始化容器创建一个文件并配置应用程序容器的存活和就绪探针，以期望该文件存在。创建 Pod 后，观察其行为。进入 Pod 删除文件并观察其行为。
- en: 8.2.3 Application monitoring and alerting
  id: totrans-189
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.2.3 应用程序监控和警报
- en: 'In addition to status and readiness/liveness, applications typically have vital
    metrics that can be used to determine their overall health. This is the foundation
    of operational monitoring and alerting: watch a set of metrics and set off alarms
    when they deviate from allowable values. But what metrics should be monitored,
    and what are the permissible values?'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 除了状态和就绪/存活之外，应用程序通常有一些关键的指标可以用来确定它们的整体健康状况。这是运营监控和警报的基础：监视一组指标，并在它们偏离允许值时触发警报。但应该监控哪些指标，以及允许的值是什么？
- en: Fortunately, there has been a lot of research done on this topic, and it has
    been well covered in other books and articles. Rob Ewaschuk described the “four
    golden signals” as the most important metrics to focus on at a high level. This
    provides a useful framework for thinking about metrics:^([9](#pgfId-1091848))
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，已经对这个主题进行了大量研究，并在其他书籍和文章中得到了很好的覆盖。Rob Ewaschuk 将“四个黄金信号”描述为在高级别上最重要的指标。这提供了一个有用的框架来思考指标：^([9](#pgfId-1091848))
- en: '*Latency*—The time it takes to service a request'
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*延迟*—处理请求所需的时间'
- en: '*Traffic*—A measure of how much demand is placed on the system'
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*流量*—对系统需求的衡量'
- en: '*Errors*—The rate of requests that are not successful'
  id: totrans-194
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误*—未成功请求的比率'
- en: '*Saturation*—How “full” your service is'
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*饱和度*—你的服务“多满”'
- en: '![](Images/CH08_F11_Yuen.png)'
  id: totrans-196
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/CH08_F11_Yuen.png)'
- en: Figure 8.11 The four “golden signals,” latency, traffic, errors, and saturation,
    are the critical metrics to measure that indicate the system’s overall health.
    Each measures a specific operational aspect of the system. Any given problem in
    the system is likely to manifest itself by adversely impacting one or more of
    the “golden signal” metrics.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.11 四个“黄金信号”，延迟、流量、错误和饱和度，是衡量系统整体健康状况的关键指标。每个指标都衡量系统的一个特定操作方面。系统中的任何给定问题都可能通过负面影响一个或多个“黄金信号”指标来表现出来。
- en: Brendan Gregg proposed the USE Method for characterizing the performance of
    system resources (like infrastructure, such as Kubernetes nodes):^([10](#pgfId-1091871))
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 布伦丹·格雷格提出了用于描述系统资源性能（如基础设施，例如 Kubernetes 节点）的 USE 方法^([10](#pgfId-1091871))。
- en: '*Utilization*—The average time that the resource was busy servicing work'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*利用率*—资源忙于服务工作的平均时间'
- en: '*Saturation*—The degree to which the resource has extra work that it can’t
    service, often queued'
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*饱和度*—资源额外工作量的程度，通常排队'
- en: '*Errors*—The count of error events'
  id: totrans-201
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误*—错误事件的计数'
- en: For request-driven applications (like microservices), Tom Wilkie defined the
    RED Method:^([11](#pgfId-1091877))
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 对于请求驱动的应用程序（如微服务），汤姆·威尔基定义了 RED 方法^([11](#pgfId-1091877))。
- en: '*Rate*—The number of requests per second a service is processing'
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*速率*—服务每秒处理的请求数量'
- en: '*Errors*—The number of failed requests per second'
  id: totrans-204
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*错误*—每秒失败的请求数量'
- en: '*Duration*—Distributions of the amount of time each request takes'
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*持续时间*—每个请求所需时间的分布'
- en: While a more in-depth discussion of determining application health through metrics
    is beyond this book’s scope, we highly recommend reading the three related footnoted
    references summarized here.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然通过指标确定应用程序健康状况的更深入讨论超出了本书的范围，但我们强烈推荐阅读这里总结的三个相关脚注参考文献。
- en: Once you have identified the metrics to evaluate application health, you need
    to monitor them and generate alerts when they fall outside of allowable values.
    In traditional operational environments, this would typically be done by a human
    operator staring at a dashboard, but perhaps by an automated system. If the monitoring
    detects an issue, an alert is raised, triggering the on-call engineer to look
    at the system. The on-call engineer analyzes the system and determines the correct
    course of action to resolve the alert. This might be to stop the rollout of a
    new release to the fleet of servers or perhaps even roll back to the previous
    version.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦你确定了评估应用程序健康状况的指标，你需要监控它们，并在它们超出允许值时生成警报。在传统的运营环境中，这通常由一个人类操作员盯着仪表板来完成，但可能由一个自动化的系统来完成。如果监控检测到问题，就会发出警报，触发值班工程师检查系统。值班工程师分析系统并确定解决警报的正确行动方案。这可能包括停止向服务器群集推出新版本，或者甚至回滚到上一个版本。
- en: All this takes time and delays the recovery of the system back to an optimal
    running state. What if the GitOps system could help improve this situation?
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些都需要时间，并延迟了系统恢复到最佳运行状态。如果 GitOps 系统能帮助改善这种情况呢？
- en: Consider the case where all the Pods come up successfully. All the readiness
    checks are successful, but once the application begins processing, the length
    of time required to process each request suddenly increases by two times (the
    duration of the RED method). It may be that a recent code change introduced a
    performance bug that is degrading the performance of the application.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑所有 Pod 都成功启动的情况。所有就绪性检查都成功了，但一旦应用程序开始处理，处理每个请求所需的时间突然增加了两倍（RED 方法的持续时间）。可能是因为最近的代码更改引入了一个性能错误，这正在降低应用程序的性能。
- en: Ideally, such a performance issue should be caught while testing in preproduction.
    If not, might it be possible for the GitOps operator and deployment mechanism
    to automatically stop or roll back the deployment if particular golden-signal
    metrics suddenly become degraded and deviate from an established baseline?
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 理想情况下，这样的性能问题应该在预生产测试期间被发现。如果没有，GitOps 运营商和部署机制是否可以自动停止或回滚部署，如果特定的黄金信号指标突然恶化并偏离既定的基线？
- en: This is covered in more detail as part of a discussion of advanced observability
    use cases in section 8.3.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 这在 8.3 节中作为高级可观测性用例讨论的一部分进行了更详细的说明。
- en: 8.3 GitOps observability
  id: totrans-212
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 8.3 GitOps 可观测性
- en: Often administrators will change the system configuration defined in GitOps
    based on observed application health characteristics. If a Pod is stuck in a `CrashLoopBackoff`
    state due to an out-of-memory condition, the Pod’s manifest may be updated to
    request more memory for the Pod. If a memory leak causes the out-of-memory condition
    in the application, perhaps the Pod’s image will be updated to one that contains
    a fix to the memory leak. Maybe the golden signals of the application indicate
    that it is reaching saturation and cannot handle the load, so the Pod manifest
    may be updated to request more CPU, or the number of replicas of the Pod is increased
    to horizontally scale the application.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 经常管理员会根据观察到的应用程序健康特征更改GitOps中定义的系统配置。如果一个Pod由于内存不足条件而卡在`CrashLoopBackoff`状态，Pod的清单可能被更新以请求更多内存。如果内存泄漏导致应用程序中的内存不足条件，也许Pod的镜像将被更新为包含修复内存泄漏的镜像。也许应用程序的黄金信号表明它正在达到饱和状态，无法处理负载，因此Pod清单可能被更新以请求更多CPU，或者Pod副本的数量增加以水平扩展应用程序。
- en: These are all GitOps operations that would be taken based on the observability
    of the application. But what about the GitOps process itself? What are the observable
    characteristics of a GitOps deployment?
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是基于应用程序的可观察性而采取的GitOps操作。但GitOps过程本身呢？GitOps部署的可观察特征是什么？
- en: 8.3.1 GitOps metrics
  id: totrans-215
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.1 GitOps指标
- en: 'If the GitOps operator or service is an application, what are its golden signals?
    Let’s consider each area to understand some of the observability characteristics
    provided by the GitOps operator:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 如果GitOps操作员或服务是一个应用程序，它的黄金信号是什么？让我们考虑每个领域，以了解GitOps操作员提供的某些可观察性特征：
- en: Latency
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 延迟
- en: The time it takes to deploy and make the system’s running state match its desired
    state
  id: totrans-218
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署系统并使其运行状态与期望状态匹配所需的时间
- en: Traffic
  id: totrans-219
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 流量
- en: Frequency of deployments
  id: totrans-220
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署频率
- en: The number of deployments in progress
  id: totrans-221
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正在进行的部署数量
- en: Errors
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 错误
- en: The number of deployments that failed and the current number of deployments
    in a failed state
  id: totrans-223
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 失败的部署数量以及当前处于失败状态的部署数量
- en: The number of out-of-sync deployments where the system’s running state does
    not match its desired state
  id: totrans-224
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 系统运行状态与期望状态不匹配的离线部署数量
- en: Saturation
  id: totrans-225
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 饱和度
- en: Length of time a deployment has been queued and not processed
  id: totrans-226
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 部署排队等待处理的时间长度
- en: '![](Images/CH08_F12_Yuen.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F12_Yuen.png)'
- en: Figure 8.12 The four golden signals for GitOps indicate the health of the GitOps
    continuous deployment system. Any issue with the GitOps operator/Service will
    likely manifest itself by adversely impacting one or more of these golden- signal
    metrics.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 图8.12 GitOps的四个黄金信号表示GitOps持续部署系统的健康状况。GitOps操作员/服务的问题可能会通过负面影响一个或多个这些黄金信号指标来体现。
- en: The implementation of each of these metrics and how they are exposed will be
    different for each GitOps tool. This will be covered in more detail in part 3
    of this book.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 这些指标的实现方式以及它们如何被公开将因每个GitOps工具而异。这将在本书的第三部分中更详细地介绍。
- en: 8.3.2 Application sync status
  id: totrans-230
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.2 应用程序同步状态
- en: The most important status the GitOps operator must provide is whether the desired
    state of the application in the Git repo is the same as the current state of the
    application (in sync) or not (out of sync). If the application is out of sync,
    the user should be alerted that a deployment (or redeployment) may be needed.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: GitOps操作员必须提供的最重要状态是Git仓库中应用程序的期望状态是否与当前应用程序状态（同步）相同，或者不同（不同步）。如果应用程序不同步，用户应被提醒可能需要进行部署（或重新部署）。
- en: But what would cause an application to become out of sync? This is part of the
    regular operation of GitOps; a user commits a change to the desired state of the
    system. At the moment that change is committed, the current state of the application
    doesn’t match the desired state.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 但是什么原因导致应用程序变得不同步？这是GitOps的常规操作的一部分；用户将更改提交到系统的期望状态。在更改提交的那一刻，应用程序的当前状态与期望状态不匹配。
- en: Let’s consider how the Basic GitOps operator described in section 2.5.1 of chapter
    2 functioned. In that example, a CronJob periodically runs the basic GitOps operator
    so that the repository is checked out, and the manifests contained in the repo
    are automatically applied to the running system.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们考虑第2章2.5.1节中描述的基本GitOps操作员是如何工作的。在那个例子中，CronJob定期运行基本GitOps操作员，以便检出仓库，并将仓库中包含的清单自动应用到运行系统中。
- en: In this basic example of a GitOps operator, the question of application sync
    status is entirely sidestepped for the sake of simplifying the example. The basic
    GitOps operator assumed that the application was out of sync at every scheduled
    execution (or polling interval) and needed to be deployed. This simplistic approach
    is not suitable for real-world production use because the user has no visibility
    into whether changes exist that need to be deployed or what those changes are.
    It might also add unnecessary additional load on the GitOps operation, Git server,
    and Kubernetes API server.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个 GitOps 运算符的基本示例中，为了简化示例，完全回避了应用程序同步状态的问题。基本的 GitOps 运算符假设在每次计划执行（或轮询间隔）时应用程序都处于不同步状态，需要部署。这种简单的方法不适合实际生产使用，因为用户无法看到是否存在需要部署的更改，以及这些更改是什么。这也可能给
    GitOps 操作、Git 服务器和 Kubernetes API 服务器增加不必要的额外负载。
- en: Sample application
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 样本应用程序
- en: Let’s run through several different deployment scenarios with our `sample-app`
    to explore other Application sync status aspects. The `sample-app` is a simple
    Go application that returns an HTTP response message of “`Kubernetes ♡ <input>`”.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过几个不同的部署场景来运行我们的 `sample-app`，以探索其他应用程序同步状态方面。`sample-app` 是一个简单的 Go 应用程序，它返回一个
    HTTP 响应消息“`Kubernetes ♡ <input>`”。
- en: First, log in to GitHub and fork the [https://github.com/gitopsbook/sample-app-deployment](https://github.com/gitopsbook/sample-app-deployment)
    repository. If you have previously forked this repo, it’s recommended to delete
    the old fork and refork to be sure to start with a clean repository without any
    changes from previous exercises.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，登录到 GitHub 并分叉 [https://github.com/gitopsbook/sample-app-deployment](https://github.com/gitopsbook/sample-app-deployment)
    仓库。如果您之前已分叉此仓库，建议删除旧的分叉并重新分叉，以确保从一个没有任何先前练习更改的干净仓库开始。
- en: 'After forking, clone your fork repository using the following command:'
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 在分叉后，使用以下命令克隆您的分叉仓库：
- en: '[PRE19]'
  id: totrans-239
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Let’s manually deploy the `sample-app` to minikube:'
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们手动将 `sample-app` 部署到 minikube：
- en: '[PRE20]'
  id: totrans-241
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: NOTE At the time of this writing, Kubernetes v1.18.3 reports additional differences
    using the `kubectl` `diff` command. If you experience this issue while completing
    the following exercises, you can start minikube with an older version of Kubernetes
    using the command `minikube start --kubernetes-version=1.16.10`.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：在撰写本文时，Kubernetes v1.18.3 使用 `kubectl diff` 命令报告了额外的差异。如果在完成以下练习时遇到此问题，您可以使用以下命令以较旧版本的
    Kubernetes 启动 minikube：`minikube start --kubernetes-version=1.16.10`。
- en: Detecting differences
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 检测差异
- en: 'Now that the `sample-app` has been successfully deployed, let’s make some changes
    to the deployment. Let’s increase the number of replicas of the `sample-app` to
    3 in the deployment.yaml file. Use the following command to change the replica
    count of the Deployment resource:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，`sample-app` 已成功部署，让我们对部署做一些更改。让我们在 deployment.yaml 文件中将 `sample-app` 的副本数量增加到
    3。使用以下命令更改 Deployment 资源中的副本数：
- en: '[PRE21]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Or you could use your favorite text editor to change `replicas:` `1` to `replicas:`
    `3` in the deployment.yaml file. Once the change has been made to deployment.yaml,
    run the following command to see the uncommitted differences in your Git fork
    repository:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，您可以使用您喜欢的文本编辑器将 deployment.yaml 文件中的 `replicas:` `1` 更改为 `replicas:` `3`。一旦对
    deployment.yaml 进行了更改，运行以下命令以查看 Git 分支仓库中的未提交差异：
- en: '[PRE22]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'Finally, use `git commit` and `git push` to push changes to the remote Git
    fork repository:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，使用 `git commit` 和 `git push` 将更改推送到远程 Git 分支仓库：
- en: '[PRE23]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Now that you have committed a change to the `sample-app` deployment repo, the
    `sample-app` GitOps sync status is out of sync because the current state of the
    deployment still only has three replicas. Let’s just confirm that this is the
    case:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，您已将更改提交到 `sample-app` 部署仓库，由于部署的当前状态仍然只有三个副本，`sample-app` 的 GitOps 同步状态处于不同步状态。让我们确认这一点：
- en: '[PRE24]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: From this command, you see there are `1/1` `READY` replicas for the `sample-app`
    deployment. But is there a better way to compare the deployment repo, which represents
    the desired state, against the running application’s actual state? Luckily Kubernetes
    provides tools for detecting differences.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 从此命令中，您可以看到 `sample-app` 部署有 `1/1` 个 `READY` 副本。但有没有更好的方法来比较表示所需状态的部署仓库与运行应用程序的实际状态？幸运的是，Kubernetes
    提供了检测差异的工具。
- en: kubectl diff
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: kubectl diff
- en: 'Kubernetes provides the `kubectl` `diff` command, which takes a file or directory
    as input and displays the differences between the resources defined in those files
    and the current resources of the same names in the Kubernetes cluster. If we run
    `kubectl diff` against our existing deployment repo, we see the following:'
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes 提供了 `kubectl diff` 命令，它接受一个文件或目录作为输入，并显示那些文件中定义的资源与 Kubernetes 集群中同名的当前资源之间的差异。如果我们对现有的部署仓库运行
    `kubectl diff`，我们会看到以下内容：
- en: '[PRE25]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: From this output, we can see that `kubectl` `diff` correctly identified that
    `replicas` was changed from `1` to `3`.
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以看到 `kubectl diff` 正确地识别出 `replicas` 从 `1` 更改为 `3`。
- en: While this is an elementary example to illustrate the point, this same technique
    can identify more extensive changes across multiple different resources. This
    gives the GitOps operator or service the ability to determine when the deployment
    repo containing the desired state in Git is out of sync with the current live
    state of the Kubernetes cluster. More importantly, the `kubectl` `diff` output
    provides a preview of the changes that would be applied to the cluster if the
    deployment repo is synced. This is a crucial feature of GitOps observability.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然这是一个基本的例子，用于说明这个观点，但同样的技术可以识别多个不同资源之间的更广泛的变化。这赋予了 GitOps 运营商或服务确定 Git 中包含所需状态的部署仓库是否与
    Kubernetes 集群的当前实时状态不同步的能力。更重要的是，`kubectl diff` 输出提供了如果部署仓库同步，将应用到集群中的更改的预览。这是
    GitOps 可观察性的一个关键特性。
- en: Exercise 8.9
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.9
- en: Make a fork of the `sample-app` deployment repository. Deploy the `sample-app`
    to your minikube cluster as described in the `sample-app-deployment` README.md.
    Now change the `sample-app` service to be of `type:` `LoadBalancer`. Run the `kubectl`
    `diff` `-f` `.` `-n` `sample-app` command. Do you see any unexpected changes?
    Why? Apply the changes using `kubectl` `apply` `-f` `.` `-n` `sample-app`. Now
    you should see the `sample-app` web page using the command `minikube` `service`
    `sample-app` `-n sample-app`.
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 将 `sample-app` 部署仓库进行分支。按照 `sample-app-deployment` README.md 中的说明将 `sample-app`
    部署到您的 minikube 集群。现在将 `sample-app` 服务的 `type:` 改为 `LoadBalancer`。运行 `kubectl diff
    `-f` `.` `-n` `sample-app` 命令。您是否看到了任何意外的更改？为什么？使用 `kubectl apply `-f` `.` `-n`
    `sample-app` 应用更改。现在您应该可以通过命令 `minikube service sample-app `-n sample-app` 来看到
    `sample-app` 网页。
- en: kubediff
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: kubediff
- en: In the previous sections, we covered how to see the differences between revisions
    in the Git repository using `git` `diff` and the differences between the Git repository
    and the live Kubernetes cluster using `kubectl diff`. In both cases, the diff
    tools give you a very raw view of the differences, outputting lines before and
    after the differences for context. And `kubectl` `diff` may also report differences
    that are system managed (like `generation`) and not relevant to the GitOps use
    case. Wouldn’t it be cool if there was a tool that gave you a concise report of
    each resource’s specific attributes that are different? As it turns out, the folks
    at Weaveworks^([12](#pgfId-1092053)) have released an open source tool called
    `kubediff`^([13](#pgfId-1092057)) that does precisely that.
  id: totrans-261
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们介绍了如何使用 `git` `diff` 来查看 Git 仓库中修订之间的差异，以及如何使用 `kubectl diff` 来查看
    Git 仓库与实时 Kubernetes 集群之间的差异。在两种情况下，diff 工具都提供了非常原始的差异视图，输出差异前后的行以提供上下文。而且 `kubectl
    diff` 也可能报告系统管理（如 `generation`）的差异，这些差异与 GitOps 用例不相关。如果有一个工具能给出每个资源特定属性差异的简洁报告，那岂不是很好？事实上，Weaveworks^([12](#pgfId-1092053))
    的人们已经发布了一个名为 `kubediff`^([13](#pgfId-1092057)) 的开源工具，它正是这样做的。
- en: 'Here is the output of `kubediff` run against our deployment repo of the `sample-app`:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是针对我们的 `sample-app` 部署仓库运行 `kubediff` 的输出：
- en: '[PRE26]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '`kubediff` can also output JSON structured output, allowing it to be more easily
    used programmatically. Here is the same command run with JSON output:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: '`kubediff` 还可以输出 JSON 结构化的输出，这使得它更容易被程序化使用。以下是使用 JSON 输出运行的相同命令：'
- en: '[PRE27]'
  id: totrans-265
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: Exercise 8.10
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 8.10
- en: Run `kubediff` against the `sample-app-deployment` repository. If not installed
    already in your environment, you will first need to install Python and `pip` and
    run `pip install` `-r` `requirements.txt` as described in the `kubediff` README.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 对 `sample-app-deployment` 仓库运行 `kubediff`。如果您的环境中尚未安装，您首先需要安装 Python 和 `pip`，然后按照
    `kubediff` README 中的说明运行 `pip install` `-r` `requirements.txt`。
- en: 8.3.3 Configuration drift
  id: totrans-268
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.3 配置漂移
- en: But how else could an application become out of sync with the desired state
    defined in the Git repo? Possibly a user modified the running application directly
    (like by performing a `kubectl edit` on the Deployment resource) instead of committing
    the desired change to the Git repo. We call this configuration drift.
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 但应用程序如何才能与Git仓库中定义的期望状态不同步呢？可能是用户直接修改了正在运行的应用程序（例如，通过在Deployment资源上执行`kubectl
    edit`），而没有将期望的更改提交到Git仓库。我们称这种情况为配置漂移。
- en: This is usually a big “no-no” when managing a system with GitOps; you should
    avoid directly modifying the system outside of GitOps. For example, if your Pods
    are running out of capacity, you might just simply perform a `kubectl` `edit`
    to increase the replica count to increase capacity.
  id: totrans-270
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用GitOps管理系统时，这通常是一个大忌；你应该避免在GitOps之外直接修改系统。例如，如果你的Pod资源不足，你可能只是简单地执行一个`kubectl
    edit`命令来增加副本数量以增加容量。
- en: This situation sometimes happens. When it does, the GitOps operator will need
    to “observe” the current state and detect a difference with the desired state
    and indicate to the user that the application is out of sync. A particularly aggressive
    GitOps operator might automatically redeploy the last previously deployed configuration,
    thereby overwriting the manual changes.
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: 这种情况有时会发生。当这种情况发生时，GitOps操作员需要“观察”当前状态，检测与期望状态的差异，并向用户指示应用程序不同步。一个特别激进的GitOps操作员可能会自动重新部署之前部署的最后配置，从而覆盖手动更改。
- en: Using minikube and the `sample-app-deployment` repository we’ve been using for
    the last few sections, run `kubectl` `apply` `-f` `.` `-n` `sample-app` to make
    sure the current contents are deployed to Kubernetes. Now run `kubectl` `diff`
    `-f` `.` `-n` `sample-app`; you should see no differences.
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 使用minikube和我们在上一节中使用的`sample-app-deployment`仓库，运行`kubectl apply -f . -n sample-app`以确保当前内容已部署到Kubernetes。现在运行`kubectl
    diff -f . -n sample-app`；你应该看不到任何差异。
- en: 'Now, let’s simulate a change being made to the application deployment outside
    of the GitOps system by running the following command:'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过运行以下命令来模拟在GitOps系统之外对应用程序部署进行更改：
- en: '[PRE28]'
  id: totrans-274
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now, if we rerun the `kubectl` `diff` command, we see that the application
    is out of sync, and we have experienced configuration drift:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，如果我们重新运行`kubectl diff`命令，我们会看到应用程序不同步，我们已经经历了配置漂移：
- en: '[PRE29]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: 'Or if you run `kubediff`, you see the following:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，如果你运行`kubediff`，你会看到以下内容：
- en: '[PRE30]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Configuration drift is very similar to an application being out of sync. In
    fact, as you see, the effect is the same; the current live state of the configuration
    is different from the desired configuration as defined in the Git deployment repo.
    The difference is that typically an application is out of sync when a new version
    is committed to the Git deployment repo that hasn’t been deployed yet. In contrast,
    configuration drift happens when changes to the configuration have been made outside
    of the GitOps system.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 配置漂移与应用程序不同步非常相似。事实上，正如你所看到的，效果是相同的；当前配置的实时状态与在Git部署仓库中定义的期望配置不同。区别在于，通常当Git部署仓库中提交了尚未部署的新版本时，应用程序会不同步。相比之下，配置漂移发生在配置更改在GitOps系统之外进行时。
- en: In general, one of two things must occur when configuration drift is encountered.
    Some systems would consider the configuration drift an error state and allow a
    self-healing process to be initiated to sync the system back to the declared state.
    Other systems may detect this drift and allow the manual change to be integrated
    back into the declared state saved in Git (such as two-way syncing). However,
    our view is that two-way syncing is not desirable because it allows and encourages
    manual changes to the cluster and bypasses the security and review process that
    GitOps provides as one of its core benefits.
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，当遇到配置漂移时，必须发生以下两种情况之一。一些系统会将配置漂移视为错误状态，并允许启动自愈过程以同步系统回到声明的状态。其他系统可能会检测到这种漂移，并允许手动更改被整合回保存在Git中的声明状态（例如双向同步）。然而，我们的观点是双向同步并不可取，因为它允许并鼓励手动更改集群，绕过了GitOps提供作为其核心优势之一的安保和审查流程。
- en: Exercise 8.11
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.11
- en: 'From the `sample-app-deployment`, run the command `kubectl` `delete` `-f` `.`
    `-n` `sample-app`. Oops, you just deleted your application! Run `kubectl` `diff`
    `-f` `.` `-n` `sample-app`. What differences do you see? How can you restore your
    application to a running state? Hint: it should be easy.'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `sample-app-deployment` 中运行命令 `kubectl delete -f . -n sample-app`。哎呀，你刚刚删除了你的应用程序！运行
    `kubectl diff -f . -n sample-app`。你看到了什么差异？你如何将应用程序恢复到运行状态？提示：这应该很容易。
- en: 8.3.4 GitOps change log
  id: totrans-283
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 8.3.4 GitOps 变更日志
- en: Earlier in this chapter, we discussed how event logs are a key aspect of observability.
    For GitOps, the “event log” of the application deployment is primarily composed
    of the deployment repository’s commit history. Since all changes to the application
    deployment are made by changing the files representing the application’s desired
    state, by observing the commits, pull request approvals, and merge requests, we
    can understand what changes have been made in the cluster.
  id: totrans-284
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章前面，我们讨论了事件日志是可观察性的一个关键方面。对于 GitOps 来说，应用程序部署的“事件日志”主要是由部署仓库的提交历史组成的。由于所有对应用程序部署的更改都是通过更改表示应用程序期望状态的文件来进行的，通过观察提交、拉取请求批准和合并请求，我们可以了解集群中发生了哪些更改。
- en: 'For example, running the `git` `log` command on the `sample-app-deployment`
    repository displays all the commits made to this repo since it was created:'
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在 `sample-app-deployment` 仓库上运行 `git log` 命令会显示自创建以来对这个仓库所做的所有提交：
- en: '[PRE31]'
  id: totrans-286
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'From this output, we can see Alex authored the first commit to this repo on
    Jan 26\. The most recent commit was authored by Todd, which, according to the
    commit title, reduces the Replica count back to one. We can look at the actual
    differences in the commit by running the following command:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个输出中，我们可以看到 Alex 在 1 月 26 日创建了该仓库的第一个提交。最近的提交是由 Todd 完成的，根据提交标题，将副本数量减少到 1。我们可以通过运行以下命令来查看提交的实际差异：
- en: '[PRE32]'
  id: totrans-288
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'From this we see that the line `replicas:` `3` was changed to `replicas: 1`.'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: '从这里我们可以看到，行 `replicas:` `3` 被更改为 `replicas: 1`。'
- en: The same information is available in the GitHub UI.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 相同的信息也可在 GitHub UI 中找到。
- en: '![](Images/CH08_F13_Yuen.png)'
  id: totrans-291
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F13_Yuen.png)'
- en: Figure 8.13 Reviewing the GitHub commit history of a deployment repository allows
    you to see all the changes that have been made to the application deployment over
    time.
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.13 通过审查部署仓库的 GitHub 提交历史，可以看到应用程序部署随时间所做的所有更改。
- en: '![](Images/CH08_F14_Yuen.png)'
  id: totrans-293
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/CH08_F14_Yuen.png)'
- en: Figure 8.14 The detail of individual commits can be inspected.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 图 8.14 可以检查单个提交的详细信息。
- en: Having a deployment audit log for each of your applications deployed to your
    cluster is critical for managing them at scale. If only one person changes a cluster,
    if something breaks, likely that person will know what change they may have made
    that caused it to break. But if you have multiple teams, perhaps in different
    geographic locations and time zones, it’s imperative to be able to answer the
    question “Who deployed the app last, and what changes did they make?”
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 为你集群中部署的每个应用程序拥有一个部署审计日志对于大规模管理它们至关重要。如果只有一个人更改集群，如果出现问题，那个人很可能会知道他们可能做了什么更改导致了问题。但是，如果你有多个团队，可能在不同的地理位置和时间区域，能够回答“谁最后部署了应用程序，他们做了什么更改？”这个问题是至关重要的。
- en: As we’ve learned in previous chapters, the GitOps repository is a collection
    of changes, additions, and deletions to the repository files representing the
    desired state of the system. Each modification to the repository is known as a
    commit. Just as application logging helps provide the history of what occurred
    in the code, Git provides a log that provides the history of changes to the repository.
    We can examine the Git log to observe the changes that have occurred to the repository
    over time.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在前面的章节中学到的，GitOps 仓库是一系列对表示系统期望状态的仓库文件的更改、添加和删除的集合。对仓库的每一次修改都称为一个提交。就像应用程序日志有助于提供代码中发生的历史记录一样，Git
    提供了一个日志，提供了仓库更改的历史记录。我们可以检查 Git 日志来观察仓库随时间发生的变化。
- en: For GitOps, looking at the deployment repository logs is just as important as
    looking at the application logs when it comes to the observability of the GitOps
    system. Because the GitOps repository is the source of truth for the desired state
    of the system, examining the logs of the repository shows us when and why (if
    the commit comment is descriptive enough) changes were made to the desired state
    of the system, as well as who made and approved those changes.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 对于GitOps来说，查看部署存储库的日志与查看应用日志一样重要，因为GitOps存储库是系统期望状态的真相来源。检查存储库的日志显示我们何时以及为什么（如果提交注释足够描述性）对系统的期望状态进行了更改，以及谁进行了更改并批准了这些更改。
- en: This is a critically important aspect of how GitOps provides observability to
    users. While the GitOps operator or service may also provide application logs
    detailing its execution, the Git log of the deployment repository usually will
    give you an excellent understanding of what changes have been going on in the
    system.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 这是GitOps向用户提供可观察性的一个关键方面。虽然GitOps操作员或服务也可能提供详细说明其执行的日志，但部署存储库的Git日志通常能给你一个很好的理解，了解系统中发生了哪些变化。
- en: Exercise 8.12
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 练习8.12
- en: Using the same Git fork of the `sample-app-deployment` repository you’ve been
    using throughout this chapter, run the command `git` `log`. Examine the output.
    Can you trace your process through the sections of this chapter? Do you see any
    earlier commits to this repo by the authors?
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 使用你在这章中一直使用的`sample-app-deployment`存储库的相同Git分叉，运行`git log`命令。检查输出。你能通过本章的各个部分追踪你的过程吗？你看到作者对这个存储库的任何早期提交吗？
- en: Summary
  id: totrans-301
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Aspects of observability can be measured by monitoring event logging, metrics,
    and tracing.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可观察性的方面可以通过监控事件日志、指标和跟踪来衡量。
- en: A data collector such as Logstash, Fluentd, or Scribe can collect the application
    output (events) in stdout and store the log messages in a centralized data store
    for later analysis.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据收集器，如Logstash、Fluentd或Scribe，可以收集应用输出（事件），并将日志消息存储在集中式数据存储中，以供后续分析。
- en: Observe the application output using `kubectl logs.`
  id: totrans-304
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl logs`观察应用输出。
- en: Prometheus collects metrics from both nodes and Pods to provide a snapshot of
    the performance and operation of all system components.
  id: totrans-305
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Prometheus从节点和Pod收集指标，以提供所有系统组件性能和操作的快照。
- en: Use Jaeger (Open Tracing) to monitor distributed calls to gain system insight
    such as error and latency.
  id: totrans-306
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用Jaeger（Open Tracing）来监控分布式调用，以获得系统洞察，如错误和延迟。
- en: 'Application health: is the application operating correctly? If a new version
    of the application is deploying using GitOps, is the system “better” than before?'
  id: totrans-307
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用健康状态：应用是否正常运行？如果应用的新版本正在使用GitOps进行部署，系统是否“更好”于之前？
- en: Use `kubectl describe` to monitor application health.
  id: totrans-308
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl describe`来监控应用健康状态。
- en: 'Application sync status: is the running state of the application the same as
    the desired state defined in the deployment Git repo?'
  id: totrans-309
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用同步状态：应用的运行状态是否与部署Git存储库中定义的期望状态相同？
- en: 'Configuration drift: has the application’s configuration been changed outside
    of the declarative GitOps system (such as either manually or imperatively)?'
  id: totrans-310
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 配置漂移：应用配置是否在声明式GitOps系统之外（例如手动或强制）被更改？
- en: Use `kubectl diff` and `kubediff` to detect Application sync status and configuration
    drift
  id: totrans-311
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用`kubectl diff`和`kubediff`检测应用同步状态和配置漂移
- en: 'GitOps change log: what changes were recently made to the GitOps system? Who
    made them, and for what reason?'
  id: totrans-312
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: GitOps变更日志：最近对GitOps系统做了哪些更改？谁做的，为什么？
- en: '* * *'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: 1.[https://github.com/cncf/sig-observability](https://github.com/cncf/sig-observability).
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 1.[https://github.com/cncf/sig-observability](https://github.com/cncf/sig-observability).
- en: 2.[http://mng.bz/4Z6a](https://shortener.manning.com/4Z6a).
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 2.[http://mng.bz/4Z6a](https://shortener.manning.com/4Z6a).
- en: 3.[https://kubernetes.io/docs/concepts/cluster-administration/logging/](https://kubernetes.io/docs/concepts/cluster-administration/logging/).
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: 3.[https://kubernetes.io/docs/concepts/cluster-administration/logging/](https://kubernetes.io/docs/concepts/cluster-administration/logging/).
- en: 4.[https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack).
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 4.[https://www.elastic.co/what-is/elk-stack](https://www.elastic.co/what-is/elk-stack).
- en: 5.[https://www.splunk.com/](https://www.splunk.com/).
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 5.[https://www.splunk.com/](https://www.splunk.com/).
- en: 6.[https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod](https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod).
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 6.[https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod](https://github.com/jaegertracing/jaeger/tree/master/examples/hotrod).
- en: 7.[http://mng.bz/XdqG](http://mng.bz/XdqG).
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 7.[http://mng.bz/XdqG](http://mng.bz/XdqG).
- en: 8.[https://minikube.sigs.k8s.io/docs/tasks/addons/](https://minikube.sigs.k8s.io/docs/tasks/addons/).
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 8.[https://minikube.sigs.k8s.io/docs/tasks/addons/](https://minikube.sigs.k8s.io/docs/tasks/addons/).
- en: 9.[https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/).
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 9.[https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/](https://landing.google.com/sre/sre-book/chapters/monitoring-distributed-systems/).
- en: 10.[http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html).
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 10.[http://www.brendangregg.com/usemethod.html](http://www.brendangregg.com/usemethod.html).
- en: 11.[http://mng.bz/MX97](http://mng.bz/MX97).
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 11.[http://mng.bz/MX97](http://mng.bz/MX97).
- en: 12.[https://www.weave.works/](https://www.weave.works/).
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 12.[https://www.weave.works/](https://www.weave.works/).
- en: 13.[https://github.com/weaveworks/kubediff](https://github.com/weaveworks/kubediff).
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 13.[https://github.com/weaveworks/kubediff](https://github.com/weaveworks/kubediff).
