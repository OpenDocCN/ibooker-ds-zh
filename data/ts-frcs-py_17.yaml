- en: 14 Baby steps with deep learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 14 深度学习的初步尝试
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Implementing linear models
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现线性模型
- en: Enacting deep neural networks
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实施深度神经网络
- en: In the last chapter, we implemented the `DataWindow` class, which allows us
    to quickly create windows of data for building single-step models, multi-step
    models, and multi-output models. With this crucial component in place, we then
    developed the baseline models that will serve as benchmarks for our more complex
    models, which we’ll start building in this chapter.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们实现了`DataWindow`类，它允许我们快速创建用于构建单步模型、多步模型和多输出模型的数据窗口。有了这个关键组件，我们随后开发了将作为我们更复杂模型基准的基线模型，我们将在本章开始构建这些模型。
- en: Specifically, we’ll implement linear models and deep neural networks. A *linear
    model* is a special case of a neural network, where there is no hidden layer.
    This model simply calculates weights for each input variable in order to output
    a prediction for the target. In contrast, a *deep neural network* has at least
    one hidden layer, allowing us to start modeling nonlinear relationships between
    the features and the target, usually resulting in better forecasts.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 具体来说，我们将实现线性模型和深度神经网络。**线性模型**是神经网络的一种特殊情况，其中没有隐藏层。该模型简单地计算每个输入变量的权重，以便输出对目标的预测。相比之下，**深度神经网络**至少有一个隐藏层，这使得我们可以开始对特征和目标之间的非线性关系进行建模，通常导致更好的预测。
- en: In this chapter, we’ll continue the work we started in chapter 13\. I recommend
    that you continue coding in the same notebook or Python scripts as in the last
    chapter, so that you can compare the performance of these linear models and deep
    neural networks to that of the baseline models from chapter 13\. We’ll also keep
    working with the same dataset as previously, and our target variable will remain
    the traffic volume for both the single-step and multi-step models. For the multi-output
    model, we’ll keep the temperature and traffic volume as our targets.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将继续第13章开始的工作。我建议你继续使用与上一章相同的笔记本或Python脚本进行编码，这样你就可以比较这些线性模型和深度神经网络与第13章中的基线模型的性能。我们还将继续使用之前相同的同一数据集，我们的目标变量将保持为单步和多步模型中的交通流量。对于多输出模型，我们将保持温度和交通流量作为我们的目标。
- en: 14.1 Implementing a linear model
  id: totrans-7
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.1 实现线性模型
- en: A *linear model* is the simplest architecture we can implement in deep learning.
    In fact, we might argue that it is not deep learning at all, since the model has
    no hidden layer. Each input feature is simply given a weight, and they are combined
    to output a prediction for the target, just like in a traditional linear regression.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: '**线性模型**是我们可以在深度学习中实现的 simplest 架构。实际上，我们可能会争论这根本不是深度学习，因为模型没有隐藏层。每个输入特征只是被赋予一个权重，然后它们被组合起来输出对目标的预测，就像在传统的线性回归中一样。'
- en: 'Let’s consider a single-step model as an example. Recall that we have the following
    features in our dataset: temperature, cloud coverage, traffic volume, and `day_sin`
    and `day_cos`, which encode the time of day as numerical values. A linear model
    simply takes all the features, calculates a weight for each of them, and sums
    them to output a prediction for the next timestep. This process is illustrated
    in figure 14.1.'
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们以单步模型为例。回想一下，在我们的数据集中有以下特征：温度、云量、交通流量以及`day_sin`和`day_cos`，它们将一天中的时间编码为数值。线性模型简单地接受所有特征，为每个特征计算一个权重，并将它们相加以输出对下一个时间步的预测。这个过程在图14.1中得到了说明。
- en: '![](../../OEBPS/Images/14-01.png)'
  id: totrans-10
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/14-01.png)'
- en: Figure 14.1 An example of a linear model as a single-step model. Each feature
    at time *t* is assigned a weight (*w*[1] to *w*[5]). They are then summed to calculate
    an output for the traffic volume at the next timestep, *t*+1\. This is similar
    to a linear regression.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1 线性模型作为单步模型的示例。在时间*t*的每个特征都被分配一个权重(*w*[1]到*w*[5])。然后它们被相加以计算下一个时间步交通流量*t*+1的输出。这类似于线性回归。
- en: The model in figure 14.1 can be mathematically expressed as equation 14.1, where
    *x*[1] is cloud coverage, *x*[2] is temperature, *x*[3] is traffic volume, *x*[4]
    is `day_sin`, and *x*[5] is `day_cos`.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.1中的模型可以用方程14.1表示，其中*x*[1]是云量，*x*[2]是温度，*x*[3]是交通流量，*x*[4]是`day_sin`，*x*[5]是`day_cos`。
- en: traffic volume[*t*+1] = *w*[1]*x*[1,*t*] + *w*[2]*x*[2,*t*] + *w*[3]*x*[3,*t*]+
    *w*[4]*x*[4,*t*] + *w*[5]*x*[5,*t*]
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 交通流量[*t*+1] = *w*[1]*x*[1,*t*] + *w*[2]*x*[2,*t*] + *w*[3]*x*[3,*t*]+ *w*[4]*x*[4,*t*]
    + *w*[5]*x*[5,*t*]
- en: Equation 14.1
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 方程14.1
- en: We can easily recognize equation 14.1 as being a simple multivariate linear
    regression. During training, the model tries multiple values for *w*[1] to *w*[5]
    in order to minimize the mean squared error (MSE) between the prediction and actual
    value of the traffic volume at the next timestep.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以很容易地识别出方程14.1是一个简单的多元线性回归。在训练过程中，模型尝试多个值来最小化预测值和下一个时间步交通量实际值之间的均方误差（MSE）。
- en: Now that you understand the concept of a linear model in deep learning, let’s
    implement it as a single-step model, multi-step model, and multi-output model.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你已经理解了深度学习中线性模型的概念，让我们将其实现为一个单步模型、多步模型和多输出模型。
- en: 14.1.1 Implementing a single-step linear model
  id: totrans-17
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.1 实现单步线性模型
- en: A single-step linear model is one of the simplest models to implement, as it
    is exactly as described in figure 14.1 and equation 14.1\. We simply take all
    the inputs, assign a weight to each, take the sum, and generate a prediction.
    Remember that we are using the traffic volume as a target.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 单步线性模型是实施起来最简单的模型之一，因为它正好如第14.1图和第14.1方程所描述的那样。我们只需取所有输入，为每个输入分配一个权重，求和，然后生成一个预测。记住，我们正在使用交通量作为目标。
- en: Assuming that you are working in the same notebook or Python script as in the
    last chapter, you should have access to the `single_step_window` for training
    and `wide_window` for plotting. Recall also that the performance of the baseline
    is stored in `val_performance` and `performance`.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 假设你正在使用与上一章相同的笔记本或Python脚本，你应该可以访问用于训练的`single_step_window`和用于绘图的`wide_window`。还要记住，基线性能存储在`val_performance`和`performance`中。
- en: Unlike a baseline model, a linear model actually requires training. Thus, we’ll
    define a `compile_and_fit` function that configures the model for training and
    then fits the model on the data, as shown in the following listing.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 与基线模型不同，线性模型实际上需要训练。因此，我们将定义一个`compile_and_fit`函数，该函数配置模型以进行训练，然后根据以下列表将模型拟合到数据上。
- en: 'Note You can consult the source code for this chapter on GitHub: [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14).'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：你可以查阅GitHub上本章的源代码：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14)。
- en: Listing 14.1 Function to configure a deep learning model and fit it on data
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 列表14.1 配置深度学习模型并在数据上拟合它的函数
- en: '[PRE0]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ The function takes a model and a window of data from the DataWindow class.
    The patience is the number of epochs after which the model should stop training
    if the validation loss does not improve; max_epochs sets a maximum number of epochs
    to train the model.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 该函数接受一个模型和数据窗口类的一个数据窗口。耐心参数表示在验证损失没有改善后模型应该停止训练的epoch数；max_epochs设置训练模型的最大epoch数。
- en: ❷ The validation loss is tracked to determine if we should apply early stopping
    or not.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 跟踪验证损失以确定是否应该应用早期停止。
- en: ❸ Early stopping occurs if 3 consecutive epochs do not decrease the validation
    loss, as set by the patience parameter.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果连续3个epoch没有降低验证损失，则发生早期停止，这是由耐心参数设置的。
- en: ❹ The MSE is used as the loss function.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 使用均方误差（MSE）作为损失函数。
- en: ❺ The MAE is used as an error metric. This is how we compare the performance
    of our models. A lower MAE means a better model.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 使用MAE作为误差度量。这是我们比较模型性能的方式。MAE越低，模型越好。
- en: ❻ The model is fit on the training set.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 模型在训练集上拟合。
- en: ❼ The model can train for at most 50 epochs, as set by the max_epochs parameter.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 模型最多可以训练50个epoch，这是由max_epochs参数设置的。
- en: ❽ We use the validation set to calculate the validation loss.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 我们使用验证集来计算验证损失。
- en: ❾ early_stopping is passed as a callback. If the validation loss does not decrease
    after 3 consecutive epochs, the model stops training. This avoids overfitting.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ early_stopping作为回调传递。如果在连续3个epoch后验证损失没有降低，则模型停止训练。这避免了过拟合。
- en: This piece of code will be reused throughout the deep learning chapters, so
    it’s important to understand what is happening. The `compile_and_fit` function
    takes in a deep learning model, a window of data from the `DataWindow` class,
    the `patience` parameter, and the `max_epochs` parameter. The `patience` parameter
    is used in the `early_stopping` function, which allows us to stop the model from
    training if there are no improvements in the validation loss, as specified by
    the `monitor` parameter. That way, we avoid useless training time and overfitting.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码将在深度学习章节中反复使用，因此理解其工作原理非常重要。`compile_and_fit` 函数接收一个深度学习模型、来自 `DataWindow`
    类的数据窗口、`patience` 参数和 `max_epochs` 参数。`patience` 参数用于 `early_stopping` 函数，它允许我们在验证损失没有根据
    `monitor` 参数指定的条件改善时停止模型的训练。这样，我们避免了无用的训练时间并防止过拟合。
- en: Then the model is compiled. In Keras, this simply configures the model to specify
    the loss function to be used, the optimizer, and metrics of evaluation. In our
    case, we’ll use the MSE as the loss function because the error is squared, meaning
    that the model is heavily penalized for large differences between the predicted
    and actual values. We’ll use the Adam optimizer because it is a fast and efficient
    optimizer. Finally, we’ll use the MAE as an evaluation metric to compare the performance
    of our models because we used it to evaluate our baseline models in the previous
    chapter, and it is easy to interpret.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 然后编译模型。在 Keras 中，这仅仅是配置模型以指定要使用的损失函数、优化器和评估指标。在我们的案例中，我们将使用均方误差（MSE）作为损失函数，因为误差是平方的，这意味着模型对预测值和实际值之间的大差异进行了重罚。我们将使用
    Adam 优化器，因为它是一种快速高效的优化器。最后，我们将使用平均绝对误差（MAE）作为评估指标来比较我们模型的性能，因为我们已经在上一章中使用它来评估我们的基线模型，并且它很容易解释。
- en: The model is then fit on the training data for up to 50 epochs, as set by the
    `max_ epochs` parameter. The validation is performed on the validation set, and
    we pass in `early_stopping` as a callback. That way, Keras will apply early stopping
    if it sees that the validation loss has not decreased after 3 consecutive epochs.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 模型将在训练数据上拟合，最多 50 个周期，这是由 `max_epochs` 参数设置的。验证在验证集上执行，我们传递 `early_stopping`
    作为回调。这样，如果 Keras 在连续 3 个周期后看到验证损失没有下降，它将应用早停。
- en: 'With `compile_and_fit` in place, we can move on to actually building our linear
    model. We’ll use the `Sequential` model from Keras, as it allows us to stack different
    layers. Since we are building a linear model here, we only have one layer—a `Dense`
    layer, which is the most basic layer in deep learning. We’ll specify the number
    of units as 1, since the model must output only one value: the prediction for
    traffic volume at the next timestep.'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `compile_and_fit` 准备就绪后，我们可以继续构建我们的线性模型。我们将使用 Keras 的 `Sequential` 模型，因为它允许我们堆叠不同的层。由于我们在这里构建的是线性模型，所以我们只有一个层——一个
    `Dense` 层，这是深度学习中最基本的层。我们将指定单元数为 1，因为模型必须输出仅一个值：下一个时间步的交通量预测。
- en: '[PRE1]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Clearly, Keras makes it very easy to build models. With this step complete,
    we can then train the model using `compile_and_fit` and store the performance
    to later compare it to the baseline.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 显然，Keras 使得构建模型变得非常容易。完成这一步后，我们可以使用 `compile_and_fit` 训练模型，并将性能存储起来以供稍后与基线比较。
- en: '[PRE2]'
  id: totrans-39
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Optionally, we can visualize the predictions of our linear model using the `plot`
    method of the `wide_window`. The result is shown in figure 14.2.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 可选地，我们可以使用 `wide_window` 的 `plot` 方法可视化我们线性模型的预测。结果如图 14.2 所示。
- en: '[PRE3]'
  id: totrans-41
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '![](../../OEBPS/Images/14-02.png)'
  id: totrans-42
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/14-02.png)'
- en: Figure 14.2 Predictions of traffic volume using the linear model as a single-step
    model. The predictions (shown as crosses) are fairly accurate, with some predictions
    overlapping the actual values (shown as squares).
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.2 使用线性模型作为单步模型预测交通量的结果。预测值（以交叉表示）相当准确，一些预测值与实际值（以正方形表示）重叠。
- en: Our model makes fairly good predictions, as we can observe some overlap between
    the forecasts and the actual values. We will wait until the end of the chapter
    to compare the performance of our models to the baselines. For now, let’s move
    on to implementing the multi-step linear and multi-output linear models.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的模型做出了相当好的预测，因为我们可以在预测值和实际值之间观察到一些重叠。我们将等到本章的末尾来比较我们模型的性能与基线。现在，让我们继续实现多步线性模型和多输出线性模型。
- en: 14.1.2 Implementing a multi-step linear model
  id: totrans-45
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.2 实现多步线性模型
- en: Our single-step linear model is built, and we can now extend it to a multi-step
    linear model. Recall that in the multi-step situation, we wish to predict the
    next 24 hours of data using an input window of 24 hours of data. Our target remains
    the traffic volume.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的单步线性模型已经构建，现在我们可以将其扩展为多步线性模型。回想一下，在多步情况下，我们希望使用24小时的数据窗口来预测接下来的24小时的数据。我们的目标仍然是交通量。
- en: This model will greatly resemble the single-step linear model, but this time
    we’ll use 24 hours of input and output 24 hours of predictions. The multi-step
    linear model is illustrated in figure 14.3\. As you can see, the model takes in
    24 hours of each feature, combines them in a single layer, and outputs a tensor
    containing the forecast for the next 24 hours.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 这个模型将非常类似于单步线性模型，但这次我们将使用24小时的输入并输出24小时的预测。多步线性模型如图14.3所示。如图所示，模型接受每个特征的24小时数据，将它们在一个单独的层中组合，并输出一个包含下一个24小时预测的张量。
- en: '![](../../OEBPS/Images/14-03.png)'
  id: totrans-48
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/14-03.png)'
- en: Figure 14.3 The multi-step linear model. We’ll take 24 hours of each feature,
    combine them in a single layer, and immediately output predictions for the next
    24 hours.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.3 多步线性模型。我们将取每个特征的24小时数据，将它们在一个单独的层中组合，并立即输出预测下一个24小时的预测。
- en: Implementing the model is easy, as our model only contains a single `Dense`
    layer. We can optionally initialize the weights to 0, which makes the training
    procedure slightly faster. We then compile and fit the model before storing its
    evaluation metrics in `ms_val_performance` and `ms_performance`.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 实现模型很简单，因为我们的模型只包含一个`Dense`层。我们可以选择性地将权重初始化为0，这会使训练过程稍微快一些。然后我们编译并拟合模型，在`ms_val_performance`和`ms_performance`中存储其评估指标。
- en: '[PRE4]'
  id: totrans-51
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: ❶ Initializing the weights to 0 makes training slightly faster.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将权重初始化为0会使训练稍微快一些。
- en: We have just built a multi-step linear model. You might feel underwhelmed, since
    the code is almost identical to the single-step linear model. This is due to our
    work building the `DataWindow` class and properly windowing our data. With that
    step done, building models becomes extremely easy.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 我们刚刚构建了一个多步线性模型。你可能会觉得有些失望，因为代码几乎与单步线性模型相同。这是由于我们构建了`DataWindow`类并正确地窗口化我们的数据。完成这一步后，构建模型变得极其容易。
- en: Next we’ll implement a multi-output linear model.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将实现一个多输出线性模型。
- en: 14.1.3 Implementing a multi-output linear model
  id: totrans-55
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.1.3 实现多输出线性模型
- en: The multi-output linear model will return predictions for the traffic volume
    and the temperature. The input is the present timestep, and the predictions are
    for the next timestep.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 多输出线性模型将返回交通量和温度的预测。输入是当前时间步，预测是下一个时间步。
- en: The model’s architecture is shown in figure 14.4\. There, you can see that our
    multi-output linear model will take all the features at *t* = 0, combine them
    in a single layer, and output both the temperature and traffic volume at the next
    timestep.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型的架构如图14.4所示。在那里，你可以看到我们的多输出线性模型将接受所有在*t* = 0的特征，将它们在一个单独的层中组合，并在下一个时间步输出温度和交通量。
- en: '![](../../OEBPS/Images/14-04.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](../../OEBPS/Images/14-04.png)'
- en: Figure 14.4 A multi-output linear model. In this case, the model takes the present
    timestep of all features and produces a forecast for the temperature and traffic
    volume at the next timestep.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.4 多输出线性模型。在这种情况下，模型接受所有特征的当前时间步，并预测下一个时间步的温度和交通量。
- en: Up to this point, we have only predicted the traffic volume, meaning that we
    had only one target, so we used the layer `Dense(units=1)`. In this case, since
    we must output a prediction for two targets, our layer will be `Dense(units=2)`.
    As before, we’ll train the model and store its performance to compare it later
    to the baseline and deep neural network.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们只预测了交通量，这意味着我们只有一个目标，所以我们使用了`Dense(units=1)`层。在这种情况下，由于我们必须为两个目标输出预测，我们的层将是`Dense(units=2)`。像以前一样，我们将训练模型并将性能存储起来，以便稍后与基线和深度神经网络进行比较。
- en: '[PRE5]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ We set units equal to the number of targets we are predicting in the output
    layer.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 我们将输出层的预测目标数量设置为单元数。
- en: Again, you can see how easy it is to build a deep learning model in Keras, especially
    when we have the proper data window as input.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 再次，你可以看到在Keras中构建深度学习模型是多么容易，尤其是在我们有适当的数据窗口作为输入时。
- en: 'With our single-step, multi-step, and multi-output linear models done, we can
    now move on to implementing a more complex architecture: a deep neural network.'
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的单步、多步和多输出线性模型完成后，我们现在可以继续实现一个更复杂的架构：深度神经网络。
- en: 14.2 Implementing a deep neural network
  id: totrans-65
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.2 实现深度神经网络
- en: With our three types of linear models implemented, it is time to move on to
    deep neural networks. It has been empirically shown that adding hidden layers
    in neural networks helps achieve better results. Furthermore, we’ll introduce
    a nonlinear activation function to capture nonlinear relationships in the data.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现了三种类型的线性模型之后，现在是时候转向深度神经网络了。经验表明，在神经网络中添加隐藏层有助于实现更好的结果。此外，我们将引入非线性激活函数来捕捉数据中的非线性关系。
- en: Linear models have no hidden layers; the model had an input layer and an output
    layer. In a deep neural network (DNN), we’ll add more layers between the input
    and output layers, called *hidden layers*. This difference in architecture is
    highlighted in figure 14.5.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 线性模型没有隐藏层；模型有一个输入层和一个输出层。在深度神经网络（DNN）中，我们将在输入层和输出层之间添加更多层，称为*隐藏层*。这种架构上的差异在图14.5中得到了强调。
- en: '![](../../OEBPS/Images/14-05.png)'
  id: totrans-68
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/14-05.png)'
- en: Figure 14.5 Comparing a linear model to a deep neural network. In the linear
    model, the input layer is directly connected to an output layer that returns a
    prediction. Therefore, only a linear relationship is derived. A deep neural network
    contains hidden layers. These layers allow it to model nonlinear relationships
    between inputs and predictions, generally resulting in better models.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.5 比较线性模型和深度神经网络。在线性模型中，输入层直接连接到输出层，输出层返回一个预测。因此，只推导出线性关系。深度神经网络包含隐藏层。这些层允许它建模输入和预测之间的非线性关系，通常导致更好的模型。
- en: The idea behind adding layers to the network is that it gives the model more
    opportunities to learn, which usually results in the model generalizing better
    on unseen data, thus improving its performance. Of course, with added layers,
    the model necessarily trains for a longer time and is thus supposed to learn better.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在网络中添加层的想法是它给模型提供了更多的学习机会，这通常会导致模型在未见过的数据上更好地泛化，从而提高其性能。当然，随着层的增加，模型必然需要更长时间来训练，因此应该学习得更好。
- en: Each circle in a hidden layer represents a neuron, and each neuron has an activation
    function. The number of neurons is equal to the number of `units` that is passed
    as an argument in the `Dense` layer in Keras. Usually we set the number of units,
    or neurons, as a power of 2, as it is more computationally efficient—calculations
    in the CPU and GPU happen in batch sizes that are also powers of 2.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 隐藏层中的每个圆圈代表一个神经元，每个神经元都有一个激活函数。神经元的数量等于在Keras的`Dense`层中作为参数传递的`units`的数量。通常，我们将单元数或神经元数设置为2的幂，因为这样更高效——CPU和GPU的计算是在2的幂大小的批次中发生的。
- en: Before implementing a DNN, we need to address the *activation function* in each
    neuron of the hidden layers. The activation function defines the output of each
    neuron based on the input. Therefore, if we wish to model nonlinear relationships,
    we need to use a nonlinear activation function.
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 在实现深度神经网络之前，我们需要解决隐藏层中每个神经元的*激活函数*问题。激活函数根据输入定义每个神经元的输出。因此，如果我们希望建模非线性关系，我们需要使用非线性激活函数。
- en: Activation function
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数
- en: The activation function is in each neuron of the neural network and is responsible
    for generating an output from the input data.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 激活函数位于神经网络中的每个神经元，并负责从输入数据生成输出。
- en: If a linear activation function is used, the model will only model linear relationships.
    Therefore, to model nonlinear relationships in the data, we must use a nonlinear
    activation function. Examples of nonlinear activation functions are ReLU, softmax,
    or tanh.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 如果使用线性激活函数，模型将只建模线性关系。因此，为了在数据中建模非线性关系，我们必须使用非线性激活函数。非线性激活函数的例子有ReLU、softmax或tanh。
- en: In our case, we’ll use the Rectified Linear Unit (ReLU) activation function.
    This nonlinear activation function basically returns either the positive part
    of its input or 0, as defined by equation 14.2.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的例子中，我们将使用修正线性单元（ReLU）激活函数。这个非线性激活函数基本上返回其输入的正部分或0，如方程14.2所定义。
- en: f(*x*) = *x*^+ = max (0, *x*)
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: f(*x*) = *x*^+ = max (0, *x*)
- en: Equation 14.2
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 方程14.2
- en: This activation function comes with many advantages, such as better gradient
    propagation, more efficient computation, and scale-invariance. For all those reasons,
    it is now the most widely used activation function in deep learning, and we’ll
    use it whenever we have a `Dense` layer that is a hidden layer.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 这种激活函数具有许多优点，例如更好的梯度传播、更高效的计算和尺度不变性。由于所有这些原因，它现在是深度学习中应用最广泛的激活函数，并且我们将在有 `Dense`
    层作为隐藏层时使用它。
- en: We are now ready to implement a deep neural network in Keras.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好在 Keras 中实现深度神经网络。
- en: 14.2.1 Implementing a deep neural network as a single-step model
  id: totrans-81
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.1 将深度神经网络实现为单步模型
- en: We are now back to the single-step model, but this time we’ll implement a deep
    neural network. The DNN takes in the features at the current timestep to output
    the prediction for traffic volume at the next timestep.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在回到单步模型，但这次我们将实现一个深度神经网络。DNN 接收当前时间步的特征，以输出下一个时间步的交通量预测。
- en: The model still makes use of the `Sequential` model, as we’ll stack `Dense`
    layers in order to build a deep neural network. In this case, we’ll use two hidden
    layers with 64 neurons each. As mentioned before, we’ll specify the activation
    function to be ReLU. The last layer is the output layer, which in this case only
    returns one value representing the prediction for traffic volume.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 模型仍然使用 `Sequential` 模型，我们将按顺序堆叠 `Dense` 层来构建深度神经网络。在这种情况下，我们将使用两个每个有 64 个神经元的隐藏层。如前所述，我们将指定激活函数为
    ReLU。最后一层是输出层，在这种情况下，它只返回一个值，表示交通量的预测。
- en: '[PRE6]'
  id: totrans-84
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ First hidden layer with 64 neurons. Specify the activation function to be
    ReLU.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 第一个隐藏层有 64 个神经元。指定激活函数为 ReLU。
- en: ❷ The output layer has only one neuron, as we output only one value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 输出层只有一个神经元，因为我们只输出一个值。
- en: With the model defined, we can now compile it, train it, and record its performance
    to compare it to the baseline and the linear model.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 模型定义完成后，我们现在可以编译它、训练它，并记录其性能，以便与基线模型和线性模型进行比较。
- en: '[PRE7]'
  id: totrans-88
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Of course, we can take a look at the model’s predictions using the `plot` method,
    as shown in figure 14.6\. Our deep neural network seems to be making quite accurate
    predictions.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，我们可以通过 `plot` 方法查看模型的预测，如图 14.6 所示。我们的深度神经网络似乎正在做出相当准确的预测。
- en: '![](../../OEBPS/Images/14-06.png)'
  id: totrans-90
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.6](../../OEBPS/Images/14-06.png)'
- en: Figure 14.6 Predicting the traffic volume using a deep neural network as a single-step
    model. Here even more predictions (shown as crosses) overlap with the actual values
    (shown as squares), suggesting that the model is making very accurate predictions.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.6 使用深度神经网络作为单步模型预测交通量。这里更多的预测（以交叉表示）与实际值（以正方形表示）重叠，表明模型正在做出非常准确的预测。
- en: Let’s compare the MAE of the DNN with the linear model and the baseline that
    we built in chapter 13\. The result is shown in figure 14.7.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们比较 DNN 与我们在第 13 章中构建的线性模型和基线的 MAE。结果如图 14.7 所示。
- en: '[PRE8]'
  id: totrans-93
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: '![](../../OEBPS/Images/14-07.png)'
  id: totrans-94
  prefs: []
  type: TYPE_IMG
  zh: '![图 14.7](../../OEBPS/Images/14-07.png)'
- en: Figure 14.7 The MAE for all of the single-step models so far. The linear model
    performs better than the baseline, which only predicts the last known value. The
    dense model outperforms both models, since it has the lowest MAE.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 图 14.7 所有单步模型的平均绝对误差（MAE）。线性模型的表现优于基线模型，后者仅预测最后一个已知值。密集模型的表现优于这两个模型，因为它具有最低的
    MAE。
- en: In figure 14.7 the MAE is highest for the baseline. It decreases with the linear
    model and decreases again with the deep neural network. Thus, both models outperformed
    the baseline, with the deep neural network having the best performance.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 14.7 中，基线的 MAE 最高。随着线性模型和深度神经网络的使用，MAE 逐渐降低。因此，这两个模型都优于基线，深度神经网络的表现最好。
- en: 14.2.2 Implementing a deep neural network as a multi-step model
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.2 将深度神经网络实现为多步模型
- en: Now let’s implement a deep neural network as a multi-step model. In this case,
    we want to predict the next 24 hours of traffic volume based on the last 24 hours
    of recorded data.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将深度神经网络实现为多步模型。在这种情况下，我们希望根据最后 24 小时的记录数据预测接下来的 24 小时的交通量。
- en: Again we’ll use two hidden layers with 64 neurons each, and we’ll use the ReLU
    activation function. Since we have a data window with 24 hours of input, the model
    will also output 24 hours of predictions; the output layer simply has one neuron
    because we are predicting traffic volume only.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将再次使用两个隐藏层，每个隐藏层有 64 个神经元，并且我们将使用 ReLU 激活函数。由于我们有 24 小时输入的数据窗口，模型也将输出 24 小时的预测；输出层只有一个神经元，因为我们只预测交通量。
- en: '[PRE9]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Then we’ll compile, train the model, and save its performance for comparison
    with the linear and baseline models.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们将编译、训练模型，并保存其性能，以便与线性模型和基线模型进行比较。
- en: '[PRE10]'
  id: totrans-102
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Just like that, we have built a multi-step deep neural network model. Let’s
    see which model performed best for the multi-step task. The result is shown in
    figure 14.8.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 就像那样，我们已经构建了一个多步深度神经网络模型。让我们看看哪个模型在多步任务中表现最佳。结果如图14.8所示。
- en: '[PRE11]'
  id: totrans-104
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '![](../../OEBPS/Images/14-08.png)'
  id: totrans-105
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/14-08.png)'
- en: Figure 14.8 The MAE for all of the multi-step models so far. The linear model
    performs better than both baselines. The dense model outperforms all models.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.8至今为止所有多步模型的MAE。线性模型的表现优于两个基线模型。密集模型优于所有模型。
- en: In figure 14.8 you’ll see that the linear model and deep neural network both
    outperform the two baselines that we built for the multi-step task in chapter
    13\. Again, the deep neural network has the lowest MAE of all, meaning that it
    is the most performant model for now.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在图14.8中，你会看到线性模型和深度神经网络都优于我们在第13章为多步任务构建的两个基线模型。同样，深度神经网络具有最低的MAE，意味着它是目前表现最好的模型。
- en: 14.2.3 Implementing a deep neural network as a multi-output model
  id: totrans-108
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 14.2.3 将深度神经网络实现为多输出模型
- en: Finally, we’ll implement a deep neural network as a multi-output model. In this
    case, we’ll use the features at the present timestep to forecast both the traffic
    volume and temperature at the next timestep.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们将实现一个深度神经网络作为多输出模型。在这种情况下，我们将使用当前时间步的特征来预测下一个时间步的交通量和温度。
- en: As for the previous DNNs that we implemented, we’ll use two hidden layers of
    64 neurons each. This time, because we are forecasting two targets, our output
    layer has two neurons or `units`.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们之前实现的DNNs，我们将使用每个隐藏层64个神经元的结构。这次，因为我们预测两个目标，所以我们的输出层有两个神经元或`units`。
- en: '[PRE12]'
  id: totrans-111
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ The output layer has two neurons, since we are forecasting two targets.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 输出层有两个神经元，因为我们正在预测两个目标。
- en: Next we’ll compile and fit the model and store its performance for comparison.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将编译和拟合模型，并存储其性能以供比较。
- en: '[PRE13]'
  id: totrans-114
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Let’s see which model performed best at the multi-output task. Note that the
    reported MAE is averaged for both targets.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看哪个模型在多输出任务中表现最佳。请注意，报告的MAE是针对两个目标平均的。
- en: '[PRE14]'
  id: totrans-116
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: As you can see in figure 14.9, our models outperform the baseline, with the
    deep learning model being the most performant.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你在图14.9中可以看到的，我们的模型优于基线模型，其中深度学习模型表现最为出色。
- en: '![](../../OEBPS/Images/14-09.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../../OEBPS/Images/14-09.png)'
- en: Figure 14.9 The MAE for all of the multi-output models built so far. Again,
    the baseline has the highest MAE, while the deep neural network achieves the lowest
    error metric.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图14.9至今为止所有多输出模型的MAE。同样，基线模型具有最高的MAE，而深度神经网络实现了最低的错误指标。
- en: 14.3 Next steps
  id: totrans-120
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.3 下一步
- en: In this chapter, we implemented both linear models and deep neural networks
    to make single-step, multi-step, and multi-output predictions. In all cases, the
    deep neural network outperformed the other models. This is generally the case,
    as DNNs can map nonlinear relationships between the features and the targets,
    which generally leads to more accurate predictions.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们实现了线性模型和深度神经网络，以进行单步、多步和多输出预测。在所有情况下，深度神经网络都优于其他模型。这通常是情况，因为DNNs可以映射特征和目标之间的非线性关系，这通常会导致更准确的预测。
- en: 'This chapter only brushed the surface of what deep learning can achieve in
    time series forecasting. In the next chapter, we’ll explore a more complex architecture:
    the *long short-term memory* (LSTM). This architecture is widely used to process
    sequences of data. Since a time series is a sequence of points equally spaced
    in time, it makes sense to apply an LSTM for time series forecasting. We will
    then test whether the LSTM outperforms the DNN or not.'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 本章仅对深度学习在时间序列预测中可以实现的表面进行了探讨。在下一章中，我们将探索一个更复杂的架构：*长短期记忆*（LSTM）。这种架构广泛用于处理数据序列。由于时间序列是时间上等间距的点序列，因此对时间序列预测应用LSTM是有意义的。然后我们将测试LSTM是否优于DNN。
- en: 14.4 Exercises
  id: totrans-123
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 14.4 练习
- en: 'In the last chapter, as an exercise, you built baseline models to forecast
    the concentration of NO[2] and temperature. Now you’ll build linear models and
    deep neural networks. The full solutions to these exercises are available on GitHub:
    [https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14).'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，作为练习，你构建了基线模型来预测NO[2]的浓度和温度。现在你将构建线性模型和深度神经网络。这些练习的完整解决方案可在GitHub上找到：[https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14](https://github.com/marcopeix/TimeSeriesForecastingInPython/tree/master/CH13%26CH14)。
- en: 'For the single-step model:'
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于单步模型：
- en: Build a linear model.
  id: totrans-126
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个线性模型。
- en: Plot its predictions.
  id: totrans-127
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the mean absolute error (MAE) and store it.
  id: totrans-128
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用平均绝对误差（MAE）来衡量其性能并存储它。
- en: Build a deep neural network (DNN).
  id: totrans-129
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个深度神经网络（DNN）。
- en: Plot its predictions.
  id: totrans-130
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the MAE and store it.
  id: totrans-131
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE来衡量其性能并存储它。
- en: Which model performs best?
  id: totrans-132
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型表现最好？
- en: 'For the multi-step model:'
  id: totrans-133
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多步模型：
- en: Build a linear model.
  id: totrans-134
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个线性模型。
- en: Plot its predictions.
  id: totrans-135
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the MAE and store it.
  id: totrans-136
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE（均方误差）来衡量其性能并存储它。
- en: Build a DNN.
  id: totrans-137
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个深度神经网络（DNN）。
- en: Plot its predictions.
  id: totrans-138
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the MAE and store it.
  id: totrans-139
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE来衡量其性能并存储它。
- en: Which model performs best?
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型表现最好？
- en: 'For the multi-output model:'
  id: totrans-141
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于多输出模型：
- en: Build a linear model.
  id: totrans-142
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个线性模型。
- en: Plot its predictions.
  id: totrans-143
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the MAE and store it.
  id: totrans-144
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE来衡量其性能并存储它。
- en: Build a DNN.
  id: totrans-145
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 构建一个深度神经网络（DNN）。
- en: Plot its predictions.
  id: totrans-146
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 绘制其预测结果。
- en: Measure its performance using the MAE and store it.
  id: totrans-147
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用MAE来衡量其性能并存储它。
- en: Which model performs best?
  id: totrans-148
  prefs:
  - PREF_IND
  - PREF_OL
  type: TYPE_NORMAL
  zh: 哪个模型表现最好？
- en: At any point, feel free to run your own experiments with the deep neural networks.
    Add layers, change the number of neurons, and see how those changes impact the
    performance of the model.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在任何时刻，都可以自由运行自己的深度神经网络实验。添加层，更改神经元数量，并观察这些变化如何影响模型性能。
- en: Summary
  id: totrans-150
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: A linear model is the simplest architecture in deep learning. It has an input
    layer and an output layer, with no activation function.
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型是深度学习中 simplest 的架构。它有一个输入层和一个输出层，没有激活函数。
- en: A linear model can only derive linear relationships between the features and
    the target.
  id: totrans-152
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性模型只能推导出特征和目标之间的线性关系。
- en: A deep neural network (DNN) has hidden layers, which are layers between the
    input and output layers. Adding more layers usually improves the performance of
    the model, as it allows it more time to train and learn the data.
  id: totrans-153
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度神经网络（DNN）有隐藏层，这些层位于输入层和输出层之间。增加更多层通常可以提高模型性能，因为它允许模型有更多时间进行训练和学习数据。
- en: To model nonlinear relationships from the data, you must use a nonlinear activation
    function in the network. Examples of nonlinear activation functions are ReLU,
    softmax, tanh, sigmoid, etc.
  id: totrans-154
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要从数据中建模非线性关系，必须在网络中使用非线性激活函数。非线性激活函数的例子有ReLU、softmax、tanh、sigmoid等。
- en: The number of neurons in a hidden layer is usually a power of 2, to make computation
    more efficient.
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 隐藏层中的神经元数量通常是2的幂，以提高计算效率。
- en: The Rectified Linear Unit (ReLU) is a popular nonlinear activation function
    that does not vary with scale and allows for efficient model training.
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 矩形线性单元（ReLU）是一种流行的非线性激活函数，它不随规模变化，并允许高效地训练模型。
