- en: Chapter 1\. Introduction to Data Lakes
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第1章 数据湖介绍
- en: 'Data-driven decision making is changing how we work and live. From data science,
    machine learning, and advanced analytics to real-time dashboards, decision makers
    are demanding data to help make decisions. Companies like Google, Amazon, and
    Facebook are data-driven juggernauts that are taking over traditional businesses
    by leveraging data. Financial services organizations and insurance companies have
    always been data driven, with quants and automated trading leading the way. The
    Internet of Things (IoT) is changing manufacturing, transportation, agriculture,
    and healthcare. From governments and corporations in every vertical to non-profits
    and educational institutions, data is being seen as a game changer. Artificial
    intelligence and machine learning are permeating all aspects of our lives. The
    world is bingeing on data because of the potential it represents. We even have
    a term for this binge: *big data*, defined by Doug Laney of Gartner in terms of
    the three Vs (volume, variety, and velocity), to which he later added a fourth
    and, in my opinion, the most important V—veracity.'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的决策正在改变我们的工作和生活方式。从数据科学、机器学习和高级分析到实时仪表盘，决策者需要数据来帮助做出决策。像Google、Amazon和Facebook这样的公司是以数据驱动为主导的巨头，正在通过利用数据来接管传统业务。金融服务组织和保险公司一直以来都是数据驱动的，量化分析和自动交易处于领先地位。物联网正在改变制造业、交通运输、农业和医疗保健。从各个行业的政府和公司到非营利组织和教育机构，数据被视为一个游戏规则的改变者。人工智能和机器学习正在渗透我们生活的方方面面。世界正在因为其潜力而狂热地消耗数据。我们甚至为这种狂热有一个术语：*大数据*，由Gartner的Doug
    Laney根据三个V（容量、多样性和速度）定义，后来他又添加了第四个V，也是我认为最重要的V——真实性。
- en: With so much variety, volume, and velocity, the old systems and processes are
    no longer able to support the data needs of the enterprise. Veracity is an even
    bigger problem for advanced analytics and artificial intelligence, where the principle
    of “GIGO” (garbage in = garbage out) is even more critical because it is virtually
    impossible to tell whether the data was bad and caused bad decisions in statistical
    and machine learning models or the model was bad.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 随着如此多的多样性、数量和速度，旧系统和流程已经无法支持企业的数据需求。对于高级分析和人工智能来说，数据的真实性是一个更大的问题，因为在统计和机器学习模型中，“GIGO”（垃圾进
    = 垃圾出）原则更加关键，几乎不可能判断数据是否有误导致了错误的决策，还是模型本身存在问题。
- en: To support these endeavors and address these challenges, a revolution is occurring
    in data management around how data is stored, processed, managed, and provided
    to the decision makers. Big data technology is enabling scalability and cost efficiency
    orders of magnitude greater than what’s possible with traditional data management
    infrastructure. Self-service is taking over from the carefully crafted and labor-intensive
    approaches of the past, where armies of IT professionals created well-governed
    data warehouses and data marts, but took months to make any changes.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 为了支持这些努力并解决这些挑战，数据管理正在发生革命，涉及数据的存储、处理、管理和提供给决策者的方式。大数据技术实现了比传统数据管理基础设施可能的规模性和成本效率的数量级增长。自服务正在取代过去精心设计和劳动密集型方法，过去需要IT专业人员创建良好治理的数据仓库和数据集市，但需要数月才能进行任何更改。
- en: The *data lake* is a daring new approach that harnesses the power of big data
    technology and marries it with agility of self-service. Most large enterprises
    today either have deployed or are in the process of deploying data lakes.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: '*数据湖* 是一种大胆的新方法，它利用大数据技术的力量，与自服务的灵活性结合起来。今天，大多数大型企业都已经部署了或正在部署数据湖。'
- en: This book is based on discussions with over a hundred organizations, ranging
    from the new data-driven companies like Google, LinkedIn, and Facebook to governments
    and traditional corporate enterprises, about their data lake initiatives, analytic
    projects, experiences, and best practices. The book is intended for IT executives
    and practitioners who are considering building a data lake, are in the process
    of building one, or have one already but are struggling to make it productive
    and widely adopted.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 这本书是基于与一百多个组织的讨论而写成的，这些组织涵盖了从新型数据驱动公司如Google、LinkedIn和Facebook到政府和传统企业的数据湖计划、分析项目、经验和最佳实践。该书旨在面向正在考虑构建数据湖、正在构建数据湖或已经构建了数据湖但难以提高其生产力和广泛采纳的IT高管和从业者。
- en: What’s a data lake? Why do we need it? How is it different from what we already
    have? This chapter gives a brief overview that will get expanded in detail in
    the following chapters. In an attempt to keep the summary succinct, I am not going
    to explain and explore each term and concept in detail here, but will save the
    in-depth discussion for subsequent chapters.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 什么是数据湖？我们为什么需要它？它与我们已有的有什么不同？本章节提供了一个简要概述，将在接下来的章节中详细展开。为了保持摘要简洁，我不打算在这里详细解释和探讨每个术语和概念，而是将深入讨论留到后续章节。
- en: 'Data-driven decision making is all the rage. From data science, machine learning,
    and advanced analytics to real-time dashboards, decision makers are demanding
    data to help make decisions. This data needs a home, and the data lake is the
    preferred solution for creating that home. The term was invented and first described
    by James Dixon, CTO of Pentaho, who wrote in his [blog](https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/):
    “If you think of a datamart as a store of bottled water—cleansed and packaged
    and structured for easy consumption—the data lake is a large body of water in
    a more *natural* state. The contents of the data lake stream in from a source
    to fill the lake, and *various users* of the lake can come to examine, dive in,
    or take samples.” I italicized the critical points, which are:'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 数据驱动的决策正在风靡。从数据科学、机器学习和高级分析到实时仪表盘，决策者们都需要数据来帮助做出决策。这些数据需要一个家，而数据湖是创建这个家的首选解决方案。这个术语是由Pentaho的首席技术官詹姆斯·迪克森（James
    Dixon）发明并首次描述的，他在他的[博客](https://jamesdixon.wordpress.com/2010/10/14/pentaho-hadoop-and-data-lakes/)中写道：“如果你把数据集市比作装瓶水的商店——经过清洁、包装和结构化，方便消费——那么数据湖则是一个更
    *自然* 状态的大水体。数据湖的内容从源头流入填满湖泊，并供湖泊的 *各种用户* 检查、潜入或取样。”我用斜体强调了关键点，它们是：
- en: The data is in its original form and format (*natural* or raw data).
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据是以其原始形式和格式 (*自然* 或原始数据) 存在。
- en: The data is used by *various users* (i.e., accessed and accessible by a large
    user community).
  id: totrans-9
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据被 *各种用户* 使用（即大型用户群体访问和可访问）。
- en: This book is all about how to build a data lake that brings raw (as well as
    processed) data to a large user community of business analysts rather than just
    using it for IT-driven projects. The reason to make raw data available to analysts
    is so they can perform self-service analytics.  Self-service has been an important
    mega-trend toward democratization of data. It started at the point of usage with
    self-service visualization tools like Tableau and Qlik (sometimes called *data
    discovery* tools) that let analysts analyze data without having to get help from
    IT. The self-service trend continues with data preparation tools that help analysts
    shape the data for analytics, and catalog tools that help analysts find the data
    that they need and data science tools that help perform advanced analytics. For
    even more advanced analytics generally referred to as data science, a new class
    of users called data scientists also usually make a data lake their primary data
    source.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本书关注如何构建一个数据湖，将原始（以及经过处理的）数据带给业务分析师的大型用户群体，而不仅仅是用于IT驱动的项目。使原始数据可供分析师进行自助服务分析的原因在于，自助服务已成为数据民主化的重要巨大趋势。它始于使用自助可视化工具如Tableau和Qlik（有时被称为
    *数据发现* 工具），让分析师能够分析数据而不必从IT获取帮助。自助服务的趋势继续发展，包括帮助分析师整理数据以进行分析的数据准备工具，帮助分析师找到他们需要的数据的目录工具以及帮助进行高级分析的数据科学工具。对于通常被称为数据科学的更高级分析，一个名为数据科学家的新用户类别也通常把数据湖作为他们的主要数据来源。
- en: Of course, a big challenge with self-service is governance and data security.
    Everyone agrees that data has to be kept safe, but in many regulated industries,
    there are prescribed data security policies that have to be implemented and it
    is illegal to give analysts access to all data. Even in some non-regulated industries,
    it is considered a bad idea. The question becomes, how do we make data available
    to the analysts without violating internal and external data compliance regulations?
    This is sometimes called data democratization and will be discussed in detail
    in subsequent chapters.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，自助服务面临的一个大挑战是治理和数据安全。每个人都同意数据必须保持安全，但在许多受监管的行业中，有指定的数据安全政策必须实施，而且给分析师访问所有数据是非法的。甚至在一些非受监管的行业中，这被认为是一个坏主意。问题变成了，我们如何在不违反内部和外部数据合规法规的情况下向分析师提供数据？这有时被称为数据民主化，并将在后续章节中详细讨论。
- en: Data Lake Maturity
  id: totrans-12
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖成熟度
- en: 'The data lake is a relatively new concept, so it is useful to define some of
    the stages of maturity you might observe and to clearly articulate the differences
    between these stages:'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 数据湖是一个相对较新的概念，因此定义您可能观察到的成熟阶段，并清楚阐明这些阶段之间的区别是非常有用的：
- en: A *data puddle* is basically a single-purpose or single-project data mart built
    using big data technology. It is typically the first step in the adoption of big
    data technology. The data in a data puddle is loaded for the purpose of a single
    project or team. It is usually well known and well understood, and the reason
    that big data technology is used instead of traditional data warehousing is to
    lower cost and provide better performance.
  id: totrans-14
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据水坑*基本上是使用大数据技术构建的单一用途或单一项目数据集市。这通常是采用大数据技术的第一步。数据水坑中的数据加载是为单个项目或团队的目的而进行的。通常情况下，这些数据是众所周知且深入理解的。采用大数据技术而不是传统数据仓库的原因是为了降低成本并提供更好的性能。'
- en: A *data pond* is a collection of data puddles. It may be like a poorly designed
    data warehouse, which is effectively a collection of colocated data marts, or
    it may be an offload of an existing data warehouse. While lower technology costs
    and better scalability are clear and attractive benefits, these constructs still
    require a high level of IT participation. Furthermore, data ponds limit data to
    only that needed by the project, and use that data only for the project that requires
    it. Given the high IT costs and limited data availability, data ponds do not really
    help us with the goals of democratizing data usage or driving self-service and
    data-driven decision making for business users.
  id: totrans-15
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据池*是数据水坑的集合。它可能类似于设计不佳的数据仓库，实际上是共同放置的数据集合，或者它可能是现有数据仓库的卸载。虽然技术成本较低且可扩展性更好是明显且有吸引力的优点，但这些结构仍需要高水平的IT参与。此外，数据池将数据限制为仅满足项目需要的数据，并且仅用于需要该数据的项目。考虑到高IT成本和有限的数据可用性，数据池实际上并未帮助我们实现数据使用民主化或推动业务用户的自助服务和数据驱动决策的目标。'
- en: A *data lake* is different from a data pond in two important ways. First, it
    supports self-service, where business users are able to find and use data sets
    that they want to use without having to rely on help from the IT department. Second,
    it aims to contain data that business users might possibly want even if there
    is no project requiring it at the time.
  id: totrans-16
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据湖*与数据池在两个重要方面有所不同。首先，它支持自助服务，业务用户能够找到并使用他们想要的数据集，无需依赖IT部门的帮助。其次，它旨在包含业务用户可能会需要的数据，即使当前没有项目需要使用这些数据。'
- en: A *data ocean* expands self-service data and data-driven decision making to
    all enterprise data, wherever it may be, regardless of whether it was loaded into
    the data lake or not.
  id: totrans-17
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*数据海洋*扩展了自助服务数据和数据驱动的决策到所有企业数据，无论数据是否加载到数据湖中。'
- en: '[Figure 1-1](#the_four_stages_of_maturity) illustrates the differences between
    these concepts. As maturity grows from a puddle to a pond to a lake to an ocean,
    the amount of data and the number of users grow—sometimes quite dramatically.
    The usage pattern moves from one of high-touch IT involvement to self-service,
    and the data expands beyond what’s needed for immediate projects.'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-1](#the_four_stages_of_maturity)展示了这些概念之间的区别。随着成熟度从水坑到池塘再到湖泊再到海洋的增长，数据量和用户数量有时会显著增长。使用模式从高度依赖IT的互动转变为自助服务，数据也扩展到超出当前项目所需的范围。'
- en: '![The four stages of maturity](Images/ebdl_0101.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![成熟度的四个阶段](Images/ebdl_0101.png)'
- en: Figure 1-1\. The four stages of maturity
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-1\. 成熟度的四个阶段
- en: The key difference between the data pond and the data lake is the focus. Data
    ponds provide a less expensive and more scalable technology alternative to existing
    relational data warehouses and data marts. Whereas the latter are focused on running
    routine, production-ready queries, data lakes enable business users to leverage
    data to make their own decisions by doing ad hoc analysis and experimentation
    with a variety of new types of data and tools, as illustrated in [Figure 1-2](#value_proposition_of_the_data_lake).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池和数据湖之间的关键区别在于焦点。数据池提供了相对于现有的关系型数据仓库和数据集市更廉价和更可扩展的技术替代方案。而后者侧重于运行例行、生产就绪的查询，数据湖使业务用户能够利用数据自主做出决策，通过使用各种新类型的数据和工具进行自由分析和实验，正如[图 1-2](#value_proposition_of_the_data_lake)所示。
- en: Before we get into what it takes to create a successful data lake, let’s take
    a closer look at the two maturity stages that lead up to it.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 在深入探讨如何创建成功的数据湖之前，让我们更仔细地看看导致其之前的两个成熟阶段。
- en: '![Value proposition of the data lake](Images/ebdl_0102.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖的价值主张](Images/ebdl_0102.png)'
- en: Figure 1-2\. Value proposition of the data lake
  id: totrans-24
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-2\. 数据湖的价值主张
- en: Data Puddles
  id: totrans-25
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据池
- en: Data puddles are usually built for a small focused team or specialized use case.
    These “puddles” are modest-sized collections of data owned by a single team, frequently
    built in the cloud by business units using shadow IT. In the age of data warehousing,
    each team was used to building a relational data mart for each of its projects.
    The process of building a data puddle is very similar, except it uses big data
    technology. Typically, data puddles are built for projects that require the power
    and scale of big data. Many advanced analytics projects, such as those focusing
    on customer churn or predictive maintenance, fall in this category.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池通常为一个小而专注的团队或专业用例构建。这些“池子”是由单个团队拥有的中等规模的数据集合，通常由业务单位在云中使用影子 IT 构建。在数据仓库时代，每个团队习惯为其项目构建关系型数据集市。构建数据池的过程非常相似，只是使用了大数据技术。通常，数据池是为需要大数据强大规模支持的项目而构建的。许多高级分析项目，如那些专注于客户流失或预测性维护的项目，属于这一类别。
- en: Sometimes, data puddles are built to help IT with automated compute-intensive
    and data-intensive processes, such as extract, transform, load (ETL) offloading,
    which will be covered in detail in later chapters, where all the transformation
    work is moved from the data warehouse or expensive ETL tools to a big data platform.
    Another common use is to serve a single team by providing a work area, called
    a *sandbox*, in which data scientists can experiment.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据池被构建来帮助 IT 处理自动化计算密集和数据密集的流程，比如详细介绍的数据抽取、转换和加载（ETL）卸载，其中所有的转换工作从数据仓库或昂贵的
    ETL 工具转移到大数据平台上。另一个常见的用途是为单个团队提供一个工作区，称为*沙盒*，供数据科学家进行实验。
- en: Data puddles usually have a small scope and a limited variety of data; they’re
    populated by small, dedicated data streams, and constructing and maintaining them
    requires a highly technical team or heavy involvement from IT.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池通常范围小、数据种类有限；它们由小型、专用的数据流填充，并且构建和维护它们需要高度技术的团队或者 IT 的深度参与。
- en: Data Ponds
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据池
- en: 'A data pond is a collection of data puddles. Just as you can think of data
    puddles as data marts built using big data technology, you can think of a data
    pond as a data warehouse built using big data technology. It may come into existence
    organically, as more puddles get added to the big data platform. Another popular
    approach for creating a data pond is as a data warehouse offload. Unlike with
    ETL offloading, which uses big data technology to perform some of the processing
    required to populate a data warehouse, the idea here is to take all the data in
    the data warehouse and load it into a big data platform. The vision is often to
    eventually get rid of the data warehouse to save costs and improve performance,
    since big data platforms are much less expensive and much more scalable than relational
    databases. However, just offloading the data warehouse does not give the analysts
    access to the raw data. Because the rigorous architecture and governance applied
    to the data warehouse are still maintained, the organization cannot address all
    the challenges of the data warehouse, such as long and expensive change cycles,
    complex transformations, and manual coding as the basis for all reports. Finally,
    the analysts often do not like moving from a finely tuned data warehouse with
    lightning-fast queries to a much less predictable big data platform, where huge
    batch queries may run faster than in a data warehouse but more typical smaller
    queries may take minutes. [Figure 1-3](#the_drawbacks_of_data_warehouse_offloadi)
    illustrates some of the typical limitations of data ponds: lack of predictability,
    agility, and access to the original untreated data.'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 数据池是数据小块的集合。正如您可以将数据小块视为使用大数据技术构建的数据集市，您可以将数据池视为使用大数据技术构建的数据仓库。它可能是有机地形成的，随着更多小块被添加到大数据平台。创建数据池的另一种流行方法是作为数据仓库卸载的一部分。与ETL卸载不同，后者使用大数据技术执行一些处理以填充数据仓库所需，这里的想法是将数据仓库中的所有数据加载到大数据平台中。其愿景通常是最终摆脱数据仓库以节省成本并提高性能，因为与关系数据库相比，大数据平台要便宜得多且可扩展性更强。然而，仅仅卸载数据仓库并不能使分析师访问原始数据。因为数据仓库所遵循的严格架构和治理依然存在，组织无法解决数据仓库的所有挑战，比如长期且昂贵的变更周期、复杂的转换以及作为所有报告基础的手工编码。最后，分析师通常不喜欢从经过精细调整的、具有闪电般快速查询的数据仓库转移到预测性较差得多的大数据平台，虽然在大数据平台上大批量查询可能比数据仓库中快，但更典型的小查询可能需要几分钟。[图 1-3](#the_drawbacks_of_data_warehouse_offloadi)展示了数据池的一些典型局限性：缺乏预测性、敏捷性和访问原始未经处理数据的能力。
- en: '![The drawbacks of data warehouse offloading](Images/ebdl_0103.png)'
  id: totrans-31
  prefs: []
  type: TYPE_IMG
  zh: '![数据仓库卸载的缺点](Images/ebdl_0103.png)'
- en: Figure 1-3\. The drawbacks of data warehouse offloading
  id: totrans-32
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-3\. 数据仓库卸载的缺点
- en: Creating a Successful Data Lake
  id: totrans-33
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 创建成功的数据湖
- en: 'So what does it take to have a successful data lake? As with any project, aligning
    it with the company’s business strategy and having executive sponsorship and broad
    buy-in are a must. In addition, based on discussions with dozens of companies
    deploying data lakes with varying levels of success, three key prerequisites can
    be identified:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，成功的数据湖需要什么？与任何项目一样，将其与公司的业务策略对齐，并获得高管赞助和广泛支持至关重要。此外，基于与部署数据湖的数十家公司的讨论，可以确定三个关键前提条件：
- en: The right platform
  id: totrans-35
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的平台
- en: The right data
  id: totrans-36
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的数据
- en: The right interfaces
  id: totrans-37
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 正确的接口
- en: The Right Platform
  id: totrans-38
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确的平台
- en: 'Big data technologies like Hadoop and cloud solutions like Amazon Web Services
    (AWS), Microsoft Azure, and Google Cloud Platform are the most popular platforms
    for a data lake. These technologies share several important advantages:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 像Hadoop和云解决方案（如Amazon Web Services (AWS)、Microsoft Azure和Google Cloud Platform）这样的大数据技术是数据湖的最流行平台。这些技术共享一些重要优势：
- en: Volume
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 体积
- en: These platforms were designed to scale out—in other words, to scale indefinitely
    without any significant degradation in performance.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 这些平台被设计为扩展性外扩——换句话说，可以在不显著降低性能的情况下无限扩展。
- en: Cost
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 成本
- en: We have always had the capacity to store a lot of data on fairly inexpensive
    storage, like tapes, WORM disks, and hard drives. But not until big data technologies
    did we have the ability to both store and process huge volumes of data so inexpensively—usually
    at one-tenth to one-hundredth the cost of a commercial relational database.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 我们一直有能力在相对便宜的存储介质上存储大量数据，比如磁带、WORM磁盘和硬盘。但直到大数据技术出现之前，我们才有能力以如此低廉的成本存储和处理大量数据，通常是商业关系数据库成本的十分之一至百分之一。
- en: Variety
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 多样性
- en: 'These platforms use filesystems or object stores that allow them to store all
    sorts of files: Hadoop HDFS, MapR FS, AWS’s Simple Storage Service (S3), and so
    on. Unlike a relational database that requires the data structure to be predefined
    (*schema on write*), a filesystem or an object store does not really care what
    you write. Of course, to meaningfully process the data you need to know its schema,
    but that’s only when you use the data. This approach is called *schema on read*
    and it’s one of the important advantages  of big data platforms, enabling what’s
    called “frictionless ingestion.” In other words, data can be loaded with absolutely
    no processing, unlike in a relational database, where data cannot be loaded until
    it is converted to the schema and format expected by the database.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 这些平台使用文件系统或对象存储，允许它们存储各种文件：Hadoop HDFS，MapR FS，AWS 的简单存储服务（S3），等等。不像关系型数据库需要预定义数据结构（*写入时模式*），文件系统或对象存储并不关心你写入了什么。当然，要想有意义地处理数据，你需要知道它的模式，但那只有在使用数据时才需要。这种方法被称为*读取时模式*，是大数据平台的重要优势之一，促成了所谓的“无摩擦摄入”。换句话说，数据可以在不经任何处理的情况下加载，这与关系型数据库不同，后者必须将数据转换为数据库所期望的模式和格式后才能加载。
- en: Future-proofing
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 未来的保障
- en: Because our requirements and the world we live in are in flux, it is critical
    to make sure that the data we have can be used to help with our future needs.
    Today, if data is stored in a relational database, it can be accessed only by
    that relational database. Hadoop and other big data platforms, on the other hand,
    are very modular. The same file can be used by various processing engines and
    programs—from Hive queries (Hive provides a SQL interface to Hadoop files) to
    Pig scripts to Spark and custom MapReduce jobs, all sorts of different tools and
    systems can access and use the same files. Because big data technology is evolving
    rapidly, this gives people confidence that any future projects will still be able
    to access the data in the data lake.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们的需求和所处的世界在变化，确保我们拥有的数据能够帮助未来的需求变得至关重要。今天，如果数据存储在关系型数据库中，只能由该关系型数据库访问。另一方面，Hadoop
    和其他大数据平台非常模块化。同一个文件可以被各种处理引擎和程序使用——从Hive查询（Hive提供对Hadoop文件的SQL接口）到Pig脚本，再到Spark和自定义MapReduce作业，各种不同的工具和系统都可以访问和使用同一个文件。由于大数据技术正在迅速发展，这使人们对任何未来项目仍能访问数据湖中的数据充满信心。
- en: The Right Data
  id: totrans-48
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 正确的数据
- en: Most data collected by enterprises today is thrown away. Some small percentage
    is aggregated and kept in a data warehouse for a few years, but most detailed
    operational data, machine-generated data, and old historical data is either aggregated
    or thrown away altogether. That makes it difficult to do analytics. For example,
    if an analyst recognizes the value of some data that was traditionally thrown
    away, it may take months or even years to accumulate enough history of that data
    to do meaningful analytics. The promise of the data lake, therefore, is to be
    able to store as much data as possible for future use.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 今天企业收集的大部分数据都被丢弃了。少部分被汇总并保存在数据仓库中几年，但大多数详细的操作数据、机器生成的数据和旧历史数据要么被汇总，要么完全被丢弃。这使得进行分析变得困难。例如，如果分析人员认识到一些传统上被丢弃的数据的价值，可能需要数月甚至数年才能积累足够的数据历史来进行有意义的分析。因此，数据湖的承诺在于能够尽可能地存储大量数据以供将来使用。
- en: So, the data lake is sort of like a piggy bank ([Figure 1-4](#a_data_lake_is_like_a_piggy_bankcomma_al))—you
    often don’t know what you are saving the data for, but you want it in case you
    need it one day. Moreover, because you don’t know how you will use the data, it
    doesn’t make sense to convert or treat it prematurely. You can think of it like
    traveling with your piggy bank through different countries, adding money in the
    currency of the country you happen to be in at the time and keeping the contents
    in their native currencies until you decide what country you want to spend the
    money in; you can then convert it all to that currency, instead of needlessly
    converting your funds (and paying conversion fees) every time you cross a border.
    To summarize, the goal is to *save as much data as possible in its native format*.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 所以，数据湖有点像一个存钱罐（[Figure 1-4](#a_data_lake_is_like_a_piggy_bankcomma_al)）——你经常不知道为什么要保存数据，但你希望以防一天需要。此外，因为你不知道如何使用数据，所以过早地转换或处理它是没有意义的。你可以将它看作是带着你的存钱罐在不同国家旅行，在当地货币的国家增加货币，并将内容保留在其原生货币中，直到你决定在哪个国家使用这些钱；然后你可以将所有钱转换为该货币，而不是每次过境时都不必要地转换资金（并支付转换费用）。总结一下，目标是*尽可能以其原生格式保存数据*。
- en: '![A data lake is like a piggy bank, allowing you to keep the data in its native
    or raw format](Images/ebdl_0104.png)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![数据湖就像一个存钱罐，让你以其原生或原始格式保存数据](Images/ebdl_0104.png)'
- en: Figure 1-4\. A data lake is like a piggy bank, allowing you to keep the data
    in its native or raw format
  id: totrans-52
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-4\. 数据湖就像一个存钱罐，让你以其原生或原始格式保存数据。
- en: Another challenge with getting the right data is *data silos*. Different departments
    might hoard their data, both because it is difficult and expensive to provide
    and because there is often a political and organizational reluctance to share.
    In a typical enterprise, if one group needs data from another group, it has to
    explain what data it needs and then the group that owns the data has to implement
    ETL jobs that extract and package the required data. This is expensive, difficult,
    and time-consuming, so teams may push back on data requests as much as possible
    and then take as long as they can get away with to provide the data. This extra
    work is often used as an excuse to not share data.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 获取正确数据的另一个挑战是*数据孤岛*。不同部门可能囤积他们的数据，一是因为提供数据困难且昂贵，二是因为在政治和组织上不愿意分享。在典型企业中，如果一个组需要另一个组的数据，它必须解释需要哪些数据，然后拥有数据的组必须实施ETL作业以提取和打包所需数据。这是昂贵、困难且耗时的，因此团队可能会尽可能推迟数据请求，然后尽可能长时间地提供数据。这种额外工作常被用作不分享数据的借口。
- en: With a data lake, because the lake consumes raw data through frictionless ingestion
    (basically, it’s ingested as is without any processing), that challenge (and excuse)
    goes away. A well-governed data lake is also centralized and offers a transparent
    process to people throughout the organization about how to obtain data, so ownership
    becomes much less of a barrier.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 有了数据湖，因为湖通过无摩擦摄取原始数据（基本上是按原样摄取），这种挑战（和借口）消失了。一个良好治理的数据湖也是集中化的，并为组织内的人们提供获取数据的透明流程，因此所有权不再是障碍。
- en: The Right Interface
  id: totrans-55
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '**正确的界面**'
- en: Once we have the right platform and we’ve loaded the data, we get to the more
    difficult aspects of the data lake, where most companies fail—choosing the right
    interface. To gain wide adoption and reap the benefits of helping business users
    make data-driven decisions, the solutions companies provide must be self-service,
    so their users can find, understand, and use the data without needing help from
    IT. IT will simply not be able to scale to support such a large user community
    and such a large variety of data.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们拥有了正确的平台并加载了数据，我们进入了数据湖更为困难的部分，这也是大多数公司失败的地方——选择正确的界面。为了获得广泛采用并享受帮助业务用户做出数据驱动决策的好处，公司提供的解决方案必须是自助服务的，这样他们的用户可以在不需要IT帮助的情况下找到、理解和使用数据。IT简直无法扩展以支持如此庞大的用户群体和如此多样化的数据。
- en: 'There are two aspects to enabling self-service: providing data at the right
    level of expertise for the users, and ensuring the users are able to find the
    right data.'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 启用自助服务有两个方面：为用户提供适合其专业水平的数据，确保用户能够找到正确的数据。
- en: Providing data at the right level of expertise
  id: totrans-58
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 为用户提供适合其专业水平的数据是另一个挑战。
- en: To get broad adoption for the data lake, we want everyone from data scientists
    to business analysts to use it. However, when considering such divergent audiences
    with different needs and skill levels, we have to be careful to make the right
    data available to the right user populations.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 为了使数据湖得到广泛采用，我们希望所有人，从数据科学家到业务分析师，都能使用它。然而，考虑到这些具有不同需求和技能水平的不同受众时，我们必须小心确保将正确的数据提供给正确的用户群体。
- en: For example, analysts often don’t have the skills to use raw data. Raw data
    usually has too much detail, is too granular, and frequently has too many quality
    issues to be easily used. For instance, if we collect sales data from different
    countries that use different applications, that data will come in different formats
    with different fields (e.g., one country may have sales tax whereas another doesn’t)
    and different units of measure (e.g., lb versus kg, $ versus €).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，分析师通常没有使用原始数据的技能。原始数据通常具有太多细节，太精细，并且经常存在太多质量问题，以至于难以轻松使用。例如，如果我们收集来自使用不同应用程序的不同国家的销售数据，那么该数据将以不同的格式和不同的字段（例如，一个国家可能有销售税而另一个国家没有）以及不同的计量单位（例如，磅对公斤，美元对欧元）的形式出现。
- en: In order for the analysts to use this data, it has to be *harmonized*—put into
    the same schema with the same field names and units of measure—and frequently
    also aggregated to daily sales per product or per customer. In other words, analysts
    want “cooked” prepared meals, not raw data.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 为了让分析师使用这些数据，必须对其进行*协调* —— 将其放入相同的架构中，并使用相同的字段名称和计量单位 —— 并且通常还需要将其聚合到每日产品或每客户的销售额。换句话说，分析师想要“熟制”的准备好的餐点，而不是原始数据。
- en: Data scientists, on the other hand, are the complete opposite. For them, cooked
    data often loses the golden nuggets that they are looking for. For example, if
    they want to see how often two products are bought together, but the only information
    they can get is daily totals by product, data scientists will be stuck. They are
    like chefs who need raw ingredients to create their culinary or analytic masterpieces.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 数据科学家则完全相反。对他们来说，处理过的数据往往会丢失他们正在寻找的重要信息。例如，如果他们想要看看两种产品一起购买的频率，但他们能获取的唯一信息是按产品的日常总量计算，数据科学家将会陷入困境。他们就像需要原材料来制作他们烹饪或分析杰作的厨师。
- en: We’ll see in this book how to satisfy divergent needs by setting up multiple
    *zones*, or areas that contain data that meets particular requirements. For example,
    the raw or landing zone contains the original data ingested into the lake, whereas
    the production or gold zone contains high-quality, governed data. We’ll take a
    quick look at zones in [“Organizing the Data Lake”](#organizing_the_data_lake);
    a more detailed discussion can be found in [Chapter 7](ch07.xhtml#architecting_the_data_lake).
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本书中看到如何通过设置多个*区域*来满足不同的需求，这些区域包含满足特定要求的数据。例如，原始区域或着陆区包含输入到数据湖中的原始数据，而生产区或黄金区包含高质量的、受管理的数据。我们将快速查看[“组织数据湖”](#organizing_the_data_lake)中的区域；更详细的讨论可以在[第7章](ch07.xhtml#architecting_the_data_lake)中找到。
- en: Getting to the data
  id: totrans-64
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 获取数据
- en: 'Most companies that I have spoken with are settling on the “shopping for data”
    paradigm, where analysts use an [Amazon.com-style](http://Amazon.com-style) interface
    to find, understand, rate, annotate, and consume data. The advantages of this
    approach are manifold, including:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我与大多数公司交流的情况是，它们都正在采用“购物数据”的范式，分析师使用类似于[Amazon.com-style](http://Amazon.com-style)的界面来查找、理解、评价、注释和消费数据。这种方法的优势多种多样，包括：
- en: A familiar interface
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 一个熟悉的界面
- en: Most people are familiar with online shopping and feel comfortable searching
    with keywords and using facets, ratings, and comments, so they require no or minimal
    training.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数人对在线购物都很熟悉，并且喜欢使用关键词搜索和使用分类、评分和评论，因此他们不需要或只需要很少的培训。
- en: Faceted search
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 分类搜索
- en: Search engines are optimized for faceted search. Faceted search is very helpful
    when the number of possible search results is large and the user is trying to
    zero in on the right result. For example, if you were to search Amazon for toasters
    ([Figure 1-5](#an_online_shopping_interface)), facets would list manufacturers,
    whether the toaster should accept bagels, how many slices it needs to toast, and
    so forth. Similarly, when users are searching for the right data sets, facets
    can help them specify what attributes they would like in the data set, the type
    and format of the data set, the system that holds it, the size and freshness of
    the data set, the department that owns it, what entitlements it has, and any number
    of other useful characteristics.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 搜索引擎被优化用于多层次搜索。当可能的搜索结果数量很大而用户试图找到正确结果时，多层次搜索非常有帮助。例如，如果你在亚马逊搜索烤面包机（[图 1-5](#an_online_shopping_interface)），多层次搜索将列出制造商、烤面包机是否需要接受贝果、需要烤多少片面包等选项。类似地，当用户搜索合适的数据集时，多层次搜索可以帮助他们指定数据集中希望的属性、数据集的类型和格式、保存数据集的系统、数据集的大小和新鲜度、拥有数据集的部门、数据集的权限以及任何其他有用的特征。
- en: Ranking and sorting
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 排名和排序
- en: The ability to present and sort data assets, widely supported by search engines,
    is important for choosing the right asset based on specific criteria.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 根据特定标准选择正确资产的能力，这是由搜索引擎广泛支持的，并且对于选择合适的资产非常重要。
- en: Contextual search
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文搜索
- en: As catalogs get smarter, the ability to find data assets using a semantic understanding
    of what analysts are looking for will become more important. For example, a salesperson
    looking for customers may really be looking for prospects, while a technical support
    person looking for customers may really be looking for existing customers.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 随着目录变得更加智能，使用语义理解分析师正在寻找的数据资产的能力将变得更加重要。例如，一个销售人员寻找客户时，可能实际上是在寻找潜在客户，而技术支持人员寻找客户时可能实际上是在寻找现有客户。
- en: '![An online shopping interface](Images/ebdl_0105.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![一个在线购物界面](Images/ebdl_0105.png)'
- en: Figure 1-5\. An online shopping interface
  id: totrans-75
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-5\. 一个在线购物界面
- en: The Data Swamp
  id: totrans-76
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据污泥
- en: While data lakes always start out with good intentions, sometimes they take
    a wrong turn and end up as *data swamps*. A data swamp is a data pond that has
    grown to the size of a data lake but failed to attract a wide analyst community,
    usually due to a lack of self-service and governance facilities. At best, the
    data swamp is used like a data pond, and at worst it is not used at all. Often,
    while various teams use small areas of the lake for their projects (the white
    data pond area in [Figure 1-6](#a_data_swamp)), the majority of the data is dark,
    undocumented, and unusable.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然数据湖始终以良好的意图开始，有时候它们会走错方向，最终成为*数据污泥*。数据污泥是一个数据池，它已经扩展到数据湖的大小，但由于缺乏自助服务和治理设施，未能吸引广泛的分析师社区。在最好的情况下，数据污泥像数据池一样使用，而在最坏的情况下，根本不被使用。通常情况下，虽然各个团队在其项目中使用湖的小区域（[图
    1-6](#a_data_swamp)中的白色数据池区域），但大部分数据是黑暗的、未记录的和无法使用的。
- en: '![A data swamp](Images/ebdl_0106.png)'
  id: totrans-78
  prefs: []
  type: TYPE_IMG
  zh: '![一个数据污泥](Images/ebdl_0106.png)'
- en: Figure 1-6\. A data swamp
  id: totrans-79
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-6\. 一个数据污泥
- en: When data lakes first came onto the scene, a lot of companies rushed out to
    buy Hadoop clusters and fill them with raw data, without a clear understanding
    of how it would be utilized. This led to the creation of massive data swamps with
    millions of files containing petabytes of data and no way to make sense of that
    data.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 当数据湖首次出现时，许多公司急忙购买 Hadoop 集群并填充原始数据，但并没有清晰地理解如何利用这些数据。这导致了大量数据污泥的形成，其中包含数百万个文件，总计几十PB的数据，但却无法对这些数据做出任何意义上的解释。
- en: 'Only the most sophisticated users were able to navigate the swamps, usually
    by carving out small puddles that they and their teams could make use of. Furthermore,
    governance regulations precluded opening up the swamps to a broad audience without
    protecting sensitive data. Since no one could tell where the sensitive data was,
    users could not be given access and the data largely remained unusable and unused.
    One data scientist shared with me his experience of how his company built a data
    lake, encrypted all the data in the lake to protect it, and required data scientists
    to prove that the data they wanted was not sensitive before it would unencrypt
    it and let them use it. This proved to be a catch-22: because everything was encrypted,
    the data scientist I talked to couldn’t find anything, much less prove that it
    was not sensitive. As a result, no one was using the data lake (or, as he called
    it, the swamp).'
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 只有最复杂的用户能够导航这些沼泽地，通常是通过开辟他们和他们的团队可以利用的小水洼。此外，治理规定禁止在不保护敏感数据的情况下向广大用户开放沼泽地。由于没有人能告诉哪些数据是敏感的，用户无法获得访问权限，数据很大程度上保持不可用和未使用。一位数据科学家与我分享了他的经历，他的公司建立了一个数据湖，将湖中所有数据加密以保护它，并要求数据科学家证明他们想要的数据不是敏感的，然后才会解密并允许他们使用。这被证明是一个进退两难的局面：因为所有东西都被加密了，我跟他谈过的那位数据科学家什么也找不到，更不用说证明它不是敏感的了。结果，没有人在使用这个数据湖（或者正如他所说的，这个沼泽地）。
- en: Roadmap to Data Lake Success
  id: totrans-82
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖成功的路线图
- en: 'Now that we know what it takes for a data lake to be successful and what pitfalls
    to look out for, how do we go about building one? Usually, companies follow this
    process:'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们知道数据湖成功所需及其需要避免的陷阱，那么我们如何开始建设一个数据湖呢？通常，公司按照以下流程进行：
- en: Stand up the infrastructure (get the Hadoop cluster up and running).
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 搭建基础设施（启动和运行Hadoop集群）。
- en: Organize the data lake (create zones for use by various user communities and
    ingest the data).
  id: totrans-85
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 组织数据湖（为各种用户社区创建使用区域并摄入数据）。
- en: Set the data lake up for self-service (create a catalog of data assets, set
    up permissions, and provide tools for the analysts to use).
  id: totrans-86
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为自助服务设置数据湖（创建数据资产目录，设置权限，并提供分析师使用的工具）。
- en: Open the data lake up to the users.
  id: totrans-87
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 开放数据湖给用户使用。
- en: Standing Up a Data Lake
  id: totrans-88
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 建立数据湖
- en: 'When I started writing this book back in 2015, most enterprises were building
    on-premises data lakes using either open source or commercial Hadoop distributions.
    By 2018, at least half of enterprises were either building their data lakes entirely
    in the cloud or building hybrid data lakes that are both on premises and in the
    cloud. Many companies have multiple data lakes, as well. All this variety is leading
    companies to redefine what a data lake is. We’re now seeing the concept of a *logical
    data lake*: a virtual data lake layer across multiple heterogeneous systems. The
    underlying systems can be Hadoop, relational, or NoSQL databases, on premises
    or in the cloud.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 当我在2015年开始写这本书时，大多数企业正在使用开源或商业的Hadoop分布式在本地构建数据湖。到2018年，至少一半的企业要么完全在云中构建他们的数据湖，要么构建混合数据湖，既在本地又在云中。许多公司还拥有多个数据湖。所有这些多样性正在推动公司重新定义数据湖的概念。现在我们看到了*逻辑数据湖*的概念：跨多个异构系统的虚拟数据湖层。底层系统可以是Hadoop、关系型数据库或NoSQL数据库，在本地或云中。
- en: '[Figure 1-7](#different_data_lake_architectures) compares the three approaches.
    All of them offer a catalog that the users consult to find the data assets they
    need. These data assets either are already in the Hadoop data lake or get provisioned
    to it, where the analysts can use them.'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-7](#different_data_lake_architectures) 比较了这三种方法。它们都提供了用户查询数据资产的目录。这些数据资产要么已经在Hadoop数据湖中，要么被提供给它，分析师可以使用它们。'
- en: '![Different data lake architectures](Images/ebdl_0107.png)'
  id: totrans-91
  prefs: []
  type: TYPE_IMG
  zh: '![不同的数据湖架构](Images/ebdl_0107.png)'
- en: Figure 1-7\. Different data lake architectures
  id: totrans-92
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-7\. 不同的数据湖架构
- en: Organizing the Data Lake
  id: totrans-93
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 组织数据湖
- en: 'Most data lakes that I have encountered are organized roughly the same way,
    into various zones:'
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 我遇到的大多数数据湖大致都是按照不同区域组织的：
- en: A *raw* or *landing* zone where data is ingested and kept as close as possible
    to its original state.
  id: totrans-95
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*原始*或*着陆*区，数据被摄入并尽可能保持其原始状态。'
- en: A *gold* or *production* zone where clean, processed data is kept.
  id: totrans-96
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*金牌*或*生产*区，存放清洁、加工后的数据。'
- en: A *dev* or *work* zone where the more technical users such as data scientists
    and data engineers do their work. This zone can be organized by user, by project,
    by subject, or in a variety of other ways. Once the analytics work performed in
    the work zone gets productized, it is moved into the gold zone.
  id: totrans-97
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*开发*或*工作*区域，更多技术型用户如数据科学家和数据工程师在此进行工作。这个区域可以按用户、项目、主题或其他方式进行组织。一旦工作区中进行的分析工作被产品化，它就会移动到金区。
- en: A *sensitive* zone that contains sensitive data.
  id: totrans-98
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个*敏感*区域，包含敏感数据。
- en: '[Figure 1-8](#zones_of_a_typical_data_lake) illustrates this organization.'
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 1-8](#zones_of_a_typical_data_lake)展示了这种组织形式。'
- en: '![Zones of a typical data lake](Images/ebdl_0108.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![典型数据湖的区域](Images/ebdl_0108.png)'
- en: Figure 1-8\. Zones of a typical data lake
  id: totrans-101
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-8\. 典型数据湖的区域
- en: For many years, the prevailing wisdom for data governance teams was that data
    should be subject to the same governance regardless of its location or purpose.
    In the last few years, however, industry analysts from Gartner have been promoting
    the concept of *multi-modal IT*—basically, the idea that governance should reflect
    data usage and user community requirements. This approach has been widely adopted
    by data lake teams, with different zones having different levels of governance
    and service-level agreements (SLAs). For example, data in the gold zone is usually
    strongly governed, is well curated and documented, and carries quality and freshness
    SLAs, whereas data in the work area has minimal governance (mostly making sure
    there is no sensitive data) and SLAs that may vary from project to project.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 多年来，数据治理团队的普遍看法是，无论数据位于何处或用途如何，都应该受到相同的治理。然而，近年来，来自Gartner的行业分析师们开始推广*多模式IT*的概念——基本上是治理应反映数据使用和用户社区的需求。这种方法已被数据湖团队广泛采纳，不同的区域具有不同的治理级别和服务级别协议（SLA）。例如，金区的数据通常受到严格的治理，具有良好的策展和文档化，以及质量和新鲜度的SLA，而工作区的数据则有较少的治理（主要确保没有敏感数据），其SLA可能会因项目而异。
- en: Different user communities naturally gravitate to different zones. Business
    analysts use data mostly in the gold zone, data engineers work on data in the
    raw zone (converting it into production data destined for the gold zone), and
    data scientists run their experiments in the work zone. While some governance
    is required for every zone to make sure that sensitive data is detected and secured,
    data stewards mostly focus on data in the sensitive and gold zones, to make sure
    it complies with company and government regulations. [Figure 1-9](#governance_expectationscomma_zone_by_zon)
    illustrates the different levels of governance and different user communities
    for different zones.
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 不同的用户群体自然而然地倾向于不同的区域。业务分析师主要在金区使用数据，数据工程师在原始区（将其转换为用于金区生产的数据）处理数据，数据科学家则在工作区进行实验。尽管每个区域都需要一定的治理来确保敏感数据被检测和保护，但数据监护人员主要关注敏感区域和金区的数据，以确保其符合公司和政府的法规要求。[图 1-9](#governance_expectationscomma_zone_by_zon)展示了不同区域的治理级别和不同用户群体。
- en: '![Governance expectations, zone by zone](Images/ebdl_0109.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![区域治理期望](Images/ebdl_0109.png)'
- en: Figure 1-9\. Governance expectations, zone by zone
  id: totrans-105
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-9\. 区域治理期望
- en: Setting Up the Data Lake for Self-Service
  id: totrans-106
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 设置自助数据湖
- en: Analysts, be they business analysts or data analysts or data scientists, typically
    go through four steps to do their job. These steps are illustrated in [Figure 1-10](#the_four_stages_of_analysis).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 无论是业务分析师、数据分析师还是数据科学家，通常都要经历四个步骤来完成他们的工作。这些步骤在[图 1-10](#the_four_stages_of_analysis)中有所展示。
- en: '![The four stages of analysis](Images/ebdl_0110.png)'
  id: totrans-108
  prefs: []
  type: TYPE_IMG
  zh: '![分析的四个阶段](Images/ebdl_0110.png)'
- en: Figure 1-10\. The four stages of analysis
  id: totrans-109
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-10\. 分析的四个阶段
- en: The first step is to *find and understand* the data. Once they find the right
    data sets, they need to *provision* the data—that is, get access to it. Once they
    have the data, they often need to *prep* it—that is, clean it and convert it to
    a format appropriate for analysis. Finally, they need to use the data to answer
    questions or create visualizations and reports.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 第一步是*查找和理解*数据。一旦找到合适的数据集，他们需要*提供*数据，即获取对其的访问权限。一旦拥有数据，他们通常需要*准备*数据，即清理并将其转换为适合分析的格式。最后，他们需要使用数据来回答问题或创建可视化和报告。
- en: 'The first three steps theoretically are optional: if the data is well known
    and understood by the analyst, the analyst already has access to it, and it is
    already in the right shape for analytics, the analyst can do just the final step.
    In reality, a lot of studies have shown that the first three steps take up to
    80% of a typical analyst’s time, with the biggest expenditure (60%) in the first
    step of finding and understanding the data (see, for example, “Boost Your Business
    Insights by Converging Big Data and BI” by Boris Evelson, Forrester Research,
    March 25, 2015).'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，前三个步骤是可选的：如果数据被分析师所熟知并理解，分析师已经可以访问该数据，并且数据已经适合进行分析，那么分析师可以直接进行最后一步。然而实际情况是，大量研究显示，前三个步骤通常占据了典型分析师时间的80%，其中最大的开销（60%）发生在第一步中，即查找和理解数据（例如参见Boris
    Evelson的《通过融合大数据和商业智能提升您的业务洞察力》，Forrester Research，2015年3月25日）。
- en: Let’s break these down, to give you a better idea of what happens in each of
    the four stages.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这些，以便让你更好地了解每个阶段发生了什么。
- en: Finding and understanding the data
  id: totrans-113
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 查找和理解数据
- en: Why is it so difficult to find data in the enterprise? Because the variety and
    complexity of the available data far exceeds human ability to remember it. Imagine
    a very small database, with only a hundred tables (some databases have thousands
    or even tens of thousands of tables, so this is truly a very small real-life database).
    Now imagine that each table has a hundred fields—a reasonable assumption for most
    databases, especially the analytical ones where data tends to be denormalized.
    That gives us 10,000 fields. How realistic is it for anyone to remember what 10,000
    fields mean and which tables these fields are in, and then to keep track of them
    whenever using the data for something new?
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么在企业中找到数据如此困难？因为可用数据的种类和复杂性远远超出了人类记忆的能力。想象一个非常小的数据库，只有一百个表（一些数据库有数千甚至数万个表，因此这确实是一个非常小的现实生活数据库）。现在想象每个表都有一百个字段——对于大多数数据库来说，特别是数据倾向于去规范化的分析数据库来说，这是一个合理的假设。这给我们带来了10,000个字段。有人能记住10,000个字段的含义以及这些字段所在的表，并且在使用数据进行新事物时随时跟踪它们，这有多现实呢？
- en: Now imagine an enterprise that has several thousand (or several hundred thousand)
    databases, most an order of magnitude bigger than our hypothetical 10,000-field
    database. I once worked with a small bank that only had 5,000 employees, but managed
    to create 13,000 databases. I can only imagine how many a large bank with hundreds
    of thousands of employees might have. The reason I say “only imagine” is because
    none of the hundreds of large enterprises that I have worked with over my 30-year
    career were able to tell me how many databases they had—much less how many tables
    or fields.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 现在想象一下一个企业有几千（或者几十万）个数据库，大多数比我们假设的有10,000个字段的数据库大一个数量级。我曾经与一个只有5,000名员工的小银行合作过，但他们设法创建了13,000个数据库。我只能想象一个有数十万名员工的大银行可能有多少数据库。我说“只能想象”的原因是因为在我职业生涯的30年里，我与数百个大型企业合作过，他们无法告诉我他们有多少个数据库——更不用说有多少个表或字段了。
- en: Hopefully, this gives you some idea of the challenge analysts face when looking
    for data.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 希望这能让你对分析师在寻找数据时面临的挑战有所了解。
- en: A typical project involves analysts “asking around” to see whether anyone has
    ever used a particular type of data. They get pointed from person to person until
    they stumble onto a data set that someone has used in one of their projects. Usually,
    they have no idea whether this is the best data set to use, how the data set was
    generated, or even whether the data is trustworthy. They are then faced with the
    awful choice of using this data set or asking around some more and perhaps not
    finding anything better.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 一个典型的项目涉及到分析员“四处打听”，看看是否有人曾经使用过某种特定类型的数据。他们被人引荐来引荐去，直到偶然发现某个数据集曾被他人在其项目中使用过。通常情况下，他们不知道这是否是最佳的数据集可供使用，数据集是如何生成的，甚至数据是否可信。然后，他们面临着一个令人痛苦的选择，要么使用这个数据集，要么再继续打听一段时间，也许找不到更好的选择。
- en: Once they decide to use a data set, they spend a lot of time trying to decipher
    what the data it contains means. Some data is quite obvious (e.g., customer names
    or account numbers), while other data is cryptic (e.g., what does a customer code
    of 1126 mean?). So, the analysts spend still more time looking for people who
    can help them understand the data. We call this information “tribal knowledge.”
    In other words, the knowledge usually exists, but it is spread throughout the
    tribe and has to be reassembled through a painful, long, and error-prone discovery
    process.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦决定使用数据集，他们会花费大量时间尝试解读数据集中包含的意义。有些数据显而易见（例如客户姓名或账号），而其他数据则比较神秘（例如客户代码1126代表什么？）。因此，分析师还要花费更多时间寻找能帮助他们理解数据的人。我们称这种信息为“部落知识”。换句话说，知识通常是存在的，但它散布在整个部落中，并且必须通过痛苦、漫长和容易出错的发现过程重新组装起来。
- en: Fortunately, there are new *analyst crowdsourcing* tools that are tackling this
    problem by collecting tribal knowledge through a process that allows analysts
    to document data sets using simple descriptions composed of business terms, and
    builds a search index to help them find what they are looking for. Tools like
    these have been custom-developed at modern data-driven companies such as Google
    and LinkedIn. Because data is so important at those companies and “everyone is
    an analyst,” the awareness of the problem and willingness to contribute to the
    solution is much higher than in traditional enterprises. It is also much easier
    to document data sets when they are first created, because the information is
    fresh. Nevertheless, even at Google, while some popular data sets are well documented,
    there is still a vast amount of dark or undocumented data.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，有一些新的*分析师众包*工具正在通过收集部落知识来解决这个问题，该过程允许分析师使用由业务术语组成的简单描述文档化数据集，并建立搜索索引来帮助他们找到所需内容。这些工具已经在谷歌和LinkedIn等现代数据驱动公司进行了定制开发。由于在这些公司中数据非常重要且“每个人都是分析师”，因此对问题的认识和为解决方案作出贡献的意愿要高得多，而在传统企业中则低得多。当数据集刚刚创建时，文档化就变得容易得多，因为信息是新鲜的。然而，即使在谷歌，虽然一些热门数据集已经有了很好的文档化，但仍然存在大量暗数据或未文档化的数据。
- en: 'In traditional enterprises, the situation is much worse. There are millions
    of existing data sets (files and tables) that will never get documented by analysts
    unless they are used—but they will never be found and used unless they are documented.
    The only practical solution is to combine crowdsourcing with automation. Waterline
    Data is a tool that my team and I have developed to provide such a solution. It
    takes the information crowdsourced from analysts working with their data sets
    and applies it to all the other dark data sets. The process is called *fingerprinting*:
    the tool crawls through all the structured data in the enterprise, adding a unique
    identifier to each field, and as fields get annotated or tagged by analysts, it
    looks for similar fields and suggests tags for them. When analysts search for
    data sets, they see both data sets tagged by analysts and data sets tagged by
    the tool automatically, and have a chance to either accept or reject these suggested
    tags. The tool then applies machine learning (ML) to improve its automated tagging
    based on the user feedback.'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 在传统企业中，情况更为糟糕。除非这些数据集（文件和表格）被使用，否则分析师永远不会对数百万个现有数据集进行文档化，但如果不文档化，这些数据集就永远不会被找到和使用。唯一的实际解决方案是将众包与自动化结合起来。Waterline
    Data是我和我的团队开发的工具，用于提供这样的解决方案。它采用从分析师工作的数据集中众包收集的信息，并将其应用于所有其他暗数据集。该过程称为*指纹识别*：该工具遍历企业中的所有结构化数据，为每个字段添加唯一标识符，并在分析师对字段进行注释或标记时，寻找类似字段并为其提供标签建议。当分析师搜索数据集时，他们既可以看到由分析师标记的数据集，也可以看到由工具自动标记的数据集，并有机会接受或拒绝这些建议的标签。然后，该工具应用机器学习（ML）根据用户反馈改进其自动标记。
- en: The core idea is that human annotation by itself is not enough, given the scope
    and complexity of the data, while purely automated annotation is undependable
    given the unique and unpredictable characteristics of the data—so, the two have
    to be brought together to achieve the best results. [Figure 1-11](#leveraging_both_human_knowledge_and_mach)
    illustrates the virtuous cycle.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 核心思想是，单靠人工注释无法胜任数据的广度和复杂性，而纯自动化注释又无法靠谱，因为数据具有独特且不可预测的特征——因此，必须将二者结合起来以达到最佳效果。[Figure 1-11](#leveraging_both_human_knowledge_and_mach)
    说明了这种良性循环。
- en: '![Leveraging both human knowledge and machine learning](Images/ebdl_0111.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![利用人类知识和机器学习](Images/ebdl_0111.png)'
- en: Figure 1-11\. Leveraging both human knowledge and machine learning
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图1-11。利用人类知识和机器学习
- en: Accessing and provisioning the data
  id: totrans-124
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 访问和供应数据
- en: 'Once the right data sets have been identified, analysts need to be able to
    use them. Traditionally, access is granted to analysts as they start or join a
    project. It is then rarely taken away, so old-timers end up with access to practically
    all the data in the enterprise that may be even remotely useful, while newbies
    have virtually no access and therefore can’t find or use anything. To solve the
    data access problem for the data lake, enterprises typically go for one of the
    two extremes: they either grant everyone full access to all the data or restrict
    all access unless an analyst can demonstrate a need. Granting full access works
    in some cases, but not in regulated industries. To make it more acceptable, enterprises
    sometimes deidentify sensitive data—but that means they have to do work ingesting
    data that no one may need. Also, as regulations change, more and more data may
    need to be deidentified (this topic will be covered in depth in later chapters).'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦确定了正确的数据集，分析员需要能够使用它们。传统上，分析员在开始或加入项目时被授予访问权限。然后很少会被收回，因此老成员最终会获得对企业中可能有用的几乎所有数据的访问权限，而新人几乎没有访问权限，因此无法找到或使用任何数据。为了解决数据湖中的数据访问问题，企业通常会选择以下两个极端之一：要么向所有人授予对所有数据的完全访问权限，要么除非分析员能证明有需求，否则所有访问权限都会受到限制。在某些情况下，授予完全访问权限是可行的，但在受监管的行业中则不然。为了使其更可接受，企业有时会去识别敏感数据，但这意味着他们必须做一些可能没有人需要的数据摄取工作。此外，随着法规的变化，可能需要去识别的数据会越来越多（这个话题将在后续章节中深入讨论）。
- en: A more practical approach is to publish information about all the data sets
    in a metadata catalog, so analysts can find useful data sets and then request
    access as needed. The requests usually include the justification for access, the
    project that requires the data, and the duration of access required. These requests
    are routed to the data stewards for the requested data. If they approve access,
    it is granted for a period of time. This period may be extended, but it is not
    indefinite, eliminating the legacy access problem. An incoming request may also
    trigger the work to deidentify sensitive data, but now it is done only if and
    when needed.
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 更实际的方法是在元数据目录中发布所有数据集的信息，这样分析员可以找到有用的数据集，然后根据需要申请访问权限。这些请求通常包括访问的理由、需要数据的项目以及所需的访问时长。这些请求会被路由到所需数据的数据管理员那里。如果他们批准访问权限，访问权限将会被授予一段时间。这段时间可以延长，但不是无限期的，从而消除了传统的访问问题。新的请求也可能会触发去识别敏感数据的工作，但现在只有在需要时才会进行。
- en: 'Provisioning or physical access can be granted to the data in a number of ways:'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 数据的供应或物理访问可以通过多种方式授予：
- en: Users can be granted read access to the entire data set.
  id: totrans-128
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户可以被授予对整个数据集的读取权限。
- en: If only partial access should be granted, a copy of the file containing just
    the data appropriate to the user can be created (and kept up to date), or a Hive
    table or view can be created that contains only the fields and rows that the analyst
    should see.
  id: totrans-129
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果只需要部分访问权限，可以创建一个仅包含适合用户数据的文件副本（并保持更新），或者创建一个仅包含分析员应看到的字段和行的Hive表或视图。
- en: If needed, a deidentified version of the data set can be generated that replaces
    sensitive information with randomly generated equivalent information, so all the
    applications still work, but no sensitive data is leaked.
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，可以生成一个去识别版本的数据集，用随机生成的等效信息替换敏感信息，这样所有应用程序仍然能够运行，但不会泄漏敏感数据。
- en: Preparing the data
  id: totrans-131
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 准备数据
- en: 'Occasionally, data comes in perfectly clean and ready for analytics. Unfortunately,
    most of the time, the data needs work to render it appropriate for the analysts.
    Data preparation generally involves the following operations:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，数据是完全干净且可以立即用于分析的。不幸的是，大多数情况下，数据需要经过处理才能使其适合分析。数据准备通常涉及以下操作：
- en: Shaping
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 塑形
- en: Selecting a subset of fields and rows to work on, combining multiple files and
    tables into one (joining), transforming and aggregating, bucketizing (for instance,
    going from discrete values to ranges or buckets—e.g., putting 0- to 18-year-olds
    into the “juvenile” bucket, 19- to 25-year-olds into the “young adult” bucket,
    etc.), converting variables into features (for instance, converting age into a
    feature that has a value of 0 if a person is over 65 and 1 if not), and many other
    possible steps.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 选择要处理的字段和行的子集，将多个文件和表合并成一个（联接），转换和聚合，分桶化（例如，从离散值到范围或桶的转换，例如将0到18岁的人放入“青少年”桶，将19到25岁的人放入“青年”桶等），将变量转换为特征（例如，将年龄转换为一个特征，如果一个人超过65岁则值为0，否则为1），以及许多其他可能的步骤。
- en: Cleaning
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 清洗
- en: Filling in missing values (for instance, guessing a missing gender from the
    first name or looking up the address in an address database), correcting bad values,
    resolving conflicting data, normalizing units of measure and codes to common units,
    and the like.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 填补缺失值（例如，从名字猜测缺失的性别或在地址数据库中查找地址），纠正错误值，解决冲突数据，将计量单位和代码规范化为常见的单位等。
- en: Blending
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 混合
- en: Harmonizing different data sets to the same schema, same units of measure, same
    codes, and so on.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 将不同的数据集调整到相同的模式、相同的计量单位、相同的代码等。
- en: As you can tell from these few examples, a lot of sophisticated work and thinking
    goes into data preparation. Automation is crucial, to take advantage of lessons
    learned by transformations and to avoid repeating the same tedious steps over
    thousands of tables and data sets.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 从这些例子中可以看出，数据准备涉及到大量复杂的工作和思考。自动化至关重要，以利用通过转换所学到的经验，并避免在成千上万个表格和数据集上重复相同的繁琐步骤。
- en: The most common data preparation tool is Excel. Unfortunately, Excel doesn’t
    scale to data lake sizes, but a plethora of new tools provide Excel-like capabilities
    for large-scale data sets. Some, like Trifacta, apply sophisticated machine learning
    techniques to suggest transformations and help analysts prep the data. Many large
    vendors have also debuted data prep tools, and analytics vendors like Tableau
    and Qlik are enhancing data prep capabilities in their tools as well.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 最常见的数据准备工具是Excel。不幸的是，Excel并不适用于数据湖大小，但是大量新工具为大规模数据集提供了类似Excel的功能。例如，Trifacta应用复杂的机器学习技术来建议转换，并帮助分析员准备数据。许多大型供应商也推出了数据准备工具，像Tableau和Qlik等分析供应商也在他们的工具中增强了数据准备功能。
- en: Analysis and visualization
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 分析和可视化
- en: Once data is prepared, it can be analyzed. Analysis ranges from creation of
    simple reports and visualizations to sophisticated advanced analytics and machine
    learning. This is a very mature space, with hundreds of vendors providing solutions
    for every type of analytics. Specifically for Hadoop data lakes, Arcadia Data,
    AtScale, and others provide analysis and visualization tools designed to run natively
    and take advantage of Hadoop’s processing power.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦数据准备好，就可以进行分析。分析范围从创建简单的报告和可视化到复杂的高级分析和机器学习。这是一个非常成熟的领域，数百家供应商为各种类型的分析提供解决方案。特别是对于Hadoop数据湖，Arcadia
    Data、AtScale等公司提供本地运行并利用Hadoop处理能力的分析和可视化工具。
- en: Data Lake Architectures
  id: totrans-143
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据湖架构
- en: Originally, most companies I talked to thought that they would have one huge,
    on-premises data lake that would contain all their data. As their understanding
    and best practices evolved, most enterprises realized that a single go-to point
    was not ideal. Between data sovereignty regulations (e.g., you are not allowed
    to take data out of Germany) and organizational pressures, multiple data lakes
    typically proved to be a better solution. Furthermore, as companies realized the
    complexity of supporting a massively parallel cluster and experienced the frustration
    at their inability to find and hire experienced administrators for Hadoop and
    other big data platforms, they started opting for cloud-based data lakes where
    most hardware and platform components are managed by the experts that work for
    Amazon, Microsoft, Google, and others.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 最初，我与大多数公司的交谈中他们认为他们会拥有一个包含所有数据的庞大的本地数据湖。随着他们的理解和最佳实践的发展，大多数企业意识到单一的去中心化点并不理想。在数据主权法规（例如，禁止将数据带出德国）和组织压力之间，多个数据湖通常被证明是更好的解决方案。此外，随着公司意识到支持大规模并行集群的复杂性，并且因为无法找到和雇佣有经验的Hadoop和其他大数据平台管理员而感到沮丧，他们开始选择由亚马逊、微软、谷歌等公司的专家管理大部分硬件和平台组件的基于云的数据湖。
- en: Data Lakes in the Public Cloud
  id: totrans-145
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 公共云中的数据湖
- en: 'Aside from the benefits of access to big data technology expertise and short
    deployment times, the low cost of storage and the elastic nature of cloud computing
    make this an extremely attractive option for implementing a data lake. Since a
    lot of data is being stored for future use, it makes sense to store it as inexpensively
    as possible. This works well with the cost optimization possibilities supported
    through various storage tiers provided by Amazon and others: access ranges from
    high-speed to glacial, with slower-access media being significantly cheaper.'
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 除了访问大数据技术专业知识和快速部署时间的好处外，存储成本低和云计算的弹性特性使其成为实施数据湖的极具吸引力的选择。由于大量数据被存储以备将来使用，因此将其尽可能廉价地存储是有意义的。这与通过亚马逊等提供的各种存储层级支持的成本优化可能性很好地配合：访问速度从高速到冰川级别不等，而慢速访问媒体的价格显著便宜。
- en: In addition, the elasticity of cloud computing allows a very large cluster to
    be spun up on demand, when needed. Compare this to an on-premises cluster, which
    has a fixed size and stores its data in attached storage (although new architectures
    with network-attached storage are being explored). That means that as nodes fill
    up with data, new nodes need to be added just for storage. Furthermore, if analytic
    loads are CPU-heavy and need more compute power, you need to add nodes even though
    you may only use them for a short time.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，云计算的弹性允许在需要时按需启动非常大的集群。与本地集群相比，后者具有固定大小并将数据存储在附加存储中（尽管正在探索具有网络附加存储的新架构）。这意味着随着节点存满数据，需要添加新节点仅用于存储。此外，如果分析负载CPU密集且需要更多计算能力，则即使可能仅需短时间使用，也需要添加节点。
- en: In the cloud, you pay only for the storage that you need (i.e., you don’t have
    to buy extra compute nodes just to get more storage) and can spin up huge clusters
    for short periods of time. For example, if you have a 100-node on-premises cluster
    and a job that takes 50 hours, it is not practical to buy and install 1,000 nodes
    just to make this one job run faster. In the cloud, however, you would pay about
    the same for the compute power of 100 nodes for 50 hours as you would for 1,000
    nodes for 5 hours. This is the huge advantage of elastic compute.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 在云中，您只需支付所需的存储空间（即不必购买额外的计算节点来获取更多存储空间），并且可以短时间内启动大型集群。例如，如果您有一个100节点的本地集群和一个需要50小时的作业，则购买和安装1000节点仅为了使此作业运行更快是不划算的。然而，在云中，您支付的计算能力大约为100节点50小时的费用，与1000节点5小时的费用相当。这是弹性计算的巨大优势。
- en: Logical Data Lakes
  id: totrans-149
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 逻辑数据湖
- en: Once enterprises realized that having one centralized data lake wasn’t a good
    solution, the idea of the *logical data lake* took hold. With this approach, instead
    of loading all the data into the data lake just in case someone may eventually
    need it, it is made available to analysts through a central catalog or through
    data virtualization software.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 企业意识到单一集中式数据湖并不是一个好的解决方案后，逻辑数据湖的理念开始流行起来。采用这种方法，不是将所有数据加载到数据湖中以备将来可能需要，而是通过集中目录或数据虚拟化软件向分析师提供数据访问。
- en: Logical data lakes address the issues of completeness and redundancy, as illustrated
    in [Figure 1-12](#completeness_and_redundancy_issues).
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 逻辑数据湖解决了完整性和冗余性问题，如[图 1-12](#completeness_and_redundancy_issues)所示。
- en: '![Completeness and redundancy issues](Images/ebdl_0112.png)'
  id: totrans-152
  prefs: []
  type: TYPE_IMG
  zh: '![完整性和冗余性问题](Images/ebdl_0112.png)'
- en: Figure 1-12\. Completeness and redundancy issues
  id: totrans-153
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-12\. 完整性和冗余性问题
- en: 'These issues can be summarized as follows:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 这些问题可以总结如下：
- en: Completeness
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 完整性
- en: How do analysts find the best data set? If the analysts can find only data that
    is already in the data lake, other data that has not been ingested into the data
    lake won’t be found or used (the crescent area on the right in [Figure 1-12](#completeness_and_redundancy_issues)).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 分析师如何找到最佳数据集？如果分析师只能找到已经存在于数据湖中的数据，那么没有被摄取到数据湖中的其他数据将无法找到或使用（如[图 1-12](#completeness_and_redundancy_issues)中右侧的弯月区域所示）。
- en: Redundancy
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 冗余性
- en: If we ingest all the data into the data lake, we will have redundancy between
    the sources of data and the data lake (illustrated as the area of overlap between
    the two circles in [Figure 1-12](#completeness_and_redundancy_issues)). With multiple
    data lakes, to achieve completeness we would need to ingest the same data into
    each data lake.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们将所有数据摄入数据湖，数据源和数据湖之间将存在冗余（如[图 1-12](#completeness_and_redundancy_issues)中两个圆的重叠区域所示）。有了多个数据湖，为了实现完整性，我们需要将相同的数据摄入每个数据湖中。
- en: To make matters worse, there is already a lot of redundancy in the enterprise.
    Traditionally, when a new project is started, the most expedient and politically
    simple approach is for the project team to spin up a new data mart, copy data
    from other sources or the data warehouse, and add its own unique data. This is
    much easier than studying existing data marts and negotiating shared usage with
    current owners and users. As a result, there is a proliferation of data marts
    that are mostly the same. If we blindly load all the data from these data marts
    into the data lake, we will have extremely high levels of redundancy in our lake.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 更糟糕的是，企业中已经存在大量冗余。传统上，在启动新项目时，最便捷和政治上简单的方法是由项目团队启动新的数据集市，从其他来源或数据仓库复制数据，并添加自己的独特数据。这比研究现有数据集市并与当前所有者和用户协商共享使用要容易得多。因此，存在大量几乎相同的数据集市。如果我们将所有这些数据集市中的数据盲目加载到数据湖中，我们的数据湖中将存在极高水平的冗余。
- en: 'The best approach to the completeness and redundancy challenges that I have
    seen involves a couple of simple rules:'
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 我见过的解决完整性和冗余挑战的最佳方法涉及一些简单的规则：
- en: To solve the completeness problem, create a catalog of all the data assets,
    so the analysts can find and request any data set that is available in the enterprise.
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要解决完整性问题，请创建所有数据资产的目录，以便分析师可以找到并请求企业中可用的任何数据集。
- en: 'To solve the redundancy problem, follow the process shown in [Figure 1-13](#managing_data_in_the_logical_data_lake):'
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 要解决冗余问题，请按照[图 1-13](#managing_data_in_the_logical_data_lake)所示的过程进行操作：
- en: Store data that is not stored anywhere else in the data lake.
  id: totrans-163
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 存储在数据湖中没有存储在其他地方的数据。
- en: Bring data that is stored in other systems into the data lake if and when it
    is needed, and keep it in sync while it is needed.
  id: totrans-164
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果需要，将存储在其他系统中的数据带入数据湖，并在需要时保持同步。
- en: Bring each data set in only once for all users.
  id: totrans-165
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将每个数据集仅一次性地为所有用户引入。
- en: '![Managing data in the logical data lake](Images/ebdl_0113.png)'
  id: totrans-166
  prefs: []
  type: TYPE_IMG
  zh: '![管理逻辑数据湖中的数据](Images/ebdl_0113.png)'
- en: Figure 1-13\. Managing data in the logical data lake
  id: totrans-167
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-13\. 管理逻辑数据湖中的数据
- en: Virtualization versus a catalog-based logical data lake
  id: totrans-168
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 虚拟化与基于目录的逻辑数据湖
- en: '*Virtualization* (sometimes also called *federation* or EII, for *enterprise
    information integration*) is a technology developed in 1980s and improved through
    several generations into the 2010s. It basically creates a virtual view or table
    that hides the location and implementation of the physical tables. In [Figure 1-14](#creating_a_custom_data_set_through_a_vie),
    a view is created by joining two tables from different databases. The query would
    then query that view and leave it up to the data virtualization system to figure
    out how to access and join the data in the two databases.'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: '*虚拟化*（有时也称为*联邦化*或EII，用于*企业信息集成*）是上世纪80年代开发的一种技术，并经过几代改进直到2010年代。它基本上创建了一个虚拟视图或表，隐藏了物理表的位置和实现方式。在[图 1-14](#creating_a_custom_data_set_through_a_vie)中，通过连接来自不同数据库的两个表创建了一个视图。然后，查询将查询该视图，并由数据虚拟化系统决定如何访问和连接这两个数据库中的数据。'
- en: '![Creating a custom data set through a view](Images/ebdl_0114.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![通过视图创建自定义数据集](Images/ebdl_0114.png)'
- en: Figure 1-14\. Creating a custom data set through a view
  id: totrans-171
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-14\. 通过视图创建自定义数据集
- en: Although this technology works well for some use cases, in a logical data lake,
    to achieve completeness, it would require every data set to be published as a
    virtual table and kept up to date as underlying table schemas change.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管这项技术对某些用例效果很好，在逻辑数据湖中，为了实现完整性，需要将每个数据集发布为虚拟表，并在底层表模式更改时保持更新。
- en: 'Even if the initial problem of publishing every data asset were solved, views
    still present significant problems:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 即使解决了发布每个数据资产的初始问题，视图仍然存在重大问题：
- en: Creating a virtual view does not make data any easier to find.
  id: totrans-174
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 创建虚拟视图并不会使数据更容易找到。
- en: Joining data from multiple heterogeneous systems is complex and compute-intensive,
    often causing massive loads on the systems and long execution cycles. These so-called
    *distributed joins* of tables that don’t fit into memory are notoriously eresource
    intensive.
  id: totrans-175
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从多个异构系统中联接数据是复杂且计算密集型的，通常会对系统造成巨大的负载和长时间的执行周期。这些所谓的*分布式联接*，涉及到内存容量不足的表，是出名的资源密集型。
- en: By contrast, in the catalog-driven approach, only metadata about each data set
    is published, in order to make it findable. Data sets are then provisioned to
    the same system (e.g., Hadoop cluster) to be processed locally, as demonstrated
    in [Figure 1-15](#providing_metadata_through_a_catalog).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 相比之下，在基于目录的方法中，仅发布关于每个数据集的元数据，以便使其可找到。然后将数据集配置到同一系统（例如 Hadoop 集群）以在本地处理，如[图 1-15](#providing_metadata_through_a_catalog)所示。
- en: '![Providing metadata through a catalog](Images/ebdl_0115.png)'
  id: totrans-177
  prefs: []
  type: TYPE_IMG
  zh: '![通过目录提供元数据](Images/ebdl_0115.png)'
- en: Figure 1-15\. Providing metadata through a catalog
  id: totrans-178
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-15\. 通过目录提供元数据
- en: In addition to making all the data findable and accessible to analysts, an enterprise
    catalog can serve as a single point of access, governance, and auditing, as shown
    in [Figure 1-16](#data_provisioning_and_governance_through). On the top, without
    a centralized catalog, access to data assets is all over the place and difficult
    to manage and track. On the bottom, with the centralized catalog, all requests
    for access go through the catalog. Access is granted on demand for a specific
    period of time and is audited by the system.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 除了使所有数据对分析师可查找和可访问之外，企业目录还可以作为访问、治理和审计的单一访问点，如[图 1-16](#data_provisioning_and_governance_through)所示。顶部，没有集中目录，对数据资产的访问处处都有，难以管理和跟踪。底部，有了集中的目录，所有访问请求都通过目录。按需授予特定时间段的访问权限，并由系统进行审计。
- en: '![Data provisioning and governance through the catalog](Images/ebdl_0116.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![通过目录提供数据配置和治理](Images/ebdl_0116.png)'
- en: Figure 1-16\. Data provisioning and governance through the catalog
  id: totrans-181
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 1-16\. 数据配置和治理通过目录
- en: Conclusion
  id: totrans-182
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 结论
- en: In summary, getting the right platform, loading it with the right data, and
    organizing and setting it up for self-service with a skills- and needs-appropriate
    interface are the keys to creating a successful data lake. In the rest of this
    book, we’ll explore how to accomplish these tasks.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，获取正确的平台，加载正确的数据，并通过技能和需求适当的界面组织和设置为自助服务，是创建成功的数据湖的关键。在本书的其余部分，我们将探讨如何完成这些任务。
