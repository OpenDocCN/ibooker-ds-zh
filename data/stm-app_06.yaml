- en: Chapter 7\. Resource contention
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第7章. 资源竞争
- en: '*This chapter covers*'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '*本章涵盖*'
- en: Contention for worker processes in a Storm cluster
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Storm集群中工作进程的竞争
- en: Memory contention within a worker process (JVM)
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作进程（JVM）内的内存竞争
- en: Memory contention on a worker node
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点上的内存竞争
- en: Worker node CPU contention
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点CPU竞争
- en: Worker node network/socket input/output (I/O) contention
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点网络/套接字输入/输出（I/O）竞争
- en: Worker node disk I/O contention
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 工作节点磁盘I/O竞争
- en: In [chapter 6](kindle_split_014.html#ch06), we discussed tuning at the individual
    topology level. Tuning is an important skill to master and will serve you well
    when you’re deploying topologies to production. But it’s only a small part of
    a bigger picture. Your topology is going to have to coexist on a Storm cluster
    with a variety of other topologies. Some of those topologies will burn CPU doing
    heavy mathematical calculations, some will consume large amounts of network bandwidth,
    and so on and so forth with a variety of resources.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第6章](kindle_split_014.html#ch06)中，我们讨论了在单个拓扑级别进行调优。调优是一项重要的技能，当你将拓扑部署到生产环境中时，它将为你提供良好的服务。但它只是更大图景中的一小部分。你的拓扑将不得不与Storm集群中的各种其他拓扑共存。其中一些拓扑将消耗大量CPU进行复杂的数学计算，一些将消耗大量的网络带宽，等等，涉及各种资源。
- en: In this chapter, we’ll present the various types of resources that can come
    under contention in a Storm cluster and explain how to address each of them. We
    hope that no single Storm cluster would have so many contention issues, so we’ve
    eschewed our usual case study format for a more appropriate cookbook approach.
    Take a quick skim through this chapter to gain a general understanding of the
    types of contention and then refer back to whatever section is relevant to you
    when you start encountering problems.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将介绍在Storm集群中可能发生竞争的各种资源类型，并解释如何解决每个问题。我们希望没有单个Storm集群会有如此多的竞争问题，因此我们放弃了我们通常的案例研究格式，转而采用更合适的食谱方法。快速浏览本章，以获得对竞争类型的一般了解，然后在开始遇到问题时，参考与你相关的任何部分。
- en: The first three recipes in this chapter focus on common solutions for addressing
    several types of contention presented later. We recommend reading through these
    three recipes first because they will give you a better understanding of what
    we’re talking about when we discuss a solution to a particular type of contention.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 本章的前三个食谱专注于解决随后提出的几种类型的竞争的常见解决方案。我们建议首先阅读这三个食谱，因为这将帮助你更好地理解当我们讨论特定类型竞争的解决方案时我们在说什么。
- en: Throughout the chapter we use certain terminology when addressing the resources
    that can come under contention. It’s important to understand what part of a Storm
    deployment we’re referencing when you see certain terms. [Figure 7.1](#ch07fig01)
    highlights these resources, with the key terms in bold. Most of this should already
    be familiar to you, but if not, make sure you take the time to study the terms
    and relationships between the various components before moving forward.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，当我们讨论可能发生竞争的资源时，我们使用某些术语。当你看到某些术语时，了解我们正在引用的Storm部署的哪个部分是很重要的。[图7.1](#ch07fig01)突出了这些资源，其中关键术语以粗体显示。其中大部分你应该已经很熟悉了，但如果不是，确保在继续前进之前花时间研究这些术语以及各种组件之间的关系。
- en: Figure 7.1\. The various types of nodes in a Storm cluster and worker nodes
    broken down as worker processes and their parts
  id: totrans-12
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.1. Storm集群中各种类型的节点以及作为工作进程及其部分的工人节点
- en: '![](07fig01_alt.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig01_alt.jpg)'
- en: With the terminology defined, let’s get started with the first of our “common
    solution” recipes in our cookbook approach, changing the number of worker processes
    (JVMs) running on a worker node. Addressing these “common solution” recipes now
    will provide a nice reference for later and allow us to focus on why each is a
    good solution for a particular scenario.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 定义了术语后，让我们开始我们的“常见解决方案”食谱中的第一个，即更改运行在工作节点上的工作进程（JVM）的数量。现在解决这些“常见解决方案”食谱将为以后提供良好的参考，并使我们能够专注于为什么每个解决方案对于特定场景都是好的。
- en: '|  |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Choosing an operating system when discussing OS-level contentions**'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: '**在讨论操作系统级别的竞争时选择操作系统**'
- en: 'Everyone’s experience administering, maintaining, and diagnosing issues in
    a Storm cluster will vary. We’ve tried to cover the major issues and some of the
    tools you’ll need. But your situation may vary from any we’ve encountered. The
    configuration of your cluster may vary in the number of machines, number of JVMs
    per machine, and so forth. No one can give you the answers for how to set up your
    cluster. The best we can do is present you with guidelines for adjusting to problems
    that arise. Because we’re addressing so many issues that exist at the operating
    system level and because there are so many operating systems that you could be
    running Storm on, we’ve decided to focus on one specific family of operating systems:
    Linux-based.'
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人在管理、维护和诊断Storm集群中的问题的经验都会有所不同。我们试图涵盖主要问题和您将需要的某些工具。但您的情况可能与我们遇到的任何情况都不同。您的集群配置可能在机器数量、每台机器的JVM数量等方面有所不同。没有人能给您提供如何设置您的集群的答案。我们能做的最好的事情是向您提供调整可能出现问题的指南。因为我们正在解决操作系统级别存在的许多问题，并且因为您可以在许多操作系统上运行Storm，所以我们决定专注于一个特定的操作系统家族：基于Linux的。
- en: The OS-level tools discussed in this chapter should be available in every variation
    of Linux. Further, these tools should either exist or have an equivalent in any
    Unix-type OS such as Solaris or FreeBSD. For those of you considering using Windows,
    you’re going to have to do more work to translate the ideas over to your OS, but
    the general principles apply. It’s important to note that our discussion of tool
    usage is far from exhaustive—it’s intended to provide a basis for you to build
    on. To administer and diagnose problems in a production cluster, you’ll be required
    to learn more about the tools and the OS you’re running on. Man pages, search
    engines, the Storm mailing list, IRC channels, and your friendly neighborhood
    operations person are all excellent resources that you should lean on to learn
    more.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 本章讨论的操作系统级工具应该在每种Linux变体中都可用。此外，这些工具应该存在于任何Unix类型的操作系统（如Solaris或FreeBSD）中，或者有等效的工具。对于那些考虑使用Windows的用户，您将不得不做更多的工作来将这些想法转换到您的操作系统上，但一般原则适用。重要的是要注意，我们关于工具使用的讨论远非详尽——它是为了为你提供一个基础来构建。为了管理和诊断生产集群中的问题，您将需要了解更多关于您正在运行的工具和操作系统。手册页、搜索引擎、Storm邮件列表、IRC频道以及您友好的本地操作人员都是您应该依赖的、学习更多知识的优秀资源。
- en: '|  |'
  id: totrans-19
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: 7.1\. Changing the number of worker processes running on a worker node
  id: totrans-20
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.1\. 在工人节点上更改运行的工人进程数量
- en: In several of the recipes throughout this chapter, one of the solutions for
    addressing the contention in question is changing the number of worker processes
    running on a worker node ([figure 7.2](#ch07fig02)).
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的几个食谱中，解决所讨论的争用问题的一个方案是改变在工人节点上运行的工人进程的数量（[图7.2](#ch07fig02)）。
- en: Figure 7.2\. Many worker processes running on a worker node
  id: totrans-22
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.2\. 工人节点上运行的许多工人进程
- en: '![](07fig02_alt.jpg)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![07fig02_alt.jpg](07fig02_alt.jpg)'
- en: In some cases, this means increasing worker processes and in others it means
    decreasing worker processes. It’s such a common solution that we’ve decided to
    break it into its own recipe so you can refer back to this section whenever we
    come across it as a solution.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这意味着增加工人进程的数量，而在其他情况下，这意味着减少工人进程的数量。这是一个如此常见的解决方案，以至于我们决定将其分解为单独的食谱，这样您就可以在遇到它作为解决方案时随时参考这一部分。
- en: 7.1.1\. Problem
  id: totrans-25
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.1\. 问题
- en: You’re experiencing a contention where you need to either increase or decrease
    the number of worker processes running on a worker node.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您正在经历一个争用，您需要增加或减少在工人节点上运行的工人进程的数量。
- en: 7.1.2\. Solution
  id: totrans-27
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.2\. 解决方案
- en: The number of worker processes running on a worker node is defined by the `supervisor.slots.ports`
    property in each worker node’s storm.yaml configuration file. This property defines
    the ports that each worker process will use to listen for messages. The next listing
    shows the default settings for this property.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 工人节点上运行的工人进程的数量由每个工人节点storm.yaml配置文件中的`supervisor.slots.ports`属性定义。该属性定义了每个工人进程将使用哪些端口来监听消息。下面的列表显示了该属性的默认设置。
- en: Listing 7.1\. Default settings for `supervisor.slots.ports`
  id: totrans-29
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.1\. `supervisor.slots.ports`的默认设置
- en: '[PRE0]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'To increase the number of worker processes that can be run on a worker node,
    add a port to this list for each worker process to be added. The opposite holds
    true for decreasing the number of worker processes: remove a port for each worker
    process to be removed.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 要增加可以在工人节点上运行的工人进程的数量，为要添加的每个工人进程添加一个端口到这个列表中。对于减少工人进程的数量，情况相反：为要移除的每个工人进程移除一个端口。
- en: 'After updating this property, you’ll need to restart the Supervisor process
    on the worker node to effect the change. If you installed Storm to /opt/storm,
    as we did in our installation run-through in [chapter 5](kindle_split_013.html#ch05),
    this would require killing the Supervisor process and starting again with the
    following command:'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 更新此属性后，你需要重新启动工作节点上的 Supervisor 进程以生效更改。如果你像我们在第 5 章安装过程中所做的那样，将 Storm 安装到 /opt/storm，这将需要终止
    Supervisor 进程，并使用以下命令重新启动：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Upon restarting, Nimbus will be aware of the updated configuration and send
    messages to only the ports defined in this list.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动后，Nimbus 将会意识到配置的更新，并且只向此列表中定义的端口发送消息。
- en: 7.1.3\. Discussion
  id: totrans-35
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.1.3\. 讨论
- en: Storm defaults to four worker processes per worker node, with each worker process
    listening on ports 6701, 6702, 6703, and 6704, respectively. This is usually good
    enough when you’re first starting to build a cluster, so don’t worry about trying
    to figure out the best configuration right away. But if you do need to add ports,
    be sure to check whether the ports you want to add are already in use by using
    a tool such as `netstat` on Linux.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: Storm 默认为每个工作节点分配四个工作进程，每个工作进程分别监听端口 6701、6702、6703 和 6704。当你刚开始构建集群时，这通常已经足够好了，所以不必担心立即尝试找出最佳配置。但如果你确实需要添加端口，请确保使用
    Linux 上的 `netstat` 等工具检查你想要添加的端口是否已被占用。
- en: Another thing to consider is the number of worker nodes you have in your cluster.
    If widespread changes are needed, updating the configuration and restarting the
    Supervisor process across hundreds or even tens of nodes is a tedious and time-consuming
    task. So we recommend a tool such as Puppet ([http://puppetlabs.com](http://puppetlabs.com))
    for automating the deployment and configuration of each node.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个需要考虑的是你集群中工作节点的数量。如果需要广泛更改，更新配置并在数百甚至数十个节点上重启 Supervisor 进程是一项繁琐且耗时的任务。因此，我们推荐使用像
    Puppet ([http://puppetlabs.com](http://puppetlabs.com)) 这样的工具来自动化每个节点的部署和配置。
- en: 7.2\. Changing the amount of memory allocated to worker processes (JVMs)
  id: totrans-38
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.2\. 更改分配给工作进程（JVMs）的内存量
- en: In a few of the recipes throughout this chapter, one of the solutions for addressing
    the contention in question is changing the amount of memory allocated to worker
    processes (JVMs) on a worker node.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的一些食谱中，解决所讨论的竞争问题的方法之一是改变工作节点上工作进程（JVMs）分配的内存量。
- en: In some cases this means increasing the amount of memory allocated and in others
    it means decreasing memory. Whatever the reason for the solution, the steps for
    changing this setting are the same, which is why we’ve dedicated a separate recipe
    to it.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在某些情况下，这意味着增加分配的内存量，而在其他情况下则意味着减少内存。无论解决方案的原因是什么，更改此设置的步骤都是相同的，这就是为什么我们专门为它编写了一个单独的食谱。
- en: 7.2.1\. Problem
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.1\. 问题
- en: You’re experiencing a contention where you need to either increase or decrease
    the amount of memory being used by the worker processes on a worker node.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 你正在经历一个需要增加或减少工作节点上工作进程（worker processes）使用的内存量的竞争（contention）。
- en: 7.2.2\. Solution
  id: totrans-43
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.2\. 解决方案
- en: The amount of memory allocated to all worker processes (JVMs) on a worker node
    can be changed in the `worker.childopts` property in each worker node’s storm.yaml
    configuration file. This property accepts any valid JVM startup option, providing
    the ability to set the startup options for the initial memory allocation pool
    (`-Xms`) and maximum memory allocation pool (`-Xmx`) for the JVMs on the worker
    node. The following listing shows what this would look like, focusing only on
    the memory-related arguments.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 可以在各个工作节点 storm.yaml 配置文件中的 `worker.childopts` 属性中更改分配给工作节点上所有工作进程（JVMs）的内存量。该属性接受任何有效的
    JVM 启动选项，提供了为工作节点上的 JVM 设置初始内存分配池（`-Xms`）和最大内存分配池（`-Xmx`）启动选项的能力。以下列表显示了这将如何看起来，仅关注与内存相关的参数。
- en: Listing 7.2\. Setting `worker.childopts` in storm.yaml
  id: totrans-45
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.2\. 在 storm.yaml 中设置 `worker.childopts`
- en: '[PRE2]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'It’s important to be aware that changing this property will update all the
    worker processes on a particular worker node. After updating this property, you’ll
    need to restart the Supervisor process on the worker node to effect the change.
    If you installed Storm to /opt/storm, as we did in our installation run-through
    in [chapter 5](kindle_split_013.html#ch05), this would require killing the Supervisor
    process and starting again with the following command:'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 重要的是要意识到更改此属性将更新特定工作节点上的所有工作进程。在更新此属性后，您需要重新启动工作节点上的Supervisor进程以生效更改。如果您像我们在[第5章](kindle_split_013.html#ch05)中的安装演练那样将Storm安装到/opt/storm，这将需要终止Supervisor进程，然后使用以下命令重新启动：
- en: '[PRE3]'
  id: totrans-48
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Upon restarting, all of the worker processes (JVMs) on the worker node should
    be running with the updated memory settings.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 重新启动后，工作节点上的所有工作进程（JVM）都应使用更新的内存设置运行。
- en: 7.2.3\. Discussion
  id: totrans-50
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.2.3. 讨论
- en: One thing to keep in mind when increasing JVM sizes is to make sure the worker
    node (machine/VM) itself has the resources for such size increases. If the worker
    node doesn’t have enough memory to support whatever you set the `–Xmx` value to,
    you’ll need to change the sizing of the actual machines/VMs before changing the
    amount of memory allocated to the JVM.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 在增加JVM大小时，需要注意的一点是确保工作节点（机器/虚拟机）本身有足够的资源来支持这种大小增加。如果工作节点没有足够的内存来支持您设置的`–Xmx`值，您需要在更改分配给JVM的内存量之前更改实际机器/虚拟机的大小。
- en: Another tip we highly recommend following is setting `–Xms` and `–Xmx` to the
    same value. If these values are different, the JVM will manage the heap, sometimes
    increasing and sometimes decreasing the heap size, depending on heap usage. We
    find the overhead of this heap management to be unnecessary and therefore recommend
    setting both to the same value to eliminate any heap management overhead. Along
    with being more efficient, this strategy has the added benefit of making it easier
    to reason about JVM memory usage, because the heap size is a fixed constant for
    the life of the JVM.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 我们强烈推荐遵循的另一个技巧是将`–Xms`和`–Xmx`设置为相同的值。如果这些值不同，JVM将管理堆，有时增加有时减少堆大小，具体取决于堆的使用情况。我们发现这种堆管理的开销是不必要的，因此建议将两者设置为相同的值以消除任何堆管理开销。除了更高效之外，这种策略还有额外的优点，即更容易理解JVM内存使用情况，因为堆大小是JVM生命周期的固定常数。
- en: 7.3\. Figuring out which worker nodes/processes a topology is executing on
  id: totrans-53
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.3.确定拓扑正在执行的工作节点/进程
- en: Many of the recipes in this chapter involve contentions at both the worker node
    and worker process level. Often these contentions will manifest themselves in
    the form of a topology throwing errors in the Storm UI, experiencing reduced throughput,
    or having no throughput at all. In all these scenarios, you’ll most likely need
    to identify which worker nodes and worker processes that particular topology is
    executing on.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 本章中的许多配方都涉及工作节点和工作进程级别的竞争。通常，这些竞争将以拓扑在Storm UI中抛出错误、吞吐量降低或完全没有吞吐量的形式表现出来。在这些所有场景中，您很可能会需要确定特定拓扑正在执行的工作节点和工作进程。
- en: 7.3.1\. Problem
  id: totrans-55
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.1. 问题
- en: You have a problematic topology and need to identify the worker nodes and worker
    processes that topology is executing on.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 您有一个有问题的拓扑，需要确定该拓扑正在执行的工作节点和工作进程。
- en: 7.3.2\. Solution
  id: totrans-57
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.2. 解决方案
- en: The way to do this is by looking at the Storm UI. You want to start out by looking
    at the UI for the specific topology in question. We suggest checking out the Bolts
    section to see if anything looks amiss. As [figure 7.3](#ch07fig03) shows, one
    of the bolts is having issues.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 要这样做，请查看Storm UI。您应该从查看特定拓扑的UI开始。我们建议检查Bolts部分，看看是否有任何异常。如图7.3所示，其中一个bolt存在问题。
- en: Figure 7.3\. Diagnosing issues for a particular topology in the Storm UI
  id: totrans-59
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.3.在Storm UI中诊断特定拓扑的问题
- en: '![](07fig03_alt.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig03_alt.jpg)'
- en: Having identified the problematic bolt, you now want to see more details about
    what’s happening with that bolt. To do so, click on that bolt’s name in the UI
    to get a more detailed view for that bolt. From here, turn your attention to the
    Executors and Errors section for the individual bolt ([figure 7.4](#ch07fig04)).
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 在确定了有问题的bolt之后，您现在想了解更多关于该bolt发生的事情的详细信息。为此，请在UI中单击该bolt的名称以获取该bolt的更详细视图。从这里，将您的注意力转向单个bolt的Executors和Errors部分（[图7.4](#ch07fig04)）。
- en: Figure 7.4\. Looking at the Executors and Errors portion of the Storm UI for
    a particular bolt to determine the type of issue the bolt is having while also
    determining the worker nodes and worker processes that bolt is executing on
  id: totrans-62
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.4\. 查看特定bolt的Storm UI中的Executors和错误部分，以确定bolt遇到的问题类型，同时确定bolt正在执行的工作节点和工作进程
- en: '![](07fig04_alt.jpg)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig04_alt.jpg)'
- en: The Executors section for an individual bolt is of particular interest; this
    tells you which worker nodes and worker processes the bolt is executing on. From
    here, given the type of contention being experienced, you can take the necessary
    steps to identify and solve the problem at hand.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 对于单个bolt的Executors部分特别有趣；这告诉你bolt正在哪些工作节点和工作进程上执行。从这里，根据正在经历的竞争类型，你可以采取必要的步骤来识别和解决问题。
- en: 7.3.3\. Discussion
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.3.3\. 讨论
- en: The Storm UI is your friend. Become familiar with its various screens. It’s
    normally the first place we look when diagnosing any type of contention. Being
    able to quickly identify a problematic topology, bolt, worker node, and worker
    process has been extremely valuable in our experience.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: Storm UI是你的朋友。熟悉它的各种屏幕。通常，我们在诊断任何类型的竞争时首先查看的地方。能够快速识别出有问题的拓扑、bolt、工作节点和工作进程在我们的经验中非常有价值。
- en: Though a great tool, the Storm UI may not always show you what you need. This
    is where additional monitoring can help. This can come in the form of monitoring
    the health of individual worker nodes or custom metrics in your bolt’s code to
    give you a deeper insight into how well the bolt is performing. The bottom line
    here is you shouldn’t rely solely on the Storm UI. Put other measures in place
    to make sure you have coverage everywhere. After all, it’s not a matter of *if*
    something will break; it’s a matter of *when*.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 尽管Storm UI是一个强大的工具，但它可能并不总是显示你需要的信息。这时，额外的监控就能发挥作用。这可以通过监控单个工作节点或你bolt代码中的自定义指标来实现，从而让你更深入地了解bolt的性能。关键在于，你不应该仅仅依赖Storm
    UI。要采取其他措施确保全面覆盖。毕竟，问题不是“是否”会出问题，而是“何时”出问题。
- en: 7.4\. Contention for worker processes in a Storm cluster
  id: totrans-68
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.4\. Storm集群中的工作进程竞争
- en: When you install a Storm cluster, you install it with a fixed number of available
    worker processes across all your worker nodes. Each time you deploy a new topology
    to the cluster, you specify how many worker processes that topology should consume.
    It’s easy to get yourself into a situation where you deploy a topology that requires
    a certain number of worker processes but you can’t obtain those worker processes
    because they’ve all been assigned to existing topologies. This renders the topology
    in question useless, because it can’t process data without worker processes. [Figure
    7.5](#ch07fig05) illustrates this point.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 当你安装一个Storm集群时，你会在所有工作节点上安装一个固定数量的可用工作进程。每次你将新的拓扑部署到集群中时，你都会指定该拓扑应该消耗多少个工作进程。很容易陷入这样的情况：你部署了一个需要一定数量工作进程的拓扑，但你无法获得这些工作进程，因为它们都已经分配给了现有的拓扑。这使得相关的拓扑变得无用，因为它没有工作进程就无法处理数据。[图7.5](#ch07fig05)说明了这一点。
- en: Figure 7.5\. Example Storm cluster where all of the worker processes have been
    assigned to topologies.
  id: totrans-70
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.5\. 示例Storm集群，其中所有工作进程都已分配给拓扑。
- en: '![](07fig05_alt.jpg)'
  id: totrans-71
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig05_alt.jpg)'
- en: '[Figure 7.5](#ch07fig05) illustrates a problem we’ve experienced firsthand
    several times. Fortunately, this problem is easy to detect; it can be found by
    looking at the cluster summary page of the Storm UI ([figure 7.6](#ch07fig06)).'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: '[图7.5](#ch07fig05)展示了我们亲身体验过多次的问题。幸运的是，这个问题很容易检测；可以通过查看Storm UI的集群摘要页面（[图7.6](#ch07fig06)）来找到。 '
- en: 'Figure 7.6\. Storm UI: Zero free slots could mean your topologies are suffering
    from slot contention.'
  id: totrans-73
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.6\. Storm UI：没有空闲槽位可能意味着你的拓扑正遭受槽位竞争。
- en: '![](07fig06_alt.jpg)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig06_alt.jpg)'
- en: 7.4.1\. Problem
  id: totrans-75
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.1\. 问题
- en: You notice a topology isn’t processing any data or has a sudden drop in throughput
    and zero free slots are available, according to the Storm UI.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 根据Storm UI，你注意到一个拓扑没有处理任何数据，或者吞吐量突然下降，且没有空闲槽位可用。
- en: 7.4.2\. Solution
  id: totrans-77
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.2\. 解决方案
- en: 'The bottom line is you have a fixed number of worker processes that can be
    allocated to the topologies requesting them. You can address this problem with
    these strategies:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 核心问题是，你有固定数量的工作进程可以分配给请求它们的拓扑。你可以通过以下策略解决这个问题：
- en: Decreasing the number of worker processes in use by existing topologies
  id: totrans-79
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少现有拓扑使用的工作进程数量
- en: Increasing the total number of worker processes in the cluster
  id: totrans-80
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加集群中工作进程的总数
- en: Decreasing the number of worker processes in use by existing topologies
  id: totrans-81
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 减少现有拓扑使用的工作进程数量
- en: This is the quickest and easiest way to free up slots for other topologies in
    your cluster. But this may or may not be possible depending on the SLAs for your
    existing topologies. If you can reduce the number of worker processes being used
    by a topology without violating the SLA, we recommend this approach.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 这是为集群中的其他拓扑释放槽位的最快和最简单的方法。但这可能或可能不取决于现有拓扑的SLAs。如果你可以在不违反SLA的情况下减少拓扑使用的工作进程数量，我们建议这种方法。
- en: The number of worker processes a topology requests is specified in the code
    for building and submitting your topology to the Storm cluster. The next listing
    shows this code.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 拓扑请求的工作进程数量在构建和提交拓扑到Storm集群的代码中指定。下面的列表显示了此代码。
- en: Listing 7.3\. Configuring the number of worker processes for a topology
  id: totrans-84
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表7.3\. 配置拓扑的工作进程数量
- en: '![](170fig01_alt.jpg)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![](170fig01_alt.jpg)'
- en: If your SLAs don’t allow you to reduce the number of slots being used by any
    of the topologies in your cluster, you’ll have to add new worker processes to
    the cluster.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的服务等级协议（SLAs）不允许你减少集群中任何拓扑使用的槽位数，你将不得不向集群中添加新的工作进程。
- en: Increasing the total number of worker processes in the cluster
  id: totrans-87
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增加集群中工作进程的总数
- en: There are two ways to increase the total number of worker processes in the cluster.
    One is by adding more worker processes to your worker nodes via the steps listed
    in [section 7.1](#ch07lev1sec1). But this won’t work if your worker nodes don’t
    have the resources to support additional JVMs. If this is the case, you’ll need
    to add more worker nodes to your cluster, thus adding to the pool of worker processes.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 有两种方法可以增加集群中工作进程的总数。一种是通过在[第7.1节](#ch07lev1sec1)中列出的步骤向你的工作节点添加更多的工作进程。但如果你的工作节点没有资源来支持额外的JVMs，这就不起作用了。如果是这种情况，你将需要向你的集群添加更多的工作节点，从而增加工作进程池。
- en: We recommend adding new worker nodes if you can. This approach has the least
    impact on existing topologies, because adding worker processes to existing nodes
    has the potential to cause other types of contention that must then be addressed.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你能够的话，我们建议添加新的工作节点。这种方法对现有拓扑的影响最小，因为向现有节点添加工作进程可能会引起其他类型的竞争，然后必须解决这些竞争。
- en: 7.4.3\. Discussion
  id: totrans-90
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.4.3\. 讨论
- en: 'Worker process contention can have a variety of causes, some of which are self-inflicted
    and some of which aren’t. Scenarios include the following:'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 工作进程竞争可能有各种原因，其中一些是自致的，而另一些则不是。场景包括以下：
- en: You deploy a topology that’s configured to consume more worker processes than
    there are slots available in the cluster.
  id: totrans-92
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你部署了一个配置为消耗比集群中可用槽位更多的拓扑的工作进程。
- en: You deploy a topology to your cluster that has no available slots.
  id: totrans-93
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你部署了一个没有可用槽位的拓扑到你的集群中。
- en: A worker node goes down, thus decreasing the number of available slots, possibly
    causing contention among existing topologies.
  id: totrans-94
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个工作节点宕机，从而减少了可用槽位的数量，可能会造成现有拓扑之间的竞争。
- en: It’s important to always be aware of the resources available in your cluster
    when deploying new topologies. If you ignore what’s available within your cluster,
    you can easily affect every topology in your cluster by deploying something that
    consumes too many resources.
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 在部署新拓扑时，始终了解集群中可用的资源非常重要。如果你忽略了集群内的可用资源，你很容易通过部署消耗过多资源的东西来影响集群中的每个拓扑。
- en: 7.5\. Memory contention within a worker process (JVM)
  id: totrans-96
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.5\. 工作进程（JVM）内的内存竞争
- en: Just as you install a Storm cluster with a fixed number of worker processes,
    you also set up each worker process (JVM) with a fixed amount of memory it can
    grow to use. The amount of memory limits the number of threads (executors) that
    can be launched on that JVM—each thread takes a certain amount of memory (the
    default is 1 MB on a 64-bit Linux JVM).
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你使用固定数量的工作进程安装Storm集群一样，你也为每个工作进程（JVM）设置了一个固定数量的内存，它可以增长并使用。内存的数量限制了可以在该JVM上启动的线程（执行器）的数量——每个线程需要一定量的内存（在64位Linux
    JVM上默认为1 MB）。
- en: JVM contention can be a problem on a per-topology basis. The combination of
    memory used by your bolts, spouts, threads, and so forth might exceed that allocated
    to the JVMs they’re running on ([figure 7.7](#ch07fig07)).
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: JVM竞争可能在每个拓扑结构的基础上成为一个问题。你的bolts、spouts、threads等使用的内存组合可能会超过分配给它们运行的JVMs的内存（[图7.7](#ch07fig07)）。
- en: Figure 7.7\. Worker processes, executors and tasks mapping to the JVM, threads
    and instances of spouts/bolts, and the threads/instances contending for memory
    in the same JVM
  id: totrans-99
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.7\. 工作进程、执行器与 JVM 的映射，以及 spouts/bolts 的线程和实例，以及同一 JVM 中争夺内存的线程/实例
- en: '![](07fig07.jpg)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![图片](07fig07.jpg)'
- en: 'JVM contention usually manifests itself as out-of-memory (OOM) errors and/or
    excessively long garbage collection (GC) pauses. OOM errors will appear in the
    Storm logs and Storm UI, usually as a stack trace starting with `java.lang.OutOfMemory-Error:
    Java heap space`.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 'JVM 竞争通常表现为内存不足（OOM）错误和/或过长的垃圾回收（GC）暂停。OOM 错误将出现在 Storm 日志和 Storm UI 中，通常以
    `java.lang.OutOfMemory-Error: Java heap space` 开头的堆栈跟踪。'
- en: Gaining visibility into GC issues requires a little more setup, but it’s something
    that’s easily supported by both the JVM and Storm configuration. The JVM offers
    startup options for tracking and logging GC usage, and Storm provides a way to
    specify JVM startup options for your worker processes. The `worker.childopts`
    property in storm.yaml is where you’d specify these JVM options. The following
    listing shows a sample storm.yaml configuration in a worker node.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 获得GC问题的可见性需要一点更多的设置，但这在 JVM 和 Storm 配置中都是容易支持的。JVM 提供了跟踪和记录 GC 使用情况的启动选项，而 Storm
    提供了一种为您的 worker 进程指定 JVM 启动选项的方法。storm.yaml 中的 `worker.childopts` 属性是您指定这些 JVM
    选项的地方。以下列表显示了工作节点中的示例 storm.yaml 配置。
- en: Listing 7.4\. Setting up GC logging for worker processes
  id: totrans-103
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.4\. 为工作进程设置 GC 日志记录
- en: '![](172fig01_alt.jpg)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](172fig01_alt.jpg)'
- en: One interesting item to note is the value for the `–Xloggc` setting. Remember
    you can have multiple worker processes per worker node. The `worker.childopts`
    property applies to all worker processes on a node, so specifying a regular log
    filename would produce one log file for all the worker processes combined. A separate
    log file per worker process would make tracking GC usage per JVM easier. Storm
    provides a mechanism for logging a specific worker process; the `ID` variable
    is unique for each worker process on a worker node. Therefore, you can add a `"%ID%"`
    string to the GC log filename and you’ll get a separate GC log file for each worker
    process.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 一个值得注意的有趣项目是 `–Xloggc` 设置的值。记住，每个工作节点可以有多个工作进程。`worker.childopts` 属性适用于节点上的所有工作进程，因此指定一个常规日志文件名将生成所有工作进程合并的一个日志文件。为每个工作进程生成单独的日志文件将使跟踪每个
    JVM 的 GC 使用情况更容易。Storm 提供了一种记录特定工作进程日志的机制；`ID` 变量对于工作节点上的每个工作进程都是唯一的。因此，您可以将 `"%ID%"`
    字符串添加到 GC 日志文件名中，您将为每个工作进程获得一个单独的 GC 日志文件。
- en: Reading GC logs can be a little daunting at first, so we’re going to run through
    a quick tutorial outlining what the options in [listing 7.4](#ch07ex04) will produce
    in the associated logs. This listing shows example output for a GC cycle that
    included both a minor (young generation) and major collection (tenured generation).
    It’s entirely possible that not every single GC log statement will include major
    collection statistics, because major collections don’t occur during every GC cycle.
    But for the sake of completeness, we wanted to include both.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 首次阅读 GC 日志可能会有些令人畏惧，所以我们将快速浏览一个教程，概述 [列表 7.4](#ch07ex04) 中的选项将在相关日志中产生什么。此列表显示了包含小（年轻代）和主要收集（持久代）的
    GC 周期的示例输出。完全有可能不是每个 GC 日志语句都会包含主要收集统计信息，因为主要收集并不在每次 GC 周期中发生。但为了完整性，我们希望包括两者。
- en: '|  |'
  id: totrans-107
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**Java generational garbage collection**'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**Java 代际垃圾回收**'
- en: Java uses what’s called generational garbage collection. This means memory is
    divided into different “generations,” and as objects survive enough GC events,
    they get promoted to older generations. Objects will start out in what’s called
    the young generation and eventually get promoted to the tenured generation if
    they survive enough GC events while in young generation. A collection of young
    generation object references is called a minor collection; a collection of tenured
    generation objects is called a major collection.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: Java 使用所谓的代际垃圾回收。这意味着内存被划分为不同的“代”，随着对象经历足够的 GC 事件，它们会被提升到更老的代。对象将开始于所谓的年轻代，如果它们在年轻代中经历了足够的
    GC 事件，最终会被提升到持久代。年轻代对象引用的集合称为小回收；持久代对象的集合称为大回收。
- en: '|  |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Listing 7.5\. Sample GC log output
  id: totrans-111
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.5\. 样本 GC 日志输出
- en: '[PRE4]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Let’s break down each of the parts in this output. [Figure 7.8](#ch07fig08)
    shows the first line, containing the length of time the application has been running
    since the last GC.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一下这个输出中的每一部分。[图7.8](#ch07fig08)显示了第一行，包含自上次GC以来应用程序运行的时间长度。
- en: Figure 7.8\. GC log output showing the output for `–XX:+PrintGCDateStamps`,
    `–XX:+PrintGCTimeStamps`, and `–XX:+PrintGCApplicationConcurrentTime`
  id: totrans-114
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.8\. GC日志输出显示`–XX:+PrintGCDateStamps`、`–XX:+PrintGCTimeStamps`和`–XX:+PrintGCApplicationConcurrentTime`的输出
- en: '![](07fig08_alt.jpg)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig08_alt.jpg)'
- en: The next line is the result of the `–XX:+PrintGCDetails` option and is broken
    down into several figures in order to better explain what’s being represented.
    We’ve excluded the date/timestamps for the sake of keeping the figures simpler.
    [Figure 7.9](#ch07fig09) shows the GC details for the minor collection of the
    young generation.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 下一行是`–XX:+PrintGCDetails`选项的结果，并分解成几个图，以便更好地解释所表示的内容。为了使图更简单，我们排除了日期/时间戳。[图7.9](#ch07fig09)显示了年轻代次要收集的GC细节。
- en: Figure 7.9\. GC log output showing details of a minor garbage collection of
    young generation memory
  id: totrans-117
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.9\. GC日志输出显示年轻代内存的垃圾回收细节
- en: '![](07fig09_alt.jpg)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig09_alt.jpg)'
- en: The GC details for the major collection of the tenured generation are shown
    in [Figure 7.10](#ch07fig10). [Figure 7.11](#ch07fig11) shows the final part of
    the `–XX:+PrintGCDetails` output, which shows the overall heap values along with
    how long the entire GC cycle took.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 旧生代主要收集的GC细节显示在[图7.10](#ch07fig10)中。[图7.11](#ch07fig11)显示了`–XX:+PrintGCDetails`输出的最后部分，它显示了整体堆的值以及整个GC周期所花费的时间。
- en: Figure 7.10\. GC log output showing details of a major garbage collection of
    tenured generation memory
  id: totrans-120
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.10\. GC日志输出显示旧生代内存的主要垃圾回收细节
- en: '![](07fig10_alt.jpg)'
  id: totrans-121
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig10_alt.jpg)'
- en: Figure 7.11\. GC log output showing entire heap values and complete GC time
  id: totrans-122
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.11\. GC日志输出显示整个堆的值和完整的GC时间
- en: '![](07fig11_alt.jpg)'
  id: totrans-123
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig11_alt.jpg)'
- en: 'With the first and second lines of the GC output covered, the last line of
    the output is simple; the `–XX:+PrintGCApplicationStoppedTime` option results
    in a line like the following: `2014-07-27T16:29:29.037+0500: 1.353: Total time
    for which application threads were stopped: 0.0107480 seconds`. This provides
    a more summary-level description of how long the application was paused due to
    GC.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '在覆盖了GC输出的第一行和第二行之后，输出的最后一行很简单；`–XX:+PrintGCApplicationStoppedTime`选项会导致如下类似的行：`2014-07-27T16:29:29.037+0500:
    1.353: 应用程序线程停止的总时间：0.0107480秒`。这提供了对由于GC而暂停的应用程序持续时间的更高级别的描述。'
- en: And that’s it. What looks daunting at first is easily explained when you break
    it down into smaller pieces. Being able to read these logs will help you tremendously
    when debugging JVM contention issues not only in Storm, but in any application
    running on a JVM. With an understanding of how to set up and read GC logs along
    with knowing how to find OOM errors, you’ll be able to identify whether your topologies
    are experiencing JVM contention.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 就这么简单。一开始看起来令人畏惧的事情，当你将其分解成更小的部分时，就会很容易解释。能够阅读这些日志将极大地帮助你在调试Storm中的JVM竞争问题时，以及在任何运行在JVM上的应用程序中。通过了解如何设置和阅读GC日志，以及知道如何找到OOM错误，您将能够确定您的拓扑是否正在经历JVM竞争。
- en: 7.5.1\. Problem
  id: totrans-126
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.1\. 问题
- en: Your spouts and/or bolts are attempting to consume more memory than what has
    been allocated to the JVM, resulting in OOM errors and/or long GC pauses.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 您的spouts和/或bolts正在尝试消耗比分配给JVM的内存更多的内存，导致OOM错误和/或长时间的GC暂停。
- en: 7.5.2\. Solution
  id: totrans-128
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.2\. 解决方案
- en: 'You can address the problem in a couple of ways:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以通过以下几种方式来解决这个问题：
- en: By increasing the number of worker processes being used by the topology in question
  id: totrans-130
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加拓扑中使用的worker进程数量
- en: By increasing the size of your JVMs
  id: totrans-131
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过增加JVM的大小
- en: Increasing the number of worker processes being used by the topology in question
  id: totrans-132
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增加拓扑中使用的worker进程数量
- en: See [section 7.1](#ch07lev1sec1) for steps on doing this. By adding a worker
    process to a topology, you’ll decrease the average load across all worker processes
    for that topology. This should result in a smaller memory footprint for each worker
    process (JVM), hopefully eliminating the JVM memory contention.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 请参阅[第7.1节](#ch07lev1sec1)中的步骤。通过向拓扑中添加一个worker进程，您将减少该拓扑所有worker进程的平均负载。这应该会导致每个worker进程（JVM）的内存占用更小，希望消除JVM内存竞争。
- en: Increasing JVM (worker process) size
  id: totrans-134
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 增加JVM（worker进程）的大小
- en: See [section 7.2](#ch07lev1sec2) for steps on how to do this. Because increasing
    the size of your JVMs could require you to change the size of the machines/VMs
    they’re running on, we recommend the “increase worker processes” solution if you
    can.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 有关如何操作的步骤，请参阅[第7.2节](#ch07lev1sec2)。因为增加JVM的大小可能需要你改变它们运行的机器/VM的大小，所以我们建议如果你可以的话，采用“增加工作进程”的解决方案。
- en: 7.5.3\. Discussion
  id: totrans-136
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.5.3. 讨论
- en: Swapping and balancing memory across JVMs has been one of our biggest challenges
    with Storm. Different topologies will have different memory usage patterns. Over
    time, we’ve gone from having four worker processes per worker node, each using
    500 MB of memory, to two worker processes per worker node, each using 1 GB of
    memory.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 在JVM之间进行交换和平衡内存一直是我们在Storm中遇到的最大挑战之一。不同的拓扑会有不同的内存使用模式。随着时间的推移，我们已经从每个工作节点有四个工作进程，每个使用500
    MB内存，转变为每个工作节点有两个工作进程，每个使用1 GB内存。
- en: Our topologies had high enough parallelism that the cost of memory per thread
    was making tuning at 500 MB problematic. At 1 GB per worker process, we have plenty
    of headroom for most topologies. Some get close to that limit, so we start spreading
    out the load more across multiple worker nodes.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的拓扑具有足够的并行度，每个线程的内存成本使得在500 MB时进行调优变得有困难。每个工作进程1 GB的内存，我们大多数拓扑都有足够的余量。有些接近那个限制，所以我们开始将负载更分散到多个工作节点上。
- en: Don’t worry if you don’t get it right initially. We’ve been running Storm in
    production for a couple of years now and are still tweaking the amount of memory
    per worker process and worker processes per machine as our topologies change,
    grow, and expand. Just remember, this is a never-ending process as the shape of
    your cluster and topologies changes.
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你一开始没有做对，不要担心。我们已经在生产环境中运行Storm两年了，并且随着我们的拓扑改变、增长和扩展，我们仍在调整每个工作进程的内存量和每台机器的工作进程数量。只需记住，这是一个永无止境的过程，因为你的集群和拓扑的形状在不断变化。
- en: Beware when increasing the memory allocated to a JVM; as a rule of thumb, when
    you cross certain key points you’ll notice a change in how long garbage collection
    takes—500 MB, 1 GB, 2 GB, and 4 GB are all around the points when our GC time
    has taken a jump. It’s more art than science, so bring your patience with you.
    There’s nothing more frustrating than addressing OOM issues by increasing JVM
    memory size only to have it noticeably impact GC times.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 当增加分配给JVM的内存时，要小心；一般来说，当你越过某些关键点时，你会注意到垃圾收集时间的变化——500 MB、1 GB、2 GB和4 GB都是我们的GC时间跳跃的点。这更多的是艺术而不是科学，所以带上你的耐心。没有什么比通过增加JVM内存大小来解决OOM问题，却发现它明显影响了GC时间更令人沮丧的。
- en: 7.6\. Memory contention on a worker node
  id: totrans-141
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.6. 工作节点上的内存竞争
- en: Much like how an individual JVM has a limited amount of available memory, so
    does a worker node as a whole. In addition to the memory needed to run your Storm
    worker processes (JVMs), you need memory to run the Supervisor process and any
    other processes on your worker node without swapping ([figure 7.12](#ch07fig12)).
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 就像单个JVM有有限的可用内存一样，整个工作节点也是如此。除了运行你的Storm工作进程（JVMs）所需的内存外，你还需要内存来运行Supervisor进程以及在工作节点上运行的任何其他进程，而不进行交换（[图7.12](#ch07fig12)）。
- en: Figure 7.12\. A worker node has a fixed amount of memory that’s being used by
    its worker processes along with any other processes running on that worker node.
  id: totrans-143
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.12. 工作节点有固定数量的内存，这些内存被其工作进程以及在该工作节点上运行的任何其他进程使用。
- en: '![](07fig12_alt.jpg)'
  id: totrans-144
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig12_alt.jpg)'
- en: If a worker node is experiencing memory contention, that worker node will be
    swapping. *Swapping* is the little death and needs to be avoided if you care about
    latency and throughput. This is a problem when using Storm; each worker node needs
    to have enough memory so that the worker processes and OS don’t swap. If you want
    to maintain consistent performance, you must avoid swapping with Storm’s JVMs.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个工作节点正在经历内存竞争，那么该工作节点将会进行交换。*交换*是小小的死亡，如果你关心延迟和吞吐量，就需要避免它。在使用Storm时，每个工作节点都需要有足够的内存，以便工作进程和操作系统不进行交换。如果你想保持一致的性能，你必须避免使用Storm的JVMs进行交换。
- en: One way to keep an eye on this in Linux is with the `sar` (System Activity Reporter)
    command. This is a Linux-based system statistics command that collects and displays
    all system activities and statistics. We run this command in the format of `sar
    [option] [interval [count]]` ([figure 7.13](#ch07fig13)).
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux中，你可以使用`sar`（系统活动报告）命令来监控这一点。这是一个基于Linux的系统统计命令，它收集并显示所有系统活动和统计信息。我们以`sar
    [option] [interval [count]]`的格式运行这个命令（[图7.13](#ch07fig13)）。
- en: Figure 7.13\. `sar` command breakdown
  id: totrans-147
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.13. `sar`命令分解
- en: '![](07fig13.jpg)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig13.jpg)'
- en: Various options can be passed in to display specific types of statistics. For
    diagnosing worker node memory contention, we use the `–S` option for reporting
    swap space utilization statistics. [Figure 7.14](#ch07fig14) illustrates the output
    for swap space utilization.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 可以传递各种选项来显示特定类型的统计信息。对于诊断工作节点内存争用，我们使用`–S`选项来报告交换空间利用率的统计信息。[图7.14](#ch07fig14)展示了交换空间利用率的输出。
- en: Figure 7.14\. Output of `sar –S 1 3` for reporting swap space utilization
  id: totrans-150
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.14\. `sar –S 1 3`命令报告交换空间利用率的输出
- en: '![](07fig14_alt.jpg)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig14_alt.jpg)'
- en: '|  |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: '**A note on operating system contentions**'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '**关于操作系统争用的说明**'
- en: The only way to avoid contention at the OS level is to sidestep it entirely!
    What do we mean by that? Well, let’s explain.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 避免操作系统级别的争用的唯一方法是完全避开它！我们这是什么意思呢？让我们来解释一下。
- en: If you run a single worker process per worker node, it’s impossible to run into
    contention between workers on that node. This can make maintaining consistent
    performance within a cluster much easier. We know of more than one development
    team that has opted for this approach. If possible, we advise you to seriously
    consider going this route.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您在每个工作节点上运行单个工作进程，那么在该节点上遇到工作进程之间的争用是不可能的。这可以使在集群内保持一致的性能变得容易得多。我们知道不止一个开发团队选择了这种方法。如果可能的话，我们建议您认真考虑走这条路。
- en: This is a nonstarter if you aren’t running in a virtualized environment. The
    cost is simply too high to do this if you’re running on “bare metal” with a single
    OS instance per physical machine. Within a virtualized environment, you’ll use
    more resources by doing this. Assume for a moment that your OS install requires
    *n* GB of disk space and uses 2 GB of memory to run effectively. If you have eight
    workers running on your cluster and you assign four workers per node, you’d use
    *n* * 2 GB of disk and 4 GB of memory to run the OS on your cluster nodes. If
    you were to run a single worker per node, that would skyrocket to *n* * 8 GB of
    disk and 16 GB of memory. That’s a fourfold increase in a rather small cluster.
    Imagine the additional usage that would result if you had a cluster that was 16,
    32, 128, or more nodes in size. If you’re running in an environment such as Amazon
    Web Services (AWS) where you pay per node, the costs can add up quickly. Therefore,
    we suggest this approach only if you’re running in a private virtualized environment
    where the cost of hardware is relatively fixed and you have disk and memory resources
    to spare.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您不在虚拟化环境中运行，这根本无法开始。如果您在单个物理机器上运行单个操作系统实例的“裸机”上运行，成本太高了。在虚拟化环境中，您将使用更多资源来做这件事。假设您的操作系统安装需要*n*
    GB的磁盘空间，并且需要2 GB的内存来有效运行。如果您在您的集群上有八个工作节点，并且每个节点分配四个工作节点，您将使用*n* * 2 GB的磁盘空间和4
    GB的内存来在您的集群节点上运行操作系统。如果您要在每个节点上运行单个工作节点，那么这会激增到*n* * 8 GB的磁盘空间和16 GB的内存。这在相当小的集群中是一个四倍的增长。想象一下，如果您有一个由16、32、128个或更多节点组成的集群，这将导致额外的使用量。如果您在像亚马逊网络服务（AWS）这样的环境中运行，您按节点付费，成本会迅速增加。因此，我们建议只有在您在硬件成本相对固定且您有额外的磁盘和内存资源的私有虚拟化环境中运行时才采用这种方法。
- en: If that limited scenario doesn’t describe you, don’t worry; we have plenty of
    tips in the following pages to help you out as well. And even if it does describe
    you, you’re going to want to familiarize yourself with the following material
    anyway because a single topology can still run up against these problems.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 如果这个有限的场景不符合您的描述，请不要担心；我们在接下来的几页中有很多提示可以帮助您。即使它符合您的描述，您也仍然需要熟悉以下材料，因为单个拓扑结构仍然会遇到这些问题。
- en: '|  |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: 7.6.1\. Problem
  id: totrans-159
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.1\. 问题
- en: Your worker node is swapping due to contention for that node’s memory.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 您的工作节点正在交换，因为对该节点内存的争用。
- en: 7.6.2\. Solution
  id: totrans-161
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.2\. 解决方案
- en: 'Here’s how you can address this problem:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 这是您如何解决这个问题：
- en: Increase the memory available to each worker node. This would mean giving more
    memory to the physical machine or VM, depending on how you configured your cluster.
  id: totrans-163
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加每个工作节点可用的内存。这意味着根据您如何配置您的集群，给物理机器或虚拟机更多的内存。
- en: Lower the collective memory used by worker processes.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 降低工作进程使用的总内存。
- en: Lowering the collective memory used by worker processes
  id: totrans-165
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 降低工作进程使用的总内存
- en: Lowering the collective memory used by worker processes can be done in one of
    two ways. The first is by reducing the number of worker processes per worker node.
    See [section 7.1](#ch07lev1sec1) for the appropriate steps. Reducing the total
    number of worker processes will lower the overall memory footprint of the combined
    remaining processes.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 通过以下两种方式之一降低工作进程使用的总内存。第一种是减少每个工人节点的工作进程数量。参见[第7.1节](#ch07lev1sec1)中的适当步骤。减少总工作进程数量将降低剩余进程组合的整体内存占用。
- en: The second way is by reducing the size of your JVMs. See [section 7.2](#ch07lev1sec2)
    for those steps. Be careful when lowering memory allocated to existing JVMs, though,
    to avoid introducing memory contention within the JVM.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种方法是减小你的JVMs的大小。参见[第7.2节](#ch07lev1sec2)中的这些步骤。不过，在降低现有JVM分配的内存时要小心，以避免在JVM内引入内存竞争。
- en: 7.6.3\. Discussion
  id: totrans-168
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.6.3\. 讨论
- en: Our solution is to always go the route of increasing the memory available to
    each machine. It’s the simplest solution and its resulting ramifications are the
    easiest to understand. If you are tight on memory, lowering memory usage can work,
    but you open yourself up to all the problems we discussed concerning GC and OOM
    on a per-JVM basis. Long story short, if you have the memory to spare, go with
    increasing memory on each machine.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的解决方案是始终选择增加每台机器可用的内存。这是最简单的解决方案，其产生的后果也最容易理解。如果你内存紧张，降低内存使用可能有效，但你会面临我们之前讨论的每个JVM的GC和OOM问题。简而言之，如果你有多余的内存，请在每台机器上增加内存。
- en: 7.7\. Worker node CPU contention
  id: totrans-170
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.7\. 工作节点CPU竞争
- en: Worker node CPU contention occurs when the demand for CPU cycles outstrips the
    amount available. This is a problem when using Storm and is one of the primary
    sources of contention in a Storm cluster. If your Storm topology’s throughput
    is lower than what you expect it to be, you may want to check the worker node(s)
    running your topology to see if CPU contention exists.
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 当对CPU周期的需求超过可用量时，就会发生工作节点CPU竞争。在使用Storm时这是一个问题，也是Storm集群中竞争的主要来源之一。如果你的Storm拓扑的吞吐量低于预期，你可能想检查运行拓扑的工作节点，看看是否存在CPU竞争。
- en: One way to keep an eye on this in Linux is with the `sar` command, passing in
    the option `–u` for displaying real-time CPU usage of all CPUs. [Figure 7.15](#ch07fig15)
    illustrates the output for CPU usage along with the columns you’ll want to keep
    an eye on.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 在Linux中，你可以使用`sar`命令来监控这个问题，通过传递选项`–u`来显示所有CPU的实时CPU利用率。[图7.15](#ch07fig15)展示了CPU利用率以及你需要关注的列。
- en: Figure 7.15\. Output `of sar –u 1 3` for reporting CPU utilization
  id: totrans-173
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.15\. 输出 `sar –u 1 3` 以报告CPU利用率
- en: '![](07fig15_alt.jpg)'
  id: totrans-174
  prefs: []
  type: TYPE_IMG
  zh: '![07fig15_alt.jpg](07fig15_alt.jpg)'
- en: 7.7.1\. Problem
  id: totrans-175
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.7.1\. 问题
- en: The throughput of your topologies is low, and based on running the `sar` command,
    you see that CPU contention exists.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 你的拓扑吞吐量低，根据运行`sar`命令的结果，你看到存在CPU竞争。
- en: 7.7.2\. Solution
  id: totrans-177
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.7.2\. 解决方案
- en: 'To address the problem, you have the following options:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题，你有以下几种选择：
- en: Increasing the number of CPUs available to the machine. This is only possible
    in a virtualized environment.
  id: totrans-179
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加机器可用的CPU数量。这仅在虚拟化环境中才可行。
- en: Upgrading to a more powerful CPU (Amazon Web Services (AWS) type of environment).
  id: totrans-180
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 升级到更强大的CPU（例如亚马逊网络服务(AWS)类型的环境）。
- en: Spreading the JVM load across more worker nodes by lowering the number of worker
    processes per worker node.
  id: totrans-181
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过减少每个工作节点的工作进程数量来将JVM负载分散到更多的工人节点上。
- en: Spreading JVM load across more worker nodes
  id: totrans-182
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 将JVM负载分散到更多的工人节点上
- en: To spread worker process (JVM) load across more worker nodes, you need to reduce
    the number of worker processes running on each worker node (see [section 7.1](#ch07lev1sec1)
    for those steps). Reducing the number of worker processes per worker node results
    in less processing (CPU requests) being done on each worker node. There are two
    scenarios you may find yourself in when attempting this solution. The first is
    you have unused worker processes in your cluster and can therefore reduce the
    number of worker processes on your existing nodes, thus spreading the load ([figure
    7.16](#ch07fig16)).
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将工作进程（JVM）负载分散到更多的工人节点上，你需要减少每个工人节点上运行的工作进程数量（参见[第7.1节](#ch07lev1sec1)中的这些步骤）。减少每个工人节点上的工作进程数量会导致每个工人节点上完成的处理（CPU请求）减少。当你尝试这个解决方案时，可能会遇到两种情况。第一种是你集群中有未使用的工作进程，因此可以减少现有节点上的工作进程数量，从而分散负载([图7.16](#ch07fig16))。
- en: Figure 7.16\. Reducing the number of worker processes per worker node in a cluster
    where there are unused worker processes
  id: totrans-184
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.16\. 在有未使用工作进程的集群中减少每个工作节点的工作进程数量
- en: '![](07fig16_alt.jpg)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig16_alt.jpg)'
- en: The second scenario is where you don’t have any unused worker processes and
    therefore need to add worker nodes in order to reduce the number of worker processes
    per worker node ([figure 7.17](#ch07fig17)).
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种情况是，你没有未使用的工作进程，因此需要添加工作节点来减少每个工作节点的工作进程数量（[图7.17](#ch07fig17)）。
- en: Figure 7.17\. Reducing the number of worker processes per worker node in a cluster
    where there are no unused worker processes, resulting in more worker nodes being
    added
  id: totrans-187
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.17\. 在没有未使用工作进程的集群中减少每个工作节点的工作进程数量，导致添加更多工作节点
- en: '![](07fig17_alt.jpg)'
  id: totrans-188
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig17_alt.jpg)'
- en: Reducing the number of worker processes per worker node is a good way to reduce
    the number of CPU cycles being requested on each node. You just need to be aware
    of what resources are available and in use and act appropriately in your given
    scenario.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 减少每个工作节点的工作进程数量是减少每个节点请求的CPU周期的有效方法。你只需要意识到可用的和正在使用的资源，并在你的特定场景中相应地采取行动。
- en: 7.7.3\. Discussion
  id: totrans-190
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.7.3\. 讨论
- en: If you’re like us and run your own private cloud, the first option is a great
    one. Your Storm nodes are running across different host machines with *x* number
    of CPUs available (in our case, 16). When we first started using Storm, our computational
    needs were much lower, and we assigned a max of two cores to each node. Eventually
    that became problematic and we moved to four and then eight. Most of the time,
    each node isn’t using all the CPU, but it’s there when needed.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你像我们一样运行自己的私有云，第一个选项是一个很好的选择。你的Storm节点正在不同的主机机器上运行，每个机器有*x*个可用的CPU（在我们的情况下，16个）。当我们最初开始使用Storm时，我们的计算需求很低，我们为每个节点分配了最多两个核心。最终这变得有问题，我们改为四个，然后是八个。大多数时候，每个节点并没有使用所有的CPU，但需要时它就在那里。
- en: You can follow the same pattern in AWS and other hosted solutions by upgrading
    to a more powerful CPU and/or number of available cores. But you’re going to hit
    a limit. There’s only so much CPU time to go around among all those guest machines
    running on a single physical box. If you hit that point or can’t scale up CPUs,
    distributing the load across more machines is your only option.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以通过升级到更强大的CPU和/或更多可用核心，在AWS和其他托管解决方案中遵循相同的模式。但你会遇到限制。在单个物理盒子上运行的那么多虚拟机中，CPU时间有限。如果你达到那个点或无法扩展CPU，将负载分配到更多机器是你的唯一选择。
- en: So far, we’ve never had to solve CPU usage issues in this way (but we’ve solved
    others’ issues in such a fashion). And sometimes, we’ve solved the problem entirely
    differently. It turned out that one time our issue was a bug that caused a topology
    to burn CPU needlessly over and over in a tight loop. That’s always what you should
    check for first, but leading with “Are you sure you didn’t mess up?” seemed like
    a less than friendly way to start the discussion.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们从未以这种方式解决过CPU使用问题（但我们以这种方式解决了别人的问题）。有时，我们以完全不同的方式解决了问题。结果发现，有一次我们的问题是由于一个错误，导致拓扑在紧密循环中反复无用地烧毁CPU。这始终是你应该首先检查的，但以“你确定你没有搞砸吗？”开始讨论似乎不是一种友好的方式。
- en: 7.8\. Worker node I/O contention
  id: totrans-194
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.8\. 工作节点I/O竞争
- en: 'I/O contention on a worker node can fall under one of two categories:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 工作节点上的I/O竞争可以分为以下两类：
- en: Disk I/O contention, reading from and writing to the file system
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 磁盘I/O竞争，从文件系统读取和写入
- en: Network/socket I/O contention, reading from and writing to a network via a socket
  id: totrans-197
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 网络套接字I/O竞争，通过套接字从网络读取和写入
- en: Both types of contention are regularly an issue for certain classes of Storm
    topologies. The first step in determining if you’re experiencing either of these
    contentions is to establish whether a worker node is experiencing I/O contention
    in general. Once you do, you can dive down into the exact type of I/O contention
    your worker node is suffering from.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种类型的竞争对于某些类别的Storm拓扑来说是常见问题。确定你是否经历这两种竞争的第一步是确定工作节点是否在总体上经历I/O竞争。一旦你做到了，你就可以深入了解你的工作节点正在遭受的确切类型的I/O竞争。
- en: One way to determine if a worker node in your cluster is experiencing I/O contention
    is by running the `sar` command with the `–u` option for displaying real-time
    CPU usage. This is the same command we ran for CPU contention in [section 7.7](#ch07lev1sec7),
    but this time we’ll focus on a different column in the output ([figure 7.18](#ch07fig18)).
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 确定你的集群中的工作节点是否经历 I/O 竞争的一种方法是通过运行带有 `–u` 选项的 `sar` 命令来显示实时 CPU 使用率。这是我们用于 [第
    7.7 节](#ch07lev1sec7) 中 CPU 竞争的相同命令，但这次我们将关注输出中的不同列（[图 7.18](#ch07fig18)）。
- en: Figure 7.18\. Output of `sar –u 1 3` for reporting CPU utilization and, in particular,
    I/O wait times
  id: totrans-200
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图 7.18\. `sar –u 1 3` 的输出，用于报告 CPU 利用率和，特别是 I/O 等待时间
- en: '![](07fig18_alt.jpg)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig18_alt.jpg)'
- en: A healthy topology that uses a lot of I/O shouldn’t spend a lot of time waiting
    for the resources to become available. That’s why we use 10.00% as the threshold
    at which you start experiencing performance degradation.
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 一个健康的拓扑结构，使用大量的 I/O，不应该花费大量时间等待资源可用。这就是为什么我们使用 10.00% 作为你开始经历性能下降的阈值。
- en: You might think distinguishing between socket/network and disk I/O contention
    is a difficult task, but you’d be surprised at how often your own intuition will
    lead you to the correct choice. Let’s explain.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会认为区分套接字/网络和磁盘 I/O 竞争是一项困难的任务，但你会惊讶于你的直觉经常能引导你做出正确的选择。让我们来解释一下。
- en: 'If you know what topologies are running on a given worker node ([section 7.3](#ch07lev1sec3)
    discusses determining this), you know that they use a lot of network resources
    or disk I/O, and you see `iowait` problems, you can probably safely assume which
    of the two is your issue. Here’s a simple test to help you with that: if you’re
    seeing troubling I/O contention signs, first attempt to determine if you’re suffering
    from socket/network I/O contention. If you aren’t, assume that you’re suffering
    from disk I/O contention. Although this might not always be the case, it can take
    you a long way as you learn the tools of the trade.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你了解在特定工作节点上运行哪些拓扑结构（[第 7.3 节](#ch07lev1sec3) 讨论了如何确定这一点），你就知道它们会使用大量的网络资源或磁盘
    I/O，并且你会看到 `iowait` 问题，你可能会安全地假设这两个问题中的哪一个是你遇到的问题。这里有一个简单的测试可以帮助你确定：如果你看到令人烦恼的
    I/O 竞争迹象，首先尝试确定你是否遭受了套接字/网络 I/O 竞争。如果你没有，那么你很可能遭受的是磁盘 I/O 竞争。尽管这不一定总是情况，但它可以在你学习行业工具时带你走很长的路。
- en: Let’s dive a little deeper into each of the I/O contentions to give you a fuller
    understanding of what we’re talking about.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入探讨每个 I/O 竞争，以便你更全面地了解我们所讨论的内容。
- en: 7.8.1\. Network/socket I/O contention
  id: totrans-206
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.8.1\. 网络/套接字 I/O 竞争
- en: If your topologies interact over a network with external services, network/socket
    I/O contention is bound to be a problem for your cluster. In our experience, the
    main cause for this type of contention is that all of the ports allocated for
    opening sockets are being used.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你的拓扑结构通过网络与外部服务交互，网络/套接字 I/O 竞争很可能是你集群的问题。根据我们的经验，这种竞争的主要原因是为打开套接字分配的所有端口都被使用了。
- en: Most Linux installs will default to 1024 maximum open files/sockets per process.
    In an I/O-intensive topology, it’s easy to hit that limit quickly. We’ve written
    topologies that open several thousand sockets per worker node. To determine the
    limits of your OS, you can examine the /proc filesystem to check your processes
    limits. In order to do this, you’ll first need to know your process ID. Once you
    do that, you can get a listing of all limits for that process. The following listing
    shows you how to use the `ps` and `grep` commands to find your process ID (aka
    PID) and then how to get your process limits from the /proc filesystem.
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 Linux 安装将默认为每个进程最多 1024 个打开文件/套接字。在一个 I/O 密集型的拓扑结构中，很容易迅速达到这个限制。我们已经编写了每个工作节点打开数千个套接字的拓扑结构。为了确定你操作系统的限制，你可以检查
    /proc 文件系统来查看你的进程限制。为了做到这一点，你首先需要知道你的进程 ID。一旦你做到了，你就可以获取该进程的所有限制列表。以下列表显示了如何使用
    `ps` 和 `grep` 命令来查找你的进程 ID（即 PID），然后如何从 /proc 文件系统中获取你的进程限制。
- en: Listing 7.6\. Determining your resource limits
  id: totrans-209
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 列表 7.6\. 确定资源限制
- en: '![](183fig01_alt.jpg)'
  id: totrans-210
  prefs: []
  type: TYPE_IMG
  zh: '![](183fig01_alt.jpg)'
- en: 'If you’re hitting this limit, the Storm UI for your topology should display
    an exception in the “Last Error” column that the max open files limit has been
    reached. This will most likely be a stack trace starting with `java.net.SocketException:
    Too many open files`.'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: '如果你遇到了这个限制，你的拓扑结构的 Storm UI 应该在“最后错误”列中显示一个异常，表明已达到最大打开文件限制。这很可能是以 `java.net.SocketException:
    Too many open files` 开头的堆栈跟踪。'
- en: '|  |'
  id: totrans-212
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '**Dealing with a saturated network link and network/socket I/O-intensive topologies**'
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: '**处理饱和的网络链路和网络/套接字I/O密集型拓扑**'
- en: We’ve never seen a saturated network link, but we know it’s theoretically possible,
    so we mention it here instead of devoting an entire recipe to it. Depending on
    your operating system, you can use various tools to determine whether your network
    link is saturated. For Linux, we recommend `iftop`.
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从未见过饱和的网络链路，但我们知道在理论上这是可能的，所以我们在这里提到它，而不是为它专门写一个完整的配方。根据你的操作系统，你可以使用各种工具来确定你的网络链路是否饱和。对于Linux，我们推荐使用`iftop`。
- en: 'There are two things you can do for a saturated network link: 1) get a faster
    network or 2) lower the number of worker processes per worker node in order to
    spread the load across more machines; this will work as long as you’re overloading
    your local network and not the entire network in general.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 对于饱和的网络链路，你可以做两件事：1) 获取更快的网络或2) 降低每个工作节点上的工作进程数量，以便将负载分散到更多的机器上；只要你的本地网络过载，而不是整个网络过载，这种方法就会有效。
- en: '|  |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: Problem
  id: totrans-217
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: Your topology is experiencing reduced throughput or no throughput at all, and
    you’re seeing errors for hitting the limit of open sockets.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 你的拓扑正在经历吞吐量降低或完全没有吞吐量，并且你看到错误，表明达到了打开套接字数量的限制。
- en: Solution
  id: totrans-219
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: 'A couple of ways to address this problem are as follows:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 解决这个问题的几种方法如下：
- en: Increasing the number of available ports on the worker node
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 增加工作节点上的可用端口
- en: Adding more worker nodes to the cluster
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 向集群中添加更多工作节点
- en: 'For increasing the number of available ports, you’ll need to edit the `/etc/security/limits.conf`
    file on most Linux distributions. You can add lines such as the following:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 为了增加可用端口，你需要在大多数Linux发行版的`/etc/security/limits.conf`文件中进行编辑。你可以添加如下所示的行：
- en: '[PRE5]'
  id: totrans-224
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: These settings will set the hard and soft limit on open files per user. The
    value we’re concerned with as a Storm user is the soft limit. We don’t advise
    going higher than 128k. If you do, then as a rule of thumb (until you learn more
    about soft/hard limits for number of files open on Linux), we suggest setting
    the hard limit to two times the value of the soft limit. Note that you need super-user
    access to change `limits.conf` and you’re going to need to reboot the system to
    make sure they take effect.
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 这些设置将设置每个用户打开文件的数量上的硬限制和软限制。作为Storm用户，我们关注的值是软限制。我们不建议超过128k。如果你这样做，那么作为一个经验法则（直到你了解更多关于Linux上打开文件数量的软/硬限制），我们建议将硬限制设置为软限制的两倍。请注意，你需要超级用户权限来更改`limits.conf`，并且你需要重新启动系统以确保它们生效。
- en: Increasing the number of worker nodes in the cluster will give you access to
    more ports. If you don’t have the resources to add more physical machines or VMs,
    you’ll have to try the first solution.
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 增加集群中的工作节点数量将为你提供更多的端口。如果你没有资源添加更多的物理机器或虚拟机，你将不得不尝试第一种解决方案。
- en: Discussion
  id: totrans-227
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: The first real contention issue we hit was the number of sockets available per
    machine. We use a lot of them because a number of our topologies make lots of
    calls to external services to look up additional information that isn’t available
    from our initial incoming data. Having a high number of sockets available is a
    must. Don’t add more workers on other machines until you’ve increased available
    sockets on each node as much as you can. Once you’ve done that, you should also
    look at your code.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 我们遇到的第一真正争用问题是每台机器上可用的套接字数量。我们使用了很多，因为我们的许多拓扑需要调用外部服务来查找从初始传入数据中不可用的额外信息。拥有大量的可用套接字是必须的。在你尽可能增加每个节点上的可用套接字之前，不要在其他机器上添加更多的工作进程。一旦你做到了这一点，你也应该看看你的代码。
- en: 'Are you opening and closing sockets all the time? If you can keep connections
    open, do that. There’s this wonderful thing called `TCP_WAIT`. It’s where a TCP
    connection will stay open after you close it waiting for any stray data. If you’re
    on a slow network link (like many were when TCP was first designed), this is a
    wonderful idea that helps prevent data corruption. If you’re on a fast modern
    LAN, it’ll drive you insane. You can tune your TCP stack via various OS-specific
    means to lower how long you linger in `TCP_WAIT`, but when you’re making tons
    of network calls, even that won’t be enough. Be smart: open and close connections
    as little as possible.'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 你是否一直在打开和关闭套接字？如果你能保持连接打开，那就这么做。有一个美妙的东西叫做`TCP_WAIT`。这是一个TCP连接在关闭后会保持打开状态，等待任何散乱的数据。如果你在一个慢速网络链路上（就像TCP最初设计时许多人那样），这是一个美妙的主意，有助于防止数据损坏。如果你在一个快速的现代局域网中，这会让你发疯。你可以通过各种操作系统特定的方法调整你的TCP堆栈，以降低你在`TCP_WAIT`中逗留的时间，但当你进行大量的网络调用时，即使这样也不够。要聪明：尽可能少地打开和关闭连接。
- en: 7.8.2\. Disk I/O contention
  id: totrans-230
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 7.8.2\. 磁盘I/O竞争
- en: Disk I/O contention affects how quickly you can write to disk. This could be
    a problem with Storm but should be exceedingly rare. If you’re writing large volumes
    of data to your logs or storing the output of calculations to files on the local
    filesystem, it might be an issue, but that should be unlikely.
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 磁盘I/O竞争会影响你向磁盘写入的速度。这可能是Storm的问题，但应该极其罕见。如果你正在向日志写入大量数据或将计算输出存储在本地文件系统上的文件中，可能会出现这个问题，但这种情况不太可能。
- en: If you have a topology that’s writing data to disk and notice its throughput
    is lower than what you’re expecting, you should check to see if the worker nodes
    it’s running on are experiencing disk I/O contention. For Linux installations,
    you can run a command called `iotop` to get a view of the disk I/O usage for the
    worker nodes in question. This command displays a table of current I/O usage by
    processes/threads in the system, with the most I/O-intensive processes/threads
    listed first. [Figure 7.19](#ch07fig19) shows the command and its associated output,
    along with the parts of the output we’re interested in.
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你有一个将数据写入磁盘的拓扑，并且注意到其吞吐量低于你的预期，你应该检查运行在工作节点上的拓扑是否正在经历磁盘I/O竞争。对于Linux安装，你可以运行一个名为`iotop`的命令来查看相关工作节点的磁盘I/O使用情况。此命令显示系统中进程/线程当前的I/O使用情况表，其中最密集的I/O进程/线程列在前面。[图7.19](#ch07fig19)显示了该命令及其相关输出，以及我们感兴趣的输出部分。
- en: Figure 7.19\. The output for the `iotop` command and what to look for when determining
    if a worker node is experiencing disk I/O contention
  id: totrans-233
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 图7.19\. `iotop`命令的输出以及确定工作节点是否经历磁盘I/O竞争时需要注意的事项
- en: '![](07fig19_alt.jpg)'
  id: totrans-234
  prefs: []
  type: TYPE_IMG
  zh: '![](07fig19_alt.jpg)'
- en: Problem
  id: totrans-235
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 问题
- en: You have a topology that reads/writes to/from disk, and it looks like the worker
    nodes it’s running on are experiencing disk I/O contention.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 你有一个读取/写入磁盘的拓扑，看起来它运行在工作节点上正经历磁盘I/O竞争。
- en: Solution
  id: totrans-237
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 解决方案
- en: To address this problem
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 为了解决这个问题
- en: Write less data to disk. This can mean cutting back within your topology. It
    can also mean putting fewer worker processes on each worker node if multiple worker
    processes are demanding disk on the same worker node.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 减少写入磁盘的数据量。这可能意味着在拓扑中减少数据量。也可能意味着如果多个工作进程在同一工作节点上对磁盘有需求，那么在每个工作节点上放置较少的工作进程。
- en: Get faster disks. This could include using a RAM disk.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 获取更快的磁盘。这可能包括使用RAM磁盘。
- en: If you’re writing to NFS or some other network filesystem, stop immediately.
    Writing to NFS is slow and you’re setting yourself up for disk I/O contention
    if you do.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你正在向NFS或其他网络文件系统写入，请立即停止。向NFS写入很慢，如果你这么做，你将为自己设置磁盘I/O竞争。
- en: Discussion
  id: totrans-242
  prefs:
  - PREF_H5
  type: TYPE_NORMAL
  zh: 讨论
- en: 'Slow disk I/O sucks. It drives us insane. The worst part is fast disks aren’t
    cheap. The disks that we run our Storm workers on are fairly slow. We save our
    fast disks for stuff where we really need speed: Elasticsearch, Solr, Riak, RabbitMQ,
    and similar write-heavy parts of our infrastructure. If you’re writing large amounts
    of data to disk and you don’t have fast disks, you’re going to have to accept
    it as a bottleneck. There’s not much you can do without throwing money at the
    problem.'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 慢速磁盘I/O令人沮丧。它让我们发疯。最糟糕的是，快速磁盘并不便宜。我们在Storm工作节点上运行的磁盘相当慢。我们将快速磁盘留给我们真正需要速度的地方：Elasticsearch、Solr、Riak、RabbitMQ以及我们基础设施中类似的写密集型部分。如果你正在向磁盘写入大量数据，而你又没有快速磁盘，你将不得不接受它作为瓶颈。如果不投入资金解决这个问题，你几乎无能为力。
- en: 7.9\. Summary
  id: totrans-244
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 7.9\. 摘要
- en: 'In this chapter, you learned the following:'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你学习了以下内容：
- en: Several types of contention exist above the topology level, so it’s helpful
    to be able to monitor things like CPU, I/O, and memory usage for the operating
    system your worker nodes are running on.
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在拓扑级别之上存在多种竞争类型，因此能够监控运行在工作节点上的操作系统如CPU、I/O和内存使用情况很有帮助。
- en: It is important to have some level of familiarity with monitoring tools for
    the operating system of the machines/VMs in your cluster. In Linux, these include
    `sar`, `netstat`, and `iotop`.
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于你集群中机器/虚拟机的操作系统监控工具有一定的熟悉程度很重要。在Linux中，这些包括`sar`、`netstat`和`iotop`。
- en: There’s value in knowing common JVM startup options, such as `–Xms`, `-Xmx`,
    and those related to GC logging.
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 了解常见的JVM启动选项很有价值，例如`–Xms`、`-Xmx`以及与GC日志相关的选项。
- en: Although the Storm UI is a great tool for the initial diagnosis of many types
    of contention, it’s smart to have other types of monitoring at the machine/VM
    level to let you know if something is awry.
  id: totrans-249
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然Storm UI是诊断许多类型竞争的出色工具，但拥有机器/虚拟机级别的其他类型监控也很明智，以便你知道是否有问题发生。
- en: Including custom metrics/monitoring in your individual topologies will give
    you valuable insights that the Storm UI may not be able to.
  id: totrans-250
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在你的单个拓扑中包含自定义指标/监控将为你提供Storm UI可能无法提供的宝贵见解。
- en: Be careful when increasing the number of worker processes running on a worker
    node because you can introduce memory and/or CPU contention at the worker node
    level.
  id: totrans-251
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在增加运行在工作节点上的工作进程数量时要小心，因为这可能会在节点级别引入内存和/或CPU竞争。
- en: Be careful when decreasing the number of worker processes running on a worker
    node because you can affect your topologies’ throughput while also introducing
    contention for worker processes across your cluster.
  id: totrans-252
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在减少运行在工作节点上的工作进程数量时要小心，因为这可能会影响你的拓扑吞吐量，同时也会在你的集群中引入对工作进程的竞争。
