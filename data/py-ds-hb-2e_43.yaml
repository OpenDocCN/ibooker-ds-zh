- en: Chapter 38\. Introducing Scikit-Learn
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第38章. 介绍 Scikit-Learn
- en: Several Python libraries provide solid implementations of a range of machine
    learning algorithms. One of the best known is [Scikit-Learn](http://scikit-learn.org),
    a package that provides efficient versions of a large number of common algorithms.
    Scikit-Learn is characterized by a clean, uniform, and streamlined API, as well
    as by very useful and complete documentation. A benefit of this uniformity is
    that once you understand the basic use and syntax of Scikit-Learn for one type
    of model, switching to a new model or algorithm is straightforward.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 几个 Python 库提供了一系列机器学习算法的可靠实现。其中最著名的之一是[Scikit-Learn](http://scikit-learn.org)，它提供了大量常见算法的高效版本。Scikit-Learn
    具有清晰、统一和简化的 API，以及非常有用和完整的文档。统一性的好处在于，一旦你理解了 Scikit-Learn 一种类型模型的基本用法和语法，切换到新模型或算法就变得简单。
- en: This chapter provides an overview of the Scikit-Learn API. A solid understanding
    of these API elements will form the foundation for understanding the deeper practical
    discussion of machine learning algorithms and approaches in the following chapters.
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章概述了 Scikit-Learn API。对这些 API 元素的扎实理解将为理解以下章节中关于机器学习算法和方法的深入实践讨论奠定基础。
- en: We will start by covering data representation in Scikit-Learn, then delve into
    the Estimator API, and finally go through a more interesting example of using
    these tools for exploring a set of images of handwritten digits.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将从 Scikit-Learn 中的数据表示开始讲起，然后深入到估计器 API，最后通过一个更有趣的示例，使用这些工具探索一组手写数字的图像。
- en: Data Representation in Scikit-Learn
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: Scikit-Learn 中的数据表示
- en: Machine learning is about creating models from data; for that reason, we’ll
    start by discussing how data can be represented. The best way to think about data
    within Scikit-Learn is in terms of *tables*.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习是关于从数据中创建模型的；因此，我们将从讨论如何表示数据开始。在 Scikit-Learn 中理解数据的最佳方式是以*表格*的形式思考。
- en: 'A basic table is a two-dimensional grid of data, in which the rows represent
    individual elements of the dataset, and the columns represent quantities related
    to each of these elements. For example, consider the [Iris dataset](https://oreil.ly/TeWYs),
    famously analyzed by Ronald Fisher in 1936\. We can download this dataset in the
    form of a Pandas `DataFrame` using the [Seaborn library](http://seaborn.pydata.org),
    and take a look at the first few items:'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 基本表格是一个二维的数据网格，其中行代表数据集中的单个元素，列代表与这些元素的每一个相关的数量。例如，考虑 1936 年由罗纳德·费舍尔著名分析的[鸢尾花数据集](https://oreil.ly/TeWYs)。我们可以使用[Seaborn
    库](http://seaborn.pydata.org)以 Pandas `DataFrame` 的形式下载这个数据集，并查看前几个条目：
- en: '[PRE0]'
  id: totrans-7
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: Here each row of the data refers to a single observed flower, and the number
    of rows is the total number of flowers in the dataset. In general, we will refer
    to the rows of the matrix as *samples*, and the number of rows as `n_samples`.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的每一行数据都指的是单个观察到的花朵，行数是数据集中花朵的总数。通常，我们将矩阵的行称为*样本*，行数称为`n_samples`。
- en: Likewise, each column of the data refers to a particular quantitative piece
    of information that describes each sample. In general, we will refer to the columns
    of the matrix as *features*, and the number of columns as `n_features`.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 同样，数据的每一列都指代描述每个样本的特定定量信息。通常，我们将矩阵的列称为*特征*，列数称为`n_features`。
- en: The Features Matrix
  id: totrans-10
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 特征矩阵
- en: The table layout makes clear that the information can be thought of as a two-dimensional
    numerical array or matrix, which we will call the *features matrix*. By convention,
    this matrix is often stored in a variable named `X`. The features matrix is assumed
    to be two-dimensional, with shape `[n_samples, n_features]`, and is most often
    contained in a NumPy array or a Pandas `DataFrame`, though some Scikit-Learn models
    also accept SciPy sparse matrices.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 表格布局清晰地表明信息可以被视为二维数字数组或矩阵，我们将其称为*特征矩阵*。按照惯例，这个矩阵通常存储在名为`X`的变量中。特征矩阵被假定为二维的，形状为`[n_samples,
    n_features]`，最常见的情况是包含在 NumPy 数组或 Pandas `DataFrame` 中，尽管一些 Scikit-Learn 模型也接受
    SciPy 稀疏矩阵。
- en: The samples (i.e., rows) always refer to the individual objects described by
    the dataset. For example, a sample might represent a flower, a person, a document,
    an image, a sound file, a video, an astronomical object, or anything else you
    can describe with a set of quantitative measurements.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 样本（即行）始终指代数据集描述的单个对象。例如，样本可以表示一朵花、一个人、一个文档、一个图像、一个声音文件、一个视频、一个天文物体，或者任何你可以用一组定量测量来描述的东西。
- en: The features (i.e., columns) always refer to the distinct observations that
    describe each sample in a quantitative manner. Features are often real-valued,
    but may be Boolean or discrete-valued in some cases.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 特征（即列）始终指的是以定量方式描述每个样本的不同观察结果。特征通常是实值，但在某些情况下可能是布尔值或离散值。
- en: The Target Array
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 目标数组
- en: In addition to the feature matrix `X`, we also generally work with a *label*
    or *target* array, which by convention we will usually call `y`. The target array
    is usually one-dimensional, with length `n_samples`, and is generally contained
    in a NumPy array or Pandas `Series`. The target array may have continuous numerical
    values, or discrete classes/labels. While some Scikit-Learn estimators do handle
    multiple target values in the form of a two-dimensional, `[n_samples, n_targets]`
    target array, we will primarily be working with the common case of a one-dimensional
    target array.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 除了特征矩阵 `X` 外，我们通常还使用一个*标签*或*目标*数组，按照惯例，我们通常称之为 `y`。目标数组通常是一维的，长度为 `n_samples`，通常包含在一个
    NumPy 数组或 Pandas `Series` 中。目标数组可以具有连续的数值，也可以是离散的类别/标签。虽然一些 Scikit-Learn 估计器确实处理多个目标值的情况，形式为二维
    `[n_samples, n_targets]` 目标数组，但我们主要处理的是一维目标数组的常见情况。
- en: 'A common point of confusion is how the target array differs from the other
    feature columns. The distinguishing characteristic of the target array is that
    it is usually the quantity we want to *predict from the features*: in statistical
    terms, it is the dependent variable. For example, given the preceding data we
    may wish to construct a model that can predict the species of flower based on
    the other measurements; in this case, the `species` column would be considered
    the target array.'
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 一个常见的困惑点是目标数组与其他特征列的区别。目标数组的显著特征是它通常是我们希望从特征中*预测出来*的数量：在统计术语中，它是因变量。例如，考虑到前述数据，我们可能希望构建一个能够基于其他测量预测花卉种类的模型；在这种情况下，`species`
    列将被视为目标数组。
- en: With this target array in mind, we can use Seaborn (discussed in [Chapter 36](ch36.xhtml#section-0414-visualization-with-seaborn))
    to conveniently visualize the data (see [Figure 38-1](#fig_0502-introducing-scikit-learn_files_in_output_9_0)).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这个目标数组，我们可以使用 Seaborn（在 [第 36 章](ch36.xhtml#section-0414-visualization-with-seaborn)
    中讨论）方便地可视化数据（参见 [图 38-1](#fig_0502-introducing-scikit-learn_files_in_output_9_0)）。
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: '![output 9 0](assets/output_9_0.png)'
  id: totrans-19
  prefs: []
  type: TYPE_IMG
  zh: '![output 9 0](assets/output_9_0.png)'
- en: Figure 38-1\. A visualization of the Iris dataset^([1](ch38.xhtml#idm45858744412448))
  id: totrans-20
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-1\. 鸢尾花数据集的可视化^([1](ch38.xhtml#idm45858744412448))
- en: 'For use in Scikit-Learn, we will extract the features matrix and target array
    from the `DataFrame`, which we can do using some of the Pandas `DataFrame` operations
    discussed in [Part III](part03.xhtml#section-0300-introduction-to-pandas):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Scikit-Learn 中使用，我们将从 `DataFrame` 中提取特征矩阵和目标数组，我们可以使用一些在 [第三部分](part03.xhtml#section-0300-introduction-to-pandas)
    中讨论过的 Pandas `DataFrame` 操作来完成：
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: '[PRE3]'
  id: totrans-23
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: To summarize, the expected layout of features and target values is visualized
    in [Figure 38-2](#fig_images_in_0502-samples-features).
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下，特征和目标值的预期布局如 [图 38-2](#fig_images_in_0502-samples-features) 所示。
- en: '![05.02 samples features](assets/05.02-samples-features.png)'
  id: totrans-25
  prefs: []
  type: TYPE_IMG
  zh: '![05.02 样本特征](assets/05.02-samples-features.png)'
- en: Figure 38-2\. Scikit-Learn’s data layout^([2](ch38.xhtml#idm45858744315904))
  id: totrans-26
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-2\. Scikit-Learn 的数据布局^([2](ch38.xhtml#idm45858744315904))
- en: With this data properly formatted, we can move on to consider Scikit-Learn’s
    Estimator API.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些数据正确格式化，我们可以继续考虑 Scikit-Learn 的估计器 API。
- en: The Estimator API
  id: totrans-28
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 估计器 API
- en: 'The Scikit-Learn API is designed with the following guiding principles in mind,
    as outlined in the [Scikit-Learn API paper](http://arxiv.org/abs/1309.0238):'
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-Learn API 的设计遵循以下指导原则，如 [Scikit-Learn API 论文](http://arxiv.org/abs/1309.0238)
    所述：
- en: '*Consistency*'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: '*一致性*'
- en: All objects share a common interface drawn from a limited set of methods, with
    consistent documentation.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 所有对象共享从有限一组方法中提取的共同接口，并提供一致的文档。
- en: '*Inspection*'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: '*检查*'
- en: All specified parameter values are exposed as public attributes.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 所有指定的参数值都作为公共属性公开。
- en: '*Limited object hierarchy*'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: '*有限的对象层次*'
- en: Only algorithms are represented by Python classes; datasets are represented
    in standard formats (NumPy arrays, Pandas `DataFrame` objects, SciPy sparse matrices)
    and parameter names use standard Python strings.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: Python 类表示算法，数据集使用标准格式（NumPy 数组、Pandas `DataFrame` 对象、SciPy 稀疏矩阵），参数名称使用标准的
    Python 字符串。
- en: '*Composition*'
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: '*组成*'
- en: Many machine learning tasks can be expressed as sequences of more fundamental
    algorithms, and Scikit-Learn makes use of this wherever possible.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 许多机器学习任务可以表示为更基础算法的序列，并且Scikit-Learn在可能的情况下会利用这一点。
- en: '*Sensible defaults*'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '*合理的默认值*'
- en: When models require user-specified parameters, the library defines an appropriate
    default value.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 当模型需要用户指定的参数时，库会定义一个合适的默认值。
- en: In practice, these principles make Scikit-Learn very easy to use, once the basic
    principles are understood. Every machine learning algorithm in Scikit-Learn is
    implemented via the Estimator API, which provides a consistent interface for a
    wide range of machine learning applications.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在实践中，一旦理解了基本原则，这些原则使得Scikit-Learn非常易于使用。Scikit-Learn中的每个机器学习算法都是通过估计器API实现的，该API为广泛的机器学习应用提供了一致的接口。
- en: Basics of the API
  id: totrans-41
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: API的基础知识
- en: 'Most commonly, the steps in using the Scikit-Learn Estimator API are as follows:'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用Scikit-Learn估计器API的步骤中，最常见的步骤如下：
- en: Choose a class of model by importing the appropriate estimator class from Scikit-Learn.
  id: totrans-43
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过从Scikit-Learn导入适当的估计器类来选择一个模型类。
- en: Choose model hyperparameters by instantiating this class with desired values.
  id: totrans-44
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过用所需值实例化这个类来选择模型超参数。
- en: Arrange data into a features matrix and target vector, as outlined earlier in
    this chapter.
  id: totrans-45
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 按照本章前面概述的方式，将数据安排为特征矩阵和目标向量。
- en: Fit the model to your data by calling the `fit` method of the model instance.
  id: totrans-46
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过调用模型实例的`fit`方法将模型拟合到您的数据中。
- en: 'Apply the model to new data:'
  id: totrans-47
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型应用于新数据：
- en: For supervised learning, often we predict labels for unknown data using the
    `predict` method.
  id: totrans-48
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于监督学习，通常我们使用`predict`方法为未知数据预测标签。
- en: For unsupervised learning, we often transform or infer properties of the data
    using the `transform` or `predict` method.
  id: totrans-49
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于无监督学习，我们经常使用`transform`或`predict`方法来转换或推断数据的属性。
- en: We will now step through several simple examples of applying supervised and
    unsupervised learning methods.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将逐步展示几个简单的示例，应用监督和无监督学习方法。
- en: 'Supervised Learning Example: Simple Linear Regression'
  id: totrans-51
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习示例：简单线性回归
- en: As an example of this process, let’s consider a simple linear regression—that
    is, the common case of fitting a line to <math alttext="left-parenthesis x comma
    y right-parenthesis"><mrow><mo>(</mo> <mi>x</mi> <mo>,</mo> <mi>y</mi> <mo>)</mo></mrow></math>
    data. We will use the following simple data for our regression example (see [Figure 38-3](#fig_0502-introducing-scikit-learn_files_in_output_20_0)).
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 作为这一过程的示例，让我们考虑一个简单的线性回归——即，将一条直线拟合到数据`（x，y）`的常见情况。我们将使用以下简单的数据作为我们回归示例的数据（见[图38-3](#fig_0502-introducing-scikit-learn_files_in_output_20_0)）。
- en: '[PRE4]'
  id: totrans-53
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![output 20 0](assets/output_20_0.png)'
  id: totrans-54
  prefs: []
  type: TYPE_IMG
  zh: '![output 20 0](assets/output_20_0.png)'
- en: Figure 38-3\. Data for linear regression
  id: totrans-55
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图38-3\. 线性回归数据
- en: With this data in place, we can use the recipe outlined earlier. We’ll walk
    through the process in the following sections.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 有了这些数据，我们可以使用前面提到的配方。我们将在接下来的几节中详细介绍这个过程。
- en: 1\. Choose a class of model
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 1\. 选择一个模型类
- en: 'In Scikit-Learn, every class of model is represented by a Python class. So,
    for example, if we would like to compute a simple `LinearRegression` model, we
    can import the linear regression class:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在Scikit-Learn中，每个模型类都由一个Python类表示。因此，例如，如果我们想计算一个简单的`LinearRegression`模型，我们可以导入线性回归类：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Note that other more general linear regression models exist as well; you can
    read more about them in the [`sklearn.linear_model` module documentation](https://oreil.ly/YVOFd).
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，还有其他更一般的线性回归模型存在；您可以在[`sklearn.linear_model`模块文档](https://oreil.ly/YVOFd)中了解更多信息。
- en: 2\. Choose model hyperparameters
  id: totrans-61
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 2\. 选择模型超参数
- en: An important point is that *a class of model is not the same as an instance
    of a model*.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 一个重要的点是，*一个模型类并不等同于一个模型的实例*。
- en: 'Once we have decided on our model class, there are still some options open
    to us. Depending on the model class we are working with, we might need to answer
    one or more questions like the following:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们决定了我们的模型类，还有一些选项是开放给我们的。根据我们正在使用的模型类，我们可能需要回答以下一个或多个类似的问题：
- en: Would we like to fit for the offset (i.e., *y*-intercept)?
  id: totrans-64
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要拟合偏移量（即*y*-截距）吗？
- en: Would we like the model to be normalized?
  id: totrans-65
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望模型被归一化吗？
- en: Would we like to preprocess our features to add model flexibility?
  id: totrans-66
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想要预处理我们的特征以增加模型的灵活性吗？
- en: What degree of regularization would we like to use in our model?
  id: totrans-67
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们希望在我们的模型中使用多少程度的正则化？
- en: How many model components would we like to use?
  id: totrans-68
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们想使用多少个模型组件？
- en: These are examples of the important choices that must be made *once the model
    class is selected*. These choices are often represented as *hyperparameters*,
    or parameters that must be set before the model is fit to data. In Scikit-Learn,
    hyperparameters are chosen by passing values at model instantiation. We will explore
    how you can quantitatively choose hyperparameters in [Chapter 39](ch39.xhtml#section-0503-hyperparameters-and-model-validation).
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 这些是在选择*模型类别确定后*必须做出的重要选择的示例。这些选择通常被表示为*超参数*，或者必须在将模型拟合到数据之前设置的参数。在Scikit-Learn中，通过在模型实例化时传递值来选择超参数。我们将探讨如何可以量化地选择超参数在[第39章](ch39.xhtml#section-0503-hyperparameters-and-model-validation)中。
- en: 'For our linear regression example, we can instantiate the `LinearRegression`
    class and specify that we’d like to fit the intercept using the `fit_intercept`
    hyperparameter:'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的线性回归示例，我们可以实例化`LinearRegression`类，并指定我们希望使用`fit_intercept`超参数来拟合截距：
- en: '[PRE6]'
  id: totrans-71
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Keep in mind that when the model is instantiated, the only action is the storing
    of these hyperparameter values. In particular, we have not yet applied the model
    to any data: the Scikit-Learn API makes very clear the distinction between *choice
    of model* and *application of model to data*.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，当实例化模型时，唯一的操作是存储这些超参数值。特别是，我们还没有将模型应用于任何数据：Scikit-Learn API非常清楚地区分了*模型选择*和*将模型应用于数据*的行为。
- en: 3\. Arrange data into a features matrix and target vector
  id: totrans-73
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3\. 将数据排列成特征矩阵和目标向量
- en: Previously we examined the Scikit-Learn data representation, which requires
    a two-dimensional features matrix and a one-dimensional target array. Here our
    target variable `y` is already in the correct form (a length-`n_samples` array),
    but we need to massage the data `x` to make it a matrix of size `[n_samples, n_features]`.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 之前我们研究了Scikit-Learn的数据表示，这需要一个二维特征矩阵和一个一维目标数组。这里我们的目标变量`y`已经是正确的形式（长度为`n_samples`的数组），但我们需要对数据`x`进行整理，使其成为大小为`[n_samples,
    n_features]`的矩阵。
- en: 'In this case, this amounts to a simple reshaping of the one-dimensional array:'
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，这相当于简单地重新整理一维数组：
- en: '[PRE7]'
  id: totrans-76
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 4\. Fit the model to the data
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 4\. 将模型拟合到数据
- en: 'Now it is time to apply our model to the data. This can be done with the `fit`
    method of the model:'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 现在是将我们的模型应用于数据的时候了。这可以通过模型的`fit`方法来完成：
- en: '[PRE8]'
  id: totrans-79
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'This `fit` command causes a number of model-dependent internal computations
    to take place, and the results of these computations are stored in model-specific
    attributes that the user can explore. In Scikit-Learn, by convention all model
    parameters that were learned during the `fit` process have trailing underscores;
    for example in this linear model, we have the following:'
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 此`fit`命令会导致进行许多依赖于模型的内部计算，并将这些计算的结果存储在用户可以探索的模型特定属性中。在Scikit-Learn中，按照惯例，在`fit`过程中学习的所有模型参数都有尾随的下划线；例如，在这个线性模型中，我们有以下内容：
- en: '[PRE9]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: '[PRE10]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'These two parameters represent the slope and intercept of the simple linear
    fit to the data. Comparing the results to the data definition, we see that they
    are close to the values used to generate the data: a slope of 2 and intercept
    of –1.'
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 这两个参数表示对数据进行简单线性拟合的斜率和截距。将结果与数据定义进行比较，我们看到它们接近用于生成数据的值：斜率为2，截距为-1。
- en: 'One question that frequently comes up regards the uncertainty in such internal
    model parameters. In general, Scikit-Learn does not provide tools to draw conclusions
    from internal model parameters themselves: interpreting model parameters is much
    more a *statistical modeling* question than a *machine learning* question. Machine
    learning instead focuses on what the model *predicts*. If you would like to dive
    into the meaning of fit parameters within the model, other tools are available,
    including the [`statsmodels` Python package](https://oreil.ly/adDFZ).'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 经常出现的一个问题是关于内部模型参数的不确定性。一般来说，Scikit-Learn不提供从内部模型参数本身得出结论的工具：解释模型参数更多是一个*统计建模*问题，而不是一个*机器学习*问题。机器学习更关注模型的*预测*。如果您想深入了解模型内的拟合参数含义，其他工具可用，包括[`statsmodels`
    Python包](https://oreil.ly/adDFZ)。
- en: 5\. Predict labels for unknown data
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5\. 预测未知数据的标签
- en: 'Once the model is trained, the main task of supervised machine learning is
    to evaluate it based on what it says about new data that was not part of the training
    set. In Scikit-Learn, this can be done using the `predict` method. For the sake
    of this example, our “new data” will be a grid of *x* values, and we will ask
    what *y* values the model predicts:'
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦模型训练完成，监督机器学习的主要任务就是基于其对未曾参与训练集的新数据的预测结果进行评估。在Scikit-Learn中，可以使用`predict`方法来实现。为了本示例的目的，我们的“新数据”将是一组*x*值，并且我们会问模型预测什么*y*值：
- en: '[PRE11]'
  id: totrans-87
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'As before, we need to coerce these *x* values into a `[n_samples, n_features]`
    features matrix, after which we can feed it to the model:'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们需要将这些*x*值强制转换为`[n_samples, n_features]`特征矩阵，之后我们可以将其馈送给模型：
- en: '[PRE12]'
  id: totrans-89
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Finally, let’s visualize the results by plotting first the raw data, and then
    this model fit (see [Figure 38-4](#fig_0502-introducing-scikit-learn_files_in_output_41_0)).
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，让我们通过首先绘制原始数据，然后是模型拟合结果来可视化结果（参见[图38-4](#fig_0502-introducing-scikit-learn_files_in_output_41_0)）。
- en: '[PRE13]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: '![output 41 0](assets/output_41_0.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![output 41 0](assets/output_41_0.png)'
- en: Figure 38-4\. A simple linear regression fit to the data
  id: totrans-93
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图38-4. 简单的线性回归拟合数据
- en: Typically the efficacy of the model is evaluated by comparing its results to
    some known baseline, as we will see in the next example.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 通常通过将模型的结果与某些已知基准进行比较来评估模型的效果，我们将在下一个示例中看到。
- en: 'Supervised Learning Example: Iris Classification'
  id: totrans-95
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 监督学习示例：鸢尾花分类
- en: 'Let’s take a look at another example of this process, using the Iris dataset
    we discussed earlier. Our question will be this: given a model trained on a portion
    of the Iris data, how well can we predict the remaining labels?'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们再看一个这个过程的例子，使用我们之前讨论过的鸢尾花数据集。我们的问题是这样的：在一个部分鸢尾花数据上训练的模型，我们能多好地预测剩余标签？
- en: For this task, we will use a simple generative model known as *Gaussian naive
    Bayes*, which proceeds by assuming each class is drawn from an axis-aligned Gaussian
    distribution (see [Chapter 41](ch41.xhtml#section-0505-naive-bayes) for more details).
    Because it is so fast and has no hyperparameters to choose, Gaussian naive Bayes
    is often a good model to use as a baseline classification, before exploring whether
    improvements can be found through more sophisticated models.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 对于这个任务，我们将使用一个称为*高斯朴素贝叶斯*的简单生成模型，它假设每个类别都来自于一个轴对齐的高斯分布（更多细节请参见[第41章](ch41.xhtml#section-0505-naive-bayes)）。由于它非常快速且没有需要选择的超参数，高斯朴素贝叶斯通常是用作基线分类的好模型，然后可以探索是否通过更复杂的模型找到改进。
- en: 'We would like to evaluate the model on data it has not seen before, so we will
    split the data into a *training set* and a *testing set*. This could be done by
    hand, but it is more convenient to use the `train_test_split` utility function:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们希望评估模型在未见过的数据上的表现，因此我们将数据分为*训练集*和*测试集*。这可以手动完成，但使用`train_test_split`实用函数更为方便：
- en: '[PRE14]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'With the data arranged, we can follow our recipe to predict the labels:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 数据整理完毕后，我们可以按照我们的步骤预测标签：
- en: '[PRE15]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Finally, we can use the `accuracy_score` utility to see the fraction of predicted
    labels that match their true values:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，我们可以使用`accuracy_score`实用函数查看预测标签与其真实值匹配的比例：
- en: '[PRE16]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: With an accuracy topping 97%, we see that even this very naive classification
    algorithm is effective for this particular dataset!
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 准确率高达97%，我们看到即使是这种非常天真的分类算法对这个特定数据集也是有效的！
- en: 'Unsupervised Learning Example: Iris Dimensionality'
  id: totrans-105
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习示例：鸢尾花维度
- en: 'As an example of an unsupervised learning problem, let’s take a look at reducing
    the dimensionality of the Iris data so as to more easily visualize it. Recall
    that the Iris data is four-dimensional: there are four features recorded for each
    sample.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 作为无监督学习问题的例子，让我们看看如何降低鸢尾花数据的维度，以便更容易地可视化它。回想一下，鸢尾花数据是四维的：每个样本记录了四个特征。
- en: 'The task of dimensionality reduction centers around determining whether there
    is a suitable lower-dimensional representation that retains the essential features
    of the data. Often dimensionality reduction is used as an aid to visualizing data:
    after all, it is much easier to plot data in two dimensions than in four dimensions
    or more!'
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 降维的任务集中在确定是否存在一个合适的低维表示，以保留数据的基本特征。通常，降维被用作辅助可视化数据的工具：毕竟，在二维中绘制数据比在四维或更多维度中更容易！
- en: Here we will use *principal component analysis* (PCA; see [Chapter 45](ch45.xhtml#section-0509-principal-component-analysis)),
    which is a fast linear dimensionality reduction technique. We will ask the model
    to return two components—that is, a two-dimensional representation of the data.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将使用 *主成分分析*（PCA；见 [第45章](ch45.xhtml#section-0509-principal-component-analysis)），这是一种快速的线性降维技术。我们将要求模型返回两个组件——也就是数据的二维表示。
- en: 'Following the sequence of steps outlined earlier, we have:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 按照前面概述的步骤序列，我们有：
- en: '[PRE17]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: Now let’s plot the results. A quick way to do this is to insert the results
    into the original Iris `DataFrame`, and use Seaborn’s `lmplot` to show the results
    (see [Figure 38-5](#fig_0502-introducing-scikit-learn_files_in_output_53_0)).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们绘制结果。一个快速的方法是将结果插入到原始的鸢尾`DataFrame`中，并使用 Seaborn 的 `lmplot` 来显示结果（见 [图 38-5](#fig_0502-introducing-scikit-learn_files_in_output_53_0)）。
- en: '[PRE18]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: We see that in the two-dimensional representation, the species are fairly well
    separated, even though the PCA algorithm had no knowledge of the species labels!
    This suggests to us that a relatively straightforward classification will probably
    be effective on the dataset, as we saw before.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到，在二维表示中，物种相当分离，即使 PCA 算法没有物种标签的知识！这向我们暗示，一个相对简单的分类对数据集可能是有效的，就像我们之前看到的那样。
- en: '![output 53 0](assets/output_53_0.png)'
  id: totrans-114
  prefs: []
  type: TYPE_IMG
  zh: '![output 53 0](assets/output_53_0.png)'
- en: Figure 38-5\. The Iris data projected to two dimensions^([3](ch38.xhtml#idm45858743482640))
  id: totrans-115
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-5\. 将 Iris 数据投影到二维空间^([3](ch38.xhtml#idm45858743482640))
- en: 'Unsupervised Learning Example: Iris Clustering'
  id: totrans-116
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习示例：鸢尾花聚类
- en: Let’s next look at applying clustering to the Iris data. A clustering algorithm
    attempts to find distinct groups of data without reference to any labels. Here
    we will use a powerful clustering method called a *Gaussian mixture model* (GMM),
    discussed in more detail in [Chapter 48](ch48.xhtml#section-0512-gaussian-mixtures).
    A GMM attempts to model the data as a collection of Gaussian blobs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来让我们看一下将聚类应用到鸢尾数据上。聚类算法试图找到不同的数据组，而不考虑任何标签。在这里，我们将使用一个强大的聚类方法，称为 *高斯混合模型*（GMM），在
    [第48章](ch48.xhtml#section-0512-gaussian-mixtures) 中有更详细的讨论。GMM 试图将数据建模为高斯斑点的集合。
- en: 'We can fit the Gaussian mixture model as follows:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以按如下方式拟合高斯混合模型：
- en: '[PRE19]'
  id: totrans-119
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: As before, we will add the cluster label to the Iris `DataFrame` and use Seaborn
    to plot the results (see [Figure 38-6](#fig_0502-introducing-scikit-learn_files_in_output_58_0)).
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 与之前一样，我们将把集群标签添加到鸢尾`DataFrame`中，并使用 Seaborn 绘制结果（见 [图 38-6](#fig_0502-introducing-scikit-learn_files_in_output_58_0)）。
- en: '[PRE20]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '![output 58 0](assets/output_58_0.png)'
  id: totrans-122
  prefs: []
  type: TYPE_IMG
  zh: '![output 58 0](assets/output_58_0.png)'
- en: Figure 38-6\. k-means clusters within the Iris data^([4](ch38.xhtml#idm45858743295632))
  id: totrans-123
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-6\. Iris 数据中的 k-means 聚类^([4](ch38.xhtml#idm45858743295632))
- en: 'By splitting the data by cluster number, we see exactly how well the GMM algorithm
    has recovered the underlying labels: the *setosa* species is separated perfectly
    within cluster 0, while there remains a small amount of mixing between *versicolor*
    and *virginica*. This means that even without an expert to tell us the species
    labels of the individual flowers, the measurements of these flowers are distinct
    enough that we could *automatically* identify the presence of these different
    groups of species with a simple clustering algorithm! This sort of algorithm might
    further give experts in the field clues as to the relationships between the samples
    they are observing.'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 通过按簇号拆分数据，我们可以看到 GMM 算法已经完美地恢复了底层标签：*setosa* 物种在簇 0 中完美分离，而 *versicolor* 和 *virginica*
    之间仍然存在少量混合。这意味着即使没有专家告诉我们单个花的物种标签，这些花的测量也是足够明显的，以至于我们可以使用简单的聚类算法*自动*识别出这些不同物种群！这种算法可能进一步给领域专家提供关于他们正在观察的样本之间关系的线索。
- en: 'Application: Exploring Handwritten Digits'
  id: totrans-125
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 应用：探索手写数字
- en: 'To demonstrate these principles on a more interesting problem, let’s consider
    one piece of the optical character recognition problem: the identification of
    handwritten digits. In the wild, this problem involves both locating and identifying
    characters in an image. Here we’ll take a shortcut and use Scikit-Learn’s set
    of preformatted digits, which is built into the library.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在一个更有趣的问题上演示这些原则，让我们考虑光学字符识别问题的一部分：手写数字的识别。在实际情况中，这个问题涉及到在图像中定位和识别字符。在这里，我们将采取捷径，使用
    Scikit-Learn 的预格式化数字集，这些数字集内置于库中。
- en: Loading and Visualizing the Digits Data
  id: totrans-127
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 加载和可视化数字数据
- en: 'We can use Scikit-Learn’s data access interface to take a look at this data:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 Scikit-Learn 的数据访问接口来查看这些数据：
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'The images data is a three-dimensional array: 1,797 samples each consisting
    of an 8 × 8 grid of pixels. Let’s visualize the first hundred of these (see [Figure 38-7](#fig_0502-introducing-scikit-learn_files_in_output_65_0)).'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 图像数据是一个三维数组：每个样本由一个 8 × 8 的像素网格组成，共 1,797 个样本。让我们可视化其中的前一百个（参见[图 38-7](#fig_0502-introducing-scikit-learn_files_in_output_65_0)）。
- en: '[PRE22]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: '![output 65 0](assets/output_65_0.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![output 65 0](assets/output_65_0.png)'
- en: Figure 38-7\. The handwritten digits data; each sample is represented by one
    8 × 8 grid of pixels
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-7\. 手写数字数据；每个样本由一个 8 × 8 的像素网格表示
- en: 'In order to work with this data within Scikit-Learn, we need a two-dimensional,
    `[n_samples, n_features]` representation. We can accomplish this by treating each
    pixel in the image as a feature: that is, by flattening out the pixel arrays so
    that we have a length-64 array of pixel values representing each digit. Additionally,
    we need the target array, which gives the previously determined label for each
    digit. These two quantities are built into the digits dataset under the `data`
    and `target` attributes, respectively:'
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 为了在 Scikit-Learn 中处理这些数据，我们需要一个二维的 `[n_samples, n_features]` 表示。我们可以通过将图像中的每个像素视为一个特征来实现这一点：即通过展开像素数组，使得我们有一个长度为
    64 的数组，其中包含代表每个数字的像素值。此外，我们还需要目标数组，它给出了每个数字的预先确定标签。这两个量已经内置在 digits 数据集的 `data`
    和 `target` 属性中了：
- en: '[PRE23]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: '[PRE24]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We see here that there are 1,797 samples and 64 features.
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里看到有 1,797 个样本和 64 个特征。
- en: 'Unsupervised Learning Example: Dimensionality Reduction'
  id: totrans-138
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 无监督学习示例：降维
- en: 'We’d like to visualize our points within the 64-dimensional parameter space,
    but it’s difficult to effectively visualize points in such a high-dimensional
    space. Instead, we’ll reduce the number of dimensions, using an unsupervised method.
    Here, we’ll make use of a manifold learning algorithm called Isomap (see [Chapter 46](ch46.xhtml#section-0510-manifold-learning))
    and transform the data to two dimensions:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们想在 64 维参数空间内可视化我们的点，但在这么高维空间中有效地可视化点是困难的。因此，我们将通过无监督方法减少维度。在这里，我们将使用一个称为 Isomap
    的流形学习算法（参见[第 46 章](ch46.xhtml#section-0510-manifold-learning)），将数据转换为二维：
- en: '[PRE25]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: We see that the projected data is now two-dimensional. Let’s plot this data
    to see if we can learn anything from its structure (see [Figure 38-8](#fig_0502-introducing-scikit-learn_files_in_output_73_0)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 我们看到投影后的数据现在是二维的。让我们绘制这些数据，看看我们是否可以从它的结构中学到一些东西（参见[图 38-8](#fig_0502-introducing-scikit-learn_files_in_output_73_0)）。
- en: '[PRE26]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: 'This plot gives us some good intuition into how well various numbers are separated
    in the larger 64-dimensional space. For example, zeros and ones have very little
    overlap in the parameter space. Intuitively, this makes sense: a zero is empty
    in the middle of the image, while a one will generally have ink in the middle.
    On the other hand, there seems to be a more or less continuous spectrum between
    ones and fours: we can understand this by realizing that some people draw ones
    with “hats” on them, which causes them to look similar to fours.'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 这个图表让我们对在较大的 64 维空间中各种数字的分离程度有了一些直观的认识。例如，零和一在参数空间中几乎没有重叠。直觉上这是有道理的：零在图像中间是空的，而一通常在图像中间有墨水。另一方面，一和四之间似乎有一个更或多或少连续的谱系：我们可以通过意识到有些人在一上画有“帽子”，这使它们看起来与四相似。
- en: 'Overall, however, despite some mixing at the edges, the different groups appear
    to be fairly well localized in the parameter space: this suggests that even a
    very straightforward supervised classification algorithm should perform suitably
    on the full high-dimensional dataset. Let’s give it a try.'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，尽管在边缘处有些混合，不同的组在参数空间中似乎被相当好地定位：这表明即使是非常简单的监督分类算法也应该在完整的高维数据集上表现适当。让我们试一试。
- en: '![output 73 0](assets/output_73_0.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![output 73 0](assets/output_73_0.png)'
- en: Figure 38-8\. An Isomap embedding of the digits data
  id: totrans-146
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-8\. 数字数据的 Isomap 嵌入
- en: Classification on Digits
  id: totrans-147
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数字分类
- en: 'Let’s apply a classification algorithm to the digits data. As we did with the
    Iris data previously, we will split the data into training and testing sets and
    fit a Gaussian naive Bayes model:'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对手写数字数据应用一个分类算法。与之前处理鸢尾花数据集时一样，我们将数据分为训练集和测试集，并拟合一个高斯朴素贝叶斯模型：
- en: '[PRE27]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: '[PRE28]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'Now that we have the model’s predictions, we can gauge its accuracy by comparing
    the true values of the test set to the predictions:'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了模型的预测结果，我们可以通过比较测试集的真实值和预测值来评估其准确性：
- en: '[PRE29]'
  id: totrans-152
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: With even this very simple model, we find about 83% accuracy for classification
    of the digits! However, this single number doesn’t tell us where we’ve gone wrong.
    One nice way to do this is to use the *confusion matrix*, which we can compute
    with Scikit-Learn and plot with Seaborn (see [Figure 38-9](#fig_0502-introducing-scikit-learn_files_in_output_81_0)).
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 即使是这个非常简单的模型，我们在数字分类上也达到了约83%的准确率！但是，这个单一数字并不能告诉我们哪里出了错。一个很好的方法是使用*混淆矩阵*，我们可以用Scikit-Learn计算它，并用Seaborn绘制（参见[图 38-9](#fig_0502-introducing-scikit-learn_files_in_output_81_0)）。
- en: '[PRE30]'
  id: totrans-154
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'This shows us where the mislabeled points tend to be: for example, many of
    the twos here are misclassified as either ones or eights.'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 这显示了误标记点的位置倾向：例如，这里的许多数字“2”被误分类为“1”或“8”。
- en: '![output 81 0](assets/output_81_0.png)'
  id: totrans-156
  prefs: []
  type: TYPE_IMG
  zh: '![output 81 0](assets/output_81_0.png)'
- en: Figure 38-9\. A confusion matrix showing the frequency of misclassifications
    by our classifier
  id: totrans-157
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-9\. 混淆矩阵显示分类器误分类的频率
- en: Another way to gain intuition into the characteristics of the model is to plot
    the inputs again, with their predicted labels. We’ll use green for correct labels
    and red for incorrect labels; see [Figure 38-10](#fig_0502-introducing-scikit-learn_files_in_output_83_0).
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 另一种直观了解模型特性的方法是重新绘制输入数据及其预测标签。我们将使用绿色表示正确标签，红色表示错误标签；详见[图 38-10](#fig_0502-introducing-scikit-learn_files_in_output_83_0)。
- en: '[PRE31]'
  id: totrans-159
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: Examining this subset of the data can give us some insight into where the algorithm
    might be not performing optimally. To go beyond our 83% classification success
    rate, we might switch to a more sophisticated algorithm such as support vector
    machines (see [Chapter 43](ch43.xhtml#section-0507-support-vector-machines)),
    random forests (see [Chapter 44](ch44.xhtml#section-0508-random-forests)), or
    another classification approach.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 检查数据子集可以帮助我们了解算法在哪些地方可能表现不佳。为了超越我们的83%分类成功率，我们可以转向更复杂的算法，如支持向量机（参见[第 43 章](ch43.xhtml#section-0507-support-vector-machines)）、随机森林（参见[第 44
    章](ch44.xhtml#section-0508-random-forests)）或其他分类方法。
- en: '![output 83 0](assets/output_83_0.png)'
  id: totrans-161
  prefs: []
  type: TYPE_IMG
  zh: '![output 83 0](assets/output_83_0.png)'
- en: Figure 38-10\. Data showing correct (green) and incorrect (red) labels; for
    a color version of this plot, see the [online version of the book](https://oreil.ly/PDSH_GitHub)
  id: totrans-162
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 38-10\. 数据显示正确（绿色）和错误（红色）标签；查看这个图的彩色版本，请参阅[书的在线版本](https://oreil.ly/PDSH_GitHub)
- en: Summary
  id: totrans-163
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter we covered the essential features of the Scikit-Learn data representation
    and the Estimator API. Regardless of the type of estimator used, the same import/instantiate/fit/predict
    pattern holds. Armed with this information, you can explore the Scikit-Learn documentation
    and try out various models on your data.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们介绍了Scikit-Learn数据表示和估计器API的基本特性。无论使用何种类型的估计器，都保持着相同的导入/实例化/拟合/预测模式。掌握了这些信息，您可以探索Scikit-Learn文档，并在您的数据上尝试各种模型。
- en: 'In the next chapter, we will explore perhaps the most important topic in machine
    learning: how to select and validate your model.'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一章中，我们将探讨机器学习中可能最重要的主题：如何选择和验证您的模型。
- en: ^([1](ch38.xhtml#idm45858744412448-marker)) A full-size, full-color version
    of this figure can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ^([1](ch38.xhtml#idm45858744412448-marker)) 这个图的全尺寸、全彩色版本可以在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
- en: ^([2](ch38.xhtml#idm45858744315904-marker)) Code to produce this figure can
    be found in the [online appendix](https://oreil.ly/J8V6U).
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ^([2](ch38.xhtml#idm45858744315904-marker)) 可在[在线附录](https://oreil.ly/J8V6U)中找到生成此图的代码。
- en: ^([3](ch38.xhtml#idm45858743482640-marker)) A full-color version of this figure
    can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: ^([3](ch38.xhtml#idm45858743482640-marker)) 这个图的全彩色版本可以在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
- en: ^([4](ch38.xhtml#idm45858743295632-marker)) A full-size, full-color version
    of this figure can be found on [GitHub](https://oreil.ly/PDSH_GitHub).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: ^([4](ch38.xhtml#idm45858743295632-marker)) 这个图的全尺寸、全彩色版本可以在[GitHub](https://oreil.ly/PDSH_GitHub)上找到。
