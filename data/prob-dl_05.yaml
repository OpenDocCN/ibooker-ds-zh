- en: Part 2\. Maximum likelihood approaches for probabilistic DL models
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第2部分\. 概率深度学习模型的极大似然方法
- en: P art 2 of this book focuses on using neural networks (NNs) as probabilistic
    models. You might remember from chapter 1 that there is a primary difference between
    a non-probabilistic and probabilistic model. A non-probabilistic model outputs
    only one best guess for the outcome, whereas a probabilistic model predicts a
    whole probability distribution over all possible outcomes. In the cab driver example
    (see section 1.1), the predicted outcome distribution for the travel time for
    a given route was a Gaussian. But until now, you haven’t learned how to set up
    an NN for a probabilistic model. You learn different methods to do so in this
    part of the book.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本书第2部分专注于使用神经网络（NN）作为概率模型。你可能还记得第1章中非概率模型和概率模型之间的主要区别。非概率模型只输出一个关于结果的最佳猜测，而概率模型预测所有可能结果的整个概率分布。在出租车司机示例中（见第1.1节），给定路线的旅行时间预测结果分布是高斯分布。但到目前为止，你还没有学习如何为概率模型设置神经网络。你将在本书的这一部分学习不同的方法来实现这一点。
- en: 'In the case of classification, you already know how to get a probability distribution
    for the outcome. In the fake banknote example (see section 2.1), you set up an
    NN that predicted for a given banknote a probability for the class fake and for
    the class real. In the MNIST classification example (see sections 2.1.3 and 2.2.4),
    you used different NN architectures to predict for a handwritten digit ten probabilities
    for ten possible classes. To do so, you defined an NN that had in the last layer
    as many nodes as there are classes. Further, you used a softmax activation to
    ensure that the output can be interpreted as a probability: the values that are
    between zero and one and that add up to one. Thus, classification NNs are probabilistic
    models by construction.'
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在分类的情况下，你已经知道如何获取结果的概率分布。在假钞示例中（见第2.1节），你设置了一个神经网络，它对一个给定的钞票预测了伪造类和真实类的概率。在MNIST分类示例中（见第2.1.3节和第2.2.4节），你使用了不同的神经网络架构来预测一个手写数字的十个可能类别的概率。为此，你定义了一个神经网络，其最后一层有与类别数量相等的节点。此外，你使用了softmax激活函数来确保输出可以解释为概率：介于零和一之间的值，且总和为1。因此，分类神经网络在结构上就是概率模型。
- en: What about regression problems? In chapter 3, we looked at linear regression.
    It turns out that all regression problems can be seen as probabilistic models.
    While classification problems are by definition probabilistic models, regression
    problems need to be interpreted a little bit to become probabilistic models.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 关于回归问题？在第3章中，我们探讨了线性回归。结果发现，所有回归问题都可以被视为概率模型。虽然分类问题根据定义是概率模型，但回归问题需要稍作解释才能成为概率模型。
- en: 'In this part of the book, you learn how to choose for different regression
    tasks an appropriate distribution and how to estimate its parameters by an NN.
    Why are we so excited about introducing probabilistic models? Recall the cab driver
    from section 1.1\. He increased his chances of getting the tip using a probabilistic
    satnav (GPS). There are many other applications in which it is critical to quantify
    the uncertainty of a prediction. Imagine, for example, a DL model that takes as
    input an X-ray image of your chest and outputs one of two treatments: surgery
    or wait and see. Now, feed your image into the model. The non-probabilistic model
    outputs surgery. The probabilistic model outputs 51% for surgery being the best
    treatment and 49% for just waiting as the best treatment. Well, we guess with
    such an uncertain prediction, you would give it a second thought if you want to
    get the operation. At least, you would like to get a second opinion by a real
    doctor who might also spot the reason for this untypical high uncertainty. The
    second great property of probabilistic models is that there is a single, unique
    way to calculate the loss functions and to rate the performance of the probabilistic
    model. This unique way is the likelihood function.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的这一部分，你将学习如何为不同的回归任务选择一个合适的分布，以及如何通过神经网络来估计其参数。为什么我们如此兴奋地介绍概率模型呢？回想一下第1.1节中的出租车司机。他通过使用概率卫星导航（GPS）增加了获得小费的机会。还有很多其他应用，在这些应用中，量化预测的不确定性是至关重要的。想象一下，例如，一个深度学习模型，它以你的胸部X光片作为输入，并输出两种治疗方案之一：手术或等待观察。现在，将你的图像输入到模型中。非概率模型输出手术。概率模型输出51%的概率认为手术是最佳治疗方案，49%的概率认为仅仅等待观察是最佳治疗方案。嗯，我们猜测，对于这样的不确定预测，如果你想要进行手术，你可能会重新考虑。至少，你希望得到一位真正的医生的第二意见，这位医生可能也会注意到这种不典型的高不确定性背后的原因。概率模型的第二个重要特性是，计算损失函数和评估概率模型性能的方式是唯一且独特的。这种独特的方式就是似然函数。
- en: In this part, you learn about the maximum likelihood approach and see how to
    use it to determine an appropriate loss function for classification tasks and
    regression tasks. As you will see, the maximum likelihood approach is a very mighty
    and intuitive principle. You’ll learn how to use the maximum likelihood approach
    for more advanced probabilistic models, such as prediction models for count data,
    text-to-speech models, or a model that generates realistic-looking facial images.
    As it turns out, the maximum likelihood principle is behind almost all loss functions
    used in DL.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 在这部分，你将了解最大似然方法，并了解如何将其用于确定分类任务和回归任务中合适的损失函数。正如你将看到的，最大似然方法是一个非常强大且直观的原则。你将学习如何使用最大似然方法来处理更高级的概率模型，例如计数数据的预测模型、文本到语音模型，或者生成逼真面部图像的模型。实际上，最大似然原理几乎隐藏在深度学习中使用的所有损失函数背后。
