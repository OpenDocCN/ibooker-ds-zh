- en: 2 Mathematical foundations of reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 2 强化学习的数学基础
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中
- en: You will learn about the core components of reinforcement learning.
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将了解强化学习的核心组件。
- en: You will learn to represent sequential decision-making problems as reinforcement
    learning environments using a mathematical framework known as Markov decision
    processes.
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将学会使用称为马尔可夫决策过程的数学框架，将顺序决策问题表示为强化学习环境。
- en: You will build from scratch environments that reinforcement learning agents
    learn to solve in later chapters.
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你将从头开始构建环境，强化学习代理将在后续章节中学习解决这些环境。
- en: Mankind’s history has been a struggle against a hostile environment. We finally
    have reached a point where we can begin to dominate our environment. ... As soon
    as we understand this fact, our mathematical interests necessarily shift in many
    areas from descriptive analysis to control theory.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 人类的历史一直是在与恶劣环境的斗争中。我们最终达到了一个可以开始主宰我们环境的阶段。 ... 一旦我们理解了这个事实，我们的数学兴趣必然会在许多领域从描述性分析转向控制理论。
- en: — Richard Bellman American applied mathematician, an IEEE medal of honor recipient
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: — 理查德·贝尔曼 美国应用数学家，IEEE荣誉奖章获得者
- en: You pick up this book and decide to read one more chapter despite having limited
    free time. A coach benches their best player for tonight’s match ignoring the
    press criticism. A parent invests long hours of hard work and unlimited patience
    in teaching their child good manners. These are all examples of complex sequential
    decision-making under uncertainty.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 你拿起这本书，尽管时间有限，还是决定再读一章。教练在今晚的比赛中将最佳球员放在板凳上，无视媒体的批评。一位家长投入大量时间和耐心，教孩子良好的行为举止。这些都是不确定情况下复杂顺序决策的例子。
- en: 'I want to bring to your attention three of the words in play in this phrase:
    complex sequential decision-making under uncertainty. The first word, *complex*,
    refers to the fact that agents may be learning in environments with vast *action
    spaces*. In the coaching example, even if you discover that your best player needs
    to rest every so often, perhaps resting them in a match with a specific opponent
    is better than with other opponents. Learning to generalize accurately is challenging
    because we learn from sampled feedback.'
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 我想提醒大家这个短语中的一些关键词：不确定情况下的复杂顺序决策。第一个词，*复杂*，指的是代理可能在具有广阔*动作空间*的环境中学习。在教练的例子中，即使你发现你的最佳球员需要时不时地休息，也许在与特定对手的比赛中休息他们比与其他对手更好。由于我们是从样本反馈中学习的，因此学会准确泛化是具有挑战性的。
- en: The second word I used is *sequential*, and this one refers to the fact that
    in many problems, there are delayed consequences. In the coaching example, again,
    let’s say the coach benched their best player for a seemingly unimportant match
    midway through the season. But, what if the action of resting players lowers their
    morale and performance that only manifests in finals? In other words, what if
    the actual consequences are delayed? The fact is that assigning credit to your
    past decisions is challenging because we learn from sequential feedback.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 我使用的第二个词是 *顺序*，这个词指的是在许多问题中，存在延迟的后果。在教练的例子中，再次，假设教练在赛季中途的一场比赛中让最佳球员休息，看似无关紧要。但是，如果休息球员的行动降低了他们的士气和表现，这种影响仅在决赛中显现出来？换句话说，如果实际后果是延迟的？事实上，由于我们是从顺序反馈中学习的，因此为过去的决策分配信用是具有挑战性的。
- en: Finally, the word *uncertainty* refers to the fact that we don’t know the actual
    inner workings of the world to understand how our actions affect it; everything
    is left to our interpretation. Let’s say the coach did bench their best player,
    but they got injured in the next match. Was the benching decision the reason the
    player got injured because the player got out of shape? What if the injury becomes
    a team motivation throughout the season, and the team ends up winning the final?
    Again, was benching the right decision? This uncertainty gives rise to the need
    for exploration. Finding the appropriate balance between exploration and exploitation
    is challenging because we learn from evaluative feedback.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，词 *不确定性* 指的是我们不知道世界的实际运作方式，以了解我们的行动如何影响它；一切都被留给了我们的解释。假设教练确实让最佳球员休息，但他们下一场比赛受伤了。是休息的决定导致球员受伤，因为球员变得不健康吗？如果伤病在整个赛季中成为团队的动力，并且球队最终赢得了决赛？再次，休息是正确的决定吗？这种不确定性引发了探索的需求。在探索和利用之间找到适当的平衡是具有挑战性的，因为我们是从评估反馈中学习的。
- en: In this chapter, you’ll learn to represent these kinds of problems using a mathematical
    framework known as *markov decision processes* (MDPs). The general framework of
    MDPs allows us to model virtually any complex sequential decision-making problem
    under uncertainty in a way that RL agents can interact with and learn to solve
    solely through experience.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，你将学习如何使用一种称为马尔可夫决策过程（MDPs）的数学框架来表示这类问题。MDPs的一般框架使我们能够以RL代理可以与之交互并仅通过经验学习解决的方式，在不确定性下对几乎任何复杂的顺序决策问题进行建模。
- en: We’ll dive deep into the challenges of learning from sequential feedback in
    chapter 3, then into the challenges of learning from evaluative feedback in chapter
    4, then into the challenges of learning from feedback that’s simultaneously sequential
    and evaluative in chapters 5 through 7, and then chapters 8 through 12 will add
    complex into the mix.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在第三章中，我们将深入探讨从顺序反馈中学习的挑战，然后在第四章中探讨从评估反馈中学习的挑战，接着在第五章到第七章中探讨同时从顺序和评估反馈中学习的挑战，最后在第八章到第十二章中，我们将把复杂性加入其中。
- en: Components of reinforcement learning
  id: totrans-13
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 强化学习组件
- en: The two core components in RL are the *agent* and the *environment*. The agent
    is the decision maker, and is the solution to a problem. The environment is the
    representation of a problem. One of the fundamental distinctions of RL from other
    ML approaches is that the agent and the environment interact; the agent attempts
    to influence the environment through actions, and the environment reacts to the
    agent’s actions.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习中的两个核心组件是*代理*和*环境*。代理是决策者，是问题的解决方案。环境是问题的表示。强化学习与其他机器学习方法的根本区别之一是代理和环境之间的交互；代理试图通过行动影响环境，而环境则对代理的行动做出反应。
- en: '![](../Images/02_01.png)'
  id: totrans-15
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_01.png)'
- en: The reinforcement learning-interaction cycle
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习-交互循环
- en: '| ![](../Images/icons_Miguel.png) | Miguel''s AnalogyThe parable of a Chinese
    farmer |'
  id: totrans-17
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Miguel.png) | 米格尔的类比故事：中国农民的寓言 |'
- en: '|  | There’s an excellent parable that shows how difficult it is to interpret
    feedback that’s simultaneously sequential, evaluative, and sampled. The parable
    goes like this:A Chinese farmer gets a horse, which soon runs away. A neighbor
    says, “So, sad. That’s bad news.” The farmer replies, “Good news, bad news, who
    can say?”The horse comes back and brings another horse with him. The neighbor
    says, “How lucky. That’s good news.” The farmer replies, “Good news, bad news,
    who can say?”The farmer gives the second horse to his son, who rides it, then
    is thrown and badly breaks his leg. The neighbor says, “So sorry for your son.
    This is definitely bad news.” The farmer replies, “Good news, bad news, who can
    say?”In a week or so, the emperor’s men come and take every healthy young man
    to fight in a war. The farmer’s son is spared.So, good news or bad news? Who can
    say?Interesting story, right? In life, it’s challenging to know with certainty
    what are the long-term consequences of events and our actions. Often, we find
    misfortune responsible for our later good fortune, or our good fortune responsible
    for our later misfortune.Even though this story could be interpreted as a lesson
    that “beauty is in the eye of the beholder,” in reinforcement learning, we assume
    there’s a correlation between actions we take and what happens in the world. It’s
    just that it’s so complicated to understand these relationships, that it’s difficult
    for humans to connect the dots with certainty. But, perhaps this is something
    that computers can help us figure out. Exciting, right?Have in mind that when
    feedback is simultaneously evaluative, sequential, and sampled, learning is a
    hard problem. And, deep reinforcement learning is a computational approach to
    learning in these kinds of problems.Welcome to the world of deep reinforcement
    learning! |'
  id: totrans-18
  prefs: []
  type: TYPE_TB
  zh: '|  | 有一个很好的寓言，说明了同时具有顺序性、评价性和样本性的反馈是多么难以解读。这个寓言是这样的：一位中国农民得到了一匹马，不久后马跑了。邻居说：“哎呀，真伤心。这是个坏消息。”农民回答说：“好消息，坏消息，谁能说呢？”马又回来了，还带来了一匹马。邻居说：“真幸运。这是个好消息。”农民回答说：“好消息，坏消息，谁能说呢？”农民把第二匹马给了他的儿子，儿子骑马，然后摔了下来，严重摔断了腿。邻居说：“为你的儿子感到抱歉。这绝对是坏消息。”农民回答说：“好消息，坏消息，谁能说呢？”大约一周后，皇帝的人来了，把所有健康的年轻人带走参加战争。农民的儿子被免除了。所以，是好消息还是坏消息？谁能说呢？有趣的故事，对吧？在生活中，确定事件和我们的行为的长期后果是具有挑战性的。我们常常发现不幸导致了我们后来的好运，或者好运导致了我们后来的不幸。尽管这个故事可以解释为“美在心中”，但在强化学习中，我们假设我们采取的行动和世界发生的事情之间存在相关性。只是这些关系如此复杂，以至于人类很难确定地连接这些点。但也许这正是计算机可以帮助我们弄清楚的事情。兴奋，对吧？记住，当反馈同时具有评价性、顺序性和样本性时，学习是一个难题。而深度强化学习是解决这类问题的计算方法。欢迎来到深度强化学习的世界！
    |'
- en: Examples of problems, agents, and environments
  id: totrans-19
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 问题、代理和环境的示例
- en: 'The following are abbreviated examples of RL problems, agents, environments,
    possible actions, and observations:'
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是对RL问题、代理、环境、可能动作和观察的简略示例：
- en: '**Problem** You’re training your dog to sit. Agent: The part of your brain
    that makes decisions. **Environment** Your dog, the treats, your dog’s paws, the
    loud neighbor, and so on. **Actions:** Talk to your dog. Wait for dog’s reaction.
    Move your hand. Show treat. Give treat. Pet. **Observations:** Your dog is paying
    attention to you. Your dog is getting tired. Your dog is going away. Your dog
    sat on command.'
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：你正在训练你的狗坐下。代理：做出决策的大脑部分。**环境**：你的狗、奖励、狗的爪子、吵闹的邻居等等。**动作**：和你的狗说话。等待狗的反应。移动你的手。展示奖励。给予奖励。抚摸。**观察**：你的狗正在注意你。你的狗感到疲倦。你的狗正在离开。你的狗按照命令坐下。'
- en: '**Problem**: Your dog wants the treats you have. **Agent**: The part of your
    dog’s brain that makes decisions. **Environment**: You, the treats, your dog’s
    paws, the loud neighbor, and so on. **Actions:** Stare at owner. Bark. Jump at
    owner. Try to steal the treat. Run. Sit. **Observations:** Owner keeps talking
    loud at dog. Owner is showing the treat. Owner is hiding the treat. Owner gave
    the dog the treat.'
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：你的狗想要你手中的奖励。**代理**：做出决策的狗脑部分。**环境**：你、奖励、狗的爪子、吵闹的邻居等等。**动作**：盯着主人看。吠叫。跳向主人。试图偷奖励。跑。坐下。**观察**：主人一直在大声对狗说话。主人正在展示奖励。主人正在隐藏奖励。主人给了狗奖励。'
- en: '**Problem**: A trading agent investing in the stock market. **Agent**: The
    executing DRL code in memory and in the CPU. **Environment**: Your internet connection,
    the machine the code is running on, the stock prices, the geopolitical uncertainty,
    other investors, day traders, and so on. **Actions:** Sell *n* stocks of *y* company.
    Buy *n* stocks of *y* company. Hold. **Observations:** Market is going up. Market
    is going down. There are economic tensions between two powerful nations. There’s
    danger of war in the continent. A global pandemic is wreaking havoc in the entire
    world.'
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题**：一个投资股市的交易代理。**代理**：内存和CPU中执行的DRL代码。**环境**：你的互联网连接，代码运行的机器，股价，地缘政治不确定性，其他投资者，日交易者等等。**动作**：卖出*y*公司的*n*股。买入*y*公司的*n*股。持有。**观察**：市场正在上涨。市场正在下跌。两个强国之间存在经济紧张。大陆有战争的危险。全球大流行正在整个世界造成破坏。'
- en: '**Problem** You’re driving your car. **Agent**: The part of your brain that
    makes decisions. **Environment**: The make and model of your car, other cars,
    other drivers, the weather, the roads, the tires, and so on. **Actions:** Steer
    by *x*, accelerate by *y*. Break by *z*. Turn the headlights on. Defog windows.
    Play music. **Observations:** You’re approaching your destination. There’s a traffic
    jam on Main Street. The car next to you is driving recklessly. It’s starting to
    rain. There’s a police officer driving in front of you.'
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**问题** 你正在开车。**代理**：你大脑中做出决策的部分。**环境**：你的汽车的品牌和型号，其他汽车，其他司机，天气，道路，轮胎等等。**动作**：通过*x*转向，通过*y*加速。通过*z*刹车。打开前灯。除雾窗户。播放音乐。**观察**：你正在接近目的地。Main
    Street上交通堵塞。旁边的车正在鲁莽驾驶。开始下雨了。前面有警察在开车。'
- en: 'As you can see, problems can take many forms: from high-level decision-making
    problems that require long-term thinking and broad general knowledge, such as
    investing in the stock market, to low-level control problems, in which geopolitical
    tensions don’t seem to play a direct role, such as driving a car.'
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 正如你所见，问题可以采取多种形式：从需要长期思考和广泛一般知识的高级决策问题，例如投资股市，到低级控制问题，其中地缘政治紧张似乎没有直接作用，例如开车。
- en: Also, you can represent a problem from multiple agents’ perspectives. In the
    dog training example, in reality, there are two agents each interested in a different
    goal and trying to solve a different problem.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，你也可以从多个代理的角度来表示一个问题。在狗训练的例子中，实际上有两个代理，每个代理都感兴趣于不同的目标，并试图解决不同的问题。
- en: Let’s zoom into each of these components independently.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们独立地深入探讨每个组件。
- en: 'The agent: The decision maker'
  id: totrans-28
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 代理：决策者
- en: As I mentioned in chapter 1, this whole book is about agents, except for this
    chapter, which is about the environment. Starting with chapter 3, you dig deep
    into the inner workings of agents, their components, their processes, and techniques
    to create agents that are effective and efficient.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我在第一章中提到的，这本书的整个内容都是关于代理的，除了这一章是关于环境的。从第三章开始，你将深入挖掘代理的内部运作、它们的组件、过程以及创建有效且高效的代理的技术。
- en: For now, the only important thing for you to know about agents is that they
    are the decision-makers in the RL big picture. They have internal components and
    processes of their own, and that’s what makes each of them unique and good at
    solving specific problems.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 目前，你需要了解关于代理的唯一重要的事情是，他们是强化学习大图景中的决策者。他们有自己内部的组件和过程，这就是使每个代理都独特并且擅长解决特定问题的原因。
- en: 'If we were to zoom in, we would see that most agents have a three-step process:
    all agents have an interaction component, a way to gather data for learning; all
    agents evaluate their current behavior; and all agents improve something in their
    inner components that allows them to improve (or at least attempt to improve)
    their overall performance.'
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们放大来看，我们会发现大多数代理都有一个三步过程：所有代理都有一个交互组件，用于收集学习数据；所有代理都会评估他们的当前行为；所有代理都会在他们的内部组件中改进某些东西，这使他们能够改进（或者至少尝试改进）他们的整体表现。
- en: '![](../Images/02_02.png)'
  id: totrans-32
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_02.png)'
- en: The three internal steps that every reinforcement learning agent goes through
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 每个强化学习代理都会经历的三步内部过程
- en: We’ll continue discussing the inner workings of agents starting with the next
    chapter. For now, let’s discuss a way to represent environments, how they look,
    and how we should model them, which is the goal of this chapter.
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在下一章继续讨论代理的内部运作。现在，让我们讨论一种表示环境的方法，它们看起来如何，以及我们应该如何建模它们，这是本章的目标。
- en: 'The environment: Everything else'
  id: totrans-35
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 环境：其他所有事物
- en: Most real-world decision-making problems can be expressed as RL environments.
    A common way to represent decision-making processes in RL is by modeling the problem
    using a mathematical framework known as Markov decision processes (MDPs). In RL,
    we assume all environments have an MDP working under the hood. Whether an Atari
    game, the stock market, a self-driving car, your significant other, you name it,
    every problem has an MDP running under the hood (at least in the RL world, whether
    right or wrong).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数现实世界的决策问题都可以表达为强化学习环境。在强化学习中表示决策过程的一种常见方式是使用一种称为马尔可夫决策过程（MDPs）的数学框架来建模问题。在强化学习中，我们假设所有环境都在底层运行着MDP。无论是Atari游戏、股市、自动驾驶汽车、你的另一半，还是其他任何问题，每个问题都在底层运行着MDP（至少在强化学习世界中是这样，无论是对是错）。
- en: The environment is represented by a set of variables related to the problem.
    The combination of all the possible values this set of variables can take is referred
    to as the *state* *space*. A state is a specific set of values the variables take
    at any given time.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 环境由与问题相关的变量集表示。这个变量集可以取的所有可能值的组合被称为*状态空间*。状态是在任何给定时间变量所取的特定值集。
- en: Agents may or may not have access to the actual environment’s state; however,
    one way or another, agents can observe something from the environment. The set
    of variables the agent perceives at any given time is called an *Observation*.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 代理可能无法访问实际环境的真实状态；然而，无论如何，代理都可以从环境中观察到某些东西。代理在任何给定时间感知到的变量集被称为*观测*。
- en: The combination of all possible values these variables can take is the *Observation
    space*. Know that state and observation are terms used interchangeably in the
    RL community. This is because often agents are allowed to see the internal state
    of the environment, but this isn’t always the case. In this book, I use state
    and observation interchangeably as well. But you need to know that there might
    be a difference between states and observations, even though the RL community
    often uses the terms interchangeably.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 这些变量可以取的所有可能值的组合是*观测空间*。要知道在强化学习社区中，状态和观测是可互换使用的术语。这是因为通常代理被允许看到环境的内部状态，但这并不总是如此。在这本书中，我也将状态和观测互换使用。但你需要知道，尽管强化学习社区经常互换使用这些术语，状态和观测之间可能存在差异。
- en: At every state, the environment makes available a set of actions the agent can
    choose from. Often the set of actions is the same for *action space*.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个状态，环境都会提供一组代理可以选择的动作。通常，动作集与*动作空间*相同。
- en: The agent attempts to influence the environment through these actions. The environment
    may change states as a response to the agent’s action. The function that is responsible
    for this transition is called the *transition function*.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 代理试图通过这些动作影响环境。环境可能会根据代理的动作改变状态。负责这种转换的函数被称为*转换函数*。
- en: After a transition, the environment emits a new observation. The environment
    may also provide a reward signal as a response. The function responsible for this
    mapping is called the *reward function*. The set of transition and reward function
    is referred to as the *model* of the environment.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在转换之后，环境会发出一个新的观测。环境还可能提供一个奖励信号作为响应。负责这种映射的函数被称为*奖励函数*。转换和奖励函数的集合被称为环境的*模型*。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe bandit walk environment
    |'
  id: totrans-43
  prefs: []
  type: TYPE_TB
  zh: '| ![具体示例](../Images/icons_Concrete.png) | 具体示例：老虎机漫步环境'
- en: '|  | Let’s make these concepts concrete with our first RL environment. I created
    this very simple environment for this book; I call it the bandit walk (BW).BW
    is a simple *grid-world* (GW) environment. GWs are a common type of environment
    for studying RL algorithms that are grids of any size. GWs can have any model
    (transition and reward functions) you can think of and can make any kind of actions
    available.But, they all commonly make move actions available to the agent: Left,
    Down, Right, Up (or West, South, East, North, which is more precise because the
    agent has no heading and usually has no visibility of the full grid, but cardinal
    directions can also be more confusing). And, of course, each action corresponds
    with its logical transition: Left goes left, and Right goes right. Also, they
    all tend to have a fully observable discrete state and observation spaces (that
    is, state equals observation) with integers representing the cell id location
    of the agent. A “walk” is a special case of grid-world environments with a single
    row. In reality, what I call a “walk” is more commonly referred to as a “Corridor.”
    But, in this book, I use the term “walk” for all the grid-world environments with
    a single row.The bandit walk (BW) is a walk with three states, but only one non-terminal
    state. Environments that have a single non-terminal state are called “bandit”
    environments. “Bandit” here is an analogy to slot machines, which are also known
    as “one-armed bandits”; they have one arm and, if you like gambling, can empty
    your pockets, the same way a *bandit* would.The BW environment has just two actions
    available: a *right*. The reward signal is a +1 when landing on the rightmost
    cell, 0 otherwise. The agent starts in the middle cell.![](../Images/02_03_Sidebar01.png)
    |'
  id: totrans-44
  prefs: []
  type: TYPE_TB
  zh: '|  | 让我们通过第一个强化学习环境来具体化这些概念。我为这本书创建了一个非常简单的环境；我称之为“老虎机漫步”（BW）。BW是一个简单的**网格世界**（GW）环境。GWs是研究强化学习算法的常见环境类型，它们是任何大小的网格。GWs可以有任何你想象得到的模型（转换和奖励函数），并且可以提供任何类型的动作。但是，它们通常都为智能体提供移动动作：左、下、右、上（或西、南、东、北，这更精确，因为智能体没有方向，通常也没有对整个网格的可见性，但基本方向也可能更令人困惑）。当然，每个动作都对应其逻辑转换：左移动到左边，右移动到右边。此外，它们通常都有一个完全可观察的离散状态和观察空间（即状态等于观察），用整数表示智能体的单元格ID位置。一个“漫步”是网格世界环境的一个特殊情况，只有一个行。实际上，我所说的“漫步”更常见地被称为“走廊”。但在本书中，我使用“漫步”一词来指代所有只有一个行的网格世界环境。“老虎机漫步”（BW）是一个有三个状态的环境，但只有一个非终止状态。只有一个非终止状态的环境被称为“老虎机”环境。“老虎机”在这里是类比于老虎机，也称为“单臂老虎机”；它们有一个臂，如果你喜欢赌博，可以让你口袋空空，就像一个**强盗**一样。BW环境只有两个动作可用：一个**右**。奖励信号是在落在最右侧单元格时为+1，否则为0。智能体从中间单元格开始。![图片](../Images/02_03_Sidebar01.png)
    |'
- en: A graphical representation of the BW environment would look like the following.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: BW环境的图形表示如下。
- en: '![](../Images/02_04.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_04.png)'
- en: Bandit walk graph
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 老虎机漫步图
- en: 'I hope this raises several questions, but you’ll find the answers throughout
    this chapter. For instance, why do the terminal states have actions that transition
    to themselves: seems wasteful, doesn’t? Any other questions? Like, what if the
    environment is stochastic? What exactly is an environments that is “stochastic”?!
    Keep reading.'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 我希望这能引发一些问题，但你会发现答案贯穿整章。例如，为什么终止状态有转换到自身的动作：这似乎是浪费的，不是吗？还有其他问题吗？比如，如果环境是随机的呢？一个“随机”的环境究竟是什么？！继续阅读。
- en: We can also represent this environment in a table form.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以用表格形式表示这个环境。
- en: '| State | Action | Next state | Transition probability | Reward signal |'
  id: totrans-50
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 动作 | 下一个状态 | 转换概率 | 奖励信号 |'
- en: '| 0 (Hole) | 0 (Left) | 0 (Hole) | 1.0 | 0 |'
  id: totrans-51
  prefs: []
  type: TYPE_TB
  zh: '| 0 (洞) | 0 (左) | 0 (洞) | 1.0 | 0 |'
- en: '| 0 (Hole) | 1 (Right) | 0 (Hole) | 1.0 | 0 |'
  id: totrans-52
  prefs: []
  type: TYPE_TB
  zh: '| 0 (洞) | 1 (右) | 0 (洞) | 1.0 | 0 |'
- en: '| 1 (Start) | 0 (Left) | 0 (Hole) | 1.0 | 0 |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 0 (左) | 0 (洞) | 1.0 | 0 |'
- en: '| 1 (Start) | 1 (Right) | 2 (Goal) | 1.0 | +1 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 1 (右) | 2 (目标) | 1.0 | +1 |'
- en: '| 2 (Goal) | 0 (Left) | 2 (Goal) | 1.0 | 0 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2 (目标) | 0 (左) | 2 (目标) | 1.0 | 0 |'
- en: '| 2 (Goal) | 1 (Right) | 2 (Goal) | 1.0 | 0 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 2 (目标) | 1 (右) | 2 (目标) | 1.0 | 0 |'
- en: Interesting, right? Let’s look at another simple example.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣，对吧？让我们看看另一个简单的例子。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe bandit slippery
    walk environment |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| ![图片](../Images/icons_Concrete.png) | 一个具体的例子老虎机滑动漫步环境 |'
- en: '|  | Okay, so how about we make this environment stochastic?Let’s say the surface
    of the walk is slippery and each action has a 20% chance of sending the agent
    *backwards*.I call this environment the bandit slippery walk (BSW).BSW is still
    a one-row-grid world, a walk, a corridor, with only Left and Right actions available.
    Again, three states and two actions. The reward is the same as before, *-*from
    itself), and zero otherwise.However, the transition function is different: 80%
    of the time the agent moves to the intended cell, and 20% of time in the opposite
    direction.A depiction of this environment would look as follows.![](../Images/02_05_Sidebar02.png)The
    bandit slippery walk (BSW) environmentIdentical to the BW environment! Interesting
    ...How do we know that the action effects are stochastic? How do we represent
    the “slippery” part of this problem? The *table* representations can help us with
    that. |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '|  | 好的，那么我们如何使这个环境变得随机呢？假设步行的表面是滑的，每个动作有20%的概率将代理送回。我把这个环境称为老虎机滑动步走（BSW）。BSW仍然是一个单行网格世界，一条步行道，只有左和右的动作可用。再次强调，三个状态和两个动作。奖励与之前相同，*（来自自身），否则为零。然而，转移函数不同：80%的时间代理移动到目标单元格，20%的时间移动到相反方向。这个环境的描述如下。![](../Images/02_05_Sidebar02.png)老虎机滑动步走（BSW）环境与BW环境相同！有趣的是……我们如何知道动作效果是随机的？我们如何表示这个问题的“滑”的部分？表格表示可以帮助我们做到这一点。
    |'
- en: A graphical representation of the *bSW* environment would look like the following.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: bSW环境的图形表示如下。
- en: '![](../Images/02_06.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_06.png)'
- en: Bandit slippery walk graph
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 老虎机滑动步走图
- en: See how the transition function is different now? The BSW environment has a
    stochastic transition function. Let’s now represent this environment in a table
    form as well.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 看看现在的转移函数有何不同？BSW环境有一个随机的转移函数。现在让我们也以表格形式表示这个环境。
- en: '| State | Action | Next state | Transition probability | Reward signal |'
  id: totrans-64
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 动作 | 下一个状态 | 转移概率 | 奖励信号 |'
- en: '| 0 (Hole) | 0 (Left) | 0 (Hole) | 1.0 | 0 |'
  id: totrans-65
  prefs: []
  type: TYPE_TB
  zh: '| 0 (洞) | 0 (左) | 0 (洞) | 1.0 | 0 |'
- en: '| 0 (Hole) | 1 (Right) | 0 (Hole) | 1.0 | 0 |'
  id: totrans-66
  prefs: []
  type: TYPE_TB
  zh: '| 0 (洞) | 1 (右) | 0 (洞) | 1.0 | 0 |'
- en: '| 1 (Start) | 0 (Left) | 0 (Hole) | 0.8 | 0 |'
  id: totrans-67
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 0 (左) | 0 (洞) | 0.8 | 0 |'
- en: '| 1 (Start) | 0 (Left) | 2 (Goal) | 0.2 | +1 |'
  id: totrans-68
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 0 (左) | 2 (目标) | 0.2 | +1 |'
- en: '| 1 (Start) | 1 (Right) | 2 (Goal) | 0.8 | +1 |'
  id: totrans-69
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 1 (右) | 2 (目标) | 0.8 | +1 |'
- en: '| 1 (Start) | 1 (Right) | 0 (Hole) | 0.2 | 0 |'
  id: totrans-70
  prefs: []
  type: TYPE_TB
  zh: '| 1 (开始) | 1 (右) | 0 (洞) | 0.2 | 0 |'
- en: '| 2 (Goal) | 0 (Left) | 2 (Goal) | 1.0 | 0 |'
  id: totrans-71
  prefs: []
  type: TYPE_TB
  zh: '| 2 (目标) | 0 (左) | 2 (目标) | 1.0 | 0 |'
- en: '| 2 (Goal) | 1 (Right) | 2 (Goal) | 1.0 | 0 |'
  id: totrans-72
  prefs: []
  type: TYPE_TB
  zh: '| 2 (目标) | 1 (右) | 2 (目标) | 1.0 | 0 |'
- en: And, we don’t have to limit ourselves to thinking about environments with discrete
    state and action spaces or even walks (corridors) or bandits (which we discuss
    in-depth in the next chapter) or grid worlds. Representing environments as MDPs
    is a surprisingly powerful and straightforward approach to modeling complex sequential
    decision-making problems under uncertainty.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们不必局限于思考具有离散状态和动作空间的环境，甚至包括走廊（我们将在下一章深入讨论）或老虎机（BSW环境），或者网格世界。将环境表示为马尔可夫决策过程（MDP）是一种出人意料强大且直接的方法，用于在不确定性下建模复杂的顺序决策问题。
- en: Here are a few more examples of environments that are powered by underlying
    MDPs.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 这里有一些由底层MDP驱动的环境的更多示例。
- en: '| Description | Observation space | Sample observation | Action space | Sample
    action | Reward function |'
  id: totrans-75
  prefs: []
  type: TYPE_TB
  zh: '| 描述 | 观察空间 | 样本观察 | 动作空间 | 样本动作 | 奖励函数 |'
- en: '| **Hotter, colder****:** Guess a randomly selected number using hints. | Int
    range 0–3.0 means no guess yet submitted, 1 means guess is lower than the target,
    2 means guess is equal to the target, and 3 means guess is higher than the target.
    | 2 | Float from –2000.0–2000.0.The float number the agent is guessing. | –909.37
    | The reward is the squared percentage of the way the agent has guessed toward
    the target. |'
  id: totrans-76
  prefs: []
  type: TYPE_TB
  zh: '| **更热，更冷**：使用提示猜测随机选择的数字。 | 整数范围0–3.0表示尚未提交猜测，1表示猜测低于目标，2表示猜测等于目标，3表示猜测高于目标。
    | 2 | 浮点数从–2000.0–2000.0。代理猜测的浮点数。 | –909.37 | 奖励是代理猜测距离目标的平方百分比。 |'
- en: '| **Cart pole****:** Balance a pole in a cart. | A four-element vector with
    ranges: from [–4.8, –Inf, –4.2, –Inf] to [4.8, Inf, 4.2, Inf].First element is
    the cart position, second is the cart velocity, third is pole angle in radians,
    fourth is the pole velocity at tip. | [–0.16, –1.61, 0.17, 2.44] | Int range 0–1.0
    means push cart left, 1 means push cart right. | 0 | The reward is 1 for every
    step taken, including the termination step. |'
  id: totrans-77
  prefs: []
  type: TYPE_TB
  zh: '| **倒立摆**：在车上保持杆的平衡。| 一个包含四个元素的向量，范围从 [–4.8, –Inf, –4.2, –Inf] 到 [4.8, Inf,
    4.2, Inf]。第一个元素是车的位置，第二个是车的速度，第三个是杆的弧度角，第四个是杆尖的速度。| [–0.16, –1.61, 0.17, 2.44]
    | 整数范围 0–1.0 表示向左推车，1 表示向右推车。| 0 | 每走一步（包括终止步）的奖励是 1。|'
- en: '| **Lunar lander****:** Navigate a lander to its landing pad. | An eight-element
    vector with ranges: from [–Inf, –Inf, –Inf, –Inf, –Inf, –Inf, 0, 0] to [Inf, Inf,
    Inf, Inf, Inf, Inf, 1, 1].First element is the x position, the second the y position,
    the third is the x velocity, the fourth is the y velocity, fifth is the vehicle’s
    angle, sixth is the angular velocity, and the last two values are Booleans indicating
    legs contact with the ground. | [0.36 , 0.23, –0.63, –0.10, –0.97, –1.73, 1.0,
    0.0] | Int range 0–3.No-op (do nothing), fire left engine, fire main engine, fire
    right engine. | 2 | Reward for landing is 200\. There’s a reward for moving from
    the top to the landing pad, for crashing or coming to rest, for each leg touching
    the ground, and for firing the engines. |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| **月球着陆器**：将着陆器导航到着陆平台。| 一个包含八个元素的向量，范围从 [–Inf, –Inf, –Inf, –Inf, –Inf, –Inf,
    0, 0] 到 [Inf, Inf, Inf, Inf, Inf, Inf, 1, 1]。第一个元素是 x 位置，第二个是 y 位置，第三个是 x 速度，第四个是
    y 速度，第五个是车辆的角度，第六个是角速度，最后两个值是布尔值，表示腿与地面的接触。| [0.36 , 0.23, –0.63, –0.10, –0.97,
    –1.73, 1.0, 0.0] | 整数范围 0–3。No-op（什么都不做），点火左引擎，点火主引擎，点火右引擎。| 2 | 着陆的奖励是 200。从顶部移动到着陆平台、碰撞或静止、每个腿接触地面以及点火引擎都会有奖励。|'
- en: '| **Pong****:** Bounce the ball past the opponent, and avoid letting the ball
    pass you. | A tensor of shape 210, 160, 3.Values ranging 0–255.Represents a game
    screen image. | [[[246, 217, 64], [ 55, 184, 230], [ 46, 231, 179], ..., [ 28,
    104, 249], [ 25, 5, 22], [173, 186, 1]], ...]] | Int range 0–5.Action 0 is No-op,
    1 is Fire, 2 is up, 3 is right, 4 is Left, 5 is Down.Notice how some actions don’t
    affect the game in any way. In reality the paddle can only move up or down, or
    not move. | 3 | The reward is a 1 when the ball goes beyond the opponent, and
    a –1 when your agent’s paddle misses the ball. |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| **Pong**：将球击过对手，并避免让球通过你。| 一个形状为 210, 160, 3 的张量。值从 0–255。表示游戏屏幕图像。| [[[246,
    217, 64], [ 55, 184, 230], [ 46, 231, 179], ..., [ 28, 104, 249], [ 25, 5, 22],
    [173, 186, 1]], ...]] | 整数范围 0–5。动作 0 是 No-op，1 是开火，2 是向上，3 是向右，4 是向左，5 是向下。注意有些动作根本不影响游戏。实际上，桨只能上下移动，或者不移动。|
    3 | 当球越过对手时，奖励是 1，当你的代理的桨错过球时，奖励是 –1。|'
- en: '| **Humanoid****:** Make robot run as fast as possible and not fall. | A 44-element
    (or more, depending on the implementation) vector.Values ranging from –Inf to
    Inf.Represents the positions and velocities of the robot’s joints. | [0.6, 0.08,
    0.9, 0\. 0, 0.0, 0.0, 0.0, 0.0, 0.045, 0.0, 0.47, ... , 0.32, 0.0, –0.22, ...
    , 0.] | A 17-element vector.Values ranging from –Inf to Inf.Represents the forces
    to apply to the robot’s joints. | [–0.9, –0.06, 0.6, 0.6, 0.6, –0.06, –0.4, –0.9,
    0.5, –0.2, 0.7, –0.9, 0.4, –0.8, –0.1, 0.8, –0.03] | The reward is calculated
    based on forward motion with a small penalty to encourage a natural gait. |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **人形机器人**：让机器人尽可能快地跑，并且不摔倒。| 一个包含 44 个元素（或更多，取决于实现）的向量。值从 –Inf 到 Inf。表示机器人关节的位置和速度。|
    [0.6, 0.08, 0.9, 0\. 0, 0.0, 0.0, 0.0, 0.0, 0.045, 0.0, 0.47, ... , 0.32, 0.0,
    –0.22, ... , 0.] | 一个包含 17 个元素的向量。值从 –Inf 到 Inf。表示施加到机器人关节的力。| [–0.9, –0.06, 0.6,
    0.6, 0.6, –0.06, –0.4, –0.9, 0.5, –0.2, 0.7, –0.9, 0.4, –0.8, –0.1, 0.8, –0.03]
    | 奖励基于前进运动，并带有轻微的惩罚以鼓励自然步态。|'
- en: Notice I didn’t add the transition function to this table. That’s because, while
    you can look at the code implementing the dynamics for certain environments, other
    implementations are not easily accessible. For instance, the transition function
    of the cart pole environment is a small Python file defining the mass of the cart
    and the pole and implementing basic physics equations, while the dynamics of Atari
    games, such as Pong, are hidden inside an Atari emulator and the corresponding
    game-specific ROM file.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 注意我没有将转换函数添加到这个表中。这是因为，虽然你可以查看实现某些环境动态的代码，但其他实现并不容易访问。例如，倒立摆环境的转换函数是一个小的 Python
    文件，定义了车的质量和杆，并实现了基本的物理方程，而像 Pong 这样的 Atari 游戏的动态则隐藏在一个 Atari 模拟器和相应的游戏特定 ROM 文件中。
- en: Notice that what we’re trying to represent here is the fact that the environment
    “reacts” to the agent’s actions in some way, perhaps even by ignoring the agent’s
    actions. But at the end of the day, there’s an internal process that’s uncertain
    (except in this and the next chapter). To represent the ability to interact with
    an environment in an MDP, we need states, observations, actions, a transition,
    and a reward function.
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，我们在这里试图表示的是环境以某种方式“反应”智能体的行动，甚至可能忽略智能体的行动。但最终，有一个内部过程是不确定的（除了本章和下一章）。为了在马尔可夫决策过程（MDP）中表示与环境交互的能力，我们需要状态、观察、行动、转移和奖励函数。
- en: '![](../Images/02_07.png)'
  id: totrans-83
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_07.png)'
- en: Process the environment goes through as a consequence of agent’s actions
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 环境处理过程是智能体行动的结果
- en: Agent-environment interaction cycle
  id: totrans-85
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 智能体-环境交互周期
- en: The environment commonly has a well-defined task. The goal of this task is defined
    through the reward signal. The reward signal can be dense, sparse, or anything
    in between. When you design environments, reward signals are the way to train
    your agent the way you want. The more dense, the more supervision the agent will
    have, and the faster the agent will learn, but the more bias you’ll inject into
    your agent, and the less likely the agent will come up with unexpected behaviors.
    The more sparse, the less supervision, and therefore, the higher the chance of
    new, emerging behaviors, but the longer it’ll take the agent to learn.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 环境通常有一个定义良好的任务。这个任务的目标是通过奖励信号来定义的。奖励信号可以是密集的、稀疏的或介于两者之间。当你设计环境时，奖励信号是训练你的智能体的方式。越密集，智能体将获得更多的监督，学习速度越快，但你会向智能体注入更多的偏差，智能体出现意外行为的可能性就越小。越稀疏，监督越少，因此，出现新的、新兴行为的机会就越高，但智能体学习的时间就会更长。
- en: The interactions between the agent and the environment go on for several cycles.
    Each cycle is called a *time step*. A time step is a unit of time, which can be
    a millisecond, a second, 1.2563 seconds, a minute, a day, or any other period
    of time.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体与环境之间的交互会持续几个周期。每个周期被称为*时间步*。时间步是时间的单位，可以是毫秒、秒、1.2563秒、分钟、一天或任何其他时间段。
- en: At each time step, the agent observes the environment, takes action, and receives
    a new observation and reward. Notice that, even though rewards can be negative
    values, they are still called rewards in the RL world. The set of the observation
    (or state), the action, the reward, and the new observation (or new state) is
    called an *experience tuple*.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个时间步，智能体观察环境，采取行动，并接收新的观察和奖励。请注意，尽管奖励可以是负值，但在强化学习领域，它们仍然被称为奖励。观察（或状态）、行动、奖励和新的观察（或新状态）的集合被称为*经验元组*。
- en: The task the agent is trying to solve may or may not have a natural ending.
    Tasks that have a natural ending, such as a game, are called *episodic tasks*.
    Tasks that don’t, such as learning forward motion, are called *continuing tasks*.
    The sequence of time steps from the beginning to the end of an episodic task is
    called an *episode*. Agents may take several time steps and episodes to learn
    to solve a task. The sum of rewards collected in a single episode is called a
    *return*. Agents are often designed to maximize the return. A time step limit
    is often added to continuing tasks, so they become episodic tasks, and agents
    can maximize the return.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 智能体试图解决的问题可能有或没有自然的结束。有自然结束的任务，如游戏，被称为*周期性任务*。没有自然结束的任务，如学习前进运动，被称为*持续任务*。从周期性任务的开始到结束的时间步序列称为*周期*。智能体可能需要几个时间步和周期来学习解决任务。单个周期中收集到的奖励总和称为*回报*。智能体通常被设计成最大化回报。在持续任务中经常添加时间步限制，使它们成为周期性任务，智能体可以最大化回报。
- en: Every experience tuple has an opportunity for learning and improving performance.
    The agent may have one or more components to aid learning. The agent may be designed
    to learn mappings from observations to actions called policies. The agent may
    be designed to learn mappings from observations to new observations and/or rewards
    called models. The agent may be designed to learn mappings from observations (and
    possibly actions) to reward-to-go estimates (a slice of the return) called value
    functions.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 每个经验元组都有学习和改进性能的机会。智能体可能有一到多个组件来辅助学习。智能体可能被设计成学习从观察到行动的映射，称为策略。智能体可能被设计成学习从观察到新的观察和/或奖励的映射，称为模型。智能体可能被设计成学习从观察（和可能的行动）到奖励到估计（回报的一部分）的映射，称为价值函数。
- en: For the rest of this chapter, we’ll put aside the agent and the interactions,
    and we’ll examine the environment and inner MDP in depth. In chapter 3, we’ll
    pick back up the agent, but there will be *neural networks for learning*.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章的剩余部分，我们将暂时放下智能体和交互，深入探讨环境和内部MDP。在第3章中，我们将重新引入智能体，但会有*用于学习的神经网络*。
- en: 'MDPs: The engine of the environment'
  id: totrans-92
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: MDPs：环境的引擎
- en: Let’s build MDPs for a few environments as we learn about the components that
    make them up. We’ll create Python dictionaries representing MDPs from descriptions
    of the problems. In the next chapter, we’ll study algorithms for planning on MDPs.
    These methods can devise solutions to MDPs and will allow us to find optimal solutions
    to all problems in this chapter.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在了解构成它们的组件时，为几个环境构建马尔可夫决策过程（MDPs）。我们将创建代表MDPs的Python字典，这些字典来自对问题的描述。在下一章中，我们将研究在MDPs上规划算法。这些方法可以设计出MDPs的解决方案，并使我们能够找到本章中所有问题的最优解。
- en: The ability to build environments yourself is an important skill to have. However,
    often you find environments for which somebody else has already created the MDP.
    Also, the dynamics of the environments are often hidden behind a simulation engine
    and are too complex to examine in detail; certain dynamics are even inaccessible
    and hidden behind the real world. In reality, RL agents don’t need to know the
    precise MDP of a problem to learn robust behaviors, but knowing *you* because
    agents are commonly designed with the assumption that an MDP, even if inaccessible,
    is running under the hood.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 能够自己构建环境是一项重要的技能。然而，你经常会发现别人已经为某些环境创建了MDP。此外，环境的动态通常隐藏在模拟引擎背后，过于复杂，无法详细检查；某些动态甚至无法访问，隐藏在现实世界背后。实际上，强化学习智能体不需要知道问题的精确MDP来学习鲁棒的行为，但了解*你*，因为智能体通常设计时假设即使无法访问，MDP也在幕后运行。
- en: '| ![](../Images/icons_Concrete.png) | A Concrete ExampleThe frozen lake environment
    |'
  id: totrans-95
  prefs: []
  type: TYPE_TB
  zh: '| ![具体示例](../Images/icons_Concrete.png) | 一个具体示例：冰冻湖环境'
- en: '|  | This is another, more challenging problem for which we will build an MDP
    in this chapter. This environment is called the frozen lake (FL).FL is a simple
    *discrete state and action spaces*. However, this time, four actions are available:
    move Left, Down, Right, or Up.The task in the FL environment is similar to the
    task in the BW and BSW environments: to go from a start location to a goal location
    while avoiding falling into holes. The challenge is similar to the *bSW*, in that
    the surface of the FL environment is slippery, it’s a frozen lake after all. But
    the environment itself is larger. Let’s look at a depiction of the FL.![](../Images/02_08_Sidebar03.png)The
    frozen lake (FL) environmentThe FL is a 4 × 4 grid (it has 16 cells, ids 0–15).
    The agent shows up in the START cell every new episode. Reaching the GOAL cell
    gives a +1 reward; anything else is a 0\. Because the surface are slippery, the
    agent moves only a third of the time as intended. The other two-thirds are split
    evenly in orthogonal directions. For example, if the agent chooses to move down,
    there’s a 33.3% chance it moves down, 33.3% chance it moves left, and 33.3% chance
    it moves right. There’s a fence around the lake, so if the agent tries to move
    out of the grid world, it will bounce back to the cell from which it tried to
    move. There are four holes in the lake. If the agent falls into one of these holes,
    it’s game over.Are you ready to start building a representation of these dynamics?
    We need a Python dictionary representing the MDP as described here. Let’s start
    building the MDP. |'
  id: totrans-96
  prefs: []
  type: TYPE_TB
  zh: '|  | 这是另一个更具挑战性的问题，我们将在本章中为它构建一个MDP。这个环境被称为冰冻湖（FL）。FL是一个简单的*离散状态和动作空间*。然而，这次有四种动作可供选择：向左移动、向下移动、向右移动或向上移动。在FL环境中的任务与BW和BWS环境中的任务相似：从起始位置移动到目标位置，同时避免掉入坑中。挑战与*bSW*相似，因为FL环境的表面很滑，毕竟它是一个冰冻湖。但环境本身更大。让我们看看FL的描述图。![FL环境](../Images/02_08_Sidebar03.png)冰冻湖（FL）环境FL是一个4×4的网格（它有16个单元格，id为0-15）。每个新剧集开始时，智能体都会出现在起始单元格中。到达目标单元格会得到+1的奖励；其他任何情况都是0。由于表面很滑，智能体的移动只有三分之一的预期效果。其他三分之二均匀地分布在正交方向上。例如，如果智能体选择向下移动，有33.3%的概率它会向下移动，33.3%的概率它会向左移动，33.3%的概率它会向右移动。湖周围有一道栅栏，所以如果智能体试图离开网格世界，它会弹回到它试图移动的单元格。湖中有四个洞。如果智能体掉入其中一个洞，游戏就结束了。你准备好开始构建这些动态的表示了吗？我们需要一个Python字典来表示这里描述的MDP。让我们开始构建MDP。
    |'
- en: 'States: Specific configurations of the environment'
  id: totrans-97
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 状态：环境的特定配置
- en: A *state* is a unique and self-contained configuration of the problem. The set
    of all possible states, the *state space*, is defined as the set *s*. The state
    space can be finite or infinite. But notice that the state space is different
    than the set of variables that compose a single state. This other set must always
    be finite and of constant size from state to state. In the end, the state space
    is a set of sets. The inner set must be of equal size and finite, as it contains
    the number of variables representing the states, but the outer set can be infinite
    depending on the types of elements of the inner sets.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: '*状态*是问题的唯一且自包含的配置。所有可能状态的集合，即*状态空间*，定义为集合*s*。状态空间可以是有限的或无限的。但请注意，状态空间与组成单个状态的一组变量不同。这个其他集合必须始终是有限的，并且从状态到状态大小恒定。最终，状态空间是一组集合。内部集合必须大小相等且有限，因为它包含表示状态的变量的数量，但外部集合可以根据内部集合的元素类型是无限的。'
- en: '![](../Images/02_09.png)'
  id: totrans-99
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_09.png)'
- en: 'State space: A set of sets'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间：一组集合
- en: For the BW, BSW, and FL environments, the state is composed of a single variable
    containing the id of the cell where the agent is at any given time. The agent’s
    location cell id is a discrete variable. But state variables can be of any kind,
    and the set of variables can be larger than one. We could have the Euclidean distance
    that would be a continuous variable and an infinite state space; for example,
    2.124, 2.12456, 5.1, 5.1239458, and so on. We could also have multiple variables
    defining the state, for instance, the number of cells away from the goal in the
    x- and y-axis. That would be two variables representing a single state. Both variables
    would be discrete, therefore, the state space finite. However, we could also have
    variables of mixed types; for instance, one could be discrete, another continuous,
    another Boolean.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BW、BSW和FL环境，状态由一个包含代理在任意给定时间所在细胞ID的单个变量组成。代理的位置细胞ID是一个离散变量。但状态变量可以是任何类型，变量集可以大于一个。我们可以有一个欧几里得距离，这将是一个连续变量，并且状态空间是无限的；例如，2.124、2.12456、5.1、5.1239458等等。我们也可以有多个变量定义状态，例如，x轴和y轴上离目标多少个细胞。这将代表单个状态的两个变量。这两个变量都是离散的，因此状态空间是有限的。然而，我们也可以有混合类型的变量；例如，一个可以是离散的，另一个可以是连续的，另一个可以是布尔值。
- en: With this state representation for the BW, BSW, and FL environments, the size
    of the state space is 3, 3, and 16, respectively. Given we have 3, 3, or 16 cells,
    the agent can be at any given time, then we have 3, 3, and 16 possible states
    in the state space. We can set the ids of each cell starting from zero, going
    left to right, top to bottom.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BW、BSW和FL环境的状态表示，状态空间的大小分别为3、3和16。给定我们有3、3或16个细胞，代理在任意给定时间可以处于任何状态，因此状态空间中有3、3和16种可能的状态。我们可以从零开始设置每个细胞的ID，从左到右，从上到下。
- en: 'In the FL, we set the ids from zero to 15, left to right, top to bottom. You
    could set the ids in any other way: in a random order, or group cells by proximity,
    or whatever. It’s up to you; as long as you keep them consistent throughout training,
    it will work. However, this representation is adequate, and it works well, so
    it’s what we’ll use.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，我们从零到15设置ID，从左到右，从上到下。你可以以任何其他方式设置ID：随机顺序，按邻近性分组细胞，或者任何其他方式。这取决于你；只要你在整个训练过程中保持一致，它就会工作。然而，这种表示是足够的，并且效果很好，所以我们将使用它。
- en: '![](../Images/02_10.png)'
  id: totrans-104
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_10.png)'
- en: States in the FL contain a single variable indicating the id of the cell in
    which the agent is at any given time step
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL中，状态包含一个变量，表示在任意给定时间步中代理所在的细胞ID。
- en: 'In the case of MDPs, the states are fully observable: we can see the internal
    state of the environment at each time step, that is, the observations and the
    states are the same. *partially observable Markov decision processes* (POMDPs)
    is a more general framework for modeling environments in which observations, which
    still depend on the internal state of the environment, are the only things the
    agent can see instead of the state. Notice that for the BW, BSW, and FL environments,
    we’re creating an MDP, so the agent will be able to observe the internal state
    of the environment.'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 在MDPs的情况下，状态是完全可观察的：我们可以在每个时间步看到环境的内部状态，也就是说，观察和状态是相同的。*部分可观察马尔可夫决策过程*（POMDPs）是一个更通用的框架，用于建模环境，其中观察结果，仍然依赖于环境的内部状态，是代理唯一能看到的东西，而不是状态。请注意，对于BW、BSW和FL环境，我们正在创建一个MDP，因此代理将能够观察到环境的内部状态。
- en: States must contain all the variables necessary to make them independent of
    all other states. In the FL environment, you only need to know the current state
    of the agent to tell its next possible states. That is, you don’t need the history
    of states visited by the agent for anything. You know that from state 2 the agent
    can only transition to states 1, 3, 6, or 2, and this is true regardless of whether
    the agent’s previous state was 1, 3, 6, or 2.
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 状态必须包含所有必要的变量，使它们独立于所有其他状态。在FL环境中，你只需要知道智能体的当前状态，就可以告诉它的下一个可能状态。也就是说，你不需要智能体访问过的状态历史。你知道从状态2，智能体只能转移到状态1、3、6或2，而且这无论智能体的先前状态是1、3、6还是2都是正确的。
- en: 'The probability of the next state, given the current state and action, is independent
    of the history of interactions. This memoryless property of MDPs is known as the
    *markov property*: the probability of moving from one state *s* to another state
    *s* on two separate occasions, given the same action *a*, is the same regardless
    of all previous states or actions encountered before that point.'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 给定当前状态和动作，下一个状态的概率与交互历史无关。MDP的这种无记忆特性被称为**马尔可夫性质**：在两次不同场合，给定相同的动作*a*，从状态*s*转移到另一个状态*s*的概率是相同的，无论之前遇到的所有状态或动作。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe Markov property |'
  id: totrans-109
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 显示数学马尔可夫性质 |'
- en: '|  | ![](../Images/02_10_sidebar04.png) |'
  id: totrans-110
  prefs: []
  type: TYPE_TB
  zh: '|  | ![图片](../Images/02_10_sidebar04.png) |'
- en: But why do you care about this? Well, in the environments we’ve explored so
    far it’s not that obvious, and it’s not that important. But because most RL (and
    DRL) agents are designed to take advantage of the Markov assumption, you must
    make sure you feed your agent the necessary variables to make it hold as tightly
    as possible (completely keeping the Markov assumption is impractical, perhaps
    impossible).
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 但你为什么关心这个？嗯，在我们已经探索的环境中，这并不明显，也不那么重要。但是，因为大多数RL（和DRL）智能体都是设计来利用马尔可夫假设的，你必须确保你向智能体提供必要的变量，使其尽可能紧密地保持（完全保持马尔可夫假设是不切实际的，也许是不可能的）。
- en: For example, if you’re designing an agent to learn to land a spacecraft, the
    agent must receive all variables that indicate velocities along with its locations.
    Locations alone are not sufficient to land a spacecraft safely, and because you
    must assume the agent is memoryless, you need to feed the agent more information
    than just its x, y, z coordinates away from the landing pad.
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果你正在设计一个学习如何着陆宇宙飞船的智能体，智能体必须接收所有表示速度的变量以及其位置。仅位置信息不足以安全着陆宇宙飞船，而且因为你必须假设智能体是无记忆的，所以你需要向智能体提供比仅其远离着陆点的x、y、z坐标更多的信息。
- en: 'But, you probably know that acceleration is to velocity what velocity is to
    position: the derivative. You probably also know that you can keep taking derivatives
    beyond acceleration. To make the MDP completely Markovian, how deep do you have
    to go? This is more of an art than a science: the more variables you add, the
    longer it takes to train an agent, but the fewer variables, the higher the chance
    the information fed to the agent is not sufficient, and the harder it is to learn
    anything useful. For the spacecraft example, often locations and velocities are
    adequate, and for grid-world environments, only the state id location of the agent
    is sufficient.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，你可能知道加速度是速度相对于位置导数的关系：导数。你可能也知道你可以继续对加速度求导。为了使MDP完全马尔可夫化，你需要求多深？这更多的是一种艺术而不是科学：你添加的变量越多，训练智能体所需的时间就越长，但变量越少，提供给智能体的信息不足的可能性就越高，学习任何有用的东西就越困难。对于宇宙飞船的例子，通常位置和速度是足够的，而对于网格世界环境，智能体的状态id位置就足够了。
- en: 'The set of all states in the MDP is denoted *si* from a probability distribution.
    This distribution can be anything, but it must be fixed throughout training: that
    is, the probabilities must be the same from the first to the last episode of training
    and for agent evaluation.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: MDP中所有状态的集合用*si*表示，来自一个概率分布。这个分布可以是任何东西，但必须在整个训练过程中保持固定：也就是说，概率必须从训练的第一集到最后一集，以及对于智能体评估都是相同的。
- en: There’s a unique state called the *absorbing* or *terminal state*, and the set
    of all non-terminal states is denoted *s*. Now, while it’s common practice to
    create a single terminal state (a sink state) to which all terminal transitions
    go, this isn’t always implemented this way. What you’ll see more often is multiple
    terminal states, and that’s okay. It doesn’t really matter under the hood if you
    make all terminal states behave as expected.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 有一个独特的状态称为*吸收状态*或*终端状态*，所有非终端状态的集合用*s*表示。现在，虽然通常的做法是创建一个单一的终端状态（一个汇状态），所有终端转换都指向它，但这并不总是这样实现的。你更常看到的是多个终端状态，这是可以的。在底层，如果你让所有终端状态都按预期行为，这实际上并不重要。
- en: 'As expected? Yes. A terminal state is a special state: it must have all available
    actions transitioning, with probability 1, to itself, and these transitions must
    provide no reward. Note that I’m referring to the transitions from the terminal
    state, not to the terminal state.'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样？是的。终端状态是一种特殊状态：它必须具有所有可用的动作，以概率1转换到自身，并且这些转换必须不提供任何奖励。请注意，我指的是从终端状态到终端状态的转换，而不是终端状态本身。
- en: It’s very commonly the case that the end of an episode provides a non-zero reward.
    For instance, in a chess game you win, you lose, or you draw. A logical reward
    signal would be +1, –1, and 0, respectively. But it’s a compatibility convention
    that allows for all algorithms to converge to the same solution to make all actions
    available in a terminal state transition from that terminal state to itself with
    probability 1 and reward 0\. Otherwise, you run the risk of infinite sums and
    algorithms that may not work altogether. Remember how the BW and BSW environments
    had these terminal states?
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，一个局部的结束会提供非零的奖励。例如，在一场棋局中，你可能赢、输或平局。一个逻辑上的奖励信号可以是+1、-1和0，分别对应赢、输和平局。但是，这是一个兼容性约定，允许所有算法收敛到相同的解决方案，使得所有动作在终端状态转换到自身时，概率为1且奖励为0。否则，你可能会遇到无限和的问题，以及可能完全无法工作的算法。还记得BW和BSW环境中的这些终端状态吗？
- en: In the FL environment, for instance, there’s only one starting state (which
    is state 0) and five terminal states (or five states that transition to a single
    terminal state, whichever you prefer). For clarity, I use the convention of multiple
    terminal states (5, 7, 11, 12, and 15) for the illustrations and code; again,
    each terminal state is a separate terminal state.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，在FL环境中，只有一个起始状态（即状态0）和五个终端状态（或者五个转换到单个终端状态的状态，根据你的喜好）。为了清晰起见，我在插图和代码中使用多个终端状态（5、7、11、12和15）的约定；再次强调，每个终端状态都是一个单独的终端状态。
- en: '![](../Images/02_11.png)'
  id: totrans-119
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_11.png)'
- en: States in the frozen lake environment
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖环境中的状态
- en: 'Actions: A mechanism to influence the environment'
  id: totrans-121
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 动作：影响环境的一种机制
- en: MDPs make available a set of actions A that depends on the state. That is, there
    might be actions that aren’t allowed in a state—in fact, A is a function that
    takes a state as an argument; that is, A(s). This function returns the set of
    available actions for state s. If needed, you can define this set to be constant
    across the state space; that is, all actions are available at every state. You
    can also set all transitions from a state-action pair to zero if you want to deny
    an action in a given state. You could also set all transitions from state s and
    action a to the same state s to denote action a as a no-intervene or no-op action.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: MDPs提供了一组依赖于状态的行动A。也就是说，可能存在在某个状态下不允许的动作——实际上，A是一个以状态为参数的函数；即，A(s)。这个函数返回状态s的可用动作集合。如果需要，你可以定义这个集合在状态空间中是常数；即，在每一个状态下都有所有动作可用。你也可以将状态-动作对的所有转换设置为0，如果你想在一个给定的状态下拒绝一个动作。你也可以将状态s和动作a的所有转换设置为相同的s，以表示动作a是一个不干预或无操作动作。
- en: Just as with the state, the action space may be finite or infinite, and the
    set of variables of a single action may contain more than one element and must
    be finite. However, unlike the number of state variables, the number of variables
    that compose an action may not be constant. The actions available in a state may
    change depending on that state. For simplicity, most environments are designed
    with the same number of actions in all states.
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 就像状态一样，动作空间可能是有限的或无限的，单个动作的变量集合可能包含多个元素，并且必须是有限的。然而，与状态变量的数量不同，组成动作的变量数量可能不是恒定的。一个状态中可用的动作可能根据该状态而变化。为了简单起见，大多数环境都是设计为所有状态下动作数量相同。
- en: The environment makes the set of all available actions known in advance. Agents
    can select actions either deterministically or stochastically. This is different
    than saying the environment reacts deterministically or stochastically to agents’
    actions. Both are true statements, but I’m referring here to the fact that agents
    can either select actions from a lookup table or from per-state probability distributions.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 环境提前知道所有可用的动作集合。智能体可以选择确定性或随机地选择动作。这与说环境对智能体的动作做出确定性或随机反应是不同的。这两个陈述都是真实的，但我在这里指的是智能体可以从查找表或每个状态的概率分布中选择动作。
- en: 'In the BW, BSW, and FL environments, actions are singletons representing the
    direction the agent will attempt to move. In FL, there are four available actions
    in all states: Up, Down, Right, or Left. There’s one variable per action, and
    the size of the action space is four.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在BW、BSW和FL环境中，动作是单例，表示智能体将尝试移动的方向。在FL中，所有状态下都有四个可用的动作：上、下、右或左。每个动作有一个变量，动作空间的大小是4。
- en: '![](../Images/02_12.png)'
  id: totrans-126
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_12.png)'
- en: The frozen lake environment has four simple move actions
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 冰冻湖环境有四种简单的移动动作
- en: 'Transition function: Consequences of agent actions'
  id: totrans-128
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 转移函数：智能体动作的后果
- en: The way the environment changes as a response to actions is referred to as the
    *state-transition* *probabilities*, or more simply, the *transition function*,
    and is denoted by *t(s, a, s')*. The transition function *t* maps a transition
    tuple *s*, *a*, *s'* to a probability; that is, you pass in a state *s*, an action
    *a*, and a next state *s'*, and it’ll return the corresponding probability of
    transition from state *s* to state *s'* when taking action *a*. You could also
    represent it as *t(s, a)* and return a dictionary with the next states for its
    keys and probabilities for its values.
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 环境对动作的反应方式被称为 *状态转移概率*，或者更简单地说，*转移函数*，用 *t(s, a, s')* 表示。转移函数 *t* 将一个转移元组 *s*，*a*，*s'*
    映射到一个概率；也就是说，你传递一个状态 *s*，一个动作 *a* 和一个下一个状态 *s'*，它将返回采取动作 *a* 时从状态 *s* 转移到状态 *s'*
    的对应概率。你也可以将其表示为 *t(s, a)*，并返回一个字典，其中键是下一个状态，值是概率。
- en: Notice that *t* also describes a probability distribution *p( · | s, a)* determining
    how the system will evolve in an interaction cycle from selecting action *a* in
    state *s*. When integrating over the next states *s'*, as any probability distribution,
    the sum of these probabilities must equal one.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，*t* 也描述了一个概率分布 *p( · | s, a)*，它决定了系统在交互周期中从状态 *s* 选择动作 *a* 后将如何演变。当对下一个状态
    *s'* 进行积分时，作为任何概率分布，这些概率的总和必须等于1。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe transition function |'
  id: totrans-131
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学公式 | 转移函数 |'
- en: '|  | ![](../Images/02_12_Sidebar05.png) |'
  id: totrans-132
  prefs: []
  type: TYPE_TB
  zh: '| ![侧边栏05](../Images/02_12_Sidebar05.png) |'
- en: The BW environment was deterministic; that is, the probability of the next state
    *s'* given the current state *s* and action *a* was always 1\. There was always
    a single possible next state *s'*. The BSW and FL environments are stochastic;
    that is, the probability of the next state *s'* given the current state *s* and
    action *a* is less than 1\. There are more than one possible next state *s'*s.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: BW环境是确定性的；也就是说，给定当前状态 *s* 和动作 *a*，下一个状态 *s'* 的概率总是1。总是只有一个可能的后继状态 *s'*. BSW和FL环境是随机的；也就是说，给定当前状态
    *s* 和动作 *a*，下一个状态 *s'* 的概率小于1。存在多个可能的后继状态 *s'*。
- en: One key assumption of many RL (and DRL) algorithms is that this distribution
    is stationary. That is, while there may be highly stochastic transitions, the
    probability distribution may not change during training or evaluation. Just as
    with the Markov assumption, the stationarity assumption is often relaxed to an
    extent. However, it’s important for most agents to interact with environments
    that at least appear to be stationary.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 许多强化学习（RL）和深度强化学习（DRL）算法的一个关键假设是，这个分布是平稳的。也就是说，虽然可能会有高度随机的转换，但在训练或评估过程中概率分布可能不会改变。正如马尔可夫假设一样，平稳性假设通常会被放宽到一定程度。然而，对于大多数智能体来说，与环境互动的环境至少看起来是平稳的是很重要的。
- en: In the FL environment, we know that there’s a 33.3% chance we’ll transition
    to the intended cell (state) and a 66.6% chance we’ll transition to orthogonal
    directions. There’s also a chance we’ll bounce back to the state we’re coming
    from if it’s next to the wall.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL环境中，我们知道有33.3%的几率我们会转换到目标单元格（状态），有66.6%的几率我们会转换到正交方向。还有可能反弹回我们来的状态，如果它靠近墙壁的话。
- en: For simplicity and clarity, I’ve added to the following image only the transition
    function for all actions of states 0, 2, 5, 7, 11, 12, 13, and 15 of the FL environment.
    This subset of states allows for the illustration of all possible transitions
    without too much clutter.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单和清晰，我在以下图像中只添加了FL环境中状态0、2、5、7、11、12、13和15的所有动作的转移函数。这个状态子集允许展示所有可能的转移，而不会过于杂乱。
- en: '![](../Images/02_13.png)'
  id: totrans-137
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_13.png)'
- en: The transition function of the frozen lake environment
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 冻结湖环境的转移函数
- en: 'It might still be a bit confusing, but look at it this way: for consistency,
    each action in non-terminal states has three separate transitions (certain actions
    in corner states could be represented with only two, but again, let me be consistent):
    one to the intended cell and two to the cells in orthogonal directions.'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 可能仍然有些困惑，但这样看：为了保持一致性，非终端状态中的每个动作都有三个独立的转移（角落状态中的某些动作可能只需要两个，但再次强调，让我保持一致性）：一个转移到目标单元格，两个转移到正交方向的单元格。
- en: 'Reward signal: Carrots and sticks'
  id: totrans-140
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 奖励信号：胡萝卜和大棒
- en: The reward function *r* maps a transition tuple *s, a, s'* to a scalar. The
    reward function gives a numeric signal of goodness to transitions. When the signal
    is positive, we can think of the reward as an income or a reward. Most problems
    have at least one positive signal—winning a chess match or reaching the desired
    destination, for example. But, rewards can also be negative, and we can see these
    as cost, punishment, or penalty. In robotics, adding a time step cost is a common
    practice because we usually want to reach a goal, but within a number of time
    steps. One thing to clarify is that whether positive or negative, the scalar coming
    out of the reward function is always referred to as the *reward*. RL folks are
    happy folks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 奖励函数 *r* 将一个状态转移元组 *s, a, s'* 映射到一个标量。奖励函数为状态转移提供了一种关于好坏的数值信号。当信号为正时，我们可以将奖励视为收入或奖励。大多数问题至少有一个正信号——比如赢得棋局或达到期望的目的地。但是，奖励也可以是负的，我们可以将其视为成本、惩罚或处罚。在机器人领域，添加时间步长成本是一种常见的做法，因为我们通常希望在有限的时间步内达到目标。有一点需要明确的是，无论是正还是负，奖励函数输出的标量始终被称为
    *奖励*。强化学习的人们都是快乐的人。
- en: It’s also important to highlight that while the reward function can be represented
    as *r(s,a,s')*, which is explicit, we could also use *r(s,a),* or even *r(s)*,
    depending on our needs. Sometimes rewarding the agent based on state is what we
    need; sometimes it makes more sense to use the action and the state. However,
    the most explicit way to represent the reward function is to use a state, action,
    and next state triplet. With that, we can compute the marginalization over next
    states in *r(s,a,s')* to obtain *r(s,a)*, and the marginalization over actions
    in *r(s,a)* to get *r(s)*. But, once we’re in *r(s)* we can’t recover *r(s,a)*
    or *r(s,a,s'),* and once we’re in *r(s,a)* we can’t recover *r(s,a,s')*.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 还需要强调的是，虽然奖励函数可以表示为 *r(s,a,s')*，这是明确的，我们也可以使用 *r(s,a)*，甚至 *r(s)*，这取决于我们的需求。有时根据状态奖励智能体是我们需要的；有时使用动作和状态更有意义。然而，表示奖励函数最明确的方式是使用状态、动作和下一个状态的三元组。有了这个，我们可以在
    *r(s,a,s')* 中对下一个状态进行边缘化，以获得 *r(s,a)*，并在 *r(s,a)* 中对动作进行边缘化以获得 *r(s)*。但是，一旦我们处于
    *r(s)*，我们就无法恢复 *r(s,a)* 或 *r(s,a,s')*，一旦我们处于 *r(s,a)*，我们就无法恢复 *r(s,a,s')*。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe reward function |'
  id: totrans-143
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学公式奖励函数'
- en: '|  | ![](../Images/02_13_Sidebar06.png)The reward function |'
  id: totrans-144
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/02_13_Sidebar06.png)奖励函数'
- en: In the FL environment, the reward function is +1 for landing in state 15, 0
    otherwise. Again, for clarity to the following image, I’ve only added the reward
    signal to transitions that give a non-zero reward, landing on the final state
    (state 15.)
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 在FL环境中，当落在状态15时，奖励函数为+1，否则为0。同样，为了使以下图像更清晰，我只添加了给予非零奖励的转移信号，即落在最终状态（状态15）。
- en: There are only three ways to land on 15\. (1) Selecting the *down* action from
    state 14 will unintentionally also transition the agent there with 33.3% probability
    for each action. See the difference between actions and transitions? It’s interesting
    to see how stochasticity complicates things, right?
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 只有三种方式可以落在15上。（1）从状态14选择 *向下* 动作，会无意中将智能体转移到那里，每个动作的概率为33.3%。你注意到动作和转移之间的区别了吗？看到随机性如何使事情复杂化，很有趣，对吧？
- en: '![](../Images/02_14.png)'
  id: totrans-147
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/02_14.png)'
- en: Reward signal for states with non-zero reward transitions
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 具有非零奖励转移状态的重奖信号
- en: Expanding the transition and reward functions into a table form is also useful.
    The following is the format I recommend for most problems. Notice that I’ve only
    added a subset of the transitions (rows) to the table to illustrate the exercise.
    Also notice that I’m being explicit, and several of these transitions could be
    grouped and refactored (for example, corner cells).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 将转移和奖励函数扩展到表格形式也是有用的。以下是我推荐的大多数问题的格式。请注意，我只为表格添加了一部分转移（行）来展示练习。另外请注意，我在这里很明确，这些转移中的几个可以分组和重构（例如，角落单元格）。
- en: '| State | Action | Next state | Transition probability | Reward signal |'
  id: totrans-150
  prefs: []
  type: TYPE_TB
  zh: '| 状态 | 动作 | 下一个状态 | 转移概率 | 奖励信号 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-151
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| 0 | Left | 0 | 0.33 | 0 |'
  id: totrans-152
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 左 | 0 | 0.33 | 0 |'
- en: '| 0 | Left | 0 | 0.33 | 0 |'
  id: totrans-153
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 左 | 0 | 0.33 | 0 |'
- en: '| 0 | Left | 4 | 0.33 | 0 |'
  id: totrans-154
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 左 | 4 | 0.33 | 0 |'
- en: '| 0 | Down | 0 | 0.33 | 0 |'
  id: totrans-155
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 下 | 0 | 0.33 | 0 |'
- en: '| 0 | Down | 4 | 0.33 | 0 |'
  id: totrans-156
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 下 | 4 | 0.33 | 0 |'
- en: '| 0 | Down | 1 | 0.33 | 0 |'
  id: totrans-157
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 下 | 1 | 0.33 | 0 |'
- en: '| 0 | Right | 4 | 0.33 | 0 |'
  id: totrans-158
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 右 | 4 | 0.33 | 0 |'
- en: '| 0 | Right | 1 | 0.33 | 0 |'
  id: totrans-159
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 右 | 1 | 0.33 | 0 |'
- en: '| 0 | Right | 0 | 0.33 | 0 |'
  id: totrans-160
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 右 | 0 | 0.33 | 0 |'
- en: '| 0 | Up | 1 | 0.33 | 0 |'
  id: totrans-161
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 上 | 1 | 0.33 | 0 |'
- en: '| 0 | Up | 0 | 0.33 | 0 |'
  id: totrans-162
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 上 | 0 | 0.33 | 0 |'
- en: '| 0 | Up | 0 | 0.33 | 0 |'
  id: totrans-163
  prefs: []
  type: TYPE_TB
  zh: '| 0 | 上 | 0 | 0.33 | 0 |'
- en: '| 1 | Left | 1 | 0.33 | 0 |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 左 | 1 | 0.33 | 0 |'
- en: '| 1 | Left | 0 | 0.33 | 0 |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 左 | 0 | 0.33 | 0 |'
- en: '| 1 | Left | 5 | 0.33 | 0 |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 左 | 5 | 0.33 | 0 |'
- en: '| 1 | Down | 0 | 0.33 | 0 |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 下 | 0 | 0.33 | 0 |'
- en: '| 1 | Down | 5 | 0.33 | 0 |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 下 | 5 | 0.33 | 0 |'
- en: '| 1 | Down | 2 | 0.33 | 0 |'
  id: totrans-169
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 下 | 2 | 0.33 | 0 |'
- en: '| 1 | Right | 5 | 0.33 | 0 |'
  id: totrans-170
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 右 | 5 | 0.33 | 0 |'
- en: '| 1 | Right | 2 | 0.33 | 0 |'
  id: totrans-171
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 右 | 2 | 0.33 | 0 |'
- en: '| 1 | Right | 1 | 0.33 | 0 |'
  id: totrans-172
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 右 | 1 | 0.33 | 0 |'
- en: '| 2 | Left | 1 | 0.33 | 0 |'
  id: totrans-173
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 左 | 1 | 0.33 | 0 |'
- en: '| 2 | Left | 2 | 0.33 | 0 |'
  id: totrans-174
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 左 | 2 | 0.33 | 0 |'
- en: '| 2 | Left | 6 | 0.33 | 0 |'
  id: totrans-175
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 左 | 6 | 0.33 | 0 |'
- en: '| 2 | Down | 1 | 0.33 | 0 |'
  id: totrans-176
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 下 | 1 | 0.33 | 0 |'
- en: '| ... | ... | ... | ... | ... |'
  id: totrans-177
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... |'
- en: '| 14 | Down | 14 | 0.33 | 0 |'
  id: totrans-178
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 下 | 14 | 0.33 | 0 |'
- en: '| 14 | Down | 15 | 0.33 | 1 |'
  id: totrans-179
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 下 | 15 | 0.33 | 1 |'
- en: '| 14 | Right | 14 | 0.33 | 0 |'
  id: totrans-180
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 右 | 14 | 0.33 | 0 |'
- en: '| 14 | Right | 15 | 0.33 | 1 |'
  id: totrans-181
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 右 | 15 | 0.33 | 1 |'
- en: '| 14 | Right | 10 | 0.33 | 0 |'
  id: totrans-182
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 右 | 10 | 0.33 | 0 |'
- en: '| 14 | Up | 15 | 0.33 | 1 |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 上 | 15 | 0.33 | 1 |'
- en: '| 14 | Up | 10 | 0.33 | 0 |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| 14 | 上 | 10 | 0.33 | 0 |'
- en: '| ... | ... | ... | ... | ... |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| ... | ... | ... | ... | ... |'
- en: '| 15 | Left | 15 | 1.0 | 0 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 左 | 15 | 1.0 | 0 |'
- en: '| 15 | Down | 15 | 1.0 | 0 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 下 | 15 | 1.0 | 0 |'
- en: '| 15 | Right | 15 | 1.0 | 0 |'
  id: totrans-188
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 右 | 15 | 1.0 | 0 |'
- en: '| 15 | Up | 15 | 1.0 | 0 |'
  id: totrans-189
  prefs: []
  type: TYPE_TB
  zh: '| 15 | 上 | 15 | 1.0 | 0 |'
- en: 'Horizon: Time changes what’s optimal'
  id: totrans-190
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 地平线：时间改变最优解
- en: We can represent time in MDPs as well. A *time step*, also referred to as epoch,
    cycle, iteration, or even interaction, is a global clock syncing all parties and
    discretizing time. Having a clock gives rise to a couple of possible types of
    tasks. An *episodic* task is a task in which there’s a finite number of time steps,
    either because the clock stops or because the agent reaches a terminal state.
    There are also continuing tasks, which are tasks that go on forever; there are
    no terminal states, so there’s an infinite number of time steps. In this type
    of task, the agent must be stopped manually.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以在马尔可夫决策过程（MDP）中表示时间。一个**时间步**，也称为纪元、周期、迭代或甚至交互，是一个全局时钟，同步所有各方并离散化时间。拥有一个时钟会产生几种可能的任务类型。一个**周期性**任务是一个时间步数有限的任务，要么因为时钟停止，要么因为智能体达到终端状态。还有持续任务，这些任务会一直进行下去；没有终端状态，因此有无限多个时间步。在这种任务中，智能体必须手动停止。
- en: 'Episodic and continuing tasks can also be defined from the agent’s perspective.
    We call it the *planning horizon*. On the one hand, a *finite horizon* is a planning
    horizon in which the agent knows the task will terminate in a finite number of
    time steps: if we forced the agent to complete the frozen lake environment in
    15 steps, for example. A special case of this kind of planning horizon is called
    a *greedy horizon*, of which the planning horizon is one. The BW and BSW have
    both a greedy planning horizon: the episode terminates immediately after one interaction.
    In fact, all bandit environments have greedy horizons.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 周期性和持续任务也可以从智能体的角度定义。我们称之为**规划地平线**。一方面，**有限地平线**是一个智能体知道任务将在有限个时间步内终止的规划地平线：例如，如果我们强迫智能体在15步内完成冻结的湖环境。这种规划地平线的一个特殊情况称为**贪婪地平线**，其规划地平线为1。BW和BSW都有一个贪婪的规划地平线：一个交互后立即结束纪元。实际上，所有赌博机环境都有贪婪地平线。
- en: On the other hand, an *infinite horizon* is when the agent doesn’t have a predetermined
    time step limit, so the agent plans for an infinite number of time steps. Such
    a task may still be episodic and therefore terminate, but from the perspective
    of the agent, its planning horizon is infinite. We refer to this type of infinite
    planning horizon task as an *indefinite horizon* task. The agent plans for infinite,
    but interactions may be stopped at any time by the environment.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，**无限视野**是指智能体没有预定的时间步限制，因此智能体计划无限多个时间步。这样的任务可能仍然是周期性的，因此可以终止，但从智能体的角度来看，其规划视野是无限的。我们将这种类型的无限规划视野任务称为**不确定视野**任务。智能体计划无限，但环境可以随时停止交互。
- en: 'For tasks in which there’s a high chance the agent gets stuck in a loop and
    never terminates, it’s common practice to add an artificial terminal state based
    on the time step: a hard time step limit using the transition function. These
    cases require special handling of the time step limit terminal state. The environment
    for chapters 8, 9, and 10, the cart pole environment, has this kind of artificial
    terminal step, and you’ll learn to handle these special cases in those chapters.'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 对于智能体有很高的可能性陷入循环而无法终止的任务，通常的做法是添加基于时间步的人工终止状态：使用转换函数的硬时间步限制。这些情况需要对时间步限制终止状态进行特殊处理。第8、9、10章的环境，即小车和杆环境，就有这种人工终止步，你将在那些章节中学习如何处理这些特殊情况。
- en: The BW, BSW, and FL environment are *indefinite planning horizon*; the agent
    plans for infinite number of steps, but interactions may stop at any time. We
    won’t add a time step limit to the FL environment because there’s a high chance
    the agent will terminate naturally; the environment is highly stochastic. This
    kind of task is the most common in RL.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: BW、BSW和FL环境是**不确定规划视野**；智能体计划无限多个步骤，但交互可以随时由环境停止。我们不会在FL环境中添加时间步限制，因为智能体自然终止的可能性很高；环境高度随机。这种任务在强化学习（RL）中最为常见。
- en: We refer to the sequence of consecutive time steps from the beginning to the
    end of an episodic task as an *episode*, *trial*, *period*, or *stage*. In indefinite
    planning horizons, an episode is a collection containing all interactions between
    an initial and a terminal state.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将一个周期性任务从开始到结束的连续时间步序列称为**周期**、**试验**、**阶段**或**阶段**。在不确定规划视野中，一个周期是一个包含从初始状态到终止状态之间所有交互的集合。
- en: 'Discount: The future is uncertain, value it less'
  id: totrans-197
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 折现：未来是不确定的，因此价值较低
- en: Because of the possibility of infinite sequences of time steps in infinite horizon
    tasks, we need a way to discount the value of rewards over time; that is, we need
    a way to tell the agent that getting +1’s is better sooner than later. We commonly
    use a positive real value less than one to exponentially discount the value of
    future rewards. The further into the future we receive the reward, the less valuable
    it is in the present.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 由于无限视野任务中可能存在无限的时间步序列，我们需要一种方法来随时间折现奖励的价值；也就是说，我们需要一种方法告诉智能体，尽早获得+1比晚些时候获得更好。我们通常使用一个小于1的正实数来指数折现未来奖励的价值。我们接收奖励的时间越远，它在当前的价值就越低。
- en: This number is called the *discount factor,* or *gamma*. The discount factor
    adjusts the importance of rewards over time. The later we receive rewards, the
    less attractive they are to present calculations. Another important reason why
    the discount factor is commonly used is to reduce the variance of return estimates.
    Given that the future is uncertain, and that the further we look into the future,
    the more stochasticity we accumulate and the more variance our value estimates
    will have, the discount factor helps reduce the degree to which future rewards
    affect our value function estimates, which stabilizes learning for most agents.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数字被称为**折现因子**，或**伽玛**。折现因子调整随时间变化的奖励的重要性。我们接收奖励的时间越晚，它们对当前计算就越没有吸引力。折现因子通常被广泛使用的一个重要原因是减少回报估计的方差。鉴于未来是不确定的，而且我们看得越远，积累的随机性就越多，我们的价值估计的方差就越大，折现因子有助于减少未来奖励对我们价值函数估计的影响，这对于大多数智能体的学习是稳定的。
- en: '![](../Images/02_15.png)'
  id: totrans-200
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/02_15.png)'
- en: Effect of discount factor and time on the value of rewards
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: 折现因子和时间对奖励价值的影响
- en: 'Interestingly, gamma is part of the MDP definition: the problem, and not the
    agent. However, often you’ll find no guidance for the proper value of gamma to
    use for a given environment. Again, this is because gamma is also used as a hyperparameter
    for reducing variance, and therefore left for the agent to tune.'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 有趣的是，伽马是MDP定义的一部分：问题，而不是智能体。然而，你通常找不到关于给定环境伽马适当值的指导。再次强调，这是因为伽马也被用作减少方差的超参数，因此留给智能体调整。
- en: You can also use gamma as a way to give a sense of “urgency” to the agent. To
    wrap your head around that, imagine that I tell you I’ll give you $1,000 once
    you finish reading this book, but I’ll discount (gamma) that reward by 0.5 daily.
    This means that every day I cut the value that I pay in half. You’ll probably
    finish reading this book today. If I say gamma is 1, then it doesn’t matter when
    you finish it, you still get the full amount.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以使用伽马（gamma）作为给智能体（agent）传达“紧迫感”的一种方式。为了更好地理解这一点，想象一下我告诉你，一旦你读完这本书，我会给你1000美元，但我每天会按0.5的比例（伽马）减少这个奖励。这意味着我每天都会将我支付的价值减半。你可能会在今天读完这本书。如果我说伽马是1，那么你何时完成它都无关紧要，你仍然会得到全额。
- en: For the BW and BSW environments, a gamma of 1 is appropriate; for the FL environment,
    however, we’ll use a gamma of 0.99, a commonly used value.
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 对于BW和BSW环境，伽马值为1是合适的；然而，对于FL环境，我们将使用0.99的伽马值，这是一个常用的值。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathThe discount factor (gamma)
    |'
  id: totrans-205
  prefs: []
  type: TYPE_TB
  zh: '| ![数学图标](../Images/icons_Math.png) | 展示数学 | 折现因子（伽马） |'
- en: '|  | ![](../Images/02_15_sidebar07.png) |'
  id: totrans-206
  prefs: []
  type: TYPE_TB
  zh: '|  | ![侧边栏图片](../Images/02_15_sidebar07.png) |'
- en: Extensions to MDPs
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: MDP的扩展
- en: 'There are many extensions to the MDP framework, as we’ve discussed. They allow
    us to target slightly different types of RL problems. The following list isn’t
    comprehensive, but it should give you an idea of how large the field is. Know
    that the acronym MDPs is often used to refer to all types of MDPs. We’re currently
    looking only at the tip of the iceberg:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 如我们之前讨论的，MDP框架有许多扩展。它们允许我们针对不同类型的强化学习（RL）问题。以下列表并不全面，但它应该能给你一个关于该领域规模的概念。要知道，MDPs的缩写通常用来指代所有类型的MDP。我们目前只看到了冰山一角：
- en: '): When the agent cannot fully observe the environment state'
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: )：当智能体无法完全观察环境状态时
- en: 'Factored Markov decision process (FMDP): Allows the representation of the transition
    and reward function more compactly so that we can represent large MDPs'
  id: totrans-210
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 因子马尔可夫决策过程（FMDP）：允许更紧凑地表示转移和奖励函数，以便我们可以表示大型MDP
- en: 'Continuous [Time|Action|State] Markov decision process: When either time, action,
    state or any combination of them are continuous'
  id: totrans-211
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 连续 [时间|动作|状态] 马尔可夫决策过程：当时间、动作、状态或它们的任何组合是连续的
- en: 'Relational Markov decision process (RMDP): Allows the combination of probabilistic
    and relational knowledge'
  id: totrans-212
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 关联马尔可夫决策过程（RMDP）：允许结合概率和关系知识
- en: 'Semi-Markov decision process (SMDP): Allows the inclusion of abstract actions
    that can take multiple time steps to complete'
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半马尔可夫决策过程（SMDP）：允许包含需要多个时间步才能完成的抽象动作
- en: 'Multi-agent Markov decision process (MMDP): Allows the inclusion of multiple
    agents in the same environment'
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多智能体马尔可夫决策过程（MMDP）：允许在相同环境中包含多个智能体
- en: '): Allows for multiple agents to collaborate and maximize a common reward'
  id: totrans-215
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: )：允许多个智能体协作并最大化共同奖励
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe bandit walk (BW) MDP
    |'
  id: totrans-216
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python智能体 | 走廊智能体（BW）马尔可夫决策过程（MDP）
    |'
- en: '|  |'
  id: totrans-217
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE0]'
  id: totrans-218
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '① The outer dictionary keys are the states.② The inner dictionary keys are
    the actions.③ The value of the inner dictionary is a list with all possible transitions
    for that state-action pair.④ The transition tuples have four values: the probability
    of that transition, the next state, the reward, and a flag indicating whether
    the next state is terminal.⑤ You can also load the MDP this way. |'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ① 外部字典的键是状态。② 内部字典的键是动作。③ 内部字典的值是一个列表，包含该状态-动作对的所有可能转移。④ 转移元组有四个值：该转移的概率、下一个状态、奖励以及一个标志，表示下一个状态是否是终端状态。⑤
    你也可以用这种方式加载MDP。 |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe bandit slippery walk
    (BSW) MDP |'
  id: totrans-220
  prefs: []
  type: TYPE_TB
  zh: '| ![Python图标](../Images/icons_Python.png) | 我会说Python智能体 | 走廊滑行智能体（BSW）马尔可夫决策过程（MDP）
    |'
- en: '|  |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE1]'
  id: totrans-222
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ① Look at the terminal states. States 0 and 2 are terminal.② This is how you
    build stochastic transitions. This is state 1, action 0.③ These are the transitions
    after taking action 1 in state 1.④ This is how you can load the Bandit Slippery
    Walk in the Notebook; make sure to check them out! |
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: ①看看终止状态。状态0和2是终止状态。②这是构建随机转换的方法。这是状态1，动作0。③这是在状态1采取动作1后的转换。④这是如何在笔记本中加载Bandit
    Slippery Walk的方法；确保查看它们！ |
- en: '| ![](../Images/icons_Python.png) | I Speak PythonThe frozen lake (FL) MDP
    |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Python.png) | 我会说Python冻结湖（FL）MDP |'
- en: '|  |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '|  |'
- en: '[PRE2]'
  id: totrans-226
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ①Probability of landing in state 0 when selecting action 0 in state 0②Probability
    of landing in state 4 when selecting action 0 in state 0③You can group the probabilities,
    such as in this line.④Or be explicit, such as in these two lines. It works fine
    either way.⑤Lots removed from this example for clarity.⑥Go to the Notebook for
    the complete FL MDP.⑧State 14 is the only state that provides a non-zero reward.
    Three out of four actions have a single transition that leads to state 15\. Landing
    on state 15 provides a +1 reward.⑨State 15 is a terminal state.⑩Again, you can
    load the MDP like so. |
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ①在状态0选择动作0时落在状态0的概率②在状态0选择动作0时落在状态4的概率③你可以将概率分组，例如在这一行中。④或者明确表示，例如在这两行中。两种方式都可以。⑤为了清晰起见，从这个例子中去掉了许多内容。⑥前往笔记本查看完整的FL
    MDP。⑧状态14是唯一提供非零奖励的状态。四个动作中有三个只有一个转换会导致状态15。落在状态15提供+1奖励。⑨状态15是一个终止状态。⑩同样，你可以这样加载MDP。
    |
- en: Putting it all together
  id: totrans-228
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 整合一切
- en: Unfortunately, when you go out to the real world, you’ll find many different
    ways that MDPs are defined. Moreover, certain sources describe POMDPs and refer
    to them as MDPs without full disclosure. All of this creates confusion to newcomers,
    so I have a few points to clarify for you going forward. First, what you saw previously
    as Python code isn’t a complete MDP, but instead only the transition functions
    and reward signals. From these, we can easily infer the state and action spaces.
    These code snippets come from a few packages containing several environments I
    developed for this book, and the FL environment is part of the OpenAI Gym package
    mentioned in the first chapter. Several of the additional components of an MDP
    that are missing from the dictionaries above, such as the initial state distribution
    *s*θ that comes from the set of initial state *sH*, are not shown in the previous
    dictionary, and the OpenAI Gym framework doesn’t provide them to you. Like I said
    before, discount factors are commonly considered hyperparameters, for better or
    worse. And the horizon is often assumed to be infinity.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 不幸的是，当你进入现实世界时，你会发现有许多不同的方式来定义MDPs。此外，某些来源描述POMDPs并将它们称为MDPs，但没有完全披露。所有这些都给新来者带来了困惑，因此我有一些要点要为你澄清。首先，你之前看到的Python代码不是一个完整的MDP，而只是转换函数和奖励信号。从这些中，我们可以轻松推断状态和动作空间。这些代码片段来自几个包含我为这本书开发的几个环境的包，FL环境是第一章节中提到的OpenAI
    Gym包的一部分。MDP中缺失的许多额外组件，如来自初始状态集合 *sH* 的初始状态分布 *s*θ，在上面的字典中没有显示，并且OpenAI Gym框架不提供这些。就像我之前说的那样，折现因子通常被认为是超参数，不管好坏。而且，通常假设视野是无限的。
- en: But don’t worry about this. First, to calculate optimal policies for the MDPs
    presented in this chapter (which we’ll do in the next chapter), we only need the
    dictionary shown previously containing the transition function and reward signal;
    from these, we can infer the state and action spaces, and I’ll provide you with
    the discount factors. We’ll assume horizons of infinity, and won’t need the initial
    state distribution. Additionally, the most crucial part of this chapter is to
    give you an awareness of the components of MDPs and POMDPs. Remember, you won’t
    have to do much more building of MDPs than what you’ve done in this chapter. Nevertheless,
    let me define MDPs and POMDPs so we’re in sync.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 但不用担心这个问题。首先，为了计算本章中提出的MDPs的最优策略（我们将在下一章中这样做），我们只需要之前显示的包含转换函数和奖励信号的字典；从这些中，我们可以推断状态和动作空间，我会提供给你折现因子。我们将假设视野是无限的，不需要初始状态分布。此外，本章最重要的部分是让你了解MDPs和POMDPs的组件。记住，你不需要比本章中做的更多MDP构建。尽管如此，让我定义MDPs和POMDPs，以便我们保持一致。
- en: '| ![](../Images/icons_Math.png) | Show Me The MathMDPs vs. POMDPs |'
  id: totrans-231
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Math.png) | 展示数学MDPs与POMDPs的比较 |'
- en: '|  | ![](../Images/02_15_sidebar11.png) |'
  id: totrans-232
  prefs: []
  type: TYPE_TB
  zh: '|  | ![](../Images/02_15_sidebar11.png) |'
- en: Summary
  id: totrans-233
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Okay. I know this chapter is heavy on new terms, but that’s its intent. The
    best summary for this chapter is on the previous page, more specifically, the
    definition of an MDP. Take another look at the last two equations and try to remember
    what each letter means. Once you do so, you can be assured that you got what’s
    necessary out of this chapter to proceed.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 好的。我知道这一章有很多新术语，但这正是其目的。这一章的最佳总结在前一页，更具体地说，是关于马尔可夫决策过程（MDP）的定义。再次看看最后两个方程式，并尝试记住每个字母的含义。一旦你做到了这一点，你就可以确信你已经从这一章中获得了继续前进所需的内容。
- en: At the highest level, a reinforcement learning problem is about the interactions
    between an agent and the environment in which the agent exists. A large variety
    of issues can be modeled under this setting. The Markov decision process is a
    mathematical framework for representing complex decision-making problems under
    uncertainty.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在最高层次上，强化学习问题涉及智能体与其存在的环境之间的交互。在这个设置下可以模拟各种问题。马尔可夫决策过程是表示不确定条件下复杂决策问题的数学框架。
- en: Markov decision processes (MDPs) are composed of a set of system states, a set
    of per-state *initial state distribution*. States describe the configuration of
    the environment. Actions allow agents to interact with the environment. The transition
    function tells how the environment evolves and reacts to the agent’s actions.
    The reward signal encodes the goal to be achieved by the agent. The horizon and
    discount factor add a notion of time to the interactions.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 马尔可夫决策过程（MDPs）由一组系统状态、每个状态的一组*初始状态分布*组成。状态描述了环境的配置。动作允许智能体与环境交互。转移函数说明了环境如何演变以及如何对智能体的动作做出反应。奖励信号编码了智能体要实现的目标。视野和折现因子为交互添加了时间概念。
- en: The state space, the set of all possible states, can be infinite or finite.
    The number of variables that make up a single state, however, must be finite.
    States can be fully observable, but in a more general case of MDPs, a POMDP, the
    states are partially observable. This means the agent can’t observe the full state
    of the system, but can observe a noisy state instead, called an observation.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 状态空间，所有可能状态的集合，可以是无限的或有限的。然而，组成单个状态的变量数量必须是有限的。状态可以是完全可观察的，但在更一般的马尔可夫决策过程（MDP）的更一般情况下，即部分可观察马尔可夫决策过程（POMDP），状态是部分可观察的。这意味着智能体不能观察到系统的完整状态，而只能观察到一种噪声状态，称为观察。
- en: The action space is a set of actions that can vary from state to state. However,
    the convention is to use the same set for all states. Actions can be composed
    with more than one variable, just like states. Action variables may be discrete
    or continuous.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 动作空间是一组可以随状态变化的动作。然而，惯例是使用相同的集合来表示所有状态。动作可以与多个变量组合，就像状态一样。动作变量可以是离散的或连续的。
- en: The transition function links a state (a next state) to a state-action pair,
    and it defines the probability of reaching that future state given the state-action
    pair. The reward signal, in its more general form, maps a transition tuple *s*,
    *a*, *s'* to scalar, and it indicates the goodness of the transition. Both the
    transition function and reward signal define the model of the environment and
    are assumed to be stationary, meaning probabilities stay the same throughout.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 转移函数将一个状态（下一个状态）与一个状态-动作对联系起来，并定义了在给定状态-动作对的情况下达到该未来状态的概率。奖励信号在其更一般的形式下，将一个转移元组
    *s*, *a*, *s'* 映射到标量，并指示转移的好坏。转移函数和奖励信号定义了环境模型，并且假设是平稳的，这意味着概率在整个过程中保持不变。
- en: By now, you
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 到现在为止，
- en: Understand the components of a reinforcement learning problem and how they interact
    with each other
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 理解强化学习问题的组成部分以及它们如何相互作用
- en: Recognize Markov decision processes and know what they are composed from and
    how they work
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 认识到马尔可夫决策过程，并了解它们由什么组成以及它们是如何工作的
- en: Can represent sequential decision-making problems as MDPs
  id: totrans-243
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以将顺序决策问题表示为马尔可夫决策过程（MDPs）
- en: '| ![](../Images/icons_Tweet.png) | Tweetable FeatWork on your own and share
    your findings |'
  id: totrans-244
  prefs: []
  type: TYPE_TB
  zh: '| ![](../Images/icons_Tweet.png) | 可分享的亮点：自己动手并分享你的发现 |'
- en: '|  | Here are several ideas on how to take what you have learned to the next
    level. If you’d like, share your results with the rest of the world, and make
    sure to check out what others have done, too. It’s a win-win situation, and hopefully,
    you’ll take advantage of it.'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '|  | 这里有一些如何将你所学的内容提升到下一个层次的想法。如果你愿意，与世界分享你的结果，并确保查看其他人所做的事情。这是一个双赢的局面，希望你能充分利用它。'
- en: '**#gdrl_ch02_tf01:** Creating environments is a crucial skill that deserves
    a book of its own. How about you create a grid-world environment of your own?
    Here, look at the code for the walk environments in this chapter ([https://github.com/mimoralea/gym-walk](https://github.com/mimoralea/gym-walk))
    and some other grid-world environments ([https://github.com/mimoralea/gym-aima](https://github.com/mimoralea/gym-aima),
    [https://github.com/mimoralea/gym-bandits](https://github.com/mimoralea/gym-bandits),
    [https://github.com/openai/gym/tree/master/gym/envs/toy_text](https://github.com/openai/gym/tree/master/gym/envs/toy_text)).
    Now, create a Python package with a new grid-world environment! Don’t limit yourself
    to simple move actions; you can create a ‘teleport’ action, or anything else.
    Also, maybe add creatures to the environment other than your agent. Maybe add
    little monsters that your agent needs to avoid. Get creative here. There’s so
    much you could do.'
  id: totrans-246
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch02_tf01:** 创建环境是一项至关重要的技能，值得有一本书来专门介绍。你为什么不自己创建一个网格世界环境呢？在这里，看看本章中步行环境的代码([https://github.com/mimoralea/gym-walk](https://github.com/mimoralea/gym-walk))以及一些其他的网格世界环境([https://github.com/mimoralea/gym-aima](https://github.com/mimoralea/gym-aima),
    [https://github.com/mimoralea/gym-bandits](https://github.com/mimoralea/gym-bandits),
    [https://github.com/openai/gym/tree/master/gym/envs/toy_text](https://github.com/openai/gym/tree/master/gym/envs/toy_text))。现在，创建一个包含新网格世界环境的Python包！不要限制自己只做简单的移动动作；你可以创建一个‘传送’动作，或者任何其他动作。也许还可以在环境中添加除了你的智能体之外的其他生物。也许可以添加一些小怪物，你的智能体需要避开。在这里发挥创意。你可以做很多事情。'
- en: '**#gdrl_ch02_tf02:** Another thing to try is to create what is called a “Gym
    environment” for a simulation engine of your choosing. First, investigate what
    exactly is a “Gym environment.” Next, explore the following Python packages ([https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py),
    [https://github.com/openai/atari-py](https://github.com/openai/atari-py), [https://github.com/google-research/football](https://github.com/google-research/football),
    and many of the packages at [https://github.com/openai/gym/blob/master/docs/environments.md](https://github.com/openai/gym/blob/master/docs/environments.md)).
    Then, try to understand how others have exposed simulation engines as Gym environments.
    Finally, create a Gym environment for a simulation engine of your choosing. This
    is a challenging one!'
  id: totrans-247
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch02_tf02:** 另一个尝试的事情是为你选择的仿真引擎创建一个所谓的“Gym环境”。首先，调查一下“Gym环境”究竟是什么。接下来，探索以下Python包([https://github.com/openai/mujoco-py](https://github.com/openai/mujoco-py),
    [https://github.com/openai/atari-py](https://github.com/openai/atari-py), [https://github.com/google-research/football](https://github.com/google-research/football),
    以及[https://github.com/openai/gym/blob/master/docs/environments.md](https://github.com/openai/gym/blob/master/docs/environments.md)中的许多包)。然后，尝试理解其他人如何将仿真引擎暴露为Gym环境。最后，为你选择的仿真引擎创建一个Gym环境。这是一个具有挑战性的任务！'
- en: '**#gdrl_ch02_tf03:** In every chapter, I’m using the final hashtag as a catchall
    hashtag. Feel free to use this one to discuss anything else that you worked on
    relevant to this chapter. There’s no more exciting homework than that which you
    create for yourself. Make sure to share what you set yourself to investigate and
    your results.'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**#gdrl_ch02_tf03:** 在每一章中，我都在使用最后的标签作为通用的标签。请随意使用这个标签来讨论与本章相关的任何其他你工作过的事情。没有什么比为自己创造作业更令人兴奋的了。确保分享你设定去调查的内容以及你的结果。'
- en: 'Write a tweet with your findings, tag me @mimoralea (I’ll retweet), and use
    the particular hashtag from this list to help interested folks find your results.
    There are no right or wrong results; you share your findings and check others’
    findings. Take advantage of this to socialize, contribute, and get yourself out
    there! We’re waiting for you!Here’s a tweet example:“Hey, @mimoralea. I created
    a blog post with a list of resources to study deep reinforcement learning. Check
    it out at <link>. #gdrl_ch01_tf01”I’ll make sure to retweet and help others find
    your work. |'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 用你的发现写一条推文，@mimoralea（我会转发），并使用这个列表中的特定标签来帮助感兴趣的人找到你的结果。没有正确或错误的结果；你分享你的发现并检查他人的发现。利用这个机会社交，做出贡献，让自己脱颖而出！我们正在等待你！以下是一条推文示例：“嘿，@mimoralea。我创建了一个包含资源列表的博客文章，用于研究深度强化学习。查看它在这里<链接>。#gdrl_ch01_tf01”我会确保转发并帮助他人找到你的工作。|
