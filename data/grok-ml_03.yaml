- en: 3 Drawing a line close to our points: Linear regression
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 画一条接近我们点的线：线性回归
- en: In this chapter
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章
- en: what is linear regression
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是线性回归
- en: fitting a line through a set of data points
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过一组数据点拟合一条线
- en: coding the linear regression algorithm in Python
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在 Python 中编写线性回归算法
- en: using Turi Create to build a linear regression model to predict housing prices
    in a real dataset
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用 Turi Create 构建线性回归模型以预测真实数据集中的房价
- en: what is polynomial regression
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是多项式回归
- en: fitting a more complex curve to nonlinear data
  id: totrans-7
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过非线性数据拟合更复杂的曲线
- en: discussing examples of linear regression in the real world, such as medical
    applications and recommender systems
  id: totrans-8
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 讨论线性回归在现实世界中的应用实例，例如医疗应用和推荐系统
- en: '![](../Images/3-unnumb.png)'
  id: totrans-9
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-unnumb.png)'
- en: In this chapter, we will learn about linear regression. Linear regression is
    a powerful and widely used method to estimate values, such as the price of a house,
    the value of a certain stock, the life expectancy of an individual, or the amount
    of time a user will watch a video or spend on a website. You may have seen linear
    regression before as a plethora of complicated formulas including derivatives,
    systems of equations, and determinants. However, we can also see linear regression
    in a more graphical and less formulaic way. In this chapter, to understand linear
    regression, all you need is the ability to visualize points and lines moving around.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将学习线性回归。线性回归是一种强大且广泛使用的方法，用于估计值，例如房屋价格、某种股票的价值、个人的预期寿命，或者用户观看视频或花费在网站上的时间。你可能之前已经见过线性回归，它包括许多复杂的公式，如导数、方程组和行列式。然而，我们也可以以更图形化和更少公式化的方式看待线性回归。在本章中，为了理解线性回归，你只需要具备可视化点和线移动的能力。
- en: Let’s say that we have some points that roughly look like they are forming a
    line, as shown in figure 3.1.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一些点，它们大致看起来像是在形成一条线，如图 3.1 所示。
- en: '![](../Images/3-1.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-1.png)'
- en: Figure 3.1 Some points that roughly look like they are forming a line
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.1 一些看起来大致形成一条线的点
- en: The goal of linear regression is to draw the line that passes as close to these
    points as possible. What line would you draw that passes close to those points?
    How about the one shown in figure 3.2?
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归的目标是画出尽可能接近这些点的线。你会画一条什么样的线来接近这些点？图 3.2 中的那条线怎么样？
- en: Think of the points as houses in a town, and our goal is to build a road that
    goes through the town. We want the line to pass as close as possible to the points
    because the town’s inhabitants all want to live close to the road, and our goal
    is to please them as much as we can.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些点想象成镇上的房子，我们的目标是建造一条穿过镇子的路。我们希望这条线尽可能接近这些点，因为镇上的居民都希望住在靠近路的地方，我们的目标是尽可能满足他们的需求。
- en: '![](../Images/3-2.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-2.png)'
- en: Figure 3.2 A line that passes close to the points
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.2 一条经过点的线
- en: We can also imagine the points as magnets lying bolted to the floor (so they
    can’t move). Now imagine throwing a straight metal rod on top of them. The rod
    will move around, but because the magnets pull it, it will eventually end up in
    a position of equilibrium, as close as it can to all the points.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以想象这些点为固定在地板上的磁铁（因此它们不能移动）。现在想象在它们上面扔一根直金属棒。棒会移动，但由于磁铁的吸引，它最终会达到一个平衡位置，尽可能接近所有点。
- en: 'Of course, this can lead to a lot of ambiguity. Do we want a road that goes
    somewhat close to all the houses, or maybe really close to a few of them and a
    bit farther from others? Some questions that arise follow:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 当然，这可能会导致很多歧义。我们是要一条大致靠近所有房子的路，还是可能真的靠近一些房子而远离其他房子？以下是一些随之而来的问题：
- en: What do we mean by “points that roughly look like they are forming a line”?
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们所说的“大致形成一条线的点”是什么意思？
- en: What do we mean by “a line that passes really close to the points”?
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们所说的“经过点非常接近的线”是什么意思？
- en: How do we find such a line?
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们如何找到这样的线？
- en: Why is this useful in the real world?
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这在现实世界中有什么用？
- en: Why is this machine learning?
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 这为什么是机器学习？
- en: In this chapter we answer all these questions, and we build a linear regression
    model to predict housing prices in a real dataset.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们将回答所有这些问题，并构建一个线性回归模型来预测真实数据集中的房价。
- en: 'You can find all the code for this chapter in the following GitHub repository:
    [https://github.com/luisguiserrano/manning/tree/master/Chapter_3_Linear_Regression](https://github.com/luisguiserrano/manning/tree/master/Chapter_3_Linear_Regression).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以在以下GitHub仓库中找到本章的所有代码：[https://github.com/luisguiserrano/manning/tree/master/Chapter_3_Linear_Regression](https://github.com/luisguiserrano/manning/tree/master/Chapter_3_Linear_Regression).
- en: 'The problem: We need to predict the price of a house'
  id: totrans-27
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 问题：我们需要预测房子的价格
- en: Let’s say that we are real estate agents in charge of selling a new house. We
    don’t know the price, and we want to infer it by comparing it with other houses.
    We look at features of the house that could influence the price, such as size,
    number of rooms, location, crime rate, school quality, and distance to commerce.
    At the end of the day, we want a formula for all these features that gives us
    the price of the house, or at least a good estimate for it.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们是负责销售新房子的房地产经纪人。我们不知道价格，但想通过与其他房子的比较来推断它。我们查看可能影响价格的房屋特征，如大小、房间数量、位置、犯罪率、学校质量和距离商业区。最终，我们希望得到一个公式，包含所有这些特征，从而给出房子的价格，或者至少是一个良好的估计。
- en: 'The solution: Building a regression model for housing prices'
  id: totrans-29
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 解决方案：为房价构建回归模型
- en: Let’s go with as simple an example as possible. We look at only one of the features—the
    number of rooms. Our house has four rooms, and there are six houses nearby, with
    one, two, three, five, six, and seven rooms, respectively. Their prices are shown
    in table 3.1.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们尽可能使用一个简单的例子。我们只看一个特征——房间数量。我们的房子有四个房间，附近有六座房子，分别有一、二、三、五、六和七个房间。它们的价格显示在表3.1中。
- en: Table 3.1 A table of houses with the number of rooms and prices. House 4 is
    the one whose price we are trying to infer.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.1 房屋表格，包含房间数量和价格。房屋4是我们试图推断价格的房屋。
- en: '| Number of rooms | Price |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 房间数量 | 价格 |'
- en: '| 1 | 150 |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 150 |'
- en: '| 2 | 200 |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 200 |'
- en: '| 3 | 250 |'
  id: totrans-35
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 250 |'
- en: '| 4 | ? |'
  id: totrans-36
  prefs: []
  type: TYPE_TB
  zh: '| 4 | ? |'
- en: '| 5 | 350 |'
  id: totrans-37
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 350 |'
- en: '| 6 | 400 |'
  id: totrans-38
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 400 |'
- en: '| 7 | 450 |'
  id: totrans-39
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 450 |'
- en: 'What price would you give to house 4, just based on the information on this
    table? If you said $300, then we made the same guess. You probably saw a pattern
    and used it to infer the price of the house. What you did in your head was linear
    regression. Let’s study this pattern more. You may have noticed that each time
    you add a room, $50 is added to the price of the house. More specifically, we
    can think of the price of a house as a combination of two things: a base price
    of $100, and an extra charge of $50 for each of the rooms. This can be summarized
    in a simple formula:'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 你会给房屋4一个什么价格，仅基于这个表格上的信息？如果你说是300美元，那么我们做出了相同的猜测。你可能注意到了一个模式，并利用它来推断房子的价格。你在脑海中做的事情是线性回归。让我们更深入地研究这个模式。你可能已经注意到，每次增加一个房间，房子的价格就会增加50美元。更具体地说，我们可以将房子的价格视为两个东西的组合：一个基础价格为100美元，以及每个房间的额外费用50美元。这可以总结为一个简单的公式：
- en: Price = 100 + 50(Number of rooms)
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 价格 = 100 + 50(房间数量)
- en: What we did here is come up with a model represented by a formula that gives
    us a *prediction* of the price of the house, based on the *feature*, which is
    the number of rooms. The price per room is called the *weight* of that corresponding
    feature, and the base price is called the *bias* of the model. These are all important
    concepts in machine learning. We learned some of them in chapter 1 and 2, but
    let’s refresh our memory by defining them from the perspective of this problem.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在这里做的是提出一个由公式表示的模型，它根据特征（即房间数量）给出房价的*预测*。每间房的价格称为该相应特征的*权重*，基础价格称为模型的*偏差*。这些都是机器学习中的重要概念。我们在第1章和第2章中学习了一些，但让我们通过从这个问题的角度来定义它们来刷新我们的记忆。
- en: 'features The features of a data point are those properties that we use to make
    our prediction. In this case, the features are the number of rooms in the house,
    the crime rate, the age of the house, the size, and so on. For our case, we’ve
    decided on one feature: the number of rooms in the house.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 特征：数据点的特征是我们用来进行预测的属性。在这种情况下，特征是房子的房间数量、犯罪率、房子的年龄、大小等。在我们的案例中，我们决定使用一个特征：房子的房间数量。
- en: labels This is the target that we try to predict from the features. In this
    case, the label is the price of the house.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 标签：这是我们尝试从特征中预测的目标。在这种情况下，标签是房子的价格。
- en: model A machine learning model is a rule, or a formula, which predicts a label
    from the features. In this case, the model is the equation we found for the price.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 模型：机器学习模型是一个规则或公式，它从特征中预测标签。在这种情况下，模型是我们找到的价格方程。
- en: prediction The prediction is the output of the model. If the model says, “I
    think the house with four rooms is going to cost $300,” then the prediction is
    300.
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 预测：预测是模型的输出。如果模型说，“我认为四间房的房子将花费300美元”，那么预测就是300。
- en: weights In the formula corresponding to the model, each feature is multiplied
    by a corresponding factor. These factors are the weights. In the previous formula,
    the only feature is the number of rooms, and its corresponding weight is 50.
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 权重：在对应于模型的公式中，每个特征都乘以一个相应的因子。这些因子是权重。在之前的公式中，唯一的特征是房间数，其相应的权重是50。
- en: bias As you can see, the formula corresponding to the model has a constant that
    is not attached to any of the features. This constant is called the bias. In this
    model, the bias is 100, and it corresponds to the base price of a house.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 偏差：正如你所见，对应于模型的公式有一个与任何特征都不相连的常数。这个常数被称为偏差。在这个模型中，偏差是100，它对应于房子的基础价格。
- en: Now the question is, how did we come up with this formula? Or more specifically,
    how do we get the computer to come up with this weight and bias? To illustrate
    this, let’s look at a slightly more complicated example. And because this is a
    machine learning problem, we will approach it using the remember-formulate-predict
    framework that we learned in chapter 2\. More specifically, we’ll *remember* the
    prices of other houses, *formulate* a model for the price, and use this model
    to *predict* the price of a new house.
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们是如何得出这个公式的？或者更具体地说，我们是如何让计算机得出这个权重和偏差的？为了说明这一点，让我们看看一个稍微复杂一点的例子。因为这是一个机器学习问题，我们将使用我们在第2章中学到的记住-制定-预测框架来处理它。更具体地说，我们将*记住*其他房子的价格，*制定*一个价格模型，并使用这个模型来*预测*新房子的价格。
- en: 'The remember step: Looking at the prices of existing houses'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 记忆步骤：查看现有房子的价格
- en: To see the process more clearly, let’s look at a slightly more complicated dataset,
    such as the one in table 3.2.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 为了更清楚地看到这个过程，让我们看看一个稍微复杂一些的数据集，比如表3.2中的数据集。
- en: Table 3.2 A slightly more complicated dataset of houses with their number of
    rooms and their price
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2 房屋及其房间数和价格的一个稍微复杂的数据集
- en: '| Number of rooms | Price |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '| 房间数 | 价格 |'
- en: '| 1 | 155 |'
  id: totrans-54
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 155 |'
- en: '| 2 | 197 |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 197 |'
- en: '| 3 | 244 |'
  id: totrans-56
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 244 |'
- en: '| 4 | ? |'
  id: totrans-57
  prefs: []
  type: TYPE_TB
  zh: '| 4 | ? |'
- en: '| 5 | 356 |'
  id: totrans-58
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 356 |'
- en: '| 6 | 407 |'
  id: totrans-59
  prefs: []
  type: TYPE_TB
  zh: '| 6 | 407 |'
- en: '| 7 | 448 |'
  id: totrans-60
  prefs: []
  type: TYPE_TB
  zh: '| 7 | 448 |'
- en: This dataset is similar to the previous one, except now the prices don’t follow
    a nice pattern, where each price is $50 more than the previous one. However, it’s
    not that far from the original dataset, so we can expect that a similar pattern
    should approximate these values well.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集与之前的一个类似，但现在价格并不遵循一个很好的模式，即每个价格比前一个高50美元。然而，它离原始数据集并不远，因此我们可以预期一个类似的模式应该很好地近似这些值。
- en: Normally, the first thing we do when we get a new dataset is to plot it. In
    figure 3.3, we can see a plot of the points in a coordinate system in which the
    horizontal axis represents the number of rooms, and the vertical axis represents
    the price of the house.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，当我们得到一个新的数据集时，我们首先做的事情是绘制它。在图3.3中，我们可以看到一个坐标系统中的点，其中水平轴代表房间数，垂直轴代表房子的价格。
- en: '![](../Images/3-3.png)'
  id: totrans-63
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-3.png)'
- en: Figure 3.3 Plot of the dataset in table 3.2\. The horizontal axis represents
    the number of rooms, and the vertical axis represents the price of the house.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 表3.2中数据集的散点图。水平轴代表房间数，垂直轴代表房子的价格。
- en: 'The formulate step: Formulating a rule that estimates the price of the house'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 公式步骤：制定一个估算房价的规则
- en: 'The dataset in table 3.2 is close enough to the one in table 3.1, so for now,
    we can feel safe using the same formula for the price. The only difference is
    that now the prices are not exactly what the formula says, and we have a small
    error. We can write the equation as follows:'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.2中的数据集与表3.1中的数据集非常接近，所以目前我们可以安全地使用相同的公式来计算价格。唯一的区别是，现在的价格并不完全符合公式，我们有一个小的误差。我们可以将方程写成以下形式：
- en: Price = 100 + 50(Number of rooms) + (Small error)
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 价格 = 100 + 50(房间数) + (小误差)
- en: If we want to predict prices, we can use this equation. Even though we are not
    sure we’ll get the actual value, we know that we are likely to get close. Now
    the question is, how did we find this equation? And most important, how does a
    computer find this equation?
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们想要预测价格，我们可以使用这个方程。尽管我们不确定我们会得到实际值，但我们知道我们很可能会接近。现在的问题是，我们是如何找到这个方程的？最重要的是，计算机是如何找到这个方程的？
- en: Let’s go back to the plot and see what the equation means there. What happens
    if we look at all the points in which the vertical (*y*) coordinate is 100 plus
    50 times the horizontal (*x*) coordinate? This set of points forms a line with
    slope 50 and *y*-intercept 100\. Before we unpack the previous statement, here
    are the definitions of slope, *y*-intercept, and the equation of a line. We delve
    into these in more detail in the “Crash course on slope and *y*-intercept” section.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到图表，看看方程在这里的含义。如果我们查看所有垂直（*y*）坐标是100加上50倍水平（*x*）坐标的点会发生什么？这组点形成了一条斜率为50，*y*截距为100的直线。在我们展开前面的陈述之前，这里有一些斜率、*y*截距和直线方程的定义。我们将在“斜率和*y*截距快速入门”部分更详细地探讨这些内容。
- en: slope The slope of a line is a measure of how steep it is. It is calculated
    by dividing the rise over the run (i.e., how many units it goes up, divided by
    how many units it goes to the right). This ratio is constant over the whole line.
    In a machine learning model, this is the weight of the corresponding feature,
    and it tells us how much we expect the value of the label to go up, when we increase
    the value of the feature by one unit. If the line is horizontal, then the slope
    is zero, and if the line goes down, the slope is negative.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率 直线的斜率是衡量其陡峭程度的度量。它是通过将上升量除以运行量（即上升了多少单位，除以向右移动了多少单位）来计算的。这个比率在整个直线上是恒定的。在机器学习模型中，这是相应特征的权重，它告诉我们当我们增加一个单位的特征值时，我们期望标签值上升多少。如果直线是水平的，那么斜率为零，如果直线向下倾斜，那么斜率为负。
- en: '*y*-intercept The *y*-intercept of a line is the height at which the line crosses
    the vertical (*y-*) axis. In a machine learning model, it is the bias and tells
    us what the label would be in a data point where all the features are precisely
    zero.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: '*y*截距 直线的*y*截距是直线与垂直（*y*）轴交叉的高度。在机器学习模型中，它是偏差，告诉我们当所有特征都精确为零的数据点中的标签值是什么。'
- en: 'linear equation This is the equation of a line. It is given by two parameters:
    the slope and the *y*-intercept. If the slope is *m* and the *y*-intercept is
    *b*, then the equation of the line is *y* = *mx* + *b*, and the line is formed
    by all the points (*x,y*) that satisfy the equation. In a machine learning model,
    *x* is the value of the feature and *y* is the prediction for the label. The weight
    and bias of the model are *m* and *b*, respectively.'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 线性方程 这是一条直线的方程。它由两个参数给出：斜率和*y*截距。如果斜率是*m*，*y*截距是*b*，那么直线的方程是*y* = *mx* + *b*，直线由满足该方程的所有点(*x,y*)形成。在机器学习模型中，*x*是特征的值，*y*是标签的预测。模型的权重和偏差分别是*m*和*b*。
- en: We can now analyze the equation. When we say that the slope of the line is 50—this
    means that each time we add one room to the house, we estimate that the price
    of the house will go up by $50\. When we say that the *y*-intercept of the line
    is 100, this means that the estimate for the price of a (hypothetical) house with
    zero rooms would be the base price of $100\. This line is drawn in figure 3.4.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们可以分析这个方程了。当我们说直线的斜率是50时——这意味着每次我们给房子增加一个房间，我们估计房子的价格将上涨50美元。当我们说直线的*y*截距是100时，这意味着一个（假设的）零房间房子的价格估计将是基础价格100美元。这条直线在图3.4中绘制。
- en: '![](../Images/3-4.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-4.png)'
- en: Figure 3.4 The model we formulate is the line that goes as close as possible
    to all the houses.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 我们构建的模型是尽可能接近所有房子的直线。
- en: Now, of all the possible lines (each with its own equation), why did we pick
    this one in particular? Because that one passes close to the points. There may
    be a better one, but at least we know this one is good, as opposed to one that
    goes nowhere near the points. Now we are back to the original problem, where we
    have a set of houses, and we want to build a road as close as possible to them.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，在所有可能的直线（每条直线都有自己的方程）中，我们为什么选择这条特定的直线？因为它接近这些点。可能还有更好的选择，但至少我们知道这条直线是好的，而不是一条根本不接近这些点的直线。现在我们回到了原始问题，我们有一组房子，我们想要建造一条尽可能靠近它们的路。
- en: How do we find this line? We’ll look at this later in the chapter. But for now,
    let’s say that we have a crystal ball that, given a bunch of points, finds the
    line that passes the closest to them.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 我们如何找到这条直线？我们将在本章后面讨论这个问题。但就目前而言，让我们假设我们有一个水晶球，给定一些点，找到最接近它们的直线。
- en: 'The predict step: What do we do when a new house comes on the market?'
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 预测步骤：当市场上出现一栋新房子时，我们该怎么办？
- en: 'Now, on to using our model to predict the price of the house with four rooms.
    For this, we plug the number four as the feature in our formula to get the following:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们使用我们的模型来预测四间房的房屋价格。为此，我们将数字四作为特征代入公式，得到以下结果：
- en: Price = 100 + 50 · 4 = 300
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 价格 = 100 + 50 · 4 = 300
- en: Therefore, our model predicts that the house costs $300\. This can also be seen
    graphically by using the line, as illustrated in figure 3.5.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，我们的模型预测这所房子的价格为$300。这也可以通过图3.5中的线图来直观地看出。
- en: '![](../Images/3-5.png)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-5.png)'
- en: Figure 3.5 Our task is now to predict the price of the house with four rooms.
    Using the model (line), we deduce that the predicted price of this house is $300.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5 我们现在的任务是预测四间房房屋的价格。使用模型（线），我们推断出这所房子的预测价格为$300。
- en: What if we have more variables? Multivariate linear regression
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们有更多变量怎么办？多元线性回归
- en: 'In the previous sections we learned about a model that predicts the price of
    a house based on one feature—the number of rooms. We may imagine many other features
    that could help us predict the price of a house, such as the size, the quality
    of the schools in the neighborhood, and the age of the house. Can our linear regression
    model accommodate these other variables? Absolutely. When the only feature is
    the number of rooms, our model predicts the price as the sum of the feature times
    their corresponding weight, plus a bias. If we have more features, all we need
    to do is multiply them by their corresponding weights and add them to the predicted
    price. Therefore, a model for the price of a house could look like this:'
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了一个基于一个特征（房间数量）预测房屋价格的模型。我们可以想象许多其他有助于预测房屋价格的特征，例如面积、邻居区的学校质量以及房屋年龄。我们的线性回归模型能否容纳这些其他变量？当然可以。当唯一的特征是房间数量时，我们的模型通过将特征乘以它们的对应权重并加上偏差来预测价格。如果我们有更多特征，我们只需要将它们乘以它们的对应权重并将它们加到预测价格上。因此，房屋价格的模型可能看起来像这样：
- en: Price = 30(number of rooms) + 1.5(size) + 10(quality of the schools) – 2(age
    of the house) + 50
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 价格 = 30（房间数量）+ 1.5（面积）+ 10（学校质量）- 2（房屋年龄）+ 50
- en: In this equation, why are all of the weights positive, except for the one corresponding
    to the age of the house? The reason is the other three features (number of rooms,
    size, and quality of the schools) are *positively correlated* to the price of
    the house. In other words, because houses that are bigger and well located cost
    more, the higher this feature is, the higher we expect the price of the house
    to be. However, because we would imagine that older houses tend to be less expensive,
    the age feature is *negatively correlated* to the price of the house.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个方程中，为什么除了房屋年龄对应的权重外，所有权重都是正的？原因是其他三个特征（房间数量、面积和学校质量）与房屋价格呈**正相关**。换句话说，因为大房子和位置好的房子价格更高，这个特征值越高，我们预计房屋价格也越高。然而，因为我们想象到老房子通常更便宜，所以年龄特征与房屋价格呈**负相关**。
- en: What if the weight of a feature is zero? This happens when a feature is irrelevant
    to the price. For example, imagine a feature that measured the number of neighbors
    whose last name starts with the letter *A*. This feature is mostly irrelevant
    to the price of the house, so we would expect that in a reasonable model, the
    weight corresponding to this feature is either zero or something very close to
    it.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个特征的权重为零怎么办？这种情况发生在特征与价格无关时。例如，想象一个测量邻居姓氏以字母“A”开头的数量的特征。这个特征对房屋价格的影响主要不大，因此我们预计在合理的模型中，对应这个特征的权重要么是零，要么非常接近零。
- en: In a similar way, if a feature has a very high weight (whether negative or positive),
    we interpret this as the model telling us that that feature is important in determining
    the price of the house. In the previous model, it seems that the number of rooms
    is an important feature, because its weight is the largest (in absolute value).
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 以类似的方式，如果一个特征的权重非常高（无论是正还是负），我们将其解释为模型在告诉我们这个特征在确定房屋价格方面很重要。在前一个模型中，似乎房间数量是一个重要的特征，因为它的权重最大（绝对值）。
- en: In the section called “Dimensionality reduction simplifies data without losing
    too much information” in chapter 2, we related the number of columns in a dataset
    to the dimension in which the dataset lives. Thus, a dataset with two columns
    can be represented as a set of points in the plane, and a dataset with three columns
    can be represented as a set of points in three-dimensional space. In such a dataset,
    a linear regression model corresponds not to a line but to a plane that passes
    as close as possible to the points. Imagine having many flies flying around in
    the room in a stationary position, and our task is to try to pass a gigantic cardboard
    sheet as close as we can to all the flies. This is multivariate linear regression
    with three variables. The problem becomes hard to visualize for datasets with
    more columns, but we can always imagine a linear equation with many variables.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 在第二章的“降维简化数据而不丢失太多信息”部分中，我们将数据集的列数与数据集所在的维度联系起来。因此，具有两列的数据集可以表示为平面上的点集，具有三列的数据集可以表示为三维空间中的点集。在这样的数据集中，线性回归模型对应于一条线，而不是一个尽可能接近点的平面。想象一下，房间里有很多苍蝇在静止状态下飞来飞去，我们的任务是尽可能接近所有苍蝇通过一张巨大的纸板。这是具有三个变量的多元线性回归。对于具有更多列的数据集，问题变得难以可视化，但我们总是可以想象一个具有许多变量的线性方程。
- en: In this chapter, we mostly deal with training linear regression models with
    only one feature, but the procedure is similar with more features. I encourage
    you to read about it while keeping this fact in the back of your mind, and imagine
    how you would generalize each of our next statements to a case with several features.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们主要处理只有一个特征的线性回归模型的训练，但如果有更多特征，程序是相似的。我鼓励你在心中记住这个事实，并想象你如何将我们接下来的每个陈述推广到具有多个特征的情况。
- en: Some questions that arise and some quick answers
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 一些出现的问题和一些快速回答
- en: OK, your head may be ringing with lots of questions. Let’s address some (hopefully
    all) of them!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 好吧，你的脑海中可能充满了许多问题。让我们解决一些（希望是所有）它们！
- en: What happens if the model makes a mistake?
  id: totrans-94
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果模型出错会怎样？
- en: How did you come up with the formula that predicts the price? And what would
    we do if instead of six houses, we had thousands of them?
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 你是如何想出预测价格的公式的？如果我们不是有六套房屋，而是有成千上万套房屋，我们会怎么做？
- en: Say we’ve built this prediction model, and then new houses start appearing in
    the market. Is there a way to update the model with new information?
  id: totrans-96
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 假设我们已经构建了这个预测模型，然后市场上开始出现新的房屋。有没有一种方法可以用新信息更新模型？
- en: 'This chapter answers all these questions, but here are some quick answers:'
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: 本章回答了所有这些问题，但这里有一些快速回答：
- en: '**What happens if the model makes a mistake?**'
  id: totrans-98
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**如果模型出错会怎样？**'
- en: The model is estimating the price of a house, so we expect it to make a small
    mistake pretty much all the time, because it is very hard to hit the exact price.
    The training process consists of finding the model that makes the smallest errors
    at our points.
  id: totrans-99
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 该模型正在估算房屋的价格，因此我们预计它几乎每次都会犯一个小错误，因为很难击中确切的价格。训练过程包括找到在数据点处使错误最小的模型。
- en: '**How did you come up with the formula that predicts the price? And what would
    we do if instead of six houses, we had thousands of them?**'
  id: totrans-100
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**你是如何想出预测价格的公式的？如果我们不是有六套房屋，而是有成千上万套房屋，我们会怎么做？**'
- en: Yes, this is the main question we address in this chapter! When we have six
    houses, the problem of drawing a line that goes close to them is simple, but if
    we have thousands of houses, this task gets hard. What we do in this chapter is
    devise an algorithm, or a procedure, for the computer to find a good line.
  id: totrans-101
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 是的，这正是我们在本章中要解决的主要问题！当我们只有六套房屋时，绘制一条接近它们的线的问题很简单，但如果我们有成千上万的房屋，这项任务就变得困难了。在本章中，我们为计算机设计了一个算法或程序，以便找到一条好的线。
- en: '**Say we’ve built this prediction model, and then new houses start appearing
    in the market. Is there a way to update the model with new information?**'
  id: totrans-102
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: '**假设我们已经构建了这个预测模型，然后市场上开始出现新的房屋。有没有一种方法可以用新信息更新模型？**'
- en: Absolutely! We will build the model in a way that it can be easily updated if
    new data appears. This is always something to look for in machine learning. If
    we’ve built our model in such a way that we need to recalculate the entire model
    every time new data comes in, it won’t be very useful.
  id: totrans-103
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 绝对！我们将构建一个模型，使其在出现新数据时可以轻松更新。这始终是机器学习中要寻找的东西。如果我们构建的模型需要每次新数据到来时都重新计算整个模型，那么它将不会很有用。
- en: 'How to get the computer to draw this line: The linear regression algorithm'
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如何让计算机绘制这条线：线性回归算法
- en: 'Now we get to the main question of this chapter: how do we get a computer to
    draw a line that passes really close to the points? The way we do this is the
    same way we do many things in machine learning: step by step. Start with a random
    line, and figure out a way to improve this line a *little bit* by moving it closer
    to the points. Repeat this process many times, and voilà, we have the desired
    line. This process is called the linear regression algorithm.'
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来探讨本章的主要问题：我们如何让计算机绘制一条非常接近点的线？我们这样做的方式与我们在机器学习中做很多事情的方式相同：一步一步。从一个随机的线开始，找出一种方法，通过将其移近点来稍微改进这条线。重复这个过程多次，
    voilà，我们就得到了想要的线。这个过程被称为线性回归算法。
- en: The procedure may sound silly, but it works really well. Start with a random
    line. Pick a random point in the dataset, and move the line slightly closer to
    that one point. Repeat this process many times, always picking a random point
    in the dataset. The pseudocode for the linear regression algorithm, viewed in
    this geometric fashion, follows. The illustration is shown in figure 3.6.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 这个步骤可能听起来很愚蠢，但它确实非常有效。从一个随机的线开始。在数据集中选择一个随机点，并将线稍微移近这个点。重复这个过程多次，每次都在数据集中选择一个随机点。以这种几何方式查看线性回归算法的伪代码如下。图3.6展示了该图示。
- en: Pseudocode for the linear regression algorithm (geometric)
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法（几何）的伪代码
- en: '**Inputs**: A dataset of points in the plane'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：平面上的点数据集'
- en: '**Outputs**: A line that passes close to the points'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：一条通过点的线'
- en: 'Procedure:'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 步骤：
- en: Pick a random line.
  id: totrans-111
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一条随机线。
- en: 'Repeat many times:'
  id: totrans-112
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-113
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Move the line a little closer to that point.
  id: totrans-114
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将线稍微移近那个点。
- en: '**Return** the line you’ve obtained.'
  id: totrans-115
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**你得到的线。'
- en: '![](../Images/3-6.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-6.png)'
- en: 'Figure 3.6 An illustration of the linear regression algorithm. We start at
    the top left with a random line and end in the bottom left with a line that fits
    the dataset well. At each stage, two things happen: (1) we pick a random point,
    and (2) the point asks the line to move closer to it. After many iterations, the
    line will be in a good position. This figure has only three iterations for illustrative
    purposes, but in real life, many more iterations are needed.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6 线性回归算法的示意图。我们从左上角的一个随机线开始，最终在左下角得到一条很好地拟合数据集的线。在每一个阶段，都会发生两件事：（1）我们选择一个随机点，（2）这个点要求线靠近它。经过多次迭代，线将处于良好的位置。这个图只有三个迭代，用于说明目的，但在现实生活中，需要更多的迭代。
- en: That was the high-level view. To study the process more in detail, we need to
    delve into the mathematical details. Let’s begin by defining some variables.
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 那是高级概述。为了更详细地研究这个过程，我们需要深入了解数学细节。让我们首先定义一些变量。
- en: '*p*: The price of a house in the dataset'
  id: totrans-119
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*p*: 数据集中房屋的价格'
- en: '![](../Images/p_cf.png): The predicted price of a house'
  id: totrans-120
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '![](../Images/p_cf.png)：房屋的预测价格'
- en: '*r*: The number of rooms'
  id: totrans-121
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*r*: 房间数量'
- en: '*m*: The price per room'
  id: totrans-122
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m*: 每间房的价格'
- en: '*b*: The base price for a house'
  id: totrans-123
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*b*: 房屋的基本价格'
- en: Why the hat over the predicted price, ![](../Images/p_cf.png)? Throughout this
    book, the hat indicates that this is the variable that our model is predicting.
    In that way, we can tell the actual price of a house in the dataset from its predicted
    price.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么预测价格上方的帽子，![](../Images/p_cf.png)？在本书中，帽子表示这是我们模型预测的变量。这样，我们可以从预测价格中区分出数据集中房屋的实际价格。
- en: Thus, the equation of a linear regression model that predicts the price as the
    base price plus the price per room times the number of rooms is
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 因此，预测价格为基础价格加上每间房的价格乘以房间数量的线性回归模型方程是
- en: '![](../Images/p_cf.png) = *mr* + *b*.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/p_cf.png) = *mr* + *b*.'
- en: This is a formulaic way of saying
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一种公式化的说法
- en: Predicted price = (Price per room)(Number of rooms) + Base price of the house.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 预测价格 = (每间房的价格)(房间数量) + 房屋的基本价格。
- en: 'To get an idea of the linear regression algorithm, imagine that we have a model
    in which the price per room is $40 and the base price of the house is $50\. This
    model predicts the price of a house using the following formula:'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 为了了解线性回归算法，想象我们有一个模型，其中每间房的价格是40美元，房屋的基本价格是50美元。这个模型使用以下公式预测房屋的价格：
- en: '![](../Images/p_cf.png) = 40 · *r* + 50'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/p_cf.png) = 40 · *r* + 50'
- en: 'To illustrate the linear regression algorithm, imagine that in our dataset
    we have a house with two rooms that costs $150\. This model predicts that the
    price of the house is 50 + 40 · 2 = 130\. That is not a bad prediction, but it
    is less than the price of the house. How can we improve the model? It seems like
    the model’s mistake is thinking that the house is too cheap. Maybe the model has
    a low base price, or maybe it has a low price per room, or maybe both. If we increase
    both by a small amount, we may get a better estimate. Let’s increase the price
    per room by $0.50 and the base price by $1\. (I picked these numbers randomly).
    The new equation follows:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明线性回归算法，想象一下在我们的数据集中有一所两居室的房子，价格为150美元。这个模型预测房子的价格是50 + 40 * 2 = 130美元。这不是一个糟糕的预测，但它低于房子的价格。我们如何改进这个模型？看起来模型的错误在于认为房子太便宜。也许模型的基础价格低，或者每间房子的价格低，或者两者都低。如果我们都稍微增加一点，我们可能会得到更好的估计。让我们将每间房子的价格增加0.50美元，基础价格增加1美元。（我随机选择了这些数字）。新的方程如下：
- en: '![](../Images/p_cf.png) = 40.5 · *r* + 51'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '![图片](../Images/p_cf.png) = 40.5 * r + 51'
- en: 'The new predicted price for the house is 40.5 · *r* + 51 = 132\. Because $132
    is closer to $150, our new model makes a better prediction for this house. Therefore,
    it is a better model for that data point. We don’t know if it is a better model
    for the other data points, but let’s not worry about that for now. The idea of
    the linear regression algorithm is to repeat the previous process many times.
    The pseudocode of the linear regression algorithm follows:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 新预测的房价为40.5 * r + 51 = 132。因为132比150更接近，所以我们的新模型对这个房子的预测更好。因此，它对这个数据点是一个更好的模型。我们不知道它是否对其他数据点也是一个更好的模型，但现在我们不必担心这一点。线性回归算法的想法是重复前面的过程多次。线性回归算法的伪代码如下：
- en: Pseudocode for the linear regression algorithm
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法的伪代码
- en: '**Inputs**: A dataset of points'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: '**输入**：一组数据点集'
- en: '**Outputs**: A linear regression model that fits that dataset'
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: '**输出**：一个适合该数据集的线性回归模型'
- en: 'Procedure:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Pick a model with random weights and a random bias.
  id: totrans-138
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个具有随机权重和随机偏差的模型。
- en: 'Repeat many times:'
  id: totrans-139
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-140
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 选择一个随机数据点。
- en: Slightly adjust the weights and bias to improve the prediction for that particular
    data point.
  id: totrans-141
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 稍微调整权重和偏差，以改善该特定数据点的预测。
- en: '**Return** the model you’ve obtained.'
  id: totrans-142
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**返回**你获得的模型。'
- en: 'You may have a few questions, such as the following:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会有一些疑问，比如以下：
- en: By how much should I adjust the weights?
  id: totrans-144
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该如何调整权重？
- en: How many times should I repeat the algorithm? In other words, how do I know
    when I’m done?
  id: totrans-145
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我应该重复算法多少次？换句话说，我如何知道何时完成？
- en: How do I know that this algorithm works?
  id: totrans-146
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我如何知道这个算法是有效的？
- en: We answer all of these questions in this chapter. In the sections “The square
    trick” and “The absolute trick,” we learn some interesting tricks to find good
    values to adjust the weights. In the sections “The absolute error” and “The square
    error,” we see the error function, which will help us decide when to stop the
    algorithm. And finally, in the section “Gradient descent,” we cover a powerful
    method called gradient descent, which justifies why this algorithm works. But
    first, let’s start by moving lines in the plane.
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在本章中回答了所有这些问题。在“平方技巧”和“绝对技巧”部分，我们学习了一些有趣的技巧来找到调整权重的良好值。在“绝对误差”和“平方误差”部分，我们看到了误差函数，它将帮助我们决定何时停止算法。最后，在“梯度下降”部分，我们介绍了一种称为梯度下降的强大方法，它证明了为什么这个算法是有效的。但首先，让我们从在平面上移动线开始。
- en: Crash course on slope and *y*-intercept
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率和y轴截距的快速课程
- en: 'In the section “The formulate step,” we talked about the equation of a line.
    In this section, we learn how to manipulate this equation to move our line. Recall
    that the equation of a line has the following two components:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 在“公式步骤”部分，我们讨论了直线的方程。在本节中，我们学习如何操作这个方程来移动我们的线。回想一下，直线的方程有两个组成部分：
- en: The slope
  id: totrans-150
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率
- en: The *y*-intercept
  id: totrans-151
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: y轴截距
- en: 'The slope tells us how steep the line is, and the *y*-intercept tells us where
    the line is located. The slope is defined as the rise divided by the run, and
    the *y*-intercept tells us where the line crosses the *y*-axis (the vertical axis).
    In figure 3.7, we can see both in an example. This line has the following equation:'
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 斜率告诉我们线的陡峭程度，y轴截距告诉我们线的位置。斜率定义为上升量除以运行量，y轴截距告诉我们线与y轴（垂直轴）相交的位置。在图3.7中，我们可以看到一个例子。这条线的方程如下：
- en: '*y* = 0.5*x* + 2'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: y = 0.5 * x + 2
- en: '![](../Images/3-7.png)'
  id: totrans-154
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-7.png)'
- en: Figure 3.7 The line with equation **y** = 0.5**x** + 2 has slope 0.5 (left)
    and **y**-intercept 2 (right).
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 方程式为y = 0.5x + 2的线具有斜率0.5（左侧）和y截距2（右侧）。
- en: What does this equation mean? It means that the slope is 0.5, and the *y*-intercept
    is 2.
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 这个方程式意味着什么？这意味着斜率是0.5，而y截距是2。
- en: When we say that the slope is 0.5, it means that when we walk along this line,
    for every unit that we move to the right, we are moving 0.5 units up. The slope
    can be zero if we don’t move up at all or negative if we move down. A vertical
    line has an undefined slope, but luckily, these don’t tend to show up in linear
    regression. Many lines can have the same slope. If I draw any line parallel to
    the line in figure 3.7, this line will also rise 0.5 units for every unit it moves
    to the right. This is where the *y*-intercept comes in. The *y*-intercept tells
    us where the line cuts the *y*-axis. This line cuts the *x*-axis at height 2,
    and that is the *y*-intercept.
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们说斜率是0.5时，这意味着当我们沿着这条线行走时，每向右移动一个单位，我们向上移动0.5个单位。如果完全不向上移动，斜率可以是零；如果向下移动，斜率可以是负数。垂直线的斜率是未定义的，但幸运的是，这些在线性回归中不太可能出现。许多线条可以具有相同的斜率。如果我画出与图3.7中的线平行的任何线条，这条线在向右移动一个单位时也会上升0.5个单位。这就是y截距的作用所在。y截距告诉我们线在哪里切割y轴。这条线在高度2处切割x轴，这就是y截距。
- en: In other words, the slope of the line tells us the *direction* in which the
    line is pointing, and the *y*-intercept tells us the *location* of the line. Notice
    that by specifying the slope and the *y*-intercept, the line is completely specified.
    In figure 3.8, we can see different lines with the same *y*-intercept, and different
    lines with the same slope.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 换句话说，线的斜率告诉我们线指向的方向，而y截距告诉我们线的位置。请注意，通过指定斜率和y截距，线就被完全确定了。在图3.8中，我们可以看到具有相同y截距的不同线条，以及具有相同斜率的不同线条。
- en: '![](../Images/3-8.png)'
  id: totrans-159
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-8.png)'
- en: Figure 3.8 Some examples of slope and **y**-intercept. On the left, we see several
    lines with the same intercept and different slopes. Notice that the higher the
    slope, the steeper the line. On the right, we see several lines with the same
    slope and different **y**-intercepts. Notice that the higher the **y**-intercept,
    the higher the line is located.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.8 斜率和y截距的一些示例。在左侧，我们看到几条具有相同截距但斜率不同的线条。请注意，斜率越高，线越陡峭。在右侧，我们看到几条具有相同斜率但y截距不同的线条。请注意，y截距越高，线所在的位置越高。
- en: In our current housing example, the slope represents the price per room, and
    the *y*-intercept represents the base price of a house. Let’s keep this in mind,
    and, as we manipulate the lines, think of what this is doing to our housing price
    model.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们当前的住房示例中，斜率代表每间房的价格，而y截距代表房屋的基准价格。让我们记住这一点，当我们操作这些线条时，思考这对我们的房价模型做了什么。
- en: 'From the definitions of slope and *y*-intercept, we can deduce the following:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 从斜率和y截距的定义中，我们可以得出以下结论：
- en: '**Changing the slope**:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: '**改变斜率**：'
- en: If we increase the slope of a line, the line will rotate counterclockwise.
  id: totrans-164
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们增加一条线的斜率，这条线将逆时针旋转。
- en: If we decrease the slope of a line, the line will rotate clockwise.
  id: totrans-165
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们减少一条线的斜率，这条线将顺时针旋转。
- en: These rotations are on the pivot shown in figure 3.9, namely, the point of intersection
    of the line and the *y*-axis.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 这些旋转是在图3.9中显示的枢轴上进行的，即线和y轴的交点。
- en: '**Changing the** *y***-intercept**:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: '**改变** y**截距**：'
- en: If we increase the *y*-intercept of a line, the line is translated upward.
  id: totrans-168
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们增加一条线的y截距，这条线将向上平移。
- en: If we decrease the *y*-intercept of a line, the line is translated downward.
  id: totrans-169
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果我们降低一条线的y截距，这条线就会向下平移。
- en: Figure 3.9 illustrates these rotations and translations, which will come in
    handy when we want to adjust our linear regression models.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9说明了这些旋转和平移，当我们想要调整我们的线性回归模型时，这些将很有用。
- en: '![](../Images/3-9.png)'
  id: totrans-171
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-9.png)'
- en: 'Figure 3.9 Left: Increasing the slope rotates the line counterclockwise, whereas
    decreasing the slope rotates it clockwise. Right: Increasing the **y**-intercept
    translates the line upward, whereas decreasing the **y**-intercept translates
    it downward.'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 左侧：增加斜率会使线逆时针旋转，而减少斜率会使线顺时针旋转。右侧：增加y截距会使线向上平移，而减少y截距会使线向下平移。
- en: As explained earlier, in general, the equation of a line is written as *y* =
    *mx* + *b*, where *x* and *y* correspond to the horizontal and vertical coordinates,
    *m* corresponds to the slope, and *b* to the *y*-intercept. Throughout this chapter,
    to match the notation, we’ll write the equation as ![](../Images/p_cf.png) = *mr*
    + *b*, where ![](../Images/p_cf.png) corresponds to the predicted price, *r* to
    the number of rooms, *m* (the slope) to the price per room, and *b* (the *y*-intercept)
    to the base price of the house.
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，一般来说，直线的方程写作 *y* = *mx* + *b*，其中 *x* 和 *y* 分别对应水平和垂直坐标，*m* 对应斜率，*b* 对应
    *y* 轴截距。在本章中，为了与符号一致，我们将方程写作 ![图片](../Images/p_cf.png) = *mr* + *b*，其中 ![图片](../Images/p_cf.png)
    对应预测价格，*r* 对应房间数量，*m*（斜率）对应每间房的价格，*b*（*y* 轴截距）对应房屋的基准价格。
- en: A simple trick to move a line closer to a set of points, one point at a time
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 将直线移近一组点的一个简单技巧，一次移动一个点
- en: Recall that the linear regression algorithm consists of repeating a step in
    which we move a line closer to a point. We can do this using rotations and translations.
    In this section, we learn a trick called the *simple trick*, which consists of
    slightly rotating and translating the line in the direction of the point to move
    it closer (figure 3.10).
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 回想一下，线性回归算法由重复一个步骤组成，即我们移动直线使其更接近一个点。我们可以通过旋转和平移来实现这一点。在本节中，我们学习了一个称为 *简单技巧*
    的技巧，该技巧包括稍微旋转并平移直线，使其朝向点的方向移动，从而使其更接近（图 3.10）。
- en: '![](../Images/3-10.png)'
  id: totrans-176
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-10.png)'
- en: Figure 3.10 Our goal is to rotate and translate the line by a small amount to
    get closer to the point.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.10 我们的目标是通过旋转和平移直线一小量，使其更接近点。
- en: 'The trick to move the line correctly toward a point is to identify where the
    point is with respect to the line. If the point is above the line, we need to
    translate the line up, and if it is below, we need to translate it down. Rotation
    is a bit harder, but because the pivot is the point of intersection of the line
    and the *y*-axis, we can see that if the point is above the line and to the right
    of the *y*-axis, or below the line and to the left of the *y*-axis, we need to
    rotate the line counterclockwise. In the other two scenarios, we need to rotate
    the line clockwise. These are summarized in the following four cases, which are
    illustrated in figure 3.11:'
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: 正确移动直线到点的技巧是确定点相对于直线的位置。如果点在直线上方，我们需要将直线向上平移，如果点在直线下方，我们需要将直线向下平移。旋转稍微困难一些，但由于旋转中心是直线与
    *y* 轴的交点，我们可以看到如果点在直线上方且在 *y* 轴右侧，或者点在直线下方且在 *y* 轴左侧，我们需要将直线逆时针旋转。在其他两种情况下，我们需要将直线顺时针旋转。这些情况总结在以下四个案例中，如图
    3.11 所示：
- en: '**Case 1**: If the point is above the line and to the right of the *y*-axis,
    we rotate the line counterclockwise and translate it upward.'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 1**：如果点位于直线上方且在 *y* 轴右侧，我们将直线逆时针旋转并将其向上平移。'
- en: '**Case 2**: If the point is above the line and to the left of the *y*-axis,
    we rotate the line clockwise and translate it upward.'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 2**：如果点位于直线上方且在 *y* 轴左侧，我们将直线顺时针旋转并将其向上平移。'
- en: '**Case 3**: If the point is below the line and to the right of the *y*-axis,
    we rotate the line clockwise and translate it downward.'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 3**：如果点位于直线下方且在 *y* 轴右侧，我们将直线顺时针旋转并将其向下平移。'
- en: '**Case 4**: If the point is below the line and to the left of the *y*-axis,
    we rotate the line counterclockwise and translate it downward.'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: '**案例 4**：如果点位于直线下方且在 *y* 轴左侧，我们将直线逆时针旋转并将其向下平移。'
- en: '![](../Images/3-11.png)'
  id: totrans-183
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-11.png)'
- en: Figure 3.11 The four cases. In each of these we must rotate the line and translate
    it in a different way to move the line closer to the corresponding point.
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.11 四种情况。在这些情况中，我们必须以不同的方式旋转并平移直线，以使直线更接近相应的点。
- en: 'Now that we have these four cases, we can write the pseudocode for the simple
    trick. But first, let’s clarify some notation. In this section we’ve been talking
    about lines with equation *y* = *mx* + *b*, where *m* is the slope and *b* is
    the *y*-intercept. In the housing example, we used the following similar notation:'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有了这四种情况，我们可以编写简单技巧的伪代码。但首先，让我们明确一些符号。在本节中，我们一直在谈论方程为 *y* = *mx* + *b* 的直线，其中
    *m* 是斜率，*b* 是 *y* 轴截距。在房屋示例中，我们使用了以下类似的符号：
- en: The point with coordinates (*r, p*) corresponds to a house with *r* rooms and
    price *p*.
  id: totrans-186
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为 (*r, p*) 的点对应有 *r* 间房且价格为 *p* 的房屋。
- en: The slope *m* corresponds to the price per room.
  id: totrans-187
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率 *m* 对应每间房的价格。
- en: The *y*-intercept *b* corresponds to the base price of the house.
  id: totrans-188
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 轴截距 *b* 对应房屋的基准价格。'
- en: The prediction ![](../Images/p_cf.png) = *mr* + *b* corresponds to the predicted
    price of the house.
  id: totrans-189
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 预测值 ![](../Images/p_cf.png) = *mr* + *b* 对应于房屋的预测价格。
- en: Pseudocode for the simple trick
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 简单技巧的伪代码
- en: 'Inputs:'
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A line with slope *m*, *y*-intercept *b*, and equation ![](../Images/p_cf.png)
    = *mr* + *b*
  id: totrans-192
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率为 *m*、*y* 截距为 *b*、方程为 ![](../Images/p_cf.png) = *mr* + *b* 的线
- en: A point with coordinates (*r*, *p*)
  id: totrans-193
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为 (*r*, *p*) 的点
- en: 'Output:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A line with equation ![](../Images/p_cf.png) = *m**'**r* + *b* that is closer
    to the point
  id: totrans-195
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程为 ![](../Images/p_cf.png) = *m**'**r* + *b* 的线，该线更接近点
- en: 'Procedure:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Pick two very small random numbers, and call them *η*[1] and *η*[2] (the Greek
    letter *eta*).
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 选择两个非常小的随机数，并称它们为 *η*[1] 和 *η*[2]（希腊字母 *eta*）。
- en: '**Case 1**: If the point is above the line and to the right of the *y*-axis,
    we rotate the line counterclockwise and translate it upward:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1**：如果点位于线上且在 *y* 轴的右侧，我们将线逆时针旋转并向上平移：'
- en: Add *η*[1] to the slope *m*. Obtain *m**'* = *m* + *η*[1].
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*[1] 加到斜率 *m* 上。得到 *m**'* = *m* + *η*[1]。
- en: Add *η*[2] to the *y*-intercept *b*. Obtain *b**'* = *b* + *η*[2].
  id: totrans-200
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*[2] 加到 *y* 截距 *b* 上。得到 *b**'* = *b* + *η*[2]。
- en: '**Case 2**: If the point is above the line and to the left of the *y*-axis,
    we rotate the line clockwise and translate it upward:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 2**：如果点位于线上且在 *y* 轴的左侧，我们将线顺时针旋转并向上平移：'
- en: Subtract *η*[1] from the slope *m*. Obtain *m**'* = *m* – *η*[1].
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从斜率 *m* 中减去 *η*[1]。得到 *m**'* = *m* – *η*[1]。
- en: Add *η*[2] to the *y*-intercept *b*. Obtain *b**'* = *b* + *η*[2].
  id: totrans-203
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*[2] 加到 *y* 截距 *b* 上。得到 *b**'* = *b* + *η*[2]。
- en: '**Case 3**: If the point is below the line and to the right of the *y*-axis,
    we rotate the line clockwise and translate it downward:'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 3**：如果点位于线下且在 *y* 轴的右侧，我们将线顺时针旋转并向下平移：'
- en: Subtract *η*[1] from the slope *m*. Obtain *m**'* = *m* – *η*[1].
  id: totrans-205
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从斜率 *m* 中减去 *η*[1]。得到 *m**'* = *m* – *η*[1]。
- en: Subtract *η*[2] from the *y*-intercept *b*. Obtain *b**'* = *b* – *η*[2].
  id: totrans-206
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 *y* 截距 *b* 中减去 *η*[2]。得到 *b**'* = *b* – *η*[2]。
- en: '**Case 4**: If the point is below the line and to the left of the *y*-axis,
    we rotate the line counterclockwise and translate it downward:'
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 4**：如果点位于线下且在 *y* 轴的左侧，我们将线逆时针旋转并向下平移：'
- en: Add *η*[1] to the slope *m*. Obtain *m**'* = *m* + *η*[1].
  id: totrans-208
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*[1] 加到斜率 *m* 上。得到 *m**'* = *m* + *η*[1]。
- en: Subtract *η*[2] from the *y*-intercept *b*. Obtain *b**'* = *b* – *η*[2].
  id: totrans-209
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从 *y* 截距 *b* 中减去 *η*[2]。得到 *b**'* = *b* – *η*[2]。
- en: '**Return**: The line with equation ![](../Images/p_cf.png) = *m**''**r* + *b**''**.*'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：方程为 ![](../Images/p_cf.png) = *m**''**r* + *b**''**.* 的线'
- en: 'Note that for our example, adding or subtracting a small number to the slope
    means increasing or decreasing the price per room. Similarly, adding or subtracting
    a small number to the *y*-intercept means increasing or decreasing the base price
    of the house. Furthermore, because the *x*-coordinate is the number of rooms,
    this number is never negative. Thus, only cases 1 and 3 matter in our example,
    which means we can summarize the simple trick in colloquial language as follows:'
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，对于我们的例子，向斜率中添加或减去一个小数意味着增加或减少每间房的价格。同样，向 *y* 截距中添加或减去一个小数意味着增加或减少房屋的基本价格。此外，因为
    *x* 坐标是房间数，这个数永远不会是负数。因此，在我们的例子中，只有情况 1 和 3 是重要的，这意味着我们可以用口语化的语言总结简单技巧如下：
- en: Simple trick
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 简单技巧
- en: If the model gave us a price for the house that is lower than the actual price,
    add a small random amount to the price per room and to the base price of the house.
  id: totrans-213
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型给出的房价低于实际价格，则将每间房的价格和房屋的基本价格中加上一个小随机数。
- en: If the model gave us a price for the house that is higher than the actual price,
    subtract a small random amount from the price per room and the base price of the
    house.
  id: totrans-214
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果模型给出的房价高于实际价格，则从每间房的价格和房屋的基本价格中减去一个小随机数。
- en: 'This trick achieves some success in practice, but it’s far from being the best
    way to move lines. Some questions may arise, such as the following:'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这个技巧在实践中取得了一些成功，但离最佳移动线的方法还远。可能会出现一些问题，例如以下问题：
- en: Can we pick better values for *η*[1] and *η*[2]?
  id: totrans-216
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否为 *η*[1] 和 *η*[2] 选择更好的值？
- en: Can we crunch the four cases into two, or perhaps one?
  id: totrans-217
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们能否将四个情况合并为两个，或者可能是一个？
- en: The answer to both questions is yes, and we’ll see how in the following two
    sections.
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 两个问题的答案都是肯定的，我们将在接下来的两个部分中看到这一点。
- en: 'The square trick: A much more clever way of moving our line closer to one of
    the points'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 平方技巧：将我们的线移近其中一个点的一个更巧妙的方法
- en: In this section, I show you an effective way to move a line closer to a point.
    I call this the *square trick*. Recall that the simple trick consisted of four
    cases that were based on the position of the point with respect to the line. The
    square trick will bring these four cases down to one by finding values with the
    correct signs (+ or –) to add to the slope and the *y*-intercept for the line
    to always move closer to the point.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示一种有效的方法，使直线更接近一个点。我称这个技巧为 *square trick*。回想一下，简单技巧基于四个基于点相对于直线位置的场景。square
    trick 将这四个场景简化为一个，通过找到正确的符号（+或-）的值来添加到斜率和 *y*-截距，使直线始终更接近点。
- en: 'We start with the *y*-intercept. Notice the following two observations:'
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从 *y*-截距开始。注意以下两个观察结果：
- en: '**Observation 1**: In the simple trick, when the point is above the line, we
    add a small amount to the *y*-intercept. When it is below the line, we subtract
    a small amount.'
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察1**：在简单技巧中，当点在直线上方时，我们向 *y*-截距添加一小部分。当它在直线下方时，我们减去一小部分。'
- en: '**Observation 2**: If a point is above the line, the value ![](../Images/pmp.png)
    (the difference between the price and the predicted price) is positive. If it
    is below the line, this value is negative. This observation is illustrated in
    figure 3.12.'
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察2**：如果一个点在直线上方，值 ![](../Images/pmp.png)（价格与预测价格的差值）是正的。如果它在直线下方，这个值就是负的。这个观察结果在图3.12中得到了说明。'
- en: '![](../Images/3-12.png)'
  id: totrans-224
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-12.png)'
- en: 'Figure 3.12 Left: When the point is above the line, the price is larger than
    the predicted price, so the difference is positive. Right: When the point is below
    the line, the price is smaller than the predicted price, so the difference is
    negative.'
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 左：当点在直线上方时，价格大于预测价格，因此差值是正的。右：当点在直线下方时，价格小于预测价格，因此差值是负的。
- en: 'Putting together observation 1 and observation 2, we conclude that if we add
    the difference ![](../Images/pmp.png) to the *y*-intercept, the line will always
    move toward the point, because this value is positive when the point is above
    the line and negative when the point is below the line. However, in machine learning,
    we always want to take small steps. To help us with this, we introduce an important
    concept in machine learning: the learning rate.'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 将观察1和观察2结合起来，我们得出结论，如果我们把差值 ![](../Images/pmp.png) 添加到 *y*-截距，直线将始终朝向点移动，因为当点在直线上方时这个值是正的，当点在直线下方时这个值是负的。然而，在机器学习中，我们总是希望采取小步骤。为了帮助我们做到这一点，我们引入了机器学习中的一个重要概念：学习率。
- en: learning rate A very small number that we pick before training our model. This
    number helps us make sure our model changes in very small amounts by training.
    In this book, the learning rate will be denoted by *η*, the Greek letter *eta*.
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: 学习率 在训练模型之前我们选择的一个非常小的数字。这个数字帮助我们确保通过训练，我们的模型以非常小的量发生变化。在这本书中，学习率将用 *η*，希腊字母
    *eta* 表示。
- en: Because the learning rate is small, so is the value *η*(![](../Images/pmp.png)).
    This is the value we add to the *y*-intercept to move the line in the direction
    of the point.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 由于学习率很小，因此 *η*(![](../Images/pmp.png)) 的值也很小。这是我们添加到 *y*-截距的值，以使直线朝着点的方向移动。
- en: 'The value we need to add to the slope is similar, yet a bit more complicated.
    Notice the following two observations:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 需要添加到斜率中的值类似，但稍微复杂一些。注意以下两个观察结果：
- en: '**Observation 3**: In the simple trick, when the point is in scenario 1 or
    4 (above the line and to the right of the vertical axis, or below the line and
    to the left of the vertical axis), we rotate the line counterclockwise. Otherwise
    (scenario 2 or 3), we rotate it clockwise.'
  id: totrans-230
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察3**：在简单技巧中，当点处于场景1或4（在垂直轴上方且在直线上方，或在垂直轴下方且在直线下方）时，我们逆时针旋转直线。否则（场景2或3），我们顺时针旋转。'
- en: '**Observation 4**: If a point (*r*, *p*) is to the right of the vertical axis,
    then *r* is positive. If the point is to the left of the vertical axis, then *r*
    is negative. This observation is illustrated in figure 3.13\. Notice that in this
    example, *r* will never be negative, because it is the number of rooms. However,
    in a general example, a feature could be negative.'
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**观察4**：如果一个点 (*r*, *p*) 在垂直轴右侧，那么 *r* 是正的。如果点在垂直轴左侧，那么 *r* 是负的。这个观察结果在图3.13中得到了说明。注意，在这个例子中，*r*
    从不会是负的，因为它代表的是房间数。然而，在一般例子中，一个特征可能是负的。'
- en: '![](../Images/3-13.png)'
  id: totrans-232
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-13.png)'
- en: 'Figure 3.13 Left: When the point is to the left of the **y**-axis, the number
    of rooms is negative. Right: When the point is to the right of the **y**-axis,
    the number of rooms is positive.'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.13 左：当点在 **y**-轴左侧时，房间数是负的。右：当点在 **y**-轴右侧时，房间数是正的。
- en: Consider the value *r*(![](../Images/pmp.png)). This value is positive when
    both *r* and ![](../Images/pmp.png) are both positive or both negative. This is
    precisely what occurs in scenarios 1 and 4\. Similarly, *r*(![](../Images/pmp.png))
    is negative in scenarios 2 and 3\. Therefore, due to observation 4, this is the
    quantity that we need to add to the slope. We want this value to be small, so
    again, we multiply it by the learning rate and conclude that adding *η**r*(![](../Images/pmp.png))
    to the slope will always move the line in the direction of the point.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑值 *r*(![](../Images/pmp.png)). 当 *r* 和 ![](../Images/pmp.png) 都为正或都为负时，此值是正的。这正是情况
    1 和 4 中发生的情况。同样，*r*(![](../Images/pmp.png)) 在情况 2 和 3 中为负。因此，根据观察 4，这是我们需要添加到斜率中的量。我们希望这个值很小，所以再次，我们将其乘以学习率，并得出结论，将
    *η**r*(![](../Images/pmp.png)) 添加到斜率中，将始终使线向点的方向移动。
- en: 'We can now write the pseudocode for the square trick as follows:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以编写平方技巧的伪代码如下：
- en: Pseudocode for the square trick
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 平方技巧的伪代码
- en: 'Inputs:'
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A line with slope *m*, *y*-intercept *b*, and equation ![](../Images/p_cf.png)
    = *mr* + *b*
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率为 *m*，*y*-截距 *b*，方程为 ![](../Images/p_cf.png) = *mr* + *b*
- en: A point with coordinates (*r*, *p*)
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为 (*r*, *p*) 的点
- en: A small positive value *η* (the learning rate)
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的正值 *η*（学习率）
- en: 'Output:'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A line with equation ![](../Images/p_cf.png) = *m**'**r* + *b**'* that is closer
    to the point
  id: totrans-242
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程为 ![](../Images/p_cf.png) = *m**'**r* + *b**'* 且更接近点的线
- en: 'Procedure:'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Add *η**r*(![](../Images/pmp.png)) to the slope *m*. Obtain *m**'* = *m* + *η**r*(![](../Images/pmp.png))
    (this rotates the line).
  id: totrans-244
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η**r*(![](../Images/pmp.png)) 添加到斜率 *m*。得到 *m**'* = *m* + *η**r*(![](../Images/pmp.png))（这会旋转线）。
- en: Add *η*(![](../Images/pmp.png)) to the *y*-intercept *b*. Obtain *b**'* = *b*
    + *η*(![](../Images/pmp.png)) (this translates the line).
  id: totrans-245
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*(![](../Images/pmp.png)) 添加到 *y*-截距 *b*。得到 *b**'* = *b* + *η*(![](../Images/pmp.png))（这会平移线）。
- en: '**Return**: The line with equation ![](../Images/p_cf.png) = *m**''**r* + *b**''*'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：方程为 ![](../Images/p_cf.png) = *m**''**r* + *b**''* 的线'
- en: 'We are now ready to code this algorithm in Python! The code for this section
    follows:'
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在准备好用 Python 编写这个算法！本节的代码如下：
- en: '**Notebook**: Coding_linear_regression.ipynb'
  id: totrans-248
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**注意**：Coding_linear_regression.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/Coding_linear_regression.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/Coding_linear_regression.ipynb)'
  id: totrans-249
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/Coding_linear_regression.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/Coding_linear_regression.ipynb)'
- en: 'And here is code for the square trick:'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是平方技巧的代码：
- en: '[PRE0]'
  id: totrans-251
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Calculates the prediction
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算预测值
- en: ❷ Translates the line
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 翻译该行
- en: ❸ Rotates the line
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 旋转线
- en: 'The absolute trick: Another useful trick to move the line closer to the points'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对技巧：另一个有用的技巧，用于将线移得更接近点
- en: 'The square trick is effective, but another useful trick, which we call the
    *absolute trick*, is an intermediate between the simple and the square tricks.
    In the square trick, we used the two quantities, ![](../Images/pmp.png) (price
    – predicted price) and *r* (number of rooms), to help us bring the four cases
    down to one. In the absolute trick, we use only *r* to help us bring the four
    cases down to two. In other words, here is the absolute trick:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 平方技巧是有效的，但另一个有用的技巧，我们称之为 *绝对技巧*，是简单技巧和平方技巧之间的中间技巧。在平方技巧中，我们使用了两个量，![](../Images/pmp.png)（价格
    - 预测价格）和 *r*（房间数），帮助我们把这四种情况简化为一种。在绝对技巧中，我们只使用 *r* 来帮助我们把这四种情况简化为两种。换句话说，这里是绝对技巧：
- en: Pseudocode for the absolute trick
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对技巧的伪代码
- en: 'Inputs:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A line with slope *m*, *y*-intercept *b*, and equation ![](../Images/p_cf.png)
    = *mr* + *b*
  id: totrans-259
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率为 *m*，*y*-截距 *b*，方程为 ![](../Images/p_cf.png) = *mr* + *b*
- en: A point with coordinates (*r*, *p*)
  id: totrans-260
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为 (*r*, *p*) 的点
- en: A small positive value *η* (the learning rate)
  id: totrans-261
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的正值 *η*（学习率）
- en: 'Output:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A line with equation ![](../Images/p_cf.png) = *m**'**r* + *b**'* that is closer
    to the point
  id: totrans-263
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程为 ![](../Images/p_cf.png) = *m**'**r* + *b**'* 且更接近点的线
- en: 'Procedure:'
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: '**Case 1**: If the point is above the line (i.e., if *p* > ![](../Images/p_cf.png)):'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况 1**：如果点在线上方（即，如果 *p* > ![](../Images/p_cf.png)）：'
- en: Add *η**r* to the slope *m*. Obtain *m**'* = *m* + *η**r* (this rotates the
    line counterclockwise if the point is to the right of the *y*-axis, and clockwise
    if it is to the left of the *y*-axis).
  id: totrans-266
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η**r* 添加到斜率 *m*。得到 *m**'* = *m* + *η**r*（如果点在y轴右侧，则使直线逆时针旋转，如果点在y轴左侧，则使直线顺时针旋转）。
- en: Add *η* to the *y*-intercept *b*. Obtain *b**'* = *b* + *η* (this translates
    the line up).
  id: totrans-267
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η* 添加到y截距 *b*。得到 *b**'* = *b* + *η*（这使直线向上移动）。
- en: '**Case 2**: If the point is below the line (i.e., if *p* < ![](../Images/p_cf.png)):'
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '**情况2**：如果点在直线下方（即，如果 *p* < ![](../Images/p_cf.png)）：'
- en: Subtract *η**r* from the slope *m*. Obtain *m**'* = *m* – *η**r* (this rotates
    the line clockwise if the point is to the right of the *y*-axis, and counterclockwise
    if it is to the left of the *y*-axis).
  id: totrans-269
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从斜率 *m* 中减去 *η**r*。得到 *m**'* = *m* – *η**r*（如果点在y轴右侧，则使直线顺时针旋转，如果点在y轴左侧，则使直线逆时针旋转）。
- en: Subtract *η* from the *y*-intercept *b*. Obtain *b**'* = *b* – *η* (this translates
    the line down).
  id: totrans-270
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从y截距 *b* 中减去 *η*。得到 *b**'* = *b* – *η*（这使直线向下移动）。
- en: '**Return**: The line with equation ![](../Images/p_cf.png) = *m**''**r* + *b**''*'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：方程 ![](../Images/p_cf.png) = *m**''**r* + *b**''*'
- en: 'Here is the code for the absolute trick:'
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: 这是绝对值技巧的代码：
- en: '[PRE1]'
  id: totrans-273
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: I encourage you to verify that the amount added to each of the weights indeed
    has the correct sign, as we did with the square trick.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 我鼓励你验证添加到每个权重中的数量确实具有正确的符号，就像我们使用平方技巧所做的那样。
- en: 'The linear regression algorithm: Repeating the absolute or square trick many
    times to move the line closer to the points'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法：重复使用绝对值或平方技巧多次，使直线更接近点
- en: 'Now that we’ve done all the hard work, we are ready to develop the linear regression
    algorithm! This algorithm takes as input a bunch of points and returns a line
    that fits them well. This algorithm consists of starting with random values for
    our slope and our *y*-intercept and then repeating the procedure of updating them
    many times using the absolute or the square trick. Here is the pseudocode:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们已经完成了所有艰苦的工作，我们准备开发线性回归算法！这个算法以一系列点作为输入，并返回一条很好地拟合这些点的直线。这个算法包括从随机值开始为我们的斜率和y截距，然后重复多次使用绝对值或平方技巧来更新它们的步骤。以下是伪代码：
- en: Pseudocode for the linear regression algorithm
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 线性回归算法的伪代码
- en: 'Inputs:'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A dataset of houses with number of rooms and prices
  id: totrans-279
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 房屋数量和价格的数据集
- en: 'Outputs:'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: 'Model weights: price per room and base price'
  id: totrans-281
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型权重：每间房的价格和基础价格
- en: 'Procedure:'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Start with random values for the slope and *y*-intercept.
  id: totrans-283
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以斜率和y截距的随机值开始。
- en: 'Repeat many times:'
  id: totrans-284
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 重复多次：
- en: Pick a random data point.
  id: totrans-285
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 随机选择一个数据点。
- en: Update the slope and the *y*-intercept using the absolute or the square trick.
  id: totrans-286
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用绝对值或平方技巧更新斜率和y截距。
- en: Each iteration of the loop is called an *epoch*, and we set this number at the
    beginning of our algorithm. The simple trick was mostly used for illustration,
    but as was mentioned before, it doesn’t work very well. In real life, we use the
    absolute or square trick, which works a lot better. In fact, although both are
    commonly used, the square trick is more popular. Therefore, we’ll use that one
    for our algorithm, but feel free to use the absolute trick if you prefer.
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 每次循环迭代被称为一个*epoch*，我们在算法开始时设置这个数字。简单的技巧主要用于说明，但如前所述，它并不非常有效。在现实生活中，我们使用绝对值或平方技巧，这要有效得多。事实上，尽管两者都常用，但平方技巧更受欢迎。因此，我们将使用这个技巧，但如果你更喜欢，也可以使用绝对值技巧。
- en: 'Here is the code for the linear regression algorithm. Note that we have used
    the Python random package to generate random numbers for our initial values (slope
    and *y*-intercept) and for selecting our points inside the loop:'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是线性回归算法的代码。请注意，我们使用了Python随机包来生成随机数作为我们的初始值（斜率和y截距）以及在选择循环内的点：
- en: '[PRE2]'
  id: totrans-289
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Imports the random package to generate (pseudo) random numbers
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 导入随机包以生成（伪）随机数
- en: ❷ Generates random values for the slope and the *y*-intercept
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 为斜率和y截距生成随机值
- en: ❸ Repeats the update step many times
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 多次重复更新步骤
- en: ❹ Picks a random point on our dataset
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在我们的数据集上随机选择一个点
- en: ❺ Applies the square trick to move the line closer to our point
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 应用平方技巧将直线移动到我们的点附近
- en: The next step is to run this algorithm to build a model that fits our dataset.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是运行这个算法来构建一个适合我们数据集的模型。
- en: Loading our data and plotting it
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 加载数据并绘制它
- en: Throughout this chapter, we load and make plots of our data and models using
    Matplotlib and NumPy, two very useful Python packages. We use NumPy for storing
    arrays and carrying out mathematical operations, whereas we use Matplotlib for
    the plots.
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用Matplotlib和NumPy这两个非常有用的Python包加载数据和绘制模型图。我们使用NumPy存储数组并执行数学运算，而使用Matplotlib进行绘图。
- en: 'The first thing we do is encode the features and labels of the dataset in table
    3.2 as NumPy arrays as follows:'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先将数据集的特征和标签编码到表3.2中，如下所示：
- en: '[PRE3]'
  id: totrans-299
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Next we plot the dataset. In the repository, we have some functions for plotting
    the code in the file [utils.py](http://utils.py), which you are invited to take
    a look at. The plot of the dataset is shown in figure 3.14\. Notice that the points
    do appear close to forming a line.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们绘制数据集。在仓库中，我们有一些用于绘制文件的函数[utils.py](http://utils.py)，你可以查看一下。数据集的绘图显示在图3.14中。注意点确实看起来接近形成一条线。
- en: '![](../Images/3-14.png)'
  id: totrans-301
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-14.png)'
- en: Figure 3.14 The plot of the points in table 3.2
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14表3.2中点的绘图
- en: Using the linear regression algorithm in our dataset
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的数据集中使用线性回归算法
- en: Now, let’s apply the algorithm to fit a line to these points. The following
    line of code runs the algorithm with the features, the labels, the learning rate
    equal to 0.01, and the number of epochs equal to 10,000\. The result is the plot
    shown in figure 3.15.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们将算法应用于拟合这些点的线条。以下代码行运行算法，使用特征、标签、学习率为0.01，epoch数为10,000。结果是图3.15中显示的绘图。
- en: '[PRE4]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: '![](../Images/3-15.png)'
  id: totrans-306
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-15.png)'
- en: Figure 3.15 The plot of the points in table 3.2 and the line that we obtained
    with the linear regression algorithm
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15表3.2中点的绘图以及我们用线性回归算法获得的线条
- en: Figure 3.15 shows the line where the (rounded) price per room is $51.05, and
    the base price is $99.10\. This is not far from the $50 and $100 we eyeballed
    earlier in the chapter.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15显示了每间房（四舍五入）的价格为51.05美元，基础价格为99.10美元的线条。这并不远于我们在本章早期用肉眼估算的50美元和100美元。
- en: To visualize the process, let’s look at the progression a bit more. In figure
    3.16, you can see a few of the intermediate lines. Notice that the line starts
    far away from the points. As the algorithm progresses, it moves slowly to fit
    better and better every time. Notice that at first (in the first 10 epochs), the
    line moves quickly toward a good solution. After epoch 50, the line is good, but
    it still doesn’t fit the points perfectly. If we let it run for the whole 10,000
    epochs, we get a great fit.
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 为了可视化这个过程，让我们更详细地看看进度。在图3.16中，你可以看到一些中间线条。注意线条开始时离点很远。随着算法的进行，它逐渐缓慢地更好地拟合每次。注意在最初（前10个epoch），线条快速地向好的解决方案移动。在第50个epoch之后，线条很好，但仍然没有完美地拟合点。如果我们让它运行整个10,000个epoch，我们会得到一个非常好的拟合。
- en: '![](../Images/3-16.png)'
  id: totrans-310
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-16.png)'
- en: Figure 3.16 Drawing some of the lines in our algorithm, as we approach a better
    solution. The first graphic shows the starting point. The second graphic shows
    the first 10 epochs of the linear regression algorithm. Notice how the line is
    moving closer to fitting the points. The third graphic shows the first 50 epochs.
    The fourth graphic shows epochs 51 to 10,000 (the last epoch).
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16展示了我们算法中一些线条的绘制，随着我们接近更好的解决方案。第一幅图显示了起点。第二幅图显示了线性回归算法的前10个epoch。注意线条是如何逐渐靠近拟合点的。第三幅图显示了前50个epoch。第四幅图显示了第51个epoch到第10,000个epoch（最后一个epoch）。
- en: Using the model to make predictions
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 使用模型进行预测
- en: 'Now that we have a shiny linear regression model, we can use it to make predictions!
    Recall from the beginning of the chapter that our goal was to predict the price
    of a house with four rooms. In the previous section, we ran the algorithm and
    obtained a slope (price per room) of 51.05 and a *y*-intercept (base price of
    the house) of 99.10\. Thus, the equation follows:'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个闪亮的线性回归模型，我们可以用它来做出预测！回想一下本章开头，我们的目标是预测四间房的房子的价格。在上一节中，我们运行了算法，得到了斜率（每间房的价格）为51.05和*y*截距（房子的基础价格）为99.10。因此，方程如下：
- en: '![](../Images/03_16_E01.png)'
  id: totrans-314
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_16_E01.png)'
- en: The prediction the model makes for a house with *r* = 4 rooms is
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 模型对有*r* = 4个房间的房子的预测是
- en: '![](../Images/03_16_E02.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/03_16_E02.png)'
- en: Note that $303.30 is not far from the $300 we eyeballed at the beginning of
    the chapter!
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 注意303.30并不远于我们在本章开头用肉眼估算的300美元！
- en: The general linear regression algorithm (optional)
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 通用线性回归算法（可选）
- en: This section is optional, because it focuses mostly on the mathematical details
    of the more abstract algorithm used for a general dataset. However, I encourage
    you to read it to get used to the notation that is used in most of the machine
    learning literature.
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: 这一节是可选的，因为它主要关注用于通用 dataset 的更抽象算法的数学细节。然而，我鼓励你阅读它，以便熟悉在大多数机器学习文献中使用的符号。
- en: In the previous sections, we outlined the linear regression algorithm for our
    dataset with only one feature. But as you can imagine, in real life, we will be
    working with datasets with many features. For this, we need a general algorithm.
    The good news is that the general algorithm is not very different from the specific
    one that we learned in this chapter. The only difference is that each of the features
    is updated in the same way that the slope was updated. In the housing example,
    we had one slope and one *y*-intercept. In the general case, think of many slopes
    and still one *y*-intercept.
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们概述了针对只有一个特征的 dataset 的线性回归算法。但正如你可以想象的那样，在现实生活中，我们将处理具有许多特征的 dataset。为此，我们需要一个通用算法。好消息是通用算法与我们在本章中学到的特定算法没有太大区别。唯一的区别是每个特征都是按照斜率更新的方式更新的。在住房示例中，我们有一个斜率和一个
    *y*-截距。在一般情况中，可以想象有许多斜率，但仍然只有一个 *y*-截距。
- en: 'The general case will consist of a dataset of *m* points and *n* features.
    Thus, the model has *m* weights (think of them as the generalization of the slope)
    and one bias. The notation follows:'
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 一般情况将包含一个包含 *m* 个点和 *n* 个特征的 dataset。因此，该模型有 *m* 个权重（可以将其视为斜率的推广）和一个偏差。符号如下：
- en: The data points are *x*^((1)), *x*^((2)), … , *x*^(^m^). Each point is of the
    form *x*^(^i^) = (*x*[1]^(^i^), *x*[2]^(^i^), … , *x*[n]^(^i^)).
  id: totrans-322
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 数据点是 *x*^((1))，*x*^((2))，…，*x*^(^m^)。每个点都是 *x*^(^i^) = (*x*[1]^(^i^)，*x*[2]^(^i^)，…，*x*[n]^(^i^))
    的形式。
- en: The corresponding labels are *y*[1], *y*[2], … , *y*[m].
  id: totrans-323
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 相应的标签是 *y*[1]，*y*[2]，…，*y*[m]。
- en: The weights of the model are *w*[1], *w*[2], … , *w*[n].
  id: totrans-324
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的权重为 *w*[1]，*w*[2]，…，*w*[n]。
- en: The bias of the model is *b*.
  id: totrans-325
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 模型的偏差为 *b*。
- en: Pseudocode for the general square trick
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 一般平方技巧的伪代码
- en: 'Inputs:'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: 输入：
- en: A model with equation *ŷ* = *w*[1]*x*[1] + *w*[2]*x*[2+] + … + *w*[n]*x*[n]
    + *b*
  id: totrans-328
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 方程为 *ŷ* = *w*[1]*x*[1] + *w*[2]*x*[2+] + … + *w*[n]*x*[n] + *b* 的模型
- en: A point with coordinates (*x*, *y*)
  id: totrans-329
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 坐标为 (*x*, *y*) 的点
- en: A small positive value *η* (the learning rate)
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个小的正值 *η*（学习率）
- en: 'Output:'
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: A model with equation *ŷ* = *w*[1]*'**x*[1] + *w*[2]*'**x*[2] + … + *w*[n]*'**x*[n]
    + *b**'* that is closer to the point
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个方程为 *ŷ* = *w*[1]*'**x*[1] + *w*[2]*'**x*[2] + … + *w*[n]*'**x*[n] + *b**'*
    的模型，该模型更接近点
- en: 'Procedure:'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 程序：
- en: Add *η*(*y* – *ŷ*) to the *y*-intercept *b*. Obtain *b**'* = *b* + *η*(*y* –
    *ŷ*).
  id: totrans-334
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η*(*y* – *ŷ*) 添加到 *y*-截距 *b*。得到 *b**'* = *b* + *η*(*y* – *ŷ*)。
- en: 'For *i* = 1, 2, …, *n*:'
  id: totrans-335
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对于 *i* = 1, 2, …, *n*：
- en: Add *η**x*(*y* – *ŷ*) to the weight *w*[i]. Obtain *w*[i]*'* = *w*[i]+ *η**r*(*y*
    – *ŷ*).
  id: totrans-336
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 将 *η**x*(*y* – *ŷ*) 添加到权重 *w*[i]。得到 *w*[i]*'* = *w*[i]+ *η**r*(*y* – *ŷ*)。
- en: '**Return**: The model with equation *ŷ* = *w*[1]*''**x*[1] + *w*[2]*''**x*[2]
    + … + *w*[n]*''**x*[n] + *b**''*'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: '**返回**：方程为 *ŷ* = *w*[1]*''**x*[1] + *w*[2]*''**x*[2] + … + *w*[n]*''**x*[n]
    + *b**'' 的模型'
- en: The pseudocode of the general linear regression algorithm is the same as the
    one in the section “The linear regression algorithm,” because it consists of iterating
    over the general square trick, so we’ll omit it.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 一般线性回归算法的伪代码与“线性回归算法”部分中的相同，因为它包含对一般平方技巧的迭代，所以我们将其省略。
- en: How do we measure our results? The error function
  id: totrans-339
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 我们如何衡量我们的结果？误差函数
- en: In the previous sections, we developed a direct approach to finding the best
    line fit. However, many times using a direct approach is difficult to solve problems
    in machine learning. A more indirect, yet more mechanical, way to do this is using
    *error functions*. An error function is a metric that tells us how our model is
    doing. For example, take a look at the two models in figure 3.17\. The one on
    the left is a bad model, whereas the one on the right is a good one. The error
    function measures this by assigning a large value to the bad model on the left
    and a small value to the good model on the right. Error functions are also sometimes
    called *loss functions* or *cost functions* on the literature. In this book, we
    call them error functions except in some special cases in which the more commonly
    used name requires otherwise.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们开发了一种直接的方法来找到最佳拟合线。然而，很多时候使用直接方法来解决机器学习中的问题是很困难的。一种更间接但更机械的方法是使用*误差函数*。误差函数是一个指标，告诉我们我们的模型表现如何。例如，看看图3.17中的两个模型。左边的是一个不好的模型，而右边的是一个好的模型。误差函数通过给左边的坏模型分配一个大的值，给右边的好模型分配一个小的值来衡量这一点。误差函数有时在文献中也被称为*损失函数*或*代价函数*。在这本书中，我们除了在某些更常用的名称需要的情况下，通常称之为误差函数。
- en: '![](../Images/3-17.png)'
  id: totrans-341
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-17.png)'
- en: Figure 3.17 Two models, a bad one (on the left) and a good one (on the right).
    The bad one is assigned a large error, and the good one is assigned a small error.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 两个模型，一个不好的（在左边）和一个好的（在右边）。不好的模型被分配了一个大的误差，而好的模型被分配了一个小的误差。
- en: Now the question is, how do we define a good error function for linear regression
    models? We have two common ways to do this called the *absolute error* and the
    *square error*. In short, the absolute error is the sum of vertical distances
    from the line to the points in the dataset, and the square error is the sum of
    the squares of these distances.
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们如何为线性回归模型定义一个好的误差函数？我们有两种常见的方法来做这件事，称为*绝对误差*和*平方误差*。简而言之，绝对误差是数据集中点到直线的垂直距离之和，平方误差是这些距离的平方之和。
- en: In the next few sections, we learn about these two error functions in more detail.
    Then we see how to reduce them using a method called gradient descent. Finally,
    we plot one of these
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 在接下来的几节中，我们将更详细地了解这两个误差函数。然后我们看看如何使用称为梯度下降的方法来减少它们。最后，我们绘制其中一个
- en: error functions in our existing example and see how quickly the gradient descent
    method helps us decrease it.
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们现有的例子中查看误差函数，看看梯度下降法如何快速帮助我们减小它。
- en: 'The absolute error: A metric that tells us how good our model is by adding
    distances'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 绝对误差：一个通过添加距离来告诉我们模型好坏的指标
- en: In this section we look at the absolute error, which is a metric that tells
    us how good our model is. The absolute error is the sum of the distances between
    the data points and the line. Why is it called the absolute error? To calculate
    each of the distances, we take the difference between the label and the predicted
    label. This difference can be positive or negative depending on whether the point
    is above or below the line. To turn this difference into a number that is always
    positive, we take its absolute value.
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们来看绝对误差，这是一个衡量我们模型好坏的指标。绝对误差是数据点与直线之间的距离之和。为什么叫绝对误差？为了计算每个距离，我们取标签与预测标签之间的差值。这个差值可以是正的或负的，取决于点是在直线上方还是下方。为了将这个差值转换成一个始终为正的数，我们取它的绝对值。
- en: By definition, a good linear regression model is one where the line is close
    to the points. What does *close* mean in this case? This is a subjective question,
    because a line that is close to some of the points may be far from others. In
    that case, would we rather pick a line that is very close to some of the points
    and far from some of the others? Or do we try to pick one that is somewhat close
    to all the points? The absolute error helps us make this decision. The line we
    pick is the one that minimizes the absolute error, that is, the one for which
    the sum of vertical distances from each of the points to the line is minimal.
    In figure 3.18, you can see two lines, and their absolute error is illustrated
    as the sum of the vertical segments. The line on the left has a large absolute
    error, whereas the one on the right has a small absolute error. Thus, between
    these two, we would pick the one on the right.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 根据定义，一个好的线性回归模型是线接近点的模型。在这种情况下，“接近”是什么意思？这是一个主观问题，因为接近一些点的线可能离其他点很远。在这种情况下，我们更愿意选择一个对一些点非常接近而对其他点较远的线，还是尝试选择一个对所有点都有些接近的线？绝对误差帮助我们做出这个决定。我们选择的线是使绝对误差最小化的线，即每个点到线的垂直距离之和最小的线。在图3.18中，你可以看到两条线，它们的绝对误差被描绘为垂直线段的总和。左侧的线绝对误差较大，而右侧的线绝对误差较小。因此，在这两条线之间，我们会选择右侧的那条线。
- en: '![](../Images/3-18.png)'
  id: totrans-349
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-18.png)'
- en: Figure 3.18 The absolute error is the sum of the vertical distances from the
    points to the line. Note that the absolute error is large for the bad model on
    the left and small for the good model on the right.
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 绝对误差是点与线之间的垂直距离之和。注意，左侧的坏模型绝对误差较大，而右侧的好模型绝对误差较小。
- en: 'The square error: A metric that tells us how good our model is by adding squares
    of distances'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差：一个度量，通过添加距离的平方来告诉我们我们的模型有多好
- en: The square error is very similar to the absolute error, except instead of taking
    the absolute value of the difference between the label and the predicted label,
    we take the square. This always turns the number into a positive number, because
    squaring a number always makes it positive. The process is illustrated in figure
    3.19, where the square error is illustrated as the sum of the areas of the squares
    of the lengths from the points to the line. You can see how the bad model on the
    left has a large square error, whereas the good model on the right has a small
    square error.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 平方误差与绝对误差非常相似，除了我们不是取标签和预测标签之间差异的绝对值，而是取平方。这总是将数字转换为正数，因为平方一个数字总是使其变为正数。这个过程在图3.19中得到了说明，其中平方误差被描绘为点与线之间长度的平方的面积之和。你可以看到左侧的坏模型平方误差较大，而右侧的好模型平方误差较小。
- en: '![](../Images/3-19.png)'
  id: totrans-353
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-19.png)'
- en: Figure 3.19 The square error is the sum of the squares of the vertical distances
    from the points to the line. Note that the square error is large for the bad model
    on the left and small for the good model on the right.
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19 平方误差是点与线之间垂直距离的平方之和。注意，左侧的坏模型平方误差较大，而右侧的好模型平方误差较小。
- en: As was mentioned earlier, the square error is used more commonly in practice
    than the absolute error. Why? A square has a much nicer derivative than an absolute
    value, which comes in handy during the training process.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 如前所述，在实践应用中，平方误差比绝对误差使用得更普遍。为什么？因为平方的导数比绝对值要平滑得多，这在训练过程中非常有用。
- en: Mean absolute and (root) mean square errors are more common in real life
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 在现实生活中，平均绝对误差和（根）均方误差更为常见
- en: Throughout this chapter we use absolute and square errors for illustration purposes.
    However, in practice, the *mean absolute error* and the *mean square error* are
    used much more commonly. These are defined in a similar way, except instead of
    calculating sums, we calculate averages. Thus, the mean absolute error is the
    average of the vertical distances from the points to the line, and the mean square
    error is the average of the squares of these same distances. Why are they more
    common? Imagine if we’d like to compare the error or a model using two datasets,
    one with 10 points and one with 1 million points. If the error is a sum of quantities,
    one for every point, then the error is probably much higher on the dataset of
    1 million points, because we are adding many more numbers. If we want to compare
    them properly, we instead use averages in the calculation of our error to obtain
    a measure of how far the line is from each point *on average*.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们使用绝对误差和平方误差进行说明。然而，在实践中，*平均绝对误差*和*平均平方误差*使用得更为普遍。它们的定义方式类似，只是我们计算的是平均值而不是总和。因此，平均绝对误差是点到线的垂直距离的平均值，平均平方误差是这些距离平方的平均值。为什么它们更常见呢？想象一下，如果我们想使用两个数据集来比较误差或模型，一个有10个点，另一个有100万个点。如果误差是每个点的数量之和，那么在100万个点的数据集中，误差可能要高得多，因为我们正在添加更多的数字。如果我们想正确比较它们，我们就在误差的计算中使用平均值，以获得一条线与每个点平均距离的度量。
- en: 'For illustration purposes, another error commonly used is the *root mean square
    error*, or *RMSE* for short. As the name implies, this is defined as the root
    of the mean square error. It is used to match the units in the problem and also
    to give us a better idea of how much error the model makes in a prediction. How
    so? Imagine the following scenario: if we are trying to predict house prices,
    then the units of the price and the predicted price are, for example, dollars
    ($). The units of the square error and the mean square error are dollars squared,
    which is not a common unit. If we take the square root, then not only do we get
    the correct unit, but we also get a more accurate idea of roughly by how many
    dollars the model is off per house. Say, if the root mean square error is $10,000,
    then we can expect the model to make an error of around $10,000 for any prediction
    we make.'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明目的，另一个常用的错误是**均方根误差**，或简称为*RMSE*。正如其名所示，这是均方误差的平方根。它用于匹配问题中的单位，并帮助我们更好地了解模型在预测中产生的误差有多大。为什么会这样呢？想象以下场景：如果我们试图预测房价，那么价格和预测价格的单位，例如，是美元（$）。平方误差和均方误差的单位是美元的平方，这不是一个常见的单位。如果我们取平方根，那么我们不仅得到正确的单位，而且还能更准确地了解模型每套房子偏离多少美元。比如说，如果均方根误差是$10,000，那么我们可以预期模型在每次预测中都会产生大约$10,000的误差。
- en: 'Gradient descent: How to decrease an error function by slowly descending from
    a mountain'
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降：如何通过缓慢下降从山上减少误差函数
- en: 'In this section, I show you how to decrease any of the previous errors using
    a similar method to the one we would use to slowly descend from a mountain. This
    process uses derivatives, but here is the great news: you don’t need derivatives
    to understand it. We already used them in the training process in the sections
    “The square trick” and “The absolute trick” earlier. Every time we “move a small
    amount in this direction,” we are calculating in the background a derivative of
    the error function and using it to give us a direction in which to move our line.
    If you love calculus and you want to see the entire derivation of this algorithm
    using derivatives and gradients, see appendix B.'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向您展示如何使用类似于我们缓慢下降从山上使用的方法来减少任何之前的误差。这个过程使用导数，但好消息是：您不需要理解导数。我们已经在之前的“平方技巧”和“绝对技巧”部分中使用了它们。每次我们“在这个方向上移动一小步”，我们都在后台计算误差函数的导数，并使用它来给我们一个移动线的方向。如果您喜欢微积分并且想看到使用导数和梯度的整个算法推导，请参阅附录B。
- en: 'Let’s take a step back and look at linear regression from far away. What is
    it that we want to do? We want to find the line that best fits our data. We have
    a metric called the error function, which tells us how far a line is from the
    data. Thus, if we could just reduce this number as much as possible, we would
    find the best line fit. This process, common in many areas in mathematics, is
    called *minimizing functions*, that is, finding the smallest possible value that
    a function can return. This is where gradient descent comes in: it is a great
    way to minimize a function.'
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们退一步，从远处看看线性回归。我们想要做什么？我们想要找到最适合我们数据的线。我们有一个称为误差函数的度量，它告诉我们一条线离数据有多远。因此，如果我们能尽可能减少这个数字，我们就能找到最佳拟合线。这个过程在数学的许多领域都很常见，被称为“最小化函数”，即找到函数可能返回的最小可能值。这就是梯度下降发挥作用的地方：它是一种很好的最小化函数的方法。
- en: In this case, the function we are trying to minimize is the error (absolute
    or square) of our model. A small caveat is that gradient descent doesn’t always
    find the exact minimum value of the function, but it may find something very close
    to it. The good news is that, in practice, gradient descent is fast and effective
    at finding points where the function is low.
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们试图最小化的函数是模型误差（绝对值或平方值）。一个小小的警告是，梯度下降并不总是找到函数的精确最小值，但它可能会找到非常接近的值。好消息是，在实践中，梯度下降在找到函数值较低的点方面既快又有效。
- en: How does gradient descent work? Gradient descent is the equivalent of descending
    from a mountain. Let’s say we find ourselves on top of a tall mountain called
    Mount Errorest. We wish to descend, but it is very foggy, and we can see only
    about one meter away. What do we do? A good method is to look around ourselves
    and figure out in what direction we can take one single step, in a way that we
    descend the most. This process is illustrated in figure 3.20.
  id: totrans-363
  prefs: []
  type: TYPE_NORMAL
  zh: 梯度下降是如何工作的？梯度下降相当于从山上下来。假设我们发现自己站在一个名叫埃罗斯特的高山上。我们希望下山，但雾很大，我们只能看到大约一米的距离。我们该怎么办？一个好方法是环顾四周，找出我们能够迈出一步的方向，这样我们就能下降得最多。这个过程在图3.20中得到了说明。
- en: '![](../Images/3-20.png)'
  id: totrans-364
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-20.png)'
- en: Figure 3.20 We are on top of Mount Errorest and wish to get to the bottom, but
    we can’t see very far. A way to go down is to look at all the directions in which
    we can take one step and figure out which one helps us descend the most. Then
    we are one step closer to the bottom.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.20 我们站在埃罗斯特山顶上，希望到达底部，但我们看得不是很远。一种下山的办法是看看我们能够迈出一步的所有方向，并找出哪一个能让我们下降得最多。这样我们就离底部更近了一步。
- en: When we find this direction, we take one small step, and because that step was
    taken in the direction of greatest descent, then most likely, we have descended
    a small amount. All we have to do is repeat this process many times until we (hopefully)
    reach the bottom. This process is illustrated in figure 3.21.
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们找到这个方向时，我们迈出一小步，因为这一步是在最大下降方向上迈出的，所以很可能会下降一小段距离。我们只需要重复这个过程很多次，直到我们（希望）到达底部。这个过程在图3.21中得到了说明。
- en: '![](../Images/3-211.png)'
  id: totrans-367
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/3-211.png)'
- en: Figure 3.21 The way to descend from the mountain is to take that one small step
    in the direction that makes us descend the most and to continue doing this for
    a long time.
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.21 从山上下来的方法是朝着让我们下降最多的方向迈出那一步，并且长时间地继续这样做。
- en: Why did I say *hopefully*? Well, this process has many caveats. We could reach
    the bottom, or we could also reach a valley and then we have nowhere else to move.
    We won’t deal with that now, but we have several techniques to reduce the probability
    of this happening. In appendix B, “Using gradient descent to train neural networks,”
    some of these techniques are outlined.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我说“希望”呢？因为这个过程有很多注意事项。我们可能会到达底部，或者我们可能会到达一个山谷，然后我们就无处可去了。我们现在不处理这个问题，但我们有一些技术可以减少这种情况发生的概率。在附录B“使用梯度下降训练神经网络”中，概述了一些这些技术。
- en: 'A lot of math here that we are sweeping under the rug is explained in more
    detail in appendix B. But what we did in this chapter was exactly gradient descent.
    How so? Gradient descent works as follows:'
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们故意忽略了很多数学内容，这些内容在附录B中有更详细的解释。但我们在本章中做的是精确的梯度下降。为什么会这样呢？梯度下降的工作原理如下：
- en: Start somewhere on the mountain.
  id: totrans-371
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从山上某个地方开始。
- en: Find the best direction to take one small step.
  id: totrans-372
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 找到迈出这一小步的最佳方向。
- en: Take this small step.
  id: totrans-373
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 走出这一小步。
- en: Repeat steps 2 and 3 many times.
  id: totrans-374
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3多次。
- en: 'This may look familiar, because in the section “The linear regression algorithm,”
    after defining the absolute and square tricks, we defined the linear regression
    algorithm in the following way:'
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 这可能看起来很熟悉，因为在“线性回归算法”部分中，在定义绝对值和平方技巧之后，我们以以下方式定义了线性回归算法：
- en: Start with any line.
  id: totrans-376
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从任何一条线开始。
- en: Find the best direction to move our line a little bit, using either the absolute
    or the square trick.
  id: totrans-377
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用绝对值或平方技巧找到移动我们的线一点的最佳方向。
- en: Move the line a little bit in this direction.
  id: totrans-378
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在这个方向上稍微移动这条线。
- en: Repeat steps 2 and 3 many times.
  id: totrans-379
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 重复步骤2和3多次。
- en: '![](../Images/3-22.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-22.png)'
- en: Figure 3.22 Each point on this mountain corresponds to a different model. The
    points below are good models with a small error, and the points above are bad
    models with a large error. The goal is to descend from this mountain. The way
    to descend is by starting somewhere and continuously taking a step that makes
    us descend. The gradient will help us decide in what direction to take a step
    that helps us descend the most.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.22 每个山上的点都对应一个不同的模型。下面的点代表误差小的良好模型，而上面的点代表误差大的不良模型。目标是下山。下山的方式是从某个地方开始，并持续采取使我们下山的步骤。梯度将帮助我们决定采取哪个方向的步骤可以帮助我们最大限度地下降。
- en: The mental picture of this is illustrated in figure 3.22\. The only difference
    is that this error function looks less like a mountain and more like a valley,
    and our goal is to descend to the lowest point. Each point in this valley corresponds
    to some model (line) that tries to fit our data. The height of the point is the
    error given by that model. Thus, the bad models are on top, and the good models
    are on the bottom. We are trying to go as low as possible. Each step takes us
    from one model to a slightly better model. If we take a step like this many times,
    we’ll eventually get to the best model (or at least, a pretty good one!).
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 这种心理图示如图3.22所示。唯一的区别是，这个误差函数看起来更像一个山谷而不是一座山，我们的目标是下降到最低点。山谷中的每个点都对应于一些试图拟合我们的数据的模型（线）。这个点的高度是由该模型给出的误差。因此，不良模型在顶部，良好模型在底部。我们试图尽可能低。每一步都使我们从一个模型到一个稍微好一点的模型。如果我们采取这样的步骤很多次，我们最终会得到最好的模型（或者至少，相当不错的一个！）。
- en: Plotting the error function and knowing when to stop running the algorithm
  id: totrans-383
  prefs: []
  type: TYPE_NORMAL
  zh: 绘制误差函数和知道何时停止运行算法
- en: 'In this section, we see a plot of the error function for the training that
    we performed earlier in the section “Using the linear regression algorithm in
    our dataset.” This plot gives us useful information about training this model.
    In the repository, we have also plotted the root mean square error function (RMSE)
    defined in the section “Mean absolute and (root) mean square errors ...”. The
    code for calculating the RMSE follows:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们看到的是我们在“使用线性回归算法在我们的数据集中”部分中较早进行的训练的误差函数图。这个图为我们提供了关于训练此模型的有用信息。在存储库中，我们还绘制了在“平均绝对值和（根）均方误差...”部分中定义的均方根误差函数（RMSE）。计算RMSE的代码如下：
- en: '[PRE5]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: dot product To code the RMSE function, we used the dot product, which is an
    easy way to write a sum of products of corresponding terms in two vectors. For
    example, the dot product of the vectors (1,2,3) and (4,5,6) is 1 · 4 + 2 · 5 +
    3 · 6 = 32\. If we calculate the dot product of a vector and itself, we obtain
    the sum of squares of the entries.
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: 点积 为了编码RMSE函数，我们使用了点积，这是一种轻松地写出两个向量中对应项乘积之和的方法。例如，向量（1,2,3）和（4,5,6）的点积是 1 ·
    4 + 2 · 5 + 3 · 6 = 32。如果我们计算一个向量与自身的点积，我们得到该向量元素平方和。
- en: 'The plot of our error is shown in figure 3.23\. Note that it quickly dropped
    after about 1,000 iterations, and it didn’t change much after that. This plot
    gives us useful information: it tells us that for this model, we can run the training
    algorithm for only 1,000 or 2,000 iterations instead of 10,000 and still get similar
    results.'
  id: totrans-387
  prefs: []
  type: TYPE_NORMAL
  zh: 我们误差的图示如图3.23所示。注意它在大约1,000次迭代后迅速下降，之后变化不大。这个图为我们提供了有用的信息：它告诉我们，对于这个模型，我们只需要运行训练算法1,000或2,000次，而不是10,000次，仍然可以得到相似的结果。
- en: '![](../Images/3-23.png)'
  id: totrans-388
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-23.png)'
- en: Figure 3.23 The plot of the root mean square error for our running example.
    Notice how the algorithm succeeded in reducing this error after a little over
    1,000 iterations. This means that we don’t need to keep running this algorithm
    for 10,000 iterations, because around 2,000 of them do the job.
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.23 我们运行示例的均方根误差的图。注意算法在超过1,000次迭代后成功减少了这个误差。这意味着我们不需要运行这个算法10,000次，因为大约有2,000次就能完成任务。
- en: 'In general, the error function gives us good information to decide when to
    stop running the algorithm. Often, this decision is based on the time and the
    computational power available to us. However, other useful benchmarks are commonly
    used in the practice, such as the following:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: 通常，误差函数为我们提供了足够的信息来决定何时停止运行算法。通常，这个决定是基于我们可用的时间和计算能力。然而，在实际应用中，通常还会使用其他有用的基准，如下所示：
- en: When the loss function reaches a certain value that we have predetermined
  id: totrans-391
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当损失函数达到我们预先设定的某个值时
- en: When the loss function doesn’t decrease by a significant amount during several
    epochs
  id: totrans-392
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当损失函数在几个时期内没有显著下降时
- en: Do we train using one point at a time or many? Stochastic and batch gradient
    descent
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 我们是一次训练一个点还是多个点？随机和批量梯度下降
- en: In the section “How to get the computer to draw this line,” we trained a linear
    regression model by repeating a step many times. This step consisted of picking
    one point and moving the line toward that point. In the section “How do we measure
    our results,” we trained a linear regression model by calculating the error (absolute
    or squared) and decreasing it using gradient descent. However, this error was
    calculated on the entire dataset, not on one point at a time. Why is this?
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: 在“如何让计算机绘制这条线”的部分，我们通过重复执行一个步骤多次来训练线性回归模型。这个步骤包括选择一个点并将线移动到该点。在“我们如何衡量我们的结果”的部分，我们通过计算误差（绝对值或平方值）并使用梯度下降来减少它来训练线性回归模型。然而，这个误差是在整个数据集上计算的，而不是逐点计算。为什么是这样？
- en: The reality is that we can train models by iterating on one point at a time
    or on the entire dataset. However, when the datasets are very big, both options
    may be expensive. We can practice a useful method called *mini-batch learning**,*
    which consists of dividing our data into many mini-batches. In each iteration
    of the linear regression algorithm, we pick one of the mini-batches and proceed
    to adjust the weights of the model to reduce the error in that mini-batch. The
    decision of using one point, a mini-batch of points, or the entire dataset on
    each iteration gives rise to three general types of gradient descent algorithms.
    When we use one point at a time, it is called *stochastic gradient descent*. When
    we use a mini-batch, it is called *mini-batch gradient descent*. When we use the
    entire dataset, it is called *batch gradient descent*. This process is illustrated
    in more detail in appendix B, “Using gradient descent to train models.”
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 事实上，我们可以通过逐点迭代或对整个数据集进行迭代来训练模型。然而，当数据集非常大时，这两种选择都可能很昂贵。我们可以练习一种有用的方法，称为 *小批量学习**，它包括将我们的数据分成许多小批量。在线性回归算法的每次迭代中，我们选择一个小批量，然后调整模型权重以减少该小批量中的误差。在每个迭代中使用一个点、一个小批量点或整个数据集的决定产生了三种类型的梯度下降算法。当我们每次使用一个点时，它被称为
    *随机梯度下降*。当我们使用小批量时，它被称为 *小批量梯度下降*。当我们使用整个数据集时，它被称为 *批量梯度下降*。这个过程在附录 B “使用梯度下降训练模型”中进行了更详细的说明。
- en: 'Real-life application: Using Turi Create to predict housing prices in India'
  id: totrans-396
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 真实应用：使用 Turi Create 预测印度房价
- en: 'In this section, I show you a real-life application. We’ll use linear regression
    to predict housing prices in Hyderabad, India. The dataset we use comes from Kaggle,
    a popular site for machine learning competitions. The code for this section follows:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我将向你展示一个真实的应用案例。我们将使用线性回归来预测印度海得拉巴的房价。我们使用的数据集来自 Kaggle，这是一个流行的机器学习竞赛网站。本节的代码如下：
- en: '**Notebook**: House_price_predictions.ipynb'
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**笔记本**：House_price_predictions.ipynb'
- en: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/House_price_predictions.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/House_price_predictions.ipynb)'
  id: totrans-399
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: '[https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/House_price_predictions.ipynb](https://github.com/luisguiserrano/manning/blob/master/Chapter_3_Linear_Regression/House_price_predictions.ipynb)'
- en: '**Dataset**: Hyderabad.csv'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**数据集**：Hyderabad.csv'
- en: 'This dataset has 6,207 rows (one per house) and 39 columns (features). As you
    can imagine, we won’t code the algorithm by hand. Instead, we use Turi Create,
    a popular and useful package in which many machine learning algorithms are implemented.
    The main object to store data in Turi Create is the SFrame. We start by downloading
    the data into an SFrame, using the following command:'
  id: totrans-401
  prefs: []
  type: TYPE_NORMAL
  zh: 这个数据集有 6,207 行（每行代表一栋房子）和 39 列（特征）。正如你所想象的，我们不会手动编写算法。相反，我们使用 Turi Create，这是一个流行的且非常有用的包，其中实现了许多机器学习算法。在
    Turi Create 中存储数据的主要对象是 SFrame。我们首先使用以下命令将数据下载到 SFrame 中：
- en: '[PRE6]'
  id: totrans-402
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: The table is too big, but you can see the first few rows and columns in table
    3.3.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 表格太大，但你可以看到表3.3中的前几行和列。
- en: Table 3.3 The first five rows and seven columns of the Hyderabad housing prices
    dataset
  id: totrans-404
  prefs: []
  type: TYPE_NORMAL
  zh: 表3.3 海得拉巴房价数据集的前五行和七列
- en: '| Price | Area | No. of Bedrooms | Resale | MaintenanceStaff | Gymnasium |
    SwimmingPool |'
  id: totrans-405
  prefs: []
  type: TYPE_TB
  zh: '| 价格 | 面积 | 卧室数量 | 二手房 | 维护人员 | 健身房 | 游泳池 |'
- en: '| 30000000 | 3340 | 4 | 0 | 1 | 1 | 1 |'
  id: totrans-406
  prefs: []
  type: TYPE_TB
  zh: '| 30000000 | 3340 | 4 | 0 | 1 | 1 | 1 |'
- en: '| 7888000 | 1045 | 2 | 0 | 0 | 1 | 1 |'
  id: totrans-407
  prefs: []
  type: TYPE_TB
  zh: '| 7888000 | 1045 | 2 | 0 | 0 | 1 | 1 |'
- en: '| 4866000 | 1179 | 2 | 0 | 0 | 1 | 1 |'
  id: totrans-408
  prefs: []
  type: TYPE_TB
  zh: '| 4866000 | 1179 | 2 | 0 | 0 | 1 | 1 |'
- en: '| 8358000 | 1675 | 3 | 0 | 0 | 0 | 0 |'
  id: totrans-409
  prefs: []
  type: TYPE_TB
  zh: '| 8358000 | 1675 | 3 | 0 | 0 | 0 | 0 |'
- en: '| 6845000 | 1670 | 3 | 0 | 1 | 1 | 1 |'
  id: totrans-410
  prefs: []
  type: TYPE_TB
  zh: '| 6845000 | 1670 | 3 | 0 | 1 | 1 | 1 |'
- en: 'Training a linear regression model in Turi Create takes only one line of code.
    We use the function create from the package linear_regression. In this function,
    we only need to specify the target (label), which is Price, as follows:'
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 在Turi Create中训练线性回归模型只需要一行代码。我们使用来自线性回归包的create函数。在这个函数中，我们只需要指定目标（标签），即价格，如下所示：
- en: '[PRE7]'
  id: totrans-412
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: It may take a few moments to train, but after it trains, it outputs some information.
    One of the fields it outputs is the root mean square error. For this model, the
    RMSE is in the order of 3,000,000\. This is a large RMSE, but it doesn’t mean
    the model makes bad predictions. It may mean that the dataset has many outliers.
    As you can imagine, the price of a house may depend on many other features that
    are not in the dataset.
  id: totrans-413
  prefs: []
  type: TYPE_NORMAL
  zh: 训练可能需要一些时间，但训练完成后，它会输出一些信息。其中之一是均方根误差。对于这个模型，RMSE的数量级为3,000,000。这是一个较大的RMSE，但这并不意味着模型做出了糟糕的预测。它可能意味着数据集中有许多异常值。正如你可以想象的那样，房价可能取决于数据集中没有的其他许多特征。
- en: 'We can use the model to predict the price of a house with an area of 1,000,
    and three bedrooms as follows:'
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个模型来预测面积为1,000平方米，有三个卧室的房子的价格如下：
- en: '[PRE8]'
  id: totrans-415
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: The model outputs that the price for a house of size 1,000 and three bedrooms
    is 2,594,841.
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 模型输出，面积为1,000平方米，有三个卧室的房子的价格为2,594,841。
- en: 'We can also train a model using fewer features. The create function allows
    us to input the features we want to use as an array. The following line of code
    trains a model called simple_model that uses the area to predict the price:'
  id: totrans-417
  prefs: []
  type: TYPE_NORMAL
  zh: 我们也可以使用较少的特征来训练模型。create函数允许我们输入作为数组想要使用的特征。以下代码行训练了一个名为simple_model的模型，它使用面积来预测价格：
- en: '[PRE9]'
  id: totrans-418
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'We can explore the weights of this model with the following line of code:'
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用以下代码行来探索这个模型的权重：
- en: '[PRE10]'
  id: totrans-420
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'The output gives us the following weights:'
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: 输出给出了以下权重：
- en: 'Slope: 9664.97'
  id: totrans-422
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 斜率：9664.97
- en: '*y*-intercept: –6,105,981.01'
  id: totrans-423
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y*-截距：-6,105,981.01'
- en: The intercept is the bias, and the coefficient for area is the slope of the
    line, when we plot area and price. The plot of the points with the corresponding
    model is shown in figure 3.24.
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 截距是偏差，面积系数是当我们在面积和价格上绘制时线的斜率。点的绘制与相应模型的关系如图3.24所示。
- en: '![](../Images/3-24.png)'
  id: totrans-425
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-24.png)'
- en: Figure 3.24 The Hyderabad housing prices dataset restricted to area and price.
    The line is the model we’ve obtained using only the area feature to predict the
    price.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.24 针对面积和价格限制的海得拉巴房价数据集。这条线是我们仅使用面积特征来预测价格所得到的模型。
- en: We could do a lot more in this dataset, and I invite you to continue exploring.
    For example, try to explore what features are more important than others by looking
    at the weights of the model. I encourage you to take a look at the Turi Create
    documentation ([https://apple.github.io/turicreate/docs/api/](https://apple.github.io/turicreate/docs/api/))
    for other functions and tricks you can do to improve this model.
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以在这个数据集中做很多事情，我邀请你继续探索。例如，通过查看模型的权重来探索哪些特征比其他特征更重要。我鼓励你查看Turi Create文档（[https://apple.github.io/turicreate/docs/api/](https://apple.github.io/turicreate/docs/api/）），了解你可以执行的其他函数和技巧来改进这个模型。
- en: What if the data is not in a line? Polynomial regression
  id: totrans-428
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 如果数据不是线性的呢？多项式回归
- en: In the previous sections, we learned how to find the best line fit for our data,
    assuming our data closely resembles a line. But what happens if our data doesn’t
    resemble a line? In this section, we learn a powerful extension to linear regression
    called *polynomial regression*, which helps us deal with cases in which the data
    is more complex.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的章节中，我们学习了如何在数据接近直线的情况下找到最佳拟合线。但如果我们发现数据并不呈线性呢？在本节中，我们将学习一种称为**多项式回归**的强大扩展，它帮助我们处理数据更复杂的情况。
- en: 'A special kind of curved functions: Polynomials'
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 一种特殊的曲线函数：多项式
- en: To learn polynomial regression, first we need to learn what polynomials are.
    *Polynomials* are a class of functions that are helpful when modeling nonlinear
    data.
  id: totrans-431
  prefs: []
  type: TYPE_NORMAL
  zh: 要学习多项式回归，首先我们需要了解什么是多项式。*多项式*是一类在模拟非线性数据时很有用的函数。
- en: 'We’ve already seen polynomials, because every line is a polynomial of degree
    1\. Parabolas are examples of polynomials of degree 2\. Formally, a polynomial
    is a function in one variable that can be expressed as a sum of multiples of powers
    of this variable. The powers of a variable *x* are 1, *x*, *x*², *x*³, …. Note
    that the two first are *x*⁰ = 1 and *x*¹ = *x*. Therefore, the following are examples
    of polynomials:'
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到过多项式了，因为每条线都是1次多项式。抛物线是2次多项式的例子。形式上，多项式是一个可以表示为该变量幂的倍数之和的函数。变量*x*的幂是1，*x*，*x*²，*x*³，……。请注意，前两个是*x*⁰
    = 1和*x*¹ = *x*。因此，以下都是多项式的例子：
- en: '*y* = 4'
  id: totrans-433
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = 4'
- en: '*y* = 3*x* + 2'
  id: totrans-434
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = 3*x* + 2'
- en: '*y* = *x*² – 2*x* + 5'
  id: totrans-435
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = *x*² – 2*x* + 5'
- en: '*y* = 2*x*³ + 8*x*² – 40'
  id: totrans-436
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* = 2*x*³ + 8*x*² – 40'
- en: We define the *degree* of the polynomial as the exponent of the highest power
    in the expression of the polynomial. For example, the polynomial *y* = 2*x*³ +
    8*x*² – 40 has degree 3, because 3 is the highest exponent that the variable *x*
    is raised to. Notice that in the example, the polynomials have degree 0, 1, 2,
    and 3\. A polynomial of degree 0 is always a constant, and a polynomial of degree
    1 is a linear equation like the ones we’ve seen previously in this chapter.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: 我们定义多项式的*次数*为多项式表达式中最高次幂的指数。例如，多项式*y* = 2*x*³ + 8*x*² – 40的次数是3，因为3是变量*x*被提升到的最高指数。请注意，在例子中，多项式的次数是0，1，2和3。0次多项式始终是一个常数，1次多项式是我们在本章之前看到的线性方程。
- en: The graph of a polynomial looks a lot like a curve that oscillates several times.
    The number of times it oscillates is related to the degree of the polynomial.
    If a polynomial has degree *d*, then the graph of that polynomial is a curve that
    oscillates at most *d* – 1 times (for *d* > 1). In figure 3.25 we can see the
    plots of some examples of polynomials.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 多项式的图像看起来很像振荡几次的曲线。振荡的次数与多项式的次数有关。如果一个多项式的次数是*d*，那么该多项式的图像是一条最多振荡*d* – 1次（对于*d*
    > 1）的曲线。在图3.25中，我们可以看到一些多项式示例的图像。
- en: '![](../Images/3-25.png)'
  id: totrans-439
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-25.png)'
- en: Figure 3.25 Polynomials are functions that help us model our data better. Here
    are the plots of four polynomials of degrees 0 to 3\. Note that the polynomial
    of degree 0 is a horizontal line, the polynomial of degree 1 is any line, the
    polynomial of degree 2 is a parabola, and the polynomial of degree 3 is a curve
    that oscillates twice.
  id: totrans-440
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.25 多项式是有助于我们更好地模拟数据的函数。以下是0到3次多项式的图像。请注意，0次多项式是一条水平线，1次多项式是任何一条直线，2次多项式是抛物线，而3次多项式是一条振荡两次的曲线。
- en: From the plot, notice that polynomials of degree 0 are flat lines. Polynomials
    of degree 1 are lines with slopes different from 0\. Polynomials of degree 2 are
    quadratics (parabolas). Polynomials of degree 3 look like a curve that oscillates
    twice (although they could potentially oscillate fewer times). How would the plot
    of a polynomial of degree 100 look like? For example, the plot of *y* = *x*^(100)
    – 8*x*^(62) + 73*x*^(27) – 4*x* + 38? We’d have to plot it to find out, but for
    sure, we know that it is a curve that oscillates at most 99 times.
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 从图中可以看出，0次多项式是平坦的直线。1次多项式是斜率不为0的直线。2次多项式是二次函数（抛物线）。3次多项式看起来像是一条振荡两次的曲线（尽管它们可能振荡的次数更少）。一个100次多项式的图像会是什么样的呢？例如，*y*
    = *x*^(100) – 8*x*^(62) + 73*x*^(27) – 4*x* + 38的图像？我们得画出来才能知道，但可以肯定的是，它是一条最多振荡99次的曲线。
- en: 'Nonlinear data? No problem: Let’s try to fit a polynomial curve to it'
  id: totrans-442
  prefs: []
  type: TYPE_NORMAL
  zh: 非线性数据？没问题：让我们尝试将其拟合到多项式曲线上
- en: In this section, we see what happens if our data is not linear (i.e., does not
    look like it forms a line), and we want to fit a polynomial curve to it. Let’s
    say that our data looks like the left side of figure 3.26\. No matter how much
    we try, we can’t really find a good line that fits this data. No problem! If we
    decide to fit a polynomial of degree 3 (also called a cubic), then we get the
    curve shown at the right of figure 3.26, which is a much better fit to the data.
  id: totrans-443
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们来看一下如果我们的数据不是线性的（即，看起来不像形成一条线），并且我们想要将其拟合到多项式曲线上会发生什么。假设我们的数据看起来像图3.26的左侧。无论我们尝试多少次，我们都无法真正找到一个适合这些数据的良好直线。没问题！如果我们决定拟合一个3次多项式（也称为三次多项式），那么我们就会得到图3.26右侧的曲线，它比数据拟合得更好。
- en: '![](../Images/3-26.png)'
  id: totrans-444
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/3-26.png)'
- en: Figure 3.26 Polynomial regression is useful when it comes to modeling nonlinear
    data. If our data looks like the left part of the figure, it will be hard to find
    a line that fits it well. However, a curve will fit the data well, as you can
    see in the right part of the figure. Polynomial regression helps us find this
    curve.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.26 多项式回归在建模非线性数据时很有用。如果我们的数据看起来像图中的左侧部分，那么很难找到一条很好地拟合它的线。然而，正如你可以从图的右侧部分看到的那样，一条曲线可以很好地拟合数据。多项式回归帮助我们找到这条曲线。
- en: 'The process to train a polynomial regression model is similar to the process
    of training a linear regression model. The only difference is that we need to
    add more columns to our dataset before we apply linear regression. For example,
    if we decide to fit a polynomial of degree 3 to the data in figure 3.26, we need
    to add two columns: one corresponding to the square of the feature and one corresponding
    to the cube of the feature. If you’d like to study this in more detail, please
    check out the section “Polynomial regression, testing, and regularization with
    Turi Create” in chapter 4, in which we learn an example of polynomial regression
    in a parabolic dataset.'
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 训练多项式回归模型的过程与训练线性回归模型的过程类似。唯一的区别是我们需要在应用线性回归之前向我们的数据集中添加更多列。例如，如果我们决定将图3.26中的数据拟合到3次多项式，我们需要添加两列：一列对应特征的平方，另一列对应特征的立方。如果您想更详细地研究这个问题，请查看第4章中的“使用Turi
    Create进行多项式回归、测试和正则化”部分，其中我们学习了一个在抛物线数据集中进行多项式回归的例子。
- en: A small caveat with training a polynomial regression model is that we must decide
    the degree of the polynomial before the training process. How do we decide on
    this degree? Do we want a line (degree 1), a parabola (degree 2), a cubic (degree
    3), or some curve of degree 50? This question is important, and we deal with it
    in chapter 4, when we learn overfitting, underfitting, and regularization!
  id: totrans-447
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练多项式回归模型时有一个小的注意事项，那就是我们必须在训练过程之前决定多项式的度数。我们如何决定这个度数？我们想要一条线（度数1）、一个抛物线（度数2）、一个立方（度数3），还是某个50度的曲线？这个问题很重要，我们将在第4章中处理它，当我们学习过拟合、欠拟合和正则化时！
- en: Parameters and hyperparameters
  id: totrans-448
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 参数和超参数
- en: Parameters and hyperparameters are some of the most important concepts in machine
    learning, and in this section, we learn what they are and how to tell them apart.
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: 参数和超参数是机器学习中一些最重要的概念，在本节中，我们将学习它们是什么以及如何区分它们。
- en: As we saw in this chapter, regression models are defined by their weights and
    bias—the *parameters* of the model. However, we can twist many other knobs before
    training a model, such as the learning rate, the number of epochs, the degree
    (if considering a polynomial regression model), and many others. These are called
    *hyperparameters.*
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在本章中看到的，回归模型由其权重和偏差定义——即模型的**参数**。然而，在训练模型之前，我们可以调整许多其他旋钮，例如学习率、迭代次数、度（如果考虑多项式回归模型），以及许多其他。这些被称为**超参数**。
- en: 'Each machine learning model that we learn in this book has some well-defined
    parameters and hyperparameters. They tend to be easily confused, so the rule of
    thumb to tell them apart follows:'
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书中，我们学习的每个机器学习模型都有一些定义良好的参数和超参数。它们往往容易混淆，所以区分它们的经验法则是：
- en: Any quantity that you set *before* the training process is a hyperparameter.
  id: totrans-452
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程之前设置的任何数量都是超参数。
- en: Any quantity that the model creates or modifies *during* the training process
    is a parameter.
  id: totrans-453
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在训练过程中创建或修改的任何数量都是参数。
- en: Applications of regression
  id: totrans-454
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 回归的应用
- en: The impact of machine learning is measured not only by the power of its algorithms
    but also by the breadth of useful applications it has. In this section, we see
    some applications of linear regression in real life. In each of the examples,
    we outline the problem, learn some features to solve it, and then let linear regression
    do its magic.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习的影响不仅在于其算法的强大，还在于其有用应用的广泛性。在本节中，我们将看到线性回归在现实生活中的应用。在每个例子中，我们概述问题，学习一些特征来解决它，然后让线性回归施展其魔法。
- en: Recommendation systems
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐系统
- en: Machine learning is used widely to generate good recommendations in some of
    the most well-known apps, including YouTube, Netflix, Facebook, Spotify, and Amazon.
    Regression plays a key part in most of these recommender systems. Because regression
    predicts a quantity, all we have to do to generate good recommendations is figure
    out what quantity is the best at indicating user interaction or user satisfaction.
    Following are some more specific examples of this.
  id: totrans-457
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习被广泛应用于生成一些最知名应用中的良好推荐，包括YouTube、Netflix、Facebook、Spotify和Amazon。在这些推荐系统中，回归扮演着关键角色。因为回归预测一个数量，我们生成良好推荐所需要做的只是找出哪个数量最能指示用户互动或用户满意度。以下是一些更具体的例子。
- en: Video and music recommendations
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 视频和音乐推荐
- en: One of the ways used to generate video and music recommendations is to predict
    the amount of time a user will watch a video or listen to a song. For this, we
    can create a linear regression model where the labels on the data are the amount
    of minutes that each song is watched by each user. The features can be demographics
    on the user, such as their age, location, and occupation, but they can also be
    behavioral, such as other videos or songs they have clicked on or interacted with.
  id: totrans-459
  prefs: []
  type: TYPE_NORMAL
  zh: 生成视频和音乐推荐的方法之一是预测用户将观看视频或听歌曲的时间长度。为此，我们可以创建一个线性回归模型，其中数据的标签是每个用户观看每首歌曲的分钟数。特征可以是关于用户的统计数据，例如他们的年龄、位置和职业，但它们也可以是行为数据，例如他们点击或与之互动的其他视频或歌曲。
- en: Product recommendations
  id: totrans-460
  prefs: []
  type: TYPE_NORMAL
  zh: 产品推荐
- en: Stores and ecommerce websites also use linear regression to predict their sales.
    One way to do this is to predict how much a customer will spend in the store.
    We can do this using linear regression. The label to predict can be the amount
    the user spent, and the features can be demographic and behavioral, in a similar
    way to the video and music recommendations.
  id: totrans-461
  prefs: []
  type: TYPE_NORMAL
  zh: 商店和电子商务网站也使用线性回归来预测其销售额。一种方法是预测顾客在商店的花费。我们可以使用线性回归来完成这项工作。要预测的标签可以是用户花费的金额，特征可以是人口统计和行为数据，类似于视频和音乐推荐。
- en: Health care
  id: totrans-462
  prefs: []
  type: TYPE_NORMAL
  zh: 医疗保健
- en: 'Regression has numerous applications in health care. Depending on what problem
    we want to solve, predicting the right label is the key. Here are a couple of
    examples:'
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 回归在医疗保健中有许多应用。根据我们想要解决的问题，预测正确的标签是关键。以下是一些例子：
- en: Predicting the life span of a patient, based on their current health conditions
  id: totrans-464
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基于患者的当前健康状况预测其寿命
- en: Predicting the length of a hospital stay, based on current symptoms
  id: totrans-465
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 根据当前症状预测住院时间长度
- en: Summary
  id: totrans-466
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Regression is an important part of machine learning. It consists of training
    an algorithm with labeled data and using it to make predictions on future (unlabeled)
    data.
  id: totrans-467
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归是机器学习的一个重要部分。它包括使用标签数据训练算法，并使用它来对未来（未标记）数据进行预测。
- en: Labeled data is data that comes with labels, which in the regression case, are
    numbers. For example, the numbers could be prices of houses.
  id: totrans-468
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 标签数据是带有标签的数据，在回归的情况下，这些标签是数字。例如，这些数字可以是房价。
- en: In a dataset, the features are the properties that we use to predict the label.
    For example, if we want to predict housing prices, the features are anything that
    describes the house and which could determine the price, such as size, number
    of rooms, school quality, crime rate, age of the house, and distance to the highway.
  id: totrans-469
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在数据集中，特征是我们用来预测标签的性质。例如，如果我们想预测房价，特征就是描述房屋并可能决定价格的一切，例如大小、房间数量、学校质量、犯罪率、房屋年龄以及到高速公路的距离。
- en: The linear regression method for predicting consists in assigning a weight to
    each of the features and adding the corresponding weights multiplied by the features,
    plus a bias.
  id: totrans-470
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用于预测的线性回归方法在于为每个特征分配一个权重，并将相应的权重乘以特征，再加上一个偏差。
- en: Graphically, we can see the linear regression algorithm as trying to pass a
    line as close as possible to a set of points.
  id: totrans-471
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从图形上看，我们可以将线性回归算法视为尝试通过一条线尽可能接近一组点。
- en: The way the linear regression algorithm works is by starting with a random line
    and then slowly moving it closer to each of the points that is misclassified,
    to attempt to classify them correctly.
  id: totrans-472
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 线性回归算法的工作方式是先从一个随机的线开始，然后逐渐将其移动到每个被错误分类的点附近，以尝试正确分类它们。
- en: Polynomial regression is a generalization of linear regression, in which we
    use curves instead of lines to model our data. This is particularly useful when
    our dataset is nonlinear.
  id: totrans-473
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 多项式回归是线性回归的推广，其中我们使用曲线而不是直线来模拟我们的数据。这在我们的数据集是非线性的情况下特别有用。
- en: Regression has numerous applications, including recommendation systems, ecommerce,
    and health care.
  id: totrans-474
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 回归有众多应用，包括推荐系统、电子商务和医疗保健。
- en: Exercises
  id: totrans-475
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 练习
- en: Exercise 3.1
  id: totrans-476
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.1
- en: A website has trained a linear regression model to predict the amount of minutes
    that a user will spend on the site. The formula they have obtained is
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 一个网站已经训练了一个线性回归模型来预测用户将在网站上花费的分钟数。他们获得的公式是
- en: '*t̂* = 0.8*d* + 0.5*m* + 0.5*y* + 0.2*a* + 1.5'
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: '*t̂* = 0.8*d* + 0.5*m* + 0.5*y* + 0.2*a* + 1.5'
- en: 'where *t̂* is the predicted time in minutes, and *d*, *m*, *y*, and *a* are
    indicator variables (namely, they take only the values 0 or 1) defined as follows:'
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 其中 *t̂* 是预测的分钟数，而 *d*，*m*，*y* 和 *a* 是指示变量（即，它们只取0或1的值），定义如下：
- en: '*d* is a variable that indicates if the user is on desktop.'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*d* 是一个变量，表示用户是否在台式电脑上。'
- en: '*m* is a variable that indicates if the user is on mobile device.'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*m* 是一个变量，表示用户是否在移动设备上。'
- en: '*y* is a variable that indicates if the user is young (under 21 years old).'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*y* 是一个变量，表示用户是否年轻（21岁以下）。'
- en: '*a* is a variable that indicates if the user is an adult (21 years old or older).'
  id: totrans-483
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*a* 是一个变量，表示用户是否为成年人（21岁或以上）。'
- en: 'Example: If a user is 30 years old and on a desktop, then *d* = 1, *m* = 0,
    *y* = 0, and *a* = 1.'
  id: totrans-484
  prefs: []
  type: TYPE_NORMAL
  zh: 示例：如果用户30岁且使用台式电脑，则*d* = 1，*m* = 0，*y* = 0，*a* = 1。
- en: If a 45-year-old user looks at the website from their phone, what is the expected
    time they will spend on the site?
  id: totrans-485
  prefs: []
  type: TYPE_NORMAL
  zh: 如果一个45岁的用户从手机上查看网站，他们预计会在网站上花费多少时间？
- en: Exercise 3.2
  id: totrans-486
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.2
- en: Imagine that we trained a linear regression model in a medical dataset. The
    model predicts the expected life span of a patient. To each of the features in
    our dataset, the model would assign a weight.
  id: totrans-487
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们在一个医学数据集上训练了一个线性回归模型。该模型预测患者的预期寿命。对于数据集中的每个特征，模型都会分配一个权重。
- en: 'a) For the following quantities, state if you believe the weight attached to
    this quantity is a positive number, a negative number, or zero. Note: if you believe
    that the weight is a very small number, whether positive or negative, you can
    say zero.'
  id: totrans-488
  prefs: []
  type: TYPE_NORMAL
  zh: a) 对于以下数量，说明你认为附加到该数量的权重是正数、负数还是零。注意：如果你认为权重是一个非常小的数，无论是正数还是负数，你可以说零。
- en: Number of hours of exercise the patient gets per week
  id: totrans-489
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 患者每周锻炼的小时数
- en: Number of cigarettes the patient smokes per week
  id: totrans-490
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 患者每周吸烟的数量
- en: Number of family members with heart problems
  id: totrans-491
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 有心脏问题的家庭成员数量
- en: Number of siblings of the patient
  id: totrans-492
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 患者的兄弟姐妹数量
- en: Whether or not the patient has been hospitalized
  id: totrans-493
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 患者是否曾住院
- en: b) The model also has a bias. Do you think the bias is positive, negative, or
    zero?
  id: totrans-494
  prefs: []
  type: TYPE_NORMAL
  zh: b) 该模型也有偏差。你认为偏差是正数、负数还是零？
- en: Exercise 3.3
  id: totrans-495
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.3
- en: The following is a dataset of houses with sizes (in square feet) and prices
    (in dollars).
  id: totrans-496
  prefs: []
  type: TYPE_NORMAL
  zh: 以下是一个包含房屋尺寸（平方英尺）和价格（美元）的数据集。
- en: '|  | Size (s) | Prize (p) |'
  id: totrans-497
  prefs: []
  type: TYPE_TB
  zh: '|  | 尺寸 (s) | 奖金 (p) |'
- en: '| House 1 | 100 | 200 |'
  id: totrans-498
  prefs: []
  type: TYPE_TB
  zh: '| 房屋 1 | 100 | 200 |'
- en: '| House 2 | 200 | 475 |'
  id: totrans-499
  prefs: []
  type: TYPE_TB
  zh: '| 房屋 2 | 200 | 475 |'
- en: '| House 3 | 200 | 400 |'
  id: totrans-500
  prefs: []
  type: TYPE_TB
  zh: '| 房屋 3 | 200 | 400 |'
- en: '| House 4 | 250 | 520 |'
  id: totrans-501
  prefs: []
  type: TYPE_TB
  zh: '| 房屋 4 | 250 | 520 |'
- en: '| House 5 | 325 | 735 |'
  id: totrans-502
  prefs: []
  type: TYPE_TB
  zh: '| 房屋 5 | 325 | 735 |'
- en: 'Suppose we have trained the model where the prediction for the price of the
    house based on size is the following:'
  id: totrans-503
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们已经训练了模型，其中基于尺寸预测房价如下：
- en: '![](../Images/p_cf.png) = 2*s* + 50'
  id: totrans-504
  prefs: []
  type: TYPE_NORMAL
  zh: '![](../Images/p_cf.png) = 2*s* + 50'
- en: Calculate the predictions that this model makes on the dataset.
  id: totrans-505
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算此模型在数据集上的预测结果。
- en: Calculate the mean absolute error of this model.
  id: totrans-506
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算此模型的平均绝对误差。
- en: Calculate the root mean square error of this model.
  id: totrans-507
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算此模型的均方根误差。
- en: Exercise 3.4
  id: totrans-508
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 3.4
- en: Our goal is to move the line with equation *ŷ* = 2*x* + 3 closer to the point
    (*x*, *y*) = (5, 15) using the tricks we’ve learned in this chapter. For the following
    two problems, use the learning rate *η* = 0.01.
  id: totrans-509
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是使用本章学到的技巧将方程 *ŷ* = 2*x* + 3 的线移动得更接近点 (*x*, *y*) = (5, 15)。对于以下两个问题，使用学习率
    *η* = 0.01。
- en: Apply the absolute trick to modify the line above to be closer to the point.
  id: totrans-510
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将上面的线通过绝对值技巧修改得更接近点。
- en: Apply the square trick to modify the line above to be closer to the point.
  id: totrans-511
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将上面的线通过平方技巧修改得更接近点。
