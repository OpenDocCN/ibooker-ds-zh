- en: 5 Operations management
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 5 运维管理
- en: Jason Quek
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: Jason Quek
- en: This chapter covers
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Using the Unified Cloud interface to manage Kubernetes clusters
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用统一云界面管理Kubernetes集群
- en: Managing Anthos clusters
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 管理Anthos集群
- en: Logging and monitoring
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 日志记录和监控
- en: Anthos deployment patterns
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Anthos部署模式
- en: 'Operations is the act of ensuring your clusters are functioning, active, secure,
    and able to serve the application to the users. To that end, one prevailing school
    of thought has gained adoption and momentum in the cloud era: an operations practice
    known as DevOps.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 运维是指确保你的集群运行正常、活跃、安全，并能向用户提供服务的行为。为此，在云时代，一种主流的思想已经得到采纳并迅速发展：一种称为DevOps的运维实践。
- en: The simplest definition of DevOps is “the combination of developers and IT operations.”
    DevOps aims to address two major points. The first is to enable continuous delivery
    through automated testing, frequent releases, and management of the entire infrastructure
    as code. You can use frameworks such as Terraform or Pulumi to implement this,
    depending on the developer’s skill set. The second, an often overlooked part of
    DevOps, is IT operations, which includes tasks like logging and monitoring and
    then using those indicators to scale and manage the system. You can use open source
    projects such as Prometheus and Grafana to manage these tasks. Teams can further
    improve performance by implementing an additional security tool chain to build
    a modern DevSecOps practice.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps最简单的定义是“开发者和IT运维的结合。”DevOps旨在解决两大要点。第一点是通过对整个基础设施进行代码化管理，通过自动化测试、频繁发布来实现持续交付。你可以根据开发者的技能集使用如Terraform或Pulumi等框架来实现这一点。第二点，DevOps经常被忽视的部分是IT运维，这包括诸如日志记录和监控等任务，然后利用这些指标来扩展和管理系统。你可以使用如Prometheus和Grafana等开源项目来管理这些任务。团队可以通过实施额外的安全工具链来进一步提高性能，从而构建现代的DevSecOps实践。
- en: Before the development of DevOps, Google developed an approach called *site
    reliability engineering (SRE)*. This approach automates and codifies all tasks
    in operating the infrastructure to enhance reliability in the systems, and if
    something goes wrong, it can be repaired automatically through code. An SRE team
    is responsible for not only keeping the environment stable but also for handling
    new operational features and improvements to the infrastructure.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 在DevOps开发之前，谷歌开发了一种称为*站点可靠性工程（SRE）*的方法。这种方法通过自动化和编码化基础设施操作中的所有任务，以增强系统的可靠性，如果出现问题，可以通过代码自动修复。SRE团队不仅负责保持环境稳定，还要处理新的运维功能和基础设施的改进。
- en: 'Both DevOps and SRE have different responsibilities assigned to different teams;
    however, they have the same goal: to be able to implement changes rapidly and
    efficiently, automate where possible, and continuously monitor and enhance the
    system. This commonality underlies the “desire” from the engineering and operations
    teams to break silos (closed teams) and take common responsibility for the system.'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: DevOps和SRE都分配了不同的责任给不同的团队；然而，它们的目标是相同的：能够快速高效地实施变更，尽可能自动化，并持续监控和提升系统。这种共性是工程和运维团队打破壁垒（封闭团队）并共同承担系统责任“愿望”的基础。
- en: Either approach will deliver many of the same advantages, but they solve problems
    in different ways. For example, in a DevOps approach, a dedicated operations team
    may take care of the operations management aspect of the infrastructure, handing
    off problems to another development team to resolve. This differs from the SRE
    approach, where operations are driven by the development team and approached from
    a software engineering point of view, allowing for a single SRE team to address
    problems within their own team.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 任何一种方法都会带来许多相同的优势，但它们以不同的方式解决问题。例如，在DevOps方法中，一个专门的运维团队可能会负责基础设施的运维管理方面，将问题转交给另一个开发团队来解决。这与SRE方法不同，在SRE方法中，运维是由开发团队驱动的，并从软件工程的角度来处理，允许一个SRE团队解决他们自己团队内部的问题。
- en: Anthos provides a path to build a strong DevOps or SRE culture using the tools
    provided in the framework. This chapter will show product owners, developers,
    and infrastructure teams that, through using Anthos, they are able to build a
    DevOps/SRE culture that will help to reduce silos in their company, build reliable
    systems, and enhance development efficiency.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos提供了一条路径，使用框架中提供的工具来构建强大的DevOps或SRE文化。本章将向产品所有者、开发者和基础设施团队展示，通过使用Anthos，他们能够构建一个DevOps/SRE文化，这将有助于减少他们公司中的壁垒，构建可靠的系统，并提高开发效率。
- en: In the next section, we will explain the tools that Anthos includes, starting
    with the unified user interface through Google Cloud console, then centralized
    logging and monitoring, and, finally, environs, which are all key concepts that
    will provide the building blocks to enable an operations practice.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将解释 Anthos 包含的工具，从 Google Cloud 控制台中的统一用户界面开始，然后是集中式日志记录和监控，最后是环境，这些都是提供构建块以实现操作实践的关键概念。
- en: 5.1 Unified user interface from Google Cloud console
  id: totrans-14
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.1 Google Cloud 控制台的统一用户界面
- en: With everything-as-code these days, a software engineer takes pride in doing
    everything from the command line or as code. However, when a production problem
    occurs affecting real-life services, an intuitive and assistive user interface
    can help an engineer identify the problem quickly. This is where Google’s unified
    user interface comes in handy, as shown in figure 5.1.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 在当今一切皆代码的时代，软件工程师以从命令行或作为代码执行所有操作为荣。然而，当出现影响现实服务的生产问题时，直观且辅助的用户界面可以帮助工程师快速识别问题。这正是
    Google 统一用户界面发挥作用的地方，如图 5.1 所示。
- en: '![05-01](../../OEBPS/Images/05-01.png)'
  id: totrans-16
  prefs: []
  type: TYPE_IMG
  zh: '![05-01](../../OEBPS/Images/05-01.png)'
- en: Figure 5.1 Multiple clusters registered to Google Cloud console
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.1 已注册到 Google Cloud 控制台的多集群
- en: These tools allow you to view multiple items, like Kubernetes clusters, in a
    single view. Having this view available to an administrator gives them oversight
    of all the resources available, without having to log in to three separate clusters,
    as shown in figure 5.1\. This view also shows where resources are located, who
    their providers are, and any actions required to manage the clusters.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 这些工具允许您在一个视图中查看多个项目，如 Kubernetes 集群。将此视图提供给管理员，使他们能够对所有可用资源进行监督，而无需登录到三个不同的集群，如图
    5.1 所示。此视图还显示了资源的位置、其提供者以及管理集群所需的任何操作。
- en: 'Accessing this view requires that the user is already logged in to Google Cloud
    console, which is secured by Google Cloud Identity, providing an additional layer
    of security to build defense against malicious actors. Having access to this type
    of view fulfills one of the DevOps principles: using tooling to provide observability
    into the system.'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 访问此视图需要用户已经登录到 Google Cloud 控制台，该控制台由 Google Cloud Identity 保护，为防御恶意行为者提供了额外的安全层。拥有此类视图的访问权限满足了
    DevOps 原则之一：使用工具来提供对系统的可观察性。
- en: To have a single-pane-of-glass view, you need to register your clusters with
    your GCP project. In the next section, we will cover how to register a cluster
    that is running Anthos on any of the major cloud service providers or on-prem.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得单视图，您需要将您的集群注册到您的 GCP 项目中。在下一节中，我们将介绍如何注册在任意主要云服务提供商或本地运行的 Anthos 上的集群。
- en: 5.1.1 Registering clusters to Google Cloud console
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.1 将集群注册到 Google Cloud 控制台
- en: The component responsible for connecting clusters to Google Cloud console is
    called Connect and is often deployed as one of the last steps after a cluster
    is created. If a cluster is deployed by Anthos on GKE, AWS, or Azure, the Connect
    Agent is automatically deployed at the time of cluster creation. However, if the
    cluster is not deployed by Anthos—such as EKS, AKS, OpenShift, and Rancher clusters—the
    agent will have to be deployed separately, because Anthos is not involved in the
    installation process. This process will be covered later in this chapter.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 负责将集群连接到 Google Cloud 控制台的组件称为 Connect，通常在集群创建后作为最后一步之一部署。如果集群由 Anthos 在 GKE、AWS
    或 Azure 上部署，Connect 代理将在集群创建时自动部署。然而，如果集群不是由 Anthos 部署的（例如 EKS、AKS、OpenShift 和
    Rancher 集群），代理将需要单独部署，因为 Anthos 不参与安装过程。此过程将在本章后面进行说明。
- en: Because Anthos is built following best practices from Kubernetes, the Connect
    Agent is represented as a Kubernetes deployment, with the image provided by Google
    as part of the Anthos framework. The agent can also be seen in the Google Cloud
    console and can be managed like any other Kubernetes object, as shown in figure
    5.2.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 由于 Anthos 是遵循 Kubernetes 最佳实践构建的，Connect 代理被表示为 Kubernetes 部署，该镜像由 Google 作为
    Anthos 框架的一部分提供。代理也可以在 Google Cloud 控制台中看到，可以像任何其他 Kubernetes 对象一样进行管理，如图 5.2
    所示。
- en: '![05-02](../../OEBPS/Images/05-02.png)'
  id: totrans-24
  prefs: []
  type: TYPE_IMG
  zh: '![05-02](../../OEBPS/Images/05-02.png)'
- en: Figure 5.2 A Connect Agent deployed on a GKE cluster
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.2 部署在 GKE 集群上的 Connect 代理
- en: 'The Connect Agent acts as a conduit for Google Cloud console to issue commands
    to the clusters in which it has been deployed and to report vCPU usage for licensing.
    This brings up one important point: the clusters need to be able to reach Google
    Cloud APIs (egress); however, the clusters do not need to be reachable by Google
    Cloud APIs (ingress). The impact on latency is minimal due to the unidirectional
    tunnel initialized after the first connection.'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: Connect Agent充当Google Cloud控制台向其已部署的集群发出命令并报告vCPU使用情况的渠道，以进行许可。这引出了一个重要观点：集群需要能够访问Google
    Cloud API（egress）；然而，集群不需要通过Google Cloud API（ingress）可访问。由于在第一次连接后初始化的单向隧道，对延迟的影响最小。
- en: So, how does Google Cloud console issue Kubernetes API commands, such as listing
    Pods to display on the Google Cloud console? The answer is through the Connect
    Agent, which establishes a persistent TLS 1.2 connection to GCP to wait for requests,
    eliminating the need for having an inbound firewall rule for the user cluster.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: 那么，Google Cloud控制台是如何发出Kubernetes API命令的，例如列出Pod以在Google Cloud控制台上显示？答案是通过对Connect
    Agent建立一个持久的TLS 1.2连接到GCP以等待请求，从而消除为用户集群设置入站防火墙规则的需要。
- en: Transport Layer Security (TLS) is a cryptographic protocol designed to provide
    privacy and data integrity between the sender and receiver. It uses symmetric
    encryption based on a shared secret to ensure that the message is private. Messages
    are signed with a public key to ensure authenticity and include a message-integrity
    check to make sure messages are complete. In short, the communication channel
    to the Connect Agent over the internet is as secure as internet bank transfers.
    The full communication flow can be seen in chapter 2, figure 2.2.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 传输层安全性（TLS）是一种加密协议，旨在在发送方和接收方之间提供隐私和数据完整性。它使用基于共享秘密的对称加密来确保消息是私密的。消息使用公钥签名以确保真实性，并包含消息完整性检查以确保消息完整。简而言之，通过互联网到Connect
    Agent的通信通道与互联网银行转账一样安全。完整的通信流程可以在第2章图2.2中查看。
- en: One important point to note is that the outbound TLS-encrypted connection over
    the internet is used for Anthos deployments to communicate with Google Cloud,
    as shown in figure 5.3\. This setup simplifies things, because no inbound firewall
    rules have to be added to Anthos deployments—only outbound traffic to Google Cloud—without
    any virtual private networks (VPN) required.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 需要注意的一个重要观点是，如图5.3所示，用于Anthos部署与Google Cloud通信的互联网上的出站TLS加密连接。这种设置简化了事情，因为不需要为Anthos部署添加入站防火墙规则——只需向Google
    Cloud发送出站流量——而且不需要任何虚拟专用网络（VPN）。
- en: '![05-03](../../OEBPS/Images/05-03.png)'
  id: totrans-30
  prefs: []
  type: TYPE_IMG
  zh: '![05-03](../../OEBPS/Images/05-03.png)'
- en: Figure 5.3 The outbound connection to Google Cloud from Anthos deployments
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.3 Anthos部署到Google Cloud的出站连接
- en: One great thing about Kubernetes is its standardization, which means this agent
    will be able to issue Kubernetes API commands on clusters created by Google or
    *any* provider that provides a Kubernetes-compliant distribution as defined by
    the Cloud Native Computing Foundation.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes的一个优点是其标准化，这意味着这个代理将能够向由Google或**任何**提供符合云原生计算基金会定义的Kubernetes兼容分布的提供商创建的集群发出Kubernetes
    API命令。
- en: In large enterprises, IT security usually wants to know what the Connect Agent
    is sending to Google Cloud APIs, and this is a tricky problem to overcome, due
    to the perceived worry that if Google shares the keys to decrypt the traffic,
    it is effectively overriding the security put into place. More details of what
    information is actually sent from the Connect Agent to Google Cloud can be found
    in this white paper by Google ([http://mng.bz/rdAZ](https://cloud.google.com/files/security-features-for-connect-for-anthos.pdf)).
    Google has also stated unequivocally that no customer data is sent via the Connect
    Agent and that it is used only to provide functionality to communicate with the
    Kubernetes API and also provide licensing metrics for Anthos billing.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在大型企业中，IT安全通常想知道Connect Agent向Google Cloud API发送了什么，这是一个棘手的问题，因为人们担心如果Google共享解密流量的密钥，它实际上是在覆盖已经实施的安全措施。有关Connect
    Agent实际向Google Cloud发送的信息的更多细节，可以在Google的这篇白皮书中找到（[http://mng.bz/rdAZ](https://cloud.google.com/files/security-features-for-connect-for-anthos.pdf)）。Google还明确表示，没有客户数据通过Connect
    Agent发送，并且它仅用于提供与Kubernetes API通信的功能，并为Anthos计费提供许可度量。
- en: 5.1.2 Authentication
  id: totrans-34
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.2 认证
- en: Authentication to Google Kubernetes Engine should use Identity and Access Management
    (IAM) roles to govern access to the GKE clusters. The following section pertains
    to GKE on-prem, GKE on AWS, GKE on Azure, and Anthos attached clusters.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 访问 Google Kubernetes Engine 应该使用身份和访问管理（IAM）角色来管理对 GKE 集群的访问。以下部分涉及 GKE 本地、GKE
    on AWS、GKE on Azure 以及与 Anthos 连接的集群。
- en: To access Anthos clusters, users with access to the project will always have
    to provide either a Kubernetes Service Account (KSA) token or basic authentication
    or authenticate against an identity provider configured for the cluster.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 要访问 Anthos 集群，拥有项目访问权限的用户始终必须提供 Kubernetes 服务账户（KSA）令牌、基本身份验证或针对集群配置的身份提供者进行身份验证。
- en: Using a KSA token would be the easiest to set up, but it requires token rotation
    and a secure way to distribute tokens regularly to the users who need access to
    the clusters. Using basic authentication would be the least secure due to having
    password management requirements, but it is still supported as an authentication
    method if an identity provider is not available. If you must use basic authentication,
    one tip would be to implement a password-rotation strategy in the event of password
    leaks.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 KSA 令牌将是最容易设置的，但它需要令牌轮换和一种安全的方式定期向需要访问集群的用户分发令牌。使用基本身份验证将是最不安全的，因为需要密码管理要求，但如果没有可用的身份提供者，它仍然作为身份验证方法得到支持。如果您必须使用基本身份验证，一个建议是在密码泄露事件中实施密码轮换策略。
- en: The recommended practice would be to set up OpenID Connect (OIDC) with Google
    Cloud Identity so that users can benefit from their existing security setup to
    manage access to their clusters as well. As of September 2020, OIDC is supported
    on GKE on-prem clusters from the command line (not from the console). A solid
    KSA token rotation and distribution strategy is highly recommended. This can be
    as simple as utilizing Google Secret Manager, where permissions to retrieve the
    token can be controlled via IAM permissions, and the token can be updated every
    seven days using Cloud Scheduler.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 推荐的做法是设置 Google Cloud Identity（OIDC）以与 Google Cloud Identity 配合使用，这样用户就可以利用现有的安全设置来管理对他们的集群的访问。截至
    2020 年 9 月，OIDC 在 GKE 本地集群中通过命令行（而不是控制台）支持。强烈建议采用稳固的 KSA 令牌轮换和分发策略。这可以像利用 Google
    Secret Manager 那样简单，其中可以通过 IAM 权限控制检索令牌的权限，并且可以使用 Cloud Scheduler 每七天更新一次令牌。
- en: Once OIDC with Google Cloud Identity has been set up, users can authenticate
    the user clusters using the gcloud CLI or from the Google Cloud console, as shown
    in figure 5.4.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦设置了 OIDC 与 Google Cloud Identity，用户可以使用 gcloud CLI 或 Google Cloud 控制台（如图 5.4
    所示）对用户集群进行身份验证。
- en: '![05-04](../../OEBPS/Images/05-04.png)'
  id: totrans-40
  prefs: []
  type: TYPE_IMG
  zh: '![05-04](../../OEBPS/Images/05-04.png)'
- en: Figure 5.4 Authentication flow for OIDC with Google Cloud Identity
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.4 使用 Google Cloud Identity 的 OIDC 身份验证流程
- en: In figure 5.4, we show the identity flow using OIDC with Google Cloud Identity.
    With Anthos Identity Service, other providers that follow the OIDC and Lightweight
    Directory Access Protocol (LDAP) protocols can provide identity. This process
    allows a seamless user administration with technologies such as Microsoft Active
    Directory or an LDAP server and follows the principle of having one single source
    of truth of identity.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在图 5.4 中，我们展示了使用 Google Cloud Identity 的 OIDC 身份流。通过 Anthos Identity Service，遵循
    OIDC 和轻量级目录访问协议（LDAP）协议的其他提供者可以提供身份。这个过程允许使用 Microsoft Active Directory 或 LDAP
    服务器等技术实现无缝的用户管理，并遵循只有一个单一的真实身份来源的原则。
- en: 5.1.3 Cluster management
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.3 集群管理
- en: After registering the clusters and authenticating, users will be able to see
    Pods, Services, ConfigMaps, and persistent volumes, which are normally available
    from GKE native clusters. In this section, the cluster management options available
    via the Google Cloud console will be covered. However, to build a good SRE practice,
    cluster management should be automated and scripted. It is nice, however, to be
    able to modify these from a user-friendly interface.
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 在注册集群和身份验证后，用户将能够看到 Pods、Services、ConfigMaps 和持久卷，这些通常可以从 GKE 原生集群中获取。在本节中，将介绍通过
    Google Cloud 控制台可用的集群管理选项。然而，为了构建良好的 SRE 实践，集群管理应该是自动化和脚本化的。然而，能够从用户友好的界面进行修改也是很不错的。
- en: Administrators who have experience in Google Kubernetes Engine on GCP know how
    easy it is to connect to the cluster from the Google Cloud console. They just
    navigate to the cluster list, as seen in figure 5.5, and click the Connect button.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 在 GCP 上有 Google Kubernetes Engine 经验的管理员知道从 Google Cloud 控制台连接到集群是多么容易。他们只需导航到集群列表，如图
    5.5 所示，然后点击连接按钮。
- en: '![05-05](../../OEBPS/Images/05-05.png)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![05-05](../../OEBPS/Images/05-05.png)'
- en: Figure 5.5 Cluster list in the GCP console
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.5 GCP 控制台中的集群列表
- en: Once they click Connect, a pop-up window, as shown in figure 5.6, provides the
    command to run in a Google Cloud Shell to connect to the selected cluster.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦点击连接，如图 5.6 所示的弹出窗口将提供在 Google Cloud Shell 中运行的命令，以连接到所选集群。
- en: '![05-06](../../OEBPS/Images/05-06.png)'
  id: totrans-49
  prefs: []
  type: TYPE_IMG
  zh: '![05-06](../../OEBPS/Images/05-06.png)'
- en: Figure 5.6 One of the best features in GKE—generating kubectl credentials via
    gcloud
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.6 GKE 中最好的功能之一——通过 gcloud 生成 kubectl 凭据
- en: For on-prem and other cloud clusters, the Connect gateway functionality, discussed
    later in this chapter, allows operations administrators to manage their clusters
    remotely but through a different command.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 对于本地和其它云集群，本章后面将讨论的 Connect 网关功能允许操作管理员通过不同的命令远程管理他们的集群。
- en: Google Cloud console provides a user-friendly interface to edit and apply YAML
    deployments, as shown in figure 5.7\. Through this interface, administrators can
    modify Kubernetes configurations without having to go through the kubectl command
    line, which can save some time in emergency situations. These actions on Google
    Cloud console translate to Kubernetes API calls, or kubectl edit commands, and
    are issued via the Connect Agent to the Anthos clusters. Of course, this method
    should be used only in triage or development situations, not necessarily in production,
    but it shows the future possibilities of opening up access to the Connect Agent
    from the local command line.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.7 所示，Google Cloud 控制台提供了一个用户友好的界面来编辑和应用 YAML 部署。通过这个界面，管理员可以在不通过 kubectl
    命令行的情况下修改 Kubernetes 配置，这在紧急情况下可以节省一些时间。这些在 Google Cloud 控制台上的操作转换为 Kubernetes
    API 调用或 kubectl edit 命令，并通过 Connect Agent 发送到 Anthos 集群。当然，这种方法应仅用于分类或开发情况，不一定适用于生产，但它展示了从本地命令行打开对
    Connect Agent 访问的未来可能性。
- en: '![05-07](../../OEBPS/Images/05-07.png)'
  id: totrans-53
  prefs: []
  type: TYPE_IMG
  zh: '![05-07](../../OEBPS/Images/05-07.png)'
- en: Figure 5.7 Editing a YAML definition from the Google Cloud console
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.7 从 Google Cloud 控制台编辑 YAML 定义
- en: Google Cloud console also provides useful information about the underlying Docker,
    kubelet, and memory pressure for the nodes, as seen in figure 5.8\. Using this,
    administrators can run a quick root cause analysis if a fault occurs with one
    of the nodes, and they can cordon off and drain the node.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 如图 5.8 所示，Google Cloud 控制台还提供了有关节点底层 Docker、kubelet 和内存压力的有用信息。利用这些信息，管理员可以在节点发生故障时快速进行根本原因分析，并且可以隔离和排空节点。
- en: '![05-08](../../OEBPS/Images/05-08.png)'
  id: totrans-56
  prefs: []
  type: TYPE_IMG
  zh: '![05-08](../../OEBPS/Images/05-08.png)'
- en: Figure 5.8 Node information from Google Cloud console
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.8 来自 Google Cloud 控制台的节点信息
- en: When listing the workloads in Google Cloud console, a user can see deployments
    across all clusters and filter them by cluster. This ability provides an overview
    of what services are running across all clusters and indicates if problems arise
    with any services and scaling limits. A common problem is that Pods cannot be
    provisioned due to lack of CPU or memory. This is clearly visible as a bright
    red error message in the console, as seen in figure 5.9.
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Google Cloud 控制台中列出工作负载时，用户可以看到所有集群的部署，并按集群进行筛选。这种能力提供了对所有集群中运行的服务概述，并指示是否有任何服务出现问题或达到扩展限制。一个常见问题是由于
    CPU 或内存不足而无法部署 Pods。这可以在控制台中清晰地看到，如图 5.9 所示的明亮的红色错误信息。
- en: '![05-09](../../OEBPS/Images/05-09.png)'
  id: totrans-59
  prefs: []
  type: TYPE_IMG
  zh: '![05-09](../../OEBPS/Images/05-09.png)'
- en: Figure 5.9 Unschedulable Pods error
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.9 无法调度的 Pods 错误
- en: Viewing a cluster interactively with tools is beneficial for real-time views
    of object states, node statuses, and more. Although this tool can be helpful in
    the right scenario (e.g., when diagnosing a previously unknown problem that impacts
    production and a user-friendly interface reduces the need to remember commands
    in a high-stress situation), you will find yourself looking at logs and creating
    monitoring events more often than real-time views. In the next section, we will
    detail the logging and monitoring features that Anthos includes.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 使用工具交互式地查看集群对于实时查看对象状态、节点状态等非常有用。尽管这个工具在适当的场景下（例如，在诊断影响生产的未知问题，且用户友好的界面减少了在高压情况下记住命令的需求）可能很有帮助，但你可能会发现自己更频繁地查看日志和创建监控事件，而不是实时查看。在下一节中，我们将详细介绍
    Anthos 包含的日志和监控功能。
- en: 5.1.4 Logging and monitoring
  id: totrans-62
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.4 日志和监控
- en: Kubernetes offers different kinds of logs, which are useful for administrators
    to investigate when managing a cluster. One type is system logs, to which Kubernetes
    system services such as kube-apiserver, etcd, kube-scheduler and kube-controller-manager
    log. Clusters also have application logs, which contain log details for all the
    workloads running on the Kubernetes cluster. These logs can be accessed through
    the Connect Agent, which communicates with the Kubernetes API and essentially
    issues a kubectl logs command.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: Kubernetes提供了不同类型的日志，这对于管理员在管理集群时进行调查非常有用。一种类型是系统日志，Kubernetes系统服务如kube-apiserver、etcd、kube-scheduler和kube-controller-manager都会记录这些日志。集群还有应用程序日志，其中包含在Kubernetes集群上运行的所有工作负载的日志详情。这些日志可以通过Connect
    Agent访问，该Agent与Kubernetes API通信，并基本上发出一个kubectl logs命令。
- en: Both of these log types, shown in figure 5.10, are not stored in the cloud but
    are retrieved on demand from the Kubernetes API, which translates to an increased
    retrieval latency but is at times necessary in case of IT security requests.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 这两种日志类型，如图5.10所示，不是存储在云端，而是根据需求从Kubernetes API中检索，这导致检索延迟增加，但在IT安全请求的情况下有时是必要的。
- en: '![05-10](../../OEBPS/Images/05-10.png)'
  id: totrans-65
  prefs: []
  type: TYPE_IMG
  zh: '![05-10](../../OEBPS/Images/05-10.png)'
- en: Figure 5.10 Container logs
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.10 容器日志
- en: Logs are primarily about errors—warnings that are output to the standard output
    stream during the execution of any Kubernetes Pod. These logs are written to the
    node itself, and if the Google Cloud operations suite (formerly Stackdriver) agent
    is enabled on the GKE cluster, the logs are aggregated and forwarded to the Cloud
    Logging API and written to the cloud.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 日志主要关于错误——在执行任何Kubernetes Pod期间输出到标准输出流的警告。这些日志被写入节点本身，如果GKE集群上启用了Google Cloud操作套件（以前称为Stackdriver）代理，则日志会被聚合并转发到云日志API并写入云端。
- en: Metrics are observations about a service, such as memory consumption or requests
    per second. These observations are saved as a historical trend, which can be used
    to scale services or identify possible problems in implementation. Given that
    each service can potentially have tens of observations occurring every second
    or minute, depending on the business requirements, managing this data in a usable
    manner is nontrivial. We propose a couple of solutions in the next subsection
    involving Google’s Cloud Logging and Monitoring services. You can also use partner
    technology such as Elastic Stack, Prometheus, Grafana, or Splunk to make sense
    of the metrics. See [http://mng.bz/VpmO](http://mng.bz/VpmO) or [http://mng.bz/xdAY](http://mng.bz/xdAY)
    for more information.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 指标是关于服务的观察，例如内存消耗或每秒请求数。这些观察结果被保存为历史趋势，可用于扩展服务或识别实施中可能存在的问题。鉴于每个服务每秒或每分钟可能都有成十上百万的观察结果，这取决于业务需求，以可用的方式管理这些数据并非易事。我们将在下一小节中提出一些解决方案，涉及Google的云日志和监控服务。您还可以使用合作伙伴技术，如Elastic
    Stack、Prometheus、Grafana或Splunk来理解指标。更多信息请参阅[http://mng.bz/VpmO](http://mng.bz/VpmO)或[http://mng.bz/xdAY](http://mng.bz/xdAY)。
- en: Logging and monitoring GKE on-prem
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 在本地部署GKE的日志和监控
- en: Administrators can choose between a few different options for observability
    when installing GKE on on-prem clusters. The first option is to use Google’s native
    Cloud Logging and Cloud Monitoring solutions. Cloud Logging and Cloud Monitoring
    handle infrastructure and cloud services, as well as Kubernetes logging and monitoring.
    All logged data can be displayed in hierarchical levels according to the Kubernetes
    object types. By default, GKE logging collects logs and metrics only from the
    kube-system, gke-system, gke-connect, istio-system, and config-management system
    namespaces, which are used to track cluster health and are sent to Cloud Logging
    and Cloud Monitoring in Google Cloud. This service is fully managed and includes
    dashboarding and alerting capabilities to build a useful monitoring control panel.
    Cloud Logging and Cloud Monitoring are often used to monitor Google Cloud resources
    and issue alerts on certain logged events and also serve as a single pane of glass
    for monitoring service health. This is the recommended option if an organization
    is open to using and learning a new logging and monitoring stack and wants a low-cost
    and fully managed option.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 当在本地集群上安装GKE时，管理员可以在几个不同的可观察性选项之间进行选择。第一种选择是使用Google的本地云日志和云监控解决方案。云日志和云监控处理基础设施和云服务，以及Kubernetes日志和监控。所有日志数据都可以根据Kubernetes对象类型以分层级别显示。默认情况下，GKE日志仅从kube-system、gke-system、gke-connect、istio-system和配置管理系统命名空间收集日志和指标，这些命名空间用于跟踪集群健康，并将数据发送到Google
    Cloud的云日志和云监控。这项服务是完全管理的，包括仪表板和警报功能，以构建一个有用的监控控制面板。云日志和云监控通常用于监控Google Cloud资源，并对某些日志事件发出警报，同时也作为监控服务健康状况的单个视图。如果组织愿意使用和学习新的日志和监控堆栈，并希望有一个低成本且完全管理的选项，这是一个推荐的选择。
- en: Certain organizations may want to disable Cloud Logging and Cloud Monitoring
    due to internal decisions. Although they can be disabled, the Google Support SLA
    will be voided and Google support will be able to help only as a best effort when
    resolving GKE on-prem operation problems.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 某些组织可能由于内部决策而希望禁用云日志和云监控。尽管可以禁用，但Google支持服务级别协议（SLA）将失效，并且Google支持在解决GKE本地操作问题时只能尽力而为。
- en: The second option is to use Prometheus, Alertmanager, and Grafana, a popular
    open source collection of projects, to collect application and system-level logs,
    and provide alerting and dashboarding capabilities. Prometheus and Grafana are
    deployed as Kubernetes monitoring add-on workloads and, as such, benefit from
    the scalability and reliability of running on Kubernetes. When using this solution,
    support from Google is limited to basic operations and basic installation and
    configuration. For more information on Prometheus and Alertmanager, visit [https://prometheus.io](https://prometheus.io),
    and for Grafana, please visit [https://grafana.com](https://grafana.com).
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 第二种选择是使用Prometheus、Alertmanager和Grafana，这是一个流行的开源项目集合，用于收集应用程序和系统级别的日志，并提供警报和仪表板功能。Prometheus和Grafana作为Kubernetes监控附加工作负载部署，因此能够享受到在Kubernetes上运行的扩展性和可靠性。当使用此解决方案时，Google的支持仅限于基本操作、基本安装和配置。有关Prometheus和Alertmanager的更多信息，请访问[https://prometheus.io](https://prometheus.io)，有关Grafana，请访问[https://grafana.com](https://grafana.com)。
- en: This option can be used across any Kubernetes setup, and many prebuilt Grafana
    packages can be used to monitor Kubernetes cluster health. One downside is that
    administrators would have to manage Prometheus, ensure its health, and manage
    its storage of historical metrics as it is running, as with any other application
    workload. Other tools such as Thanos can be used to query, aggregate, downsample,
    and manage multiple Prometheus sources, as well as store historical metrics in
    object storage such as Google Cloud Storage or any S3-compatible object stores.
    For more information on Thanos, please visit [https://thanos.io/](https://thanos.io/).
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 此选项可以用于任何Kubernetes配置，并且可以使用许多预构建的Grafana包来监控Kubernetes集群的健康状况。一个缺点是，管理员必须管理Prometheus，确保其健康，并管理其运行时的历史指标存储，就像管理任何其他应用程序工作负载一样。可以使用其他工具，如Thanos，来查询、聚合、降采样和管理多个Prometheus源，以及将历史指标存储在对象存储中，如Google
    Cloud Storage或任何兼容S3的对象存储。有关Thanos的更多信息，请访问[https://thanos.io/](https://thanos.io/)。
- en: This option is easy for organizations that have built logging and monitoring
    services using open source technologies and have deployed this stack before. It
    also improves portability and reduces vendor lock-in due to the open source technologies
    used.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 对于已经使用开源技术构建了日志和监控服务并且之前已经部署过此堆栈的组织来说，这个选项很简单。它还提高了可移植性，并减少了由于使用开源技术而导致的供应商锁定。
- en: The third option is to use validated solutions, such as Elastic Stack, Splunk,
    or Datadog, to consume logs and metrics from Anthos clusters and make them available
    to the operations team. This option is attractive if these current logging methods
    are already in place and the organization relies on partners to manage the logging
    and monitoring systems’ uptime. Organizations that choose this option often have
    already purchased this stack and use it for their overall operations with many
    heterogeneous systems.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 第三种选择是使用经过验证的解决方案，如Elastic Stack、Splunk或Datadog，从Anthos集群中消费日志和指标，并将其提供给运维团队。如果当前日志方法已经到位，并且组织依赖合作伙伴来管理日志和监控系统的高可用性，那么这个选项很有吸引力。选择这个选项的组织通常已经购买了此堆栈，并使用它来处理其整体操作中的许多异构系统。
- en: A fourth option is also a tiered telemetry approach, which is recommended for
    organizations embarking on a hybrid journey with Anthos. Multiple reasons exist
    for this approach, the first being that platform and system data from Anthos clusters
    is always tightly coupled with Cloud Monitoring and Cloud Logging, so administrators
    would have to learn Cloud Monitoring and Cloud Logging to get the most up-to-date
    logs and metrics anyway. In addition, it does not have any extra costs and is
    part of the Anthos suite. The second reason is that building a hybrid environment
    often requires migrating applications to the hybrid environment, with developers
    who are used to working with these partner solutions and have built debugging
    and operating models around that stack. This makes it a supported option that
    reduces the operational friction of moving workloads to a hybrid environment.
    The third reason is to build the ability to balance points of failure among different
    providers and have a backup option.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 第四种选择也是一种分层遥测方法，对于开始使用Anthos进行混合之旅的组织来说，这是一个推荐的方法。这种方法的理由有很多，首先是因为Anthos集群的平台和系统数据始终与Cloud
    Monitoring和Cloud Logging紧密耦合，因此管理员无论如何都需要学习Cloud Monitoring和Cloud Logging以获取最新的日志和指标。此外，它没有任何额外成本，并且是Anthos套件的一部分。第二个原因是，构建混合环境通常需要将应用程序迁移到混合环境，开发者习惯于与这些合作伙伴解决方案一起工作，并围绕该堆栈构建调试和运营模式。这使得它成为一个受支持的选项，可以减少将工作负载迁移到混合环境时的运营摩擦。第三个原因是建立在不同提供商之间平衡故障点的能力，并有一个备份选项。
- en: 5.1.5 Service Mesh logging
  id: totrans-77
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.5 服务网格日志
- en: Anthos Service Mesh is an optional component but included in the Anthos platform,
    as explained in depth in chapter 4\. It is an extended and supported version of
    open source Istio, included with Anthos and supported by Google. Part of what
    Google extended is the ability to upload telemetry data from sidecar proxies injected
    with your Pods directly to the Cloud Monitoring API and Cloud Logging API on Google
    Cloud. These metrics are then used to visualize preconfigured dashboards in the
    Google Cloud console. For more details, please refer to chapter 3.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 如第4章所述，Anthos Service Mesh是一个可选组件，但包含在Anthos平台中。它是开源Istio的扩展和受支持版本，包含在Anthos中并由Google支持。Google扩展的部分能力是将注入您的Pod的边车代理的遥测数据直接上传到Google
    Cloud的Cloud Monitoring API和Cloud Logging API。然后，这些指标用于在Google Cloud控制台中可视化预配置的仪表板。有关更多详细信息，请参阅第3章。
- en: Storing these metrics on Google Cloud also allows you to have historical information
    on latency, errors, and traffic between microservices, so that you can conduct
    a postmortem on any problems. You can further use these metrics to drive your
    service-level indicators and Pod-scaling strategy and identify services for optimization.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 将这些指标存储在Google Cloud上还允许您拥有关于微服务之间延迟、错误和流量的历史信息，这样您就可以对任何问题进行事后分析。您还可以进一步使用这些指标来驱动服务级别指标和Pod扩展策略，并识别需要优化的服务。
- en: 5.1.6 Using service-level indicators and agreements
  id: totrans-80
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.1.6 使用服务级别指标和协议
- en: Anthos Service Mesh service-level indicators (SLIs), service-level objectives
    (SLOs), and service-level agreements (SLAs) are features that you can use to build
    an SRE practice where Anthos is deployed. It is necessary to consider these concepts
    when designing operations management procedures in Anthos.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 服务网格的服务级别指标（SLIs）、服务级别目标（SLOs）和服务级别协议（SLAs）是您可以使用来构建 Anthos 部署的 SRE 实践的功能。在设计
    Anthos 的运营管理流程时，考虑这些概念是必要的。
- en: 'Two indicators measure service levels: latency and availability. Latency is
    how long the service takes to respond, whereas availability represents how often
    the service responds. When the system is designed from a DevOps view, administrators
    must consider Anthos upgrade and scaling needs and plan accordingly so they do
    not affect these indicators.'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 两个指标衡量服务级别：延迟和可用性。延迟是服务响应所需的时间，而可用性表示服务响应的频率。当系统从 DevOps 视角进行设计时，管理员必须考虑 Anthos
    升级和扩展需求，并相应地规划，以确保它们不影响这些指标。
- en: For service-level objectives, you should think from the angle of the worst-case
    scenario, and not the best-case scenario, making that decision as data driven
    as possible. For example, if the latency is unrealistic and does not affect the
    user experience, there will be no way to even release the service. Find the highest
    latency acceptable according to the user experience and then work on reducing
    that based on business needs. Educate your business stakeholders that a target
    approaching 99.99999% availability is very expensive to attain and that a practical
    trade-off often must be agreed on. An important concept mentioned in the SRE book
    by Google is to strive to make a service reliable enough but no more than it has
    to be. You can find more information on the SRE book by Google at [http://mng.bz/Al77](http://mng.bz/Al77).
    Understanding the procedures and risks of Anthos upgrades, rollbacks, and security
    updates is essential input to determine whether a service-level objective is realistic.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 对于服务级别目标，您应该从最坏情况的角度考虑，而不是最佳情况，尽可能使决策数据驱动。例如，如果延迟不切实际且不影响用户体验，甚至无法发布服务。根据用户体验找到可接受的最高延迟，然后根据业务需求进行降低。教育您的业务利益相关者，接近
    99.99999% 的可用性目标非常昂贵，并且通常必须达成一个实际的权衡。谷歌 SRE 书籍中提到的一个重要概念是，努力使服务足够可靠，但不超过必须的程度。您可以在
    [http://mng.bz/Al77](http://mng.bz/Al77) 上找到更多关于谷歌 SRE 书籍的信息。了解 Anthos 升级、回滚和安全更新的程序和风险对于确定服务级别目标是否现实是至关重要的。
- en: You should also define a compliance period for the service-level objective to
    be measured against. The set SLO can be any period of measurement—a day, a week,
    or a month. This allows for the teams responsible for the service to decide when
    it is time to roll back, make a hotfix, or slow down development to prioritize
    fixing bugs. The SLI and SLO also empower product owners to propose service-level
    agreements with users that require them and offer a realistic latency and availability
    agreement.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 您还应该为要衡量的服务级别目标定义一个合规期。设置的 SLO 可以是任何测量周期——一天、一周或一个月。这允许负责服务的团队决定何时回滚、发布热补丁或放慢开发速度以优先修复错误。SLI
    和 SLO 还使产品所有者能够向需要它们的用户提供服务级别协议，并提供现实的延迟和可用性协议。
- en: 5.2 Anthos command-line management
  id: totrans-85
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.2 Anthos 命令行管理
- en: You can use various command-line tools to deal with cluster creation, scaling,
    and upgrading of Anthos versions, such as gkectl, gkeadmin and anthos-gke. This
    chapter is not meant to replace the documentation on Google Cloud, but it summarizes
    the actions and some of the gotchas to look out for.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以使用各种命令行工具来处理 Anthos 版本的集群创建、扩展和升级，例如 gkectl、gkeadmin 和 anthos-gke。本章的目的不是取代
    Google Cloud 的文档，而是总结了一些操作和需要注意的问题。
- en: 'Reminder: Admin clusters are deployed purely to monitor and administer user
    clusters. Think of them as the invisible control plane analogous to GKE, and do
    not deploy services that can affect it there.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 提醒：管理员集群仅用于监控和管理用户集群。将其视为类似于 GKE 的无形控制平面，不要在那里部署可能影响它的服务。
- en: Tip You can use a kubeconfig manager like ktx from [https://github.com/heptiolabs/ktx](https://github.com/heptiolabs/ktx),
    which allows administrators to switch between admin and user cluster contexts
    easily.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 小贴士：您可以使用类似 ktx 的 kubeconfig 管理器，从 [https://github.com/heptiolabs/ktx](https://github.com/heptiolabs/ktx)，它允许管理员轻松地在管理员集群和用户集群上下文之间切换。
- en: In the next section, we’ll break up the segments into GKE on-prem and GKE on
    AWS because the tools and installation process differ.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 在下一节中，我们将把段拆分为 GKE on-prem 和 GKE on AWS，因为工具和安装过程不同。
- en: 5.2.1 Using CLI tools for GKE on-prem
  id: totrans-90
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.1 使用 GKE on-prem 的 CLI 工具
- en: 'GKE on-prem installation uses the APIs from VMware[¹](#pgfId-1106022) to build
    an admin workstation, admin cluster nodes, and user cluster nodes programmatically.
    Persistent volumes are powered from individual VMware datastores or vSAN, and
    networking is provided by either distributed or standard vSphere switches. These
    act like the IaaS components provided by Google Cloud when building a GKE cluster:
    thus the name, GKE on-prem. The concept of having an admin cluster with user clusters
    and node pools mirrors GKE best practices.'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 本地安装使用 VMware 的 API[¹](#pgfId-1106022)以编程方式构建管理工作站、管理集群节点和用户集群节点。持久卷由单个
    VMware 数据存储或 vSAN 提供电力，网络由分布式或标准 vSphere 交换机提供。这些就像在构建 GKE 集群时 Google Cloud 提供的
    IaaS 组件：因此得名 GKE on-prem。拥有管理集群、用户集群和节点池的概念反映了 GKE 的最佳实践。
- en: 'The current installation process is to download a tool named gkeadm, which
    creates an admin workstation. It is from this admin workstation that the admin
    cluster and user clusters are installed, as described next. Although versions
    of gkeadm are available for Windows, Linux, and macOS, this section will explain
    only an abbreviated process for Linux:'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 当前的安装过程是下载一个名为 gkeadm 的工具，该工具创建一个管理工作站。管理集群和用户集群都是从该管理工作站安装的，如以下所述。尽管 gkeadm
    的版本适用于 Windows、Linux 和 macOS，但本节将仅解释 Linux 的简略过程：
- en: 'The first step is to download the tool from the cloud storage bucket:'
  id: totrans-93
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一步是从云存储桶下载工具：
- en: '[PRE0]'
  id: totrans-94
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'Next, create a prepopulated config file:'
  id: totrans-95
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，创建一个预填充的配置文件：
- en: '[PRE1]'
  id: totrans-96
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Fill in the vCenter credentials, GCP whitelisted service account key path (after
    purchasing Anthos, customers are asked to provide a service account, which Google
    will whitelist to be able to download images and other proprietary tools), and
    vCenter Certificate Authority certification path.
  id: totrans-97
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 填写 vCenter 凭证、GCP 白名单服务账户密钥路径（在购买 Anthos 后，客户被要求提供一个服务账户，Google 将将其列入白名单以便下载镜像和其他专有工具），以及
    vCenter 证书授权机构证书路径。
- en: 'The vCenter Certificate Authority certifications can be downloaded as follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: vCenter 证书授权机构的证书可以按以下方式下载：
- en: '[PRE2]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: After unzipping the download.zip file, the relevant certifications can be found
    in the certs/lin folder. The file with the .0 suffix is the root certificate.
    Rename it to vcenter.crt, and use it in the reference from the installation config
    file.
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 解压 download.zip 文件后，相关证书可以在 certs/lin 文件夹中找到。带有 .0 后缀的文件是根证书。将其重命名为 vcenter.crt，并在安装配置文件的引用中使用它。
- en: The vCenter and F5 Big-IP credentials are saved in plain text in the config
    file when you create new user clusters or on installation. One way to secure the
    F5 credentials is through using a wrapper around Google Cloud Secret Manager and
    gcloud.
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 当您创建新的用户集群或在安装过程中时，vCenter 和 F5 Big-IP 凭证以纯文本形式保存在配置文件中。保护 F5 凭证的一种方法是通过围绕 Google
    Cloud Secret Manager 和 gcloud 的包装。
- en: 'To create a password secured by Google Secret Manager, use the following code:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建一个由 Google Secret Manager 保护的密码，请使用以下代码：
- en: '[PRE3]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'To retrieve a password secured by Google Secret Manager, enter the following
    code:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 要检索由 Google Secret Manager 保护的密码，请输入以下代码：
- en: '[PRE4]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: This secret is now protected via Google IAM policies and a wrapper script can
    be written to retrieve the secret, replace the placeholder in the config file,
    apply it, and then delete the file.
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 此秘密现在通过 Google IAM 策略得到保护，并且可以编写一个包装脚本以检索秘密，替换配置文件中的占位符，应用它，然后删除文件。
- en: The process to create Anthos cluster components is quickly evolving, and it’s
    not uncommon for a newer version to have some changes to the config file. You
    can learn about the latest release procedures at [http://mng.bz/Zova](http://mng.bz/Zova).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 创建 Anthos 集群组件的过程正在迅速演变，对于较新版本对配置文件进行一些更改并不罕见。您可以在 [http://mng.bz/Zova](http://mng.bz/Zova)
    了解最新的发布流程。
- en: 'Cluster management: Creating a new user cluster'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：创建新的用户集群
- en: The *gkectl* command is used for this operation. As a rule of thumb, admins
    should constrain their setups so that they contain a ratio of one admin cluster
    to 10 user clusters. User clusters should have a minimum of three nodes, with
    a maximum of 100 nodes. As previously mentioned, newer releases may increase these
    numbers. When a new Anthos release is published, you can check the new limits
    in the Quotas and Limits section of the respective release.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '*gkectl* 命令用于此操作。一般来说，管理员应该限制他们的设置，使其包含一个管理员集群与10个用户集群的比例。用户集群应至少有3个节点，最多有100个节点。如前所述，新版本可能会增加这些数字。当发布新的Anthos版本时，您可以在相应版本的配额和限制部分中检查新的限制。'
- en: The general advice is to leave some space for at least one cluster, which can
    be created in your on-prem environment. This gives the operations team space to
    recreate clusters and move Pods over when upgrading or during triage.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 一般建议至少为至少一个集群留出一些空间，该集群可以在您的本地环境中创建。这为运维团队提供了重新创建集群并在升级或分类期间移动Pod的空间。
- en: Keep good documentation, like which IP addresses have been already assigned
    for other user clusters, so that nonoverlapping IPs can be determined easily.
    Consider that user clusters can be resized to 100 nodes, so reserve up to 100
    IP addresses per range to keep that possibility.
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 保持良好的文档，例如，哪些IP地址已经被分配给其他用户集群，以便可以轻松地确定不重叠的IP地址。考虑到用户集群可以扩展到100个节点，因此请为每个范围保留多达100个IP地址以保持这种可能性。
- en: Source control your configuration files, but do not commit the vSphere username
    and passwords. Committing such sensitive information to repositories can open
    security risks because anyone with access to the repository will be able to get
    those login details. Tools like ytt can be used to template configuration YAML
    and perform code reviews, and you should use repository scanners to prevent such
    mistakes from taking place (e.g., [http://mng.bz/Rl7O](http://mng.bz/Rl7O)).
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 将配置文件源代码控制起来，但不要提交vSphere用户名和密码。将此类敏感信息提交到存储库可能会带来安全风险，因为任何有权访问存储库的人都可以获取这些登录详情。可以使用ytt等工具来模板化配置YAML并执行代码审查，您应该使用存储库扫描器来防止此类错误发生（例如，[http://mng.bz/Rl7O](http://mng.bz/Rl7O)）。
- en: Node pools can also be created with different machine shapes, so size them correctly
    to accommodate your workloads. Doing so also gives you granular control over which
    machine types to scale and saves costs. For production workloads, use three replicas
    for the user cluster master nodes for high availability, but for development,
    one should be fine.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池也可以使用不同的机器形状创建，因此请正确设置它们以适应您的工作负载。这样做还可以让您对要扩展的机器类型有更细粒度的控制，并节省成本。对于生产工作负载，使用三个副本的用户集群主节点以实现高可用性，但对于开发，一个副本应该就足够了。
- en: 'Validate the configuration file to make sure the file is valid. The checks
    are both syntactic and programmatic, such as checking for IP range clashes and
    IP availability using the gkectl check-config command:'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 验证配置文件以确保文件有效。检查既包括语法检查，也包括程序性检查，例如使用 gkectl check-config 命令检查IP范围冲突和IP可用性：
- en: '[PRE5]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: After the first few validations, most time-consuming validations can be skipped
    by passing the —fast flag.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 在进行几次验证后，可以通过传递 —fast 标志跳过大多数耗时验证。
- en: 'Next, the seesaw load balancer should be created if the bundled load balancer
    is chosen. If you do not create the seesaw node(s) before attempting a cluster
    build that has been configured with the integrated load-balancer option, you will
    receive an error during the cluster precheck. To create the seesaw node(s), use
    the gkectl create loadbalancer command:'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，如果选择了捆绑式负载均衡器，则应创建seesaw负载均衡器。如果您在配置了集成负载均衡器选项的集群构建之前没有创建seesaw节点，您将在集群预检查期间收到错误。要创建seesaw节点，请使用
    gkectl create loadbalancer 命令：
- en: '[PRE6]'
  id: totrans-118
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'After the creation of a new user cluster, remember that for the bundled load-balanced
    seesaw version, the user will then be able to create the user cluster as follows:'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在创建新的用户集群后，请记住，对于捆绑式负载均衡的seesaw版本，用户随后可以按以下方式创建用户集群：
- en: '[PRE7]'
  id: totrans-120
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: You can also add the —skip-validation-all flag if the config file has already
    been validated.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 如果配置文件已经过验证，您也可以添加 —skip-validation-all 标志。
- en: The whole user cluster process, which consists of starting up new VMware virtual
    machines with the master and worker node images and joining them into a cluster,
    can take 20-30 minutes, depending on the hardware. The administrator is also able
    to see the nodes being created from the VMware vCenter console.
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 整个用户集群过程，包括启动带有主节点和工作节点镜像的新VMware虚拟机并将它们加入集群，可能需要20-30分钟，具体取决于硬件。管理员还可以从VMware
    vCenter控制台看到正在创建的节点。
- en: High-availability setup
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: 高可用性配置
- en: High availability is necessary for Anthos deployments in production environments
    because failures can occur at different parts of the stack, ranging from networking,
    to hardware, to the virtualization layer.
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: 在生产环境中，由于堆栈的不同部分可能会出现故障，从网络到硬件再到虚拟化层，高可用性对于Anthos部署是必要的。
- en: High availability (HA) for admin clusters makes use of the vSphere HA in a vSphere
    cluster setup to protect GKE on-prem clusters from going down in the event of
    a host failure. This ensures that admin cluster nodes are distributed among different
    physical nodes in a vSphere cluster, so in the event of a physical node failure,
    the admin cluster will still be available.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 管理集群的高可用性（HA）利用vSphere集群设置中的vSphere HA来保护GKE本地集群在主机故障时不会中断。这确保了管理集群节点在vSphere集群的不同物理节点之间分布，因此，在物理节点故障的情况下，管理集群仍然可用。
- en: 'To enable HA user control planes, simply specify usercluster.master.replicas:
    3 in the GKE on-prem configuration file. This will create three user cluster masters
    for each user cluster, consuming three times the resources but providing an HA
    Kubernetes setup.'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '要启用HA用户控制平面，只需在GKE本地配置文件中指定usercluster.master.replicas: 3。这将为每个用户集群创建三个用户集群主节点，消耗三倍的资源，但提供高可用Kubernetes设置。'
- en: 'Cluster management: Scaling'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：扩展
- en: 'Administrators can use the *gkectl* CLI to scale up or down nodes. They change
    the config file to set the number of expected replicas and execute the following
    command to update the node pool:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以使用`gkectl` CLI来扩展或缩减节点。他们更改配置文件以设置期望的副本数量，并执行以下命令来更新节点池：
- en: '[PRE8]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'Cluster management: Upgrading Anthos'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：升级Anthos
- en: 'Like any upgrade process, failures can occur during the process. A lot of effort
    has been put into making the upgrade process robust, including the addition of
    prechecks before executing the upgrade to catch potential problems before they
    occur. Each product team at Google works closely together when an upgrade is being
    developed to avoid any potential incompatibilities between components like Kubernetes,
    ACM, and ASM. For ease of access, bookmark this link for quick access: [http://mng.bz/nJV8](http://mng.bz/nJV8).'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 与任何升级过程一样，在过程中可能会出现故障。已经投入了大量努力来使升级过程健壮，包括在执行升级之前添加预检查来捕捉潜在问题。当开发升级时，Google的每个产品团队都紧密合作，以避免Kubernetes、ACM和ASM等组件之间出现任何潜在的不兼容性。为了方便访问，将此链接添加为书签以快速访问：[http://mng.bz/nJV8](http://mng.bz/nJV8)。
- en: New Anthos versions appear frequently due to industry demand for new features,
    and so upgrading Anthos is a common activity. That can also mean upgrading to
    a new version of Kubernetes, which impacts Anthos Service Mesh due to Istio dependency
    on Kubernetes. The upgrade chain is complex, which is why we recommend keeping
    some spare hardware resources that can be used to create new versions of Anthos
    clusters and then move workloads to the new cluster before tearing down the older-version
    cluster. This process reduces the risk associated with upgrades by providing an
    easy rollback path in case of a failed upgrade. In this type of upgrade path,
    you should have a load balancer in front of the microservices running in the old
    cluster to be upgraded, which can direct traffic from the old cluster to the new
    cluster, because they will exist at the same time. However, if this is not an
    option, administrators can upgrade Anthos clusters in place.
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 由于对新功能的需求，新的Anthos版本频繁出现，因此升级Anthos是一种常见活动。这也可能意味着升级到Kubernetes的新版本，由于Istio对Kubernetes的依赖，这会影响Anthos
    Service Mesh。升级链很复杂，这就是为什么我们建议保留一些备用硬件资源，这些资源可以用来创建Anthos集群的新版本，然后在拆除旧版本集群之前将工作负载迁移到新集群。这个过程通过在升级失败时提供简单的回滚路径来降低升级相关的风险。在这种升级路径中，你应该在即将升级的旧集群中运行的微服务前面有一个负载均衡器，它可以引导流量从旧集群到新集群，因为它们将同时存在。然而，如果这不是一个选项，管理员可以在原地升级Anthos集群。
- en: First, consult the upgrade paths. From GKE on-prem 1.3.2 onward, administrators
    can upgrade directly to any version in the same minor release; otherwise, sequential
    upgrades are required. From version 1.7 onward, administrators can keep their
    admin cluster on an older version, while only upgrading the admin workstation
    and the user cluster. As a best practice, administrators should still schedule
    the admin cluster upgrades to keep up to date.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，咨询升级路径。从GKE on-prem 1.3.2版本开始，管理员可以直接升级到同一次要版本中的任何版本；否则，需要按顺序升级。从版本1.7开始，管理员可以将他们的管理集群保留在较旧版本上，而只需升级管理工作站和用户集群。作为最佳实践，管理员仍然应该安排管理集群的升级以保持最新。
- en: Next, download the gkeadm tool, which must be the same as the target version
    of your upgrade, and run gkeadm to upgrade the admin workstation and gkectl to
    upgrade your user cluster and, finally, the admin cluster.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，下载gkeadm工具，它必须与您升级的目标版本相同，并运行gkeadm来升级管理工作站，运行gkectl来升级您的用户集群，最后升级管理集群。
- en: When upgrading in place, a new node is created with the image of the latest
    version, and workloads are drained from the older version and shifted over to
    the latest version, one node after the other. Administrators should plan for additional
    resources in their physical hosts to accommodate at least one user node for upgrade
    purposes. The full flow can be seen in figure 5.11.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 在原地升级时，会创建一个具有最新版本镜像的新节点，并将工作负载从旧版本中移除并逐步转移到最新版本，一个节点接一个节点。管理员应该为物理主机计划额外的资源，以容纳至少一个用于升级目的的用户节点。完整的流程可以在图5.11中查看。
- en: '![05-11](../../OEBPS/Images/05-11.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![05-11](../../OEBPS/Images/05-11.png)'
- en: Figure 5.11 Upgrading flow
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.11 升级流程
- en: For a detailed list of commands, consult the upgrade documentation at [http://mng.bz/Px7v](http://mng.bz/Px7v)
    for exact details.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 对于命令的详细列表，请参阅[http://mng.bz/Px7v](http://mng.bz/Px7v)以获取确切细节。
- en: 'Cluster management: Backing up clusters'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：备份集群
- en: Anthos admin clusters can be backed up by following the steps found at [http://mng.bz/Jl7a](http://mng.bz/Jl7a)*.*
    It is recommended you do this as part of a production Anthos environment setup
    to regularly schedule backups and to do on-demand backups when upgrading Anthos
    versions.
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 可以通过遵循[http://mng.bz/Jl7a](http://mng.bz/Jl7a)中找到的步骤来备份Anthos管理集群。建议您将其作为生产Anthos环境设置的一部分，定期安排备份，并在升级Anthos版本时进行按需备份。
- en: An Anthos user clusters’ etcd can be backed up by running a backup script, which
    you can read more about at [http://mng.bz/wPAa](http://mng.bz/wPAa). Do note that
    this backs up only the etcd of the clusters, meaning the Kubernetes configuration.
    Google also states this should be a last resort. Backup for GKE promises to make
    this simpler, and was recently made available ([http://mng.bz/X5MM](http://mng.bz/X5MM)).
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 通过运行备份脚本可以备份Anthos用户集群的etcd，您可以在此处了解更多信息[http://mng.bz/wPAa](http://mng.bz/wPAa)。请注意，这仅备份集群的etcd，即Kubernetes配置。Google还表示，这应该是最后的手段。GKE的备份承诺将使这个过程更简单，并且最近已提供([http://mng.bz/X5MM](http://mng.bz/X5MM))。
- en: Any application-specific data such as persistent volumes are not backed up by
    this process. Those should be backed up regularly to another storage device using
    one of the several available tools, like Velero.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 此过程不会备份任何特定于应用程序的数据，例如持久卷。这些应该定期使用如Velero等几种可用的工具之一备份到另一个存储设备。
- en: You should treat your cluster backups the same as any data that is backed up
    from a server. The recommendation is to practice restoring an admin and user cluster
    from backup, along with application-specific data, to gain confidence in the backup
    and recovery process.
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 您应该将集群备份视为与从服务器备份的任何数据相同。建议您练习从备份中恢复管理集群和用户集群，以及特定于应用程序的数据，以增强对备份和恢复过程的信心。
- en: Google has several additions in development for Anthos. One important feature
    being added will be named Anthos Enterprise Data Protection and will provide the
    functionality to back up clusterwide config such as custom resource definitions,
    and namespace-wide configuration and application data from Google Cloud console
    into a Cloud storage bucket, as well the ability to restore using the backup.
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: Google正在为Anthos开发几个新增功能。其中一个即将添加的重要功能将被命名为Anthos企业数据保护，它将提供备份集群级配置的功能，例如自定义资源定义，以及命名空间级配置和应用数据从Google
    Cloud控制台备份到云存储桶，以及使用备份进行恢复的能力。
- en: 5.2.2 GKE on AWS
  id: totrans-145
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.2.2 AWS上的GKE
- en: GKE on AWS uses AWS EC2 instances and other components to build GKE clusters,
    which means these are not EKS clusters. If a user logs in to the AWS console,
    they will be able to see the admin cluster and user cluster nodes only as individual
    AWS EC2 instances. It is important to differentiate this from managing EKS clusters
    in Anthos because the responsibilities assigned to the various cloud providers
    according to each cluster type differ.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 在 AWS 使用 AWS EC2 实例和其他组件来构建 GKE 集群，这意味着这些不是 EKS 集群。如果用户登录 AWS 控制台，他们将只能看到管理集群和用户集群节点作为单独的
    AWS EC2 实例。区分这一点与在 Anthos 中管理 EKS 集群非常重要，因为根据集群类型，分配给各个云提供商的责任不同。
- en: 'GKE on AWS installation is done via the gcloud CLI with the command gcloud
    container aws clusters create. For Terraform users, sample terraform code is available
    to install GKE on Anthos in the following repository: [http://mng.bz/71R7](http://mng.bz/71R7).
    This sample code will further simplify the installation process and remove the
    need for a bastion host and management server mentioned in the steps that follow.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 在 AWS 的安装是通过 gcloud CLI 的 gcloud container aws clusters create 命令完成的。对于
    Terraform 用户，以下仓库中提供了安装 Anthos 上 GKE 的示例 terraform 代码：[http://mng.bz/71R7](http://mng.bz/71R7)。此示例代码将进一步简化安装过程，并消除后续步骤中提到的堡垒主机和管理服务器需求。
- en: The installation process is to first get an AWS Key Management Service (KMS)
    key, then use anthos-gke, which in turn uses Terraform to generate Terraform code.
    Terraform, offered by HashiCorp, is an Infrastructure as Code open source tool
    to define a target state of a computing environment. Terraform code is declarative
    and uses Terraform providers, which are often contributed by cloud providers such
    as Google, AWS, and Microsoft, to map their cloud provisioning APIs to Terraform
    code. The resulting Terraform code describes how the GKE on AWS infrastructure
    will look. It has components analogous to GKE on-prem, such as a load balancer
    and EC2 virtual machines, but it uses the Terraform AWS provider to instantiate
    the infrastructure on AWS. You can learn more about Terraform at [https://www.terraform.io/](https://www.terraform.io/).
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 安装过程是首先获取 AWS 密钥管理服务 (KMS) 密钥，然后使用 anthos-gke，它反过来使用 Terraform 生成 Terraform
    代码。Terraform 是 HashiCorp 提供的基础设施即代码开源工具，用于定义计算环境的目标状态。Terraform 代码是声明性的，并使用 Terraform
    提供商，这些提供商通常由云提供商如 Google、AWS 和 Microsoft 贡献，将它们的云配置 API 映射到 Terraform 代码。生成的 Terraform
    代码描述了 GKE 在 AWS 基础设施将如何看起来。它具有类似于 GKE on-prem 的组件，如负载均衡器和 EC2 虚拟机，但它使用 Terraform
    AWS 提供商在 AWS 上实例化基础设施。您可以在 [https://www.terraform.io/](https://www.terraform.io/)
    上了解更多关于 Terraform 的信息。
- en: The architecture of GKE on AWS can be seen in figure 5.12, which is from the
    Google Cloud documentation at [http://mng.bz/mJAW](http://mng.bz/mJAW).
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 在 AWS 的架构可以在图 5.12 中看到，该图来自 Google Cloud 文档，网址为 [http://mng.bz/mJAW](http://mng.bz/mJAW)。
- en: '![05-12](../../OEBPS/Images/05-12.png)'
  id: totrans-150
  prefs: []
  type: TYPE_IMG
  zh: '![05-12](../../OEBPS/Images/05-12.png)'
- en: Figure 5.12 GKE on AWS architecture
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.12 GKE 在 AWS 架构
- en: The use of node pools is like GKE, with the ability to have different machine
    sizes within a cluster.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 节点池的使用类似于 GKE，能够在集群内拥有不同的机器大小。
- en: Note To do any GKE on AWS operations management, the administrator will have
    to log in to the bastion host, which is part of the management service.
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：要进行任何 GKE 在 AWS 的操作管理，管理员将不得不登录到堡垒主机，它是管理服务的一部分。
- en: Connecting to the management service
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 连接到管理服务
- en: When doing any management operations, the administrator needs to connect to
    the bastion host deployed during the initial installation of the management service.
    This script is named bastion-tunnel.sh and is generated by Terraform during the
    management service installation.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行任何管理操作时，管理员需要连接到在管理服务初始安装期间部署的堡垒主机。此脚本名为 bastion-tunnel.sh，并在管理服务安装期间由 Terraform
    生成。
- en: 'Cluster management: Creating a new user cluster'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：创建新的用户集群
- en: 'Use the bastion-tunnel script to connect to the management service. After connecting
    to the bastion host, the administrator uses Terraform to generate a manifest,
    configuring an example cluster in a YAML file:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 使用 bastion-tunnel 脚本连接到管理服务。连接到堡垒主机后，管理员使用 Terraform 生成一个清单，在 YAML 文件中配置一个示例集群：
- en: '[PRE9]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: In this YAML file, the administrator then changes the AWSCluster and AWSNodePool
    specifications. Be sure to save the cluster file to a code repository because
    it will be reused for scaling the user cluster.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 在此 YAML 文件中，管理员随后更改 AWSCluster 和 AWSNodePool 规范。请确保将集群文件保存到代码仓库中，因为它将被用于扩展用户集群。
- en: Custom resources are extensions of Kubernetes to add additional functionality,
    such as for provisioning AWS EC2 instances. AWS clusters and objects are represented
    as YAML files, referencing the AWSCluster and AWSNodePool custom resources in
    the management service cluster, which interpret this YAML file and adjust the
    resources in AWS accordingly. To read more about custom resources, refer to [http://mng.bz/51R8](http://mng.bz/51R8).
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 自定义资源是Kubernetes的扩展，用于添加额外的功能，例如用于配置AWS EC2实例。AWS集群和对象以YAML文件的形式表示，在管理服务集群中引用AWSCluster和AWSNodePool自定义资源，这些资源解释这个YAML文件并根据AWS相应地调整资源。要了解更多关于自定义资源的信息，请参阅[http://mng.bz/51R8](http://mng.bz/51R8)。
- en: 'Cluster management: Scaling'
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：扩展
- en: 'You may experience a situation where a cluster requires additional compute
    power, and you need to scale out the cluster. Luckily, an Anthos node pool can
    scale a cluster, including a minimum and maximum node count. If you created a
    cluster with the same count in both the minimum and maximum nodes, you can change
    that setting at a later date to grow your cluster. To scale a cluster for GKE
    on AWS, you just require the administrator to modify the YAML file by updating
    the minNodeCount while creating the user cluster and applying it to the management
    service:'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会遇到需要额外计算能力的情况，这时你需要扩展集群。幸运的是，Anthos节点池可以扩展集群，包括最小和最大节点数。如果你创建了一个在最小和最大节点数上具有相同计数的集群，你可以在以后更改该设置以扩展你的集群。要扩展AWS上的GKE集群，你只需要管理员通过在创建用户集群时更新minNodeCount并应用到管理服务中来修改YAML文件：
- en: '[PRE10]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'Cluster management: Upgrading'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 集群管理：升级
- en: 'Upgrading GKE on AWS is done in two steps, with the management service handled
    first, and then the user clusters. To upgrade a GKE on the AWS management service,
    the administrator must upgrade a GKE on the AWS management service from the directory
    with the GKE on AWS configuration. Then, the user must first download the latest
    version of the anthos-gke binary. Next, the user will have to modify the anthos-gke.yaml
    file to the target version:'
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 在AWS上升级GKE分为两个步骤，首先处理管理服务，然后是用户集群。要升级AWS管理服务上的GKE，管理员必须从具有AWS GKE配置的目录升级AWS管理服务上的GKE。然后，用户必须首先下载最新的anthos-gke二进制文件。接下来，用户将需要修改anthos-gke.yaml文件到目标版本：
- en: '[PRE11]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'Finally, to validate and apply the version changes, run the next code:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，为了验证和应用版本更改，运行以下代码：
- en: '[PRE12]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: The management service will be down, so no changes to user clusters can be applied,
    but user clusters continue to run their workloads.
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 管理服务将关闭，因此无法应用对用户集群的更改，但用户集群将继续运行其工作负载。
- en: 'To upgrade the user cluster, the administrator switches context in the management
    service from the GKE on the AWS directory using the following command:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 要升级用户集群，管理员需要使用以下命令在管理服务中切换上下文，从AWS目录上的GKE切换：
- en: '[PRE13]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Then, upgrading the version of the user cluster is as easy as using the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，升级用户集群的版本就像使用以下命令一样简单：
- en: '[PRE14]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'Edit the YAML file to point to the right GKE version:'
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 编辑YAML文件以指向正确的GKE版本：
- en: '[PRE15]'
  id: totrans-175
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: On submission of this change, the CRD starts to go through the nodes in the
    control plane one by one and upgrades them to the latest version of GKE on AWS.
    This upgrade process causes a downtime of the control plane, which means the cluster
    may be unable to report the status of the different node pools until it is completed.
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 提交此更改后，CRD开始逐个遍历控制平面中的节点并将它们升级到AWS上GKE的最新版本。此升级过程会导致控制平面停机，这意味着集群可能无法在完成之前报告不同节点池的状态。
- en: 'Finally, the last step is to upgrade the actual node pool. The same procedure
    applies: the administrator just edits the YAML file to the version required and
    applies the YAML file to the management service as follows:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最后，最后一步是升级实际的节点池。相同的程序适用：管理员只需编辑YAML文件到所需的版本，并按照以下方式将YAML文件应用到管理服务：
- en: '[PRE16]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 5.3 Anthos attached clusters
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.3 Anthos附加集群
- en: Anthos attached clusters let you view your Kubernetes clusters and are provisioned
    and managed by Elastic Kubernetes Service (EKS) by AWS, Azure Kubernetes Service
    (AKS), or any conformant Kubernetes cluster. In this case, the scaling and provisioning
    of the clusters are done from the respective clouds. However, these clusters can
    still be attached and managed by Anthos by registering them to Google Cloud through
    deploying the Connect Agent, as illustrated in figure 5.13.
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos 附加集群允许您查看您的 Kubernetes 集群，并由 AWS 的 Elastic Kubernetes Service (EKS)、Azure
    Kubernetes Service (AKS) 或任何符合规范的 Kubernetes 集群提供配置和管理。在这种情况下，集群的扩展和配置是从各自的云中完成的。然而，这些集群仍然可以通过将它们注册到
    Google Cloud 并部署 Connect Agent 来由 Anthos 附加和管理，如图 5.13 所示。
- en: '![05-13](../../OEBPS/Images/05-13.png)'
  id: totrans-181
  prefs: []
  type: TYPE_IMG
  zh: '![05-13](../../OEBPS/Images/05-13.png)'
- en: Figure 5.13 Adding an external cluster (bring your own Kubernetes)
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 图 5.13 添加外部集群（自行提供 Kubernetes）
- en: 'GKE is also handled in the same way and can be attached from another project
    into the Anthos project as follows:'
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: GKE 也以相同的方式处理，可以从另一个项目附加到 Anthos 项目中，如下所示：
- en: 'The administrator must generate a kubeconfig to the EKS or AKS cluster and
    then provide that kubeconfig in a generated cluster registration command in gcloud.
    Consult the documentation from AWS and Azure on how to generate a kubeconfig file
    for the EKS or AKS clusters. The administrator can also generate one manually
    using the following template and providing the necessary certificate, server info,
    and service account token:'
  id: totrans-184
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理员必须为 EKS 或 AKS 集群生成一个 kubeconfig，然后在该生成的集群注册命令中的 gcloud 中提供该 kubeconfig。有关如何为
    EKS 或 AKS 集群生成 kubeconfig 文件的 AWS 和 Azure 文档。管理员还可以使用以下模板手动生成一个，并提供必要的证书、服务器信息和服务账户令牌：
- en: '[PRE17]'
  id: totrans-185
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE17]'
- en: The administrator must create a Google service account and a service account
    key to provide for the registration, as shown in figure 5.14.
  id: totrans-186
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理员必须创建一个 Google 服务账户和一个服务账户密钥，以提供注册，如图 5.14 所示。
- en: '![05-14](../../OEBPS/Images/05-14.png)'
  id: totrans-187
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![05-14](../../OEBPS/Images/05-14.png)'
- en: Figure 5.14 Generating a registration command for the external cluster
  id: totrans-188
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.14 为外部集群生成注册命令
- en: The administrator will provide these two items in the generated registration
    command, and after the Connect Agent has been deployed to the external cluster,
    it will be visible in the Google Cloud console.
  id: totrans-189
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 管理员将在生成的注册命令中提供这两项内容，并在 Connect Agent 部署到外部集群后，在 Google Cloud 控制台中可见。
- en: 5.4 Anthos on bare metal
  id: totrans-190
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.4 裸金属上的 Anthos
- en: Operating and managing Anthos on bare metal often requires additional skill
    sets in the OS configuration space because it is based on installing Anthos on
    RHEL, Ubuntu, or CentOS. For the detailed steps for installing and upgrading Anthos
    on bare metal, consult chapter 17.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 在裸金属上运行和管理 Anthos 通常需要在操作系统配置空间中具备额外的技能集，因为它基于在 RHEL、Ubuntu 或 CentOS 上安装 Anthos。有关在裸金属上安装和升级
    Anthos 的详细步骤，请参阅第 17 章。
- en: Anthos on bare metal is similar to Anthos on VMware but with more flexibility
    in its deployment models and no dependency on VMware. The benefits of using Anthos
    on bare metal versus managing Kubernetes running on a set of virtualized machines
    include the performance gain, the support provided by Google during installation
    of all Anthos components, and the ability to deploy applications from the Google
    Cloud Marketplace directly into the Anthos cluster. However, the team will have
    to manage their own storage devices to provide durable and performant storage
    to Anthos on bare metal clusters, compared to having that natively supported with
    Anthos on VMware with custom-built storage drivers.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: 裸金属上的 Anthos 与 VMware 上的 Anthos 类似，但在其部署模型上具有更多灵活性，且不依赖于 VMware。与在虚拟化机器上运行 Kubernetes
    相比，使用裸金属上的 Anthos 的好处包括性能提升、Google 在安装所有 Anthos 组件期间提供支持，以及能够直接从 Google Cloud
    Marketplace 将应用程序部署到 Anthos 集群。然而，与在 VMware 上使用 Anthos 并具有自定义存储驱动程序的原生支持相比，团队将不得不管理自己的存储设备，为裸金属集群上的
    Anthos 提供持久和性能良好的存储。
- en: You must make a few key decisions when designing the operations management procedures
    for Anthos on bare metal. First is capacity planning and resource estimation.
    Unlike the rest of the setups, where new nodes need to be provisioned using either
    public cloud resources or a pool of VMware resources, new bare metal nodes must
    be provisioned. This requires additional capacity requirements if there is a zero-downtime
    requirement during upgrades of the nodes because there is always a risk of nodes
    failing during upgrade and causing a decrease in capacity.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 在为裸金属上的 Anthos 设计操作管理流程时，你必须做出几个关键决策。首先是容量规划和资源估算。与需要使用公共云资源或 VMware 资源池来配置新节点的其他设置不同，新的裸金属节点必须进行配置。如果在节点升级期间有零停机时间的要求，这需要额外的容量需求，因为在升级过程中节点总是有可能失败，从而导致容量下降。
- en: Second is automating the prerequisite installation of the nodes as much as possible.
    Many companies also require a golden image of a base operating system, which must
    be vetted by a security team and continuously updated with security patches and
    latest versions. This should be built into the Anthos on bare metal provisioning
    process to verify compatibility with Anthos installation. One option is to set
    up PXE boot servers and have newly provisioned bare metal servers point to the
    PXE boot servers to install the operating system of bare metal nodes to the right
    configuration.
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 第二点是尽可能自动化节点的先决条件安装。许多公司还需要一个基础操作系统的金盘镜像，这必须由安全团队审核，并持续更新安全补丁和最新版本。这应该集成到裸金属上的
    Anthos 配置过程中，以验证与 Anthos 安装的兼容性。一个选项是设置 PXE 引导服务器，并让新配置的裸金属服务器指向 PXE 引导服务器，以将裸金属节点的操作系统安装到正确的配置。
- en: 'Third is determining the different deployments to run Anthos on bare metal,
    in standalone, multicluster, or hybrid cluster deployments. Flexibility also means
    complexity and having to build different operational models for the different
    deployments. Chapter 17 goes into more detail about the differences, but this
    chapter highlights the following operational considerations when choosing the
    different deployment models:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 第三点是确定在裸金属上运行 Anthos 的不同部署方式，包括独立、多集群或混合集群部署。灵活性也意味着复杂性，并且需要为不同的部署构建不同的操作模型。第
    17 章将更详细地介绍差异，但本章强调了在选择不同的部署模型时以下操作考虑因素：
- en: '*Standalone cluster deployment*—This deployment model, shown in figure 5.15,
    has the admin and user clusters in the same cluster. In such a configuration,
    workloads run in the same nodes, which have SSH credentials and in which Google
    service account keys are stored. This configuration is well suited for edge deployment,
    and as such, operational models should introduce SSH credential and service account
    key generation for each new standalone cluster provisioned and deployed, as well
    as a plan to decommission those credentials when a cluster is compromised or lost.
    There is a minimum requirement of five nodes for a high-availability setup.'
  id: totrans-196
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*独立集群部署*——这种部署模型，如图 5.15 所示，将管理员集群和用户集群放在同一个集群中。在这种配置下，工作负载在具有 SSH 凭证的同一节点上运行，并且在这些节点中存储了
    Google 服务账户密钥。这种配置非常适合边缘部署，因此，操作模型应该为每个新配置和部署的独立集群引入 SSH 凭证和服务账户密钥生成，以及当集群被入侵或丢失时撤销这些凭证的计划。高可用性设置至少需要五个节点。'
- en: '![05-15](../../OEBPS/Images/05-15.png)'
  id: totrans-197
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![05-15](../../OEBPS/Images/05-15.png)'
- en: Figure 5.15 Standalone deployment
  id: totrans-198
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.15 独立部署
- en: '*Multicluster deployment*—This deployment model, illustrated in figure 5.16,
    has an admin cluster and one or more user clusters, like Anthos on VMware. This
    model features many benefits, such as admin/user isolation for the clusters, multitenanted
    setups (i.e., each team can have their own cluster), and a centralized plan for
    upgrades. The downside is the increased footprint in node requirements and a minimum
    of eight nodes for a high-availability setup. Because of this, this model requires
    more effort when setting up for multiple edge locations and is more for a data
    center setup.'
  id: totrans-199
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*多集群部署*——这种部署模型，如图 5.16 所示，有一个管理员集群和一个或多个用户集群，类似于 VMware 上的 Anthos。这种模型具有许多优点，例如集群的管理员/用户隔离、多租户设置（即每个团队都可以有自己的集群）以及升级的集中计划。缺点是节点需求增加，高可用性设置至少需要八个节点。因此，在为多个边缘位置设置时需要更多努力，并且更适合数据中心设置。'
- en: '![05-16](../../OEBPS/Images/05-16.png)'
  id: totrans-200
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![05-16](../../OEBPS/Images/05-16.png)'
- en: Figure 5.16 Multicluster deployment
  id: totrans-201
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图 5.16 多集群部署
- en: '*Hybrid cluster deployment*—This deployment model, shown in figure 5.17, allows
    for running user workloads on the admin clusters and managing other user clusters.
    This model reduces the footprint required for multicluster deployment to five
    nodes for a high-availability setup, but it has the same security concern of running
    user workloads on nodes that may contain sensitive data from the standalone cluster
    deployment. Using a hybrid cluster deployments grants flexibility to tier workloads
    by security levels and introduces user clusters for workloads that require higher
    security.'
  id: totrans-202
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*混合集群部署*——这种部署模型，如图5.17所示，允许在管理集群上运行用户工作负载并管理其他用户集群。这种模型将多集群部署所需的节点数量减少到五个，以实现高可用性设置，但它与在可能包含独立集群部署中敏感数据的节点上运行用户工作负载具有相同的安全问题。使用混合集群部署可以通过安全级别对工作负载进行分层，并为需要更高安全性的工作负载引入用户集群。'
- en: '![05-17](../../OEBPS/Images/05-17.png)'
  id: totrans-203
  prefs:
  - PREF_IND
  type: TYPE_IMG
  zh: '![05-17](../../OEBPS/Images/05-17.png)'
- en: Figure 5.17 Hybrid deployment
  id: totrans-204
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 图5.17 混合部署
- en: 5.5 Connect gateway
  id: totrans-205
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.5 连接网关
- en: Registering Anthos clusters allows the user to interact with them through the
    UI, but administrators often have a toolbox of scripts, which they use to work
    with clusters through the kubectl command line. With GKE on-prem, on AWS, or on
    Azure, these clusters often can be accessed only via the admin workstation or
    a bastion host. GKE users, on the other hand, can use gcloud and generate kubeconfig
    details from kubectl to their clusters on their local machines. With Connect gateway,
    this problem is solved.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 注册Anthos集群允许用户通过UI与他们交互，但管理员通常有一套脚本工具箱，他们使用这些脚本来通过kubectl命令行与集群交互。在GKE本地、AWS或Azure上，这些集群通常只能通过管理工作站或堡垒主机访问。另一方面，GKE用户可以使用gcloud并从kubectl生成kubeconfig详细信息到他们的本地机器上的集群。通过连接网关，这个问题得到了解决。
- en: Administrators can connect to any Anthos-registered cluster and generate kubeconfig
    that enables the user to use kubectl for those clusters via the Connect Agent.
    With this feature, administrators will not be required to use jump hosts to deploy
    to the GKE on clusters, but instead can run a gcloud command to generate a kubeconfig
    to connect via kubectl.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 管理员可以连接到任何已注册的Anthos集群，并通过连接代理生成kubeconfig，使用户能够通过kubectl使用这些集群。有了这个功能，管理员将不需要使用跳转主机来部署到集群上的GKE，而是可以通过运行gcloud命令来生成kubeconfig，通过kubectl进行连接。
- en: 'The setup requires an impersonation policy, which allows the Connect Agent
    service account to impersonate a user account to issue commands on their behalf.
    An example of the YAML file, which creates the ClusterRole and ClusterRoleBinding
    for impersonation, can be seen here:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 设置需要模拟策略，该策略允许连接代理服务帐户模拟用户帐户，代表他们发出命令。创建模拟的ClusterRole和ClusterRoleBinding的YAML文件示例在此处可见：
- en: '[PRE18]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: After the impersonation policy has been set up, the administrator must run the
    command shown in figure 5.18 to generate a kubeconfig, which appears in figure
    5.19.
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 在设置好模拟策略后，管理员必须运行图5.18中显示的命令来生成kubeconfig，如图5.19所示。
- en: '![05-18](../../OEBPS/Images/05-18.png)'
  id: totrans-211
  prefs: []
  type: TYPE_IMG
  zh: '![05-18](../../OEBPS/Images/05-18.png)'
- en: Figure 5.18 Command to get credentials to the GKE on-prem cluster
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.18 获取GKE本地集群凭证的命令
- en: '![05-19](../../OEBPS/Images/05-19.png)'
  id: totrans-213
  prefs: []
  type: TYPE_IMG
  zh: '![05-19](../../OEBPS/Images/05-19.png)'
- en: Figure 5.19 kubeconfig generated via gcloud
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.19 通过gcloud生成的kubeconfig
- en: With this kubeconfig in place, administrators can manage GKE on-prem workloads,
    even from their local machine, while being secured by Google Cloud Identity. This
    method also opens the possibility for building pipelines to deploy to the different
    Anthos clusters.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 在此kubeconfig就绪后，管理员即使从本地机器也可以管理GKE本地工作负载，同时受到Google Cloud Identity的安全保护。这种方法还开启了构建将部署到不同Anthos集群的管道的可能性。
- en: 5.6 Anthos on Azure
  id: totrans-216
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 5.6 Azure上的Anthos
- en: Anthos GKE clusters can be installed on Azure with an architecture consisting
    of a multicloud API hosted on GCP that provides life cycle management capabilities
    to GKE clusters in Azure, as shown in figure 5.20\. You can also access Azure
    GKE clusters via the Connect gateway mentioned in this chapter. Anthos on Azure
    uses Azure-native technologies like the Azure Load Balancer, Azure Active Directory,
    and Azure Virtual Machines but relies on Anthos via the multicloud API to manage
    GKE cluster life cycle operations. This creates a uniform way to deploy applications
    across the three major public clouds and on-prem.
  id: totrans-217
  prefs: []
  type: TYPE_NORMAL
  zh: Anthos GKE集群可以安装在Azure上，其架构由一个托管在GCP上的多云API组成，该API为Azure中的GKE集群提供生命周期管理功能，如图5.20所示。您还可以通过本章中提到的Connect网关访问Azure
    GKE集群。Anthos在Azure上使用Azure原生技术，如Azure负载均衡器、Azure Active Directory和Azure虚拟机，但通过多云API依赖Anthos来管理GKE集群的生命周期操作。这为在三个主要公有云和本地部署应用程序提供了一种统一的方式。
- en: '![05-20](../../OEBPS/Images/05-20.png)'
  id: totrans-218
  prefs: []
  type: TYPE_IMG
  zh: '![05-20](../../OEBPS/Images/05-20.png)'
- en: Figure 5.20 Anthos on Azure architecture
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 图5.20 Anthos在Azure的架构
- en: 'As a prerequisite, the administrator must install the gcloud CLI. In addition,
    the administrator has to have the following Azure built-in roles shown here:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 作为先决条件，管理员必须安装gcloud CLI。此外，管理员必须拥有以下Azure内置角色，如所示：
- en: Application administrator
  id: totrans-221
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 应用程序管理员
- en: User access administrator
  id: totrans-222
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 用户访问管理员
- en: Contributor
  id: totrans-223
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 贡献者
- en: The next steps would be to create an Azure Active Directory application, a virtual
    network, and a resource group for the clusters, and grant the necessary permissions
    to the Azure Active Directory application. Detailed prerequisite information can
    be found in the public documentation at [http://mng.bz/61Rp](http://mng.bz/61Rp).
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 下一步是创建Azure Active Directory应用程序、虚拟网络和集群的资源组，并授予Azure Active Directory应用程序必要的权限。详细的先决条件信息可以在公共文档中找到，网址为[http://mng.bz/61Rp](http://mng.bz/61Rp)。
- en: '5.6.1 Cluster management: Creation'
  id: totrans-225
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.1 集群管理：创建
- en: 'To create a new user cluster, the administrator must first set up an Azure
    client with an SSH key pair:'
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: 要创建新的用户集群，管理员必须首先设置一个带有SSH密钥对的Azure客户端：
- en: '[PRE19]'
  id: totrans-227
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Next, the administrator will need to assign Azure resource groups, VNet, and
    subnet IDs to environment variables; add IAM permissions; and run the gcloud command
    to create the Anthos on Azure cluster:'
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，管理员需要将Azure资源组、VNet和子网ID分配给环境变量；添加IAM权限；并运行gcloud命令以创建Anthos在Azure的集群：
- en: '[PRE20]'
  id: totrans-229
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This cluster should then be available on the administrator’s GKE console. Finally,
    the admin adds a node pool to deploy workloads to the cluster:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，此集群应可在管理员的GKE控制台中使用。最后，管理员向集群添加节点池以部署工作负载：
- en: '[PRE21]'
  id: totrans-231
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: '5.6.2 Cluster management: Deletion'
  id: totrans-232
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 5.6.2 集群管理：删除
- en: 'To delete a cluster, administrators must first delete all the node pools that
    belong to a cluster:'
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 要删除集群，管理员必须首先删除属于集群的所有节点池：
- en: '[PRE22]'
  id: totrans-234
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: With an autoscaler ready to go with Anthos on Azure, it is easy for the administrator
    to control costs and manage minimum resource requirements for each cluster. It
    is recommended to have a security device like HashiCorp Vault to store the SSH
    keys for retrieval and rotation.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 在Anthos在Azure上准备好自动扩展后，管理员控制成本和管理每个集群的最小资源需求变得容易。建议使用像HashiCorp Vault这样的安全设备来存储SSH密钥以进行检索和轮换。
- en: Summary
  id: totrans-236
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: The best way to learn is by trying, and the best advice is to try building clusters
    on the various providers to understand the optimizations available and the actions
    that an administrator would need to do in their day-to-day life managing operations
    in Anthos. This is key to building a continuous improvement process because new
    features in Anthos are released frequently to make Kubernetes cluster management
    easier and faster.
  id: totrans-237
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 最佳的学习方式是通过实践，最好的建议是尝试在各个提供商上构建集群，以了解可用的优化以及管理员在日常管理Anthos操作时需要执行的操作。这是构建持续改进过程的关键，因为Anthos经常发布新功能，以使Kubernetes集群管理更加简单和快速。
- en: We covered how to use the Google Cloud console to operate and manage workloads
    of the various Anthos cluster types.
  id: totrans-238
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们介绍了如何使用Google Cloud控制台操作和管理各种Anthos集群类型的工作负载。
- en: You read about the various logging and monitoring options available with Anthos
    deployments and the criteria to consider.
  id: totrans-239
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 您可以了解Anthos部署中可用的各种日志记录和监控选项以及需要考虑的标准。
- en: You now understand how to operate and manage various types of Anthos deployments
    through the command line and how and what kind of communication happens between
    Google Cloud and the deployments.
  id: totrans-240
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你现在已经了解了如何通过命令行操作和管理各种类型的 Anthos 部署，以及 Google Cloud 与部署之间发生什么样的通信。
- en: You learned how to upgrade, scale, and design operations management procedures
    in Anthos across a hybrid environment.
  id: totrans-241
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 你学习了如何在混合环境中升级、扩展以及设计 Anthos 的操作管理流程。
- en: '* * *'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: '* * *'
- en: ^(1.)In addition to VMware, it is possible to use Anthos on bare metal. That
    is the topic discussed in chapter 17.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ^ (1.) 除了 VMware，还可以在裸金属上使用 Anthos。这是第 17 章讨论的主题。
