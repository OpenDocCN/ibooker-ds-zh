- en: Chapter 6\. Prepare the Dataset for Model Training
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第6章。为模型训练准备数据集
- en: In the previous chapter, we explored our dataset using SageMaker Studio and
    various Python-based visualization libraries. We gained some key business insights
    into our product catalog using the Amazon Customer Reviews Dataset. In addition,
    we analyzed summary statistics and performed quality checks on our dataset using
    SageMaker Processing Jobs, Apache Spark, and the AWS Deequ open source library.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们使用SageMaker Studio和各种基于Python的可视化库探索了我们的数据集。我们使用亚马逊客户评论数据集获得了一些关键的业务见解。此外，我们使用SageMaker处理作业、Apache
    Spark和AWS Deequ开源库对数据集执行了汇总统计和质量检查。
- en: In this chapter, we discuss how to transform human-readable text into machine-readable
    vectors in a process called “feature engineering.” Specifically, we will convert
    the raw `review_body` column from the Amazon Customer Reviews Dataset into BERT
    vectors. We use these BERT vectors to train and optimize a review-classifier model
    in Chapters [7](ch07.html#train_your_first_model) and [8](ch08.html#train_and_optimize_models_at_scale),
    respectively. We will also dive deep into the origins of natural language processing
    and BERT in [Chapter 7](ch07.html#train_your_first_model).
  id: totrans-2
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们讨论如何将人类可读文本转换为机器可读向量，这个过程称为“特征工程”。具体来说，我们将从亚马逊客户评论数据集的原始`review_body`列转换为BERT向量。我们将使用这些BERT向量来训练和优化评审分类器模型，在第[7](ch07.html#train_your_first_model)章和第[8](ch08.html#train_and_optimize_models_at_scale)章分别进行。我们还将深入探讨自然语言处理和BERT在第7章的起源。
- en: We will use the review-classifier model to predict the `star_rating` of product
    reviews from social channels, partner websites, etc. By predicting the `star_rating`
    of reviews in the wild, the product management and customer service teams can
    use these predictions to address quality issues as they escalate publicly—not
    wait for a direct inbound email or phone call. This reduces the mean time to detect
    quality issues down to minutes/hours from days/months.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用评审分类器模型来预测社交渠道、合作伙伴网站等的产品评论的`star_rating`。通过预测在实际环境中的评论`star_rating`，产品管理和客户服务团队可以使用这些预测来在问题公开发生之前解决质量问题，而不是等待直接的入站电子邮件或电话。这将把检测质量问题的平均时间从几天/几月缩短到分钟/小时。
- en: Perform Feature Selection and Engineering
  id: totrans-4
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 执行特征选择和工程
- en: AI and machine learning algorithms are numerical-optimization methods that operate
    on numbers and vectors instead of raw text and images. These vectors, often called
    “embeddings,” are projected into a high-dimensional vector space. The algorithms
    perform optimizations in this high-dimensional vector space.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: AI和机器学习算法是数值优化方法，操作的是数字和向量，而不是原始文本和图像。这些向量通常被称为“嵌入”，它们被投射到高维向量空间中。算法在这个高维向量空间中执行优化。
- en: One-hot encoding is a form of embedding for categorical data in a tabular dataset.
    With one-hot encoding, we represent each categorical value with a unique vector
    of 0s and 1s. The number of dimensions—the size of each vector—is equal to the
    number of unique categorical values.
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: One-hot编码是在表格数据集中用于分类数据的一种嵌入形式。使用One-hot编码，我们将每个分类值表示为一个唯一的0和1组成的向量。向量的维度数量——每个向量的大小——等于唯一分类值的数量。
- en: One of the most important aspects of AI and machine learning, feature engineering
    usually requires more time than any other phase in the typical machine learning
    pipeline. Feature engineering helps to reduce data dimensionality, prevent certain
    features from statistically dominating the algorithm, speed up model-training
    time, reduce numerical instabilities, and improve overall model-prediction accuracy.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: AI和机器学习中最重要的一个方面是特征工程，通常需要比典型机器学习流水线中的任何其他阶段更多的时间。特征工程有助于减少数据的维度，防止某些特征在统计上主导算法，加快模型训练时间，减少数值不稳定性，并提高整体模型预测准确性。
- en: With many feature-engineering iterations and visualizations, we will start to
    really understand our dataset, including outliers, correlations, and principal
    components. Analyzing the features in the context of our models, we will also
    gain intuition about which features are more important than others. Some features
    will improve model performance, while other features show no improvement or reduce
    model performance.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 通过许多特征工程迭代和可视化，我们将开始真正理解我们的数据集，包括异常值、相关性和主成分。在模型的背景下分析特征，我们还会对哪些特征比其他特征更重要有直觉。某些特征将改善模型性能，而其他特征则显示没有改进或降低模型性能。
- en: Careless feature engineering can lead to disastrous results. At worst, poor
    feature engineering can lead to socially destructive models that propagate racial,
    gender, and age bias. At best, poor feature engineering produces suboptimal models
    that make poor movie recommendations, overstate revenue forecasts, or create excess
    inventory.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 粗心的特征工程可能会导致灾难性的结果。最糟糕的情况下，糟糕的特征工程可能会导致社会上具有种族、性别和年龄偏见的模型传播。最好的情况下，糟糕的特征工程会产生次优的模型，导致推荐电影不准确，过度估计收入预测或者创建过多的库存。
- en: While domain experts can certainly help evaluate which features to include and
    how they should be engineered, there are certain “latent” features hidden in our
    datasets not immediately recognizable by a human. Netflix’s recommendation system
    is famous for discovering new movie genres beyond the usual drama, horror, and
    romantic comedy. For example, they discovered very specific genres such as “Gory
    Canadian Revenge Movies,” “Sentimental Movies About Horses for Ages 11–12,” “Romantic
    Crime Movies Based on Classic Literature,” and “Raunchy Mad Scientist Comedies.”
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然领域专家确实可以帮助评估包括哪些特征及其工程化方式，但我们数据集中存在一些“潜在”特征，人类并不立即能够识别。Netflix 的推荐系统以发现超出传统戏剧、恐怖和浪漫喜剧等通常流派的新电影流派而闻名。例如，他们发现了非常具体的流派，如“血腥的加拿大复仇电影”、“11-12岁观众的关于马的感情电影”、“根据经典文学改编的浪漫犯罪电影”和“淫秽的疯狂科学家喜剧”。
- en: Note
  id: totrans-11
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Many of these “secret” genres were discovered using Netflix’s Viewing History
    Service, also called “VHS”—a throwback to the popular video-tape format from the
    1980s and 1990s.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 许多这些“秘密”流派是通过 Netflix 的观看历史服务——也称为“VHS”，这是对上世纪八九十年代流行的录像带格式的一种回顾——发现的。
- en: 'At a high level, feature engineering is divided into three logical types: selection,
    creation, and transformation. Not all may apply to our use case, but they should
    all be considered and explored.'
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 在高层次上，特征工程分为三种逻辑类型：选择、创建和转换。并非所有类型都适用于我们的用例，但应考虑并进行探索。
- en: Feature selection identifies the data attributes that best represent our dataset.
    In addition, feature selection filters out irrelevant and redundant attributes
    using statistical methods. For example, if two data points are highly correlated,
    such as `total_votes` and `helpful_votes`, then perhaps only one is needed to
    train our model. Selecting only one of these attributes helps to reduce feature
    dimensionality and train models faster while preserving model accuracy.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 特征选择确定最能代表我们数据集的数据属性。此外，特征选择使用统计方法过滤掉不相关和冗余的属性。例如，如果两个数据点高度相关，比如`total_votes`和`helpful_votes`，那么也许只需要一个来训练我们的模型。只选择其中一个属性有助于降低特征维度并更快地训练模型，同时保持模型准确性。
- en: Feature creation combines existing data points into new features that help improve
    the predictive power of our model. For example, combining `review_headline` and
    `review_body` into a single feature may lead to more accurate predictions than
    does using them separately.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 特征创建将现有的数据点结合成新特征，有助于提高我们模型的预测能力。例如，将`review_headline`和`review_body`组合成一个单一特征可能比单独使用它们更能提高预测准确性。
- en: Feature transformation converts data from one representation to another to facilitate
    machine learning. Transforming continuous values such as a timestamp into categorical
    “bins” like hourly, daily, or monthly helps to reduce dimensionality. While we
    lose some information and granularity during the binning transformation, our models
    may actually benefit from the broader generalization. Two common statistical feature
    transformations are normalization and standardization. Normalization scales all
    values of a particular data point between 0 and 1, while standardization transforms
    the values to a mean of 0 and standard deviation of 1\. Standardization is often
    preferred as it better handles outliers than normalization does and allows us
    to compare features of different units and scales. These techniques help reduce
    the impact of large-valued data points, such as number of reviews (represented
    in thousands), versus small-valued data points, such as `helpful_votes` (represented
    in tens). Without these techniques, the model could potentially favor the number
    of reviews over `helpful_votes` given the order of magnitude difference in values.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 特征转换将数据从一种表示转换为另一种，以便于机器学习。将连续值如时间戳转换为小时、天或月的分类“桶”，有助于减少维度。虽然在分桶转换过程中我们会失去一些信息和细节，但我们的模型实际上可能会从更广泛的泛化中受益。两种常见的统计特征转换是归一化和标准化。归一化将特定数据点的所有值缩放到
    0 到 1 之间，而标准化则将值转换为均值为 0、标准差为 1。与归一化相比，标准化通常更受欢迎，因为它比归一化更好地处理异常值，并允许我们比较不同单位和尺度的特征。这些技术有助于减少大数值数据点（如以千计表示的评论数）和小数值数据点（如以十计表示的`helpful_votes`）之间的影响差异。如果没有这些技术，模型可能会在数值大小上偏向于评论数而忽略`helpful_votes`。
- en: Let’s walk through a typical feature engineering pipeline from feature selection
    to feature transformation, as shown in [Figure 6-1](#steps_in_a_typical_feature_engineering).
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从特征选择到特征转换走一遍典型的特征工程流水线，如图[6-1](#steps_in_a_typical_feature_engineering)所示。
- en: '![](assets/dsaw_0601.png)'
  id: totrans-18
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0601.png)'
- en: Figure 6-1\. Steps in a typical feature engineering pipeline.
  id: totrans-19
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-1\. 典型特征工程流水线中的步骤。
- en: Select Training Features Based on Feature Importance
  id: totrans-20
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 基于特征重要性选择训练特征
- en: We can use SageMaker Data Wrangler’s “Quick Model” analysis to evaluate which
    columns of our data are most useful when making predictions for a given label,
    `star_rating` in our case. We simply select the data that we want Data Wrangler
    to analyze along with the `star_rating` label that we want to predict. Data Wrangler
    automatically preprocesses the data, trains a “quick model,” evaluates the model,
    and calculates a feature importance score for each feature. [Figure 6-2](#analyzing_feature_importance_with_data)
    shows the feature importance results for our Amazon Customer Reviews Dataset using
    Data Wrangler’s Quick Model analysis feature.
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用 SageMaker Data Wrangler 的“快速模型”分析来评估我们的数据中哪些列在预测给定标签（在我们的情况下是`star_rating`）时最有用。我们只需选择要让
    Data Wrangler 分析的数据，以及我们想要预测的`star_rating`标签。Data Wrangler 自动预处理数据，训练“快速模型”，评估模型，并计算每个特征的特征重要性分数。图[6-2](#analyzing_feature_importance_with_data)展示了使用
    Data Wrangler 的快速模型分析功能对我们的亚马逊客户评论数据集进行特征重要性分析的结果。
- en: '![](assets/dsaw_0602.png)'
  id: totrans-22
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0602.png)'
- en: Figure 6-2\. Data Wrangler’s Quick Model analysis allows us to analyze feature
    importance.
  id: totrans-23
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-2\. 数据整理师快速模型分析允许我们分析特征重要性。
- en: Following the Quick Model analysis, the most important feature for our dataset
    is `review_body`, with `review_headline`, `product_title`, and `product_category`
    being next-most important.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 在快速模型分析之后，我们数据集中最重要的特征是`review_body`，其次是`review_headline`、`product_title`和`product_category`。
- en: Because we plan to use our model to classify product reviews from social channels
    and partner websites “in the wild” that only have the raw review text, we have
    decided to only use the `review_body` column to predict a `star_rating`. In our
    case, `star_rating` is the “label,” and a transformed version of the `review_body`
    is the “feature.” The `star_rating` label is the actual `star_rating` value, 1
    through 5, from our training dataset. This is the value that our trained model
    will learn to predict in [Chapter 7](ch07.html#train_your_first_model). The `review_body`
    feature, transformed from raw text into a series of BERT vectors, is the input
    for our model-training process. Later in this chapter, we will demonstrate how
    to transform the raw text into BERT vectors.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 因为我们计划使用我们的模型来分类来自社交渠道和合作伙伴网站的产品评论，“在野外”，这些评论只有原始的评论文本，所以我们决定只使用`review_body`列来预测`star_rating`。在我们的情况下，`star_rating`是“标签”，而`review_body`的转换版本是“特征”。`star_rating`标签是我们训练数据集中的实际`star_rating`值，从中我们训练出的模型将学习在[第
    7 章](ch07.html#train_your_first_model)中预测的值。从原始文本转换为一系列BERT向量的`review_body`特征，是我们模型训练过程的输入。在本章的后面部分，我们将演示如何将原始文本转换为BERT向量。
- en: 'We use both the feature and the label to train our model to predict a `star_rating`
    label from `review_body` text from social channels and partner websites. Following,
    we view the `star_rating` and `review_body` columns as a pandas DataFrame:'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们同时使用特征和标签来训练我们的模型，以从社交渠道和合作伙伴网站的`review_body`文本中预测`star_rating`标签。接下来，我们将`star_rating`和`review_body`列视为pandas
    DataFrame：
- en: '[PRE0]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: '| star_rating | review_body |'
  id: totrans-28
  prefs: []
  type: TYPE_TB
  zh: '| star_rating | review_body |'
- en: '| --- | --- |'
  id: totrans-29
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 1 | Poor business decision to strip away user abil... |'
  id: totrans-30
  prefs: []
  type: TYPE_TB
  zh: '| 1 | 剥夺用户能力的差劲商业决策... |'
- en: '| 5 | Avast is an easy to use and download. I feel i... |'
  id: totrans-31
  prefs: []
  type: TYPE_TB
  zh: '| 5 | Avast 是一个易于使用和下载的产品。我觉得...... |'
- en: '| 2 | Problems from the start. It has been 30 days,... |'
  id: totrans-32
  prefs: []
  type: TYPE_TB
  zh: '| 2 | 从一开始就有问题。已经过去30天，... |'
- en: '| 4 | Works well. |'
  id: totrans-33
  prefs: []
  type: TYPE_TB
  zh: '| 4 | 运行良好。 |'
- en: '| 3 | Hard to use |'
  id: totrans-34
  prefs: []
  type: TYPE_TB
  zh: '| 3 | 使用起来困难 |'
- en: 'Since the `star_rating` label is discrete and categorical (1, 2, 3, 4, 5),
    we will use a “classification” algorithm. We are not treating this as a regression
    problem since we are using `star_rating` as a categorical feature with only five
    possible values: 1, 2, 3, 4, or 5\. If `star_rating` contained continuous values,
    such as 3.14, 4.20, or 1.69, then we would potentially use `star_rating` as a
    continuous feature with a regression model.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`star_rating`标签是离散的分类特征（1、2、3、4或5），我们将使用“分类”算法。我们不将其视为回归问题，因为我们使用的是仅具有五个可能值（1、2、3、4或5）的`star_rating`作为分类特征。如果`star_rating`包含连续值，比如3.14、4.20或1.69，则我们可能会使用`star_rating`作为具有回归模型的连续特征。
- en: Instead of the traditional machine learning classification algorithms, we will
    use a neural-network-based classification model using the Keras API with TensorFlow
    2.x. We will dive deep into model training in the next chapter. Let’s move forward
    and prepare our Amazon Customer Reviews Dataset to train a model that predicts
    `star_rating` (1–5) from `review_body` text.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将不再使用传统的机器学习分类算法，而是使用基于Keras API和TensorFlow 2.x的基于神经网络的分类模型。我们将在下一章节深入研究模型训练。让我们继续并准备我们的亚马逊客户评论数据集，以训练一个能够从`review_body`文本中预测`star_rating`（1–5）的模型。
- en: Balance the Dataset to Improve Model Accuracy
  id: totrans-37
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 平衡数据集以提高模型准确性
- en: In the previous chapter, we showed the breakdown of `star_rating` for all reviews
    in our dataset, and we saw that approximately 62% of all reviews have a `star_rating`
    of 5, as shown in [Figure 6-3](#our_dataset_contains_an_unbalanced_numb).
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章节中，我们展示了数据集中所有评论的`star_rating`分布情况，发现大约62%的评论的`star_rating`为5，如[图 6-3](#our_dataset_contains_an_unbalanced_numb)所示。
- en: '![](assets/dsaw_0603.png)'
  id: totrans-39
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0603.png)'
- en: Figure 6-3\. Our dataset contains an unbalanced number of reviews by star rating.
  id: totrans-40
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-3\. 我们的数据集包含了星级评价不平衡的评论数量。
- en: 'If we naively train on this unbalanced dataset, our classifier may simply learn
    to predict 5 for the `star_rating` since 62% accuracy is better than 20% random
    accuracy across the 5 classes: 1, 2, 3, 4, or 5\. In other words, an unbalanced
    training dataset may create a model with artificially high accuracy when, in reality,
    it learned to just predict 5 every time. This model would not do well in a production
    setting.'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们在这个不平衡的数据集上进行天真的训练，我们的分类器可能会简单地学会预测`star_rating`为5，因为在5个类别（1、2、3、4或5）上，62%
    的准确率比随机准确率（20%）要好。换句话说，在现实中，一个不平衡的训练数据集可能会创建一个模型，该模型只学会在每次预测时预测5。这种模型在生产环境中表现不佳。
- en: Note
  id: totrans-42
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Some algorithms like XGBoost support a scaling factor to counteract the problem
    of unbalanced classes. However, in general, it’s a good idea to handle class imbalance
    during the feature engineering process to avoid misusing these features later.
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 一些算法如 XGBoost 支持缩放因子以抵消不平衡类的问题。然而，一般来说，在特征工程过程中处理类别不平衡是个好主意，以避免后续误用这些特征。
- en: 'There are two common ways to balance a dataset and prevent bias toward a particular
    class: undersampling the majority classes (`star_rating` 5) and oversampling the
    minority classes (`star_rating` 2 and 3). When choosing a sampling strategy, we
    should carefully consider how the sampling affects the overall mean and standard
    deviation of the feature’s data distribution. We see examples of undersampling
    in [Figure 6-4](#undersampling_the_majority_class_down_t) and oversampling in
    [Figure 6-5](#oversampling_the_minority_class_up_to_t).'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 平衡数据集并防止偏向特定类别有两种常见方法：对多数类进行欠采样（`star_rating` 5）和对少数类进行过采样（`star_rating` 2 和
    3）。在选择采样策略时，我们应仔细考虑采样如何影响特征数据分布的整体均值和标准差。我们在[图 6-4](#undersampling_the_majority_class_down_t)中看到欠采样的示例和[图 6-5](#oversampling_the_minority_class_up_to_t)中看到过采样的示例。
- en: '![](assets/dsaw_0604.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0604.png)'
- en: Figure 6-4\. Undersampling the majority class down to the minority class.
  id: totrans-46
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-4\. 将多数类欠采样至少数类。
- en: '![](assets/dsaw_0605.png)'
  id: totrans-47
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0605.png)'
- en: Figure 6-5\. Oversampling the minority class up to the majority class.
  id: totrans-48
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-5\. 将少数类过采样至多数类。
- en: 'The idea is to evenly distribute data along a particular label or “class,”
    as it’s commonly called. In our case, the class is our categorical `star_rating`
    field. Therefore, we want our training dataset to contain a consistent number
    of reviews for each `star_rating`: 1, 2, 3, 4, and 5\. Here is the code to undersample
    the original dataset using `star_rating`:'
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 思路是沿着特定标签或常称的“类”均匀分布数据。在我们的情况下，类别是我们的分类 `star_rating` 字段。因此，我们希望我们的训练数据集包含每个
    `star_rating`：1、2、3、4 和 5 的一致数量的评论。以下是使用 `star_rating` 对原始数据集进行欠采样的代码：
- en: '[PRE1]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: We now have a balanced dataset, as shown in [Figure 6-6](#balanced_dataset_for_star_rating_class).
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们有一个平衡的数据集，如[图 6-6](#balanced_dataset_for_star_rating_class)所示。
- en: '![](assets/dsaw_0606.png)'
  id: totrans-52
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0606.png)'
- en: Figure 6-6\. Balanced dataset for `star_rating` class.
  id: totrans-53
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-6\. `star_rating` 类的平衡数据集。
- en: One drawback to undersampling is that the training dataset size is sampled down
    to the size of the smallest category. This can reduce the predictive power and
    robustness of the trained models by reducing the signal from undersampled classes.
    In this example, we reduced the number of reviews by 65%, from approximately 100,000
    to 35,000.
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 欠采样的一个缺点是，训练数据集的大小被采样到最小类别的大小。这可能通过减少来自欠采样类的信号，降低了训练模型的预测能力和鲁棒性。在本例中，我们将评论数量从大约
    100,000 减少到 35,000，减少了 65%。
- en: Oversampling will artificially create new data for the underrepresented class.
    In our case, `star_rating` 2 and 3 are underrepresented. One common technique
    is called the *Synthetic Minority Oversampling Technique*, which uses statistical
    methods to synthetically generate new data from existing data. They tend to work
    better when we have a larger dataset, so be careful when using oversampling on
    small datasets with a low number of minority class examples.
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 过采样将为代表性不足的类别人工创建新数据。在我们的情况下，`star_rating` 2 和 3 代表性不足。一种常见的技术称为*合成少数类过采样技术*，它使用统计方法从现有数据中合成生成新数据。当我们拥有更大的数据集时，它们通常能发挥更好的效果，因此在使用少数样本的小数据集时要小心使用过采样。
- en: Split the Dataset into Train, Validation, and Test Sets
  id: totrans-56
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将数据集分割为训练、验证和测试集
- en: 'Model development typically follows three phases: model *training*, model *validating*,
    and model *testing* ([Figure 6-7](#phases_of_a_typical_model_development_l)).'
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 模型开发通常遵循三个阶段：模型*训练*，模型*验证*和模型*测试*（参见[图 6-7](#phases_of_a_typical_model_development_l)）。
- en: '![](assets/dsaw_0607.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0607.png)'
- en: Figure 6-7\. Phases of a typical model development life cycle.
  id: totrans-59
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-7\. 典型模型开发生命周期的阶段。
- en: To align with these three phases, we split the balanced data into separate train,
    validation, and test datasets. The train dataset is used for model training. The
    validation dataset is used to validate the model training configuration called
    the “hyper-parameters.” And the test dataset is used to test the chosen hyper-parameters.
    For our model, we chose 90% train, 5% validation, and 5% test, as this breakdown,
    shown in [Figure 6-8](#dataset_splits_for_the_typical_phases_o), works well for
    our dataset and model.
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 为了与这三个阶段保持一致，我们将平衡的数据分割成单独的训练、验证和测试数据集。训练数据集用于模型训练。验证数据集用于验证称为“超参数”的模型训练配置。而测试数据集用于测试选择的超参数。对于我们的模型，我们选择了90%的训练数据、5%的验证数据和5%的测试数据，因为这种分布，在[图表 6-8](#dataset_splits_for_the_typical_phases_o)中显示的工作效果很好，适合我们的数据集和模型。
- en: '![](assets/dsaw_0608.png)'
  id: totrans-61
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0608.png)'
- en: Figure 6-8\. Dataset splits for the typical phases of the model development
    life cycle.
  id: totrans-62
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图表 6-8\. 模型开发生命周期典型阶段的数据集拆分。
- en: 'Let’s split the data using scikit-learn’s `train_test_split` function, with
    the `stratify` parameter set to the `star_rating` field to preserve our previous
    balance efforts. If we don’t specify the `stratify` parameter, the split function
    is free to choose any data in the given dataset, causing the splits to become
    unbalanced:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用scikit-learn的`train_test_split`函数来分割数据，其中`stratify`参数设置为`star_rating`字段，以保持我们先前的平衡努力。如果不指定`stratify`参数，拆分函数可以自由选择给定数据集中的任何数据，导致拆分变得不平衡。
- en: '[PRE2]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: 'In this case, we are not using k-folds cross-validation—a classic machine learning
    technique that reuses each row of data across different splits, including train,
    validation, and test. K-folds cross-validation is traditionally applied to smaller
    datasets, and, in our case, we have a large amount of data so we can avoid the
    downside of k-folds: data “leakage” between the train, validation, and test phases.
    Data leakage can lead to artificially inflated model accuracy for our trained
    models. These models don’t perform well on real-world data outside of the lab.
    In summary, each of the three phases, train, validation, and test, should use
    separate and independent datasets, otherwise leakage may occur.'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们不使用k折交叉验证——一种经典的机器学习技术，该技术在不同的拆分中重复使用数据的每一行，包括训练、验证和测试。K折交叉验证传统上适用于较小的数据集，在我们的情况下，由于数据量很大，我们可以避免k折的缺点：在训练、验证和测试阶段之间的数据“泄漏”。数据泄漏可能会导致我们训练模型的人工膨胀的模型准确性。这些模型在实验室外的真实世界数据上表现不佳。总之，训练、验证和测试的每个阶段都应使用单独和独立的数据集，否则可能会发生泄漏。
- en: On a related note, time-series data is often prone to leakage across splits.
    Companies often want to validate a new model using “back-in-time” historical information
    before pushing the model to production. When working with time-series data, make
    sure the model does not peek into the future accidentally. Otherwise, these models
    may appear more accurate than they really are.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 在相关说明中，时间序列数据通常容易在拆分之间泄漏。公司通常希望在将模型推向生产之前，使用“回溯”历史信息验证新模型。在处理时间序列数据时，请确保模型不会意外地窥视未来。否则，这些模型可能看起来比它们实际上更准确。
- en: Note
  id: totrans-67
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: Peeking into the future almost ended in disaster for the characters in the movie
    *Back to the Future*. Similarly, peeking into the future may cause trouble for
    our modeling efforts as well.
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 几乎在电影《回到未来》中，窥探未来几乎导致了灾难。同样地，窥探未来可能会给我们的建模工作带来麻烦。
- en: Additionally, we may want to keep all data for the same customer in the same
    split. Otherwise, an individual’s customer data is spread across multiple splits,
    which could cause problems. In this case, we would group the data by `customer_id`
    before creating the splits. Our model does not require us to group our data by
    `customer_id`, so we will skip this step.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，我们可能希望将同一客户的所有数据保留在同一拆分中。否则，个人客户数据将分散在多个拆分中，可能会引发问题。在这种情况下，我们将在创建拆分之前按`customer_id`对数据进行分组。我们的模型不要求我们按`customer_id`分组数据，所以我们将跳过这一步。
- en: When processing large datasets at scale with SageMaker, we can split the data
    across multiple instances in a cluster. This is called *sharding* and we will
    demonstrate this later when we transform our data using multiple instances in
    a SageMaker cluster using scikit-learn, Apache Spark, and TensorFlow.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用SageMaker处理大规模数据时，我们可以将数据跨多个实例分布在一个集群中。这被称为*分片*，我们稍后将演示如何在SageMaker集群中使用scikit-learn、Apache
    Spark和TensorFlow转换数据时进行多实例处理。
- en: Transform Raw Text into BERT Embeddings
  id: totrans-71
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将原始文本转换为BERT嵌入向量。
- en: 'We will use TensorFlow and a state-of-the-art natural language processing (NLP)
    and natural language understanding neural network architecture called [BERT](https://oreil.ly/HBic8).
    We will dive deep into *BERT* in a bit. At a high level—and unlike previous generations
    of NLP models, such as [Word2Vec](https://oreil.ly/nKuFP)—BERT captures the bidirectional
    (left-to-right and right-to-left) context of each word in a sentence. This allows
    BERT to learn different meanings of the same word across different sentences.
    For example, the meaning of the word *bank* is different in these two sentences:
    “A thief stole money from the *bank* vault” and “Later, he was arrested while
    fishing on a river *bank*.”'
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用 TensorFlow 和一种称为[BERT](https://oreil.ly/HBic8)的最先进的自然语言处理（NLP）和自然语言理解神经网络架构。稍后我们将深入探讨*BERT*。从高层次来看——与以往的NLP模型（如[Word2Vec](https://oreil.ly/nKuFP)）不同，BERT
    捕捉了每个句子中每个单词的双向（从左到右和从右到左）上下文。这使得 BERT 能够学习同一个单词在不同句子中的不同含义。例如，单词*bank*在以下两个句子中的含义不同：“一个小偷从*银行*金库偷走了钱”和“后来，他在河边*钓鱼*时被逮捕。”
- en: For each `review_body`, we use BERT to create a feature vector within a previously
    learned, high-dimensional vector space of 30,000 words, or “tokens.” BERT learned
    these tokens by training on millions of documents, including Wikipedia and Google
    Books.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个`review_body`，我们使用 BERT 在先前学习的、30,000 个词或“令牌”的高维向量空间内创建特征向量。BERT 通过在包括维基百科和谷歌图书在内的数百万文档上进行训练来学习这些令牌。
- en: '[Figure 6-9](#bert_converts_raw_input_text_into_embed) shows how BERT converts
    the raw input text into the final BERT embedding, which is passed through the
    actual model architecture.'
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 6-9](#bert_converts_raw_input_text_into_embed) 展示了 BERT 如何将原始输入文本转换为最终的
    BERT 嵌入，然后通过实际的模型架构进行传递。'
- en: '![](assets/dsaw_0609.png)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0609.png)'
- en: Figure 6-9\. BERT converts raw input text into embeddings.
  id: totrans-76
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-9\. BERT 将原始输入文本转换为嵌入。
- en: BERT first applies WordPiece tokenization to the raw input text. WordPiece is
    a technique to segment words to the subword level in NLP tasks with a vocabulary
    dimension of approximately 30,000 tokens. Note that BERT also adds special tokens
    at the beginning of the input sequence, such as [CLS] to mark a classification
    task.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: BERT 首先将原始输入文本应用 WordPiece 令牌化。WordPiece 是一种在NLP任务中将单词分割为子词级别的技术，其词汇维度约为30,000个令牌。请注意，BERT
    还在输入序列的开头添加特殊令牌，如[CLS]以标记分类任务。
- en: In a next step, BERT creates the token embedding by looking up the 768-dimensional
    vector representation of any input token. The `input_id` is the actual ID that
    points to the relevant token embedding vector. An `input_mask` specifies which
    tokens BERT should pay attention to (0 or 1). In case we pass multiple sentences
    to BERT, the segment embedding will map each token to the corresponding input
    sentence (0 refers to the first sentence, 1 refers to the second sentence). Then
    the position embedding keeps track of the position of each token in the input
    sequence (0, 1, 2, etc.). We will learn that a very important hyper-parameter
    for BERT is `max_seq_length`, which defines the maximum number of input tokens
    we can pass to BERT per sample. As the maximum value for this parameter is 512,
    the position embedding is a lookup table of dimension (512, 768).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，BERT 通过查找任何输入令牌的 768 维向量表示来创建令牌嵌入。`input_id` 是指向相关令牌嵌入向量的实际ID。`input_mask`
    指定了BERT应关注的令牌（0或1）。如果我们将多个句子传递给BERT，片段嵌入将把每个令牌映射到相应的输入句子（0表示第一个句子，1表示第二个句子）。然后，位置嵌入跟踪输入序列中每个令牌的位置（0、1、2等）。我们将了解到，对于BERT来说，一个非常重要的超参数是`max_seq_length`，它定义了我们可以在每个样本中传递给BERT的最大输入令牌数。由于该参数的最大值为512，位置嵌入是一个维度为
    (512, 768) 的查找表。
- en: In a final step, BERT creates the element-wise sum of the token embedding, the
    segment embedding, and the position embedding. The resulting embedding of dimension
    (n, 768) where n stands for the number of input tokens will be passed as the input
    embedding for BERT.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 在最后一步，BERT 创建了令牌嵌入、片段嵌入和位置嵌入的逐元素求和。得到的维度为 (n, 768) 的嵌入，其中 n 表示输入令牌的数量，将作为 BERT
    的输入嵌入。
- en: Let’s use a variant of BERT called [DistilBERT](https://oreil.ly/t90gS). DistilBERT
    is a lightweight version of BERT that is 60% faster and 40% smaller while preserving
    97% of BERT’s language understanding capabilities. We use the popular Hugging
    Face Python library called Transformers to perform the transformation. To install
    this library, simply type `pip install transformers:`
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用BERT的一个变种称为[DistilBERT](https://oreil.ly/t90gS)。DistilBERT是BERT的轻量级版本，速度快60%，体积小40%，同时保留了97%的BERT语言理解能力。我们使用名为Transformers的流行Hugging
    Face Python库来执行转换。要安装此库，只需输入`pip install transformers:`
- en: '[PRE3]'
  id: totrans-81
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: 'The tokenizer performs lower-casing and parses the text into a set of words
    contained in the pre-trained DistilBERT vocabulary. The Transformers library uses
    another popular library called WordPieces to parse the text into word tokens:'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 分词器执行小写处理，并将文本解析成预训练的DistilBERT词汇表中包含的一组单词。Transformers库使用另一个流行的名为WordPieces的库来将文本解析为单词token：
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: 'Most BERT variants, including DistilBERT, have a concept of a “maximum sequence
    length” that defines the maximum number of tokens used to represent each text
    input. In our case, any reviews that end up with `max_seq_length` tokens (after
    tokenization) will be truncated down to 64\. Reviews that end up with less than
    64 tokens will be padded to a length of 64\. Empirically, we have chosen 64 for
    the maximum sequence length as 80% of our reviews are under 64 words, as we saw
    in [Chapter 5](ch05.html#explore_the_dataset), and, while not exact, the number
    of words is a good indication of the number of tokens. Below is a distribution
    of the number of words per review presented in [Chapter 5](ch05.html#explore_the_dataset):'
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数BERT变体，包括DistilBERT，都有一个“最大序列长度”的概念，它定义了用于表示每个文本输入的最大token数量。在我们的情况下，任何经过tokenization后具有`max_seq_length`
    token的评论将被截断为64个token。少于64个token的评论将被填充到长度为64个token。经验上，我们选择64作为最大序列长度，因为我们的评论中有80%少于64个单词，正如我们在[第5章](ch05.html#explore_the_dataset)中看到的，虽然不精确，但单词数是token数的一个很好的指示。以下是在[第5章](ch05.html#explore_the_dataset)中呈现的每个评论的单词数量分布：
- en: '[PRE5]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: We must use this same maximum sequence length during feature engineering and
    model training. So if we want to try a different value, we need to regenerate
    the BERT embeddings with the updated value. If we aren’t sure which value to choose,
    we may want to generate multiple versions of the embeddings using 128, 256, and
    512 as the maximum sequence length. These seem to work well for most BERT tasks.
    Larger values will likely increase model training time due to higher dimensionality.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在特征工程和模型训练期间必须使用相同的最大序列长度。因此，如果我们想尝试不同的值，我们需要使用更新后的值重新生成BERT嵌入。如果我们不确定选择哪个值，可以尝试使用128、256和512作为最大序列长度生成多个版本的嵌入。这些对大多数BERT任务效果良好。更大的值可能会增加模型训练时间，因为维度更高。
- en: 'We still have some more processing to do, however, as our DistilBERT model
    uses numeric arrays of length 64 derived from the preceding text-based tokens:'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 但是，我们仍然需要进行更多处理，因为我们的DistilBERT模型使用了从前面基于文本的token导出的长度为64的数字数组：
- en: '`input_ids`'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_ids`'
- en: The numeric ID of the token from the BERT vocabulary
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: BERT词汇表中token的数字ID
- en: '`input_mask`'
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: '`input_mask`'
- en: Specifies which tokens BERT should pay attention to (0 or 1)
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 指定BERT应该关注哪些token（0或1）
- en: '`segment_ids`'
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: '`segment_ids`'
- en: Always 0 in our case since we are doing a single-sequence NLP task (1 if we
    were doing a two-sequence NLP task such as next-sentence prediction)
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的情况下始终为0，因为我们正在执行单序列NLP任务（如果我们执行双序列NLP任务，如下一句预测，则为1）
- en: Fortunately, the Transformers tokenizer creates two of the three arrays for
    us—and even pads and truncates the arrays as needed based on the maximum sequence
    length!
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，Transformers分词器为我们创建了三个数组中的两个——并且根据最大序列长度进行了必要的填充和截断！
- en: '[PRE6]'
  id: totrans-95
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Output:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE7]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'Output:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE8]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: 'The third array, `segment_ids`, is easy to generate on our own since it contains
    all 0s in our case as we are performing a single-sequence classification NLP task.
    For two-sequence NLP tasks such as question-and-answer, the `sequence_id` is either
    0 (question) or 1 (answer):'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 第三个数组，`segment_ids`，在我们的情况下很容易生成，因为它包含了所有的0，因为我们正在执行单序列分类NLP任务。对于诸如问答的双序列NLP任务，`sequence_id`要么是0（问题），要么是1（答案）：
- en: '[PRE9]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: 'Output:'
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 输出：
- en: '[PRE10]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: Convert Features and Labels to Optimized TensorFlow File Format
  id: totrans-104
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将特征和标签转换为优化的TensorFlow文件格式
- en: The last step in our feature engineering journey is to store our newly engineered
    features in the `TFRecord` file format (*.tfrecord* file extension). `TFRecord`
    is a binary, lightweight file format optimized for TensorFlow data processing
    and is based on protocol buffers (“protobufs”). `TFRecord`s are cross-platform
    and cross-language—as well as highly efficient for data processing workloads.
    They are encoded and optimized for sequential, row-based access used during model
    training. This encoding contains features and labels—as well as any corresponding
    metadata for each example.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 我们特征工程旅程的最后一步是将我们新设计的特征存储在`TFRecord`文件格式（*.tfrecord*文件扩展名）中。`TFRecord`是一种二进制、轻量级文件格式，针对TensorFlow数据处理进行了优化，基于协议缓冲区（“protobufs”）。`TFRecord`是跨平台和跨语言的，对于数据处理工作负载非常高效。它们被编码和优化用于在模型训练期间使用的顺序、基于行的访问。此编码包含特征和标签，以及每个示例的任何相应元数据。
- en: Note
  id: totrans-106
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: The term “example” in the machine learning context means a row of data used
    for model training (including the label) or predicting (predicting the label).
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 在机器学习上下文中，“示例”一词表示用于模型训练（包括标签）或预测（预测标签）的数据行。
- en: While `TFRecord` is the file format, `tf.train.Example` and `tf.train.Feature`
    are the most common data structures stored within `TFRecord`s. `tf.train.Feature`
    stores lists of either `byte`, `float`, or `int64` using `tf.train.BytesList`,
    `FloatList`, and `Int64List`, respectively.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然`TFRecord`是文件格式，`tf.train.Example`和`tf.train.Feature`是存储在`TFRecord`中最常见的数据结构。`tf.train.Feature`使用`tf.train.BytesList`、`FloatList`和`Int64List`分别存储`byte`、`float`或`int64`的列表。
- en: 'Here is the code to convert our features into `TFRecord`s using the TensorFlow
    API:'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 这是使用TensorFlow API将我们的特征转换为`TFRecord`的代码：
- en: '[PRE11]'
  id: totrans-110
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: '`tf.train.Example.SerializeToString()` produces a serialized, binary, and human-unreadable
    string, as follows:'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`tf.train.Example.SerializeToString()`生成一个序列化的、二进制的、不可读的字符串，如下所示：'
- en: '[PRE12]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: Scale Feature Engineering with SageMaker Processing Jobs
  id: totrans-113
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用SageMaker处理作业扩展特征工程
- en: Up until now, we’ve been working in a SageMaker Notebook on a sample of the
    dataset. Let’s move our custom Python code into a SageMaker Processing Job and
    scale our feature engineering to all 150 million reviews in our dataset. SageMaker
    Processing Jobs will parallelize our custom scripts (aka “Script Mode”) or Docker
    images (aka “Bring Your Own Container”) over many SageMaker instances in a cluster,
    as shown in [Figure 6-10](#sagemaker_processing_jobs_can_paralleli).
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们一直在SageMaker笔记本上处理数据集的一个样本。现在让我们把我们的自定义Python代码移到SageMaker处理作业中，并将我们的特征工程扩展到数据集中的所有1.5亿条评论。SageMaker处理作业将我们的自定义脚本（即“脚本模式”）或Docker镜像（即“自定义容器”）并行化在许多SageMaker实例上，如[图6-10](#sagemaker_processing_jobs_can_paralleli)所示。
- en: '![](assets/dsaw_0610.png)'
  id: totrans-115
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0610.png)'
- en: Figure 6-10\. SageMaker Processing Jobs can parallelize code and Docker images
    over many SageMaker instances in a cluster.
  id: totrans-116
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-10\. SageMaker处理作业可以在集群中的多个SageMaker实例上并行化代码和Docker镜像。
- en: In a later chapter, we will automate this step with an end-to-end pipeline.
    For now, let’s just focus on scaling our feature engineering step to a SageMaker
    cluster using Processing Jobs.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 在后面的章节中，我们将使用端到端的流水线自动化这一步骤。现在，让我们把注意力集中在使用处理作业将我们的特征工程扩展到SageMaker集群上。
- en: Transform with scikit-learn and TensorFlow
  id: totrans-118
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用scikit-learn和TensorFlow进行转换
- en: Let’s balance, split, and transform the entire dataset across a cluster using
    TensorFlow, scikit-learn, BERT, and SageMaker Processing Jobs, as shown in [Figure 6-11](#transform_raw_text_into_bert_embeddings).
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们使用TensorFlow、scikit-learn、BERT和SageMaker处理作业在集群中平衡、拆分和转换整个数据集，如[图6-11](#transform_raw_text_into_bert_embeddings)所示。
- en: '![](assets/dsaw_0611.png)'
  id: totrans-120
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0611.png)'
- en: Figure 6-11\. Transform raw text into BERT embeddings with scikit-learn and
    SageMaker Processing Jobs.
  id: totrans-121
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-11\. 使用scikit-learn和SageMaker处理作业将原始文本转换为BERT嵌入。
- en: 'First we configure the scikit-learn Processing Job with the version of scikit-learn,
    the instance type, and number of instances for our cluster. Since the transformations
    are stateless, the more instances we use, the faster the processing will happen.
    Note that we are only using scikit-learn to balance and split the data. The heavy
    lifting is done using TensorFlow and Transformers:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们配置了scikit-learn处理作业的版本、实例类型和集群中的实例数量。由于变换是无状态的，我们使用的实例越多，处理速度就越快。请注意，我们仅使用scikit-learn来平衡和拆分数据。重要的工作是使用TensorFlow和Transformers完成的：
- en: '[PRE13]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: Tip
  id: totrans-124
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: We can specify `instance_type='local'` in the SageMaker Processing Job to run
    the script either inside our notebook or on our local laptop. This lets us “locally”
    run the processing job on a small subset of data in a notebook before launching
    a full-scale SageMaker Processing Job.
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: 在SageMaker处理作业中，我们可以指定`instance_type='local'`来在笔记本内或本地笔记本上运行脚本。这使我们可以在笔记本上的小数据子集上“本地”运行处理作业，然后再启动全面的SageMaker处理作业。
- en: 'Next, we start the SageMaker Processing Job by specifying the location of the
    transformed features and sharding the data across the two instances in our Processing
    Job cluster to reduce the time needed to transform the data. We specify the S3
    location of the input dataset and various arguments, such as the train, validation,
    and test split percentages. We also provide the `max_seq_length` that we chose
    for BERT:'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们通过指定转换特征的位置并将数据分片到我们处理作业集群中的两个实例，以减少转换数据所需的时间来启动SageMaker处理作业。我们指定输入数据集的S3位置和各种参数，如训练、验证和测试数据集的分割百分比。我们还提供了我们为BERT选择的`max_seq_length`：
- en: '[PRE14]'
  id: totrans-127
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: 'When the job completes, we retrieve the S3 output locations as follows:'
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 作业完成后，我们按如下方式检索S3输出位置：
- en: '[PRE15]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: Transform with Apache Spark and TensorFlow
  id: totrans-130
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 使用Apache Spark和TensorFlow进行转换
- en: Apache Spark is a powerful data processing and feature transformation engine
    supported by SageMaker Processing Jobs. While Apache Spark does not support BERT
    natively, we can use the Python-based BERT Transformers library within a PySpark
    application to scale our BERT Transformations across a distributed Spark cluster.
    In this case, we are using Spark as just a distributed processing engine and Transformers
    as just another Python library installed in the cluster, as shown in Figures [6-12](#apache_spark_cluster_with_multiple_libr)
    and [6-13](#transform_raw_text_into_bert_embeddin).
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark是由SageMaker处理作业支持的强大数据处理和特征转换引擎。虽然Apache Spark本身不原生支持BERT，但我们可以在PySpark应用程序中使用基于Python的BERT
    Transformers库来在分布式Spark集群上扩展我们的BERT转换。在这种情况下，我们将Spark仅用作分布式处理引擎，Transformers则作为集群中安装的另一个Python库，如图[6-12](#apache_spark_cluster_with_multiple_libr)和[6-13](#transform_raw_text_into_bert_embeddin)所示。
- en: '![](assets/dsaw_0612.png)'
  id: totrans-132
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0612.png)'
- en: Figure 6-12\. Apache Spark cluster with multiple popular libraries installed,
    including TensorFlow and BERT.
  id: totrans-133
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-12\. 安装了多个流行库的Apache Spark集群，包括TensorFlow和BERT。
- en: The Apache Spark ML library includes a highly parallel, distributed implementation
    of term frequency–inverse document frequency (TF-IDF) for text-based feature engineering.
    TF-IDF, dating back to the 1980s, requires a stateful pre-training step to count
    the term frequencies and build up a “vocabulary” on the given dataset. This limits
    TF-IDF’s ability to learn a broader language model outside of the given dataset.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: Apache Spark ML库包括文本特征工程的高度并行、分布式实现的词频-逆文档频率（TF-IDF）。TF-IDF可以追溯到1980年代，需要一个有状态的预训练步骤来计算术语频率并在给定数据集上建立“词汇表”。这限制了TF-IDF在给定数据集之外学习更广泛的语言模型的能力。
- en: On the other hand, BERT has been pre-trained on millions of documents and generally
    performs better on our natural-language dataset than TF-IDF, so we will use BERT
    for the feature engineering tasks presented here.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 另一方面，BERT已在数百万文档上进行了预训练，并且通常在我们的自然语言数据集上比TF-IDF表现更好，因此我们将在这里使用BERT进行呈现的特征工程任务。
- en: '![](assets/dsaw_0613.png)'
  id: totrans-136
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0613.png)'
- en: Figure 6-13\. Transform raw text into BERT embeddings with Apache Spark.
  id: totrans-137
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-13\. 使用Apache Spark将原始文本转换为BERT嵌入。
- en: If we prefer to use Apache Spark because we already have a Spark-based feature
    engineering pipeline, we can spin up a cloud-native, serverless, pay-for-what-you-use
    Apache Spark cluster to create the BERT vectors from the raw `review_body` data
    using SageMaker Processing Jobs.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们喜欢使用Apache Spark，因为我们已经有一个基于Spark的特征工程管道，我们可以启动一个云原生、无服务器、按使用付费的Apache Spark集群来使用SageMaker处理作业从原始`review_body`数据创建BERT向量。
- en: 'We just need to provide our PySpark script, specify the instance type, and
    decide on the cluster instance count—SageMaker will run our Spark job on the cluster.
    Since Spark performs better with more RAM, we use a high-RAM `r5` instance type:'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 我们只需提供我们的PySpark脚本，指定实例类型，并决定集群实例计数 - SageMaker将在集群上运行我们的Spark作业。由于Spark在更多内存的情况下表现更佳，我们使用高内存`r5`实例类型：
- en: '[PRE16]'
  id: totrans-140
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: 'Let’s run the Processing Job. Since Apache Spark efficiently reads and writes
    to S3 directly, we don’t need to specify the usual `ProcessingInput` and `ProcessingOutput`
    parameters to our `run()` function. Instead, we use the `arguments` parameter
    to pass in four S3 locations: one for the raw TSV files and the three S3 locations
    for the generated BERT vectors for the train, validation, and test splits. We
    also pass the split percentages and `max_seq_length` for BERT:'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们运行处理作业。由于Apache Spark能够直接读写S3，我们不需要为我们的`run()`函数指定通常的`ProcessingInput`和`ProcessingOutput`参数。相反，我们使用`arguments`参数传入四个S3位置：一个用于原始TSV文件，另外三个用于训练、验证和测试集的生成BERT向量。我们还传递了BERT的分割百分比和`max_seq_length`：
- en: '[PRE17]'
  id: totrans-142
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'The preceding code runs in the notebook and launches the *preprocess-spark-text-to-bert.py*
    script on the SageMaker Processing Job Cluster running Apache Spark. The following
    code is a snippet from this PySpark script:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 上述代码在笔记本中运行，并在运行Apache Spark的SageMaker处理作业集群上启动*preprocess-spark-text-to-bert.py*脚本。以下代码是此PySpark脚本的片段：
- en: '[PRE18]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: 'Here is the Spark user-defined function (UDF) to transform the raw text into
    BERT embeddings using the Transformers Python library:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 这是Spark用户定义函数（UDF），用于使用Transformers Python库将原始文本转换为BERT嵌入：
- en: '[PRE19]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'Here is the Spark code that invokes the UDF on each worker in the cluster.
    Note that we are preparing to write a `TFRecord`, so we are setting up a PySpark
    schema that matches the desired `TFRecord` format:'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 这是调用集群中每个工作节点上的UDF的Spark代码。请注意，我们准备写入`TFRecord`，因此我们正在设置与所需`TFRecord`格式匹配的PySpark模式：
- en: '[PRE20]'
  id: totrans-148
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'Next, we split the data into train, validation, and test and save the splits
    in S3 in the `TFRecord` format:'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将数据分成训练、验证和测试集，并以`TFRecord`格式保存在S3中：
- en: '[PRE21]'
  id: totrans-150
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: Note
  id: totrans-151
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 注意
- en: We are using `format('tfrecord')` from an open source library that implements
    the Apache Spark `DataFrameReader` and `DataFrameWriter` interfaces for `TFRecord`.
    References to this library are in this book’s GitHub repository.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 我们正在使用开源库中实现的Apache Spark `DataFrameReader`和`DataFrameWriter`接口提供的`format('tfrecord')`。关于这个库的参考资料可以在本书的GitHub仓库中找到。
- en: Share Features Through SageMaker Feature Store
  id: totrans-153
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 通过SageMaker特征存储共享特征
- en: Feature engineering requires intuition, patience, trial, and error. As more
    teams utilize AI and machine learning to solve business use cases, the need arises
    for a centralized, discoverable, and reusable repository of features. This type
    of repository is called a *Feature Store*.
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 特征工程需要直觉、耐心、试验和错误。随着越来越多的团队利用AI和机器学习解决业务用例，需要一个集中、可发现和可重复使用的特征库。这种类型的库称为*特征存储*。
- en: Feature stores are data lakes for machine learning features. Since features
    sometimes require heavy compute processing, as we demonstrated earlier with our
    BERT features using SageMaker Processing Jobs, we would like to store and reuse
    these features, if possible, throughout the organization.
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储是用于机器学习特征的数据湖。由于特征有时需要进行大量计算处理，例如我们之前使用SageMaker Processing Jobs演示的BERT特征，我们希望在整个组织中存储和重复使用这些特征，如果可能的话。
- en: It is likely that different transformations are needed for the feature store
    targeted at machine learning workflows with SageMaker and the data warehouse targeted
    at business intelligence reports and dashboards with Amazon Redshift. For example,
    we would store our BERT embeddings in the feature store, while we store cleaned
    and enriched data in our data warehouse, as shown in [Figure 6-14](#relationship_between_feature_storecomma).
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 针对面向SageMaker的机器学习工作流程和面向Amazon Redshift的业务智能报表和仪表板的特征存储，可能需要不同的转换。例如，我们会将BERT嵌入存储在特征存储中，而将清洁和增强数据存储在我们的数据仓库中，如[图6-14](#relationship_between_feature_storecomma)所示。
- en: '![](assets/dsaw_0614.png)'
  id: totrans-157
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0614.png)'
- en: Figure 6-14\. Relationship between feature store, data lake, and data warehouse.
  id: totrans-158
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-14\. 特征存储、数据湖和数据仓库之间的关系。
- en: Instead of building a feature store ourselves, we can leverage a managed feature
    store through Amazon SageMaker. SageMaker Feature Store can store both offline
    and online features. Offline features are stored in repositories optimized for
    high-throughput and batch-retrieval workloads such as model training. Online features
    are stored in repositories optimized for low-latency and real-time requests such
    as model inference.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以通过Amazon SageMaker利用托管的特征存储，而不是自己构建一个特征存储。SageMaker特征存储可以存储离线和在线特征。离线特征存储在针对高吞吐量和批量检索工作负载进行优化的存储库中，例如模型训练。在线特征存储在针对低延迟和实时请求进行优化的存储库中，例如模型推断。
- en: Since we spent a fair amount of time generating our BERT features, we’d like
    to share them with other teams in our organization. Perhaps these other teams
    can discover new and improved combinations of our features that we never explored.
    We’d like to use our feature store to help us safely “travel in time” and avoid
    leakage.
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们花了相当多的时间生成我们的BERT特征，我们希望与我们组织中的其他团队共享它们。也许这些团队可以发现我们从未探索过的特征的新组合和改进。我们希望使用我们的特征存储来帮助我们安全地“穿越时间”，避免泄漏。
- en: Feature stores can cache frequently-accessed features into memory to reduce
    model-training times. They can also provide governance and access control to regulate
    and audit our features. Last, a feature store can provide consistency between
    model training and inference by ensuring the same features are present for both
    batch training and real-time predicting.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 特征存储可以将频繁访问的特征缓存在内存中，以减少模型训练时间。它还可以提供治理和访问控制，以规范和审核我们的特征。最后，特征存储可以通过确保批量训练和实时预测中存在相同的特征来确保模型训练和推断之间的一致性。
- en: Ingest Features into SageMaker Feature Store
  id: totrans-162
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 将特征注入到SageMaker特征存储中
- en: 'Assume we have the following data frame, `df_records`, which contains the processed
    BERT features using DistilBERT, with a maximum sequence length of 64:'
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有以下数据框`df_records`，其中包含使用DistilBERT处理的BERT特征，最大序列长度为64：
- en: '| input_ids | input_mask | segment_ids | label_id | review_id | date | label
    | split_type |'
  id: totrans-164
  prefs: []
  type: TYPE_TB
  zh: '| input_ids | input_mask | segment_ids | label_id | review_id | date | label
    | split_type |'
- en: '| --- | --- | --- | --- | --- | --- | --- | --- |'
  id: totrans-165
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- | --- | --- |'
- en: '| [101, 1045, 2734, 2019, 1000, 3424, 23350, 100... | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 4 | ABCD12345 | 2021-01-30T20:55:33Z | 5 | train |'
  id: totrans-166
  prefs: []
  type: TYPE_TB
  zh: '| [101, 1045, 2734, 2019, 1000, 3424, 23350, 100... | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 4 | ABCD12345 | 2021-01-30T20:55:33Z | 5 | train |'
- en: '| [101, 1996, 3291, 2007, 10777, 23663, 2003, 20.. | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 2 | EFGH12345 | 2021-01-30T20:55:33Z | 3 | train |'
  id: totrans-167
  prefs: []
  type: TYPE_TB
  zh: '| [101, 1996, 3291, 2007, 10777, 23663, 2003, 20.. | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 2 | EFGH12345 | 2021-01-30T20:55:33Z | 3 | train |'
- en: '| [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499... | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 0 | IJKL2345 | 2021-01-30T20:55:33Z | 1 | train |'
  id: totrans-168
  prefs: []
  type: TYPE_TB
  zh: '| [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499... | [1, 1, 1, 1, 1, 1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...
    | 0 | IJKL2345 | 2021-01-30T20:55:33Z | 1 | train |'
- en: 'We will now ingest the BERT features, `df_records`, into our feature store
    with the feature group name `reviews_distilbert_64_max_seq_length`:'
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们将BERT特征`df_records`注入到特征存储中，特征组名称为`reviews_distilbert_64_max_seq_length`：
- en: '[PRE22]'
  id: totrans-170
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'We need to specify the unique record identifier column, `review_id`, in our
    case. In addition, we need to specify an event time that corresponds to the time
    a record was created or updated in our feature store. In our case, we will generate
    a timestamp at the time of ingestion. All records must have a unique ID and event
    time:'
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要在我们的情况下指定唯一的记录标识列`review_id`。此外，我们需要指定一个事件时间，该事件时间对应于在特征存储中创建或更新记录的时间。在我们的情况下，我们将在注入时生成一个时间戳。所有记录必须具有唯一ID和事件时间：
- en: '[PRE23]'
  id: totrans-172
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'The SageMaker Feature Store Python SDK will auto-detect the data schema based
    on input data. Here is the detected schema:'
  id: totrans-173
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker特征存储Python SDK将根据输入数据自动检测数据模式。以下是检测到的模式：
- en: '[PRE24]'
  id: totrans-174
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'In order to create the feature group, we also need to specify the S3 bucket
    to store the `df_records` as well as a flag to enable the online feature store
    option for inference:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 为了创建特征组，我们还需要指定用于存储`df_records`的S3存储桶以及一个标志，以启用推断的在线特征存储选项：
- en: '[PRE25]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Now let’s ingest the data into the feature store. The data is ingested into
    both the offline and online repositories unless we specify one or the other:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们将数据注入到特征存储中。数据被注入到离线和在线存储库中，除非我们指定其中之一：
- en: '[PRE26]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: Retrieve Features from SageMaker Feature Store
  id: totrans-179
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 从SageMaker特征存储检索特征
- en: 'We can retrieve features from the offline feature store using Athena. We can
    use these features in our model training, for example:'
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用Athena从离线特征存储中检索特征。我们可以在模型训练中使用这些特征，例如：
- en: '[PRE27]'
  id: totrans-181
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Here is the output from the feature store query showing our BERT features:'
  id: totrans-182
  prefs: []
  type: TYPE_NORMAL
  zh: 这是来自特征存储查询的输出，显示我们的BERT特征：
- en: '| review_body | input_ids | input_mask | segment_ids | label_id |'
  id: totrans-183
  prefs: []
  type: TYPE_TB
  zh: '| review_body | input_ids | input_mask | segment_ids | label_id |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-184
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| I needed an “antivirus” application and know t... | [101, 1996, 3291, 2007,
    10777, 23663, 2003, 20... | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
    | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 2 |'
  id: totrans-185
  prefs: []
  type: TYPE_TB
  zh: '| 我需要一个“防病毒”应用程序并知道它有。 | [101, 1996, 3291, 2007, 10777, 23663, 2003, 20...
    | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, ... | 2 |'
- en: '| The problem with ElephantDrive is that it requ... | [101, 6659, 1010, 3904,
    1997, 2026, 9537, 2499... | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...
    | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 0 |'
  id: totrans-186
  prefs: []
  type: TYPE_TB
  zh: '| ElephantDrive 的问题在于它要... | [101, 6659, 1010, 3904, 1997, 2026, 9537, 2499...
    | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, ... | 0 |'
- en: '| Terrible, none of my codes work. | [101, 1045, 2734, 2019, 1000, 3424, 23350,
    100... | [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, 0, 0, 0, 0, 0, ... | 4 |'
  id: totrans-187
  prefs: []
  type: TYPE_TB
  zh: '| 我的代码一个都不好用。 | [101, 1045, 2734, 2019, 1000, 3424, 23350, 100... | [1, 1,
    1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ... | [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,
    0, 0, 0, 0, ... | 4 |'
- en: Note that the `label_id` is 0-indexed. In this case, the `label_id` of 0 corresponds
    to the `star_rating` class 1, 4 represents `star_rating` 5, etc.
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`label_id` 是从 0 开始索引的。在这种情况下，`label_id` 为 0 对应于 `star_rating` 类别 1，4 表示 `star_rating`
    为 5 等。
- en: 'We can also query a specific feature in our feature group to use for model
    predictions by its record identifier as follows:'
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以通过其记录标识符查询特定特征组中的特征，用于模型预测：
- en: '[PRE28]'
  id: totrans-190
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: Ingest and Transform Data with SageMaker Data Wrangler
  id: totrans-191
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 SageMaker Data Wrangler 进行数据摄取和转换
- en: Data Wrangler is SageMaker native, focuses on machine learning use cases, and
    preserves artifact lineage across the full model development life cycle (MDLC),
    including data ingestion, feature engineering, model training, model optimization,
    and model deployment. In addition to analyzing our data in [Chapter 5](ch05.html#explore_the_dataset),
    SageMaker Data Wrangler prepares and transforms our machine-learning features
    with support for over 300+ built-in transformations—as well as custom SQL, pandas,
    and Apache Spark code. Data Wrangler is used for many purposes, such as converting
    column data types, imputing missing data values, splitting datasets into train/validation/test,
    scaling and normalizing columns, and dropping columns.
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: Data Wrangler 是 SageMaker 原生的，专注于机器学习用例，并跨完整的模型开发生命周期（MDLC）保留工件血统，包括数据摄取、特征工程、模型训练、模型优化和模型部署。除了在
    [第5章](ch05.html#explore_the_dataset) 中分析我们的数据外，SageMaker Data Wrangler 还准备并转换我们的机器学习特征，支持超过
    300+ 内置转换，以及自定义 SQL、pandas 和 Apache Spark 代码。Data Wrangler 用于许多目的，例如转换列数据类型、填充缺失数据值、将数据集分割为训练/验证/测试集、缩放和归一化列以及删除列。
- en: The data transformation steps are stored as a Data Wrangler *.flow* definition
    file and are reused as new data arrives into the system.
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: 数据转换步骤存储为 Data Wrangler 的 *.flow* 定义文件，并在新数据到达系统时重新使用。
- en: 'We can also export the *.flow* Data Wrangler transformation to a SageMaker
    Processing Job, Pipeline, Feature Store, or raw Python script. Let’s export our
    Data Wrangler flow to a SageMaker Pipeline to automate the transformation and
    track the lineage with SageMaker Lineage. We will dive deeper into lineage in
    the next section and SageMaker Pipelines in [Chapter 10](ch10.html#pipelines_and_mlops).
    Here is an excerpt of the code that was generated by Data Wrangler when we export
    the flow to a SageMaker Pipeline:'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以将 *.flow* 数据处理转换导出为 SageMaker 处理作业、流水线、特征存储或原始 Python 脚本。让我们将我们的 Data Wrangler
    流导出到 SageMaker 流水线中，以自动化转换并使用 SageMaker Lineage 追踪其血统。我们将在下一节深入探讨血统，以及在 [第10章](ch10.html#pipelines_and_mlops)
    中的 SageMaker 流水线。这是由 Data Wrangler 生成的代码摘录，当我们将流导出到 SageMaker 流水线时：
- en: '[PRE29]'
  id: totrans-195
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: Track Artifact and Experiment Lineage with Amazon SageMaker
  id: totrans-196
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用 Amazon SageMaker 跟踪工件和实验血统
- en: Humans are naturally curious. When presented with an object, people will likely
    want to know how that object was created. Now consider an object as powerful and
    mysterious as a predictive model learned by a machine. We naturally want to know
    how this model was created. Which dataset was used? Which hyper-parameters were
    chosen? Which other hyper-parameters were explored? How does this version of the
    model compare to the previous version? All of these questions can be answered
    by SageMaker ML Lineage Tracking and SageMaker Experiments.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 人类天生好奇。当展示一个物体时，人们可能想知道该物体是如何制造的。现在考虑一下由机器学习学习的预测模型这样一个强大而神秘的物体。我们自然想知道这个模型是如何创建的。使用了哪个数据集？选择了哪些超参数？探索了哪些其他超参数？这个模型版本与上一个版本相比如何？所有这些问题都可以通过
    SageMaker ML Lineage Tracking 和 SageMaker Experiments 来解答。
- en: As a best practice, we should track the lineage of data transformations used
    in our overall MDLC from feature engineering to model training to model deployment.
    SageMaker Data Wrangler automatically tracks the lineage of any data it ingests
    or transforms. Additionally SageMaker Processing Jobs, Training Jobs, and Endpoints
    track their lineage. We inspect the lineage at any time using either the SageMaker
    Studio IDE or the SageMaker Lineage API directly. For each step in our workflow,
    we store the input artifacts, the action, and the generated output artifacts.
    We can use the Lineage API to inspect the lineage graph and analyze the relationships
    between steps, actions, and artifacts.
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 作为最佳实践，我们应该跟踪从特征工程到模型训练再到模型部署的整体MDLC的数据转换的血统。SageMaker数据整理器自动跟踪其摄取或转换的任何数据的血统。此外，SageMaker处理作业、训练作业和端点也跟踪它们的血统。我们可以随时使用SageMaker
    Studio IDE或直接使用SageMaker血统API检查血统。对于我们工作流程中的每个步骤，我们存储输入工件、操作和生成的输出工件。我们可以使用血统API检查血统图并分析步骤、操作和工件之间的关系。
- en: We can leverage the SageMaker Lineage API and lineage graphs for a number of
    purposes, such as maintaining a history of model experiments, sharing work with
    colleagues, reproducing workflows to enhance the model, tracing which datasets
    were used to train each model in production, determining where the model has been
    deployed, and complying with regulatory standards and audits.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以利用SageMaker血统API和血统图来达到多种目的，例如维护模型实验的历史记录，与同事分享工作，重现工作流以增强模型，在生产中追踪用于训练每个模型的数据集，确定模型已部署的位置，以及遵守法规标准和审计要求。
- en: Understand Lineage-Tracking Concepts
  id: totrans-200
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 理解血统追踪概念
- en: 'The SageMaker Lineage Tracking API utilizes the following key concepts:'
  id: totrans-201
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker血统追踪API利用以下关键概念：
- en: Lineage graph
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 血统图
- en: The connected graph tracing our machine learning workflow end to end.
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: 连接的图追踪我们的机器学习工作流程端到端。
- en: Artifacts
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 工件
- en: Represents a URI addressable object or data. Artifacts are typically inputs
    or outputs to actions.
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 代表URI可寻址对象或数据。工件通常是操作的输入或输出。
- en: Actions
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 操作
- en: Represents an action taken, such as a computation, transformation, or job.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 表示所采取的操作，如计算、转换或作业。
- en: Contexts
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 上下文
- en: Provides a method to logically group other entities.
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: 提供一种逻辑上组织其他实体的方法。
- en: Associations
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: 关联
- en: A directed edge in the lineage graph that links two entities. Associations can
    be of type `Produced`, `DerivedFrom`, `AssociatedWith`, or `ContributedTo`.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 在血统图中连接两个实体的有向边。关联可以是`Produced`、`DerivedFrom`、`AssociatedWith`或`ContributedTo`类型。
- en: Lineage traversal
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: 血统遍历
- en: Starting from an arbitrary point, trace the lineage graph to discover and analyze
    relationships between steps in the workflow either upstream or downstream.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 从任意点开始，追踪血统图以发现和分析工作流中步骤之间的关系，无论是上游还是下游。
- en: Experiments
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 实验
- en: Experiment entities including trials and trial components are part of the lineage
    graph. They are associated with SageMaker Lineage core components, including artifacts,
    actions, and contexts.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 包括试验和试验组件在内的实验实体是血统图的一部分。它们与SageMaker血统核心组件相关联，包括工件、操作和上下文。
- en: SageMaker automatically creates the lineage tracking entities for every step
    in a SageMaker Pipeline, including SageMaker Processing Jobs, Training Jobs, Models,
    Model Packages, and Endpoints. Each pipeline step is associated with input artifacts,
    actions, output artifacts, and metadata. We will continue to build up our lineage
    graph as we train, tune, and deploy our model in Chapters [7](ch07.html#train_your_first_model),
    [8](ch08.html#train_and_optimize_models_at_scale), and [9](ch09.html#deploy_models_to_production).
    We will then tie everything together in a pipeline with a full end-to-end lineage
    graph in [Chapter 10](ch10.html#pipelines_and_mlops).
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker自动为SageMaker管道中的每个步骤创建血统跟踪实体，包括SageMaker处理作业、训练作业、模型、模型包和端点。每个管道步骤与输入工件、操作、输出工件和元数据相关联。我们将在第[7](ch07.html#train_your_first_model)、[8](ch08.html#train_and_optimize_models_at_scale)和[9](ch09.html#deploy_models_to_production)章继续建立我们的血统图，然后在[第10章](ch10.html#pipelines_and_mlops)中将所有内容汇总到一个完整的端到端血统图中。
- en: Show Lineage of a Feature Engineering Job
  id: totrans-217
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 显示特征工程作业的血统
- en: 'We can show the lineage information that has been captured for the SageMaker
    Processing Job used to create the BERT embeddings from the raw review text:'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以展示已捕获的SageMaker处理作业用于从原始评审文本创建BERT嵌入的血统信息：
- en: '[PRE30]'
  id: totrans-219
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: 'The output should look similar to this:'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 输出应该类似于这样：
- en: '| Name/Source | Direction | Type | Association Type | Lineage Type |'
  id: totrans-221
  prefs: []
  type: TYPE_TB
  zh: '| 名称/来源 | 方向 | 类型 | 关联类型 | 衍生类型 |'
- en: '| --- | --- | --- | --- | --- |'
  id: totrans-222
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- |'
- en: '| s3://../amazon-reviews-pds/tsv/ | Input | DataSet | ContributedTo | artifact
    |'
  id: totrans-223
  prefs: []
  type: TYPE_TB
  zh: '| s3://../amazon-reviews-pds/tsv/ | 输入 | 数据集 | 贡献于 | 工件 |'
- en: '| 68331.../sagemaker-scikit-learn:0.20.0-cpu-py3 | Input | Image | ContributedTo
    | artifact |'
  id: totrans-224
  prefs: []
  type: TYPE_TB
  zh: '| 68331.../sagemaker-scikit-learn:0.20.0-cpu-py3 | 输入 | 镜像 | 贡献于 | 工件 |'
- en: '| s3://.../output/bert-test | Output | DataSet | Produced | artifact |'
  id: totrans-225
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-test | 输出 | 数据集 | 产出 | 工件 |'
- en: '| s3://.../output/bert-validation | Output | DataSet | Produced | artifact
    |'
  id: totrans-226
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-validation | 输出 | 数据集 | 产出 | 工件 |'
- en: '| s3://.../output/bert-train | Output | DataSet | Produced | artifact |'
  id: totrans-227
  prefs: []
  type: TYPE_TB
  zh: '| s3://.../output/bert-train | 输出 | 数据集 | 产出 | 工件 |'
- en: SageMaker Lineage Tracking automatically recorded the input data (TSVs), output
    data (`TFRecord`s), and SageMaker container image. The association type shows
    that the inputs have `ContributedTo` this pipeline step.
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker Lineage Tracking自动记录了输入数据（TSV）、输出数据（TFRecord）和SageMaker容器镜像。关联类型显示输入数据对该流水线步骤的`ContributedTo`。
- en: The generated training data split into train, validation, and test datasets
    has been recorded as outputs of this step. The association type correctly classifies
    them as `Produced` artifacts of this step.
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 生成的训练数据分为训练、验证和测试数据集，已记录为此步骤的输出。关联类型正确地将它们分类为此步骤的`Produced`工件。
- en: Understand the SageMaker Experiments API
  id: totrans-230
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 了解SageMaker实验API
- en: 'SageMaker Experiments is a valuable tool in our data science toolkit that gives
    us deep insight into the model training and tuning process. With Experiments,
    we can track, organize, visualize, and compare our AI and machine learning models
    across all stages of the MDLC, including feature engineering, model training,
    model tuning, and model deploying. Experiments are seamlessly integrated with
    SageMaker Studio, Processing Jobs, Training Jobs, and Endpoints. The SageMaker
    Experiments API is made up of the following key abstractions:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: SageMaker实验是我们数据科学工具包中的一个宝贵工具，可以深入了解模型训练和调整过程。通过实验，我们可以跟踪、组织、可视化和比较我们的AI和机器学习模型在MDLC的所有阶段（包括特征工程、模型训练、模型调整和模型部署）中的表现。实验与SageMaker
    Studio、处理作业、训练作业和端点无缝集成。SageMaker实验API由以下关键抽象组成：
- en: Experiment
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 实验
- en: A collection of related Trials. Add Trials to an Experiment that we wish to
    compare together.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 一组相关的试验。将试验添加到一个我们希望进行比较的实验中。
- en: Trial
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 试验
- en: A description of a multistep machine learning workflow. Each step in the workflow
    is described by a Trial Component.
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 多步机器学习工作流程的描述。工作流程中的每个步骤由一个试验组件描述。
- en: Trial Component
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 试验组件
- en: A description of a single step in a machine learning workflow, for example,
    data transformation, feature engineering, model training, model evaluation, etc.
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 机器学习工作流程中的单个步骤描述，例如数据转换、特征工程、模型训练、模型评估等。
- en: Tracker
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 跟踪器
- en: A logger of information about a single Trial Component.
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 单个试验组件的信息记录器。
- en: While SageMaker Experiments are natively integrated into SageMaker, we can track
    experiments from any Jupyter notebook or Python script by using the SageMaker
    Experiments API and just a few lines of code.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 虽然SageMaker实验本质上是集成在SageMaker中的，但我们可以通过使用SageMaker实验API和几行代码，从任何Jupyter笔记本或Python脚本跟踪实验。
- en: '[Figure 6-15](#compare_training_runs_with_different_hy) shows three trials
    within a single experiment: Trials A, B, and C. All Trials reuse the same feature-engineering
    Trial Component, “Prepare A,” to train three different models using different
    hyper-parameters. Trial C provides the best accuracy, so we deploy the model and
    track the deployment Trial Component, “Deploy C.”'
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: '[图6-15](#compare_training_runs_with_different_hy)显示了单个实验中的三个试验：试验A、B和C。所有试验都重复使用相同的特征工程试验组件“准备A”，使用不同的超参数训练三个不同的模型。试验C提供了最佳精度，因此我们部署模型并跟踪部署试验组件“部署C”。'
- en: '![](assets/dsaw_0615.png)'
  id: totrans-242
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0615.png)'
- en: Figure 6-15\. Compare training runs with different hyper-parameters using SageMaker
    Experiments.
  id: totrans-243
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图6-15。使用SageMaker实验比较具有不同超参数的训练运行。
- en: 'Using the SageMaker Experiments API, we create a complete record of every step
    and hyper-parameter used to re-create Models A, B, and C. At any given point in
    time, we can determine how a model was trained, including the exact dataset and
    hyper-parameters used. This traceability is essential for auditing, explaining,
    and improving our models. We will dive deeper into tracking the model train, optimize,
    and deploy steps in Chapters [7](ch07.html#train_your_first_model), [8](ch08.html#train_and_optimize_models_at_scale),
    and [9](ch09.html#deploy_models_to_production), respectively. For now, let’s use
    the SageMaker Experiment API to track the lineage of our feature-engineering step.
    First, we create the `Experiment` as follows:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 使用SageMaker Experiments API，我们创建了每一步和超参数的完整记录，用于重新创建模型A、B和C。在任何给定时间点，我们都可以确定模型是如何训练的，包括使用的确切数据集和超参数。这种可追溯性对于审计、解释和改进我们的模型至关重要。我们将在章节[7](ch07.html#train_your_first_model)，[8](ch08.html#train_and_optimize_models_at_scale)和[9](ch09.html#deploy_models_to_production)深入探讨跟踪模型训练、优化和部署步骤。目前，让我们使用SageMaker
    Experiment API跟踪我们的特征工程步骤。首先，我们创建`Experiment`如下：
- en: '[PRE31]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Next, let’s create the `experiment_config` parameter that we will pass to the
    processor when we create our BERT embeddings. This `experiment_config` is used
    by the SageMaker Processing Job to add a new `TrialComponent` named `prepare`
    that tracks the S3 locations of the raw-review inputs as well as the transformed
    train, validation, and test output splits:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建`experiment_config`参数，当我们创建BERT嵌入时将其传递给处理器。这个`experiment_config`参数将在SageMaker处理作业中使用，用于添加一个名为`prepare`的新`TrialComponent`，用于跟踪原始评论输入和转换后的训练、验证和测试输出拆分的S3位置：
- en: '[PRE32]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'We can use the SageMaker Experiments API to display the parameters used in
    our `prepare` step, as shown in the following. We will continue to track our experiment
    lineage through model training, hyper-parameter tuning, and model deployment in
    Chapters [7](ch07.html#train_your_first_model), [8](ch08.html#train_and_optimize_models_at_scale),
    and [9](ch09.html#deploy_models_to_production), respectively:'
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用SageMaker Experiments API显示我们`prepare`步骤中使用的参数，如下所示。我们将继续通过章节[7](ch07.html#train_your_first_model)，[8](ch08.html#train_and_optimize_models_at_scale)和[9](ch09.html#deploy_models_to_production)跟踪我们的实验谱系：
- en: '[PRE33]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: '| TrialComponentName | DisplayName | max_seq​_length | train_split​_percentage
    | validation_split​_percentage | test_split​_percentage |'
  id: totrans-250
  prefs: []
  type: TYPE_TB
  zh: '| TrialComponentName | DisplayName | max_seq​_length | train_split​_percentage
    | validation_split​_percentage | test_split​_percentage |'
- en: '| --- | --- | --- | --- | --- | --- |'
  id: totrans-251
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- | --- | --- | --- |'
- en: '| bert-transformation-​2021-01-09-​062410-​pxuy | prepare | 64.0 | 0.90 | 0.05
    | 0.05 |'
  id: totrans-252
  prefs: []
  type: TYPE_TB
  zh: '| bert-transformation-​2021-01-09-​062410-​pxuy | prepare | 64.0 | 0.90 | 0.05
    | 0.05 |'
- en: Ingest and Transform Data with AWS Glue DataBrew
  id: totrans-253
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 使用AWS Glue DataBrew摄取和转换数据
- en: We can use the built-in Glue DataBrew data transformations to combine, pivot,
    or transpose the data. The sequence of applied data transformations is captured
    in a recipe that we can apply to new data as it arrives. SageMaker Data Wrangler
    is preferred over Glue DataBrew for machine learning use cases as Data Wrangler
    is integrated with SageMaker and tracks the complete lineage across all phases
    of the MDLC. While Data Wrangler focuses on the machine learning use cases and
    the data transformations can be exported as processing code, we can leverage Glue
    DataBrew for scheduled, initial data cleaning and transformations.
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用内置的Glue DataBrew数据转换工具来合并、旋转或转置数据。应用的数据转换序列被记录在一个配方中，我们可以将其应用于到达的新数据。SageMaker
    Data Wrangler优于Glue DataBrew用于机器学习用例，因为Data Wrangler与SageMaker集成，并跟踪MDLC所有阶段的完整谱系。虽然Data
    Wrangler专注于机器学习用例，数据转换可以导出为处理代码，但我们可以利用Glue DataBrew进行定期的初始数据清洗和转换。
- en: The sequence of applied data transformations is captured in a recipe that we
    can apply to new data as it arrives. While DataBrew is focused on traditional
    extract-transform-load workflows, it includes some very powerful statistical functions
    to analyze and transform data, including the text-based data in the Amazon Reviews
    Customer Dataset.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 应用的数据转换序列被记录在一个配方中，我们可以将其应用于到达的新数据。虽然DataBrew专注于传统的抽取-转换-加载工作流，但它包括一些非常强大的统计函数，用于分析和转换数据，包括亚马逊评论客户数据集中的文本数据。
- en: 'Let’s create a simple recipe to remove some unused fields from our dataset
    by creating a recipe called `amazon-reviews-dataset-recipe` in the DataBrew UI.
    After exporting *recipe.json* from the UI we can programmatically drop the columns
    using the DataBrew Python SDK. Here is the *recipe.json* that drops unused columns
    from our dataset:'
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们在DataBrew UI中创建一个简单的配方，通过创建名为`amazon-reviews-dataset-recipe`的配方来删除数据集中的一些未使用字段。从UI中导出*recipe.json*后，我们可以使用DataBrew
    Python SDK以编程方式删除列。这是从我们的数据集中删除未使用列的*recipe.json*：
- en: '[PRE34]'
  id: totrans-257
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'We need to create a DataBrew project for our dataset and recipe:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 我们需要为我们的数据集和配方创建一个DataBrew项目：
- en: '[PRE35]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now let’s call the DataBrew Python SDK to create a transformation job based
    on the *recipe.json* listed earlier:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们调用DataBrew Python SDK，基于前面列出的*recipe.json*创建一个转换作业：
- en: '[PRE36]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'We start the data transformation job as follows:'
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 我们开始数据转换作业如下：
- en: '[PRE37]'
  id: totrans-263
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: DataBrew keeps track of the lineage of each data transformation step, as shown
    in [Figure 6-16](#aws_glue_databrew_lineage_shows_the_dat).
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: DataBrew跟踪每个数据转换步骤的谱系，如[图 6-16](#aws_glue_databrew_lineage_shows_the_dat)所示。
- en: 'Once the DataBrew job completes, we have our transformed data in S3\. Here
    is a sample of the data as a pandas DataFrame:'
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: DataBrew作业完成后，我们的转换数据就在S3中。这里是作为pandas DataFrame的数据样本：
- en: '| star_rating | review_body |'
  id: totrans-266
  prefs: []
  type: TYPE_TB
  zh: '| star_rating | review_body |'
- en: '| --- | --- |'
  id: totrans-267
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| 5 | After attending a few Qigong classes, I wanted... |'
  id: totrans-268
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 参加了几节气功课程后，我想要... |'
- en: '| 4 | Krauss traces the remarkable transformation in... |'
  id: totrans-269
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Krauss追溯了出色的转变... |'
- en: '| 4 | Rebecca, a dental hygienist, receives a call a... |'
  id: totrans-270
  prefs: []
  type: TYPE_TB
  zh: '| 4 | Rebecca，一名牙科卫生师，接到一通... |'
- en: '| 5 | Good characters and plot line. I spent a pleas... |'
  id: totrans-271
  prefs: []
  type: TYPE_TB
  zh: '| 5 | 好人物和情节线。我花了一些愉快的时光... |'
- en: '![](assets/dsaw_0616.png)'
  id: totrans-272
  prefs: []
  type: TYPE_IMG
  zh: '![](assets/dsaw_0616.png)'
- en: Figure 6-16\. Glue DataBrew lineage shows the data transformation steps applied
    to the dataset.
  id: totrans-273
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 6-16\. Glue DataBrew谱系显示应用于数据集的数据转换步骤。
- en: Summary
  id: totrans-274
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: In this chapter, we explored feature engineering using a real-world example
    of transforming raw Amazon Customer Reviews into machine learning features using
    BERT and TensorFlow. We described how to use SageMaker Data Wrangler to select
    features and perform transformations on our data to prepare for model training.
    And we demonstrated how to track and analyze the lineage of transformations using
    the SageMaker Lineage and Experiment APIs. We also showed how to use Glue DataBrew
    as another option for data analysis and transformation outside of SageMaker.
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们通过一个真实的例子探讨了特征工程，将原始的亚马逊客户评论转换为机器学习特征，使用BERT和TensorFlow。我们描述了如何使用SageMaker
    Data Wrangler选择特征并对数据进行转换，以便进行模型训练的准备。我们还展示了如何使用SageMaker Lineage和Experiment API跟踪和分析转换的谱系。我们还展示了如何使用Glue
    DataBrew作为SageMaker之外的另一种数据分析和转换选项。
- en: In [Chapter 7](ch07.html#train_your_first_model), we will use these features
    to train a review-classification model to predict the `star_rating` from review
    text captured in the wild from social channels, partner websites, and other sources
    of product reviews. We will dive deep into various model-training and deep-learning
    options, including TensorFlow, PyTorch, Apache MXNet, and even Java! We demonstrate
    how to profile training jobs, detect model biases, and explain model predictions
    with SageMaker Debugger.
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 在[第7章](ch07.html#train_your_first_model)中，我们将使用这些特征训练一个评估模型，以预测从社交渠道、合作伙伴网站和其他产品评论来源捕获的评论文本中的`star_rating`。我们将深入探讨各种模型训练和深度学习选项，包括TensorFlow、PyTorch、Apache
    MXNet甚至Java！我们演示了如何分析训练作业的配置文件、检测模型偏差，并通过SageMaker Debugger解释模型预测。
