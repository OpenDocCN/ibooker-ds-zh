- en: Chapter 3\. Mapper Transformations
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 第三章 映射器转换
- en: This chapter will introduce the most common Spark mapper transformations through
    simple working examples. Without a clear understanding of transformations, it
    is hard to use them in a proper and meaningful way to solve any data problem.
    We will examine mapper transformations in the context of RDD data abstractions.
    A mapper is a function that is used to process all the elements of a source RDD
    and generate a target RDD. For example, a mapper can transform a `String` record
    into tuples, (key, value) pairs, or whatever your desired output may be. Informally,
    we can say that a mapper transforms a source `RDD[V]` into a target `RDD[T]`,
    where `V` and `T` are the data types of the source and target RDDs, respectively.
    You may apply mapper transformations to DataFrames as well, by either applying
    DataFrame functions (using `select()` and UDFs) to all rows or converting your
    DataFrame (a table of rows and columns) to an RDD and then using Spark’s mapper
    transformations.
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章将通过简单的工作示例介绍最常见的Spark映射器转换。如果不清楚转换的含义，很难正确有效地解决任何数据问题。我们将在RDD数据抽象的背景下检查映射器转换。映射器是一个用于处理源RDD的所有元素并生成目标RDD的函数。例如，映射器可以将`String`记录转换为元组（key,
    value）对或其他您希望的输出形式。非正式地说，映射器将源`RDD[V]`转换为目标`RDD[T]`，其中`V`和`T`分别是源和目标RDD的数据类型。您也可以通过将DataFrame（行和列的表）转换为RDD，然后使用Spark的映射器转换，或者直接应用DataFrame函数（使用`select()`和UDF）来对所有行应用映射器转换。
- en: Data Abstractions and Mappers
  id: totrans-2
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 数据抽象和映射器
- en: Spark has many transformations and actions, but this chapter is dedicated to
    explaining the ones that are most often used in building Spark applications. Spark’s
    simple and powerful mapper transformations enable us to perform ETL operations
    in a simple way.
  id: totrans-3
  prefs: []
  type: TYPE_NORMAL
  zh: Spark有许多转换和操作，但本章专注于解释在构建Spark应用程序中最常用的那些。Spark简单而强大的映射器转换使我们能够以简单的方式执行ETL操作。
- en: 'As I’ve mentioned, the RDD is an important data abstraction in Spark that is
    suitable for unstructured and semi-structured data: an immutable, partitioned
    collection of elements that can be operated on in parallel. The RDD is a lower-level
    API than Spark’s other main data abstraction, the DataFrame (see [Figure 3-1](#sparks_data_abstractions)).
    In an RDD, each element may have a data type `T`, denoted by `RDD[T]`.'
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我之前提到的，RDD是Spark中的一个重要数据抽象，适用于非结构化和半结构化数据：它是一个不可变的、分区的元素集合，可以并行操作。RDD是比Spark的另一个主要数据抽象DataFrame更低级的API（参见[图 3-1](#sparks_data_abstractions)）。在RDD中，每个元素可能具有数据类型`T`，用`RDD[T]`表示。
- en: '![daws 0301](Images/daws_0301.png)'
  id: totrans-5
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0301](Images/daws_0301.png)'
- en: Figure 3-1\. Spark’s data abstractions
  id: totrans-6
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-1 Spark的数据抽象
- en: In every data solution, we use mapper transformations to convert one form of
    data into another desired form of data (for example, converting a record (as a
    `String`) into a (key, value) form). Spark provides five important mapper transformations
    that are used heavily in RDD transformations, which are summarized in [Table 3-1](#mapper_transformations).
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在每个数据解决方案中，我们使用映射器转换将一种形式的数据转换为另一种期望的数据形式（例如，将记录（作为`String`）转换为（key, value）形式）。Spark提供了五种重要的映射器转换，在RDD转换中被广泛使用，这些转换在[表格 3-1](#mapper_transformations)中进行了总结。
- en: Table 3-1\. Mapper transformations
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 表格 3-1 映射器转换
- en: '| Transformation | Relation type | Description |'
  id: totrans-9
  prefs: []
  type: TYPE_TB
  zh: '| 转换 | 关系类型 | 描述 |'
- en: '| --- | --- | --- |'
  id: totrans-10
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- | --- |'
- en: '| `map(f)` | 1-to-1 | Return a new RDD by applying a function (`f()`) to each
    element of this RDD. Source and target RDDs will have the same number of elements
    (transforms each element of the source `RDD[V]` into one element of the resulting
    target `RDD[T]`). |'
  id: totrans-11
  prefs: []
  type: TYPE_TB
  zh: '| `map(f)` | 1-to-1 | 通过将函数（`f()`）应用于该RDD的每个元素，返回一个新的RDD。源RDD和目标RDD将具有相同数量的元素（将源`RDD[V]`的每个元素转换为结果目标`RDD[T]`的一个元素）。'
- en: '| `mapValues(f)` | 1-to-1 | Pass each value in the (key, value) pair RDD through
    a `map(f)` function without changing the keys; this also retains the original
    RDD’s partitioning. Source and target RDDs will have the same number of elements
    (transforms each element of the source `RDD[K, V]` into one element of the resulting
    target `RDD[K, T]`). |'
  id: totrans-12
  prefs: []
  type: TYPE_TB
  zh: '| `mapValues(f)` | 1-to-1 | 将键值对RDD中的每个值通过`map(f)`函数进行处理，但不改变键；同时保留原始RDD的分区。源RDD和目标RDD将具有相同数量的元素（将源`RDD[K,
    V]`的每个元素转换为结果目标`RDD[K, T]`的一个元素）。'
- en: '| `flatMap(f)` | 1-to-many | Return a new RDD by first applying a function
    (`f()`) to all elements of this RDD, and then flattening the results. Source and
    target RDDs might not have the same number of elements (transforms each element
    of the source `RDD[V]` into zero or more elements of the target `RDD[T]`). |'
  id: totrans-13
  prefs: []
  type: TYPE_TB
  zh: '| `flatMap(f)` | 一对多 | 通过首先将函数(`f()`)应用于此RDD的所有元素，然后展开结果，返回一个新的RDD。源RDD和目标RDD的元素数量可能不相同（将源`RDD[V]`的每个元素转换为目标`RDD[T]`的零个或多个元素）。
    |'
- en: '| `flatMapValues(f)` | 1-to-many | Pass each value in the (key, value) pair
    RDD through a `flatMap(f)` function without changing the keys; this also retains
    the original RDD’s partitioning. Source and target RDDs might not have the same
    number of elements. |'
  id: totrans-14
  prefs: []
  type: TYPE_TB
  zh: '| `flatMapValues(f)` | 一对多 | 将（键，值）对RDD中的每个值通过`flatMap(f)`函数处理而不更改键；这也保留了原始RDD的分区。源RDD和目标RDD的元素数量可能不相同。
    |'
- en: '| `mapPartitions(f)` | Many-to-1 | Return a new RDD by applying a function
    (`f()`) to each partition of the source RDD. Source and target RDDs might not
    have the same number of elements (transforms each partition of the source `RDD[V]`,
    which may be composed of hundreds, thousands, or millions of elements, into one
    element of the resulting target `RDD[T]`). |'
  id: totrans-15
  prefs: []
  type: TYPE_TB
  zh: '| `mapPartitions(f)` | 多对一 | 通过将函数(`f()`)应用于源RDD的每个分区来返回一个新的RDD。源RDD和目标RDD的元素数量可能不相同（将源`RDD[V]`的每个分区（可能由数百、数千或数百万个元素组成）转换为结果目标`RDD[T]`的一个元素）。
    |'
- en: We’ll dig into each of these later in this chapter with practical examples of
    their use, but first let’s talk some more about what transformations actually
    are.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将在本章稍后通过实际示例深入探讨每个内容的使用，但首先让我们进一步讨论转换的实际含义。
- en: What Are Transformations?
  id: totrans-17
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 转换是什么？
- en: A transformation is defined as “a thorough or dramatic change in form or appearance.”
    This exactly matches the semantics of Spark transformations, which transform data
    from one form to another. For example, a `map()` transformation can transform
    a record of movie information (as a `String` object of `<user_id><,><username><,><movie_name><,><movie_id><,><rating><,><timestamp><,>​<director>​<,>`…)
    into a triplet of `(user_id, movie_id, rating)`. Another example of a transformation
    might be converting a chromosome “chr7:890766:T” into a tuple of `(chr7, 890766,
    T, 47)`, where 47 (as a derived partition number) is 890766 % 101 (the modulo
    of 101).
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 转换被定义为“形式或外观上的彻底或显著变化”。这与Spark转换的语义完全匹配，它将数据从一种形式转换为另一种形式。例如，`map()`转换可以将电影信息记录（作为`String`对象的`<user_id><,><username><,><movie_name><,><movie_id><,><rating><,><timestamp><,>​<director>​<,>`…）转换为三元组`(user_id,
    movie_id, rating)`。另一个转换的例子可能是将染色体“chr7:890766:T”转换为`(chr7, 890766, T, 47)`的元组，其中47（作为派生的分区号）是890766除以101的余数。
- en: 'As we learned in [Chapter 1](ch01.xhtml#Chapter-01), Spark supports two types
    of operations on RDDs: transformations and actions. As a reminder:'
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 正如我们在[第1章](ch01.xhtml#Chapter-01)中学到的，Spark支持对RDD的两种操作：转换和行动。作为提醒：
- en: Most RDD transformations accept a single source RDD and create a single target
    RDD.
  id: totrans-20
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 大多数RDD转换接受单个源RDD并创建单个目标RDD。
- en: Some Spark transformations create multiple target RDDs.
  id: totrans-21
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些Spark转换会创建多个目标RDD。
- en: Actions create non-RDD elements (values such as integers, strings, lists, tuples,
    dictionaries, and files).
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Actions创建非RDD元素（如整数、字符串、列表、元组、字典和文件）。
- en: 'There are at least three ways to create a brand new RDD:'
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: 至少有三种方法可以创建全新的RDD：
- en: RDDs can be created from datafiles. You can use `SparkContext.textFile()` or
    `SparkSession.spark.read.text()` to read datafiles from Amazon S3, HDFS, the Linux
    filesystem, and many other data sources, as discussed in [Chapter 1](ch01.xhtml#Chapter-01).
  id: totrans-24
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD可以从数据文件创建。您可以使用`SparkContext.textFile()`或`SparkSession.spark.read.text()`从Amazon
    S3、HDFS、Linux文件系统和许多其他数据源读取数据文件，如在[第1章](ch01.xhtml#Chapter-01)中讨论的。
- en: RDDs can be created from collections such as a list data structure (e.g., a
    list of numbers, or a list of strings, or a list of pairs) using `SparkContext.parallelize()`.
  id: totrans-25
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD可以从集合（例如列表数据结构，例如数字列表、字符串列表或对列表）创建，使用`SparkContext.parallelize()`。
- en: Given a source RDD, you can apply a transformation (such as `filter()` or `map()`)
    to create a new RDD.
  id: totrans-26
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 给定一个源RDD，您可以应用转换（例如`filter()`或`map()`）来创建一个新的RDD。
- en: Spark offers many useful transformations, which are the topic of this chapter.
    [Figure 3-2](#spark_operations_and_rdds) illustrates these options.
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: Spark提供了许多有用的转换，这是本章的主题。[图3-2](#spark_operations_and_rdds)说明了这些选项。
- en: '![daws 0302](Images/daws_0302.png)'
  id: totrans-28
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0302](Images/daws_0302.png)'
- en: Figure 3-2\. Different options for creating RDDs
  id: totrans-29
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图3-2 不同创建RDD的选项
- en: 'Informally, the `textFile()` and `parallelize()` operations can be stated as:'
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 简言之，`textFile()` 和 `parallelize()` 操作可以表述为：
- en: '[PRE0]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: 'A transformation (such as `map()` or `filter()`) on a source RDD (with an element
    type of `U`) creates a new RDD (target RDD [with an element type of `V`]):'
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 在源 RDD 上的转换（具有元素类型 `U`，例如 `map()` 或 `filter()`）会创建一个新的 RDD（目标 RDD 元素类型为 `V`）：
- en: '[PRE1]'
  id: totrans-33
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: 'An action (such as `collectAsMap()` or `count()`) on a source RDD creates a
    tangible result (non-RDD) such as integer, string, list, file, or dictionary:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 在源 RDD 上执行的行动（例如 `collectAsMap()` 或 `count()`）会创建一个具体的结果（非 RDD），例如整数、字符串、列表、文件或字典：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Some basic Spark operations (transformations and actions) are illustrated in
    [Figure 3-3](#spark_operations).
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一些基本的 Spark 操作（转换和行动）如 [图 3-3](#spark_operations) 所示。
- en: '![daws 0303](Images/daws_0303.png)'
  id: totrans-37
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0303](Images/daws_0303.png)'
- en: Figure 3-3\. Spark transformations and actions
  id: totrans-38
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-3\. Spark 转换和行动
- en: 'Let’s walk through what’s happening in [Figure 3-3](#spark_operations). Four
    RDDs are created—`rdd0`, `rdd1`, `rdd2`, and `rdd3`—through the following transformations:'
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 [图 3-3](#spark_operations) 中发生了什么。通过以下转换创建了四个 RDDs：`rdd0`、`rdd1`、`rdd2`
    和 `rdd3`：
- en: Transformation 1
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 转换 1
- en: '`SparkSession.sparkContext.textFile()` reads our input from a text file and
    creates the first RDD as `rdd0`:'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '`SparkSession.sparkContext.textFile()` 从文本文件读取输入并创建第一个 RDD `rdd0`：'
- en: '[PRE3]'
  id: totrans-42
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: '`rdd0` is denoted as `RDD[String]`, meaning that each element of `rdd0` is
    a `String` object.'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd0` 被标记为 `RDD[String]`，表示 `rdd0` 的每个元素是一个 `String` 对象。'
- en: Transformation 2
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 转换 2
- en: '`rdd1` (an `RDD[(String, Integer)]`) is created by the `rdd0.map()` transformation,
    which maps each element of `rdd0` into (key, value) pairs:'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd1`（一个 `RDD[(String, Integer)]`）通过 `rdd0.map()` 转换创建，将 `rdd0` 的每个元素映射为 (key,
    value) 对：'
- en: '[PRE4]'
  id: totrans-46
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Transformation 3
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 转换 3
- en: '`rdd2` (an `RDD[(String, Integer)]`) is created by `rdd1.map()`, where the
    mapper doubles the value part of the (key, value) pairs:'
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2`（一个 `RDD[(String, Integer)]`）通过 `rdd1.map()` 创建，其中 mapper 将 (key, value)
    对的值部分加倍：'
- en: '[PRE5]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Transformation 4
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 转换 4
- en: '`rdd3` (an `RDD[(String, Integer)]`) is created by `rdd1.reduceByKey()`, where
    the reducer sums up the values of the same keys:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd3`（一个 `RDD[(String, Integer)]`）通过 `rdd1.reduceByKey()` 创建，其中 reducer 对相同键的值进行求和：'
- en: '[PRE6]'
  id: totrans-52
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Then, the following actions are used to create three additional non-RDD outputs:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，使用以下行动创建了三个额外的非 RDD 输出：
- en: Action 1
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 行动 1
- en: '`rdd2.count()` is called to count the number of elements of `rdd2` (the result
    is an integer number):'
  id: totrans-55
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `rdd2.count()` 来计算 `rdd2` 中元素的数量（结果为整数）：
- en: '[PRE7]'
  id: totrans-56
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: Action 2
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 行动 2
- en: '`rdd3.count()` is called to count the number of elements of `rdd3` (again,
    the result is an integer number):'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `rdd3.count()` 来计算 `rdd3` 中元素的数量（结果为整数）：
- en: '[PRE8]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: Action 3
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 行动 3
- en: '`rdd3.saveAsText()` is called to persist the content of `rdd3` into a filesystem
    (the result is a text file):'
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 调用 `rdd3.saveAsText()` 将 `rdd3` 的内容持久化到文件系统中（结果为文本文件）：
- en: '[PRE9]'
  id: totrans-62
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: Let’s take a look at another example, illustrated in [Figure 3-4](#spark_transformation).
    You can view this sequence of transformations and actions as a directed acyclic
    graph (DAG), where nodes or vertices represent the RDDs and the edges represent
    the operations to be applied on the RDDs. As you’ll see shortly, Spark uses the
    DAG to optimize the operations.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看另一个例子，图示于 [图 3-4](#spark_transformation)。您可以将此转换和操作序列视为有向无环图（DAG），其中节点或顶点表示
    RDDs，边表示要应用于 RDDs 的操作。稍后您会看到，Spark 使用 DAG 优化操作。
- en: '![daws 0304](Images/daws_0304.png)'
  id: totrans-64
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0304](Images/daws_0304.png)'
- en: Figure 3-4\. Spark operations
  id: totrans-65
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-4\. Spark 操作
- en: 'Let’s walk through what’s happening in [Figure 3-4](#spark_transformation):'
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们看看 [图 3-4](#spark_transformation) 中发生了什么：
- en: RDD1 and RDD3 are created from text files.
  id: totrans-67
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD1 和 RDD3 是从文本文件创建的。
- en: 'RDD2 is created by a `map()` transformation:'
  id: totrans-68
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD2 由 `map()` 转换创建：
- en: '[PRE10]'
  id: totrans-69
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'RDD4 is created by a `flatMap()` transformation:'
  id: totrans-70
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD4 通过 `flatMap()` 转换创建：
- en: '[PRE11]'
  id: totrans-71
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE11]'
- en: 'RDD5 is created by joining two RDDs (RDD2 and RDD4). The result of this transformation
    is an RDD containing all pairs of elements with matching keys in RDD2 and RDD4:'
  id: totrans-72
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: RDD5 通过连接两个 RDDs（RDD2 和 RDD4）创建。此转换的结果是一个包含所有具有匹配键的元素对的 RDD：
- en: '[PRE12]'
  id: totrans-73
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Finally, the elements of RDD5 are saved as a hash map:'
  id: totrans-74
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，RDD5 的元素被保存为哈希映射：
- en: '[PRE13]'
  id: totrans-75
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'Until the `saveAsMap()` action is executed, no transformation will be evaluated
    or executed: this is called *lazy evaluation*.'
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 在执行 `saveAsMap()` 行动之前，不会评估或执行任何转换：这称为*延迟评估*。
- en: Lazy Transformations
  id: totrans-77
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 懒加载转换
- en: Let’s dig a little deeper into Spark’s lazy transformations. When running a
    Spark application (in Python, Java, or Scala), Spark creates a DAG and that graph.
    Because Spark transformations are lazily evaluated, the execution of the DAG will
    not start until an action (such as `collect()` or `count()`) is triggered. This
    means that the Spark engine can make optimization decisions after it has had a
    chance to look at the DAG in its entirety, rather than looking only at the individual
    transformations and actions. For example, it is possible to write a Spark program
    that creates 10 RDDs, 3 of which are never used (these are called *nonreachable
    RDDs*). The Spark engine does not need to compute those three RDDs, and by avoiding
    doing so it reduces the total execution time.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们深入了解一下 Spark 的惰性转换。当运行 Spark 应用程序（无论是 Python、Java 还是 Scala）时，Spark 创建一个 DAG
    和该图。由于 Spark 的转换是惰性评估的，直到触发某个动作（如 `collect()` 或 `count()`）才会开始执行 DAG。这意味着 Spark
    引擎可以在查看整个 DAG 之后做出优化决策，而不是仅仅查看单个转换和动作。例如，可以编写一个 Spark 程序，创建 10 个 RDD，其中有 3 个从不被使用（这些被称为
    *不可达 RDD*）。Spark 引擎不需要计算这三个 RDD，通过避免这样做可以减少总执行时间。
- en: As mentioned previously, a DAG in Apache Spark is a set of vertices and edges,
    where vertices represent the RDDs and the edges represent the operations (transformations
    or actions) to be applied on the RDDs. In a Spark DAG, every edge is directed
    from earlier to later in the sequence. On calling of an action (such as `saveAsMap()`,
    `count()`, `collect()`, or `collectAsMap()`), the created DAG is submitted to
    Spark’s `DAG Scheduler` which further splits the graph into the stages of the
    task, as illustrated in [Figure 3-5](#sparks_dag).
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 正如前面提到的，在 Apache Spark 中，DAG 是一组顶点和边，其中顶点表示 RDD，边表示要在 RDD 上应用的操作（转换或动作）。在 Spark
    DAG 中，每条边从序列中较早的 RDD 指向较晚的 RDD。在调用动作（如 `saveAsMap()`、`count()`、`collect()` 或 `collectAsMap()`）时，创建的
    DAG 被提交给 Spark 的 `DAG Scheduler`，后者进一步将图分割为任务的阶段，如 [图 3-5](#sparks_dag) 所示。
- en: '![daws 0305](Images/daws_0305.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0305](Images/daws_0305.png)'
- en: Figure 3-5\. Spark’s DAG
  id: totrans-81
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-5\. Spark 的 DAG
- en: Tip
  id: totrans-82
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 提示
- en: Every `SparkContext` launches a web UI (by default on port 4040, with multiple
    `SparkContext`s binding to successive ports) that displays useful information
    about the application, including a visualization of the DAG. You can view the
    DAG by going to *http://<master>/4040*.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个 `SparkContext` 启动一个 Web UI（默认端口为 4040，具有多个 `SparkContext` 绑定到连续端口），显示有关应用程序的有用信息，包括
    DAG 的可视化。您可以通过访问 *http://<master>/4040* 查看 DAG。
- en: Lazy evaluation in Spark has several benefits. It increases the manageability
    of transactions, and enables the Spark engine to perform various optimizations.
    This reduces complexity, saves computation, and increases speed. Reducing the
    execution time of the RDD operations improves performance, and the lineage graph
    (DAG) helps Spark achieve fault tolerance by providing a record of the operations
    performed on the RDDs.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 在 Spark 中的惰性评估有多个优点。它增加了事务的可管理性，并使 Spark 引擎能够执行各种优化。这降低了复杂性，节省了计算资源，并提高了速度。减少
    RDD 操作的执行时间可以提高性能，而血统图（DAG）通过记录在 RDD 上执行的操作来帮助 Spark 实现容错。
- en: Now that you understand a bit more about transformations, we’ll dive into Spark’s
    most common mapper transformations in a little more detail. We’ll start with the
    `map()` transformation, which is the most widely used transformation in any Spark
    application.
  id: totrans-85
  prefs: []
  type: TYPE_NORMAL
  zh: 现在您对转换有了更多了解，我们将更详细地介绍 Spark 中最常见的映射器转换。我们将从 `map()` 转换开始，这是任何 Spark 应用程序中最常用的转换。
- en: The map() Transformation
  id: totrans-86
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: map() 转换
- en: The `map()` transformation is the most common transformation in the Spark and
    MapReduce paradigm. This transformation can be applied to RDDs and Dataframes.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()` 转换是 Spark 和 MapReduce 范式中最常见的转换。该转换可以应用于 RDD 和 Dataframe。'
- en: RDD mapper
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: RDD 映射器
- en: 'The goal of `RDD.map()` is to transform every element of the source `RDD[V]`
    into a mapped element of the target `RDD[T]` by applying a function `f()` to it.
    This function can be a predefined one or a custom user-defined function. The `map()`
    transformation is defined as:'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD.map()` 的目标是通过将函数 `f()` 应用于源 `RDD[V]` 的每个元素，将其转换为目标 `RDD[T]` 的映射元素。这个函数可以是预定义的，也可以是自定义的。`map()`
    转换定义如下：'
- en: '[PRE14]'
  id: totrans-90
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO1-1)'
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO1-1)'
- en: Function `f()` accepts a `V` type element and returns an element of type `T`.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 函数 `f()` 接受 `V` 类型的元素并返回 `T` 类型的元素。
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO1-2)'
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO1-2)'
- en: Using function `f()`, the `map()` transformation transforms `RDD[V]` to `RDD[T]`.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 使用函数 `f()`，`map()` 转换将 `RDD[V]` 转换为 `RDD[T]`。
- en: 'This is a 1-to-1 transformation: if your source RDD has *`N`* elements, then
    the resulting/target RDD will have exactly *`N`* elements as well. Bear in mind
    that the `map()` transformation is not a sequential function. Your source RDD
    is partitioned into *`P`* partitions, which are then processed independently and
    concurrently. For example, if your source RDD has 40 billion elements and *`P`*
    `=` `20,000`, then each partition will have roughly 2 million elements (40 billion
    = 20,000 x 2 million). If the number of available mappers is 80 (this number depends
    on the available resources in your cluster), then 80 partitions can be mapped
    at the same time independently and concurrently.'
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个一对一的转换：如果您的源 RDD 有 *`N`* 个元素，则结果/目标 RDD 也将有完全相同的 *`N`* 个元素。请记住，`map()` 转换不是一个顺序函数。您的源
    RDD 被分区成 *`P`* 个分区，然后独立并发地处理这些分区。例如，如果您的源 RDD 有 400 亿个元素，并且 *`P`* `=` `20,000`，那么每个分区大约会有
    200 万个元素（400 亿 = 20,000 x 2 百万）。如果可用的映射器数量为 80（这个数字取决于集群中的可用资源），则可以同时独立并发地映射 80
    个分区。
- en: 'The function `f()` for the `map()` transformation can be defined as:'
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()` 转换的函数 `f()` 可以定义为：'
- en: '[PRE15]'
  id: totrans-97
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: 'Or you may create your target RDD (`rdd_v`) by using a lambda expression, as
    follows:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 或者您可以使用 lambda 表达式创建目标 RDD (`rdd_v`)，如下所示：
- en: '[PRE16]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: '[Figure 3-6](#the_map_transformation) illustrates the semantics of the `map()`
    transformation.'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: '[Figure 3-6](#the_map_transformation) 说明了 `map()` 转换的语义。'
- en: '![daws 0306](Images/daws_0306.png)'
  id: totrans-101
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0306](Images/daws_0306.png)'
- en: Figure 3-6\. The `map()` transformation
  id: totrans-102
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-6\. `map()` 转换
- en: 'The following example shows how to use the `map()` transformation using the
    PySpark shell. The example maps a source `RDD[Integer]` to a target `RDD[Integer]`:
    it transforms an RDD that contains a list of numbers into a new RDD in which the
    value of each positive element has been increased by 5 while all other elements
    have been changed to 0.'
  id: totrans-103
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何在 PySpark shell 中使用 `map()` 转换。该示例将源 `RDD[Integer]` 映射到目标 `RDD[Integer]`：它将包含数字列表的
    RDD 转换为新的 RDD，其中每个正数元素的值增加了 5，而所有其他元素都被更改为 0。
- en: 'First, let’s define our mapper function as `mapper_func()`:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，让我们将我们的映射函数定义为 `mapper_func()`：
- en: '[PRE17]'
  id: totrans-105
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'Next, we’ll apply the a `map()` transformation and see how it works:'
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将应用 `map()` 转换，并看看它是如何工作的：
- en: '[PRE18]'
  id: totrans-107
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO2-1)'
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO2-1)'
- en: '`rdd` is an `RDD[Integer]`.'
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd` 是一个 `RDD[Integer]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO2-2)'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO2-2)'
- en: '`rdd2` is an `RDD[Integer]`.'
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 是一个 `RDD[Integer]`。'
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO2-3)'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO2-3)'
- en: '`rdd3` is an `RDD[Integer]`.'
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd3` 是一个 `RDD[Integer]`。'
- en: '[![4](Images/4.png)](#co_mapper_transformations_CO2-4)'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_mapper_transformations_CO2-4)'
- en: '`rdd4` is an `RDD[(Integer, Integer)]`.'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd4` 是一个 `RDD[(Integer, Integer)]`。'
- en: 'Here’s another example, which maps an `RDD[(String, Integer)]` to an `RDD[(String,
    Integer, String)]`. This example transforms elements in the form of (key, value)
    pairs into (key, value, value+100) triplets:'
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个例子，将 `RDD[(String, Integer)]` 映射到 `RDD[(String, Integer, String)]`。该例子将形式为
    (key, value) 对的元素转换为 (key, value, value+100) 三元组：
- en: '[PRE19]'
  id: totrans-117
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO3-1)'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO3-1)'
- en: '`rdd` is an `RDD[(String, Integer)]`.'
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd` 是一个 `RDD[(String, Integer)]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO3-2)'
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO3-2)'
- en: '`rdd2` is an `RDD[(String, Integer, Integer)]`.'
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 是一个 `RDD[(String, Integer, Integer)]`。'
- en: 'It’s also straightforward to create (key, value) pairs from `String` objects:'
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 从 `String` 对象创建 (key, value) 对也很简单：
- en: '[PRE20]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO4-1)'
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO4-1)'
- en: '`rdd` is an `RDD[String]`.'
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd` 是一个 `RDD[String]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO4-2)'
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO4-2)'
- en: '`rdd2` is an `RDD[(String, (Integer, Integer))]`.'
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 是一个 `RDD[(String, (Integer, Integer))]`。'
- en: Next, I’ll discuss custom mapper functions.
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将讨论自定义映射函数。
- en: Custom mapper functions
  id: totrans-129
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 自定义映射函数
- en: When using Spark’s transformations, you may use custom Python functions to parse
    records, perform computations, and finally create your desired output.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 在使用 Spark 的转换时，您可以使用自定义的 Python 函数来解析记录，执行计算，最终创建您想要的输出。
- en: 'Suppose we have a sample dataset, where each record has the following format:'
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个样本数据集，每条记录的格式如下：
- en: '[PRE21]'
  id: totrans-132
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: 'Our data looks like this:'
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的数据看起来像这样：
- en: '[PRE22]'
  id: totrans-134
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: 'For each age category, we want to get the average number of friends. We can
    write our own custom mapper function:'
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 对于每个年龄类别，我们想要得到朋友平均数。我们可以编写自己的自定义映射器函数：
- en: '[PRE23]'
  id: totrans-136
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: 'Then read our data and use the custom function:'
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 然后读取我们的数据并使用自定义函数：
- en: '[PRE24]'
  id: totrans-138
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO5-1)'
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO5-1)'
- en: '`users` is an `RDD[String]`.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: '`users`是一个`RDD[String]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO5-2)'
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO5-2)'
- en: '`pairs` is an `RDD[(Integer, Integer)]`, where each record gets sent through
    `parse_record()`.'
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: '`pairs`是一个`RDD[(Integer, Integer)]`，其中每条记录都会通过`parse_record()`处理。'
- en: 'For our sample data, `pairs` will be:'
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的样本数据，`pairs`将是：
- en: '[PRE25]'
  id: totrans-144
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'To get the average per age category, we first get the sum and the number of
    entries per age:'
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 要获得每个年龄类别的平均数，我们首先获取每个年龄的总和和条目数：
- en: '[PRE26]'
  id: totrans-146
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO6-1)'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO6-1)'
- en: '`pairs` is an `RDD[(Integer, Integer)]`.'
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: '`pairs`是一个`RDD[(Integer, Integer)]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO6-2)'
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO6-2)'
- en: Convert the `number_of_friends` field to a `(number_of_friends, 1)` pair.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 将`number_of_friends`字段转换为`(number_of_friends, 1)`对。
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO6-3)'
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO6-3)'
- en: Perform reduction on age to find `(sum_of_friends, frequecy_count)` per age.
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 在年龄上执行减少以找到每个年龄的`(sum_of_friends, frequecy_count)`。
- en: '[![4](Images/4.png)](#co_mapper_transformations_CO6-4)'
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_mapper_transformations_CO6-4)'
- en: '`totals_by_age` is `RDD[(Integer, (Integer, Integer))]`'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: '`totals_by_age`是`RDD[(Integer, (Integer, Integer))]`'
- en: 'For our data, `totals_by_age` will be:'
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，`totals_by_age`将是：
- en: '[PRE27]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'Now, to compute the average number of friends for each age, we need to do one
    more transformation, dividing the sum by the frequency count to get the average:'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，要计算每个年龄段的平均朋友数，我们需要进行另一个转换，将总和除以频率计数以获得平均值：
- en: '[PRE28]'
  id: totrans-158
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: 'For our data, `averages_by_age` (an `RDD[(Integer, Integer)]`) will be:'
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 对于我们的数据，`averages_by_age`（一个`RDD[(Integer, Integer)]`）将是：
- en: '[PRE29]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: DataFrame Mapper
  id: totrans-161
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: DataFrame映射器
- en: 'Spark’s DataFrame does not have a `map()` function, but we can achieve the
    `map()` equivalency in many ways: we can add new columns by applying `DataFrame.withColumn()`
    and drop existing columns with `DataFrame.drop()`. The new column values can be
    computed based on existing row values or other requirements.'
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的DataFrame没有`map()`函数，但我们可以通过多种方式实现`map()`的等效操作：可以通过`DataFrame.withColumn()`添加新列，并使用`DataFrame.drop()`删除现有列。新列的值可以根据现有行值或其他要求计算得出。
- en: Mapper to single DataFrame column
  id: totrans-163
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射到单个DataFrame列
- en: 'Consider the following DataFrame:'
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑以下DataFrame：
- en: '[PRE30]'
  id: totrans-165
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: Suppose we want to calculate a 10% bonus to the “amount” column and create a
    new “bonus” column. There are multiple ways to accomplish this mapper task.
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们想要对“amount”列计算10%的奖金并创建一个新的“bonus”列。有多种方法可以完成这个映射任务。
- en: 'To keep all of the columns, do the following:'
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 要保留所有列，请执行以下操作：
- en: '[PRE31]'
  id: totrans-168
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: You have to map the row to a tuple containing all of the existing columns, then
    add in the new column(s).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 您必须将行映射到包含所有现有列的元组，然后添加新列。
- en: If you have too many columns to enumerate, you could also just add a tuple to
    the existing row.
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 如果您有太多列要列举，也可以将一个元组添加到现有行。
- en: '[PRE32]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'There is another way to add a `bonus` column with using `DataFrame.withColumn()`:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 还有另一种方法可以使用`DataFrame.withColumn()`添加`bonus`列：
- en: '[PRE33]'
  id: totrans-173
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: Mapper to multiple DataFrame columns
  id: totrans-174
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 映射到多个DataFrame列
- en: 'Now, assume that you want to add a `bonus` column, which depends on two columns:
    “amount” and “education”: The `bonus` column is calculated as:'
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，假设您要添加一个依赖于两个列“amount”和“education”的`bonus`列：`bonus`列计算如下：
- en: '[PRE34]'
  id: totrans-176
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: 'the simplest way to do this is with a user-defined function (UDF): define a
    Python function and then register it as a UDF:'
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 最简单的方法是使用用户定义的函数（UDF）：定义一个Python函数，然后将其注册为UDF：
- en: '[PRE35]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Now, register your Python function as a UDF:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，将您的Python函数注册为UDF：
- en: '[PRE36]'
  id: totrans-180
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: 'Once your UDF is ready, then you can apply it:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦您的UDF准备就绪，那么您就可以应用它：
- en: '[PRE37]'
  id: totrans-182
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Next, we’ll take a look at the `flatMap()` transformation.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下`flatMap()`转换。
- en: The flatMap() Transformation
  id: totrans-184
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: flatMap()转换
- en: 'The `flatMap()` transformation returns a new RDD by applying a function to
    each element of the source RDD, then flattening the results. This is a 1-to-many
    transformation: every element of the source RDD can be mapped into 0, 1, 2, or
    many elements of the target RDD. In other words, The `flatMap()` transforms a
    source `RDD[U]` of length *`N`* into a target `RDD[V]` of length *`M`* (where
    *`M`* and *`N`* can be different), When using `flatMap()`, you need to make sure
    that the source RDD’s elements are iterable (such as a list of items).'
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap()`转换通过将函数应用于源RDD的每个元素，然后展平结果来返回新的RDD。这是一种一对多的转换：源RDD的每个元素可以映射为目标RDD的0、1、2或多个元素。换句话说，`flatMap()`将长度为`N`的源`RDD[U]`转换为长度为`M`的目标`RDD[V]`（`M`和`N`可以不同）。使用`flatMap()`时，您需要确保源RDD的元素是可迭代的（例如，一个包含项目列表的列表）。'
- en: For example, if an element of the source RDD is `[10, 20, 30]` (an iterable
    list of three numbers), then it will be mapped as three elements (`10`, `20`,
    and `30`) of the target RDD; if an element of the source RDD is `[]` (an empty
    list, which is iterable), then it will be dropped and will not be mapped to the
    target RDD at all. If any element of source RDD is not iterable, then an exception
    will be raised.
  id: totrans-186
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果源RDD的元素是`[10, 20, 30]`（包含三个数字的可迭代列表），那么它将被映射为目标RDD的三个元素（`10`，`20`和`30`）；如果源RDD的元素是`[]`（空列表，可迭代），那么它将被丢弃，并且根本不会映射到目标RDD。如果源RDD的任何元素不可迭代，则会引发异常。
- en: Note that whereas `map()` transforms an RDD of length *`N`* into another RDD
    of length *`N`* (the same length), `flatMap()` transforms an RDD of length *`N`*
    into a set of *`N`* iterable collections, then flattens these into a single RDD
    of results. Therefore, the source and target RDDs may have different sizes.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，`map()`将长度为`N`的RDD转换为另一个长度为`N`的RDD（长度相同），而`flatMap()`将长度为`N`的RDD转换为一组`N`个可迭代集合，然后展平这些集合为单个结果RDD。因此，源RDD和目标RDD可能具有不同的大小。
- en: 'The `flatMap()` transformation is defined as:'
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap()`转换定义如下：'
- en: '[PRE38]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO7-1)'
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO7-1)'
- en: The function `f()` accepts an element of type `U` and converts it into a list
    of elements of type `V` (this list may have 0, 1, 2, or more elements), which
    is then flattened. Note that empty lists are dropped. The function `f()` must
    create an iterable object.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f()`接受类型为`U`的元素，并将其转换为类型为`V`的元素列表（此列表可以包含0、1、2或多个元素），然后展平。注意，空列表将被丢弃。函数`f()`必须创建一个可迭代对象。
- en: '[Figure 3-7](#the_flatmap_transformation) shows an example of the `flatMap()`
    transformation.'
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-7](#the_flatmap_transformation)展示了`flatMap()`转换的示例。'
- en: '![daws 0307](Images/daws_0307.png)'
  id: totrans-193
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0307](Images/daws_0307.png)'
- en: Figure 3-7\. The `flatMap()` transformation
  id: totrans-194
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-7。`flatMap()`转换
- en: In [Figure 3-7](#the_flatmap_transformation), each element (a `String`) of the
    source RDD is tokenized into a list of `Strings` and then flattened into a `String`
    object. For example, the first element, `"[red fox jumped]"`, is converted into
    a list of `Strings` as `["red", "fox", "jumped"]` and then the list is flattened
    into three `String` objects as `"red"`, `"fox"`, and `"jumped"`. The first source
    element is thus mapped into three target elements.
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3-7](#the_flatmap_transformation)中，源RDD的每个元素（一个`String`）被分词为一个`Strings`列表，然后展平为一个`String`对象。例如，第一个元素`"[red
    fox jumped]"`被转换为`Strings`列表`["red", "fox", "jumped"]`，然后该列表被展平为三个`String`对象`"red"`，`"fox"`和`"jumped"`。因此，第一个源元素被映射为三个目标元素。
- en: 'The following example shows how to use the `flatMap()` transformation:'
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 以下示例展示了如何使用`flatMap()`转换：
- en: '[PRE39]'
  id: totrans-197
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'Let’s examine how `rdd2` is created:'
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来看看如何创建`rdd2`：
- en: '[PRE40]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'You can also use a function, instead of a lambda expression:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 您也可以使用函数，而不是lambda表达式：
- en: '[PRE41]'
  id: totrans-201
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO8-1)'
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO8-1)'
- en: '`rdd` is an `RDD[Integer]` with five elements.'
  id: totrans-203
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd`是一个包含五个元素的`RDD[Integer]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO8-2)'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO8-2)'
- en: '`rdd4` is an `RDD[(Integer, Integer)]` with 10 elements.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd4`是一个包含十个元素的`RDD[(Integer, Integer)]`。'
- en: 'The following example illustrates how `flatMap()` can return zero or more elements
    in the target RDD for each element in the source RDD:'
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap()`函数示例说明了如何在目标RDD中，针对源RDD中的每个元素返回零个或多个元素：'
- en: '[PRE42]'
  id: totrans-207
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO9-1)'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO9-1)'
- en: '`rdd` is an `RDD[String]` with five elements.'
  id: totrans-209
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd`是一个包含五个元素的`RDD[String]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO9-2)'
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO9-2)'
- en: Empty lists are dropped.
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 空列表将被丢弃。
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO9-3)'
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO9-3)'
- en: This will map to three target elements.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 这将映射到三个目标元素。
- en: '[![4](Images/4.png)](#co_mapper_transformations_CO9-4)'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_mapper_transformations_CO9-4)'
- en: '`flattened` is an `RDD[String]` with nine elements.'
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: '`flattened` 是一个具有九个元素的 `RDD[String]`。'
- en: 'The following example clearly shows the difference between `map()` and `flatMap()`.
    As you can see from the outputs, `flatMap()` flattens its output, while the `map()`
    transformation is a 1-to-1 mapping and does not flatten its output:'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的例子清楚地展示了 `map()` 和 `flatMap()` 之间的区别。如您从输出中看到的那样，`flatMap()` 将其输出展平，而 `map()`
    转换是一对一映射，不会展平其输出：
- en: '[PRE43]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO10-1)'
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO10-1)'
- en: Create an `RDD[Integer]`.
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `RDD[Integer]`。
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO10-2)'
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO10-2)'
- en: Each element of `rdd1` is a list of integer numbers (as `RDD[[Integer]]`).
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd1` 的每个元素都是整数列表（作为 `RDD[[Integer]]`）。'
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO10-3)'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO10-3)'
- en: Create an `RDD[[Integer]]`.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个 `RDD[[Integer]]`。
- en: '[![4](Images/4.png)](#co_mapper_transformations_CO10-4)'
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: '[![4](Images/4.png)](#co_mapper_transformations_CO10-4)'
- en: Each element of `rdd2` is an integer number (as `RDD[Integer]`).
  id: totrans-225
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 的每个元素都是整数（作为 `RDD[Integer]`）。'
- en: A visual representation of the `flatMap()` transformation is presented in the
    [Figure 3-8](#the_flatmap_visualization).
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-8](#the_flatmap_visualization) 中呈现了 `flatMap()` 转换的视觉表示。'
- en: '![daws 0308](Images/daws_0308.png)'
  id: totrans-227
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0308](Images/daws_0308.png)'
- en: Figure 3-8\. A `flatMap()` transformation
  id: totrans-228
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-8\. 一个 `flatMap()` 转换
- en: 'Let’s walk through what’s happening here. We’ll start by examining the content
    of the input file, *2recs.txt*:'
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们逐步分析这里发生的情况。我们将从检查输入文件 *2recs.txt* 的内容开始：
- en: '[PRE44]'
  id: totrans-230
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'Here are the steps:'
  id: totrans-231
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是步骤：
- en: 'First, we create an `RDD[String]` with only two records/elements:'
  id: totrans-232
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，我们创建一个仅包含两条记录/元素的 `RDD[String]`：
- en: '[PRE45]'
  id: totrans-233
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE45]'
- en: 'Next, we apply a `map()` transformation to all elements of this RDD that removes
    all punctuation, reduces multiple spaces into a single space, and converts all
    letters to lowercase. This is accomplished by a simple Python function:'
  id: totrans-234
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们对此RDD的所有元素应用 `map()` 转换，删除所有标点符号，将多个空格缩减为单个空格，并将所有字母转换为小写。这是通过一个简单的Python函数实现的：
- en: '[PRE46]'
  id: totrans-235
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE46]'
- en: 'We then apply a `flatMap()` transformation to `rdd_cleaned`, first tokenizing
    the elements of this RDD and then flattening it:'
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 然后我们对 `rdd_cleaned` 应用 `flatMap()` 转换，首先对此RDD的元素进行标记化，然后展开它：
- en: '[PRE47]'
  id: totrans-237
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE47]'
- en: Finally, the `filter()` transformation drops elements of the `flattened` RDD,
    keeping only elements with a length greater than 2.
  id: totrans-238
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，`filter()` 转换将丢弃 `flattened` RDD 的元素，仅保留长度大于 2 的元素。
- en: '[PRE48]'
  id: totrans-239
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE48]'
- en: The filtered-out elements are indicated with an X in [Figure 3-8](#the_flatmap_visualization).
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: 在 [图 3-8](#the_flatmap_visualization) 中用 X 标出的筛选出的元素。
- en: map() Versus flatMap()
  id: totrans-241
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: '`map()` 与 `flatMap()` 对比'
- en: 'You’ve now seen some examples of `map()` and `flatMap()` transformations, but
    it’s important to understand the differences between them. To recap:'
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: 您现在已经看到了一些 `map()` 和 `flatMap()` 转换的例子，但重要的是要理解它们之间的区别。总结一下：
- en: '`map()`'
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: '`map()`'
- en: This is a 1-to-1 transformation. It returns a new RDD by applying the given
    function to each element of the RDD. The function in `map()` returns only one
    item.
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个一对一的转换。它通过将给定函数应用于RDD的每个元素来返回一个新的RDD。`map()` 中的函数只返回一个项。
- en: '`flatMap()`'
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMap()`'
- en: This is a 1-to-many transformation. It also returns a new RDD by applying a
    function to each element of the source RDD, but the function may return 0, 1,
    2, or more elements per source element, and the output is flattened.
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一个一对多的转换。它还通过对源RDD的每个元素应用函数来返回一个新的RDD，但函数可以针对每个源元素返回0、1、2或多个元素，并且输出被展平。
- en: The difference between `map()` and `flatMap()` is illustrated in [Figure 3-9](#the_difference_between_map_and_flatmap).
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '[图 3-9](#the_difference_between_map_and_flatmap) 中阐明了 `map()` 和 `flatMap()`
    的区别。'
- en: '![daws 0309](Images/daws_0309.png)'
  id: totrans-248
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0309](Images/daws_0309.png)'
- en: Figure 3-9\. The difference between `map()` and `flatMap()`
  id: totrans-249
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-9\. `map()` 和 `flatMap()` 的区别
- en: Apply `flatMap()` to a DataFrame
  id: totrans-250
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 应用 `flatMap()` 到 DataFrame
- en: 'The `RDD.flatMap()` is a one-to-many transformation: it takes one element of
    source RDD and transforms it into many (0, 1, 2, 3, or more) target elements.
    PySpark’s DataFame does not have `flatMap()` transformation, however, DataFrame
    has the function `pyspark.sql.functions.explode(col)`, which is used to flatten
    the column. The `explode(column)` returns a new row for each element in the given
    `column` (expressed as a list or dictionary) and uses the default column name
    `col` for elements in the array and key and value for elements in the dictionary
    unless specified otherwise.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: '`RDD.flatMap()`是一对多的转换：它接收源RDD的一个元素并将其转换为多个（0、1、2、3或更多）目标元素。PySpark的DataFrame没有`flatMap()`转换，但是DataFrame具有函数`pyspark.sql.functions.explode(col)`，用于展平列。`explode(column)`为给定的`column`（表示为列表或字典）中的每个元素返回一行，并使用默认列名`col`来表示数组中的元素，以及用键和值来表示字典中的元素，除非另有指定。'
- en: Below is a complete example, which shows how to use the `explode()` function
    as an equivalent to `RDD.flatMap()` transformation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个完整的示例，展示了如何使用`explode()`函数作为`RDD.flatMap()`转换的等价物。
- en: Let’s first create a DataFrame, in which one column is a list (to be exploded
    by the `explode()` function).
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们首先创建一个DataFrame，其中一个列是一个列表（将由`explode()`函数展开）。
- en: '[PRE49]'
  id: totrans-254
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: 'Next, we will flatten the `known_languages` column:'
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将展开`known_languages`列：
- en: '[PRE50]'
  id: totrans-256
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: As you can see, when exploding a column, if a column is an empty list, it’s
    dropped from the exploded result (`tex` and `max` are dropped since they have
    associated empty lists).
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，当展开列时，如果列是空列表，则从展开结果中删除该列（`tex`和`max`被删除，因为它们关联的是空列表）。
- en: 'Next, we’ll look at exploding multiple columns for a given DataFrame. Note
    that only one generator is allowed per `select` clause: this means you can not
    explode two columns at the same time (but you can explode them iteratively one-by-one).
    The following example shows how to explode two columns:'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将看一下如何对给定的DataFrame展开多列。请注意，每个`select`子句只允许一个生成器：这意味着您不能同时展开两列（但可以逐个迭代展开它们）。以下示例显示了如何展开两列：
- en: '[PRE51]'
  id: totrans-259
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: 'Next we explode the `languages` column, which is an array:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们展开`languages`列，它是一个数组：
- en: '[PRE52]'
  id: totrans-261
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: Note that the names `ted` and `dan` were dropped since the exploded column value
    was an empty list.
  id: totrans-262
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于爆炸列值为空列表，名称`ted`和`dan`被丢弃。
- en: 'Next, we explode the `education` column:'
  id: totrans-263
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们展开`education`列：
- en: '[PRE53]'
  id: totrans-264
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: Note that the name `max` is dropped since the exploded column value was an empty
    list.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，由于爆炸列值为空列表，名称`max`被丢弃。
- en: Next we’ll look at a transformation that is specific to RDDs whose elements
    are (key, value) pairs.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 下面我们将讨论一种仅适用于其元素为(key, value)对的RDD的转换。
- en: The mapValues() Transformation
  id: totrans-267
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`mapValues()`转换'
- en: The `mapValues()` transformation is only applicable for pair RDDs (`RDD[(K,
    V)]`, where `K` is the key and `V` is the value). It operates on the value only
    (`V`), leaving the key unchanged, unlike the `map()` transformation, which operates
    on the entire RDD element.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapValues()`转换仅适用于键值对RDD（`RDD[(K, V)]`，其中`K`是键，`V`是值）。它仅对值(`V`)操作，保持键不变，与`map()`转换不同，后者操作整个RDD元素。'
- en: 'Informally, given the source RDD `RDD[(K, V)]` and the function `f: V -> T`,
    we may say that `rdd.mapValues(f)` is equivalent to the following `map()`:'
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: '从非正式的角度来看，给定源RDD `RDD[(K, V)]` 和函数 `f: V -> T`，我们可以说 `rdd.mapValues(f)` 等价于以下
    `map()`：'
- en: '[PRE54]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: 'The `mapValues()` transformation is defined as:'
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapValues()`转换定义如下：'
- en: '[PRE55]'
  id: totrans-272
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO11-1)'
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO11-1)'
- en: The function `f()` can transform the data type `V` to any desired data type
    `T`. `V` and `T` can be the same or different.
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 函数`f()`可以将数据类型`V`转换为任何所需的数据类型`T`。`V`和`T`可以相同也可以不同。
- en: The `mapValues()` transformation passes each value in the pair RDD through a
    `map()` function without changing the keys; this also retains the original RDD’s
    partitioning (the changes are done in place and the structure and number of partitions
    are not changed).
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapValues()`转换将键值对RDD中的每个值通过`map()`函数传递，而不改变键；这也保留了原始RDD的分区（更改是在原地完成的，分区的结构和数量不变）。'
- en: 'The following is an example of a `mapValues()` transformation:'
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是`mapValues()`转换的示例：
- en: '[PRE56]'
  id: totrans-277
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO12-1)'
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO12-1)'
- en: '`rdd` is an `RDD[(String, [Integer])]`.'
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd`是一个`RDD[(String, [Integer])]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO12-2)'
  id: totrans-280
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO12-2)'
- en: '`rdd2` is an `RDD[(String, Integer)]`.'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2`是一个`RDD[(String, Integer)]`。'
- en: '`mapValues()` is a 1-to-1 transformation, as illustrated by [Figure 3-10](#the_mapvalues_transformation).'
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapValues()` 是一个一对一的转换，如[图 3-10](#the_mapvalues_transformation)所示。'
- en: '![daws 0310](Images/daws_0310.png)'
  id: totrans-283
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0310](Images/daws_0310.png)'
- en: Figure 3-10\. The `mapValues()` transformation
  id: totrans-284
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-10\. `mapValues()` 转换
- en: The flatMapValues() Transformation
  id: totrans-285
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`flatMapValues()` 转换'
- en: 'The `flatMapValues()` transformation is a combination of `flatMap()` and `m⁠a⁠p​V⁠a⁠l⁠u⁠e⁠s⁠(⁠)`.
    It’s similar to `mapValues()`, but `flatMapValues()` runs the `flatMap()` function
    on the values of `RDD[(K, V)]` (an RDD of (key, value) pairs) instead of the `map()`
    function. It does this without changing the keys, retaining the original RDD’s
    partitioning. Here’s an example:'
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: '`flatMapValues()` 转换是 `flatMap()` 和 `m⁠a⁠p​V⁠a⁠l⁠u⁠e⁠s⁠(⁠)` 的组合。它类似于 `mapValues()`，但
    `flatMapValues()` 在 `RDD[(K, V)]`（即 (键, 值) 对的 RDD）的值上运行 `flatMap()` 函数，而不是 `map()`
    函数。它在不改变键的情况下保留了原始 RDD 的分区。以下是一个例子：'
- en: '[PRE57]'
  id: totrans-287
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO13-1)'
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO13-1)'
- en: This element will be dropped since the value is empty.
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 此元素将被丢弃，因为其值为空。
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO13-2)'
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO13-2)'
- en: '`rdd` is an `RDD[(String, [Integer])]`.'
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd` 是一个 `RDD[(String, [Integer])]`。'
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO13-3)'
  id: totrans-292
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO13-3)'
- en: '`rdd2` is an `RDD[(String, Integer)]`; note that the key `S` is dropped since
    its value was an empty list'
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 是一个 `RDD[(String, Integer)]`；注意，由于键 `S` 的值是空列表，所以该键被丢弃了。'
- en: 'Here’s another example:'
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这里是另一个例子：
- en: '[PRE58]'
  id: totrans-295
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO14-1)'
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO14-1)'
- en: '`rdd` is an `RDD[(String, [String])]`.'
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd` 是一个 `RDD[(String, [String])]`。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO14-2)'
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO14-2)'
- en: '`rdd2` is an `RDD[(String, String)]`.'
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: '`rdd2` 是一个 `RDD[(String, String)]`。'
- en: Again, if the value for a key is empty `([])`, then no output value is generated
    (the key is dropped as well). Therefore, no element is generated for the `D` key.
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 同样地，如果某个键的值为空`([])`，那么不会生成输出值（该键也会被丢弃）。因此，对于 `D` 键，不会生成任何元素。
- en: Next we’ll look at the `mapPartitions()` transformation, which, in my opinion,
    is the most important of Spark’s mapper transformations.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来我们将看看 `mapPartitions()` 转换，这在我看来是 Spark 中最重要的映射器转换之一。
- en: The mapPartitions() Transformation
  id: totrans-302
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: '`mapPartitions()` 转换'
- en: '`mapPartitions()` is a powerful distributed mapper transformation that processes
    a single partition (instead of an element) at a time. It implements the summarization
    design pattern, summarizing each partition of a source RDD into a single element
    of the target RDD. The goal of this transformation is to process one partition
    at a time (although, many partitions can be processed independently and concurrently),
    iterate through all of the partition’s elements, and summarize the result in a
    compact data structure such as a dictionary, list of elements, tuples, or list
    of tuples.'
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitions()` 是一个强大的分布式映射器转换，它一次处理一个分区（而不是一个元素）。它实现了汇总设计模式，将源 RDD 的每个分区汇总为目标
    RDD 的单个元素。该转换的目标是一次处理一个分区（虽然许多分区可以独立并发地处理），遍历分区的所有元素，并将结果汇总到紧凑的数据结构中，例如字典、元素列表、元组或元组列表。'
- en: 'The `mapPartitions()` transformation has the following signature:'
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitions()` 转换的签名如下：'
- en: '[PRE59]'
  id: totrans-305
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: 'Let’s say that your source RDD has *`N`* partitions. The `mapPartitions()`
    transformation maps a single partition of the source RDD into your desired data
    type, `T` (for example, this could be a single value, a tuple, a list, or a dictionary).
    Therefore, the target RDD will be an `RDD[T]`, of length *`N`*. This is an ideal
    transformation when you want to reduce (or aggregate) each partition comprised
    of a set of source RDD elements into a condensed data structure of type `T`: it
    maps a single partition into a single element of the target RDD.'
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您的源 RDD 有*`N`*个分区。`mapPartitions()` 转换将源 RDD 的单个分区映射为所需的数据类型 `T`（例如，可以是单个值、元组、列表或字典）。因此，目标
    RDD 将是长度为*`N`*的 `RDD[T]`。当您想要将由源 RDD 元素集合组成的每个分区减少（或聚合）为类型 `T` 的紧凑数据结构时，这是一个理想的转换：它将单个分区映射到目标
    RDD 的单个元素中。
- en: A high-level overview is presented in [Figure 3-11](#the_mappartition_transformation).
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: 在[图 3-11](#the_mappartition_transformation)中提供了一个高级概述。
- en: '![daws 0311](Images/daws_0311.png)'
  id: totrans-308
  prefs: []
  type: TYPE_IMG
  zh: '![daws 0311](Images/daws_0311.png)'
- en: Figure 3-11\. The `mapPartition()` transformation
  id: totrans-309
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 图 3-11\. `mapPartition()` 转换
- en: To help you understand the logic of the `mapPartitions()` transformation, I’ll
    present a simple, concrete example. Suppose you have a source `RDD[Integer]` with
    100,000,000,000 elements and your RDD is partitioned into 10,000 chunks or partitions.
    So, each partition will have about 10,000,000 elements. If you have enough cluster
    resources to run 10,000 mappers in parallel, then each mapper will receive a partition.
    Since you will be processing one partition at time, you have the chance to filter
    elements and summarize each partition into a single desired data structure (such
    as a tuple, list, or dictionary).
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: 为了帮助您理解`mapPartitions()`转换的逻辑，我将呈现一个简单具体的例子。假设您有一个包含100,000,000,000个元素的源`RDD[Integer]`，并且您的RDD被分成了10,000个块或分区。因此，每个分区将有大约10,000,000个元素。如果您有足够的集群资源可以并行运行10,000个映射器，那么每个映射器将接收一个分区。由于您将一次处理一个分区，您有机会过滤元素并将每个分区汇总到单个期望的数据结构中（如元组、列表或字典）。
- en: 'Let’s say that you want to find the `(minimum, maximum, count)` for the source
    RDD of numbers. Each mapper will find a local `(minimum, maximum, count)` per
    partition, and then eventually, you can find the final `(minimum, maximum, count)`
    for all of the partitions. Here, the target data type is a triplet:'
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 假设您想要找到数字源RDD的`(minimum, maximum, count)`。每个映射器将为每个分区找到本地的`(minimum, maximum,
    count)`，最终您可以找到所有分区的最终`(minimum, maximum, count)`。这里，目标数据类型是三元组：
- en: '[PRE60]'
  id: totrans-312
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: '`mapPartitions()` is an ideal transformation when you want to map each partition
    into small amount of condensed or reduced information. You can filter out undesired
    elements of the source RDD and then summarize the remaining elements in your data
    structure of choice.'
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: '`mapPartitions()`是一种理想的转换，当您想将每个分区映射为少量紧凑或减少的信息时。您可以过滤源RDD中不需要的元素，然后在您选择的数据结构中总结剩余的元素。'
- en: 'Let’s walk through the main flow of the `mapPartitions()` transformation:'
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们来详细了解`mapPartitions()`转换的主要流程：
- en: 'First, define a function that accepts a single partition of the source RDD
    (an `RDD[Integer]`) and returns a data type `T`, where:'
  id: totrans-315
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 首先，定义一个接受源RDD的单个分区（`RDD[Integer]`）并返回数据类型`T`的函数，其中：
- en: '[PRE61]'
  id: totrans-316
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE61]'
- en: 'Let *`N`* be the number of partitions for your source RDD. Given a partition
    `p` (where `p` in `{1, 2, …, *N*}`), `mapPartitions()` will compute (`minimum[p],
    maximum[p], count[p]`) per partition `p`:'
  id: totrans-317
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 让 *`N`* 是您源RDD的分区数。给定一个分区`p`（其中`p`在`{1, 2, …, *N*}`中），`mapPartitions()`将计算每个分区`p`的(`minimum[p],
    maximum[p], count[p]`)：
- en: '[PRE62]'
  id: totrans-318
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE62]'
- en: 'Next, apply the `mapPartitions()` transformation:'
  id: totrans-319
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，应用`mapPartitions()`转换：
- en: '[PRE63]'
  id: totrans-320
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Finally, we need to collect the content of `min_max_count_rdd` and find the
    final `(minimum, maximum, count)`:'
  id: totrans-321
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们需要收集`min_max_count_rdd`的内容，并找到最终的`(minimum, maximum, count)`：
- en: '[PRE64]'
  id: totrans-322
  prefs:
  - PREF_IND
  type: TYPE_PRE
  zh: '[PRE64]'
- en: 'We can define our function as follows. Note that by using a Boolean flag, `first_time`,
    we avoid making any assumptions about range of numeric values:'
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以定义我们的函数如下。请注意，通过使用布尔标志`first_time`，我们避免对数字值范围做出任何假设：
- en: '[PRE65]'
  id: totrans-324
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: 'Next, let’s create an `RDD[Integer]` and then apply the `mapPartitions()` transformation:'
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们创建一个`RDD[Integer]`，然后应用`mapPartitions()`转换：
- en: '[PRE66]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO15-1)'
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO15-1)'
- en: The `collect()` is scalable here, because the number of partitions will be in
    the thousands and not the millions.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: 这里的`collect()`是可扩展的，因为分区数将在千级而不是百万级。
- en: 'In summary, if you have a large amount of data that you want to reduce to a
    smaller amount of information (a summarization task), the `mapPartitions()` transformation
    is a possible option. For example, it’s very useful for finding the minimum and
    maximum or top 10 values in your dataset. The `mapPartitions()` transformation:'
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: 总之，如果您有大量数据要减少到更少的信息量（汇总任务），则`mapPartitions()`转换是一个可能的选择。例如，它非常适用于查找数据集中的最小值和最大值或前10个值。`mapPartitions()`转换：
- en: Implements the summarization design pattern, combining all the source RDD elements
    in a single partition into a single, compact element of the target RDD (such as
    a dictionary, tuple, or list of objects or tuples).
  id: totrans-330
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现了汇总设计模式，将源RDD中所有元素组合成目标RDD的单个紧凑元素（如字典、元组或对象或元组的列表）。
- en: Can be used as an alternative to `map()` and `foreach()`, but is called once
    per partition instead of for each element in an RD.
  id: totrans-331
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 可以用作`map()`和`foreach()`的替代方案，但是每个分区只调用一次，而不是每个元素调用一次。
- en: Enables the programmer to do initialization on a per-partition rather than per-element
    basis.
  id: totrans-332
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使程序员可以在每个分区而不是每个元素的基础上进行初始化。
- en: 'Next, I’ll discuss a very important topic: how to handle and process an empty
    partition when using the `mapPartitions()` transformation.'
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我将讨论一个非常重要的话题：在使用`mapPartitions()`转换时如何处理和处理空分区。
- en: Handling Empty Partitions
  id: totrans-334
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 处理空分区
- en: In our previous solution, we used the `mapPartitions(func)` transformation,
    which separates input data into many partitions and then applies the function
    `func()` (provided by the programmer) to each partition in parallel. But what
    if one or more of these partitions are empty? In this case, there will be no data
    (no elements in that partition) to iterate. We need to write our custom function
    `func()` (the partition handler) in such a way that it will handle empty partitions
    properly and gracefully. We cannot just ignore them.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们先前的解决方案中，我们使用了`mapPartitions(func)`转换，它将输入数据分成许多分区，然后并行地对每个分区应用程序员提供的函数`func()`。但如果其中一个或多个分区为空怎么办？在这种情况下，将没有数据（该分区中没有元素）进行迭代。我们需要编写我们自己的自定义函数`func()`（分区处理程序），以便正确和优雅地处理空分区。我们不能只是忽略它们。
- en: Empty partitions may occur for various reasons. If there is an exception while
    the Spark partitioner is partitioning the data (for example, due to corrupted
    records after a network failure mid-transfer), then some partitions might be empty.
    Another reason might be that the partitioner does not have enough data to put
    any into a given partition. Regardless of why these partitions exist, we need
    to handle them proactively.
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 空分区可能由于各种原因而发生。如果 Spark 分区器在分区数据时出现异常（例如，在网络传输中途由于损坏的记录导致），则某些分区可能为空。另一个原因可能是分区器没有足够的数据放入某个分区中。无论这些分区为何存在，我们都需要主动处理它们。
- en: 'To illustrate the concept of an empty partition, I’ll first define a function,
    `debug_partition()`, to show the contents of each partition:'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明空分区的概念，我首先定义一个函数`debug_partition()`来显示每个分区的内容：
- en: '[PRE67]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE67]'
- en: Warning
  id: totrans-339
  prefs:
  - PREF_H6
  type: TYPE_NORMAL
  zh: 警告
- en: Remember that displaying or debugging the content of a partition can be costly
    and should be avoided by all means in production environments. I have included
    `print` statements for teaching and debugging purposes only.
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 请记住，在生产环境中，显示或调试分区内容可能会很昂贵，并且应尽量避免。我仅包含了用于教学和调试目的的`print`语句。
- en: 'Now let’s create an RDD and partition it in a way that will force the creation
    of empty partitions. We do this by setting the number of partitions higher than
    the number of RDD elements:'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 现在让我们创建一个 RDD，并以一种方式对其进行分区，以强制创建空分区。我们通过将分区数设置为大于 RDD 元素数来实现这一点：
- en: '[PRE68]'
  id: totrans-342
  prefs: []
  type: TYPE_PRE
  zh: '[PRE68]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO16-1)'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO16-1)'
- en: Force the creation of empty partitions.
  id: totrans-344
  prefs: []
  type: TYPE_NORMAL
  zh: 强制创建空分区。
- en: 'We can examine each partition using the `debug_partition()` function:'
  id: totrans-345
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用`debug_partition()`函数检查每个分区：
- en: '[PRE69]'
  id: totrans-346
  prefs: []
  type: TYPE_PRE
  zh: '[PRE69]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO17-1)'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO17-1)'
- en: An empty partition
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一个空分区
- en: 'From this test program we can observe the following:'
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 从这个测试程序中，我们可以观察到以下内容：
- en: A partition can be empty (with no RDD elements). Your custom function must handle
    empty partitions proactively and gracefully—that is, it must return a proper value.
    Empty partitions cannot be just ignored.
  id: totrans-350
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一个分区可能为空（没有 RDD 元素）。您的自定义函数必须主动和优雅地处理空分区，即必须返回一个合适的值。不能只是忽略空分区。
- en: The `iterator` data type (which represents a single partition and is passed
    as a parameter to `mapPartitions()`) is `itertools.chain`. `itertools.chain` is
    an iterator that returns elements from the first iterable until it is exhausted,
    then proceeds to the next iterable, until all of the iterables are exhausted.
    It’s used for treating consecutive sequences as a single sequence.
  id: totrans-351
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`iterator`数据类型（代表单个分区，并作为参数传递给`mapPartitions()`）是`itertools.chain`。`itertools.chain`是一个迭代器，它从第一个可迭代对象返回元素，直到耗尽，然后继续到下一个可迭代对象，直到所有可迭代对象都耗尽。它用于将连续的序列视为单个序列。'
- en: 'Now the question is, how do we handle an empty partition in PySpark? The following
    pattern can be used to handle an empty partition. The basic idea is to use Python’s
    `try-except` combination, where the `try` block lets you test a block of code
    for errors and the `except` block lets you handle the error:'
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 现在的问题是，我们如何处理 PySpark 中的空分区？以下模式可用于处理空分区。基本思想是使用 Python 的`try-except`组合，其中`try`块允许您测试一段代码的错误，并且`except`块让您处理错误：
- en: '[PRE70]'
  id: totrans-353
  prefs: []
  type: TYPE_PRE
  zh: '[PRE70]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO18-1)'
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO18-1)'
- en: '`iterator` represents a single partition of elements of type `T`.'
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: '`iterator`表示类型为`T`的单个分区元素。'
- en: '[![2](Images/2.png)](#co_mapper_transformations_CO18-2)'
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: '[![2](Images/2.png)](#co_mapper_transformations_CO18-2)'
- en: Try to get the first element (as `first_element`, of type `T`) for a given partition.
    If this fails (throws an exception), then control will go to the `except` (exception
    happened) block.
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: 尝试获取给定分区的第一个元素（作为类型为`T`的`first_element`）。如果失败（抛出异常），控制将转移到`except`（发生异常）块。
- en: '[![3](Images/3.png)](#co_mapper_transformations_CO18-3)'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: '[![3](Images/3.png)](#co_mapper_transformations_CO18-3)'
- en: You will be here when a given partition is empty. You cannot just ignore empty
    partitions, you must handle the error and return a proper value.
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: 当给定分区为空时，您不能简单地忽略空分区，必须处理错误并返回适当的值。
- en: Handling Empty Partitions
  id: totrans-360
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 处理空分区。
- en: Typically, for empty partitions you should return some special value that can
    be filtered out easily by the `filter()` transformation. For example, for the
    DNA base count problem, you might return a `null` value (instead of an actual
    dictionary) and then filter the `null` values after the completion of the `mapPartitions()`
    transformation.
  id: totrans-361
  prefs: []
  type: TYPE_NORMAL
  zh: 通常情况下，对于空分区，您应该返回一些特殊值，这些值可以通过`filter()`转换轻松过滤掉。例如，在DNA碱基计数问题中，您可以返回一个`null`值（而不是实际的字典），然后在`mapPartitions()`转换完成后过滤掉`null`值。
- en: 'To handle an empty partition when looking for the `(min, max, count)`, we will
    rewrite the partition handler function as follows:'
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 当寻找`(min, max, count)`时，为了处理空分区，我们将重写分区处理函数如下：
- en: '[PRE71]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE71]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO19-1)'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO19-1)'
- en: We return `[None]` so that we can filter it out.
  id: totrans-365
  prefs: []
  type: TYPE_NORMAL
  zh: 我们返回`[None]`以便我们可以将其过滤掉。
- en: 'The following code shows how to filter out empty partitions:'
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的代码展示了如何过滤掉空分区：
- en: '[PRE72]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE72]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO20-1)'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO20-1)'
- en: Drop the result of empty partitions.
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 放弃空分区的结果。
- en: Benefits and Drawbacks
  id: totrans-370
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 优缺点。
- en: 'Spark’s `mapPartitions()` is an efficient transformation with numerous benefits,
    summarized here:'
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的`mapPartitions()`是一种效率高、好处多的转换，总结如下：
- en: Low processing overhead
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 低处理开销。
- en: The mapper function is applied once per RDD partition rather than per RDD element,
    which limits the number of function calls to the number of partitions rather than
    the number of elements. Note that for some transformations, such as `map()` and
    `flatMap()`, the overhead of invoking a function for each element in all the partitions
    can be substantial.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: Mapper函数仅对RDD分区应用一次，而不是对RDD元素应用一次，这限制了函数调用次数，使其等于分区数，而不是元素数。请注意，对于某些转换操作（如`map()`和`flatMap()`），为所有分区的每个元素调用函数的开销可能很大。
- en: Efficient local aggregation
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 高效的本地聚合。
- en: Since `mapPartitions()` works on the partition level, it gives the user the
    opportunity to perform filtering and aggregation at that level. This local aggregation
    greatly reduces the amount of shuffled data. With `mapPartitions()`, we are reducing
    a partition into a small, contained data structure. Reducing the amount of sorting
    and shuffling results in greater efficiency and reliability of reduce operations.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 由于`mapPartitions()`在分区级别上工作，它使用户有机会在该级别执行过滤和聚合。这种本地聚合极大地减少了被洗牌的数据量。通过`mapPartitions()`，我们将一个分区减少为一个小而完整的数据结构。减少排序和洗牌操作的数量，提高了减少操作的效率和可靠性。
- en: Avoidance of explicit filtering step
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 避免显式过滤步骤。
- en: This transformation enables us to squeeze in the `filter()` step during iteration
    of a partition (which may be comprised of thousands or millions of elements),
    effectively combining a `map()`/`flatMap()` operation with a `filter()` operation.
    As you iterate partition elements, you can drop the ones you don’t need, then
    map and aggregate the remaining elements into your desired data type (such as
    a list, tuple, dictionary, or custom data type). You can even apply multiple filters
    at the same time. This results in greater efficiency, as you avoid the overhead
    of setting up and managing multiple data transformation steps.
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 此转换使我们能够在迭代分区（可能包含数千或数百万个元素）期间插入`filter()`步骤，有效地将`map()`/`flatMap()`操作与`filter()`操作结合起来。当您迭代分区元素时，可以丢弃不需要的元素，然后将剩余元素映射和聚合成所需的数据类型（例如列表、元组、字典或自定义数据类型）。甚至可以同时应用多个过滤器。这样可以提高效率，避免设置和管理多个数据转换步骤的开销。
- en: Avoidance of repetitive heavy initialization
  id: totrans-378
  prefs: []
  type: TYPE_NORMAL
  zh: 避免重复的繁重初始化。
- en: With `mapPartitions()` you may use broadcast variables (shared among all cluster
    nodes) to initialize the data structures required for aggregation of partition
    elements. If you need to do heavyweight initialization, then you will not pay
    a heavy price, since the number of initializations is limited to the number of
    partitions. When using narrow transformations like `map()` and `flatMap()`, the
    creation of such data structures can be very inefficient due to repetitive initialization
    and de-initialization. With `mapPartitions()`, the initialization is performed
    only once (at the beginning of a function) for all the data records residing in
    a given partition. An example of heavy initialization could be the initialization
    of a database (relational or HBase) connection to read/update/insert a record.
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mapPartitions()`，您可以使用广播变量（在所有集群节点之间共享）来初始化聚合分区元素所需的数据结构。如果需要进行大量的初始化操作，那么代价并不高，因为初始化的次数限于分区的数量。在使用`map()`和`flatMap()`等窄转换时，由于重复的初始化和反初始化，这些数据结构的创建可能非常低效。而使用`mapPartitions()`，初始化仅在函数开始时执行一次，对给定分区中的所有数据记录生效。一个重初始化的例子可能是初始化数据库（关系型或HBase）连接以读取/更新/插入记录。
- en: 'There are also a few potential drawbacks to using the `mapPartitions()` transformation:'
  id: totrans-380
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`mapPartitions()`转换也存在一些潜在的缺点：
- en: Since we are applying a function to the whole partition, debugging might be
    harder than with other mapper transformations.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 由于我们将函数应用于整个分区，调试可能比其他映射器转换更困难。
- en: Proper partitioning of data for `mapPartitions()` is critical. You want to maximize
    the cluster utilization for this kind of transformation; the number of partitions
    should be greater than the number of available mappers/executors so that there
    will not be any idle mappers/executors.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 适当的数据分区对于`mapPartitions()`非常重要。你希望最大化集群对这种转换的利用，分区的数量应该大于可用的映射器/执行器数量，这样就不会有任何空闲的映射器/执行器。
- en: DataFrames and mapPartitions() Transformation
  id: totrans-383
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 数据框架和mapPartitions()转换
- en: 'Given a DataFrame, you can easily summarize your data with a SQL transformation:'
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个DataFrame，你可以使用SQL转换轻松总结你的数据：
- en: '[PRE73]'
  id: totrans-385
  prefs: []
  type: TYPE_PRE
  zh: '[PRE73]'
- en: 'Spark’s DataFrame does not have a direct support for mapPartitions(), but it
    is very easy to apply equivalent of `mapPartitions()` to a DataFrame. The following
    example finds the minimum price for a group of items:'
  id: totrans-386
  prefs: []
  type: TYPE_NORMAL
  zh: Spark的DataFrame并不直接支持mapPartitions()，但很容易将mapPartitions()的等效操作应用于DataFrame。以下示例查找一组物品的最低价格：
- en: '[PRE74]'
  id: totrans-387
  prefs: []
  type: TYPE_PRE
  zh: '[PRE74]'
- en: 'You may apply multiple aggregation functions to a DataFrame:'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: 您可以对DataFrame应用多个聚合函数：
- en: '[PRE75]'
  id: totrans-389
  prefs: []
  type: TYPE_PRE
  zh: '[PRE75]'
- en: 'PySpark’s DataFrame data abstraction does not directly support `mapPartitions()`
    transformation, but if you wish to use it, you may convert your DataFrame into
    an RDD (by applying `DataFrame.rdd`) and then apply `mapPartitions()` transformation
    to an RDD:'
  id: totrans-390
  prefs: []
  type: TYPE_NORMAL
  zh: PySpark的DataFrame数据抽象并不直接支持`mapPartitions()`转换，但如果你希望使用它，你可以将你的DataFrame转换为RDD（通过应用`DataFrame.rdd`），然后对RDD应用`mapPartitions()`转换：
- en: '[PRE76]'
  id: totrans-391
  prefs: []
  type: TYPE_PRE
  zh: '[PRE76]'
- en: 'We may now apply `mapPartitions()` to `my_rdd`:'
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以将`mapPartitions()`应用于`my_rdd`：
- en: '[PRE77]'
  id: totrans-393
  prefs: []
  type: TYPE_PRE
  zh: '[PRE77]'
- en: '[![1](Images/1.png)](#co_mapper_transformations_CO21-1)'
  id: totrans-394
  prefs: []
  type: TYPE_NORMAL
  zh: '[![1](Images/1.png)](#co_mapper_transformations_CO21-1)'
- en: Note that when iterating `partition`, each element (`single_row`) will be a
    `Row` object.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 请注意，在迭代分区时，每个元素（single_row）将是一个Row对象。
- en: Summary
  id: totrans-396
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 总结
- en: 'To recap:'
  id: totrans-397
  prefs: []
  type: TYPE_NORMAL
  zh: 总结一下：
- en: Spark offers many simple and powerful transformations (such as `map()`, `flatMap()`,
    `filter()`, and `mapPartitions()`) that you can use to convert one form of data
    into another. Spark transformations enable us to perform ETL operations in a simple
    way.
  id: totrans-398
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Spark提供了许多简单而强大的转换（如`map()`、`flatMap()`、`filter()`和`mapPartitions()`），可以用来将一种形式的数据转换为另一种形式。Spark转换使我们能够以简单的方式执行ETL操作。
- en: If your data requires you to map one element (such as a `String`) into another
    element (such as a tuple, (key, value pair, or list), you can use the `map()`
    or `flatMap()` transformation.
  id: totrans-399
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果你的数据需要将一个元素（如一个`String`）映射到另一个元素（如元组、键值对或列表），你可以使用`map()`或`flatMap()`转换。
- en: When you want to summarize a lot of data into a small amount of meaningful information
    (the summarization design pattern), `mapPartitions()` is a good choice.
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当你想将大量数据总结为少量有意义的信息时（总结设计模式），`mapPartitions()`是一个不错的选择。
- en: The `mapPartitions()` transformation allows you to do heavy initialization (for
    example, setting up a database connection) once for each partition instead of
    for every RDD element. This can help the performance of your data analysis when
    you are dealing with heavyweight initialization on large datasets.
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`mapPartitions()` 转换允许你在每个分区中进行一次重量级初始化（例如，设置数据库连接），而不是针对每个 RDD 元素都初始化一次。在处理大型数据集上的重量级初始化时，这有助于提高数据分析的性能。'
- en: Some Spark transformations have differences in performance, so you need to select
    the transformations you use in a way that suits both your data and your performance
    needs. For example, for summarizing data, `mapPartitions()` will usually perform
    and scale better than `map()`.
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 一些 Spark 转换在性能上有差异，因此你需要根据数据和性能需求选择合适的转换方式。例如，对于数据汇总，`mapPartitions()` 通常比 `map()`
    的性能和扩展性更好。
- en: The next chapter will focus on reductions in Spark.
  id: totrans-403
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来的章节将专注于 Spark 中的减少操作。
