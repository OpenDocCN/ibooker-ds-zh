- en: 22 Training nonlinear classifiers with decision tree techniques
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 22 使用决策树技术训练非线性分类器
- en: This section covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本节涵盖
- en: Classifying datasets that are not linearly separable
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对非线性可分的数据集进行分类
- en: Automatically generating `if`/`else` logical rules from training data
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 从训练数据自动生成`if`/`else`逻辑规则
- en: What is a decision tree?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是决策树？
- en: What is a random forest?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 什么是随机森林？
- en: Training tree-based models using scikit-learn
  id: totrans-6
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用scikit-learn训练基于树的模型
- en: Thus far, we have investigated supervised learning techniques that rely on the
    geometry of data. This association between learning and geometry does not align
    with our everyday experiences. On a cognitive level, people do not learn through
    abstract spatial analysis; they learn by making logical inferences about the world.
    These inferences can then be shared with others. A toddler realizes that by throwing
    a fake tantrum, they can sometimes get an extra cookie. A parent realizes that
    indulging the toddler inadvertently leads to even more bad behavior. A student
    realizes that through preparation and study, they will usually do well on their
    exam. Such realizations are not particularly new; they are part of our collective
    social wisdom. Once a useful logical inference has been made, it can be shared
    with others for broader use. Such sharing is the basis of modern science. A scientist
    realizes that certain viral proteins make good targets for a drug. They publish
    their inferences in a journal, and that knowledge propagates across the entire
    scientific community. Eventually, a new antiviral drug is developed based on the
    scientific findings.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 到目前为止，我们已经研究了依赖于数据几何形状的监督学习技术。这种学习和几何之间的关联并不符合我们的日常经验。在认知层面上，人们不是通过抽象的空间分析来学习的；他们通过关于世界的逻辑推理来学习。这些推理可以与他人分享。一个蹒跚学步的孩子意识到，通过假装发脾气，他们有时可以得到额外的饼干。一个家长意识到，无意中纵容孩子会导致更多的坏行为。一个学生意识到，通过准备和学习，他们通常会在考试中做得很好。这样的认识并不特别新颖；它们是我们集体社会智慧的一部分。一旦做出有用的逻辑推理，就可以与他人分享，以供更广泛的使用。这种分享是现代科学的基础。一位科学家意识到某些病毒蛋白是药物的良好靶点。他们在期刊上发表他们的推理，这些知识在整个科学界传播。最终，基于科学发现，开发出了一种新的抗病毒药物。
- en: In this section, we learn how to algorithmically derive logical inferences from
    our training data. These simple, logical rules will provide us with predictive
    models that are not constrained by the data’s geometry.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们学习如何从我们的训练数据中算法性地推导出逻辑推理。这些简单的逻辑规则将为我们提供不受数据几何形状约束的预测模型。
- en: 22.1 Automated learning of logical rules
  id: totrans-9
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.1 自动学习逻辑规则
- en: Let’s analyze a seemingly trivial problem. Suppose a lightbulb hangs above a
    stairwell. The bulb is connected to two switches at the top and bottom of the
    stairs. When both switches are off, the bulb stays off. If either switch is turned
    on, the bulb shines. However, if both switches are flipped on, the bulb turns
    off. This arrangement allows us to activate the light when we are at the bottom
    of the stairs and then deactivate it after we ascend.
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们分析一个看似简单的问题。假设一个灯泡悬挂在楼梯井上方。灯泡连接到楼梯顶部和底部的两个开关。当两个开关都关闭时，灯泡保持关闭状态。如果任一开关打开，灯泡就会亮起。然而，如果两个开关都打开，灯泡会熄灭。这种安排允许我们在楼梯底部激活灯光，然后在上升后将其关闭。
- en: We can represent the off and on states of the switches and the bulb as binary
    digits 0 and 1\. Given two switch variables `switch0` and `switch1`, we can trivially
    show that the bulb is on whenever `switch0 + switch1 == 1`. Using the material
    from the previous two sections, can we train a classifier to learn this trivial
    relationship? We’ll find out by storing every possible light switch combination
    in a two-column feature matrix `X`. Then we’ll plot the matrix rows in 2D space
    while marking each point based on the corresponding on/off state of the bulb (figure
    22.1). The plot will give us insight into how KNN and linear classifiers can handle
    this classification problem.
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以将开关的开启和关闭状态以及灯泡的状态表示为二进制数字0和1。给定两个开关变量`switch0`和`switch1`，我们可以简单地表明，当`switch0
    + switch1 == 1`时，灯泡是开启的。使用前两节的内容，我们能否训练一个分类器来学习这种简单的关系？我们将通过存储所有可能的光开关组合在一个两列的特征矩阵`X`中，然后我们将绘制矩阵的行在二维空间中，并标记每个点基于灯泡的对应开启/关闭状态（图22.1）。这个图将给我们提供关于KNN和线性分类器如何处理这个分类问题的洞察。
- en: '![](../Images/22-01.png)'
  id: totrans-12
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-01.png)'
- en: Figure 22.1 Plotting all the states of the light switch system. An activated
    lightbulb is represented by an X, and a deactivated bulb is represented by an
    O. The nearest neighbors of each O are X markers (and vice versa). Thus, KNN cannot
    be used for classification. Also, there’s no linear separation between the X markers
    and O markers, so linear classification cannot be applied.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.1 绘制灯泡开关系统的所有状态。激活的灯泡用 X 表示，未激活的灯泡用 O 表示。每个 O 的最近邻是 X 标记（反之亦然）。因此，KNN 不能用于分类。此外，X
    标记和 O 标记之间没有线性分离，所以不能应用线性分类。
- en: Listing 22.1 Plotting the two-switch problem in 2D space
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.1 在二维空间绘制双开关问题
- en: '[PRE0]'
  id: totrans-15
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: The four plotted points lie on the four corners of a square. Each pair of points
    in the same class is positioned diagonally on that square, and all adjacent points
    belong to different classes. The two nearest neighbors of each on-switch combination
    are members of the off class (and vice versa). Therefore, KNN will fail to properly
    classify the data. There also is no linear separation between the labeled classes,
    so we cannot draw a linear boundary without cutting through a diagonal that connects
    two identically classified points. Consequently, training a linear classifier
    is also out of the question. What should we do? One approach is to define two
    nested `if`/`else` statements as our prediction model. Let’s code and test this
    `if`/`else` classifier.
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 四个绘制的点位于正方形的四个角上。同一类中的每对点在该正方形上对角排列，所有相邻的点属于不同的类。每个开启开关组合的两个最近邻是关闭类成员（反之亦然）。因此，KNN
    将无法正确分类数据。此外，标记的类之间也没有线性分离，所以我们不能在不穿过连接两个相同分类点的对角线的情况下绘制线性边界。因此，训练线性分类器也成问题。我们应该怎么办？一种方法是将两个嵌套的
    `if`/`else` 语句定义为我们的预测模型。让我们编写并测试这个 `if`/`else` 分类器。
- en: Listing 22.2 Classifying data using nested `if`/`else` statements
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.2 使用嵌套 `if`/`else` 语句分类数据
- en: '[PRE1]'
  id: totrans-18
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: Our `if`/`else` classifier is 100% accurate, but we didn’t train it. Instead,
    we programmed the classifier ourselves. Manual model construction does not count
    as supervised machine learning, so we need to find a way to automatically derive
    accurate `if`/`else` statements from the training data. Let’s figure out how.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的 `if`/`else` 分类器是 100% 准确的，但我们没有对其进行训练。相反，我们亲自编程了分类器。手动模型构建不算作监督机器学习，因此我们需要找到一种方法，从训练数据中自动推导出准确的
    `if`/`else` 语句。让我们弄清楚如何做到这一点。
- en: We start with a simple training example. Our training set represents a series
    of recorded observations between a single light switch and a single bulb. Whenever
    the switch is on, the bulb is on, and vice versa. We randomly flip the lightbulb
    on and off and record what we see. The state of the bulb is recorded in a `y_simple`
    array. Our single feature, corresponding to the switch, is recorded in a single-column
    `X_simple` matrix. Of course, `X_simple[i][0]` will always equal `y[i]`. Let’s
    generate this basic training set.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 我们从一个简单的训练示例开始。我们的训练集代表了一个单一开关和单一灯泡之间的一系列记录观察。每当开关开启时，灯泡也开启，反之亦然。我们随机地打开和关闭灯泡，并记录我们所看到的情况。灯泡的状态记录在
    `y_simple` 数组中。我们对应开关的单个特征记录在一个单列的 `X_simple` 矩阵中。当然，`X_simple[i][0]` 将始终等于 `y[i]`。让我们生成这个基本训练集。
- en: Listing 22.3 Generating a single-switch training set
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.3 生成单个开关训练集
- en: '[PRE2]'
  id: totrans-22
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ The state of the bulb is simulated using a random coin flip.
  id: totrans-23
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 灯泡的状态是通过随机抛硬币来模拟的。
- en: ❷ The state of the switch is always equal to the state of the bulb.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 开关的状态始终等于灯泡的状态。
- en: Next, we count all observations in which the switch is off and the light is
    off.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们计算所有开关关闭且灯泡关闭的观察结果。
- en: Listing 22.4 Counting the off-state co-occurrences
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.4 计算关闭状态共现次数
- en: '[PRE3]'
  id: totrans-27
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: Now, let’s count the instances in which both the switch and the lightbulb are
    turned on.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们计算开关和灯泡都开启的实例。
- en: Listing 22.5 Counting the on-state co-occurrences
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.5 计算开启状态共现次数
- en: '[PRE4]'
  id: totrans-30
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: These co-occurrences will prove useful during classifier training. Let’s track
    the counts more systematically in a co-occurrence matrix `M`. The matrix rows
    track the off/on state of the switch, and the matrix columns track the state of
    the bulb. Each element `M[i][j]` counts the number of examples where the switch
    is in state `i` and the bulb is in state `j`. Hence, `M[0][0]` should equal 7,
    and `M[1][1]` should equal 3.
  id: totrans-31
  prefs: []
  type: TYPE_NORMAL
  zh: 这些共现将在分类器训练期间非常有用。让我们在一个共现矩阵 `M` 中更系统地跟踪计数。矩阵行跟踪开关的开启/关闭状态，矩阵列跟踪灯泡的状态。每个元素 `M[i][j]`
    计算开关处于状态 `i` 且灯泡处于状态 `j` 的示例数量。因此，`M[0][0]` 应等于 7，`M[1][1]` 应等于 3。
- en: We now define a `get_co_occurrence` function to compute the co-occurrence matrix.
    The function takes as input a training set `(X, y)`, as well as a column index
    `col`. It returns the co-occurrences between all classes in `y` and all feature
    states in `X[:,col]`.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在定义一个`get_co_occurrence`函数来计算共现矩阵。该函数接受一个训练集`(X, y)`和一个列索引`col`作为输入。它返回`y`中所有类别与`X[:,col]`中所有特征状态之间的共现。
- en: Listing 22.6 Computing a co-occurrence matrix
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.6 计算共现矩阵
- en: '[PRE5]'
  id: totrans-34
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: Using `get_co_occurrence`, we’ve computed matrix `M`. All co-occurrences lie
    along the diagonal of the matrix. The bulb is never on when the switch is flipped
    off, and vice versa. However, let’s suppose that there’s a flaw in the switch
    toggle. We flip the switch off, but the bulb stays on! Let’s add this anomalous
    observation to our data and then recompute matrix `M`.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 使用`get_co_occurrence`，我们已经计算了矩阵`M`。所有的共现都位于矩阵的对角线上。当开关被翻转关闭时，灯泡永远不会开启，反之亦然。然而，假设开关切换有缺陷。我们关闭开关，但灯泡保持开启！让我们将这个异常观察结果添加到我们的数据中，然后重新计算矩阵`M`。
- en: Listing 22.7 Adding a flawed mismatch to the data
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.7 向数据中添加一个有缺陷的不匹配
- en: '[PRE6]'
  id: totrans-37
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: When we shut off the switch, the lightbulb turns off most of the time, but it
    doesn’t turn off every time. How accurately can we predict the state of the lightbulb
    if we know that the switch is off? To find out, we must divide `M[0]` by `M[0].sum()`.
    Doing so produces a probability distribution over possible lightbulb states whenever
    the switch state is set to `0`.
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们关闭开关时，灯泡大多数时候会关闭，但并不是每次都会关闭。如果我们知道开关是关闭的，我们如何准确地预测灯泡的状态？为了找出答案，我们必须将`M[0]`除以`M[0].sum()`。这样做会产生一个概率分布，当开关状态设置为`0`时，它覆盖了可能的光泡状态。
- en: Listing 22.8 Computing bulb probabilities when the switch is off
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.8 当开关关闭时计算灯泡概率
- en: '[PRE7]'
  id: totrans-40
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: When the switch is off, we should assume that the bulb is off. Our guess will
    be correct 75% of the time. This fraction of correction fits our definition of
    accuracy from section 20, so when the switch is off, we can predict the state
    of the bulb with 75% accuracy.
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 当开关关闭时，我们应该假设灯泡是关闭的。我们的猜测将有75%的时间是正确的。这个修正分数符合我们在第20节中定义的准确度，因此当开关关闭时，我们可以以75%的准确度预测灯泡的状态。
- en: Now let’s optimize the accuracy for the scenario in which the switch is on.
    We start by computing `bulb_probs` over `M[1]`. Next, we choose the switch state
    corresponding to the maximum probability. Basically, we infer that the bulb state
    is equal to `bulb_probs.argmax()` with an accuracy score of `bulb_probs.max()`.
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来优化开关开启时的准确度。我们首先在`M[1]`上计算`bulb_probs`。接下来，我们选择对应最大概率的开关状态。基本上，我们推断灯泡状态等于`bulb_probs.argmax()`，准确度评分为`bulb_probs.max()`。
- en: Listing 22.9 Predicting the state of the bulb when the switch is on
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.9 预测开关开启时灯泡的状态
- en: '[PRE8]'
  id: totrans-44
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: When the switch is off, we assume that the bulb is off with 75% accuracy. When
    the switch is on, we assume the bulb is on with 100% accuracy. How do we combine
    these values into a single accuracy score? Naively, we could simply average 0.75
    and 1.0, but that approach would be erroneous. The two accuracies should not be
    weighted evenly since the switch is on nearly twice as often as it is off. We
    can confirm by summing over the columns of our co-occurrence matrix `M`. Running
    `M.sum(axis=1)` returns the count of `off` states and `on` states for the switch.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 当开关关闭时，我们以75%的准确度假设灯泡是关闭的。当开关开启时，我们以100%的准确度假设灯泡是开启的。我们如何将这些值组合成一个单一的准确度评分？天真地，我们可以简单地平均0.75和1.0，但这种做法是错误的。这两个准确度不应该平均加权，因为开关开启的频率几乎是关闭的两倍。我们可以通过在共现矩阵`M`的列上求和来确认。运行`M.sum(axis=1)`返回开关的开启和关闭状态的计数。
- en: Listing 22.10 Counting the `on` and `off` states of the switch
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.10 计算开关的开启和关闭状态
- en: '[PRE9]'
  id: totrans-47
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: The switch is on more frequently than it is off. Hence, to get a meaningful
    accuracy score, we need to take the weighted average of 0.75 and 1.0\. The weights
    should correspond to the on/off switch counts obtained from `M`.
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 开关处于开启状态比关闭状态的频率更高。因此，为了得到一个有意义的准确度评分，我们需要对0.75和1.0进行加权平均。权重应与从`M`中获得的开启/关闭开关计数相对应。
- en: Listing 22.11 Computing total accuracy
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.11 计算总准确度
- en: '[PRE10]'
  id: totrans-50
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'If the switch is off, we predict that the bulb is off; otherwise, we predict
    that the bulb is on. This model is 91% accurate. Furthermore, the model can be
    represented as a simple `if`/`else` statement in Python. Most importantly, we
    are able to train the model from scratch using the following steps:'
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 如果开关关闭，我们预测灯泡是关闭的；否则，我们预测灯泡是开启的。这个模型准确率为91%。此外，该模型可以用Python中的简单`if`/`else`语句表示。最重要的是，我们能够使用以下步骤从头开始训练模型：
- en: Choose a feature in our feature matrix `X`.
  id: totrans-52
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在我们的特征矩阵`X`中选择一个特征。
- en: Count the co-occurrences between the two possible feature states and the two
    class types. These co-occurrence counts are stored in a two-by-two matrix `M`.
  id: totrans-53
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算两个可能的特征状态和两个类别类型之间的共现次数。这些共现次数存储在一个二乘二的矩阵`M`中。
- en: 'For row `i` in `M`, compute the probability distribution of classes whenever
    the feature is in state `i`. This probability distribution is equal to `M[i] /
    M[i].sum()`. There are just two rows in `M`, so we can store distributions in
    two variables: `probs0` and `probs1`.'
  id: totrans-54
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于`M`中的行`i`，当特征处于状态`i`时，计算类别的概率分布。这个概率分布等于`M[i] / M[i].sum()`。`M`中只有两行，因此我们可以将分布存储在两个变量中：`probs0`和`probs1`。
- en: Define the `if` portion of our conditional model. If the feature equals 0, we
    return a label of `probs0.argmax()`. This maximizes the accuracy of the `if` statement.
    That accuracy is equal to `probs0.max()`.
  id: totrans-55
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们条件模型的`if`部分。如果特征等于0，我们返回`probs0.argmax()`标签。这最大化了`if`语句的准确性。这个准确性等于`probs0.max()`。
- en: Define the `else` portion of our conditional model. When the feature does not
    equal 0, we return a label of `probs1.argmax()`. This maximizes the accuracy of
    the `else` statement. That accuracy is equal to `probs1.max()`.
  id: totrans-56
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 定义我们条件模型的`else`部分。当特征不等于0时，我们返回`probs1.argmax()`标签。这最大化了`else`语句的准确性。这个准确性等于`probs1.max()`。
- en: Combine the `if` and `else` statements into a single conditional `if`/`else`
    statement. Occasionally, `probs0.argmax()` will equal `probs1.argmax()`. In such
    circumstances, using an `if`/`else` statement is redundant. Instead, we can return
    the trivial rule `f"prediction = {probs0.argmax()}"`.
  id: totrans-57
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`if`和`else`语句合并为一个单一的`if`/`else`条件语句。偶尔，，`probs0.argmax()`将等于`probs1.argmax()`。在这种情况下，使用`if`/`else`语句是多余的。相反，我们可以返回简单的规则`f"prediction
    = {probs0.argmax()}"`。
- en: The accuracy of the combined `if`/`else` statement equals the weighted average
    of `probs0.max()` and `probs1.max()`. The weights correspond to the count of feature
    states obtained by summing up the columns of `M`.
  id: totrans-58
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 结合的`if`/`else`语句的准确性等于`probs0.max()`和`probs1.max()`的加权平均值。权重对应于通过求和`M`的列获得的特征状态的数量。
- en: Let’s define a `train_if_else` function to carry out these seven steps. The
    function returns a trained `if`/`else` statement along with the corresponding
    accuracy.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`train_if_else`函数来执行这七个步骤。该函数返回训练好的`if`/`else`语句及其相应的准确性。
- en: Listing 22.12 Training a simple `if`/`else` model
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 列出22.12训练简单的`if`/`else`模型
- en: '[PRE11]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: ❶ Trains an if/else statement on the training set (X, y) and returns the written
    statement along with the corresponding accuracy. The statement is trained on the
    feature in X[:,feature_col]. The corresponding feature name is stored in feature_name.
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在训练集（X, y）上训练if/else语句，并返回相应的语句及其准确性。该语句在X[:,feature_col]中的特征上训练。相应的特征名称存储在feature_name中。
- en: ❷ Creates the written if/else statement
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 创建if/else语句
- en: ❸ If both parts of the conditional statement return the same prediction, we
    simplify the statement to just that prediction.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 如果条件语句的两个部分都返回相同的预测，我们将语句简化为仅该预测。
- en: We’re able to train a simple `if`/`else` model using a single feature. Now,
    let’s figure out how to train a nested `if`/`else` model using two features. Later,
    we’ll expand that logic to more than two features.
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够使用单个特征训练一个简单的`if`/`else`模型。现在，让我们弄清楚如何使用两个特征训练嵌套的`if`/`else`模型。稍后，我们将扩展这个逻辑到超过两个特征。
- en: 22.1.1 Training a nested if/else model using two features
  id: totrans-66
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.1.1 使用两个特征训练嵌套的if/else模型
- en: 'Let’s return to our system of two light switches connecting to a single staircase
    bulb. As a reminder, all states of this system are represented by dataset `(X,
    y)`, generated in listing 22.1\. Our two features, `switch0` and `switch1`, correspond
    to columns `0` and `1` of matrix `X`. However, the `train_if_else` function can
    only train on one column at a time. Let’s train two separate models: one on `switch0`
    and another on `switch1`. How well will each of the models perform? We’ll find
    out by outputting their accuracies.'
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的两个开关连接到单个楼梯灯泡的系统。作为提醒，这个系统的所有状态都由列表 22.1 生成的数据集 `(X, y)` 表示。我们的两个特征
    `switch0` 和 `switch1` 对应于矩阵 `X` 的列 `0` 和 `1`。然而，`train_if_else` 函数一次只能训练一个列。让我们训练两个单独的模型：一个在
    `switch0` 上，另一个在 `switch1` 上。每个模型的表现将如何？我们将通过输出它们的准确度来找出答案。
- en: Listing 22.13 Training models on the two-switch system
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.13 在双开关系统中训练模型
- en: '[PRE12]'
  id: totrans-69
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: 'Both models perform terribly! A single `if`/`else` statement is insufficient
    to capture the complexity of the problem. What should we do? One approach is to
    break the problem into parts by training two separate models: Model A considers
    only those scenarios in which `switch0` is off, and Model B considers all remaining
    scenarios in which `switch0` is on. Later, we’ll combine Model A and Model B into
    a single coherent classifier.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 两个模型都表现得很糟糕！一个单独的 `if`/`else` 语句不足以捕捉问题的复杂性。我们应该怎么办？一种方法是将问题分解成几个部分，通过训练两个单独的模型：模型
    A 考虑 `switch0` 关闭的所有场景，模型 B 考虑 `switch0` 开启的所有剩余场景。稍后，我们将模型 A 和模型 B 结合成一个单一的、连贯的分类器。
- en: Let’s investigate the first scenario, where `switch0` is off. When it is off,
    `X[:,0] == 0`. Hence, we start by isolating a training subset that satisfies this
    Boolean requirement. We store this training subset in variables `X_switch0_off`
    and `y_switch0_off`.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们调查第一种情况，其中 `switch0` 是关闭的。当它关闭时，`X[:,0] == 0`。因此，我们首先隔离一个满足这个布尔要求的训练子集。我们将这个训练子集存储在变量
    `X_switch0_off` 和 `y_switch0_off` 中。
- en: Listing 22.14 Isolating a training subset where `switch0` is off
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.14 隔离 `switch0` 关闭的训练子集
- en: '[PRE13]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: ❶ All elements of column 0 now equal 0.
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 列 0 的所有元素现在都等于 0。
- en: In the training subset, `switch0` is always off. Hence, `X_switch0_off[:,0]`
    always equals zero. This zero column is now redundant, and we can delete the useless
    column using NumPy’s `np.delete` function.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 在训练子集中，`switch0` 总是关闭的。因此，`X_switch0_off[:,0]` 总是等于零。现在这个零列是冗余的，我们可以使用 NumPy
    的 `np.delete` 函数删除这个无用的列。
- en: Listing 22.15 Deleting a redundant feature column
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.15 删除冗余特征列
- en: '[PRE14]'
  id: totrans-77
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: ❶ Running np.delete(X, r) returns a copy of X with row r deleted, and running
    np.delete(X, c, axis=1) returns a copy of X with column c deleted. Here, we delete
    the redundant column 0.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 运行 np.delete(X, r) 返回删除了第 r 行的 X 的副本，运行 np.delete(X, c, axis=1) 返回删除了第 c 列的
    X 的副本。在这里，我们删除了冗余的列 0。
- en: ❷ The 0 element column has been deleted.
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 已删除 0 元素列。
- en: Next, we train an `if`/`else` model on the training subset. The model predicts
    bulb activation based on the `switch1` state. These predictions are valid only
    if `switch0` is off. We store the model in a `switch0_off_model` variable, and
    we store the model’s accuracy in a corresponding `switch0_off_accuracy` variable.
  id: totrans-80
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们在训练子集上训练一个 `if`/`else` 模型。该模型根据 `switch1` 的状态预测灯泡的激活。这些预测仅在 `switch0`
    关闭时有效。我们将模型存储在 `switch0_off_model` 变量中，并将模型的准确度存储在相应的 `switch0_off_accuracy` 变量中。
- en: Listing 22.16 Training a model when `switch0` is off
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.16 当 `switch0` 关闭时训练模型
- en: '[PRE15]'
  id: totrans-82
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: If `switch0` is off, then our trained `if`/`else` model can predict the bulb
    state with 100% accuracy. Now, let’s train a corresponding model to cover all
    the cases where `switch0` is on. We start by filtering our training data based
    on the condition `X[:,0] == 1`.
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `switch0` 关闭，那么我们训练的 `if`/`else` 模型可以以 100% 的准确度预测灯泡的状态。现在，让我们训练一个相应的模型来覆盖
    `switch0` 开启的所有情况。我们首先根据条件 `X[:,0] == 1` 过滤我们的训练数据。
- en: Listing 22.17 Isolating a training subset where `switch0` is on
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.17 隔离 `switch0` 开启的训练子集
- en: '[PRE16]'
  id: totrans-85
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Filters the training data based on the feature in the feature_col column of
    matrix X. Returns a subset of the training data where the feature equals a specified
    condition value.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 根据矩阵 X 中特征列的特征过滤训练数据。返回一个子集，其中特征等于指定的条件值。
- en: ❷ A Boolean array where the *i*th element is True if X[i][feature_col] equals
    condition
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 一个布尔数组，其中第 *i* 个元素为 True，如果 X[i][feature_col] 等于条件
- en: ❸ The column feature_col becomes redundant since all the filtered values equal
    condition. Hence, this column is filtered from the training data.
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 由于所有过滤后的值都等于条件，列特征 feature_col 变得冗余。因此，这个列从训练数据中被过滤掉。
- en: Next, we train `switch0_on_model` using the filtered training set.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用过滤后的训练集训练 `switch0_on_model`。
- en: Listing 22.18 Training a model when `switch0` is on
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.18 当 `switch0` 开启时训练模型
- en: '[PRE17]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: If `switch == 0`, then `switch0_off_model` performs with 100% accuracy. In all
    other cases, the `switch1_on_model` performs with 100% accuracy. Together, the
    two models can easily be combined into a single nested `if`/`else` statement.
    Here, we define a `combine_if_else` function that merges two separate `if`/`else`
    statements; we then apply that function to our two models.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 如果 `switch == 0`，则 `switch0_off_model` 以 100% 的准确率执行。在所有其他情况下，`switch1_on_model`
    以 100% 的准确率执行。这两个模型可以很容易地合并成一个嵌套的 `if`/`else` 语句。在这里，我们定义了一个 `combine_if_else`
    函数来合并两个单独的 `if`/`else` 语句；然后我们将该函数应用于我们的两个模型。
- en: Listing 22.19 Combining separate `if`/`else` models
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.19 合并单独的 `if`/`else` 模型
- en: '[PRE18]'
  id: totrans-94
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: ❶ Combines two if/else statements if_else_a and if_else_b into a single nested
    statement
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将两个 if/else 语句 if_else_a 和 if_else_b 合并成一个嵌套语句
- en: ❷ The standard four-space Python indent is added to each statement during nesting.
  id: totrans-96
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在嵌套过程中，每个语句都添加了标准的四个空格缩进。
- en: ❸ This helper function helps indent all statements during nesting.
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 这个辅助函数有助于在嵌套过程中缩进所有语句。
- en: We’ve reproduced the nested `if`/`else` model from listing 22.2\. This model
    is 100% accurate. We can confirm by taking the weighted average of `off_accuracy`
    and `on_accuracy`. These accuracies correspond to the `off`/`on` states of `switch0`,
    so their weights should correspond to the off/on counts associated with `switch0`.
    The counts equal the lengths of the `y_switch0_off` and `y_switch0_on` arrays.
    Let’s take the weighted average and confirm that the total accuracy equals 1.0.
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经重现了列表 22.2 中的嵌套 `if`/`else` 模型。此模型准确率为 100%。我们可以通过取 `off_accuracy` 和 `on_accuracy`
    的加权平均来确认。这些准确率对应于 `switch0` 的 `off`/`on` 状态，因此它们的权重应该对应于与 `switch0` 相关的 off/on
    计数。这些计数等于 `y_switch0_off` 和 `y_switch0_on` 数组的长度。让我们取加权平均并确认总准确率等于 1.0。
- en: Listing 22.20 Computing total nested accuracy
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.20 计算总嵌套准确率
- en: '[PRE19]'
  id: totrans-100
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: 'We’re able to generate a nested two-feature model in an automated manner. Our
    strategy hinges on the creation of separate training sets. That separation is
    determined by the `on`/`off` states of one of the features. This type of separation
    is called a *binary split*. Typically, we split training set `(X, y)` using two
    parameters:'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够以自动化的方式生成嵌套的双特征模型。我们的策略依赖于创建单独的训练集。这种分离由一个特征的一个 `on`/`off` 状态决定。这种类型的分离称为
    *二元分割*。通常，我们使用两个参数来分割训练集 `(X, y)`：
- en: Feature `i` corresponding to column `i` of `X`. For example, `switch0` in column
    `0` of `X`.
  id: totrans-102
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 特征 `i` 对应于 `X` 的第 `i` 列。例如，`switch0` 在 `X` 的第 `0` 列。
- en: Condition `c`, where `X[:,i] == c` is `True` for some but not all of the data
    points. For example, condition `0` corresponding to the `off` state.
  id: totrans-103
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 条件 `c`，其中 `X[:,i] == c` 对于某些但不是所有数据点是 `True`。例如，条件 `0` 对应于 `off` 状态。
- en: 'A split on feature `i` and condition `c` can be carried out as follows:'
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 在特征 `i` 和条件 `c` 上进行分割可以按以下方式进行：
- en: Obtain a training subset `(X_a, y_a`) in which `X_a[:,i] == c`.
  id: totrans-105
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个训练子集 `(X_a, y_a)`，其中 `X_a[:,i] == c`。
- en: Obtain a training subset `(X_b, y_b`) in which `X_b[:,i] != c`.
  id: totrans-106
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 获取一个训练子集 `(X_b, y_b)`，其中 `X_b[:,i] != c`。
- en: Delete column `i` from both `X_a` and `X_b`.
  id: totrans-107
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 从 `X_a` 和 `X_b` 中删除第 `i` 列。
- en: Return the separated subsets `(X_a, y_a)` and `(X_b, y_b)`.
  id: totrans-108
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 返回分离的子集 `(X_a, y_a)` 和 `(X_b, y_b)`。
- en: Note These steps are not intended to run on a continuous feature. Later in this
    section, we discuss how to transform a continuous feature into a binary variable
    to carry out the split.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些步骤不是为了在连续特征上运行而设计的。在本节稍后，我们将讨论如何将连续特征转换为二元变量以执行分割。
- en: Let’s define a `split` function to execute these steps. Then we’ll incorporate
    this function into a systematic training pipeline.
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个 `split` 函数来执行这些步骤。然后我们将此函数集成到一个系统化的训练流程中。
- en: Listing 22.21 Defining a binary split function
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.21 定义二元分割函数
- en: '[PRE20]'
  id: totrans-112
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: ❶ Carries out a binary split across the feature in column feature_col of feature
    matrix X
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在特征矩阵 `X` 的第 `feature_col` 列的特征上进行二元分割
- en: ❷ The split creates two training sets (X_a, y_a) and (X_b, y_b). In the first
    training set, X_a[:,feature_col] always equals condition.
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 分割创建了两个训练集 (X_a, y_a) 和 (X_b, y_b)。在第一个训练集中，X_a[:,feature_col] 总是等于条件。
- en: ❸ In the second training set, X_a[:,feature_col] never equals condition.
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 在第二个训练集中，X_a[:,feature_col] 从不等于条件。
- en: By splitting on `switch0`, we were able to train a nested model. Before that
    split, we first tried training simple `if`/`else` models. These models performed
    terribly—we had no choice but to split the training data. But trained nested models
    should still be compared with simpler models returned by `train_if_else`. If a
    simpler model shows comparable performance, it should be returned instead.
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在`switch0`上拆分，我们能够训练一个嵌套模型。在拆分之前，我们首先尝试训练简单的`if`/`else`模型。这些模型表现极差——我们别无选择，只能拆分训练数据。但是，训练好的嵌套模型应该与`train_if_else`返回的简单模型进行比较。如果简单模型表现出可比较的性能，则应返回该简单模型。
- en: 'Note A nested two-feature model will never perform worse than a simple model
    based on a single feature. However, it is possible for the two models to perform
    equally well. Under these circumstances, it’s best to follow Occam’s razor: when
    two competing theories make exactly the same predictions, the simpler one is the
    better theory.'
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：嵌套的双特征模型永远不会比基于单个特征的简单模型表现得更差。然而，两个模型可能表现同样好。在这种情况下，最好遵循奥卡姆剃刀原则：当两个竞争的理论做出完全相同的预测时，更简单的理论是更好的理论。
- en: 'Let’s formalize our process for training two-feature nested models. Given training
    set `(X, y)`, we carry out these steps:'
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们形式化训练双特征嵌套模型的过程。给定训练集`(X, y)`，我们执行以下步骤：
- en: Choose a feature `i` to split on. Initially, that feature is specified using
    a parameter. Later, we learn how to choose that feature in an automated manner.
  id: totrans-119
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 选择一个用于拆分的特征`i`。最初，该特征由参数指定。稍后，我们将学习如何以自动化的方式选择该特征。
- en: Attempt to train a simple, single feature model on feature `i`. If that model
    performs with 100% accuracy, return it as our output.
  id: totrans-120
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试在特征`i`上训练一个简单的单特征模型。如果该模型以100%的准确度运行，则将其作为我们的输出返回。
- en: Theoretically, we could train two single-feature models on columns `0` and `1`
    using `train_if_else`. Then we could compare all single-feature models systematically.
    However, this approach won’t scale when we increase the feature count from two
    to *N*.
  id: totrans-121
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 理论上，我们可以使用`train_if_else`在列`0`和`1`上训练两个单特征模型。然后我们可以系统地比较所有单特征模型。然而，当我们从两个特征增加到*N*个特征时，这种方法不会扩展。
- en: Split on feature `i` using `split`. The function returns two training sets `(X_a,
    y_a)` and `(X_b, y_b)`.
  id: totrans-122
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`split`在特征`i`上拆分。该函数返回两个训练集`(X_a, y_a)`和`(X_b, y_b)`。
- en: Train two simple models `if_else_a` and `if_else_b` using the training sets
    returned by `split`. The corresponding accuracies equal `accuracy_a` and `accuracy_b`.
  id: totrans-123
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`split`返回的训练集训练两个简单模型`if_else_a`和`if_else_b`。相应的准确度等于`accuracy_a`和`accuracy_b`。
- en: Combine `if_else_a` and `if_else_b` into a nested `if`/`else` conditional model.
  id: totrans-124
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将`if_else_a`和`if_else_b`合并为一个嵌套的`if`/`else`条件模型。
- en: Compute the nested model’s accuracy using the weighted mean of `accuracy_a`
    and `accuracy_b`. The weights equal `y_a.size` and `y_b.size`.
  id: totrans-125
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用`accuracy_a`和`accuracy_b`的加权平均值来计算嵌套模型的准确度。权重等于`y_a.size`和`y_b.size`。
- en: Return the nested model if it outperforms the simple model computed in step
    2\. Otherwise, return the simple model.
  id: totrans-126
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果嵌套模型在步骤2中计算出的简单模型上表现更好，则返回嵌套模型。否则，返回简单模型。
- en: Let’s define a `train_nested_if_else` function to execute these steps. The function
    returns the trained model and that model’s accuracy.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们定义一个`train_nested_if_else`函数来执行这些步骤。该函数返回训练好的模型及其准确度。
- en: Listing 22.22 Training a nested `if`/`else` model
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.22：训练嵌套的`if`/`else`模型
- en: '[PRE21]'
  id: totrans-129
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: ❶ Trains a nested if/else statement on the two-feature training set (X, y) and
    returns the written statement along with the corresponding accuracy. The statement
    is trained by splitting on the feature in X[:,split_col]. The feature names in
    the statement are stored in the feature_names array.
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在双特征训练集`(X, y)`上训练嵌套的`if`/`else`语句，并返回相应的准确度。该语句通过在`X[:,split_col]`中的特征上拆分来训练。声明中的特征名称存储在`feature_names`数组中。
- en: ❷ The feature names in the statement are stored in the feature_names array.
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 声明中的特征名称存储在`feature_names`数组中。
- en: ❸ The name of the feature located in the inner portion of the nested statement
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 嵌套语句内部位置的特征名称
- en: ❹ Trains two simple models
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 训练两个简单模型
- en: ❺ Combines the simple models
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 合并简单模型
- en: Our function trained a model that’s 100% accurate. Given our current training
    set, that accuracy should hold even if we split on `switch1` instead of `switch0`.
    Let’s verify.
  id: totrans-135
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的功能训练了一个100%准确的模型。鉴于我们当前的训练集，即使我们在`switch1`而不是`switch0`上拆分，该准确度也应保持不变。让我们验证一下。
- en: Listing 22.23 Splitting on `switch1` instead of `switch0`
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.23：在`switch1`而不是`switch0`上拆分
- en: '[PRE22]'
  id: totrans-137
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: Splitting on either feature yields the same result. This is true of our two-switch
    system, but it is not the case for many real-world training sets. It’s common
    for one split to outperform another. In the next subsection, we explore how to
    prioritize the features during splitting.
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 分割任一特征都会得到相同的结果。这适用于我们的双开关系统，但对于许多现实世界的训练集来说并非如此。通常一个分割会优于另一个分割。在下一个小节中，我们将探讨如何在分割时优先考虑特征。
- en: 22.1.2 Deciding which feature to split on
  id: totrans-139
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.1.2 决定分割哪个特征
- en: 'Suppose we wish to train an `if`/`else` model that predicts whether it’s raining
    outside. The model returns `1` if it is raining and `0` otherwise. The model relies
    on the following two features:'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们希望训练一个`if`/`else`模型来预测外面是否在下雨。如果正在下雨，模型返回`1`，否则返回`0`。该模型依赖于以下两个特征：
- en: Is the current season autumn? Yes or No?
  id: totrans-141
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 当前季节是秋季吗？是或否？
- en: We’ll assume that fall/autumn is the local rainy season and that the feature
    predicts rain 60% of the time.
  id: totrans-142
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们假设秋季是当地的雨季，并且该特征预测降雨的概率为60%。
- en: Is it currently wet outside? Yes or No?
  id: totrans-143
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 目前外面是否潮湿？是或否？
- en: Usually it’s raining when it’s wet. Occasionally, wetness is caused by a sprinkler
    system on a sunny day; and conditions might appear dry during a drizzly morning
    in the forest, if the trees impede the raindrops. We’ll assume this feature predicts
    rain 95% of the time.
  id: totrans-144
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 通常情况下，潮湿时就会下雨。偶尔，晴天时由喷水系统引起的潮湿；在森林中，如果树木阻挡了雨滴，细雨的早晨可能会出现干燥的条件。我们假设这个特征预测降雨的概率为95%。
- en: Let’s simulate the features and class labels through random sampling. We sample
    across 100 weather observations and store the outputs in training set `(X_rain,
    y_rain)`.
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过随机抽样模拟特征和类别标签。我们在100次天气观测中抽样，并将输出存储在训练集`(X_rain, y_rain)`中。
- en: Listing 22.24 Simulating a rainy-day training set
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.24 模拟雨天训练集
- en: '[PRE23]'
  id: totrans-147
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ It rains 60% of the time.
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 60%的时间会下雨。
- en: ❷ 95% of the time, the state of wetness equals the state of rain.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 95%的时间，潮湿状态等于降雨状态。
- en: ❸ 60% of the time, the state of autumn equals the state of rain.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 60%的时间，秋季状态等于降雨状态。
- en: Now, let’s train a model by splitting on the autumn feature.
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们通过分割秋季特征来训练一个模型。
- en: Listing 22.25 Training a model with an autumn split
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.25 使用秋季分割训练模型
- en: '[PRE24]'
  id: totrans-153
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: We trained a nested model that is 95% accurate. What if we split on the wetness
    feature instead?
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练了一个准确率达到95%的嵌套模型。如果我们根据湿度特征来分割会怎样呢？
- en: Listing 22.26 Training a model with a wetness split
  id: totrans-155
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.26 使用湿度分割训练模型
- en: '[PRE25]'
  id: totrans-156
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: 'Splitting on wetness yields a simpler (and hence better) model while retaining
    the previously seen accuracy. Not all splits are equal: splitting on some features
    leads to preferable results. How should we select the best feature for the split?
    Naively, we could iterate over all features in `X`, train models by splitting
    on each feature, and return the simplest model with the highest accuracy. This
    brute-force approach works when `X.size[1] == 2`, but it won’t scale as the feature
    count increases. Our goal is to develop a technique that can scale to thousands
    of features, so we need an alternate approach.'
  id: totrans-157
  prefs: []
  type: TYPE_NORMAL
  zh: 根据湿度特征分割可以得到一个更简单（因此更好）的模型，同时保持之前看到的准确率。并非所有分割都是平等的：根据某些特征分割会导致更好的结果。我们应该如何选择最佳的分割特征？直观地，我们可以遍历`X`中的所有特征，通过每个特征分割来训练模型，并返回最简单且准确率最高的模型。当`X.size[1]
    == 2`时，这种暴力方法有效，但随着特征数量的增加，它将无法扩展。我们的目标是开发一种可以扩展到数千个特征的技巧，因此我们需要一个替代方法。
- en: 'One solution requires us to examine the distribution of classes in the training
    set. Currently, our `y_rain` array contains two binary classes: `0` and `1`. The
    label `1` corresponds to a rainy observation. Therefore, the array sum equals
    the number of rainy observations. Meanwhile, the array size equals the total number
    of observations, so `y_rain.sum() / y_rain.size` equals the overall probability
    of rain. Let’s print that probability.'
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案要求我们检查训练集中类别的分布。目前，我们的`y_rain`数组包含两个二进制类别：`0`和`1`。标签`1`对应于降雨观测。因此，数组的和等于降雨观测的数量。同时，数组的大小等于观测总数，所以`y_rain.sum()
    / y_rain.size`等于降雨的整体概率。让我们打印出这个概率。
- en: Listing 22.27 Computing the probability of rain
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.27 计算降雨概率
- en: '[PRE26]'
  id: totrans-160
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: It rains in 61% of total observations. How is that probability altered when
    we split on autumn? Well, the split returns two training sets with two class-label
    arrays. We’ll call these arrays `y_fall_a` and `y_fall_b`. Dividing `y_fall_b.sum()`
    by the array size returns the likelihood of rain during autumn. Let’s print that
    likelihood and the probability of rain during the other seasons.
  id: totrans-161
  prefs: []
  type: TYPE_NORMAL
  zh: 在所有观测中，有 61% 降雨。当我们根据秋天分割时，这个概率是如何改变的？嗯，分割返回两个训练集和两个类别标签数组。我们将这些数组称为 `y_fall_a`
    和 `y_fall_b`。将 `y_fall_b.sum()` 除以数组大小返回秋天降雨的可能性。让我们打印出这个可能性和其他季节降雨的可能性。
- en: Listing 22.28 Computing the probability of rain based on the season
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.28 基于季节计算降雨概率
- en: '[PRE27]'
  id: totrans-163
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: As expected, we’re more likely to see rain during the autumn season, but that
    likelihood difference is not very large. It rains 66% of the time during autumn,
    and 55% of the time during the other seasons. It’s worth noting that these two
    probabilities are close to our overall rain likelihood of 61%. If we know that
    it is autumn, we are slightly more confident about rain. Still, our confidence
    increase is not very substantial relative to the original training set, so the
    split on autumn is not very informative. What if instead we split on wetness?
    Let’s check if that split shifts the observed probabilities.
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，我们更有可能在秋天看到降雨，但这种可能性差异并不很大。在秋天，有 66% 的时间会下雨，而在其他季节有 55% 的时间会下雨。值得注意的是，这两个概率接近我们整体的降雨可能性
    61%。如果我们知道现在是秋天，那么我们对降雨的信心会稍微增加。然而，与原始训练集相比，我们的信心增加并不非常显著，所以根据秋天分割并不是很有信息量。如果我们根据潮湿来分割呢？让我们检查这个分割是否改变了观察到的概率。
- en: Listing 22.29 Computing the probability of rain based on wetness
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.29 基于潮湿计算降雨概率
- en: '[PRE28]'
  id: totrans-166
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: If we know it’s wet outside, then we have nearly perfect confidence in rain.
    Whenever it is dry, the chance of rain remains at 10%. This percentage is low
    but very significant to our classifier. We know that dry conditions predict a
    lack of rain with 90% accuracy.
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: 如果我们知道外面是湿的，那么我们对降雨的信心几乎是完美的。每当它是干燥的时候，降雨的可能性仍然是 10%。这个百分比很低，但对我们的分类器来说非常显著。我们知道干燥条件可以以
    90% 的准确率预测没有降雨。
- en: Intuitively, wetness is a more informative feature than autumn. How should we
    quantify our intuition? Well, the wetness split returns two class-label arrays
    whose corresponding rain probabilities are either very low or very high. These
    extreme probabilities are an indication of class imbalance. As we learned in section
    20, an imbalanced dataset has far more of some Class A relative to another Class
    B. This makes it easier for a model to isolate Class A in the data. By comparison,
    the autumn split returns two arrays whose likelihoods are in the moderate range
    of 55 to 66%. The classes in `y_fall_a` and `y_fall_b` are more balanced. Thus,
    it’s not as easy to tell the rain/not-rain classes apart.
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 直觉上，潮湿是一个比秋天更有信息量的特征。我们该如何量化我们的直觉呢？嗯，潮湿分割返回两个类别标签数组，其对应的降雨概率要么非常低，要么非常高。这些极端概率是类别不平衡的指示。正如我们在第
    20 节中学到的，不平衡的数据集中，某些类别 A 相对于另一个类别 B 的数量要多得多。这使得模型更容易在数据中隔离类别 A。相比之下，秋天分割返回两个数组，其可能性在
    55% 到 66% 的适中范围内。`y_fall_a` 和 `y_fall_b` 中的类别更加平衡。因此，区分降雨/非降雨类别并不那么容易。
- en: When choosing between two splits, we should select the split that yields more
    imbalanced class labels. Let’s figure out how to quantify class imbalance. Generally,
    imbalance is associated with the shape of the class probability distribution.
    We can treat this distribution as a vector `v`, where `v[i]` equals the probability
    of observing Class *i*. A higher value of `v.max()` indicates a greater class
    imbalance. In our two-class dataset, we can compute `v` as `[1 - prob_rain, prob_rain]`,
    where `prob_rain` is the probability of rain. This two-element vector can be visualized
    as a line segment in 2D space, per our discussion in section 12 (figure 22.2).
  id: totrans-169
  prefs: []
  type: TYPE_NORMAL
  zh: 在选择两个分割点之间时，我们应该选择产生更多不平衡类别标签的分割点。让我们弄清楚如何量化类别不平衡。通常，不平衡与类别概率分布的形状有关。我们可以将这个分布视为一个向量
    `v`，其中 `v[i]` 等于观察类别 *i* 的概率。`v.max()` 的值越高，表示类别不平衡越严重。在我们的二类数据集中，我们可以将 `v` 计算为
    `[1 - prob_rain, prob_rain]`，其中 `prob_rain` 是降雨的概率。这个两元素向量可以像我们在第 12 节（图 22.2）中讨论的那样，在二维空间中可视化为一条线段。
- en: '![](../Images/22-02.png)'
  id: totrans-170
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22-02.png)'
- en: Figure 22.2 The probability distribution over class labels in `y_fall_a` (is
    autumn) visualized as a 2D line segment. The y-axis represents the probability
    of rain (`0.66`), and the x-axis represents the probability of no rain (`1` `-`
    `0.66` `=` `0.36`).
  id: totrans-171
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.2 `y_fall_a`（是否为秋季）的概率分布以2D线段可视化。y轴表示降雨的概率（`0.66`），x轴表示无降雨的概率（`1` `-` `0.66`
    `=` `0.36`）。
- en: 'Such visualizations can prove insightful. We’ll now do the following:'
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的可视化可以提供有价值的见解。我们现在将执行以下操作：
- en: Compute the class distribution vectors for the autumn split using arrays `y_fall_a`
    and `y_fall_b`.
  id: totrans-173
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数组`y_fall_a`和`y_fall_b`计算秋季分割的类别分布向量。
- en: Compute the class distribution vectors for the wetness split using arrays `y_wet_a`
    and `y_wet_b`.
  id: totrans-174
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用数组`y_wet_a`和`y_wet_b`计算湿度分割的类别分布向量。
- en: Visualize all four arrays as line segments in 2D space.
  id: totrans-175
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将所有四个数组作为2D空间中的线段可视化。
- en: That visualization will reveal how to effectively measure class imbalance (figure
    22.3).
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: 该可视化将揭示如何有效地测量类别不平衡（图22.3）。
- en: Listing 22.30 Plotting the class distribution vectors
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.30 绘制类别分布向量
- en: '[PRE29]'
  id: totrans-178
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Returns the probability distribution across class labels in a binary, two-class
    system. The distribution can be treated as a 2D vector.
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回二元、两类系统中跨越类别标签的概率分布。该分布可以被视为2D向量。
- en: ❷ Plots a 2D vector v as a line segment that stretches from the origin to v
  id: totrans-180
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将2D向量v绘制为从原点到v的线段
- en: ❸ Iterates over the four unique distribution vectors that result from every
    possible split and then plots these four vectors
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历由每个可能的分割产生的四个独特的分布向量，然后绘制这四个向量
- en: '![](../Images/22-03.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22-03.png)'
- en: Figure 22.3 A plot of the four vector distributions across each feature split.
    The wetness vectors are much more imbalanced and thus are positioned closer to
    the axes. More importantly, the wetness vectors appear longer than the autumn
    vectors.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.3 每个特征分割的四个向量分布的图。湿度向量不平衡得更多，因此更靠近轴。更重要的是，湿度向量看起来比秋季向量更长。
- en: 'In our plot, the two imbalanced wetness vectors skew heavily toward the x-
    and y-axes. Meanwhile, the two balanced autumn vectors are approximately equidistant
    from both axes. However, what really stands out is not vector direction but vector
    length: the balanced autumn vectors are much shorter than the vectors associated
    with wetness. This isn’t a coincidence. Imbalanced distributions are proven to
    have greater vector magnitudes. Also, as we showed in section 13, the magnitude
    is equal to the square root of `v @ v`. Hence, the dot product of a distribution
    vector with itself is greater if that vector is more imbalanced!'
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的图中，两个不平衡的湿度向量在x轴和y轴上严重倾斜。同时，两个平衡的秋季向量与两个轴的距离大致相等。然而，真正引人注目的是向量长度，而不是向量方向：平衡的秋季向量比与湿度相关的向量短得多。这不是巧合。不平衡的分布已被证明具有更大的向量幅度。此外，正如我们在第13节中所示，幅度等于`v
    @ v`的平方根。因此，如果一个向量越不平衡，那么与自身点积的分布向量就越大！
- en: Let’s demonstrate this property for every 2D vector `v = [1 - p, p]`, where
    `p` is the probability of rain. Listing 22.31 plots the magnitude of `v` across
    rain likelihoods ranging from 0 to 1\. We also plot the square of the magnitude,
    which is equal to `v @ v`. The plotted values should be maximized when `p` is
    very low or very high and minimized when `v` is perfectly balanced at `p = 0.5`
    (figure 22.4).
  id: totrans-185
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们为每个2D向量 `v = [1 - p, p]`（其中`p`是降雨的概率）演示这个属性。列表22.31绘制了`v`在从0到1的降雨可能性范围内的幅度。我们还绘制了幅度的平方，它等于`v
    @ v`。当`p`非常低或非常高时，绘制的值应该最大化；当`v`在`p = 0.5`时完全平衡时，值应该最小化（图22.4）。
- en: '![](../Images/22-04.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22-04.png)'
- en: Figure 22.4 A plot of distribution vector magnitudes and squared magnitudes
    across each distribution vector `[1` `-` `p,` `p]`. The plotted values are minimized
    when the vector is perfectly balanced at `p` `=` `0.5`.
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.4 每个分布向量 `[1` `-` `p,` `p]` 的分布向量幅度和平方幅度的图。当向量在`p` `=` `0.5`时完全平衡时，绘制的值最小化。
- en: Listing 22.31 Plotting the distribution vector magnitudes
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.31 绘制分布向量幅度
- en: '[PRE30]'
  id: totrans-189
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ The probability of rain ranges from 0 to 1.0 (inclusive).
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 降雨概率的范围从0到1.0（包含）。
- en: ❷ Vectors represent all possible two-class distributions, where the classes
    are rain and not rain.
  id: totrans-191
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 向量代表所有可能的两类分布，其中类别是降雨和非降雨。
- en: ❸ Vector magnitudes computed using NumPy
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用NumPy计算向量幅度
- en: ❹ Computes the squares of vector magnitudes as a simple dot product
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算向量幅度的平方作为简单的点积
- en: 'The squared magnitude is maximized at 1.0 when `v` is fully imbalanced at `p
    = 0.0` and `p = 1.0`. It’s also minimized at 0.5 when `v` is balanced. Thus, `v
    @ v` serves as an excellent metric for class-label imbalance, but data scientists
    prefer the slightly different metric of `1 - v @ v`. This metric, called the *Gini
    impurity*, essentially flips the plotted curve: it is minimized at 0 and maximized
    at 0.5\. Let’s confirm by plotting the Gini impurity across all values of `p`
    (figure 22.5).'
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: 当 `v` 在 `p = 0.0` 和 `p = 1.0` 时完全失衡时，平方范数在 1.0 处达到最大。当 `v` 平衡时，它也最小化到 0.5。因此，`v
    @ v` 作为类别不平衡的优秀指标，但数据科学家更喜欢 `1 - v @ v` 这个略有不同的指标。这个指标被称为 *基尼不纯度*，它实际上翻转了绘制的曲线：它在
    0 处最小化，在 0.5 处最大化。让我们通过绘制 `p` 的所有值上的基尼不纯度来确认（图 22.5）。
- en: '![](../Images/22-05.png)'
  id: totrans-195
  prefs: []
  type: TYPE_IMG
  zh: '![](../Images/22-05.png)'
- en: Figure 22.5 A plot of Gini impurities across each distribution vector `[1` `-`
    `p,` `p]`. The Gini impurity is maximized when the vector is perfectly balanced
    at `p` `=` `0.5`.
  id: totrans-196
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.5 每个分布向量 `[1` `-` `p,` `p]` 的基尼不纯度图。当向量在 `p` `=` `0.5` 时完全平衡时，基尼不纯度达到最大。
- en: Note The Gini impurity has a concrete interpretation in probability theory.
    Suppose that for any data point, we randomly assign a class of `i` with probability
    `v[i]`, where `v` is the vectorized distribution. The probability of choosing
    a point belonging to Class *i* is also equal to `v[i]`. Hence, the probability
    of choosing a point belonging to Class *i* and correctly labeling that point is
    equal to `v[i] * v[i]`. It follows that the probability of correctly labeling
    any point is equal to `sum(v[i] * v[i] for i in range(len(v))`. This simplifies
    to `v @ v`. Thus, `1 - v @ v` equals the probability of incorrectly labeling our
    data. The Gini impurity is equal to the probability of error, which decreases
    as the data grows more imbalanced.
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：基尼不纯度在概率论中有一个具体的解释。假设对于任何数据点，我们以概率 `v[i]` 随机分配一个类别 `i`，其中 `v` 是向量化的分布。选择属于类别
    *i* 的点的概率也等于 `v[i]`。因此，选择属于类别 *i* 并正确标记该点的概率等于 `v[i] * v[i]`。由此可知，正确标记任何点的概率等于
    `sum(v[i] * v[i] for i in range(len(v)))`。这简化为 `v @ v`。因此，`1 - v @ v` 等于我们数据被错误标记的概率。基尼不纯度等于错误概率，随着数据的失衡程度增加而降低。
- en: Listing 22.32 Plotting the Gini impurity
  id: totrans-198
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.32 绘制基尼不纯度
- en: '[PRE31]'
  id: totrans-199
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: 'Gini impurity is a standard measure of class imbalance. Highly imbalanced datasets
    are considered more “pure” since labels lean heavily toward one class over any
    other. When training a nested model, we should split on the feature that minimizes
    overall impurity. For any split with class labels `y_a` and `y_b`, we can compute
    the impurity like this:'
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 基尼不纯度是类别不平衡的标准度量。高度不平衡的数据集被认为是更“纯净”的，因为标签在所有其他类别中偏向一个类别。在训练嵌套模型时，我们应该在最小化整体不纯度的特征上进行拆分。对于任何具有类别标签
    `y_a` 和 `y_b` 的拆分，我们可以像这样计算不纯度：
- en: Compute the impurity of `y_a`.
  id: totrans-201
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 计算标签 `y_a` 的不纯度。
- en: It’s equal to `1 - v_a @ v_a`, where `v_a` is the class distribution over `y_a`.
  id: totrans-202
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它等于 `1 - v_a @ v_a`，其中 `v_a` 是 `y_a` 上的类别分布。
- en: Next, we compute the impurity of `y_b`.
  id: totrans-203
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 接下来，我们计算 `y_b` 的不纯度。
- en: It’s equal to `1 - v_b @ v_b`, where `v_b` is the class distribution over `y_b`.
  id: totrans-204
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 它等于 `1 - v_b @ v_b`，其中 `v_b` 是 `y_b` 上的类别分布。
- en: Finally, we take the weighted mean of the two impurities.
  id: totrans-205
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 最后，我们取两个不纯度的加权平均值。
- en: The weights will equal `y_a.size` and `y_b.size`, just like in our total accuracy
    computation.
  id: totrans-206
  prefs:
  - PREF_IND
  - PREF_UL
  type: TYPE_NORMAL
  zh: 权重将与 `y_a.size` 和 `y_b.size` 相等，就像在我们的总准确度计算中一样。
- en: Let’s compute the impurities associated with autumn and wetness.
  id: totrans-207
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们计算与秋季和潮湿相关的关联不纯度。
- en: Listing 22.33 Computing each feature’s Gini impurity
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.33 计算每个特征的基尼不纯度
- en: '[PRE32]'
  id: totrans-209
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: ❶ Returns the weighted Gini impurity associated with class labels stored in
    arrays y_a and y_b
  id: totrans-210
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 返回与存储在数组 `y_a` 和 `y_b` 中的类别标签相关的加权基尼不纯度
- en: ❷ The class distribution vector of y_a
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ `y_a` 的类别分布向量
- en: ❸ The class distribution vector of y_b
  id: totrans-212
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ `y_b` 的类别分布向量
- en: ❹ The Gini impurities across the two class distributions
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 两个类别分布的基尼不纯度
- en: ❺ Returns the weighted mean of the two Gini impurities
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 返回两个基尼不纯度的加权平均值
- en: As expected, the impurity is minimized when we split on wetness. This split
    leads to more imbalanced training data, which simplifies the training of the classifier.
    Going forward, we will split on features whose Gini impurity is minimized. With
    this in mind, let’s define a `sort_feature_indices` function. The function takes
    as input a training set `(X, y)` and returns a list of sorted feature indices
    based on the impurity associated with each feature split.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 如预期的那样，当我们根据湿度进行拆分时，杂质最小化。这种拆分导致训练数据不平衡，从而简化了分类器的训练。因此，我们将根据基尼杂质最小化的特征进行拆分。考虑到这一点，让我们定义一个
    `sort_feature_indices` 函数。该函数接受一个训练集 `(X, y)` 作为输入，并返回一个基于每个特征拆分关联的杂质排序的特征索引列表。
- en: Listing 22.34 Sorting features by Gini impurity
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.34 按基尼杂质排序特征
- en: '[PRE33]'
  id: totrans-217
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: ❶ Sorts the feature indices in X by their associated Gini impurities, from smallest
    to largest
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 按关联的基尼杂质从小到大对 X 中的特征索引进行排序
- en: ❷ Splits on the feature in column i and computes the impurity of the split
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在第 i 列的特征上进行拆分，并计算拆分的杂质
- en: ❸ Returns the sorted column indices of X. The first column corresponds to the
    smallest impurity.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 返回 X 的排序列索引。第一列对应最小的杂质。
- en: The `sort_feature_indices` function will prove invaluable as we train nested
    `if`/`else` models with more than two features.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: '`sort_feature_indices` 函数在我们训练具有两个以上特征的嵌套 `if`/`else` 模型时将非常有价值。'
- en: 22.1.3 Training if/else models with more than two features
  id: totrans-222
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.1.3 使用两个以上特征训练 if/else 模型
- en: 'Training a model to predict the current weather is a relatively trivial task.
    We’ll now train a more complicated model that predicts whether it will rain tomorrow.
    The model relies on the following three features:'
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 训练一个模型来预测当前天气是一个相对简单的工作。现在我们将训练一个更复杂的模型，该模型预测明天是否会下雨。该模型依赖于以下三个特征：
- en: Has it rained at any point today?
  id: totrans-224
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天有没有下雨？
- en: If it rained today, then it is very likely to rain tomorrow.
  id: totrans-225
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 如果今天下雨，那么明天很可能也会下雨。
- en: Is today a cloudy day? Yes or No?
  id: totrans-226
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天是多云的一天吗？是或否？
- en: It’s more likely to rain on a cloudy day. This increases the chance that it
    will rain tomorrow.
  id: totrans-227
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 在多云的日子里下雨的可能性更大。这增加了明天会下雨的概率。
- en: Is today an autumn day? Yes or No?
  id: totrans-228
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天是秋季的一天吗？是或否？
- en: We assume that it’s both rainier and cloudier in autumn.
  id: totrans-229
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 我们假设秋季降雨和多云的概率更高。
- en: 'We further assume a complex but realistic interrelationship between the three
    features, to make the problem much more interesting:'
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 我们进一步假设三个特征之间存在复杂但现实的相互关系，从而使问题变得更加有趣：
- en: There’s a 25% chance that today is autumn.
  id: totrans-231
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 今天是秋季的概率是 25%。
- en: During autumn, it is cloudy 70% of the time. Otherwise, it’s cloudy 30% of the
    time.
  id: totrans-232
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 秋季，70% 的时间是多云的。否则，30% 的时间是多云的。
- en: If today is cloudy, there’s a 40% of rain at some point in the day. Otherwise,
    the chance of rain is 5%.
  id: totrans-233
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果今天是多云的，那么在一天中的某个时刻下雨的概率是 40%。否则，下雨的概率是 5%。
- en: If it rains today, there’s a 50% chance it will rain tomorrow.
  id: totrans-234
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果今天下雨，那么明天下雨的概率是 50%。
- en: If today is a dry and sunny autumn day, there’s a 15% chance it will rain tomorrow.
    Otherwise, on dry and sunny spring, summer, and winter days, the chance of rain
    tomorrow falls to 5%.
  id: totrans-235
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果今天是干燥且晴朗的秋季，那么明天会下雨的概率是 15%。否则，在干燥且晴朗的春季、夏季和冬季，明天会下雨的概率下降到 5%。
- en: The following code simulates a training set `(X_rain, y_rain)` based on the
    probabilistic relationships between the features.
  id: totrans-236
  prefs: []
  type: TYPE_NORMAL
  zh: 以下代码基于特征之间的概率关系模拟了一个训练集 `(X_rain, y_rain)`。
- en: Listing 22.35 Simulating a three-feature training set
  id: totrans-237
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.35 模拟三个特征的训练集
- en: '[PRE34]'
  id: totrans-238
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: ❶ Simulates the features for today, as well as the weather for tomorrow
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 模拟今天的特征以及明天的天气
- en: ❷ 25% of the time, it is autumn.
  id: totrans-240
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 25% 的时间是秋季。
- en: ❸ During autumn, it’s cloudy 70% of the time. Otherwise, it’s cloudy 30% of
    the time.
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 秋季时，70% 的时间是多云的。否则，30% 的时间是多云的。
- en: ❹ If it rained today, there’s a 50% chance that it will also rain tomorrow.
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 如果今天下雨，那么明天也下雨的概率是 50%。
- en: ❺ There’s a 40% chance of rain on a cloudy day. Otherwise, the chance of rain
    falls to 5%.
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在多云的日子里，有 40% 的可能性会下雨。否则，下雨的概率下降到 5%。
- en: ❻ Simulates the lower likelihood of rain after a dry day
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 模拟干旱天气后降雨可能性降低
- en: ❼ Returns the simulated features and whether it will rain tomorrow
  id: totrans-245
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 返回模拟的特征以及明天是否会下雨
- en: ❽ Simulates a dataset with 1,000 training examples
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 模拟包含 1,000 个训练示例的数据集
- en: The columns in `X_rain` correspond to features `'is_fall'`, `'is_cloudy'`, and
    `'rained_ today'`. We can sort these features by Gini impurity to measure how
    well they split the data.
  id: totrans-247
  prefs: []
  type: TYPE_NORMAL
  zh: '`X_rain` 中的列对应于特征 `''is_fall''`、`''is_cloudy''` 和 `''rained_today''`。我们可以按基尼杂质对这些特征进行排序，以衡量它们分割数据的好坏。'
- en: Listing 22.36 Sorting three features by Gini impurity
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.36 按基尼不纯度对三个特征进行排序
- en: '[PRE35]'
  id: totrans-249
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: 'Splitting on the autumn feature yields the lowest Gini impurity, and cloudiness
    ranks second. The rainy feature has the highest Gini impurity: it yields the most
    balanced datasets and is not a good candidate for splitting.'
  id: totrans-250
  prefs: []
  type: TYPE_NORMAL
  zh: 在秋季特征上进行分割产生最低的基尼不纯度，多云特征排名第二。雨天特征具有最高的基尼不纯度：它产生最平衡的数据集，不是一个好的分割候选。
- en: Note The high Gini impurity of the rainy feature may seem surprising. After
    all, if it rained today, we know it’s much more likely to rain tomorrow. Hence,
    when `X_rain[:,0] == 1`, the Gini impurity is low. But on a dry day, we have little
    indication of tomorrow’s weather; so when `X_rain[:,0] == 0`, the Gini impurity
    is high. There are more dry days than there are rainy days during the year, so
    the mean Gini impurity is high. In contrast, the autumn feature is much more informative.
    It provides us with insight into tomorrow’s weather on both autumn and non-autumn
    days.
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：雨天特征的基尼不纯度可能看起来有些意外。毕竟，如果今天下雨，我们知道明天下雨的可能性要大得多。因此，当 `X_rain[:,0] == 1` 时，基尼不纯度低。但在干燥的日子里，我们对明天的天气几乎没有线索；因此，当
    `X_rain[:,0] == 0` 时，基尼不纯度高。在一年中，干燥的日子比雨天多，所以平均基尼不纯度高。相比之下，秋季特征提供了更多的信息。它让我们在秋季和非秋季的日子里都能了解明天的天气。
- en: Given our ranked feature list, how should we train our model? After all, `trained_
    nested_if_else` is intended to process two features, not three. One intuitive
    solution is to train the model on just the two top-ranked features. These features
    lead to a greater training set imbalance, making it easier to decipher between
    rainy and non-rainy class labels.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 给定我们的特征排名列表，我们应该如何训练我们的模型？毕竟，`trained_nested_if_else` 的目的是处理两个特征，而不是三个。一个直观的解决方案是在仅包含两个最高排名特征的模型上训练。这些特征导致训练集不平衡性更大，这使得区分雨天和非雨天类别标签更容易。
- en: Here, we train a two-feature model on just the autumn and cloudiness features.
    We also set the split column to autumn since autumn has the lowest Gini impurity.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们仅在秋季和多云特征上训练一个双特征模型。我们还设置了分割列为秋季，因为秋季具有最低的基尼不纯度。
- en: Listing 22.37 Training a model on the two best features
  id: totrans-254
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.37 在两个最佳特征上训练模型
- en: '[PRE36]'
  id: totrans-255
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: ❶ Ignores the final feature, which has the worst Gini impurity
  id: totrans-256
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 忽略最后一个特征，因为它具有最差的基尼不纯度
- en: ❷ The feature subset with the two best features
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 具有两个最佳特征的特征子集
- en: ❸ Adjusts the column of the best split feature relative to the filtered index
    of the worst, deleted feature
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 调整最佳分割特征列相对于最差、已删除特征过滤索引的位置
- en: ❹ Trains a nested model on the two best features
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在两个最佳特征上训练嵌套模型
- en: 'Our trained model is a frivolously simple. It always predicts no rain, no matter
    what! This trivial model is only 74% accurate—that accuracy is not disastrous,
    but we can definitely do better. Ignoring the rainy feature has limited our predictive
    capacity. We must incorporate all three features to raise the accuracy score.
    We can incorporate all three features like this:'
  id: totrans-260
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的模型非常简单。它总是预测没有雨，无论什么情况！这个简单的模型只有74%的准确率——这个准确率不是灾难性的，但我们肯定可以做得更好。忽略雨天特征限制了我们的预测能力。我们必须结合所有三个特征来提高准确率。我们可以这样结合所有三个特征：
- en: Split on the feature with the lowest Gini impurity. This, of course, is autumn.
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在具有最低基尼不纯度的特征上进行分割。当然，这是秋季。
- en: Train two nested models using the `train_nested_if_else` function. Model A will
    consider only those scenarios in which the season is not autumn, and Model B will
    consider all remaining scenarios in which the season is autumn.
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `train_nested_if_else` 函数训练两个嵌套模型。模型A将考虑那些季节不是秋季的场景，而模型B将考虑所有剩余的季节是秋季的场景。
- en: Combine Model A and Model B into a single coherent classifier.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将模型A和模型B合并成一个单一的连贯分类器。
- en: Note These steps are nearly identical to the logic behind the `nested_if_ else`
    function. The main difference is that now we’re expanding that logic to more than
    two features.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：这些步骤几乎与 `nested_if_else` 函数背后的逻辑相同。主要区别在于现在我们将这个逻辑扩展到超过两个特征。
- en: Let’s start by splitting on the autumn feature, whose index is stored in `indices[0]`.
  id: totrans-265
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从分割秋季特征开始，其索引存储在 `indices[0]`。
- en: Listing 22.38 Splitting on the feature with the lowest impurity
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.38 在具有最低不纯度的特征上进行分割
- en: '[PRE37]'
  id: totrans-267
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: Next, let’s train a nested model on `(X_a, y_a)`. This training set contains
    all of our non-autumn observations.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们在 `(X_a, y_a)` 上训练一个嵌套模型。这个训练集包含我们所有的非秋季观测数据。
- en: Listing 22.39 Training a model when the season is not autumn
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.39 在季节不是秋季时训练模型
- en: '[PRE38]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: ❶ Splits on the feature in X_a that yields the best (lowest) Gini impurity
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 X_a 中拆分以产生最佳（最低）Gini 不纯度的特征
- en: ❷ Trains a nested two-feature model on (X_a, y_a)
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 (X_a, y_a) 上训练嵌套的两个特征模型
- en: Our trained `model_a` is highly accurate. Now we will train a second `model_b`
    based on the autumn observations stored in `(X_b, y_b)`.
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练的 `model_a` 准确性很高。现在我们将基于存储在 `(X_b, y_b)` 中的秋季观测结果训练第二个 `model_b`。
- en: Listing 22.40 Training a model when the season is autumn
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.40 在秋季训练模型
- en: '[PRE39]'
  id: totrans-275
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: ❶ Splits on the feature in X_b that yields the best (lowest) Gini impurity
  id: totrans-276
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 X_b 中拆分以产生最佳（最低）Gini 不纯度的特征
- en: ❷ Trains a nested two-feature model on (X_b, y_b)
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在 (X_b, y_b) 上训练嵌套的两个特征模型
- en: When it is autumn, `model_b` performs with 79% accuracy. Otherwise, `model_a`
    performs with 88% accuracy. Let’s combine these models into a single nested statement.
    We use our `combine_if_else` function, which we previously defined for this exact
    purpose. We also compute the total accuracy, which equals the weighted mean of
    `accuracy_a` and `accuracy_b`.
  id: totrans-278
  prefs: []
  type: TYPE_NORMAL
  zh: 当是秋天时，`model_b` 的准确率为 79%。否则，`model_a` 的准确率为 88%。让我们将这些模型组合成一个单一的嵌套语句。我们使用之前为这个目的定义的
    `combine_if_else` 函数。我们还计算了总准确率，它等于 `accuracy_a` 和 `accuracy_b` 的加权平均值。
- en: Listing 22.41 Combining the models into a nested statement
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.41 将模型组合成嵌套语句
- en: '[PRE40]'
  id: totrans-280
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: 'We were able to generate a nested 3-feature model. The process was very similar
    to how we trained a nested 2-feature model. In this manner, we can extend our
    logic to train a 4-feature model, or a 10-feature model, or a 100-feature model.
    In fact, our logic can generalize to training any nested *N*-feature model. Suppose
    we’re given a training set `(X, y)`, where `X` contains *N* columns. We should
    be able to easily train a model by executing the following steps:'
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 我们能够生成一个嵌套的 3 个特征模型。这个过程与我们训练嵌套的 2 个特征模型非常相似。以这种方式，我们可以扩展我们的逻辑来训练一个 4 个特征模型，或者一个
    10 个特征模型，或者一个 100 个特征模型。事实上，我们的逻辑可以推广到训练任何嵌套 *N*-特征模型。假设我们被给定了包含 `X` 的训练集 `(X,
    y)`，其中 `X` 包含 *N* 列。我们应该能够通过执行以下步骤轻松地训练一个模型：
- en: If *N* equals `1`, return the simple, non-nested `train_if_else(X, y)` output.
    Otherwise, go to the next step.
  id: totrans-282
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果 *N* 等于 `1`，则返回简单的非嵌套 `train_if_else(X, y)` 输出。否则，进入下一步。
- en: Sort our *N* features based on the Gini impurity, from lowest to highest.
  id: totrans-283
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 根据Gini不纯度从低到高对 *N* 个特征进行排序。
- en: Attempt to train a simpler, *N* – 1 feature model (using the top-ranked features
    from step 2). If that model performs with 100% accuracy, return it as our output.
    Otherwise, go to the next step.
  id: totrans-284
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 尝试训练一个更简单的、*N* – 1 个特征模型（使用步骤 2 中的顶级特征）。如果该模型以 100% 的准确率运行，则将其作为我们的输出。否则，进入下一步。
- en: Split on the feature with the smallest Gini impurity. That split returns two
    training sets `(X_a, y_a)` and `(X_b, y_b)`. Each training set contains *N* –
    1 features.
  id: totrans-285
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在具有最小 Gini 不纯度的特征上进行拆分。这个拆分返回两个训练集 `(X_a, y_a)` 和 `(X_b, y_b)`。每个训练集包含 *N* –
    1 个特征。
- en: 'Train two *N* – 1 feature models: `model_a` and `model_b`, using the training
    sets from the previous step. The corresponding accuracies equal `accuracy_a` and
    `accuracy_b`.'
  id: totrans-286
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用上一步中的训练集训练两个 *N* – 1 个特征模型：`model_a` 和 `model_b`，相应的准确率等于 `accuracy_a` 和 `accuracy_b`。
- en: Combine `model_a` and `model_b` into a nested `if`/`else` conditional model.
  id: totrans-287
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将 `model_a` 和 `model_b` 结合成一个嵌套的 `if`/`else` 条件模型。
- en: Compute the nested model’s accuracy using the weighted mean of `accuracy_a`
    and `accuracy_b`. The weights equal `y_a.size` and `y_b.size`.
  id: totrans-288
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用 `accuracy_a` 和 `accuracy_b` 的加权平均值计算嵌套模型的准确率。权重等于 `y_a.size` 和 `y_b.size`。
- en: Return the nested model if it outperforms the simpler model computed in step
    3\. Otherwise, return the simpler model.
  id: totrans-289
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 如果嵌套模型在步骤 3 中计算出的简单模型表现更好，则返回嵌套模型。否则，返回简单模型。
- en: Here, we define a recursive `train` function that carries out these steps.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们定义了一个递归的 `train` 函数，用于执行这些步骤。
- en: Listing 22.42 Training a nested model with *N* features
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.42 使用 *N* 个特征训练嵌套模型
- en: '[PRE41]'
  id: totrans-292
  prefs: []
  type: TYPE_PRE
  zh: '[PRE41]'
- en: ❶ Trains a nested if/else statement on the N feature training set (X, y) and
    returns the written statement along with the corresponding accuracy. The feature
    names in the statement are stored in the feature_names array.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在 N 个特征训练集 (X, y) 上训练嵌套的 if/else 语句，并返回相应的语句及其准确率。语句中的特征名称存储在 feature_names
    数组中。
- en: ❷ Sorts the feature indices by Gini impurity
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 按照Gini不纯度对特征索引进行排序
- en: ❸ Tries to train a simpler N – 1 feature model to see if it performs with 100%
    accuracy
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 尝试训练一个更简单的 N – 1 个特征模型，以查看其是否以 100% 的准确率运行
- en: ❹ Splits on the feature with the lowest Gini impurity
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在具有最低 Gini 不纯度的特征上进行拆分
- en: ❺ Trains two simpler N – 1 feature models on the two training sets returned
    after the split
  id: totrans-297
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 在分割后的两个训练集上训练两个更简单的 N – 1 特征模型
- en: ❻ Combines the simpler models
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 合并更简单的模型
- en: The branching `if`/`else` statements in our trained output resemble the branches
    of a tree. We can make the resemblance more explicit by visualizing the output
    as a *decision tree diagram*. Decision trees are special network structures used
    to symbolize `if`/`else` decisions. Features are nodes in the network, and conditions
    are edges. The `if` condition branches to the right of the feature nodes, and
    the `else` condition branches to the left. Figure 22.6 represents our rain-prediction
    model using a decision tree diagram.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 我们训练好的输出中的分支 `if`/`else` 语句类似于树的分支。我们可以通过将输出可视化为一个 *决策树图* 来使这种相似性更加明确。决策树是特殊的网络结构，用于表示
    `if`/`else` 决策。特征是网络中的节点，条件是边。`if` 条件从特征节点右侧分支，而 `else` 条件从左侧分支。图 22.6 使用决策树图表示我们的降雨预测模型。
- en: 'Any nested `if`/`else` statement can be visualized as a decision tree, so trained
    `if`/`else` conditional classifiers are referred to as *decision tree classifiers*.
    Trained decision tree classifiers have been in common use since the 1980s. Numerous
    strategies exist for training these classifiers effectively, all of which have
    the following properties in common:'
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 任何嵌套的 `if`/`else` 语句都可以被可视化为一个决策树，因此训练好的 `if`/`else` 条件分类器被称为 *决策树分类器*。自 1980
    年代以来，训练好的决策树分类器已被广泛使用。存在许多训练这些分类器的策略，它们都具有以下共同特性：
- en: The *N* feature training problem is simplified down to multiple *N* – 1 feature
    subproblems by splitting on one of the features.
  id: totrans-301
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*N* 个特征训练问题通过在一个特征上分割简化为多个 *N* – 1 个特征子问题。'
- en: The split is carried out by choosing the feature that yields the highest class
    imbalance. This is commonly done using the Gini impurity, although alternative
    metrics do exist.
  id: totrans-302
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分割是通过选择产生最高类别不平衡的特征来进行的。这通常使用基尼不纯度来完成，尽管也存在其他指标。
- en: Caution is taken to avoid needlessly complex `if`/`else` statements if simpler
    statements work equally well. This process is called *pruning*, since excessive
    `if`/`else` branches are pruned out.
  id: totrans-303
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 如果简单的语句同样有效，则应谨慎避免不必要的复杂 `if`/`else` 语句。这个过程被称为 *剪枝*，因为过多的 `if`/`else` 分支被剪除。
- en: '![](../Images/22-06.png)'
  id: totrans-304
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-06.png)'
- en: Figure 22.6 Visualizing the rain-prediction model using a decision tree diagram.
    The diagram is a network. The network nodes represent the model’s features, such
    as “Is Autumn?”. The edges represent the conditional `if`/`else` statements. For
    instance, if it’s autumn, the diagram’s Yes edge branches left; otherwise, the
    No edge branches right.
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.6 使用决策树图可视化降雨预测模型。该图是一个网络。网络节点代表模型的特征，例如“是否是秋季？”。边代表条件 `if`/`else` 语句。例如，如果是秋季，图中的
    Yes 边向左分支；否则，No 边向右分支。
- en: Scikit-learn includes a highly optimized decision tree implementation. We explore
    it in the next subsection.
  id: totrans-306
  prefs: []
  type: TYPE_NORMAL
  zh: Scikit-learn 包含一个高度优化的决策树实现。我们将在下一小节中探讨它。
- en: 22.2 Training decision tree classifiers using scikit-learn
  id: totrans-307
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.2 使用 scikit-learn 训练决策树分类器
- en: In scikit-learn, decision tree classification is carried out by the `DecisionTreeClassifier`
    class. Let’s import that class from `sklearn.tree`.
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: 在 scikit-learn 中，决策树分类是通过 `DecisionTreeClassifier` 类实现的。让我们从 `sklearn.tree`
    中导入这个类。
- en: Listing 22.43 Importing scikit-learn’s `DecisionTreeClassifier` class
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.43 导入 scikit-learn 的 `DecisionTreeClassifier` 类
- en: '[PRE42]'
  id: totrans-310
  prefs: []
  type: TYPE_PRE
  zh: '[PRE42]'
- en: Next, we initialize the class as `clf`. Then we train `clf` on the two-switch
    system introduced at the beginning of the section. That training set is stored
    in parameters `(X, y)`.
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们将类初始化为 `clf`。然后，我们在本节开头介绍的二开关系统上训练 `clf`。该训练集存储在参数 `(X, y)` 中。
- en: Listing 22.44 Initializing and training a decision tree classifier
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.44 初始化和训练决策树分类器
- en: '[PRE43]'
  id: totrans-313
  prefs: []
  type: TYPE_PRE
  zh: '[PRE43]'
- en: We can visualize the trained classifier using a decision tree diagram. Scikit-learn
    includes a `plot_tree` function, which uses Matplotlib to carry out that visualization.
    Calling `plot_tree(clf)` plots the trained decision tree diagram. The feature
    names and class names in that plot can be controlled using the `feature_names`
    and `class_names` parameters.
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用决策树图来可视化训练好的分类器。Scikit-learn 包含一个 `plot_tree` 函数，它使用 Matplotlib 来执行可视化。调用
    `plot_tree(clf)` 将绘制训练好的决策树图。在该图中，可以使用 `feature_names` 和 `class_names` 参数来控制特征名称和类名称。
- en: 'Let’s import `plot_tree` from `sklearn.tree` and visualize `clf` (figure 22.7).
    In the plot, the feature names equal Switch0 and Switch1, and the class labels
    equal the two bulb states: `Off` and `On`.'
  id: totrans-315
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们从`sklearn.tree`导入`plot_tree`并可视化`clf`（图22.7）。在图中，特征名称等于Switch0和Switch1，类别标签等于两个灯泡状态：`Off`和`On`。
- en: '![](../Images/22-07.png)'
  id: totrans-316
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-07.png)'
- en: Figure 22.7 A plot of the two-switch system’s decision tree diagram. Each top
    node contains a feature name along with additional statistics such as the Gini
    impurity and the dominant class. The bottom nodes contain the final predicted
    bulb classifications.
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.7 两个开关系统的决策树图。每个顶层节点包含一个特征名称以及额外的统计数据，如基尼不纯度和主导类别。底层节点包含最终的预测灯泡分类。
- en: Listing 22.45 Displaying a trained decision tree classifier
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.45 显示训练好的决策树分类器
- en: '[PRE44]'
  id: totrans-319
  prefs: []
  type: TYPE_PRE
  zh: '[PRE44]'
- en: 'The visualized diagram tracks the class distribution at each conditional position
    in the tree. It also tracks the associated Gini impurity, as well as the dominant
    class. Such visualizations can be useful, but these tree plots can get unwieldy
    when the total feature count is large. That is why scikit-learn provides an alternative
    visualization function: `export_text` allows us to display the tree using a simplified
    text-based diagram. Calling `export_text(clf)` returns a string. Printing that
    string reveals a tree composed of `|` and `-` characters. The feature names in
    that text tree can be specified with the `feature_names` parameter; but due to
    the limited nature of the output, we can’t print the class names. Let’s import
    `export_text` from `sklearn.tree` and then visualize our tree as a simple string.'
  id: totrans-320
  prefs: []
  type: TYPE_NORMAL
  zh: 可视化图跟踪树中每个条件位置上的类别分布。它还跟踪相关的基尼不纯度和主导类别。这样的可视化可能很有用，但当特征总数很大时，这些树图可能会变得难以处理。这就是为什么scikit-learn提供了一个替代的可视化函数：`export_text`允许我们使用简化的基于文本的图来显示树。调用`export_text(clf)`返回一个字符串。打印这个字符串会揭示一个由`|`和`-`字符组成的树。该文本树中的特征名称可以用`feature_names`参数指定；但由于输出的限制性，我们无法打印类别名称。让我们从`sklearn.tree`导入`export_text`，然后以简单的字符串形式可视化我们的树。
- en: Listing 22.46 Displaying a decision tree classifier as a string
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.46 以字符串形式显示决策树分类器
- en: '[PRE45]'
  id: totrans-322
  prefs: []
  type: TYPE_PRE
  zh: '[PRE45]'
- en: In the text, we clearly see the branching logic. Initially, the data is split
    using `Switch0`. The branch selection depends on whether `Switch0 <= 0.50`. Of
    course, because `Switch0` is either 0 or 1, that logic is identical to `Switch0
    == 0`. Why does the tree use an inequality when the simple `Switch0 == 0` statement
    should suffice? The answer has to do with how `DecisionTreeClassifier` handles
    continuous features. Thus far, all of our features have been Booleans; but in
    most real-world problems, the features are numeric. Fortunately, any numerical
    feature can be transformed into a Boolean feature. We simply need to run `feature
    >= thresh`, where `thresh` is some numeric threshold. In scikit-learn, decision
    trees scan for this threshold automatically.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 在文本中，我们可以清楚地看到分支逻辑。最初，数据使用`Switch0`进行分割。分支选择取决于`Switch0 <= 0.50`。当然，因为`Switch0`要么是0要么是1，所以这个逻辑等同于`Switch0
    == 0`。为什么树使用不等式，而简单的`Switch0 == 0`语句就足够了呢？答案与`DecisionTreeClassifier`处理连续特征的方式有关。到目前为止，我们所有的特征都是布尔值；但在大多数现实世界问题中，特征是数值的。幸运的是，任何数值特征都可以转换为布尔特征。我们只需要运行`feature
    >= thresh`，其中`thresh`是某个数值阈值。在scikit-learn中，决策树会自动扫描这个阈值。
- en: How should we select the optimal threshold for splitting a numeric feature?
    It’s easy; we just need to choose the threshold that minimizes the Gini impurity.
    Suppose that we’re examining a dataset that’s driven by a single numeric feature.
    In that data, the class always equals 0 when the feature is less than 0.7, or
    1 otherwise. Hence, `y = (v >= 0.7).astype(int)`, where `v` is the feature vector.
    By applying a threshold of 0.7, we can perfectly separate our class labels. Splitting
    on that threshold leads to a Gini impurity of 0.0, so we can isolate the threshold
    by computing the Gini impurity across a range of possible threshold values. Then
    we can select the value at which the impurity is minimized. Listing 22.47 samples
    a `feature` vector from a normal distribution, sets `y` to equal `(feature >=
    0.7).astype(int)`, computes the impurity across a range of thresholds, and plots
    the results (figure 22.8). The minimal impurity appears at the threshold of 0.7.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何选择分割数值特征的最佳阈值？这很简单；我们只需选择最小化Gini不纯度的阈值。假设我们正在检查一个由单个数值特征驱动的数据集。在该数据中，当特征值小于0.7时，类别始终等于0，否则为1。因此，`y
    = (v >= 0.7).astype(int)`，其中`v`是特征向量。通过应用0.7的阈值，我们可以完美地分离我们的类别标签。在该阈值上进行分割会导致Gini不纯度为0.0，因此我们可以通过计算一系列可能的阈值值范围内的Gini不纯度来隔离该阈值。然后我们可以选择使不纯度最小化的值。列表22.47从一个正态分布中采样`feature`向量，将`y`设置为`(feature
    >= 0.7).astype(int)`，计算一系列阈值的不纯度，并绘制结果（图22.8）。最小的不纯度出现在0.7的阈值处。
- en: Listing 22.47 Choosing a threshold by minimizing the Gini impurity
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.47 通过最小化Gini不纯度选择阈值
- en: '[PRE46]'
  id: totrans-326
  prefs: []
  type: TYPE_PRE
  zh: '[PRE46]'
- en: ❶ Randomly samples a numeric feature from a normal distribution
  id: totrans-327
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从正态分布中随机采样一个数值特征
- en: ❷ Class labels are 0 when the feature falls below a threshold of 0.7 and 1 otherwise.
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 当特征值低于0.7的阈值时，类别标签为0，否则为1。
- en: ❸ Iterates over thresholds ranging from 0 to 1.0
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 遍历从0到1.0的范围内的阈值
- en: ❹ At every threshold, we do a split and calculate the resulting Gini impurity.
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 在每个阈值处，我们进行一次分割并计算结果的不纯度。
- en: ❺ Chooses the threshold at which the Gini impurity is minimized. That threshold
    should be equal to 0.7.
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 选择使Gini不纯度最小化的阈值。该阈值应等于0.7。
- en: '![](../Images/22-08.png)'
  id: totrans-332
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-08.png)'
- en: Figure 22.8 A plot of Gini impurities across each possible threshold of a feature.
    The Gini impurity is minimized at a threshold of 0.7\. Thus, we can convert the
    numeric feature `f` into a binary feature `f` `>=` `0.7`.
  id: totrans-333
  prefs: []
  type: TYPE_NORMAL
  zh: 图22.8 特征的每个可能阈值的Gini不纯度图。Gini不纯度在0.7的阈值处最小化。因此，我们可以将数值特征`f`转换为二元特征`f >= 0.7`。
- en: In this manner, scikit-learn obtains inequality thresholds for all features
    used to train `DecisionTreeClassifier`, so the classifier can derive conditional
    logic from numeric data. Let’s now train `clf` on the numeric wine data introduced
    in the previous section. After training, we visualize the tree.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 以这种方式，scikit-learn获取了用于训练`DecisionTreeClassifier`的所有特征的不等式阈值，因此分类器可以从数值数据中推导出条件逻辑。现在，让我们在上一节中引入的数值葡萄酒数据上训练`clf`。训练后，我们可视化这棵树。
- en: Note As a reminder, the wine dataset contains three classes of wine. Thus far,
    we’ve only trained decision trees on two-class systems. However, our branching
    `if`/`else` logic can easily be extended to predict more than two classes. Consider,
    for instance, the statement `0 if x == 0 else 1 if y == 0 else 2`. The statement
    returns `0` if `x == 0`. Otherwise, the statement returns `1` if `y == 0` and
    `2` if `y != 0`. It’s straightforward to incorporate this added conditional logic
    into our classifier.
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：提醒一下，葡萄酒数据集包含三种葡萄酒类别。到目前为止，我们只对双类别系统训练了决策树。然而，我们的分支`if`/`else`逻辑可以很容易地扩展到预测超过两个类别。例如，考虑以下语句`0
    if x == 0 else 1 if y == 0 else 2`。如果`x == 0`，则该语句返回`0`。否则，如果`y == 0`，则返回`1`；如果`y
    != 0`，则返回`2`。将这种附加的条件逻辑纳入我们的分类器是直接的。
- en: Listing 22.48 Training a decision tree on numeric data
  id: totrans-336
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.48 在数值数据上训练决策树
- en: '[PRE47]'
  id: totrans-337
  prefs: []
  type: TYPE_PRE
  zh: '[PRE47]'
- en: The printed tree is larger than any tree or conditional statement seen thus
    far. The tree is large because it is deep. In machine learning, tree depth equals
    the number of nested `if`/`else` statements required to capture the logic within
    the tree. For instance, our single switch example required a single `if`/`else`
    statement. It therefore had a depth of 1\. Meanwhile, our two-switch system had
    a depth of 2\. Also, our three-feature weather predictor had a depth of 3\. Our
    wine predictor is even deeper, making it more difficult to follow the logic. In
    scikit-learn, we can limit the depth of a trained tree using the `max_depth` hyperparameter.
    For example, running `DecisionTreeClassifier(max_ depth=2)` will create a classifier
    whose depth cannot exceed two nested statements. Let’s demonstrate by training
    a limited depth classifier on our wine data.
  id: totrans-338
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的树比迄今为止看到的任何树或条件语句都要大。树之所以大，是因为它很深。在机器学习中，树深度等于捕获树内逻辑所需的嵌套`if`/`else`语句的数量。例如，我们的单个开关示例需要一个`if`/`else`语句。因此，它有深度1。同时，我们的双开关系统有深度2。我们的三特征天气预测器有深度3。我们的葡萄酒预测器甚至更深，这使得逻辑更难追踪。在scikit-learn中，我们可以使用`max_depth`超参数限制训练树的深度。例如，运行`DecisionTreeClassifier(max_depth=2)`将创建一个深度不能超过两个嵌套语句的分类器。让我们通过在我们的葡萄酒数据上训练一个有限深度的分类器来演示。
- en: Listing 22.49 Training a tree with limited depth
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.49 训练有限深度的树
- en: '[PRE48]'
  id: totrans-340
  prefs: []
  type: TYPE_PRE
  zh: '[PRE48]'
- en: 'The printed tree is two `if`/`else` statements deep. The outer statement is
    determined by the proline concentration: if the proline concentration is greater
    than 755, flavonoids are used to identify the wine.'
  id: totrans-341
  prefs: []
  type: TYPE_NORMAL
  zh: 打印的树是两个`if`/`else`语句深。外层语句由脯氨酸浓度决定：如果脯氨酸浓度大于755，则使用黄酮类化合物来识别葡萄酒。
- en: Note The dataset has *flavonoids* misspelled as *flavanoids*.
  id: totrans-342
  prefs: []
  type: TYPE_NORMAL
  zh: 注意数据集中将*黄酮类化合物*误拼为*黄酮*。
- en: 'Otherwise, the `OD280` / `OD315` of diluted wines is utilized for class determination.
    Based on the output, we can fully comprehend the working logic in the model. Furthermore,
    we can infer the relative importance of features that are driving class prediction:'
  id: totrans-343
  prefs: []
  type: TYPE_NORMAL
  zh: 否则，用于分类的稀释葡萄酒的`OD280` / `OD315`被利用。基于输出，我们可以完全理解模型中的工作逻辑。此外，我们可以推断出驱动类别预测的特征的相对重要性：
- en: Proline is the most important feature. It appears at the top of the tree and
    therefore has the lowest Gini impurity. Splitting on that feature must thus lead
    to the most imbalanced data. In an imbalanced dataset, it is much easier to isolate
    one class over another, so knowing the proline concentration allows us to more
    easily separate the different classes of wine.
  id: totrans-344
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 脯氨酸是最重要的特征。它出现在树的顶部，因此具有最低的基尼不纯度。因此，在该特征上分割必须导致最不平衡的数据。在不平衡的数据集中，相对于另一个类别更容易隔离一个类别，因此知道脯氨酸浓度使我们更容易区分不同的葡萄酒类别。
- en: This is consistent with our linear model trained in section 21, where proline
    coefficients yielded the most noticeable signal.
  id: totrans-345
  prefs:
  - PREF_IND
  type: TYPE_NORMAL
  zh: 这与我们在第21节中训练的线性模型一致，其中脯氨酸系数产生了最明显的信号。
- en: Flavonoids and `OD280` / `OD315` are also important drivers of prediction (although
    not as important as proline).
  id: totrans-346
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 黄酮类化合物和`OD280` / `OD315`也是预测的重要驱动因素（尽管不如脯氨酸重要）。
- en: The remaining 10 features are less relevant.
  id: totrans-347
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 剩余的10个特征的相关性较低。
- en: The depth at which a feature appears in the tree is an indicator of its relative
    importance. That depth is determined by the Gini impurity. Hence, the Gini impurity
    can be used to compute an importance score. The importance scores across all features
    are stored in the `feature_importances_` attribute of `clf.` Listing 22.50 prints
    `clf.feature_importances_`.
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 一个特征在树中出现的深度是其相对重要性的指标。这个深度由基尼不纯度决定。因此，基尼不纯度可以用来计算重要性分数。所有特征的重要性分数存储在`clf.`的`feature_importances_`属性中。列表22.50打印了`clf.feature_importances_`。
- en: Note More precisely, scikit-learn computes feature importance by subtracting
    the Gini impurity of the feature split from the Gini impurity of the previous
    split. For instance, in the wine tree, the impurity of the flavonoids at depth
    2 is subtracted from the impurity of proline at depth 1\. After the subtraction,
    the importance is weighted by the fraction of training samples represented during
    the split.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 注意更精确地说，scikit-learn通过从先前分割的基尼不纯度中减去特征分割的基尼不纯度来计算特征重要性。例如，在葡萄酒树中，深度2处黄酮类化合物的杂质被从深度1处脯氨酸的杂质中减去。减去后，重要性通过在分割期间表示的训练样本的分数加权。
- en: Listing 22.50 Printing the feature importances
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.50 打印特征重要性
- en: '[PRE49]'
  id: totrans-351
  prefs: []
  type: TYPE_PRE
  zh: '[PRE49]'
- en: In the printed array, the importance of feature `i` is equal to `feature_importances_[i]`.
    Most of the features receive a score of 0 because they are not represented in
    the trained tree. Let’s rank the remaining features based on their importance
    score.
  id: totrans-352
  prefs: []
  type: TYPE_NORMAL
  zh: 在打印的数组中，特征 `i` 的重要性等于 `feature_importances_[i]`。大多数特征得分都是0，因为它们在训练的树中没有表示。让我们根据它们的重要性分数对剩余的特征进行排序。
- en: Listing 22.51 Ranking relevant features by importance
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.51 按重要性排序相关特征
- en: '[PRE50]'
  id: totrans-354
  prefs: []
  type: TYPE_PRE
  zh: '[PRE50]'
- en: Among our features, proline is ranked as most important. It’s followed by `OD280`
    / `OD315` and the flavonoids.
  id: totrans-355
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的特征中，脯氨酸被列为最重要的。其次是 `OD280` / `OD315` 和类黄酮。
- en: Tree-based feature ranking can help draw meaningful insights from our data.
    We’ll emphasize this point by exploring the serious problem of cancer diagnosis.
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: 基于树的特性排序可以帮助我们从数据中提取有意义的见解。我们将通过探讨癌症诊断的严重问题来强调这一点。
- en: 22.2.1 Studying cancerous cells using feature importance
  id: totrans-357
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 22.2.1 使用特征重要性研究癌细胞
- en: 'An identified tumor could be cancerous. The tumor needs to be examined under
    a microscope to determine if it is malignant (cancerous) or benign (noncancerous).
    Zooming in on the tumor reveals individual cells. Each cell has a multitude of
    measurable features, including these:'
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: 确定的肿瘤可能是恶性的。需要通过显微镜检查肿瘤以确定它是恶性的（癌症）还是良性的（非癌症）。放大肿瘤可以看到单个细胞。每个细胞都有许多可测量的特征，包括以下这些：
- en: Area
  id: totrans-359
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 面积
- en: Perimeter
  id: totrans-360
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 周长
- en: Compactness (the ratio of the squared perimeter to the area)
  id: totrans-361
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 紧凑度（周长平方与面积的比率）
- en: Radius (a cell is not perfectly round, so the radius is computed as the mean
    distance from the center to the perimeter)
  id: totrans-362
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 半径（由于细胞不是完美的圆形，因此半径被计算为从中心到周长的平均距离）
- en: Smoothness (variations in the distance from the cell center to the perimeter)
  id: totrans-363
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 平滑度（从细胞中心到周长的距离变化）
- en: Concave points (the number of inward curves on the perimeter)
  id: totrans-364
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹点（周长上的内向曲线数量）
- en: Concavity (the average inward angle of the concave points)
  id: totrans-365
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 凹度（凹点的平均内向角度）
- en: Symmetry (if one side of the cell resembles the other)
  id: totrans-366
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 对称性（如果细胞的一侧与另一侧相似）
- en: Texture (he standard deviation of color shades in the cell image)
  id: totrans-367
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 纹理（细胞图像中颜色阴影的标准差）
- en: Fractal dimension (the “wriggliness” of the perimeter, based on the number of
    separate straight-ruler measurements required to measure the wriggly border)
  id: totrans-368
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 分形维度（周长的“曲折度”，基于测量曲折边界的单独直尺测量次数）
- en: 'Imaging technology allows us to compute these features for each individual
    cell. However, the tumor biopsy will reveal dozens of cells beneath the microscope
    (figure 22.9), so the individual features must somehow be aggregated together.
    The simplest way to aggregate the features is to compute their mean and standard
    deviation. We can also store the most extreme value computed across each cell:
    for instance, we can record the largest concavity measured across the cells. Informally,
    we’ll refer to this statistic as the *worst concavity*.'
  id: totrans-369
  prefs: []
  type: TYPE_NORMAL
  zh: 成像技术使我们能够为每个单个细胞计算这些特征。然而，肿瘤活检将在显微镜下揭示数十个细胞（图 22.9），因此必须以某种方式将这些单个特征聚合在一起。聚合特征的最简单方法就是计算它们的平均值和标准差。我们还可以存储每个细胞计算出的最极端值：例如，我们可以记录细胞间测量的最大凹度。非正式地，我们将这个统计量称为
    *最严重的凹度*。
- en: '![](../Images/22-09.png)'
  id: totrans-370
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-09.png)'
- en: Figure 22.9 Dozens of tumor cells seen through a microscope. Each cell has 10
    different measurable features. We can aggregate these features across cells using
    three different statistics, so we obtain 30 features total for determining whether
    the tumor is malignant or benign.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.9 通过显微镜看到的数十个肿瘤细胞。每个细胞有10个不同的可测量特征。我们可以使用三种不同的统计方法来聚合这些特征，因此我们获得了总共30个特征，以确定肿瘤是恶性的还是良性的。
- en: Note Usually, these features are not computed on the cell itself. Instead, they
    are computed on the nucleus of the cell. The nucleus is an enclosed, circular
    structure in the center of the cell that is easily visible through a microscope.
  id: totrans-372
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：通常，这些特征不是在细胞本身上计算的。相反，它们是在细胞的细胞核上计算的。细胞核是细胞中心的一个封闭的圆形结构，通过显微镜很容易看到。
- en: 'The three different aggregations across 10 measured features lead to 30 features
    total. Which features are most important for determining cancer-cell malignancy?.
    We can find out. Scikit-learn includes a cancer-cell dataset: let’s import it
    from `sklearn.datasets` and print the feature names and class names.'
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 在10个测量的不同聚合中，总共产生了30个特征。哪些特征对于确定癌细胞恶性程度最为重要？我们可以找到答案。Scikit-learn 包含一个癌细胞数据集：让我们从
    `sklearn.datasets` 导入它，并打印特征名称和类别名称。
- en: Listing 22.52 Importing scikit-learn’s cancer-cell dataset
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.52 导入 scikit-learn 的癌细胞数据集
- en: '[PRE51]'
  id: totrans-375
  prefs: []
  type: TYPE_PRE
  zh: '[PRE51]'
- en: The dataset contains 30 different features. Let’s rank them by importance and
    output the ranked features along with their importance scores. We ignore features
    whose importance scores are close to zero.
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含 30 个不同的特征。让我们按重要性对它们进行排序，并输出排序后的特征及其重要性得分。我们忽略重要性得分接近零的特征。
- en: Listing 22.53 Ranking tumor features by importance
  id: totrans-377
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.53 按重要性排序肿瘤特征
- en: '[PRE52]'
  id: totrans-378
  prefs: []
  type: TYPE_PRE
  zh: '[PRE52]'
- en: 'The three top-ranking features are *worst radius*, *worst concave points*,
    and *worst texture*. Neither the mean nor the standard deviation drives the malignancy
    of the tumor; instead, it is the presence of a few extreme outliers that determines
    the cancer diagnosis. Even one or two irregularly shaped cells can indicate malignancy.
    Among the top-ranking features, worst radius particularly stands out: it has an
    importance score of 0.70\. The next-highest importance score is 0.14\. This difference
    suggests that the radius of the largest cell is an extremely important indicator
    of cancer. We an check this hypothesis by plotting histograms of the worst-radius
    measurements across the two classes (figure 22.10).'
  id: totrans-379
  prefs: []
  type: TYPE_NORMAL
  zh: 排名前三的特征是**最坏半径**、**最坏凹点**和**最坏纹理**。既不是平均值也不是标准差推动了肿瘤的恶性；相反，是存在一些极端的异常值决定了癌症的诊断。甚至一个或两个不规则形状的细胞也可以表明恶性。在排名靠前的特征中，最坏半径尤其突出：它的重要性得分为
    0.70。下一个最高的重要性得分为 0.14。这种差异表明，最大细胞的半径是癌症的一个极其重要的指标。我们可以通过绘制两个类别中最大半径测量的直方图来检验这个假设（图
    22.10）。
- en: '![](../Images/22-10.png)'
  id: totrans-380
  prefs: []
  type: TYPE_IMG
  zh: '![图片](../Images/22-10.png)'
- en: Figure 22.10 A histogram of worst-radius measurements across cancerous and noncancerous
    tumors. That radius is noticeably greater when the tumor is malignant and not
    benign.
  id: totrans-381
  prefs: []
  type: TYPE_NORMAL
  zh: 图 22.10 恶性和非恶性肿瘤最坏半径测量的直方图。当肿瘤是恶性的而不是良性的时，该半径明显更大。
- en: Listing 22.54 Plotting two worst radius histograms
  id: totrans-382
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.54 绘制两个最坏半径直方图
- en: '[PRE53]'
  id: totrans-383
  prefs: []
  type: TYPE_PRE
  zh: '[PRE53]'
- en: The histogram reveals an enormous separation between malignant and benign worst-radius
    measurements. In fact, the presence of any cell radius greater than 20 units is
    a sure-fire indication of malignancy.
  id: totrans-384
  prefs: []
  type: TYPE_NORMAL
  zh: 直方图揭示了恶性和良性最坏半径测量之间的巨大差异。事实上，任何大于 20 个单位的细胞半径的存在都是恶性的明确迹象。
- en: 'By training a decision tree, we gained insights into medicine and biology.
    Generally, decision trees are very useful tools for comprehending signals in complex
    datasets. The trees are very interpretable; their learned logical statements can
    easily be probed by data science. Furthermore, decision trees offer additional
    benefits:'
  id: totrans-385
  prefs: []
  type: TYPE_NORMAL
  zh: 通过训练决策树，我们获得了对医学和生物学的洞察。通常，决策树是理解复杂数据集中信号非常有用的工具。这些树非常易于解释；它们学习到的逻辑语句可以很容易地被数据科学探究。此外，决策树还提供了额外的优势：
- en: Decision tree classifiers are very quick to train. They are orders of magnitude
    faster than KNN classifiers. Also, unlike linear classifiers, they are not dependent
    on repeated training iterations.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器训练非常快。它们的速度比 KNN 分类器快得多。此外，与线性分类器不同，它们不依赖于重复的训练迭代。
- en: Decision tree classifiers don’t depend on data manipulation before training.
    Logistic regression requires us to standardize the data before training; decision
    trees do not require standardization. Also, linear classifiers cannot handle categorical
    features without pretraining transformations, but a decision tree can handle these
    features directly.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树分类器在训练前不依赖于数据操作。逻辑回归要求我们在训练前对数据进行标准化；决策树不需要标准化。此外，线性分类器无法处理未经预训练转换的类别特征，但决策树可以直接处理这些特征。
- en: Decision trees are not limited by the geometric shape of the training data.
    In contrast, as shown in figure 22.1, KNN and linear classifiers cannot handle
    certain geometric configurations.
  id: totrans-388
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树不受训练数据几何形状的限制。相反，如图 22.1 所示，KNN 和线性分类器无法处理某些几何配置。
- en: 'All these benefits come at a price: trained decision tree classifiers sometimes
    perform poorly on real-world data.'
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些好处都需要付出代价：训练有素的决策树分类器在现实世界的数据上有时表现不佳。
- en: 22.3 Decision tree classifier limitations
  id: totrans-390
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.3 决策树分类器限制
- en: Decision trees learn the training data well—sometimes too well. Certain trees
    simply memorize the data without yielding any useful real-world insights. There
    are serious limits to rote memorization. Imagine a college student who is studying
    for a physics final. The previous year’s exam is available online and includes
    written answers to all the questions. The student memorizes last year’s exam.
    Given last year’s questions, the student can easily recite the answers. The student
    is feeling confident—but on the day of the final, disaster strikes! The questions
    on the final are slightly different. Last year’s exam asked for the velocity of
    a tennis ball dropped from 20 feet, but this exam asks for the velocity of a billiard
    ball dropped from 50 feet. The student is at a loss. They learned the answers
    but not the general patterns driving those answers; they can’t do well on the
    exam because their knowledge does not generalize.
  id: totrans-391
  prefs: []
  type: TYPE_NORMAL
  zh: 决策树能够很好地学习训练数据—有时过于完美。某些树只是简单地记住数据，而没有提供任何有用的现实世界见解。死记硬背有严重的局限性。想象一个大学生正在为物理期末考试做准备。去年的考试在网上有，包括所有问题的书面答案。学生记住了去年的考试。面对去年的问题，学生可以轻松地背诵答案。学生感到自信—但在期末考试的那天，灾难发生了！期末考试的问题略有不同。去年的考试要求计算从
    20 英尺高的地方掉落的网球的速度，但这次考试要求计算从 50 英尺高的地方掉落的台球的速度。学生感到困惑。他们记住了答案，但没有记住驱动这些答案的一般模式；由于他们的知识不能推广，他们在考试中表现不佳。
- en: Overmemorization limits the usefulness of our trained models. In supervised
    machine learning, this phenomenon is referred to as *overfitting*. An overfitted
    model corresponds too closely to the training data, so it may fail to predict
    accurately on new observations. Decision tree classifiers are particularly prone
    to overfitting since they can memorize the training data. For instance, our cancer
    detector `clf` has perfectly memorized the training set `(X, y)`. We can confirm
    by outputting the accuracy with which `clf.predict(X)` corresponds to `y`.
  id: totrans-392
  prefs: []
  type: TYPE_NORMAL
  zh: 过度记忆限制了我们的训练模型的有用性。在监督机器学习中，这种现象被称为 *过拟合*。过拟合的模型与训练数据过于接近，因此它可能在新的观察上无法准确预测。决策树分类器特别容易过拟合，因为它们可以记住训练数据。例如，我们的癌症检测器
    `clf` 完美地记住了训练集 `(X, y)`。我们可以通过输出 `clf.predict(X)` 与 `y` 对应的准确率来确认这一点。
- en: Listing 22.55 Checking the accuracy of the cancer-cell model
  id: totrans-393
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.55 检查癌细胞模型的准确性
- en: '[PRE54]'
  id: totrans-394
  prefs: []
  type: TYPE_PRE
  zh: '[PRE54]'
- en: The classifier can identify any training example with 100% accuracy, but this
    does not mean it can generalize as well to real-world data. We can better gauge
    the classifier’s true accuracy using cross-validation. Listing 22.56 splits `(X,
    y)` into training set `(X_train, y_train)` and validation set `(X_test, y_test)`.
    We train `clf` to perfectly memorize `(X_train, y_train)` and then check how well
    the model can generalize to data that it has not encountered before. We do this
    by computing the model’s accuracy on the validation set.
  id: totrans-395
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器可以以 100% 的准确率识别任何训练示例，但这并不意味着它可以将现实世界数据推广得同样好。我们可以通过交叉验证更好地衡量分类器的真实准确率。列表
    22.56 将 `(X, y)` 分割为训练集 `(X_train, y_train)` 和验证集 `(X_test, y_test)`。我们训练 `clf`
    完美地记住 `(X_train, y_train)`，然后检查模型在之前未遇到的数据上的泛化能力如何。我们通过计算模型在验证集上的准确率来完成这项工作。
- en: Listing 22.56 Checking model accuracy with cross-validation
  id: totrans-396
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.56 使用交叉验证检查模型准确性
- en: '[PRE55]'
  id: totrans-397
  prefs: []
  type: TYPE_PRE
  zh: '[PRE55]'
- en: The classifier’s true accuracy is at 90%. That accuracy is decent, but we can
    definitely do better. We need a way to improve the performance by limiting overfitting
    in the tree. This can be done by training multiple decision trees at once using
    a technique called *random forest classification*.
  id: totrans-398
  prefs: []
  type: TYPE_NORMAL
  zh: 分类器的真实准确率为 90%。这个准确率还不错，但我们肯定可以做得更好。我们需要一种方法来通过限制树中的过拟合来提高性能。这可以通过使用称为 *随机森林分类*
    的技术同时训练多个决策树来实现。
- en: Relevant scikit-learn decision tree classifier methods
  id: totrans-399
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的 scikit-learn 决策树分类器方法
- en: '`clf = DecisionTreeClassifier()`—Initializes a decision tree classifier'
  id: totrans-400
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = DecisionTreeClassifier()`—初始化一个决策树分类器'
- en: '`clf = DecisionTreeClassifier(max_depth=x)`—Initializes a decision tree classifier
    with a maximum depth of `x`'
  id: totrans-401
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = DecisionTreeClassifier(max_depth=x)`—初始化一个最大深度为 `x` 的决策树分类器'
- en: '`clf.feature_importances_`—Accesses the feature importance scores of a trained
    decision tree classifier'
  id: totrans-402
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.feature_importances_`—访问训练好的决策树分类器的特征重要性分数'
- en: '`plot_tree(clf)`—Plots a decision tree diagram associated with tree `clf`'
  id: totrans-403
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_tree(clf)`—绘制与树 `clf` 相关的决策树图'
- en: '`plot_tree(clf, feature_names=x, class_names=y)`—Plots a decision tree diagram
    with customized feature names and class labels'
  id: totrans-404
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`plot_tree(clf, feature_names=x, class_names=y)`—使用自定义的特征名称和类标签绘制决策树图'
- en: '`export_text(clf)`—Represents the decision tree diagram as a simple string'
  id: totrans-405
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`export_text(clf)`——将决策树图表示为一个简单的字符串'
- en: '`export_text(clf, feature_names=x)`—Represents the decision tree diagram as
    a simple string with customized feature names'
  id: totrans-406
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`export_text(clf, feature_names=x)`——将决策树图表示为一个带有自定义特征名称的简单字符串'
- en: 22.4 Improving performance using random forest classification
  id: totrans-407
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.4 使用随机森林分类提高性能
- en: Sometimes, in human affairs, the aggregated viewpoint of a crowd outperforms
    all individual predictions. In 1906, a crowd gathered at the Plymouth country
    fair to guess the weight of a 1,198-pound ox. Each person present wrote down their
    best guess, and the median of these guesses was tallied. The final median estimate
    of 1,207 pounds was within 1% of the actual weight. This aggregated triumph of
    collective intelligence is called the *wisdom of the crowd*.
  id: totrans-408
  prefs: []
  type: TYPE_NORMAL
  zh: 有时，在人类事务中，大众的聚合观点优于所有个体的预测。在 1906 年，一群人在普利茅斯乡村集市上猜测一头 1,198 磅的公牛的重量。在场的每个人写下他们最好的猜测，然后统计这些猜测的中位数。最终的估计中位数
    1,207 磅与实际重量相差 1%。这种集体智慧的聚合胜利被称为 *大众智慧*。
- en: Modern democratic institutions are built on the wisdom of the crowd. In democratic
    nations, the people come together and vote on the future of their country. Usually,
    the voters have incredibly diverse political views, opinions, and life experiences.
    But somehow, their accumulated choices average out into a decision that can benefit
    their country in the long run. This democratic process is partially dependent
    on the diversity of the populace. If everyone has the exact same opinion, everyone
    is prone to the same errors—but a diversity of views helps limit those errors.
    The crowd tends to make its best decisions when its members think in different
    ways.
  id: totrans-409
  prefs: []
  type: TYPE_NORMAL
  zh: 现代民主机构建立在大众智慧的基础之上。在民主国家，人民聚集在一起，投票决定他们国家的未来。通常，选民拥有极其多样化的政治观点、意见和生活经验。但不知何故，他们的累积选择平均化成一个可以长期造福国家的决策。这个民主过程部分依赖于人口的多样性。如果每个人都持有相同的观点，那么每个人都容易犯相同的错误——但观点的多样性有助于限制这些错误。当成员们以不同的方式思考时，大众往往会做出最好的决策。
- en: The wisdom of the crowd is a natural phenomenon. It’s seen not just in people
    but also in animals. Bats, fish, birds, and even flies can optimize their behavior
    when surrounded by other members of their species. The phenomenon can also be
    observed in machine learning. A crowd of decision trees can sometimes outperform
    a single tree; however, for this to happen, the inputs into each tree must be
    diverse.
  id: totrans-410
  prefs: []
  type: TYPE_NORMAL
  zh: 大众智慧是一种自然现象。它不仅存在于人类中，也存在于动物中。蝙蝠、鱼类、鸟类甚至苍蝇都可以在周围有其他物种成员的情况下优化它们的行为。这种现象也可以在机器学习中观察到。一群决策树有时可以超越一棵单独的树；然而，为了实现这一点，每棵树必须具有不同的输入。
- en: Let’s explore the wisdom of the crowd by initializing 100 decision trees. A
    large collection of trees is (not surprisingly) referred to as a *forest*. Hence,
    we store our trees in a `forest` list.
  id: totrans-411
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们通过初始化 100 个决策树来探索大众智慧的奥秘。大量树木的集合（不出所料）被称为 *森林*。因此，我们将我们的树存储在 `forest` 列表中。
- en: Listing 22.57 Initializing a 100-tree forest
  id: totrans-412
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.57 初始化 100 树森林
- en: '[PRE56]'
  id: totrans-413
  prefs: []
  type: TYPE_PRE
  zh: '[PRE56]'
- en: How should we train the trees in the forest? Naively, we could train each individual
    tree on our cancer training set `(X_train, y_train)`. However, we would end up
    with 100 trees that memorized the exact same data. The trees would therefore make
    identical predictions, so the key element of diversity would be missing from the
    forest. Without diversity, the wisdom of the crowd cannot be applied. What should
    we do?
  id: totrans-414
  prefs: []
  type: TYPE_NORMAL
  zh: 我们应该如何训练森林中的树？天真地，我们可以在我们的癌症训练集 `(X_train, y_train)` 上训练每一棵单独的树。然而，我们最终会得到 100
    棵记忆了完全相同数据的树。因此，这些树将做出相同的预测，从而使得森林中缺失了多样性的关键元素。没有多样性，大众智慧就无法应用。我们应该怎么做？
- en: One solution is to randomize our training data. In section 7, we studied a technique
    called bootstrapping with replacement. In this technique, the contents of an *N*-element
    dataset are sampled repeatedly. The elements are sampled with replacement, which
    means duplicate elements are allowed. Through sampling, we can generate a new
    *N*-element dataset whose contents differ from the original data. Let’s bootstrap
    our training data to randomly generate a new training set `(X_train_new, y_train_new)`.
  id: totrans-415
  prefs: []
  type: TYPE_NORMAL
  zh: 一种解决方案是对我们的训练数据进行随机化。在第 7 节中，我们研究了一种称为带替换的 bootstrapping 的技术。在这种技术中，一个 *N*-元素数据集的内容被反复采样。元素被带替换地采样，这意味着允许重复元素。通过采样，我们可以生成一个新的
    *N*-元素数据集，其内容与原始数据不同。让我们对训练数据进行 bootstrapping，以随机生成一个新的训练集 `(X_train_new, y_train_new)`。
- en: Listing 22.58 Randomly sampling a new training set
  id: totrans-416
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.58 随机采样新的训练集
- en: '[PRE57]'
  id: totrans-417
  prefs: []
  type: TYPE_PRE
  zh: '[PRE57]'
- en: ❶ Applies bootstrapping to training set (X, y) to generate a brand-new training
    set
  id: totrans-418
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将bootstrap应用于训练集（X, y）以生成全新的训练集
- en: ❷ Samples random indices of data points in (X, y). The sampling is carried out
    with replacement, so certain indices may be sampled twice.
  id: totrans-419
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在（X, y）中采样数据点的随机索引。采样是有放回的，因此某些索引可能会被采样两次。
- en: ❸ Returns a random training set based on the sampled indices
  id: totrans-420
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 根据采样索引返回随机训练集
- en: ❹ The bootstrapped training set is as large as the original training set.
  id: totrans-421
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 通过bootstrap生成的训练集与原始训练集大小相同。
- en: ❺ The bootstrapped training set is not equal to the original set.
  id: totrans-422
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 通过bootstrap生成的训练集不等于原始集。
- en: Now, let’s run our `bootstrap` function 100 times to generate 100 different
    training sets.
  id: totrans-423
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，让我们运行我们的`bootstrap`函数100次，以生成100个不同的训练集。
- en: Listing 22.59 Randomly sampling 100 new training sets
  id: totrans-424
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.59 随机采样100个新的训练集
- en: '[PRE58]'
  id: totrans-425
  prefs: []
  type: TYPE_PRE
  zh: '[PRE58]'
- en: Across our 100 training sets, the data may be different, but all the features
    are the same. However, we can increase the overall diversity by randomizing the
    features in `features_train`. In general, the wisdom of the crowd works best when
    different individuals pay attention to diverging features. For instance, in a
    democratic election, the top priorities of urban voters might not overlap with
    those of rural voters. Urban voters might focus on housing policies and crime,
    and rural voters might focus on crop tariffs and property taxes. These diverging
    priorities can lead to a consensus that benefits both urban and rural voters in
    the long run.
  id: totrans-426
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的100个训练集中，数据可能不同，但所有特征都是相同的。然而，我们可以通过在`features_train`中随机化特征来增加整体多样性。一般来说，当不同个体关注不同的特征时，群众的智慧效果最好。例如，在民主选举中，城市选民的最高优先事项可能不会与农村选民的重叠。城市选民可能关注住房政策和犯罪，而农村选民可能关注农产品关税和财产税。这些不同的优先事项可能导致一个长期有利于城市和农村选民的一致意见。
- en: 'More concretely, in supervised machine learning, feature diversity can limit
    overfitting. Consider, for example, our cancer dataset. As we have seen, “worst
    radius” is an incredibly impactful feature. Thus, all trained models where that
    feature is included will rely on the radius as a crutch. But on certain rare occasions,
    a tumor might be cancerous even if the radius is low. Conformist models will mislabel
    that tumor if they all rely on the same features. Imagine, however, if we train
    some models without including the radius in their feature sets: these models will
    be forced to search for alternate, subtle patterns of cell malignancy and will
    be more resilient to fluctuating real-world observations. Individually, each model
    might not perform as well because its feature set is limited. Collectively, models
    should perform better than each individual tree.'
  id: totrans-427
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在监督机器学习中，特征多样性可以限制过拟合。例如，考虑我们的癌症数据集。正如我们所见，“最坏半径”是一个影响极大的特征。因此，包含该特征的任何训练模型都将依赖于半径作为支撑。但在某些罕见的情况下，即使半径较低，肿瘤也可能是癌性的。如果所有模型都依赖于相同的特征，那么守成模型可能会错误地标记该肿瘤。然而，想象一下，如果我们训练一些不包含半径在其特征集中的模型：这些模型将被迫寻找替代的、微妙的细胞恶性模式，并将对波动的现实世界观察更加有弹性。单个模型可能表现不佳，因为其特征集有限。但是，集体模型应该比单个树的表现更好。
- en: We’ll aim to train the trees in the forest on random samples of features in
    `features_ train`. Currently, each feature matrix holds 30 cancer-related measurements.
    We need to lower the feature count in each random sample from 30 to a smaller
    number. What is an appropriate sample count? Well, it has been shown that the
    square root of the total feature count is usually a good choice for the sample
    size. The square root of 30 equals approximately 5, so we’ll set our sample size
    to 5.
  id: totrans-428
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的目标是在森林中的树上训练随机样本的`features_train`特征。目前，每个特征矩阵包含30个与癌症相关的测量值。我们需要将每个随机样本中的特征计数从30降低到更小的数字。合适的样本计数是多少？嗯，已经证明，总特征数的平方根通常是样本大小的良好选择。30的平方根大约等于5，所以我们将样本大小设置为5。
- en: Let’s iterate over `features_train` and filter each feature matrix to five random
    columns. We also track the indices of the randomly chosen features for later use
    during validation.
  id: totrans-429
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们对`features_train`进行迭代，并对每个特征矩阵过滤出五个随机列。我们还会跟踪随机选择的特征索引，以便在验证时使用。
- en: Listing 22.60 Randomly sampling the training features
  id: totrans-430
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.60 随机采样训练特征
- en: '[PRE59]'
  id: totrans-431
  prefs: []
  type: TYPE_PRE
  zh: '[PRE59]'
- en: ❶ The feature sample size is equal to approximately the square root of the total
    feature count.
  id: totrans-432
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 特征样本大小大约等于总特征数的平方根。
- en: ❷ Given 30 features total, we expect the sample size to equal 5.
  id: totrans-433
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 给定总共30个特征，我们期望样本大小等于5。
- en: ❸ We randomly sample 5 of 30 feature columns for each tree. Sampling is carried
    out without replacement because a duplicate feature will not yield novel signals
    during training.
  id: totrans-434
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 我们为每棵树随机采样 5 个 30 个特征列。抽样是不放回的，因为重复的特征在训练期间不会产生新的信号。
- en: ❹ Prints the randomly sampled feature names in the very first and very last
    feature subsets
  id: totrans-435
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 打印出非常第一个和最后一个特征子集中的随机采样特征名称
- en: ❺ Five randomly sampled features do not include the worst-radius measurement.
  id: totrans-436
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 五个随机采样的特征不包括最坏半径测量。
- en: ❻ Five randomly sampled features include the impactful worst-radius measurement.
  id: totrans-437
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 五个随机采样的特征包括影响最大的最坏半径测量。
- en: We’ve randomized each of our 100 feature matrices by both row (data point) and
    column (feature). The training data for every tree is very diverse. Let’s train
    each *i* th tree in `forest` on training set `(features_train[i], classes_train[i])`.
  id: totrans-438
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经通过行（数据点）和列（特征）对每个 100 个特征矩阵进行了随机化。每棵树的训练数据都非常多样化。让我们在 `forest` 上的训练集 `(features_train[i],
    classes_train[i])` 上训练第 *i* 棵树。
- en: Listing 22.61 Training the trees in the forest
  id: totrans-439
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.61 训练森林中的树
- en: '[PRE60]'
  id: totrans-440
  prefs: []
  type: TYPE_PRE
  zh: '[PRE60]'
- en: 'We’ve trained every tree in the forest. Now, let’s put the trained trees to
    a vote. What is the class label of the data point at `X_test[0]`? We can check
    using the wisdom of the crowd. Here, we iterate across every trained `clf_tree`
    in the forest. For every *i* th iteration, we do the following:'
  id: totrans-441
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经训练了森林中的每一棵树。现在，让我们让这些训练过的树木进行投票。`X_test[0]` 数据点的类别标签是什么？我们可以通过群众的智慧来检查。在这里，我们遍历森林中的每一个训练过的
    `clf_tree`。对于每一次 *i* 次迭代，我们执行以下操作：
- en: Utilize the tree to predict the class label at `X_test[0]`. As a reminder, each
    *i* th tree in `forest` depends on a random subset of features. The chosen feature
    indices are stored in `feature_indices[i]`. Hence, we need to filter the `X_test[0]`
    by the chosen indices before making the prediction.
  id: totrans-442
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用树来预测 `X_test[0]` 处的类别标签。提醒一下，`forest` 中的每一棵 *i* 棵树依赖于一个随机特征子集。所选特征索引存储在 `feature_indices[i]`
    中。因此，在做出预测之前，我们需要通过所选索引过滤 `X_test[0]`。
- en: Record the prediction as the *vote* of the tree at index `i`.
  id: totrans-443
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将预测记录为索引 `i` 处树的 *投票*。
- en: Once all the trees have voted, we tally the 100 votes and select the class label
    that has received the plurality of the votes.
  id: totrans-444
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦所有树木都投票完毕，我们统计 100 票，并选择获得多数票的类别标签。
- en: Note This process is very similar to the KNN plurality voting that we utilized
    in section 20.
  id: totrans-445
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：此过程与我们在第 20 节中使用的 KNN 多数投票非常相似。
- en: Listing 22.62 Using tree voting to classify a data point
  id: totrans-446
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.62 使用树投票对数据点进行分类
- en: '[PRE61]'
  id: totrans-447
  prefs: []
  type: TYPE_PRE
  zh: '[PRE61]'
- en: ❶ Iterates over 100 trained trees
  id: totrans-448
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 遍历 100 棵训练过的树
- en: ❷ Adjusts the columns in feature_vector to correspond with the five random feature
    indices associated with each tree
  id: totrans-449
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 调整 feature_vector 中的列以对应于每个树关联的五个随机特征索引
- en: ❸ Each tree casts a vote by returning a class-label prediction.
  id: totrans-450
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 每棵树通过返回一个类别标签预测来进行投票。
- en: ❹ Counts all votes
  id: totrans-451
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 计算所有投票
- en: 93% of the trees voted for Class 0\. Let’s check if this majority vote is correct.
  id: totrans-452
  prefs: []
  type: TYPE_NORMAL
  zh: 93% 的树木投票选择了类别 0。让我们检查这个多数投票是否正确。
- en: Listing 22.63 Checking the true class of the predicted label
  id: totrans-453
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.63 检查预测标签的真实类别
- en: '[PRE62]'
  id: totrans-454
  prefs: []
  type: TYPE_PRE
  zh: '[PRE62]'
- en: The forest has successfully identified the point at `X_test[0]`. Now, we will
    use voting to identify all points in the `X_test` validation set and utilize `y_test`
    to measure our prediction accuracy.
  id: totrans-455
  prefs: []
  type: TYPE_NORMAL
  zh: 森林已经成功识别了 `X_test[0]` 处的点。现在，我们将使用投票来识别 `X_test` 验证集中的所有点，并利用 `y_test` 来衡量我们的预测准确率。
- en: Listing 22.64 Measuring the accuracy of the forest model
  id: totrans-456
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 22.64 测量森林模型的准确率
- en: '[PRE63]'
  id: totrans-457
  prefs: []
  type: TYPE_PRE
  zh: '[PRE63]'
- en: 'Our randomly generated forest has predicted the validation outputs with 96%
    accuracy. It outperformed our single trained decision tree, whose accuracy hovered
    at 90%. By using the wisdom of the crowd, we have managed to improve performance.
    In the process, we have also trained a *random forest classifier* : a collection
    of trees whose training inputs are randomized to maximize diversity. Random forest
    classifiers are trained in the following manner:'
  id: totrans-458
  prefs: []
  type: TYPE_NORMAL
  zh: 我们随机生成的森林以 96% 的准确率预测了验证输出。它优于我们单独训练的决策树，其准确率徘徊在 90%。通过使用群众的智慧，我们已经提高了性能。在这个过程中，我们还训练了一个
    *随机森林分类器*：一组树的训练输入是随机化的，以最大化多样性。随机森林分类器以以下方式训练：
- en: Initialize *N* decision trees. The number of trees is a hyperparameter. Generally,
    more trees lead to higher accuracy, but using too many trees increasess the classifier’s
    running time.
  id: totrans-459
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 初始化 *N* 个决策树。树的数量是一个超参数。一般来说，更多的树会导致更高的准确率，但使用过多的树会增加分类器的运行时间。
- en: Generate *N* random training sets by sampling with replacement.
  id: totrans-460
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 通过替换抽样生成 *N* 个随机训练集。
- en: Choose `N ** 0.5` feature columns at random for each of our *N* training sets.
  id: totrans-461
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 为我们*N*个训练集中的每一个随机选择`N ** 0.5`个特征列。
- en: Train all the decision trees across the *N* random training sets.
  id: totrans-462
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在*N*个随机训练集上训练所有的决策树。
- en: After training, each tree in the forest casts a vote on how to label inputted
    data. These votes are tallied, and the class with the most votes is outputted
    by the classifier.
  id: totrans-463
  prefs: []
  type: TYPE_NORMAL
  zh: 训练后，森林中的每棵树都会对如何标记输入数据进行投票。这些投票被统计，并且具有最多投票的类别由分类器输出。
- en: Random forest classifiers are very versatile and are not prone to overfitting.
    Scikit-learn, of course, includes a random forest implementation.
  id: totrans-464
  prefs: []
  type: TYPE_NORMAL
  zh: 随机森林分类器非常灵活，并且不易过拟合。当然，scikit-learn包括随机森林的实现。
- en: 22.5 Training random forest classifiers using scikit-learn
  id: totrans-465
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 22.5 使用scikit-learn训练随机森林分类器
- en: In scikit-learn, random forest classification is carried out by the `RandomForestClassifier`
    class. Let’s import that class from `sklearn.ensemble`, initialize the class,
    and train it using `(X_train, y_train)`. Finally, we check the classifier’s performance
    using the validation set `(X_test, y_test)`.
  id: totrans-466
  prefs: []
  type: TYPE_NORMAL
  zh: 在scikit-learn中，随机森林分类是通过`RandomForestClassifier`类实现的。让我们从`sklearn.ensemble`导入该类，初始化该类，并使用`(X_train,
    y_train)`进行训练。最后，我们使用验证集`(X_test, y_test)`来检查分类器的性能。
- en: Listing 22.65 Training a random forest classifier
  id: totrans-467
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.65 训练一个随机森林分类器
- en: '[PRE64]'
  id: totrans-468
  prefs: []
  type: TYPE_PRE
  zh: '[PRE64]'
- en: ❶ This result is slightly higher than our earlier result of 96% due to random
    fluctuations as well as additional optimizations provided by scikit-learn.
  id: totrans-469
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 这个结果略高于我们之前96%的结果，这是由于随机波动以及scikit-learn提供的额外优化所导致的。
- en: By default, scikit-learn’s random forest classifier utilizes 100 decision trees.
    However, we can specify a lower or higher count using the `n_estimators` parameter.
    The following code reduces the number of trees to 10 by running `RandomForestClassifier
    (n_estimators=10)`. Then we recompute the accuracy.
  id: totrans-470
  prefs: []
  type: TYPE_NORMAL
  zh: 默认情况下，scikit-learn的随机森林分类器使用100个决策树。然而，我们可以使用`n_estimators`参数指定一个较低或较高的数量。以下代码通过运行`RandomForestClassifier
    (n_estimators=10)`将树的数量减少到10，然后我们重新计算准确率。
- en: Listing 22.66 Training a 10-tree random forest classifier
  id: totrans-471
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.66 训练一个10树随机森林分类器
- en: '[PRE65]'
  id: totrans-472
  prefs: []
  type: TYPE_PRE
  zh: '[PRE65]'
- en: Even with the lower tree count, the total accuracy remains very high. Sometimes,
    10 trees are more than sufficient to train a very accurate classifier.
  id: totrans-473
  prefs: []
  type: TYPE_NORMAL
  zh: 即使树的数量减少，总准确率仍然非常高。有时，10棵树就足以训练一个非常准确的分类器。
- en: Each of the 10 trees in `clf_forest` is assigned a random subset of five features.
    Every feature in the subset contains its own feature importance score. Scikit-learn
    can average all these scores across all the trees, and the aggregated averages
    can be accessed by calling `clf_forest.feature_importances_`. Let’s utilize the
    `feature_ importances_` attribute to print the top three features in the forest.
  id: totrans-474
  prefs: []
  type: TYPE_NORMAL
  zh: '`clf_forest`中的每个10棵树都被分配了一个包含五个特征的随机子集。子集中的每个特征都包含其自己的特征重要性分数。Scikit-learn可以平均所有这些分数，并且可以通过调用`clf_forest.feature_importances_`来访问聚合的平均值。让我们利用`feature_importances_`属性来打印森林中的前三个特征。'
- en: Listing 22.67 Ranking the random forest features
  id: totrans-475
  prefs: []
  type: TYPE_NORMAL
  zh: 列表22.67 对随机森林特征进行排名
- en: '[PRE66]'
  id: totrans-476
  prefs: []
  type: TYPE_PRE
  zh: '[PRE66]'
- en: The worst-radius feature continues to rank high, but its ranking is now on par
    with worst area and worst perimeter. Unlike our decision tree, the random forest
    does not over-rely on any individual inputted feature. This gives the random forest
    more flexibility in handling fluctuating signals in new data. The classifier’s
    versatile nature makes it a popular choice when training on medium-sized datasets.
  id: totrans-477
  prefs: []
  type: TYPE_NORMAL
  zh: 最坏半径特征继续排名很高，但其排名现在与最坏面积和最坏周长相当。与我们的决策树不同，随机森林不会过度依赖任何单个输入特征。这使随机森林在处理新数据中的波动信号时具有更大的灵活性。分类器的多功能性使其在训练中等规模数据集时成为流行的选择。
- en: Note Random forest classifiers function very well on multifeature datasets with
    hundreds or thousands of points. However, once the dataset size enters the millions,
    the algorithm can no longer scale. When processing exceedingly large datasets,
    more powerful deep learning techniques are required. None of the problems in this
    book fall in the scope of that requirement.
  id: totrans-478
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：随机森林分类器在具有数百或数千个点的多特征数据集上表现非常好。然而，一旦数据集大小达到数百万，该算法就无法扩展。在处理极其庞大的数据集时，需要更强大的深度学习技术。本书中的所有问题都不属于该要求范围。
- en: Relevant scikit-learn random forest classifier methods
  id: totrans-479
  prefs: []
  type: TYPE_NORMAL
  zh: 相关的scikit-learn随机森林分类器方法
- en: '`clf = RandomForestClassifier()`—Initializes a random forest classifier'
  id: totrans-480
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = RandomForestClassifier()`—初始化一个随机森林分类器'
- en: '`clf = RandomForestClassifier(n_estimators=x)`—Initializes a random forest
    classifier in which the number of trees is set to `x`'
  id: totrans-481
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf = RandomForestClassifier(n_estimators=x)`—初始化一个随机森林分类器，其中树的数量设置为`x`'
- en: '`clf.feature_importances_`—Accesses the feature importance scores of a trained
    random forest classifier'
  id: totrans-482
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '`clf.feature_importances_`—访问训练好的随机森林分类器的特征重要性分数'
- en: Summary
  id: totrans-483
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Certain classification problems can be handled with nested `if`/`else` statements
    but not with KNN or logistic regression classifiers.
  id: totrans-484
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 某些分类问题可以使用嵌套的`if`/`else`语句处理，但不能使用KNN或逻辑回归分类器。
- en: We can train a single-feature `if`/`else` model by maximizing accuracy across
    the co-occurrence counts between each feature state and each class label.
  id: totrans-485
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过最大化每个特征状态和每个类别标签之间的共现计数来训练一个单特征`if`/`else`模型。
- en: We can train a two-feature nested `if`/`else` model by doing a *binary split*
    on one of the features. Splitting on the feature returns two different training
    sets. Each training set is associated with a unique split-feature state. The training
    sets can be used to compute two single-feature models, and then we can combine
    the models into a nested `if`/`else` statement. The nested model’s accuracy is
    equal to the weighted mean of the simpler model accuracies.
  id: totrans-486
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过对一个特征进行**二分分割**来训练一个具有两个特征的嵌套`if`/`else`模型。对特征进行分割会返回两个不同的训练集。每个训练集都与一个独特的分割特征状态相关联。这些训练集可以用来计算两个单特征模型，然后我们可以将这些模型组合成一个嵌套的`if`/`else`语句。嵌套模型的准确率等于简单模型准确率的加权平均值。
- en: The choice of feature for the binary split can impact the quality of the model.
    Generally, the split leads to superior results if it generates imbalanced training
    data. A training set’s imbalance can be captured using its class-label distribution.
    More imbalanced training sets have a higher distribution vector magnitude. Hence,
    imbalanced datasets yield a higher value for `v @ v`, where `v` is the distribution
    vector. Additionally, the value `1 - v @ v` is referred to as the *Gini impurity*.
    Minimizing the Gini impurity minimizes the training set imbalance, so we should
    always split on the feature that yields the minimal Gini impurity.
  id: totrans-487
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 二分分割的特征选择可能会影响模型的质量。一般来说，如果分割生成了不平衡的训练数据，则分割会产生更好的结果。可以使用其类别标签分布来捕获训练集的不平衡。更不平衡的训练集具有更高的分布向量幅度。因此，不平衡的数据集在`v
    @ v`（其中`v`是分布向量）中产生更高的值。此外，值`1 - v @ v`被称为**基尼不纯度**。最小化基尼不纯度可以最小化训练集的不平衡，因此我们应该始终在产生最小基尼不纯度的特征上进行分割。
- en: We can extend two-feature model training to handle *N* features. We train an
    *N*-feature model by splitting on the feature with the minimum Gini impurity.
    Then we train two simpler models, each of which handles *N* – 1 features. The
    two simpler models are then combined into a more complex nested model whose accuracy
    is equal to the weighted mean of the simpler model accuracies.
  id: totrans-488
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以将两个特征的模型训练扩展到处理**N**个特征。我们通过在具有最小基尼不纯度的特征上进行分割来训练一个**N**特征模型。然后我们训练两个更简单的模型，每个模型处理**N**
    - 1个特征。这两个更简单的模型随后被组合成一个更复杂的嵌套模型，其准确率等于简单模型准确率的加权平均值。
- en: The branching `if`/`else` statements in the trained conditional models resemble
    the branches of a tree. We can make the resemblance more explicit by visualizing
    the output as a *decision tree diagram*. Decision trees are special network structures
    used to symbolize `if`/`else` decisions. Any nested `if`/`else` statement can
    be visualized as a decision tree, so trained `if`/`else` conditional classifiers
    are referred to as *decision tree classifiers*.
  id: totrans-489
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 训练的条件下模型的分支`if`/`else`语句类似于树的分支。我们可以通过将输出可视化为一个**决策树图**来使这种相似性更加明确。决策树是特殊的网络结构，用于表示`if`/`else`决策。任何嵌套的`if`/`else`语句都可以可视化为一个决策树，因此训练的`if`/`else`条件分类器被称为**决策树分类器**。
- en: Decision tree *depth* equals the number of nested `if`/`else` statements required
    to capture the logic in the tree. Limiting the depth can yield more interpretable
    diagrams.
  id: totrans-490
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 决策树的**深度**等于捕获树中逻辑所需的嵌套`if`/`else`语句的数量。限制深度可以产生更可解释的图表。
- en: The depth at which a feature appears in the tree is an indicator of its relative
    importance. That depth is determined by the Gini impurity, so the Gini impurity
    can be used to compute an importance score.
  id: totrans-491
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 在树中出现特征的深度是该特征相对重要性的指标。这个深度由基尼不纯度决定，因此基尼不纯度可以用来计算一个重要性分数。
- en: Overmemorization limits the usefulness of our trained models. In supervised
    machine learning, this phenomenon is referred to as *overfitting*. An overfitted
    model corresponds too closely to the training data, so it may fail to accurately
    predict on new observations. Decision tree classifiers are particularly prone
    to overfitting since they can memorize the training data.
  id: totrans-492
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 过度记忆限制了我们的训练模型的有用性。在监督机器学习中，这种现象被称为*过拟合*。过拟合的模型与训练数据过于接近，因此它可能无法准确预测新的观察结果。决策树分类器特别容易过拟合，因为它们可以记住训练数据。
- en: We can limit overfitting by training multiple decision trees in parallel. This
    collection of trees is called a *forest*. The collective wisdom of the forest
    can outperform an individual tree, but this requires us to introduce diversity
    to the forest. We can add diversity by generating random training sets with randomly
    chosen features. Then every tree in the forest is trained on a diverging training
    set. After training, each tree in the forest casts a vote on how to label inputted
    data. This voting-based ensemble model is referred to as a *random forest classifier*.
  id: totrans-493
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 我们可以通过并行训练多个决策树来限制过拟合。这些树的集合被称为*森林*。森林的集体智慧可以超越单个树，但这需要我们向森林中引入多样性。我们可以通过生成具有随机选择特征的随机训练集来增加多样性。然后森林中的每一棵树都在一个分叉的训练集上接受训练。训练完成后，森林中的每一棵树都会对如何标记输入数据进行投票。这种基于投票的集成模型被称为*随机森林分类器*。
