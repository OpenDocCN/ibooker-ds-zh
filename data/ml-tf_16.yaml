- en: 13 Reinforcement learning
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 13 强化学习
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖
- en: Defining reinforcement learning
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 定义强化学习
- en: Implementing reinforcement learning
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现强化学习
- en: Humans learn from experiences (or at least *should* ). You didn’t get so charming
    by accident. Years of positive compliments as well as negative criticism have
    all helped shape who you are today. This chapter is about designing a machine-learning
    system driven by criticisms and rewards.
  id: totrans-4
  prefs: []
  type: TYPE_NORMAL
  zh: 人类从经验中学习（或者至少*应该*）。你的迷人并不是偶然的。多年的正面赞美以及负面批评都帮助你塑造了今天的你。本章是关于设计一个由批评和奖励驱动的机器学习系统。
- en: You learn what makes people happy, for example, by interacting with friends,
    family members, or even strangers, and you figure out how to ride a bike by trying
    out various muscle movements until riding clicks. When you perform actions, you’re
    sometimes rewarded immediately. Finding a good restaurant nearby might yield instant
    gratification, for example. At other times, the reward doesn’t appear right away;
    you might have to travel a long distance to find an exceptional place to eat.
    Reinforcement learning is about choosing the right actions, given any state—such
    as in figure 13.1, which shows a person making decisions to arrive at their destination.
  id: totrans-5
  prefs: []
  type: TYPE_NORMAL
  zh: 你通过与他人、家人或甚至陌生人的互动来学习什么让人们快乐，例如，通过尝试各种肌肉运动直到骑行变得顺畅，你就能学会如何骑自行车。当你执行动作时，你有时会立即得到奖励。例如，找到附近的美食可能会带来即时的满足感。有时，奖励不会立即出现；你可能需要长途跋涉才能找到一个出色的用餐地点。强化学习是关于在任何状态下选择正确的动作——例如，在图13.1中，它显示了一个人在交通和意外情况下导航以到达目的地。
- en: '![CH13_F01_Mattmann2](../Images/CH13_F01_Mattmann2.png)'
  id: totrans-6
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F01_Mattmann2](../Images/CH13_F01_Mattmann2.png)'
- en: Figure 13.1 A person navigating to reach a destination in the midst of traffic
    and unexpected situations is a problem setup for reinforcement learning.
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.1 在交通和意外情况下导航以到达目的地的人是一个强化学习的问题设置。
- en: Moreover, suppose that on your drive from home to work, you always choose the
    same route. But one day, your curiosity takes over, and you decide to try a different
    path in the hope of shortening your commute. This dilemma—trying new routes or
    sticking to the best-known route—is an example of *exploration versus exploitation**.*
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 此外，假设你在从家到工作的路上，你总是选择同一条路线。但有一天，你的好奇心占了上风，你决定尝试一条不同的路线，希望缩短通勤时间。这种困境——尝试新路线或坚持已知的最佳路线——是*探索与利用*的一个例子。
- en: Note Why is the trade-off between trying new things and sticking with old ones
    called *exploration versus exploitation*? Exploration makes sense, but you can
    think of exploitation as exploiting your knowledge of the status quo by sticking
    with what you know.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 注意 为什么尝试新事物和坚持旧事物之间的权衡被称为*探索与利用*？探索是有意义的，但你可以将利用视为通过坚持你所知道的东西来利用你对现状的了解。
- en: 'All these examples can be unified under a general formulation: performing an
    action in a scenario can yield a reward. A more technical term for scenario is
    *state*. And we call the collection of all possible states a *state space*. Performing
    an action causes the state to change. This shouldn’t be too unfamiliar to you
    if you remember chapters 9 and 10, which deal with hidden Markov models (HMMs).
    You transitioned from state to state in those chapters based on observations.
    But what series of actions yields the highest expected rewards?'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 所有这些例子都可以统一在一个通用公式下：在某个场景中执行一个动作可以产生一个奖励。对于场景的更技术性的术语是*状态*。我们称所有可能状态的集合为*状态空间*。执行一个动作会导致状态发生变化。如果你还记得第9章和第10章，它们讨论了隐马尔可夫模型（HMMs），那么这对你来说不应该太陌生。你根据观察从状态过渡到状态。但哪一系列动作会产生最高的预期奖励呢？
- en: 13.1 Formal notions
  id: totrans-11
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.1 形式概念
- en: Whereas supervised and unsupervised learning appear at opposite ends of the
    spectrum, reinforcement learning (RL) exists somewhere in the middle. It’s not
    supervised learning, because the training data comes from the algorithm deciding
    between exploration and exploitation. And it’s not unsupervised, because the algorithm
    receives feedback from the environment. As long as you’re in a situation in which
    performing an action in a state produces a reward, you can use reinforcement learning
    to discover a good sequence of actions to take to maximize expected rewards.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 与监督学习和无监督学习出现在光谱的两端不同，强化学习（RL）存在于中间某个位置。它不是监督学习，因为训练数据来自算法在探索和利用之间做出决定。它也不是无监督学习，因为算法从环境中接收反馈。只要你在执行动作在某个状态下产生奖励的情况下，你就可以使用强化学习来发现一个好的动作序列，以最大化预期奖励。
- en: You may notice that reinforcement-learning lingo involves anthropomorphizing
    the algorithm into taking *actions* in *situations* to *receive rewards*. The
    algorithm is often referred to as an *agent* that acts with the environment. It
    shouldn’t be a surprise that much of reinforcement-learning theory is applied
    in robotics. Figure 13.2 demonstrates the interplay among states, actions, and
    rewards.
  id: totrans-13
  prefs: []
  type: TYPE_NORMAL
  zh: 你可能会注意到，强化学习术语涉及将算法拟人化为在*情境*中采取*行动*以*获得奖励*。该算法通常被称为*代理*，它在环境中采取行动。在机器人学中应用大量的强化学习理论并不令人惊讶。图
    13.2 展示了状态、行动和奖励之间的相互作用。
- en: '![CH13_F02_Mattmann2](../Images/CH13_F02_Mattmann2.png)'
  id: totrans-14
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F02_Mattmann2](../Images/CH13_F02_Mattmann2.png)'
- en: Figure 13.2 Actions are represented by arrows, and states are represented by
    circles. Performing an action on a state produces a reward. If you start at state
    s1, you can perform action a1 to obtain a reward r(s1, a1).
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.2 行动由箭头表示，状态由圆圈表示。对状态执行行动会产生奖励。如果你从状态 s1 开始，你可以执行行动 a1 来获得奖励 r(s1, a1)。
- en: Do humans use reinforcement learning?
  id: totrans-16
  prefs: []
  type: TYPE_NORMAL
  zh: 人类使用强化学习吗？
- en: Reinforcement learning seems to be the best way to explain how to perform the
    next action based on the current situation. Perhaps humans behave the same way
    biologically. But let’s not get ahead of ourselves; consider the following example.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习似乎是最好的方式来解释如何根据当前情况执行下一步行动。也许人类在生物学上也是以同样的方式行事。但让我们不要过于乐观；考虑以下例子。
- en: Sometimes, humans act without thinking. If I’m thirsty, I might instinctively
    grab a cup of water to quench my thirst. I don’t iterate through all possible
    joint motions in my head and choose the optimal one after thorough calculations.
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 有时候，人类会无意识地采取行动。如果我渴了，我可能会本能地抓起一杯水来解渴。我不会在脑海中迭代所有可能的联合动作，并在彻底计算后选择最优的一个。
- en: Most important, the actions we make aren’t characterized solely by our observations
    at each moment. Otherwise, we’re no smarter than bacteria, which act deterministically
    given their environment. There seems to be a lot more going on, and a simple RL
    model might not explain human behavior fully.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 最重要的是，我们采取的行动并不仅仅由我们每个时刻的观察所决定。否则，我们就不比细菌更聪明，因为细菌会根据其环境确定性地采取行动。似乎还有更多的事情在进行中，一个简单的强化学习模型可能无法完全解释人类的行为。
- en: A robot performs actions to change states. But how does it decide which action
    to take? Section 13.1.1 introduces a new concept to answer this question.
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: 机器人通过执行行动来改变状态。但它如何决定采取哪种行动？第 13.1.1 节介绍了一个新概念来回答这个问题。
- en: 13.1.1 Policy
  id: totrans-21
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.1 策略
- en: 'Everyone cleans their room differently. Some people start by making their bed.
    I prefer cleaning my room clockwise so I don’t miss a corner. Have you ever seen
    a robotic vacuum cleaner, such as a Roomba? Someone programmed a strategy the
    robot can follow to clean any room. In reinforcement-learning lingo, the way that
    an agent decides which action to take is a *policy*: the set of actions that determines
    the next state (figure 13.3).'
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 每个人打扫房间的习惯都不同。有些人从整理床铺开始。我更喜欢顺时针打扫房间，这样就不会错过任何一个角落。你见过机器人吸尘器，比如 Roomba 吗？有人编写了一个机器人可以遵循的策略来清洁任何房间。在强化学习术语中，代理决定采取哪种行动的方式是一个*策略*：决定下一个状态的行动集合（图
    13.3）。
- en: '![CH13_F03_Mattmann2](../Images/CH13_F03_Mattmann2.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F03_Mattmann2](../Images/CH13_F03_Mattmann2.png)'
- en: Figure 13.3 A policy suggests which action to take, given a state.
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.3 一项策略建议在给定状态下采取哪种行动。
- en: The goal of reinforcement learning is to discover a good policy. A common way
    to create that policy is to observe the long-term consequences of actions at each
    state. The *reward* is the measure of the outcome of taking an action. The best
    possible policy is called the *optimal policy*, which is the Holy Grail of reinforcement
    learning. The optimal policy tells you the optimal action, given any state—but
    as in real life, it may not provide the highest reward at the moment.
  id: totrans-25
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习的目标是发现一个好的策略。创建该策略的一种常见方式是观察每个状态下行动的长期后果。*奖励*是采取行动结果的衡量标准。最佳可能的策略被称为*最优策略*，这是强化学习的圣杯。最优策略告诉你给定任何状态下的最佳行动——但就像现实生活中一样，它可能不会在当下提供最高的奖励。
- en: If you measure the reward by looking at the immediate consequence—the state
    of things after taking the action—it’s easy to calculate. This strategy is called
    the *greedy strategy*. But it’s not always a good idea to “greedily” choose the
    action that provides the best immediate reward. When you’re cleaning your room,
    for example, you might make your bed first, because the room looks neater with
    the bed made. But if another goal is to wash your sheets, making the bed first
    may not be the best overall strategy. You need to look at the results of the next
    few actions and the eventual end state to come up with the optimal approach. Similarly,
    in chess, grabbing your opponent’s queen may maximize the points for the pieces
    on the board—but if it puts you in checkmate five moves later, it isn’t the best
    possible move.
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你通过观察采取动作后的直接后果——即采取动作后的状态——来衡量奖励，那么计算起来很容易。这种策略被称为*贪婪策略*。但“贪婪地”选择提供最佳即时奖励的动作并不总是好主意。例如，当你打扫房间时，你可能会先整理床铺，因为床铺整理好了房间看起来更整洁。但如果你还有其他目标，比如洗床单，那么先整理床铺可能不是最佳的整体策略。你需要考虑接下来几个动作的结果以及最终的状态，才能得出最佳的方法。同样，在棋类游戏中，抓住对手的皇后可能会最大化棋盘上棋子的分数——但如果这让你在五步之后被将军，那么这并不是最佳的可能走法。
- en: Limitations of (Markovian) reinforcement learning
  id: totrans-27
  prefs: []
  type: TYPE_NORMAL
  zh: (马尔可夫) 强化学习的局限性
- en: Most RL formulations assume that you can figure out the best action to take
    from knowing the current state, instead of considering the longer-term history
    of states and actions that got you there. This approach to making decisions is
    called Markovian, and the general framework is often referred to as the Markov
    decision process (MDP). I hinted at this earlier.
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数 RL 公式假设你可以从知道当前状态中找出最佳动作，而不是考虑导致你到达该状态的更长期的状态和动作历史。这种决策方法被称为马尔可夫，而通用框架通常被称为马尔可夫决策过程（MDP）。我之前已经暗示过这一点。
- en: Situations in which the state sufficiently captures what to do next can be modeled
    with the RL algorithms discussed in this chapter. But most real-world situations
    aren’t Markovian and therefore need a more realistic approach, such as a hierarchical
    representation of states and actions. In a grossly oversimplified sense, hierarchical
    models are like context-free grammars, whereas MDPs are like finite-state machines.
    The expressive leap of modeling a problem as an MDP to something more hierarchical
    can dramatically improve the effectiveness of the planning algorithm.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 当状态足够捕捉到下一步要做什么时，可以使用本章讨论的 RL 算法来建模这些情况。但大多数现实世界的情况都不是马尔可夫的，因此需要更现实的方法，例如状态和动作的分层表示。在极其简化的意义上，分层模型类似于上下文无关文法，而
    MDPs 则类似于有限状态机。将问题建模为 MDP 而不是更分层的模型，这种建模的飞跃可以显著提高规划算法的有效性。
- en: You can also choose an action arbitrarily, which is a *random policy*. If you
    come up with a policy to solve a reinforcement-learning problem, it’s often a
    good idea to double-check that your learned policy performs better than both the
    random and greedy policies, which are often called a baseline.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 你也可以任意选择一个动作，这被称为*随机策略*。如果你提出了一种策略来解决强化学习问题，通常一个好的做法是检查你学习到的策略是否比随机和贪婪策略表现更好，后者通常被称为基线。
- en: 13.1.2 Utility
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 13.1.2 效用
- en: The long-term reward is called a *utility*. If you know the utility of performing
    an action at a state, learning the policy is easy with reinforcement learning.
    To decide which action to take, you select the action that produces the highest
    utility. The hard part, as you might have guessed, is uncovering these utility
    values.
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: 长期奖励被称为*效用*。如果你知道在某个状态下执行动作的效用，使用强化学习来学习策略就很容易。为了决定采取哪个动作，你选择产生最高效用的动作。正如你可能猜到的，困难的部分是揭示这些效用值。
- en: The utility of performing an action (*a*) at a state (*s*) is written as a function
    *Q*(*s*, *a*), called the *utility function*, shown in figure 13.4.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 在某个状态(*s*)下执行动作(*a*)的效用被表示为函数*Q*(*s*, *a*)，称为*效用函数*，如图 13.4 所示。
- en: '![CH13_F04_Mattmann2](../Images/CH13_F04_Mattmann2.png)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F04_Mattmann2](../Images/CH13_F04_Mattmann2.png)'
- en: 'Figure 13.4 Given a state and the action taken, applying a utility function
    Q predicts the expected and the total rewards: the immediate reward (next state)
    plus rewards gained later by following an optimal policy.'
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.4 给定一个状态和采取的动作，应用效用函数 Q 预测预期的和总奖励：即时奖励（下一个状态）加上随后通过遵循最佳策略获得的奖励。
- en: Exercise 13.1
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 13.1
- en: If you were given the utility function Q(s, a), how could you use it to derive
    a policy function?
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你被给出了效用函数 Q(s, a)，你如何使用它来推导策略函数？
- en: '**Answer**'
  id: totrans-38
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Policy(s) = argmax_a Q(s, a)
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 政策(s) = argmax_a Q(s, a)
- en: 'An elegant way to calculate the utility of a particular state-action pair (*s*,
    *a*) is to recursively consider the utilities of future actions. The utility of
    your current action is influenced not only by the immediate reward, but also by
    the next best action, as shown in the following formula. In the formula, *s*''
    is the next state, and *a*'' denotes the next action. The reward of taking action
    *a* in state *s* is denoted by *r* (*s*, *a*):'
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 计算特定状态-行动对(*s*, *a*)的效用的一种优雅方法是递归地考虑未来行动的效用。你当前行动的效用不仅受即时奖励的影响，还受下一个最佳行动的影响，如下公式所示。在公式中，*s*'是下一个状态，*a*'表示下一个行动。在状态*s*中采取行动*a*的奖励用*r*(*s*,
    *a*)表示：
- en: '*Q*(*s, a* ) = *r* (*s, a*) + γmax *Q*(*s'', a''*)'
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*(*s, a*) = *r* (*s, a*) + γmax *Q*(*s'', a''*)'
- en: Here, γ is a hyperparameter that you get to choose, called the *discount factor*.
    If γ is 0, the agent chooses the action that maximizes the immediate reward. Higher
    values of γ will make the agent put more importance on considering long-term consequences.
    You can read the formula as “the value of this action is the immediate reward
    provided by taking this action, added to the discount factor times the best thing
    that can happen after that.”
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，γ是一个你可以选择的超参数，称为*折现因子*。如果γ为0，代理会选择最大化即时奖励的行动。γ的更高值会使代理更加重视考虑长期后果。你可以将公式读作“这个行动的价值是采取这个行动提供的即时奖励，加上折现因子乘以之后可能发生的最好事情。”
- en: 'Looking to future rewards is one type of hyperparameter you can play with,
    but there’s another. In some applications of reinforcement learning, newly available
    information might be more important than historical records, or vice versa. If
    a robot is expected to learn to solve tasks quickly but not necessarily optimally,
    you might want to set a faster learning rate. Or if a robot is allowed more time
    to explore and exploit, you might tune down the learning rate. Let’s call the
    learning rate α, and change the utility function as follows (noting that when
    α = 1, the equations are identical):'
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 寻求未来奖励是你可以调整的一种超参数，但还有另一个。在一些强化学习应用中，新可获得的信息可能比历史记录更重要，反之亦然。如果期望机器人快速学习解决任务但不一定是最佳方案，你可能想要设置一个更快的学习率。或者如果允许机器人有更多时间探索和利用，你可能降低学习率。让我们称学习率为α，并按以下方式更改效用函数（注意当α
    = 1时，方程式是相同的）：
- en: '*Q*(*s, a*) ← *Q*(*s, a*) + α ( *r*(*s, a*) + γmax *Q*(*s'', a''*) – *Q*(*s,
    a*))'
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: '*Q*(*s, a*) ← *Q*(*s, a*) + α (*r*(*s, a*) + γmax *Q*(*s'', a''*) – *Q*(*s,
    a*))'
- en: 'Reinforcement learning can be solved if you know the Q-function: *Q*(*s*, *a*).
    Conveniently for us, *neural networks* (chapters 11 and 12) approximate functions,
    given enough training data. TensorFlow is the perfect tool to deal with neural
    networks because it comes with many essential algorithms that simplify neural-network
    implementation.'
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你知道Q函数：*Q*(*s*, *a*)，强化学习就可以解决。对我们来说，*神经网络*（第11章和第12章）在足够训练数据的情况下可以近似函数。TensorFlow是处理神经网络的完美工具，因为它附带了许多简化神经网络实现的必要算法。
- en: 13.2 Applying reinforcement learning
  id: totrans-46
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.2 应用强化学习
- en: Application of reinforcement learning requires defining a way to retrieve rewards
    after an action is taken from a state. A stock-market trader fits these requirements
    easily, because buying and selling a stock changes the state of the trader (cash
    on hand), and each action generates a reward (or loss).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 应用强化学习需要定义一种从状态执行行动后检索奖励的方法。股票市场交易者很容易满足这些要求，因为买卖股票会改变交易者的状态（现金在手），并且每个行动都会产生奖励（或损失）。
- en: Exercise 13.2
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 练习13.2
- en: What are some possible disadvantages of using reinforcement learning for buying
    and selling stocks?
  id: totrans-49
  prefs: []
  type: TYPE_NORMAL
  zh: 使用强化学习进行买卖股票可能有哪些可能的缺点？
- en: '**Answer**'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: By performing actions in the market, such as buying or selling shares, you could
    end up influencing the market, causing it to change dramatically from your training
    data.
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: 通过在市场上执行行动，如买卖股票，你可能会影响市场，导致它从你的训练数据中发生剧烈变化。
- en: The states in this situation are a vector containing information about the current
    budget, the current number of stocks, and a recent history of stock prices (the
    last 200 stock prices). Each state is a 202-dimensional vector.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，状态是一个包含当前预算、当前股票数量和最近股票价格历史（最后200个股票价格）信息的向量。每个状态是一个202维向量。
- en: 'For simplicity, the only three actions are buy, sell, and hold:'
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 为了简单起见，只有三种行动：买入、卖出和持有：
- en: Buying a stock at the current stock price decreases the budget while incrementing
    the current stock count.
  id: totrans-54
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以当前股价买入股票会减少预算，同时增加当前股票数量。
- en: Selling a stock trades it in for money at the current share price.
  id: totrans-55
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 以当前股价卖出股票，将其换成现金。
- en: Holding does neither thing. This action waits a single time period and yields
    no reward.
  id: totrans-56
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 持有既不增加也不减少预算，这个动作等待一个时间周期，不产生任何奖励。
- en: Figure 13.5 demonstrates one possible policy, given stock market data.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 展示了一种基于股票市场数据的可能策略。
- en: '![CH13_F05_Mattmann2](../Images/CH13_F05_Mattmann2.png)'
  id: totrans-58
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F05_Mattmann2](../Images/CH13_F05_Mattmann2.png)'
- en: Figure 13.5 Ideally, our algorithm should buy low and sell high. Doing so once,
    as shown here, might yield a reward of around $160\. But the real profit rolls
    in when you buy and sell more frequently. Have you ever heard the term high-frequency
    trading? This type of trading involves buying low and selling high as frequently
    as possible to maximize profits within a given period.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.5 理想情况下，我们的算法应该低位买入，高位卖出。像这里展示的那样做一次，可能会获得大约 160 美元的回报。但真正的利润来自于你更频繁地买卖。你听说过高频交易这个术语吗？这种交易涉及尽可能频繁地低位买入，高位卖出，以在给定期间内最大化利润。
- en: The goal is to learn a policy that gains the maximum net worth from trading
    in a stock market. Wouldn’t that be cool? Let’s do it!
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 目标是学习一种策略，从股票市场的交易中获得最大净收益。这难道不是一件很酷的事情吗？让我们试试吧！
- en: 13.3 Implementing reinforcement learning
  id: totrans-61
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.3 实现强化学习
- en: 'To gather stock prices, you’ll use the `ystockquote` library in Python. You
    can install it by using `pip` or follow the official guide ([https://github.com/cgoldberg/ystockquote](https://github.com/cgoldberg/ystockquote)).
    The command to install it with `pip` is as follows:'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 为了收集股票价格，你将使用 Python 中的 `ystockquote` 库。你可以通过使用 `pip` 或遵循官方指南（[https://github.com/cgoldberg/ystockquote](https://github.com/cgoldberg/ystockquote)）来安装它。使用
    `pip` 安装它的命令如下：
- en: '[PRE0]'
  id: totrans-63
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: With that library installed, let’s import all the relevant libraries (listing
    13.1).
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 安装了该库后，让我们导入所有相关的库（列表 13.1）。
- en: Listing 13.1 Importing relevant libraries
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.1 导入相关库
- en: '[PRE1]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ For obtaining stock-price raw data
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 用于获取股票价格原始数据
- en: ❷ For plotting stock prices
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 用于绘制股票价格
- en: ❸ For numeric manipulation and machine learning
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 用于数值操作和机器学习
- en: 'Create a helper function to get stock prices by using the `ystockquote` library.
    The library requires three pieces of information: share symbol, start date, and
    end date. When you pick each of the three values, you’ll get a list of numbers
    representing the share prices in that period by day.'
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 创建一个辅助函数，使用 `ystockquote` 库获取股票价格。该库需要三块信息：股票符号、起始日期和结束日期。当你选择这三个值时，你会得到一个代表该期间每日股票价格的数字列表。
- en: If you choose start and end dates that are too far apart, fetching that data
    will take some time. It might be a good idea to save (cache) the data to disk
    so that you can load it locally next time. Listing 13.2 shows to use the library
    and cache the data.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你选择的起始日期和结束日期相隔太远，获取这些数据将花费一些时间。将数据保存到磁盘上以便下次本地加载可能是个好主意。列表 13.2 展示了如何使用库和缓存数据。
- en: Listing 13.2 Helper function to get prices
  id: totrans-72
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.2 获取价格的帮助函数
- en: '[PRE2]'
  id: totrans-73
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: ❶ Tries to load the data from file if it has already been computed
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 尝试从文件中加载数据，如果它已经被计算过
- en: ❷ Retrieves stock prices from the library
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 从库中检索股票价格
- en: ❸ Extracts only relevant info from the raw data
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从原始数据中提取相关信息
- en: ❹ Caches the result
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 缓存结果
- en: As a sanity check, it’s a good idea to visualize the stock-price data. Create
    a plot, and save it to disk (listing 13.3).
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 为了进行合理性检查，可视化股票价格数据是个好主意。创建一个图表，并将其保存到磁盘上（列表 13.3）。
- en: Listing 13.3 Helper function to plot the stock prices
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.3 绘制股票价格的辅助函数
- en: '[PRE3]'
  id: totrans-80
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: You can grab some data and visualize it by using listing 13.4.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以使用列表 13.4 获取一些数据并将其可视化。
- en: Listing 13.4 Getting data and visualizing it
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.4 获取数据和可视化
- en: '[PRE4]'
  id: totrans-83
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Figure 13.6 shows the chart produced by running listing 13.4.
  id: totrans-84
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 展示了运行列表 13.4 生成的图表。
- en: '![CH13_F06_Mattmann2](../Images/CH13_F06_Mattmann2.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F06_Mattmann2](../Images/CH13_F06_Mattmann2.png)'
- en: Figure 13.6 This chart summarizes the opening stock prices of Microsoft (MSFT)
    from July 22, 1992, to July 22, 2016\. Wouldn’t it have been nice to buy around
    day 3,000 and sell around day 5,000? Let’s see whether our code can learn to buy,
    sell, and hold to make optimal gain.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.6 该图表总结了从 1992 年 7 月 22 日到 2016 年 7 月 22 日微软（MSFT）的开盘价。如果能在第 3,000 天左右买入，在第
    5,000 天左右卖出，那岂不是很好？让我们看看我们的代码是否能够学会买入、卖出和持有以实现最佳收益。
- en: 'Most reinforcement-learning algorithms follow similar implementation patterns.
    As a result, it’s a good idea to create a class with the relevant methods to reference
    later, such as an abstract class or interface. See listing 13.5 for an example
    and figure 13.7 for an illustration. Reinforcement learning needs two well-defined
    operations: how to select an action and how to improve the utility Q-function.'
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数强化学习算法遵循类似的实现模式。因此，创建一个具有相关方法以供以后参考的类是一个好主意，例如一个抽象类或接口。请参阅列表 13.5 中的示例和图
    13.7 中的说明。强化学习需要两个明确定义的操作：如何选择动作以及如何改进效用 Q 函数。
- en: '![CH13_F07_Mattmann2](../Images/CH13_F07_Mattmann2.png)'
  id: totrans-88
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F07_Mattmann2](../Images/CH13_F07_Mattmann2.png)'
- en: 'Figure 13.7 Most reinforcement-learning algorithms boil down to three main
    steps: infer, do, and learn. During the first step, the algorithm selects the
    best action (a), given a state (s), using the knowledge it has so far. Next, it
    does the action to find out the reward (r) as well as the next state (s''). Then
    it improves its understanding of the world by using the newly acquired knowledge
    (s, r, a, s'').'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.7 大多数强化学习算法归结为三个主要步骤：推断、执行和学习。在第一步中，算法根据到目前为止的知识，在给定状态（s）的情况下选择最佳动作（a）。接下来，它执行动作以找出奖励（r）以及下一个状态（s'）。然后它通过使用新获得的知识（s,
    r, a, s'）来提高对世界的理解。
- en: Listing 13.5 Defining a superclass for all decision policies
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.5 定义所有决策策略的超类
- en: '[PRE5]'
  id: totrans-91
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: ❶ Given a state, the decision policy will calculate the next action to take.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 给定一个状态，决策策略将计算下一步要采取的动作。
- en: ❷ Improves the Q-function from a new experience of taking an action
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 通过采取动作的新经验改进 Q 函数
- en: Next, let’s inherit from this superclass to implement a policy in which decisions
    are made at random, otherwise known as a *random decision policy*. You need to
    define only the `select_action` method, which randomly picks an action without
    even looking at the state. Listing 13.6 shows how to implement it.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，让我们从这个超类继承以实现一个在随机决策的策略，也称为随机决策策略。你需要定义的只有 `select_action` 方法，该方法随机选择一个动作，甚至不查看状态。列表
    13.6 展示了如何实现它。
- en: Listing 13.6 Implementing a random decision policy
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.6 实现随机决策策略
- en: '[PRE6]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: ❶ Inherits from DecisionPolicy to implement its functions
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 DecisionPolicy 继承以实现其函数
- en: ❷ Randomly chooses the next action
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 随机选择下一个动作
- en: In listing 13.7, you assume that a policy is given to you (such as the one from
    listing 13.6) and run it on the real-world stock-price data. This function takes
    care of exploration and exploitation at each interval of time. Figure 13.8 illustrates
    the algorithm from listing 13.7.
  id: totrans-99
  prefs: []
  type: TYPE_NORMAL
  zh: 在列表 13.7 中，你假设有一个策略被提供给你（例如来自列表 13.6 的策略）并在现实世界的股票价格数据上运行它。这个函数负责在每个时间间隔处理探索和利用。图
    13.8 展示了列表 13.7 中的算法。
- en: '![CH13_F08_Mattmann2](../Images/CH13_F08_Mattmann2.png)'
  id: totrans-100
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F08_Mattmann2](../Images/CH13_F08_Mattmann2.png)'
- en: 'Figure 13.8 A rolling window of a certain size iterates through the stock prices,
    as shown by the chart segmented into states S1, S2, and S3\. The policy suggests
    an action to take: you may choose to exploit it or randomly explore another action.
    As you get rewards for performing an action, you can update the policy function
    over time.'
  id: totrans-101
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.8 某个大小的滚动窗口迭代通过股票价格，如图所示，分为状态 S1、S2 和 S3。策略建议采取的行动：你可以选择利用它或随机探索另一个行动。随着你为执行动作获得奖励，你可以随着时间的推移更新策略函数。
- en: Listing 13.7 Using a given policy to make decisions and returning the performance
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.7 使用给定的策略进行决策并返回性能
- en: '[PRE7]'
  id: totrans-103
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: ❶ Initializes values that depend on computing the net worth of a portfolio
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化依赖于计算投资组合净值的值
- en: ❷ The state is a hist + 2D vector. You’ll force it to be a NumPy matrix.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 状态是一个 hist + 2D 向量。你将强制它成为一个 NumPy 矩阵。
- en: ❸ Calculates the portfolio value
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算投资组合价值
- en: ❹ Selects an action from the current policy
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 从当前策略中选择一个动作
- en: ❺ Updates portfolio values based on action
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 根据动作更新投资组合价值
- en: ❻ Computes a new portfolio value after taking action
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 执行动作后计算新的投资组合价值
- en: ❼ Computes the reward from taking an action at a state
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 计算在状态采取动作的奖励
- en: ❽ Updates the policy after experiencing a new action
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 在经历新的动作后更新策略
- en: ❾ Computes the final portfolio worth
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 计算最终的投资组合价值
- en: To obtain a more robust measurement of success, run the simulation a couple
    of times and average the results (listing 13.8). Doing so may take a while (perhaps
    5 minutes), but your results will be more reliable.
  id: totrans-113
  prefs: []
  type: TYPE_NORMAL
  zh: 为了获得更稳健的成功测量，运行模拟几次并平均结果（列表 13.8）。这样做可能需要一段时间（可能需要 5 分钟），但你的结果将更可靠。
- en: Listing 13.8 Running multiple simulations to calculate average performance
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.8 运行多次模拟以计算平均性能
- en: '[PRE8]'
  id: totrans-115
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: ❶ Decides the number of times to rerun the simulations
  id: totrans-116
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 决定重新运行模拟的次数
- en: ❷ Stores the portfolio worth of each run in this array
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每次运行的组合价值存储在这个数组中
- en: ❸ Runs this simulation
  id: totrans-118
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 运行此模拟
- en: In the `main` function, append the lines in listing 13.9 to define the decision
    policy, then run simulations to see how the policy performs.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 在 `main` 函数中，将列表 13.9 中的行添加到定义决策策略，然后运行模拟以查看策略的表现。
- en: Listing 13.9 Defining the decision policy
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.9 定义决策策略
- en: '[PRE9]'
  id: totrans-121
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Defines the list of actions the agent can take
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 定义代理可以采取的动作列表
- en: ❷ Initializes a random decision policy
  id: totrans-123
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 初始化一个随机决策策略
- en: ❸ Sets the initial amount of money available to use
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 设置可用于使用的初始金额
- en: ❹ Sets the number of stocks already owned
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设置已拥有的股票数量
- en: ❺ Runs simulations multiple times to compute the expected value of your final
    net worth
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 多次运行模拟以计算最终净值的期望值
- en: Now that you have a baseline to compare your results, let’s implement a neural
    network approach to learn the Q-function. The decision policy is often called
    the *Q-learning decision policy*. Listing 13.10 introduces a new hyperparameter,
    `epsilon`, to keep the solution from getting “stuck” when applying the same action
    over and over. The lower the value of `epsilon`, the more often it will randomly
    explore new actions. The Q-function is defined by the function depicted in figure
    13.9.
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: 现在你有了比较结果的基准，让我们实现一个神经网络方法来学习 Q 函数。决策策略通常被称为 *Q-learning 决策策略*。列表 13.10 引入了一个新的超参数
    `epsilon`，以防止在反复应用相同动作时解决方案“卡住”。`epsilon` 的值越低，随机探索新动作的频率就越高。Q 函数由图 13.9 所示的函数定义。
- en: '![CH13_F09_Mattmann2](../Images/CH13_F09_Mattmann2.png)'
  id: totrans-128
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F09_Mattmann2](../Images/CH13_F09_Mattmann2.png)'
- en: 'Figure 13.9 input is the state space vector, with three outputs: one for each
    output’s Q-value'
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: 图 13.9 的输入是状态空间向量，有三个输出：每个输出的 Q 值一个
- en: Exercise 13.3
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 练习 13.3
- en: What other possible factors that your state-space representation ignores can
    affect the stock prices? How could you factor them into the simulation?
  id: totrans-131
  prefs: []
  type: TYPE_NORMAL
  zh: 你状态空间表示中忽略的其他可能影响股票价格的因素有哪些？你如何将它们纳入模拟？
- en: '**Answer**'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: '**答案**'
- en: Stock prices depend on a variety of factors, including general market trends,
    breaking news, and specific industry trends. Each of these factors, once quantified,
    could be applied to the model as an additional dimension.
  id: totrans-133
  prefs: []
  type: TYPE_NORMAL
  zh: 股票价格取决于多种因素，包括整体市场趋势、突发新闻和特定行业趋势。一旦量化，这些因素中的每一个都可以作为额外的维度应用于模型。
- en: Listing 13.10 Implementing a more intelligent decision policy
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 13.10 实现更智能的决策策略
- en: '[PRE10]'
  id: totrans-135
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: ❶ Sets the hyperparameters from the Q-function
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 从 Q 函数设置超参数
- en: ❷ Sets the number of hidden nodes in the neural networks
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 设置神经网络中的隐藏节点数量
- en: ❸ Defines the input and output tensors
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 定义输入和输出张量
- en: ❹ Designs the neural network architecture
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 设计神经网络架构
- en: ❺ Defines the op to compute the utility
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 定义计算效用的操作
- en: ❻ Sets the loss as the square error
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 将损失设置为平方误差
- en: ❼ Uses an optimizer to update model parameters to minimize the loss
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: ❼ 使用优化器更新模型参数以最小化损失
- en: ❽ Sets up the session and initializes variables
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: ❽ 设置会话并初始化变量
- en: ❾ Exploits the best option with probability epsilon
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: ❾ 以概率 epsilon 利用最佳选项
- en: ❿ Explores a random option with probability 1 - epsilon
  id: totrans-145
  prefs: []
  type: TYPE_NORMAL
  zh: ❿ 以概率 1 - epsilon 探索随机选项
- en: ⓫ Updates the Q-function by updating its model parameters
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: ⓫ 通过更新其模型参数来更新 Q 函数
- en: 'The output from the entire script is shown in figure 13.10\. Two key functions
    are part of the `QLearningDecisionPolicy`: `update_q` and `select_action`, which
    implement the learned value of actions over time. Five percent of the time, the
    functions yield a random action. In `select_action`, every 1,000 or so prices,
    the function forces a random action and exploration as defined by `self.epsilon`.
    In `update_q`, the agent takes the current state and next desired action as defined
    by the `argmax` of `q` policy value for those states. Because the algorithm is
    initialized with a Q-function and weights with `tf.random_normal`, it will take
    some actions over time for the agent to start to realize the true longer-term
    reward, but then again, that realization is the point. The agent starts with a
    random understanding, takes actions, and learns the optimal policy over the simulation.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 整个脚本的输出显示在图13.10中。`QLearningDecisionPolicy`中包含两个关键函数：`update_q`和`select_action`，这两个函数实现了随时间推移的动作学习值。五分之一的几率，这些函数会输出一个随机动作。在`select_action`中，大约每1,000个价格，函数会强制执行一个随机动作和探索，这由`self.epsilon`定义。在`update_q`中，智能体根据`q`策略值中那些状态的`argmax`定义当前状态和下一个期望动作。因为算法是用`tf.random_normal`初始化的Q函数和权重，所以智能体需要一段时间才能开始意识到真正的长期奖励，但再次强调，这种认识正是重点。智能体从随机理解开始，采取行动，并在模拟过程中学习最优策略。
- en: '![CH13_F10_Mattmann2](../Images/CH13_F10_Mattmann2.png)'
  id: totrans-148
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F10_Mattmann2](../Images/CH13_F10_Mattmann2.png)'
- en: Figure 13.10 The algorithm learns a good policy for trading Microsoft stock.
  id: totrans-149
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.10 算法学习了一个良好的交易微软股票的策略。
- en: You can imagine the neural network for the Q-policy function learning to resemble
    the flow shown in figure 13.11\. X is your input, representing the history of
    three stock prices along with the current balance and number of stocks. The first
    hidden layer (20, 1) learns a history over time of the reward for particular actions,
    and the second hidden layer (3, 1) maps those actions to the probability of `Buy`,
    `Sell`, `Hold` at any time.
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: 你可以想象Q策略函数学习的神经网络类似于图13.11所示的流程。X是你的输入，代表三个股票价格的历史、当前余额和股票数量。第一层隐藏层（20, 1）学习特定动作的奖励随时间的历史，第二层隐藏层（3,
    1）将这些动作映射到任何时间的`Buy`（买入）、`Sell`（卖出）、`Hold`（持有）的概率。
- en: '![CH13_F11_Mattmann2](../Images/CH13_F11_Mattmann2.png)'
  id: totrans-151
  prefs: []
  type: TYPE_IMG
  zh: '![CH13_F11_Mattmann2](../Images/CH13_F11_Mattmann2.png)'
- en: Figure 13.11 The neural architecture construction for the Q-policy function
    with two hidden layers
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: 图13.11 Q策略函数的两个隐藏层的神经网络结构
- en: 13.4 Exploring other applications of reinforcement learning
  id: totrans-153
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 13.4 探索强化学习的其他应用
- en: 'Reinforcement learning is more common than you might expect. It’s too easy
    to forget that it exists when you’ve learned supervised- and unsupervised-learning
    methods. But the following examples will open your eyes to successful uses of
    RL by Google:'
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 强化学习比你想象的更常见。当你学习了监督学习和无监督学习方法时，很容易忘记它的存在。但以下例子将让你看到谷歌成功使用强化学习的案例：
- en: '*Game playing* —In February 2015, Google developed a reinforcement-learning
    system called Deep RL to learn how to play arcade video games from the Atari 2600
    console. Unlike most RL solutions, this algorithm had a high-dimensional input;
    it perceived the raw frame-by-frame images of the video game. That way, the same
    algorithm could work with any video game without much reprogramming or reconfiguring.'
  id: totrans-155
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*游戏玩法*——2015年2月，谷歌开发了一个名为Deep RL的强化学习系统，用于学习如何从Atari 2600游戏机玩街机视频游戏。与大多数强化学习解决方案不同，这个算法具有高维输入；它感知视频游戏的原始帧帧图像。这样，相同的算法可以与任何视频游戏一起工作，而无需太多的重新编程或重新配置。'
- en: '*More game playing* —In January 2016, Google released a paper about an AI agent
    that was capable of winning the board game Go. The game is known to be unpredictable
    because of the enormous number of possible configurations (even more than in chess!),
    but this algorithm, using RL, could beat top human Go players. The latest version,
    AlphaGo Zero, was released in late 2017 and was able to beat the earlier version
    consistently—100 games to 0—in only 40 days of training. AlphaGo Zero will be
    considerably better than its 2017 performance by the time you read this book.'
  id: totrans-156
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*更多游戏玩法*——2016年1月，谷歌发布了一篇关于一个能够赢得棋盘游戏围棋的AI智能体的论文。由于可能的配置数量巨大（甚至比棋盘还多！），这款游戏因其不可预测性而闻名，但这个使用强化学习的算法能够击败顶级的人类围棋选手。最新版本AlphaGo
    Zero于2017年底发布，并在仅40天的训练后，以100比0的比分一致击败了早期版本。到你看这本书的时候，AlphaGo Zero的表现将比2017年有显著提升。'
- en: '*Robotics and control* —In March 2016, Google demonstrated a way for a robot
    to learn how to grab an object via many examples. Google collected more than 800,000
    grasp attempts by using multiple robots and developed a model to grasp arbitrary
    objects. Impressively, the robots were capable of grasping an object with the
    help of camera input alone. Learning the simple concept of grasping an object
    required aggregating the knowledge of many robots, spending many days in brute-force
    attempts until enough patterns were detected. Clearly, robots have a long way
    to go to be able to generalize, but this project is an interesting start nonetheless.'
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*机器人学与控制* ——2016年3月，谷歌展示了一种让机器人通过许多示例学习如何抓取物体的方法。谷歌使用多个机器人收集了超过80万个抓取尝试，并开发了一个模型来抓取任意物体。令人印象深刻的是，机器人仅通过摄像头输入就能抓取物体。学习抓取物体的简单概念需要聚合许多机器人的知识，花费许多天进行暴力尝试，直到检测到足够的模式。显然，机器人要能够泛化还有很长的路要走，但这个项目仍然是一个有趣的开始。'
- en: Note Now that you’ve applied reinforcement learning to the stock market, it’s
    time for you to drop out of school or quit your job and start gaming the system—your
    payoff, dear reader, for making it this far into the book! I’m kidding. The actual
    stock market is a much more complicated beast, but the techniques in this chapter
    generalize to many situations.
  id: totrans-158
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：现在你已经将强化学习应用于股市，是时候你从学校辍学或辞职，开始操纵系统了——亲爱的读者，你读到这本书的这一部分，你的回报就是！我在开玩笑。实际的股市是一个更加复杂的生物，但本章中的技术可以推广到许多情况。
- en: Summary
  id: totrans-159
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Reinforcement learning is a natural tool for problems that can be framed by
    states that change due to actions taken by an agent to discover rewards.
  id: totrans-160
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 强化学习是处理那些可以通过状态来框架化的问题的自然工具，这些状态会因为代理采取的行动而改变，以发现奖励。
- en: 'Implementing the reinforcement-learning algorithm requires three primary steps:
    infer the best action from the current state, perform the action, and learn from
    the results.'
  id: totrans-161
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 实现强化学习算法需要三个主要步骤：从当前状态推断最佳行动，执行该行动，并从结果中学习。
- en: Q-learning is an approach to solving reinforcement learning whereby you develop
    an algorithm to approximate the utility function (Q-function). After a good enough
    approximation is found, you can start inferring the best actions to take from
    each state.
  id: totrans-162
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: Q-learning 是一种解决强化学习的方法，通过开发一个算法来近似效用函数（Q函数）。在找到足够好的近似之后，你可以开始从每个状态推断出最佳的行动。
