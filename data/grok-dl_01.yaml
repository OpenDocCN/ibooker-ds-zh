- en: 'Chapter 2\. Fundamental concepts: how do machines learn?'
  id: totrans-0
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 第二章\. 基本概念：机器是如何学习的？
- en: '**In this chapter**'
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: '**本章**'
- en: What are deep learning, machine learning, and artificial intelligence?
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 深度学习、机器学习和人工智能是什么？
- en: What are parametric models and nonparametric models?
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**参数模型**和**非参数模型**是什么？'
- en: What are supervised learning and unsupervised learning?
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**监督学习**和**无监督学习**是什么？'
- en: How can machines learn?
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 机器如何学习？
- en: “Machine learning will cause every successful IPO win in five years.”
  id: totrans-6
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “机器学习将在五年内导致每一次成功的IPO胜利。”
- en: ''
  id: totrans-7
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Eric Schmidt, Google executive chairman, keynote speech, Cloud Computing Platform
    conference, 2016*'
  id: totrans-8
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*埃里克·施密特，谷歌执行董事长，云计算平台会议主题演讲，2016年*'
- en: What is deep learning?
  id: totrans-9
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 深度学习是什么？
- en: Deep learning is a subset of methods for machine learning
  id: totrans-10
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 深度学习是机器学习方法的一个子集
- en: Deep learning is a subset of machine learning, which is a field dedicated to
    the study and development of machines that can learn (sometimes with the goal
    of eventually attaining general artificial intelligence).
  id: totrans-11
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习是机器学习的一个子集，机器学习是一个致力于研究和发展能够学习的机器（有时目标是最终实现通用人工智能）的领域。
- en: In industry, deep learning is used to solve practical tasks in a variety of
    fields such as computer vision (image), natural language processing (text), and
    automatic speech recognition (audio). In short, deep learning is a subset of *methods*
    in the machine learning toolbox, primarily using *artificial neural networks*,
    which are a class of algorithm loosely inspired by the human brain.
  id: totrans-12
  prefs: []
  type: TYPE_NORMAL
  zh: 在工业界，深度学习被用于解决计算机视觉（图像）、自然语言处理（文本）和自动语音识别（音频）等各个领域的实际任务。简而言之，深度学习是机器学习工具箱中的一种方法子集，主要使用**人工神经网络**，这是一种类算法，其灵感来源于人脑。
- en: '![](Images/f0010-01.jpg)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0010-01.jpg)'
- en: Notice in this figure that not all of deep learning is focused around pursuing
    generalized artificial intelligence (sentient machines as in the movies). Many
    applications of this technology are used to solve a wide variety of problems in
    industry. This book seeks to focus on teaching the fundamentals of deep learning
    behind both cutting-edge research and industry, helping to prepare you for either.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到在这个图中，并非所有深度学习都集中在追求通用人工智能（如电影中的感知机器）上。许多这种技术的应用都是用来解决工业中各种各样的问题。本书旨在专注于教授深度学习的基础知识，无论是前沿研究还是工业应用，都有助于为这两种情况做好准备。
- en: What is machine learning?
  id: totrans-15
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 机器学习是什么？
- en: “A field of study that gives computers the ability to learn without being explicitly
    programmed.”
  id: totrans-16
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: “一个研究领域，它赋予计算机在没有明确编程的情况下学习的能力。”
- en: ''
  id: totrans-17
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
- en: '*Attributed to Arthur Samuel*'
  id: totrans-18
  prefs:
  - PREF_BQ
  type: TYPE_NORMAL
  zh: '*归功于亚瑟·萨缪尔*'
- en: Given that deep learning is a subset of machine learning, what is machine learning?
    Most generally, it is what its name implies. Machine learning is a subfield of
    computer science wherein *machines learn* to perform tasks for which they were
    *not explicitly programmed*. In short, machines observe a pattern and attempt
    to imitate it in some way that can be either direct or indirect.
  id: totrans-19
  prefs: []
  type: TYPE_NORMAL
  zh: 由于深度学习是机器学习的一个子集，那么机器学习是什么？最普遍地说，它就是其名称所暗示的。机器学习是计算机科学的一个子领域，其中**机器学习**执行那些它们没有被明确编程的任务。简而言之，机器观察到一个模式，并试图以某种方式直接或间接地模仿它。
- en: '![](Images/f0011-01.jpg)'
  id: totrans-20
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0011-01.jpg)'
- en: 'I mention direct and indirect imitation as a parallel to the two main types
    of machine learning: *supervised* and *unsupervised*. Supervised machine learning
    is the direct imitation of a pattern between two datasets. It’s always attempting
    to take an input dataset and transform it into an output dataset. This can be
    an incredibly powerful and useful capability. Consider the following examples
    (*input* datasets in bold and *output* datasets in italic):'
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: 我将直接和间接模仿与机器学习的两种主要类型相提并论：**监督学习**和**无监督学习**。监督机器学习是两个数据集之间模式的直接模仿。它总是试图将一个输入数据集转换成一个输出数据集。这可以是一种非常强大和有用的能力。考虑以下例子（**输入**数据集加粗，**输出**数据集斜体）：
- en: Using the **pixels** of an image to detect the *presence* or *absence of a cat*
  id: totrans-22
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用图像的**像素**来检测**猫**的**存在**或**不存在**
- en: Using the **movies you’ve liked** to predict more *movies you may like*
  id: totrans-23
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用你喜欢的**电影**来预测你可能喜欢的更多**电影**
- en: Using someone’s **words** to predict whether they’re *happy* or *sad*
  id: totrans-24
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用某人的**话语**来预测他们是否**快乐**或**悲伤**
- en: Using weather sensor **data** to predict the *probability of rain*
  id: totrans-25
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用天气传感器**数据**来预测**降雨****概率**
- en: Using car engine **sensors** to predict the optimal tuning *settings*
  id: totrans-26
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用汽车发动机**传感器**来预测最佳的**调整****设置**
- en: Using news **data** to predict tomorrow’s stock *price*
  id: totrans-27
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用新闻**数据**来预测明天的**股价**
- en: Using an input **number** to predict a *number* double its size
  id: totrans-28
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用一个输入**数字**来预测其两倍大小的**数字**
- en: Using a raw **audio file** to predict a *transcript* of the audio
  id: totrans-29
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 使用原始**音频文件**来预测音频的**转录**
- en: These are all supervised machine learning tasks. In all cases, the machine learning
    algorithm is attempting to imitate the pattern between the two datasets in such
    a way that it can *use one dataset to predict the other*. For any of these examples,
    imagine if you had the power to predict the *output* dataset given only the **input**
    dataset. Such an ability would be profound.
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 这些都是监督机器学习任务。在所有情况下，机器学习算法都试图模仿两个数据集之间的模式，以便它能够**使用一个数据集来预测另一个数据集**。对于这些例子中的任何一个，想象一下，如果你只有输入数据集，就有能力预测**输出**数据集。这种能力将是深远的。
- en: Supervised machine learning
  id: totrans-31
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督机器学习
- en: Supervised learning transforms datasets
  id: totrans-32
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 监督学习转换数据集
- en: Supervised learning is a method for transforming one dataset into another. For
    example, if you had a dataset called Monday Stock Prices that recorded the price
    of every stock on every Monday for the past 10 years, and a second dataset called
    Tuesday Stock Prices recorded over the same time period, a supervised learning
    algorithm might try to use one to predict the other.
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: 监督学习是将一个数据集转换为另一个数据集的方法。例如，如果你有一个名为“周一股票价格”的数据集，它记录了过去10年每周一每只股票的价格，以及另一个名为“周二股票价格”的数据集，它记录了同一时间段内的价格，一个监督学习算法可能会尝试使用一个来预测另一个。
- en: '![](Images/f0012-01.jpg)'
  id: totrans-34
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0012-01.jpg)'
- en: If you successfully trained the supervised machine learning algorithm on 10
    years of Mondays and Tuesdays, then you could predict the stock price on any Tuesday
    in the future given the stock price on the immediately preceding Monday. I encourage
    you to stop and consider this for a moment.
  id: totrans-35
  prefs: []
  type: TYPE_NORMAL
  zh: 如果你成功地在10年的周一和周二数据上训练了监督机器学习算法，那么你就可以根据前一个周一的股票价格预测未来任何一天的周二股票价格。我鼓励你停下来思考一下这一点。
- en: Supervised machine learning is the bread and butter of applied artificial intelligence
    (also known as narrow AI). It’s useful for taking *what you know* as input and
    quickly transforming it into *what you want to know*. This allows supervised machine
    learning algorithms to extend human intelligence and capabilities in a seemingly
    endless number of ways.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 监督机器学习是应用人工智能（也称为窄AI）的精髓。它有助于将你已知的**知识**作为输入，快速转化为你想要知道的**知识**。这使得监督机器学习算法能够在看似无限多的方式上扩展人类智能和能力。
- en: The majority of work using machine learning results in the training of a supervised
    classifier of some kind. Even unsupervised machine learning (which you’ll learn
    more about in a moment) is typically done to aid in the development of an accurate
    supervised machine learning algorithm.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 大多数使用机器学习的工作结果都是训练某种类型的监督分类器。即使是未监督的机器学习（你将在下一刻了解更多关于它的信息）通常也是为了帮助开发一个准确的监督机器学习算法。
- en: '![](Images/f0012-02.jpg)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0012-02.jpg)'
- en: For the rest of this book, you’ll be creating algorithms that can take input
    data that is observable, recordable, and, by extension, *knowable* and transform
    it into valuable output data that requires logical analysis. This is the power
    of supervised machine learning.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 在本书的剩余部分，你将创建能够接受可观察、可记录的输入数据，并通过扩展，**可知**的数据，并将其转换为需要逻辑分析的有价值输出数据算法。这就是监督机器学习的力量。
- en: Unsupervised machine learning
  id: totrans-40
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 未监督机器学习
- en: Unsupervised learning groups your data
  id: totrans-41
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 未监督学习对数据进行分组
- en: 'Unsupervised learning shares a property in common with supervised learning:
    it transforms one dataset into another. But the dataset that it transforms into
    is *not previously known or understood*. Unlike supervised learning, there is
    no “right answer” that you’re trying to get the model to duplicate. You just tell
    an unsupervised algorithm to “find patterns in this data and tell me about them.”'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: 未监督学习与监督学习有一个共同属性：它将一个数据集转换为另一个数据集。但它转换成的数据集**不是事先已知或理解的**。与监督学习不同，没有“正确答案”是你试图让模型复制的。你只需告诉未监督算法“在这个数据中找到模式，并告诉我关于它们的信息。”
- en: For example, *clustering a dataset into groups* is a type of unsupervised learning.
    Clustering transforms a sequence of *datapoints* into a sequence of *cluster labels*.
    If it learns 10 clusters, it’s common for these labels to be the numbers 1–10\.
    Each datapoint will be assigned to a number based on which cluster it’s in. Thus,
    the dataset turns from a bunch of datapoints into a bunch of labels. Why are the
    labels numbers? The algorithm doesn’t tell you what the clusters are. How could
    it know? It just says, “Hey scientist! I found some structure. It looks like there
    are groups in your data. Here they are!”
  id: totrans-43
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，*将数据集聚类成组*是一种无监督学习。聚类将一系列*数据点*转换成一系列*聚类标签*。如果它学习了10个聚类，这些标签通常是数字1-10。每个数据点将根据它所在的聚类分配一个数字。因此，数据集从一系列数据点变成一系列标签。为什么标签是数字？算法没有告诉你这些聚类的名称。它怎么能知道呢？它只是说：“嘿科学家！我找到了一些结构。看起来你的数据中有些组。这里它们是！”
- en: '![](Images/f0013-01.jpg)'
  id: totrans-44
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0013-01.jpg)'
- en: I have good news! This idea of clustering is something you can reliably hold
    onto in your mind as the definition of unsupervised learning. Even though there
    are many forms of unsupervised learning, *all forms of unsupervised learning can
    be viewed as a form of clustering*. You’ll discover more on this later in the
    book.
  id: totrans-45
  prefs: []
  type: TYPE_NORMAL
  zh: 我有个好消息！这种聚类的想法，你可以作为无监督学习的定义，可靠地保存在你的脑海中。尽管无监督学习有许多形式，*所有形式的无监督学习都可以被视为一种聚类形式*。你将在本书后面的内容中了解更多。
- en: '![](Images/f0013-02.jpg)'
  id: totrans-46
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0013-02.jpg)'
- en: 'Check out this example. Even though the algorithm didn’t tell what the clusters
    are named, can you figure out how it clustered the words? (Answer: 1 == cute and
    2 == delicious.) Later, we’ll unpack how other forms of unsupervised learning
    are also just a form of clustering and why these clusters are useful for supervised
    learning.'
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 查看这个例子。尽管算法没有告诉这些聚类的名称，你能想出它是如何聚类这些单词的吗？（答案：1 == cute 和 2 == delicious。）稍后，我们将解开其他形式的无监督学习也是聚类形式的原因，以及这些聚类对于监督学习是有用的。
- en: Parametric vs. nonparametric learning
  id: totrans-48
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 参数化与非参数化学习
- en: 'Oversimplified: Trial-and-error learning vs. counting and probability'
  id: totrans-49
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过于简化：试错学习与计数和概率
- en: 'The last two pages divided all machine learning algorithms into two groups:
    supervised and unsupervised. Now, we’re going to discuss another way to divide
    the same machine learning algorithms into two groups: parametric and nonparametric.
    So, if we think about our little machine learning cloud, it has two settings:'
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: 最后两页将所有机器学习算法分为两组：监督学习和无监督学习。现在，我们将讨论另一种将相同的机器学习算法分为两组的方法：参数化和非参数化。所以，如果我们考虑我们的小型机器学习云，它有两个设置：
- en: '![](Images/f0014-01.jpg)'
  id: totrans-51
  prefs: []
  type: TYPE_IMG
  zh: '![图片](Images/f0014-01.jpg)'
- en: As you can see, there are really four different types of algorithms to choose
    from. An algorithm is either unsupervised or supervised, and either parametric
    or nonparametric. Whereas the previous section on supervision is about the *type
    of pattern* being learned, parametricism is about the way the learning is *stored*
    and often, by extension, the *method for learning*. First, let’s look at the formal
    definitions of parametricism versus nonparametricism. For the record, there’s
    still some debate around the exact difference.
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: 如您所见，实际上有四种不同的算法可供选择。一个算法要么是无监督的，要么是监督的，要么是参数化的，要么是非参数化的。而前一个关于监督的部分是关于学习到的*模式类型*，参数化则是关于学习是如何*存储*的，通常，通过扩展，也是关于*学习方法*。首先，让我们看看参数化与非参数化的正式定义。据记录，关于确切差异的讨论仍然存在。
- en: '|  |'
  id: totrans-53
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: A parametric model is characterized by having a fixed number of parameters,
    whereas a nonparametric model’s number of parameters is *infinite* (determined
    by data).
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 参数化模型的特点是具有固定数量的参数，而非参数化模型的参数数量是*无限的*（由数据决定）。
- en: '|  |'
  id: totrans-55
  prefs: []
  type: TYPE_TB
  zh: '|'
- en: As an example, let’s say the problem is to fit a square peg into the correct
    (square) hole. Some humans (such as babies) just jam it into all the holes until
    it fits somewhere (parametric). A teenager, however, may count the number of sides
    (four) and then search for the hole with an equal number (nonparametric). Parametric
    models tend to use trial and error, whereas nonparametric models tend to count.
    Let’s look closer.
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，假设问题是将一个正方体放入正确的（正方形）孔中。有些人（如婴儿）只是把它塞进所有的孔，直到它适合某个地方（参数化）。然而，一个青少年可能会数一数边的数量（四条），然后寻找具有相同数量的孔（非参数化）。参数化模型倾向于使用试错法，而非参数化模型倾向于计数。让我们更深入地了解一下。
- en: Supervised parametric learning
  id: totrans-57
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 监督参数化学习
- en: 'Oversimplified: Trial-and-error learning using knobs'
  id: totrans-58
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过于简化：使用旋钮的试错学习
- en: Supervised parametric learning machines are machines with a fixed number of
    knobs (that’s the parametric part), wherein learning occurs by turning the knobs.
    Input data comes in, is processed based on the angle of the knobs, and is transformed
    into a *prediction*.
  id: totrans-59
  prefs: []
  type: TYPE_NORMAL
  zh: 监督参数学习机器是具有固定数量旋钮的机器（这就是参数部分），学习是通过旋转旋钮来进行的。输入数据进来，根据旋钮的角度进行处理，并转换为*预测*。
- en: '![](Images/f0015-01.jpg)'
  id: totrans-60
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0015-01.jpg)'
- en: Learning is accomplished by turning the knobs to different angles. If you’re
    trying to predict the probability that the Red Sox will win the World Series,
    then this model would first take data (such as sports stats like win/loss record
    or average number of toes per player) and make a prediction (such as 98% chance).
    Next, the model would observe whether or not the Red Sox actually won. After it
    knew whether they won, the learning algorithm would *update the knobs* to make
    a more accurate prediction the next time it sees the *same or similar input data*.
  id: totrans-61
  prefs: []
  type: TYPE_NORMAL
  zh: 学习是通过调整旋钮到不同的角度来完成的。如果您试图预测红袜队是否会赢得世界系列赛，那么这个模型首先会收集数据（例如体育统计数据，如胜负记录或平均每名球员的脚趾数量）并做出预测（例如98%的机会）。接下来，模型会观察红袜队是否真的获胜。在知道他们是否获胜后，学习算法会*更新旋钮*，以便在下一次看到*相同或类似输入数据*时做出更准确的预测。
- en: Perhaps it would “turn up” the “win/loss record” knob if the team’s win/loss
    record was a good predictor. Inversely, it might “turn down” the “average number
    of toes” knob if that datapoint wasn’t a good predictor. This is how parametric
    models learn!
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 如果球队的胜负记录是一个好的预测因素，它可能会“提高”胜负记录旋钮。相反，如果这个数据点不是一个好的预测因素，它可能会“降低”平均每名球员的脚趾数量旋钮。这就是参数模型学习的方式！
- en: Note that the entirety of what the model has learned can be captured in the
    positions of the knobs at any given time. You can also think of this type of learning
    model as a search algorithm. You’re “searching” for the appropriate knob configuration
    by trying configurations, adjusting them, and retrying.
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，模型所学习的一切都可以通过任何给定时间旋钮的位置来捕捉。您也可以将这种学习模型视为一种搜索算法。您通过尝试配置、调整它们并重新尝试来“搜索”适当的旋钮配置。
- en: Note further that the notion of trial and error isn’t the formal definition,
    but it’s a common (with exceptions) property to parametric models. When there
    is an arbitrary (but fixed) number of knobs to turn, some level of searching is
    required to find the optimal configuration. This is in contrast to nonparametric
    learning, which is often count based and (more or less) adds new knobs when it
    finds something new to count. Let’s break down supervised parametric learning
    into its three steps.
  id: totrans-64
  prefs: []
  type: TYPE_NORMAL
  zh: 进一步注意，试错的概念并不是正式的定义，但它是非参数模型的一个常见（但有例外）属性。当存在任意（但固定）数量的旋钮可以旋转时，需要一定程度的搜索来找到最佳配置。这与非参数学习形成对比，非参数学习通常是基于计数，并且（或多或少）在发现新的计数对象时添加新的旋钮。让我们将监督参数学习分解为其三个步骤。
- en: 'Step 1: Predict'
  id: totrans-65
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第1步：预测
- en: To illustrate supervised parametric learning, let’s continue with the sports
    analogy of trying to predict whether the Red Rox will win the World Series. The
    first step, as mentioned, is to gather sports statistics, send them through the
    machine, and make a prediction about the probability that the Red Sox will win.
  id: totrans-66
  prefs: []
  type: TYPE_NORMAL
  zh: 为了说明监督参数学习，让我们继续使用体育类比，尝试预测红袜队是否会赢得世界系列赛。正如之前提到的，第一步是收集体育统计数据，将它们通过机器，并预测红袜队获胜的概率。
- en: '![](Images/f0016-01.jpg)'
  id: totrans-67
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0016-01.jpg)'
- en: 'Step 2: Compare to the truth pattern'
  id: totrans-68
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第2步：与真实模式比较
- en: The second step is to compare the prediction (98%) with the pattern you care
    about (whether the Red Sox won). Sadly, they lost, so the comparison is
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 第二步是将预测结果（98%）与您关注的模式（例如红袜队是否获胜）进行比较。遗憾的是，他们输了，所以比较是这样的
- en: '**Pred**: 98% > **Truth**: 0%'
  id: totrans-70
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '**预测**：98% > **事实**：0%'
- en: This step recognizes that if the model had predicted 0%, it would have perfectly
    predicted the upcoming loss of the team. You want the machine to be accurate,
    which leads to step 3.
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步认识到，如果模型预测了0%，它就能完美地预测球队即将到来的失败。您希望机器准确，这导致了第3步。
- en: 'Step 3: Learn the pattern'
  id: totrans-72
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 第3步：学习模式
- en: This step adjusts the knobs by studying both how *much* the model missed by
    (98%) and what the input data *was* (sports stats) at the time of prediction.
    This step then turns the knobs to make a more accurate prediction given the input
    data.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 这一步通过研究模型在预测时*多少*次错过了（98%）以及输入数据*是什么*（体育统计数据）来调整旋钮。然后，根据输入数据调整旋钮，以便做出更准确的预测。
- en: In theory, the next time this step saw the same sports stats, the prediction
    would be lower than 98%. Note that each knob represents the *prediction’s sensitivity
    to different types of input data*. That’s what you’re changing when you “learn.”
  id: totrans-74
  prefs: []
  type: TYPE_NORMAL
  zh: 理论上，下次这个步骤看到相同的体育统计数据时，预测值将低于98%。请注意，每个旋钮代表*预测对不同类型输入数据的敏感性*。这就是你在“学习”时改变的内容。
- en: '![](Images/f0016-02.jpg)'
  id: totrans-75
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0016-02.jpg)'
- en: Unsupervised parametric learning
  id: totrans-76
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 无监督参数学习
- en: Unsupervised parametric learning uses a very similar approach. Let’s walk through
    the steps at a high level. Remember that unsupervised learning is all about grouping
    data. Unsupervised *parametric* learning uses knobs to group data. But in this
    case, it usually has several knobs for each group, each of which maps the input
    data’s affinity to that particular group (with exceptions and nuance—this is a
    high-level description). Let’s look at an example that assumes you want to divide
    the data into three groups.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 无监督参数学习采用了一种非常相似的方法。让我们从高层次的角度来梳理一下步骤。记住，无监督学习完全是关于数据分组。无监督参数学习使用旋钮来分组数据。但在这种情况下，通常为每个组提供几个旋钮，每个旋钮将输入数据的亲和力映射到特定的组（有例外和细微差别——这是一个高层次描述）。让我们来看一个例子，假设你想要将数据分成三个组。
- en: '| Home or away | # fans |'
  id: totrans-78
  prefs: []
  type: TYPE_TB
  zh: '| 主场或客场 | # 球迷数量 |'
- en: '| --- | --- |'
  id: totrans-79
  prefs: []
  type: TYPE_TB
  zh: '| --- | --- |'
- en: '| **home** away **home** **home** away *away* *away* | **100k** 50k **100k**
    **99k** 50k *10k* *11k* |'
  id: totrans-80
  prefs: []
  type: TYPE_TB
  zh: '| **主队** **客队** **主队** **客队** **客队** **客队** | **100k** 50k **100k** **99k**
    50k **10k** **11k** |'
- en: In the dataset, I’ve identified three clusters in the data that you might want
    the parametric model to find. They’re indicated via formatting as **group 1**,
    group 2, and *group 3*. Let’s propagate the first datapoint through a trained
    unsupervised model, as shown next. Notice that it maps most strongly to **group
    1**.
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 在数据集中，我已经识别出三个你可能会希望参数模型找到的簇。它们通过格式化为**组 1**、**组 2**和*组 3*来表示。让我们将第一个数据点通过一个训练好的无监督模型传播，如下所示。注意，它最强烈地映射到**组
    1**。
- en: '![](Images/f0017-01_alt.jpg)'
  id: totrans-82
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0017-01_alt.jpg)'
- en: Each group’s machine attempts to transform the input data to a number between
    0 and 1, telling us the *probability that the input data is a member of that group*.
    There is a great deal of variety in how these models train and their resulting
    properties, but at a high level they adjust parameters to transform the input
    data into its subscribing group(s).
  id: totrans-83
  prefs: []
  type: TYPE_NORMAL
  zh: 每个组的机器试图将输入数据转换为一个介于0到1之间的数字，告诉我们输入数据是该组的*概率是成员的概率*。这些模型在训练方式和最终属性上存在很大差异，但就高层次而言，它们调整参数以将输入数据转换为其所属的组（们）。
- en: Nonparametric learning
  id: totrans-84
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 非参数学习
- en: 'Oversimplified: Counting-based methods'
  id: totrans-85
  prefs:
  - PREF_H4
  type: TYPE_NORMAL
  zh: 过于简化的说法：基于计数的方法
- en: Nonparametric learning is a class of algorithm wherein the number of parameters
    is based on data (instead of predefined). This lends itself to methods that generally
    count in one way or another, thus increasing the number of parameters based on
    the number of items being counted within the data. In the supervised setting,
    for example, a nonparametric model might count the number of times a particular
    color of streetlight causes cars to “go.” After counting only a few examples,
    this model would then be able to predict that *middle* lights always (100%) cause
    cars to go, and *right* lights only sometimes (50%) cause cars to go.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 非参数学习是一类算法，其中参数的数量基于数据（而不是预定义的）。这使得它适用于一般以某种方式计数的方法，从而根据数据中计数项的数量增加参数的数量。例如，在监督设置中，一个非参数模型可能会计算特定颜色的街灯导致汽车“行驶”的次数。在只计算了几个例子之后，这个模型就能够预测*中间*的灯总是（100%）导致汽车行驶，而*右侧*的灯只有时（50%）导致汽车行驶。
- en: '![](Images/f0018-01_alt.jpg)'
  id: totrans-87
  prefs: []
  type: TYPE_IMG
  zh: '![](Images/f0018-01_alt.jpg)'
- en: 'Notice that this model would have three parameters: three counts indicating
    the number of times each colored light turned on and cars would go (perhaps divided
    by the number of total observations). If there were five lights, there would be
    five counts (five parameters). What makes this simple model *nonparametric* is
    this trait wherein the number of parameters changes based on the data (in this
    case, the number of lights). This is in contrast to parametric models, which start
    with a set number of parameters and, more important, can have more or fewer parameters
    purely at the discretion of the scientist training the model (regardless of data).'
  id: totrans-88
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到这个模型将有三个参数：三个计数表示每种颜色的灯光开启和汽车行驶的次数（可能除以总观察次数）。如果有五个灯光，就会有五个计数（五个参数）。使这个简单模型成为*非参数模型*的特点是参数的数量根据数据变化（在这种情况下，是灯光的数量）。这与参数模型形成对比，参数模型从一组固定的参数开始，更重要的是，科学家可以根据自己的意愿调整模型参数的数量或减少（无论数据如何）。
- en: A close eye might question this idea. The parametric model from before seemed
    to have a knob for each input datapoint. Most parametric models still have to
    have some sort of *input* based on the number of classes in the data. Thus you
    can see that there is a *gray area* between parametric and nonparametric algorithms.
    Even parametric algorithms are somewhat influenced by the number of classes in
    the data, even if they aren’t explicitly counting patterns.
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 仔细观察的人可能会质疑这个想法。之前的参数模型似乎为每个输入数据点都有一个旋钮。大多数参数模型仍然需要基于数据中的类别数量某种形式的*输入*。因此，你可以看到参数和非参数算法之间存在一个*灰色区域*。即使是参数算法也多少会受到数据中类别数量的影响，即使它们没有明确地计数模式。
- en: This also illuminates that *parameters* is a generic term, referring only to
    the set of numbers used to model a pattern (without any limitation on how those
    numbers are used). Counts are parameters. Weights are parameters. Normalized variants
    of counts or weights are parameters. Correlation coefficients can be parameters.
    The term refers to the set of numbers used to model a pattern. As it happens,
    deep learning is a class of parametric models. We won’t discuss nonparametric
    models further in this book, but they’re an interesting and powerful class of
    algorithm.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 这也说明了“参数”是一个通用术语，仅指用于建模模式的数字集合（没有任何关于这些数字如何使用的限制）。计数是参数。权重是参数。计数或权重的归一化变体是参数。相关系数可以是参数。这个术语指的是用于建模模式的数字集合。实际上，深度学习是一类参数模型。我们在这本书中不会进一步讨论非参数模型，但它们是一类有趣且强大的算法。
- en: Summary
  id: totrans-91
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 摘要
- en: In this chapter, we’ve gone a level deeper into the various flavors of machine
    learning. You learned that a machine learning algorithm is either supervised or
    unsupervised and either parametric or nonparametric. Furthermore, we explored
    exactly what makes these four different groups of algorithms distinct. You learned
    that supervised machine learning is a class of algorithm where you learn to predict
    one dataset given another and that unsupervised learning generally groups a single
    dataset into various kinds of clusters. You learned that parametric algorithms
    have a fixed number of *parameters* and that nonparametric algorithms adjust their
    number of parameters based on the dataset.
  id: totrans-92
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们深入探讨了机器学习的各种风味。你了解到机器学习算法要么是监督的，要么是无监督的，要么是参数的，要么是非参数的。此外，我们还探讨了是什么使得这四组不同的算法各具特色。你了解到监督机器学习是一类算法，其中你学会根据另一个数据集预测一个数据集，而无监督学习通常将单个数据集分组为各种类型的聚类。你了解到参数算法具有固定数量的*参数*，而非参数算法根据数据集调整其参数数量。
- en: Deep learning uses neural networks to perform both supervised and unsupervised
    prediction. Until now, we’ve stayed at a conceptual level as you got your bearings
    in the field as a whole and your place in it. In the next chapter, you’ll build
    your first neural network, and all subsequent chapters will be *project based*.
    So, pull out your Jupyter notebook, and let’s jump in!
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 深度学习使用神经网络进行监督学习和无监督预测。到目前为止，我们一直停留在概念层面，帮助你在这个领域找到自己的位置。在下一章，你将构建你的第一个神经网络，所有随后的章节都将基于*项目*。所以，拿出你的Jupyter笔记本，让我们开始吧！
