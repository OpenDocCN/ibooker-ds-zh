- en: '3 Heterogeneous parallel ensembles: Combining strong learners'
  id: totrans-0
  prefs:
  - PREF_H1
  type: TYPE_NORMAL
  zh: 3 异构并行集成：结合强大学习者
- en: This chapter covers
  id: totrans-1
  prefs: []
  type: TYPE_NORMAL
  zh: 本章涵盖了
- en: Combining base-learning models by performance-based weighting
  id: totrans-2
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过基于性能的加权结合基础学习模型
- en: Combining base-learning models with meta-learning by stacking and blending
  id: totrans-3
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过堆叠和混合结合基础学习模型与元学习
- en: Avoiding overfitting by ensembling with cross validation
  id: totrans-4
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 通过交叉验证集成避免过拟合
- en: Exploring a large-scale, real-world, text-mining case study with heterogeneous
    ensembles
  id: totrans-5
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 探索一个大规模、真实世界的文本挖掘案例研究，使用异构集成
- en: 'In the previous chapter, we introduced two parallel ensemble methods: bagging
    and random forests. These methods (and their variants) train *homogeneous ensembles*,
    where every base estimator is trained using the same base-learning algorithm.
    For example, in bagging classification, all the base estimators are decision-tree
    classifiers.'
  id: totrans-6
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一章中，我们介绍了两种并行集成方法：bagging和随机森林。这些方法（及其变体）训练的是*同质集成*，其中每个基础估计器都使用相同的基学习算法进行训练。例如，在bagging分类中，所有基础估计器都是决策树分类器。
- en: 'In this chapter, we continue exploring parallel ensemble methods, but this
    time focusing on *heterogeneous ensembles*. Heterogeneous ensemble methods use
    different base-learning algorithms to directly ensure ensemble diversity. For
    example, a heterogeneous ensemble can consist of three base estimators: a decision
    tree, a support vector machine (SVM), and an artificial neural network (ANN).
    These base estimators are still trained independently of each other.'
  id: totrans-7
  prefs: []
  type: TYPE_NORMAL
  zh: 在本章中，我们继续探索并行集成方法，但这次专注于*异构集成*。异构集成方法使用不同的基学习算法，直接确保集成多样性。例如，一个异构集成可以由三个基础估计器组成：一个决策树、一个支持向量机（SVM）和一个人工神经网络（ANN）。这些基础估计器仍然是相互独立地训练的。
- en: The earliest heterogeneous ensemble methods, such as stacking, were developed
    as far back as 1992\. However, these methods really came to the fore during the
    Netflix Prize competition in the mid-2000s. The top three teams, including the
    one that eventually won the $1 million prize, were ensemble teams, and their solutions
    were a complex blend of hundreds of different base models. This success was a
    striking and very public demonstration of the effectiveness of many of the methods
    we’ll be discussing in this chapter.
  id: totrans-8
  prefs: []
  type: TYPE_NORMAL
  zh: 最早期的异构集成方法，如堆叠，早在1992年就已经开发出来。然而，这些方法直到2005年左右的Netflix Prize竞赛中才真正崭露头角。前三名团队，包括最终赢得100万美元奖金的团队，都是集成团队，他们的解决方案是数百个不同基础模型的复杂混合。这一成功是对我们将在本章讨论的许多方法有效性的显著和公开的证明。
- en: Inspired by this success, stacking and blending have become widely popular.
    With sufficient base-estimator diversity, these algorithms can often boost performance
    on your data set and serve as powerful ensembling tools in any data analyst’s
    arsenal.
  id: totrans-9
  prefs: []
  type: TYPE_NORMAL
  zh: 受此成功启发，堆叠和混合变得非常流行。在有足够的基础估计器多样性的情况下，这些算法通常可以提高数据集的性能，并成为任何数据分析师工具箱中的强大集成工具。
- en: 'Another reason for their popularity is that they can easily combine existing
    models, which allows us to use previously trained models as base estimators. For
    example, say you and a friend were working independently on a data set for a Kaggle
    competition. You trained an SVM, while your friend trained a logistic regression
    model. While your individual models are doing okay, you both figure that you may
    do better if you put your heads (and models) together. You build a heterogeneous
    ensemble with these existing models without having to train them all over again.
    All you need to figure out is a way to combine your two models. Heterogeneous
    ensembles come in two flavors, depending on how they combine individual base-estimator
    predictions into a final prediction (see figure 3.1):'
  id: totrans-10
  prefs: []
  type: TYPE_NORMAL
  zh: 另一个它们受欢迎的原因是它们可以轻松地结合现有模型，这使得我们可以使用先前训练的模型作为基础估计器。例如，假设你和一位朋友独立地在一个Kaggle竞赛的数据集上工作。你训练了一个支持向量机（SVM），而你的朋友训练了一个逻辑回归模型。虽然你的个人模型表现不错，但你俩都认为如果你们把头（和模型）放在一起，可能会做得更好。你们无需重新训练这些现有模型，就可以构建一个异构集成。你只需要找出一种方法来结合你的两个模型。异构集成有两种类型，这取决于它们如何将个别基础估计器的预测组合成最终预测（见图3.1）：
- en: '*Weighting methods*—These methods assign individual base-estimator predictions
    a weight that corresponds to their strength. Better base estimators are assigned
    higher weights and influence the overall final prediction more. The predictions
    of individual base estimators are fed into a predetermined combination function,
    which makes the final predictions.'
  id: totrans-11
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*加权方法*—这些方法为每个基础估计器的预测分配一个与其强度相对应的权重。更好的基础估计器被分配更高的权重，对最终预测的整体影响更大。个别基础估计器的预测被输入到一个预定的组合函数中，从而生成最终预测。'
- en: '*Meta-learning methods*—These methods use a learning algorithm to combine the
    predictions of base estimators; the predictions of individual base estimators
    are treated as metadata and passed to a second-level meta-learner, which is trained
    to make final predictions. Figure 3.1 Homogeneous ensembles (chapter 2), such
    as bagging and random forests, use the same learning algorithm to train base estimators,
    and they achieve ensemble diversity through random sampling. Heterogeneous ensembles
    (this chapter) use different learning algorithms to achieve ensemble diversity.'
  id: totrans-12
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: '*元学习方法*—这些方法使用学习算法来组合基础估计器的预测；个别基础估计器的预测被视为元数据并传递给第二级元学习器，该学习器被训练以做出最终预测。图3.1（第2章）中的同质集成（如bagging和随机森林）使用相同的学习算法来训练基础估计器，并通过随机采样实现集成多样性。异构集成（本章）使用不同的学习算法来实现集成多样性。'
- en: '![CH03_F01_Kunapuli](../Images/CH03_F01_Kunapuli.png)'
  id: totrans-13
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F01_Kunapuli](../Images/CH03_F01_Kunapuli.png)'
- en: Figure 3.1 Homogeneous ensembles (chapter 2), such as bagging and random forests,
    use the *same* learning algorithm to train base estimators, and they achieve ensemble
    diversity through random sampling. Heterogeneous ensembles (this chapter) use
    *different* learning algorithms to achieve ensemble diversity.
  id: totrans-14
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.1 同质集成（第2章），如bagging和随机森林，使用*相同*的学习算法来训练基础估计器，并通过随机采样实现集成多样性。异构集成（本章）使用*不同*的学习算法来实现集成多样性。
- en: We begin by introducing weighting methods, which combine classifiers by weighting
    the contribution of each one based on how effective it is.
  id: totrans-15
  prefs: []
  type: TYPE_NORMAL
  zh: 我们首先介绍加权方法，这些方法通过根据每个分类器的有效性加权其贡献来组合分类器。
- en: 3.1 Base estimators for heterogeneous ensembles
  id: totrans-16
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.1 异构集成的基础估计器
- en: In this section, we’ll set up a learning framework for fitting heterogeneous
    base estimators and getting predictions from them. This is the first step in building
    heterogeneous ensembles for any application and corresponds to training the individual
    base estimators *H*[1], *H*[2], *...*, *H*[m] at the bottom of figure 3.1, shown
    previously.
  id: totrans-17
  prefs: []
  type: TYPE_NORMAL
  zh: 在本节中，我们将设置一个学习框架来调整异构基础估计器并从中获取预测。这是构建任何应用的异构集成的第一步，对应于图3.1底部之前显示的*H*[1]，*H*[2]，*...*，*H*[m]的个别基础估计器的训练。
- en: 'We’ll train our base estimators using a simple 2D data set so we can explicitly
    visualize the decision boundaries and behavior of each base estimator as well
    as the diversity of the estimators. Once trained, we can construct a heterogeneous
    ensemble using a weighting method (section 3.2) or a meta-learning method (section
    3.3):'
  id: totrans-18
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用一个简单的二维数据集来训练我们的基础估计器，这样我们可以明确地可视化每个基础估计器的决策边界和行为以及估计器的多样性。一旦训练完成，我们可以使用加权方法（第3.2节）或元学习方法（第3.3节）来构建异构集成：
- en: '[PRE0]'
  id: totrans-19
  prefs: []
  type: TYPE_PRE
  zh: '[PRE0]'
- en: ❶ Sets aside 25% of the data for validation
  id: totrans-20
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将25%的数据留作验证
- en: ❷ Sets aside a further 25% of the data for hold-out testing
  id: totrans-21
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将另外25%的数据留作留出测试
- en: This code snippet generates 600 synthetic training examples equally distributed
    into two classes, which are visualized in figure 3.2 as circles and squares.
  id: totrans-22
  prefs: []
  type: TYPE_NORMAL
  zh: 这段代码片段生成了600个等量分布到两个类别的合成训练示例，如图3.2所示，这些示例被可视化成圆圈和正方形。
- en: '![CH03_F02_Kunapuli](../Images/CH03_F02_Kunapuli.png)'
  id: totrans-23
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F02_Kunapuli](../Images/CH03_F02_Kunapuli.png)'
- en: 'Figure 3.2 Synthetic data set with two classes: 300 examples each in Class
    0 (circles) and Class 1 (squares)'
  id: totrans-24
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.2包含两个类别的合成数据集：类别0（圆圈）和类别1（正方形）各有300个示例
- en: 3.1.1 Fitting base estimators
  id: totrans-25
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.1 调整基础估计器
- en: 'Our first task is to train the individual base estimators. Unlike homogeneous
    ensembles, we can use any number of different learning algorithms and parameter
    settings to train base estimators. The key is to ensure that we choose learning
    algorithms that are different enough to produce a diverse collection of estimators.
    The more diverse our set of base estimators, the better the resulting ensemble
    will be. For this scenario, we use six popular machine-learning algorithms, all
    of which are available in scikit-learn: DecisionTreeClassifier, SVC, GaussianProcessClassifier,
    KNeighborsClassifier, RandomForestClassifier, and GaussianNB (see figure 3.3).'
  id: totrans-26
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的首要任务是训练个别的基础估计器。与同质集成不同，我们可以使用任意数量的不同学习算法和参数设置来训练基础估计器。关键是确保我们选择足够不同的学习算法，以产生多样化的估计器集合。我们的基础估计器集合越多样化，最终的集成效果就越好。对于这种情况，我们使用了六个流行的机器学习算法，它们都在scikit-learn中可用：DecisionTreeClassifier、SVC、GaussianProcessClassifier、KNeighborsClassifier、RandomForestClassifier和GaussianNB（见图3.3）。
- en: '![CH03_F03_Kunapuli](../Images/CH03_F03_Kunapuli.png)'
  id: totrans-27
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F03_Kunapuli](../Images/CH03_F03_Kunapuli.png)'
- en: Figure 3.3 Fitting six base estimators using scikit-learn
  id: totrans-28
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.3 使用scikit-learn拟合六个基础估计器
- en: The following listing initializes the six base estimators shown in figure 3.3
    and trains them. Note the individual parameter settings used to initialize each
    base estimator (e.g., max_depth=5 for DecisionTreeClassifier or n_neighbors=3
    for KNeighborsClassifier). In practice, these parameters have to be chosen carefully.
    For this simple data set, we can guess or just use the default parameter recommendations.
  id: totrans-29
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表初始化了图3.3中显示的六个基础估计器，并对其进行了训练。注意用于初始化每个基础估计器的个别参数设置（例如，DecisionTreeClassifier的max_depth=5或KNeighborsClassifier的n_neighbors=3）。在实际应用中，这些参数必须仔细选择。对于这个简单的数据集，我们可以猜测或者直接使用默认参数推荐。
- en: Listing 3.1 Fitting different base estimators
  id: totrans-30
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.1 拟合不同的基础估计器
- en: '[PRE1]'
  id: totrans-31
  prefs: []
  type: TYPE_PRE
  zh: '[PRE1]'
- en: ❶ Initializes several base-learning algorithms
  id: totrans-32
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化几个基础学习算法
- en: ❷ Fits base estimators on the training data using these different learning algorithms
  id: totrans-33
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用这些不同的学习算法在训练数据上拟合基础估计器
- en: 'We train our base estimators on the training data:'
  id: totrans-34
  prefs: []
  type: TYPE_NORMAL
  zh: 我们在训练数据上训练我们的基础估计器：
- en: '[PRE2]'
  id: totrans-35
  prefs: []
  type: TYPE_PRE
  zh: '[PRE2]'
- en: Once trained, we can also visualize how each base estimator behaves on our data
    set. It appears we were able to produce some pretty decently diverse base estimators.
  id: totrans-36
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦训练完成，我们还可以可视化每个基础估计器在我们数据集上的行为。看起来我们能够产生一些相当多样化的基础估计器。
- en: Aside from ensemble diversity, one other aspect that is immediately apparent
    from the visualization of individual base estimators is that they all don’t perform
    equally well on a held-out test set. In figure 3.4, 3-nearest neighbor (3nn) has
    the best test set performance, while Gaussian naïve Bayes (gnb) has the worst.
  id: totrans-37
  prefs: []
  type: TYPE_NORMAL
  zh: 除了集成多样性之外，从单个基础估计器的可视化中立即显而易见的一个方面是，它们在保留的测试集上的表现并不相同。在图3.4中，3-最近邻（3nn）在测试集上表现最佳，而高斯朴素贝叶斯（gnb）表现最差。
- en: '![CH03_F04_Kunapuli](../Images/CH03_F04_Kunapuli.png)'
  id: totrans-38
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F04_Kunapuli](../Images/CH03_F04_Kunapuli.png)'
- en: Figure 3.4 Base estimators in our heterogeneous ensemble. Each base estimator
    was trained using a different learning algorithm, which generally leads to a diverse
    ensemble.
  id: totrans-39
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.4 我们异构集成中的基础估计器。每个基础估计器都使用不同的学习算法进行训练，这通常会导致多样化的集成。
- en: For instance, DecisionTreeClassifier (dt) produces classifiers that partition
    the feature space into decision regions using axis-parallel boundaries (because
    each decision node in the tree splits on a single variable). Alternatively, the
    svm classifier SVC uses a radial basis function (RBF) kernel, which leads to smoother
    decision boundaries. Thus, while both learning algorithms can learn nonlinear
    classifiers, they are nonlinear in different ways.
  id: totrans-40
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，DecisionTreeClassifier (dt) 通过使用轴平行边界（因为树中的每个决策节点都基于一个变量进行分割）将特征空间划分为决策区域来生成分类器。另一方面，svm分类器SVC使用径向基函数（RBF）核，这导致决策边界更加平滑。因此，虽然这两种学习算法都可以学习非线性分类器，但它们以不同的方式非线性。
- en: Kernel methods
  id: totrans-41
  prefs: []
  type: TYPE_NORMAL
  zh: 核方法
- en: 'An SVM is an example of a *kernel method*, which is a type of machine-learning
    algorithm that can use kernel functions. A kernel function can efficiently measure
    the similarity between two data points implicitly in a high-dimensional space
    without explicitly transforming the data into that space. A linear estimator can
    be turned into a nonlinear estimator by replacing inner product computations with
    a kernel function. Commonly used kernels include the polynomial kernel and the
    Gaussian (also known as the RBF) kernel. For details, see chapter 12 of *The Elements
    of Statistical Learning: Data Mining, Inference, and Prediction*, 2nd ed., by
    Trevor Hastie, Robert Tibshirani, and Jerome Friedman (Springer, 2016).'
  id: totrans-42
  prefs: []
  type: TYPE_NORMAL
  zh: SVM是核方法的一个例子，核方法是一种可以使用核函数的机器学习算法。核函数可以在高维空间中隐式地高效测量两个数据点之间的相似性，而无需显式地将数据转换到该空间。通过用核函数替换内积计算，可以将线性估计器转换为非线性估计器。常用的核包括多项式核和高斯核（也称为RBF核）。有关详细信息，请参阅Trevor
    Hastie、Robert Tibshirani和Jerome Friedman所著的《统计学习的要素：数据挖掘、推理和预测》第2版第12章（Springer，2016年）。
- en: 3.1.2 Individual predictions of base estimators
  id: totrans-43
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.1.2 基础估计器的个体预测
- en: Given test data to predict (Xtst), we can get the predictions of each test example
    using each base estimator. In our scenario, given that we have six base estimators,
    each test example will have six predictions, one corresponding to each base estimator
    (see figure 3.5).
  id: totrans-44
  prefs: []
  type: TYPE_NORMAL
  zh: 给定用于预测的测试数据（Xtst），我们可以使用每个基础估计器来获取每个测试示例的预测。在我们的场景中，因为我们有六个基础估计器，所以每个测试示例将有六个预测，每个对应一个基础估计器（见图3.5）。
- en: '![CH03_F05_Kunapuli](../Images/CH03_F05_Kunapuli.png)'
  id: totrans-45
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F05_Kunapuli](../Images/CH03_F05_Kunapuli.png)'
- en: Figure 3.5 Individual predictions of a test set with the six trained base estimators
    in scikit-learn
  id: totrans-46
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.5展示了scikit-learn中六个训练基础估计器的测试集的个体预测
- en: Our task now is to collect the predictions of each test example by each trained
    base estimator into an array. In listing 3.2, the variable y is the structure
    that holds the predictions and is of size n_samples * n_estimators. That is, the
    entry y[15, 1] will be the prediction of the 2nd classifier (SVC) on the 16th
    test example (remember that indices in Python begin from 0).
  id: totrans-47
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的任务是收集每个训练基础估计器的每个测试示例的预测到一个数组中。在列表3.2中，变量y是存储预测的结构，其大小为n_samples * n_estimators。也就是说，y[15,
    1]将表示第2个分类器（SVC）对第16个测试示例的预测（记住Python中的索引从0开始）。
- en: Listing 3.2 Individual predictions of base estimators
  id: totrans-48
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.2基础估计器的个体预测
- en: '[PRE3]'
  id: totrans-49
  prefs: []
  type: TYPE_PRE
  zh: '[PRE3]'
- en: ❶ The flag “proba” allows us to predict labels or probability over the labels.
  id: totrans-50
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ “proba”标志允许我们预测标签或标签的概率。
- en: ❷ If true, predicts the probability of Class 1 (returns a float point probability
    value between 0 and 1)
  id: totrans-51
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 如果为真，则预测第1类的概率（返回一个介于0和1之间的浮点概率值）
- en: ❸ Otherwise, directly predicts Class 1 (returns an integer class label 0 or
    1)
  id: totrans-52
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 否则，直接预测第1类（返回整数类标签0或1）
- en: Observe that our function predict_individual has the flag proba. When we set
    proba=False, predict_individual returns the predicted labels according to each
    estimator. The predicted labels take the values *y*[pred] = 0 or *y*[pred] = 1,
    which tells us the estimator has predicted that the example belongs to Class 0
    or Class 1, respectively.
  id: totrans-53
  prefs: []
  type: TYPE_NORMAL
  zh: 注意到我们的函数predict_individual有proba标志。当我们设置proba=False时，predict_individual根据每个估计器返回预测的标签。预测的标签取值为*y*[pred]
    = 0或*y*[pred] = 1，这告诉我们估计器预测该示例属于类0或类1。
- en: 'When we set proba=True, however, each estimator will return the class prediction
    probabilities instead via each base estimator’s predict_proba() function:'
  id: totrans-54
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，当我们设置proba=True时，每个估计器将通过每个基础估计器的predict_proba()函数返回类预测概率：
- en: '[PRE4]'
  id: totrans-55
  prefs: []
  type: TYPE_PRE
  zh: '[PRE4]'
- en: Note
  id: totrans-56
  prefs: []
  type: TYPE_NORMAL
  zh: 注意
- en: Most classifiers in scikit-learn can return the probability of a label rather
    than the label directly. Some of them, such as SVC, should be explicitly told
    to do so (notice that we set probability=True when initializing SVC), while others
    are natural probabilistic classifiers and can represent and reason over class
    probabilities. These probabilities represent each base estimator’s *confidence*
    in its prediction.
  id: totrans-57
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn中的大多数分类器可以返回标签的概率而不是直接返回标签。其中一些，如SVC，需要明确告知这样做（注意我们在初始化SVC时设置了probability=True），而其他一些则是自然的概率分类器，可以表示和推理类概率。这些概率代表了每个基础估计器对其预测的*置信度*。
- en: 'We can use this function to predict the test examples:'
  id: totrans-58
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数来预测测试示例：
- en: '[PRE5]'
  id: totrans-59
  prefs: []
  type: TYPE_PRE
  zh: '[PRE5]'
- en: 'This produces the following output:'
  id: totrans-60
  prefs: []
  type: TYPE_NORMAL
  zh: 这将产生以下输出：
- en: '[PRE6]'
  id: totrans-61
  prefs: []
  type: TYPE_PRE
  zh: '[PRE6]'
- en: 'Each row contains six predictions, and each one corresponds to the prediction
    of each base estimator. We sanity check our predictions: Xtst has 113 test examples,
    and y_individual has six predictions for each of them, which gives us a 113 ×
    6 array of predictions.'
  id: totrans-62
  prefs: []
  type: TYPE_NORMAL
  zh: 每一行包含六个预测，每个预测对应于每个基估计器的预测。我们检查我们的预测：Xtst有113个测试示例，y_individual为每个示例提供六个预测，这给我们一个113
    × 6的预测数组。
- en: 'When proba=True, predict_individual returns the probability that an example
    belongs to Class 1, which we denote with *P*(*y*[pred] = 1). For two-class (binary)
    classification problems such as this one, the probability that the example belongs
    to Class 0 is simply 1 - *P*(*y*[pred] = 1) because the example can only belong
    to one or the other, and probabilities over all possibilities sum to 1\. We compute
    them as follows:'
  id: totrans-63
  prefs: []
  type: TYPE_NORMAL
  zh: 当proba=True时，predict_individual返回一个示例属于类别1的概率，我们用*P*(*y*[pred] = 1)表示。对于像这样二分类（二元）分类问题，示例属于类别0的概率简单地是1
    - *P*(*y*[pred] = 1)，因为示例只能属于一个或另一个，所有可能性的概率之和为1。我们按以下方式计算它们：
- en: '[PRE7]'
  id: totrans-64
  prefs: []
  type: TYPE_PRE
  zh: '[PRE7]'
- en: 'This produces the following output:'
  id: totrans-65
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE8]'
  id: totrans-66
  prefs: []
  type: TYPE_PRE
  zh: '[PRE8]'
- en: In the third row of this output, the third entry is 0.76, which indicates that
    our third base estimator, the GaussianProcessClassifier, is 76% confident that
    the third test example belongs to Class 1\. On the other hand, the first entry
    in the third row is 0.98, which means that the DecisionTreeClassifier is 98% confident
    that the first test example belongs to Class 1.
  id: totrans-67
  prefs: []
  type: TYPE_NORMAL
  zh: 在这个输出的第三行中，第三个条目是0.76，这表明我们的第三个基估计器，GaussianProcessClassifier，有76%的信心认为第三个测试示例属于类别1。另一方面，第三行的第一个条目是0.98，这意味着DecisionTreeClassifier有98%的信心认为第一个测试示例属于类别1。
- en: Such prediction probabilities are often called *soft predictions*. Soft predictions
    can be converted to hard (0-1) predictions by simply picking the class label with
    the highest probability; in this example, according to the GaussianProcessClassifier,
    the hard prediction would be *y* = 0 because *P*(*y* = 0) > *P*(*y* = 1).
  id: totrans-68
  prefs: []
  type: TYPE_NORMAL
  zh: 这样的预测概率通常被称为*软预测*。通过简单地选择具有最高概率的类别标签，可以将软预测转换为硬（0-1）预测；在这个例子中，根据GaussianProcessClassifier，硬预测将是*y*
    = 0，因为*P*(*y* = 0) > *P*(*y* = 1)。
- en: For the purpose of building a heterogeneous ensemble, we can either use the
    predictions directly or use their probabilities. Using the latter typically produces
    a smoother output.
  id: totrans-69
  prefs: []
  type: TYPE_NORMAL
  zh: 为了构建异构集成，我们可以直接使用预测，或者使用它们的概率。使用后者通常会产生更平滑的输出。
- en: CAUTION The prediction function just discussed is specifically written for two-class,
    that is, binary classification, problems. It can be extended to multiclass problems
    if care is taken to store the prediction probabilities for each class. That is,
    for multiclass problems, you’ll need to store the individual prediction probabilities
    in an array of size n_samples * n_estimators * n_classes.
  id: totrans-70
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：刚刚讨论的预测函数是专门为二分类，即二元分类问题编写的。如果注意存储每个类别的预测概率，它可以扩展到多分类问题。也就是说，对于多分类问题，您需要在大小为n_samples
    * n_estimators * n_classes的数组中存储个别预测概率。
- en: 'We’ve now set up the basic infrastructure necessary to create a heterogeneous
    ensemble. We’ve trained six classifiers, and we have a function that gives us
    their individual predictions on a new example. Of course, the last and most important
    step is how we combine these individual predictions: by weighting or by meta-learning.'
  id: totrans-71
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在已经建立了创建异构集成所需的基本基础设施。我们已经训练了六个分类器，并且有一个函数可以给我们提供它们对新例子的个别预测。当然，最后也是最重要的步骤是如何组合这些个别预测：通过加权或通过元学习。
- en: 3.2 Combining predictions by weighting
  id: totrans-72
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.2 通过加权组合预测
- en: What do weighting methods aim to do? Let’s return to the performance of the
    3nn and the gnb classifiers on our simple 2D data set (see figure 3.6). Imagine
    we were trying to build a very simple heterogeneous classifier using these two
    classifiers as base estimators.
  id: totrans-73
  prefs: []
  type: TYPE_NORMAL
  zh: 加权方法的目标是什么？让我们回到3nn和gnb分类器在我们简单的2D数据集上的性能（见图3.6）。想象一下，我们试图使用这两个分类器作为基估计器构建一个非常简单的异构分类器。
- en: '![CH03_F06_Kunapuli](../Images/CH03_F06_Kunapuli.png)'
  id: totrans-74
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Kunapuli](../Images/CH03_F06_Kunapuli.png)'
- en: Figure 3.6 Two base estimators can have very different behaviors on the same
    data set. A weighting strategy should reflect their performance by weighting better-performing
    classifiers higher.
  id: totrans-75
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.6两个基估计器在相同的数据集上可能会有非常不同的行为。加权策略应该通过加权表现更好的分类器来反映它们的性能。
- en: Let’s say we compare the behavior of these two classifiers using test error
    as our evaluation metric. The test error can be evaluated using the examples in
    Xtst, which was held out during training; this gives us a good estimate of how
    the models will behave on future, unseen data.
  id: totrans-76
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们使用测试错误作为评估指标来比较这两个分类器的行为。测试错误可以使用 Xtst 中的示例来评估，Xtst 在训练期间被保留出来；这为我们提供了一个很好的估计，即模型在未来的未见数据上的行为。
- en: The 3nn classifier has a test error rate of 3.54%, while gnb has a test error
    rate of 11.5%. Intuitively, we would trust the 3nn classifier more *on this* data
    set than the gnb classifier. However, this doesn’t mean that gnb is useless and
    should be discarded. For many examples, it can reinforce the decision made by
    3nn. What we don’t want it to do is contradict 3nn when it isn’t confident of
    its predictions.
  id: totrans-77
  prefs: []
  type: TYPE_NORMAL
  zh: 3nn 分类器的测试错误率为3.54%，而 gnb 的测试错误率为11.5%。直观上，我们会在这个数据集上更信任 3nn 分类器而不是 gnb 分类器。然而，这并不意味着
    gnb 是无用的，应该被丢弃。对于许多示例，它可以加强 3nn 做出的决策。我们不希望它在没有信心的情况下与 3nn 产生矛盾。
- en: This notion of base-estimator confidence can be captured by assigning weights.
    When we’re looking to assign weights to base classifiers, we should do so in a
    manner consistent with this intuition, such that the final prediction is influenced
    more by the stronger classifiers and less by the weaker classifiers.
  id: totrans-78
  prefs: []
  type: TYPE_NORMAL
  zh: 这种基础估计量置信度的概念可以通过分配权重来捕捉。当我们想要为基分类器分配权重时，我们应该以与这种直觉一致的方式去做，使得最终预测更多地受到强大分类器的影响，而较少受到较弱分类器的影响。
- en: 'Say we’re given a new data point x, and the individual predictions are *y*[3nn]
    and *y*[gnb]. A simple way to combine them would be to weight them based on their
    performance. The test set accuracy of 3nn is *a*[3nn] = 1 - 0.0354 = 0.9646, and
    the test set accuracy of gnb *a*[gnb] = 1 - 0.115 = 0.885\. The final prediction
    can be computed as follows:'
  id: totrans-79
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们得到了一个新的数据点 x，并且个别预测是 *y*[3nn] 和 *y*[gnb]。一种简单的方法是根据它们的性能来加权它们。3nn 的测试集准确率
    *a*[3nn] = 1 - 0.0354 = 0.9646，gnb 的测试集准确率 *a*[gnb] = 1 - 0.115 = 0.885。最终的预测可以计算如下：
- en: '![CH03_F06_Kunapuli-eqs-0x](../Images/CH03_F06_Kunapuli-eqs-0x.png)'
  id: totrans-80
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F06_Kunapuli-eqs-0x](../Images/CH03_F06_Kunapuli-eqs-0x.png)'
- en: The estimator weights *w*[3nn] and *w*[gnb] are proportional to their respective
    accuracies, and the higher accuracy classifier will have the higher weight. In
    this example, we have *w*[3nn] = 0.522 and *w*[gnb] = 0.478\. We’ve combined the
    two base estimators using a simple linear combination function (technically, a
    convex combination, since all the weights are positive and sum to 1).
  id: totrans-81
  prefs: []
  type: TYPE_NORMAL
  zh: 估计量权重 *w*[3nn] 和 *w*[gnb] 与它们各自的准确性成比例，准确性更高的分类器将具有更高的权重。在这个例子中，我们有 *w*[3nn]
    = 0.522 和 *w*[gnb] = 0.478。我们使用一个简单的线性组合函数（技术上，是一个凸组合，因为所有权重都是正的，且总和为1）将两个基础估计量结合起来。
- en: 'Let’s continue with the task of classifying our 2D two-moons data set and explore
    various weighting and combination strategies. This will typically consist of two
    steps (see figure 3.7):'
  id: totrans-82
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们继续进行对2D双月数据集进行分类的任务，并探索各种加权组合策略。这通常包括两个步骤（见图3.7）：
- en: Assign weights (*w*[clf]) to each classifier (clf) in some way, reflecting its
    importance.
  id: totrans-83
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 以某种方式为每个分类器（clf）分配权重（*w*[clf]），反映其重要性。
- en: Combine the weighted predictions (*w*[clf] ⋅ *y*[clf]) using a combination function
    *h*[c].
  id: totrans-84
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用组合函数 *h*[c] 将加权预测（*w*[clf] ⋅ *y*[clf]）结合起来。
- en: '![CH03_F07_Kunapuli](../Images/CH03_F07_Kunapuli.png)'
  id: totrans-85
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F07_Kunapuli](../Images/CH03_F07_Kunapuli.png)'
- en: Figure 3.7 Each base classifier is assigned an importance weight that reflects
    how much its opinion contributes to the final decision. Weighted decisions of
    each base classifier are combined using a combination function.
  id: totrans-86
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.7 每个基础分类器都被分配了一个重要性权重，它反映了其意见对最终决策的贡献程度。使用组合函数将每个基础分类器的加权决策结合起来。
- en: We now look at several such strategies that generalize this intuition for both
    predictions and prediction probabilities. Many of these strategies are very easy
    to implement and commonly used in fusing predictions from multiple models.
  id: totrans-87
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们来看几种这样的策略，这些策略概括了预测和预测概率的这种直觉。许多这些策略都非常容易实现，并且在融合多个模型的预测中常用。
- en: 3.2.1 Majority vote
  id: totrans-88
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.1 多数投票
- en: 'You’re already familiar with one type of weighted combination from the previous
    chapter: the majority vote. We briefly revisit majority vote here to show that
    it’s just one of many combination schemes and to put it into the general framework
    of combination methods.'
  id: totrans-89
  prefs: []
  type: TYPE_NORMAL
  zh: 你已经熟悉了前一章中的一种加权组合类型：多数投票。在这里我们简要回顾多数投票，以表明它只是众多组合方案中的一种，并将其纳入组合方法的通用框架中。
- en: Majority voting can be viewed as a weighted combination scheme in which each
    base estimator is assigned an equal weight; that is, if we have *m* base estimators,
    each base estimator has a weight of *w*[clf] = 1/*m*. The (weighted) predictions
    of the individual base estimators are combined using the majority vote.
  id: totrans-90
  prefs: []
  type: TYPE_NORMAL
  zh: 多数投票可以看作是一种加权组合方案，其中每个基本估计器被分配一个相等的权重；也就是说，如果我们有 *m* 个基本估计器，每个基本估计器的权重为 *w*[clf]
    = 1/*m*。个体基本估计器的（加权）预测通过多数投票相结合。
- en: Like bagging, this strategy can be extended to heterogeneous ensembles as well.
    In the general combination scheme presented in figure 3.8, to implement this weighting
    strategy, we set *w*[clf] = 1/*m* and *h*[c] = majority vote, which is the statistical
    mode.
  id: totrans-91
  prefs: []
  type: TYPE_NORMAL
  zh: 与 bagging 类似，这种策略也可以扩展到异质集成。在图 3.8 中展示的通用组合方案中，为了实现这种加权策略，我们设置 *w*[clf] = 1/*m*
    和 *h*[c] = 多数投票，这是统计上的众数。
- en: '![CH03_F08_Kunapuli](../Images/CH03_F08_Kunapuli.png)'
  id: totrans-92
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Kunapuli](../Images/CH03_F08_Kunapuli.png)'
- en: Figure 3.8 Combining by majority voting. Bagging can be viewed as a simple weighting
    method applied to a homogeneous ensemble. All classifiers have equal weights,
    and the combination function is the majority vote. We can adopt the majority voting
    strategy for heterogeneous ensembles as well.
  id: totrans-93
  prefs: []
  type: TYPE_NORMAL
  zh: 图 3.8 多数投票组合。Bagging 可以看作是应用于同质集成的简单加权方法。所有分类器具有相等的权重，组合函数是多数投票。我们也可以为异质集成采用多数投票策略。
- en: The following listing combines the individual predictions y_individual from
    a heterogeneous set of base estimators using majority voting. Note that because
    the weights of the base estimators are all equal, we don’t explicitly compute
    them.
  id: totrans-94
  prefs: []
  type: TYPE_NORMAL
  zh: 以下列表使用多数投票将来自异质基本估计器集的个体预测 y_individual 结合起来。请注意，由于基本估计器的权重都是相等的，所以我们没有显式地计算它们。
- en: Listing 3.3 Combining predictions using majority vote
  id: totrans-95
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.3 使用多数投票组合预测
- en: '[PRE9]'
  id: totrans-96
  prefs: []
  type: TYPE_PRE
  zh: '[PRE9]'
- en: ❶ Reshapes the vector to ensure it returns one prediction per example
  id: totrans-97
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 重塑向量以确保每个示例返回一个预测
- en: 'We can use this function to make predictions on the test data set, Xtst, using
    our previously trained base estimators:'
  id: totrans-98
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数使用我们之前训练的基本估计器对测试数据集 Xtst 进行预测：
- en: '[PRE10]'
  id: totrans-99
  prefs: []
  type: TYPE_PRE
  zh: '[PRE10]'
- en: 'This produces the following test error:'
  id: totrans-100
  prefs: []
  type: TYPE_NORMAL
  zh: 这产生了以下测试错误：
- en: '[PRE11]'
  id: totrans-101
  prefs: []
  type: TYPE_PRE
  zh: '[PRE11]'
- en: This weighting strategy produces a heterogeneous ensemble with a test error
    of 6.19%.
  id: totrans-102
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权策略产生了一个具有 6.19% 测试错误的异质集成。
- en: 3.2.2 Accuracy weighting
  id: totrans-103
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.2 准确率加权
- en: Recall our motivating example at the start of this section, where we were trying
    to build a very simple heterogeneous classifier using 3nn and gnb as base estimators.
    In that example, our intuitive ensembling strategy was to weight each estimator
    by its performance, specifically, the accuracy score. That was a very simple example
    of accuracy weighting.
  id: totrans-104
  prefs: []
  type: TYPE_NORMAL
  zh: 回想本节开头我们讨论的激励示例，其中我们试图使用 3nn 和 gnb 作为基本估计器构建一个非常简单的异质分类器。在那个例子中，我们直观的集成策略是按每个估计器的性能对其进行加权，具体来说，是准确率分数。这是一个非常简单的准确率加权示例。
- en: Here, we generalize this procedure to more than two estimators, as in figure
    3.8\. To get *unbiased performance estimates* for the base classifiers, we’ll
    use a *validation set*.
  id: totrans-105
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们将此过程推广到两个以上估计器，如图 3.8 所示。为了获得基本分类器的 *无偏性能估计*，我们将使用一个 *验证集*。
- en: Why do we need a validation set?
  id: totrans-106
  prefs: []
  type: TYPE_NORMAL
  zh: 为什么我们需要一个验证集？
- en: When we generated our data set, we partitioned it into a training set, a validation
    set, and a hold-out test set. The three subsets are mutually exclusive; that is,
    they don’t have any overlapping examples. So, which of these three should we use
    to obtain unbiased estimates of the performance of each individual base classifier?
  id: totrans-107
  prefs: []
  type: TYPE_NORMAL
  zh: 当我们生成数据集时，我们将它划分为训练集、验证集和保留的测试集。这三个子集是互斥的；也就是说，它们没有任何重叠的示例。那么，我们应该使用这三个中的哪一个来获得每个个体基本分类器性能的无偏估计？
- en: It’s always good machine-learning practice to *not* reuse the training set for
    performance estimates because we’ve already seen this data, so the performance
    estimate will be biased. This is like seeing a previously assigned homework problem
    on your final exam. It doesn’t really tell the professor that you’re performing
    well because you’ve learned the concept; it just shows that you’re good at that
    specific problem. In the same way, using training data to estimate performance
    doesn’t tell us if a classifier can generalize well; it just tells us how well
    it does on examples it’s already seen. To get an effective and unbiased estimate,
    we’ll need to evaluate performance on data that the model has never seen before.
  id: totrans-108
  prefs: []
  type: TYPE_NORMAL
  zh: 总是好的机器学习实践是**不**重用训练集来估计性能，因为我们已经看到了这些数据，所以性能估计将是有偏的。这就像在期末考试中看到之前分配的作业问题一样。这并不能真正告诉教授你表现良好，因为你已经学会了概念；它只是表明你擅长那个特定的问题。同样，使用训练数据来估计性能并不能告诉我们分类器是否可以很好地泛化；它只是告诉我们它在已经看到的例子上的表现如何。为了得到有效且无偏的估计，我们需要在模型从未见过的数据上评估性能。
- en: We can get unbiased estimates using either the validation set or the hold-out
    test set. However, the test set will often be used to evaluate the *final model
    performance*, that is, the performance of the *overall ensemble*.
  id: totrans-109
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用验证集或保留的测试集来获得无偏估计。然而，测试集通常用于评估**最终模型性能**，即**整体集成**的性能。
- en: 'Here, we’re interested in estimating the performance of *each base classifier*.
    For this reason, we’ll use the validation set to obtain unbiased estimates of
    each base classifier’s performance: accuracy.'
  id: totrans-110
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们感兴趣的是估计每个**基本分类器**的性能。因此，我们将使用验证集来获得每个基本分类器性能的无偏估计：准确率。
- en: Accuracy weights using a validation set
  id: totrans-111
  prefs: []
  type: TYPE_NORMAL
  zh: 使用验证集进行准确度加权
- en: 'Once we’ve trained each base classifier (clf), we evaluate its performance
    on a validation set. Let *α*[t] be the validation accuracy of the *t*th classifier,
    *H*[t]. The weight of each base classifier is then computed as follows:'
  id: totrans-112
  prefs: []
  type: TYPE_NORMAL
  zh: 一旦我们训练了每个基本分类器（clf），我们将在验证集上评估其性能。令 *α*[t] 为第 *t* 个分类器 *H*[t] 的验证准确率。然后，每个基本分类器的权重计算如下：
- en: '![CH03_F08_Kunapuli-eqs-2x](../Images/CH03_F08_Kunapuli-eqs-2x.png)'
  id: totrans-113
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Kunapuli-eqs-2x](../Images/CH03_F08_Kunapuli-eqs-2x.png)'
- en: 'The denominator is a normalization term: the sum of all the individual validation
    accuracies. This computation ensures that a classifier’s weight is proportional
    to its accuracy and that all the weights sum to 1.'
  id: totrans-114
  prefs: []
  type: TYPE_NORMAL
  zh: 分母是一个归一化项：所有单个验证准确率的总和。这个计算确保了分类器的权重与其准确性成比例，并且所有权重之和为1。
- en: 'Given a new example to predict x, we can get the predictions of the individual
    classifiers, *y*[t] (using predict_individual). Now, the final prediction can
    be computed as a weighted sum of the individual predictions:'
  id: totrans-115
  prefs: []
  type: TYPE_NORMAL
  zh: 给定一个新示例来预测 x，我们可以得到单个分类器的预测，*y*[t]（使用 predict_individual）。现在，最终预测可以计算为单个预测的加权总和：
- en: '![CH03_F08_Kunapuli-eqs-3x](../Images/CH03_F08_Kunapuli-eqs-3x.png)'
  id: totrans-116
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F08_Kunapuli-eqs-3x](../Images/CH03_F08_Kunapuli-eqs-3x.png)'
- en: This procedure is illustrated in figure 3.9.
  id: totrans-117
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程如图3.9所示。
- en: '![CH03_F09_Kunapuli](../Images/CH03_F09_Kunapuli.png)'
  id: totrans-118
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli](../Images/CH03_F09_Kunapuli.png)'
- en: Figure 3.9 Combining by performance weighting. Each classifier is assigned a
    weight proportional to its accuracy. The final prediction is computed as a weighted
    combination of the individual predictions.
  id: totrans-119
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.9 通过性能加权进行组合。每个分类器被分配一个与其准确度成比例的权重。最终预测是单个预测的加权组合。
- en: Listing 3.4 implements the combination by accuracy weighting. Note that while
    the individual classifier predictions will have values of 0 or 1, the overall
    final prediction will be a real number between 0 and 1, as the weights are fractions.
    This fractional prediction can be converted to a 0-1 final prediction easily by
    thresholding the weighted predictions on 0.5.
  id: totrans-120
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4实现了通过准确度加权的组合。请注意，尽管单个分类器的预测值将为0或1，但最终的整体预测将是一个介于0和1之间的实数，因为权重是分数。可以通过在0.5阈值上对加权预测进行阈值处理，轻松地将这种分数预测转换为0-1的最终预测。
- en: For example, a combined prediction of y_final=0.75 will be converted to y_final=1
    (because 0.75 > the 0.5 threshold), while a combined prediction of y_final=0.33
    will be converted to y_final=0 (because 0.33 < the 0.5 threshold). Ties, while
    extremely rare, can be broken arbitrarily.
  id: totrans-121
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，y_final=0.75的联合预测将被转换为y_final=1（因为0.75大于0.5阈值），而y_final=0.33的联合预测将被转换为y_final=0（因为0.33小于0.5阈值）。虽然平局非常罕见，但可以任意打破。
- en: Listing 3.4 Combining using accuracy weighting
  id: totrans-122
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.4 使用准确度加权进行组合
- en: '[PRE12]'
  id: totrans-123
  prefs: []
  type: TYPE_PRE
  zh: '[PRE12]'
- en: ❶ Takes the validation set as input
  id: totrans-124
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 将验证集作为输入
- en: ❷ Gets individual predictions on the validation set
  id: totrans-125
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在验证集上获取单个预测
- en: ❸ Sets the weight for each base classifier as its accuracy score
  id: totrans-126
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 为每个基本分类器设置其准确率分数作为权重
- en: ❹ Normalizes the weights
  id: totrans-127
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 归一化权重
- en: ❺ Computes the weighted combination of individual labels efficiently
  id: totrans-128
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 高效地计算单个标签的加权组合
- en: ❻ Converts the combined prediction into a 0-1 label by rounding
  id: totrans-129
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 通过四舍五入将组合预测转换为0-1标签
- en: 'We can use this function to make predictions on the test data set, Xtst, using
    our previously trained base estimators:'
  id: totrans-130
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数使用我们之前训练的基本估计器对测试数据集 Xtst 进行预测：
- en: '[PRE13]'
  id: totrans-131
  prefs: []
  type: TYPE_PRE
  zh: '[PRE13]'
- en: 'This produces the following output:'
  id: totrans-132
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE14]'
  id: totrans-133
  prefs: []
  type: TYPE_PRE
  zh: '[PRE14]'
- en: This weighting strategy produces a heterogeneous ensemble with a test error
    of 3.54%.
  id: totrans-134
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权策略产生了一个异质集成，测试错误率为3.54%。
- en: 3.2.3 Entropy weighting
  id: totrans-135
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.3 熵加权
- en: The entropy weighting approach is another performance-based weighting approach,
    except that it uses entropy as the evaluation metric to judge the value of each
    base estimator. Entropy is a measure of *uncertainty* or impurity in a set; a
    more disorderly set will have higher entropy.
  id: totrans-136
  prefs: []
  type: TYPE_NORMAL
  zh: 熵加权方法是一种基于性能的加权方法，但它使用熵作为评估指标来判断每个基本估计器的价值。熵是集合中*不确定性*或*杂质*的度量；一个更无序的集合将具有更高的熵。
- en: Entropy
  id: totrans-137
  prefs: []
  type: TYPE_NORMAL
  zh: 熵
- en: 'Entropy, or *information entropy* to be precise, was originally devised by
    Claude Shannon to quantify the “amount of information” conveyed by a variable.
    This is determined by two factors: (1) the number of distinct values the variable
    can take, and (2) the uncertainty associated with each value.'
  id: totrans-138
  prefs: []
  type: TYPE_NORMAL
  zh: 熵，或更确切地说，*信息熵*，最初由克劳德·香农提出，用于量化一个变量所传递的“信息量”。这取决于两个因素：（1）变量可以取的不同值的数量，（2）与每个值相关的不确定性。
- en: Consider that three patients—Ana, Bob, and Cam—are in the doctor’s office awaiting
    the doctor’s diagnosis of a disease. Ana is told with 90% confidence that she
    is healthy (i.e., 10% chance she is sick). Bob is told with 95% confidence that
    he is ill (i.e., 5% chance he is healthy). Cam is told that his test results are
    inconclusive (i.e., 50%/50%).
  id: totrans-139
  prefs: []
  type: TYPE_NORMAL
  zh: 考虑有三个病人——Ana、Bob和Cam——在医生的办公室等待医生的疾病诊断。Ana被告知有90%的信心她很健康（即有10%的可能性她生病）。Bob被告知有95%的信心他生病了（即有5%的可能性他健康）。Cam被告知他的检测结果不明确（即50%/50%）。
- en: 'Ana has received good news and there is little uncertainty in her diagnosis.
    Even though Bob has received bad news, there is little uncertainty in his diagnosis
    as well. Cam’s situation has the highest uncertainty: he has received neither
    good nor bad news and is in for more tests.'
  id: totrans-140
  prefs: []
  type: TYPE_NORMAL
  zh: Ana收到了好消息，她的诊断几乎没有不确定性。尽管Bob收到了坏消息，但他的诊断几乎没有不确定性。Cam的情况具有最高的不确定性：他没有收到好坏消息，需要进行更多的测试。
- en: Entropy quantifies this notion of uncertainty across various outcomes. Entropy-based
    measures are commonly used during decision-tree learning to greedily identify
    the best variables to split on and are used as loss functions in deep neural networks.
  id: totrans-141
  prefs: []
  type: TYPE_NORMAL
  zh: 熵量化了这种关于各种结果的不确定性概念。基于熵的度量在决策树学习期间通常用于贪婪地识别最佳分割变量，并在深度神经网络中用作损失函数。
- en: Instead of using accuracy to weight classifiers, we can use entropy. However,
    because lower entropies are desirable, we need to ensure that base classifier
    weights are *inversely proportional* to their corresponding entropies.
  id: totrans-142
  prefs: []
  type: TYPE_NORMAL
  zh: 我们不是用准确率来权衡分类器，而是可以使用熵。然而，由于较低的熵是可取的，我们需要确保基本分类器的权重与其对应的熵成**反比**。
- en: Computing entropy over predictions
  id: totrans-143
  prefs: []
  type: TYPE_NORMAL
  zh: 计算预测的熵
- en: 'Let’s say that we have a test example, and an ensemble of 10 base estimators
    returned a vector of predicted labels: [1, 1, 1, 0, 0, 1, 1, 1, 0, 0]. This set
    has six predictions of *y* = 1 and four predictions of *y* = 0\. These *label
    counts* can be equivalently expressed as *label probabilities*: the probability
    of predicting *y* = 1 is *P*(*y* = 1) = 6/10 = 0.6, and the probability of predicting
    *y* = 0 is *P*(*y* = 0) = 4/10 = 0.4\. With these label probabilities, we can
    compute the entropy over this set of base estimator predictions as'
  id: totrans-144
  prefs: []
  type: TYPE_NORMAL
  zh: 假设我们有一个测试示例，一个由10个基本估计器组成的集成返回了一个预测标签的向量：[1, 1, 1, 0, 0, 1, 1, 1, 0, 0]。这个集合有六个预测的
    *y* = 1 和四个预测的 *y* = 0。这些 *标签计数* 可以等价地表示为 *标签概率*：预测 *y* = 1 的概率是 *P*(*y* = 1)
    = 6/10 = 0.6，预测 *y* = 0 的概率是 *P*(*y* = 0) = 4/10 = 0.4。有了这些标签概率，我们可以计算这个基本估计器预测集合的熵：
- en: '![CH03_F09_Kunapuli-eqs-6x](../Images/CH03_F09_Kunapuli-eqs-6x.png)'
  id: totrans-145
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-6x](../Images/CH03_F09_Kunapuli-eqs-6x.png)'
- en: In this case, we’ll have *E* = -0.4 log0.4 - 0.6 log0.6 = 0.971.
  id: totrans-146
  prefs: []
  type: TYPE_NORMAL
  zh: 在这种情况下，我们将有 *E* = -0.4 log0.4 - 0.6 log0.6 = 0.971。
- en: 'Alternatively, consider that a second test example, where the 10 base estimators
    returned a vector of predicted labels: [1, 1, 1, 1, 0, 1, 1, 1, 1, 1]. This set
    has nine predictions of *y* = 1 and one prediction of *y* = 0\. The *label probabilities*
    in this case are *P*(*y* = 1) = 9/10 = 0.9 and *P*(*y* = 0) = 1/10 = 0.1\. The
    entropy in this case will be *E* = -0.1 log0.1 - 0.9 log0.9 = 0.469\. This set
    of predictions has a lower entropy because it’s *purer* (mostly all predictions
    are *y* = 1). Another way of viewing this is to say that the 10 base estimators
    are less uncertain about the predictions on the second example. The following
    listing can be used to compute the entropy of a set of discrete values.'
  id: totrans-147
  prefs: []
  type: TYPE_NORMAL
  zh: 或者，考虑第二个测试示例，其中 10 个基础估计器返回了一个预测标签的向量：[1, 1, 1, 1, 0, 1, 1, 1, 1, 1]。这个集合有九个预测
    *y* = 1 和一个预测 *y* = 0。在这种情况下，*标签概率* 是 *P*(*y* = 1) = 9/10 = 0.9 和 *P*(*y* = 0)
    = 1/10 = 0.1。在这种情况下，熵将是 *E* = -0.1 log0.1 - 0.9 log0.9 = 0.469。这个预测集合的熵较低，因为它更*纯净*（大多数预测都是
    *y* = 1）。另一种看法是，10 个基础估计器对第二个示例的预测更不确定。以下列表可以用来计算离散值集合的熵。
- en: Listing 3.5 Computing entropy
  id: totrans-148
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.5 计算熵
- en: '[PRE15]'
  id: totrans-149
  prefs: []
  type: TYPE_PRE
  zh: '[PRE15]'
- en: ❶ Computes label counts
  id: totrans-150
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 计算标签计数
- en: ❷ Converts counts to probabilities
  id: totrans-151
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将计数转换为概率
- en: ❸ Computes entropy as a dot product
  id: totrans-152
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 计算熵作为点积
- en: Entropy weighting with a validation set
  id: totrans-153
  prefs: []
  type: TYPE_NORMAL
  zh: 使用验证集的熵权重
- en: Let *E*[t] be the validation entropy of the *t*th classifier, *H*[t]. The weight
    of each base classifier is
  id: totrans-154
  prefs: []
  type: TYPE_NORMAL
  zh: 设 *E*[t] 为第 *t* 个分类器的验证熵 *H*[t]。每个基础分类器的权重为
- en: '![CH03_F09_Kunapuli-eqs-9x](../Images/CH03_F09_Kunapuli-eqs-9x.png)'
  id: totrans-155
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-9x](../Images/CH03_F09_Kunapuli-eqs-9x.png)'
- en: 'There are two key differences between entropy weighting and accuracy weighting:'
  id: totrans-156
  prefs: []
  type: TYPE_NORMAL
  zh: 熵权重和准确度权重之间有两个主要区别：
- en: The accuracy of a base classifier is computed using both the true labels ytrue
    and the predicted labels ypred. In this manner, the accuracy metric measures how
    well a classifier performs. A classifier with high accuracy is better.
  id: totrans-157
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础分类器的准确度使用真实标签 ytrue 和预测标签 ypred 计算。这样，准确度度量指标衡量分类器的性能。准确度高的分类器更好。
- en: The entropy of a base classifier is computed using only the predicted labels
    ypred, and the entropy metric measures how uncertain a classifier is about its
    predictions. A classifier with low entropy (uncertainty) is better. Thus, individual
    base classifier weights are inversely proportional to their corresponding entropies.
  id: totrans-158
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 基础分类器的熵仅使用预测标签 ypred 计算，熵度量指标衡量分类器对其预测的不确定性。熵（不确定性）低的分类器更好。因此，单个基础分类器权重与其对应的熵成反比。
- en: As with accuracy weighting, the final predictions need to be thresholded at
    0.5\. The following listing implements combining with entropy weighting.
  id: totrans-159
  prefs: []
  type: TYPE_NORMAL
  zh: 与准确度权重一样，最终预测需要在 0.5 处进行阈值处理。以下列表实现了使用熵权重的组合。
- en: Listing 3.6 Combining using entropy weighting
  id: totrans-160
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.6 使用熵权重的组合
- en: '[PRE16]'
  id: totrans-161
  prefs: []
  type: TYPE_PRE
  zh: '[PRE16]'
- en: ❶ Takes only the validation examples
  id: totrans-162
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 仅取验证示例
- en: ❷ Gets individual predictions on the validation set
  id: totrans-163
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在验证集上获取单个预测
- en: ❸ Sets the weight for each base classifier as its inverse entropy
  id: totrans-164
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将每个基础分类器的权重设置为它的逆熵
- en: ❹ Normalizes the weights
  id: totrans-165
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 归一化权重
- en: ❺ Computes the weighted combination of individual labels efficiently
  id: totrans-166
  prefs: []
  type: TYPE_NORMAL
  zh: ❺ 高效地计算单个标签的加权组合
- en: ❻ Returns the rounded predictions
  id: totrans-167
  prefs: []
  type: TYPE_NORMAL
  zh: ❻ 返回四舍五入的预测
- en: 'We can use this function to make predictions on the test data set, Xtst, using
    our previously trained base estimators:'
  id: totrans-168
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数使用先前训练的基础估计器对测试数据集 Xtst 进行预测：
- en: '[PRE17]'
  id: totrans-169
  prefs: []
  type: TYPE_PRE
  zh: '[PRE17]'
- en: 'This produces the following output:'
  id: totrans-170
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE18]'
  id: totrans-171
  prefs: []
  type: TYPE_PRE
  zh: '[PRE18]'
- en: This weighting strategy produces a heterogeneous ensemble with a test error
    of 3.54%.
  id: totrans-172
  prefs: []
  type: TYPE_NORMAL
  zh: 这种加权策略产生了一个具有 3.54% 测试错误的异构集成。
- en: 3.2.4 Dempster-Shafer combination
  id: totrans-173
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.2.4 Dempster-Shafer 组合
- en: The methods we’ve seen so far combine predictions of individual base estimators
    directly (notice that we’ve set the flag proba=False when calling predict_ individual).
    When we set proba=True in predict_individual, each classifier returns its individual
    estimate of the probability of belonging to Class 1\. That is, when proba=True,
    instead of returning *y*[pred] = 0 or *y*[pred] = 1, each estimator will return
    *P*(*y*[pred] = 1).
  id: totrans-174
  prefs: []
  type: TYPE_NORMAL
  zh: 我们之前看到的方法直接组合了单个基础估计器的预测（注意，我们在调用 predict_ individual 时设置了标志 proba=False）。当我们设置
    proba=True 在 predict_individual 中时，每个分类器都会返回其属于类别 1 的概率的个体估计。也就是说，当 proba=True
    时，而不是返回 *y*[pred] = 0 或 *y*[pred] = 1，每个估计器将返回 *P*(*y*[pred] = 1)。
- en: This probability reflects a classifier’s belief in what the prediction should
    be and offers a more nuanced view of the predictions. While the methods described
    in this section can also work with probabilities, the Dempster-Shafer theory (DST)
    method is another way to fuse these base-estimator beliefs into an overall final
    belief, or prediction probability.
  id: totrans-175
  prefs: []
  type: TYPE_NORMAL
  zh: 这个概率反映了分类器对预测应该是什么的信念，并提供了对预测的更细致的看法。虽然本节中描述的方法也可以与概率一起工作，但Dempster-Shafer理论（DST）方法是将这些基估计器信念融合成一个整体最终信念或预测概率的另一种方式。
- en: DST for label fusion
  id: totrans-176
  prefs: []
  type: TYPE_NORMAL
  zh: DST用于标签融合
- en: DST is a generalization of probability theory that supports reasoning under
    uncertainty and with incomplete knowledge. While the foundations of DST are beyond
    the scope of this book, the theory itself provides a way to fuse beliefs and evidence
    from multiple sources into a single belief.
  id: totrans-177
  prefs: []
  type: TYPE_NORMAL
  zh: DST是概率论的一种推广，它支持在不确定性和不完整知识下的推理。虽然DST的基础超出了本书的范围，但该理论本身提供了一种将来自多个来源的信念和证据融合成一个单一信念的方法。
- en: DST uses a number between 0 and 1 to indicate belief in a proposition, such
    as “the test example x belongs to Class 1.” This number is known as a *basic probability
    assignment* (BPA) and expresses the certainty that the text example x belongs
    to Class 1\. BPA values closer to 1 characterize decisions made with more certainty.
    The BPA allows us to translate an estimator’s confidence into a belief about the
    true label.
  id: totrans-178
  prefs: []
  type: TYPE_NORMAL
  zh: DST使用介于0和1之间的数字来表示对命题的信念，例如“测试示例x属于类别1”。这个数字被称为*基本概率分配*（BPA），它表达了文本示例x属于类别1的确定性。接近1的BPA值表示更确定的决策。BPA允许我们将估计器的置信度转换为对真实标签的信念。
- en: 'Let’s say a 3nn classifier is used to classify a test example x, and it returns
    *P*(*y*[pred] = 1 | 3*nn*) = 0.75\. Now, gnb is also used to classify the same
    test example and returns *P*(*y*[pred] = 1 | *gnb*) = 0.6\. According to DST,
    we can compute the BPA for the proposition “test example *x* belongs to Class
    1 according to both 3nn and gnb.” We do this by fusing their individual prediction
    probabilities:'
  id: totrans-179
  prefs: []
  type: TYPE_NORMAL
  zh: 假设使用3nn分类器对测试示例x进行分类，并返回 *P*(*y*[pred] = 1 | 3*nn*) = 0.75。现在，gnb也被用来对相同的测试示例进行分类，并返回
    *P*(*y*[pred] = 1 | *gnb*) = 0.6。根据DST，我们可以计算命题“测试示例 *x* 根据两者3nn和gnb都属于类别1”的BPA。我们通过融合它们的个别预测概率来完成这项工作：
- en: '![CH03_F09_Kunapuli-eqs-10x](../Images/CH03_F09_Kunapuli-eqs-10x.png)'
  id: totrans-180
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-10x](../Images/CH03_F09_Kunapuli-eqs-10x.png)'
- en: 'We can also compute the BPA for the proposition “test example x belongs to
    Class 0 according to both 3nn and gnb”:'
  id: totrans-181
  prefs: []
  type: TYPE_NORMAL
  zh: 我们还可以计算命题“测试示例x根据3nn和gnb都属于类别0”的BPA：
- en: '![CH03_F09_Kunapuli-eqs-11x](../Images/CH03_F09_Kunapuli-eqs-11x.png)'
  id: totrans-182
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-11x](../Images/CH03_F09_Kunapuli-eqs-11x.png)'
- en: Based on these scores, we’re more certain that the test example x belongs to
    Class 1\. The BPAs can be thought of as certainty scores, with which we can compute
    our final belief of belonging to Class 0 or Class 1.
  id: totrans-183
  prefs: []
  type: TYPE_NORMAL
  zh: 根据这些分数，我们更有信心认为测试示例x属于类别1。BPAs可以被视为置信度分数，我们可以用它们来计算属于类别0或类别1的最终信念。
- en: The BPAs are used to compute beliefs. The unnormalized beliefs (denoted Bel)
    that “test example *x* belongs to Class 1” are computed as
  id: totrans-184
  prefs: []
  type: TYPE_NORMAL
  zh: BPAs用于计算信念。未归一化的信念（表示为Bel）是“测试示例 *x* 属于类别1”的计算如下
- en: '![CH03_F09_Kunapuli-eqs-12x](../Images/CH03_F09_Kunapuli-eqs-12x.png)'
  id: totrans-185
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-12x](../Images/CH03_F09_Kunapuli-eqs-12x.png)'
- en: '![CH03_F09_Kunapuli-eqs-13x](../Images/CH03_F09_Kunapuli-eqs-13x.png)'
  id: totrans-186
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F09_Kunapuli-eqs-13x](../Images/CH03_F09_Kunapuli-eqs-13x.png)'
- en: 'These unnormalized beliefs can be normalized using the normalization factor
    *Z* = *Bel*(*y*[pred] = 1) + *Bel*(*y*[pred] = 0) +1, to give us *Bel*(*y*[pred]
    = 1) = 0.80 and *Bel*(*y*[pred] = 0) = 0.11\. Finally, we can use these beliefs
    to get the final prediction: the class with the highest belief. For this test
    example, the DST method produces a final prediction of *y*[pred] = 1.'
  id: totrans-187
  prefs: []
  type: TYPE_NORMAL
  zh: 这些未归一化的信念可以使用归一化因子 *Z* = *Bel*(*y*[pred] = 1) + *Bel*(*y*[pred] = 0) +1 进行归一化，以得到
    *Bel*(*y*[pred] = 1) = 0.80 和 *Bel*(*y*[pred] = 0) = 0.11。最后，我们可以使用这些信念来得到最终的预测：信念最高的类别。对于这个测试示例，DST方法产生了最终的预测
    *y*[pred] = 1。
- en: Combining using DST
  id: totrans-188
  prefs: []
  type: TYPE_NORMAL
  zh: 结合使用DST
- en: The following listing implements this approach.
  id: totrans-189
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表实现了这种方法。
- en: Listing 3.7 Combining using Dempster-Shafer
  id: totrans-190
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.7 结合使用Dempster-Shafer
- en: '[PRE19]'
  id: totrans-191
  prefs: []
  type: TYPE_PRE
  zh: '[PRE19]'
- en: ❶ Gets individual predictions on the validation set
  id: totrans-192
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 在验证集上获取个别预测
- en: ❷ Stacks the beliefs for Class 0 and Class 1 side by side for every test example
  id: totrans-193
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将每个测试示例的类别0和类别1的信念并排堆叠
- en: ❸ Selects the final label as the class with the highest belief
  id: totrans-194
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 选择最终标签为信念最高的类别
- en: 'We can use this function to make predictions on the test data set, Xtst, using
    our previously trained base estimators:'
  id: totrans-195
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用这个函数来使用我们之前训练的基础估计器对测试数据集Xtst进行预测：
- en: '[PRE20]'
  id: totrans-196
  prefs: []
  type: TYPE_PRE
  zh: '[PRE20]'
- en: 'This produces the following output:'
  id: totrans-197
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE21]'
  id: totrans-198
  prefs: []
  type: TYPE_PRE
  zh: '[PRE21]'
- en: This output means that DST achieved 5.31% accuracy.
  id: totrans-199
  prefs: []
  type: TYPE_NORMAL
  zh: 这个输出意味着DST达到了5.31%的准确率。
- en: We’ve seen four methods of combining predictions into one final prediction.
    Two use the predictions directly, while two use prediction probabilities. We can
    visualize the decision boundaries produced by these weighting methods, as shown
    in figure 3.10.
  id: totrans-200
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经看到了将预测组合成一个最终预测的四种方法。两种直接使用预测，而另外两种使用预测概率。我们可以可视化这些加权方法产生的决策边界，如图3.10所示。
- en: '![CH03_F10_Kunapuli](../Images/CH03_F10_Kunapuli.png)'
  id: totrans-201
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F10_Kunapuli](../Images/CH03_F10_Kunapuli.png)'
- en: Figure 3.10 Decision boundaries of different weighting methods
  id: totrans-202
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.10 不同加权方法的决策边界
- en: 3.3 Combining predictions by meta-learning
  id: totrans-203
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.3 通过元学习组合预测
- en: 'In the previous section, we saw one approach to constructing heterogeneous
    ensembles of classifiers: weighting. We weighted each classifier by its performance
    and used a *predetermined combination function* to combine predictions of each
    classifier. In doing so, we had to carefully design the combination function to
    reflect our performance priorities.'
  id: totrans-204
  prefs: []
  type: TYPE_NORMAL
  zh: 在上一节中，我们看到了构建分类器异构集成的另一种方法：加权。我们根据每个分类器的性能对每个分类器进行加权，并使用一个*预定的组合函数*来组合每个分类器的预测。在这样做的时候，我们必须仔细设计组合函数，以反映我们的性能优先级。
- en: 'Now, we’ll look at another approach to constructing heterogeneous ensembles:
    meta-learning. Instead of carefully designing a combination function to combine
    predictions, we’ll *train a combination function* over individual predictions.
    That is, the predictions of the base estimators are given as inputs to a second-level
    learning algorithm. Thus, rather than designing one ourselves, we’ll train a second-level
    *meta-classification function*.'
  id: totrans-205
  prefs: []
  type: TYPE_NORMAL
  zh: 现在，我们将探讨构建异构集成的一种另一种方法：元学习。我们不会精心设计一个组合函数来组合预测，而是会在单个预测上训练一个组合函数。也就是说，基础估计器的预测被作为输入提供给第二级学习算法。因此，我们不会自己设计，而是训练一个第二级的*元分类函数*。
- en: Meta-learning methods have been widely and successfully applied to a variety
    of tasks in chemometrics analysis, recommendation systems, text classification,
    and spam filtering. For recommendation systems, meta-learning methods of stacking
    and blending were brought to prominence after they were used by several top teams
    during the Netflix prize competition.
  id: totrans-206
  prefs: []
  type: TYPE_NORMAL
  zh: 元学习方法已经在化学计量分析、推荐系统、文本分类和垃圾邮件过滤等众多任务中得到了广泛和成功的应用。对于推荐系统，堆叠和混合的元学习方法在Netflix奖项竞赛中由几个顶级团队使用后，被带到了显眼的位置。
- en: 3.3.1 Stacking
  id: totrans-207
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.1 堆叠
- en: 'Stacking is the most common meta-learning method and gets its name because
    it stacks a second classifier on top of its base estimators. The general stacking
    procedure has two steps:'
  id: totrans-208
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠是最常见的元学习方法，其名称来源于它在其基础估计器之上堆叠第二个分类器。一般的堆叠过程有两个步骤：
- en: 'Level 1: Fit base estimators on the training data. This step is the same as
    before and aims to create a diverse, heterogeneous set of base classifiers.'
  id: totrans-209
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第一级：在训练数据上拟合基础估计器。这一步与之前相同，目的是创建一个多样化、异构的基础分类器集。
- en: 'Level 2: Construct a new data set from the predictions of the base classifiers,
    which become *meta-features*. Meta-features can either be the predictions or the
    probability of predictions.'
  id: totrans-210
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 第二级：从基础分类器的预测中构建一个新的数据集，这些预测成为*元特征*。元特征可以是预测本身或预测的概率。
- en: Let’s return to our example, where we construct a simple heterogeneous ensemble
    from a 3nn classifier and a gnb classifier on our 2D synthetic data set. After
    training the classifiers (3nn and gnb), we create new features, called *meta-features
    from classifications*, of these two classifiers (see figure 3.11).
  id: totrans-211
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回到我们的例子，我们从一个3nn分类器和gnb分类器在我们的2D合成数据集上构建一个简单的异构集成。在训练分类器（3nn和gnb）之后，我们创建了新的特征，称为*分类元特征*，这些特征来自这两个分类器（见图3.11）。
- en: '![CH03_F11_Kunapuli](../Images/CH03_F11_Kunapuli.png)'
  id: totrans-212
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F11_Kunapuli](../Images/CH03_F11_Kunapuli.png)'
- en: Figure 3.11 The probability of prediction of each training example according
    to 3nn and gnb are used as meta-features for a new classifier. Data points in
    darker regions indicate high-confidence predictions. Each training example now
    has two meta-features, one each from 3nn and gnb.
  id: totrans-213
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.11 根据3nn和gnb的预测概率，每个训练样本的预测概率被用作新分类器的元特征。较暗区域的点表示高置信度的预测。每个训练样本现在有两个元特征，分别来自3nn和gnb。
- en: 'Since we have two base classifiers, we can use each one to generate one meta-feature
    in our meta-example. Here we use the prediction probabilities of 3nn and gnb as
    meta-features. Thus, for each training example, say *x*[i], we obtain two meta
    features: *y*^i[3nn] and *y*^i[gnb], which are the prediction probabilities of
    *x*[i] according to 3nn and gnb, respectively.'
  id: totrans-214
  prefs: []
  type: TYPE_NORMAL
  zh: 由于我们有两个基础分类器，我们可以使用每个分类器生成我们元示例中的一个元特征。在这里，我们使用3nn和gnb的预测概率作为元特征。因此，对于每个训练示例，比如说*x*[i]，我们获得两个元特征：*y*^i[3nn]和*y*^i[gnb]，它们分别是3nn和gnb根据*x*[i]进行的预测概率。
- en: These meta-features become metadata for a second-level classifier. Contrast
    this stacking approach to combination by weighting. For both approaches, we obtain
    individual predictions using the function predict_individual. For combination
    by weighting, we use these predictions directly in some *predetermined combination
    function*. In stacking, we use these predictions as a new training set *to train
    a combination function*.
  id: totrans-215
  prefs: []
  type: TYPE_NORMAL
  zh: 这些元特征成为第二级分类器的元数据。将这种堆叠方法与加权组合进行对比。对于这两种方法，我们使用函数predict_individual获得单个预测。对于加权组合，我们直接将这些预测用于某些*预定的组合函数*。在堆叠中，我们使用这些预测作为新的训练集*来训练一个组合函数*。
- en: 'Stacking can use any number of level-1 base estimators. Our goal, as always,
    will be to ensure that there is sufficient diversity among these base estimators.
    Figure 3.12 shows the stacking schematic for the six popular algorithms we’ve
    used previously to explore combining by weighting: DecisionTreeClassifier, SVC,
    GaussianProcess Classifier, KNeighborsClassifier, RandomForestClassifier, and
    GaussianNB.'
  id: totrans-216
  prefs: []
  type: TYPE_NORMAL
  zh: 堆叠可以使用任意数量的第一级基础估计器。我们的目标，一如既往，将是确保这些基础估计器之间存在足够的多样性。图3.12显示了用于探索通过加权组合的六个先前使用的流行算法的堆叠示意图：DecisionTreeClassifier，SVC，GaussianProcess
    Classifier，KNeighborsClassifier，RandomForestClassifier，和GaussianNB。
- en: '![CH03_F12_Kunapuli](../Images/CH03_F12_Kunapuli.png)'
  id: totrans-217
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F12_Kunapuli](../Images/CH03_F12_Kunapuli.png)'
- en: Figure 3.12 Stacking with six level-1 base estimators produces a metadata set
    of six meta-features that can be used to train a level-2 meta-classifier (here,
    logistic regression).
  id: totrans-218
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.12 使用六个第一级基础估计器的堆叠产生了一个包含六个元特征的元数据集，这些特征可以用来训练第二级元分类器（此处为逻辑回归）。
- en: 'The level-2 estimator here can be trained using any base-learning algorithm.
    Historically, linear models such as linear regression and logistic regression
    have been used. An ensembling method that uses such linear models in the second
    level is called *linear stacking*. Linear stacking is generally popular because
    it’s fast: learning linear models is generally computationally efficient, even
    for large data sets. Often, linear stacking can also be an effective exploratory
    step in analyzing your data set.'
  id: totrans-219
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的第二级估计器可以使用任何基础学习算法进行训练。历史上，线性模型，如线性回归和逻辑回归，已被使用。在第二级使用此类线性模型的集成方法称为*线性堆叠*。线性堆叠通常很受欢迎，因为它速度快：学习线性模型通常计算效率高，即使是对于大型数据集。通常，线性堆叠也可以是分析数据集的有效探索步骤。
- en: However, stacking can also employ powerful nonlinear classifiers in its second
    level, including SVMs and ANNs. This allows the ensemble to combine meta-features
    in complex ways, though at the expense of interpretability inherent in linear
    models.
  id: totrans-220
  prefs: []
  type: TYPE_NORMAL
  zh: 然而，堆叠也可以在其第二级使用强大的非线性分类器，包括SVMs和ANNs。这允许集成以复杂的方式组合元特征，尽管这牺牲了线性模型固有的可解释性。
- en: NOTE scikit-learn (v1.0 and above) contains StackingClassifier and StackingRegressor,
    which can be used directly for training. In the following subsections, we implement
    our own stacking algorithms to understand the finer details of how meta-learning
    works under the hood.
  id: totrans-221
  prefs: []
  type: TYPE_NORMAL
  zh: 注意：scikit-learn（v1.0及以上版本）包含StackingClassifier和StackingRegressor，可以直接用于训练。在以下小节中，我们将实现自己的堆叠算法，以了解元学习在底层如何工作的更详细细节。
- en: 'Let’s revisit the task of classifying our 2D two-moons data set. We’ll implement
    a linear stacking procedure, which consists of the following steps: (1) train
    individual base estimators (level 1), (2a) construct meta-features, and (2b) train
    a linear regression model (level 2).'
  id: totrans-222
  prefs: []
  type: TYPE_NORMAL
  zh: 让我们回顾一下对二维双月数据集进行分类的任务。我们将实现一个线性堆叠过程，该过程包括以下步骤：(1)训练单个基础估计器（第一级），(2a)构建元特征，和(2b)训练一个线性回归模型（第二级）。
- en: We’ve already developed most of the framework we need to quickly implement linear
    stacking. We can train individual base estimators using fit (refer to listing
    3.1) and obtain meta-features from predict_individual (refer to listing 3.2).
    The following listing uses these functions to fit a stacking model with any level-2
    estimator. Since the level-2 estimator uses generated features or meta-features,
    it’s also called a *meta-estimator*.
  id: totrans-223
  prefs: []
  type: TYPE_NORMAL
  zh: 我们已经开发出了快速实现线性堆叠所需的大部分框架。我们可以使用fit（参见图表3.1）来训练单个基础估计器，并从predict_individual（参见图表3.2）中获取元特征。以下列表使用这些函数来拟合任何二级估计器的堆叠模型。由于二级估计器使用生成的特征或元特征，它也被称为*元估计器*。
- en: Listing 3.8 Stacking with a second-level estimator
  id: totrans-224
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.8 使用二级估计器的堆叠
- en: '[PRE22]'
  id: totrans-225
  prefs: []
  type: TYPE_PRE
  zh: '[PRE22]'
- en: ❶ Trains level-1 base estimators
  id: totrans-226
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 训练一级基础估计器
- en: ❷ Gets meta-features as individual predictions or prediction probabilities (proba=True/False)
  id: totrans-227
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 获取元特征作为单个预测或预测概率（proba=True/False）
- en: ❸ Trains level-2 meta-estimator
  id: totrans-228
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练二级元估计器
- en: ❹ Saves the level-1 estimators and level-2 estimator in a dictionary
  id: totrans-229
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将一级估计器和二级估计器保存在字典中
- en: This function can learn by either using the predictions directly (use_probabilities=False)
    or by using the prediction probabilities (use_probabilities=True), as shown in
    figure 3.13.
  id: totrans-230
  prefs: []
  type: TYPE_NORMAL
  zh: 此函数可以通过直接使用预测（use_probabilities=False）或使用预测概率（use_probabilities=True）来学习，如图3.13所示。
- en: '![CH03_F13_Kunapuli](../Images/CH03_F13_Kunapuli.png)'
  id: totrans-231
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F13_Kunapuli](../Images/CH03_F13_Kunapuli.png)'
- en: Figure 3.13 Final models produced by stacking with logistic regression using
    either predictions (left) or prediction probabilities (right) as meta-features
  id: totrans-232
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.13 使用逻辑回归堆叠并使用预测（左侧）或预测概率（右侧）作为元特征的最终模型
- en: The level-2 estimator here can be any classification model. Logistic regression
    is a common choice, which leads the ensemble to stack level-1 predictions using
    a linear model.
  id: totrans-233
  prefs: []
  type: TYPE_NORMAL
  zh: 此处的二级估计器可以是任何分类模型。逻辑回归是一个常见的选择，它导致集成使用线性模型堆叠一级预测。
- en: A nonlinear model can also be used as a level-2 estimator. In general, any learning
    algorithm can be used to train a level2_estimator over the meta-features. A learning
    algorithm such as an SVM with RBF kernels or an ANN can learn powerful nonlinear
    models at the second level and potentially improve performance even more.
  id: totrans-234
  prefs: []
  type: TYPE_NORMAL
  zh: 也可以使用非线性模型作为二级估计器。一般来说，任何学习算法都可以用于在元特征上训练二级估计器。例如，使用RBF核的SVM或ANN这样的学习算法可以在第二级学习强大的非线性模型，并可能进一步提高性能。
- en: 'Prediction proceeds in two steps:'
  id: totrans-235
  prefs: []
  type: TYPE_NORMAL
  zh: 预测分为两个步骤：
- en: For each test example, get the meta-features using the trained level-1 estimators
    and create a corresponding test meta-example.
  id: totrans-236
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个测试示例，使用训练好的一级估计器获取元特征，并创建相应的测试元示例。
- en: For each meta-example, get the final prediction using the level-2 estimator.
  id: totrans-237
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 对于每个元示例，使用二级估计器获取最终预测。
- en: Making predictions with a stacked model can also be implemented easily, as shown
    in listing 3.9.
  id: totrans-238
  prefs: []
  type: TYPE_NORMAL
  zh: 使用堆叠模型进行预测也可以轻松实现，如图3.9所示。
- en: Listing 3.9 Making predictions with a stacked model
  id: totrans-239
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.9 使用堆叠模型进行预测
- en: '[PRE23]'
  id: totrans-240
  prefs: []
  type: TYPE_PRE
  zh: '[PRE23]'
- en: ❶ Gets level-1 base estimators
  id: totrans-241
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 获取一级基础估计器
- en: ❷ Gets meta-features using the level-1 base estimators
  id: totrans-242
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 使用一级基础估计器获取元特征
- en: ❸ Gets level-2 estimator and uses it to make the final predictions on the meta-features
  id: totrans-243
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 获取二级估计器并使用它对元特征进行最终预测
- en: 'In the following example, we use the same six base estimators from the previous
    section in level 1 and logistic regression as the level-2 meta-estimator:'
  id: totrans-244
  prefs: []
  type: TYPE_NORMAL
  zh: 在以下示例中，我们使用上一节中相同的六个基础估计器作为第一级，并使用逻辑回归作为第二级元估计器：
- en: '[PRE24]'
  id: totrans-245
  prefs: []
  type: TYPE_PRE
  zh: '[PRE24]'
- en: 'This produces the following output:'
  id: totrans-246
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE25]'
  id: totrans-247
  prefs: []
  type: TYPE_PRE
  zh: '[PRE25]'
- en: In the preceding snippet, we used the prediction probabilities as meta-features.
    This linear stacking model obtains a test error of 6.19%.
  id: totrans-248
  prefs: []
  type: TYPE_NORMAL
  zh: 在前面的代码片段中，我们使用了预测概率作为元特征。此线性堆叠模型获得了6.19%的测试误差。
- en: 'This simple stacking procedure is often effective. However, it does suffer
    from one significant drawback: overfitting, especially in the presence of noisy
    data. The effects of overfitting can be observed in figure 3.14\. In the case
    of stacking, the overfitting occurs because we used the same data set to train
    all the base estimators.'
  id: totrans-249
  prefs: []
  type: TYPE_NORMAL
  zh: 这种简单的堆叠过程通常很有效。然而，它确实存在一个显著的缺点：过拟合，尤其是在存在噪声数据的情况下。过拟合的影响可以在图3.14中观察到。在堆叠的情况下，过拟合发生是因为我们使用了相同的数据集来训练所有基础估计器。
- en: '![CH03_F14_Kunapuli](../Images/CH03_F14_Kunapuli.png)'
  id: totrans-250
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F14_Kunapuli](../Images/CH03_F14_Kunapuli.png)'
- en: 'Figure 3.14 Stacking can overfit the data. There is evidence of overfitting
    here: the decision boundaries are highly jagged where the classifiers have attempted
    to fit individual, noisy examples.'
  id: totrans-251
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.14 堆叠可能会过拟合数据。这里有过拟合的证据：决策边界在分类器尝试拟合单个、有噪声的示例的地方非常锯齿状。
- en: To guard against overfitting, we can incorporate *k-fold cross validation* (CV)
    such that each base estimator isn’t trained on the exact same data set. You may
    have previously encountered and used CV for parameter selection and model evaluation.
  id: totrans-252
  prefs: []
  type: TYPE_NORMAL
  zh: 为了防止过拟合，我们可以结合*k*折交叉验证（CV），这样每个基础估计器就不会在完全相同的数据集上训练。你可能之前遇到过并使用CV进行参数选择和模型评估。
- en: Here, we use CV to partition the data set into subsets so that different base
    estimators are trained on different subsets. This often leads to more diversity
    and robustness, while decreasing the chances of overfitting.
  id: totrans-253
  prefs: []
  type: TYPE_NORMAL
  zh: 在这里，我们使用交叉验证（CV）将数据集划分为子集，以便不同的基础估计器在不同的子集上训练。这通常会导致更多样化和鲁棒性，同时降低过拟合的风险。
- en: 3.3.2 Stacking with cross validation
  id: totrans-254
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.3.2 带有交叉验证的堆叠
- en: CV is a model validation and evaluation procedure that is commonly employed
    to simulate out-of-sample testing, tune model hyperparameters, and test the effectiveness
    of machine-learning models. The prefix “k-fold” is used to describe the number
    of subsets we’ll be partitioning our data set into. For example, in 5-fold CV,
    data is (often randomly) partitioned into five nonoverlapping subsets. This gives
    rise to five folds, or combinations, of these subsets for training and validation,
    as shown in figure 3.15.
  id: totrans-255
  prefs: []
  type: TYPE_NORMAL
  zh: CV是一种模型验证和评估过程，通常用于模拟样本外测试、调整模型超参数以及测试机器学习模型的有效性。前缀“k-fold”用于描述我们将数据集划分为多少个子集。例如，在5折交叉验证中，数据（通常是随机地）被划分为五个非重叠的子集。这产生了五个折，或组合，用于训练和验证，如图3.15所示。
- en: '![CH03_F15_Kunapuli](../Images/CH03_F15_Kunapuli.png)'
  id: totrans-256
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F15_Kunapuli](../Images/CH03_F15_Kunapuli.png)'
- en: Figure 3.15 k-fold CV (here, *k*=5) generates *k* different splits of the data
    set into a training set and a validation set. This simulates out-of-sample validation
    during training.
  id: totrans-257
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.15中的k折交叉验证（此处，*k*=5）将数据集分成*k*个不同的训练集和验证集。这模拟了训练过程中的样本外验证。
- en: 'More concretely, in 5-fold CV, let’s say the data set *D* is partitioned into
    five subsets: *D*[1], *D*[2], *D*[3], *D*[4], and *D*[5]. These subsets are disjointed,
    that is, any example in the data set appears in only one of the subsets. The third
    fold will comprise the training set trn[3] = {*D*[1],*D*[2],*D*[4],*D*[5]} (all
    subsets *except D*[3]) and the validation set val[3] = {*D*[3]} (*only* *D*[3]).
    This fold allows us to train and validate one model. Overall, 5-fold CV will allow
    us to train and validate five models.'
  id: totrans-258
  prefs: []
  type: TYPE_NORMAL
  zh: 更具体地说，在5折交叉验证中，假设数据集*D*被划分为五个子集：*D*[1]，*D*[2]，*D*[3]，*D*[4]，和*D*[5]。这些子集是互斥的，也就是说，数据集中的任何示例只出现在这些子集中的一个中。第三个折将包含训练集trn[3]
    = {*D*[1]，*D*[2]，*D*[4]，*D*[5]}（除了*D*[3]的所有子集）和验证集val[3] = {*D*[3]}（只有*D*[3]）。这个折允许我们训练和验证一个模型。总体而言，5折交叉验证将允许我们训练和验证五个模型。
- en: 'In our case, we’ll use the cross-validation procedure slightly differently
    in order to ensure robustness of our level-2 estimator. Instead of using the validation
    sets val[k] for evaluation, we’ll use them for generating meta-features for the
    level-2 estimator. The precise steps for combining stacking with CV are as follows:'
  id: totrans-259
  prefs: []
  type: TYPE_NORMAL
  zh: 在我们的案例中，我们将以略微不同的方式使用交叉验证过程，以确保我们二级估计器的鲁棒性。我们不会使用验证集val[k]进行评估，而是将它们用于为二级估计器生成元特征。将堆叠与CV结合的精确步骤如下：
- en: Randomly split the data into *k* equal-sized subsets.
  id: totrans-260
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将数据随机划分为*k*个大小相等的子集。
- en: Train *k* models for each base estimator using the training data from the corresponding
    *k*th fold, trn[k].
  id: totrans-261
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对应第k个折的训练数据trn[k]，为每个基础估计器训练*k*个模型。
- en: Generate *k* sets of meta-examples from each trained base estimator using the
    validation data from the corresponding *k*th fold, *val*[k].
  id: totrans-262
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用对应第k个折的验证数据*val*[k]，从每个训练好的基础估计器生成*k*组元示例。
- en: Retrain each level-1 base estimator on the full data set.
  id: totrans-263
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在完整数据集上重新训练每个一级基础估计器。
- en: The first three steps of this procedure are illustrated in figure 3.16.
  id: totrans-264
  prefs: []
  type: TYPE_NORMAL
  zh: 该过程的头三个步骤在图3.16中进行了说明。
- en: '![CH03_F16_Kunapuli](../Images/CH03_F16_Kunapuli.png)'
  id: totrans-265
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F16_Kunapuli](../Images/CH03_F16_Kunapuli.png)'
- en: Figure 3.16 Stacking with k-fold CV. *k* versions of each level-1 base estimator
    are trained using the training sets within each fold, and *k* subsets of meta-examples
    are generated from the validation sets in each fold for the level-2 estimator.
  id: totrans-266
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.16 使用k折CV的堆叠。每个第一层基础估计器的*k*个版本在每个折的训练集中训练，并为第二层估计器从每个折的验证集中生成*k*个子元示例。
- en: A key part of stacking with CV is to split the data set into training and validation
    sets for each fold. scikit-learn contains many utilities to perform precisely
    this, and the one we’ll use is called model_selection.StratifiedKFold. The StratifiedKFold
    class is a variation of the model_selection.KFold class that returns *stratified
    folds*. This means that the folds preserve the class distributions in the data
    set when generating folds.
  id: totrans-267
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CV进行堆叠的一个关键部分是将数据集分割为每个折的训练集和验证集。scikit-learn包含许多用于执行此操作的实用工具，我们将使用的一个称为model_selection.StratifiedKFold。StratifiedKFold类是model_selection.KFold类的变体，它返回*分层折*。这意味着在生成折时，折保留了数据集中的类别分布。
- en: For example, if the ratio of positive examples to negative examples in our data
    set is 2:1, StratifiedKFold will ensure that this ratio is preserved in the folds
    as well. Finally, it should be noted that rather than creating multiple copies
    of the data set for each fold (which is very wasteful in terms of storage), StratifiedKFold
    actually returns indices of the data points in the training and validation subsets
    of each fold. The following listing demonstrates how to perform stacking with
    cross validation.
  id: totrans-268
  prefs: []
  type: TYPE_NORMAL
  zh: 例如，如果我们的数据集中正例与负例的比例是2:1，StratifiedKFold将确保这个比例在折中也被保留。最后，应该注意的是，StratifiedKFold实际上返回的是每个折的训练集和验证集数据点的索引，而不是为每个折创建数据集的多个副本（这在存储方面非常浪费）。下面的列表展示了如何执行带有交叉验证的堆叠。
- en: Listing 3.10 Stacking with cross validation
  id: totrans-269
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.10 使用交叉验证的堆叠
- en: '[PRE26]'
  id: totrans-270
  prefs: []
  type: TYPE_PRE
  zh: '[PRE26]'
- en: ❶ Initializes the metadata matrix
  id: totrans-271
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 初始化元数据矩阵
- en: ❷ Trains level-1 estimators and then makes meta-features for the level-2 estimator
    with individual predictions
  id: totrans-272
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 训练第一层估计器，然后使用单个预测为第二层估计器生成元特征
- en: ❸ Trains level-2 meta-estimator
  id: totrans-273
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 训练第二层元估计器
- en: ❹ Saves the level-1 estimators and level-2 estimator in a dictionary
  id: totrans-274
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将第一层估计器和第二层估计器保存在字典中
- en: 'We can use this function to train a stacking model with CV:'
  id: totrans-275
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以使用此函数使用CV训练堆叠模型：
- en: '[PRE27]'
  id: totrans-276
  prefs: []
  type: TYPE_PRE
  zh: '[PRE27]'
- en: 'This produces the following output:'
  id: totrans-277
  prefs: []
  type: TYPE_NORMAL
  zh: 这会产生以下输出：
- en: '[PRE28]'
  id: totrans-278
  prefs: []
  type: TYPE_PRE
  zh: '[PRE28]'
- en: With CV, stacking obtains a test error of 5.31%. As before, we can visualize
    our stacked model, as shown in figure 3.17\. We see that the decision boundary
    is smoother, less jagged, and less prone to overfitting overall.
  id: totrans-279
  prefs: []
  type: TYPE_NORMAL
  zh: 使用CV，堆叠获得了5.31%的测试误差。和之前一样，我们可以可视化我们的堆叠模型，如图3.17所示。我们看到决策边界更平滑，更少锯齿状，整体上更不容易过拟合。
- en: '![CH03_F17_Kunapuli](../Images/CH03_F17_Kunapuli.png)'
  id: totrans-280
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F17_Kunapuli](../Images/CH03_F17_Kunapuli.png)'
- en: Figure 3.17 Stacking with CV is more robust to overfitting.
  id: totrans-281
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.17 使用CV进行堆叠对过拟合更稳健。
- en: TIP In our example scenario, we have six base estimators; if we choose to perform
    stacking with 5-fold CV, we’ll have to train 6 × 5 = 30 models totally. Each base
    estimator is trained on a (*k* - 1)/*k* fraction of the data set. For smaller
    data sets, the corresponding increase in training time is modest, and is often
    well worth the cost. For larger data sets, this training time can be significant.
    If a full cross-validation-based stacking model is too prohibitively expensive
    to train, then it’s usually sufficient to hold out a single validation set, rather
    than several cross-validation subsets. This procedure is known as *blending*.
  id: totrans-282
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 在我们的示例场景中，我们有六个基础估计器；如果我们选择使用5折交叉验证进行堆叠，我们总共需要训练6 × 5 = 30个模型。每个基础估计器都在数据集的(*k*
    - 1)/*k*部分上训练。对于较小的数据集，相应的训练时间增加是适度的，并且通常值得付出代价。对于较大的数据集，这种训练时间可能是显著的。如果基于完整交叉验证的堆叠模型训练成本过高，那么通常保留一个单独的验证集，而不是几个交叉验证子集，就足够了。这个程序被称为*混合*。
- en: 'We can now see meta-learning in action on a large-scale, real-world classification
    task with our next case study: sentiment analysis.'
  id: totrans-283
  prefs: []
  type: TYPE_NORMAL
  zh: 现在我们可以通过我们的下一个案例研究：情感分析，看到元学习在大型、真实世界分类任务中的实际应用。
- en: '3.4 Case study: Sentiment analysis'
  id: totrans-284
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 3.4 案例研究：情感分析
- en: Sentiment analysis is a natural language processing (NLP) task widely used to
    identify and analyze opinion in text. In its simplest form, it’s mainly concerned
    with identifying the *effect* or the *polarity* of opinion as positive, neutral,
    or negative. Such “voice of the customer” analytics are a key part of brand monitoring,
    customer service, and market research.
  id: totrans-285
  prefs: []
  type: TYPE_NORMAL
  zh: 情感分析是一种自然语言处理（NLP）任务，广泛应用于识别和分析文本中的观点。在其最简单的形式中，它主要关注识别观点的*效果*或*极性*，即正面、中性或负面。这种“客户之声”分析是品牌监控、客户服务和市场研究的关键部分。
- en: This case study explores a supervised sentiment analysis task for movie reviews.
    The data set we’ll use is the Large Movie Review Dataset, which was originally
    collected and curated from IMDB.com for NLP research by a group at Stanford University.[¹](#pgfId-1155072)
    It’s a large, publicly available data set that has become a text-mining/machine-learning
    benchmark over the past few years and has also featured in several Kaggle competitions
    ([www.kaggle.com/c/word2vec-nlp-tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial)).
  id: totrans-286
  prefs: []
  type: TYPE_NORMAL
  zh: 本案例研究探讨了针对电影评论的监督情感分析任务。我们将使用的数据集是大型电影评论数据集，它最初由斯坦福大学的一个小组收集和整理，用于NLP研究，并来自IMDB.com。[¹](#pgfId-1155072)
    这个大型、公开可用的数据集在过去几年已成为文本挖掘/机器学习的基准，并出现在几个Kaggle竞赛中([www.kaggle.com/c/word2vec-nlp-tutorial](https://www.kaggle.com/c/word2vec-nlp-tutorial))。
- en: 'The data set contains 50,000 movie reviews split into training (25,000) and
    test (25,000) sets. Each review is also associated with a numerical rating from
    1 to 10\. This data set, however, only considers strongly opinionated labels,
    that is, reviews that are strongly positive about a movie (7-10) or strongly negative
    about a movie (1-4). These labels are condensed into binary sentiment polarity
    labels: strongly positive sentiment (Class 1) and strongly negative sentiment
    (Class 0). Here’s an example of a positive review (label = 1) from the data set:'
  id: totrans-287
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集包含50,000条电影评论，分为训练集（25,000）和测试集（25,000）。每条评论还关联一个从1到10的数值评分。然而，这个数据集只考虑了强烈观点的标签，即对电影强烈正面（7-10）或强烈负面（1-4）的评论。这些标签被压缩为二进制情感极性标签：强烈正面情感（类别1）和强烈负面情感（类别0）。以下是从数据集中一个正面评论（标签=1）的例子：
- en: What a delightful movie. The characters were not only lively but alive, mirroring
    real every day life and strife within a family. Each character brought a unique
    personality to the story that the audience could easily associate with someone
    they know within their own family or circle of close friends.
  id: totrans-288
  prefs: []
  type: TYPE_NORMAL
  zh: 这是一部多么令人愉快的电影。角色不仅活泼，而且充满生命力，反映了家庭中的真实日常生活和冲突。每个角色都给故事带来了独特的个性，观众可以轻易地将其与他们自己家庭或亲密朋友圈子中认识的人联系起来。
- en: 'And here’s an example of a negative review (label = 0):'
  id: totrans-289
  prefs: []
  type: TYPE_NORMAL
  zh: 下面是一个负面评论（标签=0）的例子：
- en: This is the worst sequel on the face of the world of movies. Once again it doesn't
    make since. The killer still kills for fun. But this time he is killing people
    that are making a movie about what happened in the first movie. Which means that
    it’s the stupidest movie ever. Don’t watch this. If you value the one precious
    hour during this movie then don't watch it.
  id: totrans-290
  prefs: []
  type: TYPE_NORMAL
  zh: 这是电影史上最糟糕的续集。再一次，它没有任何意义。杀手仍然为了乐趣而杀人。但这次他杀的是那些在拍关于第一部电影的电影的人。这意味着这是史上最愚蠢的电影。不要看这部电影。如果你珍视这部电影中的宝贵一小时，那就不要看它。
- en: Note the misspelling of “sense” as “since” above. Real-world text data can be
    highly noisy due to such spelling, grammatical, and linguistic idiosyncrasies,
    which makes these problems very challenging for machine learning. To begin, download
    and unzip this data set.
  id: totrans-291
  prefs: []
  type: TYPE_NORMAL
  zh: 注意上面“sense”拼写为“since”的错误。由于这样的拼写、语法和语言上的特殊性，现实世界的文本数据可能非常嘈杂，这使得这些问题对机器学习来说非常具有挑战性。首先，下载并解压这个数据集。
- en: 3.4.1 Preprocessing
  id: totrans-292
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.1 预处理
- en: The data set is preprocessed to bring each review from an unstructured, free-text
    form to a structured, vector representation. Put another way, preprocessing aims
    to bring this corpus (collection) of text files into a *term-document matrix*
    representation.
  id: totrans-293
  prefs: []
  type: TYPE_NORMAL
  zh: 数据集经过预处理，将每个评论从非结构化的自由文本形式转换为结构化的向量表示。换句话说，预处理的目标是将这个文本文件集合（语料库）转换为*词-文档矩阵*表示。
- en: This usually involves steps such as removing special symbols, tokenization (chopping
    it up into tokens, typically individual words), lemmatization (recognizing different
    usages of the same word, e.g., organize, organizes, organizing), and count vectorization
    (counting the words that appear in each document). The last step produces a *bag-of-words*
    (BoW) representation of the corpus. In our case, each row (example) of the data
    set will be a review, and each column (feature) will be a word.
  id: totrans-294
  prefs: []
  type: TYPE_NORMAL
  zh: 这通常涉及以下步骤：去除特殊符号、分词（将其分割成标记，通常是单个单词）、词形还原（识别同一单词的不同用法，例如organize, organizes,
    organizing）和计数向量化（计算每个文档中出现的单词）。最后一步产生语料库的*词袋模型*（BoW）表示。在我们的情况下，数据集的每一行（示例）将是一个评论，每一列（特征）将是一个单词。
- en: The example in figure 3.18 illustrates this representation when the sentence
    “this is a terrible terrible movie” is converted to a BoW representation with
    the vocabulary consisting of the words {this, is, a, brilliant, terrible, movie}.
  id: totrans-295
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18中的示例说明了当句子“this is a terrible terrible movie”转换为包含单词{this, is, a, brilliant,
    terrible, movie}的词汇表时的BoW表示。
- en: Since the word “brilliant” doesn’t occur in the review, its count is 0, while
    most of the other entries are 1 corresponding to the fact that they appear once
    in the review. This reviewer apparently thought the movie was doubly terrible—captured
    in our count features as the entry for the feature “terrible” is 2.
  id: totrans-296
  prefs: []
  type: TYPE_NORMAL
  zh: 由于单词“brilliant”在评论中未出现，其计数为0，而大多数其他条目为1，对应于它们在评论中只出现一次的事实。这位评论者显然认为这部电影非常糟糕——在我们的计数特征中，对于“terrible”特征的条目是2。
- en: '![CH03_F18_Kunapuli](../Images/CH03_F18_Kunapuli.png)'
  id: totrans-297
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F18_Kunapuli](../Images/CH03_F18_Kunapuli.png)'
- en: Figure 3.18 Text is converted to a term-document matrix, where each row is an
    example (corresponding to a single review), and each column is a feature (corresponding
    to a word in the review). The entries are word counts, making each example a count
    vector. Removing stop words improves representation and often also improves performance.
  id: totrans-298
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.18 文本被转换为词-文档矩阵，其中每一行是一个示例（对应单个评论），每一列是一个特征（对应评论中的单词）。条目是单词计数，使得每个示例成为一个计数向量。去除停用词可以改善表示，并且通常也会提高性能。
- en: Fortunately, this data set has already been preprocessed by count vectorization.
    These preprocessed term-document count features, our data set, can be found in
    /train/labeledBow.feat and /test/labeledBow.feat. Both the train and test sets
    are of size 25,000 × 89,527\. Thus, there are about 90,000 features (i.e., words),
    meaning that the entire set of reviews used about 90,000 unique words. We preprocess
    the data further with two additional steps, as discussed in the following subsections.
  id: totrans-299
  prefs: []
  type: TYPE_NORMAL
  zh: 幸运的是，这个数据集已经通过计数向量化进行了预处理。这些预处理过的词-文档计数特征，即我们的数据集，可以在/train/labeledBow.feat和/test/labeledBow.feat中找到。训练集和测试集的大小都是25,000
    × 89,527。因此，大约有90,000个特征（即单词），这意味着整个评论集使用了大约90,000个独特的单词。我们进一步通过以下小节中讨论的两个附加步骤对数据进行预处理。
- en: Stop-word removal
  id: totrans-300
  prefs: []
  type: TYPE_NORMAL
  zh: 停用词去除
- en: This step aims to remove common words such as “the,” “is,” “a,” and “an.” Traditionally,
    stop-word removal can reduce the dimensionality of the data (to make processing
    faster) and can improve classification performance. This is because words like
    “the” are often not really informative for information retrieval and text-mining
    tasks.
  id: totrans-301
  prefs: []
  type: TYPE_NORMAL
  zh: 此步骤旨在移除诸如“the”、“is”、“a”和“an”等常见词汇。传统上，停用词去除可以降低数据的维度（使处理更快），并且可以提高分类性能。这是因为像“the”这样的词通常对信息检索和文本挖掘任务并不真正具有信息性。
- en: WARNING Care should be taken with certain stop words such as “not,” as this
    common word significantly affects the underlying semantics and sentiment. For
    example, if we don’t account for negation and apply stop-word removal on the sentence
    “not a good movie,” we get “good movie,” which completely changes the sentiment.
    Here, we don’t selectively account for such stop words, and we rely on the strength
    of other expressive words, such as “awful,” “brilliant,” and “mediocre,” to capture
    sentiment. However, performance on your own data set can be improved by careful
    feature engineering based on an understanding of the vocabulary as well as how
    pruning (or maybe even augmenting) it will affect your task.
  id: totrans-302
  prefs: []
  type: TYPE_NORMAL
  zh: 警告：在处理某些停用词，如“not”时，应谨慎，因为这个常见词会显著影响潜在的语义和情感。例如，如果我们不考虑否定并应用停用词去除到句子“not a good
    movie”上，我们得到“good movie”，这完全改变了情感。在这里，我们不选择性地考虑这样的停用词，而是依靠其他表达性强的单词，如“awful”、“brilliant”和“mediocre”，来捕捉情感。然而，通过基于对词汇以及剪枝（或甚至增强）如何影响您的任务的理解进行仔细的特征工程，可以在您自己的数据集上提高性能。
- en: The Natural Language Toolkit (NLTK) is a powerful Python package that provides
    many tools for NLP. In listing 3.11, we use NLTK’s standard stop-word removal
    tool. The entire vocabulary for the IMDB data set is available in the file imdb.vocab,
    sorted by their frequency, from most common to least common.
  id: totrans-303
  prefs: []
  type: TYPE_NORMAL
  zh: 自然语言处理工具包（NLTK）是一个强大的Python包，它提供了许多NLP工具。在列表3.11中，我们使用NLTK的标准停用词删除工具。IMDB数据集的整个词汇表都可在文件imdb.vocab中找到，按频率排序，从最常见到最少见。
- en: We can directly apply stop-word removal on this set of features to identify
    which words we’ll keep. In addition, we only keep the 5,000 most common words
    in order for our running time to be more manageable.
  id: totrans-304
  prefs: []
  type: TYPE_NORMAL
  zh: 我们可以直接应用停用词删除来识别我们将保留哪些单词。此外，我们只保留最常见的5000个单词，以便我们的运行时间更加可控。
- en: Listing 3.11 Dropping stop words from the vocabulary
  id: totrans-305
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.11 从词汇表中删除停用词
- en: '[PRE29]'
  id: totrans-306
  prefs: []
  type: TYPE_PRE
  zh: '[PRE29]'
- en: ❶ Loads the vocabulary file
  id: totrans-307
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载词汇文件
- en: ❷ Converts the list of stop words to a set for faster processing
  id: totrans-308
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将停用词列表转换为集合以加快处理速度
- en: ❸ Removes stop words from the vocabulary
  id: totrans-309
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 从词汇表中删除停用词
- en: ❹ Keeps the top 5,000 words
  id: totrans-310
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 保留前5000个单词
- en: TF-IDF transformation
  id: totrans-311
  prefs: []
  type: TYPE_NORMAL
  zh: TF-IDF转换
- en: Our second preprocessing step converts the count features to *term frequency-inverse
    document frequency* (TF-IDF) features. TF-IDF represents a statistic that weights
    each feature in a document (in our case, a single review) relative to how often
    it appears in that document as well as how often it appears in the entire corpus
    (in our case, all the reviews).
  id: totrans-312
  prefs: []
  type: TYPE_NORMAL
  zh: 我们的第二个预处理步骤将计数特征转换为**词频-逆文档频率**（TF-IDF）特征。TF-IDF表示一个统计量，它根据每个特征在文档中的出现频率（在我们的情况下，单个评论）以及在整个语料库中的出现频率（在我们的情况下，所有评论）来加权每个文档中的特征。
- en: Intuitively, TF-IDF weights words by how often they appear in a document and
    also adjusts for how often they appear overall, and accounts for the fact that
    some words are generally used more often than others. We can use scikit-learn’s
    preprocessing tool- box to convert our count features to TF-IDF features using
    the TfidfTransformer. Listing 3.12 creates and saves training and test sets, each
    of which comprises 25,000 reviews × 5,000 TF-IDF features.
  id: totrans-313
  prefs: []
  type: TYPE_NORMAL
  zh: 直观地说，TF-IDF通过单词在文档中出现的频率来加权单词，同时也调整了它们在整体中出现的频率，并考虑到了某些单词通常比其他单词使用得更频繁的事实。我们可以使用scikit-learn的预处理工具箱，通过TfidfTransformer将我们的计数特征转换为TF-IDF特征。列表3.12创建并保存了训练集和测试集，每个集包含25,000条评论×5000个TF-IDF特征。
- en: Listing 3.12 Extracting TF-IDF features and saving the data set
  id: totrans-314
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.12 提取TF-IDF特征并保存数据集
- en: '[PRE30]'
  id: totrans-315
  prefs: []
  type: TYPE_PRE
  zh: '[PRE30]'
- en: ❶ Loads train and test data
  id: totrans-316
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载训练和测试数据
- en: ❷ Converts sentiments to binary labels
  id: totrans-317
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将情感转换为二进制标签
- en: ❸ Converts count features to TF-IDF features
  id: totrans-318
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 将计数特征转换为TF-IDF特征
- en: ❹ Saves the preprocessed data sets in the HDF5 binary data format
  id: totrans-319
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 以HDF5二进制数据格式保存预处理后的数据集
- en: 3.4.2 Dimensionality reduction
  id: totrans-320
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.2 降维
- en: We continue to process the data with dimensionality reduction, which aims to
    represent the data more compactly. The main purpose of applying dimensionality
    reduction is to avoid the “curse of dimensionality,” where algorithm performance
    deteriorates as the dimensionality of the data increases.
  id: totrans-321
  prefs: []
  type: TYPE_NORMAL
  zh: 我们继续使用降维处理数据，其目的是更紧凑地表示数据。应用降维的主要目的是避免“维度诅咒”，即随着数据维度的增加，算法性能会下降。
- en: We adopt the popular dimensionality reduction approach of *principal components
    analysis* (PCA), which aims to compress and embed the data into a lower-dimensional
    feature space in a manner that preserves as much of the variability (measured
    using standard deviation or variance) as possible. This ensures that we’re able
    to extract a lower-dimensional representation without too much loss of information.
  id: totrans-322
  prefs: []
  type: TYPE_NORMAL
  zh: 我们采用流行的降维方法**主成分分析**（PCA），其目的是以尽可能保留尽可能多的可变性（使用标准差或方差来衡量）的方式压缩和嵌入数据到低维特征空间中。这确保了我们能够在不损失太多信息的情况下提取低维表示。
- en: This data set contains thousands of examples as well as features, which means
    that applying PCA to the entire data set will likely be highly computationally
    intensive and very slow. To avoid loading the entire data set into memory and
    to process the data more efficiently, we perform incremental PCA (IPCA) instead.
  id: totrans-323
  prefs: []
  type: TYPE_NORMAL
  zh: 此数据集包含数千个示例以及特征，这意味着对整个数据集应用PCA可能会非常计算密集且非常慢。为了避免将整个数据集加载到内存中并更有效地处理数据，我们执行增量PCA（IPCA）。
- en: IPCA breaks the data set down into chunks that can be easily loaded into memory.
    Note, however, that while this chunking reduces the number of samples (rows) loaded
    into memory substantially, it still loads all the features (columns) for each
    row.
  id: totrans-324
  prefs: []
  type: TYPE_NORMAL
  zh: IPCA 将数据集分解成可以轻松加载到内存中的块。然而，请注意，尽管这种分块大大减少了加载到内存中的样本（行）数量，但它仍然为每一行加载了所有特征（列）。
- en: scikit-learn provides the class sklearn.decomposition.IncrementalPCA, which
    is far more memory efficient. The following listing performs PCA to reduce the
    dimension of the data to 500 dimensions.
  id: totrans-325
  prefs: []
  type: TYPE_NORMAL
  zh: scikit-learn 提供了 sklearn.decomposition.IncrementalPCA 类，它具有更高的内存效率。以下列表执行 PCA
    以将数据的维度降低到 500 维。
- en: Listing 3.13 Performing dimensionality reduction using IPCA
  id: totrans-326
  prefs: []
  type: TYPE_NORMAL
  zh: 列表 3.13 使用 IPCA 进行降维
- en: '[PRE31]'
  id: totrans-327
  prefs: []
  type: TYPE_PRE
  zh: '[PRE31]'
- en: ❶ Loads preprocessed train and test data
  id: totrans-328
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 加载预处理后的训练和测试数据
- en: ❷ Applies IPCA to the data in manageable chunks
  id: totrans-329
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 将 IPCA 应用到可管理的数据块中
- en: ❸ Reduces the dimension of both the train and test examples
  id: totrans-330
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 降低训练和测试示例的维度
- en: ❹ Saves the preprocessed data sets in the HDF5 binary data format
  id: totrans-331
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 将预处理后的数据集保存为 HDF5 二进制数据格式
- en: Note that IncrementalPCA is fit using *only* the training set. Recall that the
    test data must *always* be held out and should only be used to provide an accurate
    estimate of how our pipeline will generalize to future, unseen data. This means
    that we can’t use the test data during any part of preprocessing or training and
    can only use it for evaluation.
  id: totrans-332
  prefs: []
  type: TYPE_NORMAL
  zh: 注意，IncrementalPCA 仅使用训练集进行拟合。回想一下，测试数据必须始终保留，并且只能用于提供我们管道如何泛化到未来未见数据的准确估计。这意味着我们无法在预处理或训练的任何部分使用测试数据，而只能用于评估。
- en: 3.4.3 Blending classifiers
  id: totrans-333
  prefs:
  - PREF_H3
  type: TYPE_NORMAL
  zh: 3.4.3 混合分类器
- en: Our goal now is to train a heterogeneous ensemble with meta-learning. Specifically,
    we’ll ensemble several base estimators by blending them. Recall that blending
    is a variant of stacking, where, instead of using CV, we use a single validation
    set.
  id: totrans-334
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在的目标是使用元学习训练一个异构集成。具体来说，我们将通过混合几个基础估计器来构建集成。回想一下，混合是堆叠的一种变体，其中我们不是使用交叉验证（CV），而是使用单个验证集。
- en: 'First, we load the data using the following function:'
  id: totrans-335
  prefs: []
  type: TYPE_NORMAL
  zh: 首先，我们使用以下函数加载数据：
- en: '[PRE32]'
  id: totrans-336
  prefs: []
  type: TYPE_PRE
  zh: '[PRE32]'
- en: 'Next, we use five base estimators: RandomForestClassifier with 100 randomized
    decision trees, ExtraTreesClassifier with 100 extremely randomized trees, Logistic
    Regression, Bernoulli naïve Bayes (BernoulliNB), and a linear SVM trained with
    stochastic gradient descent (SGDClassifier):'
  id: totrans-337
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用五个基础估计器：具有 100 个随机决策树的 RandomForestClassifier，具有 100 个极端随机树的 ExtraTreesClassifier，逻辑回归，伯努利朴素贝叶斯（BernoulliNB），以及使用随机梯度下降（SGDClassifier）训练的线性
    SVM：
- en: '[PRE33]'
  id: totrans-338
  prefs: []
  type: TYPE_PRE
  zh: '[PRE33]'
- en: The Bernoulli naïve Bayes classifier learns linear models but is especially
    effective for count-based data arising from text-mining tasks such as ours. Logistic
    regression and SVM with SGDClassifier both learn linear models. Random forests
    and Extra Trees are two homogeneous ensembles that produce highly nonlinear classifiers
    using decision trees as base estimators. This is a diverse set of base estimators,
    containing a good mix of linear and nonlinear classifiers.
  id: totrans-339
  prefs: []
  type: TYPE_NORMAL
  zh: 伯努利朴素贝叶斯分类器学习线性模型，但特别适用于来自文本挖掘任务（如我们的任务）的基于计数的文本数据。逻辑回归和 SGDClassifier 的 SVM
    都学习线性模型。随机森林和 Extra Trees 是两种同质集成，它们使用决策树作为基础估计器产生高度非线性分类器。这是一个多样化的基础估计器集合，包含线性和非线性分类器的良好混合。
- en: 'To blend these base estimators into a heterogeneous ensemble with meta-learning,
    we use the following procedure:'
  id: totrans-340
  prefs: []
  type: TYPE_NORMAL
  zh: 为了将这些基础估计器混合成一个具有元学习的异构集成，我们使用以下程序：
- en: Split the training data into a training set (Xtrn, ytrn) with 80% of the data
    and a validation set (Xval, yval) with the remaining 20% of the data.
  id: totrans-341
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 将训练数据分为一个包含 80% 数据的训练集（Xtrn, ytrn）和一个包含剩余 20% 数据的验证集（Xval, yval）。
- en: Train each of the level-1 estimators on the training set (Xtrn, ytrn).
  id: totrans-342
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 在训练集（Xtrn, ytrn）上训练每个一级估计器。
- en: Generate meta-features Xmeta with the trained estimators using Xval.
  id: totrans-343
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用训练估计器通过 Xval 生成元特征 Xmeta。
- en: 'Augment the validation data with the meta-features: [Xval, Xmeta]; this augmented
    validation set will have 500 original features + 5 meta-features.'
  id: totrans-344
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用元特征增强验证数据：[Xval, Xmeta]；这个增强的验证集将包含 500 个原始特征 + 5 个元特征。
- en: Train the level-2 estimator with the augmented validation set ([Xval, Xmeta],
    yval).
  id: totrans-345
  prefs:
  - PREF_OL
  type: TYPE_NORMAL
  zh: 使用增强的验证集（[Xval, Xmeta], yval）训练二级估计器。
- en: 'The key to our combining-by-meta-learning procedure is meta-feature augmentation:
    we augment the validation set with the meta-features produced by the base estimators.'
  id: totrans-346
  prefs: []
  type: TYPE_NORMAL
  zh: 我们通过元学习组合程序的关键是元特征增强：我们使用基础估计器产生的元特征增强验证集。
- en: 'This leaves one final decision: the choice of the level-2 estimator. Previously,
    we used simple linear classifiers. For this classification task, we use a neural
    network.'
  id: totrans-347
  prefs: []
  type: TYPE_NORMAL
  zh: 这留下了最后一个决定：选择第二级估计器。之前，我们使用了简单的线性分类器。对于这个分类任务，我们使用神经网络。
- en: Neural networks and deep learning
  id: totrans-348
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络和深度学习
- en: Neural networks are one of the oldest machine-learning algorithms. There has
    been a significant resurgence of interest in neural networks, especially deep
    neural networks, owing to their widespread success in many applications.
  id: totrans-349
  prefs: []
  type: TYPE_NORMAL
  zh: 神经网络是机器学习算法中最古老的之一。由于在许多应用中取得了广泛的成功，人们对神经网络，尤其是深度神经网络，的兴趣显著复苏。
- en: For a quick refresher on neural networks and deep learning, see chapter 2 of
    *Probabilistic Deep Learning with Python*, *Keras*, and *TensorFlow Probability*
    by Oliver Dürr, Beate Sick, and Elvis Murina (Manning, 2020).
  id: totrans-350
  prefs: []
  type: TYPE_NORMAL
  zh: 为了快速回顾神经网络和深度学习，请参阅Oliver Dürr、Beate Sick和Elvis Murina（Manning, 2020）所著的《Python概率深度学习》、《Keras》和《TensorFlow
    Probability》的第2章。
- en: 'We’ll use a shallow neural network as our level-2 estimator. This will produce
    a highly nonlinear meta-estimator that can combine the predictions of the level-1
    classifiers:'
  id: totrans-351
  prefs: []
  type: TYPE_NORMAL
  zh: 我们将使用浅层神经网络作为我们的第二级估计器。这将产生一个高度非线性的元估计器，可以结合第一级分类器的预测：
- en: '[PRE34]'
  id: totrans-352
  prefs: []
  type: TYPE_PRE
  zh: '[PRE34]'
- en: The following listing implements our strategy.
  id: totrans-353
  prefs: []
  type: TYPE_NORMAL
  zh: 下面的列表实现了我们的策略。
- en: Listing 3.14 Blending models with a validation set
  id: totrans-354
  prefs: []
  type: TYPE_NORMAL
  zh: 列表3.14 使用验证集混合模型
- en: '[PRE35]'
  id: totrans-355
  prefs: []
  type: TYPE_PRE
  zh: '[PRE35]'
- en: ❶ Splits into training and validation sets
  id: totrans-356
  prefs: []
  type: TYPE_NORMAL
  zh: ❶ 分割为训练集和验证集
- en: ❷ Initializes and fits the base estimators on the training data
  id: totrans-357
  prefs: []
  type: TYPE_NORMAL
  zh: ❷ 在训练数据上初始化和拟合基础估计器
- en: ❸ Augments the validation set with the newly generated meta-features
  id: totrans-358
  prefs: []
  type: TYPE_NORMAL
  zh: ❸ 使用新生成的元特征增强验证集
- en: ❹ Fits the level-2 meta-estimator
  id: totrans-359
  prefs: []
  type: TYPE_NORMAL
  zh: ❹ 符合第二级元估计器的级别
- en: 'We can now fit a heterogeneous ensemble on the training data:'
  id: totrans-360
  prefs: []
  type: TYPE_NORMAL
  zh: 我们现在可以在训练数据上拟合一个异构集成：
- en: '[PRE36]'
  id: totrans-361
  prefs: []
  type: TYPE_PRE
  zh: '[PRE36]'
- en: Then, we evaluate it on both the training and test data to compute the training
    and test error. First, we compute the training error with
  id: totrans-362
  prefs: []
  type: TYPE_NORMAL
  zh: 然后，我们在训练数据和测试数据上评估它，以计算训练和测试错误。首先，我们使用以下方法计算训练错误
- en: '[PRE37]'
  id: totrans-363
  prefs: []
  type: TYPE_PRE
  zh: '[PRE37]'
- en: 'which gives us a training error of 7.84%:'
  id: totrans-364
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了7.84%的训练错误：
- en: '[PRE38]'
  id: totrans-365
  prefs: []
  type: TYPE_PRE
  zh: '[PRE38]'
- en: Next, we compute the test error with
  id: totrans-366
  prefs: []
  type: TYPE_NORMAL
  zh: 接下来，我们使用以下方法计算测试错误
- en: '[PRE39]'
  id: totrans-367
  prefs: []
  type: TYPE_PRE
  zh: '[PRE39]'
- en: 'which gives us a test error of 17.2%:'
  id: totrans-368
  prefs: []
  type: TYPE_NORMAL
  zh: 这给我们带来了17.2%的测试错误：
- en: '[PRE40]'
  id: totrans-369
  prefs: []
  type: TYPE_PRE
  zh: '[PRE40]'
- en: So how well did we actually do? Did our ensembling procedure help at all? To
    answer these questions, we compare the performance of the ensemble to the performance
    of each base estimator in the ensemble.
  id: totrans-370
  prefs: []
  type: TYPE_NORMAL
  zh: 我们实际上做得怎么样？我们的集成过程是否有所帮助？为了回答这些问题，我们将集成性能与集成中每个基础估计器的性能进行比较。
- en: Figure 3.19 shows the training and test errors of the individual base estimators
    as well as the stacking/blending ensemble. Some individual classifiers achieve
    a training error of 0%, which means they are likely overfitting the training data.
    This affects their performance as evidenced by the test error.
  id: totrans-371
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19显示了各个基础估计器的训练和测试错误，以及堆叠/混合集成。一些个别分类器达到了0%的训练错误，这意味着它们很可能是过度拟合了训练数据。这影响了它们的性能，正如测试错误所证明的那样。
- en: '![CH03_F19_Kunapuli](../Images/CH03_F19_Kunapuli.png)'
  id: totrans-372
  prefs: []
  type: TYPE_IMG
  zh: '![CH03_F19_Kunapuli](../Images/CH03_F19_Kunapuli.png)'
- en: Figure 3.19 Comparing the performance of each individual base classifier with
    the meta-classifier ensemble. Stacking/blending improves classification performance
    by ensembling diverse base classifiers.
  id: totrans-373
  prefs: []
  type: TYPE_NORMAL
  zh: 图3.19比较了每个个别基础分类器与元分类器集成的性能。堆叠/混合通过集成多样化的基础分类器提高了分类性能。
- en: Overall, stacking/blending these heterogeneous models produces a test error
    of 17.2%, which is better than all the other models. In particular, let’s compare
    this result to logistic regression with a test error of 18%. Recall that the test
    set contains 25,000 examples, which means that our stacked model classifies (approximately)
    another 200 examples correctly!
  id: totrans-374
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，堆叠/混合这些异构模型产生了17.2%的测试错误，这比所有其他模型都要好。特别是，让我们将这个结果与测试错误为18%的逻辑回归进行比较。回想一下，测试集包含25,000个示例，这意味着我们的堆叠模型正确分类（大约）另外200个示例！
- en: On the whole, the performance of the heterogeneous ensemble is better than a
    lot of the base estimators that contribute to it. This is an example of how heterogeneous
    ensembles can improve the overall performance of the underlying individual base
    estimators.
  id: totrans-375
  prefs: []
  type: TYPE_NORMAL
  zh: 总体而言，异构集成的性能优于许多为其做出贡献的基础估计器。这是异构集成如何提高底层个别基础估计器整体性能的一个例子。
- en: TIP Remember that any linear or nonlinear classifier can be used as a meta-estimator.
    Common choices include decision trees, kernel SVMs, and even other ensembles!
  id: totrans-376
  prefs: []
  type: TYPE_NORMAL
  zh: TIP 记住，任何线性或非线性分类器都可以用作元估计器。常见的选择包括决策树、核支持向量机（SVMs），甚至是其他集成方法！
- en: Summary
  id: totrans-377
  prefs:
  - PREF_H2
  type: TYPE_NORMAL
  zh: 摘要
- en: Heterogeneous ensemble methods promote ensemble diversity through heterogeneity;
    that is, they use different base-learning algorithms to train the base estimators.
  id: totrans-378
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 异构集成方法通过异构性促进集成多样性；也就是说，它们使用不同的基础学习算法来训练基础估计器。
- en: Weighting methods assign individual base-estimator predictions a weight that
    corresponds to their performance; better base estimators are assigned higher weights
    and influence the overall final prediction more.
  id: totrans-379
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权方法为每个基础估计器的预测分配一个与其性能相对应的权重；更好的基础估计器被分配更高的权重，对最终预测的影响更大。
- en: Weighting methods use a predefined combination function to combine the weighted
    predictions of the individual base estimators. Linear combination functions (e.g.,
    weighted sum) are often effective and easy to interpret. Nonlinear combination
    functions can also be used, though the added complexity may lead to overfitting.
  id: totrans-380
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权方法使用预定义的组合函数来组合单个基础估计器的加权预测。线性组合函数（例如，加权求和）通常有效且易于解释。也可以使用非线性组合函数，尽管增加的复杂性可能导致过拟合。
- en: Meta-learning methods learn a combination function from the data, in contrast
    to weighting methods, where we have to make one up ourselves.
  id: totrans-381
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元学习方法从数据中学习一个组合函数，与加权方法不同，后者我们必须自己想出一个。
- en: Meta-learning methods create multiple layers of estimators. The most common
    meta-learning method is stacking, so called because it literally stacks learning
    algorithms in a pyramid-like learning scheme.
  id: totrans-382
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 元学习方法创建了多个估计器层。最常用的元学习方法是堆叠（stacking），这个名字来源于它实际上是在一种金字塔式的学习方案中堆叠学习算法。
- en: Simple stacking creates two levels of estimators. The base estimators are trained
    in the first level, and their outputs are used to train a second-level estimator
    called a meta-estimator. More complex stacking models with many more levels of
    estimators are possible.
  id: totrans-383
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 简单的堆叠创建了两个估计器层。基础估计器在第一层进行训练，它们的输出用于训练第二层估计器，称为元估计器。更复杂的堆叠模型，具有更多估计器层，也是可能的。
- en: Stacking can often overfit, especially in the presence of noisy data. To avoid
    overfitting, stacking is combined with cross validation (CV) to ensure that different
    base estimators see different subsets of the data set for increased ensemble diversity.
  id: totrans-384
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 堆叠往往会导致过拟合，尤其是在存在噪声数据的情况下。为了避免过拟合，堆叠与交叉验证（CV）结合使用，以确保不同的基础估计器看到数据集的不同子集，从而增加集成多样性。
- en: Stacking with CV, though it reduces overfitting, can also be computationally
    intensive, leading to long training times. To speed up training while guarding
    against overfitting, a single validation set can be used. This procedure is known
    as blending.
  id: totrans-385
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 虽然带有交叉验证（CV）的堆叠可以减少过拟合，但它也可能计算密集，导致训练时间过长。为了在防止过拟合的同时加快训练速度，可以使用单个验证集。这个过程被称为混合。
- en: Any machine-learning algorithm can be used as a meta-estimator in stacking.
    Logistic regression is the most common and leads to linear models. Nonlinear models,
    obviously, have greater representative power, but they are also at a greater risk
    for overfitting.
  id: totrans-386
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 任何机器学习算法都可以用作堆叠中的元估计器。逻辑回归是最常见的，它导致线性模型。显然，非线性模型具有更大的代表性能力，但它们也面临着更大的过拟合风险。
- en: Both weighting and meta-learning approaches can use either the base-estimator
    predictions directly or the prediction probabilities. The latter typically leads
    to a smoother, more nuanced model.
  id: totrans-387
  prefs:
  - PREF_UL
  type: TYPE_NORMAL
  zh: 加权和元学习方法都可以直接使用基础估计器的预测或预测概率。后者通常导致更平滑、更细腻的模型。
- en: '* * *'
  id: totrans-388
  prefs: []
  type: TYPE_NORMAL
  zh: '***'
- en: ^(1.) Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,
    and Christopher Potts, “Learning Word Vectors for Sentiment Analysis,” 2011, [http://mng.bz/nJRe](http://mng.bz/nJRe).
  id: totrans-389
  prefs: []
  type: TYPE_NORMAL
  zh: ^（1.）Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng,
    和 Christopher Potts, “用于情感分析的学习词向量，”2011年，[http://mng.bz/nJRe](http://mng.bz/nJRe)。
